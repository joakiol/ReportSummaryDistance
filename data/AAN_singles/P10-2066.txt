Proceedings of the ACL 2010 Conference Short Papers, pages 359?364,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsDistributional Similarity vs. PU Learning for Entity Set ExpansionXiao-Li  LiInstitute for Infocomm Research,1 Fusionopolis Way #21-01 ConnexisSingapore 138632xlli@i2r.a-star.edu.sgLei ZhangUniversity of Illinois at Chicago,851 South Morgan Street, Chicago,Chicago, IL 60607-7053, USAzhang3@cs.uic.eduBing LiuUniversity of Illinois at Chicago,851 South Morgan Street, Chicago,Chicago, IL 60607-7053, USAliub@cs.uic.eduSee-Kiong  NgInstitute for Infocomm Research,1 Fusionopolis Way #21-01 ConnexisSingapore 138632skng@i2r.a-star.edu.sgAbstractDistributional similarity is a classic tech-nique for entity set expansion, where thesystem is given a set of seed entities of aparticular class, and is asked to expand theset using a corpus to obtain more entitiesof the same class as represented by theseeds.
This paper shows that a machinelearning model called positive and unla-beled learning (PU learning) can modelthe set expansion problem better.
Basedon the test results of 10 corpora, we showthat a PU learning technique outperformeddistributional similarity significantly.1 IntroductionThe entity set expansion problem is defined asfollows: Given a set S of seed entities of a partic-ular class, and a set D of candidate entities (e.g.,extracted from a text corpus), we wish to deter-mine which of the entities in D belong to S. Inother words, we ?expand?
the set S based on thegiven seeds.
This is clearly a classification prob-lem which requires arriving at a binary decisionfor each entity in D (belonging to S or not).However, in practice, the problem is often solvedas a ranking problem, i.e., ranking the entities inD based on their likelihoods of belonging to S.The classic method for solving this problem isbased on distributional similarity (Pantel et al2009; Lee, 1998).
The approach works by com-paring the similarity of the surrounding worddistributions of each candidate entity with theseed entities, and then ranking the candidate enti-ties using their similarity scores.In machine learning, there is a class of semi-supervised learning algorithms that learns frompositive and unlabeled examples (PU learning forshort).
The key characteristic of PU learning isthat there is no negative training example availa-ble for learning.
This class of algorithms is lessknown to the natural language processing (NLP)community compared to some other semi-supervised learning models and algorithms.PU learning is a two-class classification mod-el.
It is stated as follows (Liu et al 2002): Givena set P of positive examples of a particular classand a set U of unlabeled examples (containinghidden positive and negative cases), a classifieris built using P and U for classifying the data inU or future test cases.
The results can be eitherbinary decisions (whether each test case belongsto the positive class or not), or a ranking basedon how likely each test case belongs to the posi-tive class represented by P. Clearly, the set ex-pansion problem can be mapped into PU learningexactly, with S and D as P and U respectively.This paper shows that a PU learning methodcalled S-EM (Liu et al 2002) outperforms distri-butional similarity considerably based on theresults from 10 corpora.
The experiments in-volved extracting named entities (e.g., productand organization names) of the same type orclass as the given seeds.
Additionally, we alsocompared S-EM with a recent method, calledBayesian Sets (Ghahramani and Heller, 2005),which was designed specifically for set expan-sion.
It also does not perform as well as PUlearning.
We will explain why PU learning per-forms better than both methods in Section 5.
Webelieve that this finding is of interest to the NLPcommunity.359There is another approach used in the Webenvironment for entity set expansion.
It exploitsWeb page structures to identify lists of items us-ing wrapper induction or other techniques.
Theidea is that items in the same list are often of thesame type.
This approach is used by Google Sets(Google, 2008) and Boo!Wa!
(Wang and Cohen,2008).
However, as it relies on Web page struc-tures, it is not applicable to general free texts.2 Three Different Techniques2.1 Distributional SimilarityDistributional similarity is a classic technique forthe entity set expansion problem.
It is based onthe hypothesis that words with similar meaningstend to appear in similar contexts (Harris, 1985).As such, a method based on distributional simi-larity typically fetches the surrounding contextsfor each term (i.e.
both seeds and candidates) andrepresents them as vectors by using TF-IDF orPMI (Pointwise Mutual Information) values (Lin,1998; Gorman and Curran, 2006; Pa?ca et al2006; Agirre et al 2009; Pantel et al 2009).
Si-milarity measures such as Cosine, Jaccard, Dice,etc, can then be employed to compute the simi-larities between each candidate vector and theseeds centroid vector (one centroid vector for allseeds).
Lee (1998) surveyed and discussed vari-ous distribution similarity measures.2.2 PU Learning and S-EMPU learning is a semi-supervised or partially su-pervised learning model.
It learns from positiveand unlabeled examples as opposed to the modelof learning from a small set of labeled examplesof every class and a large set of unlabeled exam-ples, which we call LU learning (L and U standfor labeled and unlabeled respectively) (Blumand Mitchell, 1998; Nigam et al 2000)There are several PU learning algorithms (Liuet al 2002; Yu et al 2002; Lee and Liu, 2003; Liet al 2003; Elkan and Noto, 2008).
In this work,we used the S-EM algorithm given in (Liu et al2002).
S-EM is efficient as it is based on na?veBayesian (NB) classification and also performswell.
The main idea of S-EM is to use a spytechnique to identify some reliable negatives(RN) from the unlabeled set U, and then use anEM algorithm to learn from P, RN and U?RN.The spy technique in S-EM works as follows(Figure 1): First, a small set of positive examples(denoted by SP) from P is randomly sampled(line 2).
The default sampling ratio in S-EM is s= 15%, which we also used in our experiments.The positive examples in SP are called ?spies?.Then, a NB classifier is built using the set P?
SPas positive and the set U?SP as negative (line 3,4, and 5).
The NB classifier is applied to classifyeach u ?
U?SP, i.e., to assign a probabilisticclass label p(+|u) (+ means positive).
The proba-bilistic labels of the spies are then used to decidereliable negatives (RN).
In particular, a probabili-ty threshold t is determined using the probabilis-tic labels of spies in SP and the input parameter l(noise level).
Due to space constraints, we areunable to explain l. Details can be found in (Liuet al 2002).
t is then used to find RN from U(lines 8-10).
The idea of the spy technique isclear.
Since spy examples are from P and are putinto U in building the NB classifier, they shouldbehave similarly to the hidden positive cases inU.
Thus, they can help us find the set RN.Algorithm Spy(P, U, s, l)1.
RN ?
?
;            // Reliable negative set2.
SP ?
Sample(P, s%);3.
Assign each example in P ?
SP the class label +1;4.
Assign each example in U ?
SP the class label -1;5.
C ?NB(P ?
S, U?SP); // Produce a NB classifier6.
Classify each u ?U?SP using C;7.
Decide a probability threshold t using SP and l;8.  for each u ?U do9.
if its probability p(+|u) < t then10.
RN ?
RN ?
{u};Figure 1.
Spy technique for extracting reliablenegatives (RN) from U.Given the positive set P, the reliable negativeset RN and the remaining unlabeled set U?RN, anExpectation-Maximization (EM) algorithm isrun.
In S-EM, EM uses the na?ve Bayesian clas-sification as its base method.
The detailed algo-rithm is given in (Liu et al 2002).2.3 Bayesian SetsBayesian Sets, as its name suggests, is based onBayesian inference, and was designed specifical-ly for the set expansion problem (Ghahramaniand Heller, 2005).
The algorithm learns from aseeds set (i.e., a positive set P) and an unlabeledcandidate set U.
Although it was not designed asa PU learning method, it has similar characteris-tics and produces similar results as PU learning.However, there is a major difference.
PU learn-ing is a classification model, while Bayesian Setsis a ranking method.
This difference has a majorimplication on the results that they produce as wewill discuss in Section 5.3.In essence, Bayesian Sets learns a score func-360tion using P and U to generate a score for eachunlabeled case u ?
U.
The function is as follows:)()|()(upPupuscore =  (1)where p(u|P) represents how probable u belongsto the positive class represented by P. p(u) is theprior probability of u.
Using the Bayes?
rule, eq-uation (1) can be re-written as:)()(),()(PpupPupuscore =                    (2)Following the idea, Ghahramani and Heller(2005) proposed a computable score function.The scores can be used to rank the unlabeledcandidates in U to reflect how likely each u ?
Ubelongs to P. The mathematics for computing thescore is involved.
Due to the limited space, wecannot discuss it here.
See (Ghahramani and Hel-ler, 2005) for details.
In (Heller and Ghahramani,2006), Bayesian Sets was also applied to an im-age retrieval application.3 Data Generation for DistributionalSimilarity, Bayesian Sets and S-EMPreparing the data for distributional similarity isfairly straightforward.
Given the seeds set S, aseeds centroid vector is produced using the sur-rounding word contexts (see below) of all occur-rences of all the seeds in the corpus (Pantel et al2009).
In a similar way, a centroid is also pro-duced for each candidate (or unlabeled) entity.Candidate entities: Since we are interested innamed entities, we select single words or phrasesas candidate entities based on their correspond-ing part-of-speech (POS) tags.
In particular, wechoose the following POS tags as entity indica-tors ?
NNP (proper noun), NNPS (plural propernoun), and CD (cardinal number).
We regard aphrase (could be one word) with a sequence ofNNP, NNPS and CD POS tags as one candidateentity (CD cannot be the first word unless itstarts with a letter), e.g., ?Windows/NNP 7/CD?and ?Nokia/NNP N97/CD?
are regarded as twocandidates ?Windows 7?
and ?Nokia N97?.Context: For each seed or candidate occurrence,the context is its set of surrounding words withina window of size w, i.e.
we use w words rightbefore the seed or the candidate and w wordsright after it.
Stop words are removed.For S-EM and Bayesian Sets, both the posi-tive set P (based on the seeds set S) and the unla-beled candidate set U are generated differently.They are not represented as centroids.Positive and unlabeled sets: For each seed si ?S,each occurrence in the corpus forms a vector as apositive example in P. The vector is formedbased on the surrounding words context (seeabove) of the seed mention.
Similarly, for eachcandidate d ?
D (see above; D denotes the set ofall candidates), each occurrence also forms avector as an unlabeled example in U.
Thus, eachunique seed or candidate entity may producemultiple feature vectors, depending on the num-ber of times that it appears in the corpus.The components in the feature vectors areterm frequencies for S-EM as S-EM uses na?veBayesian classification as its base classifier.
ForBayesian Sets, they are 1?s and 0?s as BayesianSets only takes binary vectors based on whethera term occurs in the context or not.4 Candidate RankingFor distributional similarity, ranking is done us-ing the similarity value of each candidate?s cen-troid and the seeds?
centroid (one centroid vectorfor all seeds).
Rankings for S-EM and BayesianSets are more involved.
We discuss them below.After it ends, S-EM produces a Bayesian clas-sifier C, which is used to classify each vector u ?U and to assign a probability p(+|u) to indicatethe likelihood that u belongs to the positive class.Similarly, Bayesian Sets produces a scorescore(u) for each u (not a probability).Recall that for both S-EM and Bayesian Sets,each unique candidate entity may generate mul-tiple feature vectors, depending on the number oftimes that the candidate entity occurs in the cor-pus.
As such, the rankings produced by S-EMand Bayesian Sets are not the rankings of theentities, but rather the rankings of the entities?occurrences.
Since different vectors representingthe same candidate entity can have very differentprobabilities (for S-EM) or scores (for BayesianSets), we need to combine them and compute asingle score for each unique candidate entity forranking.To this end, we also take the entity frequencyinto consideration.
Typically, it is highly desira-ble to rank those correct and frequent entities atthe top because they are more important than theinfrequent ones in applications.
With this inmind, we define a ranking method.Let the probabilities (or scores) of a candidateentity d ?
D be Vd = {v1 , v2 ?, vn} for the n fea-ture vectors of the candidate.
Let Md be the me-dian of Vd.
The final score (fs) for d is defined as:)1log()( nMdfs d +?=         (3)361The use of the median of Vd can be justifiedbased on the statistical skewness (Neter et al1993).
If the values in Vd are skewed towards thehigh side (negative skew), it means that the can-didate entity is very likely to be a true entity, andwe should take the median as it is also high(higher than the mean).
However, if the skew istowards the low side (positive skew), it meansthat the candidate entity is unlikely to be a trueentity and we should again use the median as it islow (lower than the mean) under this condition.Note that here n is the frequency count ofcandidate entity d in the corpus.
The constant 1 isadded to smooth the value.
The idea is to pushthe frequent candidate entities up by multiplyingthe logarithm of frequency.
log is taken in orderto reduce the effect of big frequency counts.The final score fs(d) indicates candidate d?soverall likelihood to be a relevant entity.
A highfs(d) implies a high likelihood that d is in theexpanded entity set.
We can then rank all thecandidates based on their fs(d) values.5 Experimental EvaluationWe empirically evaluate the three techniques inthis section.
We implemented distribution simi-larity and Bayesian Sets.
S-EM was downloadedfrom http://www.cs.uic.edu/~liub/S-EM/S-EM-download.html.
For both Bayesian Sets and S-EM, we used their default parameters.
EM in S-EM ran only two iterations.
For distributionalsimilarity, we tested TF-IDF and PMI as featurevalues of vectors, and Cosine and Jaccard as si-milarity measures.
Due to space limitations, weonly show the results of the PMI and Cosinecombination as it performed the best.
This com-bination was also used in (Pantel et al, 2009).5.1 Corpora and Evaluation MetricsWe used 10 diverse corpora to evaluate the tech-niques.
They were obtained from a commercialcompany.
The data were crawled and extractedfrom multiple online message boards and blogsdiscussing different products and services.
Wesplit each message into sentences, and the sen-tences were POS-tagged using Brill?s tagger(Brill, 1995).
The tagged sentences were used toextract candidate entities and their contexts.
Ta-ble 1 shows the domains and the number of sen-tences in each corpus, as well as the three seedentities used in our experiments for each corpus.The three seeds for each corpus were randomlyselected from a set of common entities in the ap-plication domain.Table 1.
Descriptions of the 10 corporaDomains # Sentences Seed EntitiesBank 17394 Citi, Chase, WesabeBlu-ray 7093 S300, Sony, SamsungCar 2095 Honda, A3, ToyotaDrug 1504 Enbrel, Hurmia, MethotrexateInsurance 12419 Cobra, Cigna, KaiserLCD 1733 PZ77U, Samsung, SonyMattress 13191 Simmons, Serta, HeavenlyPhone 14884 Motorola, Nokia, N95Stove 25060 Kenmore, Frigidaire, GEVacuum 13491 Dc17, Hoover, RoombaThe regular evaluation metrics for named enti-ty recognition such as precision and recall are notsuitable for our purpose as we do not have thecomplete sets of gold standard entities to com-pare with.
We adopt rank precision, which iscommonly used for evaluation of entity set ex-pansion techniques (Pantel et al, 2009):Precision @ N: The percentage of correct enti-ties among the top N entities in the ranked list.5.2 Experimental ResultsThe detailed experimental results for windowsize 3 (w=3) are shown in Table 2 for the 10 cor-pora.
We present the precisions at the top 15-,30- and 45-ranked positions (i.e., precisions@15, 30 and 45) for each corpus, with the aver-age given in the last column.
For distributionalsimilarity, to save space Table 2 only shows theresults of Distr-Sim-freq, which is the distribu-tional similarity method with term frequencyconsidered in the same way as for Bayesian Setsand S-EM, instead of the original distributionalsimilarity, which is denoted by Distr-Sim.
Thisis because on average, Distr-Sim-freq performsbetter than Distr-Sim.
However, the summaryresults of both Distr-Sim-freq and Distr-Sim aregiven in Table 3.From Table 2, we observe that on average S-EM outperforms Distr-Sim-freq by about 12 ?20% in terms of Precision @ N. Bayesian-Setsis also more accurate than Distr-Sim-freq, but S-EM outperforms Bayesian-Sets by 9 ?
10%.To test the sensitivity of window size w, wealso experimented with w = 6 and w = 9.
Due tospace constraints, we present only their averageresults in Table 3.
Again, we can see the sameperformance pattern as in Table 2 (w = 3): S-EMperforms the best, Bayesian-Sets the second, andthe two distributional similarity methods thethird and the fourth, with Distr-Sim-freq slightlybetter than Distr-Sim.3625.3 Why does S-EM Perform Better?From the tables, we can see that both S-EM andBayesian Sets performed better than distribution-al similarity.
S-EM is better than Bayesian Sets.We believe that the reason is as follows: Distri-butional similarity does not use any informationin the candidate set (or the unlabeled set U).
Ittries to rank the candidates solely through simi-larity comparisons with the given seeds (or posi-tive cases).
Bayesian Sets is better because itconsiders U.
Its learning method produces aweight vector for features based on their occur-rence differences in the positive set P and theunlabeled set U (Ghahramani and Heller 2005).This weight vector is then used to compute thefinal scores used in ranking.
In this way, Baye-sian Sets is able to exploit the useful informationin U that was ignored by distributional similarity.S-EM also considers these differences in its NBclassification; in addition, it uses the reliablenegative set (RN) to help distinguish negativeand positive cases, which both Bayesian Sets anddistributional similarity do not do.
We believethis balanced attempt by S-EM to distinguish thepositive and negative cases is the reason for thebetter performance of S-EM.
This raises an inter-esting question.
Since Bayesian Sets is a rankingmethod and S-EM is a classification method, canwe say even for ranking (our evaluation is basedon ranking) classification methods produce betterresults than ranking methods?
Clearly, our singleexperiment cannot answer this question.
But in-tuitively, classification, which separates positiveand negative cases by pulling them towards twoopposite directions, should perform better thanranking which only pulls the data in one direc-tion.
Further research on this issue is needed.6 Conclusions and Future WorkAlthough distributional similarity is a classictechnique for entity set expansion, this papershowed that PU learning performs considerablybetter on our diverse corpora.
In addition, PUlearning also outperforms Bayesian Sets (de-signed specifically for the task).
In our futurework, we plan to experiment with various otherPU learning methods (Liu et al 2003; Lee andLiu, 2003; Li et al 2007; Elkan and Noto, 2008)on this entity set expansion task, as well as othertasks that were tackled using distributional simi-larity.
In addition, we also plan to combine somesyntactic patterns (Etzioni et al 2005; Sarmentoet al 2007) to further improve the results.Acknowledgements: Bing Liu and Lei Zhangacknowledge the support of HP Labs InnovationResearch Grant 2009-1062-1-A, and would liketo thank Suk Hwan Lim and Eamonn O'Brien-Strain for many helpful discussions.Table 2.
Precision @ top N (with 3 seeds, and window size w = 3)Bank Blu-ray Car  Drug Insurance LCD Mattress Phone Stove  Vacuum Avg.Top 15Distr-Sim-freq 0.466 0.333 0.800 0.666 0.666 0.400 0.666 0.533 0.666 0.733 0.592Bayesian-Sets 0.533 0.266 0.600 0.666 0.600 0.733 0.666 0.533 0.800 0.800 0.617S-EM 0.600 0.733 0.733 0.733 0.533 0.666 0.933 0.533 0.800 0.933 0.720Top 30Distr-Sim-freq 0.466 0.266 0.700 0.600 0.500 0.333 0.500 0.466 0.600 0.566 0.499Bayesian-Sets 0.433 0.300 0.633 0.666 0.400 0.566 0.700 0.333 0.833 0.700 0.556S-EM 0.500 0.700 0.666 0.666 0.566 0.566 0.733 0.600 0.600 0.833 0.643Top 45Distr-Sim-freq 0.377 0.288 0.555 0.500 0.377 0.355 0.444 0.400 0.533 0.400 0.422Bayesian-Sets 0.377 0.333 0.666 0.555 0.377 0.511 0.644 0.355 0.733 0.600 0.515S-EM 0.466 0.688 0.644 0.733 0.533 0.600 0.644 0.555 0.644 0.688 0.620Table 3.
Average precisions over the 10 corpora of different window size (3 seeds)Window-size w = 3   Window-size  w = 6  Window-size  w = 9Top Results Top 15 Top 30 Top 45  Top 15 Top 30 Top 45  Top 15 Top 30 Top 45Distr-Sim 0.579 0.466 0.410  0.553 0.483 0.439  0.519 0.473 0.412Distr-Sim-freq 0.592 0.499 0.422  0.553 0.492 0.441  0.559 0.476 0.410Bayesian-Sets 0.617 0.556 0.515  0.593 0.539 0.524  0.539 0.522 0.497S-EM 0.720 0.643 0.620  0.666 0.606 0.597  0.666 0.620 0.604363ReferencesAgirre, E., Alfonseca, E., Hall, K., Kravalova, J.,Pasca, M., and Soroa, A.
2009.
A study on si-milarity and relatedness using distributionaland WordNet-based approaches.
NAACLHLT.Blum, A. and Mitchell, T. 1998.
Combining la-beled and unlabeled data with co-training.
InProc.
of Computational Learning Theory, pp.92?100, 1998.Brill, E. 1995.
Transformation-Based error-Driven learning and natural languageprocessing: a case study in part of speechtagging.
Computational Linguistics.Bunescu, R. and Mooney, R. 2004.
Collectiveinformation extraction with relational MarkovNetworks.
ACL.Cheng T., Yan X. and Chang C. K. 2007.
Entity-Rank: searching entities directly and holisti-cally.
VLDB.Chieu, H.L.
and Ng, H. Tou.
2002.
Name entityrecognition: a maximum entropy approachusing global information.
In The 6th Work-shop on Very Large Corpora.Downey, D., Broadhead, M. and Etzioni, O.2007.
Locating complex named entities inWeb Text.
IJCAI.Elkan, C. and Noto, K. 2008.
Learning classifi-ers from only positive and unlabeled data.KDD, 213-220.Etzioni, O., Cafarella, M., Downey.
D., Popescu,A., Shaked, T., Soderland, S., Weld, D. Yates.2005.
A. Unsupervised named-entity extrac-tion from the Web: An Experimental Study.Artificial Intelligence, 165(1):91-134.Ghahramani, Z and Heller, K.A.
2005.
Bayesiansets.
NIPS.Google Sets.
2008.
System and methods for au-tomatically creating lists.
US Patent:US7350187, March 25.Gorman, J. and Curran, J. R. 2006.
Scaling dis-tributional similarity to large corpora.
ACL.Harris, Z. Distributional Structure.
1985.
In:Katz, J. J.
(ed.
), The philosophy of linguistics.Oxford University Press.Heller, K. and Ghahramani, Z.
2006.
A simpleBayesian framework for content-based imageretrieval.
CVPR.Isozaki, H. and Kazawa, H. 2002.
Efficient sup-port vector classifiers for named entity recog-nition.
COLING.Jiang, J. and Zhai, C. 2006.
Exploiting domainstructure for named entity recognition.
HLT-NAACL.Lafferty J., McCallum A., and Pereira F. 2001.Conditional random fields: probabilisticmodels for segmenting and labeling sequencedata.
ICML.Lee, L. 1999.
Measures of distributional similar-ity.
ACL.Lee, W-S. and Liu, B.
2003.
Learning with Posi-tive and Unlabeled Examples Using WeightedLogistic Regression.
ICML.Li, X., Liu, B.
2003.
Learning to classify textsusing positive and unlabeled data, IJCAI.Li, X., Liu, B., Ng, S. 2007.
Learning to identifyunexpected instances in the test sSet.
IJCAI.Lin, D. 1998.
Automatic retrieval and clusteringof similar words.
COLING/ACL.Liu, B, Lee, W-S, Yu, P. S, and Li, X.
2002.Partially supervised text classification.
ICML,387-394.Liu, B, Dai, Y., Li, X., Lee, W-S., and Yu.
P.2003.
Building text classifiers using positiveand unlabeled examples.
ICDM, 179-188.Neter, J., Wasserman, W., and Whitmore, G. A.1993.
Applied Statistics.
Allyn and Bacon.Nigam, K., McCallum, A., Thrun, S. and Mit-chell, T. 2000.
Text classification from la-beled and unlabeled documents using EM.Machine Learning, 39(2/3), 103?134.Pantel, P., Eric Crestan, Arkady Borkovsky,Ana-Maria Popescu, Vishnu, Vyas.
2009.Web-Scale Distributional similarity and entityset expansion, EMNLP.Pa?ca, M. Lin, D. Bigham, J. Lifchits, A. Jain, A.2006.
Names and similarities on the web: fastextraction in the fast lane.
ACL.Sarmento, L., Jijkuon, V. de Rijke, M. andOliveira, E. 2007.
?More like these?
: growingentity classes from seeds.
CIKM.Wang, R. C. and Cohen, W. W. 2008.
Iterativeset expansion of named entities using the web.ICDM.Yu, H., Han, J., K. Chang.
2002.
PEBL: Positiveexample based learning for Web page classi-fication using SVM.
KDD, 239-248.364
