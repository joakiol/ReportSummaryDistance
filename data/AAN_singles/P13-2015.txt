Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 81?86,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsDomain-Specific Coreference Resolution with Lexicalized FeaturesNathan Gilbert and Ellen RiloffSchool of ComputingUniversity of Utah50 S. Central Campus Dr.Salt Lake City, UT 84112USA{ngilbert,riloff}@cs.utah.eduAbstractMost coreference resolvers rely heavily onstring matching, syntactic properties, andsemantic attributes of words, but they lackthe ability to make decisions based on in-dividual words.
In this paper, we ex-plore the benefits of lexicalized featuresin the setting of domain-specific corefer-ence resolution.
We show that addinglexicalized features to off-the-shelf coref-erence resolvers yields significant perfor-mance gains on four domain-specific datasets and with two types of coreference res-olution architectures.1 IntroductionCoreference resolvers are typically evaluated oncollections of news articles that cover a wide rangeof topics, such as the ACE (ACE03, 2003; ACE04,2004; ACE05, 2005) and OntoNotes (Pradhanet al, 2007) data sets.
Many NLP applica-tions, however, involve text analysis for special-ized domains, such as clinical medicine (Goochand Roudsari, 2012; Glinos, 2011), legal text anal-ysis (Bouayad-Agha et al, 2009), and biologicalliterature (Batista-Navarro and Ananiadou, 2011;Castan?o et al, 2002).
Learning-based corefer-ence resolvers can be easily retrained for a spe-cialized domain given annotated training texts forthat domain.
However, we found that retrainingan off-the-shelf coreference resolver with domain-specific texts showed little benefit.This surprising result led us to question the na-ture of the feature sets used by noun phrase (NP)coreference resolvers.
Nearly all of the featuresemployed by recent systems fall into three cate-gories: string match and word overlap, syntacticproperties (e.g., appositives, predicate nominals,parse features, etc.
), and semantic matching (e.g.,gender agreement, WordNet similarity, named en-tity classes, etc.).
Conspicuously absent from mostsystems are lexical features that allow the classi-fier to consider the specific words when making acoreference decision.
A few researchers have ex-perimented with lexical features, but they achievedmixed results in evaluations on broad-coveragecorpora (Bengston and Roth, 2008; Bjo?rkelundand Nugues, 2011; Rahman and Ng, 2011a).We hypothesized that lexicalized features canhave a more substantial impact in domain-specificsettings.
Lexical features can capture domain-specific knowledge and subtle semantic distinc-tions that may be important within a domain.For example, based on the resolutions found indomain-specific training sets, our lexicalized fea-tures captured the knowledge that ?tomcat?
canbe coreferent with ?plane?, ?UAW?
can be coref-erent with ?union?, and ?anthrax?
can be coref-erent with ?diagnosis?.
Capturing these types ofdomain-specific information is often impossibleusing only general-purpose resources.
For exam-ple, WordNet defines ?tomcat?
only as an animal,does not contain an entry for ?UAW?, and catego-rizes ?anthrax?
and ?diagnosis?
very differently.1In this paper, we evaluate the impact of lexi-calized features on 4 domains: management suc-cession (MUC-6 data), vehicle launches (MUC-7data), disease outbreaks (ProMed texts), and ter-rorism (MUC-4 data).
We incorporate lexical-ized feature sets into two different coreference ar-chitectures: Reconcile (Stoyanov et al, 2010), apairwise coreference classifier, and Sieve (Raghu-nathan et al, 2010), a rule-based system.
Our re-sults show that lexicalized features significantlyimprove performance in all four domains and inboth types of coreference architectures.2 Related WorkWe are not the first researchers to use lexicalizedfeatures for coreference resolution.
However, pre-1WordNet defines ?anthrax?
as a disease (condition/state)and ?diagnosis?
as an identification (discovery event).81PPPPPPTrainTest MUC-6 MUC-7 Promed MUC-4P R F P R F P R F P R FMUC-6 80.79 62.71 70.61 84.33 61.74 71.29 83.54 70.34 76.37 80.22 60.81 69.18MUC-7 74.78 65.59 69.88 82.73 64.09 72.23 85.29 71.82 77.98 77.35 64.19 70.16Promed 73.60 64.20 68.60 82.88 63.37 71.82 80.31 72.66 76.29 74.52 65.65 69.80MUC-4 69.27 65.66 67.42 71.49 67.22 69.29 76.92 74.25 75.56 71.76 67.37 69.50Table 1: Cross-domain B3 (Bagga and Baldwin, 1998) results for Reconcile with its general feature set.The Paired Permutation test (Pesarin, 2001) was used for statistical significance testing and gray cellsrepresent results that are not significantly different from the best result.vious work has evaluated the benefit of lexical fea-tures only for broad-coverage data sets.Bengston and Roth (2008) incorporated a mem-orization feature to learn which entities can re-fer to one another.
They created a binary fea-ture for every pair of head nouns, including pro-nouns.
They reported no significant improvementfrom these features on the ACE 2004 data.Rahman and Ng (2011a) also utilized lexicalfeatures, going beyond strict memorization withmethods to combat data sparseness and incorpo-rating semantic information.
They created a fea-ture for every ordered pair of head nouns (forpronouns and nominals) or full NPs (for propernouns).
Semi-lexical features were also used whenone NP was a Named Entity, and unseen featureswere used when the NPs were not in the trainingset.
Their features did yield improvements on boththe ACE 2005 and OntoNotes-2 data, but the semi-lexical features included Named Entity classes aswell as word-based features.Rahman and Ng (2011b) explored the use oflexical features in greater detail and showed theirbenefit on the ACE05 corpus independent of, andcombined with, a conventional set of coreferencefeatures.
The ACE05 corpus is drawn from sixsources (Newswire, Broadcast News, BroadcastConversations, Conversational Telephone Speech,Webblogs, and Usenet).
The authors experi-mented with utilizing lexical information drawnfrom different sources.
The results showed thatthe best performance came from training and test-ing with lexical knowledge drawn from the samesource.
Although our approach is similar, this pa-per focuses on learning lexical information fromdifferent domains as opposed to the different gen-res found in the six sources of the ACE05 corpus.Bjo?rkelund and Nugues (2011) used lexicalword pairs for the 2011 CoNLL Shared Task,showing significant positive impact on perfor-mance.
They used over 2000 annotated docu-ments from the broad-coverage OntoNotes corpusfor training.
Our work aims to show the benefit oflexical features using much smaller training sets(< 50 documents) focused on specific domains.Lexical features have also been used for slightlydifferent purposes.
Florian et al (2004) utilizedlexical information such as mention spelling andcontext for entity tracking in ACE.
Ng (2007) usedlexical information to assess the likelihood of anoun phrase being anaphoric, but this did not showclear improvements on ACE data.There has been previous work on domain-specific coreference resolution for several do-mains, including biological literature (Castan?o etal., 2002; Liang and Lin, 2005; Gasperin andBriscoe, 2008; Kim et al, 2011; Batista-Navarroand Ananiadou, 2011), clinical medicine (He,2007; Zheng et al, 2011; Glinos, 2011; Gooch andRoudsari, 2012) and legal documents (Bouayad-Agha et al, 2009).
In addition, BABAR (Bean andRiloff, 2004) used contextual role knowledge forcoreference resolution in the domains of terrorismand natural disasters.
But BABAR acquired andused lexical information to match the compatibil-ity of contexts surrounding NPs, not the NPs them-selves.
To the best of our knowledge, our work isthe first to examine the impact of lexicalized fea-tures for domain-specific coreference resolution.3 Exploiting Lexicalized FeaturesTable 1 shows the performance of a learning-basedcoreference resolver, Reconcile (Stoyanov et al,2010), with its default feature set using differentcombinations of training and testing data.
Recon-cile does not include any lexical features, but doescontain over 60 general features covering seman-tic agreement, syntactic constraints, string matchand recency.Each row represents a training set, each columnrepresents a test set, and each cell shows precision(P), recall (R), and F score results under the B3metric when using the corresponding training andtest data.
The best results for each test set appear82MUC-6 MUC-7 ProMED MUC-4P R F P R F P R F P R FReconcile 80.79 62.71 70.61 82.73 64.09 72.23 80.31 72.66 76.29 71.76 67.37 69.50+LexLookup 87.01 63.40 73.35 87.39 62.86 73.12 86.66 70.95 78.02 82.89 67.53 74.42+LexSets 86.50 63.76 73.41 85.86 64.35 73.56 86.19 72.14 78.54 81.98 67.73 74.18Sieve 92.20 61.70 73.90 91.46 59.59 72.16 94.43 67.25 78.55 91.30 59.84 72.30+LexBegin 91.22 62.97 74.51 91.24 60.28 72.59 93.51 69.15 79.51 89.01 62.84 73.67+LexEnd 90.59 63.47 74.64 91.17 60.56 72.78 93.99 68.87 79.49 89.04 64.03 74.47Table 2: B3 results for baselines and lexicalized feature sets across four domains.in boldface.We performed statistical significance testing us-ing the Paired Permutation test (Pesarin, 2001) andthe gray cells represent results where there wasnot significant difference from the best results inthe same column.
If just one cell is gray in a col-umn, that indicates the result was significantly bet-ter than the other results in the same column withp ?
0.05.Table 1 does not show much benefit from train-ing on the same domain as the test set.
Threedifferent training sets produce F scores that arenot significantly different for both the MUC-6and MUC-4 test data.
For ProMed, training onthe MUC-7 data yields significantly better resultsthan training on all the other data sets, includ-ing ProMed texts!
Based on these results, itwould seem that training on the MUC-7 texts islikely to yield the best results no matter what do-main you plan to use the coreference resolver for.The goal of our work is to investigate whetherlexical features can extract additional knowledgefrom domain-specific training texts to help tailora coreference resolver to perform better for a spe-cific domain.3.1 Extracting Coreferent Training PairsWe adopt the terminology introduced by Stoyanovet al (2009) to define a coreference element (CE)as a noun phrase that can participate in a corefer-ence relation based on the task definition.Each training document has manually annotatedgold coreference chains corresponding to the setsof CEs that are coreferent.
For each CE in a goldchain, we pair that CE with all of the other CEs inthe same chain.
We consider the coreference rela-tion to be bi-directional, so we don?t retain infor-mation about which CE was the antecedent.
Wedo not extract CE pairs that share the same headnoun because they are better handled with stringmatch.
For nominal NPs, we retain only the headnoun, but we use the entire NP for proper names.We discard pairs that include a pronoun, and nor-malize strings to lower case for consistency.3.2 Lexicalized Feature SetsWe explore two ways to capture lexicalized infor-mation as features.
The first approach indicateswhether two CEs have ever been coreferent in thetraining data.
We create a single feature calledLEXLOOKUP(X,Y) that receives a value of 1 whenx and y have been coreferent at least twice, ora value of 0 otherwise.2 LEXLOOKUP(X,Y) is asingle feature that captures all CE pairs that werecoreferent in the training data.We also created set-based features that capturethe set of terms that have been coreferent with aparticular CE.
The CorefSet(x) is the set of CEsthat have appeared in the same coreference chainas mention x at least twice.We create a set of binary-valued featuresLEXSET(X,Y), one for each CE x in the trainingdata.
Given a pair of CEs, x and y, LEXSET(X,Y)= 1 if y ?
CorefSet(x), or 0 otherwise.
The ben-efit of the set-based features over a single mono-lithic feature is that the classifier has one set-basedfeature for each mention found in the training data,so it can learn to handle individual terms differ-ently.We also tried encoding a separate feature foreach distinct pair of words, analogous to the mem-orization feature in Bengston and Roth (2008).This did not improve performance as much as theother feature representations presented here.4 Evaluation4.1 Data SetsWe evaluated the performance of lexicalized fea-tures on 4 domain-specific corpora including twostandard coreference benchmarks, the MUC-6 andMUC-7 data sets.
The MUC-6 domain is manage-ment succession and consists of 30 training textsand 30 test texts.
The MUC-7 domain is vehicle2We require a frequency ?
2 to minimize overfitting be-cause many cases occur only once in the training data.83launches and consists of 30 training texts and 20test texts.
We used these standard train/test splitsto be consistent with previous work.We also created 2 new coreference data setswhich we will make freely available.
Wemanually annotated 45 ProMed-mail articles(www.promedmail.org) about disease outbreaksand 45 MUC-4 texts about terrorism, followingthe MUC guidelines (Hirschman, 1997).
Inter-annotator agreement between two annotators was.77 (?)
on ProMed and .84 (MUC F Score)(Villainet al, 1995) on both ProMed and MUC-4.3 Weperformed 5-fold cross-validation on both datasets and report the micro-averaged results.Gold CE spans were used in all experiments tofactor out issues with markable identification andanaphoricity across the different domains.4.2 Coreference Resolution ModelsWe conducted experiments using two coreferenceresolution architectures.
Reconcile4 (Stoyanov etal., 2010) is a freely available pairwise mentionclassifier.
For classification, we chose Weka?s(Witten and Frank, 2005) Decision Tree learnerinside Reconcile.
Reconcile contains roughly 60features (none lexical), largely modeled after Ngand Cardie (2002).
We modified Reconcile?s Sin-gle Link clustering scheme to enforce an addi-tional rule that non-overlapping proper names can-not be merged into the same chain.We also conducted experiments with the Sievecoreference resolver, which applies high precisionheuristic rules to incrementally build coreferencechains.
We implemented the LEXLOOKUP(X,Y)feature as an additional heuristic rule.
We triedinserting this heuristic before Sieve?s other rules(LexBegin), and also after Sieve?s other rules(LexEnd).4.3 Experimental ResultsTable 2 presents results for Reconcile trained withand without lexical features and when addinga lexical heuristic with data drawn from same-domain texts to Sieve.The first row shows the results without the lex-icalized features (from Table 1).
All F scoresfor Reconcile with lexicalized features are signifi-cantly better than without these features based onthe Paired Permutation test (Pesarin, 2001) with3We also computed ?
on MUC-4, but unfortunately thescore and original data were lost.4http://www.cs.utah.edu/nlp/reconcile/p ?
0.05.
MUC-4 showed the largest gain forReconcile, with the F score increasing from 69.5to over 74.
For most domains, adding the lexicalfeatures to Reconcile substantially increased pre-cision with comparable levels of recall.The bottom half of Table 2 contains the resultsof adding a lexical heuristic to Sieve.
The firstrow shows the default system with no lexical in-formation.
All F scores with the lexical heuristicare significantly better than without it.
In Sieve?shigh-precision coreference architecture, the lexi-cal heuristic yields additional recall gains withoutsacrificing much precision.ACE 2004P R FReconcile 70.59 83.09 76.33+LexLookup 71.32 82.93 76.69+LexSets 71.44 83.45 76.98Sieve 90.09 74.23 81.39+LexBegin 86.54 75.43 80.61+LexEnd 87.00 75.45 80.82Table 3: B3 results for baselines and lexicalizedfeature sets on the broad-coverage ACE 2004 dataset.Table 3 shows the results for Reconcile andSieve when training and testing on the ACE 2004data.
Here, we see little improvement from addinglexical information.
For Reconcile, the small dif-ferences in F scores are not statistically significant.For Sieve, the unlexicalized system yields a signif-icantly higher F score than when adding the lexi-cal heuristic.
These results support our hypothesisthat lexicalized information can be beneficial forcapturing domain-specific word associations, butmay not be as helpful in a broad-coverage settingwhere the language covers a diverse set of topics.Table 4 shows a re-evaluation of the cross-domain experiments from Table 1 for Reconcilewith the LexSet features added.
The bottom halfof the table shows cross-domain experiments forSieve using the lexical heuristic at the end of itsrule set (LexEnd).
Results are presented usingboth the B3 metric and the MUC Score (Villainet al, 1995).Training and testing on the same domain al-ways produced the highest recall scores for MUC-7, ProMed, and MUC-4 when utilizing lexicalfeatures.
In all cases, lexical features acquiredfrom same-domain texts yield results that are ei-ther clearly the best or not significantly differentfrom the best.84PPPPPPTrainTest MUC-6 MUC-7 Promed MUC-4P R F P R F P R F P R FReconcile (B3 Score)MUC-6 86.50 63.76 73.41 90.44 60.75 72.68 89.28 68.14 77.29 84.05 60.61 70.44MUC-7 80.65 63.42 71.01 85.86 64.46 73.56 89.41 70.05 78.55 80.61 63.26 70.89Promed 81.69 62.73 70.96 88.32 62.79 73.40 86.19 72.14 78.54 84.81 62.58 72.02MUC-4 81.20 62.34 70.53 87.23 63.13 73.25 87.52 71.11 78.46 81.98 67.73 74.18Reconcile (MUC Score)MUC-6 89.56 71.17 79.32 90.85 67.43 77.41 89.61 65.67 75.79 88.27 66.98 76.16MUC-7 86.14 72.22 78.57 89.56 72.01 79.83 89.34 68.08 77.27 87.30 70.22 77.83Promed 86.92 70.68 77.97 90.93 70.33 79.31 88.54 69.55 77.90 88.83 68.89 78.23MUC-4 85.72 70.50 77.37 88.78 71.24 79.05 88.24 68.18 77.55 87.89 74.18 80.45Sieve (B3 Score)MUC-6 90.59 63.47 74.64 91.20 59.91 72.32 94.30 67.25 78.51 91.30 59.90 72.34MUC-7 91.62 63.67 75.13 91.17 60.56 72.78 94.43 67.35 78.62 91.14 60.44 72.68Promed 92.14 61.70 73.90 91.46 59.93 72.41 93.99 68.87 79.49 91.27 60.76 72.96MUC-4 91.76 61.88 73.91 91.26 59.93 72.34 94.30 67.35 78.58 89.04 64.03 74.47Sieve (MUC Score)MUC-6 91.80 70.87 79.99 91.38 65.52 76.32 92.08 64.71 76.01 90.38 66.98 77.10MUC-7 91.82 69.70 79.25 91.68 66.36 76.99 92.20 64.86 76.15 90.71 67.09 77.13Promed 91.99 69.15 78.95 91.68 65.52 76.42 91.70 66.33 76.98 90.85 67.09 77.18MUC-4 91.79 69.39 79.03 91.48 65.52 76.36 92.00 64.86 76.08 90.31 69.62 78.62Table 4: Cross-domain B3 and MUC results for Reconcile and Sieve with lexical features.
Gray cellsrepresent results that are not significantly different from the best results in the column at the 0.05 p-level.For MUC-6 and MUC-7, the highest F score re-sults almost always come from training on same-domain texts, although in some cases these re-sults are not significantly different from trainingon other domains.
Lexical features can yield im-provements when training on a different domain ifthere is overlap in the vocabulary across the do-mains.
For the ProMed domain, the Sieve systemperforms significantly better, under both metrics,with same-domain lexical features than with lexi-cal features acquired from a different domain.
ForReconcile, there is not a significant difference inthe F score for ProMed when training on ProMed,MUC-4, or MUC-7.
In the MUC-4 domain, usingsame-domain lexical information always producesthe best F score, under both metrics and in bothcoreference systems.5 ConclusionsWe explored the use of lexical information fordomain-specific coreference resolution using 4domain-specific data sets and 2 coreference re-solvers.
Lexicalized features consistently im-proved performance for all of the domains and inboth coreference architectures.
We see benefitsfrom lexicalized features in cross-domain training,but the gains are often more substantial when uti-lizing same-domain lexical knowledge.In the future, we plan to explore additional typesof lexical information to benefit domain-specificcoreference resolution.AcknowledgmentsThis material is based upon work supported bythe National Science Foundation under GrantNo.
IIS-1018314 and the Defense Advanced Re-search Projects Agency (DARPA) Machine Read-ing Program under Air Force Research Laboratory(AFRL) prime contract no.
FA8750-09-C-0172.Any opinions, findings, and conclusion or recom-mendations expressed in this material are those ofthe authors and do not necessarily reflect the viewof the DARPA, AFRL, or the U.S. government.ReferencesACE03.
2003.
NIST ACE evaluation website.
Inhttp://www.nist.gov/speech/tests/ace/2003.ACE04.
2004.
NIST ACE evaluation website.
Inhttp://www.nist.gov/speech/tests/ace/2004.ACE05.
2005.
NIST ACE evaluation website.
Inhttp://www.nist.gov/speech/tests/ace/2005.Amit Bagga and Breck Baldwin.
1998.
Entity-basedcross-document coreference using the Vector SpaceModel.
Proceedings of the 17th international con-ference on Computational Linguistics (COLING).Riza Theresa Batista-Navarro and Sophia Ananiadou.2011.
Building a coreference-annotated corpusfrom the domain of biochemistry.
In Proceedings ofBioNLP 2011 Workshop, BioNLP ?11, pages 83?91.David Bean and Ellen Riloff.
2004.
Unsupervisedlearning of Contextual Role Knowledge for coref-erence resolution.
Proceedings of the HLT/NAACL2004.85Eric Bengston and Dan Roth.
2008.
Understanding thevalue of features for coreference resolution.
Empir-ical Methods in Natural Language Processing.Anders Bjo?rkelund and Pierre Nugues.
2011.
Explor-ing lexicalized features for coreference resolution.Proceedings of the Fifteenth Conference on Compu-tational Natural Language Learning: Shared Task,pages 45?50.Nadjet Bouayad-Agha, Gerard Casamayor, GabrielaFerraro, Simon Mille, Vanesa Vidal, and Leo Wan-ner.
2009.
Improving the comprehension of legaldocumentation: the case of patent claims.
In Pro-ceedings of the 12th International Conference on Ar-tificial Intelligence and Law, pages 78?87.Jose?
Castan?o, Jason Zhang, and James Pustejovsky.2002.
Anaphora resolution in biomedical literature.International Symposium on Reference Resolution.Radu Florian, Hany Hassan, Abraham Ittycheriah,Hongyan Jing, Nanda Kambhatla, Xiaoqiang Luo,Nicolas Nicolov, Salim Roukos, and T Zhang.
2004.A statistical model for multilingual entity detectionand tracking.
HLT-NAACL.Caroline Gasperin and Ted Briscoe.
2008.
Statisticalanaphora resolution in biomedical texts.
Proceed-ings of the 22nd Annual Conference on Computa-tional Linguistics, pages 257?264.Demetrios G. Glinos.
2011.
A search based method forclinical text coreference resolution.
In Proceedingsof the Fifth i2b2/VA Track on Challenges in NaturalLanguage Processing for Clinical Data (i2b2 2011).Phil Gooch and Abdul Roudsari.
2012.
Lexical pat-terns, features and knowledge resources for corefer-ence resolution in clinical notes.
Journal of Biomed-ical Informatics, 45.Tian Ye He.
2007.
Coreference resolution on entitiesand events for hospital discharge summaries.
Ph.D.thesis, Massachusetts Institute of Technology.Lynette Hirschman.
1997.
MUC-7 task definition.Proceedings of MUC-7.Youngjun Kim, Ellen Riloff, and Nathan Gilbert.
2011.The taming of Reconcile as a Biomedical corefer-ence resolver.
ACL/HLT 2011 Workshop on Biomed-ical Natural Language Processing (BioNLP 2011)Shared Task Paper.Tyne Liang and Yu-Hsiang Lin.
2005.
Anaphoraresolution for biomedical literature by exploitingmultiple resources.
Natural Language Processing?IJCNLP 2005, pages 742?753.Vincent Ng and Claire Cardie.
2002.
Improving ma-chine learning approaches to coreference resolution.Proceedings of the 40th Annual Meeting of the ACL,pages 104?111.Vincent Ng.
2007.
Shallow semantics for corefer-ence resolution.
Proceedings of the Twentieth Inter-national Joint Conference on Artificial Intelligence(IJCAI-07), pages 1689?1694.Fortunato Pesarin.
2001.
Multivariate permutationtests: with applications in biostatistics, volume 240.Wiley Chichester.Sameer S. Pradhan, Lance Ramshaw, RalphWeischedel, Jessice MacBride, and Linnea Micci-ulla.
2007.
Unrestricted coreference: Identifyingentities and events in ontonotes.
In Proceedingsof the International Conference on SemanticComputing.Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-garajan, Nathanael Chambers, Mihai Surdeanu, DanJurafsky, and Christopher Manning.
2010.
A Multi-Pass Sieve for coreference resolution.
EmpiricalMethods in Natural Langugage Processing 2010.Altaf Rahman and Vincent Ng.
2011a.
Coreferenceresolution with world knowledge.
Proceedings ofthe 49th Annual Meeting of the Association for Com-putational Linguistics and Human Language Tech-nologies (ACL-HLT), pages 814?824.Altaf Rahman and Vincent Ng.
2011b.
Narrowing themodelling gap: A cluster-ranking approach to coref-erence resolution.
Journal of Artificial IntelligenceResearch.Veselin Stoyanov, Nathan Gilbert, Claire Cardie, andEllen Riloff.
2009.
Conundrums in noun phrasecoreference resolution: Making sense of the State-of-the-Art.
Proceedings of the Joint Conference ofthe 47th Annual Meeting of the ACL and the 4th IJC-NLP (ACL-IJCNLP 2009).Veselin Stoyanov, Nathan Gilbert, Claire Cardie, andEllen Riloff.
2010.
Coreference resolution withReconcile.
Proceedings of the Joint Conference ofthe 48th Annual Meeting of the Association for Com-putational Linguistics (ACL 2010).Marc Villain, John Aberdeen, John Berger, DennisConnolly, and Lynette Hirschman.
1995.
A model-theoretic coreference scoring scheme.
Proceedingsof the 6th conference on Message understanding.Ian H. Witten and Eibe Frank.
2005.
Data Mining:Practical machine learning tools and techniques.Morgan Kaufmann, 2nd edition.Jiaping Zheng, Wendy Chapman, Rebecca Crowley,and Guergana Savova.
2011.
Coreference resolu-tion: A review of general methodologies and appli-cations in the clinical domain.
Journal of Biomedi-cal Informatics, 44:1113?1122.86
