Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 172?180,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsUnsupervised Modeling of Twitter ConversationsAlan Ritter?Computer Sci.
& Eng.University of WashingtonSeattle, WA 98195aritter@cs.washington.eduColin Cherry?National Research Council CanadaOttawa, Ontario, K1A 0R6Colin.Cherry@nrc-cnrc.gc.caBill DolanMicrosoft ResearchRedmond, WA 98052billdol@microsoft.comAbstractWe propose the first unsupervised approachto the problem of modeling dialogue acts inan open domain.
Trained on a corpus ofnoisy Twitter conversations, our method dis-covers dialogue acts by clustering raw utter-ances.
Because it accounts for the sequentialbehaviour of these acts, the learned model canprovide insight into the shape of communica-tion in a new medium.
We address the chal-lenge of evaluating the emergent model with aqualitative visualization and an intrinsic con-versation ordering task.
This work is inspiredby a corpus of 1.3 million Twitter conversa-tions, which will be made publicly available.This huge amount of data, available only be-cause Twitter blurs the line between chattingand publishing, highlights the need to be ableto adapt quickly to a new medium.1 IntroductionAutomatic detection of dialogue structure is an im-portant first step toward deep understanding of hu-man conversations.
Dialogue acts1 provide aninitial level of structure by annotating utteranceswith shallow discourse roles such as ?statement?,?question?
and ?answer?.
These acts are useful inmany applications, including conversational agents(Wilks, 2006), dialogue systems (Allen et al, 2007),dialogue summarization (Murray et al, 2006), andflirtation detection (Ranganath et al, 2009).Dialogue act tagging has traditionally followed anannotate-train-test paradigm, which begins with the?This work was conducted at Microsoft Research.1Also called ?speech acts?design of annotation guidelines, followed by the col-lection and labeling of corpora (Jurafsky et al, 1997;Dhillon et al, 2004).
Only then can one train a tag-ger to automatically recognize dialogue acts (Stol-cke et al, 2000).
This paradigm has been quite suc-cessful, but the labeling process is both slow andexpensive, limiting the amount of data available fortraining.
The expense is compounded as we con-sider new methods of communication, which mayrequire not only new annotations, but new annota-tion guidelines and new dialogue acts.
This issue be-comes more pressing as the Internet continues to ex-pand the number of ways in which we communicate,bringing us e-mail, newsgroups, IRC, forums, blogs,Facebook, Twitter, and whatever is on the horizon.Previous work has taken a variety of approachesto dialogue act tagging in new media.
Cohen et al(2004) develop an inventory of dialogue acts specificto e-mail in an office domain.
They design their in-ventory by inspecting a large corpus of e-mail, andrefine it during the manual tagging process.
Jeong etal.
(2009) use semi-supervised learning to transferdialogue acts from labeled speech corpora to the In-ternet media of forums and e-mail.
They manuallyrestructure the source act inventories in an attemptto create coarse, domain-independent acts.
Each ap-proach relies on a human designer to inject knowl-edge into the system through the inventory of avail-able acts.As an alternative solution for new media, we pro-pose a series of unsupervised conversation models,where the discovery of acts amounts to clusteringutterances with similar conversational roles.
Thisavoids manual construction of an act inventory, andallows the learning algorithm to tell us somethingabout how people converse in a new medium.172There is surprisingly little work in unsuperviseddialogue act tagging.
Woszczyna and Waibel (1994)propose an unsupervised Hidden Markov Model(HMM) for dialogue structure in a meeting schedul-ing domain, but model dialogue state at the wordlevel.
Crook et al (2009) use Dirichlet process mix-ture models to cluster utterances into a flexible num-ber of acts in a travel-planning domain, but do notexamine the sequential structure of dialogue.2In contrast to previous work, we address the prob-lem of discovering dialogue acts in an informal,open-topic domain, where an unsupervised learnermay be distracted by strong topic clusters.
We alsotrain and test our models in a new medium: Twit-ter.
Rather than test against existing dialogue inven-tories, we evaluate using qualitative visualizationsand a novel conversation ordering task, to ensure ourmodels have the opportunity to discover dialoguephenomena unique to this medium.2 DataTo enable the study of large-data solutions to di-alogue modeling, we have collected a corpus of1.3 million conversations drawn from the micro-blogging service, Twitter.
3 To our knowledge,this is the largest corpus of naturally occurring chatdata that has been available for study thus far.
Sim-ilar datasets include the NUS SMS corpus (Howand Kan, 2005), several IRC chat corpora (Elsnerand Charniak, 2008; Forsyth and Martell, 2007),and blog datasets (Yano et al, 2009; Gamon et al,2008), which can display conversational structure inthe blog comments.As it characterizes itself as a micro-blog, it shouldnot be surprising that structurally, Twitter conversa-tions lie somewhere between chat and blogs.
Likeblogs, conversations on Twitter occur in a public en-vironment, where they can be collected for researchpurposes.
However, Twitter posts are restricted to beno longer than 140 characters, which keeps interac-tions chat-like.
Like e-mail and unlike IRC, Twit-ter conversations are carried out by replying to spe-cific posts.
The Twitter API provides a link fromeach reply to the post it is responding to, allowing2The Crook et al model should be able to be combined withthe models we present here.3Will be available at http://www.cs.washington.edu/homes/aritter/twitter_chat/1 2 3 4 502468101214log lengthlogfrequencyFigure 1: Conversation length versus frequencyaccurate thread reconstruction without requiring aconversation disentanglement step (Elsner and Char-niak, 2008).
The proportion of posts on Twitter thatare conversational in nature are somewhere around37% (Kelly, 2009).To collect this corpus, we crawled Twitter usingits publicly available API.
We monitored the publictimeline4 to obtain a sample of active Twitter users.To expand our user list, we also crawled up to 10users who had engaged in dialogue with each seeduser.
For each user, we retrieved all posts, retain-ing only those that were in reply to some other post.We recursively followed the chain of replies to re-cover the entire conversation.
A simple function-word-driven filter was used to remove non-Englishconversations.We crawled Twitter for a 2 month period duringthe summer of 2009.
The resulting corpus consistsof about 1.3 million conversations, with each con-versation containing between 2 and 243 posts.
Themajority of conversations on Twitter are very short;those of length 2 (one status post and a reply) ac-count for 69% of the data.
As shown in Figure 1, thefrequencies of conversation lengths follow a power-law relationship.While the style of writing used on Twitter iswidely varied, much of the text is very similar toSMS text messages.
This is likely because manyusers access Twitter through mobile devices.
Postsare often highly ungrammatical, and filled withspelling errors.
In order to illustrate the spellingvariation found on Twitter, we ran the Jcluster wordclustering algorithm (Goodman, 2001) on our cor-4http://twitter.com/public_timeline pro-vides the 20 most recent posts on Twitter173coming commingenough enought enuff enufbe4 b4 befor beforeyuhr yur your yor ur youur yhurmsgs messagescouldnt culdnt cldnt cannae cudnt couldentabout bou abt abour abut bowtTable 1: A sample of Twitter spelling variation.pus, and manually picked out clusters of spellingvariants; a sample is displayed in Table 1.Twitter?s noisy style makes processing Twittertext more difficult than other domains.
While mov-ing to a new domain (e.g.
biomedical text) is a chal-lenging task, at least the new words found in thevocabulary are limited mostly to verbs and nouns,while function words remain constant.
On Twit-ter, even closed-class words such as prepositions andpronouns are spelled in many different ways.3 Dialogue AnalysisWe propose two models to discover dialogue acts inan unsupervised manner.
An ideal model will giveinsight into the sorts of conversations that happenon Twitter, while providing a useful tool for laterprocessing.
We first introduce the summarizationtechnology we apply to this task, followed by twoBayesian extensions.3.1 Conversation modelOur base model structure is inspired by the con-tent model proposed by Barzilay and Lee (2004)for multi-document summarization.
Their sentence-level HMM discovers the sequence of topics usedto describe a particular type of news event, such asearthquakes.
A news story is modeled by first gen-erating a sequence of hidden topics according to aMarkov model, with each topic generating an ob-served sentence according to a topic-specific lan-guage model.
These models capture the sequentialstructure of news stories, and can be used for sum-marization tasks such as sentence extraction and or-dering.Our goals are not so different: we wish to dis-cover the sequential dialogue structure of conversa-tion.
Rather than learning a disaster?s location isfollowed by its death toll, we instead wish to learnthat a question is followed by an answer.
An initiala0a1a2w0,jw1,jw2,jW0W1W2CkFigure 2: Conversation Modela0a1a2w0,jw1,jw2,jW0W1W2Cks0,js1,js2,j?k?EpikFigure 3: Conversation + Topic Modelconversation model can be created by simply apply-ing the content modeling framework to conversationdata.
We rename the hidden states acts, and assumeeach post in a Twitter conversation is generated bya single act.5 During development, we found that aunigram language model performed best as the actemission distribution.The resulting conversation model is shown as aplate diagram in Figure 2.
Each conversation C isa sequence of acts a, and each act produces a post,represented by a bag of words shown using the Wplates.
The number of acts available to the modelis fixed; we experimented with between 5 and 40.Starting with a random assignment of acts, we trainour conversation model using EM, with forward-backward providing act distributions during the ex-pectation step.
The model structure in Figure 2 is5The short length of Twitter posts makes this assumptionreasonable.174sadly no.
some pasta bake, but coffee and pasta bake is not acontender for tea and toast... .yum!
Ground beef tacos?
We ?re grilling out.
Turkey dogs forme, a Bubba Burger for my dh, and combo for the kids.ha!
They gotcha!
You had to think about Arby?s to write that tweet.Arby?s is conducting a psychlogical study.
Of roast beef.Rumbly tummy soon to be tamed by Dominos for lunch!
Nomnom nom!Table 2: Example of a topical cluster discovered bythe EM Conversation Model.similar to previous HMMs for supervised dialogueact recognition (Stolcke et al, 2000), but our modelis trained unsupervised.3.2 Conversation + Topic modelOur conversations are not restricted to any partic-ular topic: Twitter users can and will talk aboutanything.
Therefore, there is no guarantee that ourmodel, charged with discovering clusters of poststhat aid in the prediction of the next cluster, will nec-essarily discover dialogue acts.
The sequence modelcould instead partition entire conversations into top-ics, such as food, computers and music, and then pre-dict that each topic self-transitions with high proba-bility: if we begin talking about food, we are likelyto continue to do so.
Since we began with a contentmodel, it is perhaps not surprising that our Conversa-tion Model tends to discover a mixture of dialogueand topic structure.
Several high probability postsfrom a topic-focused cluster discovered by EM areshown in Table 2.
These clusters are undesirable, asthey have little to do with dialogue structure.In general, unsupervised sentence clustering tech-niques need some degree of direction when a par-ticular level of granularity is desired.
Barzilay andLee (2004) mask named entities in their contentmodels, forcing their model to cluster topics aboutearthquakes in general, and not instances of specificearthquakes.
This solution is not a good fit for Twit-ter.
As explained in Section 2, Twitter?s noisinessresists off-the-shelf tools, such as named-entity rec-ognizers and noun-phrase chunkers.
Furthermore,we would require a more drastic form of prepro-cessing in order to mask all topic words, and notjust alter the topic granularity.
During development,we explored coarse methods to abstract away con-tent while maintaining syntax, such as replacing to-kens with either parts-of-speech or automatically-generated word clusters, but we found that these ap-proaches degrade model performance.Another approach to filtering out topic informa-tion leaves the data intact, but modifies the modelto account for topic.
To that end, we adopt a LatentDirichlet Allocation, or LDA, framework (Blei et al,2003) similar to approaches used recently in sum-marization (Daume?
III and Marcu, 2006; Haghighiand Vanderwende, 2009).
The goal of this extendedmodel is to separate content words from dialogue in-dicators.
Each word in a conversation is generatedfrom one of three sources:?
The current post?s dialogue act?
The conversation?s topic?
General EnglishThe extended model is shown in Figure 3.6 In addi-tion to act emission and transition parameters, themodel now includes a conversation-specific wordmultinomial ?k that represents the topic, as well as auniversal general English multinomial ?E .
A newhidden variable, s determines the source of eachword, and is drawn from a conversation-specific dis-tribution over sources pik.
Following LDA conven-tions, we place a symmetric Dirichlet prior overeach of the multinomials.
Dirichlet concentrationparameters for act emission, act transition, conver-sation topic, general English, and source become thehyper-parameters of our model.The multinomials ?k, pik and ?E create non-localdependencies in our model, breaking our HMM dy-namic programing.
Therefore we adopt Gibbs sam-pling as our inference engine.
Each hidden vari-able is sampled in turn, conditioned on a completeassignment of all other hidden variables through-out the data set.
Again following LDA convention,we carry out collapsed sampling, where the variousmultinomials are integrated out, and are never ex-plicitly estimated.
This results in a sampling se-quence where for each post we first sample its act,and then sample a source for each word in the post.The hidden act and source variables are sampled ac-cording to the following transition distributions:6This figure omits hyperparameters as well as act transitionand emission multinomials to reduce clutter.
Dirichlet priors areplaced over all multinomials.175Ptrans(ai|a?i, s,w) ?P (ai|a?i)Wi?j=1P (wi,j |a, s,w?
(i,j))Ptrans(si,j |a, s?
(i,j),w) ?P (si,j |s?
(i,j))P (wi,j |a, s,w?
(i,j))These probabilities can be computed analogously tothe calculations used in the collapsed sampler for abigram HMM (Goldwater and Griffiths, 2007), andthose used for LDA (Griffiths and Steyvers, 2004).Note that our model contains five hyperparame-ters.
Rather than attempt to set them using an ex-pensive grid search, we treat the concentration pa-rameters as additional hidden variables and sampleeach in turn, conditioned on the current assignmentto all other variables.
Because these variables arecontinuous, we apply slice sampling (Neal, 2003).Slice sampling is a general technique for drawingsamples from a distribution by sampling uniformlyfrom the area under its density function.3.3 Estimating Likelihood on Held-Out DataIn Section 4.2 we evaluate our models by comparingtheir probability on held-out test conversations.
Ascomputing this probability exactly is intractable inour model, we employ a recently proposed Chibb-style estimator (Murray and Salakhutdinov, 2008;Wallach et al, 2009).
Chibb estimators estimate theprobability of unseen data, P (w) by selecting a highprobability assignment to hidden variables h?, andtaking advantage of the following equality whichcan be easily derived from the definition of condi-tional probability:P (w) =P (w,h?
)P (h?|w)As the numerator can be computed exactly, this re-duces the problem of estimating P (w) to the eas-ier problem of estimating P (h?|w).
Murray andSalakhutdinov (2008) provide an unbiased estimatorfor P (h?|w), which is calculated using the station-ary distribution of the Gibbs sampler.3.4 Bayesian Conversation modelGiven the infrastructure necessary for the Conver-sation+Topic model described above, it is straight-forward to also implement a Bayesian version ofof the conversation model described in Section 3.1.This amounts to replacing the add-x smoothing ofdialogue act emission and transition probabilitieswith (potentially sparse) Dirichlet priors, and replac-ing EM with Gibbs sampling.
There is reason tobelieve that integrating out multinomials and usingsparse priors will improve the performance of theconversation model, as improvements have been ob-served when using a Bayesian HMM for unsuper-vised part-of-speech tagging (Goldwater and Grif-fiths, 2007).4 ExperimentsEvaluating automatically discovered dialogue actsis a difficult problem.
Unlike previous work, ourmodel automatically discovers an appropriate set ofdialogue acts for a new medium; these acts willnot necessarily have a close correspondence to di-alogue act inventories manually designed for othercorpora.
Instead of comparing against human anno-tations, we present a visualization of the automati-cally discovered dialogue acts, in addition to mea-suring the ability of our models to predict post orderin unseen conversations.
Ideally we would evaluateperformance using an end-use application such as aconversational agent; however as this is outside thescope of this paper, we leave such an evaluation tofuture work.For all experiments we train our models on a set of10,000 randomly sampled conversations with con-versation length in posts ranging from 3 to 6.
Notethat our implementations can likely scale to largerdata by using techniques such as SparseLDA (Yaoet al, 2009).
We limit our vocabulary to the 5,000most frequent words in the corpus.When using EM, we train for 100 iterations, eval-uating performance on the test set at each iteration,and reporting the maximum.
Smoothing parametersare set using grid search on a development set.When performing inference with Gibbs Sam-pling, we use 1,000 samples for burn-in and take10 samples at a lag of 100.
Although using multi-ple samples introduces the possibility of poor resultsdue to ?act drift?, we found this not to be a problemin practice; in fact, taking multiple samples substan-tially improved performance during development.Recall that we infer hyperparameters using slice176sampling.
The concentration parameters chosen inthis manner were always sparse (< 1), which pro-duced a moderate improvement over an uninformedprior.4.1 Qualitative EvaluationWe are quite interested in what our models can tellus about how people converse on Twitter.
To vi-sualize and interpret our competing models, we ex-amined act-emission distributions, posts with high-confidence acts, and act-transition diagrams.
Ofthe three competing systems, we found the Conver-sation+Topic model by far the easiest to interpret:the 10-act model has 8 acts that we found intuitive,while the other 2 are used only with low probabil-ity.
Conversely, the Conversation model, whethertrained by EM or Gibbs sampling, suffered fromthe inclusion of general terms and from the confla-tion of topic and dialogue.
For example, the EM-trained conversation model discovered an ?act?
thatwas clearly a collection of posts about food, with nounderlying dialogue theme (see Table 2).In the remainder of this section, we reproduceour visualization for the 10-act Conversation+Topicmodel.
Word lists summarizing the discovered dia-logue acts are shown in Table 3.
For each act, thetop 40 words are listed in order of decreasing emis-sion probability.
An example post, drawn from theset of highest-confidence posts for that act, is alsoincluded.
Figure 4 provides a visualization of thematrix of transition probabilities between dialogueacts.
An arrow is drawn from one act to the nextif the probability of transition is above 0.15.7 Notethat a uniform model would transition to each actwith probability 0.10.
In both Table 3 and Figure 4,we use intuitive names in place of cluster numbers.These are based on our interpretations of the clus-ters, and are provided only to benefit the reader wheninterpreting the transition diagram.8From inspecting the transition diagram (Figure 4),one can see that the model employs three distinctacts to initiate Twitter conversations.
These initialacts are quite different from one another, and lead to7After setting this threshold, two Acts were cut off from therest of the graph (had no incoming edges), and were thereforeremoved8In some cases, the choice in name is somewhat arbitrary,ie: answer versus response, reaction versus comment.Figure 4: Transitions between dialogue acts.
Seetable 3 for word lists and example posts for each actdifferent sets of possible responses.
We discuss eachof these in turn.The Status act appears to represent a post in whichthe user is broadcasting information about what theyare currently doing.
This can be seen by the highamount of probability mass given to words like Iand my, in addition to verbs such as go and get, aswell as temporal nouns such as today, tomorrow andtonight.The Reference Broadcast act consists mostly ofusernames and urls.9 Also prominent is the word rt,which has special significance on Twitter, indicatingthat the user is re-posting another user?s post.
Thisact represents a user broadcasting an interesting linkor quote to their followers.
Also note that this nodetransitions to the Reaction act with high probability.Reaction appears to cover excited or appreciative re-sponses to new information, assigning high proba-bility to !, !
!, !!
!, lol, thanks, and haha.Finally Question to Followers represents a userasking a question to their followers.
The presenceof the question mark and WH question words indi-cate a question, while words like anyone and knowindicate that the user is asking for information or anopinion.
Note that this is distinct from the Questionact, which is in response to an initial post.Another interesting point is the alternation be-9As part of the preprocessing of our corpus we replaced allusernames and urls with the special tokens -usr- and -url-.177Status I .
to !
my , is for up in ... and going was today so at go get back day got this am but Im now tomorrow night worktonight off morning home had gon need !!
be just gettingI just changed my twitter page bkgornd and now I can?t stop looking at it, lol!
!Question to Followers ?
you is do I to -url- what -usr- me , know if anyone why who can ?
this or of that how does - : on your are needany rt u should people want get did have would tellanyone using google voice?
just got my invite, should i??
don?t know what it is?
-url- for the video and breakdownReference Broadcast -usr- !
-url- rt : -usr-: - ?
my the , is ( you new ?
?
!! )
this for at in follow of on ?
lol u are twitter your thanks via!!!
by :) here 2 please checkrt -usr-: -usr- word that mac lip gloss give u lock jaw!
lolQuestion ?
you what !
are is how u do the did your that , lol where why or ??
hey about was have who it in so haha ondoing going know good up get like were for there :) canDWL!!
what song is that?
?Reaction !
you I :) !!
, thanks lol it haha that love so good too your thank is are u !!!
was for :d me -usr- ?
hope ?
my 3 omg... oh great hey awesome - happy now awwsweet!
im so stoked now!Comment you I .
to , !
do ?
it be if me your know have we can get will :) but u that see lol would are so want go let up wellneed - come ca make or think themwhy are you in tx and why am I just now finding out about it?!
i?m in dfw, till I get a job.
i?ll have to come toHtown soon!Answer .
I , you it ?
that ?
is but do was he the of a they if not would know be did or does think ) like ( as have what in are- no them said who say ?my fave was ?keeping on top of other week?Response .
I , it was that lol but is yeah !
haha he my know yes you :) like too did well she so its ... though do had no - oneas im thanks they think would not good ohnah im out in maryland, leaving for tour in a few days.Table 3: Word lists and example posts for each Dialogue Act.
Words are listed in decreasing order ofprobability given the act.
Example posts are in italics.tween the personal pronouns you and I in the actsdue to the focus of conversation and speaker.
TheStatus act generates the word I with high probability,whereas the likely response state Question generatesyou, followed by Response which again generates I.4.2 Quantitative EvaluationQualitative evaluations are both time-consumingand subjective.
The above visualization is useful forunderstanding the Twitter domain, but it is of littleuse when comparing model variants or selecting pa-rameters.
Therefore, we also propose a novel quan-titative evaluation that measures the intrinsic qual-ity of a conversation model by its ability to predictthe ordering of posts in a conversation.
This mea-sures the model?s predictive power, while requiringno tagged data, and no commitment to an existingtag inventory.Our test set consists of 1,000 randomly selectedconversations not found in the training data.
Foreach conversation in the test set, we generate alln!
permutations of the posts.
The probability ofeach permutation is then evaluated as if it were anunseen conversation, using either the forward algo-rithm (EM) or the Chibb-style estimator (Gibbs).Following work from the summarization community(Barzilay and Lee, 2004), we employ Kendall?s ?
tomeasure the similarity of the max-probability per-mutation to the original order.The Kendall ?
rank correlation coefficient mea-sures the similarity between two permutations basedon their agreement in pairwise orderings:?
=n+ ?
n?
(n2)where n+ is the number of pairs that share the sameorder in both permutations, and n?
is the numberthat do not.
This statistic ranges between -1 and +1,where -1 indicates inverse order, and +1 indicatesidentical order.
A value greater than 0 indicates apositive correlation.Predicting post order on open-domain Twitterconversations is a much more difficult task than ontopic-focused news data (Barzilay and Lee, 2004).We found that a simple bigram model baseline doesvery poorly at predicting order on Twitter, achievingonly a weak positive correlation of ?
= 0.0358 onour test data as compared with 0.19-0.74 reported byBarzilay and Lee on news data.Note that ?
is not a perfect measure of model qual-ity for conversations; in some cases, multiple order-1785 10 15 20 25 30 35 40EM ConversationConversation+TopicBayesian Conversation# actstau0.00.10.20.30.4Figure 5: Performance at conversation ordering task.ings of the same set of posts may form a perfectlyacceptable conversation.
On the other hand, thereare often strong constraints on the type of responsewe might expect to follow a particular dialogue act;for example, answers follow questions.
We wouldexpect an effective model to use these constraints topredict order.Performance at the conversation ordering taskwhile varying the number of acts for each model isdisplayed in Figure 5.
In general, we found that us-ing Bayesian inference outperforms EM.
Also notethat the Bayesian Conversation model outperformsthe Conversation+Topic model at predicting conver-sation order.
This is likely because modeling conver-sation content as a sequence can in some cases helpto predict post ordering; for example, adjacent postsare more likely to contain similar content words.
Re-call though that we found the Conversation+Topicmodel to be far more interpretable.Additionally we compare the likelihood of thesemodels on held out test data in Figure 6.
Note thatthe Bayesian methods produce models with muchhigher likelihood.10 For the EM models, likelihoodtends to decrease on held out test data as we increasethe number of hidden states, due to overfitting.5 ConclusionWe have presented an approach that allows theunsupervised induction of dialogue structure fromnaturally-occurring open-topic conversational data.10Likelihood of the test data is estimated using the ChibbStyle estimator described in (Murray and Salakhutdinov, 2008;Wallach et al, 2009).
This method under-estimates likelihoodin expectation.
The maximum likelihood (EM) likelihoods areexact.5 10 15 20 25 30 35 40EM ConversationConversation+TopicBayesian Conversation# actsnegative log likelihood310000315000320000325000330000335000340000Figure 6: Negative log likelihood on held out testdata (smaller values indicate higher likelihood).By visualizing the learned models, coherent patternsemerge from a stew of data that human readers finddifficult to follow.
We have extended a conversa-tion sequence model to separate topic and dialoguewords, resulting in an interpretable set of automat-ically generated dialogue acts.
These discoveredacts have interesting differences from those foundin other domains, and reflect Twitter?s nature as amicro-blog.We have introduced the task of conversation or-dering as an intrinsic measure of conversation modelquality.
We found this measure quite useful inthe development of our models and algorithms, al-though our experiments show that it does not nec-essarily correlate with interpretability.
We have di-rectly compared Bayesian inference to EM on ourconversation ordering task, showing a clear advan-tage for Bayesian methods.Finally, we have collected a corpus of 1.3 millionTwitter conversations, which we will make availableto the research community, and which we hope willbe useful beyond the study of dialogue.
In the fu-ture, we wish to scale our models to the full corpus,and extend them with more complex notions of dis-course, topic and community.
Ultimately, we hopeto put the learned conversation structure to use in theconstruction of a data-driven, conversational agent.AcknowledgementsWe are grateful to everyone in the NLP and TMSNgroups at Microsoft Research for helpful discussionsand feedback.
We thank Oren Etzioni, Michael Ga-mon, Mausam and Fei Wu, and the anonymous re-viewers for helpful comments on a previous draft.179ReferencesJames Allen, Nathanael Chambers, George Ferguson,Lucian Galescu, Hyuckchul Jung, Mary Swift, andWilliam Taysom.
2007.
Plow: a collaborative tasklearning agent.
In Proceedings of AAAI.Regina Barzilay and Lillian Lee.
2004.
Catching thedrift: Probabilistic content models, with applicationsto generation and summarization.
In Proceedings ofHLT-NAACL, pages 113?120.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alocation.
J. Mach.
Learn.Res., 3:993?1022.William W. Cohen, Vitor R. Carvalho, and Tom M.Mitchell.
2004.
Learning to classify email into?speech acts?.
In Proceedings of EMNLP.Nigel Crook, Ramon Granell, and Stephen Pulman.2009.
Unsupervised classification of dialogue acts us-ing a Dirichlet process mixture model.
In Proceedingsof SIGDIAL, pages 341?348.Hal Daume?
III and Daniel Marcu.
2006.
Bayesian query-focused summarization.
In Proceedings of ACL.Rajdip Dhillon, Sonali Bhagat, Hannah Carvey, and Eliz-abeth Shriberg.
2004.
Meeting recorder project: Dia-log act labeling guide.
Technical report, InternationalComputer Science Institute.Micha Elsner and Eugene Charniak.
2008.
You talkingto me?
A corpus and algorithm for conversation dis-entanglement.
In Proceedings of ACL-HLT.Eric N. Forsyth and Craig H. Martell.
2007.
Lexical anddiscourse analysis of online chat dialog.
In Proceed-ings of ICSC.Michael Gamon, Sumit Basu, Dmitriy Belenko, DanyelFisher, Matthew Hurst, and Arnd Christian Knig.2008.
Blews: Using blogs to provide context for newsarticles.
In Proceedings of ICWSM.Sharon Goldwater and Tom Griffiths.
2007.
A fullybayesian approach to unsupervised part-of-speech tag-ging.
In Proceedings of ACL, pages 744?751.Joshua T. Goodman.
2001.
A bit of progress in languagemodeling.
Technical report.T.
L. Griffiths and M. Steyvers.
2004.
Finding scientifictopics.
Proc Natl Acad Sci, 101 Suppl 1:5228?5235.Aria Haghighi and Lucy Vanderwende.
2009.
Exploringcontent models for multi-document summarization.
InProceedings of HLT-NAACL, pages 362?370.Yijue How and Min-Yen Kan. 2005.
Optimizing pre-dictive text entry for short message service on mobilephones.
In Proceedings of HCII.Minwoo Jeong, Chin-Yew Lin, and Gary Geunbae Lee.2009.
Semi-supervised speech act recognition inemails and forums.
In Proceedings of EMNLP, pages1250?1259.Dan Jurafsky, Liz Shriberg, and Debra Biasca.
1997.Switchboard swbd-damsl shallow-discourse-functionannotation coders manual, draft 13.
Technical report,University of Colorado Institute of Cognitive Science.Ryan Kelly.
2009.
Pear analytics twitter study.
Whitepa-per, August.Iain Murray and Ruslan Salakhutdinov.
2008.
Evalu-ating probabilities under high-dimensional latent vari-able models.
In Proceedings of NIPS, pages 1137?1144.Gabriel Murray, Steve Renals, Jean Carletta, and JohannaMoore.
2006.
Incorporating speaker and discoursefeatures into speech summarization.
In Proceedings ofHLT-NAACL, pages 367?374.Radford M. Neal.
2003.
Slice sampling.
Annals ofStatistics, 31:705?767.Rajesh Ranganath, Dan Jurafsky, and Dan Mcfarland.2009.
It?s not you, it?s me: Detecting flirting andits misperception in speed-dates.
In Proceedings ofEMNLP, pages 334?342.Andreas Stolcke, Noah Coccaro, Rebecca Bates, PaulTaylor, Carol Van Ess-Dykema, Klaus Ries, Eliza-beth Shriberg, Daniel Jurafsky, Rachel Martin, andMarie Meteer.
2000.
Dialogue act modeling forautomatic tagging and recognition of conversationalspeech.
Computational Linguistics, 26(3):339?373.Hanna M. Wallach, Iain Murray, Ruslan Salakhutdinov,and David M. Mimno.
2009.
Evaluation methods fortopic models.
In Proceedings of ICML, page 139.Yorick Wilks.
2006.
Artificial companions as a new kindof interface to the future internet.
In OII Research Re-port No.
13.M.
Woszczyna and A. Waibel.
1994.
Inferring linguisticstructure in spoken language.
In Proceedings of IC-SLP.Tae Yano, William W. Cohen, and Noah A. Smith.
2009.Predicting response to political blog posts with topicmodels.
In Proceedings of NAACL, pages 477?485.Limin Yao, David Mimno, and Andrew McCallum.2009.
Efficient methods for topic model inference onstreaming document collections.
In Proceedings ofKDD, pages 937?946.180
