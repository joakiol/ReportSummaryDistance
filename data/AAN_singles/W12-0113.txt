Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 93?100,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsBootstrapping Method for Chunk Alignment in Phrase Based SMTSantanu Pal Sivaji BandyopadhyayDepartment of Computer Science and Engi-neeringDepartment of Computer Science and Engi-neeringJadavpur University Jadavpur Universitysantanu.pal.ju@gmail.com sivaji_cse@yahoo.comAbstractThe processing of parallel corpus playsvery crucial role for improving the over-all performance in Phrase Based Statisti-cal Machine Translation systems (PB-SMT).
In this paper the automatic align-ments   of different kind of chunks havebeen studied that boosts up the wordalignment as well as the machine transla-tion quality.
Single-tokenization ofNoun-noun MWEs, phrasal preposition(source side only) and reduplicatedphrases (target side only) and the align-ment of named entities and complexpredicates provide the best SMT modelfor bootstrapping.
Automatic bootstrap-ping on the alignment of various chunksmakes significant gains over the previousbest English-Bengali PB-SMT system.The source chunks are translated into thetarget language using the PB-SMT sys-tem and the translated chunks are com-pared with the original target chunk.
Thealigned chunks increase the size of theparallel corpus.
The processes are run ina bootstrapping manner until all thesource chunks have been aligned with thetarget chunks or no new chunk alignmentis identified by the bootstrapping process.The proposed system achieves significantimprovements (2.25 BLEU over the bestSystem and 8.63 BLEU points absoluteover the baseline system, 98.74% relativeimprovement over the baseline system)on an English- Bengali translation task.1 IntroductionThe objective of the present research work is toanalyze effects of chunk alignment in English ?Bengali parallel corpus in a Phrase Based Statis-tical Machine Translation system.
The initial sen-tence level aligned English-Bengali corpus iscleaned and filtered using a semi-automaticprocess.
More effective chunk level alignmentsare carried out by bootstrapping on the trainingcorpus to the PB-SMT system.The objective in the present task is to align thechunks in a bootstrapping manner using a Singletokenized MWE aligned SMT model and thenmodifying the model by inserting the alignedchunks to the parallel corpus after  each iterationof the bootstrapping process, thereby enhancingthe performance of the SMT system.
In turn, thismethod deals with the many-to-many wordalignments in the parallel corpus.
Several typesof MWEs like phrasal prepositions and Verb-object combinations are automatically identifiedon the source side while named-entities andcomplex predicates are identified on both sidesof the parallel corpus.
In the target side only,identification of the Noun-noun MWEs and re-duplicated phrases are carried out.
Simple rule-based and statistical approaches have been usedto identify these MWEs.
The parallel corpus ismodified by considering the MWEs as singletokens.
Source and target language NEs arealigned using a statistical transliteration tech-nique.
These automatically aligned NEs andComplex predicates are treated as translation ex-amples, i.e., as additional entries in the phrasetable (Pal.et al2010, 2011).
Using this aug-mented phrase table each individual sourcechunk is translated into the target chunk and thenvalidated with the target chunks on the targetside.
The validated source-target chunks are con-93sidered as further parallel examples, which ineffect are instances of atomic translation pairs tothe parallel corpus.
This is a well-known practicein domain adaptation in SMT (Eck et al, 2004;Wu et al, 2008).
The preprocessing of the paral-lel corpus results in improved MT quality interms of automatic MT evaluation metrics.The remainder of the paper is organized as fol-lows.
Section 2 briefly elaborates the relatedwork.
The PB-SMT system is described in Sec-tion 3.
The resources used in the present workare described in Section 4.
The various experi-ments carried out and the corresponding evalua-tion results have been reported in Section 5.
Theconclusions are drawn in Section 6 along withfuture work roadmap.2 Related workA multi lingual filtering algorithm generates bi-lingual chunk alignment from Chinese-Englishparallel corpus (Zhou.et al 2004).
The algorithmhas  three steps, first, the most frequent bilingualchunks are extracted from the parallel corpus,second, a clustering algorithm has been used forcombining chunks which are participating foralignment and finally one English chunk is gen-erated corresponding to a Chinese chunk by ana-lyzing the highest co-occurrences of Englishchunks.
Bilingual knowledge can be extractedusing chunk alignment (Zhou.et al 2004).
Thealignment strategies include the comparison ofdependency relations between source and targetsentences.
The dependency related candidates arethen compared with the bilingual dictionary andfinally the chunk is aligned using the extracteddependency related words.
Ma.et al (2007) sim-plified the task of automatic word alignment asseveral consecutive words together correspond toa single word in the opposite language by usingthe word aligner itself, i.e., by bootstrapping onits output.
Zhu and Chang (2008) extracted a dic-tionary from the aligned corpus, used the dic-tionary to re-align the corpus and then extractedthe new dictionary from the new alignment re-sult.
The process goes on until the threshold isreached.An automatic extraction of bilingual MWEs iscarried out by Ren et al (2009), using a log like-lihood ratio based hierarchical reducing algo-rithm to investigate the usefulness of bilingualMWEs in SMT by integrating bilingual MWEsinto the Moses decoder (Koehn et al, 2007).
Thesystem has observed the highest improvementwith an additional feature that identifies whetheror not a bilingual phrase contains bilingualMWEs.
This approach was generalized in Car-puat and Diab (2010) where the binary feature isreplaced by a count feature which is representingthe number of MWEs in the source languagephrase.MWEs on the source and the target sidesshould be both aligned in the parallel corpus andtranslated as a whole.
However, in the state-of-the-art PB-SMT systems, the constituents of anMWE are marked and aligned as parts of con-secutive phrases, since PB-SMT (or any otherapproaches to SMT) does not generally treatMWEs as special tokens.
Another problem withSMT systems is the wrong translation of somephrases.
Sometimes some phrases are not foundin the output sentence.
Moreover, the source andtarget phrases are mostly many-to-many, particu-larly so for the English?Bengali language pair.The main objective of the present work is to seewhether prior automatic alignment of chunks canbring any improvement in the overall perform-ance of the MT system.3 PB-SMT System DescriptionThe system follows three steps; the first step isprepared an SMT system with improved wordalignment that produces a best SMT model forbootstrapping.
And the second step is produced achunk level parallel corpus by using the bestSMT model.
These chunk level parallel corpusesare added with the training corpus to generate thenew SMT model in first iteration.
And finally thewhole process repeats to achieve better chunklevel alignments as well as the better SMTmodel.3.1 SMT System with improved WordAlignmentThe initial English-Bengali parallel corpus iscleaned and filtered using a semi-automaticprocess.
Complex predicates are first extractedon both sides of the parallel corpus.
The analysisand identification of various complex predicateslike, compound verbs (Verb + Verb), conjunctverbs (Noun /Adjective/Adverb + Verb) and se-rial verbs (Verb + Verb + Verb) in Bengali aredone following the strategy in Das.et al (2010).Named-Entities and complex predicates arealigned following a similar technique as reportedin Pal.et al(2011).
Reduplicated phrases do notoccur very frequently in the English corpus;some of them (like correlatives, semantic redu-plications) are not found in English (Chakraborty94and Bandyopadhyay, 2010).
But reduplicationplays a crucial role on the target Bengali side asthey occur with high frequency.
These redupli-cated phrases are considered as a single-token sothat they may map to a single word on the sourceside.
Phrasal prepositions and verb object combi-nations are also treated as single tokens.
Once thecompound verbs and the NEs are identified onboth sides of the parallel corpus, they are assem-bled into single tokens.
When converting theseMWEs into single tokens, the spaces are replacedwith underscores (?_?).
Since there are alreadysome hyphenated words in the corpus, hyphena-tion is not used for this purpose.
Besides, the useof a special word separator (underscore in thiscase) facilitates the job of deciding which single-token MWEs to be de-tokenized into its constitu-ent words, before evaluation.3.1.1 MWE Identification on Source SideThe UCREL1 Semantic analysis System(USAS) developed by Lancaster University(Rayson.et al 2004) has been adopted for MWEidentification.
The USAS is a software tool forthe automatic semantic analysis of English spo-ken and written data.
Various types of Multi-Word Units (MWU) that are identified by theUSAS software include: verb-object combina-tions (e.g.
stubbed out), noun phrases (e.g.
ridingboots), proper names (e.g.
United States ofAmerica), true idioms (e.g.
living the life of Ri-ley) etc.
In English, Noun-Noun (NN) com-pounds, i.e., noun phrases occur with high fre-quency and high lexical and semantic variability(Tanaka.et al 2003).
The USAS software has areported precision value of 91%.3.1.2 MWE Identification on Target SideCompound nouns are identified on the targetside.
Compound nouns are nominal compoundswhere two or more nouns are combined to form asingle phrase such as ?golf club?
or ?computerscience department?
(Baldwin.et al 2010).
Eachelement in a compound noun can function as alexeme in independent of the other lexemes indifferent context.
The system uses Point-wiseMutual Information (PMI), Log-likelihood Ratio(LLR) and Phi-coefficient, Co-occurrence meas-urement and Significance function (Agarwal.etal, 2004) measures for identification of com-pound nouns.
Final evaluation has been carriedout by combining the results of all the methods.A predefined cut-off score has been considered1  http://www.comp.lancs.ac.uk/ucreland the candidates having scores above thethreshold value have been considered as MWEs.The repetition of noun, pronoun, adjective andverb are generally classified as two categories:repetition at the (a) expression level and at the(b) contents or semantic level.
In case of Bengali,The expression-level reduplication are classifiedinto five fine-grained subcategories:  (i) Ono-matopoeic expressions (khat khat, knock knock),(ii) Complete Reduplication (bara-bara, big big),(iii) Partial Reduplication (thakur-thukur, God),(iv) Semantic Reduplication (matha-mundu,head) and (v) Correlative Reduplication(maramari, fighting).For identifying reduplications, simple rulesand morphological properties at lexical levelhave been used (Chakraborty and Bandyop-adhyay, 2010).
The Bengali monolingual dic-tionary has been used for identification of seman-tic reduplications.An NE and Complex Predicates parallel cor-pus is created by extracting the source and thetarget (single token) NEs from the NE-taggedparallel corpus and aligning the NEs using thestrategies as applied in (Pal.et al 2010, 2011).3.1.3 Verb Chunk / Complex PredicateAlignmentInitially, it is assumed that all the members of theEnglish verb chunk in an aligned sentence pairare aligned with the members of the Bengalicomplex predicates.
Verb chunks are alignedusing a statistical aligner.
A pattern generatorextracts patterns from the source and the targetside based on the correct alignment list.
The rootform of the main verb, auxiliary verb present inthe verb chunk and the associated tense, aspectand modality information are extracted for thesource side token.
Similarly, root form of theBengali verb and the associated vibhakti (inflec-tion) are identified on the target side token.
Simi-lar patterns are extracted for each alignment inthe doubtful alignment list.Each pattern alignment for the entries in thedoubtful alignment list is checked with the pat-terns identified in the correct alignment list.
Ifboth the source and the target side patterns for adoubtful alignment match with the source and thetarget side patterns of a correct alignment, thenthe doubtful alignment is considered as a correctone.The doubtful alignment list is checked again tolook for a single doubtful alignment for a sen-tence pair.
Such doubtful alignments are consid-ered as correct alignment.95The above alignment list as well as NEaligned lists are added with the parallel corpusfor creating the SMT model for chunk alignment.The system has reported 15.12 BLEU score fortest corpus and 6.38 (73% relative) point im-provement over the baseline system (Pal.et al2011).3.2 Automatic chunk alignment3.2.1 Source chunk extractionThe source corpus is preprocessed after identify-ing the MWEs using the UCREL tool and singletokenizing the extracted MWEs.
The source sen-tences of the parallel corpus have been parsedusing Stanford POS tagger and then the chunksof the sentences are extracted using CRF chun-ker2 The CRF chunker detects the chunk bounda-ries of noun, verb, adjective, adverb and preposi-tional chunks from the sentences.
After detectionof the individual chunks by the CRF chunker, theboundary of the prepositional phrase chunks areexpanded by examining the series of  nounchunks separated by conjunctions such as'comma', 'and' etc.
or a single noun chunk fol-lowed by a preposition.
For each individualchunk, the head words are identified.
A synony-mous bag of words is generated for each headword.
These bags of words produce more alter-native chunks which are decoded using the bestSMT based system (Section 3.1).
Additionaltranslated target chunks for a single source chunkare generated.CRF Chunker outputbodies/NNS/B-NP of/IN/B-PP all/DT/B-NPages/NNS/I-NP ,/,/O colors/NNS/I-NP and/CC/Osizes/NNS/I-NP don/VB/B-VP the/DT/B-NPvery/JJ/I-NP minimum/NN/I-NP in/IN/B-PP beach-wear/NN/B-NP and/CC/O idle/VB/B-VP away/RP/B-PRT the/DT/B-NP days/NNS/I-NP on/IN/B-PPthe/DT/B-NP sun/NN/I-NP kissed/VBN/I-NP co-pacabana/NN/I-NP and/CC/O ipanema/NN/I-NPbeaches/NNS/I-NP ././ONoun chunk Expansion and boundary detection(bodies/NNS/B-NP) (of/IN/B-PP) (all/DT/B-NPages/NNS/I-NP ,/,/I-NP colors/NNS/I-NP and/CC/I-NP sizes/NNS/I-NP) (don/VB/B-VP) (the/DT/B-NPvery/JJ/I-NP minimum/NN/I-NP) (in/IN/B-PP)(beachwear/NN/B-NP) (and/CC/B-O) (idle/VB/B-VP)(away/RP/B-PRT) (the/DT/B-NP days/NNS/I-NP)2  http://crfchunker.sourceforge.net/(on/IN/B-PP) (the/DT/B-NP sun/NN/I-NPkissed/VBN/I-NP copacabana/NN/I-NP and/CC/I-NPipanema/NN/I-NP beaches/NNS/I-NP) (././B-O)Prepositional phrase expansion and extractionbodiesof all ages , colors and sizesdonthe very minimumin beachwearandidleawaythe dayson the sun kissed copacabana and ipanemabeachesFigure 1.System architecture of the Automatic chunkalignment model3.2.2 Target chunk extractionThe target side of the parallel corpus is cleanedand parsed using the shallow parser developed bythe consortia mode project ?Development of In-dian Language to Indian Language MachineTranslation (IL-ILMT) System Phase II?
fundedby Department of Information Technology, Gov-ernment of India.
The individual chunks are ex-tracted from the parsed output.
The individualchunk boundary is expanded if any noun chunkcontains only single word and several nounchunks occur consecutively.
The content of theindividual chunks are examined by checkingtheir POS categories.
At the time of boundaryexpansion, if the system detects other POS cate-gory words except noun or conjunction then theexpansion process stops immediately and newchunk boundary beginning is identified.
The IL-ILMT system generates the head word for eachindividual chunk.
The chunks for each sentenceare stored in a separate list.
This list is used as a96validation resource for validate the output of thestatistical chunk aligner.3.2.3 Source-Target chunk AlignmentThe extracted source chunks are translated usingthe generated SMT model.
The translated chunksas well as their alternatives are validated with theoriginal target chunk.
During validation check-ing, if any match is found between the translatedchunk and the target chunk then the source chunkis directly aligned with the original target chunk.Otherwise, the source chunk is ignored in thecurrent iteration for any possible alignment.
Thesource chunk will be considered in the nextalignment.
After the current iteration is com-pleted, two lists are produced: a chunk levelalignment list and an unaligned source chunk list.The produced alignment lists are added with theparallel corpus as the additional training corpusto produce new SMT model for the next iterationprocess.
The next iteration process translates thesource chunks that are in the unaligned list pro-duced by the previous iteration.
This processcontinues until the unaligned source chunk list isempty or no further alignment is identified.3.2.4 Source-Target chunk ValidationThe translated target chunks are validated withthe original target list of the same sentence.
Theextracted noun, verb, adjective, adverb andprepositional chunks of the source side may nothave a one to one correspondence with the targetside except for the verb chunk.
There is no con-cept of prepositional chunks on the target side.Some time adjective or adverb chunks may betreated as noun chunk on the target side.
So,chunk level validation for individual categoriesof chunks is not possible.
Source side verbchunks are compared with the target side verbchunks while all the other chunks on the sourceside are compared with all the other chunks onthe target side.
Head words are extracted for eachsource chunk and the translated head words areactually compared on the target side taking intothe consideration the synonymous target words.When the validation system returns positive, thesource chunk is aligned with the identified origi-nal target chunk.4 Tools and Resources usedA sentence-aligned English-Bengali parallel cor-pus containing 14,187 parallel sentences from thetravel and tourism domain has been used in thepresent work.
The corpus has been collectedfrom the consortium-mode project ?Developmentof English to Indian Languages Machine Trans-lation (EILMT) System Phase II3?.
The StanfordParser4, Stanford NER, CRF chunker5 and theWordnet 3.06 have been used for identifyingcomplex predicates in the source English side ofthe parallel corpus.The sentences on the target side (Bengali) areparsed and POS-tagged by using the tools ob-tained from the consortium mode project ?De-velopment of Indian Language to Indian Lan-guage Machine Translation (IL-ILMT) SystemPhase II?.
NEs in Bengali are identified using theNER system of Ekbal and Bandyopadhyay(2008).The effectiveness of the MWE-aligned andchunk aligned parallel corpus is demonstrated byusing the standard log-linear PB-SMT model asour baseline system: GIZA++ implementation ofIBM word alignment model 4, phrase-extractionheuristics described in (Koehn et al, 2003),minimum-error-rate training (Och, 2003) on aheld-out development set, target language modeltrained using SRILM toolkit  (Stolcke, 2002)with Kneser-Ney smoothing (Kneser and Ney,1995) and the Moses decoder (Koehn et al,2007).5 Experiments and Evaluation ResultsWe have randomly identified 500 sentences eachfor the development set and the test set from theinitial parallel corpus.
The rest are considered asthe training corpus.
The training corpus was fil-tered with the maximum allowable sentencelength of 100 words and sentence length ratio of1:2 (either way).
Finally the training corpus con-tains 13,176 sentences.
In addition to the targetside of the parallel corpus, a monolingual Ben-gali corpus containing 293,207 words from thetourism domain was used for the target languagemodel.
The experiments have been carried outwith different n-gram settings for the languagemodel and the maximum phrase length and foundthat a 4-gram language model and a maximumphrase length of 4 produce the optimum baselineresult.
The rest of the experiments have been car-ried out using these settings.3    The EILMT and ILILMT projects are funded bythe Department of Information Technology (DIT), Ministryof Communications and Information Technology (MCIT),Government of India.4    http://nlp.stanford.edu/software/lex-parser.shtml5    http://crfchunker.sourceforge.net/6    http://wordnet.princeton.edu/97The system continues with the various pre-processing of the corpus.
The hypothesis is thatas more and more MWEs and chunks are identi-fied and aligned properly, the system shows theimprovement in the translation procedure.
Table1 shows the MWE statistics of the parallel train-ing corpus.
It is observed from Table 1 that NEsoccur with high frequency in both sides com-pared to other types of MWEs.
It suggests thatprior alignment of the NEs and complex predi-cates plays a role in improving the system per-formance.English Bengali Training setT U T UCPs 4874 2289 14174 7154redupli-cated word- - 85 50Noun-nouncompound892 711 489 300Phrasalpreposition982 779 - -Phrasalverb549 532 - -Total NEwords22931 8273 17107 9106Table 1.
MWE Statistics.
(T - Total occurrence,U ?
Unique, CP ?
complex predicates, NE ?Named Entities)Single tokenization of NEs and MWEs of anylength on both the sides followed by GIZA++alignment has given a huge impetus to systemperformance (6.38 BLEU points absolute, 73%relative improvement over the baseline).
In thesource side, the system treats the phrasal preposi-tions, verb-object combinations and noun-nouncompounds as a single token.
In the target side,single tokenization of reduplicated phrases andnoun-noun compounds has been done followedby alignments using the GIZA++ tool.
From theobservation of Table 2, during first iteration thereare 81821 chunks are identified from the sourcecorpus and 14534 has been aligned by the sys-tem.
For iteration 2, there are 67287 sourcechunks are remaining to align.
At the final itera-tion almost 65% of the source chunks have beenaligned.TrainingsetEnglish BengaliIteration T U T U1 81821 70321 65429 596272 67287 62575 50895 47139final 32325 31409 15933 15654Table 2.
Chunk Statistics.
(T - Total occurrence,U ?
Unique)The system performance improves when thealignment list of NEs and complex predicates aswell as sentence level aligned chunk are incorpo-rated in the baseline best system.
It achieves theBLEU score of 17.37 after the final iteration.This is the best result obtained so far with respectto the baseline system (8.63 BLEU points abso-lute, 98.74% relative improvement in Table 3).
Itmay be observed from Table 3 that baselineMoses without any preprocessing of the datasetproduces a BLEU score of 8.74.Experiments Exp BLEU NISTBaseline 1 8.74 3.98Best System (Alignmentof NEs and ComplexPredicates and SingleTokenization of variousMWEs)2 15.12 4.48Iteration 1 3 15.87 4.49Iteration 2 4 16.28 4.51Iteration 3 5 16.40 4.51Iteration 4 6 16.68 4.52Base-lineBestSys-tem +ChunkAlignmentFinal Iteration?
7 17.37 4.55Table 3.
Evaluation results for different experi-mental setups.
(The ???
marked systems producestatistically significant improvements on BLEUover the baseline system)Intrinsic evaluation of the chunk alignmentcould not be performed as gold-standard wordalignment was not available.
Thus, extrinsicevaluation was carried out on the MT qualityusing the well known automatic MT evaluationmetrics: BLEU (Papineni et al, 2002) and NIST(Doddington, 2002).
Bengali is a morphologi-cally rich language and has relatively free phraseorder.
Proper evaluation of the English-Bengali98MT evaluation ideally requires multiple set ofreference translations.
Moreover, the training setwas smaller in size.6.
Conclusions and Future workA methodology has been presented in this paperto show how the simple yet effective preprocess-ing of various types of MWEs and alignment ofNEs, complex predicates and chunks can boostthe performance of PB-SMT system on an Eng-lish?Bengali translation task.
The best systemyields 8.63 BLEU points improvement over thebaseline, a 98.74% relative increase.
A subset ofthe output from the best system has been com-pared with that of the baseline system, and theoutput of the best system almost always looksbetter in terms of either lexical choice or wordordering.
It is observed that only 28.5% of thetest set NEs appear in the training set, yet priorautomatic alignment of the NEs complex predi-cates and chunk improves the translation quality.This suggests that not only the NE alignmentquality in the phrase table but also the wordalignment and phrase alignment quality improvessignificantly.
At the same time, single-tokenization of MWEs makes the dataset sparser,but improves the quality of MT output to someextent.
Data-driven approaches to MT, specifi-cally for scarce-resource language pairs forwhich very little parallel texts are available,should benefit from these preprocessing meth-ods.
Data sparseness is perhaps the reason whysingle-tokenization of NEs and compound verbs,both individually and in collaboration, did notadd significantly to the scores.
However, a sig-nificantly large parallel corpus can take care ofthe data sparseness problem introduced by thesingle-tokenization of MWEs.AcknowledgementThe work has been carried out with support fromthe consortium-mode project ?Development ofEnglish to Indian Languages Machine Transla-tion (EILMT) System funded by Department ofInformation Technology, Government of India.ReferencesAgarwal, Aswini, Biswajit Ray, Monojit Choudhury,Sudeshna Sarkar and Anupam Basu.
AutomaticExtraction of Multiword Expressions in Bengali:An Approach for Miserly Resource Scenario.
InProc.
of International Conference on Natural Lan-guage Processing (ICON), pp.
165-174.
( 2004)Baldwin, Timothy and Su Nam Kim Multiword Ex-pressions, in Nitin Indurkhya and Fred J.
Damerau(eds.)
Handbook of Natural Language Processing,Second Edition, CRC Press, Boca Raton, USA, pp.267?292 (2010)Banerjee, Satanjeev, and Alon Lavie.. An AutomaticMetric for MT Evaluation with Improved Correla-tion with Human Judgments.
In proceedings of theACL-2005 Workshop on Intrinsic and ExtrinsicEvaluation Measures for MT and/or Summariza-tion, pp.
65-72.
Ann Arbor, Michigan., pp.
65-72.
(2005)Carpuat, Marine, and Mona Diab.
Task-based Evalua-tion of Multiword Expressions: a Pilot Study inStatistical Machine Translation.
In Proc.
of HumanLanguage Technology conference and the NorthAmerican Chapter of the Association for Computa-tional Linguistics conference (HLT-NAACL2010), Los Angeles, CA (2010)Chakraborty, Tanmoy and Sivaji Bandyopadhyay.Identification of Reduplication in Bengali Corpusand their Semantic Analysis: A Rule Based Ap-proach.
In proc.
of the 23rd International Confer-ence on Computational Linguistics (COLING2010), Workshop on Multiword Expressions: fromTheory to Applications (MWE 2010).
Beijing,China.
(2010)Das, Dipankar, Santanu Pal, Tapabrata Mondal, Tan-moy Chakraborty, Sivaji Bandyopadhyay.
Auto-matic Extraction of Complex Predicates in BengaliIn proc.
of the workshop on Multiword expression:from theory to application (MWE-2010), The 23rdInternational conference of computational linguis-tics (Coling 2010),Beijing, Chaina, pp.
37-46.
(2010)Doddington, George.
Automatic evaluation of ma-chine translation quality using n-gram cooccur-rence statistics.
In Proc.
of the Second InternationalConference on Human Language Technology Re-search (HLT-2002), San Diego, CA, pp.
128-132(2002)Eck, Matthias, Stephan Vogel, and Alex Waibel.
Im-proving statistical machine translation in the medi-cal domain using the Unified Medical LanguageSystem.
In Proc.
of the 20th International Confer-ence on Computational Linguistics (COLING2004), Geneva, Switzerland, pp.
792-798 (2004)Ekbal, Asif, and Sivaji Bandyopadhyay.
Voted NERsystem using appropriate unlabeled data.
In proc.of the ACL-IJCNLP-2009 Named Entities Work-shop (NEWS 2009), Suntec, Singapore, pp.202-210 (2009).Huang, Young-Sook, Kyonghee Paik, Yutaka Sasaki,?Bilingual Knowledge Extraction Using ChunkAlignment?, PACLIC 18, Tokiyo, pp.
127-138,(2004).99Kneser, Reinhard, and Hermann Ney.
Improved back-ing-off for m-gram language modeling.
In Proc.
ofthe IEEE Internation Conference on Acoustics,Speech, and Signal Processing (ICASSP), vol.
1,pp.
181?184.
Detroit, MI.
(1995)Koehn, Philipp, Franz Josef Och, and Daniel Marcu.Statistical phrase-based translation.
In Proc.
ofHLT-NAACL 2003: conference combining HumanLanguage Technology conference series and theNorth American Chapter of the Association forComputational Linguistics conference series,  Ed-monton, Canada, pp.
48-54.
(2003)Koehn, Philipp, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Ber-toldi, Brooke Cowan, Wade Shen, ChristineMoran, Richard Zens, Chris Dyer, Ond?ej Bojar,Alexandra Constantin, and Evan Herbst.
Moses:open source toolkit for statistical machine transla-tion.
In Proc.
of the 45th Annual meeting of theAssociation for Computational Linguistics (ACL2007): Proc.
of demo and poster sessions, Prague,Czech Republic, pp.
177-180.
(2007)Koehn, Philipp.
Statistical significance tests for ma-chine translation evaluation.
In  EMNLP-2004:Proc.
of the 2004 Conference on Empirical Meth-ods in Natural Language Processing, 25-26 July2004, Barcelona, Spain, pp 388-395.
(2004)Ma, Yanjun, Nicolas Stroppa, AndyWay.
Proceedingsof the 45th Annual Meeting of the Association ofComputational Linguistics, ,Prague, Czech Repub-lic, June 2007, pp.
304?311 (2007).Moore, Robert C. Learning translations of named-entity phrases from parallel corpora.
In Proc.
of10th Conference of the European Chapter of theAssociation for Computational Linguistics (EACL2003), Budapest, Hungary; pp.
259-266.
(2003)Och, Franz J.
Minimum error rate training in statisti-cal machine translation.
In Proc.
of the 41st AnnualMeeting of the Association for Computational Lin-guistics (ACL-2003), Sapporo, Japan, pp.
160-167.
(2003)Pal Santanu, Sudip Kumar Naskar, Pavel Pecina,Sivaji Bandyopadhyay and Andy Way.
HandlingNamed Entities and Compound Verbs in Phrase-Based Statistical Machine Translation, In proc.
ofthe workshop on Multiword expression: from the-ory to application (MWE-2010), The 23rd Interna-tional conference of computational linguistics (Col-ing 2010),Beijing, Chaina, pp.
46-54 (2010)Pal, Santanu Tanmoy Chakraborty , Sivaji Bandyop-adhyay, ?Handling Multiword Expressions inPhrase-Based Statistical Machine Translation?,Machine Translation Summit XIII(2011),Xiamen,China, pp.
215-224 (2011)Papineni, Kishore, Salim Roukos, Todd Ward, andWei-Jing Zhu.
BLEU: a method for automaticevaluation of machine translation.
In Proc.
of the40th Annual Meeting of the Association for Com-putational Linguistics (ACL-2002), Philadelphia,PA, pp.
311-318 (2002)Rayson, Paul, Dawn Archer, Scott Piao, and TonyMcEnery.
The UCREL Semantic Analysis System.In proc.
Of LREC-04 Workshop: Beyond NamedEntity Recognition Semantic Labeling for NLPTasks, pages 7-12, Lisbon, Porugal (2004)Ren, Zhixiang, Yajuan L?, Jie Cao, Qun Liu, and YunHuang.
Improving statistical machine translationusing domain bilingual multiword expressions.In Proc.
of the 2009 Workshop on Multiword Ex-pressions, ACL-IJCNLP 2009, Suntec, Singapore,pp.
47-54 (2009).Stolcke, A. SRILM?An Extensible Language Mod-eling Toolkit.
Proc.
Intl.
Conf.
on Spoken Lan-guage Processing, vol.
2, pp.
901?904, Denver(2002).Tanaka, Takaaki and Timothy Baldwin.
Noun- NounCompound Machine Translation: A FeasibilityStudy on Shallow Processing.
In Proc.
of the Asso-ciation for Computational Linguistics- 2003,Workshop on Multiword Expressions: Analysis,Acquisition and Treatment, Sapporo, Japan, pp.17?24 (2003)Wu, Hua Haifeng Wang, and Chengqing Zong.
Do-main adaptation for statistical machine translationwith domain dictionary and monolingual cor-pora.
In Proc.
of the 22nd International Conferenceon Computational Linguistics (COLING2008),  Manchester, UK, pp.
993-1000 (2008)Xuan-Hieu Phan, "CRFChunker: CRF English PhraseChunker", http://crfchunker.sourceforge.net/,(2006)Zhou, Yu, chengquing Zong, Bo Xu, ?BilingualChunk Aliment in Statistical Machine Translation?,IEEE International Conference on Systems, Manand Cybernetics, pp.
1401-1406, (2004)100
