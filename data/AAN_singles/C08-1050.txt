Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 393?400Manchester, August 2008The Effect of Syntactic Representation on Semantic Role LabelingRichard Johansson and Pierre NuguesLund University, Sweden{richard, pierre}@cs.lth.seAbstractAlmost all automatic semantic role label-ing (SRL) systems rely on a preliminaryparsing step that derives a syntactic struc-ture from the sentence being analyzed.This makes the choice of syntactic repre-sentation an essential design decision.
Inthis paper, we study the influence of syn-tactic representation on the performanceof SRL systems.
Specifically, we com-pare constituent-based and dependency-based representations for SRL of Englishin the FrameNet paradigm.Contrary to previous claims, our resultsdemonstrate that the systems based on de-pendencies perform roughly as well asthose based on constituents: For the ar-gument classification task, dependency-based systems perform slightly higher onaverage, while the opposite holds for theargument identification task.
This is re-markable because dependency parsers arestill in their infancy while constituent pars-ing is more mature.
Furthermore, the re-sults show that dependency-based seman-tic role classifiers rely less on lexicalizedfeatures, which makes them more robustto domain changes and makes them learnmore efficiently with respect to the amountof training data.1 IntroductionThe role-semantic paradigm has a long and richhistory in linguistics, and the NLP communityhas recently devoted much attention to develop-ing accurate and robust methods for performingc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.role-semantic analysis automatically (Gildea andJurafsky, 2002; Litkowski, 2004; Carreras andM?rquez, 2005; Baker et al, 2007).
It is widelyconjectured that an increased SRL accuracy willlead to improvements in certain NLP applica-tions, especially template-filling systems.
SRL hasalso been used in prototypes of more advancedsemantics-based applications such as textual en-tailment recognition.It has previously been shown that SRL systemsneed a syntactic structure as input (Gildea andPalmer, 2002; Punyakanok et al, 2008).
An im-portant consideration is then what information thisinput should represent.
By habit, most systems forautomatic role-semantic analysis have used Penn-style constituents (Marcus et al, 1993) producedby Collins?
(1997) or Charniak?s (2000) parsers.The influence of the syntactic formalism on SRLhas only been considered in a few previous arti-cles.
For instance, Gildea and Hockenmaier (2003)reported that a CCG-based parser gives improvedresults over the Collins parser.Dependency syntax has only received little at-tention for the SRL task, despite a surge of inter-est in dependency parsing during the last few years(Buchholz and Marsi, 2006).
Early examples ofdependency-based SRL systems, which used gold-standard dependency treebanks, include ?abokrt-sk?
et al (2002) and Hacioglu (2004).
Two stud-ies that compared the respective performances ofconstituent-based and dependency-based SRL sys-tems (Pradhan et al, 2005; Swanson and Gor-don, 2006), both using automatic parsers, reportedthat the constituent-based systems outperformedthe dependency-based ones by a very wide mar-gin.
However, the figures reported in these studiescan be misleading since the comparison involved a10-year-old rule-based dependency parser versus astate-of-the-art statistical constituent parser.
Therecent progress in statistical dependency parsinggives grounds for a new evaluation.393In addition, there are a number of linguistic mo-tivations why dependency syntax could be bene-ficial in an SRL context.
First, complex linguis-tic phenomena such as wh-word extraction andtopicalization can be transparently represented byallowing nonprojective dependency links.
Theselinks also justify why dependency syntax is of-ten considered superior for free-word-order lan-guages; it is even very questionable whether thetraditional constituent-based SRL strategies are vi-able for such languages.
Second, grammaticalfunction such as subject and object is an integralconcept in dependency syntax.
This concept is in-tuitive when reasoning about the link between syn-tax and semantics, and it has been used earlier insemantic interpreters such as Absity (Hirst, 1983).However, except from a few tentative experiments(Toutanova et al, 2005), grammatical function isnot explicitly used by current automatic SRL sys-tems, but instead emulated from constituent treesby features like the constituent position and thegoverning category.
More generally, these lin-guistic reasons have made a number of linguistsargue that dependency structures are more suit-able for explaining the syntax-semantics interface(Mel?c?uk, 1988; Hudson, 1984).In this work, we provide a new evaluation ofthe influence of the syntactic representation on se-mantic role labeling in English.
Contrary to previ-ously reported results, we show that dependency-based systems are on a par with constituent-basedsystems or perform nearly as well.
Furthermore,we show that semantic role classifiers using a de-pendency parser learn faster than their constituent-based counterparts and therefore need less train-ing data to achieve similar performances.
Finally,dependency-based role classifiers are more robustto vocabulary change and outperform constituent-based systems when using out-of-domain test sets.2 Statistical Dependency Parsing forEnglishExcept for small-scale efforts, there is no depen-dency treebank of significant size for English.
Sta-tistical dependency parsers of English must there-fore rely on dependency structures automaticallyconverted from a constituent corpus such as thePenn Treebank (Marcus et al, 1993).Typical approaches to conversion of constituentstructures into dependencies are based on hand-constructed head percolation rules, an idea that hasits roots in lexicalized constituent parsing (Mager-man, 1994; Collins, 1997).
The head rules cre-ated by Yamada and Matsumoto (2003) have beenused in almost all recent work on statistical depen-dency parsing of English (Nivre and Scholz, 2004;McDonald et al, 2005).Recently, Johansson and Nugues (2007) ex-tended the head percolation strategy to incorporatelong-distance links such as wh-movement and top-icalization, and used the full set of grammaticalfunction tags from Penn in addition to a number ofinferred tags (in total 57 function tags).
A depen-dency parser based on this syntax was used in thebest-performing system in the SemEval-2007 taskon Frame-semantic Structure Extraction (Baker etal., 2007), and the conversion method (in two dif-ferent forms) was used for the English data in theCoNLL Shared Tasks of 2007 and 2008.3 Automatic Semantic Role Labelingwith Constituents and DependenciesTo study the influence of syntactic representationon SRL performance, we developed a frameworkthat could be easily parametrized to process eitherconstituent or dependency input1.
This section de-scribes its implementation.
As the role-semanticparadigm, we used FrameNet (Baker et al, 1998).3.1 SystemsWe built SRL systems based on six differentparsers.
All parsers were trained on the Penn Tree-bank, either directly for the constituent parsers orthrough the LTH constituent-to-dependency con-verter (Johansson and Nugues, 2007).
Our systemsare identified as follows:LTH.
A dependency-based system using the LTHparser (Johansson and Nugues, 2008).Malt.
A dependency-based system usingMaltParser (Nivre et al, 2007).MST.
A dependency-based system usingMSTParser (McDonald et al, 2005).C&J.
A constituent-based system using thereranking parser (the May 2006 version) byCharniak and Johnson (2005).Charniak.
A constituent-based system usingCharniak?s parser (Charniak, 2000).Collins.
A constituent-based system usingCollins?
parser (Collins, 1997).1Our implementation is available for download athttp://nlp.cs.lth.se/fnlabeler.394MaltParser is an incremental greedy classifier-based parser based on SVMs, while the LTH parserand MSTParser use exact edge-factored searchwith a linear model trained using online margin-based structure learning.
MaltParser and MST-Parser have achieved state-of-the-art results for awide range of languages in the 2006 and 2007CoNLL Shared Tasks on dependency parsing, andthe LTH parser obtained the best result in the 2008CoNLL Shared Task on joint syntactic and seman-tic parsing.
Charniak?s and Collins?
parsers arewidely used constituent parsers for English, andthe C&J parser is the best-performing freely avail-able constituent parser at the time of writing ac-cording to published figures.
Charniak?s parserand the C&J parser come with a built-in part-of-speech tagger; all other systems used the Stanfordtagger (Toutanova et al, 2003).Following Gildea and Jurafsky (2002), the SRLproblem is traditionally divided into two subtasks:identifying the arguments and labeling them withsemantic roles.
Although state-of-the-art SRL sys-tems use sophisticated statistical models to per-form these two tasks jointly (e.g.
Toutanova etal., 2005, Johansson and Nugues, 2008), we im-plemented them as two independent support vectorclassifiers to be able to analyze the impact of syn-tactic representation on each task separately.
Thefeatures used by the classifiers are traditional, al-though the features for the dependency-based clas-sifiers needed some adaptation.
Table 1 enumer-ates the features, which are described in more de-tail in Appendix A.
The differences in the fea-ture sets reflect the structural differences betweenconstituent and dependency trees: The constituent-only features are based on phrase tags and thedependency-only features on grammatical func-tions labels.3.2 Dependency-based ArgumentIdentificationThe argument identification step consists of find-ing the arguments for a given predicate.
Forconstituent-based SRL, this problem is formulatedas selecting a subset of the constituents in a parsetree.
This is then implemented in practice as abinary classifier that determines whether or not agiven constituent is an argument.
We approachedthe problem similarly in the dependency frame-work, applying the classifier on dependency nodesrather than constituents.
In both cases, the identi-Argument ArgumentFeatures identification classificationTARGETLEMMA C,D C,DFES C,D C,DTARGETPOS C,D C,DVOICE C,D C,DPOSITION C,D C,DARGWORD/POS C,D C,DLEFTWORD/POS C,D C,DRIGHTWORD/POS C,D C,DPARENTWORD/POS C,DC-SUBCAT C CC-PATH C CPHRASETYPE C CGOVCAT C CD-SUBCAT D DD-PATH D DCHILDDEPSET D DPARENTHASOBJ DRELTOPARENT DFUNCTION DTable 1: Classifier features.
The features used bythe constituent-based and the dependency-basedsystems are marked C and D, respectively.fication step was preceded by a pruning stage thatheuristically removes parse tree nodes unlikely torepresent arguments (Xue and Palmer, 2004).To score the performance of the argument iden-tifier, traditional evaluation procedures treat theidentification as a bracketing problem, meaningthat the entities scored by the evaluation procedureare labeled snippets of text; however, it is ques-tionable whether this is the proper way to evalu-ate a task whose purpose is to find semantic re-lations between logical entities.
We believe thatthe same criticisms that have been leveled at thePARSEVAL metric for constituent structures areequally valid for the bracket-based evaluation ofSRL systems.
The inappropriateness of the tra-ditional metric has led to a number of alternativemetrics (Litkowski, 2004; Baker et al, 2007).We have stuck to the traditional bracket-basedscoring metric for compatibility with previous re-sults, but since it represents the arguments as la-beled spans, a conversion step is needed when us-ing dependencies.
Algorithm 1 shows how thespans are constructed from the argument depen-dency nodes.
For each argument node, the algo-rithm computes the yield Y , the set of dependencynodes to include in the bracketing.
This set is thenpartitioned into contiguous parts, which are thenconverted into spans.
In most cases, the yield isjust the subtree dominated by the argument node.However, if the argument dominates the predi-395cate, then the branch containing the predicate isremoved.
Also, FrameNet alows arguments to co-incide with the predicate; in this case, the yield isjust the predicate node.Algorithm 1 Span creation from argument depen-dency nodes.input Predicate node p, argument node aif a does not dominate pY ?
{n; a dominates n}else if p = aY ?
{p}elsec?
the child of a that dominates pY ?
{n; a dominates n} \ {n; c dominates n}end ifS ?
partition of Y into contiguous subsetsreturn {(min-index s,max-index s); s ?
S}that we have been relying onthe ideasROOT?FRAGVCSBJ VC CLRPMODNMODNMODFigure 1: Example of a dependency tree containinga predicate relyingwith three arguments: the ideas,we, and on .
.
.
that.To illustrate Algorithm 1, consider Figure 1.
Inthis sentence, the predicate relying has three argu-ments: the ideas, we, and on .
.
.
that.
The simplestof them is we, which does not dominate its predi-cate and which is not discontinuous.
A more com-plex case is the discontinuous argument headed byon, where the yield {on, that} is partitioned intotwo subsets that result in two separate spans.
Fi-nally, the dependency node ideas dominates thepredicate.
In this case, the algorithm removes thesubtree headed by have, so the remaining yield is{the, ideas}.4 ExperimentsWe carried out a number of experiments to com-pare the influence of the syntactic representationon different aspects of SRL performance.
Weused the FrameNet example corpus and running-text corpus, from which we randomly sampled atraining and test set.
The training set consistedof 134,697 predicates and 271,560 arguments, andthe test set of 14,952 predicates and 30,173 argu-ments.
This does not include null-instantiated ar-guments, which were removed from the trainingand test sets.4.1 Argument IdentificationBefore evaluating the full automatic argumentidentification systems, we studied the effect ofthe span creation from dependency nodes (Algo-rithm 1).
To do this, we measured the upper-boundrecall of argument identification using the con-ventional span-based evaluation metric.
We com-pared the quality of pruned spans (Algorithm 1)to unpruned spans (a baseline method that brack-ets the full subtree).
Table 2 shows the re-sults of this experiment.
The figures show thatproper span creation is essential when the tradi-tional metrics are used: For all dependency-basedsystems, the upper-bound recall increases signif-icantly.
However, the dependency-based systemsgenerally have lower figures for the upper-boundrecall than constituent-based ones.System Pruned UnprunedLTH 83.9 82.1Malt 82.1 78.3MST 80.4 77.1C&J 85.3Charniak 83.4Collins 81.8Table 2: Upper-bound recall for argument identifi-cation.Our first experiment investigated how the syn-tactic representation influenced the performanceof the argument identification step.
Table 3shows the result of this evaluation.
As can beseen, the constituent-based systems outperform thedependency-based systems on average.
However,the picture is not clear enough to draw any firmconclusion about a fundamental structural differ-ence.
There are also a number of reasons to be cau-tious: First, the dependency parsers were trainedon a treebanks that had been automatically cre-ated from a constituent treebank, which probablyresults in a slight decrease in annotation quality.Second, dependency parsing is still a developingfield, while constituent parsing is more mature.The best constituent parser (C&J) is a rerankingparser utilizing global features, while the depen-dency parsers use local features only; we believe396that a reranker could be used to improve the de-pendency parsers as well.System P R F1LTH 79.7 77.3 78.5Malt 77.4 73.8 75.6MST 73.9 71.9 72.9C&J 81.4 77.3 79.2Charniak 79.8 75.0 77.3Collins 78.4 72.9 75.6Table 3: Argument identification performance.Differences between parsers using the same syn-tactic formalism are also considerable, which sug-gests that the attachment accuracy is probably themost important parameter when choosing a parserfor this task.4.2 Argument ClassificationTo evaluate the argument classification accuracies,we provided the systems with gold-standard argu-ments, which were then automatically classified.Table 4 shows the results.System AccuracyLTH 89.6Malt 88.5MST 88.1C&J 88.9Charniak 88.5Collins 88.3Table 4: Semantic role classification accuracy.Here, the situation is different: the bestdependency-based system make 6.3% fewer errorsthan the best constituent-based one, a statisticallysignificant difference at the 99.9% level accordingto a McNemar test.
Again, there are no clear differ-ences that can be attributed to syntactic formalism.However, this result is positive, because it showsclearly that SRL can be used in situations whereonly dependency parsers are available.On the other hand, it may seem paradoxicalthat the rich set of grammatical functions used bythe dependency-based systems did not lead to aclearer difference between the groups, despite thelinguistic intuition that this feature should be use-ful for argument classification.
Especially for forthe second- and third-best systems (Malt and MSTversus Charniak and Collins), the performance fig-ures are almost identical.
However, all systemsuse lexical features of the argument, and givenenough training data, one may say that the gram-matical function is implicitly encoded in these fea-tures.
This suggests that lexical features are moreimportant for constituent-based systems than fordependency-based ones.4.3 Robustness of SRL ClassifiersIn this section, we test the hypothesis that theSRL systems based on dependency syntax rely lessheavily on lexical features.
We also investigatetwo parameters that are influenced by lexicaliza-tion: domain sensitivity and the amount of trainingdata required by classifiers.Tests of Unlexicalized ModelsTo test the hypothesis about the reliance on lex-icalization, we carried out a series of experimentswhere we set aside the lexical features of the argu-ment.
Table 5 shows the results.As expected, there is a sharp drop in perfor-mance for all systems, but the results are veryclear: When no argument lexical features are avail-able, the dependency-based systems have a supe-rior performance.
The difference between MSTand C&J constitutes an error reduction of 6.9% andis statistically significant at the 99.9% level.System AccuracyLTH 83.0Malt 81.9MST 81.7C&J 80.3Charniak 80.0Collins 79.8Table 5: Accuracy for unlexicalized role classi-fiers.
Dependency-based systems make at least6.9% fewer errors.Training Set SizeSince the dependency-based systems rely lesson lexicalization, we can expect them to have asteeper learning curve.
To investigate this, wetrained semantic role classifiers using training setsof varying sizes and compared the average clas-sification accuracies of the two groups.
Fig-ure 2 shows the reduction in classification errorof the dependency-based group compared to theconstituent-based group (again, all systems werelexicalized).
For small training sets, the differ-ences are large; the largest observed error reduc-397tion was 5.4% with a training set of 25,000 in-stances.
When the training set size increases, thedifference between the groups decreases.
The plotis consistent with our hypothesis that the gram-matical function features used by the dependency-based systems make generalization easier for sta-tistical classifiers.
We interpret the flatter learningcurves for constituent-based systems as a conse-quence of lexicalization ?
these systems need moretraining data to use lexical information to capturegrammatical function information implicitly.10511.522.533.544.555.56Training set sizeError reductionFigure 2: Error reduction of average dependency-based systems as a function of training set size.Out-of-domain Test SetsWe finally conducted an evaluation of the se-mantic role classification accuracies on an out-of-domain test set: the FrameNet-annotated NuclearThreat Initiative texts from SemEval task (Bakeret al, 2007).
Table 6 shows the results.
This cor-pus contained 9,039 predicates and 15,343 argu-ments.
The writing style is very different fromthe FrameNet training data, and the annotated datacontain several instances of predicates and framesunseen in the training set.
We thus see that all sys-tems suffer severely from domain sensitivity, butwe also see that the dependency-based systems aremore resilient ?
the difference between MST andC&J is statistically significant at the 97.5% leveland corresponds to an error reduction of 2%.
Theexperiment reconfirms previous results (Carrerasand M?rquez, 2005) that the argument classifica-tion part of SRL systems is sensitive to domainchanges, and Pradhan et al (2008) argued that animportant reason for this is that the lexical fea-tures are heavily domain-dependent.
Our resultsare consistent with this hypothesis, and suggestthat the inclusion of grammatical function featuresis an effective way to mitigate this sensitivity.System AccuracyLTH 71.1Malt 70.1MST 70.1C&J 69.5Charniak 69.3Collins 69.3Table 6: Classification accuracy on the NTI texts.Dependency-based systems make 2% fewer errors.5 DiscussionWe have described a set of experiments that in-vestigate the relation between syntactic represen-tation and semantic role labeling performance,specifically focusing on a comparison betweenconstituent- and dependency-based SRL systems.A first conclusion is that our dependency-basedsystems perform more or less as well as the moremature constituent-based systems: For the argu-ment classification task, dependency-based sys-tems are slightly better on average, while theconstituent-based systems perform slightly higherin argument identification.This result contrasts with previously publishedcomparisons, which used less accurate depen-dency parsers, and shows that semantic analyz-ers can be implemented for languages where con-stituent parsers are not available.
While traditionalconstituent-based SRL techniques have so far beenapplied to languages characterized by simple mor-phology and rigid word order, such as English andChinese, we think that dependency-based SRL canbe particularly useful for languages with a freeword order.For dependency-based systems, the conversionfrom parse tree nodes to argument spans, whichare needed to use the traditional span-based evalu-ation method, is less trivial than in the constituentcase.
To make a comparison feasible, we imple-mented an algorithm for span creation from ar-gument nodes.
However, the fundamental prob-lem lies in evaluation ?
the field needs to designnew evaluation procedures that use some sort oflink-based scoring method.
The evaluation met-rics used in the SemEval task on Frame-semantic398Structure Extraction and the 2008 CoNLL SharedTask are examples of steps in the right direction.Our second main result is that for argumentclassification, dependency-based systems rely lessheavily on lexicalization, and we suggest that thisis because they use features based on grammaticalfunction labels.
These features make the learningcurve steeper when training the classifier, and im-prove robustness to domain changes.A Features Used by the ClassifiersThe following subsections itemize the featuresused by the systems.
All examples are given withrespect to the sentence she gave the horse an apple.The constituent and dependency trees are shownin Figure 3.
For this sentence, the predicate isgave, which has the FrameNet frame GIVING.
Ithas three arguments: she, which has the DONORrole; the horse, the RECIPIENT; and an apple, theTHEME.NP NP NPVPSgave the horse an appleshegave the horse an applesheSBJROOT?SNMODNMODIOBJOBJFigure 3: Examples of parse trees.A.1 Common FeaturesThe following features are used by both theconstituent-based and the dependency-based se-mantic analyzers.
Head-finding rules (Johanssonand Nugues, 2007) were applied when heads ofconstituents were needed.TARGETLEMMA.
The lemma of the target worditself, e.g.
give.FES.
For a given frame, the set of available frameelements listed in FrameNet.
For instance, forgive in the GIVING frame, we have 12 frameelements: DONOR, RECIPIENT, THEME, .
.
.TARGETPOS.
Part-of-speech tag for the targetword.VOICE.
For verbs, this feature is Active or Pas-sive.
For other types of words, it is not de-fined.POSITION.
Position of the head word of the argu-ment with respect to the target word: Before,After, or On.ARGWORD and ARGPOS.
Lexical form andpart-of-speech tag of the head word of the ar-gument.LEFTWORD and LEFTPOS.
Form and part-of-speech tag of the leftmost dependent of theargument head.RIGHTWORD and RIGHTPOS.
Form and part-of-speech tag of the rightmost dependent ofthe argument head.PARENTWORD and PARENTPOS.
Form andpart-of-speech tag of the parent node of thetarget.A.2 Features Used by the Constituent-basedAnalyzer OnlyC-SUBCAT.
Subcategorization frame: corre-sponds to the phrase-structure rule used to ex-pand the phrase around the target.
For give inthe example, this feature is VP?VB NP NP.C-PATH.
A string representation of the paththrough the constituent tree from the targetword to the argument constituent.
For in-stance, the path from gave to she is ?VP-?S-?NP.PHRASETYPE.
Phrase type of the argument con-stituent, e.g.
NP for she.GOVCAT.
Governing category: this feature is ei-ther S or VP, and is found by starting at the ar-gument constituent and moving upwards untileither a VP or a sentence node (S, SINV, orSQ) is found.
For instance, for she, this fea-ture is S, while for the horse, it is VP.
Thiscan be thought of as a very primitive way ofdistinguishing subjects and objects.A.3 Features Used by the Dependency-basedAnalyzer OnlyD-SUBCAT.
Subcategorization frame: thegrammatical functions of the dependentsconcatenated.
For gave, this feature isSBJ+IOBJ+OBJ.D-PATH.
A string representation of the paththrough the dependency tree from the targetnode to the argument node.
Moving upwardsthrough verb chains is not counted in this pathstring.
In the example, the path from gave toshe is ?SBJ.399CHILDDEPSET.
The set of grammatical func-tions of the direct dependents of the targetnode.
For instance, for give, this is { SBJ,IOBJ, OBJ }.PARENTHASOBJ.
Binary feature that is set totrue if the parent of the target has an object.RELTOPARENT.
Dependency relation betweenthe target node and its parent.FUNCTION.
The grammatical function of the ar-gument node.
For direct dependents of thetarget, this feature is identical to the D-PATH.ReferencesBaker, Collin F., Charles J. Fillmore, and John B. Lowe.1998.
The Berkeley FrameNet Project.
In Proceedingsof COLING/ACL-1998.Baker, Collin, Michael Ellsworth, and Katrin Erk.
2007.
Se-mEval task 19: Frame semantic structure extraction.
InProceedings of SemEval-2007.Buchholz, Sabine and Erwin Marsi.
2006.
CoNLL-X sharedtask on multilingual dependency parsing.
In Proceedingsof the CoNLL-X.Carreras, Xavier and Llu?s M?rquez.
2005.
Introduction tothe CoNLL-2005 shared task: Semantic role labeling.
InProceedings of CoNLL-2005.Charniak, Eugene and Mark Johnson.
2005.
Coarse-to-finen-best parsing and MaxEnt discriminative reranking.
InProceedings of ACL.Charniak, Eugene.
2000.
A maximum-entropy-inspiredparser.
In Proceedings of NAACL-2000.Collins, Michael.
1997.
Three generative, lexicalised modelsfor statistical parsing.
In Proceedings of ACL/EACL-1997.Gildea, Daniel and Julia Hockenmaier.
2003.
Identifyingsemantic roles using combinatory categorial grammar.
InProceedings of EMNLP-2003.Gildea, Daniel and Daniel Jurafsky.
2002.
Automaticlabeling of semantic roles.
Computational Linguistics,28(3):245?288.Gildea, Daniel and Martha Palmer.
2002.
The necessity ofsyntactic parsing for predicate argument recognition.
InProceedings of the ACL-2002.Hacioglu, Kadri.
2004.
Semantic role labeling using depen-dency trees.
In Proceedings of COLING-2004.Hirst, Graeme.
1983.
A foundation for semantic interpreta-tion.
In Proceedings of the ACL-1983.Hudson, Richard.
1984.
Word Grammar.
Blackwell.Johansson, Richard and Pierre Nugues.
2007.
Extendedconstituent-to-dependency conversion for English.
In Pro-ceedings of NODALIDA 2007.Johansson, Richard and Pierre Nugues.
2008.
Dependency-based syntactic?semantic analysis with PropBank andNomBank.
In Proceedings of CoNLL?2008.Litkowski, Ken.
2004.
Senseval-3 task: Automatic labelingof semantic roles.
In Proceedings of Senseval-3.Magerman, David M. 1994.
Natural language parsing as sta-tistical pattern recognition.
Ph.D. thesis, Stanford Univer-sity.Marcus, Mitchell P., Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated corpusof English: the Penn Treebank.
Computational Linguis-tics, 19(2):313?330.McDonald, Ryan, Koby Crammer, and Fernando Pereira.2005.
Online large-margin training of dependency parsers.In Proceedings of ACL-2005.Mel?c?uk, Igor A.
1988.
Dependency Syntax: Theory andPractice.
State University Press of New York.Nivre, Joakim and Mario Scholz.
2004.
Deterministic depen-dency parsing of English text.
In Proceedings of COLING-2004.Nivre, Joakim, Johan Hall, Jens Nilsson, Atanas Chanev,G?ls?en Eryig?it, Sandra K?bler, Svetoslav Marinov, and Er-win Marsi.
2007.
MaltParser: A language-independentsystem for data-driven dependency parsing.
Natural Lan-guage Engineering, 13(2):95?135.Pradhan, Sameer, Wayne Ward, Kadri Hacioglu, James Mar-tin, and Daniel Jurafsky.
2005.
Semantic role labelingusing different syntactic views.
In Proceedings of ACL-2005.Pradhan, Sameer, Wayne Ward, and James H. Martin.
2008.Towards robust semantic role labeling.
ComputationalLinguistics, 34(2):289?310.Punyakanok, Vasin, Dan Roth, and Wen-tau Yih.
2008.
Theimportance of syntactic parsing and inference in semanticrole labeling.
Computational Linguistics, 34(2):257?287.Swanson, Reid and Andrew S. Gordon.
2006.
A comparisonof alternative parse tree paths for labeling semantic roles.In Proceedings of COLING/ACL-2006.Toutanova, Kristina, Dan Klein, Christopher Manning, andYoram Singer.
2003.
Feature-rich part-of-speech taggingwith a cyclic dependency network.
In Proceedings of HLT-NAACL-2003.Toutanova, Kristina, Aria Haghighi, and Christopher D. Man-ning.
2005.
Joint learning improves semantic role label-ing.
In Proceedings of ACL-2005.
?abokrtsk?, Zdene?k, Petr Sgall, and Sa?o D?eroski.
2002.A machine learning approach to automatic functor assign-ment in the Prague dependency treebank.
In Proceedingsof LREC-2002.Xue, Nianwen and Martha Palmer.
2004.
Calibrating featuresfor semantic role labeling.
In Proceedings of EMNLP-2004.Yamada, Hiroyasu and Yuji Matsumoto.
2003.
Statistical de-pendency analysis with support vector machines.
In Pro-ceedings of IWPT-2003.400
