Proceedings of the ACL 2011 Workshop on Relational Models of Semantics (RELMS 2011), pages 2?10,Portland, Oregon, USA, June 23, 2011. c?2011 Association for Computational LinguisticsCustomizing an Information Extraction System to a New DomainMihai Surdeanu, David McClosky, Mason R. Smith, Andrey Gusev,and Christopher D. ManningDepartment of Computer ScienceStanford UniversityStanford, CA 94305{mihais,mcclosky,mrsmith,manning}@stanford.eduagusev@cs.stanford.eduAbstractWe introduce several ideas that improve theperformance of supervised information ex-traction systems with a pipeline architecture,when they are customized for new domains.We show that: (a) a combination of a se-quence tagger with a rule-based approach forentity mention extraction yields better perfor-mance for both entity and relation mentionextraction; (b) improving the identification ofsyntactic heads of entity mentions helps rela-tion extraction; and (c) a deterministic infer-ence engine captures some of the joint domainstructure, even when introduced as a post-processing step to a pipeline system.
All in all,our contributions yield a 20% relative increasein F1 score in a domain significantly differ-ent from the domains used during the devel-opment of our information extraction system.1 IntroductionInformation extraction (IE) systems generally con-sist of multiple interdependent components, e.g., en-tity mentions predicted by an entity mention detec-tion (EMD) model connected by relations via a re-lation mention detection (RMD) component (Yao etal., 2010; Roth and Yih, 2007; Surdeanu and Cia-ramita, 2007).
Figure 1 shows a sentence from asports domain where both entity and relation men-tions are annotated.
When training data exists, thebest performance in IE is generally obtained by su-pervised machine learning approaches.
In this sce-nario, the typical approach for domain customiza-tion is apparently straightforward: simply retrainon data from the new domain (and potentially tunemodel parameters).
In this paper we argue that, evenwhen considerable training data is available, this isnot sufficient to maximize performance.
We applyseveral simple ideas that yield a significant perfor-mance boost, and can be implemented with minimaleffort.
In particular:?
We show that a combination of a conditionalrandom field model (Lafferty et al, 2001) witha rule-based approach that is recall orientedyields better performance for EMD and forthe downstream RMD component.
The rule-based approach includes gazetteers, which havebeen shown to be important by Mikheev et al(1999), among others.?
We improve the unification of the predicted se-mantic annotations with the syntactic analy-sis of the corresponding text, i.e., finding thesyntactic head of a given semantic constituent.Since many features in an IE system depend onsyntactic analysis, this leads to more consistentfeatures and better extraction models.?
We add a simple inference engine that gener-ates additional relation mentions based solelyon the relation mentions extracted by the RMDmodel.
This engine mitigates some of the limi-tations of a text-based RMD model, which can-not extract relations not explicitly stated in text.We investigate these ideas using an IE system thatperforms recognition of entity mentions followed byextraction of binary relations between these men-tions.
We used as target a sports domain that is sig-nificantly different from the corpora previously usedwith this IE system.
The target domain is also sig-nificantly different from the dataset used to train the2Rookie?Mike?Anderson?scored?two?second-??half?touchdowns,??leading?the?Broncos?to?their?sixth?straight?victory,?31?-???24??over?the?Sea?le?Seahawks?on?Sunday.?ScoreType-??2?FinalScore?
FinalScore?NFLGame?NFLTeam?NFLTeam?
Date?teamInGame?gameWinner?touchdownPar?alCount?teamScoringAll?teamInGame?gameDate?gameLoser?teamScoringAll?teamFinalScore?teamFinalScore?Figure 1: Sample sentence from the NFL domain.
The domain contains entity mentions (underlined with entity typesin bold) and binary relations between entity mentions (indicated by arrows; relation types are italicized).supporting natural language processing tools (e.g.,syntactic parser).
Our investigation shows that, de-spite their simplicity, all our proposals help, yieldinga 20% relative improvement in RMD F1 score.The paper is organized as follows: Section 2 sur-veys related work.
Section 3 describes the IE systemused.
We cover the target domain that serves as usecase in this paper in Section 4.
Section 5 introducesour ideas and evaluates their impact in the target do-main.
Finally, Section 6 concludes the paper.2 Related WorkOther recent works have analyzed the robustness ofinformation extraction systems.
For example, Flo-rian et al (2010) observed that EMD systems per-form badly on noisy inputs, e.g., automatic speechtranscripts, and propose system combination (sim-ilar to our first proposal) to increase robustness insuch scenarios.
Ratinov and Roth (2009) also in-vestigate design challenges for named entity recog-nition, and showed that other design choices, suchas the representation of output labels and using fea-tures built on external knowledge, are more impor-tant than the learning model itself.
These works areconceptually similar to our paper, but we proposeseveral additional directions to improve robustness,and we investigate their impact in a complete IE sys-tem instead of just EMD.Several of our lessons are drawn from the BioCre-ative challenge1 and the BioNLP shared task (Kim1http://biocreative.sourceforge.net/et al, 2009).
These tasks have shown the impor-tance of high quality syntactic annotations and usingheuristic fixes to correct systematic errors (Schumanand Bergler, 2006; Poon and Vanderwende, 2010,among others).
Systems in the latter task have alsoshown the importance of high recall in the earlierstages of pipeline system.3 Description of the Generic IE SystemWe illustrate our proposed ideas using a simple IEsystem that implements a pipeline architecture: en-tity mention extraction followed by relation men-tion extraction.
Note however that the domain cus-tomization discussion in Section 5 is independent ofthe system architecture or classifiers used for EMDand RMD, and we expect the proposed ideas to ap-ply to other IE approaches as well.We performed all pre-processing (tokenization,part-of-speech (POS) tagging) with the StanfordCoreNLP toolkit.2 For EMD we used the Stanfordnamed entity recognizer (Finkel et al, 2005).
In allour experiments we used a generic set of features(?macro?)
and the IO notation3 for entity mention la-bels (e.g., the labels for the tokens ?over the SeattleSeahawks on Sunday?
(from Figure 1) are encodedas ?O O NFLTEAM NFLTEAM O DATE?
).2http://nlp.stanford.edu/software/corenlp.shtml3The IO notation facilitates faster inference than the IOBor IOB2 notations with minimal impact on performance, whenthere are fewer adjacent mentions with the same type.3ArgumentFeatures?
Head words of the two argumentsand their combination?
Entity mention labels of the twoarguments and their combinationSyntacticFeatures?
Sequence of dependency labelsin the dependency path linking theheads of the two arguments?
Lemmas of all words in the de-pendency path?
Syntactic path in the constituentparse tree between the largest con-stituents headed by the same wordsas the two arguments (similarto Gildea and Jurafsky (2002))SurfaceFeatures?
Concatenation of POS tags be-tween arguments?
Binary indicators set to true ifthere is an entity mention with agiven type between the two argu-mentsTable 1: Feature set used for RMD.The RMD model was built from scratch as amulti-class classifier that extracts binary relationsbetween entity mentions in the same sentence.
Dur-ing training, known relation mentions become pos-itive examples for the corresponding label and allother possible combinations between entity men-tions in the same sentence become negative exam-ples.
We used a multiclass logistic regression classi-fier with L2 regularization.
Our feature set is takenfrom (Yao et al, 2010; Mintz et al, 2009; Roth andYih, 2007; Surdeanu and Ciaramita, 2007) and mod-els the relation arguments, the surface distance be-tween the relation arguments, and the syntactic pathbetween the two arguments, using both constituencyand dependency representations.
For syntactic in-formation, we used the Stanford parser (Klein andManning, 2003) and the Stanford dependency repre-sentation (de Marneffe et al, 2006).For RMD, we implemented an additive feature se-lection algorithm similar to the one in (Surdeanuet al, 2008), which iteratively adds the featurewith the highest improvement in F1 score to thecurrent feature set, until no improvement is seen.The algorithm was configured to select featuresthat yielded the best combined performance on thedataset from Roth and Yih (2007) and the trainingpartition of ACE 2007.4 We used ten-fold cross val-4LDC catalog numbers LDC2006E54 and LDC2007E11Documents Words Entity RelationMentions Mentions110 70,119 2,188 1,629Table 2: Summary statistics of the NFL corpus, after ourconversion to binary relations.idation on both datasets.
We decided to use a stan-dard F1 score to evaluate RMD performance ratherthan the more complex ACE score because we be-lieve that the former is more interpretable.
We usedgold entity mentions for the feature selection pro-cess.
Table 1 summarizes the final set of featuresselected.Despite its simplicity, our approach achievescomparable performance with other state-of-the-artresults reported on these datasets (Roth and Yih,2007; Surdeanu and Ciaramita, 2007).
For exam-ple, Surdeanu and Ciaramita report a RMD F1 scoreof 59.4 for ACE relation types (i.e., ignoring sub-types) when gold entity mentions are used.
Underthe same conditions, our RMD model obtains a F1score of 59.2.4 Description of the Target DomainIn this paper we report results on the ?MachineReading NFL Scoring?
corpus.5 This corpus wasdeveloped by LDC for the DARPA Machine Read-ing project.
The corpus contains 110 newswire arti-cles on National Football League (NFL) games.
Theannotations cover game information, such as partici-pating teams, winners and losers, partial (e.g., a sin-gle touchdown or three field goals) and final scores.Most of the annotated relations in the original corpusare binary (e.g.
GAMEDATE(NFLGAME, DATE))but some are n-ary relations or include other at-tributes in addition of the relation type.
We reducethese to annotations compatible with our RMD ap-proach as follows:?
We concatenate the cardinality of each scoringevent (i.e.
how many scoring events are be-ing talked about) to the corresponding SCORE-TYPE entity label.
Thus SCORETYPE-2 in-dicates that there were two of a given typeof scoring event (touchdown, field goal, etc.
).This operation is necessary because the cardi-nality of scoring events is originally annotatedas an additional attribute of the SCORETYPE5LDC catalog number LDC2009E1124Entity Mentions Correct Predicted Actual P R F1Date 141 190 174 74.2 81.0 77.5FinalScore 299 328 347 91.2 86.2 88.6NFLGame 71 109 147 65.1 48.3 55.5NFLPlayoffGame 8 25 38 32.0 21.1 25.4NFLTeam 651 836 818 77.9 79.6 78.7ScoreType-1 329 479 525 68.7 62.7 65.5ScoreType-2 49 68 79 72.1 62.0 66.7ScoreType-3 17 26 36 65.4 47.2 54.8ScoreType-4 6 11 14 54.5 42.9 48.0Total 1571 2076 2188 75.7 71.8 73.7Relation Mentions Correct Predicted Actual P R F1fieldGoalPartialCount 33 41 101 80.5 32.7 46.5gameDate 32 36 115 88.9 27.8 42.4gameLoser 22 44 124 50.0 17.7 26.2gameWinner 6 15 123 40.0 4.9 8.7teamFinalScore 95 101 232 94.1 40.9 57.1teamInGame 49 105 257 46.7 19.1 27.1teamScoringAll 202 232 321 87.1 62.9 73.1touchDownPartialCount 156 191 322 81.7 48.4 60.8Total 595 766 1629 77.7 36.5 49.7Table 3: Baseline results: stock system without any domain customization.
Correct/Predicted/Actual indicate the num-ber of mentions (entities or relations) that are correctly predicted/predicted/gold.
P/R/F1 indicate precision/recall/F1scores for the corresponding label.entity and our EMD approach does not modelmention attributes.?
We split all n-ary relations into several newbinary relations.
For example, the originalTEAMFINALSCORE(NFLTEAM, NFLGAME,FINALSCORE) relation is split into three binaryrelations: TEAMSCORINGALL(NFLTEAM,FINALSCORE), TEAMINGAME(NFLGAME,NFLTEAM), and TEAMFINALSCORE(NFL-GAME, FINALSCORE).Figure 1 shows an example annotated sentence af-ter the above conversion and Table 2 lists the corpussummary statistics for the new binary relations.The purpose behind this corpus is to encouragethe development of systems that answer structuredqueries that go beyond the functionality of informa-tion retrieval engines, e.g.
:?For each NFL game, identify the win-ning and losing teams and each team?s fi-nal score in the game.?
?For each team losing to the Green BayPackers, tell us the losing team and thenumber of points they scored.
?66These queries would be written in a formal language but5 Domain CustomizationTable 3 lists the results of the generic IE system de-scribed in Section 3 on the NFL domain.
Through-out this paper we will report results using ten-foldcross-validation on all 110 documents in the cor-pus.7 We consider an entity mention as correct ifboth its boundaries and label match exactly the goldmention.
We consider a relation mention correct ifboth its arguments and label match the gold relationmention.
For RMD, we report results using the ac-tual mentions predicted by our EMD model (insteadof using gold entity mentions for RMD).
For clar-ity, we do not show in the tables some labels that arehighly uncommon in the data (e.g., SCORETYPE-5appears only four times in the entire corpus); but the?Total?
results include all entity and relation men-tions.Table 3 shows that the stock IE system obtains anare presented here in English for clarity.7Generally, we do not condone reporting results using cross-validation because it may be a recipe for over-fitting on thecorresponding corpus.
However, all our domain customizationideas were developed using outside world and domain knowl-edge and were not tuned on this data, so we believe that there isminimal over-fitting in this case.5Entity Mentions P R F1Date 74.2 81.0 77.5FinalScore 91.3 87.3 89.2NFLGame 61.2 48.3 54.0NFLPlayoffGame 33.3 21.1 25.8NFLTeam 77.9 81.3 79.5ScoreType-1 68.8 62.3 65.4ScoreType-2 72.1 62.0 66.7ScoreType-3 65.4 47.2 54.8ScoreType-4 54.5 42.9 48.0Total 75.6 72.5 74.0Relation Mentions P R F1fieldGoalPartialCount 78.0 31.7 45.1gameDate 91.4 27.8 42.7gameLoser 50.0 18.5 27.1gameWinner 40.0 4.9 8.7teamFinalScore 94.1 40.9 57.1teamInGame 45.9 19.5 27.3teamScoringAll 87.0 64.8 74.3touchDownPartialCount 82.4 49.4 61.7Total 77.6 37.1 50.2Table 4: Performance after gazetteer-based features wereadded to the EMD model.EMD F1 score of 73.7 and a RMD F1 score of 49.7.These are respectable results, in line with state-of-the-art results in other domains.8 However, thereare some obvious areas for improvement.
For exam-ple, the score for a few relations (e.g., GAMELOSERand GAMEWINNER) is quite low.
This is caused bythe fact that these relations are often not explicitlystated in text but rather implied (e.g., based on teamscores).
Furthermore, the low recall of entity typesthat are crucial for all relations (e.g., NFLTEAM andNFLGAME) negatively impacts the overall recall ofRMD.5.1 Combining a Rule-based Model withConditional Random Fields for EMDA straightforward way to improve EMD perfor-mance is to construct domain-specific gazetteers andinclude gazetteer-based features in the model.
Weconstructed a NFL-specific gazetteer as follows: (a)we included all 32 NFL team names; (b) we built alexicon for NFLGame nouns and verbs that includedgame types (e.g., ?semi-final?, ?quarter-final?)
and8As a comparison, the best RMD system in ACE 2007 ob-tained an ACE score of less than 35%, even though the ACEscore gives credit for approximate matches of entity mentionboundaries (Surdeanu and Ciaramita, 2007).Entity Mentions P R F1Date 74.2 81.0 77.5FinalScore 91.3 87.3 89.2NFLGame 61.2 48.3 54.0NFLPlayoffGame 33.3 21.1 25.8NFLTeam 71.4 96.9 82.3ScoreType-1 68.8 62.3 65.4ScoreType-2 72.1 62.0 66.7ScoreType-3 65.4 47.2 54.8ScoreType-4 54.5 42.9 48.0Total 72.8 78.4 75.5Relation Mentions P R F1fieldGoalPartialCount 81.2 38.6 52.3gameDate 93.9 27.0 41.9gameLoser 51.1 19.4 28.1gameWinner 38.9 5.7 9.9teamFinalScore 94.1 40.9 57.1teamInGame 47.4 24.5 32.3teamScoringAll 87.0 68.8 76.9touchDownPartialCount 81.6 56.5 66.8Total 77.2 40.6 53.2Table 5: Performance after gazetteer-based features wereadded to the EMD model, and NFLTeam entity mentionswere extracted using the rule-based model rather thanclassification.typical game descriptors.
The game descriptorswere manually bootstrapped from three seed words(?victory?, ?loss?, ?game?)
using Dekang Lin?sdependency-based thesaurus.9 This process addedother relevant game descriptors such as ?triumph?,?defeat?, etc.
All in all, our gazetteer includes 32team names and 50 game descriptors.
The gazetteerwas built in less than four person hours.We added features to our EMD model to indi-cate if a sequence of words matches a gazetteer en-try, allowing approximate matches (e.g., ?Cowboys?matches ?Dallas Cowboys?).
Table 4 lists the resultsafter this change.
The improvements are modest: 0.3for both EMD and RMD, caused by a 0.8 improve-ment for NFLTEAM.
The score for NFLGAME suf-fers a loss of 1.5 F1 points, probably caused by thefact that our NFLGAME gazetteer is incomplete.These results are somewhat disappointing: eventhough our gazetteer contains an exhaustive list ofNFL team names, the EMD recall for NFLTEAMis still relatively low.
This happens because city9http://webdocs.cs.ualberta.ca/?lindek/Downloads/sim.tgz6names that are not references to team names are rela-tively common in this corpus, and the CRF model fa-vors the generic city name interpretation.
However,since the goal is to answer structured queries overthe extracted relations, we would prefer a modelthat favors recall for EMD, to avoid losing candi-dates for RMD.
While this can be achieved in dif-ferent ways (Minkov et al, 2006), in this paper weimplement a very simple approach: we recognizeNFLTEAM mentions with a rule-based system thatextracts all token sequences that begin, end, or areequal to a known team name.
For example, ?GreenBay?
and ?Packers?
are marked as team mentions,but not ?Bay?.
Note that this approach is prone to in-troducing false positives, e.g., ?Green?
in the aboveexample.
For all other entity types we use the CRFmodel with gazetteer-based features.
Table 5 liststhe results for this model combination.
The tableshows that the RMD performance is improved by 3F1 points.
The F1 score for NFLTEAM mentions isalso improved by 3 points, due to a significant in-crease in recall (from 81% to 97%).Of course, this simple idea works only for en-tity types with low ambiguity.
In fact, it does notimprove results if we apply it to NFLGAME orSCORETYPE-*.
However, low ambiguity entitiesare common in many domains (e.g., medical).
Insuch domains, our approach offers a straightforwardway to address potential recall errors of a machinelearned model.5.2 Improving Head Identification for EntityMentionsTable 1 indicates that most RMD features (e.g., lex-ical information on arguments, dependency pathsbetween arguments) depend on the syntactic headsof entity mentions.
This observation applies toother natural language processing (NLP) tasks aswell, e.g., semantic role labeling or coreference res-olution (Gildea and Jurafsky, 2002; Haghighi andKlein, 2009).
It is thus crucial that syntactic headsof mentions be correctly identified.
Originally weemployed a common heuristic: we first try to find aconstituent with the exact same span as the given en-tity mention in the parse tree of the entire sentence,and extract its head.
If no such constituent exists,we parse only the text corresponding to the mentionand return the head of the generated tree (HaghighiEntity Mentions P R F1Date 69.5 75.9 72.5FinalScore 90.9 88.8 89.8NFLGame 60.5 51.0 55.4NFLPlayoffGame 37.0 26.3 30.8NFLTeam 72.4 98.3 83.4ScoreType-1 69.7 62.1 65.7ScoreType-2 76.9 63.3 69.4ScoreType-3 64.3 50.0 56.3ScoreType-4 72.7 57.1 64.0Total 73.2 79.2 76.1Relation Mentions P R F1fieldGoalPartialCount 81.2 55.4 65.9gameDate 93.9 27.0 41.9gameLoser 51.2 17.7 26.3gameWinner 50.0 8.9 15.2teamFinalScore 96.5 47.4 63.6teamInGame 48.3 33.5 39.5teamScoringAll 86.7 72.9 79.2touchDownPartialCount 89.1 61.2 72.6Total 78.5 45.9 57.9Table 6: Performance with the improved syntactic headidentification rules.and Klein, 2009).
Here we argue that the last step ofthis heuristic is flawed: since most parsers are heav-ily context dependent, they are likely to not parsecorrectly arbitrarily short text fragments.
For exam-ple, the Stanford parser generates the incorrect parsetree:The syntactic head is ?5?
for the mention ?a 5-yardscoring pass?
instead of ?pass.
?10 This problem isexacerbated out of domain, where the parse tree ofthe entire sentence is likely to be incorrect, whichwill often trigger the parsing of the isolated men-tion text.
For example, in the NFL domain, morethan 25% of entity mentions cannot be matched toa constituent in the parse tree of the correspondingsentence.10We tokenize around dashes in this domain because scoresare often dash separated.
However, this mention is incorrectlyparsed even when ?5-yard?
is a single token.7teamFinalScore(G, S) :- teamInGame(T, G), teamScoringAll(T, S).teamFinalScore(G, S) :- gameWinner(T, G), teamScoringAll(T, S).teamFinalScore(G, S) :- gameLoser(T, G), teamScoringAll(T, S).teamInGame(G, T) :- teamScoringAll(T, S), teamFinalScore(G, S).gameWinner(G, T1) :- teamInGame(G, T1), teamInGame(G, T2),teamFinalScore(G, S1), teamFinalScore(G, S2),teamScoringAll(T1, S1), teamScoringAll(T2, S2),greaterThan(S1, S2).gameLoser(G, T1) :- teamInGame(G, T1), teamInGame(G, T2),teamFinalScore(G, S1), teamFinalScore(G, S2),teamScoringAll(T1, S1), teamScoringAll(T2, S2),lessThan(S1, S2).Table 7: Deterministic inference rules for the NFL domain as first-order Horn clauses.
G, T, and S indicate game,team, and score variables.In this work, we propose several simple heuristicsthat improve the parsing of isolated mention texts:?
We append ?It was ?
to the beginning of the textto be parsed.
Since entity mentions are nounphrases (NP), the new text is guaranteed to bea coherent sentence.
A similar heuristic wasused by Moldovan and Rus for the parsing ofWordNet glosses (2001).?
Because dashes are uncommon in the PennTreebank, we remove them from the text beforeparsing.?
We guide the Stanford parser such that the finaltree contains a constituent with the same spanas the mention text.11After implementing these heuristics, the Stanfordparser correctly parses the mention in the above ex-ample as a NP headed by ?pass?.
Table 6 liststhe overall extraction scores after deploying theseheuristics.
The table shows that the RMD F1 scoreis a considerable 4.7 points higher than before thischange (Table 5).5.3 Deterministic Inference for RMDFigure 1 underlines the fact that relations in the NFLdomain are highly inter-dependent.
This is a com-mon occurrence in many extraction tasks and do-mains (Poon and Vanderwende, 2010; Carlson etal., 2010).
The typical way to address these situa-tions is to jointly model these relations, e.g., usingMarkov logic networks (MLN) (Poon and Vander-wende, 2010).
However, this implies a completeredesign of the corresponding IE system, whichwould essentially ignore all the effort behind exist-ing pipeline systems.11This is supported by the parser API.Relation Mentions P R F1fieldGoalPartialCount 81.2 55.4 65.9gameDate 93.9 27.0 41.9gameLoser 45.9 27.4 34.3gameWinner 45.6 25.2 32.5teamFinalScore 96.5 47.4 63.6teamInGame 48.1 44.7 46.4teamScoringAll 86.7 72.9 79.2touchDownPartialCount 89.1 61.2 72.6Total 74.2 49.6 59.5Table 8: Performance after adding deterministic infer-ence.
The EMD scores are not affected by this change,so they are not listed here.In this work, we propose a simple method thatcaptures some of the joint domain structure indepen-dently of the IE architecture and the EMD and RMDmodels.
We add a deterministic inference compo-nent that generates new relation mentions based onthe data already extracted by the pipeline model.
Ta-ble 7 lists the rules of this inference component thatwere developed for the NFL domain.
These rulesare domain-dependent, but they are quite simple: thefirst four rules implement transitive-closure rules forrelation mentions centered around the same NFL-GAME mention; the last two add domain knowledgethat is not captured by the text extractors, e.g., thegame winner is the team with the higher score.
Ta-ble 8, which lists the RMD scores after inference, in-dicates that the inference component is responsiblefor an increase of approximately 2 F1 points, causedby a recall boost of approximately 4%.Table 9 lists the results of a post-hoc experiment,where we removed several relation types from theRMD classifier (the ones predicted with poor perfor-mance) and let the deterministic inference compo-nent generate them instead.
This experiment shows8Without Inference With InferenceP R F1 P R F1Skip gameWinner, gameLoser 78.6 45.6 57.7 75.1 48.4 58.8Skip teamInGame 77.0 43.6 55.7 71.7 49.4 58.5Skip teamInGame, teamFinalScore 74.5 37.1 49.6 70.9 47.6 56.9Skip nothing 78.5 45.9 57.9 74.2 49.6 59.5Table 9: Analysis of different combination strategies between the RMD classifier and inference: the RMD model skipsthe relation types listed in the first column; the inference component generates all relation types.
The other columnsshow relation mention scores under the various configurations.EMD RMDF1 F1Baseline 73.7 49.7+ gazetteer features 74.0 50.2+ rule-based model for NFLTeam 75.5 53.2+ improved head identification 76.1 57.9+ inference 76.1 59.5Table 10: Summary of domain customization results.that inference helps in all configurations, and, mostimportantly, it is robust: even though the RMD scorewithout inference decreases by up to 8 F1 pointsas relations are removed, the score after inferencevaries by less than 3 F1 points (from 56.9 to 59.5F1).
This proves that deterministic inference is ca-pable of generating relation mentions that are eithermissed or cannot be modeled by the RMD classifier.Finally, Table 10 summarizes the experimentspresented in this paper.
It is clear that, despite theirsimplicity, all our proposed ideas help.
All in all,our contributions yielded an improvement of 9.8 F1points (approximately 20% relative) over the stockIE system without these changes.
Our best IE sys-tem was used in a blind evaluation within the Ma-chine Reading project.
In this evaluation, systemswere required to answer 50 queries similar to theexamples in Section 4 and were evaluated on thecorrectness of the individual facts extracted.
Notethat this evaluation is more complex than the exper-iments reported until now, because the correspond-ing IE system requires additional components, e.g.,the normalization of all DATE mentions and eventcoreference (i.e., are two different game mentionsreferring to the same real-world game?).
For thisevaluation, we used an internal script for date nor-malization and we did not implement event corefer-ence.
This system was evaluated at 46.7 F1 (53.7precision and 41.2 recall), a performance that wasapproximately 80% of the F1 score obtained by hu-man annotators.
This further highlights that strongIE performance can be obtained with simple models.6 ConclusionsThis paper introduces a series of simple ideas thatimprove the performance of IE systems when theyare customized to new domains.
We evaluated ourcontributions on a sports domain (NFL game sum-maries) that is significantly different from the do-mains used to develop our IE system or the languageprocessors used by our system.Our analysis revealed several interesting and non-obvious facts.
First, we showed that accurate identi-fication of syntactic heads of entity mentions, whichhas received little attention in IE literature, is cru-cial for good performance.
Second, we showed thata deterministic inference component captures someof the joint domain structure, even when the under-lying system follows a pipeline architecture.
Lastly,we introduced a simple way to tune precision andrecall by combining our entity mention extractorwith a rule-based system.
Overall, our contributionsyielded a 20% improvement in the F1 score for rela-tion mention extraction.We believe that our contributions are model inde-pendent and some, e.g., the better head identifica-tion, even task independent.
Some of our ideas re-quire domain knowledge, but they are all very sim-ple to implement.
We thus expect them to impactother problems as well, e.g., coreference resolution,semantic role labeling.AcknowledgmentsWe thank the reviewers for their detailed comments.This material is based upon work supported by the AirForce Research Laboratory (AFRL) under prime contractno.
FA8750-09-C-0181.
Any opinions, findings, andconclusion or recommendations expressed in this mate-rial are those of the authors and do not necessarily reflectthe view of the Air Force Research Laboratory (AFRL).9ReferencesAndrew Carlson, Justin Betteridge, Richard C. Wang, Es-tevam R. Hruschka Jr., and Tom M. Mitchell.
2010.Coupled semi-supervised learning for information ex-traction.
In Proceedings of the Third ACM Interna-tional Conference on Web Search and Data Mining(WSDM).Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InLREC.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informationinto information extraction systems by gibbs sampling.In Proceedings of the 43nd Annual Meeting of the As-sociation for Computational Linguistics (ACL 2005).Radu Florian, John Pitrelli, Salim Roukos, and Imed Zi-touni.
2010.
Improving mention detection robustnessto noisy input.
In Proc.
of Empirical Methods in Nat-ural Language Processing (EMNLP).Daniel Gildea and Daniel Jurafsky.
2002.
Automatic la-beling of semantic roles.
Computational Linguistics,28(3).Aria Haghighi and Dan Klein.
2009.
Simple coreferenceresolution with rich syntactic and semantic features.In Proc.
of Empirical Methods in Natural LanguageProcessing (EMNLP).Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-nobu Kano, and Jun?ichi Tsujii.
2009.
Overview ofBioNLP?09 shared task on event extraction.
In Pro-ceedings of the Workshop on BioNLP: Shared Task,pages 1?9.
Association for Computational Linguistics.Dan Klein and Christopher D. Manning.
2003.
Accu-rate unlexicalized parsing.
In Proceedings of the 41stMeeting of the Association for Computational Linguis-tics.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional Random Fields: Probabilistic mod-els for segmenting and labeling sequence data.
InProc.
of the International Conference on MachineLearning (ICML).Andrei Mikheev, Marc Moens, and Claire Grover.
1999.Named entity recognition without gazetteers.
InEACL, pages 1?8.Einat Minkov, Richard C. Wang, Anthony Tomasic, andWilliam W. Cohen.
2006.
Ner systems that suit user?spreferences: Adjusting the recall-precision trade-offfor entity extraction.
In Proc.
of HLT/NAACL.M.
Mintz, S. Bills, R. Snow, and D. Jurafsky.
2009.
Dis-tant supervision for relation extraction without labeleddata.
In Proc.
of the Conference of the Association forComputational Linguistics (ACL-IJCNLP).Dan I. Moldovan and Vasile Rus.
2001.
Logic formtransformation of wordnet and its applicability to ques-tion answering.
In Proceedings of the Annual Meetingof the Association for Computational Linguistics.Hoifung Poon and Lucy Vanderwende.
2010.
Joint in-ference for knowledge extraction from biomedical lit-erature.
In Proceedings of the North American Chap-ter of the Association for Computational Linguistics -Human Language Technologies Conference (NAACL-HLT).Lev Ratinov and Dan Roth.
2009.
Design challenges andmisconceptions in named entity recognition.
In Proc.of the Annual Conference on Computational NaturalLanguage Learning (CoNLL).D.
Roth and W. Yih.
2007.
Global inference for entityand relation identification via a linear programmingformulation.
In Introduction to Statistical RelationalLearning.
MIT Press.Jonathan Schuman and Sabine Bergler.
2006.
Postnom-inal prepositional phrase attachment in proteomics.
InProceedings of the HLT-NAACL BioNLP Workshop onLinking Natural Language and Biology, pages 82?89.Association for Computational Linguistics, June.Mihai Surdeanu and Massimiliano Ciaramita.
2007.
Ro-bust information extraction with perceptrons.
In Pro-ceedings of the NIST 2007 Automatic Content Extrac-tion Workshop (ACE07).Mihai Surdeanu, Massimiliano Ciaramita, and HugoZaragoza.
2008.
Learning to rank answers on largeonline qa collections.
In Proceedings of the 46th An-nual Meeting of the Association for ComputationalLinguistics (ACL 2008).Limin Yao, Sebastian Riedel, and Andrew McCallum.2010.
Collective cross-document relation extractionwithout labelled data.
In Proc.
of Empirical Methodsin Natural Language Processing (EMNLP).10
