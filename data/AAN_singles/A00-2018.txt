A Max imum-Ent ropy- Insp i red  Parser  *Eugene CharniakBrown Laboratory for Linguistic Information ProcessingDepartment  of Computer  ScienceBrown University, Box 1910, Providence, RI  02912ec@cs.brown.eduAbstractWe present a new parser for parsing down toPenn tree-bank style parse trees that achieves90.1% average precision/recall for sentences oflength 40 and less, and 89.5% for sentences oflength 100 and less when trMned and tested onthe previously established \[5,9,10,15,17\] "stan-dard" sections of the Wall Street Journal tree-bank.
This represents a 13% decrease in er-ror rate over the best single-parser results onthis corpus \[9\].
The major technical innova-tion is tire use of a "ma~ximum-entropy-inspired"model for conditioning and smoothing that letus successfully to test and combine many differ-ent conditioning events.
We also present somepartial results showing the effects of differentconditioning information, including a surpris-ing 2% improvement due to guessing the lexicalhead's pre-terminal before guessing the lexicalhead.1 IntroductionWe present a new parser for parsing downto Penn tree-bank style parse trees \[16\] thatachieves 90.1~ average precision/recall for sen-tences of length < 40, and 89.5% for sentencesof length < 100, when trained and tested on thepreviously established \[5,9,10,15,17\] "standard"sections of the Wall Street Journal tree-bank.This represents a 13% decrease in error rate overthe best single-parser results on this corpus \[9\].Following \[5,10\], our parser is based upon aprobabilistic generative model.
That is, for allsentences s and MI parses 7r, the parser assigns aprobability p(s, ~) = p(Tr), the equality holdingwhen we restrict consideration to ~r whose yield* This research was supported in part by NSF grantLIS SBR 9720368.
The author would like to thank MarkJohnson and all the rest of the Brown Laboratory forLinguistic Information Processing.is s. Then for any s the parser eturns the parse7r that maximizes this probability.
That is, theparser implements the functionarg.
a= p(  I == arg maxTrp(lr).What fundamentally distinguishes probabilis-tic generative parsers is how they compute p(~r),and it is to that topic we turn next.2 The Generative ModelThe model assigns a probability to a parse bya top-down process of considering each con-stituent c in ~r and for each c first guessing thepre-terminal of c, t(c) (t for "tag"), then thelexical head of c, h(c), and then the expansionof c into further constituents c(c).
Thus theprobability of a parse is given by the equation1"I P(t(c) l l(c),H(c))cE;?.v(h(c) l t(c),l(c),H(c)).p(e(c) i l(c),t(c),h(c),H(c))where l(c) is the label of c (e.g., whether it is anoun phrase (np), verb-phrase, tc.)
and H(c)isthe relevant history of c - -  information outside cthat our probability model deems important indetermining the probability in question.
Muchof the interesting work is determining what goesinto H(c).
Whenever it is clear to which con-stituent we are referring we omit the (c) in, e.g.,h(c).
In this notation the above equation takesthe following form:= 1-\[ v(t I l , z ) .v (h  I t , l ,H) .v(?
IcE;?132Next we describe how we assign a probabilityto the expansion e of a constituent.
In Sec-tion 5 we present some results in which thepossible expansions of a constituent are fixedin advanced by extracting a tree-bank grammar\[3\] from the training corpus.
The method thatgives the best results, however, uses a Markovgrammar - -  a method for assigning probabil-ities to any possible expansion using statisticsgathered from the training corpus \[6,10,15\].
Themethod we use follows that of \[10\].
In thisscheme a traditional probabilistic context-freegrammar (PCFG) rule can be thought of as con-sisting of a left-hand side with a label l(e) drawnfrom the non-terminal symbols of our grammar,and a right-hand side that is a sequence of one ormore such symbols.
(We assume that all termi-nal symbols are generated by rules of the form"preterm -+ word' and we treat these as a spe-cial case.)
For us the non-terminal symbols arethose of the tree-bank, augmented by the sym-bols aux and auxg, which have been assigned e-terministically to certain auxiliary verbs such as"have" or "having".
For each expansion we dis-tinguish one of the right-hand side labels as the"middle" or "head" symbol M(c).
M(c) is theconstituent from which the head lexical item his obtained according to deterministic rules thatpick the head of a constituent from among theheads of its children.
To the left of M is a se-quence of one or more left labels Li (c) includingthe special termination symbol A, which indi-cates that there are no more symbols to the left,and similarly for the labels to the right, Ri(c).Thus an expansion e(c) looks like:1 --~ ALm...L1MRI...RnA.
(2)The expansion is generated by guessing first M,then in order L1 through Lm+t (= A), and sim-ilarly for R1 through R,+~.In a pure Markov PCFG we are given theleft-hand side label l and then probabilisticailygenerate the right-hand side conditioning on noinformation other than I and (possibly) previ-ously generated pieces of the right-hand sideitself.
In the simplest of such models, a zero-order Markov grammar, each label on the right-hand side is generated conditioned only on l - -that is, according to the distributions p(Li I l),p(M l I), and p(Ri l l).More generally, one can condition on the mpreviously generated labels, thereby obtainingan mth-order Markov grammar.
So, for ex-ample, in a second-order Markov PCFG, L2would be conditioned on L1 and M. In ourcomplete model, of course, the probability ofeach label in the expansions i  also conditionedon other material as specified in Equation 1,e.g., p(e I l, t, h, H).
Thus we would use p(L2 IL1, M, l, t, h, H).
Note that the As on both endsof the expansion in Expression 2 are conditionedjust like any other label in the expansion.3 Max imum-Ent ropy- Insp i redParsingThe major problem confronting the author ofa generative parser is what information to useto condition the probabilities required in themodel, and how to smooth the empirically ob-tained probabilities to take the sting out of thesparse data problems that are inevitable witheven the most modest conditioning.
For exam-ple, in a second-order Markov grammar we con-ditioned the L2 label according to the distribu-tion p(L2 I Lt ,M, I , t ,h ,H) .
Also, rememberthat H is a placeholder for any other informa-tion beyond the constituent e that may be usefulin assigning c a probability.In the past few years the maximum entropy,or log-linear, approach as recommended itselfto probabilistic model builders for its flexibilityand its novel approach to smoothing \[1,17\].
Acomplete review of log-linear models is beyondthe scope of this paper.
Rather, we concentrateon the aspects of these models that most di-rectly influenced the model presented here.To compute a probability in a log-linearmodel one first defines a set of "features",functions from the space of configurations overwhich one is trying to compute probabilities tointegers that denote the number of times somepattern occurs in the input.
In our work we as-sume that any feature can occur at most once,so features are boolean-valued: 0 if the patterndoes not occur, 1 if it does.In the parser we further assume that fea-tures are chosen from certain feature schemataand that every feature is a boolean conjunc-tion of sub-features.
For example, in computingthe probability of the head's pre-terminal t wemight want a feature schema f(t ,  l) that returns1 if the observed pre-terminal of c = t and the133label of c = l, and zero otherwise.
This featureis obviously composed of two sub-features, onerecognizing t, the other 1.
If both return 1, thenthe feature returns 1.Now consider computing a conditional prob-ability p(a I H) with a set of features f l .
.
.
f jthat connect a to the history H. In a log-linearmodel the probability function takes the follow-ing form:1 eXl(a,H)fl(a,H)+...+,km(a,H).fm(a,H) p(a I H )  - Z(H)(3)Here the Ai are weights between negative andpositive infinity that indicate the relative impor-tance of a feature: the more relevant he featureto the value of the probability, the higher the ab-solute value of the associated X.
The functionZ(H), called the partition function, is a normal-izing constant (for fixed H), so the probabilitiesover all a sum to one.Now for our purposes it is useful to rewritethis as a sequence of multiplicative functionsgi(a,H) for 0 < i < j :p(a I H)= go(a,H)gl(a,H) .. .gj(a,H).
(4)Here go(a,H) = 1/Z(H) and gi(a,H) =e'~(a'n)f~(a'H).
The intuitive idea is that eachfactor gi is larger than one if the feature in ques-tion makes the probability more likely, one if thefeature has no effect, and smaller than one if itmakes the probability less likely.Maximum-entropy models have two benefitsfor a parser builder.
First, as already implicit inour discussion, factoring the probability compu-tation into a sequence of values correspondingto various 'tfeatures" suggests that the proba-bility model should be easily changeable - - justchange the set of features used.
This point isemphasized by Ratnaparkhi n discussing hisparser \[17\].
Second, and this is a point we havenot yet mentioned, the features used in thesemodels need have no particular independence ofone another.
This is useful if one is using a log-linear model for smoothing.
That is, supposewe want to compute a conditional probabilityp(a \] b,c), but we are not sure that we haveenough examples of the conditioning event b, cin the training corpus to ensure that the empiri-cally obtained probability/~(a \[ b, c) is accurate.The traditional way to handle this is also tocompute/~(a I b), and perhaps iS(a I c) as well,and take some combination of these values asone's best estimate for p(a I b, c).
This methodis known as "deleted interpolation" smoothing.In max-entropy models one can simply includefeatures for all three events fl(a, b, c), f2(a, b),and f3(a, c) and combine them in the model ac-cording to Equation 3, or equivalently, Equation4.
The fact that the features are very far fromindependent is not a concern.Now let us note that we can get an equationof exactly the same form as Equation 4 in thefollowing fashion:p(alb, c)p(alb, c,d)p(alb, c ,d )=p(a lb ) -~a lb )  p(alb, c)(5)Note that the first term of the equation gives aprobability based upon little conditioning infor-mation and that each subsequent term is a num-ber from zero to positive infinity that is greateror smaller than one if the new information be-ing considered makes the probability greater orsmaller than the previous estimate.As it stands, this last equation is pretty muchcontent-free.
But let us look at how it works fora particular case in our parsing scheme.
Con-sider the probability distribution for choosingthe pre-terminal for the head of a constituent.In Equation I we wrote this as p(t I l, H).
Aswe discuss in more detail in Section 5, severaldifferent features in the context surrounding care useful to include in H: the label, headpre-terminal and head of the parent of c (de-noted as lv, tv, hp), the label of c's left sibling(lb for "before"), and the label of the grand-parent of c (la).
That is, we wish to computep(t I l, lv, tv, lb, lg, by).
We can now rewrite thisin the form of Equation 5 as follows:p(t I 1, Iv, tv, lb, IQ, hv) =p(t l t)P(t l t, tv) P(t l t, tv, tv) p(t l t, tp, tv, tb)p(t l l) p(t l l, lp) p(t l t, tp, tp)P(t l t'Iv'tv'Ib'Ig)p(t l t'Ip'tv'Ib'Ig'hP).
(6)p(t I z, t,, t,, lb) p(t I t, l,, t,, lb, t,)Here we have sequentially conditioned onsteadily increasing portions of c's history.
Inmany cases this is clearly warranted.
For ex-ample, it does not seem to make much senseto condition on, say, h v without first condition-ing on tp.
In other cases, however, we seem134to be conditioning on apples and oranges, soto speak.
For example, one can well imaginethat one might want to condition on the par-ent's lexical head without conditioning on theleft sibling, or the grandparent label.
One wayto do this is to modify the simple version shownin Equation 6 to allow this:p(t I l, l., b, h,) =p(t t l)P(t l l, lv) P(t l l, lp, tv) P(t l l, lv, tp, lb)p(t i l ) p(t l l ,lp) p(t l l ,lv,tv)p(t I l, lp, tp, p(t I l, t,,,p(t I l, lp, tp) p(t I l, tp, (7)Note the changes to the last three terms inEquation 7.
Rather than conditioning eachterm on the previous ones, they are now condi-tioned only on those aspects of the history thatseem most relevant.
The hope is that by doingthis we will have less difficulty with the splittingof conditioning events, and thus somewhat lessdifficulty with sparse data.We make one more point on the connec-tion of Equation 7 to a maximum entropy for-mulation.
Suppose we were, in fact, goingto compute a true maximum entropy modelbased upon the features used in Equation 7,f l(t, l),f2(t, l ,  lp),f3(t,l, lv) ....
This requiresfinding the appropriate his for Equation 3,which is accomplished using an algorithm suchas iterative scaling \[11\] in which values for the Aiare initially "guessed" and then modified untilthey converge on stable values.
With no priorknowledge of values for the )q one traditionallystarts with )~i = 0, this being a neutral assump-tion that the feature has neither a positive nornegative impact on the probability in question.With some prior knowledge, non-zero values cangreatly speed up this process because fewer it-erations are required for convergence.
We com-ment on this because in our example we can sub-stantially speed up the process by choosing val-ues picked so that, when the maximum-entropyequation is expressed in the form of Equation4, the gi have as their initial values the valuesof the corresponding terms in Equation 7.
(Ourexperience is that rather than requiring 50 or soiterations, three suffice.)
Now we observe thatif we were to use a maximum-entropy approachbut run iterative scaling zero times, we would,in fact, just have Equation 7.The major advantage of using Equation 7 isthat one can generally get away without com-puting the partition function Z(H).
In the sim-ple (content-free) form (Equation 6), it is clearthat Z(H) = 1.
In the more interesting version,Equation 7, this is not true in general, but onewould not expect it to differ much from one,and we assume that as long as we are not pub-lishing the raw probabilities (as we would bedoing, for example, in publishing perplexity re-sults) the difference from one should be unim-portant.
As partition-function calculation istypically the major on-line computational prob-lem for maximum-entropy models, this simpli-fies the model significantly.Naturally, the distributions required byEquation 7 cannot be used without smooth-ing.
In a pure maximum-entropy model this isdone by feature selection, as in Ratnaparkhi'smaximum-entropy arser \[17\].
While we couldhave smoothed in the same fashion, we chooseinstead to use standard deleted interpolation.
(Actually, we use a minor variant described in\[4\].
)4 The Exper imentWe created a parser based upon the maximum-entropy-inspired model of the last section,smoothed using standard eleted interpolation.As the generative model is top-down and weuse a standard bottom-up best-first probabilis-tic chart parser \[2,7\], we use the chart parser asa first pass to generate candidate possible parsesto be evaluated in the second pass by our prob-abilistic model.
For runs with the generativemodel based upon Markov grammar statistics,the first pass uses the same statistics, but con-ditioned only on standard PCFG information.This allows the second pass to see expansionsnot present in the training corpus.We use the gathered statistics for all observedwords, even those with very low counts, thoughobviously our deleted interpolation smoothinggives less emphasis to observed probabilities forrare words.
We guess the preterminals of wordsthat are not observed in the training data usingstatistics on capitalization, hyphenation, wordendings (the last two letters), and the probabil-ity that a given pre-terminal is realized using apreviously unobserved word.As noted above, the probability model uses135Parser LR LP CB 0CB 2CB< 40 words (2245 sentences)Char97 87.5 87.4 1.00 62.1 86.1Co1199 88.5 88.7 0.92 66.7 87.1Char00 90.1 90.1 0.74 70.1 89.6< 100 words (2416 sentences)Char97 86.7 86.6 1.20 59.9 83.2Coll99 88.1 88.3 1.06 64.0 85.1Ratna99 86.3 87.5Char00 89.6 89.5 0.88 67.6 87.7Figure 1: Parsing results compared with previ-ous workfive smoothed probability distributions, oneeach for L~, M, Ri, t, and h. The equation forthe (unsmoothed) conditional probability distri-bution for t is given in Equation 7.
The otherfour equations can be found in a longer versionof this paper available on the author's website(www.cs.brown.edu/~.,ec).
L and R are condi-tioned on three previous labels so we are usinga third-order Markov grammar.
Also, the labelof the parent constituent Ip is conditioned uponeven when it is not obviously related to the fur-ther conditioning events.
This is due to the im-portance of this factor in parsing, as noted in,e.g., \[14\].In keeping with the standard methodology \[5,9,10,15,17\], we used the Penn Wall Street Jour-nal tree-bank \[16\] with sections 2-21 for train-ing, section 23 for testing, and section 24 fordevelopment (debugging and tuning).Performance on the test corpus is measuredusing the standard measures from \[5,9,10,17\].In particular, we measure labeled precision(LP) and recall (LR), average number of cross-brackets per sentence (CB), percentage of sen-tences with zero cross brackets (0CB), and per-centage of sentences with < 2 cross brackets(2CB).
Again as standard, we take separatemeasurements for all sentences of length <_ 40and all sentences of length < 100.
Note thatthe definitions of labeled precision and recall arethose given in \[9\] and used in all of the previouswork.
As noted in \[5\], these definitions typicallygive results about 0.4% higher than the moreobvious ones.
The results for the new parseras well as for the previous top-three individualparsers on this corpus are given in Figure 1.As is typical, all of the standard measures tellpretty much the same story, with the new parseroutperforming the other three parsers.
Lookingin particular at the precision and recall figures,the new parser's give us a 13% error reductionover the best of the previous work, Co1199 \[9\].5 DiscussionIn the previous sections we have concentratedon the relation of the parser to a maximum-entropy approach, the aspect of the parser thatis most novel.
However, we do not think thisaspect is the sole or even the most importantreason for its comparative success.
Here we listwhat we believe to be the most significant con-tributions and give some experimental resultson how well the program behaves without them.We take as our starting point the parserlabled Char97 in Figure 1 \[5\], as that is theprogram from which our current parser derives.That parser, as stated in Figure 1, achieves anaverage precision/recall of 87.5.
As noted in \[5\],that system is based upon a "tree-bank gram-mar" - -  a grammar ead directly off the train-ing corpus.
This is as opposed to the "Markov-grammar" approach used in the current parser.Also, the earlier parser uses two techniques notemployed in the current parser.
First, it usesa clustering scheme on words to give the sys-tem a "soft" clustering of heads and sub-heads.
(It is "soft" clustering in that a word can be-long to more than one cluster with differentweights - -  the weights express the probabilityof producing the word given that one is goingto produce a word from that cluster.)
Second,Char97 uses unsupervised learning in that theoriginal system was run on about thirty millionwords of unparsed text, the output was takenas "correct", and statistics were collected onthe resulting parses.
Without these enhance-ments Char97 performs at the 86.6% level forsentences of length < 40.In this section we evaluate the effects of thevarious changes we have made by running var-ious versions of our current program.
To avoidrepeated evaluations based upon the testing cor-pus, here our evaluation is based upon sen-tences of length < 40 from the development cor-pus.
We note here that this corpus is somewhatmore difficult than the "official" test corpus.For example, the final version of our system136System Precision RecallOld 86.3 86.1Explicit Pre-Term 88.0 88.1Marked Coordination 88.6 88.7Standard Interpolation 88.2 88.3MaxEnt-Inspired 89.0 89.2First-order Markov 88.6 87.4Second-order Markov 89.5 89.3Best 89.8 89.6Figure 2: Labeled precision/recall for length <40, development corpusachieves an average precision/recall of 90.1% onthe test corpus but an average precision/recallof only 89.7% on the development corpus.
Thisis indicated in Figure 2, where the model la-beled "Best" has precision of 89.8% and recall of89.6% for an average of 89.7%, 0.4% lower thanthe results on the official test corpus.
This is inaccord with our experience that development-corpus results are from 0.3% to 0.5% lower thanthose obtained on the test corpus.The model abeled "Old" attempts to recreatethe Char97 system using the current program.It makes no use of special maximum-entropy-inspired features (though their presence madeit much easier to perform these experiments), itdoes not guess the pre-terminal before guess-ing the lexical head, and it uses a tree-bankgrammar rather than a Markov grammar.
Thisparser achieves an average precision/recall of86.2%.
This is consistent with the average pre-cision/recall of 86.6% for \[5\] mentioned above,as the latter was on the test corpus and the for-mer on the development corpus.Between the Old model and the Best model,Figure 2 gives precision/recall measurements forseveral different versions of our parser.
One ofthe first and without doubt the most signifi-cant change we made in the current parser is tomove from two stages of probabilistic decisionsat each node to three.
As already noted, Char97first guesses the lexical head of a constituentand then, given the head, guesses the PCFGrule used to expand the constituent in question.In contrast, the current parser first guesses thehead's pre~terminal, then the head, and then theexpansion.
It turns out that usefulness of thisprocess had a/ready been discovered by Collins\[10\], who in turn notes (personal communica-tion) that it was previously used by Eisner \[12\].However, Collins in \[10\] does not stress the de-cision to guess the head's pre-terminal first, andit might be lost on the casual reader.
Indeed,it was lost on the present author until he wentback after the fact and found it there.
In Figure2 we show that this one factor improves perfor-mance by nearly 2%.It may not be obvious why this should makeso great a difference, since most words are ef-fectively unambiguous.
(For example, part-of-speech tagging using the most probable pre-terminal for each word is 90% accurate \[8\].)
Webelieve that two factors contribute to this per-formance gain.
The first is simply that if we firstguess the pre~terminal, when we go to guess thehead the first thing we can condition upon isthe pre-terminal, i.e., we compute p(h I t).
Thisquantity is a relatively intuitive one (as, for ex-ample, it is the quantity used in a PCFG to re-late words to their pre-terminals) and it seemsparticularly good to condition upon here sincewe use it, in effect, as the unsmoothed probabil-ity upon which all smoothing of p(h) is based.This one '~fix" makes lightly over a percent dif-ference in the results.The second major reason why first guessingthe pre-terminal makes so much difference isthat it can be used when backing off the lexicalhead in computing the probability of the ruleexpansion.
For example, when we first guessthe lexical head we can move from computingp(r I 1, lp, h) to p(r I l,t, lp, h).
So, e.g., evenif the word "conflating" does not appear in thetraining corpus (and it does not)~ the "ng" end-ing allows our program to guess with relativesecurity that the word has the vbg pre-terminal,and thus the probability of various rule expan-sions can be considerable sharpened.
For exam-ple, the tree-bank PCFG probability of the rule"vp --+ vbg np" is 0.0145, whereas once we con-dition on the fact that the lexical head is a vbgwe get a probability of 0.214.The second modification is the explicit mark-ing of noun and verb-phrase coordination.
Wehave already noted the importance of condition-ing on the parent label l v. So, for example,information about an np is conditioned on theparent - -  e.g., an s, vp, pp, etc.
Note that whenan np is part of an np coordinate structure the137vpaux vpvbd npFigure 3: Verb phrase with both main and aux-iliary verbsparent will itself be an np, and similarly for avp.
But nps and vps can occur with np andvp parents in non-coordinate structures as well.For example, in the Penn Treebank a vp withboth main and auxiliary verbs has the structureshown in Figure 3.
Note that the subordinatevp has a vp parent.Thus np and vp parents of constituents aremarked to indicate if the parents are a coor-dinate structure.
A vp coordinate structureis defined here as a constituent with two ormore vp children, one or more of the con-stituents comma, cc, conjp (conjunctive phrase),and nothing else; coordinate np phrases are de-fined similarly.
Something very much like this isdone in \[15\].
As shown in Figure 2, condition-ing on this information gives a 0.6% improve-ment.
We believe that this is mostly due toimprovements in guessing the sub-constituent'spre-terminai and head.
Given we are alreadyat the 88% level of accuracy, we judge a 0.6%improvement to be very much worth while.Next we add the less obvious conditioningevents noted in our previous discussion of thefinal model - -  grandparent label I a and leftsibling label lb.
When we do so using ourmaximum-entropy-inspired conditioning, we getanother 0.45% improvement in average preci-sion/recall, as indicated in Figure 2 on the linelabeled "MaocEnt-Inspired'.
Note that we alsotried including this information using a stan-dard deleted-interpolation model.
The resultshere are shown in the line "Standard Interpola-tion".
Including this information within a stan-dard deleted-interpolation model causes a 0.6%decrease from the results using the less conven-tional model.
Indeed, the resulting performanceis worse than not using this information at all.Up to this point all the models consideredin this section are tree-bank grammar models.That is, the PCFG grammar ules are read di-rectly off the training corpus.
As already noted,our best model uses a Markov-grammar ap-proach.
As one can see in Figure 2, a first-order Markov grammar (with all the aforemen-tioned improvements) performs slightly worsethan the equivalent tree-bank-grammar parser.However, a second-order grammar does slightlybetter and a third-order grammar does signifi-cantly better than the tree-bank parser.6 Conc lus ionWe have presented a lexicalized Markov gram-mar parsing model that achieves (using the nowstandard training/testing/development sectionsof the Penn treebank) an average preci-sion/recall of 91.1% on sentences of length <40 and 89.5% on sentences of length < 100.This corresponds to an error reduction of 13%over the best previously published single parserresults on this test set, those of Collins \[9\].That the previous three best parsers on thistest \[5,9,17\] all perform within a percentagepoint of each other, despite quite different ba-sic mechanisms, led some researchers to won-der if there might be some maximum level ofparsing performance that could be obtained us-ing the treebank for training, and to conjec-ture that perhaps we were at it.
The resultsreported here disprove this conjecture.
The re-sults of \[13\] achieved by combining the afore-mentioned three-best parsers also suggest hatthe limit on tree-bank trained parsers is muchhigher than previously thought.
Indeed, it maybe that adding this new parser to the mix mayyield still higher results.From our perspective, perhaps the two mostimportant numbers to come out of this re-search are the overall error reduction of 13%over the results in \[9\] and the intermediate-result improvement ofnearly 2% on labeled pre-cision/recall due to the simple idea of guess-ing the bead's pre-terminal before guessing thehead.
Neither of these results were anticipatedat the start of this research.As noted above, the main methodologicalinnovation presented here is our "maximum-entropy-inspired" model for conditioning andsmoothing.
Two aspects of this model deservesome comment.
The first is the slight, but im-portant, improvement achieved by using thismodel over conventional deleted interpolation,as indicated in Figure 2.
We expect that as138we experiment with other, more semantic on-ditioning information, the importance of this as-pect of the model will increase.More important in our eyes, though, isthe flexibility of the maximum-entropy-inspiredmodel.
Though in some respects not quite asflexible as true maximum entropy, it is muchsimpler and, in our estimation, has benefitswhen it comes to smoothing.
Ultimately it isthis flexibility that let us try the various condi-tioning events, to move on to a Markov gram-mar approach, and to try several Markov gram-mars of different orders, without significant pro-gramming.
Indeed, we initiated this line of workin an attempt o create a parser that would beflexible enough to allow modifications for pars-ing down to more semantic levels of detail.
It isto this project hat our future parsing work willbe devoted.References1.
BERGER, A. L., PIETRA, S. A. D. ANDPIETRA, V. J. D. A maximum entropy ap-proach to natural anguage processing.
Com-putational Linguistics 22 1 (1996), 39-71.2.
CARABALLO, S. AND CHARNIAK, E. Newfigures of merit for best-first probabilisticchart parsing.
Computational Linguistics 24(1998), 275-298.3.
CHARNIAK, E. Tree-bank grammars.
InProceedings of the Thirteenth NationalConference on Artificial Intelligence.
AAAIPress/MIT Press, Menlo Park, 1996, 1031-1036.4.
CHARNIAK, E. Expected-frequency interpo-lation.
Department of Computer Science,Brown University, Technical Report CS96-37,1996.5.
CHARNIAK, E. Statistical parsing with acontext-free grammar and word statistics.In Proceedings of the Fourteenth NationalConference on Artificial Intelligence.
AAAIPress/MIT Press, Menlo Park, CA, 1997,598-603.6.
CHARNIAK, E. Statistical techniques fornatural anguage parsing.
AI Magazine 18 4(1997), 33-43.7.
CHARNIAK, E., GOLDWATER, S. AND JOHN-SON, M. Edge-based best-first chart pars-ing.
In Proceedings of the Sixth Workshopon Very Large Corpora.
1998, 127-133.8.
CHARNIAK, E., HENDRICKSON, C., JACOB-SON, N. AND PERKOWITZ, M. Equationsfor part-of-speech tagging.
In Proceedings ofthe Eleventh National Conference on Arti-ficial Intelligence.
AAAI Press/MIT Press,Menlo Park, 1993, 784-789.9.
COLLINS, M. Head-Driven Statistical Mod-els for Natural Language Parsing.
Universityof Pennsylvania, Ph.D. Disseration, 1999.10.
COLLINS, M. J.
Three generative l xicalisedmodels for statistical parsing.
In Proceedingsof the 35th Annual Meeting of the ACL.
1997,16-23.11.
DARROCH, J. N. AND RATCLIFF, D. Gener-alized iterative scaling for log-linear models.Annals of Mathematical Statistics 33 (1972),1470-1480.12.
EISNER~ J. M. An empirical comparison ofprobability models for dependency grammar.Institute for Research in Cognitive Science,University of Pennsylvania, Technical ReportIRCS-96-11, 1996.13.
HENDERSON, J. C. AND BRILL, E. Exploit-ing diversity in natural language process-ing: combining parsers.
In 1999 Joint SigdatConference on Empirical Methods in Natu-red Language Processing and Very Large Cor-pora.
ACL, New Brunswick N J, 1999, 187-194.14.
JOHNSON, M. PCFG models of linguistictree representations.
Computational Linguis-tics 24 4 (1998), 613-632.15.
MAGERMAN, D.M.
Statistical decision-treemodels for parsing.
In Proceedings of the 33rdAnnual Meeting of the Association for Com-putational Linguistics.
1995, 276-283.16.
MARCUS, M. P., SANTORINI, B. ANDMARCINKIEWICZ, M. A.
Building a largeannotated corpus of English: the Penn tree-bank.
Computational Linguistics 19 (1993),313-330.17.
RATNAPARKHI, A.
Learning to parse natu-ral language with maximum entropy models.Machine Learning 341/2/3 (1999), 151-176.139
