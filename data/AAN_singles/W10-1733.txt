Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 216?223,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsIncremental Decoding for Phrase-based Statistical Machine TranslationBaskaran Sankaran, Ajeet Grewal and Anoop SarkarSchool of Computing ScienceSimon Fraser University8888 University DriveBurnaby BC.
V5A 2Y1.
Canada{baskaran, asg10, anoop}@cs.sfu.caAbstractIn this paper we focus on the incrementaldecoding for a statistical phrase-based ma-chine translation system.
In incrementaldecoding, translations are generated incre-mentally for every word typed by a user,instead of waiting for the entire sentenceas input.
We introduce a novel modifi-cation to the beam-search decoding algo-rithm for phrase-based MT to address thisissue, aimed at efficient computation of fu-ture costs and avoiding search errors.
Ourobjective is to do a faster translation dur-ing incremental decoding without signifi-cant reduction in the translation quality.1 IntroductionStatistical Machine Translation has matured sig-nificantly in the past decade and half, resulting inthe proliferation of several web-based and com-mercial translation services.
Most of these ser-vices work on sentence or document level, wherea user enters a sentence or chooses a documentfor translation, which are then translated by theservers.
Translation in such typical scenarios isstill offline in the sense that the user input andtranslation happen sequentially without any inter-action between the two phases.In this paper we study decoding for SMT withthe constraint that translations are to be gener-ated incrementally for every word typed in by theuser.
Such a translation service can be used forlanguage learning, where the user is fluent in thetarget language and experiments with many differ-ent source language sentences interactively, or inreal-time translation environments such as speech-speech translation or translation during interactivechats.We use a phrase-based decoder similar toMoses (Koehn et al, 2007) and propose novelmodifications in the decoding algorithm to tackleincremental decoding.
Our system maintains apartial decoder state at every stage and uses itwhile decoding for each newly added word.
Asthe decoder has access only to the partial sentenceat every stage, the future costs change with ev-ery additional word and this has to be taken intoaccount while continuing from an existing partialdecoder state.
Another major issue is that as incre-mental decoding is provided new input one wordat at time, some of the entries that were pruned outat an earlier decoder state might later turn out tobetter candidates resulting in search errors com-pared to decoding the entire sentence at once.
Itis to be noted that, the search error problem is re-lated to the inability to compute full future costin incremental decoding.
Our proposed modifica-tions address these twin challenges and allow forefficient incremental decoding.2 Incremental Decoding2.1 Beam Search for Phrase-based SMTIn this section we review the usual beam search de-coder for phrase-based MT because we present ourmodifications for incremental decoding using thesame notation.
Beam search decoding for phrase-based SMT (Koehn, 2004) begins by collectingthe translation options from the phrase table for allpossible phrases of a given input sentence and pre-computes the future cost for all possible contigu-ous sequences in the sentence.
The pseudo-codefor the usual beam-search decoding algorithm isillustrated in Algorithm 1.The decoder creates n bins for storing hypothe-ses grouped by the number of source words cov-ered.
Starting from a null hypothesis in bin 0, thedecoder iterates through bins 1 though n fillingthem with new hypotheses by extending the en-tries in the earlier bins.A hypothesis contains the target words gener-ated (e), the source positions translated so far (f )commonly known as coverage set and the scoreof the current translation (p) computed by theweighted log-linear combination of different fea-ture functions.
It also contains a back-pointer to216Algorithm 1 Phrase-based Decoder pseudocode(Koehn, 2004)1: Given: sentence Sn: s1s2...sn of length n2: Pre-compute future costs for all contiguoussequences3: Initialize bins bi where i = 1 .
.
.
n4: Create initial hypothesis: {e : (), f : (), p :1.0}5: for i = 1 to n do6: for hyp ?
bi do7: for newHyp that extends hyp do8: nf := num src words covered bynewHyp9: Add newHyp to bin bnf10: Prune bin bnf using future costs11: Find best hypothesis in bn12: Output best path that leads to best hypothesisits parent hypothesis in the previous state and otherinformation used for pruning and computing costin later iterations.As a new hypothesis is generated by extendingan existing hypothesis with a new phrase pair, de-coder updates the associated information such ascoverage set, the target words generated, futurecost (for translating rest of the source words) andits translation score.
For example, consider Span-ish to English translation: for the source sentenceMaria no daba una bofetada, the hypothesis {e :(Mary), f : (1), p : 0.534} which is the hypoth-esis that covers Maria can be extended to a newhypothesis {e : (Mary, slap), f : (1, 3, 4, 5), p :0.043} by choosing a new phrase pair (daba unabofetada, slap) covering the source phrases Mariaand daba una bofetada.
The probability score isobtained by weighted log-linear sum of the fea-tures of the phrases contained in the derivation sofar.An important aspect of beam search decodingis the pruning away of low-scoring hypotheses ineach bin to reduce the search space and thus mak-ing the decoding faster.
To do this effectively,beam search decoding uses the future cost of a hy-pothesis together with its current cost.
The futurecost is an estimate of the translation cost of theinput words that are yet to be translated, and istypically pre-computed for all possible contiguoussequences in the input sentence before the decod-ing step.
The future cost prevents the any hypothe-ses that are low-scoring, but potentially promising,from being pruned.2.2 Incremental Decoder - ChallengesOur goal for the incremental decoder (ID) is togenerate output translations incrementally for par-tial phrases as the source sentence is being inputby the user.
We assume white-space to be the worddelimiter and the partial sentence is decoded forevery encounter of the space character.
We furtherassume the return key to mark end-of-sentence(EOS) and use it to compute language model scorefor the entire sentence.As we noted above, future costs cannot be pre-computed as in regular decoding because the com-plete input sentence is not known while decod-ing incrementally.
Thus the incremental decodercan only use a partial future cost until the EOSis reached.
The partial future cost could resultin some of the potentially better candidates beingpruned away in earlier stages.
This leads to searcherrors and result in lower translation quality.2.3 ApproachWe use a modified beam search for incrementaldecoding (ID) and the two key modifications areaimed at addressing the issues of future cost andsearch errors.
Beam search for ID begins witha single bin for the first word and more bins areadded as the sentence is completed by the user.Our approach requires that the decoder states forthe partial source sentence can be stored in a waythat allows efficient retrieval.
It also maintains acurrent decoder state, which includes all the binsand the hypotheses contained in them, all pertain-ing to the present sentence.At each step ID goes through a pre-processphase, where it recomputes the partial future costsfor all the spans accounting for the new word andupdates the current decoder state with new partialfuture costs.
It then generates new hypotheses intoall the earlier bins and in the newly created us-ing any new phrases (resulting from the new wordadded by the user) not used earlier.Algorithm 2 shows the pseudocode of our incre-mental decoder.
Given a partial sentence Si1 IDstarts with the pre-process phase illustrated sepa-rately in algorithm 3.
We use Ptype(l) to denotephrases of length l words and Htype to denote theset of hypotheses; in both cases type correspond toeither old or new, indicating if it was not known inthe previous decoding state or not.1we use Si and si to denote a i word partial sentence andith word in a (partial) sentence respectively217Algorithm 2 Incremental Decoder pseudocode1: Input: (partial) sentence Sp: s1s2...si?1siwith ls words where si is the new word2: PreProcess(Sp) (Algorithm 3)3: for every bin bj in (1 .
.
.
i) do4: Update future cost and cover set ?
Hold5: Add any new phrase of length bj (subject tod)6: for bin bk in (bj?MaxPhrLen .
.
.
bj?1) do7: Generate Hnew for bj by extending:8: every Hold with every other Pnew(bj ?bk)9: every Hnew with every other Pany(bj ?bk)10: Prune bin bjAlgorithm 3 PreProcess subroutine1: Input: partial sentence Sp of length ls2: Retrieve partial decoder object for Sp?13: Identify possible Pnew (subject to Max-PhrLen)4: Recompute fc for all spans in 1...ls5: for every Pnew in local phrase table do6: Load translation options to table7: for every Pold in local phrase table do8: Update fc with the recomputed costGiven Si, the pre-process phase extracts the newset of phrases (Pnew) for the ith word and addsthem to the existing phrases (Pold).
It then recom-putes the future-cost (fc) for all the contiguous se-quences in the partial input and updates existingentries in the local copy of phrase table with newfc.In decoding phase, ID generates new hypothe-ses in two ways: i) by extending the existing hy-potheses Hold in the previous decoder state Si?1with new phrases Pnew and ii) by generating newhypotheses Hnew that are unknown in the previousstate.The main difference between incremental de-coding and regular beam-search decoding is insidethe two ?for?
loops corresponding to lines 3?
9 inalgorithm 2.
In the outer loop each of the existinghypotheses are updated to reflect the recomputedfc and coverage set.
Any new phrases belongingto the current bin are also added to it2.2Based on our implementation of lazier cube pruning theyare added to a priority queue, the contents of which areflushed into the bin at the end of inner for-loop and beforethe pruning stepHypothesis surfacesP.
QueueHypstackhyp 2hyp 1A single surface                         Figure 1: Illustration of Lazier Cube PruningThe inner for-loop corresponds to the extensionof hypotheses sets (grouped by same coverage set)to generate new hypotheses.
Here a distinction ismade between hypotheses Hold corresponding toprevious decoder state Sp?1 and hypotheses Hnewresulting from the addition of word si.
Hold is ex-tended only using the newly found phrases Pnew,whereas the newer hypotheses are processed as inregular beam-search.2.4 Lazier Cube PruningWe have adapted the pervasive lazy algorithm(or ?lazier cube pruning?)
proposed originally forHiero-style systems by (Pust and Knight, 2009)for our phrase-based system.
This step corre-sponds to the lines 5?9 of algorithm 2 and allowsus to only generate as many hypotheses as speci-fied by the configurable parameters, beam size andbeam threshold.
Figure 1 illustrates the process oflazier cube pruning for a single bin.At the highest level it uses a priority queue,which is populated by the different hyper-edgesor surfaces3, each corresponding to a pair of hy-potheses that are being merged to create a newhypothesis.
New hypotheses are generated iter-atively, such that the hypothesis with the highestscore is chosen in each iteration from among dif-ferent hyper-edges bundles.However, this will lead to search errors as havebeen observed earlier.
Any hyper-edge that hasbeen discarded due to poor score in an early stagemight later become a better candidate.
The prob-lem worsens further when using smaller beamsizes (for interactive decoding in real-time set-tings, we even consider a beam size of 3).
In3Unlike Hiero-style systems, only two hypotheses aremerged in a phrase-based system and hence the term surface218the next section, we introduce the idea of delayedpruning to reduce search errors.3 Delayed PruningDelayed pruning (DP) in our decoder was inspiredby the well known fable about the race betweena tortoise and a hare.
If the decoding is consid-ered to be a race between competing candidate hy-potheses with the winner being the best hypothe-sis for Viterbi decoding or among the top-n candi-dates for n-best decoding.4In this analogy, a hypothesis having a poorscore, might just be a tortoise having a slow start(due to a bad estimate of the true future cost forwhat the user intends to type in the future) as op-posed to a high scoring hare in the same state.Pruning such hypotheses early on is not risk-freeand might result in search errors.
We hypothe-size that, given enough chance it might improve itsscore and move ahead of a hare in terms of trans-lation score.We implement DP by relaxing the lazier cubepruning step to generate a small, fixed numberof hypotheses for coverage sets that are not rep-resented in the priority queue and place them inthe bin.
These hypotheses are distinct from theusual top-k derivations.
Thus, the resulting binwill have entries from all possible hyper-edge bun-dles.
Though this reduces the search error prob-lem, it leads to increasing number of possibilitiesto be explored at later stages with vast majorityof them being worse hypotheses that should bepruned away.We use a two level strategy of delay and thenprune, to avoid such exponentially increasingsearch space and at the same time to reduce searcherror.
At the delay level, the idea is to delay thepruning for few promising tortoises, instead of re-taining a fixed number of hypotheses from all un-represented hyper-edges.
We use the normalizedlanguage model scores of the top-hypotheses ineach hyper-edge that is not represented in cubepruning and based on a threshold (which is ob-tained using a development test set), we selec-tively choose few hyper-edge bundles and gen-erate a small number (typically 1-3) of hypothe-ses from each of them and flag them as tortoises.4The analogy is used to compare two or more hypothesesin terms of their translation scores and not speed.
Though ourobjective is faster incremental decoding, we use the analogyhere to compare the scores.These tortoises are extended minimally at each it-eration subject to their normalized LM score.While this significantly reduces the total num-ber of hypotheses at initial bins, many of thesetortoises might not show improvement even afterseveral bins.
Thus at the prune level, we prune outtortoises that does not improve beyond a thresholdnumber of bins called race course limit.
The racecourse limit signifies the number of steps a tortoisehas in order to get into the decoder beam.When a tortoise improves in score and breaksinto the beam during cube pruning, it is de-flagged as a tortoise and enters the regular decod-ing stream.
We found DP to be effective in reduc-ing the search error for incremental decoder in ourexperiments.4 Evaluation and DiscussionThe evaluation was performed using our own im-plementation of the beam-search decoding algo-rithms.
The architecture of our system is similarto Moses, which we also use for training and forminimum error rate training (MERT) of the log-linear model for translation (Och, 2003; Koehn etal., 2007).
Our features include 7 standard phrase-based features: 4 translation model features, i.e.p(f |e), p(e|f), plex(f |e) and plex(e|f), where eand f are target and source phrases respectively;features for phrase penalty, word penalty and lan-guage model, and we do not include the reorder-ing feature.
We used Giza++ and Moses respec-tively for aligning the sentences and training thesystem.
The decoder was written in Java and in-cludes cube pruning (Huang and Chiang, 2007)and lazier cube pruning (Pust and Knight, 2009)functionalities as part of the decoder.
Our de-coder supports both regular beam search (similarto Moses) and incremental decoding.In our experiments we experimented various ap-proaches for storing partial decoder states includ-ing memcache and transactional persistence usingJDBM but found that the serialization and deseri-alization of decoder objects directly into and fromthe memory to work better in terms of speed andmemory requirements.
The partial object is re-trieved and deserialized from the memory whenrequired by the incremental decoder.We evaluated the incremental decoder for trans-lations between French and English (in both direc-tions).
We used the Workshop on Machine Trans-lation shared task (WMT07) dataset for training,219optimizing and testing.
The system was trained us-ing Moses and the feature weights were optimizedusing MERT.
To benchmark our Java decoder, wecompare it with Moses by running it in regularbeam search mode.
The Moses systems were alsooptimized separately on the WMT07 devsets.Apart from comparing our decoder with Mosesin regular beam search, we also compared the in-cremental decoding with regular regular beam us-ing our decoder.
To make it comparable withincremental decoding, we used the regular beamsearch to re-decode the sentence fragments for ev-ery additional word in the input sentence.
Wemeasured the following parameters in our empir-ical analysis: translation quality (as measured byBLEU (Papineni et al, 2002) and TER (Snover etal., 2006)), search errors and translation speed.
Fi-nally, we also measured the effect of different racecourse limits on BLEU and decoding speed for in-cremental decoding.4.1 Benchmarking our decoderIn this section we compare our decoder withMoses for regular beam search decoding.
Table 1gives the BLEU and TER for the two languagepairs.
Our decoder implementation comparesfavourably with Moses for Fr-En: the slightly bet-ter BLEU and TER for our decoder in Fr-En ispossibly due to the minor differences in the con-figuration settings.
For En-Fr translation, Mosesperforms better in both metrics.
There are differ-ences in the beam size between the two decoders,in our system the beam size is set to 100 comparedto the default value of 1000 (the cube pruning poplimit) in Moses; we are planning to explore thisand remove any other differences between them.However based on our understanding of the Mosesimplementation and our experiments, we believeour decoder to be comparable in accuracy with theMoses implementation.
The numbers in the bold-face are statistically significant at 95% confidenceinterval.4.2 Re-decoding v.s.
Incremental decodingWe test our hypothesis that incremental decod-ing can benefit by using partial decoder states fordecoding every additional word in the input sen-tence.
In order to do this, we run our incremen-tal decoder in both regular beam search mode andin incremental decoding mode.
In regular beamsearch mode, we forced the beam search decoderto re-decode the sentence fragments for every ad-ditional word and in incremental decoding mode,we used the partial decoding states to incremen-tally decode lastly added word.
We then comparethe BLEU and TER scores between them to vali-date our hypothesis.We further test effectiveness of delayed prun-ing (DP) in incremental decoding by comparingit to the case where we turn off the DP.
For in-cremental decoding, we set the beam size and therace course limit (for DP) to be 3.
Additionally,we used a threshold of?2.0 (in log-scale) for nor-malized LM in the delay phase of DP, which wasobtained by testing on a separate development testset.We would like to highlight two observationsfrom the results in Table 2.
First the regular beamsearch indicate possible search errors due to thesmall beam size (cube pruning pop limit) and theBLEU scores has decreased by 0.56 for Fr-Enand by over 2.5 for En-Fr, than the scores cor-responding to a beam size of 100 shown in Ta-ble 1.
Secondly, we find the incremental decodingto perform better for the same beam size.
How-ever, incremental decoding without delay pruningstill seems to incur search errors when comparedwith the regular decoding with a larger beam.
De-layed pruning alleviates this issue and improvesthe BLEU and TER significantly.
This we believe,is mainly because the strategy to delay the pruningretains the potentially better partial hypotheses forevery coverage set.
It should be noted that resultsin Table 2 pertain only to our decoder implemen-tation and not with Moses.We now give a comparative note between ourapproach and the pruning strategy in regular beamsearch.
Delaying the hypothesis pruning is the im-portant aspect in our approach to incremental de-coding.
In the case of regular beam search, thehypotheses are pruned when they fall out of thebeam and the idea is to have a larger beam sizeto avoid the early pruning of potentially good can-didates.
With the advent of cube pruning (Huangand Chiang, 2007), the ?cube pruning pop limit?
(in Moses) determines the number of hypothesesretained in each stack.
In both the cases, it is pos-sible that some of the coverage sets go unrepre-sented in the stack due to poor candidate scores.This is not desirable in the incremental decodingsetting as this might lead to search errors whiledecoding a partial sentence.Additionally, Moses offers an option (cube220DecoderFr-En En-FrBLEU TER BLEU TERMoses 26.98 0.551 27.24 0.610Our decoder 27.53 0.541 26.96 0.657Table 1: Regular beam search: Moses v.s.
Our decoderDecoderFr-En En-FrBLEU TER BLEU TERRe-decode w/ beam search 26.96 0.548 24.33 0.635ID w/o delay pruning 27.01 0.547 25.00 0.618ID w/ delay pruning 27.62 0.545 25.45 0.616Table 2: BLEU and TER: Re-decoding v.s.
Incremental Decoding (ID)pruning diversity) to control the number of hy-potheses generated for each coverage set (thoughset to ?0?
by default).
It might be possible to usethis in conjunction with cube pruning pop limit asan alternative to our delayed pruning in the incre-mental decoding setting (with the risk of combina-torial explosion in the search space).In contrast, the delayed pruning not only avoidssearch errors but also provides a dynamically man-ageable search space (refer section 4.2.2) by re-taining the best of the potential candidates.
In apractical scenario like real-time translation of in-ternet chat, translation speed is an important con-sideration.
Furthermore, it is better to avoid largenumber of candidates and generate only few bestones, as only the top few translations will be usedby the system.
Thus we believe our delayed prun-ing approach to be a principled pruning strategythat combines the different factors in an elegantframework.4.2.1 Search ErrorsAs BLEU only indirectly indicates the numberof search errors made by algorithm, we used amore direct way of quantifying the search errorsincurred by the ID in comparison to regular beamsearch.
We define the search error to be the differ-ence between the translation scores of the best hy-potheses produced by the ID and the regular beamsearch and then compute the mean squared error(MSE) for the entire test set.
We use this methodto compare ID in the two settings of delayed prun-ing being turned off (using a smaller beam sizeof 3 to simulate the requirements of near instanta-neous translations in real-time environments) anddelayed pruning turned on.
We compare the modelscore in these cases with the model score for thebest result obtained from the regular beam searchdecoder (using a larger beam of size 100).DirectionBeam search againstIncremental Decodingw/o DP w/ DPFr-En 0.3823 0.3235En-Fr 1.1559 0.6755Table 3: Search Errors in Incremental DecodingThe results are shown in Table 3 and as can beclearly seen, ID shows much lesser mean squareerror with the DP turned on than when it is turnedoff.
Together the BLEU and TER numbers andthe mean square search error show that delayedpruning is useful in the incremental decoding set-ting.
Comparing the En-Fr and Fr-En results showthat the two language pairs show slightly differentcharacteristics but the experiments in both direc-tions support our overall conclusions.4.2.2 SpeedIn this experiment, we set out to evaluate theID against the regular beam-search in which sen-tence fragments are incrementally decoded for ad-ditional words.
In order compare with the in-cremental decoder, we modified the regular de-coder to decode the partial phrases, so that it re-decodes the partial phrase from the scratch insteadof reusing the earlier state.We ran the timing experiments on a Dell ma-chine with an Intel Core i7 processor and 12 GBmemory, clocking 2.67 GHz and running Linux(CentOS 5.3).
We measured the time taken for de-coding the fragment with every word added and221averaged it first over the sentence and then the en-tire test set.
The average time (in msecs) includesthe future cost computation for both.
We also mea-sured the average number of hypotheses for everybin at the end of decoding a complete sentence,which was also averaged over the test set.The results in Table 4 show that the incremen-tal decoder was significantly faster than the beamsearch in re-decoding mode almost by a factor of9 in the best case (for Fr-En).
The speedup is pri-marily due to two factors, i) computing the futurecost for the new phrases as opposed to computingit for all the phrases and ii) using partial decoderstates without having to re-generate hypothesesthrough the cube pruning step and the latenciesassociated with computing LM scores for them.The addition of delayed pruning slowed down thespeed at most by 7 msecs (for En-Fr).
In addition,delayed pruning can be seen generating far morehypotheses than the other two cases.
Clearly, thisis because of the delay in pruning the tortoises un-til the race course limit.
Even with such signifi-cantly large number of hypotheses being retainedfor every bin, DP results in improved speed (overre-decoding from scratch) and better performanceby avoiding search errors (compared to the incre-mental decoder that does not use DP).4.3 Effect of Race course limitTable 5 shows the effect of different race courselimits on translation quality measured usingBLEU.
We generally expect the race course limitto behave similar to the beam size as they both al-low more hypotheses in the bin thereby reducingsearch error although at the expense of increasingdecoding time.However, in our experiments for Fr-En, we didnot find significant variations in BLEU for differ-ent race course limits.
This could be due to theabsence of long distance re-orderings between En-glish and French and that the smallest race courselimit of 3 is sufficient for capturing all cases of lo-cal re-ordering.
As expected, we find the decodingspeed to slightly decrease and the average numberof hypotheses per bin to increase with the increas-ing race course limit.5 Related WorkGoogle5 does seem to perform incremental decod-ing, but the underlying algorithms are not public5translate.google.comknowledge.
They may be simply re-translating theinput each time using a fast decoder or re-usingprior decoder states as we do here.Intereactive translation using text predictionstrategies have been studied well (Foster et al,1997; Foster et al, 2002; Och et al, 2003).
Theyall attempt to interactively help the human user inthe postediting process, by suggesting completionof the word/phrase based on the user accepted pre-fix and the source sentece.
Incremental feedbackis part of Caitra (Koehn, 2009) an interactive toolfor human-aided MT and works on a similar set-ting to interactive MT.
In Caitra, the source textis pre-translated first and during the interactions itdynamically generates user suggestions.Our incremental decoder work differs fromthese text prediction based approaches, in thesense that the input text is not available to the de-coder beforehand and the decoding is being donedynamically for every source word as opposed togenerating suggestions dynamically for complet-ing target sentece.6 Conclusion and Future WorkWe presented a modified beam search algorithmfor an efficient incremental decoder (ID), whichwill allow translations to be generated incremen-tally for every word typed by a user, instead ofwaiting for the entire sentence as input by reusingthe partial decoder state.
Our proposed modifica-tions help us to efficiently compute partial futurecosts in the incremental setting.
We introduced thenotion of delayed pruning (DP) to avoid searcherrors in incremental decoding.
We showed thatreusing the partial decoder states is faster than re-decoding the input from the scratch every time anew word is typed by the user.
Our exhaustive ex-periments further demonstrated DP to be highlyeffective in avoiding search errors under the in-cremental decoding setting.
In our experiments inthis paper we used a very tight beam size; in fu-ture work, we would like to explore the tradeoffbetween speed, accuracy and the utility of delayedpruning by varying the beam size in our experi-ments.ReferencesGeorge Foster, Pierre Isabelle, and Pierre Plamondon.1997.
Target-text mediated interactive machinetranslation.
Machine Translation, 12(1/2):175?194.222DecoderFr-En En-FrAvg time Avg Hyp/ bin Avg time Avg Hyp/ binRe-decode 724.46 2.21 130.29 2.32ID w/o DP 84.85 2.89 27.58 2.89ID w/ DP 87.01 85.11 34.35 60.46Table 4: Speed: Re-decoding v.s.
Incremental Decoding (ID)Race Fr-En En-FrCourseBLEU Avg time Avg Hyp/ bin BLEU Avg time Avg Hyp/ binLimit3 26.75 87.83 85.11 25.39 36.15 75.034 26.77 91.14 86.35 25.37 36.21 77.695 26.77 90.81 86.52 25.37 36.25 78.476 26.77 95.91 86.56 25.37 37.34 78.717 26.77 91.67 86.57 25.37 36.26 78.81Table 5: Effect of different race course limitsGeorge Foster, Philippe Langlais, and Guy Lapalme.2002.
User-friendly text prediction for translators.In EMNLP ?02: Proceedings of the ACL-02 con-ference on Empirical methods in natural languageprocessing, pages 148?155, Morristown, NJ, USA.Association for Computational Linguistics.Liang Huang and David Chiang.
2007.
Forest rescor-ing: Faster decoding with integrated language mod-els.
In Proceedings of the 45th Annual Meeting ofthe Association of Computational Linguistics, pages144?151, Prague, Czech Republic, June.
Associa-tion for Computational Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-dra Constantin, and Evan Herbst.
2007.
Moses:Open source toolkit for statistical machine transla-tion.
In Proceedings of the 45th Annual Meeting ofthe Association for Computational Linguistics Com-panion Volume Proceedings of the Demo and PosterSessions, pages 177?180, Prague, Czech Republic,June.
Association for Computational Linguistics.Philipp Koehn.
2004.
Pharaoh: A beam searchdecoder for phrase-based statistical machine trans-lation models.
In Robert E. Frederking andKathryn Taylor, editors, AMTA, volume 3265 of Lec-ture Notes in Computer Science, pages 115?124.Springer.Philipp Koehn.
2009.
A web-based interactive com-puter aided translation tool.
In In Proceedings ofACL-IJCNLP 2009: Software Demonstrations, Sun-tec, Singapore, August.Franz Josef Och, Richard Zens, and Hermann Ney.2003.
Efficient search for interactive statistical ma-chine translation.
In EACL ?03: Proceedings of thetenth conference on European chapter of the Asso-ciation for Computational Linguistics, pages 387?393, Morristown, NJ, USA.
Association for Compu-tational Linguistics.Franz Josef Och.
2003.
Minimum error rate train-ing in statistical machine translation.
In Proceed-ings of the 41st Annual Meeting of the Associationfor Computational Linguistics, pages 160?167, Sap-poro, Japan, July.
Association for ComputationalLinguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wie-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proc.
ACL.Michael Pust and Kevin Knight.
2009.
Faster mtdecoding through pervasive laziness.
In Proceed-ings of Human Language Technologies: The 2009Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,Companion Volume: Short Papers, pages 141?144,Boulder, Colorado, June.
Association for Computa-tional Linguistics.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of Association for Machine Transla-tion in the Americas: AMTA 2006.223
