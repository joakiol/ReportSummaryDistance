Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 856?865,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsStructured prediction models for RNN based sequence labeling in clinicaltextAbhyuday N Jagannatha1, Hong Yu1,21 University of Massachusetts, MA, USA2 Bedford VAMC and CHOIR, MA, USAabhyuday@cs.umass.edu , hong.yu@umassmed.eduAbstractSequence labeling is a widely used methodfor named entity recognition and informationextraction from unstructured natural languagedata.
In the clinical domain one major ap-plication of sequence labeling involves ex-traction of relevant entities such as medica-tion, indication, and side-effects from Elec-tronic Health Record Narratives.
Sequence la-beling in this domain presents its own set ofchallenges and objectives.
In this work weexperiment with Conditional Random Fieldbased structured learning models with Recur-rent Neural Networks.
We extend the pre-viously studied CRF-LSTM model with ex-plicit modeling of pairwise potentials.
We alsopropose an approximate version of skip-chainCRF inference with RNN potentials.
We usethese methods1 for structured prediction in or-der to improve the exact phrase detection ofclinical entities.1 IntroductionPatient data collected by hospitals falls into two cat-egories, structured data and unstructured natural lan-guage texts.
It has been shown that natural textclinical documents such as discharge summaries,progress notes, etc are rich sources of medically rel-evant information like adverse drug events, medica-tion prescriptions, diagnosis information etc.
Infor-mation extracted from these natural text documentscan be useful for a multitude of purposes ranging1Code is available at https://github.com/abhyudaynj/LSTM-CRF-modelsfrom drug efficacy analysis to adverse effect surveil-lance.A widely used method for Information Extrac-tion from natural text documents involves treatingthe text as a sequence of tokens.
This format al-lows sequence labeling algorithms to label the rel-evant information that should be extracted.
Sev-eral sequence labeling algorithms such as Condi-tional Random Fields (CRFs), Hidden Markov Mod-els (HMMs), Neural Networks have been used forinformation extraction from unstructured text.
CRFsand HMMs are probabilistic graphical models thathave a rich history of Natural Language Process-ing (NLP) related applications.
These methods tryto jointly infer the most likely label sequence for agiven sentence.Recently, Recurrent (RNN) or ConvolutionalNeural Network (CNN) models have increasinglybeen used for various NLP related tasks.
These Neu-ral Networks by themselves however, do not treatsequence labeling as a structured prediction prob-lem.
Different Neural Network models use dif-ferent methods to synthesize a context vector foreach word.
This context vector contains informa-tion about the current word and its neighboring con-tent.
In the case of CNN, the neighbors compriseof words in the same filter size window, while inBidirectional-RNNs (Bi-RNN) they contain the en-tire sentence.Graphical models and Neural Networks have theirown strengths and weaknesses.
While graphicalmodels predict the entire label sequence jointly, theyusually rely on special hand crafted features to pro-vide good results.
Neural Networks (especially Re-856current Neural Networks), on the other hand, havebeen shown to be extremely good at identifying pat-terns from noisy text data, but they still predict eachword label in isolation and not as a part of a se-quence.
In simpler terms, RNNs benefit from rec-ognizing patterns in the surrounding input features,while structured learning models like CRF benefitfrom the knowledge about neighboring label predic-tions.
Recent work on Named Entity Recognitionby (Huang et al, 2015) and others have combinedthe benefits of Neural Networks(NN) with CRF bymodeling the unary potential functions of a CRF asNN models.
They model the pairwise potentials asa paramater matrix [A] where the entry Ai,j corre-sponds to the transition probability from the label ito label j.
Incorporating CRF inference in NeuralNetwork models helps in labeling exact boundariesof various named entities by enforcing pairwise con-straints.This work focuses on labeling clinical events(medication, indication, and adverse drug events)and event related attributes (medication dosage,route, etc) in unstructured clinical notes from Elec-tronic Health Records.
Later on in the Section 4,we explicitly define the clinical events and attributesthat we evaluate on.
In the interest of brevity, for therest of the paper, we use the broad term ?Clinical En-tities?
to refer to all medically relevant informationthat we are interested in labeling.Detecting entities in clinical documents such asElectronic Health Record notes composed by hospi-tal staff presents a somewhat different set of chal-lenges than similar sequence labeling applicationsin the open domain.
This difference is partly dueto the critical nature of medical domain, and partlydue to the nature of clinical texts and entities therein.Firstly, in the medical domain, extraction of exactclinical phrase is extremely important.
The namesof clinical entities often follow polynomial nomen-clature.
Disease names such as Uveal melanomaor hairy cell leukemia need to be identified exactly,since partial names ( hairy cell or melanoma) mighthave significantly different meanings.
Addition-ally, important clinical entities can be relatively rareevents in Electronic Health Records.
For example,mentions of Adverse Drug Events occur once ev-ery six hundred words in our corpus.
CRF inferencewith NN models cited previously do improve exactphrase labeling.
However, better ways of modelingthe pairwise potential functions of CRFs might leadto improvements in labeling rare entities and detect-ing exact phrase boundaries.Another important challenge in this domain is aneed to model long term label dependencies.
For ex-ample, in the sentence ?the patient exhibited A sec-ondary to B?, the label for A is strongly related tothe label prediction of B.
A can either be labeled asan adverse drug reaction or a symptom if B is a Med-ication or Diagnosis respectively.
Traditional linearchain CRF approaches that only enforce local pair-wise constraints might not be able to model thesedependencies.
It can be argued that RNNs may im-plicitly model label dependencies through patternsin input features of neighboring words.
While this istrue, explicitly modeling the long term label depen-dencies can be expected to perform better.In this work, we explore various methods of struc-tured learning using RNN based feature extractors.We use LSTM as our RNN model.
Specifically,we model the CRF pairwise potentials using NeuralNetworks.
We also model an approximate version ofskip chain CRF to capture the aforementioned longterm label dependencies.
We compare the proposedmodels with two baselines.
The first baseline is astandard Bi-LSTM model with softmax output.
Thesecond baseline is a CRF model using handcraftedfeature vectors.
We show that our frameworks im-prove the performance when compared to the base-lines or previously used CRF-LSTM models.
Tothe best of our knowledge, this is the only work fo-cused on usage and analysis of RNN based struc-tured learning techniques on extraction of clinicalentities from EHR notes.2 Related WorkAs mentioned in the previous sections, both NeuralNetworks and Conditional Random Fields have beenwidely used for sequence labeling tasks in NLP.Specially, CRFs (Lafferty et al, 2001) have a longhistory of being used for various sequence labelingtasks in general and named entity recognition in par-ticular.
Some early notable works include McCal-lum et.
al.
(2003), Sarawagi et al (2004) and Sha et.al.
(2003).
Hammerton et.
al.
(2003) and Chiu et.al.
(2015) used Long Short Term Memory (LSTM)857(Hochreiter and Schmidhuber, 1997) for named en-tity recognition.Several recent works on both image and text baseddomains have used structured inference to improvethe performance of Neural Network based mod-els.
In NLP, Collobert et al(2011) used Convolu-tional Neural Networks to model the unary poten-tials.
Specifically for Recurrent Neural Networks,Lample et al (2016) and Huang et.
al.
(2015) usedLSTMs to model the unary potentials of a CRF.In biomedial named entity recognition, severalapproaches use a biological corpus annotated withentities such as protein or gene name.
Settles (2004)used Conditional Random Fields to extract occur-rences of protein, DNA and similar biological en-tity classes.
Li et.
al.
(2015) recently used LSTMfor named entity recognition of protein/gene namesfrom BioCreative corpus.
Gurulingappa et.
al.
(2010) evaluated various existing biomedical dictio-naries on extraction of adverse effects and diseasesfrom a corpus of Medline abstracts.This work uses a real world clinical corpus ofElectronic Health Records annotated with variousclinical entities.
Jagannatha et.
al.
(2016) recentlyshowed that RNN based models outperform CRFmodels on the task of Medical event detection onclinical documents.
Other works using a real worldclinical corpus include Rochefort et al (2015), whoworked on narrative radiology reports.
They used aSVM-based classifier with bag of words feature vec-tor to predict deep vein thrombosis and pulmonaryembolism.
Miotto et.
al.
(2016) used a denoisingautoencoder to build an unsupervised representationof Electronic Health Records which could be usedfor predictive modeling of patient?s health.3 MethodsWe evaluate the performance of three different Bi-LSTM based structured prediction models describedin section 3.2, 3.3 and 3.4.
We compare this perfor-mance with two baseline methods of Bi-LSTM(3.1)and CRF(3.5) model.3.1 Bi-LSTM (baseline)This model is a standard bidirectional LSTM neu-ral network with word embedding input and a Soft-max Output layer.
The raw natural language inputsentence is processed with a regular expression to-kenizer into sequence of tokens x = [xt]T1 .
The to-ken sequence is fed into the embedding layer, whichproduces dense vector representation of words.
Theword vectors are then fed into a bidirectional RNNlayer.
This bidirectional RNN along with the em-bedding layer is the main machinery responsible forlearning a good feature representation of the data.The output of the bidirectional RNN produces afeature vector sequence ?
(x) = [?
(x)]T1 with thesame length as the input sequence x.
In this base-line model, we do not use any structured inference.Therefore this model alone can be used to predict thelabel sequence, by scaling and normalizing [?
(x)]T1 .This is done by using a softmax output layer, whichscales the output for a label l where l ?
{1, 2, ..., L}as follows:P (y?t = j|x) = exp(?
(x)tWj)?Ll=1 exp(?
(x)tWl)(1)The entire model is trained end-to-end using cate-gorical cross-entropy loss.3.2 Bi-LSTM CRFThis model is adapted from the Bi-LSTM CRFmodel described in Huang et.
al.
(2015).
Itcombines the framework of bidirectional RNNlayer[?
(x)]T1 described above, with linear chainCRF inference.
For a general linear chain CRF theprobability of a label sequence y?
for a given sentencex can be written as :P (y?|x) = 1ZN?t=1exp{?
(y?t) + ?
(y?t, y?t+1)} (2)Where ?
(yt) is the unary potential for the label po-sition t and ?
(yt, yt+1) is the pairwise potential be-tween the positions t,t+1.
Similar to Huang et.
al.
(2015), the outputs of the bidirectional RNN layer?
(x) are used to model the unary potentials of a lin-ear chain CRF.
In particular, the NN based unary po-tential ?nn(yt) is obtained by passing ?
(x)t througha standard feed-forward tanh layer.
The binary po-tentials or transition scores are modeled as a matrix[A]L?L.
Here L equals the number of possible la-bels including the Outside label.
Each element Ai,jrepresents the transition score from label i to j. The858probability for a given sequence y?
can then be cal-culated as :P (y?|x; ?)
= 1ZT?t=1exp{?nn(y?t) +Ay?t,y?t+1} (3)The network is trained end-to-end by minimizingthe negative log-likelihood of the ground truth labelsequence y?
for a sentence x as follows:L(x, y?
; ?)
= ??t?yt?
(yt = y?t) logP (yt|x; ?
)}(4)The negative log likelihood of given label se-quence for an input sequence is calculated by sum-product message passing.
Sum-product messagepassing is an efficient method for exact inference inMarkov chains or trees.3.3 Bi-LSTM CRF with pairwise modelingIn the previous section, the pairwise potential is cal-culated through a transition probability matrix [A]irrespective of the current context or word.
For rea-sons mentioned in section 1, this might not be aneffective strategy.
Some clinical entities are rela-tively rare.
Therefore transition from an Outside la-bel to a clinical label might not be effectively mod-eled by a fixed parameter matrix.
In this method,the pairwise potentials are modeled through a non-linear Neural Network which is dependent on thecurrent word and context.
Specifically, the pairwisepotential ?
(yt, yt+1) in equation 2 is computed byusing a one dimensional CNN with 1-D filter size 2and tanh non-linearity.
At every label position t, ittakes [?(x)t;?
(x)t+1] as input and produces a L?Lpairwise potential output ?nn(yt, yt+1).
This CNNlayer effectively acts as a non-linear feed-forwardneuron layer, which is repeatedly applied on con-secutive pairs of label positions.
It uses the outputof the bidirectional LSTM layer at positions t andt+ 1 to prepare the pairwise potential scores.The unary potential calculation is kept the same asin Bi-LSTM-CRF.
Substituting the neural networkbased pairwise potential ?nn(yt, yt+1) into equation2 we can reformulate the probability of the label se-quence y?
given the word sequence x as :P (y?|x; ?)
= 1ZN?t=1exp{?nn(y?t) + ?nn(y?t, y?t+1)}(5)Labels Num.
ofInstancesAvg word length?
stdADE 1807 1.68 ?
1.22Indication 3724 2.20 ?
1.79Other SSD 40984 2.12 ?
1.88Severity 3628 1.27 ?
0.62Drugname 17008 1.21 ?
0.60Duration 926 2.01 ?
0.74Dosage 5978 2.09 ?
0.82Route 2862 1.20?
0.47Frequency 5050 2.44?
1.70Table 1: Annotation statistics for the corpus.The neural network is trained end-to-end with theobjective of minimizing the negative log likelihoodin equation 4.
The negative log-likelihood scores areobtained by sum-product message passing.3.4 Approximate Skip-chain CRFSkip chain models are modifications to linear chainCRFs that allow long term label dependenciesthrough the use of skip edges.
These are basicallyedges between label positions that are not adjacentto each other.
Due to these skip edges, the skip chainCRF model (Sutton and McCallum, 2006) explicitlymodels dependencies between labels which mightbe more than one position apart.
The joint inferenceover these dependencies are taken into account whiledecoding the best label sequence.
However, unlikethe two models explained in the preceding section,the skip-chain CRF contains loops between labelvariables.
As a result we cannot use the sum-productmessage passing method to calculate the negativelog-likelihood.The loopy structure of the graph inskip chain CRF renders exact message passing in-ference intractable.
Approximate solutions for thesemodels include loopy belief propagation(BP) whichrequires multiple iterations of message passing.However, an approach like loopy BP is pro-hibitively expensive in our model with large NeuralNet based potential functions.
The reason for this isthat each gradient descent iteration for a combinedRNN-CRF model requires a fresh calculation of themarginals.
In one approach to mitigate this, Lin et.al.
(2015) directly model the messages in the mes-sage passing inference of a 2-D grid CRF for imagesegmentation.
This bypasses the need for modelingthe potential function, as well as calculating the ap-859proximate messages on the graph using loopy BP.Approximate CRF message passing inference:Lin et.
al.
(2015) directly estimate the factor tovariable message using a Neural Network that usesinput image features.
Their underlying reasoning isthat the factor-to-variable message from factor F tolabel variable yt for any iteration of loopy BP canbe approximated as a function of all the input vari-ables and previous messages that are a part of thatfactor.
They only model one iteration of loopy BP,and empirically show that it leads to an apprecia-ble increase in performance.
This allows them tomodel the messages as a function of only the inputvariables, since the messages for the first iteration ofmessage passing are computed using the potentialfunctions alone.We follow a similar approach for calculationof variable marginals in our skip chain model.However, instead of estimating individual factor-to-variable messages, we exploit the sequence struc-ture in our problem and estimate groups of factor-to-variable messages.
For any label node yt, the firstgroup contains factors that involve nodes which oc-cur before yt in the sentence (from left).
The secondgroup of factor-to-variable messages corresponds tofactors involving nodes occurring later in the sen-tence.
We use recurrent computational units likeLSTM to estimate the sum of log factor-to-variablemessages within a group.
Essentially, we use bidi-rectional recurrent computation to estimate all theincoming factors from left and right separately.To formulate this, let us assume for now that weare using skip edges to connect the current node tto m preceding and m following nodes.
Each edge,skip or otherwise, is denoted by a factor which con-tains the binary potential of the edge and the unarypotential of the connected node.
As mentioned ear-lier, we will divide the factors associated with nodet into two sets, FL(t) and FR(t).
Here FL(t) , con-tains all factors formed between the variables fromthe group {yt?m, ..., yt?1} and yt.
So we can for-mulate the combined message from factors in FL(t)as?L(yt) = [?F?FL(t)?F?t(yt)] (6)The combined messages from factors in FR(t)which contains variables from yt+1 to yt+m can beformulated as :?R(yt) = [?F?FR(t)?F?t(yt)] (7)We also need the unary potential of the label vari-able t to compose its marginal.
The unary po-tentials of each variable from {yt?m, ..., yt?1} and{yt+1, ..., yt+m} should already be included in theirrespective factors.
The log of the unnormalizedmarginal P?
(yt|x) for the variable yt, can thereforebe calculated bylog P?
(yt|x) = ?R(yt) + ?L(yt) + ?
(yt) (8)Similar to Lin et.
al.
(2015), in the interest oflimited network complexity, we use only one mes-sage passing iteration.
In our setup, this means that avariable-to-factor message from a neighboring vari-able yi to the current variable yt contains only theunary potentials of yi and binary potential betweenyi , yt.
As a consequence of this, we can see that?L(yt) can be written as :?L(yt) =t?m?i=t?1log?yi[exp?
(yt, yi) + ?
(yi)] (9)Similarly, we can formulate a function for ?R(yt) ina similar way :?R(yt) =t+m?i=t+1log?yi[exp?
(yt, yi) + ?
(yi)](10)Modeling the messages using RNN: As mentionedpreviously in equation 8, we only need to estimate?L(yt), ?R(yt) and ?
(yt) to calculate the marginalof variable yt.
We can use ?nn(yt) framework intro-duced in section 3.2 to estimate the unary potentialfor yt.
We use different directions of a bidirectionalLSTM to estimate ?R(yt) and ?L(yt).
This elim-inates the need to explicitly model and learn pair-wise potentials for variables that are not immediateneighbors.The input to this layer at position t is[?nn(yt);?nn(yt, yt+1)] (composed of potentialfunctions described in section 3.3).
This can beviewed as an LSTM layer aggregating beliefs aboutyt from the unary and binary potentials of [y]t?11860Strict Evaluation ( Exact Match) Relaxed Evaluation (Word based)Models Recall Precision F-score Recall Precision F-scoreCRF 0.7385 0.8060 0.7708 0.7889 0.8040 0.7964Bi-LSTM 0.8101 0.7845 0.7971 0.8402 0.8720 0.8558Bi-LSTM CRF 0.7890 0.8066 0.7977 0.8068 0.8839 0.8436Bi-LSTM CRF-pair 0.8073 0.8266 0.8169 0.8245 0.8527 0.8384Approximate Skip-Chain CRF 0.8364 0.8062 0.8210 0.8614 0.8651 0.8632Table 2: Cross validated micro-average of Precision, Recall and F-score for all clinical tagsto approximate the sum of messages from left side?L(yt).
Similarly, ?R(yt) can be approximated fromthe LSTM aggregating information from the oppo-site direction.
Formally, ?L(yt) is approximated asa function of neural network based unary and binarypotentials as follows:?L(yt) ?
f ([?nn(yi);?nn(yi, yi+1)]t?11 ) (11)Using LSTM as a choice for recurrent compu-tation here is advantageous, because LSTMs areable to learn long term dependencies.
In ourframework, this allows them to learn to prioritizemore relevant potential functions from the sequence[[?nn(yi);?nn(yi, yi+1)]t?11 .
Another advantage ofthis method is that we can approximate skip edgesbetween all preceding and following nodes, insteadof modeling just m surrounding ones.
This is be-cause LSTM states are maintained throughout thesentence.The partition function for yt can be easily ob-tained by using logsumexp over all label entries ofthe unnormalized log marginal shown in equation 8as follows:Zt =?ytexp[?R(yt) + ?L(yt) + ?
(yt)] (12)Here the partition function Z is a different for differ-ent positions of t. Due to our approximations, it isnot guaranteed that the partition function calculatedfrom different marginals of the same sentence areequal.
The normalized marginal can be now calcu-lated by normalizing log P?
(yt|x) in equation 8 usingZt.L(x, y?
; ?)
= ??t?yt?
(yt = y?t)(?R(yt; ?
)+?L(yt; ?)
+ ?
(yt; ?)?
logZt(?
))(13)The model is optimized using cross entropyloss between the true marginal and the predictedmarginal.
The loss for a sentence x with a groundtruth label sequence y?
is provided in equation 13.3.5 CRF (baseline)We use the linear chain CRF, which is a widely usedmodel in extraction of clinical named entities.
Asmentioned previously, Conditional Random Fieldsexplicitly model dependencies between output vari-ables conditioned on a given input sequence.The main inputs to CRF in this model are notRNN outputs, but word inputs and their correspond-ing word vector representations.
We add additionalsentence features consisting of four vectors.
Two ofthem are bag of words representation of the sentencesections before and after the word respectively.
Theremaining two vectors are dense vector representa-tions of the same sentence sections.
The dense vec-tors are calculated by taking the mean of all indi-vidual word vectors in the sentence section.
We addthese features to explicitly mimic information pro-vided by the bidirectional chains of the LSTM mod-els.4 DatasetWe use an annotated corpus of 1154 English Elec-tronic Health Records from cancer patients.
Eachnote was annotated2 by two annotators who labelclinical entities into several categories.
These cate-gories can be broadly divided into two groups, Clin-ical Events and Attributes.
Clinical events includeany specific event that causes or might contribute toa change in a patient?s medical status.
Attributesare phrases that describe certain important proper-ties about the events.2The annotation guidelines can be foundat https://github.com/abhyudaynj/LSTM-CRF-models/blob/master/annotation.md861Figure 1: Plots of Recall, Precision and F-score for RNN based methods.
The metrics with prefix Strict are using phrase basedevaluation.
Relaxed metrics use word based evaluation.Bar-plots are in order with Bi-LSTM on top and Approx-skip-chain-CRF atthe bottom.Clinical Event categories in this corpus are Ad-verse Drug Event (ADE), Drugname , Indicationand Other Sign Symptom and Diseases (Other SSD).ADE, Indication and Other SSD are events havinga common vocabulary of Sign, Symptoms and Dis-eases (SSD).
They can be differentiated based on thecontext that they are used in.
A certain SSD shouldbe labeled as ADE if it can be manually identified asa side effect of a drug based on the evidence in theclinical note.
It is an Indication if it is an afflictionthat a doctor is actively treating with a medication.Any other SSD that does not fall into the above twocategories ( for e.g.
an SSD in patients history) islabeled as Other SSD.
Drugname event labels anymedication or procedure that a physician prescribes.The attribute categories contain the followingproperties, Severity , Route, Frequency, Durationand Dosage.
Severity is an attribute of the SSD eventtypes , used to label the severity a disease or symp-tom.
Route, Frequency, Duration and Dosage areattributes of Drugname.
They are used to label themedication method, frequency of dosage, durationof dosage, and the dosage quantity respectively.
Theannotation statistics of the corpus are provided in theTable 1.5 ExperimentsEach document is split into separate sentences andthe sentences are tokenized into individual word andspecial character tokens.
The models operate onthe tokenized sentences.
In order to accelerate thetraining procedure, all LSTM models use batch-wisetraining using a batch of 64 sentences.
In order to dothis, we restricted the sentence length to 50 tokens.All sentences longer than 50 tokens were split intoshorter size samples, and shorter sentences were pre-padded with masks.
The CRF baseline model(3.5)does not use batch training and so the sentences wereused unaltered.The first layer for all LSTM models was a 200dimensional word embedding layer.
In order to im-prove performance, we initialized embedding layervalues in these models with a skip-gram word em-bedding (Mikolov et al, 2013).
The skip-gram em-bedding was calculated using a combined corpus862of PubMed open access articles, English Wikipediaand an unlabeled corpus of around hundred thousandElectronic Health Records.
The EHRs used in theannotated corpus are not in this unlabeled EHR cor-pus.
This embedding is also used to provide wordvector representation to the CRF baseline model.The bidirectional LSTM layer which outputs?
(x) contains LSTM neurons with a hidden sizeranging from 200 to 250.
This hidden size iskept variable in order to control for the number oftrainable parameters between different LSTM basedmodels.
This helps ensure that the improved perfor-mance in these models is only because of the modi-fied model structure, and not an increase in trainableparameters.
The hidden size is varied in such a waythat the number of trainable parameters are close to3.55 million parameters.
Therefore, the Approx skipchain CRF has 200 hidden layer size, while stan-dard Bi-LSTM model has 250 hidden layer.
Sincethe?
(x) layer is bidirectional, this effectively meansthat the Bi-LSTM model has 500 hidden layer size,while Approx skip chain CRF model has 400 dimen-sional hidden layer.We use dropout (Srivastava et al, 2014) with aprobability of 0.50 in all LSTM models in order toimprove regularization performance.
We also usebatch norm (Ioffe and Szegedy, 2015) between lay-ers wherever possible in order to accelerate training.All RNN models are trained in an end-to-end fashionusing Adagrad (Duchi et al, 2011) with momentum.The CRF model was trained using L-BFGS with L2regularization.
We use Begin Inside Outside (BIO)label modifiers for models that use CRF objective.We use ten-fold cross validation for our results.The documents are divided into training and testdocuments.
From each training set fold, 20% of thesentences form the validation set which is used formodel evaluation during training and for early stop-ping.We report the word based and exact phrase matchbased micro-averaged recall, precision and F-score.Exact phrase match based evaluation is calculatedon a per phrase basis, and considers a phrase as pos-itively labeled only if the phrase exactly matchesthe true boundary and label of the reference phrase.Word based evaluation metric is calculated on labelsof individual words.
A word?s predicted label is con-sidered as correct if it matches the reference label,irrespective of whether the remaining words in itsphrase are labeled correctly.
Word based evaluationis a more relaxed metric than phrase based evalua-tion.6 ResultsThe micro-averaged Precision, Recall and F-scorefor all five models are shown in Table 2.
We reportboth strict (exact match) and relaxed (word based)evaluation results.
As shown in Table 2, the best per-formance is obtained by Skip-Chain CRF (0.8210for strict and 0.8632 for relaxed evaluation).
AllLSTM based models outperform the CRF baseline.Bi-LSTM-CRF and Bi-LSTM-CRF-pair models us-ing exact CRF inference improve the precision ofstrict evaluation by 2 to 5 percentage points.
Bi-LSTM CRF-pair achieved the highest precision forexact-match.
However, the recall (both strict and re-laxed) for exact CRF-LSTM models is less than Bi-LSTM.
This reduction in recall is much less in theBi-LSTM-pair model.
In relaxed evaluation, onlythe Skip Chain model has a better F-score than thebaseline LSTM.
Overall, Bi-LSTM-CRF-pair andApprox-Skip-Chain models lead to performance im-provements.
However, the standard Bi-LSTM-CRFmodel does not provide an appreciable increase overthe baseline.Figure 1 shows the breakdown of performancefor each RNN model with respect to individualclinical entity labels.
CRF baseline model perfor-mance is not shown in Figure 1, because its per-formance is consistently lower than Bi-LSTM-CRFmodel across all label categories.
We use pairwiset-test on strict evaluation F-score for each fold incross validation, to calculate the statistical signifi-cance of our scores.
The improvement in F-scorefor Bi-LSTM-CRF-pair and Approx-Skip Chain ascompared to Bi-LSTM baseline is statistically sig-nificant (p < 0.01).
The difference in Bi-LSTM-CRF and Bi-LSTM baseline, does not appear to bestatistically significant (p > 0.05).
However, the im-provements over CRF baseline for all LSTM modelsare statistically significant.7 DiscussionOverall, Approx-Skip-Chain CRF model achievedbetter F-scores than CRF,Bi-LSTM and Bi-LSTM-863CRF in both strict and relaxed evaluations.
The re-sults of strict evaluation, as shown in Figure 1, areour main focus of discussion due to their impor-tance in the clinical domain.
As expected, two ex-act inference-based CRF-LSTM models (Bi-LSTM-CRF and Bi-LSTM-CRF-pair) show the highest pre-cision for all labels.
Approx-Skip-Chain CRF?s pre-cision is lower(due to approximate inference) but itstill mostly outperforms Bi-LSTM.
The recall forSkip Chain CRF is almost equal or better than allother models due to its robustness in modeling de-pendencies between distant labels.
The variations inrecall contribute to the major differences in F-scores.These variations can be due to several factors includ-ing the rarity of that label in the dataset, the com-plexity of phrases of a particular label, etc.We believe, exact CRF-LSTM models describedhere require more training samples than the baselineBi-LSTM to achieve a comparable recall for labelsthat are complex or ?difficult to detect?.
For exam-ple, as shown in table 1, we can divide the labels intofrequent ( Other SSD, Indication, Severity, Drug-name, Dosage, and Frequency) and rare or sparse(Duration, ADE, Route).
We can make a broad gen-eralization, that exact CRF models (especially Bi-LSTM-CRF) have somewhat lower recall for rarelabels.
This is true for most labels except for Route,Indication, and Severity.
The CRF models have veryclose recall (0.780,0.782) to the baseline Bi-LSTM(0.803) for Route even though its number of inci-dences are lower (2,862 incidences) than Indication(3,724 incidences) and Severity (3,628 incidences),both of which have lower recall even though theirincidences are much higher.Complexity of each label can explain the afore-mentioned phenomenon.
Route for instance, fre-quently contains unique phrases such as ?by mouth?or ?p.o.,?
and is therefore easier to detect.
In con-trast, Indication is ambiguous.
Its vocabulary isclose to two other labels: ADE (1,807 incidences)and the most populous Other SSD (40,984 inci-dences).
As a consequence, it is harder to sepa-rate the three labels.
Models need to learn cuesfrom surrounding context, which is more difficultand requires more samples.
This is why the re-call for Indication is lower for CRF-LSTM models,even though its number of incidences is higher thanRoute.
To further support our explanation, our re-sults show that the exact CRF-LSTM models mis-labeled around 40% of Indication words as OtherSSD, as opposed to just 20 % in case of the Bi-LSTM baseline.
The label Severity is a similar case.It contains non-label-specific phrases such as ?notterribly?, ?very rare?
and ?small area,?
which mayexplain why almost 35% of Severity words are mis-labeled as Outside by the bi-LSTM-CRF as opposedto around 20% by the baseline.It is worthwhile to note that among exact CRF-LSTM models, the recall for Bi-LSTM-CRF-pair ismuch better than Bi-LSTM-CRF even for sparse la-bels.
This validates our initial hypothesis that Neu-ral Net based pairwise modeling may lead to betterdetection of rare labels.8 ConclusionWe have shown that modeling pairwise potentialsand using an approximate version of Skip-chain in-ference increase the performance of the LSTM-CRFmodels.
We also show that these models performmuch better than baseline LSTM and CRF models.These results suggest that the structured predictionmodels are good directions for improving the exactphrase extraction for clinical entities.AcknowledgmentsWe thank the UMassMed annotation team: ElaineFreund, Wiesong Liu, Steve Belknap, Nadya Frid,Alex Granillo, Heather Keating, and Victoria Wangfor creating the gold standard evaluation set used inthis work.
We also thank the anonymous reviewersfor their comments and suggestions.This work was supported in part by the grantHL125089 from the National Institutes of Health(NIH).
We also acknowledge the support from theUnited States Department of Veterans Affairs (VA)through Award 1I01HX001457.
This work was alsosupported in part by the Center for Intelligent Infor-mation Retrieval.
The contents of this paper do notrepresent the views of CIIR, NIH, VA or the UnitedStates Government.ReferencesJason PC Chiu and Eric Nichols.
2015.
Named en-tity recognition with bidirectional lstm-cnns.
arXivpreprint arXiv:1511.08308.864Ronan Collobert, Jason Weston, Le?on Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011.Natural language processing (almost) from scratch.The Journal of Machine Learning Research, 12:2493?2537.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learning andstochastic optimization.
The Journal of MachineLearning Research, 12:2121?2159.Harsha Gurulingappa, Roman Klinger, Martin Hofmann-Apitius, and Juliane Fluck.
2010.
An empirical evalu-ation of resources for the identification of diseases andadverse effects in biomedical literature.
In 2nd Work-shop on Building and evaluating resources for biomed-ical text mining (7th edition of the Language Resourcesand Evaluation Conference).James Hammerton.
2003.
Named entity recognition withlong short-term memory.
In Proceedings of the sev-enth conference on Natural language learning at HLT-NAACL 2003-Volume 4, pages 172?175.
Associationfor Computational Linguistics.Sepp Hochreiter and Ju?rgen Schmidhuber.
1997.
Longshort-term memory.
Neural computation, 9(8):1735?1780.Zhiheng Huang, Wei Xu, and Kai Yu.
2015.
Bidirec-tional lstm-crf models for sequence tagging.
arXivpreprint arXiv:1508.01991.Sergey Ioffe and Christian Szegedy.
2015.
Batchnormalization: Accelerating deep network trainingby reducing internal covariate shift.
arXiv preprintarXiv:1502.03167.Abhyuday Jagannatha and Hong Yu.
2016.
Bidirectionalrnn for medical event detection in electronic healthrecords.
In Proceedings of NAACL-HLT, pages 473?482.John Lafferty, Andrew McCallum, and Fernando CNPereira.
2001.
Conditional random fields: Probabilis-tic models for segmenting and labeling sequence data.Guillaume Lample, Miguel Ballesteros, Sandeep Subra-manian, Kazuya Kawakami, and Chris Dyer.
2016.Neural architectures for named entity recognition.arXiv preprint arXiv:1603.01360.Lishuang Li, Liuke Jin, Zhenchao Jiang, Dingxin Song,and Degen Huang.
2015.
Biomedical named entityrecognition based on extended recurrent neural net-works.
In Bioinformatics and Biomedicine (BIBM),2015 IEEE International Conference on, pages 649?652.
IEEE.Guosheng Lin, Chunhua Shen, Ian Reid, and Antonvan den Hengel.
2015.
Deeply learning the messagesin message passing inference.
In Advances in NeuralInformation Processing Systems, pages 361?369.Andrew McCallum and Wei Li.
2003.
Early resultsfor named entity recognition with conditional randomfields, feature induction and web-enhanced lexicons.In Proceedings of the seventh conference on Natu-ral language learning at HLT-NAACL 2003-Volume 4,pages 188?191.
Association for Computational Lin-guistics.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013.
Distributed representa-tions of words and phrases and their compositionality.In Advances in neural information processing systems,pages 3111?3119.Riccardo Miotto, Li Li, Brian A Kidd, and Joel T Dud-ley.
2016.
Deep patient: An unsupervised representa-tion to predict the future of patients from the electronichealth records.
Scientific reports, 6:26094.Christian M Rochefort, Aman D Verma, TewodrosEguale, Todd C Lee, and David L Buckeridge.
2015.A novel method of adverse event detection can ac-curately identify venous thromboembolisms (vtes)from narrative electronic health record data.
Jour-nal of the American Medical Informatics Association,22(1):155?165.Sunita Sarawagi and William W Cohen.
2004.
Semi-markov conditional random fields for information ex-traction.
In Advances in neural information process-ing systems, pages 1185?1192.Burr Settles.
2004.
Biomedical named entity recognitionusing conditional random fields and rich feature sets.In Proceedings of the International Joint Workshop onNatural Language Processing in Biomedicine and itsApplications, pages 104?107.
Association for Compu-tational Linguistics.Fei Sha and Fernando Pereira.
2003.
Shallow parsingwith conditional random fields.
In Proceedings of the2003 Conference of the North American Chapter of theAssociation for Computational Linguistics on HumanLanguage Technology-Volume 1, pages 134?141.
As-sociation for Computational Linguistics.Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,Ilya Sutskever, and Ruslan Salakhutdinov.
2014.Dropout: A simple way to prevent neural networksfrom overfitting.
The Journal of Machine LearningResearch, 15(1):1929?1958.Charles Sutton and Andrew McCallum.
2006.
An in-troduction to conditional random fields for relationallearning.
Introduction to statistical relational learn-ing, pages 93?128.865
