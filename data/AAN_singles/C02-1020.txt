Extracting Word Sequence Correspondenceswith Support Vector MachinesKengo SATO and Hiroaki SAITODepartment of Information and Computer ScienceKeio University3?14?1, Hiyoshi, Kohoku, Yokohama 223?8522, Japan{satoken,hxs}@nak.ics.keio.ac.jpAbstractThis paper proposes a learning and extractingmethod of word sequence correspondences fromnon-aligned parallel corpora with Support VectorMachines, which have high ability of the generaliza-tion, rarely cause over-fit for training samples andcan learn dependencies of features by using a kernelfunction.
Our method uses features for the trans-lation model which use the translation dictionary,the number of words, part-of-speech, constituentwords and neighbor words.
Experiment results inwhich Japanese and English parallel corpora areused archived 81.1 % precision rate and 69.0 % re-call rate of the extracted word sequence correspon-dences.
This demonstrates that our method couldreduce the cost for making translation dictionaries.1 IntroductionTranslation dictionaries used in multilingual natu-ral language processing such as machine transla-tion have been made manually, but a great deal oflabor is required for this work and it is difficultto keep the description of the dictionaries consis-tent.
Therefore, researches of extracting transla-tion pairs from parallel corpora automatically be-come active recently (Gale and Church, 1991; Kajiand Aizono, 1996; Tanaka and Iwasaki, 1996; Kita-mura and Matsumoto, 1996; Fung, 1997; Melamed,1997; Sato and Nakanishi, 1998).This paper proposes a learning and extract-ing method of bilingual word sequence correspon-dences from non-aligned parallel corpora with Sup-port Vector Machines (SVMs) (Vapnik, 1999).SVMs are ones of large margin classifiers (Smolaet al, 2000) which are based on the strategy wheremargins between separating boundary and vectorsof which elements express the features of train-ing samples is maximized.
Therefore, SVMs havehiger ability of the generalization than other learn-ing models such as the decision trees and rarelycause over-fit for training samples.
In addition, byusing kernel functions, they can learn non-linearseparating boundary and dependencies between thefeatures.
Therefore, SVMs have been recently usedfor the natural language processing such as textcategorization (Joachims, 1998; Taira and Haruno,1999), chunk identification (Kudo and Matsumoto,2000b), dependency structure analysis (Kudo andMatsumoto, 2000a).The method proposed in this paper does not re-quire aligned parallel corpora which do not exist toomany at present.
Therefore, without limiting appli-cable domains, word sequence correspondences canbeen extracted.2 Support Vector MachinesSVMs are binary classifiers which linearly separated dimension vectors to two classes.
Each vector rep-resents the sample which has d features.
It is distin-guished whether given sample ~x = (x1, x2, .
.
.
, xd)belongs to X1 or X2 by equation (1) :f (~x) = sign(g(~x)) ={1(~x ?
X1)?1 (~x ?
X2) (1)where g(~x) is the hyperplain which separates twoclasses in which ~w and b are decided by optimiza-tion.g(~x) = ~w ?
~x + b (2)Let supervise signals for the training samples beexpressed asyi ={1(~xi ?
X1)?1 (~xi ?
X2)where X1 is a set of positive samples and X2 is a setof negative samples.If the training samples can be separated linearly,there could exist two or more pairs of ~w and b thatPSfrag replacements X1X22/||~w||~w ?
~x + b = 0~w ?
~x + b = 1~w ?
~x + b = ?1Figure 1: A separating hyperplainsatisfy equation (1).
Therefore, give the followingconstraints :?i, yi(~w ?
~xi + b) ?
1 ?
0 (3)Figure 1 shows that the hyperplain which sepa-rates the samples.
In this figure, solid line showsseparating hyperplain ~w ?
~x + b = 0 and two dottedlines show hyperplains expressed by ~w ?
~x+ b = ?1.The constraints (3) mean that any vectors must notexist inside two dotted lines.
The vectors on dottedlines are called support vectors and the distance be-tween dotted lines is called a margin, which equalsto 2/||~w||.The learning algorithm for SVMs could optimize~w and b which maximize the margin 2/||~w|| or min-imize ||~w||2/2 subject to constraints (3).
Accordingto Lagrange?s theory, the optimization problem istransformed to minimizing the Lagrangian L :L = 12||~w||2 +n?i=1?i(yi(~w ?
~xi + b ?
1)) (4)where ?i ?
0 (i = 1, .
.
.
, n) are the Lagrange mul-tipliers.
By differentiating with respect to ~w and b,the following relations are obtained,?L?~w = ~w ?n?i=1?iyi~x = 0 (5)?L?b =n?i=1?iyi = 0 (6)and substituting equations (5) (6) into equation (4)to obtainD = ?12n?i=1n?j=1?i?
jyiy j~xi ?
~x j +n?i=1?i (7)Consequently, the optimization problem is trans-formed to maximizing the object function D subjectto?ni=1 ?iyi = 0 and ?i ?
0.
For the optimal pa-rameters ??
= arg max?
D, each training sample ~xiwhere ?
?i > 0 is corresponding to support vector.~w can be obtained from equation (5) and b can beobtained fromb = yi ?
~w ?
~xiwhere ~xi is an arbitrary support vector.
From equa-tion (2) (5), the optimal hyperplain can be expressedas the following equation with optimal parameters??
:g(~x) =n?i=1?
?i yi~xi ?
~x + b (8)The training samples could be allowed in somedegree to enter the inside of the margin by changingequation (3) to :?i, yi(~w ?
~xi + b) ?
1 + ?i ?
0 (9)where ?i ?
0 are called slack variables.
At this time,the maximal margin problem is enhanced as mini-mizing ||~w||2/2 + C?ni=1 ?i, where C expresses theweight of errors.
As a result, the problem is to max-imize the object function D subject to ?ni=1 ?iyi = 0and 0 ?
?i ?
C.For the training samples which cannot be sepa-rated linearly, they might be separated linearly inhigher dimension by mapping them using a non-linear function:?
: Rd 7?
Rd?A linear separating in Rd?
for ?
(~x) is same as a non-linear separating in Rd for ~x.
Let ?
satisfyK(~x, ~x?)
= ?
(~x) ?
?(~x?)
(10)where K(~x, ~x?)
is called kernel function.
As a result,the object function is rewritten toD = ?12n?i=1n?j=1?i?
jyiy jK(~xi, ~x j) +n?i=1?i (11)and the optimal hyperplain is rewritten tog(~x) =n?i=1?
?i yiK(~xi, ~x) + b (12)Note that ?
does not appear in equation (11) (12).Therefore, we need not calculate ?
in higher dimen-sion.The well-known kernel functions are the polyno-mial kernel function (13) and the Gaussian kernelfunction (14).K(~x, ~x?)
= (~x ?
~x?
+ 1)p (13)K(~x, ~x?)
= exp??????
?||~x ?~x?||22?2??????
(14)A non-linear separating using one of these kernelfunctions is corresponding to separating with con-sideration of the dependencies between the featuresin Rd.3 Extracting Word SequenceCorrespondences with SVMs3.1 OutlineThe method proposed in this paper can obtain wordsequence correspondences (translation pairs) in theparallel corpora which include Japanese and En-glish sentences.
It consists of the following threesteps:1.
Make training samples which include positivesamples as translation pairs and negative sam-ples as non-translation pairs from the train-ing corpora manually, and learn a translationmodel from these with SVMs.2.
Make a set of candidates of translation pairswhich are pairs of phrases obtained by pars-ing both Japanese sentences and English sen-tences.3.
Extract translation pairs from the candidates byinputting them to the translation model made instep 1.3.2 Features for the Translation ModelTo apply SVMs for extracting translation pairs, thecandidates of the translation pairs must be convertedinto feature vectors.
In our method, they are com-posed of the following features:1.
Features which use an existing translation dic-tionary.
(a) Bilingual word pairs in the translationdictionary which are included in the can-didates of the translation pairs.
(b) Bilingual word pairs in the translationdictionary which are co-occurred in thecontext in which the candidates appear.2.
Features which use the number of words.
(a) The number of words in Japanese phrases.
(b) The number of words in English phrases.3.
Features which use the part-of-speech.
(a) The ratios of appearance of noun, verb,adjective and adverb in Japanese phrases.
(b) The ratios of appearance of noun, verb,adjective and adverb in English phrases.4.
Features which use constituent words.
(a) Constituent words in Japanese phrases.
(b) Constituent words in English phrases.5.
Features which use neighbor words.
(a) Neighbor words which appear in Japanesephrases just before or after.
(b) Neighbor words which appear in Englishphrases just before or after.Two types of the features which use an existingtranslation dictionary are used because the improve-ment of accuracy can be expected by effectively us-ing existing knowledge in the features.
For features(1a), words included in a candidate of the trans-lation pair are looked up with the translation dic-tionary and the bilingual word pairs in the candi-date become features.
They are based on the ideathat a translation pair would include many bilingualword pairs.
Each bilingual word pair included inthe dictionary is allocated to the dimension of thefeature vectors.
If a bilingual word pair appears inthe candidate of translation pair, the value of thecorresponding dimension of the vector is set to 1,and otherwise it is set to 0.
For features (1b), allpairs of words which co-occurred with a candidateof the translation pair are looked up with the trans-lation dictionary and the bilingual word pairs in thedictionary become features.
They are based on theidea that the context of the words which appear inneighborhood looks like each other for the trans-lation pairs although expressed in the two differentlanguages (Kaji and Aizono, 1996).
The candidatesare converted into the feature vectors just like (1a).Features (2a) (2b) are based on the idea that thereis a correlation in the number of constituent wordsof the phrases of both languages in the translationpair.
The number of constituent words of each lan-guage is used for the feature vector.Features (3a) (3b) are based on the idea that thereis a correlation in the ratio of content words (noun,verb, adjective and adverb) which appear in thephrases of both languages in a translation pair.
Theratios of the numbers of noun, verb, adjective andadverb to the number of words of the phrases ofeach language are used for the feature vector.For features (4a) (4b), each content word (noun,verb, adjective and adverb) is allocated to the di-mension of the feature vectors for each language.
Ifa word appears in the candidate of translation pair,the value of the corresponding dimension of the vec-tor is set to 1, and otherwise it is set to 0.For features (5a) (5b), each content words (noun,verb, adjective and adverb) is allocated to the di-mension of the feature vectors for each language.
Ifa word appears in the candidate of translation pairjust before or after, the value of the correspondingdimension of the vector is set to 1, and otherwise itis set to 0.3.3 Learning the Translation ModelTraining samples which include positive samples asthe translation pairs and negative samples as thenon-translation pairs are made from the trainingcorpora manually, and are converted into the fea-ture vectors by the method described in section 3.2.For supervise signals yi, each positive sample is as-signed to +1 and each negative sample is assignedto ?1.
The translation model is learned from themby SVMs described in section 2.
As a result, theoptimal parameters ??
for SVMs are obtained.3.4 Making the Candidate of the TranslationPairsA set of candidates of translation pairs is made fromthe combinations of phrases which are obtained byparsing both Japanese and English sentences.
Howto make the combinations does not require sen-tence alignments between both languages.
Becausethe set grows too big for all the combinations, thephrases used for the combinations are limited in up-per bound of the number of constituent words andonly noun phrases and verb phrases.3.5 Extracting the Translation PairsThe candidates of the translation pairs are convertedinto the feature vectors with the method describedin section 3.2.
By inputting them to equation (8)with the optimal parameters ??
obtained in section3.3, +1 or ?1 could be obtained as the output foreach vector.
If the output is +1, the candidate corre-sponding to the input vector is the translation pair,otherwise it is not the translation pair.4 ExperimentsTo confirm the effectiveness of the method de-scribed in section 3, we did the experiments wherethe English Business Letter Example Collectionpublished from Nihon Keizai Shimbun Inc. are usedas parallel corpora, which include Japanese and En-glish sentences which are examples of business let-ters, and are marked up at translation pairs.As both training and test corpora, 1,000 sentenceswere used.
The translation pairs which are alreadymarked up in the corpora were corrected to the formdescribed in section 3.4 to be used as the positivesamples.
Japanese sentences were parsed by KNP 1and English sentences were parsed by Apple PieParser 2.
The negative samples of the same numberas the positive samples were randomly chosen fromcombinations of phrases which were made by pars-ing and of which the numbers of constituent wordswere below 8 words.
As a result, 2,000 samples(1,000 positives and 1,000 negatives) for both train-ing and test were prepared.The obtained samples must be converted into thefeature vectors by the method described in section3.2.
For features (1a) (1b), 94,511 bilingual wordpairs included in EDICT 3 were prepared.
For fea-tures (4a) (4b) (5a) (5b), 1,009 Japanese words and890 English words which appeared in the trainingcorpora above 3 times were used.
Therefore, thenumber of dimensions for the feature vectors was94, 511?2+1?2+4?2+1, 009+890+1, 009+890 =192, 830.S V Mlight 4 was used for the learner and the clas-sifier of SVMs.
For the kernel function, the squaredpolynomial kernel (p = 2 in equation (13)) wasused, and the error weight C was set to 0.01.The translation model was learned by the train-ing samples and the translation pairs were extractedfrom the test samples by the method described insection 3.1http://www-lab25.kuee.kyoto-u.ac.jp/nl-resource/knp.html2http://www.cs.nyu.edu/cs/projects/proteus/app/3http://www.csse.monash.edu.au/?jwb/edict.html4http://svmlight.joachims.org/0204060801000  2  4  6  8  10  12  14  16  18  20rate(%)the number of the training samples (x1.0e02)PrecisionRecallFigure 2: Transition in the precision rate and therecall rate when the number of the training samplesare increasedTable 1 shows the precision rate and the recallrate of the extracted translation pairs, and table 2shows examples of the extracted translation pairs.Table 1: Precision and recall rateOutputs Corrects Precision Recall851 690 81.1 % 69.0 %5 DiscussionFigure 2 shows the transition in the precision rateand the recall rate when the number of the trainingsamples are increased from 100 to 2,000 by every100 samples.
The recall rate rose according to thenumber of the training samples, and reaching thelevel-off in the precision rate since 1,300.
There-fore, it suggests that the recall rate can be improvedwithout lowering the precision rate too much by in-creasing the number of the training samples.Figure 3 shows that the transition in the precisionrate and the recall rate when the number of the bilin-gual word pairs in the translation dictionary are in-creased from 0 to 90,000 by every 5,000 pairs.
Theprecision rate rose almost linearly according to thenumber of the pairs, and reaching the level-off in therecall rate since 30,000.
Therefore, it suggests thatthe precision rate can be improved without loweringthe recall rate too much by increasing the number ofthe bilingual word pairs in the translation dictionary.Table 3 shows the precision rate and the recallrate when each kind of features described in section3.2 was removed.
The values in parentheses in thecolumns of the precision rate and the recall rate are0204060801000  10  20  30  40  50  60  70  80  90  100rate(%)the size of dictionary (x1.0e03)PrecisionRecallFigure 3: Transition in the precision rate and therecall rate when the number of the bilingual wordpairs in the translation dictionary are increaseddifferences with the values when all the features areused.
The fall of the precision rate when the featureswhich use the translation dictionary (1a) (1b) wereremoved and the fall of the recall rate when the fea-tures which use the number of words (2a) (2b) wereremoved were especially large.It is clear that feature (1a) (1b) could restrictthe translation model most strongly in all features.Therefore, if feature (1a) (1b) were removed, itcauses a good translation model not to be able tobe learned only by the features of the remainderbecause of the weak constraints, wrong outputs in-creased, and the precision rate has fallen.Only features (2a) (2b) surely appear in all sam-ples although some other features appeared in thetraining samples may not appear in the test samples.So, in the test samples, the importance of features(2a) (2b) are increased on the coverage of the sam-ples relatively.
Therefore, if features (2a) (2b) wereremoved, it causes the recall rate to fall because ofthe low coverage of the samples.6 Related WorksWith difference from our method, there have beenresearches which are based on the assumption ofthe sentence alignments for parallel corpora (Galeand Church, 1991; Kitamura and Matsumoto, 1996;Melamed, 1997).
(Gale and Church, 1991) has usedthe ?2 statistics as the correspondence level of theword pairs and has showed that it was more effectivethan the mutual information.
(Kitamura and Mat-sumoto, 1996) has used the Dice coefficient (Kayand Ro?schesen, 1993) which was weighted by thelogarithm of the frequency of the word pair as theTable 2: Examples of translation pairs extracted by our methodJapanese English 	chairman of a special program committeeofficially retired as 	fffiflffi !"would like to say an official farewell30 #$!%'&()my thirty years of experience*+, -fl.
/%sharpen up on my golfTable 3: Precision rate and recall rate when each kind of features is removedFeature Num.
Outputs Corrects Precision (%) Recall (%)(1a) 94,511 891 686 77.0 (?4.1) 68.6 (?0.4)(1b) 94,511 1,058 719 68.0 (?13.1) 71.9 (+2.9)(1) 189,022 1,237 756 61.1 (?20.0) 75.6 (+6.6)(2a) 1 742 611 82.3 (+1.3) 61.1 (?7.9)(2b) 1 755 600 79.5 (?1.6) 60.0 (?9.0)(2) 2 489 404 82.6 (+1.5) 40.4 (?28.6)(3a) 4 846 685 81.0 (?0.1) 68.5 (?0.5)(3b) 4 834 660 79.1 (?1.9) 66.0 (?3.0)(3) 8 840 661 78.7 (?2.4) 66.1 (?2.9)(4a) 1,009 814 668 82.1 (+1.0) 66.8 (?2.2)(4b) 890 855 698 81.6 (+0.6) 69.8 (+0.8)(4) 1,899 838 689 82.2 (+1.1) 68.9 (?0.1)(5a) 1,009 844 683 80.9 (?0.2) 68.3 (?0.7)(5b) 890 851 688 80.8 (?0.3) 68.8 (?0.2)(5) 1,899 845 682 80.7 (?0.4) 68.2 (?0.8)All features 192,830 851 690 81.1 69.0correspondence level of the word pairs.
(Melamed,1997) has proposed the Competitive Linking Algo-rithm for linking the word pairs and a method whichcalculates the optimized correspondence level of theword pairs by hill climbing.These methods could archive high accuracy be-cause of the assumption of the sentence alignmentsfor parallel corpora, but they have the problem withnarrow applicable domains because there are not toomany parallel corpora with sentence alignments atpresent.
However, because our method does notrequire sentence alignments, it can be applied forwider applicable domains.Like our method, researches which are not basedon the assumption of the sentence alignments forparallel corpora have been done (Kaji and Aizono,1996; Tanaka and Iwasaki, 1996; Fung, 1997).They are based on the idea that the context ofthe words which appear in neighborhood lookslike each other for the translation pairs althoughexpressed in two different languages.
(Kaji andAizono, 1996) has proposed the correspondencelevel calculated by the size of intersection betweenco-occurrence sets with the word included in an ex-isting translation dictionary.
(Tanaka and Iwasaki,1996) has proposed a method for obtaining thebilingual word pairs by optimizing the matrix of thetranslation probabilities so that the distance of thematrices of the probabilities of co-occurrences ofwords which appeared in each language might be-come small.
(Fung, 1997) has calculated the vectorsin which the weighted mutual information betweenthe word in the corpora and the word included in anexisting translation dictionary was an element, andhas used these inner products as the correspondencelevel of word pairs.There is a common point between these methodand ours on the idea that the context of the wordswhich appear in neighborhood looks like each otherfor the translation pairs because features (1b) arebased on the same idea.
However, since our methodcaught extracting the translation pairs as the ap-proach of the statistical machine learning, it couldbe expected to improve the performance by addingnew features to the translation model.
In addition,if learning the translation model for the trainingsamples is done once with our method, the modelneed not be learned again for new samples althoughit needs the positive and negative samples for thetraining data.
However, the methods introducedabove must learn a new model again for new cor-pora.
(Sato and Nakanishi, 1998) has proposed amethod for learning a probabilistic translationmodel with Maximum Entropy (ME) modelingwhich was the same approach of the statistical ma-chine learning as SVMs, in which co-occurrenceinformation and morphological information wereused as features and has archived 58.25 % accuracywith 4,119 features.
ME modeling might be similarto SVMs on using features for learning a model, butfeature selection for ME modeling is more difficultbecause ME modeling is easier to cause over-fit fortraining samples than SVMs.
In addition, ME mod-eling cannot learn dependencies between features,but SVMs can learn them automatically using a ker-nel function.
Therefore, SVMs could learn morecomplex and effective model than ME modeling.7 ConclusionIn this paper, we proposed a learning and ex-tracting method of bilingual word sequence corre-spondences from non-aligned parallel corpora withSVMs.
Our method used features for the transla-tion model which use the translation dictionary, thenumber of words, the part-of-speech, constituentwords and neighbor words.
Experiment results inwhich Japanese and English parallel corpora areused archived 81.1 % precision rate and 69.0 %recall rate of the extracted translation pairs.
Thisdemonstrates that our method could reduce the costfor making translation dictionaries.AcknowledgmentsWe would like to thank Nihon Keizai Shimbun Inc.for giving us the research application permission ofthe English Business Letter Example Collection.ReferencesPascale Fung.
1997.
Finding terminology translationfrom non-parallel corpora.
In Proceeding of the 5thWorkshop on Very Large Corpora, pages 192?202.William A. Gale and Kenneth W. Church.
1991.
Identi-fying word correspondances in parallel texts.
In Pro-ceedings of the 2nd Speech and Natural LanguageWorkshop, pages 152?157.Thorsten Joachims.
1998.
Text categorization with sup-port vector machines: Learning with many relevantfeatures.
In the 10th European Conference on Ma-chine Learning, pages 137?142.Hiroyuki Kaji and Toshiko Aizono.
1996.
Extractingword correspondences from bilingual corpora basedon word co-occurrence information.
In Proceedingsof the 16th International Conference on Computa-tional Linguistics, pages 23?28.Martin Kay and Martin Ro?schesen.
1993.
Text-translation alignment.
Computational Linguistics,19(1):121?142.Mihoko Kitamura and Yuji Matsumoto.
1996.
Auto-matic extraction of word sequence correspondences inparallel corpora.
In Proceeding of the 4th Workshopon Very Large Corpora, pages 78?89.Taku Kudo and Yuji Matsumoto.
2000a.
Japanese de-pendency structure analysis based on support vectormachines.
In Proceedings of the 2000 Joint SIGDATConference on Emprical Methods in Natural Lan-guage Processing and Very Large Corpora, pages 18?25, Hong Kong, October.Taku Kudo and Yuji Matsumoto.
2000b.
Use of supportvector learning for chunk identification.
In Proceed-ings of the 4th Conference on Computational NaturalLanguage Learning and the 2nd Learning Languagein Logic Workshop, pages 142?144, Lisbon, Septem-ber.I.
Dan Melamed.
1997.
A word-to-word model of trans-lation equivalence.
In Proceedings of the 35th AnnualMeeting of the Association for Computational Lin-guistics, pages 490?497.Kengo Sato and Masakazu Nakanishi.
1998.
Maximumentropy model learning of the translation rules.
InProceedings of the 36th Annual Meeting of the Asso-ciation for Computational Linguistics and the 17th In-ternational Conference on Computational Linguistics,pages 1171?1175, August.Alexander J. Smola, Peter J. Bartlett, bernha Scho?lkopf,and Dale Schuurmans, editors.
2000.
Advances inLarge Margin Classifiers.
MIT Press.Hirotoshi Taira and Masahiko Haruno.
1999.
Featureselection in svm text categorization.
In Proceedingsof the 16th National Conference of the American As-socitation of Artificial Intelligence, pages 480?486,Florida, July.Kumiko Tanaka and Hideya Iwasaki.
1996.
Extractionof lexical translatins from non-aligned corpora.
InProceedings of the 16th International Conference onComputational Linguistics, pages 580?585.Vladimir Naumovich Vapnik.
1999.
The Nature of Sta-tistical Learning Theory (Statistics for Engineeringand Information Seience).
Springer-Verlag Telos, 2ndedition, December.
