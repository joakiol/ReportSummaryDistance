Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 294?303,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsMinimally Supervised Event Causality IdentificationQuang Xuan Do Yee Seng Chan Dan RothDepartment of Computer ScienceUniversity of Illinois at Urbana-ChampaignUrbana, IL 61801, USA{quangdo2,chanys,danr}@illinois.eduAbstractThis paper develops a minimally supervisedapproach, based on focused distributional sim-ilarity methods and discourse connectives,for identifying of causality relations betweenevents in context.
While it has been shownthat distributional similarity can help identify-ing causality, we observe that discourse con-nectives and the particular discourse relationthey evoke in context provide additional in-formation towards determining causality be-tween events.
We show that combining dis-course relation predictions and distributionalsimilarity methods in a global inference pro-cedure provides additional improvements to-wards determining event causality.1 IntroductionAn important part of text understanding arises fromunderstanding the semantics of events described inthe narrative, such as identifying the events that arementioned and how they are related semantically.For instance, when given a sentence ?The policearrested him because he killed someone.
?, humansunderstand that there are two events, triggered bythe words ?arrested?
and ?killed?, and that there isa causality relationship between these two events.Besides being an important component of discourseunderstanding, automatically identifying causal re-lations between events is important for various nat-ural language processing (NLP) applications suchas question answering, etc.
In this work, we auto-matically detect and extract causal relations betweenevents in text.Despite its importance, prior work on eventcausality extraction in context in the NLP litera-ture is relatively sparse.
In (Girju, 2003), the au-thor used noun-verb-noun lexico-syntactic patternsto learn that ?mosquitoes cause malaria?, where thecause and effect mentions are nominals and not nec-essarily event evoking words.
In (Sun et al, 2007),the authors focused on detecting causality betweensearch query pairs in temporal query logs.
(Beamerand Girju, 2009) tried to detect causal relations be-tween verbs in a corpus of screen plays, but limitedthemselves to consecutive, or adjacent verb pairs.In (Riaz and Girju, 2010), the authors first clustersentences into topic-specific scenarios, and then fo-cus on building a dataset of causal text spans, whereeach span is headed by a verb.
Thus, their focus wasnot on identifying causal relations between events ina given text document.In this paper, given a text document, we first iden-tify events and their associated arguments.
We thenidentify causality or relatedness relations betweenevent pairs.
To do this, we develop a minimally su-pervised approach using focused distributional sim-ilarity methods, such as co-occurrence counts ofevents collected automatically from an unannotatedcorpus, to measure and predict existence of causal-ity relations between event pairs.
Then, we build onthe observation that discourse connectives and theparticular discourse relation they evoke in contextprovide additional information towards determiningcausality between events.
For instance, in the ex-ample sentence provided at the beginning of thissection, the words ?arrested?
and ?killed?
probablyhave a relatively high apriori likelihood of being ca-294sually related.
However, knowing that the connec-tive ?because?
evokes a contingency discourse re-lation between the text spans ?The police arrestedhim?
and ?he killed someone?
provides further ev-idence towards predicting causality.
The contribu-tions of this paper are summarized below:?
Our focus is on identifying causality betweenevent pairs in context.
Since events are of-ten triggered by either verbs (e.g.
?attack?)
ornouns (e.g.
?explosion?
), we allow for detec-tion of causality between verb-verb, verb-noun,and noun-noun triggered event pairs.
To thebest of our knowledge, this formulation of thetask is novel.?
We developed a minimally supervised ap-proach for the task using focused distributionalsimilarity methods that are automatically col-lected from an unannotated corpus.
We showthat our approach achieves better performancethan two approaches: one based on a frequentlyused metric that measures association, and an-other based on the effect-control-dependency(ECD) metric described in a prior work (Riazand Girju, 2010).?
We leverage on the interactions between eventcausality prediction and discourse relationsprediction.
We combine these knowledgesources through a global inference procedure,which we formalize via an Integer Linear Pro-gramming (ILP) framework as a constraint op-timization problem (Roth and Yih, 2004).
Thisallows us to easily define appropriate con-straints to ensure that the causality and dis-course predictions are coherent with each other,thereby improving the performance of causalityidentification.2 Event CausalityIn this work, we define an event as an action or oc-currence that happens with associated participantsor arguments.
Formally, we define an event eas: p(a1, a2, .
.
.
, an), where the predicate p is theword that triggers the presence of e in text, anda1, a2, .
.
.
, an are the arguments associated withe.
Examples of predicates could be verbs such as?attacked?, ?employs?, nouns such as ?explosion?,?protest?, etc., and examples of the arguments of?attacked?
could be its subject and object nouns.To measure the causality association between apair of events ei and ej (in general, ei and ejcould be extracted from the same or different doc-uments), we should use information gathered abouttheir predicates and arguments.
A simple approachwould be to directly calculate the pointwise mu-tual information (PMI)1 between pi(ai1, ai2, .
.
.
, ain)and pj(aj1, aj2, .
.
.
, ajm).
However, this leads to verysparse counts as the predicate pi with its list of ar-guments ai1, .
.
.
, ain would rarely co-occur (withinsome reasonable context distance) with predicate pjand its entire list of arguments aj1, .
.
.
, ajm.
Hence,in this work, we measure causality association us-ing three separate components and focused distribu-tional similarity methods collected about event pairsas described in the rest of this section.2.1 Cause-Effect AssociationWe measure the causality or cause-effect association(CEA) between two events ei and ej using the fol-lowing equation:CEA(ei, ej) =spp(ei, ej) + spa(ei, ej) + saa(ei, ej) (1)where spp measures the association between eventpredicates, spa measures the association between thepredicate of an event and the arguments of the otherevent, and saa measures the association betweenevent arguments.
In our work, we regard each evente as being triggered and rooted at a predicate p.2.1.1 Predicate-Predicate AssociationWe define spp as follows:spp(ei, ej) = PMI(pi, pj)?max(ui, uj)?IDF (pi, pj)?Dist(pi, pj) (2)which takes into account the PMI between pred-icates pi and pj of events ei and ej respectively,as well as various other pieces of information.
InSuppes?
Probabilistic theory of Casuality (Suppes,1970), he highlighted that event e is a possible causeof event e?, if e?
happens more frequently with e than1PMI is frequently used to measure association betweenvariables.295by itself, i.e.
P (e?|e) > P (e?).
This can be easilyrewritten as P (e,e?
)P (e)P (e?)
> 1, similar to the definitionof PMI:PMI(e, e?)
= log P (e, e?
)P (e)P (e?
)which is only positive when P (e,e?
)P (e)P (e?)
> 1.Next, we build on the intuition that event predi-cates appearing in a large number of documents areprobably not important or discriminative.
Thus, wepenalize these predicates when calculating spp byadopting the inverse document frequency (idf):IDF (pi, pj) = idf(pi)?
idf(pj)?
idf(pi, pj),where idf(p) = log D1+N , D is the total number ofdocuments in the collection and N is the number ofdocuments that p occurs in.We also award event pairs that are closer together,while penalizing event pairs that are further apart intexts, by incorporating the distance measure of Lea-cock and Chodorow (1998), which was originallyused to measure similarity between concepts:Dist(pi, pj) = ?log |sent(pi)?
sent(pj)|+ 12?
ws ,where sent(p) gives the sentence number (index) inwhich p occurs and ws indicates the window-size(of sentences) used.
If pi and pj are drawn from thesame sentence, the numerator of the above fractionwill return 1.
In our work, we set ws to 3 and thus,if pi occurs in sentence k, the furthest sentence thatpj will be drawn from, is sentence k + 2.The final component of Equation 2, max(ui, uj),takes into account whether predicates (events) pi andpj appear most frequently with each other.
ui and ujare defined as follows:ui = P (pi, pj)maxk[P (pi, pk)]?
P (pi, pj) + uj = P (pi, pj)maxk[P (pk, pj)]?
P (pi, pj) +  ,where we set  = 0.01 to avoid zeros in the denom-inators.
ui will be maximized if there is no otherpredicate pk having a higher co-occurrence proba-bility with pi, i.e.
pk = pj .
uj is treated similarly.2.1.2 Predicate-Argument andArgument-Argument AssociationWe define spa as follows:spa(ei, ej) =1|Aej |?a?AejPMI(pi, a)+ 1|Aei |?a?AeiPMI(pj , a), (3)where Aei and Aej are the sets of arguments of eiand ej respectively.Finally, we define saa as follows:saa(ei, ej) =1|Aei ||Aej |?a?Aei?a?
?AejPMI(a, a?)
(4)Together, spa and saa provide additional contextsand robustness (in addition to spp) for measuring thecause-effect association between events ei and ej .Our formulation of CEA is inspired by the ECDmetric defined in (Riaz and Girju, 2010):ECD(a, b) = max(v, w)?
?log dis(a, b)2?maxDistance , (5)wherev = P (a, b)P (b)?
P (a, b) +  ?P (a, b)maxt[P (a, bt)]?
P (a, b) + w= P (a, b)P (a)?
P (a, b) +  ?P (a, b)maxt[P (at, b)]?
P (a, b) +  ,where ECD(a,b) measures the causality between twoevents a and b (headed by verbs), and the sec-ond component in the ECD equation is similar toDist(pi, pj).
In our experiments, we will evaluatethe performance of ECD against our proposed ap-proach.So far, our definitions in this section are genericand allow for any list of event argument types.
Inthis work, we focus on two argument types: agent(subject) and patient (object), which are typical corearguments of any event.
We describe how we extractevent predicates and their associated arguments inthe section below.3 Verbal and Nominal PredicatesWe consider that events are not only triggered byverbs but also by nouns.
For a verb (verbal predi-cate), we extract its subject and object from its as-sociated dependency parse.
On the other hand, since296events are also frequently triggered by nominal pred-icates, it is important to identify an appropriate listof event triggering nouns.
In our work, we gatheredsuch a list using the following approach:?
We first gather a list of deverbal nouns from theset of most frequently occurring (in the Giga-word corpus) 3,000 verbal predicate types.
Foreach verb type v, we go through all its Word-Net2 senses and gather all its derivationally re-lated nouns Nv 3.?
From Nv, we heuristically remove nouns thatare less than three characters in length.
We alsoremove nouns whose first three characters aredifferent from the first three characters of v. Foreach of the remaining nouns in Nv, we mea-sured its Levenstein (edit) distance from v andkeep the noun(s) with the minimum distance.When multiple nouns have the same minimumdistance from v, we keep all of them.?
To further prune the list of nouns, we next re-moved all nouns ending in ?er?, ?or?, or ?ee?,as these nouns typically refer to a person, e.g.
?writer?, ?doctor?, ?employee?.
We also re-move nouns that are not hyponyms (children)of the first WordNet sense of the noun ?event?4.?
Since we are concerned with nouns denotingevents, FrameNet (Ruppenhofer et al, 2010)(FN) is a good resource for mining such nouns.FN consists of frames denoting situations andevents.
As part of the FN resource, each FNframe consists of a list of lexical units (mainlyverbs and nouns) representing the semantics ofthe frame.
Various frame-to-frame relations arealso defined (in particular the inheritance re-lation).
Hence, we gathered all the childrenframes of the FN frame ?Event?.
From thesechildren frames, we then gathered all their nounlexical units (words) and add them to our list of2http://wordnet.princeton.edu/3The WordNet resource provides derivational informationon words that are in different syntactic (i.e.
part-of-speech) cat-egories, but having the same root (lemma) form and that aresemantically related.4The first WordNet sense of the noun ?event?
has the mean-ing: ?something that happens at a given place and time?nouns.
Finally, we also add a few nouns denot-ing natural disaster from Wikipedia5.Using the above approach, we gathered a list ofabout 2,000 noun types.
This current approach isheuristics based which we intend to improve in thefuture, and any such improvements should subse-quently improve the performance of our causalityidentification approach.Event triggering deverbal nouns could have as-sociated arguments (for instance, acting as subject,object of the deverbal noun).
To extract these ar-guments, we followed the approach of (Gurevichet al, 2008).
Briefly, the approach uses linguisticpatterns to extract subjects and objects for deverbalnouns, using information from dependency parses.For more details, we refer the reader to (Gurevich etal., 2008).4 Discourse and CausalityDiscourse connectives are important for relating dif-ferent text spans, helping us to understand a piece oftext in relation to its context:[The police arrested him] because [he killed someone].In the example sentence above, the discourse con-nective (?because?)
and the discourse relation itevokes (in this case, the Cause relation) allows read-ers to relate its two associated text spans, ?The po-lice arrested him?
and ?he killed someone?.
Also,notice that the verbs ?arrested?
and ?killed?, whichcross the two text spans, are causally related.
Toaid in extracting causal relations, we leverage on theidentification of discourse relations to provide addi-tional contextual information.To identify discourse relations, we use the PennDiscourse Treebank (PDTB) (Prasad et al, 2007),which contains annotations of discourse relationsin context.
The annotations are done over theWall Street Journal corpus and the PDTB adopts apredicate-argument view of discourse relations.
Adiscourse connective (e.g.
because) takes two textspans as its arguments.
In the rest of this section,we briefly describe the discourse relations in PDTBand highlight how we might leverage them to aid indetermining event causality.5http://en.wikipedia.org/wiki/Natural disaster297Coarse-grained relations Fine-grained relationsComparison Concession, Contrast, Pragmatic-concession, Pragmatic-contrastContingency Cause, Condition, Pragmatic-cause, Pragmatic-conditionExpansion Alternative, Conjunction, Exception, Instantiation, List, RestatementTemporal Asynchronous, SynchronousTable 1: Coarse-grained and fine-grained discourse relations.4.1 Discourse RelationsPDTB contains annotations for four coarse-graineddiscourse relation types, as shown in the left columnof Table 1.
Each of these are further refined intoseveral fine-grained discourse relations, as shown inthe right column of the table.6 Next, we briefly de-scribe these relations, highlighting those that couldpotentially help to determine event causality.Comparison A Comparison discourse relationbetween two text spans highlights prominent differ-ences between the situations described in the textspans.
An example sentence is:Contrast: [According to the survey, x% of Chinese Inter-net users prefer Google] whereas [y% prefer Baidu].According to the PDTB annotation manual(Prasad et al, 2007), the truth of both spans is in-dependent of the established discourse relation.
Thismeans that the text spans are not causally related andthus, the existence of a Comparison relation shouldimply that there is no causality relation across thetwo text spans.Contingency A Contingency relation betweentwo text spans indicates that the situation describedin one text span causally influences the situation inthe other.
An example sentence is:Cause: [The first priority is search and rescue] because[many people are trapped under the rubble].Existence of a Contingency relation potentiallyimplies that there exists at least one causal eventpair crossing the two text spans.
The PDTB an-notation manual states that while the Cause andCondition discourse relations indicate casual influ-ence in their text spans, there is no causal in-fluence in the text spans of the Pragmatic-causeand Pragmatic-condition relations.
For instance,Pragmatic-condition indicates that one span pro-6PDTB further refines these fine-grained relations into a fi-nal third level of relations, but we do not use them in this work.vides the context in which the description of the sit-uation in the other span is relevant; for example:Pragmatic-condition: If [you are thirsty], [there?s beer inthe fridge].Hence, there is a need to also identify fine-graineddiscourse relations.Expansion Connectives evoking Expansion dis-course relations expand the discourse, such as byproviding additional information, illustrating alter-native situations, etc.
An example sentence is:Conjunction: [Over the past decade, x women werekilled] and [y went missing].Most of the Expansion fine-grained relations (ex-cept for Conjunction, which could connect arbitrarypieces of text spans) should not contain causality re-lations across its text spans.Temporal These indicate that the situations de-scribed in the text spans are related temporally.
Anexample sentence is:Synchrony: [He was sitting at his home] when [the wholeworld started to shake].Temporal precedence of the (cause) event over the(effect) event is a necessary, but not sufficient req-uisite for causality.
Hence by itself, Temporal re-lations are probably not discriminative enough fordetermining event causality.4.2 Discourse Relation Extraction SystemOur work follows the approach and features de-scribed in the state-of-the-art Ruby-based discoursesystem of (Lin et al, 2010), to build an in-house Java-based discourse relation extraction sys-tem.
Our system identifies explicit connectives intext, predict their discourse relations, as well as theirassociated text spans.
Similar to (Lin et al, 2010),we achieved a competitive performance of slightlyover 80% F1-score in identifying fine-grained rela-tions for explicit connectives.
Our system is devel-oped using the Learning Based Java modeling lan-298guage (LBJ) (Rizzolo and Roth, 2010) and will bemade available soon.
Due to space constraints, werefer interested readers to (Lin et al, 2010) for de-tails on the features, etc.In the example sentences given thus far in this sec-tion, all the connectives were explicit, as they appearin the texts.
PDTB also provides annotations for im-plicit connectives, which we do not use in this work.Identifying implicit connectives is a harder task andincorporating these is a possible future work.5 Joint Inference for Causality ExtractionTo exploit the interactions between event paircausality extraction and discourse relation identifi-cation, we define appropriate constraints betweenthem, which can be enforced through the Con-strained Conditional Models framework (aka ILP forNLP) (Roth and Yih, 2007; Chang et al, 2008).
Indoing this, the predictions of CEA (Section 2.1) andthe discourse system are forced to cohere with eachother.
More importantly, this should improve theperformance of using only CEA to extract causalevent pairs.
To the best of our knowledge, this ap-proach for causality extraction is novel.5.1 CEA & Discourse: Implementation DetailsLet E denote the set of event mentions in a docu-ment.
Let EP = {(ei, ej) ?
E ?
E | ei ?
E , ej ?E , i < j, |sent(ei) ?
sent(ej)| ?
2} denote theset of event mention pairs in the document, wheresent(e) gives the sentence number in which event eoccurs.
Note that in this work, we only extract eventpairs that are at most two sentences apart.
Next, wedefine LER = {?causal?, ??
causal?}
to be the set ofevent relation labels that an event pair ep ?
EP canbe associated with.Note that the CEA metric as defined in Section 2.1simply gives a score without it being bounded to bebetween 0 and 1.0.
However, to use the CEA scoreas part of the inference process, we require that it bebounded and thus can be used as a binary prediction,that is, predicting an event pair as causal or ?causal.To enable this, we use a few development documentsto automatically find a threshold CEA score that sep-arates scores indicating causal vs ?causal.
Basedon this threshold, the original CEA scores are thenrescaled to fall within 0 to 1.0.
More details on thisare in Section 6.2.Let C denote the set of connective mentions in adocument.
We slightly modify our discourse sys-tem as follows.
We define LDR to be the set ofdiscourse relations.
We initially add all the fine-grained discourse relations listed in Table 1 to LDR.In the PDTB corpus, some connective examples arelabeled with just a coarse-grained relation, with-out further specifying a fine-grained relation.
Toaccommodate these examples, we add the coarse-grained relations Comparison, Expansion, and Tem-poral to LDR.
We omit the coarse-grained Con-tingency relation from LDR, as we want to sepa-rate Cause and Condition from Pragmatic-cause andPragmatic-condition.
This discards very few exam-ples as only a very small number of connective ex-amples are simply labeled with a Contingency labelwithout further specifying a fine-grained label.
Wethen retrained our discourse system to predict labelsin LDR.5.2 ConstraintsWe now describe the constraints used to supportjoint inference, based on the predictions of the CEAmetric and the discourse classifier.
Let sc(dr) bethe probability that connective c is predicated to beof discourse relation dr, based on the output of ourdiscourse classifier.
Let sep(er) be the CEA pre-diction score (rescaled to range in [0,1]) that eventpair ep takes on the causal or ?causal label er.
Letx?c,dr?
be a binary indicator variable which takes onthe value 1 iff c is labeled with the discourse relationdr.
Similarly, let y?ep,er?
be a binary variable whichtakes on the value 1 iff ep is labeled as er.
We thendefine our objective function as follows:max[|LDR|?c?C?dr?LDRsc(dr) ?
x?c,dr?+|LER|?ep?EP?er?LERsep(er) ?
y?ep,er?
](6)subject to the following constraints:?dr?LDRx?c,dr?
= 1 ?c ?
C (7)?er?LERy?ep,er?
= 1 ?ep ?
EP (8)x?c,dr?
?
{0, 1} ?c ?
C, dr ?
LDR (9)y?ep,er?
?
{0, 1} ?ep ?
EP, er ?
LER(10)299Equation (7) requires that each connective c canonly be assigned one discourse relation.
Equation(8) requires that each event pair ep can only becausal or ?causal.
Equations (9) and (10) indicatethat x?c,dr?
and y?ep,er?
are binary variables.To capture the relationship between event paircausality and discourse relations, we use the follow-ing constraints:x?c,?Cause??
??ep?EPcy?ep,?causal??
(11)x?c,?Condition??
??ep?EPcy?ep,?causal?
?, (12)where both equations are defined ?c ?
C. EPc isdefined to be the set of event pairs that cross the twotext spans associated with c. For instance, if the firsttext span of c contains two event mentions ei, ej ,and there is one event mention ek in the second textspan of c, then EPc = {(ei, ek), (ej , ek)}.
Finally,the logical form of Equation (11) can be written as:x?c,?Cause??
?
y?epi,?causal??
?
.
.
.
?
y?epj ,?causal?
?,where epi, .
.
.
, epj are elements in EPc.
This statesthat if we assign the Cause discourse label to c,then at least one of epi, .
.
.
, epj must be assigned ascausal.
The interpretation of Equation (12) is simi-lar.We use two more constraints to capture the inter-actions between event causality and discourse rela-tions.
First, we defined Cep as the set of connectivesc enclosing each event of ep in each of its text spans,i.e.
: one of the text spans of c contain one of theevent in ep, while the other text span of c contain theother event in ep.
Next, based on the discourse rela-tions in Section 4.1, we propose that when an eventpair ep is judged to be causal, then the connectivec that encloses it should be evoking one of the dis-course relations in LDRa = {?Cause?, ?Condition?,?Temporal?, ?Asynchronous?, ?Synchrony?, ?Con-junction?}.
We capture this using the following con-straint:y?ep,?causal??
??dra?LDRax?c,dra?
?c ?
Cep (13)The logical form of Equation (13) can be written as:y?ep,?causal??
?
x?c,?Cause??
?
x?c,?Condition??
.
.
.
?x?c,?Conjunction??.
This states that if we assign ep ascausal, then we must assign to c one of the labels inLDRa .Finally, we propose that for any connectives evok-ing discourse relations LDRb = {?Comparison?,?Concession?, ?Contrast?, ?Pragmatic-concession?,?Pragmatic-contrast?, ?Expansion?, ?Alternative?,?Exception?, ?Instantiation?, ?List?, ?Restate-ment?
}, any event pair(s) that it encloses should be?causal.
We capture this using the following con-straint:x?c,drb?
?
y?ep,??causal???
drb ?
LDRb , ep ?
EPc, (14)where the logical form of Equation (14) can be writ-ten as: x?c,drb?
?
y?ep,??causal?
?.6 Experiments6.1 Experimental SettingsTo collect the distributional statistics for measuringCEA as defined in Equation (1), we applied part-of-speech tagging, lemmatization, and dependencyparsing (Marneffe et al, 2006) on about 760K docu-ments in the English Gigaword corpus (LDC catalognumber LDC2003T05).We are not aware of any benchmark corpus forevaluating event causality extraction in contexts.Hence, we created an evaluation corpus using thefollowing process: Using news articles collectedfrom CNN7 during the first three months of 2010, werandomly selected 20 articles (documents) as evalu-ation data, and 5 documents as development data.Two annotators annotated the documents forcausal event pairs, using two simple notions forcausality: the Cause event should temporally pre-cede the Effect event, and the Effect event occurs be-cause the Cause event occurs.
However, sometimesit is debatable whether two events are involved in acausal relation, or whether they are simply involvedin an uninteresting temporal relation.
Hence, we al-lowed annotations of C to indicate causality, and Rto indicate relatedness (for situations when the exis-tence of causality is debatable).
The annotators willsimply identify and annotate the C or R relations be-tween predicates of event pairs.
Event arguments arenot explicitly annotated, although the annotators arefree to look at the entire document text while mak-ing their annotation decisions.
Finally, they are free7http://www.cnn.com300System Rec% Pre% F1%PMIpp 26.6 20.8 23.3ECDpp &PMIpa,aa 40.9 23.5 29.9CEA 62.2 28.0 38.6CEA+Discourse 65.1 30.7 41.7Table 2: Performance of baseline systems and our ap-proaches on extracting Causal event relations.System Rec% Pre% F1%PMIpp 27.8 24.9 26.2ECDpp &PMIpa,aa 42.4 28.5 34.1CEA 63.1 33.7 43.9CEA+Discourse 65.3 36.5 46.9Table 3: Performance of the systems on extracting Causaland Related event relations.to annotate relations between predicates that haveany number of sentences in between and are not re-stricted to a fixed sentence window-size.After adjudication, we obtained a total of 492C+R relation annotations, and 414C relation anno-tations on the evaluation documents.
On the devel-opment documents, we obtained 92 C+R and 71 Crelation annotations.
The annotators overlapped on10 evaluation documents.
On these documents, thefirst (second) annotator annotated 215 (199) C + Rrelations, agreeing on 166 of these relations.
To-gether, they annotated 248 distinct relations.
Us-ing this number, their agreement ratio would be 0.67(166/248).
The corresponding agreement ratio forC relations is 0.58.
These numbers highlight thatcausality identification is a difficult task, as therecould be as many as N2 event pairs in a document(N is the number of events in the document).
Weplan to make this annotated dataset available soon.86.2 EvaluationAs mentioned in Section 5.1, to enable translat-ing (the unbounded) CEA scores into binary causal,?causal predictions, we need to rescale or calibratethese scores to range in [0,1].
To do this, we firstrank all the CEA scores of all event pairs in the de-velopment documents.
Most of these event pairs willbe ?causal.
Based on the relation annotations inthese development documents, we scanned through8http://cogcomp.cs.illinois.edu/page/publication view/6630510152025303540455055605  10  15  20  25  30  35  40Precision(%)K (number of causality predictions)Precision(%) on top K event causality predictionsCEAECDpp & PMIpa,aaPMIppFigure 1: Precision of the top K causality C predictions.this ranked list of scores to locate the CEA scoret that gives the highest F1-score (on the develop-ment documents) when used as a threshold betweencausal vs ?causal decisions.
We then ranked allthe CEA scores of all event pairs gathered from the760K Gigaword documents, discretized all scoreshigher than t into B bins, and all scores lower thant into B bins.
Together, these 2B bins represent therange [0,1].
We used B = 500.
Thus, consecu-tive bins represent a difference of 0.001 in calibratedscores.To measure the causality between a pair ofevents ei and ej , a simple baseline is to calculatePMI(pi, pj).
Using a similar thresholding and cali-bration process to translate PMI(pi, pj) scores intobinary causality decisions, we obtained a F1 score of23.1 when measured over the causality C relations,as shown in the row PMIpp of Table 2.As mentioned in Section 2.1.2, Riaz and Girju(2010) proposed the ECD metric to measurecausality between two events.
Thus, as a point ofcomparison, we replaced spp of Equation (1) withECD(a, b) of Equation (5), substituting a = pi andb = pj .
After thresholding and calibrating the scoresof this approach, we obtained a F1-score of 29.7, asshown in the row ECDpp&PMIpa,aa of Table 2.Next, we evaluated our proposed CEA approachand obtained a F1-score of 38.6, as shown in the rowCEA of Table 2.
Thus, our proposed approach ob-tained significantly better performance than the PMIbaseline and the ECD approach.
Next, we per-formed joint inference with the discourse relationpredictions as described in Section 5 and obtained301an improved F1-score of 41.7.
We note that we ob-tained improvements in both recall and precision.This means that with the aid of discourse relations,we are able to recover more causal relations, as wellas reduce false-positive predictions.Constraint Equations (11) and (12) help to re-cover causal relations.
For improvements in pre-cision, as stated in the last paragraph of Section5.2, identifying other discourse relations such as?Comparison?, ?Contrast?, etc., provides counter-evidence to causality.
Together with constraintEquation (14), this helps to eliminate false-positiveevent pairs as classified by CEA and contributestowards CEA+Discourse having a higher precisionthan CEA.The corresponding results for extracting bothcausality and relatedness C + R relations are givenin Table 3.
For these experiments, the aim was for amore relaxed evaluation and we simply collapsed Cand R into a single label.Finally, we also measured the precision of thetop K causality C predictions, showing the preci-sion trends in Figure 1.
As shown, CEA in generalachieves higher precision when compared toPMIppand ECDpp&PMIpa,aa.
The trends for C+R pre-dictions are similar.Thus far, we had included both verbal and nom-inal predicates in our evaluation.
When we repeatthe experiments for ECDpp&PMIpa,aa and CEAon just verbal predicates, we obtained the respectiveF1-scores of 31.8 and 38.3 on causality relations.The corresponding F1-scores for casuality and relat-edness relations are 35.7 and 43.3.
These absoluteF1-scores are similar to those in Tables 2 and 3, dif-fering by 1-2%.7 AnalysisWe randomly selected 50 false-positive predictionsand 50 false-negative causality relations to analyzethe mistakes made by CEA.Among the false-positives (precision errors), themost frequent error type (56% of the errors) is thatCEA simply assigns a high score to event pairs thatare not causal; more knowledge sources are requiredto support better predictions in these cases.
The nextlargest group of error (22%) involves events contain-ing pronouns (e.g.
?he?, ?it?)
as arguments.
Ap-plying coreference to replace these pronouns withtheir canonical entity strings or labeling them withsemantic class information might be useful.Among the false-negatives (recall errors), 23%of the errors are due to CEA simply assigning alow score to causal event pairs and more contex-tual knowledge seems necessary for better predic-tions.
19% of the recall errors arises from causalevent pairs involving nominal predicates that are notin our list of event evoking noun types (described inSection 3).
A related 17% of recall errors involvesnominal predicates without any argument.
For these,less information is available for CEA to make pre-dictions.
The remaining group (15% of errors) in-volves events containing pronouns as arguments.8 Related WorkAlthough prior work in event causality extractionin context is relatively sparse, there are many priorworks concerning other semantic aspects of eventextraction.
Ji and Grishman (2008) extracts eventmentions (belonging to a predefined list of targetevent types) and their associated arguments.
In otherprior work (Chen et al, 2009; Bejan and Harabagiu,2010), the authors focused on identifying anothertype of event pair semantic relation: event corefer-ence.
Chambers and Jurafsky (2008; 2009) chainevents sharing a common (protagonist) participant.They defined events as verbs and given an existingchain of events, they predict the next likely event in-volving the protagonist.
This is different from ourtask of detecting causality between arbitrary eventpairs that might or might not share common argu-ments.
Also, we defined events more broadly, asthose that are triggered by either verbs or nouns.
Fi-nally, although our proposed CEA metric has resem-blance the ECD metric in (Riaz and Girju, 2010), ourtask is different from theirs and our work differs inmany aspects.
They focused on building a dataset ofcausal text spans, whereas we focused on identifyingcausal relations between events in a given text doc-ument.
They considered text spans headed by verbswhile we considered events triggered by both verbsand nouns.
Moreover, we combined event causalityprediction and discourse relation prediction througha global inference procedure to further improve theperformance of event causality prediction.3029 ConclusionIn this paper, using general tools such as the depen-dency and discourse parsers which are not trainedspecifically towards our target task, and a minimalset of development documents for threshold tuning,we developed a minimally supervised approach toidentify causality relations between events in con-text.
We also showed how to incorporate discourserelation predictions to aid event causality predictionsthrough a global inference procedure.
There are sev-eral interesting directions for future work, includingthe incorporation of other knowledge sources suchas coreference and semantic class predictions, whichwere shown to be potentially important in our er-ror analysis.
We could also use discourse relationsto aid in extracting other semantic relations betweenevents.AcknowledgmentsThe authors thank the anonymous reviewers for theirinsightful comments and suggestions.
University ofIllinois at Urbana-Champaign gratefully acknowl-edges the support of Defense Advanced ResearchProjects Agency (DARPA) Machine Reading Pro-gram under Air Force Research Laboratory (AFRL)prime contract No.
FA8750-09-C-0181.
The firstauthor thanks the Vietnam Education Foundation(VEF) for its sponsorship.
Any opinions, findings,and conclusion or recommendations expressed inthis material are those of the authors and do not nec-essarily reflect the view of the VEF, DARPA, AFRL,or the US government.ReferencesBrandon Beamer and Roxana Girju.
2009.
Using a bi-gram event model to predict causal potential.
In CI-CLING.Cosmin Adrian Bejan and Sanda Harabagiu.
2010.
Un-supervised event coreference resolution with rich lin-guistic features.
In ACL.Nathanael Chambers and Dan Jurafsky.
2008.
Unsuper-vised learning of narrative event chains.
In ACL-HLT.Nathanael Chambers and Dan Jurafsky.
2009.
Unsuper-vised learning of narrative schemas and their partici-pants.
In ACL.Ming-Wei Chang, Lev Ratinov, Nicholas Rizzolo, andDan Roth.
2008.
Learning and inference with con-straints.
In AAAI.Zheng Chen, Heng Ji, and Robert Haralick.
2009.
Apairwise event coreference model, feature impact andevaluation for event coreference resolution.
In RANLPworkshop on Events in Emerging Text Types.Roxana Girju.
2003.
Automatic detection of causal re-lations for question answering.
In ACL workshop onMultilingual Summarization and Question Answering.Olga Gurevich, Richard Crouch, Tracy Holloway King,and Valeria de Paiva.
2008.
Deverbal nouns in knowl-edge representation.
Journal of Logic and Computa-tion, 18, June.Heng Ji and Ralph Grishman.
2008.
Refining event ex-traction through unsupervised cross-document infer-ence.
In ACL.Claudia Leacock and Martin Chodorow, 1998.
Combin-ing Local Context and WordNet Similarity for WordSense Identification.
MIT Press.Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2010.A pdtb-styled end-to-end discourse parser.
Tech-nical report.
http://www.comp.nus.edu.sg/ linzi-hen/publications/tech2010.pdf.Marie-catherine De Marneffe, Bill Maccartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InLREC.Rashmi Prasad, Eleni Miltsakaki, Nikhil Dinesh,Alan Lee, Aravind Joshi, Livio Robaldo, andBonnie Webber.
2007.
The penn discourse tree-bank 2.0 annotation manual.
Technical report.http://www.seas.upenn.edu/ pdtb/PDTBAPI/pdtb-annotation-manual.pdf.Mehwish Riaz and Roxana Girju.
2010.
Another look atcausality: Discovering scenario-specific contingencyrelationships with no supervision.
In ICSC.N.
Rizzolo and D. Roth.
2010.
Learning based java forrapid development of nlp systems.
In LREC.Dan Roth and Wen Tau Yih.
2004.
A linear program-ming formulation for global inference in natural lan-guage tasks.
In CoNLL.Dan Roth and Wen Tau Yih.
2007.
Global inference forentity and relation identification via a linear program-ming formulation.
In Lise Getoor and Ben Taskar, ed-itors, Introduction to Statistical Relational Learning.MIT Press.Josef Ruppenhofer, Michael Ellsworth, Miriam R. L.Petruck, Christopher R. Johnson, and Jan Scheffczyk.2010.
FrameNet II: Extended Theory and Practice.http://framenet.icsi.berkeley.edu.Yizhou Sun, Ning Liu, Kunqing Xie, Shuicheng Yan,Benyu Zhang, and Zheng Chen.
2007.
Causal rela-tion of queries from temporal logs.
In WWW.Patrick Suppes.
1970.
A Probabilistic Theory of Causal-ity.
Amsterdam: North-Holland Publishing Company.303
