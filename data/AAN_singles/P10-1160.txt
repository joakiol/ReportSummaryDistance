Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1583?1592,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsBeyond NomBank:A Study of Implicit Arguments for Nominal PredicatesMatthew Gerber and Joyce Y. ChaiDepartment of Computer ScienceMichigan State UniversityEast Lansing, Michigan, USA{gerberm2,jchai}@cse.msu.eduAbstractDespite its substantial coverage, Nom-Bank does not account for all within-sentence arguments and ignores extra-sentential arguments altogether.
These ar-guments, which we call implicit, are im-portant to semantic processing, and theirrecovery could potentially benefit manyNLP applications.
We present a study ofimplicit arguments for a select group offrequent nominal predicates.
We show thatimplicit arguments are pervasive for thesepredicates, adding 65% to the coverage ofNomBank.
We demonstrate the feasibil-ity of recovering implicit arguments witha supervised classification model.
Our re-sults and analyses provide a baseline forfuture work on this emerging task.1 IntroductionVerbal and nominal semantic role labeling (SRL)have been studied independently of each other(Carreras and Ma`rquez, 2005; Gerber et al, 2009)as well as jointly (Surdeanu et al, 2008; Hajic?
etal., 2009).
These studies have demonstrated thematurity of SRL within an evaluation setting thatrestricts the argument search space to the sentencecontaining the predicate of interest.
However, asshown by the following example from the PennTreeBank (Marcus et al, 1993), this restriction ex-cludes extra-sentential arguments:(1) [arg0 The two companies] [pred produce][arg1 market pulp, containerboard and whitepaper].
The goods could be manufacturedcloser to customers, saving [pred shipping]costs.The first sentence in Example 1 includes the Prop-Bank (Kingsbury et al, 2002) analysis of the ver-bal predicate produce, where arg0 is the agentiveproducer and arg1 is the produced entity.
The sec-ond sentence contains an instance of the nominalpredicate shipping that is not associated with argu-ments in NomBank (Meyers, 2007).From the sentences in Example 1, the reader caninfer that The two companies refers to the agents(arg0) of the shipping predicate.
The reader canalso infer that market pulp, containerboard andwhite paper refers to the shipped entities (arg1of shipping).1 These extra-sentential argumentshave not been annotated for the shipping predi-cate and cannot be identified by a system that re-stricts the argument search space to the sentencecontaining the predicate.
NomBank also ignoresmany within-sentence arguments.
This is shownin the second sentence of Example 1, where Thegoods can be interpreted as the arg1 of shipping.These examples demonstrate the presence of argu-ments that are not included in NomBank and can-not easily be identified by systems trained on theresource.
We refer to these arguments as implicit.This paper presents our study of implicit ar-guments for nominal predicates.
We began ourstudy by annotating implicit arguments for a se-lect group of predicates.
For these predicates, wefound that implicit arguments add 65% to the ex-isting role coverage of NomBank.2 This increasehas implications for tasks (e.g., question answer-ing, information extraction, and summarization)that benefit from semantic analysis.
Using our an-notations, we constructed a feature-based modelfor automatic implicit argument identification thatunifies standard verbal and nominal SRL.
Our re-sults indicate a 59% relative (15-point absolute)gain in F1 over an informed baseline.
Our analy-ses highlight strengths and weaknesses of the ap-proach, providing insights for future work on thisemerging task.1In PropBank and NomBank, the interpretation of eachrole (e.g., arg0) is specific to a predicate sense.2Role coverage indicates the percentage of roles filled.1583In the following section, we review related re-search, which is historically sparse but recentlygaining traction.
We present our annotation effortin Section 3, and follow with our implicit argu-ment identification model in Section 4.
In Section5, we describe the evaluation setting and presentour experimental results.
We analyze these resultsin Section 6 and conclude in Section 7.2 Related workPalmer et al (1986) made one of the earliest at-tempts to automatically recover extra-sententialarguments.
Their approach used a fine-grained do-main model to assess the compatibility of candi-date arguments and the slots needing to be filled.A phenomenon similar to the implicit argu-ment has been studied in the context of Japaneseanaphora resolution, where a missing case-markedconstituent is viewed as a zero-anaphoric expres-sion whose antecedent is treated as the implicit ar-gument of the predicate of interest.
This behaviorhas been annotated manually by Iida et al (2007),and researchers have applied standard SRL tech-niques to this corpus, resulting in systems thatare able to identify missing case-marked expres-sions in the surrounding discourse (Imamura etal., 2009).
Sasano et al (2004) conducted sim-ilar work with Japanese indirect anaphora.
Theauthors used automatically derived nominal caseframes to identify antecedents.
However, as notedby Iida et al, grammatical cases do not stand ina one-to-one relationship with semantic roles inJapanese (the same is true for English).Fillmore and Baker (2001) provided a detailedcase study of implicit arguments (termed null in-stantiations in that work), but did not provide con-crete methods to account for them automatically.Previously, we demonstrated the importance of fil-tering out nominal predicates that take no local ar-guments (Gerber et al, 2009); however, this workdid not address the identification of implicit ar-guments.
Burchardt et al (2005) suggested ap-proaches to implicit argument identification basedon observed coreference patterns; however, the au-thors did not implement and evaluate such meth-ods.
We draw insights from all three of thesestudies.
We show that the identification of im-plicit arguments for nominal predicates leads tofuller semantic interpretations when compared totraditional SRL methods.
Furthermore, motivatedby Burchardt et al, our model uses a quantitativeanalysis of naturally occurring coreference pat-terns to aid implicit argument identification.Most recently, Ruppenhofer et al (2009) con-ducted SemEval Task 10, ?Linking Events andTheir Participants in Discourse?, which evaluatedimplicit argument identification systems over acommon test set.
The task organizers annotatedimplicit arguments across entire passages, result-ing in data that cover many distinct predicates,each associated with a small number of annotatedinstances.
In contrast, our study focused on a se-lect group of nominal predicates, each associatedwith a large number of annotated instances.3 Data annotation and analysis3.1 Data annotationImplicit arguments have not been annotated withinthe Penn TreeBank, which is the textual and syn-tactic basis for NomBank.
Thus, to facilitateour study, we annotated implicit arguments forinstances of nominal predicates within the stan-dard training, development, and testing sections ofthe TreeBank.
We limited our attention to nom-inal predicates with unambiguous role sets (i.e.,senses) that are derived from verbal role sets.
Wethen ranked this set of predicates using two piecesof information: (1) the average difference betweenthe number of roles expressed in nominal form (inNomBank) versus verbal form (in PropBank) and(2) the frequency of the nominal form in the cor-pus.
We assumed that the former gives an indica-tion as to how many implicit roles an instance ofthe nominal predicate might have.
The product of(1) and (2) thus indicates the potential prevalenceof implicit arguments for a predicate.
To focus ourstudy, we ranked the predicates in NomBank ac-cording to this product and selected the top ten,shown in Table 1.We annotated implicit arguments document-by-document, selecting all singular and plural nounsderived from the predicates in Table 1.
For eachmissing argument position of each predicate in-stance, we inspected the local discourse for a suit-able implicit argument.
We limited our attention tothe current sentence as well as all preceding sen-tences in the document, annotating all mentions ofan implicit argument within this window.In the remainder of this paper, we will use iargnto refer to an implicit argument position n. Wewill use argn to refer to an argument provided byPropBank or NomBank.
We will use p to mark1584Pre-annotation Post-annotationRole averagePredicate # Role coverage (%) Noun Verb Role coverage (%) Noun role averageprice 217 42.4 1.7 1.7 55.3 2.2sale 185 24.3 1.2 2.0 42.0 2.1investor 160 35.0 1.1 2.0 54.6 1.6fund 109 8.7 0.4 2.0 21.6 0.9loss 104 33.2 1.3 2.0 46.9 1.9plan 102 30.9 1.2 1.8 49.3 2.0investment 102 15.7 0.5 2.0 33.3 1.0cost 101 26.2 1.1 2.3 47.5 1.9bid 88 26.9 0.8 2.2 72.0 2.2loan 85 22.4 1.1 2.5 41.2 2.1Overall 1,253 28.0 1.1 2.0 46.2 1.8Table 1: Predicates targeted for annotation.
The second column gives the number of predicate instancesannotated.
Pre-annotation numbers only include NomBank annotations, whereas Post-annotation num-bers include NomBank and implicit argument annotations.
Role coverage indicates the percentage ofroles filled.
Role average indicates how many roles, on average, are filled for an instance of a predicate?snoun form or verb form within the TreeBank.
Verbal role averages were computed using PropBank.predicate instances.
Below, we give an exampleannotation for an instance of the investment predi-cate:(2) [iarg0 Participants] will be able to transfer[iarg1 money] to [iarg2 other investmentfunds].
The [p investment] choices arelimited to [iarg2 a stock fund and amoney-market fund].NomBank does not associate this instance of in-vestment with any arguments; however, we wereable to identify the investor (iarg0), the thing in-vested (iarg1), and two mentions of the thing in-vested in (iarg2).Our data set was also independently annotatedby an undergraduate linguistics student.
For eachmissing argument position, the student was askedto identify the closest acceptable implicit argu-ment within the current and preceding sentences.The argument position was left unfilled if no ac-ceptable constituent could be found.
For a miss-ing argument position, the student?s annotationagreed with our own if both identified the sameconstituent or both left the position unfilled.
Anal-ysis indicated an agreement of 67% using Cohen?skappa coefficient (Cohen, 1960).3.2 Annotation analysisRole coverage for a predicate instance is equal tothe number of filled roles divided by the numberof roles in the predicate?s lexicon entry.
Role cov-erage for the marked predicate in Example 2 is0/3 for NomBank-only arguments and 3/3 whenthe annotated implicit arguments are also consid-ered.
Returning to Table 1, the third column givesrole coverage percentages for NomBank-only ar-guments.
The sixth column gives role coveragepercentages when both NomBank arguments andthe annotated implicit arguments are considered.Overall, the addition of implicit arguments createda 65% relative (18-point absolute) gain in role cov-erage across the 1,253 predicate instances that weannotated.The predicates in Table 1 are typically associ-ated with fewer arguments on average than theircorresponding verbal predicates.
When consid-ering NomBank-only arguments, this difference(compare columns four and five) varies from zero(for price) to a factor of five (for fund).
When im-plicit arguments are included in the comparison,these differences are reduced and many nominalpredicates express approximately the same num-ber of arguments on average as their verbal coun-terparts (compare the fifth and seventh columns).In addition to role coverage and average count,we examined the location of implicit arguments.Figure 1 shows that approximately 56% of the im-plicit arguments in our data can be resolved withinthe sentence containing the predicate.
The remain-ing implicit arguments require up to forty-six sen-15850.40.50.60.70.80.910 2 4 6 8 10 12 18 28 46Sentences priorImplicitargumentsresolvedFigure 1: Location of implicit arguments.
Formissing argument positions with an implicit filler,the y-axis indicates the likelihood of the filler be-ing found at least once in the previous x sentences.tences for resolution; however, a vast majority ofthese can be resolved within the previous few sen-tences.
Section 6 discusses implications of thisskewed distribution.4 Implicit argument identification4.1 Model formulationIn our study, we assumed that each sentence in adocument had been analyzed for PropBank andNomBank predicate-argument structure.
Nom-Bank includes a lexicon listing the possible ar-gument positions for a predicate, allowing us toidentify missing argument positions with a simplelookup.
Given a nominal predicate instance p witha missing argument position iargn, the task is tosearch the surrounding discourse for a constituentc that fills iargn.
Our model conducts this searchover all constituents annotated by either PropBankor NomBank with non-adjunct labels.A candidate constituent c will often form acoreference chain with other constituents in thediscourse.
Consider the following abridged sen-tences, which are adjacent in their Penn TreeBankdocument:(3) [Mexico] desperately needs investment.
(4) Conservative Japanese investors are put offby [Mexico?s] investment regulations.
(5) Japan is the fourth largest investor in[c Mexico], with 5% of the total[p investments].NomBank does not associate the labeled instanceof investment with any arguments, but it is clearfrom the surrounding discourse that constituent c(referring to Mexico) is the thing being invested in(the iarg2).
When determining whether c is theiarg2 of investment, one can draw evidence fromother mentions in c?s coreference chain.
Example3 states that Mexico needs investment.
Example4 states that Mexico regulates investment.
Thesepropositions, which can be derived via traditionalSRL analyses, should increase our confidence thatc is the iarg2 of investment in Example 5.Thus, the unit of classification for a candi-date constituent c is the three-tuple ?p, iargn, c?
?,where c?
is a coreference chain comprising c andits coreferent constituents.3 We defined a binaryclassification function Pr(+| ?p, iargn, c??)
thatpredicts the probability that the entity referred toby c fills the missing argument position iargn ofpredicate instance p. In the remainder of this pa-per, we will refer to c as the primary filler, dif-ferentiating it from other mentions in the corefer-ence chain c?.
In the following section, we presentthe feature set used to represent each three-tuplewithin the classification function.4.2 Model featuresStarting with a wide range of features, we per-formed floating forward feature selection (Pudilet al, 1994) over held-out development data com-prising implicit argument annotations from section24 of the Penn TreeBank.
As part of the featureselection process, we conducted a grid search forthe best per-class cost within LibLinear?s logisticregression solver (Fan et al, 2008).
This was doneto reduce the negative effects of data imbalance,which is severe even when selecting candidatesfrom the current and previous few sentences.
Ta-ble 2 shows the selected features, which are quitedifferent from those used in our previous work toidentify traditional semantic arguments (Gerber etal., 2009).4 Below, we give further explanationsfor some of the features.Feature 1 models the semantic role relationshipbetween each mention in c?
and the missing argu-ment position iargn.
To reduce data sparsity, thisfeature generalizes predicates and argument posi-tions to their VerbNet (Kipper, 2005) classes and3We used OpenNLP for coreference identification:http://opennlp.sourceforge.net4We have omitted many of the lowest-ranked features.Descriptions of these features can be obtained by contactingthe authors.1586# Feature value description1* For every f , the VerbNet class/role of pf /argf concatenated with the class/role of p/iargn.2* Average pointwise mutual information between ?p, iargn?
and any ?pf , argf ?.3 Percentage of all f that are definite noun phrases.4 Minimum absolute sentence distance from any f to p.5* Minimum pointwise mutual information between ?p, iargn?
and any ?pf , argf ?.6 Frequency of the nominal form of p within the document that contains it.7 Nominal form of p concatenated with iargn.8 Nominal form of p concatenated with the sorted integer argument indexes from all argn of p.9 Number of mentions in c?.10* Head word of p?s right sibling node.11 For every f , the synset (Fellbaum, 1998) for the head of f concatenated with p and iargn.12 Part of speech of the head of p?s parent node.13 Average absolute sentence distance from any f to p.14* Discourse relation whose two discourse units cover c (the primary filler) and p.15 Number of left siblings of p.16 Whether p is the head of its parent node.17 Number of right siblings of p.Table 2: Features for determining whether c fills iargn of predicate p. For each mention f (denoting af iller) in the coreference chain c?, we define pf and argf to be the predicate and argument position of f .Features are sorted in descending order of feature selection gain.
Unless otherwise noted, all predicateswere normalized to their verbal form and all argument positions (e.g., argn and iargn) were interpretedas labels instead of word content.
Features marked with an asterisk are explained in Section 4.2.semantic roles using SemLink.5 For explanationpurposes, consider again Example 1, where we aretrying to fill the iarg0 of shipping.
Let c?
containa single mention, The two companies, which is thearg0 of produce.
As described in Table 2, fea-ture 1 is instantiated with a value of create.agent-send.agent, where create and send are the VerbNetclasses that contain produce and ship, respectively.In the conversion to LibLinear?s instance repre-sentation, this instantiation is converted into a sin-gle binary feature create.agent-send.agent whosevalue is one.
Features 1 and 11 are instantiatedonce for each mention in c?, allowing the modelto consider information from multiple mentions ofthe same entity.Features 2 and 5 are inspired by the workof Chambers and Jurafsky (2008), who inves-tigated unsupervised learning of narrative eventsequences using pointwise mutual information(PMI) between syntactic positions.
We used a sim-ilar PMI score, but defined it with respect to se-mantic arguments instead of syntactic dependen-cies.
Thus, the values for features 2 and 5 arecomputed as follows (the notation is explained in5http://verbs.colorado.edu/semlinkthe caption for Table 2):pmi(?p, iargn?
, ?pf , argf ?)
=logPcoref (?p, iargn?
, ?pf , argf ?
)Pcoref (?p, iargn?
, ?
)Pcoref (?pf , argf ?
, ?
)(6)To compute Equation 6, we first labeled a subset ofthe Gigaword corpus (Graff, 2003) using the ver-bal SRL system of Punyakanok et al (2008) andthe nominal SRL system of Gerber et al (2009).We then identified coreferent pairs of argumentsusing OpenNLP.
Suppose the resulting data hasN coreferential pairs of argument positions.
Alsosuppose that M of these pairs comprise ?p, argn?and ?pf , argf ?.
The numerator in Equation 6 isdefined as MN .
Each term in the denominator isobtained similarly, except that M is computed asthe total number of coreference pairs compris-ing an argument position (e.g., ?p, argn?)
and anyother argument position.
Like Chambers and Ju-rafsky, we also used the discounting method sug-gested by Pantel and Ravichandran (2004) for low-frequency observations.
The PMI score is some-what noisy due to imperfect output, but it providesinformation that is useful for classification.1587Feature 10 does not depend on c?
and is specificto each predicate.
Consider the following exam-ple:(7) Statistics Canada reported that its [arg1industrial-product] [p price] index dropped2% in September.The ?
[p price] index?
collocation is rarely associ-ated with an arg0 in NomBank or with an iarg0 inour annotations (both argument positions denotethe seller).
Feature 10 accounts for this type of be-havior by encoding the syntactic head of p?s rightsibling.
The value of feature 10 for Example 7 isprice:index.
Contrast this with the following:(8) [iarg0 The company] is trying to preventfurther [p price] drops.The value of feature 10 for Example 8 isprice:drop.
This feature captures an important dis-tinction between the two uses of price: the for-mer rarely takes an iarg0, whereas the latter oftendoes.
Features 12 and 15-17 account for predicate-specific behaviors in a similar manner.Feature 14 identifies the discourse relation (ifany) that holds between the candidate constituentc and the filled predicate p. Consider the followingexample:(9) [iarg0 SFE Technologies] reported a net lossof $889,000 on sales of $23.4 million.
(10) That compared with an operating [p loss] of[arg1 $1.9 million] on sales of $27.4 millionin the year-earlier period.In this case, a comparison discourse relation (sig-naled by the underlined text) holds between thefirst and sentence sentence.
The coherence pro-vided by this relation encourages an inference thatidentifies the marked iarg0 (the loser).
Through-out our study, we used gold-standard discourse re-lations provided by the Penn Discourse TreeBank(Prasad et al, 2008).5 EvaluationWe trained the feature-based logistic regressionmodel over 816 annotated predicate instances as-sociated with 650 implicitly filled argument posi-tions (not all predicate instances had implicit ar-guments).
During training, a candidate three-tuple?p, iargn, c??
was given a positive label if the can-didate implicit argument c (the primary filler) wasannotated as filling the missing argument position.To factor out errors from standard SRL analyses,the model used gold-standard argument labels pro-vided by PropBank and NomBank.
As shown inFigure 1 (Section 3.2), implicit arguments tend tobe located in close proximity to the predicate.
Wefound that using all candidate constituents cwithinthe current and previous two sentences workedbest on our development data.We compared our supervised model with thesimple baseline heuristic defined below:6Fill iargn for predicate instance pwith the nearest constituent in the two-sentence candidate window that fillsargn for a different instance of p, whereall nominal predicates are normalized totheir verbal forms.The normalization allows an existing arg0 for theverb invested to fill an iarg0 for the noun in-vestment.
We also evaluated an oracle modelthat made gold-standard predictions for candidateswithin the two-sentence prediction window.We evaluated these models using the methodol-ogy proposed by Ruppenhofer et al (2009).
Foreach missing argument position of a predicate in-stance, the models were required to either (1) iden-tify a single constituent that fills the missing argu-ment position or (2) make no prediction and leavethe missing argument position unfilled.
We scoredpredictions using the Dice coefficient, which is de-fined as follows:2 ?
|Predicted?True||Predicted|+ |True|(11)Predicted is the set of tokens subsumed by theconstituent predicted by the model as filling amissing argument position.
True is the set oftokens from a single annotated constituent thatfills the missing argument position.
The model?sprediction receives a score equal to the maxi-mum Dice overlap across any one of the annotatedfillers.
Precision is equal to the summed predic-tion scores divided by the number of argument po-sitions filled by the model.
Recall is equal to thesummed prediction scores divided by the numberof argument positions filled in our annotated data.Predictions not covering the head of a true fillerwere assigned a score of zero.6This heuristic outperformed a more complicated heuris-tic that relied on the PMI score described in section 4.2.1588Baseline Discriminative Oracle# Imp.
# P R F1 P R F1 p R F1sale 64 60 50.0 28.3 36.2 47.2 41.7 44.2 0.118 80.0 88.9price 121 53 24.0 11.3 15.4 36.0 32.6 34.2 0.008 88.7 94.0investor 78 35 33.3 5.7 9.8 36.8 40.0 38.4 < 0.001 91.4 95.5bid 19 26 100.0 19.2 32.3 23.8 19.2 21.3 0.280 57.7 73.2plan 25 20 83.3 25.0 38.5 78.6 55.0 64.7 0.060 82.7 89.4cost 25 17 66.7 23.5 34.8 61.1 64.7 62.9 0.024 94.1 97.0loss 30 12 71.4 41.7 52.6 83.3 83.3 83.3 0.020 100.0 100.0loan 11 9 50.0 11.1 18.2 42.9 33.3 37.5 0.277 88.9 94.1investment 21 8 0.0 0.0 0.0 40.0 25.0 30.8 0.182 87.5 93.3fund 43 6 0.0 0.0 0.0 14.3 16.7 15.4 0.576 50.0 66.7Overall 437 246 48.4 18.3 26.5 44.5 40.4 42.3 < 0.001 83.1 90.7Table 3: Evaluation results.
The second column gives the number of predicate instances evaluated.The third column gives the number of ground-truth implicitly filled argument positions for the predicateinstances (not all instances had implicit arguments).
P , R, and F1 indicate precision, recall, and F-measure (?
= 1), respectively.
p-values denote the bootstrapped significance of the difference in F1between the baseline and discriminative models.
Oracle precision (not shown) is 100% for all predicates.Our evaluation data comprised 437 predicate in-stances associated with 246 implicitly filled ar-gument positions.
Table 3 presents the results.Predicates with the highest number of implicit ar-guments - sale and price - showed F1 increasesof 8 points and 18.8 points, respectively.
Over-all, the discriminative model increased F1 perfor-mance 15.8 points (59.6%) over the baseline.We measured human performance on this taskby running our undergraduate assistant?s annota-tions against the evaluation data.
Our assistantachieved an overall F1 score of 58.4% using thesame candidate window as the baseline and dis-criminative models.
The difference in F1 betweenthe discriminative and human results had an ex-act p-value of less than 0.001.
All significancetesting was performed using a two-tailed bootstrapmethod similar to the one described by Efron andTibshirani (1993).6 Discussion6.1 Feature ablationWe conducted an ablation study to measure thecontribution of specific feature sets.
Table 4presents the ablation configurations and results.For each configuration, we retrained and retestedthe discriminative model using the features de-scribed.
As shown, we observed significant losseswhen excluding features that relate the seman-tic roles of mentions in c?
to the semantic rolePercent change (p-value)Configuration P R F1Remove 1,2,5 -35.3(< 0.01)-36.1(< 0.01)-35.7(< 0.01)Use 1,2,5 only -26.3(< 0.01)-11.9(0.05)-19.2(< 0.01)Remove 14 0.2(0.95)1.0(0.66)0.7(0.73)Table 4: Feature ablation results.
The first columnlists the feature configurations.
All changes arepercentages relative to the full-featured discrimi-native model.
p-values for the changes are indi-cated in parentheses.of the missing argument position (first configura-tion).
The second configuration tested the effect ofusing only the SRL-based features.
This also re-sulted in significant performance losses, suggest-ing that the other features contribute useful infor-mation.
Lastly, we tested the effect of removingdiscourse relations (feature 14), which are likelyto be difficult to extract reliably in a practical set-ting.
As shown, this feature did not have a statis-tically significant effect on performance and couldbe excluded in future applications of the model.6.2 Unclassified true implicit argumentsOf all the errors made by the system, approxi-mately 19% were caused by the system?s failure to1589generate a candidate constituent c that was a cor-rect implicit argument.
Without such a candidate,the system stood no chance of identifying a cor-rect implicit argument.
Two factors contributed tothis type of error, the first being our assumptionthat implicit arguments are also core (i.e., argn)arguments to traditional SRL structures.
Approxi-mately 8% of the overall error was due to a failureof this assumption.
In many cases, the true im-plicit argument filled a non-core (i.e., adjunct) rolewithin PropBank or NomBank.More frequently, however, true implicit argu-ments were missed because the candidate windowwas too narrow.
This accounts for 12% of theoverall error.
Oracle recall (second-to-last col-umn in Table 3) indicates the nominals that suf-fered most from windowing errors.
For exam-ple, the sale predicate was associated with thehighest number of true implicit arguments, butonly 80% of those could be resolved within thetwo-sentence candidate window.
Empirically, wefound that extending the candidate window uni-formly for all predicates did not increase perfor-mance on the development data.
The oracle re-sults suggest that predicate-specific window set-tings might offer some advantage.6.3 The investment and fund predicatesIn Section 4.2, we discussed the price predicate,which frequently occurs in the ?
[p price] index?collocation.
We observed that this collocationis rarely associated with either an overt arg0 oran implicit iarg0.
Similar observations can bemade for the investment and fund predicates.
Al-though these two predicates are frequent, they arerarely associated with implicit arguments: invest-ment takes only eight implicit arguments across its21 instances, and fund takes only six implicit ar-guments across its 43 instances.
This behavior isdue in large part to collocations such as ?
[p in-vestment] banker?, ?stock [p fund]?, and ?mutual[p fund]?, which use predicate senses that are noteventive.
Such collocations also violate our as-sumption that differences between the PropBankand NomBank argument structure for a predicateare indicative of implicit arguments (see Section3.1 for this assumption).Despite their lack of implicit arguments, it isimportant to account for predicates such as in-vestment and fund because incorrect prediction ofimplicit arguments for them can lower precision.This is precisely what happened for the fund pred-icate, where the model incorrectly identified manyimplicit arguments for ?stock [p fund]?
and ?mu-tual [p fund]?.
The left context of fund should helpthe model avoid this type of error; however, ourfeature selection process did not identify any over-all gains from including this information.6.4 Improvements versus the baselineThe baseline heuristic covers the simple casewhere identical predicates share arguments in thesame position.
Thus, it is interesting to examinecases where the baseline heuristic failed but thediscriminative model succeeded.
Consider the fol-lowing sentence:(12) Mr. Rogers recommends that [p investors]sell [iarg2 takeover-related stock].Neither NomBank nor the baseline heuristic asso-ciate the marked predicate in Example 12 with anyarguments; however, the feature-based model wasable to correctly identify the marked iarg2 as theentity being invested in.
This inference captured atendency of investors to sell the things they haveinvested in.We conclude our discussion with an example ofan extra-sentential implicit argument:(13) [iarg0 Olivetti] has denied that it violatedthe rules, asserting that the shipments wereproperly licensed.
However, the legality ofthese [p sales] is still an open question.As shown in Example 13, the system was able tocorrectly identify Olivetti as the agent in the sell-ing event of the second sentence.
This inferenceinvolved two key steps.
First, the system identifiedcoreferent mentions of Olivetti that participated inexporting and supplying events (not shown).
Sec-ond, the system identified a tendency for exportersand suppliers to also be sellers.
Using this knowl-edge, the system extracted information that couldnot be extracted by the baseline heuristic or a tra-ditional SRL system.7 Conclusions and future workCurrent SRL approaches limit the search for ar-guments to the sentence containing the predicateof interest.
Many systems take this assumptiona step further and restrict the search to the predi-cate?s local syntactic environment; however, pred-icates and the sentences that contain them rarely1590exist in isolation.
As shown throughout this paper,they are usually embedded in a coherent and se-mantically rich discourse that must be taken intoaccount.
We have presented a preliminary studyof implicit arguments for nominal predicates thatfocused specifically on this problem.Our contribution is three-fold.
First, we havecreated gold-standard implicit argument annota-tions for a small set of pervasive nominal predi-cates.7 Our analysis shows that these annotationsadd 65% to the role coverage of NomBank.
Sec-ond, we have demonstrated the feasibility of re-covering implicit arguments for many of the pred-icates, thus establishing a baseline for future workon this emerging task.
Third, our study suggestsa few ways in which this research can be movedforward.
As shown in Section 6, many errors werecaused by the absence of true implicit argumentswithin the set of candidate constituents.
More in-telligent windowing strategies in addition to al-ternate candidate sources might offer some im-provement.
Although we consistently observeddevelopment gains from using automatic coref-erence resolution, this process creates errors thatneed to be studied more closely.
It will also beimportant to study implicit argument patterns ofnon-verbal predicates such as the partitive percent.These predicates are among the most frequent inthe TreeBank and are likely to require approachesthat differ from the ones we pursued.Finally, any extension of this work is likely toencounter a significant knowledge acquisition bot-tleneck.
Implicit argument annotation is difficultbecause it requires both argument and coreferenceidentification (the data produced by Ruppenhoferet al (2009) is similar).
Thus, it might be produc-tive to focus future work on (1) the extraction ofrelevant knowledge from existing resources (e.g.,our use of coreference patterns from Gigaword) or(2) semi-supervised learning of implicit argumentmodels from a combination of labeled and unla-beled data.AcknowledgmentsWe would like to thank the anonymous review-ers for their helpful questions and comments.
Wewould also like to thank Malcolm Doering for hisannotation effort.
This work was supported in partby NSF grants IIS-0347548 and IIS-0840538.7Our annotation data can be freely downloaded athttp://links.cse.msu.edu:8000/lair/projects/semanticrole.htmlReferencesAljoscha Burchardt, Anette Frank, and ManfredPinkal.
2005.
Building text meaning representa-tions from contextually related frames - a case study.In Proceedings of the Sixth International Workshopon Computational Semantics.Xavier Carreras and Llu?
?s Ma`rquez.
2005.
Introduc-tion to the CoNLL-2005 shared task: Semantic rolelabeling.Nathanael Chambers and Dan Jurafsky.
2008.
Unsu-pervised learning of narrative event chains.
In Pro-ceedings of the Association for Computational Lin-guistics, pages 789?797, Columbus, Ohio, June.
As-sociation for Computational Linguistics.Jacob Cohen.
1960.
A coefficient of agreementfor nominal scales.
Educational and PsychologicalMeasurement, 20(1):3746.Bradley Efron and Robert J. Tibshirani.
1993.
An In-troduction to the Bootstrap.
Chapman & Hall, NewYork.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
LIBLINEAR:A Library for Large Linear Classification.
Journalof Machine Learning Research, 9:1871?1874.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database (Language, Speech, and Commu-nication).
The MIT Press, May.C.J.
Fillmore and C.F.
Baker.
2001.
Frame semanticsfor text understanding.
In Proceedings of WordNetand Other Lexical Resources Workshop, NAACL.Matthew Gerber, Joyce Y. Chai, and Adam Meyers.2009.
The role of implicit argumentation in nominalSRL.
In Proceedings of the North American Chap-ter of the Association for Computational Linguistics,pages 146?154, Boulder, Colorado, USA, June.David Graff.
2003.
English Gigaword.
LinguisticData Consortium, Philadelphia.Jan Hajic?, Massimiliano Ciaramita, Richard Johans-son, Daisuke Kawahara, Maria Anto`nia Mart?
?, Llu?
?sMa`rquez, Adam Meyers, Joakim Nivre, SebastianPado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,Nianwen Xue, and Yi Zhang.
2009.
The CoNLL-2009 shared task: Syntactic and semantic dependen-cies in multiple languages.
In Proceedings of theThirteenth Conference on Computational NaturalLanguage Learning (CoNLL 2009): Shared Task,pages 1?18, Boulder, Colorado, June.
Associationfor Computational Linguistics.Ryu Iida, Mamoru Komachi, Kentaro Inui, and YujiMatsumoto.
2007.
Annotating a Japanese text cor-pus with predicate-argument and coreference rela-tions.
In Proceedings of the Linguistic AnnotationWorkshop in ACL-2007, page 132139.1591Kenji Imamura, Kuniko Saito, and Tomoko Izumi.2009.
Discriminative approach to predicate-argument structure analysis with zero-anaphora res-olution.
In Proceedings of the ACL-IJCNLP 2009Conference Short Papers, pages 85?88, Suntec, Sin-gapore, August.
Association for Computational Lin-guistics.P.
Kingsbury, M. Palmer, and M. Marcus.
2002.Adding semantic annotation to the Penn TreeBank.In Proceedings of the Human Language TechnologyConference (HLT?02).Karin Kipper.
2005.
VerbNet: A broad-coverage, com-prehensive verb lexicon.
Ph.D. thesis, Departmentof Computer and Information Science University ofPennsylvania.Mitchell Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of English: the Penn TreeBank.
Computa-tional Linguistics, 19:313?330.Adam Meyers.
2007.
Annotation guidelines forNomBank - noun argument structure for PropBank.Technical report, New York University.Martha S. Palmer, Deborah A. Dahl, Rebecca J. Schiff-man, Lynette Hirschman, Marcia Linebarger, andJohn Dowding.
1986.
Recovering implicit infor-mation.
In Proceedings of the 24th annual meetingon Association for Computational Linguistics, pages10?19, Morristown, NJ, USA.
Association for Com-putational Linguistics.Patrick Pantel and Deepak Ravichandran.
2004.Automatically labeling semantic classes.
InDaniel Marcu Susan Dumais and Salim Roukos, ed-itors, HLT-NAACL 2004: Main Proceedings, pages321?328, Boston, Massachusetts, USA, May 2 -May 7.
Association for Computational Linguistics.Rashmi Prasad, Alan Lee, Nikhil Dinesh, Eleni Milt-sakaki, Geraud Campion, Aravind Joshi, and BonnieWebber.
2008.
Penn discourse treebank version 2.0.Linguistic Data Consortium, February.P.
Pudil, J. Novovicova, and J. Kittler.
1994.
Floatingsearch methods in feature selection.
Pattern Recog-nition Letters, 15:1119?1125.Vasin Punyakanok, Dan Roth, and Wen-tau Yih.
2008.The importance of syntactic parsing and infer-ence in semantic role labeling.
Comput.
Linguist.,34(2):257?287.Josef Ruppenhofer, Caroline Sporleder, RoserMorante, Collin Baker, and Martha Palmer.
2009.Semeval-2010 task 10: Linking events and theirparticipants in discourse.
In Proceedings ofthe Workshop on Semantic Evaluations: RecentAchievements and Future Directions (SEW-2009),pages 106?111, Boulder, Colorado, June.
Associa-tion for Computational Linguistics.Ryohei Sasano, Daisuke Kawahara, and Sadao Kuro-hashi.
2004.
Automatic construction of nominalcase frames and its application to indirect anaphoraresolution.
In Proceedings of Coling 2004, pages1201?1207, Geneva, Switzerland, Aug 23?Aug 27.COLING.Mihai Surdeanu, Richard Johansson, Adam Meyers,Llu?
?s Ma`rquez, and Joakim Nivre.
2008.
TheCoNLL 2008 shared task on joint parsing of syn-tactic and semantic dependencies.
In CoNLL 2008:Proceedings of the Twelfth Conference on Computa-tional Natural Language Learning, pages 159?177,Manchester, England, August.
Coling 2008 Orga-nizing Committee.1592
