Automatic Detect ion of Text GenreBret t  Kess le r  Geof f rey  Nunberg  H inr i ch  Schf i t zeXerox  Palo Alto Research  Center3333 Coyote  Hill RoadPalo Al to CA 94304 USADepar tment  o f  L ingu is t i csS tanford  Un ivers i tyS tanford  CA 94305-2150 USAemaih  {bkess le r ,nunberg ,schuetze}~parc .xerox .comURL: ftp://parcftp.xerox.com/pub/qca/papers/genreAbst rac tAs the text databases available to users be-come larger and more heterogeneous, genrebecomes increasingly important for com-putational linguistics as a complement totopical and structural principles of classifi-cation.
We propose a theory of genres asbundles of facets, which correlate with var-ious surface cues, and argue that genre de-tection based on surface cues is as success-ful as detection based on deeper structuralproperties.1 In t roduct ionComputational linguists have been concerned for themost part with two aspects of texts: their structureand their content.
That is.
we consider texts onthe one hand as formal objects, and on the otheras symbols with semantic or referential values.
Inthis paper we want to consider texts from the pointof view of genre: that is.
according to the variousfunctional roles they play.Genre is necessarily a heterogeneous classificatoryprinciple, which is based among other things on theway a text was created, the way it is distributed,the register of language it uses, and the kind of au-dience it is addressed to.
For all its complexity, thisattribute can be extremely important for many ofthe core problems that computational linguists areconcerned with.
Parsing accuracy could be increasedby taking genre into account (for example, certainobject-less constructions occur only in recipes in En-glish).
Similarly for POS-tagging (the frequency ofuses of trend as a verb in the Journal of Commerceis 35 times higher than in Sociological Abstracts).
Inword-sense disambiguation, many senses are largelyrestricted to texts of a particular style, such as col-loquial or formal (for example the word pretty is farmore likely to have the meaning "rather" in informalgenres than in formal ones).
In information retrieval,genre classification could enable users to sort searchresults according to their immediate interests.
Peo-ple who go into a bookstore or library are not usuallylooking simply for information about a particulartopic, but rather have requirements of genre as well:they are looking for scholarly articles about hypno-tism, novels about the French Revolution, editorialsabout the supercollider, and so forth.If genre classification isso useful, why hasn't it fig-ured much in computational linguistics before now?One important reason is that, up to now, the digi-tized corpora and collections which are the subjectof much CL research have been for the most partgenerically homogeneous (i.e., collections of scientificabstracts or newspaper articles, encyclopedias, andso on), so that the problem of genre identificationcould be set aside.
To a large extent, the problemsof genre classification don't become salient until weare confronted with large and heterogeneous searchdomains like the World-Wide Web.Another reason for the neglect of genre, though, isthat it can be a difficult notion to get a conceptualhandle on.
particularly in contrast with properties ofstructure or topicality, which for all their complica-tions involve well-explored territory.
In order to dosystematic work on automatic genre classification.by contrast, we require the answers to some basictheoretical and methodological questions.
Is genre asingle property or attribute that can be neatly laidout in some hierarchical structure?
Or are we reallytalking about a muhidimensional space of propertiesthat have little more in common than that they aremore or less orthogonal to topicality?
And once wehave the theoretical prerequisites in place, we haveto ask whether genre can be reliably identified bymeans of computationally tractable cues.In a broad sense, the word "genre" is merely aliterary substitute for "'kind of text," and discus-sions of literary classification stretch back to Aris-32totle.
We will use the term "'genre" here to re-fer to any widely recognized class of texts definedby some common communicative purpose or otherfunctional traits, provided the function is connectedto some formal cues or commonalities and that theclass is extensible.
For example an editorial is ashortish prose argument expressing an opinion onsome matter of immediate public concern, typicallywritten in an impersonal and relatively formal stylein which the author is denoted by the pronoun we.But we would probably not use the term "genre"to describe merely the class of texts that have theobjective of persuading someone to do something,since that class - -  which would include editorials,sermons, prayers, advertisements, and so forth - -has no distinguishing formal properties.
At the otherend of the scale, we would probably not use "genre"to describe the class of sermons by John Donne, sincethat class, while it has distinctive formal characteris-tics, is not extensible.
Nothing hangs in the balanceon this definition, but it seems to accord reasonablywell with ordinary usage.The traditional literature on genre is rich withclassificatory schemes and systems, some of whichmight in retrospect be analyzed as simple at-tribute systems.
(For general discussions of lit-erary theories of genre, see, e.g., Butcher (1932),Dubrow (1982), Fowler (1982), Frye (1957), Her-nadi (1972), Hobbes (1908), Staiger (1959), andTodorov (1978).)
We will refer here to the attributesused in classifying genres as GENERIC FACETS.
Afacet is simply a property which distinguishes a classof texts that answers to certain practical interests~and which is moreover associated with a characteris-tic set of computable structural or linguistic proper-ties, whether categorical or statistical, which we willdescribe as "generic cues."
In principle, a given textcan be described in terms of an indefinitely largenumber of facets.
For example, a newspaper storyabout a Balkan peace initiative is an example of aBROADCAST as opposed to DIRECTED communica-tion, a property that correlates formally with cer-tain uses of the pronoun you.
It is also an exampleof a NARRATIVE, as opposed to a DIRECTIVE (e.g..in a manual), SUASXVE (as in an editorial), or DE-SCRIPTIVE (as in a market survey) communication;and this facet correlates, among other things, witha high incidence of preterite verb forms.Apart from giving us a theoretical framework forunderstanding genres, facets offer two practical ad-vantages.
First.
some applications benefit from cat-egorization according to facet, not genre.
For ex-ample, in an information retrieval context, we willwant to consider the OPINION feature most highlywhen we are searching for public reactions to thesupercollider, where newspaper columns, editorials.and letters to the editor will be of roughly equal in-terest.
For other purposes we will want to stressnarrativity, for example in looking for accounts ofthe storming of the Bastille in either novels or his-tories.Secondly.
we can extend our classification to gen-res not previously encountered.
Suppose that weare presented with the unfamiliar category FINAN-CIAL ANALYSTS' REPORT.
By analyzing genres asbundles of facets, we can categorize this genre asINSTITUTIONAL (because of the use of we as in edi-torials and annual reports) and as NON-SUASIVE ornon-argumentative (because of the low incidence ofquestion marks, among other things), whereas a sys-tem trained on genres as atomic entities would notbe able to make sense of an unfamiliar category.1.1 P rev ious  Work  on Genre  Ident i f i ca t ionThe first linguistic research on genre that uses quan-titative methods is that of Biber (1986: 1988; 1992;1995), which draws on work on stylistic analysis,readability indexing, and differences between spo-ken and written language.
Biber ranks genres alongseveral textual "dimensions", which are constructedby applying factor analysis to a set of linguistic syn-tactic and lexical features.
Those dimensions arethen characterized in terms such as "informative vs.involved" or "'narrative vs.
non-narrative."
Factorsare not used for genre classification (the values of atext on the various dimensions are often not infor-mative with respect to genre).
Rather, factors areused to validate hypotheses about the functions ofvarious linguistic features.An important and more relevant set of experi-ments, which deserves careful attention, is presentedin Karlgren and Cutting {1994).
They too beginwith a corpus of hand-classified texts, the Browncorpus.
One difficulty here.
however, is that it isnot clear to what extent the Brown corpus classi-fication used in this work is relevant for practicalor theoretical purposes.
For example, the category"Popular Lore" contains an article by the decidedlyhighbrow Harold Rosenberg from Commentary.
andarticles from Model Railroader and Gourmet, surelynot a natural class by any reasonable standard.
Inaddition, many of the text features in Karlgren andCutting are structural cues that require tagging.
Wewill replace these cues with two new classes of cuesthat are easily computable: character-level cues anddeviation cues.332 Ident i fy ing  Genres :  Gener ic  CuesThis section discusses generic cues, the "'observable'"properties of a text that are associated with facets.2.1 St ructura l  CuesExamples of structural cues are passives, nominal-izations, topicalized sentences, and counts of the fre-quency of syntactic ategories (e.g.. part-of-speechtags).
These cues are not much discussed in the tra-ditional literature on genre, but have come to thefore in recent work (Biber, 1995; Karlgren and Cut-ting, 1994).
For purposes of automatic lassificationthey have the limitation that they require tagged orparsed texts.2.2 Lexical CuesMost facets are correlated with lexical cues.
Exam-ples of ones that we use are terms of address (e.g.,Mr., Ms.).
which predominate in papers like the New~brk Times: Latinate affixes, which signal certainhighbrow registers like scientific articles or scholarlyworks; and words used in expressing dates, which arecommon in certain types of narrative such as newsstories.2.3 Character -Level  CuesCharacter-level cues are mainly punctuation cuesand other separators and delimiters used to marktext categories like phrases, clauses, and sentences(Nunberg, 1990).
Such features have not been usedin previous work on genre recognition, but we be-lieve they have an important role to play, being atonce significant and very frequent.
Examples includecounts of question marks, exclamations marks, cap-italized and hyphenated words, and acronyms.2.4 Derivat ive CuesDerivative cues are ratios and variation measures de-rived from measures of lexical and character-levelfeatures.Ratios correlate in certain ways with genre, andhave been widely used in previous work.
We repre-sent ratios implicitly as sums of other cues by trans-forming all counts into natural ogarithms.
For ex-ample, instead of estimating separate weights o, 3,and 3' for the ratios words per sentence (averagesentence length), characters per word (average wordlength) and words per type (token/type ratio), re-spectively, we express this desired weighting:, I I '+ l  C+I  W+Ia log~+31og~+3,1og T+Ias follows:"(c~ - /3  + 7) log(W + 1)-a log(S + 1) + 31og(C + 1) - ~.
log(T + l)(where W = word tokens.
S = sentences.
C =char-acters, T = word types).
The 55 cues in our ex-periments can be combined to almost 3000 differentratios.
The log representation e sures that.
all theseratios are available implicitly while avoiding overfit-ting and the high computational cost of training ona large set of cues.Variation measures capture the amount of varia-tion of a certain count cue in a text (e.g.. the stan-dard deviation in sentence length).
This type of use-ful metric has not been used in previous work ongenre.The experiments in this paper are based on 55cues from the last three groups: lexical, character-level and derivative cues.
These cues are easily com-putable in contrast o the structural cues that havefigured prominently in previous work on genre.3 Method3.1 CorpusThe corpus of texts used for this study was theBrown Corpus.
For the reasons mentioned above,we used our own classification system, and elimi-nated texts that did not fall unequivocally into oneof our categories.
W'e ended up using 499 of the802 texts in the Brown Corpus.
(While the Corpuscontains 500 samples, many of the samples containseveral texts.
)For our experiments, we analyzed the texts interms of three categorical facets: BROW, NARRA-TIVE, and GENRE.
BROW characterizes a text interms of the presumptions made with respect o therequired intellectual background of the target au-dience.
Its levels are POPULAR, MIDDLE.
UPPER-MIDDLE, and HIGH.
For example, the mainstreamAmerican press is classified as MIDDLE and tabloidnewspapers as POPULAR.
The ,NARRATIVE facet isbinary, telling whether a text is written in a narra-tive mode, primarily relating a sequence of events.The GENRE facet has the values REPORTAGE, ED-ITORIAL, SCITECH, LEGAL.
NONFICTION, FICTION.The first two characterize two types of articles fromthe daily or weekly press: reportage and editorials.The level SCITECH denominates scientific or techni-cal writings, and LEGAL characterizes various typesof writings about law and government administra-tion.
Finally, NONFICTION is a fairly diverse cate-gory encompassing most other types of expositorywriting, and FICTION is used for works of fiction.Our corpus of 499 texts was divided into a train-"ing subcorpus (402 texts) and an evaluation subcor-pus (97).
The evaluation subcorpus was designed34to have approximately equal numbers of all repre-sented combinations of facet levels.
Most such com-binations have six texts in the evaluation corpus, butdue to small numbers of some types of texts, someextant combinations are underrepresented.
Withinthis stratified framework, texts were chosen by apseudo random-number generator.
This setup re-sults in different quantitative compositions of train-ing and evaluation set.
For example, the most fre-quent genre level in the training subcorpus is RE-PORTAGE, but in the evaluation subcorpus NONFIC-TION predominates.3.2 Logist ic Regress ionWe chose logistic regression (LR) as our basic numer-ical method.
Two informal pilot studies indicatedthat it gave better results than linear discriminationand linear regression.LR is a statistical technique for modeling a binaryresponse variable by a linear combination of one ormore predictor variables, using a logit link function:g(r)  = log(r~(1 - zr))and modeling variance with a binomial random vari-able, i.e., the dependent variable log(r~(1 - ,7)) ismodeled as a linear combination of the independentvariables.
The model has the form g(,'r) = zi,8 where,'r is the estimated response probability (in our casethe probability of a particular facet value), xi is thefeature vector for text i, and ~q is the weight vectorwhich is estimated from the matrix of feature vec-tors.
The optimal value of fl is derived via maximumlikelihood estimation (McCullagh and Netder, 1989),using SPlus (Statistical Sciences, 1991).For binary decisions, the application of LR wasstraightforward.
For the polytomous facets GENREand BROW, we computed a predictor function inde-pendently for each level of each facet and chose thecategory with the highest prediction.The most discriminating of the 55 variables wereselected using stepwise backward selection based onthe AIC criterion (see documentation for STEP.GLMin Statistical Sciences (1991)).
A separate set ofvariables was selected for each binary discriminationtask.3.2.1 S t ructura l  CuesIn order to see whether our easily-computable sur-face cues are comparable in power to the structuralcues used in Karlgren and Cutting (1994), we alsoran LR with the cues used in their experiment.
Be-cause we use individual texts in our experiments in-stead of the fixed-length conglomerate samples ofKarlgren and Cutting, we averaged all count fea-tures over text length.3.3 Neura l  NetworksBecause of the high number of variables in our ex-periments, there is a danger that overfitting occurs.LR also forces us to simulate polytomous decisionsby a series of binary decisions, instead of directlymodeling a multinomial response.
Finally.
classicalLR does not model variable interactions.For these reasons, we ran a second set of experi-ments with neural networks, which generally do wellwith a high number of variables because they pro-tect against overfitting.
Neural nets also naturallymodel variable interactions.
We used two architec-tures, a simple perceptron (a two-layer feed-forwardnetwork with all input units connected to all outputunits), and a multi-layer perceptron with all inputunits connected to all units of the hidden layer, andall units of the hidden layer connected to all out-put units.
For binary decisions, such as determiningwhether or not a text is :NARRATIVE, the outputlayer consists of one sigmoidal output unit: for poly-tomous decisions, it consists of four (BRow) or six(GENRE) softmax units (which implement a multi-nomial response model} (Rumelhart et al, 1995).The size of the hidden layer was chosen to be threetimes as large as the size of the output layer (3 unitsfor binary decisions, 12 units for BRow, 18 units forGENRE).For binary decisions, the simple perceptron fitsa logistic model just as LR does.
However, it isless prone to overfitting because we train it usingthree-fold cross-validation.
Variables are selectedby summing the cross-entropy error over the threevalidation sets and eliminating the variable that ifeliminated results in the lowest cross-entropy error.The elimination cycle is repeated until this summedcross-entropy error starts increasing.
Because thisselection technique is time-consuming, we only ap-ply it to a subset of the discriminations.4 Resu l tsTable 1 gives the results of the experiments.
~For eachgenre facet, it compares our results using surfacecues (both with logistic regression and neural nets)against results using Karlgren and Cutting's struc-tural cues on the one hand (last pair of columns)and against a baseline on the other (first column).Each text in the evaluation suite was tested for eachfacet.
Thus the number 78 for NARRATIVE undermethod "LR (Surf.)
All" means that when all textswere subjected to the NARRATIVE test, 78% of themwere classified correctly.There are at least two major ways of conceivingwhat the baseline should be in this experiment.
If35the machine were to guess randomly among k cat-egories, the probability of a correct guess would be1/k.
i.e., 1/2 for NARRATIVE.
1/6 for GENRE.
and1/4 for BROW.
But one could get dramatic improve-ment just by building a machine that always guessesthe most populated category: NONFICT for GENRE.MIDDLE for BROW, and No for NARRATIVE.
Thefirst approach would be fair.
because our machinesin fact have no prior knowledge of the distribution ofgenre facets in the evaluation suite, but we decidedto be conservative and evaluate our methods againstthe latter baseline.
No matter which approach onetakes, however, each of the numbers in the table issignificant at p < .05 by a binomial distribution.That is, there is less than a 5% chance that a ma-chine guessing randomly could have come up withresults so much better than the baseline.It will be recalled that in the LR models, thefacets with more than two levels were computed bymeans of binary decision machines for each level,then choosing the level with the most positive score.Therefore some feeling for the internal functioning ofour algorithms can be obtained by seeing what theperformance is for each of these binary machines,and for the sake of comparison this information isalso given for some of the neural net models.
Ta-ble 2 shows how often each of the binary machinescorrectly determined whether a text did or did notfall in a particular facet level.
Here again the ap-propriate baseline could be determined two ways.In a machine that chooses randomly, performancewould be 50%, and all of the numbers in the tablewould be significantly better than chance (p < .05,binomial distribution).
But a simple machine thatalways guesses No would perform much better, andit is against his stricter standard that we computedthe baseline in Table 2.
Here, the binomial distribu-tion shows that some numbers are not significantlybetter than the baseline.
The numbers that are sig-nificantly better than chance at p < .05 by the bi-nomial distribution are starred.Tables 1 and 2 present aggregate results, whenall texts are classified for each facet or level.
Ta-ble 3, by contrast, shows which classifications areassigned for texts that actually belong to a specificknown level.
For example, the first row shows thatof the 18 texts that really are of the REPORTAGEGENRE level, 83% were correctly classified as RE-PORTAGE, 6% were misclassified as EDITORIAL, and11% as NONFICTION.
Because of space constraints,we present this amount of detail only for the sixGENRE levels, with logistic regression on selectedsurface variables.5 DiscussionThe experiments indicate that categorization deci-sions can be made with reasonable accuracy on thebasis of surface cues.
All of the facet level assign-ments are significantly better than a baseline of al-ways choosing the most frequent level (Table 1).
andthe performance appears even better when one con-siders that the machines do not actually know whatthe most frequent level is.When one takes a closer look at the performanceof the component machines, it is clear that somefacet levels are detected better than others.
Table 2shows that within the facet GENRE, our systems doa particularly good job on REPORTAGE and FICTION.trend correctly but not necessarily significantly forSCITECH and NONFICTION, but perform less well forEDITORIAL and LEGAL texts.
We suspect hat theindifferent performance in SCITECH and LEGAL textsmay simply reflect the fact that these genre levels arefairly infrequent in the Brown corpus and hence inour training set.
Table 3 sheds some light on theother cases.
The lower performance on the EDITO-RIAL and NONFICTION tests stems mostly from mis-classifying many NONFICTION texts as EDITORIAL.Such confusion suggests that these genre types areclosely related to each other, as ill fact they are.
Ed-itorials might best be treated in future experimentsas a subtype of NONFICTION, perhaps distinguishedby separate facets such as OPINION and INSTITU-TIONAL AUTHORSHIP.Although Table 1 shows that our methods pre-dict BROW at above-baseline levels, further analysis(Table 2) indicates that most of this performancecomes from accuracy in deciding whether or not atext is HIGH BROW.
The other levels are identifiedat near baseline performance.
This suggests prob-lems with the labeling of the BRow feature in thetraining data.
In particular, we had labeled journal-istic texts on the basis of the overall brow of the hostpublication, a simplification that ignores variationamong authors and the practice of printing featuresfrom other publications.
Vv'e plan to improve thoselabelings in future experiments by classifying browon an article-by-article basis.The experiments uggest that there is only asmall difference between surface and structural cues,Comparing LR with surface cues and LR with struc-tural cues as input, we find that they yield about thesame performance: averages of 77.0% (surface) vs.77.5% (structural) for all variables and 78.4% (sur-face) vs. 78.9% (structural) for selected variables.Looking at the independent binary decisions on atask-by-task basis, surface cues are worse in 10 cases36Table 1: Classification Results for All Facets.Baseline LR (Surf.)
\[ 2LP 3LP LR (Struct.
)Facet All Sel.
\] All Sel.
All Sel.
All Sel.Narrative 54 78 80 82 82 86 82 78 80Genre 33 61 66 75 79 71 74 66 62Brow 32 44 46 47 - -  54 - -  46 53Note.
Numbers are the percentage of the evaluation subcorpus (:V = 97) which were correctly assigned tothe appropriate facet level: the Baseline column tells what percentage would be correct if the machine alwaysguessed the most frequent level.
LR is Logistic Regression, over our surface cues (Surf.)
or Karlgren andCutting's tructural cues (Struct.
): 2LP and 3LP are 2- or 3-layer perceptrons using our surface cues.
Undereach experiment.
All tells the results when all cues are used, and Sel.
tells the results when for each levelone selects the most discriminating cues.
A dash indicates that an experiment was not run.LevelsTable 2: Classification Results for Each Facet Level.Baseline LR (Surf.)
2LP 3LP LR (Struct.
)GenreRepEditLegalScitechNonfictFictBrowPopularMiddleUppermiddleHighAll81 89*81 7595 9694 100"67 6781 93*74 7468 6688 7470 84*Sel.8896966896*75677888*94*749599*78*99*74648689*All All94*8O9594678174S48890"All Sel.90* 90*79 7793 9393 9673 7496* 96*72 7358 6479 8285* 86*Note.
Numbers are the percentage of the evaluation subcorpus (N = 97) which was correctly classified on abinary discrimination task.
The Baseline column tells what percentage would be got correct by guessing Nofor each level.
Headers have the same meaning as in Table 1.
* means significantly better than Baseline at p < .05, using a binomial distribution (N=97, p as per firstcolumn).Table 3: Genre BinaryActualRepEditLegalScitechNonfictFictLevel Classification Results by Genre Level.GuessRep Edit Legal Scitech Nonfict Fict83 6 0 0 11 017 61 0 0 17 620 0 20 0 60 00 0 0 83 17 03 34 0 6 47 90 6 0 0 0 94N1818563218Note.
Numbers are the percentage of the texts actually belonging to the GENRE level indicated in the firstcolumn that were classified as belonging to each of the GENRE levels indicated in the column headers.
Thusthe diagonals are correct guesses, and each row would sum to 100%, but for rounding error.37and better in 8 cases.
Such a result is expected ifwe assume that either cue representation is equallylikely to do better than the other (assuming a bino-mial model, the probability of getting this or a more8 extreme result is ~-':-i=0 b(i: 18.0.5) = 0.41).
We con-clude that there is at best a marginal advantage tousing structural cues.
an advantage that will not jus-tify the additional computational cost in most cases.Our goal in this paper has been to prepare theground for using genre in a wide variety of areas innatural language processing.
The main remainingtechnical challenge is to find an effective strategy forvariable selection in order to avoid overfitting dur-ing training.
The fact that the neural networks havea higher performance on average and a much higherperformance for some discriminations (though at theprice of higher variability of performance) indicatesthat overfitting and variable interactions are impor-tant problems to tackle.On the theoretical side.
we have developed a tax-onomy of genres and facets.
Genres are consideredto be generally reducible to bundles of facets, thoughsometimes with some irreducible atomic residue.This way of looking at the problem allows us todefine the relationships between different genres in-stead of regarding them as atomic entities.
We alsohave a framework for accommodating ew genres asyet unseen bundles of facets.
Finally, by decompos-ing genres into facets, we can concentrate on what-ever generic aspect is important in a particular appli-cation (e.g., narrativity for one looking for accountsof the storming of the Bastille).Further practical tests of our theory will comein applications of genre classification to tagging,summarization, and other tasks in computationallinguistics.
We are particularly interested in ap-plications to information retrieval where users areoften looking for texts with particular, quite nar-row generic properties: authoritatively written doc-uments, opinion pieces, scientific articles, and so on.Sorting search results according to genre will gainimportance as the typical data base becomes in-creasingly heterogeneous.
We hope to show that theusefulness of retrieval tools can be dramatically im-proved if genre is one of the selection criteria thatusers can exploit.Re ferencesBiber, Douglas.
1986.
Spoken and written textualdimensions in English: Resolving the contradic-tory findings.
Language, 62(2):384-413.Biber.
Douglas.
1988.
Variation across Speechand Writing.
Cambridge University Press.
Cam-bridge.
England.Biber.
Douglas.
1992.
The multidimensional p-proach to linguistic analyses of genre variation:An overview of methodology and finding.
Com-puters in the Humanities, 26(5-6):331-347.Biber.
Douglas.
1995.
Dimensions of Register Vari-ation: A Cross-Linguistic Comparison.
Cam-bridge University Press.
Cambridge.
England.Butcher, S. H.. editor.
1932.
Aristotle's Theory ofPoetry and Fine Arts.
with The Poetics.
Macmil-lan, London.
4th edition.Dubrow, Heather.
1982, Genre.
Methuen.
Londonand New York.Fowler, Alistair.
1982.
Kinds of Literature.
HarvardUniversity Press.
Cambridge.
Massachusetts.Frye.
Northrop.
1957.
The Anatomy of Criticism,Princeton University Press.
Princeton, New Jer-sey.Hernadi, Paul.
1972.
Beyond Genre.
Cornell Uni-versity Press.
Ithaca, New York.Hobbes, Thomas.
1908.
The answer of mr Hobbesto Sir William Davenant's preface before Gondib-ert.
In J.E.
Spigarn, editor.
Critical Essays of theSeventeenth Century.
The Clarendon Press, Ox-ford.Karlgren, Jussi and Douglass Cutting.
1994.
Recog-nizing text genres with simple metrics using dis-criminant analysis.
In Proceedings of Coling 94,Kyoto.McCullagh, P. and J.A.
Nelder.
1989.
GeneralizedLinear Models.
chapter 4, pages 101-123.
Chap-man and Hall, 2nd edition.Nunberg, Geoffrey.
1990.
The Linguistics of Punc-tuation.
CSLI Publications.
Stanford.
California.Rumelhart.
David E.. Richard Durbin.
RichardGolden.
and Yves Chauvin.
1995.
Backprop-agation: The basic theory.
In Yves Chau-vin and David E. Rumelhart, editors, Back.propagation: Theory, Architectures, and Applica-tions.
Lawrence Erlbaum.
Hillsdale, New Jersey,pages 1-34.Staiger, Emil.
1959.
Grundbegriffe der Poetik.
At-lantis, Zurich.Statistical Sciences.
1991.
S-PLUS Reference Man-ual.
Statistical Sciences.
Seattle, Washington.Todorov, Tsvetan.
1978.
Les genres du discours.Seuil, Paris.3B
