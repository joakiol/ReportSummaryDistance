Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 158?166, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsSemEval-2013 Task 10: Cross-lingual Word Sense DisambiguationEls Lefever1,2 and Ve?ronique Hoste1,31LT3, Language and Translation Technology Team, University College GhentGroot-Brittannie?laan 45, 9000 Gent, Belgium2Department of Applied Mathematics, Computer Science and Statistics, Ghent UniversityKrijgslaan 281 (S9), 9000 Gent, Belgium3Department of Linguistics, Ghent UniversityBlandijnberg 2, 9000 Gent, Belgium{Els.Lefever,Veronique.Hoste}@hogent.beAbstractThe goal of the Cross-lingual Word Sense Disam-biguation task is to evaluate the viability of multilin-gual WSD on a benchmark lexical sample data set.The traditional WSD task is transformed into a mul-tilingual WSD task, where participants are asked toprovide contextually correct translations of Englishambiguous nouns into five target languages, viz.French, Italian, English, German and Dutch.
We re-port results for the 12 official submissions from 5different research teams, as well as for the ParaSensesystem that was developed by the task organizers.1 IntroductionLexical ambiguity remains one of the major prob-lems for current machine translation systems.
Inthe following French sentence ?Je cherche des ide?espour manger de l?avocat?1, the word ?avocat?
isclearly referring to the fruit, whereas both GoogleTranslate2 as well as Babelfish3 translate the wordas ?lawyer?.
Although ?lawyer?
is a correct transla-tion of the word ?avocat?, it is the wrong translationin this context.
Other language technology applica-tions, such as Question Answering (QA) systems orinformation retrieval (IR) systems, also suffer fromthe poor contextual disambiguation of word senses.Word sense disambiguation (WSD) is still consid-ered one of the most challenging problems within1English translation: ?I?m looking for ideas to eat avocado?.2http://translate.google.com3http://be.bing.com/translator/language technology today.
It requires the construc-tion of an artificial text understanding as the sys-tem should detect the correct word sense based onthe context of the word.
Different methodologieshave been investigated to solve the problem; see forinstance Agirre and Edmonds (2006) and Navigli(2009) for a detailed overview of WSD algorithmsand evaluation.This paper reports on the second edition ofthe ?Cross-Lingual Word Sense Disambiguation?
(CLWSD) task, that builds further on the insights wegained from the SemEval-2010 evaluation (Lefeverand Hoste, 2010b) and for which new test data wereannotated.
The task is an unsupervised Word SenseDisambiguation task for English nouns, the senselabel of which is composed of translations in dif-ferent target languages (viz.
French, Italian, Span-ish, Dutch and German).
The sense inventory isbuilt up on the basis of the Europarl parallel corpus;all translations of a polysemous word were manu-ally grouped into clusters, which constitute differentsenses of that given word.
For the test data, nativespeakers assigned a translation cluster(s) to each testsentence and gave their top three translations fromthe predefined list of Europarl translations, in orderto assign weights to the set of gold standard transla-tions.The decision to recast the more traditional mono-lingual WSD task into a cross-lingual WSD task wasmotivated by the following arguments.
Firstly, usingmultilingual unlabeled parallel corpora contributesto clearing the data acquisition bottleneck for WSD,because using translations as sense labels excludesthe need for manually created sense-tagged corpora158and sense inventories such as WordNet (Fellbaum,1998) or EuroWordNet (Vossen, 1998).
Moreover,as there is fairly little linguistic knowledge involved,the framework can be easily deployed for a varietyof different languages.
Secondly, a cross-lingual ap-proach also deals with the sense granularity prob-lem; finer sense distinctions are only relevant as faras they get lexicalized in different translations ofthe word.
If we take the English word ?head?
asan example, we see that this word is always trans-lated as ?hoofd?
in Dutch (both for the ?chief?
andfor the ?body part?
sense of the word).
At the sametime, the subjectivity problem is tackled that ariseswhen lexicographers have to construct a fixed set ofsenses for a particular word that should fit all possi-ble domains and applications.
In addition, the useof domain-specific corpora allows to derive senseinventories that are tailored towards a specific tar-get domain or application and to train a dedicatedCLWSD system using these particular sense inven-tories.
Thirdly, working immediately with transla-tions instead of more abstract sense labels allows tobypass the need to map abstract sense labels to cor-responding translations.
This makes it easier to inte-grate a dedicated WSD module into real multilingualapplications such as machine translation (Carpuatand Wu, 2007) or information retrieval (Clough andStevenson, 2004).Many studies have already shown the validity of across-lingual approach to Word Sense Disambigua-tion (Brown et al 1991; Gale and Church, 1993;Ng et al 2003; Diab, 2004; Tufis?
et al 2004;Chan and Ng, 2005; Specia et al 2007; Apidi-anaki, 2009).
The Cross-lingual WSD task con-tributes to this research domain by the constructionof a dedicated benchmark data set where the am-biguous words were annotated with the senses froma multilingual sense inventory extracted from a par-allel corpus.
This benchmark data sets allows a de-tailed comparison between different approaches tothe CLWSD task.The remainder of this paper is organized as fol-lows.
Section 2 focuses on the task description andbriefly recapitalizes the construction of the sense in-ventory and the annotation procedure of the test sen-tences.
Section 3 presents the participating systemsto the task, whereas Section 4 gives an overview ofthe experimental setup and results.
Section 5 con-cludes this paper.2 Task set upThe ?Cross-lingual Word Sense Disambiguation?
(CLWSD) task was organized for the first time in theframework of SemEval-2010 (Lefever and Hoste,2010b) and resulted in 16 submissions from fivedifferent research teams.
Many additional researchteams showed their interest and downloaded the trialdata, but did not manage to finish their systems intime.
In order to gain more insights into the com-plexity and the viability of cross-lingual WSD, weproposed a second edition of the task for SemEval-2013 for which new test data were annotated.The CLWSD task is an unsupervised Word SenseDisambiguation task for a lexical sample of twentyEnglish nouns.
The sense label of the nouns is com-posed of translations in five target languages (viz.Spanish, French, German, Italian and Dutch) andthe sense inventory is built up on the basis of theEuroparl parallel corpus4.
This section briefly de-scribes the data construction process for the task.For a more detailed description of the gold stan-dard creation and data annotation process, we referto Lefever and Hoste (2010a; 2010b).2.1 Sense inventoryThe starting point for the gold standard sense inven-tory creation was the parallel corpus Europarl.
Weselected six languages from Europarl (English andthe five target languages) and only considered the 1-1 sentence alignments between English and the fivetarget languages5.
In order to obtain the multilingualsense inventory we:1. performed word alignment on the parallel cor-pus in order to find all possible translations forour set of ambiguous focus nouns2.
clustered the resulting translations by meaningand manually lemmatized all translationsThe resulting sense inventory was then used to an-notate the sentences in the test set that was devel-oped for the SemEval-2013 CLWSD task.4http://www.statmt.org/europarl/5This six-lingual sentence-aligned subcor-pus of Europarl can be downloaded fromhttp://lt3.hogent.be/semeval/.1592.2 Test dataFor the creation of the test data set, we manually se-lected 50 sentences per ambiguous focus word fromthe part of the ANC corpus that is publicly avail-able6.
In total, 1000 sentences were annotated us-ing the sense inventory that was described in Sec-tion 2.1.
Three annotators per target language wereasked to first select the correct sense cluster and nextto choose the three contextually most appropriatetranslations from this sense cluster.
They could alsoprovide fewer translations in case they could not findthree good translations for this particular occurrenceof the test word.
These translations were used to(1) compose the set of gold standard translations pertest instance and (2) to assign frequency weights toall translations in the gold standard (e.g.
translationsthat were chosen by all three annotators get a fre-quency weight of 3 in the gold standard).2.3 Evaluation tasksTwo subtasks were proposed for the Cross-lingualWSD task: a best evaluation and an Out-of-five eval-uation task.
For the best evaluation, systems canpropose as many guesses as the system believes arecorrect, but the score is divided by the number ofguesses.
In case of the Out-of-five evaluation, sys-tems can propose up to five guesses per test instancewithout being penalized for wrong translation sug-gestions.
Both evaluation tasks are explained inmore detail in Section 4.1.3 Systems3.1 Systems participating to the officialCLWSD evaluation campaignFive different research teams participated to theCLWSD task and submitted up to three differentruns of their system, resulting in 12 different sub-missions for the task.
All systems took part in boththe best and the Out-of-five evaluation tasks.
Thesesystems took very different approaches to solve thetask, ranging from statistical machine translation,classification and sense clustering to topic modelbased approaches.The XLING team (Tan and Bond, 2013) submit-ted three runs of their system for all five target lan-guages.
The first version of the system presents a6http://www.americannationalcorpus.org/topic matching and translation approach to CLWSD(TnT run), where LDA is applied on the Europarlsentences containing the ambiguous focus word inorder to train topic models.
Each sentence in thetraining corpus is assigned a topic that contains alist of associated words with the topic.
The topicof the test sentence is then inferred and comparedto the matching training sentences by means of thecosine similarity between the training and test vec-tors.
WordNet (WN) is used as a fallback in casethe system returns less than 5 answers.
The second -and best performing - flavor of the system (SnT run)calculates the cosine similarity between the contextwords of the test and training sentences.
The out-put of the system then contains the translation thatresults from running word alignment on the focusword in the training corpus.
As a fallback, Word-Net is again used.
The WN senses are sorted by fre-quency in the SemCor corpus and the correspond-ing translation is selected from the aligned WordNetin the target language.
The third run of the system(merged) combines the output from the other twoflavors of the system.The LIMSI system (Apidianaki, 2013) applies anunsupervised CLWSD method that was proposed in(Apidianaki, 2009) for three target languages, viz.Spanish, Italian and French.
First, word alignmentis applied on the parallel corpus and three bilinguallexicons are built, containing for each focus wordthe translations in the three target languages.
In anext step, a vector is built for each translation of theEnglish focus word, using the cooccurrences of theword in the sentences in which it gets this particu-lar translation.
A clustering algorithm then groupsthe feature vectors using the Weighted Jaccard mea-sure.
New instances containing the ambiguous focusword are then compared to the training feature vec-tors and assigned to one of the sense clusters.
Incase the highest-ranked translation in the cluster hasa score below the threshold, the system falls back tothe most frequent translation.Two very well performing systems take aclassification-based approach to the CLWSD task:the HLTDI and WSD2 systems.
The HLTDI sys-tem (Rudnick et al 2013) performs word alignmenton the intersected Europarl corpus to locate train-ing instances containing the ambiguous focus words.The first flavor of the system (l1) uses a maxent clas-160sifier that is trained over local context features.
TheL2 model (l2 run) also adds translations of the fo-cus word into the four other target languages to thefeature vector.
To disambiguate new test instances,these translations into the four other languages areestimated using the classifiers built in the first ver-sion of the system (l1).
The third system run (mrf )builds a Markov network of L1 classifiers in order tofind the best translation into all five target languagesjointly.
The nodes of this network correspond to thedistribution produced by the L1 classifiers, while theedges contain pairwise potentials derived from thejoint probabilities of translation labels occurring to-gether in the training data.Another classification-based approach is pre-sented by the WSD2 system (van Gompel andvan den Bosch, 2013), that uses a k-NN classifierto solve the CLWSD task.
The first configurationof the system (c1l) uses local context features for awindow of three words containing the focus word.Parameters were optimized on the trial data.
Thesecond flavor of the system (c1lN) uses the sameconfiguration of the system, but without parameteroptimization.
The third configuration of the system(var) is heavily optimized on the trial data, selectingthe winning configuration per trial word and evalua-tion metric.
In addition to the local context features,also global bag-of-word context features are consid-ered for this version of the system.A completely different approach is taken by theNRC-SMT system (Carpuat, 2013), that uses a sta-tistical machine translation approach to tackle theCLWSD task.
The baseline version of the system(SMTbasic) represents a standard phrase-based SMTbaseline, that is trained only on the intersected Eu-roparl corpus.
Translations for the test instances areextracted from the top hypothesis (for the best eval-uation) or from the 100-best list (for the Out-of-fiveevaluation).
The optimized version of the system(SMTadapt2) is trained on the Europarl corpus andadditional news data, and uses mixture models thatare developed for domain adaptation in SMT.In addition to the five systems that participated tothe official evaluation campaign, the organizers alsopresent results for their ParaSense system, which isdescribed in the following section.3.2 ParaSense systemThe ParaSense system (Lefever et al 2013)is a multilingual classification-based approach toCLWSD.
A combination of both local context in-formation and translational evidence is used to dis-criminate between different senses of the word, theunderlying hypothesis being that using multilingualinformation should be more informative than onlyhaving access to monolingual or bilingual features.The local context features contain the word form,lemma, part-of-speech and chunk information for awindow of seven words containing the ambiguousfocus word.
In addition, a set of bag-of-words fea-tures is extracted from the aligned translations thatare not the target language of the classifier.
Perambiguous focus word, a list of all content words(nouns, adjectives, adverbs and verbs) that occurredin the linguistically preprocessed aligned transla-tions of the English sentences containing this word,were extracted.
Each content word then correspondsto exactly one binary feature per language.
For theconstruction of the translation features for the train-ing set, we used the Europarl aligned translations.As we do not dispose of similar aligned transla-tions for the test instances for which we only havethe English test sentences at our disposal, we usedthe Google Translate API7 to automatically gener-ate translations for all English test instances in thefive target languages.As a classifier, we opted for the k Nearest neigh-bor method as implemented in TIMBL (Daelemansand van den Bosch, 2005).
As most classifiers canbe initialized with a wide range of parameters, weused a genetic algorithm to optimize the parametersettings for our classification task.4 Results4.1 Experimental set upTest set The lexical sample contains 50 Englishsentences per ambiguous focus word.
All instanceswere manually annotated per language, which re-sulted in a set of gold standard translation labels perinstance.
For the construction of the test dataset, werefer to Section 2.7http://code.google.com/apis/language/161Evaluation metric The BEST precision and recallmetric was introduced by (McCarthy and Navigli,2007) in the framework of the SemEval-2007 com-petition.
The metric takes into account the frequencyweights of the gold standard translations: transla-tions that were picked by different annotators re-ceived a higher associated frequency which is incor-porated in the formulas for calculating precision andrecall.
For the BEST precision and recall evaluation,the system can propose as many guesses as the sys-tem believes are correct, but the resulting score isdivided by the number of guesses.
In this way, sys-tems that output many guesses are not favored andsystems can maximize their score by guessing themost frequent translation from the annotators.
Wealso calculate Mode precision and recall, where pre-cision and recall are calculated against the transla-tion that is preferred by the majority of annotators,provided that one translation is more frequent thanthe others.The following variables are used for the BEST pre-cision and recall formulas.
Let H be the set of an-notators, T the set of test words and hi the set oftranslations for an item i ?
T for annotator h ?
H .Let A be the set of words from T where the systemprovides at least one answer and ai the set of guessesfrom the system for word i ?
A.
For each i, we cal-culate the multiset union (Hi) for all hi for all h ?
Hand for each unique type (res) in Hi that has an as-sociated frequency (freqres).
Equation 1 lists theBEST precision formula, whereas Equation 2 liststhe formula for calculating the BEST recall score:Precision =?ai:i?A?res?aifreqres|ai||Hi||A|(1)Recall =?ai:i?T?res?aifreqres|ai||Hi||T |(2)Most Frequent translation baseline As a base-line, we selected the most frequent lemmatizedtranslation that resulted from the automated wordalignment (GIZA++) for all ambiguous nouns in thetraining data.
This baseline is inspired by the mostfrequent sense baseline often used in WSD evalu-ations.
The main difference between the most fre-quent sense baseline and our baseline is that the lat-ter is corpus-dependent: we do not take into accountthe overall frequency of a word as it would be mea-sured based on a large general purpose corpus, butcalculate the most frequent sense (or translation inthis case) based on our training corpus.4.2 Experimental resultsFor the system evaluation results, we show preci-sion and Mode precision figures for both evaluationtypes (best and Out-of-five).
In our case, precisionrefers to the number of correct translations in rela-tion to the total number of translations generated bythe system, while recall refers to the number of cor-rect translations generated by the classifier.
As allparticipating systems predict a translation label forall sentences in the test set, precision and recall willgive identical results.
As a consequence, we do notlist the recall and Mode recall figures that are in thiscase identical to the corresponding precision scores.Table 1 lists the averaged best precision scoresfor all systems, while Table 2 gives an overviewof the best Mode precision figures for all five tar-get languages, viz.
Spanish (Es), Dutch (Nl), Ger-man (De), Italian (It) and French (Fr).
We list scoresfor all participating systems in the official CLWSDevaluation campaign, as well as for the organiz-ers?
system ParaSense, that is not part of the offi-cial SemEval competition.
The best results for thebest precision evaluation are achieved by the NRC-SMTadapt2 system for Spanish and by the WSD2system for the other four target languages, closelyfollowed by the HLTDI system.
The latter two sys-tems also obtain the best results for the best Modeprecision metric.Table 3 lists the averaged Out-of-five precisionscores for all systems, while Table 4 gives anoverview of the Out-of-five Mode precision figuresfor all five target languages, viz.
Spanish (Es), Dutch(Nl), German (De), Italian (It) and French (Fr).
Forthe Out-of-five evaluation, where systems are al-lowed to generate up to five unique translations with-out being penalized for wrong translations, again theHLTDI and WSD2 systems obtain the best classifi-cation performance.Although the winning systems use different ap-proaches (statistical machine translation and classi-162fication algorithms), they have in common that theyonly use a parallel corpus to extract disambiguatinginformation, and do not use external resources suchas WordNet.
As a consequence, this makes the sys-tems very flexible and language-independent.
TheParaSense system, that incorporates translation in-formation from four other languages, outperformsall other systems, except for the best precision met-ric in Spanish, where the NRC-SMT system obtainsthe overall best results.
This confirms the hypothe-sis that a truly multilingual approach to WSD, whichincorporates translation information from multiplelanguages into the feature vector, is more effectivethan only using monolingual or bilingual features.A possible explanation could be that the differencesbetween the different languages that are integratedin the feature vector enable the system to refinethe obtained sense distinctions.
We indeed see thatthe ParaSense system outperforms the classification-based bilingual approaches which exploit similar in-formation (e.g.
training corpora and machine learn-ing algorithms).Es Nl De It FrBaseline23.23 20.66 17.43 20.21 25.74results for the HLTDI systemhltdi-l1 29.01 21.53 19.50 24.52 27.01hltdi-l2 28.49 22.36 19.92 23.94 28.23hltdi-mrf 29.36 21.61 19.76 24.62 27.46results for the XLING systemmerged 11.09 4.91 4.08 6.93 9.57snt 19.59 9.89 8.13 12.74 17.33tnt 18.60 7.40 5.29 10.70 16.48results for the LIMSI systemlimsi 24.70 21.20 24.56results for the NRC-SMT systembasic 27.24adapt2 32.16results for the WSD2 systemc1l 28.40 23.14 20.70 25.43 29.88c1lN 28.65 23.61 20.82 25.66 30.11var 23.31 17.17 16.20 20.38 25.89results for the PARASENSE system31.72 25.29 24.54 28.15 31.21Table 1: BEST precision scores averaged over all twentytest words for Spanish (Es), Dutch (Nl), German (De),Italian (It) and French (Fr).Es Nl De It FrBaseline27.48 24.15 15.30 19.88 20.19results for the HLTDI systemhltdi-l1 36.32 25.39 24.16 26.52 21.24hltdi-l2 37.11 25.34 24.74 26.65 21.07hltdi-mrf 36.57 25.72 24.01 26.26 21.24results for the XLING systemmerged 24.31 8.54 5.82 7.54 11.63snt 21.36 9.56 10.36 11.27 11.57tnt 24.31 8.54 5.82 7.54 11.63results for the LIMSI systemlimsi 32.09 23.06 22.16results for the NRC-SMT systembasic 32.28adapt2 36.2results for the WSD2 systemc1l 33.89 26.32 24.73 31.61 26.62c1lN 33.70 27.96 24.27 30.67 25.27var 27.98 18.74 21.74 20.69 16.71results for the PARASENSE system40.26 30.29 25.48 30.11 26.33Table 2: BEST Mode precision scores averaged over alltwenty test words for Spanish (Es), Dutch (Nl), German(De), Italian (It) and French (Fr).Es Nl De It FrBaseline53.07 43.59 38.86 42.63 51.36results for the HLTDI systemhltdi-l1 61.69 46.55 43.66 53.57 57.76hltdi-l2 59.51 46.36 42.32 53.05 58.20hltdi-mrf 9.89 5.69 4.15 3.91 7.11results for the XLING systemmerged 43.76 24.30 19.83 33.95 38.15snt 44.83 27.11 23.71 32.38 38.44tnt 39.52 23.27 19.13 33.28 35.30results for the LIMSI systemlimsi 49.01 40.25 45.37results for the NRC-SMT systembasic 37.98adapt2 41.65results for the WSD2 systemc1l 58.23 47.83 43.17 52.22 59.07c1lN 57.62 47.62 43.24 52.73 59.80var 55.70 46.85 41.46 51.18 59.19Table 3: OUT-OF-FIVE precision scores averaged over alltwenty test words for Spanish (Es), Dutch (Nl), German(De), Italian (It) and French (Fr).163Es Nl De It FrBaseline57.35 41.97 44.35 41.69 47.42results for the HLTDI systemhltdi-l1 64.65 47.34 53.50 56.61 51.96hltdi-l2 62.52 44.06 49.03 54.06 53.57hltdi-mrf 11.39 5.09 3.14 3.87 7.79results for the XLING systemmerged 48.63 23.64 24.64 31.74 30.11snt 50.04 27.30 30.57 29.17 32.45tnt 44.96 22.98 23.54 29.61 28.02results for the LIMSI systemlimsi 51.41 47.21 39.54results for the NRC-SMT systembasic 42.92adapt2 45.38results for the WSD2 systemc1l 63.75 45.27 50.11 54.13 57.57c1lN 63.80 44.53 50.26 54.37 56.40var 61.51 41.82 49.23 54.73 54.97Table 4: OUT-OF-FIVE Mode precision scores averagedover all twenty test words for Spanish (Es), Dutch (Nl),German (De), Italian (It) and French (Fr).In general, we notice that French and Spanishhave the highest scores, while Dutch and Germanseem harder to tackle.
Italian is situated some-where in between the Romance and Germanic lan-guages.
This trend confirms the results that were ob-tained during the first SemEval Cross-lingual WSDtask (Lefever and Hoste, 2010b).
As pointed out af-ter the first competition, the discrepancy between thescores for the Romance and Germanic languages canprobably be explained by the number of classes (ortranslations in this case) the systems have to choosefrom.
Germanic languages are typically charac-terized by a very productive compounding system,where compounds are joined together in one ortho-graphic unit, which results in a much higher numberof different class labels.
As the Romance languagestypically write compounds in separate orthographicunits, they dispose of a smaller number of differenttranslations for each ambiguous noun.We can also notice large differences between thescores for the individual words.
Figure 1 illustratesthis by showing the best precision scores in Span-ish for the different test words for the best run persystem.
Except for some exceptions (e.g.
coach inthe NRC-SMT system), most system performancescores follow a similar curve.
Some words (e.g.match, range) are particularly hard to disambiguate,while others obtain very high scores (e.g.
mission,soil).
One possible explanation for the very goodscores for some words (e.g.
soil) can be attributedto a very generic translation which accounts for allsenses of the word even though there might be moresuitable translations for each of the senses depend-ing on the context.
Because the manual annota-tors were able to select three good translations foreach test instance, the most generic translation is of-ten part of the gold standard translations.
This isalso reflected in the high baseline scores for thesewords.
For the words performing badly in most sys-tems, an inspection of the training data propertiesrevealed two possible explanations for these poorclassification results.
Firstly, there seems to be alink with the number of training instances, corre-sponding to the frequency of the word in the train-ing corpus.
Both for coach and match, two wordsconsistently performing bad in all systems, there arevery few training examples in the corpus (66 and109 respectively).
This could also explain why theNRC-SMT system, that also uses additional paral-lel data, achieves better results for coach than allother systems.
Secondly, the ambiguity or numberof valid translations per word in the training dataalso seems to play a role in the classification results.Both job and range appear very hard to classify cor-rectly, and both words are very ambiguous, with nofewer than 121 and 125 translations, respectively, tochoose from in Spanish.5 ConclusionThe Cross-lingual Word Sense Disambiguation taskattempts to address three important challenges forWSD, namely (1) the data acquisition bottleneck,which is caused by the lack of manually created re-sources, (2) the sense granularity and subjectivityproblem of the existing sense inventories and (3) theneed to make WSD more suited for practical appli-cations.
The task contributes to the WSD researchdomain by the construction of a dedicated bench-mark data set that allows to compare different ap-proaches to the Cross-lingual WSD task.The evaluation results lead to the following ob-servations.
Firstly, languages which make exten-164Figure 1: Spanish best precision scores for all systems per ambiguous focus word.sive use of single word compounds seem harderto tackle, which can probably be explained by thehigher number of translations these classifiers haveto choose from.
Secondly, we can notice large dif-ferences between the performances of the individualtest words.
For the words that appear harder to dis-ambiguate, both the number of training instances aswell as the ambiguity of the word seem to play a rolefor the classification performance.
Thirdly, both theParaSense system as well as the two winning sys-tems from the competition extract all disambiguat-ing information from the parallel corpus and do notuse any external resources.
As a result, these sys-tems are very flexible and can be easily extended toother languages and domains.
In addition, the goodscores of the ParaSense system, that incorporates in-formation from four additional languages, confirmsthe hypothesis that a truly multilingual approach isan effective way to tackle the CLWSD task.AcknowledgmentsWe would like to thank all annotators for their hardwork.ReferencesEneko Agirre and Philip Edmonds.
2006.
Word SenseDisambiguation.
Algorithms and Applications.
Text,Speech and Language Technology.
Springer.M.
Apidianaki.
2009.
Data-driven semantic analysis formultilingual WSD and lexical selection in translation.In Proceedings of the 12th Conference of the EuropeanChapter of the Association for Computational Linguis-tics (EACL), pages 77?85, Athens, Greece.Marianna Apidianaki.
2013.
LIMSI : Cross-lingualWord Sense Disambiguation using Translation SenseClustering.
In Proceedings of the 7th InternationalWorkshop on Semantic Evaluation (SemEval 2013), inconjunction with the Second Joint Conference on Lex-ical and Computational Semantcis (*SEM 2013), At-lanta, USA.P.F.
Brown, S.A.D.
Pietra, V.J.D.
Pietra, and R.L.
Mer-cer.
1991.
Word-sense disambiguation using statisti-cal methods.
In Proceedings of the 29th Annual Meet-ing of the Association for Computational Linguistics,pages 264?270, Berkeley, California.M.
Carpuat and D. Wu.
2007.
Improving statisticalmachine translation using word sense disambiguation.In Proceedings of the 2007 Joint Conference on Em-pirical Methods in Natural Language Processing andComputational Natural Language Learning (EMNLP-CoNLL), pages 61?72, Prague, Czech Republic.Marine Carpuat.
2013.
NRC: A Machine TranslationApproach to Cross-Lingual Word Sense Disambigua-tion (SemEval-2013 Task 10).
In Proceedings of the7th International Workshop on Semantic Evaluation(SemEval 2013), in conjunction with the Second JointConference on Lexical and Computational Semantcis(*SEM 2013), Atlanta, USA.165Y.S.
Chan and H.T.
Ng.
2005.
Scaling Up Word SenseDisambiguation via Parallel Texts.
In Proceedings ofthe 20th National Conference on Artificial Intelligence(AAAI 2005), pages 1037?1042, Pittsburgh, Pennsyl-vania, USA.P.
Clough and M. Stevenson.
2004.
Cross-language in-formation retrieval using eurowordnet and word sensedisambiguation.
In Advances in Information Retrieval,26th European Conference on IR Research (ECIR),pages 327?337, Sunderland, UK.W.
Daelemans and A. van den Bosch.
2005.
Memory-based Language Processing.
Cambridge UniversityPress.M.
Diab.
2004.
Word Sense Disambiguation within aMultilingual Framework.
Phd, University of Mary-land, USA.C.
Fellbaum.
1998.
WordNet: An Electronic LexicalDatabase.
MIT Press.W.A.
Gale and K.W.
Church.
1993.
A program for align-ing sentences in bilingual corpora.
ComputationalLinguistics, 19(1):75?102.E.
Lefever and V. Hoste.
2010a.
Constructionof a Benchmark Data Set for Cross-Lingual WordSense Disambiguation.
In Nicoletta Calzolari, KhalidChoukri, Bente Maegaard, Joseph Mariani, Jan Odijk,Stelios Piperidis, and Daniel Tapias, editors, Proceed-ings of the seventh International Conference on Lan-guage Resources and Evaluation (LREC?10), Valletta,Malta, May.
European Language Resources Associa-tion (ELRA).E.
Lefever and V. Hoste.
2010b.
SemEval-2010 Task3: Cross-Lingual Word Sense Disambiguation.
InProceedings of the 5th International Workshop on Se-mantic Evaluation, ACL 2010, pages 15?20, Uppsala,Sweden.E.
Lefever, V. Hoste, and M. De Cock.
2013.
Fivelanguages are better than one: an attempt to bypassthe data acquisition bottleneck for wsd.
In In Alexan-der Gelbukh (ed.
), CICLing 2013, Part I, LNCS 7816,pages 343?354.
Springer-Verlag Berlin Heidelberg.D.
McCarthy and R. Navigli.
2007.
SemEval-2007 Task10: English Lexical Substitution Task.
In Proceedingsof the 4th International Workshop on Semantic Eval-uations (SemEval-2007), pages 48?53, Prague, CzechRepublic.R.
Navigli.
2009.
Word Sense Disambiguation: a Sur-vey.
ACM Computing Surveys, 41(2):1?69.H.T.
Ng, B. Wang, and Y.S.
Chan.
2003.
Exploiting par-allel texts for word sense disambiguation: An empiri-cal study.
In 41st Annual Meeting of the Associationfor Computational Linguistics (ACL), pages 455?462,Sapporo, Japan.Alex Rudnick, Can Liu, and Michael Gasser.
2013.HLTDI: CL-WSD Using Markov Random Fields forSemEval-2013 Task 10.
In Proceedings of the 7thInternational Workshop on Semantic Evaluation (Se-mEval 2013), in conjunction with the Second JointConference on Lexical and Computational Semantcis(*SEM 2013), Atlanta, USA.L.
Specia, M.G.V.
Nunes, and M. Stevenson.
2007.Learning Expressive Models for Word Sense Disam-biguation.
In Proceedings of the 45th Annual Meetingof the Association of Computational Linguistics, pages41?48, Prague, Czech Republic.Liling Tan and Francis Bond.
2013.
XLING: Match-ing Query Sentences to a Parallel Corpus using TopicModels for WSD.
In Proceedings of the 7th Inter-national Workshop on Semantic Evaluation (SemEval2013), in conjunction with the Second Joint Confer-ence on Lexical and Computational Semantcis (*SEM2013), Atlanta, USA.D.
Tufis?, R. Ion, and N. Ide.
2004.
Fine-GrainedWord Sense Disambiguation Based on Parallel Cor-pora, Word Alignment, Word Clustering and AlignedWordnets.
In Proceedings of the 20th InternationalConference on Computational Linguistics (COLING2004), pages 1312?1318, Geneva, Switzerland, Au-gust.
Association for Computational Linguistics.Maarten van Gompel and Antal van den Bosch.
2013.Parameter optimisation for Memory-based Cross-Lingual Word-Sense Disambiguation.
In Proceedingsof the 7th International Workshop on Semantic Eval-uation (SemEval 2013), in conjunction with the Sec-ond Joint Conference on Lexical and ComputationalSemantcis (*SEM 2013), Atlanta, USA.P.
Vossen, editor.
1998.
EuroWordNet: a multilingualdatabase with lexical semantic networks.
Kluwer Aca-demic Publishers, Norwell, MA, USA.166
