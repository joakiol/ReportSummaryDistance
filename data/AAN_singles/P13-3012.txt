Proceedings of the ACL Student Research Workshop, pages 81?88,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsMultigraph Clustering for Unsupervised Coreference ResolutionSebastian MartschatHeidelberg Institute for Theoretical Studies gGmbHSchloss-Wolfsbrunnenweg 3569118 Heidelberg, Germanysebastian.martschat@h-its.orgAbstractWe present an unsupervised model forcoreference resolution that casts the prob-lem as a clustering task in a directed la-beled weighted multigraph.
The modeloutperforms most systems participating inthe English track of the CoNLL?12 sharedtask.1 IntroductionCoreference resolution is the task of determiningwhich mentions in a text refer to the same en-tity.
With the advent of machine learning andthe availability of annotated corpora in the mid1990s the research focus shifted from rule-basedapproaches to supervised machine learning tech-niques.
Quite recently, however, rule-based ap-proaches regained popularity due to Stanford?smulti-pass sieve approach which exhibits state-of-the-art performance on many standard coref-erence data sets (Raghunathan et al 2010) andalso won the CoNLL-2011 shared task on coref-erence resolution (Lee et al 2011; Pradhan etal., 2011).
These results show that carefullycrafted rule-based systems which employ suitableinference schemes can achieve competitive perfor-mance.
Such a system can be considered unsuper-vised in the sense that it does not employ trainingdata for optimizing parameters.In this paper we present a graph-based approachfor coreference resolution that models a documentto be processed as a graph.
The nodes are men-tions and the edges correspond to relations be-tween mentions.
Coreference resolution is per-formed via graph clustering.
Our approach be-longs to a class of recently proposed graph modelsfor coreference resolution (Cai and Strube, 2010;Sapena et al 2010; Martschat et al 2012) andis designed to be a simplified version of existingapproaches.
In contrast to previous models be-longing to this class we do not learn any edgeweights but perform inference on the graph struc-ture only which renders our model unsupervised.On the English data of the CoNLL?12 shared taskthe model outperforms most systems which partic-ipated in the shared task.2 Related WorkGraph-based coreference resolution.
Whilenot developed within a graph-based framework,factor-based approaches for pronoun resolution(Mitkov, 1998) can be regarded as greedy clus-tering in a multigraph, where edges representingfactors for pronoun resolution have negative orpositive weight.
This yields a model similar tothe one presented in this paper though Mitkov?swork has only been applied to pronoun resolu-tion.
Nicolae and Nicolae (2006) phrase coref-erence resolution as a graph clustering problem:they first perform pairwise classification and thenconstruct a graph using the derived confidence val-ues as edge weights.
In contrast, work by Culottaet al(2007), Cai and Strube (2010) and Sapenaet al(2010) omits the classification step entirely.Sapena et al(2010) and Cai and Strube (2010)perform coreference resolution in one step usinggraph partitioning approaches.
These approachesparticipated in the recent CoNLL?11 shared task(Pradhan et al 2011; Sapena et al 2011; Caiet al 2011b) with excellent results.
The ap-proach by Cai et al(2011b) has been modified byMartschat et al(2012) and ranked second in theEnglish track at the CoNLL?12 shared task (Prad-han et al 2012).
The top performing system atthe CoNLL?12 shared task (Fernandes et al 2012)81also represents the problem as a graph by per-forming inference on trees constructed using themulti-pass sieve approach by Raghunathan et al(2010) and Lee et al(2011), which in turn wonthe CoNLL?11 shared task.Unsupervised coreference resolution.
Cardieand Wagstaff (1999) present an early approach tounsupervised coreference resolution based on astraightforward clustering approach.
Angheluta etal.
(2004) build on their approach and devise moresophisticated clustering algorithms.
Haghighi andKlein (2007), Ng (2008) and Charniak and El-sner (2009) employ unsupervised generative mod-els.
Poon and Domingos (2008) present a MarkovLogic Network approach to unsupervised corefer-ence resolution.
These approaches reach competi-tive performance on gold mentions but not on sys-tem mentions (Ng, 2008).
The multi-pass sieveapproach by Raghunathan et al(2010) can also beviewed as unsupervised.3 A Multigraph ModelWe aim for a model which directly represents therelations between mentions in a graph structure.Clusters in the graph then correspond to entities.3.1 MotivationTo motivate the choice of our model, let us con-sider a simple made-up example.Leaders met in Paris to discuss recentdevelopments.
They left the city today.We want to model that Paris is not a likely candi-date antecedent for They due to number disagree-ment, but that Leaders and recent developmentsare potential antecedents for They.
We want toexpress that Leaders is the preferred antecedent,since Leaders and They are in a parallel construc-tion both occupying the subject position in theirrespective sentences.In other words, our model should express thefollowing relations for this example:?
number disagreement for (They, Paris), whichindicates that the mentions are not coreferent,?
the anaphor being a pronoun for (They, Lead-ers), (They, recent developments) and (They,Paris), which is a weak indicator for corefer-ence if the mentions are close to each other,?
syntactic parallelism for (They, Leaders): bothmentions are in a parallel construction in adja-cent sentences (both in the subject slot), whichis also a weak coreference indicator.We denote these relations as N Number,P AnaPron and P Subject respectively.
Thegraphical structure depicted in Figure 1 mod-els these relations between the four mentionsLeaders, Paris, recent developments and They.Leadersrecent de-velopmentsTheyParisP AnaPronP SubjectP AnaPronNNumberPAnaPronFigure 1: An example graph modeling relationsbetween mentions.A directed edge from a mention m to n indi-cates that n precedes m and that there is some rela-tion between m and n that indicates coreference ornon-coreference.
Labeled edges describe the rela-tions between the mentions, multiple relations canhold between a pair.
Edges may be weighted.3.2 Multigraphs for Coreference ResolutionFormally, the model is a directed labeled weightedmultigraph.
That is a tuple D = (R, V,A,w)where?
R is the set of labels (in our case relations suchas P Subject that hold between mentions),?
V is the set of nodes (the mentions extractedfrom a document),?
A ?
V ?
V ?
R is the set of edges (relationsbetween two mentions),?
w is a mapping w : A?
R?
{??}
(weightsfor edges).Many graph models for coreference resolution op-erate on A = V ?V .
Our multigraph model allowsus to have multiple edges with different labels be-tween mentions.To have a notion of order we employ a directedgraph: We only allow an edge from m to n if mappears later in the text than n.To perform coreference resolution for a docu-ment d, we first construct a directed labeled multi-graph (Section 3.3).
We then assign a weight toeach edge (Section 3.4).
The resulting graph is82clustered to obtain the mentions that refer to thesame entity (Section 3.5).3.3 Graph ConstructionGiven a set M of mentions extracted from a doc-ument d, we set V = M , i.e.
the nodes of thegraph are the mentions.
To construct the edgesA, we consider each pair (m,n) of mentions withn ?
m. We then check for every relation r ?
Rif r holds for the pair (m,n).
If this is the casewe add the edge (m,n, r) to A.
For simplicity,we restrict ourselves to binary relations that holdbetween pairs of mentions (see Section 4).The graph displayed in Figure 1 is the graphconstructed for the mentions Leaders, Paris, re-cent developments and They from the examplesentence at the beginning of this Section, whereR = {P AnaPron, P Subject, N Number}.3.4 Assigning WeightsDepending on whether a relation r ?
R is indica-tive for non-coreference (e.g.
number disagree-ment) or for coreference (e.g.
string matching) itshould be weighted differently.
We therefore di-vide R into a set of negative relations R?
and aset of positive relations R+.Previous work on multigraphs for coreferenceresolution disallows any edge between mentionsfor which a negative relations holds (Cai et al2011b; Martschat et al 2012).
We take a sim-ilar approach and set w(m,n, r) = ??
for(m,n, r) ?
A when r ?
R?1.Work on graph-based models similar to ours re-port robustness with regard to the amount of train-ing data used (Cai et al 2011b; Cai et al 2011a;Martschat et al 2012).
Motivated by their obser-vations we treat every positive relation equally andset w(m,n, r) = 1 for (m,n, r) ?
A if r ?
R+.In contrast to previous work on similar graphmodels we do not learn any edge weights fromtraining data.
We compare this unsupervisedscheme with supervised variants empirically inSection 5.3.5 ClusteringTo describe the clustering algorithm used in thiswork we need some additional terminology.
Ifthere exists an edge (m,n, r) ?
A we say that n isa child of m.1We experimented with different weighting schemesfor negative relations on development data (e.g.
settingw(m,n, r) = ?1) but did not observe a gain in performance.In the graph constructed according to the pro-cedure described in Section 3.3, all children of amention m are candidate antecedents for m. Therelations we employ are indicators for coreference(which get a positive weight) and indicators fornon-coreference (which get a negative weight).We aim to employ a simple and efficient cluster-ing scheme on this graph and therefore choose1-nearest-neighbor clustering: for every m, wechoose as antecedent m?s child n such that the sumof edge weights is maximal and positive.
We breakties by choosing the closest mention.In the unsupervised setting described in Section3.4 this algorithm reduces to choosing the childthat is connected via the highest number of posi-tive relations and via no negative relation.For the graph depicted in Figure 1 this algorithmcomputes the clusters {They, Leaders}, {Paris}and {recent developments}.4 RelationsThe graph model described in Section 3 is basedon expressing relations between pairs of mentionsvia edges built from such relations.
We now de-scribe the relations currently used by our system.They are well-known indicators and constraintsfor coreference and are taken from previous work(Cardie and Wagstaff, 1999; Soon et al 2001;Rahman and Ng, 2009; Lee et al 2011; Cai et al2011b).
All relations operate on pairs of mentions(m,n), where m is the anaphor and n is a candi-date antecedent.
If a relation r holds for (m,n),the edge (m,n, r) is added to the graph.
We final-ized the set of relations and their distance thresh-olds on development data.4.1 Negative RelationsNegative relations receive negative weights.
Theyallow us to introduce well-known constraints suchas agreement into our model.
(1) N Gender, (2) N Number: Two mentions donot agree in gender or number.
We computenumber and gender for common nouns us-ing the number and gender data provided byBergsma and Lin (2006).
(3) N SemanticClass: Two mentions do notagree in semantic class (we only use the topcategories Object, Date and Person fromWordNet (Fellbaum, 1998)).
(4) N ItDist: The anaphor is it or they and thesentence distance to the antecedent is larger83than one.
(5) N Speaker12Pron: Two first person pro-nouns or two second person pronouns with dif-ferent speakers, or one first person pronounand one second person pronoun with the samespeaker2.
(6) N ContraSubObj: Two mentions are in thesubject/object positions of the same verb, theanaphor is a non-possessive/reflexive pronoun.
(7) N Mod: Two mentions have the same syntac-tic heads, and the anaphor has a nominal mod-ifier which does not occur in the antecedent.
(8) N Embedding: Two mentions where one em-beds the other, which is not a reflexive or pos-sessive pronoun.
(9) N 2PronNonSpeech: Two second personpronouns without speaker information and notin direct speech.4.2 Positive RelationsPositive relations are coreference indicators whichare added as edges with positive weights.
(10) P NonPron StrMatch: Applies only if theanaphor is definite or a proper name3.
This re-lation holds if after discarding stop words thestrings of mentions completely match.
(11) P HeadMatch: If the syntactic heads ofmentions match.
(12) P Alias: If mentions are aliases of each other(i.e.
proper names with partial match, fullnames and acronyms, etc.).
(13) P Speaker12Pron: If the speaker of the sec-ond person pronoun is talking to the speakerof the first person pronoun (applies only tofirst/second person pronouns).
(14) P DSPron: One mention is a speak verb?ssubject, the other mention is a first person pro-noun within the corresponding direct speech.
(15) P ReflPronSub: If the anaphor is a reflexivepronoun, and the antecedent is the subject ofthe sentence.
(16) P PossPronSub: If the anaphor is a posses-sive pronoun, and the antecedent is the subjectof the anaphor?s sentence or subclause.
(17) P PossPronEmb: The anaphor is a posses-2Like all relations using speaker information, this relationdepends on the gold speaker annotation layer in the corpus.3This condition is necessary to cope with the high-recalloutput of the mention tagger.sive pronoun embedded in the antecedent.
(18) P AnaPron: If the anaphor is a pronoun andnone of the mentions is a first or second per-son pronoun.
This relation is restricted to asentence distance of 3.
(19) P VerbAgree: If the anaphor is a third per-son pronoun and has the same predicate as theantecedent.
This relation is restricted to a sen-tence distance of 1.
(20) P Subject, (21) P Object: The anaphor is athird person pronoun and both mentions aresubjects/objects.
These relations are restrictedto a sentence distance of 1.
(22) P Pron StrMatch: If both mentions arepronouns and their strings match.
(23) P Pron Agreement: If both mentions aredifferent pronoun tokens but agree in number,gender and person.5 Evaluation5.1 Data and Evaluation MetricsWe use the data provided for the English track ofthe CoNLL?12 shared task on multilingual coref-erence resolution (Pradhan et al 2012) which isa subset of the upcoming OntoNotes 5.0 releaseand comes with various annotation layers providedby state-of-the-art NLP tools.
We used the officialdev/test split for development and evaluation.
Weevaluate the model in a setting that correspondsto the shared task?s closed track, i.e.
we use onlyWordNet (Fellbaum, 1998), the number and gen-der data of Bergsma and Lin (2006) and the pro-vided annotation layers.
To extract system men-tions we employ the mention extractor describedin Martschat et al(2012).We evaluate our system with the coreferenceresolution evaluation metrics that were used forthe CoNLL shared tasks on coreference, which areMUC (Vilain et al 1995), B3 (Bagga and Bald-win, 1998) and CEAFe (Luo, 2005).
We also re-port the unweighted average of the three scores,which was the official evaluation metric in theshared tasks.
To compute the scores we employedthe official scorer supplied by the shared task or-ganizers.5.2 ResultsTable 1 displays the performance of our model andof the systems that obtained the best (Fernandeset al 2012) and the median performance in the84MUC B3 CEAFe averageR P F1 R P F1 R P F1CoNLL?12 English development databest 64.88 74.74 69.46 66.53 78.28 71.93 54.93 43.68 48.66 63.35median 62.3 62.8 62.0 66.7 71.8 69.1 46.4 44.9 45.6 58.9this work (weights fraction) 64.00 68.56 66.20 66.59 75.67 70.84 50.48 45.52 47.87 61.63this work (weights MaxEnt) 63.72 65.78 64.73 66.60 73.76 70.00 47.46 45.30 46.36 60.36this work (unsupervised) 64.01 68.58 66.22 67.00 76.45 71.41 51.10 46.16 48.51 62.05CoNLL?12 English test databest 65.83 75.91 70.51 65.79 77.69 71.24 55.00 43.17 48.37 63.37median 62.08 63.02 62.55 66.23 70.45 68.27 45.74 44.74 45.23 58.68this work (weights fraction) 64.25 68.31 66.22 65.44 74.20 69.54 49.18 44.71 46.84 60.87this work (weights MaxEnt) 63.58 64.70 64.14 65.63 72.09 68.71 45.58 44.41 44.99 59.28this work (unsupervised) 63.95 67.99 65.91 65.47 74.93 69.88 49.83 45.40 47.51 61.10Table 1: Results of different systems on the CoNLL?12 English data sets.CoNLL?12 shared task, which are denoted as bestand median respectively.
best employs a struc-tured prediction model with learned combinationsof 70 basic features.
We also compare with twosupervised variants of our model which use thesame relations and the same clustering algorithmas the unsupervised model: weights fraction setsthe weight of a relation to the fraction of posi-tive instances in training data (as in Martschat etal.
(2012)).
weights MaxEnt trains a mention-pairmodel (Soon et al 2001) via the maximum en-tropy classifier implemented in the BART toolkit(Versley et al 2008) and builds a graph wherethe weight of an edge connecting two mentionsis the classifier?s prediction4.
We use the officialCoNLL?12 English training set for training.Our unsupervised model performs considerablybetter than the median system from the CoNLL?12shared task on both data sets according to all met-rics.
It also seems to be able to accommodate wellfor the relations described in Section 4 since it out-performs both supervised variants5.
The modelperforms worse than best, the gap according to B3and CEAFe being considerably smaller than ac-cording to MUC.
While we observe a decrease of1 point average score when evaluating on test datathe model still would have ranked fourth in the En-glish track of the CoNLL?12 shared task with only0.2 points difference in average score to the sec-ond ranked system.4The classifier?s output is a number p ?
[0, 1].
In order tohave negative weights we use the transformation p?
= 2p?1.5Compared with the supervised variants all improvementsin F1 score are statistically significant according to a pairedt-test (p < 0.05) except for the difference in MUC F1 toweights fraction.6 Error AnalysisIn order to understand weaknesses of our modelwe perform an error analysis on the developmentdata.
We distinguish between precision and recallerrors.
For an initial analysis we split the errorsaccording to the mention type of anaphor and an-tecedent (name, nominal and pronoun).6.1 Precision ErrorsOur system operates in a pairwise fashion.
Wetherefore count one precision error whenever theclustering algorithm assigns two non-coreferentmentions to the same cluster.
Table 2 shows theNAM NOM PRONAM 3413 (21%) 67 (66%) 11 (46%)NOM 43 (67%) 2148 (49%) 9 (89%)PRO 868 (32%) 1771 (55%) 5308 (24%)Table 2: Number of clustering decisions made ac-cording to mention type (rows anaphor, columnsantecedent) and percentage of wrong decisions.number of clustering decisions made according tothe mention type and in brackets the fraction of de-cisions that erroneously assign two non-coreferentmentions to the same cluster.
We see that two mainsources of error are nominal-nominal pairs and theresolution of pronouns.
We now focus on gain-ing further insight into the system?s performancefor pronoun resolution by investigating the perfor-mance per pronoun type.
The results are displayedin Table 3.
We obtain good performance for I andmy which in the majority of cases can be resolvedunambiguously by the speaker relations employedby our system.
The relations we use also seem85Anaphor all anaphoricI 1260 (13%) 1239 (11%)my 192 (14%) 181 (9%)he 824 (14%) 812 (13%).
.
.
.
.
.they 764 (29%) 725 (26%).
.
.
.
.
.you 802 (41%) 555 (15%)it 1114 (64%) 720 (44%)Table 3: Precision statistics for pronouns.
Rowsare pronoun surfaces, columns number of cluster-ing decisions and percentage of wrong decisionsfor all and only anaphoric pronouns respectively.to work well for he.
In contrast, the local, shal-low approach we currently employ is not able toresolve highly ambiguous pronouns such as they,you or it in many cases.
The reduction in error ratewhen only considering anaphoric pronouns showsthat our system could benefit from an improveddetection of expletive it and you.6.2 Recall ErrorsEstimating recall errors by counting all missingpairwise links would consider each entity manytimes.
Therefore, we instead count one recall er-ror for a pair (m,n) of anaphor m and antecedentn if (i) m and n are coreferent, (ii) m and n arenot assigned to the same cluster, (iii) m is the firstmention in its cluster that is coreferent with n, and(iv) n is the closest mention coreferent with m thatis not in m?s cluster.This can be illustrated by an example.
Consid-ering mentions m1, .
.
.
,m5, assume that m1, m3,m4 and m5 are coreferent but the system clustersare {m2,m3} and {m4,m5}.
We then count tworecall errors: one for the missing link from m3 tom1 and one for the missing link from m4 to m3.According to this definition we count 3528 re-call errors on the development set.
The distribu-tion of errors is displayed in Table 4.
We see thatNAM NOM PRONAM 321 220 247NOM 306 797 330PRO 306 476 525Table 4: Number of recall errors according tomention type (rows anaphor, columns antecedent).the main source of recall errors are missing linksof nominal-nominal pairs.
We randomly extracted50 of these errors and manually assigned them todifferent categories.29 errors: missing semantic knowledge.
In thesecases lexical or world knowledge is needed tobuild coreference links between mentions with dif-ferent heads.
For example our system misses thelink between the sauna and the hotbox sweatbox.14 errors: too restrictive N Mod.
In these casesthe heads of the mentions matched but no link wasbuilt due to N Mod.
An example is the missinglink between our island?s last remaining forest ofthese giant trees and the forest of Chilan.4 errors: too cautious string match.
We onlyapply string matching for common nouns when thenoun is definite.Three errors could not be attributed to any of theabove categories.7 Conclusions and Future WorkWe presented an unsupervised graph-based modelfor coreference resolution.
Experiments show thatour model exhibits competitive performance onthe English CoNLL?12 shared task data sets.An error analysis revealed that two mainsources of errors of our model are the inaccurateresolution of highly ambiguous pronouns such asit and missing links between nominals with dif-ferent heads.
Future work should investigate howsemantic knowledge and more complex relationscapturing deeper discourse properties such as co-herence or information status can be added to themodel.
Processing these features efficently mayrequire a more sophisticated clustering algorithm.We are surprised by the good performance ofthis unsupervised model in comparison to thestate-of-the-art which uses sophisticated machinelearning techniques (Fernandes et al 2012) orwell-engineered rules (Lee et al 2011).
We arenot sure how to interpret these results and want toleave different interpretations for discussion:?
our unsupervised model is really that good(hopefully),?
the evaluation metrics employed are to bequestioned (certainly),?
efficiently making use of annotated trainingdata still remains a challenge for the state-of-the-art (likely).AcknowledgmentsThis work has been funded by the Klaus TschiraFoundation, Germany.
The author has been sup-ported by a HITS PhD scholarship.86ReferencesRoxana Angheluta, Patrick Jeuniaux, Rudradeb Mitra,and Marie-Francine Moens.
2004.
Clustering al-gorithms for noun phrase coreference resolution.
InProceedings of the 7e`mes Journe?es Internationalesd?Analyse Statistique des Donne?es Textuelles, Lou-vain La Neuve, Belgium, 10?12 March 2004, pages60?70.Amit Bagga and Breck Baldwin.
1998.
Algorithmsfor scoring coreference chains.
In Proceedingsof the 1st International Conference on LanguageResources and Evaluation, Granada, Spain, 28?30May 1998, pages 563?566.Shane Bergsma and Dekang Lin.
2006.
Bootstrap-ping path-based pronoun resolution.
In Proceedingsof the 21st International Conference on Computa-tional Linguistics and 44th Annual Meeting of theAssociation for Computational Linguistics, Sydney,Australia, 17?21 July 2006, pages 33?40.Jie Cai and Michael Strube.
2010.
End-to-end coref-erence resolution via hypergraph partitioning.
InProceedings of the 23rd International Conferenceon Computational Linguistics, Beijing, China, 23?27 August 2010, pages 143?151.Jie Cai, E?va Mu?jdricza-Maydt, Yufang Hou, andMichael Strube.
2011a.
Weakly supervised graph-based coreference resolution for clinical data.
InProceedings of the 5th i2b2 Shared Tasks and Work-shop on Challenges in Natural Language Processingfor Clinical Data, Washington, D.C., 20-21 October2011.Jie Cai, E?va Mu?jdricza-Maydt, and Michael Strube.2011b.
Unrestricted coreference resolution viaglobal hypergraph partitioning.
In Proceedings ofthe Shared Task of the 15th Conference on Computa-tional Natural Language Learning, Portland, Oreg.,23?24 June 2011, pages 56?60.Claire Cardie and Kiri Wagstaff.
1999.
Noun phrasecoreference as clustering.
In Proceedings of the1999 SIGDAT Conference on Empirical Methods inNatural Language Processing and Very Large Cor-pora, College Park, Md., 21?22 June 1999, pages82?89.Eugene Charniak and Micha Elsner.
2009.
EM worksfor pronoun anaphora resolution.
In Proceedings ofthe 12th Conference of the European Chapter of theAssociation for Computational Linguistics, Athens,Greece, 30 March ?
3 April 2009, pages 148?156.Aron Culotta, Michael Wick, and Andrew McCal-lum.
2007.
First-order probabilistic models forcoreference resolution.
In Proceedings of HumanLanguage Technologies 2007: The Conference ofthe North American Chapter of the Association forComputational Linguistics, Rochester, N.Y., 22?27April 2007, pages 81?88.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press, Cambridge,Mass.Eraldo Fernandes, C?
?cero dos Santos, and Ruy Milidiu?.2012.
Latent structure perceptron with feature in-duction for unrestricted coreference resolution.
InProceedings of the Shared Task of the 16th Confer-ence on Computational Natural Language Learning,Jeju Island, Korea, 12?14 July 2012, pages 41?48.Aria Haghighi and Dan Klein.
2007.
Unsupervisedcoreference resolution in a nonparametric Bayesianmodel.
In Proceedings of the 45th Annual Meetingof the Association for Computational Linguistics,Prague, Czech Republic, 23?30 June 2007, pages848?855.Heeyoung Lee, Yves Peirsman, Angel Chang,Nathanael Chambers, Mihai Surdeanu, and Dan Ju-rafsky.
2011.
Stanford?s multi-pass sieve corefer-ence resolution system at the CoNLL-2011 sharedtask.
In Proceedings of the Shared Task of the15th Conference on Computational Natural Lan-guage Learning, Portland, Oreg., 23?24 June 2011,pages 28?34.Xiaoqiang Luo.
2005.
On coreference resolutionperformance metrics.
In Proceedings of the Hu-man Language Technology Conference and the 2005Conference on Empirical Methods in Natural Lan-guage Processing, Vancouver, B.C., Canada, 6?8October 2005, pages 25?32.Sebastian Martschat, Jie Cai, Samuel Broscheit, E?vaMu?jdricza-Maydt, and Michael Strube.
2012.
Amultigraph model for coreference resolution.
InProceedings of the Shared Task of the 16th Confer-ence on Computational Natural Language Learning,Jeju Island, Korea, 12?14 July 2012, pages 100?106.Ruslan Mitkov.
1998.
Robust pronoun resolution withlimited knowledge.
In Proceedings of the 17th In-ternational Conference on Computational Linguis-tics and 36th Annual Meeting of the Associationfor Computational Linguistics, Montre?al, Que?bec,Canada, 10?14 August 1998, pages 869?875.Vincent Ng.
2008.
Unsupervised models for corefer-ence resolution.
In Proceedings of the 2008 Con-ference on Empirical Methods in Natural LanguageProcessing, Waikiki, Honolulu, Hawaii, 25?27 Oc-tober 2008, pages 640?649.Cristina Nicolae and Gabriel Nicolae.
2006.
BestCut:A graph algorithm for coreference resolution.
InProceedings of the 2006 Conference on EmpiricalMethods in Natural Language Processing, Sydney,Australia, 22?23 July 2006, pages 275?283.Hoifung Poon and Pedro Domingos.
2008.
Joint unsu-pervised coreference resolution with Markov Logic.In Proceedings of the 2008 Conference on EmpiricalMethods in Natural Language Processing, Waikiki,Honolulu, Hawaii, 25?27 October 2008, pages 650?659.87Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,Martha Palmer, Ralph Weischedel, and NianwenXue.
2011.
CoNLL-2011 Shared Task: Modelingunrestricted coreference in OntoNotes.
In Proceed-ings of the Shared Task of the 15th Conference onComputational Natural Language Learning, Port-land, Oreg., 23?24 June 2011, pages 1?27.Sameer Pradhan, Alessandro Moschitti, and NianwenXue.
2012.
CoNLL-2012 Shared Task: Modelingmultilingual unrestricted coreference in OntoNotes.In Proceedings of the Shared Task of the 16th Con-ference on Computational Natural Language Learn-ing, Jeju Island, Korea, 12?14 July 2012, pages 1?40.Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-garajan, Nathanael Chambers, Mihai Surdeanu, DanJurafsky, and Christopher Manning.
2010.
A multi-pass sieve for coreference resolution.
In Proceed-ings of the 2010 Conference on Empirical Methodsin Natural Language Processing, Cambridge, Mass.,9?11 October 2010, pages 492?501.Altaf Rahman and Vincent Ng.
2009.
Supervised mod-els for coreference resolution.
In Proceedings of the2009 Conference on Empirical Methods in NaturalLanguage Processing, Singapore, 6?7 August 2009,pages 968?977.Emili Sapena, Llu?
?s Padro?, and Jordi Turmo.
2010.
Aglobal relaxation labeling approach to coreferenceresolution.
In Proceedings of Coling 2010: PosterVolume, Beijing, China, 23?27 August 2010, pages1086?1094.Emili Sapena, Llu?
?s Padro?, and Jordi Turmo.
2011.RelaxCor participation in CoNLL shared task oncoreference resolution.
In Proceedings of theShared Task of the 15th Conference on Computa-tional Natural Language Learning, Portland, Oreg.,23?24 June 2011, pages 35?39.Wee Meng Soon, Hwee Tou Ng, and DanielChung Yong Lim.
2001.
A machine learning ap-proach to coreference resolution of noun phrases.Computational Linguistics, 27(4):521?544.Yannick Versley, Simone Paolo Ponzetto, MassimoPoesio, Vladimir Eidelman, Alan Jern, Jason Smith,Xiaofeng Yang, and Alessandro Moschitti.
2008.BART: A modular toolkit for coreference resolu-tion.
In Companion Volume to the Proceedings ofthe 46th Annual Meeting of the Association for Com-putational Linguistics, Columbus, Ohio, 15?20 June2008, pages 9?12.Marc Vilain, John Burger, John Aberdeen, Dennis Con-nolly, and Lynette Hirschman.
1995.
A model-theoretic coreference scoring scheme.
In Proceed-ings of the 6th Message Understanding Conference(MUC-6), pages 45?52, San Mateo, Cal.
MorganKaufmann.88
