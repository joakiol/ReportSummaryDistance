Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 378?381,Prague, June 2007. c?2007 Association for Computational LinguisticsUCD-S1: A hybrid model for detecting semantic relations between nounpairs in textCristina ButnariuSchool of Computer Science and InformaticsUniversity College DublinBelfield, Dublin 4, Ireland.ioana.butnariu@UCD.ieTony VealeSchool of Computer Science and InformaticsUniversity College DublinBelfield, Dublin 4, Ireland.tony.veale@UCD.ieAbstractWe describe a supervised learning approach tocategorizing  inter-noun  relations,  based  onSupport Vector Machines, that builds a differ-ent classifier for each of seven semantic rela-tions.
Each  model  uses  the  same  learningstrategy,  while  a  simple  voting  procedurebased on five trained discriminators with vari-ous  blends  of  features  determines  the  finalcategorization.
The features that  characterizeeach of the noun pairs are a blend of lexical-semantic  categories extracted  from WordNetand  several  flavors  of  syntactic  patterns  ex-tracted  from  various  corpora,  includingWikipedia and the WMTS corpus.1 IntroductionThe  SemEval  task  for  classifying  inter-nounsemantic  relations  employs  seven  semanticrelations  that  are  not  exhaustive:  Cause-Effect,Instrument-Agency,  Product-Producer  Origin-Entity,  Theme-Tool,  Part-Whole  and  Content-Container.
The  task  is  to  classify  the  relationsbetween pairs of concepts that are part of the samesyntactic  structure  in  a  given  sentence.
Thisapproach  employs  a  context-dependentclassification,  as  opposed to  usual  out-of-contextapproaches  in  classifying  semantic  relationsbetween noun pairs (e.g., (Turney, 2005), (Nastaseet.
al., 2006)).Our  approach  is  based  on  the  Support  VectorMachines  learning  paradigm  (Vapnik,  1995),  inwhich supervised machine learning is used to findthe most  salient combination of features for eachsemantic relation.
These features include semanticgeneralizations of  the noun-senses as encoded asWordNet (WN) hyponyms,  some manually selec-ted  linguistic  features  (e.g.,  agentive,  gerundive,etc.)
as well as the observed relational behaviour ofthe given nouns in three different corpora: the col-lected  glosses  of  WordNet;  the  collected  text  ofWikipedia; and the WMTS corpus.One can find similar approaches in the literatureto the semantic classification of noun compounds.Turney (2005) uses automatically extracted para-phrases to build a similarity measure between pairsof concepts, while  Nastase et.
al.
(2006) proposesseparate models for two different word representa-tions  when  determining  the  semantic  relation  inmodifier-noun compounds:  a model  based on thelexico-semantic aspects of words and a model thatuses contextual information from corpora.
Our ap-proach is different in that we use all the availablefeatures of word representations and concept inter-actions in a single hybrid model.2 System descriptionOur  system,  named  the  Semantic  Relation  Dis-criminator (or SRD), takes as input a set of nounpairs that are manually classified as positive/negat-ive for a given semantic relation and produces asoutput  a  discriminator  for  that  semantic  relation.We used SRD to learn different models for each ofthe  seven  semantic  relations  in  the  classificationscheme for task 4 in the SemEval Workshop.
TheSRD system relies  on several  data-resources  andtools:  the  WN  noun-sense  hierarchy,  a  corpusmade up of the WordNet glosses, the complete textof  Wikipedia  (downloaded June,  2005),  a  searchengine indexing a very large corpus of text, and theWEKA  Data  Mining  software  package  (version3.5).SRD combines  two types  of  features  for  eachnoun pair:  semantic  features  extracted  from  WNnoun-sense hierarchy, for which the WN synset-id378information of each noun is used and syntactic fea-tures extracted from the unlabeled and unstructuredcorpora mentioned above for which a shallow pars-ing approach is employed.2.1 Feature acquisitionSRD follows four steps in acquiring features:?
Select  semantic  generalizations.
For  eachnoun-sense in a pair, SRD extracts all hyper-nyms  at  depth  8 or  higher  in  the  WordNetnoun-sense hierarchy.?
Extract  syntactic  phrases.
SRD  looks  forphrases in corpora that occur before or aftereach noun in a pair and which obey one ofseveral  syntactic  templates.
SRD also looksfor  joining  phrases  between  each  pair  ofnouns that contain 5 words or less.?
Clean-up these phrases.
SRD lemmatizes thewords in each phrase and removes functionwords such as articles, possessive pronouns,adjective and adverbs.?
Record  observed  patterns.
For  each  nounpair, SRD records the following types of syn-tactic patterns together with their corpus fre-quencies: joining terms that comprise at leastone verb; phrases that are composed of oneverb  and  one  preposition;  and  phrases  thatare composed of a simple verb or a phrasalverb.2.2 Selecting the featuresDue to the large number of  features extracted inthese  steps,  SRD employs  five  different  modelsthat  use  different  combination  of  features  andwhich pool their votes to determine a single predic-ation for each learning task.
We describe below thefeature sets used for each component.
The featureshave binary values: 1 if the feature is present for anoun pair, and 0 otherwise.Each model employs WordNet hypernyms (fromthe  top  8  layers  of  the  noun  hierarchy)  of  bothnoun-senses as semantic features, while models 1and 2 employ the following additional features foreach noun pair (N1, N2):1.
The  most  frequent  syntactic  patterns  thatappear between N1 and N2 in corpora2.
The  most  frequent  syntactic  patterns  thatappear between N2 and N1 in corporaModel 1 and Model 2 differ only in the syntactictemplates  used  to  validate  inter-noun  patterns.Model  1  fixates  on  patterns  that  contain  a  verb,while Model 2 accepts patterns that contain either apreposition or a verb, or both.
This yields, on aver-age, 5,000 binary features for Model 1 for each ofthe seven relation types, and an average of 10,000binary features for Model 2.In addition to WN-derived hypernymic-features,models 3 and 4 employ the following:1.
The  most  frequent  syntactic  patterns  thatimmediately precede N1 in a corpus2.
The  most  frequent  syntactic  patterns  thatimmediately follow N1 in a corpus3.
The  most  frequent  syntactic  patterns  thatimmediately precede N2 in a corpus4.
The  most  frequent  syntactic  patterns  thatimmediately follow N2 in a corpusIn Model  3 each syntactic  pattern comprises ahyphenated  verb,  while  the  syntactic  patterns  inModel 4 each contain a preposition or a verb.
SRDgenerates,  on  average,  1,500  binary  features  inModel 3 and 2,500 features in Model 4 for each re-lation-type.In addition to WN-derived hypernymic-features,model 5 employs the following:1.
A set of linguistic features for N1, indicat-ing  whether  this  noun  is  a  nominalizedverb, or whether it frequently appears in aspecific semantic case role (e.g., agent).2.
The same set of linguistic features as de-termined for N2.SRD generates, on average, approximately 700binary features for each relation-type in Model 5.2.3 Building the modelsThe  SVM  learning  paradigm  seems  particularlysuitable  to  our  task  for  a  number  of  reasons.Firstly, it  behaves robustly for all  seven learningtasks, ignoring the noise in the training set.
This isimportant, since e.g., some training pairs for the In-strument-Agency relation were labeled as both trueand false.
Secondly, SVM has an automated mech-anism  for  parameter  tuning,  which  reduces  theoverall computational effort.SRD employs  polynomial  SVMs because  theyappear  to  perform  better  for  this  task  compared379with simple linear SVMs or radial-basis functions.We used the WEKA implementation of John Plat-t?s Sequential Minimal Optimization method (Platt,1998) to train the feature weights on all the avail-able training data.
Using SMO to train the polyno-mial SVM takes approx.
2.8 CPU sec.
per model.The motivation for a multiple model scheme ap-proach comes from empirical results.
SRD yieldshigher  results  relative  to  the  five  single  modelsschemes that compose our system when evaluatedusing 10-fold cross validation on the training data.3 Experiments and ResultsThe SemEval  data-set  for  each  of  the  seven  se-mantic relations comprises 140 annotated instancesfor training and between 70 to 90 for testing.
Eachinstance  is  manually  labelled  with  the  part  ofspeech of each concept in a pair, as well as the WNsynset-id of the intended word-sense and a samplesentential context.
SRD?s predictions fall into eval-uation category B, as the system uses WN synset-id but not the query pattern used to originally pop-ulate the data-sets with instances.
SRD also skipsthose  training  instances  where  WN sense-ids  arenot provided, so that the actual number of traininginstances used ranges from 129 to 138 manually la-belled examples per relation-type.SRD?s  precision,  recall,  F-score  and  accuracyfor each relation is given by Table 1.P R F1 Acc #t inst.Cause-Effect 69.8 73.2 71.4 70.0 80Instrument-Agency 72.5 76.3 74.4 74.4 78Product-Producer 80.6 87.1 83.7 77.4 93Origin-Entity 60.0 50.0 54.5 63.0 81Theme-Tool 50.0 34.5 40.8 59.2 71Part-Whole 71.4 57.7 63.8 76.4 72Content-Container 84.8 73.7 78.9 79.7 74Average 69.9 64.6 66.8 71.4 78.4Table1.
Results for SRD across the seven learning tasksTo  assess  the  effect  of  varying  quantities  oftraining  data,  the  model  was  tested  on  differentfractions of the training data: dataset B1 comprisesthe first quarter of the training data, dataset B2 thefirst  half,   while  B3  dataset  comprises  the  firstthree  quarters  and  B4  comprises  the  completetraining dataset.
We report the behavior of SRD inpredicting the unseen test data when learning fromthese datasets in table 2.
The measures of table 2represent an average of SRD?s performance acrossall relation-types.P R F1 AccDataset B1  65.4 53.3 56.4 66.2Dataset B2 67.8 63.8 63.5 69.6Dataset B3 71.7 64.0 66.8 71.6Dataset B4 69.9 64.6 66.8 71.4Table2.
Results for SRD on different training datasets3.1 Error analysisThree types of baseline values were proposed forthis  task.
Baseline  1 (?majority baseline?)
is  ob-tained by always guessing either "true" or "false",according to whichever is the majority category inthe testing data-set for the given relation.
Baseline2 (?alltrue baseline?)
is achieved by always guess-ing  ?true?.
Baseline  3  (?probmatch  baseline?)
isobtained  by  randomly  guessing  "true"  or  "false"with  a  probability  matching  the  distribution  of"true" or "false" in the testing dataset.0102030405060708090class1 class2 class3 class4 class5 class6 class7SRD Baseline1 Baseline2 Baseline3Figure1.
Comparison  of  SRD?s  F-scores  for  each  se-mantic relation and the corresponding baselines.Figure 1 plots the F-scores obtained for each se-mantic relation.
We observe that SRD has exhibitspoor performance on two particular relations, Ori-gin-Entity and Theme-Tool, denoted ?class4?
and?class5?
in the plot of Figure 1.
SRD achieves thesame  F-measure  score  as  the  random predictionbaseline for Theme-Tool class, suggesting that thefeatures used are simply not capable of building adiscriminator for this semantic relation.
SRD?s F-score for Origin-Entity class is 10% higher than therandom baseline, but still performs below the othertwo baselines.
SRD?s best performance is achievedfor Product-Producer and Part-Whole,  with an F-score 11% higher than the highest baseline.380Table3.
SRD F-measures using different feature sets3.2 ImprovementsOne obvious problem with SRD is that we use ahigh-dimensional feature-space to train each mod-el.
Research in text categorization (e.g., Dumais etal., 1998) shows that feature selection algorithmslike information gain can identify the most produc-tive dimensions of the feature space and simultane-ously boost classification accuracy.To explore  this  potential  for  improvement,  weapplied two types of feature selection filters (usingWEKA): the InfoGainAttrEval filter that evaluatesthe utility of  a feature  by measuring informationgain w.r.t.
the class; and the  CfsSubsetEval filter,which evaluates the utility of a subset of featuresby considering the individual predictive ability ofeach individually and the degree of redundancy be-tween  them  collectively.
Results  of  our  experi-ments with SRD using different subsets of featuresets are displayed in Table 3.
Set 1 is the completeset of all features.
Set 2 is the subset obtained withthe  top  n  features  as  ranked  by the  InfoGainAt-trEval filter  (n is  determined using 10-fold crossvalidation on the training data).
Set 3 is a tailoredfeature-set created for each relation-type using theCfsSubsetEval filter.
Set 4 is the subset of all fea-tures extracted from WN.We find that feature-filtering boosts the perfor-mance of some learning tasks by up to 14 % (e.g.,the Theme-Tool relation), but it can also decreaseperformance by the same amount (e.g., the Origin-Entity relation).
SRD achieves its best performance-- an overall F-measure of 71.7% -- when using afeature set that is tailored to each of the semanticrelation classification tasks (e.g., Set 4 (WN only)for Origin-Entity, Set 1 (all) for Product-Producerand Container-Content, Set 4 and Set 3 (relation-specific subsets) for everything else).4 ConclusionsSRD  is  an  SVM-based  approach  to  classifyingnoun-pairs into categories that best reflect the se-mantic relationship underlying each pair.
Withoutfeature-filtering, SRD shows modest classificationcapability, performing better than the highest base-lines for five of the seven relational classes.
Exper-iments  with  feature  filtering  encourage  us  to  tryand refine SRD?s feature space to focus on morediscriminatory and semantically-revealing featuresof nouns.
Feature-filtering can diminish as well asimprove performance, and thus, should ideally belinked to an insightful theory of how particular fea-tures  contribute  to  the  human-understanding  ofnoun-noun  pairs.
Filtering  techniques  provide  agood basis for formulating feature-based hypothe-ses, but the most productive feature sets will come,we hope, from a cognitive and conceptual under-standing of  the  processes  of  phrase  construction,rather than from an exhaustive and largely theory-free exploration of different feature-sets.AcknowledgmentsWe would like to thank Peter Turney for grantingus access to the NRC copy of the WMTS.ReferencesJoachims,  T.  (1998)  Text  categorization  with  supportvector  machines:  learning  with  many  relevant  fea-tures.
Proceedings of ECML-98, 10th European Con-ference on Machine Learning.Dumais,  S.  T.,  Platt,  J.,  Heckerman  D.,  Sahami  M.,(1998) Inductive learning algorithms and representa-tions  for  text  categorization,  Proceedings  of  ACM-CIKM98Nastase, V., Sayyad-Shirabad, J., Sokolova, M., and Sz-pakowicz, S. (2006).
Learning noun-modifier seman-tic  relations  with  corpus-based and WordNet-basedfeatures.
In Proceedings of the 21st National Confer-ence on Artificial Intelligence, Boston, MA.Platt, J.
(1998), Fast Training of SVMs Using Sequen-tial Minimal Optimization,  Support Vector MachineLearning, MIT Press, Cambridge.Turney, P.D.
(2005).
Measuring semantic similarity bylatent relational analysis.
In Proceedings of the Nine-teenth  International  Joint  Conference  on  ArtificialIntelligence, Edinburgh, Scotland.Vapnik, V. (1995).
The Nature of Statistical  LearningTheory, Springer-Verlag, New YorkFeatureSet1FeatureSet2FeatureSet3FeatureSet4Cause-Effect 71.4 72.7 75.7 61.3Instrument-Agency 74.4 74.6 76.3 72Product-Producer 83.7 81.3 80.5 77Origin-Entity 54.5 44.8 38 61.5Theme-Tool 40.8 42.8 53.8 42.5Part-Whole 63.8 72.3 62.7 60Content-Container 78.9 75.6 77.1 73.2Average 66.8 66.3 66.3 64381
