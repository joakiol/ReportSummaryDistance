Word sense disambiguation criteria: a systematic studyLaurent AUDIBERTDELIC, Universit?
de Provence29 Av.
Robert SCHUMAN ?
13621Aix-en-Provence Cedex 1, FRANCElaurent.audibert@up.univ-aix.frAbstractThis article describes the results of a systematic in-depth study of the criteria used for word sensedisambiguation.
Our study is based on 60 targetwords: 20 nouns, 20 adjectives and 20 verbs.
Ourresults are not always in line with some practices inthe field.
For example, we show that omitting non-content words decreases performance and thatbigrams yield better results than unigrams.1 IntroductionThe task of word sense disambiguation (WSD) isto identify the correct sense of a word in context.WSD is usually performed by matchinginformation from the context in which the wordoccurs with disambiguation knowledge source.
Ourapproach uses supervised machine-learningtechniques to automatically acquire suchdisambiguation knowledge from sense-taggedcorpora.
At present, this type of approach is widelyused and seems to yield the best results (Kilgarriff,Rosenzweig, 2000; Ng, 1997b).Information conveyed by context words(morphological form) is enriched with furtherannotations: part-of-speech tag, lemma, etc.
Eachindividual piece of information is called a feature.A good feature should capture an important sourceof knowledge that is critical in determining thesense of the word to be disambiguated.
The choiceof the appropriate set of features is an importantissue for WSD (Bruce, Wiebe, Perdersen, 1996;Ng, Zelle, 1997; Pedersen, 2001).
Thus, this paperdescribes the results of a systematic and in-depthstudy of homogenous criteria (i.e.
set of features)that can be used for WSD.2 Methodology2.1 CorpusThe corpus we worked on is composed ofdifferent types of texts and comprises 6 468 522words.
It was put together within the framework ofthe SyntSem project that aims at producing aFrench corpus which is morphologically andsyntactically tagged, lemmatised and thatcomprises a light syntactical tagging as well as alexical tagging of 60 target words selected for theirstrongly polysemous nature (V?ronis, 1998)1.These 60 target words are evenly divided into 20nouns, 20 adjectives and 20 verbs, having a total of53 796 occurrences in the corpus.The inadequacy of standard dictionaries(V?ronis, 2001) and computational lexicons(Palmer, 1998) for natural language processing ispresently one of the major difficulties encounteredin word sense disambiguation.
For instance, byusing these dictionaries, the inter-annotatoragreement may sometimes reach only 57% (Ng,Lee, 1996) or may simply be equivalent to randomsense allocation (V?ronis, 1998).
To overcome thisweakness, a dictionary more specific to naturallanguage processing is being developed in ourteam (Reymond, 2002).
It has been used to tag theoccurrences of the 60 target words of the SyntSemcorpus.Table 8 in the appendix gives quantitativeinformation for each target word.
The number ofsenses per word may be very large for it includesidioms and phrasal verbs such as: ?
mettre surpied ?, ?
mettre ?
pied ?, ?
pied de nez ?, etc.A general agreement seems to emerge accordingto which morpho-syntatic disambiguation andsense disambiguation can be disentangled(Kilgarriff, 1997; Ng, Zelle, 1997).
We haveentrusted the part-of-speech tagging of our corpusto the Cordial software (developed by SynapseD?veloppement company) as it offerslemmatisation and part-of-speech tagging of asatisfactory accuracy (Valli, V?ronis, 1999).mform lemma ems cgems sensemettre mettre VINF VINF 1.12.7fin fin NCFS NCOM?
?
PREP PREPla le DETDFS DETpratique pratique NCFS NCOMdes de DETDPIG DETd?tentions d?tention NCFP NCOM 1Table 1: SyntSem tagged corpus extract.1 These words are those used in the French part of theSenseval-1 evaluation exercice (Segond, 2000) but thecorpus and dictionary are different in the present study.Table 1 displays an extract of the SyntSemcorpus.
It shows all the tags of each word.
We usethe information provided by these tags in ourlexical disambiguation criteria.2.2 CriteriaThe aim of our study is to evaluate a largevariety of homogenous criteria (i.e.
set of features).The name of each criterion specifies its nature andtakes the following form [par1|par2|par3|par4].Parameter par1 indicates whether the criteriontakes into account unigrams (par1=1gr), bigrams(par1=2gr) or trigrams (par1=3gr), knowing thatan n-gram represents the juxtaposition of n words.Parameter par2 indicates which word tag isconsidered: morphological form (par2=mform),lemma (par2=lemma), part-of-speech (par2=ems)or coarse-grained part-of-speech (par2=cgems).Parameter par3 indicates if we take into accountword positions (par3=position), if we onlydistinguish left from right context (par3=leftright),or if we simply consider unordered set ofsurrounding words (par3=unordered).
Lastly,parameter par4 shows whether the criterion takesinto account all the words (par4=all) or contentwords only (par3=content).
We call these criteria?homogeneous criteria?
as the four parameterstogether determine the nature of all pieces ofcontextual evidence selected by the criterion.For contexts within a range of ?1 to ?
8 words,the combination of all parameters generates 576(3?4?3?2?8) distinct criteria.
We havesystematically evaluated each one of these criteriaas well as other criteria in order to answer specificquestions and to validate or invalidate certainhypothesis.Within the framework of this study, we havedeveloped an application used to model thesecriteria and to further apply them to the corpus inorder to generate feature vectors used by ourclassifiers (Audibert, 2001).2.3 ClassifiersWe have selected two complementary classifiers.We have chosen the Na?ve-Bayes classifier (NB)for its simplicity and widespread use, as well as forits well-known state-of-the-art accuracy onsupervised WSD (Domingos, Pazzani, 1997;Mooney, 1996; Ng, 1997a).
The NB classifierassumes the features are independent given thesense.
During classification, it chooses the sensewith the highest posterior probability.
We havealso selected a decision list classifier (DL) which issimilar to the classifier used by (Yarowsky, 1994)for words having two senses, and extended formore senses by (Golding, 1995).
DL classifier isfurther developed in (Audibert, 2003).
In DL,features are sorted in order of decreasing strength,where strength reflects feature reliability fordecision-making.
The DL classifier distinguishesitself clearly from the NB classifier as it does notcombine the features, but bases its classificationssolely on the single most reliable feature identifiedin the target context selected by the criteria.
Wewill make a use of this decision-makingtransparency several times in this article.
Someother advantages of DL classifier are its significantsimplicity and its ease of implementation.Both of the classifiers we used requireprobability estimates.
Given the data-sparseness,we have to deal with zero or low frequency counts.For this reason, we have decided to use m-estimation (Cussens, 1993) rather than classicalestimations of probabilities or Laplace (?add one?
)smoothing.When a classifier is not able to disambiguate atarget word, which is very rare, it selects the mostfrequent sense from the training data.
Thus, alloccurrences are tagged.
As in this case precisionequals the recall, the present article speaks in termsof precision only.To evaluate a criterion in the corpus, we use a k-fold cross-validation method (in accordance withthe common use, in our experiment, k=10).
Despitethe fact that this method takes much computingtime, it enables the evaluation of the criterion inthe whole corpus.Throughout the tests, the two classifiers havegenerally obtained comparable accuracy, althoughthe NB classifier has almost systematicallyoutperformed the DL classifier.3 Results3.1 Best criteria precisionTable 2 displays for each of the target wordsstudied the optimal context size and thedisambiguation precision obtained by the bestunigram, bigram and trigram-based criteria.This table points out that best criteria take intoaccount all words in the context.
Section 3.2 willconcentrate on feature reliability according to theirpart-of-speech.
Then, section 3.2 will deal with theimpact of different feature selections based onNouns Adjectives VerbsCriterion P% S P% S P% S[1gr|lemma|ordered|all] 81.9 ?2 76.8 ?1 71.8 ?3[2gr|lemma|leftright|all] 83.6 ?4 77.9 ?3 74.0 ?4[3gr|lemma|leftright|all] 82.3 ?5 72.7 ?3 71.2 ?5Table 2: Optimal context size (S) and criteriaprecision (P%) using NB classifier.features part-of-speech.
Nouns Adjectives VerbsP% U% P% U% P% U%NCOM 93.0 12.7 93.7 25.3 87.7 26.0DET 73.8 30.2 69.8 21.3 48.1 12.7PREP 78.3 24.2 61.4 15.2 62.9 17.4ADJ 94.9 13.7 80.3 2.2 65.5 3.2ADV 57.0 1.1 79.2 9.6 60.3 5.7PROPE 63.6 0.6 67.0 2.6 65.9 11.4PCTFORTE 72.1 3.00 69.0 4.2 80.9 2.9VCON 67.9 1.5 53.9 2.4 54.4 3.0SUB 78.6 0.6 58.1 1.8 79.9 2.7VINF 90.2 0.9 80.7 0.7 87.2 2.9NPRO 86.8 0.3 92.0 0.6 81.8 1.2VPAR 89.7 0.3 50.0 0.2 81.0 0.9PRODE 100 0.0 35.0 0.2 68.6 0.8Table 3: Precision (P%) and usage proportion(U%) by coarse-grained part-of-speech of mostreliable contextual evidences using DL classifier.Figure 1: space distribution by part-of-speech ofmost reliable pieces of contextual evidence used fordisambiguation with DL classifier.According to the Table 2, the optimal contextsize comprises ?1 to ?5 words.
Furtherdevelopments of the context optimality will bemade in section 3.4.Surprisingly, Table 2 outlines the fact that thecriterion that obtains the best precision is based onbigrams and not on unigrams.
This subject is dealtwith in section 3.5.3.2 The most reliable parts-of-speechIn this section, we aim at learning the part-of-speech and the space distribution of all pieces ofcontextual evidence used for disambiguation.To this end, we use the DL classifier because itbases its classifications solely on the most reliablepiece of evidence identified by the criteria.
Thus,DL classifier enables us to learn which is the part-of-speech and the space distribution of thisindicator by using the criterion[1gr|mform|ordered|all].
Table 3 and graphicspresented in Figure 1 synthesize this study results.Table 3 enables to bring out the following results(we quote between brackets and in order the mainresults obtained for the nouns, the adjectives andthe verbs):?
common nouns (NCOM) obtain one of the bestprecisions (93.0%; 93.7%; 87.8%) and representone of the most used indicators (12.7%; 25.3%;26%) for the three term categories;?
adjectives (ADJ) represent good indicators fornouns (p=94.9%) and adjectives (p=80.3%), butthey are especially useful for nouns since theyare used in 13.7% of the cases against 2.2% onlyfor the adjectives;?
adverbs (ADV) are mainly useful for adjectives;their precision reaches 79.2% and they are usedin 9.6% of the cases;?
verbs in the infinitive (VINF) are very reliableindicators for the three parts-of-speech (90.2%;80.6%; 87.2%), but they are rarely used as theyare not very often encountered in the context(0.9%; 0.7%; 2.9%);?
conjugated verbs (VCON) obtain poor precision(67.9%; 53.9%; 54.4%).Figure 1 graphics show the space distribution ofthe main parts-of-speech of the indicators used todisambiguate each one of the three term categories.The dissymmetric shape of verbs, and moreprecisely, the strong prevalence of indicatorslocated in position +1, +2, +3, makes us believethat disambiguating verbs is done moreaccordingly to their object than to their subjectsince as a rule the main form encountered issubject?verb?object.Table 4 summarizes these graphs.
Our resultsand those of (Yarowsky, 1993) agree in manyrespects, although his study applies only to pseudo-words having only two ?senses?:?
?Adjectives derive almost all of theirdisambiguating information from the nouns theymodify?;?
?Nouns are best disambiguated by directlyadjacent adjectives or nouns?;?
?Verbs derive more disambiguating informationfrom their objects than from their subjects?.3.3 The importance of stop-wordsNouns Adjectives VerbsUnigrams 0.3% 2.5% 6.9%Bigrams 2.7% 3.4% 13.5%Trigrams 12.4% 15.9% 20.2%Table 5: precision decrease when omitting non-content words using DL classifier.Many studies do not consider all the words ofthe context (El-B?ze, Loupy, Marteau, 1998;Mooney, 1996; Ng, Lee, 2002).
The assumptionaccording to which content words represent themost reliable indicators underlies the choice to useonly content words based criteria.
This seems to beobvious, but it is not confirmed in Table 2.
Table 5shows the average decrease of the precision of thecontent words based criteria([par1|par2|par3|content]) compared to the samecriteria based on all words ([par1|par2|par3|all]).This table shows that the decrease is low when thecriteria are based on unigrams and are used todisambiguate nouns, but it can become very highin the other cases, and in particular for verbsdisambiguation.Table 3 informs us about the disambiguationprecision according to the coarse-grained part-of-speech tag.
This table shows that using contentwords only is probably not the most appropriatefeature selection (for example inflected verbs arenot relevant).
We have therefore chosen to try out amore subtle selection (we refer to it bypar4=selected) by restraining to the most reliableparts-of-speech according to Table 3:?
For nouns, we use indicators having thefollowing coarse-grained part-of-speech tagging:NCOM, PREP, ADJ, SUB, VINF, NPRO, VPARor PRODE;Most reliable contextual evidencesNCOM ADJ ADV DET PREPNouns -2, +2 +1  -1 -1, +1Adject.
-1, +1  -1 -1, -2 +1Verbs +2, +3   +1, +2 +1Table 4: space distribution of most reliablepieces of contextual evidence used fordisambiguation with DL classifier.Nouns Adj.
Verbs[1gr|mform|ordered|all] 81.5 75.7 71.0[1gr|mform|ordered|content] 78.9 71.6 59.5[1gr|mform|ordered|selected] 79.2 71.5 66.3Table 6: precision with and without featureselections using NB classifier.?
For adjectives, we use indicators having thefollowing coarse-grained part-of-speech tagging:NCOM, DET, ADJ, ADV, VINF or NPRO;?
For verbs, we use indicators having the followingcoarse-grained part-of-speech tagging: NCOM,ADJ, PROPE, PCTFORTE, SUB, VINF, NPRO,VPAR or PRODE.Table 6 gives a comparison of the precisionreached by the following 3 criteria:?
[1gr|mform|ordered|all],?
[1gr|mform|ordered|content],?
[1gr|mform|ordered|selected].We observe that this subtler selection lowers thedisambiguation precision too.
We assume then thatall words, whatever their part-of-speech, contributeto the disambiguation.3.4 Optimal context3.4.1 Size and symmetryWe tested up to ?10 000 word contexts.However, the best precision is always obtained forshort contexts ranging from ?1 to ?5 words.
Theseresults are similar to those obtained by many otherresearches (El-B?ze, et al, 1998; Yarowsky, 1993;2000).Optimal context size is criteria, target part-of-speech and n-gram size dependent.
In particular, itincreases with the n-gram size.Table 7 shows, for all the criteria we examined,the average size of the optimal context by the n-gram size and by the part-of-speech.The main indicators used to disambiguate nounsand adjectives surround roughly symmetrically theword we want to disambiguate.
On the contrary,indicators for verbs tend to be mainly situated afterthe verb.
Therefore, a non-symmetrical contextshifted forward by a word proves to be moreappropriate.
Our experiments show that the use ofthis shifted context improves the precision of theverbs disambiguation by 0.75% in average.Nouns Adjectives VerbsUnigrams 1.5 1.1 1.8Bigrams 2.4 2 2.8Trigrams 3.1 3.4 3.8Table 7: optimal context size using bothclassifiers.3.4.2 Do n-grams have to contain the targetword?The lemma being unique for a given word, ifonly lemmas are considered, an n-gram which isadjacent to the target word contains precisely thesame information as the same n-gram to which thetarget word is added in order to compose a (n+1)-gram.
The n-gram that is situated next to the wordto disambiguate can thus be assimilated to the(n+1)-gram which contains it.
Consequently, thequestion becomes: do n-grams have to contain theword to disambiguate or at least to be adjacent toit?
Several studies set themselves this constraintprobably because n-grams are used to capture fixedconstructions containing the word to disambiguate.Table 2 shows that the optimal context size forbest bigram or trigram-based criteria does not fitthis constraint.
The relevant n-grams do notnecessarily contain the target word and are notnecessarily adjacent to it.
For example, for nounsand verbs, the ?4 words context is the optimalcontext size of the bigram-based criteria whichobtains the best disambiguation precision.
Thiscriterion produces some bigrams separated fromthe target word by one or two words.
However,this single observation cannot enable us to abandonthe constraint in terms of containing or beingadjacent to the target word.
Indeed, the bigramincreasing distance may help locating aninformation which could be captured by the jointuse of one or several larger n-grams.
We have thusevaluated a combination of criteria in which all n-grams contain the target word in a context up to ?4words:?
[2gr|lemma|leftright|all] with context size of ?1words;?
[3gr|lemma|leftright|all] with context size of ?2words;?
[4gr|lemma|leftright|all] with context size of ?3words;?
[5gr|lemma|leftright|all] with context size of ?4words.This combination leads to a disambiguatingprecision of 74.3%, which is lower than the oneobtained using the criteria[2gr|lemma|leftright|all] alone with a ?4 wordscontext.
This experiment confirms thatconstraining the context to contain the word todisambiguate, or at least to be adjacent to it,decreases disambiguation accuracy.
Consequently,nothing justifies this constraint on criteria.3.5 Surprising bigrams accuracyContrary to all expectations, Table 2 shows thatthe best unigram-based criterion([1gr|lemma|ordered|all]) is definitely lessaccurate than the best bigram-based criterion([2gr|lemma|leftright|all]).
However, in practice,bigrams and trigrams are seldom used alone.
Whenused, they are taken in conjunction with unigramswhich are supposed to convey the most reliablepiece of information.Why does the criterion [2gr|lemma|leftright|all]work so well?
First, since this criterion is ajuxtaposition of lemmas, among the featuresgenerated by this criterion, the left and the rightunigrams are to be found, even if these unigramsare disguised as bigrams (cf.
section 3.4.2).
Asthese pieces of contextual evidence are certainlythe most important ones (cf.
section 3.4), it makessense that this bigram-based criterion obtains goodresults.Second, in a context of a higher size, thejuxtaposition of two words seems more relevantthan one isolated word.
For example, todisambiguate the word utile, the bigram pour_le isrelevant, whereas the single unigrams pour and leare not of much help.Lastly, sometimes, the presence of a unigramnoncontiguous to the target word can be sufficientto solve the ambiguity.
But using bigram-basedcriteria does not necessarily lose the piece ofinformation conveyed by unigram-based criteria.For example, a determiner, a preposition or anapostrophe often precedes a common noun.
Thelemmatisation variability of this determiner, thispreposition or this apostrophe is low for a givencommon noun located at a given distance from agiven target word.
Therefore, the piece ofinformation brought out by the juxtaposition of thenoun and the preceding word is often very similarto the piece of information provided by the nounalone.4 ConclusionWe have described here the results of asystematic and in-depth research on WSD criteria.This may be the first research of this extent carriedout within a unified framework.
This study enabledus to confirm certain results stated in the fieldliterature such as:?
importance of short contexts;?
importance of adjacent noun or adverb foradjective disambiguation;?
importance of adjacent adjective, or noun in avery short context for noun disambiguation;?
importance of the noun in the area after the verband use of dissymmetrical contexts for verbdisambiguation.We have also obtained more original results, notalways in line with some practices in the field suchas:?
importance of stop-words whose withdrawaldecreases the performance almost systematically;?
better results obtained by bigrams taken alonethan unigrams alone;?
unnecessary constraint of including or beadjacent to the target word.Disambiguation accuracy could probably beimproved by the study of other sources ofinformation useful in disambiguation, such as:?
criteria based on binary syntactic relations (noun-noun, noun-verb, adjective-noun, etc.)
to captureinformation which can be absent from shortcontexts;?
the use of thesauri or other sources ofinformation to carry out generalizations oncontext words to overcome data sparsenessproblem;?
topical text information;?
selectional restrictions.This preliminary study focuses on homogenouscriteria (for example: lemmas located from ?2 to+2 position).
To improve the disambiguationaccuracy, we have to look for heterogeneouscriteria by gathering the most relevant pieces ofcontextual evidence not necessarily of the sametype (for example: lemma in position ?2, part-of-speech in position ?1, morphological form oftarget word and lemma in position +2).
Thisfeature selection leads to a combinatorial explosionthat can be solved by genetic algorithms(Daelemans, Hoste, Meulder, Naudts, 2003).ReferencesAudibert L. (2001), LoX: Outil Polyvalent pourl'Exploration de Corpus Annot?s, 5?meRencontre des ?tudiants Chercheurs enInformatique  pour le Traitement Automatiquedes Langues (RECITAL-2001), 411-419.Audibert L. (2003), Etude des Crit?res deD?sambigu?sation S?mantique Automatique:R?sultats sur les Cooccurrences, 10?meconf?rence sur le Traitement Automatique desLangues Naturelles (TALN-2003), 35-44.Bruce R., Wiebe J., Perdersen T. (1996), TheMeasure of a Model, 1st Conference onEmpirical Methods in Natural LanguageProcessing (EMNLP-1996), 101-112.Cussens J.
(1993), Bayes and Pseudo-BayesEstimates of Conditional Probability and theirReliability, 6th European Conference onMachine Learning (ECML-1993), 136-152.Daelemans W., Hoste V., Meulder F. D., Naudts B.
(2003), Combined Optimization of FeatureSelection and Algorithm Parameter Interaction inMachine Learning of Language, 14th EuropeanConference on Machine Learning (ECML-2003),84-95.Domingos P., Pazzani M. (1997), BeyondIndependence: Conditions for the Optimality ofthe Simple Bayesian Classifier, MachineLearning, 29: 103-130.El-B?ze M., Loupy C. d., Marteau P.-F. (1998),WSD Based on Three Short Context Methods,SENSEVAL Workshop, in press.Golding A. R. (1995), A Bayesion Hybrid Methodfor Context-Sensitive Spelling Correction, 3thWorkshop on Very Large Corpora, 39-53.Kilgarriff A.
(1997), Evaluating Word SenseDisambiguation Programs: Progress Report,Speech and Language Technology (SALT-1997)Workshop on Evaluation in Speech andLanguage Technology, 114-120.Kilgarriff A., Rosenzweig J.
(2000), EnglishSenseval: Report and Results, 2nd InternationalConference on Language Resources andEvaluation (LREC-2000), 3: 1239-1244.Mooney R. J.
(1996), Comparative Experiments onDisambiguating Word Senses: an Illustration ofthe Role of Bias in Machine Learning, 1stConference on Empirical Methods in NaturalLanguage Processing (EMNLP-1996), 82-91.Ng H. T. (1997a), Exemplar-Based Word SenseDisambiguation: Some Recent Improvements,2nd Conference on Empirical Methods inNatural Language Processing (EMNLP-1997),208-213.Ng H. T. (1997b), Getting Serious About WordSense Disambiguation, Association forComputational Linguistics Special InterestGroup on the Lexicon (ACL-SIGLEX-1997):Workshop "Tagging Text with LexicalSemantics: Why, What, and How ?"
1-7.Ng H. T., Lee Y. K. (1996), Integrating MultipleKnowledge Sources to Disambiguate WordSense: An Exemplar-Based Approach, 34thAnnual Meeting of the Society for ComputationalLinguistics, 40-47.Ng H. T., Lee Y. K. (2002), An EmpiricalEvaluation of Knowledge Sources and LearningAlgorithms for Word Sense Disambiguation, 7thConference on Empirical Methods in NaturalLanguage Processing (EMNLP-2002), 41-48.Ng H. T., Zelle J.
(1997), Corpus-BasedApproaches to Semantic Interpretation in NaturalLanguage Processing, Artificial IntelligenceMagazine - Special Issue on Natural LanguageProcessing, 18: 45-64.Palmer M. (1998), Are WordNet SenseDistinctions Appropriate for ComputationalLexicons, Association for ComputationalLinguistics Special Interest Group on theLexicon (ACL-SIGLEX-1998): Senseval, inpress.Pedersen T. (2001), Machine Learning withLexical Features: the Duluth Approach toSenseval-2, 2nd International Workshop onEvaluating Word Sense Disambiguation Systems(Senseval-2), 139-142.Reymond D. (2002), M?thodologie pour laCr?ation d'un Dictionnaire Distributionnel dansune Perspective d'?tiquetage Lexical Semi-Automatique, 6?me Rencontre des ?tudiantsChercheurs en Informatique pour le TraitementAutomatique des Langues (RECITAL-2002),405-414.Segond F. (2000), Framework and Results forFrench, Computers and the Humanities, 34: 49-60.Valli A., V?ronis J.
(1999), ?tiquetageGrammatical de Corpus Oraux: Probl?mes etPerpectives, Revue Fran?aise de LinguistiqueAppliqu?e, 4: 113-133.V?ronis J.
(1998), A Study of PolysemyJudgements and Inter-Annotator Agreement,Programme and Advanced Papers of theSenseval-1 Workshop, 2-4.V?ronis J.
(2001), Sense Tagging: Does It MakeSense ?, Corpus Linguistics Conference,http://www.up.univ-mrs.fr/~veronis/pdf/2001-lancaster-sense.pdf.Yarowsky D. (1993), One Sense Per Collocation,ARPA Workshop on Human LanguageTechnology, 266-271.Yarowsky D. (1994), A Comparision of Corpus-Based Techniques for Restoring Accents inSpanish and French Text, 2nd Annual Workshopon Very Large Text Corpora, 19-32.Yarowsky D. (2000), Hierarchical Decision Listfor Word Sense Disambiguation, Computers andthe Humanities, 34: 179-186.AppendixNouns F S H MFSbarrage 92 5 1.2 76.1%chef 1133 11 1.5 76.0%communication 1703 13 2.4 40.6%compagnie 412 12 1.6 71.4%concentration 246 6 2 45.1%constitution 422 6 1.6 50.0%degr?
507 18 2.5 58.6%d?tention 112 2 0.9 72.3%?conomie 930 10 2.2 49.1%formation 1528 9 1.7 63.8%lancement 138 5 1 79.7%observation 572 3 0.7 86.0%observation 572 3 0.7 86.0%organe 366 6 2.2 38.3%passage 600 19 2.7 37.0%pied 960 62 3.5 37.6%restauration 104 5 1.8 43.3%solution 880 4 0.4 93.3%station 266 8 2.6 32.0%suspension 110 5 1.5 61.8%vol 278 10 2.2 40.3%Average 568 14.2 1.9 57.3%Adjectives F S H MFSbiologique 475 4 0.5 89.9%clair 557 20 3.1 29.3%correct 116 5 1.8 53.4%courant 170 4 0.6 90.0%exceptionnel 226 3 1.4 53.1%frais 184 18 3.1 36.4%haut 1017 29 3.5 25.0%historique 620 3 0.7 87.4%plein 844 35 4 17.1%populaire 457 5 2 47.9%r?gulier 181 11 2.5 32.6%sain 129 10 2.4 40.3%secondaire 195 5 1.7 53.8%sensible 425 11 2.6 29.9%simple 1051 14 2.1 41.3%strict 220 9 2.2 45.5%s?r 645 14 2.6 45.9%traditionnel 447 2 0.5 89.5%utile 359 9 2.4 42.9%vaste 368 6 2.1 42.4%Average 434 14.1 2.3 46.4%Verbs F S H MFSarr?ter 916 15 3 23.9%comprendre 2145 13 2.8 32.6%conclure 727 16 2.4 45.5%conduire 1093 15 2.3 38.2%conna?tre 1635 16 2.2 40.1%couvrir 543 22 3.3 33.3%entrer 1258 39 3.7 26.6%exercer 698 8 1.5 59.5%importer 576 8 2.6 27.6%mettre 5246 140 3.7 42.2%ouvrir 919 41 3.8 26.0%parvenir 654 8 2.3 36.7%passer 2556 84 4.5 15.8%porter 2347 59 4 29.4%poursuivre 978 16 2.7 36.2%pr?senter 2142 18 2.6 40.1%rendre 1990 27 2.9 46.4%r?pondre 2529 9 1 78.3%tirer 1002 47 3.9 28.9%venir 3797 33 3.2 24.9%Average 1688 47.4 3.1 37.2%Table 8: target word frequency (F), averagenumber of senses (S), sense repartition entropy (H)and base-line accuracy (Most FrequentSense: MFS).
