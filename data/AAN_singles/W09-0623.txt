Proceedings of the 12th European Workshop on Natural Language Generation, pages 130?137,Athens, Greece, 30 ?
31 March 2009. c?2009 Association for Computational LinguisticsInvestigating Content Selection for Language Generation using MachineLearningColin KellyComputer LaboratoryUniversity of Cambridge15 JJ Thomson AvenueCambridge, UKAnn CopestakeComputer LaboratoryUniversity of Cambridge15 JJ Thomson AvenueCambridge, UK{colin.kelly,ann.copestake,nikiforos.karamanis}@cl.cam.ac.ukNikiforos KaramanisDepartment of Computer ScienceTrinity College DublinDublin 2IrelandAbstractThe content selection component of a nat-ural language generation system decideswhich information should be communi-cated in its output.
We use informa-tion from reports on the game of cricket.We first describe a simple factoid-to-textalignment algorithm then treat content se-lection as a collective classification prob-lem and demonstrate that simple ?group-ing?
of statistics at various levels of granu-larity yields substantially improved resultsover a probabilistic baseline.
We addi-tionally show that holding back of specifictypes of input data, and linking databasestructures with commonality further in-crease performance.1 IntroductionContent selection is the task executed by a natu-ral language generation (NLG) system of decid-ing, given a knowledge-base, which subset of theinformation available should be conveyed in thegenerated document (Reiter and Dale, 2000).Consider the task of generating a cricket matchreport, given the scorecard for that match.
Sucha scorecard would typically contain a large num-ber of statistics pertaining to the game as a wholeas well as individual players (e.g.
see Figure 1).Our aim is to identify which statistics should beselected by the NLG system.Much work has been done in the field of con-tent selection, in a diverse range of domains e.g.weather forecasts (Coch, 1998).
Approaches areusually domain specific and predominantly basedon structured tables of well-defined input data.Duboue and McKeown (2003) attempted a sta-tistical approach to content selection using a sub-stantial corpus of biographical summaries pairedwith selected content, where they extracted rulesand patterns linking the two.
They then used ma-chine learning to ascertain what was relevant.Barzilay and Lapata (2005) extended this ap-proach but applying it to a sports domain (Amer-ican football), similarly viewing content selectionas a classification task and additionally taking ac-count of contextual dependencies between data,and found that this improved results compared toa content-agnostic baseline.
We aim throughoutto extend and improve upon Barzilay and Lapata?smethods.We emphasise that content selection throughstatistical machine learning is a relatively new area?
approaches prior to Duboue and McKeown?s are,in principle, much less portable ?
and as such thereis not an enormous body of work to build upon.This work offers a novel algorithm for data-to-text alignment, presents a new ?grouping?
methodfor sharing knowledge across similar but distinctlearning instances and shows that holding backcertain data from the machine learner, and rein-troducing it later on can improve results.2 Data Acquisition & AlignmentWe first must obtain appropriately aligned cricketdata, for the purposes of machine learning.Our data comes from the online Wisden al-manack (Cricinfo, 2007), which we used to down-load 133 match report/scorecard pairs.
We em-ployed an HTML parser to extract the main textfrom the match report webpage, and the matchdata-tables from the scorecard webpage.
An ex-ample scorecard can be found in Figure 11.1Cricket is a bat-and-ball sport, contested by two oppos-ing teams of eleven players.
Each side?s objective is to scoremore ?runs?
than their opponents.
An ?innings?
refers to thecollective performance of the batting team, and (usually) endswhen all eleven players have batted.In Figure 1, in the batting section R stands for ?runs made?,M for ?minutes played on the field?, B for ?number of ballsfaced?.
4s and 6s are set numbers of runs awarded for hit-ting balls that reach the boundary.
SR is the number of runsper 100 balls.
In the bowling section, O stands for ?overs130Result India won by 63 runsIndia innings (50 overs maximum) R M B 4s 6s SRSC Ganguly?
run out (Silva/Sangakarra?)
9 37 19 2 0 47.36V Sehwag run out (Fernando) 39 61 40 6 0 97.50D Mongia b Samaraweera 48 91 63 6 0 76.19SR Tendulkar c Chandana b Vaas 113 141 102 12 1 110.78. .
.Extras (lb 6, w 12, nb 7) 25Total (all out; 50 overs; 223 mins) 304Fall of wickets 1-32 (Ganguly, 6.5 ov), 2-73 (Sehwag, 11.2 ov), 3-172 (Mongia,27.4 ov), 4-199 (Dravid, 32.1 ov), .
.
.
, 10-304 (Nehra, 49.6 ov)Bowling O M R W EconWPUJC Vaas 10 1 64 1 6.40 (2w)DNT Zoysa 10 0 66 1 6.60 (6nb, 2w).
.
.TT Samaraweera 8 0 39 2 4.87 (2w)Figure 1: Statistics in a typical cricket scorecard.2.1 Report AlignmentWe use a supervised method to train our data, andthus need to find all ?links?
between the scorecardand match report.
We execute this alignment byfirst creating tags with tag attributes according tothe common structure of the scorecards, and tagvalues according to the data within a particularscorecard.
We then attempt to automatically alignthe values of those tags with factoids, single piecesof information found in the report.For example, from Figure 1 the fact that Ten-dulkar was the fourth player to bat on the first teamis captured by constructing a tag with tag attributeteam1 player4, and tag value ?SR Tendulkar?.
Thefact he achieved 113 runs is encapsulated by an-other tag, with tag attribute as team1 player4 Rand tag value as ?113?.
Then if the report con-tained the phrase ?Tendulkar made 113 off 102balls?
we would hope to match the ?Tendulkar?factoid with our tag value ?SR Tendulkar?, the?113?
factoid with our tag value ?113?
and replaceboth factoids with their respective tag attributes, inthis case team1 player4 and team1 player4 R re-spectively.
Similar methods for this problem havebeen employed by Barzilay and Lapata (2005) andDuboue and McKeown (2003).The basic idea behind our 6-step process foralignment is that we align those factoids we arebowled?, M for ?maiden overs?, R for ?runs conceded?
and Wfor ?wickets taken?.
Econ is ?economy rate?, or number ofruns per over.It is important to note that Figure 1 omits the opposingteam?s innings (comprising new instances of the ?Batting?,?Fall of Wickets?
and ?Bowling?
sections), and some addi-tional statistics found at the bottom of the scorecard.most certain of first.
The main obstacle we facewhen aligning is the large incidence of repeatednumbers occurring within the scorecard, as thiswould imply we have multiple, different tags allwith the same tag values.
It is wholly possible(and quite typical) that single figures will be re-peated many times within a single scorecard2.Therefore it would be advantageous for us tohave some means to differentiate amongst tags,and hopefully select the correct tag when encoun-tering a factoid which corresponds to repeated tagvalues.
Our algorithm is as follows:Preprocessing We began by converting all ver-balised numbers to their cardinal equivalents, e.g.
?one?, ?two?
to ?1?, ?2?, and selected instances of?a?
into ?1?.Proper Nouns In the first round of tagging weattempt to match proper names from the scorecardwith strings within the report.
Additionally, wemaintain a list of all players referenced thus far.Player-Relevant Details Using the list of play-ers we have accumulated, we search the report formatches on tag values relating to only those play-ers.
This step was based on the assumption that afactoid about a specific player is unlikely to appearunless that player has been named.Non-Player-Relevant Details The next stageinvolves attempting to match factoids to tag valueswhose attributes don?t refer to a particular playere.g., more general match information as well asteam statistics.2For example in Figure 1 we can see the number 6 appear-ing four times: twice as the number of 4s for two differentplayers, once as an lb statistic and once as an nb statistic.131Anchor-Based Matching We next use sur-rounding text anchor-based matching: for exam-ple, if a sentence contains the string ?he bowledfor 3 overs?
we will preferentially attempt to matchthe factoid ?3?
with tag values from tags which weknow refer to overs.Remaining Matches The final step acts as our?catch-all?
?
we proceed through all remainingwords in the report and try to match each poten-tial factoid with the first (if any) tag found whosetag value is the same.2.2 EvaluationThe output of our program is the original text withall aligned figures and strings (factoids) replacedwith their corresponding tag attributes.
We can seean extract from an aligned report in Figure 2 wherewe show the aligned factoids in bold, and their cor-responding tag attributes in italics.
We also note atthis point that much of commentary shown doesnot in fact appear in the scorecard, and thereforeadditional knowledge sources would typically berequired to generate a full match report ?
this isbeyond the scope of our paper, but Robin (1995)attempts to deal with this problem in the domainof basketball using revision-based techniques forincluding additional content.We asked a domain expert to evaluate five ofour aligned match reports ?
he did this by creat-ing his own ?gold standard?
for each report, a listof aligned tags.
Compared to our automaticallyaligned tags, we obtained 79.0% average preci-sion, 75.8% average recall and a mean F of 77.0%.3 CategorizationWe are using the methods of Barzilay and Lapata(henceforth B&L) as our starting point, so we de-scribe what we did to emulate and extend them.3.1 Barzilay and Lapata?s MethodB&L?s corpus was composed of a relationaldatabase of football statistics.
Within the databasewere multiple tables, which we will refer to as?categories?
(actions within a game, e.g.
touch-downs and fumbles).
Each category was com-posed of ?groups?
(the rows within a category ta-ble), with each row referring to a distinct player,and each column referring to different types of ac-tion within that category (?attributes?
).B&L?s technique for the purposes of the ma-chine learning was to assign a ?1?
or ?0?
to eachNatWest Series (series), match 9 (team1 player1 R)India v Sri Lanka (matchtitle)At Bristol (venue town), July 11 (date) (day/night(daynight)).India (team1) won by 63 runs (winmethod).India (team1) 5 (team1 points) pts.Toss: India (team1).The highlight of a meaningless match was a sublime in-nings from Tendulkar (team1 player4), who resumedhis fleeting love affair with Nevil Road to the delightof a flag-waving crowd.
On India (team1)?s only othervisit to Bristol (venue town), for a World Cup gamein 1999 against Kenya, Tendulkar (team1 player4)had creamed an unbeaten 140, and this time he drovewith elan to make 113 (team1 player4 R) off just 102(team1 player4 B) balls with 12 (team1 player4 4s)fours and a (team1 player4 6s) six.. .
.Figure 2: Aligned match report extractrow, where a row would receive the value ?1?
ifone or more of the entries in the row was ver-balised in the report.
In the context of our datawe could apply a similar division, for example, byconstructing a category entitled ?Batting?
with at-tributes (columns) ?Runs?, ?Balls?, ?Minutes?, ?4s?and ?6s?
etc., and rows corresponding to players.In this case a group within that category wouldcorrespond to one line of the ?Innings?
table in Fig-ure 1.We note that B&L were selecting content on arow basis, while we are aiming to select individualtag attributes (i.e., specific row/column cell refer-ences) within the categories, a more difficult task.We discuss this further in Section 6.The technique above allows the machine learn-ing algorithm to be aware that different statisticsare semantically related ?
i.e., each group within acategory contains the same ?type?
of information.We therefore think this is a logical starting pointfor our work, and we aim to expand upon it.3.2 Classifying TagsThe key step was deciding upon an appropriatedivision of our scorecard into various categoriesand the groups for each category in the style ofB&L.
As can be seen from Figure 1 our input in-formation is a mixture of structured (e.g.
Bowling,Batting sections), semi-structured (Fall of Wicketssection) and almost unstructured (Result) informa-tion.
This is somewhat unlike B&L?s data, whichwas fully structured in database form.
We deal132Category Attributes VerbBatting 9 47.0Bowling 11 10.2Fall of Wickets 8 46.4Match Details 11 75.2Match Result 8 45.1Officials 8 6.0Partnerships 11 75.5Team Statistics 13 46.2Table 1: Number of attributes per category withpercent verbalised (Verb)with this by enforcing a stronger structure ?
di-viding the information into eight of our own ?cat-egories?, based roughly on the formatting of thewebpages.
These are outlined in Table 1.The first three categories in the table are quiteintuitive and implicit from the respective sectionsof the scorecard.
There is additional informationin a typical scorecard (not shown in Figure 1),which we must also categorise.
The ?Team Statis-tics?
category contains details about the ?extras?3scored by each team, as well as the number ofpoints gained by the team towards that particularseries4.
We divide the remaining tag attributes asfollows into three categories: ?Officials?
?
personsparticipating in the match, other than the teams(e.g.
umpires, referees); ?Match Details?
?
infor-mation that would have been known before thematch was played (e.g.
venue, date, season); and?Match Result?
?
data that could only be knownonce the match was over (e.g.
final result, playerof the match).Finally we have an additional ?Partnerships?5category which is given explicitly on a separatewebpage referenced from each scorecard, but isalso implicit from information contained in the?Fall of Wickets?
and ?Batting?
sections.
We an-ticipate that this category will help us manage theissue of data sparsity.
For instance, in our domainwe could group partnerships (which could con-tain a multitude of player combinations and there-3Additional runs awarded to the batting team for specificactions executed by the bowling team.
There are four types:No Ball, Wide, Bye, Leg Bye.4Each cricket game is part of a specific ?series?
of games.e.g.
India would receive five points for their win within theNatWest series.5A ?partnership?
refers to a pair of players who bat to-gether, and usually comprises information such as the num-ber of runs scored between them, the number of deliveriesfaced and so on.fore distinct tags) with the various possible binarycombinations of players together for shared learn-ing.
We discuss this further in Section 8.3.Within 5 of the categories described above, weare further able to divide the data into ?groups?
-the Batting, Bowling, Fall of Wickets and Partner-ships categories refer to multiple players and thushave multiple rows.
The Team Statistics categorycontains two groups, one for each team.
The othercategories merely form one-line tables.4 Machine LearningOur task is to establish which tag attributes (andhence tag values) should be included in the finalmatch report, and is a multi-label classificationproblem.
We chose to use BoosTexter (Schapireand Singer, 2000) as it has been shown to be aneffective classifier (Yang, 1999), and it is one ofthe few text classification tools which directly sup-ports multi-label classification.
This is also whatB&L used.Schapire and Singer?s BoosTexter (2000) uses?decision stumps?, or single level decision treesto classify its input data.
The predicates of thesestumps are defined, for text, by the presence orabsence of a single term, and, for numerical at-tributes, whether the attribute exceeds a giventhreshold, decided dynamically.4.1 Running BoosTexterBoosTexter requires two input files to train a hy-pothesis, ?Names?
and ?Data?.Names The Names file contains, for each pos-sible tag attribute, t, across all scorecards, the typeof its corresponding tag value.
These are contin-uous for numbers and text for normal text.
Fromour 133 scorecards we extracted a total of 61,063tag values, of which 82.2% were continuous, theremainder being text.Data The Data file contains, for each scorecard,a comma-delimited list of all tag values for a par-ticular scorecard, with a ???
for unknown values,followed by a list of the verbalised tag attributes.Testing We can now run BoosTexter with auser-defined number of rounds, T , which createsa hypothesis file.
Using this hypothesis file anda test ?data?
file (without the list of verbalised tagattributes), BoosTexter will give its hypothesizedpredictions, a value f for each tag attribute t. Thesign of f determines whether the classifier be-lieves the tag value corresponding to t is relevant133to the test scorecard, while |f | is a measure of theconfidence the classifier has in its assertion.4.2 Data SparsityThe very nature of the data means that there area large number of tag values which do not occurin every scorecard ?
the average scorecard con-tained 24 values, yet our ?names?
file contained1193 possible tag attributes.
A lot of this was dueto partnership tag attributes which formed 43.6%of the ?names?
entries.
This large figure is becausea large number of all possible binary combinationsof players existed in the training data across bothteams6.
This implies we will be unable to train fora significant number of tag attributes as many spe-cific tag values occur very rarely.
Indeed we foundthat of 158,669 entries, 97,666 (61.55%) were ?un-known?.5 Evaluation BaselinesIt is not clear what constitutes a suitable baselineso we considered multiple options.
The issue ofambiguous reference baselines is not specific tothe cricket domain, as there is no standardizedbaseline approach across the prior literature.
Weemploy ten-fold cross validation throughout.5.1 Majority BaselineB&L created a ?majority baseline?
whereby theyreturned those categories (i.e., tables) which wereverbalised more than half of the time in theiraligned reports.As explained in Section 3.2 we divided our tagattributes into 8 categories.
We emulated B&L?sbaseline method as follows: For each category, ifany of the tag values within a particular ?group?was tagged as verbalised, we counted that as a?vote?
for that particular category.
We then cal-culated the total number of ?votes?
divided by thetotal number of ?groups?
within each category.
Allcategories which had a ratio of 50% or greaterin this calculation were considered to be ?major-ity categories?.
Our baseline Bmaj then consistedof all tag attributes forming part of those majoritycategories.
As shown in Table 1 there were onlytwo categories which exceeded the 50% threshold,?Match Details?
and ?Partnerships?.We can see that this baseline performsabysmally.
The reason for this poor behaviour is693 of the possible 2?10i=1 i = 110 combinations oc-curred.Bmaj ?
min max ?Precision 0.0966 0.0333 0.1583 0.0250Recall 0.4879 0.2727 0.7895 0.0977F 0.1603 0.0620 0.2568 0.0384Table 2: Majority Baseline, Bmajthat since so many tag attributes contribute to thecategories we are including far too many possibil-ities in our baseline.5.2 Probabilistic BaselineThis baseline is based on the premise that thosetag attributes which occur with highest frequencyacross the training data refer to those tag valueswhich will often occur in a typical match report.To create our baseline set of tag attributes Bprobwe extract the a most frequently verbalised tag at-tributes across all the training data where a is theaverage length of the verbalised tag attribute listsfor each report/scorecard pair.Bprob ?
min max ?Precision 0.5157 0.2174 0.7391 0.1010Recall 0.5157 0.1389 0.7647 0.0990F 0.5100 0.1695 0.6939 0.0852Table 3: Probabilistic Baseline, BprobThis baseline achieves a mean F score of 51%,however the tag attributes being returned are veryinconsistent with a typical match report ?
theycorrespond in the majority to player names butnot one refers to any other tag attributes relevantto those players.
This renders the output mostlymeaningless in terms of our aim to select contentfor an NLG system.5.3 No-Player Probabilistic BaselineTaking the above into account we create a base-line which derives its choice of tag attributes frommatch statistics only.
This baseline is similar tothe Probabilistic Baseline above, with the excep-tion that when summing the numbers of tag at-tributes in the sets we do not consider player-nametag attributes in our counts.
Instead, we extractthe a?
most frequent tag attributes, where a?
isthe average size of the sets excluding player-nametag attributes.
To finally obtain our baseline setBnops we merge our a?
most frequent tag attributes134with any and all corresponding player-name tag at-tributes7.Bnops ?
min max ?Precision 0.4923 0.1765 0.6875 0.0922Recall 0.3529 0.1111 0.5625 0.0842F 0.4064 0.1538 0.5946 0.0767Table 4: No-Player Probabilistic Baseline, BnopsAs can be seen from Table 4, this method suffersan absolute F-score drop of more than 10% fromthe previous method.
However if we analyse theoutput more closely we can see that although theaccuracy has dropped, the returned tag attributesare more thematically consistent with the trainingdata.
This is our preferred baseline.6 Evaluation ParadigmThe main difficulty we encountered arose whenwe came to assessing the Precision and Recall fig-ures as we have yet to decide on what level we areconsidering the output of our system to be correct.We see three possibilities for the level:Category We could simply count the ?votes?predicted on a per category basis (as describedin sections 3.1 and 5.1), and evaluate categoriesbased on the number of votes given for each.
Wewould expect this to generate very good results aswe are effectively overgrouping, once on a groupbasis (grouping together all attributes) and once ona category basis (unifying all groups within a cate-gory), but the output would be so general and triv-ial (effectively stating something to the effect that?a match report should contain information aboutbatting, bowling and team statistics?)
that it wouldbe of no use in an NLG system.Groups Here we compare which ?groups?
wereverbalised within each category, and which werepredicted to be verbalised (as we did for the Major-ity Baseline of Section 5.1).
Our implicit groupingmeans that we do not have to necessarily return thecorrect statistic pertaining to a group since eachgroup acts as a basket for the statistics containedwithin it, and is susceptible to ?false positives?.This method is most similar to B&L?s.Tags Since we are trying to establish which tagattributes should be included rather than whichgroups are likely to contain verbalised tag at-tributes, we could say that even the above method7e.g., if team1 player4 R is in a?
then we would also in-clude team1 player4 in our final set.is too liberal in its definition of correctness.
Thuswe also evaluate our groups on the basis of theircomponent parts, i.e., if a particular group of tagattributes is estimated to be verbalised by Boos-Texter, then we include all attributes from thatgroup.7 Initial ResultsOur ?categorized?
results are derived from present-ing BoosTexter with each individual category asdescribed in Section 3.2, then merging the selectedtag attributes together and evaluating based on thecriteria described above.
We then show BoosTex-ter?s performance ?as is?, by running the programon the full output of our alignment stage with nocategorization/grouping.7.1 Categorized ?
Groups LevelOur ?Categorized Groups?
results can be found inFigure 3 and Table 5.
For each of our tests we varythe value of T (the number of rounds) to see howit affects our accuracy.Here we see we have a maximum F score of0.7039 for T = 25.
This is a very high result,performing far better than all our baselines, how-ever we feel the ?basketing?
mentioned in Section6 means that the results are not particularly in-structive ?
instead of specific ?interesting?
tag at-tributes, we return a grouped list of tag attributes,only some of which are likely to be ?interesting?.Thus we decide to no longer pursue ?grouping?as a valid evaluation method, and evaluate all ourmethods at the ?tag attribute?
level.Best ?
?Precision 0.7620 0.7473 0.0320CG Recall 0.6795 0.6680 0.0322F 0.7039 0.6897 0.0106Table 5: Categorized Groups with Best value forT = 25.7.2 Categorized ?
Tags LevelWhat is notable here is that, for all values of Twhich we ran our tests on (ranging from 1 to3000), we obtained just one set of results for ?Cat-egorized Tags?, displayed in Table 6.This behaviour indicates that the boosting is nothelping to improve the results.
Rather, it is repeat-edly producing the same hypotheses, with vary-ing confidence levels.
The low F score is due to1350.40.50.60.70.81  10  100  1000TUnassisted BoostingCategorized GroupsNo PlayersEnhanced CategorizationFigure 3: All F scores Results?
min max ?Precision 0.0880 0.0496 0.1933 0.0223Recall 0.7872 0.5417 1.0000 0.1096F 0.1575 0.0924 0.3151 0.0361Table 6: Categorized Tags Resultsthe very low Precision value.
This method is ef-fectively a direct application of B&L?s method toour domain, however because of our strict accu-racy measurement, it does not perform particularlywell.
In fact it is even worse thanBmaj, our worst-performing baseline.
We believe this is becausethe Majority Baseline is limited in the breadth oftags returned, whereas this method returns verylarge sets of over 200 tag attributes (due to themany contributing tag attributes of each category)while the average size of the training sets is 24.Ideally we want to strike a balance betweenthe improved granularity of the Categorized Tagsevaluation (without the low accuracy) with theexcellent performance of the Categorized Groupsevaluation (without the too-broad basketing).7.3 Unassisted BoostingOur results are in Table 7 (row UB) and Figure 3.We can see F values are increasing on the whole,and that we have nearly reached our Probabilis-tic Baseline.
Inspecting the contents of the setsreturned by BoosTexter, we see they are slightlymore in line with a typical training set, but still suf-fer from an over-emphasis on player names.
Wealso believe the high number of rounds requiredfor our best result (T = 2250) is caused by thesparsity issue described in Section 4.2.Best ?
?Precision 0.4965 0.4730 0.0253UB Recall 0.4961 0.4723 0.0252F 0.4907 0.4673 0.0249Precision 0.4128 0.3976 0.0094NP Recall 0.4759 0.4633 0.0126F 0.4367 0.4227 0.0091Precision 0.4440 0.4318 0.0136EC Recall 0.5127 0.4753 0.0271F 0.4703 0.4467 0.0194Table 7: Unassisted Boosting (UB), No Players(NP) and Enhanced Categorization (EC).
Best val-ues for T = 2250, 250 and 20 respectively.8 No-Players & EnhancedCategorizationWe now consider alternative, novel methods forimproving our results.8.1 Player ExclusionWe have thus far ignored coherency in our data?
for example we want to make sure that playerstatistics will be accompanied by their correspond-ing player name.One problem so far with our approach has beenthat we are effectively double-counting the play-ers.
Our methods inspect which player namesshould appear at the same time as finding ap-propriate match statistics, whereas we believe weshould instead be finding relevant statistics in thefirst instance, holding back player names, then in-cluding only those players to whom the statisticsrefer.
Thus we restate our task in this way.This is also sensible as in previous incarnationsthe learning algorithm had been learning from theliteral strings of the player names.
Although aplayer could be more likely to be named for vari-ous reasons, these reasons would not appear in thescorecard and we feel the strings are best ignored.Thus we decide to remove all player namesfrom the machine learning input, reinstating onlyrelevant ones once BoosTexter has selected itschosen tag attributes.8.2 Player Exclusion ResultsAs can be seen from Table 7 (row NP) and Figure3, we have a maximum F value of 0.4367 whenT = 250, and have achieved a 3% absolute in-crease, over ourBnops baseline, a static implemen-tation of the above ideas.1368.3 Enhanced CategorizationOur final method combines the ideas of Section8.1 above with the benefits of categorization, andhandles data sparsity issues.The method is identical to that of Section 3.1,with two important exceptions: The first is thatwe reintroduce player names after the learning, asabove.
The second is that instead of just a bi-nary include/don?t-include decision for each tagattribute, we offer a list of verbalised tag attributesto the learner, but anonymising them with respectto the group in which they appear.
This enablesthe learner to, given any group, predict which tagattributes should be returned, independent of thegroup in question.
This means groups with often-empty tag values are able to leverage the informa-tion from groups with usually populated tag val-ues, hence solving our data-sparsity issues.
Forexample, this will solve the issue, referenced inSection 4.2 of a lack of training data for particularplayer-combination partnerships.Having held back the group to which the tag at-tributes belong, we reintroduce them enabling dis-covery of the original tag attribute.
This offers thebenefits of categorization, but with a finer-grainedapproach to the returned sets of tag attributes.8.4 Enhanced Categorization ResultsOur results are in Table 7 (row EC) and Figure3.
We achieved our best F score result of 0.4703for a relatively low value of T = 20, and we canclearly see that boosting establishes a reasonableruleset after a small number of iterations ?
we be-lieve we have resolved the issue of data sparsity.The fact that this grouping has improved our re-sults compared to feeding the information in ?flat?
(as in Section 7.3) emphasises that the construc-tion and make-up of the categories play a key rolein defining performance.9 Conclusions & Future WorkThis paper has presented an exploration of variousmethods which could prove useful when select-ing content given a partially structured databaseof statistics and output text to emulate.
We be-gan by acquiring the necessary domain data, in theform of scorecards and reports, and employed asix-step process to align scorecard statistics ver-balised in the reports.
We next categorised ourstatistics based on the scorecard format.
We es-tablished three baselines ?
one ?unthinking?
proba-bilistic baseline, a ?sensible?
probabilistic one, andanother using categorization.We found that unassisted boosting actually per-formed worse than our comparable probabilisticbaseline, Bprob, but its output was marginallymore in line with the typical training data.
Weexplored how categorization affected our results,and showed that by grouping similar sets of tagattributes together we achieved a 7.4% improve-ment over the comparable baseline value, Bnops(Table 4).
We further improved this technique ina novel way by sharing structural information be-tween learning instances, and by holding back cer-tain information from the learner.
Our final best F-value marked a relative 15.7% increase on Bnops.There are multiple avenues still available for ex-ploration.
One possibility would be to further in-vestigate the effects of categorization from Section3.2, for example by varying the size and number ofcategories.
We would also like to apply our meth-ods to another domain (e.g.
rugby games) to es-tablish the relative generality of our approach.AcknowledgmentsThis paper is based on Colin Kelly?s M.Phil.
thesis, writtentowards his completion of the University of Cambridge Com-puter Laboratory?s Computer Speech, Text and Internet Tech-nology course.
Grateful thanks go to the EPSRC for funding.ReferencesRegina Barzilay and Mirella Lapata.
2005.
Collective Con-tent Selection for Concept-To-Text Generation.
In HLT?05, pages 331?338.
Association for Computational Lin-guistics.Jose Coch.
1998.
Multimeteo: multilingual production ofweather forecasts.
ELRA Newsletter, 3(2).Cricinfo.
2007.
Wisden Almanack.http://cricinfo.com/wisdenalmanack.
Retrieved 28April 2007.
Registration required.Pablo A. Duboue and Kathleen R. McKeown.
2003.
Statis-tical Acquisition of Content Selection Rules for NaturalLanguage Generation.
EMNLP ?03, pages 121?128.Ehud Reiter and Robert Dale.
2000.
Building Natural Lan-guage Generation Systems.
Cambridge University Press.Jacques Robin.
1995.
Revision-based generation of natu-ral language summaries providing historical background:corpus-based analysis, design, implementation and evalu-ation.
Ph.D. thesis, Columbia University.Robert E. Schapire and Yoram Singer.
2000.
BoosTexter:A boosting-based system for text categorization.
MachineLearning, 39(2/3):135?168.Yiming Yang.
1999.
An evaluation of statistical approachesto text categorization.
Information Retrieval, 1(1/2):69?90.137
