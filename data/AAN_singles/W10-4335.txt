Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 205?208,The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational LinguisticsCoherent Back-Channel Feedback Tagging ofIn-Car Spoken Dialogue CorpusYuki KamiyaGraduate School ofInformation Science,Nagoya University, Japankamiya@el.itc.nagoya-u.ac.jpTomohiro OhnoGraduate School ofInternational Development,Nagoya University, Japanohno@nagoya-u.jpShigeki MatsubaraGraduate School ofInformation Science,Nagoya University, Japanmatubara@nagoya-u.jpAbstractThis paper describes the design of a back-channel feedback corpus and its evalua-tion, aiming at realizing in-car spoken di-alogue systems with high responsiveness.We constructed our corpus by annotatingthe existing in-car spoken dialogue datawith back-channel feedback timing infor-mation in an off-line environment.
Ourcorpus can be practically used in devel-oping dialogue systems which can pro-vide verbal back-channel feedbacks.
Asthe results of our evaluation, we confirmedthat our proposed design enabled the con-struction of back-channel feedback cor-pora with high coherency and naturalness.1 IntroductionIn-car spoken dialogue processing is one of themost prevailing applications of speech technol-ogy.
Until now, to realize the system which cansurely achieve such tasks navigation and informa-tion retrieval, the development of speech recogni-tion, speech understanding, dialogue control andso on has been promoted.
Now, it becomes impor-tant to increase responsiveness of the system notonly for the efficient achievement of the task butfor increasing drivers?
comfortableness in a dia-logue.One way to increase responsiveness of a sys-tem is to timely disclose system?s state of under-standing, by making the system show some kindof reaction during user?s utterances.
In humandialogues, such disclosure is performed by ac-tions such as nods, facial expressions, gestures andback-channel feedbacks.
However, since driversdo not look towards a spoken dialogue systemwhile driving, the system has to inevitably usevoice responses, that is, back-channel feedbacks.Furthermore, in the response strategy for realiz-ing in-car dialogues in which drivers feel com-fortable, it is necessary for the system to provideback-channel feedbacks during driver?s utterancesaggressively as well as timely.This paper describes the design of a back-channel feedback corpus having coherency (tag-ging is performed by different annotators equally)and naturalness, and its evaluation, aiming at re-alizing in-car spoken dialogue systems with highresponsiveness.
Although there have been sev-eral researches on back-channel feedback timings(Cathcart et al, 2003; Maynard, 1989; Takeuchiet al, 2004; Ward and Tsukahara, 2000), in manyof them, back-channel feedback timings in humandialogues were observed and analyzed by usinga general spoken dialogue corpus.
On the otherhand, we constructed our corpus by annotating theexisting in-car spoken dialogue data with back-channel feedback timing information in an off-lineenvironment.
Our corpus can be practically usedin developing dialogue systems which can provideback-channel feedbacks.In our research, the driver utterances (11,181turns) in the CIAIR in-car spoken dialogue corpus(Kawaguchi et al, 2005) were used as the existingdata.
We created the Web interface for the anno-tation of back-channel feedbacks and constructedthe corpus including 5,416 back-channel feed-backs.
Experiments have shown that our proposedcorpus design enabled the construction of back-channel feedback corpora with high coherency andnaturalness.2 Corpus DesignA back-channel feedback is a sign to inform aspeaker that the listener received the speaker?s ut-terances.
Thus, in an in-car dialogue between adriver and a system, it is preferable that the sys-tem provides as many back-channel feedbacks aspossible.
However, if back-channel feedbacks areunnecessarily provided, they can not play the pri-mary role because the driver wonders if the systemreally comprehends the speech.205For this reason, the timings at which the sys-tem provides back-channel feedbacks become im-portant.
Several researches investigated back-channel feedback timings in human-human dia-logues (Cathcart et al, 2003; Maynard, 1989;Takeuchi et al, 2004; Ward and Tsukahara, 2000).They reported back-channel feedbacks had the fol-lowing tendencies: ?within or after a pause,?
?aftera conjunction or sentence-final particle,?
and ?af-ter a clause wherein the final pitch descends.
?However, it is difficult to systematize the ap-propriate timings of back-channel feedbacks sincetheir detection is intertwined in a complex waywith various acoustic and linguistic factors.
Al-though machine learning using large-scale datawould be a solution to the problem, existing spo-ken dialogue corpora are not suitable for directuse as data, because the timings of the back-channel feedbacks lack coherency due to the in-fluence of factors such as the psychological stateof a speaker, the environment and so on.In our research, to create more pragmatic datain which the above-mentioned problem is solved,we constructed the back-channel feedback corpuswith coherency.
To this end, we established thefollowing policies for annotation:?
Comprehensive tagging: Back-channelfeedback tags are provided for all timingswhich are not unnatural.
In human-humandialogues, there are some cases that even if atiming is suited for providing a back-channelfeedback, no back-channel feedback is notprovided (Ward and Tsukahara, 2000).
Onthe other hand, in our corpus, comprehensivetagging enables coherent tagging.?
Off-line tagging: Annotators tag all tim-ings at which back-channel feedbacks can beprovided after listening to the target speechone or more times.
Compared with providingback-channel feedbacks in on-line environ-ment, the off-line annotation decreases thechances of tagging wrong positions or failingin tagging back-channel feedbacks, realizingcoherent tagging.?
Discretization of tagging points: Taggingis performed for each segment into whichdriver?s utterances are divided.
In a nor-mal dialogue, the listener can provide back-channel feedbacks whenever he/she wants to,but the inconsistency in the timings to givesuch feedbacks becomes larger in exchange0035 - 03:10:170-03:13:119 F:D:I:D:(F ?)
(well?)
&   to??
(clothes)                  &   fuku-o????????
(I wan to buy, so) &   kai-tai-n-da-kedo???
(somewhere)           &   dok-ka??
?<H>                  (near here)              &   chikaku-ni<H>0036 - 03:15:132-03:16:623 F:D:I:DI:??
(an inexpensive)     &   yasui??
(shop)                     &   o-mise????
?<SB>        (is there)                 &   aru-ka-na<SB>0037 - 03:17:302-03:20:887 F:O:I:AO:??
(here)                      &   kono?????
(near)                      &   chikaku-desu-to??????
(ANNEX)               &   anekkusu-to???????
(Nagoya PARCO)  &    nagoya-paruko-ga?????
?<SB>   (there are)                &   gozai-masu-ga<SB>Well?, I want to buy clothes, so, is there  an inexpensive shop somewhere near here?Near here, there are ANNEX and Nagoya PARCO.driver?s utterancedriver?s turnoperator?s turndriver?s utteranceoperator?s utteranceFigure 1: Sample of transcribed textfor smaller restrictions.
The discretization oftagging points enables not only coherent tag-ging but also the reduction of tagging cost.?
Elaboration using synthesized sound: Anannotator checks the validity of the anno-tation by listening to the sounds.
In otherwords, an annotator elaborates the annotationby revising it many times by listening to theautomatically created dialogue sound whichincludes not only driver?s voices but alsosounds of back-channel feedbacks generatedaccording to the provided timings.
The back-channel feedbacks had been synthesized byusing a speech synthesizer because our cor-pus aims to be used for implementing thesystemwhich can provide back-channel feed-backs.3 Corpus ConstructionWe constructed the back-channel feedback corpusby annotating an in-car speech dialogue corpus.3.1 CIAIR in-car spoken dialogue corpusWe used the CIAIR in-car spoken dialogue corpus(Kawaguchi et al, 2005) as the target of annota-tion.
The corpus consists of the speech and tran-scription data of dialogues between a driver andan operator about shopping guides, driving direc-tions, and so on.
Figure 1 shows an example ofthe transcription.
We used only the utterances ofdrivers in the corpus.
We divided the utterancesinto morphemes by using the morphological ana-lyzer Chasen1.
In addition, each morpheme wasprovided start and end times estimated by usingthe continuous speech recognition system Julius2.3.2 Tagging of spoken dialogue corpusWe constructed the corpus by providing the back-channel feedback tags at the proper timings forthe driver?s utterances, according to the design de-scribed in Section 2.1http://chasen-legacy.sourceforge.jp2http://julius.sourceforge.jp206sp [short pause](F?)
(Well?)?
(clothes)?
(no translation)sp [short pause]??
(buy)??
(want to)?
(no translation)?
(no translation)??
(so)??
(somewhere)?
(no translation)??
(near hear)?
(no translation)sp [short pause]pause [pause]??
(inexpensive)?
(no translation)?
(shop)??
(is there)?
(no translation)??
(no translation)0.0000.0300.0900.3400.5200.6100.8501.0801.1501.2401.4201.6701.8502.1902.8803.0804.9925.3625.4225.6525.8325.9820.0300.0900.3400.5200.6100.8501.0801.1501.2401.4201.6701.8502.1902.8803.0804.9925.3625.4225.6525.8325.9826.272content start time end timeFigure 2: Sample of division of a dialogue turninto basic segmentsFor ?comprehensive tagging,?
an annotator lis-tens to each dialogue turn3 from the start and tagsa position where a back-channel feedback can beprovided when the timing is found.
Here, the tim-ing of the last back-channel feedback is also usedfor judging whether or not the timing is unnatural.For ?off-line tagging,?
an annotator tags thetranscribed text of each dialogue turn of drivers.To perform ?discretization of tagging points,?
adialogue turn is assumed to be a sequence of mor-phemes or pauses (hereafter, we call them basicsegments), which are continuously arranged onthe time axis, and it is judged whether or not aback-channel feedback should be provided at eachbasic segment.
Here, in consideration of the un-equal pause durations, if the length of a pause isover 200ms, the pause is divided into the initial200ms pause and the subsequent pause, each ofwhich is considered as a basic segment.
Figure 2shows an example of a dialogue turn divided intobasic segments.Furthermore, for ?elaboration using synthesizedsound,?
we prepared the annotation environmentwhere the dialogue sound including not onlydriver?s voice but also back-channel feedbacksgenerated according to the provided timings is au-tomatically created in real time for annotators tolisten to.
There are several types of back-channelfeedbacks and in normal conversations, we chooseand use appropriate back-channel feedbacks fromamong them according to the scene.
In our study,3A dialogue turn is defined as the interval between thetime at which the driver starts to utter just after the opera-tor finishes uttering and the time at which the driver finishesuttering just before the operator starts to utter.play buttonturn IDdriver IDupdate buttonlist of turn IDsFigure 3: Web interface for taggingTable 1: Size of back-channel feedback corpusdrivers 346dialogue turns 11,181clauses 16,896bunsetsus4 12,689morpheme segments 94,030pause segments 19,142back-channel feedbacks 5,416we used the most general form ???
hai (yes)?for the synthesized speech since our focus wason the timing of back-channel feedbacks.
Theback-channel feedbacks had been created by us-ing Hitachi?s speech synthesizer ?HitVoice,?
andone feedback was placed 50 milli-seconds after thestart time of a tagged basic segment.We developed a Web interface for tagging back-channel feedbacks.
Figure 3 shows the Web inter-face.
The interface displays a sequence of basicsegments in a dialogue turn in table format.
Anno-tators perform tagging by checking basic segmentswhere a back-channel feedback can be provided.3.3 Size of back-channel feedback corpusTable 1 shows the size of our corpus constructedby two trained annotators.
The corpus includes5,416 back-channel feedbacks.
This means that aback-channel feedback is generated at intervals ofabout 21 basic segments.4 Corpus EvaluationWe conducted experiments for evaluating the tag-ging in the constructed corpus.4Bunsetsu is a linguistic unit in Japanese that roughly cor-responds to a basic phrase in English.
A bunsetsu consists ofone independent word and zero or more ancillary words.207Table 2: Kappa values of the existing corpusa,c a,d a,b c,d b,c b,d?
0.536 0.438 0.322 0.311 0.310 0.1674.1 Coherency of corpus taggingWe conducted an evaluation experiment to con-firm that the tagging is coherently performed inthe corpus.
In the experiment, two different an-notators performed tagging on the same data, andthen we measured the degree of the agreement be-tween them.
As the indicator, we used Cohen?skappa value (Cohen, 1960), calculated as follows:?
= P (O)?
P (E)1?
P (E)where P (O) is the observed agreement betweenannotators, and P (E) is the hypothetical proba-bility of chance agreement.
A subject who hasa certain level of knowledge annotated 673 dia-logue turns.
The kappa value was 0.731 (P (O) =0.975, P (E) = 0.907), and thus we can see thesubstantial agreement between annotators.As the target for comparison, we used the kappavalue in the existing back-channel feedback cor-pus (Kamiya et al, 2010).
The corpus had beenconstructed by the way that the recorded driver?svoice was replayed and 4 subjects independentlyproduced back-channel feedbacks for the samesound.
This means that the policies for taggingthe existing corpus differ from those of our corpus,and are ?on-line tagging,?
?tagging on the timeaxis?
and ?tagging without elaborating.?
In theexisiting corpus, 297 dialogue turns were used asdriver?s sound.
Table 2 shows the kappa value be-tween two among the 4 subjects.
The kappa valueof our corpus was higher than that between anysubjects of the existing corpus, substantiating thehigh coherency of our corpus.4.2 Validity of corpus taggingIn our corpus, we discretized the tagging pointsto enhance the coherency of tagging.
However,such constraint restricts the points available fortagging and may make annotators provide tags atthe unnatural timings.
Therefore, we conducteda subjective experiment to evaluate the natural-ness of the back-channel feedback timings.
Inthe experiment, one subject listened to the replayof our back-channel feedback corpus and subjec-tively judged the naturalness of each timing.
Theback-channel feedback sound was generated in thesame way described in Section 3.2.In the experiment, we used 345 dialogue turnsincluding 131 back-channel feedbacks.
98.47%of all the back-channel feedbacks were judged tobe natural.
Only 2 back-channel feedbacks werejudged to be unnatural because the intervals be-tween them and the back-channel feedbacks pro-vided immediately before them were felt too short.This showed the validity of our discretization oftagging points.5 ConclusionThis paper described the design, construction andevaluation of the back-channel feedback corpuswhich had the coherency of tagged back-channelfeedback timings.
We constructed the spoken di-alogue corpus including 5,416 back-channel feed-backs in 11,181 dialogue turns.
The results of ourevaluation confirmed high coherency and enoughnaturalness of our corpus.In the future, we will use our corpus to seeto what extent the timings of back-channel feed-backs that have been annotated correlate with thecues provided by earlier researchers.
Then we willdevelop a system which can detect back-channelfeedback timings comprehensively.Acknowledgments: This research was supportedin part by the Grant-in-Aid for Challenging Ex-ploratory Research (No.21650028) of JSPS.ReferencesN.
Cathcart, J. Carletta, and E. Klein.
2003.
A shal-low model of backchannel continuers in spoken dia-logue.
In Proc.
of 10th EACL, pages 51?58.J.
Cohen.
1960.
A coefficient of agreement for nom-inal scales.
Educational and Psychological Mea-surement, 20:37?46.Y.
Kamiya, T. Ohno, S. Matsubara, and H. Kashioka.2010.
Construction of back-channel utterance cor-pus for responsive spoken dialogue system develop-ment.
In Proc.
of 7th LREC.N.
Kawaguchi, S. Matsubara, K. Takeda, andF.
Itakura.
2005.
CIAIR in-car speech corpus ?influence of driving status?.
IEICE Trans.
on Info.and Sys., E88-D(3):578?582.S.
K. Maynard.
1989.
Japanese conversation :self-contextualization through structure and interac-tional management.
Ablex.M.
Takeuchi, N. Kitaoka, and S. Nakagawa.
2004.Timing detection for realtime dialog systems usingprosodic and linguistic information.
In Proc.
ofSpeech Prosody 2004, pages 529?532.N.
Ward and W. Tsukahara.
2000.
Prosodic featureswhich cue back-channel responses in English andJapanese.
Journal of Pragmatics, 32:1177?1207.208
