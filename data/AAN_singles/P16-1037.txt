Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 388?398,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsNews Citation Recommendation with Implicit and Explicit SemanticsHao Peng?,1Jing Liu,2Chin-Yew Lin21School of EECS, Peking University, Beijing, 100871, China2Microsoft Research, Beijing, 100080, Chinapenghao.pku@gmail.com {liudani, cyl}@microsoft.comAbstractIn this work, we focus on the problemof news citation recommendation.
Thetask aims to recommend news citationsfor both authors and readers to createand search news references.
Due to thesparsity issue of news citations and theengineering difficulty in obtaining infor-mation on authors, we focus on contentsimilarity-based methods instead of col-laborative filtering-based approaches.
Inthis paper, we explore word embedding(i.e., implicit semantics) and grounded en-tities (i.e., explicit semantics) to addressthe variety and ambiguity issues of lan-guage.
We formulate the problem as a re-ranking task and integrate different simi-larity measures under the learning to rankframework.
We evaluate our approach ona real-world dataset.
The experimental re-sults show the efficacy of our method.1 IntroductionWhen an author writes an online news article,s/he often cites previously published news reportsto elaborate a mentioned event or support his/herpoint of view.
For the convenience of the read-ers, the editor usually associates the words withhyperlinks.
Through the links the readers can di-rectly access the referenced articles to know moredetails about the events.
If there is no referencefor a mentioned event, the readers may search therelated news reports for further reading.
Hence,it is valuable to have automatic news citation rec-ommendations for authors and readers to create orsearch news references.In this paper, we focus on the problem of newscitation recommendation.
As shown in Table 1,?Work done during internship at Microsoft Research.given a snippet of citing context (left), the taskaims to retrieve a list of news articles (right) asreferences.
This task differs from traditional rec-ommendation tasks, e.g., citation recommendationfor scientific papers, in that: (a) based on the statis-tics from our dataset, the number of references pernews article is 4.56 on average, much less thanthe number of citations per academic paper (typ-ically dozens); (b) the author-topic information isusually unavailable, since it is technically difficultto obtain author information from news articles.These differences make the collaborative filtering-based methods, which have been widely appliedto paper citation recommendation, less availablein our scenario.
Therefore, in this paper we focuson content similarity-based methods to deal withthe task of news citation recommendation.Previous studies use string-based overlap (Xu etal., 2014), machine translation measures (Madnaniet al, 2012), and dependency syntax (Wan et al,2006; Wang et al, 2015) to model text similar-ity.
More recent work focuses on neural networkmethods (Yin and Sch?utze, 2015; He et al, 2015;Hu et al, 2014; dos Santos et al, 2015; Lei et al,2016).
There are two major challenges renderingthese approaches not suitable for this task: (i) thevariety and (ii) the ambiguity of language.
By va-riety, we mean that the same meaning may be ex-pressed with different phrases.
Taking the first rowin Table 1 for example, Vlaar in the citing contextrefers to Ron Vlaar, a Dutch football player, whois referred to as Dutch star and Netherlands in-ternational in the cited article.
By ambiguity, wemean that the same expression may have differentmeanings in different contexts.
In the second ex-ample in Table 1, the mention tiger refers to tigerthe mammal.
By contrast, in ?Detroit Tigers links:The Tigers are in trouble?
for example, the wordTiger is the name of a team.
In this paper, we ex-plore both implicit and explicit semantics to ad-388Citing Context Cited Article?
?
?Man United and Arsenal on red alert as top Dutch star officiallyjoins free agent listManchester United and Arsenal have both beeninterested in Vlaar in the past, suggestingSouthampton will have to fight hard to land him.The Netherlands international has joined the free agent list today and isno longer contractually obliged to remain at Villa Park.?
?
?
?
?
??
?
?
Bangladesh ?s abundant tiger population has collapsed to just 100Conservationists want the Bangladeshigovernment to step up and help save the tigersthrough greater administration and enforcementof anti-poaching laws, as Bangladesh does notlegally protect tigers to the extent that othergovernments do, according to Inhabitat.In Bangladesh, a new census shows that tiger populations in theSundarbans mangrove forest are more endangered than ever.
Thestudy, which used hidden cameras to track and record tigers, provides amore accurate update than previous surveys that used other methods.The year-long census, which ended this April, revealed only around100 of the big cats remain in what was once home to the largestpopulation of tigers on earth.?
?
?
?
?
?Table 1: Two pair of news snippets.
For readability concerns, we keep only the sentence associated with an anchor link in theciting part, and the title and lead paragraph of the cited part.dress the above issues.
Specifically, the implicitsemantics can be obtained from the word embed-ding trained on large scale corpus, and the explicitsemantics through linking entity mentions to thegrounded entities in a knowledge base.In this paper, we explore using both word em-bedding and grounded knowledge to model the re-latedness between citing context and articles.
Weformulate the problem as a re-ranking task.
Weuse learning to rank to integrate different similar-ity measures and evaluate the models on a realworld dataset constructed from Bing News1.
Wefurther give quantitative analysis of the effects ofword embedding and grounded entities in the task.In summary, the main contributions of this pa-per are three-fold:?
We propose the task of news citation recom-mendation and construct a real-world datasetfor this task.?
We utilize both word embedding based sim-ilarity measures and knowledge-based meth-ods to tackle the problem.
We formulate theproblem as a re-ranking task and leveragelearning to rank algorithm to integrate differ-ent similarity measures.?
We conduct extensive experiments on a largedataset.
The results show the effectivenessof word embedding and grounded entities.We further quantitatively analyze how the im-plicit semantics from word embedding andexplicit semantics from grounded knowledgebenefit the task of interest.1https://www.bing.com/news2 Problem FormulationIn this section, we introduce the news citation rec-ommendation problem and formulate it as a re-ranking task.
We first introduce definitions thatwill be used through the rest of the paper:Citing Context.
Citing context is a sentence whichcontains an anchor text associated with a hyper-link.
As shown in Table 1, the underlined wordsare associated with a hyperlink pointing to anothernews article, and the sentence (left) which containsthe anchor is the citing context.Cited Article.
Given a piece of citing context, thearticle that the hyperlink links to is defined as itscited article.
It is expected that a news article iswell-structured, and its headline together with itslead paragraph gives a good brief description ofthe whole story (Kianmehr et al, 2009).
In thispaper, a news article can either be represented byits title and lead paragraph or by the passage as awhole.
We conduct experiments under both of thetwo different settings.Candidate Article Set.
Considering efficiency, wefollow the procedure adopted by many recommen-dation systems (Lei et al, 2016; Tan et al, 2015)and formulate the problem as a re-ranking task.
Inanother word, given a citing context, we first useefficient retrieval methods with high recall to gen-erate a list of articles as the candidate article set,and then run the system to get a re-ranked list.News Citation Recommendation.
Given a citingcontext, the task aims to construct an ordered listof news articles, top of which are most relevant tothe context, and can serve as the cited articles.3893 MethodIn this section, we first explain the similarity mea-sures based on word embedding (implicit seman-tics) and grounded knowledge (explicit semantics)to deal with variety and ambiguity problems.
Thenwe briefly introduce the baselines and the learningto rank framework.3.1 Implicit Semantics for VarietyThe distributed word representation by word2vecfactors word distance and captures semanticsimilarities through vector arithmetic (Mikolovet al, 2013).
In this work, we train a skip-grammodel to bridge the vocabulary gap betweencontext-article pairs.
Previous work representsthe documents with averaged vectors of words(Tang et al, 2014; Tan et al, 2015).
However, thismay lead to the loss of detailed information of thedocuments.
In this paper, we adopt a differentapproach, explained below.Word Mover?s Distance (WMD).
Kusner etal.
(2015) combine distributed word represen-tations with the earth mover?s distance (EMD)(Rubner et al, 1998; Wan, 2007) to measurethe distance between documents.
They use theEuclidean distance between words?
low dimen-sional representations as building blocks, andoptimize a special case of the EMD to obtainthe cumulative distance.
More formally, letX = {(x1, wx1), (x2, wx2), ?
?
?
, (xm, wxm)} bethe normalized bag-of-words representation for aciting context after removing stop-words, whereword xiappears wxitimes (then normalized by thetotal count of words in X), i = 1, 2, ?
?
?
,m. Sim-ilarly, we have the representation for a candidatearticle, Y ={(y1, wy1), (y2, wy2), ?
?
?
, (yn, wyn)}.The WMD calculates the minimum cumulativecost by solving the linear programming problembelow:minTm?i=1n?j=1Tijcijs.t.n?j=1Tij= wxi, i = 1, 2, ?
?
?
,m,m?i=1Tij= wyj, j = 1, 2, ?
?
?
, n,where T ?
Rm?nis the transportation flow ma-trix, and cijindicates the distance between xiandyj.
Here cij= ?vector(xi)?
vector(yj)?, wherefunction vector(w) returns the word vector of w.Then the distance is normalized by the total flow:WMD(X,Y ) =?mi=1?nj=1Tijcij?mi=1?nj=1Tij3.2 Explicit Semantics for AmbiguityNews articles tend to be well written, and con-tain many named entity mentions.
Making use ofthis property, we deal with the ambiguity prob-lem by using grounded entities (explicit seman-tics).
Given a context-article pair, we first rec-ognize all named entity mentions on both sidesand link them to knowledge bases (e.g., Wikipediaand Freebase), then use the following measures tomodel the similarity.?
Entity Overlap.
Given a context?article pair,we consider two metrics, namely, precisionand recall, to measure their entity overlap.The precision is defined as:precision =entity-overlap(citing, cited)entity-count(citing)and recall as:recall =entity-overlap(citing, cited)entity-count(cited)?
Embedding Based Matching.
We build twoseparate information networks for Wikipediaentities using (a) the anchor links onWikipedia pages and (b) the Freebase en-tity graph (Bollacker et al, 2008).
Then weapply Large-scale Information Network Em-bedding (LINE) (Tang et al, 2015) system2to the networks to embed the entities intolow-dimensional spaces.
We then measurethe similarity by the minimized cosine dis-tance between entities?
on the citing and thecited side:minDISciting=1|citing||citing|?i=1minyj?Y(1?cos(xi, yj)),and vice versa:minDIScited=1|citied||cited|?j=1minxi?X(1?
cos(xi, yj)),where X refers to the citing context, andxi?
X are the grounded entities in the citingpart.
Similar with Y and yj.2https://github.com/tangjianpku/LINE390?
Wikipedia Evidence.
Given a context-article pair, we refer to world knowledge forsupporting evidence.
In particular, we firstapply an entity linking system to detect theentity mentions on both sides and groundthem into Wikipedia entries, each of whichhas its own description page.
Second, we col-lect the descriptions for entities from the can-didate article and extract as evidence thosesentences containing entities from citing con-text.
We refer to this evidence as cited ev-idence.
For instance, the article in Table 8contains grounded entity Scottish NationalParty.
And in the description for it, thereis a sentence containing the entity Scotlandfrom the citing context: ?The Scottish Na-tional Party (SNP) is a Scottish nationalistand social-democratic political party in Scot-land.?
Thus we extract this sentence as citedevidence supporting this pair.We count the overlapping nouns between theciting context and the cited evidence to cal-culate precision and recall,precision=noun-overlap(context, cited evidence)noun-count(context)recall=noun-overlap(article, citing evidence)noun-count(article)3.3 BaselinesWe design several baseline features for the twogroups of features mentioned above:?
TF-IDF Distance.
We use TF-IDF distanceas a basic measure.
The similarity is calcu-lated with cosine distance based on TF-IDFvector representations for the text.?
Ungrounded Mentions.
Note that entityoverlap features also adapt to ungroundedmentions.
The embedding-based matchingfeatures for ungrounded mentions are simi-lar to those for grounded entities.
The onlydifference is that here each mention is rep-resented by the averaged vectors of all thewords it contains.
Wikipedia evidence is notfeasible for ungrounded mentions.Table 2 summarizes all the features we use.
Acited article can either be represented by its head-line+lead paragraph or as a whole.
Therefore,we extract features under two different settings:(a) headlines and lead paragraphs only; (b) thefull articles.
Most of the features are extractedunder both of the settings.
However, feature 2is much too computation-intensive and feature 7needs POS-tagging as the preprocessing.
Thusthese two are only extracted under setting (a).3.4 Learning to Rank FrameworkMany different learning to rank algorithms havebeen proposed to deal with the ranking problem,including pointwise, pairwise, and listwise ap-proaches (Xia et al, 2008).
Listwise methods re-ceive ranked lists as training samples, and directlyoptimize the metric of interest by minimizing therespective cost.
And it has been reported thatthe listwise method usually achieves better perfor-mance compared to others (Qin et al, 2008; Caoet al, 2007).
In this work, we use the linear modeland apply coordinate ascent for parameter opti-mization.4 Experiments4.1 Data CollectionWe collect one month?s news articles from BingNews.
The citing context set consists of all thesentences associated with anchor link(s).
For eachpiece of citing context, its cited article is extractedthrough its hyperlink.
If there are multiple linksassociated with the context, only the first one isconsidered.
We pair each citing context and itscited article as a ground truth sample.
We fur-ther label as ground truths those articles sharingthe same title as the cited article.
This is ratherreasonable since a single passage may have mul-tiple reprints by difference sources.
On average,there are 2.20 ground truth cited articles for eachciting context in the dataset.In order to focus only on news events, we fil-ter out those pairs whose hyperlinks are associatedwith three words or less (usually names for per-sons or places, and lead to definition pages).
Wealso discard those samples whose citing contextscontain or are exactly the same as the titles of thecited articles.
For example, ?READ MORE: Thestories you need to read, in one handy email?
linksto an article titled ?The stories you need to read, inone handy email?.The dataset is preprocessed with StanfordCoreNLP toolkit (Manning et al, 2014), includingsentence splitting, tokenizing for whole passages,and POS-tagging for titles and lead paragraphs.We use the JERL system by Luo et al (2015) forentity detection and grounding.
It recognizes en-tity mentions and links them to Wikipedia entries.391Feature Full Article?
# of features DescriptionDealing with Variety1 WMD n 1 Word vector based earth mover?s distance.Dealing with Ambiguity2 Grounded Entity Overlap y 4 Precision and recall for grounded named entities.3 Embedding-based Matching y 16 Minimized matching distance with LINE vectors.4 Wikipedia Evidence n 2 Precision and recall for evidence from Wikipedia.Baselines5 TF-IDF y 2 The cosine distance with TF-IDF.6 Ungrounded Mention Overlap y 4 Precision and recall for ungrounded mentions.7 Embedding-based Matching y 4 Minimized matching distance with averaged vectors.Total 33Table 2: A list of all features used in the experiments.
The third column indicates whether the corresponding feature is extractedfrom the full articles.
If not, it?s extracted only from the headlines and lead paragraphs.We use each mention?s text span as an ungroundedmention, and its corresponding Wikipedia ID asa grounded entity.
For instance, in Table 8, thedetected text span Westminster is an ungroundedmention, and it?s grounded to the entry Parliamentof the United Kindom.4.2 Selecting CandidatesGiven a citing context, we construct its candidatearticle set with the top 200 articles retrieved by TF-IDF distance.
In the experiments, approximately92.61% of the ground truth cited articles appear inthe candidate sets.
We discard those that do not.We further randomly split the remaining 33318pairs into training/validation/test sets with the pro-portion of 3:1:1.For each training pair, we randomly sample 5articles from its candidate article set (excludingground truth) and pair them with the citing con-text as negative samples.
According to Tan etal.
(2015), the number of negative samples doesnot significantly affect the linear learning to rankmodel?s performance.
During validation and test-ing, all of the 200 candidates are taken into ac-count.4.3 Experimental SetupIn the experiments, we set the TF-IDF as the base-line, and incrementally add different groups of fea-tures to the system.The word embedding is pretrained with skip-gram model (Mikolov et al, 2013) on Wikipediacorpus and then fine-tuned using the method pro-posed in Wieting et al (2015) on PPDB (Ganitke-vitch et al, 2013).
The embedding fine-tuned withparaphrase pairs can better capture the semanticrelatedness of different phrase.
In the experiments,we observe a 1% ?
2% improvement by the fine-tuned word representations compared to vanillaskip-gram vectors.We use the linear model in RankLib3for thelearning to rank implementation.
Coordinate as-cent is used for parameter optimization.
Themodel is trained to directly optimize the evaluationmetrics, Precision@1, Precision@5, NDCG@5and MAP, respectively.For NDCG@5 measure, we set a binary rele-vance score, i.e., the scores equal to 1 for groundtruths, 0 for negative samples.4.4 Experimental ResultsTable 3 gives the performance of the baselinesand the systems using different groups of fea-tures on test and validation sets.
The results showthat WMD brings a consistent improvement overits TF-IDF baseline, and so do grounded entitiescompared to ungrounded mentions.Individually added to the TF-IDF baseline,WMD has the largest performance boost, followedby grounded entity features.
Besides, the addi-tional information from grounded entity knowl-edge helps the model outperform the ungroundedmentions, with a consistent margin of 1.0%-2.0%NDCG@5.We further compare the performance of themodels when using features from headlines+leadparagraphs only and those from full passages.
Asshown in Table 3, the former brings much betterperformance on each metric compared to the lat-ter.It?s worth noting that there are ground truthsmis-labeled as irrelevant in the dataset.
A primary3https://sourceforge.net/p/lemur/wiki/RankLib/392Precision@1 Precision@5 NDCG@5 MAPid Features Test Dev Test Dev Test Dev Test DevHeadline + Lead Para1 TF-IDF 42.61 42.21 19.84 19.78 52.72 52.22 53.50 53.062 + Ungrounded Mentions 43.67 43.04 19.45 19.30 53.84 53.26 54.46 54.123 + Grounded Entities 44.52 44.02 20.84 20.51 55.99 55.0 56.55 56.094 + Ungrounded+Grounded 43.93 44.05 20.2 19.66 55.99 55.17 56.52 56.135 TF-IDF + WMD 45.94 45.84 21.11 21.62 57.20 57.50 58.12 58.346 + Ungrounded Mentions 46.44 46.63 21.05 21.56 57.61 57.80 58.55 58.787 + Grounded Entities 47.63 47.5 21.96 22.07 58.52 58.41 60.01 59.838 + Ungrounded+Grounded 47.23 46.84 21.58 21.56 59.01 58.91 59.88 59.66Full Article9 TF-IDF 49.3 48.11 23.33 23.06 60.51 59.54 60.71 59.7310 + Ungrounded Mentions 50.46 50.42 23.81 23.67 61.97 61.73 62.6 61.9411 + Grounded Entities 51.42 50.27 23.91 23.78 63.26 62.09 63.23 62.1512 + Ungrounded+Grounded 51.46 50.23 23.85 23.74 62.94 62.48 63.15 63.0213 TF-IDF + WMD 52.31 51.82 23.87 24.04 63.71 63.99 64.08 63.6214 + Ungrounded Mentions 53.26 53.3 23.98 24.16 64.57 64.29 64.52 64.3715 + Grounded Entities 54.12 53.29 24.37 24.05 65.29 64.48 65.32 64.5316 + Ungrounded+Grounded 54.04 53.21 24.52 24.33 65.56 65.11 65.35 64.56Table 3: Experimental results in percentage on the dataset collected from Bing News.id FeaturesNDCG@5 on SNDCG@5 on?SHeadline + Lead Para1 TF-IDF52.7756.282 + Ungrounded Mentions53.3456.863 + Grounded Entities55.0358.574 + Ungrounded+Grounded55.1858.865 TF-IDF + WMD56.5160.136 + Ungrounded Mentions57.0460.827 + Grounded Entities57.461.478 + Ungrounded+Grounded58.0561.78Table 4: Experimental results in percentage onS and?S.
S is arandomly constructed subset of the test set, and?S is obtainedby manually labeling samples in S.reason is that news sites sometimes individuallypublish different reports on a certain event.
Andthe articles don?t necessarily share the same title.To see how this affects the model, we randomlybuild a subset S of the test set and manually la-bel the selected samples, which gives?S4.
Table4 compares the model?s performance on S and?Sunder Headline+Lead paragraph setting.
There isa consistent improvement of NDCG@5 score on?S compared to that on S. Besides that, on manu-ally labeled data, the model?s performance acrossdifferent feature settings is almost in accord withthat on the full test set.
These results show thatthere are indeed mis-labeled ground truths in thedataset, but they have little influence when com-paring different groups of features.4Manually labeling all of the dev and test samples wouldbe too time consuming, and we leave it to future work.5 AnalysisIn this section, we give detailed win-loss analy-sis for the models trained with NDCG@5 metricunder headlines+lead paragraphs setting.
Specif-ically, given two systems with different featureconfigurations, we compare their performance oneach test sample.
The results are shown as aheatmap in Figure 1.
X and Y axises indicate theidentifiers for each feature group, following thosein Table 3.
For example, the data point at (5, 1)indicates that the inclusion of WMD brings bet-ter ranking scores to TF-IDF on 18.4% of the testsamples; and as a trade off, it lowers the scoreson 11.4% of the samples.
We also observe thatgrounded entities brings gain to 15.9% of the sam-ples, and loss for 9.6% of them.
On average, twodifferent groups of features disagree on 26.4% ofthe test samples.We further give several mis-predictions by themodel using certain groups of features, and il-lustrate how they are corrected by the inclusionof others (or the other way round).
By mis-prediction, we mean that no ground truth cited arti-cle appears in the top 5 predictions of the returnedlist.5.1 Dealing with VarietyTable 5 shows a mis-prediction by TF-IDF, butcorrected after including WMD.TF-IDF distance favors the high-score match-393ing keywords approval and rating between the cit-ing context and mis-predicted article.
On the otherhand, distributed word representations factor thedistances between word pairs, which helps to cap-ture their semantic closeness, e.g., (Argentines,Argentina, Cosince distance: 0.210), (poll, elec-tion, 0.020), and (increasingly, growing, 0.286).WMD helps to bridge the vocabulary gap betweenthe citing context and the cited article.On the other hand, though not often, the useof distributed representation can also create mis-takes.
Table 6 gives an example where the inclu-sion of the WMD feature changes a correct pre-diction by TF-IDF into a mistake.
By analyzingthe WMD?s transportation flow matrix T, we findthat the used word embedding relates MP to min-ister, and publicly to government.
More curiously,persons?
names are very similar in its semanticspace: (Davies, Stephen, 0.602), and (Davies,Harper, 0.635).
A possible reason could be thatboth of the two names are very common, andthus the cooccurrence-based representation learn-ing method is not able to distinguish them.
Thisalso justifies our use of grounded entities as ad-ditional information: from the Wikipedia descrip-tion for entity Stephen Harper, the system mightbe able to find out that he actually serves in Cana-dian government, not in the UK?s nor in the Welsh.5.2 Dealing with AmbiguityEntity grounding helps by resolving the ambiguitye.g., alias, abbreviation, of the entity mentions.As shown in Table 7, tiger refers to the mam-mal in the ground truth pair.
However, the sameword refers to Detroit Tigers the team in the mis-predicted article.
This ambiguity is resolved whenthe mention is grounded to its Wikipedia entry.
Inanother example shown in Table 8, ungroundedmention SNP, though detected, contributes little tosupporting the ground truth pair.
However, whenit?s grounded to the entry Scottish National Party,the system leverages world knowledge and relatesit to the mention of Scotland in the citing context.The inclusion of grounded entity informationmay also lead to mistakes, many of which are dueto the limited performance of the entity recogni-tion and disambiguation system.
We?d like to dis-cuss another kind of error here, shown in Table9.
In the citing context, The Daily Telegraph isa newspaper published in the UK.
It has little todo with the involved event except for reporting it.However, the system favors a farmers?
story whichFigure 1: Heatmap for win-loss analysis results.
Point (x, y) indicates howmuch feature x wins (loses if negative) against y.The X and Y axises indicatethe identifiers for each feature group, following those in Table 3.actually happened in the UK.
We find that this con-tributes a lot to the system?s errors when includinggrounded entities.
We leave it to future work tofigure out how to deal with this issue.6 Related WorkThis section reviews three lines of related work: (i)document recommendation, (ii) pharaphrase iden-tification, (iii) question retrieval.6.1 Document RecommendationExisting literature mainly focuses on content-based methods and collaborative filtering (Ado-mavicius and Tuzhilin, 2005).
There are studiestrying to recommend documents based on citationcontexts, either through identifying the motiva-tions of the citations (Aya et al, 2005), or throughthe topical similarity (Ritchie, 2008; Ramachan-dran and Amir, 2007).
On the other hand, Mcneeet al (2002) leverage multiple information sourcesfrom authorship, paper-citation relations, and co-citations to recommend research papers.Combining the context-based approaches andcollaborative filtering, Torres et al (2004) andStrohman et al (2007) report better performance.Tang and Zhang (2009) use the Restricted Boltz-mann Machine to model citations for placeholders,and Tan et al (2015) integrate multiple features torecommend quotes for writings.In the news domain, context-based approachesare presumably favorable due to the fact that thearticles are relatively content-rich and citation-sparse.
Previous studies manage to utilize infor-mation retrieval techniques to recommend newsarticles given a seed article (Yang et al, 2009;Bogers and van den Bosch, 2007).394Sides ?Samplesciting ?An earlier poll showed Argentines are also increasingly happy with her performance as President, putting her approvalrating at almost 43%, up from 31% in September.citedGround TruthKirchner?s Growing Popularity Could Skew Argentine ElectionAs Argentina gears up for a presidential election in October, the approval ratings of the current president, CristinaKirchner, are improving and her rising reputation could affect the results of the election to replace her.Top-1 PredictionBill Shorten?s Approval Rating Falls in Wake of Royal CommissionThe opposition leader gained approval from only 27% of the voters surveyed, while 52% disapproved.Table 5: A mis-prediction by TF-IDF corrected by the inclusion of WMD.Sides ?Samplesciting ?Chris Grayling was responding to a question from Gower Conservative MP Byron Davies about the regenerationinvestment fund for Wales ?and the underselling of a large amount of publicly owned property?.citedGround TruthWales land deal leaves taxpayers 15m shortA Welsh government spokesperson said there were conflicting valuations.Top-1 PredictionConservative MP compares Stephen Harper government to Jesus, inspiring hilarious #CPCJesus tweetsIs it time we started referring to Prime Minister Stephen Harper as ?Our Lord and Saviour?
?Table 6: A correct prediction by TF-IDF but then changes into a mistake when including WMD.Sides ?Samplesciting ?Conservationists want the Bangladeshi governmentGovernment of Bangladeshto step up and help save the tigersthrough greater administration and enforcement of anti-poaching laws, as BangladeshiBangladeshdoesnot legally protect tigers to the extent that other governments do, according to Inhabitat.citedGround TruthBangladeshi?sBangladeshabundant tiger population has collapsed to just 100In BangladeshiBangladesh, a new census shows that tiger populations in the SundarbansSundarbansmangroveforest are more endangered than ever.
The study, which used hidden cameras to track andrecord tigers, provides a more accurate update than previous surveys that used other methods.Top-1 PredictionDetroit TigersDetroit Tigerslinks: The Tigers areDetroit Tigersin troubleAfter losing three straight games prior to All-Star break, theMajor League Baseball All-Star Gamethe Tigers don?tDetroit Tigershave muchmore time to waste if they want to stay in contention.Table 7: A mis-prediction by TF-IDF corrected by the inclusion of grounded entity features.
The linked Wikipedia entries are indicated below the underlined entitymentions.Sides ?Samplesciting ?With activities at Westminster challengingParliament of the United Kingdoma narrow view of nationalism, and a plannedcharm offensive across the UK and Ireland, it isIrelandUnited Kingdom relationsthat the party intends to significantly expandits reach beyond ScotlandScotland.citedGround TruthSNP launchesScottish National Partybid to extend influence beyond ScotlandScotlandFirst Minister of Scotland and SNP leader Nicola SturgeonScottish National Party Nicola Sturgeonworked hard to reassure voters inthe election campaign.Top-1 PredictionApple Pay UK launchApple Pay United Kingdomconfirmed for mid-JulyLeaked documents from retailers suggest a launch date early next week.Table 8: A mis-prediction by TF-IDF+ungrounded mention features corrected by the TF-IDF+grounded entity features.
The linked Wikipedia entries are indicatedbelow the underlined entity mentions.395Sides ?Samplesciting ?According to a UK Telegraph report,United Kindom The Daily Telegraphthe government is now forcing farmers and foodmanufacturers to sell anywhere from 30-100% of their products to the state , as opposed tostores and supermarkets.citedGround TruthVenezuelanVenezuelafarmers ordered to hand over produce to stateAs Venezuela?sVenezuelafood shortages worsen, the president of the country?s Food IndustryFood IndustryChamber has said that authorities ordered producers of milkmilk, pasta, oiloil, ricerice, sugarsugarand flourto supply their products to the state stores.Top-1 PredictionWelsh farmersUnited Kindomlaunch #NoLambWeek price campaignFed-up Welsh farmersUnited Kindomare encouraging others to withhold their fat lambs for a week inprotest at the current slump in the UK lamb trade.Table 9: A correct prediction by TF-IDF but then changes into a mistake when including grounded entity features.
The linked Wikipedia entries are indicated belowthe underlined entity mentions.6.2 Paraphrase IdentificationSeveral hand-crafted features have proven help-ful in modeling sentence/phrase similarity, e.g.,string-based overlap (Xu et al, 2014), machinetranslation measures (Madnani et al, 2012), anddependency syntax (Wan et al, 2006; Wang etal., 2015).
Using the combination and discrimina-tive re-weighting of the mentioned features, Ji andEisenstein (2013) manage to obtain more compet-itive results.More recent work has switched the focus ontoneural methods.
Socher et al (2011) recursivelyencode the representations of sentences by thecompositions of words.
Convolutional neural nets(LeCun et al, 1998; Collobert and Weston, 2008)are also exploited in the tasks of paraphrase identi-fication and sentence matching (Yin and Sch?utze,2015; He et al, 2015; Hu et al, 2014).Story link detection (SLD) is a similar taskwhich aims to classify whether two news storiesdiscuss the same event.
Farahat et al (2003) lever-age part of speech tagging technique as well astask-specific similarity measures to boost the sys-tem?s performance.
Shah et al (2006) show thatentity based document representation is a betterchoice compared to word-based representationsin SLD.
In our scenario, the query is typically apiece of context sentence instead of an entire arti-cle.
Therefore, we find that document level meth-ods yield sub-optimal performance when used tomodel the similarity of citing context and the arti-cles.
Besides, due to the fact that there might bemultiple reports for a single event, we consider itreasonable to formulate our problem into a rank-ing task instead of classification.6.3 Question RetrievalThe key problem in question retrieval lies in mod-eling questions?
similarity.
Machine translationtechniques (Jeon et al, 2005) and topic models(Duan et al, 2008) have been utilized by previousworks.
An alternative is representation learning.Zhou et al (2015) use category-based meta-data tolearn word embeddings.
dos Santos et al (2015)and Lei et al (2016) obtain superior performanceover hand-crafted features with CNN.News articles are more well-written than mostdocuments in QA communities, which results inthe feasibility of high-quality entity detection andgrounding.7 DiscussionsIn this paper, we propose a novel problem of newscitation recommendation, which aims to recom-mend news citations for references based on a cit-ing context.
We develop a re-ranking system lever-aging implicit and explicit semantics for contentsimilarity.
We construct a real-world dataset.
Theexperimental results show the efficacy of our ap-proach.8 AcknowledgmentsThis research is partially supported by NationalBasic Research Program of China under Grant No.2015CB352201, National Natural Science Foun-dation of China under Grant No.
61502014, andChina Post-doctoral Foundation under Grant No.2015M580927.396ReferencesGediminas Adomavicius and Alexander Tuzhilin.2005.
Toward the next generation of recommendersystems: A survey of the state-of-the-art and pos-sible extensions.
IEEE Trans.
Knowl.
Data Eng.,pages 734?749.Selcuk Aya, Carl Lagoze, and Thorsten Joachims.2005.
Citation classification and its applications.In Proceedings of the International Conference onKnowledge Management, pages 287?298.Toine Bogers and Antal van den Bosch.
2007.
Com-paring and evaluating information retrieval algo-rithms for news recommendation.
In Proceedingsof the 2007 ACM Conference on Recommender Sys-tems, RecSys ?07, pages 141?144.Kurt Bollacker, Colin Evans, Praveen Paritosh, TimSturge, and Jamie Taylor.
2008.
Freebase: A col-laboratively created graph database for structuringhuman knowledge.
In Proceedings of the 2008 ACMSIGMOD International Conference on Managementof Data, pages 1247?1250.Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, andHang Li.
2007.
Learning to rank: From pairwise ap-proach to listwise approach.
In Proceedings of the24th International Conference on Machine Learn-ing, pages 129?136.Ronan Collobert and Jason Weston.
2008.
A unifiedarchitecture for natural language processing: deepneural networks with multitask learning.
In ICML.C?
?cero Nogueira dos Santos, Luciano Barbosa, DashaBogdanova, and Bianca Zadrozny.
2015.
Learn-ing hybrid representations to retrieve semanticallyequivalent questions.
In ACL-IJCNLP 2015, pages694?699.Huizhong Duan, Yunbo Cao, Chin-Yew Lin, and YongYu.
2008.
Searching questions by identifying ques-tion topic and question focus.
In ACL, pages 156?164.Ayman Farahat, Francine Chen, and Thorsten Brants.2003.
Optimizing story link detection is not equiva-lent to optimizing new event detection.
In Proceed-ings of the 41st Annual Meeting on Association forComputational Linguistics - Volume 1, pages 232?239.Juri Ganitkevitch, Benjamin Van Durme, and ChrisCallison-Burch.
2013.
PPDB: the paraphrasedatabase.
In Human Language Technologies: Con-ference of the North American Chapter of the As-sociation of Computational Linguistics, pages 758?764.Hua He, Kevin Gimpel, and Jimmy Lin.
2015.
Multi-perspective sentence similarity modeling with con-volutional neural networks.
In Proceedings of the2015 Conference on Empirical Methods in NaturalLanguage Processing, pages 1576?1586.Baotian Hu, Zhengdong Lu, Hang Li, and QingcaiChen.
2014.
Convolutional neural network archi-tectures for matching natural language sentences.In Advances in Neural Information Processing Sys-tems, pages 2042?2050.Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee.
2005.Finding similar questions in large question and an-swer archives.
In Proceedings of the 14th ACM In-ternational Conference on Information and Knowl-edge Management, pages 84?90.Yangfeng Ji and Jacob Eisenstein.
2013.
Discrimi-native improvements to distributional sentence sim-ilarity.
In Proceedings of the 2013 Conference onEmpirical Methods in Natural Language Process-ing, pages 891?896.Keivan Kianmehr, Shang Gao, Jawad Attari, M. Mush-fiqur Rahman, Kofi Akomeah, Reda Alhajj, JonRokne, and Ken Barker.
2009.
Text summarizationtechniques: Svm versus neural networks.
In Pro-ceedings of the 11th International Conference on In-formation Integration and Web-based ApplicationsServices, pages 487?491.Matt J. Kusner, Yu Sun, Nicholas I. Kolkin, and Kil-ian Q. Weinberger.
2015.
From word embeddingsto document distances.
In ICML, pages 957?966.Yann LeCun, Leon Bottou, Yoshua Bengio, and PatrickHaffner.
1998.
Gradient-based learning applied todocument recognition.
Proceedings of the IEEE.Tao Lei, Hrishikesh Joshi, Regina Barzilay, Tommi S.Jaakkola, Kateryna Tymoshenko, Alessandro Mos-chitti, and Llu?
?s M`arquez i Villodre.
2016.
Semi-supervised question retrieval with gated convolu-tions.
In NAACL HLT 2016.Gang Luo, Xiaojiang Huang, Chin-Yew Lin, and Za-iqing Nie.
2015.
Joint entity recognition and disam-biguation.
In Proceedings of the 2015 Conferenceon Empirical Methods in Natural Language Pro-cessing, EMNLP 2015, Lisbon, Portugal, September17-21, 2015, pages 879?888.Nitin Madnani, Joel R. Tetreault, and MartinChodorow.
2012.
Re-examining machine transla-tion metrics for paraphrase identification.
In HumanLanguage Technologies: Conference of the NorthAmerican Chapter of the Association of Computa-tional Linguistics, pages 182?190.Christopher D. Manning, Mihai Surdeanu, John Bauer,Jenny Finkel, Steven J. Bethard, and David Mc-Closky.
2014.
The Stanford CoreNLP natural lan-guage processing toolkit.
In Association for Compu-tational Linguistics (ACL) System Demonstrations,pages 55?60.Sean M. Mcnee, Istvan Albert, Dan Cosley, PrateepGopalkrishnan, Shyong K. Lam, Al M. Rashid,Joseph A. Konstan, and John Ried.
2002.
On therecommending of citations for research papers.
Pro-ceedings of the 2002 ACM conference on Computersupported cooperative work, pages 116?125.397Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013.
Efficient estimation of word represen-tations in vector space.
CoRR, abs/1301.3781.Tao Qin, Xu-Dong Zhang, Ming-Feng Tsai, De-ShengWang, Tie-Yan Liu, and Hang Li.
2008.
Query-level loss functions for information retrieval.
Inf.Process.
Manage., 44:838?855.Deepak Ramachandran and Eyal Amir.
2007.Bayesian Inverse Reinforcement Learning.
Pro-ceedings of the 20th International Joint Conferenceon Artical Intelligence, 51:2586?2591.Anna Ritchie.
2008.
Citation Context Analysis for In-formation Retrieval.
Ph.D. thesis.Yossi Rubner, Carlo Tomasi, and Leonidas J. Guibas.1998.
A metric for distributions with applications toimage databases.
In Proceedings of the Sixth Inter-national Conference on Computer Vision.Chirag Shah, W. Bruce Croft, and David Jensen.
2006.Representing documents with named entities forstory link detection (sld).
In Proceedings of the 15thACM International Conference on Information andKnowledge Management, pages 868?869.Richard Socher, Eric H. Huang, Jeffrey Pennington,Andrew Y. Ng, and Christopher D. Manning.
2011.Dynamic pooling and unfolding recursive autoen-coders for paraphrase detection.
In Advances inNeural Information Processing Systems, pages 801?809.Trevor Strohman, W. Bruce Croft, and David Jensen.2007.
Recommending citations for academic pa-pers.
In Proceedings of the 30th Annual Interna-tional ACM SIGIR Conference on Research and De-velopment in Information Retrieval, pages 705?706.Jiwei Tan, Xiaojun Wan, and Jianguo Xiao.
2015.Learning to recommend quotes for writing.
In Pro-ceedings of the Twenty-Ninth AAAI Conference onArtificial Intelligence, pages 2453?2459.Jie Tang and Jing Zhang.
2009.
A discriminative ap-proach to topic-based citation recommendation.
InAdvances in Knowledge Discovery and Data Min-ing, volume 5476, pages 572?579.Xuewei Tang, Xiaojun Wan, and Xun Zhang.
2014.Cross-language context-aware citation recommen-dation in scientific articles.
In Proceedings ofthe 37th International ACM SIGIR Conference onResearch &#38; Development in Information Re-trieval, pages 817?826.Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, JunYan, and Qiaozhu Mei.
2015.
Line: Large-scaleinformation network embedding.
In WWW.
ACM.R.
Torres, S.M.
McNee, M. Abel, J.A.
Konstan, andJ.
Riedl.
2004.
Enhancing digital libraries withtechlens.
In Digital Libraries, 2004.
Proceedingsof the 2004 Joint ACM/IEEE Conference on, pages228?236.Stephen Wan, Mark Dras, Robert Dale, and CecileParis.
2006.
Using dependency-based features totake the ?para-farce?
out of paraphrase.
In Pro-ceedings of the Australasian Language TechnologyWorkshop 2006, pages 131?138.Xiaojun Wan.
2007.
A novel document similaritymeasure based on earth mover?s distance.
Inf.
Sci.,pages 3718?3730.Mingxuan Wang, Zhengdong Lu, Hang Li, and QunLiu.
2015.
Syntax-based deep matching of shorttexts.
In IJCAI, pages 1354?1361.John Wieting, Mohit Bansal, Kevin Gimpel, and KarenLivescu.
2015.
From paraphrase database tocompositional paraphrase model and back.
TACL,3:345?358.Fen Xia, Tie-Yan Liu, Jue Wang, Wensheng Zhang, andHang Li.
2008.
Listwise approach to learning torank: Theory and algorithm.
In Proceedings of the25th International Conference on Machine Learn-ing, pages 1192?1199.Wei Xu, Alan Ritter, Chris Callison-Burch, William B.Dolan, and Yangfeng Ji.
2014.
Extracting lexicallydivergent paraphrases from twitter.
TACL, 2:435?448.Yin Yang, Nilesh Bansal, Wisam Dakka, Panagio-tis Ipeirotis, Nick Koudas, and Dimitris Papadias.2009.
Query by document.
In Proceedings ofthe Second ACM International Conference on WebSearch and Data Mining, pages 34?43.Wenpeng Yin and Hinrich Sch?utze.
2015.
Convo-lutional neural network for paraphrase identifica-tion.
In The 2015 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages901?911.Guangyou Zhou, Tingting He, Jun Zhao, and Po Hu.2015.
Learning continuous word embedding withmetadata for question retrieval in community ques-tion answering.
In ACL-IJCNLP 2015, pages 250?259.398
