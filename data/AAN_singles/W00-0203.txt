Evaluation of a Practical Interlinguafor Task-Oriented DialogueLori Levin, Donna Gates, Alon Lavie, Fabio Pianesi,Dorcas Wallace, Taro Watanabe, Monika WoszczynaLanguage Technologies Inst i tute,  Carnegie Mellon Univers i ty  andIRST  ITC,  Trento, I talyInternet:  l s l?cs ,  cmu.
eduAbstractIF (Interchange Format), the interlingua used bythe C-STAR consortium, is a speech-act based in-terlingua for task-oriented ialogue.
IF was de-signed as a practical interlingua that could strikea balance between expressivity and simplicity.
Ifit is too simple, components of meaning will belost and coverage of unseen data will be low.
Onthe other hand, if it is too complex, it cannot beused with a high degree of consistency by collab-orators on different continents.
In this paper, wesuggest methods for evaluating the coverage of IFand the consistency with which it was used in theC-STAR consortium.IntroductionIF (Interchange Format) is an interlingua used bythe C-STAR consortium 1 for task-oriented ia-logues.
Because it is used in five different coun-tries for six different languages, it had to achievea careful balance between being expressive houghand being simple enough to be used consistently.If it was not expressive nough, components ofmeaning would be lost and coverage of unseen datawould be low.
On the other hand, if was not sim-ple enough, different system developers would useit inconsistently and the wrong meanings would betranslated.
IF is described in our previous papers(\[PT98, LGLW98, LLW+\]).For this paper, we have proposed methods forevaluating the coverage of IF and the degree towhich it can be used consistently across C-STARsites.
Coverage was measured by having human IFspecialists annotate unseen data.
Consistency wasmeasured by two means.
The first was inter-coderagreement among IF specialists at Carnegie Mel-lonUniversity and ITC-irst (Centre per la ricercalhttp://www.c-star.org18scientifica e tecnologica).
The second, less directmethod, was a cross-site nd-to-end evaluation ofEnglish-to-Italian translation where the English-to-IF analysis grammars were written at CMU andIF-to-Italian generation was developed at IRST.
Ifthe English and Italian grammar writers did notagree on the meaning of the IF, wrong transla-tions will be produced.
In this way, the cross-siteevaluation can be an indirect indicator of whetherthe CMU and IRST IF specialists agreed on themeaning of IF representations.
For comparison,we also present within-site nd-to-end evaluationsof English-to-German, English-to-Japanese, andEnglish-to-IF-to-English, where all of the analysisand generation grammars were written at CMU.The  In terchange FormatBecause we are working with task-oriented dia-logues, adequate rendering of the speech act in thetarget language often overshadows the need for lit-eral translation of the words.
IF is therefore basedon domain actions (DAs), which consist of onspeech acts plus domain-specific concepts.
An ex-ample of a DA is give-information+price+room(giving information about the price of a room).DAs are composed from 45 general speech acts(e.g., acknowledge, give- information, accept)and about 96 domain-specific oncepts (e.g,pr ice,  temporal, room, f l ight ,  ava i lab i l i ty ) .In addition to the DA, IF representations can con-tain arguments such as room-type, dest inat ion,and price.
There are about 119 argument types.In the following example, the DA consistsof a speaker tag (a: for agent), the speech-act give- information,  and two main concepts,+price and +room.
The DA is followed by a listof arguments: room-type= and price=.
The ar-guments have values that represent-informationfor the type of room double and the cost repre-PercentCumulatlve Percent CountCoverage15.7 15,7 65219.8 4.1 17228.3 3.4 14326.0 2.7 11328.0 2.0 8530.1 2.0 8531,9 1.9 7833.7 1.8 7535.5 1.8 7337.2 1.7 7038.8 1.6 6640.3 l .S 6441.7 1,4 6043.2 1.4 6044.5 1.3 5645.8 1.3 5246.9 1.2 4848.0 1.1 4649.1 1.1 4450.1 1.0 42NA* ;:; 244DAacknowledgeaff i rmthankintroduce-selfgive-lnformation+prtcegreetinggive-lnfor marion+tern poralgive-lnformatlon+numeralgive-in formation+ pr ice+roomrequest-in for matio n+ paymentgive-information + paymentg ive- inform+features+roomgive-in form -t- availabil ity + roomacceptgive-information+personal-datareq-act +reserv+ feat ures+roomreq- verif-give-inforra +numera loffer+helpapologizerequest-inform+personal-datano-tagFigure 1: Coverage of Top20 DAs and No-tag indevelopment datasented with the complex argument price= whichhas its own arguments quantity=, currency= andper-unit=.
This IF representation is neutral be-tween sentences that have different verbs, sub-jects, and objects uch as A double room costs 150dollars a night, The price of  a double room is 150dollars a night, and A double room is 150 dollarsa night.
~AGENT: ''a double room costs $150 a night.
''a:give-information+price+room( room-type=doub le ,price=(quantity=lSO,currency=dollar,per-unit=night)Coverage and D is t r ibut ion  ofDia logue  ActsIn this section, we address the coverage of IF fortask-oriented dialogues about ravel planning.
Wewant to know whether a very simple interlingualike IF can have good coverage.
We are using arather subjective measure of coverage: IF expertshand-tagged unseen data with IF representationsand counted the percentage ofutterances towhichno IF could be assigned.
(When they tagged theunseen data, they were not told that the IF wasbeing tested for coverage.
The tagging was donefor system development purposes.)
Our end-to-end evaluation described in the following sectionscan be taken as a less subjective measure of cov-2When we add anaphora resolution, we will needto know whether a verb (cost) or a noun (price) wasused.
This will be an issue our new project, NESPOLEI(http://nespole.
itc.
it/).PercentCumulative Percent Count Speech ActCoverage30.1 80.1 1250 glve-lnformation45,8 15.7 655 acknowledge57,7 11.9 498 request- lnformation62,7 5,0 209 request-verif ication-give-inform87.6 4.9 203 request-actlon71.7 4.1 172 affirm75,1 3.4 143 thank77,9 2.7 113 introduce-self80.2 2.4 98 offer82,4 2.1 89 accept84.4 2.0 85 greeting85.7 1.3 55 suggest66.8 I .
I  44 apologize87.8 1.0 41 closing88.5 0.8 32 negate.give-information89.2 0.6 27 delay-action89,8 0.6 25 introduce-topic90,2 0.5 19 please-wait90.6 0.4 15 reject91.0 0.4 15 request-suggestlonFigure 2:dataCoverage of speech-acts in developmenterage.
However, the score of an end-to-end evalu-ation encompasses grammar coverage problems aswell as IF coverage problems.The development portion of the coverage x-periment proceeded as follows.
Over a period oftwo years, a database of travel planning dialogueswas collected by C-STAR partners in the U.S.,Italy, and Korea.
The dialogues were role-playingdialogues between a person pretending to be atraveller and a person pretending to be a travelagent.
For the English and Italian dialogues, thetraveller and agent were talking face-to-face in thesame language - -  both speaking English or bothspeaking Italian.
The Korean dialogues were alsorole playing dialogues, but one participant wasspeaking Korean and the other was speaking En-glish.
From these dialogues, only the Korean ut-terances are included in the database.
Each utter-ance in the database is annotated with an Englishtranslation and an IF representation.
Table 1 sum-marizes the amount of data in each language.
TheEnglish, Italian, and Korean data was used for IFdevelopment.The development database contains over 4000dialogue act units, which are covered by a total ofabout 542 distinct DAs (346 agent DAs and 278client DAs).
Figures 1 and 2 show the cumulativecoverage of the top twenty DA's and speech actsin the development data.
Figure 1 also shows thepercentage ofno-tag utterances (the ones we de-cided not to cover) in the development data.
Thefirst column shows the percent of the developmentdata that is covered cumulatively by the DA's orspeech acts from the top of the table to the cur-rent line.
For example, acknowledg e and aff irmtogether account for 19.8 percent of the data.
The19Language(s) Type  of Dialogue Number  of DA UnitsD'evelopment Data:EnglishItalianKorean-EnglishTest Data:Japanese-Englishmonolingualmonolingualbiiingual (only 'Koreanutterances are included)bilingual (Japanese andEnglish utterances areincluded)Table 1: The IF Database269811426069Percent' Cumulat ive Percent Count DACover~,--= - " 4.6 263 no-tag15.6 15.6 ?
- 885 acknowledge20.2 4,6 260 thank23.7 3.5 200 introduce-self27.0 3.4 191 affirm29.7 2.7 153 apologize32.3 2.6 147 greeting34.6 2.3 128 closing36.3 1.7 98 give- information+personal-data38.0 1.7 95 glve-inform ation +t  em poraI39.5 1.6 89 give-in formation +price41.1 1.5 88 please-wait42.5 1.4 82 give-inform+telephone-number43.8 1.3 75 g ive- informat ion+features+room45.0 I .
I  65 request- inform+personal-data46.0 1.0 59 give-in for m ?temp oral-.
{- arrival47.0 1.0 55 accept48.0 l.O 55 give-infor m +avai labi l i ty + room48.9 1.0 55 give-information+price-broom49.8 0.9 50 verify50.7 0.9 49 request-in form +tempora l+arr iva lFigure 3: Coverage of Top 20 DAs and No-tag intest dataPercentCumulat iveCoverage25.6Percent Count DA25.6 1454 give-information41.7 16.1 916 acknowledge53.6 11.9 677 request- information58.2 4.6 260 thank62,0 3.7 213 request-verification-give-inform65.5 3.5 200 introduce-self68.8 3.4 191 a f f i rm72.0 3.2 181 request -act ion74.8 2.8 159 accept77.5 2.7 153 apologize80.1 2.6 147 greet ing82.4 2.3 130 closing84.4 2.1 117 suggest86.3 1.8 104 verlfy-give-information87.9 1.7 94 offer89.5 1.5 88 please-wait90.6 I .
I  65 negate-glve-lnformation91.5 0.9 50 verify92.0 0.5 30 negate92.5 0.5 .
26 request-aff irmatlonFigure 4: Coverage of Top 20 SAs in test datasecond column shows the percent of the develop-ment data covered by each DA or speech act.
Thethird column shows the number of times each DAor speech act occurs in the development data.The evaluation portion of the coverage x-periment was carried out on 124 dialogues (6069dialogue act units) that were collected at ATR,Japan.
One participant in each dialogue wasspeaking Japanese and the other was speaking En-glish.
Both Japanese and English utterances areincluded in the data.
The 124 Japanese-Englishdialogues were not examined closely by system de-velopers during IF development.
After the IF de-sign was finalized and frozen in Summer 1999, theJapanese-English data was tagged with IFs.
Nofurther IF development took place at this pointexcept hat values for arguments were added.
Forexample, Miyako could be added as a hotel name,but no new speech acts, concepts, or argumenttypes could be added.
Sentences were tagged asno-tag if the IF did not cover them.Figures 3 and 4 show the cumulative cover-age of the top twenty DAs and speech acts in theJapanese-English data, including the percent ofno-tag sentences.Notice that the percentage of no-tag waslower in our test data than in our developmentdata.
This is because the role playing instructionsfor the test data were more restrictive than therole playing instructions for the development data.Figures 1 and 3 show that slightly more of the testdata is covered by slightly fewer DAs.Cross-Site Reliability of IFRepresentationsIn this section we attempt o measure how reliablyIF is used by researchers at different sites.
Recallthat one of the design criteria of IF was consis-tency of use by researchers who are separated byoceans.
This criterion limits the complexity of IF.Two measures of consistency are used - inter-coderagreement and a cross-site nd-to-end evaluation.Inter -Coder  Agreement:  Inter-coder agree-ment is a direct measure of consistency among20Percent AgreementSpeech-act 82.14Dialog-act 65.48Concept lists 88.00Argument lists I 85.79Table 2: Inter-coder Agreement between CMUand IRSTC-STAR partners.
We used 84 DA units fromthe Japanese-English data described above.
The84 DA units consisted of some coherent dialoguefragments and and some isolated sentences.
Thedata was coded at CMU and at IRST.
We countedagreement on ~he components ofthe IF separately.Table 2 shows agreement on speech acts, dialogueacts (speech act plus concepts), concepts, and ar-guments.
The results are reported in Table 2 interms of percent agreement.
Further work mightinclude some other calculation of agreement suchas Kappa or precision and recall of the codersagainst each other.
Figure 5 shows a fragment ofa dialogue coded by CMU and IRST.
The codersdisagreed on the IF middle sentence, I'd like a twinroom please.
One coded it as an acceptance of atwin room, the other coded it as a preference fora twin room.Cross-Site Evaluation: As an approximate andindirect measure of consistency, we have comparedintra-site end-to-end evaluation with cross-siteend-to-end evaluation.
An end-to-end evaluationincludes an analyzer, which maps the source lan-guage input into IF and a generator, which mapsIF into target language sentences.
The intra-siteevaluation was carried out on English-German,English-Japanese, and English-IF-English trans-lation.
The English analyzer and the German,Japanese, and English generators were all writ-ten at CMU by IF experts who worked closelywith each other.
The cross-site valuation was car-ried out on English-Italian translation, involvingan English analyzer written at CMU and an Ital-ian generator written at IRST.
The IF experts atCMU and IRST were in occasional contact witheach other by email, and met in person two orthree times between 1997 and 1999.A number of factors contribute to the successof an inter-site valuation, just one of which is thatthe sites used IF consistently with each other.
An-other factor is that the two sites used similar de-velopment data and have approximately the samecoverage.
If the inter-site valuation results areabout as good as the intra-site results, we can con-clude that all factors are handled acceptably, in-cluding consistency of IF usage.
If the inter-siteresults are worse than the intra-site results, con-sistency of IF use or some other factor may beto blame.
Before conducting this evaluation, wealready knew that there was some degree of cross-site consistency in IF usage because we conductedsuccessful inter-continental demos with speechtranslation and video conferencing in Summer1999.
(The demos and some of the press coverageare reported on the C-STAR web site.)
The de-mos included ialogues in English-Italian, English-German, English-Japanese, English-Korean, andEnglish-French.
At a later date, an Italian-Koreandemo was produced with no additional work, thusillustrating the well-cited advantage of an inter-lingual approach in a multi-lingual situation.
Theend-to-end evaluation reported here goes beyondthe demo situation to include data that was un-seen by system developers.Evaluation Data: The Summer 1999 intra-siteevaluation was conducted on about 130 utterancesfrom a CMU user study.
The traveller was playedby a second time user - -  someone who had partici-pated in one previous user study, but had no otherexperience with our MT system.
The travel agentwas played by a system developer.
Both peoplewere speaking English, but they were in differentrooms, and their utterances were paraphrased us-ing IF.
The end-to-end procedure was that (1) anEnglish utterance was spoken and decoded by theJANUS speech recognizer, (2) the output of the rec-ognizer was parsed into an IF representation, and(3) a different English utterance (supposedly withthe same meaning) was generated from the IF rep-resentation.
The speakers had no other means ofcommunication with each other.In order to evaluate English-German andEnglish-Japanese translation, the IFs of the 130test sentences were fed into German and Japanesegeneration components atCMU.
The data used inthe evaluation was unseen by system developersat the time of the evaluation.
For English-Italiantranslation, the IF representations produced bythe English analysis component were sent to IRSTto be generated in Italian.Evaluation Scoring: In order to score the eval-uation, input and output sentences were comparedby bilingual people, or monolingual people in thecase of English-IF-English evaluation.
A score ofok is assigned if the target language utterance iscomprehensible and no components ofmeaning aredeleted, added, or" changed by the translation.
A21We have singles, and t,ins and also Japanese rooms available on the eleventh.CMU a:give-information+availability+room(room-type=(single ~ twin ~ japanese_style), time=mdll)IRST a:give-in2ormation+availability+room(room-type=(single ~ twin & japanese_style), time=mdll)I'd like a twin room, please.CMU c:accept+features+room (room-typeffitwin)IBST c:give-information+preference+features+room (room-type=twin)A twin room is fourteen thousand yen.CMU a:give-information+price+room(room-type=twin, price=(currency=yen, quantity=f4000))IRST a:give-in.formation+price+room(room-type=twin, price=(currency=yen, quantity=f4000))Figure 5: Examples of IF coding from CMU and IRST.oMethod1 Recosnition only2 Transcription3 Recosnition4 Transcription5 Recognition6 Transcription7 Recognition8 Transcription9 Recognition10 Transcription11 RecognitionI OutPut Language II OK+Perfect Perfect Grader I No.
of GradersEn$1ishEnglish.En$1ishJapaneseJapaneseGermanGermanGermanGermanItalianItalian78 % 62 % CMU 374 % 54 % CMU 359 ~ 42 % CMU 3777 % 59 % CMU 262 % 4,5 % CMU 270 %- ..... s9 % CMU "58 % 34 % CMU 267 ~ 43 % IRST 259 % 36 % IRST 273 % 51% IRST .... .
.
661% 42 % IRST 6Figure 6: Translation Grades for English to English, Japanese, German, and Italianscore of perfect is assigned if, in addition to theprevious criteria, the translation is fluent in thetarget language.
A score of bad is assigned if thetarget language sentence is incomprehensible orsome element of meaning has been added, deleted,or changed.
The evaluation procedure isdescribedin detail in \[GLL+96\].
In Figure 6, acceptable isthe sum of per fec t  and ok scores, sFigure 6 shows the results of the intra-siteand inter-site evaluations.
The first row gradesthe speech recognition output against a human-produced transcript of what was said.
This givesus a ceiling for how well we could do if trans-lation were perfect, given speech recognition er-rors.
Rows 2 through 7 show the results of theintra-site evaluation.
All analyzers and genera-tors were written at CMU, and the results weregraded by CMU researchers.
(The German re-sults are a lower than the English and Japaneseresults because a shorter time was spent on gram-mar development.)
Rows 8 and 9 report on CMU'sintra~site valuation of English-German transla~Sin another paper (\[LBL+00\]), we describe a task-based evaluation which focuses on success of commu-nicative goals and how long it takes to achieve them.tion (the same system as in Rows 6 and 7), butthe results were graded by researchers at IRST.Comparing Rows 6 and 7 with Rows 8 and 9, wecan check that CMU and IRST graders were us-ing roughly the same grading criteria: a differenceof up to ten percent among graders is normal inour experience.
Rows 10 and 11 show the resultsof the inter-site English-Italian evaluation.
TheCMU English analyzer produced IF representa-tions which were sent to IRST and were fed intoIRST's Italian generator.
The results were gradedby IRST researchers.Conclusions drawn from the inter-site valuation:Since the inter-site evaluation results are compa-rable to the intra-site results, we conclude that re-searchers at IRST and CMU are using IF at leastas consistently as researchers within CMU.Future PlansIn the next phase of C-STAR, we will cover de-scriptive sentences (e.g., The castle was built inthe thirteenth century and someone was impris-oned in the tower) as well as task-oriented sen-tences.
Descriptive sentences will be represented22in a more traditional frame-based interlingua fo-cusing on lexical meaning and grammatical fea-tures of the sentences.
We are working on disam-biguating literal from task-oriented meanings incontext.
For example That's great could be an ac-ceptance (like I'll take it) (task oriented) or couldjust express appreciation.
Sentences may also con-tain a combination of task oriented (e.g., Can youtell me) and descriptive (how long the castle hasbeen standing) components.\[GLL+96\]\[LBL+O0\]\[LGLW98\]Re ferencesDonna Gates, A. Lavie, L. Levin,A.
Waibel, M. Gavald~, L. Mayfield,M:-Woszczyna, and P. Zhan.
End-to-End Evaluation in JANUS: A Speech-to-Speech Translation System.
In Pro-ceedings of ECAI-96, Budapest, Hun-gary, 1996.Lori Levin, Boris Bartlog, Ari-adna Font Llitjos, Donna Gates, AlonLavie, Dorcas Wallace, Taro Watan-abe, and Monika Woszczyna.
LessonsLearned from a Task-Based Evaluationof Speech-to-Speech MT.
In Proceed-ings of LREC 2000, Athens, Greece,June to appear, 2000.Lori Levin, D. Gates, A. Lavie, andA.
Waibel.
An Interlingua Based onDomain Actions for Machine Transla-tion of Task-Oriented Dialogues.
InProceedings of the International Con-ference on Spoken Language Process-ing (ICSLP'98), Sydney, Australia,1998.\[LLW +\]\[PT98\]Lori Levin, A. Lavie, M. Woszczyna,D.
Gates, M. Gavald~, D. Koll, andA.
Waibel.
The Janus-III TranslationSystem.
Machine Translation.
To ap-pear.Fabio Pianesi and Lucia Tovena.
Us-ing the Interchange Format for Encod-ing Spoken Dialogue.
In Proceedings ofSIG-IL Workshop, 1998.23
