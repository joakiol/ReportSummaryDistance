Natural Language IBonnie Lynn WebberUniversity of PennsylvaniaExcept for the final presentation by Hovy, this session focussed on the use of superficial features ofNatural Language in text processing (messages, in the case of the first two presentations, unrestrictedtext in the case of the second two).
This is a very brief summary of a moderator's view of the action.The first presentation was given by Ralph Grishman (NYU) describing work done by himself andJohn Sterling in preparing for the second ARPA workshop on message understanding (MUCK-2).Up to that point, their approach to message understanding had been to build a very rich semanticsfor a domain, which then could be accessed by their message processor in understanding ellipses,noun-noun complementation a d other implicit relations, etc.
Its very rich semantics gave theirsystem a great deal of power.
However, the restricted time participants were given to adapt theirsystem to a new domain for MUCK-2 did not allow Grishman & Sterling to construct a uniformrich semantics for the whole domain.
In his presentation, Grishman described their use of Wilk'sPreference Semantics, which allowed them, in a short time, to capture some of the semantics of allthe domain, rather than all the semantics of some of the domain, and thereby achieve a greateroverall success in the MUCK-2 challenge.In the second presentation, Jerry Hobbs (SRI International) compared a variety of parse pref-erence strategies that made use of syntactic criteria alone rather than attempting to draw uponsemantic and pragmatic riteria as well.
For each of the preference strategies, Hobbs presented ex-amples that would fail under that strategy.
As one might expect, there was no one purely syntacticstrategy that was found to improve results all around.The next two presentations discussed language statistics and their application to processingunrestricted text.
The first of these was given by Ken Church (AT&T Bell Labs) who, in his alloted15 minutes, presented the results of two separate pieces of work.
He first described experimentscarried out by himself and his colleagues (Gale, Hanks and Hindle) over several millions of wordsof text, to characterize co-occurrence r lations among words in English texts.
He then describedother work by himself and Gale, in which they investigated two different methods of estimating thefrequency of given bi-grams in a test corpus, given a training corpus - one based on a method due toGood and Turing, the other, a purely empirical method they call Categorize-Calibrate or Cat-Cal.Their results led them to advocate the latter in the case of small counts, the former in all othercases.Julian Kupiec (Xerox PARC) then described a stochastic method for assigning part-of-speechcategories to unrestricted text, in a way that eliminates the need for a pre-tagged training corpusand allows some word dependency across phrases.In the final presentation of the session, Ed Hovy (USC-ISI) advocated a new US effort in machinetranslation (MT) that would meld current ransfer and inter-lingua pproaches into a single approachthat would take advantage of recent advances in grammatical theory.
He characterized a two-phase effort that would begin with a single modest MT project, and then move to a few small3-5 person efforts working on limited application domains.
Hovy emphasized that the need forMT (including machine-aided human translation, and human-aided machine translation) has notonly not gone away, but can only increase, given the changes in Europe brought about by theEuropean Community's plans for 1992 and beyond, and the increasing economic inter-dependencyof the Pacific Rim countries.
He ended by asking that people interested in getting this new MTeffort started contact him.69
