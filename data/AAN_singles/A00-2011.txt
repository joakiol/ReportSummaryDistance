Word-for-Word Glossing with Contextually Similar WordsPatrick Pantel and Dekang LinDepartment of Computer ScienceUniversity of ManitobaWinnipeg, Manitoba R3T 2N2 Canada{ppantel, indek} @cs.umanitoba.caAbstractMany corpus-based machine translationsystems require parallel corpora.
In thispaper, we present a word-for-word glossingalgorithm that requires only a sourcelanguage corpus.
To gloss a word, we firstidentify its similar words that occurred inthe same context in a large corpus.
We thendetermine the gloss by maximizing thesimilarity between the set of contextuallysimilar words and the different ranslationsof the word in a bilingual thesaurus.1.
IntroductionWord-for-word glossing is the process ofdirectly translating each word or term in adocument without considering the word order.Automating this process would benefit manyNLP applications.
For example, in cross-language information retrieval, glossing adocument often provides a sufficient ranslationfor humans to comprehend the key concepts.Furthermore, a glossing algorithm can be usedfor lexical selection in a full-fledged machinetranslation (MT) system.Many corpus-based MT systems requireparallel corpora (Brown et al, 1990; Brown etal., 1991; Gale and Church, 1991; Resnik,1999).
Kikui (1999) used a word sensedisambiguation algorithm and a non-paralMbilingual corpus to resolve translationambiguity.In this paper, we present a word-for-wordglossing algorithm that requires only a sourcelanguage corpus.
The intuitive idea behind ouralgorithm is the following.
Suppose w is a wordto be translated.
We first identify a set of wordssimilar to w that occurred in the same context asw in a large corpus.
We then use this set (calledthe contextually similar words of w) to select atranslation for w. For example, the contextuallysimilar words of duty in fiduciary duty includeresponsibility, obligation, role, ...
This list isthen used to select a translation for duty.In the next section, we describe the resourcesrequired by our algorithm.
In Section 3, wepresent an algorithm for constructing thecontextually similar words of a word in acontext.
Section 4 presents the word-for-wordglossing algorithm and Section 5 describes thegroup similarity metric used in our algorithm.
InSection 6, we present some experimental resultsand finally, in Section 7, we conclude with adiscussion of future work.2.
ResourcesThe input to our algorithm includes a collocationdatabase (Lin, 1998b) and a corpus-basedthesaurus (Lin, 1998a), which are both availableon the Interne0.
In addition, we require abilingual thesaurus.
Below, we briefly describethese resources.2.1.
Collocation databaseGiven a word w in a dependency relationship(such as subject or object), the collocationdatabase can be used to retrieve the words thatoccurred in that relationship with w, in a largecorpus, along with their frequencies 2.
Figure 1shows excerpts of the entries in the collocationdatabase for the words corporate, duty, andfiduciary.
The database contains a total of 11million unique dependency relationships.I Available at www.cs.umanitoba.ca/-lindek/depdb.htmand www.cs.umanitoba.ca/-lindek/simdb.htm2 We use the term collocation to refer to a pair ofwords that occur in a dependency relationship (ratherthan the linear proximity of a pair of words).78Table 1.
Clustered similar words of  duty as given by (Lin,1998a).CLUSTER CLUSTERED SIMILAR WORDS OF DUTY(WITH SIMILARITY SCORE)responsibility 0.16, obligation 0.109, task 0.101,function 0.098, role 0.091, post 0.087, position0.086, job 0.084, chore 0.08, mission 0.08,assignment 0.079, liability 0.077 ....tariff0.091, restriction 0.089, tax 0.086,regulation 0.085, requirement 0.081, procedure0.079, penalty 0.079, quota 0.074, rule 0.07, levy0.061 ....fee 0.085, salary 0.081, pay 0.064, fine 0.058personnel 0.073, staff0.073training 0.072, work 0.064, exercise 0.061privilege 0.069, right 0.057, license 0.0562.2.
Corpus-based thesaurusUsing the collocation database, Lin used anunsupervised method to construct a corpus-based thesaurus (Lin, 1998a) consisting of11839 nouns, 3639 verbs and 5658adjectives/adverbs.
Given a word w, thethesaurus returns a clustered list of similar wordsof w along with their similarity to w. Forexample, the clustered similar words of duty areshown in Table 1.2.3.
Bi l ingual thesaurusUsing the corpus-based thesaurus and a bilingualdictionary, we manually constructed a bilingualthesaurus.
The entry for a source language wordw is constructed by manually associating one ormore clusters of similar words of w to eachcandidate translation of w. We refer to theassigned clusters as Words Associated with aTranslation (WAT).
For example, Figure 2shows an excerpt of our Engl ish~Frenchbilingual thesaurus for the words account  andduty.Although the WAT assignment is a manualprocess, it is a considerably easier task thanproviding lexicographic definitions.
Also, weonly require entries for source language wordsthat have multiple translations.
In Section 7, wecorporate:modifier-of:duty:objeet-of:subject-of:adj-modifier:fiduciary:modifier-of:client 196, debt 236, development 179, fee 6,function 16, headquarter 316, IOU 128, levy3, liability 14, manager 203, market 195,obligation 1, personnel 7, profit 595,responsibility 27, rule 7, staff 113, tax 201,training 2, vice president 231 ....assume 177, breach 111, carry out 71, do114, have 257, impose 114, perform 151 ....affect 4, apply 6, include 42, involve 8, keep5, officer 22, protect 8, require 13, ...active 202, additional 46, administrative 44,fiduciary 317, official 66, other 83, ...act 2, behavior I, breach 2, claim I,company 2, duty 317, irresponsibility 2,obligation 32, requirement 1, responsibility89, role 2, ...Figure 1.
Excepts o f  entries in the collocation database forthe words corporate, duty, and fiduciary.account:1. compte:2. rapport:duty:1. devoir:2. taxe:investment, transaction, payment, saving, imoney, contract, Budget, reserve, security,!contribution, debt, property holdingreport, statement, testimony, card, story,record, document, data, information, view,cheek, figure, article, description, estimate,assessment, number, statistic, comment,letter, picture, note, ...responsibility, obligation, task, function,role, post, position, job, chore, mission,assignment, liability .
.
.
.tariff, restriction, tax, regulation,requirement, procedure, penalty, quota, rule,levy, ...WAT forii~::::= ........................Figure 2.
Bilingual thesaurus entries for account and duty.discuss a method for automatically assigning theWATs.3.
Contextual ly Similar WordsThe contextually similar words of a word w arewords similar to the intended meaning of w in itscontext.
Figure 3 gives the data flow diagram forour algorithm for identifying the contextuallysimilar words of w. Data are represented byovals, external resources by double ovals andprocesses by rectangles.By parsing a sentence with Minipar 3, weextract the dependency relationships involvingw.
For each dependency relationship, we retrieve3 Available at www.cs.umanitoba.ca/-lindek/minipar.htm79Input1 IRetrieveIsContextually ~'~ imilar Words,,\]Figure 3.
Data flow diagram for identifying thecontextually similar words of a word in context.from the collocation database the words thatoccurred in the same dependency relationship asw.
We refer to this set of words as the cohort ofw for that dependency relationship.
Consider theword duty in the contexts corporate duty andfiduciary duty.
The cohort of duty in corporateduty consists of nouns modified by corporate inFigure 1 (e.g.
client, debt, development .
.
.
.  )
andthe cohort of duty in fiduciary duty consists ofnouns modified by fiduciary in Figure 1 (e.g.act, behaviour, breach .
.
.
.
).Intersecting the set of similar words and thecohort then forms the set of contextually similarwords of w. For example, Table 2 shows thecontextually similar words of duty in thecontexts corporate duty and fiduciary duty.
Thewords in the first row are retrieved byintersecting the words in Table 1 with the nounsmodified by corporate in Figure 1.
Similarly,the second row represents he intersection of thewords in Table I and the nouns modified byfiduciary in Figure 1.The first set of contextually similar words inTable 2 contains words that are similar to bothTable 2.
The words similar to duty that occurred in thecontexts corporate duty and fiduciary duty.CONTEXT CONTEXTUALLY SIMILAR WORDS OF DUTYcorporate duty fee, function, levy, liability, obligation,personnel, responsibility, rule, staff, tax,trainingobligation, requirement, responsibility, role fiducia~ dutythe responsibility and tax senses of duty,reflecting the fact that the meaning of duty isindeed ambiguous if corporate duty is its solecontext.
In contrast, the second row in Table 2clearly indicates the responsibility sense of duty.While previous word sense disambiguationalgorithms rely on a lexicon to provide senseinventories of words, the contextually similarwords provide a way of distinguishing betweendifferent senses of words without committing toany particular sense inventory.4.
Overview of the Word-for-WordGlossing AlgorithmFigure 4 illustrates the data flow of the word-for-word glossing algorithm and Figure 5describes it.For example, suppose we wish to translateinto French the word duty in the contextcorporate fiduciary duty.
Step 1 retrieves thecandidate translations for duty and its WATsfrom Figure 2.
In Step 2, we construct two listsof contextually similar words, one for thedependency context corporate duty and one forthe dependency context fiduciary duty, shown inTable 2.
The proposed translation for the contextis obtained by maximizing the group similaritiesbetween the lists of contextually similar wordsand the WATs.Using the group similarity measure fromSection 5, Table 3 lists the group similarityscores between each list of contextually similarwords and each WAT as well as the finalcombined score for each candidate translation.The combined score for a candidate is the sumof the logs of all group similarity scoresinvolving its WAT.
The correct proposedtranslation for duty in this context is devoir sinceits WAT received the highest score.80InputI Step 1 Step 2 getWATs getCSWLists(  w?,?slStep 3 groupSim IlMatrix of ~%~I Step4 I ?ornbineScoresTranslation )Figure 4.
Data flow diagram for the word-for-wordglossing algorithm.Table 3.
Group similarity scores between the contextuallysimilar words of duty in corporate duty and fiduciary dutywith the WATs for candidate translations devoir and taxe.CANDIDATE CANDIDATEDEVOIR TAXEcorporate duty 60.3704 16.569fiduciary duty 51.2960 4.8325Combined Score 8.0381 4.3829Figure 6.
An example illustrating the difference betweenthe interconnectivity and closeness measures.
Theinterconnectivity in (a) and (b) remains constant while thecloseness in (a) is higher than in (b) since there are morezero similarity pairs in (b).Input:Step 1:Step 2:Step 3:Step 4:Output:A word w to be translated and a set ofdependency contexts involving w.Retrieve the candidate translations ofw andthe corresponding WATs from the bilingualthesaurus.Find the contextually similar words of w ineach dependency context using the algorithmfrom Section 3.Compute the group similarity (see details inSection 5) between each set of contextuallysimilar words and each WAT; the results arestored in a matrix t, where t\[i,j\] is the groupsimilarity between the ?h list of contextuallysimilar words and thef  h WAT.Add the logs of the group similarity scores incolumn of t  to obtain a score for each WAT.The candidate translation corresponding tothe WAT with the highest score.Figure 5.
The word-for-word glossing algorithm.5.
Group SimilarityThe corpus-based thesaurus contains only thesimilarities between individual pairs of words.
Inour algorithm, we require the similarity betweengroups of  words.
The group similarity measurewe use is proposed by Karypis et al (1999).
Ittakes as input two groups of  elements, Gl andG2, and a similarity matrix, sim, which specifiesthe similarity between individual elements.
GIand G2 are describable by graphs where thevertices are the words and each weighted edgebetween vertices wl and w2 represents thesimilarity, sim(wl, w2), between the words wland Wz.Karypis et al consider both theinterconnectivity and the closeness of thegroups.
The absolute interconnectivity betweenG t and G 2, AI(G t, G2), is defined as the aggregatesimilarity between the two groups:x~Gi YEG2The absolute closeness between G~ and G2,AC(G~, G2), is defined as the average similaritybetween a pair of elements, one from eachgroup:Ic, lc l81Table 4.
Candidate translations for each testing word along with their frequency of occurrencein the test corpus.WORD CANDIDATE ENGLISH SENSE FREQUENCY OFTRANSLATION OCCURRENCEaccount compte bank account, business 245rapport report, statement 55duty devoir responsibility, obligation 80taxe tax 30race course contest 87race racial group 23suit proems lawsuit 281costume garment 17check ch6que draft, bank order 105contr61e evaluation, verification 25record record unsurpassed statistic/performance 98enregistremen t recorded ata or documentation 12The difference between the absoluteinterconnectivity and the absolute closeness isthat the latter takes zero similarity pairs intoaccount.
In Figure 6, the interconnectivity n (a)and (b) remains constant.
However, thecloseness in (a) is higher than in (b) since thereare more zero similarity pairs in (b).Karypis et al normalized the absoluteinterconnectivity and closeness by the internalinterconnectivity and closeness of the individualgroups.
The normalized measures are referred toas relative interconnectivity, RI(GI, G2), andrelative closeness, RC(GI, G2).
The internalinterconnectivity and closeness are obtained byfirst computing a minimal edge bisection ofeach group.
An even-sized partition {G', G"} ofa group G is called a minimal edge bisection ofG if AI(G', G") is minimal among all suchpartitions.
The internal interconnectivity of G,II(G), is defined as II(G) = AI(G', G") and theinternal closeness of G, IC(G), as IC(G) =AC(G', G").Minimal edge bisection is performed for allWATs and all sets of contextually similar words.However, the minimal edge bisection problem isNP-complete (Garey and Johnson, 1979).Fortunately, state of the art graph partitioningalgorithms can approximate these bisections inpolynomial time (Goehring and Saad, 1994;Karypis and Kumar, 1999; Kernighan and Lin,1970).
We used the same approximationmethods as in (Karypis et al, 1999).The similarity between G1 and G2 is thendefined as follows:groupSim(G,, G2)= R/(G,, G2)?
RC(G,, G 2 )where2AI(G,,G2)xI(G,)+ II(G )is the relative interconnectivity andRC(G,,G2)= AC(G,,G )IG'I IC(G,)4 IG2I IC(G2)IG, I+IG=I IG, I?IG21is the relative closeness.6.
Experimental ResultsThe design of our glossing algorithm isapplicable to any source/destination languagepair as long as a source language parser isavailable.
We considered English-to-Frenchtranslations in our experiments.We experimented with six English nouns thathave multiple French translations: account, duty,race, suit, check, and record.
Using the 1987Wall Street Journal files on the LDC/DCI CD-8RROM, we extracted a testing corpus 4consistingof the first 100 to 300 sentences containing thenon-idiomatic usage of the six nouns .
Then, wemanually tagged each sentence with one of thecandidate translations shown in Table 4.Each noun in Table 4 translates morefrequently to one candidate translation than theother.
In fact, always choosing the candidateprocbs as the translation for suit yields 94%accuracy.
A better measure for evaluating thesystem's classifications considers both thealgorithm's precision and recall on eachcandidate translation.
Table 5 illustrates theprecision and recall of our glossing algorithm foreach candidate translation.
Albeit precision andrecall are used to evaluate the quality of theclassifications, overall accuracy is sufficient forcomparing different approaches with our system.In Section 3, we presented an algorithm foridentifying the contextually similar words of aword in a context using a corpus-based thesaurusand a collocation database.
Each of the six nounshas similar words in the corpus-based thesaurus.However, in order to find contextually similarwords, at least one similar word for each nounmust occur in the collocation database in a givencontext.
Thus, the algorithm for constructingcontextually similar words is dependent on thecoverage of the collocation database.
Weestimated this coverage by counting the numberof times each of the six nouns, in severaldifferent contexts, has at least one contextuallysimilar word.
The result is shown in Table 6.In Section 5, we described a group similaritymetric, groupSim, which we use for comparing aWAT with a set of contextually similar words.In Figure 7, we compare the translation accuracyof our algorithm using other group similaritymetrics.
Suppose G~ and (/2 are two groups ofwords and w is the word that we wish totranslate.
The metrics used are:I. closest&sum of similarity of the three closestpairs of words from each group.4 Available at fip.cs.umanitoba.ca/pub/ppantei/download/wfwgtest.zip5 Omitted idiomatic phrases include take intoaccount, keep in check, check out, ...Table 5.
Precision vs. Recall for each candidate translation.WORD CANDIDATE PRECISION RECALLaccount compte 0.982 0.902rapport 0.680 0.927duty devoir 0.951 0.963taxe 0.897 0.867race course 0.945 0.989race 0.947 0.783suit proc6s 0.996 0.993costume 0.889 0.941check ch6que 0.951 0.924contr61e 0.714 0.800record record 0.968 0.918enregistrement 0.529 0.750Table 6.
The coverage of the collocation database, shownby the frequency with which a word in a given context hasat least one contextually similar word.WORD NUMBER OF COVERAGECONTEXTSaccount 1074 95.7%duty 343 93.3%race 294 92.5%suit 332 91.9%check 2519 87.5%record 1655 92.8%2.
gs:Z sim(x, w )x max sire(x, y)+ Z sire(y, w)x max sire(y, x)Z.,'im{x, w)+ Z ~im@, ~)3. dC:as defined in Section 5.4.
AI:as defined in Section 5.5.
RC:as defined in Section 5.6.
RI:as defined in Section 5.83Group Similarity Comparison01.01 .... ..... 'i ~ ~R o 7 ?
i, 06 .50.4 ; ~ I ;02 ;~0oi}il;;i  iii ijii !ii!!
i}ii iii iiiiiFigure 7.
Performance comparison of different group similarity metrics.In mostFrequent, we include the resultsobtained if we always choose the translation thatoccurs most frequently in the testing corpus.We also compared the accuracy of ourglossing algorithm with Systran's translationsystem by feeding the testing sentences intoSystran's web interface 6 and manuallyexamining the results.
Figure 8 summarizes theoverall accuracy obtained by each system andthe baseline on the testing corpus.
Systrantended to prefer one candidate translation overthe other and committed the majority of itserrors on the non-preferred senses.Consequently, Systran is very accurate if itspreferred sense is the frequent sense (as inaccount and duty) but is very inaccurate if itspreferred sense is the infrequent one (as in race,suit, and check).7.
Conclusion and Future WorkThis paper presents a word-for-word glossingalgorithm.
The gloss of a word is determined bymaximizing the similarity between the set ofcontextually similar words and the differenttranslations of the word in a bilingual thesaurus.6 Available at babelfish.altavista.com/cgi-bin/translateThe algorithm presented in this paper can beimproved and extended in many ways.
Atpresent, our glossing algorithm does not take theprior probabilities of translations into account.For example, in WSJ, the bank account sense ofaccount is much more common than the reportsense.
We should thus tend to prefer this senseof account.
This is achievable by weighting thetranslation scores by the prior probabilities ofthe translations.
We are investigating anExpectation-Maximization (EM) (Dempster etal., 1977) algorithm to learn these priorprobabilities.
Initially, we assume that thecandidate translations for a word are uniformlydistributed.
After glossing each word in a largecorpus, we refine the prior probabilities usingthe frequency counts obtained.
This process isrepeated several times until the empirical priorprobabilities closely approximate the true priorprobabilities.Finally, as discussed in Section 2.3,automatically constructing the bilingualthesaurus is necessary to gloss wholedocuments.
This is attainable by adding acorpus-based destination language thesaurus toour system.
The process of assigning a cluster ofsimilar words as a WAT to a candidatetranslation c is as follows.
First, we841.00.90.80.70.60.50.40.30.20.10.0\[\] mostFrequent?
Systran\[\]  G oss ngWord-for-Word Glossing vs. SystranZ\]ccour0,816;0.856;0.906;[ t~ mostFrequent ?
Systran\[\] GlossingFigure 8.
Performance omparison fthe word-for-word glossing algorithm and Systran.automatically obtain the candidate translationsfor a word using a bilingual dictionary.
With thedestination language thesaurus, we obtain a list Sof all words similar to c. With the bilingualdictionary, replace each word in S by its sourcelanguage translations.
Using the group similaritymetric from Section 5, assign as the WAT thecluster of similar words (obtained from thesource language thesaurus) most similar to S.AcknowledgementsThe authors wish to thank the reviewers for theirhelpful comments.
This research was partlysupported by Natural Sciences and EngineeringResearch Council of Canada grants OGP 121338and PGSA207797.ReferencesPeter F. Brown; John Cocke; Stephen A. Della Pietra;Vincent J. Della Pietra; Fredrick Jelinek; John D.Lafferty; Robert L. Mercer and Paul S. Roossin.1990.
A Statistical Approach to MachineTranslation.
Computation Linguistics, 16(2).Peter F. Brown; Jennifer C. Lai and Robert L.Mercer.
1991.
Aligning Sentences in ParallelCorpora.
In Proceedings ofACL91.
Berkeley.A.
P. Dempster; N. M. Laird; & D. B. Rubin.
1977.Maximum likelihood from incomplete data via theEM algorithm.
Journal of the Royal StatisticalSociety, Series B, 39(I).W.
A. Gale and K. W. Church.
1991.
A Program forAligning Sentences in Bilingual Corpora.
InProceedings of ACL91.
Berkeley.M.
R. Garey and D. S. Johnson.
1979.
Computersand Intractability: A Guide to the Theory of NP-Completeness.
W H. Freeman.T.
Goehring and Y. Saad.
1994.
Heuristic Algorithmsfor Automatic Graph Partitioning.
TechnicalReport.
Department of Computer Science,University of Minnesota.George Karypis and Vipin Kumar.
1999.
A Fast andHigh Quality Multilevel Scheme for PartitioningIrregular Graphs.
SIAM Journal on ScientificComputing, 20(1 ).George Karypis; Eui-Hong Han and Vipin Kumar.1999.
Chameleon: A Hierarchical ClusteringAlgorithm Using Dynamic Modeling.
IEEEComputer: Special Issue on Data Analysis andMining, 32(8).
http:l/www-users.cs.umn.edu/-karypis/publications/Papers/PDF/chameleon.pdfB.
W. Kernighan and S. Lin.
1970.
An EfficientHeuristic Procedure for Partitioning Graphs.
TheBell System Technical Journal.Genichiro Kikui.
1999.
Resolving Translationambiguity using Non-parallel Bilingual Corpora.In Proceedings of ACL99 Workshop onUnsupervised Learning in Natural LanguageProcessing.Dekang Lin.
1998a.
Automatic Retrieval andClustering of Similar Wordv.
In Proceedings ofCOLING-ACL98.
Montreal, Canada.Dekang Lin.
1998b.
Extracting Collocations fromText Corpora.
Workshop on ComputationalTerminology.
Montreal, Canada.Philip Resnik.
1999.
Mining the Web for BilingualText.
In Proceedings of ACL99.
College Park,Maryland.85
