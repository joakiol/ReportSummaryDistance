Coling 2010: Poster Volume, pages 837?845,Beijing, August 2010Machine Translation with Lattices and ForestsHaitao Mi??
Liang Huang?
Qun Liu?
?Key Lab.
of Intelligent Information Processing ?Information Sciences InstituteInstitute of Computing Technology Viterbi School of EngineeringChinese Academy of Sciences University of Southern California{htmi,liuqun}@ict.ac.cn {lhuang,haitaomi}@isi.eduAbstractTraditional 1-best translation pipelinessuffer a major drawback: the errors of 1-best outputs, inevitably introduced by eachmodule, will propagate and accumulatealong the pipeline.
In order to alleviatethis problem, we use compact structures,lattice and forest, in each module insteadof 1-best results.
We integrate both lat-tice and forest into a single tree-to-stringsystem, and explore the algorithms of lat-tice parsing, lattice-forest-based rule ex-traction and decoding.
More importantly,our model takes into account all the proba-bilities of different steps, such as segmen-tation, parsing, and translation.
The mainadvantage of our model is that we canmake global decision to search for the bestsegmentation, parse-tree and translation inone step.
Medium-scale experiments showan improvement of +0.9 BLEU points overa state-of-the-art forest-based baseline.1 IntroductionStatistical machine translation (SMT) has wit-nessed promising progress in recent years.
Typi-cally, conventional SMT is characterized as a 1-best pipeline system (Figure 1(a)), whose mod-ules are independent of each other and only takeas input 1-best results from the previous module.Though this assumption is convenient to reducethe complexity of SMT systems.
It also bring amajor drawback of error propagation.
The errorsof 1-best outputs, introduced inevitably in eachphase, will propagate and accumulate along thepipeline.
Not recoverable in the final decoding(b)source segmentation latticeparse forest targetsource 1-best segmentation1-best tree target(a)Figure 1: The pipeline of tree-based system: (a) 1-best (b) lattice-forest.step.
These errors will severely hurt the translationquality.
For example, if the accuracy of each mod-ule is 90%, the final accuracy will drop to 73%after three separate phases.To alleviate this problem, an obvious solutionis to widen the pipeline with k-best lists ratherthan 1-best results.
For example Venugopal etal.
(2008) use k-best alignments and parses in thetraining phase.
However, with limited scope andtoo many redundancies, it is inefficient to searchseparately on each of these similar lists (Huang,2008).Another efficient method is to use compact datastructures instead of k-best lists.
A lattice or forest,compactly encoded exponentially many deriva-tions, have proven to be a promising technique.For example, Mi and Huang (2008), Mi et al(2008), Liu et al (2009) and Zhang et al (2009)use forests in rule extraction and decoding phasesto extract more general rules and weaken the influ-ence of parsing errors; Dyer et al (2008) use wordlattice in Chinese word segmentation and Arabicmorphological variation phases to weaken the in-fluence of segmentation errors; Huang (2008) and8370 1 2 3 4 5 6 7 8 9c0:Bu` c1:sh??
c2:yu?
c3:Sha?
c4:lo?ng c5:ju?
c6:x?
?ng c7:ta?o c8:lu`n(0, 2, NR) (2, 3, CC) (3, 5, NR) (5, 6, VV) (6, 8, NN) (8, 9, NN)(5, 7, VV) (7, 9, NN)(2, 3, P)Figure 2: The lattice of the example:?
Bu` sh??
yu?
Sha?
lo?ng ju?
x?
?ng ta?o lu`n.?
The solid lines show the 1-bestresult, which is wrong.Jiang et al (2008b) stress the problems in re-ranking phase.
Both lattices and forests have be-come popular in machine translation literature.However, to the best of our knowledge, previouswork only focused on one module at a time.
In thispaper, we investigate the combination of latticeand forest (Section 2), as shown in Figure 1(b).We explore the algorithms of lattice parsing (Sec-tion 3.2), rule extraction (Section 4) and decod-ing (Section 5).
More importantly, in the decodingstep, our model can search among not only moreparse-trees but also more segmentations encodedin the lattice-forests and can take into account allthe probabilities of segmentations and parse-trees.In other words, our model postpones the disambi-guition of segmentation and parsing into the finaltranslation step, so that we can do global searchfor the best segmentation, parse-tree and transla-tion in one step.
When we integrate a lattice intoa forest system, medium-scale experiments (Sec-tion 6) show another improvement of +0.9 BLEUpoints over a state-of-the-art forest-based system.2 Compact StructuresA word lattice (Figure 2) is a compact representa-tion of all the possible of segmentations and POStags, while a parse forest (Figure 5) is a compactrepresentation of all parse trees.2.1 Word LatticeFor a given input sentence C = c0..cn?1, whereci denotes a character at position i, and n is thelength of the sentence.A word lattice (Figure 2), or lattice in short, isa set of edges L, where each edge is in the formof (i, j, X), which denotes a word of tag X , cov-ering characters ci through cj?1.
For example, inFigure 2, (7, 9, NN) is a noun ?ta?olu`n?
of two char-acters.The lattice in Figure 2 shows result of the ex-ample:?
Bu` sh??
yu?
Sha?
lo?ng ju?
x?
?ng ta?o lu`n ?.One ambiguity comes from the POS tag of word?yu??
(preposition (P) or conjunction (CC)).
Theother one is the segmentation ambiguity of the lastfour characters, we can segment into either ?ju?x?
?ngta?o lu`n?
(solid lines), which means lift, beg-ging and argument separately for each word or?ju?x?
?ng ta?olu`n?
(dashed lines), which means holda discussion.lift begging argument5 ju?
6 x?
?ng 7 ta?o 8 lu`n 9hold a discussionThe solid lines above (and also in Figure 2)show the 1-best result, which is obviously wrong.If we feed it into the next modules in the SMTpipeline, parsing and translation will be becomemuch more difficult, since the segmentation is notrecoverable.
So it is necessary to postpone er-ror segmentation decisions to the final translationstep.2.2 Parse ForestIn parsing scenario, a parse forest (Figrure 5), orforest for short, can be formalized as a hyper-graph H , a pair ?V, E?, where node v ?
V is inthe form of Xi,j , which denotes the recognition ofnonterminal X spanning the substring ci:j?1 frompositions ci through cj?1.
Each hyperedge e ?
Eis a pair ?tails(e), head(e)?, where head(e) ?
Vis the consequent node in an instantiated deduc-tive step, and tails(e) ?
(V )?
is the list of an-tecedent nodes.For the following deduction:NR0,2 CC2,3 NR3,5NP0,5 (*)838its hyperedge e?
is notated:?
(NR0,2, CC2,3, NR3,5), NP0,5?.wherehead(e?)
= {NP0,5}, andtails(e?)
= {NR0,2,CC2,3,NR3,5}.We also denote IN (v) to be the set of incominghyperedges of node v, which represents the dif-ferent ways of deriving v. For simplicity, we onlyshow a tree in Figure 5(a) over 1-best segmenta-tion and POS tagging result in Figure 2.
So theIN (NP0,5) is {e?
}.3 Lattice ParsingIn this section, we first briefly review the con-ventional CYK parsing, and then extend to latticeparsing.
More importantly, we propose a more ef-ficient parsing paradigm in Section 3.3.3.1 Conventional ParsingThe conventional CYK parsing algorithm in Fig-ure 3(a) usually takes as input a single sequence ofwords, so the CYK cells are organized over words.This algorithm consists of two steps: initializationand parsing.
The first step is to initialize the CYKcells, whose span size is one, with POS tags pro-duced by a POS tagger or defined by the inputstring1.
For example, the top line in Figure 3(a)is initialized with a series of POS tags in 1-bestsegmentation.
The second step is to search for thebest syntactic tree under a context-free grammar.For example, the tree composed by the solid linesin Figure 5(a) shows the parsing tree for the 1-bestsegmentation and POS tagging results.3.2 Lattice ParsingThe main differences of our lattice parsing in Fig-ure 3(b) from conventional approach are listed infollowing: First, the CYK cells are organized overcharacters rather than words.
Second, in the ini-tialization step, we only initialize the cells withall edges L in the lattice.
Take the edge (7, 9,NN) in Figure 2 for example, the correspondingcell should be (7, 9), then we add a leaf nodev = NN7,9 with a word ta?olu`n.
The final initial-ization is shown in Figure 3(b), which shows that1For simplicity, we assume the input of a parser is a seg-mentation and POS tagging result0 Bu` 1 sh??
2 yu?
3Sha?
4lo?ng 5 ju?
6x?
?ng 7ta?o 8 lu`n 9NR CC NR VV NN NNNP VPBIPO(n3w)(a): Parsing over 1-best segmentation0 Bu` 1 sh??
2 yu?
3Sha?
4lo?ng 5 ju?
6x?
?ng 7ta?o 8 lu`n 9NRCC,PNRVVVV NN NNNNNP VPBIPPPVPO(n3)(b): Parsing over characters0 Bu` 1 sh??
2 yu?
3Sha?
4lo?ng 5 ju?
6x?
?ng 7ta?o 8 lu`n 9NR CC,P NR VVVV NN NNNNNP VPBIPPPVPO(n3r)(c): Parsing over most-refined segmentationFigure 3: CKY parsing charts (a): Conventionalparsing over 1-best segmentation.
(b): Latticeparsing over characters of input sentence.
(c): Lat-tice parsing over most-refined segmentation of lat-tice.
nw and nr denotes the number of tokens overthe 1-best segmentation and the most-refined seg-menation respectively, and nw ?
nr ?
n.lattice parsing can initialize the cells, whose spansize is larger than one.
Third, in the deduction stepof the parsing algorithm i, j, k are the indexes be-tween characters rather than words.We formalize our lattice parser as a deductiveproof system (Shieber et al, 1994) in Figure 4.Following the definitions of the previous Sec-839tion, given a set of edges L of a lattice for an in-put sentence C = c0..cn?1 and a PCFG grammar:a 4-tuple ?N, ?, P, S?, where N is a set of non-terminals, ?
is a set of terminal symbols, P is aset of inference rules, each of which is in the formof X ?
?
: p for X ?
N , ?
?
(N ?
?)?
and p isthe probability, and S ?
N is the start symbol.
Thedeductive proof system (Figure 4) consists of ax-ioms, goals and inference rules.
The axioms areconverted by edges in L. Take the (5, 7, NN) as-sociated with a weight p1 for example, the corre-sponding axiom is NN ?
ta?olu`n : p1.
All axiomsconverted from the lattice are shown in Figure 3(b)exclude the italic non-terminals.
Please note thatall the probabilities of the edges L in a lattice aretaken into account in the parsing step.
The goalsare the recognition X0,n ?
S of the whole sen-tence.
The inference rules are the deductions inparsing.
Take the deduction (*) for example, it willprove a new item NP0,5 (italic NP in Figure 3(b))and generate a new hyper-edge e?
(in Figure 5(b)).So the parsing algorithm starts with the axioms,and then applies the inference rules to prove newitems until a goal item is proved.
The final wholeforest for the input lattice (Figure 2) is shown inFigure 5(b).
The extra hyper-edges of lattice-forestare highlighted with dashed lines, which can in-ference the input sentence correctly.
For example:?yu??
is tagged into P rather than CC.3.3 Faster Parsing with Most-refined LatticeHowever, our statistics show that the average num-ber of characters n in a sentence is 1.6 times thanthe number of words nw in its 1-best segmenta-tion.
As a result, the parsing time over the charac-ters will grow more than 4 times than parsing overthe 1-best segmentation, since the time complexityis O(n3).
In order to alleviate this problem, we re-duce the parsing time by using most-refined seg-mentation for a lattice, whose number of tokensis nr and has the property nw ?
nr ?
n.Given a lattice with its edges L over indexes(0, .., n), a index i is a split point, if and only ifthere exists some edge (i, j, X) ?
L or (k, i, X) ?L.
The most-refined segmentation, or ms forshort, is the segmentation result by using all splitpoints in a lattice.
For example, the correspondingms of the example is ?Bu`sh??
yu?
Sha?lo?ng ju?
x?
?ngta?o lu`n?
since points 1 and 4 are not split points.Item form: Xi,jAxioms: Xi,j : p(i, j, X)(i, j, X) ?
LInfer.
rules:Xi,k : p1 Yk,j : p2Zi,j : pp1p2Z ?
XY : p ?
PGoals: X0,nFigure 4: Lattice parsing as deductive proof sys-tem.
The i, j, k are the indexes between characters.Figure 3(c) shows the CKY parsing cells overmost-refined segmentation, the average numberof tokens nr is reduced by combining columns,which are shown with red dashed boxes.
As a re-sult, the search space is reduced without losing anyderivations.
Theoretically, the parsing over fs willspeed up in O((n/nr)3).
And our experiments inSection 6 show the efficiency of our new approach.It turns out that the parsing algorithm developedin lattice-parsing Section 3.2 can be used herewithout any change.
The non-terminals inductedare also shown in Figure 3(c) in italic style.4 Rule Extraction with Lattice & ForestWe now explore the extraction algorithm fromaligned source lattice-forest and target string2,which is a tuple ?F, ?, a?
in Figure 5(b).
FollowingMi and Huang (2008), we extract minimal rulesfrom a lattice-forest also in two steps:(1) frontier set computation(2) fragmentationFollowing the algorithms developed by Mi andHuang (2008) in Algorithm 1, all the nodes infrontier set (fs) are highlighted with gray in Fig-ure 5(b).Our process of fragmentation (lines 1- 13) isto visit each frontier node v and initial a queue(open) of growing fragments with a pair of emptyfragment and node v (line 3).
Each fragment is as-sociated with a list of expansion sites (front) being2For simplicity and consistency, we use character-basedlattice-forest for the running example.
The ?Bu`?
and ?sh??
?are aligned to the same word ?Bush?.
In our experiment,we use most-refined segmentation to run lattice-parsing andword alignment.840IP0,9NP0,5 VPB5,9(a)0 1 2 3 4 5 6 7 8 9.Bu` .sh??
.yu?
.Sha?
.lo?ng .ju?
.x?
?ng .ta?o .lu`n.NR0,2 .CC2,3 .NR3,5 .VV5,6 .NN6,8 .NN8,9e?IP0,9NP0,5 VP2,9PP2,5 VPB5,9(b)0 1 2 3 4 5 6 7 8 9.Bu` .sh??
.yu?
.Sha?
.lo?ng .ju?
.x?
?ng .ta?o .lu`n.
NR0,2 .
CC2,3 .
NR3,5 .VV5,6 .NN6,8 .NN8,9.
VV5,7 .
NN7,9.
P2,3e?Bush held a discussion with SharonForest only (Minimal rules) Lattice & forest (Extra minimal rules)(c)IP(NP(x1:NR x2:CC x3:NR) x4:VPB) IP(x1:NR x2:VP) ?
x1 x2?
x1 x4 x2 x3 VP(x1:PP x2:VPB) ?
x2 x1CC(yu?)
?with PP(x1:P x2:NR) ?
x1 x2NR(Sha?lo?ng) ?Sharon P(yu?)
?withNR(Bu`sh??)
?Bush VPB(x1:VV x2:NN) ?
x1 x2VPB(VV(ju?)
NN(x?
?ngta?o) NN(lu`n)) VV(ju?x?
?ng) ?held?held a discussion NN(ta?olu`n) ?a discussionFigure 5: (a): The parse forest over the 1-best segmentation and POS tagging result.
(b): Word-alignedtuple ?F, ?, a?
: the lattice-forest F , the target string ?
and the word alingment a.
The solid hyperedgesform the forest in (a).
The dashed hyperedges are the extra hyperedges introduced by the lattice-forest.
(c): The minimal rules extracted on forest-only (left column), and the extra minimal rules extracted onlattice-forest (right column).the subset of leaf nodes of the current fragmentthat are not in the fs except for the initial nodev.
Then we keep expanding fragments in open infollowing way.
If current fragment is complete,whose expansion sites is empty, we extract rulecorresponding to the fragment and its target string841Code 1 Rule Extraction (Mi and Huang, 2008).Input: lattice-forest F , target sentence ?
, andalignment aOutput: minimal rule set R1: fs ?
FROSET(F, ?, a)  frontier set2: for each v ?
fs do3: open ?
{?
?, {v}?}
 initial queue4: while open 6= ?
do5: ?frag , front?
?
open.pop()6: if front = ?
then  finished?7: generate a rule r using frag8: R.append(r)9: else  incomplete: further expand10: u ?
front .pop()  expand frontier11: for each e ?
IN (u) do12: f ?
front ?
(tails(e) \ fs)13: open .append(?frag ?
{e}, f ?
)(line 7) .
Otherwise we pop one expansion nodeu to grow and spin-off new fragments by IN (u),adding new expansion sites (lines 11- 13), until allactive fragments are complete and open queue isempty.The extra minimal rules extracted on lattice-forest are listed at the right bottom of Figure 5(c).Compared with the forest-only approach, we canextract smaller and more general rules.After we get al the minimal rules, we com-pose two or more minimal rules into composedrules (Galley et al, 2006), which will be used inour experiments.For each rule r extracted, we also assign a frac-tional count which is computed by using inside-outside probabilities:c(r) =?
(root(r)) ?
P(lhs(r)) ?
Qv?yield(root(r)) ?(v)?
(TOP) ,(1)where root(r) is the root of the rule, lhs(r) isthe left-hand-side of rule, rhs(r) is the right-hand-side of rule, P(lhs(r)) is the product ofall probabilities of hyperedges involved in lhs(r),yield(root(r)) is the leave nodes, TOP is the rootnode of the forest, ?
(v) and ?
(v) are outside andinside probabilities, respectively.Then we compute three conditional probabili-ties for each rule:P(r | lhs(r)) = c(r)?r?:lhs(r?
)=lhs(r) c(r?
)(2)P(r | rhs(r)) = c(r)?r?:rhs(r?
)=rhs(r) c(r?
)(3)P(r | root(r)) = c(r)?r?:root(r?
)=root(r) c(r?).
(4)All these probabilities are used in decoding step(Section 5).
For more detail, we refer to the algo-rithms of Mi and Huang (2008).5 Decoding with Lattice & ForestGiven a source-side lattice-forest F , our decodersearches for the best derivation d?
among the set ofall possible derivation D, each of which convertsa tree in lattice-forest into a target string ?
:d?
= argmaxd?D,T?FP (d|T )?0 ?
e?1|d|?
LM(?
(d))?2 ?
e?3|?
(d)|,(5)where |d| is the penalty term on the number ofrules in a derivation, LM(?
(d)) is the languagemodel and e?3|?
(d)| is the length penalty term ontarget translation.
The P (d|T ) decomposes intothe product of rule probabilities P (r), each ofwhich is decomposed further intoP (d|T ) =?r?dP (r).
(6)Each P (r) in Equation 6 is decomposed furtherinto the production of five probabilities:P(r) = P(r|lhs(r))?4?
P(r|rhs(r))?5?
P(r|root(lhs(r))?6?
Plex(lhs(r)|rhs(r))?7?
Plex(rhs(r)|lhs(r))?8 ,(7)where the last two are the lexical probabilities be-tween the terminals of lhs(r) and rhs(r).
All theweights of those features are tuned by using Min-imal Error Rate Training (Och, 2003).Following Mi et al (2008), we first convert thelattice-forest into lattice translation forest with theconversion algorithm proposed byMi et al (2008),842and then the decoder finds the best derivation onthe lattice translation forest.
For 1-best search, weuse the cube pruning technique (Chiang, 2007;Huang and Chiang, 2007) which approximatelyintersects the translation forest with the LM.
Fork-best search after getting 1-best derivation, weuse the lazy Algorithm 3 of Huang and Chiang(2005) to incrementally compute the second, third,through the kth best alternatives.For more detail, we refer to the algorithms ofMi et al (2008).6 Experiments6.1 Data PreparationOur experiments are on Chinese-to-English trans-lation.
Our training corpus is FBIS corpus withabout 6.9M/8.9M words in Chinese/English re-spectively.We use SRI Language Modeling Toolkit (Stol-cke, 2002) to train a 4-gram language model withKneser-Ney smoothing on the first 1/3 of the Xin-hua portion of Gigaword corpus.We use the 2002 NIST MT Evaluation test setas development set and the 2005 NIST MT Eval-uation test set as test set.
We evaluate the trans-lation quality using the case-insensitive BLEU-4metric (Papineni et al, 2002).
We use the standardMERT (Och, 2003) to tune the weights.6.1.1 Baseline Forest-based SystemWe first segment the Chinese sentences into the1-best segmentations using a state-of-the-art sys-tem (Jiang et al, 2008a), since it is not necessaryfor a conventional parser to take as input the POStagging results.
Then we parse the segmentationresults into forest by using the parser of Xiong etal.
(2005).
Actually, the parser will assign multiplePOS tags to each word rather than one.
As a result,our baseline system has already postponed thePOS tagging disambiguition to the decoding step.Forest is pruned by using a marginal probability-based pruning algorithm similar to Huang (2008).The pruning threshold are pf = 5 and pf = 10 atrule extraction and decoding steps respectively.We word-align the strings of 1-best segmenta-tions and target strings with GIZA++ (Och andNey, 2000) and apply the refinement method?grow-diag-final-and?
(Koehn et al, 2003) to getthe final alignments.
Following Mi and Huang(2008) and Mi et al (2008), we also extract rulesfrom forest-string pairs and translate forest tostring.6.1.2 Lattice-forest SystemWe first segment and POS tag the Chinese sen-tences into word lattices using the same sys-tem (Jiang et al, 2008a), and prune each lat-tice into a reasonable size using the marginalprobability-based pruning algorithm.Then, as current GIZA++ (Och and Ney, 2000)can only handle alignment between string-stringpairs, and word-alingment with the pairs of Chi-nese characters and target-string will obviously re-sult in worse alignment quality.
So a much betterway to utilize GIZA++ is to use the most-refinedsegmentation for each lattice instead of the char-acter sequence.
This approach can be viewed as acompromise between character-string and lattice-string word-alignment paradigms.
In our exper-iments, we construct the most-refined segmen-tations for lattices and word-align them againstthe English sentences.
We again apply the refine-ment method ?grow-diag-final-and?
(Koehn et al,2003) to get the final alignments.In order to get the lattice-forests, we modi-fied Xiong et al (2005)?s parser into a latticeparser, which produces the pruned lattice forestsfor both training, dev and test sentences.
Finally,we apply the rule extraction algorithm proposed inthis paper to obtain the rule set.
Both lattices andforests are pruned using a marginal probability-based pruning algorithm similar to Huang (2008).The pruning threshold of lattice is pl = 20 at boththe rule extraction and decoding steps, the thresh-olds for the latice-forests are pf = 5 and pf = 10at rule extraction and decoding steps respectively.6.2 Results and AnalysisTable 1 shows results of two systems.
Our lattice-forest (LF) system achieves a BLEU score of29.65, which is an absolute improvement of 0.9points over the forest (F) baseline system, and theimprovement is statistically significant at p < 0.01using the sign-test of Collins et al (2005).The average number of tokens for the 1-bestand most-refined segmentations are shown in sec-ond column.
The average number of charactersis 46.7, which is not shown in Table 1.
Com-843Sys Avg # of Rules BLEUtokens links All dev&tstF 28.7 35.1 29.6M 3.3M 28.75LF 37.1 37.1 23.5M 3.4M 29.65Table 1: Results of forest (F) and lattice-forest(LF) systems.
Please note that lattice-forest systemonly extracts 23.5M rules, which is only 79.4% ofthe rules extracted by forest system.
However, indecoding step, lattice-forest system can use morerules after filtered on dev and test sets.pared with the characters-based lattice parsing, ourmost-refined lattice parsing speeds up parsing by(37.1/46.7)3 ?
2 times, since parsing complexityis O(n3).More interestingly, our lattice-forest model onlyextracts 23.5M rules, which is 79.4% percent ofthe rules extracted by the baseline system.
Themain reason lies in the larger average numberof words for most-refined segmentations over lat-tices being 37.1 words vs 28.7 words over 1-bestsegmentations.
With much finer granularity, moreword aligned links and restrictions are introducedduring the rule extraction step by GIZA++.
How-ever, more rules can be used in the decoding stepfor the lattice-forest system, since the lattice-forestis larger than the forest over 1-best segmentation.We also investigate the question of how oftenthe non 1-best segmentations are picked in the fi-nal translation.
The statistic on our dev set sug-gests 33% of sentences choose non 1-best segmen-tations.
So our lattice-forest model can do globalsearch for the best segmentation and parse-tree todirect the final translation.
More importantly, wecan use more translation rules in the translationstep.7 Related WorksCompactly encoding exponentially many deriva-tions, lattice and forest have been used in someprevious works on SMT.
To alleviate the prob-lem of parsing error in 1-best tree-to-string trans-lation model, Mi et al (2008) first use forest todirect translation.
Then Mi and Huang (2008) useforest in rule extraction step.
Following the samedirection, Liu et al (2009) use forest in tree-to-tree model, and improve 1-best system by 3BLEU points.
Zhang et al (2009) use forest intree-sequence-to-string model and also achieve apromising improvement.
Dyer et al (2008) com-bine multiple segmentations into word lattice andthen use lattice to direct a phrase-based transla-tion decoder.
Then Dyer (2009) employ a singleMaximum Entropy segmentation model to gen-erate more diverse lattice, they test their modelon the hierarchical phrase-based system.
Latticesand forests can also be used in Minimal ErrorRate Training and Minimum Bayes Risk Decod-ing phases (Macherey et al, 2008; Tromble et al,2008; DeNero et al, 2009; Kumar et al, 2009; Liand Eisner, 2009).
Different from the works listedabove, we mainly focus on how to combine latticeand forest into a single tree-to-string system.8 Conclusion and Future WorkIn this paper, we have proposed a lattice-forestbased model to alleviate the problem of error prop-agation in traditional single-best pipeline frame-work.
Unlike previous works, which only focus onone module at a time, our model successfully in-tegrates lattice into a state-of-the-art forest tree-to-string system.
We have explored the algorithms oflattice parsing, rule extraction and decoding.
Ourmodel postpones the disambiguition of segmenta-tion and parsing into the final translation step, sothat we can make a more global decision to searchfor the best segmentation, parse-tree and transla-tion in one step.
The experimental results showthat our lattice-forest approach achieves an abso-lute improvement of +0.9 points in term of BLEUscore over a state-of-the-art forest-based model.For future work, we would like to pay moreattention to word alignment between lattice pairsand forest pairs, which would be more principledthan our current method of word alignment be-tween most-refined segmentation and string.AcknowledgementWe thank Steve DeNeefe and the three anony-mous reviewers for comments.
The work is sup-ported by National Natural Science Foundationof China, Contracts 90920004 and 60736014,and 863 State Key Project No.
2006AA010108(H. M and Q. L.), and in part by DARPA GALEContract No.
HR0011-06-C-0022, and DARPAunder DOI-NBC Grant N10AP20031 (L. H andH.
M).844ReferencesDavid Chiang.
2007.
Hierarchical phrase-based trans-lation.
Comput.
Linguist., 33(2):201?228.Michael Collins, Philipp Koehn, and Ivona Kucerova.2005.
Clause restructuring for statistical machinetranslation.
In Proceedings of ACL, pages 531?540,Ann Arbor, Michigan, June.John DeNero, David Chiang, and Kevin Knight.
2009.Fast consensus decoding over translation forests.
InProceedings of ACL/IJCNLP.Christopher Dyer, Smaranda Muresan, and PhilipResnik.
2008.
Generalizing word lattice translation.In Proceedings of ACL-08: HLT, pages 1012?1020,Columbus, Ohio, June.C.
Dyer.
2009.
Using a maximum entropy model tobuild segmentation lattices for mt.
In Proceedingsof NAACL.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Pro-ceedings of COLING-ACL, pages 961?968, Sydney,Australia, July.Liang Huang and David Chiang.
2005.
Better k-bestparsing.
In Proceedings of IWPT.Liang Huang and David Chiang.
2007.
Forest rescor-ing: Faster decoding with integrated language mod-els.
In Proceedings of ACL, pages 144?151, June.Liang Huang.
2008.
Forest reranking: Discriminativeparsing with non-local features.
In Proceedings ofACL.Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Lu?.2008a.
A cascaded linear model for joint chineseword segmentation and part-of-speech tagging.
InProceedings of ACL-08: HLT.Wenbin Jiang, Haitao Mi, and Qun Liu.
2008b.
Wordlattice reranking for chinese word segmentation andpart-of-speech tagging.
In Proceedings of Coling2008.Philipp Koehn, Franz Joseph Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of HLT-NAACL, pages 127?133, Edmon-ton, Canada, May.Shankar Kumar, Wolfgang Macherey, Chris Dyer, andFranz Och.
2009.
Efficient minimum error ratetraining and minimum bayes-risk decoding for trans-lation hypergraphs and lattices.
In Proceedings ofthe ACL/IJCNLP 2009.Zhifei Li and Jason Eisner.
2009.
First- and second-order expectation semirings with applications tominimum-risk training on translation forests.
InProceedings of EMNLP, pages 40?51, Singapore,August.
Association for Computational Linguistics.Yang Liu, Yajuan Lu?, and Qun Liu.
2009.
Improvingtree-to-tree translation with packed forests.
In Pro-ceedings of ACL/IJCNLP, August.Wolfgang Macherey, Franz Och, Ignacio Thayer, andJakob Uszkoreit.
2008.
Lattice-based minimum er-ror rate training for statistical machine translation.In Proceedings of EMNLP 2008.Haitao Mi and Liang Huang.
2008.
Forest-based trans-lation rule extraction.
In Proceedings of EMNLP2008, pages 206?214, Honolulu, Hawaii, October.Haitao Mi, Liang Huang, and Qun Liu.
2008.
Forest-based translation.
In Proceedings of ACL-08:HLT,pages 192?199, Columbus, Ohio, June.Franz J. Och and Hermann Ney.
2000.
Improved sta-tistical alignment models.
In Proceedings of ACL,pages 440?447.Franz J. Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings ofACL, pages 160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings ofACL, pages 311?318, Philadephia, USA, July.Stuart M. Shieber, Yves Schabes, and Fernando C. N.Pereira.
1994.
Principles and implementation of de-ductive parsing.Andreas Stolcke.
2002.
SRILM - an extensible lan-guage modeling toolkit.
In Proceedings of ICSLP,volume 30, pages 901?904.Roy Tromble, Shankar Kumar, Franz Och, and Wolf-gang Macherey.
2008.
Lattice Minimum Bayes-Risk decoding for statistical machine translation.
InProceedings of EMNLP 2008.Ashish Venugopal, Andreas Zollmann, Noah A. Smith,and Stephan Vogel.
2008.
Wider pipelines: N-bestalignments and parses in MT training.
In Proceed-ings of AMTA.Deyi Xiong, Shuanglong Li, Qun Liu, and ShouxunLin.
2005.
Parsing the Penn Chinese Treebank withSemantic Knowledge.
In Proceedings of IJCNLP2005, pages 70?81.Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw, andChew Lim Tan.
2009.
Forest-based tree sequenceto string translation model.
In Proceedings of theACL/IJCNLP 2009.845
