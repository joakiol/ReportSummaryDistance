Proceedings of the Second Workshop on Statistical Machine Translation, pages 177?180,Prague, June 2007. c?2007 Association for Computational LinguisticsDomain Adaptation in Statistical Machine Translation with MixtureModelling ?Jorge Civera and Alfons JuanUniversidad Polite?cnica de ValenciaCamino de Vera s/n46022 Valencia, Spain{jorcisai,ajuan}@iti.upv.esAbstractMixture modelling is a standard techniquefor density estimation, but its use in sta-tistical machine translation (SMT) has juststarted to be explored.
One of the mainadvantages of this technique is its capabil-ity to learn specific probability distributionsthat better fit subsets of the training dataset.This feature is even more important in SMTgiven the difficulties to translate polysemicterms whose semantic depends on the con-text in which that term appears.
In this pa-per, we describe a mixture extension of theHMM alignment model and the derivation ofViterbi alignments to feed a state-of-the-artphrase-based system.
Experiments carriedout on the Europarl and News Commentarycorpora show the potential interest and limi-tations of mixture modelling.1 IntroductionMixture modelling is a popular approach for densityestimation in many scientific areas (G. J. McLach-lan and D. Peel, 2000).
One of the most interest-ing properties of mixture modelling is its capabilityto model multimodal datasets by defining soft parti-tions on these datasets, and learning specific proba-bility distributions for each partition, that better ex-plains the general data generation process.
?Work supported by the EC (FEDER) and the SpanishMEC under grant TIN2006-15694-CO2-01, the Conseller?
?ad?Empresa, Universitat i Cie`ncia - Generalitat Valenciana un-der contract GV06/252, the Universidad Polite?cnica de Valen-cia with ILETA project and Ministerio de Educacio?n y Ciencia.In Machine Translation (MT), it is common toencounter large parallel corpora devoted to hetero-geneous topics.
These topics usually define setsof topic-specific lexicons that need to be translatedtaking into the semantic context in which they arefound.
This semantic dependency problem couldbe overcome by learning topic-dependent translationmodels that capture together the semantic contextand the translation process.However, there have not been until very recentlythat the application of mixture modelling in SMThas received increasing attention.
In (Zhao andXing, 2006), three fairly sophisticated bayesian top-ical translation models, taking IBM Model 1 as abaseline model, were presented under the bilingualtopic admixture model formalism.
These modelscapture latent topics at the document level in order toreduce semantic ambiguity and improve translationcoherence.
The models proposed provide in somecases better word alignment and translation qualitythan HMM and IBM models on an English-Chinesetask.
In (Civera and Juan, 2006), a mixture exten-sion of IBM model 2 along with a specific dynamic-programming decoding algorithm were proposed.This IBM-2 mixture model offers a significant gainin translation quality over the conventional IBMmodel 2 on a semi-synthetic task.In this work, we present a mixture extension of thewell-known HMM alignment model first proposedin (Vogel and others, 1996) and refined in (Och andNey, 2003).
This model possesses appealing proper-ties among which are worth mentioning, the simplic-ity of the first-order word alignment distribution thatcan be made independent of absolute positions while177taking advantage of the localization phenomenonof word alignment in European languages, and theefficient and exact computation of the E-step andViterbi alignment by using a dynamic-programmingapproach.
These properties have made this modelsuitable for extensions (Toutanova et al, 2002)and integration in a phrase-based model (Deng andByrne, 2005) in the past.2 HMM alignment modelGiven a bilingual pair (x, y), where x and y are mu-tual translation, we incorporate the hidden variablea = a1a2 ?
?
?
a|x| to reveal, for each source word po-sition j, the target word position aj ?
{0, 1, .
.
.
, |y|}to which it is connected.
Thus,p(x | y) =?a?A(x,y)p(x, a | y) (1)where A(x, y) denotes the set of all possible align-ments between x and y.
The alignment-completedprobability P (x, a | y) can be decomposed in termsof source position-dependent probabilities as:p(x, a | y)=|x|?j=1p(aj | aj?11 , xj?11 , y) p(xj | aj1, xj?11 , y)(2)The original formulation of the HMM alignmentmodel assumes that each source word is connectedto exactly one target word.
This connection dependson the target position to which was aligned the pre-vious source word and the length of the target sen-tence.
Here, we drop both dependencies in order tosimplify to a jump width alignment probability dis-tribution:p(aj | aj?11 , xj?11 , y) ?
{p(aj) j = 1p(aj?aj?1) j > 1(3)p(xj | aj1, xj?11 , y) ?
p(xj | yaj ) (4)Furthermore, the treatment of the NULL word isthe same as that presented in (Och and Ney, 2003).Finally, the HMM alignment model is defined as:p(x | y) =?a?A(x,y)p(a1)|x|?j=2p(aj?aj?1)|x|?j=1p(xj |yaj )(5)3 Mixture of HMM alignment modelsLet us suppose that p(x | y) has been generated usinga T-component mixture of HMM alignment models:p(x | y) =T?t=1p(t | y) p(x | y, t)=T?t=1p(t | y)?a?A(x,y)p(x, a | y, t) (6)In Eq.
6, we introduce mixture coefficients p(t | y)to weight the contribution of each HMM alignmentmodel in the mixture.
While the term p(x, a | y, t) isdecomposed as in the original HMM model.The assumptions of the constituent HMM mod-els are the same than those of the previous section,but we obtain topic-dependent statistical dictionariesand word alignments.
Apropos of the mixture coef-ficients, we simplify these terms dropping its depen-dency on y, leaving as future work its inclusion inthe model.
Formally, the assumptions are:p(t | y) ?
p(t) (7)p(aj | aj?11 , xj?11 , y, t)?
{p(aj | t) j=1p(aj?aj?1 | t)j>1(8)p(xj | aj1, xj?11 , y, t) ?
p(xj | yaj , t) (9)Replacing the assumptions in Eq.
6, we obtain the(incomplete) HMM mixture model as follows:p(x | y) =T?t=1p(t)?a?A(x,y)p(a1 | t)?
?|x|?j=2p(aj?aj?1 | t)|x|?j=1p(xj |yaj , t) (10)and the set of unknown parameters comprises:~?
=??????
?p(t) t = 1 .
.
.
Tp(i | t) j = 1p(i?
i?
| t) j > 1p(u | v, t) ?u ?
X and v ?
Y(11)X and Y , being the source and target vocabular-ies.The estimation of the unknown parameters inEq.
10 is troublesome, since topic and alignment178data are missing.
Here, we revert to the EM opti-misation algoritm to compute these parameters.In order to do that, we define the complete versionof Eq.
10 incorporating the indicator variables zt andza, uncovering, the until now hidden variables.
Thevariable zt is a T -dimensional bit vector with 1 inthe position corresponding to the component gener-ating (x, y) and zeros elsewhere, while the variableza = za1 .
.
.
za|x| where zaj is a |y|-dimensional bitvector with 1 in the position corresponding to the tar-get position to which position j is aligned and zeroselsewhere.
Then, the complete model is:p(x, zt, za | y) ?T?t=1p(t)zt|y|?i=1p(i | t)za1izt?
?|x|?j=1|y|?i=1p(xj | yi, t)zajizt|y|?i?=1p(i?
i?
| t)zaj?1i?
zajizt(12)Given the complete model, the EM algorithmworks in two basic steps in each iteration: theE(xpectation) step and the M(aximisation) step.
Atiteration k, the E step computes the expected valueof the hidden variables given the observed data(x, y) and the estimate of the parameters ~?
(k).The E step reduces to the computation of the ex-pected value of zt, zajizt and zaj?1i?zajizt for eachsample n:zt ?
p(t)|y|?i=1?|x|it (13)zajizt = zajit zt (14)zaj?1i?zajizt = (zaj?1i?zaji)t zt (15)wherezajit?|y|?k=1?jkt?jkt(zaj?1i?zaji)t?
?j?1it p(i?
i?
| t) p(xj |yi, t)?jitand the recursive functions ?
and ?
defined as:?jit =??
?p(i | t) p(xj | yi, t) j = 1|y|?k=1?j?1kt p(i?
k | t) p(xj | yi, t) j > 1?jit =??
?1 j = |x||y|?k=1p(k ?
i | t) p(xj+1 | yk, t)?j+1kt j < |x|The M step finds a new estimate of ~?, by max-imising Eq.
12, using the expected value of the miss-ing data from Eqs.
13,14 and 15 over all sample n:p(t) =1NN?n=1zntp(i | t) ?N?n=1zna1itp(i?
i?
| t) ?N?n=1|xn|?j=1(znaj?1i?znaji)tp(u | v, t) ?N?n=1|xn|?j=1|yn|?i=1znajit ?
(xnj , u)?
(yni, v)3.1 Word alignment extractionThe HMM mixture model described in the previoussection was used to generate Viterbi alignments onthe training dataset.
These optimal alignments arethe basis for phrase-based systems.In the original HMM model, the Viterbi align-ment can be efficiently computed by a dynamic-programming algorithm with a complexity O(|x| ?|y|2).
In the mixture HMM model, we approximatethe Viterbi alignment by maximising over the com-ponents of the mixture:a?
?
argmaxamaxtp(t) p(x, a | y, t)So we have that the complexity of the compu-tation of the Viterbi alignment in a T-componentHMM mixture model is O(T ?
|x| ?
|y|2).4 Experimental resultsThe data that was employed in the experiments totrain the HMM mixture model corresponds to theconcatenation of the Spanish-English partitions ofthe Europarl and the News Commentary corpora.The idea behind this decision was to let the mixturemodel distinguish which bilingual pairs should con-tribute to learn a given HMM component in the mix-ture.
Both corpora were preprocessed as suggestedfor the baseline system by tokenizing, filtering sen-tences longer than 40 words and lowercasing.Regarding the components of the translation sys-tem, 5-gram language models were trained on themonolingual version of the corpora for English(En)179and Spanish(Es), while phrase-based models withlexicalized reordering model were trained using theMoses toolkit (P. Koehn and others, 2007), but re-placing the Viterbi alignments, usually provided byGIZA++ (Och and Ney, 2003), by those of the HMMmixture model with training scheme mix 15H5.This configuration was used to translate both test de-velopment sets, Europarl and News Commentary.Concerning the weights of the different models,we tuned those weights by minimum error rate train-ing and we employed the same weighting schemefor all the experiments in the same language pair.Therefore, the same weighting scheme was usedover different number of components.BLEU scores are reported in Tables 1 and 2 as afunction of the number of components in the HMMmixture model on the preprocessed development testsets of the Europarl and News Commentary corpora.Table 1: BLEU scores on the Europarl developmenttest dataT 1 2 3 4En-Es 31.27 31.08 31.12 31.11Es-En 31.74 31.70 31.80 31.71Table 2: BLEU scores on the News-Commentarydevelopment test dataT 1 2 3 4En-Es 29.62 30.01 30.17 29.95Es-En 29.15 29.22 29.11 29.02As observed in Table 1, if we compare the BLEUscores of the conventional single-component HMMmodel to those of the HMM mixture model, it seemsthat there is little or no gain from incorporatingmore topics into the mixture for the Europarl cor-pus.
However, in Table 2, the BLEU scores onthe English-Spanish pair significantly increase as thenumber of components is incremented.
We believethat this is due to the fact that the News Commen-tary corpus seems to have greater influence on themixture model than on the single-component model,specializing Viterbi alignments to favour this corpus.5 Conclusions and future workIn this work, a novel mixture version of the HMMalignment model was introduced.
This model wasemployed to generate topic-dependent Viterbi align-ments that were input into a state-of-the-art phrase-based system.
The preliminary results reported onthe English-Spanish partitions of the Europarl andNews-Commentary corpora may raise some doubtsabout the applicability of mixture modelling to SMT,nonetheless in the advent of larger open-domain cor-pora, the idea behind topic-specific translation mod-els seem to be more than appropriate, necessary.
Onthe other hand, we are fully aware that indirectlyassessing the quality of a model through a phrase-based system is a difficult task because of the differ-ent factors involved (Ayan and Dorr, 2006).Finally, the main problem in mixture modelling isthe linear growth of the set of parameters as the num-ber of components increases.
In the HMM, and alsoin IBM models, this problem is aggravated becauseof the use of statistical dictionary entailing a largenumber of parameters.
A possible solution is the im-plementation of interpolation techniques to smoothsharp distributions estimated on few events (Och andNey, 2003; Zhao and Xing, 2006).ReferencesN.
F. Ayan and B. J. Dorr.
2006.
Going beyond AER: anextensive analysis of word alignments and their impacton MT.
In Proc.
of ACL?06, pages 9?16.J.
Civera and A. Juan.
2006.
Mixtures of IBM Model 2.In Proc.
of EAMT?06, pages 159?167.Y.
Deng and W. Byrne.
2005.
HMM word and phrasealignment for statistical machine translation.
In Proc.of HLT-EMNLP?05, pages 169?176.G.
J. McLachlan and D. Peel.
2000.
Finite Mixture Mod-els.
Wiley.F.
J. Och and H. Ney.
2003.
A systematic comparison ofvarious statistical alignment models.
ComputationalLinguistics, 29(1):19?51.P.
Koehn and others.
2007.
Moses: Open SourceToolkit for Statistical Machine Translation.
In Proc.of ACL?07 Demo Session, page To be published.K.
Toutanova, H. T. Ilhan, and C. D. Manning.
2002.Extensions to HMM-based statistical word alignmentmodels.
In Proc.
of EMNLP ?02, pages 87?94.S.
Vogel et al 1996.
HMM-based word alignment instatistical translation.
In Proc.
of CL, pages 836?841.B.
Zhao and E. P. Xing.
2006.
BiTAM: Bilingual TopicAdMixture Models for Word Alignment.
In Proc.
ofCOLING/ACL?06.180
