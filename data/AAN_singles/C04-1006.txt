Improved Word Alignment Using a Symmetric Lexicon ModelRichard Zens and Evgeny Matusov and Hermann NeyLehrstuhl fu?r Informatik VI, Computer Science DepartmentRWTH Aachen UniversityD-52056 Aachen, Germany{zens,matusov,ney}@cs.rwth-aachen.deAbstractWord-aligned bilingual corpora are animportant knowledge source for manytasks in natural language processing.
Weimprove the well-known IBM alignmentmodels, as well as the Hidden-Markovalignment model using a symmetric lex-icon model.
This symmetrization takesnot only the standard translation direc-tion from source to target into account,but also the inverse translation directionfrom target to source.
We present a the-oretically sound derivation of these tech-niques.
In addition to the symmetriza-tion, we introduce a smoothed lexiconmodel.
The standard lexicon model isbased on full-form words only.
We proposea lexicon smoothing method that takesthe word base forms explicitly into ac-count.
Therefore, it is especially usefulfor highly inflected languages such as Ger-man.
We evaluate these methods on theGerman?English Verbmobil task and theFrench?English Canadian Hansards task.We show statistically significant improve-ments of the alignment quality comparedto the best system reported so far.
Forthe Canadian Hansards task, we achievean improvement of more than 30% rela-tive.1 IntroductionWord-aligned bilingual corpora are an impor-tant knowledge source for many tasks in nat-ural language processing.
Obvious applica-tions are the extraction of bilingual word orphrase lexica (Melamed, 2000; Och and Ney,2000).
These applications depend heavily onthe quality of the word alignment (Och andNey, 2000).
Word alignment models were firstintroduced in statistical machine translation(Brown et al, 1993).
The alignment describesthe mapping from source sentence words totarget sentence words.Using the IBM translation models IBM-1to IBM-5 (Brown et al, 1993), as well asthe Hidden-Markov alignment model (Vogelet al, 1996), we can produce alignments ofgood quality.
In (Och and Ney, 2003), it isshown that the statistical approach performsvery well compared to alternative approaches,e.g.
based on the Dice coefficient or the com-petitive linking algorithm (Melamed, 2000).A central component of the statistical trans-lation models is the lexicon.
It models theword translation probabilities.
The standardtraining procedure of the statistical modelsuses the EM algorithm.
Typically, the modelsare trained for one translation direction only.Here, we will perform a simultaneous trainingof both translation directions, source-to-targetand target-to-source.
After each iteration ofthe EM algorithm, we combine the two lexicato a symmetric lexicon.
This symmetric lex-icon is then used in the next iteration of theEM algorithm for both translation directions.We will propose and justify linear and loglin-ear interpolation methods.Statistical methods often suffer from thedata sparseness problem.
In our case, manywords in the bilingual sentence-aligned textsare singletons, i.e.
they occur only once.
Thisis especially true for the highly inflected lan-guages such as German.
It is hard to obtainreliable estimations of the translation proba-bilities for these rarely occurring words.
Toovercome this problem (at least partially), wewill smooth the lexicon probabilities of thefull-form words using a probability distribu-tion that is estimated using the word baseforms.
Thus, we exploit that multiple full-form words share the same base form and havesimilar meanings and translations.We will evaluate these methods on theGerman?English Verbmobil task and theFrench?English Canadian Hansards task.
Wewill show statistically significant improve-ments compared to state-of-the-art results in(Och and Ney, 2003).
On the CanadianHansards task, the symmetrization methodswill result in an improvement of more than30% relative.2 Statistical Word Alignment ModelsIn this section, we will give a short descriptionof the commonly used statistical word align-ment models.
These alignment models stemfrom the source-channel approach to statisti-cal machine translation (Brown et al, 1993).We are given a source language sentence fJ1 :=f1...fj ...fJ which has to be translated intoa target language sentence eI1 := e1...ei...eI .Among all possible target language sentences,we will choose the sentence with the highestprobability:e?I1 = argmaxeI1{Pr(eI1|fJ1 )}= argmaxeI1{Pr(eI1) ?
Pr(fJ1 |eI1)}This decomposition into two knowledgesources allows for an independent modeling oftarget language model Pr(eI1) and translationmodel Pr(fJ1 |eI1).
Into the translation model,the word alignment A is introduced as a hid-den variable:Pr(fJ1 |eI1) =?APr(fJ1 , A|eI1)Usually, we use restricted alignments in thesense that each source word is aligned to atmost one target word, i.e.
A = aJ1 .
A de-tailed description of the popular translationmodels IBM-1 to IBM-5 (Brown et al, 1993),as well as the Hidden-Markov alignment model(HMM) (Vogel et al, 1996) can be found in(Och and Ney, 2003).
All these models includeparameters p(f |e) for the single-word basedlexicon.
They differ in the alignment model.A Viterbi alignment A?
of a specific model isan alignment for which the following equationholds:A?
= argmaxA{Pr(fJ1 , A|eI1)}We measure the quality of an alignment modelusing the quality of the Viterbi alignment com-pared to a manually produced reference align-ment.In Section 3, we will apply the lexicon sym-metrization methods to the models describedpreviously.
Therefore, we will now sketch thestandard training procedure for the lexiconmodel.
The EM algorithm is used to trainthe free lexicon parameters p(f |e).In the E-step, the lexical counts for eachsentence pair (fJ1 , eI1) are calculated and thensummed over all sentence pairs in the trainingcorpus:N(f, e) =?
(fJ1 ,eI1)?aJ1p(aJ1 |fJ1 , eI1)?i,j?
(f, fj)?
(e, ei)In the M-step the lexicon probabilities are:p(f |e) = N(f, e)?f?N(f?
, e)3 Symmetrized Lexicon ModelDuring the standard training procedure, thelexicon parameters p(f |e) and p(e|f) were es-timated independent of each other in strictlyseparate trainings.
In this section, we presenttwo symmetrization methods for the lexiconmodel.
As a starting point, we use thejoint lexicon probability p(f, e) and determinethe conditional probabilities for the source-to-target direction p(f |e) and the target-to-source direction p(e|f) as the correspondingmarginal distribution:p(f |e) = p(f, e)?f?p(f?
, e) (1)p(e|f) = p(f, e)?e?p(f, e?)
(2)The nonsymmetric auxiliary Q-functions forreestimating the lexicon probabilities duringthe EM algorithm can be represented as fol-lows.
Here, NST (f, e) and NTS(f, e) denotethe lexicon counts for the source-to-target(ST ) direction and the target-to-source (TS)direction, respectively.QST ({p(f |e)}) =?f,eNST (f, e) ?
log p(f, e)?f?p(f?
, e)QTS({p(e|f)}) =?f,eNTS(f, e) ?
log p(f, e)?e?p(f, e?
)3.1 Linear InterpolationTo estimate the joint probability using the EMalgorithm, we define the auxiliary Q-functionas a linear interpolation of the Q-functions forthe source-to-target and the target-to-sourcedirection:Q?
({p(f, e)}) = ?
?QST ({p(f |e)})+(1?
?)
?QTS({p(e|f)})= ?
?
?f,eNST (f, e) ?
log p(f, e)+(1?
?)
?
?f,eNTS(f, e) ?
log p(f, e)??
?
?eNST (e) ?
log?f?p(f?
, e)?(1?
?)
?
?fNTS(f) ?
log?e?p(f, e?
)The unigram counts N(e) and N(f) are deter-mined, for each of the two translation direc-tions, by taking a sum of N(f, e) over f andover e, respectively.
We define the combinedlexicon count N?
(f, e):N?
(f, e) := ?
?NST (f, e) + (1?
?)
?NTS(f, e)Now, we derive the symmetrized Q-functionover p(f, e) for a certain word pair (f, e).Then, we set this derivative to zero to deter-mine the reestimation formula for p(f, e) andobtain the following equation:N?
(f, e)p(f, e) = ?
?NST (e)?f?p(f?
, e) + (1?
?)
?NTS(f)?e?p(f, e?
)We do not know a closed form solution for thisequation.
As an approximation, we use thefollowing term:p?
(f, e) = N?
(f, e)?f?
,e?N?(f?
, e?
)This estimate is an exact solution, if the uni-gram counts for f and e are independent of thetranslation direction, i. e. NST (f) = NTS(f)and NST (e) = NTS(e).
We make this approx-imation and thus we interpolate the lexiconcounts linear after each iteration of the EMalgorithm.
Then, we normalize these counts(according to Equations 1 and 2) to determinethe lexicon probabilities for each of the twotranslation directions.3.2 Loglinear InterpolationWe will show in Section 5 that the linear in-terpolation results in significant improvementsover the nonsymmetric system.
Motivated bythese experiments, we investigated also theloglinear interpolation of the lexicon counts ofthe two translation directions.
The combinedlexicon count N?
(f, e) is now defined as:N?
(f, e) = NST (f, e)?
?NTS(f, e)1?
?The normalization is done in the same way asfor the linear interpolation.
The linear inter-polation resembles more a union of the two lex-ica whereas the loglinear interpolation is moresimilar to an intersection of both lexica.
Thusfor the linear interpolation, a word pair (f, e)obtains a large combined count, if the count inat least one direction is large.
For the loglin-ear interpolation, the combined count is largeonly if both lexicon counts are large.In the experiments, we will use the interpo-lation weight ?
= 0.5 for both the linear andthe loglinear interpolation, i. e. both transla-tion directions are weighted equally.3.3 Evidence TrimmingInitially, the lexicon contains all word pairsthat cooccur in the bilingual training corpus.The majority of these word pairs are not trans-lations of each other.
Therefore, we wouldlike to remove those lexicon entries.
Evidencetrimming is one way to do this.
The evidenceof a word pair (f, e) is the estimated countN(f, e).
Now, we discard a word pair if its ev-idence is below a certain threshold ?
.1 In thecase of the symmetric lexicon, we can furtherrefine this method.
For estimating the lex-icon in the source-to-target direction p?
(f |e),the idea is to keep all entries from this di-rection and to boost the entries that have ahigh evidence in the target-to-source directionNTS(f, e).
We obtain the following formula:N?ST (f, e) =???
?NST (f, e) + (1?
?
)NTS(f, e)if NST (f, e) > ?0 elseThe count N?ST (f, e) is now used to estimatethe source-to-target lexicon p?
(f |e).
With thismethod, we do not keep entries in the source-to-target lexicon p?
(f |e) if their evidence is low,even if their evidence in the target-to-source1Actually, there is always implicit evidence trim-ming caused by the limited machine precision.direction NTS(f, e) is high.
For the target-to-source direction, we apply this method in asimilar way.4 Lexicon SmoothingThe lexicon model described so far is based onfull-form words.
For highly inflected languagessuch as German this might cause problems,because many full-form words occur only a fewtimes in the training corpus.
Compared to En-glish, the token/type ratio for German is usu-ally much lower (e.g.
Verbmobil: English 99.4,German 56.3).
The information that multiplefull-form words share the same base form isnot used in the lexicon model.
To take this in-formation into account, we smooth the lexiconmodel with a backing-off lexicon that is basedon word base forms.
The smoothing methodwe apply is absolute discounting with interpo-lation:p(f |e) = max {N(f, e)?
d, 0}N(e) + ?
(e) ?
?
(f, e?
)This method is well known from languagemodeling (Ney et al, 1997).
Here, e?
de-notes the generalization, i.e.
the base form,of the word e. The nonnegative value d isthe discounting parameter, ?
(e) is a normal-ization constant and ?
(f, e?)
is the normalizedbacking-off distribution.The formula for ?
(e) is:?
(e) = 1N(e)??
?f :N(f,e)>dd+?f :N(f,e)?dN(f, e)?
?= 1N(e)?fmin{d,N(f, e)}This formula is a generalization of the onetypically used in publications on languagemodeling.
This generalization is necessary,because the lexicon counts may be fractionalwhereas in language modeling typically inte-ger counts are used.
Additionally, we wantto allow for discounting values d greater thanone.
The backing-off distribution ?
(f, e?)
is es-timated using relative frequencies:?
(f, e?)
= N(f, e?)?f?N(f?
, e?
)Here, N(f, e?)
denotes the count of the eventthat the source language word f and the targetlanguage base form e?
occur together.
Thesecounts are computed by summing the lexiconcounts N(f, e) over all full-form words e whichshare the same base form e?.5 Results5.1 Evaluation CriteriaWe use the same evaluation criterion as de-scribed in (Och and Ney, 2000).
The gen-erated word alignment is compared to a ref-erence alignment which is produced by hu-man experts.
The annotation scheme explic-itly takes the ambiguity of the word alignmentinto account.
There are two different kindsof alignments: sure alignments (S) which areused for alignments that are unambiguous andpossible alignments (P ) which are used foralignments that might or might not exist.
TheP relation is used especially to align wordswithin idiomatic expressions, free translations,and missing function words.
It is guaranteedthat the sure alignments are a subset of thepossible alignments (S ?
P ).
The obtainedreference alignment may contain many-to-oneand one-to-many relationships.The quality of an alignment A is computedas appropriately redefined precision and recallmeasures.
Additionally, we use the alignmenterror rate (AER), which is derived from thewell-known F-measure.recall = |A ?
S||S| , precision =|A ?
P ||A|AER(S, P ;A) = 1?
|A ?
S|+ |A ?
P ||A|+ |S|With these definitions a recall error can onlyoccur if a S(ure) alignment is not found and aprecision error can only occur if a found align-ment is not even P (ossible).5.2 Experimental SetupWe evaluated the presented lexicon sym-metrization methods on the Verbmobil andthe Canadian Hansards task.
The German?English Verbmobil task (Wahlster, 2000) is aspeech translation task in the domain of ap-pointment scheduling, travel planning and ho-tel reservation.
The French?English CanadianHansards task consists of the debates in theCanadian Parliament.The corpus statistics are shown in Table 1and Table 2.
The number of running wordsand the vocabularies are based on full-formwords including punctuation marks.
As inTable 1: Verbmobil: Corpus statistics.German EnglishTrain Sentences 34KWords 329 625 343 076Vocabulary 5 936 3 505Singletons 2 600 1 305Test Sentences 354Words 3 233 3 109Table 2: Canadian Hansards: Corpus statistics.French EnglishTrain Sentences 128KWords 2.12M 1.93MVocabulary 37 542 29 414Singletons 12 986 9 572Test Sentences 500Words 8 749 7 946(Och and Ney, 2003), the first 100 sentencesof the test corpus are used as a developmentcorpus to optimize model parameters that arenot trained via the EM algorithm, e.g.
thediscounting parameter for lexicon smoothing.The remaining part of the test corpus is usedto evaluate the models.We use the same training schemes (modelsequences) as presented in (Och and Ney,2003).
As we use the same training and test-ing conditions as (Och and Ney, 2003), we willrefer to the results presented in that article asthe baseline results.
In (Och and Ney, 2003),the alignment quality of statistical models iscompared to alternative approaches, e.g.
us-ing the Dice coefficient or the competitivelinking algorithm.
The statistical approachshowed the best performance and therefore wereport only the results for the statistical sys-tems.5.3 Lexicon SymmetrizationIn Table 3 and Table 4, we present the follow-ing experiments performed for both the Verb-mobil and the Canadian Hansards task:?
Base: the system taken from (Och andNey, 2003) that we use as baseline system.?
Lin.
: symmetrized lexicon using a lin-ear interpolation of the lexicon counts af-ter each training iteration as described inSection 3.1.?
Log.
: symmetrized lexicon using a log-linear interpolation of the lexicon countsafter each training iteration as describedin Section 3.2.Table 3: Comparison of alignment perfor-mance for the Verbmobil task (S?T: source-to-target direction, T?S: target-to-source di-rection; all numbers in percent).S?T T?SPre.
Rec.
AER Pre.
Rec.
AERBase 93.5 95.3 5.7 91.4 88.7 9.9Lin.
96.0 95.4 4.3 93.7 89.6 8.2Log.
93.6 95.6 5.5 94.5 89.4 7.94681012141618100  1000  10000  100000AERCorpus SizebaselinelinearloglinearFigure 1: AER[%] of different alignment meth-ods as a function of the training corpus sizefor the Verbmobil task (source-to-target direc-tion).In Table 3, we compare both interpolationvariants for the Verbmobil task to (Och andNey, 2003).
We observe notable improvementsin the alignment error rate using the linear in-terpolation.
For the translation direction fromGerman to English (S?T), an improvement ofabout 25% relative is achieved from an align-ment error rate of 5.7% for the baseline systemto 4.3% using the linear interpolation.
Per-forming the loglinear interpolation, we observea substantial reduction of the alignment errorrate as well.
The two symmetrization methodsimprove both precision and recall of the result-ing Viterbi alignment in both translation di-rections for the Verbmobil task.
The improve-ments with the linear interpolation is for bothtranslation directions statistically significantat the 99% level.
For the loglinear interpo-lation, the target-to-source translation direc-tion is statistically significant at the 99% level.The statistical significance test were done us-ing boostrap resampling.We also performed experiments on sub-corpora of different sizes.
For the Verbmo-bil task, the results are illustrated in Figure 1.Table 4: Comparison of alignment perfor-mance for the Canadian Hansards task (S?T:source-to-target direction, T?S: target-to-source direction; all numbers in percent).S?T T?SPre.
Rec.
AER Pre.
Rec.
AERBase 85.4 90.6 12.6 85.6 90.9 12.4Lin.
89.3 91.4 9.9 89.0 92.0 9.8Log.
91.0 92.0 8.6 91.2 92.1 8.4We observe that both symmetrization variantsresult in improvements for all corpus sizes.With increasing training corpus size the per-formance of the linear interpolation becomessuperior to the performance of the loglinearinterpolation.In Table 4, we compare the symmetriza-tion methods with the baseline system for theCanadian Hansards task.
Here, the loglin-ear interpolation performs best.
We achievea relative improvement over the baseline ofmore than 30% for both translation directions.For instance, the alignment error rate for thetranslation direction from French to English(S?T) improves from 12.6% for the baselinesystem to 8.6% for the symmetrized systemwith loglinear interpolation.
Again, the twosymmetrization methods improve both preci-sion and recall of the Viterbi alignment.For the Canadian Hansards task, all the im-provements of the alignment error rate are sta-tistically significant at the 99% level.5.4 Generalized AlignmentsIn (Och and Ney, 2003) generalized alignmentsare used, thus the final Viterbi alignments ofboth translation directions are combined us-ing some heuristic.
Experimentally, the bestheuristic for the Canadian Hansards task isthe intersection.
For the Verbmobil task, therefined method of (Och and Ney, 2003) isused.
The results are summarized in Table 5.We see that both the linear and the loglinearlexicon symmetrization methods yield an im-provement with respect to the alignment errorrate.
For the Verbmobil task, the improve-ment with the loglinear interpolation is sta-tistically significant at the 99% level.
For theCanadian Hansards task, both lexicon sym-metrization methods result in statistically sig-nificant improvements at the 95% level.
Addi-tionally, we observe that precision and recallare more balanced for the symmetrized lexiconvariants, especially for the Canadian HansardsTable 6: Effect of smoothing the lexicon prob-abilities on the alignment performance for theVerbmobil task (S?T: source-to-target direc-tion, smooth English; T?S: target-to-sourcedirection, smooth German; all numbers in per-cent).S?T T?SPre.
Rec.
AER Pre.
Rec.
AERBase 93.5 95.3 5.7 91.4 88.7 9.9smooth 94.8 94.8 5.2 93.4 88.2 9.1task.5.5 Lexicon SmoothingIn Table 6, we present the results for the lex-icon smoothing as described in Section 4 onthe Verbmobil corpus2.
As expected, a no-table improvement in the AER is reached ifthe lexicon smoothing is performed for Ger-man (i.e.
for the target-to-source direction),because many full-form words with the samebase form are present in this language.
Theseimprovements are statistically significant atthe 95% level.6 Related WorkThe popular IBM models for statistical ma-chine translation are described in (Brown etal., 1993).
The HMM-based alignment modelwas introduced in (Vogel et al, 1996).
Agood overview of these models is given in(Och and Ney, 2003).
In that article Model6 is introduced as the loglinear interpolationof the other models.
Additionally, state-of-the-art results are presented for the Verbmo-bil task and the Canadian Hansards task forvarious configurations.
Therefore, we chosethem as baseline.
Compared to our work,these publications kept the training of thetwo translation directions strictly separatewhereas we integrate both directions into onesymmetrized training.
Additional linguisticknowledge sources such as dependency treesor parse trees were used in (Cherry and Lin,2003) and (Gildea, 2003).
In (Cherry andLin, 2003) a probability model Pr(aJ1 |fJ1 , eI1) isused, which is symmetric per definition.
Bilin-gual bracketing methods were used to producea word alignment in (Wu, 1997).
(Melamed,2000) uses an alignment model that enforcesone-to-one alignments for nonempty words.
In2The base forms were determined using LingSofttools.Table 5: Effect of different lexicon symmetrization methods on alignment performance for thegeneralized alignments for the Verbmobil task and the Canadian Hansards task.task: Verbmobil Canadian HansardsPrecision[%] Recall[%] AER[%] Precision[%] Recall[%] AER[%]Base 93.3 96.0 5.5 96.6 86.0 8.2Lin.
96.1 94.0 4.9 95.2 88.5 7.7Loglin.
95.2 95.3 4.7 93.6 90.8 7.5(Toutanova et al, 2002), extensions to theHMM-based alignment model are presented.7 ConclusionsWe have addressed the task of automaticallygenerating word alignments for bilingual cor-pora.
This problem is of great importance formany tasks in natural language processing, es-pecially in the field of machine translation.We have presented lexicon symmetrizationmethods for statistical alignment models thatare trained using the EM algorithm, in par-ticular the five IBM models, the HMM andModel 6.
We have evaluated these meth-ods on the Verbmobil task and the Cana-dian Hansards task and compared our resultsto the state-of-the-art system of (Och andNey, 2003).
We have shown that both thelinear and the loglinear interpolation of lexi-con counts after each iteration of the EM al-gorithm result in statistically significant im-provements of the alignment quality.
For theCanadian Hansards task, the AER improvedby about 30% relative; for the Verbmobil taskthe improvement was about 25% relative.Additionally, we have described lexiconsmoothing using the word base forms.
Es-pecially for highly inflected languages such asGerman, this smoothing resulted in statisti-cally significant improvements.In the future, we plan to optimize the inter-polation weights to balance the two transla-tion directions.
We will also investigate thepossibility of generating directly an uncon-strained alignment based on the symmetrizedlexicon probabilities.AcknowledgmentThis work has been partially funded by theEU project LC-Star, IST-2001-32216.ReferencesP.
F. Brown, S. A. Della Pietra, V. J. Della Pietra,and R. L. Mercer.
1993.
The mathematics ofstatistical machine translation: Parameter esti-mation.
Computational Linguistics, 19(2):263?311, June.C.
Cherry and D. Lin.
2003.
A probability modelto improve word alignment.
In Proc.
of the 41thAnnual Meeting of the Association for Compu-tational Linguistics (ACL), pages 88?95, Sap-poro, Japan, July.D.
Gildea.
2003.
Loosely tree-based alignment formachine translation.
In Proc.
of the 41th An-nual Meeting of the Association for Computa-tional Linguistics (ACL), pages 80?87, Sapporo,Japan, July.I.
D. Melamed.
2000.
Models of translationalequivalence among words.
Computational Lin-guistics, 26(2):221?249.H.
Ney, S. Martin, and F. Wessel.
1997.
Statisti-cal language modeling using leaving-one-out.
InS.
Young and G. Bloothooft, editors, Corpus-Based Methods in Language and Speech Process-ing, pages 174?207.
Kluwer.F.
J. Och and H. Ney.
2000.
Improved statisticalalignment models.
In Proc.
of the 38th AnnualMeeting of the Association for ComputationalLinguistics (ACL), pages 440?447, Hong Kong,October.F.
J. Och and H. Ney.
2003.
A systematic com-parison of various statistical alignment models.Computational Linguistics, 29(1):19?51, March.K.
Toutanova, H. T. Ilhan, and C. D. Manning.2002.
Extensions to hmm-based statistical wordalignment models.
In Proc.
Conf.
on EmpiricalMethods for Natural Language Processing, pages87?94, Philadelphia, PA, July.S.
Vogel, H. Ney, and C. Tillmann.
1996.
HMM-based word alignment in statistical translation.In COLING ?96: The 16th Int.
Conf.
on Com-putational Linguistics, pages 836?841, Copen-hagen, Denmark, August.W.
Wahlster, editor.
2000.
Verbmobil: Founda-tions of speech-to-speech translations.
SpringerVerlag, Berlin, Germany, July.D.
Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel cor-pora.
Computational Linguistics, 23(3):377?403, September.
