Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 82?87,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsImproved Features and Grammar Selection for Syntax-Based MTGreg Hanneman and Jonathan Clark and Alon LavieLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213 USA{ghannema, jhclark, alavie}@cs.cmu.eduAbstractWe present the Carnegie Mellon Univer-sity Stat-XFER group submission to theWMT 2010 shared translation task.
Up-dates to our syntax-based SMT systemmainly fell in the areas of new feature for-mulations in the translation model and im-proved filtering of SCFG rules.
Comparedto our WMT 2009 submission, we reporta gain of 1.73 BLEU by using the newfeatures and decoding environment, and again of up to 0.52 BLEU from improvedgrammar selection.1 IntroductionFrom its earlier focus on linguistically rich ma-chine translation for resource-poor languages, thestatistical transfer MT group at Carnegie MellonUniversity has expanded in recent years to the in-creasingly successful domain of syntax-based sta-tistical MT in large-data scenarios.
Our submis-sion to the 2010 Workshop on Machine Transla-tion is a syntax-based SMT system with a syn-chonous context-free grammar (SCFG), where theSCFG rules are derived from full constituencyparse trees on both the source and target sides ofparallel training sentences.
We participated in theFrench-to-English shared translation task.This year, we focused our efforts on makingmore and better use of syntactic grammar.
Muchof the work went into formulating a more expan-sive feature set in the translation model and a newmethod of assigning scores to phrase pairs andgrammar rules.
Following a change of decoderthat allowed us to experiment with systems usingmuch larger syntactic grammars than previously,we also adapted a technique to more intelligentlypre-filter grammar rules to those most likely to beuseful.2 System OverviewWe built our system on a partial selection ofthe provided French?English training data, us-ing the Europarl, News Commentary, and UNsets, but ignoring the Giga-FrEn data.
Aftertokenization and some pruning of our trainingdata, this left us with a corpus of approximately8.6 million sentence pairs.
We word-aligned thecorpus with MGIZA++ (Gao and Vogel, 2008),a multi-threaded implementation of the standardword alignment tool GIZA++ (Och and Ney,2003).
Word alignments were symmetrized withthe ?grow-diag-final-and?
heuristic.
We automati-cally parsed the French side of the corpus with theBerkeley parser (Petrov and Klein, 2007), whilewe used the fast vanilla PCFG model of the Stan-ford parser (Klein and Manning, 2003) for theEnglish side.
These steps resulted in a parallelparsed corpus from which to extract phrase pairsand grammar rules.Phrase extraction involves three distinct steps.In the first, we perform standard (non-syntactic)phrase extraction according to the heuristics ofphrase-based SMT (Koehn et al, 2003).
In thesecond, we obtain syntactic phrase pairs usingthe tree-to-tree matching method of Lavie et al(2008).
Briefly, this method aligns nodes in par-allel parse trees by projecting up from the wordalignments.
A source-tree node s will be alignedto a target-tree node t if the word alignments in theyield of s all land within the yield of t, and viceversa.
This node alignment is similar in spirit tothe subtree alignment method of Zhechev and Way(2008), except our method is based on the spe-cific Viterbi word alignment links found for each82sentence rather than on the general word trans-lation probabilities computed for the corpus as awhole.
This enables us to use efficient dynamicprogramming to infer node alignments, rather thanresorting to a greedy search or the enumeration ofall possible alignments.
Finally, in the third step,we use the node alignments from syntactic phrasepair extraction to extract grammar rules.
Eachaligned node in a tree pair specifies a decompo-sition point for breaking the parallel trees into aseries of SCFG rules.
Like Galley et al (2006),we allow ?composed?
(non-minimal) rules whenthey build entirely on lexical items.
However, tocontrol the size of the grammar, we do not producecomposed rules that build on other non-terminals,nor do we produce multiple possible rules whenwe encounter unaligned words.
Another differ-ence is that we discard internal structure of com-posed lexical rules so that we produce SCFG rulesrather than synchronous tree substitution grammarrules.The extracted phrase pairs and grammar rulesare collected together and scored according to avariety of features (Section 3).
Instead of decod-ing with the very large complete set of extractedgrammar rules, we select only a small number ofrules meeting certain criteria (Section 4).In contrast to previous years, when we used theStat-XFER decoder, this year we switched to thethe Joshua decoder (Li et al, 2009) to take advan-tage of its more efficient architecture and imple-mentation of modern decoding techniques, such ascube pruning and multi-threading.
We also man-aged system-building workflows with LoonyBin(Clark and Lavie, 2010), a toolkit for managingmulti-step experiments across different servers orcomputing clusters.
Section 5 details our experi-mental results.3 Translation Model ConstructionOne major improvement in our system this yearis the feature scores we applied to our grammarand phrase pairs.
Inspired largely by the Syntax-Augmented MT system (Zollmann and Venu-gopal, 2006), our translation model contains 22features in addition to the language model.
In con-trast to earlier formulations of our features (Han-neman and Lavie, 2009), our maximum-likelihoodfeatures are now based on a strict separation be-tween counts drawn from non-syntactic phrase ex-traction heuristics and our syntactic rule extractor;no feature is estimated from counts in both spaces.We define an aggregate rule instance as a 5-tuple r = (L,S, T,Cphr, Csyn) that contains aleft-hand-side label L, a sequence of terminalsand non-terminals for the source (S) and target(T ) right-hand sides, and aggregated counts fromphrase-based SMT extraction heuristics Cphr andthe syntactic rule extractor Csyn.In preparation for feature scoring, we:1.
Run phrase instance extraction using stan-dard phrase-based SMT heuristics to obtaintuples (PHR, S, T,Cphr, ?)
where S and Tnever contain non-terminals2.
Run syntactic rule instance extraction as de-scribed in Section 2 above to obtain tuples(L,S, T, ?, Csyn)3.
Share non-syntactic counts such that, forany two tuples r1 = (PHR, S, T,Cphr, ?
)and r2 = (L2, S, T, ?, Csyn) with equiv-alent S and T values, we produce r2 =(L2, S, T,Cphr, Csyn)Note that there is no longer any need to retainPHR rules (PHR, S, T ) that have syntactic equiv-alents (L 6= PHR, S, T ) since they have the samefeatures In addition, we assume there will be notuples where S and T contain non-terminals whileCphr = 0 and Csyn > 0.
That is, the syntacticphrases are a subset of non-syntactic phrases.3.1 Maximum-Likelihood FeaturesOur most traditional features are Pphr(T |S) andPphr(S |T ), estimated using only counts Cphr.These features apply only to rules not con-taining any non-terminals.
They are equiva-lent to the phrase P (T |S) and P (S |T ) fea-tures from the Moses decoder, even when L 6=PHR.
In contrast, we used Psyn?phr(L,S |T ) andPsyn?phr(L, T |S) last year, which applied to allrules.
The new features are no longer subject toincreased sparsity as the number of non-terminalsin the grammar increases.We also have grammar rule probabili-ties Psyn(T |S), Psyn(S |T ), Psyn(L |S),Psyn(L |T ), and Psyn(L |S, T ) estimated usingCsyn; these apply only to rules where S and Tcontain non-terminals.
By no longer includingcounts from phrase-based SMT extraction heuris-tics in these features, we encourage rules whereL 6= PHR since the smaller counts from the rulelearner would have otherwise been overshadowed83by the much larger counts from the phrase-basedSMT heuristics.Finally, we estimate ?not labelable?
(NL) fea-tures Psyn(NL |S) and Psyn(NL |T ).
With R de-noting the set of all extracted rules,Psyn(NL |S) =Csyn?r?
?R s.t.
S?=S C ?syn(1)Psyn(NL |T ) =Csyn?r?
?R s.t.
T ?=T C ?syn(2)We use additive smoothing (with n = 1 for our ex-periments) to avoid a probability of 0 when thereis no syntactic label for an (S, T ) pair.
These fea-tures can encourage syntactic rules when syntaxis likely given a particular string since probabilitymass is often distributed among several differentsyntactic labels.3.2 Instance FeaturesWe add several features that use sufficient statis-tics local to each rule.
First, we add three binarylow-count features that take on the value 1 whenthe frequency of the rule is exactly 1, 2, or 3.
Thereare also two indicator features related to syntax:one each that fires when L = PHR and whenL 6= PHR.
Other indicator features analyze theabstractness of grammar rules: AS = 1 when thesource side contains only non-terminals, AT = 1when the target side contains only non-terminals,TGTINSERTION = 1 when AS = 1, AT = 0,SRCDELETION = 1 when AS = 0, AT = 1, andINTERLEAVED = 1 when AS = 0, AT = 0.Bidirectional lexical probabilities for each ruleare calculated from a unigram lexicon MLE-estimated over aligned word pairs in the trainingcorpus, as is the default in Moses.Finally, we include a glue rule indicator featurethat fires whenever a glue rule is applied duringdecoding.
In the Joshua decoder, these monotonicrules stitch syntactic parse fragments together atno model cost.4 Grammar SelectionWith extracted grammars typically reaching tensof millions of unique rules ?
not to mentionphrase pairs ?
our systems clearly face an en-gineering challenge when attempting to includethe full grammar at decoding time.
Iglesias et al(2009) classified SCFG rules according to the pat-tern of terminals and non-terminals on the rules?right-hand sides, and found that certain patternscould be entirely left out of the grammar withoutloss of MT quality.
In particular, large classes ofmonotonic rules could be removed without a lossin automatic metric scores, while small classes ofreordering rules contributed much more to the suc-cess of the system.
Inspired by that approach, wepassed our full set of extracted grammar rule in-stances through a filter after scoring.
Using therule notation from Section 3, the filter retainedonly those rules that matched one of the follow-ing patterns:S = X1 w, T = w X1S = w X1, T = X1 wS = X1 X2, T = X2 X1S = X1 X2, T = X1 X2where X represents any non-terminal and w rep-resents any span of one or more terminals.
Thechoice of the specific reordering patterns abovecaptures our intuition that binary swaps are a fun-damental ordering divergence between languages,while the inclusion of the abstract monotonic pat-tern (X1 X2,X1 X2) ensures that the decoder isnot disproportionately biased towards applying re-ordering rules without supporting lexical evidencemerely because in-order rules are left out.Orthogonally to the pattern-based pruning, wealso selected grammars by sorting grammar rulesin decreasing order of frequency count and usingthe top n in the decoder.
We experimented withn = 0, 100, 1000, and 10,000.
In all cases ofgrammar selection, we disallowed rules that in-serted unaligned target-side terminals unless theinserted terminals were among the top 100 mostfrequent unigrams in the target-side vocabulary.5 Results and Analysis5.1 Comparison with WMT 2009 ResultsWe performed our initial development work onan updated version of our previous WMT sub-mission (Hanneman et al, 2009) so that the ef-fects of our changes could be directly compared.Our 2009 system was trained from the full Eu-roparl and News Commentary data available thatyear, plus the pre-release version of the Giga-FrEndata, for a total of 9.4 million sentence pairs.
Weused the news-dev2009a set for minimum error-rate training and tested system performance onnews-dev2009b.
To maintain continuity with ourpreviously reported scores, we report new scoreshere using the same training, tuning, and test-ing sets, using the uncased versions of IBM-style84System Configuration METEOR BLEU1.
WMT ?09 submission 0.5263 0.20732.
Joshua decoder 0.5231 0.21583.
New TM features 0.5348 0.2246Table 1: Dev test results (on news-dev2009b) fromour WMT 2009 system when updating decodingenvironment and feature formulations.System Configuration METEOR BLEU1.
n = 100 0.5314 0.22002. n = 100, filtered 0.5341 0.22423. n = 1000 0.5324 0.22064. n = 1000, filtered 0.5330 0.22335. n = 10,000 0.5332 0.21986. n = 10,000, filtered 0.5350 0.2250Table 2: Dev test results (on news-dev2009b) fromour WMT 2009 system with and without pattern-based grammar selection.BLEU 1.04 (Papineni et al, 2002) and METEOR0.6 (Lavie and Agarwal, 2007).Table 1 shows the effect of our new scoring anddecoding environment.
Line 2 uses the same ex-tracted phrase pairs and grammar rules as line 1,but the system is tuned and tested with the Joshuadecoder instead of Stat-XFER.
For line 3, we re-scored the extracted phrase pairs from lines 1 and2 using the updated features discussed in Sec-tion 3.1 The difference in automatic metric scoresshows a significant benefit from both the new de-coder and the updated feature formulations: 0.8BLEU points from the change in decoder, and 0.9BLEU points from the expanded set of 22 transla-tion model features.Our next test was to examine the usefulness ofthe pattern-based grammar selection described inSection 4.
For various numbers of rules n, Ta-ble 2 shows the scores obtained with and withoutfiltering the grammar before the n most frequentrules are skimmed off for use.
We observe a smallbut consistent gain in scores from the grammar se-lection process, up to half a BLEU point in thelargest-grammar systems (lines 5 and 6).1In line 2, we did not control for difference in formulationof the translation length feature: Stat-XFER uses a lengthratio, while Joshua uses a target word count.
Line 3 doesnot include 26 manually selected grammar rules present inlines 1 and 2; this is because our new feature scoring requiresinformation from the grammar rules that was not present inour 2009 extracted resources.Source Targetun ro?le AP1 ADJP1 rolesl?
instabilite?
AP1 ADJP1 instabilityl?
argent PP1 NP1 moneyune pression AP1 ADJP1 pressurela gouvernance AP1 ADJP1 governancela concurrence AP1 ADJP1 competitiondes preuves AP1 ADJP1 evidenceles outils AP1 ADJP1 toolsdes changements AP1 ADJP1 changesTable 3: Rules fitting the pattern (S = w X1, T =X1 w) that applied on the news-test2010 test set.5.2 WMT 2010 Results and AnalysisWe built the WMT 2010 version of our systemfrom the training data described in Section 2.
(Thesystem falls under the strictly constrained track:we used neither the Giga-FrEn data for trainingnor the LDC Gigaword corpora for language mod-eling.)
We used the provided news-test2008 setfor system tuning, while news-test2009 servedas our 2010 dev test set.
Based on the resultsin Table 2, our official submission to this year?sshared task was constructed as in line 6, with10,000 syntactic grammar rules chosen after apattern-based grammar selection step.
On thenews-test2010 test set, this system scored 0.2327on case-insensitive IBM-style BLEU 1.04, 0.5614on METEOR 0.6, and 0.5519 on METEOR 1.0(Lavie and Denkowski, 2009).The actual application of grammar rules in thesystem is quite surprising.
Despite having a gram-mar of 10,000 rules at its disposal, the decoderchose to only apply a total of 20 unique rulesin 392 application instances in the 2489-sentencenews-test2010 set.
On a per-sentence basis, thisis actually fewer rule applications than our sys-tem performed last year with a 26-rule handpickedgrammar!
The most frequently applied rules arefully abstract, monotonic structure-building rules,such as for stitching together compound nounphrases with adverbial phrases or prepositionalphrases.
Nine of the 20 rules, listed in Table 3,demonstrate the effect of our pattern-based gram-mar selection.
These partially lexicalized rules fitthe pattern (S = w X1, T = X1 w) and han-dle cases of lexicalized binary reordering betweenFrench and English.
Though the overall impact ofthese rules on automatic metric scores is presum-85ably quite small, we believe that the key to effec-tive syntactic grammars in our MT approach liesin retaining precise rules of this type for commonlinguistically motivated reordering patterns.The above pattern of rule applications is alsoobserved in our dev test set, news-test2009, where16 distinct rules apply a total of 352 times.
Sevenof the fully abstract rules and three of the lexical-ized rules that applied on news-test2009 also ap-plied on news-test2010, while a further two ab-stract and four lexicalized rules applied on news-test2009 alone.
We thus have a general trend of aset of general rules applying with higher frequencyacross test sets, while the set of lexicalized rulesused varies according to the particular set.Since, overall, we still do not see as much gram-mar application in our systems as we would like,we plan to concentrate future work on further im-proving this aspect.
This includes a more detailedstudy of grammar filtering or refinement to selectthe most useful rules.
We would also like to ex-plore the effect of the features of Section 3 individ-ually, on different language pairs, and using differ-ent grammar types.AcknowledgmentsThis research was supported in part by NSF grantIIS-0534217 (LETRAS) and the DARPA GALEprogram.
We thank Yahoo!
for the use of the M45research computing cluster, where we ran manysteps of our experimental pipeline.ReferencesJonathan Clark and Alon Lavie.
2010.
LoonyBin:Keeping language technologists sane throughautomated management of experimental (hy-per)workflows.
In Proceedings of the SeventhInternational Language Resources and Evaluation(LREC ?10), Valletta, Malta, May.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Pro-ceedings of the 21st International Conference onComputational Linguistics and 44th Annual Meet-ing of the ACL, pages 961?968, Sydney, Australia,July.Qin Gao and Stephan Vogel.
2008.
Parallel implemen-tations of word alignment tool.
In Software Engi-neering, Testing, and Quality Assurance for NaturalLanguage Processing, pages 49?57, Columbus, OH,June.Greg Hanneman and Alon Lavie.
2009.
Decodingwith syntactic and non-syntactic phrases in a syntax-based machine translation system.
In Proceedings ofthe Third Workshop on Syntax and Structure in Sta-tistical Translations, pages 1?9, Boulder, CO, June.Greg Hanneman, Vamshi Ambati, Jonathan H. Clark,Alok Parlikar, and Alon Lavie.
2009.
An improvedstatistical transfer systems for French?English ma-chine translation.
In Proceedings of the FourthWorkshop on Statistical Machine Translation, pages140?144, Athens, Greece, March.Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,and William Byrne.
2009.
Rule filtering by patternfor efficient hierarchical translation.
In Proceedingsof the 12th Conference of the European Chapter ofthe ACL, pages 380?388, Athens, Greece, March?April.Dan Klein and Christopher D. Manning.
2003.
Fastexact inference with a factored model for naturallanguage parsing.
In Advances in Neural Informa-tion Processing Systems 15, pages 3?10.
MIT Press,Cambridge, MA.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of HLT-NAACL 2003, pages 48?54, Ed-monton, Alberta, May?June.Alon Lavie and Abhaya Agarwal.
2007.
METEOR:An automatic metric for MT evaluation with highlevels of correlation with human judgments.
In Pro-ceedings of the Second Workshop on Statistical Ma-chine Translation, pages 228?231, Prague, CzechRepublic, June.Alon Lavie and Michael J. Denkowski.
2009.
The ME-TEOR metric for automatic evaluation of machinetranslation.
Machine Translation, 23(2?3):105?115.Alon Lavie, Alok Parlikar, and Vamshi Ambati.
2008.Syntax-driven learning of sub-sentential translationequivalents and translation rules from parsed parallelcorpora.
In Proceedings of the Second ACL Work-shop on Syntax and Structure in Statistical Transla-tion, pages 87?95, Columbus, OH, June.Zhifei Li, Chris Callison-Burch, Chris Dyer, JuriGanitkevitch, Sanjeev Khudanpur, Lane Schwartz,Wren N.G.
Thornton, Jonathan Weese, and Omar F.Zaidan.
2009.
Joshua: An open source toolkitfor parsing-based machine translation.
In Proceed-ings of the Fourth Workshop on Statistical Ma-chine Translation, pages 135?139, Athens, Greece,March.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A method for automaticevalution of machine translation.
In Proceedings of86the 40th Annual Meeting of the Association for Com-putational Linguistics, pages 311?318, Philadelphia,PA, July.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In Proceedings of NAACLHLT 2007, pages 404?411, Rochester, NY, April.Ventsislav Zhechev and Andy Way.
2008.
Automaticgeneration of parallel treebanks.
In Proceedingsof the 22nd International Conference on Compu-tational Linguistics, pages 1105?1112, Manchester,England, August.Andreas Zollmann and Ashish Venugopal.
2006.
Syn-tax augmented machine translation via chart parsing.In Proceedings of the Workshop on Statistical Ma-chine Translation, pages 138?141, New York, NY,June.87
