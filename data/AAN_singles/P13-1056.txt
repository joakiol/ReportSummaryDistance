Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 572?582,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsModels of Semantic Representation with Visual AttributesCarina Silberer, Vittorio Ferrari, Mirella LapataSchool of Informatics, University of Edinburgh10 Crichton Street, Edinburgh EH8 9ABc.silberer@ed.ac.uk, vferrari@inf.ed.ac.uk, mlap@inf.ed.ac.ukAbstractWe consider the problem of grounding themeaning of words in the physical worldand focus on the visual modality which werepresent by visual attributes.
We createa new large-scale taxonomy of visual at-tributes covering more than 500 conceptsand their corresponding 688K images.
Weuse this dataset to train attribute classi-fiers and integrate their predictions withtext-based distributional models of wordmeaning.
We show that these bimodalmodels give a better fit to human word as-sociation data compared to amodal modelsand word representations based on hand-crafted norming data.1 IntroductionRecent years have seen increased interest ingrounded language acquisition, where the goal isto extract representations of the meaning of nat-ural language tied to the physical world.
Thelanguage grounding problem has assumed sev-eral guises in the literature such as semantic pars-ing (Zelle and Mooney, 1996; Zettlemoyer andCollins, 2005; Kate and Mooney, 2007; Lu etal., 2008; Bo?rschinger et al, 2011), mapping nat-ural language instructions to executable actions(Branavan et al, 2009; Tellex et al, 2011), associ-ating simplified language to perceptual data suchas images or video (Siskind, 2001; Roy and Pent-land, 2002; Gorniak and Roy, 2004; Yu and Bal-lard, 2007), and learning the meaning of wordsbased on linguistic and perceptual input (Bruniet al, 2012b; Feng and Lapata, 2010; Johns andJones, 2012; Andrews et al, 2009; Silberer andLapata, 2012).In this paper we are concerned with the lattertask, namely constructing perceptually groundeddistributional models.
The motivation for modelsthat do not learn exclusively from text is twofold.From a cognitive perspective, there is mountingexperimental evidence suggesting that our inter-action with the physical world plays an impor-tant role in the way we process language (Barsa-lou, 2008; Bornstein et al, 2004; Landau et al,1998).
From an engineering perspective, the abil-ity to learn representations for multimodal data hasmany practical applications including image re-trieval (Datta et al, 2008) and annotation (Chaiand Hung, 2008), text illustration (Joshi et al,2006), object and scene recognition (Lowe, 1999;Oliva and Torralba, 2007; Fei-Fei and Perona,2005), and robot navigation (Tellex et al, 2011).One strand of research uses feature norms as astand-in for sensorimotor experience (Johns andJones, 2012; Andrews et al, 2009; Steyvers, 2010;Silberer and Lapata, 2012).
Feature norms are ob-tained by asking native speakers to write down at-tributes they consider important in describing themeaning of a word.
The attributes represent per-ceived physical and functional properties associ-ated with the referents of words.
For example,apples are typically green or red, round, shiny,smooth, crunchy, tasty, and so on; dogs have fourlegs and bark, whereas chairs are used for sit-ting.
Feature norms are instrumental in reveal-ing which dimensions of meaning are psychologi-cally salient, however, their use as a proxy for peo-ple?s perceptual representations can itself be prob-lematic (Sloman and Ripps, 1998; Zeigenfuse andLee, 2010).
The number and types of attributesgenerated can vary substantially as a function ofthe amount of time devoted to each concept.
It isnot entirely clear how people generate attributesand whether all of these are important for repre-senting concepts.
Finally, multiple participants arerequired to create a representation for each con-572cept, which limits elicitation studies to a smallnumber of concepts and the scope of any compu-tational model based on feature norms.Another strand of research focuses exclusivelyon the visual modality, even though the groundingproblem could involve auditory, motor, and hap-tic modalities as well.
This is not entirely sur-prising.
Visual input represents a major source ofdata from which humans can learn semantic rep-resentations of linguistic and non-linguistic com-municative actions (Regier, 1996).
Furthermore,since images are ubiquitous, visual data can begathered far easier than some of the other modali-ties.
Distributional models that integrate the visualmodality have been learned from texts and im-ages (Feng and Lapata, 2010; Bruni et al, 2012b)or from ImageNet (Deng et al, 2009), e.g., byexploiting the fact that images in this databaseare hierarchically organized according to WordNetsynsets (Leong and Mihalcea, 2011).
Images aretypically represented on the basis of low-level fea-tures such as SIFT (Lowe, 2004), whereas textsare treated as bags of words.Our work also focuses on images as a wayof physically grounding the meaning of words.We, however, represent them by high-level vi-sual attributes instead of low-level image fea-tures.
Attributes are not concept or category spe-cific (e.g., animals have stripes and so do cloth-ing items; balls are round, and so are oranges andcoins), and thus allow us to express similaritiesand differences across concepts more easily.
Fur-thermore, attributes allow us to generalize to un-seen objects; it is possible to say something aboutthem even though we cannot identify them (e.g., ithas a beak and a long tail).
We show that thisattribute-centric approach to representing imagesis beneficial for distributional models of lexicalmeaning.
Our attributes are similar to those pro-vided by participants in norming studies, however,importantly they are learned from training data (adatabase of images and their visual attributes) andthus generalize to new images without additionalhuman involvement.In the following we describe our efforts to cre-ate a new large-scale dataset that consists of 688Kimages that match the same concrete conceptsused in the feature norming study of McRae et al(2005).
We derive a taxonomy of 412 visual at-tributes and explain how we learn attribute clas-sifiers following recent work in computer vision(Lampert et al, 2009; Farhadi et al, 2009).
Next,we show that this attribute-based image represen-tation can be usefully integrated with textual datato create distributional models that give a better fitto human word association data over models thatrely on human generated feature norms.2 Related WorkGrounding semantic representations with visualinformation is an instance of multimodal learn-ing.
In this setting the data consists of multipleinput modalities with different representations andthe learner?s objective is to extract a unified repre-sentation that fuses the modalities together.
Theliterature describes several successful approachesto multimodal learning using different variants ofdeep networks (Ngiam et al, 2011; Srivastava andSalakhutdinov, 2012) and data sources includingtext, images, audio, and video.Special-purpose models that address the fusionof distributional meaning with visual informationhave been also proposed.
Feng and Lapata (2010)represent documents and images by a commonmultimodal vocabulary consisting of textual wordsand visual terms which they obtain by quantizingSIFT descriptors (Lowe, 2004).
Their model is es-sentially Latent Dirichlet Allocation (LDA, Blei etal., 2003) trained on a corpus of multimodal docu-ments (i.e., BBC news articles and their associatedimages).
Meaning in this model is represented asa vector whose components correspond to word-topic distributions.
A related model has been pro-posed by Bruni et al (2012b) who obtain distinctrepresentations for the textual and visual modali-ties.
Specifically, they extract a visual space fromimages contained in the ESP-Game data set (vonAhn and Dabbish, 2004) and a text-based seman-tic space from a large corpus collection totalingapproximately two billion words.
They concate-nate the two modalities and subsequently projectthem to a lower-dimensionality space using Sin-gular Value Decomposition (Golub et al, 1981).Traditionally, computer vision algorithms de-scribe visual phenomena (e.g., objects, scenes,faces, actions) by giving each instance a categor-ical label (e.g., cat, beer garden, Brad Pitt, drink-ing).
The ability to describe images by their at-tributes allows to generalize to new instances forwhich there are no training examples available.Moreover, attributes can transcend category andtask boundaries and thus provide a generic de-scription of visual data.Initial work (Ferrari and Zisserman, 2007)573focused on simple color and texture attributes(e.g., blue, stripes) and showed that these can belearned in a weakly supervised setting from im-ages returned by a search engine when using theattribute as a query.
Farhadi et al (2009) wereamong the first to use visual attributes in an ob-ject recognition task.
Using an inventory of 64 at-tribute labels, they developed a dataset of approx-imately 12,000 instances representing 20 objectsfrom the PASCAL Visual Object Classes Chal-lenge 2008 (Everingham et al, 2008).
Visualsemantic attributes (e.g., hairy, four-legged) wereused to identify familiar objects and to describeunfamiliar objects when new images and bound-ing box annotations were provided.
Lampert et al(2009) showed that attribute-based representationscan be used to classify objects when there are notraining examples of the target classes available.Their dataset contained over 30,000 images repre-senting 50 animal concepts and used 85 attributesfrom the norming study of Osherson et al (1991).Attribute-based representations have also been ap-plied to the tasks of face detection (Kumar et al,2009), action identification (Liu et al, 2011), andscene recognition (Patterson and Hays, 2012).The use of visual attributes in models of distri-butional semantics is novel to our knowledge.
Weargue that they are advantageous for two reasons.Firstly, they are cognitively plausible; humans em-ploy visual attributes when describing the proper-ties of concept classes.
Secondly, they occupy themiddle ground between non-linguistic low-levelimage features and linguistic words.
Attributescrucially represent image properties, however bybeing words themselves, they can be easily inte-grated in any text-based distributional model thuseschewing known difficulties with rendering im-ages into word-like units.A key prerequisite in describing images bytheir attributes is the availability of training datafor learning attribute classifiers.
Although ourdatabase shares many features with previous work(Lampert et al, 2009; Farhadi et al, 2009) it dif-fers in focus and scope.
Since our goal is todevelop distributional models that are applicableto many words, it contains a considerably largernumber of concepts (i.e., more than 500) and at-tributes (i.e., 412) based on a detailed taxonomywhich we argue is cognitively plausible and ben-eficial for image and natural language processingtasks.
Our experiments evaluate a number of mod-els previously proposed in the literature and inAttribute Categories Example Attributescolor patterns (25) is red, has stripesdiet (35) eats nuts, eats grassshape size (16) is small, is chubbyparts (125) has legs, has wheelsbotany;anatomy (25;78) has seeds, has furbehavior (in)animate (55) flies, waddles, peckstexture material (36) made of metal, is shinystructure (3) 2 pieces, has pleatsTable 1: Attribute categories and examples of at-tribute instances.
Parentheses denote the numberof attributes per category.all cases show that the attribute-based represen-tation brings performance improvements over justusing the textual modality.
Moreover, we showthat automatically computed attributes are compa-rable and in some cases superior to those providedby humans (e.g., in norming studies).3 The Attribute DatasetConcepts and Images We created a dataset ofimages and their visual attributes for the nounscontained in McRae et al?s (2005) feature norms.The norms cover a wide range of concrete con-cepts including animate and inanimate things(e.g., animals, clothing, vehicles, utensils, fruits,and vegetables) and were collected by presentingparticipants with words and asking them to listproperties of the objects to which the words re-ferred.
To avoid confusion, in the remainder ofthis paper we will use the term attribute to refer toproperties of concepts and the term feature to referto image features, such as color or edges.Images for the concepts in McRae et al?s (2005)production norms were harvested from ImageNet(Deng et al, 2009), an ontology of images basedon the nominal hierarchy of WordNet (Fellbaum,1998).
ImageNet has more than 14 million im-ages spanning 21K WordNet synsets.
We chosethis database due to its high coverage and the highquality of its images (i.e., cleanly labeled and highresolution).
McRae et al?s norms contain 541 con-cepts out of which 516 appear in ImageNet1 andare represented by 688K images overall.
The av-erage number of images per concept is 1,310 withthe most popular being closet (2,149 images) andthe least popular prune (5 images).1Some words had to be modified in order to match the cor-rect synset, e.g., tank (container) was found as storage tank.574behavior eats, walks, climbs, swims, runsdiet drinks water, eats anythingshape size is tall, is largeanatomy has mouth, has head, has nose, has tail, has claws,has jaws, has neck, has snout, has feet, has tonguecolor patterns is black, is brown, is whitebotany has skin, has seeds, has stem, has leaves, has pulpcolor patterns purple, white, green, has green topshape size is oval, is longtexture material is shinybehavior rollsparts has step through frame, has fork, has 2 wheels, has chain, has pedalshas gears, has handlebar, has bell, has breaks has seat, has spokestexture material made of metalcolor patterns different colors, is black, is red, is grey, is silverTable 2: Human-authored attributes for bear, eggplant, and bike.The images depicting each concept were ran-domly partitioned into a training, development,and test set.
For most concepts the developmentset contained a maximum of 100 images and thetest set a maximum of 200 images.
Concepts withless than 800 images in total were split into 1/8test and development set each, and 3/4 training set.The development set was used for devising and re-fining our attribute annotation scheme.
The train-ing and test sets were used for learning and eval-uating, respectively, attribute classifiers (see Sec-tion 4).Attribute Annotation Our aim was to develop aset of visual attributes that are both discriminatingand cognitively plausible, i.e., humans would gen-erally use them to describe a concrete concept.
Asa starting point, we thus used the visual attributesfrom McRae et al?s (2005) norming study.
At-tributes capturing other primary sensory informa-tion (e.g., smell, sound), functional/motor proper-ties, or encyclopaedic information were not takeninto account.
For example, is purple is a valid vi-sual attribute for an eggplant, whereas a vegetableis not, since it cannot be visualized.
Collating allthe visual attributes in the norms resulted in a to-tal of 673 which we further modified and extendedduring the annotation process explained below.The annotation was conducted on a per-conceptrather than a per-image basis (as for example inFarhadi et al (2009)).
For each concept (e.g., bearor eggplant), we inspected the images in the devel-opment set and chose all McRae et al (2005) vi-sual attributes that applied.
If an attribute was gen-erally true for the concept, but the images did notprovide enough evidence, the attribute was never-theless chosen and labeled with <no evidence>.For example, a plum has a pit, but most images inImageNet show plums where only the outer partof the fruit is visible.
Attributes supported bythe image data but missing from the norms wereadded.
For example, has lights and has bumperare attributes of cars but are not included in thenorms.
Attributes were grouped in eight generalclasses shown in Table 1.
Annotation proceededon a category-by-category basis, e.g., first all food-related concepts were annotated, then animals, ve-hicles, and so on.
Two annotators (both co-authorsof this paper) developed the set of attributes foreach category.
One annotator first labeled con-cepts with their attributes, and the other annota-tor reviewed the annotations, making changes ifneeded.
Annotations were revised and comparedper category in order to ensure consistency acrossall concepts of that category.Our methodology is slightly different fromLampert et al (2009) in that we did not simplytransfer the attributes from the norms to the con-cepts in question but refined and extended themaccording to the visual data.
There are severalreasons for this.
Firstly, it makes sense to se-lect attributes corroborated by the images.
Sec-ondly, by looking at the actual images, we couldeliminate errors in McRae et al?s (2005) norms.For example, eight study participants erroneouslythought that a catfish has scales.
Thirdly, dur-ing the annotation process, we normalized syn-onymous attributes (e.g., has pit and has stone)and attributes that exhibited negligible variations575has 2 pieces, has pointed end, has strap, has thumb, has buckles, has heelshas shoe laces, has soles, is black, is brown, is white, made of leather, made of rubberclimbs, climbs trees, crawls, hops, jumps, eats, eats nuts, is small, has bushy tailhas 4 legs, has head, has neck, has nose, has snout, has tail, has clawshas eyes, has feet, has toes,diff colours, has 2 legs, has 2 wheels, has windshield, has floorboard, has stand, has tankhas mudguard, has seat, has exhaust pipe, has frame, has handlebar, has lights, has mirrorhas step-through frame, is black, is blue, is red, is white, made of aluminum, made of steelTable 3: Attribute predictions for sandals, squirrel, and motorcycle.in meaning (e.g., has stem and has stalk).
Finally,our aim was to collect an exhaustive list of vi-sual attributes for each concept which is consis-tent across all members of a category.
This is un-fortunately not the case in McRae et al?s norms.Participants were asked to list up to 14 differentproperties that describe a concept.
As a result, theattributes of a concept denote the set of propertieshumans consider most salient.
For example, both,lemons and oranges have pulp.
But the norms pro-vide this attribute only for the second concept.On average, each concept was annotated with19 attributes; approximately 14.5 of these werenot part of the semantic representation created byMcRae et al?s (2005) participants for that con-cept even though they figured in the representa-tions of other concepts.
Furthermore, on averagetwo McRae et al attributes per concept were dis-carded.
Examples of concepts and their attributesfrom our database2 are shown in Table 2.4 Attribute-based ClassificationFollowing previous work (Farhadi et al, 2009;Lampert et al, 2009) we learned one classifier perattribute (i.e., 350 classifiers in total).3 The train-ing set consisted of 91,980 images (with a maxi-mum of 350 images per concept).
We used an L2-regularized L2-loss linear SVM (Fan et al, 2008)to learn the attribute predictions.
We adopted thetraining procedure of Farhadi al.
(2009).4 To learna classifier for a particular attribute, we used allimages in the training data.
Images of conceptsannotated with the attribute were used as positiveexamples, and the rest as negative examples.
The2Available from http://homepages.inf.ed.ac.uk/mlap/index.php?page=resources.3We only trained classifiers for attributes corroborated bythe images and excluded those labeled with <no evidence>.4http://vision.cs.uiuc.edu/attributes/data was randomly split into a training and valida-tion set of equal size in order to find the optimalcost parameter C. The final SVM for the attributewas trained on the entire training data, i.e., on allpositive and negative examples.The SVM learners used the four different fea-ture types proposed in Farhadi et al (2009),namely color, texture, visual words, and edges.Texture descriptors were computed for each pixeland quantized to the nearest 256 k-means centers.Visual words were constructed with a HOG spa-tial pyramid.
HOG descriptors were quantizedinto 1000 k-means centers.
Edges were detectedusing a standard Canny detector and their orien-tations were quantized into eight bins.
Color de-scriptors were sampled for each pixel and quan-tized to the nearest 128 k-means centers.
Shapesand locations were represented by generating his-tograms for each feature type for each cell in a gridof three vertical and horizontal blocks.
Our clas-sifiers used 9,688 features in total.
Table 3 showstheir predictions for three test images.Note that attributes are predicted on an image-by-image basis; our task, however, is to describe aconcept w by its visual attributes.
Since conceptsare represented by many images we must some-how aggregate their attributes into a single repre-sentation.
For each image iw ?
Iw of concept w,we output an F-dimensional vector containing pre-diction scores scorea(iw) for attributes a = 1, ...,F.We transform these attribute vectors into a singlevector pw ?
[0,1]1?F , by computing the centroidof all vectors for concept w. The vector is nor-malized to obtain a probability distribution overattributes given w:pw = (?iw?Iw scorea(iw))a=1,...,F?Fa=1?iw?Iw scorea(iw)(1)We additionally impose a threshold ?
on pw by set-5760.40.50.60.70.80.910  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1PrecisionRecallFigure 1: Attribute classifier performance for dif-ferent thresholds ?
(test set).ting each entry less than ?
to zero.Figure 1 shows the results of the attribute pre-diction on the test set on the basis of the computedcentroids; specifically, we plot recall against pre-cision based on threshold ?.5 Table 4 shows the10 nearest neighbors for five example conceptsfrom our dataset.
Again, we measure the cosinesimilarity between a concept and all other con-cepts in the dataset when these are represented bytheir visual attribute vector pw.5 Attribute-based Semantic ModelsWe evaluated the effectiveness of our attributeclassifiers by integrating their predictions with tra-ditional text-only models of semantic representa-tion.
These models have been previously proposedin the literature and were also described in a recentcomparative study (Silberer and Lapata, 2012).We represent the visual modality by attributevectors computed as shown in Equation (1).
Thelinguistic environment is approximated by textualattributes.
We used Strudel (Baroni et al, 2010)to obtain these attributes for the nouns in ourdataset.
Given a list of target words, Strudel ex-tracts weighted word-attribute pairs from a lem-matized and pos-tagged text corpus (e.g., egg-plant?cook-v, eggplant?vegetable-n).
The weightof each word-attribute pair is a log-likelihood ratioscore expressing the pair?s strength of association.In our experiments we learned word-attribute pairsfrom a lemmatized and pos-tagged (2009) dumpof the English Wikipedia.6 In the remainder ofthis section we will briefly describe the models we5Threshold values ranged from 0 to 0.9 with 0.1 stepsize.6The corpus can be downloaded from http://wacky.sslmit.unibo.it/doku.php?id=corpora.Concept Nearest Neighborsboat ship, sailboat, yacht, submarine, canoe,whale, airplane, jet, helicopter, tank (army)rooster chicken, turkey, owl, pheasant, peacock, stork,pigeon, woodpecker, dove, ravenshirt blouse, robe, cape, vest, dress, coat, jacket,skirt, camisole, nightgownspinach lettuce, parsley, peas, celery, broccoli, cab-bage, cucumber, rhubarb, zucchini, asparagussquirrel chipmunk, raccoon, groundhog, gopher, por-cupine, hare, rabbit, fox, mole, emuTable 4: Ten most similar concepts computed onthe basis of averaged attribute vectors and orderedaccording to cosine similarity.used in our study and how the textual and visualmodalities were fused to create a joint representa-tion.Concatenation Model Variants of this modelwere originally proposed in Bruni et al (2011)and Johns and Jones (2012).
Let T ?
RN?D de-note a term-attribute co-occurrence matrix, whereeach cell records a weighted co-occurrence scoreof a word and a textual attribute.
Let P ?
[0,1]N?Fdenote a visual matrix, representing a probabilitydistribution over visual attributes for each word.A word?s meaning can be then represented by theconcatenation of its normalized textual and visualvectors.Canonical Correlation Analysis The secondmodel uses Canonical Correlation Analysis (CCA,Hardoon et al (2004)) to learn a joint semanticrepresentation from the textual and visual modali-ties.
Given two random variables x and y (or twosets of vectors), CCA can be seen as determiningtwo sets of basis vectors in such a way, that the cor-relation between the projections of the variablesonto these bases is mutually maximized (Borga,2001).
In effect, the representation-specific de-tails pertaining to the two views of the same phe-nomenon are discarded and the underlying hiddenfactors responsible for the correlation are revealed.The linguistic and visual views are the same asin the simple concatenation model just explained.We use a kernelized version of CCA (Hardoon etal., 2004) that first projects the data into a higher-dimensional feature space and then performs CCAin this new feature space.
The two kernel matricesare KT = T T ?
and KP = PP?.
After applying CCAwe obtain two matrices projected onto l basis vec-tors, T?
?RN?l , resulting from the projection of the577textual matrix T onto the new basis and P?
?RN?l ,resulting from the projection of the correspondingvisual attribute matrix.
The meaning of a word isthen represented by T?
or P?.Attribute-topic Model Andrews et al (2009)present an extension of LDA (Blei et al, 2003)where words in documents and their associatedattributes are treated as observed variables thatare explained by a generative process.
Theidea is that each document in a document col-lection D is generated by a mixture of com-ponents {x1, ...,xc, ...,xC} ?
C , where a compo-nent xc comprises a latent discourse topic coupledwith an attribute cluster.
Inducing these attribute-topic components from D with the extended LDAmodel gives two sets of parameters: word prob-abilities given components PW (wi|X = xc) for wi,i = 1, ...,n, and attribute probabilities given com-ponents PA(ak|X = xc) for ak, k = 1, ...,F .
For ex-ample, most of the probability mass of a compo-nent x would be reserved for the words shirt, coat,dress and the attributes has 1 piece, has seams,made of material and so on.Word meaning in this model is represented bythe distribution PX |W over the learned compo-nents.
Assuming a uniform distribution over com-ponents xc in D , PX |W can be approximated as:PX=xc|W=wi =P(wi|xc)P(xc)P(wi) ?P(wi|xc)C?l=1P(wi|xl)(2)where C is the total number of components.In our work, the training data is a corpus D oftextual attributes (rather than documents).
Eachattribute is represented as a bag-of-concepts,i.e., words demonstrating the property expressedby the attribute (e.g., vegetable-n is a property ofeggplant, spinach, carrot).
For some of these con-cepts, our classifiers predict visual attributes.
Inthis case, the concepts are paired with one of theirvisual attributes.
We sample attributes for a con-cept w from their distribution given w (Eq.
(1)).6 Experimental SetupEvaluation Task We evaluated the distribu-tional models presented in Section 5 on theword association norms collected by Nelson et al(1998).7 These were established by presentinga large number of participants with a cue word(e.g., rice) and asking them to name an associate7From http://w3.usf.edu/FreeAssociation/.word in response (e.g., Chinese, wedding, food,white).
For each cue, the norms provide a setof associates and the frequencies with which theywere named.
We can thus compute the prob-ability distribution over associates for each cue.Analogously, we can estimate the degree of sim-ilarity between a cue and its associates using ourmodels.
The norms contain 63,619 unique cue-associate pairs.
Of these, 435 pairs were coveredby McRae et al (2005) and our models.
We alsoexperimented with 1,716 pairs that were not partof McRae et al?s study but belonged to conceptscovered by our attribute taxonomy (e.g., animals,vehicles), and were present in our corpus and Ima-geNet.
Using correlation analysis (Spearman?s ?
),we examined the degree of linear relationship be-tween the human cue-associate probabilities andthe automatically derived similarity values.8Parameter Settings In order to integrate the vi-sual attributes with the models described in Sec-tion 5 we must select the appropriate thresholdvalue ?
(see Eq.
(1)).
We optimized this valueon the development set and obtained best resultswith ?
= 0.
We also experimented with thresh-olding the attribute prediction scores and with ex-cluding attributes with low precision.
In bothcases, we obtained best results when using all at-tributes.
We could apply CCA to the vectors rep-resenting each image separately and then computea weighted centroid on the projected vectors.
Werefrained from doing this as it involves additionalparameters and assumes input different from theother models.
We measured the similarity betweentwo words using the cosine of the angle.
For theattribute-topic model, the number of predefinedcomponents C was set to 10.
In this model, sim-ilarity was measured as defined by Griffiths et al(2007).
The underlying idea is that word associa-tion can be expressed as a conditional distribution.With regard to the textual attributes, weobtained a 9,394-dimensional semantic spaceafter discarding word-attribute pairs with alog-likelihood ratio score less than 19.9 We alsodiscarded attributes co-occurring with less thantwo different words.8Previous work (Griffiths et al, 2007) which also predictsword association reports how many times the word with thehighest score under the model was the first associate in thehuman norms.
This evaluation metric assumes that there aremany associates for a given cue which unfortunately is notthe case in our study which is restricted to the concepts rep-resented in our attribute taxonomy.9Baroni et al (2010) use a similar threshold of 19.51.578Nelson Concat CCA TopicAttr TextAttrConcat 0.24CCA 0.30 0.72TopicAttr 0.26 0.55 0.28TextAttr 0.21 0.80 0.83 0.34VisAttr 0.23 0.65 0.52 0.40 0.39Table 5: Correlation matrix for seen Nelson et al(1998) cue-associate pairs and five distributionalmodels.
All correlation coefficients are statisti-cally significant (p < 0.01, N = 435).7 ResultsOur experiments were designed to answer fourquestions: (1) Do visual attributes improve theperformance of distributional models?
(2) Arethere performance differences among differentmodels, i.e., are some models better suited to theintegration of visual information?
(3) How docomputational models fare against gold standardnorming data?
(4) Does the attribute-based repre-sentation bring advantages over more conventionalapproaches based on raw image features?Our results are broken down into seen (Table 5)and unseen (Table 6) concepts.
The former areknown to the attribute classifiers and form partof our database, whereas the latter are unknownand are not included in McRae et al?s (2005)norms.
We report the correlation coefficients weobtain when human-derived cue-associate proba-bilities (Nelson et al, 1998) are compared againstthe simple concatenation model (Concat), CCA,and Andrews et al?s (2009) attribute-topic model(TopicAttr).
We also report the performance ofa distributional model that is based solely on theoutput of our attribute classifiers, i.e., without anytextual input (VisAttr) and conversely the perfor-mance of a model that uses textual informationonly (i.e., Strudel attributes) without any visual in-put (TextAttr).
The results are displayed as a cor-relation matrix so that inter-model correlations canalso be observed.As can be seen in Table 5 (second column), twomodalities are in most cases better than one whenevaluating model performance on seen data.
Dif-ferences in correlation coefficients between mod-els with two versus one modality are all statis-tically significant (p < 0.01 using a t-test), withthe exception of Concat when compared againstVisAttr.
It is also interesting to note that Topi-cAttr is the least correlated model when comparedagainst other bimodal models or single modali-Nelson Concat CCA TopicAttr TextAttrConcat 0.11CCA 0.15 0.66TopicAttr 0.17 0.69 0.48TextAttr 0.11 0.65 0.25 0.39VisAttr 0.13 0.57 0.87 0.57 0.34Table 6: Correlation matrix for unseen Nelsonet al (1998) cue-associate pairs and five distribu-tional models.
All correlation coefficients are sta-tistically significant (p < 0.01, N = 1,716).ties.
This indicates that the latent space obtainedby this model is most distinct from its constituentparts (i.e., visual and textual attributes).
Perhapsunsuprisingly Concat, CCA, VisAttr, and TextAttrare also highly intercorrelated.On unseen pairs (see Table 6), Concat faresworse than CCA and TopicAttr, achieving simi-lar performance to TextAttr.
CCA and TopicAttrare significantly better than TextAttr and VisAttr(p < 0.01).
This indicates that our attribute classi-fiers generalize well beyond the concepts found inour database and can produce useful visual infor-mation even on unseen images.
Compared to Con-cat and CCA, TopicAttr obtains a better fit with thehuman association norms on the unseen data.To answer our third question, we obtained dis-tributional models from McRae et al?s (2005)norms and assessed how well they predict Nelsonet al?s (1998) word-associate similarities.
Eachconcept was represented as a vector with dimen-sions corresponding to attributes generated by par-ticipants of the norming study.
Vector componentswere set to the (normalized) frequency with whichparticipants generated the corresponding attributewhen presented with the concept.
We measuredthe similarity between two words using the co-sine coefficient.
Table 7 presents results for dif-ferent model variants which we created by ma-nipulating the number and type of attributes in-volved.
The first model uses the full set of at-tributes present in the norms (All Attributes).
Thesecond model (Text Attributes) uses all attributesbut those classified as visual (e.g., functional, en-cyclopaedic).
The third model (Visual Attributes)considers solely visual attributes.We observe a similar trend as with our compu-tational models.
Taking visual attributes into ac-count increases the fit with Nelson?s (1998) associ-ation norms, whereas visual and textual attributeson their own perform worse.
Interestingly, CCA?s579Models SeenAll Attributes 0.28Text Attributes 0.20Visual Attributes 0.25Table 7: Model performance on seen Nelson etal.
(1998) cue-associate pairs; models are basedon gold human generated attributes (McRae et al,2005).
All correlation coefficients are statisticallysignificant (p < 0.01, N = 435).Models Seen UnseenConcat 0.22 0.10CCA 0.26 0.15TopicAttr 0.23 0.19TextAttr 0.20 0.08VisAttr 0.21 0.13MixLDA 0.16 0.11Table 8: Model performance on a subset of Nelsonet al (1998) cue-associate pairs.
Seen are conceptsknown to the attribute classifiers and covered byMixLDA (N = 85).
Unseen are concepts coveredby LDA but unknown to the attribute classifiers(N = 388).
All correlation coefficients are statisti-cally significant (p < 0.05).performance is comparable to the All Attributesmodel (see Table 5, second column), despite us-ing automatic attributes (both textual and visual).Furthermore, visual attributes obtained throughour classifiers (see Table 5) achieve a marginallylower correlation coefficient against human gener-ated ones (see Table 7).Finally, to address our last question, we com-pared our approach against Feng and Lapata(2010) who represent visual information via quan-tized SIFT features.
We trained their MixLDAmodel on their corpus consisting of 3,361 BBCnews documents and corresponding images (Fengand Lapata, 2008).
We optimized the model pa-rameters on a development set consisting of cue-associate pairs from Nelson et al (1998), exclud-ing the concepts in McRae et al (2005).
Weused a vocabulary of approximately 6,000 words.The best performing model on the development setused 500 visual terms and 750 topics and the asso-ciation measure proposed in Griffiths et al (2007).The test set consisted of 85 seen and 388 unseencue-associate pairs that were covered by our mod-els and MixLDA.Table 8 reports correlation coefficients for ourmodels and MixLDA against human probabili-ties.
All attribute-based models significantly out-perform MixLDA on seen pairs (p < 0.05 using at-test).
MixLDA performs on a par with the con-catenation model on unseen pairs, however CCA,TopicAttr, and VisAttr are all superior.
Althoughthese comparisons should be taken with a grainof salt, given that MixLDA and our models aretrained on different corpora (MixLDA assumesthat texts and images are collocated, whereas ourimages do not have collateral text), they seem toindicate that attribute-based information is indeedbeneficial.8 ConclusionsIn this paper we proposed the use of automaticallycomputed visual attributes as a way of physicallygrounding word meaning.
Our results demonstratethat visual attributes improve the performance ofdistributional models across the board.
On aword association task, CCA and the attribute-topicmodel give a better fit to human data when com-pared against simple concatenation and modelsbased on a single modality.
CCA consistently out-performs the attribute-topic model on seen data (itis in fact slightly better over a model that uses goldstandard human generated attributes), whereas theattribute-topic model generalizes better on unseendata (see Tables 5, 6, and 8).
Since the attribute-based representation is general and text-based weargue that it can be conveniently integrated withany type of distributional model or indeed othergrounded models that rely on low-level image fea-tures (Bruni et al, 2012a; Feng and Lapata, 2010)In the future, we would like to extend ourdatabase to actions and show that this attribute-centric representation is useful for more appliedtasks such as image description generation and ob-ject recognition.
Finally, we have only scratchedthe surface in terms of possible models for inte-grating the textual and visual modality.
Interest-ing frameworks which we plan to explore are deepbelief networks and Bayesian non-parametrics.ReferencesM.
Andrews, G. Vigliocco, and D. Vinson.
2009.Integrating Experiential and Distributional Data toLearn Semantic Representations.
Psychological Re-view, 116(3):463?498.M.
Baroni, B. Murphy, E. Barbu, and M. Poesio.2010.
Strudel: A Corpus-Based Semantic Model580Based on Properties and Types.
Cognitive Science,34(2):222?254.L.
W. Barsalou.
2008.
Grounded Cognition.
AnnualReview of Psychology, 59:617?845.D.
M. Blei, A. Y. Ng, and M. I. Jordan.
2003.
LatentDirichlet Allocation.
Journal of Machine LearningResearch, 3:993?1022, March.M.
Borga.
2001.
Canonical Correlation ?
a Tutorial,January.M.
H. Bornstein, L. R. Cote, S. Maital, K. Painter, S.-Y.Park, L. Pascual, M. G. Pe?cheux, J. Ruel, P. Venuti,and A. Vyt.
2004.
Cross-linguistic Analysis ofVocabulary in Young Children: Spanish, Dutch,French, Hebrew, Italian, Korean, and American En-glish.
Child Development, 75(4):1115?1139.B.
Bo?rschinger, B. K. Jones, and M. Johnson.
2011.Reducing Grounded Learning Tasks to GrammaticalInference.
In Proceedings of the 2011 Conferenceon Empirical Methods in Natural Language Pro-cessing, pages 1416?1425, Edinburgh, UK.S.R.K.
Branavan, H. Chen, L. S. Zettlemoyer, andR.
Barzilay.
2009.
Reinforcement Learning forMapping Instructions to Actions.
In Proceedings ofthe Joint Conference of the 47th Annual Meeting ofthe ACL and the 4th International Joint Conferenceon Natural Language Processing of the AFNLP,pages 82?90, Suntec, Singapore.E.
Bruni, G. Tran, and M. Baroni.
2011.
DistributionalSemantics from Text and Images.
In Proceedings ofthe GEMS 2011 Workshop on GEometrical Modelsof Natural Language Semantics, pages 22?32, Edin-burgh, UK.E.
Bruni, G. Boleda, M. Baroni, and N. Tran.
2012a.Distributional Semantics in Technicolor.
In Pro-ceedings of the 50th Annual Meeting of the Associa-tion for Computational Linguistics (Volume 1: LongPapers), pages 136?145, Jeju Island, Korea.E.
Bruni, J. Uijlings, M. Baroni, and N. Sebe.
2012b.Distributional semantics with eyes: Using im-age analysis to improve computational representa-tions of word meaning.
In Proceedings of the20th ACM International Conference on Multimedia,pages 1219?1228., New York, NY.C.
Chai and C. Hung.
2008.
Automatically AnnotatingImages with Keywords: A Review of Image Annota-tion Systems.
Recent Patents on Computer Science,1:55?68.R.
Datta, D. Joshi, J. Li, and J.
Z. Wang.
2008.
ImageRetrieval: Ideas, Influences, and Trends of the NewAge.
ACM Computing Surveys, 40(2):1?60.J.
Deng, W. Dong, R. Socher, L. Li, K. Li, and L. Fei-Fei.
2009.
ImageNet: A Large-Scale Hierarchi-cal Image Database.
In Proceedings of the IEEEComputer Society Conference on Computer Visionand Pattern Recognition, pages 248?255, Miami,Florida.M.
Everingham, L. Van Gool, C. K. I. Williams,J.
Winn, and A. Zisserman.
2008.
ThePASCAL Visual Object Classes Challenge2008 (VOC2008) Results.
http://www.pascal-network.org/challenges/VOC/voc2008/workshop.R.
Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin.2008.
LIBLINEAR: A Library for Large LinearClassification.
Journal of Machine Learning Re-search, 9:1871?1874.A.
Farhadi, I. Endres, D. Hoiem, and D. Forsyth.
2009.Describing Objects by their Attributes.
In Proceed-ings of the IEEE Computer Society Conference onComputer Vision and Pattern Recognition, pages1778?1785, Miami Beach, Florida.L.
Fei-Fei and P. Perona.
2005.
A Bayesian Hierarchi-cal Model for Learning Natural Scene Categories.
InProceedings of the IEEE Computer Society Confer-ence on Computer Vision and Pattern Recognition,pages 524?531, San Diego, California.C.
Fellbaum, editor.
1998.
WordNet: an ElectronicLexical Database.
MIT Press.Y.
Feng and M. Lapata.
2008.
Automatic image anno-tation using auxiliary text information.
In Proceed-ings of ACL-08: HLT, pages 272?280, Columbus,Ohio.Y.
Feng and M. Lapata.
2010.
Visual Informa-tion in Semantic Representation.
In Human Lan-guage Technologies: The 2010 Annual Conferenceof the North American Chapter of the Associationfor Computational Linguistics, pages 91?99, LosAngeles, California.
ACL.V.
Ferrari and A. Zisserman.
2007.
Learning VisualAttributes.
In J.C. Platt, D. Koller, Y.
Singer, andS.
Roweis, editors, Advances in Neural InformationProcessing Systems 20, pages 433?440.
MIT Press,Cambridge, Massachusetts.G.
H. Golub, F. T. Luk, and M. L. Overton.
1981.A block lanczoz method for computing the singularvalues and corresponding singular vectors of a ma-trix.
ACM Transactions on Mathematical Software,7:149?169.P.
Gorniak and D. Roy.
2004.
Grounded SemanticComposition for Visual Scenes.
Journal of ArtificialIntelligence Research, 21:429?470.T.
L. Griffiths, M. Steyvers, and J.
B. Tenenbaum.2007.
Topics in Semantic Representation.
Psycho-logical Review, 114(2):211?244.D.
R. Hardoon, S. R. Szedmak, and J. R. Shawe-Taylor.
2004.
Canonical Correlation Analysis: AnOverview with Application to Learning Methods.Neural Computation, 16(12):2639?2664.B.
T. Johns and M. N. Jones.
2012.
Perceptual Infer-ence through Global Lexical Similarity.
Topics inCognitive Science, 4(1):103?120.D.
Joshi, J.Z.
Wang, and J. Li.
2006.
The Story Pictur-ing Engine?A System for Automatic Text illustra-tion.
ACM Transactions on Multimedia Computing,Communications, and Applications, 2(1):68?89.581R.
J. Kate and R. J. Mooney.
2007.
Learning Lan-guage Semantics from Ambiguous Supervision.
InProceedings of the 22nd Conference on Artificial In-telligence, pages 895?900, Vancouver, Canada.N.
Kumar, A. C. Berg, P. N. Belhumeur, and S. K. Na-yar.
2009.
Attribute and Simile Classifiers for FaceVerification.
In Proceedings of the IEEE 12th In-ternational Conference on Computer Vision, pages365?372, Kyoto, Japan.C.
H. Lampert, H. Nickisch, and S. Harmeling.
2009.Learning To Detect Unseen Object Classes byBetween-Class Attribute Transfer.
In Computer Vi-sion and Pattern Recognition, pages 951?958, Mi-ami Beach, Florida.B.
Landau, L. Smith, and S. Jones.
1998.
Object Per-ception and Object Naming in Early Development.Trends in Cognitive Science, 27:19?24.C.
Leong and R. Mihalcea.
2011.
Going BeyondText: A Hybrid Image-Text Approach for MeasuringWord Relatedness.
In Proceedings of 5th Interna-tional Joint Conference on Natural Language Pro-cessing, pages 1403?1407, Chiang Mai, Thailand.J.
Liu, B. Kuipers, and S. Savarese.
2011.
RecognizingHuman Actions by Attributes.
In Proceedings of theIEEE Conference on Computer Vision and PatternRecognition, pages 3337?3344, Colorado Springs,Colorado.D.
G. Lowe.
1999.
Object Recognition from LocalScale-invariant Features.
In Proceedings of the In-ternational Conference on Computer Vision, pages1150?1157, Corfu, Greece.D.
Lowe.
2004.
Distinctive Image Features fromScale-invariant Keypoints.
International Journal ofComputer Vision, 60(2):91?110.W.
Lu, H. T. Ng, W.S.
Lee, and L. S. Zettlemoyer.2008.
A Generative Model for Parsing Natural Lan-guage to Meaning Representations.
In Proceedingsof the 2008 Conference on Empirical Methods inNatural Language Processing, pages 783?792, Hon-olulu, Hawaii.K.
McRae, G. S. Cree, M. S. Seidenberg, and C. Mc-Norgan.
2005.
Semantic Feature Production Normsfor a Large Set of Living and Nonliving Things.
Be-havior Research Methods, 37(4):547?559.D.
L. Nelson, C. L. McEvoy, and T. A. Schreiber.
1998.The University of South Florida Word Association,Rhyme, and Word Fragment Norms.J.
Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, andA.
Y. Ng.
2011.
Multimodal deep learning.
InProceedings of the 28th International Conference onMachine Leanring, pages 689?696, Bellevue, Wash-ington.A.
Oliva and A. Torralba.
2007.
The Role of Context inObject Recognition.
Trends in Cognitive Sciences,11(12):520?527.D.
N. Osherson, J. Stern, O. Wilkie, M. Stob, and E. E.Smith.
1991.
Default Probability.
Cognitive Sci-ence, 2(15):251?269.G.
Patterson and J. Hays.
2012.
SUN AttributeDatabase: Discovering, Annotating and Recogniz-ing Scene Attributes.
In Proceedings of the IEEEConference on Computer Vision and Pattern Recog-nition, pages 2751?2758, Providence, Rhode Island.T.
Regier.
1996.
The Human Semantic Potential.
MITPress, Cambridge, Massachusetts.D.
Roy and A. Pentland.
2002.
Learning Words fromSights and Sounds: A Computational Model.
Cog-nitive Science, 26(1):113?146.C.
Silberer and M. Lapata.
2012.
Grounded Mod-els of Semantic Representation.
In Proceedings ofthe 2012 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning, pages 1423?1433, JejuIsland, Korea.J.
M. Siskind.
2001.
Grounding the Lexical Semanticsof Verbs in Visual Perception using Force Dynamicsand Event Logic.
Journal of Artificial IntelligenceResearch, 15:31?90.S.
A. Sloman and L. J. Ripps.
1998.
Similarity as anExplanatory Construct.
Cognition, 65:87?101.N.
Srivastava and R. Salakhutdinov.
2012.
Multimodallearning with deep boltzmann machines.
In Pro-ceedings of the 26th Annual Conference on NeuralInformation Processing Systems, pages 2231?2239,Lake Tahoe, Nevada.M.
Steyvers.
2010.
Combining feature norms andtext data with topic models.
Acta Psychologica,133(3):234?342.S.
Tellex, T. Kollar, S. Dickerson, M. R. Walter,A.
Gopal Banerjee, S. Teller, and N. Roy.
2011.Understanding Natural Language Commands forRobotic Navigation and Manipulation.
In Proceed-ings of the 25th National Conference on ArtificialIntelligence, pages 1507?1514, San Francisco, Cali-fornia.L.
von Ahn and L. Dabbish.
2004.
Labeling imageswith a computer game.
In Proceeings of the HumanFactors in Computing Systems Conference, pages319?326, Vienna, Austria.C.
Yu and D. H. Ballard.
2007.
A Unified Model ofEarly Word Learning Integrating Statistical and So-cial Cues.
Neurocomputing, 70:2149?2165.M.
D. Zeigenfuse and M. D. Lee.
2010.
Finding theFeatures that Represent Stimuli.
Acta Psychologi-cal, 133(3):283?295.J.
M. Zelle and R. J. Mooney.
1996.
Learning to ParseDatabase Queries Using Inductive Logic Program-ming.
In Proceedings of the 13th National Con-ference on Artificial Intelligence, pages 1050?1055,Portland, Oregon.L.
S. Zettlemoyer and M. Collins.
2005.
Learning toMap Sentences to Logical Form: Structured Classi-fication with Probabilistic Categorial Grammars.
InProceedings of the Conference on Uncertainty in Ar-tificial Intelligence, pages 658?666, Edinburgh, UK.582
