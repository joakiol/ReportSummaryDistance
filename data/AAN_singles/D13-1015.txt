Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 141?151,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsStudying the recursive behaviour of adjectival modificationwith compositional distributional semanticsEva Maria Vecchi and Roberto Zamparelli and Marco BaroniCenter for Mind/Brain Sciences (University of Trento, Italy)(evamaria.vecchi|roberto.zamparelli|marco.baroni)@unitn.itAbstractIn this study, we use compositional distribu-tional semantic methods to investigate restric-tions in adjective ordering.
Specifically, wefocus on properties distinguishing Adjective-Adjective-Noun phrases in which there is flex-ibility in the adjective ordering from thosebound to a rigid order.
We explore a numberof measures extracted from the distributionalrepresentation of AAN phrases which may in-dicate a word order restriction.
We find thatwe are able to distinguish the relevant classesand the correct order based primarily on thedegree of modification of the adjectives.
Ourresults offer fresh insight into the semanticproperties that determine adjective ordering,building a bridge between syntax and distri-butional semantics.1 IntroductionA prominent approach for representing the meaningof a word in Natural Language Processing (NLP) isto treat it as a numerical vector that codes the pat-tern of co-occurrence of that word with other ex-pressions in a large corpus of language (Sahlgren,2006; Turney and Pantel, 2010).
This approach tosemantics (sometimes called distributional seman-tics) scales well to large lexicons and does not re-quire words to be manually disambiguated (Schu?tze,1997).
Until recently, however, this method hadbeen almost exclusively limited to the level of sin-gle content words (nouns, adjectives, verbs), and hadnot directly addressed the problem of composition-ality (Frege, 1892; Montague, 1970; Partee, 2004),the crucial property of natural language which al-lows speakers to derive the meaning of a complexlinguistic constituent from the meaning of its imme-diate syntactic subconstituents.Several recent proposals have strived to ex-tend distributional semantics with a component thatalso generates vectors for complex linguistic con-stituents, using compositional operations in the vec-tor space (Baroni and Zamparelli, 2010; Guevara,2010; Mitchell and Lapata, 2010; Grefenstette andSadrzadeh, 2011; Socher et al 2012).
All ofthese approaches construct distributional represen-tations for novel phrases starting from the corpus-derived vectors for their lexical constituents andexploiting the geometric quality of the representa-tion.
Such methods are able to capture complex se-mantic information of adjective-noun (AN) phrases,such as characterizing modification (Boleda et al2012; Boleda et al 2013), and can detect seman-tic deviance in novel phrases (Vecchi et al 2011).Furthermore, these methods are naturally recursive:they can derive a representation not only for, e.g.,red car, but also for new red car, fast new red car,etc.
This aspect is appealing since trying to extractmeaningful representations for all recursive phrasesdirectly from a corpus will result in a problem ofsparsity, since most large phrases will never occur inany finite sample.Once we start seriously looking into recursivemodification, however, the issue of modifier order-ing restrictions naturally arises.
Such restrictionshave often been discussed in the theoretical linguis-tic literature (Sproat and Shih, 1990; Crisma, 1991;Scott, 2002), and have become one of the key in-141gredients of the ?cartographic?
approach to syntax(Cinque, 2002).
In this paradigm, the ordering isderived by assigning semantically different classesof modifiers to the specifiers of distinct functionalprojections, whose sequence is hard-wired.
Whileit is accepted that in different languages movementcan lead to a principled rearrangement of the linearorder of the modifiers (Cinque, 2010; Steddy andSamek-Lodovici, 2011), one key assumption of thecartographic literature is that exactly one intonation-ally unmarked order for stacked adjectives shouldbe possible in languages like English.
The possi-bility of alternative orders, when discussed at all,is attributed to the presence of idioms (high Amer-ican building, but American high officer), to asyn-detic conjunctive meanings (e.g.
new creative ideaparsed as [new & creative] idea, rather than [new[creative idea]]), or to semantic category ambiguityfor any adjective which appears in different orders(see Cinque (2004) for discussion).In this study, we show that the existence of bothrigid and flexible order cases is robustly attested atleast for adjectival modification, and that flexible or-dering is unlikely to reduce to idioms, coordinationor ambiguity.
Moreover, we show that at least forsome recursively constructed adjective-adjective-noun phrases (AANs) we can extract meaning-ful representations from the corpus, approximatingthem reasonably well by means of compositionaldistributional semantic models, and that the seman-tic information contained in these models character-izes which AA will have rigid order (as with rapidsocial change vs. *social rapid change), or flexibleorder (e.g.
total estimated population vs. estimatedtotal population).
In the former case, we find thatthe same distributional semantic cues discriminatebetween correct and wrong orders.To achieve these goals, we consider variousproperties of the distributional representation ofAANs (both corpus-extracted and compositionally-derived), and explore their correlation with restric-tions in adjective ordering.
We conclude that mea-sures that quantify the degree to which the modifiershave an impact on the distributional meaning of theAAN can be good predictors of ordering restrictionsin AANs.2 Materials and methods2.1 Semantic spaceOur initial step was to construct a semantic space forour experiments, consisting of a matrix where eachrow represents the meaning of an adjective, noun,AN or AAN as a distributional vector, each columna semantic dimension of meaning.
We first introducethe source corpus, then the vocabulary of words andphrases that we represent in the space, and finally theprocedure adopted to build the vectors representingthe vocabulary items from corpus statistics, and ob-tain the semantic space matrix.
We work here with atraditional, window-based semantic space, since ourfocus is on the effect of different composition meth-ods given a common semantic space.
In addition,Blacoe and Lapata (2012) found that a vanilla spaceof this sort performed best in their composition ex-periments, when compared to a syntax-aware spaceand to neural language model vectors such as thoseused for composition by Socher et al(2011).Source corpus We use as our source corpus theconcatenation of the Web-derived ukWaC corpus, amid-2009 dump of the English Wikipedia and theBritish National Corpus1.
The corpus has been tok-enized, POS-tagged and lemmatized with the Tree-Tagger (Schmid, 1995), and it contains about 2.8 bil-lion tokens.
We extract all statistics at the lemmalevel, meaning that we consider only the canonicalform of each word ignoring inflectional information,such as pluralization and verb inflection.Semantic space vocabulary The words/phrasesin the semantic space must of course include theitems that we need for our experiments (adjectives,nouns, ANs and AANs used for model training, asinput to composition and for evaluation).
Therefore,we first populate our semantic space with a core vo-cabulary containing the 8K most frequent nouns andthe 4K most frequent adjectives from the corpus.The ANs included in the semantic space are com-posed of adjectives with very high frequency in thecorpus so that they are generally able to combinewith many classes of nouns.
They are composedof the 700 most frequent adjectives and 4K mostfrequent nouns in the corpus, which were manually1http://wacky.sslmit.unibo.it, http://en.wikipedia.org, http://www.natcorp.ox.ac.uk142controlled for problematic cases ?
excluding adjec-tives such as above, less, or very, and nouns suchas cant, mph, or yours ?
often due to tagging errors.We generated the set of ANs by crossing the filtered663 adjectives and 3,910 nouns.
We include thoseANs that occur at least 100 times in the corpus inour vocabulary, which amounted to a total of 128KANs.Finally, we created a set of AAN phrases com-posed of the adjectives and nouns used to gener-ate the ANs.
Additional preprocessing of the gen-erated AxAyNs includes: (i) control that both AxNand AyN are attested in the corpus; (ii) discard anyAxAyN in which AxN or AyN are among the top200 most frequent ANs in the source corpus (as inthis case, order will be affected by the fact that suchphrases are almost certainly highly lexicalized); and(iii) discard AANs seen as part of a conjunction inthe source corpus (i.e., where the two adjectives ap-pear separated by comma, and, or or; this addressesthe objection that a flexible order AAN might be ahidden A(&)A conjunction: we would expect thatsuch a conjunction should also appear overtly else-where).
The set of AANs thus generated is then di-vided into two types of adjective ordering:1.
Flexible Order (FO): phrases where both or-ders, AxAyN and AyAxN, are attested (f>10in both orders).2.
Rigid Order (RO): phrases with one order,AxAyN, attested (20<f<200)2 and AyAxNunattested.All AANs that did not meet either condition wereexcluded from our semantic space vocabulary.
Thepreserved set resulted in 1,438 AANs: 621 flexibleorder and 817 rigid order.
Note that there are almostas many flexible as rigid order cases; this speaksagainst the idea that free order is a marginal phe-nomenon, due to occasional ambiguities that reas-sign the adjective to a different semantic class.
Theexistence of freely ordered stacked adjectives is a ro-bust phenomenon, which needs to be addressed.2The upper threshold was included as an additional filteragainst potential multiword expressions.
Of course, the bound-ary between phrases that are at least partially compositional andthose that are fully lexicalized is not sharp, and we leave it tofurther work to explore the interplay between the semantic fac-tors we study here and patterns of lexicalization.Model ?
M&LCORP 0.41 0.43W.ADD 0.41 0.44F.ADD 0.40 ?MULT 0.33 0.46LFM 0.40 ?Table 1: Correlation scores (Spearman?s ?, all signif-icant at p<0.001) between cosines of corpus-extractedor model-generated AN vectors and phrase similarity rat-ings collected in Mitchell and Lapata (2010), as well asbest reported results from Mitchell & Lapata (M&L).Semantic vector construction For each ofthe items in our vocabulary, we first build 10K-dimensional vectors by recording the item?ssentence-internal co-occurrence with the top 10Kmost frequent content lemmas (nouns, adjectives,verbs or adverbs) in the corpus.
We built a rankof these co-occurrence counts, and excluded asstop words from the dimensions any element ofany POS whose rank was from 0 to 300.
The rawco-occurrence counts were then transformed into(positive) Pointwise Mutual Information (pPMI)scores (Church and Hanks, 1990).
Next, we reducethe full co-occurrence matrix to 300 dimensionsapplying the Non-negative Matrix Factorization(NMF) operation (Lin, 2007).
We did not tune thesemantic vector construction parameters, since wefound them to work best in a number of independentearlier experiments.Corpus-extracted vectors (corp) were computedfor the ANs and for the flexible order and attestedrigid order AANs, and then mapped onto the 300-dimension NMF-reduced semantic space.
As a san-ity check, the first row of Table 1 reports the corre-lation between the AN phrase similarity ratings col-lected in Mitchell and Lapata (2010) and the cosinesof corpus-extracted vectors in our space, for thesame ANs.
For the AAN vectors, which are sparser,we used human judgements to build a reliable sub-set to serve as our gold standard, as detailed in Sec-tion 2.4.2.2 Composition modelsWe focus on four composition functions proposedin recent literature with high performance in a num-ber of semantic tasks.
We first consider meth-ods proposed by Mitchell and Lapata (2010) in143which the model-generated vectors are simply ob-tained through component-wise operations on theconstituent vectors.
Given input vectors ~u and ~v, themultiplicative model (MULT) computes a composedvector by component-wise multiplication () of theconstituent vectors, where the i-th component of thecomposed vector is given by pi = uivi.3 Given anAxAyN phrase, this model extends naturally to therecursive setting of this experiment, as seen in Equa-tion (1).~p = ~ax  ~ay  ~n (1)This composition method is order-insensitive, theformula above corresponding to the representationof both AxAyN and AyAxN.In the weighted additive model (W.ADD), we ob-tain the composed vector as a weighted sum of thetwo component vectors: ~p = ?~u+ ?~v, where ?
and?
are scalars.
Again, we can easily apply this func-tion recursively, as in Equation (2).~p = ?~ax + ?
(?~ay + ?~n) = ?~ax + ?
?~ay + ?2~n(2)We also consider the full extension of the addi-tive model (F.ADD), presented in Guevara (2010)and Zanzotto et al(2010), such that the componentvectors are pre-multiplied by weight matrices beforebeing added: ~p = W1~u + W2~v.
Similarly to theW.ADD model, Equation (3) describes how we applythis function recursively.~p = W1~ax + W2(W1~ay + W2~n) (3)= W1~ax + W2W1~ay + W22~nFinally, we consider the lexical function model(LFM), first introduced in Baroni and Zamparelli(2010), in which attributive adjectives are treated asfunctions from noun meanings to noun meanings.This is a standard approach in Montague semantics(Thomason, 1974), except noun meanings here aredistributional vectors, not denotations, and adjec-tives are (linear) functions learned from a large cor-pus.
In this model, predicted vectors are generated3We conjecture that the different performance of our multi-plicative model and M&L?s (cf.
Table 1) is due to the fact thatwe use log-transformed pPMI scores, making their multiplica-tive model more akin to our additive approach.by multiplying a function matrix U with a compo-nent vector: ~p = U~v.
Given a weight matrix, A, foreach adjective in the phrase, we apply the functionsin sequence recursively as shown in Equation (4).~p = Ax(Ay~n) (4)Composition model estimation Parameters forW.ADD, F.ADD and LFM were estimated followingthe strategy proposed by Guevara (2010) and Ba-roni and Zamparelli (2010), recently extended to allcomposition models by Dinu et al(2013b).
Specif-ically, we learn parameter values that optimize themapping from the noun to the AN as seen in ex-amples of corpus-extracted N-AN vector pairs, us-ing least-squares methods.
All parameter estima-tions and phrase compositions were implementedusing the DISSECT toolkit4 (Dinu et al 2013a),with a training set of 74,767 corpus-extracted N-AN vector pairs, ranging from 100 to over 1K itemsacross the 663 adjectives.
Importantly, while belowwe report experimental results on capturing variousproperties of recursive AAN constructions, no AANwas seen during training, which was based entirelyon mapping from N to AN.
Table 1 reports the re-sults attained by our model implementations on theMitchell and Lapata AN similarity data set.2.3 Measures of adjective orderingOur general goal is to determine whichlinguistically-motivated factors distinguish thetwo types of adjective ordering.
We hypothesizethat in cases of flexible order, the two adjectiveswill have a similarly strong effect on the noun, thustransforming the meaning of the noun equivalentlyin the direction of both adjectives and componentANs.
For example, in the phrase creative new idea,the idea is both new and creative, so we wouldexpect a similar impact of modification by bothadjectives.On the other hand, we predict that in rigid ordercases, one adjective, the one closer to the noun, willdominate the meaning of the phrase, distorting themeaning of the noun by a significant amount.
Forexample, the phrase different architectural style in-tuitively describes an architectural style that is dif-4http://clic.cimec.unitn.it/composes/toolkit144ferent, rather than a style that is to the same extentarchitectural and different.We consider a number of measures that could cap-ture our intuitions and quantify this difference, ex-ploring the distance relationship between the AANvectors and each of the AAN subparts.
First, weexamine how the similarity of an AAN to its com-ponent adjectives affects the ordering, using the co-sine between the AxAyN vector and each of thecomponent A vectors as an expression of similarity(we abbreviate this as cosAx and cosAy for the firstand second adjective, respectively).5 Our hypothe-sis predicts that flexible order AANs should remainsimilarly close to both component As, while rigidorder AANs should remain systematically closer totheir Ay than to their Ax.Next, we consider the similarity between theAxAyN vector and its component N vector (cosN ).This measure is aimed at verifying if the degree towhich the meaning of the head noun is distortedcould be a property that distinguishes the two typesof adjective ordering.
Again, vectors for flexible or-der AANs should remain closer to their componentnouns in the semantic space, while rigid order AANsshould distort the meaning of the head noun morenotably.We also inspect how the similarity of the AANto its component AN vectors affects the type of ad-jective ordering (cosAxN and cosAyN ).
Consid-ering the examples above, we predict that the flex-ible order AAN creative new idea will share manyproperties with both creative idea and new idea, asrepresented in our semantic space, while rigid or-der AANs, like different architectural style, shouldremain quite similar to the AyN, i.e., architecturalstyle, and relatively distant from the AxN, i.e., dif-ferent style.Finally, we consider a measure that does not ex-ploit distributional semantic representations, namelythe difference in PMI between AxN and AyN(?PMI).
Based on our hypothesis described for theother measures, we expect the association in the cor-pus of AyN to be much greater than AxN for rigidorder AANs, resulting in a large negative ?PMI val-ues.
While flexible order AANs should have similar5In the case of LFM, we compare the similarity of the AANwith the AN centroids for each adjective, since the model doesnot make use of A vectors (Baroni and Zamparelli, 2010).association strengths for both AxN and AyN, thuswe expect ?PMI to be closer to 0 than for rigid or-der AANs.2.4 Gold standardTo our knowledge, this is the first study to usedistributional representations of recursive modifi-cation; therefore we must first determine if thecomposed AAN vector representations are seman-tically coherent objects.
Thus, for vector analysis,a gold standard of 320 corpus-extracted AAN vec-tors were selected and their quality was establishedby inspecting their nearest neighbors.
In order tocreate the gold standard, we ran a crowdsourcingexperiment on CrowdFlower6 (Callison-Burch andDredze, 2010; Munro et al 2010), as follows.First, we gathered a randomly selected set of 600corpus-extracted AANs, containing 300 flexible or-der and 300 attested rigid order AANs.
We thenextracted the top 3 nearest neighbors to the corpus-extracted AAN vectors as represented in the seman-tic space7.
Each AAN was then presented with eachof the nearest neighbors, and participants were askedto judge ?how strongly related are the two phrases?
?on a scale of 1-7.
The rationale was that if weobtained a good distributional representation of theAAN, its nearest neighbors should be closely relatedwords and phrases.
Each pair was judged 10 times,and we calculated a relatedness score for the AANby taking the average of the 30 judgments (10 foreach of the three neighbors).The final set for the gold standard contains the 320AANs (152 flexible order and 168 attested rigid or-der) which had a relatedness score over the median-split (3.9).
Table 2 shows examples of gold stan-dard AANs and their nearest neighbors.
As theseexample indicate, the gold standard AANs reside insemantic neighborhoods that are populated by in-tuitively strongly related expressions, which makesthem a sensible target for the compositional modelsto approximate.We also find that the neighbors for the AANs rep-resent an interesting variety of types of semantic6http://www.crowdflower.com7The top 3 neighbors included adjectives, nouns, ANs andAANs.
The preference for ANs and AANs, as seen in Table 2,is likely a result of the dominance of those elements in the se-mantic space (c.f.
Section 2.1).145medieval old town contemp.
political issuefascinating town cultural topicimpressive cathedral contemporary debatemedieval street contemporary politicsrural poor people British naval powerpoor rural people naval warrural infrastructure British navyrural people naval powerfriendly helpful staff last live performancenear hotel final gighelpful staff live dvdquick service live releasecreative new idea rapid social changeinnovative effort social conflictcreative design social transitiondynamic part cultural consequencenational daily newspaper new regional governmentnational newspaper regional governmentmajor newspaper local reformdaily newspaper regional councildaily national newspaper fresh organic vegetablenational daily newspaper organic vegetablewell-known journalist organic fruitweekly column organic productTable 2: Examples of the nearest neighbors of the goldstandard, both flexible order (left column) and rigid order(right column) AANs.similarity.
For example, the nearest neighbors to thecorpus-extracted vectors for medieval old town andrapid social change include phrases which describequite complex associations, cf.
Table 2.
In addition,we find that the nearest neighbors for flexible orderAAN vectors are not necessarily the same for bothadjective orders, as seen in the difference in neigh-bors of national daily newspaper and daily nationalnewspaper.
We can expect that the change in or-der, when acceptable and frequent, does not neces-sarily yield synonymous phrases, and that corpus-extracted vector representations capture subtle dif-ferences in meaning.3 Results3.1 Quality of model-generated AAN vectorsOur nearest neighbor analysis suggests that thecorpus-extracted AAN vectors in the gold standardare meaningful, semantically coherent objects.
Wecan thus assess the quality of AANs recursively gen-erated by composition models by how closely theyGold FO ROW.ADD 0.565 0.572 0.558F.ADD 0.618 0.622 0.614MULT 0.424 0.468 0.384LFM 0.655 0.675 0.637Table 3: Mean cosine similarities between the corpus-extracted and model-generated gold AAN vectors.
Allpairwise differences between models are significant ac-cording to Bonferroni-corrected paired t-tests (p<0.001).For MULT and LFM, the difference between mean flexibleorder (FO) and rigid order (RO) cosines is also signifi-cant.approximate these vectors.
We find that the perfor-mances of most composition models in approximat-ing the vectors for the gold AANs is quite satisfac-tory (cf.
Table 3).
To put this evaluation into per-spective, note that 99% of the simulated distribu-tion of pairwise cosines of corpus-extracted AANsis below the mean cosine of the worst-performingmodel (MULT), that is, a cosine of 0.424 is very sig-nificantly above what is expected by chance for tworandom corpus-extracted AAN vectors.
Also, ob-serve that the two more parameter-rich models arebetter than W.ADD, and that LFM also significantlyoutperforms F.ADD.Further, the results show that the models are ableto approximate flexible order AAN vectors betterthan rigid order AANs, significantly so for LFM andMULT.
This result is quite interesting because it sug-gests that flexible order AANs express a more lit-eral (or intersective) modification by both adjectives,which is what we would expect to be better capturedby compositional models.
Clearly, a more complexmodification process is occurring in the case of rigidorder AANs, as we predicted to be the case.3.2 Distinguishing flexible vs. rigid orderIn the results reported below, we test how both ourbaseline ?PMI measure and the distance from theAAN and its component parts changes depending onthe type of adjective ordering to which the AAN be-longs.
From this point forward, we only use goldstandard items, where we are sure of the quality ofthe corpus-extracted vectors.
The first block of Ta-ble 4 reports the t-normalized difference betweenflexible order and rigid order mean cosines for thecorpus-extracted vectors.146Measure t sig.CORPcosAx 2.478cosAy -4.348 * RO>FOcosN 4.656 * FO>ROcosAxN 5.913 * FO>ROcosAyN 1.970W.ADDcosAx 4.805 * FO>ROcosAy -1.109cosN 1.140cosAxN 1.059cosAyN 0.584F.ADDcosAx 2.050cosAy -1.451cosN 4.493 * FO>ROcosAxN -0.445cosAyN 2.300MULTcosAx 3.830 * FO>ROcosAy -0.503cosN 5.090 * FO>ROcosAxN 4.435 * FO>ROcosAyN 3.900 * FO>ROLFMcosAx -1.649cosAy -1.272cosN 5.539 * FO>ROcosAxN 3.336 * FO>ROcosAyN 4.215 * FO>RO?PMI 8.701 * FO>ROTable 4: Flexible vs.
Rigid Order AANs.
t-normalizeddifferences between flexible order (FO) and rigid order(FO) mean cosines (or mean ?PMI values) for corpus-extracted and model-generated vectors.
For significantdifferences (p<0.05 after Bonferroni correction), the lastcolumn reports whether mean cosine (or ?PMI) is largerfor flexible order (FO) or rigid order (RO) class.These results show, in accordance with our con-siderations in Section 2.3 above: (i) flexible or-der AxAyNs are closer to AxN and the componentN than rigid order AxAyNs, and (ii) rigid orderAxAyNs are closer to their Ay (flexible order AANsare also closer to Ax but the effect does not reachsignificance).8 The results imply that the degree ofmodification of the Ay on the noun is a significantindicator of the type of ordering present.8As an aside, the fact that mean cosines are significantlylarger for the flexible order class in two cases but for the rigid or-der class in another addresses the concern, raised by a reviewer,that the words and phrases in one of the two classes might sys-tematically inhabit denser regions of the space than those of theother class, thus distorting results based on comparing meancosines.In particular, rigid order AxAyNs are heavilymodified by Ay, distorting the meaning of the headnoun in the direction of the closest adjective quitedrastically, and only undergoing a slight modifica-tion when the Ax is added.
In other words, in rigidorder phrases, for example rapid social change, theAyN expresses a single concept (probably a ?kind?,in the terminology of formal semantics), strongly re-lated to social, social change, which is then mod-ified by the Ax.
Thus, the change is not both so-cial and rapid, rather, the social change is rapid.
Onthe other hand, flexible order AANs maintain the se-mantic value of the head noun while being modi-fied only slightly by both adjectives, almost equiv-alently.
For example, in the phrase friendly help-ful staff, one is saying that the staff is both friendlyand helpful.
Most importantly, the corpus-extracteddistributional representations are able to model thisphenomenon inherently and can significantly distin-guish the two adjective orders.The results of the composition models (cf.
Ta-ble 4) show that for all models at least some prop-erties do distinguish flexible and rigid order AANs,although only MULT and LFM capture the two prop-erties that show the largest effect for the corpus-extracted vectors, namely the asymmetry in similar-ity to the noun and the AxN (flexible order AANsbeing more similar to both).It is worth remarking that MULT approximated thepatterns observed in the corpus vectors quite well,despite producing order-insensitive representationsof recursive structures.
For flexible order AANs, or-der is indeed only slightly affecting the meaning, soit stands to reason that MULT has no problems mod-eling this class.
For rigid order AANs, where weconsider here the attested-order only, evidently theorder-insensitive MULT representation is sufficientto capture their relations to their constituents.Finally, we see that the ?PMI measure is the bestat distinguishing between the two classes of AANordering.
This confirms our hypothesis that a lot hasto do with how integrated Ay and N are.
While itis somewhat disappointing that ?PMI outperformsall distributional semantic cues, note that this mea-sure conflates semantic and lexical factors, as thehigh PMI of AyN in at least some rigid order AANsmight be also a cue of the fact that the latter bigramis a lexicalized phrase (as discussed in footnote 2, it147is unlikely that our filtering strategies sifted out allmultiword expressions).
Moreover, ?PMI does notproduce a semantic representation of the phrase (seehow composed distributional vectors approximate ofhigh quality AAN vectors in Table 3).
Finally, thismeasure will not scale up to cases where the ANsare not attested, whereas measures based on compo-sition only need corpus-harvested representations ofadjectives and nouns.3.3 Properties of the correct adjective orderHaving shown that flexible order and rigid orderAANs are significantly distinguished by variousproperties, we proceed now to test whether thosesame properties also allow us to distinguish betweencorrect (corpus-attested) and wrong (unattested) ad-jective ordering in rigid AANs (recall that we areworking with cases where the attested-order occursmore than 20 times in the corpus, and both adjec-tives modify the nouns at least 10 times, so we areconfident that there is a true asymmetry).We expect that the fundamental property that dis-tinguishes the orders is again found in the degreeof modification of both component adjectives.
Wepredict that the single concept created by the AyNin attested-order rigid AANs, such as legal statusin formal legal status, is an effect of the modifica-tion strength of the Ay on the head noun, and whenseen in the incorrect ordering, i.e., ?legal formal sta-tus, the strong modification of legal will still domi-nate the meaning of the AAN.
Composition modelsshould be able to capture this effect based on the dis-tance from both the component adjectives and ANs.Clearly, we cannot run these analyses on corpus-extracted vectors since the unattested order, by def-inition, is not seen in our corpus, and therefore wecannot collect co-occurrence statistics for the AANphrase.
Thus, we test our measures of adjective or-dering on the model-generated AAN vectors, for allgold rigid order AANs in both orders.We also consider the ?PMI measure which wasso effective in distinguishing flexible vs. rigid or-der AANs.
We expect that the greater associationwith AyN for attested-order AANs will again leadto large, negative differences in PMI scores, whilethe expectation that unattested-order AANs will behighly associated with their AxN will correspond tolarge, positive differences in PMI.Measure t sig.W.ADDcosAx -7.840 * U>AcosAy 7.924 * A>UcosN 2.394cosAxN -5.462 * U>AcosAyN 3.627 * A>UF.ADDcosAx -8.418 * U>AcosAy 6.534 * A>UcosN -1.927cosAxN -3.583 * U>AcosAyN -2.185MULTcosAx -5.100 * U>AcosAy 5.100 * A>UcosN 0.000cosAxN -0.598cosAyN 0.598LFMcosAx -7.498 * U>AcosAy 7.227 * A>UcosN -2.172cosAxN -5.792 * U>AcosAyN 0.774?PMI -11.448 * U>ATable 5: Attested- vs. unattested-order rigid orderAANs.
t-normalized mean paired cosine (or ?PMI) dif-ferences between attested (A) and unattested (U) AANswith their components.
For significant differences (pairedt-test p<0.05 after Bonferroni correction), last columnreports whether cosines (or ?PMI) are on average largerfor A or U.Across all composition models, we find that thedistance between the model-generated AAN and itscomponent adjectives, Ax and Ay, are significant in-dicators of attested vs. unattested adjective ordering(cf.
Table 5).
Specifically, we find that rigid orderAANs in the correct order are closest to their Ay,while we can detect the unattested order when therigid order AAN is closer to its Ax.
This findingis quite interesting, since it shows that the order inwhich the composition functions are applied doesnot alter the fact that the modification of one ad-jective in rigid order AANs (the Ay in the case ofattested-order rigid order AANs) is much strongerthan the other.
Unlike the measures that differenti-ated flexible and rigid order AANs, here we see thatthe distance from the component N is not an indi-cator of the correct adjective ordering (trivially sofor MULT, where attested and unattested AANs areidentical).Next, we find that for W.ADD, F.ADD and LFM,148the distance from the component AxN is a strongindicator of attested- vs. unattested-order rigid orderAANs.
Specifically, attested-order AANs are furtherfrom their AxN than unattested-order AANs.
Thisfinding is in line with our predictions and followsthe findings of the impact of the distance from thecomponent adjectives.
?PMI, as seen in the ability to distinguish flexi-ble vs. rigid order AANs, is the strongest indicatorof correct vs wrong adjective ordering.
This mea-sure confirms that the association of one adjective(the Ay in attested-order AANs) with the head nounis indeed the most significant factor distinguishingthese two classes.
However, as we mentioned be-fore, this measure has its limitations and is likely notto be entirely sufficient for future steps in modelingrecursive modification.4 ConclusionWhile AN constructions have been extensively stud-ied within the framework of compositional distri-butional semantics (Baroni and Zamparelli, 2010;Boleda et al 2012; Boleda et al 2013; Guevara,2010; Mitchell and Lapata, 2010; Turney, 2012;Vecchi et al 2011), for the first time, we extendedthe investigation to recursively built AAN phrases.First, we showed that composition functions ap-plied recursively can approximate corpus-extractedAAN vectors that we know to be of high semanticquality.Next, we looked at some properties of the samehigh-quality corpus-extracted AAN vectors, findingthat the distinction between ?flexible?
AANs, wherethe adjective order can be flipped, and ?rigid?
ones,where the order is fixed, is reflected in distributionalcues.
These results all derive from the intuition thatthe most embedded adjective in a rigid AAN has avery strong effect on the distributional semantic rep-resentation of the AAN.
Most compositional modelswere able to capture at least some of the same cuesthat emerged in the analysis of the corpus-extractedvectors.Finally, similar cues were also shown to distin-guish (compositional) representations of rigid AANsin the ?correct?
(corpus-attested) and ?wrong?
(unattested) orders, again pointing to the degree towhich the (attested-order) closest adjective affectsthe overall AAN meaning as an important factor.Comparing the composition functions, we findthat the linguistically motivated LFM approach hasthe most consistent performance across all our tests.This model significantly outperformed all others inapproximating high-quality corpus-extracted AANvectors, it provided the closest approximation to thecorpus-observed patterns when distinguishing flexi-ble and rigid AANs, and it was one of the modelswith the strongest cues distinguishing attested andunattested orders of rigid AANs.From an applied point of view, a natural next stepwould be to use the cues we proposed as features totrain a classifier to predict the preferred order of ad-jectives, to be tested also in cases where neither or-der is found in the corpus, so direct corpus evidencecannot help.
For a full account of adjectival order-ing, non-semantic factors should also be taken intoaccount.
As shown by the effectiveness in our ex-periments of PMI, which is a classic measure usedto harvest idioms and other multiword expressions(Church and Hanks, 1990), ordering is affected byarbitrary lexicalization patterns.
Metrical effects arealso likely to play a role, like they do in the well-studied case of ?binomials?
such as salt and pep-per (Benor and Levy, 2006; Copestake and Herbe-lot, 2011).
In a pilot study, we found that indeedword length (roughly quantified by number of let-ters) is a significant factor in predicting adjectiveordering (the shorter adjective being more likely tooccur first), but its effect is not nearly as strong asthat of the semantic measures we considered here.In our future work, we would like to develop an or-der model that exploits semantic, metrical and lexi-calization features jointly for maximal classificationaccuracy.Adjectival ordering information could be usefulin parsing: in English, it could tell whether anAANN sequence should be parsed as A[[AN]N]or A[A[NN]]; in languages with pre- and post-N adjectives, like Italian or Spanish, it could tellwhether ANA sequences should be parsed as A[NA]or [AN]A.
The ability to detect ordering restric-tions could also help Natural Language Generationtasks (Malouf, 2000), especially for the generationof unattested combinations of As and Ns.From a theoretical point of view, we would like toextend our analysis to adjective coordination (what?s149the difference between new and creative idea andnew creative idea?).
Additionally, we could go moregranular, looking at whether compositional modelscan help us to understand why certain classes of ad-jectives are more likely to precede or follow others(why is size more likely to take scope over color,so that big red car sounds more natural than red bigcar?)
or studying the behaviour of specific adjectives(can our approach capture the fact that strong alco-holic drink is preferable to alcoholic strong drinkbecause strong pertains to the alcoholic propertiesof the drink?
).In the meantime, we hope that the results we re-ported here provide convincing evidence of the use-fulness of compositional distributional semantics intackling topics, such as recursive adjectival modifi-cation, that have been of traditional interest to theo-retical linguists from a new perspective.AcknowledgmentsWe would like to thank the anonymous reviewers,Fabio Massimo Zanzotto, Yao-Zhong Zhang and themembers of the COMPOSES team.
This researchwas supported by the ERC 2011 Starting Indepen-dent Research Grant n. 283554 (COMPOSES).ReferencesMarco Baroni and Roberto Zamparelli.
2010.
Nounsare vectors, adjectives are matrices: Representingadjective-noun constructions in semantic space.
InProceedings of EMNLP, pages 1183?1193, Boston,MA.Sarah Bunin Benor and Roger Levy.
2006.
The chickenor the egg?
A probabilistic analysis of english binomi-als.
Language, pages 233?278.William Blacoe and Mirella Lapata.
2012.
A comparisonof vector-based representations for semantic composi-tion.
In Proceedings of the 2012 Joint Conference onEMNLP and CoNLL, pages 546?556, Jeju Island, Ko-rea.Gemma Boleda, Eva Maria Vecchi, Miquel Cornudella,and Louise McNally.
2012.
First-order vs. higher-order modification in distributional semantics.
In Pro-ceedings of the 2012 Joint Conference on EMNLP andCoNLL, pages 1223?1233, Jeju Island, Korea.Gemma Boleda, Marco Baroni, Louise McNally, andNghia Pham.
2013.
Intensionality was only alleged:On adjective-noun composition in distributional se-mantics.
In Proceedings of IWCS, pages 35?46, Pots-dam, Germany.Chris Callison-Burch and Mark Dredze.
2010.
Creatingspeech and language data with amazon?s mechanicalturk.
In Proceedings of the NAACL HLT 2010 Work-shop on Creating Speech and Language Data withAmazon?s Mechanical Turk, pages 1?12, Los Angeles,CA.Kenneth Church and Peter Hanks.
1990.
Word asso-ciation norms, mutual information, and lexicography.Computational Linguistics, 16(1):22?29.Guglielmo Cinque, editor.
2002.
Functional Structure inDP and IP - The Carthography of Syntactic Structures,volume 1.
Oxford University Press.Guglielmo Cinque.
2004.
Issues in adverbial syntax.Lingua, 114:683?710.Guglielmo Cinque.
2010.
The syntax of adjectives: acomparative study.
MIT Press.Ann Copestake and Aure?lie Herbelot.
2011.
Excitingand interesting: issues in the generation of binomials.In Proceedings of the UCNLG+ Eval: Language Gen-eration and Evaluation Workshop, pages 45?53, Edin-burgh, UK.Paola Crisma.
1991.
Functional categories inside thenoun phrase: A study on the distribution of nominalmodifiers.
?Tesi di Laurea?, University of Venice.Georgiana Dinu, Nghia The Pham, and Marco Baroni.2013a.
DISSECT: DIStributional SEmantics Compo-sition Toolkit.
In Proceedings of the System Demon-strations of ACL 2013, East Stroudsburg, PA.Georgiana Dinu, Nghia The Pham, and Marco Baroni.2013b.
General estimation and evaluation of compo-sitional distributional semantic models.
In Proceed-ings of the ACL 2013 Workshop on Continuous Vec-tor Space Models and their Compositionality (CVSC2013), East Stroudsburg, PA.Gottlob Frege.
1892.
U?ber sinn und bedeutung.Zeitschrift fuer Philosophie un philosophische Kritik,100.Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011.Experimental support for a categorical compositionaldistributional model of meaning.
In Proceedings ofEMNLP, Edinburgh, UK.Emiliano Guevara.
2010.
A regression model ofadjective-noun compositionality in distributional se-mantics.
In Proceedings of the ACL GEMS Workshop,pages 33?37, Uppsala, Sweden.Chih-Jen Lin.
2007.
Projected gradient methods forNonnegative Matrix Factorization.
Neural Computa-tion, 19(10):2756?2779.Robert Malouf.
2000.
The order of prenominal adjec-tives in natural language generation.
In Proceedingsof ACL, pages 85?92, East Stroudsburg, PA.150Jeff Mitchell and Mirella Lapata.
2010.
Composition indistributional models of semantics.
Cognitive Science,34(8):1388?1429.Richard Montague.
1970.
Universal Grammar.
Theoria,36:373?398.Robert Munro, Steven Bethard, Victor Kuperman,Vicky Tzuyin Lai, Robin Melnick, Christopher Potts,Tyler Schnoebelen, and Harry Tily.
2010.
Crowd-sourcing and language studies: the new generation oflinguistic data.
In Proceedings of the NAACL HLT2010 Workshop on Creating Speech and LanguageData with Amazon?s Mechanical Turk, pages 122?130,Los Angeles, CA.Barbara Partee.
2004.
Compositionality.
In Compo-sitionality in Formal Semantics: Selected Papers byBarbara H. Partee.
Blackwell, Oxford.Magnus Sahlgren.
2006.
The Word-Space Model.
Dis-sertation, Stockholm University.Helmut Schmid.
1995.
Improvements in part-of-speechtagging with an application to German.
In Proceed-ings of the EACL-SIGDAT Workshop, Dublin, Ireland.Hinrich Schu?tze.
1997.
Ambiguity Resolution in NaturalLanguage Learning.
CSLI, Stanford, CA.Gary-John Scott.
2002.
Stacked adjectival modificationand the structure of nominal phrases.
In GuglielmoCinque, editor, Functional Structure in DP and IP.
TheCarthography of Syntactic Structures, volume 1.
Ox-ford University Press.Richard Socher, E.H. Huang, J. Pennington, Andrew Y.Ng, and C.D.
Manning.
2011.
Dynamic pooling andunfolding recursive autoencoders for paraphrase de-tection.
Advances in Neural Information ProcessingSystems, 24:801?809.Richard Socher, Brody Huval, Christopher D. Manning,and Andrew Y. Ng.
2012.
Semantic compositionalitythrough recursive matrix-vector spaces.
In Proceed-ings of EMNLP, pages 1201?1211, Edinburgh, UK.Richard Sproat and Chilin Shih.
1990.
The cross-linguistics distribution of adjective ordering restric-tions.
In C. Georgopoulos and Ishihara R., editors,Interdisciplinary approaches to language: essays inhonor of Yuki Kuroda, pages 565?593.
Kluver, Dor-drecht.Sam Steddy and Vieri Samek-Lodovici.
2011.
On theungrammaticality of remnant movement in the deriva-tion of greenberg?s universal 20.
Linguistic Inquiry,42(3):445?469.Richmond H. Thomason, editor.
1974.
Formal Philoso-phy: Selected Papers of Richard Montague.
Yale Uni-versity Press, New York.Peter Turney and Patrick Pantel.
2010.
From frequencyto meaning: Vector space models of semantics.
Jour-nal of Artificial Intelligence Research, 37:141?188.Peter Turney.
2012.
Domain and function: A dual-spacemodel of semantic relations and compositions.
Jour-nal of Artificial Intelligence Research, 44:533?585.Eva Maria Vecchi, Marco Baroni, and Roberto Zampar-elli.
2011.
(Linear) maps of the impossible: Cap-turing semantic anomalies in distributional space.
InProceedings of the ACL Workshop on DistributionalSemantics and Compositionality, pages 1?9, Portland,OR.Fabio Zanzotto, Ioannis Korkontzelos, Francesca Faluc-chi, and Suresh Manandhar.
2010.
Estimating linearmodels for compositional distributional semantics.
InProceedings of COLING, pages 1263?1271, Beijing,China.151
