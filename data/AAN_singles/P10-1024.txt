Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 226?236,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsFully Unsupervised Core-Adjunct Argument ClassificationOmri Abend?Institute of Computer ScienceThe Hebrew Universityomria01@cs.huji.ac.ilAri RappoportInstitute of Computer ScienceThe Hebrew Universityarir@cs.huji.ac.ilAbstractThe core-adjunct argument distinction is abasic one in the theory of argument struc-ture.
The task of distinguishing betweenthe two has strong relations to various ba-sic NLP tasks such as syntactic parsing,semantic role labeling and subcategoriza-tion acquisition.
This paper presents anovel unsupervised algorithm for the taskthat uses no supervised models, utilizinginstead state-of-the-art syntactic inductionalgorithms.
This is the first work to tacklethis task in a fully unsupervised scenario.1 IntroductionThe distinction between core arguments (hence-forth, cores) and adjuncts is included in most the-ories on argument structure (Dowty, 2000).
Thedistinction can be viewed syntactically, as onebetween obligatory and optional arguments, orsemantically, as one between arguments whosemeanings are predicate dependent and indepen-dent.
The latter (cores) are those whose function inthe described event is to a large extent determinedby the predicate, and are obligatory.
Adjuncts areoptional arguments which, like adverbs, modifythe meaning of the described event in a predictableor predicate-independent manner.Consider the following examples:1.
The surgeon operated [on his colleague].2.
Ron will drop by [after lunch].3.
Yuri played football [in the park].The marked argument is a core in 1 and an ad-junct in 2 and 3.
Adjuncts form an independentsemantic unit and their semantic role can often beinferred independently of the predicate (e.g., [af-ter lunch] is usually a temporal modifier).
Core?
Omri Abend is grateful to the Azrieli Foundation forthe award of an Azrieli Fellowship.roles are more predicate-specific, e.g., [on his col-league] has a different meaning with the verbs ?op-erate?
and ?count?.Sometimes the same argument plays a differentrole in different sentences.
In (3), [in the park]places a well-defined situation (Yuri playing foot-ball) in a certain location.
However, in ?The troopsare based [in the park]?, the same argument isobligatory, since being based requires a place tobe based in.Distinguishing between the two argument typeshas been discussed extensively in various formu-lations in the NLP literature, notably in PP attach-ment, semantic role labeling (SRL) and subcatego-rization acquisition.
However, no work has tack-led it yet in a fully unsupervised scenario.
Unsu-pervised models reduce reliance on the costly anderror prone manual multi-layer annotation (POStagging, parsing, core-adjunct tagging) commonlyused for this task.
They also allow to examine thenature of the distinction and to what extent it isaccounted for in real data in a theory-independentmanner.In this paper we present a fully unsupervised al-gorithm for core-adjunct classification.
We utilizeleading fully unsupervised grammar induction andPOS induction algorithms.
We focus on preposi-tional arguments, since non-prepositional ones aregenerally cores.
The algorithm uses three mea-sures based on different characterizations of thecore-adjunct distinction, and combines them us-ing an ensemble method followed by self-training.The measures used are based on selectional prefer-ence, predicate-slot collocation and argument-slotcollocation.We evaluate against PropBank (Palmer et al,2005), obtaining roughly 70% accuracy whenevaluated on the prepositional arguments andmore than 80% for the entire argument set.
Theseresults are substantially better than those obtainedby a non-trivial baseline.226Section 2 discusses the core-adjunct distinction.Section 3 describes the algorithm.
Sections 4 and5 present our experimental setup and results.2 Core-Adjunct in Previous WorkPropBank.
PropBank (PB) (Palmer et al, 2005)is a widely used corpus, providing SRL annotationfor the entire WSJ Penn Treebank.
Its core labelsare predicate specific, while adjunct (or modifiersunder their terminology) labels are shared acrosspredicates.
The adjuncts are subcategorized intoseveral classes, the most frequent of which arelocative, temporal and manner1.The organization of PropBank is based onthe notion of diathesis alternations, which are(roughly) defined to be alternations between twosubcategorization frames that preserve meaning orchange it systematically.
The frames in whicheach verb appears were collected and sets of al-ternating frames were defined.
Each such set wasassumed to have a unique set of roles, named ?role-set?.
These roles include all roles appearing in anyof the frames, except of those defined as adjuncts.Adjuncts are defined to be optional argumentsappearing with a wide variety of verbs and frames.They can be viewed as fixed points with respect toalternations, i.e., as arguments that do not changetheir place or slot when the frame undergoes analternation.
This follows the notions of optionalityand compositionality that define adjuncts.Detecting diathesis alternations automaticallyis difficult (McCarthy, 2001), requiring an initialacquisition of a subcategorization lexicon.
Thisalone is a challenging task tackled in the past us-ing supervised parsers (see below).FrameNet.
FrameNet (FN) (Baker et al, 1998)is a large-scale lexicon based on frame semantics.It takes a different approach from PB to semanticroles.
Like PB, it distinguishes between core andnon-core arguments, but it does so for each andevery frame separately.
It does not commit that asemantic role is consistently tagged as a core ora non-core across frames.
For example, the se-mantic role ?path?
is considered core in the ?SelfMotion?
frame, but as non-core in the ?Placing?frame.
Another difference is that FN does not al-low any type of non-core argument to attach toa given frame.
For instance, while the ?Getting?1PropBank annotates modals and negation words as mod-ifiers.
Since these are not arguments in the common usage ofthe term, we exclude them from the discussion in this paper.frame allows a ?Duration?
non-core argument, the?Active Perception?
frame does not.PB and FN tend to agree in clear (prototypical)cases, but to differ in others.
For instance, bothschemes would tag ?Yuri played football [in thepark]?
as an adjunct and ?The commander placeda guard [in the park]?
as a core.
However, in ?Hewalked [into his office]?, the marked argument istagged as a directional adjunct in PB but as a ?Di-rection?
core in FN.Under both schemes, non-cores are usually con-fined to a few specific semantic domains, no-tably time, place and manner, in contrast to coresthat are not restricted in their scope of applica-bility.
This approach is quite common, e.g., theCOBUILD English grammar (Willis, 2004) cate-gorizes adjuncts to be of manner, aspect, opinion,place, time, frequency, duration, degree, extent,emphasis, focus and probability.Semantic Role Labeling.
Work in SRL doesnot tackle the core-adjunct task separately but aspart of general argument classification.
Super-vised approaches obtain an almost perfect scorein distinguishing between the two in an in-domainscenario.
For instance, the confusion matrix in(Toutanova et al, 2008) indicates that their modelscores 99.5% accuracy on this task.
However,adaptation results are lower, with the best twomodels in the CoNLL 2005 shared task (Carrerasand Ma`rquez, 2005) achieving 95.3% (Pradhan etal., 2008) and 95.6% (Punyakanok et al, 2008) ac-curacy in an adaptation between the relatively sim-ilar corpora WSJ and Brown.Despite the high performance in supervised sce-narios, tackling the task in an unsupervised man-ner is not easy.
The success of supervised methodsstems from the fact that the predicate-slot com-bination (slot is represented in this paper by itspreposition) strongly determines whether a givenargument is an adjunct or a core (see Section 3.4).Supervised models are provided with an anno-tated corpus from which they can easily learn themapping between predicate-slot pairs and theircore/adjunct label.
However, induction of themapping in an unsupervised manner must be basedon inherent core-adjunct properties.
In addition,supervised models utilize supervised parsers andPOS taggers, while the current state-of-the-art inunsupervised parsing and POS tagging is consid-erably worse than their supervised counterparts.This challenge has some resemblance to un-227supervised detection of multiword expressions(MWEs).
An important MWE sub-class is thatof phrasal verbs, which are also characterized byverb-preposition pairs (Li et al, 2003; Sporlederand Li, 2009) (see also (Boukobza and Rappoport,2009)).
Both tasks aim to determine semanticcompositionality, which is a highly challengingtask.Few works addressed unsupervised SRL-relatedtasks.
The setup of (Grenager and Manning,2006), who presented a Bayesian Network modelfor argument classification, is perhaps closest toours.
Their work relied on a supervised parserand a rule-based argument identification (both dur-ing training and testing).
Swier and Stevenson(2004, 2005), while addressing an unsupervisedSRL task, greatly differ from us as their algorithmuses the VerbNet (Kipper et al, 2000) verb lex-icon, in addition to supervised parses.
Finally,Abend et al (2009) tackled the argument identi-fication task alone and did not perform argumentclassification of any sort.PP attachment.
PP attachment is the task of de-termining whether a prepositional phrase whichimmediately follows a noun phrase attaches to thelatter or to the preceding verb.
This task?s relationto the core-adjunct distinction was addressed inseveral works.
For instance, the results of (Hindleand Rooth, 1993) indicate that their PP attachmentsystem works better for cores than for adjuncts.Merlo and Esteve Ferrer (2006) suggest a sys-tem that jointly tackles the PP attachment and thecore-adjunct distinction tasks.
Unlike in this work,their classifier requires extensive supervision in-cluding WordNet, language-specific features anda supervised parser.
Their features are generallymotivated by common linguistic considerations.Features found adaptable to a completely unsuper-vised scenario are used in this work as well.Syntactic Parsing.
The core-adjunct distinctionis included in many syntactic annotation schemes.Although the Penn Treebank does not explicitlyannotate adjuncts and cores, a few works sug-gested mapping its annotation (including func-tion tags) to core-adjunct labels.
Such a mappingwas presented in (Collins, 1999).
In his Model2, Collins modifies his parser to provide a core-adjunct prediction, thereby improving its perfor-mance.The Combinatory Categorial Grammar (CCG)formulation models the core-adjunct distinctionexplicitly.
Therefore, any CCG parser can be usedas a core-adjunct classifier (Hockenmaier, 2003).Subcategorization Acquisition.
This task spec-ifies for each predicate the number, type and orderof obligatory arguments.
Determining the allow-able subcategorization frames for a given predi-cate necessarily involves separating its cores fromits allowable adjuncts (which are not framed).
No-table works in the field include (Briscoe and Car-roll, 1997; Sarkar and Zeman, 2000; Korhonen,2002).
All these works used a parsed corpus inorder to collect, for each predicate, a set of hy-pothesized subcategorization frames, to be filteredby hypothesis testing methods.This line of work differs from ours in a fewaspects.
First, all works use manual or super-vised syntactic annotations, usually including aPOS tagger.
Second, the common approach to thetask focuses on syntax and tries to identify the en-tire frame, rather than to tag each argument sep-arately.
Finally, most works address the task atthe verb type level, trying to detect the allowableframes for each type.
Consequently, the commonevaluation focuses on the quality of the allowableframes acquired for each verb type, and not on theclassification of specific arguments in a given cor-pus.
Such a token level evaluation was conductedin a few works (Briscoe and Carroll, 1997; Sarkarand Zeman, 2000), but often with a small num-ber of verbs or a small number of frames.
A dis-cussion of the differences between type and tokenlevel evaluation can be found in (Reichart et al,2010).The core-adjunct distinction task was tackled inthe context of child language acquisition.
Villav-icencio (2002) developed a classifier based onpreposition selection and frequency informationfor modeling the distinction for locative preposi-tional phrases.
Her approach is not entirely corpusbased, as it assumes the input sentences are givenin a basic logical form.The study of prepositions is a vibrant researcharea in NLP.
A special issue of Computational Lin-guistics, which includes an extensive survey of re-lated work, was recently devoted to the field (Bald-win et al, 2009).2283 AlgorithmWe are given a (predicate, argument) pair in a testsentence, and we need to determine whether theargument is a core or an adjunct.
Test argumentsare assumed to be correctly bracketed.
We are al-lowed to utilize a training corpus of raw text.3.1 OverviewOur algorithm utilizes statistics based on the(predicate, slot, argument head) (PSH) joint dis-tribution (a slot is represented by its preposition).To estimate this joint distribution, PSH samplesare extracted from the training corpus using unsu-pervised POS taggers (Clark, 2003; Abend et al,2010) and an unsupervised parser (Seginer, 2007).As current performance of unsupervised parsersfor long sentences is low, we use only short sen-tences (up to 10 words, excluding punctuation).The length of test sentences is not bounded.
Ourresults will show that the training data accountswell for the argument realization phenomena inthe test set, despite the length bound on its sen-tences.
The sample extraction process is detailedin Section 3.2.Our approach makes use of both aspects of thedistinction ?
obligatoriness and compositionality.We define three measures, one quantifying theobligatoriness of the slot, another quantifying theselectional preference of the verb to the argumentand a third that quantifies the association betweenthe head word and the slot irrespective of the pred-icate (Section 3.3).The measures?
predictions are expected to coin-cide in clear cases, but may be less successful inothers.
Therefore, an ensemble-based method isused to combine the three measures into a singleclassifier.
This results in a high accuracy classifierwith relatively low coverage.
A self-training stepis now performed to increase coverage with only aminor deterioration in accuracy (Section 3.4).We focus on prepositional arguments.
Non-prepositional arguments in English tend to becores (e.g., in more than 85% of the cases inPB sections 2?21), while prepositional argumentstend to be equally divided between cores and ad-juncts.
The difficulty of the task thus lies in theclassification of prepositional arguments.3.2 Data CollectionThe statistical measures used by our classifierare based on the (predicate, slot, argument head)(PSH) joint distribution.
This section details theprocess of extracting samples from this joint dis-tribution given a raw text corpus.We start by parsing the corpus using the Seginerparser (Seginer, 2007).
This parser is unique in itsability to induce a bracketing (unlabeled parsing)from raw text (without even using POS tags) withstrong results.
Its high speed (thousands of wordsper second) allows us to use millions of sentences,a prohibitive number for other parsers.We continue by tagging the corpus usingClark?s unsupervised POS tagger (Clark, 2003)and the unsupervised Prototype Tagger (Abend etal., 2010)2.
The classes corresponding to preposi-tions and to verbs are manually selected from theinduced clusters3.
A preposition is defined to beany word which is the first word of an argumentand belongs to a prepositions cluster.
A verb isany word belonging to a verb cluster.
This manualselection requires only a minute, since the numberof classes is very small (34 in our experiments).In addition, knowing what is considered a prepo-sition is part of the task definition itself.Argument identification is hard even for super-vised models and is considerably more so for un-supervised ones (Abend et al, 2009).
We there-fore confine ourselves to sentences of length notgreater than 10 (excluding punctuation) whichcontain a single verb.
A sequence of words willbe marked as an argument of the verb if it is a con-stituent that does not contain the verb (accordingto the unsupervised parse tree), whose parent isan ancestor of the verb.
This follows the pruningheuristic of (Xue and Palmer, 2004) often used bySRL algorithms.The corpus is now tagged using an unsupervisedPOS tagger.
Since the sentences in question areshort, we consider every word which does not be-long to a closed class cluster as a head word (anargument can have several head words).
A closedclass is a class of function words with relativelyfew word types, each of which is very frequent.Typical examples include determiners, preposi-tions and conjunctions.
A class which is not closedis open.
In this paper, we define closed classes tobe clusters in which the ratio between the numberof word tokens and the number of word types ex-2Clark?s tagger was replaced by the Prototype Taggerwhere the latter gave a significant improvement.
See Sec-tion 4.3We also explore a scenario in which they are identifiedby a supervised tagger.
See Section 4.229ceeds a threshold T 4.Using these annotation layers, we traverse thecorpus and extract every (predicate, slot, argumenthead) triplet.
In case an argument has several headwords, each of them is considered as an inde-pendent sample.
We denote the number of timesthat a triplet occurred in the training corpus byN(p, s, h).3.3 Collocation MeasuresIn this section we present the three types of mea-sures used by the algorithm and the rationale be-hind each of them.
These measures are all basedon the PSH joint distribution.Given a (predicate, prepositional argument) pairfrom the test set, we first tag and parse the argu-ment using the unsupervised tools above5.
Eachword in the argument is now represented by itsword form (without lemmatization), its unsuper-vised POS tag and its depth in the parse tree of theargument.
The last two will be used to determinewhich are the head words of the argument (see be-low).
The head words themselves, once chosen,are represented by the lemma.
We now computethe following measures.Selectional Preference (SP).
Since the seman-tics of cores is more predicate dependent than thesemantics of adjuncts, we expect arguments forwhich the predicate has a strong preference (in aspecific slot) to be cores.Selectional preference induction is a well-established task in NLP.
It aims to quantify thelikelihood that a certain argument appears in acertain slot of a predicate.
Several methods havebeen suggested (Resnik, 1996; Li and Abe, 1998;Schulte im Walde et al, 2008).We use the paradigm of (Erk, 2007).
For a givenpredicate slot pair (p, s), we define its preferenceto the argument head h to be:SP (p, s, h) =?h?
?HeadsPr(h?|p, s) ?
sim(h, h?
)Pr(h|p, s) = N(p, s, h)?h?N(p, s, h?
)sim(h, h?)
is a similarity measure between argu-ment heads.
Heads is the set of all head words.4We use sections 2?21 of the PTB WSJ for these counts,containing 0.95M words.
Our T was set to 50.5Note that while current unsupervised parsers have lowperformance on long sentences, arguments, even in long sen-tences, are usually still short enough for them to operate well.Their average length in the test set is 5.1 words.This is a natural extension of the naive (and sparse)maximum likelihood estimator Pr(h|p, s), whichis obtained by taking sim(h, h?)
to be 1 if h = h?and 0 otherwise.The similarity measure we use is based on theslot distributions of the arguments.
That is, twoarguments are considered similar if they tend toappear in the same slots.
Each head word h is as-signed a vector where each coordinate correspondsto a slot s. The value of the coordinate is the num-ber of times h appeared in s, i.e.
?p?N(p?, s, h)(p?
is summed over all predicates).
The similaritymeasure between two head words is then definedas the cosine measure of their vectors.Since arguments in the test set can be quite long,not every open class word in the argument is takento be a head word.
Instead, only those appearing inthe top level (depth = 1) of the argument under itsunsupervised parse tree are taken.
In case there areno such open class words, we take those appearingin depth 2.
The selectional preference of the wholeargument is then defined to be the arithmetic meanof this measure over all of its head words.
If the ar-gument has no head words under this definition orif none of the head words appeared in the trainingcorpus, the selectional preference is undefined.Predicate-Slot Collocation.
Since cores areobligatory, when a predicate persistently appearswith an argument in a certain slot, the argumentsin this slot tends to be cores.
This notion can becaptured by the (predicate, slot) joint distribu-tion.
We use the Pointwise Mutual Informationmeasure (PMI) to capture the slot and the predi-cate?s collocation tendency.
Let p be a predicateand s a slot, then:PS(p, s) = PMI(p, s) = log Pr(p, s)Pr(s) ?
Pr(p) == log N(p, s)?p?,s?N(p?, s?
)?s?N(p, s?
)?p?N(p?, s)Since there is only a meager number of possi-ble slots (that is, of prepositions), estimating the(predicate, slot) distribution can be made by themaximum likelihood estimator with manageablesparsity.In order not to bias the counts towards predi-cates which tend to take more arguments, we de-fine here N(p, s) to be the number of times the(p, s) pair occurred in the training corpus, irre-spective of the number of head words the argu-ment had (and not e.g., ?hN(p, s, h)).
Argu-230ments with no prepositions are included in thesecounts as well (with s = NULL), so not to biasagainst predicates which tend to have less non-prepositional arguments.Argument-Slot Collocation.
Adjuncts tend tobelong to one of a few specific semantic domains(see Section 2).
Therefore, if an argument tends toappear in a certain slot in many of its instances, itis an indication that this argument tends to have aconsistent semantic flavor in most of its instances.In this case, the argument and the preposition canbe viewed as forming a unit on their own, indepen-dent of the predicate with which they appear.
Wetherefore expect such arguments to be adjuncts.We formalize this notion using the followingmeasure.
Let p, s, h be a predicate, a slot and ahead word respectively.
We then use6:AS(s, h) = 1?Pr(s|h) = 1?
?p?N(p?, s, h)?p?,s?N(p?, s?, h)We select the head words of the argument aswe did with the selectional preference measure.Again, the AS of the whole argument is definedto be the arithmetic mean of the measure over allof its head words.Thresholding.
In order to turn these measuresinto classifiers, we set a threshold below which ar-guments are marked as adjuncts and above whichas cores.
In order to avoid tuning a parameter foreach of the measures, we set the threshold as themedian value of this measure in the test set.
Thatis, we find the threshold which tags half of the ar-guments as cores and half as adjuncts.
This relieson the prior knowledge that prepositional argu-ments are roughly equally divided between coresand adjuncts7.3.4 Combination ModelThe algorithm proceeds to integrate the predic-tions of the weak classifiers into a single classi-fier.
We use an ensemble method (Breiman, 1996).Each of the classifiers may either classify an argu-ment as an adjunct, classify it as a core, or ab-stain.
In order to obtain a high accuracy classifier,to be used for self-training below, the ensembleclassifier only tags arguments for which none of6The conditional probability is subtracted from 1 so thathigher values correspond to cores, as with the other measures.7In case the test data is small, we can use the median valueon the training data instead.the classifiers abstained, i.e., when sufficient infor-mation was available to make all three predictions.The prediction is determined by the majority vote.The ensemble classifier has high precision butlow coverage.
In order to increase its coverage, aself-training step is performed.
We observe that apredicate and a slot generally determine whetherthe argument is a core or an adjunct.
For instance,in our development data, a classifier which assignsall arguments that share a predicate and a slot theirmost common label, yields 94.3% accuracy on thepairs appearing at least 5 times.
This property ofthe core-adjunct distinction greatly simplifies thetask for supervised algorithms (see Section 2).We therefore apply the following procedure: (1)tag the training data with the ensemble classifier;(2) for each test sample x, if more than a ratio of ?of the training samples sharing the same predicateand slot with x are labeled as cores, tag x as core.Otherwise, tag x as adjunct.Test samples which do not share a predicate anda slot with any training sample are considered outof coverage.
The parameter ?
is chosen so halfof the arguments are tagged as cores and half asadjuncts.
In our experiments ?
was about 0.25.4 Experimental SetupExperiments were conducted in two scenarios.
Inthe ?SID?
(supervised identification of prepositionsand verbs) scenario, a gold standard list of prepo-sitions was provided.
The list was generated bytaking every word tagged by the preposition tag(?IN?)
in at least one of its instances under thegold standard annotation of the WSJ sections 2?21.
Verbs were identified using MXPOST (Ratna-parkhi, 1996).
Words tagged with any of the verbtags, except of the auxiliary verbs (?have?, ?be?
and?do?)
were considered predicates.
This scenariodecouples the accuracy of the algorithm from thequality of the unsupervised POS tagging.In the ?Fully Unsupervised?
scenario, preposi-tions and verbs were identified using Clark?s tag-ger (Clark, 2003).
It was asked to produce a tag-ging into 34 classes.
The classes correspondingto prepositions and to verbs were manually identi-fied.
Prepositions in the test set were detected with84.2% precision and 91.6% recall.The prediction of whether a word belongs to anopen class or a closed was based on the output ofthe Prototype tagger (Abend et al, 2010).
ThePrototype tagger provided significantly more ac-231curate predictions in this context than Clark?s.The 39832 sentences of PropBank?s sections 2?21 were used as a test set without bounding theirlengths8.
Cores were defined to be any argumentbearing the labels ?A0?
?
?A5?, ?C-A0?
?
?C-A5?or ?R-A0?
?
?R-A5?.
Adjuncts were defined tobe arguments bearing the labels ?AM?, ?C-AM?
or?R-AM?.
Modals (?AM-MOD?)
and negation mod-ifiers (?AM-NEG?)
were omitted since they do notrepresent adjuncts.The test set includes 213473 arguments, 45939(21.5%) are prepositional.
Of the latter, 22442(48.9%) are cores and 23497 (51.1%) are adjuncts.The non-prepositional arguments include 145767(87%) cores and 21767 (13%) adjuncts.
The aver-age number of words per argument is 5.1.The NANC (Graff, 1995) corpus was used as atraining set.
Only sentences of length not greaterthan 10 excluding punctuation were used (see Sec-tion 3.2), totaling 4955181 sentences.
7673878(5635810) arguments were identified in the ?SID?
(?Fully Unsupervised?)
scenario.
The averagenumber of words per argument is 1.6 (1.7).Since this is the first work to tackle this taskusing neither manual nor supervised syntactic an-notation, there is no previous work to compareto.
However, we do compare against a non-trivialbaseline, which closely follows the rationale ofcores as obligatory arguments.Our Window Baseline tags a corpus using MX-POST and computes, for each predicate andpreposition, the ratio between the number of timesthat the preposition appeared in a window of Wwords after the verb and the total number oftimes that the verb appeared.
If this number ex-ceeds a certain threshold ?, all arguments hav-ing that predicate and preposition are tagged ascores.
Otherwise, they are tagged as adjuncts.
Weused 18.7M sentences from NANC of unboundedlength for this baseline.
W and ?
were fine-tunedagainst the test set9.We also report results for partial versions ofthe algorithm, starting with the three measuresused (selectional preference, predicate-slot col-location and argument-slot collocation).
Resultsfor the ensemble classifier (prior to the bootstrap-ping stage) are presented in two variants: one8The first 15K arguments were used for the algorithm?sdevelopment and therefore excluded from the evaluation.9Their optimal value was found to be W=2, ?=0.03.
Thelow optimal value of ?
is an indication of the noisiness of thistechnique.in which the ensemble is used to tag argumentsfor which all three measures give a prediction(the ?Ensemble(Intersection)?
classifier) and onein which the ensemble tags all arguments forwhich at least one classifier gives a prediction (the?Ensemble(Union)?
classifier).
For the latter, a tieis broken in favor of the core label.
The ?Ensem-ble(Union)?
classifier is not a part of our modeland is evaluated only as a reference.In order to provide a broader perspective on thetask, we compare the measures in the basis of ouralgorithm to simplified or alternative measures.We experiment with the following measures:1.
Simple SP ?
a selectional preference measuredefined to be Pr(head|slot, predicate).2.
Vast Corpus SP ?
similar to ?Simple SP?but with a much larger corpus.
It uses roughly100M arguments which were extracted from theweb-crawling based corpus of (Gabrilovich andMarkovitch, 2005) and the British National Cor-pus (Burnard, 2000).3.
Thesaurus SP ?
a selectional preference mea-sure which follows the paradigm of (Erk, 2007)(Section 3.3) and defines the similarity betweentwo heads to be the Jaccard affinity between theirtwo entries in Lin?s automatically compiled the-saurus (Lin, 1998)10.4.
Pr(slot|predicate) ?
an alternative to the usedpredicate-slot collocation measure.5.
PMI(slot, head) ?
an alternative to the usedargument-slot collocation measure.6.
Head Dependence ?
the entropy of the pred-icate distribution given the slot and the head (fol-lowing (Merlo and Esteve Ferrer, 2006)):HD(s, h) = ?
?pPr(p|s, h) ?
log(Pr(p|s, h))Low entropy implies a core.For each of the scenarios and the algorithms,we report accuracy, coverage and effective accu-racy.
Effective accuracy is defined to be the ac-curacy obtained when all out of coverage argu-ments are tagged as adjuncts.
This procedure al-ways yields a classifier with 100% coverage andtherefore provides an even ground for comparingthe algorithms?
performance.We see accuracy as important on its own rightsince increasing coverage is often straightforwardgiven easily obtainable larger training corpora.10Since we aim for a minimally supervised scenario,we used the proximity-based version of his thesauruswhich does not require parsing as pre-processing.http://webdocs.cs.ualberta.ca/?lindek/Downloads/sims.lsp.gz232Collocation Measures Ensemble + Cov.Sel.
Preference Pred-Slot Arg-Slot Ensemble(I) Ensemble(U) E(I) + STSID Scenario Accuracy 65.6 64.5 72.4 74.1 68.7 70.6Coverage 35.6 77.8 44.7 33.2 88.1 74.2Eff.
Acc.
56.7 64.8 58.8 58.8 67.8 68.4Fully Unsupervised Accuracy 62.6 61.1 69.4 70.6 64.8 68.8Scenario Coverage 24.8 59.0 38.7 22.8 74.2 56.9Eff.
Acc.
52.6 57.5 55.8 53.8 61.0 61.4Table 1: Results for the various models.
Accuracy, coverage and effective accuracy are presented in percents.
Effectiveaccuracy is defined to be the accuracy resulting from labeling each out of coverage argument with an adjunct label.
Therows represent the following models (left to right): selectional preference, predicate-slot collocation, argument-slot collocation,?Ensemble(Intersection)?, ?Ensemble(Union)?
and the ?Ensemble(Intersection)?
followed by self-training (see Section 3.4).
?En-semble(Intersection)?
obtains the highest accuracy.
The ensemble + self-training obtains the highest effective accuracy.Selectional Preference Measures Pred-Slot Measures Arg-Slot MeasuresSP?
S. SP V.C.
SP Lin SP PS?
Pr(s|p) Window AS?
PMI(s, h) HDAcc.
65.6 41.6 44.8 49.9 64.5 58.9 64.1 72.4 67.5 67.4Cov.
35.6 36.9 45.3 36.7 77.8 77.8 92.6 44.7 44.7 44.7Eff.
Acc.
56.7 48.2 47.7 51.3 64.8 60.5 65.0 58.8 56.6 56.6Table 2: Comparison of the measures used by our model to alternative measures in the ?SID?
scenario.
Results are in percents.The sections of the table are (from left to right): selectional preference measures, predicate-slot measures, argument-slot mea-sures and head dependence.
The measures are (left to right): SP?, Simple SP, Vast Corpus SP, Lin SP, PS?, Pr(slot|predicate),Window Baseline, AS?, PMI(slot, head) and Head Dependence.
The measures marked with ?
are the ones used by our model.See Section 4.Another reason is that a high accuracy classifiermay provide training data to be used by subse-quent supervised algorithms.For completeness, we also provide results forthe entire set of arguments.
The great majority ofnon-prepositional arguments are cores (87% in thetest set).
We therefore tag all non-prepositional ascores and tag prepositional arguments using ourmodel.
In order to minimize supervision, we dis-tinguish between the prepositional and the non-prepositional arguments using Clark?s tagger.Finally, we experiment on a scenario whereeven argument identification on the test set isnot provided, but performed by the algorithm of(Abend et al, 2009), which uses neither syntacticnor SRL annotation but does utilize a supervisedPOS tagger.
We therefore run it in the ?SID?
sce-nario.
We apply it to the sentences of length atmost 10 contained in sections 2?21 of PB (11586arguments in 6007 sentences).
Non-prepositionalarguments are invariably tagged as cores and outof coverage prepositional arguments as adjuncts.We report labeled and unlabeled recall, preci-sion and F-scores for this experiment.
An un-labeled match is defined to be an argument thatagrees in its boundaries with a gold standard ar-gument and a labeled match requires in additionthat the arguments agree in their core/adjunct la-bel.
We also report labeling accuracy which is theratio between the number of labeled matches andthe number of unlabeled matches11.5 ResultsTable 1 presents the results of our main experi-ments.
In both scenarios, the most accurate of thethree basic classifiers was the argument-slot col-location classifier.
This is an indication that thecollocation between the argument and the prepo-sition is more indicative of the core/adjunct labelthan the obligatoriness of the slot (as expressed bythe predicate-slot collocation).Indeed, we can find examples where adjuncts,although optional, appear very often with a certainverb.
An example is ?meet?, which often takes atemporal adjunct, as in ?Let?s meet [in July]?.
Thisis a semantic property of ?meet?, whose syntacticexpression is not obligatory.All measures suffered from a comparable dete-rioration of accuracy when moving from the ?SID?to the ?Fully Unsupervised?
scenario.
The dete-rioration in coverage, however, was considerablylower for the argument-slot collocation.The ?Ensemble(Intersection)?
model in bothcases is more accurate than each of the basic clas-sifiers alone.
This is to be expected as it combinesthe predictions of all three.
The self-training stepsignificantly increases the ensemble model?s cov-11Note that the reported unlabeled scores are slightly lowerthan those reported in the 2009 paper, due to the exclusion ofthe modals and negation modifiers.233Precision Recall F-score lAcc.Unlabeled 50.7 66.3 57.5 ?Labeled 42.4 55.4 48.0 83.6Table 3: Unlabeled and labeled scores for the experi-ments using the unsupervised argument identification systemof (Abend et al, 2009).
Precision, recall, F-score and label-ing accuracy are given in percents.erage (with some loss in accuracy), thus obtainingthe highest effective accuracy.
It is also more accu-rate than the simpler classifier ?Ensemble(Union)?
(although the latter?s coverage is higher).Table 2 presents results for the comparison tosimpler or alternative measures.
Results indicatethat the three measures used by our algorithm(leftmost column in each section) obtain superiorresults.
The only case in which performance iscomparable is the window baseline compared tothe Pred-Slot measure.
However, the baseline?sscore was obtained by using a much larger corpusand a careful hand-tuning of the parameters12.The poor performance of Simple SP can be as-cribed to sparsity.
This is demonstrated by themedian value of 0, which this measure obtainedon the test set.
Accuracy is only somewhat betterwith a much larger corpus (Vast Corpus SP).
TheThesaurus SP most probably failed due to insuffi-cient coverage, despite its applicability in a similarsupervised task (Zapirain et al, 2009).The Head Dependence measure achieves a rel-atively high accuracy of 67.4%.
We therefore at-tempted to incorporate it into our model, but failedto achieve a significant improvement to the overallresult.
We expect a further study of the relationsbetween the measures will suggest better ways ofcombining their predictions.The obtained effective accuracy for the entireset of arguments, where the prepositional argu-ments are automatically identified, was 81.6%.Table 3 presents results of our experiments withthe unsupervised argument identification modelof (Abend et al, 2009).
The unlabeled scoresreflect performance on argument identificationalone, while the labeled scores reflect the joint per-formance of both the 2009 and our algorithms.These results, albeit low, are potentially benefi-cial for unsupervised subcategorization acquisi-tion.
The accuracy of our model on the entireset (prepositional argument subset) of correctlyidentified arguments was 83.6% (71.7%).
This is12We tried about 150 parameter pairs for the baseline.
Theaverage of the five best effective accuracies was 64.3%.somewhat higher than the score on the entire testset (?SID?
scenario), which was 83.0% (68.4%),probably due to the bounded length of the test sen-tences in this case.6 ConclusionWe presented a fully unsupervised algorithm forthe classification of arguments into cores and ad-juncts.
Since most non-prepositional argumentsare cores, we focused on prepositional arguments,which are roughly equally divided between coresand adjuncts.
The algorithm computes three sta-tistical measures and utilizes ensemble-based andself-training methods to combine their predictions.The algorithm applies state-of-the-art unsuper-vised parser and POS tagger to collect statisticsfrom a large raw text corpus.
It obtains an accu-racy of roughly 70%.
We also show that (some-what surprisingly) an argument-slot collocationmeasure gives more accurate predictions than apredicate-slot collocation measure on this task.We speculate the reason is that the head word dis-ambiguates the preposition and that this disam-biguation generally determines whether a preposi-tional argument is a core or an adjunct (somewhatindependently of the predicate).
This calls fora future study into the semantics of prepositionsand their relation to the core-adjunct distinction.In this context two recent projects, The Preposi-tion Project (Litkowski and Hargraves, 2005) andPrepNet (Saint-Dizier, 2006), which attempt tocharacterize and categorize the complex syntacticand semantic behavior of prepositions, may be ofrelevance.It is our hope that this work will provide a betterunderstanding of core-adjunct phenomena.
Cur-rent supervised SRL models tend to perform worseon adjuncts than on cores (Pradhan et al, 2008;Toutanova et al, 2008).
We believe a better under-standing of the differences between cores and ad-juncts may contribute to the development of betterSRL techniques, in both its supervised and unsu-pervised variants.ReferencesOmri Abend, Roi Reichart and Ari Rappoport, 2009.Unsupervised Argument Identification for SemanticRole Labeling.
ACL ?09.Omri Abend, Roi Reichart and Ari Rappoport, 2010.Improved Unsupervised POS Induction through Pro-totype Discovery.
ACL ?10.234Collin F. Baker, Charles J. Fillmore and John B. Lowe,1998.
The Berkeley FrameNet Project.
ACL-COLING ?98.Timothy Baldwin, Valia Kordoni and Aline Villavicen-cio, 2009.
Prepositions in Applications: A Sur-vey and Introduction to the Special Issue.
Computa-tional Linguistics, 35(2):119?147.Ram Boukobza and Ari Rappoport, 2009.
Multi-Word Expression Identification Using Sentence Sur-face Features.
EMNLP ?09.Leo Breiman, 1996.
Bagging Predictors.
MachineLearning, 24(2):123?140.Ted Briscoe and John Carroll, 1997.
Automatic Ex-traction of Subcategorization from Corpora.
Ap-plied NLP ?97.Lou Burnard, 2000.
User Reference Guide for theBritish National Corpus.
Technical report, OxfordUniversity.Xavier Carreras and Llu?`s Ma`rquez, 2005.
Intro-duction to the CoNLL?2005 Shared Task: SemanticRole Labeling.
CoNLL ?05.Alexander Clark, 2003.
Combining Distributional andMorphological Information for Part of Speech In-duction.
EACL ?03.Michael Collins, 1999.
Head-driven statistical modelsfor natural language parsing.
Ph.D. thesis, Univer-sity of Pennsylvania.David Dowty, 2000.
The Dual Analysis of Adjunctsand Complements in Categorial Grammar.
Modify-ing Adjuncts, ed.
Lang, Maienborn and Fabricius?Hansen, de Gruyter, 2003.Katrin Erk, 2007.
A Simple, Similarity-based Modelfor Selectional Preferences.
ACL ?07.Evgeniy Gabrilovich and Shaul Markovitch, 2005.Feature Generation for Text Categorization usingWorld Knowledge.
IJCAI ?05.David Graff, 1995.
North American News Text Cor-pus.
Linguistic Data Consortium.
LDC95T21.Trond Grenager and Christopher D. Manning, 2006.Unsupervised Discovery of a Statistical Verb Lexi-con.
EMNLP ?06.Donald Hindle and Mats Rooth, 1993.
Structural Am-biguity and Lexical Relations.
Computational Lin-guistics, 19(1):103?120.Julia Hockenmaier, 2003.
Data and Models for Sta-tistical Parsing with Combinatory Categorial Gram-mar.
Ph.D. thesis, University of Edinburgh.Karin Kipper, Hoa Trang Dang and Martha Palmer,2000.
Class-Based Construction of a Verb Lexicon.AAAI ?00.Anna Korhonen, 2002.
Subcategorization Acquisition.Ph.D.
thesis, University of Cambridge.Hang Li and Naoki Abe, 1998.
Generalizing CaseFrames using a Thesaurus and the MDL Principle.Computational Linguistics, 24(2):217?244.Wei Li, Xiuhong Zhang, Cheng Niu, Yuankai Jiang andRohini Srihari, 2003.
An Expert Lexicon Approachto Identifying English Phrasal Verbs.
ACL ?03.Dekang Lin, 1998.
Automatic Retrieval and Cluster-ing of Similar Words.
COLING?ACL ?98.Ken Litkowski and Orin Hargraves, 2005.
The Prepo-sition Project.
ACL-SIGSEM Workshop on ?TheLinguistic Dimensions of Prepositions and TheirUse in Computational Linguistic Formalisms andApplications?.Diana McCarthy, 2001.
Lexical Acquisition at theSyntax-Semantics Interface: Diathesis Alternations,Subcategorization Frames and Selectional Prefer-ences.
Ph.D. thesis, University of Sussex.Paula Merlo and Eva Esteve Ferrer, 2006.
The No-tion of Argument in Prepositional Phrase Attach-ment.
Computational Linguistics, 32(3):341?377.Martha Palmer, Daniel Gildea and Paul Kingsbury,2005.
The Proposition Bank: A Corpus Annotatedwith Semantic Roles.
Computational Linguistics,31(1):71?106.Sameer Pradhan, Wayne Ward and James H. Martin,2008.
Towards Robust Semantic Role Labeling.Computational Linguistics, 34(2):289?310.Vasin Punyakanok, Dan Roth and Wen-tau Yih, 2008.The Importance of Syntactic Parsing and Inferencein Semantic Role Labeling.
Computational Linguis-tics, 34(2):257?287.Adwait Ratnaparkhi, 1996.
Maximum Entropy Part-Of-Speech Tagger.
EMNLP ?96.Roi Reichart, Omri Abend and Ari Rappoport, 2010.Type Level Clustering Evaluation: New Measuresand a POS Induction Case Study.
CoNLL ?10.Philip Resnik, 1996.
Selectional constraints: Aninformation-theoretic model and its computationalrealization.
Cognition, 61:127?159.Patrick Saint-Dizier, 2006.
PrepNet: A MultilingualLexical Description of Prepositions.
LREC ?06.Anoop Sarkar and Daniel Zeman, 2000.
AutomaticExtraction of Subcategorization Frames for Czech.COLING ?00.Sabine Schulte im Walde, Christian Hying, ChristianScheible and Helmut Schmid, 2008.
CombiningEM Training and the MDL Principle for an Auto-matic Verb Classification Incorporating SelectionalPreferences.
ACL ?08.235Yoav Seginer, 2007.
Fast Unsupervised IncrementalParsing.
ACL ?07.Caroline Sporleder and Linlin Li, 2009.
UnsupervisedRecognition of Literal and Non-Literal Use of Id-iomatic Expressions.
EACL ?09.Robert S. Swier and Suzanne Stevenson, 2004.
Unsu-pervised Semantic Role Labeling.
EMNLP ?04.Robert S. Swier and Suzanne Stevenson, 2005.
Ex-ploiting a Verb Lexicon in Automatic Semantic RoleLabelling.
EMNLP ?05.Kristina Toutanova, Aria Haghighi and Christopher D.Manning, 2008.
A Global Joint Model for Se-mantic Role Labeling.
Computational Linguistics,34(2):161?191.Aline Villavicencio, 2002.
Learning to Distinguish PPArguments from Adjuncts.
CoNLL ?02.Dave Willis, 2004.
Collins Cobuild Intermedia En-glish Grammar, Second Edition.
HarperCollins Pub-lishers.Nianwen Xue and Martha Palmer, 2004.
CalibratingFeatures for Semantic Role Labeling.
EMNLP ?04.Ben?at Zapirain, Eneko Agirre and Llu?
?s Ma`rquez,2009.
Generalizing over Lexical Features: Selec-tional Preferences for Semantic Role Classification.ACL ?09, short paper.236
