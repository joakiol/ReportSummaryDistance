Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1629?1638,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsContext-Sensitive Lexicon Features for Neural Sentiment AnalysisZhiyang Teng, Duy-Tin Vo and Yue ZhangSingapore University of Technology and Design{zhiyang teng, duytin vo}@mymail.sutd.edu.sgyue zhang@sutd.edu.sgAbstractSentiment lexicons have been leveraged as auseful source of features for sentiment anal-ysis models, leading to the state-of-the-artaccuracies.
On the other hand, most ex-isting methods use sentiment lexicons with-out considering context, typically taking thecount, sum of strength, or maximum senti-ment scores over the whole input.
We pro-pose a context-sensitive lexicon-based methodbased on a simple weighted-sum model, usinga recurrent neural network to learn the sen-timents strength, intensification and negationof lexicon sentiments in composing the sen-timent value of sentences.
Results show thatour model can not only learn such operationdetails, but also give significant improvementsover state-of-the-art recurrent neural networkbaselines without lexical features, achievingthe best results on a Twitter benchmark.1 IntroductionSentiment lexicons (Hu and Liu, 2004; Wilson et al,2005; Esuli and Sebastiani, 2006) have been a usefulresource for opinion mining (Kim and Hovy, 2004;Agarwal et al, 2011; Moilanen and Pulman, 2007;Choi and Cardie, 2008; Mohammad et al, 2013;Guerini et al, 2013; Vo and Zhang, 2015).
Contain-ing sentiment attributes of words such as polaritiesand strengths, they can serve to provide a word-levelfoundation for analyzing the sentiment of sentencesand documents.
We investigate an effective way touse sentiment lexicon features.A traditional way of deciding the sentiment of adocument is to use the sum of sentiment values ofIt?s an insignificant [criticism]?1?
?0.5.Nobody gives a [good]+3?
?1 performance in thismovieShe?s not [terrific]+5?+1 but not [terrible]?5?
?1either.It?s not a very [good]+3?
?0.25 movie song!It removes my [doubts]?3?+1.Figure 1: Example sentiment compositions.all words in the document that exist in a sentimentlexicon (Turney, 2002; Hu and Liu, 2004).
Thissimple method has been shown to give surprisinglycompetitive accuracies in several sentiment analysisbenchmarks (Kiritchenko et al, 2014), and is stillthe standard practice for specific research commu-nities with mature domain-specific lexicons, suchas finance (Kearney and Liu, 2014) and product re-views (Ding et al, 2008).More sophisticated sentence-level features suchas the counts of positive and negative words, theirtotal strength, and the maximum strength, etc, havealso been exploited (Kim and Hovy, 2004; Wilson etal., 2005; Agarwal et al, 2011).
Such lexicon fea-tures have been shown highly effective, leading tothe best accuracies in the SemEval shared task (Mo-hammad et al, 2013).
On the other hand, they aretypically based on bag-of-word models, hence suf-fering two limitations.
First, they do not explicitlyhandle semantic compositionality (Polanyi and Za-enen, 2006; Moilanen and Pulman, 2007; Taboadaet al, 2011), some examples of which are shown inFigure 1.
The composition effects can exhibit in-tricacies such as negation over intensification (e.g.not very good), shifting (e.g.
not terrific) vs flip-1629ping negation (e.g.
not acceptable), content wordnegation (e.g.
removes my doubts) and unboundeddependencies (e.g.
No body gives a good perfor-mance).Second, they cannot effectively deal with wordsense variations (Devitt and Ahmad, 2007; De-necke, 2009).
Guerini et al (2013) show chal-lenges in modeling the correlation between context-dependent posterior word sentiments and their con-text independent priors.
For example, the sentimentvalue of ?cold?
varies between ?cold beer?, ?coldpizza?
and ?cold person?
due to sense and contextdifferences.
Such variations raise difficulties for asentiment classifier with bag-of-word nature, sincethey can depend on semantic information over longphrases or the full sentence.We investigate a method that can potentially ad-dress the above issues, by using a recurrent neu-ral network to capture context-dependent seman-tic composition effects over sentences.
Shown inFigure 2, the model is conceptually simple, us-ing a weighted sum of lexicon sentiments and asentence-level bias to estimate the sentiment valueof a sentence.
The key idea is to use a bi-directionallong-short-term-memory (LSTM) (Hochreiter andSchmidhuber, 1997; Graves et al, 2013) model tocapture global syntactic dependencies and seman-tic information, based on which the weight of eachsentiment word together with a sentence-level sen-timent bias score are predicted.
Such weights arecontext-sensitive, and can express flipping negationby having negative values.The advantages of the recurrent network modelover existing semantic-composition-aware discretemodels such as (Choi and Cardie, 2008) include itscapability of representing non-local and subtle se-mantic features without suffering from the challengeof designing sparse manual features.
On the otherhand, compared with neural network models, whichrecently give the state-of-the-art accuracies (Li etal., 2015; Tai et al, 2015), our model has the ad-vantage of leveraging sentiment lexicons as a usefulresource.
To our knowledge, we are the first to in-tegrate the operation into sentiment lexicons and adeep neural model for sentiment analysis.The conceptually simple model gives strong em-pirical performances.
Results on standard sentimentbenchmarks show that our method gives competitiveFigure 2: Overall model structure.
The sentiment score of thesentence ?not a bad movie at all?
is a weighted sum of the scoresof sentiment words ?not?, ?bad?
and a sentence-level bias scoreb.
score(not) and score(bad) are prior scores obtained fromsentiment lexicons.
?1 and ?3 are context-sensitive weights forsentiment words ?not?
and ?bad?, respectively.accuracies to the state-of-the-art models in the liter-ature.
As a by-product, the model can also correctlyidentify the compositional changes on the sentimentvalues of each word given a sentential context.Our code is released athttps://github.com/zeeeyang/lexicon rnn.2 Related WorkThere exist many statistical methods that exploitsentiment lexicons (Kim and Hovy, 2004; Agarwalet al, 2011; Mohammad et al, 2013; Guerini et al,2013; Tang et al, 2014b; Vo and Zhang, 2015; Cam-bria, 2016).
Mohammad et al (2013) leverage alarge sentiment lexicon in a SVM model, achiev-ing the best results in the SemEval 2013 bench-mark on sentence-level sentiment analysis (Nakov etal., 2013).
Compared to these methods, our modelhas two main advantages.
First, we use a recurrentneural network to model context, thereby exploitingnon-local semantic information.
Second, our modeloffers context-sensitive operational details on eachword.Several previous methods move beyond bag-of-word models in leveraging lexicons.
Most notably,Moilanen and Pulman (2007) introduce the ideasfrom compositional semantics (Montague, 1974)into sentiment operations, developing a set of com-position rules for handling negations.
Along theline, Taboada et al (2011) developed a lexicon anda collection of sophisticated rules for addressing in-tensification, negation and other phenomena.
Differ-1630ent from these rule-based methods, Choi and Cardie(2008) use a structured linear model to learn seman-tic compositionality relying on a set of manual fea-tures.
In contrast, we leverage a recurrent neuralmodel for inducing semantic composition featuresautomatically.
Our weighted-sum representation ofsemantic compositionality is formally simpler com-pared with fine-grained rules such as (Taboada et al,2011).
However, it is sufficient for describing theresulting effect of complex and context-dependentoperations, with the semantic composition processbeing modeled by LSTM.
Our sentiment analyzeralso enjoys a more competitive LSTM baseline com-pared to a traditional discrete models.Our work is also related to recent work on us-ing deep neural networks for sentence-level senti-ment analysis, which exploits convolutional (Kalch-brenner et al, 2014; Kim, 2014; Ren et al, 2016),recursive (Socher et al, 2013; Dong et al, 2014;Nguyen and Shirai, 2015) and recurrent neural net-works (Liu et al, 2015; Wang et al, 2015; Zhang etal., 2016), giving highly competitive accuracies.
Asour baseline, LSTM (Tai et al, 2015; Li et al, 2015)stands among the best neural methods.
Our modelis different from these prior methods in mainly twoaspects.
First, we introduce sentiment lexicon fea-tures, which effectively improve classification ac-curacies.
Second, we learn extra operation details,namely the weights on each word, automatically ashidden variables.
While the baseline uses LSTMfeatures to perform end-to-end mapping betweensentences and sentiments, our model uses them to in-duce the lexicon weights, via which word level sen-timent are composed to derive sentence level senti-ment.3 ModelFormally, given a sentence s = w1w2...wn and asentiment lexicon D, denote the subjective words ins as wDj1wDj2 ...wDjm .
Our model calculates the senti-ment score of s according to D in the form ofScore(s) =m?t=1?jtscore(wDjt ) + b, (1)where Score(wDjt ) is the sentiment value of wjt , ?jtare sentiment weights and b is a sentence-level bias.The sentiment values of words and sentences are realnumbers, with the sign indicating the polarity andthe absolute value indicating the strength.As shown in Figure 2, our neural model consistsof three main layers, namely the input layer, thefeature layer and the output layer.
The input layermaps each word in the input sentence into a densereal-value vector.
The feature layer exploits a bi-directional LSTM (Graves and Schmidhuber, 2005;Graves et al, 2013) to extract non-local semantic in-formation over the sequence.
The output layer cal-culates a weight score for each sentiment word, aswell as an overall sentiment bias of the sentence.In this figure, the score of the sentence ?not abad movie at all?
is decided by a weighted sum ofthe sentiments of ?bad?
and ?not?1, and a sentimentshift bias based on the sentence structure.
Ideally,the weight on ?not?
should be a small negative value,which results in a slightly positive sentiment shift.The weight on ?bad?
should be negative, which rep-resents a flip in the polarity.
These weights jointlymodel a negation effect that involves both shiftingand flipping.3.1 Bidirectional LSTMWe use LSTM (Hochreiter and Schmidhuber, 1997)for feature extraction, which recurrently processessentence s token by token.
For each word wt, themodel calculate a hidden state vector ht.
A LSTMcell block makes use of an input gate it, a memorycell ct, a forget gate ft and an output gate ot to con-trol information flow from the history x1...xt andh1...ht?1 to the current state ht.
Formally, ht iscomputed as follows:it = ?
(Wixt + Uiht?1 + Vict?1 + bi)ft = 1.0?
itgt = tanh(Wgxt + Ught?1 + bg)ct = ft  ct?1 + it  gtot = ?
(Woxt + Uoht?1 + Voct + bo)ht = ot  tanh(ct)Here xt is the word embedding of word wt, ?
de-notes the sigmoid function,  is element-wise mul-tiplication.
Wi, Ui, Vi, bi, Wg, Ug, bg, Wo, Uo,Vo and bo are LSTM parameters.1Most sentiment lexicons assign a negative score to the word?not?.1631We apply a bidirectional extension of LSTM(BiLSTM) (Graves and Schmidhuber, 2005; Graveset al, 2013), shown in Figure 2, to encode the inputsentence s both left-to-right and right-to-left.
TheBiLSTM model maps each word wt to a pair ofhidden vectors hLt and hRt , which denote the hid-den vector of the left-to-right LSTM and right-to-left LSTM, respectively.
We use different parame-ters for the left-to-right LSTM and the right-to-leftLSTM.
These state vectors are used as features forcalculating the sentiment weights ?.In addition, we append a sentence end markerw<e> to the left-to-right LSTM and a sentence startmarker w<s> to the right-to-left LSTM.
The hiddenstate vector of w<s> and w<e> are denoted as hRsand hLe , respectively.3.2 Output LayerThe base score.
Given a lexicon word wjt in thesentence s (wjt ?
D), we use the hidden state vec-tors hLjt and hRjt in the feature layer to calculate aweight value ?jt .
As shown in Figure 3, a two-layerneural network is used to induce ?jt .
In particular,a hidden layer combines hLt and hRt using a non-linear tanh activationpsjt = tanh(WLpshLjt + WRpshRjt + bps) (2)The resulting hidden vector psjt is then mapped into?jt using another tanh layer.?
sjt = 2 tanh(Wpwpsjt + bpw) (3)We choose the 2tanh function to make the learnedweights conceptually useful.
The factor 2 is in-troduced for modelling the effect of intensification.Since the range of tanh function is [?1, 1], the rangeof 2tanh is [?2, 2].
Intuitively, a weight value of1 maps the word sentiment directly to the sentencesentiment, such as the weight for ?good?
in ?This isgood?.
A weight value in (1, 2] represents intensifi-cation, such as the weight for ?bad?
in ?very bad?.Similarly, a weight value in (0, 1) represents weak-ening, and a weight in (?2, 0) represents variousscales of negations.Given all lexicon words wDjt in the sentence, wecalculate a base score for the sentenceSbase =?mt=1 ?jtscore(wDjt )m (4)Figure 3: Weight score calculation.By averaging the score of each word, the resultingSbase is confined to [?2?, 2?
], where ?
is the maxi-mum absolute value of word sentiment.
In the aboveequations, WLps, WRps, bps, Wpw and bpw aremodel parameters.The bias score.
We use the same neural networkstructure in Figure 3 to calculate the overall bias ofthe input sentence.
The input to the neural networkincludes hRs and hLe , and the output is a bias scoreSbias .
Intuitively, the calculation of Sbias relies oninformation of the full sentence.
hRs and hLe arechosen because they have commonly been used inthe research literature to represent overall sententialinformation (Graves et al, 2013; Cho et al, 2014).We use a dedicated set of parameters for calculat-ing the bias, wherepB = tanh(WLpbhLe + WRpbhRs + bpb) (5)andSbias = 2 tanh(WbpB + bp) (6)WLpb, WRpb, bpb, Wb and bLp are parameters.3.3 Final Score CalculationThe base Sbase and bias Sbias are linearly interpo-lated to derive the final sentiment value for the sen-tence s.Score(s) = ?Sbase + (1?
?
)Sbias (7)?
?
[0, 1] reflects the relative importance of the basescore in the sentence.
It offers a new degree of modelflexibility, and should be calculated for each sen-tence specifically.
We use the attention model (Bah-danau et al, 2014) to this end.
In particular, thebase score features hLt /hRt and the bias score fea-tures hLe /hRs are combined in the calculation?
= ?
(Ws?hbase + Wb?hbias + b?)
(8)1632wherehbias = hLe ?
hRs (9)andhbase =?mt=1 hLjt ?
hRjtm (10)Here ?
denotes the sigmoid activation function and?
denotes vector concatenation.
Ws?, Wb?
andb?
are model parameters.The final score of the sentence isScore(s) = ?Sbase + (1?
?
)Sbias= ?mm?t=1?jtscore(wDjt ) + (1?
?
)SbiasThis corresponds to the original Equation 1 by ?jt =?m?jt and b = (1?
?
)Sbias.3.4 Training and TestingOur training data contains two different settings.The first is binary sentiment classification.
In thistask, every sentence si is annotated with a sentimentlabel li, where li = 0 and li = 1 to indicate negativeand positive sentiment, respectively.
We apply logis-tic regression on the output layer.
Denote the proba-bility of a sentence si being positive and negative asp1si and p0si respectively.
p0si and p1si are estimated asp1si = ?
(Score(si))p0si = 1?
p1si(11)Suppose that there areN training sentences, the lossfunction over the training set is defined asL(?)
= ?N?i=1log plisi +?r2 ||?||2, (12)where ?
is the set of model parameters.
?r is a pa-rameter for L2 regularization.The second setting is multi-class classification.
Inthis task, every sentence si is assigned a sentimentlabel li from 0 to 4, which represent very negative,negative, neutral, positive and very positive, respec-tively.
We apply least square regression on the out-put layer.
Since the output range of 2tanh is [-2, 2],the value of the base score and the bias score bothbelongs to [-2, 2].
The final score is a weighted sumof the base score and the bias score, also belongingto [-2, 2].
However, the gold sentiment label rangesPositive Negative TotalTrain 3,009 1,187 4,196Dev 483 283 766Test 1,313 490 1,803Table 1: Statistics of the Twitter dataset.Task Label TrainingSentencesDevSentencesTestSentences5-class-2 1,092 139 279-1 2,218 289 6330 1,624 229 3891 2,322 279 5102 1,288 165 3992-class 0 3,310 444 9091 3,610 428 912Table 2: Statistics of SST.from 0 to 4.
We add an offset -2 to every gold sen-timent label to both adapt our model to the train-ing data and to increase the interpretability of thelearned weights.
The loss function for this problemis then defined asL(?)
=N?i=1(Score(si)?
li)2 +?r2 ||?||2 (13)During testing, we predict the sentiment label l?i ofa sentence si byl?i =???????????????
?2 if Score(si) ?
?1.5?1 if ?
1.5 < Score(si) ?
?0.50 if ?
0.5 < Score(si) ?
0.51 if 0.5 < Score(si) ?
1.52 if Score(si) > 1.5(14)4 Experiments4.1 Experimental SettingsData.
We test our model on three datasets, includ-ing a dataset on Twitter sentiment classification, adataset on movie review and a dataset with mixeddomains.
The Twitter dataset is taken from Se-mEval 2013 (Nakov et al, 2013).
We downloadedthe dataset according to the released ids.
The statis-tics of the dataset are shown in Table 1.The movie review dataset is Stanford SentimentTreebank2 (SST) (Socher et al, 2013).
For each sen-tence in this treebank, a corresponding constituent2http://nlp.stanford.edu/sentiment/index.html1633Polarity books dvds electronics music videogamesPositive 19 19 19 20 20Negative 29 20 19 20 20Table 3: Document distribution of the mixed domain dataset.tree is given.
Each internal constituent node is an-notated with a sentiment label ranging from 0 to 4.We follow Socher et al (2011) and Li et al (2015)to perform five-class and binary classification, withthe data statistics being shown in Table 2.In order to examine cross-domain robustness,we apply our model on a product review cor-pus (Ta?ckstro?m and McDonald, 2011), which con-tains 196 documents covering 5 domains: books,dvds, electronics, music and videogames.
The doc-ument distribution is listed in Table 3.Lexicons.
We use four sentiment lexicons,namely TS-Lex, S140-Lex, SD-Lex and SWN-Lex.TS-Lex3 is a large-scale sentiment lexicon builtfrom Twitter by Tang et al (2014a) for learningsentiment-specific phrase embeddings.
S140-Lex4is the Sentiment140 lexicon, which is built frompoint-wise mutual information using distant super-vision (Go et al, 2009; Mohammad et al, 2013).SD-Lex is built from SST.
We construct a sen-timent lexicon from the training set by excludingall neutral words and adding the aforementionedoffset -2 to each entry.
SWN-Lex is a sentimentlexicon extracted from SentimentWordNet3.0 (Bac-cianella et al, 2010).
For words with different part-of-speech tags, we keep the minimum negative scoreor the maximum positive score.
The original scorein the SentimentWordNet3.0 is a probability valuebetween 0 and 1, and we scale it to [-2, 2]5.When building these lexicons, we only use thesentiment scores for unigrams.
Ambiguous wordsare discarded.
Both TS-Lex and S140-Lex areTwitter-specific sentiment lexicons.
They are usedin the Twitter sentiment classification task.
SD-Lexand SWN-Lex are exploited for the Stanford dataset.The statistics of lexicons are listed in Table 4.3http://ir.hit.edu.cn/ dytang/paper/14coling/data.zip4http://saifmohammad.com/Lexicons/Sentiment140-Lexicon-v0.1.zip5Taboada et al (2011) also mentioned two methods to derivesentiment score for a sentiment word from SentimentWordNet.We leave them for future work.Lexicon Positive Negative TotalSD-Lex 2,547 2,448 4,995SWN-Lex 15,568 17,412 32,980TS-Lex 33,997 32,026 66,023S140-Lex 24,156 38,312 62,468Table 4: Statistics of sentiment lexicons.4.2 Implementation DetailsWe implement our model based on the CNNtoolkit.6 Parameters are optimized using stochasticgradient descent with momentum (Sutskever et al,2013).
The decay rate is 0.1.
For initial learning rate,L2 and other hyper-parameters, we adopt the defaultvalues provided by the CNN toolkits.
We select thebest model parameter according to the classificationaccuracy on the development set.For the Twitter data, we use the glove.twitter.27B7as pretrained word embeddings.
For the Stan-ford dataset, following Li et al (2015), we useglove.840B.300d8 as pretrained word embeddings.Words that do not exist in both the training setand the pretrained lookup table are treated as out-of-vocabulary (OOV) words.
Following Dyer etal.
(2015), singletons in the training data are ran-domly mapped to UNK with a probability punk dur-ing training.
We set punk = 0.1.
All word em-beddings are fine-tuned.
We use dropout (Srivastavaet al, 2014) in the input layer to prevent overfittingduring training.One-layered BiLSTM is used for all tasks.
Thedimension of the hidden vector in LSTM is 150.
Thesize of the second layer in Figure 3 is 64.4.3 Development ResultsTable 5 shows results on the Twitter developmentset.
Bi-LSTM is our model using the bias scoreSbias only, which is equivalent to bidirectionalLSTM model of Li et al (2015) and Tai et al(2015), since they use same features and only dif-fer in the output layer.
Bi-LSTM+avg.lexiconis a baseline model integrating the average sen-timent scores of lexicon words as a feature, andBi-LSTM+flex.lexicon is our final model, whichconsiders both the Bi-LSTM score (Sbias) and thecontext-sensitive score (Sbase).6https://github.com/clab/cnn7http://nlp.stanford.edu/data/glove.twitter.27B.zip8http://nlp.stanford.edu/data/glove.840B.300d.zip1634Method Dict Dev(%)Bi-LSTM None 84.2Bi-LSTM+avg.lexicon S140-Lex 84.9Bi-LSTM+flex.lexicon S140-Lex 86.4Table 5: Results on the Twitter development set.Method Test(%)SVM6 (Zhu et al, 2014) 78.5Tang et al (2014a) 82.4Bi-LSTM 86.7Bi-LSTM + TS-Lex 87.6Bi-LSTM + S140-Lex 88.0Table 6: Results on the Twitter test set.Bi-LSTM+avg.lexicon improves the classifica-tion accuracy over Bi-LSTM by 0.7 point, whichshows the usefulness of sentiment lexicons to re-current neural models using a vanilla method.
Itis consistent with previous research on discretemodels.
By considering context-sensitive weight-ing for sentiment words Bi-LSTM+flex.lexicon fur-ther outperforms Bi-LSTM+avg.lexicon, improv-ing the accuracy by 1.5 points (84.9?
86.4), whichdemonstrates the strength of context-sensitive scor-ing.
Base on the development results, we use Bi-LSTM+flex.lexicon for the remaining experiments.4.4 Main ResultsTwitter.
Table 6 shows results on the Twitter test set.SVM6 is our implementation of Zhu et al (2014),which extracts six types of manual features from TS-Lex for SVM classification.
The features include:(1) the number of sentiment words in the sentence;(2) the total sentiment scores of the sentence; (3) themaximum sentiment score; (4) the total positive andnegative sentiment scores; (5) the sentiment score ofthe last word in the sentence.
The system of Tanget al (2014a) is a state-of-the-art system that ex-tracts various manually designed features from TS-Lex, such as bag-of-words, term frequency, parts-of-speech, the sum of sentiment scores of all words ina tweet, etc, for SVM.
The Bi-LSTM rows are ourfinal models with different lexicons.Both SVM6 and Tang et al (2014a) exploit dis-crete features.
Compared to them, Bi-LSTM givesbetter accuracies without using lexicons, whichdemonstrates the relative strength of deep neural net-work for sentiment analysis.
Compared with Tang etal.
(2014a), our Bi-LSTM+TS-Lex model improvesMethod 5-class 2-classRAE (Socher et al, 2011) 43.2 82.4MV-RNN (Socher et al, 2012) 44.4 82.9RNTN (Socher et al, 2013) 45.7 85.4DRNN (Irsoy and Cardie, 2014) 49.8 88.6Dependency TreeLSTM (Tai et al, 2015) 48.4 85.7Constituency TreeLSTM (Tai et al, 2015) 51.0 88.0Constituency TreeLSTM (Li et al, 2015) 50.4 86.7S-LSTM (Zhu et al, 2015) 50.1 -LSTM-RNN (Le and Zuidema, 2015) 49.9 88.0CNN-non-static (Kim, 2014) 48.0 87.2CNN-multichannel (Kim, 2014) 47.4 88.1DCNN (Kalchbrenner et al, 2014) 48.5 86.8Paragraph-Vec (Le and Mikolov, 2014) 48.7 87.8NBoW (Kalchbrenner et al, 2014) 42.4 80.5SVM (Socher et al, 2013) 40.7 79.4BiLSTM (Tai et al, 2015) 49.1 87.5BiLSTM (Li et al, 2015) 49.8 86.7Hier-Sequence (Li et al, 2015) 50.7 86.9Bi-LSTM+SD-Lex 50.0 88.1Bi-LSTM+SWN-Lex 51.1 89.2Table 7: Results on SST.
5-class shows fine-grained classifica-tion.
The last block lists our results.the sentiment classification accuracy from 82.4 to87.6, which again shows the strength of context-sensitive features.
S140-Lex gives slight improve-ments over TS-Lex.SST.
Table 7 shows the results on SST.
We in-clude various results of recursive (the first block),convolutional (the second block), and sequentialLSTM models (the fourth block).
These neural mod-els give the recent state-of-the-art on this dataset.Our method achieves highly competitive accuracies.In particular, compared to sequential LSTMs, ourbest model gives the top result both on the binaryand fine-grained classification task.
This shows theusefulness of lexicons to neural models.
In addition,SWN-Lex gives better results compared with SD-Lex.
This is intuitive because SD-Lex is a smallerlexicon compared to SWN-Lex (4,999 entries v.s.32,980 entries).
SD-Lex does not bring externalknowledge to this dataset, while SWN-Lex does.Cross-domain Results.
Lexicon-based methodscan be robust for cross-domain sentiment analy-sis (Taboada et al, 2011).
We test the robustnessof our model in the mixed domain dataset of prod-uct reviews (Ta?ckstro?m and McDonald, 2011).
Thisdataset contains document level sentiments.
We takethe majority voting strategy to transform sentiment1635Model Train Test Books Dvds Electronics Music Videogames AverageBi-LSTM None None 71.79 89.74 65.79 95 85 81.63Bi-LSTM+flex.lexicon SD-Lex SD-Lex 76.92 84.62 78.95 92.5 80 82.65Bi-LSTM+flex.lexicon SD-Lex SWN-Lex 82.05 92.31 73.68 92.5 80 84.18Bi-LSTM+flex.lexicon SWN-Lex SWN-Lex 84.62 92.31 68.42 100 85 86.22Table 8: Cross-domain sentiment analysis.
Training domain is movie review.Figure 4: Sentiment composition examples.of sentences to the document level.
We comparethe effects of different lexicons over a baseline Bi-LSTM trained on SST (movie domain).Table 8 shows the results.
Introducing the sen-timent lexicons SD-Lex and SWN-Lex consistentlyimproves the classification accuracy across five do-mains compared with the baseline Bi-LSTM model.When trained and tested using the same lexicon,SWN-Lex gives better performances on three out offive domains.
SD-Lex gives better results only onElectronics.
This shows that the results are sensi-tive to the domain of the sentiment lexicon, which isintuitive.We also investigate a model trained using SD-Lex but tested by replacing SD-Lex with SWN-Lex.This is to examine the generalizability of a source-domain model on different target domains by plug-ging in relevant domain-specific lexicons, withoutbeing retrained.
Results show that the mode still out-performs the SD-Lex lexicon on two out of five do-mains, but is less accurate than full retraining usingSWN-Lex.4.5 DiscussionFigure 4 shows the details of sentiment compositionfor two sentences in the SST, learned automaticallyby our model.
For the first sentence, the three sub-jective words in the lexicon ?pure?, ?excitement?ID Sentence Bi-LSTM SWN-Lex1 The issue of faith is notexplored very deeply 0 -12Steers turns in a snappyscreenplay that curls atthe edges; it ?s so cleveryou want to hate it.2 13A film so tedious that it isimpossible to care whetherthat boast is true or not.-2 -1Table 9: Example predictions made by the Bi-LSTM model andour Bi-LSTM+SWN-Lex model for fine-grained classificationtask.
Red words and blue words are positive and negative entriesin the SentimentWordNet3.0 lexicon, respectively.and ?not?
receives weights of 1.6, 1.9 and ?0.6,respectively, and the overall bias of the sentence ispositive.
A ?
value (0.58) that slightly biases to-wards the base score leads to a final sentiment scoreis 1.8, which is close to the gold label 2.In the second example, both negation words re-ceived positive weight values, and the bias over thesentence is negative.
A ?
(0.3) value that biasestowards the bias score results in a final score of?1.2, which is close to the gold label ?1.
Theseresults demonstrate the capacity of the model to de-cide how word-level sentiments composite accord-ing to sentence-level context.Table 9 shows three sentences in the Stanfordtest set which are incorrectly classified by Bi-LSTM model, but correctly labeled by our Bi-LSTM+SWN-Lex model.
These examples showthat our model is more sensitive to context-dependent sentiment changes, thanks to the use oflexicons as a basis.5 ConclusionWe proposed a conceptually-simple, yet empiricallyeffective method of introducing sentiment lexiconfeatures to state-of-the-art LSTM models for sen-timent analysis.
Compared to the simple averag-1636ing method in traditional bag-of-word models, oursystem leverages the strength of semantic featurelearning by LSTM models to calculate a context-dependent weight for each word given an input sen-tence.
The method gives competitive results on var-ious sentiment analysis benchmarks.
In addition,thanks to the use of lexicons, our model can im-prove the cross-domain robustness of recurrent neu-ral models for sentiment analysis.AcknowledgmentsYue Zhang is the corresponding author.
Thanksto anonymous reviewers for their helpful com-ments and suggestions.
Yue Zhang is supportedby NSFC61572245 and T2MOE201301 from Sin-gapore Ministry of Education.ReferencesApoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow,and Rebecca Passonneau.
2011.
Sentiment analysisof twitter data.
In Proceedings of the Workshop onLanguages in Social Media, pages 30?38.Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-tiani.
2010.
Sentiwordnet 3.0: An enhanced lexicalresource for sentiment analysis and opinion mining.
InProceedings of LREC, volume 10, pages 2200?2204.Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio.
2014.
Neural machine translation by jointlylearning to align and translate.
CoRR, abs/1409.0473.Erik Cambria.
2016.
Affective computing and sentimentanalysis.
IEEE Intelligent Systems, 31(2):102?107.Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre,Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk,and Yoshua Bengio.
2014.
Learning phrase represen-tations using rnn encoder?decoder for statistical ma-chine translation.
In Proceedings of EMNLP, pages1724?1734.Yejin Choi and Claire Cardie.
2008.
Learning with com-positional semantics as structural inference for subsen-tential sentiment analysis.
In Proceedings of EMNLP,pages 793?801.Kerstin Denecke.
2009.
Are sentiwordnet scores suitedfor multi-domain sentiment classification?
In ICDIM,pages 1?6.
IEEE.Ann Devitt and Khurshid Ahmad.
2007.
Sentimentpolarity identification in financial news: A cohesion-based approach.Xiaowen Ding, Bing Liu, and Philip S Yu.
2008.
A holis-tic lexicon-based approach to opinion mining.
In Pro-ceedings of the 2008 International Conference on WebSearch and Data Mining, pages 231?240.Li Dong, Furu Wei, Chuanqi Tan, Duyu Tang, MingZhou, and Ke Xu.
2014.
Adaptive recursive neuralnetwork for target-dependent twitter sentiment classi-fication.
In Proceedings of ACL, pages 49?54.Chris Dyer, Miguel Ballesteros, Wang Ling, AustinMatthews, and Noah A. Smith.
2015.
Transition-based dependency parsing with stack long short-termmemory.
In ACL.Andrea Esuli and Fabrizio Sebastiani.
2006.
Sentiword-net: A publicly available lexical resource for opinionmining.
In Proceedings of LREC, volume 6, pages417?422.Alec Go, Richa Bhayani, and Lei Huang.
2009.
Twit-ter sentiment classification using distant supervision.CS224N Project Report, Stanford, 1:12.Alex Graves and Ju?rgen Schmidhuber.
2005.
Frame-wise phoneme classification with bidirectional lstmand other neural network architectures.
Neural Net-works, 18(5):602?610.A.
Graves, A. Mohamed, and G. Hinton.
2013.
Speechrecognition with deep recurrent neural networks.Marco Guerini, Lorenzo Gatti, and Marco Turchi.
2013.Sentiment analysis: How to derive prior polaritiesfrom SentiWordNet.
In Proceedings of EMNLP, pages1259?1269.Sepp Hochreiter and Ju?rgen Schmidhuber.
1997.
Longshort-term memory.
Neural computation, 9(8):1735?1780.Minqing Hu and Bing Liu.
2004.
Mining and summariz-ing customer reviews.
In Proceedings of KDD, KDD?04, pages 168?177.Ozan Irsoy and Claire Cardie.
2014.
Deep recursiveneural networks for compositionality in language.
InAdvances in Neural Information Processing Systems,pages 2096?2104.Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-som.
2014.
A convolutional neural network for mod-elling sentences.
In Proceedings of ACL, pages 655?665.Colm Kearney and Sha Liu.
2014.
Textual sentiment infinance: A survey of methods and models.
Interna-tional Review of Financial Analysis, 33:171?185.Soo-Min Kim and Eduard Hovy.
2004.
Determining thesentiment of opinions.
In Proceedings of the 20th in-ternational conference on Computational Linguistics,page 1367.Yoon Kim.
2014.
Convolutional neural networks for sen-tence classification.
In Proceedings of EMNLP, pages1746?1751.Svetlana Kiritchenko, Xiaodan Zhu, and Saif M. Moham-mad.
2014.
Sentiment analysis of short informal texts.J.
Artif.
Intell.
Res.
(JAIR), 50:723?762.1637Quoc V. Le and Tomas Mikolov.
2014.
Distributedrepresentations of sentences and documents.
CoRR,abs/1405.4053.Phong Le and Willem Zuidema.
2015.
Compositionaldistributional semantics with long short term memory.In Proceedings of the Fourth Joint Conference on Lex-ical and Computational Semantics, pages 10?19.Jiwei Li, Minh-Thang Luong, Dan Jurafsky, and EudardHovy.
2015.
When are tree structures necessary fordeep learning of representations?Pengfei Liu, Xipeng Qiu, Xinchi Chen, Shiyu Wu, andXuanjing Huang.
2015.
Multi-timescale long short-term memory neural network for modelling sentencesand documents.
In Proceedings of EMNLP, pages2326?2335.Saif M. Mohammad, Svetlana Kiritchenko, and XiaodanZhu.
2013.
Nrc-canada: Building the state-of-the-art in sentiment analysis of tweets.
In Proceedings ofSemEval-2013, June.Karo Moilanen and Stephen Pulman.
2007.
Sentimentcomposition.Richard Montague.
1974.
Formal Philosophy: SelectedPapers of Richard Montague.
Ed.
and with an Introd.by Richmond H. Thomason.
Yale University Press.Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,Veselin Stoyanov, Alan Ritter, and Theresa Wilson.2013.
Semeval-2013 task 2: Sentiment analysis intwitter.
In Second Joint Conference on Lexical andComputational Semantics (*SEM), Volume 2: Pro-ceedings of SemEval-2013, pages 312?320.Thien Hai Nguyen and Kiyoaki Shirai.
2015.
Phrasernn:Phrase recursive neural network for aspect-based sen-timent analysis.
In Proceedings of EMNLP.Livia Polanyi and Annie Zaenen.
2006.
Contextual va-lence shifters.
In Computing attitude and affect in text:Theory and applications, pages 1?10.Yafeng Ren, Yue Zhang, Meishan Zhang, and DonghongJi.
2016.
Context-sensitive twitter sentiment classifi-cation using neural network.
In Proceedings of AAAI.Richard Socher, Jeffrey Pennington, Eric H. Huang, An-drew Y. Ng, and Christopher D. Manning.
2011.Semi-Supervised Recursive Autoencoders for Pre-dicting Sentiment Distributions.
In Proceedings ofEMNLP.Richard Socher, Brody Huval, Christopher D. Manning,and Andrew Y. Ng.
2012.
Semantic compositionalitythrough recursive matrix-vector spaces.
In Proceed-ings of EMNLP, pages 1201?1211.Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang,D.
Christopher Manning, Andrew Ng, and ChristopherPotts.
2013.
Recursive deep models for semanticcompositionality over a sentiment treebank.
In Pro-ceedings of EMNLP, pages 1631?1642.Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,Ilya Sutskever, and Ruslan Salakhutdinov.
2014.Dropout: A simple way to prevent neural networksfrom overfitting.
The Journal of Machine LearningResearch, 15(1):1929?1958.Ilya Sutskever, James Martens, George Dahl, and Geof-frey Hinton.
2013.
On the importance of initializationand momentum in deep learning.
In Proceedings ofthe 30th international conference on machine learning(ICML-13), pages 1139?1147.Maite Taboada, Julian Brooke, Milan Tofiloski, KimberlyVoll, and Manfred Stede.
2011.
Lexicon-based meth-ods for sentiment analysis.
Computational linguistics,37(2):267?307.Oscar Ta?ckstro?m and Ryan McDonald.
2011.
Discov-ering fine-grained sentiment with latent variable struc-tured prediction models.
In Advances in InformationRetrieval, pages 368?374.Kai Sheng Tai, Richard Socher, and Christopher D. Man-ning.
2015.
Improved semantic representations fromtree-structured long short-term memory networks.
InProceedings of ACL, pages 1556?1566.Duyu Tang, Furu Wei, Bing Qin, Ming Zhou, and TingLiu.
2014a.
Building large-scale twitter-specific sen-timent lexicon : A representation learning approach.In Proceedings of COLING, pages 172?182, August.Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting Liu,and Bing Qin.
2014b.
Learning sentiment-specificword embedding for twitter sentiment classification.In Proceedings of ACL, pages 1555?1565, June.Peter Turney.
2002.
Thumbs up or thumbs down?
se-mantic orientation applied to unsupervised classifica-tion of reviews.
In Proceedings of ACL.Duy-Tin Vo and Yue Zhang.
2015.
Target-dependenttwitter sentiment classification with rich automaticfeatures.
In Proceedings of IJCAI, pages 1347?1353,July.Xin Wang, Yuanchao Liu, Chengjie SUN, Baoxun Wang,and Xiaolong Wang.
2015.
Predicting polaritiesof tweets by composing word embeddings with longshort-term memory.
In Proceedings of ACL, pages1343?1353.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-levelsentiment analysis.
In Proceedings of HLT-EMNLP.Meishan Zhang, Yue Zhang, and Duy-Tin Vo.
2016.Gated neural networks for targeted sentiment analysis.Xiaodan Zhu, Svetlana Kiritchenko, and Saif M Moham-mad.
2014.
Nrc-canada-2014: Recent improvementsin the sentiment analysis of tweets.Xiaodan Zhu, Parinaz Sobhani, and Hongyu Guo.
2015.Long short-term memory over tree structures.
CoRR,abs/1503.04881.1638
