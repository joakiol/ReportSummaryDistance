Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 471?477, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsSU-Sentilab : A Classification System for Sentiment Analysis in TwitterGizem Gezici, Rahim Dehkharghani, Berrin Yanikoglu, Dilek Tapucu, Yucel SayginSabanci UniversityIstanbul, Turkey 34956{gizemgezici,rdehkharghani,berrin,dilektapucu,ysaygin}@sabanciuniv.eduAbstractSentiment analysis refers to automatically ex-tracting the sentiment present in a given natu-ral language text.
We present our participationto the SemEval2013 competition, in the senti-ment analysis of Twitter and SMS messages.Our approach for this task is the combinationof two sentiment analysis subsystems whichare combined together to build the final sys-tem.
Both subsystems use supervised learningusing features based on various polarity lexi-cons.1 IntroductionBusiness owners are interested in the feedback oftheir customers about the products and services pro-vided by businesses.
Social media networks andmicro-blogs such as Facebook and Twitter play animportant role in this area.
Micro-blogs allow usersshare their ideas with others in terms of small sen-tences; while Facebook updates may indicate anopinion inside a longer text.
Automatic sentimentanalysis of text collected from social media makes itpossible to quantitatively analyze this feedback.In this paper we describe our sentiment analy-sis system identified as SU-Sentilab in the SemEval2013 competition, Task 2: Sentiment analysis inTwitter.
One or the problems in this competition wasto label a given tweet or sms message with the cor-rect sentiment orientation, as positive, negative orneutral.
In the second task of the same competition,the polarity of a given word or word sequence in themessage was asked.
Details are described in (Man-andhar and Yuret, 2013).We participated in both of these tasks using aclassifier combination consisting of two sub-systemsthat are based on (Dehkharghani et al 2012)(Geziciet al 2012) and adapted to the tweet domain.
Bothsub-systems use supervised learning in which thesystem is trained using tweets with known polari-ties and used to predict the label (polarity) of tweetsin the test set.
Both systems use features thatare based on well-known polarity resources namelySentiWordNet (Baccianella et al 2010), SenticNet(Cambria et al 2012) and NRC Emotion Lexicon(Mohammad, 2012).
Also a set of positive and neg-ative seed words (Liu et al 2005) is used in featureextraction.The remainder of paper is organized as follows:Related works are presented in Section 2; the pro-posed approach is described in Section 3 and exper-imental evaluation is presented in Section 4.2 Related WorksThere has been much work on sentiment analysis inthe last ten years (Riloff and Wiebe, 2003) (Wilsonet al 2009) (Taboada et al 2011) (Pang and Lee,2008).
The two fundamental methods for sentimentanalysis are lexicon-based and supervised methods.The lexicon-based technique adopts the idea of de-termining the review sentiment by obtaining wordpolarities from a lexicon, such as the SentiWordNet(Baccianella et al 2010), SenticNet (Cambria et al2012).
This lexicon can be domain-independent ordomain-specific.
One can use a domain-specific lex-icon whenever available, to get a better performanceby obtaining the correct word polarities in the givendomain (e.g., the word ?small?
has a positive mean-471ing in cell phone domain, while it has a negativemeaning in hotel domain).
On the other hand, estab-lishing a domain-specific lexicon is costly, so manysystems use a domain-independent lexicon, such asthe SentiWordNet, shortly SWN, (Baccianella et al2010) and SenticNet (Cambria et al 2012).
Partof Speech (POS) information is commonly indicatedin polarity lexicons, partly to overcome word-sensedisambiguity and therefore help achieve a better sen-timent classification performance.Alternatively, supervised methods use machinelearning techniques to build models or discrimina-tors for the different classes (e.g.
positive reviews),using a large corpus.
For example, in (Pang et al2002) (Yu and Hatzivassiloglou, 2003), the NaiveBayes algorithm is used to separate positive reviewsfrom negative ones.
Note that supervised learningtechniques can also use a lexicon in the feature ex-traction stage.
They also generally perform bet-ter compared to lexicon-based approaches; howevercollecting a large training data may be an issue.In estimating the sentiment of a given natural lan-guage text, many issues are considered.
For instanceone important problem is determining the subjectiv-ity of a given sentence.
In an early study, the ef-fects of adjective orientation and gradability on sen-tence subjectivity was studied (Hatzivassiloglou andWiebe, 2000).
Wiebe et al(Wiebe et al 2004)presents a broad survey on subjectivity recognitionand the key elements that may have an impact on it.In estimating the sentiment polarity, the use ofhigher-order n-grams is also studied.
Pang et.
alreport results where unigrams work better than bi-grams for sentiment classification on a movie dataset(Pang et al 2002).
Similarly, occurrence of rarewords (Yang et al 2006) or the position of words ina text are examined for usefulness (Kim and Hovy,2006)(Pang et al 2002).
In connection with the oc-currences of rare words, different variations of deltatf*idf scores of words, indicating the difference inoccurrences of words in different classes (positive ornegative reviews), have been suggested (Paltoglouand Thelwall, 2010).In addition to sentiment classification, obtainingthe opinion strength is another issue which may beof interest.
Wilson et al(Wilson et al 2004) forinstance, attempts to determine clause-level opinionstrength.
Since this is a difficult task, one of the re-cent studies also investigated the relations betweenword disambiguation and subjectivity, in order toobtain sufficient information for a better sentimentclassification (Wiebe and Mihalcea, 2006).
A recentsurvey describing the fundamental approaches canbe found in (Liu, 2012).Two sub-systems combined to form the SU-Sentilab submission are slightly modified from ourprevious work (Gezici et al 2012) (Dehkharghaniet al 2012) (Demiroz et al 2012).
For subsys-tem SU1, we presented some new features in addi-tion to the ones suggested in (Dehkharghani et al2012).
For subsystem SU2, we combined two sys-tems (Demiroz et al 2012) (Gezici et al 2012).The detailed descriptions for our subsystems SU1and SU2 as well as our combined system can befound in the following sections.3 System DescriptionWe built two sentiment analysis systems using su-pervised learning techniques with labelled tweets fortraining.
Then, another classifier was trained forcombining the two systems, which is what is sub-mitted to SemEval-2013 Task 2.
The subsystems,SU1 and SU2, and also the combination method areexplained in the following subsections.3.1 Subsystem SU1Subsystem SU1 uses subjectivity based features thatare listed in Table 1.
These features are divided intotwo groups:?
F1 through F8, using domain independentresources SenticNet (SN) (Cambria et al2012), SentiWordNet (SWN) (Baccianella etal., 2010) and the NRC Emotion lexicons(NRC) (Mohammad, 2012),?
F9 through F13 using the seed word list (calledSubjWords).In the remainder of this subsection, we describethe features which are grouped according to the lex-ical resource used.SentiWordNet In SentiWordNet (Baccianella etal., 2010), three scores are assigned to each connota-tion of a word: positivity, negativity and objectivity.472The summation of these three scores equals to one:Pos(w) + Neg(w) + Obj(w) = 1 (1)where w stands for a given word; and the threescores stand for its positivity, negativity and objec-tivity scores, respectively.
Furthermore, we definethe the polarity of a word w as:Pol(w) = Pos(w)?Neg(w) (2)We also do not do word sense disambiguation(WSD) because it is an ongoing problem that has notbeen completely solved.
The average polarity of allwords in a review, r, denoted by AP (r) is computedas in (3).AP (r) =1|r|?wi?rPol(wi) (3)where |r| is the number of words in tweet r andPol(wi) is the polarity of the word wi as definedabove.Feature nameF1: Avg.
polarity of all words using SWNF2: Avg.
polarity of negative words using SWNF3: Avg.
polarity of positive words using SWNF4: Avg.
polarity of negative words using SNF5: Avg.
polarity of positive words using SNF6: term frequency of negative words using NRCF7: term frequency of positive words using NRCF8: term frequency of swear wordsF9: Cumulative frequency of positive SubjWordsF10: Cumulative frequency of negative SubjWordsF11: Proportion of positive to negative SubjWordsF12: Weighted probability of positive SubjWordsF13: Weighted probability of negative SubjWordsTable 1: Features extracted for each tweet in subsystemSU1The first three features (F1, F2, F3) are based onthe average polarity concept (AP).
A word w is de-cided as positive if Pol(w) > 0, and decided as neg-ative if Pol(w) < 0.SenticNet SenticNet (Cambria et al 2012) is apolarity lexicon that assigns numerical values be-tween -1 and +1 to a phrase.Unlike SentiWordNet (Baccianella et al 2010),we did not need to do word sense disambiguationfor SenticNet.
Two features, F4 and F5 use the aver-age polarity of negative and positive words extractedfrom SenticNet.
A term is considered as positive ifits overall polarity score is greater than 0 and is con-sidered as negative if this score is lower than 0.NRC Emotion Lexicon The NRC Emotion Lex-icon (Mohammad, 2012) is similar to SenticNetin terms of considering different emotions such asanger and happiness; but it is different from Sentic-Net because it only assigns a binary value (0 or 1)to words.
Features F6 and F7 use the number ofnegative and positive words seen according to thislexicon.Feature F8 is an isolated feature from othergroups which is the list of English swear words col-lected from the Internet.
As an indication to negativesentiment, we counted the number of appearances ofthose swear words in tweets and used it as a feature.Subjective Words (SubjWords) We also use a setof seed words which is a subset of the seed word listproposed in (Liu et al 2005), which we called Sub-jWords.
The filtering of the original set of subjec-tive words, for a particular domain, is done througha supervised learning process, where words that arenot seen in any tweet in the training set are elimi-nated.
Specifically, we add a positive seed word tothe positive subset of SubjWords if it has been seenin at least one positive tweet; and similarly a nega-tive seed word is added to negative subset if it hasbeen seen in a negative tweet.The number of positive and negative words in theinitial set before filtering is 2005 and 4783 respec-tively.
Those numbers decrease to 387 positive and558 negative words after filtering.
Note that this fil-tering helps us to make the seed word sets domain-specific, which in turn helps increase the accuracyof sentiment classification.The mentioned filtered seed words are used in fea-tures F9 through F13 in different ways.
For F9 andF10, we compute the cumulative term frequency ofpositive and negative seed words for each tweet inthe training set, respectively.F9(r) =?ti?PStf(ti, r) (4)473F10(r) =?ti?NStf(ti, r) (5)The feature F11 is the proportion of positive seedwords (the number of occurrences) to the negativeones in a review (tweet):F11(r) =p + 1n + 1(6)where p and n are the number of positive and nega-tive seed words, respectively.Finally features F12 and F13 are the weightedprobabilities of positive and negative words in a re-view, calculated as follows:F12(r) = p ?
(1?
P+(p)) (7)F13(r) = n ?
(1?
P?
(n)) (8)where p is the number of positive seed wordsin review r and P+(p) is the probability of seeingp positive words in a review.
Similarly, F13(r) isthe weighted probability of negative words in a re-view r; n is the number of negative seed words inthe review, and P?
(n) is the probability of seeingn negative words in a review.
Probabilities P+(p)and P?
(n) are calculated from training set.
Table 2presents the values of P+(p) for n = 1 .
.
.
5.
Forinstance, the probability of seeing at least one posi-tive subjective word in a positive tweet is 0.87, whileseeing three positive words is only 0.47.p 1 2 3 4 5P+(p) 0.87 0.69 0.47 0.17 0.06Table 2: The probability of seeing p positive words in apositive tweet.Classifier The extracted features are fed into a lo-gistic regression classifier, chosen for its simplicityand successful use in many problems.
We have usedWEKA 3.6 (Hall et al 2009) implementation forthis classifier, all with default parameters.3.2 Subsystem SU2Subsystem SU2 uses word-based and sentence-based features proposed in (Gezici et al 2012) andsummarized in Table 3.
For adapting to the tweetdomain, we also added some new features regardingsmileys.The features consist of an extensive set of 24 fea-tures that can be grouped in five categories: (1) basicfeatures, (2) features based on subjective word oc-currence statistics, (3) delta-tf-idf weighting of wordpolarities, (4) punctuation based features, and (5)sentence-based features.
They are as follows:Basic Features In this group of features, we ex-ploit word-based features and compute straightfor-ward features which were proposed several times be-fore in the literature (e.g.
avg.
review polarity andreview purity).
Moreover, smileys which are crucialsymbols in Twitter are also included here.Seed Words Features In the second group of fea-tures, we have two seed sets as positive and negativeseed words.
These seed words are the words that areobviously positive or negative irrelevant of the con-text.
As seed words features, we make calculationsrelated to their occurrences in a review to captureseveral clues for sentiment determination.
?tf-idf Features This group consists of featuresbased on the ?tf-idf score of a word-sense pair,indicating the relative occurrence of a word-senseamong positive and negative classes (Demiroz et al2012).Punctuation-based Features This group containsthe number of question and exclamation marks in themessage, as they may give some information aboutthe sentiment of a review, especially for the Twitterdomain.Sentence-based Features In this last group of fea-tures, we extract features based on sentence type(e.g.
subjective, pure, and non-irrealis) (Taboada etal., 2011) and sentence position (e.g.
first line andlast line) (Zhao et al 2008).
Features include sev-eral basic ones such as the average polarity of thefirst sentence and the average polarity of subjectiveor pure sentences.
We also compute ?tf-idf scoreson sentence level.Finally, we consider the number of sentenceswhich may be significant in SMS messages and theestimated review subjectivity as a feature derivedfrom sentence-level processing.
The review is con-sidered subjective if it contains at least one subjec-474tive sentence.
In turn, a sentence is subjective if andonly if it contains at least one subjective word-sensepair or contains at least one smiley.
A word-sensepair is subjective if and only if the sum of its posi-tive and negative polarity taken from SentiWordNet(Baccianella et al 2010) is bigger than 0.5 (Zhangand Zhang, 2006).
These features are described indetail in (Gezici et al 2012).Feature nameF1: Average review polarityF2: Review purityF3: # of positive smileysF4: # of negative smileysF5: Freq.
of seed wordsF6: Avg.
polarity of seed wordsF7: Std.
of polarities of seed wordsF8: ?tf-idf weighted avg.
polarity of wordsF9: ?tf-idf scores of wordsF10: # of Exclamation marksF11: # of Question marksF12: Avg.
First Line PolarityF13: Avg.
Last Line PolarityF14: First Line PurityF15: Last Line PurityF16: Avg.
pol.
of subj.
sentencesF17: Avg.
pol.
of pure sentencesF18: Avg.
pol.
of non-irrealis sentencesF19: ?tf-idf weighted polarity of first lineF20: ?tf-idf scores of words in the first lineF21: ?tf-idf weighted polarity of last lineF22: ?tf-idf scores of words in the last lineF23: Review subjectivity (0 or 1)F24: Number of sentences in reviewTable 3: Features extracted for each tweet in subsystemSU2Obtaining Polarities from SentiWordNet For allthe features in subsystem SU2, we use SentiWord-Net (Baccianella et al 2010) as a lexicon.
Al-though, we use the same lexicon for our two subsys-tems, the way we include the lexicon to our subsys-tems differs.
In this subsystem, we obtain the domi-nant polarity of the word-sense pair from the lexiconand use the sign for the indication of polarity direc-tion.
The dominant polarity of a word w, denoted byPol(w), is calculated as:Pol(w) =????????????????
?0 if max(p=, p+, p?)
= p=p+ else if p+ ?
p??p?
otherwisewhere p+, p= and p?
are the positive, objective andnegative polarities of a word w, respectively.After obtaining the dominant polarities of wordsfrom SentiWordNet (Baccianella et al 2010), weupdate these polarities using our domain adaptationtechnique (Demiroz et al 2012).
The ?tf ?
idfscores of words are computed and if there is a dis-agreement between the ?tf ?
idf and the domi-nant polarity of a word indicated by the lexicon, thenthe polarity of the word is updated.
This adaptationis described in detail in one of our previous works(Demiroz et al 2012).Classifier The extracted features are fed into aNaive Bayes classifier, also chosen for its simplic-ity and successful use in many problems.
We haveused WEKA 3.6 (Hall et al 2009) implementationfor this classifier, where the Kernel estimator param-eter was set to true.3.3 Combination of SubsystemsAs we had two independently developed systemsthat were only slightly adapted for this competition,we wanted to apply a sophisticated classifier combi-nation technique.
Rather than averaging the outputsof the two classifiers, we used the development setto train a new classifier, to learn how to best combinethe two systems.
Note that in this way the combinertakes into account the different score scales and ac-curacies of the two sub-systems automatically.The new classifier takes as features the probabil-ities assigned by the systems to the three possibleclasses (positive, objective, negative) and anotherfeature which is an estimate of subjectivity of thetweet or SMS messages.
We trained the system us-ing these 7 features obtained from the developmentdata for which we had the groundtruth, with the goalof predicting the actual class label based on the esti-mates of the two subsystems.4754 Evaluation4.1 Competition TasksThere were two tasks in this competition: 1) Task Awhere the aim was to determine the sentiment of aphrase within the message and 2) Task B where theaim was to obtain the overall sentiment of a mes-sage.
In each task, the classification involves the as-signment of one of the three sentiment classes, posi-tive, negative and objective/neutral.
There were twodifferent datasets for each task, namely tweet andSMS datasets (Manandhar and Yuret, 2013).
Due tothe different nature of tweets and SMS and the twotasks (A and B), we in fact considered this as fourdifferent tasks.4.2 Submitted SystemsDue to time constraints, we mainly worked onTaskB where we had some prior experience, andonly submitted participated in TaskA for complete-ness.As we did not use any outside labelled data(tweets or SMS), we trained our systems on theavailable training data which consisted only oftweets and submitted them on both tweets and SMSsets.
In fact, we separated part of the training dataas validation set and comparison of the two subsys-tems.Since only one system is allowed for each task,we selected the submitted system from our 3 sys-tems (SU1, SU2, combined) based on their perfor-mance on the validation set.
The performances ofthese systems are summarized in Table 4.Finally, we re-trained the selected system with thefull training data, to use all available data.For the implementation, we used C# for subsys-tem SU1 and Java & Stanford NLP Parser (De Marn-effe and Manning, 2008) for subsystem SU2 andWEKA (Hall et al 2009) for the classification partfor both of the systems.4.3 ResultsIn order to evaluate and compare the performancesof our two systems, we separated a portion of thetraining data as validation set, and kept it separate.Then we trained each system on the training set andtested it on the validation set.
These test results aregiven in Table 4.We obtained 75.60% accuracy on the validationset with subsystem SU1 on TaskA twitter using lo-gistic regression.
For the same dataset, we obtained70.74% accuracy on the validation set with subsys-tem SU2 using a Naive Bayes classifier.For TaskB Twitter dataset on the other hand, webenefited from our combined system in order to getbetter results.
With this combined system using lo-gistic regression as a classifier, we achieved 64%accuracy on the validation set.
The accuracies ob-tained by the individual subsystems on this task was63.10% by SU1 and 62.92% by SU2.Dataset System AccuracyTaskA Twitter SU1 75.60%SU2 70.74%SU1 63.10%TaskB Twitter SU2 62.92%Combined 64.00%Table 4: Performance of Our Systems on Validation Data4.4 Discussion & Future WorkThe accuracy of our submitted systems for differenttasks are not very high due to many factors.
First ofall, both domains (tweets and SMSs) were new to usas we had only worked on review polarity estimationon hotel and movie domains before.For tweets, the problem is quite difficult due toespecially short message length; misspelled words;and lack of domain knowledge (e.g.
?Good Girl, BadGirl?
does not convey a sentiment, rather it is a stageplay?s name).
As for the SMS data, there were notraining data for SMSs, so we could not tune or re-train our existing systems, either.
Finally, for TaskA, we had some difficulty with the phrase index, dueto some ambiguity in the documentation.
Nonethe-less, we thank the organizers for a chance to evaluateourselves among others.This was our first experience with this competi-tion and with the Twitter and SMS domains.
Giventhe nature of tweets, we used simple features ex-tracted from term polarities obtained from domain-independent lexicons.
In the future, we intend to usemore sophisticated algorithms, both in the naturallanguage processing stage, as well as the machinelearning algorithms.476ReferencesStefano Baccianella, Andrea Esuli, and Fabrizio Sebas-tiani.
2010.
Sentiwordnet 3.0: An enhanced lexicalresource for sentiment analysis and opinion mining.
InProc.
of LREC.Erik Cambria, Catherine Havasi, and Amir Hussain.2012.
Senticnet 2: A semantic and affective re-source for opinion mining and sentiment analysis.
InG.
Michael Youngblood and Philip M. McCarthy, edi-tors, FLAIRS Conf.
AAAI Press.Marie-Catherine De Marneffe and Christopher DManning.
2008.
Stanford typed depen-dencies manual.
URL http://nlp.
stanford.edu/software/dependencies manual.
pdf.Rahim Dehkharghani, Berrin Yanikoglu, Dilek Tapucu,and Yucel Saygin.
2012.
Adaptation and use ofsubjectivity lexicons for domain dependent sentimentclassification.
In Data Mining Workshops (ICDMW),2012 IEEE 12th International Conf., pages 669?673.Gulsen Demiroz, Berrin Yanikoglu, Dilek Tapucu, andYucel Saygin.
2012.
Learning domain-specific polar-ity lexicons.
In Data Mining Workshops (ICDMW),2012 IEEE 12th International Conf.
on, pages 674?679.Gizem Gezici, Berrin Yanikoglu, Dilek Tapucu, andYu?cel Sayg?n.
2012.
New features for sentiment anal-ysis: Do sentences matter?
In SDAD 2012 The 1stInternational Workshop on Sentiment Discovery fromAffective Data, page 5.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H Witten.
2009.The weka data mining software: an update.
ACMSIGKDD Explorations Newsletter, 11(1):10?18.Vasileios Hatzivassiloglou and Janyce M Wiebe.
2000.Effects of adjective orientation and gradability on sen-tence subjectivity.
In Proc.
of the 18th Conf.
on Comp.Ling.-Volume 1, pages 299?305.Soo-Min Kim and Eduard Hovy.
2006.
Automatic iden-tification of pro and con reasons in online reviews.In Proc.
of the COLING/ACL Main Conf.
Poster Ses-sions, pages 483?490.Bing Liu, Minqing Hu, and Junsheng Cheng.
2005.Opinion observer: analyzing and comparing opinionson the web.
In WWW ?05: Proc.
of the 14th Interna-tional Conf.
on World Wide Web.Bing Liu.
2012.
Sentiment analysis and opinion mining.Synthesis Lectures on Human Language Technologies,5(1):1?167.Suresh Manandhar and Deniz Yuret.
2013.
Semevaltweet competition.
In Proc.
of the 7th InternationalWorkshop on Semantic Evaluation (SemEval 2013) inconjunction with the Second Joint Conference on Lex-ical and Comp.Semantics (*SEM 2013).Saif Mohammad.
2012.
#emotional tweets.
In *SEM2012: The First Joint Conf.
on Lexical and Comp.
Se-mantics, pages 246?255.
Association for Comp.
Ling.Georgios Paltoglou and Mike Thelwall.
2010.
A study ofinformation retrieval weighting schemes for sentimentanalysis.
In Proc.
of the 48th Annual Meeting of theAssociation for Comp.
Ling., pages 1386?1395.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and Trends in Infor-mation Retrieval, 2(1-2):1?135, January.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
sentiment classification using ma-chine learning techniques.
In Proc.
of EMNLP, pages79?86.Ellen Riloff and Janyce Wiebe.
2003.
Learning extrac-tion patterns for subjective expressions.
In Proc.
ofthe 2003 Conf.
on Empirical methods in natural lan-guage processing, pages 105?112, Morristown, NJ,USA.
Association for Comp.
Ling.Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-berly Voll, and Manfred Stede.
2011.
Lexicon-basedmethods for sentiment analysis.
Comput.
Linguist.,37(2):267?307.Janyce Wiebe and Rada Mihalcea.
2006.
Word sense andsubjectivity.
In ACL.Janyce Wiebe, Theresa Wilson, Rebecca F. Bruce,Matthew Bell, and Melanie Martin.
2004.
Learningsubjective language.
Comp.
Ling., 30(3):277?308.Theresa Wilson, Janyce Wiebe, and Rebecca Hwa.
2004.Just how mad are you?
finding strong and weak opin-ion clauses.
In Proc.
of the 19th national Conf.
onArtifical intelligence, AAAI?04, pages 761?767.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2009.
Recognizing contextual polarity: An explo-ration of features for phrase-level sentiment analysis.Comp.
Ling., pages 399?433.Kiduk Yang, Ning Yu, Alejandro Valerio, and Hui Zhang.2006.
Widit in trec-2006 blog track.
In Proc.
of TREC,pages 27?31.Hong Yu and Vasileios Hatzivassiloglou.
2003.
Towardsanswering opinion questions: Separating facts fromopinions and identifying the polarity of opinion sen-tences.
In Proc.
of the 2003 Conf.
on Empirical meth-ods in natural language processing, pages 129?136.Association for Comp.
Ling.Ethan Zhang and Yi Zhang.
2006.
Ucsc on trec 2006blog opinion mining.
In Text Retrieval Conference.Jun Zhao, Kang Liu, and Gen Wang.
2008.
Addingredundant features for crfs-based sentence sentimentclassification.
In Proc.
of the conference on empiricalmethods in natural language processing, pages 117?126.
Association for Comp.
Ling.477
