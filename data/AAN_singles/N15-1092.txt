Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 912?921,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsRepresentation Learning Using Multi-Task Deep Neural Networksfor Semantic Classification and Information RetrievalXiaodong Liu?
?, Jianfeng Gao?, Xiaodong He?, Li Deng?, Kevin Duh?and Ye-yi Wang?
?Nara Institute of Science and Technology, 8916-5 Takayama, Ikoma, Nara 630-0192, Japan?Microsoft Research, One Microsoft Way, Redmond, WA 98052, USAxiaodong-l@is.naist.jp, {jfgao,xiaohe,deng}@microsoft.comkevinduh@is.naist.jp, yeyiwang@microsoft.comAbstractMethods of deep neural networks (DNNs)have recently demonstrated superior perfor-mance on a number of natural language pro-cessing tasks.
However, in most previouswork, the models are learned based on ei-ther unsupervised objectives, which does notdirectly optimize the desired task, or single-task supervised objectives, which often suf-fer from insufficient training data.
We de-velop a multi-task DNN for learning represen-tations across multiple tasks, not only leverag-ing large amounts of cross-task data, but alsobenefiting from a regularization effect thatleads to more general representations to helptasks in new domains.
Our multi-task DNNapproach combines tasks of multiple-domainclassification (for query classification) and in-formation retrieval (ranking for web search),and demonstrates significant gains over strongbaselines in a comprehensive set of domainadaptation.1 IntroductionRecent advances in deep neural networks (DNNs)have demonstrated the importance of learningvector-space representations of text, e.g., words andsentences, for a number of natural language process-ing tasks.
For example, the study reported in (Col-lobert et al, 2011) demonstrated significant accu-racy gains in tagging, named entity recognition, andsemantic role labeling when using vector space word?This research was conducted during the author?s internship atMicrosoft Research.representations learned from large corpora.
Fur-ther, since these representations are usually in a low-dimensional vector space, they result in more com-pact models than those built from surface-form fea-tures.
A recent successful example is the parser by(Chen and Manning, 2014), which is not only accu-rate but also fast.However, existing vector-space representationlearning methods are far from optimal.
Most pre-vious methods are based on unsupervised objectivessuch as word prediction for training (Mikolov et al,2013c; Pennington et al, 2014).
Other methods usesupervised training objectives on a single task, e.g.
(Socher et al, 2013), and thus are often constrainedby limited amounts of training data.
Motivated bythe success of multi-task learning (Caruana, 1997),we propose in this paper a multi-task DNN approachfor representation learning that leverages superviseddata from many tasks.
In addition to the benefit ofhaving more data for training, the use of multi-taskalso profits from a regularization effect, i.e., reduc-ing overfitting to a specific task, thus making thelearned representations universal across tasks.Our contributions are of two-folds: First, we pro-pose a multi-task deep neural network for represen-tation learning, in particular focusing on semanticclassification (query classification) and semantic in-formation retrieval (ranking for web search) tasks.Our model learns to map arbitrary text queries anddocuments into semantic vector representations ina low dimensional latent space.
While the generalconcept of multi-task neural nets is not new, ourmodel is novel in that it successfully combines tasksas disparate as operations necessary for classifica-912tion or ranking.Second, we demonstrate strong results on queryclassification and web search.
Our multi-task rep-resentation learning consistently outperforms state-of-the-art baselines.
Meanwhile, we show that ourmodel is not only compact but it also enables ag-ile deployment into new domains.
This is becausethe learned representations allow domain adaptationwith substantially fewer in-domain labels.2 Multi-Task Representation Learning2.1 PreliminariesOur multi-task model combines classification andranking tasks.
For concreteness, throughout this pa-per we will use query classification as the classifica-tion task and web search as the ranking task.
Theseare important tasks in commercial search engines:Query Classification: Given a search query Q,the model classifies in the binary fashion as towhether it belongs to one of the domains of inter-est.
For example, if the query Q is ?Denver sushi?,the classifier should decide that it belongs to the?Restaurant?
domain.
Accurate query classificationenables a richer personalized user experience, sincethe search engine can tailor the interface and results.It is however challenging because queries tend to beshort (Shen et al, 2006).
Surface-form word fea-tures that are common in traditional document clas-sification problems tend to be too sparse for queryclassification, so representation learning is a promis-ing solution.
In this study, we classify queries intofour domains of interest: (?Restaurant?, ?Hotel?,?Flight?, ?Nightlife?).
Note that one query can be-long to multiple domains.
Therefore, a set of bi-nary classifiers are built, one for each domain, toperform the classification.
We frame the problemas four binary classification tasks.
Thus, for do-main Ct, our goal is binary classification based onP (Ct| Q) (Ct= {0, 1} ).
For each domain t, weassume supervised data (Q, yt= {0, 1} with ytasbinary labels.1Web Search: Given a search queryQ and a docu-ment list L, the model ranks documents in the order1One could frame the problem as a a single multi-class clas-sification task, but our formulation is more practical as it al-lows adding new domains without retraining existing classi-fiers.
This will be relevant in domain adaptation (?3.3).of relevance.
For example, if the queryQ is ?Denversushi?, model returns a list of documents that satis-fies such information need.
Formally, we estimateP (D1|Q), P (D2|Q), .
.
.
for each document Dnandrank according to these probabilities.
We assumethat supervised data exist; I.e., there is at least onerelevant document Dnfor each query Q.2.2 The Proposed Multi-Task DNN ModelBriefly, our proposed model maps any arbi-trary queries Q or documents D into fixed low-dimensional vector representations using DNNs.These vectors can then be used to perform queryclassification or web search.
In contrast to exist-ing representation learning methods which employeither unsupervised or single-task supervised objec-tives, our model learns these representations usingmulti-task objectives.The architecture of our multi-task DNN modelis shown in Figure 1.
The lower layers are sharedacross different tasks, whereas the top layers repre-sent task-specific outputs.
Importantly, the input X(either a query or document), initially represented asa bag of words, is mapped to a vector (l2) of dimen-sion 300.
This is the shared semantic representationthat is trained by our multi-task objectives.
In thefollowing, we elaborate the model in detail:Word Hash Layer (l1): Traditionally, each wordis represented by a one-hot word vector, where thedimensionality of the vector is the vocabulary size.However, due to the large size of vocabulary in real-world tasks, it is very expensive to learn such kindof models.
To alleviate this problem, we adopt theword hashing method (Huang et al, 2013).
Wemap a one-hot word vector, with an extremely highdimensionality, into a limited letter-trigram space(e.g., with the dimensionality as low as 50k).
Forexample, word cat is hashed as the bag of letter tri-gram {#-c-a, c-a-t, a-t-#}, where # is a boundarysymbol.
Word hashing complements the one-hotvector representation in two aspects: 1) out of vo-cabulary words can be represented by letter-trigramvectors; 2) spelling variations of the same word canbe mapped to the points that are close to each otherin the letter-trigram space.Semantic-Representation Layer (l2): This is ashared representation learned across different tasks.this layer maps the letter-trigram inputs into a 300-913X: Bag-of-Words Input (500k)l1: Letter 3gram (50k)l2: Semantic Representation (300)QC1QC2QSqDSd1 DSd2HW1Wt=C12Wt=C13P (C1|Q)Wt=C22Wt=C23P (C2|Q)Wt=Sq2 Wt=Sd2P (D1|Q)Wt=Sd2P (D2|Q)l3: Task-SpecificRepresentation(128)Query classifi-cation posteriorprobability com-puted by sigmoidWeb search pos-terior probabilitycomputed by soft-maxRelevance mea-sured by cosinesimilaritySharedlayersQuery ClassificationWeb Search1Figure 1: Architecture of the Multi-task Deep Neural Network (DNN) for Representation Learning:The lower layers are shared across all tasks, while top layers are task-specific.
The inputX (either a query ordocument, with vocabulary size 500k) is first represented as a bag of words, then hashed into letter 3-gramsl1.
Non-linear projection W1generates the shared semantic representation, a vector l2(dimension 300) thatis trained to capture the essential characteristics of queries and documents.
Finally, for each task, additionalnon-linear projections Wt2generate task-specific representations l3(dimension 128), followed by operationsnecessary for classification or ranking.dimensional vector byl2= f(W1?
l1) (1)where f(?)
is the tanh nonlinear activation f(z) =1?e?2z1+e?2z.
This 50k-by-300 matrix W1is responsiblefor generating the cross-task semantic representationfor arbitrary text inputs (e.g., Q or D).Task-Specific Representation (l3): For eachtask, a nonlinear transformation maps the 300-dimension semantic representation l2into the 128-dimension task-specific representation byl3= f(Wt2?
l2) (2)where, t denotes different tasks (query classificationor web search).Query Classification Output: Suppose QC1?l3= f(Wt=C12?
l2) is the 128-dimension task-specific representation for a query Q.
The proba-bility that Q belongs to class C1is predicted by alogistic regression, with sigmoid g(z) =11+e?z:P (C1|Q) = g(Wt=C13?QC1) (3)Web Search Output: For the web searchtask, both the query Q and the document D aremapped into 128-dimension task-specific represen-tations QSqand DSd.
Then, the relevance score isAlgorithm 1: Training a Multi-task DNNInitialize model ?
: {W1,Wt2,Wt3} randomlyfor iteration in 0...?
do1.
Pick a task t randomly2.
Pick sample(s) from task t(Q, yt= {0, 1}) for query classification(Q,L) for web search3.
Compute loss: L(?)L(?)=Eq.
5 for query classificationL(?)=Eq.
6 for web search4.
Compute gradient: ?(?)5.
Update model: ?
= ??
?(?
)endThe task t is one of the query classification tasks or web searchtask, as shown in Figure 1.
For query classification, each train-ing sample includes one query and its category label.
For websearch, each training sample includes query and document list.computed by cosine similarity as:R(Q,D) = cos(QSq, DSd) =QSq?DSd||QSq||||DSd||(4)2.3 The Training ProcedureIn order to learn the parameters of our model, we usemini-batch-based stochastic gradient descent (SGD)as shown in Algorithm 1.
In each iteration, a task tis selected randomly, and the model is updated ac-914cording to the task-specific objective.
This approx-imately optimizes the sum of all multi-task objec-tives.
For query classification of class Ct, we usethe cross-entropy loss as the objective:?
{ytlnP (Ct|Q)+(1?yt) ln(1?P (Ct|Q))} (5)where yt= {0, 1} is the label and the loss issummed over all samples in the mini-batch (1024samples in experiments).The objective for web search used in this paperfollows the pair-wise learning-to-rank paradigm out-lined in (Burges et al, 2005).
Given a query Q, weobtain a list of documents L that includes a clickeddocument D+(positive sample), and J randomly-sampled non-clicked documents {D?j}j=1,.,J.
Wethen minimize the negative log likelihood of theclicked document (defined in Eq.
7) given queriesacross the training data?
log?
(Q,D+)P (D+|Q) (6)where the probability of a given document D+iscomputedP (D+|Q) =exp(?R(Q,D+))?D??Lexp(?R(Q,D?
))(7)here, ?
is a tuning factor determined on held-outdata.Additional training details: (1) Model parametersare initialized with uniform distribution in the range(?
?6/(fanin+ fanout),?6/(fanin+ fanout))(Montavon et al, 2012).
Empirically, we havenot observed better performance by initializationwith layer-wise pre-training.
(2) Moment methodsand AdaGrad training (Duchi et al, 2011) speedup the convergence speed but gave similar resultsas plain SGD.
The SGD learning rate is fixed at = 0.1/1024.
(3) We run Algorithm 1 for 800Kiterations, taking 13 hours on an NVidia K20 GPU.2.4 An Alternative View of the Multi-TaskModelOur proposed multi-task DNN (Figure 1) can beviewed as a combination of a standard DNN for clas-sification and a Deep Structured Semantic Model(DSSM) for ranking, shown in Figure 2.
Other waysto merge the models are possible.
Figure 3 showsan alternative multi-task architecture, where only thequery part is shared among all tasks and the DSSM500kQ50k300128HW1W2P (C|Q)DNN model500kQ50k300128HWq1Wq2500kD150k300128HWd1Wd2R(Q,D1)P (D1|Q)500kD250k300128HWd1Wd2R(Q,D2)P (D2|Q)DSSM model4Figure 2: A DNN model for classification and aDSSM model (Huang et al, 2013) for ranking.retains independent parameters for computing thedocument representations.
This is more similar tothe original DSSM.
We have attempted training thismodel using Algorithm 1, but it achieves good re-sults on query classification at the expense of websearch.
This is likely due to unbalanced updates (i.e.parameters for queries are updated more often thanthat of documents), and implying that the amount ofsharing is an important design choice in multi-taskmodels.500kQ50k300QC1QC2QSq500kD50k300DSd3Figure 3: An alternative multi-task architecture.Compared with Figure 1, only the query part isshared across tasks here.3 Experimental Evaluation3.1 Data Sets and Evaluation MetricsWe employ large-scale, real data sets in our eval-uation.
See Table 1 for statistics.
The test data forquery classification were sampled from one-year logfiles of a commercial search engine with labels (yesor no) judged by humans.
The test data for websearch contains 12,071 English queries, where eachquery-document pair has a relevance label manuallyannotated on a 5-level relevance scale: bad, fair,915TaskQuery Classification WebRestaurant Hotel Flight Nightlife SearchTraining 1,585K 2,131K 1,880K 1,214K 4,084K queries & click-through documentsTest 3,074 6,307 6,199 298 12,071 queries / 897,770 documentsTable 1: Statistics of the data sets used in the experiments.good, excellent and perfect.
The evaluation metricfor query classification is the Area under of ReceiverOperating Characteristic (ROC) curve (AUC) score(Bradley, 1997).
For web search, we employ theNormalized Discounted Cumulative Gain (NDCG)(J?arvelin and Kek?al?ainen, 2000).3.2 Results on AccuracyFirst, we evaluate whether our model can robustlyimprove performance, measured as accuracy acrossmultiple tasks.Table 2 summarizes the AUC scores for queryclassification, comparing the following classifiers:?
SVM-Word: a SVM model2with unigram, bi-gram and trigram surface-form word features.?
SVM-Letter: a SVM model with letter trigramfeatures (i.e.
l1in Figure 1 as input to SVM).?
DNN: single-task deep neural net (Figure 2).?
MT-DNN: our multi-task proposal (Figure 1).The results show that the proposed MT-DNN per-forms best in all four domains.
Further, we observe:1.
MT-DNN outperforms DNN, indicating theusefulness of the multi-task objective (that in-cludes web search) over the single-task objec-tive of query classification.2.
Both DNN and MT-DNN outperform SVM-Letter, which initially uses the same input fea-tures (l1).
This indicates the importance oflearning a semantic representation l2on top ofthese letter trigrams.3.
Both DNN and MT-DNN outperform a strongSVM-Word baseline, which has a large featureset that consists of 3 billion features.Table 3 summarizes the NDCG results on websearch, comparing the following models:2In this paper, we use the liblinear to build SVMclassifiers and optimize the corresponding parame-ter C by using 5-fold cross-validation in training data.http://www.csie.ntu.edu.tw/ cjlin/liblinear/SystemQuery ClassificationRestaurant Hotel Flight NightlifeSVM-Word 90.91 75.82 91.17 91.27SVM-Letter 88.75 69.65 85.51 87.71DNN 97.38 76.81 95.58 93.24MT-DNN 97.57 78.56 96.21 94.20Table 2: Query Classification AUC results.?
Popular baselines in the web search literature,e.g.
BM25, Language Model, PLSA?
DSSM: single-task ranking model (Figure 2)?
MT-DNN: our multi-task proposal (Figure 1)Again, we observe that MT-DNN performs best.
Forexample, MT-DNN achieves NDCG@1=0.334, out-performing the current state-of-the-art single-taskDSSM (0.327) and the classic methods like PLSA(0.308) and BM25 (0.305).
This is a statistically sig-nificant improvement (p < 0.05) over DSSM andother baselines.To recap, our MT-DNN robustly outperformsstrong baselines across all web search and queryclassification tasks.
Further, due to the use of largertraining data (from different domains) and the reg-ularization effort as we discussed in Section 1, weconfirm the advantage of multi-task models overthan single-task ones.33.3 Results on Model Compactness andDomain AdaptationImportant criteria for building practical systems areagility of deployment and small memory footprintand fast run-time.
Our model satisfies both with3We have also trained SVM using Word2Vec (Mikolov et al,2013b; Mikolov et al, 2013a) features.
Unfortunately, the re-sults are poor at 60-70 AUC, indicating the sub-optimality ofunsupervised representation learning objectives for actual pre-diction tasks.
We optimized the Word2Vec features in the SVMbaseline by scaling and normalizing as well, but did not ob-serve much improvement.916Models NDCG@1 NDCG@3 NDCG@10TF-IDF model (BM25) 0.305 0.328 0.388Unigram Language Model (Zhai and Lafferty, 2001) 0.304 0.327 0.385PLSA(Topic=100) (Hofmann, 1999; Gao et al, 2011) 0.305 0.335 0.402PLSA(Topic=500) (Hofmann, 1999; Gao et al, 2011) 0.308 0.337 0.402Latent Dirichlet Allocation (Topic=100) (Blei et al, 2003) 0.308 0.339 0.403Latent Dirichlet Allocation (Topic=500) (Blei et al, 2003) 0.310 0.339 0.405Bilingual Topic Model (Gao et al, 2011) 0.316 0.344 0.410Word based Machine Translation model (Gao et al, 2010) 0.315 0.342 0.411DSSM, J=50 (Figure 2, (Huang et al, 2013)) 0.327 0.359 0.432MT-DNN (Proposed, Figure 3) 0.334* 0.363 0.434Table 3: Web Search NDCG results.
Here, * indicates statistical significance improvement compared to thebest baseline (DSSM) measured by t-test at p-value of 0.05.3.0 2.5 2.0 1.5 1.0 0.5 0.0Log10(Percentage of Training Data)657075808590AUC ScoreSemanticRepresentationLetter3gramWord3gram(a) Hotel3.0 2.5 2.0 1.5 1.0 0.5 0.0Log10(Percentage of Training Data)7580859095AUC ScoreSemanticRepresentationLetter3gramWord3gram(b) Flight3.0 2.5 2.0 1.5 1.0 0.5 0.0Log10(Percentage of Training Data)7880828486889092AUC ScoreSemanticRepresentationLetter3gramWord3gram(c) Restaurant3.0 2.5 2.0 1.5 1.0 0.5 0.0Log10(Percentage of Training Data)5560657075AUC ScoreSemanticRepresentationLetter3gramWord3gram(d) NightlifeFigure 4: Domain Adaption in Query Classification: Comparison of features using SVM classifiers.
TheX-axis indicates the amount of labeled samples used in training the SVM.
Intuitively, the three featurerepresentations correspond to different layers in Figure 1.
SemanticRepresentation is the l2layer trainedby MT-DNN.
Word3gram is input X and Letter3gram is word hash layer (l1), both not trained/adapted.Generally, SemanticRepresentation performs best for small training labels, indicating its usefulness indomain adaptation.
Note that the numbers -3.0, -2.0, -1.0 and 0.0 in x-axis denote 0.1, 1, 10 and 100 percenttraining data in each domain, respectively.9173.0 2.5 2.0 1.5 1.0 0.5 0.0Log10(Percentage of Training Data)7580859095AUC ScoreSVMDNN2DNN1(a) Hotel3.0 2.5 2.0 1.5 1.0 0.5 0.0Log10(Percentage of Training Data)80859095AUC ScoreSVMDNN2DNN1(b) Flight3.0 2.5 2.0 1.5 1.0 0.5 0.0Log10(Percentage of Training Data)788082848688909294AUC ScoreSVMDNN2DNN1(c) Restaurant3.0 2.5 2.0 1.5 1.0 0.5 0.0Log10(Percentage of Training Data)5560657075AUC ScoreSVMDNN2DNN1(d) NightlifeFigure 5: Domain Adaptation in Query Classification.
Comparison of different DNNs.high model compactness.
The key to the compact-ness is the aggressive compression from the 500k-dimensional bag-of-words input to 300-dimensionalsemantic representation l2.
This significantly re-duces the memory/run-time requirements comparedto systems that rely on surface-form features.
Themost expensive portion of the model is storage of the50k-by-300 W1and its matrix multiplication withl1, which is sparse: this is trivial on modern hard-ware.
Our multi-task DNN takes < 150KB in mem-ory whereas e.g.
SVM-Word takes about 200MB.Compactness is particularly important for queryclassification, since one may desire to add new do-mains after discovering new needs from the querylogs of an operational system.
On the other hand, itis prohibitively expensive to collect labeled trainingdata for new domains.
Very often, we only have verysmall training data or even no training data.To evaluate the models using the above crite-ria, we perform domain adaptation experiments onquery classification using the following procedure:(1) Select one query classification task t?.
Train MT-DNN on the remaining tasks (including Web Searchtask) to obtain a semantic representation (l2); (2)Given a fixed l2, train an SVM on the training datat?, using varying amounts of labels; (3) Evaluate theAUC on the test data of t?We compare three SVM classifiers trained us-ing different feature representations: (1) Semanti-cRepresentation uses the l2features generated ac-cording to the above procedure.
(2) Word3gramuses unigram, bigram and trigram word features.
(3) Letter3gram uses letter-trigrams.
Note thatWord3gram and Letter3gram correspond to SVM-Word and SVM-Letter respectively in Table 2.The AUC results for different amounts of t?train-ing data are shown in Figure 4.
In the Hotel, Flightand Restaurant domains, we observe that our seman-tic representation dominated the other two featurerepresentations (Word3gram and Letter3gram) inall cases except the extremely large-data regime(more than 1 million training samples in domain t?
).Given sufficient labels, SVM is able to train well onWord3gram sparse features, but for most cases Se-918manticRepresentation is recommended.4In a further experiment, we compare the follow-ing two DNNs using the same domain adaptationprocedure: (1) DNN1: DNN where W1is ran-domly initialized and parametersW1,W2,Wt?3aretrained on varying amounts of data in t?
; (2) DNN2:DNN where W1is obtained from other tasks (i.e.SemanticRepresentation) and fixed, while param-eters W2,Wt?3are trained on varying amounts ofdata in t?.
The purpose is to see whether shared se-mantic representation is useful even under a DNNarchitecture.
Figure 5 show the AUC results ofDNN1 vs. DNN2 (the results SVM denotes thesame system as SemanticRepresentation in Figure4, plotted here for reference).
We observe that whenthe training data is extremely large (millions of sam-ples), one does best by training all parameters fromscratch (DNN1).
Otherwise, one is better off usinga shared semantic representation trained by multi-task objectives.
Comparing DNN2 and SVM withSemanticRepresentation, we note that SVM worksbest for training data of several thousand samples;DNN2 works best in the medium data range.4 Related WorkThere is a large body of work on representationlearning for natural language processing, sometimesusing different terminologies for similar concepts;e.g., feature generation, dimensionality reduction,and vector space models.
The main motivation issimilar: to abstract away from surface forms inwords, sentences, or documents, in order to alle-viate sparsity and approximate semantics.
Tradi-tional techniques include LSA (Deerwester et al,1990), ESA (Gabrilovich and Markovitch, 2007),PCA (Karhunen, 1998), and non-linear kernel vari-ants (Sch?olkopf et al, 1998).
Recently, learning-based approaches inspired by neural networks, es-pecially DNNs, have gained in prominence, due totheir favorable performance (Huang et al, 2013; Ba-roni et al, 2014; Milajevs et al, 2014).Popular methods for learning word representa-tions include (Collobert et al, 2011; Mikolov et al,2013c; Mnih and Kavukcuoglu, 2013; Penningtonet al, 2014): all are based on unsupervised objec-4The trends differ slightly in the Nightlife domain.
We believethis may be due to data bias on test data (only 298 samples).tives of predicting words or word frequencies fromraw text.
End-to-end neural network models for spe-cific tasks (e.g.
parsing) often use these word repre-sentations as initialization, which are then iterativelyimproved by optimizing a supervised objective (e.g.parsing accuracy).
A selection of successful appli-cations of this approach include sequence labeling(Turian et al, 2010), parsing (Chen and Manning,2014), sentiment (Socher et al, 2013), question an-swering (Iyyer et al, 2014) and translation modeling(Gao et al, 2014a).Our model takes queries and documents as in-put, so it learns sentence/document representations.This is currently an open research question, the chal-lenge being how to properly model semantic com-positionality of words in vector space (Huang et al,2013; M. Baroni and Zamparelli, 2013; Socher etal., 2013).
While we adopt a bag-of-words approachfor practical reasons (memory and run-time), ourmulti-task framework is extensible to other meth-ods for sentence/document representations, such asthose based on convolutional networks (Kalchbren-ner et al, 2014; Shen et al, 2014; Gao et al, 2014b),parse tree structure (Irsoy and Cardie, 2014), andrun-time inference (Le and Mikolov, 2014).The synergy between multi-task learning and neu-ral nets is quite natural; the general idea dates backto (Caruana, 1997).
The main challenge is in design-ing the tasks and the network structure.
For exam-ple, (Collobert et al, 2011) defined part-of-speechtagging, chunking, and named entity recognition asmultiple tasks in a single sequence labeler; (Bordeset al, 2012) defined multiple data sources as tasksin their relation extraction system.
While concep-tually similar, our model is novel in that it com-bines tasks as disparate as classification and rank-ing.
Further, considering that multi-task models of-ten exhibit mixed results (i.e.
gains in some tasks butdegradation in others), our accuracy improvementsacross all tasks is a very satisfactory result.5 ConclusionIn this work, we propose a robust and practical rep-resentation learning algorithm based on multi-taskobjectives.
Our multi-task DNN model success-fully combines tasks as disparate as classificationand ranking, and the experimental results demon-919strate that the model consistently outperforms strongbaselines in various query classification and websearch tasks.
Meanwhile, we demonstrated com-pactness of the model and the utility of the learnedquery/document representation for domain adapta-tion.Our model can be viewed as a general method forlearning semantic representations beyond the wordlevel.
Beyond query classification and web search,we believe there are many other knowledge sources(e.g.
sentiment, paraphrase) that can be incorporatedeither as classification or ranking tasks.
A compre-hensive exploration will be pursued as future work.AcknowledgmentsWe thank Xiaolong Li, Yelong Shen, Xinying Song,Jianshu Chen, Byungki Byun, Bin Cao and theanonymous reviewers for valuable discussions andcomments.ReferencesMarco Baroni, Georgiana Dinu, and Germ?an Kruszewski.2014.
Don?t count, predict!
a systematic compari-son of context-counting vs. context-predicting seman-tic vectors.
In Proceedings of the 52nd Annual Meet-ing of the Association for Computational Linguistics(Volume 1: Long Papers), pages 238?247, Baltimore,Maryland, June.
Association for Computational Lin-guistics.David M Blei, Andrew Y Ng, and Michael I Jordan.2003.
Latent dirichlet alocation.
the Journal of ma-chine Learning research, 3:993?1022.Antoine Bordes, Xavier Glorot, Jason Weston, andYoshua Bengio.
2012.
Joint learning of words andmeaning representations for open-text semantic pars-ing.
In AISTATS.Andrew P Bradley.
1997.
The use of the area under theroc curve in the evaluation of machine learning algo-rithms.
Pattern recognition, 30(7):1145?1159.Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier,Matt Deeds, Nicole Hamilton, and Greg Hullender.2005.
Learning to rank using gradient descent.
InProceedings of the 22nd international conference onMachine learning, pages 89?96.
ACM.Rich Caruana.
1997.
Multitask learning.
MachineLearning, 28.Danqi Chen and Christopher Manning.
2014.
A fast andaccurate dependency parser using neural networks.In Proceedings of the 2014 Conference on EmpiricalMethods in Natural Language Processing (EMNLP),pages 740?750, Doha, Qatar, October.
Association forComputational Linguistics.R.
Collobert, J. Weston, L. Bottou, M. Karlen,K.
Kavukcuoglu, and P. Kuksa.
2011.
Natural lan-guage processing (almost) from scratch.
Journal ofMachine Learning Research, 12:2493?2537.Scott Deerwester, Susan T. Dumais, George W. Furnas,Thomas K. Landauer, and Richard Harshman.
1990.Indexing by latent semantic analysis.
Journal of theAmerican Society for Information Science, 41(6).John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learning andstochastic optimization.
The Journal of MachineLearning Research, 12:2121?2159.E.
Gabrilovich and S. Markovitch.
2007.
Computing se-mantic relatedness using wikipedia-based explicit se-mantic analysis.
In IJCAI.Jianfeng Gao, Xiaodong He, and Jian-Yun Nie.
2010.Clickthrough-based translation models for web search:from word models to phrase models.
In Proceedings ofthe 19th ACM international conference on Informationand knowledge management, pages 1139?1148.
ACM.Jianfeng Gao, Kristina Toutanova, and Wen-tau Yih.2011.
Clickthrough-based latent semantic models forweb search.
In Proceedings of the 34th internationalACM SIGIR conference on Research and developmentin Information Retrieval, pages 675?684.
ACM.Jianfeng Gao, Xiaodong He, Wen-tau Yih, and Li Deng.2014a.
Learning continuous phrase representationsfor translation modeling.
In Proceedings of the 52ndAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 699?709,Baltimore, Maryland, June.
Association for Computa-tional Linguistics.Jianfeng Gao, Patrick Pantel, Michael Gamon, XiaodongHe, and Li Deng.
2014b.
Modeling interestingnesswith deep neural networks.
In Proceedings of the 2014Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP), pages 2?13, Doha, Qatar,October.
Association for Computational Linguistics.Thomas Hofmann.
1999.
Probabilistic latent semanticindexing.
In Proceedings of the 22nd annual interna-tional ACM SIGIR conference on Research and devel-opment in information retrieval, pages 50?57.
ACM.Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,Alex Acero, and Larry Heck.
2013.
Learning deepstructured semantic models for web search using click-through data.
In Proceedings of the 22nd ACM inter-national conference on Conference on information &knowledge management, pages 2333?2338.
ACM.Ozan Irsoy and Claire Cardie.
2014.
Deep recursiveneural networks for compositionality in language.
InNIPS.920Mohit Iyyer, Jordan Boyd-Graber, Leonardo Claudino,Richard Socher, and Hal Daum?e III.
2014.
A neu-ral network for factoid question answering over para-graphs.
In Proceedings of the 2014 Conference onEmpirical Methods in Natural Language Processing(EMNLP), pages 633?644, Doha, Qatar, October.
As-sociation for Computational Linguistics.Kalervo J?arvelin and Jaana Kek?al?ainen.
2000.
Ir evalua-tion methods for retrieving highly relevant documents.In Proceedings of the 23rd annual international ACMSIGIR conference on Research and development in in-formation retrieval, pages 41?48.
ACM.Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-som.
2014.
A convolutional neural network for mod-elling sentences.
In Proceedings of the 52nd AnnualMeeting of the Association for Computational Linguis-tics (Volume 1: Long Papers), pages 655?665, Balti-more, Maryland, June.
Association for ComputationalLinguistics.Juha Karhunen.
1998.
Principal component neural net-workstheory and applications.
Pattern Analysis & Ap-plications, 1(1):74?75.Quoc Le and Tomas Mikolov.
2014.
Distributed rep-resentations of sentences and documents.
In Pro-ceedings of the International Conference on MachineLearning (ICML).R.
Bernardi M. Baroni and R. Zamparelli.
2013.
Frege inspace: A program for compositional distributional se-mantics.
Linguistic Issues in Language Technologies.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013a.
Distributed represen-tations of words and phrases and their composition-ality.
In Advances in Neural Information ProcessingSystems, pages 3111?3119.Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013b.
Linguistic regularities in continuous spaceword representations.
In Proceedings of the 2013 Con-ference of the North American Chapter of the Associa-tion for Computational Linguistics: Human LanguageTechnologies, pages 746?751, Atlanta, Georgia, June.Association for Computational Linguistics.Tom?a?s Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado,and Jeffrey Dean.
2013c.
Distributed representationsof words and phrases and their compositionality.
InNIPS.Dmitrijs Milajevs, Dimitri Kartsaklis, MehrnooshSadrzadeh, and Matthew Purver.
2014.
Evaluatingneural word representations in tensor-based composi-tional settings.
In Proceedings of the 2014 Conferenceon Empirical Methods in Natural Language Process-ing (EMNLP), pages 708?719, Doha, Qatar, October.Association for Computational Linguistics.Andriy Mnih and Koray Kavukcuoglu.
2013.
Learningword embeddings efficiently with noise-contrastive es-timation.
In Advances in Neural Information Process-ing Systems 26 (NIPS 2013).Gregoire Montavon, Genevieve Orr, and Klaus-RobertMuller.
2012.
Neural Networks: Tricks of the Trade2nd ed.
springer.Jeffrey Pennington, Richard Socher, and ChristopherManning.
2014.
Glove: Global vectors for word rep-resentation.
In Proceedings of the 2014 Conferenceon Empirical Methods in Natural Language Process-ing (EMNLP), pages 1532?1543, Doha, Qatar, Octo-ber.
Association for Computational Linguistics.B.
Sch?olkopf, A. Smola, and K.-R. M?uller.
1998.
Non-linear component analysis as kernel eigenvalue prob-lem.
Neural Computation, 10.Dou Shen, Rong Pan, Jian-Tao Sun, Jeffrey Junfeng Pan,Kangheng Wu, Jie Yin, and Qiang Yang.
2006.
Queryenrichment for web-query classification.
ACM Trans.Inf.
Syst., 24(3):320?352, July.Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng, andGr?egoire Mesnil.
2014.
A latent semantic modelwith convolutional-pooling structure for informationretrieval.
In Proceedings of the 23rd ACM Interna-tional Conference on Conference on Information andKnowledge Management, pages 101?110.
ACM.Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang,Christopher D. Manning, Andrew Ng, and ChristopherPotts.
2013.
Recursive deep models for semanticcompositionality over a sentiment treebank.
In Pro-ceedings of the 2013 Conference on Empirical Meth-ods in Natural Language Processing, pages 1631?1642, Seattle, Washington, USA, October.
Associationfor Computational Linguistics.Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.2010.
Word representations: A simple and generalmethod for semi-supervised learning.
In Proceed-ings of the 48th Annual Meeting of the Association forComputational Linguistics, pages 384?394, Uppsala,Sweden, July.Chengxiang Zhai and John Lafferty.
2001.
A study ofsmoothing methods for language models applied toad hoc information retrieval.
In Proceedings of the24th annual international ACM SIGIR conference onResearch and development in information retrieval,pages 334?342.
ACM.921
