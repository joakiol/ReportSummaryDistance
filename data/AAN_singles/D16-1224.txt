Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2084?2089,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsAMR-to-text generation as a Traveling Salesman ProblemLinfeng Song1, Yue Zhang3, Xiaochang Peng1, Zhiguo Wang2 and Daniel Gildea11Department of Computer Science, University of Rochester, Rochester, NY 146272IBM T.J. Watson Research Center, Yorktown Heights, NY 105983Singapore University of Technology and DesignAbstractThe task of AMR-to-text generation is to gen-erate grammatical text that sustains the seman-tic meaning for a given AMR graph.
We at-tack the task by first partitioning the AMRgraph into smaller fragments, and then gener-ating the translation for each fragment, beforefinally deciding the order by solving an asym-metric generalized traveling salesman prob-lem (AGTSP).
A Maximum Entropy classifieris trained to estimate the traveling costs, and aTSP solver is used to find the optimized solu-tion.
The final model reports a BLEU score of22.44 on the SemEval-2016 Task8 dataset.1 IntroductionAbstract Meaning Representation (AMR) (Ba-narescu et al, 2013) is a semantic formalism en-coding the meaning of a sentence as a rooted, di-rected graph.
Shown in Figure 1, the nodes ofan AMR graph (e.g.
?boy?, ?go-01?
and ?want-01?)
represent concepts, and the edges (e.g.
?ARG0?and ?ARG1?)
represent relations between concepts.AMR jointly encodes a set of different semantic phe-nomena, which makes it useful in applications likequestion answering and semantics-based machinetranslation.
AMR has served as an intermediaterepresentation for various text-to-text NLP applica-tions, such as statistical machine translation (SMT)(Jones et al, 2012).The task of AMR-to-text generation is to gener-ate grammatical text containing the same semanticmeaning as a given AMR graph.
This task is im-portant yet alo challenging since each AMR graphwant-01boygo-01ARG1ARG0ARG0Figure 1: AMR graph for ?The boy wants to go?.usually has multiple corresponding sentences, andsyntactic structure and function words are abstractedaway when transforming a sentence into AMR (Ba-narescu et al, 2013).
There has been work deal-ing with text-to-AMR parsing (Flanigan et al, 2014;Wang et al, 2015; Peng et al, 2015; Vanderwendeet al, 2015; Pust et al, 2015; Artzi et al, 2015).
Onthe other hand, relatively little work has been doneon AMR-to-text generation.
One recent exceptionis Flanigan et al (2016), who first generate a span-ning tree for the input AMR graph, and then applya tree transducer to generate the sentence.
Here, wedirectly generate the sentence from an input AMRby treating AMR-to-text generation as a variant ofthe traveling salesman problem (TSP).Given an AMR as input, our method first cutsthe graph into several rooted and connected frag-ments (sub-graphs), and then finds the translationfor each fragment, before finally generating the sen-tence for the whole AMR by ordering the transla-tions.
To cut the AMR and translate each fragment,we match the input AMR with rules, each consistingof a rooted, connected AMR fragment and a corre-sponding translation.
These rules serve in a similarway to rules in SMT models.
We learn the rules by amodified version of the sampling algorithm of Peng2084et al (2015), and use the rule matching algorithm ofCai and Knight (2013).For decoding the fragments and synthesizing theoutput, we define a cut to be a subset of matchedrules without overlap that covers the AMR, and anordered cut to be a cut with the rules being or-dered.
To generate a sentence for the whole AMR,we search for an ordered cut, and concatenate trans-lations of all rules in the cut.
TSP is used to traversedifferent cuts and determine the best order.
Intu-itively, our method is similar to phrase-based SMT,which first cuts the input sentence into phrases, thenobtains the translation for each source phrase, beforefinally generating the target sentence by ordering thetranslations.
Although the computational cost of ourmethod is low, the initial experiment is promising,yielding a BLEU score of 22.44 on a standard bench-mark.2 MethodWe reformulate the problem of AMR-to-text gener-ation as an asymmetric generalized traveling sales-man problem (AGTSP), a variant of TSP.2.1 TSP and its variantsGiven a non-directed graph GN with n cities, sup-posing that there is a traveling cost between eachpair of cities, TSP tries to find a tour of the minimaltotal cost visiting each city exactly once.
In contrast,the asymmetric traveling salesman problem (ATSP)tries to find a tour of the minimal total cost on a di-rected graph, where the traveling costs between twonodes are different in each direction.
Given a di-rected graph GD with n nodes, which are clusteredinto m groups, the asymmetric generalized travelingsalesman problem (AGTSP) tries to find a tour of theminimal total cost visiting each group exactly once.2.2 AMR-to-text Generation as AGTSPGiven an input AMR A, each node in the AGTSPgraph can be represented as (c, r), where c is a con-cept in A and r = (Asub, Tsub) is a rule that con-sists of an AMR fragment containing c and a trans-lation of the fragment.
We put all nodes containingthe same concept into one group, thereby translatingeach concept in the AMR exactly once.To show a brief example, consider the AMR inFigure 1 and the following rules,ns (b,r4) (w,r3)(w,r1) (g,r2)(g,r3) neFigure 2: An example AGTSP graphr1 (w/want-01) ||| wantsr2 (g/go-01) ||| to gor3 (w/want-01 :ARG1 g/go-01) ||| wants to gor4 (b/boy) ||| The boyWe build an AGTSP graph in Figure 2, where eachcircle represents a group and each tuple (such as(b, r4)) represents a node in the AGTSP graph.
Weadd two nodes ns and ne representing the start andend nodes respectively.
Each belongs to a specificgroup that only contains that node, and a tour al-ways starts with ns and ends with ne.
Legal movesare shown in black arrows, while illegal moves areshown in red.
One legal tour is ns ?
(b, r4) ?
(w, r3) ?
(g, r3) ?
ne.
The order in which nodeswithin a rule are visited is arbitrary; for a rule withN concepts, the number of visiting orders is O(N !
).To reduce the search space, we enforce the breadthfirst order by setting costs to zero or infinity.
In ourexample, the traveling cost from (w, r3) to (g, r3) is0, while the traveling cost from (g, r3) to (w, r3) isinfinity.
Traveling from (g, r2) to (w, r3) also hasinfinite cost, since there is overlap on the concept?w/want-01?
between them.The traveling cost is calculated by Algorithm 1.We first add ns and ne serving the same functionas Figure 2.
The traveling cost from ns directly tone is infinite, since a tour has to go through othernodes before going to the end.
On the other hand,the traveling cost from ne to ns is 0 (Lines 3-4), asa tour always goes back to the start after reachingthe end.
The traveling cost from ns to ni = (ci, ri)is the model score only if ci is the first node of theAMR fragment of ri, otherwise the traveling costis infinite (Lines 6-9).
Similarly, the traveling costfrom ni to ne is the model score only if ci is the lastnode of the fragment of ri.
Otherwise, it is infinite(Lines 10-13).
The traveling cost from ni = (ci, ri)to nj = (cj , rj) is 0 if ri and rj are the same ruleand cj is the next node of ci in the AMR fragment ofri (Lines 16-17).A tour has to travel through an AMR fragment be-2085Data: Nodes in AGTSP graph GResult: Traveling Cost Matrix T1 ns ?
(?<s>?,?<s>?
);2 ne ?
(?</s>?,?</s>?
);3 T[ns][ne]??
;4 T[ne][ns]?
0;5 for ni ?
(ci, ri) in G do6 if ci = ri.frag.first then7 T[ns][ni]?ModelScore(ns,ni);8 else9 T[ns][ni]??
;10 if ci = ri.frag.last then11 T[ni][ne]?ModelScore(ni,ne);12 else13 T[ni][ne]??
;14 for ni ?
(ci, ri) in G do15 for nj ?
(cj , rj) in G do16 if ri = rj and ri.frag.next(ci) = cj then17 T[ni][nj]?
018 else if ri.frag ?
rj .frag = ?
and ci =ri.frag.last and cj = rj .frag.first then19 T[ni][nj]?ModelScore(ni,nj)20 else21 T[ni][nj]?
?Algorithm 1: Traveling cost algorithmfore jumping to another fragment.
We choose thebreadth-first order of nodes within the same rule,which is guaranteed to exist, as each AMR fragmentis rooted and connected.
Costs along the breadth-first order within a rule ri are set to 0, while othercosts with a rule are infinite.If ri is not equal to rj , then the traveling costis the model score if there is no overlap betweenri and rj?s AMR fragment and it moves from ri?slast node to rj?s first node (Lines 18-19), other-wise the traveling cost is infinite (Lines 20-21).
Allother cases are illegal and we assign infinite travel-ing cost.
We do not allow traveling between overlap-ping nodes, whose AMR fragments share commonconcepts.
Otherwise the traveling cost is evaluatedby a maximum entropy model, which will be dis-cussed in detail in Section 2.4.2.3 Rule AcquisitionWe extract rules from a corpus of (sentence, AMR)pairs using the method of Peng et al (2015).
Givenan aligned (sentence, AMR) pair, a phrase-fragmentpair is a pair ([i, j], f), where [i, j] is a span of thesentence and f represents a connected and rootedAMR fragment.
A fragment decomposition forestconsists of all possible phrase-fragment pairs thatsatisfy the alignment agreement for phrase-basedMT (Koehn et al, 2003).
The rules that we use forgeneration are the result of applying an MCMC pro-cedure to learn a set of likely phrase-fragment pairsfrom the forests containing all possible pairs.
Onedifference from the work of Peng et al (2015) isthat, while they require the string side to be tight(does not include unaligned words on both sides),we expand the tight phrases to incorporate unalignedwords on both sides.
The intuition is that they dotext-to-AMR parsing, which often involves discard-ing function words, while our task is AMR-to-textgeneration, and we need to be able to fill in these un-aligned words.
Since incorporating unaligned wordswill introduce noise, we rank the translation candi-dates for each AMR fragment by their counts in thetraining data, and select the top N candidates.1We also generate concept rules which directly usea morphological string of the concept for transla-tion.
For example, for concept ?w/want-01?
in Fig-ure 1, we generate concept rules such as ?
(w/want-01) ||| want?, ?
(w/want-01) ||| wants?, ?
(w/want-01)||| wanted?
and ?
(w/want-01) ||| wanting?.
The al-gorithm (described in section 2.2) will choose themost suitable one from the rule set.
It is similar tomost MT systems in creating a translation candidatefor each word, besides normal translation rules.
Itis easy to guarantee that the rule set can fully coverevery input AMR graph.Some concepts (such as ?have-rel-role-91?)
in anAMR graph do not contribute to the final translation,and we skip them when generating concept rules.Besides that, we use a verbalization list2 for conceptrule generation.
For rule ?VERBALIZE peacekeep-ing TO keep-01 :ARG1 peace?, we will create a con-cept rule ?
(k/keep-01 :ARG1 (p/peace)) ||| peace-keeping?
if the left-hand-side fragment appears inthe target graph.1Our code for grammar induction can be downloaded fromhttps://github.com/xiaochang13/AMR-generation2http://amr.isi.edu/download/lists/verbalization-list-v1.06.txt20862.4 Traveling costConsidering an AGTSP graph whose nodes are clus-tered into m groups, we define the traveling cost fora tour T in Equation 1:cost(ns, ne) = ?m?i=0log p(?yes?|nTi , nTi+1) (1)where nT0 = ns, nTm+1 = ne and each nTi (i ?
[1 .
.
.m]) belongs to a group that is different fromall others.
Here p(?yes?|nj , ni) represents a learnedscore for a move from nj to ni.
The choices be-fore nTi are independent from choosing nTi+1 givennTi because of the Markovian property of the TSPproblem.
Previous methods (Zaslavskiy et al, 2009)evaluate traveling costs p(nTi+1 |nTi) by using a lan-guage model.
Inevitably some rules may only coverone translation word, making only bigram languagemodels naturally applicable.
Zaslavskiy et al (2009)introduces a method for incorporating a trigram lan-guage model.
However, as a result, the number ofnodes in the AGTSP graph grows exponentially.To tackle the problem, we treat it as a local binary(?yes?
or ?no?)
classification problem whether weshould move to nj from ni.
We train a maximumentropy model, where p(?yes?|ni, nj) is defined as:p(?yes?|ni, nj) =1Z(ni, nj)exp[ k?i=1?ifi(?yes?, ni, nj)](2)The model uses 3 real-valued features: a languagemodel score, the word count of the concatenatedtranslation from ni to nj , and the length of the short-est path from ni?s root to nj?s root in the input AMR.If either ni or nj is the start or end node, we set thepath length to 0.
Using this model, we can use what-ever N-gram we have at each time.
Although lan-guage models favor shorter translations, word countwill balance the effect, which is similar to MT sys-tems.
The length of the shortest path is used as afeature because the concepts whose translations areadjacent usually have lower path length than others.3 Experiments3.1 SetupWe use the dataset of SemEval-2016 Task8 (Mean-ing Representation Parsing), which contains 16833System Dev TestPBMT 13.13 16.94OnlyConceptRule 13.15 14.93OnlyInducedRule 17.68 18.09OnlyBigramLM 17.19 17.75All 21.12 22.44JAMR-gen 23.00 23.00Table 1: Main results.training instances, 1368 dev instances and 1371test instances.
Each instance consists of an AMRgraph and a sentence representing the same mean-ing.
Rules are extracted from the training data, andhyperparameters are tuned on the dev set.
For tuningand testing, we filter out sentences that have morethan 30 words, resulting in 1103 dev instances and1055 test instances.
We train a 4-gram languagemodel (LM) with gigaword (LDC2011T07), and useBLEU (Papineni et al, 2002) as the evaluation met-ric.
To solve the AGTSP, we use Or-tool3.Our graph-to-string rules are reminiscent ofphrase-to-string rules in phrase-based MT (PBMT).We compare our system to a baseline (PBMT) thatfirst linearizes the input AMR graph by breadth firsttraversal, and then adopts the PBMT system fromMoses4 to translate the linearized AMR into a sen-tence.
To traverse the children of an AMR con-cept, we use the original order in the text file.
TheMT system is trained with the default setting on thesame dataset and LM.
We also compare with JAMR-gen5 (Flanigan et al, 2016), which is trained on thesame dataset but with a 5-gram LM from gigaword(LDC2011T07).To evaluate the importance of each module in oursystem, we develop the following baselines: Only-ConceptRule uses only the concept rules, OnlyIn-ducedRule uses only the rules induced from the frag-ment decomposition forest, OnlyBigramLM usesboth types of rules, but the traveling cost is evalu-ated by a bigram LM trained with gigaword.3.2 ResultsThe results are shown in Table 1.
Our method(All) significantly outperforms the baseline (PBMT)3https://developers.google.com/optimization/4http://www.statmt.org/moses/5https://github.com/jflanigan/jamr/tree/Generator2087(w / want-01:ARG0 (b / boy):ARG1 (b2 / believe-01:ARG0 (g / girl):ARG1 b))Ref: the boy wants the girl to believe himAll: a girl wanted to believe himJAMR-gen: boys want the girl to believeTable 2: Case study.on both the dev and test sets.
PBMT does notoutperform OnlyBigramLM and OnlyInducedRule,demonstrating that our rule induction algorithm iseffective.
We consider rooted and connected frag-ments from the AMR graph, and the TSP solverfinds better solutions than beam search, as consis-tent with Zaslavskiy et al (2009).
In addition, On-lyInducedRule is significantly better than OnlyCon-ceptRule, showing the importance of induced ruleson performance.
This also confirms the reason thatAll outperforms PBMT.
This result confirms our ex-pectation that concept rules, which are used for ful-filling the coverage of an input AMR graph in caseof OOV, are generally not of high quality.
More-over, All outperforms OnlyBigramLM showing thatour maximum entropy model is stronger than a bi-gram language model.
Finally, JAMR-gen outper-forms All, while JAMR-gen uses a higher order lan-guage model than All (5-gram VS 4-gram).For rule coverage, around 31% AMR graphs and84% concepts in the development set are covered byour induced rules extracted from the training set.3.3 Analysis and DiscussionsWe further analyze All and JAMR-gen with an ex-ample AMR and show the AMR graph, the refer-ence, and results in Table 2.
First of all, both Alland JAMR-gen outputs a reasonable translation con-taining most of the meaning from the AMR.
On theother hand, All fails to recognize ?boy?
as the sub-ject.
The reason is that the feature set does not in-clude edge labels, such as ?ARG0?
and ?ARG1?.Finally, neither All and JAMR-gen can handle thesituation when a re-entrance node (such as ?b/boy?in example graph of Table 2) need to be translatedtwice.
This limitation exists for both works.4 Related WorkOur work is related to prior work on AMR (Ba-narescu et al, 2013).
There has been a list of workon AMR parsing (Flanigan et al, 2014; Wang et al,2015; Peng et al, 2015; Vanderwende et al, 2015;Pust et al, 2015; Artzi et al, 2015), which predictsthe AMR structures for a given sentence.
On the re-verse direction, Flanigan et al (2016) and our workhere study sentence generation from a given AMRgraph.
Different from Flanigan et al (2016) whomap a input AMR graph into a tree before lineariza-tion, we apply synchronous rules consisting of AMRgraph fragments and text to directly transfer a AMRgraph into a sentence.
In addition to AMR parsingand generation, there has also been work using AMRas a semantic representation in machine translation(Jones et al, 2012).Our work also belongs to the task of text genera-tion (Reiter and Dale, 1997).
There has been workon generating natural language text from a bag ofwords (Wan et al, 2009; Zhang and Clark, 2015),surface syntactic trees (Zhang, 2013; Song et al,2014), deep semantic graphs (Bohnet et al, 2010)and logical forms (White, 2004; White and Rajku-mar, 2009).
We are among the first to investigategeneration from AMR, which is a different type ofsemantic representation.5 ConclusionIn conclusion, we showed that a TSP solver with afew real-valued features can be useful for AMR-to-text generation.
Our method is based on a set ofgraph to string rules, yet significantly better thana PBMT-based baseline.
This shows that our ruleinduction algorithm is effective and that the TSPsolver finds better solutions than beam search.AcknowledgmentsWe are grateful for the help of Jeffrey Flanigan, LinZhao, and Yifan He.
This work was funded byNSF IIS-1446996, and a Google Faculty ResearchAward.
Yue Zhang is funded by NSFC61572245and T2MOE201301 from Singapore Ministry of Ed-ucation.2088ReferencesYoav Artzi, Kenton Lee, and Luke Zettlemoyer.
2015.Broad-coverage CCG semantic parsing with AMR.
InConference on Empirical Methods in Natural Lan-guage Processing (EMNLP-15), pages 1699?1710.Laura Banarescu, Claire Bonial, Shu Cai, MadalinaGeorgescu, Kira Griffitt, Ulf Hermjakob, KevinKnight, Philipp Koehn, Martha Palmer, and NathanSchneider.
2013.
Abstract meaning representationfor sembanking.
In Proceedings of the 7th LinguisticAnnotation Workshop and Interoperability with Dis-course, pages 178?186.Bernd Bohnet, Leo Wanner, Simon Mill, and AliciaBurga.
2010.
Broad coverage multilingual deep sen-tence generation with a stochastic multi-level real-izer.
In Proceedings of the 23rd International Con-ference on Computational Linguistics (COLING-10),pages 98?106.Shu Cai and Kevin Knight.
2013.
Smatch: an evaluationmetric for semantic feature structures.
In Proceedingsof the 51st Annual Meeting of the Association for Com-putational Linguistics (ACL-13), pages 748?752.Jeffrey Flanigan, Sam Thomson, Jaime Carbonell, ChrisDyer, and Noah A. Smith.
2014.
A discriminativegraph-based parser for the abstract meaning represen-tation.
In Proceedings of the 52nd Annual Meeting ofthe Association for Computational Linguistics (ACL-14), pages 1426?1436.Jeffrey Flanigan, Chris Dyer, Noah A. Smith, and JaimeCarbonell.
2016.
Generation from abstract mean-ing representation using tree transducers.
In Proceed-ings of the 2016 Meeting of the North American chap-ter of the Association for Computational Linguistics(NAACL-16), pages 731?739.Bevan Jones, Jacob Andreas, Daniel Bauer, Karl MoritzHermann, and Kevin Knight.
2012.
Semantics-based machine translation with hyperedge replacementgrammars.
In Proceedings of the International Con-ference on Computational Linguistics (COLING-12),pages 1359?1376.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proceed-ings of the 2003 Meeting of the North American chap-ter of the Association for Computational Linguistics(NAACL-03), pages 48?54.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of the40th Annual Conference of the Association for Com-putational Linguistics (ACL-02), pages 311?318.Xiaochang Peng, Linfeng Song, and Daniel Gildea.2015.
A synchronous hyperedge replacement gram-mar based approach for AMR parsing.
In Proceedingsof the Nineteenth Conference on Computational Natu-ral Language Learning (CoNLL-15), pages 731?739.Michael Pust, Ulf Hermjakob, Kevin Knight, DanielMarcu, and Jonathan May.
2015.
Parsing English intoabstract meaning representation using syntax-basedmachine translation.
In Conference on EmpiricalMethods in Natural Language Processing (EMNLP-15), pages 1143?1154.Ehud Reiter and Robert Dale.
1997.
Building appliednatural language generation systems.
Natural Lan-guage Engineering, 3(1):57?87.Linfeng Song, Yue Zhang, Kai Song, and Qun Liu.2014.
Joint morphological generation and syntacticlinearization.
In Proceedings of the National Confer-ence on Artificial Intelligence (AAAI-14), pages 1522?1528.Lucy Vanderwende, Arul Menezes, and Chris Quirk.2015.
An AMR parser for English, French, German,Spanish and Japanese and a new AMR-annotated cor-pus.
In Proceedings of the 2015 Meeting of the NorthAmerican chapter of the Association for Computa-tional Linguistics (NAACL-15), pages 26?30.Stephen Wan, Mark Dras, Robert Dale, and Ce?cile Paris.2009.
Improving grammaticality in statistical sentencegeneration: Introducing a dependency spanning treealgorithm with an argument satisfaction model.
InProceedings of the 12th Conference of the EuropeanChapter of the ACL (EACL-09), pages 852?860.Chuan Wang, Nianwen Xue, and Sameer Pradhan.
2015.A transition-based algorithm for AMR parsing.
InProceedings of the 2015 Meeting of the North Ameri-can chapter of the Association for Computational Lin-guistics (NAACL-15), pages 366?375.Michael White and Rajakrishnan Rajkumar.
2009.
Per-ceptron reranking for CCG realization.
In Conferenceon Empirical Methods in Natural Language Process-ing (EMNLP-09), pages 410?419.Michael White.
2004.
Reining in CCG chart realiza-tion.
In International Conference on Natural Lan-guage Generation (INLG-04), pages 182?191.Mikhail Zaslavskiy, Marc Dymetman, and Nicola Can-cedda.
2009.
Phrase-based statistical machine trans-lation as a traveling salesman problem.
In Proceed-ings of the 47th Annual Meeting of the Association forComputational Linguistics (ACL-09), pages 333?341.Yue Zhang and Stephen Clark.
2015.
Discriminativesyntax-based word ordering for text generation.
Com-putational Linguistics, 41(3):503?538.Yue Zhang.
2013.
Partial-tree linearization: Generalizedword ordering for text synthesis.
In Proceedings ofthe International Joint Conference on Artificial Intelli-gence (IJCAI-13), pages 2232?2238.2089
