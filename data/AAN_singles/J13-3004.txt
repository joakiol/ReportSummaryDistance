Computing Lexical ContrastSaif M. Mohammad?National Research Council CanadaBonnie J.
Dorr?
?University of MarylandGraeme Hirst?University of TorontoPeter D. Turney?National Research Council CanadaKnowing the degree of semantic contrast between words has widespread application in naturallanguage processing, including machine translation, information retrieval, and dialogue sys-tems.
Manually created lexicons focus on opposites, such as hot and cold.
Opposites are ofmany kinds such as antipodals, complementaries, and gradable.
Existing lexicons often do notclassify opposites into the different kinds, however.
They also do not explicitly list word pairsthat are not opposites but yet have some degree of contrast in meaning, such as warm and coldor tropical and freezing.
We propose an automatic method to identify contrasting word pairsthat is based on the hypothesis that if a pair of words, A and B, are contrasting, then there isa pair of opposites, C and D, such that A and C are strongly related and B and D are stronglyrelated.
(For example, there exists the pair of opposites hot and cold such that tropical is relatedto hot, and freezing is related to cold.)
We will call this the contrast hypothesis.We begin with a large crowdsourcing experiment to determine the amount of humanagreement on the concept of oppositeness and its different kinds.
In the process, we flesh outkey features of different kinds of opposites.
We then present an automatic and empirical measureof lexical contrast that relies on the contrast hypothesis, corpus statistics, and the structure ofa Roget-like thesaurus.
We show how, using four different data sets, we evaluated our approachon two different tasks, solving ?most contrasting word?
questions and distinguishing synonymsfrom opposites.
The results are analyzed across four parts of speech and across five different kindsof opposites.
We show that the proposed measure of lexical contrast obtains high precision andlarge coverage, outperforming existing methods.?
National Research Council Canada.
E-mail: saif.mohammad@nrc-cnrc.gc.ca.??
Department of Computer Science and Institute of Advanced Computer Studies, University of Maryland.E-mail: bonnie@umiacs.umd.edu.?
Department of Computer Science, University of Toronto.
E-mail: gh@cs.toronto.edu.?
National Research Council Canada.
E-mail: peter.turney@nrc-cnrc.gc.ca.Submission received: 14 January 2010; revised submission received: 26 June 2012; accepted for publication:16 July 2012.doi:10.1162/COLI a 00143?
2013 Association for Computational LinguisticsComputational Linguistics Volume 39, Number 31.
IntroductionNative speakers of a language intuitively recognize different degrees of lexical contrast?for example, most people will agree that hot and cold have a higher degree of contrastthan cold and lukewarm, and cold and lukewarm have a higher degree of contrast thanpenguin and clown.
Automatically determining the degree of contrast between wordshas many uses, including: Detecting and generating paraphrases (Marton, El Kholy, and Habash2011) (The dementors caught Sirius Black / Black could not escape thedementors). Detecting certain types of contradictions (de Marneffe, Rafferty, andManning 2008; Voorhees 2008) (Kyoto has a predominantly wet climate /It is mostly dry in Kyoto).
This is in turn useful in effectively rerankingtarget language hypotheses in machine translation, and for rerankingquery responses in information retrieval. Understanding discourse structure and improving dialogue systems.Opposites often indicate the discourse relation of contrast (Marcu andEchihabi 2002). Detecting humor (Mihalcea and Strapparava 2005).
Satire and jokes tendto have contradictions and oxymorons. Distinguishing near-synonyms from word pairs that are semanticallycontrasting in automatically created distributional thesauri.
Measuresof distributional similarity typically fail to do so.Detecting lexical contrast is not sufficient by itself to solve most of these problems, butit is a crucial component.Lexicons of pairs of words that native speakers consider opposites have beencreated for certain languages, but their coverage is limited.
Opposites are of many kinds,such as antipodals, complementaries, and gradable (summarized in Section 3).
Existinglexicons often do not classify opposites into the different kinds, however.
Further, theterminology is inconsistent across different sources.
For example, Cruse (1986) definesantonyms as gradable adjectives that are opposite in meaning, whereas the WordNetantonymy link connects some verb pairs, noun pairs, and adverb pairs too.
In thisarticle, we will follow Cruse?s terminology, and we will refer to word pairs connectedby WordNet?s antonymy link as opposites, unless referring specifically to gradableadjectival pairs.Manually created lexicons also do not explicitly list word pairs that are notopposites but yet have some degree of contrast in meaning, such as warm and cold ortropical and cold.
Further, contrasting word pairs far outnumber those that are commonlyconsidered opposites.
In our own experiments described later in this article, we find thatmore than 90% of the contrasting pairs in GRE ?most contrasting word?
questions arenot listed as antonyms in WordNet.
We should not infer from this that WordNet or anyother lexicographic resource is a poor source for detecting opposites, but rather thatidentifying the large number of contrasting word pairs requires further computation,possibly relying on other semantic relations stored in the lexicographic resource.Even though a number of computational approaches have been proposed for se-mantic closeness (Curran 2004; Budanitsky and Hirst 2006), and some for hypernymy?556Mohammad et alComputing Lexical Contrasthyponymy (Hearst 1992), measures of lexical contrast have been less successful.
To someextent, this is because lexical contrast is not as well understood as other classical lexical?semantic relations.Over the years, many definitions of semantic contrast and opposites have beenproposed by linguists (Lehrer and Lehrer 1982; Cruse 1986), cognitive scientists (Kagan1984), psycholinguists (Deese 1965), and lexicographers (Egan 1984), which differ fromeach other in various respects.
Cruse (1986, page 197) observes that even though peoplehave a robust intuition of opposites, ?the overall class is not a well-defined one.?
Hepoints out that a defining feature of opposites is that they tend to have many commonproperties, but differ saliently along one dimension of meaning.
We will refer to thissemantic dimension as the dimension of opposition.
For example, giant and dwarf areboth living beings; they both eat, they both walk, they are both capable of thinking, andso on.
They are most saliently different, however, along the dimension of height.
Crusealso points out that sometimes it is difficult to identify or articulate the dimension ofopposition (for example, city?farm).Another way to define opposites is that they are word pairs with a ?binary incom-patible relation?
(Kempson 1977, page 84).
That is to say that one member entails theabsence of the other, and given one member, the identity of the other member is obvious.Thus, night and day are good examples of opposites because night is best paraphrasedby not day, rather than the negation of any other term.
On the other hand, blue and yellowmake poor opposites because even though they are incompatible, they do not have anobvious binary relation such that blue is understood to be a negation of yellow.
It shouldbe noted that there is a relation between binary incompatibility and difference along justone dimension of meaning.For this article, we define opposites to be term pairs that clearly satisfy eitherthe property of binary incompatibility or the property of salient difference across adimension of meaning.
Word pairs may satisfy the two properties to different degrees,however.
We will refer to all word pairs that satisfy either of the two properties to somedegree as contrasting.
For example, daylight and darkness are very different along thedimension of light, and they satisfy the binary incompatibity property to some degree,but not as strongly as day and night.
Thus we will consider both daylight and darkness aswell as day and night as semantically contrasting pairs (the former pair less so than thelatter), but only day and night as opposites.
Even though there are subtle differences inthe meanings of the terms contrasting, opposite, and antonym, they have often been usedinterchangeably in the literature, dictionaries, and common parlance.
Thus, we list herewhat we use these terms to mean in this article: Opposites are word pairs that have a strong binary incompatibilityrelation with each other and/or are saliently different across a dimensionof meaning. Contrasting word pairs are word pairs that have some non-zero degreeof binary incompatibility and/or have some non-zero difference acrossa dimension of meaning.
Thus, all opposites are contrasting, but not allcontrasting pairs are opposites. Antonyms are opposites that are also gradable adjectives.11 We follow Cruse?s (1986) definition for antonyms.
The WordNet antonymy link, however, also connectssome verb pairs, noun pairs, and adverb pairs.557Computational Linguistics Volume 39, Number 3In this article, we present an automatic method to identify contrasting word pairsthat is based on the following hypothesis:Contrast Hypothesis: If a pair of words, A and B, are contrasting, then there is a pair ofopposites, C and D, such that A and C are strongly related and B and D are stronglyrelated.For example, there exists the pair of opposites night and day such that darkness is relatedto night, and daylight is related to day.
We then determine the degree of contrast betweentwo words using this hypothesis:Degree of Contrast Hypothesis: If a pair of words, A and B, are contrasting, then theirdegree of contrast is proportional to their tendency to co-occur in a large corpus.For example, consider the contrasting word pairs top?low and top?down; because top anddown occur together much more often than top and low, our method concludes that thepair top?down has a higher degree of lexical contrast than the pair top?low.
The degreeof contrast hypothesis is inspired by the idea that opposites tend to co-occur more oftenthan chance (Charles and Miller 1989; Fellbaum 1995).
Murphy and Andrew (1993)claim that this is because together opposites convey contrast well, which is rhetoricallyuseful.
Thus we hypothesize that the higher the degree of contrast between two words,the higher the tendency of people to use them together.Because opposites are a key component of our method, we begin by first under-standing different kinds of opposites (Section 3).
Then we describe a crowdsourcedproject on the annotation of opposites into different kinds (Section 4).
In Section 5.1,we examine whether opposites and other highly contrasting word pairs occur togetherin text more often than randomly chosen word pairs.
This experiment is crucial tothe degree of contrast hypothesis because if our assumption is true, then we shouldfind that highly contrasting pairs are used together much more often than randomlychosen word pairs.
Section 5.2 examines this question.
Section 6 presents our methodto automatically compute the degree of contrast between word pairs by relying onthe contrast hypothesis, the degree of contrast hypothesis, seed opposites, and thestructure of a Roget-like thesaurus.
(This method was first described in Mohammad,Dorr, and Hirst [2008].)
Finally we present experiments that evaluate various aspects ofthe automatic method (Section 7).
Following is a summary of the key research questionsaddressed by this article:(1) On the kinds of opposites:Research questions: How good are humans at identifying different kindsof opposites?
Can certain term pairs belong to more than one kind ofopposite?Experiment: In Sections 3 and 4, we describe how we designed a ques-tionnaire to acquire annotations about opposites.
Because the annotationsare done by crowdsourcing, and there is no control over the educationalbackground of the annotators, we devote extra effort to make sure that thequestions are phrased in a simple, yet clear, manner.
We deploy a qualitycontrol method that uses a word-choice question to automatically identifyand discard dubious and outlier annotations.Findings: We find that humans agree markedly in identifying opposites;there is significant variation in the agreement for different kinds of558Mohammad et alComputing Lexical Contrastopposites, however.
We find that a large number of opposing word pairshave properties pertaining to more than one kind of opposite.
(2) On the manifestation of opposites and other highly contrasting pairs in text:Research questions: How often do highly contrasting word pairs co-occurin text?
How strong is this tendency compared with random word pairs,and compared with near-synonym word pairs?Experiment: Section 5 describes how we compiled sets of highly contrast-ing word pairs (including opposites), near-synonym pairs, and randomword pairs, and determine the tendency for pairs in each set to co-occur ina corpus.Findings: Highly contrasting word pairs co-occur significantly more oftenthan both the random word pairs set and also the near-synonyms set.We also find that the average distributional similarity of highly contrastingword pairs is higher than that of synonymous words.
The standard de-viations of the distributions for the high-contrast set and the synonymsset are large, however, and so the tendency to co-occur is not sufficient todistinguish highly contrasting word pairs from near-synonymous pairs.
(3) On an automatic method for computing lexical contrast:Research questions: How can the contrast hypothesis and the degreeof contrast hypothesis be used to develop an automatic method foridentifying contrasting word pairs?
How can we automatically generatethe list of opposites, which are needed as input for a method relying onthe contrast hypothesis?Proposed Method: Section 6 describes an empirical method for deter-mining the degree of contrast between two words by using the contrasthypothesis, the degree of contrast hypothesis, the structure of a thesaurus,and seed opposite pairs.
The use of affixes to generate seed opposite pairsis also described.
(This method was first proposed in Mohammad, Dorr,and Hirst [2008].
)(4) On the evaluation of automatic methods of contrast:Research questions: How accurate are automatic methods at identifyingwhether one word pair has a higher degree of contrast than another?
Whatis the accuracy of this method in detecting opposites (a notable subset ofthe contrasting pairs)?
How does this accuracy vary for different kinds ofopposites?2 How easy is it for automatic methods to distinguish betweenopposites and synonyms?
How does the proposed method perform whencompared with other automatic methods?Experiments: We conduct three experiments (described in Sections 7.1,7.2, and 7.3) involving three different data sets and two tasks to answer2 Note that though linguists have classified opposites into different kinds, we know of no work doing sofor contrasts more generally.
Thus this particular analysis must be restricted to opposites alone.559Computational Linguistics Volume 39, Number 3these questions.
We compare performance of our method with methodsproposed by Lin et al(2003) and Turney (2008).
We automaticallygenerate a new set of 1,296 ?most contrasting word?
questions to evaluateperformance of our method on five different kinds of opposites and acrossfour parts of speech.
(The evaluation described in Section 7.1 was firstdescribed in Mohammad, Dorr, and Hirst [2008].
)Findings: We find that the proposed measure of lexical contrast obtainshigh precision and large coverage, outperforming existing methods.Our method performs best on gradable pairs, antipodal pairs, and com-plementary pairs, but poorly on disjoint opposite pairs.
Among differentparts of speech, the method performs best on noun pairs, and relativelyworse on verb pairs.All of the data created and compiled as part of this research are summarized in Table 18(Section 8), and is available for download.32.
Related WorkCharles and Miller (1989) proposed that opposites occur together in a sentence moreoften than chance.
This is known as the co-occurrence hypothesis.
Paradis, Willners,and Jones (2009) describe further experiments to show how canonical opposites tendto have high textual co-occurrence.
Justeson and Katz (1991) gave evidence in supportof the hypothesis using 35 prototypical opposites (from an original set of 39 oppositescompiled by Deese [1965]) and also with an additional 22 frequent opposites.
They alsoshowed that opposites tend to occur in parallel syntactic constructions.
All of thesepairs were adjectives.
Fellbaum (1995) conducted similar experiments on 47 noun, verb,adjective, and adverb pairs (noun?noun, noun?verb, noun?adjective, verb?adverb, etc.
)pertaining to 18 concepts (for example, lose(v)?gain(n) and loss(n)?gain(n), where lose(v)and loss(n) pertain to the concept of ?failing to have/maintain?).
Non-opposite semanti-cally related words also tend to occur together more often than chance, however.
Thus,separating opposites from these other classes has proven to be difficult.Some automatic methods of lexical contrast rely on lexical patterns in text.
Forexample, Lin et al(2003) used patterns such as ?from X to Y ?
and ?either X or Y ?to separate opposites from distributionally similar pairs.
They evaluated their methodon 80 pairs of opposites and 80 pairs of synonyms taken from the Webster?s CollegiateThesaurus (Kay 1988).
The evaluation set of 160 word pairs was chosen such that itincluded only high-frequency terms.
This was necessary to increase the probabilityof finding sentences in a corpus where the target pair occurred in one of the chosenpatterns.
Lobanova, van der Kleij, and Spenader (2010) used a set of Dutch adjectiveseed pairs to learn lexical patterns commonly containing opposites.
The patterns werein turn used to create a larger list of Dutch opposites.
The method was evaluated bycomparing entries to Dutch lexical resources and by asking human judges to determinewhether an automatically found pair is indeed an opposite.
Turney (2008) proposed asupervised method for identifying synonyms, opposites, hypernyms, and other lexical-semantic relations between word pairs.
The approach learns patterns corresponding todifferent relations.3 http://www.purl.org/net/saif.mohammad/research.560Mohammad et alComputing Lexical ContrastHarabagiu, Hickl, and Lacatusu (2006) detected contrasting word pairs for thepurpose of identifying contradictions by using WordNet chains?synsets connected bythe hypernymy?hyponymy links and exactly one antonymy link.
Lucerto, Pinto, andJimen?ez-Salazar (2002) proposed detecting contrasting word pairs using the numberof tokens between two words in text and also cue words such as but, from, and and.Unfortunately, they evaluated their method on only 18 word pairs.
Neither Harabagiu,Hickl, and Lacatusu nor Lucerto, Pinto, and Jime?nez-Salazar determined the degree ofcontrast between words, and their methods have not been shown to have substantialcoverage.Schwab, Lafourcade, and Prince (2002) created an oppositeness vector for a targetword.
The closer this vector is to the context vector of the other target word, themore opposite the two target words are.
The oppositeness vectors were created by firstmanually identifying possible opposites and then generating suitable vectors for eachusing dictionary definitions.
The approach was evaluated on only a handful of wordpairs.There is a large amount of work on sentiment analysis and opinion mining aimedat determining the polarity of words (Pang and Lee 2008).
For example, Pang, Lee, andVaithyanathan (2002) detected that adjectives such as dazzling, brilliant, and gripping casttheir qualifying nouns positively whereas adjectives such as bad, cliched, and boringportray the noun negatively.
Many of these gradable adjectives have opposites, butthese approaches, with the exception of that of Hatzivassiloglou and McKeown (1997),did not attempt to determine pairs of positive and negative polarity words that areopposites.
Hatzivassiloglou and McKeown proposed a supervised algorithm that usesword usage patterns to generate a graph with adjectives as nodes.
An edge betweentwo nodes indicates either that the two adjectives have the same or opposite polarity.
Aclustering algorithm then partitions the graph into two subgraphs such that the nodes ina subgraph have the same polarity.
They used this method to create a lexicon of positiveand negative words, and argued that the method could also be used to detect opposites.3.
The Heterogeneous Nature of OppositesOpposites, unlike synonyms, can be of different kinds.
Many different classificationshave been proposed, one of which is given by Cruse (1986) (Chapters 9, 10, and 11).It consists of complementaries (open?shut, dead?alive), antonyms (long?short, slow?fast) (further classified into polar, overlapping, and equipollent opposites), directionalopposites (up?down, north?south) (further classified into antipodals, counterparts, andreversives), relational opposites (husband?wife, predator?prey), indirect converses (give?receive, buy?pay), congruence variants (huge?little, doctor?patient), and pseudo opposites(black?white).Various lexical relations have also received attention at the Educational TestingServices, as analogies and ?most contrasting word?
questions are part of the tests theyconduct.
They classify opposites into contradictories (alive?dead, masculine?feminine),contraries (old?young, happy-sad), reverses (attack?defend, buy?sell), directionals ( front?back, left?right), incompatibles (happy?morbid, frank?hypocritical), asymmetric contraries(hot?cool, dry?moist), pseudo-opposites (popular?shy, right?bad), and defectives (default?payment, limp?walk) (Bejar, Chaffin, and Embretson 1991).Keeping in mind the meanings and subtle distinctions between each of these kindsof opposites is not easy even if we provide extensive training to annotators.
Becausewe crowdsource the annotations, and we know that Turkers prefer to spend theirtime doing the task (and making money) rather than reading lengthy descriptions, we561Computational Linguistics Volume 39, Number 3focused only on five kinds of opposites that we believed would be easiest to annotate,and which still captured a majority of the opposites: Antipodals (top?bottom, start?finish): Antipodals are opposites in which?one term represents an extreme in one direction along some salient axis,while the other term denotes the corresponding extreme in the otherdirection?
(Cruse 1986, page 225). Complementaries (open?shut, dead?alive): The essential characteristic of apair of complementaries is that ?between them they exhaustively dividethe conceptual domain into two mutually exclusive compartments, so thatwhat does not fall into one of the compartments must necessarily fall intothe other?
(Cruse 1986, page 198). Disjoint (hot?cold, like?dislike): Disjoint opposites are word pairs thatoccupy non-overlapping regions in the semantic dimension such that thereare regions not covered by either term.
This set of opposites includesequipollent adjective pairs (for example, hot?cold) and stative verb pairs(for example, like?dislike).
We refer the reader to Sections 9.4 and 9.7 ofCruse (1986) for details about these sub-kinds of opposites. Gradable opposites (long?short, slow?fast): are adjective-pair oradverb-pair opposites that are gradable, that is, ?members of the pairdenote degrees of some variable property such as length, speed, weight,accuracy, etc.?
(Cruse 1986, page 204). Reversibles (rise?fall, enter?exit): Reversibles are opposite verb pairs suchthat ?if one member denotes a change from A to B, its reversive partnerdenotes a change from B to A?
(Cruse 1986, page 226).It should be noted that there is no agreed-upon number of kinds of opposites.
Differentresearchers have proposed various classifications that overlap to a greater or lesserdegree.
It is possible that for a certain application or study one may be interested ina kind of opposite not listed here.4.
CrowdsourcingWe used the Amazon Mechanical Turk (AMT) service to obtain annotations for differentkinds of opposites.
We broke the task into small independently solvable units calledHITs (Human Intelligence Tasks) and uploaded them on the AMT Web site.4 Each HIThad a set of questions, all of which were to be answered by the same person (a Turker, inAMT parlance).
We created HITs for word pairs, taken from WordNet, that we expectedto have some degree of contrast in meaning.In WordNet, words that are close in meaning are grouped together in a set called asynset.
If one of the words in a synset is an opposite of another word in a different synset,then the two synsets are called head synsets and WordNet records the two words asdirect antonyms (Gross, Fischer, and Miller 1989)?WordNet regards the terms oppositeand antonym as synonyms.
Other word pairs across the two head synsets are calledindirect antonyms.
Because we follow Cruse?s definition of antonyms, which requires4 https://www.mturk.com/mturk/welcome.562Mohammad et alComputing Lexical ContrastTable 1Target word pairs chosen for annotation.
Each term was annotated about eight times.part of speech # of word pairsadverbs 185adjectives 646nouns 416verbs 309all 1,556antonyms to be gradable adjectives, and because WordNet?s direct antonyms includenoun, verb, and adverb pairs too, for the rest of the article we will refer to WordNetdirect antonyms as direct opposites and WordNet indirect antonyms as indirect op-posites.
We will refer to the union of both the direct and indirect opposites simply asWordNet opposites.
Note that the WordNet opposites are highly contrasting term pairs.We chose as target pairs all the direct or indirect opposites from WordNet that werealso listed in the Macquarie Thesaurus.
This condition was a mechanism to ignore less-frequent and obscure words, and apply our resources on words that are more common.Additionally, as we will describe subsequently, we use the presence of the words inthe thesaurus to help generate Question 1, which we use for quality control of theannotations.
Table 1 gives a breakdown of the 1,556 pairs chosen by part of speech.Because we do not have any control over the educational background of theannotators, we made efforts to phrase questions about the kinds of opposites in a simpleand clear manner.
Therefore we avoided definitions and long instructions in favor ofexamples and short questions.
We believe this strategy is beneficial even in traditionalannotation scenarios.We created separate questionnaires (HITs) for adjectives, adverbs, nouns, andverbs.
A complete example adjective HIT with directions and questions is shownin Figure 1.
The adverb, noun, and verb questionnaires had similar questions, butwere phrased slightly differently to accommodate differences in part of speech.
Thesequestionnaires are not shown here due to lack of space, but all four questionnaires areavailable for download.5 The verb questionnaire had an additional question, shownin Figure 2.
Because nouns and verbs are not considered gradable, the correspondingquestionnaires did not have Q8 and Q9.
We requested annotations from eight differentTurkers for each HIT.4.1 The Word Choice Question: Q1Q1 is an automatically generated word choice question that has a clear correct answer.
Ithelps identify outlier and malicious annotations.
If this question is answered incorrectly,then we assume that the annotator does not know the meanings of the target words, andwe ignore responses to the remaining questions.
Further, as this question makes theannotator think about the meanings of the words and about the relationship betweenthem, we believe it improves the responses for subsequent questions.The options for Q1 were generated automatically.
Each option is a set of fourcomma-separated words.
The words in the answer are close in meaning to both of thetarget words.
In order to create the answer option, we first generated a much larger5 http://www.purl.org/net/saif.mohammad/research.563Computational Linguistics Volume 39, Number 3Word-pair: musical ?
dissonantQ1.
Which set of words is most related to the word pair musical:dissonant? useless, surgery, ineffectual, institution sequence, episode, opus, composition youngest, young, youthful, immature consequential, important, importance, heavyQ2.
Do musical and dissonant have some contrast in meaning? yes  noFor example, up?down, lukewarm?cold, teacher?student, attack?defend, all have at leastsome degree of contrast in meaning.
On the other hand, clown?down, chilly?cold,teacher?doctor, and attack?rush DO NOT have contrasting meanings.Q3.
Some contrasting words are paired together so often that given one we naturallythink of the other.
If one of the words in such a pair were replaced with another word ofalmost the same meaning, it would sound odd.
Are musical:dissonant such a pair? yes  noExamples for ?yes?
: tall?short, attack?defend, honest?dishonest, happy?sad.Examples for ?no?
: tall?stocky, attack?protect, honest?liar, happy?morbid.Q5.
Do musical and dissonant represent two ends or extremes? yes  noExamples for ?yes?
: top?bottom, basement?attic, always?never, all?none, start?finish.Examples for ?no?
: hot?cold (boiling refers to more warmth than hot and freezing refersto less warmth than cold), teacher?student (there is no such thing as more or less teacherand more or less student), always?sometimes (never is fewer times than sometimes).Q6.
If something is musical, would you assume it is not dissonant, and vice versa?In other words, would it be unusual for something to be both musical and dissonant? yes  noExamples for ?yes?
: happy?sad, happy?morbid, vigilant?careless, slow?stationary.Examples for ?no?
: happy?calm, stationary?still, vigilant?careful, honest?truthful.Q7.
If something or someone could possibly be either musical or dissonant, is itnecessary that it must be either musical or dissonant?
In other words, is it true that forthings that can be musical or dissonant, there is no third possible state, except perhapsunder highly unusual circumstances? yes  noExamples for ?yes?
: partial?impartial, true?false, mortal?immortal.Examples for ?no?
: hot?cold (an object can be at room temperature is neither hot nor cold),tall?short (a person can be of medium or average height).Q8.
In a typical situation, if two things or two people are musical, then can one be moremusical than the other? yes  noExamples for ?yes?
: quick, exhausting, loving, costly.Examples for ?no?
: dead, pregnant, unique, existent.Q9.
In a typical situation, if two things or two people are dissonant, can one be moredissonant than the other? yes  noExamples for ?yes?
: quick, exhausting, loving, costly, beautiful.Examples for ?no?
: dead, pregnant, unique, existent, perfect, absolute.Figure 1Example HIT: Adjective pairs questionnaire.Note: Perhaps ?musical ?
dissonant?
might be better written as ?musical versus dissonant,?
but we havekept ???
here to show the reader exactly what the Turkers were given.Note: Q4 is not shown here, but can be seen in the on-line version of the questionnaire.
It was an exploratoryquestion, and it was not multiple choice.
Q4?s responses have not been analyzed.564Mohammad et alComputing Lexical ContrastWord-pair: enabling ?
disablingQ10.
In a typical situation, do the sequence of actions disabling and then enabling bringsomeone or something back to the original state, AND do the sequence of actions enablingand disabling also bring someone or something back to the original state? yes, both ways: the transition back to the initial state makes much sense in bothsequences. yes, but only one way: the transition back to the original state makes much more senseone way than the other way. none of the aboveExamples for ?yes, both ways?
: enter?exit, dress?undress, tie?untie, appear?disappear.Examples for ?yes, but only one way?
: live?die, create?destroy, damage?repair, kill?resurrect.Examples for ?none of the above?
: leave?exit, teach?learn, attack?defend (attacking and thendefending does not bring one back to the original state).Figure 2Additional question in the questionnaire for verbs.source pool of all the words that were in the same thesaurus category as any of the twotarget words.
(Words in the same category are closely related.)
Words that had the samestem as either of the target words were discarded.
For each of the remaining words, weadded their Lesk similarities with the two target words (Banerjee and Pedersen 2003).The four words with the highest sum were chosen to form the answer option.The three distractor options were randomly selected from the pool of correct an-swers for all other word choice questions.
Finally, the answer and distractor optionswere presented to the Turkers in random order.4.2 Post-ProcessingThe response to a HIT by a Turker is called an assignment.
We obtained about 12,448assignments in all (1,556 pairs ?
8 assignments each).
About 7% of the adjective, adverb,and noun assignments and about 13% of the verb assignments had an incorrect answerto Q1.
These assignments were discarded, leaving 1,506 target pairs with three or morevalid assignments.
We will refer to this set of assignments as the master set, and allfurther analysis in this article is based on this set.
Table 2 gives a breakdown of theaverage number of annotations for each of the target pairs in the master set.4.3 Prevalence of Different Kinds of Contrasting PairsFor each question pertaining to every word pair in the master set, we determined themost frequent response by the annotators.
Table 3 gives the percentage of word pairs inTable 2Number of word pairs and average number of annotations per word pair in the master set.part of # of average # ofspeech word pairs annotationsadverbs 182 7.80adjectives 631 8.32nouns 405 8.44verbs 288 7.58all 1,506 8.04565Computational Linguistics Volume 39, Number 3Table 3Percentage of word pairs that received a response of ?yes?
for the questions in the questionnaire.adj.
= adjectives; adv.
= adverbs.% of word pairsQuestion answer adj.
adv.
nouns verbsQ2.
Do X and Y have some contrast?
yes 99.5 96.8 97.6 99.3Q3.
Are X and Y opposites?
yes 91.2 68.6 65.8 88.8Q5.
Are X and Y at two ends of a dimension?
yes 81.8 73.5 81.1 94.4Q6.
Does X imply not Y?
yes 98.3 92.3 89.4 97.5Q7.
Are X and Y mutually exhaustive?
yes 85.1 69.7 74.1 89.5Q8.
Does X represent a point on some scale?
yes 78.5 77.3 ?
?Q9.
Does Y represent a point on some scale?
yes 78.5 70.8 ?
?Q10.
Does X undo Y OR does Y undo X?
one way ?
?
?
3.8both ways ?
?
?
90.9Table 4Percentage of WordNet source pairs that are contrasting, opposite, and ?contrasting but notopposite.
?category basis adj.
adv.
nouns verbscontrasting Q2 yes 99.5 96.8 97.6 99.3opposites Q2 yes and Q3 yes 91.2 68.6 60.2 88.9contrasting, but not opposite Q2 yes and Q3 no 8.2 28.2 37.4 10.4the master set that received a most frequent response of ?yes.?
The first column in thetable lists the question number followed by a brief description of question.
(Note thatthe Turkers saw only the full forms of the questions, as shown in the example HIT.
)Observe that most of the word pairs are considered to have at least some contrastin meaning.
This is not surprising because the master set was constructed using wordsconnected through WordNet?s antonymy relation.6 Responses to Q3 show that not allcontrasting pairs are considered opposite, and this is especially the case for adverbpairs and noun pairs.
The rows in Table 4 show the percentage of words in the masterset that are contrasting (row 1), opposite (row 2), and contrasting but not opposite(row 3).Responses to Q5, Q6, Q7, Q8, and Q9 (Table 3) show the prevalence of differentkinds of relations and properties of the target pairs.Table 5 shows the percentage of contrasting word pairs that may be classified intothe different types discussed in Section 3.
Observe that rows for all categories otherthan the disjoints have percentages greater than 60%.
This means that a number ofcontrasting word pairs can be classified into more than one kind.
Complementariesare the most common kind in case of adverbs, nouns, and verbs, whereas antipodalsare most common among adjectives.
A majority of the adjective and adverb contrastingpairs are gradable, but more than 30% of the pairs are not.
Most of the verb pairs arereversives (91.6%).
Disjoint pairs are much less common than all the other categories6 All of the direct antonyms were marked as contrasting by the Turkers.
Only a few indirect antonymswere marked as not contrasting.566Mohammad et alComputing Lexical ContrastTable 5Percentage of contrasting word pairs belonging to various subtypes.
The subtype ?reversives?applies only to verbs.
The subtype ?gradable?
applies only to adjectives and adverbs.subtype basis adv.
adj.
nouns verbsAntipodals Q2 yes, Q5 yes 82.3 75.9 82.5 95.1Complementaries Q2 yes, Q7 yes 85.6 72.0 84.8 98.3Disjoint Q2 yes, Q7 no 14.4 28.0 15.2 1.7Gradable Q2 yes, Q8 yes, Q9 yes 69.6 66.4 - -Reversives Q2 yes, Q10 both ways - - - 91.6considered, and they are most prominent among adjectives (28%), and least among verbpairs (1.7%).4.4 AgreementPeople do not always agree on linguistic classifications of terms, and one of the goals ofthis work was to determine how much people agree on properties relevant to differentkinds of opposites.
Table 6 lists the breakdown of agreement by target-pair part ofspeech and question, where agreement is the average percentage of the number ofTurkers giving the most-frequent response to a question?the higher the numberof Turkers that vote for the majority answer, the higher is the agreement.Observe that agreement is highest when asked whether a word pair has somedegree of contrast in meaning (Q2), and that there is a marked drop when asked if thetwo words are opposites (Q3).
This is true for each of the parts of speech, although thedrop is highest for verbs (94.7% to 75.2%).For Questions 5 through 9, we see varying degrees of agreement?Q6 obtaining thehighest agreement and Q5 the lowest.
There is marked difference across parts of speechfor certain questions.
For example, verbs are the easiest to identify (highest agreementfor Q5, Q7, and Q8).
For Q6, nouns have markedly lower agreement than all other partsof speech?not surprising considering that the set of disjoint opposites is traditionallyassociated with equipollent adjectives and stative verbs.
Adverbs and adjectives havemarkedly lower agreement scores for Q7 than nouns and verbs.Table 6Breakdown of answer agreement by target-pair part of speech and question: For every targetpair, a question is answered by about eight annotators.
The majority response is chosen as theanswer.
The ratio of the size of the majority and the number of annotators is indicative of theamount of agreement.
The table shows the average percentage of this ratio.question adj.
adv.
nouns verbs averageQ2.
Do X and Y have some contrast?
90.7 92.1 92.0 94.7 92.4Q3.
Are X and Y opposites?
79.0 80.9 76.4 75.2 77.9Q5.
Are X and Y at two ends of a dimension?
70.3 66.5 73.0 78.6 72.1Q6.
Does X imply not Y?
89.0 90.2 81.8 88.4 87.4Q7.
Are X and Y mutually exhaustive?
70.4 69.2 78.2 88.3 76.5average (Q2, Q3, Q5, Q6, and Q7) 82.3 79.8 80.3 85.0 81.3Q8.
Does X represent a point on some scale?
77.9 71.5 ?
?
74.7Q9.
Does Y represent a point on some scale?
75.2 72.0 ?
?
73.6Q10.
Does X undo Y OR does Y undo X?
?
?
?
73.0 73.0567Computational Linguistics Volume 39, Number 35.
Manifestation of Highly Contrasting Word Pairs in TextAs pointed out earlier, there is work on a small set of opposites showing that oppositesco-occur more often than chance (Charles and Miller 1989; Fellbaum 1995).
Section 5.1describes experiments on a larger scale to determine whether highly contrasting wordpairs (including opposites) occur together more often than randomly chosen wordpairs of similar frequency.
The section also compares co-occurrence associations withsynonyms.Research in distributional similarity has found that entries in distributional thesauritend to also contain terms that are opposite in meaning (Lin 1998; Lin et al2003).Section 5.2 describes experiments to determine whether highly contrasting word pairs(including opposites) occur in similar contexts as often as randomly chosen pairs ofwords with similar frequencies, and whether highly contrasting words occur in similarcontexts as often as synonyms.5.1 Co-OccurrenceIn order to compare the tendencies of highly contrasting word pairs, synonyms, andrandom word pairs to co-occur in text, we created three sets of word pairs: the high-contrast set, the synonyms set, and the control set of random word pairs.
The high-contrast setwas created from a pool of direct and indirect opposites (nouns, verbs, and adjectives)from WordNet.
We discarded pairs that did not meet the following conditions: (1) bothmembers of the pair must be unigrams, (2) both members of the pair must occur inthe British National Corpus (BNC) (Burnard 2000), and (3) at least one member of thepair must have a synonym in WordNet.
A total of 1,358 word pairs remained, and theseform the high-contrast set.Each of the pairs in the high-contrast set was used to create a synonym pair bychoosing a WordNet synonym of exactly one member of the pair.7 If a word has morethan one synonym, then the most frequent synonym is chosen.8 These 1,358 word pairsform the synonyms set.
Note that for each of the pairs in the high-contrast set, there is acorresponding pair in the synonyms set, such that the two pairs have a common term.For example, the pair agitation and calmness in the high-contrast set has a correspondingpair agitation and ferment in the synonyms set.
We will refer to the common terms(agitation in this example) as the focus words.
Because we also wanted to compareoccurrence statistics of the high-contrast set with the random pairs set, we created thecontrol set of random pairs by taking each of the focus words and pairing them withanother word in WordNet that has a frequency of occurrence in BNC closest to the termcontrasting with the focus word.
This is to ensure that members of the pairs across thehigh-contrast set and the control set have similar unigram frequencies.We calculated the pointwise mutual information (PMI) (Church and Hanks 1990)for each of the word pairs in the high-contrast set, the random pairs set, and thesynonyms set using unigram and co-occurrence frequencies in the BNC.
If two wordsoccurred within a window of five adjacent words in a sentence, they were marked asco-occurring (same window as Church and Hanks [1990] used in their seminal work onword?word associations).
Table 7 shows the average and standard deviation in each set.7 If both members of a pair have WordNet synonyms, then one is chosen at random, and its synonym istaken.8 WordNet lists synonyms in order of decreasing frequency in the SemCor corpus.568Mohammad et alComputing Lexical ContrastTable 7Pointwise mutual information (PMI) of word pairs.
High positive values imply a tendency toco-occur in text more often than random chance.average PMI standard deviationhigh-contrast set 1.471 2.255random pairs set 0.032 0.236synonyms set 0.412 1.110Observe that the high-contrast pairs have a much higher tendency to co-occur than therandom pairs control set, and also the synonyms set.
The high-contrast set has a largestandard deviation, however.
A two-sample t-test revealed that the high-contrast setis significantly different from the random set (p < 0.05), and also that the high-contrastset is significantly different from the synonyms set (p < 0.05).On average, however, the PMI between a focus word and its contrasting term waslower than the PMI between the focus word and 3,559 other words in the BNC.
Thesewere often words related to the focus words, but neither contrasting nor synonymous.Thus, even though a high tendency to co-occur is a feature of highly contrasting pairs,it is not a sufficient condition for detecting them.
We use PMI as part of our methodfor determining the degree of lexical contrast (described in Section 6).5.2 Distributional SimilarityCharles and Miller (1989) proposed that in most contexts, opposites may be inter-changed.
The meaning of the utterance will be inverted, of course, but the sentencewill remain grammatical and linguistically plausible.
This came to be known as thesubstitutability hypothesis.
Their experiments did not support this claim, however.
Theyfound that given a sentence with the target adjective removed, most people did notconfound the missing word with its opposite.
Justeson and Katz (1991) later showedthat in sentences that contain both members of an adjectival opposite pair, the targetadjectives do indeed occur in similar syntactic structures at the phrasal level.
Jones et al(2007) show how the tendency to appear in certain textual constructions such as ?fromX to Y?
and ?either X or Y?
are indicative of prototypicalness of opposites.
Thus, we canformulate the distributional hypothesis of highly contrasting pairs: highly contrastingpairs occur in similar contexts more often than non-contrasting word pairs.We used the same sets of high-contrast pairs, synonyms, and random pairs de-scribed in the previous section to gather empirical proof of the distributional hypothesis.We calculated the distributional similarity between each pair in the three sets usingLin?s (1998) measure.
Table 8 shows the average and standard deviation in each set.Observe that the high-contrast set has a much higher average distributional similarityTable 8Distributional similarity of word pairs.
The measure proposed in Lin (1998) was used.average distributional similarity standard deviationopposites set 0.064 0.071random pairs set 0.036 0.034synonyms set 0.056 0.057569Computational Linguistics Volume 39, Number 3than the random pairs control set, and interestingly it is also higher than the synonymsset.
Once again, the high-contrast set has a large standard deviation.
A two-samplet-test revealed that the high-contrast set is significantly different from both the randomset and the synonyms set with a confidence interval of 0.05.
This demonstrates thatrelative to other word pairs, high-contrast pairs tend to occur in similar contexts.
Wealso find that the synonyms set has a significantly higher distributional similarity thanthe random pairs set (p< 0.05).
This shows that near-synonymous word pairs also occurin similar contexts (the distributional hypothesis of similarity).
Further, a consequenceof the large standard deviations in the cases of both high-contrast pairs and synonymsmeans that distributional similarity alone is not sufficient to determine whether twowords are contrasting or synonymous.
An automatic method for recognizing contrastwill require additional cues.
Our method uses PMI and other sources of informationdescribed in the next section.6.
Computing Lexical ContrastIn this section, we recapitulate the automatic method for determining lexical con-trast that we first proposed in Mohammad, Dorr, and Hirst (2008).
Additional detailsare provided regarding the lexical resources used (Section 6.1) and the method itself(Section 6.2).6.1 Lexical ResourcesOur method makes use of a published thesaurus and co-occurrence information fromtext.
Optionally, it can use opposites listed in WordNet if available.
We briefly describethese resources here.6.1.1 Published Thesauri.
Published thesauri, such as Roget?s and Macquarie, divide thevocabulary of a language into about a thousand categories.
Words within a category aresemantically related to each other, and they tend to pertain to a coarse concept.
Eachcategory is represented by a category number (unique ID) and a head word?a word thatbest represents the meanings of the words in the category.
One may also find oppositesin the same category, but this is rare.
Words with more than one meaning may be foundin more than one category; these represent its coarse senses.Within a category, the words are grouped into finer units called paragraphs.
Wordsin the same paragraph are closer in meaning than those in differing paragraphs.
Eachparagraph has a paragraph head?a word that best represents the meaning of the wordsin the paragraph.
Words in a thesaurus paragraph belong to the same part of speech.
Athesaurus category may have multiple paragraphs belonging to the same part of speech.For example, a category may have three noun paragraphs, four verb paragraphs, andone adjective paragraph.
We will take advantage of the structure of the thesaurus inour approach.6.1.2 WordNet.
As mentioned earlier, WordNet encodes certain opposites.
We foundin our experiments (Section 7, subsequently) that more than 90% of contrasting pairsincluded in Graduate Record Examination (GRE) ?most contrasting word?
questionsare not encoded in WordNet, however.
Also, neither WordNet nor any other manuallycreated repository of opposites provides the degree of contrast between word pairs.Nevertheless, we investigate the usefulness of WordNet as a source of seed oppositesfor our approach.570Mohammad et alComputing Lexical Contrast6.2 Proposed Measure of Lexical ContrastOur method for determining lexical contrast has two parts: (1) determining whetherthe target word pair is contrasting or not, and (2) determining the degree of contrastbetween the words.6.2.1 Detecting Whether a Target Word Pair is Contrasting.
We use the contrast hypothesisto determine whether two words are contrasting.
The hypothesis is repeated here:Contrast Hypothesis: If a pair of words, A and B, are contrasting, then there is a pair ofopposites, C and D, such that A and C are strongly related and B and D are stronglyrelated.Even if a few exceptions to this hypothesis are found (we are not aware of any), thehypothesis would remain useful for practical applications.
We first determine pairs ofthesaurus categories that have at least one word in each category that are opposites ofeach other.
We will refer to these categories as contrasting categories and the oppositeconnecting the two categories as the seed opposite.
Because each thesaurus categoryis a collection of closely related terms, all of the word pairs across two contrastingcategories satisfy the contrast hypothesis, and they are considered to be contrastingword pairs.
Note also that words within a thesaurus category may belong to differentparts of speech, and they may be related to the seed opposite word through any ofthe many possible semantic relations.
Thus a small number of seed opposites canhelp identify a large number of contrasting word pairs.
We determine whether twocategories are contrasting using the three methods described here, which may be usedalone or in combination with each other:Method 1: Using word pairs generated from affix patterns.Opposites such as hot?cold and dark?light occur frequently in text, but in terms of type-pairs they are outnumbered by those created using affixes, such as un- (clear?unclear)and dis- (honest?dishonest).
Further, this phenomenon is observed in most languages(Lyons 1977).Table 9 lists 15 affix patterns that tend to generate opposites in English.
Theywere compiled by the first author by examining a small list of affixes for the Englishlanguage.9 These patterns were applied to all words in the thesaurus that are at leastthree characters long.
If the resulting term was also a valid word in the thesaurus, thenthe word pair was added to the affix-generated seed set.
These fifteen rules generated 2,682word pairs when applied to the words in the Macquarie Thesaurus.
Category pairs thathad these opposites were marked as contrasting.
Of course, not all of the word pairsgenerated through affixes are truly opposites, for example sect?insect and part?impart.For now, such pairs are sources of error in the system.
Manual analysis of these 2,682word pairs can help determine whether this error is large or small.
(We have releasedthe full set of word pairs.)
Evaluation results (Section 7) indicate that these seed pairsimprove the overall accuracy of the system, however.Figure 3 presents such an example pair.
Observe that categories 360 and 361 havethe words cover and uncover, respectively.
Affix pattern 8 from Table 9 produces seed9 http://www.englishclub.com/vocabulary/prefixes.htm.571Computational Linguistics Volume 39, Number 3Table 9Fifteen affix patterns used to generate opposites.
Here ?X?
stands for any sequence of letterscommon to both words w1 and w2.affix patternpattern # word 1 word 2 # word pairs example pair1 X antiX 41 clockwise?anticlockwise2 X disX 379 interest?disinterest3 X imX 193 possible?impossible4 X inX 690 consistent?inconsistent5 X malX 25 adroit?maladroit6 X misX 142 fortune?misfortune7 X nonX 72 aligned?nonaligned8 X unX 833 biased?unbiased9 lX illX 25 legal?illegal10 rX irrX 48 regular?irregular11 imX exX 35 implicit?explicit12 inX exX 74 introvert?extrovert13 upX downX 22 uphill?downhill14 overX underX 52 overdone?underdone15 Xless Xful 51 harmless?harmfulTotal: 2,682pair cover?uncover, and so the system concludes that the two categories have contrastingmeaning.
The contrast in meaning is especially strong for the paragraphs cover andexpose because words within these paragraphs are very close in meaning to cover anduncover, respectively.
We will refer to such thesaurus paragraph pairs that have oneword each of a seed pair as prime contrasting paragraphs.
We expect the words acrossprime contrasting paragraphs to have a high degree of antonymy (for example, maskand bare), whereas words across other contrasting category paragraphs may have asmaller degree of antonymy as the meaning of these words may diverge significantlyfrom the meanings of the words in the prime contrasting paragraphs (for example,white lie and disclosure).Method 2: Using opposites from WordNet.We compiled a list of 20,611 pairs that WordNet records as direct and indirect opposites.
(Recall discussion in Section 4 about direct and indirect opposites.)
A large number ofthese pairs include multiword expressions.
Only 10,807 of the 20,611 pairs have bothwords in the Macquarie Thesaurus?the vocabulary used for our experiments.
We willrefer to them as the WordNet seed set.
Category pairs that had these opposites weremarked as contrasting.Method 3: Using word pairs in adjacent thesaurus categories.Most published thesauri, such as Roget?s, are organized such that categories correspond-ing to opposing concepts are placed adjacent to each other.
For example, in the MacquarieThesaurus: category 369 is about honesty and category 370 is about dishonesty; as shownin Figure 3, category 360 is about hiding and category 361 is about revealing.
There area number of exceptions to this rule, and often a category may be contrasting in meaningto several other categories.
Because this was an easy enough heuristic to implement,however, we investigated the usefulness of considering adjacent thesaurus categories572Mohammad et alComputing Lexical ContrastFigure 3Example contrasting category pair.
The system identifies the pair to be contrasting through theaffix-based seed pair cover?uncover.
The paragraphs of cover and expose are referred to as primecontrasting paragraphs.
Paragraph heads are shown in bold italic.as contrasting.
We will refer to this as the adjacency heuristic.
Note that this method ofdetermining contrasting categories does not explicitly identify a seed opposite, but onecan assume the head words of these category pairs as the seed opposites.To determine how accurate the adjacency heuristic is, the first author manuallyinspected adjacent thesaurus categories in the Macquarie Thesaurus to determine whichof them were indeed contrasting.
Because a category, on average, has about a hundredwords, the task was made less arduous by representing each category by just the firstten words listed in it.
This way it took only about five hours to manually determine that209 pairs of the 811 adjacent Macquarie category pairs were contrasting.
Twice, it wasfound that category number X was contrasting not just to category number X+1 but alsoto category number X+2: category 40 (ARISTOCRACY) has a meaning that contrasts thatof category 41 (MIDDLE CLASS) as well as category 42 (WORKING CLASS); category 542(PAST) contrasts with category 543 (PRESENT) as well as category 544 (FUTURE).
Boththese X ?
(X+2) pairs are also added to the list of manually annotated contrastingcategories.6.2.2 Computing the Degree of Contrast Between Two Words.
Charles and Miller (1989)and Fellbaum (1995) argued that opposites tend to co-occur more often than randomchance.
Murphy and Andrew (1993) claimed that the greater-than-chance co-occurrenceof opposites is because together they convey contrast well, which is rhetorically useful.We showed earlier in Section 5.1 that highly contrasting pairs (including opposites)co-occur more often than randomly chosen pairs.
All of these support the degree ofcontrast hypothesis stated earlier in the introduction:Degree of Contrast Hypothesis: If a pair of words, A and B, are contrasting, then theirdegree of contrast is proportional to their tendency to co-occur in a large corpus.573Computational Linguistics Volume 39, Number 3We used PMI to capture the tendency of word?word co-occurrence.
We collectedthese co-occurrence statistics from the Google n-gram corpus (Brants and Franz 2006),which was created from a text collection of over one trillion words.
Words that occurredwithin a window of five words were considered to be co-occurring.We expected that some features may be more accurate than others.
If multiplefeatures give evidence towards opposing information, then it is useful for the systemto know which feature is more reliable.
Therefore, we held out some data from theevaluation data described in Section 7.1 as the development set.
Experiments on thedevelopment set showed that contrasting words may be placed in three bins corre-sponding to the amount of reliability of the source feature: high, medium, or acceptable. High reliability (Class I): target words that belong to adjacent thesauruscategories.
For example, all the word pairs across categories 360 and 361,shown in Figure 3.
Examples of Class I contrasting word pairs from thedevelopment set include graceful?ungainly, fortunate?hapless, obese?slim,and effeminate?virile.
(Note, there need not be any affix or WordNet seedpairs across adjacent thesaurus categories for these word pairs to bemarked Class I.)
As expected, if we use only those adjacent categories thatwere manually identified to be contrasting (as described in Section 6.2.1,Method 3), then the system obtains even better results than those obtainedusing all adjacent thesaurus categories.
(Experiments and results shownin Section 7.1). Medium reliability (Class II): target words that are not Class I contrastingpairs, but belong to one paragraph each of a prime contrasting paragraph.For example, all the word pairs across the paragraphs of sympathetic andindifferent.
See Figure 4.
Examples of Class II contrasting word pairsfrom the development set include altruism?avarice, miserly?munificent,accept?repudiate, and improper?prim. Acceptable reliability (Class III): target words that are not Class I orClass II contrasting pairs, but occur across contrasting category pairs.
Forexample, all word pairs across categories 423 and 230 except those thathave one word each from the paragraphs of sympathetic and indifferent.See Figure 4.
Examples of Class III contrasting word pairs from thedevelopment set include pandemonium?calm, probity?error, artifice?sincerity,and hapless?wealthy.Even with access to very large textual data sets, there is always a long tail of wordsthat occur so few times that there is not enough co-occurrence information for them.Thus we assume that all word pairs in Class I have a higher degree of contrast thanall word pairs in Class II, and that all word pairs in Class II have a higher degree ofcontrast than the pairs in Class III.
If two word pairs belong to the same class, then wecalculate their tendency to co-occur with each other in text to determine which pair ismore contrasting.
All experiments in the evaluation section ahead follow this method.6.2.3 Lexicon of Contrasting Word Pairs.
Using the method described in the previoussections, we generated a lexicon of word pairs pertaining to Class I and Class II.
Thelexicon has 6.3 million contrasting word pairs, about 3.5 million of which belong toClass I and about 2.8 million to Class II.
Class III pairs are even more numerous and,given a word pair, our algorithm checked whether it is a Class III pair, but we did not574Mohammad et alComputing Lexical Contrast1.
nouns:Category number: 423Category head: KINDNESS1.
nouns:Category number: 230Category head: APATHYkindnessconsideratenessnicenessgoodness...2. adjectives:sympatheticconsolatorycaringinvolved...3. adverbs:benevolentbeneficientlygraciouslykindheartedly......apathyacediadepressionmoppishness...2. nouns:nonchalanceinsouciancecarelessnesscasualness3.
adjectives:...indifferentdetachedirresponsiveuncaring......Figure 4Example contrasting category pair that has Class II and Class III contrasting pairs.
The systemidentifies the pair to be contrasting through the affix-based seed pair caring (second word inparagraph 2 or category 423) and uncaring (fourth word in paragraph 3 or category 230).
Theparagraphs of sympathetic and indifferent are therefore the prime contrasting paragraphs and soall word pairs that have one word each from these two paragraphs are Class II contrasting pairs.All other pairs formed by taking one word each from the two contrasting categories are theClass III contrasting pairs.
Paragraph heads are shown in bold italic.create a complete set of all Class III contrasting pairs.
Class I and II lexicons are availablefor download and summarized in Table 18.7.
EvaluationWe evaluate our algorithm on two different tasks and four data sets.
Section 7.1 de-scribes experiments on solving existing GRE ?choose the most contrasting word?
ques-tions (a recapitulation of the evaluation reported in Mohammad, Dorr, and Hirst [2008]).Section 7.2 describes experiments on solving newly created ?choose the most contrast-ing word?
questions specifically designed to determine performance on different kindsof opposites.
And lastly, Section 7.3 describes experiments on two different data setswhere the goal is to identify whether a given word pair is synonymous or antonymous.7.1 Solving GRE?s ?Choose the Most Contrasting Word?
QuestionsThe GRE is a test taken by thousands of North American graduate school applicants.The test is administered by Educational Testing Service (ETS).
The Verbal Reasoningsection of GRE is designed to test verbal skills.
Until August 2011, one of its sections hada set of questions pertaining to word-pair contrast.
Each question had a target word andfour or five alternatives, or option words.
The objective was to identify the alternativewhich was most contrasting with respect to the target.
For example, consider:adulterate: a. renounce b. forbid c. purify d. criticize e. correct575Computational Linguistics Volume 39, Number 3Here the target word is adulterate.
One of the alternatives provided is correct, whichas a verb has a meaning that contrasts with that of adulterate; purify, however, hasa greater degree of contrast with adulterate than correct does and must be chosenin order for the instance to be marked as correctly answered.
ETS referred to thesequestions as ?antonym questions,?
where the examinees had to ?choose the wordmost nearly opposite?
to the target.
Most of the target?answer pairs are not gradableadjectives, however, and because most of them are not opposites either, we will referto these questions as ?choose the most contrasting word?
questions or contrast questionsfor short.Evaluation on this data set tests whether the automatic method is able to identifynot just opposites but also those pairs that are not opposites but that have some degreeof semantic contrast.
Notably, for these questions, the method must be able to identifythat one word pair has a higher degree of contrast than all others, even though thatword pair may not necessarily be an opposite.7.1.1 Data.
A Web search for large sets of contrast questions yielded two independentsets of questions designed to prepare students for the GRE.
The first set consists of162 questions.
We used this set while we were developing our lexical contrast algo-rithm described in Section 4.
Therefore, we will refer to it as the development set.
Thedevelopment set helped determine which features of lexical contrast were more reliablethan others.
The second set has 1,208 contrast questions.
We discarded questions thathad a multiword target or alternative.
After removing duplicates we were left with790 questions, which we used as the unseen test set.
This data set was used (and seen)only after our algorithm for determining lexical contrast was frozen.Interestingly, the data contains many instances that have the same target word usedin different senses.
For example:1. obdurate: a. meager b. unsusceptible c. right d. tender e. intelligent2.
obdurate: a. yielding b. motivated c. moribund d. azure e. hard3.
obdurate: a. transitory b. commensurate c. complaisant d. similar e. laconicIn (1), obdurate is used in the sense of HARDENED IN FEELINGS and is most contrastingwith tender.
In (2), it is used in the sense of RESISTANT TO PERSUASION and is mostcontrasting with yielding.
In (3), it is used in the sense of PERSISTENT and is mostcontrasting with transitory.The data sets also contain questions in which one or more of the alternatives is anear-synonym of the target word.
For example:astute: a. shrewd b. foolish c. callow d. winning e. debatingObserve that shrewd is a near-synonym of astute.
The word most contrasting with astuteis foolish.
A manual check of a randomly selected set of 100 test-set questions revealedthat, on average, one in four had a near-synonym as one of the alternatives.7.1.2 Results.
Table 10 presents results obtained on the development and test datausing two baselines, a re-implementation of the method described in Lin et al(2003),and variations of our method.
Some of the results are for systems that refrain from576Mohammad et alComputing Lexical ContrastTable 10Results obtained on contrast questions.
The best performing system and configuration areshown in bold.development data test dataP R F P R FBaselines:a. random baseline 0.20 0.20 0.20 0.20 0.20 0.20b.
WordNet antonyms 0.23 0.23 0.23 0.23 0.23 0.23Related work:a. Lin et al(2003) 0.23 0.23 0.23 0.24 0.24 0.24Our method:a. affix-generated pairs as seeds 0.72 0.53 0.61 0.71 0.51 0.59b.
WordNet antonyms as seeds 0.79 0.52 0.63 0.72 0.49 0.58c.
both seed sets (a + b) 0.77 0.65 0.70 0.72 0.58 0.64d.
adjacency heuristic only 0.81 0.43 0.56 0.83 0.44 0.57e.
manual annotation of adjacent categories 0.88 0.41 0.56 0.87 0.41 0.55f.
affix seed set and adjacency heuristic (a + d) 0.75 0.60 0.67 0.76 0.60 0.67g.
both seed sets and adjacency heuristic (a + b + d) 0.76 0.66 0.70 0.76 0.63 0.69h.
affix seed set and annotation of adjacent 0.79 0.63 0.70 0.78 0.60 0.68categories (a + e)i. both seed sets and annotation of adjacent 0.79 0.66 0.72 0.77 0.63 0.69categories (a + b + e)attempting questions for which they do not have sufficient information.
We thereforereport precision (P), recall (R), and balanced F-score (F).P =# of questions answered correctly# of questions attempted(1)R =# of questions answered correctly# of questions(2)F =2 ?
P ?
RP+ R(3)Baselines.
If a system randomly guesses one of the five alternatives with equal probabil-ity (random baseline), then it obtains an accuracy of 0.2.
A system that looks up the list ofWordNet antonyms (10,807 pairs) to solve the contrast questions is our second baseline.That obtained the correct answer in only 5 instances of the development set (3.09% ofthe 162 instances) and 25 instances of the test set (3.17% of the 790 instances), however.Even if the system guesses at random for all other instances, it attains only a modestimprovement over the random baseline (see row b, under ?Baselines,?
in Table 10).Re-implementation of related work.
In order to estimate how well the method of Linet al(2003) performs on this task, we re-implemented their method.
For each closest-antonym question, we determined frequency counts in the Google n-gram corpus forthe phrases ?from ?target word?
to ?known correct answer?,?
?from ?known correctanswer?
to ?target word?,?
?either ?target word?
or ?known correct answer?,?
and?either ?known correct answer?
or ?target word?.?
We then summed up the four countsfor each contrast question.
This resulted in non-zero counts for only 5 of the 162 in-stances in the development set (3.09%), and 35 of the 790 instances in the test set (4.43%).Thus, these patterns fail to cover a vast majority of closest-antonyms, and even if the577Computational Linguistics Volume 39, Number 3system guesses at random for all other instances, it attains only a modest improvementover the baseline (see row a, under ?Related work,?
in Table 10).Our method.
Table 10 presents results obtained on the development and test data usingdifferent combinations of the seed sets and the adjacency heuristic.
The best performingsystem is marked in bold.
It has significantly higher precision and recall than that of themethod proposed by Lin et al(2003), with 95% confidence according to the Fisher ExactTest (Agresti 1990).We performed experiments on the development set first, using our method withconfigurations described in rows a, b, and d. These results showed that markingadjacent categories as contrasting has the highest precision (0.81), followed by usingWordNet seeds (0.79), followed by the use of affix rules to generate seeds (0.72).
Thisallowed us to determine the relative reliability of the three features as described inSection 6.2.2.
We then froze all system development and ran the remaining experiments,including those on the test data.Observe that all of the results shown in Table 10 are well above the random baselineof 0.20.
Using only the small set of 15 affix rules, the system performs almost as well aswhen it uses 10,807 WordNet opposites.
Using both the affix-generated and the Word-Net seed sets, the system obtains markedly improved precision and coverage.
Usingonly the adjacency heuristic gave precision values (upwards of 0.8) with substantialcoverage (attempting more than half of the questions).
Using the manually identifiedcontrasting adjacent thesaurus categories gave precision values just short of 0.9.
Thebest results were obtained using both seed sets and the contrasting adjacent thesauruscategories (F-scores of 0.72 and 0.69 on the development and test set, respectively).In order to determine whether our method works well with thesauri other than theMacquarie Thesaurus, we determined performance of configurations a, b, c, d, f, and husing the 1911 U.S. edition of the Roget?s Thesaurus, which is available freely in the publicdomain.10 The results were similar to those obtained using the Macquarie Thesaurus.
Forexample, configuration g obtained a precision of 0.81, recall of 0.58, and F-score of 0.68on the test set.
It may be possible to obtain even better results by combining multiplelexical resources; that is left for future work.
The remainder of this article reports resultsobtained with the Macquarie Thesaurus; the 1911 vocabulary is less suited for practicaluse in the 21st century.7.1.3 Discussion.
These results show that our method performs well on questions de-signed to be challenging for humans.
In tasks that require higher precision, using onlythe contrasting adjacent categories is best, whereas in tasks that require both precisionand coverage, the seed sets may be included.
Even when both seed sets were included,only four instances in the development set and twenty in the test set had target?answerpairs that matched a seed opposite pair.
For all remaining instances, the approach hadto generalize to determine the most contrasting word.
This also shows that even theseemingly large number of direct and indirect antonyms from WordNet (more than10,000) are by themselves insufficient.The comparable performance obtained using the affix rules alone suggests that evenin languages that do not have a WordNet-like resource, substantial accuracies may beobtained.
Of course, improved results when using WordNet antonyms as well suggeststhat the information they provide is complementary.10 http://www.gutenberg.org/ebooks/10681.578Mohammad et alComputing Lexical ContrastError analysis revealed that at times the system failed to identify that a categorypertaining to the target word contrasted with a category pertaining to the answer.Additional methods to identify seed opposite pairs will help in such cases.
Certainother errors occurred because one or more alternatives other than the official answerwere also contrasting with the target.
For example, one of the questions has chasten asthe target word.
One of the alternatives is accept, which has some degree of contrast inmeaning to the target.
Another alternative, reward, has an even higher degree of contrastwith the target, however.
In this instance, the system erred by choosing accept as theanswer.7.2 Determining Performance of Automatic Method on Different Kinds of OppositesThe previous section showed the overall performance of our method.
The performanceof a method may vary significantly on different subsets of data, however.
In order todetermine performance on different kinds of opposites, we generated new contrastquestions from the crowdsourced term pairs described in Section 4.
Note that for solvingcontrast questions with this data set, again the method must be able to identify thatone word pair has a higher degree of contrast than the other pairs; unlike the previoussection, however, here the correct answer is often an opposite of the target.7.2.1 Generating Contrast Questions.
For each word pair from the list of WordNet oppo-sites, we chose one word randomly to be the target word, and the other as one of itscandidate options.
Four other candidate options were chosen from Lin?s distributionalthesaurus (Lin 1998).11 An entry in the distributional thesaurus has a focus word anda number of other words that are distributionally similar to the focus word.
The wordsare listed in decreasing order of similarity.
Note that these entries include not justnear-synonymous words but also at times contrasting words because contrastingwords tend to be distributionally similar (Lin et al2003).For each of the target words in our contrast questions, we chose the four distribu-tionally closest words from Lin?s thesaurus to be the distractors.
If a distractor had thesame first three letters as the target word or the correct answer, then it was replacedwith another word from the distributional thesaurus.
This ad hoc filtering criterion iseffective at discarding distractors that are morphological variants of the target or theanswer.
For example, if the target word is adulterate, then words such as adulterated andadulterates will not be included as distractors even if they are listed as closely similarterms in the distributional thesaurus.We place the four distractors and the correct answer in random order.
Some of theWordNet opposites were not listed in Lin?s thesaurus, and the corresponding questionwas not generated.
In all, 1,269 questions were generated.
We created subsets of thesequestions corresponding to the different kinds of opposites and also corresponding todifferent parts of speech.
Because a word pair may be classified as more than one kindof opposite, the corresponding question may be part of more than one subset.7.2.2 Experiments and Results.
We applied our method of lexical contrast to solve thecomplete set of 1,269 questions and also the various subsets.
Because this test set is11 http://webdocs.cs.ualberta.ca/?lindek/downloads.htm.579Computational Linguistics Volume 39, Number 3Table 11Percentage of contrast questions correctly answered by the automatic method, where differentquestion sets correspond to target?answer pairs of different kinds.
The automatic method did notuse WordNet seeds for this task.
The results shown for ?ALL?
are micro-averages, that is, they arethe results for the master set of 1,269 contrast questions.# instances P R FAntipodals 1,044 0.95 0.84 0.89Complementaries 1,042 0.95 0.83 0.89Disjoint 228 0.81 0.59 0.69Gradable 488 0.95 0.85 0.90Reversives 203 0.93 0.74 0.82ALL 1,269 0.93 0.79 0.85created from WordNet opposites, we applied the algorithm without the use of WordNetseeds (no WordNet information was used by the method).Table 11 shows the precision (P), recall (R), and F-score (F) obtained by the methodon the data sets corresponding to different kinds of opposites.
The column ?# instances?shows the number of questions in each of the data sets.
The performance of our methodon the complete data set is shown in the last row ALL.
Observe that the F-score of0.85 is markedly higher than the score obtained on the GRE-preparatory questions.This is expected because the GRE questions involved vocabulary from a higher readinglevel, and included carefully chosen distractors to confuse the examinee.
The automaticmethod obtains highest F-score on the data sets of gradable adjectives (0.90), antipodals(0.89), and complementaries (0.89).
The precisions and recalls for these opposites aresignificantly higher than those of disjoint opposites.
The recall for reversives is also sig-nificantly lower than that for the gradable adjectives, antipodals, and complementaries,but precision on reversives is quite good (0.93).Table 12 shows the precision, recall, and F-score obtained by the method on the thedata sets corresponding to different parts of speech.
Observe that performances on allparts of speech are fairly high.
The method deals with adverb pairs best (F-score of 0.89),and the lowest performance is for verbs (F-score of 0.80).
The differences in precisionvalues between various parts of speech are not significant.
The recall obtained on theadverbs is significantly higher than that obtained on adjectives, however, and the recallon adjectives is significantly higher than that obtained on verbs.
The difference betweenthe recalls on adverbs and nouns is not significant.
We used the Fisher Exact Test and aconfidence interval of 95% for all significance testing reported in this section.Table 12Percentage of contrast questions correctly answered by the automatic method, where differentquestion sets correspond to different parts-of-speech.# instances P R FAdjectives 551 0.92 0.79 0.85Adverbs 165 0.95 0.84 0.89Nouns 330 0.93 0.81 0.87Verbs 226 0.93 0.71 0.80ALL 1,269 0.93 0.79 0.85580Mohammad et alComputing Lexical Contrast7.3 Distinguishing Synonyms from OppositesOur third evaluation follows that of Lin et al(2003) and Turney (2008).
We developeda system for automatically distinguishing synonyms from opposites, and applied it totwo data sets.
The approach and experiments are described herein.7.3.1 Data.
Lin et al(2003) compiled 80 pairs of synonyms and 80 pairs of oppositesfrom the Webster?s Collegiate Thesaurus (Kay 1988) such that each word in a pair is alsoin their list of the 50 distributionally most similar words of the other.
(Distributionalsimilarity was calculated using the algorithm proposed by Lin et al[1998].)
Turney(2008) compiled 136 pairs of words (89 opposites and 47 synonyms) from various Websites for learners of English as a second language; the objective for the learners is toidentify whether the words in a pair are opposites or synonyms of each other.
Thegoals of this evaluation are to determine whether our automatic method can distinguishopposites from near-synonyms, and to compare our method with the closest relatedwork on an evaluation task for which published results are already available.7.3.2 Method.
The core of our method is this:1.
Word pairs that occur in the same thesaurus category are close in meaningand so are marked as synonyms.2.
Word pairs that occur in contrasting thesaurus categories or paragraphs(as described in Section 6.2.1 above) are marked as opposites.Even though opposites often occur in different thesaurus categories, they can sometimesalso be found in the same category, however.
For example, the word ascent is listed inthe Macquarie Thesaurus categories of 49 (CLIMBING) and 694 (SLOPE), whereas the worddescent is listed in the categories 40 (ARISTOCRACY), 50 (DROPPING), 538 (PARENTAGE),and 694 (SLOPE).
Observe that ascent and descent are both listed in the same category694 (SLOPE), which makes sense here because both words are pertinent to the concept ofslope.
On the other hand, two separate clues independently inform our system that thewords are opposites of each other: (1) Category 49 has the word upwardness in the sameparagraph as ascent, and category 50 has the word downwardness in the same paragraphas descent.
The 13th affix pattern from Table 9 (upX and downX) indicates that the twothesaurus paragraphs have contrasting meaning.
Thus, ascent and descent occur in primecontrasting thesaurus paragraphs.
(2) One of the ascent categories (49) is adjacent to oneof the descent categories (50), and further this adjacent category pair has been manuallymarked as contrasting.Thus the words in a pair may be deemed both synonyms and opposites simultane-ously by our methods of determining synonyms and opposites, respectively.
Some ofthe features we use to determine opposites were found to be more precise (e.g., wordslisted in adjacent categories) than others (e.g., categories identified as contrasting basedon affix and WordNet seeds), however.
Thus we apply the following rules as a decisionlist: If one rule fires, then the subsequent rules are ignored.1.
Rule 1 (high confidence for opposites): If the words in a pair occur inadjacent thesaurus categories, then they are marked as opposites.2.
Rule 2 (high confidence for synonyms): If both the words in a pair occurin the same thesaurus category, then they are marked as synonyms.581Computational Linguistics Volume 39, Number 33.
Rule 3 (medium confidence for opposites): If the words in a pair occur inprime contrasting thesaurus paragraphs, as determined by an affix-basedor WordNet seed set, then they are marked as opposites.If a word pair is not tagged as synonym or opposite: (a) the system can refrainfrom attempting an answer (this will attain high precision), or (b) the system canrandomly guess the lexical relation (this will obtain 50% accuracy for the pairs), or (c) itcould mark all remaining word pairs with the predominant lexical relation in the data(this will obtain an accuracy proportional to the skew in distribution of opposites andsynonyms).
For example, if after step 3, the system finds that 70% of the marked wordpairs were tagged opposites, and 30% as synonyms, then it could mark every hithertountagged word pair (word pair for which it has insufficient information) as opposites.We implemented all three variants.
Note that option (b) is indeed expected to performpoorly compared to option (c), but we include it as part of our evaluation to measureusefulness of option (c).7.3.3 Results and Discussion.
Table 13 shows the precision (P), recall (R), and balancedF-score (F) of various systems and baselines in identifying synonyms and oppositesfrom the data set described in Lin et al(2003).
We will refer to this data set as LZQZ(the first letters of the authors?
last names).If a system guesses at random (random baseline) it will obtain an accuracy of 50%.Choosing opposites (or synonyms) as the predominant class also obtains an accuracy of50% because the data set has an equal number of opposites and synonyms.
Publishedresults on LZQZ (Lin et al2003) are shown here again for convenience.
The resultsobtained with our system and the three variations on handling word pairs for whichit does not have enough information are shown in the last three rows.
The precision ofour method in configuration (a) is significantly higher than that of Lin et al(2003), with95% confidence according to the Fisher Exact Test (Agresti 1990).
Because precision andrecall are the same for configuration (b) and (c), as well as for the methods described inLin et al(2003) and Turney (2011), we can also refer to these results simply as accuracy.Table 13Results obtained on the synonym-or-opposite questions in LZQZ.
The best performing systemsare marked in bold.
The difference in precision and recall of method by Lin et al(2003) and ourmethod in configurations (b) and (c) is not statistically significant.P R FBaselines:a. random baseline 0.50 0.50 0.50b.
supervised most-frequent baseline?
0.50 0.50 0.50Related work:a. Lin et al(2003) 0.90 0.90 0.90b.
Turney (2011) 0.82 0.82 0.82Our method: if no informationa.
refrain from guessing 0.98 0.78 0.87b.
make random guess 0.88 0.88 0.88c.
mark the predominant class?
0.87 0.87 0.87?This data set has an equal number of opposites and synonyms.
Results reported are whenchoosing opposites as the predominant class.
?The system concluded that opposites were slightly more frequent than synonyms.582Mohammad et alComputing Lexical ContrastTable 14Results obtained on the synonym-or-opposite questions in TURN.
The best performing systemsare marked in bold.P R FBaselinesa.
random baseline 0.50 0.50 0.50b.
supervised most-frequent baseline?
0.65 0.65 0.65Related worka.
Turney (2008) 0.75 0.75 0.75b.
Lin et al(2003) 0.35 0.35 0.35Our method: if no informationa.
refrain from guessing 0.97 0.69 0.81b.
make random guess 0.84 0.84 0.84c.
mark the predominant class?
0.90 0.90 0.90?About 65.4% of the pairs in this data set are opposites.
So this row reports baseline results whenchoosing opposites as the predominant class.
?The system concluded that opposites were much more frequent than synonyms.We found that the differences in accuracies between the method of Lin et al(2003) andour method in configurations (b) and (c) are not statistically significant.
The method byLin et al(2003) and our method in configuration (b) have significantly higher accuracythan the method described in Turney (2011), however.
The lexical contrast features usedin configurations (a), (b), and (c) correspond to row i in Table 10.
The next subsectionpresents an analysis of the usefulness of the different features listed in Table 10.Observe that when our method refrains from guessing in case of insufficient infor-mation, it obtains excellent precision (0.98), while still providing very good coverage(0.78).
As expected, the results obtained with (b) and (c) do not differ much from eachother because the data set has an equal number of synonyms and opposites.
(Notethat the system was not privy to this information.)
After step 3 of the algorithm,however, the system had marked 65 pairs as opposites and 63 pairs as synonyms,and so it concluded that opposites are slightly more dominant in this data set andtherefore the guess-predominant-class variant marked all previously unmarked pairs asopposites.It should be noted that the LZQZ data set was chosen from a list of high-frequencyterms.
This was necessary to increase the probability of finding sentences in a corpuswhere the target pair occurred in one of the chosen patterns proposed by Lin et al(2003).
As shown in Table 10, the Lin et al(2003) patterns have a very low coverageotherwise.
Further, the test data compiled by Lin et alonly had opposites whereas thecontrast questions had many contrasting word pairs that were not opposites.Table 14 shows results on the data set described in Turney (2008).
We will refer tothis data set as TURN.
The supervised baseline of always guessing the most frequentclass (in this case, opposites), will obtain an accuracy of 65.4% (P = R = F = 0.654).Turney (2008) obtains an accuracy of 75% using a supervised method and 10-foldcross-validation.
A re-implementation of the method proposed by Lin et al(2003) asdescribed in Section 7.1.3 did not recognize any of the word pairs in TURN as opposites;that is, none of the word pairs in TURN occurred in the Google n-gram corpus inpatterns used by Lin et al(2003).
Thus it marked all words in TURN as synonyms.The results obtained with our method are shown in the last three rows.
The precisionand recall of our method in configurations (b) and (c) are significantly higher than those583Computational Linguistics Volume 39, Number 3obtained by the methods by Turney (2008) and Lin et al(2003), with 95% confidenceaccording to the Fisher Exact Test (Agresti 1990).Observe that once again our method, especially the variant that refrains fromguessing in case of insufficient information, obtains excellent precision (0.97), whilestill providing good coverage (0.69).
Also observe that results obtained by guessing thepredominant class (method (c)) are markedly better than those obtained by randomlyguessing in case of insufficient information (method (b)).
This is because, as mentionedearlier, the distribution of opposites and synonyms is somewhat skewed in this dataset (65.4% of the pairs are opposites).
Of course, again the system was not privy to thisinformation, but method (a) marked 58 pairs as opposites and 39 pairs as synonyms.Therefore, the system concluded that opposites are more dominant and method (c)marked all previously unmarked pairs as opposites, obtaining an accuracy of 90%.Recall that in Section 7.3.2 we described how opposite pairs may occasionally belisted in the same thesaurus category because the category may be pertinent to bothwords.
For 12 of the word pairs in the Lin et aldata and 3 of the word pairs in theTurney data, both words occurred together in the same thesaurus category, and yetthe system marked them as opposites because they occurred in adjacent thesauruscategories (Class I).
For 11 of the 12 pairs from LZQZ and for all 3 of the TURN pairs,this resulted in the correct answer.
These pairs are shown in Table 15.
By contrast, onlyone of the term pairs in this table occurred in one of Lin?s patterns of oppositeness, andwas thus the only one correctly identified by their method as a pair of opposites.It should also be noted that a word may have multiple meanings such that it may besynonymous to a word in one sense and opposite to it in another sense.
Such pairs arealso expected to be marked as opposites by our system.
Two such pairs in the Turney(2008) data are: fantastic?awful and terrific?terrible.
The word awful can mean INSPIRINGAWE (and so close to the meaning of fantastic in some contexts), and also EXTREMELYDISAGREEABLE (and so opposite to fantastic).
The word terrific can mean FRIGHTFUL(and so close to the meaning of terrible), and also UNUSUALLY FINE (and so oppositeto terrible).
Such pairs are probably not the best synonym-or-opposite questions.
Facedwith these questions, however, humans probably home in on the dominant senses ofTable 15Pairs from LZQZ and TURN that have at least one category in common but are still marked asopposites by our method.LZQZ TURNword 1 word 2 official solution word 1 word 2 official solutionamateur professional opposite fantastic awful oppositeascent descent opposite dry wet oppositeback front opposite terrific terrible oppositebottom top oppositebroadside salvo synonymentrance exit oppositeheaven hell oppositeinside outside oppositejunior senior oppositelie truth oppositemajority minority oppositenadir zenith oppositestrength weakness opposite584Mohammad et alComputing Lexical ContrastTable 16Results for individual components as well as certain combinations of components on thesynonym-or-opposite questions in LZQZ.
The best performing configuration is shown in bold.P R FBaselines:a. random baseline 0.50 0.50 0.50b.
supervised most-frequent baseline?
0.50 0.50 0.50Our methods:a. affix-generated seeds only 0.86 0.54 0.66b.
WordNet seeds only 0.88 0.65 0.75c.
both seed sets (a + b) 0.88 0.65 0.75d.
adjacency heuristic only 0.95 0.74 0.83e.
manual annotation of adjacent categories 0.98 0.74 0.84f.
affix seed set and adjacency heuristic (a + d) 0.95 0.75 0.84g.
both seed sets and adjacency heuristic (a + b + d) 0.95 0.78 0.86h.
affix seed set and annotation of adjacent categories 0.98 0.77 0.86(a + e)i. both seed sets and annotation of adjacent categories 0.98 0.78 0.87(a + b + e)?This data set has equal number of opposites and synonyms, so either class can be chosen to bepredominant.
Baseline results shown here are for choosing opposites as the predominant class.the target words to determine an answer.
For example, in modern-day English terrificis used more frequently in the sense of UNUSUALLY FINE than the sense of FRIGHTFUL,and so most people will say that terrific and terrible are opposites (in fact that is thesolution provided with these data).7.3.4 Analysis.
We carried out additional experiments to determine how useful individ-ual components of our method were in solving the synonym-or-opposite questions.The results on LZQZ are shown in Table 16 and the results on TURN are shown inTable 17.
These results are for the case when the system refrains from guessing in caseof insufficient information.
The rows in the tables correspond to the rows in Table 10shown earlier that gave results on the contrast questions.Observe that the affix-generated seeds give a marked improvement over the base-lines, and that knowing which categories are contrasting (either from the adjacencyheuristic or manual annotation of adjacent categories) proves to be the most usefulfeature.
Also note that even though manual annotation and WordNet seeds eventuallylead to the best results (F = 0.87 for LZQZ and F = 0.81 for TURN), using only theadjacency heuristic and the affix-generated seeds gives competitive results (F = 0.84 forthe Lin set and F = 0.78 for the Turney set).
We are interested in developing methodsto make the approach cross-lingual, so that we can use a thesaurus from one language(say, English) to compute lexical contrast in a resource-poor target language.The precision of our method is very good (>0.95).
Thus future work will be aimedat improving recall.
This can be achieved by developing methods to generate more seedopposites.
This is also an avenue through which some of the pattern-based approaches(such as the methods described by Lin et al[2003] and Turney [2008]) can be incorpo-rated into our method.
For instance, we could use n-gram patterns such as ?either X orY?
and ?from X to Y?
to identify pairs of opposites that can be used as additional seedsin our method.585Computational Linguistics Volume 39, Number 3Recall can also be improved by using affix patterns in other languages to identifycontrasting thesaurus paragraphs in the target language.
Thus, constructing a cross-lingual framework in which words from one language will be connected to thesauruscategories in another language will be useful not only in computing lexical contrast in aresource-poor language, but also in using affix information from different languages toimprove results in the target, possibly even resource-rich, language.8.
Conclusions and Future WorkDetecting semantically contrasting word pairs has many applications in natural lan-guage processing.
In this article, we proposed a method for computing lexical contrastthat is based on the hypothesis that if a pair of words, A and B, are contrasting, thenthere is a pair of opposites, C and D, such that A and C are strongly related and B andD are strongly related?the contrast hypothesis.
We used pointwise mutual informationto determine the degree of contrast between two contrasting words.
The method outper-formed others on the task of solving a large set of ?choose the most contrasting word?questions wherein the system not only identified whether two words are contrastingbut also distinguished between pairs of contrasting words with differing degrees ofcontrast.
We further determined performance of the method on five different kinds ofopposites and across four parts of speech.
We used our approach to solve synonym-or-opposite questions described in Turney (2008) and Lin et al(2003).Because opposites were central to our methodology, we designed a questionnaire tobetter understand different kinds of opposites, which we crowdsourced with AmazonMechanical Turk.
We devoted extra effort to making sure the questions are phrasedin a simple, yet clear manner.
Additionally, a quality control method was developed,using a word-choice question, to automatically identify and discard dubious and outlierannotations.
From these data, we created a data set of different kinds of opposites thatTable 17Results for individual components as well as certain combinations of components on thesynonym-or-opposite questions in TURN.
The best performing configuration is shown in bold.P R FBaselines:a. random baseline 0.50 0.50 0.50b.
supervised most-frequent baseline?
0.65 0.65 0.65Our methods:a. affix-generated seeds only 0.92 0.54 0.68b.
WordNet seeds only 0.93 0.61 0.74c.
both seed sets (a + b) 0.93 0.61 0.74d.
adjacency heuristic only 0.94 0.60 0.74e.
manual annotation of adjacent categories 0.96 0.60 0.74f.
affix seed set and adjacency heuristic (a + d) 0.95 0.67 0.78g.
both seed sets and adjacency heuristic (a + b + d) 0.95 0.68 0.79h.
affix seeds and annotation of adjacent categories 0.97 0.68 0.80(a + e)i. both seed sets and annotation of adjacent categories 0.97 0.69 0.81(a + b + e)?About 65.4% of the pairs in this data set are opposites.
So this row reports baseline results whenchoosing opposites as the predominant class.586Mohammad et alComputing Lexical ContrastTable 18A summary of the data created as part of this research on lexical contrast.
Available fordownload at: http://www.purl.org/net/saif.mohammad/research.Name # of itemsAffix patterns that tend to generate opposites 15 rulesContrast questions:GRE preparatory questions:Development set 162 questionsTest set 790 questionsNewly created questions: 1,269 questionsData from work on types of opposites:Crowdsourced questionnaires 4 sets (one for every pos)Responses to questionnaires 12,448 assignments (in four files)Lexicon of opposites generated by theMohammad et almethod:Class I opposites 3.5 million word pairsClass II opposites 2.5 million word pairsManually identified contrasting categoriesin the Macquarie Thesaurus 209 category pairsWord-pairs used in Section 5 experiments:WordNet opposites set 1,358 word pairsWordNet random word pairs set 1,358 word pairsWordNet synonyms set 1,358 word pairswe have made available.
We determined the amount of agreement among humans inidentifying lexical contrast, and also in identifying different kinds of opposites.
We alsoshowed that a large number of opposing word pairs have properties pertaining to morethan one kind.
Table 18 summarizes the data created as part of this research on lexicalcontrast, all of which is available for download.New questions that target other types of lexical contrast not addressed in this papermay be added in the future.
It may be desirable to break a complex question into twoor more simpler questions.
For example, if a word pair is considered to be a certainkind of opposite when it has both properties M and N, then it is best to have twoseparate questions asking whether the word pair has properties M and N, respectively.The crowdsourcing study can be replicated for other languages by asking the samequestions in the target language for words in the target language.
Note, however, thatas of February 2012, most of the Mechanical Turk participants are native speakers ofEnglish, certain Indian languages, and some European languages.Our future goals include porting this approach to a cross-lingual framework todetermine lexical contrast in a resource-poor language by using a bilingual lexicon toconnect the words in that language with words in another resource-rich language.
Wecan then use the structure of the thesaurus from the resource-rich language as describedin this article to detect contrasting categories of terms.
This is similar to the approachdescribed by Mohammad et al(2007), who compute semantic distance in a resource-poor language by using a bilingual lexicon and a sense disambiguation algorithm toconnect text in the resource-poor language with a thesaurus in a different language.
Thisenables automatic discovery of lexical contrast in a language even if it does not have aRoget-like thesaurus.
The cross-lingual method still requires a bilingual lexicon to mapwords between the target language and the language with the thesaurus, however.587Computational Linguistics Volume 39, Number 3Our method used only one Roget-like published thesaurus, but even more gainsmay be obtained by combining many dictionaries and thesauri using methods proposedby Ploux and Victorri (1998) and others.We modified our algorithm to create lexicons of words associated with positiveand negative sentiment (Mohammad, Dunne, and Dorr 2009).
We also used the lexi-cal contrast algorithm in some preliminary experiments to identify contrast betweensentences and use that information to improve cohesion in automatic summarization(Mohammad et al2008).
Since its release, the lexicon of contrasting word pairs was usedto improve textual paraphrasing and in turn help improve machine translation (Marton,El Kholy, and Habash 2011).
We are interested in using contrasting word pairs as seedsto identify phrases that convey contrasting meaning.
These will be especially helpfulin machine translation where current systems have difficulty separating translationhypotheses that convey the same meaning as the source sentences, and those thatdo not.Given a particular word, our method computes a single score as the degree ofcontrast with another.
A word may be more or less contrasting with another word whenused in different contexts (Murphy 2003), however.
Just as in the lexical substitution task(McCarthy and Navigli 2009), where a system has to find the word that can best replacea target word in context to preserve meaning, one can imagine a lexical substitutiontask to generate contradictions where the objective is to replace a given target wordwith one that is contrasting so as to generate a contradiction.
Our future work includesdeveloping a context-sensitive measure of lexical contrast that can be used for exactlysuch a task.There is considerable evidence that children are aware of lexical contrast at avery early age (Murphy and Jones 2008).
They rely on it to better understand var-ious concepts and in order to communicate effectively.
Thus we believe that com-puter algorithms that deal with language can also obtain significant gains through theability to detect contrast and the ability to distinguish between differing degrees ofcontrast.AcknowledgmentsWe thank Tara Small, Smaranda Muresan,and Siddharth Patwardhan for their valuablefeedback.
This work was supported in partby the National Research Council Canada;in part by the National Science Foundationunder grant no.
IIS-0705832; in part by theHuman Language Technology Center ofExcellence; and in part by the NaturalSciences and Engineering Research Councilof Canada.
Any opinions, findings, andconclusions or recommendations expressedin this material are those of the authors anddo not necessarily reflect the views of thesponsors.ReferencesAgresti, Alan.
1990.
Categorical Data Analysis.Wiley, New York, NY.Banerjee, Satanjeev and Ted Pedersen.
2003.Extended gloss overlaps as a measureof semantic relatedness.
In InternationalJoint Conference on Artificial Intelligence,pages 805?810, Acapulco, Mexico.Bejar, Isaac I., Roger Chaffin, and SusanEmbretson.
1991.
Cognitive andPsychometric Analysis of Analogical ProblemSolving.
Springer-Verlag, New York.Brants, Thorsten and Alex Franz.
2006.
Web1T 5-gram version 1.
Linguistic DataConsortium, Philadelphia, PA.Budanitsky, Alexander and Graeme Hirst.2006.
Evaluating WordNet-basedmeasures of semantic distance.Computational Linguistics, 32(1):13?47.Burnard, Lou.
2000.
Reference Guide for theBritish National Corpus (World Edition).Oxford University Computing Services,Oxford.Charles, Walter G. and George A. Miller.1989.
Contexts of antonymous adjectives.Applied Psychology, 10:357?375.Church, Kenneth and Patrick Hanks.1990.
Word association norms, mutual588Mohammad et alComputing Lexical Contrastinformation and lexicography.Computational Linguistics, 16(1):22?29.Cruse, David A.
1986.
Lexical Semantics.Cambridge University Press, Cambridge.Curran, James R. 2004.
From Distributional toSemantic Similarity.
Ph.D. thesis, School ofInformatics, University of Edinburgh,Edinburgh, UK.de Marneffe, Marie-Catherine, AnnaRafferty, and Christopher D. Manning.2008.
Finding contradictions in text.
InProceedings of the 46th Annual Meeting of theAssociation for Computational Linguistics(ACL-08), pages 1,039?1,047, Columbus, OH.Deese, James.
1965.
The Structure ofAssociations in Language and Thought.The Johns Hopkins University Press,Baltimore, MD.Egan, Rose F. 1984.
Survey of the historyof English synonymy.
Webster?s NewDictionary of Synonyms, pages 5a?25a.Fellbaum, Christiane.
1995.
Co-occurrenceand antonymy.
International Journal ofLexicography, 8:281?303.Gross, Derek, Ute Fischer, and George A.Miller.
1989.
Antonymy and therepresentation of adjectival meanings.Memory and Language, 28(1):92?106.Harabagiu, Sanda M., Andrew Hickl,and Finley Lacatusu.
2006.
Lacatusu:Negation, contrast and contradiction intext processing.
In Proceedings of the 23rdNational Conference on Artificial Intelligence(AAAI-06), pages 755?762, Boston, MA.Hatzivassiloglou, Vasileios and Kathleen R.McKeown.
1997.
Predicting the semanticorientation of adjectives.
In Proceedings ofthe Eighth Conference on European Chapter ofthe Association for Computational Linguistics,pages 174?181, Madrid.Hearst, Marti.
1992.
Automatic acquisitionof hyponyms from large text corpora.
InProceedings of the Fourteenth InternationalConference on Computational Linguistics,pages 539?546, Nantes.Jones, Steven, Carita Paradis, M. LynneMurphy, and Caroline Willners.
2007.Googling for ?opposites?
: A Web-basedstudy of antonym canonicity.
Corpora,2(2):129?154.Justeson, John S. and Slava M. Katz.1991.
Co-occurrences of antonymousadjectives and their contexts.Computational Linguistics, 17:1?19.Kagan, Jerome.
1984.
The Nature of the Child.Basic Books, New York.Kay, Maire Weir, editor.
1988.
Webster?sCollegiate Thesaurus.
Merriam-Webster,Springfield, MA.Kempson, Ruth M. 1977.
Semantic Theory.Cambridge University Press, Cambridge.Lehrer, Adrienne and K. Lehrer.
1982.Antonymy.
Linguistics and Philosophy,5:483?501.Lin, Dekang.
1998.
Automatic retrieval andclustering of similar words.
In Proceedingsof the 17th International Conference onComputational Linguistics, pages 768?773,Montreal.Lin, Dekang, Shaojun Zhao, Lijuan Qin,and Ming Zhou.
2003.
Identifyingsynonyms among distributionallysimilar words.
In Proceedings of the 18thInternational Joint Conference on ArtificialIntelligence (IJCAI-03), pages 1,492?1,493,Acapulco.Lobanova, Anna, Tom van der Kleij, andJennifer Spenader.
2010.
Definingantonymy: A corpus-based study ofopposites by lexico-syntactic patterns.International Journal of Lexicography,23(1):19?53.Lucerto, Cupertino, David Pinto, andHe?ctor Jime?nez-Salazar.
2002.
Anautomatic method to identify antonymy.In Workshop on Lexical Resources and theWeb for Word Sense Disambiguation,pages 105?111, Puebla.Lyons, John.
1977.
Semantics, volume 1.Cambridge University Press, Cambridge.Marcu, Daniel and Abdesammad Echihabi.2002.
An unsupervised approachto recognizing discourse relations.In Proceedings of the 40th Annual Meetingof the Association for ComputationalLinguistics (ACL-02), pages 368?375,Philadelphia, PA.Marton, Yuval, Ahmed El Kholy, andNizar Habash.
2011.
Filtering antonymous,trend-contrasting, and polarity-dissimilardistributional paraphrases for improvingstatistical machine translation.
InProceedings of the Sixth Workshop onStatistical Machine Translation,pages 237?249, Edinburgh.McCarthy, Diana and Roberto Navigli.2009.
The English lexical substitutiontask.
Language Resources And Evaluation,43(2):139?159.Mihalcea, Rada and Carlo Strapparava.2005.
Making computers laugh:Investigations in automatic humorrecognition.
In Proceedings of theConference on Human Language Technologyand Empirical Methods in Natural LanguageProcessing, pages 531?538, Vancouver.Mohammad, Saif, Bonnie Dorr, andGraeme Hirst.
2008.
Computing word-pair589Computational Linguistics Volume 39, Number 3antonymy.
In Proceedings of the Conferenceon Empirical Methods in Natural LanguageProcessing (EMNLP-2008), pages 982?991,Waikiki, HI.Mohammad, Saif, Bonnie J. Dorr,Melissa Egan, Nitin Madnani, David Zajic,and Jimmy Lin.
2008.
Multiple alternativesentence compressions and word-pairantonymy for automatic textsummarization and recognizing textualentailment.
In Text Analysis Conference(TAC), Gaithersburg, MD.Mohammad, Saif, Cody Dunne, andBonnie Dorr.
2009.
Generatinghigh-coverage semantic orientationlexicons from overtly marked wordsand a thesaurus.
In Proceedings ofEmpirical Methods in Natural LanguageProcessing (EMNLP-2009), pages 599?608,Singapore.Mohammad, Saif, Iryna Gurevych,Graeme Hirst, and Torsten Zesch.
2007.Cross-lingual distributional profiles ofconcepts for measuring semantic distance.In Proceedings of the Joint Conference onEmpirical Methods in Natural LanguageProcessing and Computational NaturalLanguage Learning (EMNLP/CoNLL-2007),pages 571?580, Prague.Murphy, Gregory L. and Jane M. Andrew.1993.
The conceptual basis of antonymyand synonymy in adjectives.
Journal ofMemory and Language, 32(3):1?19.Murphy, Lynne M. 2003.
Semantic Relationsand the Lexicon: Antonymy, Synonymy, andOther Paradigms.
Cambridge UniversityPress, Cambridge.Murphy, Lynne M. and Steven Jones.2008.
Antonyms in children?s andchild-directed speech.
First Language,28(4):403?430.Pang, Bo and Lillian Lee.
2008.
Opinionmining and sentiment analysis.Foundations and Trends in InformationRetrieval, 2(1?2):1?135.Pang, Bo, Lillian Lee, and ShivakumarVaithyanathan.
2002.
Thumbs up?
:Sentiment classification using machinelearning techniques.
In Proceedings of theConference on Empirical Methods in NaturalLanguage Processing, pages 79?86,Philadelphia, PA.Paradis, Carita, Caroline Willners,and Steven Jones.
2009.
Good andbad opposites using textual andexperimental techniques to measureantonym canonicity.
The Mental Lexicon,4(3):380?429.Ploux, Sabine and Bernard Victorri.
1998.Construction d?espaces se?mantiques a`l?aide de dictionnaires de synonymes.TAL, 39(1):161?182.Schwab, Didier, Mathieu Lafourcade, andViolaine Prince.
2002.
Antonymy andconceptual vectors.
In Proceedings ofthe 19th International Conference onComputational Linguistics (COLING-02),pages 904?910, Taipei, Taiwan.Turney, Peter D. 2008.
A uniform approachto analogies, synonyms, antonyms, andassociations.
In Proceedings of the 22ndInternational Conference on ComputationalLinguistics (COLING-08), pages 905?912,Manchester.Turney, Peter D. 2011.
Analogy perceptionapplied to seven tests of wordcomprehension.
Journal of Experimentaland Theoretical Artificial Intelligence -Psychometric Artificial Intelligence,23(3):343?362.Voorhees, Ellen M. 2008.
Contradictions andjustifications: Extensions to the textualentailment task.
In Proceedings of the46th Annual Meeting of the Association forComputational Linguistics (ACL-08), 63?71,Columbus, OH.590
