Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 43?46,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsAn Application of Latent Semantic Analysis to Word Sense Discriminationfor Words with Related and Unrelated MeaningsJuan Pino and Maxine Eskenazi(jmpino, max)@cs.cmu.eduLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213, USAAbstractWe present an application of Latent SemanticAnalysis to word sense discrimination withina tutor for English vocabulary learning.
Weattempt to match the meaning of a word in adocument with the meaning of the same wordin a fill-in-the-blank question.
We comparethe performance of the Lesk algorithm to La-tent Semantic Analysis.
We also compare theperformance of Latent Semantic Analysis on aset of words with several unrelated meaningsand on a set of words having both related andunrelated meanings.1 IntroductionIn this paper, we present an application of LatentSemantic Analysis (LSA) to word sense discrimi-nation (WSD) within a tutor for English vocabu-lary learning for non-native speakers.
This tutor re-trieves documents from the Web that contain tar-get words a student needs to learn and that are atan appropriate reading level (Collins-Thompson andCallan, 2005).
It presents a document to the studentand then follows the document reading with practicequestions that measure how the student?s knowledgehas evolved.
It is important that the fill-in-the-blankquestions (also known as cloze questions) that weask to the students allow us to determine their vocab-ulary knowledge accurately.
An example of clozequestion is shown in Figure 1.Some words have more than one meaning and sothe cloze question we give could be about a differentmeaning than the one that the student learned in thedocument.
This is something that can lead to confu-sion and must be avoided.
To do this, we need to usesome automatic measure of semantic similarity.Figure 1: Example of cloze question.To define the problem formally, given a targetword w, a string r (the reading) containing w andn strings q1, ..., qn (the sentences used for the ques-tions) each containing w, find the strings qi wherethe meaning of w is closest to its meaning in r.We make the problem simpler by selecting only onequestion.This problem is challenging because the contextdefined by cloze questions is short.
Furthermore,a word can have only slight variations in meaningthat even humans find sometimes difficult to distin-guish.
LSA was originally applied to InformationRetrieval (Dumais et al, 1988).
It was shown to beable to match short queries to relevant documentseven when there were no exact matches between thewords.
Therefore LSA would seem to be an appro-priate technique for matching a short context, suchas a question, with a whole document.So we are looking to first discriminate betweenthe meanings of words, such as ?compound?, thathave several very different meanings (a chemicalcompound or a set of buildings) and then to dis-ambiguate words that have senses that are closelyrelated such as ?comprise?
(?be composed of?
or?compose?).
In the following sections, we present43LSA and some of its applications, then we presentsome experimental results that compare a baseline tothe use of LSA for both tasks we have just described.We expect the task to be easier on words with unre-lated meanings.
In addition, we expect that LSA willperform better when we use context selection on thedocuments.2 Related WorkLSA was originally applied to Information Retrieval(Dumais et al, 1988) and called Latent Semantic In-dexing (LSI).
It is based on the singular value de-composition (SVD) theorem.
A m ?
n matrix Xwith m ?
n can be written as X = U ?S ?V T whereU is a m ?
n matrix such that UT ?
U = Im; S isa n?n diagonal matrix whose diagonal coefficientsare in decreasing order; and V is a n?n matrix suchthat V T ?
V = In.X is typically a term-document matrix that repre-sents the occurrences of vocabulary words in a set ofdocuments.
LSI uses truncated SVD, that is it con-siders the first r columns of U (written Ur), the rhighest coefficients in S (Sr) and the first r columnsof V (Vr).
Similarity between a query and a docu-ment represented by vectors d and q is performed bycomputing the cosine similarity between S?1r ?UTr ?dand S?1r ?UTr ?q.
The motivation for computing sim-ilarity in a different space is to cope with the sparsityof the vectors in the original space.
The motivationfor truncating SVD is that only the most meaning-ful semantic components of the document and thequery are represented after this transformation andthat noise is discarded.LSA was subsequently applied to number of prob-lems, such as synonym detection (Landauer et al,1998), document clustering (Song and Park, 2007),vocabulary acquisition simulation (Landauer andDumais, 1997), etc.Levin and colleagues (2006) applied LSA to wordsense discrimination.
They clustered documentscontaining ambiguous words and for a test instanceof a document, they assigned the document to itsclosest cluster.
Our approach is to assign to a doc-ument the question that is closest.
In addition, weexamine the cases where a word has several unre-lated meanings and where a word has several closelyrelated meanings.3 Experimental SetupWe used a database of 62 manually generated clozequestions covering 16 target words1.
We manuallyannotated the senses of the target words in thesequestions using WordNet senses (Fellbaum, 1998).For each word and for each sense, we manually gath-ered documents from the Web containing the targetword with the corresponding sense.
There were 84documents in total.
We added 97 documents ex-tracted from the tutor database of documents thatcontained at least one target word but we did not an-notate their meaning.We wanted to evaluate the performances of LSAfor WSD for words with unrelated meanings and forwords with both related and unrelated meanings.
Forthe first type of evaluation, we retained four targetwords.
For the second type of evaluation, all 16words were included.
We also wanted to evaluatethe influence of the size of the context of the tar-get words.
We therefore considered two matrices:a term-document matrix and a term-context matrixwhere context designates five sentences around thetarget word in the document.
In both cases eachcell of the matrix had a tf-idf weight.
Finally, wewanted to investigate the influence of the dimensionreduction on performance.
In our experiments, weexplored these three directions.4 Results4.1 BaselineWe first used a variant of the Lesk algorithm (Lesk,1986), which is based on word exact match.
This al-gorithm seems well suited for the unsupervised ap-proach we took here since we were dealing withdiscrimination rather than disambiguation.
Givena document d and a question q, we computed thenumber of word tokens that were shared between dand q, excluding the target word.
The words werelower cased and stemmed using the Porter stem-mer.
Stop words and punctuation were discarded;we used the standard English stopword list.
Finally,we selected a window of nw words around the tar-get word in the question q and a window of nssentences around the target word in the documentd.
In order to detect sentence boundaries, we used1available at: www.cs.cmu.edu/ jmpino/questions.xls44the OpenNLP toolkit (Baldridge et al, 2002).
Withnw = 10 and ns = 2, we obtained an accuracy of61% for the Lesk algorithm.
This can be comparedto a random baseline of 44% accuracy.4.2 LSAWe indexed the document database using the Lemurtoolkit (Allan et al, 2003).
The database containedboth the manually annotated documents and the doc-uments used by the tutor and containing the targetwords.
The Colt package (Binko et al, ) was usedto perform singular value decomposition and matrixoperations because it supports sparse matrix oper-ations.
We explored three directions in our analy-sis.
We investigated how LSA performs for wordswith related meanings and for words with unrelatedmeanings.
We also explored the influence of thetruncation parameter r. Finally, we examined if re-ducing the document to a selected context of the tar-get word improved performance.Figures 2 and 3 plot accuracy versus dimensionreduction in different cases.
In all cases, LSA out-performs the baseline for certain values of the trun-cation parameter and when context selection wasused.
This shows that LSA is well suited for measur-ing semantic similarity between two contexts whenat least one of them is short.
In general, using the fulldimension in SVD hurts the performances.
Dimen-sion reduction indeed helps discarding noise andnoise is certainly present in our experiments sincewe do not perform stemming and do not use a stop-word list.
One could argue that filling the matrixcells with tf-idf weights already gives less impor-tance to noisy words.Figure 2 shows that selecting context in docu-ments does not give much improvement in accuracy.It might be that the amount of context selected de-pends on each document.
Here we had a fixed sizecontext of five sentences around the target word.In Figure 3, selecting context gives some im-provement, although not statistically significant,over the case with the whole document as context.The best performance obtained for words with un-related meanings and context selection is also betterthan the performance for words with related and un-related meanings.00.20.40.60.8160  80  100  120  140  160  180AccuracyTruncation ParameterAccuracy vs. Dimension Reduction for Related Meanings with Different Contextswhole documentselected contextLesk baselineFigure 2: Accuracy vs. r, the truncation parameter,for words with related and unrelated meanings and withwhole document or selected context (95% confidence forwhole document: [0.59; 0.65], 95% confidence for se-lected context: [0.52; 0.67])00.20.40.60.815  10  15  20  25  30  35AccuracyTruncation ParameterAccuracy vs. Dimension Reduction for Unrelated Meanings with Different Contextswhole documentselected contextLesk baselineFigure 3: Accuracy vs. r, the truncation parameter, forwords with unrelated meanings only and with whole doc-uments or selected context ((95% confidence for wholedocument: [0.50; 0.59], 95% confidence for selected con-text: [0.52; 0.71]))455 DiscussionLSA helps overcome sparsity of short contexts suchas questions and gives an improvement over the ex-act match baseline.
However, reducing the contextof the documents to five sentences around the tar-get word does not seem to give significant improve-ment.
This might be due to the fact that capturingthe right context for a meaning is a difficult taskand that a fixed size context does not always rep-resent a relevant context.
It is yet unclear how to setthe truncation parameter.
Although dimension re-duction seems to help, better results are sometimesobtained when the truncation parameter is close tofull dimension or when the truncation parameter isfarther from the full dimension.6 Conclusion and Future WorkWe have shown that LSA, which can be consideredas a second-order representation of the documentsand question vectors, is better suited than the Leskalgorithm, which is a first-order representation ofvectors, for measuring semantic similarity betweena short context such as a question and a longer con-text such as a document.
Dimension reduction wasshown to play an important role in the performances.However, LSA is relatively difficult to apply to largeamounts of data because SVD is computationally in-tensive when the vocabulary size is not limited.
Inthe context of tutoring systems, LSA could not beapplied on the fly, the documents would need to bepreprocessed and annotated beforehand.We would like to further apply this promisingtechnique for WSD.
Our tutor is able to provide def-initions when a student is reading a document.
Wecurrently provide all available definitions.
It wouldbe more beneficial to present only the definitionsthat are relevant to the meaning of the word in thedocument or at least to order them according to theirsemantic similarity with the context.
We would alsolike to investigate how the size of the selected con-text in a document can affect performance.
Finally,we would like to compare LSA performance to othersecond-order vector representations such as vectorsinduced from co-occurrence statistics.AcknowledgmentsThanks Mehrbod Sharifi, David Klahr and KenKoedinger for fruitful discussions.
This research issupported by NSF grant SBE-0354420.
Any conclu-sions expressed here are those of the authors.ReferencesJames Allan, Jamie Callan, Kevin Collins-Thompson,Bruce Croft, Fangfang Feng, David Fisher, John Laf-ferty, Leah Larkey, Thi N. Truong, Paul Ogilvie, et al2003.
The lemur toolkit for language modeling andinformation retrieval.Jason Baldridge, Thomas Morton, and Gann Bierner.2002.
The opennlp maximum entropy package.
Tech-nical report, Technical report, SourceForge.Pavel Binko, Dino Ferrero Merlino, Wolfgang Hoschek,Tony Johnson, Andreas Pfeiffer, et al Open sourcelibraries for high performance scientific and technicalcomputing in java.Kevyn Collins-Thompson and Jamie Callan.
2005.Predicting reading difficulty with statistical languagemodels.
Journal of the American Society for Informa-tion Science and Technology, 56(13):1448?1462.Susane T. Dumais, George W. Furnas, Thomas K.Landauer, Scott Deerwester, and Richard Harshman.1988.
Using latent semantic analysis to improve ac-cess to textual information.
In Proceedings of theSIGCHI conference on Human factors in computingsystems, pages 281?285.Christiane Fellbaum.
1998.
WordNet: An electronic lex-ical database.
MIT press.Thomas K. Landauer and Susan T. Dumais.
1997.
A so-lution to plato?s problem: The latent semantic analysistheory of acquisition, induction and representation ofknowledge.
Psychological review, 104:211?240.Thomas K. Landauer, Peter W. Foltz, and Darrell La-ham.
1998.
An introduction to latent semantic analy-sis.
Discourse processes, 25:259?284.Michael Lesk.
1986.
Automatic sense disambiguationusing machine readable dictionaries: How to tell a pinecone from an ice cream cone.
In Proceedings of the5th annual international conference on Systems Docu-mentation, pages 24?26.Esther Levin, Mehrbod Sharifi, and Jerry Ball.
2006.Evaluation of utility of lsa for word sense discrimina-tion.
In Proceedings of HLT/NAACL, pages 77?80.Wei Song and Soon Cheol Park.
2007.
A novel docu-ment clustering model based on latent semantic analy-sis.
In Proceedings of the Third International Confer-ence on Semantics, Knowledge and Grid, pages 539?542.46
