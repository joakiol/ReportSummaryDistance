Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 638?647,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsA Bayesian Mixture Model for Part-of-Speech InductionUsing Multiple FeaturesChristos ChristodoulopoulosSchool of InformaticsUniversity of Edinburghchristos.c@ed.ac.ukSharon GoldwaterSchool of InformaticsUniversity of Edinburghsgwater@inf.ed.ac.ukMark SteedmanSchool of InformaticsUniversity of Edinburghsteedman@inf.ed.ac.ukAbstractIn this paper we present a fully unsupervisedsyntactic class induction system formulated asa Bayesian multinomial mixture model, whereeach word type is constrained to belong to asingle class.
By using a mixture model ratherthan a sequence model (e.g., HMM), we areable to easily add multiple kinds of features,including those at both the type level (mor-phology features) and token level (context andalignment features, the latter from parallel cor-pora).
Using only context features, our sys-tem yields results comparable to state-of-theart, far better than a similar model without theone-class-per-type constraint.
Using the addi-tional features provides added benefit, and ourfinal system outperforms the best publishedresults on most of the 25 corpora tested.1 IntroductionResearch on unsupervised learning for NLP has be-come widespread recently, with part-of-speech in-duction, or syntactic class induction, being a partic-ularly popular task.1 However, despite a recent pro-liferation of syntactic class induction systems (Bie-mann, 2006; Goldwater and Griffiths, 2007; John-son, 2007; Ravi and Knight, 2009; Berg-Kirkpatricket al, 2010; Lee et al, 2010), careful compari-son indicates that very few systems perform betterthan some much simpler and quicker methods datingback ten or even twenty years (Christodoulopoulos1The task is more commonly referred to as part-of-speechinduction, but we prefer the term syntactic class induction sincethe induced classes may not coincide with part-of-speech tags.et al, 2010).
This fact suggests that we should con-sider which features of the older systems led to theirsuccess, and attempt to combine these features withsome of the machine learning methods introducedby the more recent systems.
We pursue this strat-egy here, developing a system based on Bayesianmethods where the probabilistic model incorporatesseveral insights from previous work.Perhaps the most important property of our modelis that it is type-based, meaning that all tokens ofa given word type are assigned to the same clus-ter.
This property is not strictly true of linguisticdata, but is a good approximation: as Lee et al(2010) note, assigning each word type to its mostfrequent part of speech yields an upper bound ac-curacy of 93% or more for most languages.
Sincethis is much better than the performance of cur-rent unsupervised syntactic class induction systems,constraining the model in this way seems likely toimprove performance by reducing the number ofparameters in the model and incorporating usefullinguistic knowledge.
Both of the older systemsdiscussed by Christodoulopoulos et al (2010), i.e.,Clark (2003) and Brown et al (1992), included thisconstraint and achieved very good performance rel-ative to token-based systems.
More recently, Lee etal.
(2010) presented a new type-based model, andalso reported very good results.A second property of our model, which distin-guishes it from the type-based Bayesian model ofLee et al (2010), is that the underlying probabilisticmodel is a clustering model, (specifically, a multino-mial mixture model) rather than a sequence model(HMM).
In this sense, our model is more closely re-638lated to several non-probabilistic systems that clus-ter context vectors or lower-dimensional represen-tations of them (Redington et al, 1998; Schu?tze,1995; Lamar et al, 2010).
Sequence models areby far the most common method of supervised part-of-speech tagging, and have also been widely usedfor unsupervised part-of-speech tagging both withand without a dictionary (Smith and Eisner, 2005;Haghighi and Klein, 2006; Goldwater and Griffiths,2007; Johnson, 2007; Ravi and Knight, 2009; Lee etal., 2010).
However, systems based on context vec-tors have also performed well in these latter scenar-ios (Schu?tze, 1995; Lamar et al, 2010; Toutanovaand Johnson, 2007) and present a viable alternativeto sequence models.One advantage of using a clustering model ratherthan a sequence model is that the features used forclustering need not be restricted to context words.Additional types of features can easily be incorpo-rated into the model and inference procedure usingthe same general framework as in the basic modelthat uses only context word features.
In particu-lar, we present two extensions to the basic model.The first uses morphological features, which serveas cues to syntactic class and seemed to partly ex-plain the success of two best-performing systemsanalysed by Christodoulopoulos et al (2010).
Thesecond extension to our model uses alignment fea-tures gathered from parallel corpora.
Previous worksuggests that using parallel text can improve perfor-mance on various unsupervised NLP tasks (Naseemet al, 2009; Snyder and Barzilay, 2008).We evaluate our model on 25 corpora in 20 lan-guages that vary substantially in both syntax andmorphology.
As in previous work (Lee et al, 2010),we find that the one-class-per-type restriction boostsperformance considerably over a comparable token-based model and yields results that are comparableto state-of-the-art even without the use of morphol-ogy or alignment features.
Including morphologyfeatures yields the best published results on 14 or 15of our 25 corpora (depending on the measure) andalignment features can improve results further.2 ModelsOur model is a multinomial mixture model withBayesian priors over the mixing weights ?
and?
?z?
?
fZMnjFigure 1: Plate diagram of the basic model with a singlefeature per token (the observed variable f ).
M , Z, andnj are the number of word types, syntactic classes z, andfeatures (= tokens) per word type, respectively.multinomial class output parameters ?.
The modelis defined so that all observations associated witha single word type are generated from the samemixing component (syntactic class).
In the basicmodel, these observations are token-level features;the morphology model adds type-level features aswell.
We begin by describing the simplest version ofour model, where each word token is associated witha single feature, for example its left context word(the word that occurs to its left in the corpus).
Wethen show how to generalise the model to multipletoken-level features and to type-level features.2.1 Basic modelIn the basic model, each word token is representedby a single feature such as its left context word.These features are the observed data; the model ex-plains the data by assuming that it has been gener-ated from some set of latent syntactic classes.
Theith class is associated with a multinomial parametervector ?i that defines the distribution over featuresgenerated from that class, and with a mixing weight?i that defines the prior probability of that class.
?and ?i are drawn from symmetric Dirichlet distribu-tions with parameters ?
and ?
respectively.The generative story goes as follows: First, gen-erate the prior class probabilities ?.
Next, for each639word type j = 1 .
.
.M , choose a class assignment zjfrom the distribution ?.
For each class i = 1 .
.
.
Z,choose an output distribution over features ?i.
Fi-nally, for each token k = 1 .
.
.
nj of word type j,generate a feature fjk from ?zj , the distribution as-sociated with the class that word type j is assignedto.
The model is illustrated graphically in Figure 1and is defined formally as follows:?
|?
?
Dirichlet(?
)zj | ?
?
Multinomial(?
)?i |?
?
Dirichlet(?
)fjk |?zj ?
Multinomial(?zj )In addition to the variables defined above, we willuse F to refer to the number of different possiblevalues a feature can take on (so that ?
is a Z ?
Fmatrix).
Thus, one way to think of the model is as avector-based clustering system, where word type j isassociated with a 1?F vector of feature counts rep-resenting the features of all nj tokens of j, and thesevectors are clustered into similar classes.
The differ-ence from other vector-based syntactic class induc-tion systems is in the method of clustering.
Here,we define a Gibbs sampler that samples from theposterior distribution of the clusters given the ob-served features; other systems have used variousstandard distance-based vector clustering methods.Some systems also include dimensionality reduction(Schu?tze, 1995; Lamar et al, 2010) to reduce thesize of the context vectors; we simply use theF mostcommon words as context features.2.2 InferenceAt inference time we want to sample a syntacticclass assignment z from the posterior of the model.We use a collapsed Gibbs sampler, integrating outthe parameters ?
and ?
and sampling from the fol-lowing distribution:P (z|f , ?, ?)
?
P (z|?
)P (f |z, ?).
(1)Rather than sampling the joint class assignmentP (z|f , ?, ?)
directly, the sampler iterates over eachword type j, resampling its class assignment zjgiven the current assignments z?j of all other wordtypes.
The posterior over zj can be computed asP (zj | z?j , f , ?, ?)?
P (zj | z?j , ?, ?
)P (fj | f?j , z, ?, ?)
(2)where fj are the features associated with word typej (one feature for each token of j).
The first (prior)factor is easy to compute due to the conjugacy be-tween the Dirichlet and multinomial distributions,and is equal toP (zj = z | z?j , ?)
=nz + ?n?
+ Z?
(3)where nz is the number of types in class z and n?is the total number of word types in all classes.
Allcounts in this and the following equations are com-puted with respect to z?j (e.g., n?
= M ?
1).Computing the second (likelihood) factor isslightly more complex due to the dependencies be-tween the different variables in fj that are inducedby integrating out the ?
parameters.
Consider first asimple case where word type j occurs exactly twicein the corpus, so fj contains two features.
The prob-ability of the first feature fj1 is equal toP (fj1 = f | zj = z, z?j , f?j , ?)
=nf,z + ?n?,z + F?
(4)where nf,z is the number of times feature f has beenseen in class z, n?,z is the total number of featuretokens in the class, and F is the number of differentpossible features.The probability of the second feature fj2 can becalculated similarly, except that it is conditioned onfj1 in addition to the other variables, so the countsfor previously observed features must include thecounts due to fj1 as well as those due to f?j .
Thus,the probability isP (fj2 = f | fj1, zj = z, z?j , f?j , ?
)= nf,z + ?
(fj1, fj2) + ?n?,z + 1 + F?
(5)where ?
is the Kronecker delta function, equal to 1if its arguments are equal and 0 otherwise.Extending this example to the general case, theprobability of a sequence of features fj is computedusing the chain rule, where the counts used in eachfactor are incremented as necessary for each addi-tional conditioning feature, yielding the followingexpression:P (fj | f?j , zj = z, z?j , ?
)=?Fk=1?njk?1i=0 (njk,z + i + ?
)?nj?1i=0 (n?,z + i + F?
)(6)640where njk is the number of instances of feature k inword type j.22.3 Extended modelsWe can extend the model above in two differentways: by adding more features at the word tokenlevel, or by adding features at the type level.
To addmore token-level features, we simply assume thateach word token generates multiple features, onefeature from each of several different kinds.3 Forexample, the left context word might be one kind offeature and the right context word another.
We as-sume conditional independence between the gener-ated features given the syntactic class, so each kindof feature t has its own output parameters ?(t).
Aplate diagram of the model with T kinds of featuresis shown in Figure 2 (a type-level feature is also in-cluded in this diagram, as described below).Due to the independence assumption between thedifferent kinds of features, the basic Gibbs sampleris easy to extend to this case by simpling multiplyingin extra factors for the additional kinds of features,with the prior (Equation 3) unchanged.
The likeli-hood becomes:P (f (1)j , .
.
.
, f(T )j | f(1...T )?j , zj = z, z?j , ?
)=T?t=1P (f (t)j | f(t)?j , zj = z, z?j , ?)
(7)where each factor in the product is computed usingEquation 6.In addition to monolingual context features, wealso explore the use of alignment features for thoselanguages where we have parallel corpora.
Thesefeatures are extracted for language ?
by word-aligning ?
to another language ??
(details of thealignment procedure are described in Section 3.1).The features used for each token e in ?
are the leftand right context words of the word token that isaligned to e (if there is one).
As with the mono-lingual context features, we use only the F most fre-quent words in ??
as possible features.2One could approximate this likelihood term by assumingindependence between all nj feature tokens of word type j.This is the approach taken by Lee et al (2010).3We use the word kind here to avoid confusion with type,which we reserve for the type-token distinction, which can ap-ply to features as well as words.Note that this model with multiple context fea-tures is deficient: it can generate data that are in-consistent with any actual corpus, because there isno mechanism to constrain the left context wordof token ei to be the same as the right contextword of token ei?1 (and similarly with alignmentfeatures).
However, deficient models have provenuseful in other unsupervised NLP tasks (Klein andManning, 2002; Toutanova and Johnson, 2007).
Inparticular, Toutanova and Johnson (2007) demon-strate good performance on unsupervised part-of-speech tagging (using a dictionary) with a Bayesianmodel similar to our own.
If we remove the part oftheir model that relies on the dictionary (the mor-phological ambiguity classes), their model is equiv-alent to our own, without the restriction of one classper type.
We use this token-based version of ourmodel as a baseline in our experiments.The final extension to our model introduces type-level features, specifically morphology features.The model is illustrated in Figure 2.
We assumeconditional independence between the morphologyfeatures and other features, so again we can simplymultiply another factor into the likelihood during in-ference.
There is only one morphological feature pertype, so this factor has the form of Equation 4.
Sincefrequent words will have many token-level featurescontributing to the likelihood and only one morphol-ogy feature, the morphology features will have agreater effect for infrequent words (as appropriate,since there is less evidence from context and align-ments).
As with the other kinds of features, we useonly a limited number Fm of morphology features,as described below.3 Experiments3.1 Experimental setupWe evaluate our models using an increasing levelof complexity, starting with a model that uses onlymonolingual context features.
We use the F = 100most frequent words as features, and consider twoversions of this model: one with two kinds of fea-tures (one left and one right context word) and onewith four (two context words on each side).For the model with morphology features we ranthe unsupervised morphological segmentation sys-tem Morfessor (Creutz and Lagus, 2005) to get a641?
?zf (1) ?
(1) ?(1).
.
.
.
.
.
.
.
.f (T ) ?
(T ) ?
(T )m?
(m) ?
(m)MnjnjZZZFigure 2: Plate diagram of the extended model with Tkinds of token-level features (f (t) variables) and a singlekind of type-level feature (morphology, m).segmentation for each word type in the corpus.
Wethen extracted the suffix of each word type4 and usedit as a feature type.
This process yielded on averageFm = 110 morphological feature types5.
Each wordtype generates at most one of these possible features.If there are overlapping possibilities (e.g.
-ingly and-y) we take the longest possible match.We also explore the idea of extending the mor-phology feature space beyond suffixes, by includingfeatures like capitalisation and punctuation.
Specif-ically we use the features described in Haghighiand Klein (2006), namely initial-capital, contains-hyphen, contains-digit and we add an extra featurecontains-punctuation.For the model with alignment features, we fol-low (Naseem et al, 2009) in using only bidirectionalalignments: using Giza++ (Och and Ney, 2003),we get the word alignments in both directions be-tween all possible language pairs in our parallel cor-pora (i.e., alternating the source and target languageswithin each pair).
We then use only those align-ments that are found in both directions.
As discussed4Since Morfessor yields multiple affixes for each word weconcatenated all the suffixes into a single suffix.5There was large variance in the number of feature types foreach language ranging from 11 in Chinese to more than 350 inGerman and Czech.above, we use two kinds of alignment features: theleft and right context words of the aligned token inthe other language.
The feature space is set to theF = 100 most frequent words in that language.Instead of fixing the hyperparameters ?
and ?, weused the Metropolis-Hastings sampler presented byGoldwater and Griffiths (2007) to get updated valuesbased on the likelihood of the data with respect tothose hyperparameters6.
In order to improve conver-gence of the sampler, we used simulated annealingwith a sigmoid-shaped cooling schedule from an ini-tial temperature of 2 down to 1.
Preliminary experi-ments indicated that we could achieve better resultsby cooling even further (approximating the MAP so-lution rather than a sample from the posterior), so forall experiments reported here, we ran the sampler fora total of 2000 iterations, with the last 400 of thesedecreasing the temperature from 1 to 0.66.Finally, we investigated two different initialisa-tion techniques: First, we use random class as-signments to word types (referred to as method 1)and second, we assign each of the Z most frequentword types to a separate class and then randomlydistribute the rest of the word types to the classes(method 2).3.2 DatasetsAlthough unsupervised systems should in principlebe language- and corpus-independent, most part-of-speech induction systems (especially in the early lit-erature) have been developed on English.
Whetherbecause English is simply an easier language, or be-cause of bias introduced during development, thesesystems?
performance is considerably worse in otherlanguages (Christodoulopoulos et al, 2010)Since we aim to use our system mostly on non-English corpora, and ones that are significantlysmaller than the large English treebank corpora, wedeveloped our models using one of the languages ofthe MULTEXT-East corpus (Erjavec, 2004), namelyBulgarian.
The other languages in the corpus wereused during development as a source of word align-ments, but otherwise were only used for testing finalversions of our models.
Since none of the authorsspeak any of the languages in the MULTEXT col-6For simplicity, we tied the ?
parameters for the two or fourkinds of context features to the same value, and similarly the ?parameters for the two kinds of alignment features.642lection, we also used the Penn Treebank WSJ cor-pus (Marcus et al, 1993) for development.
Fol-lowing Christodoulopoulos et al (2010) we createda smaller version of the WSJ corpus (referred toas wsj-s) to approximate the size of the corpora inMULTEXT-East.
For comparison to other systems,we also used the full WSJ at test time.For further testing, we used the remaining MUL-TEXT languages, as well as the languages of theCONNL-X (Buchholz and Marsi, 2006) shared task.This dataset contains 13 languages, 4 of whichare freely available (Danish, Dutch, Portugueseand Swedish) and 9 that are used with permissionfrom the creators of the corpora ( Arabic7, Bul-garian8, Czech9, German10, Chinese11, Japanese12,Slovene13, Spanish14, Turkish15 ).
Following Lee etal.
(2010) we used only the training sections for eachlanguage.Finally, to widen the scope of our system, we gen-erated two more corpora in French16 and AncientGreek17, extracting the gold standard parts of speechfrom the respective dependency treebanks.3.3 BaselinesWe chose three baselines for comparison.
The firstis the basic k-means clustering algorithm, which weapplied to the same feature vectors we extracted forour system (context + extended morphology), usinga Euclidean distance metric.
This provides a verysimple vector-based clustering baseline.
The secondbaseline is a more recent vector-based syntactic classinduction method, the SVD approach of (Lamar etal., 2010), which extends Schu?tze (1995)?s originalmethod and, like ours, enforces a one-class-per-tagrestriction.
As a third baseline we use the system ofClark (2003) since it is a type-level system that mod-7Part of the Prague Arabic Treebank (Hajic?
et al, 2003;Smrz?
and Pajas, 2004)8Part of the BulTreeBank (Simov et al, 2004).9Part of the Prague Dep.
Treebank (Bo?hmova?
et al, 2001)10Part of the TIGER Treebank (Brants et al, 2002)11Part of the Sinica Treebank (Keh-Jiann et al, 2003)12Part of the Tu?bingen Treebank of Spoken Japanese (for-merly VERMOBIL Treebank - Kawata and Bartels (2000)).13Part of the Slovene Dep.
Treebank (Dz?eroski et al, 2006)14Part of the Cast3LB Treebank (Civit et al, 2006)15Part of the METU-Sabanci Treebank (Oflazer et al, 2003).16French Treebank (Abeille?
et al, 2000)17Greek Dependency Treebank (Bamman et al, 2009)els morphology and has produced very good resultson multilingual corpora.4 Results and Analysis4.1 Development resultsTables 1 and 2 present the results from develop-ment runs, which were used to decide which fea-tures to incorporate in the final system.
We used V-Measure (Rosenberg and Hirschberg, 2007) as ourprimary evaluation score, but also present many-to-one matching accuracy (M-1) scores for better com-parison with previously published results.
We choseV-Measure (VM) as our evaluation score because itis less sensitive to the number of classes induced bythe model (Christodoulopoulos et al, 2010), allow-ing us to develop our models without using the num-ber of classes as a parameter.
We fixed the numberof classes in all systems to 45 during development;note however that the gold standard tag set for Bul-garian contains only 12 tags, so the results in Ta-ble 1 (especially the M-1 scores) are not comparableto previous results.
For results using the number ofgold-standard tags refer to Table 4.The first conclusion that can be drawn from theseresults is the large difference between the token-and type-based versions of our system, which con-firms that the one-class-per-type restriction is help-ful for unsupervised syntactic class induction.
Wealso see that for both languages, the performance ofthe model using 4 context words (?2 on each side) isworse than the 2 context words model.
We thereforeused only two context words for all of our additionaltest languages (below).We can clearly see that morphological featuresare helpful in both languages; however the extendedfeatures of Haghighi and Klein (2006) seem to helponly on the English data.
This could be due to thefact that Bulgarian has a much richer morphologyand thus the extra features contribute little to theoverall performance of the model.The contribution of the alignment features on theBulgarian corpus (aligned with English) is less sig-nificant than that of morphology but when com-bined, the two sets of features yield the best per-formance.
This provides evidence in favor of usingmultiple features.Finally, initialisation method 2 does not yield643system ?1 words ?2 wordsVM/M-1 VM/M-1base 58.1 / 70.8 55.4 / 67.6base(tokens) 48.3 / 62.5 37.0 / 54.4base(init) 57.6 / 70.1 56.1 / 68.6+morph 58.3 / 74.9 57.4 / 71.9+morph(ext) 57.8 / 73.7 57.8 / 70.1(init)+morph 57.8 / 74.3 57.3 / 69.5(init)+morph(ext) 58.1 / 74.3 57.2 / 71.3+aligns(EN) 58.1 / 72.6 56.7 / 71.1+aligns(EN)+morph 59.0 / 75.4 57.5 / 69.7Table 1: V-measure (VM) and many-to-one (M-1) resultson the MULTEXT-Bulgarian corpus for various mod-els using either ?1 or ?2 context words as features.base: context features only; (tokens): token-based model;(init): Initialisation method 2?other results use method1; (ext): Extended morphological features.system ?1 words ?2 wordsVM/M-1 VM/M-1base 63.3 / 64.3 62.4 / 63.3base(tokens) 48.6 / 57.8 49.3 / 38.3base(init) 62.7 / 62.9 62.2 / 62.4+morph 66.4 / 66.7 65.1 / 67.2+morph(ext) 67.7 / 72.0 65.6 / 67.0(init)+morph 64.8 / 66.9 64.2 / 66.0(init)+morph(ext) 67.4 / 71.3 65.7 / 67.1Table 2: V-measure and many-to-one results on the wsj-scorpus for various models, as described in Table 1..consistent improvements over the standard ran-dom initialisation?if anything, it seems to performworse.
We therefore use only method 1 in the re-maining experiments.4.2 Overall resultsTable 3 presents the results on our parallel corpora.We tested all possible combinations of two lan-guages to align, and present both the average scoreover all alignments, and the score under the bestchoice of aligned language.18 Also shown are theresults of adding morphology features to the basicmodel (context features only) and to the best align-ment model for each language.
In accord with our18The choice of language was based on the same test data, sothe ?best-language?
results should be viewed as oracle scores.development results, adding morphology to the ba-sic model is generally useful.
The alignment resultsare mixed: on the one hand, choosing the best pos-sible language to align yields improvements, whichcan be improved further by adding morphologicalfeatures, resulting in the best scores of all modelsfor most languages.
On the other hand, withoutknowing which language to choose, alignment fea-tures do not help on average.
We note, however,that three out of the seven languages have Englishas their best-aligned pair (perhaps due to its betteroverall scores), which suggests that in the absenceof other knowledge, aligning with English may be agood choice.The low average performance of the alignmentfeatures is disappointing, but there are many pos-sible variations on our method for extracting thesefeatures that we have not yet tested.
For example,we used only bidirectional alignments in an effort toimprove alignment precision, but these alignmentstypically cover less than 40% of tokens.
It is pos-sible that a higher-recall set of alignments could bemore useful.We turn now to our results on all 25 corpora,shown in Table 4 along with corpus statistics, base-line results, and the best published results for eachlanguage (when available).
Our system, includ-ing morphology features in all cases, is listed asBMMM (Bayesian Multinomial Mixture Model).We do not include alignment features for the MUL-TEXT languages since these features only yieldedimprovements for the oracle case where we knowwhich aligned language to choose.
Nevertheless, ourMULTEXT scores mostly outperform all other sys-tems.
Overall, we acheive the highest published re-sults on 14 (VM) or 15 (M-1) of the 25 corpora.One surprising discovery is the high performanceof the k-means clustering system.
Despite its sim-plicity, it is competitive with the other systems andin a few cases even achieves the best published re-sults.5 ConclusionWe have presented a Bayesian model for syntacticclass induction that has two important properties.First, it is type-based, assigning the same class toevery token of a word type.
We have shown by644BASE ALIGNMENTSLang.
base +morph Avg.
Best +morphVM/M-1 VM/M-1 VM/M-1 VM/M1 VM/M1Bulgarian 54.4 / 61.5 54.5 / 64.3 53.1 / 60.5 55.2 / 64.5(EN) 55.7 / 66.0Czech 54.2 / 58.9 53.9 / 64.2 52.6 / 58.4 53.8 / 59.7(EN) 55.4 / 66.4English 62.9 / 72.4 63.3 / 73.3 62.5 / 72.0 63.2 / 71.9(HU) 63.5 / 73.7Estonian 52.8 / 63.5 53.3 / 67.4 52.8 / 63.9 53.5 / 65.0(EN) 54.3 / 66.9Hungarian 53.3 / 60.4 54.8 / 68.2 53.3 / 60.8 53.9 / 61.1(RO) 55.9 / 67.1Romanian 53.9 / 62.4 52.3 / 61.1 56.2 / 63.7 57.5 / 64.6(ES) 54.5 / 63.4Slovene 57.2 / 65.9 56.7 / 67.9 54.7 / 64.1 55.9 / 64.4(HU) 56.7 / 67.9Serbian 49.1 / 56.6 49.0 / 62.0 47.3 / 55.6 48.9 / 59.4(CZ) 48.3 / 60.8Table 3: V-measure (VM) and many-to-one (M-1) results on the languages in the MULTEXT-East corpus usingthe gold standard number of classes shown in Table 4.
BASE results use ?1-word context features alone or withmorphology.
ALIGNMENTS adds alignment features, reporting the average score across all possible choices of pairedlanguage and the scores under the best performing paired language (in parens), alone or with morphology features.Language Types Tags k-means SVD2 clark Best Pub.
BMMMWSJ wsj 49,190 45 59.5 / 61.6 58.2 / 64.0 65.6 / 71.2 68.8 / 76.1?
66.1 / 72.8wsj-s 16,850 45 56.7 / 60.1 54.3 / 60.7 63.8 / 68.8 62.3 / 70.7?
67.7 / 72.0MULTEXT-EastBulgarian 16,352 12 50.3 / 59.3 41.7 / 51.0 55.6 / 66.5 - 54.5 / 64.4Czech 19,115 12 48.6 / 56.7 35.5 / 50.9 52.6 / 64.1 - 53.9 / 64.2English 9,773 12 56.5 / 65.4 52.3 / 65.5 60.5 / 70.6 - 63.3 / 73.3Estonian 17,845 11 45.3 / 55.6 38.7 / 55.3 44.4 / 58.4 - 53.3 / 64.4Hungarian 20,321 12 46.7 / 53.9 39.8 / 49.5 48.9 / 61.4 - 54.8 / 68.2Romanian 15,189 14 45.2 / 55.1 42.1 / 52.6 40.9 / 49.9 - 52.3 / 61.1Slovene 17,871 12 46.9 / 56.2 39.5 / 54.2 54.9 / 69.4 - 56.7 / 67.9Serbian 18,095 12 41.4 / 47.0 39.1 / 54.6 51.0 / 64.1 - 49.0 / 62.0CoNLL06SharedTaskArabic 12,915 20 43.3 / 60.7 27.6 / 49.0 40.6 / 59.8 - 42.4 / 61.5Bulgarian 32,439 54 53.6 / 65.6 49.0 / 65.3 59.6 / 70.4 - 58.8 / 68.9Chinese 40,562 15 32.6 / 61.1 24.5 / 54.6 31.8 / 56.7 - 42.6 / 69.4Czech 130,208 12 - - 47.1 / 65.5 - 48.4 / 65.7Danish 18,356 25 51.7 / 61.6 40.8 / 57.6 52.7 / 65.3 - / 66.7?
59.0 / 71.1Dutch 28,393 13 45.3 / 60.5 36.7 / 52.4 52.2 / 67.9 - / 67.3?
54.7 / 71.1German 72,326 54 58.7 / 67.5 54.1 / 64.2 63.0 / 73.9 - / 68.4?
61.9 / 74.4Japanese 3,231 80 76.1 / 76.2 74.4 / 75.5 78.6 / 77.4 - 77.4 / 78.5Portuguese 28,931 22 51.6 / 64.4 45.9 / 63.1 57.4 / 69.2 - / 75.3?
63.9 / 76.8Slovene 7,128 29 52.6 / 64.2 44.0 / 60.3 53.9 / 63.5 - 49.4 / 56.2Spanish 16,458 47 59.5 / 69.2 54.8 / 68.2 61.6 / 71.9 - / 73.2?
63.2 / 71.7Swedish 20,057 41 53.2 / 62.2 47.4 / 59.1 58.9 / 68.7 - / 60.6?
58.0 / 68.2Turkish 17,563 30 40.8 / 62.8 27.4 / 52.4 36.8 / 58.1 - 40.2 / 58.7French 49,964 23 48.2 / 68.6 46.3 / 68.5 57.3 / 77.8 - 55.0 / 76.6A.Greek 15,194 15 38.6 / 44.8 24.2 / 38.5 33.3 / 45.4 - 40.5 / 45.1Table 4: Final results on 25 corpora in 20 languages, with the number of induced classes equal to the number of goldstandard tags in all cases.
k-means and SVD2 models could not produce a clustering in the Czech CoNLL corpus dueits size.
Best published results are from ?Christodoulopoulos et al (2010), ?Berg-Kirkpatrick et al (2010) and ?Leeet al (2010).
The latter two papers do not report VM scores.
No best published results are shown for the MULTEXTlanguages; Christodoulopoulos et al (2010) report results based on 45 tags suggesting that clark performs best onthese corpora.645comparison with a token-based version of the modelthat this restriction is very helpful.
Second, it isa clustering model rather than a sequence model.This property makes it easy to incorporate multi-ple kinds of features into the model at either the to-ken or the type level.
Here, we experimented withtoken-level context features and alignment featuresand type-level morphology features, showing thatmorphology features are helpful in nearly all cases,and alignment features can be helpful if the alignedlanguage is properly chosen.
Our results even with-out these extra features are competitive with state-of-the-art; with the additional features we achievethe best published results in the majority of the 25corpora tested.Since it is so easy to add extra features to ourmodel, one direction for future work is to exploreother possible features.
For example, it could beuseful to add dependency features from an unsuper-vised dependency parser.
We are also interested inimproving our morphology features, either by con-sidering other ways to extract features during pre-processing (for example, including prefixes or notconcatenating together all suffixes), or by develop-ing a joint model for inducing both morphology andsyntactic classes simultaneously.
Finally, our modelcould be extended by replacing the standard mixturemodel with an infinite mixture model (Rasmussen,2000) in order to induce the number of syntacticclasses automatically.AcknowledgmentsThe authors would like to thank Emily Thomforde,Ioannis Konstas, Tom Kwiatkowski and the anony-mous reviewers for their comments and suggestions.We would also like to thank Kiril Simov, Toni Marti,Tomaz Erjavec, Jess Lin and Kathrin Beck for pro-viding us with CoNLL data.
This work was sup-ported by an EPSRC graduate Fellowship, and byERC Advanced Fellowship 249520 GRAMPLUS.ReferencesAnne Abeille?, Lionel Cle?ment, and Alexandra Kinyon.2000.
Building a treebank for French.
In In Proceed-ings of the LREC 2000.David Bamman, Francesco Mambrini, and GregoryCrane.
2009.
An ownership model of annotation: TheAncient Greek dependency treebank.
In TLT 2009-Eighth International Workshop on Treebanks and Lin-guistic Theories.Taylor Berg-Kirkpatrick, Alexandre B.
Co?te?, John DeN-ero, and Dan Klein.
2010.
Painless unsupervisedlearning with features.
In Proceedings of NAACL2010, pages 582?590, Los Angeles, California, June.Chris Biemann.
2006.
Unsupervised part-of-speech tag-ging employing efficient graph clustering.
In Proceed-ings of COLING ACL 2006, pages 7?12, Morristown,NJ, USA.Alena Bo?hmova?, Jan Hajic?, Eva Hajic?ova?, and BarboraHladka?.
2001.
The Prague dependency treebank:Three-level annotation scenario.
In Anne Abeille?, ed-itor, Treebanks: Building and Using Syntactically An-notated Corpora, pages 103 ?
126.
Kluwer AcademicPublishers.Sabine Brants, Stefanie Dipper, Silvia Hansen, WolfgangLezius, and George Smith.
2002.
The TIGER tree-bank.
In Proceedings of the Workshop on Treebanksand Linguistic Theories, Sozopol.Peter F. Brown, Vincent J. Della Pietra, Peter V. Desouza,Jennifer C. Lai, and Robert L. Mercer.
1992.
Class-based n-gram models of natural language.
Computa-tional Linguistics, 18(4):467?479.Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-X shared task on multilingual dependency parsing.In Proceedings of the Tenth Conference on Compu-tational Natural Language Learning, CoNLL-X ?06,pages 149?164, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Christos Christodoulopoulos, Sharon Goldwater, andMark Steedman.
2010.
Two decades of unsupervisedPOS induction: How far have we come?
In Proceed-ings of the 2010 Conference on Empirical Methods inNatural Language Processing, pages 575?584, Cam-bridge, MA, October.
Association for ComputationalLinguistics.Montserrat Civit, Ma.
Mart?
?, and Nu?ria Buf??.
2006.Cat3lb and cast3lb: From constituents to dependen-cies.
In Tapio Salakoski, Filip Ginter, Sampo Pyysalo,and Tapio Pahikkala, editors, Advances in NaturalLanguage Processing, volume 4139 of Lecture Notesin Computer Science, pages 141?152.
Springer Berlin/ Heidelberg.Alexander Clark.
2003.
Combining distributional andmorphological information for part of speech induc-tion.
In Proceedings of EACL 2003, pages 59?66,Morristown, NJ, USA.Mathias Creutz and Krista Lagus.
2005.
Induc-ing the morphological lexicon of a natural languagefrom unannotated text.
In In Proceedings of theInternational and Interdisciplinary Conference on646Adaptive Knowledge Representation and Reasoning(AKRR?05), volume 5, pages 106?113.Sas?o Dz?eroski, Tomaz?
Erjavec, Nina Ledinek, Petr Pajas,Zdenek Z?abokrtsky, and Andreja Z?ele.
2006.
Towardsa Slovene dependency treebank.
In Proceedings Int.Conf.
on Language Resources and Evaluation (LREC).Tomaz?
Erjavec.
2004.
MULTEXT-East version 3: Mul-tilingual morphosyntactic specifications, lexicons andcorpora.
In Fourth International Conference on Lan-guage Resources and Evaluation, (LREC?04), pages1535 ?
1538, Paris.
ELRA.Sharon Goldwater and Tom Griffiths.
2007.
A fullybayesian approach to unsupervised part-of-speech tag-ging.
In Proceedings of ACL 2007, pages 744?751,Prague, Czech Republic, June.Aria Haghighi and Dan Klein.
2006.
Prototype-drivenlearning for sequence models.
In Proceedings ofNAACL 2006, pages 320?327, Morristown, NJ, USA.Jan Hajic?, Jarmila Panevova?, Zden?ka Ures?ova?, AlevtinaBe?mova?, and Petr Pajas.
2003.
PDTVALLEX: cre-ating a large-coverage valency lexicon for treebankannotation.
In Proceedings of The Second Workshopon Treebanks and Linguistic Theories, pages 57?68.Vaxjo University Press.Mark Johnson.
2007.
Why doesn?t EM find good HMMPOS-taggers?
In Proceedings of EMNLP-CoNLL2007, pages 296?305, Prague, Czech Republic, June.Yasushira Kawata and Julia Bartels.
2000.
Stylebookfor the Japanese treebank in VERMOBIL.
Technicalreport, Universita?t Tu?ubingen.Chen Keh-Jiann, Chu-Ren Huang, Feng-Yi Chen, Chi-Ching Luo, Ming-Chung Chang, Chao-Jan Chen, andZhao-Ming Gao.
2003.
Sinica treebank: Design cri-teria, representational issues and implementation.
InAnne Abeille?, editor, Treebanks: Building and Us-ing Syntactically Annotated Corpora, pages 231?248.Kluwer Academic Publishers.Dan Klein and Christopher D. Manning.
2002.
A gener-ative constituent-context model for improved grammarinduction.
In Proceedings of ACL 40, pages 128?135.Michael Lamar, Yariv Maron, Mark Johnson, and ElieBienenstock.
2010.
SVD and clustering for unsuper-vised POS tagging.
In Proceedings of the ACL 2010Conference Short Papers, pages 215?219, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Yoong Keok Lee, Aria Haghighi, and Regina Barzilay.2010.
Simple type-level unsupervised POS tagging.In Proceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, EMNLP?10, pages 853?861, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.Mitchell Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: the Penn Treebank.
ComputationalLinguistics, 19(2):331?330.Tahira Naseem, Benjamin Snyder, Jacob Eisenstein, andRegina Barzilay.
2009.
Multilingual Part-of-Speechtagging: Two unsupervised approaches.
Journal of Ar-tificial Intelligence Research, 36:341?385.Franz J. Och and Hermann Ney.
2003.
A systematiccomparison of various statistical alignment models.Comput.
Linguist., 29(1):19?51.Kemal Oflazer, Bilge Say, Dilek Z. Hakkani-Tu?r, andGo?khan Tu?r, 2003.
Building A Turkish Treebank,chapter 1, pages 1?17.
Kluwer Academic Publishers.Carl Rasmussen.
2000.
The infinite Gaussian mixturemodel.
In Advances in Neural Information ProcessingSystems 12.Sujith Ravi and Kevin Knight.
2009.
Minimized mod-els for unsupervised Part-of-Speech tagging.
In Pro-ceedings of ACL-IJCNLP 2009, pages 504?512, Sun-tec, Singapore, August.Martin Redington, Nick Chater, and Steven Finch.
1998.Distributional information: a powerful cue for acquir-ing syntactic categories.
Cognitive Science, 22:425 ?469.Andrew Rosenberg and Julia Hirschberg.
2007.
V-measure: A conditional entropy-based external clus-ter evaluation measure.
In Proceedings of EMNLP-CoNLL 2007, pages 410?420.Hinrich Schu?tze.
1995.
Distributional part-of-speechtagging.
In Proceedings of EACL 7, pages 141?148,San Francisco, CA, USA.Kiril Simov, Petya Osenova, Alexander Simov, andMilenKouylekov.
2004.
Design and implementation of theBulgarian HPSG-based treebank.
Research on Lan-guage &amp; Computation, 2(4):495?522.Noah A. Smith and Jason Eisner.
2005.
Contrastive esti-mation: training log-linear models on unlabeled data.In Proceedings of ACL 2005, pages 354?362, Morris-town, NJ, USA.Otakar Smrz?
and Petr Pajas.
2004.
Morphotrees of Ara-bic and their annotation in the TrEd environment.
InProceedings of the NEMLAR International Conferenceon Arabic Language Resources and Tools, pages 38?41.Ben Snyder and Regina Barzilay.
2008.
Unsupervisedmultilingual learning for morphological segmentation.In Proceedings of ACL.Kristina Toutanova and Mark Johnson.
2007.
ABayesian LDA-based model for semi-supervised part-of-speech tagging.
In Proceedings of NIPS 2007.647
