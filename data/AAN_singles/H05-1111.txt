Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 883?890, Vancouver, October 2005. c?2005 Association for Computational LinguisticsExploiting a Verb Lexicon in Automatic Semantic Role LabellingRobert S. Swier and Suzanne StevensonDepartment of Computer ScienceUniversity of TorontoToronto, Ontario, Canada M5S 3G4{swier,suzanne}@cs.toronto.eduAbstractWe develop an unsupervised semantic rolelabelling system that relies on the directapplication of information in a predicatelexicon combined with a simple probabil-ity model.
We demonstrate the usefulnessof predicate lexicons for role labelling,as well as the feasibility of modifying anexisting role-labelled corpus for evaluat-ing a different set of semantic roles.
Weachieve a substantial improvement over aninformed baseline.1 IntroductionIntelligent language technologies capable of fullsemantic interpretation of domain-general text re-main an elusive goal.
However, statistical advanceshave made it possible to address core pieces ofthe problem.
Recent years have seen a wealth ofresearch on one important component of seman-tic interpretation?automatic role labelling (e.g.,Gildea and Jurafsky, 2002; Pradhan et al, 2004; Ha-cioglu et al, 2004, and additional papers from Car-reras and Marquez, 2004).
Such work aims to an-notate each constituent in a clause with a semantictag indicating the role that the constituent plays withrespect to the target predicate, as in (1):(1) [Yuka]Agent [whispered]Pred to [Dar]RecipientSemantic role labelling systems address a crucialfirst step in the automatic extraction of semantic re-lations from domain-general text, taking us closer tothe goal of comprehensive semantic mark-up.Most work thus far on domain-general role la-belling depends on supervised learning over statis-tical features extracted from a hand-labelled corpus.The reliance on such a resource?one in which thearguments of each predicate are manually identifiedand assigned a semantic role?limits the portabilityof such methods to other languages or even to othergenres of corpora.In this study, we explore the possibility of using averb lexicon, rather than a hand-labelled corpus, asthe primary resource in the semantic role labellingtask.
Perhaps because of the focus on what canbe gleaned from labelled data, existing supervisedapproaches have made little use of the additionalknowledge available in the predicate lexicon asso-ciated with the labelled corpus.
By contrast, we ex-ploit the explicit knowledge of the role assignmentpossibilities for each verb within an existing lexi-con.
Moreover, we utilize a very simple probabilitymodel within a highly efficient algorithm.We use VerbNet (Kipper et al, 2000), a computa-tional lexicon which lists the possible semantic roleassignments for each of its verbs.
Our algorithmextracts automatically parsed arguments from a cor-pus, and assigns to each a list of the compatible rolesaccording to VerbNet.
Arguments which are givenonly a single role possibility are considered to havebeen assigned an unambiguous role label.
This setof arguments constitutes our primary-labelled data,which serves as the noisy training data for a simpleprobability model which is then used to label the re-maining (role ambiguous) arguments.This method has several advantages, the foremostof which is that it eliminates the dependence on arole labelled corpus, a very expensive resource toproduce.
Of course, a verb lexicon is also an expen-sive resource, but one that is highly reusable across arange of NLP tasks.
Moreover, the approach pointsat some potentially useful information that current883supervised methods have failed to exploit.
Even ifone has access to an annotated corpus for training,our work shows that directly calling on additionalinformation from the lexicon itself may prove usefulin restricting the possible labels for an argument.The method has disadvantages as well.
The in-formation available in a predicate lexicon is less di-rectly applicable to building a learning model.
In-evitably, our results are noisier than in a super-vised approach which has access to a labelled sam-ple of what it must produce.
Still, the method showspromise: on unseen test data, the system yields anF-measure of .83 on labelling of correctly extractedarguments, compared to an informed baseline of .74,and an F-measure of .65 (compared to .52) on theoverall identification and labelling task.
The latter iswell below the best supervised performance of about.80 on similar tasks, but it must be emphasized thatit is achieved with a simple probability model andwithout the use of hand-labelled data.
We view thisas a starting point by which to demonstrate the util-ity of deriving more explicit knowledge from a pred-icate lexicon, which can be later extended throughthe use of additional probabilistic features.We face a methodological challenge arising fromthe particular choice of VerbNet for the prototyp-ing of our method: the lexicon has no associatedsemantic role labelled corpus.
While this under-scores the need for approaches which do not relyon such a resource, it also means that we lack alabelled sample of data against which to evaluateour results.
To address this, we use the existinglabelled corpus of FrameNet (Baker et al, 1998),and develop a mapping for converting the FrameNetroles to corresponding VerbNet roles.
Our mappingmethod demonstrates the possibility of leveragingexisting resources to support the development of rolelabelling systems based on verb lexicons that do nothave an associated hand-labelled corpus.2 VerbNet Roles and the Role MappingBefore describing our labelling algorithm, we firstbriefly introduce the semantic role informationavailable in VerbNet, and describe how we mapFrameNet roles to VerbNet roles.whisperFrames:Agent VAgent V Prep(+dest) RecipientAgent V TopicVerbs in same (sub)class:[bark, croon, drone, grunt, holler, ...]Figure 1: A portion of a VerbNet entry.2.1 The VerbNet LexiconVerbNet is a manually developed hierarchical lexi-con based on the verb classification of Levin (1993).For each of almost 200 classes containing a total of3000 verbs, VerbNet specifies the syntactic framesalong with the semantic role assigned to each argu-ment position of a frame.1 Figure 1 shows an exam-ple VerbNet entry.
The thematic roles used in Verb-Net are more general than the situation-specific rolesof FrameNet.
For example, the roles Speaker, Mes-sage, and Addressee of a Communication verb suchas whisper in FrameNet would be termed Agent,Topic, and Recipient in VerbNet.
These coarser-grained roles are often assumed in linguistic the-ory, and have some advantages in terms of capturingcommonalities of argument relations across a widerange of predicates.2.2 Mapping FrameNet to VerbNet RolesAs noted, VerbNet lacks a corpus of example role as-signments against which to evaluate a role labellingbased upon it.
We create such a resource by adaptingthe existing FrameNet corpus.
We formulate a map-ping between FrameNet?s larger role set and Verb-Net?s much smaller one, and create a new corpuswith our mapped roles substituted for the originalroles in the FrameNet corpus.We perform the mapping in three steps.
First weuse an existing mapping between the semantically-specific roles in FrameNet and a much smaller inter-mediate set of 39 semantic roles which subsume allFrameNet roles.2 The associations in this mappingare straightforward?e.g., the Place role for Abusingverbs and the Area role for Operate-vehicle verbs areboth mapped to Location.1Throughout the paper we use the term ?frame?
to refer toa syntactic frame?a configuration of syntactic arguments of averb?possibly labelled with roles, as in Figure 1.2This mapping was provided by Roxana Girju, UIUC.884Second, from this intermediate set we create asimple mapping to the set of 22 VerbNet roles.
Someroles are unaffected by the mapping (e.g., Causealone in the intermediate set maps to Cause in theVerbNet set).
Other roles are merged (e.g., Degreeand Measure both map to Amount).
Moreover, someroles in FrameNet (and the intermediate set) must bemapped to more than one VerbNet role.
For exam-ple, an Experiencer role in FrameNet is consideredExperiencer by some VerbNet classes, but Agent byothers.
In such cases, our mappings in this step mustbe specific to the VerbNet class.In this second step, some roles have no subsum-ing VerbNet role, because FrameNet provides rolesfor a wider variety of relations.
For example, bothFrameNet and the intermediate role set contain aManner role, which VerbNet does not have.
Wecreate a catch-all label, ?NoRole,?
to which wemap eight such intermediate roles: Condition, Man-ner, Means, Medium, Part-Whole, Property, Pur-pose, and Result.
These phrases labelled NoRole areadjuncts?constituents not labelled by VerbNet.In the third step of our mapping, some of the rolesin VerbNet?such as Theme and Topic, Asset andAmount?which appear to be too-fine grained for usto distinguish reliably, are mapped to a more coarse-grained set of VerbNet roles.
The final set consistsof 16 roles: Agent, Amount, Attribute, Beneficiary,Cause, Destination, Experiencer, Instrument, Loca-tion, Material, Predicate, Recipient, Source, Stimu-lus, Theme and Time; plus the NoRole label.3 The Frame Matching ProcessA main goal of our system is to demonstrate theusefulness of predicate lexicons for the role la-belling task.
The primary way that we apply theknowledge in our lexicon is via a process we callframe matching, adapted from Swier and Steven-son (2004).
The automatic frame matcher alignsarguments extracted from an automatically parsedsentence with the frames in VerbNet for the targetverb in the sentence.
The output of this process isa highly constrained set of candidate roles (possi-bly of size one) for each potential argument.
Theresulting singleton sets constitute a (noisy) role as-signment for their corresponding arguments, form-ing our primary-labelled data.
This data is then usedto train a probability model, described in Section 4,which we employ to label the remaining arguments(those having more than one candidate role).3.1 Initialization of Candidate RolesThe frame matcher construes extracted argumentsfrom the parsed sentence as being in one of thefour main types of syntactic positions (or slots) usedby VerbNet frames: subject, object, indirect object,and PP-object.3 Additionally, we specialize the lat-ter by the individual preposition, such as ?object offor.?
For the first three slot types, alignment be-tween the extracted arguments and the frames is rel-atively straightforward.
An extracted subject wouldbe aligned with the subject position in a VerbNetframe, for instance, and the subject role from theframe would be listed as a possible label for the ex-tracted subject.The alignment of PP-objects is similar to thatof the other slot types, except that we add an ad-ditional constraint that the associated prepositionsmust match.
For PP-object slots, VerbNet frames of-ten provide an explicit list of allowable prepositions.Alternatively, the frame may specify a required se-mantic feature such as +path or +loc.
In orderfor an extracted PP-object to align with one of theseframe slots, its associated preposition must be in-cluded in the list provided by the frame, or have thespecified feature.
To determine the latter, we manu-ally create lists of prepositions that we judge to haveeach of the possible semantic features.In general, this matching procedure assumes thatframes describing a syntactic argument structuresimilar to that of the parsed sentence are more likelyto correctly describe the semantic roles of the ex-tracted arguments.
Thus, the frame matcher onlychooses roles from frames that are the best syntac-tic matches with the extracted argument set.
Thisis achieved by adopting the scoring method of Swierand Stevenson (2004), in which we compute the por-tion %Frame of frame slots that can be mapped toan extracted argument, and the portion %Sent ofextracted arguments from the sentence that can bemapped to the frame.
The score for each frame isgiven by %Frame+%Sent, and only frames havingthe highest score contribute candidate roles to the3Since VerbNet has very few verbs with sentential comple-ments, we do not consider them for now.885Extracted SlotsPossible Frames for Verb V SUBJ OBJ %Frame %Sent ScoreAgent V Agent 100 50 150Agent V Theme Agent Theme 100 100 200Instrument V Theme Instrument Theme 100 100 200Agent V Recipient Theme Agent Theme 67 100 167Table 1: An example of frame matching.extracted arguments.
An example scoring is shownin Table 1.
Note that two of the frames are tied forthe highest score of 200, resulting in two possibleroles for the subject (Agent and Instrument), andTheme as the only possible role for the object.As mentioned, this frame matching step is veryrestrictive, and it greatly reduces role ambiguity.Many potential arguments receive only a single can-didate role, providing the primary-labelled data weuse to train our probability model.
Some slots re-ceive no candidate roles, which is an error for argu-ment slots but which is correct for adjuncts.
The re-duction of candidate roles in general is very helpfulin lightening the subsequent load on the probabilitymodel to be applied next, but note that it may alsocause the correct role to be omitted.
We experimentwith choosing roles from the frames that are the bestsyntactic matches, and from all possible frames.3.2 Adjustments to the Role MappingWe further extend the frame matcher, which has ex-tensive knowledge of VerbNet, for the separate taskof helping to eliminate some of the inconsistenciesthat are introduced by our role mapping procedure.This is a process that applies concurrently with theinitialization of candidate roles described above, butonly affects the gold standard labelling of evaluationdata.4For instance, FrameNet assigns the role Side2 tothe object of the preposition with occurring with theverb brawl.
Side2 is mapped to Theme by our rolemapping; however, in VerbNet, brawl does not ac-cept Theme as the object of with.
Our mapping thuscreates a target (i.e., gold standard) label in the eval-uation data that is inconsistent with VerbNet.
Sincethere is no possibility of the role labeller assigning alabel that matches such a target, this unfairly raises4Of course, the fact that the frame matcher ?sees?
the evalu-ation set as part of its dual duties is not allowed to influence itsassignment of candidate roles.the task difficulty.
However, since brawl does ac-cept Theme in another slot, it is not an option toentirely eliminate this role in the mapping for theverb.
Instead, we use our frame matcher to verifythat each target role generated by our mapping fromFrameNet is allowed by VerbNet in the relevant slot.If the target role is not allowed, then it is converted toNoRole in the evaluation set.
Constituents labelledas NoRole are not considered target arguments, andit is correct for the system to not assign labels inthese cases.The NoRole conversions help to ensure that ourgold standard evaluation data is consistent with ourlexicon, but the method does have limitations.
Forinstance, some of the arguments which the sys-tem fails to extract might have had their target rolechanged to NoRole if they were properly extracted.Additionally, in some cases a target role is convertedto NoRole when there is an actual role that VerbNetwould have assigned instead.4 The Probability ModelOnce argument slots are initialized with sets of pos-sible roles, the algorithm uses a probability modelto label slots having two or more possibilities.
Sinceour primary goal is to demonstrate how much can beaccomplished through the frame matcher, we com-pare a number of very simple probability models:?
P(r|v, s): the probability of a role given thetarget verb and the slot; the latter includes sub-ject, object, indirect object, and prepositionalobject, where each PP slot is specialized by theidentity of the preposition;?
P(r|s): the probability of a role given the slot;?
P(r|sc): the probability of a role given the slotclass, in which all prepositional slots are treatedtogether.886Each probability model predicts a role given certainconditioning information, with maximum likelihoodestimates determined by the primary-labelled datadirectly resulting from the frame matching step.5We also compare one non-probabilistic model toresolve the same set of ambiguous cases:?
Default assignment: candidate roles for am-biguous slots are ignored; the four slot classesof subject, object, indirect object and PP-objectare assigned the roles Agent, Theme, Recipi-ent, and Location, respectively.These are the most likely roles assigned by the framematcher over our development data.For comparison, we also apply the iterative algo-rithm developed by Swier and Stevenson (2004), us-ing the same bootstrapping parameters.
The methoduses backoff over three levels of specificity of prob-abilities.5 Materials and Methods5.1 The Target VerbsFor ease of comparison, we use the same verbs as inSwier and Stevenson (2004), except that we measureperformance over a much larger superset of verbs.
Inthat work, a core set of 54 target verbs are selectedto represent a variety of classes with interesting roleambiguities, and the system is evaluated against onlythose verbs.
An additional 1105 verbs?all verbssharing at least one class with the target verbs?arealso labelled, in order to provide more data for theprobability estimations.
Here, we consider our sys-tem?s performance over the 1159 target verbs thatconsist of the union of these two sets of verbs.5.2 The Corpus and PreprocessingThe majority of sentences in FrameNet II are takenfrom the British National Corpus (BNC ReferenceGuide, 2000).
Our development and test data con-sists of a percentage of these sentences.
For someexperiments, these sentences are then merged witha random selection of additional sentences from theBNC in order to provide more training data for theprobability estimations.
We evaluate performance5Note that we assume the probability of a role for a slot is in-dependent of other slots?that is, we do not ensure a consistentrole assignment to all arguments across an instance of a verb.only on FrameNet sentences that include our targetverbs.All of our corpus data was parsed using theCollins parser (Collins, 1999).
Next, we use TGrep2(Rohde, 2004) to automatically extract from theparse trees the constituents forming potential argu-ments of the target verbs.
For each verb, we label asthe subject the lowest NP node, if it exists, that is im-mediately to the left of a VP node which dominatesthe verb.
Other arguments are identified by findingsister NP or PP nodes to the right of the verb.
Headsof noun phrases are identified using the method ofCollins (1999), which primarily chooses the right-most noun in the phrase that is not inside a preposi-tional phrase or subordinate clause.
Error may be in-troduced at each step of this preprocessing?the sen-tence may be misparsed, some arguments (such asdistant subjects) may not be extracted, or the wrongword may be found as the phrase head.5.3 Validation and Test DataA random selection of 30% of the preprocessedFrameNet data is set aside for testing, and anotherrandom 30% is used for development and valida-tion.
For experiments involving additional BNCdata, each 30% of the FrameNet sentences is em-bedded in a random selection of 20% of the BNC.We selected these percentages to yield a sufficientamount of data for experimentation, while reservingsome unseen data for future work.
The FrameNetportion of the validation set includes 515 types ofour target verbs (across 161 VerbNet classes) in4300 sentences, and contains a total of 6636 targetconstituents?i.e., constituents that receive a validVerbNet role as their gold standard label, not No-Role.
The test set includes 517 of the target verbs(from 163 classes) in 4308 sentences, yielding 6705target constituents.6To create an evaluation set, we map the manuallyannotated FrameNet roles in the corpus to VerbNetroles (or NoRole), as described in Sections 2.2 and3.2.
We use this role information to calculate perfor-mance: the system should assign roles matching thetarget VerbNet roles, and make no assignment whenthe target is NoRole.6The verbs appearing in the validation and test sets occurrespectively across 161 and 165 FrameNet classes (what inFrameNet are called ?frames?
).8875.4 Methods of Argument IdentificationOne of the decisions we face is how to evaluate theidentification of extracted arguments generated bythe system against the manually annotated target ar-guments provided by FrameNet.
We try two meth-ods, the most strict of which is to require full-phraseagreement: an extracted argument and a target ar-gument must cover exactly the same words in thesentence in order for the argument to be consideredcorrectly extracted.
This means, for instance, thata prepositional phrase incorrectly attached to an ex-tracted object would render the object incompatiblewith the target argument, and any system label onit would be counted as incorrect.
This evaluationmethod is commonly used in other work (e.g., Car-reras and Marquez, 2004).The other method we use is to require that onlythe head of an extracted argument and a target argu-ment match.
This latter method helps to provide afuller picture of the range of arguments found by thesystem, since there are fewer near-misses caused byattachment errors.
Since heads of phrases are oftenthe most semantically relevant part of an argument,labels on heads provide much of the same informa-tion as labels on whole phrases.
For these reasons,we use head matching for most of our experimentsbelow.
For comparison, however, we provide resultsbased on full-phrase matching as well.6 Experimental Results6.1 Experimental SetupWe evaluate our system?s performance on several as-pects of the overall role labelling task; all results aregiven in terms of F-measure, 2PR/(P + R).7 Thefirst task is argument identification, in which con-stituents considered by our system to be arguments(i.e., those that are extracted and labelled) are eval-uated against actual target arguments.
The secondtask is labelling extracted arguments, which evalu-ates the labelling of only those arguments that werecorrectly extracted.
Last is the overall role labellingtask, which evaluates the system on the combinedtasks of identification and labelling of all target ar-guments.We compare our results to an informed baselinethat has access to the same set of extracted argu-7In each case, P and R are close in value.ments as does the frame matcher.
The baseline la-bels all extracted arguments using the default roleassignments described in Section 4.In addition to experiments in which we employvarious methods of resolving ambiguous assign-ments, we also evaluate the system with varyingtypes and amounts of training data, and with two al-ternate methods for choosing frames from which todraw candidate roles.6.2 Evaluation of Probability ModelsWe first evaluate our system with the three verysimple probability models, as well as the non-probabilistic default assignment, to determine rolesfor the extracted arguments that the frame matcherconsiders to be ambiguous.
We also report resultsafter only the frame matcher has been applied, toindicate how much work is being done by it alone.Because we have constructed the frame matcher tobe highly restrictive in assigning candidate roles toextracted arguments, a large number (about 62%)become primary-labelled data and so do not requireresolution of ambiguous roles.
Only about 16% ofour extracted arguments have role ambiguities, andabout 22% (many of which are adjuncts) do not re-ceive any candidates and remain unlabelled.Task: Id.
Lab.
Id.
+ Lab.Baseline .80 .74 .52FM + P (r|sc) .83 .83 .65FM + P (r|s) .83 .84 .65FM + P (r|v, s) .83 .78 .61FM + Dflt.
Assgnmt.
.83 .82 .64FM only .83 .76 .60As shown in the table, all models perform equallywell on identification, which is determined by theframe matcher (FM); i.e., any extracted argumentreceiving one or more candidate roles is ?identi-fied?
as an argument.
Performance is somewhatabove the baseline, which must label all extractedarguments.
For the task of labelling correctly ex-tracted arguments and for the combined task, thesimplest probability models, P (r|sc) and P (r|s),perform about the same.
On the combined task, theyachieve .13 above the informed baseline, indicatingthe effectiveness of such simple models when com-bined with the frame matcher.
The more specificmodel, P (r|v, s), performs less well, and may beover-fitting on this relatively small amount of train-ing data.888Two observations indicate the power of the framematcher.
First, even using the non-probabilistic de-fault assignments to resolve ambiguous roles sub-stantially outperforms the baseline (and indeed per-forms quite close to the best results, since the defaultrole assignment is often the same as that chosen bythe probability models).
Importantly, the baselineuses the same default assignments, but without thebenefit of the frame matcher to further narrow downthe possible arguments.
Second, the frame matcheralone achieves .60 F-measure on the combined task,not far below the performance of the best models.These results show that once arguments have beenextracted, much of the labelling work is performedby the frame matcher?s careful application of lexicalinformation.Henceforth we consider the use of the framematcher plus P (r|sc) as our basic system, since thisis our simplest model, and no other outperforms it.6.3 Evaluation of Training MethodsIn our above experiments, the probabilistic mod-els are trained only on primary-labelled data fromthe frame matcher run on the FrameNet data.
Wewould like to determine whether using either moredata or less noisy data may improve results.
To pro-vide more data, we ran the frame matcher on theadditional 20% of the BNC.
This provides almost600K more sentences containing our target verbs,yielding a much higher amount of primary-labelleddata.
To provide less-noisy data, we trained theprobability models on manually annotated target la-bels from system-identified arguments in 1000 sen-tences.
While fewer sentences are used, all argu-ments in the training data are guaranteed to have acorrect role assignment, in contrast to the primary-labelled data output by the frame matcher.
(Wechose 1000 sentences as an upper bound on anamount of data that could be relatively easily anno-tated by human judges.
)Training Prim.-lab.
Prim.-lab.
1K sentsData: FN BNC annot?dBaseline .52FM + P (r|sc) .65 .65 .65FM + P (r|v, s) .61 .62 .63For our basic model, P (r|sc), these variations intraining data do not affect performance.
Only themost specific model, P (r|v, s), shows improvementwhen trained on more data or on manually annotateddata, although it still does not perform as well as thesimplest model.
Because the models only choosefrom among candidate roles selected by the framematcher, differences in the learned probability esti-mations must be quite large to have an effect.
Atleast for the simplest model, these estimations donot vary with a larger corpus or one lacking in noise.However, the increase in performance seen here forthe more specific model, albeit small, may indicatethat richer probability models may require more orcleaner training data.6.4 Evaluation of Frame Choice?Best?
frames All FramesBaseline .52FM + P (r|sc) .65 .63The frame matcher has been shown to shouldermuch of the responsibility in our system, and it isworth considering variations in its operation.
Forexample, by having the frame matcher only chooseroles from the frames that are the best syntacticmatches to the sentence, role ambiguity is mini-mized at the cost of possibly excluding the correctrole.
To determine whether we may do better by re-lying more on the probability model and less on theframe matcher, we instead include role candidatesfrom all frames in a verb?s lexical entry.
The effectof this choice is more role ambiguity, decreasing thenumber of primary-labelled slots by roughly 30%.We see that performance using P (r|sc) is slightlyworse with the greater ambiguity admitted by usingall frames, indicating the benefit of precise selectionof candidate roles.6.5 Differing Argument Evaluation MethodsHeads Full PhraseBaseline .52 .49FM + P (r|sc) .65 .61As mentioned, for most of our evaluations we matchthe arguments extracted by the system to the tar-get arguments via a match on phrase heads, sincehead labels provide much useful semantic informa-tion.
When we instead require that the extractedarguments match the targets exactly, the number ofcorrectly extracted arguments falls from about 80%of the roughly 6700 targets to about 74%, due to in-creased parsing difficulty.
As expected, this results889in both the system and the baseline having perfor-mance decreases on the overall task.7 Related WorkMost role labelling systems have required hand-labelled training data.
Two exceptions are the sub-categorization frame based work of Atserias et al(2001) and the bootstrapping labeller of Swier andStevenson (2004), but both are evaluated on only asmall number of verbs and arguments.
In related un-supervised tasks, Riloff and colleagues have learned?case frames?
for verbs (e.g., Riloff and Schmelzen-bach, 1998), while Gildea (2002) has learned role-slot mappings (but does not apply the knowledge forthe labelling task).Other role labelling systems have also relied onthe extraction of much more complex features orprobability models than we adopt here.
As a pointof comparison, we apply the iterative backoff modelfrom Swier and Stevenson (2004), trained on 20% ofthe BNC, with our frame matcher and test data.
Thebackoff model achieves an F-measure of .63, slightlybelow the performance of .65 for our simplest proba-bility model, which uses less training data and takesfar less time to run (minutes rather than hours).In general, it is not possible to make direct com-parisons between our work and most other role la-bellers because of differences in corpora and rolesets, and, perhaps more significantly, differences inthe selection of target arguments.
However, thebest supervised systems, using automatic parses toidentify full argument phrases in PropBank, achieveabout .82 on the task of identifying and labellingarguments (Pradhan et al, 2004).
Though this ishigher than our performance of .61 on full phrase ar-guments, our system does not require manually an-notated data.8 ConclusionIn this work, we employ an expensive but highlyreusable resource?a verb lexicon?to perform rolelabelling with a simple probability model and asmall amount of unsupervised training data.
We out-perform similar work that uses much more data anda more complex model, showing the benefit of ex-ploiting lexical information directly.
To achieve per-formance comparable to that of supervised methodsmay require human filtering or augmentation of theinitial labelling.
However, given the expense of pro-ducing a large semantically annotated corpus, evensuch ?human in the loop?
approaches may lead toa decrease in overall resource demands.
We usesuch a corpus for evaluation purposes only, modi-fying it with a role mapping to correspond to ourlexicon.
We thus demonstrate that such existing re-sources can be bootstrapped for lexicons lacking anassociated annotated corpus.AcknowledgmentsWe gratefully acknowledge the support of NSERCof Canada.
We also thank Afsaneh Fazly, who as-sisted with much of our corpus pre-processing.ReferencesJ.
Atserias, L.
Padro?, and G. Rigau.
2001.
Integrating multipleknowledge sources for robust semantic parsing.
In Proc.
ofthe International Conf.
on Recent Advances in NLP.C.
F. Baker, C. J. Fillmore, and J.
B. Lowe.
1998.
The BerkeleyFrameNet Project.
In Proc.
of COLING-ACL, p. 86?90.BNC Reference Guide.
2000.
Reference Guide for the BritishNational Corpus (World Edition), second edition.X.
Carreras and L. Marquez, editors.
2004.
CoNLL-04 SharedTask.M.
Collins.
1999.
Head-Driven Statistical Models for NaturalLanguage Parsing.
Ph.D. thesis, University of Pennsylvania.D.
Gildea.
2002.
Probabilistic models of verb-argument struc-ture.
In Proc.
of the 19th International CoNLL, p. 308?314.D.
Gildea and D. Jurafsky.
2002.
Automatic labeling of seman-tic roles.
Computational Linguistics, 23(3):245?288.K.
Hacioglu, S. Pradhan, W. Ward, J. H. Martin, and D. Ju-rafsky.
2004.
Semantic role labeling by tagging syntacticchunks.
In Proc.
of the 8th CoNLL, p. 110?113.K.
Kipper, H. T. Dang, and M. Palmer.
2000.
Class based con-struction of a verb lexicon.
In Proc.
of the 17th AAAI Conf.B.
Levin.
1993.
English Verb Classes and Alternations: A Pre-liminary Investigation.
University of Chicago Press.S.
Pradhan, W. Ward, K. Hacioglu, J. Martin, and D. Jurafsky.2004.
Shallow semantic parsing using support vector ma-chines.
In Proc.
of HLT/NAACL.E.
Riloff and M. Schmelzenbach.
1998.
An empirical approachto conceptual case frame acquisition.
In Proc.
of the 6thWVLC.D.
L. T. Rohde.
2004.
TGrep2 user manual ver.
1.11.http://tedlab.mit.edu/ d?r/Tgrep2.R.
Swier and S. Stevenson.
2004.
Unsupervised semantic rolelabelling.
In Proc.
of the 2004 Conf.
on EMNLP, p. 95?102.890
