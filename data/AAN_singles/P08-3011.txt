Proceedings of the ACL-08: HLT Student Research Workshop (Companion Volume), pages 61?66,Columbus, June 2008. c?2008 Association for Computational LinguisticsAdaptive Language Modeling for Word PredictionKeith TrnkaUniversity of DelawareNewark, DE 19716trnka@cis.udel.eduAbstractWe present the development and tuning of atopic-adapted language model for word pre-diction, which improves keystroke savingsover a comparable baseline.
We outline ourplans to develop and integrate style adap-tations, building on our experience in topicmodeling to dynamically tune the model toboth topically and stylistically relevant texts.1 IntroductionPeople who use Augmentative and Alternative Com-munication (AAC) devices communicate slowly, of-ten below 10 words per minute (wpm) compared to150 wpm or higher for speech (Newell et al, 1998).AAC devices are highly specialized keyboards withspeech synthesis, typically providing single-buttoninput for common words or phrases, but requiring auser to type letter-by-letter for other words, calledfringe vocabulary.
Many commercial systems (e.g.,PRC?s ECO) and researchers (Li and Hirst, 2005;Trnka et al, 2006; Wandmacher and Antoine, 2007;Matiasek and Baroni, 2003) have leveraged wordprediction to help speed AAC communication rate.While the user is typing an utterance letter-by-letter,the system continuously provides potential comple-tions of the current word to the user, which the usermay select.
The list of predicted words is generatedusing a language model.At best, modern devices utilize a trigram modeland very basic recency promotion.
However, one ofthe lamented weaknesses of ngram models is theirsensitivity to the training data.
They require sub-stantial training data to be accurate, and increasinglymore data as more of the context is utilized.
For ex-ample, Lesher et al (1999) demonstrate that bigramand trigram models for word prediction are not satu-rated even when trained on 3 million words, in con-trast to a unigram model.
In addition to the prob-lem of needing substantial amounts of training textto build a reasonable model, ngrams are sensitiveto the difference between training and testing/usertexts.
An ngram model trained on text of a differ-ent topic and/or style may perform very poorly com-pared to a model trained and tested on similar text.Trnka and McCoy (2007) and Wandmacher and An-toine (2006) have demonstrated the domain sensitiv-ity of ngram models for word prediction.The problem of utilizing ngram models for con-versational AAC usage is that no substantial cor-pora of AAC text are available (much less conver-sational AAC text).
The most similar available cor-pora are spoken language, but are typically muchsmaller than written corpora.
The problem of cor-pora for AAC is that similarity and availability areinversely related, illustrated in Figure 1.
At one ex-treme, a very large amount of formal written Englishis available, however, it is very dissimilar from con-versational AAC text, making it less useful for wordprediction.
At the other extreme, logged text fromthe current conversation of the AAC user is the mosthighly related text, but it is extremely sparse.
Whilethis trend is demonstrated with a variety of languagemodeling applications, the problem is more severefor AAC due to the extremely limited availability ofAAC text.
Even if we train our models on both alarge number of general texts in addition to highlyrelated in-domain texts to address the problem, we61Figure 1: The most relevant text available is often the smallest, while the largest corpora are often the least relevantfor AAC word prediction.
This problem is exaggerated for AAC.must focus the models on the most relevant texts.We address the problem of balancing training sizeand similarity by dynamically adapting the languagemodel to the most topically relevant portions of thetraining data.
We present the results of experiment-ing with different topic segmentations and relevancescores in order to tune existing methods to topicmodeling.
Our approach is designed to seamlesslydegrade to the baseline model when no relevant top-ics are found, by interpolating frequencies as well asensuring that all training documents contribute somenon-zero probabilities to the model.
We also out-line our plans to adapt ngram models to the style ofdiscourse and then combine the topical and stylisticadaptations.1.1 Evaluating Word PredictionWord prediction is evaluated in terms of keystrokesavings ?
the percentage of keystrokes saved bytaking full advantage of the predictions compared toletter-by-letter entry.KS =keysletter-by-letter ?
keyswith predictionkeysletter-by-letter?
100%Keystroke savings is typically measured automati-cally by simulating a user typing the testing data of acorpus, where any prediction is selected with a sin-gle keystroke and a space is automatically enteredafter selecting a prediction.
The results are depen-dent on the quality of the language model as well asthe number of words in the prediction window.
Wefocus on 5-word prediction windows.
Many com-mercial devices provide optimized input for the mostcommon words (called core vocabulary) and offerword prediction for all other words (fringe vocabu-lary).
Therefore, we limit our evaluation to fringewords only, based on a core vocabulary list fromconversations of young adults.We focus our training and testing on Switchboard,which we feel is similar to conversational AAC text.Our overall evaluation varies the training data fromSwitchboard training to training on out-of-domaindata to estimate the effects of topic modeling in real-world usage.2 Topic ModelingTopic models are language models that dynamicallyadapt to testing data, focusing on the most relatedtopics in the training data.
It can be viewed as atwo stage process: 1) identifying the relevant topicsby scoring and 2) tuning the language model basedon relevant topics.
Various other implementationsof topic adaptation have been successful in wordprediction (Li and Hirst, 2005; Wandmacher andAntoine, 2007) and speech recognition (Bellegarda,2000; Mahajan et al, 1999; Seymore and Rosen-feld, 1997).
The main difference of the topic mod-eling approach compared to Latent Semantic Anal-ysis (LSA) models (Bellegarda, 2000) and triggerpair models (Lau et al, 1993; Matiasek and Baroni,2003) is that topic models perform the majority ofgeneralization about topic relatedness at testing timerather than training time, which potentially allowsuser text to be added to the training data seamlessly.Topic modeling follows the framework belowPtopic(w | h) =?t?topicsP (t | h) ?
P (w | h, t)where w is the word being predicted/estimated, hrepresents all of the document seen so far, and t rep-resents a single topic.
The linear combination fortopic modeling shows the three main areas of vari-ation in topic modeling.
The posterior probability,62P (w | h, t) represents the sort of model we have;how topic will affect the adapted language model inthe end.
The prior, P (t | h), represents the way topicis identified.
Finally, the meaning of t ?
topics, re-quires explanation ?
what is a topic?2.1 Posterior Probability ?
Topic ApplicationThe topic modeling approach complicates the esti-mation of probabilities from a corpus because theadditional conditioning information in the posteriorprobability P (w | h, t) worsens the data sparsenessproblem.
This section will present our experience inlessening the data sparseness problem in the poste-rior, using examples on trigram models.The posterior probability requires more datathan a typical ngrammodel, potentially causing datasparseness problems.
We have explored the pos-sibility of estimating it by geometrically combin-ing a topic-adapted unigram model (i.e., P (w | t))with a context-adapted trigram model (i.e., P (w |w?1, w?2)), compared to straightforward measure-ment (P (w | w?1, w?2, t)).
Although the firstapproach avoids the additional data sparseness, itmakes an assumption that the topic of discourseonly affects the vocabulary usage.
Bellegarda (2000)used this approach for LSA-adapted modeling, how-ever, we found this approach to be inferior to di-rect estimation of the posterior probability for wordprediction (Trnka et al, 2006).
Part of the reasonfor the lesser benefit is that the overall model isonly affected slightly by topic adaptations due tothe tuned exponential weight of 0.05 on the topic-adapted unigram model.
We extended previous re-search by forcing trigram predictions to occur overbigrams and so on (rather than backoff) and usingthe topic-adapted model for re-ranking within eachset of predictions, but found that the forced orderingof the ngram components was overly detrimental tokeystroke savings.Backoff models for topic modeling can be con-structed either before or after the linear interpola-tion.
If the backoff is performed after interpolation,we must also choose whether smoothing (a prereq-uisite for backoff) is performed before or after theinterpolation.
If we smooth before the interpolation,then the frequencies will be overly discounted, be-cause the smoothing method is operating on a smallfraction of the training data, which will reduce thebenefit of higher-order ngrams in the overall model.Also, if we combine probability distributions fromeach topic, the combination approach may have dif-ficulties with topics of varying size.
We addressthese issues by instead combining frequencies andperforming smoothing and backoff after the combi-nation, similar to Adda et al (1999), although theyused corpus-sized topics.
The advantage of this ap-proach is that the held-out probability for each dis-tribution is appropriate for the training data, becausethe smoothing takes place knowing the number ofwords that occurred in the whole corpus, rather thanfor each small segment.
This is especially importantwhen dealing with small and different sized topics.The linear interpolation affects smoothingmethods negatively ?
because the weights are lessthan one, the combination decreases the total sumof each conditional distribution.
This will causesmoothing methods to underestimate the reliabilityof the models, because smoothing methods estimatethe reliability of a distribution based on the absolutenumber of occurrences.
To correct this, after inter-polating the frequencies we found it useful to scalethe distribution back to its original sum.
The scal-ing approach improved keystroke savings by 0.2%?0.4% for window size 2?10 and decreased savingsby 0.1% for window size 1.
Because most AAC sys-tems provide 5?7 predictions, we use this approach.Also, because some smoothing methods operate onfrequencies, but the combination model producesreal-valued weights for each word, we found it nec-essary to bucket the combined frequencies to convertthem to integers.Finally, we required an efficient smoothingmethod that could discount each conditional distri-bution individually to facilitate on-demand smooth-ing for each conditional distribution, in contrast toa method like Katz?
backoff (Katz, 1987) whichsmoothes an entire ngram model at once.
Also,Good-Turing smoothing proved too cumbersome, aswe were unable to rely on the ratio between words ingiven bins and also unable to reliably apply regres-sion.
Instead, we used an approximation of Good-Turing smoothing that performed similarly, but al-lowed for substantial optimization.632.2 Prior Probability ?
Topic IdentificationThe topic modeling approach uses the current testingdocument to tune the language model to the mostrelevant training data.
The benefit of adaptation isdependent on the quality of the similarity scores.
Wewill first present our representation of the currentdocument, which is compared to unigram models ofeach topic using a similarity function.
We determinethe weight of each word in the current document us-ing frequency, recency, and topical salience.The recency of use of a word contributes to therelevance of the word.
If a word was used somewhatrecently, we would expect to see the word again.
Wefollow Bellegarda (2000) in using an exponentiallydecayed cache with weight of 0.95 to model this ef-fect of recency on importance at the current positionin the document.
The weight of 0.95 represents apreservation in topic, but with a decay for very stalewords, whereas a weight of 1 turns the exponen-tial model into a pure frequency model and lowerweights represent quick shifts in topic.The importance of each word occurrence in thecurrent document is a factor of not just its frequencyand recency, but also it?s topical salience ?
howwell the word discriminates between topics.
For thisreason, we decided to use a technique like InverseDocument Frequency (IDF) to boost the weight ofwords that occur in only a few documents and de-press the weights of words that occur in most docu-ments.
However, instead of using IDF to measuretopical salience, we use Inverse Topic Frequency(ITF), which is more specifically tailored to topicmodeling and the particular kinds of topics used.We evaluated several similarity functions fortopic modeling, initially using the cosine measurefor similarity scoring and scaling the scores to bea probability distribution, following Florian andYarowsky (1999).
The intuition behind the co-sine measure is that the similarity between two dis-tributions of words should be independent of thelength of either document.
However, researchershave demonstrated that cosine is not the best rele-vance metric for other applications, so we evaluatedtwo other topical similarity scores: Jacquard?s coef-ficient, which performed better than most other sim-ilarity measures in a different task for Lee (1999)and Na?
?ve Bayes, which gave better results than co-sine in topic-adapted language models for Seymoreand Rosenfeld (1997).
We evaluated all three simi-larity metrics using Switchboard topics as the train-ing data and each of our corpora for testing us-ing cross-validation.
We found that cosine is con-sistently better than both Jacquard?s coefficient andNa?
?ve Bayes, across all corpora tested.
The differ-ences between cosine and the other methods are sta-tistically significant at p < 0.001.
It may be possiblethat the ITF or recency weighting in the cache had anegative interaction with Nav?e Bayes; traditionallyraw frequencies are used.We found it useful to polarize the similarityscores, following Florian and Yarowsky (1999),who found that transformations on cosine similarityreduced perplexity.
We scaled the scores such thatthe maximum score was one and the minimum scorewas zero, which improved keystroke savings some-what.
This helps fine-tune topic modeling by furtherboosting the weights of the most relevant topics anddepressing the weights of the less relevant topics.Smoothing the scores helps prevent some scoresfrom being zero due to lack of word overlap.
One ofthe motivations behind using a linear interpolation ofall topics is that the resulting ngram model will havethe same coverage of ngrams as a model that isn?tadapted by topic.
However, the similarity score willbe zero when no words overlap between the topicand history.
Therefore we decided to experimentwith similarity score smoothing, which records theminimum nonzero score and then adds a fraction ofthat score to all scores, then only apply upscaling,where the maximum is scaled to 1, but the minimumis not scaled to zero.
In pilot experiments, we foundthat smoothing the scores did not affect topic mod-eling with traditional topic clusters, but gave minorimprovements when documents were used as topics.Stemming is another alternative to improving thesimilarity scoring.
This helps to reduce problemswith data sparseness by treating different forms ofthe same word as topically equivalent.
We foundthat stemming the cache representations was veryuseful when documents were treated as topics (0.2%increase across window sizes), but detrimental whenlarger topics were used (0.1?0.2% decrease acrosswindow sizes).
Therefore, we only use stemmingwhen documents are treated as topics.642.3 What?s in a Topic ?
Topic GranularityWe adapt a language model to the most relevant top-ics in training text.
But what is a topic?
Tradition-ally, document clusters are used for topics, wheresome researchers use hand-crafted clusters (Trnkaet al, 2006; Lesher and Rinkus, 2001) and oth-ers use automatic clustering (Florian and Yarowsky,1999).
However, other researchers such as Mahajanet al (1999) have used each individual document asa topic.
On the other end of the spectrum, we canuse whole corpora as topics when training on mul-tiple corpora.
We call this spectrum of topic defini-tions topic granularity, where manual and automaticdocument clusters are called medium-grained topicmodeling.
When topics are individual documents,we call the approach fine-grained topic modeling.
Infine-grained modeling, topics are very specific, suchas seasonal clothing in the workplace, compared toa medium topic for clothing.
When topics are wholecorpora, we call the approach coarse-grained topicmodeling.
Coarse-grained topics model much morehigh-level topics, such as research or news.The results of testing on Switchboard across dif-ferent topic granularities are showin in Table 1.
Thein-domain test is trained on Switchboard only.
Out-of-domain training is performed using all other cor-pora in our collection (a mix of spoken and writ-ten language).
Mixed-domain training combines thetwo data sets.
Medium-grained topics are only pre-sented for in-domain training, as human-annotatedtopics were only available for Switchboard.
Stem-ming was used for fine-grained topics, but similarityscore smoothing was not used due to lack of time.The topic granularity experiment confirms ourearlier findings that topic modeling can significantlyimprove keystroke savings.
However, the variationof granularity shows that the size of the topics hasa strong effect on keystroke savings.
Human anno-tated topics give the best results, though fine-grainedtopic modeling gives similar results without the needfor annotation, making it applicable to training onnot just Switchboard but other corpora as well.
Thecoarse grained topic approach seems to be limitedto finding acceptable interpolation weights betweenvery similar and very dissimilar data, but is poor atselecting the most relevant corpora from a collectionof very different corpora in the out-of-domain test.Another problem may be that many of the corporaare only homogeneous in style but not topic.
Wewould like to extend our work in topic granularity totesting on other corpora in the future.3 Future Work ?
Style and CombinationTopic modeling balances the similarity of the train-ing data against the size by tuning a large trainingset to the most topically relevant portions.
However,keystroke savings is not only affected by the topicalsimilarity of the training data, but also the stylisticsimilarity.
Therefore, we plan to also adapt modelsto the style of text.
Our success in adapting to thetopic of conversation leads us to believe that a sim-ilar process may be applicable to style modeling ?splitting the model into style identification and styleapplication.
Because we are primarily interested insyntactic style, we will focus on part of speech asthe mechanism for realizing grammatical style.
Asa pilot experiment, we compared a collection of ourtechnical writings on word prediction with a collec-tion of our research emails on word prediction, find-ing that we could observe traditional trends in thePOS ngram distributions (e.g., more pronouns andphrasal verbs in emails).
Therefore, we expect thatdistributional similarity of POS tags will be usefulfor style identification.
We envision a single style saffecting the likelihood of each part of speech p in aPOS ngram model like the one below:P (w | w?1,w?2, s) =?p?POS(w)P (p | p?1, p?2, s) ?
P (w | p)In this reformulation of a POS ngram model, theprior is conditioned on the style and the previouscouple tags.
We will use the overall framework tocombine style identification and modeling:Pstyle(w | h) =?s?stylesP (s | h) ?
P (w | w?1, w?2, s)The topical and stylistic adaptations can be com-bined by adding topic modeling into the style modelshown above.
The POS posterior probability P (w |p) can be additionally conditioned on the topic ofdiscourse.
Topic identification and the topic sum-mation would be implemented consistently with thestandalone topic model.
Also, the POS framework65Model type In-domain Out-of-domain Mixed-domainTrigram baseline 60.35% 53.88% 59.80%Switchboard topics (medium grained) 61.48% (+1.12%) ?
?Document as topic (fine grained) 61.42% (+1.07%) 54.90% (+1.02%) 61.17% (+1.37%)Corpus as topic (coarse grained) ?
52.63% (-1.25%) 60.62% (+0.82%)Table 1: Keystroke savings across different granularity topics and training domains, tested on Switchboard.
Improve-ment over baseline is shown in parentheses.
All differences from baseline are significant at p < 0.001facilitates cache modeling in the posterior, allowingdirect adaptation to the current text, but with lesssparseness than other context-aware models.4 ConclusionsWe have created a topic adapted language model thatutilizes the full training data, but with focused tuningon the most relevant portions.
The inclusion of allthe training data as well as the usage of frequenciesaddresses the problem of sparse data in an adaptivemodel.
We have demonstrated that topic modelingcan significantly increase keystroke savings for tra-ditional testing as well as testing on text from otherdomains.
We have also addressed the problem ofannotated topics through fine-grained modeling andfound that it is also a significant improvement over abaseline ngram model.
We plan to extend this workto build models that adapt to both topic and style.AcknowledgmentsThis work was supported by US Department of Ed-ucation grant H113G040051.
I would like to thankmy advisor, Kathy McCoy, for her help as well asthe many excellent and thorough reviewers.ReferencesGilles Adda, Miche`le Jardino, and Jean-Luc Gauvain.1999.
Language modeling for broadcast news tran-scription.
In Eurospeech, pages 1759?1762.Jerome R. Bellegarda.
2000.
Large vocabularyspeech recognition with multispan language models.IEEE Transactions on Speech and Audio Processing,8(1):76?84.Radu Florian and David Yarowsky.
1999.
DynamicNonlocal Language Modeling via Hierarchical Topic-Based Adaptation.
In ACL, pages 167?174.Slava M. Katz.
1987.
Estimation of probabilities fromsparse data for the language model component of aspeech recognizer.
IEEE Transactions on AcousticsSpeech and Signal Processing, 35(3):400?401.R.
Lau, R. Rosenfeld, and S. Roukos.
1993.
Trigger-based language models: a maximum entropy ap-proach.
In ICASSP, volume 2, pages 45?48.Lillian Lee.
1999.
Measures of distributional similarity.In ACL, pages 25?32.Gregory Lesher and Gerard Rinkus.
2001.
Domain-specific word prediction for augmentative communi-cation.
In RESNA, pages 61?63.Gregory W. Lesher, Bryan J. Moulton, and D. JefferyHiggonbotham.
1999.
Effects of ngram order andtraining text size on word prediction.
In RESNA, pages52?54.Jianhua Li and Graeme Hirst.
2005.
Semantic knowl-edge in word completion.
In ASSETS, pages 121?128.Milind Mahajan, Doug Beeferman, and X. D. Huang.1999.
Improved topic-dependent language modelingusing information retrieval techniques.
In ICASSP,volume 1, pages 541?544.Johannes Matiasek and Marco Baroni.
2003.
Exploitinglong distance collocational relations in predictive typ-ing.
In EACL-03 Workshop on Language Modeling forText Entry, pages 1?8.Alan Newell, Stefan Langer, andMarianne Hickey.
1998.The ro?le of natural language processing in alternativeand augmentative communication.
Natural LanguageEngineering, 4(1):1?16.Kristie Seymore and Ronald Rosenfeld.
1997.
UsingStory Topics for Language Model Adaptation.
In Eu-rospeech, pages 1987?1990.Keith Trnka and Kathleen F. McCoy.
2007.
Corpus Stud-ies in Word Prediction.
In ASSETS, pages 195?202.Keith Trnka, Debra Yarrington, Kathleen McCoy, andChristopher Pennington.
2006.
Topic Modeling inFringe Word Prediction for AAC.
In IUI, pages 276?278.Tonio Wandmacher and Jean-Yves Antoine.
2006.Training Language Models without Appropriate Lan-guage Resources: Experiments with an AAC Systemfor Disabled People.
In LREC.T.
Wandmacher and J.Y.
Antoine.
2007.
Methods to in-tegrate a language model with semantic informationfor a word prediction component.
In EMNLP, pages506?513.66
