Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 405?413,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPJointly Identifying Temporal Relations with Markov LogicKatsumasa YoshikawaNAIST, Japankatsumasa-y@is.naist.jpSebastian RiedelUniversity of Tokyo, Japansebastian.riedel@gmail.comMasayuki AsaharaNAIST, Japanmasayu-a@is.naist.jpYuji MatsumotoNAIST, Japanmatsu@is.naist.jpAbstractRecent work on temporal relation iden-tification has focused on three types ofrelations between events: temporal rela-tions between an event and a time expres-sion, between a pair of events and betweenan event and the document creation time.These types of relations have mostly beenidentified in isolation by event pairwisecomparison.
However, this approach ne-glects logical constraints between tempo-ral relations of different types that we be-lieve to be helpful.
We therefore propose aMarkov Logic model that jointly identifiesrelations of all three relation types simul-taneously.
By evaluating our model on theTempEval data we show that this approachleads to about 2% higher accuracy for allthree types of relations ?and to the bestresults for the task when compared to thoseof other machine learning based systems.1 IntroductionTemporal relation identification (or temporal or-dering) involves the prediction of temporal orderbetween events and/or time expressions mentionedin text, as well as the relation between events in adocument and the time at which the document wascreated.With the introduction of the TimeBank corpus(Pustejovsky et al, 2003), a set of documents an-notated with temporal information, it became pos-sible to apply machine learning to temporal order-ing (Boguraev and Ando, 2005; Mani et al, 2006).These tasks have been regarded as essential forcomplete document understanding and are usefulfor a wide range of NLP applications such as ques-tion answering and machine translation.Most of these approaches follow a simpleschema: they learn classifiers that predict the tem-poral order of a given event pair based on a set ofthe pair?s of features.
This approach is local in thesense that only a single temporal relation is consid-ered at a time.Learning to predict temporal relations in this iso-lated manner has at least two advantages over anyapproach that considers several temporal relationsjointly.
First, it allows us to use off-the-shelf ma-chine learning software that, up until now, has beenmostly focused on the case of local classifiers.
Sec-ond, it is computationally very efficient both interms of training and testing.However, the local approach has a inherentdrawback: it can lead to solutions that violate logi-cal constraints we know to hold for any sets of tem-poral relations.
For example, by classifying tempo-ral relations in isolation we may predict that eventA happened before, and event B after, the timeof document creation, but also that event A hap-pened after event B?a clear contradiction in termsof temporal logic.In order to repair the contradictions that the localclassifier predicts, Chambers and Jurafsky (2008)proposed a global framework based on Integer Lin-ear Programming (ILP).
They showed that largeimprovements can be achieved by explicitly incor-porating temporal constraints.The approach we propose in this paper is similarin spirit to that of Chambers and Jurafsky: we seekto improve the accuracy of temporal relation iden-tification by predicting relations in a more globalmanner.
However, while they focused only on thetemporal relations between events mentioned in adocument, we also jointly predict the temporal or-der between events and time expressions, and be-tween events and the document creation time.Our work also differs in another important as-pect from the approach of Chambers and Jurafsky.Instead of combining the output of a set of localclassifiers using ILP, we approach the problem ofjoint temporal relation identification using MarkovLogic (Richardson and Domingos, 2006).
In this405framework global correlations can be readily cap-tured through the addition of weighted first orderlogic formulae.Using Markov Logic instead of an ILP-based ap-proach has at least two advantages.
First, it allowsus to easily capture non-deterministic (soft) rulesthat tend to hold between temporal relations but donot have to.
1 For example, if event A happens be-fore B, and B overlaps with C, then there is a goodchance that A also happens before C, but this is notguaranteed.Second, the amount of engineering required tobuild our system is similar to the efforts requiredfor using an off-the-shelf classifier: we only needto define features (in terms of formulae) and pro-vide input data in the correct format.
2 In particu-lar, we do not need to manually construct ILPs foreach document we encounter.
Moreover, we canexploit and compare advanced methods of globalinference and learning, as long as they are imple-mented in our Markov Logic interpreter of choice.Hence, in our future work we can focus entirelyon temporal relations, as opposed to inference orlearning techniques for machine learning.We evaluate our approach using the data of the?TempEval?
challenge held at the SemEval 2007Workshop (Verhagen et al, 2007).
This challengeinvolved three tasks corresponding to three typesof temporal relations: between events and time ex-pressions in a sentence (Task A), between events ofa document and the document creation time (TaskB), and between events in two consecutive sen-tences (Task C).Our findings show that by incorporating globalconstraints that hold between temporal relationspredicted in Tasks A, B and C, the accuracy forall three tasks can be improved significantly.
Incomparison to other participants of the ?TempE-val?
challenge our approach is very competitive:for two out of the three tasks we achieve the bestresults reported so far, by a margin of at least 2%.
3Only for Task Bwe were unable to reach the perfor-mance of a rule-based entry to the challenge.
How-ever, we do perform better than all pure machine1It is clearly possible to incorporate weighted constraintsinto ILPs, but how to learn the corresponding weights is notobvious.2This is not to say that picking the right formulae inMarkov Logic, or features for local classification, is alwayseasy.3To be slightly more precise: for Task C we achieve thismargin only for ?strict?
scoring?see sections 5 and 6 for moredetails.learning-based entries.The remainder of this paper is organized as fol-lows: Section 2 describes temporal relation identi-fication including TempEval; Section 3 introducesMarkov Logic; Section 4 explains our proposedMarkov Logic Network; Section 5 presents the set-up of our experiments; Section 6 shows and dis-cusses the results of our experiments; and in Sec-tion 7 we conclude and present ideas for future re-search.2 Temporal Relation IdentificationTemporal relation identification aims to predictthe temporal order of events and/or time expres-sions in documents, as well as their relations to thedocument creation time (DCT).
For example, con-sider the following (slightly simplified) sentence ofSection 1 in this paper.With the introduction of the TimeBank cor-pus (Pustejovsky et al, 2003), machinelearning approaches to temporal orderingbecame possible.Here we have to predict that the ?Machine learn-ing becoming possible?
event happened AFTERthe ?introduction of the TimeBank corpus?
event,and that it has a temporal OVERLAP with the year2003.
Moreover, we need to determine that bothevents happened BEFORE the time this paper wascreated.Most previous work on temporal relation iden-tification (Boguraev and Ando, 2005; Mani et al,2006; Chambers and Jurafsky, 2008) is based onthe TimeBank corpus.
The temporal relations inthe Timebank corpus are divided into 11 classes;10 of them are defined by the following 5 relationsand their inverse: BEFORE, IBEFORE (immedi-ately before), BEGINS, ENDS, INCLUDES; the re-maining one is SIMULTANEOUS.In order to drive forward research on temporalrelation identification, the SemEval 2007 sharedtask (Verhagen et al, 2007) (TempEval) includedthe following three tasks.TASK A Temporal relations between events andtime expressions that occur within the samesentence.TASK B Temporal relations between the Docu-ment Creation Time (DCT) and events.TASK C Temporal relations between the mainevents of adjacent sentences.44The main event of a sentence is expressed by its syntacti-cally dominant verb.406To simplify matters, in the TempEval data, theclasses of temporal relations were reduced fromthe original 11 to 6: BEFORE, OVERLAP, AFTER,BEFORE-OR-OVERLAP, OVERLAP-OR-AFTER,and VAGUE.In this work we are focusing on the three tasks ofTempEval, and our running hypothesis is that theyshould be tackled jointly.
That is, instead of learn-ing separate probabilistic models for each task, wewant to learn a single one for all three tasks.
Thisallows us to incorporate rules of temporal consis-tency that should hold across tasks.
For example, ifan event X happens before DCT, and another eventY after DCT, then surely X should have happenedbefore Y.
We illustrate this type of transition rule inFigure 1.Note that the correct temporal ordering of eventsand time expressions can be controversial.
For in-stance, consider the example sentence again.
Hereone could argue that ?the introduction of the Time-Bank?
may OVERLAP with ?Machine learning be-coming possible?
because ?introduction?
can beunderstood as a process that is not finished withthe release of the data but also includes later adver-tisements and announcements.
This is reflected bythe low inter-annotator agreement score of 72% onTasks A and B, and 68% on Task C.3 Markov LogicIt has long been clear that local classificationalone cannot adequately solve all prediction prob-lems we encounter in practice.5 This observa-tion motivated a field within machine learning,often referred to as Statistical Relational Learn-ing (SRL), which focuses on the incorporationof global correlations that hold between statisticalvariables (Getoor and Taskar, 2007).One particular SRL framework that has recentlygained momentum as a platform for global learn-ing and inference in AI is Markov Logic (Richard-son and Domingos, 2006), a combination of first-order logic and Markov Networks.
It can be under-stood as a formalism that extends first-order logicto allow formulae that can be violated with somepenalty.
From an alternative point of view, it is anexpressive template language that uses first orderlogic formulae to instantiate Markov Networks ofrepetitive structure.From a wide range of SRL languages we choseMarkov Logic because it supports discriminative5It can, however, solve a large number of problems surpris-ingly well.Figure 1: Example of Transition Rule 1training (as opposed to generative SRL languagessuch as PRM (Koller, 1999)).
Moreover, sev-eral Markov Logic software libraries exist and arefreely available (as opposed to other discrimina-tive frameworks such as Relational Markov Net-works (Taskar et al, 2002)).In the following we will explain Markov Logicby example.
One usually starts out with a setof predicates that model the decisions we need tomake.
For simplicity, let us assume that we onlypredict two types of decisions: whether an event ehappens before the document creation time (DCT),and whether, for a pair of events e1 and e2, e1happens before e2.
Here the first type of deci-sion can be modeled through a unary predicatebeforeDCT(e), while the latter type can be repre-sented by a binary predicate before(e1, e2).
Bothpredicates will be referred to as hidden because wedo not know their extensions at test time.
We alsointroduce a set of observed predicates, representinginformation that is available at test time.
For ex-ample, in our case we could introduce a predicatefutureTense(e) which indicates that e is an eventdescribed in the future tense.With our predicates defined, we can now go onto incorporate our intuition about the task usingweighted first-order logic formulae.
For example,it seems reasonable to assume thatfutureTense (e) ?
?beforeDCT (e) (1)often, but not always, holds.
Our remaining un-certainty with regard to this formula is capturedby a weight w we associate with it.
Generallywe can say that the larger this weight is, the morelikely/often the formula holds in the solutions de-scribed by our model.
Note, however, that we donot need to manually pick these weights; insteadthey are learned from the given training corpus.The intuition behind the previous formula canalso be captured using a local classifier.6 However,6Consider a log-linear binary classifier with a ?past-tense?407Markov Logic also allows us to say more:beforeDCT (e1) ?
?beforeDCT (e2)?
before (e1, e2) (2)In this case, we made a statement about moreglobal properties of a temporal ordering that can-not be captured with local classifiers.
This formulais also an example of the transition rules as seen inFigure 2.
This type of rule forms the core idea ofour joint approach.A Markov Logic Network (MLN) M is a set ofpairs (?,w) where ?
is a first order formula and wis a real number (the formula?s weight).
It defines aprobability distribution over sets of ground atoms,or so-called possible worlds, as follows:p (y) = 1Zexp???
(?,w)?Mw?c?C?f?c (y)??
(3)Here each c is a binding of free variables in ?
toconstants in our domain.
Each f?c is a binary fea-ture function that returns 1 if in the possible worldy the ground formula we get by replacing the freevariables in ?
with the constants in c is true, and0 otherwise.
C?
is the set of all bindings for thefree variables in ?.
Z is a normalisation constant.Note that this distribution corresponds to a MarkovNetwork (the so-called Ground Markov Network)where nodes represent ground atoms and factorsrepresent ground formulae.Designing formulae is only one part of the game.In practice, we also need to choose a trainingregime (in order to learn the weights of the formu-lae we added to the MLN) and a search/inferencemethod that picks the most likely set of groundatoms (temporal relations in our case) given ourtrained MLN and a set of observations.
How-ever, implementations of these methods are oftenalready provided in existing Markov Logic inter-preters such as Alchemy 7 and Markov thebeast.
84 Proposed Markov Logic NetworkAs stated before, our aim is to jointly tackleTasks A, B and C of the TempEval challenge.
Inthis section we introduce the Markov Logic Net-work we designed for this goal.We have three hidden predicates, correspondingto Tasks A, B, and C: relE2T(e, t, r) represents thetemporal relation of class r between an event efeature: here for every event e the decision ?e happens be-fore DCT?
becomes more likely with a higher weight for thisfeature.7http://alchemy.cs.washington.edu/8http://code.google.com/p/thebeast/Figure 2: Example of Transition Rule 2and a time expression t; relDCT(e, r) denotes thetemporal relation r between an event e and DCT;relE2E(e1, e2, r) represents the relation r betweentwo events of the adjacent sentences, e1 and e2.Our observed predicates reflect information wewere given (such as the words of a sentence), andadditional information we extracted from the cor-pus (such as POS tags and parse trees).
Note thatthe TempEval data also contained temporal rela-tions that were not supposed to be predicted.
Theserelations are represented using two observed pred-icates: relT2T(t1, t2, r) for the relation r betweentwo time expressions t1 and t2; dctOrder(t, r) forthe relation r between a time expression t and afixed DCT.An illustration of all ?temporal?
predicates, bothhidden and observed, can be seen in Figure 3.4.1 Local FormulaOur MLN is composed of several weighted for-mulae that we divide into two classes.
The firstclass contains local formulae for the Tasks A, Band C. We say that a formula is local if it onlyconsiders the hidden temporal relation of a singleevent-event, event-time or event-DCT pair.
Theformulae in the second class are global: they in-volve two or more temporal relations at the sametime, and consider Tasks A, B and C simultane-ously.The local formulae are based on features em-ployed in previous work (Cheng et al, 2007;Bethard andMartin, 2007) and are listed in Table 1.What follows is a simple example in order to illus-trate how we implement each feature as a formula(or set of formulae).Consider the tense-feature for Task C. For thisfeature we first introduce a predicate tense(e, t)that denotes the tense t for an event e. Then we408Figure 3: Predicates for Joint Formulae; observedpredicates are indicated with dashed lines.Table 1: Local FeaturesFeature A B CEVENT-word X XEVENT-POS X XEVENT-stem X XEVENT-aspect X X XEVENT-tense X X XEVENT-class X X XEVENT-polarity X XTIMEX3-word XTIMEX3-POS XTIMEX3-value XTIMEX3-type XTIMEX3-DCT order X Xpositional order Xin/outside Xunigram(word) X Xunigram(POS) X Xbigram(POS) Xtrigram(POS) X XDependency-Word X X XDependency-POS X Xadd a set of formulae such astense(e1, past) ?
tense(e2, future)?
relE2E(e1, e2, before) (4)for all possible combinations of tenses and tempo-ral relations.94.2 Global FormulaOur global formulae are designed to enforce con-sistency between the three hidden predicates (andthe two observed temporal predicates we men-tioned earlier).
They are based on the transition9This type of ?template-based?
formulae generation can beperformed automatically by the Markov Logic Engine.rules we mentioned in Section 3.Table 2 shows the set of formula templates weuse to generate the global formulae.
Here eachtemplate produces several instantiations, one foreach assignment of temporal relation classes to thevariables R1, R2, etc.
One example of a templateinstantiation is the following formula.dctOrder(t1, before) ?
relDCT(e1, after)?
relE2T(e1, t1, after) (5a)This formula is an expansion of the formula tem-plate in the second row of Table 2.
Note that itutilizes the results of Task B to solve Task A.Formula 5a should always hold,10 and hence wecould easily implement it as a hard constraint inan ILP-based framework.
However, some transi-tion rules are less determinstic and should ratherbe taken as ?rules of thumb?.
For example, for-mula 5b is a rule which we expect to hold often,but not always.dctOrder(t1, before) ?
relDCT(e1, overlap)?
relE2T(e1, t1, after) (5b)Fortunately, this type of soft rule poses no prob-lem for Markov Logic: after training, Formula 5bwill simply have a lower weight than Formula 5a.By contrast, in a ?Local Classifier + ILP?-basedapproach as followed by Chambers and Jurafsky(2008) it is less clear how to proceed in the caseof soft rules.
Surely it is possible to incorporateweighted constraints into ILPs, but how to learn thecorresponding weights is not obvious.5 Experimental SetupWith our experiments we want to answer twoquestions: (1) does jointly tackling Tasks A, B,and C help to increase overall accuracy of tempo-ral relation identification?
(2) How does our ap-proach compare to state-of-the-art results?
In thefollowing we will present the experimental set-upwe chose to answer these questions.In our experiments we use the test and trainingsets provided by the TempEval shared task.
Wefurther split the original training data into a trainingand a development set, used for optimizing param-eters and formulae.
For brevity we will refer to thetraining, development and test set as TRAIN, DEVand TEST, respectively.
The numbers of temporalrelations in TRAIN, DEV, and TEST are summa-rized in Table 3.10However, due to inconsistent annotations one will find vi-olations of this rule in the TempEval data.409Table 2: Joint Formulae for Global ModelTask FormulaA?
B dctOrder(t, R1) ?
relE2T(e, t, R2) ?
relDCT(e,R3)B ?
A dctOrder(t, R1) ?
relDCT(e,R2) ?
relE2T(e, t, R3)B ?
C relDCT(e1, R1) ?
relDCT(e2, R2) ?
relE2E(e1, e2, R3)C ?
B relDCT(e1, R1) ?
relE2E(e1, e2, R2) ?
relDCT(e2, R3)A?
C relE2T(e1, t1, R1) ?
relT2T(t1, t2, R2) ?
relE2T(e2, t2, R3) ?
relE2E(e1, e2, R4)C ?
A relE2T(e2, t2, R1) ?
relT2T(t1, t2, R2) ?
relE2E(e1, e2, R3) ?
relE2T(e1, t1, R4)Table 3: Numbers of Labeled Relations for AllTasksTRAIN DEV TEST TOTALTask A 1359 131 169 1659Task B 2330 227 331 2888Task C 1597 147 258 2002For feature generation we use the followingtools.
11 POS tagging is performed with TnTver2.2;12 for our dependency-based features we useMaltParser 1.0.0.13 For inference in our modelswe use Cutting Plane Inference (Riedel, 2008) withILP as a base solver.
This type of inference is ex-act and often very fast because it avoids instantia-tion of the complete Markov Network.
For learningwe apply one-best MIRA (Crammer and Singer,2003) with Cutting Plane Inference to find the cur-rent model guess.
Both training and inference algo-rithms are provided by Markov thebeast, a MarkovLogic interpreter tailored for NLP applications.Note that there are several ways to manually op-timize the set of formulae to use.
One way is topick a task and then choose formulae that increasethe accuracy for this task on DEV.
However, ourprimary goal is to improve the performance of allthe tasks together.
Hence we choose formulae withrespect to the total score over all three tasks.
Wewill refer to this type of optimization as ?averagedoptimization?.
The total scores of the all three tasksare defined as follows:Ca + Cb + CcGa +Gb +Gcwhere Ca, Cb, and Cc are the number of the cor-rectly identified labels in each task, and Ga, Gb,and Gc are the numbers of gold labels of each task.Our system necessarily outputs one label to one re-lational link to identify.
Therefore, for all our re-11Since the TempEval trial has no restriction on pre-processing such as syntactic parsing, most participants usedsome sort of parsers.12http://www.coli.uni-saarland.de/?thorsten/tnt/13http://w3.msi.vxu.se/?nivre/research/MaltParser.htmlsults, precision, recall, and F-measure are the exactsame value.For evaluation, TempEval proposed the two scor-ing schemes: ?strict?
and ?relaxed?.
For strict scor-ing we give full credit if the relations match, and nocredit if they do not match.
On the other hand, re-laxed scoring gives credit for a relation accordingto Table 4.
For example, if a system picks the re-lation ?AFTER?
that should have been ?BEFORE?according to the gold label, it gets neither ?strict?nor ?relaxed?
credit.
But if the system assigns?B-O (BEFORE-OR-OVERLAP)?
to the relation,it gets a 0.5 ?relaxed?
score (and still no ?strict?score).6 ResultsIn the following we will first present our com-parison of the local and global model.
We will thengo on to put our results into context and comparethem to the state-of-the-art.6.1 Impact of Global FormulaeFirst, let us show the results on TEST in Ta-ble 5.
You will find two columns, ?Global?
and?Local?, showing scores achieved with and with-out joint formulae, respectively.
Clearly, the globalmodels scores are higher than the local scores forall three tasks.
This is also reflected by the last rowof Table 5.
Here we see that we have improvedthe averaged performance across the three tasks byapproximately 2.5% (?
< 0.01, McNemar?s test 2-tailed).
Note that with 3.5% the improvements areparticularly large for Task C.The TempEval test set is relatively small (see Ta-ble 3).
Hence it is not clear how well our resultswould generalize in practice.
To overcome this is-sue, we also evaluated the local and global modelusing 10-fold cross validation on the training data(TRAIN + DEV).
The corresponding results can beseen in Table 6.
Note that the general picture re-mains: performance for all tasks is improved, andthe averaged score is improved only slightly lessthan for the TEST results.
However, this time thescore increase for Task B is lower than before.
We410Table 4: Evaluation Weights for Relaxed ScoringB O A B-O O-A VB 1 0 0 0.5 0 0.33O 0 1 0 0.5 0.5 0.33A 0 0 1 0 0.5 0.33B-O 0.5 0.5 0 1 0.5 0.67O-A 0 0.5 0.5 0.5 1 0.67V 0.33 0.33 0.33 0.67 0.67 1B: BEFORE O: OVERLAPA: AFTER B-O: BEFORE-OR-OVERLAPO-A: OVERLAP-OR-AFTER V: VAGUETable 5: Results on TEST SetLocal Globaltask strict relaxed strict relaxedTask A 0.621 0.669 0.645 0.687Task B 0.737 0.753 0.758 0.777Task C 0.531 0.599 0.566 0.632All 0.641 0.682 0.668 0.708Table 6: Results with 10-fold Cross ValidationLocal Globaltask strict relaxed strict relaxedTask A 0.613 0.645 0.662 0.691Task B 0.789 0.810 0.799 0.819Task C 0.533 0.608 0.552 0.623All 0.667 0.707 0.689 0.727see that this is compensated by much higher scoresfor Task A and C. Again, the improvements for allthree tasks are statistically significant (?
< 10?8,McNemar?s test, 2-tailed).To summarize, we have shown that by tightlyconnecting tasks A, B and C, we can improve tem-poral relation identification significantly.
But arewe just improving a weak baseline, or can jointmodelling help to reach or improve the state-of-the-art results?
We will try to answer this question inthe next section.6.2 Comparison to the State-of-the-artIn order to put our results into context, Table 7shows them along those of other TempEval par-ticipants.
In the first row, TempEval Best givesthe best scores of TempEval for each task.
Notethat all but the strict scores of Task C are achievedby WVALI (Puscasu, 2007), a hybrid system thatcombines machine learning and hand-coded rules.In the second row we see the TempEval averagescores of all six participants in TempEval.
Thethird row shows the results of CU-TMP (Bethardand Martin, 2007), an SVM-based system thatachieved the second highest scores in TempEval forall three tasks.
CU-TMP is of interest because it isthe best pure Machine-Learning-based approach sofar.The scores of our local and global model comein the fourth and fifth row, respectively.
The lastrow in the table shows task-adjusted scores.
Herewe essentially designed and applied three globalMLNs, each one tailored and optimized for a dif-ferent task.
Note that the task-adjusted scores arealways about 1% higher than those of the singleglobal model.Let us discuss the results of Table 7 in detail.
Wesee that for task A, our global model improves analready strong local model to reach the best resultsboth for strict scores (with a 3% points margin) andrelaxed scores (with a 5% points margin).For Task C we see a similar picture: here addingglobal constraints helped to reach the best strictscores, again by a wide margin.
We also achievecompetitive relaxed scores which are in close rangeto the TempEval best results.Only for task B our results cannot reach the bestTempEval scores.
While we perform slightly betterthan the second-best system (CU-TMP), and hencereport the best scores among all pure Machine-Learning based approaches, we cannot quite com-pete with WVALI.6.3 DiscussionLet us discuss some further characteristics andadvantages of our approach.
First, notice thatglobal formulae not only improve strict but also re-laxed scores for all tasks.
This suggests that weproduce more ambiguous labels (such as BEFORE-OR-OVERLAP) in cases where the local model hasbeen overconfident (and wrongly chose BEFOREor OVERLAP), and hence make less ?fatal errors?.Intuitively this makes sense: global consistency iseasier to achieve if our labels remain ambiguous.For example, a solution that labels every relationas VAGUE is globally consistent (but not very in-formative).Secondly, one could argue that our solution tojoint temporal relation identification is too com-plicated.
Instead of performing global inference,one could simply arrange local classifiers for thetasks into a pipeline.
In fact, this has been done byBethard and Martin (2007): they first solve task Band then use this information as features for TasksA and C.While they do report improvements (0.7%411Table 7: Comparison with Other SystemsTask A Task B Task Cstrict relaxed strict relaxed strict relaxedTempEval Best 0.62 0.64 0.80 0.81 0.55 0.64TempEval Average 0.56 0.59 0.74 0.75 0.51 0.58CU-TMP 0.61 0.63 0.75 0.76 0.54 0.58Local Model 0.62 0.67 0.74 0.75 0.53 0.60Global Model 0.65 0.69 0.76 0.78 0.57 0.63Global Model (Task-Adjusted) (0.66) (0.70) (0.76) (0.79) (0.58) (0.64)on Task A, and about 0.5% on Task C), generallythese improvements do not seem as significant asours.
What is more, by design their approach cannot improve the first stage (Task B) of the pipeline.On the same note, we also argue that our ap-proach does not require more implementation ef-forts than a pipeline.
Essentially we only have toprovide features (in the form of formulae) to theMarkov Logic Engine, just as we have to providefor a SVM or MaxEnt classifier.Finally, it became more clear to us that there areproblems inherent to this task and dataset that wecannot (or only partially) solve using global meth-ods.
First, there are inconsistencies in the trainingdata (as reflected by the low inter-annotator agree-ment) that often mislead the learner?this prob-lem applies to learning of local and global formu-lae/features alike.
Second, the training data is rela-tively small.
Obviously, this makes learning of re-liable parameters more difficult, particularly whendata is as noisy as in our case.
Third, the tempo-ral relations in the TempEval dataset only directlyconnect a small subset of events.
This makes globalformulae less effective.147 ConclusionIn this paper we presented a novel approach totemporal relation identification.
Instead of usinglocal classifiers to predict temporal order in a pair-wise fashion, our approach uses Markov Logic toincorporate both local features and global transi-tion rules between temporal relations.We have focused on transition rules betweentemporal relations of the three TempEval subtasks:temporal ordering of events, of events and time ex-pressions, and of events and the document creationtime.
Our results have shown that global transitionrules lead to significantly higher accuracy for allthree tasks.
Moreover, our global Markov Logic14See (Chambers and Jurafsky, 2008) for a detailed discus-sion of this problem, and a possible solution for it.model achieves the highest scores reported so farfor two of three tasks, and very competitive resultsfor the remaining one.While temporal transition rules can also be cap-tured with an Integer Linear Programming ap-proach (Chambers and Jurafsky, 2008), MarkovLogic has at least two advantages.
First, handlingof ?rules of thumb?
between less specific tempo-ral relations (such as OVERLAP or VAGUE) isstraightforward?we simply let the Markov LogicEngine learn weights for these rules.
Second, thereis less engineering overhead for us to perform, be-cause we do not need to generate ILPs for each doc-ument.However, potential for further improvementsthrough global approaches seems to be limited bythe sparseness and inconsistency of the data.
Toovercome this problem, we are planning to use ex-ternal or untagged data along with methods for un-supervised learning in Markov Logic (Poon andDomingos, 2008).Furthermore, TempEval-2 15 is planned for 2010and it has challenging temporal ordering tasks infive languages.
So, we would like to investigate theutility of global formulae for multilingual tempo-ral ordering.
Here we expect that while lexical andsyntax-based features may be quite language de-pendent, global transition rules should hold acrosslanguages.AcknowledgementsThis work is partly supported by the IntegratedDatabase Project, Ministry of Education, Culture,Sports, Science and Technology of Japan.ReferencesSteven Bethard and James H. Martin.
2007.
Cu-tmp:Temporal relation classification using syntactic andsemantic features.
In Proceedings of the 4th Interna-tional Workshop on SemEval-2007., pages 129?132.15http://www.timeml.org/tempeval2/412Branimir Boguraev and Rie Kubota Ando.
2005.Timeml-compliant text analysis for temporal reason-ing.
In Proceedings of the 19th International JointConference on Artificial Intelligence, pages 997?1003.Nathanael Chambers and Daniel Jurafsky.
2008.Jointly combining implicit constraints improves tem-poral ordering.
In Proceedings of the 2008 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 698?706, Honolulu, Hawaii, Oc-tober.
Association for Computational Linguistics.Yuchang Cheng, Masayuki Asahara, and Yuji Mat-sumoto.
2007.
Naist.japan: Temporal relation iden-tification using dependency parsed tree.
In Proceed-ings of the 4th International Workshop on SemEval-2007., pages 245?248.Koby Crammer and Yoram Singer.
2003.
Ultracon-servative online algorithms for multiclass problems.Journal of Machine Learning Research, 3:951?991.Lise Getoor and Ben Taskar.
2007.
Introduction to Sta-tistical Relational Learning (Adaptive Computationand Machine Learning).
The MIT Press.Daphne Koller, 1999.
Probabilistic Relational Models,pages 3?13.
Springer, Berlin/Heidelberg, Germany.Inderjeet Mani, Marc Verhagen, Ben Wellner,Chong Min Lee, and James Pustejovsky.
2006.Machine learning of temporal relations.
In ACL-44:Proceedings of the 21st International Conferenceon Computational Linguistics and the 44th annualmeeting of the Association for Computational Lin-guistics, pages 753?760, Morristown, NJ, USA.Association for Computational Linguistics.Hoifung Poon and Pedro Domingos.
2008.
Joint unsu-pervised coreference resolution with Markov Logic.In Proceedings of the 2008 Conference on Empiri-cal Methods in Natural Language Processing, pages650?659, Honolulu, Hawaii, October.
Association forComputational Linguistics.Georgiana Puscasu.
2007.
Wvali: Temporal relationidentification by syntactico-semantic analysis.
InProceedings of the 4th International Workshop onSemEval-2007., pages 484?487.James Pustejovsky, Jose Castano, Robert Ingria, ReserSauri, Robert Gaizauskas, Andrea Setzer, and Gra-ham Katz.
2003.
The timebank corpus.
In Proceed-ings of Corpus Linguistics 2003, pages 647?656.Matthew Richardson and Pedro Domingos.
2006.Markov logic networks.
In Machine Learning.Sebastian Riedel.
2008.
Improving the accuracy andefficiency of map inference for markov logic.
In Pro-ceedings of UAI 2008.Ben Taskar, Abbeel Pieter, and Daphne Koller.
2002.Discriminative probabilistic models for relationaldata.
In Proceedings of the 18th Annual Conferenceon Uncertainty in Artificial Intelligence (UAI-02),pages 485?492, San Francisco, CA.
Morgan Kauf-mann.Marc Verhagen, Robert Gaizaukas, Frank Schilder,Mark Hepple, Graham Katz, and James Pustejovsky.2007.
Semeval-2007 task 15: Tempeval temporal re-lation identification.
In Proceedings of the 4th Inter-national Workshop on SemEval-2007., pages 75?80.413
