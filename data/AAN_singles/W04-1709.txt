Sentence Completion Tests for Training and Assessment in aComputational Linguistics CurriculumCerstin Mahlow, Michael HessInstitute of Computational Linguistics, University of ZurichWinterthurerstr.
190CH-8057 Zurich,Switzerland,{mahlow, hess}@cl.unizh.chAbstractThis paper presents a novel type of test, halfwaybetween multiple-choice and free-form text,used for training and assessment in severalcourses in a Computational Linguistics curricu-lum.
We will describe the principles of the test,the different ways in which it can be used bylearners, and the tools developed for authoring.Use of this type of test is not limited to the fieldof Computational Linguistics.
Wherever textheavy or even picture based topics are taughtuse of this type of test is possible.1 IntroductionStudents of Computational Linguistics (CL)at the University of Zurich come from twodifferent faculties, viz.
the Faculty of Artsand the Faculty of Economics, Business Man-agement and Information Technology.
Thusthey have a very uneven previous knowledge oflinguistics and programming.
The introductorylectures touch upon most aspects of CL butcannot compensate for these differences in asatisfactory way.
We are trying to ease theproblem by supplying students with extensiveadditional on-line reading material for individ-ual study.
However, until recently students hadno way of testing the knowledge they acquiredthrough self-study against the requirementsof the courses.
For this reason we developedweb-based tools for individual training andself-assessment.Most assessments in web-based learningcourses use Multiple Choice (MC) tests.
Thesetests are easy to create for authors and easyto use for students.
Unfortunately the conceptof MC imposes a very restrictive format onthe tests, and they can basically test only thepresence or absence of small ?knowledge bites?.More general and abstract types of knowledgeare hard to test by means of MC.Free-form text tests, i.e.
tests allowing repliesin the form of mini-essays, are, of course, farless restrictive but the costs of assessing themby hand is, in many institutional contexts,prohibitively high.
Systems for reliable andconsistent automatic assessment of free-formtext are not yet available.
Those that existeither test writing style, or test the presenceor absence in an essay of (explicit) terms or of(implicit) concepts (example: IEA; (Landaueret al, 1998, p295-284)), or use a combinationof surface lexical, syntactic, discourse, andcontent features (example: e-rater; (Burstein,2003)).
It was shown, by the system developersthemselves, that the most advanced of thesesystems, e-rater, can be tricked rather easilyinto giving marks that are far too good, byusing some knowledge of the techniques usedby the system (Powers et al, 2001).
Sinceknowledge of the techniques used by ratingsystems can hardly be kept secret for anylength of time all such feature based systemsare open to this kind of tricks.This is why we developed a new type of test,called ?Satzerga?nzungstests?
(SET) ?
SET1,positioned halfway between multiple-choicetests and free-form text tests.
We use this typeof test for training as well as for assessments,and it is part of our web-based curriculum.The development was funded by the Universityin view of the implementation of the Bache-lor/Master/PhD based ?Bologna scheme?
inmost European universities (see (EuropeanMinisters of Education, 1999)).With SETs we are able to create far moredemanding tasks for training and assessmentthan we could otherwise.
The philosophybehind SETs will be presented in Section 2.1?Sentence Completion Tests?.In Section 3 we will show how the individualstudent can use a test.
In Section 4 we willshow how to create tests.
In section 5, finally,we will give an overview of the courses in whichwe use these tests for teaching ComputationalLinguistics (CL), and discuss in which othercontexts they could be used.2 The philosophy behind SETs(Ru?tter, 1973) creates an extensive topology forassessments.
He distinguishes between open,semi-open and closed tasks.
The distinctionderives from the type of answer expectedfrom the learner: There is no certain answerthe author expects (open tasks), the authorexpects a certain answer the learner has tocreate themselves (semi-open tasks), the learnerhas to choose the right answer(s) from givenpossibilities (closed tasks).
Multiple Choicetasks (MC) belong to the closed tasks.The topology presented by Ru?tter is notrestricted to the easy tasks.
You will also findso-called ?Erweiterungswahlaufgaben?
in theclass of closed assigns.
This task consists ofa piece of information the tested person hasto extend so as to create a coherent pieceof new information.
The learner can choosesuitable extensions from a given list.
Ru?tter?sdescription includes the hint that these tasksare hard to design but present a very clearstructure for the test person.Our Sentence Completion Tests can be seenas an instance of such Erweiterungswahlauf-gaben.
The learner has to answer a complexquestion in near-free-form on the basis ofextensive choices of possible answer compo-nents supplied by the system.
There will beanswer components considered indispensable,some considered correct but optional, otherscategorised as outright wrong, and others stillrated as irrelevant to the question at hand.The required components of the answer will allhave to occur in the answer but not in a fixedorder.In concrete terms this means that a learnerwill author an answer in the form of a completesentence, in a piecemeal fashion and under theguidance of the system, by picking answer frag-ments from dynamically adapted choice menuspopping up as the answer is growing.
At eachstep the user input will be checked by the sys-tem against the answer model that contains allthe expected answer parts, essential relation-ships between them, and possible restrictionson the order of these parts.
At each step aswell as at the very end the system can generatefeedback for the user which will make him un-derstand why and in which aspects his answeris correct or incorrect.3 How to use a SETAll SETs are presented under a web interface.The student has to start a browser2 and choosea single SET3.3.1 Basic elements of a SETThe student sees four coloured fields4, each la-beled with a number and a functional descrip-tion.
These fields are:1.
Text up to now2.
Comments/Feedback3.
List of elements to continue4.
PreviewText up to now contains the question and theanswer in its present state.
List of elementsto continue consists of possible continuationsof the answer.
Clicking on one of the radiobuttons activates the Preview showing all theoptions that will become available once thelearner has committed himself to the givencontinuation.
That way the user is alwaysaware of the consequences his choice mighthave.
The listing field includes two submitbuttons, one for submitting the choice, one forundoing the last choice.
The element list willshow the elements in different order each timethe user reloads or restarts a SET.The crucial field is the one for Com-ment/Feedback.
The user does not merely geta ?Right - Wrong?
feedback but rather?
If the answer contains the correct compo-nents but a wrong relationship the feedbackwill point this out and invite the user to tryagain and find the correct combination.2At the moment the SET web interface is tested withNetscape and InternetExplorer.3Where to find all SETs will be described in sec-tion 5.2.4The same colours are used in our ordinary MC-teststo give the student a familiar feeling for the assessmentsituation.?
If the answer consists of correct compo-nents as well as of wrong ones the feedbackwill say so and point out which componentsare wrong.?
If the answer is one of the correct ones,the feedback will approve this solution andmention the other possible correct answers.This way for every possible combination ofanswer components the user gets a differentoptimised feedback.The text inside the feedback field is displayedas HTML so that it is possible to include linksto related SETs, back to the lecture notes orassociated material.
A feedback text also caninclude a link to a new SET, as a followup.Sometimes it is useful to have the systemgenerate a comment before a complete an-swer has been created by the learner.
Oncethe learner has chosen a certain number ofwrong answer components he will get suitablefeedback before finishing.
In this case thefeedback is used to warn the user that he is ona completely wrong track and that he ought toundo some of the last choices, or to start againfrom scratch.3.2 A sample SETSee figure 1 for a sample session with SET.The initial question is: ?Was ist ein Parser??
(What is a parser?
).Here the user chose ?Ein Parser ist eineProzedur?
(?A parser is a procedure?)
as nextelement in the third field.
This will be thebeginning of his answer.
Clicking on the corre-sponding radio button activated the preview inthe fourth field.
Before submitting the choice,the user can think about the combinationshis choice will allow.
The preview shows 4possibilities to continue with the description ofthe aim of this procedure.If the user is satisfied with his choice, he willclick the submit button ?Auswahl besta?tigen?
(Confirm choice).
This will result in reloadingthe site with the new information.Text bisher (Text up to now) will containthe question, the beginning of the answer andthe fragments added by the learner so far ?EinParser ist eine Prozedur?.
The feedback fieldwill still be empty.
Auswahl der Fortsetzungen(List of elements to continue) will show allpossible continuations.
Vorschau (Preview)will be empty until the user clicks on one of theradio buttons in the list of elements to continue.This sequence of actions will be repeateduntil the user has created a complete sentence.He then gets the feedback.
If he is not satisfiedwith one of his choices before finishing, he canundo the last choice, or simply restart the SET.In case the user is on a probably wrongway he will get feedback before finishing theSET.
See figure 2 for an example.
The usercreated an answer start ?Ein Parser ist eineProzedur, um die syntaktische Korrektheit einerSprache...?
(?A parser is a prozedure to ... thesyntactical correctness of a language?).
Theintervening feedback points to the principle ofcorrectness concerning certain constructionsof languages and prompts the user to undothe last decission(s).
(?Was wohl soll diesyntaktische Korrektheit einer Sprache sein?
!Nur einzelne Konstruktionen einer Spracheko?nnen korrekt oder inkorrekt sein.
EinenSchritt zuru?ck!
?Figure 3 shows the finished SET.
The userfollowed the hint in the intervening feedbackshown in figure 2.
He removed the part ?einerSprache?
(?of a language?).
The answercreated by the user is ?Ein Parser ist eineProzedur um die syntaktische Korrektheit einesSatzes zu ermitteln?
(?A parser is a procedureto detect the syntactical correctnes of a sen-tence?).
Clearly, this answer is not correct.
Itdescribes rather an acceptor than a parser.
Thecomment says so and then offers a correct def-inition with a hint to the latin origins of Parser.3.3 Training mode and assessmentmodeSETs can be used in E-Learning for training aswell as for assessments.
Self-assessment can beseen as an instrument for training ?
users getelaborate feedback for their answers and areinvited to try again.In the training/self-assessment mode usersget feedback after completing the answer orwhile composing it.
The feedback always takesinto account all components collected up toFigure 1: Snapshot of a SET session for answering the SET ?What is a parser?
?that point as well as the user?s undo or redoactions.
The user is allowed to undo a decisionas often as he likes.
This way finding the rightanswer is a question of either knowing it orfollowing the hints in the feedback.In the assessment mode the user gets anumber of points credited.
The points total iscompiled the same way the feedback is created.Depending on the answer fragments chosen bythe learner, and on their order, the points totalwill be computed.
It is also possible to chainseveral SETs one after another5, collect thecredits collected in each of them, and presentthe grand total at the very end.The user can be allowed to use the undo but-ton in different manners.
Three settings arepossible:?
The undo button can be used as often asthe learner wants but each use is logged inthe background.5SETs can be linked in linear or network like fashionvia HTML links or followups in the comments.?
Each use of the undo button results in adeduction of a certain number of points,and its use is logged.?
The use of the button is allowed only a pre-set number of times ?
if the user tries toundo more often, the button is disabled.That way tutors can track whether the studentarrived at the answer by merely trying out allpossible continuations.4 How to create a SETWhat does an author have to consider whencreating a SET?
First, he has to decide whichanswer elements the user can choose from atany given step.
Second, he must make surethat any of the answer components offered asa choice at a given step will contribute to awell-formed sentence only.
Finally, helpful andsyntactically well-formed comments have to bedefined for any of the possible answers.What the presentation of a SET ultimatelyboils down to is running a Finite State Automa-ton (FSA), with answer components as statesFigure 2: Snapshot of an intermidate result while answering the SET ?What is a parser?
?and user choices as input.
This is done by aProlog program as the back-end for a singleSET.
As input it takes the SET specific Prologautomaton, the path up to now, and the currentchoice of the user.
As output it creates the newcurrent answer, the new list of elements to con-tinue, the preview, comments, paths and points.The author of a SET has thus to writea (potentially large) FSA.
This is a tediousand error-prone task.
How can this be doneefficiently and reliably?4.1 The machinery behind a SETDeveloping the automaton normally startswith the author writing a number of possiblecorrect and incorrect answers, in a way similarto the development of an ordinary MC.
Theauthor then marks where these sentences couldbe split into fragments.
Splitting must allowthe combination of various sentence fragmentsfrom different sentences in a way that onlywell-formed passages result.
To limit thenumber of such combinations the author candefine constraints that explicitly include orexclude certain combinations.To increase readability, answer fragmentsthat are of the same syntactic type can becollected in boxes.
It is, however, advisableto create distinct boxes for correct fragments,wrong fragments, and indifferent fragments ofthe same syntactic type; this makes the designof complex automata considerably easier.
Eachbox has an ID, in-going and outgoing boxes6,information concerning specific constraintson allowed combinations, and (positive ornegative) credits the user will collect whenchoosing this element.
Boxes are linked byvectored edges to create a number of pathsthrough the answer fragments, each one ofwhich will define a complete and syntacticallywell-formed sentence.Splitting answer sentences into fragmentsthat can be combined freely creates, of course,a large number of potential answers (in fact,6Except start boxes ?
no in-going box ?
and the boxesat the end of a sentence path ?
no outgoing box.Figure 3: Snapshot of the finished SET session for answering the SET ?What is a parser?
?a potentially infinite number).
It wouldbe clearly impossible to write individualcomments for each of these answers.
We over-come this obstacle by generating comments,semi-automatically in some cases, and fullyautomatically in others.
The semi-automaticalcreation relies on the fact that each answerfragment can be rated according to its correct-ness and relevance for a given question.
It isrelatively easy to attach, to a limited numberof ?strategically important?
answer fragments,comment fragments specifying in what waythey are (in)correct and (ir)relevant.
We thenhave SET collect the comment fragments ofall answer fragments chosen by the learner,and combine them into complete and syntac-tically well-formed comments that refer to theindividual parts of an answer and point outsuperfluous, missing, or wrong bits, in anydegree of detail desired by the author.
Wecan even generate comments on the basis ofarbitrarily complex logical conditions over an-swer fragments, thus identifying, among others,contradictions in answers.
That way we cangenerate a potentially infinite number of com-ments on the basis of relatively few commentfragments.
This is the semi-automatic creationof comments, taking into account the localproperties of an answer path.
We also allowthe fully automatic creation of comments thattake into consideration the global propertiesof answer paths.
Thus the fact that a learnerused the undo button very often in variousplaces, or took a very circuitous way to arriveat his answer, may be detected by measuringglobal values of the answer path and can thenbe commented upon automatically7.
For adetailed documentation see (Brodersen andLee, 2003).4.2 Developing a sample SETClearly the author of a SET must be supportedin the design of even moderately complex FSAs.To this end we developed an authoring toolcalled Satzerga?nzungstest-Ersteller-Interface(SETEI), a Java application with a GUI.
It uses7Resulting in comments like ?You used the undo but-ton way too often.?
or ?Correct but your answer couldhave been much shorter?, etc.a text-based format for saving data and has anexport function to create the FSA.
Figure 4shows the final stages in the development ofthe SET ?Was ist ein Parser??
(?What is aparser??)
used as example in section 3.2.The box in the left upper corner is thestart box, containing the question.
Boxes1, 2, 3, 4, 6, 7, 8, 9, 13 are answer boxescontaining answer fragments.
Boxes 10, 11, 12,14 are comment boxes containing commentsfor complete answers or certain combinationsof answer parts (box 14).One of the boxes, box 14, is selected, andinside this box the text element 72 is selected.As the boxes offer limited space the full text ofa selected element is shown at the very bottomof the window.
Here we can also see the boxnumber, fragment number, and the creditsattached to the selected answer fragment.These credits can be used, in assessment mode,to grade the answer.
Creating, filling, andmodifying boxes is a matter of a few clicks.The possible answer paths are represented,obviously, as vectored edges between boxes.Each path must end in a comment box.?
Two paths contain three boxes ?
1?8?9and 1?2?7?
Two paths contain four boxes ?
1?2?3?7and 1?2?6?13?
One path contains five boxes ?1?2?3?4?7Possible answers in the above example maythus consist of three, four or five parts.
Sinceeach answer box contains at least two textelements this automaton defines many moreanswers than there are paths.
On path 1?2, forinstance, the user can combine each element inbox 1 with each element in box 2.
Connectionsbetween boxes are created or deleted by simpledragging or clicking operations.
Whenever acircular connection is created, even an indirectone, the user is asked whether this is what hereally wanted to do.The top menu in the window contains thevarious tools for the manipulation of boxes.Thus, to see all text elements in one box plusall the in-going and out-going boxes and theconstraints for elements, the author may usethe box browser Ansicht (view).
The browserpresents a magnified view on the given boxwith additional functionalities to edit thebox content.
The user can also zoom outand see the bare structure of the entire FSA,without box contents, can select sub-parts ofthe automaton and try them out in isolation,etc.To allow intermediate feedback, commentboxes may be placed in the middle of the FSA(such as, in this SET, comment box 14).
Allanswer paths end with a comment box to givefeedback after creating a complete sentence.5 Where to use SET5.1 Where we use SETsSince winter term 2003/2004 we use SETs atour institute as a training and self-assessmenttool in introductory courses on CL.
They areoften used as final element in learning units in-tended for self-study by students.
These learn-ing units each cover one particular aspect ofComputational Linguistics that may be unfa-miliar to part of the audience (such as regu-lar expression, tokenising, tagging or parsing).They are organised around Problem-based In-teractive Learning Applications.8 While simpleskills can be tested with standard MC meth-ods, for more general and more abstract typesof knowledge SETs turned out to be a much bet-ter solution.
Any type of question that would,ideally, require a free form answer can be turnedinto a SET.
These are definitional questions(?What is a parser??)
as well as a questionsrequiring comparisons between concepts (?Howdoes a parser differ from an acceptor??)
and thedescription of procedures (?What are the pro-cessing steps of a transfer Machine Translationsystem??).
It is important that SETs can deter-mine, and comment upon, non-local properitiesof answers.
Thus a SET can detect contradic-tions between different parts of an answer, or awrong sequencing in the description of process-ing steps (say, putting tokenising after parsing),or repetitions, all of which may occur in parts ofan answer that are arbitrarily far removed fromeach other.8See (Carstensen and Hess, 2003) for more informa-tion.Figure 4: SETEI session for creating the SET ?What is a parser?
?5.2 Real Examples of SETsSETs have been developed mainly for the intro-ductory classes in Computational Linguisticsat Zurich but new tests for more advancedcourses are under development.
Since classesare taught in German, all SETs are in German,too.Students can access SETs in two ways:?
As most SETs are used in Learning Unitsstudents will encounter SETs for the firsttime when they are working their waythrough the Learning Units.?
When preparing for exams students wantto have random access to SETs.
For thisreason all SETs ever developed are accessi-ble via one big collection, our Setcenter.The Setcenterwww.cl.unizh.ch/ict-open/satztest/setcenter.htmloffers a check-box list to create a customisedweb page containing a short introduction toSETs, help for using them, and a list of linksto the chosen SETs.
For a first look at SETsthe page www.ifi.unizh.ch/cl/ict-open/satztest/,with pre-defined examples from outside thefield of Computational Linguistics, may also beuseful.Most of the SETs we developed ask questionsabout the basic concepts and terms of the field.Some examples are listed in table 1.In some case we also ?abuse?
SETs tofunction itself as authoring tool with feedbackfacilities.
In one case students are asked toIntro to CL 1 Intro to CL 2CL Extension / IntensionLinguistics PropositionsMorphology PresuppositionsSemantics AxiomsParsing Modus PonensFSA Lambda AbstractingGenerative PowerTypes of AmbiguityIndexingInformation ExtractionInformation RetrievalMachine TranslationTable 1: SETs in the introductory lectures forCLwrite specific rules for a chunking grammar.In a SET, they get a set of rule elementsto choose from (pre-terminal and terminalcategories, parentheses, Kleene star, etc.)
andhave to combine them, step by step, creating agrammar rule in the process.
If their choice of asymbol is completely off track (such as a gram-mar rule beginning with a closing parenthesis)they are warned right away.
Otherwise thestructure of the completed rule is commentedupon.
If the rule is not correct, users are sentback to the beginning.
Otherwise they are sentto a subsequent SET, with a more demandingtask.
That way, by chaining SETs, we teachthem to write increasingly complex chunkingrules, under close guidance of the system.
Thisturned out to be a very promising use of SETs.5.3 Use of SET in other topicsThe question arises whether it would bepossible to use SETs in fields other than CL.In general, in all fields where short textualdescriptions are the best way to answer ques-tions, SETs are a good way to automatisetraining and testing.
SETs are of particularinterest to the Arts and Humanities, butthe Medical Sciences might also be a fieldthat could benefit form SETs (for instance,a picture is presented and the user is askedto describe what seems important or abnormal).6 ConclusionsIn training or assessment situations wherecorrect answers to questions do not consist ofone (or a few) isolated items (words, numbers,symbols) but where a complete description innatural language is required, and when humantutors are not available, SET is the right toolto use.
It allows to simulate, to some extent,the detailed comments to individual aspects ofan answer that make human tutors so valuable.While SETs are great once they have beenwritten, the process of authoring them is stillpainful, demanding, error-prone, and thusextremely time-consuming.
We will needauthoring tools that allow a top-down kind ofdesign for SETs, with stepwise refinement ofthe code and on-the-fly testing of selected partsof the FSA, instead of the low-level designprocess used now.
It would also be very usefulto have programs that work, bottom-up, frompossible answers to FSAs, by automaticallyidentifying common phrases in answers andcollecting them in boxes.
We developed sucha system and found it very useful but itsgrammatical coverage is too small to make itviable in practice.
The automatic creation ofterminological variations in potential answers,by accessing on-line lexical resources, will beanother feature that might make life easier fortest developers.
We continue work on all ofthese lines of research.7 AcknowledgementsOur thanks go to Sonja Brodersen and DavidLee, who developed the SET and SETEI en-vironments, to Esther Kaufmann, who createdmost of the existing SETs, and to Kai-UweCarstensen for valuable feedback on the results.ReferencesSonja Brodersen and David Lee.
2003.Dynamisches Multiple-Choice mit Satz-Erga?nzungstests.
Dokumentation zumgesamten Satztestprojekt.
unpublished,December 2003.Jill Burstein.
2003.
The e-rater scoring en-gine: Automated essay scoring with nat-ural language processing.
In M. D. Sher-mis and J. Burstein, editors, Automated es-say scoring: A cross-disciplinary perspective.Lawrence Erlbaum Associates, Inc., Hillsdale,NJ.Kai-Uwe Carstensen and Michael Hess.2003.
Problem-based web-based teachingin a computational linguistics curriculum.www.linguistik-online.de, 17(5/2003).European Ministers of Education.
1999.The bologna declaration of 19 june 1999.www.bologna-berlin2003.de/pdf/bologna-declaration.pdf, June 1999.T.
K. Landauer, P. W. Foltz, and D. Laham.1998.
Introduction to Latent Semantic Anal-ysis.
Discourse Processes.Donald E. Powers, Jill Burstein, MartinChodorow, Mary E. Fowles, and Karen Ku-kich.
2001.
Stumping E-Rater: Challeng-ing the Validity of Automated Essay Scoring.GRE Research, GRE Board Professional Re-port No.
98-08bP, ETS Research Report 01-03.Theodor Ru?tter.
1973.
Formen der Tes-taufgabe.
Eine Einfu?hrung fu?r didaktischeZwecke.
C.H.Beck.
