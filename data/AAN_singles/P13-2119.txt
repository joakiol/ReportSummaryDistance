Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 678?683,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsAdaptation Data Selection using Neural Language Models:Experiments in Machine TranslationKevin Duh, Graham NeubigGraduate School of Information ScienceNara Institute of Science and Technology8916-5 Takayama, Ikoma, Japankevinduh@is.naist.jpneubig@is.naist.jpKatsuhito Sudoh, Hajime TsukadaNTT Communication Science Labs.NTT Corporation2-4 Hikaridai, Seika, Kyoto, Japansudoh.katsuhito@lab.ntt.co.jptsukada.hajime@lab.ntt.co.jpAbstractData selection is an effective approachto domain adaptation in statistical ma-chine translation.
The idea is to use lan-guage models trained on small in-domaintext to select similar sentences from largegeneral-domain corpora, which are thenincorporated into the training data.
Sub-stantial gains have been demonstrated inprevious works, which employ standard n-gram language models.
Here, we explorethe use of neural language models for dataselection.
We hypothesize that the con-tinuous vector representation of words inneural language models makes them moreeffective than n-grams for modeling un-known word contexts, which are prevalentin general-domain text.
In a comprehen-sive evaluation of 4 language pairs (En-glish to German, French, Russian, Span-ish), we found that neural language mod-els are indeed viable tools for data se-lection: while the improvements are var-ied (i.e.
0.1 to 1.7 gains in BLEU), theyare fast to train on small in-domain dataand can sometimes substantially outper-form conventional n-grams.1 IntroductionA perennial challenge in building Statistical Ma-chine Translation (SMT) systems is the dearthof high-quality bitext in the domain of interest.An effective and practical solution is adaptationdata selection: the idea is to use language models(LMs) trained on in-domain text to select similarsentences from large general-domain corpora.
Theselected sentences are then incorporated into theSMT training data.
Analyses have shown that thisaugmented data can lead to better statistical esti-mation or word coverage (Duh et al, 2010; Had-dow and Koehn, 2012).Although previous works in data selection (Ax-elrod et al, 2011; Koehn and Haddow, 2012; Ya-suda et al, 2008) have shown substantial gains, wesuspect that the commonly-used n-gram LMs maybe sub-optimal.
The small size of the in-domaintext implies that a large percentage of general-domain sentences will contain words not observedin the LM training data.
In fact, as many as 60% ofgeneral-domain sentences contain at least one un-known word in our experiments.
Although the LMprobabilities of these sentences could still be com-puted by resorting to back-off and other smoothingtechniques, a natural question remains: will alter-native, more robust LMs do better?We hypothesize that the neural language model(Bengio et al, 2003) is a viable alternative, sinceits continuous vector representation of words iswell-suited for modeling sentences with frequentunknown words, providing smooth probability es-timates of unseen but similar contexts.
Neu-ral LMs have achieved positive results in speechrecognition and SMT reranking (Schwenk et al,2012; Mikolov et al, 2011a).
To the best of ourknowledge, this paper is the first work that exam-ines neural LMs for adaptation data selection.2 Data Selection MethodWe employ the data selection method of (Ax-elrod et al, 2011), which builds upon (Mooreand Lewis, 2010).
The intuition is to selectgeneral-domain sentences that are similar to in-domain text, while being dis-similar to the averagegeneral-domain text.To do so, one defines the score of an general-domain sentence pair (e, f) as:[INE(e)?GENE(e)] + [INF (f)?GENF (f)](1)where INE(e) is the length-normalized cross-entropy of e on the English in-domain LM.GENE(e) is the length-normalized cross-entropy678Figure 1: Recurrent neural LM.of e on the English general-domain LM, whichis built from a sub-sample of the general-domaintext.
Similarly, INF (f) and GENF (f) are thecross-entropies of f on Foreign-side LM.
Finally,sentence pairs are ranked according to Eq.
1 andthose with scores lower than some empirically-chosen threshold are added to the bitext for trans-lation model training.2.1 Neural Language ModelsThe four LMs used to compute Eq.
1 have con-ventionally been n-grams.
N-grams of the formp(w(t)|w(t ?
1), w(t ?
2), .
.
.)
predict words byusing multinomial distributions conditioned on thecontext (w(t?1), w(t?2), .
.
.).
But when the con-text is rare or contains unknown words, n-gramsare forced to back-off to lower-order models, e.g.p(w(t)|w(t ?
1)).
These backoffs are unfortu-nately very frequent in adaptation data selection.Neural LMs, in contrast, model word probabili-ties using continuous vector representations.
Fig-ure 1 shows a type of neural LMs called recurrentneural networks (Mikolov et al, 2011b).1 Ratherthan representing context as an identity (n-gramhit-or-miss) function on [w(t ?
1), w(t ?
2), .
.
.
],neural LMs summarize the context by a hiddenstate vector s(t).
This is a continuous vector ofdimension |S| whose elements are predicted bythe previous word w(t ?
1) and previous states(t ?
1).
This is robust to rare contexts becausecontinuous representations enable sharing of sta-tistical strength between similar contexts.
Bengio(2009) shows that such representations are betterthan multinomials in alleviating sparsity issues.1Another major type of neural LMs are the so-calledfeed-forward networks (Bengio et al, 2003; Schwenk, 2007;Nakamura et al, 1990).
Both types of neural LMs have seenmany improvements recently, in terms of computational scal-ability (Le et al, 2011) and modeling power (Arisoy et al,2012; Wu et al, 2012; Alexandrescu and Kirchhoff, 2006).We focus on recurrent networks here since there are fewerhyper-parameters and its ability to model infinite context us-ing recursion is theoretically attractive.
But we note that feed-forward networks are just as viable.Now, given state vector s(t), we can predict theprobability of the current word.
Figure 1 is ex-pressed formally in the following equations:w(t) = [w0(t), .
.
.
, wk(t), .
.
.
w|W |(t)] (2)wk(t) = g??|S|?j=0sj(t)Vkj??
(3)sj(t)=f?
?|W |?i=0wi(t?
1)Uji +|S|?i?=0si?(t?
1)Aji???
(4)Here, w(t) is viewed as a vector of dimension|W | (vocabulary size) where each element wk(t)represents the probability of the k-th vocabularyitem at sentence position t. The function g(zk) =ezk/?k ezk is a softmax function that ensures theneural LM outputs are proper probabilities, andf(z) = 1/(1 + e?z) is a sigmoid activation thatinduces the non-linearity critical to the neural net-work?s expressive power.
The matrices V , U , andA are trained by maximizing likelihood on train-ing data using a ?backpropagation-through-time?method.2 Intuitively, U and A compress the con-text (|S| < |W |) such that contexts predictive ofthe same word w(t) are close together.Since proper modeling of unknown contexts isimportant in our problem, training text for both n-gram and neural LM is pre-processed by convert-ing all low-frequency words in the training data(frequency=1 in our case) to a special ?unknown?token.
This is used only in Eq.
1 for selectinggeneral-domain sentences; these words retain theirsurface forms in the SMT train pipeline.3 Experiment SetupWe experimented with four language pairs in theWIT3 corpus (Cettolo et al, 2012), with English(en) as source and German (de), Spanish (es),French (fr), Russian (ru) as target.
This is thein-domain corpus, and consists of TED Talk tran-scripts covering topics in technology, entertain-ment, and design.
As general-domain corpora,we collected bitext from the WMT2013 campaign,including CommonCrawl and NewsCommentaryfor all 4 languages, Europarl for de/es/fr, UN fores/fr, Gigaword for fr, and Yandex for ru.
The in-domain data is divided into a training set (for SMT2The recurrent states are unrolled for several time-steps,then stochastic gradient descent is applied.679en-de en-es en-fr en-ruIn-domain Training Set#sentence 129k 140k 139k 117k#token (en) 2.5M 2.7M 2.7M 2.3M#vocab (en) 26k 27k 27k 25k#vocab (f) 42k 39k 34k 58kGeneral-domain Bitext#sentence 4.4M 14.7M 38.9M 2.0M#token (en) 113M 385M 1012M 51M%unknown 60% 58% 64% 65%Table 1: Data statistics.
?%unknown?=fraction ofgeneral-domain sentences with unknown words.pipeline and neural LM training), a tuning set (forMERT), a validation set (for choosing the optimalthreshold in data selection), and finally a testset of1616 sentences.3 Table 1 lists data statistics.For each language pair, we built a baseline in-data SMT system trained only on in-domain data,and an alldata system using combined in-domainand general-domain data.4 We then built 3 systemsfrom augmented data selected by different LMs:?
ngram: Data selection by 4-gram LMs withKneser-Ney smoothing (Axelrod et al, 2011)?
neuralnet: Data selection by Recurrent neu-ral LM, with the RNNLM Toolkit.5?
combine: Data selection by interpolated LMusing n-gram & neuralnet (equal weight).All systems are built using standard settings inthe Moses toolkit (GIZA++ alignment, grow-diag-final-and, lexical reordering models, and SRILM).Note that standard n-grams are used as LMs forSMT; neural LMs are only used for data selection.Multiple SMT systems are trained by thresholdingon {10k,50k,100k,500k,1M} general-domain sen-tence subsets, and we empirically determine thesingle system for testing based on results on a sep-arate validation set (in practice, 500k was chosenfor fr and 1M for es, de, ru.
).3The original data are provided by http://wit3.fbk.eu andhttp://www.statmt.org/wmt13/.
Our domain adaptation sce-nario is similar to the IWSLT2012 campaign but we used ourown random train/test splits, since we wanted to ensure thetestset for all languages had identical source sentences forcomparison purposes.
For replicability, our software is avail-able at http://cl.naist.jp/?kevinduh/a/acl2013.4More advanced phrase table adaptation methods are pos-sible.
but our interest is in comparing data selection methods.The conclusions should transfer to advanced methods such as(Foster et al, 2010; Niehues and Waibel, 2012).5http://www.fit.vutbr.cz/?imikolov/rnnlm/4 Results4.1 LM Perplexity and Training TimeFirst, we measured perplexity to check the gen-eralization ability of our neural LMs as languagemodels.
Recall that we train four LMs to com-pute each of the components of Eq.
1.
In Table 2,we compared each of the four versions of ngram,neuralnet, and combine LMs on in-domain testsets or general-domain held-out sets.
It re-affirmsprevious positive results (Mikolov et al, 2011a),with neuralnet outperforming ngram by 20-30%perplexity across all tasks.
Also, combine slightlyimproves the perplexity of neuralnet.Task ngram neuralnet combineIn-Domain Test Seten-de de 157 110 (29%) 110 (29%)en-de en 102 81 (20%) 78 (24%)en-es es 129 102 (20%) 98 (24%)en-es en 101 80 (21%) 77 (24%)en-fr fr 90 67 (25%) 65 (27%)en-fr en 102 80 (21%) 77 (24%)en-ru ru 208 167 (19%) 155 (26%)en-ru en 103 83 (19%) 79 (23%)General-Domain Held-out Seten-de de 234 174 (25%) 161 (31%)en-de en 218 168 (23%) 155 (29%)en-es es 62 43 (31%) 43 (31%)en-es en 84 61 (27%) 59 (30%)en-fr fr 64 43 (33%) 43 (33%)en-fr en 95 67 (30%) 65 (32%)en-ru ru 242 199 (18%) 176 (27%)en-ru en 191 153 (20%) 142 (26%)Table 2: Perplexity of various LMs.
Number inparenthesis is percentage improvement vs. ngram.Second, we show that the usual concern of neu-ral LM training time is not so critical for the in-domain data sizes used domain adaptation.
Thecomplexity of training Figure 1 is dominated bycomputing Eq.
3 and scales as O(|W | ?
|S|) inthe number of tokens.
Since |W | can be large, onepractical trick is to cluster the vocabulary so thatthe output dimension is reduced.
Table 3 showsthe training times on a 3.3GHz XeonE5 CPU byvarying these two main hyper-parameters (|S| andcluster size).
Note that the setting |S| = 200 andcluster size of 100 already gives good perplexityin reasonable training time.
All neural LMs in thispaper use this setting, without additional tuning.680|S| Cluster Time Perplexity200 100 198m 110100 |W | 12915m 110200 400 208m 113100 100 52m 118100 400 71m 120Table 3: Training time (in minutes) for variousneural LM architectures (Task: en-de de).4.2 End-to-end SMT EvaluationTable 4 shows translation results in terms of BLEU(Papineni et al, 2002), RIBES (Isozaki et al,2010), and TER (Snover et al, 2006).
We observethat all three data selection methods essentiallyoutperform alldata and indata for all languagepairs, and neuralnet tend to be the best in all met-rics.
E.g., BLEU improvements over ngram arein the range of 0.4 for en-de, 0.5 for en-es, 0.1for en-fr, and 1.7 for en-ru.
Although not all im-provements are large in absolute terms, many arestatistically significant (95% confidence).We therefore believe that neural LMs are gen-erally worthwhile to try for data selection, as itrarely underperform n-grams.
The open questionis: what can explain the significant improvementsin, for example Russian, Spanish, German, but thelack thereof in French?
One conjecture is thatneural LMs succeeded in lowering testset out-of-vocabulary (OOV) rate, but we found that OOVreduction is similar across all selection methods.The improvements appear to be due to betterprobability estimates of the translation/reorderingmodels.
We performed a diagnostic by decodingthe testset using LMs trained on the same test-set, while varying the translation/reordering ta-bles with those of ngram and neuralnet; this is akind of pseudo forced-decoding that can inform usabout which table has better coverage.
We foundthat across all language pairs, BLEU differences oftranslations under this diagnostic become insignif-icant, implying that the raw probability value isthe differentiating factor between ngram and neu-ralnet.
Manual inspection of en-de revealed thatmany improvements come from lexical choice inmorphological variants (?meinen Sohn?
vs.
?meinSohn?
), segmentation changes (?baking soda?
??Backpulver?
vs. ?baken Soda?
), and handling ofunaligned words at phrase boundaries.Finally, we measured the intersection betweenthe sentence set selected by ngram vs neural-Task System BLEU RIBES TERen-de indata 20.8 80.1 59.0alldata 21.5 80.1 59.1ngram 21.5 80.3 58.9neuralnet 21.9+ 80.5+ 58.4combine 21.5 80.2 58.8en-es indata 30.4 83.5 48.7alldata 31.2 83.2 49.9ngram 32.0 83.7 48.4neuralnet 32.5+ 83.7 48.3+combine 32.5+ 83.8 48.3+en-fr indata 31.4 83.9 51.2alldata 31.5 83.5 51.4ngram 32.7 83.7 50.4neuralnet 32.8 84.2+ 50.3combine 32.5 84.0 50.5en-ru indata 14.8 72.5 69.5alldata 23.4 75.0 62.3ngram 24.0 75.7 61.4neuralnet 25.7+ 76.1 60.0+combine 23.7 75.9 61.9?Table 4: End-to-end Translation Results.
The bestresults are bold-faced.
We also compare neuralLMs to ngram using pairwise bootstrap (Koehn,2004): ?+?
means statistically significant im-provement and ???
means significant degradation.net.
They share 60-75% of the augmented train-ing data.
This high overlap means that ngramand neuralnet are actually not drastically differentsystems, and neuralnet with its slightly better se-lections represent an incremental improvement.65 ConclusionsWe perform an evaluation of neural LMs foradaptation data selection, based on the hypothe-sis that their continuous vector representations areeffective at comparing general-domain sentences,which contain frequent unknown words.
Com-pared to conventional n-grams, we observed end-to-end translation improvements from 0.1 to 1.7BLEU.
Since neural LMs are fast to train in thesmall in-domain data setting and achieve equal orincrementally better results, we conclude that theyare an worthwhile option to include in the arsenalof adaptation data selection techniques.6This is corroborated by another analysis: taking theunion of sentences found by ngram and neuralnet gives sim-ilar BLEU scores as neuralnet.681AcknowledgmentsWe thank Amittai Axelrod for discussions aboutdata selection implementation details, and ananonymous reviewer for suggesting the union ideafor results analysis.
K. D. would like to credit Spy-ros Matsoukas (personal communication, 2010)for the trick of using LM-based pseudo forced-decoding for error analysis.ReferencesAndrei Alexandrescu and Katrin Kirchhoff.
2006.Factored neural language models.
In Proceed-ings of the Human Language Technology Confer-ence of the NAACL, Companion Volume: Short Pa-pers, NAACL-Short ?06, pages 1?4, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Ebru Arisoy, Tara N. Sainath, Brian Kingsbury, andBhuvana Ramabhadran.
2012.
Deep neural networklanguage models.
In Proceedings of the NAACL-HLT 2012 Workshop: Will We Ever Really Replacethe N-gram Model?
On the Future of LanguageModeling for HLT, pages 20?28, Montre?al, Canada,June.
Association for Computational Linguistics.Amittai Axelrod, Xiaodong He, and Jianfeng Gao.2011.
Domain adaptation via pseudo in-domain dataselection.
In Proceedings of the 2011 Conference onEmpirical Methods in Natural Language Process-ing, pages 355?362, Edinburgh, Scotland, UK., July.Association for Computational Linguistics.Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, andChristian Jauvin.
2003.
A neural probabilistic lan-guage models.
JMLR.Yoshua Bengio.
2009.
Learning Deep Architecturesfor AI, volume Foundations and Trends in MachineLearning.
NOW Publishers.Mauro Cettolo, Christian Girardi, and Marcello Fed-erico.
2012.
Wit3: Web inventory of transcribedand translated talks.
In Proceedings of the 16th Con-ference of the European Association for MachineTranslation (EAMT), pages 261?268, Trento, Italy,May.Kevin Duh, Katsuhito Sudoh, and Hajime Tsukada.2010.
Analysis of translation model adaptation forstatistical machine translation.
In Proceedings of theInternational Workshop on Spoken Language Trans-lation (IWSLT) - Technical Papers Track.George Foster, Cyril Goutte, and Roland Kuhn.
2010.Discriminative instance weighting for domain adap-tation in statistical machine translation.
In EMNLP.Barry Haddow and Philipp Koehn.
2012.
Analysingthe effect of out-of-domain data on smt systems.
InProceedings of the Seventh Workshop on Statisti-cal Machine Translation, pages 422?432, Montre?al,Canada, June.
Association for Computational Lin-guistics.Hideki Isozaki, Tsutomu Hirao, Kevin Duh, KatsuhitoSudoh, and Hajime Tsukada.
2010.
Automaticevaluation of translation quality for distant languagepairs.
In Proceedings of the 2010 Conference onEmpirical Methods in Natural Language Process-ing, pages 944?952, Cambridge, MA, October.
As-sociation for Computational Linguistics.Philipp Koehn and Barry Haddow.
2012.
Towardseffective use of training data in statistical machinetranslation.
In WMT.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In EMNLP.Hai-Son Le, I. Oparin, A. Allauzen, J. Gauvain, andF.
Yvon.
2011.
Structured output layer neural net-work language model.
In Acoustics, Speech and Sig-nal Processing (ICASSP), 2011 IEEE InternationalConference on, pages 5524?5527.Toma?s?
Mikolov, Anoop Deoras, Daniel Povey, Luka?s?Burget, and Jan C?ernocky?.
2011a.
Strategies fortraining large scale neural network language model.In ASRU.Toma?s?
Mikolov, Stefan Kombrink, Luka?s?
Burget, JanC?ernocky?, and Sanjeev Khudanpur.
2011b.
Exten-sions of recurrent neural network language model.In Proceedings of the 2011 IEEE International Con-ference on Acoustics, Speech, and Signal Processing(ICASSP).Robert C. Moore and William Lewis.
2010.
Intelligentselection of language model training data.
In Pro-ceedings of the ACL 2010 Conference Short Papers,pages 220?224, Uppsala, Sweden, July.
Associationfor Computational Linguistics.Masami Nakamura, Katsuteru Maruyama, TakeshiKawabata, and Kiyohiro Shikano.
1990.
Neuralnetwork approach to word category prediction forenglish texts.
In Proceedings of the 13th conferenceon Computational linguistics - Volume 3, COLING?90, pages 213?218, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.Jan Niehues and Alex Waibel.
2012.
Detailed analysisof different strategies for phrase table adaptation inSMT.
In AMTA.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A method for automaticevaluation of machine translation.
In ACL.Holger Schwenk, Anthony Rousseau, and MohammedAttik.
2012.
Large, pruned or continuous spacelanguage models on a gpu for statistical machinetranslation.
In Proceedings of the NAACL-HLT 2012Workshop: Will We Ever Really Replace the N-gramModel?
On the Future of Language Modeling for682HLT, pages 11?19, Montre?al, Canada, June.
Associ-ation for Computational Linguistics.Holger Schwenk.
2007.
Continuous space languagemodels.
Comput.
Speech Lang., 21(3):492?518,July.M.
Snover, B. Dorr, R. Schwartz, L. Micciulla, andJ.
Makhoul.
2006.
A study of translation edit ratewith targeted human annotation.
In AMTA.Youzheng Wu, Xugang Lu, Hitoshi Yamamoto,Shigeki Matsuda, Chiori Hori, and Hideki Kashioka.2012.
Factored language model based on recurrentneural network.
In Proceedings of COLING 2012,pages 2835?2850, Mumbai, India, December.
TheCOLING 2012 Organizing Committee.Keiji Yasuda, Ruiqiang Zhang, Hirofumi Yamamoto,and Eiichiro Sumita.
2008.
Method of selectingtraining data to build a compact and efficient trans-lation model.
In ICJNLP.683
