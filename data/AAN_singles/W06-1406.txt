Proceedings of the Fourth International Natural Language Generation Conference, pages 33?40,Sydney, July 2006. c?2006 Association for Computational LinguisticsUsing Distributional Similarity to Identify Individual Verb ChoiceJing LinDepartment of Computing ScienceUniversity of Aberdeenjlin@csd.abdn.ac.ukAbstractHuman text is characterised by the indi-vidual lexical choices of a specific au-thor.
Significant variations exist be-tween authors.
In contrast, natural lan-guage generation systems normally pro-duce uniform texts.
In this paper weapply distributional similarity measuresto help verb choice in a natural lan-guage generation system which tries togenerate text similar to individual au-thor.
By using a distributional sim-ilarity (DS) measure on corpora col-lected from a recipe domain, we getthe most likely verbs for individual au-thors.
The accuracy of matching verbpairs produced by distributional similar-ity is higher than using the synonym out-puts of verbs from WordNet.
Further-more, the combination of the two meth-ods provides the best accuracy.1 IntroductionHuman text is characterised by the individual lex-ical choices of the specific author.
It varies fromauthor to author.
Individual authors use differentverbs to describe the same action.
Natural lan-guage generation (NLG) systems, in contrast, nor-mally produce uniform outputs without consider-ing other lexical possibilities.
Consider the fol-lowing example from our corpora that are the BBCcorpus and the Recipes for health eating corpus.1.
BBC Corpus: Finely grate the ginger andsqueeze out the juice into a shallow non-metallic dish.
(BBC online recipes)2.
Author2: Extract juice from orange and addthis with the water to the saucepan.
(Recipesfor health eating).Here, we can see that the two authors express thesame type of action with different verbs, ?squeeze?and ?extract?.
In fact, when expressing this action,the BBC corpus always use the verb ?squeeze?, andAuthor2 only uses the verb ?extract?.
Therefore,we can assume that Author2 considers the verb?extract?
to describe the same action as the verb?squeeze?
used by the BBC corpus.
The purpose ofour research is to develop a NLG system that candetect these kinds of individual writing features,such as the verb choice of individual authors, andcan then generate personalised text.The input of our personalised NLG system is anunseen recipe from the BBC food website.
Oursystem, then, translates all sentences into the styleof a personal author based on features drawn fromanalysing an individual corpus we collected.
Inthis paper, we address the verb choice of the indi-vidual author in the translation process.Our system defines the writing style of an in-dividual author by analysing an individual cor-pus.
Therefore, our system is a corpus-based NLGsystem.
Lexical choice for individual authors ispredicted by analysing the distributional similar-ity between words in a general large recipe cor-pus that is used to produce the verbs as the ac-tion representation and words in a specific indi-33vidual recipe corpus.
Firstly, we collected a largecorpus in the recipe domain from the BBC onlinewebsite.
This large recipe corpus is used to ex-tract feature values, for example verb choice, byanalysing an individual corpus.
Secondly, we col-lected our individual corpora for a number of in-dividual authors.
Each of them is used to extractfeature values that may define the individual writ-ing style.
The individual author may choose thesame or a different verb to describe cooking ac-tions.
The question is how can we identify the in-dividual choice?
For example, Author2 uses theverb ?extract?
instead of the verb ?squeeze?.
How-ever, if the author does express the action by a dif-ferent verb, the problem is how our system picksout verbs according to the individual choice of anauthor.One way to solve this problem is to accesslarge-scale manually constructed thesauri such asWordNet (Fellbaum, 1998), Roget?s (Roget, 1911)or the Macquarie (Bernard, 1990) to get al syn-onyms and choose the most frequent one in theindividual corpus.
Another possible way is to usea lexical knowledge based system, like VerbNet(Kipper et al, 2000) to get more possible lexicalchoices.
However, both methods only provide anumber of pre-produced lexical choices that mayor may not be the words that the individual authorwould choose.
In other words, the lexical choiceof an author may not be based on the synonymsextracted from one of the thesauri or may not evenbelong to the same semantic class.
In our exam-ple, ?squeeze?
and ?extract?
are neither synonymsnor Coordinate Terms in WordNet.
In a small do-main, it is possible to manually build a verb listso that each action is described by a set of possi-ble verbs.
The drawback is that this is expensive.Furthermore, it still cannot catch verbs that are notincluded in the list.
Is it possible to predict theindividual verbs automatically?The distributional hypothesis (Harris, 1968)says the following:The meaning of entities, and themeaning of grammatical relationsamong them, is related to the restrictionof combinations of these entities relativeto other entities.Over recent years, many applications (Lin,1998), (Lee, 1999), (Lee, 2001), (Weeds et al,2004), and (Weeds and Weir, 2006) have been in-vestigating the distributional similarity of words.Similarity means that words with similar meaningtend to appear in similar contexts.
In NLG, theconsideration of semantic similarity is usually pre-ferred to just distributional similarity.
However, inour case, the most important thing is to capture themost probable choice of a verb of an individual au-thor for expressing an action.
The expression ofan action can be either the same verb, synonyms,or Coordinate terms to the verb in the big corpus,or any verbs that an individual author chooses forthis action.
If we check an individual corpus, thereare a set of verbs in our list that do not occur.
Ifthese actions occur in the individual corpus, theindividual author must use different verbs.
Distri-butional similarity technology helps us to build thelinks between verbs in our list and the verbs in anindividual corpus.The rest of this paper is organised as follows.In Section 2, we describe the recipe domain, ourcorpora and our verb list.
Section 3 disscuss ourbaseline system.
In Section 4, we present the dis-tributional similarity measures that we are propos-ing for analysing our corpora.
The combinationmethod is disscussed in Section5.
In Section 6,we present an evaluation of our results.
In Section7, we draw conclusions and discuss future work.2 The Recipe Domain and our CorporaTo find the most expressive verb pairs, we have tohave corpora to be analysed.
Therefore, the se-lection of a corpus is very important.
As the re-search of authorship attribution (AA) shows (Bur-row, 1987), (Holmes and Forsyth, 1995), (Keueljet al, 2003), (Peng, 2003), and (Clement andSharp, 2003), there can be style variations of an in-dividual author.
This happens even with the sametopic and genre, and for the same action expres-sions.
Firstly, a person?s writing style can changeas time, genre, and topic change.
Can and Pat-ton (Can and Patton, 2004) have drawn the con-clusion:A higher time gap may have positiveimpact in separation and categorization.34Even within one text, the style may not be uni-form.
(Burrow, 1987) has pointed out that, for ex-ample, in fiction:The language of its dialogue and thatof its narrative usually differ from eachother in some obvious and many less ob-vious ways.These problems require us to collect high-qualitycorpora.
The recipe domain is a good start inthis case.
Sentences in it are narrative, impera-tive and objective, compared with other normalhuman text.
For example, journal articles nor-mally contain a large number of quotations, andthey are more subjective.
Furthermore, journal ar-ticles are more varied in content, even within thesame topic.
Secondly, most large corpora are notauthor-categorised.
This requires us to collect ourown individual corpora.2.1 Our CorporaAs we mentioned before, we collected a generalcorpus in the recipe domain from the BBC foodwebsite.
To make recipes varied enough, this cor-pus contains different cooking styles from west-ern to eastern, different courses, including starters,main courses and desserts, and a number of recipesof famous cooks, such as Ainsley Harriott.
Sincerecipes are widely-available both from the Internetand from publications, it is easy to collect author-categorised corpora.
Our individual recipe corporainclude four individual authors so far.
Two of themare from two published recipe books, and anothertwo we collected online.
Recipe books are useful,because they are written in an unique style.
Ta-ble 1 shows information about both our individualcorpora and our large general corpus.Although we are focusing on a small domain,verb variation between individual authors is acommon phenomenon.
Here are a few further ex-amples from our corpora, which we want to cap-ture.1.
BBC corpus: Preheat the oven to200C/400F/Gas 6.
(BBC online foodrecipes)2.
Author2: Switch on oven to 200C, 400F orGas Mark 6 and grease a 12 litre ovenproofserving dish.
(Recipes for Healthy Eating)Our Corpora Number Total Totalof Recipes Lines WordsLarge corpus 823 6325 85594(BBC online recipes)Recipes for 76 961 9212Health EatingFood for Health 113 1347 11791CM 48 537 6432(www.cooks.com)Jo Pratt 91 904 15417(www.bbc.co.uk)Table 1: Our corpora information3.
Author3.
Put the oven on.
(Food for Health)1.
BBC corpus: Sift the flour, baking powderand salt into a large bowl.
(BBC onlineRecipes)2.
Author2: Sieve the flour, baking powder andbicarbonate of soda into a large mixing bowl.
(Recipes for Health Eating)3.
Author3: Sieve the flour in, one-third at atime.
(Food for Health)2.2 Our Verb ListFigure 1: The information of the verblistWe manually built a verb list with 146 verbs intotal from our BBC corpus.
Each verb representsan unique cooking action, associated with defini-tions and synonyms extracted from WordNet.
Forexample, the verb ?squeeze?
contains the follow-ing information shown in Figure 1.
The BBC Cor-pus also contains a number of synonyms, such asthe verb sift and the verb sieve.
In this case, weonly pick up the most frequent verb, which is theverb sift in this case, as an cooking action, and we35record its synonyms, such as the verb sieve, in thelate part of our verb list.2.3 Using RASP in our corporaOur data consists of verb-object pairs for verbs ob-tained from our BBCCorpus using RASP (Briscoeand Carroll, 2002).
To derive reliable results, wedeal with our data by the following rules.
To avoidthe sparse data problem and parsing mistakes, weremoved a number of verbs that occur less than3 times in our large corpus, and a set of mistakeverbs made by the parser.
We consider both directobjects and indirect objects together at the sametime.3 The Baseline Method - WordNetSynonymsAfter the individual corpus is parsed, there are anumber of main verbs appearing only in the BBCrecipe corpus, but not in the individual corpus.This kind of main verbs is called missing verbin a corpus.
For example, verbs such as ?roast?,?insert?, ?drizzle?
appear in the BBC corpus, butnot in the Food for Health corpus.
We say theyare missing verbs in the Food for Health corpus.In this case, if the individual author expresses ac-tions in the missing verb group, other verbs mustbe chosen instead.
Our purpose is to find alter-natives used by the individual author.
To solvethis problem, our baseline measure is the WordNetsynonyms.
If the missing verb contains synonymsin the verb list, we pick one as the candidate verb,called an available candiate.
The following waysdecide the verb alternatives for a missing verb.
Ifthere is more than one candidate verb for one miss-ing verb, the most frequent synonym of the miss-ing verb in the individual corpus is chosen as thealternative.
The chosen synonym also has to be amain verb in the individual corpus.
If the miss-ing verb does not have a synonym or all availablecandidates do not appear in the individual corpus,we assign no alternative to this missing verb.
Inthis case, we say there is no available alternativefor the missing verb.
The number of available al-ternatives for the missing verb and the accuracy isshown in Table 2, and Figure 2.4 Distributional Similarity MeasureIn this section, we introduce the idea of using dis-tributional similarity measures, and discuss howthis methodology can help us to capture verbsfrom individual authors.By calculating the co-occurrence types of targetwords, distributional similarity defines the similar-ity between target word pairs.
The co-occurrencetypes of a target word (w) are the context, c, inwhich it occurs and these have associated frequen-cies which may be used to form probability esti-mates (Weeds et al, 2004).
In our case, the tar-get word is main verbs of sentences and the co-occurrence types are objects.
In section 6, simi-larity between verbs is derived from their objects,since normally there is no subject in the recipe do-main.
We are using the Additive t-test based Co-occurrence Retrieval Model of (Weeds and Weir,2006).
This method considers for each word wwhich co-occurrence types are retrieved.
In ourcase, objects have been extracted from both theBBC Corpus and an individual corpus.
Weeds andWeir use the the co-occurrence types as the fea-tures of word (w), F(w):F (w) = {c : D(w, c) > 0}where D(w, c) is the weight associated with wordw and co-occurrence type c. T-test is used as aweight function, which is listed later.Weeds and Weir use the following formula todescribe the set of True Positives of co-occurrencetypes, which w1 and w2 are considered main verbsin copora:TP (w1, w2) = F (w1) ?
F (w2)They use the t-test from (Manning and Schu?tze,1999) as the weight formula Dt(w, c):Dt(w, c) =p(c, w) ?
P (c)P (w)?P (c,w)NWeeds and Weir then calculate the precision byusing the proportion of features of w1 which oc-curs in both words, and the recall by using theproportion of features of w2 which occur in bothwords.
In our experiment, precision is relative to36Total Available Available Available CorrectIndividual Corpora Numbers Candidates Candidates Candidates Alternatives byof Missing by by by (DS VS. WordNetVerbs WordNet DS Combination VS.
Combination)Recipes for Health Eating 56 A = 36 A = 47 A = 52 8 VS. 10 VS. 17Food for Health 57 A = 34 A = 52 A = 54 12 VS. 18 VS. 27CM (www.cooks.com) 58 A = 25 A = 44 A = 51 10 VS. 4 VS. 14Jo Pratt (www.bbc.co.uk) 26 A = 13 A = 22 A = 24 4 VS. 5 VS. 8Table 2: The number of available missing verbs by the Distributional Similarity (DS) and by WordNetand by combination of DS and WordNet.
(?A?
means the total number of missing verbs in the individualcorpus that have candidate alternatives in an individual corpus from methods.
)the BBC Corpus, and the recall is relative to anindividual corpus.P add(w1, w2) =?TP (w1,w2) D(w1, c)?F (w1) D(w1, c)Radd(w1, w2) =?TP (w1,w2) D(w2, c)?F (w2) D(w2,c)Finally, Weeds and Weir combine precision andrecall together by the following formulas:mh(P (w1, w2), R(w1, w2)) =2.P (w1, w2).R(w1, w2)P (w1, w2) + R(w1, w2)ma(P (w1, w2), R(w1, w2)) =?.P (w1, w2) + (1 ?
?
).R(w1, w2)sim(w1, w2) = r.mh(P (w1, w2), R(w1, w2))+(1 ?
r).ma(P (w1, w2), R(w1, w2))where both r, ?
are between [0, 1].
In our ex-periments, we only assigned r=1.
However, fur-ther performs can be done by assigning differentvalues to r and ?.4.1 The Distributional Similarity methodEach missing verb in the BBC corpus is assignedthe most likely verb as the available candidatefrom the individual corpus.
The most likely verbis always chosen according to the largest similarityusing the DS measure.
In our case, if the largestsimilarity of the verb pair is larger than a certainvalue (-5.0), we say the missing verb has an avail-able candidate.
Otherwise, there is no availablecandidate existing in the individual corpus.
Forinstance, DS suggests verb ?switch?
is the mostlikely-exchangable verb for missing verb ?preheat?in the Recipes for Health Eating corpus.
?switch?appears 33 times in the individual corpus, in whichthere are 33 times that ?switch?
has the same ob-ject as ?preheat?.
Meanwhile, ?preheat?
shows 191times in total in the BBC corpus, with the sameobjects as ?switch?
176 times.
By using the DS for-mulas, the similarity value between ?preheat?
and?switch?
is 11.99.
The number of available can-didates of the missing verbs and the accuracy areshown in Table 2, and Figure 2.There is only one corpus in the DS measures.In our case, w1 and w2 are from different corpora.For example, verb ?preheat?
is from the BBC cor-pus, and verb ?switch?
is in the Recipes for HealthEating.
Although the co-occurence type is objectsof the main verb, the precision is for the generalcorpus ?
?the BBC corpus, and the recall is forthe individual corpus in our experiments.5 The Combination methodWe also combine the baseline and the DS methodtogether in the combination method.
The combi-nation method tries the baseline first.
For eachmissing verb, if the baseline returns an availablealternative, this is the final available alternative ofthe combination method.
If not, the available al-ternative is calculated by the DS method.
If thereis still no candidate for the missing verb, there is37no available alternative in this case.6 EvaluationTo justify accuracy of results by both the baselinemethod and the DS method, we manually judgewhether or not the alternatives are inter change-able for the missing verbs.
Table 2 shows the totalnumber of missing verbs for each individual cor-pus and numbers of available alternatives as well.Also, it presents the number of correct alternativesfor cases where both methods return answers, andresults of a combination of two methods.
In thefuture, we would like to evaluate the accuracy bymore judges.From Table 2, accuracies of distributional simi-larity are higher than WordNet synonyms in mostcases, except in the individual corpus CM.
Thereason that CM got worse results is probably thatthe corpus size is not big enough.
Since CM isthe only individual corpus that has less than 50recipes, this could lead to unreliable accuracy.
Intable 2, ?A?
means the total number of missingverbs in the individual corpus that have candidatealternatives in an individual corpus from meth-ods.
It is obvious that distributional methods pro-duce more available verbs than the synonyms ofWordNet.
In this case, we assume that WordNetis not very productive to provide alternative verbchoices for individual authors compared with dis-tributional similarity in a domain.Figure 2 represents the accuracies of all meth-ods.
In Figure 2, we can see the overall accuracyof WordNet is not as good as the distributionalsimilarity method.
Moreover, we calculate the ac-curacy for the available verb pairs from the com-bination method of both the distributional similar-ity and WordNet.
We can see that all combina-tion accuracies are significantly better than accu-racies of either distributional similarity or Word-Net synonyms.
In this case, distributional similar-ity and WordNet find different types of verbs.
Inother words, the similarity distributional methodis very useful to find verbs that are not synonymsbut represent the same type of action in individualcorpora.
And the type of verbs found by distribu-tional similarity could not be pre-predicted, whichmakes the verb choice personalised.In our verb pair outputs from distributional sim-ilarity, one problem is that we got similar verbpairs, for instances the verb ?simmer?
matches to?fry?.
This is a common problem with distribu-tional similarity, since it is not based on semanticmeaning.
This problem can perhaps be solved bybuilding some hierarchical relationships betweenverbs.
For instance, roast is one type of cooking.The following examples are correct cases ofverb pairs that are captured by distributional simi-larity.
In each example, the semantic meanings ofsentences are different, but the representation ofaction are the same.roast (BBC Corpus) - cook (Food for Health):1.
BBC Corpus: Season generously and roastfor 30 minutes until softened and a littlecharred.
(BBC online recipes)2.
Author2: Cover with a lid or foil and cookin the centre of the oven for 20 minutes, thenturn down the oven to Reg 3 or 160C and con-tinue cooking for 1 hour or until the kidneysare cooked.
(Food for Health)saute (BBC Corpus) - fry (Food for Health):1.
BBC Corpus: Melt the butter in a small tomedium ovenproof pan and saute the cashewnuts for 2-3 minutes.
(BBC online recipes)2.
Author2: Add the carrots and fry quickly for5 minutes, stirring continuously.
(Food forHealth)preheat (BBC Corpus) - switch on (Food forHealth):1.
BBC Corpus: Preheat the oven to200C/400F/Gas 6.
(BBC online recipes)2.
Author2: Switch on oven to 190C, 375F orGas Mark 5.
(Food for Health)So far distributional similarity cannot capturethe prepositions such as on in the third example.This is our future work.7 ConclusionIn this paper, we used a distributional similar-ity method to help us to find matching verbs in38Figure 2: The Accuracy for Missing Verbs in Individual Corporaan individual corpus.
We have compared the re-sult between the distributional similarity methodand WordNet and the overall accuracy of distribu-tional similarity is better than WordNet.
Further-more, the combination of the distributional simi-larity method and WordNet achieved the best ac-curacy.
This suggests that distributional similar-ity is very helpful in choosing the proper verbsfor individual authors.
It is especially useful tofind verbs that are not synonyms but represent thesame type of action in individual corpora.
Thismeans distributional similarity can capture unpre-dicted verb preferences of individual authors fromthe individual corpora.ReferencesJohn R.L Bernard.
1990.
The Macquarie Encyclo-pedic Thesaurus.
The Macquarie Library, Sydney,Australia.Edward Briscoe and John Carroll.
2002.
Robust Accu-rate Statistical Annotation of General Text.
In Pro-ceedings of the LREC-2002, pages 1499?1504.John F. Burrow.
1987.
Word-patterns and Story-shapes: the Statistical Analusis of Narrative style.Literary and Linguistic Computing, 2(2):61?70.Fazli Can and Jon M. Patton.
2004.
Change of Writ-ing Style with Time.
Computers and Humanities,38:61?82.R.
Clement and D. Sharp.
2003.
Ngram andBayesian Classification of Documents for Topic andAuthorship.
Literary and Linguistic Computing,18(4):423?447.Christiance Fellbaum, editor.
1998.
WordNet: AnElectronic lexical Database.
MIT Press.Zelig S. Harris.
1968.
Mathematical Structures of Lan-guage.
John Wiley.D.
I Holmes and R.S Forsyth.
1995.
The Federalist Re-visited: New Directions in Authoriship attribution.Literary and Linguistic Computing, 10(2):111?127.V.
Keuelj, F. C. Peng, N. Cercone, and C. Thomas.2003.
N-Gram-Based Author Profiles for Author-ship Attribution.
In Proceedings the Pacific Associ-ation for Computational Linguistics.Karin Kipper, Hoa Trang Dang, and Martha Palmer.2000.
Class-Based Construction of a Verb Lexicon.In Proceedings the AAAI/IAAI, pages 691?696.Lillian Lee.
1999.
Measure of Distributional Similar-ity.
In Proceedings of the Association for Computa-tional Linguistics (ACL).Lillian Lee.
2001.
On the Effectiveness of the SkewDivergence for Statistical Language.
In AIR.Dekang Lin.
1998.
Automatic Retrieval and Clus-tering of Similar Words.
In Proceedings of theCOLING-ACL.Christopher D. Manning and Hinrich Schu?tze.
1999.Foundations of Statistical Natural Language Pro-cessing.
The MIT Press.39F.
C. Peng.
2003.
Language Independent AuthorshipAttribution using Character Level Language Mod-els.
In Proceedings of the European Association forComputational Linguistics (ACL).Peter Roget.
1911.
Thesaurus of English Words andPhrases.
Longmans.Julie Weeds and David Weir.
2006.
Co-occurrenceRetrieval: A Flexible Framework for lexical Dis-tributional Similarity.
Computational Linguistics,31(4):440?475.Julie Weeds, David Weir, and Diana McCarthy.
2004.Charactersing Measures of Lexical DistributionalSimilarity.
In Proceedings of the COLING.40
