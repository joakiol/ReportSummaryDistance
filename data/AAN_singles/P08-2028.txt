Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 109?112,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsThe Good, the Bad, and the Unknown:Morphosyllabic Sentiment Tagging of Unseen WordsKaro Moilanen and Stephen PulmanOxford University Computing LaboratoryWolfson Building, Parks Road, Oxford, OX1 3QD, England{ Karo.Moilanen | Stephen.Pulman }@comlab.ox.ac.ukAbstractThe omnipresence of unknown words is aproblem that any NLP component needs to ad-dress in some form.
While there exist manyestablished techniques for dealing with un-known words in the realm of POS-tagging, forexample, guessing unknown words?
semanticproperties is a less-explored area with greaterchallenges.
In this paper, we study the seman-tic field of sentiment and propose five methodsfor assigning prior sentiment polarities to un-known words based on known sentiment carri-ers.
Tested on 2000 cases, the methods mirrorhuman judgements closely in three- and two-way polarity classification tasks, and reach ac-curacies above 63% and 81%, respectively.1 IntroductionOne of the first challenges in sentiment analysisis the vast lexical diversity of subjective language.Gaps in lexical coverage will be a problem for anysentiment classification algorithm that does not havesome way of intelligently guessing the polarity ofunknown words.
The problem is exacerbated furtherby misspellings of known words and POS-taggingerrors which are often difficult to distinguish fromgenuinely unknown words.
This study explores theextent to which it is possible to categorise wordswhich present themselves as unknown, but whichmay contain known components using morpholog-ical, syllabic, and shallow parsing devices.2 Morphosyllabic ModellingOur core sentiment lexicon contains 41109 entriestagged with positive (+), neutral (N), or nega-tive (-) prior polarities (e.g.
lovely(+), vast(N),murder(-)) across all word classes.
Polarity rever-sal lexemes are tagged as [?]
(e.g.
never(N)[?]).
Wefurthermore maintain an auxiliary lexicon of 314967known neutral words such as names of people, or-ganisations, and geographical locations.Each unknown word is run through a series ofsentiment indicator tests that aim at identifying in itat least one possible sentiment stem - the longestsubpart of the word with a known (+), (N), or(-) prior polarity.
An unknown word such ashealthcare-related(?)
can be traced back to the stemshealth(N)(+), care(+), healthcare(+), or relate(d)(N)which are all more likely to be found in the lexica,for example.
Note that the term ?stem?
here does nothave its usual linguistic meaning but rather means?known labelled form?, whether complex or not.We employ a classifier society of five rule-drivenclassifiers that require no training data.
Each classi-fier adopts a specific analytical strategy within a spe-cific window inside the unknown word, and outputsthree separate polarity scores based on the numberof stems founds (Spos, Sntr, Sneg) (initially 1).
Thescore for polarity p for unknown word w is calcu-lated as follows:(1) scr(p) = ?pLsLw1SwSpSpos + Sntr + Snegwhere ?p = polarity coefficient (default 1)Ls = # of characters in the stemLw = # of characters in wSw = # of punctuation splits in wPolarity coefficients balance the stem counts: in par-ticular, (N) polarity is suppressed by a ?ntr of < 1109because (N) stem counts dominate in the vast major-ity of cases.
Ls reflects differing degrees of reliabil-ity between short and long stems in order to favourthe latter.
Sw targets the increased ambiguity poten-tial in longer punctuated constructs.
The highest-scoring polarity across the three polarity scores fromeach of the five classifiers is assigned to w.Conversion [A].
It is generally beneficial to im-pose word class polarity constraints in the lexicon(e.g.
[smart](+) ADJ vs. [smart](-) V).
Due to cre-ative lexical conversion across word classes, hardconstraints can however become counterproductive.The first classifier estimates zero-derived paronymsby retagging the unknown word with different POStags and requerying the lexica.Morphological Derivation [B].
The second clas-sifier relies on regular derivational (e.g.
-ism, -ify,-esque) and inflectional (e.g.
-est, -s) morphology.The unknown word is transformed incrementallyinto shorter paronymic aliases using pure affixes and(pseudo and neo-classical) combining forms.
A re-cursive derivation table of find/replace pairs is usedto model individual affixes and their regular spellingalternations (e.g.
-ppingp; -atione; -inessy;-some?
; re-?).
Polarity reversal affixes suchas -less(N)[?]
and not-so-(N)[?]
are supported.
Thetable is traversed until a non-neutral sentiment(NB.
not morphological) stem is found.
Prefixesare matched first.
Note that the prefix-drivenconfiguration we have adopted is an approximationto a (theoretically) full morphemic parse.
Thederivation for antirationalistic(?
), for example, firstmatches the prefix anti-(N)[?
], and then truncates theimmediate constituent rationalistic(?)
incrementallyuntil a sentiment stem (e.g.
rational(N)(+)) isencountered.
The polarity reversal prefix anti-(N)[?
]then reverses the polarity of the stem: hence,antirationalistic(?)rationalistic(?)rationalist(?)rational(+)antirationalistic(-).
322 (N) and 67[?]
prefixes, and 174 (N) and 28 [?]
suffixeswere used.Affix-like Polarity Markers [C].
Beyondthe realm of pure morphemes, many non-neutral sentiment markers exist.
Examplesinclude prefix-like elements in well-built(+),badly-behaving(-), and strange-looking(-); andsuffix-like ones in rat-infested(-), burglarproof(+),and fruit-loving(+).
Because the polarity of anon-neutral marker commonly dominates over itshost, the marker propagates its sentiment across theentire word.
Hence, a full-blown derivation is notrequired (e.g.
easy-to-install(?)easy-to-install(+);necrophobia(?)necrophobia(-)).
We experimentedwith 756 productive prefixes and 640 suffixesderived from hyphenated tokens with a frequencyof ?
20 amongst 406253 words mined fromthe WAC 2006 corpus1.
Sentiment markers arecaptured through simple regular expression-basedlongest-first matching.Syllables [D].
We next split unknown words intoindividual syllables based on syllabic onset, nucleus,and coda boundaries obtained from our own rule-based syllable chunker.
Starting with the longest,the resultant monosyllabic and permutative order-preserving polysyllabic words are used as aliasesto search the lexica.
Aliases not found in our lex-ica are treated as (N).
Consider the unknown wordfreedomfortibet(?).
In the syllabified set of singularsyllables {free, dom, for, ti, bet} and combinatorypermutations such as {free.dom, dom.ti, for.ti.bet,.
.
.
}, free or free.dom are identified as (+) while allothers become (N).
Depending on the ?ntr value,free.dom.for.ti.bet(?)
can then be tagged as (+) dueto the (+) stem(s).
Note that cruder substring-basedmethods can always be used instead.
However, a syl-labic approach shrinks the search space and ensuresthe phonotactic well-formedness of the aliases.Shallow Parsing [E].
At a deepest level, weapproximate the internal quasi-syntactic structureof unknown words that can be split based onvarious punctuation characters.
Both exotic phrasalnonce forms (e.g.
kill-the-monster-if-it?s-green-and-ugly(-)) and simpler punctuated compounds(e.g.
butt-ugly(-), girl-friend(+)) follow observablesyntactic hierarchies amongst their subconstituents.Similar rankings can be postulated for sentiment.Since not all constituents are of equal importance,the sentiment salience of each subconstituentis estimated using a subset of the grammaticalpolarity rankings and compositional processesproposed in Moilanen and Pulman (2007).
Theunknown word is split into a virtual sentenceand POS-tagged2.
The rightmost subconstituent1Fletcher, W. H. (2007).
English Web Corpus 2006. www.webascorpus.org/searchwc.html2Connexor Machinese Syntax 3.8. www.connexor.com110Table 1: Average (A)ccuracy, kappa, and error distribution against ANN-2 and ANN-3ALL POL NON-NTR ?LAZY ERROR DISTRIBUTIONClassifier ?ntr A k A k A FATAL GREEDY LAZY[A] CONVERSION .2 76.70 .03 96.88 .94 99.53 0.08 2.47 97.44[B] DERIVATION .8 74.15 .11 80.05 .59 93.90 2.81 22.86 74.33[C] AFFIX MARKERS .2 72.33 .21 77.93 .55 88.05 6.10 39.07 54.83[D] SYLLABLES .8 69.55 .23 71.88 .45 82.75 9.37 48.62 42.01[E] PARSING .7 64.33 .25 79.09 .59 73.50 9.03 65.40 25.57ALL 63.20 .28 80.61 .61 70.20 9.49 71.41 19.10ALL ?UNSURE 64.60 .28 82.19 .64 69.71 7.43 77.95 14.62in the word is expanded incrementally leftwardsby combining it with its left neighbour until thewhole word has been analysed.
At each step, thesentiment grammar in idem.
controls (i) non-neutralsentiment propagation and (ii) polarity conflictresolution to calculate a global polarity for thecurrent composite construct.
The unknown wordhelp-children-in-distress(?)
follows the sequenceN:[distress(-)](-)PP:[in(N)distress(-)](-)NP:[child-ren(N)[in distress](-)](-)VP:[help(+)[children indistress](-)](+), and is thus tagged as (+).3 EvaluationWe compiled a dataset of 2000 infrequent wordscontaining hapax legomena from the BNC3 and?junk?
entries from the WAC 2006 corpus (Foot-note 1).
The dataset contains simple, medium-complexity, and extreme complex cases cover-ing single words, (non-)hyphenated compounds,nonce forms, and spelling anomalies (e.g.
anti-neo-nazi-initiatives, funny-because-its-true, ands?gonnacostyaguvna).
Three human annotators clas-sified the entries as (+), (-), or (N) (with an op-tional UNSURE tag) with the following distribution:(2)Human (+) (N) (-) UNSUREANN-1 24.55 53.45 22 11.75ANN-2 12.60 68.60 18.80 10.85ANN-3 5.25 84.55 10.20 0.65We report results using all polarities (ALL-POL)and non-neutral polarities (NON-NTR) resulting inaverage pairwise inter-annotator Kappa scores of3Kilgarriff, A.
(1995).
BNC database and word frequencylists.
www.kilgarriff.co.uk/bnc-readme.html.40 (ALL-POL) and .74 (NON-NTR), or .48 (ALL-POL) and .83 (NON-NTR) without UNSURE cases.We used ANN-1?s data to adjust the ?ntr coefficientsof individual classifiers, and evaluated the systemagainst both ANN-2 and ANN-3.
The average scoresbetween ANN-2 and ANN-3 are given in Table 1.Since even human polarity judgements becomefuzzier near the neutral/non-neutral boundary dueto differing personal degrees of sensitivity towardsneutrality (cf.
low (N) agreement in Ex.
2; An-dreevskaia and Bergler (2006)), not all classificationerrors are equal for classifying a (+) case as (N)is more tolerable than classifying it as (-), for ex-ample.
We therefore found it useful to characterisethree distinct disagreement classes between humanH and machine M encompassing FATAL (H(+)M(-)or H(-)M(+)), GREEDY (H(N)M(-) or H(N)M(+)), andLAZY (H(+)M(N) or H(-)M(N)) cases.The classifiers generally mimic human judge-ments in that accuracy is much lower in the three-way classification task - a pattern concurring withpast observations (cf.
Esuli and Sebastiani (2006);Andreevskaia and Bergler (2006)).
Crucially, FA-TAL errors remain below 10% throughout.
Furtheradvances can be made by fine-tuning the ?ntr coef-ficients, and by learning weights for individual clas-sifiers which can currently mask each other and sup-press the correct analysis when run collectively.4 Related WorkPast research in Sentiment Tagging (cf.
OpinionMining, Sentiment Extraction) has targeted classifi-cation along the subjectivity, sentiment polarity, andstrength/degree dimensions towards a common goal111of (semi-)automatic compilation of sentiment lexica.The utility of word-internal sentiment clues has notyet been explored in the area, to our knowledge.Lexicographic Methods.
Static dictionary-/thesaurus-based methods rely on the lexical-semantic knowledge and glosses in existing lexi-cographic resources alongside known non-neutralseed words.
The semi-supervised learning methodin Esuli and Sebastiani (2005) involves constructinga training set of non-neutral words using WordNetsynsets, glosses and examples by iteratively addingsyn- and antonyms to it and learning a term classifieron the glosses of the terms in the training set.
Esuliand Sebastiani (2006) used the method to cover ob-jective (N) cases.
Kamps et al (2004) developed agraph-theoretic model of WordNet?s synonymy rela-tions to determine the polarity of adjectives based ontheir distance to words indicative of subjective eval-uation, potency, and activity dimensions.
Takamuraet al (2005) apply to words?
polarities a physicalspin model inspired by the behaviour of electronswith a (+) or (-) direction, and an iterative term-neighbourhood matrix which models magnetisation.Non-neutral adjectives were extracted from Word-Net and assigned fuzzy sentiment category member-ship/centrality scores and tags in Andreevskaia andBergler (2006).Corpus-based Methods.
Lexicographic methodsare necessarily confined within the underlying re-sources.
Much greater coverage can be had withsyntactic or co-occurrence patterns across large cor-pora.
Hatzivassiloglou and McKeown (1997) clus-tered adjectives into (+) and (-) sets based on con-junction constructions, weighted similarity graphs,minimum-cuts, supervised learning, and clustering.A popular, more general unsupervised method wasintroduced in Turney and Littman (2003) which in-duces the polarity of a word from its Pointwise Mu-tual Information (PMI) or Latent Semantic Analy-sis (LSA) scores obtained from a web search en-gine against a few paradigmatic (+) and (-) seeds.Kaji and Kitsuregawa (2007) describe a methodfor harvesting sentiment words from non-neutralsentences extracted from Japanese web documentsbased on structural layout clues.
Strong adjecti-val subjectivity clues were mined in Wiebe (2000)with a distributional similarity-based word clus-tering method seeded by hand-labelled annotation.Riloff et al (2003) mined subjective nouns fromunannotated texts with two bootstrapping algorithmsthat exploit lexico-syntactic extraction patterns andmanually-selected subjective seeds.5 ConclusionIn this study of unknown words in the domainof sentiment analysis, we presented five methodsfor guessing the prior polarities of unknown wordsbased on known sentiment carriers.
The evaluationresults, which mirror human sentiment judgements,indicate that the methods can account for many un-known words, and that over- and insensitivity to-wards neutral polarity is the main source of errors.ReferencesAlina Andreevskaia and Sabine Bergler.
2006.
MiningWordNet for Fuzzy Sentiment: Sentiment Tag Extrac-tion from WordNet Glosses.
In Proceedings of EACL2006.Andrea Esuli and Fabrizio Sebastiani.
2005.
Determin-ing the Semantic Orientation of Terms through GlossClassification.
In Proceedings of CIKM 2005.Andrea Esuli and Fabrizio Sebastiani.
2006.
Determin-ing Term Subjectivity and Term Orientation for Opin-ion Mining.
In Proceedings of EACL 2006.Vasileios Hatzivassiloglou and Kathleen R. McKeown.1997.
Predicting the Semantic Orientation of Adjec-tives.
In Proceedings of ACL 1997.Jaap Kamps, Maarten Marx, Robert J. Mokken andMaarten de Rijke 2004.
Using WordNet to MeasureSemantic Orientations of Adjectives.
In Proceedingsof LREC 2004.Nabuhiro Kaji and Masaru Kitsuregawa.
2007.
Build-ing Lexicon for Sentiment Analysis from MassiveCollection of HTML Documents.
In Proceedings ofEMNLP-CoNLL 2007.Karo Moilanen and Stephen Pulman.
2007.
SentimentComposition.
In Proceedings of RANLP 2007.Ellen Riloff, Janyce Wiebe and Theresa Wilson.
2003.Learning Subjective Nouns using Extraction PatternBootstrapping.
In Proceedings of CoNLL 2003.Hiroya Takamura, Takashi Inui and Manabu Okumura.2005.
Extracting semantic orientations of words usingspin model.
In Proceedings of ACL 2005.Peter Turney and Michael Littman.
2003.
MeasuringPraise and Criticism: Inference of Semantic Orienta-tion from Association.
ACM Transactions on Infor-mation Systems, October, 21(4): 315?46.Janyce Wiebe.
2000.
Learning Subjective Adjectivesfrom Corpora.
In Proceedings of AAAI 2000.112
