A New Predictive Analyzer of EnglishHiroyuki MushaDepartment of Information ScienceTokyo Institute of TechnologyOhokayama, Meguro-ku, Tokyo 152, JAPANABSTRACTAspects of syntactic predictions made during the recognitionof English sentences are investigated.
We reinforce Kuno's origi-nal predictive analyzer\[i\] by introducing five types of predictions.For each type of prediction, we discuss and present its necessity, itsdescription method, and recognition mechanism.
We make use ofthree kinds of stacks whose behavior is specified by grammar rulesin an extended version of Greibach normal form.
We also investi-gate other factors that affect the predictive recognition process,i.e., preferences among syntactic ambiguities and necessary amountof lookahead.
These factors as well as the proposed handlingmechanisms of predictions are tested by analyzing two kinds ofarticles.
In our experiment, more than seventy percent of sen-tences are recognized and looking two words ahead seems to be thecritical length for the predictive recognition.1.
IntroductionWhen human reads normal sentences, we rarely feel some-thing is wrong with the structure we are constructing and are sel-dom compelled to backtrack for reconstructing an alternative.
Ifwe could simulate the internal mechanism that makes it possible toselect deterministically the unique syntactic structure in a simpleway, we may be able to construct more natural and efficientlanguage processing systems.
In this paper, we focus our attentionon syntax of natural languages, particularly English, and predic-tions or expectations that can be made solely with syntactic infor-mation during the sentence recognition process are analyzed indetail.
It includes machine executable mechanisms that enableproper handling of analyzed aspects and a description method ofthe mechanisms a  grammar rules.
The recognition method can beseen as a deterministic one \[2\] if we permit looking some wordsahead.
Also included in this paper are results of an experimentalanalysis in which more than seventy percent of sentences are recog-nized.An analyzer which gives special attention to predictions wasonce developed by Kuno \[1\].
The analyzer makes use of the sim-ple stack mechanism whose behavior is specified by rules describedin Greibach normal form.
In the method, however, we can findseveral kinds of rules that do not correspond to human predictiverecognition process, which will be pointed out in this paper.The following discussion is based mainly on the author's (sub-jective) retrospect of the recognition process of English sentences.The author's mother tongue is Japanese and he has been learningEnglish as a second language.
It seems to the author that he canunderstand better how he recognizes English than how he recog-nizes Japanese since he has been learning English consciously andcan observe rather objectively the process of recognition.The rest of this paper is organized as follows.
In the nextsection, we discuss aspects of predictions, laying stress upon theirproper handling by computers.
The following section presents theresults of an experiment.
The conclusions are presented in the lastsection.2.
Aspects of PredictionsWhile reading or hearing English, we constantly predict orexpect what may follow next.
Such predictions can be classifiedinto six types which we will describe below.2.1.
Essential PredictionsThe simplest ype of prediction, which forms the basis of thefollowing discussions, is presented in this subsection.
The charac-teristic of this type of prediction is that it is essential in forming anacceptable sentence structure.Phrase structure grammar ules, especially those in Greibachnormal form, can naturally describe this kind of prediction: we canconsider the terminal symbol (or the lexical category) on the right-hand side of a rule as the current word and the nonterminal sym-bols that follow the terminal symbol as new predictions \[3\].
Forexample, the following rule describes what we predict when weencounter a transitive verb at the beginning of a verb phrase.VP - vt NPNote that the new prediction, NP, is essential to form a verbphrase.
By adopting this kind of rules as a means of structuraldescription of sentences, we can easily capture the structures byusing the stack mechanism \[1\].In the following subsections, except for the last subsection,these rules and the mechanism are gradually reinforced in order tohandle a newly introduced prediction type.
The extended mechan-ism provides us with a simpler (yet still powerful) means for recog-nition of sentence structures than, for example, ATN framework\[4\].
Other factors that affect the predictive recognition process arediscussed in the last subsection.2.2.
Optional PredictionsWe now extend our recognition mechanism by introducingoptional predictions.
This type of prediction is needed to handlepostpositional modifiers that are not essential to form a sentence.In the previous ubsection, we saw that rules in Greibach nor-mal form are suitable for expressing our predictive recognition pro-cess, but any rule should not predict too much.
Consider the fol-lowing rule that explains a possible structure of noun phrases.NP ~ article NP-ART ADJ_CLAUSEConcerning the correspondence with human language understand-ing process, however, the rule cannot be considered a good simula-tion of our understanding process: we predict a postpositionalmodifier, like an adjectival clause, not at the beginning of a nounphrase but at the beginning of the modifier.
For our purpose,therefore, we must exclude this kind of rule that do not express ourpredictions properly.Optional predictions are used to capture these structures.Here, we also extend the rule description to keep the correspon-dence between the grammar rules and the recognizing mechanism:we introduce the shifting flag.
The following rules are used to cap-ture postpositional modifiers.CW CP SF NPr(1) art NP ~ t NP-ART(2) noun NP-ART ~ t *NP-N(3) rel_pro NP-N ~ nil ADJ_CLAUSEThe first rule, for example, can he interpreted as follows: IF thecurrent word (CW) is an article and the current prediction (CP: thetop element of the stack) is NP, THEN shift the current wordpointer (since the shifting flag (SF) is t) and replace the currentprediction by the new prediction (NPr).The shifting flag enables us to proceed two or more statechanges while looking at a single word.
By using these notations470and the rules we can specify the state changes of the stack as shownin Figure 2-1.
The prediction NP-N, with a prefix '*' which showsit is optional, is interpreted as the state in wlfich a noun essential toform a noun phrase has already appeared and it may end there.
Itwill be popped out from the stack or will be replaced by a newprediction according to the word that follows.
*NP-N ~ PE~RIODJ / LP5 R~?-%LI I INI  Np \[ .
-~ NP-ART period period\ I art *NP-N / -~  ADJ.
CL~ d  L.~ RI?D/rel pro rel proFigure ~.-1.
Handling of the optional prediction.2.3.
Bunch PredictionsWe extend our model by introducing bunch predictions whichenable us to predict a set of syntactic ategories simultaneously.
Inthe following subsection, we see that this kind of prediction is use-ful for handling coordinate conjunctions, too.Various kinds of syntactic units can follow the verb be in averb phrase and we cam~ot selectively predict one of these possibil-ities when we are reading the words, such as am or were, etc.
Thebunch predictions we introduce nable us to cope with this kind ofpredictions.The following rule shows how to write a bunch prediction ill arule.
(be flit) (VP fat) ~ t \[bunch (NP) (ADJ-) ((VP ing))\] *VP_MODWhen a bnnch prediction is pushed onto tile stack, it works as if itwere a single prediction until it becomes the top of tile stack, andone of the constitnent of the bunch prediction is, then, chosen to beappropriate according to the word encountered.2.4.
And StackIn this subsection, we introduce another stack called the andstack to handle coordinate conjunctions.
The method describedhere resembles that in \[5\] or \[6\], but with the and stack we can han-dle them quite simply.The appearance of coordinate conjunctions are usually netpredictable and it triggers a new kind of operation.
Let us considerthe following sentences.
(1) Mary had a little lamb and a kitten.
(2) Mary had a little lamb and washed him every day.
(3) Mary had a little lamb and she was always with him.Conventional phrase structm'e grammar rules like:S ~ S and Sare not directly useful for predictive recognition of the sentences.The structure that follows and depends not on the word itself buton the proceeding syntactic units being constructed.
In the abovesentences, a noun phrase, a verb phrase, and a clause are beingconstructed before the word, and each of these categories reap-pears in each of the three sentences, respectively.By using the and stack, we can easily recognize these struc-tures.
Figure 2-2 shows the relationship between the predictionstack (the stack that holds predictions) and the and stack whereunnecessary details are omitted.
At stage (ii), the first predictionis replaced by two predictions NP and VP only by looking the firstword Mary.
The lower element of the and stack is dmnged to (VPS), which shows that while the VP of the prediction stack is beingprocessed, we are constructing both VP and S. In the same way,the stacks change their states as shown in the figure and a list (NPVP S) is made and pushed on the and stack when we reach theword and.
The only thing we have to do is that we make a bunchpl'ediction \[bunch (NP) (VP) (S)\] and replace *NP-N by the bunchprediction.
By looking at the words that follow we can choose oneof the constituent predictions of the bunch prediction and processthe rest of the sentence.Note that the following sentence can also be i'eeognized bythis strategy:Mary lookedJbr and Jound the unicorn.A list (NP VP S) has been built when we encounter tile conjunc-tion, and VP is used to capture the structure of the rest of the sen-tence.The following rule description is used to trigger the aboveexplained operation:and ?P -, t (special and_stack)where ?P indicates that applicability of the rule does net depend onthe current prediction.Prediction StackMary Mmy had a little andlambAnd Stack(i) (ii) (ill) (iv) (v)Figure 2-2.
Relation between and stack and prediction stack.2.5.
Insertive StructuresSome kinds of words trigger insertive sm~ctures which areusually not predicted, and cause a kind of suspension of construc-tion of structures being built.
Some adverbs, prepositional phrasesand adverbial phrases and clauses are such structures.
\]{ere arethree examples, where we use a pair of quotes to distinguish inser-tire structures.
(1) There are economic risks and "generally" a lack of available data.
(2) He adapted "for linguists" an existing system of formalization.In order to express insertive sta'uctures, we use the followingnotation.
(A-l) adverb ?P ~ t(A-2) pt'eposition ?P - nil PPThese rules are applicable for ahnost all old predictions providedthat the current word belongs to the CW part of the rules.
In thiscase, however, the top element of the prediction stack will not bepopped.
The new prediction(s), if they exist, will be pushed ontothe current prediction.For example, (2) will be processed as shown in Figure 2-3.At first, the object noun phrase, NP, of the verb "adapted" ispredicted.
The rule (A-2) is then applied and the recognition ofNP is suspended until the prepositional phrase is recognized by theprediction PP.He for .for the theadapted linguists existing...Figure 2-3.
I-Iandling of insertive structures.4712.6.
NP stackWe introduce yet another stack, the NP stack, to handle struc.tures where a noun phrase is missing, e.g.
relative clauses, Thisapproach is widely used, e.g.
in \[7\].
The fact is that people do nothandle these structures in a totally different way comparing withnormal clauses.
It seems that when we encounter a relative pro.noun, we push a noun phrase onto a kind of stack, which we callthe NP stack, and pop it when it is needed to fill out the gap after-wards.The following rule is used to simulate the above operations.rel pro ADJ CLAUSE ~ nil +SThe prefix '+ '  of the new prediction indicates that we push a nounphrase onto the NP stack.2.7.
Looking ahead and PreferenceIn this subsection we discuss necessity of looking ahead andpreference among syntactic ambiguities that affect the predictiverecognition process.Some sort of lookahead facility is necessary to reflect thedelay in making syntactic structure of sentences.
In sharp contrastwith Marcus's deterministic parser \[2\], we only make use of a wordas the unit of lookahead.In the middle of a sentence we usually do not look back to seewhat the preceding structure was in order to build up a dominatingstructure.
In Marcus's parser, however, we can make a rule like:"IF the first element is NP and the second element is VP, THENlet NP and VP be sons of S," where NP which was recognizedsome time before is referenced again.
This framework seems to betoo strong as a simulation of our internal process.
The approachtaken in this research is to permit more appropriate and general-ized predictions as described in the previous ubsections.In our experiment, we make use of lookahead by permittingbacktracking within a limited range: once the analyzer eached then-th word, it would not cancel the previous decision maOe when itwas processing (n-k)-th word, where k is the length of lookahead.The necessary length of the lookahead is investigated in the experi-ment.Currently, preference factors are treated in the followingmanner.
The syntactic ategories a word belongs to are linearlyordered.
Grammer ules are divided into two groups, usual andunusual: the rules that trigger insertive structures with some otheruncommon rules are included in the latter group.
Although thestrategies are not fixed, generally we try each syntactic ategoryone by one according to the order induced, and the usual rules aretried before unusual ones.3.
ExperimentThe mechanisms described in the previous ection were testedby analyzing two kinds of articles.
The articles used in the experi-ment were a manual of a computer software and an abstract articleon world economics.
At first, basic grammar ules were writtenand they were revised and reinforced by looking at the result of theprevious analysis.The output of the analyzer is a kind of tree structure as shownin Figure 3-1.
(1) ... SI vt /Prini / / .... I\ I ......... art adj noun noun 'PPFinally foun the old man and woman w\ [ t~copeFigure 3-1.
Tree structure constructed by the analyzer.Structures that are captured by optional predictions and predictions Jmade through the and stack or insertive rules are called pendingstructures.
In the example, (1), (2) and (3) are pending structures:(i) is recognized as an insertive structure; (2) is captured tlu.oughthe and stack; and (3) is captured by an optional prediction.
Asshown in the figure, we temporarily attach them to the precedingpredictions.
In this representation, the word woman is modified bythe prepositional phrase with the telescope.
We, however, caneasily obtain other plausible sentence structures.
For example, ifwe attach (3) to VP, we get a tree structure where the prepositionalphrase modifies the verb phrase.
In our experiment, a sentence issaid to be successfully recognized if we can get an appropriate treestructure by moving pending structures (if necessary).The success rate and its relation with the length of lookaheadwas as follows.
Of the 85 sentences from each article, 65 (manual)and 70 (abstract) of them were analyzed as desired by making useof looking two words ahead, the current and the next word, whileonly one additional success was reported on each article by lookingone more word ahead.4.
ConclusionBased on the observation of human recognition process ofEnglish sentences of a non-native speaker, predictions we makeduring file process are analyzed.
We have also presented a descrip-tion method of such predictions as grammar ules which is basedon Greibach normal form, mad recognition mechanisms that arespecified by these rules, realized by using three stacks: the predic-tion stack, the and stack, and the NP stack.
The extension of therule description and introduction of these stacks provide us with asimple yet powerful means for recognition of syntactic structures.An experimental nalysis of more than 150 sentences i car-ried out, and necessary length of lookahead and preference factorsas well as the plausibility of the above mechanisms are tested.Over 70 percent of the sentences are recognized as desired andlooking two words ahead seems to be the critical length for thepredictive recognition.AcknowledgementsI would especially like to thank my adviser, Prof. A.Yonezawa of Tokyo Institute of Technology, for his valuable com-ments on this researdl and encouragement.
I also thank themembers of Yonezawa Lab.
for their comments on my research.
Ialso give my special thanks to the managers of Resource SharingCompany who allowed me to use their valuable dictionary for myresearch.References\[1\] S. Kuno, "The Predictive Analyzer and a Path EliminationTechnique," Comm.
ACM, Vol.
8, pp.
453-462, 1965.\[2\] M .P .
Marcus, A Theory of Syntactic Recognition for NaturalLanguage, MIT Press, Cambridge, i980.\[3\] M. Nagao, Language Engineering, Shoko-do, Tokyo, 1983,(in Yapanese).\[4\] W.A .
Woods, "Transition Network Grammars for NaturalLanguage Analysis," Comm.
ACM, Vol.
13, pp.
591-606,1970.\[5\] T. Winograd, Understanding Natural Language.
AcademicPress, New York, 1972.\[6\] B. K. Boguraev, "Recognising Conjunctions within the ATNFramework," pp.
39-45, in Automatic Natural Language Pars-ing (K.S.
\]'ones and Y. Wilks, eds.
), Ellis Horwood limited,1983.\[7\] T. Winograd, Language as a Cognitive Process, Vol.
1: Syntax,Addison-Wesley, 1983.472
