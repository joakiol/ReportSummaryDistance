Proceedings of the Workshop on A Broader Perspective on Multiword Expressions, pages 25?32,Prague, June 2007. c?2007 Association for Computational LinguisticsSemantics-based Multiword Expression ExtractionTim Van de Cruys and Begon?a Villada Moiro?nAlfa Informatica, University of GroningenOude Kijk in ?t Jatstraat 269712 EK Groningen, The Netherlands{T.Van.de.Cruys|M.B.Villada.Moiron}@rug.nlAbstractThis paper describes a fully unsupervisedand automated method for large-scale ex-traction of multiword expressions (MWEs)from large corpora.
The method aims at cap-turing the non-compositionality of MWEs;the intuition is that a noun within a MWEcannot easily be replaced by a semanti-cally similar noun.
To implement this intu-ition, a noun clustering is automatically ex-tracted (using distributional similarity mea-sures), which gives us clusters of semanti-cally related nouns.
Next, a number of statis-tical measures ?
based on selectional prefer-ences ?
is developed that formalize the intu-ition of non-compositionality.
Our approachhas been tested on Dutch, and automaticallyevaluated using Dutch lexical resources.1 IntroductionMWEs are expressions whose linguistic behaviour isnot predictable from the linguistic behaviour of theircomponent words.
Baldwin (2006) characterizes theidiosyncratic behavior of MWEs as ?a lack of com-positionality manifest at different levels of analysis,namely, lexical, morphological, syntactic, seman-tic, pragmatic and statistical?.
Some MWEs showproductive morphology and/or syntactic flexibility.Therefore, these two aspects are not sufficient con-ditions to discriminate actual MWEs from productiveexpressions.
Nonetheless, the mentioned character-istics are useful indicators to distinguish literal andidiomatic expressions (Fazly and Stevenson, 2006).One property that seems to affect MWEs the mostis semantic non-compositionality.
MWEs are typi-cally non-compositional.
As a consequence, it is notpossible to replace the noun of a MWE by semanti-cally related nouns.
Take for example the expres-sions in (1) and (2):(1) a. break the vaseb.
break the cupc.
break the dish(2) a. break the iceb.
*break the snowc.
*break the hailExpression (1-a) is fully compositional.
Therefore,vase can easily be replaced with semantically re-lated nouns such as cup and dish.
Expression (2-a),on the contrary, is non-compositional; ice cannot bereplaced with semantically related words, such assnow and hail without loss of the original meaning.Due to the idiosyncratic behavior, current propos-als argue that MWEs need to be described in the lexi-con (Sag et al, 2002).
In most languages, electroniclexical resources (such as dictionaries, thesauri, on-tologies) suffer from a limited coverage of MWEs.To facilitate the update and expansion of languageresources, the NLP community would clearly bene-fit from automated methods that extract MWEs fromlarge text collections.
This is the main motivation topursue an automated and fully unsupervised MWEextraction method.252 Previous WorkRecent proposals that attempt to capture seman-tic compositionality (or lack thereof) employ vari-ous strategies.
Approaches evaluated so far makeuse of dictionaries with semantic annotation (Piaoet al, 2006), WordNet (Pearce, 2001), automati-cally generated thesauri (Lin, 1999; McCarthy etal., 2003; Fazly and Stevenson, 2006), vector-basedmethods that measure semantic distance (Baldwin etal., 2003; Katz and Giesbrecht, 2006), translationsextracted from parallel corpora (Villada Moiro?nand Tiedemann, 2006) or hybrid methods that usemachine learning techniques informed by featurescoded using some of the above methods (Venkata-pathy and Joshi, 2005).Pearce (2001) describes a method to extract collo-cations from corpora by measuring semantic compo-sitionality.
The underlying assumption is that a fullycompositional expression allows synonym replace-ment of its component words, whereas a collocationdoes not.
Pearce measures to what degree a collo-cation candidate allows synonym replacement.
Themeasurement is used to rank candidates relative totheir compositionality.Building on Lin (1998), McCarthy et al (2003)measure the semantic similarity between expres-sions (verb particles) as a whole and their compo-nent words (verb).
They exploit contextual featuresand frequency information in order to assess mean-ing overlap.
They established that human composi-tionality judgements correlate well with those mea-sures that take into account the semantics of the par-ticle.
Contrary to these measures, standard associ-ation measures poorly correlate with human judge-ments.A different approach proposed by Villada Moiro?nand Tiedemann (2006) measures translational en-tropy as a sign of meaning predictability, and there-fore non-compositionality.
The entropy observedamong word alignments of a potential MWE varies:highly predictable alignments show less entropy andprobably correspond to compositional expressions.Data sparseness and polysemy pose problems be-cause the entropy cannot be accurately calculated.Fazly and Stevenson (2006) use lexical andsyntactic fixedness as partial indicators of non-compositionality.
Their method uses Lin?s (1998)automatically generated thesaurus to compute a met-ric of lexical fixedness.
Lexical fixedness mea-sures the deviation between the pointwise mutualinformation of a verb-object phrase and the aver-age pointwise mutual information of the expres-sions resulting from substituting the noun by itssynonyms in the original phrase.
This measure issimilar to Lin?s (1999) proposal for finding non-compositional phrases.
Separately, a syntactic flexi-bility score measures the probability of seeing a can-didate in a set of pre-selected syntactic patterns.
Theassumption is that non-compositional expressionsscore high in idiomaticity, that is, a score resultingfrom the combination of lexical fixedness and syn-tactic flexibility.
The authors report an 80% accu-racy in distinguishing literal from idiomatic expres-sions in a test set of 200 expressions.
The perfor-mance of both metrics is stable across all frequencyranges.In this study, we are interested in establishingwhether a fully unsupervised method can capturethe (partial or) non-compositionality of MWEs.
Themethod should not depend on the existence of large(open domain) parallel corpora or sense tagged cor-pora.
Also, the method should not require numer-ous adjustments when applied to new subclassesof MWEs, for instance, when coding empirical at-tributes of the candidates.
Similar to Lin (1999),McCarthy et al (2003) and Fazly and Stevenson(2006), our method makes use of automatically gen-erated thesauri; the technique used to compile thethesauri differs from previous work.
Aiming at find-ing a method of general applicability, the measuresto capture non-compositionality differ from thoseemployed in earlier work.3 MethodologyIn the description and evaluation of our algorithm,we focus on the extraction of verbal MWEs that con-tain prepositional complements, although we believethe method can be easily generalized to other kindsof MWEs.In our semantics-based approach, we want to for-malize the intuition of non-compositionality, so thatMWE extraction can be done in a fully automatedway.
A number of statistical measures are developedthat try to capture the MWE?s non-compositional26bond between a verb-preposition combination andits noun by comparing the particular noun of a MWEcandidate to other semantically related nouns.3.1 Data extractionThe MWE candidates (verb + prepositional phrase)are automatically extracted from the Twente NieuwsCorpus (Ordelman, 2002), a large corpus of Dutchnewspaper texts (500 million words), which hasbeen automatically parsed by the Dutch dependencyparser Alpino (van Noord, 2006).
Next, a matrix iscreated of the 5,000 most frequent verb-prepositioncombinations by the 10,000 most frequent nouns,containing the frequency of each MWE candidate.1To this matrix, a number of statistical measures areapplied to determine the non-compositionality of thecandidate MWEs.
These statistical measures are ex-plained in 3.3.3.2 ClusteringIn order to compare a noun to its semantically re-lated nouns, a noun clustering is created.
Theseclusters are automatically extracted using standarddistributional similarity techniques (Weeds, 2003;van der Plas and Bouma, 2005).
First, depen-dency triples are extracted from the Twente NieuwsCorpus.
Next, feature vectors are created for eachnoun, containing the frequency of the dependencyrelations in which the noun occurs.2 This way, afrequency matrix of 10K nouns by 100K depen-dency relations is constructed.
The cell frequenciesare replaced by pointwise mutual information scores(Church et al, 1991), so that more informative fea-tures get a higher weight.
The noun vectors are thenclustered into 1,000 clusters using a simple K-meansclustering algorithm (MacQueen, 1967) with cosinesimilarity.
During development, several other clus-tering algorithms and parameters have been tested,but the settings described above gave us the bestEuroWordNet similarity score (using Wu and Palmer(1994)).Note that our clustering algorithm is a hard clus-tering algorithm, which means that a certain noun1The lowest frequency verb-preposition combination (withregard to the 10,000 nouns) appears 3 times.2e.g.
dependency relations that qualify apple might be ?ob-ject of eat?
and ?adjective red?.
This gives us dependency tripleslike < apple, obj, eat >.can only be assigned to one cluster.
This may pose aproblem for polysemous nouns.
On the other hand,this makes the computation of our metrics straight-forward, since we do not have to decide among var-ious senses of a word.3.3 MeasuresThe measures used to find MWEs are inspired byResnik?s method to find selectional preferences(Resnik, 1993; Resnik, 1996).
Resnik uses a numberof measures based on the Kullback-Leibler diver-gence, to measure the difference between the priorprobability of a noun class p(c) and the probabil-ity of the class given a verb p(c|v).
We adopt themethod for particular nouns, and add a measure fordetermining the ?unique preference?
of a noun givenother nouns in the cluster, which, we claim, yieldsa measure of non-compositionality.
In total, 4 mea-sures are used, the latter two being the symmetriccounterpart of the former two.The first two measures, Av?n (equation 2) andRv?n (equation 3), formalize the unique prefer-ence of the verb3 for the noun.
Equation 1 givesthe Kullback-Leibler divergence between the overallprobability distribution of the nouns and the proba-bility distribution of the nouns given a verb; it is usedas a normalization constant in equation 2.
Equa-tion 2 models the actual preference of the verb forthe noun.Sv =?np(n | v) logp(n | v)p(n) (1)Av?n =p(n | v) log p(n|v)p(n)Sv(2)When p(n|v) is 0, Av?n is undefined.
In thiscase, we assign a score of 0.Equation 3 gives the ratio of the verb preferencefor a particular noun, compared to the other nounsthat are present in the cluster.Rv?n =Av?n?n?
?C Av?n?
(3)When Rv?n is more or less equally dividedamong the different nouns in the cluster, there is no3We will use ?verb?
to designate a prepositional verb, i.e.
acombination of a verb and a preposition.27preference of the verb for a particular noun in thecluster, whereas scores close to 1 indicate a ?unique?preference of the verb for a particular noun in thecluster.
Candidates whose Rv?n value approaches1 are likely to be non-compositional expressions.In the latter two measures, An?v and Rn?v, thedirection of preference is changed: equations 4 and 5are the symmetric counterparts of equations 2 and 3.Instead of the preference of the verb for the noun,the preference of the noun for the verb is modelled.Except for the change of preference direction, thecharacteristics of the former and the latter two mea-sures are the same.An?v =p(v | n) log p(v|n)p(v)Sn(4)Rn?v =An?v?n?
?C An?
?v(5)Note that, despite their symmetry, the measuresfor verb preference and the measures for noun pref-erence are different in nature.
It is possible thata certain verb only selects a restricted number ofnouns, while the nouns themselves can co-occurwith many different verbs.
This brings about differ-ent probability distributions.
In our evaluation, wewant to investigate the impact of both preferences.3.4 ExampleIn this section, an elaborated example is presented,to show how our method works.
Take for examplethe two MWE candidates in (3):(3) a. inindethesmaaktastevallenfallto be appreciatedb.
inindetheputwellvallenfallto fall down the wellIn the first expression, smaak cannot be replacedwith other semantically similar nouns, such as geur?smell?
and zicht ?sight?, whereas in the second ex-pression, put can easily be replaced with other se-mantically similar words, such as kuil ?hole?
andkrater ?crater?.The first step in the formalization of this intuition,is the extraction of the clusters in which the wordssmaak and put appear from our clustering database.This gives us the clusters in (4).
(4) a. smaak: aroma ?aroma?, gehoor ?hear-ing?, geur ?smell?, gezichtsvermogen?sight?, reuk ?smell?, spraak ?speech?,zicht ?sight?b.
put: afgrond ?abyss?, bouwput ?build-ing excavation?, gaatje ?hole?, gat?hole?, hiaat ?gap?, hol ?cave?, kloof?gap?, krater ?crater?, kuil ?hole?, lacune?lacuna?, leemte ?gap?, valkuil ?pitfall?Next, the various measures described in section 3.3are applied.
Resulting scores are given in tables 1and 2.MWE candidate Av?n Rv?n An?v Rn?vval#in smaak .12 1.00 .04 1.00val#in geur .00 .00 .00 .00val#in zicht .00 .00 .00 .00Table 1: Scores for MWE candidate in de smaakvallen and other nouns in the same cluster.Table 1 gives the scores for the MWE in de smaakvallen, together with some other nouns that arepresent in the same cluster.
Av?n shows that thereis a clear preference (.12) of the verb val in for thenoun smaak.
Rv?n shows that there is a uniquepreference of the verb for the particular noun smaak.For the other nouns (geur, zicht, .
.
.
), the verb has nopreference whatsoever.
Therefore, the ratio of verbpreference for smaak compared to the other nounsin the cluster is 1.00.An?v and Rn?v show similar behaviour.
Thereis a preference (.04) of the noun smaak for the verbval in, and this preference is unique (1.00).MWE candidate Av?n Rv?n An?v Rn?vval#in put .00 .05 .00 .05val#in kuil .01 .11 .02 .37val#in kloof .00 .02 .00 .03val#in gat .04 .71 .01 .24Table 2: Scores for MWE candidate in de put vallenand other nouns in same cluster.28Table 2 gives the scores for the instance in de putvallen ?
which is not a MWE ?
together with othernouns from the same cluster.
The results are quitedifferent from the ones in table 1.
Av?n ?
the pref-erence of the verb for the noun ?
is quite low in mostcases, the highest score being a score of .04 for gat.Furthermore, Rv?n does not show a unique pref-erence of val in for put (a low ratio score of .05).Instead, the preference mass is divided among thevarious nouns in the cluster, the highest preferenceof val in being assigned to the noun gat (.71).4The other two scores show again a similar ten-dency; An?v ?
the preference of the noun for theverb ?
is low in all cases, and when all nouns in thecluster are considered (Rn?v), there is no ?unique?preference of one noun for the verb val in.
Instead,the preference mass is divided among all nouns inthe cluster.4 Results & Evaluation4.1 Quantitative evaluationIn this section, we quantitatively evaluate ourmethod, and compare it to the lexical and syntacticfixedness measures proposed by Fazly and Steven-son (2006).
More information about Fazly andStevenson?s measures can be found in their paper.The potential MWEs that are extracted with thefully unsupervised method described above and withFazly and Stevenson?s (2006) method (FS from hereonwards) are automatically evaluated by compar-ing the extracted list to handcrafted MWE databases.Since we have extracted Dutch MWEs, we are us-ing the two Dutch resources available: the Refer-entie Bestand Nederlands (RBN, Martin and Maks(2005)) and the Van Dale Lexicographical Informa-tion System (VLIS) database.
Evaluation scores arecalculated with regard to the MWEs that are presentin our evaluation resources.
Among the MWEs in ourreference data, we consider only those expressionsthat are present in our frequency matrix: if the verbis not among the 5,000 most frequent verbs, or thenoun is not among the 10,000 most frequent nouns,the frequency information is not present in our input4The expression is ambiguous: it can be used in a lit-eral sense (in een gat vallen, ?to fall down a hole?)
and in ametaphorical sense (in een zwart gat vallen, ?to get depressedafter a joyful or busy period?).data.
Consequently, our algorithm would never beable to find those MWEs.The first six rows of table 3 show precision, re-call and f-measure for various parameter thresholdswith regard to the measures Av?n, Rv?n, An?vand Rn?v, together with the number of candidatesfound (n).
The last 3 rows show the highest val-ues we were able to reach by using FS?s fixednessscores.Using only two parameters ?
Av?n and Rv?n ?gives the highest f-measure (?
14%), with a pre-cision and recall of about 17% and about 12% re-spectively.
Adding parameter Rn?v increases preci-sion but degrades recall, and this tendency continueswhen adding both parameters An?v and Rn?v.
Inall cases, a higher threshold increases precision butdegrades recall.
When using a high threshold for allparameters, the algorithm is able to reach a precisionof ?
38%, but recall is low (?
4%).Lexical fixedness reaches an f-measure of ?
12%(threshold of 3.00).
These scores show the best per-formance that we reached using lexical fixedness.Following FS, we evaluated the syntactic fixednessscores of expressions falling above a frequency cut-off.
Since our corpus is much larger than that usedby FS, a frequency cutoff of 50 was chosen.
The pre-cision, recall and f-measure of the syntactic fixed-ness measure (shown on table 3) are ?
10%, 41%and 16% respectively, showing worse precision thanour method but much better recall and f-measure.As shown by FS, syntactic fixedness performs betterthan lexical fixedness; Fixednessoverall improveson the syntactic fixedness results and also reachesbetter overall performance than our method.The compared methods show a different behav-ior.
FS?s method favours high recall whereas ourmethod prefers the best trade-off between precisionand recall.
We wish to highlight that our methodreaches better precision than FS?s method while han-dling many low frequency candidates (minimum fre-quency is 3); this makes our method preferable insome NLP tasks.
It is possible that the two methodsare capturing different properties of MWEs; in futurework, we want to analyse whether the expressionsextracted by the two methods differ.29parameters precision recall f-measureAv?n Rv?n An?v Rn?v n (%) (%) (%).10 .80 ?
?
3175 16.09 13.11 14.45.10 .90 ?
?
2655 17.59 11.98 14.25.10 .80 ?
.80 2225 19.19 10.95 13.95.10 .90 ?
.90 1870 20.70 9.93 13.42.10 .80 .01 .80 1859 20.33 9.69 13.13.20 .99 .05 .99 404 38.12 3.95 7.16Fixednesslex(v, n) 3.00 3899 15.14 9.92 11.99Fixednesssyn(v, n) 50 15,630 10.20 40.90 16.33Fixednessoverall(v, n) 50 7819 13.73 27.54 18.33Table 3: Evaluation results compared to RBN & VLIS4.2 Qualitative evaluationNext, we elaborate upon advantages and disadvan-tages of our semantics-based MWE extraction algo-rithm by examining the output of the procedure, andlooking at the characteristics of the MWEs found andthe errors made by the algorithm.First of all, our algorithm is able to filter out gram-matical collocations that cause problems in tradi-tional MWE extraction paradigms.
An example isgiven in (5).
(5) voldoenmeetaantoeisen,demands,voorwaardenconditionsmeet the {demands, conditions}In traditional MWE extraction algorithms, based oncollocations, highly frequent expressions like theones in (5) often get classified as a MWE, eventhough they are fully compositional.
Such algo-rithms correctly identify a strong lexical affinity be-tween two component words (voldoen, aan), whichmake up a grammatical collocation; however, theyfail to capture the fact that the noun may be filled inby a semantic class of nouns.
Our algorithm filtersout those expressions, because semantic similaritybetween nouns that fill in the object slot is taken intoaccount.Our quantitative evaluation shows that the algo-rithm reaches the best results (i.e.
the highest f-measures) when using only two parameters (Av?nand Rv?n).
Upon closer inspection of the output,we noticed that An?v and Rn?v are often able tofilter out non-MWEs like the expressions b in (6)and (7).
(6) a. verschijnenappearopontoneelstageto appearb.
zingensingopontoneelstageto sing on the stage(7) a. liglieiningeheugenmemorybe in memoryb.
liglieininziekenhuishospitallie in the hospitalIt is only when the two other measures (a uniquepreference of the noun for the verb) are taken intoaccount that the b expressions are filtered out ?
ei-ther because the noun preference for the verb is verylow, or because it is more evenly distributed amongthe cluster.
The b expressions, which are non-MWEs,result from the combination of a verb with a highlyfrequent PP.
These PPs are typically locative, direc-tional or predicative PPs, that may combine with nu-merous verbs.Also, expressions like the ones in (8), where thefixedness of the expression lies not so much in theverb-noun combination, but more in the noun part(naar school, naar huis) are filtered out by the lat-ter two measures.
These preposition-noun combina-tions seem to be institutionalized PPs, so-called de-terminerless PPs.30(8) a. naartoschoolschoolwillenwantwant to go to schoolb.
naartohuishomewillenwantwant to go homeWe will now look at some errors made by our algo-rithm.
First of all, our algorithm highly depends onthe quality of the noun clustering.
If a noun appearsin a cluster with unrelated words, the measures willoverrate the semantic uniqueness of the expressionsin which the noun appears.Secondly, syntax might play an important role.Sometimes, there are syntactic restrictions betweenthe preposition and the noun.
A noun like pagina?page?
can only appear with the preposition op ?on?,as in lees op pagina ?read on page?.
Other, semanti-cally related nouns, such as hoofdstuk ?chapter?, pre-fer in ?in?.
Due to these restrictions, the measureswill again overrate the semantic uniqueness of thenoun (pagina in the example).Finally, our hard clustering method does not takepolysemous nouns into account.
A noun may onlyoccur in one cluster, ignoring other possible mean-ings.
Schaal, for example, means ?dish?
as well as?scale?.
In our clustering, it only appears in a clusterof dish-related nouns.
Therefore, expressions likemaak gebruik op [grote] schaal ?make use of [sth.
]on a [large] scale?, receive again overrated measuresof semantic uniqueness, because the ?scale?
sense ofthe noun is compared to nouns related to the ?dish?sense.5 Conclusions and further workOur algorithm based on non-compositionality ex-plores a new approach aimed at large-scale MWEextraction.
Using only two parameters, Av?n andRv?n, yields the highest f-measure.
Using the twoother parameters, An?v and Rn?v, increases preci-sion but degrades recall.
Due to the formalization ofthe intuition of non-compositionality (using an auto-matic noun clustering), our algorithm is able to ruleout various expressions that are coined MWEs by tra-ditional algorithms.Note that our algorithm has taken on a purelysemantics-based approach.
?Syntactic fixedness?
ofthe expressions is not taken into account.
Combin-ing our semantics-based approach with other extrac-tion techniques such as the syntactic fixedness mea-sure proposed by Fazly and Stevenson (2006) mightimprove the results significantly.We conclude with some issues saved for futurework.
First of all, we would like to combine oursemantics-based method with other methods that areused to find MWEs (especially syntax-based meth-ods), and implement the method in general classifi-cation models (decision tree classifier and maximumentropy model).
This includes the use of a moreprincipled (machine learning) framework in order toestablish the optimal threshold values.Next, we would like to investigate a number oftopics to improve on our semantics-based method.First of all, using the top k similar nouns for a certainnoun ?
instead of the cluster in which a noun appears?
might be more beneficial to get a grasp of the com-positionality of MWE candidates.
Also, making useof a verb clustering in addition to the noun clusteringmight help in determining the non-compositionalityof expressions.
Disambiguating among the varioussenses of nouns should also be a useful improve-ment.
Furthermore, we would like to generalize ourmethod to other syntactic patterns (e.g.
verb objectcombinations), and test the approach for English.One final issue is the realization of a manual eval-uation of our semantics-based algorithm, by hav-ing human judges decide whether a MWE candidatefound by our algorithm is an actual MWE.
Our au-tomated evaluation framework is error-prone due tomistakes and incompleteness of our resources.
Dur-ing qualitative evaluation, we found many actualMWEs found by our algorithm, that were not con-sidered correct by our resources (e.g.
[iemand] inde gordijnen jagen ?to drive s.o.
mad?, op het [ver-keerde] paard gokken ?back the wrong horse?, [dekat] uit de boom kijken ?wait to see which way thewind blows?, uit het [goede] hout gesneden ?be atrustworthy person?).
Conversely, there were alsoquestionable MWE candidates that were describedas actual MWEs in our evaluation resources (val opwoensdag ?fall on a wednesday?, neem als voorzitter?take as chairperson?, ruik naar haring ?smell likeherring?, ben voor [.
.
. ]
procent ?to be .
.
.
percent?
).A manual evaluation could overcome these difficul-ties.We believe that our method provides a genuine31and successful approach to get a grasp of the non-compositionality of MWEs in a fully automated way.We also believe that it is one of the first methodsable to extract MWEs based on non-compositionalityon a large scale, and that traditional MWE extrac-tion algorithms will benefit from taking this non-compositionality into account.AcknowledgementsThis research was carried out as part of the researchprogram IRME STEVIN project.
We would also liketo thank Gertjan van Noord and the two anonymousreviewers for their helpful comments on an earlierversion of this paper.ReferencesT.
Baldwin, C. Bannard, T. Tanaka, and D. Widdows.
2003.
AnEmpirical Model of Multiword Expressions Decomposabil-ity.
In Proc.
of the ACL-2003 Workshop on Multiword Ex-pressions: Analysis, Acquisition and Treatment, pages 89?96, Sapporo, Japan.T.
Baldwin.
2006.
Compositionality and Multiword Expres-sions: Six of One, Half a Dozen of the Other?
Invited talkgiven at the COLING/ACL?06 Workshop on Multiword Ex-pressions: Identifying and Exploiting Underlying Properties,July.K.
Church, W. Gale, P. Hanks, and D. Hindle.
1991.
Usingstatistics in lexical analysis.
In Uri Zernik, editor, LexicalAcquisition: Exploiting On-line resources to build a lexicon,pages 115?164.
Lawrence Erlbaum Associates, New Jersey.A.
Fazly and S. Stevenson.
2006.
Automatically constructinga lexicon of verb phrase idiomatic combinations.
In Pro-ceedings of the 11th Conference of the European Chapter ofthe Association for Computational Linguistics (EACL-2006),Trento, Italy.G.
Katz and E. Giesbrecht.
2006.
Automatic identification ofnon-compositional multi-word expressions using Latent Se-mantic Analysis.
In Proc.
of the COLING/ACL?06 Work-shop on Multiword Expressions: Identifying and ExploitingUnderlying Properties, pages 12?19, Sydney, Australia.D.
Lin.
1998.
Automatic retrieval and clustering of simi-lar words.
In Proceedings of COLING/ACL 98, Montreal,Canada.D.
Lin.
1999.
Automatic identification of non-compositionalphrases.
In Proceedings of ACL-99, pages 317?324.
Univer-sity of Maryland.J.
B. MacQueen.
1967.
Some methods for classification andanalysis of multivariate observations.
In Proceedings of 5-thBerkeley Symposium on Mathematical Statistics and Prob-ability, volume 1, pages 281?297, Berkeley.
University ofCalifornia Press.W.
Martin and I. Maks, 2005.
Referentie Bestand Nederlands.Documentatie, April.D.
McCarthy, B. Keller, and J. Carroll.
2003.
Detecting a Con-tinuum of Compositionality in Phrasal Verbs.
In Proc.
ofthe ACL-2003 Workshop on Multiword Expressions: Analy-sis, Acquisition and Treatment, Sapporo, Japan.R.J.F.
Ordelman.
2002.
Twente Nieuws Corpus (TwNC), Au-gust.
Parlevink Language Techonology Group.
University ofTwente.D.
Pearce.
2001.
Synonymy in collocation extraction.
In Word-Net and Other lexical resources: applications, extensions& customizations (NAACL 2001), pages 41?46, Pittsburgh.Carnegie Mellon University.S.
Piao, P. Rayson, O. Mudraya, A. Wilson, and R. Garside.2006.
Measuring mwe compositionality using semantic an-notation.
In Proceedings of the Workshop on MultiwordExpressions: Identifying and Exploiting Underlying Prop-erties, pages 2?11, Sydney, Australia.
Association for Com-putational Linguistics.P.
Resnik.
1993.
Selection and Information: A Class-BasedApproach to Lexical Relationships.
PhD Thesis, Universityof Pennsylvania.P.
Resnik.
1996.
Selectional constraints: An information-theoretic model and its computational realization.
Cogni-tion, 61:127?159.I.
Sag, T. Baldwin, F. Bond, A. Copestake, and D. Flickinger.2002.
Multiword Expressions: a pain in the neck for NLP.In Proceedings of the Third International Conference onIntelligent Text Processing and Computational Linguistics,pages 1?15, Mexico City, Mexico.L.
van der Plas and G. Bouma.
2005.
Syntactic contexts forfinding semantically similar words.
Computational Linguis-tics in the Netherlands 2004.
Selected Papers from the Fif-teenth CLIN Meeting, pages 173?184.G.
van Noord.
2006.
At Last Parsing Is Now Operational.In P. Mertens, C. Fairon, A. Dister, and P. Watrin, editors,TALN06.
Verbum Ex Machina.
Actes de la 13e conferencesur le traitement automatique des langues naturelles, pages20?42, Leuven.S.
Venkatapathy and A. Joshi.
2005.
Measuring the relativecompositionality of verb-noun collocations by integratingfeatures.
In Proceedings of the Human Language Technol-ogy Conference and Conference on Empirical Methods inNatural Language Processing, pages 899?906, Vancouver.B.
Villada Moiro?n and J. Tiedemann.
2006.
Identifying id-iomatic expressions using automatic word-alignment.
InProceedings of the EACL 2006 Workshop on Multi-word-expressions in a multilingual context?, pages 33?40, Trento,Italy.J.
Weeds.
2003.
Measures and Applications of Lexical Distri-butional Similarity.
PhD Thesis, University of Sussex.Z.
Wu and M. Palmer.
1994.
Verb semantics and lexical selec-tion.
In 32nd.
Annual Meeting of the Association for Com-putational Linguistics, pages 133?138, New Mexico StateUniversity, Las Cruces, New Mexico.32
