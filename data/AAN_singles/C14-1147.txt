Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 1544?1555, Dublin, Ireland, August 23-29 2014.Collective Named Entity Disambiguation using Graph Ranking andClique Partitioning ApproachesAyman Alhelbawy?
?and Robert Gaizauskas?
?The University of Sheffield, Regent Court, 211 Portobello Street, Sheffield, S1 4DP, U.K?Faculty of Computers and Information, Fayoum University, Fayoum, Egyptayman,R.Gaizauskas@dcs.shef.ac.ukAbstractDisambiguating named entities (NE) in running text to their correct interpretations in a specificknowledge base (KB) is an important problem in NLP.
This paper presents two collective disam-biguation approaches using a graph representation where possible KB candidates for NE textualmentions are represented as nodes and the coherence relations between different NE candidatesare represented by edges.
Each node has a local confidence score and each edge has a weight.The first approach uses Page-Rank (PR) to rank all nodes and selects a candidate based on PRscore combined with local confidence score.
The second approach uses an adapted Clique Par-titioning technique to find the most weighted clique and expands this clique until all NE textualmentions are disambiguated.
Experiments on 27,819 NE textual mentions show the effectivenessof both approaches, outperforming both baseline and state-of-the-art approaches.1 IntroductionNamed entities (NEs) have received a lot of attention from the NLP community over the last two decades(see, e.g.
Nadeau and Sekine (2007)).
Most of this work has focussed on the task of recognizing theboundaries of NE mentions in text and classifying them into one of several classes, such as Person,Organization or Location.
However, references to entities in the real world are often ambiguous: there isa many-to-many relation between NE mentions in text and the entities denoted by these mentions in theworld.
For example, the same NE mention ?Norfolk?
may refer to a person, ?Peter Norfolk, a wheelchairtennis player?, a place in the United Kingdom, ?Norfolk County?, or a place in the United States like?Norfolk, Massachusetts?
; conversely, one entity many be known by many names, such as ?Cat Stevens?,?Yusuf Islam?
and ?Steven Georgiou?.
The task of named entity disambiguation (NED) is to establisha correct mapping between each NE mention in a document and the entity it denotes in the real world.Following most researchers in this area, we treat entries in a large knowledge base (KB) as surrogates forreal world entities when carrying out NED and, in particular, use Wikipedia as the reference KB againstwhich to disambiguate NE mentions.
NED is important for tasks like KB population, where we want toextract new information from text about an entity and add it to a pre-existing entry for that entity in aKB, or for information retrieval where we may want to cluster or filter results for different entities withthe same textual mentions.The main hypothesis underlying this work is that different NEs in a document help to disambiguateeach other.
However, other textual mentions in the document are also ambiguous.
So, what is needed isa collective disambiguation approach that jointly disambiguates all NE textual mentions.In our approaches we model each possible candidate for every NE mention in a document as a distinctnode in a graph and model candidate coherence by links between the nodes.
Figure 1 shows an exampleof the disambiguation graph for three mentions ?A?, ?B?, and ?C?
found in a document, where thecandidate entities for each mention are referred to using the lower case form of the mention?s lettertogether with a distinguishing subscript.
The goal of disambiguation is to find a set of nodes where onlyone candidate is selected from the set of entities associated with each mention, e.g.
a3, b2, c2.This work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedings footerare added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/1544Figure 1: Example of solution graphWe propose two different approaches to find the best disam-biguation candidates in the graph.
The first approach starts byfinding the most confident and coherent set of disambiguationentities and iteratively expands this set until all NE textual men-tions are disambiguated.
The second approach ranks all nodesin the solution graph using the Page-Rank algorithm, then re-ranks all nodes by combining the initial confidence and graphranking scores.
We consider several different measures for com-puting the initial confidence assigned to each node and severalmeasures for determining and weighting the graph edges.
Nodelinking relies on the fact that the textual portion of KB entriestypically contains mentions of other NEs.
When these mentionsare hyper-linked to KB entries, we can infer that there is some relation between the real world entitiescorresponding to the KB entries, i.e.
that they should be linked in our solution graph.
These links alsoallow us to build up statistical co-occurrence counts between entities that occur in the same context,which may be used to weight edges in our graph.We evaluate our approaches on the AIDA dataset (Hoffart et al., 2011).
Comparison with the baselineand some state-of-the-art approaches shows our proposed approaches offers substantial improvements indisambiguation accuracy.The rest of the paper is organized as follows: Section 2 discusses related work.
Section 3 discussesselection of NE candidate entities from the Wikipedia knowledge base and the assignment of confidencescores to each candidate.
Formulation of the NE disambiguation problem in terms of a graph model ispresented in section 4.
Sections 5 and 6 describe the clique partitioning and ranking disambiguation ap-proaches for collective NED.
The experimental dataset and experimental results are presented in Section7.
Section 8 concludes the paper and presents some suggestions for future work to improve the results.2 Related WorkNamed Entity Disambiguation has received a lot attention in the past few years.
Perhaps the best knownrelated work is the Entity Linking (EL) shared task challenge first proposed by the National Institute ofStandards and Technology (NIST) as part of the Knowledge Base Population (KBP) track within the TextAnalysis Conference (TAC) in 2009 (McNamee and Dang, 2009).
EL is a similar but broader task thanNED: NED is concerned with disambiguating a textual NE mention where the correct entity is known tobe one of the KB entries, while EL also requires systems to deal with the case where there is no entry forthe NE in the reference KB.
Ji et al.
(2011) group and summarise the different approaches to EL takenby participating systems.In general, there are two main lines of approach to the NED problem.
The first, single entity dis-ambiguation approaches (SNED), disambiguates one entity at a time without considering the effect ofother NEs.
These approaches use local context textual features of the mention and compare them to thetextual features of NE candidate documents in the KB, and link to the most similar.
The first approach inthis line was Bunescu and Pasca (2006), who measure similarity between the textual context of the NEmention and the Wikipedia categories of the candidate.
More similarity features were added by Cucerzan(2007) who realized that topical coherence between a candidate entity and other entities in the contextwill improve NED accuracy by calculating the nodes?
coherence based on the their incoming links inWikipedia and the overlaps in Wikipedia categories.
Milne and Witten (2008) improve Cucerzan?s workby calculating the topical coherence using Normalized Google Distance and restrict the context entitiesto the unambiguous entities.
Different query expansion approaches are incorporated into this framework,such as using context term expansion (Gottipati and Jiang, 2011) and acronym expansion (Zhang et al.,2011).
Sen (2012) proposed a latent topic model to learn the context entity association.
Machine learningis widely used in SNED as some approaches deal with the problem as a search result ranking problem.Supervised learn-to-rank models are used to re-rank the ambiguous candidate set (Zheng et al., 2010;Dredze et al., 2010; Alhelbawy and Gaizauskas, 2012; Nebhi, 2013).1545The second line of approach is collective named entity disambiguation (CNED), where all mentions ofentities in the document are disambiguated jointly.
These approaches try to model the interdependencebetween the different candidate entities for different NE mentions in the query document, and reformulatethe problem of NED as a global optimization problem whose aim is to find the best set of entities.
Asthis new formulation is NP-hard, many approximations have been proposed.
Kulkarni et.
al.
(2009)presents a collective approach for entity linking that models the coherence between all pairs of entitycandidates for different mentions as a probabilistic factor graph.
They present two approximations tosolve this optimization problem where the interdependence between decisions is modelled as the sumof the pairs?
dependencies.
Alhelbawy and Gaizauskas (2013) proposed a sequence dependency modelusing HMMs to model NE interdependency.
Another approximation uses a mixture of local and globalfeatures to train the coefficients of a linear ranking SVM to rank different NE candidates (Ratinov et al.,2011).
Shirakawa et al.
(2011) cluster related textual mentions and assign a concept to each cluster usinga probabilistic taxonomy.
The concept associated with a mention is used in selecting the correct entityfrom the Freebase KB.Graph models are widely used in collective disambiguation approaches.
All these approaches modelNE interdependencies, while different methods may be used for disambiguation.
Han (2011) uses localdependency between NE mention and the candidate entity, and semantic relatedness between candidateentities to construct a referent graph, proposing a collective inference algorithm to infer the correctreference node in the graph.
Hoffert (2011) poses the problem as one of finding a dense sub-graph,which is infeasible in a huge graph.
So, an algorithm originally used to find strongly interconnected,size-limited groups in social media is adapted to prune the graph, and then a greedy algorithm is used tofind the densest graph.The word sense disambiguation (WSD) task has many similarities to NED, since in both cases thegoal is to determine which of a set of predefined senses or reference entities is the correct interpretationof a surface string in context.
Many researchers have used graph-based approaches successfully forthe WSD task.
Sinha and Michalecea (2007) proposed using four different graph centrality algorithms?
Indegree, PageRank, Closeness and Betweenness for WSD.
We propose to use a clique partitioningalgorithm, originally proposed by Born et al.
(1973), for NED.
Clique algorithms have been successfullyused for WSD problems.
Guti?errez et al.
(2011; 2012), for example, use an N-cliques graph partitioningtechnique to identify sets of highly related senses.
However, this approach has not been used for NED.Our second proposed model uses the Page-Rank algorithm (PR), which to our knowledge has also notpreviously been applied to NED.
PR was proposed by Page et al.
(1999) to produce a global rank forweb pages based on the hyperlink structure of the web.
Xing and Ghorbani (2004) adapted PR to takeinto account the weights of links and the nodes?
importance.
PR and Personalized PR algorithms havebeen used successfully in WSD (e.g.
Sinha and Mihalcea (2007), Agirre and Soroa (2008; 2009)).3 Named Entity Candidates SelectionGiven an input document D containing a set of pre-tagged NE textual mentions M ={m1,m2,m3.
.
.mk}, we need to select all possible candidate interpretations for each mifrom theknowledge base.
I.e.
for each NE textual mention mi?
M we select a set of candidates Ei={ei,1, ei,2, ei,3.
.
.
ei,j} from the KB.
The NE textual mention miis used to search the KB entry ti-tles using Lucene1to find entries with titles that fully or partially contain the NE textual mention.The following example shows the possible candidates for the textual mention ?Sheffield?
: ?Sheffield,New Zealand,?, ?University of Sheffield?, ?Sheffield United F.C.
?, ?Sheffield, Massachusetts?, ?FredSheffield?, ?Sheffield, Alabama?, etc.
The result of this search is quite large and this increases thelikelihood of the correct entry occurring somewhere in the list, i.e.
it improves recall.
However, thechallenge now moves to the disambiguation step.
In this step, we need to assign a confidence score toeach candidate, as shown in the following section.1https://lucene.apache.org/15463.1 Candidate Confidence ScoreFor each candidate ei,j, a set of initial confidence scores IConf(ei,j) is assigned.
These scores arecalculated for each NE candidate independent of other candidates or the candidates for other NE textualmentions in the document.
Three scores are calculated locally using the NE textual mention context.There is also one global confidence score, entity popularity (EP), which is calculated globally indepen-dent of the document or the textual mention context by using the Freebase KB (Bollacker et al., 2008).The four confidence scores to be calculated for each NE candidate as follows:?
Cos: The cosine similarity between the NE textual mention and the KB entry title.?
JwSim: While the cosine similarity between a textual mention in the document and the candidateNE title in the KB is widely used in NED, this similarity is a misleading feature.
For example,the textual mention ?Essex?
may refer to either of the following candidates ?Essex County CricketClub?
or ?Danbury, Essex?, both of which are returned by the candidate generation process.
Thecosine similarity between ?Essex?
and ?Danbury, Essex?
is higher than that between ?Essex?
and?Essex County Cricket Club?, which is not helpful in the NED setting.
We adopted a new mention-candidate similarity function, jwSim, which uses Jaro-Winkler similarity as a first estimate of theinitial confidence value for each candidate.
This function considers all terms found in the candidateentity KB entry title, but not in the textual mention as disambiguation terms.
The percentage ofdisambiguation terms found in the query document is used to boost in the initial jwSim value, inaddition to an acronym check (whether the NE textual mention could be an acronym for a specificcandidate entity title).
Experiments show that jwSim performs much better than the standard cosinesimilarity.?
Ctxt: The cosine similarity between the sentence containing the NE mention in the query documentand the textual description of the candidate NE in the KB (we use the first section of the Wikipediaarticle as the candidate entity description).?
EP: Entity popularity refers to connectivity to this entity.
It has been used successfully as a dis-criminative feature for NED (Nebhi, 2013).
Freebase provides an API interface to get an entity?spopularity score, which is computed during Freebase data indexing.
This score is a function of theentity?s inbound and outbound link counts in Freebase and Wikipedia2.Initial confidence scores are calculated independently for each candidate entity for an NE mention.
How-ever, after the initial calculation, initial confidence scores for all candidates for a single NE mention arenormalized to sum to 1.4 Disambiguation Graph ModelIn this section we discuss the graph model we use for NED.
All candidate entities for the different NEtextual mentions in the document are represented as an undirected graph G = (V,D) where V is thenode set of all possible candidate entities for different NE textual mentions in the input document and Dis the set of edges between nodes.
Because the same entity may be found multiple times as a candidatefor different textual mentions and each occurrence must be evaluated independently, each node is formedas an ordered pair of textual mention miand candidate entity ei,j.
So, the graph nodes are formulated asa set V = {(mi, ei,j) | ?ei,j?
Ei,?mi?M}.A set of entities is coherent if real world relations hold between them.
We use such relations to linkcandidate entities for different NE textual mentions in our graph model.
Edges are not drawn betweendifferent nodes for the same mention.
However, they are drawn between two entities when there is arelation between them.
Different approaches to determine and weight entity coherence relations arepresented in the following section.2https://developers.google.com/freebase/v1/search15474.1 Entity CoherenceEntity coherence refers to the real world relatedness of different entities which are candidate interpreta-tions of different textual mentions in the document.
Such relatedness is not based on documentcontext, sothe relatedness of two candidate entities is always the same regardless of the query document.
Coherenceis represented as an edge between nodes in the graph.
We used two measures for coherence:?
Entity Reference Relation (Ref): This is a boolean relation between two entities e1and e2.
The Refrelation holds if the Wikipedia document for either entity has a link to the other.
Since the Wikipediahyperlinks are directed, this relation is implicitly directed.
However, we assume an inverse relationalso exists and represented the relation as undirected.Ref(ei, ej) ={1, if eior ejrefers to the other0, otherwise(1)?
Entity Co-occurrence (Jprob): An estimate of the probability of both entities appearing in the samesentence.
Wikipedia documents are used to estimate this probability, as shown in (2), where S(e) isthe set of all sentences that contain a hyperlink reference to the entity e and S is the set of sentencescontaining any such entity references.Jprob(ei, ej) =|S(ei)?S(ej)||S|(2)5 Cliques Partitioning DisambiguationThe clique model originated in social network studies when Luce and Perry (1949) defined a clique as aset of two or more people who are mutual friends.
In graph theory, this pattern is known as a completesub-graph.
Assuming that NEs that appear in the same document can be split into groups of highlycohesive entities, we adopt the clique partitioning technique to find the biggest clique in terms of sizeand weight.
Given an undirected graph G(V,D) where V is the set of all nodes and D is the set ofall edges, Gs= (Vs, Ds) is a sub-graph of G where Vs?
V and Ds?
D. Gsis called completesub-graph or clique if and only if each node in Vshas a link in Dsto all other nodes in Vs.
The cliquepartitioning algorithm aims to find all possible complete sub-graphs Gsin an undirected graph G. Ourapproach iteratively finds the ?best?
clique, deletes all ?wrong?
candidate entities for textual mentions thatare disambiguated by the selected clique and converts the selected clique to a node in the graph to beused in the next iteration.
The details are shown in algorithm 1.
Figure 2 shows an exampler of the cliquepartitioning disambiguation algorithm given a graph of candidate entities for six NE textual mentions,?A?,?B?,?C?,?D?,?E?,?F?.
Candidate entities are coded with the lower case letter of the NE textual mentionplus an index subscript, e.g., ?a1?, ?a2?, ?a3?, etc.
Cliques are shown with bold links in different colours.As described in section 4, one of the properties of the disambiguation graph is that there are no linksbetween candidates of the same NE textual mention.
Because of this property, we can guarantee thatthere is no more than one candidate for each textual mention in any clique.Data: Undirected graph G(V,E) and for each node v ?
V an associated IConf scoreResult: Solution sub-graphwhile not all textual mentions are disambiguated do1- clique-List = find cliques(G);2- weight each clique by summing the IConf scores of all nodes in the clique;3- select the highest scoring clique and use its nodes as disambiguation candidates;4- remove all wrong candidates for any mention disambiguated in step 3;5- merge all nodes in the selected clique into one node with IConf score of thenew node = sum of the IConf scores of the merged nodes;endAlgorithm 1: Clique Disambiguation AlgorithmThis approach does not use an entity coherence weighting (e.g.
Jprob).
Rather it just uses the entity1548Figure 2: Example of Clique Partitioning Disambiguationlinks to find the cliques regardless the relation strength.
Because of the huge number of nodes, the cliquefinder algorithm is not fast.
To speed-up the disambiguation, we filtered the nodes with low confidencein the graph, keeping only the top confidence scored 50 NE candidates for each NE textual mention.6 Graph Ranking DisambiguationThe clique approach disambiguates different NE textual mentions iteratively, where in each iterationone or more NE mentions are disambiguated taking into account the disambiguated mentions from theprevious iteration.
The graph Ranking approach iteratively ranks all graph nodes depending on the links.So, all NE candidates of all NE textual mentions in the text are ranked together without ignoring any ofthem.
Hence, a selection algorithm is used to combine the initial confidence and the graph rank score,and select the most appropriate NE candidate.Graph Ranking: The links between different candidates in the graph represent real world relations.These relations are used to reliably boost relevant candidates.
In some setups, the weight of these linksare set to 1 and in some others they are set to the entities?
coherence score.
All nodes in the graphare ranked according to these relations using Page-Rank.
We adapted a version of the PR algorithmwith normalization term to rank the different NE candidates according to entity coherence as shown inequation 3, where N is the number of nodes in the graph, coh(ei) is the set of nodes that cohere withnode eiand W (ei, ej) is the weight of the edge between eiand ejnodes.
The original PR uses a directedgraph while our graph is an undirected graph; so all links are treated as bidirectional.PR(ei) =(1?
d)N+dF (ei)?ej?coh(ei)PR(ej)?W (ei, ej) (3)F (ei) =?ej?coh(ei)W (ei, ej) (4)The standard PR algorithm assumes the initial rank of all nodes is uniformly equal, while in ourapproach we used the initial confidence as an initial weight for the candidate nodes.
A problem withPage-Rank for our purposes is the dissipation of initial node weight (confidence) over all linked nodes.The final rank of a node is based solely on the importance of linked nodes and the initial confidence playsno further role.
In our case this is not appropriate, so the final rank for each mention is calculated after1549graph ranking, by combining the graph rank with the initial confidence score.
Let us refer to the graphrank of a candidate as PR(ei).
We used two different combination schemes Rsand Rmas described inequations 6 and 5.Rm(ei,j) = IConf(ei,j)?
PR(ei,j) (5) Rs(ei,j) = IConf(ei,j) + PR(ei,j) (6)Data: Eiis a candidate list of one NE textualmention miResult: The best disambiguation NE candidate e?giR1 = {(Rm(ei,j), ei,j) | ?ei,j?
Ei};R2 = {(Rs(ei,j), ei,j) | ?ei,j?
Ei};Sort R1 in descending order ;Sort R2 in descending order ;R1diff = R1[0]-R1[1];R2diff = R2[0]-R2[1];if R1diff > R2diff thenreturn highest rank scored entity of R1, (R1[0])elsereturn highest rank scored entity of R2, (R2[0])endAlgorithm 2: Selection AlgorithmDecision Making: Selecting the propercandidate is the final phase in the disam-biguation process.
The simplest approachis to select the highest ranked entity in thelist for each mention miaccording to equa-tion 7 or 8, which correpond to the rankcombining schemes expressed in equations5 and 6.
Experiments show that overallusing the Rmcombining scheme is betterthan the Rsscheme.
However, the high-est rank, after combining graph rank scoreand initial confidence score, is not alwayscorrect.
So we developed a dynamic selec-tion algorithm which uses both combina-tion schemes to pick the best disambigua-tion candidate.
We found that a dynamicchoice between the re-ranking schemes,based on the difference between the toptwo candidates, as described in Algorithm 2, works best.
The selected candidate entity is referred toas e?
with the superscript showing the selection scheme.e?mi= argmaxei,jRm(ei,j) (7) e?si= argmaxei,jRs(ei,j) (8)7 Experiments and Results7.1 DatasetNIST has released a dataset for use in the TAC KBP entity linking task (EL).
But, the task of named entitydisambiguation is different from entity linking task, as noted above in Section 2.
Also, the NIST datasetis not suitable for evaluating the collective NE disambiguation task because only one NE mention is an-notated and disambiguated per query document while we need all mentions of NEs in the document to beannotated and disambiguated to evaluate the performance of the collective named entity disambiguationtechnique.
Another dataset manually annotated for NED is reported in (Kulkarni et al., 2009), but it usesan old version of Wikipedia and it is quite small.
We have used another dataset, the AIDA dataset, whichis based on the CoNLL 2003 data for NER tagging and in which most tagged NE mentions have beenmanually disambiguated against Wikipedia (Hoffart et al., 2011).
This dataset contains 1393 documents,and 34,965 annotated mentions, where 7136 mention are not linked to Wikipedia3.We compare our results to Hoffart?s work ?
Accurate Online Disambiguation of Named Entities(AIDA).
For fair comparison, we only considered NE mentions with an entry in the Wikipedia KB,ignoring the 20% of query mentions without a link to the KB, as Hoffart did.7.2 Evaluation MetricWe use accuracy as the evaluation metric.
Micro-averaged accuracy is used as the official metric for thedisambiguation task and has been used in much previous and related work.
Micro-averaged accuracy3AIDA dataset is available on the web to download http://www.mpi-inf.mpg.de/yago-naga/aida/1550corresponds to the percentage of the correctly disambiguated textual mentions and it is calculated asshown in equation 9.Amicro=#correctly disambiguated mentionsNumber of NE Mentions(9)Macro-averaged accuracy is used to calculate the average percentage of correctly identified namedentities.
Macro-averaged accuracy is calculated as shown in equation 10.Amacro=?numiNum Correct(Ei)Num Queries(Ei)# of unique entities(10)7.3 ResultsIn addition to the state-of-the-art, we used two strong baselines to evaluate the performance of the pro-posed approaches.
The first baseline is a setup where the IConf scores only are used to disambiguatethe NE textual mention.
In this setup a ranking based on Entity Popularity (EP) does best, with micro-and macro-averaged accuracy scores of 80.55% and 78.09% respectively.
This high baseline is close tothe state-of-the-art.
A summary of the first baseline is shown in Table 1.
The second baseline is the basicPR algorithm, where both IConf scores and link weights are ignored.
Links between nodes are createdwherever any non-zero entity coherence relation, REF or JProb, is found.
Micro- and macro-averagedaccuracy scores of 70.60% and 60.91% respectively were obtained with this baseline.Baseline1 Cliques PRIe?gIConf AmicroAmacroAmicroAmacroAmicroAmacroAmicroAmacrocos 38.44 45.68 71.59 64.83 70.6 60.83 78.41 72.35jwSim 61.01 58.81 72.26 69.53 70.61 60.94 83.16 78.28ctxt 24.58 21.44 58.06 57.37 70.61 60.83 75.45 65.22EP 80.55 78.09 86.10 81.79 71.78 81.07 87.59 84.19Table 1: Results using different IConf scores with different approachesThe clique partitioning disambiguation algorithm experiments are setup so a link between nodes iscreated whenever a non-zero coherence relation is found between nodes regardless its weight.
We useddifferent settings for the candidates filter.
In the case where no candidates filter is applied, all nodes areconsidered to find the best initial clique.
So, bigger cliques with nodes that have lower confidence maybe selected in the first iteration.
This approach is very sensitive to the results of the first iteration.
Conse-quently, the accuracy goes down.
Also, because of the huge graph size, the clique partitioning algorithmtakes a long time.
At the other extreme, if we use only a small number of candidates with the highestconfidence scores, then the accuracy also goes down because in most cases the correct disambiguationentity is filtered out of the graph.
We used the highest 50 candidates in the graph and all other nodesare deleted.
Table 1 shows the results of using different initial confidence scores in clique partitioningdisambiguation.Graph ranking disambiguation experiments were setup in three different settings in order to evaluatethe contribution of different features like initial confidence and link weights.
For all setups, we useddifferent decision making approaches e?m, e?sand e?g.
The results when using e?gare better than e?mand e?sfor all setups.
So, we report the results of e?gonly.
Different setups are as follows:?
PRI: In this setup, the IConf scores are used to be the initial rank for Page-Rank while the linksbetween nodes are uniformly weighted to one.
As in the PR baseline, links are created whereverRef or Jprob are not zero.
Table 1 shows the results both without IConf combination, i.e.
usingonly the PR score for ranking, and after combining the initial confidence score using dynamicdecision making (indicated by e?g) When comparing these results to the PR baseline, we notice aslight positive effect of using the initial confidence as an initial rank instead of uniform ranking.
Themajor improvement comes by combining the initial confidence with the PR score.
All combining1551methods improve the results over the baseline results when using the the same confidence scorewhile the dynamic selection algorithm overcomes other basic methods, i.e.
e?mand e?s.?
PRC: In the second setup, entity coherence features are tested by setting the edge weights to thecoherence score and the initial node rank is set to be uniform when running the PR algorithm.
So,initial confidence scores are not considered in graph ranking but just considered in disambiguationdecision making.
This setting is intended to evaluate the contribution of different coherence rela-tions.
We compared Jprob and Ref edge weighting approaches, where for each approach edgeswere created only where the coherence score according to the approach was non-zero.
We also in-vestigated a variant, called Jprob+Ref , in which the Ref edge weights are normalized to sum to1 over the whole graph and then added to the JProb edge weights (here edges result wherever Jproband Ref scores are non-zero).
Results in Table 2 show the JProb feature seems to be more dis-criminative than the Ref feature but the combined Jprob+Ref feature performs better than eachseparately, just outperforming the baseline.
We used the best IConf score, i.e.
EP, for re-ranking.Again, combining the IConf with the PR score improves the results.?
PRIC: This setup uses different combinations of IConf and entity coherence scores in PR.
Table3 shows the accuracy when using different combinations of all entity coherence scores and someselected (i.e.
the best) IConf scores.
Here the Jprob + Ref combination does not add any valueover Jprob alone.
Interestingly using IConf score with differentially weighted edges does notshow any benefit over using IConf score and uniformly weighted edges (Table 1).PR e?gEdge Weight AmicroAmacroAmicroAmacroJprob 66.52 55.83 83.31 80.38Ref 67.48 59.76 81.80 78.53Jprob+ ref 72.69 65.71 83.46 80.69Table 2: Results using weighted edges (PRC)e?gIConf Edge Weight AmicroAmacrojwSim Jprob 82.56 76.16jwSim Ref 78.61 71.12jwSim Jprob+Ref 81.97 75.63EP Jprob 86.29 82.77EP Ref 83.16 80.01EP Jprob+Ref 86.10 82.80Table 3: Results using initial confidence andweighted edges (PRIC)To compare our results with the state-of-the-art, we report Hoffart et al.
?s (2011) results as they re-implemented two other systems and ran them over the AIDA dataset which we used to evaluate ourapproach.
We also compare with Alhelbawy and Gaizauskas (2013) and Shirakawa et al.
(2011) whocarried out their experiments using the same dataset.
Table 4 shows a comparison between the resultsof our proposed approaches and the state-of-the-art.
Both proposed approaches exceed the results of thestate-of-the-art.
However our approaches are very simple and direct to apply, unlike Hoffart et al.
?s andShirakawa et al.
?s which are considerably more complex.
Also, our approaches do not need any kind oftraining, unlike the Alhelbawy approach.7.4 DiscussionThe Page-Rank algorithm was originally designed for directed graphs while our coherence features areundirected.
So, the node rank depends on both incoming and outgoing links (when converting the undi-rected graph to a directed graph).
That explains the little improvement over basic PR when using theB1 B2 Cliques PRCPRIPRCICucerzan Kulkarni Hoffart Shirakawa AlhelbawyAmacro78.09 60.91 81.79 80.98 84.19 82.80 43.74 76.74 81.91 83.02 74.18Amicro80.55 70.60 86.11 83.59 87.59 86.10 51.03 72.87 81.82 82.29 78.49Table 4: Summary of Presented Approaches and State-of-the-art Results.
B1 and B2 are baselines.1552initial confidence as an initial rank before using PR (see Table 1).
However, when comparing PR resultsin Tables 2 and 1, we can see that the PR algorithm is more sensitive to the links than to initial ranks.The combined coherence approach (Jprob + Ref ) actually has a value other than the different weight-ing it supplies; the approach results in more edges than either of the combined approaches do alone.
Inall PR results wherever edge weights are applied, the result of using the combined coherence measuresoutperforms either of them singly.Informal failure analysis was carried out to determine reasons for disambiguation failure.
Reasonsidentified include:1.
The correct NE candidate does not exist in the graph.
In such cases the disambiguation approachselected is irrelevant and what is needed is improved candidate selection.2.
Lack of edges.
When there are no edges between any of the query NE mention candidate entitiesand other mentions?
candidates.
In this case the decision depends only on the IConf score.3.
Where the Freebase popularity score (EP) is used, whenever this score for the correct NE candidateis 0, which means the selection process is based on the PR score.Table 5 shows an example of the highest three NE candidates for three NE mentions taken froma document (overall the document contains textual mentions for ten different NEs).
The first one is?Ford?
and is disambiguated correctly to ?Ford Motor Company?, where the PR and popularity scoresare higher than any of the other candidates.
The second one is ,?Magna?, disambiguated correctly, wherethe first two NE candidates have the same PR score but the popularity score discriminates between them.The third, ?Markham?, is disambiguated to ?Clements Markham?
while it should be disambiguated to?Markham, Ontario?.
The problem in this case is that all NE candidates for the mention ?Markham?are not linked to any entity candidates for any other NE mentions in the document (problem 2 above).Therefore, the popularity score dominates the final rank score.NE Candidate PR score FB Rank our Rank?10?3?10?3?10?3FordFord Motor Company 21.37 62.12 1.32Ford Galaxie 4.59 10.94 0.05Ford GT 2.83 11.43 0.03MagnaMagna International 2.65 4.78 0.013Magna Powertrain 2.65 2.18 0.005Germania 0.83 3.46 0.003MarkhamClements Markham 0.83 4.42 0.004Markham Waxers 0.83 3.67 0.003Edwin Markham 0.83 2.89 0.002Table 5: Example show the first three NE candidates for three NE mentions with scores8 ConclusionOur results show that graph ranking and cliques partitioning approaches in conjunction with the candi-date confidence scores and entity coherence across a disambiguation graph can be used as an effectiveapproach to collectively disambiguate named entity textual mentions in a document.
Our proposed fea-tures are very simple and easy to extract, and work well when employed in PR or clique partitioningalgorithms.
Also, entity coherence is a discriminative feature when using graph models for NED.
Infuture work we plan to explore enriching the edges between nodes, by incorporating semantic relationsextracted from an ontology, and extending the scope of entity co-occurrence to be the document insteadof the sentence.
Also, it is worth investigating whether using the entity coherence score can help whenevaluating clique weight in the clique partitioning algorithm.1553ReferencesEneko Agirre and Aitor Soroa.
2008.
Using the multilingual central repository for graph-based word sensedisambiguation.
In LREC.Eneko Agirre and Aitor Soroa.
2009.
Personalizing pagerank for word sense disambiguation.
In Proceedings ofthe 12th Conference of the European Chapter of the Association for Computational Linguistics, pages 33?41.Association for Computational Linguistics.Ayman Alhelbawy and Rob Gaizauskas.
2012.
Named entity based document similarity with svm-based re-ranking for entity linking.
In Advanced Machine Learning Technologies and Applications, volume 322 ofCommunications in Computer and Information Science, pages 379?388.
Springer Berlin Heidelberg.Ayman Alhelbawy and Robert Gaizauskas.
2013.
Named entity disambiguation using hmms.
In Web Intelli-gence (WI) and Intelligent Agent Technologies (IAT), 2013 IEEE/WIC/ACM International Joint Conferences on,volume 3, pages 159?162.Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor.
2008.
Freebase: A collaborativelycreated graph database for structuring human knowledge.
In Proceedings of the 2008 ACM SIGMOD Interna-tional Conference on Management of Data, SIGMOD ?08, pages 1247?1250, New York, NY, USA.
ACM.Coen Bron and Joep Kerbosch.
1973.
Algorithm 457: finding all cliques of an undirected graph.
Communicationsof the ACM, 16(9):575?577.Razvan C. Bunescu and Marius Pasca.
2006.
Using encyclopedic knowledge for named entity disambiguation.
InEACL.
The Association for Computer Linguistics.S.
Cucerzan.
2007.
Large-scale named entity disambiguation based on wikipedia data.
In Proceedings of EMNLP-CoNLL, volume 6, pages 708?716.M.
Dredze, P. McNamee, D. Rao, A. Gerber, and T. Finin.
2010.
Entity disambiguation for knowledge basepopulation.
In Proceedings of the 23rd International Conference on Computational Linguistics, pages 277?285.
Association for Computational Linguistics.S.
Gottipati and J. Jiang.
2011.
Linking entities to a knowledge base with query expansion.
In Proceedingsof the Conference on Empirical Methods in Natural Language Processing, pages 804?813.
Association forComputational Linguistics.Yoan Guti?errez, Sonia V?azquez, and Andr?es Montoyo.
2011.
Word sense disambiguation: a graph-based approachusing n-cliques partitioning technique.
In Natural Language Processing and Information Systems, pages 112?124.
Springer.Yoan Guti?errez, Sonia V?azquez, and Andr?es Montoyo.
2012.
A graph-based approach to wsd using relevantsemantic trees and n-cliques model.
In Computational Linguistics and Intelligent Text Processing, pages 225?237.
Springer.X.
Han, L. Sun, and J. Zhao.
2011.
Collective entity linking in web text: a graph-based method.
In Proceedingsof the 34th international ACM SIGIR conference on Research and development in Information Retrieval, pages765?774.
ACM.J.
Hoffart, M.A.
Yosef, I. Bordino, H. F?urstenau, M. Pinkal, M. Spaniol, B. Taneva, S. Thater, and G. Weikum.2011.
Robust disambiguation of named entities in text.
In Proceedings of the Conference on Empirical Methodsin Natural Language Processing, pages 782?792.
Association for Computational Linguistics.H.
Ji and R. Grishman.
2011.
Knowledge base population: successful approaches and challenges.
In Proceedingsof the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 1148?1158.
Association for Computational Linguistics.S.
Kulkarni, A. Singh, G. Ramakrishnan, and S. Chakrabarti.
2009.
Collective annotation of wikipedia entities inweb text.
In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and datamining, pages 457?466.
ACM.R.Duncan Luce and AlbertD.
Perry.
1949.
A method of matrix analysis of group structure.
Psychometrika,14(2):95?116.P.
McNamee and H.T.
Dang.
2009.
Overview of the tac 2009 knowledge base population track.
In Text AnalysisConference (TAC).1554D.
Milne and I.H.
Witten.
2008.
Learning to link with wikipedia.
In Proceeding of the 17th ACM conference onInformation and knowledge management, pages 509?518.
ACM.D.
Nadeau and S. Sekine.
2007.
A survey of named entity recognition and classification.
Lingvisticae Investiga-tiones, 30(1):3?26.Kamel Nebhi.
2013.
Named entity disambiguation using freebase and syntactic parsing.
In CEUR-WS.org, editor,Proceedings of the First International Workshop on Linked Data for Information Extraction (LD4IE 2013) co-located with the 12th International Semantic Web Conference (ISWC 2013).
Gentile, A.L.
; Zhang, Z. ; d?Amato,C.
& Paulheim, H.Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd.
1999.
The pagerank citation ranking: Bring-ing order to the web.
Technical Report 1999-66, November.L.
Ratinov, D. Roth, D. Downey, and M. Anderson.
2011.
Local and global algorithms for disambiguation towikipedia.
In Proceedings of the Annual Meeting of the Association of Computational Linguistics (ACL).P.
Sen. 2012.
Collective context-aware topic models for entity disambiguation.
In Proceedings of the 21st inter-national conference on World Wide Web, pages 729?738.
ACM.Masumi Shirakawa, Haixun Wang, Yangqiu Song, Zhongyuan Wang, Kotaro Nakayama, Takahiro Hara, and Sho-jiro Nishio.
2011.
Entity disambiguation based on a.
Technical report, Technical report, Technical ReportMSR-TR-2011-125, Microsoft Research.Ravi Sinha and Rada Mihalcea.
2007.
Unsupervised graph-basedword sense disambiguation using measuresof word semantic similarity.
In Semantic Computing, 2007.
ICSC 2007. International Conference on, pages363?369.
IEEE.Wenpu Xing and Ali Ghorbani.
2004.
Weighted pagerank algorithm.
In Communication Networks and ServicesResearch, 2004.
Proceedings.
Second Annual Conference on, pages 305?314.
IEEE.Wei Zhang, Yan Chuan Sim, Jian Su, and Chew Lim Tan.
2011.
Entity linking with effective acronym expansion,instance selection and topic modeling.
In Proceedings of the Twenty-Second international joint conference onArtificial Intelligence-Volume Volume Three, pages 1909?1914.
AAAI Press.Z.
Zheng, F. Li, M. Huang, and X. Zhu.
2010.
Learning to link entities with knowledge base.
In HumanLanguage Technologies: The 2010 Annual Conference of the North American Chapter of the Association forComputational Linguistics, HLT ?10, pages 483?491.
Association for Computational Linguistics.1555
