Coling 2010: Poster Volume, pages 1354?1362,Beijing, August 2010Boosting Relation Extraction with Limited Closed-World KnowledgeFeiyu Xu Hans Uszkoreit Sebastian Krause Hong LiLanguage Technology LabGerman Research Center for Artificial Intelligence (DFKI GmbH){feiyu,uszkoreit,sebastian.krause,lihong}@dfki.deAbstractThis paper presents a new approach to im-proving relation extraction based on min-imally supervised learning.
By addingsome limited closed-world knowledge forconfidence estimation of learned rules tothe usual seed data, the precision of re-lation extraction can be considerably im-proved.
Starting from an existing base-line system we demonstrate that utilizinglimited closed world knowledge can ef-fectively eliminate ?dangerous?
or plainlywrong rules during the bootstrapping pro-cess.
The new method improves the re-liability of the confidence estimation andthe precision value of the extracted in-stances.
Although recall suffers to a cer-tain degree depending on the domain andthe selected settings, the overall perfor-mance measured by F-score considerablyimproves.
Finally we validate the adapt-ability of the best ranking method to a newdomain and obtain promising results.1 IntroductionMinimally supervised machine-learning ap-proaches to learning rules or patterns for relationextraction (RE) in a bootstrapping framework areregarded as very effective methods for buildinginformation extraction (IE) systems and foradapting them to new domains (e. g., (Riloff,1996), (Brin, 1998), (Agichtein and Gravano,2000), (Yangarber, 2001), (Sudo et al, 2003),(Jones, 2005), (Greenwood and Stevenson,2006), (Agichtein, 2006), (Xu et al, 2007),(Xu, 2007)).
On the one hand, these approachesshow very promising results by utilizing minimaldomain knowledge as seeds.
On the other hand,they are all confronted with the same problem,i.e., the acquisition of wrong rules because ofmissing knowledge for their validation duringbootstrapping.
Various approaches to confidenceestimation of learned rules have been proposedas well as methods for identifying ?so-called?negative rules for increasing the precision value(e.g., (Brin, 1998), (Agichtein and Gravano,2000), (Agichtein, 2006), (Yangarber, 2003),(Pantel and Pennacchiotti, 2006), (Etzioni et al,2005), (Xu et al, 2007) and (Uszkoreit et al,2009)).In this paper, we present a new approach to esti-mating or ranking the confidence value of learnedrules by utilizing limited closed-world knowl-edge.
As many predecessors, our ranking methodis built on the ?Duality Principle?
(e. g., (Brin,1998), (Yangarber, 2001) and (Agichtein, 2006)).We extend the validation method by an evalu-ation of extracted instances against some lim-ited closed-world knowledge, while also allowingcases in which knowledge for informed decisionsis not available.
In comparison to previous ap-proaches to negative examples or negative rulessuch as (Yangarber, 2003), (Etzioni et al, 2005)and (Uszkoreit et al, 2009), we implicitly gener-ate many negative examples by utilizing the pos-itive examples in the closed-world portion of ourknowledge.
Rules extracting wrong instances arelowered in rank.In (Xu et al, 2007) and (Xu, 2007), we developa generic framework for learning rules for rela-tions of varying complexity, called DARE (Do-main Adaptive Relation Extraction).
Furthermore,there is a systematic error analysis of the base-1354line system conducted in (Xu, 2007).
We employour system both as a baseline reference and as aplatform for implementing and evaluating our newmethod.Our first experiments conducted on the samedata used in (Xu et al, 2007) demonstrate: 1) lim-ited closed-world knowledge is very useful and ef-fective for improving rule confidence estimationand precision of relation extraction; 2) integrationof soft constraints boosts the confidence value ofthe good and relevant rules, but without stronglydecreasing the recall value.
In addition, we val-idate our method on a new corpus of newspapertexts about celebrities and obtain promising re-sults.The remainder of the paper is organized as fol-lows: Section 2 explains the relevant related work.Sections 3 and 4 describe DARE and our exten-sions.
Section 5 reports the experiments withtwo ranking strategies and their results.
Section6 gives a summary and discusses future work.2 Related WorkIn the existing minimally supervised rule learningsystems for relation extraction based on bootstrap-ping, they already employ various approaches toconfidence estimation of learned rules and differ-ent methods for identification of so-called nega-tive rules.
For estimation of confidence/relevancevalues of rules, most of the approaches followthe so-called ?Duality Principle?
as mentioned byBrin (1998) and Yangarber (2001), namely, theconfidence value of learned rules is dependenton the confidence value of their origins, whichcan be documents or relation instances.
For ex-ample, Riloff (1996), Yangarber (2001), Sudo etal.
(2003) and Greenwood and Stevenson (2006)use domain relevance of documents in which pat-terns are discovered as well as the distribution fre-quency of these patterns in those relevant docu-ments as an indication of good patterns.
Theirmethods are aimed at detecting all patterns fora specific domain, but those patterns cannot beapplied directly to a specific relation.
In con-trast, systems presented by Brin (1998), Agichteinand Gravano (2000), Agichtein (2006), Panteland Pennacchiotti (2006) as well as our base-line system (Xu et al, 2007) are designed tolearn rules for a specific relation.
They start withsome relation instances as their so-called ?seman-tic seeds?
and detect rules from texts matchingwith these instances.
The new rules are appliedto new texts for extracting new instances.
Thesenew instances in turn are utilized as new seeds.All these systems calculate their rule confidencebased on the confidence values of the instancesfrom which they stem.
In addition to the confi-dence value of the seed instances, most of themalso consider frequency information and includesome heuristics for extra validation.
For exam-ple, Agichtein (2006) intellectually defines certainconstraints for evaluating the truth value of ex-tracted instances.
But it is not clear whether thisstrategy can be adapted to new domains and otherrelations.
In (Xu et al, 2007) we make use of do-main relevance values of terms occurring in rules.This method is not applicable to general relations.Parallel to confidence estimation strategies, thelearning of negative rules is useful for identifyingwrong rules straightforwardly.
Yangarber (2003)and Etzioni et al (2005) utilize the so-calledCounter-Training for detecting negative rules fora specific domain or a specific class by learningfrom multiple domains or classes at the same time.Examples of one certain domain or class are re-garded as negative examples for the other ones.Bunescu and Mooney (2007) follow a classifi-cation-based approach to RE.
They use positiveand negative sentences of a target relation for aSVM classifier.
Uszkoreit et al (2009) exploitnegative examples as seeds for learning furthernegative instances and negative rules.
The dis-advantage of the above four approaches is thatthe selected negative domains or classes or neg-ative instances cover only a subset of the neg-ative domains/classes/relations of the target do-main/class/relation.3 DARE Baseline SystemOur baseline system DARE is a minimally super-vised learning system for relation extraction, ini-tialized by so-called ?semantic seeds?, i.e., exam-ples of the target relations, labelled with their se-mantic roles.
The system supports domain adap-tation through a compositional rule representationand a bottom-up rule discovery strategy.
In this1355way, DARE can handle target relations of varyingarity.
The following example is a relation instanceof the target relation from (Xu, 2007) concerningNobel Prize awards: <Mohamed ElBaradei, No-bel, Peace, 2005>.
The target relation containsfour arguments: WINNER, PRIZE NAME, PRIZE AREAand YEAR.
This example refers to an event men-tioned in the sentence in example (1).
(1) Mohamed ElBaradei, won the 2005 NobelPrize for Peace on Friday because of ....Figure 1 is a simplified dependency tree of ex-ample (1).
DARE utilizes a bottom-up rule dis-covery strategy to extract rules from such depen-dency trees.
All sentences are processed withnamed entity recognition and dependency parsing.
?win?subjectwwnnnnn object''PPPPPWinner ?Prize?lex-modssggggggggggggglex-mod  mod ''OOOOOYear Prize ?for?pcomp-n AreaFigure 1: Dependency tree for example (1)From the tree in Figure 1, DARE learns threerules.
The first rule is dominated by the prepo-sition ?for?, extracting the argument PRIZE AREA(Area).
The second rule is dominated by the noun?Prize?, extracting the arguments YEAR (Year) andPRIZE NAME (Prize), and calling the first rule forthe argument PRIZE AREA (Area).
The rule ?win-ner prize area year 1?
from Figure 2 extracts allfour arguments from the verb phrase dominatedby the verb ?win?
and calls the second rule tohandle the arguments embedded in the linguisticargument ?object?.Rule name :: winner prize area year 1Rule body ::?????????????head?
?pos verbmode activelex-form ?win??
?daughters <[subject[head 1 Winner]],???object?
?rule year prize area 1 ::< 4 Year, 2 Prize,3 Area >?????>????????????
?Output :: < 1 Winner, 2 Prize, 3 Area, 4 Year >Figure 2: DARE extraction rule.We conduct a systematic error analysis basedon our experiments with the Nobel Prize awarddata (Xu, 2007).
The learned rules are dividedinto four groups: good, useless, dangerous andbad.
The good rules are rules that only extract cor-rect instances, while bad ones exclusively producewrong instances.
Useless rules are those that donot detect any new instances.
Dangerous rules aredangerous because they extract both correct andwrong instances.
Most good rules are rules withhigh specificity, namely, extracting all or most ar-guments of the target relation.
The 14.7% extrac-tion errors are from bad rules and dangerous rules.Other errors are caused by wrong reported con-tent, negative modality, parsing and named entityrecognition errors.4 Our Approach: Boosting Relation Ex-traction4.1 Closed-World Knowledge: Modeling andConstructionThe error analysis of DARE confirms that theidentification of bad rules or dangerous rules isimportant for the precision of an extraction sys-tem.
Using closed-world knowledge with largenumbers of implicit negative instances opens apossibility to detect such rules directly.
In ourwork, closed-world knowledge for a target rela-tion is the total set of positive relation instancesfor entire relations or for some selected subsetsof individuals.
For most real world applications,closed-world knowledge can only be obtained forrelatively small subsets of individuals participat-ing in the relevant relations.
We store the closed-world knowledge in a relational database, whichwe dub ?closed-world knowledge database?
(abbr.cwDB).
Thus, a cwDB for a target relation shouldfill the following condition:A cwDB must contain all correct relationinstances (insts) for an instantiation value(argValue) of a selected relation argumentcwArg in the target relation.Given R (the total set of relation instances of atarget relation), a cwDB is defined as follows:cwDB={inst ?
R : cwArg(inst) = argValue}.An example of a cwDB is the set of all prize win-ners of a specific prize area such as Peace, wherePRIZE AREA is the selected cwArg and argValue isPeace.
Note that the merger of two cwDBs, forexample with PRIZE AREAs Peace and Literature,is again a cwDB (with two argValues in this case).13564.2 Modified Learning AlgorithmIn Algorithm 1, we present the modification of theDARE algorithm (Xu, 2007).
The basic idea ofDARE is that it takes some initial seeds as inputand learns relation extraction rules from sentencesin the textual corpus matching the seeds.
Giventhe learned rules, it extracts new instances fromthe texts.
The modified algorithm adds the val-idate step to evaluate the new instances againstthe closed-world knowledge cwDB.
Based on theevaluation result, both new instances and learnedrules are ranked with a confidence value.INPUT: initial seeds1 i?
0 (iteration of bootstrapping)2 seeds ?
initial seeds3 all instances ?
{}4 while (seeds 6= {})5 rulesi ?
getRules(seeds)6 instancesi ?
getInstances(rulesi)7 new instancesi ?
instancesi ?
all instances8 validate(new instances i , cwDB)9 rank(new instancesi)10 rank(rulesi)11 seeds ?
new instancesi12 all instances ?
all instances + new instancesi13 i?
i+ 1OUTPUT: all instancesAlgorithm 1: Extended DARE4.3 Validation against cwDBGiven a cwDB of a target relation and its argValueof its selected argument cwArg, the validation ofan extracted instance (inst) against the cwDB isdefined as follows.inst correct ?
inst ?
cwDB (1)inst wrong ?
inst 6?
cwDB ?cwArg(inst) = argValueinst unknown ?
( inst 6?
cwDB ?cwArg(inst) 6= argValue )?
( inst 6?
cwDB ?cwArg(inst) is unspecified )4.4 Rule Confidence Ranking with cwDBWe develop two rule-ranking strategies for con-fidence estimation, in order to investigate thebest way of integrating the closed-world knowl-edge: (a) exclusive ranking: This ranking strat-egy excludes every rule which extracts wrong in-stances after their validation against the closed-world knowledge; (b) soft ranking: This rankingstrategy is built on top of the duality principle andtakes specificity and the depth of learning into ac-count.Exclusive Ranking The exclusive rankingmethod is a very naive ranking method whichestimates the confidence value of a learned rule(e.g., rule) depending on the truth value of itsextracted instances (getInstances(rule)) againsta cwDB.
Any rule with one wrong extractionis regarded as a bad rule in this method.
Thismethod works effectively in a special scenariowhere the total list of the instances of the targetrelation is available as the cwDB.confidence(rule) ={1 if getInstances(rule) ?
cwDB,0 otherwise.
(2)Soft Ranking The soft ranking method worksin the spirit of the ?Duality Principle?, the con-fidence value of rules is dependent on the truthvalue of their extracted instances and on the seedinstances from which they stem.
The confi-dence value of the extracted instances is estimatedbased on their validation against the cwDB or theconfidence value of their ancestor seed instancesfrom which their extraction rules stem.
Further-more, the specificity of the instances (percentageof the filled arguments) and the learning depth(iteration step of bootstrapping) are parameterstoo.
The definition of instance scoring, namely,score(inst), is given as follows:score(inst) =????
> 0 if validate(inst , cwDB) = correct,0 if validate(inst , cwDB) = wrong,UN inst if validate(inst , cwDB) = unknown.
(3)As defined above, if a new instance is con-firmed as correct by the cwDB, it will obtain apositive value.
In our experiment, we set ?=10in order to boost the precision.
In the case of un-known about its truth value, the confidence valueof a new instance (inst) is dependent on the confi-dence values of the seed instances (ancestor seeds)from which its mother rules (Rinst ) stem.
Below,the scoring of the unknown case, namely, UN inst ,is defined, where Rinst are rules that extract thenew instance inst , while Irule are instances fromwhich a rule inRinst is learned and ?
is the speci-ficity value of inst while ?
is utilized to expressthe noisy potential of each further iteration duringbootstrapping.1357UN inst =?rule?Rinst(?j?Irule score(j)|Irule | ?
?irule)|Rinst |?
?whereRinst = getMotherRulesOf(inst),Irule = getMotherInstancesOf(rule),?
= specificity,?
= 0.8,irule = i-th iteration where rule occurs(4)Given the scoring of instance inst , the confidenceestimation of a rule is the average score of allinsts extracted by this rule:confidence(rule) =?inst?I score(inst)|I|where I = getInstances(rule) (5)5 Experiments5.1 Corpora and Closed-World KnowledgeWe conduct our experiments with two differentdomains.
We start with the Nobel Prize award do-main reported in (Xu, 2007) and apply our methodto the same corpus, a collection from various on-line newspapers.
The target relation is the onewith the four arguments as mentioned in Sec-tion 3.
In this way, we can compare our resultswith those reported in (Xu, 2007).
Furthermore,all Nobel Prize winners can be found from http://nobelprize.org, so it is easy to constructa cwDB for Nobel Prize winners.
We take thePRIZE AREA as our selected argument for closingsub-relations and construct various cwDBs withthe instantiation of this argument (e.g., all win-ners of Nobel Peace Prize).
The second domainis about celebrities.
Our text corpus is collectedfrom tabloid newspaper texts, containing 6850 ar-ticles from the years 2001 and 2002.
The targetrelation is the marriage relationship between twopersons.
We construct a cwDB of 289 persons inwhich we have listed all their (ex-)spouses as wellas the time span of the marriage relation.Table 1 summarizes the size of the corpus dataof the two domains.Domain Space #Doc.Nobel Prize 18,4 MB 3328Celebrity Marr.
16,6 MB 6850Table 1: Corpus data.5.2 Nobel Prize DomainWe apply the extended DARE system to the NobelPrize corpus at first and conduct two rule rank-ing strategies with different sizes of the cwDB.We conduct all our experiments with the seed<Guenter Grass, Nobel, Literature, 1999>.
TheDARE-Baseline performance is shown in Table 2.Precision Absolute RecallBaseline 77.98% 89.01%Table 2: DARE-Baseline PerformanceExclusive RankingGiven the complete list of Nobel Laureates, wecan apply the exclusive ranking strategy to this do-main.
Our cwDB is the total list of Nobel Prizewinners.
The wrong instances will not be used asseed for the next iteration.
Rules that extractedat least one wrong instance are marked as bad, theother rules as good.
We utilize only the good rulesfor relation extraction.Prec.
Rel.
Recall Rel.
F-Measure100.00% 82.88% 90.64%Table 3: Performance of Exclusive Ranking inNobel Prize award domain.In comparison to the DARE baseline system,given the same seed setup, this experiment resultsin a precision boost from 77.98% to 100% (seeTable 3).
This is not surprising since the cwDBcovers all relation instances for the target rela-tion.
Nevertheless, this experiment shows that theclosed-world knowledge approach is effective toexclude bad rules.
However, the recall decreasesand is only 82.88% of the one of the baseline sys-tem.
As we explain above, not all rules extractingwrong instances are bad rules because wrong ex-tractions can also be caused by other error sourcessuch as named entity recognition.
Therefore, evengood rules can be excluded because of other er-ror sources.
The exclusive ranking strategy is use-ful for application scenarios where people want tolearn rules for achieving 100% precision perfor-mance and do not expect high recall.
It is espe-cially effective when a big cwDB is available.Soft RankingThis ranking strategy does not exclude anyrules and assigns a score to each rule based on1358the definition in Section 4.4.
Rules which extractcorrect instances, more specific relation instancesand stem from high-scored seed instances obtaina better value than others.
In our approach, thespecificity is dependent on the number of the ar-guments in the extracted instances.
For this do-main, the most specific instances contain all fourarguments.
In the following, we conduct two ex-periments with two different sizes of the cwDB:1) with the total list of winners (complete cwDB)and 2) with only winners in one PRIZE AREA (lim-ited cwDB).1) Complete closed-world database Figure 3displays the correlation between the score of rulesand their extraction precision performance.
Eachpoint stands for a set of rules with the samescore and extraction precision.
In this setup, thehigher the score, the higher the precision.
Giventhe scored rules, Figure 4 depicts precision, re-call and F-Measure for different score thresholds.For a given threshold j we take all rules withscore(rule) ?
j and use the instances they ex-tract.
The recall value here is the relative recallw.
r. t. to the DARE baseline performance: i. e. thenumber of correct extracted instances divided bythe number of correct instances extracted by theDARE baseline system.
The F-Measure value iscalculated by using the relative recall values, wetherefore refer to it as the relative F-Measure.
Ifthe system takes all rules with score ?
7, the sys-tem achieves the best relative F-Measure.0 1 2 3 4 5 6 7 8 9 100,00%10,00%20,00%30,00%40,00%50,00%60,00%70,00%80,00%90,00%100,00%Rule-ScoreCorrectness of extracted instancesFigure 3: Rule scores vs. precisions with thecomplete closed-world database.2) Limited closed-world database This experi-ment investigates the system performance in casesin which only a limited cwDB is available.
This isthe typical situation for most real world RE appli-cations.
Therefore, this experiment is much more0 012 3 312 4 412 5 512 6 612 2 212 7 712 8 812 9 912 , ,12 3020%00R22%00R70%00R72%00R80%00R82%00R90%00R92%00R,0%00R,2%00R300%00Rule-ScSorCtrcnsr-ec  efsnSxeC e-sff  efsnSxeCadiesc?le?
?lec?of?C?olC ?fed?-oleFigure 4: Performance with the complete closed-world database.important than the previous one.
We constructa smaller database containing only Peace NobelPrize winners, which is about 1/8 of the previouscomplete cwDB.0 1 2 3 4 5 6 7 8 9 100,00%10,00%20,00%30,00%40,00%50,00%60,00%70,00%80,00%90,00%100,00%Rule-ScoreCorrectness of extracted instancesFigure 5: Rule score vs. precision with the lim-ited closed-world database0 012 3 312 4 412 5 512 6 612 2 212 7 712 8 812 9 912 , ,12 300%00R30%00R40%00R50%00R60%00R20%00R70%00R80%00R90%00R,0%00R300%00Rule-ScSorCtrcnsr-ec  efsnSxeC e-sff  efsnSxeCadiesc?le?
?lec?of?C?olC ?fed?-oleFigure 6: Performance with the limited closed-world databaseFigure 5 shows the correlation between thescore of the rules and their extraction precision.Although the development curve here is not assmooth as depicted in Figure 3, the higher scoredrules have better precision values than most of thelower scored rules.
However, we can observe thatsome very good rules are scored low, located in1359Thresh.
Good Dangerous BadBaseline 58.94% 26.49% 14.57%1 64.96% 29.20% 5.84%2 66.67% 27.91% 5.43%3 69.23% 26.50% 4.27%4 73.27% 23.76% 2.97%5 76.00% 22.67% 1.33%6 77.59% 20.69% 1.72%7 77.50% 22.50% 0.00%8 87.50% 12.50% 0.00%9 85.71% 14.29% 0.00%10 90.00% 10.00% 0.00%Table 4: Quality analysis of rules with the limitedclosed-world databasethe left upper corner.
The reason is that many oftheir extracted instances are unknown, even if theirextracted instances are mostly correct.As shown in Figure 6, even with the limitedcwDB, the precision values are comparable withthe complete cwDB (see Figure 4).
However, therecall value drops much earlier than with the com-plete cwDB.
With a threshold of score 4, the sys-tem achieves the best modified F-Measure 92,21%with an improvement of precision of about 11 per-centage points compared to the DARE baselinesystem (89.39% vs. 77.98%).
These results showthat even with a limited cwDB this ranking systemcan help to improve the precision without loosingtoo much recall.We take a closer look on the useful (actively ex-tracting) rules and their extraction performance,using the same rule classification as (Xu, 2007).As shown in Table 4, more than one fourth ofthe extraction rules created by the baseline systemare dangerous ones and almost 15% are plainlywrong.
Applying the rule scoring with the limitedcwDB increases the fraction of good rules to al-most three quarters and nearly eliminates all badrules at threshold 4.
By choosing higher thresh-olds, surviving good rules raises to 90%.
The totalremaining set of rules then only consists of rulesthat at least partially extract correct instances.5.3 Celebrity DomainAs presented above, the soft ranking method de-livers very promising result.
In order to val-idate this ranking method, we choose an ad-ditional domain and decide to learn marriagerelations among celebrities, where the targetrelation consists of the following arguments:[ NAME OF SPOUSE, NAME OF SPOUSE, YEAR].The value of the marriage year is valid whenthe year is within the marriage time interval.
Themotivation of selecting this target relation is thelarge number of possible relations between twopersons leading to dangerous or even bad rules.For example, the rule in Figure 7 is a very dan-gerous rule because ?meeting?
events of two mar-ried celebrities are often reported.
A good confi-dence estimation method is very useful for boost-ing the good rules like the one in Figure 8.
Fromour text corpus we extract 37.000 sentences thatmention at least two persons.
The cwDB con-sists of sample relation instances, in which oneNAME OF SPOUSE is instantiated, i. e. we manu-ally construct a database which contains all (ex-)spouses of 289 celebrities.head([SPOUSE<ne_person>]),mod({head(("meet", VB)),subj({head([SPOUSE<ne_person>])})})Figure 7: A dangerous extraction rule examplehead(("marry", VB)),aux({head(("be", VB))}),dep({head([SPOUSE<ne_person>]),dep({head([DATE<point>])})}),nsubj({head([SPOUSE<ne_person>])})Figure 8: Example of a positive ruleSince a gold standard of mentions for this cor-pus is not available, we manually validate 100 ran-dom samples from each threshold group.
Thisevaluation gives us an opportunity to estimate theeffect of a cwDB in this domain.
Table 5 presentsthe performance of the rules with different thresh-olds.
The precision value of the baseline systemis very low.
Threshold 3 slightly improves theprecision of the DARE baseline without damag-ing recall too much.
Step 4 excludes dangerousrules such as the one in Figure 7 which drasticallyboosts the precision.
Unfortunately, the exclusionof such general rules leads to the loss of many cor-rect relation instances too, therefore, the immensedrop of recall from threshold 3 to 4 as well as fromthreshold 4 to 5.
Positive extraction rules such asFigure 8 are quite highly scored.
Because of thelarge number of rules and instances, we start thequality analysis of rules with score 3.
As the tableindicates, the use of the rule scoring in this domainclearly improves the quality of the created extrac-tion rules.
The error analysis shows that the ma-jor error resource for this domain is wrong coref-erence resolution or identity resolution.
For ex-1360Thresh.
# Instances Prec.
Rel.
Rec.
Rel.
F-Meas.
# Rules Good Dangerous BadBaseline 25183 9.00% 100.00% 16.51% 122581 19806 7.00% 61.17% 12.56% 5622 14542 9.00% 57.75% 15.57% 1593 11259 15.00% 74.51% 24.97% 121 19.83% 33.88% 46.28%4 788 65.00% 22.60% 33.54% 72 25.00% 27.78% 47.22%5 195 67.00% 5.76% 10.62% 29 37.93% 17.24% 44.83%6 115 84.00% 4.26% 8.11% 11 45.45% 27.27% 27.27%7 55 89.09% 2.16% 4.22% 6 50.00% 33.33% 16.67%8 9 77.78% 0.31% 0.62% 4 75.00% 0.00% 25.00%9 5 60.00% 0.13% 0.26% 3 66.67% 0.00% 33.33%10 5 60.00% 0.13% 0.26% 3 66.67% 0.00% 33.33%Table 5: Soft ranking for the celebrity marriage domain with a limited cwDB.ample, the inability to distinguish Prince Charles(former husband of British princess Diana) fromCharles Spencer (her brother) is the reason thatDARE crosses the border between the marriageand the sibling relation.
In comparison to theNobel Prize award event, the marriage relationbetween persons is often used as additional in-formation to a person which is involved in a re-ported event.
Therefore, anaphoric references oc-cur more often in their mentionings, as the exam-ple relation in (3).
(3) ?My kids, I really don?t like them towatch that much television,?
said :::::Cruise, 40, whoadopted Isabella and Connor while ::he was mar-ried to second wife Nicole Kidman.6 SummaryWe propose a new way in which prior knowledgeabout domains can be efficiently used as addi-tional criteria for confidence estimation of learnednew rules or new instances in a minimally su-pervised machine learning framework.
By intro-ducing rule scoring on the basis of available do-main knowledge (the cwDB), rules can be eval-uated during the bootstrapping process with re-spect to their extraction precision.
The resultsare rather promising.
The rule score threshold isan easy way for users of an extraction system toadjust the precision-recall-trade-off to their ownneeds.
The rule estimation method is also generalenough to extend to integration of common senseknowledge.
Although the relation instances inthe closed-world knowledge database can also beused as seed in the beginning, the core idea of ourresearch work is to develop a general confidenceestimation strategy for discovered new informa-tion.
As discussed in (Xu, 2007) and (Uszkoreitet al, 2009), the size of seed is not always rele-vant for the learning and extraction performance,in particular if the data corpus exhibits the smallworld property.
Using all instances in the cwDBas seed, our experiments with the baseline systemyield worse precision performance than the modi-fied DARE algorithm with only one seed instance.This approach is quite general and easily adapt-able to many domains; the only prerequisite isthe existence of a database with relation instancesfrom the target domain with a fulfilled closed-world property on some relational argument.
Adatabase of this kind should be easily obtainablefor many domains, e. g. by exploiting structuredand semi-structured information sources in the In-ternet, such as YAGO (Suchanek et al (2007)) andDBpedia (Bizer et al (2009)).
Furthermore, insome areas, such as Business Intelligence, thereis nearly complete knowledge already present forpast years, while the task is to extract informa-tion only from recent news articles.
Construct-ing closed-worlds out of the present knowledge toimprove the learning of new information is there-fore a straightforward approach.
Even the manualcollection of suitable data might be a reasonablechoice since appropriate closed worlds could berather small if cwDBis chosen properly.AcknowledgmentsThe work presented here has been partially sup-ported through the prject KomParse by the ProFITprogram of the Federal State of Berlin which inturn is co-funded by the EFRE program of theEuropean Union.
It is additionally supportedthrough a grant to the project TAKE, funded bythe German Ministry for Education and Research(BMBF, FKZ: 01IW08003).1361ReferencesAgichtein, Eugene and Luis Gravano.
2000.
Snow-ball: extracting relations from large plain-text col-lections.
In DL ?00: Proceedings of the fifth ACMconference on Digital libraries, pages 85?94, NewYork, NY, USA.
ACM.Agichtein, Eugene.
2006.
Confidence estimationmethods for partially supervised information extrac-tion.
In Proceedings of the Sixth SIAM InternationalConference on Data Mining, Bethesda, MD, USA,April.
SIAM.Bizer, Christian, Jens Lehmann, Georgi Kobilarov,So?ren Auer, Christian Becker, Richard Cyganiak,and Sebastian Hellmann.
2009.
DBpedia - a crys-tallization point for the web of data.
Journal of WebSemantics, 7(3):154?165.Brin, Sergey.
1998.
Extracting patterns and rela-tions from the world wide web.
In WebDB Work-shop at 6th International Conference on ExtendingDatabase Technology, EDBT?98.Bunescu, Razvan C. and Raymond J. Mooney.
2007.Learning to extract relations from the web usingminimal supervision.
In Proceedings of the 45thAnnual Meeting of the Association for Computa-tional Linguistics.Etzioni, Oren, Michael Cafarella, Doug Downey, Ana-Maria Popescu, Tal Shaked, Stephen Soderland,Daniel S. Weld, and Alexander Yates.
2005.
Un-supervised named-entity extraction from the web:An experimental study.
Artificial Intelligence,165(1):91 ?
134.Greenwood, Mark A. and Mark Stevenson.
2006.
Im-proving semi-supervised acquisition of relation ex-traction patterns.
In Proceedings of the Workshopon Information Extraction Beyond The Document,pages 29?35, Sydney, Australia, July.
Associationfor Computational Linguistics.Jones, R. 2005.
Learning to Extract Entities from La-beled and Unlabeled Text.
Ph.D. thesis, Universityof Utah.Pantel, Patrick and Marco Pennacchiotti.
2006.Espresso: Leveraging generic patterns for automati-cally harvesting semantic relations.
In Proceedingsof the 21st International Conference on Computa-tional Linguistics and 44th Annual Meeting of theAssociation for Computational Linguistics, Sydney,Australia, July.
The Association for Computer Lin-guistics.Riloff, Ellen.
1996.
Automatically generating extrac-tion patterns from untagged text.
In Proceedingsof Thirteenth National Conference on Artificial In-telligence (AAAI-96), pages 1044?1049.
The AAAIPress/MIT Press.Suchanek, Fabian M., Gjergji Kasneci, and GerhardWeikum.
2007.
Yago: A Core of Semantic Knowl-edge.
In 16th international World Wide Web con-ference (WWW 2007), New York, NY, USA.
ACMPress.Sudo, K., S. Sekine, and R. Grishman.
2003.
An im-proved extraction pattern representation model forautomatic IE pattern acquisition.
Proceedings ofACL 2003, pages 224?231.Uszkoreit, Hans, Feiyu Xu, and Hong Li.
2009.
Anal-ysis and improvement of minimally supervised ma-chine learning for relation extraction.
In 14th In-ternational Conference on Applications of NaturalLanguage to Information Systems.
Springer.Xu, Feiyu, Hans Uszkoreit, and Hong Li.
2007.
Aseed-driven bottom-up machine learning frameworkfor extracting relations of various complexity.
InProceedings of ACL 2007, 45th Annual Meetingof the Association for Computational Linguistics,Prague, Czech Republic, June.Xu, Feiyu.
2007.
Bootstrapping Relation Extractionfrom Semantic Seeds.
Phd-thesis, Saarland Univer-sity.Yangarber, Roman.
2001.
Scenarion Customizationfor Information Extraction.
Dissertation, Depart-ment of Computer Science, Graduate School of Artsand Science, New York University, New York, USA.Yangarber, Roman.
2003.
Counter-training in dis-covery of semantic patterns.
In Proceedings of the41st Annual Meeting of the Association for Compu-tational Linguistics, pages 343?350, Sapporo Con-vention Center, Sapporo, Japan, July.1362
