Detecting Text Similarity over Short Passages: ExploringLinguistic Feature Combinations via Machine LearningVasi le ios Hatzivassiloglou*, Judith L. K lavans  *t, and E leazar  Eskin**Department of Computer ScienceColumbia University1214 Amsterdam AvenueNew York, N.Y. 10027tCenter for Research on Information AccessCo lumbia  University535 West  l l4th StreetNew York, N.Y.  10027{vh, klavans, eeskin}@cs, columbia, eduAbst ractWe present a new composite similarity metricthat combines information from multiple lin-guistic indicators to measure semantic distancebetween pairs of small textual units.
Severalpotential features are investigated and an opti-real combination is selected via machine learn-ing.
We discuss a more restrictive definitionof similarity than traditional, document-leveland information retrieval-oriented, notions ofsimilarity, and motivate it by showing its rel-evance to the multi-document text summariza-tion problem.
Results from our system are eval-uated against standard information retrievaltechniques, establishing that the new methodis more effective in identifying closely relatedtextual units.1 Research  GoalsIn this paper, we focus on the problem of detect-ing whether two small textual units (paragraph-or sentence-sized) contain common information,as a necessary step towards extracting suchcommon information and constructing thematicgroups of text units across multiple documents.Identifying similar pieces of text has many ap-plications (e.g., summarization, i formation re-trieval, text clustering).
Most research in thisarea has centered on detecting similarity be-tween documents \[Willet 1988\], similarity be-tween a query and a document \[Salton 1989\] orbetween a query and a segment of a document\[Callan 1994\].
While effective techniques havebeen developed for document clustering andclassification which depend on inter-documentsimilarity measures, these techniques mostlyrely on shared words, or occasionally colloca-tions of words \[Smeaton 1992\].
When largerunits of text are compared, overlap may be suf-ficient to detect similarity; but when the unitsof text are small, simple surface matching ofwords and phrases is less likely to succeed sincethe number of potential matches is smaller.Our task differs from typical text matchingapplications not only in the smaller size of thetext units compared, but also in its overall goal.Our notion of similarity is more restrictive thantopical similarity--we provide a detailed defi-nition in the next section.
We aim to recoversets of small textual units from a collectionof documents so that each text unit within agiven set describes the same action.
Our sys-tem, which is fully implemented, is further mo-tivated by the need for determining similaritybetween small pieces of text across documentsthat potentially span different topics duringmulti-document summarization.
It serves as thefirst component ofa domain-independent multi-document summarization system \[McKeown etal.
1999\] which generates a summary throughtext reformulation \[Barzilay et al 1999\] by com-bining information from these similar text pas-sages.We address concerns of sparse data and thenarrower than topical definition of similarity byexploring several linguistic features, in additionto shared words or collocations, as indicators oftext similarity.
Our primit ive features includelinked noun phrases, WordNet synonyms, andsemantically similar verbs.
We also define com-posite features over pairs of primitive features.We then provide an effective method for aggre-gating the feature values into a similarity mea-sure using machine learning, and present results203on a manually annotated corpus of 10,345 pairsof compared paragraphs.
Our new features,and especially the composite ones, are shownto outperform traditional techniques uch asTF*IDF \[Buckley 1985; Salton 1989\] for deter-mining similarity over small text units.2 Definit ion of SimilaritySimilarity is a complex concept which has beenwidely discussed in the linguistic, philosophi-cal, and information theory communities.
Forexample, Frawley \[1992\] discusses all semantictyping in terms of two mechanisms: the de-tection of similarity and difference.
Jackendoff\[1983\] argues that standard semantic relationssuch as synonymy, paraphrase, redundancy, andentailment all result from judgments of like-ness whereas antonymy, contradiction, and in-consistency derive from judgments of differ-ence.
Losee \[1998\] reviews notions of similarityand their impact on information retrieval tech-niques.For our task, we define two text units as sim-ilar if they share the same focus on a commonconcept, actor, object, or action.
In addition,the common actor or object must perform orbe subjected to the same action, or be the sub-ject of the same description.
For example, Fig-ure 1 shows three input text fragments (para-graphs) taken from the TDT pilot corpus (seeSection 5.1)', all from the same topic on theforced landing of a U.S. helicopter in North Ko-rea.We consider units (a) and (b) in Figure 1 tobe similar, because they both focus on the sameevent (loss of contact) with the same primaryparticipant (the helicopter).
On the other hand,unit (c) in Figure 1 is not similar to either (a)or (b).
Although all three refer to a helicopter,the primary focus in (c) is on the emergencylanding rather than the loss of contact.We discuss an experimental validation of oursimilarity definition in Section 5.2, after we in-troduce the corpus we use in our experiments.3 Re la ted  WorkAlthough: there is related empirical research ondetermining text similarity, primarily in the in-formation retrieval community, there are twomajor differences between the goals of this ear-lier work and the problem we address in this(a) An OH-58 helicopter, carrying a crewof two, was on a routine training orien-tation when contact was lost at about11:30 a.m. Saturday (9:30 p.m. EST Fri-day).
(b) "There were two people on board," saidBacon.
"We lost radar contact withthe helicopter about 9:15 EST (0215GMT).
"(c) An OH-58 U.S. military scout helicoptermade an emergency landing in NorthKorea at about 9.15 p.m. EST Friday(0215 GMT Saturday), the Defense De-partment said.Figure 1: Input text units (from the TDT pilotcorpus, topic 11).paper.
First, the notion of similarity as de-fined in the previous section is more restric-tive than the traditional definition of similar-ity \[Anderberg 1973; Willet 1988\].
Standardnotions of similarity generally involve the cre-ation of a vector or profile of characteristics ofa text fragment, and then computing on thebasis of frequencies the distance between vec-tors to determine conceptual distance \[Saltonand Buckley 1988; Salton 1989\].
Features typ-ically include stemmed words although some-times multi-word units and collocations havebeen used \[Smeaton 1992\], as well as typolog-ical characteristics, uch as thesaural features.The distance between vectors for one text (usu-ally a query) and another (usually a document)then determines closeness or similarity \[van Ri-jsbergen 1979\].
In some cases, the texts are rep-resented as vectors of sparse n-grams of wordoccurrences and learning is applied over thosevectors \[Schapire and Singer 1999\].
But sinceour definition of similarity is oriented to thesmall-segment goal, we make more fine-graineddistinctions.
Thus, a set of passages that wouldprobably go into the same class by standard IRcriteria would be further separated by our meth-ods.Second, we have developed a method thatfunctions over pairs of small units of text, sothe size of the input text to be compared is dif-ferent.
This differs from document-to-document204or query-to-document comparison.
A closely re-lated problem is that of matching a query tothe relevant segment from a longer document\[Callan 1994; Kaszkiel and Zobel 1998\], whichprimarily involves determining which segmentof a longer document is relevant to a query,whereas our focus is on which segments are sim-ilar to each other.
In both cases, we have lessdata to compare, and thus have to explore ad-ditional or more informative indicators of simi-larity.4 Methodo logyWe compute a feature vector over a pair of tex-tual units, where features are either primitive,consisting of one characteristic, or composite,consisting of pairs of primitive features.4.1 P r imi t ive  FeaturesOur features draw on a number of linguistic ap-proaches to text analysis, and are based on bothsingle words and simplex noun phrases (headnouns preceded by optional premodifiers butwith no embedded recursion).
Each of thesemorphological, syntactic, and semantic featureshas several variations.
We thus consider the fol-lowing potential matches between text units:?
Word  co -occur rence ,  i.e., sharing of a sin-gle word between text units.
Variations ofthis feature restrict matching to cases wherethe parts of speech of the words also match,or relax it to cases where just the stems of thetwo words are identical.?
Match ing  noun phrases .
We use theLINKIT tool \[Wacholder 1998\] to identify sim-plex noun phrases and match those that sharethe same head.?
WordNet  synonyms.
WordNet \[Miller etal.
1990\] provides sense information, placingwords in sets of synonyms (synsets).
Wematch words that appear in the same synset.Variations on this feature restrict the wordsconsidered to a specific part-of-speech class.?
Common semant ic  c lasses for verbs.Levin's \[1993\] semantic lasses for verbs havebeen found to be useful for determining doc-ument type and text similarity \[Klavans andKan 1998\].
We match two verbs that sharethe same semantic lass.?
Shared  proper  nouns.
Proper nouns areidentified using the ALEMBIC tool set \[Ab-erdeen et al 1995\].
Variations on proper nounmatching include restricting the proper nountype to a person, place, or an organization(these subcategories are also extracted withALEMBIC's named entity finder).In order to normalize for text length and fre-quency effects, we experimented with two typesof optional normalization of feature values.
Thefirst is for text length (measured in words),where each feature value is normalized by thesize of the textual units in the pair.
Thus, for apair of textual units A and B, the feature valuesare divided by:v/length(A) ?
length(B) (1)This operation removes potential bias in favorof longer text units.The second type of normalization we exam-ined was based on the relative frequency of oc-currence of each primitive.
This is motivatedby the fact that infrequently matching primi-tive elements are likely to have a higher impacton similarity than primitives which match morefrequently.
We perform this normalization ina manner similar to the IDF part of TF* IDF\[Salton 1989\].
Every primitive element is asso-ciated with a value which is the number of tex-tual units in which the primitive appeared in thecorpus.
For a primitive element which comparessingle words, this is the number of textual unitswhich contain that word in the corpus; for anoun phrase, this is the number of textual unitsthat contain noun phrases that share the samehead; and similarly for other primitive types.We multiply each feature's value by:Total number of textual unitslog Number of textual units (2)containing this primitiveSince each normalization is optional, there arefour variations for each primitive feature.4.2 Compos i te  FeaturesIn addition to the above primitive features thatcompare single items from each text unit, weuse composite features which combine pairs ofprimitive features.
Composite features are de-fined by placing different types of restrictionson the participating primitive features:205An OH-58 helicopter, carrying acrew o f~ lwas  on a routine training(a) orientation when a s ~ s t  out 11:30 a.m. Saturday(9:30 p.m. EST F r ~ y ~  ~ - _ ~- ~ _ _ _ ~ _(b) "There weret~eop le  on board," said Bacon.
"We lost radar~with the helicopter about 9:15 EST (0215 GMT).
"Figure 2: A composite feature over word primitives with a restriction on order would count the pair"two" and "contact" as a match because they occur with the same relative order in both textualunits.An OH-58 helicopter, carrying acrew of two, was on a routine trainingorientation when i4~.~_ .~,about  11:30 a.m. Saturday(a) (9:30 p.m. EST Friday).
(b) "There were two ~with the helicopter about 9:15 EST (0215 GMT).
"Figure 3: A composite feature over word primitives with a restriction on distance would match onthe pair "lost" and "contact" because they occur within two words of each other in both textualunits.arrying a crew of two, was on a routine training~orientation when contact was~at  about 11:30 a.m. Saturday(a) (~ 0 p'm" EST Friday)"(b) "T ere we~eop le  on board," said Bacon.
i lWe~radar  contact~ ~ ~ b o u t  9:15 EST (0215 GMT).Figure 4: A composite feature with restrictions on the primitives' type.
One primitive must bea matching simplex noun phrase (in this case, a helicopter), while the other primitive must be amatching verb (in this case, "lost" .)
The example shows a pair of textual units where this compositefeature detects a valid match.?
Order ing .
Two pairs of primitive elementsare required to have the same relative orderin both textual units (see Figure 2).?
D is tance.
Two pairs of primitive elementsare required to occur within a certain dis-tance in both textual units (see Figure 3).The maximum distance between the primi-t ire elements can vary as an additional pa-rameter.
A distance of one matches rigid col-locations whereas a distance of five capturesrelated primitives within a region of the textunit \[Smeaton 1992; Smadja 1993\].?
P r imi t ive .
Each element of the pair of prim-itive elements can be restricted to a specificprimitive, allowing more expressiveness in thecomposite features.
For example, we can re-strict one of the primitive features to be a sim-plex noun phrase and the other to be a verb;then, two noun phrases, one from each textunit, must match according to the rule formatching simplex noun phrases (i.e., sharingthe same head), and two verbs must matchaccording to the rule for verbs (i.e., shar-ing the same semantic lass); see Figure 4.1This particular combination loosely approx-imates grammatical relations, e.g., matchingsubject-verb pairs.1Verbs can also be matched by the first (and more re-strictive) rule of Section 4.1, namely requiring that theirstemmed forms be identical.206Since these restrictions can be combined,many different composite features can be de-fined, although our empirical results indicatethat the most successful tend to include a dis-tance constraint.
As we put more restrictions ona composite feature, the fewer times it occurs inthe corpus; however, some of the more restric-tive features are most effective in determiningsimilarity.
Hence, there is a balance betweenthe discriminatory power of these features andtheir applicability to a large number of cases.Composite features are normalized as primitivefeatures are (i.e., for text unit length and forfrequency of occurrence).
This type of normal-ization also uses equation (2) but averages thenormalization values of each primitive in thecomposite feature.4.3 Learning a ClassifierFor each pair of text units, we compute a vec-tor of primitive and composite feature values.To determine whether the units match overall,we employ a machine learning algorithm, RIP-PER \[Cohen 1996\], a widely used and effectiverule induction system.
R IPPER is trained overa corpus of manually marked pairs of units; wediscuss the specifics of our corpus and of the an-notation process in the next session.
We exper-iment with varying R IPPER's  loss ratio, whichmeasures the cost of a false positive relative tothat of a false negative (where we view "simi-lar" as the positive class), and thus controls therelative weight of precision versus recall.
Thisis an important step in dealing with the sparsedata problem; most text units are not similar,given our restrictive definition, and thus posi-tive instances are rare.5 Resu l ts5.1 The  Eva luat ion  CorpusFor evaluation, we use a set of articles alreadyclassified into topical subsets which we obtainedfrom the Reuters part of the 1997 pilot TopicDetection and Tracking (TDT) corpus.
TheTDT corpus, developed by NIST and DARPA,is a collection of 16,000 news articles fromReuters and CNN where many of the articlesand transcripts have been manually groupedinto 25 categories each of which correspondsto a single event (see ht tp : / /morph .
ldc .uperm, edu/Cat alog/LDC98T25, html).
Usingthe Reuters part of the corpus, we selected fiveof the larger categories and extracted all articlesassigned to them from severM randomly chosendays, for a total of 30 articles.Since paragraphs in news stories tend to beshort--typically one or two sentences--in thisstudy we use paragraphs as our small text units,although sentences would also be a possibility.In total, we have 264 text units and 10,345 com-parisons between units.
As comparisons aremade between all pairs of paragraphs from thesame topic, the total number of comparisons iequal toEi=1where Ni is the number of paragraphs in all se-lected articles from topical category i.Training of our machine learning componentwas done by three-fold cross-validation, ran-domly splitting the 10,345 pairs of paragraphsinto three (almost) equally-sized subsets.
Ineach of the three runs, two of these subsets wereused for training and one for testing.To create a reference standard, the entire col-lection of 10,345 paragraph pairs was marked forsimilarity by two reviewers who were given ourdefinition and detailed instructions.
Each re--viewer independently marked each pair of para-graphs as similar or not similar.
Subsequently,the two reviewers jointly examined eases wherethere was disagreement, discussed reasons, andreconciled the differences.5.2 Exper imenta l  Val idat ion of  theSimilar ity Def in i t ionIn order to independently validate our defini-tion of similarity, we performed two additionalexperiments.
In the first, we asked three addi-tional judges to determine similarity for a ran-dom sample of 40 paragraph pairs.
High agree-ment between judges would indicate that ourdefinition of similarity reflects an objective re-ality and can be mapped unambiguously to anoperational procedure for marking text units assimilar or not.
At the same time, it would alsovalidate the judgments between text units thatwe use for our experiments (see Section 5.1).In this task, judges were given the opportu-nity to provide reasons for claiming similarityor dissimilarity, and comments on the task werelogged for future analysis.
The three additional207judges agreed with the manually marked andstandardized corpus on 97.6% of the compar-isons.Unfortunately, approximately 97% (depend-ing on the specific experiment) of the compar-isons in both our model and the subsequent val-idation experiment receive the value "not sim-ilar".
This large percentage is due to our fine-grained notion of similarity, and is parallel towhat happens in randomly sampled IR collec-tions, since in that case most documents willnot be relevant o any given query.
Neverthe-less, we can account for the high probabilityof inter-reviewer agreement expected by chance,0.97.0.97+ (1-0.97)-(1-0.97) -- 0.9418, by re-ferring to the kappa statistic \[Cohen 1960; Car-letta 1996\].
The kappa statistic is defined asPA -- Pog-~-  - -l - P0where PA is the probability that two reviewersagree in practice, and P0 is the probability thatthey would agree solely by chance.
In our case,PA = 0.976, P0 = 0.9418, and K = 0.5876,indicating that the observed agreement by thereviewers is indeed significant.
2 If P0 is esti-mated from the particular sample used in thisexperiment rather than from our entire corpus,it would be only 0.9, producing a value of 0.76for K.In addition to this validation experiment thatused randomly  sampled pairs of paragraphs(and reflected the disproportionate rate of oc-currence of dissimilar pairs), we  performed abalanced experiment by randomly  selecting 50of the dissimilar pairs and 50 of the similarpairs, in a manner  that guaranteed generationof an independent sample.
3 Pairs in this sub-set were rated for similarity by two additionalindependent reviewers, who agreed on their de-cisions 91% of the time, versus 50% expectedby chance; in this case, K --- 0.82.
Thus, wefeel confident in the reliability of our annotation2K is always between 0 and I, with 0 indicating nobetter agreement than expected by chance and 1 indi-cating perfect agreement.3To guarantee independence, pairs of paragraphswere randomly selected for inclusion in the samplebut a pair (A, B) was immediately rejected if therewere paragraphs X1,.. .
,X,~ for n > 0 such that allpairs (A, X1), (X1, X2), ?
?
?, (Xn, B) had already been in-cluded in the sample.process, and can use the annotated corpus to as-sess the performance of our similarity measureand compare it to measures proposed earlier inthe information retrieval literature.5.3 Per fo rmance  Compar isonsWe compare the performance of our systemto three other methods.
First, we use stan-dard TF*IDF, a method that with various alter-ations, remains at the core of many informationretrieval and text matching systems \[Salton andBuckley 1988; Salton 1989\].
We compute the to-tal frequency (TF) of words in each text unit.We also compute the number of units each wordappears in in our training set (DF, or documentfrequency).
Then each text unit is representedas a vector of TF*IDF scores calculated asTotal number of unitsTF (word/) ?
logDF(wordi)Similarity between text units is measured by thecosine of the angle between the correspondingtwo vectors (i.e., the normalized inner productof the two vectors).
A further cutoff point isselected to convert similarities to hard decisionsof "similar" or "not similar"; different cutoffsresult in different radeoffs between recall andprecision.Second, we compare our method againsta standard, widely available information re-trieval system developed at Cornell University,SMART \[Buckley 1985\].
4 SMART utilizes amodified TF*IDF measure (ATC) plus stem-ming and a fairly sizable stopword list.Third, we use as a baseline method the de-fault selection of the most frequent category,i.e., "not similar".
While this last method can-not be effectively used to identify similar para-graphs, it offers a baseline for the overall ac-curacy of any more sophisticated technique forthis task.5.4 Exper imenta l  ResultsOur system was able to recover 36.6% of thesimilar paragraphs with 60.5% precision, asshown in Table 1.
In comparison, the unmodi-fied TF*IDF approach obtained only 32.6% pre-cision when recall is 39.1%, i.e., close to oursystem's recall; and only 20.8% recall at pre-cision of 62.2%, comparable to our classifier'saWe used version 11.0 of SMART, released in July1992.208Recall Precision AccuracyMachinelearningoverlinguisticindicatorsTF*IDFSMARTDefaultchoice(baseline)36.6%30.0%29.1%0%60.5% 98.8%47.4% 97.2%48.3% 97.1%undefined 97.5%Table I: Experimental  results for different sim-ilarity metrics.
For comparison purposes, welist the average recall, precision, and accuracyobtained by TF* IDF  and SMART at the twopoints in the precision-recall curve identified foreach method in the text (i.e., the point wherethe method's precision is most similar to ours,and the point where its recall is most similar toours).precision.
SMART (in its default configura-tion) offered only a small improvement over thebase TF*IDF implementation, and significantlyunderperformed our method, obtaining 34.1%precision at recall of 36.7%, and 21.5% recallat 62.4% precision.
The default method of al-ways marking a pair as dissimilar obtains ofcourse 0% recall and undefined precision.
Fig-ure 5 illustrates the difference between our sys-tern and straight TF*IDF at different points ofthe precision-recall spectrum.When overall accuracy (total percentage ofcorrect answers over both categories of similarand non-similar pairs) is considered, the num-bers are much closer together: 98.8% for ourapproach; 96.6% and 97.8% for TF*IDF onthe two P-R points mentioned for that methodabove; 96.5% and 97.6% for SMART, againat the two P-R points mentioned for SMARTearlier; and 97.5% for the default baseline.
5Nevertheless, ince the challenge of identifyingsparsely occurring similar small text units isour goal, the accuracy measure and the base-line technique of classifying everything as notsimilar are included only for reference but do5Statistical tests of significance cannot be performedfor comparing these values, since paragraphs appear inmultiple comparisons and consequently the comparisonsare not independent.I .G0.8-0.7-0.60.50.~0.90.2-O.OO.0"\0.1 0.2 0.3 0.4 0.5 0,6 0.7 O.B 0.9 1.0RecallFigure 5: Precision-recall graph comparing ourmethod using RIPPER (solid line with squares)versus TF*IDF (dotted line with triangles).not reflect our task.6 Ana lys i s  and  D iscuss ion  o f  FeaturePer fo rmanceWe computed statistics on how much each fea-ture helps in identifying similarity, summarizedin Table 2.
Primitive features are named ac-cording to the type of the feature (e.g., Verb forthe feature that counts the number of matchingverbs according to exact matches).
Compositefeature names indicate the restrictions appliedto primitives.
For example, the composite fea-ture Distance < ~ restricts a pair of matchingprimitives to occur within a relative distance offour words.
If the composite feature also re-stricts the types of the primitives in the pair,the name of the restricting primitive feature isadded to the composite feature name.
For ex-ample the feature named Verb Distance < 5 re-quires one member of the pair to be a verb andthe relative distance between the primitives tobe at most five.The second column in Table 2 shows whetherthe feature value has been normalized accord-ing to its overall rarity 6, while the third columnindicates the actual threshold used in decisionsassuming that only this feature is used for clas-sification.
The fourth column shows the applica-bility of that feature, that is, the percentage of6All results reported in Table 2 include our first nor-malization step that accounts for the difference in thelength of text units.209Feature NameAny wordNounProper nounVerbSimplex NPSemantic lass of verbsWordNetDistance < 2Distance _< 3Distance < 4Distance < 5Order Distance < 5Normalized?YesYesYesNoYesNoYesYesYesYesYesYesThreshold0.3600.5050.1500.2750.2000.7750.1500.3500.8750.2500.0750.2500.2750.2000.200Noun Distance < 5 Yes 0.175Yes 0.200No Verb Distance < 5 !l,~IIApplicability\[Recall2.2%0.6%8.1%1.5%0.2%1.6%5.7%2.7%0.7%0.1%5.4%4.7%0.5%1.9%1.9%1.5%1.9%0.3%0.6%31.4%16.7%43.2%20.9%2.0%10.6%35.5%10.1%3.7%2.0%4.1%24.9%10.2%14.6%22.4%20.4%21.2%7.3%11.0%\[ Precision41.8%75.4%15.9%37.0%30.8%19.7%18.6%44.6%69.2%3.4%2.3%15.7%55.6%5O.O%53.4%40.7%31.9%66.7%56.3%Table 2: Statistics for a selected subset of features.
Performance measures are occasionally givenmultiple times for the same feature and normalization option, highlighting the effect of differentdecision thresholds.paragraph pairs for which this feature would ap-ply (i.e., have a value over the specified thresh-old).
Finally, the fifth and sixth columns howthe recall and precision on identifying similarparagraphs for each independent feature.
Notethat some features have low applicability overthe entire corpus, but target the hard-to-findsimilar pairs, resulting in significant gains in re-call and precision.Table 2 presents a selected subset of primitiveand composite features in order to demonstrateour results.
For example, it was not surprisingto observe that the most effective primitive fea-tures in determining similarity are Any word,Simplex NPi and Noun while other primitivessuch as Verb were not as effective independently.This is to be expected since nouns name ob-jects, entities, and concepts, and frequently ex-hibit more sense constancy.
In contrast, verbsare functions and tend to shift senses in a morefluid fashion depending on context.
Further-more, our technique does not label phrasal verbs(e.g.
look up, look out, look over, look for, etc.
),which are a major source of verbal ambiguity inEnglish.Whereas primitive features viewed indepen-dently might not have a directly visible effecton identifying similarity, when used in compos-ite features they lead to some novel results.
Themost pronounced case of this is for Verb, which,in the composite feature Verb Distance _< 5,can help identify similarity effectively, as seenin Table 2.
This composite feature approxi-mates verb-argument and verb-collocation rela-tions, which are strong indicators of similarity.At the same time, the more restrictive a featureis, the fewer occurrences of that feature appearin the training set.
This suggests that we couldconsider adding additional features uggestedby current results in order to further efine andimprove our similarity identification algorithm.7 Conc lus ion  and  Future  WorkWe have presented a new method to detectsimilarity between small textual units, whichcombines primitive and composite features us-ing machine learning.
We validated our sim-ilarity definition using human judges, applied210our method to a substantial number of para-graph pairs from news articles, and comparedresults to baseline and standard information re-trieval techniques.
Our results indicate that ourmethod outperforms the standard techniquesfor detecting similarity, and the system has beensuccessfully integrated into a larger multiple-document summarization system \[McKeown etal.
1999\].We are currently working on incorporating aclustering algorithm in order to give as outputa set of textual units which are mutually sim-ilar rather than just pairwise similar.
Futurework includes testing on textual units of differ-ent size, comparing with additional techniquesproposed for document similarity in the infor-mation retrieval and computational linguisticsliterature, and extending the feature set to in-corporate other types of linguistic informationin the statistical learning method.AcknowledgmentsWe are grateful to Regina Barzilay, HongyanJing, Kathy McKeown, Shimei Pan, and YoramSinger for numerous discussions of earlier ver-sions of this paper and for their help with settingup and running RIPPER and SMART.
This re-search has been supported in part by an NSFSTIMULATE grant, IRI-96-1879.
Any opin-ions, findings, and conclusions or recommenda-tions expressed in this paper are those of theauthors and do not necessarily reflect he viewsof the National Science Foundation.References\[Aberdeen et al 1995\] John Aberdeen, JohnBurger, David Day, Lynette Hirschman, Pa-tricia Robinson, and Marc Vilain.
MITRE:Description of the Alembic System as Usedfor MUC-6.
In Proceedings of the SixthMessage Understanding Conference (MUC-6), 1995.\[Anderberg 1973\] Michael R. Anderberg.
Clus-ter Analysis fo r Applications.
AcademicPress, New York, 1973.
Revised version ofthe author's thesis, University of Texas atAustin, 1971.\[Barzilay et al 1999\] Regina Barzilay, Kath-leen R. McKeown, and Michael Elhadad.
In-formation Fusion in the Context of Multi-Document Summarization.
In Proceedingsof the 37th Annual Meeting of the A CL, Col-lege Park, Maryland, June 1999 (to appear).Association for Computational Linguistics.\[Buckley 1985\] Christopher Buckley.
Imple-mentation of the SMART Information Ree-treival System.
Technical Report 85-686,Cornell University, 1985.\[Callan 1994\] Jaime P. Callan.
Passage-LevelEvidence in Document Retrieval.
In Pro-ceedings of the 17th A CM SIGIR Interna-tional Conference on Research and Develop-ment in Information Retrieval, pages 302-309, Dublin, Ireland, 1994.\[Carletta 1996\] Jean Carletta.
AssessingAgreement on Classification Tasks: TheKappa Statistic.
Computational Linguistics,22(2):249-254, June 1996.\[Cohen 1960\] Jacob Cohen.
A Coefficient ofAgreement for Nominal Scales.
Educationaland Psychological Measurement, 20:37-46,1960.\[Cohen 1996\] William Cohen.
Learning Treesand Rules with Set-Valued Features.
In Pro-ceedings of the Fourteenth National Confer-ence on Artificial Intelligence (AAAI-96).American Association for Artificial Intelli-gence, 1996.\[Frawley 1992\] William Frawley.
Linguistic Se-mantics.
Lawrence Erlbaum Associates,Hillsdale, New Jersey, 1992.\[Jackendoff 1983\] Ray Jackendoff.
Semanticsand Cognition.
MIT Press, Cambridge,Massachusetts, 1983.\[Kaszkiel and Zobel 1998\] Marcin Kaszkiel andJustin Zobel.
Passage Retrieval Revisited.In Proceedings of the 21st Annual Interna--tional A CM SIGIR Conference on Researchand Development in Information Retrieval,Melbourne, Australia, August 1998.\[Klavans and Kan 1998\] Judith L. Klavans andMin-Yen Kan.
The Role of Verbs inDocument Access.
In Proceedings of the21136th Annual Meeting of the Association forComputational Linguistics and the 17th In-ternational Conference on ComputationalLinguistics (ACL/COLING-98), Montreal,Canada, 1998.\[Levin 1993\] Beth Levin.
English Verb Classesand Alternations: A Preliminary Investiga-tion.
University of Chicago Press, Chicago,Illinois, 1993.\[Losee 1998\] Robert M. Losee.
Text Retrievaland Filtering: Analytic Models of Per-formance.
Kluwer Academic Publishers,Boston, Massachusetts, 1998.\[McKeown et al 1999\] Kathleen R. McKeown,Judith L. Klavans, Vasileios Hatzivas-siloglou, Regina Barzilay, and Eleazar Eskin.Towards Multidocument Summarization byReformulation: Progress and Prospects.
InProceedings of the Seventeenth NationalConference on Artificial Intelligence (AAAI-99), Orlando, Florida, 1999 (to appear).American Association for Artificial Intelli-gence.\[Miller et al 1990\] George A. Miller, RichardBeckwith, Christiane Fellbaum, DerekGross, and Katherine J. Miller.
Intro-duction to WordNet: An On-Line LexicalDatabase.
International Journal of Lexicog-raphy, 3(4):235-312, 1990.\[Salton and Buckley 1988\] Gerard Salton andChristopher Buckley.
Term WeightingApproaches in Automatic Text Retrieval.Information Processing and Management,25(5):513-523, 1988.\[Salton 1989\] Gerard Salton.
Automatic TextProcessing: The Transformation, Analysis,and Retrieval of Information by Computer.Addison-Wesley, Reading, Massachusetts,1989.\[Schapire and Singer 1999\] Robert E. Schapireand Yoram Singer.
BoosTexter: A Boosting-Based System for Text Categorization.
Ma-chine Learning, 1999 (to appear).\[Smadja 1993\] Frank Smadja.
Retrieving Col-locations from Text: Xtract.
ComputationalLinguistics, 19(1):143-177, March 1993.\[Smeaton 1992\] Alan F. Smeaton.
Progress inthe Application of Natural Language Pro-cessing to Information Retrieval Tasks.
TheComputer Journal, 35(3):268-278, 1992.\[van Rijsbergen 1979\] C. J. van Rijsbergen.
In-formation Retrieval.
Butterworths, London,2nd edition, 1979.\[Wacholder 1998\] Nina Wacholder.
SimplexNPs Clustered by Head: A Method For Iden-tifying Significant Topics in a Document.
InProceedings of the Workshop on the Com-putational Treatment of Nominals, pages70-79, Montreal, Canada, October 1998.COLING-ACL.\[Willet 1988\] Peter Willet.
Recent Trendsin Hierachical Document Clustering.
In-formation Processing and Management,24(5):577-597, 1988.212
