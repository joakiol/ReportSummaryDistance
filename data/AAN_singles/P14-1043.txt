Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 457?467,Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational LinguisticsAmbiguity-aware Ensemble Training for Semi-supervisedDependency ParsingZhenghua Li , Min Zhang?, Wenliang ChenProvincial Key Laboratory for Computer Information Processing TechnologySoochow University{zhli13,minzhang,wlchen}@suda.edu.cnAbstractThis paper proposes a simple yeteffective framework for semi-superviseddependency parsing at entire tree level,referred to as ambiguity-aware ensembletraining.
Instead of only using 1-best parse trees in previous work, ourcore idea is to utilize parse forest(ambiguous labelings) to combinemultiple 1-best parse trees generatedfrom diverse parsers on unlabeled data.With a conditional random field basedprobabilistic dependency parser, ourtraining objective is to maximize mixedlikelihood of labeled data and auto-parsedunlabeled data with ambiguous labelings.This framework offers two promisingadvantages.
1) ambiguity encoded inparse forests compromises noise in 1-bestparse trees.
During training, the parser isaware of these ambiguous structures, andhas the flexibility to distribute probabilitymass to its preferred parse trees as longas the likelihood improves.
2) diversesyntactic structures produced by differentparsers can be naturally compiled intoforest, offering complementary strengthto our single-view parser.
Experimentalresults on benchmark data show thatour method significantly outperformsthe baseline supervised parser andother entire-tree based semi-supervisedmethods, such as self-training, co-trainingand tri-training.1 IntroductionSupervised dependency parsing has made greatprogress during the past decade.
However, itis very difficult to further improve performance?Correspondence authorof supervised parsers.
For example, Koo andCollins (2010) and Zhang and McDonald (2012)show that incorporating higher-order features intoa graph-based parser only leads to modest increasein parsing accuracy.
In contrast, semi-supervisedapproaches, which can make use of large-scaleunlabeled data, have attracted more and moreinterest.
Previously, unlabeled data is explored toderive useful local-context features such as wordclusters (Koo et al, 2008), subtree frequencies(Chen et al, 2009; Chen et al, 2013), and wordco-occurrence counts (Zhou et al, 2011; Bansaland Klein, 2011).
A few effective learning meth-ods are also proposed for dependency parsing toimplicitly utilize distributions on unlabeled data(Smith and Eisner, 2007; Wang et al, 2008;Suzuki et al, 2009).
All above work leads tosignificant improvement on parsing accuracy.Another line of research is to pick up somehigh-quality auto-parsed training instances fromunlabeled data using bootstrapping methods, suchas self-training (Yarowsky, 1995), co-training(Blum and Mitchell, 1998), and tri-training (Zhouand Li, 2005).
However, these methods gainlimited success in dependency parsing.
Althoughworking well on constituent parsing (McClosky etal., 2006; Huang and Harper, 2009), self-trainingis shown unsuccessful for dependency parsing(Spreyer and Kuhn, 2009).
The reason may be thatdependency parsing models are prone to amplifyprevious mistakes during training on self-parsedunlabeled data.
Sagae and Tsujii (2007) applya variant of co-training to dependency parsingand report positive results on out-of-domain text.S?gaard and Rish?j (2010) combine tri-trainingand parser ensemble to boost parsing accuracy.Both work employs two parsers to process theunlabeled data, and only select as extra trainingdata sentences on which the 1-best parse trees ofthe two parsers are identical.
In this way, the auto-parsed unlabeled data becomes more reliable.457w0He1saw2a3deer4riding5a6bicycle7in8the9park10.11Figure 1: An example sentence with an ambiguous parse forest.However, one obvious drawback of these methodsis that they are unable to exploit unlabeled datawith divergent outputs from different parsers.Our experiments show that unlabeled data withidentical outputs from different parsers tends to beshort (18.25 words per sentence on average), andonly has a small proportion of 40% (see Table 6).More importantly, we believe that unlabeled datawith divergent outputs is equally (if not more)useful.
Intuitively, an unlabeled sentence withdivergent outputs should contain some ambiguoussyntactic structures (such as preposition phraseattachment) that are very hard to resolve andlead to the disagreement of different parsers.Such sentences can provide more discriminativeinstances for training which may be unavailablein labeled data.To solve above issues, this paper proposesa more general and effective framework forsemi-supervised dependency parsing, referred toas ambiguity-aware ensemble training.
Differentfrom traditional self/co/tri-training which only use1-best parse trees on unlabeled data, our approachadopts ambiguous labelings, represented by parseforest, as gold-standard for unlabeled sentences.Figure 1 shows an example sentence with anambiguous parse forest.
The forest is formed bytwo parse trees, respectively shown at the upperand lower sides of the sentence.
The differencesbetween the two parse trees are highlightedusing dashed arcs.
The upper tree take ?deer?as the subject of ?riding?, whereas the lowerone indicates that ?he?
rides the bicycle.
Theother difference is where the preposition phrase(PP) ?in the park?
should be attached, whichis also known as the PP attachment problem, anotorious challenge for parsing.
Reserving suchuncertainty has three potential advantages.
First,noise in unlabeled data is largely alleviated, sinceparse forest encodes only a few highly possibleparse trees with high oracle score.
Please notethat the parse forest in Figure 1 contains fourparse trees after combination of the two differentchoices.
Second, the parser is able to learn usefulfeatures from the unambiguous parts of the parseforest.
Finally, with sufficient unlabeled data, it ispossible that the parser can learn to resolve suchuncertainty by biasing to more reasonable parsetrees.To construct parse forest on unlabeled data, weemploy three supervised parsers based on differentparadigms, including our baseline graph-baseddependency parser, a transition-based dependencyparser (Zhang and Nivre, 2011), and a generativeconstituent parser (Petrov and Klein, 2007).
The1-best parse trees of these three parsers are aggre-gated in different ways.
Evaluation on labeled datashows the oracle accuracy of parse forest is muchhigher than that of 1-best outputs of single parsers(see Table 3).
Finally, using a conditional randomfield (CRF) based probabilistic parser, we traina better model by maximizing mixed likelihoodof labeled data and auto-parsed unlabeled datawith ambiguous labelings.
Experimental resultson both English and Chinese datasets demon-strate that the proposed ambiguity-aware ensem-ble training outperforms other entire-tree basedmethods such as self/co/tri-training.
In summary,we make following contributions.1.
We propose a generalized ambiguity-awareensemble training framework for semi-supervised dependency parsing, which can458make better use of unlabeled data, especiallywhen parsers from different views producedivergent syntactic structures.2.
We first employ a generative constituent pars-er for semi-supervised dependency parsing.Experiments show that the constituent parseris very helpful since it produces more diver-gent structures for our semi-supervised parserthan discriminative dependency parsers.3.
We build the first state-of-the-art CRF-baseddependency parser.
Using the probabilisticparser, we benchmark and conduct systemat-ic comparisons among ours and all previousbootstrapping methods, including self/co/tri-training.2 Supervised Dependency ParsingGiven an input sentence x = w0w1...wn, the goalof dependency parsing is to build a dependencytree as depicted in Figure 1, denoted by d ={(h,m) : 0 ?
h ?
n, 0 < m ?
n}, where (h,m)indicates a directed arc from the head word whto the modifier wm, and w0is an artificial nodelinking to the root of the sentence.In parsing community, two mainstream meth-ods tackle the dependency parsing problem fromdifferent perspectives but achieve comparable ac-curacy on a variety of languages.
The graph-based method views the problem as finding anoptimal tree from a fully-connected directed graph(McDonald et al, 2005; McDonald and Pereira,2006; Carreras, 2007; Koo and Collins, 2010),while the transition-based method tries to find ahighest-scoring transition sequence that leads toa legal dependency tree (Yamada and Matsumoto,2003; Nivre, 2003; Zhang and Nivre, 2011).2.1 Graph-based Dependency Parser(GParser)In this work, we adopt the graph-based paradigmbecause it allows us to naturally derive conditionalprobability of a dependency tree d given a sen-tence x, which is required to compute likelihoodof both labeled and unlabeled data.
Under thegraph-based model, the score of a dependency treeis factored into the scores of small subtrees p.Score(x,d;w) = w ?
f(x,d)=?p?dScore(x,p;w)h m(a) single dependencyh s(b) adjacent siblingmFigure 2: Two types of scoring subtrees in oursecond-order graph-based parsers.Dependency features fdep(x, h,m):wh, wm, th, tm, th?1, tm?1, tb, dir(h,m), dist(h,m)Sibling features fsib(x, h,m, s):wh, ws, wm, th, tm, ts, th?1, tm?1, ts?1dir(h,m), dist(h,m)Table 1: Brief illustration of the syntactic features.tidenotes the POS tag of wi.
b is an indexbetween h and m. dir(i, j) and dist(i, j) denotethe direction and distance of the dependency (i, j).We adopt the second-order graph-based depen-dency parsing model of McDonald and Pereira(2006) as our core parser, which incorporatesfeatures from the two kinds of subtrees in Fig.
2.1Then the score of a dependency tree is:Score(x,d;w) =?{(h,m)}?dwdep?
fdep(x, h,m)+?{(h,s),(h,m)}?dwsib?
fsib(x, h, s,m)where fdep(x, h,m) and fsib(x, h, s,m) are thefeature vectors of the two subtree in Fig.
2;wdep/sibare feature weight vectors; the dot prod-uct gives scores contributed by corresponding sub-trees.For syntactic features, we adopt those of Bohnet(2010) which include two categories correspond-ing to the two types of scoring subtrees in Fig.
2.We summarize the atomic features used in eachfeature category in Table 1.
These atomic featuresare concatenated in different combinations to com-pose rich feature sets.
Please refer to Table 4 ofBohnet (2010) for the complete feature list.2.2 CRF-based GParserPrevious work on graph-based dependency pars-ing mostly adopts linear models and perceptronbased training procedures, which lack probabilis-tic explanations of dependency trees and do notneed to compute likelihood of labeled training1Higher-order models of Carreras (2007) and Koo andCollins (2010) can achieve higher accuracy, but has muchhigher time cost (O(n4)).
Our approach is applicable to thesehigher-order models, which we leave for future work.459data.
Instead, we build a log-linear CRF-baseddependency parser, which is similar to the CRF-based constituent parser of Finkel et al (2008).Assuming the feature weights w are known, theprobability of a dependency tree d given an inputsentence x is defined as:p(d|x;w) =exp{Score(x,d;w)}Z(x;w)Z(x;w) =?d??Y(x)exp{Score(x,d?
;w)}(1)where Z(x) is the normalization factor and Y(x)is the set of all legal dependency trees for x.Suppose the labeled training data isD = {(xi,di)}Ni=1.
Then the log likelihoodof D is:L(D;w) =N?i=1log p(di|xi;w)The training objective is to maximize the loglikelihood of the training data L(D).
The partialderivative with respect to the feature weights w is:?L(D;w)?w=N?i=1??
?f(xi,di) ??d??Y(xi)p(d?|xi;w)f(xi,d?)???
(2)where the first term is the empirical counts andthe second term is the model expectations.
SinceY(xi) contains exponentially many dependencytrees, direct calculation of the second term isprohibitive.
Instead, we can use the classic inside-outside algorithm to efficiently compute the modelexpectations within O(n3) time complexity, wheren is the input sentence length.3 Ambiguity-aware Ensemble TrainingIn standard entire-tree based semi-supervisedmethods such as self/co/tri-training, automaticallyparsed unlabeled sentences are used as additionaltraining data, and noisy 1-best parse trees areconsidered as gold-standard.
To alleviate thenoise, the tri-training method only uses unlabeleddata on which multiple parsers from differentviews produce identical parse trees.
However,unlabeled data with divergent syntactic structuresshould be more useful.
Intuitively, if severalparsers disagree on an unlabeled sentence, itimplies that the unlabeled sentence containssome difficult syntactic phenomena which arenot sufficiently covered in manually labeleddata.
Therefore, exploiting such unlabeled datamay introduce more discriminative syntacticknowledge, largely compensating labeled trainingdata.To address above issues, we propose ambiguity-aware ensemble training, which can be interpretedas a generalized tri-training framework.
The keyidea is the use of ambiguous labelings for thepurpose of aggregating multiple 1-best parse treesproduced by several diverse parsers.
Here, ?am-biguous labelings?
mean an unlabeled sentencemay have multiple parse trees as gold-standardreference, represented by parse forest (see Figure1).
The training procedure aims to maximizemixed likelihood of both manually labeled andauto-parsed unlabeled data with ambiguous label-ings.
For an unlabeled instance, the model isupdated to maximize the probability of its parseforest, instead of a single parse tree in traditionaltri-training.
In other words, the model is free todistribute probability mass among the trees in theparse forest to its liking, as long as the likelihoodimproves (Ta?ckstro?m et al, 2013).3.1 Likelihood of the Unlabeled DataThe auto-parsed unlabeled data with ambiguouslabelings is denoted as D?
= {(ui,Vi)}Mi=1, whereuiis an unlabeled sentence, and Viis the corre-sponding parse forest.
Then the log likelihood ofD?
is:L(D?
;w) =M?i=1log???d??Vip(d?|ui;w)?
?where p(d?|ui;w) is the conditional probability ofd?
given ui, as defined in Eq.
(1).
For an unlabeledsentence ui, the probability of its parse forest Viisthe summation of the probabilities of all the parsetrees contained in the forest.Then we can derive the partial derivative of thelog likelihood with respect to w:?L(D?;w)?w=M?i=1????d??Vip?(d?|ui,Vi;w)f(ui,d?)??d??Y(ui)p(d?|ui;w)f(ui,d?)???
(3)where p?
(d?|ui,Vi;w) is the probability of d?
un-460der the space constrained by the parse forest Vi.p?
(d?|ui,Vi;w) =exp{Score(ui,d?
;w)}Z(ui,Vi;w)Z(ui,Vi;w) =?d??Viexp{Score(ui,d?
;w)}The second term in Eq.
(3) is the same with thesecond term in Eq.
(2).
The first term in Eq.
(3)can be efficiently computed by running the inside-outside algorithm in the constrained search spaceVi.3.2 Stochastic Gradient Descent (SGD)TrainingWe apply L2-norm regularized SGD training toiteratively learn feature weights w for our CRF-based baseline and semi-supervised parsers.
Wefollow the implementation in CRFsuite.2 At eachstep, the algorithm approximates a gradient witha small subset of the training examples, and thenupdates the feature weights.
Finkel et al (2008)show that SGD achieves optimal test performancewith far fewer iterations than other optimizationroutines such as L-BFGS.
Moreover, it is veryconvenient to parallel SGD since computationsamong examples in the same batch is mutuallyindependent.Training with the combined labeled and unla-beled data, the objective is to maximize the mixedlikelihood:L(D;D?)
= L(D) + L(D?
)Since D?
contains much more instances than D(1.7M vs. 40K for English, and 4M vs. 16K forChinese), it is likely that the unlabeled data mayoverwhelm the labeled data during SGD training.Therefore, we propose a simple corpus-weightingstrategy, as shown in Algorithm 1, where Dbi,kis the subset of training data used in kth updateand b is the batch size; ?kis the update step,which is adjusted following the simulated anneal-ing procedure (Finkel et al, 2008).
The idea isto use a fraction of training data (Di) at eachiteration, and do corpus weighting by randomlysampling labeled and unlabeled instances in acertain proportion (N1vs.
M1).Once the feature weights w are learnt, we can2http://www.chokkan.org/software/crfsuite/Algorithm 1 SGD training with mixed labeled andunlabeled data.1: Input: Labeled data D = {(xi,di)}Ni=1, and unlabeleddata D?
= {(ui,Vi)}Mj=1; Parameters: I , N1, M1, b2: Output: w3: Initialization: w(0) = 0, k = 0;4: for i = 1 to I do {iterations}5: Randomly select N1instances from D and M1instances from D?
to compose a new dataset Di, andshuffle it.6: Traverse Di: a small batch Dbi,k?
Diat one step.7: wk+1= wk+ ?k1b?L(Dbi,k;wk)8: k = k + 19: end forparse the test data to find the optimal parse tree.d?= arg maxd?
?Y(x)p(d?|x;w)= arg maxd??Y(x)Score(x,d?
;w)This can be done with the Viterbi decoding algo-rithm described in McDonald and Pereira (2006)in O(n3) parsing time.3.3 Forest Construction with Diverse ParsersTo construct parse forests for unlabeled data, weemploy three diverse parsers, i.e., our baselineGParser, a transition-based parser (ZPar3) (Zhangand Nivre, 2011), and a generative constituen-t parser (Berkeley Parser4) (Petrov and Klein,2007).
These three parsers are trained on labeleddata and then used to parse each unlabeled sen-tence.
We aggregate the three parsers?
outputs onunlabeled data in different ways and evaluate theeffectiveness through experiments.4 Experiments and AnalysisTo verify the effectiveness of our proposed ap-proach, we conduct experiments on Penn Tree-bank (PTB) and Penn Chinese Treebank 5.1 (CT-B5).
For English, we follow the popular practiceto split data into training (sections 2-21), devel-opment (section 22), and test (section 23).
ForCTB5, we adopt the data split of (Duan et al,2007).
We convert original bracketed structuresinto dependency structures using Penn2Malt withits default head-finding rules.For unlabeled data, we follow Chen et al (2013)and use the BLLIP WSJ corpus (Charniak et al,2000) for English and Xinhua portion of Chinese3http://people.sutd.edu.sg/?yue_zhang/doc/4https://code.google.com/p/berkeleyparser/461Train Dev Test UnlabeledPTB 39,832 1,700 2,416 1.7MCTB5 16,091 803 1,910 4MTable 2: Data sets (in sentence number).Gigaword Version 2.0 (LDC2009T14) (Huang,2009) for Chinese.
We build a CRF-based bigrampart-of-speech (POS) tagger with the features de-scribed in (Li et al, 2012), and produce POS tagsfor all train/development/test/unlabeled sets (10-way jackknifing for training sets).
The tagging ac-curacy on test sets is 97.3% on English and 94.0%on Chinese.
Table 2 shows the data statistics.We measure parsing performance using the s-tandard unlabeled attachment score (UAS), ex-cluding punctuation marks.
For significance test,we adopt Dan Bikel?s randomized parsing evalua-tion comparator (Noreen, 1989).54.1 Parameter SettingWhen training our CRF-based parsers with SGD,we use the batch size b = 100 for all experiments.We run SGD for I = 100 iterations and choosethe model that performs best on developmentdata.
For the semi-supervised parsers trained withAlgorithm 1, we use N1= 20K and M1= 50Kfor English, and N1= 15K and M1= 50K forChinese, based on a few preliminary experiments.To accelerate the training, we adopt parallelizedimplementation of SGD and employ 20 threads foreach run.
For semi-supervised cases, one iterationtakes about 2 hours on an IBM server having 2.0GHz Intel Xeon CPUs and 72G memory.Default parameter settings are used for trainingZPar and Berkeley Parser.
We run ZPar for 50iterations, and choose the model that achieveshighest accuracy on the development data.
ForBerkeley Parser, we use the model after 5 split-merge iterations to avoid over-fitting the train-ing data according to the manual.
The phrase-structure outputs of Berkeley Parser are convertedinto dependency structures using the same head-finding rules.4.2 Methodology Study on Development DataUsing three supervised parsers, we have manyoptions to construct parse forest on unlabeled data.To examine the effect of different ways for forestconstruction, we conduct extensive methodologystudy on development data.
Table 3 presents the5http://www.cis.upenn.edu/?dbikel/software.htmlresults.
We divide the systems into three types: 1)supervised single parsers; 2) CRF-based GParserwith conventional self/co/tri-training; 3) CRF-based GParser with our approach.
For the lattertwo cases, we also present the oracle accuracy andaveraged head number per word (?Head/Word?
)of parse forest when applying different ways toconstruct forests on development datasets.The first major row presents performance ofthe three supervised parsers.
We can see that thethree parsers achieve comparable performance onEnglish, but the performance of ZPar is largelyinferior on Chinese.The second major row shows the results whenwe use single 1-best parse trees on unlabeleddata.
When using the outputs of GParser itself(?Unlabeled ?
G?
), the experiment reproducestraditional self-training.
The results on both En-glish and Chinese re-confirm that self-trainingmay not work for dependency parsing, whichis consistent with previous studies (Spreyer andKuhn, 2009).
The reason may be that dependencyparsers are prone to amplify previous mistakes onunlabeled data during training.The next two experiments in the second ma-jor row reimplement co-training, where anotherparser?s 1-best results are projected into unlabeleddata to help the core parser.
Using unlabeleddata with the results of ZPar (?Unlabeled ?
Z?
)significantly outperforms the baseline GParser by0.30% (93.15-82.85) on English.
However, theimprovement on Chinese is not significant.
Usingunlabeled data with the results of Berkeley Parser(?Unlabeled?
B?)
significantly improves parsingaccuracy by 0.55% (93.40-92.85) on English and1.06% (83.34-82.28) on Chinese.
We believe thereason is that being a generative model designedfor constituent parsing, Berkeley Parser is moredifferent from discriminative dependency parsers,and therefore can provide more divergent syntacticstructures.
This kind of syntactic divergence ishelpful because it can provide complementaryknowledge from a different perspective.
Surdeanuand Manning (2010) also show that the diversity ofparsers is important for performance improvementwhen integrating different parsers in the super-vised track.
Therefore, we can conclude thatco-training helps dependency parsing, especiallywhen using a more divergent parser.The last experiment in the second major rowis known as tri-training, which only uses unla-462English ChineseUAS Oracle Head/Word UAS Oracle Head/WordGParser 92.85?
?82.28?
?Supervised ZPar 92.50 81.04Berkeley 92.70 82.46Unlabeled?
G (self-train) 92.88 92.851.00082.14 82.281.000Semi-supervised GParser Unlabeled?
Z (co-train) 93.15 ?
92.50 82.54 81.04with Single 1-best Trees Unlabeled?
B (co-train) 93.40 ?
92.70 83.34 ?
82.46Unlabeled?
B=Z (tri-train) 93.50 ?
97.52 83.10 ?
95.05Unlabeled?
Z+G 93.18 ?
94.97 1.053 82.78 86.66 1.136Unlabeled?
B+G 93.35 ?
96.37 1.080 83.24 ?
89.72 1.188Semi-supervised GParser Unlabeled?
B+Z 93.78 ??
96.18 1.082 83.86 ??
89.54 1.199Ambiguity-aware Ensemble Unlabeled?
B+(Z?G) 93.77 ??
95.60 1.050 84.26 ??
87.76 1.106Unlabeled?
B+Z+G 93.50 ?
96.95 1.112 83.30 ?
91.50 1.281Table 3: Main results on development data.
G is short for GParser, Z for ZPar, and B for Berkeley Parser.?
means the corresponding parser significantly outperforms supervised parsers, and ?
means the resultsignificantly outperforms co/tri-training at confidence level of p < 0.01.beled sentences on which Berkeley Parser andZPar produce identical outputs (?Unlabeled ?B=Z?).
We can see that with the verification oftwo views, the oracle accuracy is much higherthan using single parsers (97.52% vs. 92.85% onEnglish, and 95.06% vs. 82.46% on Chinese).Although using less unlabeled sentences (0.7Mfor English and 1.2M for Chinese), tri-trainingachieves comparable performance to co-training(slightly better on English and slightly worse onChinese).The third major row shows the results ofthe semi-supervised GParser with our proposedapproach.
We experiment with different com-binations of the 1-best parse trees of the threesupervised parsers.
The first three experimentscombine 1-best outputs of two parsers to composeparse forest on unlabeled data.
?Unlabeled ?B+(Z?G)?
means that the parse forest is initializedwith the Berkeley parse and augmented with theintersection of dependencies of the 1-best outputsof ZPar and GParser.
In the last setting, the parseforest contains all three 1-best results.When the parse forests of the unlabeled dataare the union of the outputs of GParser and ZPar,denoted as ?Unlabeled ?
Z+G?, each word has1.053 candidate heads on English and 1.136 onChinese, and the oracle accuracy is higher thanusing 1-best outputs of single parsers (94.97%vs.
92.85% on English, 86.66% vs. 82.46%on Chinese).
However, we find that althoughthe parser significantly outperforms the supervisedGParser on English, it does not gain significant im-provement over co-training with ZPar (?Unlabeled?
Z?)
on both English and Chinese.Combining the outputs of Berkeley Parser andGParser (?Unlabeled ?
B+G?
), we get higheroracle score (96.37% on English and 89.72% onChinese) and higher syntactic divergence (1.085candidate heads per word on English, and 1.188on Chinese) than ?Unlabeled ?
Z+G?, whichverifies our earlier discussion that Berkeley Pars-er produces more different structures than ZPar.However, it leads to slightly worse accuracy thanco-training with Berkeley Parser (?Unlabeled ?B?).
This indicates that adding the outputs ofGParser itself does not help the model.Combining the outputs of Berkeley Parser andZPar (?Unlabeled ?
B+Z?
), we get the best per-formance on English, which is also significantlybetter than both co-training (?Unlabeled ?
B?
)and tri-training (?Unlabeled ?
B=Z?)
on bothEnglish and Chinese.
This demonstrates that ourproposed approach can better exploit unlabeleddata than traditional self/co/tri-training.
Moreanalysis and discussions are in Section 4.4.During experimental trials, we find that ?Unla-beled?B+(Z?G)?
can further boost performanceon Chinese.
A possible explanation is that byusing the intersection of the outputs of GParserand ZPar, the size of the parse forest is bettercontrolled, which is helpful considering that ZParperforms worse on this data than both BerkeleyParser and GParser.Adding the output of GParser itself (?Unlabeled?
B+Z+G?)
leads to accuracy drop, although theoracle score is higher (96.95% on English and91.50% on Chinese) than ?Unlabeled ?
B+Z?.We suspect the reason is that the model is likely todistribute the probability mass to these parse treesproduced by itself instead of those by BerkeleyParser or ZPar under this setting.463Sup SemiMcDonald and Pereira (2006) 91.5?Koo and Collins (2010) [higher-order] 93.04Zhang and McDonald (2012) [higher-order] 93.06Zhang and Nivre (2011) [higher-order] 92.9Koo et al (2008) [higher-order] 92.02 93.16Chen et al (2009) [higher-order] 92.40 93.16Suzuki et al (2009) [higher-order,cluster] 92.70 93.79Zhou et al (2011) [higher-order] 91.98 92.64Chen et al (2013) [higher-order] 92.76 93.77This work 92.34 93.19Table 4: UAS comparison on English test data.In summary, we can conclude that our proposedambiguity-aware ensemble training is significant-ly better than both the supervised approaches andthe semi-supervised approaches that use 1-bestparse trees.
Appropriately composing the forestparse, our approach outperforms the best results ofco-training or tri-training by 0.28% (93.78-93.50)on English and 0.92% (84.26-83.34) on Chinese.4.3 Comparison with Previous WorkWe adopt the best settings on development datafor semi-supervised GParser with our proposedapproach, and make comparison with previousresults on test data.
Table 4 shows the results.The first major row lists several state-of-the-art supervised methods.
McDonald and Pereira(2006) propose a second-order graph-based parser,but use a smaller feature set than our work.
Kooand Collins (2010) propose a third-order graph-based parser.
Zhang and McDonald (2012) ex-plore higher-order features for graph-based de-pendency parsing, and adopt beam search forfast decoding.
Zhang and Nivre (2011) proposea feature-rich transition-based parser.
All workin the second major row adopts semi-supervisedmethods.
The results show that our approachachieves comparable accuracy with most previoussemi-supervised methods.
Both Suzuki et al(2009) and Chen et al (2013) adopt the higher-order parsing model of Carreras (2007), and Suzu-ki et al (2009) also incorporate word clusterfeatures proposed by Koo et al (2008) in their sys-tem.
We expect our approach may achieve higherperformance with such enhancements, which weleave for future work.
Moreover, our methodmay be combined with other semi-supervised ap-proaches, since they are orthogonal in method-ology and utilize unlabeled data from differentperspectives.Table 5 make comparisons with previous resultsUASSupervisedLi et al (2012) [joint] 82.37Bohnet and Nivre (2012) [joint] 81.42Chen et al (2013) [higher-order] 81.01This work 81.14Semi Chen et al (2013) [higher-order] 83.08This work 82.89Table 5: UAS comparison on Chinese test data.Unlabeled data UAS #Sent Len Head/Word OracleNULL 92.34 0 ?
?
?Consistent (tri-train) 92.94 0.7M 18.25 1.000 97.65Low divergence 92.94 0.5M 28.19 1.062 96.53High divergence 93.03 0.5M 27.85 1.211 94.28ALL 93.19 1.7M 24.15 1.087 96.09Table 6: Performance of our semi-supervisedGParser with different sets of ?Unlabeled ?B+Z?
on English test set.
?Len?
means averagedsentence length.on Chinese test data.
Li et al (2012) and Bohnetand Nivre (2012) use joint models for POS taggingand dependency parsing, significantly outperform-ing their pipeline counterparts.
Our approach canbe combined with their work to utilize unlabeleddata to improve both POS tagging and parsingsimultaneously.
Our work achieves comparableaccuracy with Chen et al (2013), although theyadopt the higher-order model of Carreras (2007).Again, our method may be combined with theirwork to achieve higher performance.4.4 AnalysisTo better understand the effectiveness of our pro-posed approach, we make detailed analysis usingthe semi-supervised GParser with ?Unlabeled ?B+Z?
on English datasets.Contribution of unlabeled data with regardto syntactic divergence: We divide the unlabeleddata into three sets according to the divergence ofthe 1-best outputs of Berkeley Parser and ZPar.The first set contains those sentences that the twoparsers produce identical parse trees, denoted by?consistent?, which corresponds to the setting fortri-training.
Other sentences are split into two setsaccording to averaged number of heads per wordin parse forests, denoted by ?low divergence?
and?high divergence?
respectively.
Then we trainsemi-supervised GParser using the three sets ofunlabeled data.
Table 6 illustrates the results andstatistics.
We can see that unlabeled data withidentical outputs from Berkeley Parser and ZPartends to be short sentences (18.25 words per sen-464tence on average).
Results show all the three setsof unlabeled data can help the parser.
Especially,the unlabeled data with highly divergent struc-tures leads to slightly higher improvement.
Thisdemonstrates that our approach can better exploitunlabeled data on which parsers of different viewsproduce divergent structures.Impact of unlabeled data size: To under-stand how our approach performs with regards tothe unlabeled data size, we train semi-supervisedGParser with different sizes of unlabeled data.
Fig.3 shows the accuracy curve on the test set.
Wecan see that the parser consistently achieves higheraccuracy with more unlabeled data, demonstratingthe effectiveness of our approach.
We expectthat our approach has potential to achieve higheraccuracy with more additional data.92.392.492.592.692.792.892.99393.193.20 50K 100K 200K 500K 1M 1.7MUASUnlabeled Data SizeB+Z ParserFigure 3: Performance of GParser with differentsizes of ?Unlabeled?
B+Z?
on English test set.5 Related WorkOur work is originally inspired by the work ofTa?ckstro?m et al (2013).
They first apply theidea of ambiguous labelings to multilingual parsertransfer in the unsupervised parsing field, whichaims to build a dependency parser for a resource-poor target language by making use of source-language treebanks.
Different from their work, weexplore the idea for semi-supervised dependencyparsing where a certain amount of labeled trainingdata is available.
Moreover, we for the firsttime build a state-of-the-art CRF-based depen-dency parser and conduct in-depth comparisonswith previous methods.
Similar ideas of learningwith ambiguous labelings are previously exploredfor classification (Jin and Ghahramani, 2002) andsequence labeling problems (Dredze et al, 2009).Our work is also related with the parser ensem-ble approaches such as stacked learning and re-parsing in the supervised track.
Stacked learninguses one parser?s outputs as guide features foranother parser, leading to improved performance(Nivre and McDonald, 2008; Torres Martins etal., 2008).
Re-parsing merges the outputs ofseveral parsers into a dependency graph, and thenapply Viterbi decoding to find a better tree (Sagaeand Lavie, 2006; Surdeanu and Manning, 2010).One possible drawback of parser ensemble is thatseveral parsers are required to parse the samesentence during the test phase.
Moreover, ourapproach can benefit from these methods in thatwe can get parse forests of higher quality onunlabeled data (Zhou, 2009).6 ConclusionsThis paper proposes a generalized trainingframework of semi-supervised dependencyparsing based on ambiguous labelings.
Foreach unlabeled sentence, we combine the 1-bestparse trees of several diverse parsers to composeambiguous labelings, represented by a parseforest.
The training objective is to maximize themixed likelihood of both the labeled data andthe auto-parsed unlabeled data with ambiguouslabelings.
Experiments show that our frameworkcan make better use of the unlabeled data,especially those with divergent outputs fromdifferent parsers, than traditional tri-training.Detailed analysis demonstrates the effectivenessof our approach.
Specifically, we find that ourapproach is very effective when using divergentparsers such as the generative parser, and it is alsohelpful to properly balance the size and oracleaccuracy of the parse forest of the unlabeled data.For future work, among other possibleextensions, we would like to see how ourapproach performs when employing more diverseparsers to compose the parse forest of higherquality for the unlabeled data, such as the easy-first non-directional dependency parser (Goldbergand Elhadad, 2010) and other constituent parsers(Collins and Koo, 2005; Charniak and Johnson,2005; Finkel et al, 2008).AcknowledgmentsThe authors would like to thank the criticaland insightful comments from our anonymousreviewers.
This work was supported by NationalNatural Science Foundation of China (Grant No.61373095, 61333018).465ReferencesMohit Bansal and Dan Klein.
2011.
Web-scalefeatures for full-scale parsing.
In Proceedings ofACL, pages 693?702.Avrim Blum and Tom Mitchell.
1998.
Combininglabeled and unlabeled data with co-training.
InProceedings of the 11th Annual Conference onComputational Learning Theory, pages 92?100.Bernd Bohnet and Joakim Nivre.
2012.
A transition-based system for joint part-of-speech tagging andlabeled non-projective dependency parsing.
InProceedings of EMNLP 2012, pages 1455?1465.Bernd Bohnet.
2010.
Top accuracy and fastdependency parsing is not a contradiction.
InProceedings of COLING, pages 89?97.Xavier Carreras.
2007.
Experiments with a higher-order projective dependency parser.
In Proceedingsof EMNLP/CoNLL, pages 141?150.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminativereranking.
In Proceedings of ACL, pages 173?180.Eugene Charniak, Don Blaheta, Niyu Ge, Keith Hall,John Hale, and Mark Johnson.
2000.
BLLIP1987-89 WSJ Corpus Release 1, LDC2000T43.Linguistic Data Consortium.Wenliang Chen, Jun?ichi Kazama, Kiyotaka Uchimoto,and Kentaro Torisawa.
2009.
Improvingdependency parsing with subtrees from auto-parseddata.
In Proceedings of EMNLP, pages 570?579.Wenliang Chen, Min Zhang, and Yue Zhang.
2013.Semi-supervised feature transformation for depen-dency parsing.
In Proceedings of EMNLP, pages1303?1313.Michael J. Collins and Terry Koo.
2005.
Dis-criminative reranking for natural language parsing.Computational Linguistics, pages 25?70.Mark Dredze, Partha Pratim Talukdar, and KobyCrammer.
2009.
Sequence learning from datawith multiple labels.
In ECML/PKDD Workshop onLearning from Multi-Label Data.Xiangyu Duan, Jun Zhao, and Bo Xu.
2007.
Proba-bilistic models for action-based Chinese dependencyparsing.
In Proceedings of ECML/ECPPKDD,pages 559?566.Jenny Rose Finkel, Alex Kleeman, and Christopher D.Manning.
2008.
Efficient, feature-based, condition-al random field parsing.
In Proceedings of ACL,pages 959?967.Yoav Goldberg and Michael Elhadad.
2010.
Anefficient algorithm for easy-first non-directionaldependency parsing.
In Proceedings of NAACL.Zhongqiang Huang and Mary Harper.
2009.
Self-training PCFG grammars with latent annotationsacross languages.
In Proceedings of EMNLP 2009,pages 832?841.Chu-Ren Huang.
2009.
Tagged Chinese GigawordVersion 2.0, LDC2009T14.
Linguistic DataConsortium.Rong Jin and Zoubin Ghahramani.
2002.
Learningwith multiple labels.
In Proceedings of NIPS.Terry Koo and Michael Collins.
2010.
Efficient third-order dependency parsers.
In ACL, pages 1?11.Terry Koo, Xavier Carreras, and Michael Collins.2008.
Simple semi-supervised dependency parsing.In Proceedings of ACL, pages 595?603.Zhenghua Li, Min Zhang, Wanxiang Che, and TingLiu.
2012.
A separately passive-aggressive trainingalgorithm for joint POS tagging and dependencyparsing.
In COLING 2012, pages 1681?1698.David McClosky, Eugene Charniak, and Mark John-son.
2006.
Effective self-training for parsing.
InProceedings of the Human Language TechnologyConference of the NAACL, pages 152?159.Ryan McDonald and Fernando Pereira.
2006.Online learning of approximate dependency parsingalgorithms.
In Proceedings of EACL, pages 81?88.Ryan McDonald, Koby Crammer, and FernandoPereira.
2005.
Online large-margin training ofdependency parsers.
In Proceedings of ACL, pages91?98.Joakim Nivre and Ryan McDonald.
2008.
Integrat-ing graph-based and transition-based dependencyparsers.
In Proceedings of ACL, pages 950?958.Joakim Nivre.
2003.
An efficient algorithm forprojective dependency parsing.
In Proceedings ofIWPT, pages 149?160.Eric W. Noreen.
1989.
Computer-intensive methodsfor testing hypotheses: An introduction.
John Wiley& Sons, Inc., New York.Slav Petrov and Dan Klein.
2007.
Improvedinference for unlexicalized parsing.
In Proceedingsof NAACL.Kenji Sagae and Alon Lavie.
2006.
Parsercombination by reparsing.
In Proceedings ofNAACL, pages 129?132.Kenji Sagae and Jun?ichi Tsujii.
2007.
Dependencyparsing and domain adaptation with LR modelsand parser ensembles.
In Proceedings of theCoNLL Shared Task Session of EMNLP-CoNLL,pages 1044?1050.David A. Smith and Jason Eisner.
2007.
Bootstrap-ping feature-rich dependency parsers with entropicpriors.
In Proceedings of EMNLP, pages 667?677.466Anders S?gaard and Christian Rish?j.
2010.
Semi-supervised dependency parsing using generalizedtri-training.
In Proceedings of ACL, pages 1065?1073.Kathrin Spreyer and Jonas Kuhn.
2009.
Data-driven dependency parsing of new languages usingincomplete and noisy training data.
In CoNLL,pages 12?20.Mihai Surdeanu and Christopher D. Manning.
2010.Ensemble models for dependency parsing: Cheapand good?
In Proceedings of NAACL, pages 649?652.Jun Suzuki, Hideki Isozaki, Xavier Carreras, andMichael Collins.
2009.
An empirical study ofsemi-supervised structured conditional models fordependency parsing.
In Proceedings of EMNLP,pages 551?560.Oscar Ta?ckstro?m, Ryan McDonald, and Joakim Nivre.2013.
Target language adaptation of discriminativetransfer parsers.
In Proceedings of NAACL, pages1061?1071.Andre?
Filipe Torres Martins, Dipanjan Das, Noah A.Smith, and Eric P. Xing.
2008.
Stackingdependency parsers.
In Proceedings of EMNLP,pages 157?166.Qin Iris Wang, Dale Schuurmans, and DekangLin.
2008.
Semi-supervised convex training fordependency parsing.
In Proceedings of ACL, pages532?540.Hiroyasu Yamada and Yuji Matsumoto.
2003.Statistical dependency analysis with support vectormachines.
In Proceedings of IWPT, pages 195?206.David Yarowsky.
1995.
Unsupervised word sensedisambiguation rivaling supervised methods.
InProceedings of ACL, pages 189?196.Hao Zhang and Ryan McDonald.
2012.
Generalizedhigher-order dependency parsing with cube pruning.In Proceedings of EMNLP-CoNLL, pages 320?331.Yue Zhang and Joakim Nivre.
2011.
Transition-baseddependency parsing with rich non-local features.
InProceedings of ACL, pages 188?193.Zhi-Hua Zhou and Ming Li.
2005.
Tri-training:Exploiting unlabeled data using three classifiers.In IEEE Transactions on Knowledge and DataEngineering, pages 1529?1541.Guangyou Zhou, Jun Zhao, Kang Liu, and Li Cai.2011.
Exploiting web-derived selectional prefer-ence to improve statistical dependency parsing.
InProceedings of ACL, pages 1556?1565.Zhi-Hua Zhou.
2009.
When semi-supervised learningmeets ensemble learning.
In MCS.467
