Automatic Learning of Language Model StructureKevin Duh and Katrin KirchhoffDepartment of Electrical EngineeringUniversity of Washington, Seattle, USA{duh,katrin}@ee.washington.eduAbstractStatistical language modeling remains a challeng-ing task, in particular for morphologically rich lan-guages.
Recently, new approaches based on factoredlanguage models have been developed to addressthis problem.
These models provide principled waysof including additional conditioning variables otherthan the preceding words, such as morphological orsyntactic features.
However, the number of possiblechoices for model parameters creates a large space ofmodels that cannot be searched exhaustively.
Thispaper presents an entirely data-driven model selec-tion procedure based on genetic search, which isshown to outperform both knowledge-based and ran-dom selection procedures on two different languagemodeling tasks (Arabic and Turkish).1 IntroductionIn spite of novel algorithmic developments andthe increased availability of large text corpora,statistical language modeling remains a diffi-cult problem, particularly for languages withrich morphology.
Such languages typically ex-hibit a large number of word types in relationto word tokens in a given text, which leadsto high perplexity and a large number of un-seen word contexts.
As a result, probability es-timates are often unreliable, even when usingstandard smoothing and parameter reductiontechniques.
Recently, a new language model-ing approach, called factored language models(FLMs), has been developed.
FLMs are a gen-eralization of standard language models in thatthey allow a larger set of conditioning variablesfor predicting the current word.
In addition tothe preceding words, any number of additionalvariables can be included (e.g.
morphological,syntactic, or semantic word features).
Sincesuch features are typically shared across mul-tiple words, they can be used to obtained bet-ter smoothed probability estimates when train-ing data is sparse.
However, the space of pos-sible models is extremely large, due to manydifferent ways of choosing subsets of condition-ing word features, backoff procedures, and dis-counting methods.
Usually, this space cannotbe searched exhaustively, and optimizing mod-els by a knowledge-inspired manual search pro-cedure often leads to suboptimal results sinceonly a small portion of the search space canbe explored.
In this paper we investigate thepossibility of determining the structure of fac-tored language models (i.e.
the set of condition-ing variables, the backoff procedure and the dis-counting parameters) by a data-driven searchprocedure, viz.
Genetic Algorithms (GAs).
Weapply this technique to two different tasks (lan-guage modeling for Arabic and Turkish) andshow that GAs lead to better models than ei-ther knowledge-inspired manual search or ran-dom search.
The remainder of this paper isstructured as follows: Section 2 describes thedetails of the factored language modeling ap-proach.
The application of GAs to the problemof determining language model structure is ex-plained in Section 3.
The corpora used in thepresent study are described in Section 4 and ex-periments and results are presented in Section 5.Section 6 compares the present study to relatedwork and Section 7 concludes.2 Factored Language ModelsA standard statistical language model com-putes the probability of a word sequence W =w1, w2, ..., wT as a product of conditional prob-abilities of each word wi given its history, whichis typically approximated by just one or two pre-ceding words (leading to bigrams, and trigrams,respectively).
Thus, a trigram language modelis described byp(w1, ..., wT ) ?T?i=3p(wi|wi?1, wi?2) (1)Even with this limitation, the estimation ofthe required probabilities is challenging: manyword contexts may be observed infrequentlyor not at all, leading to unreliable probabil-ity estimates under maximum likelihood estima-tion.
Several techniques have been developedto address this problem, in particular smooth-ing techniques (Chen and Goodman, 1998) andclass-based language models (Brown and oth-ers, 1992).
In spite of such parameter reduc-tion techniques, language modeling remains adifficult task, in particular for morphologicallyrich languages, e.g.
Turkish, Russian, or Arabic.Such languages have a large number of wordtypes in relation to the number of word tokensin a given text, as has been demonstrated ina number of previous studies (Geutner, 1995;Kiecza et al, 1999; Hakkani-Tu?r et al, 2002;Kirchhoff et al, 2003).
This in turn resultsin a high perplexity and in a large number ofout-of-vocabulary (OOV) words when applyinga trained language model to a new unseen text.2.1 Factored Word RepresentationsA recently developed approach that addressesthis problem is that of Factored Language Mod-els (FLMs) (Kirchhoff et al, 2002; Bilmes andKirchhoff, 2003), whose basic idea is to decom-pose words into sets of features (or factors) in-stead of viewing them as unanalyzable wholes.Probabilistic language models can then be con-structed over (sub)sets of word features insteadof, or in addition to, the word variables them-selves.
For instance, words can be decomposedinto stems/lexemes and POS tags indicatingtheir morphological features, as shown below:Word: Stock prices are risingStem: Stock price be riseTag: Nsg N3pl V3pl VpartSuch a representation serves to express lexicaland syntactic generalizations, which would oth-erwise remain obscured.
It is comparable toclass-based representations employed in stan-dard class-based language models; however, inFLMs several simultaneous class assignmentsare allowed instead of a single one.
In general,we assume that a word is equivalent to a fixednumber (K) of factors, i.e.
W ?
f1:K .
The taskthen is to produce a statistical model over theresulting representation - using a trigram ap-proximation, the resulting probability model isas follows:p(f1:K1 , f1:K2 , ..., f1:KT ) ?T?t=3p(f1:Kt |f1:Kt?1 , f1:Kt?2 )(2)Thus, each word is dependent not only on a sin-gle stream of temporally ordered word variables,but also on additional parallel (i.e.
simultane-ously occurring) features.
This factored repre-sentation can be used in two different ways toimprove over standard LMs: by using a productmodel or a backoff model.
In a product model,Equation 2 can be simplified by finding con-ditional independence assumptions among sub-sets of conditioning factors and computing thedesired probability as a product of individualmodels over those subsets.
In this paper weonly consider the second option, viz.
using thefactors in a backoff procedure when the wordn-gram is not observed in the training data.For instance, a word trigram that is found inan unseen test set may not have any counts inthe training set, but its corresponding factors(e.g.
stems and morphological tags) may havebeen observed since they also occur in otherwords.2.2 Generalized parallel backoffBackoff is a common smoothing technique inlanguage modeling.
It is applied wheneverthe count for a given n-gram in the trainingdata falls below a certain threshold ?
.
In thatcase, the maximum-likelihood estimate of then-gram probability is replaced with a probabil-ity derived from the probability of the lower-order (n ?
1)-gram and a backoff weight.
N-grams whose counts are above the threshold re-tain their maximum-likelihood estimates, dis-counted by a factor that re-distributes proba-bility mass to the lower-order distribution:pBO(wt|wt?1, wt?2) (3)={dcpML(wt|wt?1, wt?2) if c > ?3?
(wt?1, wt?2)pBO(wt|wt?1) otherwisewhere c is the count of (wt, wt?1, wt?2), pMLdenotes the maximum-likelihood estimate anddc is a discounting factor that is applied to thehigher-order distribution.
The way in which thediscounting factor is estimated determines theactual smoothing method (e.g.
Good-Turing,Kneser-Ney, etc.)
The normalization factor?
(wt?1, wt?2) ensures that the entire distribu-tion sums to one.
During standard backoff, themost distant conditioning variable (in this casewt?2) is dropped first, then the second most dis-tant variable etc.
until the unigram is reached.This can be visualized as a backoff path (Fig-ure 1(a)).
If the only variables in the model arewords, such a backoff procedure is reasonable.tW 1tW?
2tW?
3tW?tW 1tW?
2tW?tW 1tW?tW(a)F 1F 2F 3FFF 1F 2F F 1F 3F F 2F 3FF 1F F 3FF 2F(b)Figure 1: Standard backoff path for a 4-gram lan-guage model over words (left) and backoff graph for4-gram over factors (right).However, if variables occur in parallel, i.e.
donot form a temporal sequence, it is not imme-diately obvious in which order they should bedropped.
In this case, several backoff paths arepossible, which can be summarized in a backoffgraph (Figure 1(b)).
In principle, there are sev-eral different ways of choosing among differentpaths in this graph:1.
Choose a fixed, predetermined backoff pathbased on linguistic knowledge, e.g.
always dropsyntactic before morphological variables.2.
Choose the path at run-time based on statis-tical criteria.3.
Choose multiple paths and combine theirprobability estimates.The last option, referred to as parallel backoff,is implemented via a new, generalized backofffunction (here shown for a 4-gram):pGBO(f |f1, f2, f3) (4)={dcpML(f |f1, f2, f3) if c > ?4?
(f1, f2, f3)g(f, f1, f2, f3) otherwisewhere c is the count of (f, f1, f2, f3),pML(f |f1, f2, f3) is the maximum likeli-hood distribution, ?4 is the count threshold,and ?
(f1, f2, f3) is the normalization factor.The function g(f, f1, f2, f3) determines thebackoff strategy.
In a typical backoff proce-dure g(f, f1, f2, f3) equals pBO(f |f1, f2).
Ingeneralized parallel backoff, however, g can beany non-negative function of f, f1, f2, f3.
Inour implementation of FLMs (Kirchhoff et al,2003) we consider several different g functions,including the mean, weighted mean, product,and maximum of the smoothed probabilitydistributions over all subsets of the conditioningfactors.
In addition to different choices for g,different discounting parameters can be chosenat different levels in the backoff graph.
Forinstance, at the topmost node, Kneser-Neydiscounting might be chosen whereas at alower node Good-Turing might be applied.FLMs have been implemented as an add-onto the widely-used SRILM toolkit1 and havebeen used successfully for the purpose ofmorpheme-based language modeling (Bilmesand Kirchhoff, 2003), multi-speaker languagemodeling (Ji and Bilmes, 2004), and speechrecognition (Kirchhoff et al, 2003).3 Learning FLM StructureIn order to use an FLM, three types of para-meters need to be specified: the initial con-ditioning factors, the backoff graph, and thesmoothing options.
The goal of structure learn-ing is to find the parameter combinations thatcreate FLMs that achieve a low perplexity onunseen test data.
The resulting model spaceis extremely large: given a factored word rep-resentation with a total of k factors, there are?kn=1(kn)possible subsets of initial condition-ing factors.
For a set of m conditioning factors,there are up to m!
backoff paths, each with itsown smoothing options.
Unless m is very small,exhaustive search is infeasible.
Moreover, non-linear interactions between parameters make itdifficult to guide the search into a particulardirection, and parameter sets that work wellfor one corpus cannot necessarily be expectedto perform well on another.
We therefore needan automatic way of identifying the best modelstructure.
In the following section, we describethe application of genetic-based search to thisproblem.3.1 Genetic AlgorithmsGenetic Algorithms (GAs) (Holland, 1975) are aclass of evolution-inspired search/optimizationtechniques.
They perform particularly wellin problems with complex, poorly understoodsearch spaces.
The fundamental idea of GAs isto encode problem solutions as (usually binary)strings (genes), and to evolve and test successivepopulations of solutions through the use of ge-netic operators applied to the encoded strings.Solutions are evaluated according to a fitnessfunction which represents the desired optimiza-tion criterion.
The individual steps are as fol-1We would like to thank Jeff Bilmes for providing andsupporting the software.lows:Initialize: Randomly generate a set (popula-tion) of strings.While fitness improves by a certain threshold:Evaluate fitness: calculate each string?s fitnessApply operators: apply the genetic operatorsto create a new population.The genetic operators include the probabilis-tic selection of strings for the next genera-tion, crossover (exchanging subparts of differ-ent strings to create new strings), and muta-tion (randomly altering individual elements instrings).
Although GAs provide no guaranteeof finding the optimal solution, they often findgood solutions quickly.
By maintaining a pop-ulation of solutions rather than a single solu-tion, GA search is robust against prematureconvergence to local optima.
Furthermore, solu-tions are optimized based on a task-specific fit-ness function, and the probabilistic nature of ge-netic operators helps direct the search towardspromising regions of the search space.3.2 Structure Search Using GAIn order to use GAs for searching over FLMstructures (i.e.
combinations of conditioningvariables, backoff paths, and discounting op-tions), we need to find an appropriate encodingof the problem.Conditioning factorsThe initial set of conditioning factors F areencoded as binary strings.
For instance, atrigram for a word representation with threefactors (A,B,C) has six conditioning variables:{A?1, B?1, C?1, A?2, B?2, C?2} which can berepresented as a 6-bit binary string, with a bitset to 1 indicating presence and 0 indicating ab-sence of a factor in F .
The string 10011 wouldcorrespond to F = {A?1, B?2, C?2}.Backoff graphThe encoding of the backoff graph is more dif-ficult because of the large number of possiblepaths.
A direct approach encoding every edgeas a bit would result in overly long strings, ren-dering the search inefficient.
Our solution is toencode a binary string in terms of graph gram-mar rules (similar to (Kitano, 1990)), whichcan be used to describe common regularities inbackoff graphs.
For instance, a node with mfactors can only back off to children nodes withm ?
1 factors.
For m = 3, the choices for pro-ceeding to the next-lower level in the backoff1.
{X1 X2 X3} ?> {X1 X2}2.
{X1 X2 X3} ?> {X1 X3}3.
{X1 X2 X3} ?> {X2 X3}4.
{X1 X2}  ?> {X1}5.
{X1 X2}  ?> {X2}PRODUCTION RULES:ABABCABABCBC ABABCBCA BABABCBCA B  014 4101103(b) Generation of Backoff Graph by rules 1, 3, and 4(a) Gene activates production rulesGENE:Figure 2: Generation of Backoff Graph from pro-duction rules selected by the gene 10110.graph can thus be described by the followinggrammar rules:RULE 1: {x1, x2, x3} ?
{x1, x2}RULE 2: {x1, x2, x3} ?
{x1, x3}RULE 3: {x1, x2, x3} ?
{x2, x3}Here xi corresponds to the factor at the ithposition in the parent node.
Rule 1 indicatesa backoff that drops the third factor, Rule 2drops the second factor, etc.
The choice ofrules used to generate the backoff graph is en-coded in a binary string, with 1 indicating theuse and 0 indicating the non-use of a rule, asshown schematically in Figure 2.
The presenceof two different rules at the same level in thebackoff graph corresponds to parallel backoff;the absence of any rule (strings consisting onlyof 0 bits) implies that the corresponding backoffgraph level is skipped and two conditioning vari-ables are dropped simultaneously.
This allowsus to encode a graph using few bits but does notrepresent all possible graphs.
We cannot selec-tively apply different rules to different nodes atthe same level ?
this would essentially requirea context-sensitive grammar, which would inturn increase the length of the encoded strings.This is a fundamental tradeoff between the mostgeneral representation and an encoding that istractable.
Our experimental results describedbelow confirm, however, that sufficiently goodresults can be obtained in spite of the abovelimitation.Smoothing optionsSmoothing options are encoded as tuples of in-tegers.
The first integer specifies the discount-ing method while second indicates the minimumcount required for the n-gram to be included inthe FLM.
The integer string consists of succes-sive concatenated tuples, each representing thesmoothing option at a node in the graph.
TheGA operators are applied to concatenations ofall three substrings describing the set of factors,backoff graph, and smoothing options, such thatall parameters are optimized jointly.4 DataWe tested our language modeling algorithms ontwo different data sets from two different lan-guages, Arabic and Turkish.The Arabic data set was drawn from theCallHome Egyptian Conversational Arabic(ECA) corpus (LDC, 1996).
The training,development, and evaluation sets containapproximately 170K, 32K, and 18K words,respectively.
The corpus was collected for thepurpose of speech recognizer development forconversational Arabic, which is mostly dialectaland does not have a written standard.
Noadditional text material beyond transcriptionsis available in this case; it is therefore im-portant to use language models that performwell in sparse data conditions.
The factoredrepresentation was constructed using linguisticinformation from the corpus lexicon, in combi-nation with automatic morphological analysistools.
It includes, in addition to the word, thestem, a morphological tag, the root, and thepattern.
The latter two are components whichwhen combined form the stem.
An exampleof this factored word representation is shownbelow:Word:il+dOr/Morph:noun+masc-sg+article/Stem:dOr/Root:dwr/Pattern:CCCFor our Turkish experiments we used a mor-phologically annotated corpus of Turkish(Hakkani-Tu?r et al, 2000).
The annotationwas performed by applying a morphologicalanalyzer, followed by automatic morphologicaldisambiguation as described in (Hakkani-Tu?ret al, 2002).
The morphological tags consistof the initial root, followed by a sequence ofinflectional groups delimited by derivationboundaries (?DB).
A sample annotation (forthe word yararlanmak, consisting of the rootyarar plus three inflectional groups) is shownbelow:yararmanlak:yarar+Noun+A3sg+Pnon+Nom?DB+Verb+Acquire+Pos?DB+Noun+Inf+A3sg+Pnon+NomWe removed segmentation marks (for titlesand paragraph boundaries) from the corpusbut included punctuation.
Words may havedifferent numbers of inflectional groups, butthe FLM representation requires the samenumber of factors for each word; we thereforehad to map the original morphological tags toa fixed-length factored representation.
Thiswas done using linguistic knowledge: accordingto (Oflazer, 1999), the final inflectional groupin each dependent word has a special statussince it determines inflectional markings onhead words following the dependent word.The final inflectional group was thereforeanalyzed into separate factors indicating thenumber (N), case (C), part-of-speech (P) andall other information (O).
Additional factorsfor the word are the root (R) and all remaininginformation in the original tag not subsumedby the other factors (G).
The word itself isused as another factor (W).
Thus, the aboveexample would be factorized as follows:W:yararlanmak/R:yarar/P:NounInf-N:A3sg/C:Nom/O:Pnon/G:NounA3sgPnonNom+Verb+Acquire+PosOther factorizations are certainly possible;however, our primary goal is not to find thebest possible encoding for our data but todemonstrate the effectiveness of the FLMapproach, which is largely independent of thechoice of factors.
For our experiments we usedsubsets of 400K words for training, 102K wordsfor development and 90K words for evaluation.5 Experiments and ResultsIn our application of GAs to language modelstructure search, the perplexity of models withrespect to the development data was used asan optimization criterion.
The perplexity ofthe best models found by the GA were com-pared to the best models identified by a lengthymanual search procedure using linguistic knowl-edge about dependencies between the word fac-tors involved, and to a random search procedurewhich evaluated the same number of strings asthe GA.
The following GA options gave goodresults: population size 30-50, crossover proba-bility 0.9, mutation probability 0.01, StochasticUniversal Sampling as the selection operator, 2-point crossover.
We also experimented with re-initializing the GA search with the best modelfound in previous runs.
This method consis-tently improved the performance of normal GAsearch and we used it as the basis for the resultsreported below.
Due to the large number of fac-N Word Hand Rand GA ?
(%)Dev Set2 593.8 555.0 556.4 539.2 -2.93 534.9 533.5 497.1 444.5 -10.64 534.8 549.7 566.5 522.2 -5.0Eval Set2 609.8 558.7 525.5 487.8 -7.23 545.4 583.5 509.8 452.7 -11.24 543.9 559.8 574.6 527.6 -5.8Table 1: Perplexity for Turkish language models.
N= n-gram order, Word = word-based models, Hand= manual search, Rand = random search, GA =genetic search.tors in the Turkish word representation, modelswere only optimized for conditioning variablesand backoff paths, but not for smoothing op-tions.
Table 1 compares the best perplexity re-sults for standard word-based models and forFLMs obtained using manual search (Hand),random search (Rand), and GA search (GA).The last column shows the relative change inperplexity for the GA compared to the betterof the manual or random search models.
Fortests on both the development set and evalu-ation set, GA search gave the lowest perplex-ity.
In the case of Arabic, the GA search wasN Word Hand Rand GA ?
(%)Dev Set2 229.9 229.6 229.9 222.9 -2.93 229.3 226.1 230.3 212.6 -6.0Eval Set2 249.9 230.1 239.2 223.6 -2.83 285.4 217.1 224.3 206.2 -5.0Table 2: Perplexity for Arabic language models(w/o unknown words).performed over conditioning factors, the back-off graph, and smoothing options.
The resultsin Table 2 were obtained by training and test-ing without consideration of out-of-vocabulary(OOV) words.
Our ultimate goal is to use theselanguage models in a speech recognizer with afixed vocabulary, which cannot recognize OOVwords but requires a low perplexity for otherN Word Hand Rand GA ?
(%)Dev Set2 236.0 195.5 198.5 193.3 -1.13 237.0 199.0 202.0 188.1 -5.5Eval Set2 235.2 234.1 247.7 233.4 -0.73 253.9 229.2 219.0 212.2 -3.1Table 3: Perplexity for Arabic language models(with unknown words).word combinations.
In a second experiment,we trained the same FLMs from Table 2 withOOV words included as the unknown word to-ken.
Table 3 shows the results.
Again, we seethat the GA outperforms other search methods.The best language models all used parallel back-off and different smoothing options at differentbackoff graph nodes.
The Arabic models madeuse of all conditioning variables (Word, Stem,Root, Pattern, and Morph) whereas the Turkishmodels used only the W, P, C, and R variables(see above Section 4).6 Related WorkVarious previous studies have investigated thefeasibility of using units other than words forlanguage modeling (e.g.
(Geutner, 1995; C?arkiet al, 2000; Kiecza et al, 1999)).
However,in all of these studies words were decomposedinto linear sequences of morphs or morph-likeunits, using either linguistic knowledge or data-driven techniques.
Standard language modelswere then trained on the decomposed represen-tations.
The resulting models essentially ex-press statistical relationships between morphs,such as stems and affixes.
For this reason, acontext larger than that provided by a trigramis typically required, which quickly leads todata-sparsity.
In contrast to these approaches,factored language models encode morphologicalknowledge not by altering the linear segmenta-tion of words but by encoding words as parallelbundles of features.The general possibility of using multiple con-ditioning variables (including variables otherthan words) has also been investigated by(Dupont and Rosenfeld, 1997; Gildea, 2001;Wang, 2003; Zitouni et al, 2003).
Mostly, theadditional variables were general word classesderived by data-driven clustering procedures,which were then arranged in a backoff latticeor graph similar to the present procedure.
Allof these studies assume a fixed path throughthe graph, which is usually obtained by anordering from more specific probability distri-butions to more general distributions.
Someschemes also allow two or more paths to becombined by weighted interpolation.
FLMs, bycontrast, allow different paths to be chosen atrun-time, they support a wider range of combi-nation methods for probability estimates fromdifferent paths, and they offer a choice of dif-ferent discounting options at every node in thebackoff graph.
Most importantly, however, thepresent study is to our knowledge the first todescribe an entirely data-driven procedure foridentifying the best combination of parameterchoices.
The success of this method will facili-tate the rapid development of FLMs for differ-ent tasks in the future.7 ConclusionsWe have presented a data-driven approach tothe selection of parameters determining thestructure and performance of factored languagemodels, a class of models which generalizesstandard language models by including addi-tional conditioning variables in a principledway.
In addition to reductions in perplexity ob-tained by FLMs vs. standard language models,the data-driven model section method furtherimproved perplexity and outperformed bothknowledge-based manual search and randomsearch.AcknowledgmentsWe would like to thank Sonia Parandekar for the ini-tial version of the GA code.
This material is basedupon work supported by the NSF and the CIA un-der NSF Grant No.
IIS-0326276.
Any opinions, find-ings, and conclusions expressed in this material arethose of the authors and do not necessarily reflectthe views of these agencies.ReferencesJeff A. Bilmes and Katrin Kirchhoff.
2003.
Factoredlanguage models and generalized parallel backoff.In Proceedings of HLT/NACCL, pages 4?6.P.F.
Brown et al 1992.
Class-based n-gram modelsof natural language.
Computational Linguistics,18(4):467?479.K.
C?arki, P. Geutner, and T. Schultz.
2000.
TurkishLVCSR: towards better speech recognition for ag-glutinative languages.
In Proceedings of ICASSP.S.
F. Chen and J. Goodman.
1998.
An empiricalstudy of smoothing techniques for language mod-eling.
Technical Report Tr-10-98, Center for Re-search in Computing Technology, Harvard Uni-versity.P.
Dupont and R. Rosenfeld.
1997.
Lattice basedlanguage models.
Technical Report CMU-CS-97-173, Department of Computer Science, CMU.P.
Geutner.
1995.
Using morphology towards betterlarge-vocabulary speech recognition systems.
InProceedings of ICASSP, pages 445?448.D.
Gildea.
2001.
Statistical Language Understand-ing Using Frame Semantics.
Ph.D. thesis, Uni-versity of California, Berkeley.D.
Hakkani-Tu?r, K. Oflazer, and Go?khan Tu?r.
2000.Statistical morphological disambiguation for ag-glutinative languages.
In Proceedings of COL-ING.D.
Hakkani-Tu?r, K. Oflazer, and Go?khan Tu?r.
2002.Statistical morphological disambiguation for ag-glutinative languages.
Journal of Computers andHumanities, 36(4).J.H.
Holland.
1975.
Adaptation in Natural and Ar-tificial Systems.
University of Michigan Press.Gand Ji and Jeff Bilmes.
2004.
Multi-speaker lan-guage modeling.
In Proceedings of HLT/NAACL,pages 137?140.D.
Kiecza, T. Schultz, and A. Waibel.
1999.
Data-driven determination of appropriate dictionaryunits for Korean LVCSR.
In Proceedings ofICASSP, pages 323?327.K.
Kirchhoff et al 2002.
Novel speech recognitionmodels for Arabic.
Technical report, Johns Hop-kins University.K.
Kirchhoff et al 2003.
Novel approaches to Ara-bic speech recognition: Report from 2002 Johns-Hopkins summer workshop.
In Proceedings ofICASSP, pages I?344?I?347.Hiroaki Kitano.
1990.
Designing neural networksusing genetic algorithms with graph generationsystem.
Complex Systems, pages 461?476.LDC.
1996. http://www.ldc.upenn.edu/Catalog/-LDC99L22.html.K.
Oflazer.
1999.
Dependency parsing with an ex-tended finite state approach.
In Proceedings of the37th ACL.W.
Wang.
2003.
Factorization of languagemodels through backing off lattices.
Com-putation and Language E-print Archive,oai:arXiv.org/cs/0305041.I.
Zitouni, O. Siohan, and C.-H. Lee.
2003.
Hierar-chical class n-gram language models: towards bet-ter estimation of unseen events in speech recogni-tion.
In Proceedings of Eurospeech - Interspeech,pages 237?240.
