Proceedings of NAACL-HLT 2013, pages 814?819,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsCoherence Modeling for the Automated Assessment ofSpontaneous Spoken ResponsesXinhao Wang, Keelan Evanini, Klaus ZechnerEducational Testing Service660 Rosedale RoadPrinceton, NJ 08541, USAxwang002,kevanini,kzechner@ets.orgAbstractThis study focuses on modeling discourse co-herence in the context of automated assess-ment of spontaneous speech from non-nativespeakers.
Discourse coherence has alwaysbeen used as a key metric in human scoringrubrics for various assessments of spoken lan-guage.
However, very little research has beendone to assess a speaker's coherence in auto-mated speech scoring systems.
To addressthis, we present a corpus of spoken responsesthat has been annotated for discourse coher-ence quality.
Then, we investigate the use ofseveral features originally developed for es-says to model coherence in spoken responses.An analysis on the annotated corpus showsthat the prediction accuracy for human holisticscores of an automated speech scoring systemcan be improved by around 10% relative afterthe addition of the coherence features.
Fur-ther experiments indicate that a weighted F-Measure of 73% can be achieved for the au-tomated prediction of the coherence scores.1 IntroductionIn recent years, much research has been conductedinto developing automated assessment systems toautomatically score spontaneous speech from non-native speakers with the goals of reducing the bur-den on human raters, improving reliability, andgenerating feedback that can be used by languagelearners.
Various features related to different as-pects of speaking proficiency have been exploited,such as delivery features for pronunciation, proso-dy, and fluency (Strik and Cucchiarini, 1999; Chenet al 2009; Cheng, 2011; Higgins et al 2011), aswell as language use features for vocabulary andgrammar, and content features (Chen and Zechner,2011; Xie et al 2012).
However, discourse-levelfeatures related to topic development have rarelybeen investigated in the context of automatedspeech scoring.
This is despite the fact that an im-portant criterion in the human scoring rubrics forspeaking assessments is the evaluation of coher-ence, which refers to the conceptual relations be-tween different units within a response.Methods for automatically assessing discoursecoherence in text documents have been widelystudied in the context of applications such as natu-ral language generation, document summarization,and assessment of text readability.
For example,Foltz et al(1998) measured the overall coherenceof a text by utilizing Latent Semantic Analysis(LSA) to calculate the semantic relatedness be-tween adjacent sentences.
Barzilay and Lee (2004)introduced an HMM-based model for the docu-ment-level analysis of topics and topic transitions.Barzilay and Lapata (2005; 2008) presented anapproach to coherence modeling which focused onthe entities in the text and their grammatical transi-tions between adjacent sentences, and calculatedthe entity transition probabilities on the documentlevel.
Pitler et al(2010) provided a summary ofthe performance of several different types offeatures for automated coherence evaluation, suchas cohesive devices, adjacent sentence similarity,Coh-Metrix (Graesser et al 2004), word co-occurrence patterns, and entity-grid.In addition to studies on well-formed text, re-searchers have also addressed coherence modelingon text produced by language learners, which maycontain many spelling and grammar errors.Utilizing LSA and Random Indexing methods,Higgins et al(2004) measured the global814coherence of students?
essays by calculating thesemantic relatedness between sentences and thecorresponding prompts.
In addition, Burstein et.
al(2010) combined entity-grid features with writingquality features produced by an automated assess-ment system of essays to predict the coherencescores of student essays.
Recently, Yannakoudakisand Briscoe (2012) systematically analyzed a vari-ety of coherence modeling methods within theframework of an automated assessment system fornon-native free text responses and indicated thatfeatures based on Incremental Semantic Analysis(ISA), local histograms of words, the part-of-speech IBM model, and word length were the mosteffective.In contrast to these previous studies involvingwell-formed text or learner text containing errors,this paper focuses on modeling coherence in spon-taneous spoken responses as well as investigatingdiscourse features in an attempt to extend the con-struct coverage of an automated speech scoringsystem.
In a related study, Hassanali et al(2012)investigated coherence modeling for spoken lan-guage in the context of a story retelling task for theautomated diagnosis of children with language im-pairment.
They annotated transcriptions of chil-dren's narratives with coherence scores as well asmarkers of narrative structure and narrative quali-ty; furthermore they built models to predict thecoherence scores based on Coh-Metrix featuresand the manually annotated narrative features.
Thecurrent study differs from this one in that it dealswith free spontaneous spoken responses providedby students at a university level; these responsestherefore contain more varied and more complicat-ed information than the child narratives.The main contributions of this paper can besummarized as follows: First, we obtained coher-ence annotations on a corpus of spontaneous spo-ken responses drawn from a university-levelEnglish language proficiency assessment, anddemonstrated an improvement of around 10% rela-tive in the accuracy of the automated prediction ofhuman holistic scores with the addition of the co-herence annotations.
Second, we applied the entity-grid features and writing quality features from anautomated essay scoring system to predict the co-herence scores; the experimental results haveshown promising correlations between some ofthese features and the coherence scores.2 Data and Annotation2.1 DataFor this study, we collected 600 spoken responsesfrom the international TOEFL?
iBT assessment ofEnglish proficiency for non-native speakers.
100responses were drawn from each of 6 different testquestions comprising two different speaking tasks:1) providing an opinion based on personal experi-ence (N = 200) and 2) summarizing or discussingmaterial provided in a reading and/or listening pas-sage (N = 400).
The spoken responses were alltranscribed by humans with punctuation and capi-talization.
The average number of words containedin the responses was 104.4 (st. dev.
= 34.4) and theaverage number of sentences was 5.5 (st. dev.
=2.1).The spoken responses were all provided withholistic English proficiency scores on a scale of 1 -4 by expert human raters in the context of opera-tional, high-stakes scoring for the spoken languageassessment.
The scoring rubrics address the fol-lowing three main aspects of speaking proficiency:delivery (pronunciation, fluency, prosody), lan-guage use (grammar and lexical choice), and topicdevelopment (content and coherence).
In order toensure a sufficient quantity of responses from eachproficiency level for training and evaluating thecoherence prediction features, the spoken respons-es selected for this study were balanced based onthe human scores as follows: 25 responses wereselected randomly from each of the 4 score points(1 - 4) for each of the 6 test questions.
In somecases, more than one response was selected from agiven test-taker; in total, 471 distinct test-takers arerepresented in the data set.2.2 Annotation and AnalysisThe coherence annotation guidelines used for thespoken responses in this study were modifiedbased on the annotation guidelines developed forwritten essays described in Burstein et al(2010).According to these guidelines, expert annotatorsprovided each response with a score on a scale of 1- 3.
The three score points were defined as follows:3 = highly coherent (contains no instances of con-fusing arguments or examples), 2 = somewhat co-herent (contains some awkward points in which thespeaker's line of argument is unclear), 1 = barely815coherent (the entire response was confusing andhard to follow; it was intuitively incoherent as awhole and the annotators had difficulties in identi-fying specific weak points).
For responses receiv-ing a coherence score of 2, the annotators wererequired to highlight the specific awkward pointsin the response.
In addition, the annotators werespecifically required to ignore disfluencies andgrammatical errors as much as possible; thus, theywere instructed to not label sentences or clauses asawkward points solely because of the presence ofdisfluent or ungrammatical speech.Two annotators (not drawn from the pool of ex-pert human raters who provided the holistic scores)made independent coherence annotations for all600 spoken responses.
The distribution of annota-tions across the three score points is presented inTable 1.
The two annotators achieved a moderateinter-annotator agreement (Landis and Koch, 1977)of ?
= 0.68 on the 3-point scale.
The average of thetwo coherence scores provided by the two annota-tors correlates with the holistic speaking proficien-cy scores at r = 0.66, indicating that the overallproficiency scores of spoken responses can benefitfrom the discourse coherence annotations.1 2 3# 1 160 (27%) 278 (46%) 162 (27%)# 2 125 (21%) 251 (42%) 224 (37%)Table 1.
Distribution of coherence annotations from twoannotatorsFurthermore, coherence features based on thehuman annotations were examined within the con-text of an automated spoken language assessmentsystem, SpeechRaterSM (Zechner et al 2007;2009).
We extracted 96 features related to pronun-ciation, prosody, fluency, language use, and con-tent development using SpeechRater.
Thesefeatures were either extracted directly from thespeech signal or were based on the output of anautomatic speech recognition system (with a worderror rate of around 28%11 Both the training and evaluation sets used to develop thespeech recognizer consist of similar spoken responses drawnfrom the same assessment.
However, there is no responseoverlap between these sets and the corpus used for discoursecoherence annotation in this study.).
By utilizing a decisiontree classifier (the J48 implementation from Weka(Hall et al 2009)), 4-fold cross validation wasconducted on the 600 responses to train and evalu-ate a scoring model for predicting the holistic pro-ficiency scores.
The resulting correlation betweenthe predicted scores (based on the 96 baselineSpeechRater features) and the human holistic pro-ficiency scores was r = 0.667.In order to model a spoken response's coher-ence, three different features were extracted fromthe human annotations.
Firstly, the average of thetwo annotators?
coherence scores was directly usedas a feature with a 5-point scale (henceforthCoh_5).
Secondly, following the work in Bursteinet al(2010), we collapsed the average coherencescores into a 2-point scale to deal with thedifficulty in distinguishing somewhat and highlycoherent responses.
For this second feature(henceforth Coh_2), scores 1 and 1.5 were mappedto score 1, and scores 2, 2.5, and 3 were mapped toscore 2.
Finally, the number of awkward pointswas also counted as a feature (henceforth Awk).As shown in Table 2, when these three coherencefeatures were combined separately with theSpeechRater features, the correlations could beimproved from r = 0.667 to r > 0.7.
Meanwhile,the accuracy (i.e., the percentage of correctly pre-dicted holistic scores) could be improved from0.487 to a range between 0.535 and 0.543.Features r AccuracySpeechRater 0.667 0.487SpeechRater+Coh_5 0.714 0.540SpeechRater+Coh_2 0.705 0.543SpeechRater+Awk 0.702 0.535SpeechRater+Coh_5+Awk 0.703 0.537SpeechRater+Coh_2+Awk 0.701 0.542Table 2.
Improvement to an automated speech scoringsystem after the addition of human-assigned coherencescores and measures, showing both Pearson r correla-tions and the ratio of correctly matched holistic scoresbetween the system and human expertsThese experimental results demonstrate that theautomatic scoring system can benefit from coher-ence modeling either by directly using a human-assigned coherence score or the identified awk-ward points.
However, the use of both kinds ofannotations does not provide further improvement.When collapsing the average scores into a 2-pointscale, there was a 0.009 correlation drop (not sta-tistically significant), but the accuracy was slightlyimproved.
In addition, due to the relatively small816size of the set of available coherence annotations,we adopted the collapsed 2-point scale instead ofthe 5-point scale for the coherence prediction ex-periments in the next section.2.3 Experimental DesignAs demonstrated in Section 2.2, the collapsed av-erage coherence score can be used to improve theperformance of an automated speech scoring sys-tem.
Therefore, this study treats coherence predic-tion as a binary classification task: low-coherentvs.
high-coherent, where the low-coherent re-sponses are those with average scores 1 and 1.5,and the high-coherent responses are those with av-erage scores 2, 2.5, and 3.For coherence modeling, we again use the J48decision tree from the Weka machine learningtoolkit (Hall et al 2009) and run 4-fold cross-validation on the 600 annotated responses.
Thecorrelation coefficient (r) and the weighted aver-age F-Measure2In this experiment, we examine the performanceof the entity-grid features and a set of features pro-duced by the e-rater?
system (an automated writ-ing assessment system for learner essays) (Attaliand Burstein, 2006) to predict the coherence scoresof the spontaneous spoken responses, where all thefeatures are extracted from human transcriptions ofthe responses.are used as evaluation metrics.2.4 Entity Grid and e-rater FeaturesFirst, we applied the algorithm from Barzilay andLapata (2008) to extract entity-grid features, whichcalculated the vector of entity transition probabili-ties across adjacent sentences.
Several differentmethods of representing the entities can be usedbefore generating the entity-grid.
First, all the enti-ties can be described by their syntactic roles in-cluding S (Subject), O (Object), and X (Other).Alternatively, these roles can also be reduced to P(Present) or N (Absent).
Furthermore, entities canbe defined as salient, when they appear two ormore times, otherwise as non-salient.
In this study,2 The data distribution in the experimental corpus is unbal-anced:  71% of the responses are high-coherent and 29% arelow-coherent.
Therefore, we adopt the weighted average F-Measure to evaluate the performance of coherence prediction:first, the F1-Measure of each category is calculated, and thenthe percentages of responses in each category are used asweights to obtain the final weighted average F-Measure.we generated there basic entity grids: EG_SOX(entity grid with the syntactic roles S, O, and X),EG_REDUCED (entity grid with the reduced rep-resentations P and N), and EG_SALIENT (entitygrid with salient and non-salient entities).
In addi-tion to these entity-grid features, we also used 130writing quality features related to grammar, usage,mechanics, and style from e-rater to model the co-herence.A baseline system for this task would simply as-sign the majority class (high-coherent) to all of theresponses; this baseline achieves an F-Measure of0.587.
Table 3 shows that the EG_REDUCED ande-rater features can obtain F-Measures of 0.677and 0.726 as well as correlations with humanscores of 0.20 and 0.33, respectively.
However, thecombination of the two sets of features only bringsa very small improvement (from 0.33 to 0.34).
Inaddition, our experiments show that by introducingthe component of co-reference resolution for entitygrid building, we can only get a very slight im-provement on EG_SALIENT, but no improvementon EG_SOX and EG_REDUCED.
That may bebecause it is generally more difficult to parse thetranscriptions of spoken language than well-formed text, and more errors are introduced duringthe process of co-reference resolution.r F-MeasureBaseline 0.0 0.587EG_SOX 0.16 0.664EG_REDUCED 0.2 0.677EG_SALIENT 0.2 0.678e-rater 0.33 0.726EG_SOX +e-rater 0.30 0.714EG_REDUCED +e-rater 0.34 0.73EG_SALIENT + e-rater 0.26 0.695Table 3.
Performance of entity grid and e-rater featureson the coherence modeling task2.5 Discussion and Future WorkIn order to further analyze these features, the  cor-relation coefficients between various features andthe average coherence scores (on a five-pointscale) were calculated; Figure 1 shows the histo-gram of these correlation values.
As the figureshows, there are a total of approximately 50 fea-tures with correlations larger than 0.1.
Four of theentity-grid features have correlations between 0.15and 0.29.
As for the writing quality features, some817of them show high correlations with the averagecoherence scores, despite the fact that they are notexplicitly related to discourse coherence, such asthe number of good lexical collocations.Based on the above analysis, we plan to investi-gate additional superficial features explicitly relat-ed to discourse coherence, such as the distributionof conjunctions, pronouns, and discourse connec-tives.
Moreover, based on the research on well-formed texts and learner essays, we will attempt toexamine more effective features and models to bet-ter cover the discourse aspects of spontaneousspeech.
For example, local semantic features relat-ed to inter-sentential coherence and the ISA featurewill be investigated on spoken responses.
In addi-tion, we will apply the features and build coher-ence models using the output of automatic speechrecognition in addition to human transcriptions.Finally, various coherence features or models willbe integrated into a practical automated scoringsystem, and further experiments will be performedto measure their effect on the performance of au-tomated assessment of spontaneous spoken re-sponses.Figure1.
Histogram of entity-grid and writing qualityfeatures based on their correlations with coherencescores3 ConclusionIn this paper, we present a corpus of coherenceannotations for spontaneous spoken responses pro-vided in the context of an English speaking profi-ciency assessment.
Entity-grid features and fea-tures from an automated essay scoring system wereexamined for coherence modeling of spoken re-sponses.
The analysis on the annotated corpusshowed promising results for improving the per-formance of an automated scoring system bymeans of modeling the coherence of spoken re-sponses.AcknowledgmentsThe authors wish to express our thanks to the dis-course annotators Melissa Lopez and Matt Mulhol-land for their dedicated work and our colleaguesJill Burstein and Slava Andreyev for their supportin generating entity-grid features.ReferencesYigal Attali and Jill Burstein.
2006.
Automated essayscoring with e-rater?
V.2.0.
Journal of Technology,Learning, and Assessment.
4(3): 159-174.Regina Barzilay and Lillian Lee.
2004.
Catching thedrift: Probabilistic content models, with applicationsto generation and summarization.
Proceedings ofNAACL-HLT, 113-120.Regina Barzilay and Mirella Lapata.
2005.
Modelinglocal coherence: An entity-based approach.Proceedings of ACL, 141-148.Regina Barzilay and Mirella Lapata.
2008.
Modelinglocal coherence: An entity-based approach.Computational Linguistics, 34(1):1-34.Jill Burstein, Joel Tetreault and Slava Andreyev.
2010.Using entity-based features to model coherence instudent essays.
Proceedings of NAACL-HLT, 681-684.
Los Angeles, California.Lei Chen, Klaus Zechner and Xiaoming Xi.
2009.Improved pronunciation features for construct-driven assessment of non-native spontaneousspeech.
Proceedings of NAACL-HLT, 442-449.Miao Chen and Klaus Zechner.
2011.
Computing andevaluating syntactic complexity features forautomated scoring of spontaneous non-nativespeech.
Proceedings of ACL, 722-731.Jian Cheng.
2011.
Automatic assessment of prosody inhigh-stakes English tests.
Proceedings ofInterspeech , 27-31.Peter W. Foltz, Walter Kintsch and Thomas K.Landauer.
1998.
The measurement of textualcoherence with Latent Semantic Analysis.
DiscourseProcesses, 25(2&3):285-307.Arthur C. Graesser,  Danielle S. McNamara,  Max M.Louwerse and Zhiqiang Cai.
2004.
Coh-Metrix:Analysis of text on cohesion and language.
Behavior818Research Methods, Instruments, & Computers,36(2):193-202.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann and Ian H. Witten.2009.
The WEKA data mining software: An update.SIGKDD Explorations, 11(1):10-18.Khairun-nisa Hassanali, Yang Liu and Thamar Solorio.2012.
Coherence in child language narratives: Acase study of annotation and automatic prediction ofcoherence.
Proceedings of the InterspeechWorkshop on Child, Computer and Interaction.Derrick Higgins, Jill Burstein, Daniel Marcu andClaudia Gentile.
2004.
Evaluating multiple aspectsof coherence in student essays.
Proceedings ofNAACL-HLT, 185-192.Derrick Higgins, Xiaoming Xi, Klaus Zechner andDavid Williamson.
2011.
A three-stage approach tothe automated scoring of spontaneous.
ComputerSpeech and Language, 25:282-306.J.
Richard Landis and Gary G. Koch.
1977.
Themeasurement of observer agreement for categoricaldata.
Biometrics, 33(1):159-174.Emily Pitler, Annie Louis and Ani Nenkova.
2010.Automatic evaluation of linguistic quality in multi-document summarization.
Proceedings of ACL.544?554.
Uppsala.Helmer Strik and Catia Cucchiarini.
1999.
Automaticassessment of second language learners' fluency.Proceedings of the 14th International Congress ofPhonetic Sciences, 759-762.
Berkeley, CA.Shasha Xie, Keelan Evanini and Klaus Zechner.
2012.Exploring content features for automated speechscoring.
Proceedings of NAACL-HLT, 103-111.Helen Yannakoudakis and Ted Briscoe.
2012.
Modelingcoherence in ESOL learner texts.
Proceedings of the7th Workshop on the Innovative Use of NLP forBuilding Educational Applications, 33-43.
Montreal.Klaus Zechner, Derrick Higgins and Xiaoming Xi.2007.
SpeechRaterSMKlaus Zechner, Derrick Higgins, Xiaoming Xi andDavid M. Williamson.
2009.
Automatic scoring ofnon-native spontaneous speech in tests of spokenEnglish.
Speech Communication, 51(10):883-895.: A construct-driven approachto scoring spontaneous non-native speech.Proceedings of the International SpeechCommunication Association Special Interest Groupon Speech and Language Technology in Education,128-131.819
