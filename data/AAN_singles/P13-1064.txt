Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 651?659,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsBridging Languages through Etymology:The case of cross language text categorizationVivi Nastase and Carlo StrapparavaHuman Language Technologies, Fondazione Bruno KesslerTrento, Italy{nastase, strappa}@fbk.euAbstractWe propose the hypothesis that word ety-mology is useful for NLP applications asa bridge between languages.
We supportthis hypothesis with experiments in cross-language (English-Italian) document cat-egorization.
In a straightforward bag-of-words experimental set-up we add etymo-logical ancestors of the words in the docu-ments, and investigate the performance ofa model built on English data, on Italiantest data (and viceversa).
The results shownot only statistically significant, but a largeimprovement ?
a jump of almost 40 pointsin F1-score ?
over the raw (vanilla bag-of-words) representation.1 IntroductionWhen exposed to a document in a language hedoes not know, a reader might be able to gleansome meaning from words that are the same (e.g.names) or similar to those in a language he knows.As an example, let us say that an Italian speakeris reading an English text that contains the wordexpense, which he does not know.
He may be re-minded however of the Latin word expensa whichis also the etymological root of the Italian wordspesa, which usually means ?cost?/?shopping?,and may thus infer that the English word refersto the cost of things.
In the experiments presentedhere we investigate whether an automatic text cat-egorization system could benefit from knowledgeabout the etymological roots of words.
The crosslanguage text categorization (CLTC) task consistsof categorizing documents in a target language Ltusing a model built from labeled examples in asource language Ls.
The task becomes more diffi-cult when the data consists of comparable corporain the two languages ?
documents on the same top-ics (e.g.
sports, economy) ?
instead of parallel cor-pora ?
there exists a one-to-one correspondencebetween documents in the corpora for the two lan-guages, one document being the translation of theother.To test the usefulness of etymological in-formation we work with comparable collec-tions of news articles in English and Ital-ian, whose articles are assigned one of fourcategories: culture and school, tourism, qual-ity of life, made in Italy.
We perform a progres-sion of experiments, which embed etymologicalinformation deeper and deeper into the model.
Westart with the basic set-up, representing the doc-uments as bag-of-words, where we train a modelon the English training data, and use this modelto categorize documents from the Italian test data(and viceversa).
The results are better than ran-dom, but quite low.
We then add the etymologicalroots of the words in the data to the bag-of-words,and notice a large ?
21 points ?
increase in per-formance in terms of F1-score.
We then use thebag-of-words representation of the training data tobuild a semantic space using LSA, and use thegenerated word vectors to represent the trainingand test data.
The improvement is an additional16 points in F1-score.Compared to related work, presented in Sec-tion 3, where cross language text categorizationis approached through translation or mapping offeatures (i.e.
words) from the source to the targetlanguage, word etymologies are a novel source ofcross-lingual knowledge.
Instead of mapping fea-tures between languages, we introduce new fea-tures which are shared, and thus do not need trans-lation or other forms of mapping.The experiments presented show unequivocallythat word etymology is a useful addition to com-putational models, just as they are to readerswho have such knowledge.
This is an interest-ing and useful result, especially in the currentresearch landscape where using and exploitingmulti-linguality is a desired requirement.651morpheme relation related morphemeeng: ex- rel:etymological origin of eng: excentriceng: expense rel:etymology lat: expensaeng: -ly rel:etymological origin of eng: absurdlyeng: -ly rel:etymological origin of eng: admirably...ita: spesa rel:etymology lat: expensaita: spesa rel:has derived form ita: spese...ita: spesare rel:etymologically related ita: spesa...lat: expensa rel:etymological origin of eng: expenselat: expensa rel:etymological origin of ita: spesa...lat: expensa rel:is derived from lat: expensus...English: muscle?French: muscle?Latin: musculus?Latin: mus?Proto Indo-European: muh2sFigure 1: Sample entries from the Etymological WordNet, and a few etymological layers2 Word EtymologyWord etymology gives us a glimpse into the evo-lution of words in a language.
Words may beadopted from a language because of cultural,scientific, economic, political or other reasons(Hitchings, 2009).
In time these words ?adjust?
tothe language that adopted them ?
their sense maychange to various degrees ?
but they are still se-mantically related to their etymological roots.
Toillustrate the point, we show an example that thereader, too, may find amusing: on the ticket vali-dation machine on Italian buses, by way of instruc-tion, it is written Per obliterare il biglietto .... Anative/frequent English speaker would most prob-ably key in on, and be puzzled by, the word oblit-erare, very similar to the English obliterate, whosemost used sense is to destroy completely / cause tophysically disappear .
The Italian obliterare hasthe ?milder?
sense of cancellare ?
cancel (whichis also shared by the English obliterate, but is lessfrequent according to Merriam-Webster), and bothcome from the Latin obliterare ?
erase, efface,cause to disappear.
While there has been somesense migration ?
in English the more (physically)destructive sense of the word has higher promi-nence, while in Italian the word is closer in mean-ing to its etymological root ?
the Italian and theEnglish words are still semantically related.Dictionaries customarily include etymologi-cal information for their entries, and recently,Wikipedia?s Wiktionary has joined this trend.
Theetymological information can, and indeed hasbeen extracted and prepared for machine con-sumption (de Melo and Weikum, 2010): Etymo-logical WordNet1 contains 6,031,431 entries for2,877,036 words (actually, morphemes) in 397languages.
A few sample entries from this re-source are shown in Figure 1.The information in Etymological WordNet isorganized around 5 relations: etymology withits inverse etymological origin of; is derived fromwith its inverse has derived form; and the sym-metrical etymologically related.
The etymologyrelation links a word with its etymological ances-tors, and it is the relation used in the experimentspresented here.
Prefixes and suffixes ?
such as ex-and -ly shown in Figure 1 ?
are filtered out, asthey bring in much noise by relating words thatmerely share such a morpheme (e.g.
absurdly andadmirably) but are otherwise semantically distant.has derived form is also used, to capture morpho-logical variations.The depth of the etymological hierarchy (con-sidering the etymology relations) is 10.
Figure 1shows an example of a word with several levels ofetymological ancestry.1http://www1.icsi.berkeley.edu/?demelo/etymwn/652??????????????????????????????????????????????
?English texts Italian textste1 te2 ?
?
?
ten?1 ten ti1 ti2 ?
?
?
tim?1 timwe1 0 1 ?
?
?
0 1 0 0 ?
?
?EnglishLexiconwe2 1 1 ?
?
?
1 0 0. .
.... .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
... 0 ...wep?1 0 1 ?
?
?
0 0. .
.
0wep 0 1 ?
?
?
0 0 ?
?
?
0 0sharednames andwordswe/i1 1 0 ?
?
?
0 0 0 0 ?
?
?
0 1... .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.commonetymologywetym1 0 1 ?
?
?
0 0 0 0 ?
?
?
1 0... .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.wi1 0 0 ?
?
?
0 1 ?
?
?
1 1ItalianLexiconwi2 0. .
.
1 1 ?
?
?
0 1... ... 0 ... .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.wiq?1.
.
.
0 0 1 ?
?
?
0 1wiq ?
?
?
0 0 0 1 ?
?
?
1 0??????????????????????????????????????????????
?Figure 2: Multilingual word-by-document matrix3 Cross Language Text CategorizationText categorization (also text classification), ?thetask of automatically sorting a set of documentsinto categories (or classes or topics) from a prede-fined set?
(Sebastiani, 2005), allows for the quickselection of documents from the same domain, orthe same topic.
It is a very well research area, dat-ing back to the 60s (Borko and Bernick, 1962).The most frequently, and successfully, used docu-ment representation is the bag-of-words (BoWs).Results using this representation achieve accuracyin the 90%s.
Most variations include feature filter-ing or weighing, and variations in learning algo-rithms (Sebastiani, 2005).Within the area of cross-language text catego-rization (CLTC) several methods have been ex-plored for producing the model for a target lan-guage Lt using information and data from thesource language Ls.
In a precursor task to CLTC,cross language information retrieval (CLIR), Du-mais et al (1997) find semantic correspondencesin parallel (different language) corpora through la-tent semantic analysis (LSA).
Most CLTC meth-ods rely heavily on machine translation (MT).
MThas been used: to cast the cross-language textcategorization problem to the monolingual setting(Fortuna and Shawe-Taylor, 2005); to cast thecross-language text categorization problem intotwo monolingual settings for active learning (Liuet al, 2012); to translate and adapt a model builton language Ls to language Lt (Rigutini et al,2005), (Shi et al, 2010); to produce parallelcorpora for multi-view learning (Guo and Xiao,2012).
Wan et al (2011) also use machine trans-lation, but enhance the processing through domainadaptation by feature weighing, assuming that thetraining data in one language and the test data inthe other come from different domains, or can ex-hibit different linguistic phenomena due to linguis-tic and cultural differences.
Prettenhofer and Stein(2010) use a word translation oracle to producepivots ?
pairs of semantically similar words ?
anduse the data partitions induced by these words tofind cross language structural correspondences.In a computationally lighter framework, not de-pendent on MT, Gliozzo and Strapparava (2006)and Wu et al (2008) use bilingual lexicons andaligned WordNet synsets to obtain shared featuresbetween the training data in language Ls and thetesting data in language Lt. Gliozzo and Strap-parava (2005), the first to use comparable as op-653posed to parallel corpora for CLTC, use LSA tobuild multilingual domain models.The bag-of-word document representationmaps a document di from a corpus D into a k-dimensional space Rk, where k is the dimensionof the (possibly filtered) vocabulary of the corpus:W = {w1, ..., wk}.
Position j in the vectorrepresentation of di corresponds to word wj , andit may have different values, among the mostcommonly used being: binary values ?
wj appears(1) or not (0) in di; frequency of occurrence of wjin di, absolute or normalized (relative to the sizeof the document or the size of the vocabulary); thetf ?
idf(wj , di, D).For the task of cross language text categoriza-tion, the problem of sharing a model across lan-guages is that the dimensions, a.k.a the vocabu-lary, of the two languages are largely different.Limited overlap can be achieved through sharednames and words.
As we have seen in the lit-erature review, machine translation and bilingualdictionaries can be used to cast these dimensionsfrom the source language Ls to the target languageLt.
In this work we explore expanding the shareddimensions through word etymologies.
Figure 2shows schematically the binary k dimensional rep-resentation for English and Italian data, and shareddimensions.Cross language text categorization could beused to obtain comparable corpora for buildingtranslation models.
In such a situation, relying ona framework that itself relies on machine transla-tion is not helpful.
Bilingual lexicons are availablefor frequently studied languages, but less so forthose poorer in resources.
Considering such short-comings, we look into additional linguistic infor-mation, in particular word etymology.
This infor-mation impacts the data representation, by intro-ducing new shared features between the differentlanguage corpora without the need for translationor other forms of mapping.
The newly producedrepresentation can be used in conjunction with anyof the previously proposed algorithms.Word etymologies are a novel source of linguis-tic information in NLP, possibly because resourcesthat capture this information in a machine readableformat are also novel.
Fang et al (2009) used lim-ited etymological information extracted from theCollins English Dictionary (CED) for text catego-rization on the British National Corpus (BNC): in-formation on the provenance of words (ranges ofprobability distribution of etymologies in differentversions of Latin ?
New Latin, Late Latin, Me-dieval Latin) was used in a ?home-made?
rangeclassifier.The experiments presented in this paper use thebag-of-word document representation with abso-lute frequency values.
To this basic representationwe add word etymological ancestors and run clas-sification experiments.
We then use LSA ?
previ-ously shown by (Dumais et al, 1997) and (Gliozzoand Strapparava, 2005) to be useful for this task ?to induce the latent semantic dimensions of docu-ments and words respectively, hypothesizing thatword etymological ancestors will lead to semanticdimensions that transcend language boundaries.The vectors obtained through LSA (on the trainingdata only) for words that are shared by the Englishtraining data and the Italian test data (names, andmost importantly, etymological ancestors of wordsin the original documents) are then used for re-representing the training and test data.
The sameprocess is applied for Italian training and Englishtest data.
Classification is done using support vec-tor machines (SVMs).3.1 DataThe data we work with consists of compara-ble corpora of news articles in English and Ital-ian.
Each news article is annotated with one ofthe four categories: culture and school, tourism,quality of life, made in Italy.
Table 1 shows thedataset statistics.
The average document length isapproximately 300 words.3.2 Raw cross-lingual text categorizationAs is commonly done in text categorization (Se-bastiani, 2005), the documents in our data arerepresented as bag-of-words, and classification isdone using support vector machines (SVMs).One experimental run consists of 4 binary ex-periments ?
one class versus the rest, for each ofthe 4 classes.
The results are reported throughmicro-averaged precision, recall and F1-score forthe targeted class, as well as overall accuracy.
Thehigh results, on a par with text categorization ex-periments in the field, validates our experimentalset-up.For the cross language categorization experi-ments described in this paper, we use the datadescribed above, and train on one language (En-glish/Italian), and test on the other, using the same654English ItalianCategories Training Test Total Training Test Totalquality of life 5759 1989 7748 5781 1901 7682made in Italy 5711 1864 7575 6111 2068 8179tourism 5731 1857 7588 6090 2015 8105culture and school 3665 1245 4910 6284 2104 8388Total 20866 6955 27821 24266 8088 32354Table 1: Dataset statisticsmonolingual BoW categorizationPrec Rec F1 AccTrain EN / Test EN 0.92 0.92 0.92 0.96Train IT / Test IT 0.94 0.94 0.94 0.97Table 2: Performance for monolingual raw textcategorizationexperimental set-up as for the monolingual sce-nario (4 binary problems).
The categorizationbaseline (BoW baseline in Figure 4) was obtainedin this set-up.
This baseline is higher than the ran-dom baseline or the positive class baseline2 (all in-stances are assigned the target class in each of the4 binary classification experiments) due to sharedwords and names between the two languages.3.3 Enriching the bag-of-wordrepresentation with word etymologyAs personal experience has shown us that etymo-logical information is useful for comprehendinga text in a different language, we set out to testwhether this information can be useful in an auto-matic processing setting.
We first verified whetherthe vocabularies of our two corpora, English andItalian, have shared word etymologies.
Relyingon word etymologies from the Etymological dic-tionary, we found that from our data?s vocabulary,518 English terms and 543 Italian terms shared490 direct etymological ancestors.
Etymologicalancestors also help cluster related terms within onelanguage ?
887 etymological ancestors for 4727English and 864 ancestors for 5167 Italian terms.This overlap further increases when adding de-rived forms (through the has derived form rela-tion).
The fact that this overlap exists strengthensthe motivation to try using etymological ancestorsfor the task of text categorization.In this first step of integrating word etymology2In this situation the random and positive class baselineare the same: 25% F1 score.into the experiment, we extract for each word ineach document in the dataset its ancestors fromthe Etymological dictionary.
Because each wordwj in a document di has associated an absolutefrequency value fij (the number of occurrences ofwj in di), for the added etymological ancestors ekin document Di we associate as value the sum offrequencies of their etymological children in di:fiek =?wj?diwjetymology ekfijWe make the depth of extraction a parameter,and generate data representation when consider-ing only direct etymological antecedents (depth 1)and then up to a distance of N. For our dataset wenoticed that the representation does not change af-ter N=4, so this is the maximum depth we con-sider.
The bag-of-words representation for eachdocument is expanded with the corresponding et-ymological features.expansion training data vo-cabulary sizevocabulary over-lap with testingTrain EN /Test ITraw 71122 14207 (19.9%)depth 1 78936 18275 (23.1%)depth 2 79068 18359 (23.2%)depth 3 79100 18380 (23.2%)depth 4 79103 18382 (23.2%)Train IT /Test ENraw 78750 14110 (17.9%)depth 1 83656 18682 (22.3%)depth 2 83746 18785 (22.4%)depth 3 83769 18812 (22.5%)depth 4 83771 18814 (22.5%)Table 3: Feature expansion with word etymologiesTable 3 shows the training data vocabulary sizeand increase in the overlap between the trainingand test data with the addition of etymological fea-655tures.
The increase is largest when introducingthe immediate etymological ancestors, of approx-imately 4000 new (overlapping) features for bothcombinations of training and testing.
Without ety-mological features the overlap was approximately14000 for both configurations.
The results ob-tained with this enriched BoW representation foretymological ancestor depth 1, 2 and 3 are pre-sented in Figure 4.3.4 Cross-lingual text categorization in alatent semantic space adding etymologyShared word etymologies can serve as a bridge be-tween two languages as we have seen in the pre-vious configuration.
When using shared word et-ymologies in the bag-of-words representation, weonly take advantage of the shallow association be-tween these new features and the classes withinwhich they appear.
But through the co-occurrenceof the etymological features and other words indifferent documents in the training data, we caninduce a deeper representation for the words ina document, that captures better the relationshipbetween the features (words) and the classes towhich the documents belong.
We use latent se-mantic analysis (LSA) (Deerwester et al, 1990)to perform this representational transformation.The process relies on the assumption that wordco-occurrences across different documents are thesurface manifestation of shared semantic dimen-sions.
Mathematically, the ?word ?
document?matrix D is expressed as a product of three ma-trices:D = V ?UTby performing singular value decomposition(SVD).
V would correspond roughly to a ?word?
latent semantic dimension?
matrix, UT is thetransposed of a ?document ?
latent semanticdimension?
matrix, and ?
is a diagonal matrixwhose values are indicative of the ?strength?
of thesemantic dimensions.
By reducing the size of ?,for example by selecting the dimensions with thetop K values, we can obtain an approximation ofthe original matrix D ?
DK = VK?KUTK , wherewe restrict the latent semantic dimensions takeninto account to the K chosen ones.
Figure 3 showsschematically the process.We perform this decomposition and dimensionreduction step on the ?word ?
document?
ma-trix built from the training data only, and usingK=400.
Both the training and test data are thenreductionSVDanddimensiondimensionlatent semantic dimensionlatent semanticdimensionlatentsemanticK x KdimensionlatentsemanticwordsV x D K x Ddocuments documentsV x Kwordsx xFigure 3: Schematic view of LSAre-represented through the new word vectors frommatrix VK .
Because the LSA space was built onlyfrom the training data, only the shared words andshared etymological ancestors are used to producerepresentations of the test data.
The categorizationis done again with SVM.
The results of this exper-iment are shown in Figure 4, together with an LSAbaseline ?
using the raw data and relying on sharedwords and names as overlap.4 DiscussionThe experiments whose results we present herewere produced using unfiltered data ?
all words inthe datasets, all etymological ancestors up to thedesired depth, no filtering based on frequency ofoccurrence.
Feature filtering is commonly done inmachine learning when the data has many features,and in text categorization when using the bag-of-words representation in particular.
We chose not toperform this step for two main reasons: (i) filter-ing is sensitive to the chosen threshold; (ii) LSAthrives on word co-occurrences, which would bedrastically reduced by word removal.
The pointthat etymology information is a useful addition tothe task of cross-language text categorization canbe made without finding the optimal filtering set-up.The baseline experiments show that despitethe relatively large word overlap (approx.
14000terms), cross-language text categorization giveslow results.
Adding a first batch of etymologicalinformation ?
approximately 4000 shared immedi-ate ancestors ?
leads to an increase of 18 points interms of F1-score on the BoW experimental set-upfor English training/Italian testing, and 21 pointsfor Italian training/English testing.
Further addi-tions of etymological ancestors at depths 2 and3 results in an increase of 21 points in terms ofF1-score for English training/Italian testing, and27 points for Italian training/English testing.
Thehigher increase in performance on this experimen-tal configuration for Italian training/English test-ing is explained by the higher term overlap be-6560.40.50.60.70.80.91F1?scoreItalian training, English testing0.40.50.60.70.80.91F1?scoreEnglish training, Italian testing0.40.50.60.70.80.91AccuracyItalian training, English testing0.40.50.60.70.80.91AccuracyEnglish training, Italian testing0.420.830.790.650.43BoW_etym BoW_etym LSA_etymLSA_etymBoW_etymLSA_etymLSA_etymBoW_etymdepth=1 depth=2 depth=30.540.84 0.87 0.820.890.740.690.800.64BoW_baseline LSA_baseline0.720.71Figure 4: CLTC results with etymological featurestween the training and test data, as evidenced bythe statistics in Table 3.The next processing step induced a represen-tation of the shared words that encodes deeperlevel dependencies between words and documentsbased on word co-occurrences in documents.
TheLSA space built on the training data leads to avector representation of the shared words, includ-ing the shared etymological ancestors, that cap-tures more than the obvious word-document co-occurrences.
Using this representation leads to afurther increase of 15 points in F1-score for En-glish training/Italian testing set-up over the BoWrepresentation, and 14 points over the baselineLSA-based categorization.
The increase for theItalian training/English testing is 5 points over theBoW representation, but 20 points over the base-line LSA.
We saw that the high performance BoWon Italian training/English testing is due to thehigh term overlap.
The clue to why the increasewhen using LSA is lower than for English train-ing/Italian testing is in the way LSA operates ?
itrelies heavily on word co-occurrences in findingthe latent semantic dimensions of documents andwords.
We expect then that in the Italian trainingcollection, words are ?less shared?
among docu-ments, which means a lower average documentfrequency.
Figure 5 shows the changes in aver-age document frequency for the two training col-lections, starting with the raw data (depth 0), andwith additional etymological features.50607080901001101201301400  1  2  3  4AverageDFEtymology depthAverage document frequency for words in the training dataENITFigure 5: Document frequency changes with theaddition of etymological featuresThe shape of the document frequency curvesmirror the LSA results ?
the largest increase is theeffect of adding the set of direct etymological an-cestors, and additions of further, more distant, an-cestors lead to smaller improvements.657We have performed the experiments describedabove on two releases of the Etymological dictio-nary.
The results described in the paper were ob-tained on the latest release (February 2013).
Thedifference in results on the two dictionary versionswas significant: a 4 and 5 points increase respec-tively in micro-averaged F1-score in the bag-of-words setting for English training/Italian testingand Italian training/English testing, and a 2 and6 points increase in the LSA setting.
This indi-cates that more etymological information is better,and the dynamic nature of Wikipedia and the Wik-tionary could lead to an ever increasing and betteretymological resource for NLP applications.5 ConclusionThe motivation for this work was to test the hy-pothesis that information about word etymology isuseful for computational approaches to language,in particular for text classification.
Cross-languagetext classification can be used to build compara-ble corpora in different languages, using a singlelanguage starting point, preferably one with moreresources, that can thus spill over to other lan-guages.
The experiments presented have shownclearly that etymological ancestors can be usedto provide the necessary bridge between the lan-guages we considered ?
English and Italian.
Mod-els produced on English data when using etymo-logical information perform with high accuracy(89%) and high F1-score (80) on Italian test data,with an increase of almost 40 points over a simplebag-of-words model, which, for crossing languageboundaries, relies exclusively on shared namesand words.
Training on Italian data and testing onEnglish data performed almost as well (87% accu-racy, 75 F1-score).
We plan to expand our experi-ments to more languages with shared etymologies,and investigate what characteristics of languagesand data indicate that etymological information isbeneficial for the task at hand.We also plan to explore further uses for this lan-guage bridge, at a finer semantic level.
Monolin-gual and cross-lingual textual entailment in par-ticular would be interesting applications, becausethey require finding shared meaning on two textfragments.
Word etymologies would allow recog-nizing words with shared ancestors, and thus withshared meaning, both within and across languages.AcknowledgementsWe thank the reviewers for the helpful comments.This work was financially supported by the EC-funded project EXCITEMENT ?
EXploring Cus-tomer Interactions through Textual EntailMENTFP7 ICT-287923.
Carlo Strapparava was partiallysupported by the PerTe project (Trento RISE).ReferencesHarold Borko and Myrna Bernick.
1962.
Auto-matic Document Classification.
System Develop-ment Corporation, Santa Monica, CA.Gerard de Melo and Gerhard Weikum.
2010.
Towardsuniversal multilingual knowledge bases.
In Prin-ciples, Construction, and Applications of Multilin-gual Wordnets.
Proceedings of the 5th Global Word-Net Conference (GWC 2010), pages 149?156, NewDelhi, India.Scott Deerwester, Susan T. Dumais, George W. Fur-nas, Thomas K. Landauer, and Richard Harshman.1990.
Indexing by latent semantic analysis.
Journalof the American Socienty for Information Science,41(6):391?407.Susan T. Dumais, Todd A. Letsche, Michael L.Littman, and Thomas K. Landauer.
1997.
Auto-matic cross-language retrieval using latent semanticindexing.
In AAAI Symposium on CrossLanguageText and Speech Retrieval.Alex Chengyu Fang, Wanyin Li, and Nancy Ide.
2009.Latin etymologies as features on BNC text cate-gorization.
In 23rd Pacific Asia Conference onLanguage, Information and Computation (PACLIC2009), pages 662?669.Blaz Fortuna and John Shawe-Taylor.
2005.
The use ofmachine translation tools for cross-lingual text min-ing.
In Learning with multiple views ?
Workshopat the 22nd International Conference on MachineLearning (ICML 2005).Alfio Gliozzo and Carlo Strapparava.
2005.
Cross lan-guage text categorization by acquiring multilingualdomain models from comparable corpora.
In Pro-ceedings of the ACL Workshop on Building and Us-ing Parallel Texts.Alfio Gliozzo and Carlo Strapparava.
2006.
Ex-ploiting comparable corpora and bilingual dictionar-ies for cross-language text categorization.
In Pro-ceedings of the 21st International Conference onComputational Linguistics and 44th Annual Meet-ing of the Association for Computational Linguis-tics (COLING-ACL 2006), pages 553?560, Sydney,Australia.658Yuhong Guo and Min Xiao.
2012.
Cross languagetext classification via subspace co-regularized multi-view learning.
In Proceedings of the 29th Inter-national Conference on Machine Learning (ICML2012), Edinburgh, Scotland, UK.Henry Hitchings.
2009.
The Secret Life of Words: HowEnglish Became English.
John Murray Publishers.Yue Liu, Lin Dai, Weitao Zhou, and Heyan Huang.2012.
Active learning for cross language text cat-egorization.
In Proceedings of the 16th Pacific-Asiaconference on Advances in Knowledge Discoveryand Data Mining (PAKDD 2012), pages 195?206,Kuala Lumpur, Malaysia.Peter Prettenhofer and Benno Stein.
2010.
Cross-language text classification using structural corre-spondence learning.
In Proceedings of the 48th An-nual Meeting of the Association for ComputationalLinguistics (ACL 2010), pages 1118?1127, Uppsala,Sweden.Leonardo Rigutini, Marco Maggini, and Bing Liu.2005.
An EM based training algorithm for cross-language text categorization.
In Proceedings of theInternational Conference on Web Intelligence (WI2005), pages 200?206, Compiegne, France.Fabrizio Sebastiani.
2005.
Text categorization.
InAlessandro Zanasi, editor, Text Mining and its Ap-plications, pages 109?129.
WIT Press, Southamp-ton, UK.Lei Shi, Rada Mihalcea, and Minhgjun Tian.
2010.Cross language text classification by model trans-lation and semi-supervised learning.
In Proceed-ings of the 48th Annual Meeting of the Associationfor Computational Linguistics (ACL 2010), pages1057?1067, Uppsala, Sweden.Chang Wan, Rong Pan, and Jifei Li.
2011.
Bi-weighting domain adaptation for cross-language textclassification.
In Proceedings of the 22nd Interna-tional Joint Conference on Artificial Intelligence (IJ-CAI 2011), pages 1535?1540, Barcelona, Catalonia,Spain.Ke Wu, Xiaolin Wang, and Bao-Liang Lu.
2008.Cross language text categorization using a bilinguallexicon.
In Third International Joint Conferenceon Natural Language Processing (IJCNLP 2008),pages 165?172, Hyderabad, India.659
