Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 197?200,Sydney, July 2006. c?2006 Association for Computational LinguisticsN-gram Based Two-Step Algorithm for Word SegmentationDong-Hee LimDept.
of Computer ScienceKookmin UniversitySeoul 136-702, Koreanlp@cs.kookmin.ac.krKyu-Baek, HwangSchool of ComputingSoongsil UniversitySeoul 156-743, Koreakbhwang@ssu.ac.krSeung-Shik KangDept.
of Computer ScienceKookmin UniversitySeoul 136-702, Koreasskang@kookmin.ac.krAbstractThis paper describes an n-gram basedreinforcement approach to the closedtrack of word segmentation in the thirdChinese word segmentation bakeoff.Character n-gram features of unigram,bigram, and trigram are extracted fromthe training corpus and its frequencies arecounted.
We investigated a step-by-stepmethodology by using the n-gram statis-tics.
In the first step, relatively definitesegmentations are fixed by the tightthreshold value.
The remaining tags aredecided by considering the left or rightspace tags that are already fixed in thefirst step.
Definite and loose segmenta-tion are performed simply based on thebigram and trigram statistics.
In order toovercome the data sparseness problem ofbigram data, unigram is used for thesmoothing.1 IntroductionWord segmentation has been one of the veryimportant problems in the Chinese languageprocessing.
It is a necessary in the informationretrieval system for the Korean language (Kangand Woo, 2001; Lee et al 2002).
Though Koreanwords are separated by white spaces, many webusers often do not set a space in a sentence whenthey write a query at the search engine.
Anothernecessity of automatic word segmentation is theindex term extraction from a sentence that in-cludes word spacing errors.The motivation of this research is to investi-gate a practical word segmentation system for theKorean language.
While we develop the system,we found that ngram-based algorithm was ex-actly applicable to the Chinese word segmenta-tion and we have participated the bakeoff (Kangand Lim, 2005).
The bakeoff result is not satis-fiable, but it is acceptable because our method islanguage independent that does not consider thecharacteristics of the Chinese language.
We donot use any language dependent features exceptthe average length of Chinese words.Another advantage of our approach is that itcan express the ambiguous word boundaries thatare error-prone.
So, there are a good possibilityof improving the performance if language de-pendent functionalities are added such as propername, numeric expression recognizer, and thepostprocessing of single character words.12 N-gram FeaturesThe n-gram features in this work are similar tothe previous one in the second bakeoff.
The basicsegmentation in (Kang and Lim, 2005) has per-formed by bigram features together with spacetags, and the trigram features has been used as apostprocessing of correcting the segmentationerrors.
Trigrams for postprocessing are the onesthat are highly biased to one type of the four tagfeatures of ?AiBjC?.2 In addition, unigram fea-tures are used for smoothing the bigram, wherebigram is not found in the training corpora.
Inthis current work, we extended the n-gram fea-tures to a trigram.
(a) trigram: AiBjC(b) bigram: iAjBk(c) unigram: iAjIn the above features, AB and ABC are aChinese character sequence of bigram and tri-gram, respectively.
The subscripts i, j, and k1 Single character words in Korean are not so common,compared to the Chinese language.
We can control theoccurrence of them through an additional processing.2 We applied the trigrams for error correction in which oneof the trigram feature occupies 95% or more.197denote word space tags, where the tags aremarked as 1(space tag) and 0(non-space tag).
Forthe unigram iAj, four types of tag features arecalculated in the training corpora and their fre-quencies are stored.
In the same way, eight typesof bigram features and four types of trigramfeatures are constructed.
If we take all the insideand outside space-tags of ABC, there are sixteentypes of trigram features hAiBjCk for h,i,j,k = 0 or1.
It will cause a data sparseness problem, espe-cially for small-sized training corpora.
In order toavoid the data sparseness problem, we ignoredthe outside-space tags h and k and constructedfour types of trigram features of AiBjC.Table 1 shows the number of n-gram featuresfor each corpora.
The total number of uniquetrigrams for CITYU corpus is 1,341,612 in which104,852 trigrams occurred more than three times.It is less than one tenth of the total number oftrigrams.
N-gram feature is a compound featureof <character, space-tag> combination.
Trigramclasses are distinguished by the space-tag context,trigram class hAiBjCk  is named as t4-trigram orC3T4.3 It is simplified into four classes of C3T2trigrams of AiBjC, in consideration of the mem-ory space savings and the data sparseness prob-lem.Table 1.
The number of n-gram featuresTrigram Bigram Unigramfreq?1 freq?2 freq?3 freq?4 freq?1 freq?1cityu 1341612 329764 165360 104852 404411 5112ckip 2951274 832836 444012 296372 717432 6121msra 986338 252656 132456 86391 303443 4767upuc 463253 96860 45775 28210 177140 42933 Word Segmentation AlgorithmWord segmentation is defined as to choose thebest tag-sequence for a sentence.where3 ?Cn?
refers to the number of characters and ?Tn?
refers tothe number of spae-tag.
According to this notation, iAjBkand iAj are expressed as C2T3 and C1T2, respectively.More specifically at each character position, thealgorithm determines a space-tag ?0?
or ?1?
byusing the word spacing features.3.1 The FeaturesWe investigated a two step algorithm of de-termining space tags in each character position ofa sentence using by context dependent n-gramfeatures.
It is based on the assumption that spacetags depend on the left and right context ofcharacters together with the space tags that itaccompanies.
Let tici be a current <space tag,character> pair in a sentence.4?
ti-2ci-2 ti-1ci-1 tici ti+1ci+1 ti+2ci+2 ?In our previous work of (Lim and Kang, 2005),n-gram features (a) and (b) are used.
These fea-tures are used to determine the space tag ti.
In thiswork, core n-gram feature is a C3T2 classes oftrigram features ci-2ti-1ci-1tici, ci-1ticiti+1ci+1.
Inaddition, a simple character trigram with nospace tag ?ticici+1ci+2?
is added.
(a) unigram:ti-1ci-1ti, ticiti+1(b) bigram:ti-2ci-2ti-1ci-1ti, ti-1ci-1ticiti+1, ticiti+1ci+1ti+2(c) trigram:ci-2ti-1ci-1tici, ci-1ticiti+1ci+1, ticici+1ci+2Extended n-gram features with space tags areeffective when left or right tags are fixed.
Sup-pose that ti-1 and ti+1 are definitely set to 0 in abigram context ?ti-1ci-1ticiti+1?, then a feature?0ci-1tici0?
(ti = 0 or 1) is applied, instead of asimple feature ?ci-1tici?.
However, none of thespace tags are fixed in the beginning that simplecharacter n-gram features with no space tag areused.53.2 Two-step AlgorithmThe basic idea of our method is a cross checkingthe n-gram features in the space position by usingthree trigram features.
For a character sequence?ci-2ci-1ticici+1ci+2?, we can set a space mark ?1?
toti, if P(ti=1) is greater than P(ti=0) in all the threetrigram features ci-2ci-1tici, ci-1ticici+1, and tici-ci+1ci+2.
Because no space tags are determined in)|(maxarg?
STPTT ?
?=nn ,c,,ccStttT KK 2121  and ,,, ==4 Tag ti is located before the character, not after the characterthat is common in other tagging problem like POS-tagging.5 Simple n-grams with no space tags are calculated from theextended n-grams.198the beginning, word segmentation is performedin two steps.
In the first step, simple n-gramfeatures are applied with strong threshold values(tlow1 and thigh1 in Table 2).
The space tags withhigh confidence are determined and the remain-ing space tags will be set in the next step.Table 2.
Strong and weak threshold values6tlow1 thigh1 tlow2 thigh2 tfinalcityu 0.36 0.69 0.46 0.51 0.48ckip 0.37 0.69 0.49 0.51 0.49msra 0.33 0.68 0.46 0.47 0.46upuc 0.38 0.69 0.45 0.47 0.47In the second step, extended bigram featuresare applied if any one of the left or right spacetags is fixed in the first step.
Otherwise, simplebigram probability will be applied, too.
In thisstep, extended bigram features are applied withweak threshold values tlow2 and thigh2.
The spacetags are determined by the final threshold tfinal, ifit was not determined by weak threshold values.Considering the fact that average length of Chi-nese words is about 1.6, the threshold values arelowered or highered.7In the final step, error correction is performedby 4-gram error correction dictionary.
It is con-structed by running the training corpus andcomparing the result to the answer.
Error correc-tion data format is 4-gram.
If a 4-gram ci-2ci-1cici+1is found in a sentence, then tag ti is modifiedunconditionally as is specified in the 4-gramdictionary.4 Experimental ResultsWe evaluated our system in the closed task on allfour corpora.
Table 3 shows the final results inbakeoff 2006.
We expect that Roov will be im-proved if any unknown word processing is per-formed.
Riv can also be improved if lexicon isapplied to correct the segmentation errors.Table 3.
Final results in bakeoff 2006R P F Roov Rivcityu 0.950  0.949 0.949  0.638 0.963ckip 0.937  0.933 0.935  0.547 0.954msra 0.933  0.939 0.936  0.526 0.948upuc 0.915  0.896 0.905  0.565 0.9496 Threshold values are optimized for each training corpus.7 The average length of Korean words is 3.2 characters.4.1 Step-by-step AnalysisIn order to analyze the effectiveness of each step,we counted the number of space positions forsentence by sentence.
If the number of charactersin a sentence is n, then the number of wordspositions is (n-1) because we ignored the first tagt0 for c0.
Table 4 shows the number of spacepositions in four test corpora.Table 4.
The number of space positions# of space positions # of spaces# of non-spacescityu 356,791 212,662 144,129ckip 135,123   80,387  54,736msra 168,236   95,995   72,241upuc 251,418 149,747 101,671As we expressed in section 3, we assumed thattrigram with space tag information will deter-mine most of the space tags.
Table 5 shows theapplication rate with strong threshold values.
Aswe expected, around 93.8%~95.9% of total spacetags are set in step-1 with the error rate1.5%~2.8%.Table 5.
N-gram results with strong threshold# of applied (%) # of errors (%)cityu 342,035 (95.9%) 5,024 (1.5%)ckip 128,081 (94.8%) 2,818 (2.2%)msra 160,437 (95.4%) 3,155 (2.0%)upuc 235,710 (93.8%) 6,601 (2.8%)Table 6 shows the application rate of n-gramwith weak threshold values in step-2.
The spacetags that are not determined in step-1 are set inthe second step.
The error rate in step-2 is24.3%~30.1%.Table 6.
N-gram results with weak threshold# of applied (%) # of errors (%)cityu 14,756 (4.1%) 3,672 (24.9%)ckip   7,042 (5.2%) 1,710 (24.3%)msra   7,799 (4.6%) 2,349 (30.1%)upuc 15,708 (6.3%) 4,565 (29.1%)1994.2 4-gram Error CorrectionWe examined the effectiveness of 4-gram errorcorrection.
The number of 4-grams that is ex-tracted from training corpora is about 10,000 to15,000.
We counted the number of space tagsthat are modified by 4-gram error correctiondictionary.
Table 7 shows the number of modi-fied space tags and the negative effects of 4-gramerror correction.
Table 8 shows the results beforeerror correction.
When compared with the finalresults in Table 3, F-measure is slightly lowerthan the final results.Table 7.
Modified space tags by error correction# of modified space tags (%)Modificationerrors (%)cityu 418 (0.1%)   47 (11.2%)ckip 320 (0.2%)   94 (29.4%)msra 778 (0.5%) 153 (19.7%)upuc 178 (0.1%)   61 (34.3%)Table 8.
Results before error correctionR P Fcityu 0.948  0.947  0.948ckip 0.935  0.931  0.933msra 0.930  0.930  0.930upuc 0.915  0.895  0.9055 ConclusionWe described a two-step word segmentationalgorithm as a result of the closed track in bake-off 2006.
The algorithm is based on the crossvalidation of the word spacing probability byusing n-gram features of <character, space-tag>.One of the advantages of our system is that it canshow the self-confidence score for ambiguous orfeature-conflict cases.
We have not applied anylanguage dependent resources or functionalitiessuch as lexicons, numeric expressions, andproper name recognition.
We expect that ourapproach will be helpful for the detection oferror-prone tags and the construction of errorcorrection dictionaries when we develop a prac-tical system.
Furthermore, the proposed algo-rithm has been applied to the Korean languageand we achieved a good improvement on propernames, though overall performance is similar tothe previous method.AcknowledgementsThis work was supported by the Korea Scienceand Engineering Foundation(KOSEF) throughAdvaned Information Technology ResearchCenter(AITrc).ReferencesAsahara, M., C. L. Go, X. Wang, and Y. Matsumoto,Combining Segmenter and Chunker for ChineseWord Segmentation, Proceedings of the 2ndSIGHAN Workshop on Chinese Language Proc-essing, pp.144-147, 2003.Chen, A., Chinese Word Segmentation Using Mini-mal Linguistic Knowledge, SIGHAN 2003,pp.148-151, 2003.Gao, J., M. Li, and C.N.
Huang, ImprovedSource-Channel Models for Chinese Word Seg-mentation, ACL 2003, pp.272-279, 2003.Kang, S. S. and C. W. Woo, Automatic Segmentationof Words using Syllable Bigram Statistics, Pro-ceedings of NLPRS'2001, pp.729-732, 2001.Kang, S. S. and D. H. Lim, Data-driven LanguageIndependent Word Segmentation Using Charac-ter-Level Information, Proceedings of the 4thSIGHAN Workshop on Chinese Language Proc-essing, pp.158-160, 2005.Lee D. G, S. Z. Lee, and H. C. Rim, H. S. Lim,Automatic Word Spacing Using Hidden MarkovModel for Refining Korean Text Corpora, Proc.
ofthe 3rd Workshop on Asian Language Resourcesand International Standardization, pp.51-57, 2002.Maosong, S., S. Dayang, and B. K. Tsou, ChineseWord Segmentation without Using Lexicon andHand-crafted Training Data, Proceedings of the17th International Conference on ComputationalLinguistics (Coling?98), pp.1265-1271, 1998.Nakagawa, T., Chinese and Japanese Word Segmen-tation Using Word-Level and Character-Level In-formation, COLING?04., pp.466-472, 2004.Ng, H.T.
and J.K. Low, Chinese Part-of-Speech Tag-ging: One-at-a-Time or All-at-Once?
Word-Basedor Character-Based, EMNLP?04, pp.277-284,2004.Shim, K. S., Automated Word-Segmentation forKorean using Mutual Information of Syllables,Journal of KISS: Software and Applications,pp.991-1000, 1996.Sproat, R. and T. Emerson, The First InternationalChinese Word Segmentation Bakeoff, SIGHAN2003.200
