Textual Properties and Task Based Evaluation: Investigating the Role ofSurface Properties, Structure and Content.Albert GattInstitute of LinguisticsUniversity of Maltaalbert.gatt@um.edu.mtFranc?ois PortetLaboratoire d?Informatique de GrenobleGrenoble Institute of Technologyfrancois.portet@imag.frAbstractThis paper investigates the relationship be-tween the results of an extrinsic, task-based evaluation of an NLG system andvarious metrics measuring both surfaceand deep semantic textual properties, in-cluding relevance.
The latter rely heav-ily on domain knowledge.
We show thatthey correlate systematically with somemeasures of performance.
The core ar-gument of this paper is that more domainknowledge-based metrics shed more lighton the relationship between deep semanticproperties of a text and task performance.1 IntroductionEvaluation methodology in NLG has generated alot of interest.
Some recent work suggested thatthe relationship between various intrinsic and ex-trinsic evaluation methods (Spa?rck-Jones and Gal-liers, 1996) is not straightforward (Reiter andBelz, 2009; Gatt and Belz, to appear), leading tosome arguments for more domain-specific intrin-sic metrics (Foster, 2008).
One reason why theseissues are important is that reliable intrinsic eval-uation metrics that correlate with performance inan extrinsic, task-based setting can inform systemdevelopment.
Indeed, this is often the stated pur-pose of evaluation metrics such as BLEU (Papineniet al, 2002) and ROUGE (Lin and Hovy, 2003),which were originally characterised as evaluation?understudies?.In this paper we take up these questions in thecontext of a knowledge-based NLG system, BT-45(Portet et al, 2009), which summarises medicaldata for decision support purposes in a Neona-tal Intensive Care Unit (NICU).
Our extrinsicdata comes from an experiment involving com-plex medical decision making based on automati-cally generated and human-authored texts (van derMeulen et al, 2009).
This gives us the oppor-tunity to directly compare the textual character-istics of generated and human-written summariesand their relationship to decision-making perfor-mance.
The present work uses data from an ear-lier study (Gatt and Portet, 2009), which presentedsome preliminary results along these lines for thesystem in question.
We extend this work in a num-ber of ways.
Our principal aim is to test the va-lidity not only of general-purpose metrics whichmeasure surface properties of text, but also of met-rics which make use of domain knowledge, in thesense that they attempt to relate the ?deep seman-tics?
of the texts to extrinsic factors, based on anontology for the BT-45 domain.After an overview of related work in section 2,the BT-45 system, its domain ontology and the ex-trinsic evaluation are described in section 3.
Theontology plays an important role in the evaluationmetrics presented in Section 5.
Finally, the eval-uation of the methods is presented in Section 6,before discussing and concluding in Section 7.2 Related WorkIn NLG evaluation, extrinsic, task-based methodsplay a significant role (Reiter et al, 2003; Karasi-mos and Isard, 2004; Stock et al, 2007).
De-pending on the study design, these studies oftenleave open the question of precisely which as-pects of a system (and of the text it generates)contribute to success or failure.
Intrinsic NLGevaluations often involve ratings of text qualityor responses to questionnaires (Lester and Porter,1997; Callaway and Lester, 2002; Foster, 2008),with some studies using post-editing by human ex-perts (Reiter et al, 2005).
Automatically com-puted metrics exploiting corpora, such as BLEU,NIST and ROUGE, have mainly been used in eval-uations of the coverage and quality of morphosyn-tactic realisers (Langkilde-Geary, 2002; Callaway,2003), though they have recently also been usedfor subtasks such as Referring Expression Gener-ation (Gatt and Belz, to appear) as well as end-to-end weather forecasting systems (Reiter and Belz,2009).
The widespread use of these metrics inNLP partly rests on the fact that they are quickand cheap, but there is controversy about their re-liability both in MT (Calliston-Burch et al, 2006)and summarisation (Dorr et al, 2005; Liu and Liu,2008).
As noted in Section 1, similar questionshave been raised in NLG.
One of the problemsassociated with these metrics is that they rely onthe notion of a ?gold standard?, which is not al-ways precisely definable given multiple solutionsto the same generation, summarisation or transla-tion task.
These observations underlie recent de-velopments in Summarisation evaluation such asthe Pyramid method (Nenkova and Passonneau,2004), which in addition also emphasises contentoverlap with a set of reference summaries, ratherthan n-gram matches.It is interesting to note that, with some excep-tions (Foster, 2008), most of the methodologi-cal studies on intrinsic evaluation cited here havefocused on ?generic?
metrics (corpus-based au-tomatic measures being foremost among them),none of which use domain knowledge to quantifythose aspects of a text related to its content.
Thereis some work in Summarisation that suggests thatincorporating more knowledge improves results.For example, Yoo and Song (Yoo et al, 2007)used the Medical Subject Headings (MeSH) toconstruct graphs representing the high-level con-tent of documents, which are then used to clus-ter documents by topic, each cluster being used toproduce a summary.
In (Plaza et al, 2009), theauthors have proposed a summarisation methodbased on WordNet concepts and showed that thishigher level representation improves the summari-sation task.The principal aim of this paper is to developmetrics with which to compare texts using domainknowledge ?
in the form of the ontology used inthe BT-45 system ?
and to correlate results to hu-man decision-making performance.
The resultingmetrics focus on aspects of content, structure andrelevance that are shown to correlate meaningfullywith task performance, in contrast to other, moresurface-oriented ones (such as ROUGE).3 The BT-45 SystemBT-45(Portet et al, 2009) was designed to gen-erate a textual summary of 45 minutes of patientdata in a Neonatal Intensive Care Unit (NICU), ofthe kind shown in Figure 1(a).
The correspondingsummary for the same data shown in Figure 1(b)is a two-step consensus summary written by twoexpert neonatologists.
These two summaries cor-respond to two of the conditions in the task-basedevaluation experiment described below.In BT-45, summaries such as Figure 1(a) weregenerated from raw input data consisting of (a)physiological signals measured using sensors forvarious parameters (such as heart rate); and (b)discrete events logged by medical staff (e.g.
drugadministration).
The system was based on apipeline architecture which extends the standardNLG tasks such as document planning and mi-croplanning with preliminary stages for data anal-ysis and reasoning.
The texts generated were de-scriptive, that is, they kept interpretation to a min-imum (for example, the system did not make di-agnoses).
Nor were they generated with a bias to-wards specific problems or actions that could beconsidered desirable for a clinician to take in a par-ticular context.Every stage of the generation process made useof a domain-specific ontology of around 550 con-cepts, an excerpt of which is shown in Figure 1(c).The ontology classified objects of type EVENTand ENTITY into several subtypes; for example,a DRUG ADMINISTRATION is an INTERVENTION,which means it involves an agent and a patient.The ontology functioned as a repository of declar-ative knowledge, on the basis of which produc-tion rules were defined to support reasoning in or-der to make abstractions and to identify relations(such as causality) between events detected in thedata based on their ontological class and their spe-cific properties.
In addition to the standard IS-Alinks, the ontology contains functional relation-ships which connect events to concepts represent-ing physiological systems (such as the respiratoryor cardiovascular systems); these are referred to asfunctional concepts.
For example, in Figure 1(c), aFEED event is linked to NUTRITION, meaning thatit is primarily relevant to the nutritional system.These links were included in the ontology follow-ing consultation with a senior neonatal consultantafter the development of BT-45 was completed.Their inclusion was motivated by the knowledge-based evaluation metrics developed for the pur-poses of the present study, and discussed furtherOver the next 38 minutes T1 stayed at around37.4.By 13:33 TcPO2 had rapidly decreased to 2.7.Previously HR had increased to 173.By 13:35 there had been 2 successive desatura-tions down to 56.
Previously T2 had increasedto 35.5.By 13:40 SaO2 had rapidly decreased to 79.
(a) BT-45 summaryAt the start of the monitoring period: HRbaseline is 145-155, oxygen saturation is99%, pO2 = 4.9 and CO2 = 10.3 Mean BP is37-47; T1 and T2 are 37.3degC and 34.6degCrespectively.At 13:33 there is a desaturation to 59%, whichis accompanied by a drop in pO2 to 1.3 anda decrease in HR to 122.
The blood pressurerises toward the end of this episode to 49.
Theseparameters return to their baselines by 13:37.
(b) Human summary (c) Ontology excerptFigure 1: Excerpts from Human and BT-45 summaries, and ontology example.in Section 5.The task-based experiment to evaluate BT-45was conducted off-ward and involved a group of35 clinicians, who were exposed to 24 scenarios,each covering approximately 45 minutes of patientdata, together with a short introductory text whichgave some background about the patient.
The pa-tient data was then presented in one of three condi-tions: graphically using a time-series plot, and tex-tually in the form of a consensus summary writtenby human experts (H; Figure 1(b)) and one gener-ated automatically by BT-45(C; Figure 1(a)).
Likethe BT-45 texts, the H texts did not give interpre-tations or diagnoses and every effort was made notto bias a reader in favour of certain courses of ac-tion.
A Latin Square design was used to ensurethat each scenario was shown to an equal numberof participants in each condition, while no partici-pant saw the same scenario in more than one con-dition.For each scenario, the task was to select oneor more appropriate clinical actions from a prede-fined set of 18, one of which was ?no action?.
Se-lections had to be made within three minutes, afterwhich the scenario timed out.
The same choice of18 actions was given in each scenario s, but foreach one, two neonatal experts identified the sub-sets of appropriate (APs), inappropriate/potentiallyharmful (INAPs) and neutral actions.
One of theappropriate actions was also deemed to be the ?tar-get?, that is, the most important action to take.In three scenarios, the ?target?
was ?no action?.For each participant p and scenario s, the perfor-mance score P ps was based on the proportion PAPsof actions selected out of APs, and the proportionPINAPs selected out of the set of inappropriate ac-tions INAPs: Pps = PAPs ?
PINAPs ?
[?1, 1].Overall, decision making in the H condition wasbetter (Ps = .45SD=.10) than either C (Ps =.41SD=.13) or G (Ps = .40SD=.15).
No sig-nificant difference was found between the lattertwo, but the H texts were significantly better thanthe C texts, as revealed in a by-subjects ANOVA(F (1, 31) = 5.266, p < 0.05).
We also performeda post-hoc analysis, comparing the proportions ofappropriate actions selected, PAP and that of inap-propriate actions PINAP in the H and C conditionsacross scenarios.
In addition, we computed a dif-ferent score SPAP, defined as the proportion of ap-propriate actions selected by a participant withina scenario out of the total number of actions se-lected (effectively a measure of ?precision?).
Acomparison between means for these three scoresobtained across scenarios showed no significantdifferences.In the analysis reported in Section 6, we com-pare our textual metrics to both the global scoreP as well as to these three other performance in-dicators.
In various follow-up analyses (van derMeulen et al, 2009; Reiter et al, 2008), it wasfound that the three scenarios in which the tar-get action was ?no action?
may have misled someparticipants, insofar as this option was includedamong a set of other actions, some of which werethemselves deemed appropriate or at least neutral(in the sense that they could be carried out withoutharming the patient).
We shall therefore excludethese scenarios from our analyses.<P>At 14:15 hours<EVENT TYPE="HEEL_PRICK" ID="e11">a heel prick is done.</EVENT><EVENT TYPE="TREND" SOURCE="HR" DIRECTION="increasing" ID="e12">The HR increases</EVENT>at this point and for 7 minutes from the start of this procedure<EVENT CARDINALITY="3" SOURCE="SaO2" TYPE="ARTIFACT" ID="e13">there is a lot of artefact in the oxygen saturation trace.</EVENT></P><TREL ARG0="e11" ARG1="TIMESTAMP" RELATION="at" /><TREL ARG0="e12" ARG1="e11" RELATION="starts" /><TREL ARG0="e13" ARG1="e11" RELATION="starts" />(a) Annotation (b) Normalised treeFigure 2: Fragment of an annotated summary and normalised tree representation.4 Corpus AnnotationFor this study, we annotated the H and C textsfrom our experiment using the ontology, in or-der to make both their semantic content and struc-ture explicit.
Figure 2(a) shows an excerpt froman annotated text.
Every paragraph of the text ismarked up explicitly.
All segments of the text cor-responding to an ontology EVENT are marked upwith a TYPE (the name of the concept in the on-tology) and other properties, such as DIRECTIONand SOURCE in the case of trends in physiolog-ical parameters.
The CARDINALITY attribute isused to indicate that a single text segment abstractsover several occurrences in the data; for example,the statement about artefacts in the example corre-sponds to three such episodes in the data.In addition to events, the markup also includesseparate nodes for all the temporal (TREL) anddiscourse (DREL) relations which are explicitlymentioned in the text, typically using adverbial orprepositional phrases or verbs of causality.
Ev-ery TREL and DREL points to two arguments andhas a RELATION attribute.
In the case of a TREL,the value is one of the temporal relations de-fined by (Allen, 1983).
For DRELs, values wererestricted to CAUSE and CONTRAST (Mann andS.Thompson, 1988).
One of the arguments of aTREL can be a timestamp, rather than an event.This is the case for the first sentence in the frag-ment, where event e11 is specified as having oc-curred at a specific time (at 14:15).
By contrast,r4 is a relation between e11 and e12, wherethe RELATION is STARTS, indicating that the textspecifies that e11 is used by the author as the an-chor point to specify the start of e12, as reflectedby the expression at this point.The markup provided the basis on which manyof the metrics described in the following sectionwere computed.
Based on the annotation, weused a normalised structural representation of thetexts as shown in Figure 2(b), consisting of PARA-GRAPH (P) nodes which subsume events and rela-tions.
Relations dominate their event arguments.For example, the starts TREL holding betweene12 and e11 is represented by a STARTS nodesubsuming the two events.
In case an event isdominated by more than one relation (for exam-ple, it is temporally related to two events, as e11is in Figure 2(a), we maintain the tree structure bycreating two copies of the event, which are sub-sumed by the two relations.
Thus, the normalisedtree representation is a ?compiled out?
version ofthe graph representing all events and their rela-tions.
The tree representation is better suited toour needs, given the complexity of comparing twographs.5 MetricsThe evaluation metrics used to score texts writtenby domain experts and those generated by the BT-45 system fall into three main classes, describedbelow.Semantic content and structure To compareboth the content and the structure of texts, we usedthree measures.
The first quantifies the numberof EVENT nodes in an annotated text, defined as?e?E c, where E is the set of events mentioned,and c is the value of the CARDINALITY attributeof an event e ?
E. Similarly, we computed thenumber of temporal (TREL) and discourse (DREL)relations mentioned in a text.
We also used theTree Edit Distance metric to compute the distancebetween the tree representations of the H and Ctexts (see Figure 2(b)).
This measure computesthe minimum number of node insertions, dele-tions and substitutions required to transform onetree into another and therefore takes into accountnot only the content (events and relations) butalso its structural arrangement in the text.
Theedit distance between two trees is computed usingthe standard Levenshtein edit distance algorithm,computed over a string that represents the preordertraversal of the two trees, using a cost of 1 for in-sertions and deletions, and 2 for substitutions.N-gram overlap As a measure of n-gram over-lap, we use ROUGE-n, which measures simplen?gram overlap (in the present paper we usen = 4).
We also use ROUGE-SU, in which over-lap is computed using skip-bigrams while also ac-counting for unigrams that a text has in commonwith its reference (in order to avoid bias againsttexts which share several unigrams but few skipbigrams).Domain-dependent relevance metrics Themetrics described so far make use of domainknowledge only to the extent that this is reflectedin the textual markup.
We now consider a familyof metrics which are much more heavily relianton domain-specific knowledge structures andreasoning.
In our domain, the relevance of atext in a given experimental scenario s can bedefined in terms of whether the events it mentionshave some relationship to the appropriate clinicalactions (APs).
We attempt to model some aspectsof this using a weighting strategy and reasoningrules.Recall from Section 3 that fc?s represent thevarious physiological systems to which an eventor action can be related.
Therefore, each event ementioned in a text can be related to a set of pos-sible actions using the functional concepts fc(e)to which that event is linked in the ontology.
LetEs,t be the set of events mentioned in text t forscenario s. An event e ?
Es,t references an actiona iff FC(e) ?
FC(a) 6= ?.
Our hypothesis is thatan appropriate action is more likely to be taken ifthere are events which reference it in the text ?
thatis, if the text mentions things which are directly orindirectly relevant to the action.
For instance, if atext mentions events related to the RESPIRATIONfc, a clinician might be more likely to make a de-cision to manage a patient?s ventilation support.It is worth emphasising that, since both the BT-45 and human-authored texts were descriptive andwere not written or generated with the appropriateactions in mind, the hypothesis that the relevanceof the content to the appropriate actions might in-crease the likelihood of these actions being chosenis far from a foregone conclusion.Part of the novelty in this way of quantifyingrelevance lies in its use of the knowledge (i.e.
theontology) that is already available to the system,rather than asking human experts to rate the rele-vance of a text, a time-consuming process whichcould be subject to experts?
personal biases.
How-ever, this way of conceptualising relevance gener-ates links to too many actions for one event.
It isoften the case that an event, through its associationwith a functional concept, references more thanone action, but not all of these are appropriate.For example, a change in oxygen saturation canbe related to RESPIRATION, which itself is relatedto several respiration-related actions in a scenario,only some of which are appropriate.
Clearly, rele-vance depends not only on a physiological connec-tion between an event and a phsiological system(functional concept), but also on the context, thatis, the other events and their relative importancein a given scenario.
Another factor that needs tobe taken into account is the overall probability ofan action.
Some actions are performed routinely,while others tend to be associated with emergen-cies (e.g.
a nappy change is much more frequentover all than resuscitating a patient).
This meansthat some actions ?
even appropriate ones ?
mayhave been less likely to be selected even thoughthey were referenced by the text and were appro-priate.We prune unwarranted connections betweenevents and actions by taking into account (a) a pa-tient?s current status (described in the text and inthe background information given to experimentalparticipants); (b) the fact that some actions havemuch higher prior probabilities than others be-cause they are performed more routinely; (c) thefact that some events may be more important thanothers (e.g.
resuscitation is much more importantthan a nappy change).
Based on this, we define theweight of an action a as follows:Wa =Pe?EPr(a)?e.importancePa?AePr(a)Pe?E e.importance(1)Where E is the set of events in the text, Ae theset of actions related to event e, e.importance ?N+ the importance of the event e and Pr(a) theprior probability of action a.
All weights are nor-malised so that the following inequalities hold:Xa?AePr(a) ?
e.importancePa?AePr(a)= e.importance (2)Xa?AWa = 1 (3)where A is the set of all possible actions.
The ideais that an event e makes some contribution (pos-sibly 0) to the relevance of some actions Ae, andthe total weight of the event is distributed amongall actions related to it using (a) the prior probabil-ity Pr(a) of each action (the most frequent actionwill have more weight) and (b) the importance ofthe event.
At the end of the process each actionwould be assigned a score representing the accu-mulated weights of the events, which is then nor-malised, so that?a?A Wa = 1.The prior probability in the equation is meantto reflect our earlier observation that clinical ac-tions differ in the frequency with which they areperformed and this may bias their selection.
Pri-ors were computed using maximum likelihood es-timates from a large database containing exhaus-tive annotations of clinical actions recorded by anon-site research nurse over a period of 4 months ina NICU, which contains a total of 43,889 recordsof actions (Hunter et al, 2003).The importance value in equation (1) is meantto reflect the fact that events in the text do notattract the attention of a reader to the same ex-tent, since they do not have the same degreeof ?severity?
or ?surprise?.
We operationalisethis by identifying the superconcepts in the on-tology (PATHOLOGICAL-FUNCTION, DISORDER,SURGICAL-INTERVENTION, etc.)
which could bethought of as representing ?drastic?
occurrences.To these we added the concept of a TREND whichcorresponds to a change in a physiological param-eter (such as an increase in heart rate), based on therationale that the primary aim of NICU staff is tokeep a patient stable, so that any physiological in-stability warrants an intervention.
The importanceof events subsumed by these superconcepts wasthen set to be three times that of ?normal?
events.Finally, we apply knowledge-based rules toprune the number of actions Ae related to an evente.
As an example, a decision to intubate a babydepends not only on events in the text which ref-erence this action, but also on whether the baby isalready intubated.
This can be assessed by check-ing whether s/he is on CMV (a type of ventila-tion which is only used after intubation).
The ruleis represented as INTUBATE ?
?on(baby, CMV).Although such rules are extremely rough, they dohelp to prune inconsistencies.Two scores were computed for both humanand computer texts using equation (1).
RELs,tis the sum of weights of actions referenced ina text t for scenario s which are appropriate:RELs,t =?a?Aap Wa.
Conversely, IRRELs,tquantifies the weights of actions referenced in tfor scenario s which are inappropriate: IRRELs,t =?a?AinapWa.6 ResultsIn what follows, we report two-tailed Pearson?s rcorrelations to compare our metrics to the threeperformance measures discussed in Section 3: P ,the global performance score; PAPP and PINAPP, theproportion of appropriate (resp.
inappropriate) ac-tions selected from the subsets of in/appropriate(resp.
inappropriate) actions in a scenario; andSPAPP, the proportion of appropriate actions se-lected by a participant out of the set of actions se-lected.
The last three are included because theyshed light more directly on the extent to whichexperimental participants chose correctly or incor-rectly.
In case a metric measures similarity or dif-ference between texts, the correlation reported iswith the difference between the H scores and the Cscores.
Where relevant, we also report correlationswith the absolute mean performance scores withinthe H and/or C conditions.
Correlations excludethe three scenarios which had ?no action?
as thetarget appropriate action, though where relevant,we will indicate whether the correlations changewhen these scenarios are also included.6.1 Content and StructureOverall, the C texts mentioned significantly fewerevents than the H texts (t20 = 2.44, p = .05),and also mentioned fewer temporal and discourserelations explicitly (t20 = 3.70, p < .05).
InP (H-C) PAPP (H-C) SPAPP (H-C) PINAP (H-C)Events (H-C) .43?
.42?
.02 -.09Relations (H-C) .34 .30 0 -.15Tree Edit .36 .33 .09 -.14Table 1: Correlations between performance differences and content/structure measures.
?significant atp = .05; ?approaches significance at p = .06the case of the H texts, the number of eventsand relations did not correlate significantly withany of the performance scores.
In the case ofthe C texts, the number of relations mentionedwas significantly negatively correlated to PINAP(r = ?.49, p < .05), and positively correlatedto SPAPP (r = .7, p < .001).
This suggeststhat temporal and discourse relations made textsmore understandable and resulted in more appro-priate actions being taken.
More unexpectedly, thenumber of events mentioned was negatively cor-related to PAPP (r = ?.53, p < .05) and to P(r = ?.5, p < .05).
This may have been due to theC texts mentioning a number of events that wererelatively unimportant and/or irrelevant to the ap-propriate actions.Table 1 displays correlations between perfor-mance differences between H and C, and differ-ences in number of events and relations, as wellas Tree Edit Distance.
The positive correlationbetween the number of events mentioned and Psuggests that a larger amount of content in theH texts is partially responsible for the differencein decision-making accuracy by experimental par-ticipants.
This is further supported by the factthat the correlation with the difference in PAPP ap-proaches significance.
It is worth noting that noneof these correlations are significant when meansfrom the three ?no action?
scenarios are includedin the computation.
This further supports our ear-lier conclusion that these three scenarios are out-liers.
Somewhat surprisingly, Tree Edit Distancedoes not correlate significantly with any of the per-formance differences, though the correlations goin the expected directions (positive in the case ofP , SPAPP and PAPP, negative in the case of PINAP).This may be due to the high variance in the EditDistance scores (mean: 66.5; SD: 34.8).Overall, these results show that differences inboth content and structure made the H texts supe-rior and human texts did a much better job at ex-plicitly relating events or situating them in time,which is crucial for comprehension and correctdecision-making.
This point has previously beenAbsolute Scores (C) Differences (H-C)P PAP PINAP SPAP P PAP PINAP SPAPR-4 .33 .38 .2 -.03 -.19 -.2 -.01 -.1R-SU -.03 -.02 .05 -.31 .04 .01 -.1 .13Table 2: Correlations between ROUGE and perfor-mance scores in the C condition.
?significant atp = .05.made in relation to the same data on the basis of aqualitative study (Reiter et al, 2008).6.2 N-gram OverlapCorrelations with ROUGE-4 and ROUGE-SU areshown in Table 2 both for absolute performancescores on the C texts, and for the differences be-tween H and C. This is because ROUGE can beinterpreted in two ways: on the one hand, it mea-sures the ?quality?
of C texts relative to the ref-erence human texts; on the other it also indicatessimilarity between C and H.There are no significant correlations betweenROUGE and any of our performance measures.
Al-though this leaves open the question of whether adifferent set of performance measures, or a differ-ent experiment, would evince a more systematiccovariation, the results suggest that it is not sur-face similarity (to the extent that this is measuredby ROUGE) that is contributing to better decisionmaking.
It is however worth noting that some cor-relations with ROUGE-4, namely those involvingP and PAPP, do turn out significant when the ?noaction?
scenarios are included.
This turns out tobe solely due to one of the ?no action?
scenar-ios, which had a much higher ROUGE-4 score thanthe others, possibly because the corresponding hu-man text was comparatively brief and the numberof events mentioned in the two texts was roughlyequal (11 for the C text, 12 for the H text).6.3 Knowledge Based Relevance MetricsFinally, we compare our knowledge-based mea-sures of the relevance of the content to appropri-ate actions (REL) and to inappropriate actions (IR-REL).
The correlations between each measure andHuman (H) BT-45 (C)P PAP PINAP SPAP P PAP PINAP SPAPREL .14 .11 -.14 .60?
.33 .24 -.49?
.7?IRREL -.25 -.22 .1 -.56?
-.34 -.26 .43 -.62?Table 3: Correlations between knowledge-based relevance scores and absolute performance scores in theC and H conditions.
?significant at p ?
.05.the absolute performance scores in each conditionare displayed in Table 3.The absolute scores in Table 3 show that bothREL and IRREL are significantly correlated toSPAPP, the proportion of appropriate actions outof the actions selected by participants.
The cor-relations are in the expected direction: there is astrong tendency for participants to choose moreappropriate actions when REL is high, and the re-verse is true for IRREL.
In the case of the C texts,there is also a negative correlation (as expected)between REL and PINAP, though this is the onlyone that reaches significance with this variable.
Ittherefore appears that the knowledge-based rele-vance measures evince a meaningful relationshipwith at least some of the more ?direct?
measures ofperformance (those assessing the relative prefer-ence of participants for appropriate actions basedon a textual summary), though not with the globalpreference score P .
One possible reason for thelow correlations with the latter is that the two mea-sures attempt to quantify directly the relevance ofthe content units in a text to in/appropriate coursesof action; hence, they have a more direct relation-ship to measures of proportions of the courses ofactions chosen.7 Discussion and ConclusionsWe conclude this paper with some observationsabout the relative merit of different measures oftextual characteristics.
?Standard?, surface-basedmeasures such as (ROUGE) do not display any sys-tematic relationship with our extrinsic measuresof performance, recalling similar observations inthe NLG literature (Gatt and Belz, to appear) andin MT and Summarisation (Calliston-Burch et al,2006; Dorr et al, 2005).
Some authors have alsoreported that ROUGE does not correlate well withhuman judgements of NLG texts (Reiter and Belz,2009).
On the other hand, we do find some evi-dence that the amount of content in texts, and theextent to which they explicitly relate content el-ements temporally and rhetorically, may impactdecision-making.
The significant correlations ob-served between the number of relations in a textand the extrinsic measures are worth emphasis-ing, as they suggest a significant role not only forcontent, but also rhetorical and temporal structure,something that many metrics do not take into ac-count.Perhaps the most important contribution of thispaper has been to emphasise knowledge-based as-pects of textual evaluation, not only by measur-ing content units and structure, but also by de-veloping a motivated relevance metric, the cru-cial assumption being that the utility of a sum-mary is contingent on its managing to convey in-formation that will motivate a reader to take the?right?
course of action.
The strong correlationsbetween the relevance measures and the extent towhich people chose the correct actions (or moreaccurately, chose more correct actions) vindicatesthis assumption.Some of the correlations which turned out not tobe significant may be due to ?noise?
in the data, inparticular, high variance in the performance scores(as suggested by the standard deviations for Pgiven in Section 3).
They therefore do not war-rant the conclusion that no relationship exists be-tween a particular measure and extrinsic task per-formance; nevertheless, where other studies havenoted similar gaps, the trends in question may besystematic and general.
This, however, can onlybe ascertained in further follow-up studies.This paper has investigated the relationship be-tween a number of intrinsic measures of text qual-ity and decision-making performance based on anexternal task.
Emphasis was placed on metricsthat quantify aspects of semantics, relevance andstructure.
We have also compared generated textsto their human-authored counterparts to identifydifferences which can motivate further system im-provements.
Future work will focus on further ex-ploring metrics that reflect the relevance of a text,as well as the role of temporal and discourse struc-ture in conveying the intended meaning.ReferencesJ.
F. Allen.
1983.
Maintaining knowledge abouttemporal intervals.
Communications of the ACM,26(11):832?843.Charles B. Callaway and James C. Lester.
2002.Narrative prose generation.
Artificial Intelligence,139(2):213?252.C.
B. Callaway.
2003.
Evaluating coverage for largesymbolic NLG grammars.
In Proc.
IJCAI?03.C.
Calliston-Burch, M. Osborne, and P. Koehn.
2006.Re-evaluating the role of BLEU in machine transla-tion research.
In Proc.
EACL?06.B.
J. Dorr, C. Monz, S. President, R. Schwartz, andD.
Zajic.
2005.
A methodology for extrinsic evalu-ation of text summarization: Does ROUGE correlate?In Proc.
Workshop on Intrinsic and Extrinsic Evalu-ation Measures.M.E.
Foster.
2008.
Automated metrics that agree withhuman judgements on generated output for an em-bodied conversational agent.
In Proc.
INLG?08.A.
Gatt and A. Belz.
to appear.
Introducing shared taskevaluation to NLG: The TUNA shared task evalua-tion challenges.
In E. Krahmer and M. Theune, ed-itors, Empirical Methods in Natural Language Gen-eration.
Springer.A.
Gatt and F. Portet.
2009.
Text content and taskperformance in the evaluation of a natural languagegeneration system.
In Proc.
RANLP?09.J.
Hunter, G. Ewing, L. Ferguson, Y.
Freer, R. Logie,P.
McCue, and N. McIntosh.
2003.
The NEONATEdatabase.
In Proc.
IDAMAP?03.A.
Karasimos and A. Isard.
2004.
Multilingual eval-uation of a natural language generation system.
InProc.
LREC?04.I.
Langkilde-Geary.
2002.
An empirical verification ofcoverage and correctness for a general-purpose sen-tence generator.
In Proc.
INLG?02.J.C.
Lester and B.W.
Porter.
1997.
Developing andempirically evaluating robust explanation genera-tors: The KNIGHT experiments.
Computational Lin-guistics, 23(1):65?101.C-Y Lin and E. Hovy.
2003.
Automatic evaluation ofsummaries using n-gram co-occurrence statistics.
in.In Proc.
of HLT-NAACL?03.F.
Liu and Y. Liu.
2008.
Correlation between rougeand human evaluation of extractive meeting sum-maries.
In Proc.
ACL?08.W.
C. Mann and S.Thompson.
1988.
Rhetorical struc-ture theory: Towards a functional theory of text or-ganisation.
Text, 8(3):243?281.A.
Nenkova and R. Passonneau.
2004.
Evaluatingcontent selection in summarisation: The Pyramidmethod.
In Proc.
NAACL-HLT?04.S.
Papineni, T. Roukos, W. Ward, and W. Zhu.
2002.BLEU: A method for automatic evaluation of ma-chine translation.
In Proc.
ACL?02.L Plaza, A D?
?az, and P Gerva?s P. 2009.
Auto-matic summarization of news using wordnet conceptgraphs.
best paper award.
In Proc.
IADIS?09.F.
Portet, E. Reiter, A. Gatt, J.
Hunter, S. Sripada,Y.
Freer, and C. Sykes.
2009.
Automatic generationof textual summaries from neonatal intensive caredata.
Artificial Intelligence, 173(7?8):789?816.E.
Reiter and A. Belz.
2009.
An investigation into thevalidity of some metrics for automatically evaluat-ing Natural Language Generation systems.
Compu-tational Linguistics, 35(4):529?558.E.
Reiter, R. Robertson, and L. Osman.
2003.
Lessonsfrom a failure: Generating tailored smoking cessa-tion letters.
Artificial Intelligence, 144:41?58.E.
Reiter, S. Sripada, J.
Hunter, J. Yu, and I. Davy.2005.
Choosing words in computer-generatedweather forecasts.
Artificial Intelligence, 167:137?169.E.
Reiter, A. Gatt, F. Portet, and M. van der Meulen.2008.
The importance of narrative and other lessonsfrom an evaluation of an NLG system that sum-marises clinical data.
In Proc.
INLG?08.K.
Spa?rck-Jones and J. R. Galliers.
1996.
Evaluatingnatural language processing systems: An analysisand review.
Springer, Berlin.O.
Stock, M. Zancanaro, P. Busetta, C. Callaway,A.
Krueger, M. Kruppa, T. Kuflik, E. Not, andC.
Rocchi.
2007.
Adaptive, intelligent presen-tation of information for the museum visitor inPEACH.
User Modeling and User-Adapted Interac-tion, 17(3):257?304.M.
van der Meulen, R. H. Logie, Y.
Freer, C. Sykes,N.
McIntosh, and J.
Hunter.
2009.
When a graph ispoorer than 100 words.
Applied Cognitive Psychol-ogy, 24(1):77?89.I.
Yoo, X. Hu, and I-Y Song.
2007.
A coherentgraph-based semantic clustering and summarizationapproach for biomedical literature and a new sum-marization evaluation method.
BMC Bioinformat-ics, 8(9).
