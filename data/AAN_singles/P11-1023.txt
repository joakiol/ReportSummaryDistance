Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 220?229,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsMEANT: An inexpensive, high-accuracy, semi-automatic metric forevaluating translation utility via semantic framesChi-kiu Lo and DekaiWuHKUSTHuman Language Technology CenterDepartment of Computer Science and EngineeringHong Kong University of Science and Technology{jackielo,dekai}@cs.ust.hkAbstractWe introduce a novel semi-automated metric,MEANT, that assesses translation utility by match-ing semantic role fillers, producing scores that cor-relate with human judgment as well as HTER butat much lower labor cost.
As machine transla-tion systems improve in lexical choice and flu-ency, the shortcomings of widespread n-gram based,fluency-oriented MT evaluation metrics such asBLEU, which fail to properly evaluate adequacy,become more apparent.
But more accurate, non-automatic adequacy-oriented MT evaluation metricslike HTER are highly labor-intensive, which bottle-necks the evaluation cycle.
We first show that whenusing untrained monolingual readers to annotate se-mantic roles in MT output, the non-automatic ver-sion of the metric HMEANT achieves a 0.43 corre-lation coefficient with human adequacy judgments atthe sentence level, far superior to BLEU at only 0.20,and equal to the far more expensive HTER.
We thenreplace the human semantic role annotators with au-tomatic shallow semantic parsing to further automatethe evaluation metric, and show that even the semi-automated evaluation metric achieves a 0.34 corre-lation coefficient with human adequacy judgment,which is still about 80% as closely correlated asHTER despite an even lower labor cost for the evalu-ation procedure.
The results show that our proposedmetric is significantly better correlated with humanjudgment on adequacy than current widespread au-tomatic evaluation metrics, while being much morecost effective than HTER.1 IntroductionIn this paper we show that evaluating machine trans-lation by assessing the translation accuracy of each argu-ment in the semantic role framework correlates with hu-man judgment on translation adequacy as well as HTER,at a significantly lower labor cost.
The correlation of thisnew metric, MEANT, with human judgment is far supe-rior to BLEU and other automatic n-gram based evalua-tion metrics.We argue that BLEU (Papineni et al, 2002) and otherautomatic n-gram basedMT evaluation metrics do not ad-equately capture the similarity in meaning between themachine translation and the reference translation?which,ultimately, is essential for MT output to be useful.
N-gram based metrics assume that ?good?
translations tendto share the same lexical choices as the reference trans-lations.
While BLEU score performs well in captur-ing the translation fluency, Callison-Burch et al (2006)and Koehn and Monz (2006) report cases where BLEUstrongly disagree with human judgment on translationquality.
The underlying reason is that lexical similaritydoes not adequately reflect the similarity in meaning.
AsMT systems improve, the shortcomings of the n-grambased evaluation metrics are becoming more apparent.State-of-the-art MT systems are often able to output flu-ent translations that are nearly grammatical and containroughly the correct words, but still fail to express mean-ing that is close to the input.At the same time, although HTER (Snover et al, 2006)is more adequacy-oriented, it is only employed in verylarge scale MT system evaluation instead of day-to-dayresearch activities.
The underlying reason is that it re-quires rigorously trained human experts to make difficultcombinatorial decisions on the minimal number of editsso as to make the MT output convey the same meaning asthe reference translation?a highly labor-intensive, costlyprocess that bottlenecks the evaluation cycle.Instead, with MEANT, we adopt at the outset theprinciple that a good translation is one that is useful,in the sense that human readers may successfully un-derstand at least the basic event structure?
?who didwhat to whom, when, where and why?
(Pradhan et al,2004)?representing the central meaning of the source ut-terances.
It is true that limited tasks might exist for whichinadequate translations are still useful.
But for meaning-ful tasks, generally speaking, for a translation to be use-ful, at least the basic event structure must be correctly un-derstood.
Therefore, our objective is to evaluate trans-lation utility: from a user?s point of view, how well is220the most essential semantic information being capturedby machine translation systems?In this paper, we detail the methodology that underliesMEANT, which extends and implements preliminary di-rections proposed in (Lo andWu, 2010a) and (Lo andWu,2010b).
We present the results of evaluating translationutility by measuring the accuracy within a semantic rolelabeling (SRL) framework.
We show empirically that ourproposed SRL based evaluation metric, which uses un-trained monolingual humans to annotate semantic framesinMT output, correlates with human adequacy judgmentsas well as HTER, and far better than BLEU and othercommonly used metrics.
Finally, we show that replacingthe human semantic role labelers with an automatic shal-low semantic parser in our proposed metric yields an ap-proximation that is about 80% as closely correlated withhuman judgment as HTER, at an even lower cost?andis still far better correlated than n-gram based evaluationmetrics.2 Related workLexical similarity based metrics BLEU (Papineni etal., 2002) is the most widely used MT evaluation met-ric despite the fact that a number of large scale meta-evaluations (Callison-Burch et al, 2006; Koehn andMonz, 2006) report cases where it strongly disagree withhuman judgment on translation accuracy.
Other lexi-cal similarity based automatic MT evaluation metrics,like NIST (Doddington, 2002), METEOR (Banerjee andLavie, 2005), PER (Tillmann et al, 1997), CDER (Leuschet al, 2006) and WER (Nie?en et al, 2000), also per-form well in capturing translation fluency, but share thesame problem that although evaluation with these metricscan be done very quickly at low cost, their underlying as-sumption?that a ?good?
translation is one that shares thesame lexical choices as the reference translation?is notjustified semantically.
Lexical similarity does not ade-quately reflect similarity in meaning.
State-of-the-art MTsystems are often able to output translations containingroughly the correct words, yet expressing meaning that isnot close to that of the input.We argue that a translation metric that reflects meaningsimilarity is better based on similarity in semantic struc-ture, rather than simply flat lexical similarity.HTER (non-automatic) Despite the fact that Human-targeted Translation Edit Rate (HTER) as proposed bySnover et al (2006) shows a high correlation with humanjudgment on translation adequacy, it is not widely used inday-to-day machine translation evaluation because of itshigh labor cost.
HTER not only requires human expertsto understand the meaning expressed in both the refer-ence translation and the machine translation, but also re-quires them to propose the minimum number of edits tothe MT output such that the post-edited MT output con-veys the same meaning as the reference translation.
Re-quiring such heavy manual decision making greatly in-creases the cost of evaluation, bottlenecking the evalua-tion cycle.To reduce the cost of evaluation, we aim to reduce anyhuman decisions in the evaluation cycle to be as simpleas possible, such that even untrained humans can quicklycomplete the evaluation.
The human decisions shouldalso be defined in a way that can be closely approximatedby automatic methods, so that similar objective functionsmight potentially be used for tuning in MT system devel-opment cycles.Task based metrics (non-automatic) Voss and Tate(2006) proposed a task-based approach to MT evaluationthat is in some ways similar in spirit to ours, but ratherthan evaluating how well people understand the mean-ing as a whole conveyed by a sentence translation, theymeasured the recall with which humans can extract one ofthe who, when, or where elements from MT output?andwithout attaching them to any predicate or frame.
Alarge number of human subjects were instructed to extractonly one particular type of wh-item from each sentence.They evaluated only whether the role fillers were cor-rectly identified, without checking whether the roles wereappropriately attached to the correct predicate.
Also, theactor, experiencer, and patient were all conflated into theundistinguished who role, while other crucial elements,like the action, purpose, manner, were ignored.Instead, we argue, evaluating meaning similarityshould be done by evaluating the semantic structure asa whole: (a) all core semantic roles should be checked,and (b) not only should we evaluate the presence of se-mantic role fillers in isolation, but also their relations tothe frames?
predicates.Syntax based metrics Unlike Voss and Tate, Liu andGildea (2005) proposed a structural approach, but it wasbased on syntactic rather than semantic structure, and fo-cused on checking the correctness of the role structurewithout checking the correctness of the role fillers.
Theirsubtree metric (STM) and headword chain metric (HWC)address the failure of BLEU to evaluate translation gram-maticality; however, the problem remains that a gram-matical translation can achieve a high syntax-based scoreeven if contains meaning errors arising from confusion ofsemantic roles.STM was the first proposed metric to incorporate syn-tactic features in MT evaluation, and STM underlies mostother recently proposed syntactic MT evaluation met-rics, for example the evaluation metric based on lexical-functional grammar of Owczarzak et al (2008).
STM isa precision-based metric that measures what fraction ofsubtree structures are shared between the parse trees of221machine translations and reference translations (averag-ing over subtrees up to some depth threshold).
UnlikeVoss and Tate, however, STM does not check whether therole fillers are correctly translated.HWC is similar, but is based on dependency trees con-taining lexical as well as syntactic information.
HWCmeasures what fraction of headword chains (a sequenceof words corresponding to a path in the dependency tree)also appear in the reference dependency tree.
This can beseen as a similarity measure on n-grams of dependencychains.
Note that the HWC?s notion of lexical similaritystill requires exact word match.Although STM-like syntax-based metrics are an im-provement over flat lexical similarity metrics like BLEU,they are still more fluency-oriented than adequacy-oriented.
Similarity of syntactic rather than semanticstructure still inadequately reflects meaning preservation.Moreover, properly measuring translation utility requiresverifying whether role fillers have been correctly trans-lated?verifying only the abstract structures fails to pe-nalize when role fillers are confused.Semantic roles as features in aggregate metricsGime?nez and Ma`rquez (2007, 2008) introduced ULC, anautomatic MT evaluation metric that aggregates manytypes of features, including several shallow semantic sim-ilarity features: semantic role overlapping, semantic rolematching, and semantic structure overlapping.
Unlike Liuand Gildea (2007) who use discriminative training to tunethe weight on each feature, ULC uses uniform weights.Although the metric shows an improved correlation withhuman judgment of translation quality (Callison-Burch etal., 2007; Gime?nez and Ma`rquez, 2007; Callison-Burchet al, 2008; Gime?nez and Ma`rquez, 2008), it is not com-monly used in large-scale MT evaluation campaigns, per-haps due to its high time cost and/or the difficulty of in-terpreting its score because of its highly complex combi-nation of many heterogenous types of features.Specifically, note that the feature based representationsof semantic roles used in these aggregate metrics do notactually capture the structural predicate-argument rela-tions.
?Semantic structure overlapping?
can be seen asthe shallow semantic version of STM: it only measuresthe similarity of the tree structure of the semantic roles,without considering the lexical realization.
?Semanticrole overlapping?
calculates the degree of lexical overlapbetween semantic roles of the same type in the machinetranslation and its reference translation, using simple bag-of-words counting; this is then aggregated into an averageover all semantic role types.
?Semantic role matching?is just like ?semantic role overlapping?, except that bag-of-words degree of similarity is replaced (rather harshly)by a boolean indicating whether the role fillers are an ex-act string match.
It is important to note that ?semanticrole overlapping?
and ?semantic role matching?
both useflat feature based representations which do not capture thestructural relations in semantic frames, i.e., the predicate-argument relations.Like system combination approaches, ULC is a vastlymore complex aggregate metric compared to widely usedmetrics like BLEU or STM.
We believe it is importantto retain a focus on developing simpler metrics whichnot only correlate well with human adequacy judgments,but nevertheless still directly provide representationaltransparency via simple, clear, and transparent scoringschemes that are (a) easily human readable to support er-ror analysis, and (b) potentially directly usable for auto-matic credit/blame assignment in tuning tree-structuredSMT systems.
We also believe that to provide a foun-dation for better design of efficient automated metrics,making use of humans for annotating semantic roles andjudging the role translation accuracy in MT output is anessential step that should not be bypassed, in order to ade-quately understand the upper bounds of such techniques.We agree with Przybocki et al (2010), who observein the NIST MetricsMaTr 2008 report that ?human [ade-quacy] assessments only pertain to the translations evalu-ated, and are of no use even to updated translations fromthe same systems?.
Instead, we aim for MT evaluationmetrics that provide fine-grained scores in a way that alsodirectly reflects interpretable insights on the strengths andweaknesses of MT systems rather than simply replicatinghuman assessments.3 MEANT: SRL for MT evaluationA good translation is one from which human readersmay successfully understand at least the basic event struc-ture?
?who did what to whom, when, where and why?
(Pradhan et al, 2004)?which represents the most essen-tial meaning of the source utterances.MEANT measures this as follows.
First, semantic rolelabeling is performed (either manually or automatically)on both the reference translation and the machine transla-tion.
The semantic frame structures thus obtained for theMT output are compared to those in the reference transla-tions, frame by frame, argument by argument.
The frametranslation accuracy is a weighted sum of the number ofcorrectly translated arguments.
Conceptually, MEANTis defined in terms of f-score, with respect to the preci-sion/recall for sentence translation accuracy as calculatedby averaging the translation accuracy for all frames in theMT output across the number of frames in the MT out-put/reference translations.
Details are given below.3.1 Annotating semantic framesIn designing a semantic MT evaluation metric, one im-portant issue that should be addressed is how to evaluatethe similarity of meaning objectively and systematically222Figure 1: Example of source sentence and reference translation with reconstructed semantic frames in Propbank format and MToutput with reconstructed semantic frames by minimal trained human annotators.
Following Propbank, there are no semantic framesfor MT3 because there is no predicate.using fine-grained measures.
We adopted the PropbankSRL style predicate-argument framework, which capturesthe basic event structure in a sentence in a way that clearlyindicates many strengths and weaknesses of MT.
Figure 1shows the reference translationwith reconstructed seman-tic frames in Propbank format and the corresponding MToutput with reconstructed semantic frames by minimaltrained human annotators.3.2 Comparing semantic framesAfter annotating the semantic frames, we must deter-mine the translation accuracy for each semantic role fillerin the reference and machine translations.
Although ulti-mately it would be nice to do this automatically, it is es-sential to first understand extremely well the upper boundof accuracy for MT evaluation via semantic frame theory.Thus, instead of resorting to excessively permissive bag-of-words matching or excessively restrictive exact stringmatching, for the experiments reported here we employeda group of human judges to evaluate the correctness ofeach role filler translation between the reference and ma-chine translations.In order to facilitate a finer-grained measurement ofutility, the human judges were not only allowed to markeach role filler translation as ?correct?
or ?incorrect?, butalso ?partial?.
Translations of role fillers are judged ?cor-rect?
if they express the same meaning as that of the refer-ence translations (or the original source input, in the bilin-guals experiment discussed later).
Translations may alsobe judged ?partial?
if only part of the meaning is correctlytranslated.
Extra meaning in a role filler is not penalizedunless it belongs in another role.
We also assume that awrongly translated predicate means that the entire seman-tic frame is incorrect; therefore, the ?correct?
and ?par-tial?
argument counts are collected only if their associatedpredicate is correctly translated in the first place.Table 1 shows an example of SRL annotation of MT1in Figure 1 by one of the annotators, along with the humanjudgment on translation accuracy of each argument.
Thepredicate ceased in the reference translation did not matchwith any predicate annotated in MT1, while the predicateresumed matched with the predicate resume annotated inMT1.
All arguments of the untranslated ceased are auto-matically considered incorrect (with no need to considereach argument individually), under our assumption that awrongly translated predicate causes the entire event frameto be considered mistranslated.
The ARGM-TMP argu-ment, Until after their sales had ceased in mainland China foralmost two months, in the reference translation is partiallytranslated to ARGM-TMP argument, So far , nearly twomonths, inMT1.
Similar decisions are made for the ARG1argument and the other ARGM-TMP argument; now inthe reference translation is missing in MT1.3.3 Quantifying semantic frame matchTo quantify the above in a summary metric, we defineMEANT in terms of an f-score that balances the precisionand recall analysis of the comparative matrices collectedfrom the human judges, as follows.Ci,j = # correct fillers of ARG j for PRED i in MTPi,j = # partial fillers of ARG j for PRED i in MTMi,j = total # fillers of ARG j for PRED i in MTRi,j = total # fillers of ARG j of PRED i in REF223Table 1: SRL annotation of MT1 in Figure 1 and the human judgment of translation accuracy for each argument (see text).SRL REF MT1 DecisionPRED (Action) ceased ?
no matchPRED (Action) resumed resume matchARG0 (Agent) ?
sk - ii the sale of products inthe mainland of ChinaincorrectARG1 (Experiencer) sales of complete range of SK - II products sales partialARGM-TMP (Temporal) Until after , their sales had ceased in mainlandChina for almost two monthsSo far , nearly two months partialARGM-TMP (Temporal) now ?
incorrectCprecision =?matched iwpred +?j wjCi,jwpred +?j wjMi,jCrecall =?matched iwpred +?j wjCi,jwpred +?j wjRi,jPprecision =?matched i?j wjPi,jwpred +?j wjMi,jPrecall =?matched i?j wjPi,jwpred +?j wjRi,jprecision = Cprecision + (wpartial ?
Pprecision)total # predicates in MTrecall = Crecall + (wpartial ?
Precall)total # predicates in REFf-score = 2 ?
precision ?
recallprecision+ recallCprecision, Pprecision, Crecall and Precall are the sum of thefractional counts of correctly or partially translated se-mantic frames in theMT output and the reference, respec-tively, which can be viewed as the true positive for pre-cision and recall of the whole semantic structure in onesource utterence.
Therefore, the SRL based MT evalua-tion metric is equivalent to the f-score, i.e., the translationaccuracy for the whole predicate-argument structure.Note that wpred, wj and wpartial are the weights for thematched predicate, arguments of type j, and partial trans-lations.
These weights can be viewed as the importanceof meaning preservation for each different category of se-mantic roles, and the penalty for partial translations.
Wewill describe below how these weights are estimated.If all the reconstructed semantic frames in the MT out-put are completely identical to those annotated in the ref-erence translation, and all the arguments in the recon-structed frames express the same meaning as the corre-sponding arguments in the reference translations, then thef-score will be equal to 1.For instance, consider MT1 in Figure 1.
The numberof frames in MT1 and the reference translation are 1 and2, respectively.
The total number of participants (includ-ing both predicates and arguments) of the resume framein both MT1 and the reference translation is 4 (one pred-icate and three arguments), with 2 of the arguments (oneARG1/experiencer and one ARGM-TMP/temporal) onlypartially translated.
Assuming for now that the metric ag-gregates ten types of semantic roles with uniform weightfor each role (optimization of weights will be discussedlater), then wpred = wj = 0.1, and so Cprecision and Crecallare both zero while Pprecision and Precallare both 0.5.
If wefurther assume thatwpartial = 0.5, then precison and recallare 0.25 and 0.125 respectively.
Thus the f-score for thisexample is 0.17.Both human and semi-automatic variants of theMEANT translation evaluation metric were meta-evaluated, as described next.4 Meta-evaluation methodology4.1 Evaluation CorpusWe leverage work from Phase 2.5 of the DARPAGALE program in which both a subset of the Chinesesource sentences, as well as their English reference, arebeing annotated with semantic role labels in Propbankstyle.
The corpus also includes three participating state-of-the-art MT systems?
output.
For present purposes, werandomly drew 40 sentences from the newswire genre ofthe corpus to form a meta-evaluation corpus.
To maintaina controlled environment for experiments and consistentcomparison, the evaluation corpus is fixed throughout thiswork.4.2 Correlation with human judgements onadequacyWe followed the benchmark assessment procedure inWMT and NIST MetricsMaTr (Callison-Burch et al,2008, 2010), assessing the performance of the proposedevaluation metric at the sentence level using ranking pref-erence consistency, which also known as Kendall?s ?
rankcorrelation coefficient, to evaluate the correlation of theproposed metric with human judgments on translation ad-equacy ranking.
A higher value for ?
indicates more simi-larity to the ranking by the evaluation metric to the humanjudgment.
The range of possible values of correlation co-efficient is [-1,1], where 1 means the systems are ranked224Table 2: List of semantic roles that human judges are requestedto label.Label Event Label EventAgent who Location whereAction did Purpose whyExperiencer what Manner howPatient whom Degree or Extent howTemporal when Other adverbial arg.
howin the same order as the human judgment and -1 meansthe systems are ranked in the reverse order as the humanjudgment.5 Experiment: Using human SRLThe first experiment aims to provide a more concreteunderstanding of one of the key questions as to the upperbounds of the proposed evaluation metric: how well canhuman annotators perform in reconstructing the semanticframes in MT output?
This is important since MT out-put is still not close to perfectly grammatical for a goodsyntactic parsing?applying automatic shallow semanticparsers, which are trained on grammatical input and validsyntactic parse trees, on MT output may significantly un-derestimate translation utility.5.1 Experimental setupWe thus introduce HMEANT, a variant of MEANTbased on the idea that semantic role labeling can be sim-plified into a task that is easy and fast even for untrainedhumans.
The human annotators are given only very sim-ple instructions of less than half a page, along with twoexamples.
Table 2 shows the list of labels annotators arerequested to annotate, where the semantic role labelinginstructions are given in the intuitive terms of ?who didwhat to whom, when, where, why and how?.
To facili-tate the inter-annotator agreement experiments discussedlater, each sentence is independently assigned to at leasttwo annotators.After calculating the SRL scores based on the confu-sion matrix collected from the annotation and evaluation,we estimate the weights using grid search to optimize cor-relation with human adequacy judgments.5.2 Results: Correlation with human judgementTable 3 shows results indicating that HMEANT corre-lates with human judgment on adequacy as well as HTERdoes (0.432), and is far superior to BLEU (0.198) or othersurface-oriented metrics.Inspection of the cross validation results shown in Ta-ble 4 indicates that the estimated weights are not over-fitting.
Recall that the weights used in HMEANT areglobally estimated (by grid search) using the evaluationTable 3: Sentence-level correlation with human adequacy judg-ments, across the evaluation metrics.Metrics Kendall ?HMEANT 0.4324HTER 0.4324NIST 0.2883BLEU 0.1982METEOR 0.1982TER 0.1982PER 0.1982CDER 0.1171WER 0.0991Table 4: Analysis of stability for HMEANT?s weight settings,withRHMEANT rank and Kendall?s ?
correlation scores (see text).Fold 0 Fold 1 Fold 2 Fold 3RHMEANT 3 1 3 5distinct R 16 29 19 17?HMEANT 0.33 0.48 0.48 0.40?HTER 0.59 0.41 0.44 0.30?CV train 0.45 0.42 0.40 0.43?CV test 0.33 0.37 0.48 0.40corpus.
To analyze stability, the corpus is also parti-tioned randomly into four folds of equal size.
For eachfold, another grid search is also run.
RHMEANT is therank at which the Kendall?s correlation for HMEANTis found, if the Kendall?s correlations for all points inthe grid search space are sorted.
Many similar weight-vectors produce the same Kendall?s correlation score, so?distinct R?
shows how many distinct Kendall?s corre-lation scores exist in each case?between 16 and 29.HMEANT?s weight settings always produce Kendall?scorrelation scores among the top 5, regardless of whichfold is chosen, indicating good stability of HMEANT?sweight-vector.Next, Kendall?s ?
correlation scores are shown forHMEANT on each fold.
They vary from 0.33 to 0.48,and are at least as stable as those shown for HTER, where?
varies from 0.30 to 0.59.Finally, ?CV shows Kendall?s correlations if the weight-vector is instead subjected to full cross-validation trainingand testing, again demonstrating good stability.
In fact,the correlations for the training set in three of the folds (0,2, and 3) are identical to those for HMEANT.5.3 Results: Cost of evaluatingThe time needed for training non-expert humans tocarry out our annotation protocol is significantly less thanHTER and gold standard Propbank annotation.
The half-page instructions given to annotators required only be-tween 5 to 15 minutes for all annotators, including time225for asking questions if necessary.
Aside from providingtwo annotated examples, no further training was given.Similarly, the time needed for running the evaluationmetric is also significantly less than HTER?under atmost 5 minutes per sentence, even for non-expert humansusing no computer-assisted UI tools.
The average timeused for annotating each sentence was lower bounded by2 minutes and upper bounded by 3 minutes, and the timeused for determing the translation accuracy of role fillersaveraged under 2 minutes.Note that these figures are for unskilled non-experts.These times tend to diminish significantly after annotatorsacquire experience.6 Experiment: Monolinguals vs. bilingualsWe now show that using monolingual annotators is es-sentially just as effective as using more expensive bilin-gual annotators.
We study the cost/benefit trade-off ofusing human annotators from different language back-grounds for the proposed evaluation metric, and comparewhether providing the original source text helps.
Notethat this experiment focuses on the SRL annotation step,rather than the judgments of role filler paraphrasing accu-racy, because the latter is only a simple three-way deci-sion between ?correct?, ?partial?, and ?incorrect?
that isfar less sensitive to the annotators?
language backgrounds.MT output is typically poor.
Therefore, readers ofMT output often guess the original meaning in the sourceinput using their own language background knowledge.Readers?
language background thus affects their under-standing of the translation, which could affect the accu-racy of capturing the key semantic roles in the translation.6.1 Experimental SetupBoth English monolinguals and Chinese-English bilin-guals (Chinese as first language and English as secondlanguage) were employed to annotate the semantic roles.For bilinguals, we also experimented with the differencein guessing constraints by optionally providing the origi-nal source input together with the translation.
Therefore,there are three variations in the experiment setup: mono-linguals seeing translation output only; bilinguals seeingtranslation output only; and bilinguals seeing both inputand output.The aim here is to do a rough sanity check on the effectof the variation of language background of the annotators;thus for these experiments we have not run the weight es-timation step after SRL based f-score calculation.
Instead,we simply assigned a uniform weight to all the seman-tic elements, and evaluated the variation under the sameweight settings.
(The correlation scores reported in thissection are thus expected to be lower than that reported inthe last section.
)Table 5: Sentence-level correlation with human adequacy judg-ments, for monolinguals vs. bilinguals.
Uniform rather than op-timized weights are used.Metrics Kendall ?HMEANT - bilinguals 0.3514HMEANT - monolinguals 0.3153HMEANT - bilinguals with input 0.31536.2 ResultsTable 5 of our results shows that using more expen-sive bilinguals for SRL annotation instead of monolin-guals improves the correlation only slightly.
The cor-relation coefficient of the SRL based evaluation metricdriven by bilingual human annotators (0.351) is slightlybetter than that driven by monolingual human annotators(0.315); however, using bilinguals in the evaluation pro-cess is more costly than using monolinguals.The results show that even allowing the bilinguals tosee the input as well as the translation output for SRLannotation does not help the correlation.
The correlationcoefficient of the SRL based evaluation metric driven bybilingual human annotators who see also the source in-put sentences is 0.315 which is the same as that driven bymonolingual human annotators.
We find that the correla-tion coefficient of the proposed with human judgment onadequacy drops when bilinguals are shown to the sourceinput sentence during annotation.
Error analyses leadus to believe that annotators will drop some parts of themeaning in the translations when trying to align them tothe source input.This suggests that HMEANT requires only monolin-gual English annotators, who can be employed at lowcost.7 Inter-annotator agreementOne of the concerns of the proposed metric is that,given only minimal training on the task, humans wouldannotate the semantic roles so inconsistently as to reducethe reliability of the evaluation metric.
Inter-annotatoragreement (IAA) measures the consistency of human inperforming the annotation task.
A high IAA suggests thatthe annotation is consistent and the evaluation results arereliable and reproducible.To obtain a clear analysis on where any inconsistencymight lie, we measured IAA in two steps: role identifica-tion and role classification.7.1 Experimental setupRole identification Since annotators are not consistentin handling articles or punctuation at the beginning orthe end of the annotated arguments, the agreement of se-mantic role identification is counted over the matching of226Table 6: Inter-annotator agreement rate on role identification(matching of word span)Experiments REF MTbilinguals working on output only 76% 72%monolinguals working on output only 93% 75%bilinguals working on input-output 75% 73%Table 7: Inter-annotator agreement rate on role classification(matching of role label associated with matched word span)Experiments Ref MTbilinguals working on output only 69% 65%monolinguals working on output only 88% 70%bilinguals working on input-output 70% 69%word span in the annotated role fillers with a toleranceof ?1 word in mismatch.
The inter-annotator agreementrate (IAA) on the role identification task is calculated asfollows.
A1 andA2 denote the number of annotated pred-icates and arguments by annotator 1 and annotator 2 re-spectively.
Mspan denotes the number of annotated pred-icates and arguments with matching word span betweenannotators.Pidentification =MspanA1Ridentification =MspanA2IAAidentification =2 ?
Pidentification ?RidentificationPidentification +RidentificationRole classification The agreement of classified rolesis counted over the matching of the semantic role labelswithin two aligned word spans.
The IAA on the role clas-sification task is calculated as follows.
Mlabel denotesthe number of annotated predicates and arguments withmatching role label between annotators.Pclassification =MlabelA1Rclassification =MlabelA2IAAclassification =2 ?
Pclassification ?RclassificationPclassification +Rclassification7.2 ResultsThe high inter-annotator agreement suggests that theannotation instructions provided to the annotators are ingeneral sufficient and the evaluation is repeatable andcould be automated in the future.
Table 6 and 7 show theannotators reconstructed the semantic frames quite con-sistently, even they were given only simple and minimaltraining.We have noticed that the agreement on role identifica-tion is higher than that on role classification.
This sug-gests that there are role confusion errors among the an-notators.
We expect a slightly more detailed instructionsand explanations on different roles will further improvethe IAA on role classification.The results also show that monolinguals seeing outputonly have the highest IAA in semantic frame reconstruc-tion.
Data analyses lead us to believe the monolingualsare the most constrained group in the experiments.
Themonolingual annotators can only guess the meaning inthe MT output using their English language knowledge.Therefore, they all understand the translation almost thesame way, even if the translation is incorrect.On the other hand, bilinguals seeing both the input andoutput discover the mistranslated portions, and often un-consciously try to compensate by re-interpreting the MToutput with information not necessarily appearing in thetranslation, in order to better annotate what they thinkit should have conveyed.
Since there are many degreesof freedom in this sort of compensatory re-interpretation,this group achieved a lower IAA than the monolinguals.Bilinguals seeing only output appear to take this even astep further: confrontedwith a poor translation, they oftenunconsciously try to guess what the original input mighthave been.
Consequently, they agree the least, becausethey have the most freedom in applying their own knowl-edge of the unseen input language, when compensatingfor poor translations.8 Experiment: Using automatic SRLIn the previous experiment, we showed that the pro-posed evaluation metric driven by human semantic roleannotators performed as well as HTER.
It is now worthasking a deeper question: can we further reduce the la-bor cost of MEANT by using automatic shallow semanticparsing instead of humans for semantic role labeling?Note that this experiment focuses on understanding thecost/benefit trade-off for the semantic frame reconstruc-tion step.
For SRL annotation, we replace humans withautomatic shallow semantic parsing.
We decouple thisfrom the ternary judgments of role filler accuracy, whichare still made by humans.
However, we believe the eval-uation of role filler accuracy will also be automatable.8.1 Experimental setupWe performed three variations of the experiments toassess the performance degradation from the automaticapproximation of semantic frame reconstruction in eachtranslation (reference translation and MT output): we ap-plied automatic shallow semantic parsing on the MT out-put only; on the reference translation only; and on bothreference translation and MT output.
For the semantic227Table 8: Sentence-level correlation with human adequacy judg-ments.
*The weights for individual roles in the metric are tunedby optimizing the correlation.Metrics Kendall ?HTER 0.4324HMEANT gold - monolinguals * 0.4324HMEANT auto - monolinguals * 0.3964MEANT gold - auto * 0.3694MEANT auto - auto * 0.3423NIST 0.2883BLEU / METEOR / TER / PER 0.1982CDER 0.1171WER 0.0991parser, we used ASSERT (Pradhan et al, 2004) whichachieves roughly 87% semantic role labeling accuracy.8.2 ResultsTable 8 shows that the proposed SRL based evaluationmetric correlates slightly worse than HTER with a muchlower labor cost.
The correlation with human judgmenton adequacy of the fully automated SRL annotation ver-sion, i.e., applying ASSERT on both the reference transla-tion and the MT output, of the SRL based evaluation met-ric is about 80% of that of HTER.
The results also showthat the correlation with human judgment on adequacy ofeither one side of translation using automatic SRL is inthe 85% to 95% range of that HTER.9 ConclusionWe have presented MEANT, a novel semantic MTevaluation metric that assesses the translation accuracyvia Propbank-style semantic predicates, roles, and fillers.MEANT provides an intuitive picture on how much in-formation is correctly translated in the MT output.MEANT can be run using inexpensive untrainedmono-linguals and yet correlates with human judgments on ad-equacy as well as HTER with a lower labor cost.
In con-trast to HTER, which requires rigorous training of humanexperts to find aminimum edit of the translation (an expo-nentially large search space), MEANT requires untrainedhumans to make well-defined, bounded decisions on an-notating semantic roles and judging translation correct-ness.
The process by which MEANT reconstructs the se-mantic frames in a translation and then judges translationcorrectness of the role fillers conceptually models howhumans read and understand translation output.We also showed that using automatic shallow seman-tic parser to further reduce the labor cost of the pro-posed metric successfully approximates roughly 80% ofthe correlation with human judgment on adequacy.
Theresults suggest future potential for a fully automatic vari-ant of MEANT that could out-perform current automaticMT evaluation metrics and still perform near the level ofHTER.Numerous intriguing questions arise from this work.
Afurther investigation into the correlation of each of the in-dividual roles to human adequacy judgments is detailedelsewhere, along with additional improvements to theMEANT family of metrics (Lo and Wu, 2011).
Anotherinteresting investigation would then be to similarly repli-cate this analysis of the impact of each individual role, butusing automatically rather than manually labeled seman-tic roles, in order to ascertain whether the more difficultsemantic roles for automatic semantic parsers might alsocorrespond to the less important aspects of end-to-endMTutility.AcknowledgmentsThis material is based upon work supported in partby the Defense Advanced Research Projects Agency(DARPA) under GALE Contract Nos.
HR0011-06-C-0022 and HR0011-06-C-0023 and by the HongKong Research Grants Council (RGC) researchgrants GRF621008, GRF612806, DAG03/04.EG09,RGC6256/00E, and RGC6083/99E.
Any opinions,findings and conclusions or recommendations expressedin this material are those of the authors and do notnecessarily reflect the views of the Defense AdvancedResearch Projects Agency.ReferencesSatanjeev Banerjee and Alon Lavie.
METEOR: An Au-tomatic Metric for MT Evaluation with Improved Cor-relation with Human Judgments.
In Proceedings of the43th Annual Meeting of the Association of Computa-tional Linguistics (ACL-05), pages 65?72, 2005.Chris Callison-Burch, Miles Osborne, and PhilippKoehn.Re-evaluating the role of BLEU in Machine Transla-tion Research.
In Proceedings of the 13th Conferenceof the European Chapter of the Association for Compu-tational Linguistics (EACL-06), pages 249?256, 2006.Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,Christof Monz, and Josh Schroeder.
(Meta-) evalua-tion of Machine Translation.
In Proceedings of the 2ndWorkshop on Statistical Machine Translation, pages136?158, 2007.Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,Christof Monz, and Josh Schroeder.
Further Meta-evaluation of Machine Translation.
In Proceedings ofthe 3rd Workshop on Statistical Machine Translation,pages 70?106, 2008.Chris Callison-Burch, Philipp Koehn, Christof Monz,Kay Peterson, Mark Pryzbocki, and Omar Zaidan.228Findings of the 2010 Joint Workshop on Statistical Ma-chine Translation andMetrics for Machine Translation.In Proceedings of the Joint 5th Workshop on StatisticalMachine Translation and MetricsMATR, pages 17?53,Uppsala, Sweden, 15-16 July 2010.G.
Doddington.
Automatic Evaluation of Machine Trans-lation Quality using N-gram Co-occurrence Statistics.In Proceedings of the 2nd International Conferenceon Human Language Technology Research (HLT-02),pages 138?145, San Francisco, CA, USA, 2002.
Mor-gan Kaufmann Publishers Inc.Jesu?s Gime?nez and Llu?is Ma`rquez.
Linguistic Featuresfor Automatic Evaluation of Heterogenous MT Sys-tems.
In Proceedings of the 2nd Workshop on Sta-tistical Machine Translation, pages 256?264, Prague,Czech Republic, June 2007.
Association for Computa-tional Linguistics.Jesu?s Gime?nez and Llu?is Ma`rquez.
A Smorgasbord ofFeatures for Automatic MT Evaluation.
In Proceed-ings of the 3rd Workshop on Statistical Machine Trans-lation, pages 195?198, Columbus, OH, June 2008.
As-sociation for Computational Linguistics.Philipp Koehn and Christof Monz.
Manual and Auto-matic Evaluation of Machine Translation between Eu-ropean Languages.
In Proceedings of the Workshop onStatistical Machine Translation, pages 102?121, 2006.Gregor Leusch, Nicola Ueffing, andHermannNey.
CDer:Efficient MT Evaluation Using Block Movements.
InProceedings of the 13th Conference of the EuropeanChapter of the Association for Computational Linguis-tics (EACL-06), 2006.Ding Liu and Daniel Gildea.
Syntactic Features for Eval-uation of Machine Translation.
In Proceedings of theACL Workshop on Intrinsic and Extrinsic EvaluationMeasures for Machine Translation and/or Summariza-tion, page 25, 2005.Ding Liu and Daniel Gildea.
Source-Language Fea-tures and Maximum Correlation Training for MachineTranslation Evaluation.
In Proceedings of the 2007Conference of the North American Chapter of the As-sociation of Computational Linguistics (NAACL-07),2007.Chi-kiu Lo and Dekai Wu.
Evaluating machine transla-tion utility via semantic role labels.
In Seventh Interna-tional Conference on Language Resources and Eval-uation (LREC-2010), pages 2873?2877, Malta, May2010.Chi-kiu Lo and Dekai Wu.
Semantic vs. syntactic vs.n-gram structure for machine translation evaluation.In Dekai Wu, editor, Proceedings of SSST-4, FourthWorkshop on Syntax and Structure in Statistical Trans-lation (at COLING 2010), pages 52?60, Beijing, Aug2010.Chi-kiu Lo and Dekai Wu.
SMT vs. AI redux: Howsemantic frames evaluate MT more accurately.
In22nd International Joint Conference on Artificial In-telligence (IJCAI-11), Barcelona, Jul 2011.
To appear.Sonja Nie?en, Franz Josef Och, Gregor Leusch, and Her-mann Ney.
A Evaluation Tool forMachine Translation:Fast Evaluation forMTResearch.
InProceedings of the2nd International Conference on Language Resourcesand Evaluation (LREC-2000), 2000.Karolina Owczarzak, Josef van Genabith, and AndyWay.Evaluating machine translation with LFG dependen-cies.
Machine Translation, 21:95?119, 2008.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
BLEU: A Method for Automatic Evaluationof Machine Translation.
In Proceedings of the 40th An-nualMeeting of the Association for Computational Lin-guistics (ACL-02), pages 311?318, 2002.Sameer Pradhan, WayneWard, Kadri Hacioglu, James H.Martin, and Dan Jurafsky.
Shallow Semantic ParsingUsing Support Vector Machines.
In Proceedings ofthe 2004 Conference on Human Language Technologyand the North American Chapter of the Association forComputational Linguistics (HLT-NAACL-04), 2004.Mark Przybocki, Kay Peterson, Se?bastien Bronsart, andGregory Sanders.
The NIST 2008Metrics for MachineTranslation Challenge - Overview, Methodology, Met-rics, and Results.
Machine Tr, 23:71?103, 2010.Matthew Snover, Bonnie J. Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
A Study of Trans-lation Edit Rate with Targeted Human Annotation.
InProceedings of the 7th Conference of the Associationfor Machine Translation in the Americas (AMTA-06),pages 223?231, 2006.Christoph Tillmann, Stephan Vogel, Hermann Ney,Arkaitz Zubiaga, and Hassan Sawaf.
AcceleratedDP Based Search For Statistical Translation.
In Pro-ceedings of the 5th European Conference on SpeechCommunication and Technology (EUROSPEECH-97),1997.Clare R. Voss and Calandra R. Tate.
Task-based Evalua-tion of Machine Translation (MT) Engines: MeasuringHowWell People ExtractWho,When,Where-Type El-ements in MT Output.
In Proceedings of the 11th An-nual Conference of the European Association for Ma-chine Translation (EAMT-2006), pages 203?212, Oslo,Norway, June 2006.229
