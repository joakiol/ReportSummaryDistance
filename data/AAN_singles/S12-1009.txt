First Joint Conference on Lexical and Computational Semantics (*SEM), pages 59?64,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsStatistical Thesaurus Construction for a Morphologically Rich LanguageChaya Liebeskind, Ido Dagan and Jonathan SchlerComputer Science DepartmentBar-Ilan UniversityRamat-Gan, Israelliebchaya@gmail.com, dagan@cs.biu.ac.il, schler@gmail.comAbstractCorpus-based thesaurus construction for Mor-phologically Rich Languages (MRL) is a com-plex task, due to the morphological variabilityof MRL.
In this paper we explore alternativeterm representations, complemented by cluster-ing of morphological variants.
We introduce ageneric algorithmic scheme for thesaurus con-struction in MRL, and demonstrate the empiri-cal benefit of our methodology for a Hebrewthesaurus.1 IntroductionCorpus-based thesaurus construction has been anactive research area (Grefenstette, 1994; Curranand Moens, 2002; Kilgarriff, 2003; Rychly andKilgarriff, 2007).
Typically, two statistical ap-proaches for identifying semantic relationshipsbetween words were investigated: first-order, co-occurrence-based methods which assume thatwords that occur frequently together are topicallyrelated (Schutze and Pederson, 1997) and second-order, distributional similarity methods (Hindle,1990; Lin, 1998; Gasperin et al 2001; Weeds andWeir, 2003; Kotlerman et al, 2010), which suggestthat words occurring within similar contexts aresemantically similar (Harris, 1968).While most prior work focused on English, weare interested in applying these methods to MRL.Such languages, Hebrew in our case, are character-ized by highly productive morphology which mayproduce as many as thousands of word forms for agiven root form.Thesauri usually provide related terms for eachentry term (denoted target term).
Since both targetand related terms correspond to word lemmas, sta-tistics collection from the corpus would be mostdirectly applied at the lemma level as well, using amorphological analyzer and tagger (Linden andPiitulainen, 2004; Peirsman et al, 2008; Rapp,2009).
However, due to the rich and challengingmorphology of MRL, such tools often have limitedperformance.
In our research, the accuracy of astate-of-the-art modern Hebrew tagger on a crossgenre corpus was only about 60%.Considering such limited performance of mor-phological processing, we propose a schematicmethodology for generating a co-occurrence basedthesaurus in MRL.
In particular, we propose andinvestigate three options for term representation,namely surface form, lemma and multiple lemmas,supplemented with clustering of term variants.While the default lemma representation is depend-ent on tagger performance, the two other represen-tations avoid choosing the right lemma for eachword occurrence.
Instead, the multiple-lemma rep-resentation assumes that the right analysis will ac-cumulate enough statistical prominence throughoutthe corpus, while the surface representation solvesmorphological disambiguation "in retrospect", byclustering term variants at the end of the extractionprocess.
As the methodology provides a genericscheme for exploring the alternative representationlevels, each corpus and language-specific tool setmight yield a different optimal configuration.2 MethodologyThesauri usually contain thousands of entries,termed here target terms.
Each entry holds a list ofrelated terms, covering various semantic relations.In this paper we assume that the list of target terms59is given as input, and focus on the process of ex-tracting a ranked list of candidate related terms(termed candidate terms) for each target term.
Thetop ranked candidates may be further examined(manually) by a lexicographer, who will select theeventual related terms for the thesaurus entry.Our methodology was applied for statisticalmeasures of first order similarity (word co-occurrence).
These statistics consider the numberof times each candidate term co-occurs with thetarget term in the same document, relative to theirtotal frequencies in the corpus.
Common co-occurrence metrics are Dice coefficient (Smadja etal, 1996), Pointwise Mutual Information (PMI)(Church and Hanks, 1990) and log-likelihood test(Dunning, 1993).2.1 Term RepresentationStatistical extraction is affected by termrepresentation in the corpus.
Usually, related termsin a thesaurus are lemmas, which can be identifiedby morphological disambiguation tools.
However,we present two other approaches for termrepresentation (either a target term or a candidaterelated term), which are less dependent onmorphological processing.Typically, a morphological analyzer producesall possible analyses for a given token in the cor-pus.
Then, a Part Of Speech (POS) tagger selectsthe most probable analysis and solves morphologydisambiguation.
However, considering the poorperformance of the POS tagger on our corpus, wedistinguish between these two analysis levels.Consequently, we examined three levels of termrepresentation: (i) Surface form (surface) (ii) Bestlemma, as indentified by a POS tagger (best), and(iii) All possible lemmas, produced by a morpho-logical analyzer (all).2.2 Algorithmic SchemeWe used the following algorithmic scheme for the-saurus construction.
Our input is a target term inone of the possible term representations (surface,best or all).
For each target term we retrieve all thedocuments in the corpus where the target term ap-pears.
Then, we define a set of candidate terms thatconsists of all the terms that appear in all thesedocuments (this again for each of the three possibleterm representations).
Next, a co-occurrence scorebetween the target term and each of the candidatesis calculated.
Then, candidates are sorted, and thehighest rated candidate terms are clustered intolemma-oriented clusters.
Finally, we rank the clus-ters according to their members' co-occurrencescores and the highest rated clusters become relat-ed terms in the thesaurus.Figure 1 presents the algorithm?s pseudo code.The notion rep(term) is used to describe the possi-ble term representations and may be either surface,best or all.
In our experiments, whenrep(target_term)=best, the correct lemma wasmanually assigned (assuming a lexicographer in-volvement with each thesaurus entry in our set-ting).
While, when rep(word)=best, the most prob-able lemma is assigned by the tagger (since thereare numerous candidates for each target term wecannot resort the manual involvement for each ofthem).
The two choices for rep(term) are inde-pendent, resulting in nine possible configurationsof the algorithm for representing both the targetterm and the candidate terms.
Thus, these 9 con-figurations cover the space of possibilities for termrepresentation.
Exploring all of them in a systemat-ic manner would reveal the best configuration in aparticular setting.Figure 1: Methodology implementation algorithm2.3 ClusteringThe algorithm of Figure 1 suggests clustering theextracted candidates before considering them forthe thesaurus.
Clustering aims at grouping togetherrelated terms with the same lemma into clusters,using some measure of morphological equivalence.Accordingly, an equivalence measure between re-lated terms needs to be defined, and a clusteringInput: target term, corpus, a pair of values forrep(target_term) and rep(word)Output: clusters of related termstarget_term   rep(target_term)docs_list   search(target_term)FOR doc IN docs_listFOR word IN docadd rep(word) to candidatesENDFORENDFORcompute co-occurrence scores for all candidatessort(candidates) by scoreclusters  cluster(top(candidates))rank(clusters)related terms  top(clusters)60algorithm needs to be selected.
Each obtained clus-ter is intended to correspond to the lemma of a sin-gle candidate term.
Obviously, clustering is mostlyneeded for surface-level representation, in order togroup all different inflections of the same lemma.Yet, we note that it was also found necessary forthe lemma-level representations, because the tag-ger often identifies slightly different lemmas forthe same term.The equivalence measure is used for building agraph representation of the related terms.
We rep-resented each term by a vertex and added an edgebetween each pair of terms that were deemedequivalent.
We investigated alternative equiva-lence measures for measuring the morphologicaldistance between two vertices in our graph.
Weconsidered the string edit distance measure andsuggested two morphological-based equivalencemeasures.
The first measure, given two vertices'terms, extracts all possible lemmas for each termand searches for an overlap of at least one lemma.The second measure considers the most probablelemma of the vertices' terms and checks whetherthese lemmas are equal.
The probability of a lem-ma was defined as the sum of probabilities for allmorphological analyses containing the lemma, us-ing a morpho-lexical context-independent proba-bilities approximation (Goldberg et al, 2008).
Theclustering was done by finding the connected com-ponents in our graph of terms using the JUNG1implementation (WeakComponentVertexClustereralgorithm with default parameters).
The connectedcomponents are expected to correspond to differentlemmas of terms.
Hierarchical clustering methods(Jain et al, 1999) were examined as well (Single-link and Complete-link clustering), but they wereinferior.After applying the clustering algorithm, we re-ranked the clusters aiming to get the best clustersat the top of clusters list.
We investigated two scor-ing approaches for cluster ranking; maximizationand averaging.
The maximization approach assignsthe maximal score of the cluster members as thecluster score.
While the averaging approach as-signs the average of the cluster members' scores asthe cluster score.
The score obtained by either ofthe approaches may be scaled by the cluster length,to account for the accumulative impact of all class1http://jung.sourceforge.net/members (corresponding to morphological variantsof the candidate term).3 Case Study: Cross-genre HebrewThesaurusOur research targets the construction of a crossgenre thesaurus for the Responsa project 2 .
Thecorpus includes questions posed to rabbis alongwith their detailed rabbinic answers, consisting ofvarious genres and styles.
It contains 76,710 arti-cles and about 100 million word tokens, and wasused for previous IR and NLP research (Choueka,1972; Fraenkel, 1976; Choueka et al, 1987; Kernelet al 2008).Unfortunately, due to the different genres in theResponsa corpus, available tools for Hebrew pro-cessing perform poorly on this corpus.
In a prelim-inary experiment, the POS tagger (Adler andElhadad, 2006) accuracy on the Responsa Corpuswas less than 60%, while the accuracy of the sametagger on modern Hebrew corpora is ~90% (Bar-Haim et al, 2007).For this project, we utilized the MILA HebrewMorphological Analyzer3 (Itai and Wintner, 2008;Yona and Wintner, 2008) and the (Adler andElhadad 2006) POS tagger for lemma representa-tion.
The latter had two important characteristics:The first is flexibility- This tagger allows adaptingthe estimates of the prior (context-independent)probability of each morphological analysis in anunsupervised manner, from an unlabeled corpus ofthe target domain (Goldberg et al, 2008).
The se-cond advantage is its mechanism for analyzing un-known tokens (Adler et al, 2008).
Since about50% of the words in our corpora are unknown(with respect to MILA's lexicon), such mechanismis essential.For statistics extraction, we used Lucene4.
Wetook the top 1000 documents retrieved for the tar-get term and extracted candidate terms from them.Dice coefficient was used as our co-occurrencemeasure, most probable lemma was considered forclustering equivalence, and clusters were rankedbased on maximization, where the maximal scorewas multiplied by cluster size.2Corpus kindly provided - http://www.biu.ac.il/jh/Responsa/3http://mila.cs.technion.ac.il/mila/eng/tools_analysis.html3http://mila.cs.technion.ac.il/mila/eng/tools_analysis.html4http://lucene.apache.org/i .il4 lucene.apache.org/614 Evaluation4.1 Dataset and Evaluation MeasuresThe results reported in this paper were obtainedfrom a sample of 108 randomly selected termsfrom a list of 5000 terms, extracted from two pub-licly available term lists: the University of Haifa?sentry list5 and Hebrew Wikipedia entries6.In our experiments, we compared the perfor-mance of the alternative 9 configurations by fourcommonly used IR measures: precision (P), rela-tive recall (R), F1, and Average Precision (AP).The scores were macro-averaged.
We assumed thatour automatically-generated candidate terms willbe manually filtered, thus, recall becomes moreimportant than precision.
Since we do not have anypre-defined thesaurus, we evaluated the relative-recall.
Our relative-recall considered the number ofsuitable related terms from the output of all meth-ods as the full set of related terms.
As our systemyielded a ranked sequence of related terms clusters,we also considered their ranking order.
Therefore,we adopted the recall-oriented AP for ranking(Voorhees and Harman, 1999).4.2  Annotation SchemeThe output of the statistical extraction is a rankedlist of clusters of candidate related terms.
Sincemanual annotation is expensive and time consum-ing, we annotated for the gold standard the top 15clusters constructed from the top 50 candidateterms, for each target term.
Then, an annotatorjudged each of the clusters' terms.
A cluster wasconsidered as relevant if at least one of its termswas judged relevant7.4.3 ResultsTable 1 compares the performance of all nine termrepresentation configurations.
Due to data sparse-ness, the lemma-based representations of the targetterm outperform its surface representation.
How-ever, the best results were obtained from candidaterepresentation at the surface level, which wascomplemented by grouping term variants to lem-mas in the clustering phase.5 http://lib.haifa.ac.il/systems/ihp.html6 http://he.wikipedia.org7This was justified by empirical results that found only a fewclusters with some terms judged positive and others negativeAll best surface CandidateTarget26.68 29.37 36.59 RSurface 18.71 21.09 24.29 P 21.99 24.55 29.20 F114.13 15.83 20.87 AP36.97 39.88 46.70 RBestlemma20.94 23.08 25.03 P26.74 29.24 32.59 F119.32 20.86 26.84 AP42.13 42.52 47.13 RAlllemmas21.23 22.47 23.72 P28.24 29.40 31.56 F121.14 22.99 27.86 APTable 1: Performances of the nine configuratrionsFurthermore, we note that the target representa-tion by all possible lemmas (all) yielded the best Rand AP scores, which we consider as most im-portant for the thesaurus construction setting.
Theimprovement over the common default best lemmarepresentation, for both target and candidate, isnotable (7 points) and is statistically significantaccording to the two-sided Wilcoxon signed-ranktest (Wilcoxon, 1945) at the 0.01 level for AP and0.05 for R.5 Conclusions and Future WorkWe presented a methodological scheme for ex-ploring alternative term representations in statisti-cal thesaurus construction for MRL, complementedby lemma-oriented clustering at the end of the pro-cess.
The scheme was investigated for a Hebrewcross-genre corpus, but can be generically appliedin other settings to find the optimal configurationin each case.We plan to adopt our methodology to secondorder distributional similarity methods as well.
Inthis case there is an additional dimension, namelyfeature representation, whose representation levelshould be explored as well.
In addition, we plan toextend our methods to deal with Multi Word Ex-pressions (MWE).AcknowledgmentsThis work was partially supported by thePASCAL-2 Network of Excellence of the Europe-an Community FP7-ICT-2007-1-216886.62ReferencesAdler Meni and Michael Elhadad.
2006.
An Unsuper-vised Morpheme-Based HMM for Hebrew Morpho-logical Disambiguation, in Proceedings of COLING-ACL, Sydney, Australia.Adler Meni, Yoav Goldberg, David Gabay and MichaelElhadad.
2008.
Unsupervised Lexicon-Based Resolu-tion of Unknown Words for Full MorphologicalAnalysis, in Proceedings of ACL.Bar-Haim Roy, Khalil Sima'an, and Yoad Winter.
2007.Part-of-speech tagging of Modern Hebrew text.
Nat-ural Language Engineering, 14(02):223.251.Choueka, Yaacov.
1972.
Fast searching and retrievaltechniques for large dictionaries and concordances.Hebrew Computational Linguistics, 6:12?32, July.Choueka, Y., A.S. Fraenkel, S.T.
Klein and E. Segal.1987.
Improved techniques for processing queries infull-text systems.
Proceedings of the 10th annual in-ternational ACM SIGIR conference on Research anddevelopment in information retrieval.Church, K. W., and Hanks, P. 1990.
Word associationnorms, mutual information and lexicography.
Com-putational Linguistics 16(1): 22?29.Curran, James R. and Marc Moens.
2002.
Improve-ments in automatic thesaurus extraction.
In Proceed-ings of the ACL-SIGLEX Workshop on UnsupervisedLexical Acquisition, pages 59?67, Philadelphia, PA.Dunning, T.E.
1993.
Accurate methods for the statisticsof surprise and coincidence.
Computational Linguis-tics 19, 61?74 (1993).Fraenkel, Aviezri S. 1976.
All about the Responsa re-trieval project ?
what you always wanted to know butwere afraid to ask.
Jurimetrics Journal, 16(3):149?156, Spring.Gasperin, C., Gamallo, P., Agustini, A., Lopes, G., andde Lima, V. 2001.
Using syntactic contexts for meas-uring word similarity.
In the Workshop on SemanticKnowledge Acquisition and Categorisation (ESSLI2001), Helsinki, Finland.Goldberg Yoav, Meni Adler and Michael Elhadad,2008.
EM Can Find Pretty Good HMM POS-Taggers(When Given a Good Start), in Proceedings of ACL.Grefenstette, G. 1994.
Explorations in Automatic The-saurus Construction.
Kluwer Academic Publishers,Boston, USAHarris, Zelig S. 1968.
Mathematical Structures of Lan-guage.
John Wiley, New York.Hindle, D. 1990.
Noun classification from predicateargument structures.
In Proceedings of ACL.Itai Alon and Shuly Wintner.
2008.
Language Re-sources for Hebrew.
Language Resources and Evalu-ation 42(1):75-98, March 2008.Jain, A. K., M. N. Murty, P. J. Flynn.
1999.
Data Clus-tering: A Review.
ACM Computing Surveys31(3):264-323.Kerner Yaakov HaCohen, Ariel Kass, Ariel Peretz.2008.
Combined One Sense Disambiguation of Ab-breviations.
In Proceedings of ACL (Short Papers),pp.
61-64.Kilgarriff, Adam.
2003.
Thesauruses for natural lan-guage processing.
In Proceedings of the Joint Con-ference on Natural Language Processing andKnowledge Engineering, pages 5?13, Beijing, China.Kotlerman Lili, Dagan Ido, Szpektor Idan, andZhitomirsky-Geffet Maayan.
2010.
Directional Dis-tributional Similarity for Lexical Inference.
NaturalLanguage Engineering, 16(4):359?389.Linden Krister, and Jussi Olavi Piitulainen.
2004.
Dis-covering Synonyms and Other Related Words.
InProceedings of COLING 2004 : CompuTerm 2004:3rd International Workshop on Computational Ter-minology, Pages 63-70, Geneva, SwitzerlandLin, D. 1998.
Automatic retrieval and clustering ofsimilar words.
In Proceedings of COLING-ACL.Peirsman Yves, Kris Heylen, and Dirk Speelman.
2008.Putting things in order.
first and second order con-texts models for the calculation of semantic similari-ty.
In Actes des 9i`emes Journ?ees internationalesd?Analyse statistique des Donn?ees Textuelles (JADT2008), pages 907?916.Rapp, R. 2009.
The Automatic Generation of Thesauriof Related Words for English, French, German, andRussian, International Journal of Speech Technolo-gy, 11, 3-4, 147-156.Rychly, P. and Kilgarriff, A.
2007.
An efficient algo-rithm for building a distributional thesaurus (and oth-er Sketch Engine developments).
In Proceedings ofACL-07, demo session.
Prague, Czech Republic.Schutze Hinrich and Jan 0.
Pederson.
1997.
Acooccurrence-based thesaurus and two applicationsto information retrieval.
Information Processingand Management, 33(3):307-318.Smadja, F., McKeown, K.R., Hatzivassiloglou, V. 1996.Translating collocations for bilingual lexicons: A sta-tistical approach.
Computational Linguistics 22, 1?3863Voorhees E.M. and D. Harman.
1999.
Overview of theseventh text retrieval conference .
In Proceedings ofthe Seventh Text Retrieval 73 Conference, 1999.NIST Special Publication.
58.Weeds, J., and Weir, D. 2003.
A general  framework fordistributional similarity.
In Proceedings of EMNLP,Sapporo, Japan.Wilcoxon F. 1945.
Individual comparisons by rankingmethods.
Biometrics Bulletin, 1:80?83.Yona Shlomo and Shuly Wintner.
2008.
A Finite-StateMorphological Grammar of Hebrew.
Natural Lan-guage Engineering 14(2):173-190, April 2008.
Lan-guage Resources and Evaluation 42(1):75-98, March2008.64
