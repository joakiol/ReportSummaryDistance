The Meeting Project at ICSINelson Morgan1;4 Don Baron1;4 Jane Edwards1;4 Dan Ellis1;2 David Gelbart1;4Adam Janin1;4 Thilo Pfau1 Elizabeth Shriberg1;3 Andreas Stolcke1;31International Computer Science Institute, Berkeley, CA2Columbia University, New York, NY3SRI International, Menlo Park, CA4University of California at Berkeley, Berkeley, CAfmorgan,dbaron,edwards,dpwe,gelbart,janin,tpfau,ees,stolckeg@icsi.berkeley.eduABSTRACTIn collaboration with colleagues at UW, OGI, IBM, and SRI, we aredeveloping technology to process spoken language from informalmeetings.
The work includes a substantial data collection and tran-scription effort, and has required a nontrivial degree of infrastruc-ture development.
We are undertaking this because the new taskarea provides a significant challenge to current HLT capabilities,while offering the promise of a wide range of potential applica-tions.
In this paper, we give our vision of the task, the challenges itrepresents, and the current state of our development, with particularattention to automatic transcription.1.
THE TASKWe are primarily interested in the processing (transcription,query, search, and structural representation) of audio recorded frominformal, natural, and even impromptu meetings.
By ?informal?
wemean conversations between friends and acquaintances that do nothave a strict protocol for the exchanges.
By ?natural?
we meanmeetings that would have taken place regardless of the recordingprocess, and in acoustic circumstances that are typical for suchmeetings.
By ?impromptu?
we mean that the conversation maytake place without any preparation, so that we cannot require spe-cial instrumentation to facilitate later speech processing (such asclose-talking or array microphones).
A plausible image for suchsituations is a handheld device (PDA, cell phone, digital recorder)that is used when conversational partners agree that their discussionshould be recorded for later reference.Given these interests, we have been recording and transcrib-ing a series of meetings at ICSI.
The recording room is one ofICSI?s standard meeting rooms, and is instrumented with bothclose-talking and distant microphones.
Close-mic?d recordingswill support research on acoustic modeling, language modeling,dialog modeling, etc., without having to immediately solve thedifficulties of far-field microphone speech recognition.
The dis-tant microphones are included to facilitate the study of these deepacoustic problems, and to provide a closer match to the operatingconditions ultimately envisaged.
These ambient signals are col-.lected by 4 omnidirectional PZM table-mount microphones, plusa ?dummy?
PDA that has two inexpensive microphone elements.In addition to these 6 distant microphones, the audio setup permitsa maximum of 9 close-talking microphones to be simultaneouslyrecorded.
A meeting recording infrastructure is also being put inplace at Columbia University, at SRI International, and by our col-leagues at the University of Washington.
Recordings from all siteswill be transcribed using standards evolved in discussions that alsoinvolved IBM (who also have committed to assist in the transcrip-tion task).
Colleagues at NIST have been in contact with us to fur-ther standardize these choices, since they intend to conduct relatedcollection efforts.A segment from a typical discussion recorded at ICSI is includedbelow in order to give the reader a more concrete sense of the task.Utterances on the same line separated by a slash indicate some de-gree of overlapped speech.A: Ok.
So that means that for each utterance, .. we?ll needthe time marks.E: Right.
/ A: the start and end of each utterance.
[a few turns omitted]E: So we - maybe we should look at the um .. the tools thatMississippi State has.D: Yeah.E: Because, I - I - I know that they published .. um .. annota-tion tools.A: Well, X-waves have some as well, .. but they?re prettylow level ..
They?re designed for uh - / D: phoneme / A: forphoneme-level / D: transcriptions.
Yeah.J: I should -A: Although, they also have a nice tool for - .. that could beused for speaker change marking.D: There?s a - there are - there?s a whole bunch of toolsJ: Yes.
/ D: web page, where they have a listing.
D: like 10of them or something.J: Are you speaking about Mississippi State per se?
orD: No no no, there?s some ..
I mean, there just - there are -there are a lot of / J: Yeah.J: Actually, I wanted to mention - / D: (??
)J: There are two projects, which are .. international .. hugeprojects focused on this kind of thing, actually .. one ofthem?s MATE, one of them?s EAGLES .. and um.D: Oh, EAGLES.D: (??)
/ J: And both of them haveJ: You know, I shou-, I know you know about the big book.E: Yeah.J: I think you got it as a prize or something.E: Yeah.
/ D: Mhm.J: Got a surprise.
flaughg fJ.
thought ?as a prize?
soundedlike ?surprise?gNote that interruptions are quite frequent; this is, in our expe-rience, quite common in informal meetings, as is acoustic overlapbetween speakers (see the section on error rates in overlap regions).2.
THE CHALLENGESWhile having a searchable, annotatable record of impromptumeetings would open a wide range of applications, there are sig-nificant technical challenges to be met; it would not be far from thetruth to say that the problem of generating a full representation of ameeting is ?AI complete?, as well as ?ASR complete?.
We believe,however, that our community can make useful progress on a rangeof associated problems, including: ASR for very informal conversational speech, including thecommon overlap problem. ASR from far-field microphones - handling the reverberationand background noise that typically bedevil distant mics, aswell as the acoustic overlap that is more of a problem formicrophones that pick up several speakers at approximatelythe same level. Segmentation and turn detection - recovering the differentspeakers and turns, which also is more difficult with overlapsand with distant microphones (although inter-microphonetiming cues can help here). Extracting nonlexical information such as speaker identifi-cation and characterization, voice quality variation, prosody,laughter, etc. Dialog abstraction - making high-level models of meet-ing ?state?
; identifying roles among participants, classifyingmeeting types, etc.
[2]. Dialog analysis - identification and characterization of fine-scale linguistic and discourse phenomena [3][10]. Information retrieval from errorful meeting transcriptions -topic change detection, topic classification, and query match-ing. Summarization of meeting content [14] - representation ofthe meeting structure from various perspectives and at vari-ous scales, and issues of navigation in thes representations. Energy and memory resource limitation issues that arise inthe robust processing of speech using portable devices [7].Clearly we and others working in this area (e.g., [15]) are at anearly stage in this research.
However, the remainder of this pa-per will show that even a preliminary effort in recording, manuallytranscribing, and recognizing data from natural meetings has pro-vided some insight into at least a few of these problems.3.
DATA COLLECTION AND HUMANTRANSCRIPTIONUsing the data collection setup described previously, we havebeen recording technical meetings at ICSI.
As of this writing wehave recorded 38 meetings for a total of 39 hours.
Note that thereare separate microphones for each participant in addition to the 6far-field microphones, and there can be as many as 15 open chan-nels.
Consequently the sound files comprise hundreds of hours ofrecorded audio.
The total number of participants in all meetings is237, and there were 49 unique speakers.
The majority of the meet-ings recorded so far have either had a focus on ?Meeting Recorder?
(that is, meetings by the group working on this technology) or ?Ro-bustness?
(primarily concerned with ASR robustness to acousticeffects such as additive noise).
A smaller number of other meetingtypes at ICSI were also included.In addition to the spontaneous recordings, we asked meeting par-ticipants to read digit strings taken from a TI digits test set.
Thiswas done to facilitate research in far-field microphone ASR, sincewe expect this to be quite challenging for the more unconstrainedcase.
At the start or end of each meeting, each participant read 20digit strings.Once the data collection was in progress, we developed a set ofprocedures for our initial transcription.
The transcripts are word-level transcripts, with speaker identifier, and some additional in-formation: overlaps, interrupted words, restarts, vocalized pauses,backchannels, and contextual comments, and nonverbal events(which are further subdivided into vocal types such as cough andlaugh, and nonvocal types such as door slams and clicks).
Eachevent is tied to the time line through use of a modified version of the?Transcriber?
interface (described below).
This Transcriber win-dow provides an editing space at the top of the screen (for addingutterances, etc), and the wave form at the bottom, with mechanismsfor flexibly navigating through the audio recording, and listeningand re-listening to chunks of virtually any size the user wishes.The typical process involves listening to a stretch of speech untila natural break is found (e.g., a long pause when no one is speak-ing).
The transcriber separates that chunk from what precedes andfollows it by pressing the Return key.
Then he or she enters thespeaker identifier and utterance in the top section of the screen.The interface is efficient and easy to use, and results in an XMLrepresentation of utterances (and other events) tied to time tags forfurther processing.The ?Transcriber?
interface [13] is a well-known tool for tran-scription, which enables the user to link acoustic events to the waveform.
However, the official version is designed only for single-channel audio.
As noted previously, our application records up to15 parallel sound tracks generated by as many as 9 speakers, and wewanted to capture the start and end times of events on each channelas precisely as possible and independently of one another acrosschannels.
The need to switch between multiple audio channels toclarify overlaps, and the need to display the time course of eventson independent channels required extending the ?Transcriber?
in-terface in two ways.
First, we added a menu that allows the user toswitch the playback between a number of audio files (which are allassumed to be time synchronized).
Secondly, we split the time-linked display band into as many independent display bands asthere are channels (and/or independent layers of time-synchronizedannotation).
Speech and other events on each of the bands can nowbe time-linked to the wave form with complete freedom and totallyindependently of the other bands.
This enables much more precisestart and end times for acoustic events.See [8] for links to screenshots of these extensions to Transcriber(as well as to other updates about our project).In the interests of maximal speed, accuracy and consistency, thetranscription conventions were chosen so as to be: quick to type,related to standard literary conventions where possible (e.g., - forinterrupted word or thought, .. for pause, using standard orthogra-phy rather than IPA), and minimalist (requiring no more decisionsby transcribers than absolutely necessary).After practice with the conventions and the interface, transcribersachieved a 12:1 ratio of transcription time to speech time.
Theamount of time required for transcription of spoken language isknown to vary widely as a function of properties of the discourse(amount of overlap, etc.
), and amount of detailed encoding (prosod-ics, etc.
), with estimates ranging from 10:1 for word-level withminimal added information to 20:1, for highly detailed discoursetranscriptions (see [4] for details).In our case, transcribers encoded minimal added detail, but hadtwo additional demands: marking boundaries of time bins, andswitching between audio channels to clarify the many instances ofoverlapping speech in our data.
We speeded the marking of timebins by providing them with an automatically segmented version(described below) in which the segmenter provided a preliminaryset of speech/nonspeech labels.
Transcribers indicated that the pre-segmentation was correct sufficiently often that it saved them time.After the transcribers finished, their work was edited for consis-tency and completeness by a senior researcher.
Editing involvedchecking exhaustive listings of forms in the data, spell check-ing, and use of scripts to identify and automatically encode cer-tain distinctions (e.g., the distinction between vocalized nonverbalevents, such as cough, and nonvocalized nonverbal events, like doorslams).
This step requires on average about 1:1 - one minute ofediting for each minute of speech.Using these methods and tools, we have currently transcribedabout 12 hours out of our 39 hours of data.
Other data havebeen sent to IBM for a rough transcription using commercial tran-scribers, to be followed by a more detailed process at ICSI.
Oncethis becomes a routine component of our process, we expect it tosignificantly reduce the time requirements for transcription at ICSI.4.
AUTOMATIC TRANSCRIPTIONAs a preliminary report on automatic word transcription, wepresent results for six example meetings, totalling nearly 7 hoursof speech, 36 total speakers, and 15 unique speakers (since manyspeakers participated in multiple meetings).
Note that these re-sults are preliminary only; we have not yet had a chance to addressthe many obvious approaches that could improve performance.
Inparticular, in order to facilitate efforts in alignment, pronuncia-tion modeling, language modeling, etc., we worked only with theclose-mic?d data.
In most common applications of meeting tran-scription (including those that are our chief targets in this research)such a microphone arrangement may not be practical.
Neverthelesswe hope the results using the close microphone data will illustratesome basic observations we have made about meeting data and itsautomatic transcription.4.1 Recognition systemThe recognizer was a stripped-down version of the large-vocabulary conversational speech recognition system fielded bySRI in the March 2000 Hub-5 evaluation [11].
The system per-forms vocal-tract length normalization, feature normalization, andspeaker adaptation using all the speech collected on each chan-nel (i.e., from one speaker, modulo cross-talk).
The acous-tic model consisted of gender-dependent, bottom-up clustered(genonic) Gaussian mixtures.
The Gaussian means are adapted bya linear transform so as to maximize the likelihood of a phone-loopmodel, an approach that is fast and does not require recognitionprior to adaptation.
The adapted models are combined with a bi-gram language model for decoding.
We omitted more elaborateadaptation, cross-word triphone modeling, and higher-order lan-guage and duration models from the full SRI recognition systemas an expedient in our initial recognition experiments (the omittedsteps yield about a 20% relative error rate reduction on Hub-5 data).It should be noted that both the acoustic models and the lan-guage model of the recognizer were identical to those used in theHub-5 domain.
In particular, the acoustic front-end assumes a tele-phone channel, requiring us to downsample the wide-band signalsof the meeting recordings.
The language model contained about30,000 words and was trained on a combination of Switchboard,CallHome English and Broadcast News data, but was not tuned foror augmented by meeting data.4.2 Speech segmentationAs noted above, we are initially focusing on recognition of theindividual channel data.
Such data provide an upper bound onrecognition accuracy if speaker segmentation were perfect, andconstitute a logical first step for obtaining high quality forced align-ments against which to evaluate performance for both near- and far-field microphones.
Individual channel recordings were partitionedinto ?segments?
of speech, based on a ?mixed?
signal (additionof the individual channel data, after an overall energy equalizationfactor per channel).
Segment boundary times were determined ei-ther by an automatic segmentation of the mixed signal followed byhand-correction, or by hand-correction alone.
For the automaticcase, the data was segmented with a speech/nonspeech detectorconsisting of an extension of an approach using an ergodic hiddenMarkov model (HMM) [1].
In this approach, the HMM consistsof two main states, one representing ?speech?
and one represent-ing ?nonspeech?
and a number of intermediate states that are usedto model the time constraints of the transitions between the twomain states.
In our extension, we are incorporating mixture den-sities rather than single Gaussians.
This appears to be useful forthe separation of foreground from background speech, which is aserious problem in these data.The algorithm described above was trained on thespeech/nonspeech segmentation provided manually for thefirst meeting that was transcribed.
It was used to provide segmentsof speech for the manual transcribers, and later for the recognitionexperiments.
Currently, for simplicity and to debug the variousprocessing steps, these segments are synchronous across chan-nels.
However, we plan to move to segments based on separatespeech/nonspeech detection in each individual channel.
The latterapproach should provide better recognition performance, since itwill eliminate cross-talk in segments in which one speaker maysay only a backchannel (e.g.
?uhhuh?)
while another speaker istalking continuously.Performance was scored for the spontaneous conversational por-tions of the meetings only (i.e., the read digit strings referred toearlier were excluded).
Also, for this study we ran recognition onlyon those segments during which a transcription was produced forthe particular speaker.
This overestimates the accuracy of wordrecognition, since any speech recognized in the ?empty?
segmentswould constitute an error not counted here.
However, adding theempty regions would increase data load by a factor of about ten?which was impractical for us at this stage.
Note that the currentNIST Hub-5 (Switchboard) task is similar in this respect: data arerecorded on separated channels and only the speech regions of aspeaker are run, not the regions in which they are essentially silent.We plan to run all speech (including these ?empty?
segments) infuture experiments, to better assess actual performance in a realmeeting task.4.3 Recognition results and discussionOverall error rates.
Table 1 lists word error rates for the sixmeetings, by speaker.
The data are organized into two groups: na-tive speakers and nonnative speakers.
Since our recognition systemis not trained on nonnative speakers, we provide results only for thenative speakers; however the word counts are listed for all partici-Table 1: Recognition performance by speaker and meeting (MRM = ?Meeting Recorder meeting?
; ROB = ?Robustness meeting?
).Speaker gender is indicated by ?M?
or ?F?
in the speaker labels.
?
* : : : *?
marks speakers using a lapel microphone; all other casesused close-talking head-mounted microphones.
???
indicates speakers with severely degraded or missing signals due to incorrectmicrophone usage.
Word error rates are in boldface, total number of words in Roman, and out-of-vocabulary (OOV) rates in italics.OOV rate is by token, relative to a Hub-5 language model.
WER is for conversational speech sections of meetings only, and are notreported for nonnative speakers.Meeting MRM002 MRM003 MRM004 MRM005 ROB005 ROB004Duration (minutes) 45 78 60 68 81 70Native speakersM 004 42.4 48.1 44.3 48.4 45.14550 3087 3432 4912 55122.07 2.75 1.60 2.12 1.61M 001 42.4 50.6 37.6 38.62311 2488 1904 34001.82 2.09 2.78 1.56F 001 45.2 43.2 42.9 41.93008 3360 2714 27052.59 3.18 4.05 2.14M 009 *100.1* *115.8* 38.2 *68.7*1122 367 1066 6961.59 2.45 1.88 2.01F 002 45.2 43.7 *46.0*1549 1481 24802.26 2.64 1.63M 002 *55.6*9902.12Speakers with low word countsM 007 55.6 ?198 692.97 2.90M 008 72.7 59.555 1215.45 5.79M 015 ?596.56Non-native speakers (total words only)M 003 (British) 2189M 011 (Spanish) 2653 1239 663F 003 (Spanish) 620 220M 010 (German) 28M 012 (German) 639M 006 (French) 3524 2648pants for completeness.1The main result to note from Table 1 is that overall word errorrates are not dramatically worse than for Switchboard-style data.This is particularly impressive since, as described earlier, no meet-ing data were used in training, and no modifications of the acousticor language models were made.
The overall WER for native speak-ers was 46.5%, or only about a 7% relative increase over a compa-rable recognition system on Hub-5 telephone conversations.
Thissuggests that from the point of view of pronunciation and language(as opposed to acoustic robustness, e.g., for distant microphones),Switchboard may also be ?ASR-complete?.
That is, talkers may notreally speak in a more ?sloppy?
manner in meetings than they do incasual phone conversation.
We further investigate this claim in thenext section, by breaking down results by overlap versus nonover-lap regions, by microphone type and by speaker.Note that in some cases there were very few contributions froma speaker (e.g., speakers M 007, M 008, and M 015), and suchspeakers also tended to have higher word error rates.
We initiallysuspected the problem was a lack of sufficient data for speakeradaptation; indeed the improvement from adaptation was less thanfor other speakers.
Thus for such speakers it would make sense topool data across meetings for repeat participants.
However, in look-ing at their word transcripts we noted that their utterances, whilefew, tended to be dense with information content.
That is, thesewere not the speakers uttering ?uhhuh?
or short common phrases(which are generally well modeled in the Switchboard recognizer)but rather high-perplexity utterances that are generally harder torecognize.
Such speakers also tend to have a generally higher over-all OOV rate than other speakers.Error rates in overlap versus nonoverlap regions.
As notedin the previous section, the overall word error rate in our sam-ple meetings was slightly higher than in Switchboard.
An obviousquestion to ask here is: what is the effect on recognition of over-lapping speech?
To address this question, we defined a crude mea-sure of overlap.
Since segments were channel-synchronous in thesemeetings, a segment was either non-overlapping (only one speakerwas talking during that time segment), or overlapping (two or morespeakers were talking during the segment).
Note that this does notmeasure amount of overlap or number of overlapping speakers;more sophisticated measures based on the phone backtrace fromforced alignment would provide a better measure for more detailedanalyses.
Nevertheless, the crude measure provides a clear firstanswer to our question.
Since we were also interested in the inter-action if any between overlap and microphone type, we computedresults separately for the head-mounted and lapel microphones.
Re-sults were also computed by speaker, since as shown earlier in Ta-ble 1, speakers varied in word error rates, total words, and words bymicrophone type.
Note that speakers M 009 and F 002 have datafrom both conditions.As shown, our measure of overlap (albeit crude), clearly showsthat overlapping speech is a major problem for the recognition ofspeech from meetings.
If overlap regions are removed, the recog-nition accuracy overall is actually better than that for Switchboard.It is premature to make absolute comparisons here, but the fact thatthe same pattern is observed for all speakers and across microphone1Given the limitations of these pilot experiments (e.g., no on-tasktraining material and general pronunciation models), recognitionon nonnative speakers is essentially not working at present.
In thecase of one nonnative speaker, we achieved a 200% word error rate,surpassing a previous ICSI record.
Word error results presentedhere are based on meeting transcripts as of March 7, 2000, and aresubject to small changes as a result of ongoing transcription errorchecking.Table 2: Word error rates broken down by whether or not seg-ment is in a region of overlapping speech.Speaker No overlap With overlapHeadset Lapel Headset LapelM 004 41.0 - 50.3 -M 001 34.2 - 47.6 -F 001 40.5 - 45.8 -M 009 30.7 41.0 40.7 117.8F 002 37.7 29.8 50.5 56.3M 002 - 48.6 - 71.3M 007 52.2 - 81.3 -M 008 50.9 - 69.9Overall 39.9 38.5 48.7 85.2conditions suggests that it is not the inherent speech properties ofparticipants that makes meetings difficult to recognize, but ratherthe presence of overlapping speech.Furthermore, one can note from Table 2 that there is a large inter-action between microphone type and the effect of overlap.
Overlapis certainly a problem even for the close-talking head-mounted mi-crophones.
However, the degradation due to overlap is far greaterfor the lapel microphone, which picks up a greater degree of back-ground speech.
As demonstrated by speaker F 002, it is possibleto have a comparatively good word error rate (29.8%) on the lapelmicrophone in regions of no overlap (in this case 964/2480 wordswere in nonoverlapping segments).
Nevertheless, since the rate ofoverlaps is so high in the data overall, we are avoiding the useof the lapel microphone where possible in the future, preferringhead-mounted microphones for obtaining ground truth for researchpurposes.
We further note that for tests of acoustic robustness fordistant microphones, we tend to prefer microphones mounted onthe meeting table (or on a mock PDA frame), since they provide amore realistic representation of the ultimate target application thatis a central interest to us - recognition via portable devices.
In otherwords, we are finding lapel mics to be too ?bad?
for near-field mi-crophone tests, and too ?good?
for far-field tests.Error rates by error type.
The effect of overlapping speech onerror rates is due almost entirely to insertion errors, as shown inFigure 1.
Rates of other error types are nearly identical to those ob-served for Switchboard (modulo a a slight increase in substitutionsassociated with the lapel condition).
This result is not surprising,since background speech obviously adds false words in the hypoth-esis.
However, it is interesting that there is little increase in theother error types, suggesting that a closer segmentation based onindividual channel data (as noted earlier) could greatly improverecognition accuracy (by removing the surrounding backgroundspeech).Error rates by meeting type.
Different types of meetingsshould give rise to differences in speaking style and social interac-tion, and we may be interested in whether such effects are realizedas differences in word error rates.
The best way to measure sucheffects is within speaker.
The collection of regular, ongoing meet-ings at ICSI offers the possibility of such within-speaker compar-isons, since multiple speakers participate in more than one type ofregular meeting.
Of the speakers shown in the data set used for thisstudy, speaker M 004 is a good case in point, since he has data fromthree ?Meeting Recorder?
meetings and two ?Robustness?
meet-ings.
These two meeting types differ in social interaction; in thefirst, there is a fairly open exchange between many of the partici-Substitutions Deletions InsertionsError Type01020304050Rate(%)SwitchboardHead?Mic, OverlapHead?Mic, NonoverlapLapel?Mic, OverlapLapel?Mic, NonoverlapFigure 1: Word error rates by error type and micro-phone/overlap condition.
Switchboard scores refer to an in-ternal SRI development testset that is a representative subsetof the development data for the 2001 hub-5 evals.
It contains41 speakers (5-minute conversation sides), from Switchboard-1, Switchboard-2 and Cellular Switchboard in roughly equalproportions, and is also balanced for gender and ASR diffi-culty.
The other scores are evaluated for the data describedin the text.pants, while in the second, speaker M 004 directs the flow of themeeting.
It can also be seen from the table that speaker M 004 con-tributes a much higher rate of words relative to overall words in thelatter meeting type.
Interestingly however, his recognition rate andOOV rates are quite similar across the meeting types.
Study of ad-ditional speakers across meetings will allow us to further examinethis issue.5.
FUTURE WORKThe areas mentioned in the earlier section on ?Challenges?
willrequire much more work in the future.
We and our colleagues atcollaborating institutions will be working in all of these.
Here, webriefly mention some of the work in our current plans for the studyof speech from meetings.Far-field microphone ASR.
Starting with the read digits andproceeding to spontaneous speech, we will have a major focus onimproving recognition on the far-field channels.
In earlier work wehave had some success in recognizing artificially degraded speech[6][5], and will be adapting and more fully developing these ap-proaches for the new data and task.
Our current focus in thesemethods is on the designing of multiple acoustic representationsand the combination of the resulting probability streams, but wewill also compare these to methods that are more standard (but im-practical for the general case) such as echo cancellation using boththe close and distant microphones.Overlap type modeling.
One of the distinctive characteristicsof naturalistic conversation (in contrast to monolog situations) isthe presence of overlapping speech.
Overlapping speech may beof several types, and affects the flow of discourse in various ways.An overlap may help to usurp the floor from another speaker (e.g.,interruptions), or to encourage a speaker to continue (e.g., backchannels).
Also, some overlaps may be accidental, or a part of jointaction (as when a group tries to help a speaker to recall a person?sname when he is in mid-sentence).
In addition, different speakersmay differ in the amount and kinds of overlap in which they engage(speaker style).
In future work we will explore types of overlapsand their physical parameters, including prosodic aspects.Language modeling.
Meetings are also especially challengingfor the language model, since they tend to comprise a diverse rangeof topics and styles, and matched training data is hard to comeby (at least in this initial phase of the project).
Therefore, we ex-pect meeting recognition to necessitate investigation into novel lan-guage model adaptation and robustness techniques.Prosodic modeling.
Finally, we plan to study the potential con-tribution of prosodic (temporal and intonational) features to auto-matic processing of meeting data.
A project just underway is con-structing a database of prosodic features for meeting data, extend-ing earlier work [10, 9].
Goals include using prosody combinedwith language model information to help segment speech into co-herent semantic units, to classify dialog acts [12], and to aid speakersegmentation.6.
ACKNOWLEDGMENTSThe current work has been funded under the DARPA Communi-cator project (in a subcontract from the University of Washington),supplemented by an award from IBM.
In addition to the authors ofthis abstract, the project involves colleagues at a number of otherinstitutions, most notably: Mari Ostendorf, Jeff Bilmes, and KatrinKirchhoff from the University of Washington; and Hynek Herman-sky from the Oregon Graduate Institute.7.
REFERENCES[1] M. Beham and G. Ruske, Adaptiver stochastischerSprache/Pause-Detektor.
Proc.
DAGM SymposiumMustererkennung, pp.
60?67, Bielefeld, May 1995, Springer.
[2] D. Biber, Variation across speech and writing.
1st pbk.
ed.Cambridge [England]; New York: Cambridge UniversityPress, 1991.
[3] W. Chafe, Cognitive constraints on information flow.
In R. S.Tomlin (ed.)
Coherence and grounding in discourse.Philadelphia: John Benjamins, pp.
21?51, 1987.
[4] J. Edwards, The transcription of Discourse.
In D. Tannen, D.Schiffrin, and H. Hamilton (eds).
The Handbook ofDiscourse Analysis.
NY: Blackwell (in press).
[5] H. Hermansky, D. Ellis, and S. Sharma, Tandemconnectionist feature stream extraction for conventionalHMM systems, Proc.
ICASSP, pp.
III-1635?1638, Istanbul,2000.
[6] H. Hermansky and N. Morgan, RASTA Processing ofSpeech, IEEE Trans.
Speech and Audio Processing 2(4),578?589, 1994.
[7] A. Janin and N. Morgan, SpeechCorder, the PortableMeeting Recorder, Workshop on hands-free speechcommunication, Kyoto, April 9-11, 2001.
[8] http://www.icsi.berkeley.edu/speech/mtgrcdr.html[9] E. Shriberg, R. Bates, A. Stolcke, P. Taylor, D. Jurafsky, K.Ries, N. Coccaro, R. Martin, M. Meteer, and C. VanEss-Dykema.
Can prosody aid the automatic classification ofdialog acts in conversational speech?
Language and Speech,41(3-4):439?487, 1998.
[10] E. Shriberg, A. Stolcke, D. Hakkani-Tu?r, and G. Tu?r.Prosody-based automatic segmentation of speech intosentences and topics.
Speech Communication,32(1-2):127?154, 2000.
[11] A. Stolcke, H. Bratt, J. Butzberger, H. Franco, V. R. RaoGadde, M.
Plauche?, C. Richey, E. Shriberg, K. So?nmez, F.Weng, and J. Zheng.
The SRI March 2000 Hub-5conversational speech transcription system.
Proc.
NISTSpeech Transcription Workshop, College Park, MD, May2000.
[12] A. Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates, D.Jurafsky, P. Taylor, R. Martin, C. Van Ess-Dykema, and M.Meteer, Dialogue Act Modeling for Automatic Tagging andRecognition of Conversational Speech, ComputationalLinguistics 26(3), 339?373, 2000.
[13] http://www.etca.fr/CTA/gip/Projets/Transcriber/[14] A. Waibel, M. Bett, M. Finke, and R. Stiefelhagen, MeetingBrowser: Tracking and Summarizing Meetings, Proc.DARPA Broadcast News Transcription and UnderstandingWorkshop, Lansdowne, VA, 1998.
[15] H. Yu, C. Clark, R. Malkin, and A. Waibel, Experiments inAutomatic Meeting Transcription Using JRTK, Proc.ICASSP, pp.
921?924, Seattle, 1998.
