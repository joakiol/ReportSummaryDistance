A SPEECH TO SPEECH TRANSLAT ION SYSTEMBUILT  FROM STANDARD COMPONENTSManny Rayner 1, Hiyan Alshawi 1, Ivan Bretan 3, David Carter a,Vassilios Digalakis 2, BjSrn Gambiick 3, Jaan Kaja 4, Jussi Karlgren z,Bertil Lyber9 4, Steve Pulman 1, Patti Price 2 and G-'hrister Samuelsson 3(1) SRI International, Cambridge, UK (2) SRI International, Menlo Park, CA(3) SICS, Stockholm, Sweden (4) Telia Research AB, Haninge, SwedenABSTRACTThis paper I describes a speech to speech translation systemusing standard components and a suite of generalizable cus-tomization techniques.
The system currently translates airtravel planning queries from English to Swedish.
The modu-lax architecture is designed to be easy to port to new domainsand languages, and consists of a pipelined series of process-ing phases.
The output of each phase consists of multiplehypotheses; tatistical preference mechanisms, the data forwhich is derived from automatic processing of domain cor-pora, are used between each pair of phases to filter hypothe-ses.
Linguistic knowledge is represented throughout he sys-tem in declarative form.
We summarize the architectures ofthe component systems and the interfaces between them, andpresent initial performance r sults.1.
INTRODUCTIONFrom standard components and a suite of generalizablecustomization techniques, we have developed an Englishto Swedish speech translation system in the air travelplanning (ATIS) domain.
The modular architecture con-sists of a pipelined series of processing phases that eachoutput multiple hypotheses filtered by statistical pref-erence mechanisms.
2 The statistical information usedin the system is derived from automatic processing ofdomain corpora.
The architecture provides greater o-bustness than a 1-best approach, and yet is more com-putationally tractable and more portable to new lan-guages and domains than a tight integration, because ofthe modularity of the components: peech recognition,source language processing, source to target languagetransfer, target language processing, and speech synthe-sis.Some aspects of adaptation to the domain task werefairly simple: addition of new lexical entries was facil-itated by existing tools, and grammar coverage required1 The research reported in this paper was sponsored by SwedishTelecom (Televerket Ngt).
Several people not listed as co-authorshave also made contributions to the project: among these we wouldparticularly like to mention Marie-Susanne AgnKs, George Chen,Dick Crouch, Bsrbro Ekholm, Arnold Smith, Tomas Svensson andTorbjSm ~hs.2The preference mechanism between target language text out-put and speech synthesis has not yet been implemented.adding only a few very domain-specific phrase structurerules, as described in Section 3.1.
Much of the effort inthe project, however, has focussed on the developmentof well-specified methods for adapting and customizingother aspects of the existing modules, and on tools forguiding the process.
In addition to the initial results(Section 5), the reported work makes several contribu-tions to speech translation i  particular and to languageprocessing in general:A general method for training statistical preferencesto filter multiple hypotheses, for use in ranking bothanalysis and translation hypotheses (Section 3.2);A method for rapid creation of a grammar for thetarget language by exploiting overlapping syntacticstructures in the source and target languages (Sec-tion 3.3);An Explanation Based Learning (EBL) techniquefor automatically chunking the grammar into com-monly occurring phrase-types, which has provenvaluable in maximizing return on effort expendedon coverage xtension, and a set of procedures forautomatic testing and reporting that helps to en-sure smooth integration across aspects of the effortperformed at the various ites involved (Section 4).2.
COMPONENTS ANDINTERFACESThe speech translation process begins with SRI's DE-CIPHER(TM) system, based on hidden Markov mod-eling and a progressive search \[12, 13\].
It outputs tothe source language processor asmall lattice of word hy-potheses generated using acoustic and language modelscores.
The language processor, for both English andSwedish, is the SRI Core Language Engine (CLE) \[1\], aunification-based, broad coverage natural anguage sys-tem for analysis and generation.
Transfer occurs at thelevel of quasi logical form (QLF); transfer ules are de-fined in a simple declarative formalism \[2\].
Speech syn-thesis is performed by the Swedish Telecom PROPHON217system \[8\], based on stored polyphones.
This sectiondescribes in more detail these components and their in-terfaces.2.1.
Speech RecognitionThe first component is a fast version of SRI's DE-CIPHER(TM) speaker-independent continuous peechrecognition system \[12\].
It uses context-dependentphonetic-based hidden Markov models with discrete ob-servation distributions for 4 features: cepstrum, delta-cepstrum, energy and delta-energy.
The models aregender-independent a d the system is trained on 19,000sentences and has a 1381-word vocabulary.
The progres-sive recognition search \[13\] is a three-pass scheme thatproduces a word lattice and an N-best list for use by thelanguage analysis component.
Two recognition passesare used to create a word lattice.
During the forwardpass, the probabilities of all words that can end at eachframe are recorded, and this information is used to prunethe word lattice generated in the backward pass.
Theword lattice is then used as a grammar to constrain thesearch space of a third recognition pass, which producesan N-best list using an exact algorithm.2.2.
Language Ana lys i s  and  Generat ionLanguage analysis and generation are performed by theSRI Core Language Engine (CLE), a general natural-language processing system developed at SRI Cambridge\[1\]; two copies of the CLE are used, equipped with En-glish and Swedish grammars respectively.
The Englishgrammar is a large, domain-independent unification-based phrase-structure grammar, augmented by a smallnumber of domain-specific rules (Section 3.1).
TheSwedish grammar is a fairly direct adaptation of the En-glish one (Section 3.3).The system's linguistic information is in declarativeform, compiled in different ways for the two tasks.
Inanalysis mode, the grammar is compiled into tables thatdrive a left-corner parser; input is supplied in the formof a word hypothesis lattice, and output is a set of pos-sible semantic analyses expressed in Quasi Logical Form(QLF).
QLF includes predicate-argument structure andsome surface features, but also allows a semantic analysisto be only partially specified \[3\].The set of QLF analyses is then ranked in order of apriori plausibility using a set of heuristic preferences,which are partially trainable from example corpus data(Section 3.2).
In generation mode, the linguistic infor-mation is compiled into another set of tables, which con-trol a version of the Semantic Head-Driven Generationalgorithm \[16\].
Here, the input is a QLF form, and theoutput is the set of possible surface strings which real-ize the form.
Early forms of the analysis and generationalgorithms used are described in \[1\].2.3.
Speech/Language In ter faceThe interface between speech recognition and source lan-guage analysis can be either a 1-best or an N-best inter-face.
In 1-best mode, the recognizer simply passes theCLE a string representing the single best hypothesis.
InN-best mode, the string is replaced by a list contain-ing all hypotheses that are active at the end of the thirdrecognition pass.
Since the word lattice generated duringthe first two recognition passes ignificantly constrainsthe search space of the third pass, we can have a largenumber of hypotheses without a significant increase incomputation.As the CLE is capable of using lattice input directly\[6\], the N-best hypotheses are combined into a new lat-tice before being passed to linguistic processing; in caseswhere divergences occur near the end of the utterance,this yields a substantial speed improvement.
The differ-ent analyses produced are scored using a weighted sumof the acoustic score received from DECIPHER and thelinguistic preference score produced by the CLE.
Whenat least one linguistically valid analysis exists, this im-plicitly results in a selection of one of the N-best hy-potheses.
Our experimental findings to date indicatethat N=5 gives a good tradeoff between speed and accu-racy, performance surprisingly being fairly insensitive tothe setting of the relative weights given to acoustic andlinguistic scoring information.
Some performance r sultsare presented in Section 5.2.4.
T rans ferUnification-based QLF transfer \[2\], compositionallytranslates a QLF of the source language to a QLF of thetarget language.
QLF is the transfer level of choice inthe system, since it is a contextually unresolved seman-tic representation reflecting both predicate-argument re-lations and linguistic features uch as tense, aspect, andmodality.
The translation process uses declarative trans-fer rules containing cross-linguistic data, i.e., it specifiesonly the differences between the two languages.
Themonolingual knowledge of grammars, lexica, and prefer-ences is used for ranking alternative target QLFs, filter-ing out ungrammatical QLFs, and finally generating thesource language utterance.A transfer ule specifies a pair of QLF patterns; the lefthand side matches a fragment of the source languageQLF and the right hand side the corresponding targetQLF.
Table 1 breaks down transfer ules by type.
As canbeen seen, over 90% map atomic constants to atomicconstants; of the remainder, about half relate to spe-218Table 1: Transfer ule statistics IAtom to atom 649 91%Complex Ilexical) 27 i 4%Complex (non-lexical) 34 I 5%Total I  1011?
?%1cific lexical items, and half are general structural trans-fer rules.
For example, the following rule expresses amapping of English NPs postnominally modified by aprogressive VP (aFiights going to Boston") to SwedishNPs modified by a relative clause ( "Flygningar som gdrtill Boston"):\[and,1;r (head),form(verb (t  enne=n, perf=P, prog=y),tr (rood))\]>=l and ,  t r  (head) ,\[island, form(verb(tense=pres ,perf=P, prog=n),tr (mod))2 \]Transfer variables, of the form tr(atom), show howsubexpressions in the source QLF correspond to subex-pressions in the target QLF.
Note how the transitionfrom a tenseless, progressive VP to a present ense, non-progressive VP can be specified irectly through chang-ing the values of the slots of the "verb" term.
Thisfairly simple transfer ule formalism seems to allow mostimportant restructuring phenomena (e.g., change of as-pect, object raising, argument switching, and to someextent also head switching) to be specified succinctly.The degree of compositionality in the rule set currentlyemployed is high; normally no special transfer ules areneeded to specify combinations of complex transfer.
Inaddition, the vast majority of the rules are reversible,providing for future Swedish to English translation.2.5.
Speech SynthesisThe Prophon speech synthesis ystem, developed atSwedish Telecom, is an interactive nvironment for de-veloping applications and conducting research in multi-lingual text-to-speech onversion.
The system includes alarge lexicon, a speech synthesizer and rule modules fortext formatting, syntactic analysis, phonetic transcrip-tion, parameter generation and prosody.
Two synthesisstrategies are included in the system, formant synthesisand polyphone synthesis, i.e., concatenation of speechunits of arbitrary size.
In the latter case, the synthesizeraccesses the database of polyphone speech waveforms ac-cording to the allophonic specification derived from thelexicon and/or phonetic transcription rules.
The poly-phones are concatenated and the prosody of the utter-anee is imposed via the PSOLA (pitch synchronous over-lap add) signal processing technique \[11\].
The Prophonsystem has access to information other than the textstring, in particular the parse tree, which can be used toprovide a better, more natural prosodic structure thannormally is possible.3.
ADAPTATIONIn this section, we describe the methods used for adapt-ing the various processing components to the English-Swedish ATIS translation task.
Section 3.1 describesthe domain customization of the language component,and section 3.2 the semi-automatic method developedto customize the linguistic preference filter.
Finally, sec-tion 3.3 summarizes the work carried out in adapting theEnglish-language rammar and lexicon to Swedish.3.1.
CLE Domain AdaptationWe begin by describing the customizations performedto adapt the general CLE English grammar and lexiconto the ATIS domain.
First, about 500 lexical entriesneeded to be added.
Of these, about 450 were regularcontent words ( airfare, Boston, seven forty seven, etc.
),all of which were added by a graduate student 3 usingthe interactive VEX lexicon acquisition tool \[7\].
About55 other entries, not of a regular form, were also added.Of these, 26 corresponded to the letters of the alphabet,which were treated as a new syntactic lass, 15 or so wereinterjections (Sure, OK, etc.
), and seven were entries forthe days of the week, which turned out to have slightlydifferent syntactic properties in American and BritishEnglish.
The only genuinely new entries were for avail-able, round trip, first class, nonstop and one way, allof which failed to fit syntactic patterns previously im-plemented within the grammar, (e.g.
"Flights availablefrom United", "Flights to Boston first class").Sixteen domain-specific phrase-structure rules were alsoadded, most of them by the graduate student.
Of these,six covered 'code' expressions (e.g.
"Q X"), and eightcovered 'double utterances' (e.g.
"Flights to Boston showme the fares").
The remaining two rules covered or-dinal expressions without determiners ("Next flight toBoston"), and PP expressions of the form 'Name toName' (e.g.
"Atlanta to Boston Friday").
Finally, thepreference metrics were augmented by a preference forattaching 'from-to' PP pairs to the same constituent,(this is a domain-independent heuristic, but is particu-larly important in the context of the ATIS task), andthe semantic ollocation preference metrics (Section 3.2)3Marie-Susanne AgnKs, the graduate student in question, was acompetent linguist but had no previous experience with the CLEor other large computational grammars.219were retrained with ATIS data.
The grammar and lexi-con customization effort has so far consumed about hreeperson-months of specialist ime, and about two and ahalf person-months of the graduate student.
The currentlevel of coverage is indicated in Section 5.3.2.
T ra in ing  Pre ference  Heur i s t i csGrammars with thorough coverage of a non-trivial sub-language tend to yield large numbers of analyses formany sentences, and rules for accurately selecting thecorrect analysis are difficult if not impossible to stateexplicitly.
We therefore use a set of about twenty pref-erence metrics to rank QLFs in order of a priori plau-sibility.
Some metrics count occurrences of phenomenasuch as adjuncts, ellipsis, particular attachment config-urations, or balanced conjunctions.
Others, which aretrained automatically, reflect the strengths of semanticcollocations between triples of logical constants occur-ring in relevant configurations in QLFs.The overall plausibility score for a QLF under thisscheme is a weighted (scaled) sum of the scores returnedby the individual metrics.
Initially, we chose scaling fac-tors by hand, but this became an increasingly skilledand difficult task as more metrics were added, and itwas clear that the choice would have to be repeated forother domains.
The following semi-automatic optimiza-tion procedure \[4\] was therefore developed.QLFs were derived for about 4600 context-independentand context-dependent ATIS sentences of 1 to 15 words.It is easy to derive from a QLF the set of segments oftheinput sentence which it analyses as being either predi-cations or arguments.
These segments, taken together,effectively define a tree of roughly the form used by theTreebank project \[5\].
A user presented with all stringsderived/.from any QLF for a sentence selected the cor-rect tree (if present).
A skilled judge was then able toassign trees to hundreds of sentences per hour.The "goodness" of a QLF Q with respect o an approvedtree T was defined as I(Q,T) - 10.
A(Q,T), whereI(Q, T) is the number of string segments induced by Qand present in T, and A(Q, T) is the number induced byQ but absent from T. This choice of goodness functionwas found, by trial and error, to lead to a good corre-lation with the metrics.
Optimization then consisted ofminimizing, with respect o scaling factors ej for eachpreference metric mi, the value of~(g,  - E~ ei*~J) 2where gl is the goodness of QLF i and sit is the scoreassigned to QLF i by metric fj ; to remove some "noise"from the data, all values were relativized by subtractingthe (average of the) corresponding scores for the best-scoring QLF(s) for the sentence.The kth simultaneous equation, derived by setting thederivative of the above expression with respect o ck tozero for the minimum, is~, s~(gi - Z~ cj,i~) = 0These equations can be solved by Gaussian elimination.The optimized and hand-selected scaling factors each re-suited in a correct QLF being selected for about 75%of the 157 sentences from an unseen test set that werewithin coverage, showing that automatic scaling canproduce results as good as those derived by labour-and skill-intensive hand-tuning.
The value of Kendall'sranking correlation coefficient between the relativized"goodness" values and the scaled sum (reflecting thedegree of agreement between the orderings induced bythe two criteria) was also almost identical for the twosets of factors.
However, the optimized factors achievedmuch better correlation (0.80 versus 0.58) under themore usual product-moment definition of correlation,o',v/o'xo'v, which the least-squares optimization usedhere is defined to maximize.
This suggests that opti-mization with respect o a (non-linear) criterion that re-fleets ranking rather than linear agreement could leadto a still better set of scaling factors that might out-perform both the hand-selected and the least-squares-optimal ones.
A hill-climbing algorithm to determinesuch factors is therefore being developed.The training process allows optimization of scaling fac-tors, and also provides data for several metrics assessingsemantic ollocations.
In our case, we use semantic ol-locations extracted from QLF expressions in the formof (H1, R, H2) triples where H1 and H2 are the headpredicates of phrases in a sentence and R indicates thesemantic relationship (e.g.
a preposition or an argumentposition) between the two phrases in the proposed anal-ysis.
We have found that a simple metric, original tous, that scores triples according to the average treebankscore of QLFs in which they occur, performs about aswell as a chi-squared metric, and better than one basedon mutual information (of \[9\]).3.3.
CLE  Language Adaptat ionThe Swedish-language customization of the CLE (S-CLE) has been developed at SICS from the English-language version by replacing English-specific mod-ules with corresponding Swedish-language v rsions.
4Swedish is a Germanic language, linguistically about as"far" from English as German is.
Our experience sug-4The S-CLE and the adaptation process i described in detailin \[lo\].220gests that adapting the English system to close languagesis fairly easy and straight-forward.
The total effort spenton the Swedish adaptation was about 14 person-months(compared with about 20 person-years for the originalCLE), resulting in coverage only slightly less than thatof the English version.The amount of work needed to adapt the various CLEmodules to Swedish declined steadily as a function oftheir "distance" from surface structure.
Thus the mor-phology rules had to be nearly completely rewritten;Swedish morphology is considerably more complex thanEnglish.
In contrast, only 33 of the 401 Swedish functionword entries were not derived from English counterparts,the differences being confined to variance in surface formand regular changes to the values of a small number offeatures.
At the level of syntax, 97 (81%) of a set of 120Swedish syntax rules were derived from exact or verysimilar English rules.
The most common difference issome small change in the features; for example, Swedishmarks for definiteness, which means that this featureoften needs to be added.
11 rules (9%) originated in En-glish rules, but had undergone major changes, e.g., somepermutation or deletion of the daughters; thus Swedishtime rules demand a word-order which in English wouldbe "o'clock five", and there is a rule that makes an NPout of a bare definite NBAR.
This last rule correspondsto the English NP ~ DET NBAR rule, with the DETdeleted but the other features instantiated as if it werepresent.
Only 12 (10%) Swedish syntax rules were com-pletely new.
The percentage of changed semantic ruleswas even smaller.The most immediately apparent surface divergences be-tween Swedish and English word-order stem from thestrongly verb-second nature of Swedish.
Formation ofboth YN- and WH-questions i by simple inversion of thesubject and verb without the introduction of an auxil-iary, thus for example "Did he fly with Delta?"
is "FlSghan rned Delta?
", lit.
"Flew he with Delta?".
It is worthnoting that these changes can all be captured by doingno more than adjusting features.
The main rules thathad to be written "from scratch" are those that coveradverbials, negation, conditionals, and the common vad.
.
.
f Jr  construction, e.g., "Vad finns det fJr flygningartill Atlanta" (lit.
"What are there for flights to Atlanta",i.e., "What flights are there to Atlanta?").4.
RAT IONAL DEVELOPMENTMETHODOLOGYIn a project like this one, where software developmentis taking place simultaneously at several sites, regulartesting is important o ensure that changes retain inter-component compatibility.
Our approach is to maintaina set of test corpora to be run through the system (fromtext analysis to text generation) whenever a significantchange is made to the code or data.
Changes in the sta-tus of a sentence - the translation it receives, or the stageat which it fails if it receives no translation - are noti-fied to developers, which facilitates bug detection anddocumentation of progress.The most difficult part of the exercise is the constructionof the test corpora.
The original training/developmentcorpus is a 4600-sentence subset of the ATIS corpus con-sisting of sentences of length not more than 15 words.For routine system testing, this corpus is too large to beconvenient; if a randomly chosen subset is used instead,it is often difficult to tell whether processing failures areimportant or not, in the sense of representing problemsthat occur in a large number of corpus sentences.
Whatis needed is a sub-corpus that contains all the commonlyoccurring types of construction, together with an indi-cation of how many sentences each example in the sub-corpus represents.We have developed a systematic method for constructingrepresentative sub-corpora, using "Explanation BasedLearning" (EBL) \[15\].
The original corpus is parsed,and the resulting analysis trees are grouped into equiva-lence classes; then one member  is chosen from each class,and stored with the number of examples it represents.
Inthe simplest version, trees are equivalent if their leavesare of the same lexical types.
The criterion for equiva-lence can be varied easily: we have experimented withschemes where all sub-trees representing NPs are deemedto be equivalent.
When generalization is performed overnon-lexical classes like NPs and PPs, the method is usedrecursively to extract representative examples of eachgeneralized class.At present, three main EBL-derived sub-corpora areused for system testing.
Corpus 1, used most frequently,was constructed by generalizing at the level of lexicalitems, and contains one sentence for each class with atleast three members.
This yields a corpus of 281 sen-tences, which together represent 1743 sentences from theoriginal corpus.
Corpus 2, the "lexical" test corpus, isa set with one analyzable phrase for each lexical itemoccuring at least four times in the original corpus, com-prising a total of 460 phrases.
Corpus 3 generalizes overNPs and PPs, and analyzes NPs by generalizing overnon-recursive NP  and PP  constituents; one to five ex-amples are included for each class that occurs ten ormore times (depending on the size of the class), giving244 examples.
This corpus is useful for finding problemslinked with constructions specific to either the NP  orthe sentence level, but not to a combination.
The timeneeded to process each corpus through the system is on221the order of an hour.5.
RESULTS OF SYSTEMEVALUATIONIn this final section we present evaluation results for thecurrent version of the system running on data previouslyunseen by the developers.
There is so far little consensuson how to evaluate spoken language translation systems;for instance, no evaluation figures on unseen materialare cited for the systems described in \[17\] and \[14\].
Wepresent he results below partly in an attempt o stimu-late discussion on this topic.The sentences of lengths 1 to 12 words from the Fall1992 test set (633 sentences from 1000) were processedthrough the system from speech signal to target languagetext output, and the translations produced were evalu-ated by a panel fluent in both languages.
Points wereawarded for meaning preservation, gramrnatieality of theoutput, naturalness of the output, and preservation ofthe style of the original, and a translation had to beclassified as acceptable on all four counts to be regardedas acceptable in general.
Judgements were also elicitedfor intermediate r sults, in particular whether a speechhypothesis could be judged as a valid variant of the refer-ence sentence in the context of the translation task, andwhether the semantic analysis ent to the transfer stagewas correct.
The criteria used to determine whether aspeech hypothesis was a valid variant of the referencewere strict, typical differences being substitution of allthe for plural the, what's for what is, or I want for I 'dlike.The results were as follows.
For 1-best recognition,62.4% of the hypotheses were equal to or valid vari-ants of the reference, and 55.3% were valid and alsowithin grammatical coverage.
For 5-best recognition,the corresponding figures were 78.2% and 69.0%.
Se-lecting the acoustically highest-ranked hypothesis thatwas inside grammatical coverage yielded an acceptablechoice in 61.1% of the examples; a scoring scheme thatchose the best hypothesis using a weighted combinationof the acoustic and linguistic scores did slightly better,increasing the proportion to 63.0%.
54% of the exam-pies received a most preferred semantic analysis that wasjudged correct, 45.3% received a translation, and 41.8%received an acceptable translation.
The correspondingerror rates for each component are shown in table 2.References1.
Alshawi, H.
(ed.
), The Core Language Engine, MITPress, 1992.2.
Alshawi, H., Carter, D., Rayner, M. and Gamb~ck,B., "Transfer through Quasi Logical Form", Proc.
~9thTable 2: Component error rates(1-best recognition)5-best recognitionpx~a.
,T J .
-  I ~l.l.l~.j~a, ,,llr'~ e.,, i ,(37.4%)21.8%8.7%Source linguistic analysis 11.8%Source analysis preferences 13.4%Transfer and generation 22.7%A CL, Berkeley, 1991.3.
Alshawi, H. and Crouch, R., "Monotonic Semantic In-terpretation", Proc.
30th A CL, Newark, 1992.4.
Alshawi, H., and Carter, D., "Optimal Scaling of Pref-erence Metrics", SRI Cambridge Research Report, 1992.5.
Black, E., et al, "A Procedure for Quantitatively Com-paring the Syntactic Coverage of English Grammars,"Proc.
Third DARPA Speech and Language Workshop,P.
Price (ed.
), Morgan Kaufmann, June 1991.6.
Carter, D.M., "Lattice-based Word Identification inCLARE", Proc 30th A CL, Newark, 1992.7.
Carter, D.M., "Lexical Acquisition in the Core LanguageEngine", Proc.
~th European A CL, Manchester, 1989.8.
Ceder, K. and Lyberg, B., "Yet Another Rule Compilerfor Text-to-Speech Conversion?
", Proc.
ICSLP, Banff,1993.9.
Church, K.W.
and Hanks, P., "Word Association Norms,Mutual Information, and Lexicography", ComputationalLinguistics 16:22-30, 1990.10.
Gamb~.ck, B. and Rayner, M., "The Swedish Core Lan-guage Engine", Proc.
3rd NOTEX, LinkSping, 1992.11.
Moulines, E. and Charpentier, F., "Pitch-SynchronousWaveform Processing Techniques for Text-to-SpeechSynthesis Using Diphones', Speech Communication Vol.9, 1990.12.
Murveit, H., Butzberger, J. and Weintraub, M., "SpeechRecognition in SRI's Resource Management and ATISSystems", Proc.
DARPA Workshop on Speech and Nat-ural Language, 1991.13.
Murveit, H., et al, "Large Vocabulary Dictation us-ing SRI's DECIPHER(TM) Speech Recognition System:Progressive Search Techniques", Proc.
1CASSP, 1993.14.
Roe, D.B., Pereira, F.C.N., Sproat, R.W., Riley, M.D.and Moreno, P.J., "Towards a Spoken-Language Trans-lator for Restricted-Domain Context-Free Languages",Proc.
Eurospeech, 1991.15.
Samuelsson, C. and Rayner, M., "Quantitative Evalua-tion of Explanation-Based Learning as an OptimizationTool for a Large-Scale Natural Language System", Proc.1Pth IJCAI, Sydney, 1991.16.
Shieber, S. M., van Noord, G., Pereira, F.C.Nand Moore, R.C., "Semantic-Head-Driven Generation",Computational Linguistics, 16:30-43, 1990.17.
Woszczyna, M. et al, "Recent advances in JANUS: ASpeech Translation System", ARPA Workshop on Hu-man Language Technology, Plainsboro, N J, 1993.222
