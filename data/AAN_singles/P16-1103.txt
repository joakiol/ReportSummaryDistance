Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1085?1094,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsSynthesizing Compound Words for Machine TranslationAustin Matthews and Eva Schlinger and Alon Lavie and Chris DyerLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213, USA{austinma,eschling,alavie,cdyer}@cs.cmu.eduAbstractMost machine translation systems con-struct translations from a closed vocabu-lary of target word forms, posing problemsfor translating into languages that haveproductive compounding processes.
Wepresent a simple and effective approachthat deals with this problem in two phases.First, we build a classifier that identifiesspans of the input text that can be trans-lated into a single compound word in thetarget language.
Then, for each identi-fied span, we generate a pool of possiblecompounds which are added to the trans-lation model as ?synthetic?
phrase trans-lations.
Experiments reveal that (i) wecan effectively predict what spans can becompounded; (ii) our compound gener-ation model produces good compounds;and (iii) modest improvements are pos-sible in end-to-end English?German andEnglish?Finnish translation tasks.
We ad-ditionally introduce KomposEval, a newmulti-reference dataset of English phrasesand their translations into German com-pounds.1 IntroductionMachine translation systems make a closed-vocabulary assumption: with the exception of ba-sic rules for copying unknown word types fromthe input to the output, they can produce words inthe target language only from a fixed, finite vo-cabulary.
While this is always a na?ve assumptiongiven the long-tailed distributions that character-ize natural language, it is particularly challengingin languages such as German and Finnish that haveproductive compounding processes.In such languages, expressing compositions ofbasic concepts can require an unbounded num-ber of words.
For example, English multiwordphrases like market for bananas, market for pears,and market for plums are expressed in Germanwith single compound words (respectively, as Ba-nanenmarkt, Birnenmarkt, and Pflaumenmarkt).Second, while they are individually rare, com-pound words are, on the whole, frequent in nativetexts (Baroni et al, 2002; Fritzinger and Fraser,2010).
Third, compounds are crucial for transla-tion quality.
Not only does generating them makethe output seem more natural, but they are content-rich.
Since each compound has, by definition, atleast two stems, they are intuitively (at least) dou-bly important for translation adequacy.Fortunately, compounding is a relatively regularprocess (as the above examples also illustrate), andit is amenable to modeling.
In this paper we intro-duce a two-stage method (?2) to dynamically gen-erate novel compound word forms given a sourcelanguage input text and incorporate these as ?syn-thetic rules?
in a standard phrase-based transla-tion system (Bhatia et al, 2014; Chahuneau et al,2013; Tsvetkov et al, 2013).
First, a binary classi-fier examines each source-language sentence andlabels each span therein with whether that spancould become a compound word when translatedinto the target language.
Second, we transducethe identified phrase into the target language usinga word-to-character translation model.
This sys-tem makes a closed vocabulary assumption, albeitat the character (rather than word) level?therebyenabling new word forms to be generated.
Train-ing data for these models is extracted from auto-matically aligned and compound split parallel cor-pora (?3).We evaluate our approach on both intrinsic andextrinsic metrics.
Since German compounds arerelatively rare, their impact on the standard MTevaluation metrics (e.g., BLEU) is minimal, as we1085show with an oracle experiment, and we find thatour synthetic phrase approach obtains only mod-est improvements in overall translation quality.
Tobetter assess its merits, we commissioned a newtest set, which we dub KomposEval (from the Ger-man word for a compound word, Komposita), con-sisting of a set of 1090 English phrases and theirtranslations as German compound words by a pro-fessional English?German translator.
The transla-tor was instructed to produce as many compound-word translations as were reasonable (?4).
Thisdataset permits us to evaluate our compound gen-eration component directly, and we show that(i) without mechanisms for generating compoundwords, MT systems cannot produce the long tailof compounds; and (ii) our method is an effectivemethod for creating correct compounds.2 Compound Generation via RuleSynthesisSuppose we want to translate the sentencethe market for bananas has collapsed .from English into German.
In order to produce thefollowing (good) translation,der bananenmarkt ist abgest?rzt .a phrase-based translation system would need tocontain a rule similar to market for bananas ?bananenmarkt.
While it is possible that such arule would be learned from parallel corpora us-ing standard rule extraction techniques, it is likelythat such a rule would not exist (unless the systemwere trained on the translation examples from thispaper).We solve the compound translation problem by?filling in?
such missing rule gaps in the phrasetable.
The process takes place in two parts:first, identifying spans in the input that appearto be translatable as compounds (?2.1), and sec-ond, generating candidate compounds for eachpositively identified span (?2.2).
Since synthe-sized rules compete along side rules which arelearned using standard rule extraction techniques(and which are often quite noisy), our rule synthe-sis system can overgenerate rule candidates, a factwhich we exploit in both phases.2.1 Phase I: Classifying CompoundableSpansGiven a source sentence, we classify each spantherein (up to some maximum length) as eithercompoundable or non-compoundable using in-dependent binary predictions.
Rather than at-tempting to hand-engineer features to representphrases, we use a bidirectional LSTM to learn afixed-length vector representation hi,jthat is com-puted by composing representations of the tokens(fi, fi+1, .
.
.
, fj) in the input sentence.
The prob-ability that a span is compoundable is then mod-eled as:p(compoundable?
|fi, fi+1, .
.
.
, fj) =?
(w>tanh(Vhi,j+ b) + a),where ?
is the logistic sigmoid function, and w,V, b, and a are parameters.To represent tokens that are inputs to the LSTM,we run a POS tagger (Toutanova et al, 2003), andfor each token concatenate a learned embedding ofthe tag and word.
Figure 1 shows the architecture.market for bananas </s><s><s> NN IN NNS </s>p(not a compound)p(is a compound)MLP hidden layerForward LSTMBackward LSTMConcatenatedEmbeddingsPart-of-speechEmbeddingsWordEmbeddingsFigure 1: A graphical representation of the neu-ral network used for classifying whether an in-put source phrase should or should not turn intoa compound word in the target language2.2 Phase II: Generating Compound WordsThe second stage of our compound-generatingpipeline is to generate hypothesis compoundwords for each source phrase that was identified as?compoundable?
by the classifier just discussed.We do this by using a word-to-character?basedmachine translation system, which enables us toreuse a standard phrase-based decoder for com-pound generation.10862.2.1 Generation ModelThe cornerstone of our generation approach is theforward and backward lexical translation tableslearned by an unsupervised word aligner.
We com-bine these two translation tables to create a word-to-character phrase table compatible with a stan-dard decoder.
This table allows our generator toknow the correct translations of individual mor-phemes, but alone does not allow the generator tobuild full compound words.To capture the small bits of ?phonetic glue?
(e.g., the n that occurs between banane and marktin the compound bananenmarkt) that may occurwhen generating compound words, we insert aspecial SUF symbol in between each pair of sourcewords.
This symbol will allow us to insert a smallsuffix in between the translations of source words.Finally, we insert a special END symbol at theend of each source phrase.
This symbol will al-low the model to generate morphological variantsdue to suffixes indicating case, number, and agree-ment that only occur at the end of a whole com-pound word, but not in between the individualpieces.
Some examples of all three types of rulesare shown in Table 1.2.2.2 Reordering and Word DroppingWe observe that in order to generate many com-pounds, including bananenmarkts from ?marketfor bananas?, a system must be able to both re-order and drop source words at will.
Imple-mented na?vely, however, these allowances mayproduce invalid interleavings of source words andSUF/END tokens.
For example, if we (correctly)drop the word ?for?
from our example, we mightfeed the decoder the sequence ?market SUF SUFbananas END.To disallow such bogus input sequences we dis-able all reordering inside the decoder, and insteadencode all possible reorderings in the form of aninput lattice (Dyer et al, 2008).
Moreover, weallow the decoder to drop non-content words byskipping over them in the lattice.
Each edge in ourlattices contains a list of features, including the in-dices, lexical forms, and parts of speech of eachword kept or dropped.
Each possible sequence inthe lattice also encodes features of the full pathof source words kept, the full list of source wordsdropped, the parts of speech of the path and alldropped words, and the order of indices traversed.With these constraints in place we can train thecompound generator as though it were a normalMT system with no decode-time reordering.3 TrainingOur approach to generating compound word formsin translation has two stages.
First, we build a clas-sifier that chooses spans of source text that couldproduce target compounds.
Second, we build acompound generator that outputs hypothesis wordforms, given a source phrase.
We will detail eachof these steps in turn.3.1 Extracting Compounds from BitextIn order to learn to generate compound words wenaturally require training data.
Ideally we wouldlike a large list of English phrases with their nat-ural contexts and translations as German com-pounds.
Of course virtually no such data exists,but it is possible to extract from parallel data, us-ing a technique similar to that used by Tsvetkovand Wintner (2012).To this end, we take our tokenized bitext andpass it through Dyer (2009)?s German compoundsplitter.
We then align the segmented variant usingthe fast_align tool in both the forward and re-verse directions, which produces both word align-ments and lexical translation tables, which givethe probability of a compound part given an En-glish phrase.
We then symmetrize the producedpair of alignments with the intersection heuris-tic.
This results in a sparse alignment in whicheach target word is aligned to either 0 or 1 sourcewords.
We then undo any splits performed bythe compound splitter, resulting in a corpus wherethe only words aligned many-to-one are preciselywell-aligned compounds.This process produces two crucially importantdata.
First, a list of English phrase pairs that maybecome compound words in German on which wetrain our classifier.
Second, the lexical translationtables, trained on compound split German data,which form the basis of our generation approach.3.2 Training the Compoundability ClassifierThe network is trained to maximize cross-entropyof its training data using the Adam optimizer(Kingma and Ba, 2014) until performance on aheld-out dev set stops improving.Due to the fact that we only need to representthe ?compoundability?
of each source-languageword, and not its full semantics, we find thatvery small (10-dimensional) word and POS em-1087Source Target Non-Zero Featuresbananas b a n a n e ?fwd= ?0.495208 ?rev= ?0.455368market m a r k t ?fwd= ?0.499118 ?rev= ?0.269879SUF n ?fwd= ?3.718241 ?uses_suf_n= 1.0END s ?fwd= ?2.840721 ?uses_end_s= 1.0Table 1: A fragment of the word-to-character rules used in the compound generation system.beddings work well.
The recurrent part of the neu-ral network uses two-layer LSTM (Hochreiter andSchmidhuber, 1997) cells with the hidden layersize set to 10.
The final MLP?s hidden layer sizeis also set to 10.The training data is processed such that eachspan of length two to four is considered one train-ing example, and is labeled as positive if it is well-aligned (Brown et al, 1993) to a single Germancompound word.
Since most spans do not trans-late as compounds, we are faced with an extremeclass imbalance problem (a ratio of about 300:1).We therefore experiment with down sampling thenegative training examples to have an equal num-ber of positive and negative examples.3.3 Training the Compound GenerationModelAs a translation model, there are two compo-nents to learning the translation system: learn-ing the rule inventory and their features (?3.3.1)and learning the parameters of the generationmodel (?3.3.2).3.3.1 Learning Word to Character SequenceTranslation RulesThe possible translations of SUF and END arelearned from the list of positive training examplesextracted for our classifier.
For each example, wefind all the possible ways the source words couldtranslate, in accordance with our translation table,into nonoverlapping substrings of the target word.Any left over letters in between pieces becomepossible translations of SUF, while extra letters atthe end of the target string become possible trans-lations of END.
Probabilities for each translationare estimated by simply counting and normalizingthe number of times each candidate was seen.
SeeFigure 2 for an example of this splitting process.3.3.2 Learning Generator Feature WeightsSince the generator model is encoded as a phrase-based machine translation system, we can train itusing existing tools for this task.
We choose totrain using MIRA (Crammer and Singer, 2003),and use a 10-gram character-based languagemodel trained on the target side of the positivetraining examples extracted for the classifier.4 KomposEval Data SetTo evaluate our compound generator we neededa dataset containing English phrases that shouldbe compounded along with their German transla-tions.
To the best of our knowledge, no substantialhuman-quality dataset existed, so we created oneas part of this work.We took our list of automatically extracted (En-glish phrase, German compound) pairs and manu-ally selected 1090 of them that should compound.We then asked a native German speaker to trans-late each English phrase into German compounds,and to list as many possibile compound transla-tions as she could think of.
The result is a test setconsisting of 1090 English phrases, with between1 and 5 possible German compound translationsfor each English phrase.
This test set is publishedas supplementary material with this article.
Someexample translations are shown in Table 2.Source phrase Reference(s)transitional period?bergangsphase?bergangsperiode?bergangszeitraumChamber of deputiesAbgeordnetenhausAbgeordnetenkammerself-doubt SelbstzweifelTable 2: Examples of human-generated com-pounds from the KomposEval data set5 ExperimentsBefore considering the problem of integrating ourcompound model with a full machine translationsystem, we perform an intrinsic evaluation of eachof the two steps of our pipeline.1088pflaumenmarktsplumsmarketforTranslation TableInput Phrasemarket for plumsTarget Compound Possible Analysespflaume+n mark+tspflaumen+?
mark+tspflaume+n markt+spflaumen+?
markt+spflaume+n markts+?pflaumen+?
markts+?SUF Counts END Counts?n?sts3 2223{?, pflaume, pflaumen}{?
}{?, mark, markt, markts}Figure 2: Decomposition of a target compound into possible analyses, given a source phrase and amorpheme-level translation table.
This process allows us to learn the ?phonetic glue?
that can go inbetween morphemes, as well as the inflections that can attach to the end of a compound word.5.1 Classifier Intrinsic EvaluationWe evaluate the effectiveness of our classifier, bymeasuring its precision and recall on the two heldout test sets described in ?2.1 taken from twolanguage pairs: English?German and English?Finnish.
Furthermore, we show results both withdown-sampling (balanced data set) and withoutdown-sampling (unbalanced data set).Our classifier can freely trade off precision andrecall by generalizing its requirement to call an ex-ample positive from p(compound | span) > 0.5 top(compound | span) > ?
, for ?
?
(0, 1), allowingus to report full precision-recall curves (Figure 3).We find that our best results for the unbalancedcases come at ?
= 0.24 for German and ?
= 0.29for Finnish, with F-scores of 20.1% and 67.8%,respectively.
In the balanced case, we achieve67.1% and 97.0% F-scores with ?
= 0.04 and?
= 0.57 on German and Finnish respectively.5.2 Generator Instrinsic EvaluationTo evaluate our compound generator, we fed it thesource side of our newly created KomposEval cor-pus and had it output a 100-best list of hypothesestranslations for each English phrase.
From this weare able to compute many intrinsic quality metrics.We report the following metrics:?
Mean reciprocal rank (MRR); which is onedivided by the average over all segments ofthe position that the reference translation ap-pears in our k-best list.?
Character error rate (CER), or the averagenumber of character-level edits that are re-quired to turn our 1-best hypothesis into theFigure 3: Precision-recall curves for our com-pound classifier for two languages: German (red)and Finnish (blue).
Unbalanced test set results areshown with solid lines.
Balanced test set resultsare shown with dashed lines.nearest of the reference translations.?
Precision at 1, 5, and 10, which indicate whatpercentage of the time a reference translationcan be found in the top 1, 5, or 10 hypothesesof our k-best list, respectively.These results can be found in Table 3.
We com-pare to a na?ve baseline that is just a standardEnglish?German phrase-based translation systemwith no special handling of compound wordforms.
We immediately see that the baseline sys-tem is simply unable to generate most of the com-pound words in the test set, resulting in extraor-dinarily low metric scores across the board.
Itsone saving grace is its tolerable CER score, whichshows that the system is capable of generating thecorrect morphemes, but is failing to correctly ad-1089MRR ?CER ?
P@1 ?
P@5 ?
P@10 ?Baseline <0.01 3.305 0% 0% <0.01%Our model 0.7004 2.506 61.38% 81.47% 84.31%Table 3: Mean reciprocal rank, character errorrate, and precision at K statistics of our baselineMT system and our compound generator.join them and add the phonological glue requiredto produce a well-formed compound word.
Oursystem, on the other hand, is capable of reachingat least one of the five references for every singlesentence in the test set, and has a reference trans-lation in the top 5 hypotheses in its k-best list over80% of the time.Qualitatively, the compounds generated by ourmodel are remarkably good, and very under-standable.
Major error classes include incorrectword sense, non-compositional phrases, and spe-cial non- concatenative effects at word boundaries.An example of each of these errors, along withsome examples of good compound generation canbe found in Table 4.5.3 Extrinsic Translation EvaluationFinally, we use our compound generator as partof a larger machine translation pipeline.
We runour compound span classifier on each of our trans-lation system?s tune and test sets, and extract ourgenerator?s top ten hypotheses for each of the pos-tively identified spans.
These English phrases arethen added to a synthetic phrase table, along withtheir German compound translations, and two fea-tures: the compound generator?s score, and an in-dicator feature simply showing that the rule repre-sents a synthetic compound.
Table 5 shows someexample rules of this form.
The weights of thesefeatures are learned, along with the standard trans-lation system weights, by the MIRA algorithm aspart of the MT training procedure.The underlying translation system is a stan-dard Hiero (Chiang et al, 2005) system usingthe cdec (Dyer et al, 2010) decoder, trained onall constrained-track WMT English?German dataas of the 2014 translation task.
Tokenizationwas done with cdec?s tokenize-anythingscript.
The first character of each sentence wasdown cased if the unigram probability of thedowncased version of the first word was higherthan that of the original casing.
Word alignmentwas performed using cdec?s fast_align tool,BLEU ?METR ?TER ?
LenWMT2012Baseline 16.2 34.5 64.8 94.1+Our Compounds 16.3 34.6 64.9 94.2+Oracle Compounds 16.9 35.2 64.6 95.5WMT2013*Baseline 18.8 37.3 62.1 93.6+Our Compounds 18.9 37.5 62.3 96.7+Oracle Compounds 19.7 38.2 61.9 97.6WMT2014Baseline 19.6 38.9 64.3 103.5+Compounds 19.6 39.0 64.5 103.9+Oracle Compounds 21.7 40.9 61.1 100.6Table 6: Improvements in English?German trans-lation quality using our method of compound gen-eration on WMT 2012, 2013, and 2014.
* indi-cates the set used for tuning the MT system.and symmetrized using the grow-diag heuris-tic.
Training is done using cdec?s implementa-tion of the MIRA algorithm.
Evaluation was doneusing MultEval (Clark et al, 2011).
A 4-gramlanguage model was estimated using KenLM?slmplz tool (Heafield et al, 2013).In addition to running our full end-to-endpipeline, we run an oracle experiment whereinwe run the same pre-processing pipeline (com-pound splitting, bidirectionally aligning, intersect-ing, and de-splitting) on each test set to identifywhich spans do, in fact, turn into compounds, aswell as their ideal translations.
We then add gram-mar rules that allow precisely these source spans totranslate into these oracle translations.
This allowsus to get an upper bound on the impact compoundgeneration could have on translation quality.The results, summarized in Table 6 and Table 7,show that adding these extra compounds has littleeffect on metric scores compared to our baselinesystem.
Nevertheless, we believe that the qualita-tive improvements of our methods are more sig-nificant than the automatic metrics would indi-cate.
Our method targets a very specific problemthat pertains only to dense content-bearing targetwords that humans find very important.
Moreover,BLEU is unable to reasonably evaluate improve-ments in these long tail phenomena, as it onlycaptures exact lexical matches, and because weare purposely generating fewer target words thana standard translation system.6 Related WorkMost prior work on compound generation hastaken a different approach from the one advo-1090Input Hypothesis Reference Commentscheese specialities Fachk?se K?sespezialit?ten Wrong sense of ?specialties?band-aid Band-hilfe (Should not compound) Idiosyncratic meaningchurch towers Kirchent?rme Kircht?rme Extra word-internal case markingsugar beet farmers Zuckerr?benbauern Zuckerr?benbauern Perfecttomato processing Tomatenverarbeitung Tomatenverarbeitung Perfectgeneration of electricity Stromerzeugung Stromerzeugung Perfect, including reorderingTable 4: Examples of erroneous (top) and correct (bottom) compounds generated by our systemSource Target Non-Zero Featuresmarket for bananas bananenmarkt ?Compound= 1 ?Score= ?38.9818market for bananas bananenmarktes ?Compound= 1 ?Score= ?49.8976market for bananas marktordnung ?Compound= 1 ?Score= ?53.2197market for bananas bananenmarkts ?Compound= 1 ?Score= ?54.4962market for bananas binnenmarkt ?Compound= 1 ?Score= ?57.6816Table 5: Example synthetic rules dynamically added to our system to translate the phrase ?market forbananas?
into a German compound word.
Note that we correctly generate both the nominative form(with no suffix) and the genitive forms (with the -s and -es suffixes).BLEU ?METR ?TER ?
LenDev*Baseline 12.3 29.0 72.7 96.5+Our Compounds 12.3 29.1 72.8 96.8DevTestBaseline 11.4 29.9 71.6 96.2+Our Compounds 11.6 30.1 71.5 96.4Test Baseline 10.8 28.4 73.4 96.7+Our Compounds 10.9 28.5 73.3 96.9Table 7: Improvements in English?Finnish trans-lation quality using our method of compound gen-eration on WMT 2014 tuning, devtest, and testsets.
* indicates the set used for tuning the MTsystem.cated here, first translating the source languageinto a morphologically analyzed and segmentedvariant of the target language, and then performingmorphological generation on this sequence (Capet al, 2014; Irvine and Callison-Burch, 2013;Denkowski et al, 2013; Clifton and Sarkar, 2011;Stymne and Cancedda, 2011).Requesting multiple translations from a transla-tor has been used in the past, most notably to cre-ate HyTER reference lattices (Dreyer and Marcu,2012).
However, in contrast to full-sentence trans-lations the space of possible grammatical com-pounds is far smaller, substantially simplifying ourtask.The splitting of German compound phrases fortranslation from German into English has been ad-dressed by Koehn and Knight (2001) and Dyer(2009).
They elegantly solve the problem of hav-ing a large, open vocabulary on the source sideby splitting compound words into their constituentmorphemes and translating German into Englishat the morpheme level.
Their approach works ex-cellently when translating out of a compoundinglanguage, but is unable to generate novel com-pound words in the target language without somesort of post processing.Dynamic generation of compounds in a targetlanguage using such post processing has been ex-amined in the past by Cap et al (2014) and Cliftonand Sarkar (2011).
Both perform compound split-ting on their parallel data, train a morpheme-based translation system, and then stitch com-pound words back together using different mod-els.
While effective, their approach runs intodifficulties if the morphemes that should com-pound get separated by the reordering model dur-ing the translation process.
Both address this us-ing more complicated models, whereas our holis-tic approach handles this problem seamlessly.Stymne (2012) gives an excellent taxonomy ofcompound types in Germanic languages, and dis-cusses many different strategies that have beenused to split and merge them for the purposes ofmachine translation.
She identifies several diffi-culties with the split-translate-merge approach andpoints out some key subtleties, such as handlingof bound morphemes that never occur outside of1091compounds, that one must bear in mind when do-ing translation to or from compounding languages.The idea of using entirely character-based trans-lation systems was introduced by Vilar et al(2007).
While their letter-level translation systemalone did not outperform standard phrase-basedMT on a Spanish?Catalan task, they demonstratedsubstantial BLEU gains when combining phrase-and character-based translation models, particu-larly in low resource scenarios.7 ConclusionIn this paper we have presented a technique forgenerating compound words for target languageswith open vocabularies by dynamically introduc-ing synthetic translation options that allow spansof source text to translate as a single compoundword.
Our method for generating such syn-thetic rules decomposes into two steps.
First anRNN classifier detects compoundable spans in thesource sentence.
Second, a word-to-character ma-chine translation system translates the span of textinto a compound word.By dynamically adding compound words to ourtranslation grammars in this way we allow the de-coder, which is in turn informed by the languagemodel, to determine which, if any, of our hypoth-esized compounds look good in context.
Our ap-proach does away with the need for post process-ing, and avoids complications caused by reorder-ing of morphemes in previous approaches.
How-ever, this technique relies heavily on a strong tar-get language model.
Therefore, one important ex-tension of our work is to further study the inter-action between our model and the underlying lan-guage model.In addition to our generation technique wehave presented a new human-quality data setthat specifically targets compounding and use itto demonstrate tremendous improvements in ourtranslation system?s ability to correctly general-ize from compound words found in parallel textto match human translations of unseen compound-able phrases.1092AcknowledgementsWe thank the anonymous reviewers for their care-ful reading of the submitted draft of this paper.Furthermore, we thank Isabelle Wolf for her workin creating the KomposEval data set.
This researchwork was supported by a Google faculty researchaward and by computing resources provided bythe NSF-sponsored XSEDE program under grantTG-CCR110017.
The statements made herein aresolely the responsibility of the authors.ReferencesMarco Baroni, Johannes Matiasek, and Harald Trost.2002.
Predicting the components of german nomi-nal compounds.
In ECAI, pages 470?474.Archna Bhatia, Chu-Cheng Lin, Nathan Schneider, Yu-lia Tsvetkov, Fatima Talib Al-Raisi, Laleh Roost-apour, Jordan Bender, Abhimanu Kumar, LoriLevin, Mandy Simons, et al 2014.
Automatic clas-sification of communicative functions of definite-ness.
Association for Computational Linguistics.Peter F Brown, Vincent J Della Pietra, Stephen A DellaPietra, and Robert L Mercer.
1993.
The mathemat-ics of statistical machine translation: Parameter esti-mation.
Computational linguistics, 19(2):263?311.Fabienne Cap, Alexander Fraser, Marion Weller, andAoife Cahill.
2014.
How to produce unseen teddybears: Improved morphological processing of com-pounds in SMT.
In Proc.
EACL.Victor Chahuneau, Eva Schlinger, Noah A Smith, andChris Dyer.
2013.
Translating into morphologicallyrich languages with synthetic phrases.David Chiang, Adam Lopez, Nitin Madnani, ChristofMonz, Philip Resnik, and Michael Subotin.
2005.The hiero machine translation system: Extensions,evaluation, and analysis.
In Proceedings of the con-ference on Human Language Technology and Em-pirical Methods in Natural Language Processing,pages 779?786.
Association for Computational Lin-guistics.Jonathan H Clark, Chris Dyer, Alon Lavie, and Noah ASmith.
2011.
Better hypothesis testing for statisticalmachine translation: Controlling for optimizer insta-bility.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies: short papers-Volume2, pages 176?181.
Association for ComputationalLinguistics.Ann Clifton and Anoop Sarkar.
2011.
Combin-ing morpheme-based machine translation with post-processing morpheme prediction.
In Proceedingsof the 49th Annual Meeting of the Associationfor Computational Linguistics: Human LanguageTechnologies-Volume 1, pages 32?42.
Associationfor Computational Linguistics.Koby Crammer and Yoram Singer.
2003.
Ultracon-servative online algorithms for multiclass problems.The Journal of Machine Learning Research, 3:951?991.Waleed Ammar Victor Chahuneau MichaelDenkowski, Greg Hanneman, Wang Ling AustinMatthews Kenton Murray, Nicola Segall YuliaTsvetkov, and Alon Lavie Chris Dyer.
2013.
Thecmu machine translation systems at wmt 2013:Syntax, synthetic translation options, and pseudo-references.
In 8th Workshop on Statistical MachineTranslation, page 70.Markus Dreyer and Daniel Marcu.
2012.
Hyter:Meaning-equivalent semantics for translation eval-uation.
In Proceedings of the 2012 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, pages 162?171.
Association for Computa-tional Linguistics.Christopher Dyer, Smaranda Muresan, and PhilipResnik.
2008.
Generalizing word lattice translation.Technical report, DTIC Document.Chris Dyer, Adam Lopez, Juri Ganitkevitch, JohnathanWeese, Ferhan Ture, Phil Blunsom, Hendra Seti-awan, Vladimir Eidelman, and Philip Resnik.
2010.cdec: A decoder, alignment, and learning frameworkfor finite-state and context-free translation models.In Proceedings of ACL.Chris Dyer.
2009.
Using a maximum entropy modelto build segmentation lattices for mt.
In Proceed-ings of Human Language Technologies: The 2009Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 406?414.
Association for Computational Lin-guistics.Fabienne Fritzinger and Alexander Fraser.
2010.
Howto avoid burning ducks: combining linguistic analy-sis and corpus statistics for german compound pro-cessing.
In Proceedings of the Joint Fifth Work-shop on Statistical Machine Translation and Metric-sMATR, pages 224?234.
Association for Computa-tional Linguistics.Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.Clark, and Philipp Koehn.
2013.
Scalable modi-fied Kneser-Ney language model estimation.
In Pro-ceedings of the 51st Annual Meeting of the Associa-tion for Computational Linguistics, pages 690?696,Sofia, Bulgaria, August.Sepp Hochreiter and J?rgen Schmidhuber.
1997.Long short-term memory.
Neural computation,9(8):1735?1780.Ann Irvine and Chris Callison-Burch.
2013.
Su-pervised bilingual lexicon induction with multiplemonolingual signals.
In HLT-NAACL, pages 518?523.1093Diederik Kingma and Jimmy Ba.
2014.
Adam: Amethod for stochastic optimization.
arXiv preprintarXiv:1412.6980.Philipp Koehn and Kevin Knight.
2001.
Knowledgesources for word-level translation models.
In Pro-ceedings of the 2001 Conference on Empirical Meth-ods in Natural Language Processing, pages 27?35.Sara Stymne and Nicola Cancedda.
2011.
Productivegeneration of compound words in statistical machinetranslation.
In Proc.
WMT.Sara Stymne.
2012.
Text harmonization strategies forphrase-based statistical machine translation.Kristina Toutanova, Dan Klein, Christopher D Man-ning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In Proceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology-Volume 1, pages 173?180.
Association for Compu-tational Linguistics.Yulia Tsvetkov and Shuly Wintner.
2012.
Extractionof multi-word expressions from small parallel cor-pora.
Natural Language Engineering, 18(04):549?573.Yulia Tsvetkov, Chris Dyer, Lori Levin, and ArchnaBhatia.
2013.
Generating English determiners inphrase-based translation with synthetic translationoptions.
In Proc.
WMT.David Vilar, Jan-T Peter, and Hermann Ney.
2007.Can we translate letters?
In Proceedings of theSecond Workshop on Statistical Machine Transla-tion, pages 33?39.
Association for ComputationalLinguistics.1094
