Probabilistic Prediction and Picky Chart Parsing*David M. MagermanStanford Universitymagerman@cs.stanford.eduCarl WeirParamax Systemsweir@prc.unisys.comABSTRACTThis paper describes Picky, a probabilistic agenda-basedchart parsing algorithm which uses a technique called prob-abilistic prediction to predict which grammar ules are likelyto lead to an acceptable parse of the input.
In tests on ran-domly selected test data, "Picky generates fewer edges on av-erage than other CKY-like algorithms, while achieving 89~first parse accuracy and also enabling the parser to processsentences with false starts and other minor disfluencies.
Fur-ther, sentences which are parsed completely by the proba-bilistic prediction technique have a 97~0 first parse accuracy.1.
IntroductionTwo important concerns in natural language parsingwhich encourage the use of probabilistic analysis are ef-ficiency and accuracy.
An accurate parser which has adomain or language model so detailed that it takes hoursto process a single sentence, while perhaps interesting, isno more useful than a simple instantaneous parser whichis always wrong.
Probabilistic modelling of the grammarof a language has been proposed as a potential solutionto the accuracy problem, disambiguating grammaticalparses generated by an ambiguous grammar.
However,little attention has been paid to the repercussions ofprobabilistic parsing on the computational complexityand average-case performance of existing parsing algo-rithms.
Effective probabilistic models of grammar whichtake into account contextual information (e. g. \[10\] [2\])cannot ake advantage ofthe O(n 3) behavior of CKY-likeparsing algorithms.
If they are to use existing grammat-ical formalisms, these models must use algorithms thatare worst-case xponential.When natural language parsers are incorporated intonatural language understanding systems, another sig-nificant issue arises: robustness.
As a component of alanguage processing system, a parser's task is to an-alyze correctly all inputs which can be understood bythe system, not just those which are precisely grammat-ical.
Or, one might say, the grammar of natural lan-*Special thanks to Jerry Hobbs and Bob Moore at SRI forproviding access to their computers, and to Salim Roukos, Pe-ter Brown, and Vincent and Steven Della P ietra at IBM for theirinstructive lessons on probabillstic modell ing of natural  anguage.guage includes fragments, run-ons, split infinitives, andother disfluencies which would receive red marks on ahigh school English paper.
At the same time, meaning-less sequences of words and other uninterpretable inputsshould nol be analyzed as though they are acceptable.Robust processing of natural anguage is an ideal appli-cation of probabilistic methods, since probability theoryprovides a well-behaved measure of expectation within agiven language.This paper proposes an agenda-based probabilistic hartparsing algorithm which is both robust and efficient.
Thealgorithm, Picky 1, is considered robust because it willpotentially generate all constituents produced by a purebottom-up arser and rank these constituents by likeli-hood.
The efficiency of the algorithm is achieved througha technique called probabilistic prediction, which helpsthe algorithm avoid worst-case behavior.
Probabilisticprediction is a trainable technique for modelling whereedges are likely to occur in the chart-parsing process.
2Once predicted edges are added to the chart using prob-abilistic prediction, they are processed in a style similarto agenda-based chart parsing algorithms.
By limitingthe edges in the chart to those which are predicted bythis model, the parser can process a sentence while gen-erating only the most likely constituents given the input.The Picky parsing algorithm is divided into three phases,where the goal of each phase is to minimize the set of rulepredictions in the chart to only those necessary to gen-erate an analysis of the input sentence.
When a phasecompletes without producing an analysis of the input,the next phase expands the set of rules which it canuse and applies these new rules to the chart from theprevious phase.
The proposed algorithm is still expo-nential in the worst-case, but only exhibits worst-casebehavior on sentences which are completely outside thedomain of the training material (i. e. contain multipleoccurrences of grammatical structures rarely seen or un-seen in training).
In this work, the efficiency of various1Pearl  ~ probabil istic Earley-style parser (P-Ear l ) .
7:'icky _=probabil istic CKY-like parser (P -CKY) .2Some familiarity with chart  parsing terminology is assumed inthis paper.
For terminological definitions, see \[7\], \[8\], \[9\], or \[14\].128algorithms and effectiveness of models is determined bya comparison ofthe number of rule predicts, rule advanc-ing operations (the basic operation i  chart parsing), andcomplete constituents detected by the parser.The results of experiments u ing this parsing algorithmare quite promising.
On a corpus of 300 randomly se-lected test sentences, Picky parses these sentences with89% first parse accuracy, and up to 92% accuracy withinthe first three parses.
Further, sentences which areparsed completely by the probabilistic prediction tech-nique, in phases I and II, have a 97% first parse accu-racy.
The algorithm isextremely efficient, with less thana 1.6:1 ratio of constituents recognized to constituentsin  the final parse for sentences parsed by phases I andII.
The performance decreases for sentence outside thetraining corpus that are parsed in phase III.This paper will present the Picky parsing algorithm, de-scribing the both the original features of the parser andthose adapted from previous work.
Then, along with ac-curacy and efficiency results, the paper will report ananalysis of the interaction between the phases of theparsing algorithm and the probabilistic models of pars-ing and prediction.2.
P robab i l i s t i c  Mode lsThe probabilistic models used in the implementation fPicky are independent of the algorithm.
To facilitate thecomparison between the performance of Picky and itspredecessor, Pearl, the probabilistie model implementedfor 'Picky is similar to Pearl's scoring model, the context-free grammar with context-sensitive probability (CFGwith CSP) model.
This probabilistic model estimatesthe probability of each parse T given the words in thesentence S, P(TIS), by assuming that each non-terminaland its immediate children are dependent on the non-terminal's siblings and parent and on the part-of-speechtrigram centered at the beginning of that rule:P(TIS) -~ ~I  P(A ~ alC ~ /3A7, aoala2) (1)AETwhere C is the non-terminal node which immediatelydominates A, al is the part-of-speech associated with theleftmost word of constituent A, and a0 and a2 are theparts-of-speech of the words to the left and to the rightof al, respectively.
See Magerman and Marcus 1991 \[10\]for a more detailed escription of the CFG with CSPmodel.3.
The  Pars ing  A lgor i thmA probabilistic language model, such as the aforemen-tioned CFG with CSP model, provides a metric for eval-uating the likelihood of a parse tree.
However, while itmay suggest a method for evaluating partial parse trees,a language model alone does not dictate the search strat-egy for determining the most likely analysis of an input.Since exhaustive search of the space of parse trees pro-duced by a natural anguage grammar is generally notfeasible, a parsing model can best take advantage of aprobabilistic language model by incorporating it into aparser which probabilistically models the parsing pro-cess.
Picky attempts to model the chart parsing processfor context-free grammars using probabilistic prediction.Picky parses sentences in three phases: covered left-corner phase (I), covered bidirectional phase (II), andtree completion phase (III).
Each phase uses a differ-ent method for proposing edges to be introduced to theparse chart.
The first phase, covered left-corner, usesprobabilistic prediction based on the left-corner word ofthe left-most daughter of a constituent topropose dges.The covered bidirectional phase also uses probabilisticprediction, but it allows prediction to occur from theleft-corner word of any daughter of a constituent, andparses that constituent outward (bidirectionally) fromthat daughter.
These phases are referred to as "cov-ered" because, during these phases, the parsing mech-anism proposes only edges that have non-zero proba-bility according to the prediction model, i.e.
that havebeen covered by the training process.
The final phase,tree completion, is essentially an exhaustive search of allinterpretations of the input according to the grammar.However, the search proceeds in best-first order, accord-ing to the measures provided by the language model.This phase is used only when the probabilistic predictionmodel fails to propose the edges necessary to completea parse of the sentence.The following sections will present and motivate the pre-diction techniques used by the algorithm, and will thendescribe how they are implemented in each phase.3.1.
P robab i l i s t i c  P red ic t ionProbabilistic prediction is a general method for usingprobabilistic nformation extracted from a parsed corpusto estimate the likelihood that predicting an edge at acertain point in the chart will lead to a correct analysisof the sentence.
The Picky algorithm is not dependenton the specific probabilistic prediction model used.
Themodel used in the implementation, which is similar tothe probabilistic language model, will be described.
3The prediction model used in the implementation f3It is not necessary for the prediction model to be the same asthe language model used to evaluate complete analyses.
However,it is helpfnl if this is the case, so that the probabil ity estimates ofincomplete dges will be consistent with the probabil ity estimatesof completed constituents.129Picky estimates the probability that an edge proposedat a point in the chart will lead to a correct parse to be:79(A ~ aB~\]aoala2), (2)where al is the part-of-speech of the left-corner word ofB, a0 is the part-of-speech of the word to the left of al,and a2 is the part-of-speech of the word to the right ofa l .To illustrate how this model is used, consider the sen-tenceThe cow raced past the barn.
(3)The word "cow" in the word sequence "the cow raced"predicts NP  ~ det  It, but not NP  ~ det  n PP ,since PP is unlikely to generate a verb, based on train-ing material.
4 Assuming the prediction model is welltrained, it will propose the interpretation of "raced"as the beginning of a participial phrase modifying "thecow," as inThe cow raced past the barn mooed.
(4)However, the interpretation of "raced" as a past par-ticiple will receive a low probability estimate relative tothe verb interpretation, since the prediction model onlyconsiders local context.The process of probabilistic prediction is analogous tothat of a human parser recognizing predictive lexicalitems or sequences in a sentence and using these hints torestrict he search for the correct analysis of the sentence.For instance, a sentence beginning with a wh-word andauxiliary inversion is very likely to be a question, and try-ing to interpret it as an assertion is wasteful.
If a verb isgenerally ditransitive, one should look for two objects tothat verb instead of one or none.
Using probabilistic pre-diction, sentences whose interpretations are highly pre-dictable based on the trained parsing model can be ana-lyzed with little wasted effort, generating sometimes nomore than ten spurious constituents for sentences whichcontain between 30 and 40 constituents!
Also, in someof these cases every predicted rule results in a completedconstituent, indicating that the model made no incorrectpredictions and was led astray only by genuine ambigu-ities in parts of the sentence.3 .2 .
Exhaust ive  P red ic t ionWhen probabilistic prediction fails to generate the edgesnecessary to complete a parse of the sentence, exhaus-tive prediction uses the edges which have been generated4Throughout  his discussion, we will describe the predict ionprocess using words as the predictors of edges.
In the implementa-tion, due to sparse data  concerns, only parts-of-speech are used topredict edges.
Given more robust  est imat ion techniques, a prob-abilistic predict ion model  condit ioned on word sequences i likelyto perform as well or better.in earlier phases to predict new edges which might com-bine with them to produce a complete parse.
Exhaus-tive prediction is a combination of two existing types ofprediction, "over-the-top" prediction \[9\] and top-downfiltering.Over-the-top rediction is applied to complete dges.
Acompleted edge A ---~ o~ will predict all edges of the formB --~ flAT .~Top-down filtering is used to predict edges in order tocomplete incomplete dges.
An edge of the form A ---~c~BoB1B2fl, where a B1 has been recognized, will predictedges of the form B0 ~ 7 before B1 and edges of theform B2 ~ 8 after B1.3 .3 .
B id i rec t iona l  Pars ingThe only difference between phases I and II is that phaseII allows bidirectional parsing.
Bidirectional parsing isa technique for initiating the parsing of a constituentfrom any point in that constituent.
Chart parsing algo-rithms generally process constituents from left-to-right.For instance, given a grammar uleA ~ B1B2.- .B~,  (5)a parser generally would attempt o recognize a B1, thensearch for a B2 following it, and so on.
Bidirectionalparsing recognizes an A by looking for any Bi.
Once aBi has been parsed, a bidirectional parser looks for aBi-1 to the left of the Bi, a Bi+l to the right, and soon.Bidirectional parsing is generally an inefficient tech-nique, since it allows duplicate edges to be introducedinto the chart.
As an example, consider a context-freerule NP ~ DET N, and assume that there is a deter-miner followed by a noun in the sentence being parsed.Using bidirectional parsing, this NP rule can be pre-dicted both by the determiner and by the noun.
Theedge predicted by the determiner will look to the rightfor a noun, find one, and introduce a new edge consistingof a.completed NP.
The edge predicted by the noun willlook to the left for a determiner, find one, and also intro-duce a new edge consisting of a completed NP.
Both ofthese NPs represent identical parse trees, and are thusredundant.
If the algorithm permits both edges to beinserted into the chart, then an edge XP ~ o~ NP fl willbe advanced by both NPs, creating two copies of everyXP edge.
These duplicate XP edges can themselves beused in other rules, and so on.5In the implementat ion of 'Picky, over-the-top redict ion forA ~ o~ will only predict  edges of the form B ~ AT.
Th is  l imitat ionon over-the-top redict ion is due to the expensive bookkeepinginvolved in bidirectional parsing.
See the sect ion on bidirectionalpars ing for more details.130To avoid this propagation ofredundant edges, the parsermust ensure that no duplicate dges are introduced intothe chart.
'Picky does this simply by verifying every timean edge is added that the edge is not already in the chart.Although eliminating redundant edges prevents exces-sive inefficiency, bidirectional parsing may still performmore work than traditional left-to-right parsing.
In theprevious example, three edges are introduced into thechart to parse the NP ~ DET N edge.
A left-to-rightparser would only introduce two edges, one when thedeterminer is recognized, and another when the noun isrecognized.The benefit of bidirectional parsing can be seen whenprobabilistic prediction is introduced into the parser.Frequently, the syntactic structure of a constituent isnot determined by its left-corner word.
For instance,in the sequence V NP PP, the prepositional phrase PPcan modify either the noun phrase NP or the entire verbphrase V NP.
These two interpretations require differentVP rules to be predicted, but the decision about whichrule to use depends on more than just the verb.
The cor-rect rule may best be predicted by knowing the preposi-tion used in the PP.
Using probabilistic prediction, thedecision is made by pursuing the rule which has the high-est probability according to the prediction model.
Thisrule is then parsed bidirectionally.
If this rule is in factthe correct rule to analyze the constituent, then no otherpredictions will be made for that constituent, and therewill be no more edges produced than in left-to-right pars-ing.
Thus, the only case where bidirectional parsing isless efficient than left-to-right parsing is when the pre-diction model fails to capture the elements of context ofthe sentence which determine its correct interpretation.3.4.
The  Three  Phases  of  P i ckyCovered Left-Corner The first phase uses probabilis-tic prediction based ,'n the part-of-speech sequences fromthe input sentence to predict all grammar rules whichhave a non-zero probability of being dominated by thattrigram (based on the training corpus), i.e.P(A  ~ BSlaoala2) > 0 (6)where al is the part-of-speech of the left-corner word ofB.
In this phase, the only exception to the probabilis-tic prediction is that any rule which can immediatelydominate the preterminal category of any word in thesentence is also predicted, regardless of its probability.This type of prediction is referred to as exhaustive pre-diction.
All of the predicted rules are processed using astandard best-first agenda processing algorithm, wherethe highest scoring edge in the chart is advanced.Covered Bidirectional If an S spanning the entireword string is not recognized by the end of the firstphase, the covered bidirectional phase continues theparsing process.
Using the chart generated by the firstphase, rules are predicted not only by the trigram cen-tered at the left-corner word of the rule, but by thetrigram centered at the left-corner word of any of thechildren of that rule, i.e.79(A ~ aBSIboblb2 ) > O.
(7)where bl is the part-of-speech associated with the left-most word of constituent B.
This phase introduces in-complete theories into the chart which need to be ex-panded to the left and to the right, as described in thebidirectional parsing section above.Tree Complet ion If the bidirectional processing failsto produce a successful parse, then it is assumed thatthere is some part of the input sentence which is notcovered well by the training material.
In the final phase,exhaustive prediction is performed on all complete the-ories which were introduced in the previous phases butwhich are not predicted by the trigrams beneath them(i.e.
P(rule \[ trigram) = 0).In this phase, edges are only predicted by their left-corner word.
As mentioned previously, bidirectionalparsing can be inefficient when the prediction model isinaccurate.
Since all edges which the prediction modelassigns non-zero probability have already been predicted,the model can no longer provide any information forfuture predictions.
Thus, bidirectional parsing in thisphase is very likely to be inefficient.
Edges already inthe chart will be parsed bidirectionally, since they werepredicted by the model, but all new edges will be pre-dicted by the left-corner word only.Since it is already known that the prediction model willassign a zero probability to these rules, these predictionsare instead scored based on the number of words spannedby the subtree which predicted them.
Thus, this phaseprocesses longer theories by introducing rules which canadvance them.
Each new theory which is proposed bythe parsing process is exhaustively predicted for, usingthe length-based scoring model.The final phase is used only when a sentence is so faroutside of the scope of the training material that noneof the previous phases are able to process it.
This phaseof the algorithm exhibits the worst-case xponential be-havior that is found in chart parsers which do not usenode packing.
Since the probabilistic model is no longeruseful in this phase, the parser is forced to propose anenormous number of theories.
The expectation (or hope)is that one of the theories which spans most of the sen-131tence will be completed by this final process.
Dependingon the size of the grammar used, it may be unfeasibleto allow the parser to exhaust all possible predicts be-fore deciding an input is ungrammatical.
The questionof when the parser should give up is an empirical issuewhich will not be explored here.Post-processing: Part ia l  Parsing Once the finalphase has exhausted all predictions made by the gram-mar, or more likely, once the probability of all edgesin the chart falls below a certain threshold, Picky deter-mines the sentence to be ungrammatical.
However, sincethe chart produced by Picky contains all recognized con-stituents, orted by probability, the chart can be used toextract partial parses.
As implemented, Picky prints outthe most probable completed S constituent.4.
Results of ExperimentsThe Picky parser was tested on 3 sets of 100 sentenceswhich were held out from the rest of the corpus duringtraining.
The training corpus consisted of 982 sentenceswhich were parsed using the same grammar that Pickyused.
The training and test corpora re samples from theMIT's Voyager direction-finding system.
6 Our experi-ments explored the accuracy, efficiency, and robustnessof the "Picky algorithm.However, we do not anticipate a significant improvementin accuracy, since the two parsers use similar languagemodels.
On the other hand, "Picky should outperformPearl in terms of robustness and efficiency.4.1.
RobustnessSince our test sets did not contain many ungrammaticalsentences, it was difficult o analyze Ticky's robustness.It is undeniable that Picky will produce a fuller chartthan will "Pearl, making partial parsing of ungrammati-cal sentences possible.
We leave it to future experimentsto explore empirically the effectiveness of semantic in-terpretation using Picky's probabilistic well-formed sub-string table.One interesting example did occur in one test set.
Thesentence "How do I how do I get to MITT' is a ungram-matical but interpretable s ntence which begins with arestart.
Pearl would have generated no analysis for thelatter part of the sentence and the corresponding sectionsof the chart would be empty.
Using bidirectional prob-abilistic prediction, Picky produced a correct partial in-terpretation of the last 6 words of the sentence, "how doI get to MIT?"
One sentence does not make for conclu-sive evidence, but it represents he type of improvements6SpeciM thanks  to Victor Zue at MIT  for the use of the speechdata  f rom MIT 's  Voyager system.which are expected from the Picky algorithm.4.2.
AccuracyPhase No.
AccuracyI + II 238 97%III 62 60%Overall 300 89.3%Figure 1: Picky's parsing accuracy, categorized by thephase which the parser reached in processing the testsentences.As we expected, Picky's parsing accuracy compares fa-vorably to Pearl's performance.
As shown in Figure 1,"Picky parsed the test sentences with an 89.3% accuracyrate.
This is a slight improvement over "Pearl's 87.5%accuracy rate reported in \[10\].But note the accuracy results for phases I and II.
Thesephases include sentences which are parsed successfullyby the probabilistic prediction mechanism.
Almost 80%of the test sentences fall into this category, and 97% ofthese sentences are parsed correctly.
This result is verysignificant because it provides a reliable measure of theconfidence the parser has in it's interpretation.
If incor-rect interpretations are worse than no interpretation atall, a natural language system might consider only parseswhich are generated in phases I and II.
This would limitcoverage, but would allow the system to have a high de-gree of confidence in the parser output.4.3.
EfficiencyPhase ParseI 37II 41III 44Overall 38Predicts Completes57 5889 98315 430111 135Figure 2: Average number of edges generated by Picky,categorized by the phase which the parser reached inprocessing the test sentences.The effectiveness of the prediction model also leads toincreased efficiency.
Figure 2 shows the average numberof edges predicted and completed by sentences, againpartitioned by phase of parse completion.
Also includedin the table is the average number of constituents in the"correct" parse.A measure of the efficiency provided by the probabilisticprediction mechanism is the parser's prediction ratio, theratio of edges predicted to edges necessary for a correct132parse.
A perfect prediction ratio is 1:1, i.e.
every edgepredicted is used in the eventual parse.
However, sincethere is ambiguity in the input sentences, a 1:1 predictionratio is not likely to be achieved.
Picky's predictionratio is less than 3:1, and its ratio of predicted edgesto completed edges is nearly 1:1.
Thus, although theprediction ratio is not perfect, on average for every edgethat is predicted one completed constituent results.Note that the prediction ratio is much lower in phaseI (1.5:1) and phase II (2.2:1) than in phase III (7:1).This is due to the accuracy of the probabilistic predic-tion model used in the first two phases, and the deficien-cies of the heuristic model used in final phase.
Furtherefficiency can be gained either by limiting the amount ofsearch which is performed in phase III before a sentenceis deemed ungrammatical or by improving the heuristicprediction model.Since Picky has the power of a pure bottom-up arser,it would be useful to compare it's performance and effi-ciency to that of a probabilistic bottom-up arser.
How-ever, an implementation of a probabilistic bottom-upparser using the same grammar produces on averageover 1000 constituents for each sentence, generating over15,000 edges without generating a parse at all!
Thissupports our claim that exhaustive CKY-like parsing al-gorithms are not feasible when probabilistic models areapplied to them.5.
Conclus ionsOne of the goals of the development of the Picky algo-rithm is to demonstrate the need to model the parsingprocess as well as modelling language.
The exponentialbehavior of statistical methods applied to standard pars-ing algorithms limits the types of stochastic grammarswhich can feasibly be used in natural language under-standing systems.
As the statisticM models of naturallanguage become richer and more expensive to compute,it is vital that we have efficient probabilistic parsing algo-rithms which avoid spurious generation of constituentsand partial constituents, since each edge produced bythe parsing process must be evaluated by the statisticallanguage model.
Picky successfully employs probabilis-tic prediction to minimize the number of constituents owhich the language model must be applied, making com-plicated language models which use fine-grained statis-tics more feasible for natural anguage applications.2.
Chitrao, M. and Grishman, R. 1990.
Statistical Parsingof Messages.
In Proceedings of the June 1990 DARPASpeech and Natural Language Workshop.
Hidden Valley,Pennsylvania.3.
Church, K. 1988.
A Stochastic Parts Program and NounPhrase Parser for Unrestricted Text.
In Proceedings ofthe Second Conference on Applied Natural LanguageProcessing.
Austin, Texas.4.
Earley, J.
1970.
An Efficient Context-Free Parsing Algo-rithm.
Communications of the ACMVol.
13, No.
2, pp.94-102.5.
Gale, W. A. and Church, K. 1990.
Poor Estimates ofContext are Worse than None.
In Proceedings of theJune 1990 DARPA Speech and Natural Language Work-shop.
Hidden Valley, Pennsylvania.6.
Jelinek, F. 1985.
Self-organizing Language Modeling forSpeech Recognition.
IBM Report.7.
Kasami, T. 1965.
An Efficient Recognition and Syn-tax Algorithm for Context-Free Languages.
ScientificReport AFCRL-65-758, Air Force Cambridge ResearchLaboratory.
Bedford, Massachusetts.8.
Kay, M. 1980.
Algorithm Schemata nd Data Structuresin Syntactic Processing.
CSL-80-12, October 1980.9.
Kimball, J.
1973.
Principles of Surface Structure Parsingin Natural Language.
Cognition, 2.15-47.10.
Magerman, D. M. and Marcus, M. P. 1991.
Pearl: AProbabilistic Chart Parser.
In Proceedings of the Euro-pean ACL Conference, March 1991.
Berlin, Germany.11.
Moore, R. and Dowding, J.
1991.
Efficient Bottom-UpParsing.
In Proceedings of the February 1991 DARPASpeech and Natural Language Workshop.
Asilomar, Cal-ifornia.
Berlin, Germany.12.
Sharman, R. A., Jelinek, F., and Mercer, R. 1990.
Gen-erating a Grammar for Statistical Training.
In Proceed-ings of the June 1990 DARPA Speech and Natural Lan-guage Workshop.
Hidden Valley, Pennsylvania.13.
Seneff, Stephanie 1989.
TINA.
In Proceedings ofthe Au-gust 1989 International Workshop in Parsing Technolo-gies.
Pittsburgh, Pennsylvania.14.
Younger, D. H. 1967.
Recognition and Parsing ofContext-Free Languages in Time n 3.
Information andControl Vol.
10, No.
2, pp.
189-208.References1.
Bobrow, R. J.
1991.
Statistical Agenda Parsing.
In Pro-ceedings of the February 1991 DARPA Speech and Nat-ural Language Workshop.
Asilomar, California.133
