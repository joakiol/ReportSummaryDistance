Proceedings of the NAACL HLT 2012 Student Research Workshop, pages 60?65,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsA Weighting Scheme for Open Information ExtractionYuval MerhavIllinois Institute of TechnologyChicago, IL USAyuval@ir.iit.eduAbstractWe study1 the problem of extracting all pos-sible relations among named entities from un-structured text, a task known as Open Infor-mation Extraction (Open IE).
A state-of-the-art Open IE system consists of natural lan-guage processing tools to identify entities andextract sentences that relate such entities, fol-lowed by using text clustering to identify therelations among co-occurring entity pairs.
Inparticular, we study how the current weightingscheme used for Open IE affects the clusteringresults and propose a term weighting schemethat significantly improves on the state-of-the-art in the task of relation extraction both whenused in conjunction with the standard tf ?
idfscheme, and also when used as a pruning fil-ter.1 IntroductionThe extraction of structured information from text isa long-standing challenge in Natural Language Pro-cessing which has been re-invigorated with the ever-increasing availability of user-generated textual con-tent online.
The large-scale extraction of unknownrelations has been termed as Open Information Ex-traction (Open IE) (Banko et al, 2007) (also referredto as Open Relationship Extraction, Relation Extrac-tion, or Relation Discovery).
Many challenges existin developing an Open IE solution, such as recogniz-ing and disambiguating entities in a multi-documentsetting, and identifying all so-called relational terms1This thesis proposal has been accepted for publicationin (Merhav et al, 2012).in the sentences connecting pairs of entities.
Rela-tional terms are words (usually one or two) that de-scribe a relation between entities (for instance, termslike ?running mate?, ?opponent?, ?governor of?
arerelational terms).One approach for Open IE is based on cluster-ing of entity pairs to produce relations, as intro-duced by Hasegawa et al (Hasegawa et al, 2004).Their and follow-up works (e.g., (Mesquita et al,2010)) extract terms in a small window between twonamed entities to build the context vector of eachentity pair, and then apply a clustering algorithmto cluster together entity pairs that share the samerelation (e.g., Google?Youtube and Google?Motorola Mobility in a cluster about the ?ac-quired?
relation).
Contexts of entity pairs are repre-sented using the vector space model.
The state-of-the-art in clustering-based Open IE assigns weightsto the terms according to the standard tf ?idf scheme.Motivation.
Intuitively, the justification for us-ing idf is that a term appearing in many documents(i.e., many contexts in our setting) would not bea good discriminator (Robertson, 2004), and thusshould weigh proportionally less than other, morerare terms.
For the task of relation extraction how-ever, we are interested specifically in terms that de-scribe relations.
In our settings, a single documentis a context vector of one entity pair, generated fromall articles discussing this pair, which means that thefewer entity pairs a term appears in, the higher itsidf score would be.
Consequently, it is not necessar-ily the case that terms that are associated with highidf weights would be good relation discriminators.On the other hand, popular relational terms that ap-60ply to many entity pairs would have relatively loweridf weights.It is natural to expect that the relations extractedby an Open IE system are strongly correlated witha given context.
For instance, marriage is a relationbetween two persons and thus belongs to the domainPER?PER.
We exploit this observation to boost theweight of relational terms associated with marriage(e.g., ?wife?, ?spouse?, etc.)
in those entity pairswhere the domain is also PER?PER.
The more dom-inant a term in a given domain compared to otherdomains, the higher its boosting score would be.Our work resembles the work on selectional pref-erences (Resnik, 1996).
Selectional preferences aresemantic constraints on arguments (e.g.
a verb like?eat?
prefers as object edible things).2 Related WorkDifferent approaches for Open IE have been pro-posed in the literature, such as bootstrapping(e.g., (Zhu et al, 2009) (Bunescu and Mooney,2007)), self or distant supervision (e.g., (Bankoet al, 2007) (Mintz et al, 2009)) and rule based(e.g., (Fader et al, 2011)).
In this work we focuson unsupervised approaches.Fully unsupervised Open IE systems are mainlybased on clustering of entity pair contexts to pro-duce clusters of entity pairs that share the same re-lations, as introduced by Hasegawa et al (Hasegawaet al, 2004) (this is the system we use in this workas our baseline).
Hasegawa et al used word uni-grams weighted by tf ?idf to build the context vec-tors and applied Hierarchical Agglomerative Clus-tering (HAC) with complete linkage deployed on a1995 New York Times corpus.
Mesquita et al ex-tended this work by using other features such as partof speech patterns (Mesquita et al, 2010).
To re-duce noise in the feature space, a common problemwith text mining, known feature selection and rank-ing methods for clustering have been applied (Chenet al, 2005; Rosenfeld and Feldman, 2007).
Bothworks used the K-Means clustering algorithm withthe stability-based criterion to automatically esti-mate the number of clusters.This work extends all previous clustering worksby utilizing domain frequency as a novel weight-ing scheme for clustering entity pairs.
The idea ofdomain frequency was first proposed for predictingentities which are erroneously typed by NER sys-tems (Merhav et al, 2010).3 Data and EvaluationThis work was implemented on top of the SONEXsystem (Mesquita et al, 2010), deployed on theICWSM 2009 Spinn3r corpus (Burton et al, 2009),focusing on posts in English (25 million out of 44million in total), collected between August 1st, 2008and October 1st, 2008.
The system uses the Illi-nois Entity Tagger (Ratinov and Roth, 2009) and Or-thomatcher from the GATE framework2 for within-a-document co-reference resolution.Evaluating Open IE systems is a difficult prob-lem.
Mesquita et al evaluated SONEX by auto-matically matching a sample of the entity pairs theirsystem identified from the Spinn3r corpus against apublicly available curated database3.
Their approachgenerated two datasets: INTER and 10PERC.
IN-TER contains the intersection pairs only (i.e., in-tersection pairs are those from Spinn3r and Free-base that match both entity names and types ex-actly), while 10PERC contains 10% of the total pairsSONEX identified, including the intersection pairs.We extended these two datasets by adding more en-tity pairs and relations.
We call the resulting datasetsINTER (395 entity pairs and 20 different relations)and NOISY (contains INTER plus approximately30,000 entity pairs as compared to the 13,000 pairsin 10PERC ).We evaluate our system by reporting f-measurenumbers for our system running on INTER andNOISY against the ground truth, using similar set-tings used by (Hasegawa et al, 2004) and (Mesquitaet al, 2010).
These include word unigrams as fea-tures, HAC with average link (outperformed singleand complete link), and tf ?idf and cosine similarityas the baseline.4 Weighting SchemeIdentifying the relationship (if any) between entitiese1, e2 is done by analyzing the sentences that men-tion e1 and e2 together.
An entity pair is defined bytwo entities e1 and e2 together with the context in2http://gate.ac.uk/3http://www.freebase.com61which they co-occur.
For our purposes, the contextcan be any textual feature that allows the identifica-tion of the relationship for the given pair.
The con-texts of entity pairs are represented using the vec-tor space model with the common tf ?idf weightingscheme.
More precisely, for each term t in the con-text of an entity pair, tf is the frequency of the termin the context, whileidf = log(|D||d : t ?
d|),where |D| is the total number of entity pairs, and|d : t ?
d| is the number of entity pairs contain-ing term t. The standard cosine similarity is used tocompute the similarity between context vectors dur-ing clustering.4.1 Domain FrequencyWe start with a motivating example before divinginto the details about how we compute domain fre-quency.
We initially built our system with the tra-ditional tf ?
idf and were unsatisfied with the re-sults.
Consequently, we examined the data to finda better way to score terms and filter noise.
Forexample, we noticed that the pair Youtube[ORG] ?Google[ORG] (associated with the ?Acquired by?relation) was not clustered correctly.
In Table 1 welisted all the Unigram features we extracted for thepair from the entire collection sorted by their domainfrequency score for ORG?ORG (recall that these arethe intervening features between the pair for eachco-occurrence in the entire dataset).
For clarity theterms were not stemmed.Clearly, most terms are irrelevant which make itdifficult to cluster the pair correctly.
We listed inbold all terms that we think are useful.
Besides ?be-longs?, all these terms have high domain frequencyscores.
However, most of these terms do not havehigh idf scores.
Term frequencies within a pair arealso not helpful in many cases since many pairs arementioned only a few times in the text.
Next, wedefine the domain frequency score (Merhav et al,2010).Definition.
Let P be the set of entity pairs, let Tbe the set of all entity types, and let D = T ?
T bethe set of all possible relation domains.
The domainfrequency (df ) of a term t, appearing in the contextof some entity pair in P , in a given relation domaini ?
D, denoted dfi(t), is defined asdfi(t) =fi(t)?1?j?n fj(t),where fi(t) is the frequency with which term t ap-pears in the context of entity pairs of domain i ?D, and n is the number of domains in D. Whencomputing the df score for a given term, it is pre-ferred to consider each pair only once.
For example,?Google[ORG] acquired Youtube[ORG]?
would becounted only once (for ?acquired?
in the ORG?ORGdomain) even if this pair and context appear manytimes in the collection.
By doing so we eliminatethe problem of duplicates (common on the web).Unlike the idf score, which is a global measureof the discriminating power of a term, the df scoreis domain-specific.
Thus, intuitively, the df scorewould favour specific relational terms (e.g., ?wife?which is specific to personal relations) as opposedto generic ones (e.g., ?member of?
which applies toseveral domains).
To validate this hypothesis, wecomputed the df scores of several relational termsfound in the clusters the system produced on themain Spinn3r corpus.Figure 1 shows the relative df scores of 4 rela-tional terms (mayor, wife, CEO, and coach) whichillustrate well the strengths of the df score.
We cansee that for the majority of terms (Figure 1(a)?
(c)),there is a single domain for which the term has aclearly dominant df score: LOC?PER for mayor,PER?PER for wife, and ORG?PER for CEO.Dependency on NER Types.
Looking again atFigure 1, there is one case in which the df score doesnot seem to discriminate a reasonable domain.
Forcoach, the dominant domain is LOC?PER, whichcan be explained by the common use of the city (orstate) name as a proxy for a team as in the sentence?Syracuse football coach Greg Robinson?.
Note,however, that the problem in this case is the dif-ficulty for the NER to determine that ?Syracuse?refers to the university.
These are some examplesof correctly identified pairs in the coach relation butin which the NER types are misleading:?
LOC?PER domain: (England, Fabio Capello);(Croatia, Slaven Bilic); (Sunderland, RoyKeane).62Table 1: Unigram features for the pair Youtube[ORG] ?
Google[ORG] with idf and df (ORG?ORG) scoresTerm idf df (ORG?ORG) Term idf df (ORG?ORG)ubiquitious 11.6 1.00 blogs 6.4 0.14sale 5.9 0.80 services 5.9 0.13parent 6.8 0.78 instead 4.0 0.12uploader 10.5 0.66 free 5.0 0.12purchase 6.3 0.62 similar 5.7 0.12add 6.1 0.33 recently 4.2 0.12traffic 7.0 0.55 disappointing 8.2 0.12downloader 10.9 0.50 dominate 6.4 0.11dailymotion 9.5 0.50 hosted 5.6 0.10bought 5.2 0.49 hmmm 9.3 0.10buying 5.8 0.47 giant 5.4 < 0.1integrated 7.3 0.44 various 5.7 < 0.1partnership 6.7 0.42 revealed 5.2 < 0.1pipped 8.9 0.37 experiencing 7.7 < 0.1embedded 7.6 0.36 fifth 6.5 < 0.1add 6.1 0.33 implication 8.5 < 0.1acquired 5.6 0.33 owner 6.0 < 0.1channel 6.3 0.28 corporate 6.4 < 0.1web 5.8 0.26 comments 5.2 < 0.1video 4.9 0.24 according 4.5 < 0.1sellout 9,2 0.23 resources 6.9 < 0.1revenues 8.6 0.21 grounds 7.8 < 0.1account 6.0 0.18 poked 6.9 < 0.1evading 9.8 0.16 belongs 6.2 < 0.1eclipsed 7.8 0.16 authors 7.4 < 0.1company 4.7 0.15 hooked 7.1 < 0.1?
MISC?PER domain: (Titans, Jeff Fisher); (Jets,Eric Mangini); (Texans, Gary Kubiak).4.2 Using the df ScoreWe use the df score for two purposes in our work.First, for clustering, we compute the weights of theterms inside all vectors using the product tf ?idf ?df .Second, we also use the df score as a filtering tool,by removing terms from vectors whenever their dfscores lower than a threshold.
Going back to theYoutube[ORG] ?
Google[ORG] example in Table 1,we can see that minimum df filtering helps with re-moving many noisy terms.
We also use maximumidf filtering which helps with removing terms thathave high df scores only because they are rare andappear only within one domain (e.g., ubiquitious(misspelled in source) and uploader in this example).As we shall see in the experimental evaluation,even in the presence of incorrect type assignmentsmade by the NER tool, the use of df scores improvesthe accuracy significantly.
It is also worth mention-ing that computing the df scores can be done fairlyefficiently, and as soon as all entity pairs are ex-tracted.5 ResultsWe now report the results on INTER and NOISY.Our baseline run is similar to the systems pub-lished by Hasegawa et al (Hasegawa et al, 2004)and Mesquita et al (Mesquita et al, 2010); thatis HAC with average link using tf ?
idf and cosinesimilarity, and stemmed word unigrams (excludingstop words) as features extracted using a windowsize of five words between pair of entities.
Fig-ure 2 shows that by integrating domain frequency6300.20.40.60.81LOC-PERPER-LOCLOC-LOCOTHERDomainFreq.
(a) mayor.00.20.40.60.81PER-PERPER-MISCPER-LOCORG-PEROTHERDomainFreq.
(b) wife.00.20.40.60.81ORG-PERPER-ORGPER-PEROTHERDomainFreq.
(c) CEO.00.20.40.60.81LOC-PERORG-PERMISC-PERPER-PEROTHERDomainFreq.
(d) coach.Figure 1: Domain Frequency examples.
(df) we significantly outperformed this baseline onboth datasets (INTER: F-1 score of 0.87 comparedto 0.75; NOISY: F-1 score of 0.72 compared to0.65).
In addition, filtering terms by minimum dfand maximum idf thresholds improved the resultsfurther on INTER.
These results are promising sincea major challenge in text clustering is reducing thenoise in the data.We also see a substantial decrease of the resultson NOISY compared to INTER.
Such a decreaseis, of course, expected: NOISY contains not onlythousands more entity pairs than INTER, but alsohundreds (if not thousands) more relations as well,making the clustering task harder in practice.6 Conclusion and Future ResearchDirectionsWe utilized the Domain Frequency (df ) score as aterm-weighting score designed for identifying rela-tional terms for Open IE.
We believe that df canbe utilized in various of applications, with the ad-vantage that in practice, for many such applica-tions, the list of terms and scores can be used off-the-shelf with no further effort.
One such applica-tion is Named Entity Recognition (NER) ?
df helpsin identifying relational patterns that are associated0.40.50.60.70.80.90  0.01  0.02  0.03  0.04  0.05f-measureclustering thresholdINTER: tf*idf*df & pruningINTER: tf*idf*dfINTER: tf*idfNOISY: tf*idf*df & pruningNOISY: tf*idf*dfNOISY: tf*idfFigure 2: tf ?
idf Vs. tf ?
idf ?
df with and with-out minimum df and maximum idf pruning on INTERand NOISY.
All results consistently dropped for cluster-ing thresholds larger than 0.05.with a certain domain (e.g., PER?PER).
If the list ofwords and phrases associated with their df scores isgenerated using an external dataset annotated withentities, it can be applied to improve results in other,more difficult domains, where the performance ofthe NER is poor.It is also appealing that the df score is proba-bilistic, and as such, it is, for the most part, lan-guage independent.
Obviously, not all languages64have the same structure as English and some adjust-ments should be made.
For example, df exploitsthe fact that relational verbs are usually placed be-tween two entities in a sentence, which may not bealways the case in other languages (e.g., German).Investigating how df can be extended and utilized ina multi-lingual environment is an interesting futuredirection.7 AcknowledgementsThe author would like to thank Professor DenilsonBarbosa from the University of Alberta, ProfessorDavid Grossman and Gady Agam from Illinois In-stitute of Technology, and Professor Ophir Friederfrom Georgetown University.
All provided greathelp in forming the ideas that led to this work.ReferencesMichele Banko, Michael J. Cafarella, Stephen Soderland,Matthew Broadhead, and Oren Etzioni.
2007.
Openinformation extraction from the web.
In Manuela M.Veloso, editor, IJCAI, pages 2670?2676.Razvan C. Bunescu and Raymond J. Mooney.
2007.Learning to extract relations from the web using mini-mal supervision.
In ACL.K.
Burton, A. Java, and I. Soboroff.
2009.
The icwsm2009 spinn3r dataset.
In Proceedings of the AnnualConference on Weblogs and Social Media.Jinxiu Chen, Donghong Ji, Chew Lim Tan, and ZhengyuNiu.
2005.
Unsupervised feature selection for re-lation extraction.
In IJCNLP-05: The 2nd Interna-tional Joint Conference on Natural Language Process-ing.
Springer.Anthony Fader, Michael Schmitz, Robert Bart, StephenSoderland, and Oren Etzioni.
2011.
Identifying re-lations for open information extraction.
Manuscriptsubmitted for publication.
University of Washington.Takaaki Hasegawa, Satoshi Sekine, and Ralph Grishman.2004.
Discovering relations among named entitiesfrom large corpora.
In ACL ?04: Proceedings of the42nd Annual Meeting on Association for Computa-tional Linguistics, page 415, Morristown, NJ, USA.Association for Computational Linguistics.Yuval Merhav, Filipe Mesquita, Denilson Barbosa,Wai Gen Yee, and Ophir Frieder.
2010.
Incorporat-ing global information into named entity recognitionsystems using relational context.
In Proceedings ofthe 33rd international ACM SIGIR conference on Re-search and development in information retrieval, SI-GIR ?10, pages 883?884, New York, NY, USA.
ACM.Yuval Merhav, Filipe Mesquita, Denilson Barbosa,Wai Gen Yee, and Ophir Frieder.
2012.
Extract-ing information networks from the blogosphere.
ACMTransactions on the Web (TWEB).
Accepted 2012.Filipe Mesquita, Yuval Merhav, and Denilson Barbosa.2010.
Extracting information networks from the blo-gosphere: State-of-the-art and challenges.
In 4th Int?lAAAI Conference on Weblogs and Social Media?DataChallenge.Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-sky.
2009.
Distant supervision for relation extractionwithout labeled data.
In ACL-IJCNLP ?09: Proceed-ings of the Joint Conference of the 47th Annual Meet-ing of the ACL and the 4th International Joint Confer-ence on Natural Language Processing of the AFNLP:Volume 2, pages 1003?1011, Morristown, NJ, USA.Association for Computational Linguistics.Lev Ratinov and Dan Roth.
2009.
Design challengesand misconceptions in named entity recognition.
InCoNLL ?09: Proceedings of the Thirteenth Conferenceon Computational Natural Language Learning, pages147?155, Morristown, NJ, USA.
Association for Com-putational Linguistics.Philip Resnik.
1996.
Selectional constraints: aninformation-theoretic model and its computational re-alization.Stephen Robertson.
2004.
Understanding inverse doc-ument frequency: On theoretical arguments for idf.Journal of Documentation, 60:2004.Benjamin Rosenfeld and Ronen Feldman.
2007.
Cluster-ing for unsupervised relation identification.
In CIKM?07, pages 411?418, New York, NY, USA.
ACM.Jun Zhu, Zaiqing Nie, Xiaojiang Liu, Bo Zhang, and Ji-Rong Wen.
2009.
Statsnowball: a statistical approachto extracting entity relationships.
In Proceedings ofthe 18th international conference on World wide web,WWW ?09, pages 101?110, New York, NY, USA.ACM.65
