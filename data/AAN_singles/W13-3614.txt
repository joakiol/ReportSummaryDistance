Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 102?108,Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational LinguisticsMemory-based grammatical error correctionAntal van den BoschRadboud University NijmegenP.O.
Box 9103NL-6500 HD Nijmegen, The Netherlandsa.vandenbosch@let.ru.nlPeter BerckTilburg UniversityP.O.
Box 90153NL-5000 LE Tilburg, The Netherlandsp.j.berck@tilburguniversity.eduAbstractWe describe the ?TILB?
team entry forthe CONLL-2013 Shared Task.
Our sys-tem consists of five memory-based classi-fiers that generate correction suggestionsfor center positions in small text windowsof two words to the left and to the right.Trained on the Google Web 1T corpus, thefirst two classifiers determine the presenceof a determiner or a preposition betweenall words in a text.
The second pair of clas-sifiers determine which is the most likelycorrection of an occurring determiner orpreposition.
The fifth classifier is a generalword predictor which is used to suggestnoun and verb form corrections.
We re-port on the scores attained and errors cor-rected and missed.
We point out a num-ber of obvious improvements to boost thescores obtained by the system.1 IntroductionOur team entry, known under the abbreviation?TILB?
in the CONLL-2013 Shared Task, is a sim-plistic text and grammar correction system basedon five memory-based classifiers implementingeight different error correctors.
The goal of thesystem is to be lightweight: simple to set up andtrain, fast in execution.
It requires a preferablyvery large but unannotated corpus to train on, andclosed lists of words that contain categories of in-terest (in our case, determiners and prepositions).The error correctors make use of information froma lemmatizer and a noun and verb inflection mod-ule.
The amount of explicit grammatical infor-mation input in the system is purposely kept toa minimum, as accurate deep grammatical infor-mation cannot be assumed to be present in mostreal-world situations and languages.
The systemdescribed in this article takes plain text as inputand produces plain text as output.Memory-based classifiers have been applied tosimilar tasks before.
(Van den Bosch, 2006) de-scribes memory based classifiers used for con-fusible disambiguation, and (Stehouwer and Vanden Bosch, 2009) shows how agreement errors canbe detected.
In the 2012 shared task ?Helping OurOwn?
(Dale et al 2012) memory based classifierswere used to solve the problem of missing andincorrect determiners and prepositions (Van denBosch and Berck, 2012).The CONLL-2013 Shared Task context limitedthe grammatical error correction task to detectingand correcting five error types:ArtOrDet Missing, unnecessary or incorrect article or de-terminer;Prep Incorrect preposition used;Nn Wrong form of noun used (e.g.
singular insteadof plural);Vform Incorrect verb form used (e.g.
I have went);SVA Incorrect subject-verb agreement (e.g.
He have).The corrections made by the system are scoredby a program provided by the organizers (Ng,2012).
It takes a plain textfile as input (the outputgenerated by the system) and outputs a list withcorrectly rectified errors followed by precision, re-call and F-score.As training material we used two corpora.
TheGoogle Web 1T corpus (Brants and Franz, 2006)was used to train the classifiers for the ArtOrDetand Prep error categories.
The GigaWord Newspa-per text corpus1 was used to create the data for theclassifier for the noun and verb-related error cat-egories.
To make the classifiers more compatible1http://www.ldc.upenn.edu/102with each other, future versions of the system willall be trained on the same corpus.
We also usedtwo lists, one consisting of 64 prepositions andone consisting of 23 determiners, both extractedfrom the CONLL-2013 Shared Task training data.Using the Google corpus means that we restrictedourselves to a simple 5-gram context, which ob-viously places a limit on the context sensitivity ofour system; on the other hand, we were able tomake use of the entire Google Web 1T corpus.
Thecontext for the grammatical error detectors waskept similar to the other classifiers, also 5-grams.2 SystemOur system is based on five memory-based clas-sifiers that all run the IGTree classifier algorithm(Daelemans et al 1997), a decision-tree approx-imation of k-nearest neighbour classification im-plemented in the TiMBL software package.2 Thefirst two classifiers determine the presence of a de-terminer or a preposition between all words in atext in which the actual determiners and prepo-sitions are masked.
The second pair of classi-fiers determine which is the most likely correctiongiven a masked determiner or preposition.
Thefifth classifier is a general word predictor that isused for suggesting noun and verb form correc-tions.All classifiers take a windowed input of twowords to the left of the focus position, and twowords to the right.
The focus may either be a posi-tion between two words, or be on a word.
In caseof a position between two words, the task is to pre-dict whether the position should actually be filledby an determiner or a preposition.
When the fo-cus is on the word in question, the task is to decidewhether it should be deleted, or whether it shouldbe corrected.It is important to note that not just one classifi-cation is returned for a given context by the IGTreeclassifier, but a distribution of results and their re-spective occurrence counts.
The classifier matchesthe words in the context to examples in the tree ina fixed order, and returns the distribution storedat that point in the tree when an unknown wordis encountered.
This is analogous to the back-off mechanisms often used in other n-gram basedlanguage modeling systems.
When even the firstfeature fails to match, the complete class distribu-tion is returned.
The output from the classifiers2http://ilk.uvt.nl/timblis filtered by the error correctors for the correctanswers.
Filtering is done based on distributionsize, occurrence counts and ratios in occurrencecounts (in the remainder of the text, where we sayfrequency we mean occurrence count), and in thecase of the noun and verb-related error types, onpart-of-speech tags.The system corrects a text from left to right,starting with the first word and working its wayto the end.
Each error corrector is tried after theother, in the order specified below, until a correc-tion is suggested.
At this point, the correction isstored, and the system starts processing the nextword.
The other classifiers are not tried anymoreafter a correction has been suggested by one of theclassifiers.The first two classifiers, preposition?
and de-terminer?, are binary classifiers that determinewhether or not there should be a preposition or adeterminer, respectively, between two words to theleft and two words to the right:?
The preposition?
classifier is trained on all120,711,874 positive cases of contexts in theGoogle Web 1T corpus in which one of the 64known prepositions are found to occur in themiddle position of a 5-gram.
To enable theclassifier to answer negatively to other con-texts, roughly the same amount of negativecases of randomly selected contexts with nopreposition in the middle are added to forma training set of 238,046,975 cases.
We in-corporate the Google corpus token counts inour model.
We performed a validation exper-iment on a single 90%-10% split of the train-ing data; the classifier is able to make a cor-rect decision on 88.6% of the 10% heldoutcases.?
Analogously, the determiner?
classifiertakes all 86,253,841 positive cases of 5-grams with a determiner in the middle po-sition, and adds randomly selected negativecases to arrive at a training set of 169,874,942cases.
On a 90%?10% split, the classifiermakes the correct decision in 90.0% of the10% heldout cases.The second pair of classifiers perform the multi-label classification task of predicting which prepo-sition or determiner is most likely given a contextof two words to the left and to the right.
Again,103determiner?which determiner?no determinernoyesdeterminersuggestedreplacement/insertion ofdeterminerdifferent andconfidentaboutalternative?preposition?which preposition?no prepositionnoyesprepositionsuggestedreplacement/insertion ofprepositiondifferent andconfidentaboutalternative?which word?verbsuggestedreplacementof verbdifferent andconfidentaboutalternative?nounsuggestedreplacementof noundifferent andconfidentaboutalternative?actual word in textsuggested correctionabsence of wordpresence classifieridentity classifierFigure 1: System architecture.
Shaded rectangles are the five classifiers.these classifiers are trained on the entire GoogleWeb 1T corpus, including its token counts:?
The which preposition?
classifier is trainedon the aforementioned 120,711,874 cases ofany of the 64 prepositions occurring in themiddle of 5-grams.
The task of the classi-fier is to generate a class distribution of likelyprepositions given an input of the four wordssurrounding the preposition, with 64 possibleoutcomes.
In a 90%-10% split experiment onthe complete training set, this classifier labels63.3% of the 10% heldout cases correctly.?
The which determiner?
classifier, by anal-ogy, is trained on the 86,253,841 positivecases of 5-grams with a determiner in themiddle position, and generates class distribu-tions composed of the 23 possible class labels(the possible determiners).
On a 90%-10%split of the training set, the classifier predicts68.3% of all heldout cases correctly.The fifth classifier predicts the most likelyword(s) between a context of two words to the leftand two to the right.?
The general word predictor, which word?,for the grammatical error types, was trainedon 10 million lines of the GigaWord En-glish Newspaper corpus.
This amounts to66,675,151 5-grams.
It predicts the word inthe middle between the two context words onthe left and on the right.From the predictions of the five classifiers thefollowing eight error correctors are derived.
Thereis no one-to-one correspondence between classi-fier and corrector.
The ArtOrDet and Prep errorcategories are handled by three separate errors cor-rectors each that handle replacement, deletion, andinsertion errors.
The three error types Nn, Vformand SVA are handled by just two correctors:1 missing preposition (Prep)2 replace preposition (Prep)3 unnecessary preposition (Prep)4 missing determiner (ArtOrDet)5 replace determiner (ArtOrDet)6 unnecessary determiner (ArtOrDet)7 noun form (Nn, SVA)8 verb form (Vform, SVA)For the latter two error correctors, 7 and 8,we make additional use of a lemmatizer3 and asingular-plural determiner and generator4 for nounform errors, and a verb tense determiner and gen-erator5 for verb form and SVA errors.The algorithms for the six preposition and de-terminer correctors will be explained in the rest ofthis section.
The algorithms use the same logic,the difference is in the different lists and parame-ters used for each error type.The algorithm for missing preposition (or deter-miner) is as follows.1 next word is not a preposition2 run positive-negative classifier P+?3 if the classification = + (i.e.
we expect a preposition),and freq(+):freq(?)
> MP PNR4 run the which preposition?
classifier5 if length distribution <= MP DS take answer as missingprepositionThe parameters (MP PNR and MP DS in theabove algorithm) are used to control the certaintywe expect from the classifier.
Their values weredetermined in our submission to the 2012 ?Helping3http://www-nlp.stanford.edu/software/corenlp.shtml4https://pypi.python.org/pypi/inflect5http://nodebox.net/code/index.php/Linguistics104Our Own?
shared task (Dale et al 2012), whichfocused on determiner and preposition errors (Vanden Bosch and Berck, 2012).
Similar classifierswere used in this year?s system, and the same pa-rameters were used this time.In step 3 above, we check the ratio between thefrequency of the positive answer and the negativeanswer.
If the ratio is larger than the parameterMP PNR (set to 20) we interpret this as being cer-tain.
In step 5, we prefer a small, sharp distributionof answers.
A large distribution indicates the clas-sifier not finding any matches in the context andreturning a large distribution with all possible an-swers.
In that case, the majority class tends to bethe majority class of the complete training data,and not the specific answer(s) in the context weare looking at.
To avoid this we only suggest ananswer when the distribution is equal to or smallerthan a certain preset threshold, MP DS, which wasset to 20 for this task.The algorithm for replacing propositions (or de-terminers) proceeds as follows:1 word in focus is a preposition p2 run which preposition?, classification is palt3 if freq(palt) > RP F and4 if word is in distribution and freq(palt):freq(p) > RP R,take palt as a correctionThis algorithm shows another parameter,namely a check on frequency (occurrence count).In order to be generated as a correction, thealternate answer must have a frequency higherthan RP F, set to 5 in our system, and the ratiobetween its frequency and that of the prepositionin the distribution that is the same as in the textmust be larger than RP R. This parameter was setto 20.The algorithm for unnecessary preposition (ordeterminer) works as follows:1 word in focus is a preposition2 run positive-negative classifier P+?3 if classification = ?
and freq(?
):freq(+) > UP NPR4 the preposition is unnecessaryThe next two algorithms show the Nn and Vfcorrectors.
The parameters these correctors usehave not been extensively tweaked, but rather usethe same settings as used in the preposition anddeterminer correctors.The first list shows the algorithm for the nountype error.
This error corrector also makes use of anoun inflection module to turn singular nouns intoplural and vice versa.
The algorithm first looks forthe alternative version of the noun in the distribu-tion returned by the classifier given the context.
Ifit is found, and if it is much more frequent in thedistribution than the noun form used in the text, anoun form error may have been found.
The alter-native form found in the distribution is returned asthe correction.1 word in focus w is a noun2 check singular or plural, determine alternate version walt3 run the which-word?
classifier, resulting in distributionD4 check if w is in D5 check if walt is in D6 if freq(w) in D < 10 and walt is in D use walt as correc-tionFinally, the verb form error corrector makes useof a verb-tense determiner and generator, and alemmatizer.
The alternative verb forms are gen-erated from the lemma of the verb and the tense ofthe verb.
To prevent the system changing, for ex-ample, give to gave, the generated alternatives arekept in the same tense as the word in the text.
Thisdoes, however, mean that it will not be able to cor-rect verb tense errors (I see him yesterday versus Isaw him yesterday).1 word in focus is a verb v2 determine the lemma of v3 determine the tense of v4 generate alternatives in same tense as word, valt5 run which-word?
predictor, resulting in distribution D6 check if v is in D7 check which valt are in D, take highest frequencyfreq(valt)8 if freq(valt):freq(v) > 10: take valt as a correction of v3 ResultsTable 1 lists the precision, recall and F-score of oursystem on the test data.
The test data (Tetreault,2013) consisted of 300 paragraphs of English textwritten by non-native speakers.
The system?s out-put is processed by a scorer supplied by the orga-nizers (Ng, 2012).
For each sentence, it reports thenumber of correct, proposed and gold edits, and arunning total of the system?s precision, recall andF-score.The system suggested a total of 1,902 edits.
Ofthese, 118 were correct.
The total number of cor-rect edits was 1,643.
To explain the score obtainedby the system, we inspect the kind of errors whichit was subjected to, and what kind of errors it didcorrect and which it missed.105Precision 6.20%Recall 7.18%F1 6.66%Table 1: Summary ScoreWe see a number of errors which are difficult tocorrect because they depend on understanding thesentence.
Take the following sentence for exam-ple:Surveillance technology such as RFID can beoperated twenty-four hours with the absence ofoperators to track done every detail about humanactivities .The gold-edit for this sentence is changing with(word 11) to without.
This edit may be question-able, but questionability aside, it is based on a un-derstanding of what is being talked about in thetext.
Correcting these kinds of errors falls outsidethe scope of the system at the moment.Multi-word edits are also a problem.
In All pas-sengers and pilots were died, the gold-edit is tochange were died to died.
In The readers are justsmiling when they flip the page because it nevercomes to their mind that one day it might cometrue, the gold-edit is to change are just smiling tojust smile.
These kind of corrections are missedby our system at the moment due to the rigid one-word, left-to-right checking of the sentence.Inserting more than one word is also problem-atic for our system at the moment.
Take the fol-lowing sentence.Firstly , security systems are improved in manyareas such as school campus or at the workplace .The gold-edit is to insert on the before school.
Apotential solution for this problem is to take mul-tiple passes over the sentence, first inserting on,followed by the in a later pass.Nevertheless, the system made a number of cor-rect edits as well.
The next subsections list exam-ples of each error type and a correction, where ap-plicable.Missing determinerIn this sentence, the missing determiner beforesmart was corrected by the system.In spite of that, the smart phone is still a device .
.
.In the following sentence however, a determineris inserted where it is not needed, before RFID.. .
.
the idea of using the RFID to track people .
.
.To illustrate the reasoning of our system, the de-terminer?
classifier thinks that it is more than 13times more likely to find a determiner between ofusing and RFID to than not.
Of the possible de-terminers, the determiner the has the highest fre-quency with 38,809 occurrences.Replace DeterminerHere is an example of a determiner which is cor-rected:.
.
.
signal and also a?the risk that their phone .
.
.It also happens that the right determiner is incor-rectly changed into another determiner, as shownin the next example.. .
.
this kind of tragedy to happen on any?the family.The determiner the had a frequency of morethan 6 million in the distribution, compared to only68,612 for any.Unnecessary DeterminerThe system did not detect any unnecessary deter-miners.
It missed, for example, removing the de-terminer the in this setentence:.
.
.
technology available for the Man ?s life .Replace PrepositionIn this example, a preposition was corrected.. .
.
to be put into?under close surveillance .
.
.But in the following sentence.
.
.
remain functional for?after a long period of .
.
.the preposition for is unfortunately changed to af-ter, which in this context is more common.Unnecessary PrepositionThe following is an example of a correct removalof a preposition:106. .
.
, many of things that are regarded .
.
.Prepositions were also incorrectly removed, asshown in the following example.
Here.
.
.
that can be out of our imaginations .
.
.of is deemed unnecessary.Missing PrepositionIn this example, the missing preposition on wasinserted after live.. .
.
find another planet to live on , the earth is .
.
.In the sentence.
.
.
especially in the elderly and the children .
.
.the system inserts the preposition in between es-pecially and and, which in this case was incorrect.Noun formThe next example shows a noun form correction.. .
.
brought harmful side effect?effects to human bodyThis can, of course, also go wrong:Since RFID tags?tag attached to the product .
.
.Here the singular form of the noun was deemedcorrect.Verb formFinally, an example of a verb form correction:People needs?need a safe environment to live .
.
.And the final example, an incorrect replacementof been to was.. .
.
that has currently been?was implemented4 DiscussionWe have described a memory-based grammarchecker specialized in correcting the five types oferrors in the CONLL-2013 Shared Task.
The sys-tem is built on five classifiers specialized in theerror categories relevant for the task.
They aretrained to find errors in a small local context oftwo words to the left and two words to the right.The system scans each word in each sentence inthe test data and calls the relevant classifier(s) todetermine if a word needs to be replaced, deleted,or inserted.
The classifiers take word tokens asinput; no deep grammatical information was sup-plied to them.
Even though the training data sup-plied for the task contained syntax trees, they werenot used in creating our system.
On the other hand,the part-of-speech information in the training datawas used to create the lists of prepositions and de-terminers.
Furthermore, a part-of-speech taggerwas used to determine if the noun or verb formerror corrector was to be applied.There are several obvious shortcomings to thisapproach.
The most obvious one is that each cor-rector is applied to single words, using only asmall local context of two words to the left andright.
This may work fine for missing preposi-tions and determiners, but for spotting grammat-ical errors like subject-verb agreement this limitedcontextual scope is insufficient.
It also means thatwe are only able to correct ?single words to singlewords?.
That is, it is not possible to substitute twowords for one, and vice versa.
One avenue thatcould be explored is larger contexts.
In addition,the classifiers are not limited to words, and con-texts with other (contextual) information could betried as well.Secondly, the correctors are applied in a strictorder one after the other.
This should not be abig problem as the classifiers are called separatelyfor their particular part-of-speech category (deter-miner, preposition, verb, or noun).
On the otherhand, this puts a lot of weight on the part of speechtagger.
Ambiguous or wrong tags could cause thewrong corrector to be tried and even applied, andcould miss a potential correct correction.Furthermore, the corrected words are not fedback into the system.
This means that the contextafter an error still contains that error.
This maycause the classifiers to mismatch and miss the nexterror.
It should be noted that the small context oftwo words to the left and right probably helps toalleviate this problem.
However, making the sys-tem insert corrections and backtracking a step (ormore) could help towards solving the problem ofmulti-word corrections.Finally, not all correctors found errors.
Thismay of course depend on the test data, but it seemsunlikely that the data contained no ?missing prepo-sition?
errors.
There is a potential gain in tuning107the parameters controlling the error correctors.4.1 UpdateThe organizers of the shared task updated the m2-scorer used to calculate the results, resulting inslightly better scores.
Table 2 shows the revisedscore of our system, with the old score betweenparentheses.Precision 7.60% (6.20%)Recall 9.29% (7.18%)F1 8.36% (6.66%)Table 2: Revised Summary ScoreAnd to conclude, we continued working on thesystem and tweaked some of the parameters con-trolling the preposition and determiner checkers.By allowing the correctors to be applied more of-ten, we see an increase in the number of proposedand correct edits (2,533 and 178 respectively).
Thedownside to this is of course that the number offalse positives increases, which decreases the pre-cision of the system.The tweaked score is shown in table 3, with therevised score between parentheses.Precision 7.03% (7.60%)Recall 10.83% (9.29%)F1 8.52% (8.36%)Table 3: Tweaked Summary ScoreThese improved scores give us good hope thatthe highest scores have not been reached yet.AcknowledgementsThe authors thank Ko van der Sloot for his sus-tained improvements of the TiMBL software.
Thiswork is rooted in earlier joint work funded througha grant from the Netherlands Organization for Sci-entific Research (NWO) for the Vici project Im-plicit Linguistics.ReferencesT.
Brants and A. Franz.
2006.
LDC2006T13: Web 1T5-gram Version 1.W.
Daelemans, A.
Van den Bosch, and A. Weijters.1997.
IGTree: using trees for compression and clas-sification in lazy learning algorithms.
Artificial In-telligence Review, 11:407?423.R.
Dale, I. Anisimoff, and G. Narroway.
2012.
HOO2012: A report on the preposition and determinererror correction shared task.
In Proceedings ofthe Seventh Workshop on Innovative Use of NLPfor Building Educational Applications, Montreal,Canada.Daniel Dahlmeier & Hwee Tou Ng.
2012.
Better eval-uation for grammatical error correction.
In Proceed-ings of the 2012 Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 568 ?
572.H.
Stehouwer and A.
Van den Bosch.
2009.
Puttingthe t where it belongs: Solving a confusion prob-lem in Dutch.
In S. Verberne, H. van Halteren, andP.-A.
Coppen, editors, Computational Linguistics inthe Netherlands 2007: Selected Papers from the 18thCLIN Meeting, pages 21?36, Nijmegen, The Nether-lands.Hwee Tou Ng & Siew Mei Wu & Yuanbin Wu & Chris-tian Hadiwinoto & Joel Tetreault.
2013.
The conll-2013 shared task on grammatical error correction.
InProceedings of the Seventeenth Conference on Com-putational Natural Language Learning.A.
Van den Bosch and P. Berck.
2012.
Memory-basedtext correction for preposition and determiner errors.In Proceedings of the 7th Workshop on the Innova-tive Use of NLP for Building Educational Applica-tions, pages 289?294, New Brunswick, NJ.
ACL.A.
Van den Bosch.
2006.
All-word prediction as theultimate confusible disambiguation.
In Proceedingsof the HLT-NAACL Workshop on Computationallyhard problems and joint inference in speech and lan-guage processing, New York, NY.108
