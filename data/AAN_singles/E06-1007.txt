Automatic Detection of Nonreferential It in Spoken Multi-Party DialogChristoph Mu?llerEML Research gGmbHVilla BoschSchlo?-Wolfsbrunnenweg 3369118 Heidelberg, Germanychristoph.mueller@eml-research.deAbstractWe present an implemented machinelearning system for the automatic detec-tion of nonreferential it in spoken dialog.The system builds on shallow features ex-tracted from dialog transcripts.
Our exper-iments indicate a level of performance thatmakes the system usable as a preprocess-ing filter for a coreference resolution sys-tem.
We also report results of an annota-tion study dealing with the classification ofit by naive subjects.1 IntroductionThis paper describes an implemented system forthe detection of nonreferential it in spoken multi-party dialog.
The system has been developed onthe basis of meeting transcriptions from the ICSIMeeting Corpus (Janin et al, 2003), and it is in-tended as a preprocessing component for a coref-erence resolution system in the DIANA-Summ di-alog summarization project.
Consider the follow-ing utterance:MN059: Yeah.
Yeah.
Yeah.
I?m sure I could learn a lotabout um, yeah, just how to - how to come up withthese structures, cuz it?s - it?s very easy to whip upsomething quickly, but it maybe then makes sense to -to me, but not to anybody else, and - and if we want toshare and integrate things, they must - well, they mustbe well designed really.
(Bed017)In this example, only one of the three instances ofit is a referential pronoun: The first it appears inthe reparandum part of a speech repair (Heeman& Allen, 1999).
It is replaced by a subsequent al-teration and is thus not part of the final utterance.The second it is the subject of an extrapositionconstruction and serves as the placeholder for thepostposed infinitive phrase to whip up somethingquickly.
Only the third it is a referential pronounwhich anaphorically refers to something.The task of the system described in the follow-ing is to identify and filter out nonreferential in-stances of it, like the first and second one in theexample.
By preventing these instances from trig-gering the search for an antecedent, the precisionof a coreference resolution system is improved.Up to the present, coreference resolution hasmostly been done on written text.
In this domain,the detection of nonreferential it has by now be-come a standard preprocessing step (e.g.
Ng &Cardie (2002)).
In the few works that exist oncoreference resolution in spoken language, on theother hand, the problem could be ignored, becausealmost none of these aimed at developing a sys-tem that could handle unrestricted input.
Eck-ert & Strube (2000) focus on an unimplementedalgorithm for determining the type of antecedent(mostly NP vs. non-NP), given an anaphoricalpronoun or demonstrative.
The system of Byron(2002) is implemented, but deals mainly with howreferents for already identified discourse-deicticanaphors can be created.
Finally, Strube & Mu?ller(2003) describe an implemented system for re-solving 3rd person pronouns in spoken dialog, butthey also exclude nonreferential it from consider-ation.
In contrast, the present work is part of aproject to develop a coreference resolution systemthat, in its final implementation, can handle unre-stricted multi-party dialog.
In such a system, noa priori knowledge is available about whether aninstance of it is referential or not.The remainder of this paper is structured as fol-lows: Section 2 describes the current state of theart for the detection of nonreferential it in writ-ten text.
Section 3 describes our corpus of tran-scribed spoken dialog.
It also reports on the anno-tation that we performed in order to collect train-ing and test data for our machine learning experi-ments.
The annotation also offered interesting in-sights into how reliably humans can identify non-referential it in spoken language, a question that,49to our knowledge, has not been adressed before.Section 4 describes the setup and results of ourmachine learning experiments, Section 5 containsconclusion and future work.2 Detecting Nonreferential It In TextNonreferential it is a rather frequent phenomenonin written text, though it still only constitutes a mi-nority of all instances of it.
Evans (2001) reportsthat his corpus of approx.
370.000 words from theSUSANNE corpus and the BNC contains 3.171examples of it, approx.
29% of which are nonref-erential.
Dimitrov et al (2002) work on the ACEcorpus and give the following figures: the news-paper part of the corpus (ca.
61.000 words) con-tains 381 instances of it, with 20.7% being nonref-erential, and the news wire part (ca.
66.000 words)contains 425 instances of it, 16.5% of which arenonreferential.
Boyd et al (2005) use a 350.000word corpus from a variety of genres.
They count2.337 instances of it, 646 of which (28%) are non-referential.
Finally, Clemente et al (2004) reportthat in the GENIA corpus of medical abstracts thepercentage of nonreferential it is as high as 44%of all instances of it.
This may be due to the factthat abstracts tend to contain more stereotypicalformulations.It is worth noting here that in all of the abovestudies the referential-nonreferential decision im-plicitly seems to have been made by the author(s).To our knowledge, no study provides figures re-garding the reliability of this classification.Paice & Husk (1987) is the first corpus-basedstudy on the detection of nonreferential it in writ-ten text.
From examples drawn from a part ofthe LOB corpus (technical section), Paice & Husk(1987) create rather complex pattern-based rules(like SUBJECT VERB it STATUS to TASK),and apply them to an unseen part of the corpus.They report a final success rate of 92.2% on thetest corpus.
Nowadays, most current coreferenceresolution systems for written text include somemeans for the detection of nonreferential it.
How-ever, evaluation figures for this task are not alwaysgiven.
As the detection of nonreferential it is sup-posed to be a filtering condition (as opposed toa selection condition), high precision is normallyconsidered to be more important than high recall.A false negative, i.e.
a nonreferential it that is notdetected, can still be filtered out later when reso-lution fails, while a false positive, i.e.
a referen-tial it that is wrongly removed, is simply lost andwill necessarily harm overall recall.
Another pointworth mentioning is that mere classification accu-racy (percent correct) is not an appropriate eval-uation measure for the detection of nonreferentialit.
Accuracy will always be biased in favor of pre-dicting the majority class referential which, as theabove figures show, can amount to over 80%.The majority of works on detecting nonreferen-tial it in written text uses some variant of the partlysyntactic and partly lexical tests described by Lap-pin & Leass (1994), the first work about computa-tional pronoun resolution to address the potentialbenefit of detecting nonreferential it.
Lappin &Leass (1994) mainly supply a short list of modaladjectives and cognitive verbs, as well as sevensyntactic patterns like It is Cogv-ed that S. Likemany works that treat the detection of nonrefer-ential it only as one of several steps of the coref-erence resolution process, Lappin & Leass (1994)do not give any figures about the performance ofthis filtering method.Dimitrov et al (2002) modify and extend theapproach of Lappin & Leass (1994) in several re-spects.
They extend the list of modal adjectivesto 86 (original: 15), and that of cognitive verbs to22 (original: seven).
They also increase the cov-erage of the syntactic patterns, mainly by allowingfor optional adverbs at certain positions.
Dimitrovet al (2002) report performance figures for eachof their syntactic patterns individually.
The firstthing to note is that 41.3% of the instances of non-referential it in their corpus do not comply withany of the patterns they use, so even if each pat-tern worked perfectly, the maximum recall to bereached with this method would be 58.7%.
The ac-tual recall is 37.7%.
Dimitrov et al (2002) do notgive any precision figures.
One interesting detailis that the pattern involving the passive cognitiveverb construction accounts for only three instancesin the entire corpus, of which only one is found.Evans (2001) employs memory-based machinelearning.
He represents instances of it as vectors of35 features.
These features encode, among otherthings, information about the parts of speech andlemmata of words in the context of it (obtained au-tomatically).
Other features encode the presenceor absence of, resp.
the distance to, certain ele-ment sequences indicative of pleonastic it, such ascomplementizers or present participles.
Some fea-tures explicitly reference structural properties of50the text, like position of the it in its sentence, andposition of the sentence in its paragraph.
Sentenceboundaries are also used to limit the search spacefor certain distance features.
Evans (2001) reportsa precision of 73.38% and a recall of 69.25%.Clemente et al (2004) work on the GENIA cor-pus of medical abstracts.
They assume perfect pre-processing by using the manually assigned POStags from the corpus.
The features are very similarto those used by Evans (2001).
Using an SVMma-chine learning approach, Clemente et al (2004)obtain an accuracy of 95.5% (majority base line:approx.
56%).
They do not report any precision orrecall figures.
Clemente et al (2004) also performan analysis of the relative importance of features invarious settings.
It turns out that features pertain-ing to the distance or number of complementizersfollowing the it are consistently among the mostimportant.Finally, Boyd et al (2005) also use a machinelearning approach.
They use 25 features, most ofwhich represent syntactic patterns like it VERBADJ that.
These features are numeric, having astheir value the distance from a given instance ofit to the end of the match, if any.
Pattern match-ing is limited to sentences, sentence breaks beingidentified by punctuation.
Other features encodethe (simplified) POS tags that surround a given in-stance of it.
Like in the system of Clemente et al(2004), all POS tag information is obtained fromthe corpus, so no (error-prone) automatic taggingis performed.
Boyd et al (2005) obtain a precisionof 82% and a recall of 71% using a memory-basedmachine learning approach, and a similar preci-sion but much lower recall (42%) using a decisiontree classifier.In summary, the best approaches for detectingnonreferential it in written text already work rea-sonably well, yielding an F-measure of over 70%(Evans, 2001; Boyd et al, 2005).
This can at leastpartly be explained by the fact that many instancesare drawn from texts coming from rather stereo-typical domains, like e.g.
news wire text or scien-tific abstracts.
Also, some make the rather unreal-istic assumption of perfect POS information, andeven those who do not make this assumption takeadvantage of the fact that automatic POS taggingis generally very good for these types of text.
Thisis especially true in the case of complementizers(like that) which have been shown to be highly in-dicative of extraposition constructions.
Structuralproperties of the context of it, including sentenceboundaries and position within sentence or para-graph, are also used frequently, either as numeri-cal features in their own right, or as means to limitthe search space for pattern matching.3 Nonreferential It in Spoken DialogSpontaneous speech differs considerably fromwritten text in at least two respects that are rele-vant for the task described in this paper: it is lessstructured and more noisy than written text, and itcontains significantly more instances of it, includ-ing some types of nonreferential it not found inwritten text.3.1 The ICSI Meeting CorpusThe ICSI Meeting Corpus (Janin et al, 2003) isa collection of 75 manually transcribed group dis-cussions of about one hour each, involving 3 to 13speakers.
It features a semiautomatically gener-ated segmentation in which the corpus developerstried to track the flow of the dialog by insertingsegment starts approximately whenever a personstarted talking.
Each of the resulting segments isassociated with a single speaker and contains startand end time information.
The transcription con-tains manually added punctuation, and it also ex-plicitly records disfluencies and speech repairs bymarking both interruption points and word frag-ments (Heeman & Allen, 1999).
Consider the fol-lowing example:ME010: Yeah.
Yeah.
No, no.
There was a whole co- Therewas a little contract signed.
It was - Yeah.
(Bed017)Note, however, that the extent of the reparandum(i.e.
the words that are replaced by followingwords) is not part of the transcription.3.2 Annotation of ItWe performed an annotation with two external an-notators.
We chose annotators outside the projectin order to exclude the possibility that our own pre-conceived ideas influence the classification.
Thepurpose of the annotation was twofold: Primar-ily, we wanted to collect training and test data forour machine learning experiments.
At the sametime, however, we wanted to investigate how re-liably this kind of annotation could be done.
Theannotators were asked to label instances of it infive ICSI Meeting Corpus dialogs1 as belonging1Bed017, Bmr001, Bns003, Bro004, and Bro00551to one of the classes normal, vague, discarded,extrapos it, prop-it, or other.2 The idea behindusing this five-fold classification (as opposed to abinary one) was that we wanted to be able to in-vestigate the inter-annotator reliability for each ofthe sub-types individually (cf.
below).
The firsttwo classes are sub-types of referential it: Normalapplies to the normal, anaphoric use of it.
Vagueit (Eckert & Strube, 2000) is a form of it whichis frequent in spoken language, but rare in writtentext.
It covers instances of it which are indeed ref-erential, but whose referent is not an identifiablelinguistic string in the context of the pronoun.
Afrequent (but not the only) type of vague it is theone referring to the current discourse topic, like inthe following example:ME011: [...] [M]y vision of it is you know each of uswill have our little P D A in front of us Pause and sothe acoustics - uh you might want to try to match theacoustics.
(Bmr001)Note that we treat vague it as referential here eventhough, in the context of a coreference resolutionpreprocessing filter, it would make sense to treatit as nonreferential since it does not have an an-tecedent that it can be linked to.
However, we fol-low Evans (2001) in assuming that the informationthat is required to classify an instance of it as amention of the discourse topic is far beyond the lo-cal information that can reasonably be representedfor an instance of it.The classes discarded, extrapos it and prop-it are sub-types of nonreferential it.
The first twotypes have already been shown in the example inSection 1.
The class prop-it3 was included tocover cases like the following:FE004: So it seems like a lot of - some of the issues are thesame.
[...] (Bed017)The annotators received instructions including de-scriptions and examples for all categories, and adecision tree diagram.
The diagram told them e.g.to use wh-question formation as a test to distin-guish extrapos it and prop-it on the one handfrom normal and vague on the other.
The crite-rion for distinguishing between the latter two phe-nomena was to use normal if an antecedent couldbe identified, and vague otherwise.
For normal2The actual tag set was larger, including categories likeidiom which, however, the annotators turned out to use ex-tremely rarely only.
These values are therefore conflated inthe category other in the following.3Quirk et al (1991)pronouns, the annotators were also asked to indi-cate the antecedent.
The annotators were also toldto tag as extrapos it only those cases in whichan extraposed element (to-infinitive, ing-form orthat-clause with or without complementizer) wasavailable, and to use prop-it otherwise.
The an-notators individually performed the annotation ofthe five dialogs.
The results of this initial anno-tation were analysed and problems and ambigui-ties in the annotation scheme were identified andcorrected.
The annotators then individually per-formed the actual annotation again.
The resultsreported in the following are from this second an-notation.We then examined the inter-annotator reliabilityof the annotation by calculating the ?
score (Car-letta, 1996).
The figures are given in Table 1.
Thecategory other contains all cases in which one ofthe minor categories was selected.
Each table cellcontains the percentage agreement and the ?
valuefor the respective category.
The final column con-tains the overall ?
for the entire annotation.The table clearly shows that the classificationof it in spoken dialog appears to be by no meanstrivial: With one exception, ?
for the categorynormal is below .67, the threshold which is nor-mally regarded as allowing tentative conclusions(Krippendorff, 1980).
The ?
for the nonreferen-tial sub-categories extrapos it and prop-it is alsovery variable, the figures for the former being onaverage slightly better than those for the latter,but still mostly below that threshold.
In view ofthese results, it would be interesting to see simi-lar annotation experiments on written texts.
How-ever, a study of the types of confusions that oc-cur showed that quite a few of the disagreementsarise from confusions of sub-categories belongingto the same super-category, i.e.
referential resp.nonreferential.
That means that a decision on thelevel of granularity that is needed for the currentwork can be done more reliably.The data used in the machine learning experi-ments described in Section 4 is a gold standardvariant that the annotators agreed upon after theannotation was complete.
The distribution of thefive classes in the gold standard data is as follows:normal: 588, vague: 48, discarded: 222, extra-pos it: 71, and prop-it: 88.52normal vague discarded extrapos it prop-it other ?Bed017 81.8% / .65 36.4% / .33 94.7% / .94 30.8% / .27 63.8% / .54 44.4% / .42 .62Bmr001 88.5% / .69 23.5% / .21 93.6% / .92 50.0% / .48 40.0% / .33 0.0% / -.01 .63Bns003 81.9% / .59 22.2% / .18 80.5% / .75 58.8% / .55 27.6% / .21 33.3% / .32 .55Bro004 84.0% / .65 0.0% / -.05 89.9% / .86 75.9% / .75 62.5% / .59 0.0% / -.01 .65Bro005 78.6% / .57 0.0% / -.03 88.0% / .84 60.0% / .58 44.0% / .36 25.0% / .23 .58Table 1: Classification of it by two annotators in a corpus subset.4 Automatic Classification4.1 Training and Test Data Generation4.1.1 SegmentationWe extracted all instances of it and the segments(i.e.
speaker units) they occurred in.
This pro-duced a total of 1.017 instances, 62.5% of whichwere referential.
Each instance was labelled asref or nonref accordingly.
Since a single segmentdoes not adequately reflect the context of the it,we used the segments?
time information to joinsegments to larger units.
We adopted the conceptand definition of spurt (Shriberg et al, 2001), i.e.a sequence of speech not interrupted by any pauselonger than 500ms, and joined segments with timedistances below this threshold.
For each instanceof it, features were generated mainly on the basisof this spurt.4.1.2 PreprocessingFor each spurt, we performed the following pre-processing steps: First, we removed all singledashes (i.e.
interruption points), non-lexicalisedfilled pauses (like em and eh), and all word frag-ments.
This affected only the string representa-tion of the spurt (used for pattern matching later),so the information that a certain spurt position wasassociated with e.g.
an interruption point or a filledpause was not lost.We then ran a simple algorithm to detect di-rect repetitions of 1 to up to 6 words, where re-moved tokens were skipped.
If a repetition wasfound, each token in the first occurrence wastagged as discarded.
Finally, we also temporarilyremoved potential discourse markers by matchingeach spurt against a short list of expressions likeactually, you know, I mean, but also so and sortof.
This was done rather agressively and withouttaking any context into account.
The rationale fordoing this was that while discourse markers doindeed convey important information to the dis-course, they are not relevant for the task at handand can thus be considered as noise that can be re-moved in order to make the (syntactic and lexical)patterns associated with nonreferential it stand outmore clearly.
For each spurt thus processed, POStags were obtained automatically with the Stan-ford tagger (Toutanova et al, 2003).
Although thistagger is trained on written text, we used it withoutany retraining.4.1.3 Feature GenerationOne question we had to address was which infor-mation from the transcription we wanted to use.One can assume that using information like sen-tence breaks or interruption points should be ex-pected to help in the classification task at hand.On the other hand, we did not want our systemto be dependent on this type of human-added in-formation.
Thus, we decided to do several setupswhich made use of this information to various de-grees.
Different setups differed with respect to thefollowing options:-use eos information: This option controls theeffect of explicit end-of-sentence information inthe transcribed data.
If this option is active, thisinformation is used in two ways: Spurt strings aretrimmed in such a way that they do not cross sen-tence boundaries.
Also, the search space for dis-tance features is limited to the current sentence.-use interruption points: This option controlsthe effect of explicit interruption points.
If this op-tion is active, this information is used in a similarway as sentence boundary information.All of the features described in the followingwere obtained fully automatically.
That meansthat errors in the shallow feature generation meth-ods could propagate into the model that waslearned from the data.
The advantage of this ap-proach is, however, that training and test data arehomogeneous.
A model trained on partly erro-neous data is supposed to be more robust againstsimilarly noisy testing data.The first group of features consists of 21 sur-face syntactic patterns capturing the left and rightcontext of it.
Each pattern is represented by a bi-nary feature which has either the value match ornomatch.
This type of pattern matching is done53for two reasons: To get a simplified symbolicrepresentation of the syntactic context of it, andto extract the other elements (nouns, verbs) fromits predicative context.
The patterns are matchedusing shallow (regular-expression based) methodsonly.The second group of features contains lexicalinformation about the predicative context of it.
Itincludes the verb that it is the grammatical sub-ject resp.
object of (if any).
Further features arethe nouns that serve as the direct object (if it issubject), and the noun resp.
adjective complementin cases where it appears in a copula construction.All these features are extracted from the patternsdescribed above, and then lemmatized.The third group of features captures the widercontext of it through distance (in tokens) to wordsof certain grammatical categories, like next com-plementizer, next it, etc.The fourth group of features contains the fol-lowing: oblique is a binary feature encodingwhether the it is preceeded by a preposition.in seemlist is a feature that encodes whether or notthe verb that it is the subject of appears in the listseem, appear, look, mean, happen, sound (fromDimitrov et al (2002)).
discarded is a binary fea-ture that encodes whether the it has been tagged asdiscarded during preprocessing.
The features arelisted in Table 2.
Features of the first group areonly given as examples.4.2 Machine Learning ExperimentWe then applied machine learning in order to buildan automatic classifier for detecting nonreferentialinstances of it, given a vector of features as de-scribed above.
We used JRip, the WEKA4 reim-plementation of Ripper (Cohen, 1995).
All fol-lowing figures were obtained by means of ten-foldcross-validation.
Table 3 contains all results dis-cussed in what follows.In a first experiment, we did not use either ofthe two options described above, so that no in-formation about interruption points or sentenceboundaries was available during training or test-ing.
With this setting, the classifier achieved a re-call of 55.1%, a precision of 71.9% and a resultingF-measure of 62.4% for the detection of the classnonreferential.
The overall classification accuracywas 75.1%.The advantage of using a machine learning sys-4http://www.cs.waikato.ac.nz/ ml/tem that produces human-readable models is thatit allows direct introspection of which of the fea-tures were used, and to which effect.
It turned outthat the discarded feature is very successful.
Themodel produced a rule that used this feature andcorrectly identified 83 instances of nonreferentialit, while it produced no false positives.
Similarly,the seem list feature alone was able to correctlyidentify 22 instances, producing nine false posi-tives.
The following is an example of a more com-plex rule involving distance features, which is alsovery successful (37 true positives, 16 false posi-tives):dist_to_next_to <= 8 anddist_to_next_adj <= 4==> class = nonref (53.0/16.0)This rule captures the common pattern for ex-traposition constructions like It is important to dothat.The following rule makes use of the feature en-coding the distance to the next complementizer(14 true positives, five false positives):obj_verb = null anddist_to_next_comp <= 5==> nonref (19.0/5.0)The fact that these rules with these conditionswere learned show that the features found to bemost important for the detection of nonreferentialit in written text (cf.
Section 2) are also highly rele-vant for performing that task for spoken language.We then ran a second experiment in which weused sentence boundary information to restrict thescope of both the pattern matching features andthe distance-related features.
We expected this toimprove the performance of the model, as patternsshould apply less generously (and thus more ac-curately), which could be expected to result in anincrease in precision.
However, the second experi-ment yielded a recall of 57.7%, a precision of only70.1% and an F-measure of 63.3% for the detec-tion of this class.
The overall accuracy was 74.9%.The system produced a mere five rules (comparedto seven before).
The model produced the identi-cal rule using the discarded-feature.
The same ap-plies to the seem list feature, with the differencethat both precision and recall of this rule were al-tered: The rule now produced 23 true positives andsix false positives.
The slightly higher recall of themodel using the sentence boundary information ismainly due to a better coverage of the rule usingthe features encoding the distance to the next to-infinitive and the next adjective: it now produced54Syntactic Patterns1.
INF it do it10.
it BE adj it was easy11.
it BE obj it?s a simple question13.
it MOD-VERBS INF obj it?ll take some more time20.
it VERBS TO-INF it seems to beLexical Features22.
noun comp noun complement (in copula construction)23. adj comp adjective complement (in copula construction)24. subj verb verb that it is the subject of25.
prep preposition before indirect object26.
ind obj indirect object of verb that it is subject of27.
obj direct object of verb that it is subject of28.
obj verb verb that it is object ofDistance Features (in tokens)29. dist to next adj distance to next adjective30.
dist to next comp distance to next complementizer (that,if,whether)31. dist to next it distance to next it32.
dist to next nominal distance to next nominal33.
dist to next to distance to next to-infinitive34.
dist to previous comp distance to previous complementizer35.
dist to previous nominal distance to previous nominalOther Features36.
oblique whether it follows a preposition37.
seem list whether subj verb is seem, appear, look, mean, happen, sound38.
discarded whether it has been marked as discarded (i.e.
in a repetition)Table 2: Our Features (selection)57 true positives and only 30 false positives.We then wanted to compare the contributionof the sentence breaks to that of the interruptionpoints.
We ran another experiment, using only thelatter and leaving everything else unaltered.
Thistime, the overall performance of the classifier im-proved considerably: recall was 60.9%, precision80.0%, F-measure 69.2%, and the overall accu-racy was 79.6%.
The resulting model was rathercomplicated, including seven complex rules.
Theincrease in recall is mainly due to the followingrule, which is not easily interpreted:5it_s = match anddist_to_next_nominal >=21 anddist_to_next_adj >=500 andsubj_verb = null==> nonref (116.0/31.0)The considerable improvement (in particularin precision) brought about by the interruptionpoints, and the comparatively small impact of sen-tence boundary information, might be explainablein several ways.
For instance, although sentenceboundary information allows to limit both thesearch space for distance features and the scope ofpattern matching, due to the shallow nature of pre-processing, what is between two sentence breaksis by no means a well-formed sentence.
In thatrespect, it seems plausible to assume that smaller5The value 500 is used as a MAX VALUE to indicate thatno match was found.units (as delimited by interruption points) may bebeneficial for precision as they give rise to fewerspurious matches.
It must also be noted that inter-ruption points do not mark arbitrary breaks in theflow of speech, but that they can signal importantinformation (cf.
Heeman & Allen (1999)).5 Conclusion and Future WorkThis paper presented a machine learning systemfor the automatic detection of nonreferential it inspoken dialog.
Given the fact that our feature ex-traction methods are only very shallow, the re-sults we obtained are satisfying.
On the one hand,the good results that we obtained when utilizinginformation about interruption points (P:80.0% /R:60.9% / F:69.2%) show the feasibility of detect-ing nonreferential it in spoken multi-party dialog.To our knowledge, this task has not been tackledbefore.
On the other hand, the still fairly goodresults obtained by only using automatically de-termined features (P:71.9% / R:55.1% / F:62.4%)show that a practically usable filtering compo-nent for nonreferential it can be created even withrather simple means.All experiments yielded classifiers that are con-servative in the sense that their precision is consid-erably higher than their recall.
This makes themparticularly well-suited as filter components.For the coreference resolution system that this55P R F % CorrectNone 71.9 % 55.1 % 62.4 % 75.1 %Sentence Breaks 70.1 % 57.7 % 63.3 % 74.9 %Interruption Points 80.0 % 60.9 % 69.2 % 79.6 %Both 74.2 % 60.4 % 66.6 % 77.3 %Table 3: Results of Automatic Classification Using Various Information Sourceswork is part of, only the fully automatic variant isan option.
Therefore, future work must try to im-prove its recall without harming its precision (toomuch).
One way to do that could be to improve therecognition (i.e.
correct POS tagging) of grammat-ical function words (in particular complementizerslike that) which have been shown to be importantindicators for constructions with nonreferential it.Other points of future work include the refinementof the syntactic pattern features and the lexical fea-tures.
E.g., the values (i.e.
mostly nouns, verbs,and adjectives) of the lexical features, which havebeen almost entirely ignored by both classifiers,could be generalized by mapping them to commonWordNet superclasses.AcknowledgementsThis work has been funded by the DeutscheForschungsgemeinschaft (DFG) in the context ofthe project DIANA-Summ (STR 545/2-1), and bythe Klaus Tschira Foundation (KTF), Heidelberg,Germany.
We thank our annotators Irina Schenkand Violeta Sabutyte, and the three anonymous re-viewers for their helpful comments.ReferencesBoyd, A., W. Gegg-Harrison & D. Byron (2005).
Identifyingnon-referential it: a machine learning approach incor-porating linguistically motivated patterns.
In Proceed-ings of the ACL Workshop on Feature Selection for Ma-chine Learning in NLP, Ann Arbor, MI, June 2005, pp.40?47.Byron, D. K. (2002).
Resolving pronominal reference to ab-stract entities.
In Proc.
of ACL-02, pp.
80?87.Carletta, J.
(1996).
Assessing agreement on classificationtasks: The kappa statistic.
Computational Linguistics,22(2):249?254.Clemente, J. C., K. Torisawa & K. Satou (2004).
Improv-ing the identification of non-anaphoric it using SupportVector Machines.
In International Joint Workshop onNatural Language Processing in Biomedicine and itsApplications, Geneva, Switzerland.Cohen, W. W. (1995).
Fast effective rule induction.
InProc.
of the 12th International Conference on MachineLearning, pp.
115?123.Dimitrov, M., K. Bontcheva, H. Cunningham & D. Maynard(2002).
A light-weight approach to coreference resolu-tion for named entities in text.
In Proc.
DAARC2.Eckert, M. & M. Strube (2000).
Dialogue acts, synchronisingunits and anaphora resolution.
Journal of Semantics,17(1):51?89.Evans, R. (2001).
Applying machine learning toward an auto-matic classification of It.
Literary and Linguistic Com-puting, 16(1):45 ?
57.Heeman, P. & J. Allen (1999).
Speech repairs, intonationalphrases, and discourse markers: Modeling speakers?utterances in spoken dialogue.
Computational Linguis-tics, 25(4):527?571.Janin, A., D. Baron, J. Edwards, D. Ellis, D. Gelbart, N. Mor-gan, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke &C. Wooters (2003).
The ICSI Meeting Corpus.
InProceedings of the IEEE International Conference onAcoustics, Speech and Signal Processing, Hong Kong,pp.
364?367.Krippendorff, K. (1980).
Content Analysis: An introductionto its methodology.
Beverly Hills, CA: Sage Publica-tions.Lappin, S. & H. J. Leass (1994).
An algorithm for pronom-inal anaphora resolution.
Computational Linguistics,20(4):535?561.Ng, V. & C. Cardie (2002).
Improving machine learning ap-proaches to coreference resolution.
In Proc.
of ACL-02,pp.
104?111.Paice, C. D. & G. D. Husk (1987).
Towards the automaticrecognition of anaphoric features in English text: theimpersonal pronoun ?it?.
Computer Speech and Lan-guage, 2:109?132.Quirk, R., S. Greenbaum, G. Leech & J. Svartvik (1991).A Comprehensive Grammar of the English Language.London, UK: Longman.Shriberg, E., A. Stolcke & D. Baron (2001).
Observationson overlap: Findings and implications for automaticprocessing of multi-party conversation.
In Proceedingsof the 7th European Conference on Speech Communi-cation and Technology (EUROSPEECH ?01), Aalborg,Denmark, 3?7 September 2001, Vol.
2, pp.
1359?1362.Strube, M. & C. Mu?ller (2003).
A machine learning approachto pronoun resolution in spoken dialogue.
In Proceed-ings of the 41st Annual Meeting of the Association forComputational Linguistics, Sapporo, Japan, 7?12 July2003, pp.
168?175.Toutanova, K., D. Klein & C. D. Manning (2003).
Feature-rich part-of-speech tagging with a cyclic dependencynetwork.
In Proceedings of HLT-NAACL 03, pp.
252?259.56
