Proceedings of the 8th International Natural Language Generation Conference, pages 147?151,Philadelphia, Pennsylvania, 19-21 June 2014. c?2014 Association for Computational LinguisticsTowards Surface Realization with CCGs Induced from DependenciesMichael WhiteDepartment of LinguisticsThe Ohio State UniversityColumbus, OH 43210, USAmwhite@ling.osu.eduAbstractWe present a novel algorithm for inducingCombinatory Categorial Grammars fromdependency treebanks, along with initialexperiments showing that it can be usedto achieve competitive realization resultsusing an enhanced version of the surfacerealization shared task data.1 IntroductionIn the first surface realization shared task (Belzet al., 2011), no grammar-based systems achievedcompetitive results, as input conversion turnedout to be more difficult than anticipated.
Sincethen, Narayan & Gardent (2012) have shown thatgrammar-based systems can be substantially im-proved with error mining techniques.
In this pa-per, inspired by recent work on converting depen-dency treebanks (Ambati et al., 2013) and seman-tic parsing (Kwiatkowksi et al., 2010; Artzi andZettlemoyer, 2013) with Combinatory CategorialGrammar (CCG), we pursue the alternative strat-egy of inducing a CCG from an enhanced versionof the shared task dependencies, with initial exper-iments showing even better results.A silver lining of the failure of grammar-basedsystems in the shared task is that it revealed someproblems with the data.
In particular, it becameevident that in cases where a constituent is an-notated with multiple roles in the Penn Treebank(PTB), the partial nature of Propbank annotationand the restriction to syntactic dependency treesmeant that information was lost between the sur-face and deep representations, leading grammar-based systems to fail for good reason.
For ex-ample, Figure 1 shows that with free object rel-atives, only one of the two roles played by howmuch manufacturing strength is captured in thedeep representation, making it difficult to linearizethis phrase correctly.
By contrast, Figure 2 (top)							 	 	!		"#	$	  !"#$%$$&'	()!$$&'"($) !!!*+$),!'!
"#$%&'(#)*'+,-./' 0#12%'+,-./'3%-%,&%,45'6$78'!"#/7'$"%'89))9,:'6$78')"#$%&'/#)*'9,-./;''<=,&'6$78'&'''/7'$"%'9,',#12%'9,-./>?
'Figure 1: Shared Task Input for Economists aredivided as to [how much manufacturing strength]ithey expect to see ti in September reports on indus-trial production and capacity utilization , also duetomorrow (wsj 2400.6, ?deep?
representation)shows an experimental version of the shallow rep-resentation intended to capture all the syntactic de-pendencies in the PTB, including the additionalobject role played by this phrase here.1 Includ-ing all PTB syntactic dependencies in the shallowrepresentation makes it feasible to define a com-patible CCG; at the bottom of the figure, a cor-responding CCG derivation for these dependen-cies is shown.
In the next section, we present analgorithm for inducing such derivations.
In con-trast to Ambati et al.
?s (2013) approach, the algo-rithm integrates the proposal of candidate lexicalcategories with the derivational process, making itpossible to derive categories involving unsaturatedarguments, such as se,dcl\npx/(se?,to\npx ); it alsomakes greater use of unary type-changing rules,as with Artzi & Zettlemoyer?s (2013) approach.1Kudos to Richard Johansson for making these enhance-ments available.147Unlike their approach though, it works in a broadcoverage setting, and makes use of all the combi-nators standardly used with CCG, including onesfor type-raising.2 Inducing CCGs from DependenciesPseudocode for the induction algorithm is givenin Figure 3.
The algorithm takes as input a setof training sentences with their gold standard de-pendencies.
We pre-processed the dependenciesto make coordinating conjunctions the head, andto include features for zero-determiners.
The algo-rithm also makes use of a seed lexicon that spec-ifies category projection by part of speech as wellas a handful of categories for function words.
Forexample, (1) shows how a tensed verb projects toa finite clause category, while (2) shows the usualCCG category for a determiner, which here intro-duces a ?NMOD?
dependency.2(1) expect ` se,dcl : @e(expect ?
?TENSE?pres)(2) the ` npx/nx : @x(?NMOD?
(d ?
the))The algorithm begins by instantiating the lexicalcategories and type-changing rules that match theinput dependency graph, tracking the categoriesin a map (edges) from nodes to edges (i.e., signswith a coverage vector).
It then recursively vis-its each node in the primary dependency tree bot-tom up (combineEdges), using a local chart (do-Combos) at each step to combine categories foradjacent phrases in all possible ways.
Along theway, it creates new categories (extendCats and co-ordCats) and unary rules (applyNewUnary).
Forexample, when processing the node for expect inFigure 2, the nodes for they and to are recursivelyprocessed first, deriving the categories npw9 andsw11 ,to\npw9 /npw8 for they and to see .
.
.
, respec-tively.
The initial category for expect is then ex-tended as shown in (3), which allows for com-position with to see .
.
.
(as well as with a cate-gory for simple application).
When there are co-ordination relations for a coordinating conjunction(or coordinating punctuation mark), the appropri-ate category for combining like types is insteadconstructed, as in (4).
Additionally, for modifiers,unary rules are instantiated and applied, e.g.
therule for noun-noun compounds in (5).2In the experiments reported here, we made use of onlysix (non-trivial) hand-specified categories and two type-changing rules; though we anticipate adding more initial cat-egories to handle some currently problematic cases, the vastmajority of the categories in the resulting grammar can beinduced automatically.Inputs Training set of sentences with dependencies.
Initiallexicon and rules.
Argument and modifier relations.
Deriva-tion scoring metric.
Maximum agenda size.Definitions edges is a map from dependency graph nodesto their edges, where an edge is a CCG sign together with acoverage bitset; agenda is a priority queue of edges sortedby the scoring metric; chart manages equivalence classes ofedges; see text for descriptions of auxiliary functions such asextendCats and coordCats below.AlgorithmbestDerivs, lexcats, unaryRules?
?For each item in training set:1. edges[node] ?
instCats(node), ruleInsts[node] ?
in-stRules(node), for node in input graph2.
combineEdges(root), with root of input graph3.
bestEdge ?
unpack (edges[root]); bestDerivs +?bestEdge.sign; lexcats +?
abstractedCats(bestEdge),unaryRules +?
abstractedRules(bestEdge), if best-Edge completedef combineEdges(node):1. combineEdges(child) for child in node.kids2.
edges[node] +?
coordCats(node) if node has co-ord relations, otherwise edges[node] ?
extend-Cats(node,rels) for argument rels3.
agenda ?
edges[node]; agenda +?
edges[child] forchild in node.kids; chart?
?4.
While agenda not empty:(a) next?
agenda.pop(b) chart +?
next(c) doCombos(next), unless next packed into an ex-isting chart item5.
edges[node]?
chart edges for node filtered for maxi-mal input coveragedef doCombos(next):1. agenda +?
applyUnary(next), if next is for node2.
For item in chart:(a) agenda +?
applyBinary(next,item), if next is ad-jacent to item(b) agenda +?
applyNewUnary(next,item), if nextconnected to item by a modifier relationOutputs bestDerivs, lexcats, unaryRulesFigure 3: CCG Induction Algorithm148economists are divided as to how much manufacturing strength they expect to see in .
.
.
.rootsubj vcobjadv pmodpmodnmodamodnmod subobjsubjsbjoprd im loc pmodp.
.
.
September reports on industrial production and capacity utilization , also due tomorrowpmodnmod nmodpmodnmod coord1coord2nmodpappoamodtmp.
.
.
to how much manuf.
strength they expect to see .
.
.pp/np wh adj sng n np sdcl\np/(sto\np) sto\np/(sb\np) sb\np/np>T >Bwh\wh n/n s/(s\np) sto\np/np< > >Bwh n sdcl\np/np>Bnp/(s/np)/n sdcl/np>np/(s/np)>np>pp.
.
.
Sept. reports on industrial production and capacity utilization .
.
.np n pp/np adj n np\?np/?np n nn/n n/n n/n> > >n n nnp np>np\?np<np>ppn\n<nnpFigure 1: exampleFigure 2: Augmented Syntactic Dependencies with Corresponding CCG Derivation (dashed dependen-cies indicate relations from additional parents beyond those in the primary tree structure)149(3) expect ` sw10 ,dcl\npw9 /(sw11 ,to\npw9 ) :@w10(expect ?
?TENSE?pres ?
?SUBJ?w9 ?
?OPRED?w11)(4) and ` npw19 \?npw18 /?npw21 :@w19(and ?
?COORD1?w18 ?
?COORD2?w21)(5) nw20 ?
nw21 /nw21 : @w21(?NMOD?w20)At the end of the recursion, the lexical cate-gories and type-changing rules are extracted fromthe highest-scoring derivation and added to theoutput sets, after first replacing indices such as w10with variables.3 Experiments and Future WorkWe ran the induction algorithm over the stan-dard PTB training sections (02?21), recoveringcomplete derivations more than 90% of the timefor most sections.
Robust treatment of coordina-tion, including argument cluster coordination andgapping, remains a known issue; other causes ofderivation failures remain to be investigated.
Toselect preferred derivations, we used a complex-ity metric that simply counts the number of stepsand the number of slashes in the categories.
Wethen trained a generative syntactic model (Hock-enmaier and Steedman, 2002) and used it alongwith a composite language model to generate n-best realizations for reranking (White and Rajku-mar, 2012), additionally using a large-scale (giga-word) language model.
Development and test re-sults appear in Table 1.
Perhaps because of theexpanded use of type-changing rules with sim-ple lexical categories, the generative model andhypertagger (Espinosa et al., 2008) performedworse than expected.
Combining the generativesyntactic model and composite language model(GEN) with equal weight yielded a devtest BLEUscore of only 0.4513, while discriminatively train-ing the generative component models (GLOBAL)increased the score to 0.7679.
Using all fea-tures increased the score to 0.8083, while dou-bling the beam size (ALL+) pushed the score to0.8210, indicating that search errors may be anissue.
Ablation results show that leaving outthe large-scale language model (NO-BIGLM) anddependency-ordering features (NO-DEPORD) sub-stantially drops the score.3 Focusing only onthe 80.5% of the sentences for which a completederivation was found (COMPLETE) yielded a scoreof 0.8668.
By comparison, realization with the3All differences were statistically significant at p < 0.01with paired bootstrap resampling (Koehn, 2004).Model Exact Complete BLEUSect 00GEN 2.4 79.5 0.4513GLOBAL 29.7 79.0 0.7679NO-BIGLM 29.1 78.2 0.7757NO-DEPORD 34.3 77.9 0.7956ALL 35.8 78.4 0.8083ALL+ 36.4 80.5 0.8210COMPLETE 44.4 - 0.8668NATIVE 48.0 88.7 0.8793Sect 23GEN 2.8 80.3 0.4560GLOBAL 31.3 78.5 0.7675ALL 37.6 77.2 0.8083ALL+ 38.1 80.4 0.8260COMPLETE 47.0 - 0.8743NATIVE 46.4 86.4 0.8694Table 1: Development set (Section 00) & test set(Section 23) results, including exact match andcomplete derivation percentages and BLEU scoresnative OpenCCG inputs (and the large-scale LM)on all sentences (NATIVE) yields a score morethan five BLEU points higher, despite using in-puts with more semantically-oriented relations andleaving out many function words, indicating thatthere is likely substantial room for improvementin the pre-processing and grammar induction pro-cess.
Towards that end, we tried selecting the bestderivations using several rounds of Viterbi EMwith the generative syntactic model, but doing sodid not improve realization quality.A similar pattern is seen in the Section 23 re-sults, with a competitive BLEU score of 0.8260with the expanded beam, much higher thanNarayan & Gardent?s (2012) score of 0.675 with38.8% coverage, the best previous score with agrammar-based system.
This score still trails theshared task scores of the top statistical dependencyrealizers by several points (STUMABA-S at 0.8911and DCU at 0.8575), though it exceeds the score ofa purpose-built system using no external resources(ATT at 0.6701).
In future work, we hope to closethe gap with the top systems by integrating an im-proved ranking model into the induction processand resolving the remaining representational is-sues with problematic constructions.AcknowledgmentsThanks to the anonymous reviewers, Richard Johansson andthe University of Sydney Schwa Lab for helpful commentsand discussion.
This work was supported in part by NSFgrants IIS-1143635 and IIS-1319318.150ReferencesBharat Ram Ambati, Tejaswini Deoskar, and MarkSteedman.
2013.
Using CCG categories to improveHindi dependency parsing.
In Proc.
ACL.Yoav Artzi and Luke Zettlemoyer.
2013.
Weakly su-pervised learning of semantic parsers for mappinginstructions to actions.
TACL, 1:49?62.Anja Belz, Michael White, Dominic Espinosa, EricKow, Deirdre Hogan, and Amanda Stent.
2011.
Thefirst surface realisation shared task: Overview andevaluation results.
In Proc.
ENLG.Dominic Espinosa, Michael White, and Dennis Mehay.2008.
Hypertagging: Supertagging for surface real-ization with CCG.
In Proc.
ACL.Julia Hockenmaier and Mark Steedman.
2002.
Gen-erative models for statistical parsing with Combina-tory Categorial Grammar.
In Proc.
ACL.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proc.
EMNLP.Tom Kwiatkowksi, Luke Zettlemoyer, Sharon Goldwa-ter, and Mark Steedman.
2010.
Inducing probabilis-tic CCG grammars from logical form with higher-order unification.
In Proc.
EMNLP.Shashi Narayan and Claire Gardent.
2012.
Error min-ing with suspicion trees: Seeing the forest for thetrees.
In Proc.
COLING.Michael White and Rajakrishnan Rajkumar.
2012.Minimal dependency length in realization ranking.In Proc.
EMNLP.151
