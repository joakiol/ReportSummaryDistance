Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1127?1138,October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational LinguisticsVote Prediction on Comments in Social PollsIsaac Persing and Vincent NgHuman Language Technology Research InstituteUniversity of Texas at DallasRichardson, TX 75083-0688{persingq,vince}@hlt.utdallas.eduAbstractA poll consists of a question and a set ofpredefined answers from which voters canselect.
We present the new problem of voteprediction on comments, which involvesdetermining which of these answers avoter selected given a comment she wroteafter voting.
To address this task, we ex-ploit not only the information extractedfrom the comments but also extra-textualinformation such as user demographic in-formation and inter-comment constraints.In an evaluation involving nearly one mil-lion comments collected from the popu-lar SodaHead social polling website, weshow that a vote prediction system that ex-ploits only textual information can be im-proved significantly when extended withextra-textual information.1 IntroductionWe introduce in this paper a new opinion miningtask, vote prediction on comments in social polls.Recall that a poll consists of a question accompa-nied by a set of predefined answers.
A user whovotes on the question will choose one of these an-swers and will be prompted to enter a commentgiving an explanation of why she chose the an-swer.
Given a poll and a user comment writtenin response to it, the task of vote prediction seeksto determine which predefined answer was chosenby the author of the comment.A solution to the vote prediction problem wouldcontribute significantly to our understanding of theunderlying attitudes of individual social pollingwebsite users.
This understanding could be ex-ploited for tasks such as improving user experi-ence or directed advertising; if we can predict howa user will vote on a question, we can make moreaccurate guesses about what kind of content/adsrelated to the question the user would like to see.Unfortunately, a major difficulty of vote predic-tion arises from the casual nature of discussion insocial media.
A comment often contains insuffi-cient information for inferring the user?s vote, orin some cases may even be entirely absent.In light of this difficulty, we exploit two addi-tional types of information in the prediction pro-cess.
First, we employ demographic features de-rived from user profiles.
Demographic featuresmay be broadly useful for other opinion miningtasks such as stance classification (Somasundaranand Wiebe, 2010), as many social media web-sites like CreateDebate1 allow users to create pro-files with similar demographic information.
Previ-ous work has attempted to predict such latent fea-tures (e.g., Rao and Yarowsky (2010), Burger etal.
(2011)) rather than employing them for opin-ion mining tasks.Second, we exploit inter-comment constraintsto help us perform joint inference over votes ondifferent questions.
Note that previous work ondebate stance recognition has also employed con-straints to improve the inference process.
Specif-ically, in stance prediction, it is typical to em-ploy so-called author constraints (e.g., Thomaset al.
(2006), Bansal et al.
(2008), Walker et al.
(2012a), Hasan and Ng (2013)), which specify thattwo documents written by the same author for thesame topic should have the same stance.
However,in vote prediction, author constraints are not use-ful because a user is not permitted to cast morethan one vote per question, unlike in stance pre-diction, where users may engage in a debate andtherefore post more than once per debate topic.Consequently, we propose two new types of con-straints for exploiting inter topic user voting pat-terns.
One constraint involves pairs of authors andthe other involves pairs of questions.
These con-straints are also potentially useful for other opin-1http://www.createdebate.com/1127ion mining tasks involving social media, as socialmedia sites typically allow users to comment onmultiple topics.
Note that enforcing constraints in-volving two questions is by no means trivial, as thepossible class values associated with the two com-ments may not necessarily be the same.Another contribution of our work lies in ouradaptation of the label propagation algorithm (Zhuand Ghahramani, 2002) to enforce constraints forvote prediction.
Recall that existing stance classi-fication approaches enforce constraints using min-imum cut (Thomas et al., 2006), integer linear pro-gramming (Lu et al., 2012), and loopy belief prop-agation (Burfoot et al., 2011).
Our decision to em-ploy label propagation stems in part from the in-ability of loopy belief propagation and integer lin-ear programming to efficiently process the nearlyone million comments we have, and in part fromthe inability of the traditional two-way minimumcut algorithm to handle multiclass classification.It is worth noting, however, that other variationsof the label propagation algorithm have been pro-posed for unrelated NLP tasks such as automati-cally harvesting temporal facts from the web (e.g.,Wang et al.
(2011) and Wang et al.
(2012)).While we are the first to address the vote predic-tion task, other researchers have previously usedsocial media to predict the outcomes of variousevents, primarily by analyzing Twitter data.
Forexample, Tumasjan et al.
(2010) and Gayo-Avelloet al.
(2011) performed the related task of predict-ing the outcomes of elections.
Rather than pre-dicting election outcomes, O?Connor et al.
(2010)focused on finding correlations between measuresderived from tweets and the outcomes of politi-cal events like elections and polls.
Finally, Asurand Huberman (2010) predicted movies?
box of-fice success.
These tasks contrast with our task ofvote prediction in that they are concerned with ag-gregate measures such as the fraction of the voteeach candidate or party will win in an election orhow much money a movie will make at the boxoffice, whereas vote prediction is concerned withpredicting how individual people will vote on amuch wider variety of news/political topics.2 CorpusSodaHead2 is a social polling website where usersvote on and ask questions about a wide variety oftopics ranging from the serious (e.g., ?Should the2http://www.sodahead.comU.S.
raise the minimum wage??)
to the silly (e.g?What is your favorite kind of pie??).
Whenever auser votes on one of these questions, choosing oneof a set of predefined answers, she is prompted toenter a comment giving an explanation of why shechose the answer she did.
Our corpus3 consists ofall the comments4 users posted under all featuredquestions in the News & Politics category of theSodaHead website between March 12, 2008 andAugust 21, 2013.This dataset consists of a total of 997,379 com-ments over 4,803 different questions, so an aver-age of 208 comments are written in response toeach question.
The length of an average commentis 49 words.
As Table 1 illustrates, these questionsmay have more than two possible answers, with anaverage question having 2.4 possible answers.Each SodaHead user has her own profile thatcontains demographic information about her.
Aswe can see from Table 2, many users choose toprovide only some information about themselves,leaving many of the demographic fields blank.108,462 users posted at least one comment in ourcorpus, with an average user commenting on 9.2of our questions.3 Baseline SystemsTo perform our experiments, we first split ourcomments into three sets, a test set for evaluatingperformance, a training set for training classifiers,and a development set for tuning parameters.
Inorder to ensure that the comparisons of our experi-ments are valid, we construct our test set using thesame 20% of comments in the dataset regardlessof experiment.
Since our goal is to plot a learningcurve illustrating how our various vote predictionsystems perform given different amounts of train-ing and development data, we vary the size of ourtraining and development sets across experimentsso that in the smallest experiment, together theycomprise 25% of the remaining (non-test) com-ments, and in the largest experiment, they com-3http://www.hlt.utdallas.edu/%7epersingq/SocialPolls/ isthe distribution site for our corpus.
We preserve useranonymity by replacing the original id of each user with arandom number in our corpus.4A ?comment?
is the text a user posted when submittingher vote on a question.
It does not include posts not associ-ated with a vote (such as responses to other posts) or voteswhere the user chose not to enter a comment.
Thus, thereis a one-to-one relationship between comments in votes inour dataset.
The vote associated with a comment is alwaysknown.1128Question Vote CommentWho Won Round Two ofthe Presidential Debate?Barack Obama Binders full of women.
That is all.Mitt Romney Obama is inept and a liar.
We can?t survive 4 more years of his crazy crap.What?s the Best Way toRead a Magazine?in print Upside down like Luna Lovegood.online Print costs money.
It also doesn?t have a Search function.on a tablet device since sooooo many people have tablet devices why read it as print or online?on a smartphone Clicked in print!!!
AarghTable 1: Sample questions and comments.
All of the pre-defined answers for these questions are repre-sented by one comment.User ID 3479864 3189372Age 25-34Smoker NoDrinker NoIncomeSexual Orientation StraightRelationship Status SinglePolitical Views Conservative ModerateEthnicityLooking ForCareer IndustryChildren UndecidedEducation High SchoolGender Female MaleReligious Views Other ChristianEmployment StatusWeight TypeTable 2: Sample user profiles.prise 100% of the remaining comments.
For eachexperiment, we maintain a ratio of three trainingcomments to one development comment.Recall that each comment in our dataset is writ-ten in response to a particular question.
For eachtest comment, our goal is to predict the user?s an-swer to the question given the text of her comment.One of the major inherent difficulties of our taskis that it consists not of one, but of 4,803 sep-arate multiclass classification problems (one foreach question).
As a result, our approach to theproblem necessarily has to be somewhat generic,as it would be too time-consuming to develop anappropriate feature set for each question.3.1 Baseline 1Our first baseline?s (B1) approach employs 4,803multiclass classifiers (one for each question).
Eachclassifier is trained on one question?s training set,representing each comment using only a bias fea-ture.
Each of our classifiers is trained using MAL-LET?s (McCallum, 2002) implementation of max-imum entropy (ME) classification.
This is equiv-alent to merely counting the number of trainingset comments that voted for each possible answer,selecting the most frequent answer, then applyingthis label to all the comments in the test set.
Thismajority baseline serves primarily to tell us howwell our more sophisticated baseline performs.3.2 Baseline 2Our second baseline (B2) is constructed in exactlythe same way as B1except that each classifier istrained using both a bias feature and a standard setof feature types described below.3.2.1 FeaturesSince the questions in our dataset come from theNews & Politics category of the SodaHead web-site, many of the questions?
topics are political.For that reason, it makes sense to use featureswhich have been shown to work well on otherpolitical classification problems.
We thereforebase our feature set on that used by Walker etal.
(2012b) for political debate classification.
Ourfeatures are described below.N-grams.
Unigrams have been shown to per-form well in ideological debates (Somasundaranand Wiebe, 2010), so we therefore present ourclassifiers with lemmatized unigram, bigram, andtrigram features.
We normalize the n-gram featurevector to unit length to avoid giving undue influ-ence to longer comments.Cue Words.
Based on other work (Fox Treeand Schrock, 1999; Fox Tree and Schrock, 2002;Groen et al., 2010; Walker et al., 2012b), we alsopresent our classifiers with features representingthe first lemmatized unigram, bigram, and trigramappearing in each comment.
These may be usefulin our task when, for example, a user?s commentbegins with or entirely consists of a restatement ofthe answer she chose.
So if the possible answersfor a given question are ?Yes?
and ?No?, a usermight write in her comment ?Yes.
Because ...?,and this would make the ?CueWord:Yes?
featureuseful for classifying this comment.Emotion Frequency.
For each word in a com-ment, we used the NRC Emotion Word Lexicon1129(Mohammad and Yang, 2011) to discover if theword conveys any emotion.
Then, for each emo-tion or sentiment covered by the lexicon (anger,anticipation, disgust, fear, joy, sadness, surprise,trust, positive, or negative) ei, we construct a fea-ture ei:C(ei)totaldescribing how much of the commentconsists of words conveying emotion ei, whereC(ei) is the count of words in the comment bear-ing emotion eiand total is the number of wordsin the comment.
To understand why this fea-ture may be useful, consider the question ?DoesSarah Palin deserve VP??
We suspect that userswho post comments laden with words associatedwith positive emotions like joy are more likelyto vote ?Yes?
because the positive emotions im-ply they are happy about a Sarah Palin vice presi-dency.
Similarly, users who post comments ladenwith negative emotions like anger might be morelikely to vote ?No?.Dependencies.
We use the Stanford Parser (deMarneffe et al., 2006) to extract a set of depen-dencies from each comment.
For an example ofhow dependencies might help in our task, con-sider the second comment in Table 1.
From thiscomment, we can extract the dependency tripledependency:(nsubj,inept,obama), which indicatesthat the user who wrote it does not like Obama andis therefore more likely to have voted for Romneyin the question.
Dependency feature vectors arenormalized to unit length.Emotion Dependencies.
To form an emo-tion dependency feature, we take a regular de-pendency feature and replace each of its wordswhere possible with the emotion it evokes as deter-mined by the NRC Emotion Word Lexicon.
Thusfrom the dependency:(nsubj,inept,obama) exam-ple above, we would generate three features: emo-tiondependency:(nsubj,anger,obama), emotionde-pendency:(nsubj,disgust,obama), and emotionde-pendency:(nsubj,negative,obama).
These featureshelp generalize dependencies, and this is use-ful because predictive features like emotiondepen-dency:(nsubj,negative,obama) appear frequentlyin the comments for this question, but depen-dency:(nsubj,inept,obama) does not.
Emotion De-pendency feature vectors are normalized to unitlength.Post Information.
Features under this categoryjust calculate some basic statistics about a com-ment.
These features may be useful because, forexample, the question ?Most Scandalous Politi-cians of 2008?
Who deserves the title??
has sixpossible answers, each except the last naming aparticular well-known politician.
The last choiceis ?The most scandalous politician of 2008 is ...?and the user is expected to name a politician in hercomment.
It would make sense for users choos-ing this option to have written longer responsessince they have to name and possibly explain theirchoice to users who might not necessarily knowwho their chosen politician is.3.2.2 Feature SelectionBecause some of the feature types (n-grams, cuewords, dependencies, and emotion dependencies)described in the previous subsection are expectedto generate a large number of non-predictive fea-tures, we trim some of the most irrelevant fea-tures out of the feature set to avoid memory prob-lems.
Therefore, following Yang and Pedersen(1997), for each question we calculate the infor-mation gain of each feature of these types on thetraining set.
We then remove those features havingthe lowest information gain as well as those fea-tures occurring less than ten times in the dataset.Early experiments showed that 1,000 was a rea-sonable number of features to keep, so for all ex-periments we keep only the top 1,000 features ofthese types.
Note that we do not apply feature se-lection to emotion frequency or post informationfeatures, as each of these sets consists of a smallnumber of real-valued features.4 Demographic FeaturesAs mentioned in the introduction, a major diffi-culty inherent to our problem is that in many casesa comment contains insufficient information forinferring the underlying vote.
Aside from beingshort, the comments shown in Table 1 are typ-ical of comments found in the dataset.
Somecomments are like the first and third in the table,requiring some obscure bit of world knowledgeto understand what the writer is saying.
Otherslike the fourth only explain why the user did notchoose a particular answer, which is always po-tentially useful, but sufficient only if the commentexcludes every other possible choice.Because it is difficult to tell how a user votedgiven her comment, we exploit the demographicinformation users provide in their profiles as anadditional source of information.
Since manyof the questions in our dataset deal with poli-tics, we anticipate that information about things1130such as whether a comment was written by aconservative or progressive user would be use-ful for predicting the answers of many comments.For each comment, we encode demographic in-formation as features in the following way.
Foreach field in the user?s profile shown in Table 2(aside from user ID), we construct a feature of theform Fi:Viif the user filled in field Fiwith valueVi.
Thus, any comment made by user 3479864would include the features Age:25?34, Politi-calViews:Conservative, Gender:Female, and Reli-gion:Other.Here is an example of a comment whosepredicted vote gets corrected by adding demo-graphic features to our system.
For the ques-tion, ?LPGA Decides to Allow Transgender Com-petitors: Good or Bad Move for Golf?
?, user2252750 writes, ?LPGA ...can let monkeys play ifthey wish....nobody gives a rip... bark?.
Of thethree possible answers for this question, ?Goodmove?, ?Bad move?, and ?Undecided?, our base-line system without demographics believes thatuser 2252750 probably voted for the third, as?nobody gives a rip?
makes him sound apathetictoward the issue.
However, our demographicsystem notices that his profile contains ?Reli-gion:Christian?, and users with this demographicattribute choose ?Bad Move?
64% of the time.Thus, demographic features allowed our system tocorrectly predict his vote for ?Bad Move?.Since demographics are also expected to gen-erate a large number of non-predictive features,we apply feature selection to them as described inSection 3.2.2.5 Enforcing ConstraintsWe mentioned earlier that an average SodaHeadquestion contains 208 comments.
This impliesthat there are only about 31?125 comments5 inthe average training set for one of our ME classi-fiers.
It would be difficult to train a good classi-fier from a training set this small even if we hadfeature sets tailored to work well on each of the4,803 questions.
While we have already attemptedto exploit user information (in the form of de-mographic features) to help improve our system?sperformance, this approach still treats the task as4,803 separate classification problems.
It does notallow for the possibility that classification on one5At the low and high end of the learning curve respec-tively.question may be improved by exploiting informa-tion gleaned from votes on other questions.One way we might exploit such information isby first noticing that, for any pair of questions,there may be multiple users who commented onboth.
This overlap between questions allows us tocalculate how predictive a user?s vote on one ques-tion is of how she will vote on the other.
For ex-ample, on the question ?Who Would You RatherHave Dinner With?
?, we found that users whovoted for ?Mitt Romney?
were much more likelyto choose ?No, I?m still voting for him?
on thequestion ?Does Mitt Romney?s ?Entitled?
RemarksChange Your Opinion of Him??.
Similarly, userswho voted to have dinner with ?Barack Obama?were much more likely to vote ?Yes, I?m not vot-ing for him anymore?
on the ?entitled?
question.A system that somehow takes into account this in-formation might correctly classify a difficult com-ment on the ?entitled?
question if it notices that thecomment was written by a user who commentedon both questions and it knows how the user votedon the ?dinner?
question.
We call the kind of con-straint described here a QuestionPair constraint.We might also exploit information from otherquestions by noticing that there are users whoshare similar attitudes on a wide variety of top-ics in our dataset.
We can gauge how often apair of users agree with each other by compar-ing their votes on every question on which theyhave both voted where their comments appear inthe training set.
So for example, if we see thattwo users have agreed on questions about GeorgeH.W.
Bush, Bill Clinton, and George W. Bush,we can guess that they will also agree on a ques-tion about Barack Obama.
Similarly, if they dis-agreed on all those questions, they are likely todisagree on the last question.
A system that takesinto account this kind of information could cor-rectly classify an otherwise difficult comment if itknows how another user voted on this question andalso knows how often the two users agree on otherquestions.
We call the kind of constraint describedhere a VoterPair constraint.In order to enforce both kinds of constraints,we introduce a variation of the label propagationalgorithm (Zhu and Ghahramani, 2002).
In ourversion of the label propagation algorithm, eachcomment in our dataset is represented by a nodein a graph.
Each node is associated with a proba-bility distribution indicating the likelihood that the1131comment belongs to each of its question?s possibleanswers.
Thus, when we initialize the graph, eachtraining set node?s probability distribution is set toreflect its comment?s actual label (with a proba-bility of 1 for the comment?s actual label and 0for each other answer), and each development ortest set node?s probability distribution is set to thevalue predicted by another classifier such as B2orB2+ Dem since the algorithm is not permittedto see the comment?s actual label.
Lines 7?12 inFigure 1 describe the graph?s initialization.Now that we have set up the graph?s nodes, weneed to explain how our graph?s edges work.
Aswe discussed earlier in this section, the edges inour graph will represent two kinds of soft con-straints.
Each edge allows one of a node?s neigh-bors to cast a vote (in the form of a probability dis-tribution over possible answers) for what it thinksthe node?s answer should be.
Let us call the com-ment node whose label we are trying to predict thetarget node and the comment node which casts thevote the source node.Our graph contains a QuestionPair edge be-tween any source and target comments written bythe same user.
Since a user cannot comment morethan once on any question, the source and targetcomments will occur in two different questions.
Inorder to determine how the source node votes overa QuestionPair edge, we need to calculate someprobabilities.
In particular, we need to determinethe probability that a user will vote for possibleanswer k in the target question QIgiven that shevoted for answer l in the source question QJ:P (QIk|QJl) =C(QIk,QJl)+??m?A(QI)(C(QIm,QJl)+?
)where C(QIn, QJl) is the number of users whovoted for answer n in QIand answer l in QJ, andA(QI) is the set of possible answers on QI.
Weset ?, the smoothing factor, to 10 since this valueworked well in earlier experiments.
The sourcenode S casts its vote on target node T for the prob-ability distribution given by:QPS,T(QIk) =?m?A(QJ)PS(QJm)P (QIk|QJm)where PS(QJm) is the probability currently asso-ciated with answer m in S?s question (QJ).The graph contains a VoterPair edge betweenany source and target nodes on the same questionif the users who posted these comments have bothvoted on at least one other question together andtheir comments on the other question(s) occurredin the training set.
To determine how the sourcenode votes over a VoterPair edge, we need to cal-culate the probability that the source and targetusers will agree on a generic issue:Pagr(US, UT) =Cagr(US,UT)+1Cagr(US,UT)+Cdis(US,UT)+2where Cagr(US, UT) is the number of questionson which users USand UTvoted for the sameanswer and both their comments occurred in thetraining set, Cdis(US, UT) is the number of ques-tions on which USand UTvoted for differentanswers where both their comments occurred inthe training set, and the +1 and +2 are used forsmoothing.
The probability distribution that thesource node S votes for on target node T is thengiven by:V PS,T(QIk) = PS(QIk)Pagr(US, UT)+?m?A(QI),m 6=k(PS(QIm))1?
Pagr(US, UT)|A(QI)| ?
1where PS(QIn) is the probability currently asso-ciated with answer n in the source node?s question(QI), and |A(QI)| is the number of possible an-swers on QI.
We divide the second term, whichdeals with disagreement, by |A(QI)| ?
1 because,even if we know that the target and source usersdisagreed on the answer to a particular questionand that the source user did not vote for answerk, there is only a 1|A(QI)|?1chance that the targetuser voted for answer k since there are |A(QI)|?1non-k answers to choose from.Now that we have described how edges areadded to the graph and how source comment nodesvote over the edges, we are ready to begin iterat-ing over the label propagation algorithm (line 13 inFigure 1).
For each iteration of the algorithm, weupdate each development or test set node?s answerprobability distribution by assigning it a weightedsum of (1) the initial probability distribution as-signed to the node, (2) the sum of the Question-Pair edges?
votes, and (3) the sum of the VoterPairedges?
votes (line 16 in Figure 1).
Upon comple-tion of the algorithm, if our soft constraints workas expected, the new labeling of comment nodesshould be more accurate than their initial labeling.We tune the parameters WI, WV, WQ, anditerations jointly by an exhaustive search of theparameter space to maximize classification accu-racy on the development set.
Each of the weightparameters is allowed to take one of the values 0,1, or 2, and the iteration parameter is allowed takeone of the values 0, 1, 2, 3, 4, 5.11321: LabelPropagation(Tr,D, Te, iterations,Wi,WV,WQ, I)2: Inputs:3: Tr,D, Te: Comments in Training, Development, and Test set4: iterations: The number of iterations to perform5: Wi,WV,WQ: Weights assigned to initial, VoterPair, and QuestionPair constraints6: I: Initial answer probability distribution for all comments.
Should reflect actual labels for training set comments andclassifier predictions for development and test set comments7: for all C ?
Tr ?D ?
Te do8: Create node representing C9: Cp?
IC10: // Cp: node C?s current probability distribution over possible answers11: // IC: initial answer probability distribution for comment C12: end for13: for j = 1 to iterations do14: for all node C ?
D ?
Te do15: Add all edges targeting node C16: Cp?
Norm(WIIC+ WV?kV Pk,C+ WQ?kQPk,C)17: // V Pk,C, QPk,C: kth VoterPair, and kth QuestionPair votes for node C18: Remove all edges targeting node C19: end for20: end forFigure 1: Our label propagation algorithm.One may be surprised to notice how we addedges to the graph in the algorithm only to deletethem three lines later (lines 15 and 18 in Figure 1).Though edges can be added at any point in the al-gorithm, one benefit of using the label propagationalgorithm is that it is simple enough that it is notnecessary store all the edges in memory at once.The only time we need to store an edge is when itstarget is being voted on.
This means that the labelpropagation algorithm can handle large datasetslike ours with huge numbers of nodes and edgeswithout being prohibitively space-expensive.6 Evaluation6.1 Experimental SetupWe mentioned in Section 3 that we split ourdataset of 997,379 comments into a test set com-prising about 20% of the dataset?s comments anda training and development set comprising somefraction of the remaining 80% of the comments.We actually split the data up like this five differenttimes so that each comment appears in an experi-ment?s test set exactly once.
In this way, throughthe use of five fold cross-validation, we can reportour results on the entire dataset.6.2 Results and DiscussionFigure 2a shows the accuracy of the predictionsmade by various systems.
First, let us compareour first and second baselines.
Recall that the firstbaseline (B1) predicts that all test comments willhave the same label as the majority of trainingcomments, and the second baseline?s (B2) predic-tions are the output of ME classifiers trained with ageneric feature set.
As we can see from the graph,at very small training set sizes, the standard set offeatures supplied to B2does little more than con-fuse the ME learner, as it performs slightly but notsignificantly worse6 than the first baseline whenthe training/development set comprises only 25%of the available data.
This is understandable, as25% of an average question?s available data is only42 comments, an extremely small number of ex-amples to learn from for most NLP tasks.
Clearlya better approach than the one provided by the sec-ond baseline is needed.
Though the average train-ing set sizes at the 50%, 75%, and 100% levels arestill relatively small, B2significantly outperformsB1at all these levels.The small improvement sizes yielded by B2may be attributable to some of the inherent dif-ficulties of the problem, particularly that (1) it iscomposed of so many (4,803) separate subprob-lems that it is impractical for us to tailor a uniquefeature set for each one, (2) the average question isassociated with a very small number of comments(about 208), making it difficult to train a reason-ably good classifier for any question, and (3) manyof the comments contain insufficient informationfor inferring the underlying votes.
Perhaps someof our proposed extensions to B2can help address6All significance tests are paired t-tests, with p < 0.05.Because we calculate a large number of significance results,the p values we report are obtained using Holm-Bonferronimultiple testing correction (Holm, 1979).113362636465666768697025  50  75  100Accuracy(%)Training/Development Set Size (%)B2+Dem+QPair+VPairB2+Dem+VPairB2+Dem+QPairB2+QPair+VPairB2+VPairB2+QPairB2+DemB2B1(a) Vote prediction.62636465666768697025  50  75  100Accuracy(%)Training/Development Set Size (%)B1+Dem+QPair+VPairB1+Dem+VPairB1+Dem+QPairB1+QPair+VPairB1+VPairB1+QPairB1+DemB1(b) Arbitrary User Vote Prediction.Figure 2: Five-fold cross-validation vote prediction learning curves.some of these problems.The first improvement we proposed involvedexploiting demographic features provided by usersto help with our prediction tasks.
When we com-bine Dem and B2?s feature sets, the resultingsystem (B2+ Dem) performs better than any ofthe systems discussed thus far at all four train-ing/development set size levels, yielding signifi-cant improvements over B2at all four levels.
Thisdemonstrates that our demographic features are auseful complement to a standard approach like theone used by B2.The second improvement we proposed involvedusing a variation of the label propagation algo-rithm to enforce QuestionPair constraints.
Ques-tionPair constraints, recall, allowed us to exploitthe observed voting patterns of users who votedin the training set on any particular pair of ques-tions.
These constraints were expected to improveour predictions for any user who voted on bothquestions when at least one of their votes appearedin the test set.
System B2+ QPair correspondsto following the algorithm in Figure 1, using sys-tem B2?s ME classifiers to initialize a label prop-agation graph, and then setting the VoterPair edgeweight (WV) to 0, thus allowing only Question-Pair constraints.
When we compare this system toB2, we see that the performance boost Question-Pair constraints give us over the baseline is consis-tently greater than the boost given by adding de-mographic features to it (B2+ Dem) across alltraining/development set sizes.
The improvementover B2is even significant at the 75% and 100%training/development set sizes.The last improvement we proposed involvedadding VoterPair constraints to the label propaga-tion graph.
Recall that VoterPair constraints al-lowed us to exploit how frequently we observedtwo users agreeing with each other to predictwhether they will agree on any question they bothvoted on.
System B2+V Pair corresponds to fol-lowing the label propagation algorithm using B2?sME classifiers to initialize the graph, then settingthe QuestionPair edge weight (WQ) to 0, thus al-lowing only VoterPair constraints.
The addition ofVoterPair constraints yields the largest significantimprovements over B2at all four levels, indicatingthat, in the absence of our other proposed improve-ments, VoterPair edge constraints are the most im-portant addition we can make to our baseline.While we have now shown that each of our pro-posed extensions yields significant improvementsover B2, this does not necessarily mean that eachone is useful in the presence of the others.
Forexample, it might be the case that QuestionPairconstraints and Demographic features correct thesame kinds of classification errors, and therefore itmay be sufficient to use either one or the other toobtain good results, but using both is unnecessary.To test how useful they are in each other?s pres-1134ence, we perform the following experiment.
First,we run the algorithm using all three improvements(B2+Dem+QPair+V Pair in Figure 2a).
Wethen run the same experiment three more times,each time removing one of the three extensions.By measuring how much performance decreaseswhen we remove each of the three improvements,we can determine whether each improvement pro-vides unique useful information, or whether the in-formation it provides is already being provided byone of the other improvements.To see what happens when we remove demo-graphic features from the full system, we need tocompare B2+Dem+QPair+V Pair and B2+QPair+V Pair in Figure 2a.
While the decreasein performance after removing demographic fea-tures was modest, the difference is neverthelesssignificant at all four training/development setsizes, suggesting that demographic features doprovide unique information to the system.By comparing line B2+ Dem + QPair +V Pair to line B2+Dem+V Pair, we can deter-mine the impact of QuestionPair constraints.
Re-moving QuestionPair constraints also had a mod-est impact on the full system?s performance, de-creasing accuracy at all four training/developmentset sizes, significantly so at the 50%, 75%, and100% levels.
Interestingly, the impact of Ques-tionPair constraints appears to grow with the train-ing set, while the demographic features appearto have a greater impact when the training set issmall.
We can see this by noting that the two linescross at around 55%.
This suggests that Question-Pair constraints are especially useful in problemswhere it is cheap to obtain a lot of training data,but in problems where the data has to be manuallyannotated, demographic features are more useful.Finally, we can compare line B2+ Dem +QPair + V Pair to line B2+ Dem + QPair tosee what happens when we remove VoterPair con-straints from our system.
This comparison illus-trates that VoterPair constraints are by far the mostimportant improvement we removed from the fullsystem, as removing them yielded large significantdecreases at all four levels.Though thus far we have only used it to analyzethe the contributions of different individual im-provements, the full system B2+Dem+QPair+V Pair is interesting in itself.
Of all the systemswe have constructed, it performs the best, yield-ing improvements of up to 5.18% and 3.88% whencompared to B1and B2respectively.
Its improve-ments over both baselines are statistically signifi-cant at all four training/development set sizes.6.3 Arbitrary User Vote PredictionOne interesting question that we have not yet ad-dressed is, is it possible to predict how a userwould vote on a question she has not yet seen?This problem is interesting because an averagequestion receives votes from only 0.2% of theusers in our dataset, and thus a system for predict-ing an arbitrary user?s vote would be able to pre-dict the votes of the other 99.8% of users.
A solu-tion to this prediction problem would have practi-cal applications in areas such as directed advertis-ing (e.g., if we could predict how a user would voteon the magazine question in Table 1, we wouldhave a better idea of what kinds of reading de-vices/services would interest her).We can mimic this problem with our datasetby treating the comment text associated with testvotes as unseen since we cannot expect an arbi-trary user to have commented on any particularquestion we are interested in7.
It does, however,make sense for us to expect our arbitrary user tohave provided some personal demographic infor-mation, and thus a system for making these typesof predictions could reasonably make use of de-mographic features.
Similarly, in this situation wewould expect to have knowledge of all users?
train-ing set voting histories.
Thus, it would also be rea-sonable for our system to exploit the QuestionPairand VoterPair constraints described in Section 5.Thus, to test how well our system performs on thistask, we repeat all experiments from the previoussection while replacing B2(which uses a ME clas-sifier trained on comment-based features) with B1(the most frequent baseline, which uses a ME clas-sifier trained using only a bias feature).
The resultsof these experiments are shown in Figure 2b.If we compare the results from B1to B1+Dem(which compliments B1?s bias feature with the de-mographic feature set), we notice that B1+Demis significantly worse than B1at all training setsizes.
This confirms our suspicion from the pre-7Although we are trying to mimic the situation in whichwe predict how an arbitrary user would vote on an arbitraryquestion, we caution that the vote data we train and evaluateon was not obtained from a set of arbitrary SodaHead users.
Itconsists only of votes from users who chose which questionsthey wanted to answer.
For this reason, the data we train andevaluate on for any question might not be a representativesample of SodaHead users as a whole.1135vious section that demographic features by them-selves serve only to confuse the learner, though wewill see in a moment that they are a helpful sup-plement to more sophisticated systems.We can evaluate QuestionPair constraints in thissetting by comparing the results from B1to B1+QPair.
B1+QPair consistently outperforms B1at all four training set sizes, significantly so at the75% and 100% levels, and thus QuestionPair con-straints are also a useful addition to our system.VoterPair constraints can be evaluated in thissetting by comparing B1to B1+ V Pair.
B1+V Pair significantly outperforms B1at all fourtraining set sizes, and from the graph it appearsto be our most beneficial improvement.To evaluate whether demographic features areuseful in the presence of the other improve-ments, we compare the full system, B1+Dem+QPair + V Pair, to its corresponding versionwithout demographic features, B1+ QPair +V Pair.
Though B1+ QPair + V Pair signif-icantly outperforms the full system at the 25%training set size, the full system significantly out-performs B1+ QPair + V Pair at the 75% and100% levels, indicating that in this setting, demo-graphic features are useful in the presence of alarge training set.We can evaluate the utility of QuestionPair con-straints in this setting by comparing the full systemto B1+Dem+ V Pair.
When we remove Ques-tionPair constraints, accuracy is consistently low-ered at all four training set sizes, significantly soat 50%, 75%, and 100%.
This tells us that Ques-tionPair constraints are useful in this setting.We can evaluate how useful VoterPair con-straints are by checking how much B1+ Dem +V Pair+QPair?s performance drops when we re-move VoterPair constraints from it, yielding B1+Dem + QPair.
Performance drops considerablyand significantly at all four training set sizes afterremoving VoterPair constraints, suggesting that inthis setting, VoterPair constraints are still the mostimportant of our proposed improvements.Finally, while we have already established thatall our proposed improvements can improve per-formance under both settings (comments visibleand comments invisible), it may be worthwhileto compare the two sets of experiments to deter-mine whether the comment features used in sys-tems with B2are useful.A casual inspection of the two figures showsthat, broadly, each system that uses comment-based features in Figure 2a tends to slightly out-perform the most comparable system in Figure 2b.At the low end of the curves, the two systems oftendiffer by about 1.0% in absolute accuracy, thoughat the high end, the difference tends to be muchsmaller, with the full system with comment fea-tures outperforming the full system without com-ment features by only 0.3%.
Since in this settingit is reasonable to assume a large training set, thislast result is the one we are most interested in, andit suggests that our full system?s performance doesnot suffer much due to the absence of commentfeatures.One final observation we can make is that, whencomments are not visible, demographic featuresappear to actively harm the performance of sys-tems trained on a small amount of data, thoughat larger training set sizes they are mostly help-ful.
We can tell this by comparing systems withdemographic features to systems without them inFigure 2b (e.g., by comparing B1+Dem+QPairto B1+QPair or B1+Dem+ V Pair to B1+V Pair) at the 25% training set size.
This is notthe case in the setting where comments are visi-ble, as we see that demographic features alwaysappear helpful in Figure 2a.
This reinforces thenotion that demographic features provide usefulinformation in general, but that they are by them-selves too sparsely available to do more than con-fuse the learner.
They need to be supplemented byother information sources in order for the learnerto draw correct conclusions.7 ConclusionWe examined the task of vote prediction on com-ments from the SodaHead website.
To address thistask, we exploited not only information extractedfrom the comments but also extra-textual informa-tion, including demographic information and twotypes of inter-comment constraints, QuestionPairconstraints and VoterPair constraints.
Our exper-iments involving 997,379 comments showed thateach of these extensions significantly improved abaseline that exploited only textual information,with VoterPair constraints being the most effectiveand demographic information being the least ef-fective.
When used in combination, they obtainedup to a 3.88% improvement in absolute accuracyover the baseline.
To stimulate research on thistask, we make our dataset publicly available.1136AcknowledgmentsWe thank the three anonymous reviewers for theirdetailed and insightful comments on an earlierdraft of this paper.
This work was supported inpart by NSF Grants IIS-1147644 and IIS-1219142.Any opinions, findings, conclusions or recommen-dations expressed in this paper are those of the au-thors and do not necessarily reflect the views or of-ficial policies, either expressed or implied, of NSF.ReferencesSitaram Asur and Bernardo A. Huberman.
2010.
Pre-dicting the future with social media.
In Proceedingsof the 2010 IEEE/WIC/ACM International Confer-ence on Web Intelligence and Intelligent Agent Tech-nology, pages 492?499.Mohit Bansal, Claire Cardie, and Lillian Lee.
2008.The power of negative thinking: Exploiting labeldisagreement in the min-cut classification frame-work.
In COLING 2008: Companion Volume:Posters, pages 15?18.Clinton Burfoot, Steven Bird, and Timothy Baldwin.2011.
Collective classification of congressionalfloor-debate transcripts.
In Proceedings of the 49thAnnual Meeting of the Association for Computa-tional Linguistics: Human Language Technologies,pages 1506?1515.John D. Burger, John Henderson, George Kim, andGuido Zarrella.
2011.
Gender discrimination ontwitter.
In Proceedings of the 2011 Conference onEmpirical Methods in Natural Language Process-ing, pages 1301?1309.Marie-Catherine de Marneffe, Bill Maccartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InProceedings of the Fifth International Conferenceon Language Resources and Evaluation, pages 449?454.Jean E. Fox Tree and Josef C. Schrock.
1999.
Dis-course markers in spontaneous speech: Oh what adifference an oh makes.
Journal of Memory andLanguage, 40:280?295.Jean E. Fox Tree and Josef C. Schrock.
2002.
Basicmeanings of you know and i mean.
Journal of Prag-matics, 34:427?447.Daniel Gayo-Avello, Panagiotis Takis Metaxas, andEni Mustafaraj.
2011.
Limits of electoral predic-tions using twitter.
In Proceedings of the Fifth In-ternational AAAI Conference on Weblogs and SocialMedia, pages 490?493.Martin Groen, Jan Noyes, and Frans Verstraten.
2010.The effect of substituting discourse markers on theirrole in dialogue.
Discourse Processes: A Multidis-ciplinary Journal, 47:388?420.Kazi Saidul Hasan and Vincent Ng.
2013.
Stanceclassification of ideological debates: Data, mod-els, features, and constraints.
In Proceedings ofthe Sixth International Joint Conference on NaturalLanguage Processing, pages 1348?1356.Sture Holm.
1979.
A simple sequentially rejectivemultiple test procedure.
Scandinavian Journal ofStatistics, 6:65?70.Yue Lu, Hongning Wang, ChengXiang Zhai, and DanRoth.
2012.
Unsupervised discovery of opposingopinion networks from forum discussions.
In Pro-ceedings of the 21st ACM International Conferenceon Information and Knowledge Management, pages1642?1646.Andrew Kachites McCallum.
2002.
Mallet: A ma-chine learning for language toolkit.
http://mallet.cs.umass.edu.Saif Mohammad and Tony Yang.
2011.
Tracking sen-timent in mail: How genders differ on emotionalaxes.
In Proceedings of the 2nd Workshop on Com-putational Approaches to Subjectivity and SentimentAnalysis, pages 70?79.Brendan O?Connor, Ramnath Balasubramanyan,Bryan R. Routledge, and Noah A. Smith.
2010.From tweets to polls: Linking text sentiment topublic opinion time series.
In Proceedings of theFourth International AAAI Conference on Weblogsand Social Media, pages 122?129.Delip Rao and David Yarowsky.
2010.
Detecting latentuser properties in social media.
In Proceedings ofthe NIPS workshop on Machine Learning for SocialNetworks.Swapna Somasundaran and Janyce Wiebe.
2010.
Rec-ognizing stances in ideological on-line debates.
InProceedings of the NAACL HLT 2010 Workshop onComputational Approaches to Analysis and Genera-tion of Emotion in Text, pages 116?124.Matt Thomas, Bo Pang, and Lillian Lee.
2006.
Get outthe vote: Determining support or opposition fromCongressional floor-debate transcripts.
In Proceed-ings of the 2006 Conference on Empirical Methodsin Natural Language Processing, pages 327?335.Andranik Tumasjan, Timm Sprenger, Philipp Sandner,and Isabell Welpe.
2010.
Predicting elections withtwitter: What 140 characters reveal about politicalsentiment.
In Proceedings of the Fourth Interna-tional AAAI Conference on Weblogs and Social Me-dia, pages 178?185.Marilyn Walker, Pranav Anand, Rob Abbott, and RickyGrant.
2012a.
Stance classification using dialogicproperties of persuasion.
In Proceedings of the 2012Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, pages 592?596.1137Marilyn A. Walker, Pranav Anand, Rob Abbott, JeanE.
Fox Tree, Craig Martell, and Joseph King.
2012b.That is your evidence?
: Classifying stance in on-line political debate.
Decision Support Systems,53(4):719?729.Yafang Wang, Bin Yang, Lizhen Qu, Marc Spaniol,and Gerhard Weikum.
2011.
Harvesting facts fromtextual web sources by constrained label propaga-tion.
In Proceedings of the 20th ACM InternationalConference on Information and Knowledge Man-agement, pages 837?846.Yafang Wang, Maximilian Dylla, Marc Spaniol, andGerhard Weikum.
2012.
Coupling label propaga-tion and constraints for temporal fact extraction.
InProceedings of the ACL 2012 Conference Short Pa-pers, pages 233?237.Yiming Yang and Jan O. Pedersen.
1997.
A compara-tive study on feature selection in text categorization.In Proceedings of the 14th International Conferenceon Machine Learning, pages 412?420.Xiaojin Zhu and Zoubin Ghahramani.
2002.
Learningfrom labeled and unlabeled data with label propaga-tion.
Technical Report CMU-CALD-02-107, CMUCALD.1138
