Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 579?586, Vancouver, October 2005. c?2005 Association for Computational LinguisticsEmotions from text: machine learning for text-based emotion predictionCecilia Ovesdotter Alm?Dept.
of LinguisticsUIUCIllinois, USAebbaalm@uiuc.eduDan RothDept.
of Computer ScienceUIUCIllinois, USAdanr@uiuc.eduRichard SproatDept.
of LinguisticsDept.
of Electrical Eng.UIUCIllinois, USArws@uiuc.eduAbstractIn addition to information, text con-tains attitudinal, and more specifically,emotional content.
This paper exploresthe text-based emotion prediction prob-lem empirically, using supervised machinelearning with the SNoW learning archi-tecture.
The goal is to classify the emo-tional affinity of sentences in the narra-tive domain of children?s fairy tales, forsubsequent usage in appropriate expres-sive rendering of text-to-speech synthe-sis.
Initial experiments on a preliminarydata set of 22 fairy tales show encourag-ing results over a na?
?ve baseline and BOWapproach for classification of emotionalversus non-emotional contents, with somedependency on parameter tuning.
Wealso discuss results for a tripartite modelwhich covers emotional valence, as wellas feature set alernations.
In addition, wepresent plans for a more cognitively soundsequential model, taking into considera-tion a larger set of basic emotions.1 IntroductionText does not only communicate informative con-tents, but also attitudinal information, includingemotional states.
The following reports on an em-pirical study of text-based emotion prediction.Section 2 gives a brief overview of the intendedapplication area, whereas section 3 summarizes re-lated work.
Next, section 4 explains the empiricalstudy, including the machine learning model, thecorpus, the feature set, parameter tuning, etc.
Sec-tion 5 presents experimental results from two classi-fication tasks and feature set modifications.
Section6 describes the agenda for refining the model, beforepresenting concluding remarks in 7.2 Application area: Text-to-speechNarrative text is often especially prone to havingemotional contents.
In the literary genre of fairytales, emotions such as HAPPINESS and ANGER andrelated cognitive states, e.g.
LOVE or HATE, becomeintegral parts of the story plot, and thus are of par-ticular importance.
Moreover, the story teller read-ing the story interprets emotions in order to orallyconvey the story in a fashion which makes the storycome alive and catches the listeners?
attention.In speech, speakers effectively express emotionsby modifying prosody, including pitch, intensity,and durational cues in the speech signal.
Thus, inorder to make text-to-speech synthesis sound as nat-ural and engaging as possible, it is important to con-vey the emotional stance in the text.
However, thisimplies first having identified the appropriate emo-tional meaning of the corresponding text passage.Thus, an application for emotional text-to-speechsynthesis has to solve two basic problems.
First,what emotion or emotions most appropriately de-scribe a certain text passage, and second, given a textpassage and a specified emotional mark-up, how torender the prosodic contour in order to convey theemotional content, (Cahn, 1990).
The text-basedemotion prediction task (TEP) addresses the first ofthese two problems.5793 Previous workFor a complete general overview of the field of af-fective computing, see (Picard, 1997).
(Liu, Lieber-man and Selker, 2003) is a rare study in text-based inference of sentence-level emotional affin-ity.
The authors adopt the notion of basic emotions,cf.
(Ekman, 1993), and use six emotion categories:ANGER, DISGUST, FEAR, HAPPINESS, SADNESS,SURPRISE.
They critique statistical NLP for beingunsuccessful at the small sentence level, and insteaduse a database of common-sense knowledge and cre-ate affect models which are combined to form a rep-resentation of the emotional affinity of a sentence.At its core, the approach remains dependent on anemotion lexicon and hand-crafted rules for concep-tual polarity.
In order to be effective, emotion recog-nition must go beyond such resources; the authorsnote themselves that lexical affinity is fragile.
Themethod was tested on 20 users?
preferences for anemail-client, based on user-composed text emailsdescribing short but colorful events.
While the userspreferred the emotional client, this evaluation doesnot reveal emotion classification accuracy, nor howwell the model generalizes on a large data set.Whereas work on emotion classification fromthe point of view of natural speech and human-computer dialogues is fairly extensive, e.g.
(Scherer,2003), (Litman and Forbes-Riley, 2004), this ap-pears not to be the case for text-to-speech synthe-sis (TTS).
A short study by (Sugimoto et al, 2004)addresses sentence-level emotion recognition forJapanese TTS.
Their model uses a composition as-sumption: the emotion of a sentence is a function ofthe emotional affinity of the words in the sentence.They obtain emotional judgements of 73 adjectivesand a set of sentences from 15 human subjects andcompute words?
emotional strength based on the ra-tio of times a word or a sentence was judged to fallinto a particular emotion bucket, given the numberof human subjects.
Additionally, they conducted aninteractive experiment concerning the acoustic ren-dering of emotion, using manual tuning of prosodicparameters for Japanese sentences.
While the au-thors actually address the two fundamental problemsof emotional TTS, their approach is impractical andmost likely cannot scale up for a real corpus.
Again,while lexical items with clear emotional meaning,such as happy or sad, matter, emotion classifica-tion probably needs to consider additional inferencemechanisms.
Moreover, a na?
?ve compositional ap-proach to emotion recognition is risky due to simplelinguistic facts, such as context-dependent seman-tics, domination of words with multiple meanings,and emotional negation.Many NLP problems address attitudinal mean-ing distinctions in text, e.g.
detecting subjectiveopinion documents or expressions, e.g.
(Wiebe etal, 2004), measuring strength of subjective clauses(Wilson, Wiebe and Hwa, 2004), determining wordpolarity (Hatzivassiloglou and McKeown, 1997) ortexts?
attitudinal valence, e.g.
(Turney, 2002), (Bai,Padman and Airoldi, 2004), (Beineke, Hastie andVaithyanathan, 2003), (Mullen and Collier, 2003),(Pang and Lee, 2003).
Here, it suffices to say thatthe targets, the domain, and the intended applicationdiffer; our goal is to classify emotional text passagesin children?s stories, and eventually use this infor-mation for rendering expressive child-directed sto-rytelling in a text-to-speech application.
This can beuseful, e.g.
in therapeutic education of children withcommunication disorders (van Santen et al, 2003).4 Empirical studyThis part covers the experimental study with a for-mal problem definition, computational implementa-tion, data, features, and a note on parameter tuning.4.1 Machine learning modelDetermining emotion of a linguistic unit can becast as a multi-class classification problem.
Forthe flat case, let T denote the text, and s an em-bedded linguistic unit, such as a sentence, wheres ?
T .
Let k be the number of emotion classes E ={em1, em2, .., emk}, where em1 denotes the specialcase of neutrality, or absence of emotion.
The goalis to determine a mapping function f : s ?
emi,such that we obtain an ordered labeled pair (s, emi).The mapping is based on F = {f1, f2, .., fn}, whereF contains the features derived from the text.Furthermore, if multiple emotion classes cancharacterize s, then given E?
?
E, the target of themapping function becomes the ordered pair (s,E?
).Finally, as further discussed in section 6, the hier-archical case of label assignment requires a sequen-580tial model that further defines levels of coarse ver-sus fine-grained classifiers, as done by (Li and Roth,2002) for the question classification problem.4.2 ImplementationWhereas our goal is to predict finer emotional mean-ing distinctions according to emotional categories inspeech; in this study, we focus on the basic task ofrecognizing emotional passages and on determiningtheir valence (i.e.
positive versus negative) becausewe currently do not have enough training data to ex-plore finer-grained distinctions.
The goal here is toget a good understanding of the nature of the TEPproblem and explore features which may be useful.We explore two cases of flat classification, us-ing a variation of the Winnow update rule imple-mented in the SNoW learning architecture (Carl-son et al, 1999),1 which learns a linear classifierin feature space, and has been successful in sev-eral NLP applications, e.g.
semantic role labeling(Koomen, Punyakanok, Roth and Yih, 2005).
Inthe first case, the set of emotion classes E consistsof EMOTIONAL versus non-emotional or NEUTRAL,i.e.
E = {N,E}.
In the second case, E has beenincremented with emotional distinctions accordingto the valence, i.e.
E = {N,PE,NE}.
Experi-ments used 10-fold cross-validation, with 90% trainand 10% test data.24.3 DataThe goal of our current data annotation project isto annotate a corpus of approximately 185 childrenstories, including Grimms?, H.C. Andersen?s and B.Potter?s stories.
So far, the annotation process pro-ceeds as follows: annotators work in pairs on thesame stories.
They have been trained separately andwork independently in order to avoid any annota-tion bias and get a true understanding of the taskdifficulty.
Each annotator marks the sentence levelwith one of eight primary emotions, see table 1, re-flecting an extended set of basic emotions (Ekman,1993).
In order to make the annotation process morefocused, emotion is annotated from the point of viewof the text, i.e.
the feeler in the sentence.
While theprimary emotions are targets, the sentences are also1Available from http://l2r.cs.uiuc.edu/?cogcomp/2Experiments were also run for Perceptron, however the re-sults are not included.
Overall, Perceptron performed worse.marked for other affective contents, i.e.
backgroundmood, secondary emotions via intensity, feeler, andtextual cues.
Disagreements in annotations are re-solved by a second pass of tie-breaking by the firstauthor, who chooses one of the competing labels.Eventually, the completed annotations will be madeavailable.Table 1: Basic emotions used in annotationAbbreviation Emotion classA ANGRYD DISGUSTEDF FEARFULH HAPPYSa SADSu+ POSITIVELY SURPRISEDSu- NEGATIVELY SURPRISEDEmotion annotation is hard; interannotator agree-ment currently range at ?
= .24 ?
.51, with the ra-tio of observed annotation overlap ranging between45-64%, depending on annotator pair and stories as-signed.
This is expected, given the subjective natureof the annotation task.
The lack of a clear defini-tion for emotion vs. non-emotion is acknowledgedacross the emotion literature, and contributes to dy-namic and shifting annotation targets.
Indeed, acommon source of confusion is NEUTRAL, i.e.
de-ciding whether or not a sentence is emotional ornon-emotional.
Emotion perception also depends onwhich character?s point-of-view the annotator takes,and on extratextual factors such as annotator?s per-sonality or mood.
It is possible that by focusingmore on the training of annotator pairs, particularlyon joint training, agreement might improve.
How-ever, that would also result in a bias, which is prob-ably not preferable to actual perception.
Moreover,what agreement levels are needed for successful ex-pressive TTS remains an empirical question.The current data set consisted of a preliminary an-notated and tie-broken data set of 1580 sentence, or22 Grimms?
tales.
The label distribution is in table2.
NEUTRAL was most frequent with 59.94%.Table 2: Percent of annotated labelsA D F H12.34% 0.89% 7.03% 6.77%N SA SU+ SU.-59.94% 7.34% 2.59% 3.10%581Table 3: % EMOTIONAL vs.
NEUTRAL examplesE N40.06% 59.94%Table 4: % POSITIVE vs.
NEGATIVE vs. NEUTRALPE NE N9.87% 30.19% 59.94%Next, for the purpose of this study, all emotionalclasses, i.e.
A, D, F, H, SA, SU+, SU-, were com-bined into one emotional superclass E for the firstexperiment, as shown in table 3.
For the second ex-periment, we used two emotional classes, i.e.
pos-itive versus negative emotions; PE={H, SU+} andNE={A, D, F, SA, SU-}, as seen in table 4.4.4 Feature setThe feature extraction was written in python.
SNoWonly requires active features as input, which resultedin a typical feature vector size of around 30 features.The features are listed below.
They were imple-mented as boolean values, with continuous valuesrepresented by ranges.
The ranges generally over-lapped, in order to get more generalization coverage.1.
First sentence in story2.
Conjunctions of selected features (see below)3.
Direct speech (i.e.
whole quote) in sentence4.
Thematic story type (3 top and 15 sub-types)5.
Special punctuation (!
and ?)6.
Complete upper-case word7.
Sentence length in words (0-1, 2-3, 4-8, 9-15,16-25, 26-35, >35)8.
Ranges of story progress (5-100%, 15-100%,80-100%, 90-100%)9.
Percent of JJ, N, V, RB (0%, 1-100%, 50-100%, 80-100%)10.
V count in sentence, excluding participles (0-1,0-3, 0-5, 0-7, 0-9, > 9)11.
Positive and negative word counts ( ?
1, ?
2,?
3, ?
4, ?
5, ?
6)12.
WordNet emotion words13.
Interjections and affective words14.
Content BOW: N, V, JJ, RB words by POSFeature conjunctions covered pairings of counts ofpositive and negative words with range of storyprogress or interjections, respectively.Feature groups 1, 3, 5, 6, 7, 8, 9, 10 and 14 are ex-tracted automatically from the sentences in the sto-ries; with the SNoW POS-tagger used for features9, 10, and 14.
Group 10 reflects how many verbsare active in a sentence.
Together with the quotationand punctuation, verb domination intends to capturethe assumption that emotion is often accompaniedby increased action and interaction.
Feature group4 is based on Finish scholar Antti Aarne?s classesof folk-tale types according to their informative the-matic contents (Aarne, 1964).
The current taleshave 3 top story types (ANIMAL TALES, ORDINARYFOLK-TALES, and JOKES AND ANECDOTES), and15 subtypes (e.g.
supernatural helpers is a subtypeof the ORDINARY FOLK-TALE).
This feature intendsto provide an idea about the story?s general affectivepersonality (Picard, 1997), whereas the feature re-flecting the story progress is hoped to capture thatsome emotions may be more prevalent in certainsections of the story (e.g.
the happy end).For semantic tasks, words are obviously impor-tant.
In addition to considering ?content words?, wealso explored specific word lists.
Group 11 uses2 lists of 1636 positive and 2008 negative words,obtained from (Di Cicco et al, online).
Group 12uses lexical lists extracted from WordNet (Fellbaum,1998), on the basis of the primary emotion wordsin their adjectival and nominal forms.
For the ad-jectives, Py-WordNet?s (Steele et al, 2004) SIMI-LAR feature was used to retrieve similar items ofthe primary emotion adjectives, exploring one addi-tional level in the hierarchy (i.e.
similar items of allsenses of all words in the synset).
For the nouns andany identical verbal homonyms, synonyms and hy-ponyms were extracted manually.3 Feature group 13used a short list of 22 interjections collected manu-ally by browsing educational ESL sites, whereas theaffective word list of 771 words consisted of a com-bination of the non-neutral words from (Johnson-Laird and Oatley, 1989) and (Siegle, online).
Only asubset of these lexical lists actually occurred.43Multi-words were transformed to hyphenated form.4At this point, neither stems and bigrams nor a list of ono-matopoeic words contribute to accuracy.
Intermediate resourceprocessing inserted some feature noise.582The above feature set is henceforth referred to asall features, whereas content BOW is just group 14.The content BOW is a more interesting baseline thanthe na?
?ve one, P(Neutral), i.e.
always assigning themost likely NEUTRAL category.
Lastly, emotionsblend and transform (Liu, Lieberman and Selker,2003).
Thus, emotion and background mood of im-mediately adjacent sentences, i.e.
the sequencing,seems important.
At this point, it is not implementedautomatically.
Instead, it was extracted from themanual emotion and mood annotations.
If sequenc-ing seemed important, an automatic method usingsequential target activation could be added next.4.5 Parameter tuningThe Winnow parameters that were tuned includedpromotional ?, demotional ?, activation threshold?, initial weights ?, and the regularization parame-ter, S, which implements a margin between positiveand negative examples.
Given the currently fairlylimited data, results from 2 alternative tuning meth-ods, applied to all features, are reported.?
For the condition called sep-tune-eval, 50%of the sentences were randomly selected andset aside to be used for the parameter tuningprocess only.
Of this subset, 10% were subse-quently randomly chosen as test set with the re-maining 90% used for training during the auto-matic tuning process, which covered 4356 dif-ferent parameter combinations.
Resulting pa-rameters were: ?
= 1.1, ?
= 0.5, ?
= 5,?
= 1.0, S = 0.5.
The remaining half ofthe data was used for training and testing in the10-fold cross-validation evaluation.
(Also, notethe slight change for P(Neutral) in table 5, dueto randomly splitting the data.)?
Given that the data set is currently small, for thecondition named same-tune-eval, tuning wasperformed automatically on all data using aslightly smaller set of combinations, and thenmanually adjusted against the 10-fold cross-validation process.
Resulting parameters were:?
= 1.2, ?
= 0.9, ?
= 4, ?
= 1, S = 0.5.
Alldata was used for evaluation.Emotion classification was sensitive to the selectedtuning data.
Generally, a smaller tuning set resultedin pejorative parameter settings.
The random selec-tion could make a difference, but was not explored.5 Results and discussionThis section first presents the results from exper-iments with the two different confusion sets de-scribed above, as well as feature experimentation.5.1 Classification resultsAverage accuracy from 10-fold cross validation forthe first experiment, i.e.
classifying sentences as ei-ther NEUTRAL or EMOTIONAL, are included in ta-ble 5 and figure 1 for the two tuning conditions onthe main feature sets and baselines.
As expected,Table 5: Mean classification accuracy: N vs. E, 2 conditionssame-tune-eval sep-tune-evalP(Neutral) 59.94 60.05Content BOW 61.01 58.30All features except BOW 64.68 63.45All features 68.99 63.31All features + sequencing 69.37 62.94degree of success reflects parameter settings, bothfor content BOW and all features.
Nevertheless, un-der these circumstances, performance above a na?
?vebaseline and a BOW approach is obtained.
More-over, sequencing shows potential for contributingin one case.
However, observations also point tothree issues: first, the current data set appears tobe too small.
Second, the data is not easily separa-ble.
This comes as no surprise, given the subjectivenature of the task, and the rather low interannota-tor agreement, reported above.
Moreover, despitethe schematic narrative plots of children?s stories,tales still differ in their overall affective orientation,which increases data complexity.
Third and finally,the EMOTION class is combined by basic emotionlabels, rather than an original annotated label.More detailed averaged results from 10-foldcross-validation are included in table 6 using allfeatures and the separated tuning and evaluationdata condition sep-tune-eval.
With these parame-ters, approximately 3% improvement in accuracyover the na?
?ve baseline P(Neutral) was recorded,and 5% over the content BOW, which obviously didpoorly with these parameters.
Moreover, precision is5830 10 20 30 40 50 60 70same-tune-evalsep-tune-evalTuning sets% AccuracyP(Neutral) Content BOWAll features except BOW All featuresAll features + sequencingFigure 1: Accuracy under different conditions (in %)Table 6: Classifying N vs. E (all features, sep-tune-eval)Measure N EAveraged accuracy 0.63 0.63Averaged error 0.37 0.37Averaged precision 0.66 0.56Averaged recall 0.75 0.42Averaged F-score 0.70 0.47higher than recall for the combined EMOTION class.In comparison, with the same-tune-eval procedure,the accuracy improved by approximately 9% overP(Neutral) and by 8% over content BOW.In the second experiment, the emotion categorywas split into two classes: emotions with positiveversus negative valence.
The results in terms of pre-cision, recall, and F-score are included in table 7, us-ing all features and the sep-tune-eval condition.
Thedecrease in performance for the emotion classes mir-rors the smaller amounts of data available for eachclass.
As noted in section 4.3, only 9.87% of thesentences were annotated with a positive emotion,and the results for this class are worse.
Thus, perfor-mance seems likely to improve as more annotatedstory data becomes available; at this point, we areexperimenting with merely around 12% of the totaltexts targeted by the data annotation project.5.2 Feature experimentsEmotions are poorly understood, and it is espe-cially unclear which features may be important fortheir recognition from text.
Thus, we experimentedTable 7: N, PE, and NE (all features, sep-tune-eval)N NE PEAveraged precision 0.64 0.45 0.13Averaged recall 0.75 0.27 0.19Averaged F-score 0.69 0.32 0.13Table 8: Feature group membersWord lists interj., WordNet, affective lists, pos/negSyntactic length ranges, % POS, V-count rangesStory-related % story-progress, 1st sent., story typeOrthographic punctuation, upper-case words, quoteConjunctions Conjunctions with pos/negContent BOW Words (N,V,Adj, Adv)with different feature configurations.
Starting withall features, again using 10-fold cross-validation forthe separated tuning-evaluation condition sep-tune-eval, one additional feature group was removed un-til none remained.
The feature groups are listed intable 8.
Figure 2 on the next page shows the accu-racy at each step of the cumulative subtraction pro-cess.
While some feature groups, e.g.
syntactic, ap-peared less important, the removal order mattered;e.g.
if syntactic features were removed first, accu-racy decreased.
This fact also illustrated that fea-tures work together; removing any group degradedperformance because features interact and there isno true independence.
It was observed that fea-tures?
contributions were sensitive to parameter tun-ing.
Clearly, further work on developing featureswhich fit the TEP problem is needed.6 Refining the modelThis was a ?first pass?
of addressing TEP for TTS.At this point, the annotation project is still on-going,and we only had a fairly small data set to draw on.Nevertheless, results indicate that our learning ap-proach benefits emotion recognition.
For example,the following instances, also labeled with the samevalence by both annotators, were correctly classifiedboth in the binary (N vs. E) and the tripartite polar-ity task (N, NE, PE), given the separated tuning andevaluation data condition, and using all features:(1a) E/NE: Then he offered the dwarfs money, and prayed andbesought them to let him take her away; but they said, ?We willnot part with her for all the gold in the world.
?584Cumulative removal of feature groups61.8163.3162.5757.9558.3058.9359.56556065All features- Word lists- Syntactic- Story-related- Orthographic- Conjunctions- Content words% AccuracyAll features P(Neutral) BOWFigure 2: Averaged effect of feature group removal, using sep-tune-eval(1b) N: And so the little girl really did grow up; her skin was aswhite as snow, her cheeks as rosy as the blood, and her hair asblack as ebony; and she was called Snowdrop.
(2a) E/NE: ?Ah,?
she answered, ?have I not reason to weep?
(2b) N: Nevertheless, he wished to try him first, and took a stonein his hand and squeezed it together so that water dropped outof it.Cases (1a) and (1b) are from the well-known FOLKTALE Snowdrop, also called Snow White.
(1a)and (1b) are also correctly classified by the sim-ple content BOW approach, although our approachhas higher prediction confidence for E/NE (1a); italso considers, e.g.
direct speech, a fairly high verbcount, advanced story progress, connotative wordsand conjunctions thereof with story progress fea-tures, all of which the BOW misses.
In addition, thesimple content BOW approach makes incorrect pre-dictions at both the bipartite and tripartite levels forexamples (2a) and (2b) from the JOKES AND ANEC-DOTES stories Clever Hans and The Valiant LittleTailor, while our classifier captures the affective dif-ferences by considering, e.g.
distinctions in verbcount, interjection, POS, sentence length, connota-tions, story subtype, and conjunctions.Next, we intend to use a larger data set to conducta more complete study to establish mature findings.We also plan to explore finer emotional meaning dis-tinctions, by using a hierarchical sequential modelwhich better corresponds to different levels of cog-nitive difficulty in emotional categorization by hu-mans, and to classify the full set of basic level emo-tional categories discussed in section 4.3.
Sequentialmodeling of simple classifiers has been successfullyemployed to question classification, for example by(Li and Roth, 2002).
In addition, we are workingon refining and improving the feature set, and givenmore data, tuning can be improved on a sufficientlylarge development set.
The three subcorpora in theannotation project can reveal how authorship affectsemotion perception and classification.Moreover, arousal appears to be an importantdimension for emotional prosody (Scherer, 2003),especially in storytelling (Alm and Sproat, 2005).Thus, we are planning on exploring degrees of emo-tional intensity in a learning scenario, i.e.
a prob-lem similar to measuring strength of opinion clauses(Wilson, Wiebe and Hwa, 2004).Finally, emotions are not discrete objects; ratherthey have transitional nature, and blend and overlapalong the temporal dimension.
For example, (Liu,Lieberman and Selker, 2003) include parallel esti-mations of emotional activity, and include smooth-585ing techniques such as interpolation and decay tocapture sequential and interactive emotional activity.Observations from tales indicate that some emotionsare more likely to be prolonged than others.7 ConclusionThis paper has discussed an empirical study of thetext-based emotion prediction problem in the do-main of children?s fairy tales, with child-directed ex-pressive text-to-speech synthesis as goal.
Besidesreporting on encouraging results in a first set of com-putational experiments using supervised machinelearning, we have set forth a research agenda fortackling the TEP problem more comprehensively.8 AcknowledgmentsWe are grateful to the annotators, in particular A.Rasmussen and S. Siddiqui.
We also thank twoanonymous reviewers for comments.
This work wasfunded by NSF under award ITR-#0205731, and NSITR IIS-0428472.
The annotation is supported byUIUC?s Research Board.
The authors take sole re-sponsibility for the work.ReferencesAntti Aarne.
1964.
The Types of the Folk-Tale: a Classificationand Bibliography.
Helsinki: Suomalainen Tiedeakatemia.Cecilia O. Alm, and Richard Sproat.
2005.
Perceptions of emo-tions in expressive storytelling.
INTERSPEECH 2005.Xue Bai, Rema Padman, and Edoardo Airoldi.
2004.
Sen-timent extraction from unstructured text using tabu search-enhanced Markov blankets.
In MSW2004, Seattle.Philip Beineke, Trevor Hastie, and Shivakumar Vaithyanathan.2004.
The sentimental factor: improving review classifi-cation via human-provided information.
In Proceedings ofACL, 263?270.Janet Cahn.
1990.
The generation of affect in synthesizedSpeech.
Journal of the American Voice I/O Society, 8:1?19.Andrew Carlson, Chad Cumby, Nicholas Rizzolo, Jeff Rosen,and Dan Roth.
1999.
The SNoW Learning Architecture.Technical Report UIUCDCS-R-99-2101, UIUC Comp.
Sci.Stacey Di Cicco et al General Inquirer Pos./Neg.
listshttp://www.webuse.umd.edu:9090/Paul Ekman.
1993.
Facial expression and emotion.
AmericanPsychologist, 48(4), 384?392.Christiane Fellbaum, Ed.
1998.
WordNet: An Electronic Lexi-cal Database.
MIT Press, Cambridge, Mass.Vasileios Hatzivassiloglou, and Kathleen McKeown.
1997.Predicting the semantic orientation of adjectives.
In Pro-ceedings of ACL, 174?181.Philip Johnson-Laird, and Keith Oatley.
1989.
The languageof emotions: an analysis of a semantic field.
Cognition andEmotion, 3:81?123.Peter Koomen, Vasin Punyakanok, Dan Roth, and Wen-tau Yih.2005.
Generalized inference with multiple semantic role la-beling systems.
In Proceedings of the Annual Conference onComputational Language Learning (CoNLL), 181?184.Diane Litman, and Kate Forbes-Riley.
2004.
Predicting stu-dent emotions in computer-human tutoring dialogues.
InProceedings of ACL, 351?358.Xin Li, and Dan Roth.
2002.
Learning question classifiers: therole of semantic information.
In Proc.
International Confer-ence on Computational Linguistics (COLING), 556?562.Hugo Liu, Henry Lieberman, and Ted Selker.
2003.
A model oftextual affect sensing using real-world knowledge.
In ACMConference on Intelligent User Interfaces, 125?132.Tony Mullen, and Nigel Collier.
2004.
Sentiment analy-sis using support vector machines with diverse informationsources.
In Proceedings of EMNLP, 412?418.Bo Pang, and Lillian Lee.
2004.
A sentimental education: sen-timent analysis using subjectivity summarization based onminimum cuts.
In Proceedings of ACL, 271?278.Rosalind Picard.
1997.
Affective computing.
MIT Press, Cam-bridge, Mass.Dan Roth.
1998.
Learning to resolve natural language ambigu-ities: a unified approach.
In AAAI, 806?813.Klaus Scherer.
2003.
Vocal communication of emotion: areview of research paradigms.
Speech Commununication,40(1-2):227?256.Greg Siegle.
The Balanced Affective Word Listhttp://www.sci.sdsu.edu/CAL/wordlist/words.prnOliver Steele et al Py-WordNethttp://osteele.com/projects/pywordnet/Futoshi Sugimoto et al 2004.
A method to classify emotionalexpressions of text and synthesize speech.
In IEEE, 611?614.Peter Turney.
2002.
Thumbs up or thumbs down?
Semanticorientation applied to unsupervised classification of reviews.In Proceedings of ACL, 417?424.Jan van Santen et al 2003.
Applications of computer gen-erated expressive speech for communication disorders.
InEUROSPEECH 2003, 1657?1660.Janyce Wiebe et al 2004.
Learning subjective language.
Jour-nal of Computational Linguistics, 30(3):277?308.Theresa Wilson, Janyce Wiebe, and Rebecca Hwa.
2004.
Justhow mad are you?
Finding strong and weak opinion clauses.In Proceedings of the Nineteenth National Conference on Ar-tificial Intelligence (AAAI), 761?769.586
