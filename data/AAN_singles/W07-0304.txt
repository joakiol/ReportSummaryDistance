Bridging the Gap: Academic and Industrial Research in Dialog Technologies Workshop Proceedings, pages 25?31,NAACL-HLT, Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsTechnical Support Dialog Systems:Issues, Problems, and SolutionsKate Acomb, Jonathan Bloom, Krishna Dayanidhi, PhillipHunter, Peter Krogh, Esther Levin, Roberto PieracciniSpeechCycle535 W 34th StreetNew York, NY 10001{kate,jonathanb,krishna,phillip,peter,roberto}@speechcycle.comesther@spacegate.comAbstractThe goal of this paper is to give a descriptionof the state of the art, the issues, the problems,and the solutions related to industrial dialogsystems for the automation of technical sup-port.
After a general description of the evolu-tion of the spoken dialog industry, and thechallenges in the development of technicalsupport applications, we will discuss two spe-cific problems through a series of experimentalresults.
The first problem is the identificationof the call reason, or symptom, from looselyconstrained user utterances.
The second is theuse of data for the experimental optimizationof the Voice User Interface (VUI).1 IntroductionSince the beginning of the telephony spoken dialogindustry, in the mid 1990, we have been witnessingthe evolution of at least three generations of sys-tems.
What differentiates each generation is notonly the increase of complexity, but also the dif-ferent architectures used.
Table 1 provides a sum-mary of the features that distinguish eachgeneration.
The early first generation systems weremostly informational, in that they would requiresome information from the user, and would pro-vide information in return.
Examples of those sys-tems, mostly developed during the mid and late1990s, are package tracking, simple financial ap-plications, and flight status information.
At thetime there were no standards for developing dialogsystems, (VoiceXML 1.0 was published as a rec-ommendation in year 2000) and thus the first gen-eration dialog applications were implemented onproprietary platforms, typically evolutions of exist-ing touch-tone IVR (Interactive Voice Response)architectures.Since the early developments, spoken dialog sys-tems were implemented as a graph, called call-flow.
The nodes of the call-flow typically representactions performed by the system and the arcs rep-resent an enumeration of the possible outcomes.Playing a prompt and interpreting the user re-sponse through a speech recognition grammar is atypical action.
Dialog modules (Barnard et al,1999) were introduced in order to reduce the com-plexity and increase reusability of call-flows.
ADialog Module (or DM) is defined as a call-flowobject that encapsulates many of the interactionsneeded for getting one piece of information fromthe user, including retries, timeout handling, dis-ambiguation, etc.
Modern commercial dialog sys-tems use DMs as their active call-flow nodes.The number of DMs in a call-flow is generally anindication of the application complexity.
First gen-eration applications showed a range of complexityof a few to tens of DMs, typically spanning a fewturns of interaction.The dialog modality is another characterization ofapplications.
Early applications supported strictdirected dialog interaction, meaning that at each25turn the system would direct the user by proposinga finite?and typically small?number of choices.That would also result in a limited grammar or vo-cabulary at each turn.The applications of the second generation weretypically transactional, in the sense that they couldperform a transaction on behalf of the user, likemoving funds between bank accounts, tradingstocks, or buying tickets.
Most of those applica-tions were developed using the new standards,typically as collections of VoiceXML documents.The complexity moved to the range of dozens ofdialog modules, spanning a number of turns of in-teractions of the order of ten or more.
At the sametime, some of the applications started using a tech-nology known as Statistical Spoken Language Un-derstanding, or SSLU (Gorin et al, 1997, Chu-Carroll et al, 1999, Goel et al 2005), for mappingloosely constrained user utterances to a finite num-ber of pre-defined semantic categories.
The naturallanguage modality?as opposed to directed dia-log?was initially used mostly for call-routing, i.e.to route calls to the appropriate call center basedon a more or less lengthy description of the reasonfor the call by the user.While the model behind the first and second gen-erations of dialog applications can be described bythe form-filling paradigm, and the interaction fol-lows a pre-determined simple script, the systems ofthe third generation have raised to a qualitativelydifferent level of complexity.
Problem solving ap-plications, like customer care, help desk, and tech-nical support, are characterized by a level ofcomplexity ranging in the thousands of DMs, for anumber of turns of dynamic interaction that canreach into the dozens.
As the sophistication of theapplications evolved, so did the system architec-ture by moving the logic from the client(VoiceXML browser, or voice-browser) to theserver (Pieraccini and Huerta, 2005).
More andmore system are today based on generic dialogapplication server which interprets a dialog speci-fication described by a?typically proprietary?markup language and serve the voice-browser withdynamically generated VoiceXML documents.Finally, the interaction modality of the third gen-eration systems is moving from the strictly directeddialog application, to directed dialog, with somenatural language (SSLU) turns, and some limitedmixed-initiative (i.e.
the possibility for the user tochange the course of the dialog by making an un-solicited request).2 Technical Support ApplicationsToday, automated technical support systems areamong the most complex types of dialog applica-tions.
The advantage of automation is clear, espe-cially for high-volume services like broadband-internet, entertainment (cable or satellite TV), andtelephony.
When something goes wrong with theservice, the only choice for subscribers is to call atechnical support center.
Unfortunately, staffing acall center with enough agents trained to help solveeven the most common problems results in pro-hibitive costs for the provider, even when out-sourcing to less costly locations End users oftenexperience long waiting times and poor servicefrom untrained agents.
With the magnitude of thedaily increase in the number of subscribers of thoseservices, the situation with human agents is boundto worsen.
Automation and self-service can, anddoes, help reduce the burden constituted by themost frequent call reasons, and resort to humanagents only for the most difficult and less commonproblems.GENERATIONFIRST SECOND THIRDTime Period 1994-2001 2000-2005 2004-todayType of Ap-plication InformationalTransac-tionalProblemSolvingExamplesPackageTracking,Flight StatusBanking,StockTrading,Train Res-ervationCustomerCare,TechnicalSupport,Help Desk.Architecture ProprietaryStaticVoiceXMLDynamicVoiceXMLComplexity(Number ofDMs) 10 100 1000InteractionTurns  few 10 10-100InteractionModality directeddirected +naturallanguage(SSLU)directed +naturallanguage(SSLU) +limitedmixed initia-tiveTable 1: Evolution of spoken dialog systems.26However, automating technical support is particu-larly challenging for several reasons.
Among them:- Troubleshooting knowledge is not readilyavailable in a form that can be used forautomation.
Most often it is based on theidiosyncratic experience of the individualagents.- End users are typically in a somewhatemotionally altered state?something forwhich they paid and that is supposed towork is broken.
They want it repairedquickly by an expert human agent; theydon?t trust a machine can help them.- The description of the problem providedby the user can be imprecise, vague, orbased on a model of the world that may beincorrect (e.g.
some users of internet can-not tell their modem from their router).- It may be difficult to instruct non-technically savvy users on how to performa troubleshooting step (e.g.
Now renewyour IP address.)
or request technical in-formation (e.g.
Are you using a Voice overIP phone service?
)- Certain events cannot be controlled.
Forinstance, the time it would take for a userto complete a troubleshooting step, like re-booting a PC, is often unpredictable.- The acoustic environment may be chal-lenging.
Users may be asked to switchtheir TV on, reboot their PC, or check thecable connections.
All these operations cancause noise that can trigger the speech rec-ognizer and affect the course of the inter-action.On the other hand, one can leverage the automateddiagnosis or troubleshooting tools that are cur-rently used by human agent and improve the effi-ciency of the interaction.
For instance, if the IPaddress of the digital devices at the user premisesis available, one can ping them, verify their con-nectivity, download new firmware, and performautomated troubleshooting steps in the backgroundwithout the intervention of the user.
However, theinterplay between automated and interactive op-erations can raise the complexity of the applica-tions such as to require higher level developmentabstractions and authoring tools.3 High Resolution SSLUThe identification of the call reason?i.e.
the prob-lem or the symptoms of the problem experiencedby the caller?is one of the first phases of the in-teraction in a technical support application.
Thereare two possible design choices with today?s spo-ken language technology:- Directed dialog.
A specific prompt enu-merates all the possible reasons for a call,and the user would choose one of them.-  Natural Language: An open prompt asksthe user to describe the reason for the call.The utterance will be automaticallymapped to one of a number of possible callreasons using SSLU technology.Directed dialog would be the preferred choice interms of accuracy and cost of development.
Unfor-tunately, in most technical support applications, thenumber of call-reasons can be very large, and thusprompting the caller through a directed dialogmenu would be impractical.
Besides, even thougha long menu can be structured hierarchically as acascade of several shorter menus, the terms usedfor indicating the different choices may be mis-leading or meaningless for some of the users (e.g.do you have a problem with hardware, software, ornetworking?).
Natural language with SSLU isgenerally the best choice for problem identifica-tion.In practice, users mostly don?t know what the ac-tual problem with their service is (e.g.
modem iswrongly configured), but typically they describetheir observations?or symptoms?which are ob-servable manifestations of the problem.
and not theproblem itself (e.g.
symptom: I can?t connect to theWeb, problem: modem wrongly configured).
Cor-rectly identifying the symptom expressed in naturallanguage by users is the goal of the SSLU module.SSLU provides a mapping between input utter-ances and a set of pre-determined categories.SSLU has been effectively used in the past to en-able automatic call-routing.
Typically call-routingapplications have a number of categories, of theorder of a dozen or so, which are designed basedon the different routes to which the IVR is sup-posed to dispatch the callers.
So, generally, in call-27routing applications, the categories are known anddetermined prior to any data collection.One could follow the same approach for the prob-lem identification SSLU, i.e.
determine a numberof a-priori problem categories and then map a col-lection of training symptom utterances to each oneof them.
There are several issues with this ap-proach.First, a complete set of categories?the prob-lems?may not be known prior to the acquisitionand analysis of a significant number of utterances.Often the introduction of new home devices or ser-vices (such as DVR, or HDTV) creates new prob-lems and new symptoms that can be discoveredonly by analyzing large amounts of utterance data.Then, as we noted above, the relationship betweenthe problems?or broad categories of problems?and the manifestations (i.e.
the symptoms) may notbe obvious to the caller.
Thus, confirming a broadcategory in response to a detailed symptom utter-ance may induce the user to deny it or to give averbose response (e.g.
Caller: I cannot get to theWeb.
System: I understand you have a problemwith your modem configuration, is that right?Caller: Hmm?no.
I said I cannot get to the Web.
).Finally, caller descriptions have different degreesof specificity (e.g.
I have a problem with my cableservice vs.
The picture on my TV is pixilated on allchannels).
Thus, the categories should reflect ahierarchy of symptoms, from vague to specific,that need to be taken into proper account in thedesign of the interaction.As a result from the above considerations, SSLUfor symptom identification needs to be designed inorder to reflect the high-resolution multitude andspecificity hierarchy of symptoms that emergefrom the analysis of a large quantity of utterances.Figure 1 shows an excerpt from the hierarchy ofsymptoms for a cable TV troubleshooting applica-tion derived from the analysis of almost 100,000utterance transcriptions.Each node of the tree partially represented by Fig-ure 1 is associated with a number of training utter-ances from users describing that particularsymptom in their own words.
For instance the top-most node of the hierarchy, ?TV Problem?, corre-sponds to vague utterances such as I have aproblem with my TV or My cable TV does notwork.
The?
Ordering?
node represents requests ofthe type I have a problem with ordering a show,which is still a somewhat vague request, since onecan order ?Pay-per-view?
or ?On-demand?
events,and they correspond to different processes andtroubleshooting steps.
Finally, at the most detailedlevel of the hierarchy, for instance for the node?TV Problem-Ordering-On Demand-Error?, onefinds utterances such as I tried to order a movie ondemand, but all I get is an error code on the TV.In the experimental results reported below, wetrained and tested a hierarchically structured SSLUfor a cable TV troubleshooting application.
A cor-pus of 97,236 utterances was collected from a de-ployed application which used a simpler, nonhierarchical, version of the SSLU.
The utteranceswere transcribed and initially annotated based onan initial set of symptoms.
The annotation was car-ried out by creating an annotation guide documentwhich includes, for each symptom, a detailed ver-bal description, a few utterance examples, andrelevant keywords.
Human annotators were in-structed to label each utterance with the correctcategory based on the annotation guide and theirTV ProblemOn DemandPay-per-viewOrderingNo PictureErrorPINOtherErrorFigure 1: Excerpt from the hierarchical symp-tom description in a cable TV technical supportapplication28work was monitored systematically by the systemdesigner.After a first initial annotation of the whole corpus,the annotation consistency was measured by com-puting a cluster similarity distance between theutterances corresponding to all possible pairs ofsymptoms.
When the consistency between a pair ofsymptoms was below a given threshold, the clus-ters were analyzed, and actions taken by the de-signer in order to improve the consistency,including reassign utterances and, if necessary,modifying the annotation guide.
The whole processwas repeated a few times until a satisfactory globalinter-cluster distance was attained.Eventually we trained the SSLU on 79 symptomsarranged on a hierarchy with a maximum depth of3.
Table 2 summarizes the results on an independ-ent test set of 10,332 utterances.
The result showsthat at the end of the process, a satisfactory batchaccuracy of 81.43% correct label assignment whatattained for the utterances which were deemed tobe in-domain, which constituted 90.22% of the testcorpus.
Also, the system was able to correctly re-ject 24.56% of out-of-domain utterances.
Theoverall accuracy of the system was considered rea-sonable for the state of the art of commercialSSLUs based on current statistical classificationalgorithms.
Improvement in the classification per-formance can result by better language models (i.e.some of the errors are due to incorrect word recog-nition by the ASR) and better classifiers, whichneed to take into account more features of the in-coming utterances, such as word order1 and con-textual information.1Current commercial SSLU modules,  as the one used in thework described here, use statistical classifiers based only onbags of words.
Thus the order of the words in the incomingutterance is not taken into consideration.3.1 Confirmation EffectivenessAccuracy is not the only measure to provide anassessment of how the symptom described by thecaller is effectively captured.
Since the user re-sponse needs to be confirmed based on the inter-pretation returned by the SSLU, the caller alwayshas the choice of accepting or denying the hy-pothesis.
If the confirmation prompts are not prop-erly designed, the user can erroneously denycorrectly detected symptoms, or erroneously acceptwrong ones.The analysis reported below was carried out for adeployed system for technical support of Internetservice.
The full symptom identification interac-tions following the initial open prompt was tran-scribed and annotated for 895 calls.
The SSLUused in this application consisted of 36 symptomsstructured in a hierarchy with a maximum depth of3.
For each interaction we tracked the followingevents:- the first user response to the open question- successive responses in case of re-prompting because of speech recognitionrejection or timeout- response to the yes/no confirmation ques-tion)- successive responses to the confirmationquestion in case the recognizer rejected itor timed out.- Successive responses to the confirmationquestion in case the user denied, and asecond best hypothesis was offered.Table 3 summarizes the results of this analysis.The first row reports the number of calls for whichthe identified symptom was correct (as comparedwith human annotation) and confirmed by thecaller.
The following rows are the number of callswhere the identified symptom was wrong and thecaller still accepted it during confirmation, thesymptom was correct and the caller denied it, andthe symptom was wrong and denied, respectively.Finally there were 57 calls where the caller did notprovide any confirmation (e.g.
hung up, timed out,ASR rejected the confirmation utterance even afterre-prompting, etc.
), and 100 calls in which it wasnot possible to collect the symptom (e.g.
rejectionsUtterances 10332 100.00%In domain 9322 90.22%Correct  in-domain 7591 81.43%Out of domain  1010 9.78%Correct rejection out-of-domain 249 24.65%Table 2: Accuracy results for HierarchicalSSLU with 79 symptoms.29of first and second re-prompts, timeouts, etc.)
Inboth cases?i.e.
no confirmation or no symptomcollection at all?the call continued with a differ-ent strategy (e.g.
moved to a directed dialog, orescalated the call to a human agent).
The interest-ing result from this experiment is that the SSLUreturned a correct symptom 59.8 + 2.5 = 62.3% ofthe times (considering both in-domain and out-of-domain utterances), but the actual ?perceived?
ac-curacy (i.e.
when the user accepted the result) washigher, and precisely 59.8 + 13.2 = 73%.
A deeperanalysis shows that for most of the wrongly ac-cepted utterances the wrong symptom identified bythe SSLU was still in the same hierarchical cate-gory, but with different degree of specificity (e.g.Internet-Slow vs. vague Internet)The difference between the actual and perceivedaccuracy of SSLU has implications for the overallperformance of the application.
One could build ahigh performance SSLU, but a wrongly confirmedsymptom may put the dialog off course and resultin reduced automation, even though the perceivedaccuracy is higher.
Confirmation of SSLU resultsis definitely an area where new research can poten-tially impact the performance of the whole system.4 Experimental VUIVoice User Interface (VUI) is typically consideredan art.
VUI designers acquire their experience byanalyzing the effect of different prompts on thebehavior of users, and can often predict whether anew prompt can help, confuse, or expedite the in-teraction.
Unfortunately, like all technologies rely-ing on the anecdotal experience of the designer, inVUI it is difficult to make fine adjustments to aninterface and predict the effect of competing simi-lar designs before the application is actually de-ployed.
However, in large volume applications,and when a global measure of performance isavailable, one can test different non-disruptive de-sign hypotheses on the field, while the applicationis running.
We call this process experimental VUI.There have been, in the past, several studies aimedat using machine learning for the design of dialogsystems (Levin et al, 2000, Young 2002, Pietquinet al 2006).
Unfortunately, the problem of full de-sign of a system based uniquely on machine learn-ing is a very difficult one, and cannot be fullyutilized yet for commercial systems.
A simpler andless ambitious goal is that of finding the optimaldialog strategy among a small number of compet-ing designs, where all the initial designs are work-ing reasonably well (Walker 2000, Paek et al2004,Lewis 2006).
Comparing competing designs re-quires carrying on an exploration based on randomselection of each design at crucial points of thedialog.
Once a reward schema is defined, one canuse it for changing the exploration probability so asto maximize a function of the accumulated rewardusing, for instance, one of the algorithms describedin (Sutton 1998).Defining many different competing designs at sev-eral points of the interaction is often impracticaland costly.
Moreover, in a deployed commercialapplication, one needs to be careful about main-taining a reasonable user experience during explo-ration.
Thus, the competing designs have to bechosen carefully and applied to portions of the dia-log where the choice of the optimal design canmake a significant difference for the reward meas-ure in use.In the experiments described below we selected thesymptom identification as a point worth exploring.in an internet technical support application Wethen defined three prompting schemas- Schema A: the system plays an openprompt- Schema B: the system plays an openprompt, and then provides some examplesof requests- Schema C: The system plays an openprompt, and then suggests a command thatprovides a list of choices.Accepted correct 535 59.8%Accepted wrong 118 13.2%Denied correct 22 2.5%Denied wrong 63 7.0%Unconfirmed 57 6.4%No result 100 11.2%TOTAL 895 100.0%Table 3: Result of the confirmation analy-sis based on the results of 895 calls30The three schemas were implemented on a de-ployed system for limited time.
There was 1/3probability for each individual call to go throughone of the above schemas.
The target function cho-sen for optimization was the average automationrate.Figure 2 shows the effect on the cumulated averageautomation rate for each one of the competing de-sign.
The exploration was carried out until the dif-ference in the automation rate among the threedesigns reached statistical significance, which wasafter 13 days with a total number of 21,491 calls.At that point in time we established that design Bhad superior performance, as compared to A andC, with a difference of 0.68 percent points.Event though the gain in total automation rate (i.e.0.68 percent points) seems to be modest, one has toconsider that this increase is simply caused only bythe selection  of the best wording of a singleprompt in an application with thousands ofprompts.
One can expect to obtain more importantimprovements by at looking to other areas of thedialog where experimental VUI can be applied andselecting the optimal prompt can have an impacton the overall automation rate.5 ConclusionsWe started this paper by describing the advancesachieved in dialog system technology for commer-cial applications during the past decade.
The indus-try moved from the first generation of systems ableto handle very structured and simple interactions,to a current third generation where the interactionis less structured and the goal is to automate com-plex tasks such as problem solving and technicalsupport.We then discussed general issues regardingthe effective development of a technical supportapplication.
In particular we focused on two areas:the collection of the symptom from natural lan-guage expressions, and the experimental optimiza-tion of the VUI strategy.
In both cases wedescribed how a detailed analysis of live data cangreatly help optimize the overall performance.6 ReferencesBarnard, E., Halberstadt, A., Kotelly, C., Phillips, M.,  1999?A Consistent Approach To Designing Spoken-dialogSystems,?
Proc.
of ASRU99 ?
IEEE Workshop, Keystone,Colorado, Dec. 1999.Gorin, A. L., Riccardi, G.,Wright, J. H.,  1997 Speech Com-munication, vol.
23, pp.
113-127, 1997.Chu-Carroll, J., Carpenter B., 1999.
?Vector-based naturallanguage call routing,?
Computational Linguistics,v.25, n.3, p.361-388, September 1999Goel, V., Kuo, H.-K., Deligne, S., Wu S.,  2005 ?LanguageModel Estimation for Optimizing End-to-end Performanceof a Natural Language Call Routing System,?
ICASSP2005Pieraccini, R., Huerta, J., Where do we go from here?
Re-search and Commercial Spoken Dialog Systems, Proc.
of6th SIGdial Workshop on Discourse and Dialog, Lisbon,Portugal, 2-3 September, 2005. pp.
1-10Levin, E., Pieraccini, R., Eckert, W., A Stochastic Model ofHuman-Machine Interaction for Learning Dialog Strate-gies,  IEEE Trans.
on Speech and Audio Processing, Vol.8, No.
1, pp.
11-23, January 2000.Pietquin, O., Dutoit, T., A Probabilistic Framework for DialogSimulation and Optimal Strategy Learning, In IEEETransactions on Audio, Speech and Language Processing,14(2):589-599, 2006Young, S., Talking to Machines (Statistically Speaking), IntConf Spoken Language Processing, Denver, Colorado.
(2002).Walker, M., An Application of Reinforcement Learning toDialogue Strategy Selection in a Spoken Dialogue Systemfor Email .
Journal of Artificial Intelligence Research,JAIR, Vol 12., pp.
387-416, 2000Paek T., Horvitz E.,.
Optimizing automated call routing byintegrating spoken dialog models with queuing models.Proceedings of HLT-NAACL, 2004, pp.
41-48.Lewis, C., Di Fabbrizio, G., Prompt Selection with Rein-forcement Learning in an AT&T Call Routing Applica-tion, Proc.
of Interspeech 2006, Pittsburgh, PA. pp.
1770-1773, (2006)Sutton, R.S., Barto, A.G. (1998).
Reinforcement Learning: AnIntroduction.
MIT Press.14.00%15.00%16.00%17.00%18.00%19.00%20.00%21.00%1 2 3 4 5 6 7 8 9 10 11 12 13Time (days)AutomationrateABCFigure 2: Daily average automation rate for com-peting designs.31
