Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conferenceand the Shared Task, pages 74?79, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational LinguisticsSXUCFN-Core: STS Models Integrating FrameNet Parsing InformationSai Wang, Ru Li, RuiboWang, ZhiqiangWang, Xia ZhangShanxi University, Taiyuan, Chinaenrique.s.wang@gmail.com{liru, wangruibo}@sxu.edu.cn{zhiq.wang, caesarzhangx}@163.comAbstractThis paper describes our system submitted to*SEM 2013 Semantic Textual Similarity (STS)core task which aims to measure semantic si-milarity of two given text snippets.
In thisshared task, we propose an interpolation STSmodel named Model_LIM integrating Fra-meNet parsing information, which has a goodperformance with low time complexity com-pared with former submissions.1 IntroductionThe goal of Semantic Textual Similarity (STS) isto measure semantic similarity of two given textsnippets.
STS has been recently proposed byAgirre et al(2012) as a pilot task, which has closerelationship with both tasks of Textual Entailmentand Paraphrase, but not equivalent with them and itis more directly applicable to a number of NLPtasks such as Question Answering (Lin and Pantel,2001), Text Summarization (Hatzivassiloglou et al1999), etc.
And yet, the acquiring of sentence simi-larity has been the most important and basic task inSTS.
Therefore, the STS core task of *SEM 2013conference, is formally defined as the degree ofsemantic equivalence between two sentences asfollows:?
5: completely equivalent, as they meanthe same thing.?
4: mostly equivalent, but some unimpor-tant details differ.?
3: roughly equivalent, but some impor-tant information differs/missing.?
2: not equivalent, but share some details.?
1: not equivalent, but are on the same top-ic.?
0: on different topics.In this paper, we attempt to integrate semanticinformation into STS task besides the lower-levelword and syntactic information.
Evaluation resultsshow that our STS model could benefit from se-mantic parsing information of two text snippets.The rest of the paper is organized as follows: Sec-tion 2 reviews prior researches on STS.
Section 3illustrates three models measuring text similarity.Section 4 describes the linear interpolation modelin detail.
Section 5 provides the experimental re-sults on the development set as well as the officialresults on all published datasets.
Finally, Section 6summarizes our paper with direction for futureworks.2 Related WorkSeveral techniques have been developed for STS.The typical approach to finding the similarity be-tween two text segments is to use simple wordmatching method.
In order to improve this simplemethod, Mihalcea et al(2006) combine two cor-pus-based and six knowledge-based measures ofword similarity, but the cost of their algorithm isexpensive.
In contrast, our method treats wordsand texts in essentially the same way.In 2012 STS task, 35 teams participate and sub-mit 88 runs.
The two top scoring systems are UKP74and Takelab.
The former system (B?r et al 2012)uses a simple log-linear regression model to com-bine multiple text similarity measures (related tocontent, structure and style) of varying complexity.While the latter system Takelab (?ari?
et al 2012)uses a support vector regression model with mul-tiple features measuring word-overlap similarityand syntax similarity.The results of them score over 80%, far exceed-ing that of a simple lexical baseline.
But both shareone characteristic: they integrate lexical and syntaxinformation without semantic information, espe-cially FrameNet parsing information.
In addition,the complexity of these algorithms is very high.Therefore, we propose a different and simple mod-el integrating FrameNet parsing information in thispaper.3 Linear Interpolation ModelIn this paper, we propose a combination interpola-tion model which is constructed by the results ofthree similarity models based on words, WordNet,FrameNet , which are called simWD(?
), simWN(?)
andsimFN(?)
respectively.
The overall similaritysimLIM(S1, S2) between a pair of texts S1, S2 is com-puted in the following equation:simLIM(S1, S2)= ?1 ?
simWD(S1, S2)+?2 ?
simWN(S1, S2) +?3 ?
simFN(S1, S2)(1)In which, ?1, ?2 and ?3 are respectively theweights of the similarity models, i.e., ?1 +?2 +?3= 1; and they are all positive hyperparameters.Now, we describe the three models used in thisequation.3.1 Similarity Based on WordsThis model is motivated by Vector Space Model(Salton et al 1975).
We present each sentence as avector in the multidimensional token space.
Let Scdenote the set of all words in the c-th text snippets(c = 1, 2); the words of bag is W = S1 ?
S2.
Hence,the similarity of a pair of sentences, formally ex-pressed as:simWD(S1, S2) = ?
??,?
?
??,?|?|?????
??,??|?|???
?
??
??,??|?|???
(2)In which, we can find ??,?
?
??
?
?
1,2, ?
, |?|;?
?
1,2?
by solving:??,?
?
?1, ??
??,?
?
?
?0, ?????????
(3)From these two equations above, we can see themore identical words in a text pair, the more simi-lar the two snippets are.
Whereas, by intuition,many high-frequency functional words would notbe helpful to the estimation of the similarity givenin Eq.(2).
Therefore, in the preprocessing stage, wecompute the word frequencies per dataset, and thenremove the high frequency words (top 1% in fre-quency list) in each segment.3.2 Similarity Based on WordNetThis model measures semantic similarity with thehelp of such resources that specifically encode re-lations between words or concepts like WordNet(Fellbaum, 1998).
We use the algorithms by Lin(1998) on WordNet to compute the similarity be-tween two words a and b, which we call simLin(a,b).
Let S1, S2 be the two word sets of two given textsnippets, we use the method below:simWN(S1, S2)= ?
???
??????????,?
?
??,???????|??|,|??|????????|??|,|??|?
(4)In which, ??,?
?
????
?
1,2?.
In the numerator ofEq.
(4),we try to max(?
), avg(?)
and mid(?)
respec-tively, then we find the max(?)
is the best.3.3 Similarity Based on FrameNetFrameNet lexicon (Fillmore et al 2003) is a richlinguistic resource containing expert knowledgeabout lexical and predicate-argument semantics inEnglish.
In a sentence, word or phrase tokens thatevoke a frame are known as targets.
Each framedefinition also includes a set of frame elements, orroles, corresponding to different aspects of theconcept represented by the frame, such as partici-pants, props, and attributes.
We use the term ar-gument to refer to a sequence of word tokensannotated as filling a frame role.All the data are automatically parsed bySEMFOR1 (Das and Smith, 2012; Das and Smith,1 See http://www.ark.cs.cmu.edu/SEMAFOR/.752011).
Figure 1 shows the parser output of a sen-tence pair given in Microsoft Research Video De-scription Corpus with annotated targets, framesand role argument pairs.
It can be noticed thatFrameNet parsing information could give someclues of the similarity of two given snippets andwe think that integrating this information couldimprove the accuracy of STS task.
For example,the sentences in the Figure 1 both illustrate ?some-body is moving?.
However, our model depends onthe precision of that parser.
If it would be im-proved, the results in STS task would be better.Figure 1: This is a pair of sentences in 2013 STS train-ing data: (a) Girls are walking on the stage; (b) Womenmodels are walking down a catwalk.
The words in boldcorrespond to targets, which evoke semantic frames thatare denoted in capital letters.
Every frame is shown in adistinct color; the arguments of each frame are anno-tated with the same color, and marked below the sen-tence, at different levels; the spans marked in the blockof dotted liens fulfill a specific role.For a given sentence Sc (c = 1,2) with a set ofevoked frame Fc = < f1,f2, ?, fn > (n is the numberof evoked frames), a set of target word with eachframe Tc = < t1, t2, ?, tn > and the set of roles(namely, frame elements) ?c = {Rc,1, Rc,2, ?,Rc,n},each frame contains one or more argumentsRc,i = {rj} (i = 1, 2, ?, n; j is an integer that isgreater or equal to zero).
Take Figure 1 as an ex-ample,T1 = <grils, walking>,F1 = <PEOPLE, SELF_MOTION>, ?1 = {R1,1, R1,2 },R1,1 = {girls},R1,2 = {girls, on the stage};T2 = <women, models, walking, down>,F2 = <PEOPLE, VEHICLE,SELF_MOTION, DIRECTION>,?2 = {R2,1, R2,2, R2,3, R2,4},R2,1 = {women}, R2,2 = {models},R2,3 = {women models}, R2,4 = {down}.In order to compute simFr(?)
simply, we also usea interpolation model to combine the similaritiesbased on target words simTg(?
), frames simFr(?)
andframe relations simRe(?).
They are estimated as thefollowing:When computing the similarity on target wordlevel simTg(S1, S2), we also consider each sentenceas a vector of target words as is seen in Eq.
(5).T = T1 ?
T2;simTg(S1, S2)= ?
??,?
?
??,?|T|?????
??,??|?|???
?
??
??,??|?|???
(5)In which, we can find t?,?
?
??
?
?
1,2,?
, |?|;?
?
1,2?
by solving:??,?
?
?1, ??
??,?
?
??
?
????,?
?
????
?
1,2, ?
, |?|?0, ?????????
(6)Let simFr(S1, S2) be the similarity on frame levelas shown in Eq.
(7), with each sentence as a vectorof frames.
We define f1,i, f2,i like ??,?
in Eq.
(3).F = F1 ?
F2;simFr(S1, S2)=?
??,?
?
??,?|?|?????
??,??|?|???
?
??
??,??|?|???
(7)Before computing the role relationship betweenthe pair of sentences, we should find the contain-ment relationship of each pair of frames in onesentence.
We use a rule to define the containmentrelationship:Given two frames fc,i, fc,j in a sentence Sc, if??,?
?
??,?
??
?
?
?, then fc,j contains fc,i - and that isfc,i is a child of fc,j.
After that we add them into theset of frame relationship ????
?
??
??,??
, ??,??
??????
?
?????,??????
, ??
?
0?.
We consider the relationship between twoframes in a sentence as a 2-tuple, and again useFigure 1 as an example,Rlt1 = ?<PEOPLE, SELF_MOTION>?
;Rlt2 = ?<PEOPLE, SELF_MOTION>,<VEHICLE, SELF_MOTION >?.76Besides, we do exactly the same with bothframes, namely ????,?
?
????
?c ?
1,2?
the valueof ????,?
is 1.
The similarity on frame relationshiplevel simRe(S1, S2) presents each sentence as a vec-tor of roles as shown in Eq.
(8).Rlt = Rlt1 ?
Rlt2;simRe(S1, S2)= ?
????,?
?
????,?|???|?????
????,??|???|???
?
??
????,??|???|???
(8)Lastly, the shallow semantic similarity betweentwo given sentences is computed as:SimFN(S1, S2)= ?
?
simTg(S1, S2)+?
?
simFr(S1, S2) +?
?
simRe(S1, S2)(9)In which, ?
+ ?
+ ?
=1, and they are all positivehyperparameters.
As shown in Figure 2, we plotthe Pearson correlation (vertical axis) against thecombination of parameters (horizontal axis) in all2013 STS train data (2012 STS data).
We noticethat generally the Pearson correlation is fluctuates,and the correlation peak is found at 32, which inTable 1 is ?=0.6, ?=0.3, ?=0.1.ID ?
?
?
ID ?
?
?
ID ?
?
?1 1 0 0 23 0.7 0.2 0.1 45 0 0.4 0.62 0.9 0 0.1 24 0.6 0.2 0.2 46 0.5 0.5 03 0.8 0 0.2 25 0.5 0.2 0.3 47 0.4 0.5 0.14 0.7 0 0.3 26 0.4 0.2 0.4 48 0.3 0.5 0.25 0.6 0 0.4 27 0.3 0.2 0.5 49 0.2 0.5 0.36 0.5 0 0.5 28 0.2 0.2 0.6 50 0.1 0.5 0.47 0.4 0 0.6 29 0.1 0.2 0.7 51 0 0.5 0.58 0.3 0 0.7 30 0 0.2 0.8 52 0.4 0.6 09 0.2 0 0.8 31 0.7 0.3 0 53 0.3 0.6 0.110 0.1 0 0.9 32 0.6 0.3 0.1 54 0.2 0.6 0.211 0 0 1 33 0.5 0.3 0.2 55 0.1 0.6 0.312 0.9 0.1 0 34 0.4 0.3 0.3 56 0 0.6 0.413 0.8 0.1 0.1 35 0.3 0.3 0.4 57 0.3 0.7 014 0.7 0.1 0.2 36 0.2 0.3 0.5 58 0.2 0.7 0.115 0.6 0.1 0.3 37 0.1 0.3 0.6 59 0.1 0.7 0.216 0.5 0.1 0.4 38 0 0.3 0.7 60 0 0.7 0.317 0.4 0.1 0.5 39 0.6 0.4 0 61 0.2 0.8 018 0.3 0.1 0.6 40 0.5 0.4 0.1 62 0.1 0.8 0.119 0.2 0.1 0.7 41 0.4 0.4 0.2 63 0 0.8 0.220 0.1 0.1 0.8 42 0.3 0.4 0.3 64 0.1 0.9 021 0 0.1 0.9 43 0.2 0.4 0.4 65 0 0.9 0.122 0.8 0.2 0 44 0.1 0.4 0.5 66 0 1 0Table 1: Different combinations of ?, ?, ?
(?
+ ?
+?
=1) with ID that is horizontal axis in Figure 2.This table also apples to different combinations of?1, ?2, ?3 (?1 +?2 +?3 =1) with ID that is hori-zontal axis in Figure 3.Figure 2: This graph shows the variation of Pearsoncorrelation (vertical axis) in all 2013 STS train data(2012 STS data), with numbers (horizontal axis) indicat-ing different combinations ?, ?, ?
in Table 1 and whenthe value of result confidence is 100.
The effect valuesare represented by a vertical line (i.e.
ID = 32).4 Tuning HyperparametersEq.
(1) is a very simple linear interpolation model,and we tune the hyperparameters on the whole2012 STS data.As shown in Figure 3,we plot the Pearson corre-lation (vertical axis) for the different combinationof parameters ?1, ?2 and ?3 (horizontal axis).
Wenotice that generally the Pearson correlation fluc-tuates with a dropping tendency in most cases, andthe correlation peak presents at 13, which in Table1 is ?1=0.8, ?2=0.1, ?3=0.1.Figure 3: This graph shows the variation of Pearsoncorrelation (vertical axis) in all 2013 STS train data(2012 STS data), with numbers (horizontal axis) indicat-ing different combinations ?1, ?2, ?3 in Table 1 and when the value of result confidence is 100.
The effectvalues are represented by a vertical line (i.e.
ID = 13).775 ResultsWe submit four runs: the first one (Model_WD) isbased on word similarity; the second one (Mod-el_WN) which is only using the similarity based onWordNet, is submitted with the team name ofSXULLL; the third one (Model_FN) which usesFrameNet similarity defined in Section 3.3; and thelast one in which we combine the three similaritiesdescribed in Section 4 together with an interpola-tion model.
In addition, we map our outputs mul-tiply by five to the [0-5] range.It is worth notice that in the first model, we lo-wercase all words and remove all numbers andpunctuations.
And in the third model, we extract allframe-semantic roles with SEMFOR.In the experiment, we use eight datasets totally -namely MSRpar, MSRvid, SMTeuroparl, OnWN,SMTnews, headlines, FNWN and SMT - with theirgold standard file to evaluate the performance ofthe submitted systems.
Evaluation is carried outusing the official scorer which computes Pearsoncorrelation between the human rated similarityscores and the system?s output.
The final measureis the score that is weighted by the number of textpairs in each dataset (?Mean?).
See Agirre et al(2012) for a full description of the metrics.5.1 Experiments on STS 2012 DataThere is no new train data in 2013, so we use 2012data as train data.
From Table 2, 3 we can see thatthe Model_LIM has better performance than theother three models.MSRpar MSRvid SMTeuroparl MeanModel_WD 0.4532  0.4487   0.6467 0.5153Model_WN 0.2718  0.5410  0.6225  0.4774Model_FN 0.4437  0.5530  0.5178  0.5048Model_LIM 0.4896  0.5533  0.6681  0.5696Table 2: Performances of the four models on 2012 traindata.
The highest correlation in each column is given inbold.From Table 2, we notice that all the models ex-cept Model_FN, are apt to handle the SMTeuroparlthat involves long sentences.
For Model_FN, itperforms well in computing on short and similarlystructured texts such as MSRvid (This will be con-firmed in test data later).
Although WordNet andFrameNet model has a mere weight of 20% inModel_LIM (i.e.
?1 +?2 = 0.2), the run which in-tegrate more semantic information displays a con-sistent performance across the three train sets (es-pecially in SMTeuroparl, the Pearson correlationrises from 0.5178 to 0.66808), when compared tothe other three.MSRpar MSRvid SMTeuroparl OnWN SMTnews MeanBaseline 0.4334 0.2996 0.4542 0.5864 0.3908 0.4356Model_WD 0.4404 0.5464 0.5059 0.6751 0.4583 0.5346Model_WN 0.1247 0.6608 0.0637 0.4089 0.3436 0.3417Model_FN 0.3830 0.6082 0.3537 0.6091 0.4061 0.4905Model_LIM 0.4489 0.6301 0.5086 0.6841 0.4872 0.5631UKP_run2 0.6830 0.8739 0.5280 0.6641 0.4937 0.6773Table 3: Performances of our three models as well asthe baseline and UKP_run2 (that is ranked 1 in last STStask) results on 2012 test data.
The highest correlation ineach column is given in bold.The 2012 STS test results obtained by first rank-ing UKP_run2 and baseline system are shown inTable 3, it is interesting to notice that performanceof Model_WD is similar with Model_LIM excepton MSRvid, the text segments in which there arefewer identical words because of the semanticequivalence.
For Model_FN, we can see it per-forms well on short and similarly structured texts(MSRvid and OnWN) as mentioned before.
This isbecause the precision of FrameNet parser took ef-fect on the FrameNet-based models performance.Compared to UKP_run2, the performance of Mod-el_LIM is obviously better on OnWN set, while onSMTeuroparl and SMTnews this model scoresslightly lower than UKP_run2.
Finally, Mod-el_LIM did not perform best on MSRpar andMSRvid compared with UKP_run2, but it has lowtime complexity and integrates semantic informa-tion.5.2 Official Results on STS 2013 Test DataTable 4 provides the official results of our submit-ted systems, along with the rank on each dataset.Generally, all results outperform the baseline,based on simple word overlap.
However, the per-formance of Model_LIM is not always the best inthe three runs for each dataset.
From the table wecan note that a particular model always performswell on the dataset including the lexicon on whichthe model is based on e.g.
Model_WN in OnWN,Model_FN in FNWN.
Besides, Model_WD andModel_LIM almost have same scores except inOnWN set, because in Model_LIM is includedwith WordNet resource.78headlines OnWN FNWN SMT MeanBaseline 0.5399 (66)  0.2828 (80) 0.2146 (66)  0.2861 (65) 0.3639 (73)Model_WD 0.6806 (24)  0.5355 (44) 0.3181 (48)  0.3980 (4)  0.5198 (27)Model_WN 0.4840 (78)  0.7146 (12) 0.0415 (83)  0.1543 (86) 0.3944 (69)Model_FN 0.4881 (76)  0.6146 (27) 0.4237 (9)  0.3844 (6)  0.4797 (46)Model_LIM 0.6761 (29)  0.6481 (23) 0.3025 (51)  0.4003 (3) 0.5458 (14)Table 4: Performances of our systems as well as base-line on STS 2013 individual test data, accompanied bytheir rank (out of 90) shown in brackets.
Scores in bolddenote significant improvements over the baseline.As seen from the system rank in table, the op-timal runs in the three submitted system remainwith Model_LIM.
Not only Model_LIM performsbest on two occasions, but also Model_FN rankstop ten twice, in FNWN and SMT respectively, weowe this result to the contribution of FrameNetparsing information.6 ConclusionWe have tested all the models on published STSdatasets.
Compared with the official results, Mod-el_LIM system is apt to handle the SMT that in-volves long sentences.
Moreover, this system justintegrates words, WordNet and FrameNet semanticinformation, thus it has low time complexity.There is still much room for improvement in ourwork.
For example, we will attempt to use multiva-riate regression software to tuning the hyperpara-meters.AcknowledgmentsThis work is supported by the National NatureScience Foundation of China (No.60970053), bythe National High-tech Research and DevelopmentProjects (863) grant No.2006AA01Z142, by theState Language Commission of China No.YB125-19 as well as by the International Cooperation ofShanxi Province, Contracts 2010081044.
And wewould like to thank the organizer for the tremend-ous effort they put into formulating this challeng-ing work.ReferencesEneko Agirre, Daniel Cer, Mona Diab, and Aitor Gon-zalez-Agirre.
2012.
SemEval-2012 Task 6: A Pilot onSemantic Textual Similarity.
In Proceedings of the6th International Workshop on Semantic Evaluation,in conjunction with the 1st Joint Conference on Lexi-cal and Computational Semantics, 385?393.Dekang Lin, Patrick Pantel.
2001.
Discovery of Infe-rence Rules for Question Answering.
Natural Lan-guage Engineering, 7(4):343-360.Vasileios Hatzivassiloglou, Judith L. Klavans, andEleazar Eskin.
1999.
Detecting Text Similarity overShort Passages: Exploring Linguistic Feature Combi-nations via Machine Learning.
In proceedings of theJoint SIGDAT Conference on Empirical Methods inNatural Language Processing and Very Large Cor-pora, 224-231.Rada Mihalcea, Courtney Corley, and Carlo Strapparava.2006.
Corpus-based and Knowledge-based Measuresof Text Semantic Similarity.
In Proceedings of theNational Conference on Artificial Intelligence, 21(1):775-780.Daniel B?r, Chris Biemann, Iryna Gurevych, and Tors-ten Zesch.
2012.
UKP: Computing Semantic TextualSimilarity by Combining Multiple Content SimilarityMeasures.
In Proceedings of the 6th InternationalWorkshop on Semantic Evaluation, in conjunctionwith the 1st Joint Conference on Lexical and Compu-tational Semantics, 435-440.Frane ?ari?, Goran Glava?, Mladen Karan, Jan ?najder,and Bojana Dalbelo Ba?i?.
2012.
TakeLab: Systemsfor Measuring Semantic Text Similarity.
In Proceed-ings of the 6th International Workshop on SemanticEvaluation, in conjunction with the 1st Joint Confe-rence on Lexical and Computational Semantics, 441-448.G.
Salton, A. Wong, C.S.
Yang.
1975.
A Vector SpaceModel for Automatic Indexing.
Communications ofthe ACM, 18(11):613-620.C.
J. Fillmore, C. R. Johnson and M. R.L.
Petruck.
2003.Background to FrameNet.
International Journal ofLexicography, 16: 235-250.Dipanjan Das and Noah A. Smith.
2012.
Graph-BasedLexicon Expansion with Sparsity-Inducing Penalties.In Proceedings of the Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics, 677-687.Dipanjan Das and Noah A. Smith.
2011.
Semi-Supervised Frame-Semantic Parsing for UnknownPredicates.
In Proceedings of Annual Meeting of theAssociation for Computational Linguistics, 1435-1444.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
MIT Press.Dekang Lin.
1998.
An information-theoretic definitionof similarity.
In Proceedings of International Confe-rence on Machine Learning, 296-340.79
