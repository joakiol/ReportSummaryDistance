Machine Translation as a testbed for multilingual analysisRichard Campbell, Carmen Lozano, Jessie Pinkham and Martine Smets*Microsoft ResearchOne Microsoft WayRedmond, WA 98052  USA{richcamp, clozano, jessiep, martines}@microsoft.com*to whom all correspondence should be addressedAbstractWe propose that machine translation (MT) is auseful application for evaluating and derivingthe development of NL components,especially in a wide-coverage analysis system.Given the architecture of our MT system,which is a transfer system based on linguisticmodules, correct analysis is expected to be aprerequisite for correct translation, suggestinga correlation between the two, given relativelymature transfer and generation components.We show through error analysis that there isindeed a strong correlation between the qualityof the translated output and the subjectivelydetermined goodness of the analysis.
We usethis correlation as a guide for development ofa coordinated parallel analysis effort in 7languages.1  IntroductionThe question of how to test natural languageanalysis systems has been central to all naturallanguage work in the past two decades.
It is adifficult question, for which researchers havefound only partial answers.
The most commonanswer is component testing, where the componentis compared against a standard of goodness,usually the Penn Treebank for English (Marcus etal., 1993),  allowing a numerical score of precisionand recall (e.g.
Collins, 1997).Such methods have limitations, however, andneed to be supplemented by additional methods.One limitation is the availability of annotatedcorpora, which do not exist for all languages.Secondly, comparison to an annotated corpus canonly measure how well a system produces the kindof analysis for which the corpus is annotated, e.g.labeled bracketing of surface syntax.
Evaluationof analysis of deeper, more semanticallydescriptive, levels requires additional annotatedcorpora, which may not exist.
A morefundamental limitation of such methods is thatthey measure the goodness of a grammar withouttaking into account what the grammar is good for.This limitation is overcome, we claim, only bymeasuring the goodness of a grammar by itssuccess in real-world applications.We propose that machine translation (MT) is agood application to evaluate and drive thedevelopment of analysis components when thetransfer component is based on linguistic modules.Multi-lingual applications such as MT allowevaluation of system components that overcomesthe limitations mentioned above, and thereforeserves as a useful complement to other evaluationtechniques.
Another significant advantage tousing MT as a testbed for the analysis system isthat it prioritizes analysis problems, highlightingthose problems that have the greatest negativeeffect on translation output.In this paper, we give an overview ofNLPWin, a multi-application natural languageanalysis and generation system under developmentat Microsoft Research (Jensen et al, 1993; Gamonet al, 1997; Heidorn 2000), incorporating analysissystems for 7 languages (Chinese, English, French,German, Japanese, Korean and Spanish).
Ourdiscussion focuses on a description of the threecomponents of the analysis system (called sketch,portrait and logical form) with a particularemphasis on the logical form derived as the end-product, which serves as the medium for transferin our MT system.We also give an overview of the architectureof the MSR-MT system, and of the evaluation weuse to measure correctness of the translations.
Wedemonstrate the correlation between the scoresassigned to translation outputs and the correctnessof the analysis, using as illustration two language-pairs at different stages of development:  Spanish-English (SE) translation, as a testbed for theSpanish analysis system, and French-English (FE)translation, as a testbed for the French analysissystem.2  Overview of the analysis component ofNLPWinAnalysis produces three representations for theinput sentence: sketch, portrait and logical form1.Sketch is the initial tree representation for thesentence, along with its associated attribute-valuestructure.
An example of sketch is given in Figure1, which shows the sketch tree for sentence (1).
(1)Ce  format est pris   en charge par Windows 2000this format is   taken in charge by  Windows 2000?This format is supported by Windows 2000?Figure 1 :  Sketch analysis of (1)Attachment sites for post-modifiers are notdetermined in sketch.
In most cases, theinformation available as the syntactic tree is builtis not sufficient to determine where e.g.prepositional phrases or relative clauses should beattached.
Post-modifiers are thus systematicallyattached to the closest possible attachment site,and reattached, if necessary, by the reattachmentmodule, a set of heuristic rules.Reattachment rules apply to the sketch toproduce the portrait; the portrait analysis of (1) isgiven in Figure 2, where the PP expressing theagent of the passive construction, originallyattached to PP1 in sketch (see Figure 1) has beenreattached at the sentence level.1 The presentation of the analysis module is verysimplified, but sufficient for our current discussion.More details can be found in the references.Figure 2: Portrait analysis of (1)The portrait is the input to the computation ofthe logical form (LF), a labeled directed unorderedgraph representing the deep syntactic relationsamong the content words of the sentence (i.e.,basic predicate-argument structure), along withsome semantic information, such as functionalrelations expressed by certain prepositions.2 Atthis level, the difference between active andpassive constructions is normalized; controlrelations and long-distance dependencies, such assubjects of infinitives, arguments associated withgaps, etc., are resolved.
The LF of (1) is shown inFigure 3.
Note that the surface subject of thepassive is rendered as the Dobj (deep object) inLF, and the par-phrase as the Dsub (deep subject).Figure 3 :  LF analysis of (1)Modifications to any of the analysiscomponents are tested using monolingualregression files containing thousands of analyzedsentences; differences caused by the modificationare examined manually by the linguist responsiblefor the change (Suzuki, 2002).
This process servesas an initial screening to ensure that modificationsto the analysis have the desired effect.3 MSR-MTIn this section we review the basics of the MSR-MT translation system and its evaluation.
Thereader is referred to Pinkham et al (2001) andRichardson et al (2001) for further details on theFrench and Spanish versions of the system.
Theoverall architecture and basic component structure2   LF as described here corresponds to the PASrepresentation of Campbell and Suzuki (2002).are the same for both the FE and SE versions ofthe system.3.1 OverviewMSR-MT uses the broad coverage analysis systemdescribed in Section 2, a large multi-purposesource-language dictionary, a learned bilingualdictionary, an application independent target-language generation component and a transfercomponent.The transfer component consists of transferpatterns automatically acquired from sentence-aligned bilingual corpora (described below) usingan alignment algorithm described in detail inMenezes and Richardson (2001).
Training takesplace on aligned sentences which have beenanalyzed by the source- and target-languageanalysis systems to yield logical forms.
Thelogical form structures, when aligned, allow theextraction of lexical and structural translationcorrespondences which are stored for use atruntime in the transfer database.
See Figure 4 foran overview of the training process.The transfer database is trained on 350,000pairs of aligned sentences from computer manualsfor SE, and 500,000 pairs of aligned Canadianparliamentary data (the Hansard corpus) for FE.Figure 4:  MSR-MT training phase3.2   Evaluation of MSR-MTSeven evaluators are asked to evaluate the sameset of sentences.
For each sentence, raters arepresented with a reference sentence, the originalEnglish sentence from which the human Frenchand Spanish translations were derived, and MSR-MT?s machine translation.3 In order to maintain3 Microsoft manuals are written in English andtranslated by hand into other languages.
We use thesetranslations as input to our system, and translate themback into English.consistency among raters who may have differentlevels of fluency in the source language, raters arenot shown the original French or Spanish sentence(for similar methodologies, see Ringger et al,2001; White et al, 1993).All the raters enter scores reflecting theabsolute quality of the translation as compared tothe reference translation given.
The overall scoreof a sentence is the average of the scores given bythe seven raters.
Scores range from 1 to 4, with 1meaning unacceptable (not comprehensible), 2meaning possibly acceptable (some information istransferred accurately), 3 meaning acceptable (notperfect, but accurate transfer of all importantinformation, and 4 meaning ideal (grammaticallycorrect and all the important information istransferred).4 Examples from FE and SEIn this section we discuss specific examples toillustrate how results from MT evaluation help usto test and develop the analysis system.4.1  FE translation: the Hansard corpusThe evaluation we are discussing in this sectionwas performed in January 2002, at the beginningof our effort on the Hansard corpus.
Theevaluation was performed on a corpus of 250sentences, of which 55.6% (139 sentences) wereassigned a score of 2 or lower, 30.4% (76sentences) were assigned a score greater than 2 butnot greater than 3, and 14% (35 sentences) wereassigned a score greater than 3.Examination of French sentences receivinglow-score translations led to the identification ofsome classes of analysis problems, such as thefollowing:- mis-identification of vocatives- clefts not represented correctly- mis-analysis of ce qui / ce que free relatives- bad representation of complex inversion(pronoun-doubling of inverted subject)- no treatment of reflexives- fitted parses (i.e., not spanning the sentence)Most of the problematic structures arecharacteristic of spoken language as opposed tomore formal, written styles (vocatives, clefts,direct questions), and had not been encountered inour previous work, which had involved mostlytranslation of technical manuals.
Other problems(free relatives, reflexives) are analysis issues thatwe had not yet addressed.
Fitted parses are parsesthat do not span the whole sentence, but are piecedtogether by the parser from partial parses; fittedparses usually result in poor translations.Examples of translations together with theirscore are given in Table I.
The source sentencesare the French sentences, the reference sentence isthe human translation to which the translation iscompared by the evaluators, and the translation isthe output of MSR-MT.
Each of the threecategories considered above is illustrated by anexample.Sentence (2) (with a score of 1.5) is a directquestion with complex inversion and the doubledsubject typical of that construction.
In the LF for(2), les ministres des finances is analyzed as amodifier, because the verb r?unir already has asubject, the pronoun ils ?they?.
There are a coupleof additional problems with this sentence: si isanalyzed as the adverb meaning ?so?
instead of asthe conjunction meaning ?if?, and a direct questionis analyzed as a complement clause; the sketch andLF analyses of this sentence are given in theAppendix..
The MSR-MT translation of thissentence has a very low score, reflecting theseverity of the analysis problems.The two other sentences, on the other hand, donot have analysis problems: the poor translation of(3) (score 2.16) is caused by bad alignment (droittranslates as right instead of law), and thetranslation of (4) (score 3) is not completely fluent,but this is due to an English generation problem,rather than to a French analysis problem.
This lastsentence is the most correct with appropriatelexical items and has the highest score of the three.Of the 139 sentences with score 2 or lower,73% were due to analysis problems, and 24% toalignment problems.
Most of the rest had bugsrelated to the learned dictionary.
There were a fewcases of very free translations, where the referencetranslation was very far from the French sentence,and our translation, based on the source sentence,was therefore penalized.These figures show that, at this stage ofdevelopment of our system, most of the problemsin translation come from analysis.
Translation canbe improved by tackling analysis problemsexhibited by the lowest scoring sentences, and,conversely, analysis issues can be discovered bylooking at the sentences with the lowest translationscore.The next section gives examples of issues withthe SE system, which is more mature than the FEsystem.4.2  SE translation: Technical manualsAn evaluation of the Spanish-English MT systemwas also performed in January 2002, after work onthe MT system had been progressing forapproximately a year and a half.
The SE systemwas developed and tested using a corpus ofsentences from Microsoft technical manuals.
Aset of 600 unseen sentences was used for theevaluation.Out of a total of 600 sentences, the number ofsentences with a score from 3 to 4 was 251 (42%),the number of sentences with a score greater than2 but less than 3 was 186 (31%), and theremaining 163 sentences, (27%) had a score of 2or lower.
Of these 163 sentences with the lowestscores, 50% (82 sentences) had analysis problems,and 17% of them (29 sentences) had fitted parses.A few of the fitted parses, 7 sentences out of 29,had faulty input, e.g.
input that contained unusualcharacters or punctuation, typos, or sentencefragments.Typical analysis problems that led to poortranslation in the SE system include the following:- incorrect analysis of arguments in relativeclauses,  especially those with a singleargument (and a possible non-overt subject)- failure to identify the referent of clitic le (i.e.usted ?you?)
in imperative sentences in LF- mis-analysis of Spanish reflexive orse constructions in LF- incorrect syntactic analysis of homographs- incorrect analysis of coordination- mis-identification of non-overt or controlledsubjects- fitted parsesTable II contains sample sentences from theSE evaluation.
For each row, the second columndisplays the Spanish source sentence with thereference sentence in the next column, thetranslation produced by the MT system is in thefourth column, and the score for the translationassigned by the human evaluators in the lastcolumn.# Source  Reference Translation Score(2) Si tel n'?tait pas le cas, pourquoi lesministres des Finances des provinces seseraient-ils r?unis hier pour essayer des'entendre sur un programme commun ?soumettre au ministre des Finances?If that were not the case,why were the financeministers of the provincescoalescing yesterday to tryand come up with a jointprogram to bring to thefinance minister?.Not was the case that they havethe ministers met whyyesterday Finances of theprovinces trying to agree on acommon program to bringFinances for the minister this solike?1.5(3) Nous ne pouvons pas appuyer cettemotion apr?s que le Bloc qu?b?cois aitrefus?
de reconna?tre la primaut?
du droitet de la d?mocratie pour  tous.We cannot support thismotion after seeing theBloc Quebecois refuse torecognize the rule of lawand the principle ofdemocracy for all.We cannot support this motionafter the Bloc Quebecois hasrefused to recognize the rule ofthe right and democracy for all.2.16(4) En tant que membre de l'oppositionofficielle, je continuerai d'exercer despressions sur le gouvernement pour qu'iltienne ses promesses ?
cet ?gard.As a member of the officialopposition I will continueto pressure the governmentto fulfil its promises in thisregard.As member of the officialopposition, I will continue toexercise pressures on thegovernment for it to keep itspromises in this regard.3Table I:  Examples of FE translation# Source Reference Translation Score(5) Este procedimiento s?lo es aplicable siest?
ejecutando una versi?n de idioma deWindows 2000 que no coincida con elidioma en el que desee escribir.This procedure applies onlyif you are running alanguage version ofWindows 2000 that doesn'tmatch the language youwant to typeThis procedure only applies ifyou are running a Windows2000 language version that doesnot match the language that youwant to type.3.8(6) Repita este proceso hasta que hayaeliminado todos los componentes de reddesde las propiedades de Red, haga clicen Aceptar y, a continuaci?n, haga clicen S?
cuando se le pregunte si deseareiniciar el equipo.Repeat this process untilyou have deleted all of thenetwork components fromNetwork properties, clickOK, and then click Yeswhen you are prompted torestart your computer.Repeat this process until youhave deleted all of the networkcomponents from the Networkproperties, you click OK, andyou click Yes then when askingthat to restart the computer iswanted for him.2.0(7) En el siguiente ejemplo se muestra elnombre de la presentaci?n que se est?ejecutando en la ventana de presentaci?ncon diapositivas uno.The following exampledisplays the name of thepresentation that's currentlyrunning in slide showwindow one.In the following example, thedisplay name that is being runin the slide show window isdisplayed I join.1.4Table II:  Examples of SE translationIn the evaluation process, human evaluatorscompared the MT translation to the referencesentence, in the manner described in Section 4.1.Example (5), with a score of 3.8, illustrates thefact that human evaluators considered thetranslation ?a Windows 2000 language version?
tobe a slightly worse translation than ?a languageversion of Windows 2000?
for una version deidioma de Windows 2000; however the differenceis so slight as to not be considered an analysisproblem.Example (6) illustrates the failure to identifyusted ?you?
(understood as the subject of theimperative) as the referent of the pronominal cliticle; as mentioned above, this is a common source ofbad SE translations.
The last example (7) is asentence with a fitted parse due to misanalysis of aword as its homograph :  uno is analyzed as thefirst person singular present form of the verb unir?join?
instead of as the noun uno ?one?
; the LF ofthis sentence is given in the Appendix.4.3 DiscussionThe examples discussed in this section are typical:The sentences for which MSR-MT produces bettertranslations tend to be the ones with fewer analysiserrors, while those which are misanalyzed tend tobe mistranslated.In this way, evaluation of MT output serves asone way to prioritize analysis problems; that is, todecide which among the many different analysisproblems lead to the most serious problems.
Forexample, the poor quality of the translation of (2)highlights the need for an improved analysis ofcomplex inversion in the French grammar, whichwill need to be incorporated into the sketch and/orLF components.
Similarly, the poor translation of(7) indicates the need to deal better withhomographs in the Spanish morphological orsketch   component.More generally, the analysis of FE and SEtranslation problems has led to the lists of analysisproblems given in Sections 4.1 and 4.2,respectively.
Analysis problems identified in thisway then become priorities for grammar/LFdevelopment.5 ConclusionWe have outlined how the output of MT can beused as testbed for linguistic analysis in the sourcelanguage, supplementing other methods.
Themain advantage of this approach, in our view, isthat it helps to prioritize analysis problems,highlighting those which have the most directbearing on the application(s), the correctfunctioning of which is the main goal of thesystem.AcknowledgementsThis paper represents the work of many people inthe NLP group at MSR; we acknowledge theircontributions.ReferencesCampbell, R. and H. Suzuki.
2002.
Language-neutralrepresentation of syntactic structure.
In R. Malaka,R.
Porzel and M. Stube, eds., Proceedings of the FirstInternational Workshop on Scalable NaturalLanguage Understanding.Collins, M. 1997.
Three generative, lexicalised modelsfor statistical parsing.
Proceedings of the 35th AnnualMeeting of the ACL, Madrid.Gamon, M., C. Lozano, J. Pinkham and T. Reutter.1997.
Practical experience with grammar sharing inmultilingual NLP.
In Burstein J., Leacock C., eds,Proceedings of the Workshop on Making NLP Work,ACL Conference, Madrid.Heidorn, G.  2000.
Intelligent writing assistance.
In R.Dale, H. Moisl and H. Somers, eds., Handbook ofNatural Language Processing.Jensen, K., G. Heidorn and S. Richardson, eds.
1993.Natural Language Processing: The PLNLP Approach,Boston, Kluwer.Marcus, M., B. Santorini, and M. Marcinkiewicz.
1993.Building a large annotated corpus of English: thePenn Treebank.
In Proceedings of the 31st AnnualMeeting of the ACL.Menezes, A. and S. Richardson.
2001.
A Best-FirstAlignment Algorithm for Automatic Extraction ofTransfer Mappings from Bilingual Corpora.
InProceedings of the Data-Driven MT workshop, ACL2001.Pinkham, J., M. Corston-Oliver, M. Smets and M.Pettenaro, 2001.
Rapid assembly of a large-scaleFrench-English MT system.
In Proceedings of the2001 MT Summit.Richardson, S., W.B.
Dolan, A. Menezes and J.Pinkham.
2001.
Achieving commercial-qualitytranslation with example-based methods.
InProceedings of the 2001 MT Summit.Ringger, E.K., M. Corston-Oliver, and R.C.
Moore.2001.
Using Word-Perplexity for AutomaticEvaluation of Machine Translation.
Unpublished ms.Suzuki, H.  2002.
A development environment forlarge-scale multi-lingual parsing systems.
Workshopon Grammar Engineering and Evaluation, COLING2002.White, J.S., T.A.
O'Connell, and L.M.
Carlson.
1993.Evaluation of machine translation.
In HumanLanguage Technology: Proceedings of a Workshop(ARPA).
206-210.AppendixFigure 5 :  Sketch analysis of (2)Figure 6 :  LF analysis of (2)Figure 7 :  LF analysis of (7)
