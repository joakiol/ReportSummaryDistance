The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 163?173,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsOn Improving the Accuracy of Readability Classificationusing Insights from Second Language AcquisitionSowmya VajjalaSeminar fu?r SprachwissenschaftUniversita?t Tu?bingensowmya@sfs.uni-tuebingen.deDetmar MeurersSeminar fu?r SprachwissenschaftUniversita?t Tu?bingendm@sfs.uni-tuebingen.deAbstractWe investigate the problem of readability as-sessment using a range of lexical and syntac-tic features and study their impact on predict-ing the grade level of texts.
As empirical ba-sis, we combined two web-based text sources,Weekly Reader and BBC Bitesize, targetingdifferent age groups, to cover a broad rangeof school grades.
On the conceptual side, weexplore the use of lexical and syntactic mea-sures originally designed to measure languagedevelopment in the production of second lan-guage learners.
We show that the develop-mental measures from Second Language Ac-quisition (SLA) research when combined withtraditional readability features such as wordlength and sentence length provide a goodindication of text readability across differentgrades.
The resulting classifiers significantlyoutperform the previous approaches on read-ability classification, reaching a classificationaccuracy of 93.3%.1 IntroductionReading plays an important role in the developmentof first and second language skills, and it is one ofthe most important means of obtaining informationabout any subject, in and outside of school.
How-ever, teachers often find it difficult to obtain textsappropriate to the reading level of their students, ona given topic.
In many cases, they end up modifyingor creating texts, which takes significant time and ef-fort.
In addition to such a traditional school setting,finding texts at the appropriate reading level is alsoimportant in a wide range of real-life contexts in-volving people with intellectual disabilities, dyslex-ics, immigrant populations, and second or foreignlanguage learners.Readability-based text classification, when usedas a ranking parameter in a search engine, can helpin retrieving texts that suit a particular target readinglevel for a given query topic.
In the context of lan-guage learning, a language aware search engine (Ottand Meurers, 2010) that includes readability classi-fication can facilitate the selection of texts from theweb that are appropriate for the students in terms ofform and content.
This is one of the main motiva-tions underlying our research.Readability assessment has a long history(DuBay, 2006).
Traditionally, only a limited set ofsurface features such as word length and sentencelength were considered to derive a formula for read-ability.
More recently, advances in computationallinguistics made it possible to automatically extracta wider range of language features from text.
Thisfacilitated building machine learning models that es-timate the reading level of a text.
On the other hand,there has also been an on-going stream of researchon reading and text complexity in other areas such asSecond Language Acquisition (SLA) research andpsycholinguistics.In SLA research, a range of measures have beenproposed to study the development of complexityin the language produced by learners.
These mea-sures are used to evaluate the oral or written pro-duction abilities of language learners.
The aim ofreadability classification, on the other hand, is to re-trieve texts to be comprehended by readers at a par-163ticular level.
Since we want to classify and retrievetexts for learners of different age groups, we hypoth-esized that these SLA-based complexity measures oflearner production, when used as features for read-ability classification will improve the performanceof the classifiers.
In this paper, we show that thisapproach indeed results in a significant performanceimprovement compared to previous research.We used the WeeklyReader website1 as one ofthe text used in previous research.
We combined itwith texts crawled from the BBC-Bitesize website2,which provides texts for a different age group.
Thecombined corpus, WeeBit, covers a comparativelylarger range of ages than covered before.To summarize, the contributions of this paper are:?
We adapt measures from second language ac-quisition research to readability classificationand show that the overall classification accu-racies of an approach including these featuressignificantly outperforms previous approaches.?
We extend the most widely used WeeklyReadercorpus by combining it with another corpus thatis graded for a different age-group, thereby cre-ating a larger and more diverse corpus as basisfor future research.The paper is organized as follows: Section 2 de-scribes related work on reading level classificationto put our work in context.
Section 3 introduces thecorpora we used.
Section 4 describes the featureswe considered in detail.
Section 5 presents the ap-proach and discusses the results.
Section 6 providesa summary and points to future work.2 Related WorkThe traditional readability formulae made use of alimited number of surface features, such as the aver-age sentence length and the average word length incharacters or syllables (Kincaid et al, 1975; Cole-man and Liau, 1975).
Some works also made useof lists of ?difficult?
words, typically based on fre-quency counts, to estimate readability of texts (Daleand Chall, 1948; Chall and Dale, 1995; Stenner,1http://www.weeklyreader.com2http://www.bbc.co.uk/bitesize1996).
Dubay (2006) provides a broad survey of tra-ditional approaches to readability assessment.
Al-though the features considered appear shallow interms of linguistic modeling, they have been popularfor many years and are widely used.More recently, the developments in computationallinguistics made it possible to consider various lex-ical and syntactic features to automatically modelreadability.
In some of the early works on statis-tical readability assessment, Si and Callan (2001)and Collins-Thompson and Callan (2004) reportedthe impact of using unigram language models to es-timate the grade level of a given text.
The modelswere built on a United States text book corpus.Heilman et al (2007; 2008b; 2008a) extendedthis approach and worked towards retrieving rele-vant reading materials for language learners in theREAP3 project.
They extended the above mentionedapproach to include a set of manually and later au-tomatically extracted grammatical features.Schwarm and Ostendorf (2005) and Petersen andOstendorf (2009) report on classification experi-ments with WeeklyReader data, considering statisti-cal language models, traditional formulae, as well ascertain basic parse tree features in building an SVM-based statistical model.
Feng et al (2010) and Feng(2010) went beyond lexical and syntactic featuresand studied the impact of several discourse-basedfeatures, comparing their performance on the Week-lyReader corpus.While the vast majority of approaches have tar-geted English texts, some work on other languagessuch as German, Portuguese, French and Italian (vorder Bru?ck et al, 2008; Aluisio et al, 2010; Fran-cois and Watrin, 2011; Dell?Orletta et al, 2011) isstarting to emerge.
Parse-tree-based features havealso been used to measure the complexity of spokenSwedish (Roll et al, 2007).The process of text comprehension and the effectof factors such as the coherence of texts have alsobeen intensively studied (e.g., Crossley et al, 2007a;2007b; Graesser et al, 2004) and measures to ana-lyze the text under this perspective have been imple-mented in the CohMetrix project.4The DARPA Machine Reading program created3http://reap.cs.cmu.edu4http://cohmetrix.memphis.edu164a corpus of general text readability containing var-ious forms of human and machine generated texts(Strassel et al, 2010).5 The aim of this program is totransform natural language texts into a format suit-able for automatic processing by machines and tofilter out poorly written documents based on the textquality.
Kate et al (2010) used this data set to builda coarse grained model of text readability.While in this paper we focus on comparing com-putational linguistic approaches to readability as-sessment and improving the state of the art on a tra-ditional and available data set, Nelson et al (2012)compared several research and commercially avail-able text difficulty assessment systems in support ofthe Common Core Standards?
goal of providing stu-dents with texts at the appropriate level of difficultythroughout their schooling.6Independent of the research on readability, thecomplexity of the texts produced by language learn-ers has been extensively investigated in SecondLanguage Acquisition (SLA) research (Housen andKuiken, 2009).
Recent approaches have automatedand compared a number of such complexity mea-sures for learner language, specifically in English asSecond Language learner narratives (Lu, 2010; Lu,2011b).
So far, there is hardly any work on usingsuch insights in computational linguistics, though,with the notable exception of Chen and Zechner(2011) using SLA features to evaluate spontaneousnon-native speech.
Given that graded corpora arealso intended to be used by incremental age groups,we started to investigate whether the insights fromSLA research can fruitfully be applied to readabilityclassification.3 CorporaWe used a combined corpus of WeeklyReader andBBC-Bitesize to develop a statistical model thatclassifies texts into five grade levels, based on theage groups.WeeklyReader7 is an educational newspaper, witharticles targeted at four grade levels (Level 2, Level3, Level 4, and Senior), corresponding to children5The corpus is apparently intended to be available for publicuse, but does not yet seem to be so; we so far were unsuccessfulin obtaining more information from the authors.6http://www.corestandards.org7http://www.weeklyreader.combetween ages 7?8, 8?9, 9?10, and 9?12 years.
Thearticles cover a wide range of non-fiction topics,from science to current affairs, written according tothe grade level of the readers.
The exact criterionof graded writing is not published by the magazine.We obtained permission to use the graded magazinearticles and downloaded the archives in 11/2011.8Though we used the same WeeklyReader textbase as the previous works, the corpus is not identi-cal since we downloaded our version more recently.Thus the archive contained more articles per leveland some preprocessing may differ.
The Week-lyReader magazine issues in addition to the actualarticles include teacher guides, student quizzes, im-ages and brain teaser games, which we did not in-clude in the corpus.
The distribution of articles afterthis preprocessing is shown in Table 1.Grade Age Number of Avg.
Number ofLevel in Years Articles Sentences/ArticleLevel 2 7?8 629 23.41Level 3 8?9 801 23.28Level 4 9?10 814 28.12Senior 10-12 1325 31.21Table 1: The Weekly Reader corpusBBC-Bitesize9 is a website with articles classi-fied into four grade levels (KS1, KS2, KS3 andGCSE), corresponding to children between ages 5?7, 8?11, 11?14 and 14?16 years.
The Bitesize cor-pus is freely available on the web, and we crawled itin 2009.
Most of the articles at KS1 consisted of im-ages and flash files and other audio-visual material,with little text.
Hence, we did not include KS1 inour corpus.
We also excluded pages that containedonly images, audio, or video files without text.To cover a broad range of non-overlapping agegroups, we used Level 2, Level 3 and Level 4 fromWeeklyReader and KS3 and GCSE from Bitesizedata respectively and built a combined corpus cover-ing learners aged 7 to 16 years.
Note that while KS2covers the age group of 8?11 years, Levels 2, 3, and8A license to use the texts on the website for research can beobtained for a small fee from support@weeklyreader.com.
Tosupport comparable research, we will share the exact corpus weused with other researchers who have obtained a license to usethe WeeklyReader materials.9http://www.bbc.co.uk/bitesize1654 together cover ages 7?10 years.
Similarly, the Se-nior Level overlaps with Level 4 and KS3.
Hence,we excluded KS2 and Senior from the combinedcorpus.
We will refer to the combined five-level cor-pus we created in this way as WeeBit.
The distribu-tion of articles in the combined WeeBit corpus afterpreprocessing and removing the overlapping gradelevels, is shown in Table 2.Grade Age Number of Avg.
Number ofLevel in Years Articles Sentences/ArticleLevel 2 7?8 629 23.41Level 3 8?9 801 23.28Level 4 9?10 814 28.12KS3 11?14 644 22.71GCSE 14?16 3500 27.85Table 2: The WeeBit corpusTo avoid a classification bias towards a class withmore training examples during, for each level in theWeeBit corpus, 500 documents were taken as train-ing set and 125 documents were taken as test set.In total, we trained on a set of 2500 documents andused a test set of 625 documents, spanning acrossfive grade levels.4 FeaturesTo build our classification models, we combinedfeatures used in previous research with other parsetree features as well as lexical richness and syntacticcomplexity features from SLA research.
We groupthe features into three broad categories: lexical, syn-tactic and traditional features.4.1 Lexical FeaturesWord n-grams have been frequently used as lexicalfeatures in the previous research (Collins-Thompsonand Callan, 2004; Schwarm and Ostendorf, 2005).10POS n-grams as well as POS-tag ratio features havealso been used in some of the later works (Feng etal., 2010; Petersen and Ostendorf, 2009).In the SLA context, independent of the readabilityresearch, Lu (2011a) studied the relationship of lexi-cal richness to the quality of English as Second Lan-guage (ESL) learners?
oral narratives and analyzed10In the readability literature, n-grams are traditionally dis-cussed as lexical features.
N-grams beyond unigrams naturallyalso encode aspects of syntax.the distribution of three dimensions of lexical rich-ness (lexical density, sophistication and variation) inthem using various metrics proposed in the languageacquisition literature.
Those measures were used toanalyze a large scale corpus of Chinese learners ofEnglish.
We adapted some of the metrics from thisresearch as our lexical features:Type-Token Ratio (TTR) is the ratio of numberof word types (T) to total number word tokens ina text (N).
It has been widely used as a measureof lexical diversity or lexical variation in languageacquisition studies.
However, since it is depen-dent on the text size, various alternative transfor-mations of TTR came into existence.
We consid-ered Root TTR (T/?N ), Corrected TTR (T/?2N ),Bilogarithmic TTR (Log T/Log N) and Uber Index(Log2T/Log(N/T )).Another recent TTR variant we considered, whichis not a part of Lu (2011a), is the Measure of TextualLexical Diversity (MTLD; McCarthy and Jarvis,2010).
It is a TTR-based approach that is not af-fected by text length.
It is evaluated sequentially, asthe mean length of string sequences that maintain adefault Type-Token Ratio value.
That is, the TTRis calculated at each word.
When the default TTRvalue is reached, the MTLD count increases by oneand TTR evaluations are again reset.
McCarthy andJarvis (2010) considered the default TTR as 0.72 andwe continued with the same default.Considering nouns, adjectives, non-modal andnon-auxiliary verbs and adverbs as lexical items,Lu (2011a) studied various syntactic category basedword ratio measures.
Lexical variation is definedas the ratio of the number of lexical types to lexi-cal tokens.
Other variants of lexical variation stud-ied in Lu (2011a) included noun, adjective, modi-fier, adverb and verb variations, which represent theproportion of the words of the respective categoriescompared to all lexical words in the document.
Al-ternative measures of verb variation, namely VerbVariation-1 (Tverb/Nverb), Squared Verb Variation-1 (T 2verb/Nverb) and Corrected Verb Variation-1(Tverb/?2Nverb) are also studied in the literature.We considered all these measures of lexical varia-tion as a part of our lexical features.
We have alsoincluded Lexical Density, which is the ratio of thenumber of lexical items in relation to the total num-ber of words in a text.166In addition to these measures from the SLA lit-erature, in our lexical features we included the aver-age number of syllables per word (NumSyll) and theaverage number of characters per word (NumChar),which are used as word-level indicators of text com-plexity in various traditional formulae (Kincaid etal., 1975; Coleman and Liau, 1975).Finally, we included the proportion of words inthe text which are found on the Academic Word Listas another lexical feature.
It refers to the word listcreated by Coxhead (2000), which contains a list ofmost frequent words found in the academic texts.11The list does not include the most frequent words inthe English language as such.
The words in this listare specific to academic contexts.
It was intended tobe used both by teachers and students as a measureof vocabulary acquisition.
We use it as an additionallexical feature in our work ?
and it turned out to beone of the most predictive features.All the lexical features we considered in this workare listed in Table 3.
The SLA based lexical featuresare referred to as SLALEX in the table.
Of these,Lexical Features from SLA research (SLALEX)?
Lexical Density (LD)?
Type-Token Ratio (TTR)?
Corrected TTR (CTTR)?
Root TTR (RTTR)?
Bilogarithmic TTR (LogTTR)?
Uber Index (Uber)?
Lexical Word Variation (LV)?
Verb Variation-1 (VV1)?
Squared VV1 (SVV1)?
Corrected VV1 (CVV1)?
Verb Variation 2 (VV2)?
Noun Variation (NV)?
Adjective Variation (AdjV)?
Adverb Variation (AdvV)?
Modifier Variation (ModV)?
Mean Textual Lexical Density (MTLD)Other Lexical Features?
Proportion of words in AWL (AWL)?
Avg.
Num.
Characters per word (NumChar)?
Avg.
Num.
Syllables per word (NumSyll)Table 3: Lexical Features (LEXFEATURES)11http://en.wikipedia.org/wiki/Academic_Word_Listsix features CTTR, RTTR, SVV1, CVV1, AdvV, ModVwere shown by Lu (2011b) to correlate best with thelearner data.
We will refer to them as BESTLEX-SLA, highlighted in italics in the table.4.2 Syntactic FeaturesSchwarm and Ostendorf (2005) implemented fourparse tree features (average parse tree height, aver-age number of SBARs, NPs per sentence and VPsper sentence) in their work.
Feng (2010) consideredmore syntactic features, adding the average lengthsof phrases (NP, VP and PP) per sentence in wordsand characters, and the total number of respectivephrases in the document.
In our work, we startedwith reconsidering the above mentioned syntacticfeatures.In addition, we included measures of syntacticcomplexity from the SLA literature.
Lu (2010) se-lected 14 measures from a large set of measures usedto monitor the syntactic development in languagelearners.
He then used these measures in the analysisof syntactic complexity in second language writingand showed that some of them correlate well withthe syntactic development of adult Chinese learnersof English.
They are grouped into five broad cate-gories:The first set consists of three measures of syn-tactic complexity based on the length of a unit atthe sentential, clausal and T-unit level respectively.The definitions for sentence, clause and T-unit wereadapted from the SLA literature.
While a sentenceis considered to be a group of words delimited withpunctuation mark, a clause is any structure with asubject and a finite verb.
Finally, a T-unit is char-acterized as one main clause plus any subordinateclause or non-clausal structure that is attached to orembedded in it.The second type of measure targets sentence com-plexity.
Clauses per sentence is considered as a sen-tence complexity measure.The third set of measures reflect the amount ofsubordination in the sentence.
They include clausesper T-unit, complex T-units per T-unit, dependentclauses per clause and dependent clauses per T-unit.A complex T-unit is considered as any T-unit thatcontains a dependent clause.The fourth type of measures measured the amountof co-ordination in a sentence.
They consist of co-167ordinate phrases per clause and co-ordinate phasesper T-unit.
Any adjective, verb, adverb or nounphrase that dominates a co-ordinating conjunction isconsidered a co-ordinate phrase.The fifth type of measures represented the rela-tionship between specific syntactic structures andlarger production units.
They include complex nom-inals per clause, complex nominals per T-unit andverb phrases per T-unit.
Complex nominals are com-prised of a) nouns plus adjective, possessive, prepo-sitional phrase, relative clause, participle or appos-itive, b) nominal clauses, c) gerunds and infinitivesin subject positions.We implemented these 14 syntactic measures asfeatures in building our classification models, in ad-dition to existing features.
Eight of these features(MLC, MLT, CP/C, CP/T, CN/C, CN/T, MLS, VP/T)were argued to correlate best with language develop-ment.
We refer to this subset of eight as BESTSYN-SLA, shown in italics in Table 4.
We will see in sec-tion 5 that a set including those features also holdsgood predictive power for classifying graded texts.We also included the number of dependentclauses, complex T-units, and co-ordinate phrasesper sentence as additional syntactic features.
Table 4summarizes the syntactic features used in this paper.4.3 ?Traditional?
FeaturesThe average number of characters per word (Num-Char), the average number of syllables per word(NumSyll), and the average sentence length in words(MLS) have been used to derive formulae for read-ability in the past.
We refer to them as TraditionalFeatures below.
We included MLS in the syntacticfeatures and NumChar, and NumSyll in the Lexi-cal features.
We also included two popular readabil-ity formulae, Flesch-Kincaid score (Kincaid et al,1975) and Coleman-Liau readability formula (Cole-man and Liau, 1975), as additional features.
Thelatter will be referred as Coleman below, and bothformulas together as Traditional Formulae.5 Experiments and EvaluationWe used the Berkeley Parser (Petrov and Klein,2007) with the standard model they provide forbuilding syntactic parse trees and defined the pat-terns for extracting various syntactic features fromSyntactic features from SLA research (SLASYN)?
Mean length of clause (MLC)?
Mean length of a sentence (MLS)?
Mean length of T-unit (MLT)?
Num.
of Clauses per Sentence (C/S)?
Num.
of T-Units per sentence (T/S)?
Num.
of Clauses per T-unit (C/T)?
Num.
of Complex-T-Units per T-unit (CT/T)?
Dependent Clause to Clause Ratio (DC/C)?
Dependent Clause to T-unit Ratio (DC/T)?
Co-ordinate Phrases per Clause (CP/C)?
Co-ordinate Phrases per T-unit (CP/T)?
Complex Nominals per Clause (CN/C)?
Complex Nominals per T-unit (CN/T)?
Verb phrases per T-unit (VP/T)Other Syntactic features?
Num.
NPs per sentence (NumNP)?
Num.
VPs per sentence (NumVP)?
Num.
PPs per sentence (NumPP))?
Avg.
length of a NP (NPSize)?
Avg.
length of a VP (VPSize)?
Avg.
length of a PP (PPSize)?
Num.
Dependent Clauses per sentence (NumDC)?
Num.
Complex-T units per sentence (NumCT)?
Num.
Co-ordinate Phrases per sentence (CoOrd)?
Num.
SBARs per sentence (NumSBAR)?
Avg.
Parse Tree Height (TreeHeight)Table 4: Syntactic features (SYNFEATURES)the trees using the Tregex pattern matcher (Levy andAndrew, 2006).
More details about the patterns fromthe SLA literature and their definitions can be foundin Lu (2010).
We used the OpenNLP12 tagger toget POS tag information and calculate Lexical Rich-ness features.
We used the WEKA (Hall et al, 2009)toolkit for our classification experiments.
We ex-plored different classification algorithms such as De-cision Trees, Support Vector Machines, and Logis-tic Regression.
The Multi-Layer Perceptron (MLP)-classifier performed best with various combinationsof features, so we focus on reporting the results forthat algorithm.12http://opennlp.apache.org168Feature set # Features Classifier PerformanceAccuracy RMSETraditional Formulae 2 38.8% 0.36Traditional Features 3 70.3% 0.25Trad.
Features + Trad.
formulae 5 72.3% 0.32SLALEX 16 68.1% 0.29SLASYN 14 71.2% 0.28SLALEX + SLASYN 30 82.3% 0.23BEST10SYN 10 69.9% 0.28All Syntactic Features 25 75.3% 0.27BEST10LEX 10 82.4% 0.22All Lexical Features 19 86.7% 0.20BEST10ALL 10 89.7% 0.18All features 46 93.3% 0.15Table 5: Classification results for WeeBit Corpus5.1 Evaluation MetricsWe report our results in terms of classification accu-racy and root mean square error.Classification accuracy refers to the percentage ofinstances in the test set that are classified correctly.The correct classifications include both true posi-tives and true negatives.
However, accuracy doesnot reflect how close the prediction is to the actualvalue.
A difference between expected and predictedvalues of one grade level is treated the same way asthe difference of, e.g., four grade levels.Root mean square error (RMSE) is a measurewhich gives a better picture of this difference.RMSE is the square root of empirical mean of thesquared prediction errors.
It is frequently used asa measure to estimate the deviation of an observedvalue from the expected value.
In readability assess-ment, it can be understood as the average differencebetween the predicted grade level and the expectedgrade level.5.2 Feature CombinationsComplementing our experiments comparing the dif-ferent lexical and syntactic features and their com-bination, we also used WEKA?s information-gain-based feature selection algorithm, and selected theTop-10 best features using the ranker method.When all features were considered, the top 10most predictive features were found to be: (Num-Char, NumSyll, MLS, AWL, ModVar, CoOrd, Cole-man, DC/C, CN/C,and AdvVar), which are referredto as BEST10ALL in the table.Considering the 25 syntactic features alone, the10 most predictive features were: (MLS, CoOrd,DC/C, CN/C, CP/C, NumPP, VPSize, C/T, CN/T andNumVP), referred to as BEST10SYN in the table.The 10 most predictive features amongst all thelexical features were: (NumChar, NumSyll, AWL,ModV, AdvV, AdjV, LV, VV1, NV and SVV1).
Theyare referred to as BEST10LEX in the table.Although the traditionally used features (Num-Char, NumSyll, MLS) seem to be the most predictive,it can be seen from the other top ranked features,that there is significant overlap between the best fea-tures identified by WEKA and the features whichLu (2010; 2011b) identified as correlating best withlanguage development (shown in italics in Table 3and Table 4), which supports our hypothesis that theSLA-based measures are useful features for read-ability classification of non-learner text too.5.3 ResultsTable 5 shows the results of our classification ex-periments using WEKA?s Multi-Layer Perceptronalgorithm with different combinations of features.Combining all features results in the best accuracyof 93.3%, which is a large improvement over thecurrent state of the art in readability classificationreported on the WeeklyReader corpus (74.01% byFeng et al, 2010).
It should, however, be kept169# Features Highest reported accuracyPrevious work (on WeeklyReader)(Feng et al, 2010) 122 74.01%(Petersen and Ostendorf, 2009) 25 63.18%Syntactic features only (Petersen and Ostendorf, 2009) 4 50.91%Our Results (on WeeklyReader alone)Syntactic features from (Petersen and Ostendorf, 2009) 4 50.68%All our Syntactic Features 25 64.3%All our Lexical Features 19 84.1%All our Features 46 91.3%Our Results (on WeeBit)All our Syntactic Features 25 75.3%All our Lexical Features 19 86.7%All our Features 46 93.3%Table 6: Overall Results and Comparison with Previous Workin mind that the improvement is achieved on theWeeBit corpus which is an extension of the Week-lyReader corpus previously used.
Interestingly, theresult of 89.7% for BEST10ALL, the top 10 featureschosen by the WEKA ranker, are quite close to ourbest result, with a very small number of features.Lexical features seem to perform better than syn-tactic features when considered separately.
How-ever, this better performance of lexical features wasmainly due to the addition of the traditionally usedfeatures NumChar and NumSyll.
So it is no won-der that these shallow features have been used inthe traditional readability formulae for such a longtime; but the predictive power of the traditional for-mulae as features by themselves is poor (38.8%), inline with the conclusions drawn in previous research(Schwarm and Ostendorf, 2005; Feng et al, 2010)about the Flesch-Kincaid and Dale-Chall formulae.Interestingly, Coleman, which was not consideredin those previous approaches, was ranked amongthe Top-10 most predictive features by the WEKAranker.
So it holds a good predictive power whenused as one of the features for the classifier.We also studied the impact of SLA based fea-tures alone on readability classification.
The perfor-mance of the SLA based lexical features (SLALEX)and syntactic features (SLASYN) when consideredseparately are still in a comparable range with thepreviously reported results on readability classifi-cation (68.1% and 71.2% respectively).
However,combining both of them resulted in an accuracy of82.3%, which is a considerable improvement overpreviously reported results.
It again adds weight tothe initial hypothesis that SLA based features can beuseful for readability classification.5.4 Comparison with previous workTable 6 provides an overall comparison of the accu-racies obtained for the key features sets in our workwith the best results reported in the literature for theWeeklyReader corpus.
However, since our classi-fication experiments were carried out with a newlycompiled corpus extending the WeeklyReader data,such a direct comparison is not particularly mean-ingful by itself.
To address this issue, we exploredtwo avenues.Firstly, we ran additional experiments, trainingand testing on the WeeklyReader data only, includ-ing the four levels used in previous work on that cor-pus.
A summary of the results can be seen in Table6.
Our approach with 46 features results in 91.3%accuracy on the WeeklyReader corpus, compared to74.01% as the best previous WeeklyReader result,reported by Feng et al (2010) for their much largerfeature set (122 features).In order to verify the impact of our choice of fea-tures, we also did a replication of the parsed syntac-tic feature measures reported by (Schwarm and Os-tendorf, 2005) on the WeeklyReader corpus and ob-tained essentially the same accuracy as the one pub-170lished (50.7% vs. 50.91%), supporting the compa-rability of the WeeklyReader data used.
The signif-icant performance increase we reported thus seemsto be due to the new features we integrated from theSLA literature.Secondly, we were interested in the impact of thetraining size on the results.
We therefore investi-gated how good our best approach (using all fea-tures) is on a training corpus that is comparable tothe WeeklyReader corpus used in previous work interms of the number of documents per class.
Whenwe took 1400 WeeklyReader documents distributedinto four classes as described in Feng et al (2010),we obtained an accuracy of 84.2%, compared to the74.01% they reported as best result.
Using 2500documents distributed into four classes as in Pe-tersen and Ostendorf (2009) we obtained 88.4%,compared to their best result of 63.18%.
Given thatthe original corpora used are not available, theseWeeklyReader corpora with the same source, num-ber of documents, and size of classes are as closeas we can get to a direct comparison.
In the future,the availability of the WeeBit corpus will support amore direct comparison of approaches.In sum, the above experiments seem to indicatethat the set of features and classifier used in our ap-proach play an important role in the resulting signif-icant increase in accuracy.6 Conclusion and DiscussionWe created a new corpus, WeeBit, by combiningtexts from two graded web sources WeeklyReaderand BBC Bitesize.
The resulting text corpus islarger and covers more grade levels, spanning theage group between 7 and 16 years.
We hope thatthe availability of this graded corpus will be usefulas an empirical basis for future studies in automaticreadability assessment.13We studied the impact of various lexical and syn-tactic features and explored their performance incombination with features encoding syntactic com-plexity and lexical richness that were inspired bySecond Language Acquisition research.
Our experi-ments show that not only the full set of features, but13As mentioned above, we will make the WeeBit corpusavailable to all researchers who have obtained the inexpensiveresearch license from WeeklyReader.also specific manually or automatically selected sub-sets of features provide results significantly improv-ing on the previously published state of the art inautomatic readability assessment.
There also seemsto be a clear correlation between the good predictorsaccording to SLA research on language learning andthose that performed well in text classification.Although the exact criteria based on which theindividual corpora (WeeklyReader, BBC-Bitesize)were created is not known, it is possible that theywere created with the well-known, traditional read-ability formulae in mind.
It would be surprising ifthe two corpora, compiled in the US and Britain bydifferent companies, were created with the same setof measures in mind, so the WeeBit corpus shouldbe less affected.
Still, it is possible that the rea-son the traditional features NumChar, NumSyll andMLS held such a strong predictive power is thatthese measures were considered when the texts werewritten.
But removing these traditional features onlystrengthens the role of the other features and therebythe main point of the paper arguing for the usefulessof SLA developmental measures for readability clas-sification.As a part of our future work, we intend to revisitand study the impact of further classes of featuresemployed in psycholinguistics and cognitive sci-ence research, such as those studied in Coh-Metrix(Graesser et al, 2004) or in the context of retrievingtexts for specific groups of readers (Feng, 2010).In terms of our overall application goal, we arecurrently studying the ability of the classificationmodels we built to generalize to web data.
We thenplan to add the classification model to a languageaware search engine (Ott and Meurers, 2010).
Sucha search engine may then also be able to integrateuser feedback on the readability levels of webpages,to build a dynamic, online model of readability.7 AcknowledgementsWe thank the anonymous reviewers and workshoporganizers for their feedback on the paper.
The re-search leading to these results has received fundingfrom the European Commission?s 7th FrameworkProgram under grant agreement number 238405(CLARA).1414http://clara.uib.no171ReferencesSandra Aluisio, Lucia Specia, Caroline Gasperin, andCarolina Scarton.
2010.
Readability assessment fortext simplification.
In Proceedings of the NAACL HLT2010 Fifth Workshop on Innovative Use of NLP forBuilding Educational Applications, pages 1?9, LosAngeles, California, June.Jeanne S. Chall and Edgar Dale.
1995.
ReadabilityRevisted: The New Dale-Chall Readability Formula.Brookline Books.Maio Chen and Klaus Zechner.
2011.
Computing andevaluating syntactic complexity features for automatedscoring of spontaneous non-native speech.
In Pro-ceedings of the 49th Annual Meeting of the Associ-ation for Computational Linguistics, pages 722?731,Portland, Oregon, June.Meri Coleman and T.L.
Liau.
1975.
A computer read-ability formula designed for machine scoring.
Journalof Applied Psychology, 60:283?284.Kevyn Collins-Thompson and Jamie Callan.
2004.
Alanguage modeling approach to predicting reading dif-ficulty.
In Proceedings of HLT/NAACL 2004, Boston,USA.Averil Coxhead.
2000.
A new academic word list.Teachers of English to Speakers of Other Languages,34(2):213?238.Scott A. Crossley, David F. Dufty, Philip M. McCarthy,and Danielle S. McNamara.
2007a.
Toward a newreadability: A mixed model approach.
In Danielle S.McNamara and Greg Trafton, editors, Proceedings ofthe 29th annual conference of the Cognitive ScienceSociety.
Cognitive Science Society.Scott A. Crossley, Max M. Louwerse, Philip M. Mc-Carthy, and Danielle S. McNamara.
2007b.
A lin-guistic analysis of simplified and authentic texts.
TheModern Language Journal, 91(1):15?30.Edgar Dale and Jeanne S. Chall.
1948.
A formula forpredicting readability.
Educational research bulletin;organ of the College of Education, 27(1):11?28.Felice Dell?Orletta, Simonetta Montemagni, and GiuliaVenturi.
2011.
Read-it: Assessing readability of ital-ian texts with a view to text simplification.
In Proceed-ings of the 2nd Workshop on Speech and LanguageProcessing for Assistive Technologies, pages 73?83.William H. DuBay.
2006.
The Classic Readability Stud-ies.
Impact Information, Costa Mesa, California.Lijun Feng, Martin Jansche, Matt Huenerfauth, andNoe?mie Elhadad.
2010.
A comparison of features forautomatic readability assessment.
In In Proceedings ofthe 23rd International Conference on ComputationalLinguistics (COLING 2010), Beijing, China.Lijun Feng.
2010.
Automatic Readability Assessment.Ph.D.
thesis, City University of New York (CUNY).Thomas Francois and Patrick Watrin.
2011.
On the con-tribution of mwe-based features to a readability for-mula for french as a foreign language.
In Proceedingsof Recent Advances in Natural Language Processing,pages 441?447.Arthur C. Graesser, Danielle S. McNamara, Max M.Louweerse, and Zhiqiang Cai.
2004.
Coh-metrix:Analysis of text on cohesion and language.
Behav-ior Research Methods, Instruments and Computers,36:193?202.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The weka data mining software: An update.The SIGKDD Explorations, 11(1).Michael Heilman, Kevyn Collins-Thompson, JamieCallan, and Maxine Eskenazi.
2007.
Combininglexical and grammatical features to improve readabil-ity measures for first and second language texts.
InHuman Language Technologies 2007: The Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics (HLT-NAACL?07),pages 460?467, Rochester, New York.Michael Heilman, Kevyn Collins-Thompson, and Max-ine Eskenazi.
2008a.
An analysis of statistical mod-els and features for reading difficulty prediction.
InProceedings of the 3rd Workshop on Innovative Use ofNLP for Building Educational Applications, Colum-bus, Ohio.Michael Heilman, Le Zhao, Juan Pino, and Maxine Eske-nazi.
2008b.
Retrieval of reading materials for vocab-ulary and reading practice.
In Proceedings of the ThirdWorkshop on Innovative Use of NLP for Building Ed-ucational Applications (BEA-3) at ACL?08, pages 80?88, Columbus, Ohio.Alex Housen and Folkert Kuiken.
2009.
Complexity,accuracy, and fluency in second language acquisition.Applied Linguistics, 30(4):461?473.Rohit J. Kate, Xiaoqiang Luo, Siddharth Patwardhan,Martin Franz, Radu Florian, Raymond J. Mooney,Salim Roukos, and Chris Welty.
2010.
Learning topredict readability using diverse linguistic features.
In23rd International Conference on Computational Lin-guistics (COLING 2010).J.
P. Kincaid, R. P. Jr. Fishburne, R. L. Rogers, and B. SChissom.
1975.
Derivation of new readability for-mulas (Automated Readability Index, Fog Count andFlesch Reading Ease formula) for Navy enlisted per-sonnel.
Research Branch Report 8-75, Naval Techni-cal Training Command, Millington, TN.Roger Levy and Galen Andrew.
2006.
Tregex and tsur-geon: tools for querying and manipulating tree datastructures.
In 5th International Conference on Lan-guage Resources and Evaluation, Genoa, Italy.172Xiaofei Lu.
2010.
Automatic analysis of syntacticcomplexity in second language writing.
InternationalJournal of Corpus Linguistics, 15(4):474?496.Xiaofei Lu.
2011a.
A corpus-based evaluation of syn-tactic complexity measures as indices of college-levelesl writers?
language development.
TESOL Quarterly,45(1):36?62, March.Xiaofei Lu.
2011b.
The relationship of lexical richnessto the quality of esl learners?
oral narratives.
The Mod-ern Languages Journal.
in press.Philip McCarthy and Scott Jarvis.
2010.
Mtld, vocd-d, and hd-d: A validation study of sophisticated ap-proaches to lexical diversity assessment.
Behavior Re-search Methods, 42(2):381?392.J.
Nelson, C. Perfetti, D. Liben, and M. Liben.
2012.Measures of text difficulty: Testing their predictivevalue for grade levels and student performance.
Tech-nical report, The Council of Chief State School Offi-cers.Niels Ott and Detmar Meurers.
2010.
Information re-trieval for education: Making search engines languageaware.
Themes in Science and Technology Education.Special issue on computer-aided language analysis,teaching and learning: Approaches, perspectives andapplications, 3(1?2):9?30.Sarah E. Petersen and Mari Ostendorf.
2009.
A machinelearning approach to reading level assessment.
Com-puter Speech and Language, 23:86?106.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In Human Language Tech-nologies 2007: The Conference of the North AmericanChapter of the Association for Computational Linguis-tics; Proceedings of the Main Conference, pages 404?411, Rochester, New York, April.Mikael Roll, Johan Frid, and Merie Horne.
2007.
Mea-suring syntactic complexity in spontaneous spokenswedish.
Language and Speech, 50(2).Sarah Schwarm and Mari Ostendorf.
2005.
Readinglevel assessment using support vector machines andstatistical language models.
In Proceedings of the43rd Annual Meeting of the Association for Computa-tional Linguistics (ACL?05), pages 523?530, Ann Ar-bor, Michigan.Luo Si and Jamie Callan.
2001.
A statistical model forscientific readability.
In Proceedings of the 10th Inter-national Conference on Information and KnowledgeManagement (CIKM), pages 574?576.
ACM.A.
Jackson Stenner.
1996.
Measuring reading compre-hension with the lexile framework.
In Fourth NorthAmerican Conference on Adolescent/Adult Literacy.Stephanie Strassel, Dan Adams, Henry Goldberg,Jonathan Herr, Ron Keesing, Daniel Oblinger, HeatherSimpson, Robert Schrag, and Jonathan Wright.
2010.The darpa machine reading program - encouraging lin-guistic and reasoning research with a series of read-ing tasks.
In Language Resources and Evaluation(LREC), Malta.Tim vor der Bru?ck, Sven Hartrumpf, and Hermann Hel-big.
2008.
A readability checker with supervisedlearning using deep syntactic and semantic indicators.Informatica, 32(4):429?-435.173
