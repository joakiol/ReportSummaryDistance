Proceedings of the Eighth Meeting of the ACL Special Interest Group on Computational Phonology at HLT-NAACL 2006, pages 41?49,New York City, USA, June 2006. c?2006 Association for Computational LinguisticsImproved morpho-phonological sequence processing withconstraint satisfaction inferenceAntal van den Bosch and Sander CanisiusILK / Language and Information ScienceTilburg University, P.O.
Box 90153, NL-5000 LE Tilburg, The Netherlands{Antal.vdnBosch,S.V.M.Canisius}@uvt.nlAbstractIn performing morpho-phonological se-quence processing tasks, such as letter-phoneme conversion or morphologicalanalysis, it is typically not enough to basethe output sequence on local decisions thatmap local-context input windows to sin-gle output tokens.
We present a globalsequence-processing method that repairsinconsistent local decisions.
The approachis based on local predictions of overlap-ping trigrams of output tokens, whichopen up a space of possible sequences;a data-driven constraint satisfaction infer-ence step then searches for the optimaloutput sequence.
We demonstrate signifi-cant improvements in terms of word accu-racy on English and Dutch letter-phonemeconversion and morphological segmenta-tion, and we provide qualitative analysesof error types prevented by the constraintsatisfaction inference method.1 IntroductionThe fields of computational phonology and mor-phology were among the earlier fields in compu-tational linguistics to adopt machine learning algo-rithms as a means to automatically construct pro-cessing systems from data.
For instance, letter-phoneme conversion was already pioneered, withneural networks initially, at the end of the 1980s(Sejnowski and Rosenberg, 1987), and was shortlyafter also investigated with memory-based learn-ing and analogical approaches (Weijters, 1991; Vanden Bosch and Daelemans, 1993; Yvon, 1996)and decision trees (Torkkola, 1993; Dietterichet al, 1995).
The development of these data-driven systems was thrusted by the early existenceof lexical databases, originally compiled to serve(psycho)linguistic research purposes, such as theCELEX lexical database for Dutch, English, andGerman (Baayen et al, 1993).
Many researchershave continued and are still continuing this line ofwork, generally producing successful systems withsatisfactory, though still imperfect performance.A key characteristic of many of these early sys-tems is that they perform decomposed or simplifiedversions of the full task.
Rather than predicting thefull phonemization of a word given its orthographyin one go, the task is decomposed in predicting in-dividual phonemes or subsequences of phonemes.Analogously, rather than generating a full word-form, many morphological generation systems pro-duce transformation codes (e.g., ?add -er and um-laut?)
that need to be applied to the input string bya post-processing automaton.
These task simplifi-cations are deliberately chosen to avoid sparsenessproblems to the machine learning systems.
Suchsystems tend to perform badly when there are manylow-frequent and too case-specific classes; task de-composition allows them to be robust and genericwhen they process unseen words.This task decomposition strategy has a severedrawback in sequence processing tasks.
Decom-posed systems do not have any global method tocheck whether their local decisions form a globally41coherent output.
If a letter-phoneme conversion sys-tem predicts schwas on every vowel in a polysyllabicword such as parameter because it is uncertain aboutthe ambiguous mapping of each of the as and es, itproduces a bad pronunciation.
Likewise, if a mor-phological analysis system segments a word such asbeing as a prefix followed by an inflection, makingthe locally most likely guesses, it generates an anal-ysis that could never exist, since it lacks a stem.Global models that coordinate, mediate, or en-force that the output is a valid sequence are typi-cally formulated in the form of linguistic rules, ap-plied during processing or in post-processing, thatconstrain the space of possible output sequences.Some present-day research in machine learningof morpho-phonology indeed focuses on satisfy-ing linguistically-motivated constraints as a post-processing or filtering step; e.g., see (Daya et al,2004) on identifying roots in Hebrew word forms.Optimality Theory (Prince and Smolensky, 2004)can also be seen as a constraint-based approach tolanguage processing based on linguistically moti-vated constraints.
In contrast to being motivated bylinguistic theory, constraints in a global model canbe learned automatically from data as well.
In thispaper we propose such a data-driven constraint sat-isfaction inference method, that finds a globally ap-propriate output sequence on the basis of a space ofpossible sequences generated by a locally-operatingclassifier predicting output subsequences.
We showthat the method significantly improves on the ba-sic method of predicting single output tokens at atime, on English and Dutch letter-phoneme conver-sion and morphological analysis.This paper is structured as follows.
The constraintsatisfaction inference method is outlined in Sec-tion 2.
We describe the four morpho-phonologicalprocessing tasks, and the lexical data from which weextracted examples for these tasks, in Section 3.
Wesubsequently list the outcomes of the experimentsin Section 4, and conclude with a discussion of ourfindings in Section 5.2 Class trigrams and constraintsatisfaction inferenceBoth the letter-phoneme conversion and the morpho-logical analysis tasks treated in this paper can beseen as sequentially-structured classification tasks,where sequences of letters are mapped to sequencesof phonemes or morphemes.
Such sequence-to-sequence mappings are a frequently reoccurringphenomenon in natural language processing, whichsuggests that it is preferable to take care of the issueof classifying sequential data once at the machinelearning level, rather than repeatedly and in differentways at the level of practical applications.
Recently,a machine learning approach for sequential data hasbeen proposed by Van den Bosch and Daelemans(2005) that is suited for discrete machine-learningalgorithms such as memory-based learners, whichhave been shown to perform well on word phonem-ization and morphological analysis before (Van denBosch and Daelemans, 1993; Van den Bosch andDaelemans, 1999).
In the remainder of this paper,we use as our classifier of choice the IB1 algorithm(Aha et al, 1991) with feature weighting, as im-plemented in the TiMBL software package1 (Daele-mans et al, 2004).In the approach to sequence processing proposedby Van den Bosch and Daelemans (2005), the el-ements of the input sequence (in the remainder ofthis paper, we will refer to words and letters ratherthan the more general terms sequences and sequenceelements) are assigned overlapping subsequences ofoutput symbols.
This subsequence corresponds tothe output symbols for a focus letter, and one let-ter to its left and one letter to its right.
Predictingsuch trigram subsequences for each letter of a wordeventually results in three output symbol predictionsfor each letter.
In many cases, those three predic-tions will not agree, resulting in a number of po-tential output sequences.
We will refer to the pro-cedure for selecting the final output sequence fromthe space of alternatives spanned by the predictedtrigrams as an inference procedure, analogously tothe use of this term in probabilistic sequence clas-sification methods (Punyakanok and Roth, 2001).The original work on predicting class trigrams im-plemented a simple inference procedure by votingover the three predicted symbols (Van den Boschand Daelemans, 2005).Predicting trigrams of overlapping output sym-bols has been shown to be an effective approach1TiMBL URL: http://ilk.uvt.nl/timbl/42to improve sequence-oriented natural language pro-cessing tasks such as syntactic chunking and named-entity recognition, where an input sequence of to-kens is mapped to an output sequence of symbolsencoding a syntactic or semantic segmentation of thesentence.
Letter-phoneme conversion and morpho-logical analysis, though sequentially structured onanother linguistic level, may be susceptible to bene-fiting from this approach as well.In addition to the practical improvement shownto be obtained with the class trigram method, thereis also a more theoretical attractiveness to it.
Sincethe overlapping trigrams that are predicted are justatomic symbols to the underlying learning algo-rithm, a classifier will only predict output symboltrigrams that are actually present in the data it wastrained on.
Consequently, predicted trigrams areguaranteed to be syntactically valid subsequencesin the target task.
There is no such guarantee inapproaches to sequence classification where an iso-lated local classifier predicts single output symbolsat a time, without taking into account predictionsmade elsewhere in the word.While the original voting-based inference proce-dure proposed by Van den Bosch and Daelemans(2005) manages to exploit the sequential informa-tion stored in the predicted trigrams to improve uponthe performance of approaches that do not considerthe sequential structure of their output at all, it doesso only partly.
Essentially, the voting-based infer-ence procedure just splits the overlapping trigramsinto their unigram components, thereby retainingonly the overlapping symbols for each individual let-ter.
As a result, the guaranteed validity of the trigramsubsequences is not put to use.
In this section we de-scribe an alternative inference procedure, based onprinciples of constraint satisfaction, that does man-age to use the sequential information provided bythe trigram predictions.At the foundation of this constraint-satisfaction-based inference procedure, more briefly constraintsatisfaction inference, is the assumption that theoutput symbol sequence should preferably be con-structed by concatenating the predicted trigrams ofoutput symbols, rather than by chaining individualsymbols.
However, as the underlying base classifieris by no means perfect, predicted trigrams should notbe copied blindly to the output sequence; they maybe incorrect.
If a trigram prediction is considered tobe of insufficient quality, the procedure backs off tosymbol bigrams or even symbol unigrams.The intuitive description of the inference proce-dure is formalized by expressing it as a weightedconstraint satisfaction problem (W-CSP).
Constraintsatisfaction is a well-studied research area withmany diverse areas of application.
Weighted con-straint satisfaction extends the traditional constraintsatisfaction framework with soft constraints; suchconstraints are not required to be satisfied for a solu-tion to be valid, but constraints a given solution doessatisfy are rewarded according to weights assignedto them.
Soft constraints are perfect for expressingour preference for symbol trigrams, with the possi-bility of a back off to lower-degree n-grams if thereis reason to doubt the quality of the trigram predic-tions.Formally, a W-CSP is a tuple (X,D,C,W ).Here, X = {x1, x2, .
.
.
, xn} is a finite set of vari-ables.
D(x) is a function that maps each variableto its domain, that is, the set of values that variablecan take on.
C is the set of constraints.
While avariable?s domain dictates the values a single vari-able is allowed to take on, a constraint specifieswhich simultaneous value combinations over a num-ber of variables are allowed.
For a traditional (non-weighted) constraint satisfaction problem, a validsolution would be an assignment of values to thevariables that (1) are a member of the correspondingvariable?s domain, and (2) satisfy all constraints inthe set C .
Weighted constraint satisfaction, however,relaxes this requirement to satisfy all constraints.
In-stead, constraints are assigned weights that may beinterpreted as reflecting the importance of satisfyingthat constraint.Let a constraint c ?
C be defined as a functionthat maps each variable assignment to 1 if the con-straint is satisfied, or to 0 if it is not.
In addition, letW : C?
IR+ denote a function that maps each con-straint to a positive real value, reflecting the weightof that constraint.
Then, the optimal solution to aW-CSP is given by the following equation.x?
= arg maxx?cW (c)c(x)43Figure 1: Illustration of the constraints yielded by a given sequence of predicted class trigrams for the wordhand.
The constraints on the right have been marked with a number (between parentheses) that refers to thetrigram prediction on the left from which the constraint was derived.That is, the assignment of values to its variablesthat maximizes the sum of weights of the constraintsthat have been satisfied.Translating the terminology used in morpho-phonological tasks to the constraint satisfaction do-main, each letter maps to a variable, the domain ofwhich corresponds to the three overlapping candi-date symbols for this letter suggested by the trigramscovering the letter.
This provides us with a defini-tion of the function D, mapping variables to theirdomain.
In the following, yi,j denotes the candi-date symbol for letter xj predicted by the trigramassigned to letter xi.D(xi) = {yi?1,i, yi,i, yi+1,i}Constraints are extracted from the predicted tri-grams.
Given the goal of retaining predicted tri-grams in the output symbol sequence as much aspossible, the most important constraints are simplythe trigrams themselves.
A predicted trigram de-scribes a subsequence of length three of the entireoutput sequence; by turning such a trigram into aconstraint, we express the wish to have this trigramend up in the final output sequence.
(xi?1, xi, xi+1) = (yi,i?1, yi,i, yi,i+1),?iNo base classifier is flawless though, and there-fore not all predicted trigrams can be expected to becorrect.
Yet, even an incorrect trigram may carrysome useful information regarding the output se-quence: one trigram also covers two bigrams, andthree unigrams.
An incorrect trigram may still con-tain smaller subsequences of length one or two thatare correct.
Therefore, all of these are also mappedto constraints.
(xi?1, xi) = (yi,i?1, yi,i), ?i(xi, xi+1) = (yi,i, yi,i+1), ?ixi?1 = yi,i?1, ?ixi = yi,i, ?ixi+1 = yi,i+1, ?iTo illustrate the above procedure, Figure 1 showsthe constraints yielded by a given output sequenceof class trigrams for the word ?hand?.
With such anamount of overlapping constraints, the satisfactionproblem obtained easily becomes over-constrained,that is, no variable assignment exists that can sat-isfy all constraints without breaking another.
Evenonly one incorrectly predicted class trigram alreadyleads to two conflicting candidate symbols for oneof the letters at least.
In Figure 1, this is the casefor the letter ?d?, for which both the symbol ?d?
and?t?
are predicted.
On the other hand, without con-flicting candidate symbols, no inference would beneeded to start with.
The choice for the weightedconstraint satisfaction method always allows a solu-tion to be found, even in the presence of conflict-ing constraints.
Rather than requiring all constraintsto be satisfied, each constraint is assigned a certainweight; the optimal solution to the problem is an as-signment of values to the variables that optimizes the44Focus Trigram output classesLeft context letter Right context Phonemization Morph.
analysisb o o k b u s -b o o k i b u - s - -b o o k i n u - k - - -b o o k i n g - k I - - io o k i n g k I N - i -o k i n g I N - i - -k i n g N - - -Table 1: Seven labeled examples of phonemization and morphological analysis trigram mappings createdfor the word booking.sum of weights of the constraints that are satisfied.As weighted constraints are defined over overlap-ping subsequences of the output sequence, the bestsymbol assignment for each letter with respect to theweights of satisfied constraints is decided upon on aglobal sequence level.
This may imply taking intoaccount symbol assignments for surrounding lettersto select the best output symbol for a certain letter.In contrast, in non-global approaches, ignorant ofany sequential context, only the local classifier pre-diction with highest confidence is considered for se-lecting a letter?s output symbol.
By formulating ourinference procedure as a constraint satisfaction prob-lem, global output optimization comes for free: inconstraint satisfaction, the aim is also to find a glob-ally optimal assignment of variables taking into ac-count all constraints defined over them.
Yet, for sucha constraint satisfaction formulation to be effective,good constraint weights should be chosen, that is,weights that favor good output sequences over badones.Constraints can directly be traced back to a pre-diction made by the base classifier.
If two con-straints are in conflict, the one which the classifierwas most certain of should preferably be satisfied.In the W-CSP framework, this preference can be ex-pressed by weighting constraints according to theclassifier confidence for the originating trigram.
Forthe memory-based learner, we define the classifierconfidence for a predicted class as the weight as-signed to that class in the neighborhood of the testinstance, divided by the total weight of all classes.Let x denote a test instance, and c?
its pre-dicted class.
Constraints derived from this class areweighted according to the following rules:?
for a trigram constraint, the weight is simplythe base classifier?s confidence value for theclass c?;?
for a bigram constraint, the weight is the sumof the confidences for all trigram classes in thenearest-neighbor set of x that assign the samesymbol bigram to the letters spanned by theconstraint;?
for a unigram constraint, the weight is the sumof the confidences for all trigram classes in thenearest-neighbor set of x that assign the samesymbol to the letter spanned by the constraint.This weighting scheme results in an inferenceprocedure that behaves exactly as we already de-scribed intuitively in the beginning of this section.The preference for retaining the predicted trigramsin the output sequence is translated into high rewardsfor output sequences that do so, since such output se-quences not only receive credit for the satisfied tri-gram constraints, but also for all the bigram and un-igram constraints derived from that trigram; they arenecessarily satisfied as well.
Nonetheless, this pref-erence for trigrams may be abandoned if composinga certain part of the output sequence from severalsymbol bigrams or even unigrams results in higherrewards than when trigrams are used.
The latter mayhappen in cases where the base classifier is not con-fident about its trigram predictions.453 Data preparationIn our experiments we train classifiers on Englishand Dutch letter-phoneme conversion and morpho-logical analysis.
All data for the experiments de-scribed in this paper are extracted from the CELEXlexical databases for English and Dutch (Baayen etal., 1993).
We encode the examples for our baseclassifiers in a uniform way, along the following pro-cedure.
Given a word and (i) an aligned phone-mic transcription or (ii) an aligned encoding of amorphological analysis, we generate letter-by-letterwindows.
Each window takes one letter in focus,and includes three neighboring letters to the leftand to the right.
Each seven-letter input windowis associated to a trigram class label, composed ofthe focus class label aligned with the middle let-ter, plus its immediately preceding and followingclass labels.
Table 1 displays the seven examplesmade on the basis of the word booking, with tri-gram classes (as explained in Section 2) both forthe letter-phoneme conversion task and for the mor-phological analysis task.
The full aligned phone-mic transcription of booking is [bu-kIN-] (using theSAMPA coding of the international phonetic alpha-bet), and the morphological analysis of booking is[book]stem[ing]inflection.
The dashes in the phone-mic transcription are inserted to ensure a one-to-one mapping between letters and phonemes; the in-sertion was done by automatical alignment throughexpectation-maximization (Dempster et al, 1977).The English word phonemization data, extractedfrom the CELEX lexical database, contains 65,467words, on the basis of which we create a databaseof 573,170 examples.
The Dutch word phonemiza-tion data set consists of 293,825 words, totaling to3,181,345 examples.
Both data sets were aligned us-ing the expectation-maximization algorithm (Demp-ster et al, 1977), using a phonemic null character toequalize the number of symbols in cases in whichthe phonemic transcription is shorter than the ortho-graphic word, and using ?double phonemes?
(e.g.
[X] for [ks]) in cases where the phonemic transcrip-tion is longer, as in taxi ?
[tAksi].CELEX contains 336,698 morphological analy-ses of Dutch (which we converted to 3,209,090examples), and 65,558 analyses of English words(573,544 examples).
We converted the availableLeft Focus Right Trigramcontext letter context classa b n o A 0a b n o r A 0 0a b n o r m 0 0 0a b n o r m a 0 0 0b n o r m a l 0 0 0n o r m a l i 0 0 0o r m a l i t 0 0 0+Dar m a l i t e 0 0+Da A ?Nm a l i t e i 0+Da A ?N 0a l i t e i t A ?N 0 0l i t e i t e 0 0 0i t e i t e n 0 0 0t e i t e n 0 0 plurale i t e n 0 plural 0i t e n plural 0Table 2: Examples with morphological analysis tri-gram classes derived from the example word abnor-maliteiten.morphological information for the two languages ina coding scheme which is rather straightforward inthe case of English, and somewhat more compli-cated for Dutch.
For English, as exemplified in Ta-ble 1, a simple segmentation label marks the begin-ning of either a stem, an inflection (?s?
and ?i?
inTable 1), a stress-affecting affix, or a stress-neutralaffix (?1?
and ?2?, not shown in Table 1).
The cod-ing scheme for Dutch incorporates additional infor-mation on the part-of-speech of every stem and non-inflectional affix, the type of inflection, and also en-codes all spelling changes between the base lemmaforms and the surface word form.To illustrate the more complicated construction ofexamples for Dutch morphological analysis, Table 2displays the 15 instances derived from the Dutchexample word abnormaliteiten (abnormalities) andtheir associated classes.
The class of the first in-stance is A, which signifies that the morpheme start-ing in a is an adjective (A).
The class of the eighthinstance, 0+Da, indicates that at that position no seg-ment starts (0), but that an a was deleted at that po-sition (+Da, ?delete a?
here).
Next to deletions, in-sertions (+I) and replacements (+R, with a deletionand an insertion argument) can also occur.
Together46Language Task Unigrams TrigramsEnglish Letter-phon.
58 13,005Morphology 5 80Dutch Letter-phon.
201 17,538Morphology 3,831 14,795Table 3: Numbers of unigram and trigram classesfor the four tasks.these two classification labels code that the first mor-pheme is the adjective abnormaal.
The second mor-pheme, the suffix iteit, has class A ?N.
This com-plex tag, which is in fact a rewrite rule, indicates thatwhen iteit attaches right to an adjective (encoded byA ), the new combination becomes a noun (?N).Rewrite rule class labels occur exclusively with suf-fixes, that do not have a part-of-speech tag of theirown, but rather seek an attachment to form a com-plex morpheme with the part-of-speech tag.
Finally,the third morpheme is en, which is a plural inflectionthat by definition attaches to a noun.Logically, the number of trigram classes for eachtask is larger than the number of atomic classes;the actual numbers for the four tasks investigatedhere are displayed in Table 3.
The English morpho-logical analysis task has the lowest number of tri-gram classes, 80, due to the fact that there are onlyfive atomic classes in the original task, but for theother tasks the number of trigram classes is quitehigh; above 10,000.
With these numbers of classes,several machine learning algorithms are practicallyruled out, given their high sensitivity to numbers ofclasses (e.g., support vector machines or rule learn-ers).
Memory-based learning algorithms, however,are among a small set of machine learning algo-rithms that are insensitive to the number of classesboth in learning and in classification.4 ResultsWe performed experiments with the memory-basedlearning algorithm IB1, equipped with constraintsatisfaction inference post-processing, on the fouraforementioned tasks.
In one variant, IB1 was sim-ply used to predict atomic classes, while in the othervariant IB1 predicted trigram classes, and constraintsatisfaction inference was used for post-processingthe output sequences.
We chose to measure the gen-Language Method Word accuracyEnglish Unigram 80.0 ?0.75CSInf 85.4 ?0.71Dutch Unigram 41.3 ?0.48CSInf 51.9 ?0.48Table 4: Word accuracies on English and Dutchmorphological analysis by the default unigram clas-sifier and the trigram method with constraint satis-faction inference, with confidence intervals.Language Method Word accuracyEnglish Unigram 79.0 ?0.82CSInf 84.5 ?0.76Dutch Unigram 92.8 ?0.25CSInf 94.4 ?0.22Table 5: Word accuracies on English and Dutchletter-phoneme conversion by the default unigramclassifier and the trigram method with constraint sat-isfaction inference, with confidence intervals.eralization performance of our trained classifiers ona single 90% training set ?
10% test set split of eachdata set (after shuffling the data randomly at theword level), and measuring the percentage of fullycorrectly phonemized words or fully correctly mor-phologically analyzed words ?
arguably the mostcritical and unbiased performance metric for bothtasks.
Additionally we performed bootstrap resam-pling (Noreen, 1989) to obtain confidence intervals.Table 4 lists the word accuracies obtained on theEnglish and Dutch morphological analysis tasks.Constraint satisfaction inference significantly out-performs the systems that predict atomic unigramclasses, by a large margin.
While the absolute differ-ences in scores between the two variants of Englishmorphological analysis is 5.4%, the error reductionis an impressive 27%.Table 5 displays the word phonemization accu-racies of both variants on both languages.
Again,significant improvements over the baseline classifiercan be observed; the confidence intervals are widelyapart.
Error reductions for both languages are im-pressive: 26% for English, and 22% for Dutch.475 DiscussionWe have presented constraint satisfaction inferenceas a global method to repair errors made by a localclassifier.
This classifier is a memory-based learnerpredicting overlapping trigrams, creating a space ofpossible output sequences in which the inferenceprocedure finds the globally optimal one.
This glob-ally optimal sequence is the one that adheres best tothe trigram, bigram, and unigram sub-sequence con-straints present in the predictions of the local classi-fier, weighted by the confidences of the classifier, ina back-off order from trigrams to unigrams.The method is shown to significantly outperforma memory-based classifier predicting atomic classesand lacking any global post-processing, which haspreviously been shown to exhibit successful perfor-mance (Van den Bosch and Daelemans, 1993; Vanden Bosch and Daelemans, 1999).
(While this wasthe reason for using memory-based learning, wenote that the constraint satisfaction inference and itsunderlying trigram-based classification method canbe applied to any machine-learning classifier.)
Thelarge improvements (27% and 26% error reductionson the two English tasks, 18% and 22% on the twoDutch tasks) can arguably be taken as an indicationthat this method may be quite effective in general inmorpho-phonological sequence processing tasks.Apparently, the constraint-satisfaction method isable to avoid more errors than to add them.
At closerinspection, comparing cases in which the atomicclassifier generates errors and constraint satisfactioninference does not, we find that the type of avoidederror, when compared to the unigram classifier, dif-fers per task.
On the morphological analysis task,we identify repairs where (1) a correct segmentationis inserted, (2) a false segmentation is not placed,and (3) a tag is switched.
As Table 6 shows in its up-per four lines, in the case of English most repairs in-volve correctly inserted segmentations, but the othertwo categories are also quite frequent.
In the case ofDutch the most common repair is a switch from anincorrect tag, placed at the right segmentation posi-tion, to the correct tag at that point.
Given that thereare over three thousand possible tags in our compli-cated Dutch morphological analysis task, this is in-deed a likely area where there is room for improve-ment.Morphological analysis repairs English DutchInsert segmentation 193 1,087Delete segmentation 158 1,083Switch tag 138 2,505Letter-phoneme repairs English DutchAlignment 1,049 239Correct vowel 32 94Correct consonant 275 73Table 6: Numbers of repaired errors divided overthree categories of morphological analysis classifi-cations (top) and letter-phoneme conversions (bot-tom) of the constraint satisfaction inference methodas compared to the unigram classifier.The bottom four lines of Table 6 lists the counts ofrepaired errors in word phonemization in both lan-guages, where we distinguish between (1) alignmentrepairs between phonemes and alignment symbols(where phonemes are corrected to phonemic nulls,or vice versa), (2) switches from incorrect non-nullphonemes to correct vowels, and (3) switches fromincorrect non-null phonemes to correct consonants.Contrary to expectation, it is not the second vowelcategory in which most repairs are made (many ofthe vowel errors in fact remain in the output), butthe alignment category, in both languages.
At pointswhere the local unigram classifier sometimes incor-rectly predicts a phoneme twice, where it shouldhave predicted it along with a phonemic null, theconstraint satisfaction inference method never gen-erates a double phoneme.
Hence, the method suc-ceeds in generating sequences that are possible, andavoiding impossible sub-sequences.
At the sametime, a possible sequence is not necessarily the cor-rect sequence, so this method can be expected to stillmake errors on the identity of labels in the output se-quence.In future work we plan to test a range of n-gramwidths exceeding the current trigrams.
Preliminaryresults suggest that the method retains a positive ef-fect over the baseline with n > 3, but it does notoutperform the n = 3 case.
We also intend to testthe method with a range of different machine learn-ing methods, since as we noted before the constraint-satisfaction inference method and its underlying n-gram output subsequence classification method can48be applied to any machine learning classification al-gorithm in principle, as is already supported by pre-liminary work in this direction.Also, we plan comparisons to the work ofStroppa and Yvon (2005) and Damper and East-mond (1997) on sequence-global analogy-basedmodels for morpho-phonological processing, sincethe main difference between this related work andours is that both alternatives are based on work-ing units of variable width, rather than our fixed-width n-grams, and also their analogical reasoningis based on interestingly different principles than ourk-nearest neighbor classification rule, such as theuse of analogical proportions by Stroppa and Yvon(2005).AcknowledgementsThis research was funded by NWO, the NetherlandsOrganization for Scientific Research, as part of theIMIX Programme.
The authors would like to thankWalter Daelemans for fruitful discussions, and threeanonymous reviewers for their insightful comments.ReferencesD.
W. Aha, D. Kibler, and M. Albert.
1991.
Instance-based learning algorithms.
Machine Learning, 6:37?66.R.
H. Baayen, R. Piepenbrock, and H. van Rijn.
1993.The CELEX lexical data base on CD-ROM.
LinguisticData Consortium, Philadelphia, PA.W.
Daelemans, J. Zavrel, K. Van der Sloot, and A. Vanden Bosch.
2004.
TiMBL: Tilburg memory basedlearner, version 5.1.0, reference guide.
Technical Re-port ILK 04-02, ILK Research Group, Tilburg Univer-sity.R.
I. Damper and J. F. G. Eastmond.
1997.
Pronuncia-tion by analogy: impact of implementational choiceson performance.
Language and Speech, 40:1?23.E.
Daya, D. Roth, and S. Wintner.
2004.
LearningHebrew roots: Machine learning with linguistic con-straints.
In Dekang Lin and Dekai Wu, editors, Pro-ceedings of EMNLP 2004, pages 357?364, Barcelona,Spain, July.
Association for Computational Linguis-tics.A.P.
Dempster, N.M. Laird, and D.B.
Rubin.
1977.
Max-imum likelihood from incomplete data via the EM al-gorithm.
Journal of the Royal Statistical Society, Se-ries B (Methodological), 39(1):1?38.T.
G. Dietterich, H. Hild, and G. Bakiri.
1995.
A com-parison of ID3 and backpropagation for English text-to-speech mapping.
Machine Learning, 19(1):5?28.E.
Noreen.
1989.
Computer-intensive methods for test-ing hypotheses: an introduction.
John Wiley and sons.A.
Prince and P. Smolensky.
2004.
Optimality The-ory: Constraint Interaction in Generative Grammar.Blackwell Publishers.V.
Punyakanok and D. Roth.
2001.
The use of classifiersin sequential inference.
In NIPS-13; The 2000 Con-ference on Advances in Neural Information ProcessingSystems, pages 995?1001.
The MIT Press.T.J.
Sejnowski and C.S.
Rosenberg.
1987.
Parallel net-works that learn to pronounce english text.
ComplexSystems, 1:145?168.N.
Stroppa and F. Yvon.
2005.
An analogical learnerfor morphological analysis.
In Proceedings of the9th Conference on Computational Natural LanguageLearning, pages 120?127.
Association for Computa-tional Linguistics.K.
Torkkola.
1993.
An efficient way to learn Englishgrapheme-to-phoneme rules automatically.
In Pro-ceedings of the International Conference on Acoustics,Speech, and Signal Processing (ICASSP), volume 2,pages 199?202, Minneapolis.A.
Van den Bosch and W. Daelemans.
1993.
Data-oriented methods for grapheme-to-phoneme conver-sion.
In Proceedings of the 6th Conference of theEACL, pages 45?53.A.
Van den Bosch and W. Daelemans.
1999.
Memory-based morphological analysis.
In Proceedings of the37th Annual Meeting of the ACL, pages 285?292, SanFrancisco, CA.
Morgan Kaufmann.A.
Van den Bosch and W. Daelemans.
2005.
Improv-ing sequence segmentation learning by predicting tri-grams.
In I. Dagan and D. Gildea, editors, Proceed-ings of the Ninth Conference on Computational Natu-ral Language Learning.A.
Weijters.
1991.
A simple look-up procedure supe-rior to NETtalk?
In Proceedings of the InternationalConference on Artificial Neural Networks - ICANN-91,Espoo, Finland.F.
Yvon.
1996.
Prononcer par analogie: motivation,formalisation et e?valuation.
Ph.D. thesis, Ecole Na-tionale Supe?rieure des Te?le?communication, Paris.49
