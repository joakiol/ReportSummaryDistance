St ructure -Shar ing  in Lex ica l  Representat ionDaniel Fllekinger, Carl Pollard, and Thomas WasowHewlett-Packard Laboratories1501 Page Mill RoadPalo Alto, CA.. 94~O3, USAAbst ractThe lexicon now plays a central role in our imple-mentation of a Head-driven Phrase Structure Grammar(HPSG), given the massive relocation into the lexiconof linguistic information that was carried by the phrasestructure rules in the old GPSG system.
HPSG's gram-max contains fewer tha4z twenty (very general) rules;its predecessor required over 350 to achieve roughlythe same coverage.
This simplification of the gram-max is made possible by an enrichment of the structureand content of lexical entries, using both inhcrit~ncemechanisms and lexical rules to represent thc linguis-tic information in a general and efficient form.
We willargue that our mechanisms for structure-sharing notonly provide the ability to express important linguisticgeneralizations about the lexicon, but also make possi-ble an efficient, readily modifiable implementation thatwe find quite adequate for continuing development of alarge natural anguage system.1.
In t roduct ionThe project we refer to as HPSG is the currentphase of an ongoing effort at Hewiett-Pa~',axd Labo-ratories to develop an English language understandingsystem which implements current work in theoreticallinguistics.
Incorporating innovations in the areas oflexicon, grammar, parser, and semantics, HPSG is thesuccessor to the GPSG system reported on at the 1982ACL meeting z Like the GPSG system, the current im-plementation is based on the linguistic theory knownGeneralized Phrase Structure Grammar, 2 though in-corporating insights from Carl Pollard's recent work onHead Grammars ~which lead us to employ a richer lex-icon and a significantly smaller grammar.
We reporthere on the structure of our lexicon, the mechanismsused in its representation, and the resulting sharp de-cre~e in the number of phrase structure rules needed.
4I Gawron, et al (1982).2 Gazdax, Klein, Puilum, and Sag (1985).3 Pollard (1984).2.
Mechan isms employedWe employ three types of mechanisms for structure-sharing in our representation of the lexicon for theI-IPSG system: inheritance, lexical rules, and an Ol>-eration to create nouns from ordinary database enti-ties.
In order to present a detailed description of thesemechanisms, we offer a brief sketch of the representa-tion language in which the lexicon is constructed.
Thislanguage is a descendant of FRL  and is currently underdevelopment at HP  Labs.
s Those readers familiar withframe-based knowledge representation will not need thereview provided in the next section.2.1.
The  representat ion  languageThe basic data structures of the representation lan-guage axe frames with slots, superficially analogous toPascal records with fields.
However, frames axe linkedtogether by mew of class and inheritance links, suchthat when a particular frame F0 is an instance or sub-class of a more general frame F1, information stored inF1 can be considered part of the description of F0.
Forexample, our lexicon database contains frames specify-ing properties of classes of words, such as the VERBclass, which numbers among its subclasses BASE andFINITE.
Having specified on the VERB class framethat all verbs have the value V for the MAJOR fea-ture, this value does not have to be stipulated againon each of the subclasses, since the information will beinherited by each subclass.
This class linkage is transi-tive, so information can be inherited through any num-ber of intermediate frames.
Thus any instance of theF IN ITE class will inherit the value F INITE for the fea-ture FORM directly from the F INITE class frame, andwill also inherit the value V for the MAJOR featureindirectly from the VERB class frame.4 Significant contributions to the basic design of this lex-icon were made by Jean Mark Gawron and Elizabeth AnnPanlson, members of the Natural Language Project whenthe work on HPSG was begun in 1983.
We axe also in-debted to Geoffrey K. Pullum, a consultant on the project,for valuable a.~istaJnce in the writing of this paper.s For a description of this language, see Rosenberg(1983).262Of course, to make good use of this information,one must be able to exercise some degree of controlover the methods of access to the information stored ina hierarchical structure of this sort, to allow for sub-regularities and exceptions, among other things.
Thelanguage provides two distinct modes of inheritance;the one we will call the normal mode, the second thecomplete mode.
When using the normal mode to col-lect inheritable information, one starts with the framein question and runs up the inheritance links in thehierarchy, stopping when the first actual value for therelevant slot is found.
The complete mode of inheri-tance simply involves collecting all available values forthe relevant slot, beginning with the particular frameand going all the way up to the top of the hierarchy.We illustrated the complete mode above in describingthe feature values that a finite verb like works wouldinherit.
To illustrate a use of the normal mode, wenote that the VERB class will specify that the CASEof.the ordinary verb's subject is OB JECT IVE ,  as inMar~/ wanted him to work.
(not Mary wanted he towork.).
But the subjects of finite verbs have nomina-tive case, so in the FINITE class frame we stipulate(CASE NOMINATIVE) for the subject.
If we used thecomplete mode of inheritance in determining the casefor a finite verb's subject, we would have a contradic-tion, but by using the normal mode, we find the mo~elocal (NOMINATIVE) value for CASE first, and stop.In short, when normal mode inheritance is employed,locally declared values override values inherited fromhigher up the hierarchy.The third and final property of the representationlanguage that is crucial to our characterization of thelexicon is the ability of a frame to inherit informationalong more than one inheritance path.
For example,the lexical frame for the finite verb work# is not only aninstance of F INITE (a subclass of VERB) ,  but also aninstance of INTRANSIT IVE ,  from which it inherits theinformation that it requires a subject and nothing elsein order to make a complete sentence.
This ability toestablish multiple inheritance links for a frame provesto be a powerful tool, as we will illustrate further below.2.2.
Inher i tanceHaving presented some of the tools for inheritance,let us now see how and why this mechanism provesuseful for representing the information about the lexi-con that is needed to parse sentences of English.
~ Wemake use of the frame-based representation languageto impose a rich hierarchical structure on our lexicon,distributing throughout this structure the informationneeded to describe the particular lexical items, so thateach distinct property which holds for a given class ofwords need only be stated once.
We do this by defin-ing generic lexicai fr-a~nes for grammatical categories atseveral levels of abstraction, beginning at the top witha generic WORD fr~rne, then dividing and subdividinginto ever more specific categories until we hit bottomin frames for actual English words.
An example willhelp clarify the way in which we use this first basicmechanism for sharing structure in representing lexicalinformation.We employ, among others, generic (class) framesfor VERB, TRANSITIVE, and AUXILIARY, each con-taining just that information which is the default for itsinstances.
The AUXILIARY frame stores the fact thatin generaJ auxiliary verbs have as their complement averb phrase in base form (e.g.
the base VP  be a man-ager in un'/\[ be a manager).
One of the exceptions tothis generalization is the auxiliary verb have as in/mvebeen consultants, where the complement VP  must be apast participle rather than in base form.
The excep-tion is handled by specifying the past participle in theCOMPLEMENT slot for the HAVE frame, then beingsure to use the normal mode of inheritance when askingfor the syntactic form of a verb's complement.To illustrate the use we make of the complete modeof inheritance, we first note that we follow most currentsyntactic theories in assuming that a syntactic categoryis composed (in part) of a set of syntactic features eachspecified for one or more out of a range of permittedvalues.
So the category to which the auxiliary verb/za8belongs can be specified (in part) by the following setof feature-value pairs:\[(MAJOR V) (TENSE PRES)(AGREEMENT 3RD-SING)(CONTROL SSR) (AUX PLUS)!Now if we have included among our generic framesone for the category of present-tense v rbs, and an in-stance of this class for third-person-singular present-tense verbs, then we can distribute the structuregiven in the list above in the following way.
Wespecify that the generic VERB frame includes in itsfeatures (MAJOR V), that the PRESENT-TENSEframe includes (TENSE PRES), that the THIRD-SINGframe includes (AGREEMENT 3RD-SING), that theSUBJECT-RAISE frame includes (CONTROL SSR),and the AUXILIARY frame includes (AUX PLUS).Then we can avoid saying anything explicitly about fea-tures in the frame for the auxiliary verb/~ave; we needonly make sure it is an instance of the three rather un-related frames THIRD-SING, SUBJECT-RAISE, andAHXILIARY.
As long as we use the complete mode6 The use of inheritance for e.~ciently representing infor-mation about the lexicon is by no means an innovation ofours; see Bobrow and Webber (1980a,b) for a descriptionof an implementation making central use of inheritance.However, we believe that the powerful tools for inheritance(particularly that of multiple inheritance) provided by the?
representation language we use have allowed us to give anunusually precise, easily modifiable characterization f thegeneric lexicon, one which greatly facilitates our continuingefforts to reduce the number of necessary phrase structurerules).263of inheritance when asking for the value of the FEA-TURES slot for the HAVE frame, we will collect thefive feature-value pairs listed above, by following thethree inheritance path links up through the hierarchy,collecting all of the values that we find.2.3.
Lexical rulesThe second principal mechanism we employ forstructure-sharing is one familiar to linguists: the lex-ical redundancy rule 7, which we use to capture bothinflectional and derivational regularities among iexicalentries.
In our current implementation, we have madethe lexical rules directional, in each case defining oneclass as input to the rule, and a related but distinctclass as output.
By providing with each lexical rule ageneric lass frame which specifies the gener~ form andpredictable properties of the rule's output, we avoidunnecessary work when the lexical rule applies.
Theparticular output frame will thus get its specificationsfrom two sources: idiosyncratic information copied orcomputed from the particular input frame, and pre-dictable information available via the class/inheritancelinks.As usual, we depend on an example to make thenotions clear; consider the lexical rule which takes ac-tive, transitive verb frames as input, and produces thecorresponding passive verb frames.
A prose descriptionof this passive lexical rule follows:Passive Lexicai RuleIf F0 is a trm~sitive verb frame with spelling XXX,then F1 is the corresponding passive frame, where(I) FI is an instance of the generic PASSIVE classframe(2) FI has as its spelling whatever the pastparticip|e's spelling is for F0 (XXXED ifregular, stipulated if irregular)(3) F1 has as its subject's role the role of F0'sobject, and assigns the role of F0's subject toF1's optional PP-BY.
(4) F1 has OBJECT deleted from its obligatory list.
(5) F1 has as its semantics the semantics of FO.It is in the TRANSITIVE frame that we declarethe applicability of the passive \[exical rule, which po-tentially can apply to each instance (unless explicitlyblocked in some frame lower in the lexicon hierarchy,for some particular verb like rc-~emble).
By triggeringparticular lexical rules from selected generic frames, weavoid unnecessary ~ttempts to apply irrelevant ruleseach time ~ new lexical item is created.
The TRANSI-TIVE frarne, then, has roughly the following structure:v See, e.g., Stanley (1967), Jackendoff (1975), Bresnan(1982).
(TRANSIT IVE(CLASSES (subcyclic))(OBL IGATORY (object) (subject))(FEATURES (control trans))(LEX-RULES (passive-rule)))The generic frame of which every output from thepassive rule is an instance looks as follows:(PASSIVE(CLASSES (verb))(FEATURES (predicative plus) (form pas))(OPTIONAL (pp-by)))An example, then, of a verb frame which serves asinput to the passive rule is the frame for the transitiveverb make, whose entry in the lexicon is given below.Keep in mind that a great deal of inherited informationis part of the description for make, but does not needto be mentio,ted in the entry for make below; put dif-ferently, the relative lack of grammatical informationappearing in the make entry below is a consequenceof our maintaining the strong position that only infor-mation which is idiosyncratic should be included in alexical entry.
(MAKE(CLASSES (main) (base) (transitive))(SPELLING (make))(SUBJECT (role (ma~e.er)))(OBJECT (role (make.ed)))(LEX-RULES (past-participle(irreg-spelh ~made"))(past(irreg-spelh ~made"))) )Upon application of the passive lexlcal rule ~o ~Lemake frame, the corresponding passive frame MADE-PASSIVE is produced, looking like this:(MADE-PASSIVE(CLASSES (main)(passive)(transitive))(SPELLING (made))(SUBJECT (role (make.ed)))(PP-BY (role (make.er))))Note that the MADE-PASSIVE frame is still amain verb and still transitive, but is not connected byany inheritance link to the active make fro, me; the pas-sive frame is not an instance of the active frame.
Thisabsence of any actual inheritance link between inputand output frames is generally true for all lexical rules,264not surprisingly once the inheritance link is understood.As a result, all idiosyncratic information must (looselyspeaking) be copied from the input to the output frame,or it will be lost.
Implicit in this last remark is the as-sumption that properties of a lexical item which areidiosyncratic should only be stated once by the creatorof the lexicon, and then propagated as appropriate bylexical rules operating on the basic frame which wasentered by hand.All of our lexical rules, including both inflectionalrules, such as the lexical rule which makes plural nounsfrom singular nouns, and derivational rules, such as thenominalization rule which produces nouns from verbs,share the following properties: each rule specifies theclass of frames which are permissible inputs; each rulespecifies a generic frame of which every one of the rule'soutputs is an instance; each rule copies idiosyncraticinformation from the input frame while avoiding copy-ing information which can still be inherited; each ruletakes as input a single-word lexical frame and producesa single-word lexical frame (no phrases in either case);each rule permits the input frame to stipulate an ir-regular spelling for the corresponding output frame,blocking the regular spelling; and each rule produces anoutput which cannot be input to the same rule.
Mostof these properties we believe to be well-motivated,though it may be that, for example, a proper treat-ment of idioms will cause us to weaken the single-wordinput and output restriction, or we may find a lexicalrule which can apply to its own output.
The wealthof work in theoretical linguistics on properties of lexi-cal rules should greatly facilitate the fine-tuning of ourimplementation we extend our coverage.One weakness of the current implementation of lex-ical rules is our failure to represent the \[exical rulesthemselves as frames, thus preventing us from tak-ing advantage of inheritance and other representationaltools that we use to good purpose both for the lexicalrules and for the phrase structure rules, about whichwe'll say more below.A final remark about lexical rules involves therole of some of our lexical rules as replacements formetarules in the standard GPSG framework.
Thosefamiliar with recent developments in that frameworkare aware that metarules axe now viewed as necessarilyconstrained to ouerate only on lexically-headed phrasestructure rules, s but once that move has been made, itis then not such a drastic move to attempt the elimionation of metarules altogether in favor of \]exical rules.
?This is the very road we are on.
We maintain that theelimination of metarules is not only a aice move theo-retically, bat also advantageous for implementation.s See Fiickinger (1983) for an initial motivation for sucha restriction on metarules.9 See Pollard (1985) for a more detailed discussion ofthis important point.2.4.
Nouns  f rom database  ent i t iesThe third mechanism we use for structure-sharingallows us to leave out of the lexicon altogether the vastmajority of cow.molt and proper nouns that refer to en-titles in the target database, including in the lexicononly those nouns which have some idiosyncratic prop-erty, such as nouns with irregular plural forms, or massnouns.
This mechanism is simply a procedure muchlike a lexical rule, but which takes as input the nameof some actual database frame, and produces a lexi-cal frame whose spelling slot now contains the name ofthe database frame, and whose semantics correspondsto the database frame.
Such a frame is ordinarily cre-ated when parsing a given sentence in which the wordnaming the database frame appears, and is then dis-carded once the query is processed.
Of course, in or-der for this strategy to work, the database frame mustsomehow be linked to the word that refers to it, ei-ther by having the frame name be the same as theword, or by having constructed a list of pairings of eachdatabase frame with the English spelling for words thatrefer to that frame.
Unlike the other two mechanisms(inheritance and lexical redundancy rules), this pair-ing of database frames with \[exical entries tends to beapplication-specific, since the front end of the systemmust depend on a particular convention for naming ormarking database frames.
Yet the underlying intuitionis a reasonable one, namely that when the parser meetsup with a word it doesn't recognize, it attempts to treatit as the name of something, either a proper noun or acommon noun, essentially leaving it up to the databaseto know whether the name actually refers to anything.As an example, imagine that the frame for Pullum(the consultant, not the prouer noun) is present in thetarget database, and that we wish to process a querywhich refers by name to Pullum (such as Does Puilurnhave a modernS).
\[t would not be necessary to haveconstructed a proper-name frame for Pullum before-hand, given that the database frame is named Pullum.Instead, the mechanism just introduced would note, inanalyzing the query, that Pullum was the name of aframe in the target database; it would consequentlycreate the necessary proper-name frame usable by theparser, possibly discarding it later if space were at apremium.
Where an application permits this elimina-tion of most common and proper nouns from the lexi-con, one gains not 0nly considerable space savings, buta sharp reduction in the seed for additions to the lexi-con by salve users as tile target database grows.2.5.
On-the-fly fra~nesAll three of the mechanisms for structure-sharingthat we have discussed here have in common the ad-ditional important property that they can be appliedwithout modification either before ever analyzing aquery, or on the fly when trying to handle a partic-ular query.
This property is important for us largely265because in developing the system we need to be able tomake alterations in the structure of the lexicon, so theability to apply these mechanisms on the fly means thatchanges to the lexicon have an immediate and pow-erful effect on the behavior of the system.
As men-tioned earlier, another significant factor has to do witht ime/space trade-offs, weighing the cost in memory ofstoring redundantly specified lexical entries against hecost in time of having to reconstruct these derived lex-ical entries afresh each time.
Depending on the par-t itular development task, one of the two options forderiving lexical items is preferabte over the other, b~ttboth options need to be available.3.
The  grammarAs we advertised above, the wholesale moving ofgrammatical information from the phrase structurerules to the lexicon has led to a dramatic reductionin the number of these rules.
The rules that remainare usually quite general in nature, and make crucialuse of the notion head of a constituent, where the headof a noun phrase is a noun, the head of a verb phrase(and of a sentence~ is a verb.
and so on.
In each case.
itis the head that carries most of the information aboutwhat syntactic and semantic properties its sister(s) inthe constituent must have.
l?
For example, the singlerule which follows is sufficient o construct he phrasestructure for the sentence The consultant works.Grammar-Rule-tX -> Ct II\[CONTROL INTRANS\]The rule is first used to construct the noun phrasethe eor~ultant, taking consultant as the head, and usingthe information on the lexical frame CONSULTANT-COMMON (which is inherited from the COMMON-NOUN class) that it requires a determiner as its onlycomplement in order to form a complete noun phrase.Then the rule is used again with the lexical frameWORK-THIRD-S ING taken as the head, ~.nd using theinformation that it requi~es a nominative singular nounphrase (which was just constructed) as its only obliga-tory complement in order to make a complete sentence.Another example will also allow ,as to illustrate howinformation once thought to be clearly the responsi-bility of phrase structure rules is in fact more simplyrepresented as lexical information, once one has thepower of a highly structured lexicon with inheritanceavailable.
A second rule in the grammar is providedto admit an optional constituent after an intransitivehead, such as wor~ on Tuesdays or modem \[or Pullura:\[ntransitive-Adj unct-RuleX -> H\[CONTROL INTRANS\] ADJUNCT10 See Pollard (1984) for a thorough discussion of headgrammars.This works quite well for prepositional phrases, butis by no means restricted to them.
Eventually we no-ticed that another standard rule of English grammarcould be eliminated given the existence of this rule;namely, the rule which admits relative clauses as inman who work~ for the sentence Smith hired the manwho works:Relative-Clause-RuleX -> II\[MAJOR N\] S\[REL\]It should soon be clear that if we add a single pieceof information to the generic COMMON-NOUN classframe, we can eliminate this rule.
All that is necessaryis to specify that a permissible adjunct for commonnouns is a relative clause (leaving aside the semantics,which is quite tractable).
By stating this fact on theCOMMON-NOUN frame, every lexical common nounwill be ready to accept a relative clause in just the rightplace using the Intransitive-Adjunct-Rule.
In fact, itseems we can use the same strategy to eliminate anyother specialized phrase structure rules for admittingpost-nominal modifiers (such as so-called reduced rel-ative clauses as in The people working for Smith arecoasultants).This example suggests one direction of research weaxe pursuing: to reduce the number of rules in thegrammar to an absolute minimum.
At present it stillseems to be the case that some small number of phrasestructure rules will always be necessary; for example,we seem to be unable to escape a PS rule which ad-mits plural nouns as full noun phrases without a de-terminer, as in Consultants work (but not *Consultantwork).
Relevant issues we will leave unaddressed hereinvolve the role of the PS rules in specifying linear or-der of constituents, whether the linking rules of GPSG(which we still employ) could ever be pushed into thelexicon, and whether in fact both order and linkingrules ought to be pushed instead into the parser.4.
ConclusionHaving sketched the mechanisms employed in re-ducing redundant specification in the lexicon for theHPSG system, and having indicated the brevity of thegrammar which results from our rich lexicon, we nowsummarize the advantages we see in representing thelexicon as we do, apart from the obvious advantage ofa much smaller grammar.
These advantages have to doin large part with the rigors of developing a large nat-ural language system, but correspond at several pointsto concerns in theoretical linguistics as well.First axe a set of advantages that derive from beingable to make a single substitution or addition which willeffect a desired change throughout the system.
Thisability obviously eases the task of development basedon experimentation, since one can quickly try severalminor variations of, say, feature combinations and accu-266rarely judge the result on the overall system.
Of equalimportance to development is the consistency provided,given that one can make a modification to, say, thefeatures for plural nouns, and be sure that all regu-lar nouns will reflect the change consistently.
Third,we can handle many additions to the lexicon by userswithout requiring expertise of the user in getting all theparticular details of a lexical entry right, for an impor-tant (though far from complete) range of cases.
Notethat this ability to handle innovations seems to have aclose parallel in people's ability to predict regular in-flected forms for a word never before encountered.A second advantage that comes largely for freegiven the inheritance mechanisms we employ involvesthe phenomenon referred to as blocking II, where theexistence of an irregular form of a word precludes theapplication of a lexical rule which would otherwise pro-duce the corresponding regular form.
By allowing in-dividual lexical entries to turn off the relevant lexicalrules based on the presence in the ,frame of an irreg-ular form, we avoid producing, say, the regular pasttense form =maked, since as we saw, the entry for makewarns explicitly of an irregular spelling for the pasttense form.Already mentioned above was a third advantageof using the mechanisms we do, namely that we canuse inheritance to help us specify quite precisely thedomain of a particular lexical rule, rather than havingto try every lexical rule on every new frame only todiscover that in most cases the rule fails to apply.Finally, we derive an intriguing benefit from hav-ing the ability to create on-the-fly noun frames for any-database entry, and from our decision to store our lex-ical items using the same representation language thatis used for the target database: we are able without ad-ditional effort to answer queries about the make-up ofthe natural language system itself.
That is, we can getan accurate answer to a question like How many verbsare there?
in exactly the way that we answer the ques-tion IIom many managers are there ?.
This ability of oursystem to reflect upon its own structure may prove tobe much more than a curiosity as the system continuesto grow; it may well become an essential tool for thecontinued development of the system itself.
The poten-tial for usefulness of this reflective property is enhancedby the fact that we now also represent our grammarand several other data structures for the system in thissame frame representation language, and may progressto representing in frames other intermediate stages ofthe processing of a sentence.
This enhanced ability toextend the lexicai coverage of our system frees us to in-vest more effort in meeting the many other challengesof developing a practical, extensible implementation ofa natural language system embedded in a aerious lin-guistic theory.REFERENCESAronoff, M. (1976) Word Formation in GenerativeGrammar.
Linguistic Inquiry Monograph i. Cam-bridge, Mass.
: The MIT Press.Bobrow, R. and B. Webber (1980a) ~Psi-Klone" inProceedings of the 1980 CSCSI/SCEIO AI Con-/create, Victoria, B.C.Bobrow, R. and B. Webber (1980b) ~Knowledge Rep-resentation for Syntactic/ Semantic Processing," inProceedinos of the First Annual National Confer-ence on Artificial Intelligence, Stanford University.Bresnan, J., ed.
(1982} The Mental Representation fGrammatical Relations.
Cambridge, Mass.
: TheMIT Press.Flickinger, Daniel P. (1983) ~Lexical Heads andPhrasal Gaps," in Flickinger, et al, Proceedinqs ofthe West Coast Conference on Formal Linguistics,vol.
2.
Stanford University Linguistics Dept.Gawron, J. et al (1982) ~Processing English with aGeneralized Phrase Structure Grammar", A CLProceedings 20.Gazdar, G., E. Klein, G. K. Pullum, and I.
A.
Sag(1985) Generalized Phrase Structure Grammar.Cambridge, Mass.
: Harvard University Press.Jackendoff, R. (1975) ~Morphological and SemanticRegularities in the Lexicon", Language 51, no.
3.Pollard, C. (1984) Generalized Phrase Structure Gram-mars, Head Grammars, and Natural Language.Doctoral dissertation, Stanford University.Pollard, C. (1985) ~Phrase Structure Grammar  With-out Metarules," presented at the 1985 meeting ofthe West Coast Conference on Formal Linguistics,Los Angeles.Rosenberg, S. (1983) ~HPRL:  A Language for BuildingExpert Systems", \[JCAI83: 215-217.Stanley, R. (1967) "Redundancy Rules in Phonology',Language 43, no.
I.tx See Aronoff (1976) for discussion.287
