Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 61?69,ACL HLT 2011, Portland, Oregon, USA, June 2011. c?2011 Association for Computational LinguisticsImproving Reordering for Statistical Machine Translation with SmoothedPriors and Syntactic FeaturesBing Xiang, Niyu Ge, and Abraham IttycheriahIBM T. J. Watson Research CenterYorktown Heights, NY 10598{bxiang,niyuge,abei}@us.ibm.comAbstractIn this paper we propose several novel ap-proaches to improve phrase reordering forstatistical machine translation in the frame-work of maximum-entropy-based modeling.A smoothed prior probability is introduced totake into account the distortion effect in thepriors.
In addition to that we propose multi-ple novel distortion features based on syntac-tic parsing.
A new metric is also introduced tomeasure the effect of distortion in the transla-tion hypotheses.
We show that both smoothedpriors and syntax-based features help to sig-nificantly improve the reordering and hencethe translation performance on a large-scaleChinese-to-English machine translation task.1 IntroductionOver the past decade, statistical machine translation(SMT) has evolved into an attractive area in naturallanguage processing.
SMT takes a source sequence,S = [s1 s2 .
.
.
sK ] from the source language, andgenerates a target sequence, T ?
= [t1 t2 .
.
.
tL], byfinding the most likely translation given by:T ?
= arg maxTp(T |S) (1)In most of the existing approaches, following(Brown et al, 1993), Eq.
(1) is factored using thesource-channel model intoT ?
= arg maxTp(S|T )p?
(T ), (2)where the two models, the translation model,p(S|T ), and the language model (LM), p(T ), are es-timated separately: the former using a parallel cor-pus and a hidden alignment model and the latter us-ing a typically much larger monolingual corpus.
Theweighting factor ?
is typically tuned on a develop-ment test set by optimizing a translation accuracycriterion such as BLEU (Papineni et al, 2002).In recent years, among all the proposed ap-proaches, the phrase-based method has becomethe widely adopted one in SMT due to its capa-bility of capturing local context information fromadjacent words.
Word order in the translationoutput relies on how the phrases are reorderedbased on both language model scores and distor-tion cost/penalty (Koehn et al, 2003), among allthe features utilized in a maximum-entropy (log-linear) model (Och and Ney, 2002).
The distor-tion cost utilized during the decoding is usually apenalty linearly proportional to the number of wordsin the source sentence that are skipped in a transla-tion path.In this paper, we propose several novel ap-proaches to improve reordering in the phrase-basedtranslation with a maximum-entropy model.
In Sec-tion 2, we review the previous work that focused onthe distortion and phrase reordering in SMT.
In Sec-tion 3, we briefly review the baseline of this work.In Section 4, we introduce a smoothed prior prob-ability by taking into account the distortions in thepriors.
In Section 5, we present multiple novel dis-tortion features based on syntactic parsing.
A newdistortion evaluation metric is proposed in Section6 and experimental results on a large-scale Chinese-English machine translation task are reported in Sec-tion 7.
Section 8 concludes the paper.612 Previous WorkSignificant amount of research has been conductedin the past on the word reordering problem in SMT.In (Brown et al, 1993) IBM Models 3 through 5model reordering based on the surface word infor-mation.
For example, Model 4 attempts to assigntarget-language positions to source-language wordsby modeling d(j|i,K,L) where j is the target-language position, i is the source-language position,K and L are respectively source and target sentencelengths.
These models are not effective in modelingreordering because they do not have enough contextand lack structural information.Phrase-based SMT systems such as (Koehn et al,2003) move from using words as translation unitsto using phrases.
One of the advantages of phrase-based SMT systems is that the local reordering is in-herent in the phrase translations.
However, phrase-based SMT systems capture reordering instancesand not reordering phenomena.
It has trouble to pro-duce the right translation order if the training datadoes not contain the specific phrase pairs.
For ex-ample, phrases do not capture the phenomenon thatArabic adjectives and nouns need to be reordered.Instead of directly modeling the distance of wordmovement, some phrase-level reordering models in-dicate how to move phrases, also called orientations.Orientations typically apply to the adjacent phrases.Two adjacent phrases can be either placed mono-tonically (sometimes called straight) or swapped(non-monotonically or inverted).
In (Och and Ney,2004; Tillmann, 2004; Kumar and Byrne, 2005; Al-Onaizan and Papineni, 2006; Xiong et al, 2006;Zens and Ney, 2006; Ni et al, 2009), people pre-sented models that use lexical features from thephrases to predict their orientations.
These modelsare very powerful in predicting local phrase place-ments.
In (Galley and Manning, 2008) a hierar-chical orientation model is introduced that capturessome non-local phrase reordering by a shift reducealgorithm.
Because of the heavy use of lexical fea-tures, these models tend to suffer from data sparse-ness problems.Syntax information has been used for reordering,such as in (Xia and McCord, 2004; Collins et al,2005; Wang et al, 2007; Li et al, 2007; Chang etal., 2009).
More recently, in (Ge, 2010) a proba-bilistic reordering model is presented to model di-rectly the source translation sequence and explicitlyassign probabilities to the reordering of the sourceinput with no restrictions on gap, length or adja-cency.
The reordering model is used to generate a re-ordering lattice which encodes many reordering andtheir costs (negative log probability).
Another recentwork is (Green et al, 2010), which estimates futurelinear distortion cost and presents a discriminativedistortion model that predicts word movement dur-ing translation based on multiple features.This work differentiates itself from all the previ-ous work on the phrase reordering as the following.Firstly, we propose a smoothed distortion prior prob-ability in the maximum-entropy-based MT frame-work.
It not only takes into account the distortionin the prior, but also alleviates the data sparsenessproblem.
Secondly, we propose multiple syntacticfeatures based on the source-side parse tree to cap-ture the reordering phenomena between two differ-ent languages.
The correct reordering patterns willbe automatically favored during the decoding, due tothe higher weights obtained through the maximumentropy training on the parallel data.
Finally, wealso introduce a new metric to quantify the effect onthe distortions in different systems.
The experimentson a Chinese-English MT task show that these pro-posed approaches additively improve both the dis-tortion and translation performance significantly.3 Maximum-Entropy Model for MTIn this section we give a brief review of a specialmaximum-entropy (ME) model as introduced in (It-tycheriah and Roukos, 2007).
The model has thefollowing form,p(t, j|s) = p0(t, j|s)Zexp?i?i?i(t, j, s), (3)where s is a source phrase, and t is a target phrase.j is the jump distance from the previously translatedsource word to the current source word.
Duringtraining j can vary widely due to automatic wordalignment in the parallel corpus.
To limit the sparse-ness created by long jumps, j is capped to a win-dow of source words (-5 to 5 words) around the lasttranslated source word.
Jumps outside the windoware treated as being to the edge of the window.
In62Eq.
(3), p0 is a prior distribution, Z is a normalizingterm, and ?i(t, j, s) are the features of the model,each being a binary question asked about the sourceand target streams.
The feature weights ?i can beestimated with the Improved Iterative Scaling (IIS)algorithm.Several categories of features have been pro-posed:?
Lexical features that examine source word, tar-get word and jump;?
Lexical context features that examine the pre-vious and next source words, and also the pre-vious two target words;?
Segmentation features based on morphologicalanalysis;?
Part-of-speech (POS) features that collect thesyntactic information from the source and tar-get words;?
Coverage features that examine the coveragestatus of the source words to the left and to theright.
They fire only if the left source is open(untranslated) or the right source is closed.<=-5          -4             -3             -2            -1            1              2             3              4           >=5jumpFigure 1: Counts of jumps for words with POS NN.4 Distortion PriorsGenerally the prior distribution in Eq.
(3) can con-tain any information we know about the future.<=-5          -4             -3             -2            -1            1              2             3              4           >=5jumpFigure 2: Counts of jumps for words with POS NT.In (Ittycheriah and Roukos, 2007), the normalizedphrase count is utilized as the prior, i.e.p0(t, j|s) ?1lp0(t|s) =C(s, t)l ?
C(s) (4)where l is the jump window size (a constant), C(s, t)is the co-ocurrence count of phrase pair (s, t), andC(s) is the source phrase count of s. It can be seenthat distortion j is not taken into account in Eq.
(4).The contribution of distortion solely comes from thefeatures.
In this work, we estimate the prior proba-bility with distortion included,p0(t, j|s) = p0(t|s)p(j|s, t) (5)where p(j|s, t) is the distortion probability for agiven phrase pair (s, t).Due to the sparseness issue in the estimation ofp(j|s, t), we choose to smooth it with the global dis-tortion probability throughp(j|s, t) = ?pl(j|s, t) + (1 ?
?
)pg(j), (6)where pl is the local distortion probability estimatedbased on the counts of jumps for each phrase pairin the training, pg is the global distortion probabilityestimated on all the training data, and ?
is the inter-polation weight.
In this work, pg is estimated basedon either source POS (if it?s a single-word sourcephrase) or source phrase size (if it?s more than oneword long), as shown below.pg(j) ={Pg(j|POS), if |s| = 1Pg(j||s|), if |s| > 1(7)63In this way, the system can differentiate the distor-tion distributions for single source words with differ-ent POS tags, such as adjectives versus nouns.
Andin the meantime, we also differentiate the distortiondistribution with different source phrase lengths.
Weshow several examples of the jump distributions inFig.
1 and 2 collected from 1M sentence pairs ina Chinese-to-English parallel corpus with automaticparsing and word alignment.
Fig.
1 shows the counthistogram for single-word phrases with POS tag asNN.
The distortion with j = 1, i.e.
monotone, domi-nates the distribution with the highest count.
The re-ordering with j = ?1 has the second highest count.Such pattern is shared by most of the other POS tags.However, Fig.
2 shows that the distribution of jumpsfor NT is quite different from NN.
The jump withj = ?1 is actually the most dominant, with highercounts than monotone translation.
This is due to thedifferent order in English when translating Chinesetemporal nouns.5 Distortion FeaturesAlthough the maximum entropy translation modelhas an explicit indicator of distortion, j, built intothe features, we discuss in this section some novelfeatures that try to capture the distortion phenomenaof translation.
These features are questions about theparse tree of the source language and in particularabout the local parse node neighborhood of the cur-rent source word being translated.
Figure 3 shows anexample sentence from the Chinese-English ParallelTreebank (LDC2009E83) and the source languageparse is displayed on the left.
The features belowcan be viewed as either being within a parse nodeor asking about the coverage status of neighborhoodnodes.Since these features are asking about the currentcoverage, they are specific to a path in the search lat-tice during the decoding phase of translation.
Train-ing these features is done by evaluating on the pathdefined by the automatic word alignment of the par-allel corpus sentence.5.1 Parse Tree ModificationsThe ?de?
construction in Chinese is by now famous.In order to ask more coherent questions about theparse neighborhood, we modify the parse structuresto ?raise?
the ?de?
structure.
The parse trees anno-tated by the LDC have a structure as shown in Fig.4.
After raising the ?de?
structure we obtain the treein Fig.
5.NP-OBJCPIP...DECdeQP...NPNNFigure 4: Original parse tree from LDC.DNPCPIP...DECdeQP...NPNNFigure 5: The parse tree after transformation.The transformation has been applied to the exam-ple shown in Figure 3.
The resulting flat structurefacilitates the parse sibling feature discussed below.5.2 Parse Coverage FeatureThe first set of new features we will introduce is thesource parse coverage feature.
This feature is in-terior to a source parse node and asks if the leavesunder this parse node are covered (translated) or notso far.
The feature has the following components:?i(SourceWord, TargetWord, SourceParseParent,jump, Coverage).Unary parents in the source parse tree are ex-cluded since the feature has no ambiguity in cover-age.
In Figure 3, the ?PP?
node above position 5 hastwo children, P, NP.
When translating source posi-tion 6, this feature indicates that the PP node has aleaf that is already covered.5.3 Parse Sibling FeatureThe second set of new features is the source parsesibling feature.
This feature asks whether the neigh-64Figure 3: Chinese-English example.boring parse node has been covered or not.
The fea-ture includes two types:?i(SourceWord, TargetWord, SourceParseSibling,jump, SiblingCoverage, SiblingOrientation)and?i(SourcePOS, TargetPOS, SourceParseSibling,jump, SiblingCoverage, SiblingOrientation).Some example features for the first type areshown in Table 1, where ?i = e?i .
The coveragestatus (Cov) of the parse sibling node indicates if thenode is covered completely (1), partially (2) or notcovered (0).
In order to capture the relationship ofthe neighborhood node, we indicate the orientationwhich can be either of {left (-1), right (1)}.
Giventhe example shown in Figure 3, at source position10, the system can now ask about the ?CP?
structureto the left and the ?QP?
and ?NP?
structures to theright.
An ?i of greater than 1.0 (meaning ?i > 0)indicates that the feature increases the probability ofthe related target block.
From these examples, it?sclear that the system prefers to produce an emptytranslation for the Chinese word ?de?
when the ?QP?and ?NP?
nodes to the right of it are already covered(the first two features in Table 1) and when the ?CP?node to left is still uncovered (the third feature).
Thelast feature in the table shows ?i for the case when?CP?
has already been covered.These features are able to capture neighborhoodsthat are much larger than the original baseline modelwhich only asked questions about the immediatelexical neighborhood of the current source word.Cnt ?i Tgt Src Parse Cov Orien-Node tation18065 2.06 e0 de QP 1 1366153 1.99 e0 de NP 1 1143433 3.41 e0 de CP 0 -199297 1.05 e0 de CP 1 -1Table 1: Parse Sibling Word Features (e0 representsempty target).6 A New Distortion Evaluation MetricMT performance is usually measured by such met-ric as BLEU which measures the MT output as awhole including word choice and reordering.
It isuseful to measure these components separately.
Un-igram BLEU (BLEUn1) measures the precision ofword choice.
We need a metric for measuring re-ordering accuracy.
The naive way of counting accu-racy at every source position does not account for thecase of the phrasal movement.
If a phrase is movedto the wrong place, every source word in the phrasewould be penalized whereas a more reasonable met-ric would penalize the phrase movement only onceif the phrase boundary is correct.We propose the following pair-wise distortionmetric.
From an MT output, we first extract thesource visit sequence:Hyp:{h1,h2, .
.
.
hn}where hi are the visit order of the source sentence.From the reference, we extract the true visit se-quence:65Ref:{r1,r2, .
.
.
rn}The Pair-wise Distortion metric PDscore can becomputed as follows:PDscore(?
?H ) =n?i=1I(hi = rj ?
hi?1 = rj?1)n(8)It measures how often the translation output getsthe pair-wise source visit order correct.
We noticethat an MT metric named LRscore was proposed in(Birch and Osborne, 2010).
It computes the distancebetween two word order sequences, which is differ-ent from the metric we proposed here.7 Experiments7.1 Data and BaselineWe conduct a set of experiments on a Chinese-to-English MT task.
The training data includes the UNparallel corpus and LDC-released parallel corpora,with about 11M sentence pairs, 320M words in to-tal (counted at the English side).
To evaluate thesmoothed distortion priors and different features, weuse an internal data set as the development set andthe NIST MT08 evaluation set as the test set, whichincludes 76 documents (691 sentences) in newswireand 33 documents (666 sentences) in weblog, bothwith 4 sets of references for each sentence.
Insteadof using all the training data, we sample the trainingcorpus based on the dev/test set to train the systemmore efficiently.
The most recent and good-qualitycorpora are sampled first.
For the given test set, weobtain the first 20 instances of n-grams (length from1 to 15) from the test that occur in the training uni-verse and the resulting sentences then form the train-ing sample.
In the end, 1M sentence pairs are se-lected for the sampled training for each genre of theMT08 test set.A 5-gram language model is trained from the En-glish Gigaword corpus and the English portion ofthe parallel corpus used in the translation modeltraining.
The Chinese parse trees are producedby a maximum entropy based parser (Ratnaparkhi,1997).
The baseline decoder is a phrase-based de-coder that employs both normal phrases and alsonon-contiguous phrases.
The value of maximumskip is set to 9 in all the experiments.
The smoothingparameter ?
for distortion prior is set to 0.9 empiri-cally based on the results on the development set.7.2 Distortion EvaluationWe evaluate the MT distortion using the metric inEq.
(8) on two hand-aligned test sets.
Test-278 in-cludes 278 held-out sentences.
Test-52 contains thefirst 52 sentences from the MT08 Newswire set, withthe Chinese input sentences manually aligned to thefirst set of reference translations.
From the handalignment, we extract the true source visit sequenceand this is the reference.The evaluation results are in Table 2.
It is shownthat the smoothed distortion prior, parse coveragefeature and parse sibling feature each provides im-provement on the PDscore on Test-278 and Test-52.The final system scores are 2 to 3 points absolutehigher than the baseline scores.
The state visit se-quence in the final system is closer to the true visitsequence than that of the baseline.
This indicatesthe advantage of using both parse-based syntacticfeatures and also the smoothed prior that takes intoaccount of the distortion effect.
We also providean upper-bound in the last row by computing thePDscore between the first and second set of refer-ences for Test-52.
The number shows the agreementbetween two human translators in terms of PDscoreis around 71%.System Test-278 Test-52ME Baseline 44.58 48.96+Prior 45.12 49.22+COV 45.00 49.03+SIB 45.43 49.20+COV+SIB 46.16 49.45+Prior+COV+SIB 47.68 51.04Ref1 vs. Ref2 - 70.99Table 2: Distortion accuracy PDscore (Prior:smootheddistortion prior; COV:parse coverage feature; SIB:parsesibling feature).7.3 Translation ResultsTranslation results on the MT08 Newswire set andMT08 Weblog set are listed in Table 3 and Table 4respectively.
The MT performance is measured withthe widely adopted BLEU and TER (Snover et al,2006) metrics.
We also compare the results fromdifferent configurations with a normal phrase-based66System Number of Features BLEU TERPBT n/a 29.71 59.40ME 9,008,382 32.12 56.78+Prior 9,008,382 32.46 56.41+COV 9,202,431 32.48 56.50+SIB 10,088,487 32.73 56.26+COV+SIB 10,282,536 32.94 55.97+Prior+COV+SIB 10,282,536 33.15 55.62Table 3: MT results on MT08 Newswire set (PBT:normal phrase-based MT; ME:Maximum-entropy baseline;Prior:smoothed distortion prior; COV:parse coverage feature; SIB:parse sibling feature).System Number of Features BLEU TERPBT n/a 20.07 62.90ME 9,192,617 22.42 60.36+Prior 9,192,617 22.70 60.11+COV 9,306,967 22.69 60.14+SIB 9,847,445 22.91 59.92+COV+SIB 9,961,795 23.04 59.78+Prior+COV+SIB 9,961,795 23.25 59.56Table 4: MT results on MT08 Weblog set (PBT:normal phrase-based MT; ME:Maximum-entropy baseline;Prior:smoothed distortion prior; COV:parse coverage feature; SIB:parse sibling feature).SMT system (Koehn et al, 2003) that is trained onthe same training data.
The number of features usedin the systems are listed in the tables.We start from the maximum-entropy baseline, asystem implemented similarly as in (Ittycheriahand Roukos, 2007).
It utilizes multiple features aslisted in Section 3, including lexical reordering fea-tures, and produces an already significantly betterperformance than the normal phrase-based MT sys-tem (PBT).
It is around 2.5 points better in bothBLEU and TER than the PBT baseline.
By addingsmoothed priors, parse coverage features or parsesibling features each separately, the MT perfor-mance is improved by 0.3 to 0.6.
The parse siblingfeature alone provides the largest individual contri-bution.
When adding both types of new features,the improvement is around 0.6 to 0.8 on two gen-res.
Finally, applying all three results in the bestperformance (the last row).
On the Newswire set,the final system is more than 3 points better than thePBT baseline and 1 point better than the ME base-line.
On the Weblog set, it is more than 3 pointsbetter than PBT and 0.8 better than the ME baseline.All the MT results above are statistically significantwith p-value < 0.0001 by using the tool described in(Zhang and Vogel, 2004).7.4 AnalysisTo better understand the distortion and translationresults, we take a closer look at the parse-based fea-tures.
In Table 5, we list the most frequent parse sib-ling features that are related to the Chinese phraseswith ?PP VV?
structures.
It is known that in Chi-nese usually the preposition phrases (?PP?)
are writ-ten/spoken before the verbs (?VV?
), with a differentorder from English.
Table 5 shows how such re-ordering phenomenon is captured by the parse sib-ling features.
Recall that when ?i is greater than 1,the system prefers the reordering with that featurefired.
When ?i is smaller than 1, the system willpenalize the corresponding translation order duringthe decoding search.
When the coverage is equal to1, it means ?PP?
has been translated before translat-ing current ?VV?.
As shown in the table, those fea-tures with coverage equal to 1 have ?i lower than 1,which will result in penalties on incorrect translationorders.In Fig.
6, we show the comparison between the67Count ?i j TgtPOS SrcPOS ParseSib Cov Orien-Node tation3052 1.10 5 VBD VV PP 0 -12662 1.10 -1 VBD VV PP 0 -12134 1.25 4 VBD VV PP 0 -150 0.73 5 VBD VV PP 1 -139 0.84 -5 VBD VV PP 1 -118 0.95 -2 VBD VV PP 1 -1Table 5: Parse Sibling Word Features related to Chinese ?PP VV?.Src1  , 1850  2005 , 1800    (were) (at) (annual) 3%  ff(rate) fifl(shrinking)ffiRef a long-term follow-up research by glacier experts at the swiss academy ofsciences found that from 1850 to 2005 the 1,800 plus glaciers in switzerlandwere shrinking at an annual rate of 3 % .Baseline the swiss academy of sciences glacier experts long-term follow-up study foundthat from 2005 to 1850 , with an average of more than 1800 glaciers inswitzerland is the reduced rate of 3 % .New the swiss academy of sciences glacier experts long-term follow-up study foundthat from 1850 to 2005 , more than 1800 of swiss glaciers shrinking at anannual rate of 3 %.Src2!
"#$%&'() , * + , -.
(had been) /0(kidnapped)(who) 12(german) 34(hostage) 56 78 9: , ;< => ?
@ ABCDEffiRef but at the same time the taliban said that another german hostage who hadbeen kidnapped was in extremely poor health , and had started to becomecomatose and to lose consciousness .Baseline but at the same time , another one was kidnapped by the taliban of thegerman hostage body very weak , began to fall into a coma and lostconsciousness .New but at the same time , the taliban said that the body of another germanhostage who was kidnapped very weak , began to fall into a coma and lostconsciousness .Figure 6: Chinese-English MT examples(Baseline:Maximum-entropy baseline; New:System with smoothed priorsand syntactic features).ME baseline output and those from the improvedsystem with the parse-based features and smootheddistortion priors.
The differences are highlightedin bold for easy understanding.
The first exampleshows that the new system fixes the order for ?PPVV?, while the second one shows the fix for thetranslation of ?CP de NP?.
This is consistent withthe features we showed in Table 1 and 5.
The newfeatures help to translate the Chinese text in the rightorder.8 ConclusionIn this paper we have presented several novel ap-proaches that improved phrase reordering in theframework of maximum entropy based translation.A smoothed prior probability was proposed to takeinto account the distortions in the priors.
Severalnovel distortion features were presented based onthe syntactic parsing.
A new metric PDscore wasalso introduced to measure the effect of distortionin the translation hypotheses.
We showed that bothsmoothed prior and syntax-based features additivelyimproved the distortion and also the translation per-formance significantly on a large-scale Chinese-English machine translation task.
How to furthertake advantage of the syntactic information to im-prove the reordering in SMT will continue to be aninteresting topic in the future.AcknowledgmentsWe would like to acknowledge the support ofDARPA under Grant HR0011-08-C-0110 for fund-68ing part of this work.
The views, opinions, and/orfindings contained in this article/presentation arethose of the author/presenter and should not be in-terpreted as representing the official views or poli-cies, either expressed or implied, of the Defense Ad-vanced Research Projects Agency or the Departmentof Defense.ReferencesYaser Al-Onaizan and Kishore Papineni.
2006.
Distor-tion models for statistical machine translation.
In Pro-ceedings of the 21st International Conference on Com-putational Linguistics and 44th Annual Meeting of theACL, pages 529?536, Sydney, Australia.Alexandra Birch and Miles Osborne.
2010.
Lrscore forevaluating lexical and reordering quality in mt.
In Pro-ceedings of the Joint 5th Workshop on Statistical Ma-chine Translation and MetricsMATR.Peter F. Brown, Vincent J. Della Pietra, Stephen A. DellaPietra, and Robert L. Mercer.
1993.
The mathemat-ics of statistical machine translation: parameter esti-mation.
Computational Linguistics, 19(2):263?311.Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, andChristopher D. Manning.
2009.
Discriminative re-ordering with chinese grammatical relations features.In Proceedings of the Third Workshop on Syntax andStructure in Statistical Translation.Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.2005.
Clause restructuring for statistical machinetranslation.
In Proceedings of ACL, pages 531?540.Michel Galley and Christoph D. Manning.
2008.
A sim-ple and effective hierarchical phrase reordering model.In Proceedings of the EMNLP.Niyu Ge.
2010.
A direct syntax-driven reordering modelfor phrase-based machine translation.
In Proceedingsof HLT-NAACL, pages 849?857.Spence Green, Michel Galley, and Christopher D. Man-ning.
2010.
Improved models of distortion cost forstatistical machine translation.
In Proceedings of HLT-NAACL.Abraham Ittycheriah and Salim Roukos.
2007.
Di-rect translation model 2.
In Proceedings HLT/NAACL,pages 57?64, April.Philipp Koehn, Franz Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In Proceedings ofNAACL/HLT.Shankar Kumar and William Byrne.
2005.
Local phrasereordering models for statistical machine translation.In Proceedings of HLT/EMNLP, pages 161?168.Chi-Ho Li, Dongdong Zhang, Mu Li, Ming Zhou,Minghui Li, and Yi Guan.
2007.
A probabilistic ap-proach to syntax-based reordering for statistical ma-chine translation.
In Proceedings of ACL.Yizhao Ni, Craig J.Saunders, Sandor Szedmak, and Mah-esan Niranjan.
2009.
Handling phrase reorderings formachine translation.
In Proceedings of ACL.Franz-Josef Och and Hermann Ney.
2002.
Discrimina-tive training and maximum entropy models for statis-tical machine translations.
In 40th Annual Meeting ofthe ACL, pages 295?302, Philadelphia, PA, July.Franz Josef Och and Hermann Ney.
2004.
The align-ment template approach to statistical machine transla-tion.
Computational Linguistics, 30(4):417?449.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-jing Zhu.
2002.
Bleu: a method for automatic evalu-ation of machine translation.
In Proceedings of ACL,pages 311?318.Adwait Ratnaparkhi.
1997.
A linear observed time sta-tistical parser based on maximum entropy models.
InProceedings of EMNLP, pages 1?10.Matthew Snover, Bonnie Dorr, Richard Schwartz, LineaMicciulla, and John Makhoul.
2006.
A study of trans-lation edit rate with targeted human annotation.
InProceedings of Association for Machine Translation inthe Americas.Christoph Tillmann.
2004.
A unigram orientation modelfor statistical machine translation.
In Proceedings ofHLT-NAACL.Chao Wang, Michael Collins, and Philipp Koehn.
2007.Chinese syntactic reordering for statistical machinetranslation.
In Proceedings of EMNLP, pages 737?745.Fei Xia and Michael McCord.
2004.
Improving a sta-tistical mt system with automatically learned rewritepatterns.
In Proceedings of COLING.Deyi Xiong, Qun Liu, and Shouxun Lin.
2006.
Maxi-mum entropy based phrase reordering model for sta-tistical machine translation.
In Proceedings of ACL.Richard Zens and Hermann Ney.
2006.
Discriminativereordering models for statistical machine translation.In Proceedings of the Workshop on Statistical MachineTranslation.Ying Zhang and Stephan Vogel.
2004.
Measuring con-fidence intervals for the machine translation evalua-tion metrics.
In Proceedings of The 10th InternationalConference on Theoretical and Methodological Issuesin Machine Translation.69
