Pronominalization in Generated Discourse and DialogueCharles B. CallawayIstituto per la Ricerca Scientifica eTecnologica (ITC-irst), Italycallaway@irst.itc.itJames C. LesterDepartment of Computer ScienceNorth Carolina State University, USAlester@adm.csc.ncsu.eduAbstractPrevious approaches to pronominalizationhave largely been theoretical rather thanapplied in nature.
Frequently, such meth-ods are based on Centering Theory, whichdeals with the resolution of anaphoric pro-nouns.
But it is not clear that complex the-oretical mechanisms, while having satis-fying explanatory power, are necessary forthe actual generation of pronouns.
We firstillustrate examples of pronouns from vari-ous domains, describe a simple method forgenerating pronouns in an implementedmulti-page generation system, and presentan evaluation of its performance.1 IntroductionPronominalization is an important element in the au-tomatic creation of multi-paragraph and multi-pagetexts using natural language generation (NLG).
Au-thors routinely use pronouns in texts spanning alltypes of genres, such as newspaper copy, sciencefiction and even academic papers.
Indeed, withoutpronouns, texts quickly become confusing as readersbegin to pay more attention to the writing style thanto the content that makes the text informative or en-joyable (Callaway and Lester, 2001a).
Even worse,incorrect pronouns can lead readers to misinterpretthe text or draw unsound inferences.Furthermore, current pronominalization strategiesare ill-equipped to deal with the wide variety ofreasons that pronouns are used in naturally occur-ring texts.
Almost without exception, they focus onanaphoric pronouns as described in Focus/CenteringTheory (Webber, 1979; Sidner, 1983; Grosz and Sid-ner, 1986; Walker, 1998), ignoring the multitude ofother possible types.
However, it is certainly truethat authors make use of pronouns which are not mo-tivated by anaphoric reference.In addition, because such approaches are orientedtowards anaphora resolution during parsing, they ig-nore structures such as the discourse plan which arepresent during generation but not parsing.
A typi-cal discourse plan can include vital information forpronominalization such as time and clause bound-aries, ordering of propositions, and semantic de-tails verbal arguments.
Current approaches basedon Centering algorithms thus attempt to recreate atext coherence structure that duplicates work alreadydone by the discourse planner.Finally, there are significant obstacles to verifyingthe correctness of existing pronominalization algo-rithms for any pronominalization theory (Not, 1996;Yeh and Mellish, 1997; McCoy and Strube, 1999;Henschel et al, 2000; Kibble and Power, 2000): thelack of natural language generation systems that canproduce large enough texts to bring discourse-levelprocesses into play.
Because of this, researchers areforced to simulate by hand how their algorithms willwork on a given text.
It is also not sufficient to usetemplate generation systems to perform this task be-cause they lack the low-level discourse representa-tion needed to provide the information upon whichmost algorithms base their decisions.In this paper we first summarize related workin both anaphora resolution and anaphora genera-tion.
We next describe the range of pronoun typesComputational Linguistics (ACL), Philadelphia, July 2002, pp.
88-95.Proceedings of the 40th Annual Meeting of the Association forthat we found in a wide variety of texts.
We pro-ceed to describe an algorithm for determining ap-propriate pronominalizations that uses existing NLGstructures and simple numeric techniques.
We alsobriefly describe an implemented generation systemthat contains enough low-level discourse informa-tion to motivate pronominalization decisions usingthis method.
Finally, we quantitatively demonstratethe performance of this simple numerical approachin both a newspaper and fictional narrative domain.2 Background and Related WorkBecause most NLG systems have focused on lin-guistic phenomena at the paragraph level and be-low, there has been intensive investigation into thecore areas of generation that are required to pro-duce them: discourse planning, sentence planningand surface realization.
Since pronouns are morelikely to be a multiparagraph, discourse-level phe-nomenon, it has been possible to ignore their inclu-sion into working NLG systems which are not calledupon to generate lengthy passages.Indeed, most work on pronouns in computationallinguistics has come under the heading of anaphoraresolution as an element of parsing rather than theheading of pronominalization as an element of gen-eration.
Since discourse anaphora resolution wasfirst studied theoretically (Grosz, 1977; Webber,1979; Sidner, 1983; Grosz and Sidner, 1986), it hascome to be dominated by Centering Theory (Groszet al, 1995; Di Eugenio, 1998; Walker, 1998) whichproposes rules for the determination of focus andsalience within a given segment of discourse.
Rel-atively little work has been done on alternate ap-proaches to pronoun resolution (Hobbs, 1976; Bald-win, 1995).While many NLG researchers have attempted totransfer the ideas of Centering Theory to genera-tion (Not, 1996; Yeh and Mellish, 1997; McCoyand Strube, 1999; Henschel et al, 2000; Kibbleand Power, 2000), there has yet been no substan-tial return contribution to the field of anaphora res-olution.
There are two principal reasons for this.First, it is extremely difficult to create an NLG sys-tem that generates the large quantity of texts neededto exhibit discourse-level phenomena while consis-tently employing the deep linguistic representationsneeded to determine appropriate pronominal forms.Second, Centering Theory is still vague on the ex-act definition of terms such as ?segment?
(Poesio etal., 1999a), making it difficult to create a mutuallyagreeable implementation.An additional area of NLG research that dealswith pronouns is that of referring expression gen-eration (Appelt, 1985; Heeman and Hirst, 1986;Claassen, 1992; Dale, 1992), which attempts to findthe optimal noun phrase (whether full description,definite description, deixis, pronoun, or reducednoun phrase) to enable a reader to mentally select theintended referent from the set of possible referents(Reiter and Dale, 1997).
Comparatively, referringexpression generation is a process for local disam-biguation and is not generally concerned with singlephenomena spanning multiple paragraphs.
Becauseof this, and because the domains and genres we havestudied typically do not involve sets of very simi-lar referents, we concentrate on discourse-motivatedsources of pronominalization.3 Examples of PronominalizationPronominalization is the appropriate determination,marking and grammatical agreement of pronouns(he, she, their, herself, it, mine, those, each other,one, etc.)
as a short-hand reference to an entity orevent mentioned in the discourse.
As with anaphoraresolution, the task of a pronominalization algorithmis to correctly predict which pronoun a person wouldprefer in the same situation.
The range of possibili-ties includes leaving the noun phrase as it is, reduc-ing it by removing some of its modifiers, or replac-ing it with a pronoun construction.Our corpora analyses have identified a number ofmotivations for converting nouns into pronouns:1.
Anaphoric pronouns: These are the most-studied cases of pronoun occurrences, whichsequentially follow a specific entity known asthe referent.
Anaphors are divided into twoclasses, short-distance (within the same sen-tence) and long-distance (previous sentences).But Johnihad never been to New Orleans,and heicouldn?t remember if anyone in hisifamily had either.2.
Cataphoric pronouns: According to Quirk etal.
(1985), cataphors are those pronouns whichoccur before their referents in the linear flow oftext within the same sentence, where the pro-noun is either at a lower structural level or ispart of a fronted circumstantial clause or prepo-sitional phrase which could have appeared afterthe reference.
Additionally, this category couldinclude clefting pronouns.Before heijoined the navy, Geraldimadepeace with his family.3.
Pronouns Lacking Textual Antecedents: Thiscategory includes document deixis (via ademonstrative pronoun), authorial or readerreference, and situational pronouns.This is the first document to show .
.
.We discuss these strategies in the next section.The group had never seen anything like it.4.
Reflexive and Reciprocal Pronouns: Mostverbs use special pronouns when the subjectand object corefer.
A discourse history algo-rithm can employ that knowledge to mark re-flexive and reciprocal pronouns appropriately.Kittensioften watch themselvesiin mirrors.Baby lionsjtackle each otherjwhen playing.5.
Partitive pronouns: It is important to know con-ceptually what it is that the pronoun is tryingto replace.
Otherwise, it becomes impossibleto achieve the types of pronominalizations thatauthors are routinely capable of creating.
Thisrequires accurate information in the knowledgebase or linguistic structure from which the sen-tences are derived.As the horses ran by, she roped one.
* As the horses ran by, she roped it.
* As the horses ran by, she roped them.In addition to these motivations, we identifiedseveral factors that prevent pronouns from occurringwhere they otherwise might:6.
Pronouns across boundaries: After a chapter,section or other obvious boundary, such as achange in time, place, or both, as in (McCoyand Strube, 1999), authors will typically ?re-set?
pronominalization just as if it were thebeginning of the entire text.
Antecedent ref-erences that break these boundaries are some-times marked by the authors in academic texts:As we saw in the previous section, .
.
.7.
Restrictions from modifiers: Because pronounscannot have modifiers like nouns, adding an ad-jective, relative clause, or some other modifierprevents a noun from being replaced by a pro-noun.
For instance:The mayor had already read the full proposal.
* The mayor had already read the full it.8.
Focused nouns: Especially after a vocallystressed discourse marker (Wolters and Byron,2000) or some other marked shift in topic, aword that normally would be pronominalizedis often not, as in this example:.
.
.
and you frequently find that mice occupyan important part of the modern medical labo-ratory.
In other words, mice are especially nec-essary for diagnosing human cancers .
.
.9.
Semantic and syntactic considerations: Asmall number of semantic relations and syntac-tic constructions prohibit pronominalization:* The stranger was just called him.
(Bob)* Roberta was no longer a her.
(child)* The father, a tyrant of a him, .
.
.
(man)10.
Optional pronominalization: Often there areborderline cases where some authors will usepronouns while others won?t.
A single algo-rithm may be tuned to match a particular au-thor?s style, but parameterization will be nec-essary to match a variety of styles.
Thus it isextremely difficult to exactly match any partic-ular text without having the ability to adjust thepronominalization algorithm.Pronominalization occurs equally as often in ex-position as in dialogue, but dialogue can haveslightly different pronominalizations depending onthe relationship between the utterer and the hearer:11.
Speaker self-reference:?John thinks John will go find John?s shoes,?John said.changes to first person singular pronouns:?I think I will go find my shoes,?
John said.12.
Speaker references hearer(s):?Mary should go find Mary?s shoes,?
Johnsaid.changes to second person pronouns:?You should go find your shoes,?
John said.13.
Reference to speaker and hearer (or to speakerand a third party):?John and Mary should go find John andMary?s shoes,?
John said.changes to first person plural pronouns:?We should go find our shoes,?
John said.14.
Reference to a third party:?Bob and Mary went to eat Bob and Mary?sbreakfast,?
John said.changes to third person plural pronouns:?They went to eat their breakfast,?
John said.15.
Finally, the treatment of pronouns differs de-pending if they are inside or outside of the di-rect quotation.
For example:?Oh man, I forgot to eat my breakfast!?
Johnmuttered to himself while grabbing his shoes.Although this enumeration is surely incomplete,it provides a basic description of the types of phe-nomena that must be handled by a generation systemin order to produce text with the types of pronounsfound in routine human-produced prose.4 Architectural ConcernsIn order to correctly account for these phenomenaduring generation, it is necessary to have detailedinformation about the underlying discourse struc-ture.
Although a template generation system couldbe augmented to record this information, in practiceonly deep structure, full-scale NLG systems have therequisite flexibility.
Because a pronominalization al-gorithm typically follows the discourse planner, itfrequently has access to the full discourse plan.A typical discourse plan is a tree structure, whereinternal nodes represent structuring relations whileleaf nodes represent individual sentential elementsthat are organized semantically.
In addition, the ele-ments of the discourse tree are typically rooted in thesemantic knowledge base which the discourse plan-ner drew from when constructing the discourse plan.The discourse plan supplies the following informa-tion that is useful for pronominalization: Linearization: The sequencing informationstored in the discourse tree can be used to mo-tivate anaphoric and cataphoric pronouns asshown in items 1 & 2 of Section 3. Semantic Structure: The original subgraphs(or semantic subnetworks) derived from theknowledge base can motivate content vs. sit-uational knowledge (item 3) reflexive and re-ciprocal pronouns via argument lists (item 4),partitive pronouns (item 5), and the existenceof NP modifiers (item 7), and can identify se-mantic types in relations (item 9). Discourse Structure: The rhetorical relationsthat hold between different sentences typicallyimply where section boundaries are located(item 6), indicate what types of discourse mark-ers are employed (item 8), and in the case ofdialogue, know which actors are speaking, lis-tening, or not present (items 11-15).This detailed knowledge of the discourse is avail-able to an implemented pronominalization compo-nent utilizing any theory, including Centering the-ory.
We thus now turn our attention to what role thisinformation plays in a pronominalization algorithm.5 A Simple Pronominalization AlgorithmAt an abstract level, the pronominalization algo-rithms derived from Centering theory are easily ex-pressed: if Centering theory predicts a pronounwould be used in anaphora resolution in a given seg-ment of text, then generate the appropriate pronoun.While this works for many cases of anaphoric pro-nouns [84.7% in (McCoy and Strube, 1999), 87-90% in (Henschel et al, 2000)], we have seen thatthese form only a subset of the potential reasons forpronominalization.
Furthermore, this approach as-sumes that the discourse tree was constructed withCentering theory in mind.Given:LNE, the linearized list of nominal elementsNE, the current nominal elementSEEN , the list of encountered nominal elementsD, the dialogue state of the current leaf nodeRS, the rhetorical structure near the leaf nodeSC, the sentence counterDo:SEEN ( ; SC ( 0while LNE 6=  doNE ( first(LNE)if NE 62 SEENthen reset-counters(NE),SEEN ( SEEN NEelse update-counters(NE)D ( updateDialogueState()RS ( updateLocalRhetoricalStructure()if (topic-shift _ time-shift) 2 RSthen SC ( SC + 10else if modifiers(NE;RS) =  ^(special-relation _ appositive) 62 RSif D == QuotedDialoguethen mark(quoted-pronoun(NE;RS))else if subject-matches-object(NE;RS)then mark(ReflexivePronoun)else if sent-distance(NE;SC) = 0then mark(MultipleInSentencePronoun)else if 3 <= sent-distance(NE;SC) < 1and nominal-distance(NE) < 3then mark(LongDistancePronoun),else if recency(NE) > 3then mark(ShortDistancePronoun),LNE ( remove-first(LNE); SC ( SC + 1Figure 1: The Pronominalization AlgorithmHowever, it is not clear that Centering theory itselfis necessary in generation, let alne its accompany-ing algorithms and data structures.
Because Cen-tering theory is typically applied to parsing (whichstarts with no discourse tree), it may not be the mostefficient technique to use in generation (which has acomplete discourse tree available for inference).Instead, we attempted to determine if the informa-tion already present in the discourse tree was enoughto motivate a simpler algorithm based on the follow-ing available data: Ordered sequence of nominal elements: Be-cause the discourse tree is linearized and in-dividual leaves of the tree annotate which ele-ments have certain semantic roles, a very goodguess can be made as to which nominal ele-ments precede others at the clause level. Known paragraph and sentence boundaries:Analysis of the rhetorical structure of the dis-course tree allows for the determination ofboundaries and thus the concept of metric dis-tance between elements. Rhetorical relations: The rhetorical relationscan tell us which nominal elements follow dis-course markers and which are used reflexivelyor reciprocally. Dialogue: By recording the participants in dia-logue, the discourse tree allows for the appro-priate assignment of pronouns both inside andoutside of the direct quote itself.The algorithm we developed considers the cur-rent discourse leaf node and the rhetorical structureabove it, and also makes use of the following data: Nominal element distance: How many total(non-distinct) nominal elements ago a particu-lar element was last used. Recency: How many distinct nominal elementshave been seen since its last use. Sentential distance: How many sentences (pro-totypical clauses) have appeared since the lastusage of this nominal element.The algorithm itself (Figure 1) is best character-ized as a counting method, that is, it loops oncethrough the linearized list of nominal elements andmakes pronominalization decisions based on the lo-cal information described above, and then updatesthose numerical counters.
Numerical parameters(e.g., recency(NE) > 3) are derived from empir-ical experimentation in generating multi-page prosein a narrative domain.While it lacks the explanatory power of a rela-tively mature linguistic theory, it also lacks the ac-companying complexity and is immediately appli-cable to real-world deep generation systems.
The al-gorithm is traced in Figure 2, although due to spacelimitations some phenomena such as dialogue, longdistance and reflexive pronouns are not shown.6 Implementation and EvaluationSTORYBOOK (Callaway and Lester, 2001b; Call-away and Lester, in press) is an implemented nar-rative generation system that converts a pre-existingSentences as seen by the reader (antecedents underlined, pronouns in bold):Now, it happened that a wolf1, a very cruel, greedy creature2also heard Little Red Riding Hood3asshe4passed, and he5longed to eat her6for his7breakfast8.
But he9knew Hugh10, the woodman11,was at work12very near with his13great dog14.Sentences as produced by the discourse planner before revision:S1: Now, it happened that a wolf1, a very cruel, greedy creature2also heard Little Red Riding Hood3as Little Red Riding Hood4passed.S2: The wolf5longed to eat Little Red Riding Hood6for the wolf?s7breakfast8.S3: But the wolf9knew Hugh10, the woodman11, was at work12very near with Hugh?s13great dog14.Each noun element is processed in the order linearized from the discourse plan:1.
The first mention of wolf1in the narrative resets its discourse history entry.2.
Creature2is the second mention of wolf, but it is in an appositive structure (see pronoun category #9).3.
LRRH3was mentioned just before in the prior paragraph, but ?Now,?
is a prosodic discourse marker(see pronoun category #8), thus modifiers(NE, RS) 6= .4.
For LRRH3and LRRH4, sentence-distance(NE, SC) = 0 resulting in a multiple-in-sentence-pronoun.5.
Sentence-distance(NE, SC) = 1, but recency(NE) = 2, resulting in a short-distance-pronoun.6.
Similarly, LRRH6is converted into a short-distance-pronoun.7.
As with element #4, this is a case resulting in a multiple-in-sentence-pronoun.9.
As with element #5, this is a case resulting in a short-distance-pronoun.10.
The first mention of Hugh10in the narrative resets its discourse history entry.11.
As with element #2, the discourse plan reports that this is an appositive.13.
Finally, Hugh13is repeated in the same sentence.Figure 2: A Brief Trace of the Pronominalization Algorithm for Anaphoric Pronouns from STORYBOOKnarrative (discourse) plan into a multi-page fic-tional narrative in the fairy tale domain.
Using apipelined generation architecture, STORYBOOK per-forms pronominalization before sentence planning,and includes a revision component that is sensitiveto pronominalization choices during clause aggre-gation.
A previous large-scale evaluation of STORY-BOOK (Callaway and Lester, 2001a) which includedboth a full version and a version with the pronomi-nalization component ablated showed that includingsuch a component significantly increases the qualityof the resulting prose.However, there are significant practical obstaclesto comparing the performance of different pronomi-nalization algorithms using corpus matching criteriainstead of ?quality?
as evaluated by human judges.Because systems that can handle a large quantity oftext are very recent and because it can require yearsto create and organize the necessary knowledge toproduce even one multi-paragraph text, much re-search on anaphora generation has instead relied onone of two techniques: Checking algorithms by hand: One verificationmethod is to manually examine a text, identify-ing candidates for pronominalization and simu-lating the rules of a particular theory.
However,this method is prone to human error. Checking algorithms semiautomatically: Otherresearchers opt instead to annotate a corpusfor pronominalization and their antecedents aswell as the pronoun forms that should occur,and then simulate a pronominalization algo-rithm on the marked-up text (Henschel et al,2000).
Similarly, this approach can suffer frominterannotator agreement errors (Poesio et al,1999b).To verify our pronominalization algorithm morerigorously, we instead used the STORYBOOK deepgeneration system to recreate pre-existing multi-page texts with automatically selected pronouns.McCoy & Strube Henschel et al STORYBOOK STORYBOOKNYT News NYT News NYT News LRRH NarrativeAnimate Anaphora 370/437 (84.7%) N/A 415/449 (92.4%) 170/174 (97.7%)All Anaphora N/A 469/527 (89.0%) 441/475 (92.8%) 177/181 (97.8%)Cataphora N/A N/A 1/2 (50.0%) 1/2 (50.0%)Dialogue N/A N/A 46/46 (100.0%) 65/65 (100.0%)Deixis N/A N/A 9/9 (100.0%) None presentReflex./Recip.
N/A N/A 5/6 (83.3%) 2/2 (100.0%)Partitive N/A N/A 1/2 (50.0%) 1/1 (100.0%)Table 1: Pronouns Correct by Algorithm/Text vs. Pronoun TypeWithout a full-scale implementation, it is impossibleto determine whether an algorithm performs imper-fectly due to human error, a lack of available corpusdata for making decisions, or if it is a fault with thealgorithm itself.Using the algorithm described in Figure 1, wemodified STORYBOOK to substitute the types ofpronouns described in Section 3.
We then createdthe discourse plan and lexicon necessary to generatethe same three articles from the New York Times as(McCoy and Strube, 1999).
The results for both thenewspaper texts and the Little Red Riding Hood nar-rative described in (Callaway and Lester, in press)are shown in Table 1.With the same three texts from the New YorkTimes, STORYBOOK performed better than the pre-vious reported results of 85-90% described in (Mc-Coy and Strube, 1999; Henschel et al, 2000) on bothanimate and all anaphora using a corpus matchingtechnique.
Furthermore, this was obtained solely byadjusting the recency parameter to 4 (it was 3 in ournarrative domain), and without considering other en-hancements such as gender/number constraints ordomain-specific alterations.17 ConclusionsPronominalization is an important element in the au-tomatic creation of multi-paragraph and multi-page1It is important to note, however, that our counts of pronounsand antecedents do not match theirs.
This may stem from a vari-ety of factors, such as including single instances of nominal de-scriptions, whether dialogue pronouns were considered, and ifborderline quantifiers and words like ?everyone?
were counted.The generation community to-date has not settled on standard,marked corpora for comparison purposes as has the rest of thecomputational linguistics community.texts.
Previous approaches, based largely on theo-retical approaches such as Centering Theory, dealexclusively with anaphoric pronouns and have com-plex processing and definitional requirements.Given the full rhetorical structure available to animplemented generation system, we devised a sim-pler method of determining appropriate pronom-inalizations which was more accurate than exist-ing methods simulated by hand or performed semi-automatically.
This shows that approaches designedfor use with anaphora resolution, which must buildup discourse knowledge from scratch, may not bethe most desirable method for use in NLG, wherediscourse knowledge already exists.
The positive re-sults from our simple counting algorithm, after onlyminor changes in parameters from a narrative do-main to that of newspaper text, indicates that futurehigh-quality prose generation systems are very near.8 AcknowledgementsWe would like to thank Michael Young and RenateHenschel for their helpful comments; Kathy McCoyvery quickly provided the original 3 NYT articlesupon request; the anonymous reviewers whose com-ments greatly improved this paper.
Support for thiswork was provided by ITC-irst and the IntelliMediaInitiative of North Carolina State University.ReferencesDouglas E. Appelt.
1985.
Planning English referringexpressions.
Artificial Intelligence, 26:1?33.Frederick Baldwin.
1995.
CogNIAC: A Discourse Pro-cessing Engine.
Ph.D. thesis, The University of Penn-sylvania, Philadelphia, PA.Charles B. Callaway and James C. Lester.
2001a.
Eval-uating the effects of natural language generation onreader satisfaction.
In Proceedings of the Twenty-Third Annual Conference of the Cognitive Science So-ciety, pages 164?169, Edinburgh, UK.Charles B. Callaway and James C. Lester.
2001b.
Nar-rative prose generation.
In Proceedings of the Seven-teenth International Joint Conference on Artificial In-telligence, pages 1241?1248, Seattle, WA.Charles B. Callaway and James C. Lester.
2003.
Narra-tive prose generation.
Artificial Intelligence.
In press.Wim Claassen.
1992.
Generating referring expressionsin a multimodal environment.
In R. Dale, E. Hovy,D.
Rosner, and O.
Stock, editors, Aspects of Auto-mated Natural Language Generation, pages 247?62.Springer-Verlag, Berlin.Robert Dale.
1992.
Generating Referring Expressions.MIT Press.Barbara Di Eugenio.
1998.
Centering in Italian.
InMarilyn A. Walker, Aravind K. Joshi, and Ellen F.Prince, editors, Centering in Discourse.
Oxford Uni-versity Press, Cambridge, MA.Barbara J. Grosz and Candace L. Sidner.
1986.
Atten-tion, intentions, and the structure of discourse.
Com-putational Linguistics, 12(3):175?204.Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.1995.
Centering: A framework for modelling the lo-cal coherence of discourse.
Computational Linguis-tics, 21(2).Barbara J. Grosz.
1977.
The representation and use offocus in a system for understanding dialogs.
In Pro-ceedings of the Fifth International Joint Conference onArtificial Intelligence, pages 67?76, Cambridge, MA.Peter Heeman and Graeme Hirst.
1986.
Collaboratingon referring expressions.
Computational Linguistics,12(3):351?382.Renate Henschel, Hua Cheng, and Massimo Poesio.2000.
Pronominalization revisited.
In COLING?2000: Proceedings of the 18th International Con-ference on Computational Linguistics, Saarbruecken,Germany.Jerry R. Hobbs.
1976.
Pronoun resolution.
TechnicalReport 76-1, Department of Computer Science, CityCollege, CUNY, New York, NY.Roger Kibble and Richard Power.
2000.
An inte-grated framework for text planning and pronominali-sation.
In Proceedings of the First International Con-ference on Natural Language Generation, pages 194?200, Mitzpe Ramon, Israel.Kathleen F. McCoy and Michael Strube.
1999.
Takingtime to structure discourse: Pronoun generation be-yond accessibility.
In Proceedings of the Twenty-FirstConference of the Cognitive Science Society, pages378?383, Vancouver, CA, August.Elena Not.
1996.
A computational model for generatingreferring expressions in a multilingual application do-main.
In COLING?1996: Proceedings of the 16th In-ternational Conference on Computational Linguistics,Copenhagen, Denmark, August.M.
Poesio, H. Cheng, R. Henschel, J. Hitzeman, R. Kib-ble, and R. Stevenson.
1999a.
Specifying the parame-ters of centering theory: A corpus-based evaluation us-ing text from application-oriented domains.
In Book-title, page Pages, Address, Month.M.
Poesio, R. Henschel, J. Hitzeman, R. Kibble, S. Mon-tague, and K. van Deemter.
1999b.
Towards an anno-tation scheme for noun phrase generation.
In Bookti-tle, page Pages, Address, Month.R.
Quirk, S. Greenbaum, G. Leech, and J. Svartvik.
1985.A Comprehensive Grammar of the English Language.Longman Publishers.Ehud Reiter and Robert Dale.
1997.
Building ap-plied natural-language generation systems.
Journal ofNatural-Language Engineering, 3:57?87.Candace L. Sidner.
1983.
Focusing in the com-prehension of definite anaphora.
In M. Brady andR.
Berwick, editors, Computational Models of Dis-course, pages 267?330.
MIT Press, Cambridge, MA.Marilyn A. Walker.
1998.
Centering, anaphora resolu-tion, and discourse structure.
In Marilyn A. Walker,Aravind K. Joshi, and Ellen F. Prince, editors, Center-ing in Discourse.
Oxford University Press, Cambridge,MA.Bonnie Webber.
1979.
A Formal Approach to DiscourseAnaphora.
Garland, NY.Maria Wolters and Donna K. Byron.
2000.
Prosody andthe resolution of pronominal anaphora.
In COLING?2000: Proceedings of the 18th International Con-ference on Computational Linguistics, Saarbruecken,Germany.C.
Yeh and C. Mellish.
1997.
An empirical study onthe generation of anaphora in Chinese.
ComputationalLinguistics, 23(1):169?190.
