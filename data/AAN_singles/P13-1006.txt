Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 53?63,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsGrounded Language Learning from Video Described with SentencesHaonan Yu and Jeffrey Mark SiskindPurdue UniversitySchool of Electrical and Computer Engineering465 Northwestern Ave.West Lafayette, IN 47907-2035 USAhaonan@haonanyu.com, qobi@purdue.eduAbstractWe present a method that learns repre-sentations for word meanings from shortvideo clips paired with sentences.
Un-like prior work on learning language fromsymbolic input, our input consists of videoof people interacting with multiple com-plex objects in outdoor environments.
Un-like prior computer-vision approaches thatlearn from videos with verb labels or im-ages with noun labels, our labels are sen-tences containing nouns, verbs, preposi-tions, adjectives, and adverbs.
The cor-respondence between words and conceptsin the video is learned in an unsupervisedfashion, even when the video depicts si-multaneous events described by multiplesentences or when different aspects of asingle event are described with multiplesentences.
The learned word meaningscan be subsequently used to automaticallygenerate description of new video.1 IntroductionPeople learn language through exposure to a richperceptual context.
Language is grounded bymapping words, phrases, and sentences to mean-ing representations referring to the world.
Siskind(1996) has shown that even with referential un-certainty and noise, a system based on cross-situational learning can robustly acquire a lexicon,mapping words to word-level meanings from sen-tences paired with sentence-level meanings.
How-ever, it did so only for symbolic representations ofword- and sentence-level meanings that were notperceptually grounded.
An ideal system would notrequire detailed word-level labelings to acquireword meanings from video but rather could learnlanguage in a largely unsupervised fashion, just asa child does, from video paired with sentences.There has been recent research on grounded lan-guage learning.
Roy (2002) pairs training sen-tences with vectors of real-valued features ex-tracted from synthesized images which depict 2Dblocks-world scenes, to learn a specific set of fea-tures for adjectives, nouns, and adjuncts.
Yu andBallard (2004) paired training images containingmultiple objects with spoken name candidates forthe objects to find the correspondence betweenlexical items and visual features.
Dominey andBoucher (2005) paired narrated sentences withsymbolic representations of their meanings, au-tomatically extracted from video, to learn objectnames, spatial-relation terms, and event names asa mapping from the grammatical structure of asentence to the semantic structure of the associatedmeaning representation.
Chen and Mooney (2008)learned the language of sportscasting by deter-mining the mapping between game commentariesand the meaning representations output by a rule-based simulation of the game.
Kwiatkowski et al(2012) present an approach that learns Montague-grammar representations of word meanings to-gether with a combinatory categorial grammar(CCG) from child-directed sentences paired withfirst-order formulas that represent their meaning.Although most of these methods succeed inlearning word meanings from sentential descrip-tions they do so only for symbolic or simple vi-sual input (often synthesized); they fail to bridgethe gap between language and computer vision,i.e., they do not attempt to extract meaning rep-resentations from complex visual scenes.
On theother hand, there has been research on trainingobject and event models from large corpora ofcomplex images and video in the computer-visioncommunity (Kuznetsova et al, 2012; Sadanandand Corso, 2012; Kulkarni et al, 2011; Ordonezet al, 2011; Yao et al, 2010).
However, mostsuch work requires training data that labels indi-vidual concepts with individual words (i.e., ob-53jects delineated via bounding boxes in images asnouns and events that occur in short video clipsas verbs).
There is no attempt to model phrasalor sentential meaning, let alne acquire the ob-ject or event models from training data labeledwith phrasal or sentential annotations.
Moreover,such work uses distinct representations for differ-ent parts of speech; i.e., object and event recogniz-ers use different representations.In this paper, we present a method that learnsrepresentations for word meanings from shortvideo clips paired with sentences.
Our work dif-fers from prior work in three ways.
First, our inputconsists of realistic video filmed in an outdoor en-vironment.
Second, we learn the entire lexicon,including nouns, verbs, prepositions, adjectives,and adverbs, simultaneously from video describedwith whole sentences.
Third we adopt a uniformrepresentation for the meanings of words in allparts of speech, namely Hidden Markov Models(HMMs) whose states and distributions allow formultiple possible interpretations of a word or asentence in an ambiguous perceptual context.We employ the following representation toground the meanings of words, phrases, and sen-tences in video clips.
We first run an object de-tector on each video frame to yield a set of de-tections, each a subregion of the frame.
In prin-ciple, the object detector need just detect the ob-jects rather than classify them.
In practice, weemploy a collection of class-, shape-, pose-, andviewpoint-specific detectors and pool the detec-tions to account for objects whose shape, pose,and viewpoint may vary over time.
Our methodscan learn to associate a single noun with detectionsproduced by multiple detectors.
We then string to-gether detections from individual frames to yieldtracks for objects that temporally span the videoclip.
We associate a feature vector with each frame(detection) of each such track.
This feature vectorcan encode image features (including the identityof the particular detector that produced that detec-tion) that correlate with object class; region color,shape, and size features that correlate with objectproperties; and motion features, such as linear andangular object position, velocity, and acceleration,that correlate with event properties.
We also com-pute features between pairs of tracks to encode therelative position and motion of the pairs of objectsthat participate in events that involve two partici-pants.
In principle, we can also compute featuresbetween tuples of any number of tracks.Following Yamoto et al (1992), Siskind andMorris (1996), and Starner et al (1998), we repre-sent the meaning of an intransitive verb, like jump,as a two-state HMM over the velocity-directionfeature, modeling the requirement that the par-ticipant move upward then downward.
We rep-resent the meaning of a transitive verb, like pickup, as a two-state HMM over both single-objectand object-pair features: the agent moving to-ward the patient while the patient is as rest, fol-lowed by the agent moving together with the pa-tient.
We extend this general approach to otherparts of speech.
Nouns, like person, can be rep-resented as one-state HMMs over image featuresthat correlate with the object classes denoted bythose nouns.
Adjectives, like red, round, and big,can be represented as one-state HMMs over regioncolor, shape, and size features that correlate withobject properties denoted by such adjectives.
Ad-verbs, like quickly, can be represented as one-stateHMMs over object-velocity features.
Intransitiveprepositions, like leftward, can be represented asone-state HMMs over velocity-direction features.Static transitive prepositions, like to the left of, canbe represented as one-state HMMs over the rela-tive position of a pair of objects.
Dynamic transi-tive prepositions, like towards, can be representedas HMMs over the changing distance between apair of objects.
Note that with this formulation,the representation of a verb, like approach, mightbe the same as a dynamic transitive preposition,like towards.
While it might seem like overkillto represent the meanings of words as one-state-HMMs, in practice, we often instead encode suchconcepts with multiple states to allow for temporalvariation in the associated features due to chang-ing pose and viewpoint as well as deal with noiseand occlusion.
Moreover, the general frameworkof modeling word meanings as temporally varianttime series via multi-state HMMs allows one tomodel denominalized verbs, i.e., nouns that denoteevents, as in The jump was fast.Our HMMs are parameterized with vary-ing arity.
Some, like jump(?
), person(?),red(?
), round(?
), big(?
), quickly(?
), andleftward(?)
have one argument, while oth-ers, like pick-up(?, ?
), to-the-left-of(?, ?
), andtowards(?, ?
), have two arguments (In principle,any arity can be supported.).
HMMs are instanti-ated by mapping their arguments to tracks.
This54involves computing the associated feature vectorfor that HMM over the detections in the trackschosen to fill its arguments.
This is done witha two-step process to support compositional se-mantics.
The meaning of a multi-word phraseor sentence is represented as a joint likelihoodof the HMMs for the words in that phrase orsentence.
Compositionality is handled by link-ing or coindexing the arguments of the conjoinedHMMs.
Thus a sentence like The person tothe left of the backpack approached the trash-can would be represented as a conjunction ofperson(p0), to-the-left-of(p0, p1), backback(p1),approached(p0, p2), and trash-can(p2) over thethree participants p0, p1, and p2.
This wholesentence is then grounded in a particular videoby mapping these participants to particular tracksand instantiating the associated HMMs over thosetracks, by computing the feature vectors for eachHMM from the tracks chosen to fill its arguments.Our algorithm makes six assumptions.
First,we assume that we know the part of speech Cmassociated with each lexical entry m, along withthe part-of-speech dependent number of states Icin the HMMs used to represent word meaningsin that part of speech, the part-of-speech depen-dent number of features Nc in the feature vec-tors used by HMMs to represent word meanings inthat part of speech, and the part-of-speech depen-dent feature-vector computation ?c used to com-pute the features used by HMMs to represent wordmeanings in that part of speech.
Second, we pairindividual sentences each with a short video clipthat depicts that sentence.
The algorithm is notable to determine the alignment between multi-ple sentences and longer video segments.
Notethat there is no requirement that the video depictonly that sentence.
Other objects may be presentand other events may occur.
In fact, nothing pre-cludes a training corpus with multiple copies ofthe same video, each paired with a different sen-tence describing a different aspect of that video.Moreover, our algorithm potentially can handlea small amount of noise, where a video clip ispaired with an incorrect sentence that the videodoes not depict.
Third, we assume that we alreadyhave (pre-trained) low-level object detectors capa-ble of detecting instances of our target event par-ticipants in individual frames of the video.
We al-low such detections to be unreliable; our methodcan handle a moderate amount of false positivesand false negatives.
We do not need to knowthe mapping from these object-detection classesto words; our algorithm determines that.
Fourth,we assume that we know the arity of each wordin the corpus, i.e., the number of arguments thatthat word takes.
For example, we assume thatwe know that the word person(?)
takes one ar-gument and the word approached(?, ?)
takes twoarguments.
Fifth, we assume that we know the to-tal number of distinct participants that collectivelyfill all of the arguments for all of the words ineach training sentence.
For example, for the sen-tence The person to the left of the backpack ap-proached the trash-can, we assume that we knowthat there are three distinct objects that partic-ipate in the event denoted.
Sixth, we assumethat we know the argument-to-participant map-ping for each training sentence.
Thus, for ex-ample, for the above sentence we would knowperson(p0), to-the-left-of(p0, p1), backback(p1),approached(p0, p2), and trash-can(p2).
The lat-ter two items can be determined by parsing thesentence, which is what we do.
One can imaginelearning the ability to automatically perform thelatter two items, and even the fourth item above,by learning the grammar and the part of speechof each word, such as done by Kwiatkowski et al(2012).
We leave such for future work.Figure 1 illustrates a single frame from a po-tential training sample provided as input to ourlearner.
It consists of a video clip paired witha sentence, where the arguments of the words inthe sentence are mapped to participants.
Froma sequence of such training samples, our learnerdetermines the objects tracks and the mappingfrom participants to those tracks, together with themeanings of the words.The remainder of the paper is organized as fol-lows.
Section 2 generally describes our problemof lexical acquisition from video.
Section 3 intro-duces our work on the sentence tracker, a methodfor jointly tracking the motion of multiple ob-jects in a video that participate in a sententially-specified event.
Section 4 elaborates on the de-tails of our problem formulation in the context ofthis sentence tracker.
Section 5 describes how togeneralize and extend the sentence tracker so thatit can be used to support lexical acquisition.
Wedemonstrate this lexical acquisition algorithm on asmall example in Section 6.
Finally, we concludewith a discussion in Section 7.551320The person to the left of the backpack carried the trash-can towards the chair.
?p0 ?p0 ?p1 ?p1 ?p0 ?p2 ?p2 ?p0 ?p3 ?p3Track 3 Track 0 Track 1 Track 2Figure 1: An illustration of our problem.
Eachword in the sentence has one or more arguments(?
and possibly ?
), each argument of each word isassigned to a participant (p0, .
.
.
, p3) in the eventdescribed by the sentence, and each participantcan be assigned to any object track in the video.This figure shows a possible (but erroneous) in-terpretation of the sentence where the mapping is:p0 7?
Track 3, p1 7?
Track 0, p2 7?
Track 1,and p3 7?
Track 2, which might (incorrectly) leadthe learner to conclude that the word person mapsto the backpack, the word backpack maps to thechair, the word trash-can maps to the trash-can,and the word chair maps to the person.2 General Problem FormulationThroughout this paper, lowercase letters are usedfor variables or hidden quantities while uppercaseones are used for constants or observed quantities.We are given a lexicon {1, .
.
.
,M}, letting mdenote a lexical entry.
We are given a sequenceD = (D1, .
.
.
, DR) of video clips Dr, eachpaired with a sentence Sr from a sequence S =(S1, .
.
.
, SR) of sentences.
We refer to Dr pairedwith Sr as a training sample.
Each sentence Sr isa sequence (Sr,1, .
.
.
, Sr,Lr) of words Sr,l, each anentry from the lexicon.
A given entry may poten-tially appear in multiple sentences and even mul-tiple times in a given sentence.
For example, thethird word in the first sentence might be the sameentry as the second word in the fourth sentence,in which case S1,3 = S4,2.
This is what allowscross-situational learning in our algorithm.Let us assume, for a moment, that we canprocess each video clip Dr to yield a sequence(?r,1, .
.
.
, ?r,Ur) of object tracks ?r,u.
Let usalso assume that Dr is paired with a sen-tence Sr = The person approached the chair,specified to have two participants, pr,0 and pr,1,with the mapping person(pr,0), chair(pr,1), andapproached(pr,0, pr,1).
Let us further assume, fora moment, that we are given a mapping fromparticipants to object tracks, say pr,0 7?
?r,39and pr,1 7?
?r,51.
This would allow us toinstantiate the HMMs with object tracks for agiven video clip: person(?r,39), chair(?r,51), andapproached(?r,39, ?r,51).
Let us further assumethat we can score each such instantiated HMM andaggregate the scores for all of the words in a sen-tence to yield a sentence score and further aggre-gate the scores for all of the sentences in the cor-pus to yield a corpus score.
However, we don?tknow the parameters of the HMMs.
These con-stitute the unknown meanings of the words in ourcorpus which we wish to learn.
The problem isto simultaneously determine (a) those parametersalong with (b) the object tracks and (c) the map-ping from participants to object tracks.
We do thisby finding (a)?
(c) that maximizes the corpus score.3 The Sentence TrackerBarbu et al (2012a) presented a method that firstdetermines object tracks from a single video clipand then uses these fixed tracks with HMMs torecognize actions corresponding to verbs and con-struct sentential descriptions with templates.
LaterBarbu et al (2012b) addressed the problem ofsolving (b) and (c), for a single object track con-strained by a single intransitive verb, without solv-ing (a), in the context of a single video clip.
Ourgroup has generalized this work to yield an algo-rithm called the sentence tracker which operatesby way of a factorial HMM framework.
We intro-duce that here as the foundation of our extension.Each video clip Dr contains Tr frames.
Werun an object detector on each frame to yield aset Dtr of detections.
Since our object detectoris unreliable, we bias it to have high recall butlow precision, yielding multiple detections in eachframe.
We form an object track by selecting a sin-gle detection for that track for each frame.
For amoment, let us consider a single video clip withlength T , with detections Dt in frame t. Further,let us assume that we seek a single object trackin that video clip.
Let jt denote the index of thedetection from Dt in frame t that is selected toform the track.
The object detector scores eachdetection.
Let F (Dt, jt) denote that score.
More-56over, we wish the track to be temporally coherent;we want the objects in a track to move smoothlyover time and not jump around the field of view.Let G(Dt?1, jt?1, Dt, jt) denote some measureof coherence between two detections in adjacentframes.
(One possible such measure is consistencyof the displacement ofDt relative toDt?1 with thevelocity of Dt?1 computed from the image by op-tical flow.)
One can select the detections to yield atrack that maximizes both the aggregate detectionscore and the aggregate temporal coherence score.maxj1,...,jT?????
?T?t=1F (Dt, jt)+T?t=2G(Dt?1, jt?1, Dt, jt)??????
(1)This can be determined with the Viterbi (1967) al-gorithm and is known as detection-based tracking(Viterbi, 1971).Recall that we model the meaning of an in-transitive verb as an HMM over a time seriesof features extracted for its participant in eachframe.
Let ?
denote the parameters of this HMM,(q1, .
.
.
, qT ) denote the sequence of states qt thatleads to an observed track, B(Dt, jt, qt, ?)
de-note the conditional log probability of observ-ing the feature vector associated with the detec-tion selected by jt among the detections Dt inframe t, given that the HMM is in state qt, andA(qt?1, qt, ?)
denote the log transition probabil-ity of the HMM.
For a given track (j1, .
.
.
, jT ),the state sequence that yields the maximal likeli-hood is given by:maxq1,...,qT?????
?T?t=1B(Dt, jt, qt, ?
)+T?t=2A(qt?1, qt, ?)??????
(2)which can also be found by the Viterbi algorithm.A given video clip may depict multiple objects,each moving along its own trajectory.
There maybe both a person jumping and a ball rolling.
Howare we to select one track over the other?
The keyinsight of the sentence tracker is to bias the selec-tion of a track so that it matches an HMM.
This isdone by combining the cost function of Eq.
1 withthe cost function of Eq.
2 to yield Eq.
3, which canalso be determined using the Viterbi algorithm.This is done by forming the cross product of thetwo lattices.
This jointly selects the optimal detec-tions to form the track, together with the optimalstate sequence, and scores that combination.maxj1,...,jTq1,...,qT??????
?T?t=1F (Dt, jt)+B(Dt, jt, qt, ?
)+T?t=2G(Dt?1, jt?1, Dt, jt)+A(qt?1, qt, ?)???????
(3)While we formulated the above around a sin-gle track and a word that contains a single partic-ipant, it is straightforward to extend this so that itsupports multiple tracks and words of higher ar-ity by forming a larger cross product.
When doingso, we generalize jt to denote a sequence of de-tections from Dt, one for each of the tracks.
Wefurther need to generalize F so that it computesthe joint score of a sequence of detections, one foreach track, G so that it computes the joint mea-sure of coherence between a sequence of pairs ofdetections in two adjacent frames, and B so thatit computes the joint conditional log probabilityof observing the feature vectors associated withthe sequence of detections selected by jt.
Whendoing this, note that Eqs.
1 and 3 maximize overj1, .
.
.
, jT which denotes T sequences of detec-tion indices, rather than T individual indices.It is further straightforward to extend the aboveto support a sequence (S1, .
.
.
, SL) of words Sldenoting a sentence, each of which applies to dif-ferent subsets of the multiple tracks, again byforming a larger cross product.
When doing so, wegeneralize qt to denote a sequence (qt1, .
.
.
, qtL) ofstates qtl , one for each word l in the sentence, anduse ql to denote the sequence (q1l , .
.
.
, qTl ) and qto denote the sequence (q1, .
.
.
, qL).
We furtherneed to generalize B so that it computes the jointconditional log probability of observing the fea-ture vectors for the detections in the tracks that areassigned to the arguments of the HMM for eachword in the sentence and A so that it computes thejoint log transition probability for the HMMs forall words in the sentence.
This allows selectionof an optimal sequence of tracks that yields thehighest score for the sentential meaning of a se-quence of words.
Modeling the meaning of a sen-tence through a sequence of words whose mean-ings are modeled by HMMs, defines a factorialHMM for that sentence, since the overall Markovprocess for that sentence can be factored into inde-57pendent component processes (Brand et al, 1997;Zhong and Ghosh, 2001) for the individual words.In this view, q denotes the state sequence for thecombined factorial HMM and ql denotes the factorof that state sequence for word l. The remainderof this paper wraps this sentence tracker in BaumWelch (Baum et al, 1970; Baum, 1972).4 Detailed Problem FormulationWe adapt the sentence tracker to training a cor-pus of R video clips, each paired with a sentence.Thus we augment our notation, generalizing jtto jtr and qtl to qtr,l.
Below, we use jr to denote(j1r , .
.
.
, jTrr ), j to denote (j1, .
.
.
, jR), qr,l to de-note (q1r,l, .
.
.
, qTrr,l ), qr to denote (qr,1, .
.
.
, qr,Lr),and q to denote (q1, .
.
.
, qR).We use discrete features, namely natural num-bers, in our feature vectors, quantized by a binningprocess.
We assume the part of speech of entry mis known as Cm.
The length of the feature vectormay vary across parts of speech.
LetNc denote thelength of the feature vector for part of speech c,xr,l denote the time-series (x1r,l, .
.
.
, xTrr,l) of fea-ture vectors xtr,l, associated with Sr,l (which re-call is some entry m), and xr denote the sequence(xr,1, .
.
.
, xr,Lr).
We assume that we are givena function ?c(Dtr, jtr) that computes the featurevector xtr,l for the word Sr,l whose part of speechis CSr,l = c. Note that we allow ?
to be depen-dent on c allowing different features to be com-puted for different parts of speech, since we candeterminem and thus Cm from Sr,l.
We choose tohaveNc and ?c depend on the part of speech c andnot on the entry m since doing so would be tanta-mount to encoding the to-be-learned word mean-ing in the provided feature-vector computation.The goal of training is to find a sequence ?
=(?1, .
.
.
, ?M ) of parameters ?m that best explainsthe R training samples.
The parameters ?m con-stitute the meaning of the entry m in the lexicon.Collectively, these are the initial state probabili-ties am0,k, for 1 ?
k ?
ICm , the transition prob-abilities ami,k, for 1 ?
i, k ?
ICm , and the out-put probabilities bmi,n(x), for 1 ?
i ?
ICm and1 ?
n ?
NCm , where ICm denotes the number ofstates in the HMM for entry m. Like before, wecould have a distinct Im for each entry m but in-stead have ICm depend only on the part of speechof entry m, and assume that we know the fixed Ifor each part of speech.
In our case, bmi,n is a dis-crete distribution because the features are binned.5 The Learning AlgorithmInstantiating the above approach requires a defini-tion for what it means to best explain the R train-ing samples.
Towards this end, we define the scoreof a video clip Dr paired with sentence Sr giventhe parameter set ?
to characterize how well thistraining sample is explained.
While the cost func-tion in Eq.
3 may qualify as a score, it is easier tofit a likelihood calculation into the Baum-Welchframework than a MAP estimate.
Thus we replacethe max in Eq.
3 with a ?
and redefine our scor-ing function as follows:L(Dr;Sr, ?)
=?jrP (jr|Dr)P (xr|Sr, ?)
(4)The score in Eq.
4 can be interpreted as an ex-pectation of the HMM likelihood over all possiblemappings from participants to all possible tracks.By definition, P (jr|Dr) = V (Dr,jr)?j?r V (Dr,j?r) , wherethe numerator is the score of a particular track se-quence jr while the denominator sums the scoresover all possible track sequences.
The log of thenumerator V (Dr, jr) is simply Eq.
1 without themax.
The log of the denominator can be com-puted efficiently by the forward algorithm (Baumand Petrie, 1966).
The likelihood for a factorialHMM can be computed as:P (xr|Sr, ?)
=?qr?lP (xr,l, qr,l|Sr,l, ?)
(5)i.e., summing the likelihoods for all possible statesequences.
Each summand is simply the joint like-lihood for all the words in the sentence condi-tioned on a state sequence qr.
For HMMs we haveP (xr,l, qr,l|Sr,l, ?)
=?taSr,lqt?1r,l ,qtr,l?nbSr,lqtr,l,n(xtr,l,n)(6)Finally, for a training corpus of R samples, weseek to maximize the joint score:L(D;S, ?)
=?rL(Dr;Sr, ?)
(7)A local maximum can be found by employingthe Baum-Welch algorithm (Baum et al, 1970;Baum, 1972).
By constructing an auxiliary func-tion (Bilmes, 1997), one can derive the reestima-tion formulas in Eq.
8, where xtr,l,n = h denotesthe selection of all possible jtr such that the nth58ami,k = ?miR?r=1Lr?l=1s.t.Sr,l=mTr?t=1L(qt?1r,l = i, qtr,l = k,Dr;Sr, ??
)L(Dr;Sr, ??)?
??
??
(r,l,i,k,t)bmi,n(h) = ?mi,nR?r=1Lr?l=1s.t.Sr,l=mTr?t=1L(qtr,l = i, xtr,l,n = h,Dr;Sr, ??
)L(Dr;Sr, ??)?
??
??
(r,l,n,i,h,t)(8)feature computed by ?Cm(Dtr, jtr) is h. The coef-ficients ?mi and ?mi,n are for normalization.The reestimation formulas involve occurrencecounting.
However, since we use a factorial HMMthat involves a cross-product lattice and use a scor-ing function derived from Eq.
3 that incorporatesboth tracking (Eq.
1) and word models (Eq.
2),we need to count the frequency of transitions inthe whole cross-product lattice.
As an exampleof such cross-product occurrence counting, whencounting the transitions from state i to k for thelth word from frame t ?
1 to t, i.e., ?
(r, l, i, k, t),we need to count all the possible paths throughthe adjacent factorial states (jt?1r , qt?1r,1 , .
.
.
, qt?1r,Lr)and (jtr, qtr,1, .
.
.
, qtr,Lr) such that qt?1r,l = i andqtr,l = k. Similarly, when counting the fre-quency of being at state i while observing h asthe nth feature in frame t for the lth word ofentry m, i.e., ?
(r, l, n, i, h, t), we need to countall the possible paths through the factorial state(jtr, qtr,1, .
.
.
, qtr,Lr) such that qtr,l = i and the nthfeature computed by ?Cm(Dtr, jtr) is h.The reestimation of a single component HMMcan depend on the previous estimate for othercomponent HMMs.
This dependence happensbecause of the argument-to-participant mappingwhich coindexes arguments of different compo-nent HMMs to the same track.
It is preciselythis dependence that leads to cross-situationallearning of two kinds: both inter-sentential andintra-sentential.
Acquisition of a word meaningis driven across sentences by entries that appearin more than one training sample and within sen-tences by the requirement that the meanings of allof the individual words in a sentence be consistentwith the collective sentential meaning.6 ExperimentWe filmed 61 video clips (each 3?5 seconds at640?480 resolution and 40 fps) that depict a va-riety of different compound events.
Each clip de-picts multiple simultaneous events between someS ?
NP VPNP ?
D N [PP]D ?
theN ?
person | backpack | trash-can | chairPP ?
P NPP ?
to the left of | to the right ofVP ?
V NP [ADV] [PPM]V ?
picked up | put down | carried | approachedADV ?
quickly | slowlyPPM ?
PM NPPM ?
towards | away fromTable 1: The grammar used for our annotation andgeneration.
Our lexicon contains 1 determiner,4 nouns, 2 spatial relation prepositions, 4 verbs,2 adverbs, and 2 motion prepositions for a total of15 lexical entries over 6 parts of speech.subset of four objects: a person, a backpack, achair, and a trash-can.
These clips were filmedin three different outdoor environments which weuse for cross validation.
We manually annotatedeach video with several sentences that describewhat occurs in that video.
The sentences wereconstrained to conform to the grammar in Table 1.Our corpus of 159 training samples pairs somevideos with more than one sentence and some sen-tences with more than one video, with an averageof 2.6 sentences per video 1.We model and learn the semantics of all wordsexcept determiners.
Table 2 specifies the arity, thestate number Ic, and the features computed by ?cfor the semantic models for words of each part ofspeech c. While we specify a different subset offeatures for each part of speech, we presume that,in principle, with enough training data, we couldinclude all features in all parts of speech and auto-matically learn which ones are noninformative andlead to uniform distributions.We use an off-the-shelf object detector (Felzen-szwalb et al, 2010a; Felzenszwalb et al, 2010b)which outputs detections in the form of scoredaxis-aligned rectangles.
We trained four object de-tectors, one for each of the four object classes in1Our code, videos, and sentential annotations areavailable at http://haonanyu.com/research/acl2013/.59c arity Ic ?cN 1 1 ?
detector indexV 2 3?
VEL MAG?
VEL ORIENT?
VEL MAG?
VEL ORIENT?-?
DIST?-?
size ratioP 2 1 ?-?
x-positionADV 1 3 ?
VEL MAGPM 2 3 ?
VEL MAG?-?
DISTTable 2: Arguments and model configurations fordifferent parts of speech c. VEL stands for veloc-ity, MAG for magnitude, ORIENT for orientation,and DIST for distance.our corpus: person, backpack, chair, and trash-can.
For each frame, we pick the two highest-scoring detections produced by each object detec-tor and pool the results yielding eight detectionsper frame.
Having a larger pool of detections perframe can better compensate for false negatives inthe object detection and potentially yield smoothertracks but it increases the size of the lattice and theconcomitant running time and does not lead to ap-preciably better performance on our corpus.We compute continuous features, such as veloc-ity, distance, size ratio, and x-position solely fromthe detection rectangles and quantize the featuresinto bins as follows:velocity To reduce noise, we compute the veloc-ity of a participant by averaging the optical flowin the detection rectangle.
The velocity magni-tude is quantized into 5 levels: absolutely station-ary, stationary, moving, fast moving, and quickly.The velocity orientation is quantized into 4 direc-tions: left, up, right, and down.distance We compute the Euclidean distance be-tween the detection centers of two participants,which is quantized into 3 levels: near, normal,and far away.size ratio We compute the ratio of detection areaof the first participant to the detection area of thesecond participant, quantized into 2 possibilities:larger/smaller than.x-position We compute the difference betweenthe x-coordinates of the participants, quantizedinto 2 possibilities: to the left/right of.The binning process was determined by a prepro-cessing step that clustered a subset of the trainingdata.
We also incorporate the index of the detectorthat produced the detection as a feature.
The par-ticular features computed for each part of speechare given in Table 2.Note that while we use English phrases, like tothe left of, to refer to particular bins of particularfeatures, and we have object detectors which wetrain on samples of a particular object class suchas backpack, such phrases are only mnemonic ofthe clustering and object-detector training process.We do not have a fixed correspondence betweenthe lexical entries and any particular feature value.Moreover, that correspondence need not be one-to-one: a given lexical entry may correspond to a(time variant) constellation of feature values andany given feature value may participate in themeaning of multiple lexical entries.We perform a three-fold cross validation, takingthe test data for each fold to be the videos filmed ina given outdoor environment and the training datafor that fold to be all training samples that containother videos.
For testing, we hand selected 24 sen-tences generated by the grammar in Table 1, whereeach sentence is true for at least one test video.Half of these sentences (designated NV) containonly nouns and verbs while the other half (des-ignated ALL) contain other parts of speech.
Thelatter are longer and more complicated than theformer.
We score each testing video paired withevery sentence in both NV and ALL.
To evaluateour results, we manually annotated the correctnessof each such pair.Video-sentence pairs could be scored withEq.
4.
However, the score depends on the sentencelength, the collective numbers of states and fea-tures in the HMMs for words in that sentence, andthe length of the video clip.
To render the scorescomparable across such variation we incorporate asentence prior to the per-frame score:L?
(Dr, Sr;?)
= [L(Dr;Sr, ?
)]1Tr pi(Sr) (9)wherepi(Sr) =expLr?l=1???
?E(ICSr,l )+NCSr,l?n=1E(ZCSr,l ,n)????
(10)In the above, ZCSr,l ,n is the number of bins forthe nth feature of Sr,l of part of speech CSr,l andE(Y ) = ?
?Yy=1 1Y log 1Y = log Y is the entropyof a uniform distribution over Y bins.
This priorprefers longer sentences which describe more in-formation in the video.60CHANCE BLIND OUR HANDNV 0.155 0.265 0.545 0.748ALL 0.099 0.198 0.639 0.786Table 3: F1 scores of different methods.Figure 2: ROC curves of trained models and hand-written models.The scores are thresholded to decide hits, whichtogether with the manual annotations, can gener-ate TP, TN, FP, and FN counts.
We select thethreshold that leads to the maximal F1 score onthe training set, use this threshold to compute F1scores on the test set in each fold, and average F1scores across the folds.The F1 scores are listed in the column labeledOur in Table 3.
For comparison, we also reportF1 scores for three baselines: Chance, Blind, andHand.
The Chance baseline randomly classifiesa video-sentence pair as a hit with probability 0.5.The Blind baseline determines hits by potentiallylooking at the sentence but never looking at thevideo.
We can find an upper bound on the F1 scorethat any blind method could have on each of ourtest sets by solving a 0-1 fractional programmingproblem (Dinkelbach, 1967) (see Appendix A fordetails).
The Hand baseline determines hits withhand-coded HMMs, carefully designed to yieldwhat we believe is near-optimal performance.
Ascan be seen from Table 3, our trained modelsperform substantially better than the Chance andBlind baselines and approach the performance ofthe ideal Hand baseline.
One can further see fromthe ROC curves in Figure 2, comparing the trainedand hand-written models on both NV and ALL, thatthe trained models are close to optimal.
Note thatperformance on ALL exceeds that on NV with thetrained models.
This is because longer sentenceswith varied parts of speech incorporate more in-formation into the scoring process.7 ConclusionWe presented a method that learns word mean-ings from video paired with sentences.
Unlikeprior work, our method deals with realistic videoscenes labeled with whole sentences, not indi-vidual words labeling hand delineated objects orevents.
The experiment shows that it can cor-rectly learn the meaning representations in termsof HMM parameters for our lexical entries, fromhighly ambiguous training data.
Our maximum-likelihood method makes use of only positive sen-tential labels.
As such, it might require more train-ing data for convergence than a method that alsomakes use of negative training sentences that arenot true of a given video.
Such can be handledwith discriminative training, a topic we plan to ad-dress in the future.
We believe that this will allowlearning larger lexicons from more complex videowithout excessive amounts of training data.AcknowledgmentsThis research was sponsored by the Army Re-search Laboratory and was accomplished underCooperative Agreement Number W911NF-10-2-0060.
The views and conclusions contained in thisdocument are those of the authors and should notbe interpreted as representing the official policies,either express or implied, of the Army ResearchLaboratory or the U.S. Government.
The U.S.Government is authorized to reproduce and dis-tribute reprints for Government purposes, notwith-standing any copyright notation herein.A An Upper Bound on the F1 Score ofany Blind MethodA Blind algorithm makes identical decisions onthe same sentence paired with different videoclips.
An optimal algorithm will try to find a de-cision si for each test sentence i that maximizesthe F1 score.
Suppose, the ground-truth yields FPifalse positives and TPi true positives on the testset when si = 1.
Also suppose that setting si = 0yields FNi false negatives.
Then the F1 score isF1 = 11 +?i siFPi + (1?
si)FNi?i 2siTPi?
??
?
?Thus we want to minimize the term ?.
This is aninstance of a 0-1 fractional programming problemwhich can be solved by binary search or Dinkel-bach?s algorithm (Dinkelbach, 1967).61ReferencesA.
Barbu, A. Bridge, Z. Burchill, D. Coroian, S. Dick-inson, S. Fidler, A. Michaux, S. Mussman, N. Sid-dharth, D. Salvi, L. Schmidt, J. Shangguan, J. M.Siskind, J. Waggoner, S. Wang, J. Wei, Y. Yin, andZ.
Zhang.
2012a.
Video in sentences out.
In Pro-ceedings of the Twenty-Eighth Conference on Un-certainty in Artificial Intelligence, pages 102?112.A.
Barbu, N. Siddharth, A. Michaux, and J. M. Siskind.2012b.
Simultaneous object detection, tracking, andevent recognition.
Advances in Cognitive Systems,2:203?220, December.L.
E. Baum and T. Petrie.
1966.
Statistical inferencefor probabilistic functions of finite state Markovchains.
The Annals of Mathematical Statistics,37:1554?1563.L.
E. Baum, T. Petrie, G. Soules, and N. Weiss.
1970.A maximization technique occuring in the statisticalanalysis of probabilistic functions of Markov chains.The Annals of Mathematical Statistics, 41(1):164?171.L.
E. Baum.
1972.
An inequality and associated maxi-mization technique in statistical estimation of proba-bilistic functions of a Markov process.
Inequalities,3:1?8.J.
Bilmes.
1997.
A gentle tutorial of the EM algorithmand its application to parameter estimation for Gaus-sian mixture and hidden Markov models.
TechnicalReport TR-97-021, ICSI.M.
Brand, N. Oliver, and A. Pentland.
1997.
Coupledhidden Markov models for complex action recog-nition.
In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition, pages994?999.D.
L. Chen and R. J. Mooney.
2008.
Learning tosportscast: A test of grounded language acquisition.In Proceedings of the 25th International Conferenceon Machine Learning.W.
Dinkelbach.
1967.
On nonlinear fractional pro-gramming.
Management Science, 13(7):492?498.P.
F. Dominey and J.-D. Boucher.
2005.
Learning totalk about events from narrated video in a construc-tion grammar framework.
Artificial Intelligence,167(12):31?61.P.
F. Felzenszwalb, R. B. Girshick, D. McAllester, andD.
Ramanan.
2010a.
Object detection with discrim-inatively trained part-based models.
IEEE Transac-tions on Pattern Analysis and Machine Intelligence,32(9):1627?1645.P.
F. Felzenszwalb, R. B. Girshick, and D. A.McAllester.
2010b.
Cascade object detection withdeformable part models.
In Proceedings of the IEEEConference on Computer Vision and Pattern Recog-nition, pages 2241?2248.G.
Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, A. C.Berg, and T. L. Berg.
2011.
Baby talk: Understand-ing and generating simple image descriptions.
InProceedings of the IEEE Conference on ComputerVision and Pattern Recognition, pages 1601?1608.P.
Kuznetsova, V. Ordonez, A. C. Berg, T. L. Berg, andY.
Choi.
2012.
Collective generation of natural im-age descriptions.
In Proceedings of the 50th AnnualMeeting of the Association for Computational Lin-guistics: Long Papers - Volume 1, pages 359?368.T.
Kwiatkowski, S. Goldwater, L. Zettlemoyer, andM.
Steedman.
2012.
A probabilistic model of syn-tactic and semantic acquisition from child-directedutterances and their meanings.
In Proceedings of the13th Conference of the European Chapter of the As-sociation for Computational Linguistics, pages 234?244.V.
Ordonez, G. Kulkarni, and T. L. Berg.
2011.Im2text: Describing images using 1 million cap-tioned photographs.
In Proceedings of Neural In-formation Processing Systems.D.
Roy.
2002.
Learning visually-grounded wordsand syntax for a scene description task.
ComputerSpeech and Language, 16:353?385.S.
Sadanand and J. J. Corso.
2012.
Action bank: Ahigh-level representation of activity in video.
InProceedings of the IEEE Conference on ComputerVision and Pattern Recognition, pages 1234?1241.J.
M. Siskind and Q. Morris.
1996.
A maximum-likelihood approach to visual event classification.
InProceedings of the Fourth European Conference onComputer Vision, pages 347?360.J.
M. Siskind.
1996.
A computational study of cross-situational techniques for learning word-to-meaningmappings.
Cognition, 61:39?91.T.
Starner, J. Weaver, and A. Pentland.
1998.
Real-time American Sign Language recognition usingdesk and wearable computer based video.
IEEETransactions on Pattern Analysis and Machine In-telligence, 20(12):1371?1375.A.
J. Viterbi.
1967.
Error bounds for convolutionalcodes and an asymtotically optimum decoding algo-rithm.
IEEE Transactions on Information Theory,13:260?267.A.
Viterbi.
1971.
Convolutional codes and their per-formance in communication systems.
IEEE Trans-actions on Communication Technology, 19(5):751?772.J.
Yamoto, J. Ohya, and K. Ishii.
1992.
Recogniz-ing human action in time-sequential images usinghidden Markov model.
In Proceedings of the IEEEConference on Computer Vision and Pattern Recog-nition, pages 379?385.62B.
Z. Yao, X. Yang, L. Lin, M. W. Lee, and S.-C. Zhu.2010.
I2T: Image parsing to text description.
Pro-ceedings of the IEEE, 98(8):1485?1508, August.C.
Yu and D. H. Ballard.
2004.
On the integration ofgrounding language and learning objects.
In Pro-ceedings of the 19th National Conference on Artifi-cal intelligence, pages 488?493.S.
Zhong and J. Ghosh.
2001.
A new formulation ofcoupled hidden Markov models.
Technical report,Department of Electrical and Computer Engineer-ing, The University of Texas at Austin.63
