Do We Need Chinese Word Segmentationfor Statistical Machine Translation?Jia Xu and Richard Zens and Hermann NeyChair of Computer Science VIComputer Science DepartmentRWTH Aachen University, Germany{xujia,zens,ney}@cs.rwth-aachen.deAbstractIn Chinese texts, words are not separated bywhite spaces.
This is problematic for many nat-ural language processing tasks.
The standardapproach is to segment the Chinese charactersequence into words.
Here, we investigate Chi-nese word segmentation for statistical machinetranslation.
We pursue two goals: the first oneis the maximization of the final translation qual-ity; the second is the minimization of the man-ual effort for building a translation system.The commonly used method for getting theword boundaries is based on a word segmenta-tion tool and a predefined monolingual dictio-nary.
To avoid the dependence of the trans-lation system on an external dictionary, wehave developed a system that learns a domain-specific dictionary from the parallel trainingcorpus.
This method produces results that arecomparable with the predefined dictionary.Further more, our translation system is ableto work without word segmentation with only aminor loss in translation quality.1 IntroductionIn Chinese texts, words composed of single ormultiple characters, are not separated by whitespaces, which is different from most of the west-ern languages.
This is problematic for manynatural language processing tasks.
Therefore,the usual method is to segment a Chinese char-acter sequence into Chinese ?words?.Many investigations have been performedconcerning Chinese word segmentation.
Forexample, (Palmer, 1997) developed a Chineseword segmenter using a manually segmentedcorpus.
The segmentation rules were learnedautomatically from this corpus.
(Sproat andShih, 1990) and (Sun et al, 1998) used amethod that does not rely on a dictionary or amanually segmented corpus.
The characters ofthe unsegmented Chinese text are grouped intopairs with the highest value of mutual informa-tion.
This mutual information can be learnedfrom an unsegmented Chinese corpus.We will present a new method for segment-ing the Chinese text without using a manuallysegmented corpus or a predefined dictionary.
Instatistical machine translation, we have a bilin-gual corpus available, which is used to obtaina segmentation of the Chinese text in the fol-lowing way.
First, we train the statistical trans-lation models with the unsegmented bilingualcorpus.
As a result, we obtain a mapping ofChinese characters to the corresponding Englishwords for each sentence pair.
By using this map-ping, we can extract a dictionary automatically.With this self-learned dictionary, we use a seg-mentation tool to obtain a segmented Chinesetext.
Finally, we retrain our translation systemwith the segmented corpus.Additionally, we have performed experimentswithout explicit word segmentation.
In thiscase, each Chinese character is interpreted asone ?word?.
Based on word groups, our ma-chine translation system is able to work withouta word segmentation, while having only a minortranslation quality relative loss of less than 5%.2 Review of the Baseline System forStatistical Machine Translation2.1 PrincipleIn statistical machine translation, we are givena source language (?French?)
sentence fJ1 =f1 .
.
.
fj .
.
.
fJ , which is to be translated intoa target language (?English?)
sentence eI1 =e1 .
.
.
ei .
.
.
eI .
Among all possible target lan-guage sentences, we will choose the sentencewith the highest probability:e?I1 = argmaxeI1{Pr(eI1|fJ1 )} (1)= argmaxeI1{Pr(eI1) ?
Pr(fJ1 |eI1)} (2)The decomposition into two knowledge sourcesin Equation 2 is known as the source-channelapproach to statistical machine translation(Brown et al, 1990).
It allows an independentmodeling of target language model Pr(eI1) andtranslation model Pr(fJ1 |eI1)1.
The target lan-guage model describes the well-formedness ofthe target language sentence.
The translationmodel links the source language sentence to thetarget language sentence.
The argmax opera-tion denotes the search problem, i.e.
the gener-ation of the output sentence in the target lan-guage.
We have to maximize over all possibletarget language sentences.The resulting architecture for the statisticalmachine translation approach is shown in Fig-ure 1 with the translation model further decom-posed into lexicon and alignment model.Source Language TextTransformationLexicon ModelLanguage ModelGlobal Search:Target Language TextoverPr(f1  J  |e1I )Pr(   e1I )Pr(f1  J  |e1I )   Pr(   e1I )e1If1 Jmaximize  Alignment ModelTransformationFigure 1: Architecture of the translation ap-proach based on Bayes decision rule.2.2 Alignment ModelsThe alignment model Pr(fJ1 , aJ1 |eI1) introducesa ?hidden?
alignment a = aJ1 , which describes1The notational convention will be as follows: we usethe symbol Pr(?)
to denote general probability distri-butions with (nearly) no specific assumptions.
In con-trast, for model-based probability distributions, we usethe generic symbol p(?
).a mapping from a source position j to a targetposition aj .
The relationship between the trans-lation model and the alignment model is givenby:Pr(fJ1 |eI1) =?aJ1Pr(fJ1 , aJ1 |eI1) (3)In this paper, we use the models IBM-1, IBM-4 from (Brown et al, 1993) and the Hidden-Markov alignment model (HMM) from (Vogel etal., 1996).
All these models provide different de-compositions of the probability Pr(fJ1 , aJ1 |eI1).A detailed description of these models can befound in (Och and Ney, 2003).A Viterbi alignment a?J1 of a specific model isan alignment for which the following equationholds:a?J1 = argmaxaJ1Pr(fJ1 , aJ1 |eI1).
(4)The alignment models are trained on a bilin-gual corpus using GIZA++(Och et al, 1999;Och and Ney, 2003).
The training is done it-eratively in succession on the same data, wherethe final parameter estimates of a simpler modelserve as starting point for a more complexmodel.
The result of the training procedure isthe Viterbi alignment of the final training iter-ation for the whole training corpus.2.3 Alignment Template ApproachIn the translation approach from Section 2.1,one disadvantage is that the contextual informa-tion is only taken into account by the languagemodel.
The single-word based lexicon modeldoes not consider the surrounding words.
Oneway to incorporate the context into the trans-lation model is to learn translations for wholeword groups instead of single words.
The keyelements of this translation approach (Och etal., 1999) are the alignment templates.
Theseare pairs of source and target language phraseswith an alignment within the phrases.The alignment templates are extracted fromthe bilingual training corpus.
The extraction al-gorithm (Och et al, 1999) uses the word align-ment information obtained from the models inSection 2.2.
Figure 2 shows an example of aword aligned sentence pair.
The word align-ment is represented with the black boxes.
Thefigure also includes some of the possible align-ment templates, represented as the larger, un-filled rectangles.
Note that the extraction algo-rithm would extract many more alignment tem-plates from this sentence pair.
In this example,the system input was the sequence of Chinesecharacters without any word segmentation.
Ascan be seen, a translation approach that is basedon phrases circumvents the problem of word seg-mentation to a certain degree.
This method willbe referred to as ?translation with no segmen-tation?
(see Section 5.2).theywillalsogotohangzhouforavisitFigure 2: Example of a word aligned sentencepair and some possible alignment templates.In the Chinese?English DARPA TIDES eval-uations in June 2002 and May 2003, carried outby NIST (NIST, 2003), the alignment templateapproach performed very well and was rankedamong the best translation systems.Further details on the alignment template ap-proach are described in (Och et al, 1999; Ochand Ney, 2002).3 Task and Corpus StatisticsIn Section 5.3, we will present results for aChinese?English translation task.
The domainof this task is news articles.
As bilingual train-ing data, we use a corpus composed of the En-glish translations of a Chinese Treebank.
Thiscorpus is provided by the Linguistic Data Con-sortium (LDC), catalog number LDC2002E17.In addition, we use a bilingual dictionary with10K Chinese word entries provided by StephanVogel (LDC, 2003b).Table 1 shows the corpus statistics of thistask.
We have calculated both the number ofwords and the number of characters in the cor-pus.
In average, a Chinese word is composedof 1.49 characters.
For each of the two lan-guages, there is a set of 20 special characters,such as digits, punctuation marks and symbolslike ?()%$...
?The training corpus will be used to train aword alignment and then extract the alignmenttemplates and the word-based lexicon.
The re-sulting translation system will be evaluated onthe test corpus.Table 1: Statistics of training and test corpus.For each of the two languages, there is a set of 20special characters, such as digits, punctuationmarks and symbols like ?()%$...
?Chinese EnglishTrain Sentences 4 172Characters 172 874 832 760Words 116 090 145 422Char.
Vocab.
3 419 + 20 26 + 20Word Vocab.
9 391 9 505Test Sentences 993Characters 42 100 167 101Words 28 247 26 2254 Segmentation Methods4.1 Conventional MethodThe commonly used segmentation method isbased on a segmentation tool and a monolingualChinese dictionary.
Typically, this dictionaryhas been produced beforehand and is indepen-dent of the Chinese text to be segmented.
Thedictionary contains Chinese words and their fre-quencies.
This information is used by the seg-mentation tool to find the word boundaries.
Inthe LDC method (see Section 5.2) we have usedthe dictionary and segmenter provided by theLDC.
More details can be found on the LDCweb pages (LDC, 2003a).
This segmenter isbased on two ideas: it prefers long words overshort words and it prefers high frequency wordsover low frequency words.4.2 Dictionary Learning from AlignmentsIn this section, we will describe our method oflearning a dictionary from a bilingual corpus.As mentioned before, the bilingual trainingcorpus listed in Section 3 is the only input to thesystem.
We firstly divide every Chinese charac-ters in the corpus by white spaces, then trainthe statistical translation models with this un-segmented Chinese text and its English trans-lation, details of the training method are de-scribed in Section 2.2.To extract Chinese words instead of phrasesas in Figure 2, we configure the training pa-rameters in GIZA++, the alignment is then re-stricted to a multi-source-single-target relation-ship, i.e.
one or more Chinese characters aretranslated to one English word.The result of this training procedure is analignment for each sentence pair.
Such an align-ment is represented as a binary matrix with J ?Ielements.An example is shown in Figure 3.
The un-segmented Chinese training sentence is plottedalong the horizontal axes and the correspondingEnglish sentence along the vertical axes.
Theblack boxes show the Viterbi alignment for thissentence pair.
Here, for example the first twoChinese characters are aligned to ?industry?,the next four characters are aligned to ?restruc-turing?.industryrestructuringmadevigorousprogressFigure 3: Example of an alignment withoutword segmentation.The central idea of our dictionary learningmethod is: a contiguous sequence of Chinesecharacters constitute a Chinese word, if theyare aligned to the same English word.
Usingthis idea and the bilingual corpus, we can au-tomatically generate a Chinese word dictionary.Table 2 shows the Chinese words that are ex-tracted from the alignment in Figure 3.Table 2: Word entries in Chinese dictionarylearned from the alignment in Figure 3.We extract Chinese words from all sentencepairs in the training corpus.
Therefore, it isstraightforward to collect word frequency statis-tics that are needed for the segmentation tool.Once, we have generated the dictionary, we canproduce a segmented Chinese corpus using themethod described in Section 4.1.
Then, weretrain the translation system using the seg-mented Chinese text.4.3 Word Length StatisticsIn this section, we present statistics of the wordlengths in the LDC dictionary as well as in theself-learned dictionary extracted from the align-ment.Table 3 shows the statistics of the wordlengths in the LDC dictionary as well as inthe learned dictionary.
For example, there are2 368 words consisting of a single character inlearned dictionary and 2 511 words in the LDCdictionary.
These single character words rep-resent 16.9% of the total number of entries inthe learned dictionary and 18.6% in the LDCdictionary.We see that in the LDC dictionary more than65% of the words consist of two characters andabout 30% of the words consist of a single char-acter or three or four characters.
Longer wordswith more than four characters constitute lessthan 1% of the dictionary.
In the learned dic-tionary, there are many more long words, about15%.
A subjective analysis showed that manyof these entries are either named entities oridiomatic expressions.
Often, these idiomaticexpressions should be segmented into shorterwords.
Therefore, we will investigate methodsto overcome this problem in the future.
Somesuggestions will be discussed in Section 6.Table 3: Statistics of word lengths in the LDCdictionary and in the learned dictionary.word LDC dictionary learned dictionarylength frequency [%] frequency [%]1 2 334 18.6 2 368 16.92 8 149 65.1 5 486 39.23 1 188 9.5 1 899 13.64 759 6.1 2 084 14.95 70 0.6 791 5.76 20 0.2 617 4.47 6 0.0 327 2.3?8 11 0.0 424 3.0total 12 527 100 13 996 1005 Translation Experiments5.1 Evaluation CriteriaSo far, in machine translation research, a sin-gle generally accepted criterion for the evalu-ation of the experimental results does not ex-ist.
We have used three automatic criteria.
Forthe test corpus, we have four references avail-able.
Hence, we compute all the following cri-teria with respect to multiple references.?
WER (word error rate):The WER is computed as the minimumnumber of substitution, insertion and dele-tion operations that have to be performedto convert the generated sentence into thereference sentence.?
PER (position-independent word errorrate):A shortcoming of the WER is that it re-quires a perfect word order.
The word or-der of an acceptable sentence can be dif-ferent from that of the target sentence, sothat the WER measure alone could be mis-leading.
The PER compares the words inthe two sentences ignoring the word order.?
BLEU score:This score measures the precision of un-igrams, bigrams, trigrams and fourgramswith respect to a reference translation witha penalty for too short sentences (Papineniet al, 2001).
The BLEU score measuresaccuracy, i.e.
large BLEU scores are bet-ter.5.2 Summary: Three TranslationMethodsIn the experiments, we compare the followingthree translation methods:?
Translation with no segmentation: EachChinese character is interpreted as a singleword.?
Translation with learned segmentation:It uses the self-learned dictionary.?
Translation with LDC segmentation:The predefined LDC dictionary is used.The core contribution of this paper is themethod we called ?translation with learned seg-mentation?, which consists of three steps:?
The input is a sequence of Chinese charac-ters without segmentation.
After the train-ing using GIZA++, we extract a mono-lingual Chinese dictionary from the align-ment.
This is discussed in Section 4.2, andan example is given in Figure 3 and Table 2.?
Using this learned dictionary, we segmentthe sequence of Chinese characters intowords.
In other words, the LDC methodis used, but the LDC dictionary is replacedby the learned dictionary (see Section 4.1).?
Based on this word segmentation, weperform another training using GIZA++.Then, after training the models IBM1,HMM and IBM4, we extract bilingual wordgroups, which are referred as alignmenttemplates.5.3 Evaluation ResultsThe evaluation is performed on the LDC corpusdescribed in Section 3.
The translation perfor-mance of the three systems is summarized inTable 4 for the three evaluation criteria WER,PER and BLEU.
We observe that the trans-lation quality with the learned segmentation issimilar to that with the LDC segmentation.
TheWER of the system with the learned segmenta-tion is somewhat better, but PER and BLEUare slightly worse.
We conclude that it is possi-ble to learn a domain-specific dictionary for Chi-nese word segmentation from a bilingual corpus.Therefore the translation system is independentof a predefined dictionary, which may be unsuit-able for a certain task.The translation system using no segmenta-tion performs slightly worse.
For example, forthe WER there is a loss of about 2% relativecompared to the system with the LDC segmen-tation.Table 4: Translation performance of differentsegmentation methods (all numbers in percent).method error rates accuracyWER PER BLEUno segment.
73.3 56.5 27.6learned segment.
70.4 54.6 29.1LDC segment.
71.9 54.4 29.25.4 Effect of Segmentation onTranslation ResultsIn this section, we present three examples of theeffect that segmentation may have on transla-tion quality.
For each of the three examples inFigure 4, we show the segmented Chinese sourcesentence using either the LDC dictionary or theself-learned dictionary, the corresponding trans-lation and the human reference translation.In the first example, the LDC dictionaryleads to a correct segmentation, whereas withthe learned dictionary the segmentation is erro-neous.
The second and third token should becombined (?Hong Kong?
), whereas the fifth to-ken should be separated (?stabilize in the longterm?).
In this case, the wrong segmentation ofthe Chinese source sentence does not result in awrong translation.
A possible reason is that thetranslation system is based on word groups andcan recover from these segmentation errors.In the second example, the segmentation withthe LDC dictionary produces at least one error.The second and third token should be combined(?this?).
It is possible to combine the seventhand eighth token to a single word because theeighth token shows only the tense.
The segmen-tation with the learned dictionary is correct.Here, the two segmentations result in differenttranslations.In the third example, both segmentations areincorrect and these segmentation errors affectthe translation results.
In the segmentationwith the LDC dictionary, the first Chinese char-acters should be segmented as a separate word.The second and third character and maybe eventhe fourth character should be combined to oneword.2 The fifth and sixth character should becombined to a single word.
In the segmentationwith the learned dictionary, the fifth and sixthtoken (seventh and eighth character) should becombined (?isolated?).
We see that this term ismissing in the translation.
Here, the segmenta-tion errors result in translation errors.6 Discussion and Future WorkWe have presented a new method for Chineseword segmentation.
It avoids the use of a pre-defined dictionary and instead learns a corpus-specific dictionary from the bilingual trainingcorpus.The idea is extracting a self-learned dictio-nary from the trained alignment models.
Thismethod has the advantage that the word entriesin the dictionary all occur in the training data,and its content is much closer to the trainingtext as a predefined dictionary, which can nevercover all possible word occurrences.
Here, if thecontent of the test corpus is closer to that of the2This is an example of an ambiguous segmentation.Example 1LDC dictionary:It will benefit Hong Kong's economy to prosperand stabilize in the long term.Learned dictionary:It will benefit Hong Kong's economy to prosperand stabilize in the long term.Reference:It will be benificial for the stability andprosperity of Hong Kong in the long run.Example 2LDC dictionary:but this meeting down or achieved certainprogress.Learned dictionary:however, this meeting straight down stillachieved certain progress.Reference:Neverless, this meeting has achieved someprogress.Example 3LDC dictionary:......... the unification of the world carried adjacentisolate of proof, ...Learned dictionary:......... in the world faced with a became anotherproof, ...Reference:... another proof that ... is facing isolation in theworld ...Figure 4: Translation examples using thelearned dictionary and the LDC dictionary.training corpus, the quality of the dictionary ishigher and the translation performance wouldbe better.The experiments showed that the transla-tion quality with the learned segmentation iscompetitive with the LDC segmentation.
Ad-ditionally, we have shown the feasibility of aChinese?English statistical machine translationsystem that works without any word segmenta-tion.
There is only a minor loss in translationperformance.
Further improvements could bepossible by tuning the system toward this spe-cific task.We expect that our method could be im-proved by considering the word length as dis-cussed in Section 4.3.
As shown in the wordlength statistics, long words with more thanfour characters occur only occasionally.
Most ofthem are named entity words, which are writ-ten in English in upper case.
Therefore, we canapply a simple rule: we accept a long Chineseword only if the corresponding English word isin upper case.
This should result in an improveddictionary.
An alternative way is to use theword length statistics in Table 3 as a prior dis-tribution.
In this case, long words would get apenalty, because their prior probability is low.Because the extraction of our dictionary isbased on bilingual information, it might be in-teresting to combine it with methods that usemonolingual information only.For Chinese?English, there is a large num-ber of bilingual corpora available at the LDC.Therefore using additional corpora, we can ex-pect to get an improved dictionary.ReferencesP.
F. Brown, J. Cocke, S. A. Della Pietra, V. J.Della Pietra, F. Jelinek, J. D. Lafferty, R. L.Mercer, and P. S. Roossin.
1990.
A statisti-cal approach to machine translation.
Compu-tational Linguistics, 16(2):79?85, June.P.
F. Brown, S. A. Della Pietra, V. J. DellaPietra, and R. L. Mercer.
1993.
The mathe-matics of statistical machine translation: Pa-rameter estimation.
Computational Linguis-tics, 19(2):263?311, June.LDC.
2003a.
LDC Chinese resources homepage.
http://www.ldc.upenn.edu/Projects/Chinese/LDC ch.htm.LDC.
2003b.
LDC resources home page.http://www.ldc.upenn.edu/Projects/TIDES/mt2004cn.htm.NIST.
2003.
Machine translation home page.http://www.nist.gov/speech/tests/mt/index.htm.F.
J. Och and H. Ney.
2002.
Discriminativetraining and maximum entropy models forstatistical machine translation.
In Proc.
ofthe 40th Annual Meeting of the Associationfor Computational Linguistics (ACL), pages295?302, Philadelphia, PA, July.F.
J. Och and H. Ney.
2003.
A systematic com-parison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51,March.F.
J. Och, C. Tillmann, and H. Ney.
1999.
Im-proved alignment models for statistical ma-chine translation.
In Proc.
of the Joint SIG-DAT Conf.
on Empirical Methods in Natu-ral Language Processing and Very Large Cor-pora, pages 20?28, University of Maryland,College Park, MD, June.D.
D. Palmer.
1997.
A trainable rule-basedalgorithm for word segmentation.
In Proc.of the 35th Annual Meeting of ACL and 8thConference of the European Chapter of ACL,pages 321?328, Madrid, Spain, August.K.
A. Papineni, S. Roukos, T. Ward, and W. J.Zhu.
2001.
Bleu: a method for automaticevaluation of machine translation.
Techni-cal Report RC22176 (W0109-022), IBM Re-search Division, Thomas J. Watson ResearchCenter, September.R.
W. Sproat and C. Shih.
1990.
A statisticalmethod for finding word boundaries in Chi-nese text.
Computer Processing of Chineseand Oriental Languages, 4:336?351.M.
Sun, D. Shen, and B. K. Tsou.
1998.
Chi-nese word segmentation without using lexi-con and hand-crafted training data.
In Proc.of the 36th Annual Meeting of ACL and17th Int.
Conf.
on Computational Linguistics(COLING-ACL 98), pages 1265?1271, Mon-treal, Quebec, Canada, August.S.
Vogel, H. Ney, and C. Tillmann.
1996.HMM-based word alignment in statisticaltranslation.
In COLING ?96: The 16th Int.Conf.
on Computational Linguistics, pages836?841, Copenhagen, Denmark, August.
