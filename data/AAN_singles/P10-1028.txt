Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 266?274,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsLearning Phrase-Based Spelling Error Modelsfrom Clickthrough DataXu Sun?Dept.
of Mathematical InformaticsUniversity of Tokyo, Tokyo, Japanxusun@mist.i.u-tokyo.ac.jpJianfeng GaoMicrosoft ResearchRedmond, WA, USAjfgao@microsoft.comDaniel MicolMicrosoft CorporationMunich, Germanydanielmi@microsoft.comChris QuirkMicrosoft ResearchRedmond, WA, USAchrisq@microsoft.com?
The work was done when Xu Sun was visiting Microsoft Research Redmond.AbstractThis paper explores the use of clickthrough datafor query spelling correction.
First, large amountsof query-correction pairs are derived by analyzingusers' query reformulation behavior encoded inthe clickthrough data.
Then, a phrase-based errormodel that accounts for the transformationprobability between multi-term phrases is trainedand integrated into a query speller system.
Expe-riments are carried out on a human-labeled dataset.
Results show that the system using thephrase-based error model outperforms signifi-cantly its baseline systems.1 IntroductionSearch queries present a particular challenge fortraditional spelling correction methods for threemain reasons (Ahmad and Kondrak, 2004).
First,spelling errors are more common in search queriesthan in regular written text: roughly 10-15% ofqueries contain misspelled terms (Cucerzan andBrill, 2004).
Second, most search queries consistof a few key words rather than grammatical sen-tences, making a grammar-based approach inap-propriate.
Most importantly, many queries con-tain search terms, such as proper nouns and names,which are not well established in the language.For example, Chen et al (2007) reported that16.5% of valid search terms do not occur in their200K-entry spelling lexicon.Therefore, recent research has focused on theuse of Web corpora and query logs, rather thanhuman-compiled lexicons, to infer knowledgeabout misspellings and word usage in searchqueries (e.g., Whitelaw et al, 2009).
Anotherimportant data source that would be useful for thispurpose is clickthrough data.
Although it iswell-known that clickthrough data contain richinformation about users' search behavior, e.g.,how a user (re-) formulates a query in order tofind the relevant document, there has been littleresearch on exploiting the data for the develop-ment of a query speller system.In this paper we present a novel method ofextracting large amounts of query-correction pairsfrom the clickthrough data.
These pairs, impli-citly judged by millions of users, are used to traina set of spelling error models.
Among thesemodels, the most effective one is a phrase-basederror model that captures the probability oftransforming one multi-term phrase into anothermulti-term phrase.
Comparing to traditional errormodels that account for transformation probabili-ties between single characters (Kernighan et al,1990) or sub-word strings (Brill and Moore,2000), the phrase-based model is more powerfulin that it captures some contextual information byretaining inter-term dependencies.
We show thatthis information is crucial to detect the correctionof a query term, because unlike in regular writtentext, any query word can be a valid search termand in many cases the only way for a spellersystem to make the judgment is to explore itsusage according to the contextual information.We conduct a set of experiments on a largedata set, consisting of human-labeled266query-correction pairs.
Results show that the errormodels learned from clickthrough data lead tosignificant improvements on the task of queryspelling correction.
In particular, the speller sys-tem incorporating a phrase-based error modelsignificantly outperforms its baseline systems.To the best of our knowledge, this is the firstextensive study of learning phase-based errormodels from clickthrough data for query spellingcorrection.
The rest of the paper is structured asfollows.
Section 2 reviews related work.
Section 3presents the way query-correction pairs are ex-tracted from the clickthrough data.
Section 4presents the baseline speller system used in thisstudy.
Section 5 describes in detail the phrase-based error model.
Section 6 presents the expe-riments.
Section 7 concludes the paper.2 Related WorkSpelling correction for regular written text is along standing research topic.
Previous researchescan be roughly grouped into two categories:correcting non-word errors and real-word errors.In non-word error spelling correction, anyword that is not found in a pre-compiled lexicon isconsidered to be misspelled.
Then, a list of lexicalwords that are similar to the misspelled word areproposed as candidate spelling corrections.
Mosttraditional systems use a manually tuned similar-ity function (e.g., edit distance function) to rankthe candidates, as reviewed by Kukich (1992).During the last two decades, statistical errormodels learned on training data (i.e.,query-correction pairs) have become increasinglypopular, and have proven more effective (Ker-nighan et al, 1990; Brill and Moore, 2000; Tou-tanova and Moore, 2002; Okazaki et al, 2008).Real-word spelling correction is also referredto as context sensitive spelling correction (CSSC).It tries to detect incorrect usages of a valid wordbased on its context, such as "peace" and "piece"in the context "a _ of cake".
A common strategy inCSSC is as follows.
First, a pre-defined confusionset is used to generate candidate corrections, thena  scoring model, such as a trigram languagemodel or na?ve Bayes classifier, is used to rank thecandidates according to their context (e.g.,Golding and Roth, 1996; Mangu and Brill, 1997;Church et al, 2007).When designed to handle regular written text,both CSSC and non-word error speller systemsrely on a pre-defined vocabulary (i.e., either alexicon or a confusion set).
However, in queryspelling correction, it is impossible to compilesuch a vocabulary, and the boundary between thenon-word and real-word errors is quite vague.Therefore, recent research on query spellingcorrection has focused on exploiting noisy Webdata and query logs to infer knowledge aboutmisspellings and word usage in search queries.Cucerzan and Brill (2004) discuss in detail thechallenges of query spelling correction, andsuggest the use of query logs.
Ahmad and Kon-drak (2005) propose a method of estimating anerror model from query logs using the EM algo-rithm.
Li et al (2006) extend the error model bycapturing word-level similarities learned fromquery logs.
Chen et al (2007) suggest using websearch results to improve spelling correction.Whitelaw et al (2009) present a query spellersystem in which both the error model and thelanguage model are trained using Web data.Compared to Web corpora and query logs,clickthrough data contain much richer informa-tion about users?
search behavior.
Although therehas been a lot of research on using clickthroughdata to improve Web document retrieval (e.g.,Joachims, 2002; Agichtein et al, 2006; Gao et al,2009), the data have not been fully explored forquery spelling correction.
This study tries to learnerror models from clickthrough data.
To ourknowledge, this is the first such attempt usingclickthrough data.Most of the speller systems reviewed above arebased on the framework of the source channelmodel.
Typically, a language model (sourcemodel) is used to capture contextual information,while an error model (channel model) is consi-dered to be context free in that it does not take intoaccount any contextual information in modelingword transformation probabilities.
In this studywe argue that it is beneficial to capture contextualinformation in the error model.
To this end, in-spired by the phrase-based statistical machinetranslation (SMT) systems (Koehn et al, 2003;Och and Ney, 2004), we propose a phrase-basederror model where we assume that query spellingcorrection is performed at the phrase level.In what follows, before presenting the phrase-based error model, we will first describe theclickthrough data and the query speller system weused in this study.3 Clickthrough Data and Spelling Cor-rectionThis section describes the way thequery-correction pairs are extracted from click-267through data.
Two types of clickthrough data areexplored in our experiment.The clickthrough data of the first type has beenwidely used in previous research and proved to beuseful for Web search (Joachims, 2002; Agichteinet al, 2006; Gao et al, 2009) and query refor-mulation (Wang and Zhai, 2008; Suzuki et al,2009).
We start with this same data with the hopeof achieving similar improvements in our task.The data consist of a set of query sessions thatwere extracted from one year of log files from acommercial Web search engine.
A query sessioncontains a query issued by a user and a ranked listof links (i.e., URLs) returned to that same useralong with records of which URLs were clicked.Following Suzuki et al (2009), we extractquery-correction pairs as follows.
First, we extractpairs of queries Q1 and Q2 such that (1) they areissued by the same user; (2) Q2 was issued within3 minutes of Q1; and (3) Q2 contained at least oneclicked URL in the result page while Q1 did notresult in any clicks.
We then scored each querypair (Q1, Q2) using the edit distance between Q1and Q2, and retained those with an edit distancescore lower than a pre-set threshold as querycorrection pairs.Unfortunately, we found in our experimentsthat the pairs extracted using the method are toonoisy for reliable error model training, even with avery tight threshold, and we did not see any sig-nificant improvement.
Therefore, in Section 6 wewill not report results using this dataset.The clickthrough data of the second type con-sists of a set of query reformulation sessionsextracted from 3 months of log files from acommercial Web browser.
A query reformulationsession contains a list of URLs that record userbehaviors that relate to the query reformulationfunctions, provided by a Web search engine.
Forexample, almost all commercial search enginesoffer the "did you mean" function, suggesting apossible alternate interpretation or spelling of auser-issued query.
Figure 1 shows a sample of thequery reformulation sessions that record the "didyou mean" sessions from three of the most pop-ular search engines.
These sessions encode thesame user behavior: A user first queries for"harrypotter sheme park", and then clicks on theresulting spelling suggestion "harry potter themepark".
In our experiments, we "reverse-engineer"the parameters from the URLs of these sessions,and deduce how each search engine encodes botha query and the fact that a user arrived at a URLby clicking on the spelling suggestion of the query?
an important indication that the spelling sug-gestion is desired.
From these three months ofquery reformulation sessions, we extracted about3 million query-correction pairs.
Compared to thepairs extracted from the clickthrough data of thefirst type (query sessions), this data set is muchcleaner because all these spelling corrections areactually clicked, and thus judged implicitly, bymany users.In addition to the "did you mean" function,recently some search engines have introduced twonew spelling suggestion functions.
One is the"auto-correction" function, where the searchengine is confident enough to automatically applythe spelling correction to the query and execute itto produce search results for the user.
The other isthe "split pane" result page, where one half por-tion of the search results are produced using theoriginal query, while the other half, usually vi-sually separate portion of results are producedusing the auto-corrected query.In neither of these functions does the user everreceive an opportunity to approve or disapproveof the correction.
Since our extraction approachfocuses on user-approved spelling suggestions,Google:http://www.google.com/search?hl=en&source=hp&q=harrypotter+sheme+park&aq=f&oq=&aqi=http://www.google.com/search?hl=en&ei=rnNAS8-oKsWe_AaB2eHlCA&sa=X&oi=spell&resnum=0&ct=result&cd=1&ved=0CA4QBSgA&q=harry+potter+theme+park&spell=1Yahoo:http://search.yahoo.com/search;_ylt=A0geu6ywckBL_XIBSDtXNyoA?p=harrypotter+sheme+park&fr2=sb-top&fr=yfp-t-701&sao=1http://search.yahoo.com/search?ei=UTF-8&fr=yfp-t-701&p=harry+potter+theme+park&SpellState=n-2672070758_q-tsI55N6srhZa.qORA0MuawAAAA%40%40&fr2=sp-topBing:http://www.bing.com/search?q=harrypotter+sheme+park&form=QBRE&qs=nhttp://www.bing.com/search?q=harry+potter+theme+park&FORM=SSREFigure 1.
A sample of query reformulation sessionsfrom three popular search engines.
These sessionsshow that a user first issues the query "harrypottersheme park", and then clicks on the resulting spellsuggestion "harry potter theme park".268we ignore the query reformulation sessions re-cording either of the two functions.
Although bydoing so we could miss some basic, obviousspelling corrections, our experiments show thatthe negative impact on error model training isnegligible.
One possible reason is that our base-line system, which does not use any error modellearned from the clickthrough data, is already ableto correct these basic, obvious spelling mistakes.Thus, including these data for training is unlikelyto bring any further improvement.We found that the error models trained usingthe data directly extracted from the query refor-mulation sessions suffer from the problem ofunderestimating the self-transformation probabil-ity of a query P(Q2=Q1|Q1), because we onlyincluded in the training data the pairs where thequery is different from the correction.
To dealwith this problem, we augmented the training databy including correctly spelled queries, i.e., thepairs (Q1, Q2) where Q1 = Q2.
First, we extracted aset of queries from the sessions where no spellsuggestion is presented or clicked on.
Second, weremoved from the set those queries that wererecognized as being auto-corrected by a searchengine.
We do so by running a sanity check of thequeries against our baseline spelling correctionsystem, which will be described in Section 6.
Ifthe system thinks an input query is misspelled, weassumed it was an obvious misspelling, and re-moved it.
The remaining queries were assumed tobe correctly spelled and were added to the trainingdata.4 The Baseline Speller SystemThe spelling correction problem is typicallyformulated under the framework of the sourcechannel model.
Given an input query ?
???.
.
.
?
?, we want to find the best spelling correc-tion ?
?
??.
.
.
??
among all candidate spellingcorrections:??
?
argmax????|??
(1)Applying Bayes' Rule and dropping the constantdenominator, we have??
?
argmax????|??????
(2)where the error model ???|??
models the trans-formation probability from C to Q, and the lan-guage model ????
models how likely C is acorrectly spelled query.The speller system used in our experiments isbased on a ranking model (or ranker), which canbe viewed as a generalization of the sourcechannel model.
The system consists of twocomponents: (1) a candidate generator, and (2) aranker.In candidate generation, an input query is firsttokenized into a sequence of terms.
Then we scanthe query from left to right, and each query term qis looked up in lexicon to generate a list of spel-ling suggestions c whose edit distance from q islower than a preset threshold.
The lexicon weused contains around 430,000 entries; these arehigh frequency query terms collected from oneyear of search query logs.
The lexicon is storedusing a trie-based data structure that allows effi-cient search for all terms within a maximum editdistance.The set of all the generated spelling sugges-tions is stored using a lattice data structure, whichis a compact representation of exponentially manypossible candidate spelling corrections.
We thenuse a decoder to identify the top twenty candi-dates from the lattice according to the sourcechannel model of Equation (2).
The languagemodel (the second factor) is a backoff bigrammodel trained on the tokenized form of one yearof query logs, using maximum likelihood estima-tion with absolute discounting smoothing.
Theerror model (the first factor) is approximated bythe edit distance function as?log???|??
?
EditDist?
?, ??
(3)The decoder uses a standard two-pass algorithmto generate 20-best candidates.
The first pass usesthe Viterbi algorithm to find the best C accordingto the model of Equations (2) and (3).
In thesecond pass, the A-Star algorithm is used to findthe 20-best corrections, using the Viterbi scorescomputed at each state in the first pass as heuris-tics.
Notice that we always include the input queryQ in the 20-best candidate list.The core of the second component of thespeller system is a ranker, which re-ranks the20-best candidate spelling corrections.
If the topC after re-ranking is different than the originalquery Q, the system returns C as the correction.Let f be a feature vector extracted from a queryand candidate spelling correction pair (Q, C).
Theranker maps f to a real value y that indicates howlikely C is a desired correction of Q.
For example,a linear ranker simply maps f to y with a learnedweight vector w such as ?
?
?
?
?, where w isoptimized w.r.t.
accuracy on a set of hu-269man-labeled (Q, C) pairs.
The features in f arearbitrary functions that map (Q, C) to a real value.Since we define the logarithm of the probabilitiesof the language model and the error model (i.e.,the edit distance function) as features, the rankercan be viewed as a more general framework,subsuming the source channel model as a specialcase.
In our experiments we used 96 features and anon-linear model, implemented as a two-layerneural net, though the details of the ranker and thefeatures are beyond the scope of this paper.5 A Phrase-Based Error ModelThe goal of the phrase-based error model is totransform a correctly spelled query C into amisspelled query Q.
Rather than replacing singlewords in isolation, this model replaces sequencesof words with sequences of words, thus incorpo-rating contextual information.
For instance, wemight learn that ?theme part?
can be replaced by?theme park?
with relatively high probability,even though ?part?
is not a misspelled word.
Weassume the following generative story: first thecorrectly spelled query C is broken into Knon-empty word sequences c1, ?, ck, then each isreplaced with a new non-empty word sequence q1,?, qk, and finally these phrases are permuted andconcatenated to form the misspelled Q.
Here, cand q denote consecutive sequences of words.To formalize this generative process, let Sdenote the segmentation of C into K phrases c1?cK,and let T denote the K replacement phrasesq1?qK ?
we refer to these (ci, qi) pairs asbi-phrases.
Finally, let M denote a permutation ofK elements representing the final reordering step.Figure 2 demonstrates the generative procedure.Next let us place a probability distribution overrewrite pairs.
Let B(C, Q) denote the set of S, T, Mtriples that transform C into Q.
If we assume auniform probability over segmentations, then thephrase-based probability can be defined as:???|??
?
?
??
?|?, ??
?
??
?|?, ?, ????,?,??????,??
(4)As is common practice in SMT, we use themaximum approximation to the sum:???|??
?
max??,?,??????,????
?|?, ??
?
??
?|?, ?, ??
(5)5.1 Forced AlignmentsAlthough we have defined a generative model fortransforming queries, our goal is not to proposenew queries, but rather to provide scores overexisting Q and C pairs which act as features forthe ranker.
Furthermore, the word-level align-ments between Q and C can most often be iden-tified with little ambiguity.
Thus we restrict ourattention to those phrase transformations consis-tent with a good word-level alignment.Let J be the length of Q, L be the length of C,and A = a1, ?, aJ be a hidden variablerepresenting the word alignment.
Each ai takes ona value ranging from 1 to L indicating its corres-ponding word position in C, or 0 if the ith word inQ is unaligned.
The cost of assigning k to ai isequal to the Levenshtein edit distance (Levensh-tein, 1966) between the ith word in Q and the kthword in C, and the cost of assigning 0 to ai is equalto the length of the ith word in Q.
We can deter-mine the least cost alignment A* between Q and Cefficiently using the A-star algorithm.When scoring a given candidate pair, we fur-ther restrict our attention to those S, T, M triplesthat are consistent with the word alignment, whichwe denote as B(C, Q, A*).
Here, consistency re-quires that if two words are aligned in A*, thenthey must appear in the same bi-phrase (ci, qi).Once the word alignment is fixed, the final per-mutation is uniquely determined, so we can safelydiscard that factor.
Thus we have:???|??
?
max??,?,??????,?,?????
?|?, ??
(6)For the sole remaining factor P(T|C, S), wemake the assumption that a segmented query T =q1?
qK is generated from left to right by trans-forming each phrase c1?cK independently:C: ?disney theme park?
correct queryS: [?disney?, ?theme park?]
segmentationT: [?disnee?, ?theme part?]
translationM: (1 ?
2, 2?
1) permutationQ: ?theme part disnee?
misspelled queryFigure 2: Example demonstrating the generativeprocedure behind the phrase-based error model.270??
?|?, ??
?
?
????|???????
, (7)where ????|???
is a phrase transformationprobability, the estimation of which will be de-scribed in Section 5.2.To find the maximum probability assignmentefficiently, we can use a dynamic programmingapproach, somewhat similar to the monotonedecoding algorithm described in Och (2002).Here, though, both the input and the output wordsequences are specified as the input to the algo-rithm, as is the word alignment.
We define thequantity ??
to be the probability of the most likelysequence of bi-phrases that produce the first jterms of Q and are consistent with the wordalignment and C. It can be calculated using thefollowing algorithm:1.
Initialization:??
?
1 (8)2.
Induction:??
?
max????,??????????????????????
(9)3.
Total:???|??
?
??
(10)The pseudo-code of the above algorithm isshown in Figure 3.
After generating Q from left toright according to Equations (8) to (10), we recordat each possible bi-phrase boundary its maximumprobability, and we obtain the total probability atthe end-position of Q.
Then, by back-tracking themost probable bi-phrase boundaries, we obtain B*.The algorithm takes a complexity of O(KL2),where K is the total number of word alignments inA* which does not contain empty words, and L isthe maximum length of a bi-phrase, which is ahyper-parameter of the algorithm.
Notice thatwhen we set L=1, the phrase-based error model isreduced to a word-based error model which as-sumes that words are transformed independentlyfrom C to Q, without taking into account anycontextual information.5.2 Model EstimationWe follow a method commonly used in SMT(Koehn et al, 2003) to extract bi-phrases andestimate their replacement probabilities.
Fromeach query-correction pair with its word align-ment (Q, C, A*), all bi-phrases consistent with theword alignment are identified.
Consistency hereimplies two things.
First, there must be at leastone aligned word pair in the bi-phrase.
Second,there must not be any word alignments fromwords inside the bi-phrase to words outside thebi-phrase.
That is, we do not extract a phrase pairif there is an alignment from within the phrasepair to outside the phrase pair.
The toy exampleshown in Figure 4 illustrates the bilingual phraseswe can generate by this process.After gathering all such bi-phrases from thefull training data, we can estimate conditionalrelative frequency estimates without smoothing.For example, the phrase transformation probabil-ity ???|??
in Equation (7) can be estimated ap-proximately asInput: biPhraseLattice ?PL?
with length = K & height= L;Initialization: biPhrase.maxProb = 0;for (x = 0; x <= K ?
1; x++)for (y = 1; y <= L; y++)for (yPre = 1; yPre <= L; yPre++){xPre = x ?
y;biPhrasePre = PL.get(xPre, yPre);biPhrase = PL.get(x, y);if (!biPhrasePre || !biPhrase)continue;probIncrs = PL.getProbIncrease(biPhrasePre,biPhrase);maxProbPre = biPhrasePre.maxProb;totalProb = probIncrs + maxProbPre;if  (totalProb > biPhrase.maxProb){biPhrase.maxProb = totalProb;biPhrase.yPre = yPre;}}Result: record at each bi-phrase boundary its maxi-mum probability (biPhrase.maxProb) and optimalback-tracking biPhrases (biPhrase.yPre).Figure 3: The dynamic programming algorithm forViterbi bi-phrase segmentation.A B C D E F  a Aa #       adc ABCDd    #    d Dc   #     dc CDf      #  dcf CDEFc Cf FFigure 4: Toy example of (left) a word alignmentbetween two strings "adcf" and "ABCDEF"; and (right)the bi-phrases containing up to four words that areconsistent with the word alignment.271???|??
???
?, ???
??
?, ?????
(11)where ??
?, ??
is the number of times that c isaligned to q in training data.
These estimates areuseful for contextual lexical selection with suffi-cient training data, but can be subject to datasparsity issues.An alternate translation probability estimatenot subject to data sparsity issues is the so-calledlexical weight estimate (Koehn et al, 2003).Assume we have a word translation distribution???|??
(defined over individual words, notphrases), and a word alignment A between q and c;here, the word alignment contains (i, j) pairs,where  ?
?
1. .
|?| and ?
?
0. .
|?|, with 0 indicat-ing an inserted word.
Then we can use the fol-lowing estimate:???
?|?, ??
??1|??|?
?, ??
?
??|?
???
?| ??????,????|?|???
(12)We assume that for every position in q, there iseither a single alignment to 0, or multiple align-ments to non-zero positions in c. In effect, thiscomputes a product of per-word translation scores;the per-word scores are averages of all the trans-lations for the alignment links of that word.
Weestimate the word translation probabilities usingcounts from the word aligned corpus: ???|??
????,???
???,?????.
Here ??
?, ??
is the number of times thatthe words (not phrases as in Equation 11) c and qare aligned in the training data.
These word basedscores of bi-phrases, though not as effective incontextual selection, are more robust to noise andsparsity.Throughout this section, we have approachedthis model in a noisy channel approach, findingprobabilities of the misspelled query given thecorrected query.
However, the method can be runin both directions, and in practice SMT systemsbenefit from also including the direct probabilityof the corrected query given this misspelled query(Och, 2002).5.3 Phrase-Based Error Model FeaturesTo use the phrase-based error model for spellingcorrection, we derive five features and integratethem into the ranker-based query speller system,described in Section 4.
These features are asfollows.?
Two phrase transformation features:These are the phrase transformation scoresbased on relative frequency estimates in twodirections.
In the correction-to-query direc-tion, we define the feature as  ????
?, ?, ??
?log ???|??
, where ???|??
is computed byEquations (8) to (10), and ???????
is the rel-ative frequency estimate of Equation (11).?
Two lexical weight features: These are thephrase transformation scores based on thelexical weighting models in two directions.For example, in the correction-to-query di-rection, we define the featureas ????
?, ?, ??
?
log ???|?
?, where ???|?
?is computed by Equations (8) to (10), and thephrase transformation probability is thecomputed as lexical weight according to Eq-uation (12).?
Unaligned word penalty feature: the featureis defined as the ratio between the number ofunaligned query words and the total numberof query words.6 ExperimentsWe evaluate the spelling error models on a largescale real world data set containing 24,172 queriessampled from one year?s worth of query logs froma commercial search engine.
The spelling of eachquery is judged and corrected by four annotators.We divided the data set into training and testdata sets.
The two data sets do not overlap.
Thetraining data contains 8,515 query-correctionpairs, among which 1,743 queries are misspelled(i.e., in these pairs, the corrections are differentfrom the queries).
The test data contains 15,657query-correction pairs, among which 2,960 que-ries are misspelled.
The average length of queriesin the training and test data is 2.7 words.The speller systems we developed in this studyare evaluated using the following three metrics.?
Accuracy: The number of correct outputsgenerated by the system divided by the totalnumber of queries in the test set.?
Precision: The number of correct spellingcorrections for misspelled queries generatedby the system divided by the total number ofcorrections generated by the system.?
Recall: The number of correct spelling cor-rections for misspelled queries generated bythe system divided by the total number ofmisspelled queries in the test set.We also perform a significance test, i.e., a t-testwith a significance level of 0.05.
A significantdifference should be read as significant at the 95%level.272In our experiments, all the speller systems areranker-based.
In most cases, other than the base-line system (a linear neural net), the ranker is atwo-layer neural net with 5 hidden nodes.
The freeparameters of the neural net are trained to optim-ize accuracy on the training data using the backpropagation algorithm, running for 500 iterationswith a very small learning rate (0.1) to avoidoverfitting.
We did not adjust the neural netstructure (e.g., the number of hidden nodes) orany training parameters for different speller sys-tems.
Neither did we try to seek the best tradeoffbetween precision and recall.
Since all the sys-tems are optimized for accuracy, we use accuracyas the primary metric for comparison.Table 1 summarizes the main spelling correc-tion results.
Row 1 is the baseline speller systemwhere the source-channel model of Equations (2)and (3) is used.
In our implementation, we use alinear ranker with only two features, derivedrespectively from the language model and theerror model models.
The error model is based onthe edit distance function.
Row 2 is the rank-er-based spelling system that uses all 96 rankingfeatures, as described in Section 4.
Note that thesystem uses the features derived from two errormodels.
One is the edit distance model used forcandidate generation.
The other is a phoneticmodel that measures the edit distance between themetaphones (Philips, 1990) of a query word andits aligned correction word.
Row 3 is the samesystem as Row 2, with an additional set of featuresderived from a word-based error model.
Thismodel is a special case of the phrase-based errormodel described in Section 5 with the maximumphrase length set to one.
Row 4 is the system thatuses the additional 5 features derived from thephrase-based error models with a maximumbi-phrase length of 3.In phrase based error model, L is the maxi-mum length of a bi-phrase (Figure 3).
This valueis important for the spelling performance.
Weperform experiments to study the impact of L;the results are displayed in Table 2.
Moreover,since we proposed to use clickthrough data forspelling correction, it is interesting to study theimpact on spelling performance from the size ofclickthrough data used for training.
We variedthe size of clickthrough data and the experi-mental results are presented in Table 3.The results show first and foremost that theranker-based system significantly outperformsthe spelling system based solely on thesource-channel model, largely due to the richerset of features used (Row 1 vs. Row 2).
Second,the error model learned from clickthrough dataleads to significant improvements (Rows 3 and 4vs.
Row 2).
The phrase-based error model, due toits capability of capturing contextual information,outperforms the word-based model with a smallbut statistically significant margin (Row 4 vs.Row 3), though using phrases longer (L > 3) doesnot lead to further significant improvement (Rows6 and 7 vs.
Rows 8 and 9).
Finally, using moreclickthrough data leads to significant improve-ment (Row 13 vs.
Rows 10 to 12).
The benefitdoes not appear to have peaked ?
further im-provements are likely given a larger data set.7 ConclusionsUnlike conventional textual documents, mostsearch queries consist of a sequence of key words,many of which are valid search terms but are notstored in any compiled lexicon.
This presents achallenge to any speller system that is based on adictionary.This paper extends the recent research on usingWeb data and query logs for query spelling cor-rection in two aspects.
First, we show that a largeamount of training data (i.e.
query-correctionpairs) can be extracted from clickthrough data,focusing on query reformulation sessions.
Theresulting data are very clean and effective forerror model training.
Second, we argue that it iscritical to capture contextual information forquery spelling correction.
To this end, we propose# System Accuracy Precision Recall1 Source-channel 0.8526 0.7213 0.35862 Ranker-based 0.8904 0.7414 0.49643 Word model 0.8994 0.7709 0.54134 Phrase model (L=3) 0.9043 0.7814 0.5732Table 1.
Summary of spelling correction results.# System Accuracy Precision Recall5 Phrase model (L=1) 0.8994 0.7709 0.54136 Phrase model (L=2) 0.9014 0.7795 0.56057 Phrase model (L=3) 0.9043 0.7814 0.57328 Phrase model (L=5) 0.9035 0.7834 0.56989 Phrase model (L=8) 0.9033 0.7821 0.5713Table 2.
Variations of spelling performance as a func-tion of phrase length.# System Accuracy Precision Recall10 L=3; 0 month data 0.8904 0.7414 0.496411 L=3; 0.5 month data 0.8959 0.7701 0.523412 L=3; 1.5 month data 0.9023 0.7787 0.566713 L=3; 3 month data 0.9043 0.7814 0.5732Table 3.
Variations of spelling performance as a func-tion of the size of clickthrough data used for training.273a new phrase-based error model, which leads tosignificant improvement in our spelling correc-tion experiments.There is additional potentially useful informa-tion that can be exploited in this type of model.For example, in future work we plan to investigatethe combination of the clickthrough data collectedfrom a Web browser with the noisy but largequery sessions collected from a commercialsearch engine.AcknowledgmentsThe authors would like to thank Andreas Bode,Mei Li, Chenyu Yan and Galen Andrew for thevery helpful discussions and collaboration.ReferencesAgichtein, E., Brill, E. and Dumais, S. 2006.
Im-proving web search ranking by incorporating userbehavior information.
In SIGIR, pp.
19-26.Ahmad, F., and Kondrak, G. 2005.
Learning aspelling error model from search query logs.
InHLT-EMNLP, pp 955-962.Brill, E., and Moore, R. C. 2000.
An improved errormodel for noisy channel spelling correction.
InACL, pp.
286-293.Chen, Q., Li, M., and Zhou, M. 2007.
Improvingquery spelling correction using web search results.In EMNLP-CoNLL, pp.
181-189.Church, K., Hard, T., and Gao, J.
2007.
Compress-ing trigram language models with Golomb cod-ing.
In EMNLP-CoNLL, pp.
199-207.Cucerzan, S., and Brill, E. 2004.
Spelling correctionas an iterative process that exploits the collectiveknowledge of web users.
In EMNLP, pp.
293-300.Gao, J., Yuan, W., Li, X., Deng, K., and Nie, J-Y.2009.
Smoothing clickthrough data for websearch ranking.
In SIGIR.Golding, A. R., and Roth, D. 1996.
Applying win-now to context-sensitive spelling correction.
InICML, pp.
182-190.Joachims, T. 2002.
Optimizing search engines usingclickthrough data.
In SIGKDD, pp.
133-142.Kernighan, M. D., Church, K. W., and Gale, W. A.1990.
A spelling correction program based on anoisy channel model.
In COLING, pp.
205-210.Koehn, P., Och, F., and Marcu, D. 2003.
Statisticalphrase-based translation.
In HLT/NAACL, pp.127-133.Kukich, K. 1992.
Techniques for automaticallycorrecting words in text.
ACM Computing Sur-veys.
24(4): 377-439.Levenshtein, V. I.
1966.
Binary codes capable ofcorrecting deletions, insertions and reversals.
So-viet Physics Doklady, 10(8):707-710.Li, M., Zhu, M., Zhang, Y., and Zhou, M. 2006.Exploring distributional similarity based modelsfor query spelling correction.
In ACL, pp.1025-1032.Mangu, L., and Brill, E. 1997.
Automatic rule ac-quisition for spelling correction.
In ICML, pp.187-194.Och, F. 2002.
Statistical machine translation: fromsingle-word models to alignment templates.
PhDthesis, RWTH Aachen.Och, F., and Ney, H. 2004.
The alignment templateapproach to statistical machine translation.Computational Linguistics, 30(4): 417-449.Okazaki, N., Tsuruoka, Y., Ananiadou, S., andTsujii, J.
2008.
A discriminative candidate gene-rator for string transformations.
In EMNLP, pp.447-456.Philips, L. 1990.
Hanging on the metaphone.Computer Language Magazine, 7(12):38-44.Suzuki, H., Li, X., and Gao, J.
2009.
Discovery ofterm variation in Japanese web search queries.
InEMNLP.Toutanova, K., and Moore, R. 2002.
Pronunciationmodeling for improved spelling correction.
InACL, pp.
144-151.Wang, X., and Zhai, C. 2008.
Mining term associa-tion patterns from search logs for effective queryreformulation.
In CIKM, pp.
479-488.Whitelaw, C., Hutchinson, B., Chung, G. Y., andEllis, G. 2009.
Using the web for language inde-pendent spellchecking and autocorrection.
InEMNLP, pp.
890-899.274
