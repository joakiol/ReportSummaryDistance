Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 489?493,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsRe-embedding WordsIgor LabutovCornell Universityiil4@cornell.eduHod LipsonCornell Universityhod.lipson@cornell.eduAbstractWe present a fast method for re-purposingexisting semantic word vectors to improveperformance in a supervised task.
Re-cently, with an increase in computing re-sources, it became possible to learn richword embeddings from massive amountsof unlabeled data.
However, some meth-ods take days or weeks to learn good em-beddings, and some are notoriously dif-ficult to train.
We propose a methodthat takes as input an existing embedding,some labeled data, and produces an em-bedding in the same space, but with a bet-ter predictive performance in the super-vised task.
We show improvement on thetask of sentiment classification with re-spect to several baselines, and observe thatthe approach is most useful when the train-ing set is sufficiently small.1 IntroductionIncorporating the vector representation of a wordas a feature, has recently been shown to benefitperformance in several standard NLP tasks suchas language modeling (Bengio et al, 2003; Mnihand Hinton, 2009), POS-tagging and NER (Col-lobert et al, 2011), parsing (Socher et al, 2010),as well as in sentiment and subjectivity analysistasks (Maas et al, 2011; Yessenalina and Cardie,2011).
Real-valued word vectors mitigate sparsityby ?smoothing?
relevant semantic insight gainedduring the unsupervised training over the rare andunseen terms in the training data.
To be effective,these word-representations ?
and the process bywhich they are assigned to the words (i.e.
embed-ding) ?
should capture the semantics relevant tothe task.
We might, for example, consider dra-matic (term X) and pleasant (term Y) to correlatewith a review of a good movie (task A), while find-ing them of opposite polarity in the context of adating profile (task B).
Consequently, good vectorsfor X and Y should yield an inner product close to1 in the context of task A, and ?1 in the contextof task B.
Moreover, we may already have on ourhands embeddings for X and Y obtained from yetanother (possibly unsupervised) task (C), in whichX and Y are, for example, orthogonal.
If the em-beddings for task C happen to be learned from amuch larger dataset, it would make sense to re-use task C embeddings, but adapt them for taskA and/or task B.
We will refer to task C and itsembeddings as the source task and the source em-beddings, and task A/B, and its embeddings as thetarget task and the target embeddings.Traditionally, we would learn the embeddingsfor the target task jointly with whatever unla-beled data we may have, in an instance of semi-supervised learning, and/or we may leverage la-bels from multiple other related tasks in a multi-task approach.
Both methods have been appliedsuccessfully (Collobert and Weston, 2008) to learntask-specific embeddings.
But while joint train-ing is highly effective, a downside is that a largeamount of data (and processing time) is requireda-priori.
In the case of deep neural embeddings,for example, training time can number in days.
Onthe other hand, learned embeddings are becomingmore abundant, as much research and computingeffort is being invested in learning word represen-tations using large-scale deep architectures trainedon web-scale corpora.
Many of said embeddingsare published and can be harnessed in their rawform as additional features in a number of super-vised tasks (Turian et al, 2010).
It would, thus, beadvantageous to learn a task-specific embeddingdirectly from another (source) embedding.In this paper we propose a fast method for re-embedding words from a source embedding S to atarget embedding T by performing unconstrainedoptimization of a convex objective.
Our objec-tive is a linear combination of the dataset?s log-489likelihood under the target embedding and theFrobenius norm of the distortion matrix ?
a ma-trix of component-wise differences between thetarget and the source embeddings.
The latter actsas a regularizer that penalizes the Euclidean dis-tance between the source and target embeddings.The method is much faster than joint training andyields competitive results with several baselines.2 Related WorkThe most relevant to our contribution is the workby Maas et.al (2011), where word vectors arelearned specifically for sentiment classification.Embeddings are learned in a semi-supervisedfashion, and the components of the embedding aregiven an explicit probabilistic interpretation.
Theirmethod produces state-of-the-art results, however,optimization is non-convex and takes approxi-mately 10 hours on 10 machines1.
Naturally, ourmethod is significantly faster because it operates inthe space of an existing embedding, and does notrequire a large amount of training data a-priori.Collobert and Weston (2008), in their seminalpaper on deep architectures for NLP, propose amultilayer neural network for learning word em-beddings.
Training of the model, depending onthe task, is reported to be between an hour andthree days.
While the obtained embeddings canbe ?fine-tuned?
using backpropogation for a su-pervised task, like all multilayer neural networktraining, optimization is non-convex, and is sensi-tive to the dimensionality of the hidden layers.In machine learning literature, joint semi-supervised embedding takes form in methods suchas the LaplacianSVM (LapSVM) (Belkin et al,2006) and Label Propogation (Zhu and Ghahra-mani, 2002), to which our approach is related.These methods combine a discriminative learnerwith a non-linear manifold learning technique in ajoint objective, and apply it to a combined set oflabeled and unlabeled examples to improve per-formance in a supervised task.
(Weston et al,2012) take it further by applying this idea to deep-learning architectures.
Our method is different inthat the (potentially) massive amount of unlabeleddata is not required a-priori, but only the resultantembedding.1as reported by author in private correspondence.
Theruntime can be improved using recently introduced tech-niques, see (Collobert et al, 2011)3 ApproachLet ?S ,?T ?
R|V |?K be the source and targetembedding matrices respectively, where K is thedimension of the word vector space, identical inthe source and target embeddings, and V is the setof embedded words, given by VS ?
VT .
Followingthis notation, ?i ?
the ith row in ?
?
is the respec-tive vector representation of wordwi ?
V .
In whatfollows, we first introduce our supervised objec-tive, then combine it with the proposed regularizerand learn the target embedding ?T by optimizingthe resulting joint convex objective.3.1 Supervised modelWe model each document dj ?
D (a movie re-view, for example) as a collection of words wij(i.i.d samples).
We assign a sentiment label sj ?
{0, 1} to each document (converting the star ratingto a binary label), and seek to optimize the con-ditional likelihood of the labels (sj)j?
{1,...,|D|},given the embeddings and the documents:p(s1, ..., s|D||D; ?T ) =?dj?D?wi?djp(sj |wi; ?T )where p(sj = 1|wi,?T ) is the probability of as-signing a positive label to document j, given thatwi ?
dj .
As in (Maas et al, 2011), we use logisticregression to model the conditional likelihood:p(sj = 1|wi; ?T ) =11 + exp(?
?T?i)where ?
?
RK+1 is a regression parameter vectorwith an included bias component.
Maximizing thelog-likelihood directly (for ?
and ?T ), especiallyon small datasets, will result in severe overfitting,as learning will tend to commit neutral words toeither polarity.
Classical regularization will mit-igate this effect, but can be improved further byintroducing an external embedding in the regular-izer.
In what follows, we describe re-embeddingregularization?
employing existing (source) em-beddings to bias word vector learning.3.2 Re-embedding regularizationTo leverage rich semantic word representations,we employ an external source embedding and in-corporate it in the regularizer on the supervisedobjective.
We use Euclidean distance between thesource and the target embeddings as the regular-490ization loss.
Combined with the supervised objec-tive, the resulting log-likelihood becomes:argmax?,?T?dj?D?wi?djlog p(sj |wi; ?T )?
?||?
?||2F (1)where ??
= ?T?
?S , ||?||F is a Frobenius norm,and ?
is a trade-off parameter.
There are almostno restrictions on ?S , except that it must matchthe desired target vector space dimension K. Theobjective is convex in ?
and ?T , thus, yielding aunique target re-embedding.
We employ L-BFGSalgorithm (Liu and Nocedal, 1989) to find the op-timal target embedding.3.3 Classification with word vectorsTo classify documents, re-embedded word vectorscan now be used to construct a document-levelfeature vector for a supervised learning algorithmof choice.
Perhaps the most direct approach is tocompute a weighted linear combination of the em-beddings for words that appear in the documentto be classified, as done in (Maas et al, 2011)and (Blacoe and Lapata, 2012).
We use the docu-ment?s binary bag-of-words vector vj , and com-pute the document?s vector space representationthrough the matrix-vector product ?T vj .
The re-sulting K + 1-dimensional vector is then cosine-normalized and used as a feature vector to repre-sent the document dj .4 ExperimentsData: For our experiments, we employ a large,recently introduced IMDB movie review dataset(Maas et al, 2011), in place of the smaller datasetintroduced in (Pang and Lee, 2004) more com-monly used for sentiment analysis.
The dataset(50,000 reviews) is split evenly between trainingand testing sets, each containing a balanced set ofhighly polar (?
7 and?
4 stars out of 10) reviews.Source embeddings: We employ three externalembeddings (obtained from (Turian et al, 2010))induced using the following models: 1) hierarchi-cal log-bilinear model (HLBL) (Mnih and Hinton,2009) and two neural network-based models ?
2)Collobert and Weston?s (C&W) deep-learning ar-chitecture, and 3) Huang et.al?s polysemous neurallanguage model (HUANG) (Huang et al, 2012).C&W and HLBL were induced using a 37M-wordnewswire text (Reuters Corpus 1).
We also inducea Latent Semantic Analysis (LSA) based embed-ding from the subset of the English project Guten-berg collection of approximately 100M words.
Nopre-processing (stemming or stopword removal),beyond case-normalization is performed in eitherthe external or LSA-based embedding.
For HLBL,C&W and LSA embeddings, we use two variantsof different dimensionality: 50 and 200.
In total,we obtain seven source embeddings: HLBL-50,HLBL-200, C&W-50, C&W-200, HUANG-50, LSA-50, LSA-200.Baselines: We generate two baseline embeddings?
NULL and RANDOM.
NULL is a set of zerovectors, and RANDOM is a set of uniformlydistributed random vectors with a unit L2-norm.NULL and RANDOM are treated as source vec-tors and re-embedded in the same way.
TheNULL baseline is equivalent to regularizing onthe target embedding without the source embed-ding.
As additional baselines, we use each of the7 source embeddings directly as a target withoutre-embedding.Training: For each source embedding matrix ?S ,we compute the optimal target embedding matrix?T by maximizing Equation 1 using the L-BFGSalgorithm.
20 % of the training set (5,000 docu-ments) is withheld for parameter (?)
tuning.
Weuse LIBLINEAR (Fan et al, 2008) logistic re-gression module to classify document-level em-beddings (computed from the ?T vj matrix-vectorproduct).
Training (re-embedding and documentclassification) on 20,000 documents and a 16,000word vocabulary takes approximately 5 secondson a 3.0 GHz quad-core machine.5 Results and DiscussionThe main observation from the results is that ourmethod improves performance for smaller trainingsets (?
5000 examples).
The reason for the perfor-mance boost is expected ?
classical regularizationof the supervised objective reduces overfitting.However, comparing to the NULL and RAN-DOM baseline embeddings, the performance isimproved noticeably (note that a percent differ-ence of 0.1 corresponds to 20 correctly classi-fied reviews) for word vectors that incorporate thesource embedding in the regularizer, than thosethat do not (NULL), and those that are based onthe random source embedding (RANDOM).
Wehypothesize that the external embeddings, gen-erated from a significantly larger dataset help?smooth?
the word-vectors learned from a smalllabeled dataset alne.
Further observations in-clude:491Features Number of training examples+ Bag-of-words features.5K 5K 20K .5K 5K 20KA.
Re-embeddings (our method)HLBL-50 74.01 79.89 80.94 78.90 84.88 85.42HLBL-200 74.33 80.14 81.05 79.22 85.05 85.95C&W-50 74.52 79.81 80.48 78.92 84.89 85.87C&W-200 74.80 80.25 81.15 79.34 85.28 86.15HUANG-50 74.29 79.90 79.91 79.03 84.89 85.61LSA-50 72.83 79.67 80.67 78.71 83.44 84.73LSA-200 73.70 80.03 80.91 79.12 84.83 85.31B.
BaselinesRANDOM-50 w/ re-embedding 72.90 79.12 80.21 78.29 84.01 84.87RANDOM-200 w/ re-embedding 72.93 79.20 80.29 78.31 84.08 84.91NULL w/ re-embedding 72.92 79.18 80.24 78.29 84.10 84.98HLBL-200 w/o re-embedding 67.88 72.60 73.10 79.02 83.83 85.83C&W-200 w/o re-embedding 68.17 72.72 73.38 79.30 85.15 86.15HUANG-50 w/o re-embedding 67.89 72.63 73.12 79.13 84.94 85.99C.
Related methodsJoint training (Maas, 2011) ?
?
84.65 ?
?
88.90Bag of Words SVM ?
?
?
79.17 84.97 86.14Table 1: Classification accuracy for the sentiment task (IMDBmovie review dataset (Maas et al, 2011)).
Subtable A comparesperformance of the re-embedded vocabulary, induced from agiven source embedding.
Subtable B contains a set of base-lines: X-w/o re-embedding indicates using a source embeddingX directly without re-embedding.BORINGsource: lethal, lifestyles, masterpiece .
.
.target: idiotic, soft-core, gimmickyBADsource: past, developing, lesser, .
.
.target: ill, madonna, low, .
.
.DEPRESSINGsource: versa, redemption, townsfolk .
.
.target: hate, pressured, unanswered ,BRILLIANTsource: high-quality, obsession, hate .
.
.target: all-out, bold, smiling .
.
.Table 2: A representative set of words from the 20 closest-ranked (cosine-distance) words to (boring, bad, depressing,brilliant) extracted from the source and target (C&W-200)embeddings.
Source embeddings give higher rank to wordsthat are related, but not necessarily indicative of sentiment,e.g.
brilliant and obsession.
Target words tend to be tunedand ranked higher based on movie-sentiment-based rela-tions.Training set size: We note that with a sufficientnumber of training instances for each word in thetest set, additional knowledge from an externalembedding does little to improve performance.Source embeddings: We find C&W embeddingsto perform best for the task of sentiment classi-fication.
These embeddings were found to per-form well in other NLP tasks as well (Turian etal., 2010).Embedding dimensionality: We observe that forHLBL, C&W and LSA source embeddings (for alltraining set sizes), 200 dimensions outperform 50.While a smaller number of dimensions has beenshown to work better in other tasks (Turian et al,2010), re-embedding words may benefit from alarger initial dimension of the word vector space.We leave the testing of this hypothesis for futurework.Additional features: Across all embeddings, ap-pending the document?s binary bag-of-words rep-resentation increases classification accuracy.6 Future WorkWhile ?semantic smoothing?
obtained from intro-ducing an external embedding helps to improveperformance in the sentiment classification task,the method does not help to re-embed words thatdo not appear in the training set to begin with.
Re-turning to our example, if we found dramatic andpleasant to be ?far?
in the original (source) em-bedding space, but re-embed them such that theyare ?near?
(for the task of movie review sentimentclassification, for example), then we might ex-pect words such as melodramatic, powerful, strik-ing, enjoyable to be re-embedded nearby as well,even if they did not appear in the training set.The objective for this optimization problem can beposed by requiring that the distance between ev-ery pair of words in the source and target embed-dings is preserved as much as possible, i.e.
min(??i?
?j ?
?i?j)2 ?i, j (where, with some abuse ofnotation, ?
and ??
are the source and target em-beddings respectively).
However, this objective isno longer convex in the embeddings.
Global re-embedding constitutes our ongoing work and maypose an interesting challenge to the community.7 ConclusionWe presented a novel approach to adapting exist-ing word vectors for improving performance ina text classification task.
While we have shownpromising results in a single task, we believe thatthe method is general enough to be applied toa range of supervised tasks and source embed-dings.
As sophistication of unsupervised methodsgrows, scaling to ever-more massive datasets, sowill the representational power and coverage of in-duced word vectors.
Techniques for leveraging thelarge amount of unsupervised data, but indirectlythrough word vectors, can be instrumental in caseswhere the data is not directly available, trainingtime is valuable and a set of easy low-dimensional?plug-and-play?
features is desired.4928 AcknowledgementsThis work was supported in part by the NSF CDIGrant ECCS 0941561 and the NSF Graduate fel-lowship.
The content of this paper is solely theresponsibility of the authors and does not neces-sarily represent the official views of the sponsor-ing organizations.
The authors would like to thankThorsten Joachims and Bishan Yang for helpfuland insightful discussions.ReferencesMikhail Belkin, Partha Niyogi, and Vikas Sindhwani.2006.
Manifold regularization: A geometric frame-work for learning from labeled and unlabeled exam-ples.
The Journal of Machine Learning Research,7:2399?2434.Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, andChristian Janvin.
2003.
A neural probabilistic lan-guage model.
The Journal of Machine Learning Re-search, 3:1137?1155.William Blacoe and Mirella Lapata.
2012.
A com-parison of vector-based representations for semanticcomposition.
In Proceedings of the 2012 Joint Con-ference on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning, pages 546?556.
Association for Compu-tational Linguistics.Ronan Collobert and Jason Weston.
2008.
A unifiedarchitecture for natural language processing: deepneural networks with multitask learning.
In Pro-ceedings of the 25th international conference onMachine learning, pages 160?167.
ACM.Ronan Collobert, Jason Weston, Le?on Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
The Journal of Machine Learning Re-search, 12:2493?2537.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
Liblinear: Alibrary for large linear classification.
The Journal ofMachine Learning Research, 9:1871?1874.Eric H Huang, Richard Socher, Christopher D Man-ning, and Andrew Y Ng.
2012.
Improving wordrepresentations via global context and multiple wordprototypes.
In Proceedings of the 50th Annual Meet-ing of the Association for Computational Linguis-tics: Long Papers-Volume 1, pages 873?882.
Asso-ciation for Computational Linguistics.Dong C Liu and Jorge Nocedal.
1989.
On the limitedmemory bfgs method for large scale optimization.Mathematical programming, 45(1-3):503?528.Andrew L Maas, Raymond E Daly, Peter T Pham, DanHuang, Andrew Y Ng, and Christopher Potts.
2011.Learning word vectors for sentiment analysis.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies-Volume 1, pages 142?150.
As-sociation for Computational Linguistics.Andriy Mnih and Geoffrey E Hinton.
2009.
A scalablehierarchical distributed language model.
Advancesin neural information processing systems, 21:1081?1088.Bo Pang and Lillian Lee.
2004.
A sentimental educa-tion: Sentiment analysis using subjectivity summa-rization based on minimum cuts.
In Proceedings ofthe 42nd annual meeting on Association for Compu-tational Linguistics, page 271.
Association for Com-putational Linguistics.Richard Socher, Christopher D Manning, and An-drew Y Ng.
2010.
Learning continuous phraserepresentations and syntactic parsing with recursiveneural networks.
In Proceedings of the NIPS-2010Deep Learning and Unsupervised Feature LearningWorkshop.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: a simple and general methodfor semi-supervised learning.
In Proceedings of the48th Annual Meeting of the Association for Compu-tational Linguistics, pages 384?394.
Association forComputational Linguistics.Jason Weston, Fre?de?ric Ratle, Hossein Mobahi, andRonan Collobert.
2012.
Deep learning via semi-supervised embedding.
In Neural Networks: Tricksof the Trade, pages 639?655.
Springer.Ainur Yessenalina and Claire Cardie.
2011.
Com-positional matrix-space models for sentiment anal-ysis.
In Proceedings of the Conference on Empiri-cal Methods in Natural Language Processing, pages172?182.
Association for Computational Linguis-tics.Xiaojin Zhu and Zoubin Ghahramani.
2002.
Learningfrom labeled and unlabeled data with label propa-gation.
Technical report, Technical Report CMU-CALD-02-107, Carnegie Mellon University.493
