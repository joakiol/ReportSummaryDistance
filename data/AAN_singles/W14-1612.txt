Proceedings of the Eighteenth Conference on Computational Language Learning, pages 109?118,Baltimore, Maryland USA, June 26-27 2014.c?2014 Association for Computational LinguisticsTemporal Scoping of Relational Facts based on Wikipedia DataAvirup Sil?Computer and Information SciencesTemple UniversityPhiladelphia, PA 19122avi@temple.eduSilviu CucerzanMicrosoft ResearchOne Microsoft WayRedmond, WA 98052silviu@microsoft.comAbstractMost previous work in informationextraction from text has focused onnamed-entity recognition, entity linking,and relation extraction.
Less attentionhas been paid given to extracting thetemporal scope for relations betweennamed entities; for example, the relationpresident-Of(John F. Kennedy, USA)is true only in the time-frame (January20, 1961 - November 22, 1963).
In thispaper we present a system for temporalscoping of relational facts, which istrained on distant supervision based on thelargest semi-structured resource available:Wikipedia.
The system employs languagemodels consisting of patterns automat-ically bootstrapped from Wikipediasentences that contain the main entity ofa page and slot-fillers extracted from thecorresponding infoboxes.
This proposedsystem achieves state-of-the-art resultson 6 out of 7 relations on the benchmarkText Analysis Conference 2013 datasetfor temporal slot filling (TSF), and out-performs the next best system in the TAC2013 evaluation by more than 10 points.1 IntroductionPrevious work on relation extraction (Agichteinand Gravano, 2000; Etzioni et al., 2004) by sys-tems such as NELL (Carlson et al., 2010), Know-ItAll (Etzioni et al., 2004) and YAGO (Suchaneket al., 2007) have targeted the extraction of en-tity tuples, such as president-Of(George W.Bush, USA), in order to build large knowl-edge bases of facts.
These systems assumethat relational facts are time-invariant.
However,this assumption is not always true, for example?This research was carried out during an internship atMicrosoft Research.president-Of(George W. Bush, USA) holdswithin the time-frame (2001-2009) only.
In thispaper, we focus on the relatively less exploredproblem of attaching temporal scope to relationbetween entities.
The Text Analysis Conference(TAC) introduced temporal slot filling (TSF) asone of the knowledge base population (KBP) tasksin 2013 (Dang and Surdeanu, 2013).
The in-put to a TAC-TSF system is a binary relation e.g.per:spouse(Brad Pitt, Jennifer Aniston) and adocument assumed to contain supporting evidencefor the relation.
The required output is a 4-tupletimestamp [T1, T2, T3, T4], where T1 and T2are normalized dates that provide a range for thestart date of the relation, and T3 and T4 providethe range for the end of the relationship.
Sys-tems must also output the offsets of the text men-tions that support the temporal information ex-tracted.
For example, from a text such as ?Pittmarried Jennifer Aniston on July 29, 2000 [...] thecouple divorced five years later in 2005.?, a sys-tem must extract the normalized timestamp [2000-07-29, 2000-07-29, 2005-01-01, 2005-12-31], to-gether with the entity and date offsets that supportthe timestamp.In this paper, we describe TSRF, a system fortemporal scoping of relational facts.
For ev-ery relation type, TSRF uses distant supervisionfrom Wikipedia infobox tuples to learn a languagemodel consisting of patterns of entity types, cate-gories, and word n-grams.
Then it uses this trainedrelation-specific language model to extract the topk sentences that support the given relation betweenthe query entity and the slot filler.
In a secondstage, TSRF performs timestamp classification byemploying models which learn ?Start?, ?End?
and?In?
predictors of entities in a relationship; it com-putes the best 4-tuple timestamp [T1, T2, T3, T4]based on the confidence values associated to thetop sentences extracted.
Following the TAC-TSFtask for 2013, TSRF is trained and evaluated forseven relation types, as shown in Table 1.109per:spouseper:titleper:employee or member oforg:top employees/membersper:cities of residenceper:statesorprovinces of residenceper:countries of residenceTable 1: Types of relations in the TAC-TSF.The remainder of the paper is organized as fol-lows: The next section describes related work.Section 3 introduces the TAC-TSF input and out-put formats.
Section 4 discusses the main chal-lenges, and Section 5 details our method for tem-poral scoping of relations.
Section 6 describes ourexperiments and results, and it is followed by con-cluding remarks.2 Related WorkTo our knowledge, there are only a small num-ber of systems that have tackled the temporalscoping of relations task.
YAGO (Wang et al.,2010) extracts temporal facts using regular expres-sions from Wikipedia infoboxes, while PRAVDA(Wang et al., 2011) uses a combination of textualpatterns and graph-based re-ranking techniques toextract facts and their temporal scopes simultane-ously.
Both systems augment an existing KB withtemporal facts similarly to the CoTS system byTalukdar et al.
(2012a; 2012b).
However, theirunderlying techniques are not applicable to arbi-trary text.
In contrast, TSRF automatically boot-straps patterns to learn relation-specific languagemodels, which can be used then for processingany text.
CoTS, a recent system that is part ofCMU?s NELL (Carlson et al., 2010) project, per-forms temporal scoping of relational facts by usingmanually edited temporal order constraints.
Whilemanual ordering is appealing and can lead to highaccuracy, it is impractical from a scalability per-spective.
Moreover, the main goal of CoTS is topredict temporal ordering of relations rather thanto scope temporally individual facts.
Conversely,our system automatically extracts text patterns,and then uses them to perform temporal classi-fication based on gradient boosted decision trees(Friedman, 2001).The TempEval task (Pustejovsky and Verhagen,2009) focused mainly on temporal event order-ing.
Systems such as (Chambers et al., 2007) and(Bethard and Martin, 2007) have been successfulCol.1: TEMP72211 Col.7: 1492Col.2: per:spouse Col.8: 1311Col.3: Brad Pitt Col.9: 1.0Col.4: AFP ENG 20081208.0592 Col.10: E0566375Col.5: Jennifer Aniston Col.11: E0082980Col.6: 1098Table 2: Input to a TSF System.in extracting temporally related events.
Sil et al.
(2011a) automatically extract STRIPS represen-tations (Fikes and Nilsson, 1971) from web text,which are defined as states of the world before andafter an event takes place.
However, all these ef-forts focus on temporal ordering of either events orstates of the world and do not extract timestampsfor events.
By contrast, the proposed system ex-tracts temporal expressions and also produces anordering of the timestamps of relational facts be-tween entities.The current state-of-the-art systems for TSFhave been the RPI-Blender system by Artiles etal.
(2011) and the UNED system by Garrido etal.
(2011; 2012).
These systems obtained thetop scores in the 2011 TAC TSF evaluation byoutperforming the other participants such as theStanford Distant Supervision system (Surdeanuet al., 2011).
Similar to our work, these sys-tems use distant supervision to assign temporal la-bels to relations extracted from text.
While weemploy Wikipedia infoboxes in conjunction withWikipedia text, the RPI-Blender and UNED sys-tems use tuples from structured repositories likeFreebase.
There are major differences in terms oflearning strategies of these systems: the UNEDsystem uses a rich graph-based document-levelrepresentation to generate novel features whereasRPI-Blender uses an ensemble of classifiers com-bining flat features based on surface text and de-pendency paths with tree kernels.
Our system em-ploys language models based on Wikipedia thatare annotated automatically with entity tags in aboosted-trees learning framework.
A less impor-tant difference between TSRF and RPI-Blender isthat the latter makes use of an additional tempo-ral label (Start-And-End) for facts within a timerange; TSRF employs Start, End, and In labels.3 The Temporal Slot Filling Task3.1 InputThe input format for a TSF system as instantiatedfor the relation per:spouse(Brad Pitt, Jennifer110Aniston) is shown in Table 2.
The field Column 1contains a unique query ID for the relation.
Col-umn 2 is the name of the relationship, which alsoencodes the type of the target entity.
Column 3contains the name of the query entity, i.e., the sub-ject of the relation.
Column 4 contains a valid doc-ument ID and Column 5 indicates the slot-filler en-tity.
Columns 6 through 8 are offsets of the slot-filler, query entity and the relationship justificationin the given text.
Column 9 contains a confidencescore set to 1 to indicate that the relation is cor-rect.
Columns 10 and 11 contain the IDs in theKBP knowledge base of the entity and filler, re-spectively.
All of the above are provided by TAC.For the query in this example, a TSF system has toscope temporally the per:spouse relation be-tween Brad Pitt and Jennifer Aniston.3.2 OutputSimilar to the regular slot filling task in TAC, theTSF output includes the offsets for at least oneentity mention and up to two temporal mentionsused for the extraction and normalization ofhypothesized answer.
For instance, assume that asystem extracts the relative timestamp ?Monday?and normalizes it to ?2010-10-04?
for the relationorg:top employee(Twitter, Williams) usingthe document date from the following document:<DOCID> AFP ENG 20101004.0053.LDC2010T13 </DOCID><DATETIME> 2010-10-04 </DATETIME><HEADLINE>Twitter co-founder steps down as CEO</HEADLINE><TEXT><P>Twitter co-founder Evan Williams announced on Mondaythat he was stepping down as chief executive [...]The system must report the offsets for both?Monday?
in the text body and ?2010-10-04?
inthe DATETIME block for the justification.The TAC-TSF task uses the following represen-tation for the temporal information extracted: Foreach relation provided in the input, TSF systemsmust produce a 4-tuple of dates: [T1, T2, T3, T4],which indicates that the relation is true for a pe-riod beginning at some point in time between T1and T2 and ending at some time between T3 andT4.
By convention, a hyphen in one of the po-sitions implies a lack of a constraint.
Thus, [-,20120101, 20120101, -] implies that the relationwas true starting on or before January 1, 2012 andending on or after January 1, 2012.
As discussedin the TAC 2011 pilot study by Ji et al.
(2011),there are situations that cannot be covered by thisrepresentation, such as recurring events, for ex-ample repeated marriages between two persons.However, the most common situations for the re-lations covered in this task are captured correctlyby this 4-tuple representation.4 ChallengesWe discuss here some of the main challenges en-countered in building a temporal scoping system.4.1 Lack of Annotated DataAnnotation of data for this task is expensive, asthe human annotators must have extensive back-ground knowledge and need to analyze the evi-dence in text and reliable knowledge resources.
Asper (Ji et al., 2013), a large team of human an-notators were able to generate only 1,172 traininginstances for 8 slots for KBP 2011.
The authorsof the study concluded that such amount of datais not enough for training a supervised temporalscoping system.
They also noted that only 32% ofemployee Of queries were found to have poten-tial temporal arguments, and only one third of thequeries could have reliable start or end dates.4.2 Date NormalizationSometimes temporal knowledge is not stated ex-plicitly in terms of dates or timestamps.
For exam-ple, from the text ?they got married on Valentine?sDay?
a system can extract Valentine?s Day as thesurface form of the start of the per:spouse re-lation.
However, for a temporal scoping system itneeds to normalize the temporal string to the dateof February 14 and the year to which the documentrefers to explicitly in text or implicitly, such as theyear in which the document was published.4.3 Lexico-Syntactic VarietyA relation can be specified in text by employingnumerous syntactic and lexical constructions; e.g.for the per:spouse relation the patterns ?gotmarried on [DATE]?
and ?vowed to spend eternityon [DATE]?
have the same meaning.
Addition-ally, entities can appear mentioned in text in vari-ous forms, different from the canonical form givenas input.
For instance, Figure 1 shows an examplein which the input entity Bernardo Hees, which isnot in Wikipedia, is mentioned three times, withtwo of the mentions using a shorter form (the lastname of the person).111org:top_members_employees    America Latina Logistica / NIL    Bernardo Hees / NIL<HEADLINE> Burger King buyer names future CEO </HEADLINE><DATELINE> NEW YORK 2010-09-09 13:00:29 UTC </DATELINE><TEXT><P> The investment firm buying Burger King has named Bernardo Hees, a LatinAmerican railroad executive, to be CEO of the company after it completes its$3.26 billion buyout of the fast-food chain.
</P><P> 3G Capital is naming Hees to replace John Chidsey, who will become co-chairman after the deal closes.
</P><P> Hees was most recently CEO of America Latina Logistica, Latin America'slargest railroad company.
Alexandre Behring, managing partner at 3G Capital, wasalso a prior CEO of the railroad.
</P><P> 3G Capital is expected to begin its effort to acquire the outstanding sharesof Burger King for $24 per share by Sept. 17.
</P></TEXT>Figure 1: Example data point from the TAC TSF 2013 training set, with the annotations hypothesizedby our system.
The entity mentions identified by the entity linking (EL) component are shown in boldblue; those that were linked to Wikipedia are also underlined.
The highlighting (blue and green) is usedto show the mentions in the coreference chains identified for the two input entities, ?America LatinaLogistica?
and ?Bernardo Hees?.4.4 Inferred MeaningA temporal scoping system also needs to learn theinter-dependence of relations, and how one eventaffects another.
For instance, in our automaticallygenerated training data, we learn that a deathevent specified by n-grams like ?was assassinated?affects the per:title relation, and it indicatesthat the relationship ended at that point.
In Fig-ure 1, while the CEO relationships for BernardoHees with America Latina Logistica and BurgerKing are indicated by clear patterns (?was most re-cently CEO of?
and ?to be CEO of?
), the temporalstamping is difficult to achieve in both cases, asthere is no standard normalization for ?recently?in the former, and it is relative to the completionof the buyout event in the latter.4.5 Pattern TrustworthinessA temporal scoping system should also be ableto model the trustworthiness of text patterns, andeven the evolution of patterns that indicate a rela-tionship over time.
For example, in current news,the birth of a child does not imply that a coupleis married, although it does carry a strong signalabout the marriage relationship.5 Learning to Attach Temporal Scope5.1 Automatically Generating Training DataAs outlined in Section 4, one of the biggest chal-lenges of a temporal scoping system is the lackof annotated data to create a strong informationextraction system.
Previous work on relation ex-traction such as (Mintz et al., 2009) has shownthat distant supervision can be highly effective inbuilding a classifier for this purpose.
Similar tosupervised classification techniques, some advan-tages of using distant supervision are:?
It allows building classifiers with a large numberof features;?
The supervision is provided intrinsically by thedetailed user-contributed knowledge;?
There is no need to expand patterns iteratively.Mintz et al.
also point out that similar to unsuper-vised systems, distant supervision also allows:?
Using large amounts of unlabeled data such asthe Web and social media;?
Employing techniques that are not sensitive tothe genre of training data.We follow the same premise as (Cucerzan, 2007;Weld et al., 2009) that the richness of theWikipedia collection, whether semantic, lexical,syntactic, or structural, is a key enabler in re-defining the state-of-the-art for many NLP andIR task.
Our target is to use distant supervisionfrom Wikipedia data to build an automatic tempo-ral scoping system.
However, for most relations,we find that Wikipedia does not indicate specificstart or end dates in a structured form.
In additionto this, we need our system to be able to predictwhether two entities are currently in a relation-ship or not based on the document date as well.112Hence, in our first step, we build an automatic sys-tem which takes as input a binary relation betweentwo entities e.g.
per:spouse(Brad Pitt, JenniferAniston) and a number of documents.
The systemneeds to extract highly ranked/relevant sentences,which indicate that the two entities are in the tar-geted relationship.
The next component takes asinput the top k sentences generated in the previousstep and extracts temporal labels for the input rela-tion.
Note that our target is to develop algorithmsthat are not relation-specific but rather can workwell for a multitude of relations.
We elaborate onthese two system components further.5.1.1 Using Wikipedia as a Resource forDistant SupervisionWikipedia is the largest freely available encyclo-pedic collection, which is built and organized asa user-contributed knowledge base (KB) of enti-ties.
The current version of the English Wikipediacontains information about 4.2 million entities.In addition to the plain text about these entities,Wikipedia also contains structured components.One of these is the infobox.
Infoboxes contain in-formation about a large number of relations for thetarget entity of the Wikipedia page, e.g.
names ofspouses, birth and death dates, residence etc.. Sim-ilar to structured databases, the infoboxes containthe most important/useful relations in which enti-ties take part, while the text of Wikipedia pagescontains mentions and descriptions of these rela-tions.
Because of this, Wikipedia can be seen as aknowledge repository that contains parallel struc-tured and unstructured information about entities,and therefore, can be employed more easily thanFreebase or other structured databases for buildinga relation extraction system.
Figure 2 shows howsentences from Wikipedia can be used to train asystem for the temporal slot filling task.5.1.2 Extracting Relevant SentencesFor every relation, we extract slot-filler namesfrom infoboxes of each Wikipedia article.
Wealso leverage Wikipedia?s rich interlinking modelto automatically retrieve labeled entity mentionsin text.
Because the format of the text values pro-vided by different users for the infobox attributescan vary greatly, we rely on regular expressions toextract slot-filler names from the infoboxes.
Forevery relation targeted, we build a large set of reg-ular expressions to extract entity names and filterout noise e.g.
html tags, redundant text etc..To extract all occurrences of named-entities inthe Wikipedia text, we relabel each Wikipedia ar-ticle with Wikipedia interlinks by employing theentity linking (EL) system by Cucerzan (2012),which obtained the top scores for the EL task insuccessive TAC evaluations.
This implementa-tion takes into account and preserves the inter-links created by the Wikipedia contributors, andextracts all other entity mentions and links them toWikipedia pages if possible or hypothesizes coref-erence chains for the mentions of entities that arenot in Wikipedia.
The latter are extremely impor-tant when the slot-filler for a relation is an entitythat does not have a Wikipedia page, as often isthe case with spouses or other family members offamous people (as shown in Figure 1 for the slot-filler Bernardo Hees).As stated in Section 4, temporal informationin text is specified in various forms.
To resolvetemporal mentions, we use the Stanford SUTime(Chang and Manning, 2012) temporal tagger.The system exhibits strong performance outper-forming state-of-the-art systems like HeidelTime(Str?otgen and Gertz, 2010) on the TempEval-2Task A (Verhagen et al., 2010) in English.
SU-Time is a rule-based temporal tagger that employsregular expression.
Its input is English text in to-kenized format; its output contains annotations inthe form of TIMEX3 tags.
TIMEX3 is a part ofthe TimeML annotation language as introduced by(Pustejovsky et al., 2003) and is used to markupdate and time, events, and their temporal rela-tions in text.
When processing Web text, we of-ten encounter date expressions that contain a rel-ative time e.g.
?last Thursday?.
To resolve themto actual dates/time is a non-trivial task.
However,the heuristic of employing the document?s publi-cation date as the reference works very well inpractice e.g.
for a document published on 2011-07-05, SUTime resolves ?last Thursday?
to 2011-06-30.
It provides temporal tags in the followinglabels: Time, Duration, Set and Interval.
For ourexperiments we used Time and Duration.After running the Stanford SUTime, which au-tomatically converts date expressions to their nor-malized form, we collect sets of contiguous sen-tences from the page that contain one mention ofthe targeted entity and one mention of the slot-filler, as extracted by the entity linking system.
Wethen build a large language model by bootstrap-ping textual patterns supporting the relations, sim-113ilar to (Agichtein and Gravano, 2000).
The generalintuition is that a set of sentences that mention thetwo entities are likely to state something about re-lationships in which they are.For assigning sentences a relevance score withrespect to a targeted relation, we represent the sen-tences in an input document (i.e., Wikipedia page)as d dimensional feature vectors, which incorpo-rate statistics about how relevant sentences areto the relation between a query entity q and theslot filler z.
For example, for the per:spouserelation, one binary feature is ?does the inputsentence contain the n-gram ?QUERY ENTITYgot married??.
Note that the various surfaceforms/mentions of q and z are resolved to theircanonical target at this stage.We were able to extract 61,872 tuples of queryentity and slot filler relations from Wikipediafor the per:spouse relation.
Figure 2 showshow we extract relevant sentences using slot-fillernames from Wikipedia.
Consider the followingtext (already processed by our EL system andStanford SUTime) taken from the Wikipedia pageof Tom Cruise:On [November 18, 2006|2006?11?18],[Holmes|Katie Holmes] and [Cruise|Tom Cruise]were married in [Bracciano|Bracciano] .
.
.On [June 29, 2012|2012?06?29],[Holmes|Katie Holmes] filed for divorcefrom [Cruise|Tom Cruise] after five and a halfyears of marriage.Considering Tom Cruise as the query entity andhis wife Katie Holmes as the slot filler for theper:spouse relation, we normalize the abovetext to the following form to extract features:On DATE, SLOT FILLER andQUERY ENTITY were married inLOCATION .
.
.On DATE, SLOT FILLER filed for divorcefrom QUERY ENTITY after five and a halfyears of marriage.Our language model consists of n-grams (n ?
5)like ?SLOT FILLER and QUERY ENTITY weremarried?, ?SLOT FILLER filed for divorce from?which provides clues for the marriage relation.These n-grams are then used as features withan implementation of a gradient boosted decisiontrees classifier similar to that described by (Fried-man, 2001; Burges, 2010).
We also use featuresprovided by the EL system which are based on en-tity types and categories.
We call this ?relation-ship?
classifier RELCL.
The output of this step isIn April 2005, Cruise began dating actress KatieHolmes.
On April 27 that year, Cruise and Holmes ?dubbed "TomKat" by the media ?
made their firstpublic appearance together in Rome.
On October 6,2005, Cruise and Holmes announced they wereexpecting a child, and their daughter, Suri, was born inApril 2006.
On November 18, 2006, Holmes and Cruisewere married in Bracciano, Italy, in a Scientologyceremony attended by many Hollywood stars.
Therehas been widespread speculation that the marriagewas arranged by the Church of Scientology.
On June 29,2012, it was announced that Holmes had filed fordivorce from Cruise after five and a half years ofmarriage.
On July 9, 2012, it was announced that thecouple had signed a divorce settlement worked out bytheir lawyers.
STARTOfmarriageENDOfmarriageSpouse: Katie HolmesFigure 2: Example of relevant sentences extractedby using query entity and slot-filler names fromWikipedia for the per:spouse relation.a ranked list of sentences which indicate whetherthere exists a relationship between the query entityand the slot filler.5.1.3 Learning AlgorithmOur objective is to rank the sentences in a docu-ment based on the premise that entities q and zare in the targeted relation r. We tackle this rank-ing task by using gradient boosted decision trees(GBDT) to learn temporal scope for entity rela-tions.
Previous work such as Sil et al.
(2011a;2011b) used SVMs for ranking event precondi-tions and (Cucerzan, 2012) and (Zhou et al., 2010)employed GBDT for ranking entities.
GBDT canachieve high accuracy as they can easily combinefeatures of different scale and missing values.
Inour experiments, GBDT outperforms both SVMsand MaxEnt models.We employ the stochastic version of GBDTsimilar to (Friedman, 2001; Burges, 2010).
Ba-sically, the model performs a numerical optimiza-tion in the function space by computing a functionapproximation in a sequence of steps.
By build-ing a smaller decision tree at each step, the modelcomputes residuals obtained in the previous step.Note that in the stochastic variant of GBDT, forcomputing the loss function, the model absorbsseveral samples instead of using the whole train-ing data.
The parameters for our GBDT modelwere tuned on a development set sampled fromour Wikipedia dump independent from the train-ing set.
These parameters include the number ofregression trees and the shrinkage factor.114Figure 3: Architecture of the proposed sys-tem.
Every input document is processed by the(Cucerzan, 2012) entity linking system and theStanford SUTime system.
Temporal informationis then extracted automatically using RELCL andDATECL.5.1.4 Gathering Relevant SentencesOn the unseen test data, we apply our trainedmodel and obtain a score for each new sentence sthat contains mentions of entities q and z that arein a targeted relationship by turning s into a featurevector as shown previously.
Among all sentencesthat contain mentions of q and z, we choose thetop k with the highest score.
The value of k wastuned based on the performance of TSRF on ourdevelopment set.5.1.5 Extracting TimestampsTo predict timestamps for each relation, we buildanother classifier, DATECL similar to that de-scribed in the previous section, by using languagemodels for ?Start?, ?End?
and ?In?
predictors ofrelationship.
The ?Start?
model predicts T1, T2;?End?
predicts T3, T4 and ?In?
predicts T2, T3.Raw Trigger Features: Similar to previouswork by (Sil et al., 2010) on using discriminativewords as features, each of these models composeof ?Trigger Words?
that indicate when a relation-ship begins or ends.
In the current implemen-tation, these triggers are chosen manually fromthe language model automatically bootstrappedfrom Wikipedia.
Future directions include howto automatically learn these triggers.
For ex-ample, for the per:spouse relation, the trig-gers for ?Start?
contain n-grams such as ?mar-ried since DATE?
and ?married SLOT FILLERon?
; the ?End?
model contains n-grams such as?estranged husband QUERY ENTITY?, ?split inDATE?
; the ?In?
model contains ?happily mar-ried?, ?QUERY ENTITY with his wife?
etc.. Foran input sentence with query entity q and slot-filler z, a first class of raw trigger features con-sists of cosine-similarity(Text(q, z), Triggers(r))where r ?
Start, End, In.
Here, Text(q, z) in-dicates the full sentence as context.
We alsoemploy another feature that computes cosine-similarity(Context(q, z), Triggers(r)), which con-structs a mini-sentence Context(q, z) from theoriginal by choosing windows of three words be-fore and after q and z, and ignoring duplicates.External Event Triggers: Our system alsoconsiders the presence of other events as triggerse.g.
a ?death?
event signaled by ?SLOT FILLERdied?
might imply that a relationship ended on thattimestamp.
Similarly, a ?birth?
event can implythat an entity started living in a particular locatione.g.
the per:born-In(Obama, Honolulu)relation from the sentence ?President Obama wasborn in Honolulu in 1961?
indicates that T1 =1961-01-01 and T2 = 1961-12-31 for the rela-tion per:cities of residence(Obama,Honolulu).At each step, TSRF extracts the top timestampsfor predicting ?Start?, ?End?
and ?In?
based onthe confidence values of DATECL.
Similar to pre-vious work by (Artiles et al., 2011), we aggregateand update the extracted timestamps using the fol-lowing heuristics:Step 1: Initialize T= [-?, +?, -?,+ ?
]Step 2: Iterate through the classified timestampsStep 3: For a new T?aggregate :T&&T?= [max(t1, t?1),min(t2, t?2),max(t3, t?3),min(t4, t?4)]Update only if: t1?
t2; t3?
t4; t1?
t4This novel two-step classification strategy re-moves noise introduced by distant supervisiontraining and decides if the extracted (entity, filler,timestamp) tuples belong to the relation underconsideration or not.
For example, for theper:spouse relation between the entities BradPitt and Jennifer Aniston, TSRF extracts sentenceslike ?..On November 22, 2001, Pitt made a guestappearance in the television series Friends, play-ing a man with a grudge against Rachel Green,played by Jennifer Aniston..?
and ?Pitt met Jen-nifer Aniston in 1998 and married her in a privatewedding ceremony in Malibu on July 29, 2000..?.Note that both sentences contain the query entityand the slot filler.
The system automatically re-jects the extraction of temporal information from115S1 S2 S3 S4 S5 S6 S7 ALL StDevBaseline 24.70 17.40 15.18 17.83 14.75 21.08 23.20 19.10 3.60TSRF 31.94 36.06 32.85 40.12 33.04 31.85 27.35 33.15 3.66RPI-Blender 31.19 13.07 14.93 26.71 29.04 17.24 34.68 23.42 7.98UNED 26.20 6.88 8.16 15.24 14.47 14.41 19.34 14.79 6.07CMU-NELL 19.95 7.46 8.47 16.52 13.43 5.65 11.95 11.53 4.77Abby-Compreno 0.0 2.42 8.56 0.0 13.50 7.91 0.0 5.14 4.99LDC 69.87 60.22 58.26 72.27 81.10 54.07 91.18 68.84 12.32Table 3: Results for the TAC-TSF 2013 test set, overall and for individual slots.
The slots notation is: S1:org:top members employees, S2: per:city of residence, S3: per:country of residence, S4: per:employeeor member of, S5: per:spouse, S6: per:statesorprovince of residence, S7: per:title.
The score for theoutput created by the LDC experts is also shown.the former even though the sentence contains men-tions of both entities.
This is because the languagemodel for the marriage relation does not matchwell this candidate sentence, which is actually fo-cussing on the two entities being in the differentrelation of co-acting/appearing in the same mo-tion picture.
The latter sentence is determined asmatching the language model for the marriage re-lation, and TSRF extracts the temporal scope July29, 2000 and attaches the START label to it.
Mostprevious systems do not perform this noise re-moval step, which is a critical component in ourdistant supervision approach.6 ExperimentsFor evaluation, we train our system on the infoboxtuples and sentences extracted from the Wikipediadump of May 2013.
We set aside a portion of thedump as our development data.
We chose to usethe top-relevant n-grams based on the performanceon the development data as features.
We employthen the TAC evaluation data, which is publiclyavailable through LDC.We utilize the evaluation metric developed forTAC (Dang and Surdeanu, 2013).
In order for atemporal constraint (T1-T4) to be valid, the doc-ument must justify both the query relation (whichis similar to the regular English slot filling task)and the temporal constraint.
Since the time in-formation provided in text may be approximate,the TAC metric measures the similarity of eachconstraint in the key and system response.
For-mally, if the date in the gold standard is ki, whilethe date hypothesized by the system is ri, anddi= |ki?
ri| is their difference measured inyears, then the score for the set of temporal con-straints on a slot is computed as:Score(slot) =144?i=1cc + diTAC sets the constant c to one year, so that pre-dictions that differ from the gold standard by oneyear get 50% credit.
The absence of a constraintin T1 or T3 is treated as a value of??
and the ab-sence of a constraint in T2 or T4 is treated as +?,which lead to zero-value terms in the scoring sum.Therefore, the overall achievable score has a rangebetween 0 and 1.We compare TSRF against four other TSF sys-tems: (i) RPI-Blender (Artiles et al., 2011), (ii)CMU-NELL (Talukdar et al.
(2012a; 2012b)),(iii) UNED (Garrido et al.
(2011; 2012)) and (iv)Abby-Compreno (Kozlova et al., 2012).
Most ofthese systems employ distant supervision strate-gies too.
RPI-Blender and UNED obtained the topscores in the 2011 TAC TSF pilot evaluation, andthus, could be considered as the state-of-the-art atthe time.We also compare our system with a reasonablebaseline similar to (Ji et al., 2011).
This baselinemakes the simple assumption that the correspond-ing relation is valid at the document date.
Thatmeans that it creates a ?within?
tuple as follows:< ?
?, doc date, doc date, +?
>.
Hence, thisbaseline system for a particular relation alwayspredicts T2 = T3 = the date of the document.Table 3 lists the results obtained by our systemon the TAC test set of 201 queries, overall and foreach individual slot, in conjunction with the re-sults of the other systems evaluated and the outputgenerated by the LDC human experts.
Only twoout of the five systems evaluated, TSRF and RPI-Blender, are able to beat the ?within?
baseline.TSRF achieves approximately 48% of humanperformance (LDC) and outperforms all other sys-116TSF Accuracy SF F1 SF Prec SF RecallLDC 68.8 83.1 97.3 72.5TSRF 33.1 77.3 96.8 64.4RPI-Blender 23.4 51.8 69.2 41.4UNED 14.8 46.6 69.9 35.0CMU-NELL 11.5 32.2 38.5 27.6Abby-Compreno 5.1 18.5 53.6 11.2Table 4: Extraction accuracy for slot-filler men-tions.
TSRF clearly outperforms all systems andcomes close to human performance (LDC).tems in overall score, as well as for all individ-ual relations with the exception of per:title,for which RPI-Blender obtains a better score.
Infact, TSRF outperforms the next best systemsby 10 and 19 points.
These two systems ob-tained the top score in TAC 2011, and outper-formed other systems such as Stanford (Surdeanuet al., 2011).
TSRF also outperforms CMU-NELL which employs a very large KB of re-lational facts already extracted from the Weband makes use of the Google N-gram corpus(http://books.google.com/ngrams).We believe that this large performance differ-ence is due in part to the fact that TSRF uses alanguage model to clean up the noise introducedby distant supervision before the actual temporalclassification step.
Also, the learning algorithmemployed, GBDT, is highly effective in using theextracted n-grams as features to decide whetherthe extracted (entity, filler, time) tuples belong tothe relation under consideration or not.
Finally,Table 4 shows another reason that gives TSRF anedge in obtaining the best score.
The employed ELcomponent (Cucerzan, 2012) is a state-of-the-artsystem for extracting and linking entities, and re-solving coreference chains.
By using this system,we have been able to extract slot-filler mentionswith a precision of 96.8% at 66.4% recall, whichis substantially higher than the extraction resultsof all other systems.
Encouragingly, the perfor-mance of this component also comes close to thatof the LDC annotators, which obtained a precisionof 97.3% at 72.5% recall.It is also important to note that our system ex-hibits a balanced performance on the relationson which it was tested.
As shown in columnStDev in Table 3, this system achieves the low-est standard deviation in the performance acrossthe relations tested.
It is interesting to note alsothat TSRF achieves the best performance on theemployee of (S4) and city of residence(S2) relations even though the system develop-ment was done on the spouse relation (S1) as anencouraging sign that our distant supervision al-gorithm can be transferred successfully across re-lations for domain-specific temporal scoping.7 Conclusion and Future WorkThe paper described an automatic temporal scop-ing system that requires no manual labeling ef-fort.
The system uses distant supervision fromWikipedia to obtain a large training set of tuplesfor training.
It uses a novel two-step classifica-tion to remove the noise introduced by the dis-tant supervision training.
The same algorithmwas employed for multiple relations and exhibitedsimilarly high accuracy.
Experimentally, the sys-tem outperforms by a large margin several othersystems that address this relatively less exploredproblem.
Future directions of development in-clude extracting joint slot filler names and tem-poral information, and leveraging the changes ob-served over time in Wikipedia for a query entityand a slot filler in a target relation.ReferencesE.
Agichtein and L. Gravano.
2000.
Snowball: Ex-tracting relations from large plain-text collections.In Procs.
of the Fifth ACM International Conferenceon Digital Libraries.Javier Artiles, Qi Li, Taylor Cassidy, SuzanneTamang, and Heng Ji.
2011.
CUNY BLENDERTACKBP2011 Temporal Slot Filling System De-scription.
In TAC.Steven Bethard and James H Martin.
2007.
Cu-tmp:Temporal relation classification using syntactic andsemantic features.
In Proceedings of the 4th Inter-national Workshop on Semantic Evaluations, pages129?132.Chris Burges.
2010.
From ranknet to lambdarank tolambdamart: An overview.
Learning, 11:23?581.Andrew Carlson, Justin Betteridge, Bryan Kisiel,Burr Settles, Estevam R Hruschka Jr, and Tom MMitchell.
2010.
Toward an architecture for never-ending language learning.
In AAAI.Nathanael Chambers, Shan Wang, and Dan Juraf-sky.
2007.
Classifying temporal relations betweenevents.
In Proceedings of the 45th Annual Meetingof the ACL on Interactive Poster and DemonstrationSessions, pages 173?176.Angel X Chang and Christopher Manning.
2012.
Su-time: A library for recognizing and normalizing timeexpressions.
In LREC, pages 3735?3740.117Silviu Cucerzan.
2007.
Large-scale named entity dis-ambiguation based on wikipedia data.
In EMNLP-CoNLL, pages 708?716.Silviu Cucerzan.
2012.
The MSR System for EntityLinking at TAC 2012.
In TAC.Hoa Trang Dang and Mihai Surdeanu.
2013.
Taskdescription for knowledge-base population at TAC2013.
In TAC.O.
Etzioni, M. Cafarella, D. Downey, S. Kok,A.
Popescu, T. Shaked, S. Soderland, D. Weld, andA.
Yates.
2004.
Web-Scale Information Extractionin KnowItAll.
In WWW, New York City, New York.R.
Fikes and N. Nilsson.
1971.
STRIPS: A newapproach to the application of theorem proving toproblem solving.
Artificial Intelligence, 2(3/4):189?208.Jerome H Friedman.
2001.
Greedy function approx-imation: a gradient boosting machine.
Annals ofStatistics, pages 1189?1232.Guillermo Garrido, Bernardo Cabaleiro, Anselmo Pe-nas, Alvaro Rodrigo, and Damiano Spina.
2011.
Adistant supervised learning system for the tac-kbpslot filling and temporal slot filling tasks.
In TAC.Guillermo Garrido, Anselmo Penas, Bernardo Ca-baleiro, and Alvaro Rodrigo.
2012.
Temporally an-chored relation extraction.
In ACL.Heng Ji, Ralph Grishman, and Hoa Trang Dang.
2011.Overview of the tac2011 knowledge base populationtrack.
In TAC.Heng Ji, Taylor Cassidy, Qi Li, and Suzanne Tamang.2013.
Tackling representation, annotation and clas-sification challenges for temporal knowledge basepopulation.
Knowledge and Information Systems,pages 1?36.Ekaterina Kozlova, Manicheva Maria, Petrova Elena,and Tatiana Popova.
2012.
The compreno semanticmodel as an integral framework for a multilinguallexical database.
In 3rd Workshop on Cognitive As-pects of the Lexicon (CogALex-III).Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-rafsky.
2009.
Distant supervision for relation ex-traction without labeled data.
In ACL, pages 1003?1011.James Pustejovsky and Marc Verhagen.
2009.Semeval-2010 task 13: evaluating events, time ex-pressions, and temporal relations (tempeval-2).
InProceedings of the Workshop on Semantic Evalua-tions: Recent Achievements and Future Directions,pages 112?116.James Pustejovsky, Jos?e M Castano, Robert Ingria,Roser Sauri, Robert J Gaizauskas, Andrea Set-zer, Graham Katz, and Dragomir R Radev.
2003.Timeml: Robust specification of event and tempo-ral expressions in text.
New directions in questionanswering, 3:28?34.Avirup Sil and Alexander Yates.
2011a.
ExtractingSTRIPS representations of actions and events.
InRANLP.Avirup Sil and Alexander Yates.
2011b.
MachineReading between the Lines: A Simple EvaluationFramework for Extracted Knowledge Bases.
InWorkshop on Information Extraction and Knowl-edge Acquisition (IEKA).Avirup Sil, Fei Huang, and Alexander Yates.
2010.Extracting action and event semantics fromweb text.In AAAI Fall Symposium on Common-Sense Knowl-edge (CSK).Jannik Str?otgen and Michael Gertz.
2010.
Heideltime:High quality rule-based extraction and normaliza-tion of temporal expressions.
In Proceedings of the5th International Workshop on Semantic Evaluation,pages 321?324.Fabian M Suchanek, Gjergji Kasneci, and GerhardWeikum.
2007.
Yago: a core of semantic knowl-edge.
In WWW.Mihai Surdeanu, Sonal Gupta, John Bauer, David Mc-Closky, Angel X Chang, Valentin I Spitkovsky, andChristopher D Manning.
2011.
Stanfords distantly-supervised slot-filling system.
In TAC.Partha Pratim Talukdar, Derry Wijaya, and TomMitchell.
2012a.
Acquiring temporal constraintsbetween relations.
In CIKM.Partha Pratim Talukdar, Derry Wijaya, and TomMitchell.
2012b.
Coupled temporal scoping of rela-tional facts.
In WSDM.Marc Verhagen, Roser Sauri, Tommaso Caselli, andJames Pustejovsky.
2010.
Semeval-2010 task 13:Tempeval-2.
In Proceedings of the 5th InternationalWorkshop on Semantic Evaluation, pages 57?62.Yafang Wang, Mingjie Zhu, Lizhen Qu, Marc Spaniol,and Gerhard Weikum.
2010.
Timely yago: harvest-ing, querying, and visualizing temporal knowledgefrom wikipedia.
In Proceedings of the 13th Interna-tional Conference on Extending Database Technol-ogy, pages 697?700.
ACM.YafangWang, Bin Yang, Lizhen Qu, Marc Spaniol, andGerhard Weikum.
2011.
Harvesting facts from tex-tual web sources by constrained label propagation.In CIKM, pages 837?846.Daniel S. Weld, Raphael Hoffmann, and Fei Wu.
2009.Using Wikipedia to Bootstrap Open Information Ex-traction.
In ACM SIGMOD Record.Yiping Zhou, Lan Nie, Omid Rouhani-Kalleh, FlavianVasile, and Scott Gaffney.
2010.
Resolving surfaceforms to wikipedia topics.
In COLING, pages 1335?1343.118
