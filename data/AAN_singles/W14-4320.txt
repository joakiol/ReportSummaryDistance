Proceedings of the SIGDIAL 2014 Conference, pages 142?150,Philadelphia, U.S.A., 18-20 June 2014.c?2014 Association for Computational LinguisticsAddressing Class Imbalance for Improved Recognition of ImplicitDiscourse RelationsJunyi Jessy LiUniversity of Pennsylvanialjunyi@seas.upenn.eduAni NenkovaUniversity of Pennsylvanianenkova@seas.upenn.eduAbstractIn this paper we address the problem ofskewed class distribution in implicit dis-course relation recognition.
We examinethe performance of classifiers for both bi-nary classification predicting if a particu-lar relation holds or not and for multi-classprediction.
We review prior work to pointout that the problem has been addresseddifferently for the binary and multi-classproblems.
We demonstrate that adoptinga unified approach can significantly im-prove the performance of multi-class pre-diction.
We also propose an approach thatmakes better use of the full annotationsin the training set when downsampling isused.
We report significant absolute im-provements in performance in multi-classprediction, as well as significant improve-ment of binary classifiers for detecting thepresence of implicit Temporal, Compari-son and Contingency relations.1 IntroductionDiscourse relations holding between adjacent sen-tences in text play an essential role in establishinglocal coherence and contribute to the semantic in-terpretation of the text.
For example, the causal re-lationship is helpful for textual entailment or ques-tion answering while restatement and exemplifica-tion are important for automatic summarization.Predicting the type of implicit relations, whichare not signaled by any of the common explicitdiscourse connectives such as because, however,has proven to be a most challenging task in dis-course analysis.
The Penn Discourse Treebank(PDTB) (Prasad et al., 2008) provided valuableannotations of implicit relations.
Most research todate has focused on developing and refining lex-ical and linguistically rich features for the task(Pitler et al., 2009; Lin et al., 2009; Park andCardie, 2012).
Mostly ignored remains the prob-lem of addressing the highly skewed distributionof implicit discourse relations.
Only about 35% ofpairs of adjacent sentences in the PDTB are con-nected by three of the four top level discourse re-lation: 5% participate in Temporal relation, 10%in Comparison (contrast) and 20% in Contingency(causal) relations.
The remaining pairs are con-nected by the catch-all Expansion relation (40%)or by some other linguistic devices (24%).
Finergrained relations of interest to particular applica-tions account for increasingly smaller percentageof the PDTB data.Class imbalance is particularly problematic fortraining a binary classifier to distinguish one rela-tion from the rest.
As we will show later, it alsoimpacts the performance of multi-class predictionin which each pair of sentences is labeled with oneof the five possible relations.All prior work has resorted to downsamplingthe training data for binary classifiers to distin-guish a particular relation and use the full train-ing set for multi-class prediction.
In this pa-per we compare several methods for address-ing the skewed class distribution during training:downsampling, upsampling and computing fea-ture weights and performing feature selection onthe unaltered full training data.
A major motiva-tion for our work is to establish if any of the alter-natives to downsampling would prove beneficial,because in downsampling most of the expensivelyannotated data is not used in the model.
In addi-tion, we seek to align the treatment of data imbal-ance for the binary and multi-class tasks.
We showthat downsampling in general leads to the best pre-diction accuracy but that the alternative modelsprovide complementary information and signifi-cant improvement can be obtained by combiningboth types of models.
We also report significantimprovement of multi-class prediction accuracy,142achieved by using the alternative binary classifiersto perform the task.2 The Penn Discourse TreebankIn the PDTB, discourse relations are viewed as apredicate with two arguments.
The predicate isthe relation, the arguments correspond to the min-imum spans of text whose interpretations are theabstract objects between which the relation holds.Consider the following example of a contrast rela-tion.
The italic and bold fonts mark the argumentsof the relation.Commonwealth Edison said the ruling could force it to slashits 1989 earnings by $1.55 a share.
[Implicit = BY COM-PARISON] For 1988, Commonwealth Edison reportedearnings of $737.5 million, or $3.01 a share.For explicit relations, the predicate is marked bya discourse connective that occurs in the text, i.e.because, however, for example.Implicit relations are marked between adjacentsentences in the same paragraph.
They are inferredby the reader but are not lexically marked.
Alter-native lexicalizations (AltLex) are the ones wherethere is a phrase in the sentence implying the rela-tion but the phrase itself was not one of the explicitdiscourse connectives.
There are 16,224 and 624examples of implicit and AltLex relations, respec-tively, in the PDTB.The sense of discourse relations in the PDTBis organized in a three-tiered hierarchy.
The fourtop level relations are: Temporal (the two argu-ments are related temporally), Comparison (con-trast), Contingency (causal) and Expansion (oneargument is the expansion of the other and contin-ues the context) (Miltsakaki et al., 2008).
Theseare the classes we focus on in our work.Finally, 5,210 pairs of adjacent sentences weremarked as related by an entity relation (EntRel),by virtue of the repetition of the same entity ortopic.
EntRels were marked only if no other rela-tion could be identified and they are not considereda discourse relation, rather an alternative discoursephenomena related to entity coherence (Grosz etal., 1995).
There are 254 pairs of sentences whereno discourse relation was identified (NoRel).Pitler et al.
(2008) has shown that performanceas high as 93% in accuracy can be easily achievedfor the explicit relations, because the connective it-self is a highly informative feature.
Efforts in iden-tifying the argument spans have also yielded highaccuracies (Lin et al., 2014; Elwell and Baldridge,2008; Ghosh et al., 2011).However, in the absence of a connective, recog-nizing non-explicit relations, which includes im-plicit relations, alternative lexicalizations, entityrelation and no relation present, has proven to be areal challenge.
Prior work on supervised implicitdiscourse recognition studied a wide range of fea-tures including lexical, syntactic, verb classes, se-mantic groups via General Inquirer and polarity(Pitler et al., 2009; Lin et al., 2009).
Park andCardie (2012) studied the combination of featuresand achieved better performance with a differentcombination for each individual relation.
Meth-ods for improving the sparsity of lexical represen-tations have been proposed (Hernault et al., 2010;Biran and McKeown, 2013), as well as web-drivenapproaches which reduce the problem to explicitrelation recognition (Hong et al., 2012).Remarkably, no prior work has discussed thehighly skewed class distribution of discourse re-lation types.
The tacitly adopted solution has beento downsample the negative examples for one-vs-all binary classification aimed at discovering if aparticular relation holds and keeping the full train-ing set for multi-class prediction.To highlight the problem, in Table 1 we showthe distribution of implicit relation classes in theentire PDTB.
In our work, we aim to develop clas-sifiers to identify the four top-level relations listedin the table1.# of samples PercentageTemporal 1038 4.3%Comparison 2550 11.3%Contingency 4532 20%Expansion 9082 40%Table 1: Distribution of implicit relations in thePDTB.3 Experimental settingsIn our experiments, we used all non-explicit in-stances in the PDTB sections 2-19 for training andthose in sections 20-24 for testing.
Like most stud-ies, we kept sections 0-1 as development set.
Inorder to ensure we have a large enough test set toproperly perform tests for statistical significanceover F scores and balanced accuracies, we did notfollow previous work (Lin et al., 2014; Park andCardie, 2012) that used only section 23 or sec-tions 23-24 for testing.
Also, the traditional ruleof thumb is to split the available data into training1The rest of the data are EntRel/NoRel.143and testing sets with 80%/20% ratio.
Our choiceensures that this is the case for all of the relations.The only features that we use in our experimentsare production rules.
We exclude features that oc-cur fewer than five times in the training set.
Pro-duction rules are the state-of-the-art representationfor discourse relation recognition.
This represen-tation leads to only slightly lower results than asystem including a much larger variety of featuresin the first end-to-end PDTB style discourse parser(Lin et al., 2014) .The production rule representation is based onthe constituency parse of the arguments and in-cludes both syntactic and lexical information.
Aproduction rule is the parent with an left-to-rightordered list of all of its children in the parse tree(for example, S?NP VP).
All non-terminal nodesare included as a parent, from the sentence headto the part-of-speech of a terminal.
Thus wordsthat occur in each sentence augmented with theirpart of speech are part of the representation (forexample, NN?company), along with more gen-eral structures of the sentence corresponding toproduction rules with only non-terminals on theright-hand side.There are three features corresponding to a pro-duction rule, tracking if the rule occurs in the parseof first argument of the relation, in the second, orin both.Adopting this representation allows us to fo-cus on the issue of class imbalance and howthe choices of tackling this problem affect even-tual prediction performance.
Our findings arerepresentation-independent and will most likelyextend to other representations.We train and evaluate a binary classifier withlinear kernel using SVMLight2(Joachims, 1999)for each of the four top level classes of relations:Temporal, Comparison, Contingency and Expan-sion.
We used SVM-Multiclass3for standard mul-tiway classification.
We also develop and evaluatetwo approaches for multiway classification for thefour classes plus the additional class of entity rela-tion and no relation.Due to the uneven distribution of classes, we useprecision, recall and f-measure to measure binaryprediction performance.
For multiway classifica-2http://svmlight.joachims.org/3http://svmlight.joachims.org/svm multiclass.htmltion, we use the balanced accuracy (BAC):BAC =1kk?i=1cini, (1)where k is the number of relations to predict, ciisthe number of instances of relation i that are cor-rectly predicted, niis the total number of instancesof relation i.Balanced accuracy (or averaged accuracy) hasa more intuitive interpretation than F-measure.
Itis not dominated by the majority class as much asstandard accuracy is.
For example for two classes,in a dataset where one class makes up 90% of thedata, predicting the majority class has accuracy of90% but balanced accuracy of 45%.In testing, we keep the original distribution in-tact and make predictions for all pairs of adjacentsentences in the same paragraph that do not havean explicit discourse relation4.
In order to per-form tests for statistical significance over F scores,precision, recall and balanced accuracies, we ran-domly partitioned the testing data into 10 groups.We kept the data distribution in each group asclose as possible to the overall testing set.
To com-pare the performance of two different systems, apaired t-test is performed over these 10 groups.4 Why downsampling?Binary classification As mentioned in the pre-vious sections, in all prior work of supervised im-plicit relation classification, the technique to copewith highly skewed distribution for binary classi-fication is to downsample the negative training in-stances so that the sizes of positive and negativeclasses are equal.
The reason for doing so is thatthe classifier can achieve high accuracy just by ig-noring the small class, learning nothing and awayspredicting the larger class.
We illustrate this ef-fect in Table 2.
Without downsampling, the onlyreasonable F measure is achieved for Expansionwhere the smaller class accounts for 40% of thedata.
Note that with downsampling, the recogni-tion of Expansion is also improved considerably.Multiway classification In prior work multiwayclassifiers are trained on all available training data.As we just saw, however, this approach leads4Note the contrast with prior work where in some casesEntRels are part of Expansion, or in some cases the perfor-mance of methods is evaluated only on pairs of sentenceswhere a discourse relation holds, excluding EntRels, NoRelsor AltLexs.144All data DownsampleTemp.
0 (nan/0.0) 15.52 (8.8/65.4)Comp.
2.17 (71.4/1.1) 27.65 (17.3/69.2)Cont.
0.96 (100.0/0.5) 47.14 (34.5/74.5)Exp.
44.27 (54.9/37.1) 55.42 (49.3/63.3)Table 2: F measure (precision/recall) of binaryclassification: including all of the data vs down-sampling.to poor results in identifying the core Temporal,Comparison and Contingency discourse relations.We propose an alternative approach to multi-classprediction, based on binary one-against-all classi-fiers for each of the four discourse relations, in-cluding Expansion, trained using downsampling.The intuition is that an instance of adjacent sen-tences Siis assigned to a discourse relation Rjif the binary classifier for Rjrecognizes Sias apositive instance with confidence higher than thatof the classifiers for other relations.
If none ofthe binary classifiers recognizes the instance as apositive example, the instance is assigned to classEntRel/NoRel.
This approach modifies the waymulti-class classifiers are normally constructed byincluding downsampling and having special treat-ment of the EntRel/NoRel class.Specifically, we first use the four binary classi-fiers Cjfor each relation j to get the confidence pjof instance i belonging to class j.
We approximatethe confidence by the distance to the hyperplaneseparating the two classes, which SVMLight pro-vides.
If at least one pjis greater than zero, assigninstance i the class k where the classifier confi-dence is the highest.
If none of the pj?s is greaterthan zero, assign i to be the EntRel/NoRel class.We show balanced accuracies of these two mul-tiway classification methods in Table 3.Multiway SVM One-Against-All5-way 32.58 37.15Table 3: Balanced accuracies for SVM-Multiclassand one-against-all 5-way classification.The one-against-all approach leads to 5% abso-lute improvement in performance.
A t-test anal-ysis confirms that the difference is significant atp < 0.05.
Note that the improvement comes en-tirely from acknowledging that skewed class dis-tribution poses a problem for the task and by ad-dressing the problem in the same way for binaryand multi-class prediction.5 Using more dataAlthough downsampling gives much better per-formance than simply including all of the origi-nal data, it still appears to be an undesirable so-lution because in essence it throws away much ofthe annotated data.
This means that for the small-est relations, as much as 90% of the data willnot be used.
Feature selection and feature val-ues are computed only based on this much smallerdataset and do not properly reflect the informationabout discourse relations encoded in the PDTB.
Inthis section we first discuss some of the widelyused methods for handling skewed data distribu-tion, that is, weighted cost and upsampling.
First,we show that with highly skewed distributions, thetwo methods result in almost identical classifiers.Then we introduce a method for feature selectionand shaping which computes feature weights onthe full dataset and thus captures much of the in-formation lost in downsampling.5.1 Weighted cost and upsamplingA number of methods have been developed forthe skewed distribution problem (Morik et al.,1999; Veropoulos et al., 1999; Akbani et al., 2004;Batista et al., 2004; Chawla et al., 2002).
Here wehighlight weighted cost and random upsampling,which are known to work well and widely used.The idea behind weighted cost (Morik et al.,1999; Veropoulos et al., 1999) is to use weightsto adjust the penalties for false positives and falsenegatives in the objective function.
As in Moriket al.
(1999), we specify the cost factor to be theratio of the size of the negative class vs. that of thepositive class.In the case of upsampling, instead of ran-domly downsampling negative instances, positiveinstances are randomly upsampled.
In our exper-iments we randomly replicate positive instanceswith replacement until the numbers of positive andnegative instances are equal to each other.The binary and multiway classification resultsfor these two methods are shown in Table 4 andTable 5.
For binary classification, we can see sig-nificantly higher F score for the smallest Temporalclass.
Weighted cost is also able to achieve signif-icantly better F-score for Expansion.
For Compar-ison and Contingency, the F-scores are similar tothat of plain downsampling.
The balanced accura-cies of multi-class classification with either meth-ods are lower, or significantly lower in the case of145weighted cost, than using downsampling in one-against-all manner.Upsample WeightCostTemp.
20.35* (16.8/25.9) 20.61* (16.9/26.3)Comp.
28.11 (20.6/44.5) 28.38 (19.9/49.6)Cont.
46.46 (37.4/61.3) 46.36 (34.6/70.1)Exp.
54.93 (50.3/60.5) 57.43* (43.9/83.1)Table 4: F-measure (precision/recall) of binaryclassification: upsampling vs. weighted cost.For Temporal and Comparison relations listedin Table 4, we noticed an interesting similaritybetween the F and precision values of upsam-pling and weighted cost.
To quantify this simi-larity, we calculated the Q-statistic (Kuncheva andWhitaker, 2003) between the two classifiers.
TheQ-statistic is a measurement of classifier agree-ment raging between -1 and 1, defined as:Qw,u=N11N00?N01N10N11N00+N01N10(2)Where w denotes the system using weighted cost,u denotes the upsampling system.
N11means bothsystems make a correct prediction, N00meansboth systems are incorrect, N10means w is incor-rect but u is correct, and N01means w is correctbut u is incorrect.We have the following Q statistics: Tempo-ral: 0.999, Comparison: 0.9938, Contingency:0.9746, Expansion: 0.7762.
These are good in-dicators that for highly skewed relations, the twomethods give classifiers that behave almost identi-cally on the test data.
In the discussions that fol-low, we discuss only weighted cost to avoid redun-dancy.5.2 Feature selection and shapingWhile weighted cost or upsampling can give bet-ter performance over downsampling for some rela-tions, their disadvantages towards multi-class clas-sification and the obvious favor towards the major-ity class give rise to the following question: is itpossible to inform the classifier of the informationencoded in the annotation of all of the data whilestill using downsampling to handle the skewedclass distribution?
Our proposal is feature valueaugmentation.
Here we introduce a relational ma-trix in which we calculate augmented feature val-ues via feature shaping.
We first compute the val-ues of features on the entire training set, then usethe downsampled set for training with these val-ues.
In this way we pass on to the classifiers infor-mation about the relative importance of featuresgleaned from the entire training data.5.2.1 Feature shapingThe idea of feature shaping was introduced in thecontext of improving the performance of linearSVMs (Forman et al., 2009).
In linear SVMsthe prediction is based on a linear combination ofweight?feature values.
The sign of weight indi-cates the preference for a class (positive or nega-tive), the value of the feature should correspond tohow strongly it indicates that class.
Thus, featuresthat are strongly discriminative should have highvalues so that they can contribute more to the finalclass decision.
Here we augment feature valuesfor a relation according to the following criteria:1.
Features are considered ?good?
if they stronglyindicate the presence of the relation; 2.
Featuresare considered ?good?
if they strongly indicate theabsence of the relation; 3. features are considered?bad?
if their presence give no information abouteither the presence or the absence of the relation.To capture this information, we first construct arelation matrix M with each entry Mijdefined asthe conditional probability of relationRjgiven thefeature Ficomputed as the maximum likelihoodestimate from the full training set:Mij= P (Rj|Fi)Each column of the relation matrix captures thepredictive power of each feature to a certain re-lation.
A feature with value Mijhigher than thecolumn mean indicates that it is predictive for thepresence of relation j, while a feature with Mijlower than the mean is predictive for its absence;the strength of such indication depends on how faraway Mijis from the mean: the further away it is,the more valuable this feature should be for rela-tion j.
With this idea we give the following aug-mented value for each feature:M?ij={Mij, if Mij?
?j.
?j+ (?j?Mij), if Mij< ?j.
(3)where ?jis the mean of the jth column corre-sponding to the jth relation.Given a feature Fi, very small and very highprobabilities of a certain relation j, i.e., P (Rj|Fi),are both useful information.
However, in linearSVMs, lower values of a feature would mean thatit contributes less to the decision of the class.
By146feature shaping, we allow features that strongly in-dicate the absence of a class to influence the deci-sion and rely on the classifier to identify the nega-tive association and reflect it by assigning a nega-tive weight to these features.When constructing the relation matrix, we usedthe top four relation classes along with an En-tRel/NoRel class.
We computed the matrix beforedownsampling to preserve the natural data distri-bution and features that strongly indicate the ab-sence of a class, then downsample the negativedata just like the previous downsampling setting.5.2.2 Feature selectionThe relation matrix also provides information forfeature selection using a binomial test for signifi-cance, B(n,N, p), which gives the probability ofobserving a feature n times in N instances of arelation if the probability of any feature occurringwith the relation is p. For each relation, we use thebinomial test to pick the features that occur signif-icantly more or less often than expected with therelation.
In the binomial test, p is set to be equal tothe probability of that relation in the PDTB train-ing set.
We select only the features which result ina low p-value for the binomial test for at least somerelation.
We used 9-fold cross validation on thetraining data to pick the best p-values for each re-lation individually; all best p-values were between0.1 and 0.2.Result listing Table 5 and Table 6 show the mul-tiway and binary classification performance usingfeature shaping and feature selection.
We alsoshow the precision and recall for binary classifiers.Multiway SVM One-Against-AllAllData 32.58 NADownsample NA 37.15Upsample NA 36.63Weighted Cost NA 34.23Selection 32.52 38.42*Shaping NA 38.81**Shape+Sel NA 39.13**Table 5: Balanced accuracy for multiwaySVM and one-against-all for 5-way classification.One asterisk (*) means significantly better thanweighted cost and upsampling, and two means sig-nificantly better than downsampling, at p < 0.05.For multi-way classification, performing featureshaping leads to significant improvements overdownsampling, upsampling and weighted cost.The binomial method for feature selection thatrelies on the full training data distribution has asimilar effect.
Combined feature shaping and se-lection leads to 2% absolute improvement in dis-course relation recognition.
For binary classifica-tion, though, the improvement is significant onlyfor Temporal.6 Classifier analysis and combination6.1 Discussion of precision and recallA careful examination of Tables 5 and 6 leadsto some intriguing observations.
For the mostskewed relations, if we consider not only the Fmeasure, but also the precision and recall, thereis an interesting difference between the systems.While downsampling has the lowest precision, itgives the highest recall.
The case for weighted costis another story.
For highly skewed relations suchas Temporal and Comparison, it gives the highestprecision and the lowest recall; but as the data setbalances out in downsampling, the classifier shiftstowards high recall and low precision.We can also rank the three feature augmentationtechniques in terms of how much they reflect dis-tributional information in the training data.
Fea-ture selection reflects the training data least amongthe three, because it uses information from all ofthe data to select the features, but the feature val-ues are still either 1 or 0.
Feature shaping engagesmore data because the value of a feature encodesits relative ?effectiveness?
for a relation.
We cansee that feature selection gives slightly higher pre-cision than just downsampling; feature shaping,on the other hand, gives precision and recall val-ues between these two.
This is most obvious insmaller relations, i.e.
Temporal and Comparison.To see if this trend is statistically significant, wedid a paired t-test over the precision and recall foreach system and each relation.
For the Temporalrelation, all systems that use more data have sig-nificantly higher (p < 0.05) precision than thatfor downsampling.
For Comparison, the changesin precision are either significant or tend towardssignificance for three methods: feature shaping(p < 0.1), feature shaping+election (p < 0.1)and weighted cost (p < 0.05).
For Contingency,feature shaping gives an improvement in precisionthat tends toward significance (p < 0.1).
Thedrops in recall using feature shaping or weightedcost for the above three relations are significant(p < 0.05).
For the Expansion relation, being thelargest class with 40% positive data, changes in147Downsample WeightCost Selection Shaping Shape+SelTemp.
15.52 (8.8/65.4) 20.61* (16.9/26.3) 18.47* (10.7/65.9) 20.37* (12.6/53.2) 21.30* (13.7/47.8)Comp.
27.65 (17.3/69.2) 28.38 (19.9/49.6) 26.98 (17.4/60.1) 27.79 (18.3/58.2) 26.92 (18.7/48.2)Cont.
47.14 (34.5/74.5) 46.36 (34.6/70.1) 47.45 (34.7/75.2) 47.62 (35.4/72.9) 46.93 (35.2/70.5)Exp.
55.42 (49.3/63.3) 57.43* (43.9/83.1) 55.52 (49.3/63.5) 55.13 (49.3/62.5) 54.90 (49.2/62.1)Table 6: F score (precision/recall) of classifiers with feature augmentation.
Asterisk(*) means F score orBAC is significantly greater than plain downsampling at p < 0.05.precision and recall with downsampling systemsare not significant; yet weighted cost shifted to-wards predicting more of the positive instances,i.e., giving a significantly higher recall by tradingwith a significantly lower precision (p < 0.05).6.2 Discussion of classifier similarityTo better understand the differences of classi-fier behaviors under the weighted cost and eachdownsampling technique (plain downsampling,feature selection, feature shaping, feature shap-ing+selection), in Table 7 we show the percentageof test instances that the weighted cost system andeach downsample system agree or do not agree.
Inparticular, we study the following situations:1.
The downsample system predicts correctlybut the weighted cost system does not (?D+C-?);2.
The weighted cost system predicts correctlybut the downsample system does not (?D-C+?);3.
Both systems are correct (?D+C+?
).At a glance of the Q statistic, it seems that thesystems are not behaving very differently.
How-ever, as only the sum of disagreements is reflectedin the Q statistic, we look more closely at wherethe systems do not agree in each situation.
If wefocus on the rarer Temporal and Comparison re-lations, first note that in the plain downsamplingvs.
weighted cost, the percentage of test instancesin the ?D+C-?
column is much smaller than thatin the ?D-C+?
column.
This aligns with the aboveobservation that plain downsampling gives muchlower precision for these relations than weightedcost.
Now, as more data is engaged from firstusing feature selection, then using feature shap-ing, then using both, the percentage of instanceswhere both systems predict correctly increase.
Atthe same time, there is a drop in the percentage oftest instances in the ?D-C+?
column.
This trend isalso a reflection of the observation that as moredata is engaged, the precision got higher as therecall drops lower.
As the data gets more evenlydistributed, this phenomenon fades away.
The ta-ble also reveals a subtle difference between fea-ture shaping and feature selection.
Compared toD+C- D-C+ D+C+ Q(%) (%) (%) StatTemporalDownsamp 2.56 28.27 61.47 0.73Selection 2.91 22.04 67.71 0.77Shaping 2.61 13.36 76.39 0.89Sel+Shape 2.83 10.42 79.32 0.90ComparisonDownsamp 5.74 18.24 53.76 0.84Selection 7.72 16.14 55.85 0.80Shaping 6.14 11.95 60.04 0.89Sel+Shape 9.69 10.99 61.01 0.83ContingencyDownsamp 6.88 7.89 58.74 0.93Selection 8.01 8.92 57.70 0.91Shaping 7.07 6.73 59.90 0.94Sel+Shape 8.68 8.13 58.49 0.91ExpansionDownsamp 16.39 8.23 44.66 0.82Selection 17.87 9.71 43.18 0.76Shaping 16.64 8.45 44.44 0.81Sel+Shape 18.36 10.30 42.59 0.73Table 7: Q statistics and agreements (in percent-ages) of each downsampling system vs. weightedcost.
?D?
denotes the respective downsample sys-tem in the left most column; ?C?
denotes theweighted cost system.
A ?+?
means that a systemmakes a correct prediction; a ?-?
means a systemmakes an incorrect prediction.downsampling, feature selection introduces an in-crease in the column ?D+C-?
(i.e.
the weightedcost system makes a mistake but the downsamplesystem is correct).
Feature shaping, on the otherhand, do not necessarily increase this new kind ofdifference between classifiers.6.3 Classifier combinationOur classifier comparisons revealed that for highlyskewed distributions, there are consistent differ-ences in the performance of classifiers obtained byusing the training data in different ways.
It standsto reason that a combination of these classifierswith different strengths will result in an overall im-proved classifier.
This idea is explored here.Suppose on a sample i, the downsampling clas-sifier predicts the target class with confidence pid,and the weighted cost classifier predicts the target148class with confidence pic.
Here again we approx-imate the confidence of the class by the distancefrom the hyperplane dividing the two classes.
Weweight the two predictions and get a new predic-tion confidence by:p?i=?dpid+ ?upic?d+ ?c.
(4)where the ?s are parameters we want to encodehow much we trust each classifier.
To get thesevalues, we train the classifiers and get the accura-cies from each of them on the development set.Since we are using linear SVMs in our experi-ments, we mark the sample as positive if pi> 0,and negative otherwise.The results for the combination are shown in Ta-ble 8.
We include the original performances of theclassifiers by themselves for reference.F measure For Temporal, the combined classi-fier performs better than the original classifiers.We see significant (p < 0.05) improvements overthe corresponding downsampling system and theweighted cost system.
If feature shaping is in-volved in the combination, it is also having bet-ter performance that tend toward significance (p <0.1) over the weighted cost classifier.
For Compar-ison, the benefits of a combined system is also ob-vious for feature shaping and/or selection.
Featureshaping combined with weighted cost gives sig-nificantly (p < 0.05) better performance than ei-ther of them individually, and feature selection andshaping+selection combined with weighted cost isbetter than themselves alone.
For Contingency,though weighted cost do not give better results, theimprovement tends toward significance (p < 0.1)when combined with plain downsampling.
For Ex-pansion where weighted cost gives the lowest pre-cision, combination with other classifiers do notgive significant improvements over F scores.Precision and recall We can also compare theprecision and recall for each system before and af-ter combination.
In all but one cases for Temporaland Comparison, we observe significantly higherprecision and much lower recall after the combi-nation.
The case for Expansion is just the oppositeas expected.7 ConclusionIn this paper, we studied the effect of the use of an-notated data for binary and multiway classificationOriginal CombinedClassifier ClassifierTemporalWeightCost 20.61 (16.9/26.3)Downsamp 15.52 (8.8/65.4) 21.78* (14.9/40.5)Selection 18.47 (10.7/65.9) 22.99* (15.8/42.0)Shaping 20.37 (12.6/53.2) 23.88* (17.5/37.6)Sel+Shape 21.30 (13.7/47.8) 23.72* (17.7/36.1)ComparisonWeightCost 28.38 (19.9/49.6)Downsamp 27.65 (17.3/69.2) 28.72 (19.3/56.4)Selection 26.98 (17.4/60.1) 29.25?
(20.1/54.0)Shaping 27.79 (18.3/58.2) 29.89*+(20.5/54.9)Sel+Shape 26.92 (18.7/48.2) 29.83* (21.3/50.0)ContingencyWeightCost 46.36 (34.6/70.1)Downsamp 47.14 (34.5/74.5) 48.38+(35.9/74.4)Selection 47.45 (34.7/75.2) 47.76+(35.5/72.9)Shaping 47.62 (35.4/72.9) 48.16+(36.0/72.9)Sel+Shape 46.93 (35.2/70.5) 47.37 (35.6/70.7)ExpansionWeightCost 57.43 (43.9/83.1)Downsamp 55.42 (49.3/63.3) 56.61* (46.4/72.7)Selection 55.52 (49.3/63.5) 57.10* (46.5/73.0)Shaping 55.13 (49.3/62.5) 56.74* (46.4/73.0)Sel+Shape 54.90 (49.2/62.1) 57.06* (46.4/74.0)Table 8: Classifier combination results for binaryclassification.
An asterisk(*) means significantlybetter than the corresponding downsampling sys-tem at, and a plus(+) means significantly betterthan weighted cost, at p < 0.05.
Improvementsthat tend toward significance (p < 0.1) are notshown here but are discussed in the text.in supervised implicit discourse relation recogni-tion.
The starting point of our work was to estab-lish the effectiveness of downsampling negativeexamples, which was practiced but not experimen-tally investigated in prior work.
We also evalu-ated alternative solutions to the skewed data prob-lem, as downsampling throws away most of thedata.
We examined the effect of upsampling andweighted cost.
In addition, we introduced the rela-tion matrix to give more emphasis on informativefeatures through augmenting the feature value viafeature shaping.
We found that as we summarizemore detailed information about the data in the fulltraining set, performance for multiway classifica-tion gets better.
We also observed through preci-sion and recall that there are fundamental differ-ences between downsampling and weighted cost,and this difference can be beneficially exploitedby combining the two classifiers.
We showed thatour way of doing such combination gives signifi-cantly higher performance results for binary clas-sification in the case of rarer relations.149ReferencesRehan Akbani, Stephen Kwek, and Nathalie Japkow-icz.
2004.
Applying support vector machines toimbalanced datasets.
In Machine Learning: ECML2004, pages 39?50.Gustavo E. A. P. A. Batista, Ronaldo C. Prati, andMaria Carolina Monard.
2004.
A study of thebehavior of several methods for balancing machinelearning training data.
ACM SIGKDD ExplorationsNewsletter - Special issue on learning from imbal-anced datasets, 6(1):20?29, June.Or Biran and Kathleen McKeown.
2013.
Aggregatedword pair features for implicit discourse relation dis-ambiguation.
In Proceedings of the 51st AnnualMeeting of the Association for Computational Lin-guistics (ACL): Short Papers, pages 69?73.Nitesh V. Chawla, Kevin W. Bowyer, Lawrence O.Hall, and W. Philip Kegelmeyer.
2002.
SMOTE:Synthetic minority over-sampling technique.
Jour-nal of Artificial Intelligence Research, 16(1):321?357, June.R.
Elwell and J. Baldridge.
2008.
Discourse connec-tive argument identification with connective specificrankers.
In IEEE International Conference on Se-mantic Computing (IEEE-ICSC), pages 198 ?205.George Forman, Martin Scholz, and Shyamsundar Ra-jaram.
2009.
Feature shaping for linear SVM classi-fiers.
In Proceedings of the 15th ACM InternationalConference on Knowledge Discovery and Data Min-ing (KDD), pages 299?308.Sucheta Ghosh, Richard Johansson, Giuseppe Ric-cardi, and Sara Tonelli.
2011.
Shallow discourseparsing with conditional random fields.
In Pro-ceedings of the 5th International Joint Conferenceon Natural Language Processing (IJCNLP), pages1071?1079.Barbara J. Grosz, Scott Weinstein, and Aravind K.Joshi.
1995.
Centering: A framework for model-ing the local coherence of discourse.
ComputationalLinguistics, 21:203?225.Hugo Hernault, Danushka Bollegala, and MitsuruIshizuka.
2010.
A semi-supervised approach to im-prove classification of infrequent discourse relationsusing feature vector extension.
In Proceedings of the2010 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pages 399?409.Yu Hong, Xiaopei Zhou, Tingting Che, Jianmin Yao,Qiaoming Zhu, and Guodong Zhou.
2012.
Cross-argument inference for implicit discourse relationrecognition.
In Proceedings of the 21st ACM Inter-national Conference on Information and KnowledgeManagement (CIKM), pages 295?304.Thorsten Joachims.
1999.
Making large-scale supportvector machine learning practical.
In Advances inkernel methods, pages 169?184.Ludmila I. Kuncheva and Christopher J. Whitaker.2003.
Measures of diversity in classifier ensemblesand their relationship with the ensemble accuracy.Machine Learning, 51(2):181?207, May.Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng.
2009.Recognizing implicit discourse relations in the PennDiscourse Treebank.
In Proceedings of the 2009Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP), pages 343?351.Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2014.
APDTB-styled end-to-end discourse parser.
NaturalLanguage Engineering, 20:151?184, 4.Eleni Miltsakaki, Livio Robaldo, Alan Lee, and Ar-avind Joshi.
2008.
Sense annotation in the PennDiscourse Treebank.
In Proceedings of the 9thInternational Conference on Computational Lin-guistics and Intelligent Text Processing (CICLing),pages 275?286.Katharina Morik, Peter Brockhausen, and ThorstenJoachims.
1999.
Combining statistical learningwith a knowledge-based approach - a case study inintensive care monitoring.
In Proceedings of the Six-teenth International Conference on Machine Learn-ing (ICML), pages 268?277.Joonsuk Park and Claire Cardie.
2012.
Improving im-plicit discourse relation recognition through featureset optimization.
In Proceedings of the 13th AnnualMeeting of the Special Interest Group on Discourseand Dialogue (SIGDIAL), pages 108?112.Emily Pitler, Mridhula Raghupathy, Hena Mehta, AniNenkova, Alan Lee, and Aravind Joshi.
2008.
Eas-ily identifiable discourse relations.
In Proceed-ings of the International Conference on Computa-tional Linguistics (COLING): Companion volume:Posters, pages 87?90.Emily Pitler, Annie Louis, and Ani Nenkova.
2009.Automatic sense prediction for implicit discourse re-lations in text.
In Proceedings of the Joint Confer-ence of the 47th Annual Meeting of the ACL andthe 4th International Joint Conference on NaturalLanguage Processing of the AFNLP (ACL-IJCNLP),pages 683?691.Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-sakaki, Livio Robaldo, Aravind Joshi, and BonnieWebber.
2008.
The Penn Discourse TreeBank 2.0.In Proceedings of the Sixth International Conferenceon Language Resources and Evaluation (LREC).Konstantinos Veropoulos, Colin Campbell, and NelloCristianini.
1999.
Controlling the sensitivity of sup-port vector machines.
In Proceedings of the Inter-national Joint Conference on Artificial Intelligence(IJCAI), volume 1999, pages 55?60.150
