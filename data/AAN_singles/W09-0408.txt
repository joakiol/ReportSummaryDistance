Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 56?60,Athens, Greece, 30 March ?
31 March 2009. c?2009 Association for Computational LinguisticsMachine Translation System Combination with Flexible Word OrderingKenneth Heafield, Greg Hanneman, Alon LavieLanguage Technologies Institute, Carnegie Mellon University5000 Forbes Avenue, Pittsburgh, PA 15213, USA{kheafiel,ghannema,alavie}@cs.cmu.eduAbstractWe describe a synthetic method for com-bining machine translations produced bydifferent systems given the same input.One-best outputs are explicitly alignedto remove duplicate words.
Hypothesesfollow system outputs in sentence order,switching between systems mid-sentenceto produce a combined output.
Experi-ments with the WMT 2009 tuning datashowed improvement of 2 BLEU and 1METEOR point over the best Hungarian-English system.
Constrained to data pro-vided by the contest, our system was sub-mitted to the WMT 2009 shared systemcombination task.1 IntroductionMany systems for machine translation, with dif-ferent underlying approaches, are of competitivequality.
Nonetheless these approaches and sys-tems have different strengths and weaknesses.
Byoffsetting weaknesses with strengths of other sys-tems, combination can produce higher quality thandoes any component system.One approach to system combination uses con-fusion networks (Rosti et al, 2008; Karakos etal., 2008).
In the most common form, a skele-ton sentence is chosen from among the one-bestsystem outputs.
This skeleton determines the or-dering of the final combined sentence.
The re-maining outputs are aligned with the skeleton, pro-ducing a list of alternatives for each word in theskeleton, which comprises a confusion network.
Adecoder chooses from the original skeleton wordand its alternatives to produce a final output sen-tence.
While there are a number of variations onthis theme, our approach differs fundamentally inthat the effective skeleton changes on a per-phrasebasis.Our system is an enhancement of our previouswork (Jayaraman and Lavie, 2005).
A hypothesisuses words from systems in order, switching be-tween systems at phrase boundaries.
Alignmentsand a synchronization method merge meaning-equivalent output from different systems.
Hy-potheses are scored based on system confidence,alignment support, and a language model.We contribute a few enhancements to this pro-cess.
First, we introduce an alignment-sensitivemethod for synchronizing available hypothesis ex-tensions across systems.
Second, we pack similarpartial hypotheses, which allows greater diversityin our beam search while maintaining the accuracyof n-best output.
Finally, we describe an improvedmodel selection process that determined our sub-missions to the WMT 2009 shared system combi-nation task.The remainder of this paper is organized as fol-lows.
Section 2 describes the system with empha-sis on our modifications.
Tuning, our experimen-tal setup, and submitted systems are described inSection 3.
Section 4 concludes.2 SystemThe system consists of alignment (Section 2.1)and phrase detection (Section 2.2) followed by de-coding.
The decoder constructs hypothesis sen-tences one word at a time, starting from the left.
Apartially constructed hypothesis comprises:Word The most recently decoded word.
Initially,this is the beginning of sentence marker.Used The set of used words from each system.Initially empty.Phrase The current phrase constraint from Sec-tion 2.2, if any.
The initial hypothesis is notin a phrase.Features Four feature values defined in Section2.4 and used in Section 2.5 for beam search56and hypothesis ranking.
Initially, all featuresare 1.Previous A set of preceding hypothesis pointersdescribed in Section 2.5.
Initially empty.The leftmost unused word from each systemcorresponds to a continuation of the partial hy-pothesis.
Therefore, for each system, we extend apartial hypothesis by appending that system?s left-most unused word, yielding several new hypothe-ses.
The appended word, and those aligned with it,are marked as used in the new hypothesis.
Sincesystems do not align perfectly, too few words maybe marked as used, a problem addressed in Sec-tion 2.3.
As described in Section 2.4, hypothesesare scored using four features based on alignment,system confidence, and a language model.
Sincethe search space is quite large, we use these partialscores for a beam search, where the beam containshypotheses of equal length.
This space containshypotheses that extend in precisely the same way,which we exploit in Section 2.5 to increase diver-sity.
Finally, a hypothesis is complete when theend of sentence marker is appended.2.1 AlignmentSentences from different systems are aligned inpairs using a modified version of the METEOR(Banerjee and Lavie, 2005) matcher.
This iden-tifies alignments in three phases: exact matchesup to case, WordNet (Fellbaum, 1998) morphol-ogy matches, and shared WordNet synsets.
Thesesources of alignments are quite precise and unableto pick up on looser matches such as ?mentioned?and ?said?
that legitimately appear in output fromdifferent systems.
Artificial alignments are in-tended to fill gaps by using surrounding align-ments as clues.
If a word is not aligned to anyword in some other sentence, we search left andright for words that are aligned into that sentence.If these alignments are sufficiently close to eachother in the other sentence, words between themare considered for artificial alignment.
An arti-ficial alignment is added if a matching part ofspeech is found.
The algorithm is described fullyby Jayaraman and Lavie (2005).2.2 PhrasesSwitching between systems is permitted outsidephrases or at phrase boundaries.
We find phrasesin two ways.
Alignment phrases are maximallylong sequences of words which align, in the sameorder and without interruption, to a word se-quence from at least one other system.
Punctua-tion phrases place punctuation in a phrase with thepreceding word, if any.
When the decoder extendsa hypothesis, it considers the longest phrase inwhich no word is used.
If a punctuation phrase ispartially used, the decoder marks the entire phraseas used to avoid extraneous punctuation.2.3 SynchronizationWhile phrases address near-equal pieces of trans-lation output, we must also deal with equallymeaningful output that does not align.
The im-mediate effect of this issue is that too few wordsare marked as used by the decoder, leading to du-plication in the combined output.
In addition, par-tially aligned system output results in lingering un-used words between used words.
Often these arefunction words that, with language model scoring,make output unnecessarily verbose.
To deal withthis problem, we expire lingering words by mark-ing them as used.
Specifically, we consider thefrontier of each system, which is the leftmost un-used word.
If a frontier lags behind, words as usedto advance the frontier.
Our two methods for syn-chronization differ in how frontiers are comparedacross systems and the tightness of the constraint.Previously, we measured frontiers from the be-ginning of sentence.
Based on this measurement,the synchronization constraint requires that thefrontiers of each system differ by at most s. Equiv-alently, a frontier is lagging if it is more than swords behind the rightmost frontier.
Lagging fron-tiers are advanced until the synchronization con-straint becomes satisfied.
We found this methodcan cause problems in the presence of variablelength output.
When the variability in outputlength exceeds s, proper synchronization requiresdistances between frontiers greater than s, whichthis constraint disallows.Alignments indicate where words are syn-chronous.
Words near an alignment are also likelyto be synchronous even without an explicit align-ment.
For example, in the fragments ?even moreserious, you?
and ?even worse, you?
from WMT2008, ?serious?
and ?worse?
do not align butdo share relative position from other alignments,suggesting these are synchronous.
We formalizethis by measuring the relative position of fron-tiers from alignments on each side.
For example,57if the frontier itself is aligned then relative posi-tion is zero.
For each pair of systems, we checkif these relative positions differ by at most s un-der an alignment on either side.
Confidence in asystem?s frontier is the sum of the system?s ownconfidence plus confidence in systems for whichthe pair-wise constraint is satisfied.
If confidencein any frontier falls below 0.8, the least confidentlagging frontier is advanced.
The process repeatsuntil the constraint becomes satisfied.2.4 ScoresWe score partial and complete hypotheses usingsystem confidence, alignments, and a languagemodel.
Specifically, we have four features whichoperate at the word level:Alignment Confidence in the system from whichthe word came plus confidence in systems towhich the word aligns.Language Model Score from a suffix array lan-guage model (Zhang and Vogel, 2006)trained on English from monolingual andFrench-English data provided by the contest.N -Gram(13)order?ngramusing language modelorder and length of ngram found.Overlap overlaporder?1 where overlap is the length ofintersection between the preceding and cur-rent n-grams.The N -Gram and Overlap features are intended toimprove fluency across phrase boundaries.
Fea-tures are combined using a log-linear modeltrained as discussed in Section 3.
Hypotheses arescored using the geometric average score of eachword in the hypothesis.2.5 SearchOf note is that a word?s score is impacted only byits alignments and the n-gram found by the lan-guage model.
Therefore two partial hypothesesthat differ only in words preceding the n-gram andin their average score are in some sense duplicates.With the same set of used words and same phraseconstraint, they extend in precisely the same way.In particular, the highest scoring hypothesis willnever use a lower scoring duplicate.We use duplicate detecting beam search to ex-plore our hypothesis space.
A beam contains par-tial hypotheses of the same length.
Duplicatehypotheses are detected on insertion and packed,with the combined hypothesis given the highestscore of those packed.
Once a beam contains thetop scoring partial hypotheses of length l, thesehypotheses are extended to length l+1 and placedin another beam.
Those hypotheses reaching endof sentence are placed in a separate beam, which isequivalent to packing them into one final hypoth-esis.
Once we remove partial hypothesis that didnot extend to the final hypothesis, the hypothesesare a lattice connected by parent pointers.While we submitted only one-best hypotheses,accurate n-best hypotheses are important for train-ing as explained in Section 3.
Unpacking the hy-pothesis lattice into n-best hypotheses is guidedby scores stored in each hypothesis.
For this task,we use an n-best beam of paths from the end ofsentence hypothesis to a partial hypothesis.
Pathsare built by induction, starting with a zero-lengthpath from the end of sentence hypothesis to itself.The top scoring path is removed and its terminalhypothesis is examined.
If it is the beginning ofsentence, the path is output as a complete hypoth-esis.
Otherwise, we extend the path to each parenthypothesis, adjusting each path score as necessary,and insert into the beam.
This process terminateswith n complete hypotheses or an empty beam.3 TuningGiven the 502 sentences made available for tun-ing by WMT 2009, we selected feature weights forscoring, a set of systems to combine, confidence ineach selected system, and the type and distance sof synchronization.
Of these, only feature weightscan be trained, for which we used minimum errorrate training with version 1.04 of IBM-style BLEU(Papineni et al, 2002) in case-insensitive mode.We treated the remaining parameters as a modelselection problem, using 402 randomly sampledsentences for training and 100 sentences for eval-uation.
This is clearly a small sample on whichto evaluate, so we performed two folds of cross-validation to obtain average scores over 200 un-trained sentences.
We chose to do only two foldsdue to limited computational time and a desire totest many models.We scored systems and our own output usingcase-insensitive IBM-style BLEU 1.04 (Papineniet al, 2002), METEOR 0.6 (Lavie and Agarwal,2007) with all modules, and TER 5 (Snover etal., 2006).
For each source language, we ex-58In Sync s BLEU METE TER Systems and Confidencescz length 8 .236 .507 59.1 google .46 cu-bojar .27 uedin .27cz align 5 .226 .499 57.8 google .50 cu-bojar .25 uedin .25cz align 7 .211 .508 65.9 cu-bojar .60 google .20 uedin .20cz .231 .504 57.8 googlede length 7 .255 .531 54.2 google .40 uka .30 stuttgart .15 umd .15de length 6 .260 .532 55.2 google .50 systran .25 umd .25de align 9 .256 .533 55.5 google .40 uka .30 stuttgart .15 umd .15de align 6 .200 .514 54.2 google .31 uedin .22 systran .18 umd .16 uka .14de .244 .523 57.5 googlees align 8 .297 .560 52.7 google .75 uedin .25es length 5 .289 .548 52.1 google .50 talp-upc .17 uedin .17 rwth .17es .297 .558 52.7 googlefr align 6 .329 .574 49.9 google .70 lium1 .30fr align 8 .314 .596 48.6 google .50 lium1 .30 limsi1 .20fr length 8 .323 .570 48.5 google .50 lium1 .25 limsi1 .25fr .324 .576 48.7 googlehu length 5 .162 .403 69.2 umd .50 morpho .40 uedin .10hu length 8 .158 .407 69.5 umd .50 morpho .40 uedin .10hu align 7 .153 .392 68.0 umd .33 morpho .33 uedin .33hu .141 .391 66.1 umdxx length 5 .326 .584 49.6 google-fr .61 google-es .39xx align 4 .328 .580 49.5 google-fr .80 google-es .20xx align 5 .324 .576 48.6 google-fr .61 google-es .39xx align 7 .319 .587 51.1 google-fr .50 google-es .50xx .324 .576 48.7 google-frTable 1: Combination models used for submission to WMT 2009.
For each language, we list our pri-mary combination, contrastive combinations, and a high-scoring system for comparison in italic.
Alltranslations are into English.
The xx source language combines translations from different languages,in our case French and Spanish.
Scores from BLEU, METEOR, and TER are the average of two cross-validation folds with 100 evaluation sentences each.
Numbers following system names indicate con-trastive systems.
More evaluation, including human scores, will be published by WMT.perimented with various sets of high-scoring sys-tems to combine.
We also tried confidence val-ues proportional to various powers of BLEU andMETEOR scores, as well as hand-picked values.Finally we tried both variants of synchronizationwith values of s ranging from 2 to 9.
In total, 405distinct models were evaluated.
For each sourcesource language, our primary system was chosenby performing well on all three metrics.
Modelsthat scored well on individual metrics were sub-mitted as contrastive systems.
In Table 1 we reportthe models underlying each submitted system.4 ConclusionWe found our combinations are quite sensitive topresence of and confidence in the underlying sys-tems.
Further, we show the most improvementwhen these systems are close in quality, as is thecase with our Hungarian-English system.
Thetwo methods of synchronization were surprisinglycompetitive, a factor we attribute to short sentencelength compared with WMT 2008 Europarl sen-tences.
Opportunities for further work include per-sentence system confidence, automatic training ofmore parameters, and different alignment models.We look forward to evaluation results from WMT2009.AcknowledgmentsThe authors wish to thank Jonathan Clark fortraining the language model and other assistance.This work was supported in part by the DARPAGALE program and by a NSF Graduate ResearchFellowship.59ReferencesSatanjeev Banerjee and Alon Lavie.
2005.
METEOR:An automatic metric for MT evaluation with im-proved correlation with human judgments.
In Proc.ACLWorkshop on Intrinsic and Extrinsic EvaluationMeasures for Machine Translation and/or Summa-rization, pages 65?72.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
MIT Press.Shyamsundar Jayaraman and Alon Lavie.
2005.Multi-engine machine translation guided by explicitword matching.
In Proc.
EAMT, pages 143?152.Damianos Karakos, Jason Eisner, Sanjeev Khudanpur,and Markus Dreyer.
2008.
Machine translation sys-tem combination using ITG-based alignments.
InProc.
ACL-08: HLT, Short Papers (Companion Vol-ume), pages 81?84.Alon Lavie and Abhaya Agarwal.
2007.
ME-TEOR: An automatic metric for MT evaluation withhigh levels of correlation with human judgments.In Proc.
Second Workshop on Statistical MachineTranslation, pages 228?231, Prague, Czech Repub-lic, June.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A method for automaticevalution of machine translation.
In Proc.
40th An-nual Meeting of the Association for ComputationalLinguistics, pages 311?318, Philadelphia, PA, July.Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,and Richard Schwartz.
2008.
Incremental hypothe-sis alignment for building confusion networks withapplication to machine translation system combina-tion.
In Proc.
Third Workshop on Statistical Ma-chine Translation, pages 183?186.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A studyof translation edit rate with targeted human annota-tion.
In Proc.
Seventh Conference of the Associa-tion for Machine Translation in the Americas, pages223?231, Cambridge, MA, August.Ying Zhang and Stephan Vogel.
2006.
Suffix arrayand its applications in empirical natural languageprocessing.
Technical Report CMU-LTI-06-010,Language Technologies Institute, School of Com-puter Science, Carnegie Mellon University, Pitts-burgh, PA, Dec.60
