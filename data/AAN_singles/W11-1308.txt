Proceedings of the Workshop on Distributional Semantics and Compositionality (DiSCo?2011), pages 43?47,Portland, Oregon, 24 June 2011. c?2011 Association for Computational LinguisticsDetecting compositionality using semanticvector space models based on syntactic context.Shared task system description?Guillermo GarridoNLP & IR Group at UNEDMadrid, Spainggarrido@lsi.uned.esAnselmo Pen?asNLP & IR Group at UNEDMadrid, Spainanselmo@lsi.uned.esAbstractThis paper reports on the participation of theNLP GROUP at UNED in the DiSCo?2011compositionality evaluation task.
The aim ofthe task is to predict compositionality judge-ments assigned by human raters to candidatephrases, in English and German, from threecommon grammatical relations: adjective-noun, subject-verb and subject-object.Our participation is restricted to adjective-noun relations in English.
We explore theuse of syntactic-based contexts obtained fromlarge corpora to build classifiers that modelthe compositionality of the semantics of suchpairs.1 IntroductionThis paper reports on the NLP GROUP at UNED ?sparticipation in DiSCo?2011 Shared Task.
We at-tempt to model the notion of compositionality fromanalyzing language use in large corpora.
In doingthis, we are assuming the distributional hypothesis:words that occur in similar contexts tend to havesimilar meanings (Harris, 1954).
For a review ofthe field, see (Turney and Pantel, 2010).1.1 ApproachIn previous approaches to compositionality detec-tion, different kinds of information have been used:morphological, lexical, syntactic, and distributional.?
This work has been partially supported by the SpanishMinistry of Science and Innovation, through the project Holo-pedia (TIN2010-21128-C02), and the Regional Government ofMadrid, through the project MA2VICMR (S2009/TIC1542).For our participation, we are interested in exploring,exclusively, the reach of pure syntactic informationto explain semantics.Our approach draws from the BackgroundKnowledge Base representation of texts introducedin (Pen?as and Hovy, 2010).
We hypothesize thatbehind syntactic dependencies in natural languagethere are semantic relations; and that syntactic con-texts can be leveraged to represent meaning, particu-larly of nouns.
A system could learn these semanticrelations from large quantities of natural languagetext, to build an independent semantic resource, aBackground Knowledge Base (BKB) (Pen?as andHovy, 2010).From a dependency-parsed corpus, we automat-ically harvest meaning-bearing patterns, matchingthe dependency trees to a set of pre-specified syn-tactic patterns, similarly to (Pado and Lapata, 2007).Patterns are matched to dependency trees to producepropositions, carriers of minimal semantic units.Their frequency in the collection is the fundamen-tal source of our representation.Our participation, due to time constraints, is re-stricted to adjective-noun pairs in English.2 System DescriptionOur hypothesis can be spelled out as: words (orword compounds) with similar syntactic contexts aresemantically similar.The intuition behind our approach is that non-compositional compounds are units of meaning.Then, the meaning of an adjective-noun combina-tion that is not compositional should be differentfrom the meaning of the noun alone; for similar43approaches, see (Baldwin et al, 2003; Katz andGiesbrecht, 2006; Mitchell and Lapata, 2010).
Wepropose studying the distributional semantics of aadjective-noun compound; in particular, we will rep-resent it via its syntactic contexts.2.1 Adjective-noun compoundsGiven a particular adjective-noun compound, de-noted ?a, n?, we want to measure its composition-ality by comparing its syntactic contexts to those ofthe noun: ?n?.
After exploring the dataset we real-ized that considering nouns alone introduced noise,as contexts of the target and different meanings ofthe noun might be hard to separate; in order to softenthis problem we decided to compare the occurrencesof the ?a, n?
pair to those of the noun with a differentadjective.Given a dependency-parsed corpus C, we denoteN the set of all nouns occurring in C and A the set ofall adjectives.
An adjective-noun pair, ?a, n?, is anoccurrence in the dependency parse of the sentenceof an arc (a, n), where n is the governor of an adjec-tival relation, with a as modifier.
We define the com-plementary of ?a, n?
as the set of all adjective-nounpairs with the same noun but a different adjective:?ac, n?
= {?b, n?
such that b ?
A, b 6= a}In order to detect compositionality, we comparethe semantics of ?a, n?
to those of its complemen-tary ?ac, n?.
We use syntactic context as the repre-sentation of these compounds?
semantics.We call target pairs those ?a, n?
in which we areinterested, as they appear in the training, validation,or test sets for the task.
For each of them, its com-plementary target is: ?ac, n?.We model the syntactic contexts of any ?a, n?
pairas a set of vectors in a set of vector spaces defined asfollows.
After inspection of the corpus, and its de-pendency parse annotation layer, we manually spec-ified a few syntactic relations, which we considercodify the relevant syntactic relations in which an?a, n?
takes part.
For each of these syntactic rela-tions, we built a vector space model, and we repre-sented as a vector in it each of the target patterns,and each of their respective complementary targets.To compute compositionality of a target, we calcu-lated the cosine similarity between the target vec-tor and the target?s complementary vector.
So, foreach syntactic relation, and for each target, we havea value of its similarity to the complementary tar-get.
These similarity values are considered features,from which to learn the compositionality of targets.For results comparability, we used the PukWaCcorpus1 as dataset.
PukWaC adds to UkWaC a layerof syntactic dependency annotation.
The corpus hasbeen POS-tagged and lemmatized with the TreeTag-ger2.
The dependency parse was done with Malt-Parser (Nivre and Scholz, 2004).2.2 Implementation detailsWe defined a set of 19 syntactic patterns that defineinteresting relations in which an ?a, n?
pair mighttake part, trying to exploit the dependencies pro-duced by the MaltParser (Nivre and Scholz, 2004),including:?
Relations to a verb, other than the auxiliary tobe and to have: subject; object; indirect object;subject of a passive construction; logical sub-ject of a passive construction.?
The relations defined in the previous point, en-riched with a noun that acts as the other elementof a [subject-verb-object] or [subject-passiveverb-logical subject] construction.?
Collapsed prepositional complexes.?
Noun complexes.?
As subject or object of the verb to be.?
Modified by a second adjective.?
As modifier of a possessive.The paths were defined manually to match our in-tuitions of which are the paths that best describe thecontext of an ?a, n?pair, similarly to (Pado and Lap-ata, 2007).
For each of the patterns, the set of wordsthat are related through it to the target ?a, n?
definethe target?s context.For most of our processing, we used simple pro-grams implemented in Prolog and Python.
We im-plemented Prolog programs to model the depen-dency parsed sentences of the full PUkWaC corpus,and to match and extract these patterns from them.After an aggregating step, where proper nouns, num-bers and dates are substituted by place-holder vari-1Available at http://wacky.sslmit.unibo.it2http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/DecisionTreeTagger.html44ables, they amount to over 16 million instances,representing the syntactic relations in which every?a, n?
pair in the corpus takes part.
In further pro-cessing, only those that affect the target pairs, or thenouns in them, have to be taken into account.As described above, each pattern we have definedyields a vector space, where each target and its com-plementary are represented as a vector.
The basevectors of the vector space model for a pattern arethe words that are syntactic contexts, with that syn-tactic pattern, of any target in the target set3.The value of the coordinate for a target and a basevector is the frequency of the context word as relatedto the target by the pattern.
All frequencies werelocally scaled using logarithms4.For each syntactic pattern, and for each targetand complementary, we have two vectors, represent-ing their meanings in the vector space distributionalmodel.
The complementary vector, in particular,represents the centroid (average) of the meanings ofall ?b, n?
pairs, that share the noun with the targetbut have a different adjective, bWe propose that a target will be more composi-tional if its meaning is more similar to the meaningof the centroid of its complementary, that codifiesthe general meaning of that noun (whenever it ap-pears with a different adjective).For each syntactic pattern and target, we can com-pute the cosine similarity to the complementary tar-get, and obtain a value to use as a feature of the com-positionality of the target.
Those features will beused to train a classifier, being the compositionalityscore of each sample the label to be learnt.We used RapidMiner5 (Mierswa et al, 2006) asour Machine Learning framework.
The classifierswe have used, that are described below, are the im-plementations available in RapidMiner.3It would have been possible to consider a common vectorspace, using all patterns as base vectors.
We decided not to doso after realising that a single similarity value for a target andits complementary was not by itself a signal strong enough topredict the compositionality score.
A second objective was toassess the relative importance of different syntactic contexts forthe task.4We did not attempt any global weighting.
We leave this forfuture work.5http://rapid-i.com2.3 Feature selectionFrom the 19 original features, inspection of the cor-relation to the compositionality score label showedthat some of them were not to be expected to havemuch predictive power, while some of them weretoo sparse in the collection.We decided to perform feature selection previ-ous to all subsequent learning steps.
We usedRapidMiner genetic algorithm for feature selection6.Among the patterns which features were not selectedwere those where the ?a, n?
pair appears in prepo-sitional complexes, in noun complexes, as indirectobject, as subject or object of the verb to be, and assubject of a possessive.
Among those selected weresubject and objects of both active and passive con-structions, and the object of possessives.2.4 Runs descriptionNumeric scores For the numeric evaluation task,we built a regression model by means of a SVMclassifier.
We used RapidMiner?s implementationof mySVMLearner (Ru?ping, 2000), that is based onthe optimization algorithm of SVM-light (Joachims,1998).
We used the default parameters for the clas-sifier.
A simple dot product kernel seemed to ob-tain the best results in 10-fold cross validation overthe union of the provided train and validation re-sults.
For the three runs, we used identical settings,optimizing different quality measures in each run:absolute error (RUN SCORE-1), Pearson?s correla-tion coefficient (RUN SCORE-2), and Spearman?srho (RUN SCORE-3).
The choice of a SVM classifierwas motivated by the objective of learning a goodparametric classifier model.
In initial experiments,SVM showed to perform better than other possiblechoices, like logistic regression.
In hindsight, therelatively small size of the dataset might be a reasonfor the relatively poor results.
Experimenting withother approaches is left for future work.Coarse scores For the coarse scoring, we decidedto build a different set of classifiers, that would learnthe nominal 3-valued compositionality label.
Theclassifiers built in our initial experiments turned out6The mutation step switches features on and off, while thecrossover step interchanges used features.
Selection is donerandomly.
The algorithm used to evaluate each of the featuresubsets was a SVM identical as the one described below.45Run avg4 r ?RUN-SCORE-1 16.395 0.483 0.487RUN-SCORE-2 15.874 0.475 0.463RUN-SCORE-3 16.318 0.494 0.486baseline 17.857 ?
?Table 1: TRAINING.
Numeric score runs results on 10-foldcross-validation for the training set.
avg4: average absoluteerror; r: Pearson?s correlation;?
: Spearman?s rho.Run avg4 r ?RUN-SCORE-1 17.016 0.237 0.267RUN-SCORE-2 17.180 0.217 0.219RUN-SCORE-3 17.289 0.180 0.189baseline 17.370 ?
?Table 2: TEST.
Numeric score runs for the test set.
Onlyfor the en-ADJ-NN samples.
avg4: average absolute error; r:Pearson?s correlation;?
: Spearman?s rho.to lazily choose the most frequent class (?high?)
formost of the test samples.
In an attempt to overcomethis situation and possibly learn non linearly separa-ble classes, we tried neural network classifiers7.
Inhindsight, from seeing the very poor performance ofthis classifiers on the test set, it is clear that any per-formance gains were due to over-fitting on the train-ing set.For RUN COARSE-2, we binned the numericscores obtained in RUN-SCORE-1, dividing the scorespace in three equal sized parts; we decided not toassume the same distribution of the three labels forthe training and test sets.
The results were worsethan the numeric scores, due to the fact that the 3classes are not equally sized.2.5 ResultsResults in the training phase For all our training,we performed 10-fold cross validation.
For refer-ence, we report the results as evaluated by averag-ing over the 10 splits of the union of the providedtraining and validation set in Table 1.
We comparedagainst a dummy baseline: return as constant scorethe average of the scores in the training and valida-7For RUN COARSE-1, we used AutoMLP (Breuel andShafait, 2010), an algorithm that learns a neural network, op-timizing both the learning rate and number of hidden nodes ofthe network.
For RUN COARSE-3, we learnt a simple neural net-work model, by means of a feed-forward neural network trainedby a backpropagation algorithm (multi-layer perceptron), witha hidden layer with sigmoid type and size 8.tion sample sets.Disappointingly, the resulting classifiers seemedto be quite lazy, yielding values significatively closeto the average of the compositionality label in thetraining and validation set.The AutoMNLP and neural network seemed toperform reasonably, and better than other classifierswe tried (e.g., SVM based).
We were wary, though,of the risk of having learnt an over-fitted model; un-fortunately, the results on the test set confirmed that:for instance, the accuracy of RUN-SCORE-3 for thetraining set was 0.548, but for the test set it was only0.327.Results in the test phase After the task resultswere distributed, we verified that our numeric scoreruns, for the subtask en-ADJ-NN performed quitewell: fifth among the 17 valid submissions for thesubtask, using the average point difference as qualitymeasure.
Nevertheless, in terms of ranking correla-tion scores, our system performs presumably worse,although separate correlation results for the en-ADJ-NN subtask were not available to us at the time ofwriting this report.Our naive baseline turns out to be strong in termsof average point score.
Of course, the ranking corre-lation of such a baseline is none; using ranking cor-relation as quality measure would be more sensible,given that it discards such a baseline.3 ConclusionsWe obtained modest results in the task.
Our threenumeric runs obtained results very similar to eachother.
Only taking part in the en-ADJ-NN subtask,we obtained the 5th best of a total of 17 valid sys-tems in average point difference.
Nevertheless, interms ranking correlation scores, our systems seemto perform worse.
The modifications we tried to spe-cialize for coarse scoring were unsuccessful, yield-ing poor results.A few conclusions we can draw at this momentare: our system could benefit from global frequencyweighting schemes that we did not try but that haveshown to be successful in the past; the relativelysmall size of the dataset has not allowed us to learn abetter classifier; finally, we believe the ranking cor-relation quality measures are more sensible than thepoint difference for this particular task.46ReferencesTimothy Baldwin, Colin Bannard, Takaaki Tanaka, andDominic Widdows.
2003.
An empirical model ofmultiword expression decomposability.
In Proceed-ings of the ACL 2003 workshop on Multiword expres-sions: analysis, acquisition and treatment - Volume 18,MWE ?03, pages 89?96, Stroudsburg, PA, USA.
Asso-ciation for Computational Linguistics.Thomas Breuel and Faisal Shafait.
2010.
Automlp: Sim-ple, effective, fully automated learning rate and sizeadjustment.
In The Learning Workshop.
Online, 4.Zellig S. Harris.
1954.
Distributional structure.
Word,pages 146?162.Thorsten Joachims.
1998.
Making large-scale svm learn-ing practical.
LS8-Report 24, Universita?t Dortmund,LS VIII-Report.Graham Katz and Eugenie Giesbrecht.
2006.
Automaticidentification of non-compositional multi-word ex-pressions using latent semantic analysis.
In Proceed-ings of the Workshop on Multiword Expressions: Iden-tifying and Exploiting Underlying Properties, MWE?06, pages 12?19, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Ingo Mierswa, Michael Wurst, Ralf Klinkenberg, MartinScholz, and Timm Euler.
2006.
Yale: rapid prototyp-ing for complex data mining tasks.
In KDD?06, pages935?940.Jeff Mitchell and Mirella Lapata.
2010.
Composition indistributional models of semantics.
Cognitive Science,34(8):1388?1429.Joakim Nivre and Mario Scholz.
2004.
Deterministicdependency parsing of english text.
COLING ?04.Sebastian Pado and Mirella Lapata.
2007.
Dependency-Based Construction of Semantic Space Models.
Com-putational Linguistics, 33(2):161?199, jun.Anselmo Pen?as and Eduard Hovy.
2010.
Semantic en-richment of text with background knowledge.
pages15?23, jun.Stefan Ru?ping.
2000. mySVM-Manual.http://www-ai.cs.uni-dortmund.de/SOFTWARE/MYSVM/.Peter D. Turney and Patrick Pantel.
2010.
From fre-quency to meaning: Vector space models of semantics.J.
Artif.
Intell.
Res.
(JAIR), 37:141?188.47
