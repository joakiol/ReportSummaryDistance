Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1239?1249,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsHow Many Words is a Picture Worth?Automatic Caption Generation for News ImagesYansong Feng and Mirella LapataSchool of Informatics, University of Edinburgh10 Crichton Street, Edinburgh EH8 9AB, UKY.Feng-4@sms.ed.ac.uk, mlap@inf.ed.ac.ukAbstractIn this paper we tackle the problem of au-tomatic caption generation for news im-ages.
Our approach leverages the vast re-source of pictures available on the weband the fact that many of them are cap-tioned.
Inspired by recent work in sum-marization, we propose extractive and ab-stractive caption generation models.
Theyboth operate over the output of a proba-bilistic image annotation model that pre-processes the pictures and suggests key-words to describe their content.
Exper-imental results show that an abstractivemodel defined over phrases is superior toextractive methods.1 IntroductionRecent years have witnessed an unprecedentedgrowth in the amount of digital information avail-able on the Internet.
Flickr, one of the best knownphoto sharing websites, hosts more than three bil-lion images, with approximately 2.5 million im-ages being uploaded every day.1 Many on-linenews sites like CNN, Yahoo!, and BBC publishimages with their stories and even provide photofeeds related to current events.
Browsing and find-ing pictures in large-scale and heterogeneous col-lections is an important problem that has attractedmuch interest within information retrieval.Many of the search engines deployed on theweb retrieve images without analyzing their con-tent, simply by matching user queries against col-located textual information.
Examples includemeta-data (e.g., the image?s file name and for-mat), user-annotated tags, captions, and gener-ally text surrounding the image.
As this limitsthe applicability of search engines (images that1http://www.techcrunch.com/2008/11/03/three-billion-photos-at-flickr/do not coincide with textual data cannot be re-trieved), a great deal of work has focused on thedevelopment of methods that generate descriptionwords for a picture automatically.
The literatureis littered with various attempts to learn the as-sociations between image features and words us-ing supervised classification (Vailaya et al, 2001;Smeulders et al, 2000), instantiations of the noisy-channel model (Duygulu et al, 2002), latent vari-able models (Blei and Jordan, 2003; Barnard et al,2002; Wang et al, 2009), and models inspired byinformation retrieval (Lavrenko et al, 2003; Fenget al, 2004).In this paper we go one step further and gen-erate captions for images rather than individualkeywords.
Although image indexing techniquesbased on keywords are popular and the method ofchoice for image retrieval engines, there are goodreasons for using more linguistically meaningfuldescriptions.
A list of keywords is often ambigu-ous.
An image annotated with the words blue,sky, car could depict a blue car or a blue sky,whereas the caption ?car running under the bluesky?
would make the relations between the wordsexplicit.
Automatic caption generation could im-prove image retrieval by supporting longer andmore targeted queries.
It could also assist journal-ists in creating descriptions for the images associ-ated with their articles.
Beyond image retrieval, itcould increase the accessibility of the web for vi-sually impaired (blind and partially sighted) userswho cannot access the content of many sites inthe same ways as sighted users can (Ferres et al,2006).We explore the feasibility of automatic captiongeneration in the news domain, and create descrip-tions for images associated with on-line articles.Obtaining training data in this setting does not re-quire expensive manual annotation as many ar-ticles are published together with captioned im-ages.
Inspired by recent work in summarization,we propose extractive and abstractive caption gen-1239eration models.
The backbone for both approachesis a probabilistic image annotation model that sug-gests keywords for an image.
We can then simplyidentify (and rank) the sentences in the documentsthat share these keywords or create a new captionthat is potentially more concise but also informa-tive and fluent.
Our abstractive model operatesover image description keywords and documentphrases.
Their combination gives rise to manycaption realizations which we select probabilisti-cally by taking into account dependency and wordorder constraints.
Experimental results show thatthe model?s output compares favorably to hand-written captions and is often superior to extractivemethods.2 Related WorkAlthough image understanding is a popular topicwithin computer vision, relatively little work hasfocused on the interplay between visual and lin-guistic information.
A handful of approaches gen-erate image descriptions automatically followinga two-stage architecture.
The picture is first ana-lyzed using image processing techniques into anabstract representation, which is then renderedinto a natural language description with a text gen-eration engine.
A common theme across differ-ent models is domain specificity, the use of hand-labeled data, and reliance on background ontolog-ical information.For example, He?de et al (2004) generate de-scriptions for images of objects shot in uniformbackground.
Their system relies on a manuallycreated database of objects indexed by an imagesignature (e.g., color and texture) and two key-words (the object?s name and category).
Imagesare first segmented into objects, their signature isretrieved from the database, and a description isgenerated using templates.
Kojima et al (2002,2008) create descriptions for human activities inoffice scenes.
They extract features of human mo-tion and interleave them with a concept hierarchyof actions to create a case frame from which a nat-ural language sentence is generated.
Yao et al(2009) present a general framework for generatingtext descriptions of image and video content basedon image parsing.
Specifically, images are hierar-chically decomposed into their constituent visualpatterns which are subsequently converted into asemantic representation using WordNet.
The im-age parser is trained on a corpus, manually an-notated with graphs representing image structure.A multi-sentence description is generated using adocument planner and a surface realizer.Within natural language processing most previ-ous efforts have focused on generating captions toaccompany complex graphical presentations (Mit-tal et al, 1998; Corio and Lapalme, 1999; Fas-ciano and Lapalme, 2000; Feiner and McKeown,1990) or on using the captions accompanying in-formation graphics to infer their intended mes-sage, e.g., the author?s goal to convey ostensibleincrease or decrease of a quantity of interest (Elzeret al, 2005).
Little emphasis is placed on imageprocessing; it is assumed that the data used to cre-ate the graphics are available, and the goal is toenable users understand the information expressedin them.The task of generating captions for news im-ages is novel to our knowledge.
Instead of relyingon manual annotation or background ontologicalinformation we exploit a multimodal database ofnews articles, images, and their captions.
The lat-ter is admittedly noisy, yet can be easily obtainedfrom on-line sources, and contains rich informa-tion about the entities and events depicted in theimages and their relations.
Similar to previouswork, we also follow a two-stage approach.
Us-ing an image annotation model, we first describethe picture with keywords which are subsequentlyrealized into a human readable sentence.
Thecaption generation task bears some resemblanceto headline generation (Dorr et al, 2003; Bankoet al, 2000; Jin and Hauptmann, 2002) where theaim is to create a very short summary for a doc-ument.
Importantly, we aim to create a captionthat not only summarizes the document but is alsoa faithful to the image?s content (i.e., the captionshould also mention some of the objects or indi-viduals depicted in the image).
We therefore ex-plore extractive and abstractive models that relyon visual information to drive the generation pro-cess.
Our approach thus differs from most work insummarization which is solely text-based.3 Problem FormulationWe formulate image caption generation as fol-lows.
Given an image I, and a related knowl-edge database ?, create a natural language descrip-tion C which captures the main content of the im-age under ?.
Specifically, in the news story sce-nario, we will generate a caption C for an image Iand its accompanying document D. The trainingdata thus consists of document-image-caption tu-1240Thousands of Tongans haveattended the funeral of KingTaufa?ahau Tupou IV, whodied last week at the ageof 88.
Representativesfrom 30 foreign countrieswatched as the king?s coffinwas carried by 1,000 mento the official royal burialground.King Tupou, who was 88,died a week ago.A Nasa satellite has doc-umented startling changesin Arctic sea ice cover be-tween 2004 and 2005.
Theextent of ?perennial?
icedeclined by 14%, losing anarea the size of Pakistanor Turkey.
The last fewdecades have seen ice covershrink by about 0.7% peryear.Satellite instruments candistinguish ?old?
Arcticice from ?new?.Contaminated Cadbury?schocolate was the mostlikely cause of an outbreakof salmonella poisoning,the Health ProtectionAgency has said.
About 36out of a total of 56 cases ofthe illness reported betweenMarch and July could belinked to the product.Cadbury will increase itscontamination testing levels.A third of children in theUK use blogs and socialnetwork websites but twothirds of parents do noteven know what theyare, a survey suggests.The children?s charityNCH said there was ?analarming gap?
in techno-logical knowledge betweengenerations.Children were found to befar more internet-wise thanparents.Table 1: Each entry in the BBC News database contains a document an image, and its caption.ples like the ones shown in Table 1.
During test-ing, we are given a document and an associatedimage for which we must generate a caption.Our experiments used the dataset created byFeng and Lapata (2008).2 It contains 3,361 articlesdownloaded from the BBC News website3 each ofwhich is associated with a captioned news image.The latter is usually 203 pixels wide and 152 pix-els high.
The average caption length is 9.5 words,the average sentence length is 20.5 words, andthe average document length 421.5 words.
Thecaption vocabulary is 6,180 words and the docu-ment vocabulary is 26,795.
The vocabulary sharedbetween captions and documents is 5,921 words.The captions tend to use half as many words asthe document sentences, and more than 50% of thetime contain words that are not attested in the doc-ument (even though they may be attested in thecollection).Generating image captions is a challenging taskeven for humans, let alne computers.
Journalistsare given explicit instructions on how to write cap-tions4 and laypersons do not always agree on whata picture depicts (von Ahn and Dabbish, 2004).Along with the title, the lead, and section head-ings, captions are the most commonly read words2Available from http://homepages.inf.ed.ac.uk/s677528/data/3http://news.bbc.co.uk/4See http://www.theslot.com/captions.html andhttp://www.thenewsmanual.net/ for tips on how to writegood captions.in an article.
A good caption must be succinct andinformative, clearly identify the subject of the pic-ture, establish the picture?s relevance to the arti-cle, provide context for the picture, and ultimatelydraw the reader into the article.
It is also worthnoting that journalists often write their own cap-tions rather than simply extract sentences from thedocument.
In doing so they rely on general worldknowledge but also expertise in current affairs thatgoes beyond what is described in the article orshown in the picture.4 Image AnnotationAs mentioned earlier, our approach relies on animage annotation model to provide descriptionkeywords for the picture.
Our experiments madeuse of the probabilistic model presented in Fengand Lapata (2010).
The latter is well-suited to ourtask as it has been developed with noisy, multi-modal data sets in mind.
The model is based on theassumption that images and their surrounding textare generated by mixtures of latent topics whichare inferred from a concatenated representation ofwords and visual features.Specifically, images are preprocessed so thatthey are represented by word-like units.
Lo-cal image descriptors are computed using theScale Invariant Feature Transform (SIFT) algo-rithm (Lowe, 1999).
The general idea behind thealgorithm is to first sample an image with thedifference-of-Gaussians point detector at different1241scales and locations.
Importantly, this detector is,to some extent, invariant to translation, scale, ro-tation and illumination changes.
Each detected re-gion is represented with a SIFT descriptor whichis a histogram of edge directions at different lo-cations.
Subsequently SIFT descriptors are quan-tized into a discrete set of visual terms via a clus-tering algorithm such as K-means.The model thus works with a bag-of-words rep-resentation and treats each article-image-captiontuple as a single document dMix consisting of tex-tual and visual words.
Latent Dirichlet Allocation(LDA, Blei et al 2003) is used to infer the latenttopics assumed to have generated dMix.
The ba-sic idea underlying LDA, and topic models in gen-eral, is that each document is composed of a prob-ability distribution over topics, where each topicrepresents a probability distribution over words.The document-topic and topic-word distributionsare learned automatically from the data and pro-vide information about the semantic themes cov-ered in each document and the words associatedwith each semantic theme.
The image annotationmodel takes the topic distributions into accountwhen finding the most likely keywords for an im-age and its associated document.More formally, given an image-caption-document tuple (I,C,D) the model finds thesubset of keywords WI (WI ?
W ) which appro-priately describe I.
Assuming that keywordsare conditionally independent, and I, D arerepresented jointly by dMix, the model estimates:W ?I ?
argmaxWt?wt?WtP(wt |dMix) (1)= argmaxWt?wt?WtK?k=1P(wt |zk)P(zk|dMix)Wt denotes a set of description keywords (the sub-script t is used to discriminate from the visualwords which are not part of the model?s output),K the number of topics, P(wt |zk) the multimodalword distributions over topics, and P(zk|dMix) theestimated posterior of the topic proportions overdocuments.
Given an unseen image-documentpair and trained multimodal word distributionsover topics, it is possible to infer the posterior oftopic proportions over the new data by maximizingthe likelihood.
The model delivers a ranked list oftextual words wt , the n-best of which are used asannotations for image I.It is important to note that the caption gener-ation models we propose are not especially tiedto the above annotation model.
Any probabilis-tic model with broadly similar properties couldserve our purpose.
Examples include PLSA-basedapproaches to image annotation (e.g., Monayand Gatica-Perez 2007) and correspondence LDA(Blei and Jordan, 2003).5 Extractive Caption GenerationMuch work in summarization to date focuses onsentence extraction where a summary is createdsimply by identifying and subsequently concate-nating the most important sentences in a docu-ment.
Without a great deal of linguistic analysis, itis possible to create summaries for a wide range ofdocuments, independently of style, text type, andsubject matter.
For our caption generation task, weneed only extract a single sentence.
And our guid-ing hypothesis is that this sentence must be max-imally similar to the description keywords gener-ated by the annotation model.
We discuss belowdifferent ways of operationalizing similarity.Word Overlap Perhaps the simplest way ofmeasuring the similarity between image keywordsand document sentences is word overlap:Overlap(WI,Sd) =|WI ?Sd ||WI ?Sd |(2)where WI is the set of keywords and Sd a sentencein the document.
The caption is then the sentencethat has the highest overlap with the keywords.Cosine Similarity Word overlap is admittedlya naive measure of similarity, based on lexicalidentity.
We can overcome this by representingkeywords and sentences in vector space (Saltonand McGill, 1983).
The latter is a word-sentenceco-occurrence matrix where each row representsa word, each column a sentence, and each en-try the frequency with which the word appearedwithin the sentence.
More precisely matrix cellsare weighted by their tf-idf values.
The similarityof the vectors representing the keywords?
?WI anddocument sentence?
?Sd can be quantified by mea-suring the cosine of their angle:sim(??WI,?
?Sd) =?
?WI ???Sd|?????WI||?
?Sd |(3)Probabilistic Similarity Recall that the back-bone of our image annotation model is a topicmodel with images and documents represented asa probability distribution over latent topics.
Un-der this framework, the similarity between an im-1242age and a sentence can be broadly measured by theextent to which they share the same topic distribu-tions (Steyvers and Griffiths, 2007).
For example,we may use the KL divergence to measure the dif-ference between the distributions p and q:D(p,q) =K?j=1p j log2p jq j(4)where p and q are shorthand for the imagetopic distribution PdMix and sentence topic distri-bution PSd , respectively.
When doing inference onthe document sentence, we also take its neighbor-ing sentences into account to avoid estimating in-accurate topic proportions on short sentences.The KL divergence is asymmetric and in manyapplications, it is preferable to apply a symmet-ric measure such as the Jensen Shannon (JS) di-vergence.
The latter measures the ?distance?
be-tween p and q through (p+q)2 , the average of pand q:JS(p,q) =12[D(p,(p+q)2)+D(q,(p+q)2)](5)6 Abstractive Caption GenerationAlthough extractive methods yield grammaticalcaptions and require relatively little linguisticanalysis, there are a few caveats to consider.Firstly, there is often no single sentence in the doc-ument that uniquely describes the image?s content.In most cases the keywords are found in the doc-ument but interspersed across multiple sentences.Secondly, the selected sentences make for longcaptions (sometimes longer than the average doc-ument sentence), are not concise and overall notas catchy as human-written captions.
For thesereasons we turn to abstractive caption generationand present models based on single words but alsophrases.Word-based Model Our first abstractive modelbuilds on and extends a well-known probabilisticmodel of headline generation (Banko et al, 2000).The task is related to caption generation, the aim isto create a short, title-like headline for a given doc-ument, without however taking visual informationinto account.
Like captions, headlines have to becatchy to attract the reader?s attention.Banko et al (2000) propose a bag-of-wordsmodel for headline generation.
It consists of con-tent selection and surface realization components.Content selection is modeled as the probability ofa word appearing in the headline given the sameword appearing in the corresponding documentand is independent from other words in the head-line.
The likelihood of different surface realiza-tions is estimated using a bigram model.
They alsotake the distribution of the length of the headlinesinto account in an attempt to bias the model to-wards generating concise output:P(w1,w2, ...,wn) =n?i=1P(wi ?
H|wi ?
D) (6)?P(len(H) = n)?n?i=2P(wi|wi?1)where wi is a word that may appear in head-line H, D the document being summarized,and P(len(H) = n) a headline length distributionmodel.The above model can be easily adapted to thecaption generation task.
Content selection is nowthe probability of a word appearing in the cap-tion given the image and its associated documentwhich we obtain from the output of our image an-notation model (see Section 4).
In addition we re-place the bigram surface realizer with a trigram:P(w1,w2, ...,wn) =n?i=1P(wi ?C|I,D) (7)?P(len(C) = n)?n?i=3P(wi|wi?1,wi?2)where C is the caption, I the image, D the accom-panying document, and P(wi ?
C|I,D) the imageannotation probability.Despite its simplicity, the caption generationmodel in (7) has a major drawback.
The contentselection component will naturally tend to ignorefunction words, as they are not descriptive of theimage?s content.
This will seriously impact thegrammaticality of the generated captions, as therewill be no appropriate function words to glue thecontent words together.
One way to remedy thisis to revert to a content selection model that ig-nores the image and simply estimates the prob-ability of a word appearing in the caption giventhe same word appearing in the document.
At thesame time we modify our surface realization com-ponent so that it takes note of the image annotationprobabilities.
Specifically, we use an adaptive lan-guage model (Kneser et al, 1997) that modifies an1243n-gram model with local unigram probabilities:P(w1,w2, ...,wn) =n?i=1P(wi ?C|wi ?
D) (8)?P(len(C) = n)?n?i=3Padap(wi|wi?1,wi?2)where P(wi ?C|wi ?D) is the probability of wi ap-pearing in the caption given that it appears inthe document D, and Padap(wi|wi?1,wi?2) the lan-guage model adapted with probabilities from ourimage annotation model:Padap(w|h) =?
(w)z(h)Pback(w|h) (9)?(w)?
(Padap(w)Pback(w))?
(10)z(h) =?w?
(w) ?Pback(w|h) (11)where Pback(w|h) is the probability of w giventhe history h of preceding words (i.e., the orig-inal trigram model), Padap(w) the probabilityof w according to the image annotation model,Pback(w) the probability of w according to the orig-inal model, and ?
a scaling parameter.Phrase-based Model The model outlined inequation (8) will generate captions with functionwords.
However, there is no guarantee that thesewill be compatible with their surrounding contextor that the caption will be globally coherent be-yond the trigram horizon.
To avoid these prob-lems, we turn our attention to phrases which arenaturally associated with function words and canpotentially capture long-range dependencies.Specifically, we obtain phrases from the out-put of a dependency parser.
A phrase is sim-ply a head and its dependents with the exceptionof verbs, where we record only the head (other-wise, an entire sentence could be a phrase).
Forexample, from the first sentence in Table 1 (firstrow, left document) we would extract the phrases:thousands of Tongans, attended, the funeral, KingTaufa?ahau Tupou IV, last week, at the age, died,and so on.
We only consider dependencies whoseheads are nouns, verbs, and prepositions, as theseconstitute 80% of all dependencies attested in ourcaption data.
We define a bag-of-phrases modelfor caption generation by modifying the contentselection and caption length components in equa-tion (8) as follows:P(?1,?2, ...,?m) ?m?j=1P(?
j ?C|?
j ?
D) (12)?P(len(C) =m?j=1len(?
j))?
?mj=1 len(?
j)?i=3Padap(wi|wi?1,wi?2)Here, P(?
j ?C|?
j ?
D) models the probability ofphrase ?
j appearing in the caption given that it alsoappears in the document and is estimated as:P(?
j ?C|?
j ?
D) = ?w j??
jP(w j ?C|w j ?
D) (13)where w j is a word in the phrase ?
j.One problem with the models discussed thusfar is that words or phrases are independent ofeach other.
It is up to the trigram model to en-force coarse ordering constraints.
These may besufficient when considering isolated words, butphrases are longer and their combinations are sub-ject to structural constraints that are not capturedby sequence models.
We therefore attempt to takephrase attachment constraints into account by es-timating the probability of phrase ?
j attaching tothe right of phrase ?i as:P(?
j|?i)= ?wi?
?i?w j??
jp(w j|wi) (14)=12 ?wi?
?i?w j??
j{f (wi,w j)f (wi,?
)+f (wi,w j)f (?,w j)}where p(w j|wi) is the probability of a phrase con-taining word w j appearing to the right of a phrasecontaining word wi, f (wi,w j) indicates the num-ber of times wi and w j are adjacent, f (wi,?)
isthe number of times wi appears on the left of anyphrase, and f (?,wi) the number of times it ap-pears on the right.5After integrating the attachment probabilitiesinto equation (12), the caption generation modelbecomes:P(?1,?2, ...,?m)?m?j=1P(?
j ?C|?
j ?
D) (15)?m?j=2P(?
j|?
j?1)?P(len(C) = ?mj=1 len(?
j))??m?j=1len(?
j)i=3 Padap(wi|wi?1,wi?2)5Equation (14) is smoothed to avoid zero probabilities.1244On the one hand, the model in equation (15) takeslong distance dependency constraints into ac-count, and has some notion of syntactic structurethrough the use of attachment probabilities.
Onthe other hand, it has a primitive notion of captionlength estimated by P(len(C) = ?mj=1 len(?
j)) andwill therefore generate captions of the same(phrase) length.
Ideally, we would like the modelto vary the length of its output depending on thechosen context.
However, we leave this to futurework.Search To generate a caption it is neces-sary to find the sequence of words that maxi-mizes P(w1,w2, ...,wn) for the word-based model(equation (8)) and P(?1,?2, ...,?m) for thephrase-based model (equation (15)).
We rewriteboth probabilities as the weighted sum of their logform components and use beam search to find anear-optimal sequence.
Note that we can makesearch more efficient by reducing the size of thedocument D. Using one of the models from Sec-tion 5, we may rank its sentences in terms oftheir relevance to the image keywords and con-sider only the n-best ones.
Alternatively, we couldconsider the single most relevant sentence togetherwith its surrounding context under the assumptionthat neighboring sentences are about the same orsimilar topics.7 Experimental SetupIn this section we discuss our experimental designfor assessing the performance of the caption gen-eration models presented above.
We give detailson our training procedure, parameter estimation,and present the baseline methods used for com-parison with our models.Data All our experiments were conducted onthe corpus created by Feng and Lapata (2008),following their original partition of the data(2,881 image-caption-document tuples for train-ing, 240 tuples for development and 240 for test-ing).
Documents and captions were parsed withthe Stanford parser (Klein and Manning, 2003) inorder to obtain dependencies for the phrase-basedabstractive model.Model Parameters For the image annotationmodel we extracted 150 (on average) SIFT fea-tures which were quantized into 750 visualterms.
The underlying topic model was trainedwith 1,000 topics using only content words(i.e., nouns, verbs, and adjectives) that appearedno less than five times in the corpus.
For allmodels discussed here (extractive and abstractive)we report results with the 15 best annotation key-words.
For the abstractive models, we used atrigram model trained with the SRI toolkit on anewswire corpus consisting of BBC and Yahoo!news documents (6.9 M words).
The attachmentprobabilities (see equation (14)) were estimatedfrom the same corpus.
We tuned the captionlength parameter on the development set using arange of [5,14] tokens for the word-based modeland [2,5] phrases for the phrase-based model.
Fol-lowing Banko et al (2000), we approximated thelength distribution with a Gaussian.
The scalingparameter ?
for the adaptive language model wasalso tuned on the development set using a rangeof [0.5,0.9].
We report results with ?
set to 0.5.For the abstractive models the beam size was setto 500 (with at least 50 states for the word-basedmodel).
For the phrase-based model, we also ex-perimented with reducing the search scope, ei-ther by considering only the n most similar sen-tences to the keywords (range [2,10]), or simplythe single most similar sentence and its neighbors(range [2,5]).
The former method delivered betterresults with 10 sentences (and the KL divergencesimilarity function).Evaluation We evaluated the performance ofour models automatically, and also by eliciting hu-man judgments.
Our automatic evaluation wasbased on Translation Edit Rate (TER, Snover et al2006), a measure commonly used to evaluate thequality of machine translation output.
TER is de-fined as the minimum number of edits a humanwould have to perform to change the system out-put so that it exactly matches a reference transla-tion.
In our case, the original captions written bythe BBC journalists were used as reference:TER(E,Er) =Ins+Del+Sub+ShftNr(16)where E is the hypothetical system output, Er thereference caption, and Nr the reference length.The number of possible edits include insertions(Ins), deletions (Del), substitutions (Sub) andshifts (Shft).
TER is similar to word error rate,the only difference being that it allows shifts.
Ashift moves a contiguous sequence to a differentlocation within the the same system output and iscounted as a single edit.
The perfect TER scoreis 0, however note that it can be higher than 1 dueto insertions.
The minimum translation edit align-1245Model TER AvgLenLead sentence 2.12?
21.0Word Overlap 2.46??
24.3Cosine 2.26?
22.0KL Divergence 1.77??
18.4JS Divergence 1.77??
18.6Abstract Words 1.11??
10.0Abstract Phrases 1.06??
10.1Table 2: TER results for extractive, abstractivemodels, and lead sentence baseline; ?
: sig.
dif-ferent from lead sentence; ?
: sig.
different fromKL and JS divergence.ment is usually found through beam search.
Weused TER to compare the output of our extractiveand abstractive models and also for parameter tun-ing (see the discussion above).In our human evaluation study participants werepresented with a document, an associated image,and its caption, and asked to rate the latter on twodimensions: grammaticality (is the sentence flu-ent or word salad?)
and relevance (does it de-scribe succinctly the content of the image and doc-ument?).
We used a 1?7 rating scale, participantswere encouraged to give high ratings to captionsthat were grammatical and appropriate descrip-tions of the image given the accompanying docu-ment.
We randomly selected 12 document-imagepairs from the test set and generated captions forthem using the best extractive system, and two ab-stractive systems (word-based and phrase-based).We also included the original human-authoredcaption as an upper bound.
We collected ratingsfrom 23 unpaid volunteers, all self reported nativeEnglish speakers.
The study was conducted overthe Internet.8 ResultsTable 2 reports our results on the test set us-ing TER.
We compare four extractive modelsbased on word overlap, cosine similarity, and twoprobabilistic similarity measures, namely KL andJS divergence and two abstractive models basedon words (see equation (8)) and phrases (see equa-tion (15)).
We also include a simple baseline thatselects the first document sentence as a captionand show the average caption length (AvgLen) foreach model.
We examined whether performancedifferences among models are statistically signifi-cant, using the Wilcoxon test.Model Grammaticality RelevanceKL Divergence 6.42??
4.10?
?Abstract Words 2.08?
3.20?Abstract Phrases 4.80?
4.96?Gold Standard 6.39??
5.55?Table 3: Mean ratings on caption output elicitedby humans; ?
: sig.
different from word-based abstractive system; ?
: sig.
different fromphrase-based abstractive system.As can be seen the probabilistic models (KL andJS divergence) outperform word overlap and co-sine similarity (all differences are statistically sig-nificant, p < 0.01).6 They make use of the sametopic model as the image annotation model, andare thus able to select sentences that cover com-mon content.
They are also significantly betterthan the lead sentence which is a competitive base-line.
It is well known that news articles are writtenso that the lead contains the most important infor-mation in a story.7 This is an encouraging resultas it highlights the importance of the visual infor-mation for the caption generation task.
In general,word overlap is the worst performing model whichis not unexpected as it does not take any lexicalvariation into account.
Cosine is slightly betterbut not significantly different from the lead sen-tence.
The abstractive models obtain the best TERscores overall, however they generate shorter cap-tions in comparison to the other models (closer tothe length of the gold standard) and as a result TERtreats them favorably, simply because the numberof edits is less.
For this reason we turn to the re-sults of our judgment elicitation study which as-sesses in more detail the quality of the generatedcaptions.Recall that participants judge the system out-put on two dimensions, grammaticality and rele-vance.
Table 3 reports mean ratings for the out-put of the extractive system (based on the KL di-vergence), the two abstractive systems, and thehuman-authored gold standard caption.
We per-formed an Analysis of Variance (ANOVA) to ex-amine the effect of system type on the generationtask.
Post-hot Tukey tests were carried out on themean of the ratings shown in Table 3 (for gram-maticality and relevance).6We also note that mean length differences are not signif-icant among these models.7As a rule of thumb the lead should answer most or all ofthe five W?s (who, what, when, where, why).1246G: King Tupou, who was 88, died a week ago.KL: Last year, thousands of Tongans took part in unprece-dented demonstrations to demand greater democracyand public ownership of key national assets.AW : King Toupou IV died at the age of Tongans last week.AP: King Toupou IV died at the age of 88 last week.G: Cadbury will increase its contamination testing levels.KL: Contaminated Cadbury?s chocolate was the mostlikely cause of an outbreak of salmonella poisoning,the Health Protection Agency has said.AW : Purely dairy milk buttons Easter had agreed to workhas caused.AP: The 105g dairy milk buttons Easter egg affected bythe recall.G: Satellite instruments can distinguish ?old?
Arctic icefrom ?new?.KL: So a planet with less ice warms faster, potentially turn-ing the projected impacts of global warming into real-ity sooner than anticipated.AW : Dr less winds through ice cover all over long timewhen.AP: The area of the Arctic covered in Arctic sea ice cover.G: Children were found to be far more internet-wise thanparents.KL: That?s where parents come in.AW : The survey found a third of children are about mobilephones.AP: The survey found a third of children in the drivingseat.Table 4: Captions written by humans (G) and gen-erated by extractive (KL), word-based abstractive(AW ), and phrase-based extractive (AP systems).The word-based system yields the least gram-matical output.
It is significantly worse than thephrase-based abstractive system (?
< 0.01), theextractive system (?
< 0.01), and the gold stan-dard (?
< 0.01).
Unsurprisingly, the phrase-basedsystem is significantly less grammatical than thegold standard and the extractive system, whereasthe latter is perceived as equally grammatical asthe gold standard (the difference in the means isnot significant).
With regard to relevance, theword-based system is significantly worse than thephrase-based system, the extractive system, andthe gold-standard.
Interestingly, the phrase-basedsystem performs on the same level with the hu-man gold standard (the difference in the means isnot significant) and significantly better than the ex-tractive system.
Overall, the captions generated bythe phrase-based system, capture the same contentas the human-authored captions, even though theytend to be less grammatical.
Examples of systemoutput for the image-document pairs shown in Ta-ble 1 are given in Table 4 (the first row correspondsto the left picture (top row) in Table 1, the secondrow to the right picture, and so on).9 ConclusionsWe have presented extractive and abstractive mod-els that generate image captions for news articles.A key aspect of our approach is to allow boththe visual and textual modalities to influence thegeneration task.
This is achieved through an im-age annotation model that characterizes picturesin terms of description keywords that are subse-quently used to guide the caption generation pro-cess.
Our results show that the visual informationplays an important role in content selection.
Sim-ply extracting a sentence from the document oftenyields an inferior caption.
Our experiments alsoshow that a probabilistic abstractive model definedover phrases yields promising results.
It generatescaptions that are more grammatical than a closelyrelated word-based system and manages to capturethe gist of the image (and document) as well as thecaptions written by journalists.Future extensions are many and varied.
Ratherthan adopting a two-stage approach, where the im-age processing and caption generation are carriedout sequentially, a more general model should in-tegrate the two steps in a unified framework.
In-deed, an avenue for future work would be to de-fine a phrase-based model for both image annota-tion and caption generation.
We also believe thatour approach would benefit from more detailedlinguistic and non-linguistic information.
For in-stance, we could experiment with features relatedto document structure such as titles, headings, andsections of articles and also exploit syntactic infor-mation more directly.
The latter is currently usedin the phrase-based model by taking attachmentprobabilities into account.
We could, however, im-prove grammaticality more globally by generatinga well-formed tree (or dependency graph).ReferencesBanko, Michel, Vibhu O. Mittal, and Micheael J.Witbrock.
2000.
Headline generation based onstatistical translation.
In Proceedings of the 38thAnnual Meeting on Association for Computa-tional Linguistics.
Hong Kong, pages 318?325.Barnard, Kobus, Pinar Duygulu, David Forsyth,Nando de Freitas, David Blei, and MichaelJordan.
2002.
Matching words and pictures.Journal of Machine Learning Research 3:1107?1135.Blei, David and Michael Jordan.
2003.
Modelingannotated data.
In Proceedings of the 26th An-1247nual International ACM SIGIR Conference onResearch and Development in Information Re-trieval.
Toronto, ON, pages 127?134.Blei, David, Andrew Ng, and Michael Jordan.2003.
Latent Dirichlet alocation.
Journal ofMachine Learning Research 3:993?1022.Corio, Marc and Guy Lapalme.
1999.
Generationof texts for information graphics.
In Proceed-ings of the 7th European Workshop on NaturalLanguage Generation.
Toulouse, France, pages49?58.Dorr, Bonnie, David Zajic, and Richard Schwartz.2003.
Hedge trimmer: A parse-and-trim ap-proach to headline generation.
In Proceed-ings of the HLT-NAACL 2003 Workshop on TextSummarization.
Edmonton, Canada, pages 1?8.Duygulu, Pinar, Kobus Barnard, Nando de Freitas,and David Forsyth.
2002.
Object recognition asmachine translation: Learning a lexicon for afixed image vocabulary.
In Proceedings of the7th European Conference on Computer Vision.Copenhagen, Denmark, pages 97?112.Elzer, Stephanie, Sandra Carberry, Ingrid Zuker-man, Daniel Chester, Nancy Green, , and SenizDemir.
2005.
A probabilistic framework for rec-ognizing intention in information graphics.
InProceedings of the 19th International Confer-ence on Artificial Intelligence.
Edinburgh, Scot-land, pages 1042?1047.Fasciano, Massimo and Guy Lapalme.
2000.
In-tentions in the coordinated generation of graph-ics and text from tabular data.
Knowledge In-formation Systems 2(3):310?339.Feiner, Steven and Kathleen McKeown.
1990.
Co-ordinating text and graphics in explanation gen-eration.
In Proceedings of National Conferenceon Artificial Intelligence.
Boston, MA, pages442?449.Feng, Shaolei Feng, Victor Lavrenko, and R Man-matha.
2004.
Multiple Bernoulli relevancemodels for image and video annotation.
InProceedings of the International Conferenceon Computer Vision and Pattern Recognition.Washington, DC, pages 1002?1009.Feng, Yansong and Mirella Lapata.
2008.
Au-tomatic image annotation using auxiliary textinformation.
In Proceedings of the 46th An-nual Meeting of the Association of Computa-tional Linguistics: Human Language Technolo-gies.
Columbus, OH, pages 272?280.Feng, Yansong and Mirella Lapata.
2010.
Topicmodels for image annotation and text illustra-tion.
In Proceedings of the 11th Annual Con-ference of the North American Chapter of theAssociation for Computational Linguistics.
LosAngeles, LA.Ferres, Leo, Avi Parush, Shelley Roberts, andGitte Lindgaard.
2006.
Helping people withvisual impairments gain access to graphical in-formation through natural language: The graphsystem.
In Proceedings of 11th InternationalConference on Computers Helping People withSpecial Needs.
Linz, Austria, pages 1122?1130.He?de, Patrick, Pierre Allain Moe?llic, Joe?l Bour-geoys, Magali Joint, and Corinne Thomas.2004.
Automatic generation of natural lan-guage descriptions for images.
In Proceed-ings of Computer-Assisted Information Re-trieval (Recherche d?Information et ses Appli-cations Ordinateur) (RIAO).
Avignon, France.Jin, Rong and Alexander G. Hauptmann.
2002.
Anew probabilistic model for title generation.
InProceedings of the 19th International Confer-ence on Computational linguistics.
Taipei, Tai-wan, pages 1?7.Klein, Dan and Christopher D. Manning.
2003.Accurate unlexicalized parsing.
In Proceedingsof the 41st Annual Meeting of the Associationof Computational Linguistics.
Sapporo, Japan,pages 423?430.Kneser, Reinhard, Jochen Peters, and DietrichKlakow.
1997.
Language model adaptationusing dynamic marginals.
In Proceedings of5th European Conference on Speech Commu-nication and Technology.
Rhodes, Greece, vol-ume 4, pages 1971?1974.Kojima, Atsuhiro, Mamoru Takaya, Shigeki Aoki,Takao Miyamoto, and Kunio Fukunaga.
2008.Recognition and textual description of humanactivities by mobile robot.
In Proceedings ofthe 3rd International Conference on Innova-tive Computing Information and Control.
IEEEComputer Society, Washington, DC, pages 53?56.Kojima, Atsuhiro, Takeshi Tamura, and KunioFukunaga.
2002.
Natural language descriptionof human activities from video images basedon concept hierarchy of actions.
InternationalJournal of Computer Vision 50(2):171?184.Lavrenko, Victor, R. Manmatha, and Jiwoon Jeon.2003.
A model for learning the semantics of1248pictures.
In Proceedings of the 16th Conferenceon Advances in Neural Information ProcessingSystems.
Vancouver, BC.Lowe, David G. 1999.
Object recognition fromlocal scale-invariant features.
In Proceedings ofInternational Conference on Computer Vision.IEEE Computer Society, pages 1150?1157.Mittal, Vibhu O., Johanna D. Moore, GiuseppeCarenini, and Steven Roth.
1998.
Describingcomplex charts in natural language: A captiongeneration system.
Computational Linguistics24:431?468.Monay, Florent and Daniel Gatica-Perez.
2007.Modeling semantic aspects for cross-mediaimage indexing.
IEEE Transactions onPattern Analysis and Machine Intelligence29(10):1802?1817.Salton, Gerard and M.J. McGill.
1983.
In-troduction to Modern Information Retrieval.McGraw-Hill, New York.Smeulders, Arnols W.M., Marcel Worring, Si-mone Santini, Amarnath Gupta, and RameshJain.
2000.
Content-based image retrieval atthe end of the early years.
IEEE Transactionson Pattern Analysis and Machine Intelligence22(12):1349?1380.Snover, Matthew, Bonnie Dorr, Richard Schwartz,Linnea Micciulla, and John Makhoul.
2006.
Astudy of translation edit rate with targeted hu-man annotation.
In Proceedings of the 7th Con-ference of the Association for Machine Trans-lation in the Americas.
Cambridge, pages 223?231.Steyvers, Mark and Tom Griffiths.
2007.
Proba-bilistic topic models.
In T. Landauer, D. Mc-Namara, S Dennis, and W Kintsch, editors, AHandbook of Latent Semantic Analysis, Psy-chology Press.Vailaya, Aditya, Ma?rio A. T. Figueiredo, Anil K.Jain, and Hong-Jiang Zhang.
2001.
Image clas-sification for content-based indexing.
IEEETransactions on Image Processing 10:117?130.von Ahn, Luis and Laura Dabbish.
2004.
Labelingimages with a computer game.
In ACM Confer-ence on Human Factors in Computing Systems.New York, NY, pages 319?326.Wang, Chong, David Blei, and Li Fei-Fei.
2009.Simultaneous image classification and annota-tion.
In Proceedings of the International Con-ference on Computer Vision and Pattern Recog-nition.
Miami, FL, pages 1903?1910.Yao, Benjamin, Xiong Yang, Liang Lin, Mun WaiLee, and Song chun Zhu.
2009.
I2t: Image pars-ing to text description.
Proceedings of IEEE (in-vited for the special issue on Internet Vision) .1249
