Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 11?21,Atlanta, Georgia, June 13 2013. c?2013 Association for Computational LinguisticsShallow Semantic Analysis of Interactive Learner SentencesLevi KingIndiana UniversityBloomington, IN USAleviking@indiana.eduMarkus DickinsonIndiana UniversityBloomington, IN USAmd7@indiana.eduAbstractFocusing on applications for analyzing learnerlanguage which evaluate semantic appropri-ateness and accuracy, we collect data from atask which models some aspects of interac-tion, namely a picture description task (PDT).We parse responses to the PDT into depen-dency graphs with an an off-the-shelf parser,then use a decision tree to classify sentencesinto syntactic types and extract the logical sub-ject, verb, and object, finding 92% accuracy insuch extraction.
The specific goal in this paperis to examine the challenges involved in ex-tracting these simple semantic representationsfrom interactive learner sentences.1 MotivationWhile there is much current work on analyzinglearner language, it usually focuses on grammati-cal error detection and correction (e.g., Dale et al2012) and less on semantic analysis.
At thesame time, Intelligent Computer-Assisted LanguageLearning (ICALL) and Intelligent Language Tutor-ing (ILT) systems (e.g., Heift and Schulze, 2007;Meurers, 2012) also tend to focus more on gram-matical feedback.
An exception to this rule is HerrKomissar, an ILT for German learners that includesrather robust content analysis and sentence genera-tion (DeSmedt, 1995), but this involves a great dealof hand-built tools and does not connect to modernNLP.
Some work addresses content assessment forshort answer tasks (Meurers et al 2011), but this isstill far from naturalistic, more conversational inter-actions (though, see Petersen, 2010).Our overarching goal is to facilitate ILTs and lan-guage assessment tools that maximize free interac-tion, building as much as possible from existingNLP resources.
While that goal is in the distantfuture, the more immediate goal in this paper isto pinpoint the precise challenges which interactivelearner sentences present to constructing semanticanalyses, even when greatly constrained.
We ap-proximate this by collecting data from a task whichmodels some aspects of interaction, namely a picturedescription task (PDT), parsing it with an off-the-shelf parser, extracting semantic forms, and notingthe challenges throughout.The focus towards interaction is in accord withcontemporary theory and research in Second Lan-guage Acquisition (SLA) and best practices in sec-ond language instruction, which emphasize the lim-iting of explicit grammar instruction and feedback infavor of an approach that subtly integrates the teach-ing of form with conversation and task-based learn-ing (Celce-Murcia, 1991, 2002; Larsen-Freeman,2002).
Indeed, Ellis (2006) states, ?a traditional ap-proach to teaching grammar based on explicit expla-nations and drill-like practice is unlikely to result inthe acquisition of the implicit knowledge needed forfluent and accurate communication.?
For our pur-poses, this means shifting the primary task of anICALL application from analyzing grammar to eval-uating semantic appropriateness and accuracy.The data for error detection work is ideal for de-veloping systems which provide feedback on essays,but not necessarily for more interactive communica-tion.
Thus, our first step is to collect data similar towhat we envision processing in something like an11ILT game, data which?as far as we know?doesnot exist.
While we desire relatively free produc-tion, there are still constraints; for games, for exam-ple, this comes in the form of contextual knowledge(pictures, rules, previous interactions).
To get a han-dle on variability under a set of known constraintsand to systematically monitor deviations from tar-get meanings, we select a PDT as a constrained taskthat still promotes interactive communication.
Col-lecting and analyzing this data is our first major con-tribution, as described in section 3.Once we have the data, we can begin to extract se-mantic forms, and our second major contribution isto outline successes and pitfalls in obtaining shal-low semantic forms in interactive learner data, asdescribed in section 4, working from existing tools.Although we observe a lot of grammatical variation,we will demonstrate in section 5 how careful se-lection of output representations (e.g., the treatmentof prepositions) from an off-the-shelf parser and ahandful of syntax-to-semantics rules allow us to de-rive accurate semantic forms for most types of tran-sitive verb constructions in our data.
At the sametime, we will discuss the difficulties in defining atrue gold standard of meanings for such a task.
Thiswork paves the way for increasing the range of con-structions and further exploring the space betweenfree and constrained productions (see also the dis-cussion in Amaral and Meurers, 2011).2 Related WorkIn terms of our overarching goals of developingan interactive ILT, a number of systems exist (e.g.,TAGARELA (Amaral et al 2011), e-Tutor (Heiftand Nicholson, 2001)), but few focus on matchingsemantic forms.
Herr Komissar (DeSmedt (1995))is one counter-example; in this game, learners takeon the role of a detective tasked with interviewingsuspects and witnesses.
The system relies largely ona custom-built database of verb classes and relatedlexical items.
Likewise, Petersen (2010) designeda system to provide feedback on questions in En-glish, extracting meanings from the Collins parser(Collins, 1999).
Our work is is in the spirit of his,though our starting point is to collect data of the typeof task we aim to analyze, thereby pinpointing howone should begin to build a system.The basic semantic analysis in this paper paral-lels work on content assessment (e.g., ETS?s c-ratersystem (Leacock and Chodorow, 2003)).
Differentfrom our task, these systems are mostly focused onessay and short answer scoring, though many fo-cus on semantic analysis under restricted conditions.As one example, Meurers et al(2011) evaluate En-glish language learners?
short answers to readingcomprehension questions, constrained by the topicat hand.
Their approach performs multiple levels ofannotation on the reading prompt, including depen-dency parsing and lexical analysis from WordNet(Fellbaum, 1998), then attempts to align elements ofthe sentence with those of the (similarly annotated)reading prompt, the question, and target answers todetermine whether a response is adequate or what itmight be missing.
Our scenario is based on images,not text, but our future processing will most likelyneed to include similar elements, e.g., determininglexical relations from WordNet.3 Data CollectionThe data involved in this study shares much in com-mon with other investigations into semantic anal-ysis of descriptions of images and video, suchas the Microsoft Research Video Description Cor-pus (MSRvid; Chen and Dolan (2011)) and theSemEval-2012 Semantic Textual Similarity (STS)task utilizing MSRvid as training data for assigningsimilarity scores to pairs of sentences (Agirre et al2012).
However, because our approach requiresboth native speaker (NS) and non-native speaker(NNS) responses and necessitates constraining boththe form and content of responses, we assembledour own small corpus of NS and NNS responses toa PDT.
Research in SLA often relies on the abilityof task design to induce particular linguistic behav-ior (Skehan et al 1998), and the PDT should in-duce more interactive behavior.
Moreover, the useof the PDT as a reliable language research tool iswell-established in areas of study ranging from SLAto Alzheimer?s disease (Ellis, 2000; Forbes-McKayand Venneri, 2005).The NNSs were intermediate and upper-leveladult English learners in an intensive English asa Second Language program at Indiana University.We rely on visual stimuli here for a number of rea-12sons.
Firstly, computer games tend to be highlyvisual, so collecting responses to visual prompts isin keeping with the nature of our desired ILT.
Sec-ondly, by using images, the information the responseshould contain is limited to the information con-tained in the image.
Relatedly, particularly simpleimages should restrict elicited responses to a tightrange of expected contents.
For this initial experi-ment, we chose or developed each of the visual stim-uli because it presents an event that we believe to betransitive in nature and likely to elicit responses withan unambiguous subject, verb and object, thereby re-stricting form in addition to content.
Finally, thisformat allows us to investigate pure interlanguagewithout the influence of verbal prompts and showslearner language in a functional context, modelingreal language use.Response (L1)He is droning his wife pitcher.
(Arabic)The artist is drawing a pretty women.
(Chinese)The artist is painting a portrait of a lady.
(English)The painter is painting a woman?s paint.
(Spanish)Figure 1: Example item and responsesThe PDT consists of 10 items (8 line drawingsand 2 photographs) intended to elicit a single sen-tence each; an example is given in Figure 1.
Par-ticipants were asked to view the image and describethe action, and care was taken to explain to partici-pants that either past or present tense (and simple orprogressive aspect) was acceptable.
Responses weretyped by the participants themselves (without auto-matic spell checking).
To date, we have collectedresponses from 53 informants (14 NSs, 39 NNSs),for a total of 530 sentences.
The distribution of firstlanguages (L1s) is as follows: 14 English, 16 Ara-bic, 7 Chinese, 2 Japanese, 4 Korean, 1 Kurdish, 1Polish, 2 Portuguese, and 6 Spanish.4 MethodWe parse a sentence into a dependency representa-tion (section 4.1) and then extract a simple seman-tic form from this parse (section 4.2), to compare togold standard semantic forms.4.1 Obtaining a syntactic formWe start analysis with a dependency parse.
Becausedependency parsing focuses on labeling dependencyrelations, rather than constituents or phrase struc-ture, it easily finds the subject, verb and object ofa sentence, which can then map to a semantic form(Ku?bler et al 2009).
Our approach must eventuallyaccount for other relations, such as negation and ad-verbial modification, but at this point, since we fo-cus on transitive verbs, we take an na?
?ve approach inwhich subject, verb and object are considered suffi-cient for deciding whether or not a response accu-rately describes the visual prompt.We use the Stanford Parser for this task, trained onthe Penn Treebank (de Marneffe et al 2006; Kleinand Manning, 2003).1 Using the parser?s options,we set the output to be Stanford typed dependencies,a set of labels for dependency relations.
The Stan-ford parser has a variety of options to choose fromfor the specific parser ouput, e.g., how one wishes totreat prepositions (de Marneffe and Manning, 2012).We use the CCPropagatedDependencies /CCprocessed option to accomplish two things:21) omit prepositions and conjunctions from the sen-tence text and instead add the word to the depen-dency label between content words; and 2) propa-gate relations across any conjunctions.
These deci-sions are important to consider for any semantically-informed processing of learner language.1http://nlp.stanford.edu/software/lex-parser.shtml2http://nlp.stanford.edu/software/dependencies_manual.pdf13To see the impetus for removing prepositions,consider the learner response (1), where the prepo-sition with is relatively unimportant to collecting themeaning.
Additionally, learners often omit, insert,or otherwise use the wrong preposition (Chodorowet al 2007).
The default parser would present aprep relation between played and with, obscuringwhat the object is; with the options set as above,however, the dependency representation folds thepreposition into the label (prep with), instead ofkeeping it in the parsed string, as shown in Figure 2.
(1) The boy played with a ball.vroot The boy played with a ballnsubjrootprep withdetdetFigure 2: The dependency parse of (1)This is a very lenient approach to prepositions,as prepositions certainly carry semantic meaning?e.g., the boy played in a ball means something quitedifferent than what (1) means.
However, becausewe ultimately compare the meaning to an expectedsemantic form (e.g., play(boy,ball)), it is easier togive the benefit of the doubt.
In the future, one maywant to consider using a semantic role labeler (e.g.,SENNA (Collobert et al 2011)).As for propagating relations across conjunctions,this ensures that each main verb connects to its argu-ments, as needed for a semantic form.
For example,in (2), the default parser returns the relation betweenthe first verb of the conjunction structure, setting andits subject, man, but not between reading and man.The options we select, however, return an nsubjrelation between setting and man and also betweenreading and man (similarly for the object, paper).
(2) The man is setting and reading the paper.In addition to these options, many dependency re-lations are irrelevant for the next step of obtaininga semantic form.
For example, we can essentiallyignore determiner (det) relations between a nounand its determiner, allowing for variability in how alearner produces or does not produce determiners.4.2 Obtaining a semantic form4.2.1 Sentence typesWe categorized the sentences in the corpus into12 types, shown in Table 1.
We established thesetypes because each type corresponds to a basic sen-tence structure and thus has consistent syntactic fea-tures, leading to predictable patterns in the depen-dency parses.
We discuss the distribution of sen-tence types in section 5.1.4.2.2 Rules for sentence typesA sentence type indicates that the logical (i.e., se-mantic) subject, verb, and object can be found in aparticular place in the parse, e.g., under a particulardependency label.
For example, for simple transi-tive sentences of type A, the words labeled nsubj,root, and dobj exactly pinpoint the informationwe require.
Thus, the patterns for extracting se-mantic information?in the form of verb(subj,obj)triples?reference particular Stanford typed depen-dency labels, part-of-speech (POS) tags, and inter-actions with word indices.More complicated sentences or those containingcommon learner errors (e.g., omission of the cop-ula be) require slightly more complicated extractionrules, but, since we examine only transitive verbs atthis juncture, these still boil down to identifying thesentence type and extracting the appropriate triple.We do this by arranging a small set of binary fea-tures into a decision tree to determine the sentencetype, as shown in Figure 3.To illustrate this process, consider (3).
We passthis sentence through the parser to obtain the depen-dency parse shown in Figure 4.
The parsed sentencethen moves to the decision tree shown in Figure 3.At the top of the tree, the sentence is checked for anexpl (expletive) label; having none, it moves right-ward to the nsubjpass (noun subject, passive)node.
Because we find an nsubjpass label, thesentence moves leftward to the agent node.
Thislabel is also found, thereby reaching a terminal nodeand being labeled as a type F2 sentence.
(3) A bird is shot by a man.With the sentence now typed as F2, we applyspecific F2 extraction rules.
The logical subject istaken from under the agent label, the verb from14Type Description Example NS NNSA Simple declarative transitive The boy is kicking the ball.
117 286B Simple + preposition The boy played with a ball.
5 23C Missing tensed verb Girl driving bicycle.
10 44D Missing tensed verb + preposition Boy playing with a ball.
0 1E Intransitive (No object) A woman is cycling.
2 21F1 Passive An apple is being cut.
4 2F2 Passive with agent A bird is shot by a man.
0 6Ax Existential version of A or C There is a boy kicking a ball.
0 0Bx Existential version of B or D There was a boy playing with a ball.
0 0Ex Existential version of E There is a woman cycling.
0 0F1x Existential version of F1 There is an apple being cut.
0 1F2x Existential version of F2 There is a bird being shot by a man.
0 0Z All other forms The man is trying to hunt a bird.
2 6Table 1: Sentence type examples, with distributions of types for native speakers (NS) and non-native speakers (NNS)expl?nsubjpass?dobj?nsubj?Dprep ?
?EBY NY Nnsubj?CAY NY Nagent?F1F2Y NY Nauxpass?dobj?prep ?
?ExBxY NAxY Nagent?F1xF2xY NY NY NFigure 3: Decision tree for determining sentence type and extracting semantic informationvroot A bird is shot by a manrootdetnsubjpassauxpassagentdetFigure 4: The dependency parse of (3)root, and the logical object from nsubjpass,to obtain shot(man,bird), which can be lemmatizedto shoot(man,bird).
Very little effort goes into thisprocess: the parser is pre-built; the decision tree issmall; and the extraction rules are minimal.We are able to use little effort in part due to theconstraints in the pictures.
For figure 1, for exam-ple, the artist, the man in the beret, and the man areall acceptable subjects, whereas if there were multi-ple men in the picture, the man would not be specificenough.
In future work, we expect to relax such con-straints on image contents by including rules to han-dle relative clauses, adjectives and other modifiersin order to distinguish between references to simi-15lar elements, e.g., a man shooting a bird vs. a manreading the newspaper.5 EvaluationTo evaluate this work, we need to address two majorquestions.
First, how accurately do we extract se-mantic information from potentially innovative sen-tences (section 5.2)?
Due to the simple structuresof the sentences (section 5.1), we find high accu-racy with our simple system.
Secondly, how manysemantic forms does one need in order to capturethe variability in meaning in learner sentences (sec-tion 5.3)?
We operationalize this second questionby asking how well the set of native speaker seman-tic forms models a gold standard, with the intuitionthat a language is defined by native speaker usage,so their answers can serve as targets.
As we willsee, this is a na?
?ve view.5.1 Basic distribution of sentencesBefore a more thorough analysis, we look at the dis-tribution of sentence types, shown in Table 1, brokendown between native speakers (NSs) and non-nativespeakers (NNSs).
A few sentence types clearly dom-inate here: if one looks only at simple declaratives,with or without a main verb (types A and C), oneaccounts for 90.7% of the NS forms and 84.6% ofthe NNS ones, slightly less.
Adding prepositionalforms (types B and D) brings the total to 94.3% and90.8%, respectively.
Although there will always bevariability and novel forms (cf.
type Z), this showsthat, for situations with basic transitive actions, de-veloping a system (by hand) for a few sentence typesis manageable.
More broadly, we see that clear andsimple images nicely constrain the task to the pointwhere shallow processing is feasible.5.2 Semantic extractionFor the purpose of evaluating our extraction system,we define two major classes of errors.
The first aretriple errors, responses for which our system fails toextract one or more of the desired subject, verb, orobject, based on the sentence at hand and without re-gard to the target content.
Second are content errors,responses for which our system extracts the desiredsubject, verb and object, but the resulting triple doesnot accurately describe the image (i.e., is an error ofthe participant?s).
We are of course concerned withreducing the triple errors.
Examples are in Table 2.Triple errors are subcategorized as speaker,parser, or extraction errors, based on the earliestpart of the process that led to the error.
Speakererrors typically involve misspellings in the originalsentence, leading to an incorrect POS tag and parse.Parser errors involve a correct sentence parsed in-correctly or in such a way as to indicate a differentmeaning from the one intended; an example is givenin Figure 5.
Extraction errors involve a failure of theextraction script to find one or more of the desiredsubject, verb or object in a correct sentence.
Thesetypically involve more complex sentence structuressuch as conjoined or embedded clauses.vroot Two boys boatCD NNS NNnumrootdepNONE(boys,NONE)vroot Two boys boatCD NNS VBPnumrootnsubjboat(boys,NONE)Figure 5: A parser error leading to a triple error (top), andthe desired parse and triple (bottom).As shown in table 2, we obtain 92.3% accuracy onextraction for NNS data and roughly the same forNS data, 92.9%.
However, many of the errors forNNSs involve misspellings, while for NSs a higherpercentage of the extraction errors stem only fromour hand-written extractor, due to native speakersusing more complex structures.
For a system inter-acting with learners, spelling errors are thus more ofa priority (cf.
Hovermale, 2008).Content errors are subcategorized as spelling ormeaning errors.
Spelling errors involve one or moreof the extracted subject, verb or object being mis-spelled severely enough that the intended spellingcannot be discerned.
A spelling error here is un-like those included in speaker errors above in that itdoes not result in downstream errors and is a well-16Error Exampletype Sentence Triple Count (%)Tripleerror NNSSpeaker A man swipped leaves.
leaves(swipped,man) 16 (4.1%)Parser Two boys boat.
NONE(boys,NONE) 5 (1.3%)Extraction A man is gathering lots of leafs.
gathering(man,lots) 9 (2.3%)Total (390) 30 (7.7%)NSSpeaker (None) 0 (0%)Parser An old man raking leaves on a path.
leaves(man,path) 2 (1.4%)Extraction A man has shot a bird that is falling from the sky.
shot(bird,sky) 8 (5.7%)Total (140) 10 (7.1%)ContenterrorNNS Spelling The artiest is drawing a portret.
drawing(artiest,portret) 36 (9.2%)Meaning The woman is making her laundry.
making(woman,laundry) 23 (5.9%)Total (390) 59 (15.1%)NSSpelling (None) 0 (0%)Meaning A picture is being taken of a girl on a bike.
taken(NONE,picture) 3 (2.1%)Total (140) 3 (2.1%)Table 2: Triple errors and content errors by subcategory, with error rates reported (e.g., 7.7% error = 92.3% accuracy)formed triple except for a misspelled target word.Meaning errors involve an inaccurate word withinthe triple.
This includes misspellings that result in areal but unintended word (e.g., shout(man,bird) in-stead of shoot(man,bird)).The goal of a system is to identify the 15.1% ofNNS sentences which are content errors, in orderto provide feedback.
Currently, the 7.7% triple er-rors would also be grouped into this set, showingthe need for further extraction improvements.
Alsonotable is that three content errors were encounteredamong the NS responses.
All three were meaningerrors involving some meta-description of the imageprompt rather than a direct description of the imagecontents, e.g., A picture is being taken of a girl on abike vs. A girl is riding a bike.5.3 Semantic coverageGiven a fairly accurate extraction system, as re-ported above, we now turn to evaluating how wella gold standard represents unseen data, in terms ofsemantic matching.
To measure coverage, we takethe intuition that a language is defined by nativespeaker usage, so their answers can serve as targets,and use NS triples as our gold standard.
The setof NS responses was manually arbitrated to removeany unacceptable triples (both triple and content er-rors), and the remaining set of lemmatized tripleswas taken as a gold standard set for each item.Similarly, with the focus on coverage, the NNStriples were amended to remove any triple errors.From the remaining NNS triples, we call an appro-priate NNS triple found in the gold standard set atrue positive (TP) (i.e., a correct match), and anappropriate NNS triple not found in the gold stan-dard set a false negative (FN) (i.e., an incorrect non-match), as shown in Table 4.
We adopt standard ter-minology here (TP, FN), but note that we are inves-tigating what should be in the gold standard, mak-ing these false negatives and not false positives.
Toaddress the question of how many (NS) sentenceswe need to obtain good coverage, we define cover-age (=recall) as TP/(TP+FN), and report, in Table 3,23.5% coverage for unique triple types and 50.8%coverage for triple tokens.NNS+ ?NSY TP FPN FN TNTable 4: Contingency table comparing presence of NSforms (Y/N) with correctness (+/?)
of NNS formsWe define an inappropriate NNS triple (i.e., a con-tent error) not found in the gold standard set as a true17Coverage AccuracyItem NS NNS TP TN FN Ty.
Tok.
Ty.
Tok.1 5 14 3 2 9 3/12 23/38 5/14 25/392 6 14 3 5 6 3/9 15/28 8/14 20/323 6 19 5 7 7 5/12 23/30 12/19 30/364 4 8 2 2 4 2/6 32/37 4/8 34/395 4 24 1 8 15 1/16 3/25 9/24 11/336 8 22 3 5 14 3/17 16/31 8/22 21/367 7 23 5 4 14 5/19 14/35 9/23 18/398 6 23 5 6 11 5/16 10/30 11/22 17/369 7 33 3 12 18 3/21 3/23 15/33 15/3510 5 21 2 13 6 2/8 14/24 15/21 27/35Total 58 201 32 64 104 32/136 153/301 96/200 218/36023.5% 50.8% 48.0% 60.6%Table 3: Matching of semantic triples: NS/NNS: number of unique triples for NSs/NNSs.
Comparing NNS types to NStriples, TP: number of true positives (types); TN: number of true negatives; FN: number of false negatives.
Coveragefor Types and Tokens = TPTP+FN ; Accuracy for Types and Tokens =TP+TNTP+TN+FNnegative (TN) (i.e., a correct non-match).
Accu-racy based on this gold standard?assuming perfectextraction?is defined as (TP+TN)/(TP+TN+FN).3We report 48.0% accuracy for types and 60.6% ac-curacy for tokens.The immediate lesson here is: NS data alone maynot make a sufficient gold standard, in that many cor-rect NNS answers are not counted as correct.
How-ever, there are a couple of issues to consider here.First, we require exact matching of triples.
Ifmaximizing coverage is desired, extracting indi-vidual subjects, verbs and objects from NS triplesand recombining them into the various possibleverb(subj,obj) combinations would lead to a sizableimprovement.
An example of triples distribution andcoverage for a single item, along with this recombi-nation approach is presented in Table 5.It should be noted, however, that automat-ing this recombination without lexical knowledgecould lead to the presence of unwanted triplesin the gold standard set.
Consider, for exam-ple, do(woman,shirt)?an incorrect triple derivedfrom the correct NS triples, wash(woman,shirt) anddo(woman,laundry).
In addition to handling pro-3Accuracy is typically defined as(TP+TN)/(TP+TN+FN+FP), but false positives (FPs) arecases where an incorrect learner response was in the goldstandard, and we have already removed such cases (i.e., FP=0).Type NNS NS Coveragecut(woman,apple) 5 0 (5)cut(someone,apple) 4 2 4cut(somebody,apple) 3 0cut(she,apple) 3 0slice(someone,apple) 2 5 2cut(person,apple) 2 1 2cut(NONE,apple) 2 0 (2)slice(woman,apple) 1 1 1slice(person,apple) 1 1 1slice(man,apple) 1 0cut(person,fruit) 1 0cut(people,apple) 1 0cut(man,apple) 1 0cut(knife,apple) 1 0chop(woman,apple) 1 0chop(person,apple) 1 0slice(NONE,apple) 0 2Total 30 12 10 (17)Table 5: Distribution of valid tokens across types for asingle PDT item.
Types in italics do not occur in the NSsample, but could be inferred to expand coverage by re-combining elements of NS types that do occur.nouns (e.g., cut(she,apple)) and lexical relations(e.g., apple as a type of fruit), one approach might be18to prompt NSs to give multiple alternative descrip-tions of each PDT item.A second issue to consider is that, even when onlyexamining cases where the meaning is literally cor-rect, NNSs produce a wider range of forms to de-scribe the prompts than NSs.
For example, for a pic-ture showing what NSs overwhelmingly describedas a raking action, many NNSs referred to a mancleaning an area.
Literally, this may be true, but it isnot native-like.
This behavior is somewhat expected,given that learners are encouraged to use words theyknow to compensate for gaps in their vocabularies(Agust?
?n Llach, 2010).
This also parallels the obser-vation in SLA research that while second languagelearners may attain native-like grammar, their abil-ity to use pragmatically native-like language is oftenmuch lower (Bardovi-Harlig and Do?rnyei, 1998).The answer to what counts as a correct meaningwill most likely lie in the purpose of an application,reflecting whether one is developing native-ness orwhether the facts of a situation are expressed cor-rectly.
In other words, rather than rejecting all non-native-like responses, an ILT may need to considerwhether a sentence is native-like or non-native-likeas well as whether it is semantically appropriate.6 Summary and OutlookWe have begun the process of examining appro-priate ways to analyze the semantics of languagelearner constructions for interactive situations bydescribing data collected for a picture descriptiontask.
We parsed this data using an off-the-shelfparser with settings geared towards obtaining appro-priate semantic forms, wrote a small set of seman-tic extraction rules, and obtained 92?93% extrac-tion accuracy.
This shows promise at using imagesto constrain the syntactic form of a ?free?
learnertext and thus be able to use pre-built software.
Atthe same time, we discussed how learners give re-sponses which are literally correct, but are non-native-like.
These results can help guide the de-velopment of ILTs which aim to process the mean-ing of interactive statements: there is much to begained with a small amount of computational effort,but much work needs to go into delineating a properset of gold standard forms.There are several ways to take this work.
First,given the preponderance of spelling errors in NNSdata and its effect on downstream processing, the ef-fect of automatic spelling correction must be takeninto account.
Secondly, we only investigated tran-sitive verbs, and much needs to be done to investi-gate interactions with other types of constructions,including the definition of more elaborate semanticforms (Hahn and Meurers, 2012).
Finally, to bet-ter model ILTs and the interactions found in activ-ities and games, one can begin by modeling morecomplex visual prompts.
By using video descriptiontasks or story retell tasks, we can elicit more com-plex narrative responses.
This would allow us toinvestigate the possibility of extending our currentapproach to tasks that involve greater learner inter-action.AcknowledgmentsWe would like to thank the task participants, DavidStringer for assistance in developing the task, Kath-leen Bardovi-Harlig, Marlin Howard and JaysonDeese for recruitment help, and Ross Israel for eval-uation discussion.
For their helpful feedback, wewould also like to thank the three anonymous re-viewers and the attendees of the Indiana UniversityLinguistics Department Graduate Student Confer-ence.ReferencesEneko Agirre, Mona Diab, Daniel Cer, and AitorGonzalez-Agirre.
2012.
Semeval-2012 task 6: apilot on semantic textual similarity.
In Proceed-ings of the First Joint Conference on Lexical andComputational Semantics - Volume 1: Proceed-ings of the main conference and the shared task,and Volume 2: Proceedings of the Sixth Inter-national Workshop on Semantic Evaluation, Se-mEval ?12, pages 385?393.
Association for Com-putational Linguistics, Stroudsburg, PA, USA.Maria Pilar Agust?
?n Llach.
2010.
Lexical gap-fillingmechanisms in foreign language writing.
System,38(4):529 ?
538.Luiz Amaral and Detmar Meurers.
2011.
On usingintelligent computer-assisted language learning inreal-life foreign language teaching and learning.ReCALL, 23(1):4?24.19Luiz Amaral, Detmar Meurers, and Ramon Ziai.2011.
Analyzing learner language: Towards aflexible NLP architecture for intelligent languagetutors.
Computer Assisted Language Learning,24(1):1?16.Kathleen Bardovi-Harlig and Zolta?n Do?rnyei.
1998.Do language learners recognize pragmatic vio-lations?
Pragmatic versus grammatical aware-ness in instructed L2 learning.
TESOL Quarterly,32(2):233?259.Marianne Celce-Murcia.
1991.
Grammar pedagogyin second and foreign language teaching.
TESOLQuarterly, 25:459?480.Marianne Celce-Murcia.
2002.
Why it makes senseto teach grammar through context and throughdiscourse.
In Eli Hinkel and Sandra Fotos, editors,New perspectives on grammar teaching in secondlanguage classrooms, pages 119?134.
LawrenceErlbaum, Mahwah, NJ.David L. Chen and William B. Dolan.
2011.
Collect-ing highly parallel data for paraphrase evaluation.In Proceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics (ACL-2011).
Portland, OR.Martin Chodorow, Joel Tetreault, and Na-Rae Han.2007.
Detection of grammatical errors involv-ing prepositions.
In Proceedings of the 4th ACL-SIGSEM Workshop on Prepositions, pages 25?30.Prague.Michael Collins.
1999.
Head-Driven StatisticalModels for Natural Language Parsing.
Ph.D. the-sis, University of Pennsylvania, Philadelphia, PA.Ronan Collobert, Jason Weston, Le?on Bottou,Michael Karlen, Koray Kavukcuoglu, and PavelKuksa.
2011.
Natural language processing (al-most) from scratch.
Journal of Machine LearningResearch (JMLR), 12:2461?2505.Robert Dale, Ilya Anisimoff, and George Narroway.2012.
HOO 2012: A report on the prepositionand determiner error correction shared task.
InProceedings of the Seventh Workshop on BuildingEducational Applications Using NLP, pages 54?62.
Montre?al.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.In Proceedings of LREC 2006.
Genoa, Italy.Marie-Catherine de Marneffe and Christopher D.Manning.
2012.
Stanford typed dependenciesmanual.
Originally published in September 2008;Revised for Stanford Parser v. 2.0.4 in November2012.William DeSmedt.
1995.
Herr Kommissar: AnICALL conversation simulator for intermedi-ate german.
In V. Holland, J. Kaplan, andM.
Sams, editors, Intelligent Language Tutors.Theory Shaping Technology, pages 153?174.Lawrence Erlbaum Associates, Inc., New Jersey.Rod Ellis.
2000.
Task-based research and lan-guage pedagogy.
Language teaching research,4(3):193?220.Rod Ellis.
2006.
Current issues in the teaching ofgrammar: An SLA perspective.
TESOL Quar-terly, 40:83?107.Christiane Fellbaum, editor.
1998.
WordNet: AnElectronic Lexical Database.
The MIT Press,Cambridge, MA.Katrina Forbes-McKay and Annalena Venneri.2005.
Detecting subtle spontaneous language de-cline in early Alzheimer?s disease with a pic-ture description task.
Neurological sciences,26(4):243?254.Michael Hahn and Detmar Meurers.
2012.
Evalu-ating the meaning of answers to reading compre-hension questions: A semantics-based approach.In Proceedings of the 7th Workshop on Innova-tive Use of NLP for Building Educational Appli-cations (BEA7), pages 326?336.
Association forComputational Linguistics, Montreal, Canada.Trude Heift and Devlan Nicholson.
2001.
Web de-livery of adaptive and interactive language tutor-ing.
International Journal of Artificial Intelli-gence in Education, 12(4):310?325.Trude Heift and Mathias Schulze.
2007.
Errorsand Intelligence in Computer-Assisted LanguageLearning: Parsers and Pedagogues.
Routledge.DJ Hovermale.
2008.
Scale: Spelling correctionadapted for learners of English.
Pre-CALICOWorkshop on ?Automatic Analysis of Learner20Language: Bridging Foreign Language Teach-ing Needs and NLP Possibilities?.
March 18-19,2008.
San Francisco, CA.Dan Klein and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings ofACL-03.
Sapporo, Japan.Sandra Ku?bler, Ryan McDonald, and Joakim Nivre.2009.
Dependency Parsing.
Morgan & ClaypoolPublishers.Diane Larsen-Freeman.
2002.
Teaching grammar.In Diane Celce-Murcia, editor, Teaching Englishas a second or foreign language, pages 251?266.Heinle & Heinle, Boston, third edition.Claudia Leacock and Martin Chodorow.
2003.
C-rater: Automated scoring of short-answer ques-tions.
Computers and Humanities, pages 389?405.Detmar Meurers.
2012.
Natural language processingand language learning.
In Carol A. Chapelle, ed-itor, Encyclopedia of Applied Linguistics.
Black-well.
to appear.Detmar Meurers, Ramon Ziai, Niels Ott, and StaceyBailey.
2011.
Integrating parallel analysis mod-ules to evaluate the meaning of answers to readingcomprehension questions.
Special Issue on Free-text Automatic Evaluation.
International Journalof Continuing Engineering Education and Life-Long Learning (IJCEELL), 21(4):355?369.Kenneth A. Petersen.
2010.
Implicit CorrectiveFeedback in Computer-Guided Interaction: DoesMode Matter?
Ph.D. thesis, Georgetown Univer-sity, Washington, DC.Peter Skehan, Pauline Foster, and Uta Mehnert.1998.
Assessing and using tasks.
In Willy Re-nandya and George Jacobs, editors, Learners andlanguage learning, pages 227?248.
Seameo Re-gional Language Centre.21
