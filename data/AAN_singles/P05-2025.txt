Proceedings of the ACL Student Research Workshop, pages 145?150,Ann Arbor, Michigan, June 2005. c?2005 Association for Computational LinguisticsUnsupervised Discrimination and Labelingof Ambiguous NamesAnagha K. KulkarniDepartment of Computer ScienceUniversity Of MinnesotaDuluth, MN 55812kulka020@d.umn.eduhttp://senseclusters.sourceforge.netAbstractThis paper describes adaptations of unsu-pervised word sense discrimination tech-niques to the problem of name discrimina-tion.
These methods cluster the contextscontaining an ambiguous name, such thateach cluster refers to a unique underlyingperson or place.
We also present new tech-niques to assign meaningful labels to thediscovered clusters.1 IntroductionA name assigned to an entity is often thought to bea unique identifier.
However this is not always true.We frequently come across multiple people sharingthe same name, or cities and towns that have iden-tical names.
For example, the top ten results fora Google search of John Gilbert return six differ-ent individuals: A famous actor from the silent filmera, a British painter, a professor of Computer Sci-ence, etc.
Name ambiguity is relatively common,and makes searching for people, places, or organiza-tions potentially very confusing.However, in many cases a human can distinguishbetween the underlying entities associated with anambiguous name with the help of surrounding con-text.
For example, a human can easily recognize thata document that mentions Silent Era, Silver Screen,and The Big Parade refers to John Gilbert the ac-tor, and not the professor.
Thus the neighborhood ofthe ambiguous name reveals distinguishing featuresabout the underlying entity.Our approach is based on unsupervised learningfrom raw text, adapting methods originally proposedby (Purandare and Pedersen, 2004).
We do notutilize any manually created examples, knowledgebases, dictionaries, or ontologies in formulating oursolution.
Our goal is to discriminate among multi-ple contexts that mention a particular name strictlyon the basis of the surrounding contents, and assignmeaningful labels to the resulting clusters that iden-tify the underlying entity.This paper is organized as follows.
First, we re-view related work in name discrimination and clus-ter labeling.
Next we describe our methodologystep-by-step and then review our experimental dataand results.
We conclude with a discussion of ourresults and outline our plans for future work.2 Related WorkA number of previous approaches to name discrim-ination have employed ideas related to context vec-tors.
(Bagga and Baldwin, 1998) proposed a methodusing the vector space model to disambiguate ref-erences to a person, place, or event across mul-tiple documents.
Their approach starts by usingthe CAMP system to find related references withina single document.
For example, it might deter-mine that he and the President refers to Bill Clin-ton.
CAMP creates co-reference chains for each en-tity in a single document, which are then extractedand represented in the vector space model.
Thismodel is used to find the similarity among referents,and thereby identify the same referent that occurs inmultiple documents.
(Mann and Yarowsky, 2003) take an approach to145name discrimination that incorporates informationfrom the World Wide Web.
They propose to usevarious contextual characteristics that are typicallyfound near and within an ambiguous proper-nounfor the purpose of disambiguation.
They utilize cat-egorical features (e.g., age, date of birth), familialrelationships (e.g., wife, son, daughter) and associ-ations that the entity frequently shows (e.g.
coun-try, company, organization).
Such biographical in-formation about the entities to be disambiguated ismined from the Web using a bootstrapping method.The Web pages containing the ambiguous name areassigned a vector depending upon the extracted fea-tures and then these vectors are grouped using ag-glomerative clustering.
(Pantel and Ravichandran, 2004) have proposedan algorithm for labeling semantic classes, whichcan be viewed as a form of cluster.
For example, asemantic class may be formed by the words: grapes,mango, pineapple, orange and peach.
Ideally thiscluster would be labeled as the semantic class offruit.
Each word of the semantic class is representedby a feature vector.
Each feature consists of syn-tactic patterns (like verb-object) in which the wordoccurs.
The similarity between a few features fromeach cluster is found using point-wise mutual infor-mation (PMI) and their average is used to group andrank the clusters to form a grammatical template orsignature for the class.
Then syntactic relationshipssuch as Noun like Noun or Noun such as Noun aresearched for in the templates to give the cluster anappropriate name label.
The output is in the formof a ranked list of concept names for each semanticclass.3 Feature IdentificationWe start by identifying features from a corpus oftext which we refer to as the feature selection data.This data can be the test data, i.e., the contexts to beclustered (each of which contain an occurrence ofthe ambiguous name) or it may be a separate cor-pus.
The identified features are used to translateeach context in the test data to a vector form.We are exploring the use of bigrams as our fea-ture type.
These are lexical features that consist ofan ordered pair of words which may occur next toeach other, or have one intervening word.
We areinterested in bigrams since they tend to be less am-biguous and more specific than individual unigrams.In order to reduce the amount of noise in the featureset, we discard all bigrams that occur only once, orthat have a log-likelihood ratio of less than 3.841.The latter criteria indicates that the words in the bi-gram are not independent (i.e., are associated) with95% certainty.
In addition, bigrams in which eitherword is a stop word are filtered out.4 Context RepresentationWe employ both first and second order representa-tions of the contexts to be clustered.
The first orderrepresentation is a vector that indicates which of thefeatures identified during the feature selection pro-cess occur in this context.The second order context representation isadapted from (Schu?tze, 1998).
First a co-occurrencematrix is constructed from the features identified inthe earlier stage, where the rows represent the firstword in the bigram, and the columns represent thesecond word.
Each cell contains the value of thelog-likelihood ratio for its respective row and col-umn word-pair.This matrix is both large and sparse, so we useSingular Value Decomposition (SVD) to reduce thedimensionality and smooth the sparsity.
SVD hasthe effect of compressing similar columns together,and then reorganizing the matrix so that the mostsignificant of these columns come first in the ma-trix.
This allows the matrix to be represented morecompactly by a smaller number of these compressedcolumns.The matrix is reduced by a factor equal to the min-imum of 10% of the original columns, or 300.
Ifthe original number of columns is less than 3,000then the matrix is reduced to 10% of the numberof columns.
If the matrix has greater than 3,000columns, then it is reduced to 300.Each row in the resulting matrix is a vector for theword the row represents.
For the second order repre-sentation, each context in the test data is representedby a vector which is created by averaging the wordvectors for all the words in the context.The philosophy behind the second order repre-sentation is that it captures indirect relationshipsbetween bigrams which cannot be done using the146first order representation.
For example if the wordergonomics occurs along with science, and work-place occurs with science, but not with ergonomics,then workplace and ergonomics are second orderco-occurrences by virtue of their respective co-occurrences with science.Once the context is represented by either a firstorder or a second order vector, then clustering canfollow.
A hybrid method known as Repeated Bisec-tions is employed, which tries to balance the qualityof agglomerative clustering with the speed of parti-tional methods.
In our current approach the numberof clusters to be discovered must be specified.
Mak-ing it possible to automatically identify the numberof clusters is one of our high priorities for futurework.5 LabelingOnce the clusters are created, we assign each clustera descriptive and discriminating label.
A label is alist of bigrams that act as a simple summary of thecontents of the cluster.Our current approach for descriptive labels is toselect the top N bigrams from contexts grouped in acluster.
We use similar techniques as we use for fea-ture identification, except now we apply them on theclustered contexts.
In particular, we select the top 5or 10 bigrams as ranked by the log-likelihood ratio.We discard bigrams if either of the words is a stop-word, or if the bigram occurs only one time.
For dis-criminating labels we pick the top 5 or 10 bigramswhich are unique to the cluster and thus capture thecontents that separates one cluster from another.6 Experimental DataOur experimental data consists of two or more un-ambiguous names whose occurrences in a corpushave been conflated in order to create ambiguity.These conflated forms are sometimes known aspseudo words.
For example, we take all occurrencesof Tony Blair and Bill Clinton and conflate them intoa single name that we then attempt to discriminate.Further, we believe that the use of artificial pseudowords is suitable for the problem of name discrim-ination, perhaps more so than is the case in wordsense disambiguation in general.
For words there isalways a debate as to what constitutes a word sense,and how finely drawn a sense distinction should bemade.
However, when given an ambiguous namethere are distinct underlying entities associated withthat name, so evaluation relative to such true cate-gories is realistic.Our source of data is the New York Times (Jan-uary 2000 to June 2002) corpus that is included as apart of the English GigaWord corpus.In creating the contexts that include our conflatednames, we retain 25 words of text to the left and alsoto the right of the ambiguous conflated name.
Wealso preserve the original names in a separate tag forthe evaluation stage.We have created three levels of ambiguity: 2-way,3-way, and 4-way.
In each of the three categories wehave 3-4 examples that represent a variety of differ-ent degrees of ambiguity.
We have created severalexamples of intra-category disambiguation, includ-ing Bill Clinton and Tony Blair (political leaders),and Mexico and India (countries).
We also haveinter-category disambiguation such as Bayer, Bankof America, and John Grisham (two companies andan author).The 3-way examples have been chosen by addingone more dimension to the 2-way examples.
For ex-ample, Ehud Barak is added to Bill Clinton and TonyBlair, and the 4-way examples are selected on simi-lar lines.7 Experimental ResultsTable 1 summarizes the results of our experiments interms of the F-Measure, which is the harmonic meanof precision and recall.
Precision is the percentageof contexts clustered correctly out of those that wereattempted.
Recall is the percentage of contexts clus-tered correctly out of the total number of contextsgiven.The variable M in Table 1 shows the number ofcontexts of that target name in the input data.
Notethat we divide the total input data into equal-sizedtest and feature selection files, so the number of fea-ture selection and test contexts is half of what isshown, with approximately the same distribution ofnames.
(N) specifies the total number of contexts inthe input data.
MAJ. represents the percentage ofthe majority name in the data as a whole, and can beviewed as a baseline measure of performance that147Table 1: Experimental Results (F-measure)MAJ. K Order 1 Order 2Target Word(M);+ (N) FSD TST FSD FSD/S TST TST/SBAYER(1271); 60.0 2 67.2 68.6 71.0 51.3 69.2 53.2BOAMERICA(846) (2117) 6 37.4 33.9 47.2 53.3 42.8 49.6BCLINTON(1900); 50.0 2 82.2 87.6 81.1 81.2 81.2 70.3TBLAIR(1900) (3800) 6 58.5 61.6 61.8 71.4 61.5 72.3MEXICO(1500); 50.0 2 42.3 52.4 52.7 54.5 52.6 54.5INDIA(1500) (3000) 6 28.4 36.6 37.5 49.0 37.9 52.4THANKS(817); 55.6 2 61.2 65.3 61.4 56.7 61.4 56.7RCROWE(652) (1469) 6 36.3 41.2 38.5 52.0 39.9 47.8BAYER(1271);BOAMERICA(846); 43.2 3 69.7 73.7 57.1 54.7 55.1 54.7JGRISHAM(828); (2945) 6 31.5 38.4 32.7 53.1 32.8 52.8BCLINTON(1900);TBLAIR(1900); 33.3 3 51.4 56.4 47.7 44.8 47.7 44.9EBARAK(1900); (5700) 6 58.0 54.1 43.8 48.1 43.7 48.1MEXICO(1500);INDIA(1500); 33.3 3 40.4 41.7 38.1 36.5 38.2 37.4CALIFORNIA(1500) (4500) 6 31.5 38.4 32.7 36.2 32.8 36.2THANKS(817);RCROWE(652); 35.4 4 42.7 61.5 42.9 38.5 42.7 37.6BAYER(1271);BOAMERICA(846) (3586) 6 47.0 53.0 43.9 34.0 43.5 34.6BCLINTON(1900);TBLAIR(1900); 25.0 4 48.4 52.3 44.2 50.1 44.7 51.4EBARAK(1900);VPUTIN(1900) (7600) 6 51.8 47.8 43.4 49.3 44.4 50.6MEXICO(1500);INDIA(1500); 25.0 4 34.4 35.7 29.2 27.4 29.2 27.1CALIFORNIA(1500);PERU(1500) (6000) 6 31.3 32.0 27.3 27.2 27.2 27.2Table 2: Sense Assignment Matrix (2-way)TBlair BClintonC0 784 50 834C1 139 845 984923 895 1818would be achieved if all the contexts to be clusteredwere placed in a single cluster.K is the number of clusters that the method willattempt to classify the contexts into.
FSD are theexperiments where a separate set of data is used asthe feature selection data.
TST are the experimentswhere the features are extracted from the test data.For FSD and TST experiments, the complete contextwas used to create the context vector to be clustered,whereas for FSD/S and TST/S in the order 2 experi-ments, only the five words on either side of the targetname are averaged to form the context-vector.For each name conflated sample we evaluate ourTable 3: Sense Assignment Matrix (3-way)BClinton TBlair EBarakC0 617 57 30 704C1 65 613 558 1236C2 215 262 356 833897 932 944 2773methods by setting K to the exact number of clus-ters, and then for 6 clusters.
The motivation for thehigher value is to see how well the method performswhen the exact number of clusters is unknown.
Ourbelief is that with an artificially- high number spec-ified, some of the resulting clusters will be nearlyempty, and the overall results will still be reason-able.
In addition, we have found that the precisionof the clusters associated with the known names re-mains high, while the overall recall is reduced due tothe clusters that can not be associated with a name.To evaluate the performance of the clustering,148Table 4: Labels for Name Discrimination Clusters (found in Table 1)Original Name Type Created LabelsCLUSTER 0: Desc.
Britain, British Prime, Camp David, Middle East, Minister, New York,TONY Prime, Prime Minister, U S, Yasser ArafatBLAIR Disc.
Britain, British Prime, Middle East, Minister, Prime, Prime MinisterCLUSTER 1: Desc.
Al Gore, Ariel Sharon, Camp David, George W, New York, U S, W Bush,BILL White House, prime ministerCLINTON Disc.
Al Gore, Ariel Sharon, George W, W BushCLUSTER 2: Desc.
Bill Clinton, Camp David, New York, President, U S, White House,EHUD Yasser Arafat, York Times, minister, prime ministerBARAK Disc.
Bill Clinton, President, York Times, ministera contingency matrix (e.g., Table 2 or 3) is con-structed.
The columns are re-arranged to maximizethe sum of the cells along the main diagonal.
Thisre-arranged matrix decides the sense that gets as-signed to the cluster.8 DiscussionThe order 2 experiments show that limiting thescope in the test contexts (and thereby creating anaveraged vector from a subset of the context) is moreeffective than using the entire context.
This corre-sponds to the findings of (Pedersen et.al., 2005).
Thewords closest to the target name are most likely tocontain identifying information, whereas those thatare further away may be more likely to introducenoise.As the amount and the number of contexts to beclustered (and to be used for feature identification)increases, the order 1 context representation per-forms better.
This is because in the larger samples ofdata it is more likely to find an exact match for a fea-ture and thereby achieve overall better results.
Webelieve that this is why the order 1 results are gener-ally better for the 3-way and 4-way distinctions, asopposed to the 2-way distinctions.
This observationis consistent with earlier findings by Purandare andPedersen for general English text.An example of a 2-way clustering is shown in Ta-ble 2, where Cluster 0 is assigned to Tony Blair, andCluster 1 is for Bill Clinton.
In this case the preci-sion is 89.60 ((1629/1818)*100), whereas the recallis 85.69 ((1629/1818+83)*100).
This suggests thatthere were 83 contexts that the clustering algorithmwas unable to assign, and so they were not clusteredand removed from the results.Table 3 shows the contingency matrix for a 3-way ambiguity.
The distribution of contexts in clus-ter 0 show that the single predominant sense in thecluster is Bill Clinton, but for cluster 1 though thenumber of contexts indicate clear demarcation be-tween BClinton and TBlair, this distinction gets lessclear between TBlair and EBarak.
This suggests thatperhaps the level of details in the New York Timesregarding Bill Clinton and his activities may havebeen greater than that for the two non-US leaders,although we will continue to analyze results of thisnature.We can see from the labeling results shown in Ta-ble 4 that clustering performance affects the qualityof cluster labels.
Thus the quality of labels for clus-ter assigned to BClinton and TBlair are more sug-gestive of the underlying entity than are the labelsfor EBarak clusters.9 Future WorkWe wish to supplement our cluster labeling tech-nique by using World Wide Web (WWW) basedmethods (like Google-Sets) for finding words relatedto the target name and other significant words in thecontext.
This would open up a venue for large andmulti-dimensional data.
We are cautious though thatwe would have to deal with the problems of noisydata that WWW brings along with the good data.Another means of improving the clustering labelingwill be using WordNet::Similarity to find the relat-edness amongst the words from the cluster using theknowledge of WordNet as is also proposed by (Mc-Carthy et.al., 2004).149Currently the number of clusters that the con-texts should be grouped into has to be specified bythe user.
We wish to automate this process suchthat the clustering algorithm will automatically de-termine the optimal number of clusters.
We are ex-ploring a number of options, including the use ofGAP statistic (Tibshirani et.al., 2000).For the order 2 representation of the contexts thereis considerable noise induced in the resulting con-text vector because of the averaging of all the word-vectors.
Currently we reduce the noise in the av-eraged vector by limiting the word vectors to thoseassociated with words that are located near the tar-get name.
We also plan to develop methods that se-lect the words to be included in the averaged vec-tor more carefully, with an emphasis on locating themost content rich words in the context.Thus far we have tested our methods for one-to-many discrimination.
This resolves cases wherethe same name is used by multiple different peo-ple.
However, we will also test our techniques forthe many-to-one kind ambiguity that occurs whenthe same person is referred by multiple names, e.g.,President Bush, George Bush, Mr. Bush, and Presi-dent George W. Bush.Finally, we will also evaluate our method on realdata.
In particular, we will use the John Smith Cor-pus as compiled by Bagga and Baldwin, and thename data generated by Mann and Yarowsky fortheir experiments.10 ConclusionsWe have shown that word sense discrimination tech-niques can be extended to address the problem ofname discrimination.
The experiments with secondorder context representation work better with limitedor localized scope.
As the dimensionality of the am-biguity increases first order context representationout-performs second order representation.
The la-beling of clusters using the simple technique of sig-nificant bigram selection also shows encouraging re-sults which highly depends on the performance ofthe clustering of contexts.11 AcknowledgmentsI would like to thank my advisor Dr. Ted Pedersenfor his continual guidance and support.I would also like to thank Dr. James Riehl, Deanof the College of Science and Engineering, and Dr.Carolyn Crouch, Director of Graduate Studies inComputer Science, for awarding funds to partiallycover the expenses to attend the Student ResearchWorkshop at ACL 2005.I am also thankful to Dr. Regina Barzilay andthe ACL Student Research Workshop organizers forawarding the travel grant.This research has been supported by a NationalScience Foundation Faculty Early CAREER Devel-opment Award (#0092784) during the 2004-2005academic year.ReferencesAmruta Purandare and Ted Pedersen.
2004.
Word sensediscrimination by clustering contexts in vector andsimilarity spaces.
The Proceedings of the Conferenceon Computational Natural Language Learning, pages41-48.
Boston, MA.Gideon Mann and David Yarowsky.
2003.
Unsupervisedpersonal name disambiguation.
The Proceedings ofthe Conference on Computational Natural LanguageLearning, pages 33-40.
Edmonton, Canada.Amit Bagga and Breck Baldwin.
1998.
Entity?basedcross?document co?referencing using the vector spacemodel.
The Proceedings of the 17th internationalconference on Computational linguistics, pages 79-85.Montreal, Quebec, Canada.Patrick Pantel and Deepak Ravichandran.
2004.
Auto-matically Labeling Semantic Classes.
The Proceed-ings of HLT-NAACL, pages 321-328.
Boston, MA.Diana McCarthy, Rob Koeling, Julie Weeds andJohn Carroll.
2004.
Finding Predominant WordSenses in Untagged Text.
The Proceedings of the 42ndAnnual Meeting of the Association for ComputationalLinguistics, pages 279-286.
Barcelona, Spain.Robert Tibshirani, Guenther Walther and Trevor Hastie.2000.
Estimating the number of clusters in a datasetvia the Gap statistic.
Journal of the Royal StatisticsSociety (Series B), 2000.Ted Pedersen, Amruta Purandare and Anagha Kulkarni2005.
Name Discrimination by Clustering SimilarContexts.
The Proceedings of the Sixth InternationalConference on Intelligent Text Processing and Com-putational Linguistics, pages 226-237.
Mexico City,Mexico.Schu?tze H. 1998.
Automatic Word Sense DiscriminationComputational Linguistics, 24(1):97-124.150
