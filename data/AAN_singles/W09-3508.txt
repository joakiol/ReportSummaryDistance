Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 44?47,Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLPExperiences with English-Hindi, English-Tamil and English-KannadaTransliteration Tasks at NEWS 2009Manoj Kumar Chinnakotla and Om P. DamaniDepartment of Computer Science and Engineering,IIT Bombay,Mumbai, India{manoj,damani}@cse.iitb.ac.inAbstractWe use a Phrase-Based Statistical Ma-chine Translation approach to Translitera-tion where the words are replaced by char-acters and sentences by words.
We employthe standard SMT tools like GIZA++ forlearning alignments and Moses for learn-ing the phrase tables and decoding.
Be-sides tuning the standard SMT parame-ters, we focus on tuning the Character Se-quence Model (CSM) related parameterslike order of the CSM, weight assigned toCSM during decoding and corpus used forCSM estimation.
Our results show thatpaying sufficient attention to CSM paysoff in terms of increased transliteration ac-curacies.1 IntroductionTransliteration of Named-Entities (NEs) is an im-portant problem that affects the accuracy of manyNLP applications like Cross Lingual Search andMachine Translation.
Transliteration is definedas the process of automatically mapping a givengrapheme sequence in the source language to agrapheme sequence in the target language suchthat it preserves the pronunciation of the origi-nal source word.
A Grapheme refers to the unitof written language which expresses a phonemein the language.
Multiple alphabets could beused to express a grapheme.
For example, shis considered a single grapheme expressing thephoneme /SH/.
For phonetic orthography like De-vanagari, each grapheme corresponds to a uniquephoneme.
However, for English, a grapheme likec may map to multiple phonemes /S/,/K/.
An ex-ample of transliteration is mapping the Devana-gari grapheme sequence E?s h{rF to its phoneti-cally equivalent grapheme sequence Prince Harryin English.This paper discusses our transliteration ap-proach taken for the NEWS 2009 MachineTransliteration Shared Task [Li et al2009b, Li etal.2009a].
We model the transliteration problemas a Phrased-Based Machine Translation prob-lem.
Later, using the development set, we tunethe various parameters of the system like order ofthe Character Sequence Model (CSM), typicallycalled language model, weight assigned to CSMduring decoding and corpus used to estimate theCSM.
Our results show that paying sufficient at-tention to the CSM pays off in terms of improvedaccuracies.2 Phrase-Based SMT Approach toTransliterationIn the Phrase-Based SMT Approach to Transliter-ation [Sherif and Kondrak2007, Huang2005], thewords are replaced by characters and sentences arereplaced by words.
The corresponding noisy chan-nel model formulation where a given english worde is to be transliterated into a foreign word h, isgiven as:h?
= argmaxhPr(h|e)= argmaxhPr(e|h) ?
Pr(h) (1)In Equation 1, Pr(e|h) is known as the translationmodel which gives the probability that the char-acter sequence h could be transliterated to e andPr(h) is known as the character sequence modeltypically called language model which gives theprobability that the character sequence h forms avalid word in the target language.44TaskRunOptimalParameter SetAccuracyintop-1MeanF-scoreMRRMAPrefMAP10MAPsysEnglish-HindiStandardLMOrder: 5,LMWeight:0.60.470.860.580.470.180.20English-HindiNon-standardLMOrder: 5,LMWeight:0.60.520.870.620.520.190.21English-TamilStandardLMOrder: 5,LMWeight:0.30.450.880.560.450.180.18English-KannadaStandardLMOrder: 5,LMWeight:0.30.440.870.550.440.170.18Figure 1: NEWS 2009 Development Set ResultsTaskRunAccuracy intop-1MeanF-scoreMRRMAPrefMAP10MAPsysEnglish-HindiStandard0.420.860.540.420.180.20English-HindiNon-standard0.490.870.590.480.200.23English-TamilStandard0.410.890.540.400.180.18English-KannadaStandard0.360.860.480.350.160.16Figure 2: NEWS 2009 Test Set ResultsGiven the parallel training data pairs, we pre-processed the source (English) and target (Hindi,Tamil and Kannada) strings into character se-quences.
We then ran the GIZA++ [Och andNey2003] aligner with default options to obtainthe character-level alignments.
For alignment, ex-cept for Hindi, we used single character-level unitswithout any segmentation.
In case of Hindi, wedid a simple segmentation where we added thehalant character (U094D) to the previous Hindicharacter.
Moses Toolkit [Hoang et al2007] wasthen used to learn the phrase-tables for English-Hindi, English-Tamil and English-Kannada.
Wealso learnt the character sequence models on thetarget language training words using the SRILMtoolkit [Stolcke2002].
Given a new English word,we split the word into sequence of characters andrun the Moses decoder with the phrase-table of tar-get language obtained above to get the transliter-ated word.
We ran Moses with the DISTINCT op-tion to obtain the top k distinct transliterated op-tions.2.1 Moses Parameter TuningThe Moses decoder computes the cost of eachtranslation as a product of probability costs of fourmodels: a) translation model b) language modelc) distortion model and d) word penalty as shownin Equation 2.
The distortion model controls theTaskRunBaselineModel (LMOrder N=3)Best Run% ImprovementEnglish-HindiStandard0.40.425.00English-HindiNon-standard0.370.4932.43English-TamilStandard0.390.4515.38English-KannadaStandard0.360.360.00Figure 3: Improvements Obtained over Baselineon Test Set due to Language Model Tuningcost of re-ordering phrases (transliteration units)in a given sentence (word) and the word penaltymodel controls the length of the final translation.The parameters ?T , ?CSM , ?D and ?W controlthe relative importance given to each of the abovemodels.Pr(h|e) = PrT (e|h)?T ?
PrCSM (h)?CSM ?PrD(h, e)?D ?
?length(h)?
?W (2)Since no re-ordering of phrases is required duringtranslation task, we assign a zero weight to ?D.Similarly, we varied the word penalty factor ?Wbetween {?1, 0,+1} and found that it achievesmaximum accuracy at 0.
All the above tuning wasdone with a trigram CSM and default weight (0.5)in Moses for ?T .452.2 Improving CSM PerformanceIn addition to the above mentioned parameters,we varied the order of the CSM and the mono-lingual corpus used to estimate the CSM.
For eachtask, we started with a trigram CSM as mentionedabove and tuned both the order of the CSM and?CSM on the development set.
The optimal setof parameters and the development set results areshown in Figure 1.
In addition, we use a mono-lingual Hindi corpus of around 0.4 million doc-uments called Guruji corpus.
We extracted the2.6 million unique words from the above corpusand trained a CSM on that.
This CSM which waslearnt on the monolingual Hindi corpus was usedfor the non-standard Hindi run.
We repeat theabove procedure of tuning the order of CSM and?CSM and find the optimal set of parameters forthe non-standard run on the development set.3 Results and DiscussionThe details of the NEWS 2009 dataset for Hindi,Kannada and Tamil are given in [Li et al2009a,Kumaran and Kellner2007].
The final results ofour system on the test set are shown in Figure 2.Figure 3 shows the improvements obtained on testset by tuning the CSM parameters.
The trigramCSM model used along with the optimal Mosesparameter set tuned on development set was takenas baseline for the above experiments.
The resultsshow that a major improvement (32.43%) was ob-tained in the non-standard run where the monolin-gual Hindi corpus was used to learn the CSM.
Be-cause of the use of monolingual Hindi corpus inthe non-standard run, the transliteration accuracyimproved by 22.5% when compared to the stan-dard run.
The improvements (15.38%) obtained inTamil are also significant.
However, the improve-ment in Hindi standard run was not significant.
InKannada, there was no improvement due to tuningof LM parameters.
This needs further investiga-tion.The above results clearly highlight the impor-tance of improving CSM accuracy since it helpsin improving the transliteration accuracy.
More-over, improving the CSM accuracy only requiresmonolingual language resources which are easyto obtain when compared to parallel transliterationtraining data.4 ConclusionWe presented the transliteration system which weused for our participation in the NEWS 2009 Ma-chine Transliteration Shared Task on Translitera-tion.
We took a Phrase-Based SMT approach totransliteration where words are replaced by char-acters and sentences by words.
In addition to thestandard SMT parameters, we tuned the CSM re-lated parameters like order of the CSM, weight as-signed to CSM and corpus used to estimate theCSM.
Our results show that improving the ac-curacy of CSM pays off in terms of improvedtransliteration accuracies.AcknowledgementsWe would like to thank the Indian search-enginecompany Guruji (http://www.guruji.com)for providing us the Hindi web content which wasused to train the language model for our non-standard Hindi runs.ReferencesHieu Hoang, Alexandra Birch, Chris Callison-burch,Richard Zens, Rwth Aachen, Alexandra Constantin,Marcello Federico, Nicola Bertoldi, Chris Dyer,Brooke Cowan, Wade Shen, Christine Moran, andOndej Bojar.
2007.
Moses: Open Source Toolkitfor Statistical Machine Translation.
In In Proceed-ings of ACL, Demonstration Session, pages 177?180.Fei Huang.
2005.
Cluster-specific Named EntityTransliteration.
In HLT ?05: Proceedings of the con-ference on Human Language Technology and Em-pirical Methods in Natural Language Processing,pages 435?442, Morristown, NJ, USA.
Associationfor Computational Linguistics.A.
Kumaran and Tobias Kellner.
2007.
A GenericFramework for Machine Transliteration.
In SIGIR?07: Proceedings of the 30th annual internationalACM SIGIR conference on Research and develop-ment in information retrieval, pages 721?722, NewYork, NY, USA.
ACM.Haizhou Li, A Kumaran, Vladimir Pervouchine, andMin Zhang.
2009a.
Report on NEWS 2009 Ma-chine Transliteration Shared Task.
In Proceed-ings of ACL-IJCNLP 2009 Named Entities Work-shop (NEWS 2009).Haizhou Li, A Kumaran, Min Zhang, and VladimirPervouchine.
2009b.
Whitepaper of NEWS 2009Machine Transliteration Shared Task.
In Proceed-ings of ACL-IJCNLP 2009 Named Entities Work-shop (NEWS 2009).46Franz Josef Och and Hermann Ney.
2003.
A System-atic Comparison of Various Statistical AlignmentModels.
Computational Linguistics, 29(1):19?51.Tarek Sherif and Grzegorz Kondrak.
2007.
Substring-Based Transliteration.
In In Proceedings of ACL2007.
The Association for Computer Linguistics.Andreas Stolcke.
2002.
SRILM - An Extensible Lan-guage Modeling Toolkit.
In In Proceedings of Intl.Conf.
on Spoken Language Processing.47
