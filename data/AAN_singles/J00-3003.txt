Dialogue Act Modeling forAutomatic Tagging and Recognitionof Conversational SpeechAndreas Stolcke*SRI InternationalNoah CoccaroUniversity of Colorado at BoulderRebecca BatesUniversity of WashingtonPaul TaylorUniversity of EdinburghCarol Van Ess-DykemaU.S.
Department of DefenseKlaus RiesCarnegie Mellon University andUniversity of KarlsruheElizabeth ShribergSRI InternationalDaniel JurafskyUniversity of Colorado at BoulderRachel MartinJohns Hopkins UniversityMarie MeteerBBN TechnologiesWe describe a statistical approach for modeling dialogue acts in conversational speech, i.e., speech-act-like units such as STATEMENT, QUESTION, BACKCHANNEL, AGREEMENT, DISAGREE-MENT, and APOLOGY.
Our model detects and predicts dialogue acts based on lexical, colloca-tional, and prosodic ues, as well as on the discourse coherence of the dialogue act sequence.The dialogue model is based on treating the discourse structure of a conversation as a hiddenMarkov model and the individual dialogue acts as observations emanating from the model states.Constraints on the likely sequence of dialogue acts are modeled via a dialogue act n-gram.
Thestatistical dialogue grammar is combined with word n-grams, decision trees, and neural networksmodeling the idiosyncratic lexical and prosodic manifestations ofeach dialogue act.
We developa probabilistic ntegration of speech recognition with dialogue modeling, to improve both speechrecognition and dialogue act classification accuracy.
Models are trained and evaluated using alarge hand-labeled database of 1,155 conversations from the Switchboard corpus of spontaneoushuman-to-human telephone speech.
We achieved good dialogue act labeling accuracy (65% basedon errorful, automatically recognized words and prosody, and 71% based on word transcripts,compared to a chance baseline accuracy of 35% and human accuracy of 84%) and a small reductionin word recognition error.?
Speech Technology and Research Laboratory, SRI International, 333 Ravenswood Ave., Menlo Park, CA94025, 1-650-859-2544.
E-mail: stolcke@speech.sri.com.
@ 2000 Association for Computational LinguisticsComputational Linguistics Volume 26, Number 3Table 1Fragment of a labeled conversation (from the Switchboard corpus).Speaker Dialogue Act UtteranceA YEs-No-QuESTION So do you go to college right now?A ABANDONED Are yo-,B YES- ANSWER Yeah,B STATEMENT it's my last year \[laughter\].A DECLARATIVE-QUESTION You're a, so you're a senior now.B YEs-ANSWER Yeah,B STATEMENT I'm working on my projects trying to graduate\[laughter\].A APPRECIATION Oh, good for you.B BACKCHANNEL Yeah.A APPRECIATION That's great,A YEs-No-QUESTION um, is, is N C University is that, uh, State,B STATEMENT N C State.A SIGNAL-NoN-UNDERSTANDING What did you say?B STATEMENT N C State.1.
IntroductionThe ability to model and automatically detect discourse structure is an importantstep toward understanding spontaneous dialogue.
While there is hardly consensuson exactly how discourse structure should be described, some agreement exists thata useful first level of analysis involves the identification of dialogue acts (DAs).
ADA represents he meaning of an utterance at the level of illocutionary force (Austin1962).
Thus, a DA is approximately the equivalent of the speech act of Searle (1969),the conversational game move of Power (1979), or the adjacency pair part of Schegloff(1968) and Saks, Schegloff, and Jefferson (1974).Table 1 shows a sample of the kind of discourse structure in which we are inter-ested.
Each utterance is assigned a unique DA label (shown in column 2), drawn froma well-defined set (shown in Table 2).
Thus, DAs can be thought of as a tag set thatclassifies utterances according to a combination of pragmatic, semantic, and syntacticcriteria.
The computational community has usually defined these DA categories so asto be relevant to a particular application, although efforts are under way to developDA labeling systems that are domain-independent, such as the Discourse ResourceInitiative's DAMSL architecture (Core and Allen 1997).While not constituting dialogue understanding in any deep sense, DA taggingseems clearly useful to a range of applications.
For example, a meeting summarizerneeds to keep track of who said what to whom, and a conversational agent needs toknow whether it was asked a question or ordered to do something.
In related workDAs are used as a first processing step to infer dialogue games (Carlson 1983; Levinand Moore 1977; Levin et al 1999), a slightly higher level unit that comprises a smallnumber of DAs.
Interactional dominance (Linell 1990) might be measured more ac-curately using DA distributions than with simpler techniques, and could serve as anindicator of the type or genre of discourse at hand.
In all these cases, DA labels wouldenrich the available input for higher-level processing of the spoken words.
Another im-portant role of DA information could be feedback to lower-level processing.
For exam-ple, a speech recognizer could be constrained by expectations of likely DAs in a givencontext, constraining the potential recognition hypotheses so as to improve accuracy.340Stolcke et al Dialogue Act ModelingTable 2The 42 dialogue act labels.
DA frequencies are given as percentages of the totalnumber of utterances in the overall corpus.Tag Example %STATEMENTBACKCHANNEL/ACKNOWLEDGEOPINIONABANDONED/UNINTERPRETABLEAGREEMENT/ACCEPTAPPRECIATIONYEs-No-QUESTIONNON-VERBALYES ANSWERSCONVENTIONAL-CLOSINGWH-QUESTIONNO ANSWERSRESPONSE ACKNOWLEDGMENTHEDGEDECLARATIVE YES-No-QuESTIONOTHERBACKCHANNEL-QUESTIONQUOTATIONSUMMARIZE/REFORMULATEAFFIRMATIVE NON-YES ANSWERSACTION-DIRECTIVECOLLABORATIVE COMPLETIONREPEAT-PHRASEOPEN-QUESTIONRHETORICAL-QUESTIONSHOLD BEFORE ANSWER/AGREEMENTREJECTNEGATIVE NON-NO ANSWERSSIGNAL-NON-UNDERSTANDINGOTHER ANSWERSCONVENTIONAL-OPENINGOR-CLAUSEDISPREFERRED ANSWERS3RD-PARTY-TALKOFFERS, OPTIONS ~ COMMITSSELF-TALKD OWNPLAYERMAYBE/AcCEPT-PARTTAG-QUESTIONDECLARATIVE WH-QUESTIONAPOLOGYTHANKINGMe, I'm in the legal department.
36%Uh-huh.
19%I think it's great 13%So, -/ 6%That's exactly it.
5%I can imagine.
2%Do you have to have any special training?
2%<Laughter>, < Throat_clearing> 2%Yes.
1%Well, it's been nice talking to you.
1%What did you wear to work today?
1%No.
1%Oh, okay.
1%I don't know if I'm making any sense or not.
1%So you can afford to get a house?
1%Well give me a break, you know.
1%Is that right?
1%You can't be pregnant and have cats .5%Oh, you mean you switched schools for the kids.
.5%It is.
.4%Why don't you go first .4%Who aren't contributing.
.4%Oh, fajitas .3%How about you ?
.3%Who would steal a newspaper?
.2%I'm drawing a blank.
.3%Well, no .2%Uh, not a whole lot.
.1%Excuse me?
.1%I don't know .1%How are you?
.1%or is it more of a company?
.1%Well, not so much that.
.1%My goodness, Diane, get down from there.
.1%I'I1 have to check that out .1%What's the word I'm looking for .1%That's all right.
.1%Something like that <.1%Right?
<.1%You are what kind of buff?
<.1%I'm sorry.
<.1%Hey thanks a lot <.1%The goal of this article is twofold: On the one hand, we aim to present a com-prehensive f ramework for model ing and automatic lassification of DAs, founded onwel l -known statistical methods.
In doing so, we will pull together previous approachesas well as new ideas.
For example, our model draws on the use of DA n-grams and thehidden Markov models of conversation present in earlier work, such as Nagata andMorimoto (1993, 1994) and Woszczyna and Waibel (1994) (see Section 7).
However,our f ramework generalizes earlier models, giving us a clean probabilistic approach forperforming DA classification from unreliable words and nonlexical evidence.
For the341Computational Linguistics Volume 26, Number 3speech recognition task, our framework provides a mathematically principled way tocondition the speech recognizer on conversation context through dialogue structure, aswell as on nonlexical information correlated with DA identity.
We will present meth-ods in a domain-independent framework that for the most part treats DA labels as anarbitrary formal tag set.
Throughout the presentation, we will highlight he simplifi-cations and assumptions made to achieve tractable models, and point out how theymight fall short of reality.Second, we present results obtained with this approach on a large, widely availablecorpus of spontaneous conversational speech.
These results, besides validating themethods described, are of interest for several reasons.
For example, unlike in mostprevious work on DA labeling, the corpus is not task-oriented in nature, and theamount of data used (198,000 utterances) exceeds that in previous tudies by at leastan order of magnitude (see Table 14).To keep the presentation i teresting and concrete, we will alternate between thedescription of general methods and empirical results.
Section 2 describes the taskand our data in detail.
Section 3 presents the probabilistic modeling framework; acentral component of this framework, the discourse grammar, is further discussed inSection 4.
In Section 5 we describe xperiments for DA classification.
Section 6 showshow DA models can be used to benefit speech recognition.
Prior and related work issummarized in Section 7.
Further issues and open problems are addressed in Section 8,followed by concluding remarks in Section 9.2.
The Dialogue Act Labeling TaskThe domain we chose to model is the Switchboard corpus of human-human con-versational telephone speech (Godfrey, Holliman, and McDaniel 1992) distributed bythe Linguistic Data Consortium.
Each conversation i volved two randomly selectedstrangers who had been charged with talking informally about one of several, self-selected general-interest topics.
To train our statistical models on this corpus, we com-bined an extensive ffort in human hand-coding of DAs for each utterance, with avariety of automatic and semiautomatic tools.
Our data consisted of a substantialportion of the Switchboard waveforms and corresponding transcripts, totaling 1,155conversations.2.1 Utterance SegmentationBefore hand-labeling each utterance in the corpus with a DA, we needed to choose anutterance segmentation, as the raw Switchboard ata is not segmented in a linguis-tically consistent way.
To expedite the DA labeling task and remain consistent withother Switchboard-based research efforts, we made use of a version of the corpus thathad been hand-segmented into sentence-level units prior to our own work and in-dependently of our DA labeling system (Meteer et al 1995).
We refer to the units ofthis segmentation as utterances.
The relation between utterances and speaker turnsis not one-to-one: a single turn can contain multiple utterances, and utterances canspan more than one turn (e.g., in the case of backchanneling by the other speaker inmidutterance).
Each utterance unit was identified with one DA, and was annotatedwith a single DA label.
The DA labeling system had special provisions for rare caseswhere utterances seemed to combine aspects of several DA types.Automatic segmentation f spontaneous speech is an open research problem in itsown right (Mast et al 1996; Stolcke and Shriberg 1996).
A rough idea of the difficultyof the segmentation problem on this corpus and using the same definition of utteranceunits can be derived from a recent study (Shriberg et al 2000).
In an automatic labeling342Stolcke t al.
Dialogue Act Modelingof word boundaries as either utterance or nonboundaries u ing a combination oflexicaland prosodic ues, we obtained 96% accuracy based on correct word transcripts, and78% accuracy with automatically recognized words.
The fact that the segmentationand labeling tasks are interdependent (Warnke et al 1997; Finke et al 1998) furthercomplicates the problem.Based on these considerations, we decided not to confound the DA classificationtask with the additional problems introduced by automatic segmentation a d assumedthe utterance-level s gmentations as given.
An important consequence of this decisionis that we can expect utterance l ngth and acoustic properties at utterance boundariesto be accurate, both of which turn out to be important features of DAs (Shriberg et al1998; see also Section 5.2.1).2.2 Tag SetWe chose to follow a recent standard for shallow discourse structure annotation, theDialog Act Markup in Several Layers (DAMSL) tag set, which was designed by thenatural language processing community under the auspices of the Discourse ResourceInitiative (Core and Allen 1997).
We began with the DAMSL markup system, but modi-fied it in several ways to make it more relevant to our corpus and task.
DAMSL aims toprovide a domain-independent framework for dialogue annotation, as reflected by thefact that our tag set can be mapped back to DAMSL categories (Jurafsky, Shriberg, andBiasca 1997).
However, our labeling effort also showed that content- and task-relateddistinctions will always play an important role in effective DA labeling.The Switchboard omain itself is essentially "task-free," thus giving few externalconstraints on the definition of DA categories.
Our primary purpose in adapting thetag set was to enable computational DA modeling for conversational speech, withpossible improvements to conversational speech recognition.
Because of the lack of aspecific task, we decided to label categories that seemed inherently interesting linguis-tically and that could be identified reliably.
Also, the focus on conversational speechrecognition led to a certain bias toward categories that were lexically or syntacticallydistinct (recognition accuracy is traditionally measured including all lexical elementsin an utterance).While the modeling techniques described in this paper are formally independent ofthe corpus and the choice of tag set, their success on any particular task will of coursecrucially depend on these factors.
For different asks, not all the techniques used inthis study might prove useful and others could be of greater importance.
However,we believe that this study represents a fairly comprehensive application of technologyin this area and can serve as a point of departure and reference for other work.The resulting SWBD-DAMSL tag set was multidimensional; pproximately 50ba-sic tags (e.g., QUESTION, STATEMENT) could each be combined with diacritics indicat-ing orthogonal information, for example, about whether or not the dialogue functionof the utterance was related to Task-Management and Communication-Management.Approximately 220 of the many possible unique combinations ofthese codes were usedby the coders (Jurafsky, Shriberg, and Biasca 1997).
To obtain a system with somewhathigher interlabeler agreement, as well as enough data per class for statistical mod-eling purposes, a less fine-grained tag set was devised.
This tag set distinguishes 42mutually exclusive utterance types and was used for the experiments reported here.Table 2 shows the 42 categories with examples and relative frequencies.
1 While some1 For the study focusing on prosodic modeling ofDAs reported elsewhere (Shriberg et al 1998), the tagset was further reduced to six categories.343Computational Linguistics Volume 26, Number 3of the original infrequent classes were collapsed, the resulting DA type distributionis still highly skewed.
This occurs largely because there was no basis for subdividingthe dominant DA categories according to task-independent a d reliable criteria.The tag set incorporates both traditional sociolinguistic and discourse-theoreticnotions, such as rhetorical relations and adjacency pairs, as well as some more form-based labels.
Furthermore, the tag set is structured so as to allow labelers to annotatea Switchboard conversation from transcripts alone (i.e., without listening) in about30 minutes.
Without hese constraints he DA labels might have included some finerdistinctions, but we felt that this drawback was balanced by the ability to cover a largeamount of data.
2Labeling was carried out in a three-month period in 1997 by eight linguisticsgraduate students at CU Boulder.
Interlabeler agreement for the 42-1abel tag set usedhere was 84%, resulting in a Kappa statistic of 0.80.
The Kappa statistic measuresagreement ormalized for chance (Siegel and Castellan, Jr. 1988).
As argued in Carletta(1996), Kappa values of 0.8 or higher are desirable for detecting associations betweenseveral coded variables; we were thus satisfied with the level of agreement achieved.
(Note that, even though only a single variable, DA type, was coded for the presentstudy, our goal is, among other things, to model associations between several instancesof that variable, e.g., between adjacent DAs.
)A total of 1,155 Switchboard conversations were labeled, comprising 205,000 ut-terances and 1.4 million words.
The data was partitioned into a training set of 1,115conversations (1.4M words, 198K utterances), used for estimating the various compo-nents of our model, and a test set of 19 conversations (29K words, 4K utterances).Remaining conversations were set aside for future use (e.g., as a test set uncompro-mised of tuning effects).2.3 Major Dialogue Act TypesThe more frequent DA types are briefly characterized below.
As discussed above, thefocus of this paper is not on the nature of DAs, but on the computational frameworkfor their recognition; full details of the DA tag set and numerous motivating examplescan be found in a separate report (Jurafsky, Shriberg, and Biasca 1997).Statements and Opinions.
The most common types of utterances were STATEMENTSand OPINIONS.
This split distinguishes "descriptive, narrative, or personal" statements(STATEMENT) from "other-directed opinion statements" (OPINION).
The distinction wasdesigned to capture the different kinds of responses we saw to opinions (which areoften countered or disagreed with via further opinions) and to statements (which moreoften elicit continuers or backchannels):Dialogue ActSTATEMENTSTATEMENTSTATEMENTOPINIONOPINIONExample UtteranceWell, we have a cat, um,He's probably, oh, a good two years old,big, old, fat and sassy tabby.He's about five months oldWell, rabbits are darling.I think it would be kind of stressful.2 The effect of lacking acoustic information on labeling accuracy was assessed by relabeling a subset ofthe data with listening, and was found to be fairly small (Shriberg et al 1998).
A conservative estimatebased on the relabeling study is that, for most DA types, at most 2% of the labels might have changedbased on listening.
The only DA types with higher uncertainty were BACKCHANNELS andAGREEMENTS, which are easily confused with each other without acoustic ues; here the rate of changewas no more than 10%.344Stolcke t al.
Dialogue Act ModelingOPINIONS often include such hedges as I think, I believe, it seems, and I mean.
Wecombined the STATEMENT and OPINION classes for other studies on dimensions inwhich they did not differ (Shriberg et al 1998).Questions.
Questions were of several types.
The YES-No-QUESTION label includes onlyutterances having both the pragmatic force of a yes-no-question and the syntactic mark-ings of a yes-no-question (i.e., subject-inversion r sentence-final t gs).
DECLARATIVE-QUESTIONS are utterances that function pragmatically as questions but do not have"question form."
By this we mean that declarative questions normally have no wh-word as the argument of the verb (except in "echo-question" format), and have "declar-ative" word order in which the subject precedes the verb.
See Weber (1993) for a surveyof declarative questions and their various realizations.Dialogue Act Example UtteranceYEs-No-QUESTIONYEs-No-QUESTIONYEs-No-QuESTIONDECLARATIVE- QUESTIONWH-QUESTIONDo you have to have any special training?But that doesn't eliminate it, does it?Uh, I guess a year ago you're probablywatching C N N a lot, right?So you're taking a government course?Well, how old are you?Backchannels.
A backchannel is a short utterance that plays discourse-structuring roles,e.g., indicating that the speaker should go on talking.
These are usually referred to inthe conversation analysis literature as "continuers" and have been studied extensively(Jefferson 1984; Schegloff 1982; Yngve 1970).
We expect recognition of backchannels tobe useful because of their discourse-structuring role (knowing that the hearer expectsthe speaker to go on talking tells us something about the course of the narrative)and because they seem to occur at certain kinds of syntactic boundaries; detecting abackchannel may thus help in predicting utterance boundaries and surrounding lexicalmaterial.For an intuition about what backchannels look like, Table 3 shows the most com-mon realizations of the approximately 300 types (35,827 tokens) of backchannel inour Switchboard subset.
The following table shows examples of backchannels in thecontext of a Switchboard conversation:Speaker Dialogue Act UtteranceB STATEMENTA BACKCHANNELB STATEMENTB STATEMENTA BACKCHANNELB STATEMENTB STATEMENTA APPRECIATIONbut, uh, we're to the point now where ourfinancial income is enough that we can considerputting some away -Uh-huh.
/- for college, /so we are going to be starting a regular payrolldeduction -Urn.
/- -  in the fall /and then the money that I will be making thissummer we'll be putting away for the collegefund.Urn.
Sounds good.Turn Exits and Abandoned Utterances.
Abandoned utterances are those that the speakerbreaks off without finishing, and are followed by a restart.
Turn exits resemble aban-doned utterances in that they are often syntactically broken off, but they are used345Computational Linguistics Volume 26, Number 3Table 3Most common realizations of backchannels in Switchboard.Frequency Form Frequency Form Frequency Form38% uh-huh 2% yes 1% sure34% yeah 2% okay 1.% um9% right 2% oh yeah 1% huh-uh3% oh 1% huh 1% uhmainly as a way of passing speakership to the other speaker.
Turn exits tend to besingle words, often so or or.Speaker Dialogue Act UtteranceA STATEMENT we're from, uh, I 'm from Ohio /A STATEMENT and my wife's from Florida /A TURN-ExIT SO, -/B BACKCHANNEL Uh-huh./A HEDGEA ABANDONEDA STATEMENTso, I don't know, /it's Klipsmack>, - /I 'm glad it's not the kind of problem I have tocome up with an answer to because it's not -Answers and Agreements.
YES-ANSWERS include yes, yeah, yep, uh-huh, and other varia-tions on yes, when they are acting as an answer to a YES-NO-QUESTION or DECLARA-TWE-0UESTION.
Similarly, we also coded NO-ANSWERS.
Detecting ANSWERS can helptell us that the previous utterance was a YES-NO-QUESTION.
Answers are also seman-tically significant since they are likely to contain new information.AGREEMENT/ACCEPT, REJECT, and MAYBE/ACCEPT-PART all mark the degreeto which a speaker accepts ome previous proposal, plan, opinion, or statement.
Themost common of these are the AGREEMENT/AccEPTS.
These are very often yes or yeah,so they look a lot like ANSWERS.
But where ANSWERS follow questions, AGREEMENTSoften follow opinions or proposals, so distinguishing these can be important for thediscourse.3.
Hidden Markov Modeling of DialogueWe will now describe the mathematical  nd computational  f ramework used in ourstudy.
Our goal is to perform DA classification and other tasks using a probabilis-tic formulation, giving us a principled approach for combining multiple knowledgesources (using the laws of probability), as well as the ability to derive model  parame-ters automatical ly from a corpus, using statistical inference techniques.Given all available evidence E about a conversation, the goal is to find the DAsequence U that has the highest posterior probabil ity P(UIE ) given that evidence.Apply ing Bayes' rule we getU* = argmaxP(UIE )UP(U)P(ElU) = argmax u P(E)= argmaxP(U)P(ElU) (1)UHere P(U) represents the prior probabil ity of a DA sequence, and P(EIU ) is the like-346Stolcke t al.
Dialogue Act ModelingTable 4Summary of random variables used in dialogue modeling.
(Speaker labels are introduced in Section 4.
)Symbol MeaningUEFAWTsequence of DA labelsevidence (complete speech signal)prosodic evidenceacoustic evidence (spectral features used in ASR)sequence of wordsspeakers labelslihood of U given the evidence.
The likelihood is usually much more straightforwardto model than the posterior itself.
This has to do with the fact that our models aregenerative or causal in nature, i.e., they describe how the evidence is produced by theunderlying DA sequence U.Estimating P (U) requires building a probabilistic discourse grammar, i.e., a statisti-cal model of DA sequences.
This can be done using familiar techniques from languagemodeling for speech recognition, although the sequenced objects in this case are DAlabels rather than words; discourse grammars will be discussed in detail in Section 4.3.1 Dialogue Act LikelihoodsThe computation of likelihoods P(EIU ) depends on the types of evidence used.
In ourexperiments we used the following sources of evidence, ither alone or in combination:Transcribed words: The likelihoods used in Equation 1 are P(WIU ), where Wrefers to the true (hand-transcribed) words spoken in a conversation.Recognized words: The evidence consists of recognizer acoustics A, and we seekto compute P(A I U).
As described later, this involves considering multiplealternative recognized word sequences.Prosodic features- Evidence is given by the acoustic features F capturing variousaspects of pitch, duration, energy, etc., of the speech signal; the associatedlikelihoods are P(F I U).For ease of reference, all random variables used here are summarized in Table 4.The same variables are used with subscripts to refer to individual utterances.
Forexample, Wi is the word transcription of the ith utterance within a conversation ( otthe ith word).To make both the modeling and the search for the best DA sequence feasible, wefurther equire that our likelihood models are decomposable byutterance.
This meansthat the likelihood given a complete conversation can be factored into likelihoodsgiven the individual utterances.
We use Ui for the ith DA label in the sequence U,i.e., U = (U1 .
.
.
.
.
Ui,..., Un), where n is the number of utterances in a conversation.In addition, we use Ei for that portion of the evidence that corresponds to the ithutterance, .g., the words or the prosody of the ith utterance.
Decomposability of thelikelihood means thatP(EIU) = P(E11 U1).... .
P(En \[Un) (2)Applied separately to the three types of evidence Ai, Wi, and Fi mentioned above,it is clear that this assumption is not strictly true.
For example, speakers tend to reuse347Computational Linguistics Volume 26, Number 3E1 Ei E.T T T<start> , U1 , .
.
.
~ Ui ) .
.
.
- - - *  UnFigure 1The discourse HMM as Bayes network.<end>words found earlier in the conversation (Fowler and Housum 1987) and an answermight actually be relevant o the question before it, violating the independence of theP(WilUi).
Similarly, speakers adjust their pitch or volume over time, e.g., to the con-versation partner or because of the structure of the discourse (Menn and Boyce 1982),violating the independence of the P(FilUi).
As in other areas of statistical modeling,we count on the fact that these violations are small compared to the properties actuallymodeled, namely, the dependence of Ei on Ui.3.2 Markov  Mode l ingReturning to the prior distribution of DA sequences P(U), it is convenient to makecertain independence assumptions here, too.
In particular, we assume that the priordistribution of U is Markovian, i.e., that each Ui depends only on a fixed number k ofpreceding DA labels:P(U i lU l ,  .
.
.
,  U i -1 )  ~- P (U i lU i -k  .
.
.
.
.
Ui -1 )  (3)(k is the order of the Markov process describing U).
The n-gram-based discourse gram-mars we used have this property.
As described later, k = 1 is a very good choice, i.e.,conditioning on the DA types more than one removed from the current one does notimprove the quality of the model by much, at least with the amount of data availablein our experiments.The importance of the Markov assumption for the discourse grammar is thatwe can now view the whole system of discourse grammar and local utterance-basedlikelihoods as a kth-order hidden Markov model (HMM) (Rabiner and Juang 1986).The HMM states correspond to DAs, observations correspond to utterances, transitionprobabilities are given by the discourse grammar (see Section 4), and observationprobabilities are given by the local likelihoods P(Eil Ui).We can represent the dependency structure (as well as the implied conditionalindependences) as a special case of Bayesian belief network (Pearl 1988).
Figure 1shows the variables in the resulting HMM with directed edges representing conditionaldependence.
To keep things simple, a first-order HMM (bigram discourse grammar)is assumed.3.3 D ia logue  Act  Decod ingThe HMM representation allows us to use efficient dynamic programming algorithmsto compute relevant aspects of the model, such as?
the most probable DA sequence (the Viterbi algorithm)?
the posterior probability of various DAs for a given utterance, afterconsidering all the evidence (the forward-backward algorithm)The Viterbi algorithm for HMMs (Viterbi 1967) finds the globally most probablestate sequence.
When applied to a discourse model with locally decomposable ike-lihoods and Markovian discourse grammar, it will therefore find precisely the DA348Stolcke et al Dialogue Act Modelingsequence with the highest posterior probability:U* = argmaxP(UIE ) (4)uThe combination of likelihood and prior modeling, HMMs, and Viterbi decoding isfundamentally the same as the standard probabilistic approaches to speech recognition(Bahl, Jelinek, and Mercer 1983) and tagging (Church 1988).
It maximizes the prob-ability of getting the entire DA sequence correct, but it does not necessarily find theDA sequence that has the most DA labels correct (Dermatas and Kokkinakis 1995).
Tominimize the total number of utterance labeling errors, we need to maximize the prob-ability of getting each DA label correct individually, i.e., we need to maximize P(UilE)for each i = 1 .
.
.
.
.
n. We can compute the per-utterance posterior DA probabilities bysumming:P(u\[E) = E P(UIE) (5)U: Ui=uwhere the summation is over all sequences U whose ith element matches the label inquestion.
The summation is efficiently carried out by the forward-backward algorithmfor HMMs (Baum et al 1970).
3For zeroth-order (unigram) discourse grammars, Viterbi decoding and forward-backward decoding necessarily ield the same results.
However, for higher-orderdiscourse grammars we found that forward-backward decoding consistently givesslightly (up to 1% absolute) better accuracies, as expected.
Therefore, we used thismethod throughout.The formulation presented here, as well as all our experiments, uses the entireconversation as evidence for DA classification.
Obviously, this is possible only duringoff-line processing, when the full conversation is available.
Our paradigm thus followshistorical practice in the Switchboard omain, where the goal is typically the off-lineprocessing (e.g., automatic transcription, speaker identification, indexing, archival) ofentire previously recorded conversations.
However, the HMM formulation used herealso supports computing posterior DA probabilities based on partial evidence, e.g.,using only the utterances preceding the current one, as would be required for on-lineprocessing.4.
Discourse GrammarsThe statistical discourse grammar models the prior probabilities P(U) of DA sequences.In the case of conversations for which the identities of the speakers are known (asin Switchboard), the discourse grammar should also model turn-taking behavior.
Astraightforward approach is to model sequences of pairs (Ui, Ti) where Ui is the DAlabel and Ti represents he speaker.
We are not trying to model speaker idiosyncrasies,so conversants are arbitrarily identified as A or B, and the model is made symmetricwith respect o the choice of sides (e.g., by replicating the training sequences withsides switched).
Our discourse grammars thus had a vocabulary of 42 x 2 = 84 labels,plus tags for the beginning and end of conversations.
For example, the second DA tagin Table 1 would be predicted by a trigram discourse grammar using the fact that thesame speaker previously uttered a YES-NO-QUESTION, which in turn was preceded bythe start-of-conversation.3 We note in passing that he Viterbi and Baum algorithms have quivalent formulations in the Bayesnetwork framework (Pearl 1988).
The HMM terminology was chosen here mainly for historical reasons.349Computational Linguistics Volume 26, Number 3Table 5Perplexities of DAs with and without urninformation.Discourse Grammar P(U) P(U, T) P(UIT )None 42 84 42Unigram 11.0 18.5 9.0Bigram 7.9 10.4 5.1Trigram 7.5 9.8 4.84.1 N-gram Discourse Mode lsA computationally convenient type of discourse grammar is an n-gram model based onDA tags, as it allows efficient decoding in the HMM framework.
We trained standardbackoff n-gram models (Katz 1987), using the frequency smoothing approach of Wittenand Bell (1991).
Models of various orders were compared by their perplexities, i.e.,the average number of choices the model predicts for each tag, conditioned on thepreceding tags.Table 5 shows perplexities for three types of models: P(U), the DAs alone; P(U, T),the combined DA/speaker ID sequence; and P(UIT ), the DAs conditioned on knownspeaker IDs (appropriate for the Switchboard task).
As expected, we see an improve-ment (decreasing perplexities) for increasing n-gram order.
However, the incrementalgain of a trigram is small, and higher-order models did not prove useful.
(This ob-servation, initially based on perplexity, is confirmed by the DA tagging experimentsreported in Section 5.)
Comparing P(U) and P(U\[T), we see that speaker identity addssubstantial information, especially for higher-order models.The relatively small improvements from higher-order models could be a result oflack of training data, or of an inherent independence of DAs from DAs further re-moved.
The near-optimality of the bigram discourse grammar is plausible given con-versation analysis accounts of discourse structure in terms of adjacency pairs (Schegloff1968; Sacks, Schegloff, and Jefferson 1974).
Inspection of bigram probabilities estimatedfrom our data revealed that conventional djacency pairs receive high probabilities, asexpected.
For example, 30% of YES-NO-QUESTIONS are followed by YES-ANSWERS,14% by NO-ANSWERS (confirming that the latter are dispreferred).
COMMANDS are fol-lowed by AGREEMENTS in 23% of the cases, and STATEMENTS elicit BACKCHANNELSin 26% of all cases.4.2 Other Discourse Mode lsWe also investigated non-n-gram discourse models, based on various language model-ing techniques known from speech recognition.
One motivation for alternative modelsis that n-grams enforce a one-dimensional representation  DA sequences, whereaswe saw above that the event space is really multidimensional (DA label and speakerlabels).
Another motivation is that n-grams fail to model long-distance dependencies,such as the fact that speakers may tend to repeat certain DAs or patterns throughoutthe conversation.The first alternative approach was a standard cache model (Kuhn and de Mori1990), which boosts the probabilities of previously observed unigrams and bigrams, onthe theory that tokens tend to repeat hemselves over longer distances.
However, thisdoes not seem to be true for DA sequences in our corpus, as the cache model showedno improvement over the standard N-gram.
This result is somewhat surprising sinceunigram dialogue grammars are able to detect speaker gender with 63% accuracy (over350Stolcke t al.
Dialogue Act Modelinga 50% baseline) on Switchboard (Ries 1999b), indicating that there are global variablesin the DA distribution that could potentially be exploited by a cache dialogue grammar.Clearly, dialogue grammar adaptation needs further esearch.Second, we built a discourse grammar that incorporated constraints on DA se-quences in a nonhierarchical way, using maximum entropy (ME) estimation (Berger,Della Pietra, and Della Pietra 1996).
The choice of features was informed by similarones commonly used in statistical language models, as well our general intuitionsabout potentially information-bearing elements in the discourse context.
Thus, themodel was designed so that the current DA label was constrained by features uch asunigram statistics, the previous DA and the DA once removed, DAs occurring within awindow in the past, and whether the previous utterance was by the same speaker.
Wefound, however, that an ME model using n-gram constraints performed only slightlybetter than a corresponding backoff n-gram.Additional constraints such as DA triggers, distance-1 bigrams, separate ncodingof speaker change and bigrams to the last DA on the same/other channel did notimprove relative to the trigram model.
The ME model thus confirms the adequacy ofthe backoff n-gram approach, and leads us to conclude that DA sequences, at leastin the Switchboard omain, are mostly characterized by local interactions, and thusmodeled well by low-order n-gram statistics for this task.
For more structured tasks thissituation might be different.
However, we have found no further exploitable structure.5.
Dialogue Act ClassificationWe now describe in more detail how the knowledge sources of words and prosodyare modeled, and what automatic DA labeling results were obtained using each of theknowledge sources in turn.
Finally, we present results for a combination of all knowl-edge sources.
DA labeling accuracy results hould be compared to a baseline (chance)accuracy of 35%, the relative frequency of the most frequent DA type (STATEMENT) inour test set.
45.1 Dialogue Act Classification Using WordsDA classification using words is based on the observation that different DAs usedistinctive word strings.
It is known that certain cue words and phrases (Hirschbergand Litman 1993) can serve as explicit indicators of discourse structure.
Similarly,we find distinctive correlations between certain phrases and DA types.
For example,92.4% of the uh-huh's occur in BACKCHANNELS, and 88.4% of the trigrams "<start>do you" occur in YES-NO-QUESTIONS.
To leverage this information source, withouthand-coding knowledge about which words are indicative of which DAs, we will usestatistical language models that model the full word sequences associated with eachDA type.5.1.1 Classification from True Words.
Assuming that the true (hand-transcribed) wordsof utterances are given as evidence, we can compute word-based likelihoods P(WIU )in a straightforward way, by building a statistical language model for each of the 42DAs.
All DAs of a particular type found in the training corpus were pooled, anda DA-specific trigram model was estimated using standard techniques (Katz backoff\[Katz 1987\] with Witten-Bell discounting \[Witten and Bell 1991\]).4 The frequency of STATEMENTS across all labeled ata was slightly different, cf.
Table 2.351Computational Linguistics Volume 26, Number 3A1 Ai AnT Twl wi w.T T T<start> ~ U1 ~ .
.
.
.
~ Ui ~ .
.
.
.
Un ~ <end>Figure 2Modified Bayes network including word hypotheses and recognizer acoustics.5.1.2 Classification from Recognized Words.
For fully automatic DA classification,the above approach is only a partial solution, since we are not yet able to recognizewords in spontaneous speech with perfect accuracy.
A standard approach is to usethe 1-best hypothesis from the speech recognizer in place of the true word transcripts.While conceptually simple and convenient, his method will not make optimal use ofall the information in the recognizer, which in fact maintains multiple hypotheses aswell as their relative plausibilities.A more thorough use of recognized speech can be derived as follows.
The classifi-cation framework is modified such that the recognizer's acoustic information (spectralfeatures) A appear as the evidence.
We compute P(A\[U) by decomposing it into anacoustic likelihood P(A\]W) and a word-based likelihood P(W\[ U), and summing overall word sequences:P(AlU) = ~-~ P(AIW, U)P(WIU)w= ~P(A IW)P(W\ [U  )w(6)The second line is justified under the assumption that the recognizer acoustics (typ-ically, cepstral coefficients) are invariant o DA type once the words are fixed.
Notethat this is another approximation i our modeling.
For example, different DAs withcommon words may be realized by different word pronunciations.
Figure 2 shows theBayes network resulting from modeling recognizer acoustics through word hypothe-ses under this independence assumption; note the added Wi variables (that have tobe summed over) in comparison to Figure 1.The acoustic likelihoods P(A\[W) correspond to the acoustic scores the recognizeroutputs for every hypothesized word sequence W. The summation over all W mustbe approximated; in our experiments we summed over the (up to) 2,500 best hypothe-ses generated by the recognizer for each utterance.
Care must be taken to scale therecognizer acoustic scores properly, i.e., to exponentiate he recognizer acoustic scoresby 1/~, where A is the language model weight of the recognizer, s5 In a standard recognizer the total og score of a hypothesis Wi is computed aslogP(AdWi ) + )~ logP(Wi) - I~\]Wi\],where \[Wi\] is the number of words in the hypothesis, and both A and/~ are parameters optimized tominimize the word error rate.
The word insertion penalty/~ represents a correction to the languagemodel that allows balancing insertion and deletion errors.
The language model weight ,~ compensatesfor acoustic score variances that are effectively too large due to severe independence assumptions inthe recognizer acoustic model.
According to this rationale, it is more appropriate o divide all scorecomponents by ),.
Thus, in all our experiments, we computed a summand in Equation 6whose352Stolcke t al.
Dialogue Act ModelingTable 6DA classification accuracies (in %) from transcribed and recognizedwords (chance = 35%).Discourse Grammar True Recognized Relative Error IncreaseNone 54.3 42.8 25.2%Unigram 68.2 61.8 20.1%Bigram 70.6 64.3 21.4%Trigram 71.0 64.8 21.4%5.1.3 Results.
Table 6 shows DA classification accuracies obtained by combining theword- and recognizer-based likelihoods with the n-gram discourse grammars de-scribed earlier.
The best accuracy obtained from transcribed words, 71%, is encour-aging given a comparable human performance of84% (the interlabeler agreement, seeSection 2.2).
We observe about a 21% relative increase in classification error when us-ing recognizer words; this is remarkably small considering that the speech recognizerused had a word error rate of 41% on the test set.We also compared the n-best DA classification approach to the more straightfor-ward 1-best approach.
In this experiment, only the single best recognizer hypothesisis used, effectively treating it as the true word string.
The 1-best method increasedclassification error by about 7% relative to the n-best algorithm (61.5% accuracy witha bigram discourse grammar).5.2 Dialogue Act Classification Using ProsodyWe also investigated prosodic information, i.e., information i dependent of the wordsas well as the standard recognizer acoustics.
Prosody is important for DA recogni-tion for two reasons.
First, as we saw earlier, word-based classification suffers fromrecognition errors.
Second, some utterances are inherently ambiguous based on wordsalone.
For example, some YES-NO-QUESTiONS have word sequences identical to thoseof STATEMENTS, but can often be distinguished by their final F0 rise.A detailed study aimed at automatic prosodic lassification of DAs in the Switch-board domain is available in a companion paper (Shriberg et al 1998).
Here we investi-gate the interaction of prosodic models with the dialogue grammar and the word-basedDA models discussed above.
We also touch briefly on alternative machine learningmodels for prosodic features.5.2.1 Prosodic Features.
Prosodic DA classification was based on a large set of fea-tures computed automatically from the waveform, without reference to word or phoneinformation.
The features can be broadly grouped as referring to duration (e.g., utter-ance duration, with and without pauses), pauses (e.g., total and mean of nonspeechregions exceeding 100 ms), pitch (e.g., mean and range of F0 over utterance, slope ofF0 regression line), energy (e.g., mean and range of RMS energy, same for signal-to-logarithm was1 logP(Ai\]Wi) + logP(WilUi) - ~lWil.
-dWe found this approach to give better esults than the standard multiplication of logP(W) by ,L Notethat for selecting the best hypothesis in a recognizer only the relative magnitudes of the score weightsmatter; however, for the summation in Equation 6 the absolute values become important.
Theparameter values for )~ and # were those used by the standard recognizer; they were not specificallyoptimized for the DA classification task.353Computational Linguistics Volume 26, Number 3~ 23.403~ an utt < 0 .3" /~U >= 0.3"/17Figure 3Decision tree for the classification of BACKCHANNELS (B) and AGREEMENTS (A).
Each node islabeled with the majority class for that node, as well as the posterior probabilities of the twoclasses.
The following features are queried in the tree: number of frames in continuous (> 1 s)speech regions (cont_speech_frames), total utterance duration (ling_dir), utterance durationexcluding pauses > 100 ms (ling_dur_minus_minlOpause), andmean signal-to-noise ratio(snr_mean_utt ).noise ratio \[SNR\]), speaking rate (based on the "enrate" measure of Morgan, Fosler,and Mirghafori \[1997\]), and gender (of both speaker and listener).
In the case of ut-terance duration, the measure correlates both with length in words and with overallspeaking rate.
The gender feature that classified speakers as either male or female wasused to test for potential inadequacies in F0 normalizations.
Where appropriate, weincluded both raw features and values normalized by utterance and/or  conversation.We also included features that are the output of the pitch accent and boundary toneevent detector of Taylor (2000) (e.g., the number of pitch accents in the utterance).
Acomplete description of prosodic features and an analysis of their usage in our modelscan be found in Shriberg et al (1998).5.2.2 Prosodic Decision Trees.
For our Prosodic classifiers, we used CART-style deci-sion trees (Breiman et al 1984).
Decision trees allow the combination of discrete andcontinuous features, and can be inspected to help in understanding the role of differentfeatures and feature combinations.To illustrate one area in which prosody could aid our classification task, we appliedtrees to DA classifications known to be ambiguous from words alone.
One frequentexample in our corpus was the distinction between BACKCHANNELS and AGREEMENTS(see Table 2), which share terms such as right and yeah.
As shown in Figure 3, a prosodictree trained on this task revealed that agreements have consistently longer durationsand greater energy (as reflected by the SNR measure) than do backchannels.354Stolcke t al.
Dialogue Act ModelingTable 7DA classification using prosodicdecision trees (chance = 35%).Discourse Grammar Accuracy (%)None 38.9Unigram 48.3Bigram 49.7The HMM framework requires that we compute prosodic likelihoods of the formP(FilUi) for each utterance Ui and associated prosodic feature values Fi.
We havethe apparent difficulty that decision trees (as well as other classifiers, such as neuralnetworks) give estimates for the posterior probabilities, P(Ui\[Fi).
The problem can beovercome by applying Bayes' rule locally:P(Ui) t rue)(7)Note that P(Fi) does not depend on Ui and can be treated as a constant for the purposeof DA classification.
A quantity proportional to the required likelihood can thereforebe obtained either by dividing the posterior tree probability by the prior P(Ui), 6 or bytraining the tree on a uniform prior distribution of DA types.
We chose the secondapproach, downsampling our training data to equate DA proportions.
This also coun-teracts a common problem with tree classifiers trained on very skewed distributionsof target classes, i.e., that low-frequency classes are not modeled in sufficient detailbecause the majority class dominates the tree-growing objective hznction.5.2.3 Results with Dec is ion Trees.
As a preliminary experiment to test the integra-tion of prosody with other knowledge sources, we trained a single tree to discriminateamong the five most frequent DA types (STATEMENT, BACKCHANNEL, OPINION, ABAN-DONED, and AGREEMENT, totaling 79% of the data) and an Other category comprisingall remaining DA types.
The decision tree was trained on a downsampled trainingsubset containing equal proportions of these six DA classes.
The tree achieved a clas-sification accuracy of 45.4% on an independent test set with the same uniform six-classdistribution.
The chance accuracy on this set is 16.6%, so the tree clearly extracts usefulinformation from the prosodic features.We then used the decision tree posteriors as scaled DA likelihoods in the dialoguemodel HMM, combining it with various n-gram dialogue grammars for testing on ourfull standard test set.
For the purpose of model integration, the likelihoods of the Otherclass were assigned to all DA types comprised by that class.
As shown in Table 7, thetree with dialogue grammar performs ignificantly better than chance on the raw DAdistribution, although not as well as the word-based methods (cf.
Table 6).5.2.4 Neural  Network  Classifiers.
Although we chose to use decision trees as prosodicclassifiers for their relative ase of inspection, we might have used any suitable proba-bilistic classifier, i.e., any model that estimates the posterior probabilities of DAs giventhe prosodic features.
We conducted preliminary experiments o assess how neural6 Bourlard and Morgan (1993) use this approach tointegrate neural network phonetic models in aspeech recognizer.355Computational Linguistics Volume 26, Number 3Table 8Performance ofvarious prosodic neural network classifiers onan equal-priors, ix-class DA set (chance = 16.6%).Network Architecture Accuracy (%)Decision tree 45.4No hidden layer, linear output function 44.6No hidden layer, softmax output function 46.040-unit hidden layer, softmax output function 46.0networks compare to decision trees for the type of data studied here.
Neural networksare worth investigating since they offer potential advantages over decision trees.
Theycan learn decision surfaces that lie at an angle to the axes of the input feature space,unlike standard CART trees, which always split continuous features on one dimen-sion at a time.
The response function of neural networks is continuous (smooth) atthe decision boundaries, allowing them to avoid hard decisions and the completefragmentation f data associated with decision tree questions.Most important, however, related work (Ries 1999a) indicated that similarly struc-tured networks are superior classifiers if the input features are words and are thereforea plug-in replacement for the language model classifiers described in this paper.
Neuralnetworks are therefore a good candidate for a jointly optimized classifier of prosodicand word-level information since one can show that they are a generalization of theintegration approach used here.We tested various neural network models on the same six-class downsampleddata used for decision tree training, using a variety of network architectures and out-put layer functions.
The results are summarized in Table 8, along with the baselineresult obtained with the decision tree model.
Based on these experiments, a softmaxnetwork (Bridle 1990) without hidden units resulted in only a slight improvementover the decision tree.
A network with hidden units did not afford any additionaladvantage, ven after we optimized the number of hidden units, indicating that com-plex combinations of features (as far as the network could learn them) do not predictDAs better than linear combinations of input features.
While we believe alternativeclassifier architectures should be investigated further as prosodic models, the resultsso far seem to confirm our choice of decision trees as a model class that gives close tooptimal performance for this task.5.2.5 Intonation Event Likel ihoods.
An alternative way to compute prosodically basedDA likelihoods uses pitch accents and boundary phrases (Taylor et al 1997).
The ap-proach relies on the intuition that different utterance types are characterized by dif-ferent intonational "tunes" (Kowtko 1996), and has been successfully applied to theclassification ofmove types in the DCIEM Map Task corpus (Wright and Taylor 1997).The system detects equences ofdistinctive pitch patterns by training one continuous-density HMM for each DA type.
Unfortunately, the event classification accuracy onthe Switchboard corpus was considerably poorer than in the Map Task domain, andDA recognition results when coupled with a discourse grammar were substantiallyworse than with decision trees.
The approach could prove valuable in the future,however, if the intonation event detector can be made more robust to corpora likeOURS.356Stolcke et al Dialogue Act ModelingA1 Ai AnT 1 1Wl Wi W,,T T t<start> - , 0"1 , .
.
.---* U/ , ... ~ Un , <end>1 1 ,tF1 Fi GFigure 4Bayes network for discourse HMM incorporating both word recognition and prosodic features.5.3 Using Multiple Knowledge SourcesAs mentioned earlier, we expect improved performance from combining word andprosodic information.
Combining these knowledge sources requires estimating a com-bined likelihood P(Ai, Fi\[Ui) for each utterance.
The simplest approach is to assumethat the two types of acoustic observations (recognizer acoustics and prosodic features)are approximately conditionally independent once Ui is given:P(ai, w,,Fdui) = P(A~, Wdui)e(Fifai, W~, Ui)~, P(ai, Wi\[Ui)P(FilUi) (8)Since the recognizer acoustics are modeled by way of their dependence on words, itis particularly important o avoid using prosodic features that are directly correlatedwith word identities, or features that are also modeled by the discourse grammars,such as utterance position relative to turn changes.
Figure 4 depicts the Bayes networkincorporating evidence from both word recognition and prosodic features.One important respect in which the independence assumption is violated is in themodeling of utterance length.
While utterance length itself is not a prosodic feature,it is an important feature to condition on when examining prosodic characteristicsof utterances, and is thus best included in the decision tree.
Utterance length is cap-tured directly by the tree using various duration measures, while the DA-specificLMs encode the average number of words per utterance indirectly through n-gramparameters, but still accurately enough to violate independence in a significant way(Finke et al 1998).
As discussed in Section 8, this problem is best addressed by jointlexical-prosodic models.We need to allow for the fact that the models combined in Equation 8 give es-timates of differing qualities.
Therefore, we introduce an exponential weight a onP(Fi\[Ui) that controls the contribution of the prosodic likelihood to the overall likeli-hood.
Finally, a second exponential weight fl on the combined likelihood controls itsdynamic range relative to the discourse grammar scores, partially compensating forany correlation between the two likelihoods.
The revised combined likelihood estimatethus becomes:P(Ai, Wi, FilUi) ~, {P(Ai, WilUi)P(Fi\[Ui)~}  (9)In our experiments, the parameters a and fl were optimized using twofold jackknifing.The test data was split roughly in half (without speaker overlap), each half was usedto separately optimize the parameters, and the best values were then tested on therespective other half.
The reported results are from the aggregate outcome on the twotest set halves.357Computational Linguistics Volume 26, Number 3Table 9Combined utterance classification accuracies (chance =35%).
The first two columns correspond to Tables 7and 6, respectively.Discourse Grammar Accuracy (%)Prosody Recognizer CombinedNone 38.9 42.8 56.5Unigram 48.3 61.8 62.4Bigram 49.7 64.3 65.0Table 10Accuracy (in %) for individualsubtasks, using uniform priorsand combined models for two(chance = 50%).Classification Task True Words Recognized WordsKnowledge SourceQUESTIONS/STATEMENTSprosody only 76.0 76.0words only 85.9 75.4words+prosody 87.6 79.8AGREEMENTS / BACKCHANNELSprosody only 72.9 72.9words only 81.0 78.2words+prosody 84.7 81.75.3.1 Results.
In this experiment we combined the acoustic n-best likelihoods basedon recognized words with the Top-5 tree classifier mentioned in Section 5.2.3.
Resultsare summarized in Table 9.As shown, the combined classifier presents a slight improvement over the rec-ognizer-based classifier, The experiment without discourse grammar indicates thatthe combined evidence is considerably stronger than either knowledge source alone,yet this improvement seems to be made largely redundant by the use of priors andthe discourse grammar.
For example, by definition DECLARATIVE-QUESTIONS are notmarked by syntax (e.g., by subject-auxiliary inversion) and are thus confusable withSTATEMENTS and OPINIONS.
While prosody is expected to help disambiguate hesecases, the ambiguity can also be removed by examining the context of the utterance,e.g., by noticing that the following utterance is a YEs-ANswER or NO-ANSWER.5.3.2 Focused Classifications.
To gain a better understanding of the potential forprosodic DA classification i dependent of the effects of discourse grammar and theskewed DA distribution i  Switchboard, we examined several binary DA classificationtasks.
The choice of tasks was motivated by an analysis of confusions committed by apurely word-based DA detector, which tends to mistake QUESTIONS for STATEMENTS,and BACKCHANNELS for AGREEMENTS (and vice versa).
We tested aprosodic lassifier,a word-based classifier (with both transcribed and recognized words), and a combinedclassifier on these two tasks, downsampling the DA distribution to equate the classsizes in each case.
Chance performance in all experiments i  therefore 50%.
Resultsare summarized in Table 10.358Stolcke et al Dialogue Act ModelingAs shown, the combined classifier was consistently more accurate than the classi-fier using words alone.
Although the gain in accuracy was not statistically significantfor the small recognizer test set because of a lack of power, replication for a largerhand-transcribed test set showed the gain to be highly significant for both subtasksby a Sign test, p < .001 and p < .0001 (one-tailed), respectively.
Across these, as wellas additional subtasks, the relative advantage of adding prosody was larger for recog-nized than for true words, suggesting that prosody is particularly helpful when wordinformation is not perfect.6.
Speech RecognitionWe now consider ways to use DA modeling to enhance automatic speech recognition(ASR).
The intuition behind this approach is that discourse context constrains thechoice of DAs for a given utterance, and the DA type in turn constrains the choice ofwords.
The latter can then be leveraged for more accurate speech recognition.6.1 Integrating DA Modeling and ASRConstraints on the word sequences hypothesized by a recognizer are expressed prob-abilistically in the recognizer language model (LM).
It provides the prior distributionP(Wi) for finding the a posteriori most probable hypothesized words for an utterance,given the acoustic evidence Ai (Bahl, Jelinek, and Mercer 1983): 7W 7 = argmaxP(WilAi)wiP(Wi)P(AilWi) = argmaxwi P(Ai)= argmaxP(Wi)P(AilWi) (10)wiThe likelihoods P(AilWi) are estimated by the recognizer's acoustic model.
In a stan-dard recognizer the language model P(Wi) is the same for all utterances; the idea hereis to obtain better-quality LMs by conditioning on the DA type Ui, since presumablythe word distributions differ depending on DA type.W7 -- argmaxP(WilAi, Ui)wiP( WilUi)P(AilWi, Ui) = argmaxWi P(AiIUi)argmaxP(WilUi)P(AirWi) (11)wiAs before in the DA classification model, we tacitly assume that the words Wi dependonly on the DA of the current utterance, and also that the acoustics are independent ofthe DA type if the words are fixed.
The DA-conditioned language models P(Wil Ui) arereadily trained from DA-specific training data, much as we did for DA classificationfrom words.
87 Note the similarity of Equations 10 and 1.
They are identical except for the fact that we are nowoperating at the level of an individual utterance, the evidence isgiven by the acoustics, and the targetsare word hypotheses instead of DA hypotheses.8 In Equation 11 and elsewhere in this section we gloss over the issue of proper weighting of modelprobabilities, which is extremely important in practice.
The approach explained in detail in footnote 5applies here as well.359Computational Linguistics Volume 26, Number 3The problem with applying Equation 11, of course, is that the DA type Ui isgenerally not known (except maybe in applications where the user interface can beengineered to allow only one kind of DA for a given utterance).
Therefore, we needto infer the likely DA types for each utterance, using available evidence E from theentire conversation.
This leads to the following formulation:W~ = argmaxP(WilAi, E)wi---- argmax ~-~ P(WilAi, Ui, E)P(UilE)Wi Uiargmax ~\[\] P( WiiAi, Ui)P( Ui\[E)W~ U~(12)The last step in Equation 12 is justified because, as shown in Figures 1 and 4, theevidence E (acoustics, prosody, words) pertaining to utterances other than i can affectthe current utterance only through its DA type Ui.We call this the mixture-of-posteriors approach, because it amounts to a mixture ofthe posterior distributions obtained from DA-specific speech recognizers (Equation 11),using the DA posteriors as weights.
This approach is quite expensive, however, as itrequires multiple full recognizer or rescoring passes of the input, one for each DAtype.A more efficient, though mathematically ess accurate, solution can be obtainedby combining uesses about the correct DA types directly at the level of the LM.
Weestimate the distribution of likely DA types for a given utterance using the entireconversation E as evidence, and then use a sentence-level mixture (Iyer, Ostendorf,and Rohlicek 1994) of DA-specific LMs in a single recognizer run.
In other words, wereplace P(WilUi) in Equation 11 with~_~ P(WilUi)P(Ui\]E),uia weighted mixture of all DA-specific LMs.
We call this the mixture-of-LMs ap-proach.
In practice, we would first estimate DA posteriors for each utterance, us-ing the forward-backward algorithm and the models described in Section 5, and thenrerecognize the conversation or rescore the recognizer output, using the new posterior-weighted mixture LM.
Fortunately, as shown in the next section, the mixture-of-LMsapproach seems to give results that are almost identical to (and as good as) the mixture-of-posteriors approach.6.2 Computational Structure of Mixture ModelingIt is instructive to compare the expanded scoring formulas for the two DA mixturemodeling approaches for ASK The mixture-of-posteriors approach yieldsP(WilAi, E) = ~ P(ailui)ui(13)whereas the mixture-of-LMs approach gives) P(A,Iw,)P(WilAi'E) ~ P(WiIUi)P(UilE) P(Ai) (14)360Stolcke t al.
Dialogue Act ModelingTable 11Switchboard word recognition error rates andLM perplexities.Model WER (%) PerplexityBaseline 41.2 76.81-best LM 41.0 69.3Mixture-of-posteriors 41.0 n/aMixture-of-LMs 40.9 66.9Oracle LM 40.3 66.8We see that the second equation reduces to the first under the crude approximationP(Ai\] Ui) ~ P(Ai).
In practice, the denominators are computed by summing the numer-ators over a finite number of word hypotheses Wi, so this difference translates intonormalizing either after or before summing over DAs.
When the normalization takesplace as the final step it can be omitted for score maximization purposes; this showswhy the mixture-of-LMs approach is less computationally expensive.6.3 Experiments and ResultsWe tested both the mixture-of-posteriors and the mixture-of-LMs approaches on ourSwitchboard test set of 19 conversations.
Instead of decoding the data from scratchusing the modified models, we manipulated n-best lists consisting of up to 2,500 besthypotheses for each utterance.
This approach is also convenient since both approachesrequire access to the full word string for hypothesis scoring; the overall model is nolonger Markovian, and is therefore inconvenient to use in the first decoding stage, oreven in lattice rescoring.The baseline for our experiments was obtained with a standard backoff trigramlanguage model estimated from all available training data.
The DA-specific languagemodels were trained on word transcripts of all the training utterances of a given type,and then smoothed further by interpolating them with the baseline LM.
Each DA-specific LM used its own interpolation weight, obtained by minimizing the perplexityof the interpolated model on held-out DA-specific training data.
Note that this smooth-ing step is helpful when using the DA-specific LMs for word recognition, but not forDA classification, since it renders the DA-specific LMs less discriminative.
9Table 11 summarizes both the word error rates achieved with the various modelsand the perplexities of the corresponding LMs used in the rescoring (note that per-plexity is not meaningful in the mixture-of-posteriors approach).
For comparison, wealso included two additional models: the 'q-best LM" refers to always using the DA-specific LM corresponding to the most probable DA type for each utterance.
It is thusan approximation to both mixture approaches where only the top DA is considered.Second, we included an "oracle LM," i.e., always using the LM that corresponds tothe hand-labeled DA for each utterance.
The purpose of this experiment was to give usan upper bound on the effectiveness of the mixture approaches, by assuming perfectDA recognition.It was somewhat disappointing that the word error rate (WER) improvement inthe oracle experiment was small (2.2% relative), even though statistically highly sig-nificant (p < .0001, one-tailed, according to a Sign test on matched utterance pairs).9 Indeed, during our DA classification experiments, wehad observed that smoothed DA-specific LMsyield lower classification accuracy.361Computational Linguistics Volume 26, Number 3Table 12Word error reductions through DA oracle, by DA type.Dialogue Act Baseline WER Oracle WER WER ReductionNO-ANSWER 29.4 11.8 -17.6BACKCHANNEL 25.9 18.6 -7.3BACKCHANNEL-QUESTION 15.2 9.1 -6.1ABANDONED/UNINTERPRETABLE 48.9 45.2 -3.7WH-QUESTION 38.4 34.9 -3.5YES-No-QUESTION 55.5 52.3 --3.2STATEMENT 42.0 41.5 --0.5OPINION 40.8 40.4 --0.4Other 8%onded/Uninterpretable 3%kchannel 3%as-No-Question 3%Statement 53%Dpinion 30%Figure 5Relative contributions to test set word counts by DA type.The WER reduction achieved with the mixture-of-LMs approach did not achieve sta-tistical significance (0.25 > p > 0.20).
The 1-best DA and the two mixture modelsalso did not differ significantly on this test set.
In interpreting these results one mustrealize, however, that WER results depend on a complex combination of factors, mostnotably interaction between language models and the acoustic models.
Since the ex-periments only varied the language models used in rescoring, it is also informative tocompare the quality of these models as reflected by perplexity.
On this measure, wesee a substantial 13% (relative) reduction, which is achieved by both the oracle andthe mixture-of-LMs.
The perplexity reduction for the 1-best LM is only 9.8%, showingthe advantage of the mixture approach.To better understand the lack of a more substantial reduction in word error, we an-alyzed the effect of the DA-conditioned rescoring on the individual DAs, i.e., groupingthe test utterances by their true DA types.
Table 12 shows the WER improvements fora few DA types, ordered by the magnitude of improvement achieved.
As shown, allfrequent DA types saw improvement, but the highest wins were observed for typicallyshort DAs, such as ANSWERS and BACKCHANNELS.
This is to be expected, as such DAstend to be syntactically and lexically highly constrained.
Furthermore, the distributionof number of words across DA types is very uneven (Figure 5).
STATEMENTS andOPINIONS, the DA types dominating in both frequency and number of words (83% oftotal), see no more than 0.5% absolute improvement, thus explaining the small overallimprovement.
In hindsight, this is also not surprising, since the bulk of the trainingdata for the baseline LM consists of these DAs, allowing only little improvement in362Stolcke et al Dialogue Act Modelingthe DA-specific LMs.
A more detailed analysis of the effect of DA modeling on speechrecognition errors can be found elsewhere (Van Ess-Dykema nd Ries 1998).In summary, our experiments confirmed that DA modeling can improve wordrecognition accuracy quite substantially in principle, at least for certain DA types,but that the skewed distribution of DAs (especially in terms of number of words pertype) limits the usefulness of the approach on the Switchboard corpus.
The benefitsof DA modeling might therefore be more pronounced on corpora with more evenDA distribution, as is typically the case for task-oriented ialogues.
Task-orienteddialogues might also feature specific subtypes of general DA categories that mightbe constrained by discourse.
Prior research on task-oriented dialogues ummarized inthe next section, however, has also found only small reductions in WER (on the orderof 1%).
This suggests that even in task-oriented domains more research is needed torealize the potential of DA modeling for ASR.7.
Prior and Related WorkAs indicated in the introduction, our work builds on a number of previous effortsin computational discourse modeling and automatic discourse processing, most ofwhich occurred over the last half-decade.
It is generally not possible to directly com-pare quantitative results because of vast differences in methodology, tag set, type andamount of training data, and, principally, assumptions made about what informationis available for "free" (e.g., hand-transcribed versus automatically recognized words,or segmented versus unsegmented utterances).
Thus, we will focus on the conceptualaspects of previous research efforts, and while we do offer a summary of previousquantitative results, these should be interpreted as informative datapoints only, andnot as fair comparisons between algorithms.Previous research on DA modeling has generally focused on task-oriented ia-logue, with three tasks in particular garnering much of the research effort.
The MapTask corpus (Anderson et al 1991; Bard et al 1995) consists of conversations betweentwo speakers with slightly different maps of an imaginary territory.
Their task is tohelp one speaker eproduce a route drawn only on the other speaker's map, all with-out being able to see each other's maps.
Of the DA modeling algorithms describedbelow, Taylor et al (1998) and Wright (1998) were based on Map Task.
The VERBMO-BIL corpus consists of two-party scheduling dialogues.
A number of the DA m6delingalgorithms described below were developed for VERBMOBIL, including those of Mastet al (1996), Warnke et al (1997), Reithinger et al (1996), Reithinger and Klesen (1997),and Samuel, Carberry, and Vijay-Shanker (1998).
The ATR Conference corpus is a sub-set of a larger ATR Dialogue database consisting of simulated dialogues between asecretary and a questioner at international conferences.
Researchers using this corpusinclude Nagata (1992), Nagata and Morimoto (1993, 1994), and Kita et al (1996).
Ta-ble 13 shows the most commonly used versions of the tag sets from those three tasks.As discussed earlier, these domains differ from the Switchboard corpus in beingtask-oriented.
Their tag sets are also generally smaller, but some of the same problemsof balance occur.
For example, in the Map Task domain, 33% of the words occur in 1of the 12 DAs 0NSTRUCT).
Table 14 shows the approximate size of the corpora, the tagset, and tag estimation accuracy rates for various recent models of DA prediction.
Theresults summarized in the table also illustrate the differences in inherent difficulty ofthe tasks.
For example, the task of Warnke et al (1997) was to simultaneously segmentand tag DAs, whereas the other results rely on a prior manual segmentation.
Similarly,the task in Wright (1998) and in our study was to determine DA types from speechinput, whereas work by others is based on hand-transcribed textual input.363Computational Linguistics Volume 26, Number 3Table 13Dialogue act tag sets used in three other extensively studied corpora.VERBMOBIL.
These 18 high-level DAs used in VERBMOBIL-1 areabstracted over a total of 43 more specific DAs; most experiments onVERBMOBIL DAs use the set of 18 rather than 43.
Examples are fromJekat et al (1995).Tag ExampleTHANKGREETINTRODUCEBYEREQUEST~COMMENTSUGGESTREJECTACCEPTREQUEST-SUGGESTINITGIVE_REASONFEEDBACKDELIBERATECONFIRMCLARIFYDIGRESSMOTIVATEGARBAGEThanksHello DanIt's me againAlright byeHow does that look?from thirteenth through seventeenth JuneNo Friday I'm booked all daySaturday sounds fine,What is a good day of the week for you?I wanted to make an appointment with youBecause I have meetings all afternoonOkayLet me check my calendar hereOkay, that would be wonderfulOkay, do you mean Tuesday the 23rd?\[we could meet for lunch\] and eat lots of ice creamWe should go to visit our subsidiary in MunichOops, I-Maptask.
The 12 DAs or "move types" used in Map Task.
Examples arefrom Taylor et al (1998).Tag ExampleINSTRUCTEXPLAINALIGNCHECKQUERY-YNQUERY-WACKNOWLEDGECLARIFYREPLY-YREPLY-NREPLY-WREADYGo round, ehm horizontally underneath diamond mineI don't have a ravineOkay?So going down to Indian Country?Have you got the graveyard written down ?In where?Okay{you want to go.
.
.
diagonally} Diagonally downI do.No, I don't{And across to?}
The pyramid.OkayATR.
The 9 DAs ("illocutionary force types") used in the ATR Dialoguedatabase task; some later models used an extended set of 15 DAs.Examples are from the English translations given by Nagata (1992).Tag ExamplePHATICEXPRESSIVERESPONSEPROMISEREQUESTINFORMQUESTIONIPQUESTIONREFQUESTIONCONFHelloThank youThat's rightI will send you a registration formPlease go to Kitaooji station by subwayWe are not giving any discount his timeDo you have the announcement of the conference ?What should I do?You have already transferred the registration fee, right ?364Stolcke et al D ia logue Act  Mode l ing,.0C~'~ o% >.~  to.~~ ~g,..lm m ~c~8 ,~.~ ~c ~ Z~m.~  K, -~NS~~ ~.~??
l l  ~ ~vr--~ ~ , O', ?~365Computational Linguistics Volume 26, Number 3The use of n-grams to model the probabilities of DA sequences, or to predictupcoming DAs on-line, has been proposed by many authors.
It seems to have beenfirst employed by Nagata (1992), and in follow-up papers by Nagata and Morimoto(1993, 1994) on the ATR Dialogue database.
The model predicted upcoming DAs byusing bigrams and trigrams conditioned on preceding DAs, trained on a corpus of2,722 DAs.
Many others subsequently relied on and enhanced this n-grams-of-DAsapproach, often by applying standard techniques from statistical language modeling.Reithinger et al (1996), for example, used deleted interpolation tosmooth the dialoguen-grams.
Chu-Carroll (1998) uses knowledge of subdialogue structure to selectivelyskip previous DAs in choosing conditioning for DA prediction.Nagata and Morimoto (1993, 1994) may also have been the first to use word n-grams as a miniature grammar for DAs, to be used in improving speech recognition.The idea caught on very quickly: Suhm and Waibel (1994), Mast et aL (1996), Warnkeet al (1997), Reithinger and Klesen (1997), and Taylor et al (1998) all use variants ofbackoff, interpolated, orclass n-gram language models to estimate DA likelihoods.
Anykind of sufficiently powerful, trainable language model could perform this function, ofcourse, and indeed Alexandersson and Reithinger (1997) propose using automaticallylearned stochastic context-free grammars.
Jurafsky, Shriberg, Fox, and Curl (1998) showthat the grammar of some DAs, such as appreciations, can be captured by finite-stateautomata over part-of-speech tags.N-gram models are likelihood models for DAs, i.e., they compute the conditionalprobabilities of the word sequence given the DA type.
Word-based posterior probabilityestimators are also possible, although less common.
Mast et al (1996) propose the useof semantic lassification trees, a kind of decision tree conditioned on word patternsas features.
Finally, Ries (1999a) shows that neural networks using only unigram fea-tures can be superior to higher-order n-gram DA models.
Warnke et al (1999) andOhler, Harbeck, and Niemann (1999) use related discriminative training algorithmsfor language models.Woszczyna nd Waibel (1994) and Suhm and Waibel (1994), followed by Chu-Carroll (1998), seem to have been the first to note that such a combination of wordand dialogue n-grams could be viewed as a dialogue HMM with word strings asthe observations.
(Indeed, with the exception of Samuel, Carberry, and Vijay-Shanker(1998), all models listed in Table 14 rely on some version of this HMM metaphor.
)Some researchers explicitly used HMM induction techniques to infer dialogue gram-mars.
Woszczyna nd Waibel (1994), for example, trained an ergodic HMM usingexpectation-maximization o model speech act sequencing.
Kita et al (1996) madeone of the few attempts at unsupervised iscovery of dialogue structure, where afinite-state grammar induction algorithm is used to find the topology of the dialoguegrammar.Computational pproaches to prosodic modeling of DAs have aimed to auto-matically extract various prosodic parameters--such as duration, pitch, and energypatterns--from the speech signal (Yoshimura et al \[1996\]; Taylor et al \[1997\]; Kompe\[1997\], among others).
Some approaches model F0 patterns with techniques such asvector quantization and Gaussian classifiers to help disambiguate utterance types.
Anextensive comparison of the prosodic DA modeling literature with our work can befound in Shriberg et al (1998).DA modeling has mostly been geared toward automatic DA classification, andmuch less work has been done on applying DA models to automatic speech recog-nition.
Nagata and Morimoto (1994) suggest conditioning word language models onDAs to lower perplexity.
Suhm and Waibel (1994) and Eckert, Gallwitz, and Niemann(1996) each condition a recognizer LM on left-to-right DA predictions and are able to366Stolcke et al Dialogue Act Modelingshow reductions in word error rate of 1% on task-oriented corpora.
Most similar toour own work, but still in a task-oriented omain, the work by Taylor et al (1998)combines DA likelihoods from prosodic models with those from 1-best recognitionoutput o condition the recognizer LM, again achieving an absolute reduction in worderror rate of 1%, as disappointing as the 0.3% improvement in our experiments.Related computational tasks beyond DA classification and speech recognition havereceived even less attention to date.
We already mentioned Warnke et al (1997) andFinke et al (1998), who both showed that utterance segmentation a d classification canbe integrated into a single search process.
Fukada et al (1998) investigate augmentingDA tagging with more detailed semantic "concept" tags, as a preliminary step towardan interlingua-based dialogue translation system.
Levin et al (1999) couple DA clas-sification with dialogue game classification; dialogue games are units above the DAlevel, i.e., short DA sequences such as question-answer pairs.All the work mentioned so far uses statistical models of various kinds.
As we haveshown here, such models offer some fundamental dvantages, uch as modularity andcomposability (e.g., of discourse grammars with DA models) and the ability to dealwith noisy input (e.g., from a speech recognizer) in a principled way.
However, manyother classifier architectures are applicable to the tasks discussed, in particular to DAclassification.
A nonprobabilistic approach for DA labeling proposed by Samuel, Car-berry, and Vijay-Shanker (1998) is transformation-based l arning (Brill 1993).
Finallyit should be noted that there are other tasks with a mathematical structure similar tothat of DA tagging, such as shallow parsing for natural anguage processing (Munk1999) and DNA classification tasks (Ohler, Harbeck, and Niemann 1999), from whichfurther techniques could be borrowed.How does the approach presented here differ from these various earlier models,particularly those based on HMMs?
Apart from corpus and tag set differences, ourapproach differs primarily in that it generalizes the simple HMM approach to copewith new kinds of problems, based on the Bayes network representations depicted inFigures 2 and 4.
For the DA classification task, our framework allows us to do classifi-cation given unreliable words (by marginalizing over the possible word strings corre-sponding to the acoustic input) and given nonlexical (e.g., prosodic) evidence.
For thespeech recognition task, the generalized model gives a clean probabilistic frameworkfor conditioning word probabilities on the conversation context via the underlying DAstructure.
Unlike previous models that did not address peech recognition or reliedonly on an intuitive 1-best approximation, our model allows computation of the opti-mum word sequence by effectively summing over all possible DA sequences as wellas all recognition hypotheses throughout the conversation, using evidence from bothpast and future.8.
D iscuss ion and Issues for Future ResearchOur approach to dialogue modeling has two major components: tatistical dialoguegrammars modeling the sequencing of DAs, and DA likelihood models expressingthe local cues (both lexical and prosodic) for DAs.
We made a number of significantsimplifications to arrive at a computationally and statistically tractable formulation.In this formulation, DAs serve as the hinges that join the various model components,but also decouple these components through statistical independence assumptions.Conditional on the DAs, the observations across utterances are assumed to be inde-pendent, and evidence of different kinds from the same utterance (e.g., lexical andprosodic) is assumed to be independent.
Finally, DA types themselves are assumedto be independent beyond a short span (corresponding to the order of the dialogue367Computational Linguistics Volume 26, Number 3n-gram).
Further research within this framework can be characterized by which ofthese simplifications are addressed.Dialogue grammars for conversational speech need to be made more aware of thetemporal properties of utterances.
For example, we are currently not modeling the factthat utterances by the conversants may actually overlap (e.g., backchannels interrupt-ing an ongoing utterance).
In addition, we should model more of the nonlocal aspectsof discourse structure, despite our negative results so far.
For example, a context-freediscourse grammar could potentially account for the nested structures proposed inGrosz and Sidner (1986).
1?The standard n-gram models for DA discrimination with lexical cues are probablysuboptimal for this task, simply because they are trained in the maximum likelihoodframework, without explicitly optimizing discrimination between DA types.
This maybe overcome by using discriminative training procedures (Warnke et al 1999; Ohler,Harbeck, and Niemann 1999).
Training neural networks directly with posterior prob-ability (Ries 1999a) seems to be a more principled approach and it also offers mucheasier integration with other knowledge sources.
Prosodic features, for example, cansimply be added to the lexical features, allowing the model to capture dependenciesand redundancies across knowledge sources.
Keyword-based techniques from the fieldof message classification should also be applicable here (Rose, Chang, and Lippmann1991).
Eventually, it is desirable to integrate dialogue grammar, lexical, and prosodiccues into a single model, e.g., one that predicts the next DA based on DA history andall the local evidence.The study of automatically extracted prosodic features for DA modeling is likewiseonly in its infancy.
Our preliminary experiments with neural networks have shown thatsmall gains are obtainable with improved statistical modeling techniques.
However,we believe that more progress can be made by improving the underlying featuresthemselves, in terms of both better understanding of how speakers use them, andways to reliably extract hem from data.Regarding the data itself, we saw that the distribution of DAs in our corpus limitsthe benefit of DA modeling for lower-level processing, in particular speech recognition.The reason for the skewed distribution was in the nature of the task (or lack thereof) inSwitchboard.
It remains to be seen if more fine-grained DA distinctions can be madereliably in this corpus.
However, it should be noted that the DA definitions are reallyarbitrary as far as tasks other than DA labeling are concerned.
This suggests usingunsupervised, self-organizing learning schemes that choose their own DA definitionsin the process of optimizing the primary task, whatever it may be.
Hand-labeled DAcategories may still serve an important role in initializing such an algorithm.We believe that dialogue-related tasks have much to benefit from corpus-driven,automatic learning techniques.
To enable such research, we need fairly large, stan-dardized corpora that allow comparisons over time and across approaches.
Despiteits shortcomings, the Switchboard omain could serve this purpose.9.
ConclusionsWe have developed an integrated probabilistic approach to dialogue act modeling forconversational speech, and tested it on a large speech corpus.
The approach combinesmodels for lexical and prosodic realizations of DAs, as well as a statistical discourse10 The inadequacy of n-gram models for nested discourse structures i  pointed out by Chu-Carroll (1998),although the suggested solution is a modified n-gram approach.368Stolcke et al Dialogue Act Modelinggrammar.
All components of the model are automatically trained, and are thus appli-cable to other domains for which labeled data is available.
Classification accuraciesachieved so far are highly encouraging, relative to the inherent difficulty of the task asmeasured by human labeler performance.
We investigated several modeling alterna-tives for the components of the model (backoff n-grams and maximum entropy modelsfor discourse grammars, decision trees and neural networks for prosodic lassification)and found performance largely independent of these choices.
Finally, we developed aprincipled way of incorporating DA modeling into the probability model of a contin-uous speech recognizer, by constraining word hypotheses using the discourse context.However, the approach gives only a small reduction in word error on our corpus,which can be attributed to a preponderance of a single dialogue act type (statements).NoteThe research described here is based on aproject at the 1997 Workshop on InnovativeTechniques in LVCSR at the Center for Speechand Language Processing at Johns HopkinsUniversity (Jurafsky et al 1997; Jurafsky etal.
1998).
The DA-labeled Switchboard tran-scripts as well as other project-related publi-cations are available at http://www.colorado.edu/ling/jurafsky/ws97/.AcknowledgmentsWe thank the funders, researchers, andsupport staff of the 1997 Johns HopkinsSummer Workshop, especially Bill Byrne,Fred Jelinek, Harriet Nock, Joe Picone,Kimberly Shiring, and Chuck Wooters.Additional support came from the NSF viagrants IRI-9619921 and IRI-9314967, andfrom the UK Engineering and PhysicalScience Research Council (grantGR/J55106).
Thanks to Mitch Weintraub, toSusann LuperFoy, Nigel Ward, James Allen,Julia Hirschberg, and Marilyn Walker foradvice on the design of the SWBD-DAMSLtag set, to the discourse labelers at CUBoulder (Debra Biasca, Marion Bond, TraciCurl, Anu Erringer, Michelle Gregory, LoriHeintzelman, Taimi Metzler, and AmmaOduro) and the intonation labelers at theUniversity of Edinburgh (Helen Wright,Kurt Dusterhoff, Rob Clark, Cassie Mayo,and Matthew Bull).
We also thank AndyKehler and the anonymous reviewers forvaluable comments on a draft of this paper.ReferencesAlexandersson, Jan and Norbert Reithinger.1997.
Learning dialogue structures from acorpus.
In G. Kokkinakis, N. Fakotakis,and E. Dermatas, editors, Proceedings ofthe5th European Conference on SpeechCommunication a d Technology, volume 4,pages 2,231-2,234.
Rhodes, Greece,September.Anderson, Anne H., Miles Bader, Ellen G.Bard, Elizabeth H. Boyle, Gwyneth M.Doherty, Simon C. Garrod, Stephen D.Isard, Jacqueline C. Kowtko, Jan M.McAllister, Jim Miller, Catherine F. Sotillo,Henry S. Thompson, and Regina Weinert.1991.
The HCRC Map Task corpus.Language and Speech, 34(4):351-366.Austin, J. L. 1962.
How to do Things withWords.
Clarendon Press, Oxford.Bahl, Lalit R., Frederick Jelinek, andRobert L. Mercer.
1983.
A maximumlikelihood approach to continuous speechrecognition.
IEEE Transactions on PatternAnalysis and Machine Intelligence,5(2):179-190, March.Bard, Ellen G., Catherine Sotillo, Anne H.Anderson, and M. M. Taylor.
1995.
TheDCIEM Map Task corpus: Spontaneousdialogues under sleep deprivation anddrug treatment.
In Isabel Trancoso andRoger Moore, editors, Proceedings oftheESCA-NATO Tutorial and Workshop onSpeech under Stress, pages 25-28, Lisbon,September.Baum, Leonard E., Ted Petrie, GeorgeSoules, and Norman Weiss.
1970.
Amaximization technique occurring in thestatistical analysis of probabilisticfunctions in Markov chains.
The Annals ofMathematical Statistics, 41(1):164-171.Berger, Adam L., Stephen A. Della Pietra,and Vincent J. Della Pietra.
1996.
Amaximum entropy approach to naturallanguage processing.
ComputationalLinguistics, 22(1):39-71.Bourlard, Herv6 and Nelson Morgan.
1993.Connectionist Speech Recognition.
A HybridApproach.
Kluwer Academic Publishers,Boston, MA.Breiman, L., J. H. Friedman, R. A. Olshen,and C. J.
Stone.
1984.
Classification andRegression Trees.
Wadsworth and Brooks,Pacific Grove, CA.369Computational Linguistics Volume 26, Number 3Bridle, J. S. 1990.
Probabilistic interpretationof feedforward classification etworkoutputs, with relationships to statisticalpattern recognition.
In F. Fogleman Soulieand J. Herault, editors, Neurocomputing:Algorithms, Architectures and Applications.Springer, Berlin, pages 227-236.Brill, Eric.
1993.
Automatic grammarinduction and parsing free text: Atransformation-based approach.
InProceedings ofthe ARPA Workshop on HumanLanguage Technology, Plainsboro, NJ,March.Carletta, Jean.
1996.
Assessing agreement onclassification tasks: The Kappa statistic.Computational Linguistics, 22(2):249-254.Carlson, Lari.
1983.
Dialogue Games: AnApproach to Discourse Analysis.
D. Reidel.Chu-Carroll, Jennifer.
1998.
A statisticalmodel for discourse act recognition indialogue interactions.
In JenniferChu-Carroll and Nancy Green, editors,Applying Machine Learning to DiscourseProcessing.
Papers from the 1998 AAAISpring Symposium.
Technical ReportSS-98-01, pages 12-17.
AAAI Press, MenloPark, CA.Church, Kenneth Ward.
1988.
A stochasticparts program and noun phrase parserfor unrestricted text.
In Second Conferenceon Applied Natural Language Processing,pages 136-143, Austin, TX.Core, Mark and James Allen.
1997.
Codingdialogs with the DAMSL annotationscheme.
In Working Notes of the AAAI FallSymposium on Communicative Action inHumans and Machines, pages 28-35,Cambridge, MA, November.Dermatas, Evangelos and GeorgeKokkinakis.
1995.
Automatic stochastictagging of natural anguage texts.Computational Linguistics, 21(2):137-163.Eckert, Wieland, Florian Gallwitz, andHeinrich Niemann.
1996.
Combiningstochastic and linguistic language modelsfor recognition of spontaneous speech.
InProceedings ofthe IEEE Conference onAcoustics, Speech, and Signal Processing,volume 1, pages 423-426, Atlanta, GA,May.Finke, Michael, Maria Lapata, Alon Lavie,Lori Levin, Laura Mayfield Tomokiyo,Thomas Polzin, Klaus Ries, Alex Waibel,and Klaus Zechner.
1998.
Clarity:Inferring discourse structure from speech.In Jennifer Chu-Carroll and Nancy Green,editors, Applying Machine Learning toDiscourse Processing.
Papers from the 1998AAAI Spring Symposium.
Technical ReportSS-98-01, pages 25-32.
AAAI Press, MenloPark, CA.Fowler, Carol A. and Jonathan Housum.1987.
Talkers' signaling of "new" and"old" words in speech and listeners'perception and use of the distinction.Journal of Memory and Language, 26:489-504.Fukada, Toshiaki, Detlef Koll, Alex Waibel,and Kouichi Tanigaki.
1998.
Probabilisticdialogue act extraction for concept basedmultilingual translation systems.
InRobert H. Mannell and JordiRobert-Ribes, editors, Proceedings oftheInternational Conference on Spoken LanguageProcessing, volume 6, pages 2,771-2,774,Sydney, December.
Australian SpeechScience and Technology Association.Godfrey, J. J., E. C. Holliman, andJ.
McDaniel.
1992.
SWITCHBOARD:Telephone speech corpus for research anddevelopment.
In Proceedings ofthe IEEEConference on Acoustics, Speech, and SignalProcessing, volume 1, pages 517-520, SanFrancisco, CA, March.Grosz, Barbara J. and Candace L. Sidner.1986.
Attention, intention, and thestructure of discourse.
ComputationalLinguistics, 12(3):175-204.Hirschberg, Julia B. and Diane J. Litman.1993.
Empirical studies on thedisambiguation of cue phrases.Computational Linguistics, 19(3):501-530.Iyer, Rukmini, Mari Ostendorf, and J. RobinRohlicek.
1994.
Language modeling withsentence-level mixtures.
In Proceedings ofthe ARPA Workshop on Human LanguageTechnology, pages 82-86, Plainsboro, NJ,March.Jefferson, Gail.
1984.
Notes on a systematicdeployment of the acknowledgementtokens 'yeah' and 'mm hm'.
Papers inLinguistics, 17:197-216.Jekat, Susanne, Alexandra Klein, ElisabethMaier, Ilona Maleck, Marion Mast, andJoachim Quantz.
1995.
Dialogue acts inVERBMOBIL.
Verbmobil-Report 65,Universit~it Hamburg, DFKI GmbH,Universit~it Erlangen, and TU Berlin,April.Jurafsky, Dan, Rebecca Bates, Noah Coccaro,Rachel Martin, Marie Meteer, Klaus Ries,Elizabeth Shriberg, Andreas Stolcke, PaulTaylor, and Carol Van Ess-Dykema.
1997.Automatic detection of discoursestructure for speech recognition andunderstanding.
In Proceedings ofthe IEEEWorkshop on Speech Recognition andUnderstanding, pages 88-95, SantaBarbara, CA, December.Jurafsky, Daniel, Rebecca Bates, NoahCoccaro, Rachel Martin, Marie Meteer,Klaus Ries, Elizabeth Shriberg, AndreasStolcke, Paul Taylor, and Carol Van370Stolcke et al Dialogue Act ModelingEss-Dykema.
1998.
Switchboard iscourselanguage modeling project final report.Research Note 30, Center for Languageand Speech Processing, Johns HopkinsUniversity, Baltimore, MD, January.Jurafsky, Daniel, Elizabeth Shriberg, andDebra Biasca.
1997.
Switchboard-DAMSLLabeling Project Coder's Manual.Technical Report 97-02, University ofColorado, Institute of Cognitive Science,Boulder, CO. http://www.colorado.edu/ling/jurafsky/manual.augustl.html.Jurafsky, Daniel, Elizabeth E. Shriberg,Barbara Fox, and Traci Curl.
1998.
Lexical,prosodic, and syntactic ues for dialogacts.
In Proceedings ofACL/COLING-98Workshop on Discourse Relations andDiscourse Markers, pages 114-120.Association for ComputationalLinguistics.Katz, Slava M. 1987.
Estimation ofprobabilities from sparse data for thelanguage model component of a speechrecognizer.
IEEE Transactions on Acoustics,Speech, and Signal Processing, 35(3):400-401,March.Kita, Kenji, Yoshikazu Fukui, MasaakiNagata, and Tsuyoshi Morimoto.
1996.Automatic acquisition of probabilisticdialogue models.
In H. Timothy Bunnelland William Idsardi, editors, Proceedings ofthe International Conference on SpokenLanguage Processing, volume 1,pages 196-199, Philadelphia, PA, October.Kompe, Ralf.
1997.
Prosody in speechunderstanding systems.
Springer, Berlin.Kowtko, Jacqueline C. 1996.
The Function ofIntonation in Task Oriented Dialogue.
Ph.D.thesis, University of Edinburgh,Edinburgh.Kuhn, Roland and Renato de Mori.
1990.
Acache-base natural anguage model forspeech recognition.
IEEE Transactions onPattern Analysis and Machine Intelligence,12(6):570-583, June.Levin, Joan A. and Johanna A. Moore.
1977.Dialogue games: Metacommunicationstructures for natural anguageinteraction.
Cognitive Science, 1(4):395-420.Levin, Lori, Klaus Ries, Ann Thym~-Gobbel,and Alon Lavie.
1999.
Tagging of speechacts and dialogue games in SpanishCallHome.
In Towards Standards and Toolsfor Discourse Tagging (Proceedings ofthe ACLWorkshop at ACL'99), pages 42-47, CollegePark, MD, June.Linell, Per.
1990.
The power of dialoguedynamics.
In Ivana Markov~ and KlausFoppa, editors, The Dynamics of Dialogue.Harvester, Wheatsheaf, New York,London, pages 147-177.Mast, M., R. Kompe, S. Harbeck,A.
Kiel~ling, H. Niemann, E. NOth, E. G.Schukat-Talamazzini, and V. Warnke.1996.
Dialog act classification with thehelp of prosody.
In H. Timothy Bunnelland William Idsardi, editors, Proceedings ofthe International Conference on SpokenLanguage Processing, volume 3,pages 1,732-1,735, Philadelphia, PA,October.Menn, Lise and Suzanne E. Boyce.
1982.Fundamental frequency and discoursestructure.
Language and Speech, 25:341-383.Meteer, Marie, Ann Taylor, RobertMacIntyre, and Rukmini Iyer.
1995.Dysfluency annotation stylebook for theSwitchboard corpus.
Distributed by LDC,ftp://ftp.cis.upenn.edu/pub/treebank/swbd/doc/DFL-book.ps, February.Revised June 1995 by Ann Taylor.Morgan, Nelson, Eric Fosler, and NikkiMirghafori.
1997.
Speech recognitionusing on-line estimation of speaking rate.In G. Kokkinakis, N. Fakotakis, and E.Dermatas, editors, Proceedings ofthe 5thEuropean Conference on SpeechCommunication a d Technology, volume 4,pages 2,079-2,082, Rhodes, Greece,September.Munk, Marcus.
1999.
Shallow StatisticalParsing for Machine Translation.
Diplomathesis, Carnegie Mellon University.Nagata, Masaaki.
1992.
Using pragmatics torule out recognition errors in cooperativetask-oriented dialogues.
In John J. Ohala,Terrance M. Nearey, Bruce L. Derwing,Megan M. Hodge, and Grace E. Wiebe,editors, Proceedings ofthe InternationalConference on Spoken Language Processing,volume 1, pages 647-650, Banff, Canada,October.Nagata, Masaaki and Tsuyoshi Morimoto.1993.
An experimental statistical dialoguemodel to predict he speech act type ofthe next utterance.
In Katsuhiko Shirai,Tetsunori Kobayashi, and YasunariHarada, editors, Proceedings oftheInternational Symposium on Spoken Dialogue,pages 83-86, Tokyo, November.Nagata, Masaaki and Tsuyoshi Morimoto.1994.
First steps toward statisticalmodeling of dialogue to predict hespeech act type of the next utterance.Speech Communication, 15:193-203.Ohler, Uwe, Stefan Harbeck, and HeinrichNiemann.
1999.
Discriminative training oflanguage model classifiers.
In Proceedingsof the 6th European Conference on SpeechCommunication a d Technology, volume 4,pages 1607-1610, Budapest, September.371Computational Linguistics Volume 26, Number 3Pearl, Judea.
1988.
Probabilistic Reasoning inIntelligent Systems: Networks of PlausibleInference.
Morgan Kaufmann, San Mateo,CA.Power, Richard J. D. 1979.
The organizationof purposeful dialogues.
Linguistics,17:107-152.Rabiner, L. R. and B. H. Juang.
1986.
Anintroduction to hidden Markov models.IEEE ASSP Magazine, 3(1):4-16, January.Reithinger, Norbert, Ralf Engel, MichaelKipp, and Martin Klesen.
1996.
Predictingdialogue acts for a speech-to-speechtranslation system.
In H. Timothy Bunnelland William Idsardi, editors, Proceedings ofthe International Conference on SpokenLanguage Processing, volume 2,pages 654-657, Philadelphia, PA, October.Reithinger, Norbert and Martin Klesen.1997.
Dialogue act classification usinglanguage models.
In G. Kokkinakis, N.Fakotakis, and E. Dermatas, editors,Proceedings ofthe 5th European Conference onSpeech Communication a d Technology,volume 4, pages 2,235-2,238, Rhodes,Greece, September.Ries, Klaus.
1999a.
HMM and neuralnetwork based speech act classification.
InProceedings ofthe IEEE Conference onAcoustics, Speech, and Signal Processing,volume 1, pages 497-500, Phoenix, AZ,March.Ries, Klaus.
1999b.
Towards the detectionand description of textual meaningindicators in spontaneous conversations.In Proceedings ofthe 6th European Conferenceon Speech Communication a d Technology,volume 3, pages 1,415--1,418, Budapest,September.Rose, R. C., E. I. Chang, and R. P.Lippmann.
1991.
Techniques forinformation retrieval from voicemessages.
In Proceedings ofthe IEEEConference on Acoustics, Speech, and SignalProcessing, volume 1, pages 317-320,Toronto, May.Sacks, H., E. A. Schegloff, and G. Jefferson.1974.
A simplest semantics for theorganization of turn-taking inconversation.
Language, 50(4):696-735.Samuel, Ken, Sandra Carberry, andK.
Vijay-Shanker.
1998.
Dialogue acttagging with transformation-basedlearning.
In Proceedings ofthe 36th AnnualMeeting of the Association for ComputationalLinguistics and 17th International Conferenceon Computational Linguistics, volume 2,pages 1,150-1,156, Montreal.Schegloff, Emanuel A.
1968.
Sequencing inconversational openings.
AmericanAnthropologist, 70:1,075-1,095.Schegloff, Emanuel A.
1982.
Discourse as aninteractional chievement: Some uses of'uh huh' and other things that comebetween sentences.
In Deborah Tannen,editor, Analyzing Discourse: Text and Talk.Georgetown University Press,Washington, D.C., pages 71-93.Searle, J. R. 1969.
Speech Acts.
CambridgeUniversity Press, London-New York.Shriberg, Elizabeth, Rebecca Bates, AndreasStolcke, Paul Taylor, Daniel Jurafsky,Klaus Ries, Noah Coccaro, Rachel Martin,Marie Meteer, and Carol VanEss-Dykema.
1998.
Can prosody aid theautomatic lassification of dialog acts inconversational speech?
Language andSpeech, 41(3-4):439--487.Shriberg, Elizabeth, Andreas Stolcke, DilekHakkani-Ti~r, and GOkhan Tiir.
2000.Prosody-based automatic segmentation fspeech into sentences and topics.
SpeechCommunication, 32(1-2).
Special Issue onAccessing Information in Spoken Audio.To appear.Siegel, Sidney and N. John Castellan, Jr.1988.
Nonparametric Statistics for theBehavioral Sciences.
Second edition.McGraw-Hill, New York.Stolcke, Andreas and Elizabeth Shriberg.1996.
Automatic linguistic segmentationof conversational speech.
In H. TimothyBunnell and William Idsardi, editors,Proceedings ofthe International Conference onSpoken Language Processing, volume 2,pages 1,005-1,008, Philadelphia, PA,October.Suhm, B. and A. Waibel.
1994.
Toward betterlanguage models for spontaneous speech.In Proceedings ofthe International Conferenceon Spoken Language Processing, volume 2,pages 831-834, Yokohama, September.Taylor, Paul A.
2000.
Analysis and synthesisof intonation using the tilt model.
Journalof the Acoustical Society of America,107(3):1,697-1,714.Taylor, Paul A., Simon King, Stephen Isard,and Helen Wright.
1998.
Intonation anddialog context as constraints for speechrecognition.
Language and Speech,41(3-4):489-508.Taylor, Paul A., Simon King, Stephen Isard,Helen Wright, and Jacqueline Kowtko.1997.
Using intonation to constrainlanguage models in speech recognition.
InG.
Kokkinakis, N. Fakotakis, and E.Dermatas, editors, Proceedings ofthe 5thEuropean Conference on SpeechCommunication a d Technology, volume 5,pages 2,763-2,766, Rhodes, Greece,September.372Stolcke et al Dialogue Act ModelingVan Ess-Dykema, Carol and Klaus Ries.1998.
Linguistically engineered tools forspeech recognition error analysis.
InRobert H. Mannell and JordiRobert-Ribes, editors, Proceedings oftheInternational Conference on Spoken LanguageProcessing, volume 5, pages 2,091-2,094,Sydney, December.
Australian SpeechScience and Technology Association.Viterbi, A.
1967.
Error bounds forconvolutional codes and anasymptotically optimum decodingalgorithm.
IEEE Transactions on InformationTheory, 13:260-269.Warnke, Volker, Stefan Harbeck, ElmarN0th, Heinrich Niemann, and MichaelLevit.
1999.
Discriminative estimation ofinterpolation parameters for languagemodel classifiers.
In Proceedings ofthe IEEEConference on Acoustics, Speech, and SignalProcessing, volume 1, pages 525-528,Phoenix, AZ, March.Warnke, Volker, R. Kompe, HeinrichNiemann, and Elmar NOth.
1997.Integrated ialog act segmentation a dclassification using prosodic features andlanguage models.
In G. Kokkinakis, N.Fakotakis, and E. Dermatas, editors,Proceedings ofthe 5th European Conference onSpeech Communication a d Technology,volume 1, pages 207-210, Rhodes, Greece,September.Weber, Elizabeth G. 1993.
Varieties ofQuestions in English Conversation.
JohnBenjamins, Amsterdam.Witten, Ian H. and Timothy C. Bell.
1991.The zero-frequency problem: Estimatingthe probabilities of novel events inadaptive text compression.
IEEETransations on Information Theory,37(4):1,085-1,094, July.Woszczyna, M. and A. Waibel.
1994.Inferring linguistic structure in spokenlanguage.
In Proceedings ofthe InternationalConference on Spoken Language Processing,volume 2, pages 847-850, Yokohama,September.Wright, Helen.
1998.
Automatic utterancetype detection using suprasegmentalfeatures.
In Robert H. Mannell and JordiRobert-Ribes, editors, Proceedings oftheInternational Conference on Spoken LanguageProcessing, volume 4, pages 1,403-1,406,Sydney, December.
Australian SpeechScience and Technology Association.Wright, Helen and Paul A. Taylor.
1997.Modelling intonational structure usinghidden Markov models.
In Intonation:Theory, Models and Applications.
Proceedingsof an ESCA Workshop, pages 333-336,Athens, September.Yngve, Victor H. 1970.
On getting a word inedgewise.
In Papers from the Sixth RegionalMeeting of the Chicago Linguistic Society,pages 567-577, Chicago, April.
Universityof Chicago.Yoshimura, Takashi, Satoru Hayamizu,Hiroshi Ohmura, and Kazuyo Tanaka.1996.
Pitch pattern clustering of userutterances in human-machine dialogue.
InH.
Timothy Bunnell and William Idsardi,editors, Proceedings ofthe InternationalConference on Spoken Language Processing,volume 2, pages 837-840, Philadelphia,PA, October.373
