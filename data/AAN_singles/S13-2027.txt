Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 148?152, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsSFS-TUE: Compound Paraphrasing with a Language Model andDiscriminative RerankingYannick VersleySfS / SFB 833University of Tu?bingenversley@sfs.uni-tuebingen.deAbstractThis paper presents an approach for gener-ating free paraphrases of compounds (task 4at SemEval 2013) by decomposing the train-ing data into a collection of templates andfillers and recombining/scoring these based ona generative language model and discrimina-tive MaxEnt reranking.The system described in this paper achievedthe highest score (with a very small margin) inthe (default) isomorphic setting of the scorer,for which it was optimized, at a disadvantageto the non-isomorphic score.1 IntroductionCompounds are an interesting phenomenon in nat-ural language semantics as they normally realize asemantic relation (between head and modifier noun)that is both highly ambiguous as to the type of rela-tion and usually nonambiguous as to the concepts itrelates (namely, those of the two nouns).Besides inventory-based approaches, where therelation is classified into a fixed number of relations,many researchers have argued that the full variabil-ity of the semantic relations inherent in compoundsis best captured with paraphrases: Lauer (1995) pro-poses to use a preposition as a proxy for the meaningof a compound.
Finin (1980) and later Nakov (2008)and others propose less restrictive schemes based onparaphrasing verbs.A previous SemEval task (task 9 in 2010; But-nariu et al 2009).
The most successsful approachesfor this task such as Nulty and Costello (2010), Liet al(2010), and Wubben (2010), or the subse-quent approach of Wijaya and Gianfortoni (2011),all make efficient use of both the training data andgeneral evidence from WordNet or statistics derivedfrom large corpora.
The paper of Li et almen-tions that solely inducing a global ranking of para-phrasing verbs from the training data (looking whichverb is ranked higher in those cases where both wereconsidered for the same compound) yielded higherscores than an unsupervised approach based on thesemantic resources, underlining the need to combinetraining data and resources efficiently.SemEval 2013 task 4 The present task on pro-viding free paraphrases for noun compounds (Hen-drickx et al 2013) uses a dataset collected from Me-chanical Turk workers asked to paraphrase a givencompound (without context).
Prepositional, verbal,and other paraphrases all occur in the data:(1) a. bar for wineb.
bar that serves winec.
bar where wine is soldd.
sweet vinegar made from wineIn the examples, the words of the compound (winebar and wine vinegar, respectively) are put in ital-ics, and other content words in the paraphrase areunderlined.It is clear that certain paraphrases (X for Y) will becommon across many compounds, whereas the onescontaining more lexical material will differ even be-tween relatively similar compounds (consider winebar from the example, and liquor store, which al-lows paraphrase c, but not paraphrase b).1482 General ApproachThe approach chosen in the SFS-TUE system isbased on first retrieving a number of similar com-pounds, then extracting a set of building blocks (pat-terns and fillers) from these compounds, recombin-ing these building blocks, and finally ranking thelist of potential paraphrases.
The final list is post-processed by keeping only one variant of each setof paraphrases that only differ in a determiner (e.g.,?strike from air?
and ?strike from the air?)
in orderto make a 1:1 mapping between system response andgold standard possible.As a first step, the system retrieves the most simi-lar compounds from the training data.This is achieved Lin?s wordnet similarity measure(Lin, 1998) using the implementation in NLTK (Birdet al 2009).
The similarity of two compounds X1Y1and X2Y2 is calculated assC = min(sim(X1, X2), sim(Y1, Y2)) +0.1 ?
(sim(X1, X2) + sim(Y1, Y2))which represents a compromise between requiringthat both modifier and head are approximately sim-ilar, and still giving a small boost to pairs that havevery high modifier similarity but low head similar-ity, or vice versa.
For training, the target compoundis excluded from the most-similar compounds list sothat candidate construction is only based on actualneighbours.The paraphrases for the most similar compoundentries (such as 2a) are broken down into templates(2b) and fillers (2c), by replacing modifier and headby X and Y , respectively, and other content wordsby their part-of-speech tag.
(2) a. bar that serves wineb.
X that VBZ Yc.
VBZ:serveConversely, template fillers consist of all the ex-tracted content words, categorized by their part-of-speech.
(Part-of-speech tags were assigned using theStanford POS tagger: Toutanova et al 2003).Both paraphrase templates and template fillers areweighted by the product of the similarity value sCbetween the target compound and the neighbour, andthe total frequency of occurrence in that neighbour?stype examplesY of Y of X (159) / Y of the X (59) / Y of a X (47)Y for Y for X (114) / Y for the X (33)Y VBZ Y that VBZ X (91)/ Y which VBZ X (45)Y VBG Y VBG X (90) / Y VBG the X/ Y VBG with XY VBN Y VBN for X (82) / Y VBN by X (52)Y in Y in X (31)Y on Y on X (38)Table 1: Most frequent paraphrase pattern types and pat-tern instancesparaphrases.
(For example, if Mechanical Turk par-ticipants named ?bar that sells wine?
twice and ?barthat serves wine?
once, the total frequency of ?Xthat VBZ Y ?
would be three).Paraphrase candidates are then constructed bycombining any paraphrase templates from a simi-larity neighbour with any fillers matching the givenpart-of-speech tag.
The list of all candidates is cutdown to a shortlist of 512 paraphrase candidates.These are subsequently ranked by assigning featuresto each of the candidate paraphrases and scoringthem using weights learned in a maximum rankerby optimizing a loss derived from the probability ofall candidates that have been mentioned at least twotimes in the training set in proportion to the probabil-ity of all candidates that are not part of the trainingannotation for that compound at all.
(Paraphrasesthat were named only once are not used for the pa-rameter estimation).After scoring, determiners are removed from theparaphrase string and duplicates are removed fromthe list.
The generated list is cut off to yield at most60 items.2.1 Data SourcesAs sources of evidence in the fit (or lack thereof)of a given verb (as a suspected template filler) withthe two target words of a compounds, we use dataderived from the fifth revision of the English Giga-word1, tokenized, tagged and parsed with the RASPparsing toolchain (Briscoe et al 2006), and fromGoogle?s web n-gram dataset2.1Robert Parker, David Graff, Junbo Kong, Ke Chen andKazuaki Maeda (2011): English Gigaword Fifth Edition.LDC2011T07, Linguistic Data Consortium, Philadelphia.2Thorsten Brants, Alex Franz (2006): Web 1T 5-gram Ver-sion 1.
LDC2006T13, Linguistic Data Consortium, Philadel-149To reproduce very general estimates of linguis-tic plausibility, we built a four-gram language modelbased on the combined text of the English Gigawordand the British National Corpus (Burnard, 1995),using the KenLM toolkit (Heafield, 2011).
On theone hand, free paraphrases are quite unrestricted,which means that the language model helps also inthe case of more exotic paraphrases such as (1d)in the first section.
On the other hand, many ofthe more specialized aspects of plausibility such aspreposition attachment or selectional preferences forsubjects and direct objects can be cast as modeling(smoothed) probabilities for a certain class of shortsurface strings, for which an n-gram model is a use-ful first approximation.Using the grammatical relations extracted by theRASP toolkit, we created a database of plausibleverb-subject and verb-object combinations, definedas having a positive pointwise mutual informationscore.In a similar fashion, we used a list of verbs andthe morphg morphological realizer (Minnen et al2001) to extract all occurrences of the patterns ?NPREP N?, ?N PREP (DET) N?
for noun-preposition-noun combinations, and ?N that VBZ?
as well as ?NVBN by?
for finding typical cases of an active or pas-sive verb that modifies a given noun.2.2 Ranking featuresThe following properties used to score each para-phrase candidate (using weights learned by the Max-Ent ranker):?
language model score lmThe score assigned by the 4-gram modellearned on the English Gigaword and the BNC.?
pattern type tp=typeThe pattern type (usually the first two ?interest-ing?
tokens from the paraphrase template, i.e.,filtering out determiners and auxiliaries).
A listof the most frequent pattern types can be foundin Table 1.?
pattern weight patThe pattern weight as the sum of the (neighboursimilarity times number of occurrences) contri-bution from each pattern template.phia.?
linking preposition prep prep=typeThis feature correlates occurring prepositions(prep) to types of patterns, with the goalof learning high feature weights for preposi-tion/type combinations that fit well together.The obvious example for this would be, e.g.,that the of preposition pattern fits well withY of X paraphrases.?
absent preposition noprep=typeThis feature is set when no X prep Y or similarpattern could be found.?
subject preference (VBG, VBZ)subj subj0, subj n that vbzobject preference (VBN)obj dobj0, obj n vbn byIn cases of verbal paraphrases where the com-pound head is the subject, we can directlycheck for corpus evidence for the correspond-ing subject-verb pattern.
A similar check isdone for verb-object (or verb-patient) patternsin the paraphrases that involve the head in apassive construction.?
frequent/infrequent subject verb (VBG, VBZ)subj verb, subj infrequentSome verbs (belong, come, concern, consist,contain, deal, give, have, involve, make, pro-vide, regard, run, sell, show, use, work) oc-cur frequent enough that we want to introducea (data-induced) bias towards or away fromthem.
Other verbs, which are more rare, aretreated as a single class in this regard (whichmeans that their goodness of fit is mostly rep-resented through the language model and theselectional preference models).?
frequent/infrequent object verb (VBN)a similar distinction is made for a list ofverbs that often occur in passive form (ap-pointed, associated, based, carried, caused,conducted, designed, found, given, held, kept,meant, needed, performed, placed, prepared,produced, provided, related, taken)?
co-occurrence of filler with X (other patterns)other POS cooc, other POS noneFor pattern types where we cannot use one of150System isomorphic non-isom.SFS 0.2313 0.1795IIITH 0.2309 0.2584MELODI I 0.1300 0.5485MELODI II 0.1358 0.5360of+for baseline 0.0472 0.8294Table 2: Official evaluation results + simple baselinethe selectional preference models, we use amodel akin to Pado&Lapata?s (2007) syntax-based model that provides association scoresbased on syntactic dependency arc distance.3 Evaluation ResultsThe official evaluation results for the task are sum-marized in Table 2.
Two evaluation scores wereused:?
Isomorphic scoring maps system paraphrasesto (unmapped) paraphrases from the referencedataset, and requires systems to produce thefull set of paraphrases gathered from Mechani-cal Turk workers in order to get a perfect score.?
Nonisomorphic scoring scores each systemparaphrase with respect to the best match fromthe reference dataset, and averages these scoresover all system paraphrases.
A system thatperforms well in nonisomorphic scoring doesnot need to produce all paraphrases, but willget punished for producing non-reliable para-phrases.As apparent from the table, systems either score wellon the isomorphic score (producing a large numberof paraphrases in order to get good coverage of therange of expressions in the reference) or on the non-isomorphic score (producing a smaller number ofparaphrases that are highly ranked in the reference).The difference is also apparent in the case of a hy-pothetical system that produces ?Y for X?
and and?Y of X?
as the paraphrase for any compound (e.g.bar for wine and bar of wine for wine bar).
Becausethese paraphrases occur quite often as most frequentresponses, this would yield a high non-isomorphicscore, but an isomorphic score that is very low.During system development, the relative qualityof system paraphrases for each compound was es-timated using Maximum Average Precision (MAP)Compound closest neighbour MAP Rmaxshare holding withdrawal line 1.000 0.800union power community life 1.000 0.750truth value accounting treatment 1.000 0.750amateur championship computer study 1.000 0.750government authority unit manager 1.000 0.680wine bar computer industry 0.000 0.040mammoth task consumer benefit 0.000 0.040obstacle course work area 0.000 0.040operating system telephone system 0.000 0.000deadweight burden divorce rate 0.000 0.000Table 3: Best and worst compounds in cross-validationon the training dataand the total achievable recall (Rmax) of the gen-erated paraphrase list.
Table 3 shows the MAPscore (for paraphrases that were listed at least twotimes) and achievable recall (for all paraphrases).These measures, unlike the official scores, do notattempt to deal with paraphrase variants (e.g.
dif-ferent prepositions for a verbal paraphrase), but arerobust and simple enough to give an impression ofthe quality of the system response.As can be seen by looking at the achievable re-call figures, it is not always the case that all refer-ence paraphrases are in the list that is ranked by theMaxEnt model.
In the lower half of table 3, we seethat for these cases, the most-similar item selectedby the WordNet-based similarity measure is not veryclose semantically; whether this is the only influ-encing factor remains to be seen since some of thebest-ranked items in the upper half are also abstractconcepts with only-somewhat-close neighbours.
Fu-ture work would therefore have to cover both im-provements to the similarity measure itself and to theranking mechanism used for the reranking of gener-ated paraphrases.AcknowledgmentsThe author?s work was funded as part of SFB833 (?Constitution of Meaning?)
by the DeutscheForschungsgemeinschaft (DFG).ReferencesBird, S., Loper, E., and Klein, E. (2009).
NaturalLanguage Processing with Python.
O?Reilly Me-dia Inc.151Briscoe, E., Carroll, J., and Watson, R. (2006).
Thesecond release of the RASP system.
In Proceed-ings of the COLING/ACL 2006 Interactive Pre-sentation Sessions.Burnard, L., editor (1995).
Users Reference GuideBritish National Corpus Version 1.0.
Oxford Uni-versity Computing Service.Butnariu, C., Kim, S. N., Nakov, P., Seaghdha,D.
O., Spakowicz, S., and Veale, T. (2009).SemEval-2010 task 9: The interpretation of nouncompounds using paraphrasing verbs and prepo-sition.
In Proceedings of the NAACL HLT Work-shop on Semantic Evaluations: Recent Achieve-ments and Future Directions.Finin, T. W. (1980).
The semantic interpretation ofcompound nominals.
Report T-96, University ofIllinois, Coordinated Science Laboratory.Heafield, K. (2011).
KenLM: Faster and smallerlanguage model queries.
In Proceedings of theEMNLP 2011 Sixth Workshop on Statistical Ma-chine Translation.Hendrickx, I., Kozareva, Z., Nakov, P., Se?aghdha,D.
O., Szpakowicz, S., and Veale, T. (2013).SemEval-2013 task 4: Free paraphrases of nouncompounds.
In Proceedings of the InternationalWorkshop on Semantic Evaluation, SemEval ?13.Lauer, M. (1995).
Corpus statistics meet the nouncompound: some empirical results.
In Proceed-ings of the 33rd Annual Meeting of the Associa-tion for Computational Linguistics (ACL 1995).Li, G., Lopez-Fernandez, A., and Veale, T. (2010).Ucd-goggle: A hybrid system for noun compoundparaphrasing.
In Proceedings of the 5th Interna-tional Workshop on Semantic Evaluation.Lin, D. (1998).
An information-theoretic defini-tion of similarity.
In Proceedings of InternationalConference on Machine Learning.Minnen, G., Caroll, J., and Pearce, D. (2001).
Ap-plied morphological processing of English.
Natu-ral Language Engineering, 7(3):207?223.Nakov, P. (2008).
Noun compound interpretationusing paraphrasing verbs: Feasibility study.
InDochev, D., Pistore, M., and Traverso, P., ed-itors, Artificial Intelligence: Methodology, Sys-tems, and Applications, volume 5253 of Lec-ture Notes in Computer Science, pages 103?117.Springer Berlin Heidelberg.Nulty, P. and Costello, F. (2010).
Ucd-pn: Selectinggeneral paraphrases using conditional probability.In Proceedings of the 5th International Workshopon Semantic Evaluation.Pado?, S. and Lapata, M. (2007).
Dependency-basedconstruction of semantic space models.
Compu-tational Linguistics, 33(2):161?199.Toutanova, K., Klein, D., Manning, C. D., andSinger, Y.
(2003).
Feature-rich part-of-speechtagging with a cyclic dependency network.
InProc.
NAACL 2003, pages 252?259.Wijaya, D. T. and Gianfortoni, P. (2011).
?nutcase: what does it mean??
: understanding se-mantic relationship between nouns in noun com-pounds through paraphrasing and ranking theparaphrases.
In Proceedings of the 1st inter-national workshop on Search and mining entity-relationship data, SMER ?11, pages 9?14.Wubben, S. (2010).
Uvt: Memory-based pairwiseranking of paraphrasing verbs.
In Proceedings ofthe 5th International Workshop on Semantic Eval-uation.152
