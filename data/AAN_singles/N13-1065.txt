Proceedings of NAACL-HLT 2013, pages 579?584,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsOvercoming the Memory Bottleneck in Distributed Training ofLatent Variable Models of TextYi YangNorthwestern UniversityEvanston, ILyiyang@eecs.northwestern.eduAlexander YatesTemple UniversityPhiladelphia, PAyates@temple.eduDoug DowneyNorthwestern UniversityEvanston, ILddowney@eecs.northwestern.eduAbstractLarge unsupervised latent variable models(LVMs) of text, such as Latent Dirichlet Al-location models or Hidden Markov Models(HMMs), are constructed using parallel train-ing algorithms on computational clusters.
Thememory required to hold LVM parametersforms a bottleneck in training more powerfulmodels.
In this paper, we show how the mem-ory required for parallel LVM training canbe reduced by partitioning the training corpusto minimize the number of unique words onany computational node.
We present a greedydocument partitioning technique for the task.For large corpora, our approach reduces mem-ory consumption by over 50%, and trains thesame models up to three times faster, whencompared with existing approaches for paral-lel LVM training.1 IntroductionUnsupervised latent variable models (LVMs) of textare utilized extensively in natural language process-ing (Griffiths and Steyvers, 2004; Ritter et al 2010;Downey et al 2007; Huang and Yates, 2009; Li andMcCallum, 2005).
LVM techniques include LatentDirichlet Allocation (LDA) (Blei et al 2003), Hid-den Markov Models (HMMs) (Rabiner, 1989), andProbabilistic Latent Semantic Analysis (Hofmann,1999), among others.LVMs become more predictive as they are trainedon more text.
However, training LVMs on mas-sive corpora introduces computational challenges, interms of both time and space complexity.
The timecomplexity of LVM training has been addressedthrough parallel training algorithms (Wolfe et al2008; Chu et al 2006; Das et al 2007; Newmanet al 2009; Ahmed et al 2012; Asuncion et al2011), which reduce LVM training time through theuse of large computational clusters.However, the memory cost for training LVMs re-mains a bottleneck.
While LVM training makes se-quential scans of the corpus (which can be stored ondisk), it requires consistent random access to modelparameters.
Thus, the model parameters must bestored in memory on each node.
Because LVMs in-clude a multinomial distribution over words for eachlatent variable value, the model parameter space in-creases with the number of latent variable valuestimes the vocabulary size.
For large models (i.e.,with many latent variable values) and large cor-pora (with large vocabularies), the memory requiredfor training can exceed the limits of the commod-ity servers comprising modern computational clus-ters.
Because model accuracy tends to increase withboth corpus size and model size (Ahuja and Downey,2010; Huang and Yates, 2010), training accurate lan-guage models requires that we overcome the mem-ory bottleneck.We present a simple technique for mitigating thememory bottleneck in parallel LVM training.
Ex-isting parallelization schemes begin by partitioningthe training corpus arbitrarily across computationalnodes.
In this paper, we show how to reduce mem-ory footprint by instead partitioning the corpus tominimize the number of unique words on each node(and thereby minimize the number of parameters thenode must store).
Because corpus partitioning isa pre-processing step in parallel LVM training, our579technique can be applied to reduce the memory foot-print of essentially any existing LVM or training ap-proach.
The accuracy of LVM training for a fixedmodel size and corpus remains unchanged, but in-telligent corpus partitioning allows us to train largerand typically more accurate models using the samememory capacity.While the general minimization problem we en-counter is NP-hard, we develop greedy approxima-tions that work well.
In experiments with bothHMM and LDA models, we show that our techniqueoffers large advantages over existing approaches interms of both memory footprint and execution time.On a large corpus using 50 nodes in parallel, our bestpartitioning method can reduce the memory requiredper node to less than 1/10th that when training with-out corpus partitioning, and to half that of a randompartitioning.
Further, our approach reduces the train-ing time of an existing parallel HMM codebase by3x.
Our work includes the release of our partitioningcodebase, and an associated codebase for the paral-lel training of HMMs.12 Problem FormulationIn a distributed LVM system, a training corpus D ={d1, d2, .
.
.
, dN} of documents is distributed acrossT computational nodes.
We first formalize the mem-ory footprint on each node nt, where t = {1, ..., T}.Let Dt ?
D denote the document collection on nodent, and Vt be the number of word types (i.e., thenumber of unique words) in Dt.
Let K be the num-ber of latent variable values in the LVM.With these quantities, we can express how manyparameters must be held in memory on each com-putational node for training LVMs in a distributedenvironment.
In practice, the LVM parameter spaceis dominated by an observation model: a condi-tional distribution over words given the latent vari-able value.
Thus, the observation model includesK(Vt?
1) parameters.
Different LVMs include var-ious other parameters to specify the complete model.For example, a first-order HMM includes additionaldistributions for the initial latent variable and latentvariable transitions, for a total of K(Vt ?
1) + K2parameters.
LDA, on the other hand, includes just a1https://code.google.com/p/corpus-partition/single multinomial over the latent variables, makinga total of K(Vt ?
1) + K ?
1 parameters.The LVM parameters comprise almost all of thememory footprint for LVM training.
Further, as theexamples above illustrate, the number of parame-ters on each node tends to vary almost linearly withVt (in practice, Vt is typically larger than K by anorder of magnitude or more).
Thus, in this paperwe attempt to minimize memory footprint by lim-iting Vt on each computational node.
We assumethe typical case in a distributed environment wherenodes are homogeneous, and thus our goal is to par-tition the corpus such that the maximum vocabularysize Vmax = maxTt=1Vt on any single node is mini-mized.
We define this task formally as follows.Definition CORPUSPART : Given a corpus ofN documents D = {d1, d2, .
.
.
, dN}, and T nodes,partition D into T subsets D1, D2, .
.
.
, DT , suchthat Vmax is minimized.For illustration, consider the following small ex-ample.
Let corpus C contain three short docu-ments {c1=?I live in Chicago?, c2=?I am studyingphysics?, c3=?Chicago is a city in Illinois?
}, andconsider partitioning C into 2 non-empty subsets,i.e., T = 2.
There are a total of three possibilities:?
{{c1, c2}, {c3}}.
Vmax = 7?
{{c1, c3}, {c2}}.
Vmax = 8?
{{c2, c3}, {c1}}.
Vmax = 10The decision problem version ofCORPUSPART is NP-Complete, by a re-duction from independent task scheduling (Zhu andIbarra, 1999).
In this paper, we develop greedyalgorithms for the task that are effective in practice.We note that CORPUSPART has a submodu-lar problem structure, where greedy algorithms areoften effective.
Specifically, let |S| denote the vo-cabulary size of a set of documents S, and let S?
?S.
Then for any document c the following inequalityholds.|S?
?
c| ?
|S?| ?
|S ?
c| ?
|S|That is, adding a document c to the subset S?
in-creases vocabulary size at least as much as addingc to S; the vocabulary size function is submodular.The CORPUSPART task thus seeks a partitionof the data that minimizes the maximum of a set ofsubmodular functions.
While formal approximation580guarantees exist for similar problems, to our knowl-edge none apply directly in our case.
For example,(Krause et al 2007) considers maximizing the mini-mum over a set of monotonic submodular functions,which is the opposite of our problem.
The distincttask of minimizing a single submodular function hasbeen investigated in e.g.
(Iwata et al 2001).It is important to emphasize that data partition-ing is a pre-processing step, after which we can em-ploy precisely the same Expectation-Maximization(EM), sampling, or variational parameter learningtechniques as utilized in previous work.
In fact,for popular learning techniques including EM forHMMs (Rabiner, 1989) and variational EM for LDA(Wolfe et al 2008), it can be shown that the param-eter updates are independent of how the corpus ispartitioned.
Thus, for those approaches our parti-tioning is guaranteed to produce the same models asany other partitioning method; i.e., model accuracyis unchanged.Lastly, we note that we target synchronized LVMtraining, in which all nodes must finish each train-ing iteration before any node can proceed to thenext iteration.
Thus, we desire balanced partitions tohelp ensure iterations have similar durations acrossnodes.
We achieve this in practice by constrainingeach node to hold at most 3% more than Z/T to-kens, where Z is the corpus size in tokens.3 Corpus Partitioning MethodsOur high-level greedy partitioning framework isgiven in Algorithm 1.
The algorithm requires an-swering two key questions: How do we select whichdocument to allocate next?
And, given a document,on which node should it be placed?
We present al-ternative approaches to each question below.Algorithm 1 Greedy Partitioning FrameworkINPUT: {D, T}OUTPUT: {D1, .
.
.
, DT }Objective: Minimize VmaxInitialize each subset Dt = ?
for T nodesrepeatdocument selection:Select document d from Dnode selection: Select node nt, and add d to DtRemove d from Duntil all documents are allocatedA baseline partitioning method commonly usedin practice simply distributes documents acrossnodes randomly.
As our experiments show, thisbaseline approach can be improved significantly.In the following, set operations are interpreted asapplying to the set of unique words in a document.For example, |d?Dt| indicates the number of uniqueword types in node nt after document d is added toits document collection Dt.3.1 Document SelectionFor document selection, previous work (Zhu andIbarra, 1999) proposed a heuristic DISSIMILARITYmethod that selects the document d that is least sim-ilar to any of the node document collections Dt,where the similarity of d and Dt is calculated as:Sim(d,DT ) = |d ?
Dt|.
The intuition behind theheuristic is that dissimilar documents are more likelyto impact future node selection decisions.
Assigningthe dissimilar documents earlier helps ensure thatmore greedy node selections are informed by theseimpactful assignments.However, DISSIMILARITY has a prohibitive timecomplexity of O(TN2), because we must compareT nodes to an order of N documents for a total ofN iterations.
To scale to large corpora, we proposea novel BATCH DISSIMILARITY method.
In BATCHDISSIMILARITY, we select the top L most dissim-ilar documents in each iteration, instead of just themost dissimilar.
Importantly, L is altered dynami-cally: we begin with L = 1, and then increase L byone for iteration i+1 iff using a batch size of L+1 initeration i would not have altered the algorithm?s ul-timate selections (that is, if the most dissimilar doc-ument in iteration i + 1 is in fact the L + 1st mostdissimilar in iteration i).
In the ideal case where Lis incremented each iteration, BATCH DISSIMILARwill have a reduced time complexity of O(TN3/2).Our experiments revealed two key findings re-garding document selection.
First, BATCH DISSIM-ILARITY provides a memory reduction within 0.1%of that of DISSIMILARITY (on small corpora whererunning DISSIMILARITY is tractable), but partitionsan estimated 2,600 times faster on our largest eval-uation corpus.
Second, we found that document se-lection has relatively minor impact on memory foot-print, providing a roughly 5% incremental benefitover random document selection.
Thus, although581we utilize BATCH DISSIMILARITY in the final sys-tem we evaluate, simple random document selectionmay be preferable in some practical settings.3.2 Node SelectionGiven a selected document d, the MINIMUMmethod proposed in previous work selects node nthaving the minimum number of word types after al-location of d to nt (Zhu and Ibarra, 1999).
That is,MINIMUM minimizes |d ?Dt|.
Here, we introducean alternative node selection method JACCARD thatselects node nt maximizing the Jaccard index, de-fined here as |d ?Dt|/|d ?Dt|.Our experiments showed that our JACCARD nodeselection method outperforms the MINIMUM selec-tion method.
In fact, for the largest corpora usedin our experiments, JACCARD offered an 12.9%larger reduction in Vmax than MINIMUM.
Ourproposed system, referred to as BJAC, utilizesour best-performing strategies for document selec-tion (BATCH DISSIMILARITY) and node selection(JACCARD).4 Evaluation of Partitioning MethodsWe evaluate our partitioning method against thebaseline and Z&I, the best performing scalablemethod from previous work, which uses randomdocument selection and MINIMUM node selection(Zhu and Ibarra, 1999).
We evaluate on three cor-pora (Table 1): the Brown corpus of newswire text(Kucera and Francis, 1967), the Reuters Corpus Vol-ume1 (RCV1) (Lewis et al 2004), and a larger Web-Sent corpus of sentences gathered from the Web(Downey et al 2007).Corpus N V ZBrown 57339 56058 1161183RCV1 804414 288062 99702278Web-Sent 2747282 214588 58666983Table 1: Characteristics of the three corpora.
N = #of documents, V = # of word types, Z = # of tokens.We treat each sentence as a document in the Brownand Web-Sent corpora.Table 2 shows how the maximum word type sizeVmax varies for each method and corpus, for T = 50nodes.
BJAC significantly decreases Vmax over theCorpus baseline Z&I BJACBrown 6368 5714 4369RCV1 49344 32136 24923Web-Sent 72626 45989 34754Table 2: Maximum word type size Vmax for eachpartitioning method, for each corpus.
For the largercorpora, BJAC reduces Vmax by over 50% comparedto the baseline, and by 23% compared to Z&I.random partitioning baseline typically employed inpractice.
Furthermore, the advantage of BJAC overthe baseline is maintained as more computationalnodes are utilized, as illustrated in Figure 1.
BJacreduces Vmax by a larger factor over the baseline asmore computational nodes are employed.020,00040,00060,00080,000100,000120,000140,00010 20 30 40 50 60 70 80 90 100numberofword typesnumber of nodesVmax by baselineVmax by BJacFigure 1: Effects of partitioning as the number ofcomputational nodes increases (Web-Sent corpus).With 100 nodes, BJac?s Vmax is half that of the base-line, and 1/10th of the full corpus vocabulary size.5 Evaluation in Parallel LVM SystemsWe now turn to an evaluation of our corpus parti-tioning within parallel LVM training systems.Table 3 shows the memory footprint required forHMM and LDA training for three different partition-ing methods.
We compare BJAC with the randompartitioning baseline, Zhu?s method, and with all-words, the straightforward approach of simply stor-ing parameters for the entire corpus vocabulary onevery node (Ahuja and Downey, 2010; Asuncion etal., 2011).
All-words has the same memory footprintas when training on a single node.For large corpora, BJAC reduces memory sizeper node by approximately a factor of two over therandom baseline, and by a factor of 8-11 over all-582LVM Corpus all-words baseline BJACHMMBrown 435.3 56.2 40.9RCV1 2205.4 384.1 197.8Web-Sent 1644.8 561.7 269.7LDABrown 427.7 48.6 33.3RCV1 2197.7 376.5 190.1Web-Sent 1637.2 554.1 262.1Table 3: Memory footprint of computational nodesin megabytes(MB), using 50 computational nodes.Both models utilize 1000 latent variable values.words.
The results demonstrate that in addition tothe well-known savings in computation time offeredby parallel LVM training, distributed computationalso significantly reduces the memory footprint oneach node.
In fact, for the RCV1 corpus, BJAC re-duces memory footprint to less than 1/10th that oftraining with all words on each computational node.We next evaluate the execution time for an itera-tion of model training.
Here, we use a parallel im-plementation of HMMs, and measure iteration timefor training on the Web-sent corpus with 50 hiddenstates as the number of computational nodes varies.We compare against the random baseline and againstthe all-words approach utilized in an existing paral-lel HMM codebase (Ahuja and Downey, 2010).
Theresults are shown in Table 4.
Moving beyond the all-words method to exploit corpus partitioning reducestraining iteration time, by a factor of two to three.However, differences in partitioning methods haveonly small effects in iteration time: BJAC has essen-tially the same iteration time as the random baselinein this experiment.It is also important to consider the additional timerequired to execute the partitioning methods them-selves.
However, in practice this additional timeis negligible.
For example, BJAC can partition theWeb-sent corpus in 368 seconds, using a single com-putational node.
By contrast, training a 200-stateHMM on the same corpus requires over a hundredCPU-days.
Thus, BJAC?s time to partition has a neg-ligible impact on total training time.6 Related WorkThe CORPUSPART task has some similaritiesto the graph partitioning task investigated in otherT all-words baseline BJAC25 4510 1295 128950 2248 740 735100 1104 365 364200 394 196 192Table 4: Average iteration time(sec) for training anHMM with 50 hidden states on Web-Sent.
Partition-ing with BJAC outperforms all-words, which storesparameters for all word types on each node.parallelization research (Hendrickson and Kolda,2000).
However, our LVM training task differs sig-nificantly from those in which graph partitioning istypically employed.
Specifically, graph partitioningtends to be used for scientific computing applica-tions where communication is the bottleneck.
Thegraph algorithms focus on creating balanced parti-tions that minimize the cut edge weight, becauseedge weights represent communication costs to beminimized.
By contrast, in our LVM training task,memory consumption is the bottleneck and commu-nication costs are less significant.Zhu & Ibarra (1999) present theoretical resultsand propose techniques for the general partitioningtask we address.
In contrast to that work, we fo-cus on the case where the data to be partitioned is alarge corpus of text.
In this setting, we show that ourheuristics partition faster and provide smaller mem-ory footprint than those of (Zhu and Ibarra, 1999).7 ConclusionWe presented a general corpus partitioning tech-nique which can be exploited in LVM training to re-duce memory footprint and training time.
We eval-uated the partitioning method?s performance, andshowed that for large corpora, our approach reducesmemory consumption by over 50% and learns mod-els up to three times faster when compared with ex-isting implementations for parallel LVM training.AcknowledgmentsThis work was supported in part by NSF GrantsIIS-101675 and IIS-1065397, and DARPA contractD11AP00268.583ReferencesAmr Ahmed, Moahmed Aly, Joseph Gonzalez, Shra-van Narayanamurthy, and Alexander J. Smola.
2012.Scalable inference in latent variable models.
In Pro-ceedings of the fifth ACM international conference onWeb search and data mining, WSDM ?12, pages 123?132, New York, NY, USA.
ACM.Arun Ahuja and Doug Downey.
2010.
Improved extrac-tion assessment through better language models.
InHuman Language Technologies: Annual Conferenceof the North American Chapter of the Association forComputational Linguistics (NAACL HLT).Arthur U. Asuncion, Padhraic Smyth, and Max Welling.2011.
Asynchronous distributed estimation of topicmodels for document analysis.
Statistical Methodol-ogy, 8(1):3 ?
17.
Advances in Data Mining and Statis-tical Learning.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alcation.
J. Mach.
Learn.Res., 3:993?1022, March.Cheng T. Chu, Sang K. Kim, Yi A. Lin, Yuanyuan Yu,Gary R. Bradski, Andrew Y. Ng, and Kunle Olukotun.2006.
Map-Reduce for machine learning on multicore.In Bernhard Scho?lkopf, John C. Platt, and ThomasHoffman, editors, NIPS, pages 281?288.
MIT Press.Abhinandan S. Das, Mayur Datar, Ashutosh Garg, andShyam Rajaram.
2007.
Google news personaliza-tion: scalable online collaborative filtering.
In Pro-ceedings of the 16th international conference on WorldWide Web, WWW ?07, pages 271?280, New York, NY,USA.
ACM.D.
Downey, S. Schoenmackers, and O. Etzioni.
2007.Sparse information extraction: Unsupervised languagemodels to the rescue.
In Proc.
of ACL.T.
L. Griffiths and M. Steyvers.
2004.
Finding scien-tific topics.
Proceedings of the National Academy ofSciences, 101(Suppl.
1):5228?5235, April.Bruce Hendrickson and Tamara G Kolda.
2000.
Graphpartitioning models for parallel computing.
Parallelcomputing, 26(12):1519?1534.Thomas Hofmann.
1999.
Probabilistic latent semanticindexing.
In Proceedings of the 22nd annual inter-national ACM SIGIR conference on Research and de-velopment in information retrieval, SIGIR ?99, pages50?57, New York, NY, USA.
ACM.Fei Huang and Alexander Yates.
2009.
Distributionalrepresentations for handling sparsity in supervised se-quence labeling.
In Proceedings of the Annual Meet-ing of the Association for Computational Linguistics(ACL).Fei Huang and Alexander Yates.
2010.
Exploringrepresentation-learning approaches to domain adapta-tion.
In Proceedings of the ACL 2010 Workshop onDomain Adaptation for Natural Language Processing(DANLP).Satoru Iwata, Lisa Fleischer, and Satoru Fujishige.
2001.A combinatorial strongly polynomial algorithm forminimizing submodular functions.
J. ACM, 48:761?777.Andreas Krause, H. Brendan Mcmahan, Google Inc, Car-los Guestrin, and Anupam Gupta.
2007.
Selectingobservations against adversarial objectives.
Technicalreport, In NIPS, 2007a.H.
Kucera and W. N. Francis.
1967.
Computationalanalysis of present-day American English.
BrownUniversity Press, Providence, RI.David D. Lewis, Yiming Yang, Tony G. Rose, Fan Li,G.
Dietterich, and Fan Li.
2004.
Rcv1: A new bench-mark collection for text categorization research.
Jour-nal of Machine Learning Research, 5:361?397.Wei Li and Andrew McCallum.
2005.
Semi-supervisedsequence modeling with syntactic topic models.
InProceedings of the 20th national conference on Artifi-cial intelligence - Volume 2, AAAI?05, pages 813?818.AAAI Press.David Newman, Arthur Asuncion, Padhraic Smyth, andMax Welling.
2009.
Distributed algorithms fortopic models.
Journal of Machine Learning Research,10:1801?1828.L.
R. Rabiner.
1989.
A tutorial on Hidden MarkovModels and selected applications in speech recogni-tion.
Proceedings of the IEEE, 77(2):257?286.Alan Ritter, Colin Cherry, and Bill Dolan.
2010.
Unsu-pervised modeling of twitter conversations.
In HumanLanguage Technologies: The 2010 Annual Conferenceof the North American Chapter of the Association forComputational Linguistics, HLT ?10, pages 172?180,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Jason Wolfe, Aria Haghighi, and Dan Klein.
2008.
Fullydistributed EM for very large datasets.
In Proceed-ings of the 25th international conference on Machinelearning, ICML ?08, pages 1184?1191, New York,NY, USA.
ACM.Huican Zhu and Oscar H. Ibarra.
1999.
On some ap-proximation algorithms for the set partition problem.In Proceedings of the 15th Triennial Conf.
of Int.
Fed-eration of Operations Research Society.584
