InfoXtract location normalization: a hybrid approach to geographicreferences in information extraction ?Huifeng Li, Rohini K. Srihari, Cheng Niu, and Wei LiCymfony Inc.600 Essjay Road, Williamsville, NY 14221, USA(hli, rohini, cniu, wei)@cymfony.com?
This work was partly supported by a grant from the Air Force Research Laboratory?s Information Directorate (AFRL/IF), Rome,NY, under contract F30602-01-C-0035.
The authors wish to thank Carrie Pine of AFRL for supporting and commenting this work.AbstractAmbiguity is very high for location names.
Forexample, there are 23 cities named ?Buffalo?
in theU.S.
Based on our previous work, this paper presentsa refined hybrid approach to geographic referencesusing our information extraction engine InfoXtract.The InfoXtract location normalization moduleconsists of local pattern matching and discourseco-occurrence analysis as well as default senses.Multiple knowledge sources are used in a number ofways: (i) pattern matching driven by local context,(ii) maximum spanning tree search for discourseanalysis, and (iii) applying default sense heuristicsand extracting default senses from the web.
Theresults are benchmarked with 96% accuracy on ourtest collections that consist of both news articles andtourist guides.
The performance contribution for eachcomponent of the module is also benchmarked anddiscussed.1 IntroductionThe task of location normalization is to decodegeographic references for extracted location  NamedEntities (NE).
Ambiguity is a very serious problem forlocation NEs.
For example, there are 23 cities named?Buffalo?, including the city in New York State and inthe state of Alabama.
Country names such as?Canada?, ?Brazil?, and ?China?
are also city names inthe USA.
Such ambiguity needs to be properlyhandled before converting location names into normalform to support Entity Profile (EP) construction,information merging/consolidation as well asvisualization of location-stamped extracted events ona map.Location normalization is a special application ofword sense disambiguation (WSD).
There isconsiderable research on WSD.
Knowledge-basedwork, such as [Hirst 1987; McRoy 1992; Ng andLee 1996] used hand-coded rules or supervisedmachine learning based on an annotated corpus toperform WSD.
Recent work emphasizes acorpus-based unsupervised approach [Dagon andItai 1994; Yarowsky 1992; Yarowsky 1995] thatavoids the need for costly truthed training data.Location normalization is different from generalWSD in that the selection restriction often used forWSD in many cases is not sufficient to distinguishthe correct sense from the other candidates.
Forexample, in the sentence ?The White House islocated in Washington?, the selection restrictionfrom the collocation ?located in?
can onlydetermine that ?Washington?
should be a locationname, but is not sufficient to decide the actual senseof this location.In terms of local context, we found that there arecertain fairly predictable keyword-driven patternswhich can decide the senses of location NEs.
Thesepatterns use keywords such as ?city?, ?town?,?province?, ?on?, ?in?
or candidate location subtypesthat can be assigned from a location gazetteer.
Forexample, the pattern ?X + city?
can determine sensetags for cases like ?New York City?
; and the pattern?Candidate-city-name + comma +Candidate-state-name?
can disambiguate casessuch as ?Albany, New York?
and ?Shanghai,Illinois?.In the absence of these patterns, co-occurringlocation NEs in the same discourse provideevidence for predicting the most probable sense of alocation name.
More specifically, locationnormalization depends on co-occurrenceconstraints of geographically related location entitiesmentioned in the same document.
For example, if?Buffalo?, ?Albany?
and ?Rochester?
are mentioned inthe same  document, the most probable senses of?Buffalo?, ?Albany?
and ?Rochester?
should refer tothe cities in New York State.For choosing the best matching sense set within adocument, we simply construct a graph where eachnode represents a sense of a location NE, and eachedge represents the relationship between two locationname senses.
A graph  spanning algorithm can be usedto select the best senses from the graph.Last but not least, proper assignment of defaultsenses is found to play a significant role in theperformance of a location normalizer.
This involvestwo issues: (i) determining default senses usingheuristics and/or other methods, such as statisticalprocessing for semi-automatic default sense extractionfrom the web [Li et al 2002]; and (ii) setting theconditions/thresholds and the proper levels whenassigning default senses, to coordinate with local anddiscourse evidence for enhanced performance.
Thesecond issue can be resolved through experimentation.In the light of the above overview, this paperpresents an effective hybrid location normalizationapproach which consists of local pattern matching anddiscourse co-occurrence analysis as well as defaultsenses.
Multiple knowledge sources are used in anumber of ways: (i) pattern matching driven by localcontext, (ii) maximum spanning tree search fordiscourse analysis, and (iii) applying heuristics-baseddefault senses and web-extracted default senses inproper stages.In the remaining text, Section 2 introduces thebackground for this research.
Section 3 describes ourprevious work in this area and Section 4 presents themodified algorithm to address the issues with theprevious method.
Experiment and benchmarks aredescribed in Section 5.
Section 6 is the conclusion.2 BackgroundThe design and implementation of the locationnormalization module is an integrated part ofCymfony?s core information extraction (IE) engineInfoXtract.
InfoXtract extracts and normalizes entities,relationships and events from natural language text.Figure 1 shows the overall system architecture ofInfoXtract, involving multiple modules in a pipelinestructure.InfoXtract involves a spectrum of linguisticprocessing and relationship/event extraction.
Thisengine, in its current state, involves over 100 levels ofprocessing and 12 major components.
Somecomponents are based on hand-crafted patternmatching rules, some are statistical models orprocedures, and others are hybrid (e.g.
NE,Co-reference, Location Normalization).
The basicinformation extraction task is NE tagging [Krupkaand Hausman 1998; Srihari et al 2000].
The NEtagger identifies and classifies proper names of typePERSON, ORGANIZATION, PRODUCT,NAMED-EVENTS, LOCATION (LOC) as well asnumerical expressions such as MEASUREMENT(e.g.
MONEY, LENGTH, WEIGHT, etc) and timeexpressions (TIME, DATE, MONTH, etc.
).Parallel to location normalization, InfoXtract alsoinvolves time normalization and measurementnormalization.Document ProcessorKnowledge ResourcesLexiconResourcesGrammarsProcessManagerTokenlistLegendOutputManagerSourceDocumentLinguistic Processor(s)TokenizerTokenlistLexicon LookupPOS TaggingNE TaggingShallowParsingRelationshipExtractionDocumentpoolNECEEPSVOTimeNormalizationProfile/EventConsolidationEventExtractionAbbreviationsNE = Named EntityCE = Correlated EntityEP = Entity ProfileSVO = Subject-Verb-ObjectGE = General EventPE = Predefined EventRule-basedPattern MatchingProcedure orStatistical ModelHybridModuleGEStatisticalModelsPEIERepositoryDeep ParsingCoreferenceLocationNormalizationMeasurementNormalizationFigure 1:  System Architecture of InfoXtractInfoXtract combines the Maximum EntropyModel (MaxEnt) and Hidden Markov Model forNE tagging [Srihari et al 2000].
MaximumEntropy Models incorporate local contextualevidence to handle ambiguity of information from alocation gazetteer.
In the Tipster LocationGazetteer used by InfoXtract, there are manycommon words, such as I, A, June, Friendship, etc.Also, there is large overlap between person namesand location names, such as Clinton, Jordan, etc.Using MaxEnt, systems learn under what situationa word is a location name, but it is very difficult todetermine the correct sense of an ambiguouslocation name.
The NE tagger in InfoXtract onlyassigns the location super-type tag LOC to theidentified location words and leaves the task oflocation sub-type tagging such as CITY or STATEand its disambiguation to the subsequent moduleLocation Normalization.Beyond NE, the major information objectsextracted by InfoXtract are Correlated Entity (CE)relationships (e.g.
AFFILIATION and POSITION),Entity Profile (EP) that is a collection of extractedentity-centric information, Subject-Verb-Object(SVO) which refers to dependency links betweenlogical subject/object and its verb governor, GeneralEvent (GE) on who did what when and where andPredefined Event (PE) such as ManagementSuccession and Company Acquisition.It is believed that these information objects capturethe key content of the processed text.
Whennormalized location, time and measurement NEs areassociated with information objects (events, inparticular) based on parsing, co-reference and/ordiscourse propagation, these events are stamped.
Theprocessing results are stored in IE Repository, adynamic knowledge warehouse used to supportcross-document consolidation, text mining for hiddenpatterns and IE applications.
For example,location-stamped events can support informationvisualization on maps (Figure 2); time-stampedinformation objects can support visualization along atimeline; measurement-stamped objects will allowadvanced retrieval such as find all CompanyAcquisition events that involve money amount greaterthan 2 million US dollars.Event type: <Die: Event 200>Who:       <Julian Werver Hill: PersonProfile 001>When:     1996-01-07Where:    <LocationProfile103>Preceding_event: <hospitalize: Event 260>Subsequent_event: <bury: Event 250>Event Visualization;  ;; ;Predicate: DieWho: Julian Werner HillWhen:Where: <LocationProfile 103>Hockessin, Delaware, USA,19707,75.688873,39.776041996-01-07Figure 2:  Location-stamped InformationVisualization3 Previous Work and IssuesThis paper is follow-up research based on our previouswork [Li et al 2002].
Some efficiency andperformance issues are identified and addressed by themodified approach.The previous algorithm [Li et al 2002] for locationnormalization consisted of five steps.Step 1.
Look up location names in thegazetteer to associate candidate senses foreach location NE;Step 2.
Call the pattern matching sub-moduleto resolve the ambiguity of the NEs involvedin local patterns like ?Williamsville, NewYork, USA?
to retain only one sense for theNE as early as possible;Step 3.
Apply the ?one sense per discourse?principle [Gale et al1992] for eachdisambiguated location name to propagatethe selected sense to its other mentionswithin a document;Step 4.
Call the discourse sub-module,which is a graph search algorithm(Kruskal?s algorithm), to resolve theremaining ambiguities;Step 5.
If the decision score for a locationname is lower than a threshold, we choose adefault sense of that name as a result.In this algorithm, Step 2, Step 4, and Step 5complement each other, and help produce betteroverall performance.Step 2 uses local context that is the co-occurringwords around a location name.
Local context can bea reliable source in deciding the sense of a location.The following are the most commonly usedpatterns for this purpose.
(1) LOC + ?,?
+ NP (headed by ?city?)e.g.
Chicago, an old city(2) ?city of?
+ LOC1 + ?,?
+ LOC2e.g.
city of Albany, New York(3) ?city of?
+ LOC(4) ?state of?
+ LOC(5) LOC1+ ?,?
+ LOC2 + ?,?
+ LOC3e.g.
(i) Williamsville, New York, USA(ii) New York, Buffalo, USA(6) ?on?/ ?in?
+ LOCe.g.
on Strawberry  ISLANDin Key West  CITYPatterns (1) , (3), (4) and (6) can be used to decide ifthe location is a city, a state or an island, whilepatterns (2) and (5) can be used to determine boththe sub-tag and its sense.Step 4 constructs a weighted graph where eachnode represents a location sense, and each edgerepresents similarity weight between locationnames.
The graph is partially complete since thereare no links among the different senses of a locationname.
The maximum weight spanning tree (MST)is calculated using Kruskal?s MinST algorithm[Cormen et al 1990].
The nodes on the resultingMST are the most promising senses of the locationnames.Figure 3 and Figure 4 show the graphs forcalculating MST.
Dots in a circle mean the numberof senses of a location name.Through experiments, we found an efficiencyproblem in Step 4 which adopted Kruskal?salgorithm for MST search to capture the impact oflocation co-occurrence in a discourse.
While thisalgorithm works fairly well for short documents (e.g.most news articles), there is a serious time complexityissue when numerous location names are contained inlong documents.
A weighted graph is constructed bylinking sense nodes for each location with the sensenodes for other locations.
In addition, there is also anassociated performance issue: the value weighting forthe calculated edges using the previous method is notdistinctive enough.
We observe that the number oflocation mentions and the distance between thelocation names impact the selection of location senses,but the previous method could not reflect these factorsin distinguishing the weights of candidate senses.Canada{Kansas,Kentucky,Country}Vancouver{British ColumbiaWashingtonport in USAPort in Canada}New York{Prov in USA,New York City,?
}Toronto(Ontorio,New South Wales,Illinois,?
}Charlottetown{Prov in USA,New York City,?
}Prince Edward Island{Island in Canada,Island in South Africa,Province in Canada}Quebec(city in Quebec,Quebec Prov,Connecticut,?
}3*4 lines2*3 lines4*11 lines11*10 lines3*10 lines8*3 lines2*8 lines2*43*113*103*33*82*102*38*48*118*108*410*43*11Figure 3:  Graph and its Spanning TreeCanada{Kansas,Kentucky,Country}Vancouver{British ColumbiaWashingtonport in USAPort in Canada}New York{Prov in USA,New York City,?
}Toronto(Ontorio,New South Wales,Illinois,?
}Charlottetown{city in New YorkPort in canada}Prince Edward Island{Island in Canada,Island in South Africa,Province in Canada}Quebec(city in Quebec,Prov in Canada,Connecticut,?
}3.63.63.663.63.601234567Figure 4:  Max Spanning TreeFinally, our research shows that default senses playa significant role in location normalization.
Forexample, people refer to ?Los Angeles?
as the city inCalifornia more than the city in the Philippines, Chile,Puerto Rico, or the city in Texas in the USA.Unfortunately, the available Tipster Gazetteer(http://crl.nmsu.edu/cgi-bin/Tools/CLR/clrcat) doesnot mark default senses for most entries.
It has171,039 location entries with 237,916  senses, amongwhich 30,711 location names are ambiguous.Manually tagging the default senses for over 30,000location names is difficult; moreover, it is also subjectto inconsistency due to the different knowledgebackgrounds of the human taggers.
This problemwas solved by developing a procedure toautomatically extract default senses from webpages using the Yahoo!
search engine [Li et al2002].
Such a procedure has the advantage ofenabling ?re-training?
of default senses whennecessary.
If the web pages obtained through Yahoo!represent a typical North American ?view?
of whatdefault sense should be assigned to location names,it may be desirable to re-train the default senses oflocation names  using other views (e.g.
an Asianview or African view) when the system needs tohandle overseas documents that contain manyforeign location names.In addition to the above automatic default senseextraction, we later found that a few simple defaultsense heuristics, when used at proper levels, canfurther enhance performance.
This finding isincorporated in our modified approach described inSection 3 below.4 Modified Hybrid ApproachTo address the issues identified in Section 2, weadopt Prim?s algorithm, which traverses each nodeof a graph to choose the most promising senses.This algorithm has much less search space andshows the advantage of being able to reflect thenumber of location mentions and their distances ina document.The following is the description of our adaptedPrim?s algorithm for the weight calculation.The weight of each sense of a node is calculatedby considering the effect of linked senses of otherlocation nodes based on a predefined weight table(Table 1) for the sense categories of co-occurringlocation names.
For example, when a location namewith a potential city sense co-occurs with a locationname with a potential state/province sense and thecity is in the state/province, the impact weight ofthe state/province name on the city name is fairlyhigh, with the weight set to 3 as shown in the 3rdrow of Table 1.Table 1.
Impact weight of Sense2 on Sense1Sense1 Sense2 Condition  WeightCity City in same state 2City in same country 1State in same state 3Country in country withoutstate (e.g.
in Europe)4Let W(Si) be the calculated weight of a sense Sj ofa location; weight(Sj->Si) means the weight of Siinfluenced by sense Sj; Num(Loci) is the number oflocation mentions; and ?/dist(Loci, Locj) is themeasure of distance between two locations.
The finalsense of a location is the one that has maximumweight.
A location name may be mentioned a numberof times in a document.
For each location name, weonly count the location mention that has the maximumsense weight summation in equation (1) andeventually propagate the selected sense of thislocation mention to all its other mentions based on onesense per discourse principle.
Equation (2) refers tothe sense with the maximum weight for Loci.
(1)( )=?=mjjijijiLocLocdistLocNumSSweightSW0),(/*)(*)()(?
(2) ))(()( maxarg jji SWLocS =wj ?
?0Through experiments, we also found that it isbeneficial to select default senses when candidatelocation senses in the discourse analysis turn out to beof the same weight.
We included two kinds of defaultsenses: heuristics-based default senses and the defaultsenses extracted semi-automatically from the webusing Yahoo.
For the first category of default senses,we observe that if a name has a country sense andother senses, such as ?China?
and ?Canada?, thecountry senses are dominant in most cases.
Thesituation is the same for a name with province senseand for a name with country capital sense (e.g.
London,Beijing).
The updated algorithm for locationnormalization is as follows.Step 1.
Look up the location gazetteer toassociate candidate senses for each locationNE;Step 2.
If a location has sense of country, thenselect that sense as the default sense of thatlocation (heuristics);Step 3.
Call the pattern matching sub-modulefor local patterns like ?Williamsville, NewYork, USA?
;Step 4.
Apply the ?one sense per discourse?principle for each disambiguated locationname to propagate the selected sense to itsother mentions within a document;Step 5.
Apply default sense heuristics for alocation with province or capital senses;Step 6.
Call Prim?s algorithm in thediscourse sub-module to resolve theremaining ambiguities (Figure 5);Step 7.
If the difference between the sensewith the maximum weight and the sensewith next largest weight is equal to or lowerthan a threshold, choose the default sense ofthat name from lexicon.
Otherwise, choosethe sense with the maximum weight asoutput.Canada{Kansas,Kentucky,Country}Vancouver{British ColumbiaWashingtonport in USAPort in Canada}New York{Prov in USA,New York City,?
}Toronto(Ontorio,New South Wales,Illinois,?
}Charlottetown{Prov in USA,New York City,?
}Prince Edward Island{Island in Canada,Island in South Africa,Province in Canada}Quebec(city in Quebec,Quebec Prov,Connecticut,?
}Figure 5:  Weight assigned to Sense Nodes5 Experiment and BenchmarkWith the information from local context, discoursecontext and the knowledge of default senses, thelocation normalization process is  efficient andprecise.The testing documents were randomly selectedfrom CNN news and from travel guide web pages.Table 2 shows the preliminary testing results usingdifferent configurations.As shown, local patterns (Column 4) alonecontribute 12% to the overall performance whileproper use of defaults senses and the heuristics(Column 5) can achieve close to 90%.
In terms ofdiscourse co-occurrence evidence, the new methodusing Prim?s algorithm (Column 7) is clearly betterthan the previous method using Kruskal?salgorithm (Column 6), with 13% enhancement(from 73.8% to 86.6%).
But both methods cannotoutperform default senses.
Finally, when using allthree types of evidence, the new hybrid methodpresented in this paper shows significantperformance enhancement (96% in Column 9) overthe previous method (81.9% in Column 8), inaddition to a satisfactory solution to the efficiencyproblem.Table 2.
Experimental evaluation for location normalizationFile # ofambiguouslocationnames# ofmentionsPatternhitsDef-sensesKruskalAlgo.onlyPrimAlgoonlyKruskal+Pattern+Def(previous)Prim+Pattern+Def(new)Cnn1 26 39 4 20 21 24 26  26Cnn2 12 20 5 11 7 10 11 11Cnn3 14 29 0 12 10 12 10 14Cnn4 8 14 2 8 4 4 4 8Cnn5 11 26 1 9 5 8 5 9Cnn6 19 35 6 16 11 16 13 18Cnn7 11 27 0 11 4 7 6 10Calif.
16 30 0 16 16 16 16 16Florida 19 28 0 19 19 19 18 19Texas 13 13 0 12 13 13 13 12Total 149 261 12% 89.9% 73.8% 86.6% 81.9% 96%We observed that if a file contains moreconcentrated locations, such as the state introductionsin the travel guides for California, Florida and Texas,the accuracy is higher than the relatively short newsarticles from CNN.6 Conclusion and Future WorkThis paper presented an effective hybrid method oflocation normalization for information extraction withpromising experimental results.
In the future, we willintegrate an expanded location gazetteer includingnames of landmarks, mountains and lakes suchas Holland Tunnel (in New York, not in Holland) andHoover Dam (in Arizona, not in Alabama), to enlargethe system coverage.
Meanwhile, more extensivebenchmarking is currently being planned in order toconduct a detailed analysis of different evidencesources and their interaction and contribution tosystem performance.ReferencesCormen, Thomas H., Charles E. Leiserson, andRonald L. Rivest.
1990.
Introduction to Algorithm.The MIT Press, 504-505.Dagon, Ido and Alon Itai.
1994.
Word SenseDisambiguation Using a Second LanguageMonolingual Corpus.
Computational Linguistics,Vol.20, 563-596.Gale, W.A., K.W.
Church, and D. Yarowsky.
1992.One Sense Per Discourse.
Proceedings of the 4thDARPA Speech and Natural Language Workshop.233-237.Hirst, Graeme.
1987.
Semantic Interpretation and theResolution of Ambiguity.
Cambridge UniversityPress, Cambridge.Huifeng Li, Rohini K. Srihari, Cheng Niu, Wei Li.2002.
Location Normalization for InformationExtraction, COLING 2002, Taipei, Taiwan.Krupka, G.R.
and K. Hausman.
1998.
IsoQuestInc.
: Description of the NetOwl (TM) ExtractorSystem as Used for MUC-7.
Proceedings ofMUC.McRoy, Susan W. 1992.
Using MultipleKnowledge Sources for Word SenseDiscrimination.
Computational Linguistics,18(1): 1-30.Ng, Hwee Tou and Hian Beng Lee.
1996.Integrating Multiple Knowledge Sources toDisambiguate Word Sense: an Exemplar-basedApproach.
ACL 1996, 40-47, California.Srihari, Rohini, Cheng Niu, and Wei Li.
2000.
AHybrid Approach for Named Entity andSub-Type Tagging.
ANLP 2000, Seattle.Yarowsky, David.
1992.
Word-senseDisambiguation Using Statistical Models ofRoget?s Categories Trained on Large Corpora.COLING 1992, 454-460, Nantes, France.Yarowsky, David.
1995.
Unsupervised Word SenseDisambiguation Rivaling Supervised Methods.ACL 1995, Cambridge, Massachusetts.
