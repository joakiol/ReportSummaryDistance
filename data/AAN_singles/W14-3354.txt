Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 414?419,Baltimore, Maryland USA, June 26?27, 2014.c?2014 Association for Computational LinguisticsBEER: BEtter Evaluation as RankingMilo?s Stanojevi?cILLCUniversity of Amsterdammstanojevic@uva.nlKhalil Sima?anILLCUniversity of Amsterdamk.simaan@uva.nlAbstractWe present the UvA-ILLC submission ofthe BEER metric to WMT 14 metrics task.BEER is a sentence level metric that canincorporate a large number of featurescombined in a linear model.
Novel con-tributions are (1) efficient tuning of a largenumber of features for maximizing corre-lation with human system ranking, and (2)novel features that give smoother sentencelevel scores.1 IntroductionThe quality of sentence level (also called segmentlevel) evaluation metrics in machine translation isoften considered inferior to the quality of corpus(or system) level metrics.
Yet, a sentence levelmetrics has important advantages as it:1. provides an informative score to individualtranslations2.
is assumed by MT tuning algorithms (Hop-kins and May, 2011).3. facilitates easier statistical testing using signtest or t-test (Collins et al., 2005)We think that the root cause for most of the diffi-culty in creating a good sentence level metric is thesparseness of the features often used.
Consider then-gram counting metrics (BLEU (Papineni et al.,2002)): counts of higher order n-grams are usu-ally rather small, if not zero, when counted at theindividual sentence level.
Metrics based on suchcounts are brittle at the sentence level even whenthey might be good at the corpus level.
Ideally weshould have features of varying granularity that wecan optimize on the actual evaluation task: relativeranking of system outputs.Therefore, in this paper we explore two kinds ofless sparse features:Character n-grams are features at the sub-wordlevel that provide evidence for translation ad-equacy - for example whether the stem is cor-rectly translated,Abstract ordering patterns found in tree factor-izations of permutations into PermutationTrees (PETs) (Zhang and Gildea, 2007), in-cluding non-lexical alignment patterns.The BEER metric combines features of both kinds(presented in Section 2).With the growing number of adequacy and or-dering features we need a model that facilitates ef-ficient training.
We would like to train for opti-mal Kendall ?
correlation with rankings by humanevaluators.
The models in the literature tackle thisproblem by1.
training for another similar objective ?
e.g.,tuning for absolute adequacy and fluencyscores instead on rankings, or2.
training for rankings directly but with meta-heuristic approaches like hill-climbing, or3.
training for pairwise rankings using learning-to-rank techniquesApproach (1) has two disadvantages.
One is theinconsistency between the training and the testingobjectives.
The other, is that absolute rankings arenot reliable enough because humans are better atgiving relative than absolute judgments (see WMTmanual evaluations (Callison-Burch et al., 2007)).Approach (2) does not allow integrating a largenumber of features which makes it less attractive.Approach (3) allows integration of a large num-ber of features whose weights could be determinedin an elegant machine learning framework.
Theoutput of learning in this approach can be either afunction that ranks all hypotheses directly (globalranking model) or a function that assigns a score414to each hypothesis individually which can be usedfor ranking (local ranking model) (Li, 2011).
Lo-cal ranking models are preferable because theyprovide absolute distance between hypotheses likemost existing evaluation metrics.In this paper we follow the learning-to-rank ap-proach which produces a local ranking model in asimilar way to PRO MT systems tuning (Hopkinsand May, 2011).2 ModelOur model is a fairly simple linear interpolation offeature functions, which is easy to train and simpleto interpret.
The model determines the similarityof the hypothesis h to the reference translation rby assigning a weight wito each feature ?i(h, r).The linear scoring function is given by:score(h, r) =?iwi?
?i(h, r) = ~w ?~?2.1 Adequacy featuresThe features used are precision P , recall R andF1-score F for different counts:Pfunction, Rfunction, Ffunctionon matched func-tion wordsPcontent, Rcontent, Fcontenton matched contentwords (all non-function words)Pall, Rall, Fallon matched words of any typePchar n?gram, Rchar n?gram, Fchar n?grammatching of the character n-gramsBy differentiating function and non-functionwords we might have a better estimate of whichwords are more important and which are less.
Thelast, but as we will see later the most important,adequacy feature is matching character n-grams,originally proposed in (Yang et al., 2013).
Thiscan reward some translations even if they did notget the morphology completely right.
Many met-rics solve this problem by using stemmers, but us-ing features based on character n-grams is morerobust since it does not depend on the qualityof the stemmer.
For character level n-grams wecan afford higher-order n-grams with less risk ofsparse counts as on word n-grams.
In our exper-iments we used character n-grams for size up to6 which makes the total number of all adequacyfeatures 27.2.2 Ordering featuresTo evaluate word order we follow (Isozaki et al.,2010; Birch and Osborne, 2010) in representingreordering as a permutation and then measuringthe distance to the ideal monotone permutation.Here we take one feature from previous work ?Kendall ?
distance from the monotone permuta-tion.
This metrics on the permutation level hasbeen shown to have high correlation with humanjudgment on language pairs with very differentword order.Additionally, we add novel features with aneven less sparse view of word order by exploitinghierarchical structure that exists in permutations(Zhang and Gildea, 2007).
The trees that representthis structure are called PETs (PErmutation Trees?
see the next subsection).
Metrics defined overPETs usually have a better estimate of long dis-tance reorderings (Stanojevi?c and Sima?an, 2013).Here we use simple versions of these metrics:?countthe ratio between the number of differentpermutation trees (PETs) (Zhang and Gildea,2007) that could be built for the given per-mutation over the number of trees that couldbe built if permutation was completely mono-tone (there is a perfect word order).?
[ ]ratio of the number of monotone nodes ina PET to the maximum possible number ofnodes ?
the lenght of the sentence n.?<>ratio of the number of inverted nodes to n?=4ratio of the number of nodes with branchingfactor 4 to n?>4ratio of the number of nodes with branchingfactor bigger than 4 to n2.3 Why features based on PETs?PETs are recursive factorizations of permutationsinto their minimal units.
We refer the reader to(Zhang and Gildea, 2007) for formal treatment ofPETs and efficient algorithms for their construc-tion.
Here we present them informally to exploitthem for presenting novel ordering metrics.A PET is a tree structure with the nodes deco-rated with operators (like in ITG) that are them-selves permutations that cannot be factorized anyfurther into contiguous sub-parts (called opera-tors).
As an example, see the PET in Figure 1a.This PET has one 4-branching node, one inverted415?2, 4, 1, 3?2 ?2, 1?
?1, 2?5 641 3(a) Complex PET?1, 2?
?2, 1?2 1?2, 1?4 3(b) PET with inversions?2, 1?
?2, 1?
?2, 1?4 321(c) Fully inverted PETFigure 1: Examples of PETsnode and one monotone.
The nodes are decoratedby operators that stand for a permutation of thedirect children of the node.PETs have two important properties that makethem attractive for observing ordering: firstly, thePET operators show the minimal units of orderingthat constitute the permutation itself, and secondlythe higher level operators capture hidden patternsof ordering that cannot be observed without fac-torization.
Statistics over patterns of ordering us-ing PETs are non-lexical and hence far less sparsethan word or character n-gram statistics.In PETs, the minimal operators on the nodestand for ordering that cannot be broken down anyfurther.
The binary monotone operator is the sim-plest, binary inverted is the second in line, fol-lowed by operators of length four like ?2, 4, 1, 3?
(Wu, 1997), and then operators longer than four.The larger the branching factor under a PET node(the length of the operator on that node) the morecomplex the ordering.
Hence, we devise possi-ble branching feature functions over the operatorlength for the nodes in PETs:?
factor 2 - with two features: ?
[ ]and ?<>(there are no nodes with factor 3 (Wu, 1997))?
factor 4 - feature ?=4?
factor bigger than 4 - feature ?>4All of the mentioned PETs node features, except?
[ ]and ?count, signify the wrong word order butof different magnitude.
Ideally all nodes in a PETwould be binary monotone, but when that is notthe case we are able to quantify how far we arefrom that ideal binary monotone PET.In contrast with word n-grams used in othermetrics, counts over PET operators are far lesssparse on the sentence level and could be morereliable.
Consider permutations 2143 and 4321and their corresponding PETs in Figure 1b and1c.
None of them has any exact n-gram matched(we ignore unigrams now).
But, it is clear that2143 is somewhat better since it has at least somewords in more or less the right order.
These ?ab-stract n-grams?
pertaining to correct ordering offull phrases could be counted using ?
[ ]whichwould recognize that on top of the PET in 1b thereis the monotone node unlike the PET in 1c whichhas no monotone nodes at all.3 Tuning for human judgmentThe task of correlation with human judgment onthe sentence level is usually posed in the followingway (Mach?a?cek and Bojar, 2013):?
Translate all source sentences using the avail-able machine translation systems?
Let human evaluators rank them by qualitycompared to the reference translation?
Each evaluation metric should do the sametask of ranking the hypothesis translations?
The metric with higher Kendall ?
correlationwith human judgment is considered betterLet us take any pair of hypotheses that have thesame reference r where one is better (hgood) thanthe other one (hbad) as judged by human evaluator.In order for our metric to give the same ranking ashuman judges do, it needs to give the higher scoreto the hgoodhypothesis.
Given that our model islinear we can derive:score(hgood, r) > score(hbad, r)?~w ?~?good> ~w ?~?bad?~w ?~?good?
~w ?~?bad> 0?~w ?
(~?good?~?bad) > 0~w ?
(~?bad?~?good) < 0The most important part here are the last twoequations.
Using them we formulate ranking prob-lem as a problem of binary classification: the pos-itive training instance would have feature values416~?good?~?badand the negative training instancewould have feature values~?bad?~?good.
This trickwas used in PRO (Hopkins and May, 2011) but forthe different task:?
tuning the model of the SMT system?
objective function was an evaluation metricGiven this formulation of the training instanceswe can train the classifier using pairs of hypothe-ses.
Note that even though it uses pairs of hypothe-ses for training in the evaluation time it uses onlyone hypothesis ?
it does not require the pair of hy-potheses to compare them.
The score of the classi-fier is interpreted as confidence that the hypothesisis a good translation.
This differs from the major-ity of earlier work which we explain in Section 6.4 Experiments on WMT12 dataWe conducted experiments for the metric whichin total has 33 features (27 for adequacy and 6for word order).
Some of the features in themetric depend on external sources of informa-tion.
For function words we use listings that arecreated for many languages and are distributedwith METEOR toolkit (Denkowski and Lavie,2011).
The permutations are extracted using ME-TEOR aligner which does fuzzy matching usingresources such as WordNet, paraphrase tables andstemmers.
METEOR is not used for any scoring,but only for aligning hypothesis and reference.For training we used the data from WMT13 hu-man evaluation of the systems (Mach?a?cek and Bo-jar, 2013).
Before evaluation, all data was low-ercased and tokenized.
After preprocessing, weextract training examples for our binary classifier.The number of non-tied human judgments per lan-guage pair are shown in Table 1.
Each humanjudgment produces two training instances : onepositive and one negative.
For learning we useregression implementation in the Vowpal Wabbittoolkit1.Tuned metric is tested on the human evaluateddata from WMT12 (Callison-Burch et al., 2012)for correlation with the human judgment.
As base-line we used one of the best ranked metrics on thesentence level evaluations from previous WMTtasks ?
METEOR (Denkowski and Lavie, 2011).The results are presented in the Table 2.
The pre-sented results are computed using definition of1https://github.com/JohnLangford/vowpal_wabbitlanguage pair #comparisonscs-en 85469de-en 128668es-en 67832fr-en 80741ru-en 151422en-cs 102842en-de 77286en-es 60464en-fr 100783en-ru 87323Table 1: Number of human judgments in WMT13languagepairBEERwithparaphrasesBEERwithoutparaphrasesMETEORen-cs 0.194 0.190 0.152en-fr 0.257 0.250 0.262en-de 0.228 0.217 0.180en-es 0.227 0.235 0.201cs-en 0.215 0.213 0.205fr-en 0.270 0.254 0.249de-en 0.290 0.271 0.273es-en 0.267 0.249 0.247Table 2: Kendall ?
correleation on WMT12 dataKendall ?
from the WMT12 (Callison-Burch etal., 2012) so the scores could be compared withother metrics on the same dataset that were re-ported in the proceedings of that year (Callison-Burch et al., 2012).The results show that BEER with and withoutparaphrase support outperforms METEOR (andalmost all other metrics on WMT12 metrics task)on the majority of language pairs.
Paraphrase sup-port matters mostly when the target language isEnglish, but even in language pairs where it doesnot help significantly it can be useful.5 WMT14 evaluation task resultsIn Table 4 and Table 3 you can see the results oftop 5 ranked metrics on the segment level evalua-tion task of WMT14.
In 5 out of 10 language pairsBEER was ranked the first, on 4 the second bestand on one third best metric.
The cases where itfailed to win the first place are:?
against DISCOTK-PARTY-TUNED on * - En-glish except Hindi-English.
DISCOTK-PARTY-TUNED participated only in evalua-tion of English which suggests that it usessome language specific components which isnot the case with the current version of BEER?
against METEOR and AMBER on English-Hindi.
The reason for this is simply that we417Direction en-fr en-de en-hi en-cs en-ruBEER .295 .258 .250 .344 .440METEOR .278 .233 .264 .318 .427AMBER .261 .224 .286 .302 .397BLEU-NRC .257 .193 .234 .297 .391APAC .255 .201 .203 .292 .388Table 3: Kendall ?
correlations on the WMT14 hu-man judgements when translating out of English.Direction fr-en de-en hi-en cs-en ru-enDISCOTK-PARTY-TUNED .433 .381 .434 .328 .364BEER .417 .337 .438 .284 .337REDCOMBSENT .406 .338 .417 .284 .343REDCOMBSYSSENT .408 .338 .416 .282 .343METEOR .406 .334 .420 .282 .337Table 4: Kendall ?
correlations on the WMT14human judgements when translating into English.did not have the data to tune our metric forHindi.
Even by treating Hindi as English wemanage to get high in the rankings for thislanguage.From metrics that participated in all languagepairs on the sentence level on average BEER hasthe best correlation with the human judgment.6 Related workThe main contribution of our metric is a linearcombination of features with far less sparse statis-tics than earlier work.
In particular, we employnovel ordering features over PETs, a range of char-acter n-gram features for adequancy, and directtuning for human ranking.There are in the literature three main approachesfor tuning the machine translation metrics.Approach 1 SPEDE (Wang and Manning, 2012),metric of (Specia and Gim?enez, 2010),ROSE-reg (Song and Cohn, 2011), ABS met-ric of (Pad?o et al., 2009) and many oth-ers train their regression models on the datathat has absolute scores for adequacy, fluencyor post-editing and then test on the rankingproblem.
This is sometimes called pointwiseapproach to learning-to-rank.
In contrast ourmetric is trained for ranking and tested onranking.Approach 2 METEOR is tuned for the rankingand tested on the ranking like our metric butthe tuning method is different.
METEOR hasa non-linear model which is hard to tune withgradient based methods so instead they tunetheir parameters by hill-climbing (Lavie andAgarwal, 2008).
This not only reduces thenumber of features that could be used but alsorestricts the fine tuning of the existing smallnumber of parameters.Approach 3 Some methods, like ours, allowtraining of a large number of parameters forranking.
Global ranking models that di-rectly rank hypotheses are used in ROSE-rank (Song and Cohn, 2011) and PAIR met-ric of (Pad?o et al., 2009).
Our work is moresimilar to the training method for local rank-ing models that give score directly (as it isusually expected from an evaluation metric)which was originally proposed in (Ye et al.,2007) and later applied in (Duh, 2008) and(Yang et al., 2013).7 Conclusion and future plansWe have shown the advantages of combiningmany simple features in a tunable linear modelof MT evaluation metric.
Unlike majority of theprevious work we create a framework for traininglarge number of features on human rankings and atthe same time as a result of tuning produce a scorebased metric which does not require two (or more)hypotheses for comparison.
The features that weused are selected for reducing sparseness on thesentence level.
Together the smooth features andthe learning algorithm produce the metric that hasa very high correlation with human judgment.For future research we plan to investigate somemore linguistically inspired features and also ex-plore how this metric could be tuned for better tun-ing of statistical machine translation systems.AcknowledgmentsThis work is supported by STW grant nr.
12271and NWO VICI grant nr.
277-89-002.ReferencesAlexandra Birch and Miles Osborne.
2010.
LRscorefor Evaluating Lexical and Reordering Quality inMT.
In Proceedings of the Joint Fifth Workshop onStatistical Machine Translation and MetricsMATR,pages 327?332, Uppsala, Sweden, July.
Associationfor Computational Linguistics.Chris Callison-Burch, Cameron Fordyce, PhilippKoehn, Christof Monz, and Josh Schroeder.
2007.418(Meta-) Evaluation of Machine Translation.
InProceedings of the Second Workshop on StatisticalMachine Translation, StatMT ?07, pages 136?158,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Chris Callison-Burch, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia.
2012.Findings of the 2012 Workshop on Statistical Ma-chine Translation.
In Proceedings of the SeventhWorkshop on Statistical Machine Translation, pages10?51, Montr?eal, Canada, June.
Association forComputational Linguistics.Michael Collins, Philipp Koehn, and Ivona Ku?cerov?a.2005.
Clause Restructuring for Statistical MachineTranslation.
In Proceedings of the 43rd AnnualMeeting on Association for Computational Linguis-tics, ACL ?05, pages 531?540, Stroudsburg, PA,USA.
Association for Computational Linguistics.Michael Denkowski and Alon Lavie.
2011.
Meteor1.3: Automatic Metric for Reliable Optimizationand Evaluation of Machine Translation Systems.
InProceedings of the EMNLP 2011 Workshop on Sta-tistical Machine Translation.Kevin Duh.
2008.
Ranking vs. Regression in Ma-chine Translation Evaluation.
In Proceedings of theThird Workshop on Statistical Machine Translation,StatMT ?08, pages 191?194, Stroudsburg, PA, USA.Association for Computational Linguistics.Mark Hopkins and Jonathan May.
2011.
Tuning asRanking.
In Proceedings of the 2011 Conference onEmpirical Methods in Natural Language Process-ing, pages 1352?1362, Edinburgh, Scotland, UK.,July.
Association for Computational Linguistics.Hideki Isozaki, Tsutomu Hirao, Kevin Duh, KatsuhitoSudoh, and Hajime Tsukada.
2010.
AutomaticEvaluation of Translation Quality for Distant Lan-guage Pairs.
In Proceedings of the 2010 Conferenceon Empirical Methods in Natural Language Pro-cessing, EMNLP ?10, pages 944?952, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Alon Lavie and Abhaya Agarwal.
2008.
METEOR:An Automatic Metric for MT Evaluation with HighLevels of Correlation with Human Judgments.
InProceedings of the ACL 2008 Workshop on Statisti-cal Machine Translation.Hang Li.
2011.
Learning to Rank for Information Re-trieval and Natural Language Processing.
SynthesisLectures on Human Language Technologies.
Mor-gan & Claypool Publishers.Matou?s Mach?a?cek and Ond?rej Bojar.
2013.
Resultsof the WMT13 Metrics Shared Task.
In Proceed-ings of the Eighth Workshop on Statistical MachineTranslation, pages 45?51, Sofia, Bulgaria, August.Association for Computational Linguistics.Sebastian Pad?o, Michel Galley, Dan Jurafsky, andChristopher D. Manning.
2009.
Textual Entail-ment Features for Machine Translation Evaluation.In Proceedings of the Fourth Workshop on Statis-tical Machine Translation, StatMT ?09, pages 37?41, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A Method for AutomaticEvaluation of Machine Translation.
In Proceedingsof the 40th Annual Meeting on Association for Com-putational Linguistics, ACL ?02, pages 311?318,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Xingyi Song and Trevor Cohn.
2011.
Regression andRanking based Optimisation for Sentence Level MTEvaluation.
In Proceedings of the Sixth Workshopon Statistical Machine Translation, pages 123?129,Edinburgh, Scotland, July.
Association for Compu-tational Linguistics.Lucia Specia and Jes?us Gim?enez.
2010.
CombiningConfidence Estimation and Reference-based Metricsfor Segment-level MT Evaluation.
In Ninth Confer-ence of the Association for Machine Translation inthe Americas, AMTA-2010, Denver, Colorado.Milo?s Stanojevi?c and Khalil Sima?an.
2013.
Eval-uating Long Range Reordering with Permutation-Forests.
In ILLC Prepublication Series, PP-2013-14.
University of Amsterdam.Mengqiu Wang and Christopher D. Manning.
2012.SPEDE: Probabilistic Edit Distance Metrics for MTEvaluation.
In Proceedings of the Seventh Work-shop on Statistical Machine Translation, WMT ?12,pages 76?83, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational linguistics, 23(3):377?403.Muyun Yang, Junguo Zhu, Sheng Li, and Tiejun Zhao.2013.
Fusion of Word and Letter Based Metricsfor Automatic MT Evaluation.
In Proceedings ofthe Twenty-Third International Joint Conference onArtificial Intelligence, IJCAI?13, pages 2204?2210.AAAI Press.Yang Ye, Ming Zhou, and Chin-Yew Lin.
2007.
Sen-tence Level Machine Translation Evaluation As aRanking Problem: One Step Aside from BLEU.
InProceedings of the Second Workshop on StatisticalMachine Translation, StatMT ?07, pages 240?247,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Hao Zhang and Daniel Gildea.
2007.
Factorization ofsynchronous context-free grammars in linear time.In In NAACL Workshop on Syntax and Structure inStatistical Translation (SSST.419
