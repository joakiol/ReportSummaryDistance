Analysis of Semantic Classes in Medical Text for Question AnsweringYun Niu and Graeme HirstDepartment of Computer ScienceUniversity of TorontoToronto, Ontario M5S 3G4Canadayun@cs.toronto.edu, gh@cs.toronto.eduAbstractTo answer questions from clinical-evidence texts,we identify occurrences of the semantic classes ?disease, medication, patient outcome ?
that arecandidate elements of the answer, and the relationsamong them.
Additionally, we determine whetheran outcome is positive or negative.1 MotivationThe published medical literature is an importantsource to help clinicians make decisions in patienttreatment (Sackett and Straus, 1998; Straus andSackett, 1999).
Clinicians often need to consult lit-erature on the latest information in patient care, suchas side effects of a medication, symptoms of a dis-ease, or time constraints in the use of a medication.For example:1Q: In a patient with a suspected MI does throm-bolysis decrease the risk of death if it is ad-ministered 10 hours after the onset of chestpain?The answer to the question can be found in Clini-cal Evidence (CE) (Barton, 2002), a regularly up-dated publication that reviews and consolidates ex-perimental results for clinical problems:A: Systematic reviews of RCTs have foundthat prompt thrombolytic treatment (within 6hours and perhaps up to 12 hours and longerafter the onset of symptoms) reduces mor-tality in people with AMI and ST elevationor bundle branch block on their presentingECG.The goal of the EpoCare project (?Evidence atPoint of Care?)
at the University of Toronto is todevelop methods for answering questions automati-cally with CE as the source text.
(We do not look at1All the examples in this paper are taken from a collectionof questions that arose over a two-week period in August 2001in a clinical teaching unit at the University of Toronto.primary medical research text.)
Currently, the sys-tem accepts keyword queries in PICO format (Sack-ett et al, 2000).
In this format, a clinical question isrepresented by a set of four fields that correspond tothe basic elements of the question:P: a description of the patient (or the problem);I: an intervention;C: a comparison or control intervention (maybe omitted);O: the clinical outcome.For example, the question shown above can be rep-resented in PICO format as follows:P: myocardial infarctionI: thrombolysisC: ?O: mortalityOur work in the project is to extend the keywordretrieval to a system that can answer questions ex-pressed in natural language.In our earlier work (Niu et al, 2003), we showedthat current technologies for factoid question an-swering (QA) are not adequate for clinical ques-tions, whose answers must often be obtained bysynthesizing relevant context.
To adapt to this newcharacteristic of QA in the medical domain, we ex-ploit semantic classes and relations between them inmedical text.
Semantic classes are important for ourtask because the information contained in them isoften a good candidate for answering clinical ques-tions.
In the example above, PICO elements cor-respond to three semantic classes: DISEASE (med-ical problem of the patient), INTERVENTION (med-ication applied to the disease) and the CLINICALOUTCOME.
They together constitute a SCENARIOof treatment.
Similarly, a diagnosis scenario oftenincludes SYMPTOMS, TESTING PROCEDURE, andHYPOTHESIZED DISEASES.
To understand the se-mantics of medical text and find answers to clinicalquestions, we need to know how these classes relateto each other in a specific scenario.
For example, isthis medication a special type of another one; is thismedication applied to this disease?
These are thekind of relations that we are interested in.
In thiswork, we use a cue-word?based approach to iden-tify semantic classes in the treatment scenario andanalyze the relations between them.
We also applyan automatic classification process to determine thepolarity of an outcome, as it is important in answer-ing clinical questions.2 Identifying Semantic Classes in MedicalText2.1 Diseases and MedicationsThe identification of named entities (NEs) in thebiomedical area, such as PROTEINS and CELLS, hasbeen extensively explored; e.g., Lee et al (2003),Shen et al (2003).
However, we are not aware ofany satisfactory solution that focuses on the recog-nition of semantic classes such as MEDICATION andDISEASE.
To straightforwardly identify DISEASEand MEDICATION in the text, we use the knowledgebase Unified Medical Language System (UMLS)(Lindberg et al, 1993) and the software MetaMap(Aronson, 2001).UMLS contains three knowledge sources: theMetathesaurus, the Semantic Network, and the Spe-cialist Lexicon.
Given an input sentence, MetaMapseparates it into phrases, identifies the medical con-cepts embedded in the phrases, and assigns propersemantic categories to them according to the knowl-edge in UMLS.
For example, for the phrase imme-diate systemic anticoagulants, MetaMap identifiesimmediate as a TEMPORAL CONCEPT, systemic asa FUNCTIONAL CONCEPT, and anticoagulants as aPHARMACOLOGIC SUBSTANCE.
More than one se-mantic category in UMLS may correspond to MED-ICATION or DISEASE.
For example, either a PHAR-MACOLOGIC SUBSTANCE or a THERAPEUTIC ORPREVENTIVE PROCEDURE can be a MEDICATION;either a DISEASE OR SYNDROME or a PATHOLOGICFUNCTION can be a DISEASE.We use some training text to find the mappingbetween UMLS categories and the two semanticclasses in the treatment scenario.
The trainingtext was tagged for us by a clinician to mark DIS-EASE and MEDICATION.
It was also processed byMetaMap.
After that, the annotated text was com-pared with the output of MetaMap to find the corre-sponding UMLS categories.
Medical text contain-ing these categories can then be identified as eitherMEDICATION or DISEASE.
In the example above,anticoagulants will be taken as a MEDICATION.
Theproblem of identification of medical terminology isstill a big challenge in this area.
MetaMap does notprovide a full solution to it.
For cases in which theoutput of MetaMap is not consistent with the judg-ment of the clinician who annotated our text, ourdecisions rely on the latter.2.2 Clinical OutcomeThe task of identifying clinical outcomes is morecomplicated.
Outcomes are often not just nounphrases; instead, they usually are expressed in com-plex syntactic structures.
The following are someexamples:(1) Thrombolysis reduces the risk of depen-dency, but increases the risk of death.
(2) The median proportion of symptom free daysimproved more with salmeterol than withplacebo.In our analysis of the text, we found another typeof outcome which is also very important: the out-come of clinical trials:(3) Several small comparative RCTs [random-ized clinical trials] have found sodium cro-moglicate to be less effective than inhaledcorticosteroids in improving symptoms andlung function.
(4) In the systematic review of calcium chan-nel antagonists, indirect and limited compar-isons of intravenous versus oral administra-tion found no significant difference in ad-verse events.We treat these as a special type of clinical outcome.For convenience, we refer to them as ?results?
in thefollowing description when necessary.
A ?result?might contain a clinical outcome within it, as resultsoften involve a comparison of the effects of two (ormore) interventions on a disease.In medical text, the appearance of some words isfound often to be a signal of the occurrence of anoutcome, and usually several words signal the oc-currence of one single outcome.
The combinationapproach that we applied for identifying outcomesis based on this observation.
Our approach doesnot extract the whole outcome at once.
Instead, ittries to identify the different parts of an outcomethat may be scattered in the sentence, and then com-bines them to form the complete outcome.2.2.1 Related workRule-based methods and machine-learning ap-proaches have been used for similar problems.Gildea and Jurafsky (2002) used a supervised learn-ing method to learn both the identifier of the seman-tic roles defined in FrameNet such as theme, target,goal, and the boundaries of the roles (Baker et al,2003).
A set of features were learned from a largetraining set, and then applied to the unseen data todetect the roles.
The performance of the system wasquite good.
However, it requires a large trainingset for related roles, which is not available in manytasks, including tasks in the medical area.Rule-based methods are explored in informationextraction (IE) to identify roles to fill slots in somepre-defined templates (Catala` et al, 2003).
Therules are represented by a set of patterns, and tem-plate role identification is usually conducted by pat-tern matching.
Slots indicating roles are embed-ded in these patterns.
Text that satisfies the con-straints of a pattern will be identified, and the con-tents corresponding to the slots are extracted.
Thisapproach has been proved to be effective in many IEtasks.
However, pattern construction is very time-consuming, especially for complicated phrasings.In order to select the roles and only the roles, theirexpression has to be customized specifically in pat-terns.
This results in increasing difficulties in pat-tern construction, and reduces the coverage of thepatterns.2.2.2 A combination approachDifferent pieces of an outcome are identified by var-ious cue words.
Each occurrence of a cue word sug-gests a portion of the expression of the outcome.Detecting all of them will increase the chance ofobtaining the complete outcome.
Also, different oc-currences of cue words provide more evidence ofthe existence of an outcome.The first step of the combination approach is tocollect the cue words.
Two sections of CE (strokemanagement, asthma in children) were analyzed fordetection of outcome.
The text was annotated by aclinician in the EpoCare project.
About two-thirdsof each section (267 sentences in total) was taken asthe analysis examples for collecting the cue words,and the rest (156 sentences) as the test set.
Somewords we found in the analysis are the following:Nouns: death, benefit, dependency, outcome, evi-dence, harm, difference.Verbs: improve, reduce, prevent, produce, in-crease.Adjectives: beneficial, harmful, negative, adverse,superior.After the cue words are identified, the next ques-tion is what portion of text each cue word suggestsas the outcome, which determines the boundary ofthe outcome.
The text was pre-processed by theApple Pie parser (Sekine, 1997) to obtain the part-of-speech and phrase information.
We found thatfor the noun cues, the noun phrase that contains thenoun will be part of the outcome.
For the verb cuewords, the verb and its object together constituteone portion of the outcome.
For the adjective cuewords, often the corresponding adjective phrase orthe noun phrase belongs to the outcome.
Cue wordsfor the results of clinical trials are processed in aslightly different way.
For example, for differenceand superior, any immediately following preposi-tional phrase is also included in the results of thetrial.Our approach does not rely on specific patterns,it is more flexible than pattern-matching techniquesin IE systems, and it does not need a large trainingset.
A limitation of this approach is that some con-nections between different portions of an outcomemay be missing.2.2.3 Evaluation and analysis of resultsWe evaluated the cue word method of detecting theoutcome on the remaining one-third of the sectionsof CE.
(The test set is rather small because of thedifficulty in obtaining the annotations.)
The out-come detection task was broken into two sub-tasks,each evaluated separately: to identify the outcomeitself and to determine its textual boundary.
The re-sult of identification is shown in Table 1.
Eighty-onesentences in the test set contain either an outcome orresult, which is 52% of all the test sentences.
Thiswas taken as the baseline of the evaluation: takingall sentences in the test set as positive (i.e., contain-ing an outcome or result).
By contrast, the accuracyof the combination approach is 83%.There are two main reasons why some outcomeswere not identified.
One is that some outcomes donot have any cue word:(5) Gastrointestinal symptoms and headacheshave been reported with both montelukastand zafirlukast.The other reason is that although some outcomescontained words that might be regarded as cuewords, we did not include them in our set; for ex-ample, fewer and higher.
Adjectives were found tohave the most irregular usages.
It is normal for themto modify both medications and outcomes, as shownin the following examples:(6) .
.
.
children receiving higher dose inhaledcorticosteroids .
.
.Table 1: Results of identifying outcomes in CEFalse FalseMethod Correct Positives Negatives Precision% Recall% Accuracy%Baseline 81 75 0 52 (81/156) 100 52Combination approach 67 14 14 83 (67/81) 83 82Table 2: Results of boundary detection of correctlyidentified outcomes in CE.
A: Identified fragments;B: true boundary.Type of Overlap Number PercentageExact match 26 39A entirely within B 19 28B entirely within A 13 19Each partiallywithin the other 8 12No match 1 1(7) .
.
.
mean morning PEFR was 4% higher inthe salmeterol group.Other adjectives such as less, more, lower, shorter,longer, and different have similar problems.
If theyare taken as identifiers of outcomes then some falsepositives are very likely to be generated.
However,if they are excluded, some true outcomes will bemissed.
There were 14 samples of false positives.The main cause was sentences containing cue wordsthat did not have any useful information:(8) We found that the balance between bene-fits and harms has not been clearly estab-lished for the evacuation of supratentorialhaematomas.
(9) The third systematic review did not evaluatethese adverse outcomes.Table 2 shows the result of boundary detectionfor those outcomes that were correctly identified.The true boundary is the boundary of an outcomethat was annotated manually.
The no match casemeans that there is a true outcome in the sentencebut the program missed the correct portions of textand marked some other portions as the outcome.The program identified 39% of the boundaries ex-actly the same as the true boundaries.
In 19% of thesamples, the true boundaries were entirely withinthe identified fragments.
The spurious text in them(the text that was not in the true boundary) wasfound to be small in many cases, both in terms ofnumber of words and in terms of the importance ofthe content.
The average number of words correctlyidentified was 7 for each outcome and the numberof spurious words was 3.4.
The most frequent con-tent in the spurious text was the medication appliedto obtain the outcome.
In the following examples,text in ?hi?
is the outcome (result) identified auto-matically, and text in ?fg?
is spurious.
(10) The RCTs found hno significant adverse ef-fects fassociated with salmeterolgi.
(11) The second RCT .
.
.
also found hno sig-nificant difference in mortality at 12 weeksfwith lubeluzole versus placebogi .
.
.Again, adjectives are most problematic.
Evenwhen a true adjective identifier is found, the bound-ary of the outcome is hard to determine by an un-supervised approach because of the variations inthe expression.
In the following examples, the trueboundaries of outcomes are indicated by ?
[ ]?, ad-jectives are highlighted.
(12) Nebulised .
.
.
, but [hserious adverseeffectsi are rare].
(13) Small RCTs .
.
.
found that [.
.
.
washeffectivei, with .
.
.
].The correctness of the output of the parser alsohad an important impact on the performance, asshown in the following example:(14) RCTs found no evidence that lubeluzoleimproved clinical outcomes in people withacute ischaemic stroke.
(S .
.
.
(NPL (DT that) (JJ lubeluzole) (JJ im-proved) (JJ clinical) (NNS outcomes)) .
.
.
)In this parse, the verb improve was incorrectly as-signed to be an adjective in a noun phrase.
Thus im-prove as a verb cue word was missed in identifyingthe outcome.
However, another cue word outcomeswas matched, so the whole noun phrase of outcomeswas identified as the outcome.
On the one hand,the example shows that the wrong parsing outputdirectly affects the identification process.
On theother hand, it also shows that missing one cue wordin identifying the outcome can be corrected by theoccurrence of other cue words in the combinationapproach.3 Analysis of RelationsRecognition of individual semantic classes is notenough for text understanding; we also need toknow how different entities in the same semanticclass are connected, as well as what relations holdbetween different classes.
Currently, all these rela-tions are considered at the sentence level.3.1 Relations within the same semantic classRelations between different medications are the fo-cus of this sub-section, as a sentence often men-tioned more than one medication.
Relations be-tween diseases can be analyzed in a similar way, al-though they occur much less often than medications.Text from CE was analyzed manually to understandwhat relations are often involved and how they arerepresented.
The text for the analysis is the sameas in the class-identification task discussed above.As with classes themselves, it was found that theserelations can be identified by a group of cue wordsor symbols.
For example, the word plus refers tothe COMBINATION of two or more medications, theword or, as well as a comma, often suggests the AL-TERNATIVE relation, and the word versus (or v) usu-ally implies a COMPARISON relation, as shown inthe following examples:(15) The combination of aspirin plus streptoki-nase significantly increased mortality at 3months.
(16) RCTs found no evidence that calcium chan-nel antagonists, lubeluzole, aminobutyricacid (GABA) agonists, glycine antagonists,or N-methyl-D-aspartate (NMDA) antago-nists improve clinical outcomes in peoplewith acute ischaemic stroke.
(17) One systematic review found no short orlong term improvement in acute ischaemicstroke with immediate systemic anticoagu-lants (unfractionated heparin, low molecu-lar weight heparin, heparinoids, or specificthrombin inhibitors) versus usual care with-out systemic anticoagulants.It is worth noting that in CE, the experimental con-ditions are often explained in the description of theoutcomes, for example:(18) .
.
.
receiving higher dose inhaled corticos-teroids (3.6cm, 95% CI 3.0 to 4.2 with dou-ble dose beclometasone v 5.1cm, 95% CI 4.5to 5.7 with salmeterol v 4.5cm, 95% CI 3.8to 5.2 with placebo).
(19) It found that .
.
.
oral theophylline .
.
.
ver-sus placebo increased the mean number ofsymptom free days (63% with theophylline v42% with placebo; P=0.02).
(20) Studies of .
.
.
inhaled steroid (see salme-terol v high dose inhaled corticosteroids un-der adult asthma).These descriptions are usually in parentheses.
Theyare often phrases and even just fragments of stringsthat are not represented in a manner that is uniformwith the other parts of the sentence.
Their behavioris more difficult to capture and therefore the rela-tions among the concepts in these descriptions aremore difficult to identify.
Because they usually areexamples and data, omission of them will not af-fect the understanding of the whole sentence in mostcases.Six common relations and their cue words werefound in the text which are shown in Table 3.
Cuewords and symbols between medical concepts werefirst collected from the training text.
Then the re-lations they signal were analyzed.
Some cue wordsare ambiguous, for example, or, and, and with.
Orcould also suggest a comparison relation althoughmost of the time it means alternative, and could rep-resent an alternative relation, and with could be aspecification relation.
It is interesting to find thatand in the text when it connects two medicationsoften suggests an alternative relation rather than acombination relation (e.g., the second and in exam-ple 5).
Also, compared with versus, plus, etc., andand with are weak cues as most of their appearancesin the text do not suggest a relation between twomedications.On the basis of this analysis, an automatic re-lation analysis process was applied to the test set,which was the same as in outcome identification.The test process was divided into two parts: onetook parenthetical descriptions into account (case 1)and the other one did not (case 2).
In the evaluation,for sentences that contain at least two medications,?correct?
means that the relation that holds betweenthe medications is correctly identified.
We do notevaluate the relation between any two medicationsin a sentence; instead, we only considered two med-ications that are related to each other by a cue wordor symbol (including those connected by cue wordsTable 3: Cue words/symbols for relations betweenmedicationsRelation(s) Cue Words/Symbolscomparison superior to, more than, versus, or,compare with, between .
.
.
and .
.
.alternative or, ?,?, andcombination plus, add to, addition of .
.
.
to .
.
.
,combined use of, and, with, ?
(?specification with, ?
(?substitute substitute, substituted forpreference rather thanTable 4: Results of relation analysisCorrect Wrong Missing False PositiveCase 1 49 7 10 9Case 2 48 7 3 6other than the set collected from the training text).The results of the two cases are shown in Table 4.Most errors are because of the weak indicatorswith and and.
As in the outcome identification task,both the training and test sets are rather small, as nostandard annotated text is available.Some of the surface relationships in Table 3 re-flect deeper relationships of the semantic classes.For example, COMPARISON, ALTERNATIVE, andPREFERENCE imply that the two (or more) medi-cations have some common effects on the disease(s)they are applied to.
The SPECIFICATION relation, onthe other hand, suggests a hierarchical relation be-tween the first medication and the following ones, inwhich the first medication is a higher-level conceptand the following medications are at a lower level.For example, in example 17 above, systemic anti-coagulants is a higher-level concept, unfractionatedheparin, low molecular weight heparin, etc., are ex-amples of it that lie at a lower level.3.2 Relations between different semanticclassesIn a specific domain such as medicine, some defaultrelations often hold between semantic classes.
Forexample, a CAUSE?EFFECT relation is strongly em-bedded in the three semantic classes appearing ina sentence of the form: ?medication .
.
.
disease.
.
.
outcome?, even if not in this exact order.
Thisdefault relation helps the relation analysis becausein most cases we do not need to depend on the textbetween the classes to understand the whole sen-tence.
For instance, the CAUSE?EFFECT relationis very likely to express the idea that applying theintervention on the disease will have the outcome.This is another reason that semantic classes are im-portant, especially in a specific domain.4 The polarity of outcomesMost clinical outcomes and the results of clinicaltrials are either positive or negative:(21) Positive: Thrombolysis reduced the risk ofdeath or dependency at the end of the stud-ies.
(22) Negative: In the systematic review, throm-bolysis increased fatal intracranial haemor-rhage compared with placebo.Polarity information is useful for several reasons.First of all, it can filter out positive outcomes if thequestion is about the negative aspects of a medica-tion.
Secondly, negative outcomes may be crucialeven if the question does not explicitly ask aboutthem.
Finally, from the number of positive or neg-ative descriptions of the outcome of a medicationapplying to a disease, clinicians can form a generalidea about how ?good?
the medication is.
As a firststep in understanding opposing relations betweenscenarios in medical text, the polarity of outcomeswas determined by an automatic classification pro-cess.We use support vector machines (SVMs) to dis-tinguish positive outcomes from negative ones.SVMs have been shown to be efficient in text clas-sification tasks (Joachims, 1998).
Given a trainingsample, the SVM finds a hyperplane with the max-imal margin of separation between the two classes.The classification is then just to determine whichside of the hyperplane the test sample lies in.
Weused the SVMlight package (Joachims, 2002) in ourexperiment.4.1 Training and test examplesThe training and test sets were built by collectingsentences from different sections in CE; 772 sen-tences were used, 500 for training (300 positive, 200negative), and 272 for testing (95 positive, 177 neg-ative).
All examples were labeled manually.4.2 EvaluationThe classification used four different sets of fea-tures.
The first feature set includes every unigramthat appears at least three times in the whole train-ing set.
To improve the performance by attenuatingthe sparse data problem, in the second feature set,all names of diseases were replaced by the same tagdisease.
This was done by pre-processing the textusing MetaMap to identify all diseases in both thetraining and the test examples.
Then the identifieddiseases were replaced by the disease tag automat-ically.
As medications often are not mentioned inoutcomes, they were not generalized in this manner.The third feature set represents changes describedin outcomes.
Our observation is that outcomes of-ten involve the change in a clinical value.
For ex-ample, after a medication was applied to a disease,something was increased (enhanced, more, .
.
. )
ordecreased (reduced, less, .
.
.
).
Thus the polarityof an outcome is often determined by how changehappens: if a bad thing (e.g., mortality) is reducedthen it is a positive outcome; if the bad thing is in-creased, then the outcome is negative.
We try tocapture this observation by adding context featuresto the feature set.
The way they were added is sim-ilar to incorporating the negation effect describedby Pang et al (2002).
But instead of just finding a?negation word?
(not, isn?t, didn?t, etc.
), we need tofind two groups of words: those indicating more andthose indicating less.
In the training text, we found9 words in the first group and 7 words in the secondgroup.
When pre-processing text for classification,following the method of Pang et al, we attached thetag MORE to all words between the more-wordsand the following punctuation mark, and the tagLESS to the words after the less-words.The fourth feature set is the combination of theeffects of feature set two and three.
In representingeach sentence by a feature vector, we tested bothpresence (feature appears or not) and frequency(count the number of occurrences of the feature inthe sentence).The accuracy of the classification is shown in Ta-ble 5.
The baseline is to assign a random class (herewe use negative, as they are more frequent in the testset) to all test samples.The presence of features performs better than fre-quency of features in general.
Using a more gen-eral category instead of specific diseases has a pos-itive effect on the presence-based classification.
Wespeculate that the effect of this generalization willbe bigger if a larger test set were used.
Pang et al(2002) did not compare the result of using and notusing the negation context effect, so it is not clearhow much it improved their result.
In our task, itis clear that the MORE/ LESS feature has a signif-icant effect on the performance, especially for thefrequency features.Table 5: Results of outcome polarity classificationPresence FrequencyFeatures (%) (%)Baseline 65.07 65.07Original unigrams 88.97 87.87Unigrams with disease 90.07 88.24Unigrams withMORE/ LESS tag 91.54 91.91Unigrams with diseaseand MORE/ LESS tag 92.65 92.285 ConclusionWe have described our work in medical text anal-ysis by identifying semantic classes and the rela-tions between them.
Our work suggests that seman-tic classes in medical scenarios play an importantrole in understanding medical text.
The scenarioview may be extended to a framework that acts asa guideline for further semantic analysis.Semantic classes and their relations have di-rect applications in medical question answering andquery refinement in information retrieval.
In ques-tion answering, the question and answer candidateswill contain some semantic classes.
After identify-ing them on both sides, the question can be com-pared with the answer to find whether there is amatch.
In information retrieval, relations betweensemantic classes can be added to the index.
If thequery posed by the user is too general, the systemwill ask the user to refine the query by adding moreconcepts and even relations so that it will be morepertinent according to the content of the source.
Forexample, a user may search for a document describ-ing the comparison of aspirin and placebo.
Insteadof just using aspirin and placebo as the query terms,the user can specify the comparison relation as wellin the query.We will continue working on the second level ofthe semantic analysis, to explore the relations onthe scenario level.
A complete scenario contains allthree semantic classes.
One scenario may be the ex-planation or justification of the previous scenario(s),or contradictory to the previous scenario(s).
De-tecting these relationships will be of great help forunderstanding-based tasks, such as context-relatedquestion answering, topic-related summarization,etc.
As different scenarios might not be adjacent toeach other in the texts, classical rhetorical analysiscannot provide a complete solution for this problem.AcknowledgementsThe EpoCare project is supported by grants fromBell University Laboratories at the University ofToronto.
Our work is also supported by a grantfrom the Natural Sciences and Engineering Re-search Council of Canada and an Ontario GraduateScholarship.
We are grateful to Sharon Straus, MD,and other members of the EpoCare project for dis-cussion and assistance.ReferencesAlan R. Aronson.
2001.
Effective mapping ofbiomedical text to the UMLS metathesaurus: TheMetaMap program.
In Proceedings of Ameri-can Medical Informatics Association Symposium,pages 17?21.Collin F. Baker, Charles J. Fillmore, and BeauCronin.
2003.
The structure of the Framenetdatabase.
International Journal of Lexicography,16(3):281?296.Stuart Barton.
2002.
Clinical Evidence.
BMJ Pub-lishing Group, London.Neus Catala`, Nu?ria Castell, and Mario Mart?in.2003.
A portable method for acquiring infor-mation extraction patterns without annotated cor-pora.
Natural Language Engineering, 9(2):151?179.Daniel Gildea and Daniel Jurafsky.
2002.
Auto-matic labeling of semantic roles.
ComputationalLinguistics, 28(3):245?288.Thorsten Joachims.
1998.
Text categorization withsupport vector machines: Learning with manyrelevant features.
In Proceedings of the EuropeanConference on Machine Learning (ECML), pages137?142.Thorsten Joachims.
2002.
SVMlight homepage.
Inhttp://svmlight.joachims.org/.Ki-Joong Lee, Young-Sook Hwang, and Hae-ChangRim.
2003.
Two-phase biomedical NE recog-nition based on SVMs.
In Proceedings of 41stannual meeting of the Association for Compu-tational Linguistics, Workshop on Natural Lan-guage Processing in Biomedicine, pages 33?40.Donald A.
B. Lindberg, Betsy L. Humphreys, andAlexa.
T. McCray.
1993.
The Unified MedicalLanguage System.
Methods of Information inMedicine, 32(4):281?291.Yun Niu, Graeme Hirst, Gregory McArthur, andPatricia Rodriguez-Gianolli.
2003.
Answeringclincal questions with role identification.
In Pro-ceedings of 41st annual meeting of the Associ-ation for Computational Linguistics, Workshopon Natural Language Processing in Biomedicine,pages 73?80.Bo Pang, Lillian Le, and ShivakumarVaithyanathan.
2002.
Thumbs up?
Senti-ment classification using machine learningtechniques.
In Proceedings of 2002 Conferenceon Empirical Methods in Natural LanguageProcessing (EMNLP), pages 79?86.David L. Sackett and Sharon E. Straus.
1998.
Find-ing and applying evidence during clinical rounds:The ?evidence cart?.
Journal of the AmericanMedical Association, 280(15):1336?1338.David L. Sackett, Sharon E. Straus, W. ScottRichardson, William Rosenberg, and R. BrianHaynes.
2000.
Evidence-Based Medicine: Howto Practice and Teach EBM.
Harcourt PublishersLimited, Edinburgh.Satoshi Sekine.
1997.
Apple pie parser homepage.In http://nlp.cs.nyu.edu/app/.Dan Shen, Jie Zhang, Guodong Zhou, Jian Su, andChew-Lim Tan.
2003.
Effective adaptation ofhidden Markov model?based named entity rec-ognizer for biomedical domain.
In Proceedingsof 41st annual meeting of the Association forComputational Linguistics, Workshop on NaturalLanguage Processing in Biomedicine, pages 49?56.Sharon E. Straus and David L. Sackett.
1999.Bringing evidence to the point of care.
Journalof the American Medical Association, 281:1171?1172.
