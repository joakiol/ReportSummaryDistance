Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1986?1991,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsModified Dirichlet Distribution: Allowing Negative Parameters to InduceStronger Sparsity?Kewei TuSchool of Information Science and TechnologyShanghaiTech University, Shanghai, Chinatukw@shanghaitech.edu.cnAbstractThe Dirichlet distribution (Dir) is one of themost widely used prior distributions in statis-tical approaches to natural language process-ing.
The parameters of Dir are required to bepositive, which significantly limits its strengthas a sparsity prior.
In this paper, we proposea simple modification to the Dirichlet distribu-tion that allows the parameters to be negative.Our modified Dirichlet distribution (mDir) notonly induces much stronger sparsity, but alsosimultaneously performs smoothing.
mDir isstill conjugate to the multinomial distribution,which simplifies posterior inference.
We in-troduce two simple and efficient algorithmsfor finding the mode of mDir.
Our experi-ments on learning Gaussian mixtures and un-supervised dependency parsing demonstratethe advantage of mDir over Dir.1 Dirichlet DistributionThe Dirichlet distribution (Dir) is defined over prob-ability vectors x = ?x1, .
.
.
, xn?
with positive pa-rameter vector ?
= ?
?1, .
.
.
, ?n?:Dir(x;?)
= 1B(?
)n?i=1x?i?1iwhere the normalization factor B(?)
is the multi-variate beta function.
When the elements in ?
arelarger than one, Dir can be used as a smoothnessprior that prefers more uniform probability vectors,with larger ?
values inducing more smoothness.
?This work was supported by the National Natural ScienceFoundation of China (61503248).When the elements in?
are less than one, Dir can beseen as a sparsity prior that prefers sparse probabil-ity vectors, with smaller ?
values inducing strongersparsity.
To better understand its sparsity preference,we take the logarithm of Dir:log Dir(x;?)
=n?i=1(?i ?
1) log xi + constantSince ?i ?
1 is negative, the closer xi is to zero,the higher the log probability becomes.
The coef-ficient ?i ?
1 controls the strength of the sparsitypreference.
However, ?i is required to be positivein Dir because otherwise the normalization factorbecomes divergent.
Consequently, the strength ofthe sparsity preference is upper bounded.
This be-comes problematic when a strong prior is needed,for instance, when the training dataset is large rela-tive to the model size (e.g., in unsupervised learningof an unlexicalized probabilistic grammar) and thusthe likelihood may dominate the posterior without astrong prior.2 Modified Dirichlet DistributionWe make a simple modification to the Dirichlet dis-tribution that allows the parameters in ?
to becomenegative.
To handle the divergent normalization fac-tor, we require that each xi must be lower boundedby a small positive constant .
Our modified Dirich-let distribution (mDir) is defined as follows.mDir(x;?, ) ={0 if ?i, xi < 1Z(?,)?ni=1 x?i?1i otherwisewhere we require 0 <  ?
1n and do not require?i to be positive.
With fixed values of ?
and , the1986unnormalized probability density is always boundedand hence the normalization factor Z(?, ) is finite.It is easy to show that mDir is still conjugate to themultinomial distribution.
Similar to Dir, mDir canbe used as a smoothness/sparsity prior depending onthe values of ?.
Because ?
is no longer requiredto be positive, we can achieve very strong sparsitypreference by using a highly negative vector of ?.Note that here we no longer achieve sparsity in itsstrict sense; instead, by sparsity we mean most ele-ments in x reach their lower bound .
Parameter can thus be seen as a smoothing factor that preventsany element in x to become too small.
Therefore,with proper parameters, mDir is able to simultane-ously achieve sparsity and smoothness.
This can beuseful in many applications where one wants to learna sparse multinomial distribution in an iterative waywithout premature pruning of components.2.1 Finding the ModeIf ?i, ?i ?
1 ?
0, then the mode of mDir can beshown to be:xi ={1?
(n?
1) if i = arg maxi ?i otherwiseOtherwise, we can find the mode with Algorithm 1.The algorithm first lets xi =  if ?i ?
1 and other-wise lets xi be proportional to ?i ?
1.
It then looksfor variables in x that are less than , increases themto , and renormalizes the rest of the variables.
Therenormalization may decrease some additional vari-ables below , so the procedure is repeated until allthe variables are larger than or equal to .Theorem 1.
If ?i, ?i > 1, then Algorithm 1 cor-rectly finds a mode of mDir(x;?, ).Proof.
First, we can show that for any i such that?i ?
1, we must have xi =  at the mode.
This isbecause if xi > , then we can increase the probabil-ity density by first decreasing xi to  (hence increas-ing x?i?1i ), and then increasing some other variablexj with ?j > 1 to satisfy the normalization condi-tion (hence also increasing x?j?1j ).
This is consis-tent with the output of the algorithm.Once we fix the value to  for any variable xis.t.
?i ?
1, the log probability density functionbecomes strictly concave on the simplex specifiedby the linear constraints ?i xi = 1 and xi ?
.Algorithm 1 Mode-finding of mDir(x;?, )1: S ?
{i|?i ?
1}2: T ?
?3: repeat4: T ?
T ?S5: for i ?
T do6: xi ?
7: end for8: z ?
?i/?T (?i ?
1)9: for i /?
T do10: xi ?
?i?1z ?
(1?
|T |)11: end for12: S ?
{i|xi < }13: until S = ?14: return ?x1, .
.
.
, xn?The strict concavity can be proved by showing thatthe log probability density function is twice differ-entiable and the Hessian is negative definite at anypoint of the simplex.With a concave function and linear constraints,the KKT conditions are sufficient for optimality.
Weneed to show that the output of the algorithm satis-fies the following KKT conditions:?
Stationarity: ?i, ?i?1xi = ?
?i + ??
Primal feasibility: ?i, xi ?
 and?i xi = 1?
Dual feasibility: ?i, ?i ?
0?
Complementary slackness: ?i, ?i(xi ?
) = 0Let x(k)i and T (k) be the values of xi and T afterk iterations of the algorithm.
Suppose the algorithmterminates after K iterations.
So the output of thealgorithm is ?x(K)1 , .
.
.
, x(K)n ?, which we will provesatisfies the KKT conditions.For any i s.t.
x(K)i > , we set ?i = 0 and ?
=?i?1x(K)ito satisfy all the conditions involving x(K)i .For any j s.t.
x(K)j = , suppose x(k)j < , i.e., xjfalls below  in iteration k and is set to  afterwards.Pick some i s.t.
i /?
T (K).
After iteration k and k+1respectively, we have:x(k)i?i ?
1=1?
?T (k)?
??j?
?T (k+1)\T (k) x(k)j??j?
/?T (k+1) ?j?
?
1x(k+1)i?i ?
1= 1?
?T(k+1)??j?
/?T (k+1) ?j?
?
11987Algorithm 2 Fast mode-finding of mDir(x;?, )1: ?
?k1 , .
.
.
, ?kn?
?
?
?1, .
.
.
, ?n?
in ascending order2: sn ?
?kn ?
13: for i = n?
1, .
.
.
, 1 do4: si = si+1 + ?ki ?
1 .
So si =?j?i(?kj ?
1)5: end for6: t?
07: for i = 1, .
.
.
, n do8: xki ?
?ki?1si ?
(1?
 t)9: if xki <  then10: xki ?
 , t?
t+ 111: end if12: end for13: return ?x1, .
.
.
, xn?Because for any j?
?
T (k+1)\T (k) we have x(k)j?
< ,from the two equations above we can deduce thatx(k)i > x(k+1)i , i.e., xi monotonically decreases overiterations.
Therefore,(?j ?
1)?x(K)i?i ?
1< (?j ?
1)?x(k)i?i ?
1= x(k)j < So we get?j ?
1 <?i ?
1x(K)i= ?So we set ?j = ?
?
?j?1 and all the conditionsinvolving x(K)j are also satisfied.
The proof is nowcomplete.The worst-case time complexity of Algorithm 1is O(n2), but in practice when  is small, the algo-rithm almost always terminates after only one iter-ation, leading to linear running time.
We also pro-vide a different mode-finding algorithm with betterworst-case time complexity ?
(n log n) (Algorithm2).
It differs from Algorithm 1 in that the elementsof ?
are first sorted, so we can finish computing xin one pass.
It can be more efficient than Algorithm1 when both  and n are larger.
Its correctness canbe proved in a similar way to that of Algorithm 1.2.2 Related DistributionThe closest previous work to mDir is the pseudo-Dirichlet distribution (Larsson and Ugander, 2011).It also allows negative parameters to achievestronger sparsity.
However, the pseudo-Dirichletdistribution is no longer conjugate to the multino-mial distribution.
Consequently, its maximum a pos-teriori inference becomes complicated and has notime-complexity guarantee.3 Learning Mixtures of GaussiansWe first evaluate mDir in learning mixtures of Gaus-sians from synthetic data.
The ground-truth modelcontains two bivariate Gaussian components withequal mixing probabilities (Figure 5(a)).
From theground-truth we sampled two training datasets of 20and 200 data points.
We then tried to fit a Gaussianmixture model with five components.Three approaches were tested: maximum like-lihood estimation using expectation-maximization(denoted by EM), which has no sparsity preference;mean-field variational Bayesian inference with a Dirprior over the mixing probabilities (denoted by VB-Dir), which is the most frequently used inferenceapproach for Dir with ?
< 1; maximum a posteri-ori estimation using expectation-maximization witha mDir prior over the mixing probabilities (denotedby EM-mDir).
The Dir and mDir priors that we usedare both symmetric, i.e., all the elements in vector?have the same value, denoted by ?.
For mDir, weset  = 10?5.
We ran each approach under eachparameter setting for 300 times with different ran-dom initialization and then reported the average re-sults.
During learning, we pruned a Gaussian com-ponent whenever its covariance matrix becomes nu-merically singular (which means the component isestimated from only one or two data samples).Figure 1?4 show the average test set log likeli-hood and the effective numbers of mixture compo-nents of the models learned with different values ofparameter ?
from 20 and 200 samples respectively.For VB-Dir, we show the results with the ?
value aslow as 10?5.
Further decreasing ?
did not improvethe results.
It can be seen that both VB-Dir andEM-mDir can achieve better test set likelihood andlower effective numbers of components than EMwith proper ?
values.
EM-mDir outperforms VB-Dir even with positive ?
values, and its performanceis further boosted when ?
becomes negative.
Theimprovement of EM-mDir when ?
becomes neg-ative is smaller in the 20-sample case than in the200-sample case.
This is because when the train-1988?1.448?1.446?1.444 00.511.5EM?mDirVB?DirEM????????????????????????????????
??
??
??
??
?
???????????????????????????????
????
???
????
?Figure 1: Test set log likelihood vs. the value of ?
(20 trainingsamples)?1.448?1.446?1.444 00.511.5EM?mDirVB?DirEM??????????????????
??
??
??
??
?
?????????????????
????
???
????
?Figure 2: Effective number of components vs. the value of ?
(20 training samples)ing dataset is small, a small positive ?
value mayalready be sufficient in inducing enough sparsity.Figure 5(b)?
(e) show the typical models learnedby VB-Dir and EM-mDir.
When the training datasetis small, both Dir and mDir are effective sparsitypriors that help prune unnecessary mixture compo-nents, though mDir can be more effective with a neg-ative ?
value.
When the training dataset is large,however, the Dir prior is overwhelmed by the like-lihood in posterior inference and cannot effectivelyprune mixture components.
On the other hand, witha highly negative ?
value, mDir is still effective as asparsity prior.4 Unsupervised Dependency ParsingUnsupervised dependency parsing aims to learn adependency grammar from unannotated text.
Pre-vious work has shown that sparsity regularizationimproves the performance of unsupervised depen-dency parsing (Johnson et al, 2007; Gillenwater etal., 2010).
In our experiments, we tried to learn adependency model with valence (DMV) (Klein andManning, 2004) from the Wall Street Journal cor-pus, with section 2-21 for training and section 23?1.448?1.446?1.444 00.511.5EM?mDirVB?DirEM??????????????????????????
????
???
????
????
??
??
??
??
?Figure 3: Test set log likelihood vs. the value of ?
(200 trainingsamples)?1.448?1.446?1.444 00.511.5EM?mDirVB?DirEM????????????????
??
??
??
??
?
??????????????
????
???
????
?Figure 4: Effective number of components vs. the value of ?
(200 training samples)for testing.
Following previous work, we used sen-tences of length ?
10 with punctuation stripped off.Since DMV is an unlexicalized model, the numberof dependency rules is small relative to the trainingcorpus size.
This suggests that a strong prior canbe helpful in counterbalancing the influence of thetraining data.We tested six approaches.
With a mDir prior,we tried EM, hard EM, and softmax-EM with ?
=0.5 (Tu and Honavar, 2012) (denoted by EM-mDir,HEM-mDir, SEM-mDir).
With a Dir prior, we triedvariational inference, hard variational inference, andsoftmax variational inference with ?
= 0.5 (Tuand Honavar, 2012) (denoted by VB-Dir, HVB-Dir,SVB-Dir).
Again, we used symmetric Dir and mDirpriors.
For mDir, we set  = 10?4 by default.Figure 6 shows the directed accuracy of parsingthe test corpus using the learned dependency mod-els.
It can be seen that with positive ?
values, Dirand mDir have very similar accuracy under the stan-dard, hard and softmax versions of inference respec-tively.
With negative ?
values, the accuracy of EM-mDir decreases; but for HEM-mDir and SEM-mDir,the accuracy is significantly improved with moder-19890 1 2 300.511.522.53(a) Ground-truth0 1 2 300.511.522.53(b) VB-Dir, ?
= 10?50 1 2 300.511.522.53(c) EM-mDir, ?
= ?20 1 2 300.511.522.53(d) VB-Dir, ?
= 10?50 1 2 300.511.522.53(e) EM-mDir, ?
= ?30Figure 5: The ground-truth model and four typical models learned by VB-Dir and EM-mDir.
(b),(c): 20 training samples.
(d),(e):200 training samples.00.20.40.60.800.20.40.60.811.2EM?mDirHEM?mDirSEM?mDirVB?DirHVB?DirSVB?Dirr0.40.450.50.550.60.65?70?60?50?40?30?20?1000.40.450.50.550.60.6500.250.50.751Figure 6: Parsing accuracy vs. the value of ?ately negative ?
values.
HEM-mDir consistentlyproduces accuracy around 0.63 with a large range of?
values (from -10 to -40), which is on a par with thebest published results in learning the original DMVmodel (Cohen and Smith, 2009; Gillenwater et al,2010; Berg-Kirkpatrick et al, 2010), even thoughthese previous approaches employed more sophis-ticated features and advanced regularization tech-niques than ours.Figure 7 shows the degree of sparsity of thelearned dependency grammars.
We computed thepercentage of dependency rules with probabilitiesbelow 10?3 to measure the degree of sparsity.
It canbe seen that even with positive ?
values, mDir leadsto significantly more sparse grammars than Dir does.With negative values of ?, mDir can induce evenmore sparsity.Figure 8 plots the parsing accuracy with differentvalues of parameter  in mDir (?
is set to -20).
Thebest accuracy is achieved when  is neither too largenor too small.
This is because if  is too large, theprobabilities of dependency rules become too uni-form to be discriminative.
On the other hand, if is too small, then the probabilities of many depen-dency rules may become too small in the early stagesof learning and never be able to recover.
Similar ob-servation was made by Johnson et al (2007) when00.20.40.60.800.20.40.60.811.2EM?mDirHEM?mDirSEM?mDirVB?DirHVB?DirSVB?DirDir0.40.450.50.550.60.650.70.750.80.850.9?70?60?50?40?30?20?1000.40.450.50.550.60.650.70.750.80.850.900.250.50.751Figure 7: Sparsity of the learned grammars vs. the value of ?r0.40.450.50.550.60.651.E?081.E?061.E?041.E?02EM?mDirHEM?mDirSEM?mDirFigu e 8: Parsing accuracy vs. the value of doing maximum a posteriori estimation with a Dirprior (hence with  = 0).5 ConclusionWe modify the Dirichlet distribution to allow nega-tive values of parameter?
so that it induces strongersparsity when used as a prior of a multinomialdistribution.
A second parameter  is introducedwhich prevents divergence of the normalization fac-tor and also acts as a smoothing factor.
Our modifiedDirichlet distribution (mDir) is still conjugate to themultinomial distribution.
We propose two efficientalgorithms for finding the mode of mDir.
Our ex-periments on learning Gaussian mixtures and unsu-pervised dependency parsing show the advantage ofmDir over the Dirichlet distribution.1990ReferencesTaylor Berg-Kirkpatrick, Alexandre Bouchard-Co?te?,John DeNero, and Dan Klein.
2010.
Painless unsu-pervised learning with features.
In Human LanguageTechnologies: The 2010 Annual Conference of theNorth American Chapter of the Association for Com-putational Linguistics, pages 582?590.
Association forComputational Linguistics.Shay B. Cohen and Noah A. Smith.
2009.
Shared logis-tic normal distributions for soft parameter tying in un-supervised grammar induction.
In HLT-NAACL, pages74?82.Jennifer Gillenwater, Kuzman Ganchev, Joa?o Grac?a, Fer-nando Pereira, and Ben Taskar.
2010.
Sparsity in de-pendency grammar induction.
In ACL ?10: Proceed-ings of the ACL 2010 Conference Short Papers, pages194?199, Morristown, NJ, USA.
Association for Com-putational Linguistics.Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-ter.
2007.
Bayesian inference for pcfgs via markovchain monte carlo.
In HLT-NAACL, pages 139?146.Dan Klein and Christopher D. Manning.
2004.
Corpus-based induction of syntactic structure: Models of de-pendency and constituency.
In Proceedings of ACL.Martin O Larsson and Johan Ugander.
2011.
A concaveregularization technique for sparse mixture models.
InAdvances in Neural Information Processing Systems,pages 1890?1898.Kewei Tu and Vasant Honavar.
2012.
Unambiguity reg-ularization for unsupervised learning of probabilisticgrammars.
In Proceedings of the 2012 Joint Confer-ence on Empirical Methods in Natural Language Pro-cessing and Computational Natural Language Learn-ing, pages 1324?1334.
Association for ComputationalLinguistics.1991
