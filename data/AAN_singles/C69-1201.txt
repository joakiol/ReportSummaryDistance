and satisfactory definition exists, and some linguistsdeny any validity of the word, relegating it to folklinguistics.Following Greenberg we take words as being composed of morphemesso that a word may be identified with a sequence of morphemes and nomorpheme overlaps two words.
From the distribution of the morphemesof a corpus we find clusters which approximate the words of the corpus.The approximating units are determined relative to the corpus from whichthe distribution is defined.
The corpus may be either considered as aclosed sublanguage in itself or as a sample from some larger corpus.We study the behavior of approximate units relative to longer and longerportions of the corpus, and also relative to the corpus considered as astatistical sample.Assuming that a word may be r~presented as a sequence of morphemes,how should this sequence be distinguished?
In the well-known paper ofTogeby, (19&9) there is a convenient summry  of structural views of theword.
In his discussion, the word is set forth as a morpheme sequencepossessing properties classified under the headings of 1 ?
Forme librej .
i 30 Permutabilite ~.
In considering how a ~ ,  2 ?
Seoarabllite, andmorpheme sequence should be distinguished as a word we will begin byexamining Togeby f s classifications.In Togeby, under the discussion of a word as a forme libre minimum,reference is made to Bloomfield's (1933) statement about the word as aminimum free form and the ~mallest items which are snoken by themselves,in isolation.The idea of minimum free form is actually found somewhat earlierin Bloomfield ' s (1926) Postulates.1differand notwe findA minimum free form is a word.
A word is th tm'a" fo rmwhich may be utteredalone (with.meaning) but cannot beanalyzedinto parts that may (all of them) be uttered alone (with meaning).Thus the~word ~ can be analyzed in to~_~ and z~ but thelatter part cannot be uttered alone; theword ~ c a n  beanalyzed into wr_wr_it_~ and -er, but the latter cannot be utteredalone (the word err he 'by  virtue of different meaning adifferent form) ...Similar views are found in the older "universal grammars."
Theyprincipally in taking the Aristotelian position that the wordsome smaller unit has meaning.
For example, in Harris (1771)a concern with min,tmumunits of meaning.But what shall we say?
Hav@ these parts (of a Qu@ntityof sound) again other parts, Which~arein like mannersignificant and may be pursued to Infinite?
9an we supposethat aliMea~ing~iike.Body, to be divisible; and to includewithin itself other Meanings without end?
If this be absurd,"' thenmhstw@~ec~ssarliy!~admit, that there is~such a thing asa Sound significant, of which no part is of itself significant.And thiS:~is ~at  we oall'.t.heproper:~haracte~ of a Wo~ Forthus though the Words (Suu) and (shineth!
have each a Meaning,yet iethere ce~ainly no-Mem~inginianT~f their Paths,neither in the syllables of one, nor the ~etters of the other.James Harris refers to Priscian's definition in which the word is definedas a minimummeaningful utterance in connected speech.'
"Dictio @S~ par t t ime oratlonis constructae, idest, in ordine compositae.
Pars autem, quantum ad' tbtU~inteiiigendum, ~ id' est, a~.~otius~ensus "-intellectum.
Hoc autem ideo dictum est, nequisconetur ix~ in duas partes dividere, hoc est, iU ,~-z.Xi et r_~; non enim ad totum intelligendumhaec fitFb~ purpo~e~6fconstruc~ng our model we~eHail~interpret.
:minimu TMfree form as follows:?
A word'S'iS a sequence ofmubword units.
If this sequence may beuttered alone, then it is to be expected that the sequence co-occursfreely with other sequences.Under the classification of separabilite, Togeby places therequirement of Jakobson (1938) that words are the separable componentsof phrases : m4nlmal actually separable comuonants of the phrase.Conversely, the constituents of a word should not be separable.The general requirement of separablllte seems to be that a wordis a morpheme sequence which may co-occur with other morpheme sequencesto give granmmtical utterances.
If the sequence is a distinct word, thenits morphemes must be contiguous, and the morphemes of a noncontiguousgra,~natical sequence cannot be identified with the same word.Under permutabilite I, Togeby quotes HJelmelev(19%3) '_'les mots pourronttout s!-~lement ~tre d~finis c~ les signes minima dont l'?soression,J .et de m~eme l  contenu, sont recluroauement Dermutables " According toTogeby, HJelmslev means that "un changement de l'ordre des roots p0~rraentrainer un changement de sens.
tandis qu'un chan~ement de l'ordre des~rt ies du roots n'en sera pas capable.
"The requirement here is that if a sequence of morphemes is identifiedwith a word, then the order of the sequence must be invariant.In Greenberg (1957), the proposed definition of the word based onsubstitution and the recognition of grau~atical sequences, we interpretas follows:Let S he a sequence of linguistic units and G the class of graummticalsequences, in Greenberg's words the class of sequences which "exist asexpressions in the language.
"Suppose that S~ X A B C D E~G is a morpheme sequence.
We want todecide whether or not the boundary between B and C is a word boundary.3To each morpheme of S there corresponds a "nucleus."
For the nucleusof B to be a word terminal it is necessary that "infinite insertion" ofnuclei $ possible between B and C, otherwise if there"is a maximum tothe number of n~e i  that can be inserted," the boundary is "intra-wordboundary.
"Nuclei  are c lasses  of morpheme sequences having s t rong ly  equ iva lentsubst i tu t ion  proper t ies .
Some of the  cond i t ions  for  c lass  membershipare so s t r i c t  that  we would expect the  de f ined  c lasses  to  be empty fo rthe  language tak~en as a whole.
Perhaps as Chomsky con jec tures  in  a reviewof Greenberg 's  essay:  " I t  might be  that  the  not ion  of word may be dsfinedr~ la t iv~ to a par t i cu la r ly  s imp leset  of sentences .
(1958)In p rac t i ce ,  Greenberg 's  cond i t ions  might be in terpreted  as fo l lows:S= X A B C D E occurs in  the  language.
The subsequence BC may belong toa single word if it is replaceable by a single morpheme and gram~aticalityis preserved.
If for a small number of morpheme sequences Si, the sequer~esX A B S i C D E are grammatical, then the subsequence BC belongs to thesame word.
If the sequences X A B S i C D E are granm~tical for a largenumber of Si, then the subsequence BC probably does not belong to thesame word.In an unpublished ~,  Juilland develops a constructive definitionof the word which requires the recognition of gra~naticality.
IfS = X A B C D E~ G is a morpheme sequence, the boundary between B and Cis classified according to the potential sentence occurrences of B and B.Boundaries are classified as "conJunctive%r "disjunctive."
Disjunctiveboundaries isolate potential words called "functional units."
Conjunctiveboundaries occur potentially within words but must be tested by an"~nsertion criterion."
Thus if BC spans a conjunctive boundary,then B is a word boundary if there exists a morpheme sequence S i suchthat X A B S i C D Eg  G.The Use of Numerical Linguistic DataOur object now is to define a quantitative procedure forapproximating words.
Th~ procedure attempts to meet the variousrequirements summarized 21 the last section.
Since our interest isin distributional methods, we do not want the procedure to include anindependent test for gra~maticality.The requirements that we attempt to fulfill are summarized byJulliand as d ~  and ~eoarabilit~.
These are realized as a con~noncharacteristic in the procedures of Greenberg and Juilland: A potentialword is isolated as a sequence of morphemes which are associated insome special way, then the potential word is tested for its functionas a word, according to some test of insertion.Let us imagine a linguist confronted by the following data.Frequency refers to text frequency.
Let X A B C D E be a sequence ofmorphemes to be segmented, Consider the boundary between B and C .
Isthis boundary a word boundary?
Assume first that B occurs only withA, E, C and G, as indicated in Case i.Molpheme Pair Frequency Mcrpheme Pair FrequencyAB 4 BC ' 3EB '6 B~ 7Case iWith no further information, we might observe that B occurs morefrequently with A than with C, and segment as AB CD.
Under thiscondition the requirement of adhesion may be met, but a simple consid-eration of frequencies is not sufficient to meet the requirement ofseparability.
This is illustrated by the hypothetical set of data ofCase 2.Morpheme Pair FrequencyABEB iGB iHB iIB iJB iKB iMorpheme Pair FrequencyBC 3~F 7Case 2In this case the frequency of AB also exceeds the frequency ofBC, but the segmentation AB CD would not agree with linguisticintuition at all.
In Case 2, B has much greater freedom of combinationon the left than on the right, and to satisfy the condition ofseparability, at least approximately, we would segment as A BCD?In formalizing these intuitions, we refer to the procedure ofHarris (1955) for grouping phonemes into morphs.
Harris assumes thatan utterance U may be represented as a sequence of phonemes a I a 2 ... a n .Let R(al) be the number of different phonemes which may follow thephoneme a I in the total language.
Similarly, let R(al, a2) be the number6of different phonemes which may follow al, a 2 and so on.
Likewise, letL(an) be the number of different phonemes which may precedean, L(an_ 1 an)the number which may precede an-1 a n, and so on.
Then the sequenceSR = R(al) R(ala 2) R(ala2a 3) ..o R(al a 2 ... a n)describes the freedom of co-occurrence on the right at each phoneme ofU, and the sequenceSL = L(a I a 2 o..an) L(a 2 ... a n ) ... L(a n)describes th~ freedom of co-occurrence on the left at each phoneme of U.Harris observes that morpheme boundaries tend to occur at positionsin U where t~ corresponding values of R and L are large or attain theirrelative maxima.
Thus if R(a I ...ak) is a relative maximum in thesequence SR, then a k is a morpheme terminal.
Likewise a k is amorpheme terminal if R(a I ... ak) exceeds a value comparable to thetotal number of different phonemes in the language.
Under similarconditions for L(aj ... an) , aj is a morpheme initial.Applied to sequences of morphemes with uncontrolled diversity,Harris's procedure becomes particularly unwieldy.
We suggest that wemight achieve the same results as Harris by using fixed-length subsequencesrather than some higher-level syntactic unit.
Thus for some fixed k, theco-occurrence measuresRk(al...a k) Rk (a2.
?.ak+ l) ... Rk(an_k+l...a n )might yield the same segments as th~ sequenceR(al)R(ala 2) ... R(a I ...a n ) ?A Segmentation ProcedureThe placing of segment boundaries at positions of maximum freedomof combination realizes separability, but the requirement that a wordshould be a morpheme sequence showing strong internal association isaccounted for only in a negative way--we do not place boundaries atpositions of low freedom of combination.
We propose another procedurefor grouping morphemes by combining both left and right freedom ofco-occurrence.
As a result we derive a scale of degrees of distributionalseparation.In Harris's procedure there is sufficient information to form aranking of boundaries.
If al...a n is the sequence to be segmented thenwe place a boundary between ak and ~ ak+ 1 if one or more of the followingconditions is met.1.
R(alo..ak) is a relative maximum in SR.2.
L(ak+l...an) is a relative maximum in SL.3.
R or L are large in comparison with the number of differentphonemes.If any two of these conditions are satisfied, we have strongerdistributional evidence for segmentation than in the case of just onealone.
Likewise, if all three conditions are fulfilled, then~we wo'~ldexpect that a k would be a morpheme terminal more often than if just twoof the conditions are fulfilled.
We shall adopt a similar line ofreasoning to segmentations based on the distributions of fixed-lengthsequences.8For convenience we introduce some notation.
LetA B ) C D indicate a right-hand boundary after B,following from the distribution of B,andA B ( C D indicate a left-hand boundary before C,following from the distribution of C.In a "first-order segmentation" ~f the sequence XABCDE~ we willuse only the distributional properties of single morphemes.
Thus, inour hypothetical Case 2, we refer only to the distributional propertiesof B.Morpheme Pair FrequencyABEB 1GB 1HB 1IB 1JB 1KB 1Morpheme Pair FrequencyBC 3BF 7In this case the text frequencies indicate that B has much greaterfreedom of combination on the left than on the right?
Given no furtherinformation, we segment as A ( B C D. We formalize this decision in thefollowing "Cutting Rule.
"If R(B)~L(B) cut as X A B ) C O E.If R (B~L(B)  cut as X A ( B C D E.If R(B) = L(B) cut either as XA B ),C D E or asXA(BCDE.Let us insert right- or left-hand boundaries at C by use of thecutting rule, as we did with B.
The strangest evidence for segmentation(separability) is in the case where R(C)>L(C), so that we place aleft-hand boundary before C; and at the same time R(B)>L(B), so thatwe place a right-hand boundary after B.
The result is indicated asA B ) ( C D. The weakest evidence for segmentation (adhesion) iswhere R(B)~L(B), and~ the same time R(C)>L(B).
The result isindicated as A ( B C ) D.There are nine possible combinations according to the distributionalproperties of B and C. These are shown in Figure 1 , which we refer toas a "Segmentation Rule."
The number of slashes--the "degree" of theboundary--indicates the relative evidence for segmentation.~R~B) - L(B):,0=0<0R(c)  - L (C)>0 ----0 <0BIIo BIIIo BIIIICBIC BIIc BIIICBIc  BIIcFigure 1.
Segmentation RuleThe first sample which we will consider for purposes of illustrationis from the primer Ted and SaSAIy.
This text contains 121 different printers'words in all.
As in other deliberately morphemically closedlOtexts, Zipf's law does ne~ operate so we have a large variety ofcontextual combinations with many repetitions.
The sample consists ofthe first &,670 morphemes and forms the main narrative.
We obtain thesegments :come//Boots//sai d//Ted////come and//ride////come and//ride///in///my wagon////jump/in///Boots//sai d//Ted////ride/ / / in / / /my //wagon //Boots / / ~jump/inland//ride~I~~herellwellgollsai dl/TedThe foregoing segmentation is first order in that inference is madeusing only the distribution properties of single morphemes.
The proceduremay be extended to consider n-tuples of units for "n-th order" rules.However rules using extended context have two difficulties.
One is thesimple difficulty of finding enough context in a short text.
A second,more interesting restriction is that certain boundaries may not followeach other, depending on the order of the segmentation.
For example, twozero-degree boundaries may not follow each other under a rule of any order.The simple type counts, as measures of freedom of co-occurrence maybe replaced by other more general measures, for example the entropy E ofthe type-frequency distribution.
See, for example, Khinchin (1957).lEntropy has the desirable property that it may be used to estimate theaverage number of morphemes that may co-occur with a given unit.
Forexample, if the unit U has entropy ~(U)  of successors 3 then the 'rdiversity"llof successors is 2 E'U'.
(~ The entropy would be the same if all the 2 E(U)successors were equally likely.Evaluation ProceduresApplied to real data, the constructive procedures of Greenbergand Juilland are developed with the aid of many illustrative examples,but are still programmatic end have not been applied to large linguisticsamples.
Likewise, Harris gives the morphemic segmentation of manysentences but does not give a numerical evaluation of his results for alarge text.-In evaluating our approximation procedures, we will be concernedwith degrees of adequacy.
The results presented so far suggest thatthere is a strong correspondence between the degree of a segment boundaryand the corresponding syntactic boundary.
It appears that segmentboundaries of zero end first degrees correspond to intra-word boundaries,second-degree segment boundaries to word boundaries, and third-end fourth-degree boundaries to phrase and sentence boundaries.To determine the correspc~enee, we give a more precise formulation.In the morpheme sequence X A B C D E let B I and CI be the lowest levelconstituents containing B and C respectively.
It may happen that B I= Band C I = C. If BI and C I belong to the same printers' word, then the~ntactic boundary between B and C is a moroheme boundary.
If B I and C Ido not belong to the same printers' word, then the syntactic boundarybetween B and C is labeled according to the highest syntactic level ofB I CI.
or12Thus in the sequence ~ ~entlemanlv the space marks a morphemeboundary, since ungentlemanly is a printers' word.
However in the~L~g_~g~,  where B = ~ and C ~_~, B1 = the ~ of Englandand C 1 = 's.
Consequently we take the boundary between ~ and's as a phrase boundary. '
In the two word sequences, ~he man and hewent, the spaces mark word and phrase boundaries respectively.Between any two morphemes we have 20 possible combinations ofsyntactic and segment boundaries.
The correspondence my be e~mluatedby the ~ statistic, or derived statistics such as the contingencycoefficient C -~/ /~?~?
See, for axmmple, Kendall (1952).13Some Distributional GroupingsWe examine the correspondence between syntactic and segmentboundaries using several samples of morphemic data.In mmny cases a ~ero-degree pair occurs in a manner which is onlybarely statistically significant.
Let us compare.look SallyR/L ~3/1~ 39/33Sign (R-L) - +andcome andR/L 19/32 21/19Sign (R-L) - +For the sequence lo0kSall~, the differences (R-L) appear to bestatistically significant, but in _qg~,  we may wonder whether theslight positive value of R(and) - L(and) is due to sampling variations.lu a statistical version of our procedure, we test the hypothesisthat R(and)~L(and).
Since there is no exact sampling theory for thistest, we construct an approximate test.
The A6~6 morpheme text isdivided into approximately equal blocks, and R-L ce~puted for each blockseparately.
The values of R-L my be viewed as independent samples,provided the individual block size is large enough.
We infer from thesigns of R-L in each block that R(come)<L(come).
But we my not inferthat R(and)}L(and), since the positive difference occurs in only onetrial in five.
On the other hand, for the pair look ~ ,  R(look)<L(look) and R(Sally)>L(Sally) in all five blocks.Considering the ~6~6 morpheme text as a statistical sample, theinferred zero-degree segments are sai d, look Ted, l~k  Sally, andrun Ted.
If they occurred, run Sally, look Ted.
say Ted and saywould also have zero degree, while come run and come lo__~ would be ofsecond degree.The next sample is from a lower school r~ader.
The corpus isthe first 21OO morphemes from a simplified version of _RobinsQn Crusoe.Even though this text is simplified, it is fairly representative ofordinary language and the frequency distribution follows Zipf's law.The words are morphemically simple, but many morphemes occur only once.Eor the first sentence, the morphemic representation and the groupingsrelative to samples of the first 300, 600, ..., 2100 morphemes follow.The ship be ing fit ed out I go ed on board the one st ofSeptember 1659.Theshlp be ingfittedout lwent onboard thefirstofSpetamberl659Theship beingfittedout I went onboard thefir st of Septemberl659Theship beingfittedout I went onboard the first of Septemberl659The ship belngfittedout I went onboard the first of Septemberl659?
?
?The ship beingfittedout I went onboard the first of Septemberl659As soon as the sample reaches 12OO morphemes, the segmentationbecomes stable.
In this first sentence ing fltt ed and Qf September 1659remain unsegmented since fit, September, and 16~ occur only once each15and we lack distributional information.
The pairs ~ ,  fitted,e~,  and fir st are coextensive with printers' words.
On board showsstrong association and is operationally a word.
The morpheme the showsstrong disassociation in the context the////f~st, but neutral associationin the context the//shio.Several high-frequency morphemes tend to occur early in the textso that we have fairly extensive distributional information for the firstsentence, but less information for morphemes occurring later in the text.In this sample there are 124 different morphemes.
Of these 215 occuronly once and 82 only twice, so we have little information for segmentation.On the other hand, the high-frequency morphemes the, ship, be, in~, out,... all occur in the first sentence.
A consequence is the poor performanceof the procedure when applied to more than the first two sentences.
SeetableA final example is Quine's Word and Ob.iect.
We show the segmentationof this sentence relative to a sample of 900 morphemes.
Even though thewords tend to be polymorphic, the morphemic diversity is smaller thanthat found for the first 900 morphemes of Kobinson Crusoe.
The valuesare &.O and 5.1, respectively.
It follows that morphemic combinationin WQ~ and 0b~ect is more restrained and the occurrence of longer wordsdoes not imply more freedom of morphemic combination.The segmentation follows.For // the / case / of //// sent // ence s //// general ly ////how ever // or //// even // the / case / of // e tern al //sent // ence s //// general ly sure ly //// there // i s ///16no I thing III approach Ing a Ill/fixed Ill/stand ard //of 111/ how I far III in II direct IIII quotation H/my IIIde viate /// from // the // di root.The morpheme groupings are:For thecaseof sent ences generally however or eventhecaseof eternal sent ences generallysurely there isno thing approachinga fixed standard of howfar indirect quotation may deviate from the direct.Some n~nerical results are summarized in Table 1 .
The measuresof correspondence are between word boundaries and segment boundariesof degrees two, three, or four.
In Table I , Length refers to the textlength in morphemes, and N is the number of boundaries for which thecorrespondence measures were computed.TextTed and ~%llyRobinson CrusQeWord andOb~ec~Length~6~621009ooRuleSecondOrderFirstOrderFirstOrderN X 2 C Diversity197 10~.A .59 2.895  5.0 .O7 7.095 35.8 .85 ~.iTable i. Stmmmry of word and segment correspondences.The general conclusion is that words do co~respond to segments ofat least second degree in a statistically significant manner.
Thecorrespondence, however,, is dependent on text length and style.17Left-Right Linguistic AsymmetryIn applying Harris's procedure to our test data, we observe thatthe segments obtained from the R's alone were different from the segmentsobtained from the L's alone.Using entropy as a measure of freedom of co-occurrence, and seg-menting after each macimum in ~,  we obtain the first-order segments:come Boots / sa id  Ted /come and ride /come / and ride / in my wagon /jump in Boots / sa id  Ted /Placing a boundary before every maximum in EL, we obtain the segments:come / Boots sa id  Ted /come / and ride /come and ride in / my wagonjump in / Boots sai d / TedCombining E R and ~,  we obtain the segments:come // Boots // sa id  // Ted ////come and // ride ////come and // ride //// in r~y wagon ////jump in // Boots sai d Ted ////Notice that the segments following from the ~ 's  alone are in betteragreement with conventional syntactic units than those following from theEL'S alone.
Using just the EL'S we obtain: Boots sai d Ted, co~ andride______~, Boots said as segments which are not easily identifiable asphrases.Notice also that fourth-degree boundaries coincide more often withthose following f~om the ER's than those following fromthe EL'S?
Thissuggests that there is more information for segmentation in~foliowingunits as compared to preceding units.If we examine the phonemic examples in Harris's paper, e.g.~ s a y 1 o w w o h 1 z w ~ r ~ pR 5 29 15 15 28 7 5 29 7 l 8 29 29 7 2 29 9 29L 24 3 23 lO 2 27 5 3 23 16 1 8 18 23 2/~ 5 23 3_IorTh~ silo walls were upi t k a n t e y n z ~ i u w m i n ~ mR i0 28 ii ii 27 7 6 6 3 28 21 9 2 9 28 & i0 2 28L 22 19 21 1 1 7 7 3 7 16 22 1 1 1 2 1 5 13 9It contains aluminum.we find that the range of following phonemes is larger than that of thepreceding.
In It contains aluminum, for example, the range of successorsis 28-2 = 26 and that of predecessors is 22-1 = 21.
Moreover, the R'sand L's give different segments.
From the R's ~e obtainit/ k a n/teynz/@lu~n/in/B mFrom the L's alone we obtaini t~  @n/teynz/@ luwmin/@m19Another example of different segmentation resulting from followingand preceding units is found in ~on (1963).
In this study thelinguistic units were Fries' classes, and the sample a text of 5000 words.The second-order segments from the following classes areIf one believes/ that all questions raised/by science/...The reverse segmentation gives:If/one believes that all/questions raised by/scienceIn this text, the variance of E R is larger than that of E L.A related result is Johnson's (1965) experiment which relatesconstituent structure to memory blocks.
Carried out in reverse order,where Ss are expected to remember preceding words, constituents are notso well isolated.In our primer data, following morphemes are more variable thanpreceding morphemes.
Using entropy as a measure of diversity,E(E~) = E(EL) = 3.18,where E indicates expected value.
It may be shown that the expectedvalue of right and left entropies must be equal.
But for the varianceswe findVar(E R) = 2.33 and Var(EL) = 1.98.The difference Var(ER) - Var(EL) is significant for this sample.For the application of our segmentation rules it is of interest thatE R - E L is more closely correlated with E R than it is with E L .
And, in20fact, in all the English samples that we have considered, Var(ER) >Var(EL).Moreover, in these samples Cor(IE R - ELI , ER)~ Cor(JE R - ELI , EL) .
Thevariances and correlations are shown in Table 2.red and Sal_~yRobinson CrusoeWord andOb'ectLength~64621009OOVar(E R)2.333.632.19Var(E L)1.983.461.99Cor(4~-F~l ,~).61.32.37Cor(~R-EL J,EL).51.24.19Table 2.
Variances and Correlations.These measures of directional diversity apparently reflect that thelanguage is a unidirectional process.
This is to be expected in asuffixing language such as English.
We wonder if some directional asy~mmtryis a property of all natural languages.21Text Specific CompoundsOne purpose of this paper was to clarify the distributionalnature of the word.
The assumption has been that a word is a clusterof morphemes.
A quantification of what one might mean by "cluster ofmorphemes" leads to the segmentation rules, and we have presented theresults of their application in numerical detail.The hypothesis that words are clusters of morphemes according toour interpretation is partially verified by the data that have beenpresented, but the results remain suggestive rather than definitive.Printers' words and distributional groupings are coextensive with amuch greater-than-chance frequency.
Moreover, in one case at least,there is a close correspondence between the degree of distributionalseparation of morphemes and the corresponding syntactic boundaries.An ofttimes unstated assumption in statistical studies of languageis that the results would become better if the sample size were larger.This assumption is confirmed, but only in a restricted sense.
Inthe specialized language of the primer Ted and Sally, we used a largesample procedure to eliminate zero-degree segments and obtain acloser correspondence with printers' words.
This procedure is applicableto the closed vocabulary of this primer, in which every morpheme is usedmany times.
It would not be applicable to texts where Zipf's law holds.,,"and most morphemes are used only once.A study of the relationship between segmentation and sample sizeshows that segments are quite stable and do not change with respect tolonger and longer portions of a text.
In some cases, of course, larger22samples break up segments which occurred initially ~or lack of distribu-tional information.
The general conclusion is that the distributionalfreedom with respect to limited contexts may be established from rela-tively small samples.With regard to establishing the distributional reality of printers'wordsj morpheme segments of fixed order do not necessarily approachwords as the sample size increases.
The distributional clusters whichdo not correspond to printers' words furnish style indicators.
Thus, wehave the segments: lookTed, ~ in Ted an~Sally; onboard andon__shore in Robinson Crusoe; and ~ _ ~  and thecaseof in Word and Object.These stylistic groupings show the same strong association that is foundbetween the morphemes occurring within words.
TheBe groupings are notnecessarily the mcst frequent in a ?sample.The groups onboard, thecaseof, etc.
function as compounds in theirrespective texts.
We may speculate about the role of morpheme frequencyin the formation of compounds.
To use our theory in a predictive sense,we would assert morphemes showing strong association, in the sense we havedefined it, operate as compounds.Our rules enabla us to make statements about the relative ease ofcombination of linguistic units.
We have already pointed out that in the.~obinson Crusoe sample th_~e, in the context the / /h~ , shows neutralassociation, while in the context th__ee //// first, the disassociation isstrong.
A parallel example, also in Robinson Crusoe, is o nn where wefind omboard, onshore.
On the other hand, in the context of theprepositional phrases onus and ca them.
we find the nentral associationson II we and on II t__h~.23These examples suggest that there are degrees of distributionalfreedom and that instead of hoping to give an absolute distributionalcharacterization of the word, we should speak of degrees of distributionalword-hood.
The degree of b oundedness of the morphemes of a word is notan absolute property but depends on the corpus containing them, and inaddition the context of surrounding morphemes.Graphemic GroupingThe segmentation rules are numerical procedures for groupinglinguistic units.
Here we apply these rules to graphemic data.
For agraphemic application we compare Ted and Sall~ and Word ~nd 0b~ect.Using letters, we can process much larger samples than we could usingmorphemes.
Relative to the first 16,6AO letters of T~.
and Sally, weobtain the segmentsCome Boots said TedIn this simple text almost all words can be isolated from letter samples.In contrast, consider the sentence fra~nent from ~ord and 0b.~ect:What counts as a word as against a string ...Relative to a sample of 15,889 letters, the second-order se~nents frommaxlma in R:What counts asa word asa gain stas tring ...Frc~ maxima in L:Wh at counts asaw ord asaga ins tast ring ...Combining the information from the R s and L s we obtain the segments.Whatcounts asa word asaga insta string ...2~..... : " The :complexitY':of the ~ext ma~es a marked ~ifference '~  the oper-..... '~' :at'?O'n of O~"segmenta~'ion;ruie, Weobt~in manywo~ bo~d~ries but also'~ '~ ,  :ins{a', :~`'~ A te~ of :one'~s~labi~:wo~s Such~:as "Ta and Sally, suchcombinations: do not ocdur.
.
.
.
.  '
~ : : ' : "' ....... :A :text "~e~ed~te  to {he las'~' ~W0 iS the l~er  school ~eader AllAround Me.
The segmentation reiative tO .
.
.
.
.
.
.
.
.
.
.
.
15760 letters ~ows '~ an is elationof meaningful letter sequences, which are not necessarily words.
The textbegins :.
.
.
.
Now.Whi~ey was eleven years old, or thereabouts, He had .-~"s~n~i~n r~ ~ves i  " ' ~ .... '~ '  .. ~:NOW W~'..te~a ~ ~leven .y,ear s. oldor there about she had .
.
.This text illustrates that the segmentation ~ ' i~8 diStribUtion,giving X, 2, she .... ,":as s~ents\[  ::No p~ctuation was involved inr~h~~-t~ ~ ;~u%~ n:ot; ~h'~e-t~e '6~Se~ 'groupings ~' sUb~ant~y for, ?
i~ .!
'- :~ ~ ~: ~,~: : ~ ,  25  ~ ~'~EFERENCESBloomfield, Leonard, "A Set of Postulates for the Science of Language,"Lan~.,_2, 1926, pp.
153-16~.Bloomfield, Leonard, ~ ,  New York, 1933, p. 178Chomsky, Noam, Word, 12, 1958, p. 217.Gannnon, E., Prgc., IX Inter, Cong.
of Ling., 1963, pp.
507-13.Greenberg, Joseph, .Essays in Linguistics, Chicago, 1957, p. 27.Harris, James, Hermes or a Philosoohical Inquiry Concerning UniversalGra~,  London, 1771, pp.
20-21.Harris, Zellig, "From Phoneme to Morpheme," _~,  $1, 1955, pp.
190~23~.Hjelmslev, L., Omkrin~ Soro~teories Grundl~ggels?, 1943, p. 66.Jakobson, R., Actes d~ IV me C~ngre8 de Linaulstes, 193~, pp.
133-3~.Johnson, N.F., "The Psychological Reality of Phase Structure Rules",J. of Verbal Learnin~ and Verbal Beh~viQr ~, 1965, pp.
~69-~75.Juilland, A.
The Word, unpublished MSKendall, M.G., The Advanced Theory of Statistics.
Vol.
l, New York,1952, p. 290 et seq.Khinchin, A. I., _Mathem~ical Foundation of Information Theory, New York,1957, pp.
2-~.Togeby, Knud, "Qu'est-ce qu'un mot?"
Travau~ du C~rcle Ling~istioue deCopenha~e, V, 19~9, pp.
97-111.TEXT SAMPLESDefoe, Daniel, "Robinson Crusoe, " _Beacon Third Reader.
Ginn and Co.,Boston, 1914.26Francis, N., The Structure of American English, New York, 1959.Gates, A. I., and Bartlett, M. M., All Around Me, New York, 1957.Gates, A. I., Haber, M. B., and Salisbury, F.S., Ted and SallE, NewYork, 1957.Quine, W., .Word and Object, M.I.T., 1960, pp.
13-1A.27
