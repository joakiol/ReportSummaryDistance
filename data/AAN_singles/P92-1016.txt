UNDERSTANDINGNATURAL LANGUAGE INSTRUCTIONS:THE CASE OF PURPOSE CLAUSESBarbara Di Eugenio *Department of Computer and Information ScienceUniversity of PennsylvaniaPhiladelphia, PAdieugeni@linc.cis.upenn.eduABSTRACTThis paper presents an analysis of purpose clauses inthe context of instruction understanding.
Such analysisshows that goals affect the interpretation and / or exe-cution of actions, lends support o the proposal of usinggeneration and enablement to model relations betweenactions, and sheds light on some inference processesnecessary to interpret purpose clauses.INTRODUCTIONA speake~ (S) gives instructions to a hearer CrI) inorder to affect H's behavior.
Researchers including(Winograd, 1972), (Chapman, 1991), (Vere and Bick-more, 1990), (Cohen and Levesque, 1990), (Alterman etal., 1991) have been and are addressing many complexfacets of the problem of mapping Natural Language in-structions onto an agent's behavior.
However, an aspectthat no one has really considered is computing the ob-jects of the intentions H's adopts, namely, the actions tobe performed.
In general, researchers have equated suchobjects with logical forms extracted from the NL input.This is perhaps sufficient for simple positive impera-tives, but more complex imperatives require that actiondescriptions be computed, not simply extracted, from theinput instruction.
To clarify my point, consider:Ex.
1 a) Place a plank between two ladders.b) Place a plank between two laddersto create a simple scaffold.In both a) and b), the action to be executed is placea plank between two ladders.
However, Ex.
1.a wouldbe correctly interpreted by placing the plank anywherebetween the two ladders: this shows that in b) H mustbe inferring the proper position for the plank from theexpressed goal to create a simple scaffold.
Therefore,the goal an action is meant to achieve constrains theinterpretation and / or the execution of the action itself.The infinitival sentence in Ex.
1.b is a purpose clause,*Mailing addxess: IRCS - 3401, Walnut St - Suite 40(0 -Philadelphia, PA, 19104 - USA.which, as its name says, expresses the agent's purposein performing a certain action.
The analysis of purposeclauses is relevant to the problem of understanding Nat-ural Language instructions, because:1.
Purpose clauses explicitly encode goals and theirinterpretation shows that the goals that H adoptsguide his/her computation of the action(s) to per-form.2.
Purpose clauses appear to express generation or en-ablement, supporting the proposal, made by (Allen,1984), (Pollack, 1986), (Grosz and Sidner, 1990),(Balkansld, 1990), that these two relations are nec-essary m model actions.After a general description of purpose clauses, I willconcentrate on the relations between actions that theyexpress, and on the inference processes that their in-terpretation requires.
I see these inferences as instan-tiations of general accommodation processes necessaryto interpret instructions, where the term accommodationis borrowed from (Lewis, 1979).
I will conclude bydescribing the algorithm that implements the proposedinference processes.PURPOSE CLAUSESI am not the first one to analyze purpose clauses: how-ever, they have received attention almost exclusivelyfrom a syntactic point of view - see for example (Jones,1985), (l-Iegarty, 1990).
Notice that I am not using theterm purpose clause in the technical way it has beenused in syntax, where it refers to infinitival to clausesadjoined to NPs.
In contrast, the infinitival clauses Ihave concentrated on are adjoined to a matrix clause,and are termed rational clauses in syntax; in fact all thedata I will discuss in this paper belong to a particularsubclass of such clauses, subject-gap rational clauses.As far as I know, very little attention has been paidto purpose clauses in the semantics literature: in (1990),Jackendoff briefly analyzes expressions ofpurpose, goal,or rationale, normally encoded as an infinitival, in order120to-phrase, or for-phrase.
He represents hem by meansof a subordinating function FOR, which has the adjunctclause as an argument; in turn, FOR plus its argumentis a restrictive modifier of the main clause.
However,Jackendoff's semantic decomposition doesn't go beyondthe construction of the logical form of a sentence, andhe doesn't pursue the issue of what the relation betweenthe actions described in the matrix and adjunct really is.The only other work that mentions purpose clauses ina computational setting is (Balkanski, 1991).
However,she doesn't present any linguistic analysis of the data; asI will show, such analysis raises many interesting issues,such as t:?
It is fairly clear that S uses purpose clauses to explainto H the goal/~ to whose achievement the execution ofcontributes.
However, an important point that had beenoverlooked so far is that the goal/~ also constrains theinterpretation f ~, as I observed with respect to Ex.
1.b.Another example in point is:Ex.
2 Cut the square in half to create two triangles.The action to be performed is cutting the square in half.However, such action description is underspecified, inthat there is an infinite number of ways of cutting asquare in half: the goal create two triangles restrictsthe choice to cutting the square along one of the twodiagonals.?
Purpose clauses relate action descriptions at differentlevels of abstraction, such as a physical action and anabstract process, or two physical actions, but at differentlevels of granularity:Ex.
3 Heat on stove to simmer.?
As far as what is described in purpose clauses, I havebeen implying that both matrix and purpose clauses de-scribe an action, c~ and/~ respectively.
There are rarecases - in fact, I found only one - in which one of thetwo clauses describes a state ~r:Ex.
4 To be successfully covered, a wood wall must beflat and smooth.I haven't found any instances in which both matrix andpurpose clauses describe a state.
Intuitively, this makessense because S uses a purpose clause to inform H ofthe purpose of a given action 2?
In most cases, the goal /~ describes a change in theworld.
However, in some cases1.
The change is not in the world, but in H's knowl-edge.
By executing o~, H can change the state ofhis knowledge with respect to a certain propositionor to the value of a certain entity.1I collected one hundred and one consecutive instances ofpurpose clauses from a how-to-do book on installing wall cov-erings, and from two craft magazines.~There are clearly other ways of describing that a state isthe goal of a certain action, for example by means of so~suchthat, but I won't deal with such data here.Ex.
5 You may want to hang a coordinating borderaround the room at the top of the walls.
To deter-mine the amount of border, measure the width (infeet) of all walls to be covered and divide by three.Since borders are sold by the yard, this will give youthe number of yards needed.Many of such examples involve verbs such ascheck, make sure etc.
followed by a that-complement describing a state ~b.
The use of suchverbs has the pragmatic effect hat not only does Hcheck whether ~b holds, but, if ~b doesn't hold, s/hewill also do something so that ff comes to hold.Ex.
6 To attach the wires to the new switch, use thepaper clip to move the spring type clip aside andslip the wire into place.
Tug gently on each wire tomake sure it 's secure.2.
The purpose clause may inform H that the worldshould not change, namely, that a given eventshould be prevented from happening:Ex.
7 Tape raw edges of fabric to prevent hreadsfrom raveling as you work.?
From a discourse processing point of view, interpret-ing a purpose clause may affect he discourse model, inparticular by introducing new referents.
This happenswhen the effect of oL is to create a new object, and/~identifies it.
Verbs frequently used in this context arecreate, make, form etc.Ex.
8 Join the short ends of the hat band to form a circle.Similarly, in Ex.
2 the discourse referents for the tri-angles created by cutting the square in half, and in Ex.
5the referent for amount of border are introduced.RELAT IONS BETWEEN ACT IONSSo far, I have mentioned that oe contributes to achiev-ing the goal/~.
The notion of contribution can be mademore specific by examining naturally occurring purposeclauses.
In the majority of cases, they express genera-tion, and in the rest enablement.
Also (Grosz and Sid-ner, 1990) use contribute as a relation between actions,and they define it as a place holder for any relation ...that can hold between actions when one can be said tocontribute (for example, by generating or enabling) tothe performance of the other.
However, they don't jus-tify this in terms of naturally occurring data.
Balkanski(1991) does mention that purpose clauses express gen-eration or enablement, but she doesn't provide evidenceto support his claim.GENERATIONGeneration is a relation between actions that has beenextensively studied, first in philosophy (Goldman, 1970)and then in discourse analysis (Allen, 1984), (Pollack,1986), (Grosz and Sidner, 1990), (Balkanski, 1990).According to Goldman, intuitively generation is the re-lation between actions conveyed by the preposition byin English - turning on the light by flipping the switch.121More formally, we can say that an action a conditionallygenerates another action/~ iff 3:1. a and/~ are simultaneous;2. a is not part of doing/~ (as in the case of playinga C note as part of playing a C triad on a piano);3. when a occurs, a set of conditions C hold, such thatthe joint occurrence of a and C imply the occur-rence o f /L  In the case of the generation relationbetween flipping the switch and turning on the light,C will include that the wire, the switch and the bulbare working.Although generation doesn't hold between o~ and fl ifis part of a sequence of actions ,4 to do/~, generationmay hold between the whole sequence ,4 and/~.Generation is a pervasive relation between action de-scriptions in naturally occurring data.
However, it ap-pears from my corpus that by clauses are used less fre-quently than purpose clauses to express generation 4:about 95% of my 101 purpose clauses express gener-ation, while in the same corpus there are only 27 byclauses.
It does look like generation i instructional textis mainly expressed by means of purpose clauses.
Theymay express either a direct generation relation betweenand/~, or an indirect generation relation betweenand/~, where by indirect generation I mean that ~ be-longs to a sequence of actions ,4 which generates 8.ENABLEMENTFollowing first Pollack (1986) and then Balkanski(1990), enablement holds between two actions ~ and/~ if and only if an occurrence of ot brings about a set ofconditions that are necessary (but not necessarily suffi-cien 0 for the subsequent performance of 8.Only about 5% of my examples express enablement:Ex.
9 Unscrew the protective plate to expose the box.Unscrew the protective plate enables taking the plate offwhich generates exposing the box.GENERATION AND ENABLEMENT INMODELING ACTIONSThat purpose clauses do express generation and enable-ment is a welcome finding: these two relations havebeen proposed as necessary to model actions (Allen,1984), (Pollack, 1986), (Grosz and Sidner, 1990),(Balkanski, 1990), but this proposal has not been jus-tiffed by offering an extensive analysis of whether andhow these relations are expressed in NL.3Goldman distinguishes among four kinds of generation re-lations: subsequent work has been mainly influenced by con-ditional generation.4Generation can also be expressed with a simple free ad-junct; however, this use of free adjuncts is not very common- see 0hrebber and Di Eugenio, 1990).122A further motivation for using generation and enable-ment in modeling actions is that they allow us to drawconclusions about action execution as well - a particu-larly useful consequence given that my work is takingplace in the framework of the Animation from NaturalLanguage - AnimNL project (Badler eta/., 1990; Web-ber et al, 1991) in which the input instructions do haveto be executed, namely, animated.As has already been observed by other researchers, ffgenerates /~, two actions are described, but only a,the generator, needs to be performed.
In Ex.
2, there isno creating action per se that has to be executed: thephysical action to be performed is cutting, constrainedby the goal as explained above.In contrast to generation, if a enables/~, after execut-ing or, fl still needs to be executed: a has to temporallyprecede/~, in the sense that a has to begin, but not nec-essarily end, before/3.
In Ex.
10, ho/d has to continuefor the whole duration offal/:Ex.
10 Hold the cup under the spigot o fill it with coffee.Notice that, in the same way that the generatee affectsthe execution of the generator, so the enabled actionaffects the execution of the enabling action.
Considerthe difference in the interpretation of to in go to themirror, depending upon whether the action to be enabledis seeing oneself or carrying the mirror somewhere else.INFERENCE PROCESSESSo far, I have been talking about the purpose clauseconstraining the interpretation of the matrix clause.
Iwill now provide some details on how such constraintsare computed.
The inferences that I have identified sofar as necessary to interpret purpose clauses can be de-scribed as1.
Computing a more specific action description.2.
Computing assumptions that have to hold for a cer-tain relation between actions to hold.Computing more specific action descriptions.In Ex.
2 - Cut the square in half to create two triangles- it is necessary to find a more specific action a l  whichwill achieve the goal specified by the purpose clause, asshown in Fig.
1.For Ex.
2 we have fl = create two triangles, o~ =cut the square in half, ~1 = cut the square in half alongthe diagonal.
The reader will notice that the inputs toaccommodation are linguistic expressions, while its out-puts are predicate - argument structures: I have usedthe latter in Fig.
1 to indicate that accommodation infersrelations between action types.
However, as I will showlater, the representation I adopt is not based on predi-cate - argument structures.
Also notice that I am usingGreek symbols for both linguistic expressions and actiontypes: the context should be sufficient o disambiguatewhich one is meant.Computing assumptions.
Let's consider:(create two(cut thetriangles)square in hal0> accommodation(create (agent, two-triangles))/~ (cut ~g~~ilt (2g21t' sZial~ng~~igonal)))Figure 1: Schematic depiction of the first kind of accommodationaccommodationA A .
.
.
A ....l 2 1?gFigure 2: Schematic depiction of the second kind of accommodationEx.
11 Go into the other room to get the urn of coffee.Presumably, H doesn't have a particular plan that dealswith getting an urn of coffee.
S/he will have a genericplan about get x, which s/he will adapt o the instructionsS gives him 5.
In particular, H has to find the connectionbetween go into the other room and get the urn of coffee.This connection requires reasoning about the effects ofgo with respect o the plan get x; notice that the (mostdirec0 connection between these two actions requiresthe assumption that the referent of the urn of coffee isin the other room.
Schematically, one could representthis kind of inference as in Fig.
2 - /~ is the goal, ~ theinstruction to accommodate, Ak the actions belongingto the plan to achieve t ,  C the necessary assumptions.It could happen that these two kinds of inference needto be combined: however, no example I have found sofar requires it.INTERPRETING Do a to do I~In this section, I will describe the algorithm that im-5Actually H may have more than one single plan for get x,.in which case go into the other room may in fact help to selectthe plan the instructor has in mind.123plements the two kinds of accommodation described inthe previous section.
Before doing that, I will makesome remarks on the action representation I adopt andon the structure of the intentions - the plan graph - thatmy algorithm contributes to building.Action representation.
To represent action types, I usean hybrid system (Brachman et al, 1983), whose primi-tives are taken from Jackendoff's Conceptual Structures(1990); relations between action types are represented inanother module of the system, the action library.I'd like to spend a few words justifying the choiceof an hybrid system: this choice is neither casual, nordetermined by the characteristics of the AnimNL project.Generally, in systems that deal with NL instructions,action types are represented as predicate - argumentstructures; the crucial assumption is then made that thelogical form of an input instruction will exactly matchone of these definitions.
However, there is an infinitenumber of NL descriptions that correspond to a basicpredicate - argument structure: just think of all the pos-sible modifiers that can be added to a basic sentencecontaining only a verb and its arguments.
Therefore itis necessary to have a flexible knowledge representationsystem that can help us understand the relation betweenthe input description and the stored one.
I claim thathybrid KR systems provide such flexibility, given theirvirtual lattice structure and the classification algorithmoperating on the lattice: in the last section of this paperI will provide an example supporting my claim.Space doesn't allow me to deal with the reason whyConceptual Structures are relevant, namely, that they areuseful to compute assumptions.
For further details, theinterested reader is referred to (Di Eugenio, 1992; DiEugenic) and White, 1992).Just a reminder to the reader that hybrid systems havetwo components: the terminological box, or T-Box,where concepts are defined, and on which the classi-fication algorithm works by computing subsumption re-lations between different concepts.
The algorithm is cru-cial for adding new concepts to the KB: it computes thesubsumption relations between the new concept and allthe other concepts in the lattice, so that it can "Position"the new concept in the right place in the lattice.
Theother component of an hybrid system is the assertionalbox, or A-box, where assertions are stored, and whichis equipped with a theorem-prover.In my case, the T-Box contains knowledge about ac-tion types, while assertions about individual actions -instances of the types - are contained in the A-Box:such individuals correspond to the action descriptionscontained in the input instructions 6The action library contains imple plans relating ac-tions; simple plans are either generation or enablementrelations between pairs: the first member of the pair iseither a single action or a sequence of action, and thesecond member is an action.
In case the first member ofthe pair is an individual action, I will talk about directgeneration or enablement.
For the moment, generationand enablement are represented in a way very similar to(Balkanski, 1990).The plan graph represents the structure of the inten-tions derived from the input instructions.
It is composedof nodes that contain descriptions of actions, and arcsthat denote relations between them.
A node containsthe Conceptual Structures representation f an action,augmented with the consequent s ate achieved after theexecution of that action.
The arcs represent, among oth-ers: temporal relations; generation; enablement.The plan graph is built by an interpretation algorithmthat works by keeping track of active nodes, which forthe moment include the goal currently in focus and thenodes just added to the graph; it is manipulated by var-ious inference processes, such as plan expansion, andplan recognition.My algorithm is described in Fig.
3 7.
Clearly theinferences I describe are possible only because I rely~Notice that these individuals are simply instances ofgeneric concepts, and not necessarily action tokens, namely,nothing is asserted with regard to their happening in the world.rAs I mentioned earlier in the paper, the Greek symbolson the other AnimNL modules for 1) parsing the in-put and providing a logical form expressed in terms ofConceptual Structures primitives; 2) managing the dis-course model, solving anaphora, performing temporalinferences etc (Webber eta/., 1991).AN EXAMPLE OF THE ALGORITHMI will conclude by showing how step 4a in Fig.
3 takesadvantage of the classification algorithm with which hy-brid systems are equipped.Consider the T-Box, or better said, the portion of T-Box shown in Fig.
4 s.Given Ex.
2 - Cut the square in half to create twotriangles - as input, the individual action descriptioncut (the) square in half will be asserted in the A-Boxand recognized as an instance of ~ - the shaded conceptcut (a) square in half - which is a descendant of cutand an abstraction of o: - cut (a) square in half alongthe diagonal, as shown in Fig.
5 9.
Notice that thisdoes not imply that the concept cut (a) square in halfis known beforehand: the classification process is ableto recognize it as a virtual concept and to find the rightplace for it in the lattice 10.
Given that a is ancestorof o J, and that oJ generates/~ - create two triangles, thefact that the action to be performed is actually o~ and notoL can be inferred.
This implements step 4(a)ii.The classification process can also help to deal withcases in which ~ is in conflict with to - step 4(a)iv.
Ifwere cut (a) square along a perpendicular axis, a con-flict with o~ - cut (a) square in half along the diagonal- would be recognized.
Given the T-Box in fig.
4, theclassification process would result in o~ being a sister tow: my algorithm would try to unify them, but this wouldnot be possible, because the role fillers of location onand w cannot be unified, being along(perpendicular-axis) and along(diagonal) respectively.
I haven't ad-dressed the issue yet of which strategies to adopt in casesuch a conflict is detected.Another point left for future work is what to do whenstep 2 yields more than one simple plan.The knowledge representation system I am using isBACK (Peltason et al, 1989); the algorithm is beingimplemented in QUINTUS PROLOG.refer both to input descriptions and to action types.SThe reader may find that the representation in Fig.
4 isnot very perspicuous, as it mixes linguistic expressions, uchas along(diagonal), with conceptual knowledge about entities.Actually, roles and concepts are expressed in terms of Con-ceptual Structures primitives, which provide a uniform wayof representing knowledge apparently belonging to differenttypes.
However, a T-Box expressed in terms of ConceptualStructures becomes very complex, so in Fig.
4 I adopted amore readable representation.9The agent role does not appear on cut square in half inthe A-Box for the sake of readability.1?In fact, such concept is not really added to the lattice.124Input: the Conceptual Structures logical forms for ~ and t ,  the current plan graph, and the list of active nodes.1.
Add to A-Box individuals corresponding to the two logical forms.
Set flag ACCOM if they don't exactly matchknown concepts.2.
Retrieve from the action library the simple plan(s) associated with /5 - generation relations in which /5 is thegenerate., enablement relations in which/5 is the enablee.3.
I f  ACCOM is not set(a) If  there is a direct generation or enablement relation between ~ and/5, augment plan graph with the structurederived from it, after calling compute-assumpt ions .
(b) If  there is no such direct relation, recursively look for possible connections between e and the components 7iof sequences that either generate or enable/5.Augment plan graph, after calling c omput  e -  a s s umpt i on s.4.
I f  ACCOM is set,(a) If  there is ~a such that oJ directly generates or enables/5, check whetheri.
w is an ancestor of c~: take c~ as the intended action.ii.
~o is a descendant of c~: take o~ as the intended action.iii.
I f  w and e are not ancestors of each other, but they can be unified - all the information they provideis compatible, as in the case of cut square in half along diagonal and cut square carefully - then theirunification w U c~ is the action to be executed.iv.
I f  o: and ~ are not ancestors of each other, and provide conflicting information - such as cut square alongdiagonal and cut square along perpendicular axis - then signal failure.
(b) If  there is no such w, look for possible connections between ~ and the components 7i of sequences that eithergenerate or enable/5, as in step 3b.
Given that ~ is not known to the system, apply the inferences describedin 4a to c~ and 7/.Figure 3: The algorithm for Do ~ to do125O earnest @ roleV/R (Value Rcm~iction)/  ,.on ....Figure 4: A portion of the action hierarchyindividual,,,.,,.,,,,,..,, instantiatesT_ .OX -,,i .\~ ,~ /--~location / /A-BOXFigure 5: Dealing with less specific action descriptions126CONCLUSIONSI have shown that the analysis of purpose clauseslends support o the proposal of using generation andenablement tomodel actions, and that the interpretationof purpose clauses originates pecific inferences: I haveillustrated two of them, that can be seen as examples ofaccommodation processes (Lewis, 1979), and that showhow the bearer's inference processes are directed by thegoal(s) s/he is adopting.Future work includes fully developing the action rep-resentation formalism, and the algorithm, especially thepart regarding computing assumptions.ACKNOWLEDGEMENTSFor financial support I acknowledge DARPA grant no.N0014-90-J-1863 and ARt  grant no.
DAALO3-89-C0031PR1.
Thanks to Bonnie Webber for support, in-sights and countless discussions, and to all the membersof the AnimNL group, in particular to Mike White.
Fi-nally, thanks to the Dipartimento di Informatica - Uni-versita' di Torino - Italy for making their computingenvironment available to me, and in particular thanks toFelice Cardone, Luca Console, Leonardo Lesmo, andVincenzo Lombardo, who helped me through a lastminute computer crash.References(Allen, 1984) James Allen.
Towards a general theoryof action and time.
Artificial Intelligence, 23:123-154, 1984.
(Alterman eta/., 1991) Richard Alterman, Roland Zito-Wolf, and Tamitha Carpenter.
Interaction, Com-prehension, and Instruction Usage.
Technical Re-port CS-91-161, Dept.
of Computer Science, Cen-ter for Complex Systems, Brandeis University,1991.
(Badler et al, 1990) Norman Badler, Bonnie Webber,Jeff Esakov, and Jugal Kalita.
Animation from in-slzuctions.
In Badler, Barsky, and Zeltzer, editors,Making them Move, MIT Press, 1990.
(Balkanski, 1990) Cecile Balkanski.
Modelling act-typerelations in collaborative activity.
Technical Re-port TR-23-90, Center for Research in ComputingTechnology, Harvard University, 1990.
(Balkanski, 1991) Cecile Balkanski.
Logical form ofcomplex sentences in task-oriented dialogues.
InProceedings of the 29th Annual Meeting of the ACL,Student Session, 1991.
(Brachman et al, 1983) R. Brachman, R.Fikes, and H.Levesque.
KRYPTON: A Functional Approachto Knowledge Representation.
Technical Re-port FLAIR 16, Fairchild Laboratories for ArtificialIntelligence, Palo Alto, California, 1983.
(Chapman, 1991) David Chapman.
Vision, InstructionandAction.
Cambridge: MIT Press, 1991.127(Cohen and Levesque, 1990) Philip Cohen and HectorLevesque.
Rational Interaction as the Basis forCommunication.
In J. Morgan, P. Cohen, andM.
Pollack, editors, Intentions in Communication,MIT Press, 1990.
(Di Eugenio, 1992) Barbara DiEugenio.
Goals andAc-tions in Natural Language Instructions.
TechnicalReport MS-CIS-92-07, University of Pennsylvania,1992.
(Di Eugenio and White, 1992) Barbara Di Eugenio andMichael White.
On the Interpretation of NaturalLanguage Instructions.
1992.
COLING 92.
(Goldman, 1970) Alvin Goldman.
A Theory of HwnanAction.
Princeton University Press, 1970.
(Grosz and Sidner, 1990) Barbara Grosz and CandaceSidner.
Plans for Discourse.
In J. Morgan, P. Co-hen, and M. Pollack, editors, Intentions in Commu-nication, MIT Press, 1990.
(Hegarty, 1990)Michael Hegarty.
Secondary Predi-cation and Null Operators in English.
1990.Manuscript.
(Jackendoff, 1990) Ray Jackendoff.
Semantic Struc-tures.
Current Studies in Linguistics Series, TheMIT Press, 1990.
(Jones, 1985) Charles Jones.
Agent, patient, and con-trol into purpose clauses.
In Chicago LinguisticSociety, 21, 1985.
(Lewis, 1979) David Lewis.
Scorekeeping in a lan-guage game.
Journal of Philosophical Language,8:339-359, 1979.
(Peltason et al, 1989) C. Peltason, A. Schmiedel, C.Kindermann, and J. Quantz.
The BACK SystemRevisited.
Technical Report KIT 75, TechnischeUniversitaet Berlin, 1989.
(Pollack, 1986) Martha Pollack.
Inferring domain plansin question-answering.
PhD thesis, University ofPennsylvania, 1986.
(Vere and Bickmore, 1990) Steven Vere and TimothyBickmore.
A basic agent.
Computational Intel-ligence, 6:41--60, 1990.
(Webber and Di Eugenio, 1990) Bonnie Webber andBarbara Di Eugenio.
Free Adjuncts in Natural Lan-guage Instructions.
In Proceedings Thirteenth In-ternational Conference on Computational Linguis-tics, COLING 90, pages 395--400, 1990.
(Webber et al, 1991) Bonnie Webber, Norman Badler,Barbara Di Eugenio, Libby Levison, and Michaelwhite.
Instructing Animated Agents.
In Proc.
US-Japan Workshop on Integrated Systems in Multi-Media Environments.
Las Cruces, NM, 1991.
(Winograd, 1972) Terry Winograd.
Understanding Nat-ural Language.
Academic Press, 1972.
