Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 91?96,Baltimore, Maryland USA, June 23-24, 2014.c?2014 Association for Computational LinguisticsAutomatic Annotation Suggestions andCustom Annotation Layers in WebAnnoSeid Muhie Yimam1Richard Eckart de Castilho2Iryna Gurevych2,3Chris Biemann1(1) FG Language Technology, Dept.
of Computer Science, Technische Universit?at Darmstadt(2) Ubiquitous Knowledge Processing Lab (UKP-TUDA)Dept.
of Computer Science, Technische Universit?at Darmstadt(3) Ubiquitous Knowledge Processing Lab (UKP-DIPF)German Institute for Educational Research and Educational Informationhttp://www.
{lt,ukp}.tu-darmstadt.deAbstractIn this paper, we present a flexible ap-proach to the efficient and exhaustive man-ual annotation of text documents.
For thispurpose, we extend WebAnno (Yimam etal., 2013) an open-source web-based an-notation tool.1While it was previouslylimited to specific annotation layers, ourextension allows adding and configuringan arbitrary number of layers through aweb-based UI.
These layers can be an-notated separately or simultaneously, andsupport most types of linguistic annota-tions such as spans, semantic classes, de-pendency relations, lexical chains, andmorphology.
Further, we tightly inte-grate a generic machine learning compo-nent for automatic annotation suggestionsof span annotations.
In two case studies,we show that automatic annotation sug-gestions, combined with our split-pane UIconcept, significantly reduces annotationtime.1 IntroductionThe annotation of full text documents is a costlyand time-consuming task.
Thus, it is important todesign annotation tools in such a way that the an-notation process can happen as swiftly as possible.To this end, we extend WebAnno with the capabil-ity of suggesting annotations to the annotator.A general-purpose web-based annotation toolcan greatly lower the entrance barrier for linguisticannotation projects, as tool development costs andpreparatory work are greatly reduced.
WebAnno1.0 only partially fulfilled desires regarding gen-erality: Although it covered already more kinds1WebAnno is open-source software under the terms of theApache Software License 2.0.
This paper describes v1.2:http://webanno.googlecode.comof annotations than most other tools, it supportedonly a fixed set of customizable annotation lay-ers (named entities, part-of-speech, lemmata, co-reference, dependencies).
Thus, we also remove alimitation of the tool, which was previously boundto specific, hardcoded annotation layers.We have generalized the architecture to supportthree configurable generic structures: spans, rela-tions, and chains.
These support all of the originallayers and allow the user to define arbitrary customannotation layers based on either of these struc-tures.
Additionally, our approach allows maintain-ing multiple properties on annotations, e.g.
to sup-port morphological annotations, while previouslyonly one property per annotation was supported.Automatic suggestion of annotations is basedon machine learning, which is common practicein annotation tools.
However, most of existingweb-based annotation tools, such as GATE (Cun-ningham et al., 2011) or brat (Stenetorp et al.,2012), depend on external preprocessing and post-processing plugins or on web services.
These toolshave limitations regarding adaptability (difficultyto adapt to other annotation tasks), reconfigurabil-ity (generating a classifier when new features andtraining documents are available is complicated),and reusability (requires manual intervention toadd newly annotated documents into the iteration).For our approach, we assume that an annota-tor actually does manually verify all annotationsto produce a completely labeled dataset.
This taskcan be sped up by automatically suggesting anno-tations that the annotator may then either acceptor correct.
Note that this setup and its goal differsfrom an active learning scenario, where a systemactively determines the most informative yet unan-notated example to be labeled, in order to quicklyarrive at a high-quality classifier that is then to beapplied to large amounts of unseen data.Our contribution is the integration of machinelearning into the tool to support exhaustive an-91notation of documents providing a shorter loopthan comparable tools (Cunningham et al., 2011;Stenetorp et al., 2012), because new documentsare added to the training set as soon as they arecompleted by the annotators.
The machine learn-ing support currently applies to sequence classifi-cation tasks only.
It is complemented by our ex-tension allowing to define custom annotation lay-ers, making it applicable to a wide range of anno-tation tasks with only little configuration effort.Section 2 reviews related work about the uti-lization of automatic supports and customiza-tion of annotation schemes in existing annotationtools.
The integration of automatic suggestionsinto WebAnno, the design principles followed, andtwo case studies are explained in Section 3.
Sec-tion 4 presents the implementation of customiz-able annotation layers into the tool.
Finally, Sec-tion 5 summarizes the main contributions and fu-ture directions of our work.2 Related WorkAutomatic annotation support The impact ofusing lexical and statistical resources to producepre-annotation automatically to increase the anno-tation speed has been studied widely for variousannotation tasks.
For the task of medical namedentity labeling, Lingren et al.
(2013) investigatethe impact of automatic suggestions on annotationspeed and potential biases using dictionary-basedannotations.
This technique results in 13.83% to21.5% time saving and in an inter-annotator agree-ment (IAA) increase by several percentage points.WordFreak (Morton and LaCivita, 2003) in-cludes an automation component, where instanceswith a low machine learning confidence are pre-sented for annotation in an active learning setup.Beck et al.
(2013) demonstrate that the use of ac-tive learning for machine translation reduces theannotation effort and show a reduced annotationload on three out of four datasets.The GoldenGATE editor (Sautter et al., 2007)integrates NLP tools and assistance features formanual XML editing.
The tool is used in correct-ing/editing an automatically annotated documentwith an editor where both text and XML markupsare modified.
GoldenGATE is merely used to fa-cilitate the correction of an annotation while pre-annotation is conducted outside of the tool.Automatic annotation support in brat (Stenetorpet al., 2012) was carried out for a semantic classdisambiguation task to investigate how such au-tomation facilitates the annotators?
progress.
Theyreport a 15.4% reduction in total annotation time.However, the automation process in brat 1) de-pends on bulk annotation imports and web serviceconfigurations, which is labor intensive, 2) is taskspecific so that it requires a lot of effort to adapt itto different annotation tasks, 3) there is no way ofusing the corrected result for the next iteration oftraining the automatic tool.The GATE Teamware (Bontcheva et al., 2013)automation component is most similar to ourwork.
It is based either on plugins and externallytrained classification models, or uses web services.Thus, it is highly task specific and requires exten-sive configuration.
The automatic annotation sug-gestion component in our tool, in contrast, is easilyconfigurable and adaptable to different annotationtasks and allows the use of annotations from thecurrent annotation project.Custom annotation layers Generic annotationdata models are typically directed graph models(e.g.
GATE, UIMA CAS (G?otz and Suhre, 2004),GrAF (Ide and Suderman, 2007)).
In addition, anannotation schema defines possible kinds of anno-tations, their properties and relations.
While thesemodels offer great expressiveness and flexibility, itis difficult to adequately transfer their power intoa convenient annotation editor.
For example, oneschema may prescribe that the part-of-speech tagis a property on a Token annotation, another onemay prescribe that the tag is a separate annotation,which is linked to the token.
An annotator shouldnot be exposed to these details in the UI and shouldbe able to just edit a part-of-speech tag, ignorant ofthe internal representation.This problem is typically addressed in twoways.
Either, the full complexity of the annota-tion model is exposed to the annotator, or the an-notation editor uses a simplified model.
The firstapproach can easily lead to an unintuitive UI andmake the annotation an inconvenient task.
Thesecond approach (e.g.
as advocated by brat) re-quires the implementation of specific import andexport filters to transform between the editor datamodel and the generic annotation data models.We propose a third approach integrating a con-figurable mapping between a generic annotationmodel (UIMA CAS) and a simplified editingmodel (brat) directly into the annotation tool.Thus, we avoid exposing the full complexity of92the generic model to the user and also avoid thenecessity for implementing import/export filters.Similar approaches have already been used to mapannotation models to visualization modules (cf.
(Zeldes et al., 2009)), but have, to our knowledge,not been used in an annotation editor.
Our ap-proach is different from schema-based annotationeditors (e.g.
GATE), which employ a schema asa template of properties and controlled vocabular-ies that can be used to annotate documents, butwhich do not allow to map structures inherent inannotations, like relations or chains, to respectiveconcepts in the UI.3 Automatic Annotation SuggestionsIt is the purpose of the automatic annotation sug-gestion component to increase the annotation ef-ficiency, while maintaining the quality of annota-tions.
The key design principle of our approach isa split-pane (Figure 1) that displays automatic an-notation suggestions in the suggestion pane (lowerpart) and only verified or manual ones in the anno-tation pane (upper part).
In this way, we force theannotators to review each automatic suggestion asto avoid overlooking wrong suggestions.Figure 1: Split-pane UI.
Upper: the annotationpane, which should be completed by the annotator.Lower: the suggestion pane, displaying predic-tions or automatic suggestions, and coding theirstatus in color.
This examples shows automaticsuggestions for parts-of-speech.
Unattended anno-tations are rendered in blue, accepted annotationsin grey and rejected annotations in red.
Here, thelast five POS annotations have been attended, fourhave been accepted by clicking on the suggestion,and one was rejected by annotating it in the anno-tation pane.3.1 Suggestion modesWe distinguish three modes of automatic annota-tion suggestion:Correction mode In this mode, we import doc-uments annotated by arbitrary external tools andpresent them to the user in the suggestion paneof the annotation page.
This mode is specifi-cally appropriate for annotation tasks where a pre-annotated document contains several possibilitiesfor annotations in parallel, and the user?s task isto select the correct annotation.
This allows toleverage specialized external automatic annotationcomponents, thus the tool is not limited to the in-tegrated automation mechanism.Repetition mode In this mode, further occur-rences of a word annotated by the user are high-lighted in the suggestion pane.
To accept sugges-tions, the user can simply click on them in the sug-gestion pane.
This basic ?
yet effective ?
sugges-tion is realized using simple string matching.Learning mode For this mode, we have inte-grated MIRA (Crammer and Singer, 2003), an ex-tension of the perceptron algorithm for online ma-chine learning which allows for the automatic sug-gestions of span annotations.
MIRA was selectedbecause of its relatively lenient licensing, its goodperformance even on small amounts of data, andits capability of allowing incremental classifier up-dates.
Results of automatic tagging are displayedin the suggestion pane.
Our architecture is flexibleto integrate further machine learning tools.3.2 Suggestion ProcessThe workflow to set up an automatically supportedannotation project consists of the following steps.Importing annotation documents We can im-port documents with existing annotations (manualor automatic).
The annotation pane of the automa-tion page allows users to annotate documents andthe suggestion pane is used for the automatic sug-gestion as shown in Figure 1.
The suggestion panefacilitates accepting correct pre-annotations withminimal effort.Configuring features For the machine learningtool, it is required to define classification featuresto train a classifier.
We have designed a UI wherea range of standard classification features for se-quence tagging can be configured.
The featuresinclude morphological features (prefixes, suffixes,and capitalization), n-grams, and other layers as afeature (for example POS annotation as a feature93Figure 2: Configuring an annotation suggestion: 1) layers for automation, 2) different features, 3) trainingdocuments, 4) start training classifier.for named entity recognition).
While these stan-dard features do not lead to state-of-the-art per-formance on arbitrary tasks, we have found themto perform very well for POS tagging, named en-tity recognition, and chunking.
Figure 2 shows thefeature configuration in the project settings.Importing training documents We offer twoways of providing training documents: importingan annotated document in one of the supported fileformats, such as CoNLL, TCF, or UIMA XMI; orusing existing annotation documents in the sameproject that already have been annotated.Starting the annotation suggestion Once fea-tures for a training layer are configured and train-ing documents are available, automatic annotationis possible.
The process can be started manuallyby the administrator from the automation settingspage, and it will be automatically re-initiated whenadditional documents for training become avail-able in the project.
While the automatic annotationis running in the background, users still can workon the annotation front end without being affected.Training and creating a classifier will be repeatedonly when the feature configuration is changed orwhen a new training document is available.Display results on the monitoring page Af-ter the training and automatic annotation are com-pleted, detailed information about the training datasuch as the number of documents (sentence, to-kens), features used for each layer, F-score onheld-out data, and classification errors are dis-played on the monitoring page, allowing an esti-mation whether the automatic suggestion is use-ful.
The UI also shows the status of the trainingprocess (not started, running, or finished).3.3 Case StudiesWe describe two case studies that demonstrate lan-guage independence and flexibility with respect tosequence label types of our automatic annotationsuggestions.
In the first case study, we address thetask of POS tagging for Amharic as an example ofan under-resourced language.
Second, we exploreGerman named entity recognition.3.3.1 Amharic POS taggingAmharic is an under-resourced language in theSemitic family, mainly spoken in Ethiopia.
POStagging research for Amharic is mostly conductedas an academic exercise.
The latest result re-ported by Gebre (2009) was about 90% accuracyusing the Walta Information Center (WIC) corpusof about 210,000 tokens (1065 news documents).We intentionally do not use the corpus as trainingdata because of the reported inconsistencies in thetagging process (Gebre, 2009).
Instead, we man-ually annotate Amharic documents for POS tag-ging both to test the performance of the automa-tion module and to produce POS-tagged corporafor Amharic.
Based upon the work by Petrov et al.
(2012) and Ethiopian Languages Research Cen-ter (ELRC) tagset, we have designed 11 POS tagsequivalent to the Universal POS tags.
The tag DETis not included as Amharic denotes definiteness asnoun suffixes.We collected some Amharic documents from anonline news portal.2Preprocessing of Amharicdocuments includes the normalization of charac-ters and tokenization (sentence and word bound-2http://www.ethiopianreporter.com/94Figure 3: Example Amharic document.
The redtags in the suggestion pane have not been con-firmed by the annotator.ary detection).
Initially, we manually annotated 21sentences.
Using these, an iterative automatic an-notation suggestion process was started until 300sentences were fully annotated.
We obtained anF-score of 0.89 with the final model.
Hence theautomatic annotation suggestion helps in decreas-ing the total annotation time, since the user hasto manually annotate only one out of ten words,while being able to accept most automatic sugges-tions.
Figure 3 shows such an Amharic documentin WebAnno.3.3.2 German Named Entity RecognitionA pilot Named Entity Recognition (NER) projectfor German was conducted by Benikova et al.(2014).
We have used the dataset ?
about 31,000sentences, over 41,000 NE annotations ?
for train-ing NER.
Using this dataset, an F-score of about0.8 by means of automatic suggestions was ob-tained, which leads to an increase in annotationspeed of about 21% with automatic suggestion.4 Customs Annotation LayersThe tasks in which an annotation editor can be em-ployed depends on the expressiveness of the un-derlying annotation model.
However, fully expos-ing the expressive power in the UI can make theeditor inconvenient to use.We propose an approach that allows the userto configure a mapping of an annotation model toconcepts well-supported in a web-based UI.
In thisway, we can avoid to expose all details of the an-notation model in the UI, and remove the need toimplement custom import/export filters.WebAnno 1.0 employs a variant of the annota-tion UI provided by brat, which offers the conceptsof spans and arcs.
Based on these, WebAnno 1.2implements five annotation layers: named entity,part-of-speech, lemmata, co-reference, and depen-dencies.
In the new WebAnno version, we gener-alized the support for these five layers into threeFigure 4: UI for custom annotation layers.structural categories: span, relation (arc), andchain.
Each of these categories is handled by ageneric adapter which can be configured to sim-ulate any of the original five layers.
Based onthis generalization, the user can now define cus-tom layers (Figure 4).Additionally, we introduced a new concept ofconstraints.
For example, NER spans should notcross sentence boundaries and attach to whole to-kens (not substrings of tokens).
Such constraintsnot only help preventing the user from making in-valid annotations, but can also offer extra conve-nience.
We currently support four hard-coded con-straints:Lock to token offsets Defines if annotationboundaries must coincide with token boundaries,e.g.
named entities, lemmata, part-of-speech, etc.For the user?s convenience, the annotation is auto-matically expanded to include the full token, evenif only a part of a token is selected during annota-tion (span/chain layers only).Allow multiple tokens Some kinds of annota-tions may only cover a single token, e.g.
part-of-speech, while others may cover multiple tokens,e.g.
named entities (span/chain layers only).Allow stacking Controls if multiple annotationsof the same kind can be at the same location, e.g.if multiple lemma annotations are allowed per to-ken.
For the user?s convenience, an existing an-notation is replaced if a new annotation is createdwhen stacking is not allowed.Allow crossing sentence boundaries Certainannotations, e.g.
named entities or dependency de-lations, may not cross sentence boundaries, whileothers need to, e.g.
coreference chains.Finally, we added the ability to define multipleproperties for annotations to WebAnno.
For exam-ple, this can be use to define a custom span-basedmorphology layer with multiple annotation prop-erties such as gender, number, case, etc.955 Conclusion and OutlookWe discussed two extensions of WebAnno: thetight and generic integration of automatic annota-tion suggestions for reducing the annotation time,and the web-based addition and configuration ofcustom annotation layers.While we also support the common practiceof using of external tools to automatically pre-annotate documents, we go one step further bytightly integrating a generic sequence classifierinto the tool that can make use of completed an-notation documents from the same project.
In twocase studies, we have shown quick convergencefor Amharic POS tagging and a substantial reduc-tion in annotation time for German NER.
The keyconcept here is the split-pane UI that allows to dis-play automatic suggestions, while forcing the an-notator to review all of them.Allowing the definition of custom annotationlayers in a web-based UI is greatly increasingthe number of annotation projects that potentiallycould use our tool.
While it is mainly an engineer-ing challenge to allow this amount of flexibilityand to hide its complexity from the user, it is a ma-jor contribution in the transition from specializedtools towards general-purpose tools.The combination of both ?
custom layers andautomatic suggestions ?
gives rise to the rapidsetup of efficient annotation projects.
Adding toexisting capabilities in WebAnno, such as cura-tion, agreement computation, monitoring and fine-grained annotation project definition, our contri-butions significantly extend the scope of annota-tion tasks in which the tool can be employed.In future work, we plan to support annota-tion suggestions for non-span structures (arcs andchains), and to include further machine learningalgorithms.AcknowledgmentsThe work presented in this paper was funded by a GermanBMBF grant to the CLARIN-D project, the Hessian LOEWEresearch excellence program as part of the research center?Digital Humanities?
and by the Volkswagen Foundation aspart of the Lichtenberg-Professorship Program under grantNo.
I/82806.ReferencesDaniel Beck, Lucia Specia, and Trevor Cohn.
2013.
Re-ducing annotation effort for quality estimation via activelearning.
In Proc.
ACL 2013 System Demonstrations,Sofia, Bulgaria.Darina Benikova, Chris Biemann, and Marc Reznicek.
2014.NoSta-D Named Entity Annotation for German: Guide-lines and Dataset.
In Proc.
LREC 2014, Reykjavik, Ice-land.Kalina Bontcheva, H. Cunningham, I. Roberts, A. Roberts,V.
Tablan, N. Aswani, and G. Gorrell.
2013.
GATETeamware: a web-based, collaborative text annota-tion framework.
Language Resources and Evaluation,47(4):1007?1029.Koby Crammer and Yoram Singer.
2003.
Ultraconservativeonline algorithms for multiclass problems.
In Journal ofMachine Learning Research 3, pages 951 ?
991.Hamish Cunningham, D. Maynard, K. Bontcheva, V. Tablan,N.
Aswani, I. Roberts, G. Gorrell, A. Funk, A. Roberts,D.
Damljanovic, T. Heitz, M. A. Greenwood, H. Saggion,J.
Petrak, Y. Li, and W. Peters.
2011.
Text Processing withGATE (Version 6).
University of Sheffield Department ofComputer Science, ISBN 978-0956599315.Binyam Gebrekidan Gebre.
2009.
Part-of-speech tagging forAmharic.
In ISMTCL Proceedings, International ReviewBulag, PUFC.T.
G?otz and O. Suhre.
2004.
Design and implementationof the UIMA Common Analysis System.
IBM SystemsJournal, 43(3):476 ?489.Nancy Ide and Keith Suderman.
2007.
GrAF: A graph-basedformat for linguistic annotations.
In Proc.
Linguistic An-notation Workshop, pages 1?8, Prague, Czech Republic.Todd Lingren, L. Deleger, K. Molnar, H. Zhai, J. Meinzen-Derr, M. Kaiser, L. Stoutenborough, Q. Li, and I. Solti.2013.
Evaluating the impact of pre-annotation on anno-tation speed and potential bias: natural language process-ing gold standard development for clinical named entityrecognition in clinical trial announcements.
In Journal ofthe American Medical Informatics Association, pages 951?
991.Thomas Morton and Jeremy LaCivita.
2003.
WordFreak: anopen tool for linguistic annotation.
In Proc.
NAACL 2003,demonstrations, pages 17?18, Edmonton, Canada.Slav Petrov, Dipanjan Das, and Ryan McDonald.
2012.
Auniversal part-of-speech tagset.
In Proc LREC 2012, Is-tanbul, Turkey.Guido Sautter, Klemens B?ohm, Frank Padberg, and WalterTichy.
2007.
Empirical Evaluation of Semi-automatedXML Annotation of Text Documents with the GoldenGATEEditor.
Budapest, Hungary.Pontus Stenetorp, Sampo Pyysalo, Goran Topi?c, TomokoOhta, Sophia Ananiadou, and Jun?ichi Tsujii.
2012. brat:a Web-based Tool for NLP-Assisted Text Annotation.
InProc.
EACL 2012 Demo Session, Avignon, France.Seid Muhie Yimam, Iryna Gurevych, Richard Eckartde Castilho, and Chris Biemann.
2013.
WebAnno: Aflexible,web-based and visually supported system for dis-tributed annotations.
In Proc.
ACL 2013 System Demon-strations, pages 1?6, Sofia, Bulgaria.Amir Zeldes, Julia Ritz, Anke L?udeling, and Christian Chiar-cos. 2009.
ANNIS: A search tool for multi-layer anno-tated corpora.
In Proc.
Corpus Linguistics 2009, Liver-pool, UK.96
