Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 288?293,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsCoreference for Learning to Extract Relations:Yes, Virginia, Coreference MattersRyan Gabbardrgabbard@bbn.comMarjorie Freedmanmfreedma@bbn.comRalph Weischedelweischedel@bbn.comRaytheon BBN Technologies, 10 Moulton St., Cambridge, MA 02138The views expressed are those of the author and do not reflect the official policy or position of the De-partment of Defense or the U.S. Government.
This is in accordance with DoDI 5230.29, January 8, 2009.AbstractAs an alternative to requiring substantial su-pervised relation training data, many have ex-plored bootstrapping relation extraction froma few seed examples.
Most techniques assumethat the examples are based on easily spottedanchors, e.g., names or dates.
Sentences in acorpus which contain the anchors are thenused to induce alternative ways of expressingthe relation.
We explore whether coreferencecan improve the learning process.
That is, ifthe algorithm considered examples such as hissister, would accuracy be improved?
With co-reference, we see on average a 2-fold increasein F-Score.
Despite using potentially errorfulmachine coreference, we see significant in-crease in recall on all relations.
Precision in-creases in four cases and decreases in six.1 IntroductionAs an alternative to requiring substantial super-vised relation training data (e.g.
the ~300k wordsof detailed, exhaustive annotation in AutomaticContent Extraction (ACE) evaluations1) many haveexplored bootstrapping relation extraction from afew (~20) seed instances of a relation.
Key to suchapproaches is a large body of unannotated text thatcan be iteratively processed as follows:1.
Find sentences containing the seed instances.2.
Induce patterns of context from the sentences.3.
From those patterns, find more instances.4.
Go to 2 until some condition is reached.Most techniques assume that relation instanc-es, like hasBirthDate(Wolfgang Amadeus Mozart,1http://www.nist.gov/speech/tests/ace/1756), are realized in the corpus as relation texts2with easily spotted anchors like WolfgangAmadeus Mozart was born in 1756.In this paper we explore whether using corefer-ence can improve the learning process.
That is, ifthe algorithm considered texts like his birth in1756 for the above relation, would performance ofthe learned patterns be better?2 Related ResearchThere has been much work in relation extractionboth in traditional supervised settings and, morerecently, in bootstrapped, semi-supervised settings.To set the stage for discussing related work, wehighlight some aspects of our system.
Our workinitializes learning with about 20 seed relation in-stances and uses about 9 million documents of un-annotated text3 as a background bootstrappingcorpus.
We use both normalized syntactic structureand surface strings as features.Much has been published on learning relationextractors using lots of supervised training, as inACE, which evaluates system performance in de-tecting a fixed set of concepts and relations in text.Researchers have typically used this data to incor-porate a great deal of structural syntactic infor-mation in their models (e.g.
Ramshaw, 2001), butthe obvious weakness of these approaches is theresulting reliance on manually annotated examples,which are expensive and time-consuming to create.2Throughout we will use relation instance to refer to a fact(e.g.
ORGHasEmployee(Apple, Steve Jobs)), while we will userelation text to refer a particular sentence entailing a relationinstance (e.g.
Steve Jobs is Apple?s CEO).3Wikipedia and the LDC?s Gigaword newswire corpus.288Others have explored automatic pattern genera-tion from seed examples.
Agichtein & Gravano(2000) and Ravichandran & Hovy (2002) reportedresults for generating surface patterns for relationidentification; others have explored similar ap-proaches (e.g.
Pantel & Pennacchiotti, 2006).Mitchell et al (2009) showed that for macro-reading, precision and recall can be improved bylearning a large set of interconnected relations andconcepts simultaneously.
In all cases, the ap-proaches used surface (word) patterns without co-reference.
In contrast, we use the structuralfeatures of predicate-argument structure and em-ploy coreference.
Section 3 describes our particularapproach to pattern and relation instance scoringand selection.Another research strand (Chen et al, 2006 &Zhou et al, 2008) explores semi-supervised rela-tion learning using the ACE corpus and assumingmanual mention markup.
They measure the accu-racy of relation extraction alone, without includingthe added challenge of resolving non-specific rela-tion arguments to name references.
They limit theirstudies to the small ACE corpora where mentionmarkup is manually encoded.Most approaches to automatic pattern genera-tion have focused on precision, e.g., Ravichandranand Hovy (2002) report results in the Text Retriev-al Conference (TREC) Question Answering track,where extracting one text of a relation instance canbe sufficient, rather than detecting all texts.
Mitch-ell et al (2009), while demonstrating high preci-sion, do not measure recall.In contrast, our study has emphasized recall.
Aprimary focus on precision allows one to ignoremany relation texts that require coreference orlong-distance dependencies; one primary goal ofour work is to measure system performance in ex-actly those areas.
There are at least two reasons tonot lose sight of recall.
For the majority of entitiesthere will be only a few mentions of that entity ineven a large corpus.
Furthermore, for many infor-mation-extraction problems the number documentsat runtime will be far less than web-scale.3 ApproachFigure 1 depicts our approach for learning patternsto detect relations.
At each iteration, the steps are:(1) Given the current relation instances, find possi-ble texts that entail the relation by finding sentenc-es in the corpus containing all arguments of an in-stance.
(2)  As in Freedman et al (2010) and Boschee etal.
(2008), induce possible patterns using the con-text in which the arguments appear.
Patterns in-clude both surface strings and normalized syntacticstructures.4 Each proposed pattern is applied to thecorpus to find a set of hypothesized texts.For eachpattern, a confidence score is assigned using esti-mated precision5 and recall.
The highest confi-dence patterns are added to the pattern set.6(3) The patterns are applied to the corpus to findadditional possible relation instances.
For eachproposed instance, we estimate a score using a Na-ive Bayes model with the patterns as the features.When using coreference, this score is penalized ifan instance?s supporting evidence involves low-confidence coreference links.
The highest scoringinstances are added to the instance set.
(4) After the desired number of iterations (in theseexperiments, 20) is complete, a human reviews theresulting pattern set and removes those patternswhich are clearly incorrect (e.g.
?X visited Y?
forhasBirthPlace).7Figure 1: Approach to learning relationsWe ran this system in two versions: ?Coref hasno access to coreference information, while +Coref(the original system) does.
The systems are other-wise identical.
Coreference information is providedby BBN?s state-of-the-art information extraction4Surface text patterns with wild cards are not proposed untilthe third iteration.5Estimated recall is the weighted fraction of known instancesfound.
Estimated precision is the weighted average of thescores of matched instances; scores for unseen instances are 0.6As more patterns are accepted in a given iteration, we raisethe confidence threshold.
Usually, ~10 patterns are acceptedper iteration.7This takes about ten minutes per relation, which is less thanthe time to choose the initial seed instances.patterndatabaseproposedinstancesproposedpatternsproposedpairsretrieve fromcorpusretrieve from corpusinducepruneand addgrantedpatentobjINVENTORINVENTIONiobjforThomas Edison ?
light bulbAlexander G. Bell ... telephoneBen Franklin ?
lightning rodEdison invented the light bulbBell built the first telephoneEdison was granted a U.S. patentfor the light bulbFranklin invented the lightning rodexample pairsinstances289system (Ramshaw, et al, 2011; NIST, 2007) in amode which sacrifices some accuracy for speed(most notably by reducing the parser?s searchspace).
The IE system processes over 50MB/hourwith an  average EDR Value score when evaluatedon an 8-fold cross-validation of the ACE 2007.+Coref can propose relation instances from textin which the arguments are expressed as eithername or non-name mentions.
When the text of anargument of a proposed instance is a non-name, thesystem uses coreference to resolve the non-name toa name.
-Coref can only propose instances basedon texts where both arguments are names.8This has several implications: If a text that en-tails a relation instance expresses one of the argu-ments as a non-name mention (e.g.
?Sue?s husbandis here.?
), -Coref will be unable to learn an in-stance from that text.
Even when all arguments areexpressed as names, -Coref may need to use morespecific, complex patterns to learn the instance(e.g.
?Sue asked her son, Bob, to set the table?
).We expect the ability to run using a ?denser,?
morelocal space of patterns to be a significant advantageof +Coref.
Certain types of patterns (e.g.
patternsinvolving possessives) may also be less likely to belearned by -Coref.
Finally, +Coref has access tomuch more training data at the outset because itcan find more matching seed instances,9 potentiallyleading to better and more stable training.4 Evaluation FrameworkEstimating recall for bootstrapped relation learningis a challenge except for corpora small enough forcomplete annotation to be feasible, e.g., the ACEcorpora.
ACE typically had a test set of ~30,000words and ~300k for training.
Yet, with a smallcorpus, rare relations will be inadequately repre-sented.10 Macro-reading evaluations (e.g.
Mitchell,2009) have not estimated recall, but have measuredprecision by sampling system output and determin-ing whether the extracted fact is true in the world.8An instance like hasChild(his father, he) would be usefulneither during training nor (without coreference) at runtime.9An average of 12,583 matches versus 2,256 matches.
If mul-tiple mentions expressing an argument occur in one sentence,each match is counted, inflating the difference.10Despite being selected to be rich in the 18 ACE relationsubtypes, the 10 most frequent subtypes account for over 90%of the relations with the 4 most frequent accounting for 62%;the 5 least frequent relation subtypes occur less than 50 times.Here we extend this idea to both precision and re-call in a micro-reading context.Precision is measured by running the systemover the background corpus and randomly sam-pleing 100 texts that the system believes entaileach relation.
From the mentions matching the ar-gument slots of the patterns, we build a relationinstance.
If these mentions are not names (onlypossible for +Coref), they are resolved to namesusing system coreference.
For example, given thepassage in Figure 2 and the pattern ?
(Y, poss:X)?,the system would match the mentions X=her andY=son, and build the relation instancehasChild(Ethel Kennedy, Robert F. Kennedy Jr.).During assessment, the annotator is askedwhether, in the context of the whole document, agiven sentence entails the relation instance.
Wethus treat both incorrect relation extraction andincorrect reference resolution as mistakes.To measure recall, we select 20 test relation in-stances and search the corpus for sentences con-taining all arguments of a test instance (explicitlyor via coreference).
We randomly sampled fromthis set, choosing at most 10 sentences for each testinstance, to form a collection of at most 200 sen-tences likely to be texts expressing the desired rela-tion.
These sentences were then manuallyannotated in the same manner as the precision an-notation.
Sentences that did not correctly conveythe relation instance were removed, and the re-maining set of sentences formed a recall set.
Weconsider a recall set instance to be found by a sys-tem if the system finds a relation of the correcttype in the sentence.
We intentionally chose tosample 10 sentences from each test example, ratherthan sampling from the set of all sentences found.This prevents one or two very commonly ex-pressed instances from dominating the recall set.As a result, the recall test set is biased away from?true?
recall, because it places a higher weight onthe ?long tail?
of instances.
However, this gives amore accurate indication of the system?s ability tofind novel instances of a relation.Ethel Kennedy says that when the family gatheredfor Thanksgiving she wanted the children to knowwhat a real turkey looked like.
So she sent her son,Robert F. Kennedy Jr., to a farm to buy two birds.Figure 2: Passage entailing hasChild relation2905 Empirical ResultsTable 1 gives results for precision, recall, and Ffor +Coref (+) and ?Coref (-).
In all cases remov-ing coreference causes a drop in recall, rangingfrom only 33%(hasBirthPlace) to over 90%(GPEEmploys).
The median drop is 68%.5.1 RecallThere are two potential sources of ?Coref?slower recall.
For some relation instances, the textwill contain only non-named instances, and as aresult -Coref will be unable to find the instance.-Coref is also at a disadvantage while learning,since it has access to fewer texts during bootstrap-ping.
Figure 311 presents the fraction of instancesin the recall test set for which both argumentnames appear in the sentence.
Even with perfectpatterns, -Coref has no opportunity to find roughly25% of the relation texts because at least one ar-gument is not expressed as a name.To further understand -Coref?s lower perfor-mance, we created a third system, *Coref, whichused coreference at runtime but not during train-ing.12 In a few cases, such as hasBirthPlace,*Coref is able to almost match the recall of thesystem that used coreference during learning(+Coref), but on average the lack of coreference atruntime accounts for only about 25% of the differ-ence, with the rest accounted for by differences inthe pattern sets learned.Figure 4 shows the distribution of argumentmention types for +Coref on the recall set.
Com-paring this to Figure 3, we see that +Coref usesname-name pairs far less often than it could (less11Figures 3 & 4 do not include hasBirthDate: There is only 1potential named argument for this relation, the other is a date.12*Coref was added after reading paper reviews, so there wasnot time to do annotation for a precision evaluation for it.than 50% of the time overall).
Instead, even whentwo names are present in a sentence that entails therelation, +Coref chooses to find the relation inname-descriptor and name-pronoun contexts whichare often more locally related in the sentences.Figure 4: Distribution of argument mention types for+Coref matches on the recall setFor the two cases with the largest drops in re-call, ORGEmploys and GPEEmploys, +Coref and ?Coref have very different trajectories during train-ing.
For example, in the first iteration, ?Coreflearns patterns involving director, president, andhead for ORGEmploys, while +Coref learns pat-terns involving joined and hired.
We speculatethat ?Coref may become stuck because the mostfrequent name-name constructions, e.g.
ORG/GPEtitle PERSON (e.g.
Brazilian President Lula daSilva), are typically used to introduce top officials.For such cases, even without co-reference, systemspecific effort and tuning could potentially haveimproved ?Coref?s ability to learn the relations.5.2 PrecisionResults on precision are mixed.
While for 4 ofthe relations +Coref is higher, for the 6 others theaddition of coreference reduces precision.
The av-erage precisions for +Coref and ?Coref are 82.2and 87.8, and the F-score of +Coref exceeded that0 %20 %40 %60 %80 %100 %1 2 3 4 5 6 7 8 9OtherCombi nati onsBoth DescName & PronName & DescBoth Na meP+ P- R+ R- R* F+ F-attendSchool (1) 83 97 49 16 27 62 27GPEEmploy(2) 91 96 29 3 3 44 5GPELeader (3) 87 99 48 28 30 62 43hasBirthPlace (4) 87 97 57 37 53 69 53hasChild (5) 70 60 37 17 11 48 27hasSibling (6) 73 69 67 17 17 70 28hasSpouse (7) 61 96 72 22 31 68 36ORGEmploys(8) 92 82 22 4 7 35 7ORGLeader (9) 88 97 73 32 42 80 48hasBirthDate (10) 90 85 45 13 32 60 23Table 1: Precision, Recall, and F scoresFigure 3: Fraction of recall instances with namementions present in the sentence for both arguments.0.000.100.200.300.400.500.600.700.800.901.001 2 3 4 5 6 7 8 9%Recall Instances291of ?Coref for all relations.
Thus while +Coref paysa price in precision for its improved recall, in manyapplications it may be a worthwhile tradeoff.Though one might expect that errors in coref-erence would reduce precision of +Coref, such er-rors may be balanced by the need to use longerpatterns in ?Coref.
These patterns often includeerror-prone wildcards which lead to a drop in pre-cision.
Patterns with multiple wildcards were alsomore likely to be removed as unreliable in manualpattern pruning, which may have harmed the recallof ?Coref, while improving its precision.5.3 Further AnalysisOur analysis thus far has focused on micro-reading which requires a system find all mentionsof an instance relation ?
i,e, in our evaluation Or-gLeader(Apple, Steve Jobs) might occur in asmany as 20 different contexts.
While ?Coref per-forms poorly at micro-reading, it could still be ef-fective for macro-reading, i.e.
finding at least oneinstance of the relation OrgLeader(Apple, SteveJobs).
As a rough measure of this, we also evaluat-ed recall by counting the number of test instancesfor which at least one answer was found by the twosystems.
With this method, +Coref?s recall is stillhigher for all but one relation type, although thegap between the systems narrows somewhat.In addition to our recall evaluation, we meas-ured the number of sentences containing relationinstances found by each of the systems when ap-plied to 5,000 documents (see Table 3).
For al-most all relations, +Coref matches many moresentences, including finding more sentences forthose relations for which it has higher precision.6 ConclusionOur experiments suggest that in contexts whererecall is important incorporating coreference into arelation extraction system may provide significantgains.
Despite being noisy, coreference infor-mation improved F-scores for all relations in ourtest, more than doubling the F-score for 5 of the10.Why is the high error rate of coreference notvery harmful to +Coref?
We speculate that thereare two reasons.
First, during training, not all co-reference is treated equally.
If the only evidencewe have for a proposed instance depends on lowconfidence coreference links, it is very unlikely tobe added to our instance set for use in future itera-tions.
Second, for both training and runtime, manyof the coreference links relevant for extracting therelation set examined here are fairly reliable, suchas wh-words in relative clauses.There is room for more investigation of thequestion, however.
It is also unclear if the sameresult would hold for a very different set of rela-tions, especially those which are more event-likethan relation-like.AcknowledgmentsThis work was supported, in part, by DARPA un-der AFRL Contract FA8750-09-C-179.
The viewsexpressed are those of the authors and do not re-flect the official policy or position of the Depart-ment of Defense or the U.S. Government.
Wewould like to thank our reviewers for their helpfulcomments and Martha Friedman, Michael Heller,Elizabeth Roman, and Lorna Sigourney for doingour evaluation annotation.+Coref -Coref #TestInstancesORGEmploys 8 2 20GPEEmploys 12 3 19hasSibling 11 4 19hasBirthDate 12 5 17hasSpouse 15 9 20ORGLeader 14 9 19attendedSchool 17 12 20hasBirthPlace 19 15 20GPELeader 15 13 19hasChild 6 6 19Table 2: Number of test seeds where at least oneinstance is found in the evaluation.Prec Number of SentencesRelation P+ P- +Cnt -Cnt *CntattendedSchool 83 97 541 212 544hasChild 91 96 661 68 106hasSpouse 87 99 1262 157 282hasSibling 87 97 313 72 272GPEEmploys 70 60 1208 308 313GPELeader 73 69 1018 629 644ORGEmploys 61 96 1698 142 209ORGLeader 92 82 1095 207 286hasBirthDate 88 97 231 131 182hasBirthPlace 90 85 836 388 558Table 3: Number of sentences in which each systemfound relation instances292ReferencesE.
Agichtein and L. Gravano.
2000.
Snowball: extract-ing relations from large plain-text collections.
InProceedings of the ACM Conference on Digital Li-braries, pp.
85-94.M.
Banko, M. Cafarella, S. Soderland, M. Broadhead,and O. Etzioni.
2007.
Open Information Extractionfrom the Web.
In Proceedings of the InternationalJoint Conference on Artificial Intelligence.A.
Baron and M. Freedman.
2008. Who is Who andWhat is What: Experiments in Cross Document Co-Reference.
In Empirical Methods in Natural Lan-guage Processing.A.
Blum and T. Mitchell.
1998.
Combining Labeled andUnlabeled Data with Co-Training.
In Proceedings ofthe 1998 Conference on Computational LearningTheory.E.
Boschee, V. Punyakanok, R. Weischedel.
2008.
AnExploratory Study Towards ?Machines that Learn toRead?.
Proceedings of AAAI BICA Fall Symposium.J.
Chen, D. Ji, C. Tan and Z. Niu.
2006.
Relation extrac-tion using label propagation based semi-supervisedlearning.
COLING-ACL 2006: 129-136.T.
Mitchell, J. Betteridge, A. Carlson, E. Hruschka, andR.
Wang.
2009.
Populating the Semantic Web byMacro-Reading Internet Text.
Invited paper, Pro-ceedings of the 8th International Semantic Web Con-ference (ISWC 2009).National Institute of Standards and Technology.
2007.NIST 2007 Automatic Content Extraction EvaluationOfficial Results.
http://www.itl.nist.gov/iad/mig/tests/ace/2007/doc/ace07_eval_official_results_20070402.htmlP.
Pantel and M. Pennacchiotti.
2006.
Espresso: Lever-aging Generic Patterns for Automatically HarvestingSemantic Relations.
In Proceedings of Conference onComputational Linguistics / Association for Compu-tational Linguistics (COLING/ACL-06).
pp.
113-120.Sydney, Australia.L.
Ramshaw, E. Boschee, S. Bratus, S. Miller, R. Stone,R.
Weischedel, A. Zamanian.
2001.
Experiments inmulti-modal automatic content extraction, In Pro-ceedings of Human Language Technology Confer-ence.L.
Ramshaw, E. Boschee, M. Freedman, J. MacBride,R.
Weischedel, A. Zamanian.
2011.
SERIF LanguageProcessing ?
Efficient Trainable Language Under-standing.
In Handbook of Natural Language Pro-cessing and Machine Translation: DARPA GlobalAutonomous Language Exploitation.
Springer.D.
Ravichandran and E. Hovy.
2002.
Learning surfacetext patterns for a question answering system.
InProceedings of the 40th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL 2002),pages 41?47, Philadelphia, PA.E.
Riloff.
1996.
Automatically generating extractionpatterns from untagged text.
In Proceedings of theThirteenth National Conference on Artificial Intelli-gence, pages 1044-1049.G.
Zhou, J. Li, L. Qian, Q. Zhu.
2008.
Semi-SupervisedLearning for Relation Extraction.
Proceedings of theThird International Joint Conference on NaturalLanguage Processing: Volume-I.Z.
Kozareva and E. Hovy.
Not All Seeds Are Equal:Measuring the Quality of Text Mining Seeds.
2010.Human Language Technologies: The 2010 AnnualConference of the North American Chapter of the As-sociation for Computational Linguistics pp.
618-626.293
