Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1?11,Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational LinguisticsProbabilistic Lexical Generalization for French Dependency ParsingEnrique Henestroza Anguiano and Marie CanditoAlpage (Universite?
Paris Diderot / INRIA)Paris, Franceenrique.henestroza anguiano@inria.fr, marie.candito@linguist.jussieu.frAbstractThis paper investigates the impact on Frenchdependency parsing of lexical generalizationmethods beyond lemmatization and morpho-logical analysis.
A distributional thesaurusis created from a large text corpus and usedfor distributional clustering and WordNet au-tomatic sense ranking.
The standard approachfor lexical generalization in parsing is to mapa word to a single generalized class, either re-placing the word with the class or adding anew feature for the class.
We use a richerframework that allows for probabilistic gener-alization, with a word represented as a prob-ability distribution over a space of general-ized classes: lemmas, clusters, or synsets.Probabilistic lexical information is introducedinto parser feature vectors by modifying theweights of lexical features.
We obtain im-provements in parsing accuracy with somelexical generalization configurations in exper-iments run on the French Treebank and twoout-of-domain treebanks, with slightly betterperformance for the probabilistic lexical gen-eralization approach compared to the standardsingle-mapping approach.1 IntroductionIn statistical, data-driven approaches to natural lan-guage syntactic parsing, a central problem is that ofaccurately modeling lexical relationships from po-tentially sparse counts within a training corpus.
Ourparticular interests are centered on reducing lexicaldata sparseness for linear classification approachesfor dependency parsing.
In these approaches, linearmodels operate over feature vectors that generallyrepresent syntactic structure within a sentence, andfeature templates are defined in part over the wordforms of one or more tokens in a sentence.
Becausetreebanks used for training are often small, lexicalfeatures may appear relatively infrequently duringtraining, especially for languages with richer mor-phology than English.
This may, in turn, impede theparsing model?s ability to generalize well outside ofits training set with respect to lexical features.Past approaches for achieving lexical generaliza-tion in dependency parsing have used WordNet se-mantic senses in parsing experiments for English(Agirre et al, 2011), and word clustering over largecorpora in parsing experiments for English (Kooet al, 2008) as well as for French (Candito et al,2010b).
These approaches map each word to a sin-gle corresponding generalized class (synset or clus-ter), and integrate generalized classes into parsingmodels in one of two ways: (i) the replacementstrategy, where each word form is simply replacedwith a corresponding generalized class; (ii) a strat-egy where an additional feature is created for thecorresponding generalized class.Our contribution in this paper is applying prob-abilistic lexical generalization, a richer frameworkfor lexical generalization, to dependency parsing.Each word form is represented as a categorical dis-tribution over a lexical target space of generalizedclasses, for which we consider the spaces of lemmas,synsets, and clusters.
The standard single-mappingapproach from previous work can be seen as a sub-case: each categorical distribution assigns a proba-bility of 1 to a single generalized class.
The method1we use for introducing probabilistic information intoa feature vector is based on that used by Bunescu(2008), who tested the use of probabilistic part-of-speech (POS) tags through an NLP pipeline.In this paper, we perform experiments for Frenchthat use the replacement strategy for integratinggeneralized classes into parsing models, comparingthe single-mapping approach for lexical generaliza-tion with our probabilistic lexical generalization ap-proach.
In doing so, we provide first results on theapplication to French parsing of WordNet automaticsense ranking (ASR), using the method of McCarthyet al (2004).
For clustering we deviate from mostprevious work, which has integrated Brown clusters(Brown et al, 1992) into parsing models, and insteaduse distributional lexical semantics to create both adistributional thesaurus - for probabilistic general-ization in the lemma space and ASR calculation -and to perform hierarchical agglomerative clustering(HAC).
Though unlexicalized syntactic HAC clus-tering has been used to improve English dependencyparsing (Sagae and Gordon, 2009), we provide firstresults on using distributional lexical semantics forFrench parsing.
We also include an out-of-domainevaluation on medical and parliamentary text in ad-dition to an in-domain evaluation.In Section 2 we describe the lexical target spacesused in this paper, as well as the method of integrat-ing probabilistic lexical information into a featurevector for classification.
In Section 3 we discuss de-pendency structure and transition-based parsing.
InSection 4 we present the experimental setup, whichincludes our parser implementation, the constructionof our probabilistic lexical resources, and evaluationsettings.
We report parsing results both in-domainand out-of-domain in Section 5, we provide a sum-mary of related work in Section 6, and we concludein Section 7.2 Probabilistic Lexical Target SpacesUsing terms from probability theory, we define a lex-ical target space as a sample space ?
over whicha categorical distribution is defined for each lexi-cal item in a given source vocabulary.
Because weare working with French, a language with relativelyrich morphology, we use lemmas as the base lexi-cal items in our source vocabulary.
The outcomescontained in a sample space represent generalizedclasses in a target vocabulary.
In this paper we con-sider three possible target vocabularies, with cor-responding sample spaces: ?l for lemmas, ?s forsynsets, and ?c for clusters.2.1 ?l Lemma SpaceIn the case of the lemma space, the source and tar-get vocabularies are the same.
To define an ap-propriate categorical distribution for each lemma,one where the possible outcomes also correspond tolemmas, we use a distributional thesaurus that pro-vides similarity scores for pairs of lemmas.
Sucha thesaurus can be viewed as a similarity functionD(x, y), where x, y ?
V and V is the vocabularyfor both the source and target spaces.The simplest way to define a categorical distribu-tion over ?l, for a lemma x ?
V , would be to usethe following probability mass function px:px(y) =D(x, y)?y?
?VD(x, y?
)(1)One complication is the identity similarity D(x, x):although it can be set equal to 1 (or the similar-ity given by the thesaurus, if one is provided), wechoose to assign a pre-specified probability mass mto the identity lemma, with the remaining mass usedfor generalization across other lemmas.
Addition-ally, in order to account for noise in the thesaurus,we restrict each categorical distribution to a lemma?sk-nearest neighbors.
The probability mass functionpx over the space ?l that we use in this paper is fi-nally as follows:px(y) =??????????
?m, if y = x(1?m)D(x, y)?y?
?Nx(k)D(x, y?
), if y ?
Nx(k)0, otherwise(2)2.2 ?s Synset SpaceIn the case of the synset space, the target vacabularycontains synsets from the Princeton WordNet sensehierarchy (Fellbaum, 1998).
To define an appro-priate categorical distribution over synsets for each2lemma x in our source vocabulary, we first use theWordNet resource to identify the set Sx of differentsenses of x.
We then use a distributional thesaurus toperform ASR, which determines the prevalence withrespect to x of each sense s ?
Sx, following theapproach of McCarthy et al (2004).
Representingthe thesaurus as a similarity function D(x, y), let-ting Nx(k) be the set of k-nearest neighbors for x,and letting W (s1, s2) be a similarity function oversynsets in WordNet, we define a prevalence functionRx(s) as follows:Rx(s) =?y?Nx(k)D(x, y)maxs?
?
SyW (s, s?)?t?Sxmaxs?
?
SyW (t, s?
)(3)This function essentially weights the semantic con-tribution that each distributionally-similar neighboradds to a given sense for x.
With the prevalencescores of each sense for x having been calculated,we use the following probability mass function pxover the space ?s, where Sx(k) is the set of k-mostprevalent senses for x:px(s) =???????Rx(s)?s??Sx(k)Rx(s?
), if s ?
Sx(k)0, otherwise(4)Note that the first-sense ASR approach to usingWordNet synsets for parsing, which has been previ-ously explored in the literature (Agirre et al, 2011),corresponds to setting k=1 in Equation 4.2.3 ?c Cluster SpaceIn the case of the cluster space, any approach forword clustering may be used to create a reduced tar-get vocabulary of clusters.
Defining a categoricaldistribution over clusters would be interesting in thecase of soft clustering of lemmas, in which a lemmacan participate in more than one cluster, but we havenot yet explored this clustering approach.In this paper we limit ourselves to the simplerhard clustering HAC method, which uses a distri-butional thesaurus and iteratively joins two clusterstogether based on the similarities between lemmasin each cluster.
We end up with a simple probabilitymass function px over the space ?c for a lemma xwith corresponding cluster cx:px(c) ={1, if c = cx0, otherwise (5)2.4 Probabilistic Feature GeneralizationIn a typical classifier-based machine learning settingin NLP, feature vectors are constructed using indi-cator functions that encode categorical information,such as POS tags, word forms or lemmas.In this section we will use a running examplewhere a and b are token positions of interest to aclassifier, and for which feature vectors are created.If we let t stand for POS tag and l stand for lemma,a feature template for this pair of tokens might thenbe [talb].
Feature templates are instantiated as ac-tual features in a vector space depending on the cat-egorical values they can take on.
One possible in-stantiation of the template [talb] would then be thefeature [ta=verb?lb=avocat], which indicates that ais a verb and b is the lemma avocat (?avocado?
or?lawyer?
), with the following indicator function:f ={1, if ta=verb ?
lb=avocat0, otherwise (6)To perform probabilistic feature generalization, wereplace the indicator function, which represents asingle original feature, with a collection of weightedfunctions representing a set of derived features.
Sup-pose the French lemma avocat is in our source vo-cabulary and has multiple senses in ?s (s1 for the?avocado?
sense, s2 for the ?lawyer?
sense, etc.
),as well as a probability mass function pav.
Wediscard the old feature [ta=verb?lb=avocat] andadd, for each si, a derived feature of the form[ta=verb?xb=si], where x represents a target spacegeneralized class, with the following weighted indi-cator function:f(i) ={pav(si), if ta=verb ?
lb=avocat0, otherwise (7)This process extends easily to generalizing multiplecategorical variables.
Consider the bilexical feature[la=manger?lb=avocat], which indicates that ais the lemma manger (?eat?)
and b is the lemmaavocat.
If both lemmas manger and avocat appear3ouvritElle portelaaveccle?laFigure 1: An unlabeled dependency tree for ?Elle ouvritla porte avec la cle??
(?She opened the door with the key?
).in our source vocabulary and have multiple sensesin ?s, with probability mass functions pma and pav,then for each pair i, j we derive a feature of theform [xa=si?xb=sj], with the following weightedindicator function:f(i,j)={pma(si)pav(sj), if la=manger?lb=avocat0, otherwise (8)3 Dependency ParsingDependency syntax involves the representation ofsyntactic information for a sentence in the form ofa directed graph, whose edges encode word-to-wordrelationships.
An edge from a governor to a de-pendent indicates, roughly, that the presence of thedependent is syntactically legitimated by the gover-nor.
An important property of dependency syntax isthat each word, except for the root of the sentence,has exactly one governor; dependency syntax is thusrepresented by trees.
Figure 1 shows an exampleof an unlabeled dependency tree.1 For languageslike English or French, most sentences can be rep-resented with a projective dependency tree: for anyedge from word g to word d, g dominates any inter-vening word between g and d.Dependency trees are appealing syntactic repre-sentations, closer than constituency trees to the se-mantic representations useful for NLP applications.This is true even with the projectivity requirement,which occasionally creates syntax-semantics mis-matches.
Dependency trees have recently seen asurge of interest, particularly with the introductionof supervised models for dependency parsing usinglinear classifiers.1Our experiments involve labeled parsing, with edges addi-tionally labeled with the surface grammatical function that thedependent bears with respect to its governor.3.1 Transition-Based ParsingIn this paper we focus on transition-based pars-ing, whose seminal works are that of Yamada andMatsumoto (2003) and Nivre (2003).
The parsingprocess applies a sequence of incremental actions,which typically manipulate a buffer position in thesentence and a stack for built sub-structures.
In thearc-eager approach introduced by Nivre et al (2006)the possible actions are as follows, with s0 being thetoken on top of the stack and n0 being the next tokenin the buffer:?
SHIFT: Push n0 onto the stack.?
REDUCE: Pop s0 from the stack.?
RIGHT-ARC(r): Add an arc labeled r from s0to n0; push n0 onto the stack.?
LEFT-ARC(r): Add an arc labeled r from n0to s0; pop s0 from the stack.The parser uses a greedy approach, where the ac-tion selected at each step is the best-scoring actionaccording to a classifier, which is trained on a de-pendency treebank converted into sequences of ac-tions.
The major strength of this framework is itsO(n) time complexity, which allows for very fastparsing when compared to more complex global op-timization approaches.4 Experimental SetupWe now discuss the treebanks used for training andevaluation, the parser implementation and baselinesettings, the construction of the probabilistic lexicalresources, and the parameter tuning and evaluationsettings.4.1 TreebanksThe treebank we use for training and in-domainevaluation is the French Treebank (FTB) (Abeille?and Barrier, 2004), consisting of 12,351 sentencesfrom the Le Monde newspaper, converted to projec-tive2 dependency trees (Candito et al, 2010a).
Forour experiments we use the usual split of 9,881 train-ing, 1,235 development, and 1,235 test sentences.2The projectivity constraint is linguistically valid for mostFrench parses: the authors report < 2% non-projective edges ina hand-corrected subset of the converted FTB.4Moving beyond the journalistic domain, we usetwo additional treebank resources for out-of-domainparsing evaluations.
These treebanks are part ofthe Sequoia corpus (Candito and Seddah, 2012),and consist of text from two non-journalistic do-mains annotated using the FTB annotation scheme:a medical domain treebank containing 574 develop-ment and 544 test sentences of public assessmentreports of medicine from the European MedicinesAgency (EMEA) originally collected in the OPUSproject (Tiedemann, 2009), and a parliamentary do-main treebank containing 561 test sentences fromthe Europarl3 corpus.4.2 Parser and Baseline SettingsWe use our own Python implementation of the arc-eager algorithm for transition-based parsing, basedon the arc-eager setting of MaltParser (Nivre et al,2007), and we train using the standard FTB trainingset.
Our baseline feature templates and general set-tings correspond to those obtained in a benchmark-ing of parsers for French (Candito et al, 2010b),under the setting which combined lemmas and mor-phological features.4 Automatic POS-tagging is per-formed using MElt (Denis and Sagot, 2009), andlemmatization and morphological analysis are per-formed using the Lefff lexicon (Sagot, 2010).
Ta-ble 1 lists our baseline parser?s feature templates.4.3 Lexical Resource ConstructionWe now describe the construction of our probabilis-tic lexical target space resources, whose prerequi-sites include the automatic parsing of a large corpus,the construction of a distributional thesaurus, the useof ASR on WordNet synsets, and the use of HACclustering.4.3.1 Automatically-Parsed CorpusThe text corpus we use consists of 125 mil-lion words from the L?Est Republicain newspa-per5, 125 million words of dispatches from theAgence France-Presse, and 225 million words froma French Wikipedia backup dump6.
The corpus is3http://www.statmt.org/europarl/4That work tested the use of Brown clusters, but obtained noimprovement compared to a setting without clusters.
Thus, wedo not evaluate Brown clustering in this paper.5http://www.cnrtl.fr/corpus/estrepublicain/6http://dumps.wikimedia.org/Feature TemplatesUnigram tn0 ; ln0 ; cn0 ; wn0 ; ts0 ; ls0 ; cs0 ; ws0 ; ds0 ;tn1 ; ln1 ; tn2 ; tn3 ; ts1 ; ts2 ; tn0l ; ln0l ; dn0l ;ds0l ; ds0r ; ls0h ; {min0 : i ?
|M |};{mis0 : i ?
|M |}Bigram ts0tn0 ; ts0 ln0 ; ls0 ln0 ; ln0tn1 ; tn0 tn0l ;tn0dn0l ; {mis0mjn0 : i; j ?
|M |};{tn0min0 : i ?
|M |}; {ts0mis0 : i ?
|M |}Trigram ts2ts1 ts0 ; ts1ts0 tn0 ; ts0 tn0tn1 ; tn0 tn1tn2 ;tn1tn2 tn3 ; ts0ds0lds0rTable 1: Arc-eager parser feature templates.
c = coarsePOS tag, t = fine POS tag, w = inflected word form, l =lemma, d = dependency label, mi = morphological fea-ture from set M .
For tokens, ni = ith token in the buffer,si = ith token on the stack.
The token subscripts l, r, andh denote partially-constructed syntactic left-most depen-dent, right-most dependent, and head, respectively.preprocessed using the Bonsai tool7, and parsed us-ing our baseline parser.4.3.2 Distributional ThesaurusWe build separate distributional thesauri fornouns and for verbs,8 using straightforward meth-ods in distributional lexical semantics based primar-ily on work by Lin (1998) and Curran (2004).
Weuse the FreDist tool (Henestroza Anguiano and De-nis, 2011) for thesaurus creation.First, syntactic contexts for each lemma are ex-tracted from the corpus.
We use all syntactic de-pendencies in which the secondary token has anopen-class POS tag, with labels included in the con-texts and two-edge dependencies used in the case ofprepositional-phrase attachment and coordination.Example contexts are shown in Figure 2.
For verblemmas we limit contexts to dependencies in whichthe verb is governor, and we add unlexicalized ver-sions of contexts to account for subcategorization.For noun lemmas, we use all dependencies in whichthe noun participates, and all contexts are lexical-ized.
The vocabulary is limited to lemmas with atleast 1,000 context occurrences, resulting in 8,171nouns and 2,865 verbs.Each pair of lemma x and context c is sub-sequently weighted by mutual informativeness us-ing the point-wise mutual information metric, with7http://alpage.inria.fr/statgram/frdep/fr_stat_dep_parsing.html8We additionally considered adjectives and adverbs, but ourinitial tests yielded no parsing improvements.5?
One-Edge Context: ?obj?
N |avocat?
One-Edge Context: ?obj?
N(unlexicalized)?
Two-Edge Context: ?mod?
P |avec ?obj?
N |avocat?
Two-Edge Context: ?mod?
P |avec ?obj?
N(unlexicalized)Figure 2: Example dependency contexts for the verblemma manger.
The one-edge contexts corresponds tothe phrase ?manger un avocat?
(?eat an avocado?
), andthe two-edge contexts corresponds to the phrase ?mangeravec un avocat?
(?eat with a lawyer?
).probabilities estimated using frequency counts:I(x, c) = log(p(x, c)p(x)p(c))(9)Finally, we use the cosine metric to calculate the dis-tributional similarity between pairs of lemmas x, y:D(x, y) =?cI(x, c)I(y, c)?
(?cI(x, c)2)?
(?cI(y, c)2)(10)4.3.3 WordNet ASRFor WordNet synset experiments we use theFrench EuroWordNet9 (FREWN).
A WordNetsynset mapping10 allows us to convert synsets in theFREWN to Princeton WordNet version 3.0, and af-ter discarding a small number of synsets that arenot covered by the mapping we retain entries for9,833 nouns and 2,220 verbs.
We use NLTK, theNatural Language Toolkit (Bird et al, 2009), to cal-culate similarity between synsets.
As explained inSection 2.2, ASR is performed using the method ofMcCarthy et al (2004).
We use k=8 for the distri-butional nearest-neighbors to consider when rankingthe senses for a lemma, and we use the synset sim-ilarity function of Jiang and Conrath (1997), withdefault information content counts from NLTK cal-culated over the British National Corpus11.9http://www.illc.uva.nl/EuroWordNet/10http://nlp.lsi.upc.edu/tools/download-map.php11http://www.natcorp.ox.ac.uk/Source Evaluation SetVocabulary FTB Eval EMEA Eval EuroparlNounsFTB train 95.35 62.87 94.69Thesaurus 96.25 79.00 97.83FREWN 80.51 73.09 87.06VerbsFTB train 96.54 94.56 97.76Thesaurus 98.33 97.82 99.54FREWN 88.32 91.48 91.98Table 2: Lexical occurrence coverage (%) of sourcevocabularies over evaluation sets.
FTB Eval containsboth the FTB development and test sets, while EMEAEval contains both the EMEA development and test sets.Proper nouns are excluded from the analysis.4.3.4 HAC ClusteringFor the HAC clustering experiments in this paper,we use the CLUTO package12.
The distributionalthesauri described above are taken as input, and theUPGMA setting is used for cluster agglomeration.We test varying levels of clustering, with a parame-ter z which determines the proportion of cluster vo-cabulary size with respect to the original vocabularysize (8,171 for nouns and 2,865 for verbs).4.3.5 Resource CoverageThe coverage of our lexical resources over theFTB and two out-of-domain evaluation sets, at thelevel of token occurrences of verbs and commonnouns, is described in Table 2.
We can see thatthe FTB training set vocabulary provides better cov-erage than the FREWN for both nouns and verbs,while the coverage of the thesauri (and derived clus-ters) is the highest overall.4.4 Tuning and EvaluationWe evaluate four lexical target space configurationsagainst the baseline of lemmatization, tuning pa-rameters using ten-fold cross-validation on the FTBtraining set.
The feature templates are the same asthose in Table 1, with the difference that featuresinvolving lemmas are modified by the probabilisticfeature generalization technique described in Sec-tion 2.4, using the appropriate categorical distribu-tions.
In all configurations, we exclude the Frenchauxiliary verbs e?tre and avoir from participation inlexical generalization, and we replace proper nouns12http://glaros.dtc.umn.edu/gkhome/cluto/cluto/download6with a special lemma13.
Below we describe thetuned parameters for each configuration.?
RC: Replacement with cluster in ?cFor clusters and the parameter z (cf.
Section4.3.4), we settled on relative cluster vocabularysize z=0.6 for nouns and z=0.7 for verbs.
Wealso generalized lemmas not appearing in thedistributional thesaurus into a single unknownclass.?
PKNL: Probabilistic k-nearest lemmas in ?lFor the parameters k and m (cf.
Section 2.1),we settled on k=4 and m=0.5 for both nounsand verbs.
We also use the unknown class forlow-frequency lemmas, as in the RC configura-tion.?
RS: Replacement with first-sense (k=1) in ?sSince the FREWN has a lower-coverage vo-cabulary, we did not use an unknown class forout-of-vocabulary lemmas; instead, we mappedthem to unique senses.
In addition, we did notperform lexical generalization for verbs, due tolow cross-validation performance.?
PKPS: Probabilistic k-prevalent senses in ?sFor this setting we decided to not place anylimit on k, due to the large variation in thenumber of senses for different lemmas.
Asin the RS configuration, we mapped out-of-vocabulary lemmas to unique senses and didnot perform lexical generalization for verbs.5 ResultsTable 3 shows labeled attachment score (LAS) re-sults for our baseline parser (Lemmas) and four lex-ical generalization configurations.
For comparison,we also include results for a setting that only usesword forms (Forms), which was the baseline for pre-vious work on French dependency parsing (Canditoet al, 2010b).
Punctuation tokens are not scored,and significance is calculated using Dan Bikel?s ran-domized parsing evaluation comparator14, at signif-icance level p=0.05.13Proper nouns tend to have sparse counts, but for computa-tional reasons we did not include them in our distributional the-saurus construction.
We thus chose to simply generalize themParse Evaluation Set LASConfiguration FTB Test EMEA Dev EMEA Test EuroparlForms 86.85 84.08 85.41 86.01Lemmas 87.30 84.34 85.41 86.26RC 87.32 84.28 85.71* 86.28PKNL 87.46 84.63* 85.82* 86.26RS 87.34 84.48 85.54 86.34PKPS 87.41 84.63* 85.68* 86.22Table 3: Labeled attachment score (LAS) on in-domain(FTB) and out-of-domain (EMEA, Europarl) evaluationsets for the baseline (Lemmas) and four lexical general-ization configurations (RC, PKNL, RS, PKPS).
Signif-icant improvements over the baseline are starred.
Forcomparison, we also include a simpler setting (Forms),which does not use lemmas or morphological features.5.1 In-Domain ResultsOur in-domain evaluation yields slight improve-ments in LAS for some lexical generalization con-figurations, with PKNL performing the best.
How-ever, the improvements are not statistically signifi-cant.
A potential explanation for this disappointingresult is that the FTB training set vocabulary cov-ers the FTB test set at high rates for both nouns(95.25%) and verbs (96.54%), meaning that lexi-cal data sparseness is perhaps not a big problemfor in-domain dependency parsing.
While WordNetsynsets could be expected to provide the added ben-efit of taking word sense into account, sense ambi-guity is not really treated due to ASR not providingword sense disambiguation in context.5.2 Out-Of-Domain ResultsOur evaluation on the medical domain yields statisti-cally significant improvements in LAS, particularlyfor the two probabilistic target space approaches.PKNL and PKPS improve parsing for both theEMEA dev and test sets, while RC improves pars-ing for only the EMEA test set and RS does not sig-nificantly improve parsing for either set.
As in ourin-domain evaluation, PKNL performs the best over-all, though not significantly better than other lexi-cal generalization settings.
One explanation for theimprovement in the medical domain is the substan-tial increase in coverage of nouns in EMEA affordedinto a single class.14http://www.cis.upenn.edu/?dbikel/software.html7by the distributional thesaurus (+26%) and FREWN(+16%) over the base coverage afforded by the FTBtraining set.Our evaluation on the parliamentary domainyields no improvement in LAS across the differentlexical generalization configurations.
Interestingly,Candito and Seddah (2012) note that while Europarlis rather different from FTB in its syntax, its vocabu-lary is surprisingly similar.
From Table 2 we can seethat the FTB training set vocabulary has about thesame high level of coverage over Europarl (94.69%for nouns and 97.76% for verbs) as it does over theFTB evaluation sets (95.35% for nouns and 96.54%for verbs).
Thus, we can use the same reasoning asin our in-domain evaluation to explain the lack ofimprovement for lexical generalization methods inthe parliamentary domain.5.3 Lexical Feature Use During ParsingSince lexical generalization modifies the lexical fea-ture space in different ways, we also provide an anal-ysis of the extent to which each parsing model?s lex-ical features are used during in-domain and out-of-domain parsing.
Table 4 describes, for each config-uration, the number of lexical features stored in theparsing model along with the average lexical fea-ture use (ALFU) of classification instances (each in-stance represents a parse transition) during trainingand parsing.15Lexical feature use naturally decreases whenmoving from the training set to the evaluation sets,due to holes in lexical coverage outside of a parsingmodel?s training set.
The single-mapping configura-tions (RC, RS) do not increase the number of lexicalfeatures in a classification instance, which explainsthe fact that their ALFU on the FTB training set (6.0)is the same as that of the baseline.
However, the de-crease in ALFU when parsing the evaluation sets isless severe for these configurations than for the base-line: when parsing EMEA Dev with the RC configu-ration, where we obtain a significant LAS improve-ment over the baseline, the reduction in ALFU isonly 13% compared to 22% for the baseline parser.For the probabilistic generalization configurations,we also see decreases in ALFU when parsing the15We define the lexical feature use of a classification instanceto be the number of lexical features in the parsing model thatreceive non-zero values in the instance?s feature vector.Parse Lexical Feats Average Lexical Feature UseConfiguration In Model FTB Train FTB Dev EMEA DevLemmas 294k 6.0 5.5 4.7RC 150k 6.0 5.8 5.2PKNL 853k 15.7 14.8 12.0RS 253k 6.0 5.6 4.9PKPS 500k 9.2 8.6 7.0Table 4: Parsing model lexical features (rounded to near-est thousand) and average lexical feature use in classifi-cation instances across different training and evaluationsets, for the baseline (Lemmas) and four lexical general-ization configurations (PKNL, RC, PKPS, and RS).evaluation sets, though their higher absolute ALFUmay help explain the strong medical domain parsingperformance for these configurations.5.4 Impact on Running TimeAnother factor to note when evaluating lexical gen-eralization is the effect that it has on running time.Compared to the baseline, the single-mapping con-figurations (RC, RS) speed up feature extraction andprediction time, due to reduced dimensionality ofthe feature space.
On the other hand, the proba-bilistic generalization configurations (PKNL, PKPS)slow down feature extraction and prediction time,due to an increased dimensionality of the featurespace and a higher ALFU.
Running time is there-fore a factor that favors the single-mapping approachover our proposed probabilistic approach.Taking a larger view on our findings, we hy-pothesize that in order for lexical generalizationto improve parsing, an approach needs to achievetwo objectives: (i) generalize sufficiently to ensurethat lemmas not appearing in the training set arenonetheless associated with lexical features in thelearned parsing model; (ii) substantially increaselexical coverage over what the training set can pro-vide.
The first of these objectives seems to be ful-filled through our lexical generalization methods, asindicated in Table 4.
The second objective, how-ever, seems difficult to attain when parsing text in-domain, or even out-of-domain if the domains havea high lexical overlap (as is the case for Europarl).Only for our parsing experiments in the medical do-main do both objectives appear to be fulfilled, asevidenced by our LAS improvements when parsingEMEA with lexical generalization.86 Related WorkWe now discuss previous work concerning the use oflexical generalization for parsing, both in the classicin-domain setting and in the more recently popularout-of-domain setting.6.1 Results in Constituency-Based ParsingThe use of word classes for parsing dates back to thefirst works on generative constituency-based pars-ing, whether using semantic classes obtained fromhand-built resources or less-informed classes cre-ated automatically.
Bikel (2000) tried incorporat-ing WordNet-based word sense disambiguation intoa parser, but failed to obtain an improvement.
Xionget al (2005) generalized bilexical dependencies ina generative parsing model using Chinese semanticresources (CiLin and HowNet), obtaining improve-ments for Chinese parsing.
More recently, Agirreet al (2008) show that replacing words with Word-Net semantic classes improves English generativeparsing.
Lin et al (2009) use the HowNet resourcewithin the split-merge PCFG framework (Petrov etal., 2006) for Chinese parsing: they use the first-sense heuristic to append the most general hyper-nym to the POS of a token, obtaining a semantically-informed symbol refinement, and then guide furthersymbol splits using the HowNet hierarchy.
Otherwork has used less-informed classes, notably unsu-pervised word clusters.
Candito and Crabbe?
(2009)use Brown clusters to replace words in a generativePCFG-LA framework, obtaining substantial parsingimprovements for French.6.2 Results in Dependency ParsingIn dependency parsing, word classes are integratedas features in underlying linear models.
In a seminalwork, Koo et al (2008) use Brown clusters as fea-tures in a graph-based parser, improving parsing forboth English and Czech.
However, attempts to usethis technique for French have lead to no improve-ment when compared to the use of lemmatizationand morphological analysis (Candito et al, 2010b).Sagae and Gordon (2009) augment a transition-based English parser with clusters using unlexical-ized syntactic distributional similarity: each word isrepresented as a vector of counts of emanating un-lexicalized syntactic paths, with counts taken froma corpus of auto-parsed phrase-structure trees, andHAC clustering is performed using cosine similarity.For semantic word classes, (Agirre et al, 2011) inte-grate WordNet senses into a transition-based parserfor English, reporting small but significant improve-ments in LAS (+0.26% with synsets and +0.36%with semantic files) on the full Penn Treebank withfirst-sense information from Semcor.We build on previous work by attempting toreproduce, for French, past improvements for in-domain English dependency parsing with general-ized lexical classes.
Unfortunately, our results forFrench do not replicate the improvements for En-glish using semantic sense information (Agirre et al,2011) or word clustering (Sagae and Gordon, 2009).The primary difference between our paper and previ-ous work, though, is our evaluation of a novel prob-abilistic approach for lexical generalization.6.3 Out-Of-Domain ParsingConcerning techniques for improving out-of-domain parsing, a related approach has been to useself-training with auto-parsed out-of-domain data,as McClosky and Charniak (2008) do for Englishconstituency parsing, though in that approachlexical generalization is not explicitly performed.Candito et al (2011) use word clustering for do-main adaptation of a PCFG-LA parser for French,deriving clusters from a corpus containing textfrom both the source and target domains, and theyobtain parsing improvements in both domains.We are not aware of previous work on the use oflexical generalization for improving out-of-domaindependency parsing.7 ConclusionWe have investigated the use of probabilistic lexi-cal target spaces for reducing lexical data sparse-ness in a transition-based dependency parser forFrench.
We built a distributional thesaurus from anautomatically-parsed large text corpus, using it togenerate word clusters and perform WordNet ASR.We tested a standard approach to lexical gener-alization for parsing that has been previously ex-plored, where a word is mapped to a single clusteror synset.
We also introduced a novel probabilis-tic lexical generalization approach, where a lemma9is represented by a categorical distribution over thespace of lemmas, clusters, or synsets.
Probabilitiesfor the lemma space were calculated using the dis-tributional thesaurus, and probabilities for the Word-Net synset space were calculated using ASR senseprevalence scores, with probabilistic clusters left forfuture work.Our experiments with an arc-eager transition-based dependency parser resulted in modest but sig-nificant improvements in LAS over the baselinewhen parsing out-of-domain medical text.
However,we did not see statistically significant improvementsover the baseline when parsing in-domain text orout-of-domain parliamentary text.
An explanationfor this result is that the French Treebank training setvocabulary has a very high lexical coverage over theevaluation sets in these domains, suggesting that lex-ical generalization does not provide much additionalbenefit.
Comparing the standard single-mapping ap-proach to the probabilistic generalization approach,we found a slightly (though not significantly) betterperformance for probabilistic generalization acrossdifferent parsing configurations and evaluation sets.However, the probabilistic approach also has thedownside of a slower running time.Based on the findings in this paper, our focusfor future work on lexical generalization for de-pendency parsing is to continue improving parsingperformance on out-of-domain text, specifically forthose domains where lexical variation is high withrespect to the training set.
One possibility is toexperiment with building a distributional thesaurusthat uses text from both the source and target do-mains, similar to what Candito et al (2011) didwith Brown clustering, which may lead to a strongerbridging effect across domains for probabilistic lex-ical generalization methods.AcknowledgmentsThis work was funded in part by the ANR projectSequoia ANR-08-EMER-013.ReferencesA.
Abeille?
and N. Barrier.
2004.
Enriching a French tree-bank.
In Proceedings of the 4th International Confer-ence on Language Resources and Evaluation, Lisbon,Portugal, May.E.
Agirre, T. Baldwin, and D. Martinez.
2008.
Improv-ing parsing and PP attachment performance with senseinformation.
In Proceedings of the 46th Annual Meet-ing of the Association for Computational Linguistics,pages 317?325, Columbus, Ohio, June.E.
Agirre, K. Bengoetxea, K. Gojenola, and J. Nivre.2011.
Improving dependency parsing with semanticclasses.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics, pages699?703, Portland, Oregon, June.D.M.
Bikel.
2000.
A statistical model for parsing andword-sense disambiguation.
In Proceedings of theEMNLP/VLC-2000, pages 155?163, Hong Kong, Oc-tober.S.
Bird, E. Loper, and E. Klein.
2009.
Natural LanguageProcessing with Python.
O?Reilly Media Inc.P.F.
Brown, P.V.
Desouza, R.L.
Mercer, V.J.D.
Pietra, andJ.C.
Lai.
1992.
Class-based n-gram models of naturallanguage.
Computational Linguistics, 18(4):467?479.R.C.
Bunescu.
2008.
Learning with probabilistic fea-tures for improved pipeline models.
In Proceedings ofthe Conference on Empirical Methods in Natural Lan-guage Processing, pages 670?679, Honolulu, Hawaii,October.M.
Candito and B. Crabbe?.
2009.
Improving generativestatistical parsing with semi-supervised word cluster-ing.
In Proceedings of the 11th International Confer-ence on Parsing Technologies, pages 138?141, Paris,France, October.M.
Candito and D. Seddah.
2012.
Le corpus Sequoia :annotation syntaxique et exploitation pour l?adaptationd?analyseur par pont lexical.
In Actes de la 19e`meconfe?rence sur le traitement automatique des languesnaturelles, Grenoble, France, June.
To Appear.M.
Candito, B.
Crabbe?, and P. Denis.
2010a.
StatisticalFrench dependency parsing: Treebank conversion andfirst results.
In Proceedings of the 7th InternationalConference on Language Resources and Evaluation,Valetta, Malta, May.M.
Candito, J. Nivre, P. Denis, and E. Henestroza An-guiano.
2010b.
Benchmarking of statistical depen-dency parsers for French.
In Proceedings of the 23rdInternational Conference on Computational Linguis-tics, pages 108?116, Beijing, China, August.M.
Candito, E. Henestroza Anguiano, D. Seddah, et al2011.
A Word Clustering Approach to Domain Adap-tation: Effective Parsing of Biomedical Texts.
In Pro-ceedings of the 12th International Conference on Pars-ing Technologies, Dublin, Ireland, October.J.R.
Curran.
2004.
From distributional to semantic simi-larity.
Ph.D. thesis, University of Edinburgh.P.
Denis and B. Sagot.
2009.
Coupling an annotated cor-pus and a morphosyntactic lexicon for state-of-the-art10POS tagging with less human effort.
In Proceedingsof the 23rd Pacific Asia Conference on Language, In-formation and Computation, Hong Kong, China, De-cember.C.
Fellbaum, editor.
1998.
WordNet: An Electronic Lex-ical Database.
MIT Press, Cambridge, MA.E.
Henestroza Anguiano and P. Denis.
2011.
FreDist:Automatic construction of distributional thesauri forFrench.
In Actes de la 18e`me confe?rence sur le traite-ment automatique des langues naturelles, pages 119?124, Montpellier, France, June.J.J.
Jiang and D.W. Conrath.
1997.
Semantic similaritybased on corpus statistics and lexical taxonomy.
In In-ternational Conference on Research in ComputationalLinguistics, Taiwan.T.
Koo, X. Carreras, and M. Collins.
2008.
Simple semi-supervised dependency parsing.
In Proceedings of the46th Annual Meeting of the Association for Compu-tational Linguistics, pages 595?603, Columbus, Ohio,June.X.
Lin, Y.
Fan, M. Zhang, X. Wu, and H. Chi.
2009.
Re-fining grammars for parsing with hierarchical semanticknowledge.
In Proceedings of the 2009 Conference onEmpirical Methods in Natural Language Processing,pages 1298?1307, Singapore, August.D.
Lin.
1998.
Automatic retrieval and clustering of simi-lar words.
In Proceedings of the 36th Annual Meetingof the Association for Computational Linguistics and17th International Conference on Computational Lin-guistics, Volume 2, pages 768?774, Montreal, Quebec,August.D.
McCarthy, R. Koeling, J. Weeds, and J. Carroll.
2004.Finding predominant word senses in untagged text.In Proceedings of the 42nd Meeting of the Associa-tion for Computational Linguistics, pages 279?286,Barcelona, Spain, July.D.
McClosky and E. Charniak.
2008.
Self-training forbiomedical parsing.
In Proceedings of the 46th AnnualMeeting of the Association for Computational Linguis-tics, pages 101?104, Columbus, Ohio, June.J.
Nivre, J.
Hall, J. Nilsson, G. Eryi it, and S. Marinov.2006.
Labeled pseudo-projective dependency pars-ing with support vector machines.
In Proceedings ofthe Tenth Conference on Computational Natural Lan-guage Learning, pages 221?225, New York City, NY,June.J.
Nivre, J.
Hall, J. Nilsson, A. Chanev, G. Eryigit,S.
Ku?bler, S. Marinov, and E. Marsi.
2007.
Malt-Parser: A language-independent system for data-driven dependency parsing.
Natural Language Engi-neering, 13(02):95?135.J.
Nivre.
2003.
An efficient algorithm for projective de-pendency parsing.
In Proceedings of the 8th Interna-tional Workshop on Parsing Technologies, pages 149?160, Nancy, France, April.S.
Petrov, L. Barrett, R. Thibaux, and D. Klein.
2006.Learning accurate, compact, and interpretable tree an-notation.
In Proceedings of the 21st InternationalConference on Computational Linguistics and the 44thannual meeting of the Association for ComputationalLinguistics, pages 433?440, Sydney, Australia, July.K.
Sagae and A. Gordon.
2009.
Clustering words bysyntactic similarity improves dependency parsing ofpredicate-argument structures.
In Proceedings of the11th International Conference on Parsing Technolo-gies, pages 192?201, Paris, France, October.B.
Sagot.
2010.
The Lefff, a freely available, accurateand large-coverage lexicon for French.
In Proceed-ings of the 7th International Conference on LanguageResources and Evaluation, Valetta, Malta, May.J.
Tiedemann.
2009.
News from OPUS - A collection ofmultilingual parallel corpora with tools and interfaces.In Recent Advances in Natural Language Processing,volume 5, pages 237?248.
John Benjamins, Amster-dam.D.
Xiong, S. Li, Q. Liu, S. Lin, and Y. Qian.
2005.
Pars-ing the penn chinese treebank with semantic knowl-edge.
In Proceedings of the International Joint Con-ference on Natural Language Processing, pages 70?81, Jeju Island, Korea, October.H.
Yamada and Y. Matsumoto.
2003.
Statistical depen-dency analysis with support vector machines.
In Pro-ceedings of the 8th International Workshop on ParsingTechnologies, pages 195?206, Nancy, France, April.11
