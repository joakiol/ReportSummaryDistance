Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1379?1387,Beijing, August 2010Syntactic Scope Resolution in Uncertainty AnalysisLilja ?vrelid??
and Erik Velldal?
and Stephan Oepen??
University of Oslo, Department of Informatics?Universit?t Potsdam, Institut f?r Linguistikovrelid@uni-potsdam.de and erikve@ifi.uio.no and oe@ifi.uio.noAbstractWe show how the use of syntactic struc-ture enables the resolution of hedge scopein a hybrid, two-stage approach to un-certainty analysis.
In the first stage, aMaximum Entropy classifier, combiningsurface-oriented and syntactic features,identifies cue words.
With a small set ofhand-crafted rules operating over depen-dency representations in stage two, we at-tain the best overall result (in terms ofboth combined ranks and average F1) inthe 2010 CoNLL Shared Task.1 Background?MotivationRecent years have witnessed an increased interestin the analysis of various aspects of sentiment innatural language (Pang & Lee, 2008).
The sub-task of hedge resolution deals with the analysis ofuncertainty as expressed in natural language, andthe linguistic means (so-called hedges) by whichspeculation or uncertainty are expressed.
Infor-mation of this kind is of importance for variousmining tasks which aim at extracting factual data.Example (1), taken from the BioScope corpus(Vincze, Szarvas, Farkas, M?ra, & Csirik, 2008),shows a sentence where uncertainty is signaled bythe modal verb may.1(1) {The unknown amino acid ?may?
be used by thesespecies}.The topic of the Shared Task at the 2010 Con-ference for Natural Language Learning (CoNLL)is hedge detection in biomedical literature?in asense ?zooming in?
on one particular aspect of thebroader BioNLP Shared Task in 2009 (Kim, Ohta,Pyysalo, Kano, & Tsujii, 2009).
It involves twosubtasks: Task 1 is described as learning to detect1In examples throughout this paper, angle brackets high-light hedge cues, and curly braces indicate the scope of agiven cue, as annotated in BioScope.sentences containing uncertainty; the objective ofTask 2 is learning to resolve the in-sentence scopeof hedge cues (Farkas, Vincze, Mora, Csirik, &Szarvas, 2010).
The organizers further suggest:This task falls within the scope of semantic analy-sis of sentences exploiting syntactic patterns [...].The utility of syntactic information within var-ious approaches to sentiment analysis in natu-ral language has been an issue of some debate(Wilson, Wiebe, & Hwa, 2006; Ng, Dasgupta,& Arifin, 2006), and the potential contribution ofsyntax clearly varies with the specifics of the task.Previous work in the hedging realm has largelybeen concerned with cue detection, i.e.
identify-ing uncertainty cues such as may in (1), whichare predominantly individual tokens (Medlock &Briscoe, 2007; Kilicoglu & Bergler, 2008).
Therehas been little previous work aimed at actuallyresolving the scope of such hedge cues, whichpresumably constitutes a somewhat different andlikely more difficult problem.
Morante and Daele-mans (2009) present a machine-learning approachto this task, using token-level, lexical informa-tion only.
To this end, CoNLL 2010 enters largelyuncharted territory, and it remains to be seen (a)whether syntactic analysis indeed is a necessarycomponent in approaching this task and, moregenerally, (b) to what degree the specific tasksetup can inform us about the strong and weakpoints in current approaches and technology.In this article, we investigate the contributionof syntax to hedge resolution, by reflecting on ourexperience in the CoNLL 2010 task.2 Our CoNLLsystem submission ranked fourth (of 24) on Task 1and third (of 15) on Task 2, for an overall best av-erage result (there appears to be very limited over-lap among top performers for the two subtasks).2It turns out, in fact, that all the top-performing systemsin Task 2 of the CoNLLShared Task rely on syntactic informa-tion provided by parsers, either in features for machine learn-ing or as input to manually crafted rules (Morante, Asch, &Daelemans, 2010; Rei & Briscoe, 2010).1379Sentences Hedged Cues Multi-Word Tokens Cue TokensSentences CuesAbstracts 11871 2101 2659 364 309634 3056Articles 2670 519 668 84 68579 782Total 14541 2620 3327 448 378213 3838Table 1: Summary statistics for the Shared Task training data.This article transcends our CoNLL system descrip-tion (Velldal, ?vrelid, & Oepen, 2010) in severalrespects, presenting updated and improved cue de-tection results (?
3 and ?
4), focusing on the roleof syntactic information rather than on machinelearning specifics (?
5 and ?
6), providing an anal-ysis and discussion of Task 2 errors (?
7), and gen-erally aiming to gauge the value of available anno-tated data and processing tools (?
8).
We presenta hybrid, two-level approach for hedge resolution,where a statistical classifier detects cue words, anda small set of manually crafted rules operatingover syntactic structures resolve scope.
We showhow syntactic information?produced by a data-driven dependency parser complemented with in-formation from a ?deep?, hand-crafted grammar?contributes to the resolution of in-sentence scopeof hedge cues, discussing various types of syn-tactic constructions and associated scope detec-tion rules in considerable detail.
We furthermorepresent a manual error analysis, which reveals re-maining challenges in our scope resolution rulesas well as several relevant idiosyncrasies of thepreexisting BioScope annotation.2 Task, Data, and System BasicsTask Definition and Evaluation MetricsTask 1 is a binary sentence classification task:identifying utterances as being certain or uncer-tain.
Following common practice, this subtaskis evaluated in terms of precision, recall, andF1 for the ?positive?
class, i.e.
uncertain.
Inour work, we approach Task 1 as a byproductof the full hedge resolution problem, labeling asentence as uncertain if it contains at least onetoken classified as a hedge cue.
In addition tothe sentence-level evaluation for Task 1, we alsopresent precision, recall, and F1 for the cue-level.Task 2 comprises two subtasks: cue detectionand scope resolution.
The official CoNLL eval-uation does not tease apart these two aspects ofthe problem, however: Only an exact match ofboth the cue and scope bracketing (in terms ofsubstring positions) will be counted as a success,again quantified in terms of precision, recall, andF1.
Discussing our results below, we report cuedetection and scope resolution performance sepa-rately, and further put scope results into perspec-tive against an upper bound based on the gold-standard cue annotation.Besides the primary biomedical domain data,some annotated Wikipedia data was providedfor Task 1, and participating systems are classi-fied as in-domain (using exclusively the domain-specific data), cross-domain (combining bothtypes of training data), or open (utilizing addi-tional uncertainty-related resources).
In our work,we focus on the interplay of syntax and the morechallenging Task 2; we ignored the Wikipediatrack in Task 1.
Despite our using general NLPtools (see below), our system falls into the mostrestrictive, in-domain category.Training and Evaluation Data The trainingdata for the CoNLL 2010 Shared Task is taken fromthe BioScope corpus (Vincze et al, 2008) andconsists of 14,541 ?sentences?
(or other root-levelutterances) from biomedical abstracts and articles(see Table 1).3 The BioScope corpus providesannotation for hedge cues as well as their scope.According to the annotation guidelines (Vincze etal., 2008), the annotation adheres to a principleof minimalism when it comes to hedge cues, i.e.the minimal unit expressing hedging is annotated.The inverse is true of scope annotations, which ad-here to a principle of maximal scope?meaningthat scope should be set to the largest syntactic3As it was known beforehand that evaluation would drawon full articles only, we put more emphasis on the articlesubset of the training data, for example in cross validationtesting and manual diagnosis of errors.1380ID FORM LEMMA POS FEATS HEAD DEPREL XHEAD XDEP1 The the DT _ 4 NMOD 4 SPECDET2 unknown unknown JJ degree:attributive 4 NMOD 4 ADJUNCT3 amino amino JJ degree:attributive 4 NMOD 4 ADJUNCT4 acid acid NN pers:3|case:nom|num:sg|ntype:common 5 SBJ 3 SUBJ5 may may MD mood:ind|subcat:MODAL|tense:pres|clauseType:decl 0 ROOT 0 ROOT6 be be VB _ 5 VC 7 PHI7 used use VBN subcat:V-SUBJ-OBJ|vtype:main|passive:+ 6 VC 5 XCOMP8 by by IN _ 7 LGS 9 PHI9 these these DT deixis:proximal 10 NMOD 10 SPECDET10 species specie NNS num:pl|pers:3|case:obl|common:count|ntype:common 8 PMOD 7 OBL-AG11 .
.
.
_ 5 P 0 PUNCTable 2: Stacked dependency representation of example (1), with MaltParser and XLE annotations.unit possible.For evaluation purposes, the task organizersprovided newly annotated biomedical articles, fol-lowing the same general BioScope principles.
TheCoNLL 2010 evaluation data comprises 5,003 ad-ditional utterances (138,276 tokens), of which 790are annotated as hedged.
The data contains a to-tal of 1033 cues, of which 87 are so-called multi-word cues (i.e.
cues spanning multiple tokens),comprising 1148 cue tokens altogether.Stacked Dependency Parsing For syntacticanalysis we employ the open-source MaltParser(Nivre, Hall, & Nilsson, 2006), a platform fordata-driven dependency parsing.
For improvedaccuracy and portability across domains and gen-res, we make our parser incorporate the pre-dictions of a large-scale, general-purpose LFGparser?following the work of ?vrelid, Kuhn, andSpreyer (2009).
A technique dubbed parser stack-ing enables the data-driven parser to learn, notonly from gold standard treebank annotations, butfrom the output of another parser (Nivre & Mc-Donald, 2008).
This technique has been shown toprovide significant improvements in accuracy forboth English and German (?vrelid et al, 2009),and a similar setup employing an HPSG gram-mar has been shown to increase domain indepen-dence in data-driven dependency parsing (Zhang& Wang, 2009).
The stacked parser combinestwo quite different approaches?data-driven de-pendency parsing and ?deep?
parsing with a hand-crafted grammar?and thus provides us with abroad range of different types of linguistic infor-mation for the hedge resolution task.MaltParser is based on a deterministic pars-ing strategy in combination with treebank-inducedclassifiers for predicting parse transitions.
It sup-ports a rich feature representation of the parse his-tory in order to guide parsing and may easily beextended to take additional features into account.The procedure to enable the data-driven parserto learn from the grammar-driven parser is quitesimple.
We parse a treebank with the XLE plat-form (Crouch et al, 2008) and the English gram-mar developed within the ParGram project (Butt,Dyvik, King, Masuichi, & Rohrer, 2002).
Wethen convert the LFG output to dependency struc-tures, so that we have two parallel versions of thetreebank?one gold standard and one with LFGannotation.
We extend the gold standard treebankwith additional information from the correspond-ing LFG analysis and train MaltParser on the en-hanced data set.Table 2 shows the enhanced dependency rep-resentation of example (1) above, taken from thetraining data.
For each token, the parsed data con-tains information on the word form, lemma, andpart of speech (PoS), as well as on the head anddependency relation in columns 6 and 7.
Theadded XLE information resides in the FEATS col-umn, and in the XLE-specific head and depen-dency columns 8 and 9.
Parser outputs, which inturn form the basis for our scope resolution rulesdiscussed in Section 5, also take this same form.The parser employed in this work is trained onthe Wall Street Journal sections 2 ?
24 of the PennTreebank (PTB), converted to dependency format(Johansson & Nugues, 2007) and extended withXLE features, as described above.
Parsing uses thearc-eager mode of MaltParser and an SVM witha polynomial kernel.
When tested using 10-foldcross validation on the enhanced PTB, the parserachieves a labeled accuracy score of 89.8.PoS Tagging and Domain Variation Ourparser is trained on financial news, and althoughstacking with a general-purpose LFG parser is ex-1381pected to aid domain portability, substantial dif-ferences in domain and genre are bound to neg-atively affect syntactic analysis (Gildea, 2001).MaltParser presupposes that inputs have been PoStagged, leaving room for variation in preprocess-ing.
On the one hand, we aim to make parserinputs maximally similar to its training data (i.e.the conventions established in the PTB); on theother hand we wish to benefit from specialized re-sources for the biomedical domain.The GENIA tagger (Tsuruoka et al, 2005) isparticularly relevant in this respect (as could bethe GENIA Treebank proper4).
However, wefound that GENIA tokenization does not match thePTB conventions in about one out of five sen-tences (for example wrongly splitting tokens like?390,926?
or ?Ca(2+)?
); also in tagging propernouns, GENIA systematically deviates from thePTB.
Hence, we adapted an in-house tokenizer(using cascaded finite-state rules) to the CoNLLtask, run two PoS taggers in parallel, and eclec-tically combine annotations across the variouspreprocessing components?predominantly giv-ing precedence to GENIA lemmatization and PoShypotheses.To assess the impact of improved, domain-adapted inputs on our hedge resolution system,we contrast two configurations: first, running theparser in the exact same manner as ?vrelid, Kuhn,and Spreyer (2010), we use TreeTagger (Schmid,1994) and its standard model for English (trainedon the PTB) for preprocessing; second, we give asinputs to the parser our refined tokenization andmerged PoS tags, as described above.
When eval-uating the two modes of preprocessing on the ar-ticles subset of the training data, and using gold-standard cues, our system for resolving cue scopes(presented in ?
5) achieves an F1 of 66.31 withTreeTagger inputs, and 72.30 using our refined to-kenization and tagger combination.
These resultsunderline the importance of domain adaptation foraccurate syntactic analysis, and in the followingwe assume our hybrid in-house setup.4Although the GENIA Treebank provides syntactic anno-tation in a form inspired by the PTB, it does not provide func-tion labels.
Therefore, our procedure for converting fromconstituency to dependency requires non-trivial adaptationbefore we can investigate the effects of retraining the parseragainst GENIA.3 Stage 1: Identifying Hedge CuesFor the task of identifying hedge cues, we devel-oped a binary maximum entropy (MaxEnt) clas-sifier.
The identification of cue words is usedfor (a) classifying sentences as certain/uncertain(Task 1), and (b) providing input to the syntac-tic rules that we later apply for resolving the in-sentence scope of the cues (Task 2).
We also re-port evaluation scores for the sub-task of cue de-tection in isolation.As annotated in the training data, it is possiblefor a hedge cue to span multiple tokens, e.g.
as inwhether or not.
The majority of the multi-wordcues in the training data are very infrequent, how-ever, most occurring only once, and the classifieritself is not sensitive to the notion of multi-wordcues.
Instead, the task of determining whether acue word forms part of a larger multi-word cue, isperformed in a separate post-processing step (ap-plying a heuristic rule targeted at only the mostfrequently occurring patterns of multi-word cuesin the training data).During development, we trained cue classifiersusing a wide variety of feature types, both syn-tactic and surface-oriented.
In the end, however,we found n-gram-based lexical features to havethe greatest contribution to classifier performance.Our best-performing classifier so far (see ?Final?in Table 3) includes the following feature types:n-grams over forms (up to 2 tokens to the right),n-grams over base forms (up to 3 tokens leftand right), PoS (from GENIA), subcategorizationframes (from XLE), and phrase-structural coordi-nation level (from XLE).
Our CoNLL system de-scription includes more details of the various otherfeature types that we experimented with (Velldalet al, 2010).4 Cue Detection EvaluationTable 3 summarizes the performance of our Max-Ent hedge cue classifier in terms of precision, re-call and F1, computed using the official SharedTask scorer script.
The sentence-level scores cor-respond to Task 1 of the Shared Task, and the cue-level scores are based on the exact-match countsfor full hedge cues (possibly spanning multiple to-kens).1382Sentence Level Cue LevelConfiguration Prec Rec F1 Prec Rec F1Baseline, Development 79.25 79.45 79.20 77.37 71.70 74.43Final, Development 91.39 86.78 89.00 90.18 79.47 84.49Final, Held-Out 85.61 85.06 85.33 81.97 76.41 79.10Table 3: Isolated evaluation of the hedge cue classifier.As the CoNLL test data was known beforehandto consist of articles only, in 10-fold cross vali-dation for classifier development we tested exclu-sively against the articles segment, while alwaysincluding all sentences from the abstracts in thetraining set.
This corresponds to the developmentresults in Table 3, while the held-out results arefor the official Shared Task evaluation data (train-ing on all the available training data).
A modelusing only unigram features serves as a baseline.5 Stage 2: Resolving ScopeHedge scope may vary quite a lot depending onlinguistic properties of the cue in question.
In ourapproach to scope resolution we rely heavily onsyntactic information, taken from the dependencystructures proposed by both MaltParser and XLE,as well as on various additional features relatingto specific syntactic constructions.We constructed a small set of heuristic ruleswhich define the scope for each cue detected inStage 1.
In developing these rules, we made useof the information provided by the guidelines forscope annotation in the BioScope corpus (Vinczeet al, 2008), combined with manual inspection ofthe training data in order to further generalize overthe phenomena discussed by Vincze et al (2008)and work out interactions of constructions for var-ious types of cues.The rules take as input a parsed sentence whichhas been further tagged with hedge cues.
Theyoperate over the dependency structures and ad-ditional features provided by the parser.
Defaultscope is set to start at the cue word and span tothe end of the sentence (modulo punctuation), andthis scope also provides the baseline for the eval-uation of our rules.
In the following, we discussbroad classes of rules, organized by categories ofhedge cues.
As there is no explicit representa-tion of phrase or clause boundaries in our depen-dency universe, we assume a set of functions overdependency graphs, for example finding the left-or rightmost (direct) dependent of a given node,or transitively selecting left- or rightmost descen-dants.Coordination The dependency analysis of co-ordination provided by our parser makes the firstconjunct the head of the coordination.
For cuesthat are coordinating conjunctions (PoS tag CC),such as or, we define the scope as spanning thewhole coordinate structure, i.e.
start scope is setto the leftmost dependent of the head of the coor-dination, e.g., roX in (2), and end scope is set toits rightmost dependent (conjunct), e.g., RNAs in(2).
This analysis provides us with coordinationsat various syntactic levels, such as NP and N (2),AP and AdvP, or VP (3):(2) [...] the {roX genes ?or?
RNAs} recruit the entire setof MSL proteins [...](3) [...] the binding interfaces are more often {kept ?or?even reused} rather than lost in the course ofevolution.Adjectives We distinguish between adjectives(JJ) in attributive (NMOD) function and adjectivesin predicative (PRD) function.
Attributive adjec-tives take scope over their (nominal) head, with allits dependents, as in (4) and (5):(4) The {?possible?
selenocysteine residues} are shownin red, [...](5) Extensive analysis of the flanks failed to show anyhallmarks of {?putative?
transposons that might beassociated with this RAG1-like protein}, [...]For adjectives in a predicative function the scopeincludes the subject argument of the head verb(the copula), as well as a (possible) clausal argu-ment, as in (6).
The scope does not, however, in-clude expletive subjects, as in (7).1383(6) Therefore, {the unknown amino acid, if it is encodedby a stop codon, is ?unlikely?
to exist in the currentdatabases of microbial genomes}.
(7) For example, it is quite {?likely?
that there exists anextremely long sequence that is entirely unique to U}.Verbs The scope of verbal cues is a bit morecomplex and depends on several factors.
In ourrules, we distinguish passive usages from activeusages, raising verbs from non-raising verbs, andthe presence or absence of a subject-control em-bedding context.
The scopes of both passive andraising verbs include the subject argument of theirhead verb, as in (8) and (9), unless it is an exple-tive pronoun, as in (10).
(8) {Interactions determined by high-throughput methodsare generally ?considered?
to be less reliable thanthose obtained by low-throughput studies} 1314 andas a consequence [...](9) {Genomes of plants and vertebrates ?seem?
to be freeof any recognizable Transib transposons} (Figure 1).
(10) It has been {?suggested?
that unstructured regions ofproteins are often involved in binding interactions,particularly in the case of transient interactions} 77.In the case of subject control involving a hedgecue, specifically modals, subject arguments are in-cluded in scopes where the controller heads a pas-sive construction or a raising verb, as in exam-ple (1) above, repeated here for convenience:(11) {The unknown amino acid ?may?
be used by thesespecies}.In general, the end scope of verbs should ex-tend over the minimal clause that contains the verbin question.
In terms of dependency structures,we define the clause boundary as comprising thechain of descendants of a verb which is not inter-vened by a token with a higher attachment in thegraph than the verb in question.
In example (8)for instance, the sentence-level conjunction andmarks the end of the clause following the cue con-sidered.Prepositions and Adverbs Cues that are taggedas prepositions (including some complementizers)take scope over their argument, with all its de-scendants, (12).
Adverbs take scope over theirhead with all its (non-subject) syntactic descen-dants (13).Configuration F1BSPDefault, Gold Cues 45.21Rules, Gold Cues 72.31Rules, System Cues 64.77BSE Rules, Gold Cues 66.73Rules, System Cues 55.75Table 4: Evaluation of scope resolution rules.
(12) {?Whether?
the codon aligned to the inframe stopcodon is a nonsense codon or not} was neglected atthis stage.
(13) These effects are {?probably?
mediated through the1,25(OH)2D3 receptor}.Multi-Word Cues In the case of multi-wordcues, such as indicate that or either ... or, we needto determine the head of the multi-word unit.
Wethen set the scope of the whole unit to the scopeof the head token.As an illustration of rule processing, considerour running example (11), with its syntactic anal-ysis as shown in Table 2 above.
This exampleinvokes a variety of syntactic properties, includ-ing parts of speech, argumenthood, voice etc.
Ini-tially, the scope of the hedge cue is set to defaultscope.
Then the subject control rule is applied,which checks the properties of the verbal argu-ment used, going through a chain of verbal depen-dents from the modal verb.
Since it is marked aspassive in the LFG analysis, the start scope is set toinclude the subject of the cue word (the leftmostdescendant in its SBJ dependent).6 Rule EvaluationTable 4 summarizes scope resolution performance(viewed as a an isolated subtask) for various con-figurations, both against the articles section of theCoNLL training data (dubbed BSP) and against theheld-out evaluation data (BSE).
First of all, we notethat the ?default scope?
baseline is quite strong:unconditionally extending the scope of a cue tothe end of the sentence yields an F1 of 45.21.Given gold standard cue information, our scoperules improve on the baseline by 27 points on thearticles section of the data set, for an F1 of 72.31;with system-assigned hedge cues, our rules still1384achieve an F1 of 64.77.
Note that scope resolu-tion scores based on classified cues also yield theend-to-end system evaluation for Task 2.The bottom rows of Table 4 show the evaluationof scope rules on the CoNLL held-out test data.
Us-ing system cues, scope resolution on the held-outdata scores at 55.75 F1.
Comparing to the resulton the (articles portion of the) training data, weobserve a substantial drop in performance (of sixpoints with gold-standard cues, nine points withsystem cues).
There are several possible explana-tions for this effect.
First of all, there may wellbe a certain degree of overfitting of our rules tothe training data.
The held-out data may containhedging constructions that are not covered by ourcurrent set of scope rules, or annotation of parallelconstructions may in some cases differ in subtleways (see ?
7 below).
Moreover, scope resolutionperformance is of course influenced by cue detec-tion (see Table 3).
The cue-level F1 of our sys-tem on the held-out data set is 79.10, compared to84.49 (using cross validation) on the training data.This drop in cue-level performance appears to af-fect classification precision far more than recall.Of course, given that our heuristics for identifyingmulti-word cues were based on patterns extractedfrom the training data, some loss in the cue-levelscore was expected.7 Error AnalysisTo start shedding some light on the significanceof our results, we performed a manual error anal-ysis on the article portion of the training material(BSP), with two of the authors (trained linguists)working in tandem.
Using gold-standard cues,our scope resolution rules fail to exactly replicatethe target annotation in 185 (of 668) cases, corre-sponding to 72.31 F1 in Table 4 above.
Our eval-uators reviewed and discussed these 185 cases,classifying 156 (84%) as genuine system errors,22 (12%) as likely5 annotation errors, and a re-5In some cases, there is no doubt that annotation is er-roneous, i.e.
in violation of the available annotation guide-lines (Vincze et al, 2008) or in conflict with otherwise un-ambiguous patterns.
In other cases, however, judgments arenecessarily based on generalizations made by the evaluators,i.e.
assumptions about the underlying system and syntacticanalyses implicit in the BioScope annotations.
Furthermore,selecting items for manual analysis that do not align with themaining seven cases as involving controversial orseemingly arbitrary decisions.The two most frequent classes of system er-rors pertain (a) to the recognition of phrase andclause boundaries and (b) to not dealing success-fully with relatively superficial properties of thetext.
Examples (14) and (15) illustrate the firstclass of errors, where in addition to the gold-standard annotation we use vertical bars (?|?)
toindicate scope predictions of our system.
(14) [...] {the reverse complement |mR of m will be?considered?
to be [...]|}(15) This |{?might?
affect the results} if there is asystematic bias on the composition of a proteininteraction set|.In our syntax-driven approach to scope resolution,system errors will almost always correspond to afailure in determining constituent boundaries, in avery general sense.
However, specifically exam-ple (15) is indicative of a key challenge in thistask, where adverbials of condition, reason, orcontrast frequently attach within the dependencydomain of a hedge cue, yet are rarely included inthe scope annotation.Example (16) demonstrates our second fre-quent class of system errors.
One in six itemsin the BSP training data contains a sentence-finalparenthesized element or trailing number, as forexample (2), (9), or (10) above; most of these arebibliographic or other in-text references, whichare never included in scope annotation.
Hence,our system includes a rule to ?back out?
from trail-ing parentheticals; in examples like (16), how-ever, syntax does not make explicit the contrastbetween an in-text reference vs. another type ofparenthetical.
(16) More specifically, {|the bristle and leg phenotypes are?likely?
to result from reduced signaling by Dl| (andnot by Ser)}.Moving on to apparent annotation errors, therules for inclusion (or not) of the subject inthe scope of verbal hedge cues and decisionson boundaries (or internal structure) of nominalspredictions made by our scope resolution rules is likely tobias our sample, such that our estimated proportion of 12%annotation errors cannot be used to project an overall errorrate.1385seem problematic?as illustrated in examples (17)to (22).6(17) [...] and |this is also {?thought?
to be true for the fullprotein interaction networks we are modeling}|.
(18) [...] {Neur |?can?
promote Ser signaling|}.
(19) |Some of the domain pairs {?seem?
to mediate a largenumber of protein interactions, thus acting as reusableconnectors}|.
(20) One {|?possible?
explanation| is functionalredundancy with the mouse Neur2 gene}.
(21) [...] |redefinition of {one of them is ?feasible?}|.
(22) |The {Bcl-2 family ?appears?
to function [...]}|.Finally, the difficult corner cases invoke non-constituent coordination, ellipsis, or NP-initial fo-cus adverbs?and of course interactions of thephenomena discussed above.
Without making thesyntactic structures assumed explicit, it is oftenvery difficult to judge such items.8 Reflections ?
OutlookOur combination of stacked dependency parsingand hand-crafted scope resolution rules provedadequate for the CoNLL 2010 competition, con-firming the central role of syntax in this task.With a comparatively small set of rules (imple-mented in a few hundred lines of code), con-structed through roughly two full weeks of ef-fort (studying BioScope annotations and develop-ing rules), our CoNLL system achieved an end-to-end F1 of 55.33 on Task 2.7 The two submis-sions with better results (at 57.32 and 55.65 F1)represent groups who have pioneered the hedgeanalysis task in previous years (Morante et al,2010; Rei & Briscoe, 2010).
Scores for other ?in-domain?
participants range from 52.24 to 2.15 F1.6Like in the presentation of system errors, we includescope predictions of our own rules here too, which we be-lieve to be correct in these cases.
Also in this class of errors,we find the occasional ?uninteresting?
mismatch, for exam-ple related to punctuation marks and inconsistencies aroundparentheses.7In ?
4 and ?
6 above, we report scores for a slightly im-proved version of our system, where (after the official CoNLLsubmission date) we eliminated a bug related to the treatmentof sentence-initial whitespace in the XML annotations.
At anend-to-end F1 of 55.75, this system would outrank the sec-ond best performer in Task 2.Doubtless there is room for straightforward exten-sion: for example retraining our parser on the GE-NIA Treebank, further improving the cue classifier,and refining scope resolution rules in the light ofthe error analysis above.At the same time, we remain mildly am-bivalent about the long-term impact of some ofthe specifics of the 2010 CoNLL task.
Sharedtasks (i.e.
system bake-offs) have become increas-ingly popular in past years, and in some sub-fields (e.g.
IE, SMT, or dependency parsing) high-visibility competitions can shape community re-search agendas.
Hence, even at this early stage, itseems appropriate to reflect on the possible con-clusions to be drawn from the 2010 hedge res-olution task.
First, we believe the harsh ?exactsubstring match?
evaluation metric underestimatesthe degree to which current technology can solvethis problem; furthermore, idiosyncratic, string-level properties (e.g.
the exact treatment of punc-tuation or parentheticals) may partly obscure theinterpretation of methods used and correspondingsystem performance.These effects are compounded by some con-cerns about the quality of available annotation.Even though we tried fine-tuning our cross vali-dation testing to the nature of the evaluation data(comprising only articles), our system performssubstantially worse on the newly annotated CoNLLtest data, in both stages.8 In our view, the anno-tation of hedge cues and scopes ideally would beovertly related to at least some level of syntacticannotation?as would in principle be possible forthe segment of BioScope drawing on the abstractsof the GENIA Treebank.AcknowledgementsWe are grateful to the organizers of the 2010CoNLL Shared Task and creators of the BioScoperesource; first, for engaging in these kinds of com-munity service, and second for many in-depth dis-cussions of annotation and task details.
We thankour colleagues at the Universities of Oslo andPotsdam for their comments and support.8We are leaving open the possibility to further refine oursystem; we have therefore abstained from an error analysison the evaluation data so far.1386ReferencesButt, M., Dyvik, H., King, T. H., Masuichi, H., & Rohrer,C.
(2002).
The Parallel Grammar Project.
In Proceed-ings of COLING workshop on grammar engineering andevaluation (pp.
1 ?
7).
Taipei, Taiwan.Crouch, D., Dalrymple, M., Kaplan, R., King, T., Maxwell,J., & Newman, P. (2008).
XLE documentation.
Palo Alto,CA.
(Palo Alto Research Center)Farkas, R., Vincze, V., Mora, G., Csirik, J., & Szarvas, G.(2010).
The CoNLL 2010 Shared Task: Learning to de-tect hedges and their scope in natural language text.
InProceedings of the 14th Conference on Natural LanguageLearning.
Uppsala, Sweden.Gildea, D. (2001).
Corpus variation and parser perfor-mance.
In Proceedings of the 2001 conference on Empir-ical Methods in Natural Language Processing (pp.
167 ?202).
Pittsburgh, PA.Johansson, R., & Nugues, P. (2007).
Extended constituent-to-dependency conversion for English.
In J. Nivre, H.-J.
Kaalep, & M. Koit (Eds.
), Proceedings of NODALIDA2007 (p. 105-112).
Tartu, Estonia.Kilicoglu, H., & Bergler, S. (2008).
Recognizing speculativelanguage in biomedical research articles: A linguisticallymotivated perspective.
In Proceedings of the BioNLP2008 Workshop.
Columbus, OH, USA.Kim, J.-D., Ohta, T., Pyysalo, S., Kano, Y., & Tsujii, J.(2009).
Overview of BioNLP 2009 Shared Task on eventextraction.
In Proceedings of the BioNLP 2009 workshopcompanion volume for shared task (pp.
1 ?
9).
Boulder,CO: Association for Computational Linguistics.Medlock, B., & Briscoe, T. (2007).
Weakly supervised learn-ing for hedge classification in scientific literature.
In Pro-ceedings of the 45th Meeting of the Association for Com-putational Linguistics (pp.
992 ?
999).
Prague, Czech Re-public: Association for Computational Linguistics.Morante, R., Asch, V. V., & Daelemans, W. (2010).Memory-based resolution of in-sentence scope of hedgecues.
In Proceedings of the 14th Conference on NaturalLanguage Learning (pp.
40 ?
47).
Uppsala, Sweden.Morante, R., & Daelemans, W. (2009).
Learning the scopeof hedge cues in biomedical texts.
In Proceedings ofthe BioNLP 2009 Workshop (pp.
28 ?
36).
Boulder, Col-orado.Ng, V., Dasgupta, S., & Arifin, S. M. N. (2006).
Examin-ing the role of linguistic knowledge sources in the auto-matic identification and classification of reviews.
In Pro-ceedings of the 21st International Conference on Compu-tational Linguistics and the 44th Annual Meeting of theAssociation for Computational Linguistics.
Sydney, Aus-tralia.Nivre, J., Hall, J., & Nilsson, J.
(2006).
MaltParser: A data-driven parser-generator for dependency parsing.
In Pro-ceedings of the Fifth International Conference on Lan-guage Resources and Evaluation (p. 2216-2219).
Genoa,Italy.Nivre, J., & McDonald, R. (2008, June).
Integrating graph-based and transition-based dependency parsers.
In Pro-ceedings of the 46th Meeting of the Association for Com-putational Linguistics (pp.
950 ?
958).
Columbus, Ohio.
?vrelid, L., Kuhn, J., & Spreyer, K. (2009).
Improving data-driven dependency parsing using large-scale LFG gram-mars.
In Proceedings of the 47th Meeting of the Associ-ation for Computational Linguistics (pp.
37 ?
40).
Singa-pore.
?vrelid, L., Kuhn, J., & Spreyer, K. (2010).
Cross-framework parser stacking for data-driven dependencyparsing.
TAL 2010 special issue on Machine Learningfor NLP, 50(3).Pang, B., & Lee, L. (2008).
Opinion mining and senti-ment analysis.
Foundations and Trends in InformationRetrieval, 2(1-2).Rei, M., & Briscoe, T. (2010).
Combining manual rules andsupervised learning for hedge cue and scope detection.
InProceedings of the 14th Conference on Natural LanguageLearning (pp.
56 ?
63).
Uppsala, Sweden.Schmid, H. (1994).
Probabilistic part-of-speech tagging us-ing decision trees.
In International conference on newmethods in language processing (p. 44-49).
Manchester,England.Tsuruoka, Y., Tateishi, Y., Kim, J.-D., Ohta, T., McNaught,J., Ananiadou, S., et al (2005).
Developing a robustPart-of-Speech tagger for biomedical text.
In Advances ininformatics (pp.
382 ?
392).
Berlin, Germany: Springer.Velldal, E., ?vrelid, L., & Oepen, S. (2010).
Resolvingspeculation: MaxEnt cue classification and dependency-based scope rules.
In Proceedings of the 14th Conferenceon Natural Language Learning.
Uppsala, Sweden.Vincze, V., Szarvas, G., Farkas, R., M?ra, G., & Csirik, J.(2008).
The BioScope corpus: Annotation for negation,uncertainty and their scope in biomedical texts.
In Pro-ceedings of the BioNLP 2008 Workshop.
Columbus, OH,USA.Wilson, T., Wiebe, J., & Hwa, R. (2006).
Recognizing strongand weak opinion clauses.
Computational Intelligence,22(2), 73 ?
99.Zhang, Y., & Wang, R. (2009).
Cross-domain dependencyparsing using a deep linguistic grammar.
In Proceedingsof the 47th Meeting of the Association for ComputationalLinguistics.
Singapore.1387
