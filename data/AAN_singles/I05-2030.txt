Opinion Extraction Using a Learning-BasedAnaphora Resolution TechniqueNozomi Kobayashi Ryu Iida Kentaro Inui Yuji MatsumotoNara Institute of Science and TechnologyTakayama, Ikoma, Nara, 630-0192, Japan{nozomi-k,ryu-i,inui,matsu}@is.naist.jpAbstractThis paper addresses the task of extract-ing opinions from a given documentcollection.
Assuming that an opinioncan be represented as a tuple ?Subject,Attribute, Value?, we propose a compu-tational method to extract such tuplesfrom texts.
In this method, the maintask is decomposed into (a) the pro-cess of extracting Attribute-Value pairsfrom a given text and (b) the process ofjudging whether an extracted pair ex-presses an opinion of the author.
Weapply machine-learning techniques toboth subtasks.
We also report on theresults of our experiments and discussfuture directions.1 IntroductionThe explosive spread of communication on theWeb has attracted increasing interest in technolo-gies for automatically mining large numbers ofmessage boards and blog pages for opinions andrecommendations.Previous approaches to the task of mining alarge-scale document collection for opinions canbe classified into two groups: the document clas-sification approach and the information extrac-tion approach.
In the document classificationapproach, researchers have been exploring tech-niques for classifying documents according to se-mantic/sentiment orientation such as positive vs.negative (e.g.
(Dave et al, 2003; Pang and Lee,2004; Turney, 2002)).
The information extractionapproach, on the other hand, focuses on the taskof extracting elements which constitute opinions(e.g.
(Kanayama and Nasukawa, 2004; Hu andLiu, 2004; Tateishi et al, 2001)).The aim of this paper is to extract opinionsthat represent an evaluation of a products togetherwith the evidence.
To achieve this, we considerour task from the information extraction view-point.
We term the above task opinion extractionin this paper.While they can be linguistically realized inmany ways, opinions on a product are in fact oftenexpressed in the form of an attribute-value pair.An attribute represents one aspect of a subject andthe value is a specific language expression thatqualifies or quantifies the aspect.
Given this ob-servation, we approach our goal by reducing thetask to a general problem of extracting four-tuples?Product, Attribute, Value, Evaluation?
from alarge-scale text collection.
Technology for thisopinion extraction task would be useful for col-lecting and summarizing latent opinions from theWeb.
A straightforward application might be gen-eration of radar charts from collected opinions assuggested by Tateishi et al (2004).Consider an example from the automobile do-main, I am very satisfied with the powerful engine(of a car).
We can extract the four-tuple ?CAR, en-gine, powerful, satisfied?
from this sentence.
Notethat the distinction between Value and Evaluationis not easy.
Many expressions used to express aValue can also be used to express an Evaluation.For this reason, we do not distinguish value andevaluation, and therefore consider the task of ex-tracting triplets ?Product, Attribute, Value?.
An-other problem with opinion extraction is that wewant to get only subjective opinions.
Given thissetting, the opinion extraction task can be decom-posed into two subtasks: extraction of attribute-value pairs related to a product and determinationof its subjectivity.As we discuss in section 3, an attribute and itsvalue may not appear in a fixed expression andmay be separated.
In some cases, the attributemay be missing from a sentence.
In this respect,finding the attribute of a value is similar to findingthe missing antecedent of an anaphoric expres-sion.
In this paper, we discuss the similaritiesand differences between opinion extraction andanaphora resolution.
Then, we apply a machinelearning-based method used for anaphora reso-173lution to the opinion extraction problem and re-port on our experiments conducted on a domain-restricted set of Japanese texts excerpted from re-view pages on the Web.2 Related workIn this section, we discuss previous approachesto the opinion extraction problem.
In the pattern-based approach (Murano and Sato, 2003; Tateishiet al, 2001), pre-defined extraction patterns and alist of evaluative expressions are used.
These ex-traction patterns and the list of evaluation expres-sions need to be manually created.
However, asis the case in information extraction, manual con-struction of rules may require considerable cost toprovide sufficient coverage and accuracy.Hu and Liu (2004) attempt to extract the at-tributes of target products on which customershave expressed their opinions using associationmining, and to determine whether the opinionsare positive or negative.
Their aim is quite sim-ilar to our aim, however, our work differs fromtheirs in that they do not identify the value corre-sponding to an attribute.
Their aim is to extractthe attributes and their semantic orientations.Taking the semantic parsing-based approach,Kanayama and Nasukawa (2004) apply the ideaof transfer-based machine translation to the ex-traction of attribute-value pairs.
They regard theextraction task as translation from a text to a sen-timent unit which consists of a sentiment value,a predicate, and its arguments.
Their idea isto replace the translation patterns and bilinguallexicons with sentiment expression patterns anda lexicon that specifies the polarity of expres-sions.
Their method first analyzes the predicate-argument structure of a given input sentence mak-ing use of the sentence analysis component of anexisting machine translation engine, and then ex-tracts a sentiment unit from it, if any, using thetransfer component.One important problem the semantic parsingapproach encounters is that opinion expres-sions often appear with anaphoric expressionsand ellipses, which need to be resolved toaccomplish the opinion extraction task.
Ourinvestigation of an opinion-tagged Japanesecorpus (described below) showed that 30% ofthe attribute-value pairs we found did not have adirect syntactic dependency relation within thesentence, mostly due to ellipsis.
For example1,?dezain-wa?a hen-daga watashi-wa ?-ga ?suki-da?v?design?a weird I [it] ?like?v(The design is weird, but I like it.
)This type of case accounted for 46 out of 100pairs that did not have direct dependency rela-tions.
To analyze predicate argument structurerobustly, we have to solve this problem.
In thenext section, we discuss the similarity betweenthe anaphora resolution task and the opinionextraction task and propose to apply to opinionextraction a method used for anaphora resolution.3 Method for opinion extraction3.1 Analogy with anaphora resolutionWe consider the task of extracting opinion tu-ples ?Product, Attribute, Value?
from review sitesand message boards on the Web dedicated to pro-viding and exchanging information about retailgoods.
On these Web pages, products are oftenspecified clearly and so it is frequently a trivialjob to extract the information for the Product slot.We therefore in this paper focus on the problemof extracting ?Attribute, Value?
pairs.In the process of attribute-value pair identifi-cation for opinion extraction, we need to dealwith the following two cases: (a) both a valueand its corresponding attribute appear in the text,and (b) a value appears in the text while its at-tribute is missing since it is inferable form thevalue expression and the context.
The upper halfof Figure 1 illustrates these two cases in the auto-mobile domain.
In (b), the writer is talking aboutthe ?size?
of the car, but the expression ?size?
isnot explicitly mentioned in the text.
In addition,(b) includes the case where the writer evaluatesthe product itself.
For example, ?I?m very satis-fied with my car!?
: in this case, a value expres-sion ?satisfied?
evaluates the product as a whole,therefore a corresponding attribute does not ex-ists.For the case (a), we first identify a value ex-pression (like in Figure 1) in a given text and thenlook for the corresponding attribute in the text.Since we also see the case (b), on the other hand,we additionally need to consider the problem ofwhether the corresponding attribute of the identi-fied value expression appears in the text or not.The structure of these problems is analogous tothat of anaphora resolution; namely, there are ex-actly two cases in anaphora resolution that havea clear correspondence with the above two casesas illustrated in Figure 1: in (a) the noun phrase(NP) is anaphoric; namely, the NP?s antecedentappears in the text, and in (b) the noun phrase isnon-anaphoric.
A non-anaphoric NP is either ex-1?
?adenotes the word sequence corresponding to the At-tribute.
Likewise, we also use ?
?vfor the Value.174Taro-wa shisetsu-wo?
?-ga?shirabe-tehoukokusho-o sakusei-shita(a) (b)Dezain-wa     hen-desugawatashi-wa ??-ga?
suki-desu?????
(?-ga) Ookii-kedo atsukai-yasui( it )        large   but    easy to handle(a) (b)anaphora resolutionopinion extractionanaphorantecedentAttributeValue(The design is weird, but I like it.
)omitted Attribute(It is large, but easy to handle)Tar?-NOM  attendance-ACC                   notedreport-ACC             wrote(Taro noted the attendanceand wrote a report.
)design-NOM            weirdI-NOM          ( it )             likeValueOnaka-ga hetta-nodekaerouto (?-ga) omouhungrygo home       (I)exophoraanaphor(I think I?ll go home  because I?m hungry.
)Figure 1: Similarity between opinion extractionand anaphora resolutionophoric (i.e.
the NP has an implicit referent) or in-definite.
While the figure shows Japanese exam-ples, the similarity between anaphora resolutionand opinion extraction is language independent.This analogy naturally leads us to think of apply-ing existing techniques for anaphora resolution toour opinion extraction task since anaphora reso-lution has been studied for a considerably longerperiod in a wider range of disciplines as we brieflyreview below.3.2 Existing techniques for anaphoraresolutionCorpus-based empirical approaches to anaphoraresolution have been reasonably successful.
Thisapproach, as exemplified by (Soon et al, 2001;Iida et al, 2003; Ng, 2004), is cost effective,while achieving a better performance than thebest-performing rule-based systems for the testsets of MUC-6 and MUC-7 2.As suggested by Figure 1, anaphora resolutioncan be decomposed into two subtasks: anaphoric-ity determination and antecedent identification.Anaphoricity determination is the task of judg-ing whether a given NP is anaphoric or non-anaphoric.
Recent research advances have pro-vided several important findings as follows:?
Learning-based methods for antecedentidentification can also benefit from the use oflinguistic clues inspired by Centering The-ory (Grosz et al, 1995).?
One useful clue for anaphoricity determina-tion is the availability of a plausible candi-date for the antecedent.
If an appropriatecandidate for the antecedent is found in thepreceding discourse context, the NP is likelyto be anaphoric.For these reasons, an anaphora resolution modelperforms best if it carries out the following pro-2The 7th Message Understanding Conference (1998):www.itl.nist.gov/iaui/894.02/related projects/muc/?????????????
?interia ?????seki?
?Dezain-wa   hen-desuga   watashi-wa suki-desu ????
?interior                  seatdesign-NOM       weird    I-NOM           likecandidatesdesign likeinterior likeseat likedesign likecandidate attributesreal attributeSelect the bestcandidate attributeDecide whether thecandidate attributestands for the realattribute or notdesign likedesign likereal attributepairednessdeterminationattributeidentificationopinionhooddeterminationJudge whether the pairexpresses an opinion or notopinionAttributedictionaryValuedictionaryinteriorseatdesignlikegood?.target valueinitializationpair extractionFigure 2: Process of opinion extractioncess in the given order (Iida et al, 2005): (1)Antecedent identification: Given an NP, iden-tify the best candidate antecedent for it, and (2)Anaphoricity determination: Judge whether thecandidate really stands for the true antecedent ofthe NP.3.3 An opinion extraction model inspired byanalogy with anaphora resolutionAs illustrated in Figure 2, an opinion extractionmodel derived from the aforementioned analogywith anaphora resolution as follows:1.
Initialization: Identify attribute and valuecandidates by dictionary lookup2.
Attribute identification: Select a value andidentify the best candidate attribute corre-sponding to the value3.
Pairedness determination: Decide whetherthe candidate attribute stands for the real at-tribute of the value or not (i.e.
the valuehas no explicit corresponding attribute in thetext)4.
Opinionhood determination: Judge wheth-er the obtained attribute-value pair3 ex-presses an opinion or notHere, the attribute identification and pairednessdetermination processes respectively correspondto the antecedent identification and anaphoricitydetermination processes in anaphora resolution.Note that our opinion extraction task requiresan additional subtask, opinionhood determination?
an attribute-value pair appearing in a text doesnot necessarily constitute an opinion.
We elabo-rate on the notion of opinionhood in section 4.1.From the above discussion, we can expect thatthe findings for anaphora resolution mentioned in3.2 stated above apply to opinion extraction aswell.
In fact, the information about the candidate3For simplicity, we call a value both with and without anattribute uniformly by the term attribute-value pair unlessthe distinction is important.175attribute is likely to be useful for pairedness deter-mination.
We therefore expect that carrying outattribute identification before pairedness determi-nation should outperform the counterpart modelwhich executes the two subtasks in the reversedorder.
The same analogy also applies to opinion-hood determination; namely, we expect that opin-ion determination is bet performed after attributedetermination.
Furthermore, our opinion extrac-tion model also can be implemented in a totallymachine learning-based fashion.4 EvaluationWe conducted experiments with Japanese Webdocuments to empirically evaluate the perfor-mance of our opinion extraction model, focus-ing particularly on the validity of the analogy dis-cussed in the previous section.4.1 OpinionhoodIn these experiments, we define an opinion as fol-lows: An opinion is a description that expressesthe writer?s subjective evaluation of a particularsubject or a certain aspect of it.By this definition, we exclude requests, factualor counter-factual descriptions and hearsay evi-dence from our target opinions.
For example, Theengine is powerful is an opinion, while a counter-factual sentence such as If only the engine weremore powerful is not regarded as opinion.4.2 Opinion-tagged corpusWe created an opinion-tagged Japanese corpusconsisting of 288 review articles in the automo-bile domain (4,442 sentences).
While it is noteasy to judge whether an expression is a value oran attribute, we asked the annotator to identify at-tribute and value expressions according to theirsubjective judgment.If some attributes are in a hierarchical rela-tion with each other, we asked the annotator tochoose the attribute lowest in the hierarchy as theattribute of the value.
For example, in a soundsystem with poor sound, only sound is annotatedas the attribute of the value poor.The corpus contains 2,191 values with an at-tribute and 420 values without an attribute.
Mostof the attributes appear in the same sentence astheir corresponding values or in the immediatelypreceding sentence (99% of the total number ofpairs).
Therefore, we extract attributes and theircorresponding values from the same sentence orfrom the preceding sentence.4.3 Experimental methodAs preprocessing, we analyzed the opinion-tagged corpus using the Japanese morphologicalanalyzer ChaSen4 and the Japanese dependencystructure analyzer CaboCha 5.We used Support Vector Machines to train themodels for attribute identification, pairedness de-termination and opinionhood determination.
Weused the 2nd order polynomial kernel as the ker-nel function for SVMs.
Evaluation was per-formed by 10-fold cross validation using all thedata.4.3.1 DictionariesWe use dictionaries for identification of at-tribute and value candidates.
We constructed aattribute dictionary and a value dictionary fromreview articles about automobiles (230,000 sen-tences in total) using the semi-automatic methodproposed by Kobayashi et al (2004).
The dataused in this process was different from theopinion-tagged corpus.
Furthermore, we addedto the dictionaries expressions which frequentlyappearing in the opinion-tagged corpus.
The finalsize of the dictionaries was 3,777 attribute expres-sions and 3,950 value expressions.4.3.2 Order of model applicationTo examine the effects of appropriately choos-ing the order of model application we mentionedin the previous section, we conducted four ex-periments using different orders (AI indicates at-tribute identification, PD indicates pairedness de-termination and OD indicates opinion determina-tion):Proc.1: OD?PD?AI, Proc.2: OD?AI?PDProc.3: AI?OD?PD, Proc.4: AI?PD?ODNote that Proc.4 is our proposed ordering.In addition to these models, we adopted a base-line model.
In this model, if the candidate valueand a candidate attribute are connected via a de-pendency relation, the candidate value is judgedto have an attribute.
When none of the candidateattributes have a dependency relation, the candi-date value is judged not to have an attribute.We adopted the tournament model for attributeidentification (Iida et al, 2003).
This model im-plements a pairwise comparison (i.e.
a match)between two candidates in reference to the givenvalue treating it as a binary classification prob-lem, and conducting a tournament which consistsof a series of matches, in which the one that pre-vails through to the final round is declared the4http://chasen.naist.jp/5http://chasen.org/?taku/software/cabocha/176winner, namely, it is identified as the most likelycandidate attribute.
Each of the matches is con-ducted as a binary classification task in which oneor other of the candidate wins.The pairedness determination task and theopinionhood determination task are also binaryclassification tasks.
In Proc.1, since pair identifi-cation is conducted before finding the best candi-date attribute, we used Soon et al?s model (Soonet al, 2001) for pairedness determination.
Thismodel picks up each possible candidate attributefor a value and determines if it is the attribute forthat value.
If all the candidates are determined notto be the attribute, the value is judged not to havean attribute.
In Proc.4, we can use the informationabout whether the value has a corresponding at-tribute or not for opinionhood determination.
Wetherefore create two separate models for when thevalue does and does not have an attribute.4.3.3 FeaturesWe extracted the following two types of fea-tures from the candidate attribute and the candi-date value:(a) surface spelling and part-of-speech of thetarget value expression, as well as those of itsdependent phrase and those in its dependedphrase(s)(b) relation between the target value and can-didate attribute (distance between them, ex-istence of dependency, existence of a co-occurrence relation)We extracted (b) if the model could use both theattribute and the value information.
Existence of aco-occurrence relation is determined by referenceto a predefined co-occurrence list that containsattribute-value pair information such as ?heightof vehicle ?
low?.
We created the list from the230,000 sentences described in section 4.3.1 byapplying the attribute and value dictionary andextracting attribute-value pairs if there is a de-pendency relation between the attribute and thevalue.
The number of pairs we extracted wasabout 48,000.4.4 ResultsTable 1 shows the results of opinion extraction.We evaluated the results by recall R and preci-sion P defined as follows (For simplicity, we sub-stitute ?A-V?
for attribute-value pair):R =correctly extracted A-V opinionstotal number of A-V opinions,P =correctly extracted A-V opinionstotal number of A-V opinions found by the system.In order to demonstrate the effectiveness ofthe information about the candidate attribute, weevaluated the results of pair extraction and opin-ionhood determination separately.
Table 2 showsthe results.
In the pair extraction, we assume thatthe value is given, and evaluate how successfullyattribute-value pairs are extracted.4.5 DiscussionsAs Table 1 shows, our proposed ordering is out-performed on the recall in Proc.3, however, theprecision is higher than Proc.3 and get the best F-measure.
In what follows, we discuss the resultsof pair extraction and opinionhood determination.Pair extraction From Table 2, we can see thatcarrying out attribute identification before paired-ness determination outperforms the reverse order-ing by 11% better precision and 3% better recall.This result supports our expectation that knowl-edge of attribute information assists attribute-value pair extraction.
Focusing on the rows la-beled ?(dependency)?
and ?
(no dependency)?
inTable 2, while 80% of the attribute-value pairs ina direct dependency relation are successfully ex-tracted with high precision, the model achievesonly 51.7% recall with 61.7% precision for thecases where an attribute and value are not in a di-rect dependency relation.According to our error analysis, a major sourceof errors lies in the attribute identification task.
Inthis experiment, the precision of attribute identifi-cation is 78%.
A major reason for this problemwas that the true attributes did not exist in ourdictionary.
In addition, a major cause of error inthe pair determination stage is cases where an at-tribute appearing in the preceding sentence causesa false decision.
We need to conduct further in-vestigations in order to resolve these problems.Opinionhood determination Table 2 alsoshows that carrying out attribute identificationfollowed by opinionhood determination out-performs the reverse ordering, which supportsour expectation that knowing the attributeinformation aids opinionhood determination.While it produces better results, our proposedmethod still has room for improvement in bothprecision and recall.
Our current error analysishas not identified particular error patterns ?
thetypes of errors are very diverse.
However, weneed to at least address the issue of modifyingthe feature set to make the model more sensitiveto modality-oriented distinctions such as subjunc-tive and conditional expressions.177Table 1: The precision and the recall for opinion extractionprocedure value with attribute value without attribute attribute-value pairsbaseline precision 60.5% (1130/1869) 10.6% (249/2340) 32.8% (1379/4209)recall 51.6% (1130/2191) 59.3% (249/420) 52.8% (1379/2611)F-measure 55.7 21.0 40.5Proc.1 precision 47.3% (864/1828) 21.6% ( 86/399) 42.7% ( 950/2227)recall 39.4% (864/2191) 20.5% ( 86/420) 36.4% ( 950/2611)F-measure 43.0 21.0 39.3Proc.2 precision 63.0% (1074/1706) 38.0% (198/521) 57.1% (1272/2227)recall 49.0% (1074/2191) 47.1% (198/420) 48.7% (1272/2611)F-measure 55.1 42.0 52.6Proc.3 precision 74.9% (1277/1632) 29.1% (151/519) 63.8% (1373/2151)recall 55.8% (1222/2191) 36.0% (151/420) 52.6% (1373/2611)F-measure 64.0 32.2 57.7Proc.4 precision 80.5% (1175/1460) 30.2% (150/497) 67.7% (1325/1957)recall 53.6% (1175/2191) 35.7% (150/420) 50.7% (1325/2611)F-measure 64.4 32.7 58.0Table 2: The result of pair extraction and opinionhood determinationprocedure precision recallpair extractionbaseline (dependency) 71.1% (1385/1929) 63.2% (1385/2191)PD?AI 65.3% (1579/2419) 72.1% (1579/2191)AI?PD 76.6% (1645/2148) 75.1% (1645/2191)(dependency) 87.7% (1303/1486) 79.6% (1303/1637)(no dependency) 51.7% ( 342/ 662) 61.7% ( 342/ 554)opinionhood determination OD 74.0% (1554/2101) 60.2% (1554/2581)AI?OD 82.2% (1709/2078) 66.2% (1709/2581)5 ConclusionIn this paper, we have proposed a machinelearning-based method for the extraction of opin-ions on consumer products by reducing the prob-lem to that of extracting attribute-value pairs fromtexts.
We have pointed out the similarity betweenthe tasks of anaphora resolution and opinion ex-traction, and have applied the machine learning-based method designed for anaphora resolution toopinion extraction.
The experimental results re-ported in this paper show that identifying the cor-responding attribute for a given value expressionis effective in both pairedness determination andopinionhood determination.ReferencesK.
Dave, S. Lawrence, and D. M. Pennock.
2003.
Min-ing the peanut gallery: opinion extraction and semanticclassification of product reviews.
In Proc.
of the 12th In-ternational World Wide Web Conference, pages 519?528.B.
J. Grosz, A. K. Joshi, and S. Weinstein.
1995.
Center-ing: A framework for modeling the local coherence ofdiscourse.
Computational Linguistics, 21(2):203?226.M.
Hu and B. Liu.
2004.
Mining and summarizing customerreviews.
In Proc.
of the Tenth International Conferenceon Knowledge Discovery and Data Mining, pages 168?177.R.
Iida, K. Inui, H. Takamura, and Y. Matsumoto.
2003.
In-corporating contextual cues in trainable models for coref-erence resolution.
In Proc.
of the EACL Workshop on theComputational Treatment of Anaphora, pages 23?30.R.
Iida, K. Inui, Y. Matsumoto, and S. Sekine.
2005.
Nounphrase coreference resolution in Japanese base on mostlikely antecedant candidates.
Journal of Information Pro-cessing Society of Japan, 46(3).
(in Japanese).H.
Kanayama and T. Nasukawa.
2004.
Deeper sentimentanalysis using machine translation technology.
In Pro-ceedings of the 20th International Conference on Com-putational Linguistics, pages 494?500.N.
Kobayashi, K. Inui, Y. Matsumoto, K. Tateishi, andT.
Fukushima.
2004.
Collecting evaluative expressionsfor opinion extraction.
In Proc.
of the 1st InternationalJoint Conference on Natural Language Processing, pages584?589.S.
Murano and S. Sato.
2003.
Automatic extraction of sub-jective sentences using syntactic patterns.
In Proc.
of theNinth Annual Meeting of the Association for Natural Lan-guage Processing, pages 67?70.
(in Japanese).V.
Ng.
2004.
Learning noun phrase anaphoricity to improvecoreference resolution: Issues in representation and opti-mization.
In Proc.
of the 42nd Annual Meeting of the As-sociation for Computational Linguistics, pages 152?159.B.
Pang and L. Lee.
2004.
A sentiment education: Sen-timent analysis using subjectivity summarization basedon minimum cuts.
In Proc.
of the 42nd Annual Meetingof the Association for Computational Linguistics, pages271?278.W.
M. Soon, H. T. Ng, and D. C. Y. Lim.
2001.
A ma-chine learning approach to coreference resolution of nounphrases.
Computational Linguistics, 27(4):521?544.K.
Tateishi, Y. Ishiguro, and T. Fukushima.
2001.
Opinioninformation retrieval from the Internet.
In IPSJ SIGNLNote 144-11, pages 75?82.
(in Japanese).K.
Tateishi, T. Fukushima, N. Kobayashi, T. Takahashi,A.
Fujita, K. Inui, and Y. Matsumoto.
2004.
Web opin-ion extraction and summarization based on viewpointsof products.
In IPSJ SIGNL Note 163, pages 1?8.
(inJapanese).P.
D. Turney.
2002.
Thumbs up or thumbs down?
semanticorientation applied to unsupervised classification of re-views.
In Proc.
of the 40th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 417?424.178
