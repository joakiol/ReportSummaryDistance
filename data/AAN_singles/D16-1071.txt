Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 742?752,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsLAMB: A Good Shepherd of Morphologically Rich LanguagesSebastian Ebert and Thomas Mu?ller and Hinrich Schu?tzeCenter for Information and Language ProcessingLMU Munich, Germanyebert@cis.lmu.deAbstractThis paper introduces STEM and LAMB, em-beddings trained for stems and lemmata in-stead of for surface forms.
For morpholog-ically rich languages, they perform signif-icantly better than standard embeddings onword similarity and polarity evaluations.
Ona new WordNet-based evaluation, STEM andLAMB are up to 50% better than standard em-beddings.
We show that both embeddingshave high quality even for small dimension-ality and training corpora.1 IntroductionDespite their power and prevalence, embeddings,i.e., (low-dimensional) word representations in vec-tor space, have serious practical problems.
First,large text corpora are necessary to train high-qualityembeddings.
Such corpora are not available for un-derresourced languages.
Second, morphologicallyrich languages (MRLs) are a challenge for stan-dard embedding models because many inflectionalforms are rare or absent even in a large corpus.
Forexample, a Spanish verb has more than 50 forms,many of which are rarely used.
This leads to miss-ing or low quality embeddings for such inflectionalforms, even for otherwise frequent verbs, i.e., spar-sity is a problem.
Therefore, we propose to com-pute normalized embeddings instead of embeddingsfor surface/inflectional forms (referred to as formsthroughout the rest of the paper): STem EMbed-dings (STEM) for word stems and LemmA eMBed-dings (LAMB) for lemmata.Stemming is a heuristic approach to reducingform-related sparsity issues.
Based on simple rules,forms are converted into their stem.1 However, oftenthe forms of one word are converted into several dif-ferent stems.
For example, present indicative formsof the German verb ?brechen?
(to break) are mappedto four different stems (?brech?, ?brich?, ?bricht?,?brecht?).
A more principled solution is lemmatiza-tion.
Lemmatization unites many individual forms,many of which are rare, in one equivalence class,represented by a single lemma.
Stems and equiva-lence classes are more frequent than each individualform.
As we will show, this successfully addressesthe sparsity issue.Both methods can learn high-quality semanticrepresentations for rare forms and thus are most ben-eficial for MRLs as we show below.
Moreover, lesstraining data is required to train lemma embeddingsof the same quality as form embeddings.
Alterna-tively, we can train lemma embeddings that have thesame quality but fewer dimensions than form em-beddings, resulting in more efficient applications.If an application such as parsing requires in-flectional information, then stem and lemma em-beddings may not be a good choice since theydo not contain such information.
However, NLPapplications such as similarity benchmarks (e.g.,MEN (Bruni et al, 2014)) and (as we show below)polarity classification are semantic and are largely1In this paper, we use the term ?stem?
not in its linguisticmeaning, but to refer to the character string that is producedwhen a stemming algorithm like SNOWBALL is applied to aword form.
The stem is usually a prefix of the word form, butsome orthographic normalization (e.g., ?possibly?
?
?possi-ble?
or ?possibli?)
is often also performed.742independent of inflectional morphology.Our contributions are the following.
(i) We intro-duce the normalized embeddings STEM and LAMBand show their usefulness on different tasks for fivelanguages.
This paper is the first study that compre-hensively compares stem/lemma-based with form-based embeddings for MRLs.
(ii) We show the ad-vantage of normalization on word similarity bench-marks.
Normalized embeddings yield better perfor-mance for MRL languages on most datasets (6 outof 7 datasets for German and 2 out of 2 datasets forSpanish).
(iii) We propose a new intrinsic related-ness evaluation based on WordNet graphs and pub-lish datasets for five languages.
On this new evalua-tion, LAMB outperforms form-based baselines by abig margin.
(iv) STEM and LAMB outperform base-lines on polarity classification for Czech and En-glish.
(v) We show that LAMB embeddings are effi-cient in that they are high-quality for small trainingcorpora and small dimensionalities.2 Related WorkThere have been a large number of studies on En-glish, a morphologically simple language, that showthat the effect of normalization, in particular stem-ming, is different for different applications.
For in-stance, Karlgren and Sahlgren (2001) analyze theimpact of morphological analysis on creating wordrepresentations for synonymy detection.
They com-pare several stemming methods.
Bullinaria andLevy (2012) use stemming and lemmatization be-fore training word representations.
The improve-ment of morphological normalization in both stud-ies is moderate in the best case.
Melamud et al(2014) compute lemma embeddings to predict re-lated words given a query word.
They do not com-pare form and lemma representations.A finding about English morphology does notprovide insight into what happens with the morphol-ogy of an MRL.
In this paper we use English toprovide a data point for morphologically poor lan-guages.
Although we show that normalization forembeddings increases performance significantly onsome applications ?
a novel finding to the best ofour knowledge ?
morphologically simple languages(for which normalization is expected to be less im-portant) are not the main focus of the paper.
Instead,MRLs are the main focus.
For these, we show largeimprovements on several tasks.Recently, Ko?per et al (2015) compared form andlemma embeddings on English and German focus-ing on morpho-syntactic and semantic relation tasks.Generally, they found that lemmatization has lim-ited impact.
We extensively study MRLs and find astrong improvement on MRLs when using normal-ization, on intrinsic as well as extrinsic evaluations.Synonymy detection is a well studied problemin the NLP community (Turney, 2001; Turney etal., 2003; Baroni and Bisi, 2004; Ruiz-Casado etal., 2005; Grigonyte?
et al, 2010).
Rei and Briscoe(2014) classify hyponomy relationships through em-bedding similarity.
Our premise is that seman-tic similarity comprises all of these relations andmore.
Our ranking-based word relation evaluationaddresses this issue.
Similar to Melamud et al(2014), our motivation is that, in contrast to standardword similarity benchmarks, large resources can beautomatically generated for any language with aWordNet.
This is also exploited by Tsvetkov et al(2015).
Their intrinsic evaluation method requiresan annotated corpus, e.g., annotated with WordNetsupersenses.
Our approach requires only the Word-Net.An alternative strategy of dealing with data spar-sity is presented by Soricut and Och (2015).
Theycompute morphological features in an unsupervisedfashion in order to construct a form embedding bythe combination of the word?s morphemes.
We ad-dress scenarios (such as polarity classification) inwhich morphological information is less important,thus morpheme embeddings are not needed.3 Stem/Lemma CreationThe main hypothesis of this work is that normaliza-tion addresses sparsity issues, especially for MRLs,because although a particular word form might nothave been seen in the text, its stem or lemma is morelikely to be known.
For all stemming experimentswe use SNOWBALL,2 a widely used stemmer.
It nor-malizes a form based on deterministic rules, such asreplace the suffix ?tional?
by ?tion?
for English.For lemmatization we use the pipeline version ofthe freely available, high-quality lemmatizer LEM-2snowball.tartarus.org743MING (Mu?ller et al, 2015).
Since it is a language-independent token-based lemmatizer it is especiallysuited for our multi-lingual experiments.
Moreover,it reaches state-of-the-art performance for the fivelanguages that we study.
We train the pipeline us-ing the Penn Treebank (Marcus et al, 1993) for En-glish, SPMRL 2013 shared task data (Seddah et al,2013) for German and Hungarian, and CoNLL 2009(Hajic?
et al, 2009) datasets for Spanish and Czech.We additionally use a unigram list extracted fromWikipedia datasets and the ASPELL dictionary ofeach language.34 Experiments4.1 Word SimilarityOur first experiment evaluates how wellSTEM/LAMB embeddings predict human wordsimilarity judgments.
Given a pair of words (m,n)with a human-generated similarity value and a setof embeddings E we compute their similarity ascosine similarity.
For form embeddings EF , wedirectly use the embeddings of the word pairs?forms (EFm and EFn ) and compute their similarity.For STEM we use ESstem(w), where stem(w) is thestem of w. For LAMB we use ELlemma(w), wherelemma(w) is the lemma of w; we randomly selectone of w?s lemmata if there are several.
We conductexperiments on English (en), German (de) andSpanish (es).
Table 1 gives dataset statistics.For good performance, high-quality embeddingstrained on large corpora are required.
Hence,the training corpora for German and Spanish areweb corpora taken from COW14 (Scha?fer, 2015).Preprocessing includes removal of XML, conver-sion of HTML characters, lowercasing, stemmingusing SNOWBALL and lemmatization using LEM-MING.
We use the entire Spanish corpus (3.7 bil-lion tokens), but cut the German corpus to approxi-mately 8 billion tokens to be comparable to Ko?peret al (2015).
We train CBOW models (Mikolovet al, 2013) for forms, stems and lemmata us-ing WORD2VEC4 with the following settings: 400dimensions, symmetric context of size 2 (no dy-namic window), 1 training iteration, negative sam-pling with 15 samples, a learning rate of 0.025, min-3ftp://ftp.gnu.org/gnu/aspell/dict4code.google.com/p/word2vec/imum count of words of 50, and a sampling param-eter of 10?5.
CBOW is chosen, because it trainsmuch faster than skip-gram, which is beneficial onthese large corpora.Since the morphology of English is rather sim-ple we do not expect STEM and LAMB to reach oreven surpass highly optimized systems on any wordsimilarity dataset (e.g., Bruni et al (2014)).
There-fore, for practical reasons we use a smaller train-ing corpus, namely the preprocessed and tokenizedWikipedia dataset of Mu?ller and Schu?tze (2015).5Embeddings are trained with the same settings (us-ing 5 iterations instead of only 1, due to the smallersize of the corpus: 1.8 billion tokens).Table 1 shows results.
We also report the Spear-man correlation on the vocabulary intersection, i.e.,only those word pairs that are covered by the vocab-ularies of all models.Results.
Although English has a simple morphol-ogy, LAMB improves over form performance onMEN and SL.
A tie is achieved on RW.
These arethe three largest English datasets, giving a more re-liable result.
Both models perform comparably onWS.
Here, STEM is ahead by 1 point.
Forms arebetter on the small datasets MC and RG, where asingle word pair can have a large influence on theresult.
Additionally, these are datasets with high fre-quency forms, where form embeddings can be welltrained.
Because of the simple morphology of En-glish, STEM/LAMB do not outperform forms or onlyby a small margin and thus they cannot compete withhighly optimized state-of-the-art systems.6On German, both STEM and LAMB perform bet-ter on all datasets except WS.
We set the new state-of-the-art of 0.79 on Gur350 (compared to 0.77(Szarvas et al, 2011)) and 0.39 on ZG (comparedto 0.25 (Botha and Blunsom, 2014)); 0.83 on Gur65(compared to 0.79 (Ko?per et al, 2015)) is the bestperformance of a system that does not need addi-tional knowledge bases (cf.
Navigli and Ponzetto(2012), Szarvas et al (2011)).LAMB?s results on Spanish are equally good.
0.82on MC and 0.58 on WS are again the best per-5cistern.cis.lmu.de/marmot/naacl20156Baroni et al (2014)?s numbers are higher on some of thedatasets for the best of 48 different parameter configurations.In contrast, we do not tune parameters.744formances of a system not requiring an additionalknowledge base (cf.
Navigli and Ponzetto (2012)).The best performance before was 0.64 for MC and0.50 for WS (both Hassan and Mihalcea (2009)).STEM cannot improve over form embeddings, show-ing the difficulty of Spanish morphology.4.2 Word RelationsWord similarity benchmarks are not available formany languages and are expensive to create.
To rem-edy this situation, we create word similarity bench-marks that leverage WordNets, which are availablefor a great number of languages.Generally, a representation is deemed good ifwords related by a lexical relation in WordNet ?
syn-onymy, hyponymy etc.
?
have high cosine similar-ity with this representation.
Since the gold standardnecessary for measuring this property of a represen-tation can be automatically derived from a WordNet,we can create very large similarity benchmarks withup to 50k lemmata for the five languages we investi-gate: Czech, English, German, Hungarian and Span-ish.We view each WordNet as a graph whose edgesare the lexical relations encoded by the WordNet,e.g., synonymy, antonymy and hyponymy.
We thendefine L as the set of lemmata in a WordNet and thedistance d(l, l?)
between two lemmata l and l?
as thelength of the shortest path connecting them in thegraph.
The k-neighborhood Nk(l) of l is the set oflemmata l?
that have distance k or less, excluding l:Nk(l) := {l?|d(l, l?)
?
k, l 6= l?}.
The rank of l foran embedding set E is defined as:rankkE(l) := argminili ?
Nk(l) (1)where li is the lemma at position i in the list of alllemmata in the WordNet, ordered according to co-sine similarity to l in descending order.
We restricti ?
[1, 10] and set k = 2 for all experiments in thispaper.
We omit the indexes k and E when they areclear from context.To measure the quality of a set of embeddingswe compute the mean reciprocal rank (MRR) on therank results of all lemmata:MRRE = 1|L|?l?L1rankE(l) (2)We create large similarity datasets for five lan-guages: Czech (cz), English (en), German (de),Hungarian (hu) and Spanish (es) by extracting alllemmata from the WordNet version of the respec-tive language.
For English and Spanish we use thepreprocessed WordNets from the Open MultilingualWordNet (Bond and Paik, 2012).
We use the Czechand Hungarian WordNets (PALA and SMRZ, 2004;Miha?ltz et al, 2008) and GermaNet (Hamp andFeldweg, 1997) for German.
We keep all lemmatathat have a known form in the form embeddings andthat exist in the lemma embeddings.
Moreover, wefilter out all synsets that contain only one lemma anddiscard all multiword phrases.
The split into devel-opment and test sets is done in a way that the distri-bution of synset sizes (i.e., the number of lemmataper synset) is nearly equal in both sets.The number of lemmata in our evaluation sets canbe found in Table 2.
For more insight, we reportresults on all parts-of-speech (POS), as well as sep-arately for nouns (n), verbs (v) and adjectives (a).7The data is provided as supplementary material.8We propose the following models for the embed-ding evaluation.
For form embeddings we com-pare three different strategies, a realistic one, an op-timistic one and a lemma approximation strategy.In the realistic strategy (form real), given a querylemma we randomly sample a form, for which wethen compute the k-neighborhood.
If the neigh-bors contain multiple forms of the same equivalenceclass, we exclude the repetitions and use the nextneighbors instead.
For instance, if house is alreadya neighbor, then houses will be skipped.
The opti-mistic strategy (form opt) works similarly, but usesthe embedding of the most frequent surface form ofa lemma.
This is the most likely form to performbest in the form model.
This strategy presupposesthe availability of information about lemma and sur-face form counts.
As a baseline lemma approxi-mation strategy, we sum up all surface form em-beddings that belong to one equivalence class (formsum).
For STEM we repeat the same experiments asdescribed for forms, leading to stem real, stem optand stem sum.For embedding training, Wikipedia comes as a7The all-POS setting includes all POS, not just n, v, a.8All supplementary material is available at www.cis.uni-muenchen.de/ebert/745full vocabulary vocabulary intersectionlang dataset reference pairs form STEM LAMB cov.
form STEM LAMB cov.de Gur30 Gurevych (2005) 29 0.76 0.83 0.80 29, 29, 29 0.76 0.83 0.80 29Gur350 Gurevych (2005) 350 0.74 0.79 0.79 336, 340, 339 0.74 0.79 0.79 336Gur65 Gurevych (2005) 65 0.80 0.83 0.82 65, 65, 65 0.80 0.83 0.82 65MSL Leviant and Reichart (2015) 999 0.44 0.44 0.47 994, 995, 995 0.44 0.44 0.47 994MWS Leviant and Reichart (2015) 350 0.60 0.61 0.62 348, 350, 350 0.60 0.61 0.61 348WS Ko?per et al (2015) 280 0.72 0.72 0.71 279, 280, 280 0.72 0.71 0.71 279ZG Zesch and Gurevych (2006) 222 0.36 0.38 0.39 200, 207, 208 0.36 0.40 0.41 200en MC Miller and Charles (1991) 30 0.82 0.77 0.80 30, 30, 30 0.82 0.77 0.80 30MEN Bruni et al (2014) 1000 0.72 0.73 0.74 1000, 1000, 1000 0.72 0.73 0.74 1000RG Rubenstein et al (1965) 65 0.82 0.79 0.79 65, 65, 65 0.82 0.79 0.79 65RW Luong et al (2013) 2034 0.47 0.47 0.47 1613, 1947, 1819 0.47 0.47 0.48 1613SL Hill et al (2014) 999 0.42 0.38 0.43 998, 999, 999 0.42 0.38 0.43 998WS Finkelstein et al (2002) 353 0.63 0.64 0.63 353, 353, 353 0.63 0.64 0.63 353es MC Hassan and Mihalcea (2009) 30 0.70 0.69 0.82 30, 30, 30 0.70 0.69 0.82 30WS Hassan and Mihalcea (2009) 352 0.54 0.54 0.58 350, 352, 352 0.54 0.54 0.58 350Table 1: Word similarity results.
The left part shows dataset information.
The right part shows Spearman correlation (?)
for themodels with their full vocabulary and for the intersection of vocabularies.
Coverage is shown for all models in order of appearance.Bold is best per vocabulary and row.lang set al a n vcz dev 9694 852 6436 2315test 9763 869 6381 2433de dev 51682 6347 40674 5018test 51827 6491 40623 5085en dev 44448 9713 30825 5661test 44545 9665 30736 5793es dev 12384 1711 8634 1989test 12476 1727 8773 1971hu dev 19387 1953 15268 2057test 19486 1928 15436 2011Table 2: Number of lemmata in WordNet datasetsnatural choice as corpus, because it is available formany languages.
Therefore, we use the prepro-cessed and tokenized Wikipedia datasets of Mu?llerand Schu?tze (2015).
We train 50-dimensionalskip-gram embeddings (Mikolov et al, 2013) withWORD2VEC on the original, the stemmed and thelemmatized corpus, respectively.
Embeddings aretrained for all tokens, because we need high cover-age; the context size is set to 5, all remaining param-eters are left at their default value.99We train smaller embeddings than before, because we havemore models to train and training corpora are smaller.Results.
The MRR results in the left half of Ta-ble 3 (?unfiltered?)
show that for all languages andfor all POS, form real has the worst performanceamong the form models.
This comes at no surprisesince this model does barely know anything aboutword forms and lemmata.
The form opt model im-proves these results based on the additional infor-mation it has access to (the mapping from lemma toits most frequent form).
form sum performs simi-lar to form opt.
For Czech, Hungarian and Spanishit is slightly better (or equally good), whereas forEnglish and German there is no clear trend.
Thereis a large difference between these two models onGerman nouns, with form sum performing consider-ably worse.
We attribute this to the fact that manyGerman noun forms are rare compounds and there-fore lead to badly trained form embeddings, whichsummed up do not lead to high quality embeddingseither.Among the stemming models, stem real also is theworst performing model.
We can further see that forall languages and almost all POS, stem sum performsworse than stem opt.
That indicates that stemmingleads to many low-frequency stems or many wordssharing the same stem.
This is especially apparentin Spanish verbs.
There, the stemming models areclearly inferior to form models.Overall, LAMB performs best for all languagesand POS types.
Most improvements of LAMB are746significant.
The improvement over the best form-model reaches up to 6 points (e.g., Czech nouns).
Incontrast to form sum, LAMB improves over form opton German nouns.
This indicates that the sparsityissue is successfully addressed by LAMB.In general, morphological normalization in termsof stemming or lemmatization improves the resulton all languages, leading to an especially substantialimprovement on MRLs.
For the morphologicallyvery rich languages Czech and Hungarian, the rela-tive improvement of STEM or LAMB to form-basedmodels is especially high, e.g., Hungarian all: 50%.Moreover, we find that MRLs yield lower absoluteperformance.
This confirms the findings of Ko?per etal.
(2015).
Surprisingly, LAMB yields better perfor-mance on English despite its simple morphology.The low absolute results ?
especially for Hungar-ian ?
show that we address a challenging task andthat our new evaluation methodology is a good eval-uation for new types of word representations.For further insight, we restrict the nearest neigh-bor search space to those lemmata that have the samePOS as the query lemma.
The general findings inthe right half of Table 3 (?filtered?)
are similar to theunrestricted experiment: Normalization leads to su-perior results.
The form real and stem real modelsyield the lowest performance.
Form opt improvesthe performance and form sum is better on averagethan form opt.
Stem sum can rarely improve on stemopt.
The best stemming model most often is betterthan the best form model.
LAMB can benefit morefrom the POS type restriction than the form models.The distance to the best form model generally in-creases, especially on German adjectives and Span-ish verbs.
In all cases except on English adjectives,LAMB yields the best performance.
Again, in al-most all cases LAMB?s improvement over the form-models is significant.4.3 Polarity ClassificationOur first two evaluations were intrinsic.
We nowshow the benefit of normalization on an extrinsictask.
The task is classification of Czech moviereviews (CSFD, Habernal et al (2013)) into posi-tive, negative or neutral (Table 4).
We reimplementlingCNN (Ebert et al, 2015), a Convolutional Neu-ral Network that uses linguistic information to im-prove polarity classification.
This model reachesclose to state-of-the-art performance on data of theSemEval 2015 Task 10B (message level polarity).LingCNN takes several features as input: (i) embed-ding features, (ii) linguistic features at word leveland (iii) linguistic features at review level.We reuse the 50-dimensional Wikipedia embed-dings from Section 4.2 and compare three experi-mental conditions: using forms, STEM and LAMB.Linguistic word level features are: (i) SubLex1.0 sentiment lexicon (Veselovska?
and Bojar, 2013)(two binary indicators that word is marked posi-tive/negative); (ii) SentiStrength10 (three binary in-dicators that word is an emoticon marked as posi-tive/negative/neutral); (iii) prefix ?ne?
(binary nega-tion indicator in Czech).11All word level features are concatenated to forma single word representation of the review?s inputwords.
The concatenation of these representationsis the input to a convolution layer, which has sev-eral filters spanning the whole representation heightand several representations (i.e., several words) inwidth.
The output of the convolution layer is inputto a k-max pooling layer (Kalchbrenner et al, 2014).The max values are concatenated with the follow-ing linguistic review level features: (i) the count ofelongated words, such as ?cooool?
; (ii) three countfeatures for the number of positive/negative/neutralemoticons using the SentiStrength list; (iii) a countfeature for punctuation sequences, such as ?!!!?
; (iv)and a feature that counts the number of negatedwords.
(v) A final feature type comprises one countfeature each for the number of sentiment words in areview, the sum of sentiment values of these wordsas provided by the sentiment lexicon, the maximumsentiment value and the sentiment value of the lastword (Mohammad et al, 2013).
The concatenationof max values and review level features is then for-warded into a fully-connected three-class (positive,negative, neutral) softmax layer.
We train lingCNNwith AdaGrad (Duchi et al, 2011) and early stop-ping, batch size = 100, 200 filters per width of 3-6;k-max pooling with k = 5; learning rate 0.01; and`2 regularization (?
= 5 ?
10?5).We also perform this experiment for English on10sentistrength.wlv.ac.uk/11We disregard words with the prefix ?nej?, because they in-dicate superlatives.
Exceptions are common negated words withthis prefix, such as ?nejsi?
(engl.
?you are not?
).747unfiltered filteredform STEM form STEMlang POS real opt sum real opt sum LAMB real opt sum real opt sum LAMBcz a 0.03 0.04 0.05 0.02 0.05 0.05 0.06 0.03?
0.05?
0.07 0.04?
0.08 0.08 0.09n 0.15?
0.21?
0.24?
0.18?
0.27?
0.26?
0.30 0.17?
0.23?
0.26?
0.20?
0.29?
0.28?
0.32v 0.07?
0.13?
0.16?
0.08?
0.14?
0.16?
0.18 0.09?
0.15?
0.17?
0.09?
0.17?
0.18 0.20all 0.12?
0.18?
0.20?
0.14?
0.22?
0.21?
0.25 - - - - - - -de a 0.14?
0.22?
0.25?
0.17?
0.26 0.21?
0.27 0.17?
0.25?
0.27?
0.23?
0.33 0.33 0.33n 0.23?
0.35?
0.30?
0.28?
0.35?
0.33?
0.36 0.24?
0.36?
0.31?
0.28?
0.36 0.35?
0.37v 0.11?
0.19?
0.18?
0.11?
0.22 0.18?
0.23 0.13?
0.20?
0.21?
0.13?
0.24?
0.23?
0.26all 0.21?
0.32?
0.28?
0.24?
0.33?
0.30?
0.34 - - - - - - -en a 0.22?
0.25?
0.24?
0.16?
0.26?
0.25?
0.28 0.25?
0.28?
0.28?
0.18?
0.29?
0.32 0.31n 0.24?
0.27?
0.28?
0.22?
0.30 0.28?
0.30 0.25?
0.28?
0.29?
0.23?
0.31?
0.31?
0.32v 0.29?
0.35?
0.37 0.17?
0.35 0.24?
0.37 0.33?
0.39?
0.42?
0.21?
0.42?
0.39?
0.44all 0.23?
0.26?
0.27?
0.20?
0.28?
0.25?
0.29 - - - - - - -es a 0.20?
0.23?
0.23?
0.08?
0.21?
0.18?
0.27 0.21?
0.25?
0.26?
0.10?
0.26?
0.26?
0.30n 0.21?
0.25?
0.25?
0.16?
0.25?
0.23?
0.29 0.22?
0.26?
0.27?
0.17?
0.27?
0.26?
0.30v 0.19?
0.35?
0.36 0.11?
0.29?
0.19?
0.38 0.22?
0.36?
0.36?
0.16?
0.36?
0.33?
0.42all 0.20?
0.26?
0.26?
0.14?
0.24?
0.21?
0.30 - - - - - - -hu a 0.02?
0.06?
0.06?
0.05?
0.08 0.08 0.09 0.04?
0.08?
0.08?
0.06?
0.12 0.11 0.12n 0.01?
0.04?
0.05?
0.03?
0.07 0.06?
0.07 0.01?
0.04?
0.05?
0.04?
0.07?
0.06?
0.07v 0.04?
0.11?
0.13?
0.07?
0.14?
0.15 0.17 0.05?
0.13?
0.14?
0.07?
0.15?
0.16?
0.19all 0.02?
0.05?
0.06?
0.04?
0.08?
0.07?
0.09 - - - - - - -Table 3: Word relation results.
MRR per language and POS type for all models.
unfiltered is the unfiltered nearest neighbor searchspace; filtered is the nearest neighbor search space that contains only one POS.
?
(resp.
?
): significantly worse than LAMB (signtest, p < .01, resp.
p < .05).
Best unfiltered/filtered result per row is in bold.the SemEval 2015 Task 10B dataset (cf.
Table 4).We reimplement Ebert et al (2015)?s lexicon fea-tures.
They exploit the fact that there are many moresentiment lexicons available in English.
Other wordlevel features are the same as above.
Sentimentcount features at review level are computed sepa-rately for the entire tweet, for all hashtag words andfor each POS type (Ebert et al, 2015).Considering the much smaller dataset size andshorter sentences of the SemEval data we chosethe following hyperparameters: 100k most frequentword types, 100 filters per filter width of 2-5; andk-max pooling with k = 1.Results.
Table 5 lists the 10-fold cross-validationresults (accuracy and macro F1) on the CSFDdataset.
LAMB/STEM results are consistently betterthan form results.In our analysis, we found the following examplefor the benefit of normalization: ?popis a na?zev za-jmavy?
a film je takova?
filmar?ska?
prasa?rna?
(engl.
?description and title are interesting, but it is badfilm-making?).
This example is correctly classifiedas negative by the LAMB model because it has anembedding for ?prasa?rna?
(bad, smut) whereas theform model does not.The out-of-vocabulary counts for form and LAMBon the first fold of the CSFD experiment are 26.3kand 25.5k, respectively.
The similarity of these twonumbers suggests that the quality of word embed-dings (form vs. LAMB) are responsible for the per-formance gain.On the SemEval data, LAMB improves the resultsover form and stem (cf.
Table 5).12 Hence, LAMBcan still pick up additional information despite thesimple morphology of English.
This is probably dueto better embeddings for rare words.
The SemEval2015 winner (Hagen et al, 2015) is a highly domain-dependent and specialized system that we do notoutperform.In the introduction, we discussed that normaliza-tion removes inflectional information that is nec-essary for NLP tasks like parsing.
For polarityclassification, comparatives and superlatives can beimportant.
Further analysis is necessary to deter-12To be comparable with published results we report themacro F1 of positive and negative classes.748dataset total pos neg neuCSFD 91379 30896 29716 30767SemEval train 9845 3636 1535 4674SemEval dev 3813 1572 601 1640SemEval test 2390 1038 365 987Table 4: Polarity classification datasetslang features acc F1cz Brychcin et al (2013) - 81.53form 80.86 80.75STEM 81.51 81.39LAMB 81.21 81.09en Hagen et al (2015) - 64.84form 66.78 62.21STEM 66.95 62.06LAMB 67.49 63.01Table 5: Polarity classification results.
Bold is best per lan-guage and column.mine whether their normalization hurts in our exper-iments.
However, note that we evaluate on polarityonly, not on valence.5 AnalysisNormalized embeddings deal better with sparsitythan form embeddings.
In this section, we demon-strate two additional benefits of LAMB based on itsrobustness against sparsity.Embedding Size.
We now show that LAMB cantrain embeddings with fewer dimensions on thesame amount of data and still reach the same per-formance as larger form embeddings.
We repeat theword relation experiments of Section 4.2 (all POS)and train all models with embedding sizes 10, 20, 30and 40 for Spanish.
We choose Spanish because ithas richer morphology than English and more train-ing data than Czech and Hungarian.Figure 1 depicts the MRR results of all modelswith respect to embedding size.
The relative rank-ing of form models is real < opt < sum.
Thatcomes from the additional information the morecomplex models have access to.
All stemming mod-els reach lower performance than their form coun-terparts (similar to results in Table 3).
That suggeststhat stemming is not a proper alternative to correctlydealing with Spanish morphology.
LAMB reacheshigher performance than form real with already 20dimensions.
The 30 dimensional LAMB model isbetter than all other models.
Thus, we can createlower-dimensional lemma embeddings that are asgood as higher-dimensional form embeddings; thishas the benefits of reducing the number of parame-ters in models using these embeddings and of reduc-ing training times and memory consumption.Corpus Size.
Our second hypothesis is that lesstraining data is necessary to train good embeddings.We create 10 training corpora consisting of the firstk percent, k ?
{10, 20, .
.
.
, 100}, of the random-ized Spanish Wikipedia corpus.
With these 10 sub-corpora we repeat the word relation experiments ofSection 4.2 (all POS).
As query lemmata, we use thelemmata from before that exist in all subcorpora.Figure 2 shows that the relative ranking amongthe models is the same as before.
This time how-ever, form sum yields slightly better performancethan form opt, especially when little training data isavailable.
The stemming models again are inferiorto their form counterparts.
Only stem opt is ableto reach performance similar to form opt.
LAMBalways reaches higher performance than form real,even when only 10% of the training corpus is used.With 30% of the training corpus, LAMB surpassesthe performance of the other models.13 Again, byrequiring less than 30% of the training data, embed-ding training becomes much more efficient.
Further-more, in low-resource languages that lack the avail-ability of a large homogeneous corpus, LAMB canstill be trained successfully.6 ConclusionWe have presented STEM and LAMB, embeddingsbased on stems and lemmata.
In three experimentswe have shown the superiority compared to com-monly used form embeddings.
Especially (but notonly) on MRLs, where data sparsity is a problem,both normalized embeddings perform better thanform embeddings by a large margin.
In a new chal-lenging WordNet-based experiment we have shownfour methods of adding morphological information13Recall that form opt is similar to an approach that is used inmost systems that have embeddings, which just use the availablesurface forms.74910 20 30 40 50embeddings size0.000.050.100.150.200.250.30MRRLAMBform sumform optform realstem sum stem optstem realFigure 1: Embedding size analysis10 20 30 40 50 60 70 80 90 100corpus size0.000.050.100.150.200.250.300.35MRRLAMBform sumform optform realstem sum stem optstem realFigure 2: Corpus size analysis(opt, sum, STEM, LAMB).
Here, LAMB is the bestof the proposed ways of using morphological infor-mation, consistently reaching higher performance,often by a large margin.
STEM methods are notconsistently better, indicating that the more princi-pled way of normalization as done by LAMB is to bepreferred.
The datasets are available as supplemen-tary material at www.cis.uni-muenchen.de/ebert/.Our analysis shows that LAMB needs fewer em-bedding dimensions and less embedding trainingdata to reach the same performance as form embed-dings, making LAMB appealing for underresourcedlanguages.As morphological analyzers are becoming morewidely available, our method ?
which is easy toimplement, only requiring running the analyzer ?should become applicable to more and more lan-guages.Acknowledgments This work was supported byDFG (grant SCHU 2246/10).750ReferencesMarco Baroni and Sabrina Bisi.
2004.
Using cooccur-rence statistics and the web to discover synonyms in atechnical language.
In Proceedings of LREC.Marco Baroni, Georgiana Dinu, and Germa?n Kruszewski.2014.
Don?t count, predict!
A systematic compari-son of context-counting vs. context-predicting seman-tic vectors.
In Proceedings of ACL.Francis Bond and Kyonghee Paik.
2012.
A Survey ofWordnets and their Licenses.
In Proceedings of the6th Global WordNet Conference.Jan A. Botha and Phil Blunsom.
2014.
CompositionalMorphology for Word Representations and LanguageModelling.
In Proceedings of ICML.Elia Bruni, Nam-Khanh Tran, and Marco Baroni.
2014.Multimodal Distributional Semantics.
Journal of Arti-ficial Intelligence Research, 49.John A. Bullinaria and Joseph P. Levy.
2012.
Extract-ing semantic representations from word co-occurrencestatistics: stop-lists, stemming, and SVD.
BehaviorResearch Methods, 44(3):890?907.John C. Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive Subgradient Methods for Online Learningand Stochastic Optimization.
Journal of MachineLearning Research, 12.Sebastian Ebert, Ngoc Thang Vu, and Hinrich Schu?tze.2015.
A Linguistically Informed Convolutional Neu-ral Network.
In Proceedings of WASSA.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and EytanRuppin.
2002.
Placing search in context: the conceptrevisited.
ACM Trans.
Inf.
Syst., 20(1).Gintare?
Grigonyte?, Joao Cordeiro, Gae?l Dias, RumenMoraliyski, and Pavel Brazdil.
2010.
Paraphrasealignment for synonym evidence discovery.
In COL-ING.Iryna Gurevych.
2005.
Using the Structure of a Concep-tual Network in Computing Semantic Relatedness.
InProceedings of IJCNLP.Ivan Habernal, Toma?s?
Pta?c?ek, and Josef Steinberger.2013.
Sentiment Analysis in Czech Social Media Us-ing Supervised Machine Learning.
In Proceedings ofWASSA.Matthias Hagen, Martin Potthast, Michel Bu?chner, andBenno Stein.
2015.
Webis: An Ensemble for TwitterSentiment Detection.
In Proceedings of SemEval.Jan Hajic?, Massimiliano Ciaramita, Richard Johans-son, Daisuke Kawahara, Maria Anto`nia Mart?
?, Llu?
?sMa`rquez, Adam Meyers, Joakim Nivre, SebastianPado?, Jan S?te?pa?nek, et al 2009.
The CoNLL-2009shared task: Syntactic and semantic dependencies inmultiple languages.
In Proceedings of CoNLL.Birgit Hamp and Helmut Feldweg.
1997.
GermaNet -a Lexical-Semantic Net for German.
In In Proceed-ings of ACL workshop Automatic Information Extrac-tion and Building of Lexical Semantic Resources forNLP Applications.Samer Hassan and Rada Mihalcea.
2009.
Cross-lingualSemantic Relatedness Using Encyclopedic Knowl-edge.
In Proceedings of EMNLP.Felix Hill, Roi Reichart, and Anna Korhonen.
2014.SimLex-999: Evaluating Semantic Models with (Gen-uine) Similarity Estimation.
CoRR, abs/1408.3456.Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-som.
2014.
A Convolutional Neural Network forModelling Sentences.
In Proceedings of ACL.Jussi Karlgren and Magnus Sahlgren.
2001.
From Wordsto Understanding.
In Foundations of Real World Intel-ligence.
CSLI Publications.Maximilian Ko?per, Christian Scheible, and SabineSchulte im Walde.
2015.
Multilingual Reliability and?Semantic?
Structure of Continuous Word Spaces.
InProceedings of IWCS.Ira Leviant and Roi Reichart.
2015.
Judgment Lan-guage Matters: Multilingual Vector Space Models forJudgment Language Aware Lexical Semantics.
CoRR,abs/1508.00106.Minh-Thang Luong, Richard Socher, and Christopher D.Manning.
2013.
Better Word Representations withRecursive Neural Networks for Morphology.
In Pro-ceedings of CoNLL.Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beat-rice Santorini.
1993.
Building a large annotated cor-pus of English: The Penn treebank.
Computationallinguistics.Oren Melamud, Ido Dagan, Jacob Goldberger, IdanSzpektor, and Deniz Yuret.
2014.
Probabilistic Mod-eling of Joint-context in Distributional Similarity.
InProceedings of CoNLL.Ma?rton Miha?ltz, Csaba Hatvani, Judit Kuti, Gyo?rgySzarvas, Ja?nos Csirik, Ga?bor Pro?sze?ky, and Tama?sVa?radi.
2008.
Methods and Results of the Hungar-ian WordNet Project.
In Proceedings of the 4th GlobalWordNet Conference.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013.
Efficient estimation of word representa-tions in vector space.
In Proceedings of ICLR: Work-shop.George A. Miller and Walter G. Charles.
1991.
Contex-tual correlates of semantic similarity.
Language andCognitive Processes, 6(1).Saif M. Mohammad, Svetlana Kiritchenko, and XiaodanZhu.
2013.
NRC-Canada: Building the State-of-the-Art in Sentiment Analysis of Tweets.
In Proceedingsof SemEval.751Thomas Mu?ller and Hinrich Schu?tze.
2015.
Robust mor-phological tagging with word representations.
In Pro-ceedings of NAACL.Thomas Mu?ller, Ryan Cotterell, Alexander Fraser, andHinrich Schu?tze.
2015.
Joint lemmatization and mor-phological tagging with Lemming.
In Proceedings ofEMNLP.Roberto Navigli and Simone Paolo Ponzetto.
2012.
Ba-belRelate!
A Joint Multilingual Approach to Comput-ing Semantic Relatedness July 22-26, 2012, Toronto,Ontario, Canada.
In Proceedings of the Twenty-SixthAAAI Conference on Artificial Intelligence.Karel PALA and Pavel SMRZ.
2004.
Building CzechWordnet.
Romanian Journal of Information Scienceand Technology, 7(1-2).Marek Rei and Ted Briscoe.
2014.
Looking for Hy-ponyms in Vector Space Language Learning, CoNLL2014, Baltimore, Maryland, USA, June 26-27, 2014.In Proceedings of CoNLL.Maria Ruiz-Casado, Enrique Alfonseca, and PabloCastells.
2005.
Using context-window overlappingin synonym discovery and ontology extension.
In Pro-ceedings of RANLP.Roland Scha?fer.
2015.
Processing and querying largeweb corpora with the COW14 architecture.
In Pro-ceedings of CMLC.Djame?
Seddah, Reut Tsarfaty, Sandra Ku?bler, MarieCandito, Jinho Choi, Richa?rd Farkas, Jennifer Fos-ter, Iakes Goenaga, Koldo Gojenola, Yoav Goldberg,et al 2013.
Overview of the SPMRL 2013 sharedtask: Cross-Framework evaluation of parsing morpho-logically rich languages.
In Proceddings of SPMRL.Radu Soricut and Franz Josef Och.
2015.
UnsupervisedMorphology Induction Using Word Embeddings.
InProceedings of NAACL-HLT.Gyo?rgy Szarvas, Torsten Zesch, and Iryna Gurevych.2011.
Combining Heterogeneous Knowledge Re-sources for Improved Distributional Semantic Models.In Proceedings of CICLing.Yulia Tsvetkov, Manaal Faruqui, Wang Ling, GuillaumeLample, and Chris Dyer.
2015.
Evaluation of WordVector Representations by Subspace Alignment.
InProceedings of EMNLP.Peter D. Turney, Michael L. Littman, Jeffrey Bigham,and Victor Shnayder.
2003.
Combining independentmodules to solve multiple-choice synonym and anal-ogy problems.
ACM Transactions on Information Sys-tems.Peter D. Turney.
2001.
Mining the Web for Synonyms:PMI-IR versus LSA on TOEFL.
In Proceedings ofECML.Kater?ina Veselovska?
and Ondr?ej Bojar.
2013.
CzechSubLex 1.0.Torsten Zesch and Iryna Gurevych.
2006.
AutomaticallyCreating Datasets for Measures of Semantic Related-ness.
In Proceedings of the Workshop on LinguisticDistances.752
