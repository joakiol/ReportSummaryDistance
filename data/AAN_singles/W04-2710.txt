Annotating WordNetHelen Langone, Benjamin R. Haskell, George A. MillerCognitive Science LaboratoryPrinceton University{helen,ben,geo}@clarity.princeton.eduAbstractHigh-quality lexical resources are needed toboth train and evaluate Word Sense Disam-biguation (WSD) systems.
The problem of am-biguity persists even in limited domains, thusthe necessity for wide-coverage inventories ofsenses (dictionaries) and corpora sense-taggedto them.
WordNet has been used extensivelyfor WSD, for both its broad coverage and itslarge network of semantic relations.
In thispaper, we present a report on the state of ourcurrent endeavor to increase the connectivityof WordNet through sense-tagging the glosses,the result of which will be to create a more in-tegrated lexical resource.1 IntroductionHigh-quality lexical resources are needed to both trainand evaluate Word Sense Disambiguation (WSD) sys-tems.
The problem of ambiguity persists even in lim-ited domains, thus the necessity for wide-coverage inven-tories of senses (dictionaries) and corpora sense-taggedto them.
WordNet (Miller et al, 1990; Fellbaum, ed.,1998) has been used extensively for WSD, both for itsbroad coverage and its large network of semantic rela-tions.
Entries in WordNet have, until now, been organizedprimarily around the semantic relations of synonymy,antonymy, hyponymy/troponymy, meronymy, and a fewothers which hold mainly among lexicalized concepts andword forms of the same grammatical class.1 The nounand verb networks have been predominantly hierarchi-1The exceptions have been the links existing between ad-jectives derivationally related to nouns or verbs (conceptual isrelated to concept and conceptuality, irritated to irritate), andlinks between adverbs and the adjectives from which they de-rive (absolutely is related to WordNet sense 1 of absolute).cal, and the definitional glosses and illustrative sentenceshave not participated in the network of relations at all.This paper reports on a project currently underway tosense-tag the glosses.
Sense-tagging is the process oflinking an instance of a word to the WordNet synset rep-resenting its context-appropriate meaning.
Monosemouswords2 in the glosses can be tagged automatically, butin order to be truly reliable, the sense-tagging of polyse-mous words3 must be done manually.
This approach is insignificant contrast with the work done at The Universityof Texas at Dallas on Extended WordNet4, in which poly-semous words in the WordNet glosses were sense-taggedprimarily by automatic means.
The result of the projectdescribed here will be to increase connectivity and makepossible the association of words with related conceptsthat cut across grammatical class and hierarchy, provid-ing a more integrated lexical resource.2 Prior workPrevious efforts to sense-tag corpora manually havedemonstrated that the task is not trivial.
To begin with,the possibility of distinguishing word senses definitivelyin general is recognized as being problematic (Atkins andLevin, 1988; Kilgarriff, 1997; Hanks, 2000); indeed thenotion of a ?sense?
has itself been the subject of long de-bate (see the Hanks and Kilgarriff papers for two recentcontributions).
These are topics in need of serious con-sideration, but outside of the scope of this paper.
Certainissues emerge that are particularly relevant to designing2Monosemous relative to WordNet (see footnote below).3A word is polysemous if it has more than one relatedsense.
A word with more than one unrelated sense is calleda homonym.
?Bank?
is the classic example, its unrelated sensesbeing ?river bank?
and ?financial institution.?
WordNet does notmake a distinction between homonymy and polysemy, thereforea monosemous word in WordNet is one which has neither re-lated nor unrelated senses.4http://xwn.hlt.utdallas.edu/a manual sense-tagging system, and it is these we willconcentrate on here.The difficulties inherent in the sense-tagging task in-clude the order in which words are presented to tag, aword?s degree of polysemy and part of speech, vaguenessof the context, the order in which senses are presented,granularity of the senses, and level of expertise of the per-son doing the tagging.
Each will be addressed briefly inthe following sections.2.1 Targeted vs. sequential or ?lexical?
vs. ?textual?There are two approaches one can take to the order inwhich words are tagged.
In the sequential approach,termed ?textual?
by Kilgarriff (1998), the tagger proceedsthrough the text one word at a time, assigning the context-appropriate sense to each open class word as it is en-countered.
The targeted approach (?lexical?
in Kilgar-riff?s terms) involves tagging all corpus instances of apre-selected word, jumping through the text to each oc-currence.
The corpora produced by the SEMCOR andREADER projects were tagged sequentially; the Kilo andHECTOR projects used the targeted approach (Kilo andREADER are described in Miller et al (1998), HECTORin Atkins (1993), and SEMCOR in Fellbaum, ed.
(1998)).In sequential tagging, the tagger is following the narra-tive, and so has the meaning of the text in mind whenselecting senses for each new word encountered (con-text is foremost).
In targeted tagging, the tagger has thevarious meanings of the word in mind when presentedwith each new context (sense distinctions are foremost).In their comparisons of the two approaches, Fellbaum etal.
(2003) and Kilgarriff (1998) both conclude that se-quential tagging increases the difficulty of the task by re-quiring the tagger to acquaint (and then reacquaint) them-selves with the senses of a word each time they are con-fronted with it in the text.
The targeted approach, onthe other hand, enables the tagger to gain mastery of thesense-distinctions of a single word at a time, reducing theamount of effort required to tag each new instance.
Milleret al (1998) present a contrasting view.
In evaluating theKilo and READER tagging tasks, they find targeted tag-ging to be more tedious for the taggers than sequentialtagging, and no faster, as time is needed to assimilate newcontexts for each word occurrence.2.2 Polysemy, POS, and sense orderIn their 1997 paper, Fellbaum et al analyzed the resultsof the SEMCOR project, in which part of the Brown Cor-pus (Kuc?era and Francis, 1967) was tagged to WordNet1.4 senses.
Their analysis identified three factors thatinfluenced the difficulty level, and thus the accuracy, ofthe tagging task: degree of polysemy of the word beingtagged, the word?s part of speech, and the order in whichthe WordNet sense choices are displayed to the persondoing the tagging.The effect of a high degree of polysemy is to presentmore choices to the tagger, usually with finer distinctionsamong the senses, increasing the difficulty of selectingone out of several closely-related senses.The correspondence of a word?s part of speech withaccuracy of tagging stems from the nature of the objectsthat words of a certain class denote.
Words that refer toconcepts that are concrete tend to have relatively fixed,easily distinguishable senses.
Words with more abstractor generic referents tend to have a more flexible seman-tics, with meanings being assigned in context, and hencemore difficult to pin down.
Nouns tend to be in the for-mer category, verbs in the latter.
More abstract classesalso tend to have a higher degree of polysemy, adding tothe effect.Finally, the presentation of the sense choices in Word-Net order, with the most frequent sense first, creates abias towards selecting the first sense.
Their study showsthat randomly ordering the senses removes this effect.2.3 Granularity of sensesPalmer et al (to appear, 2004) examines the relationshipbetween manual and automatic tagging accuracy and thegranularity of the sense inventory.
Granularity has to dowith fineness of distinctions made from a lexicographer?spoint of view, and not the number of senses that a wordform exhibits in context.
It is related to polysemy, inthat the greater a word?s degree of polysemy, the finer thedistinctions that can be made in defining senses for thatword.
In their experiment, Palmer et al have lexicogra-phers group WordNet 1.75 senses according to syntacticand semantic criteria, which are used by taggers to tagcorpus instances.
An automatic WSD system trained onthe data tagged using grouped senses shows a 10% overallimprovement in performance against running it on datatagged without using the groupings.
Their study showsthat improvement came not from the fewer number ofsenses resulting from the groupings, but from the group-ings themselves, which increased the manual tags?
accu-racy (defined as agreement between taggers), thereby in-creasing the accuracy of the systems that learned fromthem.
This effect arises from the slippery nature of wordsenses and the impossibility of capturing them in neatlydelimited, universally agreed-upon sense-boxes.
New us-ages of words extending old meanings, vague contextsthat select for multiple senses, and the limits of the tag-ger?s own knowledge of a specialized domain, all defythe assignment of a single, unequivocal sense to a word?sinstance across annotators.
Palmer et al propose sensegroupings as a practical solution in these situations.5The words used for this experiment were the polysemousverbs from the lexical sample task for SENSEVAL-2 (Edmondsand Cotton, 2001).2.4 Tagger expertiseFinally, there is the question of whether novice tag-gers with adequate training can attain the level of accu-racy of experienced lexicographers and linguists.
Fell-baum et al (1997) answer this in the negative.
Theirfindings show novice tagger accuracy decreasing as thenumber of senses, or fineness of distinctions among thesenses, increases.
Level of expertise likely influenced theslow pace of tagging reported for the Kilo and READERprojects, which employed novice taggers.
During thetagging of the evaluation dataset for SENSEVAL-1, thehighly experienced lexicographers who did the taggingreported the time spent absorbing new contexts droppedoff rapidly after a slow start-up period (Krishnamurthyand Nicholls, 2000).2.5 The present approachTo the extent that these difficulties can be addressed, wehave attempted to do so.
We feel the most accurate re-sults can be obtained from the targeted approach usinglinguistically-trained taggers.
The nature of the glosses(relatively short, completely self-contained) means thata fairly restricted context will need to be assimilated foreach instance of a token, eliminating one factor of diffi-culty associated with the targeted approach.
Since a def-inition is, by definition, unambiguous, the context pro-vided by a gloss should, in theory, never be insufficient todisambiguate the words used within it.
In this respect, theglosses differ from KWIC (Key Word In Context) lines ina concordance, with which they can be compared.
KWICconcordances, so named because they display corpus in-stances of a (key) word along with surrounding text, areused by lexicographers in a manner very similar to thetargeted approach to sense-tagging.
There the task is todefine the word, to determine and delineate its sensesgiven its contexts of use.
One further difference to beexploited is the fact that, unlike a sentence in a typicalcorpus, a gloss is embedded within a network of WordNetrelations.
This means that immediate hypernym, domaincategory, and other relations can be made available to theuser as additional disambiguation aids.The order of the senses will be scrambled in the man-ual tagging interface so as to prevent a bias towards thefirst sense listed.
To avoid putting any additional burdenon the tagger, the order of senses will be fixed at the be-ginning of the session, and kept constant until the taggerexits the program or selects another word to tag.Underspecified word senses are expressed in WordNetin the form of verb groups6, which will be presented tothe tagger in the sense display with the option to selecteither the entire group, or individual senses within the6Groupings exist for many, though not all, polysemous verbsin WordNet.group.
Where no appropriate grouping exists, and con-text and domain category are not enough to fully disam-biguate, multiple tags can be assigned.
Precise guidelinesfor when multiple senses can be assigned, and under whatcriteria, will need to be developed, and taggers will needto be extensively trained on them.3 Annotating the glossesThere are six major components to the present sense-tagging system.
They function in pipeline fashion, withthe output of one being fed as input to the next.
Eachpass through the data produces output in valid XML, thestructure of which is covered in Subsection 3.1.
The sixcomponents are: (1) a gloss parser, (2) a tokenizer, (3) anignorable text chunker and classifier, (4) a WordNet col-location7 recognizer, (5) an automatic sense tagger, and(6) a manual tagging tool.Prior to and in conjunction with building the prepro-cessor (the first four components), analysis of the Word-Net glosses was undertaken to determine what should bepresented for tagging, and what was not to be tagged.Ignorable classes of word forms and multi-word formswere determined during this phase.
These were used asa basis for the development of a stop list of words andphrases to ignore completely, and a second, semi-stop listthat we have dubbed the ?wait list?.
The stop list is re-served for unequivocally closed-class words and phrasesincluding prepositions, conjunctions, determiners, pro-nouns and modals, plus multi-word forms that functionas prepositions (e.g., ?by means of?).
Words on the waitlist will be held out from the automatic tagging stage formanual review and tagging later.
Since WordNet coversonly open-class parts of speech, word forms that have ho-mographs in both open and closed-class parts of speechare on this list.
During the manual tagging stage, theopen-class senses will be tagged.
Highly polysemouswords such as ?be?
and ?have?
are also waitlisted.Many glosses also contain example sentences.
Whilenot an essential part of the semantic makeup of thesynset, they do give some information about the illus-trated word?s sense-specific context of use, contributingto meaning in a different way.8 For this reason, we willbe tagging the synset word (and only that word) of whichthe sentence is an exemplar.
By virtue of being locatedwithin the synset, the exemplified form is in effect auto-matically disambiguated?it?s just a matter of assigningthe tag.7In WordNet terminology, a collocation is a superordinateterm for a variety of multi-word forms, including, but not re-stricted to, names, compounds, phrasal verbs, and idiomaticphrases.8Insofar as meaning is defined in part by use.3.1 The glosstag DTDDevelopment of the formal model for the sense-taggedglosses took the DTD from the SEMCOR project as astarting point (Landes et al, 1998).
It went through sev-eral iterations of modification, first to accommodate thespecifics of the dataset being tagged (WordNet glossesas opposed to open text), and then to refine the han-dling of WordNet collocations.
Prior tagging efforts hademployed the WordNet method of representing colloca-tions as single word forms, with underscores replacingthe spaces between words.
While it is a practical solu-tion that gives the collocation the same status and repre-sentational form that it has as an entry in WordNet, bytreating a collocation as a ?word?, we lose the fact that itis decomposable into smaller units.
This renders difficultthe coding of discontinuous collocations (that is, colloca-tions interrupted by one or more intervening words, forexample ?His performance blew the competition out ofthe water?, where ?blow out of the water?
is a WordNetcollocation).
A scheme that enables collocations to betreated both as individual words and as multi-word unitsis therefore desirable, particularly if future parsing passesneed to identify the internal structure of a collocation, asfor distinguishing phrase heads from non-heads.The smallest structural unit, then, is a word (or piece ofpunctuation), marked as <cf> if it is part of a WordNetcollocation, and as <wf> otherwise.
Attributes on the<wf> and <cf> elements identify each form uniquelyin the gloss, and link together the constituent <cf>?s of acollocation.The major structural units of a gloss are <def>, <ex>,and <aux>.
<def> contains the definitional portion ofthe gloss, the main interest of the tagging task.
A <def>may be followed by one or more <ex>?s, each contain-ing an example sentence.
Auxiliary information,9 codedas <aux>, may precede or follow the <def>, or occurwithin it.
Figure 1 shows the marked up gloss for sense11 of life (the gloss text is ?living things collectively;?
),as it looks after preprocessing.Prior to sense-tagging, the lemma attribute on the<wf> or head10 <cf> of a collocation is set to allpossible lemma forms, as determined during lemmatiza-tion (explicated more fully below).
After sense-tagging,the lemma attribute is set to only the lemma of theword/collocation that it is tagged to, all other options aredeleted.
An <id> element representing the sense tagis inserted as a child of the <wf> (or <cf>), if multi-9Auxiliary text is a cover term for a range of numeric andsymbolic classes (dates, times, numbers, numeric ranges, mea-surements, formulas, and the like), and parenthesized and othersecondary text that are inessential to the meaning of a synset.10?Head?
here refers simply to the first word in the collo-cation, and not the syntactic head.
The head <cf> bears thelemma and sense-tag(s) for the entire collocation.ple sense-tags are assigned, then multiple <id>?s are as-signed, one for each tag.
Figure 2 shows the sense-taggedgloss for life.3.2 Preprocessing and automatic taggingThe preprocessing stage segments the gloss into chunksand tokenizes the gloss contents into words and WordNetcollocations.
The tokenization pass isolates word formsand disambiguates lexical from non-lexical punctuation.Lexical punctuation is retained as part of the word form,non-lexical punctuation is encoded as ignorable <wf>?s.Abbreviations and acronyms are recognized, contractionssplit, and stop list and wait list forms are handled.
All<wf>?s other than punctuation are lemmatized, that is,they are reduced to their WordNet entry form using an in-house tool, moan11, that was developed for this purpose.Part of speech is not disambiguated during preprocess-ing, therefore lemmatizing assigns all potential lemmaforms for all part of speech classes that moan returns forthe token.
Part of speech disambiguation will occur as aside-effect of sense-tagging, avoiding the introduction oferrors related to POS-tagging.
Lemmatizing serves twofunctions, first when searching the database of glossesfor the term being tagged, and then when displaying thesense choices for a particular instance.The targeted tagging approach introduces the problemof locating all inflected forms of the word/collocation tobe tagged.
Rather than build a tool to generate inflectedforms, our solution was to pre-lemmatize the corpus andsearch on the lemma forms, on the assumption that whilethe search will overgenerate matches, it will not miss any.Locating alternations in hyphenation will be handled in asimilar way, via the pre-indexing of alternate forms ofhyphenated words/collocations in WordNet.The ignorable text classifier recognizes ignorable textas described earlier, chunking multi-word terms and as-signing attributes indicating semantic class.
The markupwill enable them to be treated as individual words or, al-ternatively, as a single form indicating the class, whichwill be of use should further parsing or semantic process-ing of the glosses be called for.The WordNet collocation recognizer, or globber, uses abag-of-words approach to locate multi-word forms in theglosses.
First, all possible collocations are pulled fromthe WordNet database.
This list is then filtered by sev-eral criteria designed to exclude candidates that cannotbe accurately identified automatically.
The largest classof excluded words is that of phrasal verbs, which cannot11Moan falls somewhere between a stemmer and full mor-phological analyzer?it recognizes inflectional endings and re-stores a corpus instance to its possible lemma form(s) classifiedby part of speech and grammatical category of the inflectionalsuffix.
Lemma form is the WordNet entry spelling, if the wordis in WordNet.<synset pos="n" ofs="00005905"><gloss desc="wsd"><def><wf wf-num="1" tag="un" lemma="living%1|live%2|living%3">living</wf><wf wf-num="2" tag="un" lemma="thing%1|things%1">things</wf><wf wf-num="3" tag="un" lemma="collectively%4" sep="">collectively</wf><wf wf-num="4" type="punc" tag="ignore">;</wf></def></gloss></synset>Figure 1: Preprocessed gloss for life, prior to semantic annotation<synset pos="n" ofs="00005905"><gloss desc="wsd"><def><cf coll="a" tag="auto"><glob coll="a" tag="auto"><id coll="a" lemma="living_thing" pos="n" ofs="00004323"/></glob>living</cf><cf coll="a" tag="cf">things</cf><wf tag="auto" sep=""><id lemma="collectively" pos="r" ofs="00119700"/>collectively</wf><wf tag="ignore" type="punc">;</wf></def></gloss></synset>Figure 2: Semantically-annotated gloss for lifeeasily be distinguished from verbs followed by preposi-tions heading prepositional phrases.12 Many of these willbe globbed by hand in the early stages of manual tagging.From this list of excluded words, we also generate a listof collocations that contain monosemous words.
This listwill later be used to prevent those words from being erro-neously tagged in the automatic sense-tagging stage.
Thefinal list of words to be automatically globbed also takesinto account variations in hyphenation and capitalization.Once the list is completed, the next step is to create anindex of the glosses referenced by the lemmatized formsthey contain.
For each collocation, the globber calculatesthe intersection of the lists of glosses containing its con-stituent word forms.
This list of possible collocations isthen ordered by gloss.The final step of the globber iterates through each ofthe glosses, three passes per gloss.
The first pass marksthe monosemous words found in excluded collocations,without globbing the collocation.
Pass two identifiesmulti-word forms that appear as consecutive <wf>?s inthe text.
The final pass attempts to locate disjoint collo-cations that follow certain set patterns of usage, such as?ribbon, bull, and garter snakes?, where ?ribbon snake?,?bull snake?, and ?garter snake?
are all in WordNet.
?Garter snake?
is globbed in pass two, and parallel struc-ture helps identify ?ribbon snake?
and ?bull snake?
in thethird pass.After preprocessing is complete, the automatic sensetagger tags monosemous <wf>?s and <cf>?s to theirWordNet senses.
Words and collocations tagged by theautomatic tagger are distinguished from manually taggedterms by an attribute in the markup.Sense-tagging the glosses to WordNet senses presup-poses that all words used in the glosses (and all sensesused of those words) exist as entries.
The preprocess-ing and auto-tagging phase will therefore include a fewdry runs to identify any typographical errors and wordsnot covered, errors will be fixed and open class words orword senses will be added to WordNet as necessary.3.3 Manual tagger interfaceThe single most important design consideration for themanual tagger interface is the repetitiveness inherent tothe task.
With approximately 550,000 polysemous open-class words and collocations13 in the glosses, each tag-ger will tag hundreds of words in a day of work.
Wehave made every effort to minimize the amount of mousemovement and the number of button presses required totag each word.The layout of the program window is simple.
The cur-rent search term is displayed in an entry box near the top12?Last year legal fees ate up our profits.?
versus ?Last nightwe ate up the street.
?13With an average polysemy of 2.59 senses per word form.of the screen.
Below this box are two text boxes, forglosses and examples, respectively.
Buttons used to alterthe current tag or tags lie above the final text box, whichis used to display and select the WordNet sense or sensesfor the current word.The tag status of each word or collocation in the glossand example boxes is indicated through the use of color,font, and highlighting.
Orange text indicates a term thathas been automatically tagged, red type denotes a manu-ally tagged word, words marked as ignorable are shownin black, and the remainder of the taggable text is blue.Words that are part of a collocation are underlined, andforms that match the targeted search term are bolded.
Thecurrent selection is highlighted in yellow.There are several ways to navigate the glosses.
For tar-geted tagging, the user chooses one or more senses, thenclicks the ?Tag?
button, assigning those senses and auto-matically jumping to the next untagged instance of thesearch word.
Other buttons allow movement to the nextor previous instance of the search term without alteringthe tag status of the current selection.
The interface wasdesigned with targeted tagging in mind, but the user canswitch between targeted and sequential modes, to fix animproperly tagged word.The interface allows a user to filter the displayed sensesby part of speech, to concentrate on the relevant options.When context is insufficient to fully disambiguate, a wordor collocation can be tagged to more than one sense or toa WordNet verb group.
To prevent bias caused by theorder of the displayed senses, each time a new targetedsearch term is entered, the senses shown in the sense boxare shuffled after being grouped by part of speech.During the targeted tagging process, the interface alsoenables a user to easily inspect and change the sense tagsassigned to words other than the search term.
The inter-face will display a box containing a tagged word?s senseswhen the cursor is placed over it, providing useful infor-mation for disambiguating the search term.
Additionally,if the user notices a tagging error, the mis-tagged wordcan be selected for editing.
Errors and omissions of glob-bing can be corrected in a similar fashion.
To ?un-glob?a collocation, one need only click on the collocation andclick on the ?un-glob?
button.
To group separate <wf>sinto a new collocation, a user can select each constituentform with the mouse and click the ?glob?
button.
The in-terface will then provide a list of potential lemmas for thecollocation, from which the user can select the appropri-ate choice.4 Looking aheadWordNet currently consists of 146,000 lexemes orga-nized into more than 116,000 synsets.
There are over117,000 bidirectional links comprising the hyponymy,troponymy, and meronymy hierarchies, and 4,000 bidi-rectional antonym links.
Once completed, the sense-tagged glosses will contribute an estimated 800,000 linksto the network, increasing the internal connectivity andassociating words with related concepts that cut acrossgrammatical class and hierarchy.
From the synset for aword, the synsets for all conceptually-related words usedin its gloss can be accessed via their sense tags.
Fromthose synsets, hierarchical and other semantic links canbe followed, as well the sense tags in those glosses, sit-uating the word in an ever-expanding network of links.The sense-tagged glosses will provide an additional di-mension of meaning not expressible by purely hierarchi-cal relations.
The hierarchical structure of WordNet rep-resents sense distinctions that stem from a polysemousword?s distinct superordinates (paradigmatic difference).Not represented are a word?s syntagmatic properties?the ways in which meaning is constrained by the differ-ing contexts in which a polysemous word appears.
Thesense-tagged glosses, taken as a corpus of disambiguatedcontexts for a word, provide just that.NLP needs high-quality sense-tagged corpora andsense inventories.
The project currently underway is astep towards providing both in one integrated lexical re-source.Acknowledgements: This work has been supportedby contracts between Princeton University and the Ad-vanced Research and Development Activity (ARDAContract No.
SO53100 000 and the ACQUAINT R&DProgram Contract No.
MDA904-01-C-0986), as wellas DARPA Subcontract No.
621-03-S-0115 under U.S.Government Contract No.
N00174-02-0-0002.
It wouldalso not have been possible without the input of RandeeTengi and the work of the on-staff linguists, ChristianeFellbaum and Susanne Wolff, and tagger Suzie Berger.Many thanks also to Jin Oh for building the initial ver-sion of the manual tagger tool.ReferencesSue Atkins.
1993.
Tools for computer-aided lexicog-raphy: the Hector project.
Papers in ComputationalLexicography: COMPLEX ?93.
Budapest.Beryl Atkins and Beth Levin.
1988.
Admitting imped-iments.
Proceedings of the 4th Annual Conference ofthe UW Center for the New OED.
Oxford, UK.Philip Edmonds and Scott Cotton.
2001.
SENSEVAL-2:Overview.
Proceedings of SENSEVAL-2: Second In-ternational Workshop on Evaluating Word Sense Dis-ambiguation Systems.
Toulouse, France.Christiane Fellbaum, ed.
1998.
WordNet: An ElectronicLexical Database.
MIT Press, Cambridge, MA.Christiane Fellbaum, Lauren Delfs, Susanne Wolff, andMartha Palmer.
2003.
Word meaning in dictionaries,corpora, and the speaker?s mind.
In Meaningful texts:The Extraction of Semantic Information from Monolin-gual and Multilingual Corpora, eds.
Geoff Barnbrook,Pernilla Danielsson and Michaela Mahlberg.
Birming-ham University Press, Birmingham, UK.Christiane Fellbaum, Joachim Grabowski, and Shari Lan-des.
1997.
Analysis of a hand-tagging task.
Proceed-ings of the ACL/Siglex workshop.
Somerset, NJ.Patrick Hanks.
2000.
Do word meanings exist?
Com-puters and the Humanities, 34(1-2):205-215.
SpecialIssue on SENSEVAL.Adam Kilgarriff.
1997.
I don?t believe in word senses.Computers and the Humanities, 31(2):91-113.Adam Kilgarriff.
1998.
Gold standard datasets for eval-uating Word Sense Disambiguation programs.
Com-puter Speech and Language, 12(3):453-472.Ramesh Krishnamurthy and Diane Nicholls.
2000.
Peel-ing an onion: the lexicographer?s experience of manualsense-tagging.
Computers and the Humanities, 34(1-2):85-97.
Special Issue on SENSEVAL.Henry Kuc?era and W. Nelson Francis.
1967.
The stan-dard corpus of present-day American English.
(Elec-tronic database.)
Brown University, Providence, RI.Shari Landes, Claudia Leacock, and Randee I. Tengi.1998.
Building semantic concordances.
In WordNet:An Electronic Lexical Database, ed.
Christiane Fell-baum.
MIT Press, Cambridge, MA.George A. Miller, Richard Beckwith, Christiane Fell-baum, Derek Gross, and Katherine J. Miller.
1990.Introduction to WordNet: an on-line lexical database.International Journal of Lexicography, 3(4):235-244.George A. Miller, Randee Tengi, and Shari Landes.1998.
Matching the tagging to the task.
In WordNet:An Electronic Lexical Database, ed.
Christiane Fell-baum.
MIT Press, Cambridge, MA.Martha Palmer, Hoa Trang Dang, and Christiane Fell-baum.
To appear, 2004.
Making fine-grained andcoarse-grained sense distinctions, both manually andautomatically.
Natural Language Engineering.
