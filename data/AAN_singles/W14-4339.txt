Proceedings of the SIGDIAL 2014 Conference, pages 282?291,Philadelphia, U.S.A., 18-20 June 2014.c?2014 Association for Computational LinguisticsWeb-style ranking and SLU combination for dialog state trackingJason D. WilliamsMicrosoft Research, Redmond, WA, USAjason.williams@microsoft.comAbstractIn spoken dialog systems, statistical statetracking aims to improve robustness tospeech recognition errors by tracking aposterior distribution over hidden dialogstates.
This paper introduces two novelmethods for this task.
First, we explainhow state tracking is structurally simi-lar to web-style ranking, enabling ma-ture, powerful ranking algorithms to be ap-plied.
Second, we show how to use mul-tiple spoken language understanding en-gines (SLUs) in state tracking ?
multipleSLUs can expand the set of dialog statesbeing tracked, and give more informationabout each, thereby increasing both recalland precision of state tracking.
We eval-uate on the second Dialog State TrackingChallenge; together these two techniquesyield highest accuracy in 2 of 3 tasks, in-cluding the most difficult and general task.1 IntroductionSpoken dialog systems interact with users via nat-ural language to help them achieve a goal.
As theinteraction progresses, the dialog manager main-tains a representation of the state of the dialog ina process called dialog state tracking (Williamset al., 2013; Henderson et al., 2014).
For exam-ple, in a restaurant search application, the dialogstate might indicate that the user is looking for aninexpensive restaurant in the center of town.
Di-alog state tracking is difficult because errors inautomatic speech recognition (ASR) and spokenlanguage understanding (SLU) are common, andcan cause the system to misunderstand the user?sneeds.
At the same time, state tracking is crucialbecause the system relies on the estimated dia-log state to choose actions ?
for example, whichrestaurants to present to the user.Historically, commercial systems have usedhand-crafted rules for state tracking, selecting theSLU result with the highest confidence score ob-served so far, and discarding alternatives.
In con-trast, statistical approaches compute a posteriordistribution over many hypotheses for the dialogstate, and in general these have been shown to besuperior (Horvitz and Paek, 1999; Williams andYoung, 2007; Young et al., 2009; Thomson andYoung, 2010; Bohus and Rudnicky, 2006; Met-allinou et al., 2013; Williams et al., 2013).This paper makes two contributions to the taskof statistical dialog state tracking.
First, we showhow to cast dialog state tracking as web-style rank-ing.
Each dialog state can be viewed as a doc-ument, and each dialog turn can be viewed as asearch instance.
The benefit of this construction isthat it enables a rich literature of powerful rank-ing algorithms to be applied.
For example, theranker we apply constructs a forest of decisiontrees, which ?
unlike existing work ?
automat-ically encodes conjunctions of low-level features.Conjunctions are attractive in dialog state trackingwhere relationships exist between low-level con-cepts like grounding and confidence score.The second contribution is to incorporate theoutput of multiple spoken language understandingengines (SLUs) into dialog state tracking.
Usingmore than one SLU can increase the number of di-alog states being tracked, improving the chancesof discovering the correct one.
Moreover, addi-tional SLUs supply more features, such as seman-tic confidence scores, improving accuracy.This paper is organized as follows.
First, sec-tion 2 states the problem formally and covers re-lated work.
Section 3 then lays out the data, fea-tures, and experimental design.
Section 4 appliesweb-style ranking, and section 5 covers the usageof multiple SLUs.
Section 6 extends the types oftracking tasks, section 7 compares performance toother entries in DSTC2, and section 8 briefly con-282cludes.2 BackgroundStatistical dialog state tracking can be formalizedas follows.
At each turn in the dialog, the statetracker maintains a set X of dialog state hypothe-ses X = {x1, x2, .
.
.
, xN}.
Each state hypothesiscorresponds to a possible true state of the dialog.The posterior of a state xiat a certain turn in thedialog is denoted P (xi).Based on this posterior, the system takes an ac-tion a, the user provides an utterance in reply,and an automatic speech recognizer (ASR) con-verts the user?s utterance into words.
Since speechrecognition is an error prone process, the speechrecognizer outputs weighted alternatives, for ex-ample an N-best list or a word-confusion network.A spoken language understanding engine (SLU)then converts the ASR output into a meaning rep-resentation U for the user?s utterance, where Ucan contain alternatives for the user?s meaning,U = {u1, .
.
.
, uL}.The state tracker then updates its internal state.This is done in three stages.
First, a hand-writtenfunction G ingests the system?s last action s, themeaning representation U , and the current set ofstates X , and yields a new set of possible states,X?= G(s, U,X), where we denote the membersof X?as {x?1, x?2, .
.
.
, x?N?}.
The number of ele-ments in X?may be different than X , and typi-cally the number of states increases as the dialogprogresses, i.e.
N?> N .
In this work, G simplytakes the Cartesian product of X and U .
Second,for each new state hypothesis x?i, a vector of J fea-tures is extracted, ?
(x?i) = [?1(x?i), .
.
.
, ?J(x?i)].In the third stage, a scoring process takes all of thefeatures for all of the new dialog states and scoresthem to produce the new distribution over dialogstates, P?(x?i).
This new distribution is used tochoose another system action, and the whole pro-cess repeats.Most early work cast dialog state tracking as agenerative model in which hidden user goals gen-erate observations in the form of SLU hypothe-ses (Horvitz and Paek, 1999; Williams and Young,2007; Young et al., 2009; Thomson and Young,2010).
More recently, discriminatively trained di-rect models have been applied, and two studieson dialog data from two publicly deployed dialogsystems suggest direct models yield better perfor-mance (Williams, 2012; Zilka et al., 2013).
Themethods introduced in this paper also use discrim-inative techniques.One of the first approaches to direct models fordialog state tracking was to consider a small, fixednumber of states and then apply a multinomialclassifier (Bohus and Rudnicky, 2006).
Since amultinomial classifier can make effective use ofmore features than a generative model, this ap-proach improves precision, but can decrease recallby only considering a small number of states (e.g.5 states).
Another discriminative approach is toscore each state using a binary model, then some-how combine the binary scores to form a distribu-tion ?
see, for example (Henderson et al., 2013b)which used a binary neural network.
This ap-proach scales to many states, but unlike a multi-nomial classifier, each binary classifier isn?t awareof its competitors, reducing accuracy.
Also, whentraining a binary model in the conventional way,the training criteria is mis-matched, since the clas-sifier is trained per hypothesis per timestep, but isevaluated only once per timestep.Maximum entropy (maxent) models have beenproposed which provide the strengths of both ofthese approaches (Metallinou et al., 2013).
Theprobability of a dialog hypothesis xibeing correct(y = i) is computed as:P (y = i|X,?)
=exp(?j?J?j?j(xi))?x?Xexp(?j?J?j?j(x)).
(1)Maximum entropy models yielded top perfor-mance in the first dialog state tracking challenge(Lee and Eskenazi, 2013).
In this paper, we usemaxent models as a baseline.A key limitation with linear (and log-linear)models such as maximum entropy models is thatthey do not automatically build conjunctions offeatures.
Conjunctions express conditional combi-nations of features such as whether the system at-tempted to confirm x and if ?yes?
was recognizedand if the confidence score of ?yes?
is high.
Con-junctions are important in dialog state tracking be-cause they are often more discriminative than indi-vidual features.
Moreover, in linear models for di-alog state tracking, one weight is learned per fea-ture (equation 1) (Metallinou et al., 2013).
As a re-sult, if a feature takes the same value for every dia-log hypothesis at a given timestep, its contributionto every hypothesis will be the same, and it willtherefore have no effect on the ranking.
For exam-ple, features describing the current system action283are identical for all state hypotheses.
Concretely,if ?j(xi) = c for all i, then changing c causes nochange in P (y = i|X,?)
for all i.Past work has shown that conjunctions improvedialog state tracking (Metallinou et al., 2013; Lee,2013).
However, past work has added conjunctionby hand, and this doesn?t scale: the number of pos-sible conjunctions increases exponentially in thenumber of terms in the conjunction, and it?s diffi-cult to predict in advance which conjunctions willbe useful.
This paper introduces algorithms fromweb-style ranking as a mechanism for automati-cally building feature conjunctions.In this paper we also use score averaging, awell-known machine learning technique for com-bining the output of several models, where eachoutput class takes the average score assigned byall the models.
Under certain assumptions ?
mostimportantly that errors are made independently ?score averaging is guaranteed to exceed the perfor-mance of the best single model.
Score averaginghas been applied to dialog state tracking in previ-ous work (Lee and Eskenazi, 2013).
Here we usescore averaging to maximize data use in cascadedmodels, and as a hedge against unlucky parametersettings.3 PreliminariesIn this paper we use data and evaluation metricsfrom the second dialog state tracking challenge(DSTC2) (Henderson et al., 2014; Henderson etal., 2013a).
Dialogs in DSTC2 are in the restau-rant search domain.
Users can search for restau-rants in multiple ways, including via constraints,or by name.
The system can offer restaurants thatmatch, confirm user input, ask for additional con-straints, etc.There are three components to the hidden di-alog state: user?s goal, search method, and re-quested slots.
The user?s goal specifies theuser?s search constraints, and consists of 4 slots:area, pricerange, foodtype, and name.
The num-ber of values for the slots ranges from 4 to113.1In DSTC2, trackers output scored listsfor each slot, and also a scored list of joint hy-potheses.
For example, at a given timestep ina given dialog, three joint goal hypothesis mightbe (area=west,food=italian), (area=west), and (),where () means the user hasn?t specified any con-straints yet.
Since tracking the joint user goal is1Including a special ?don?t care?
value.the most general and most difficult task, we?ll fo-cus on this first, and return to the other tasks insection 6.3.1 User goal featuresFor features, we broadly follow past work (Leeand Eskenazi, 2013; Lee, 2013; Metallinou et al.,2013).
For a hypothesis xi, for each slot the fea-tures encode 253 low-level quantities, such as:whether the slot value appears in this hypothesis;how many times the slot value has been observed;whether the slot value has been observed in thisturn; functions of recognition metrics such as con-fidence score and position on N-best list; goal pri-ors and confusion probabilities estimated on train-ing data (Williams, 2012; Metallinou et al., 2013);results of confirmation attempts (?Italian food, isthat right??
); output of the four rule-based base-line trackers; and the system act and its relation tothe goal?s slot value (e.g., whether the system actmentions this slot value).Of these 253 features for each slot, 119 are thesame for all values of that slot in a given turn,such as which system acts were observed in thisturn.
For these, we add 238 conjunctions withslot-specific features like confidence score, whichmakes these features useful to our maxent base-line.
This results in a total of 253+238 = 491 fea-tures per slot.
The features for each of the 4 slotsare concatenated together to yield 491 ?
4 = 1964features per joint hypothesis.3.2 Evaluation metricsIn DSTC2, there are 3 primary metrics for eval-uation ?
accuracy of the top-scored hypothesis,the L2 probability quality, and an ROC measure-ment.
The ROC measurement is only meaningfulwhen compared across systems with similar accu-racy; since our variants differ in accuracy, we omitROC.
However, note that all of the metrics, includ-ing ROC, for our final entries on the developmentset and test set are available for public downloadfrom the DSTC2 website.2The DSTC2 corpus consists of three partitions:train, development, and test.
Throughout sections4-6, we report accuracy by training on the train-ing set, and report accuracy on the developmentset and test set.
The development set was avail-able during development of the models, whereasthe test set was not.2camdial.org/?mh521/dstc/2843.3 BaselinesWe first compare to the four rule-based trackersprovided by DSTC2.
These were carefully de-signed by other research groups, and earlier ver-sions of them scored very well in the first DSTC(Wang and Lemon, 2013).
In each column in Ta-bles 2 and 3, we report the best result from anyrule-based tracker.
We also compare to a maxentmodel as in Eq 1.
Our implementation includesL1 and L2 regularization which was automaticallytuned via cross-validation.4 Web-style rankingThe ranking task is to order a set of N docu-ments by relevance given a query.
The inputto a ranker is a query Q and set of documentsX = {D1, .
.
.
, DN}, where each document is de-scribed in terms of features of that document andthe query ?
(Di, Q).
The output is a score foreach document, where the highest score indicatesthe most relevant document.
The overall objectiveis to order the documents by relevance, given thequery.
Training data indicates the relevance of ex-ample query/document pairs.
Training labels areprovided by judges, and relevance is typically de-scribed in terms of several levels, such as ?excel-lent?, ?good?, ?fair?, and ?not relevant?.The application of ranking to dialog state track-ing is straightforward: instead of ranking featuresof documents and queries ?
(Di, Q), we rank fea-tures of dialog states ?(Xi).
For labeling, the cor-rect dialog state is ?relevant?
and all other statesare ?not relevant?.Like dialog state tracking, ranking tasks oftenhave features which are constant over all docu-ments ?
particularly features of the query.
This isone reason why ranking algorithms have incorpo-rated methods for automatically building conjunc-tions.
The specific algorithm we use here is lamb-daMART (Wu et al., 2010; Burges, 2010).
Lamb-daMART is a mature, scalable ranking algorithm:it has underpinned the winning entry in a commu-nity ranking challenge task (Chapelle and Chang,2011), and is the foundation of the ranker in theBing search engine.
LambdaMART constructs aforest of M decision trees, where each tree con-sists of binary branches on features, and the leafnodes are real values.
Each binary branch speci-fies a threshold to apply to a single feature.
For aforest of M trees, the score of a dialog state x isF (x) =M?m=1?mfm(x) (2)where ?mis the weight of treem and fm(x) is thevalue of the leaf node obtained by evaluating de-cision tree m by features [?1(x), .
.
.
, ?J(x)].
Thetraining objective is to maximize ranking quality,which here means one-best accuracy.
The deci-sion trees are learned by regularized gradient de-scent, where trees are added successively to im-prove ranking quality ?
in our case, to maximizehow often the correct dialog state is ranked first.The number of trees to create and the number ofleaves per tree are tuning parameters.
Throughcross-validation, we found that 500 decision treeseach with 32 leaves were the best settings.
We usethe same set of 1964 features for lambdaMART aswas used for the maxent baseline.Results are shown in row 3 of table 2 under?Joint goal?.
Ranking outperforms both baselineson both the development and training set.
This re-sult illustrates that automatically-constructed con-junctions do indeed improve accuracy in dialogstate tracking.
An example of a single tree com-puted by lambdaMART is shown in AppendixA.
The complexity of this tree suggests that hu-man designers would find it difficult to specify atractable set of good conjunction features.5 Multiple SLU enginesAs described in the introduction, dialog statetracking typically proceeds in three stages: enu-meration of the set of dialog states to score, fea-ture extraction, and scoring.
Incorporating the out-put of multiple SLUs requires changing the firsttwo steps.
Continuing with notation from sec-tion 2, with a single SLU output U , the enumer-ation step is X?= G(s, U,X) ?
recall that Uis a set of SLU hypotheses from an SLU engine.With multiple SLU engines we have K SLU out-puts U1, .
.
.
, UK, and the enumeration step is thusX?= G(s, U1, .
.
.
, UK, X).
In our implementa-tion, we simply take the union of all concepts onall SLU N-best lists and enumerate states as in thesingle SLU case ?
i.e., the Cartesian product of di-alog states X with concepts on the SLU output.The feature extraction step is modified to out-put features derived from all of the SLU engines.Concretely, if a feature ?j(x) includes informa-tion from an SLU engine (such as confidence score285or position on the N-best list), it is duplicated Ktimes ?
i.e., once for each SLU engine.
Additionalbinary features are added to encode whether eachSLU engine has output the slot value of this dialogstate.
This allows for the situation that a slot valueis not output by all SLU engines, in which caseits confidence score, N-best list position, etc.
willnot be present from some SLU engines.
Using twoSLU engines on our data increases the number offeatures per joint goal from 1964 to 3140.5.1 SLU EnginesWe built two new SLU engines, broadly following(Henderson et al., 2012).
Both consist of manybinary classifiers.
In the first engine SLU1, abinary classifier is estimated for each slot/valuepair, and predicts the presence of that slot/valuepair in the utterance.
Similarly, a binary classi-fier is estimated for each user dialog act.
Inputfeatures are word n-grams from the ASR N-bestlist.
We only considered n-grams which were ob-served at least c times in the training data; infre-quent n-grams were mapped to a special UNKfeature.
For binary classification we used deci-sion trees, which marginally outperformed logis-tic regression, SVMs, and deep neural networks.Through cross-validation we set n = 2 and c = 2?
i.e., uni-grams and bi-grams which appear atleast twice in the training data.At runtime, the top SLU output on the N-bestlist is formed by taking the most likely combina-tion of all the binary classifiers; the second SLUoutput is formed by taking the second most likelycombination of all the binary classifiers; and so on,where only valid SLU combinations are consid-ered.
For example, the ?bye?
dialog act takes noarguments, so if ?bye?
and ?food=italian?
were themost likely combination, this combination wouldbe skipped.
Scores are formed by taking the prod-uct of all the binary classifiers, with some smooth-ing.The second SLU engine SLU2 is identical ex-cept that it also includes features from the wordconfusion network.
Specifically, each word (uni-gram) appearing in the word confusion network isa feature.
Bi-gram confusion network features didnot improve performance.If we train a new SLU engine and a ranker onthe same data, this will introduce unwanted bias.Therefore, we divided the training data in half, anduse the first half for training the SLU, and the sec-ond for training the ranker.
Table 1 shows severalevaluation metrics for each SLU engine, includ-ing the SLU included in the corpus, which we de-note SLU0.
SLU precision, recall, and F-measureare computed on the top hypotheses.
Item cross-entropy (ICE) (Thomson et al., 2008) measures thequality of the scores for all the items on the SLUN-best list.
Table 1 also shows joint goal accu-racy by using SLU0, SLU1, or SLU2, for either arule-based baseline or the ranking model.
Over-all, our SLU engines performed better on isolatedSLU metrics, but did not yield better state trackingperformance when used instead of the SLU resultsin the corpus.5.2 Results with multiple SLU enginesTable 2, rows 4 and 7 show that an improvementin performance does results from using 2 SLU en-gines.
In rows 4 and 7, the additional SLU en-gine is trained on the first half of the data, and theranker is trained on the second half ?
we call thisarrangement Fold A.
To maximize use of the data,it?s possible to train a second SLU/ranker pair byinverting the training data ?
i.e., train a secondSLU on the second half, and a second ranker (us-ing the second SLU) on the first half.
We call thisarrangement Fold B.
These two configurations canbe combined by running both trackers on test data,then averaging their scores.
We call this arrange-ment Fold AB.
If a hypothesis is output by onlyone configuration, it is assumed the other configu-ration output a zero score.Table 2, rows 5 and 8 show that the fold AB con-figuration yields an additional performance gain.5.3 Model averagingA small further improvement is possible by aver-aging across multiple models (rankers) with dif-ferent parameter settings.
Since all of the mod-els will be estimated on the same data, this is un-likely to make a large improvement, but it canhedge against an unlucky parameter setting, sincethe performance after averaging is usually close tothe maximum.To test this, we trained a second pair of rank-ing models, with a different number of leaves pertree (8 instead of 32).
We then applied this sec-ond model, and averaged the scores between thetwo variants.
Results are in Table 2, rows 6 and9.
Averaging scores across two parameter settingsgenerally results in performance equal to or betterthan the maximum of the two models.286Dev set Test setSLU SLU Metrics Goal track.
acc.
SLU Metrics Goal track.
acc.source Prec.
Recall F-meas.
ICE Rules Ranking Prec.
Recall F-meas.
ICE Rules RankingSLU0 0.883 0.666 0.759 2.185 0.623 0.666 0.900 0.691 0.782 1.955 0.719 0.739SLU1 0.818 0.729 0.771 2.189 0.598 0.637 0.846 0.762 0.802 1.943 0.667 0.709SLU2 0.844 0.742 0.789 2.098 0.605 0.658 0.870 0.777 0.821 1.845 0.685 0.734Table 1: Performance of three SLU engines.
SLU0 is the DSTC2 corpus; SLU1 is our engine withuni-grams and bi-grams of ASR results in the corpus; and SLU2 is SLU1 with the addition of unigramfeatures from the word confusion network.
Precision, Recall, F-measure, and ICE evaluate the qualityof the SLU output, not state tracking.
?ICE?
is item-wise cross entropy ?
smaller numbers are better(Thomson et al., 2008).
?Rules?
indicates dialog state tracking accuracy for user joint goals by runningthe rule-based baseline tracker on the indicated SLU (alone); ?Ranking?
indicates joint goal accuracy ofrunning a ranker trained on the indicated SLU (alone).
For training, goal tracking results use the ?FoldA?
configuration (c.f.
Section 5.2).5.4 Joint goal tracking summaryThe overall process used to train the joint goaltracker is summarized in Appendix B.
For jointgoal tracking, web-style ranking and multipleSLUs both yield improvements in accuracy on thedevelopment and test sets, with the improvementassociated with multiple SLUs being larger.
Wealso observe that ranking produces relatively poorL2 results.
This can be attributed to its trainingobjective, which explicitly maximizes 1-best ac-curacy without regard to the distribution of thescores.
This is in contrast to maxent models whichexplicitly minimize the L2 loss.
We examined thedistribution of scores, and qualitatively the rankeris usually placing less mass on its top guess thanmaxent, and spreading more mass out among other(usually wrong) entries.
We return to this in thefuture work section.6 Fixed-size state componentsDSTC2 consists of three tracking tasks: in addi-tion to the user?s goal, the user?s search methodand which slots they requested to hear were alsotracked.
These other two tasks were comparativelysimpler because their domains are of a small, fixedsize.
Thus classical machine learning methods canbe applied ?
i.e., ranking is not directly applicableto tracking the method and required slots.
How-ever, applying multiple SLU engines is still appli-cable.The search method specifies how the userwants to search.
There are 5 values: by-constraintssuch as area=west,food=italian, by-name such as?royal spice?, by-alternatives as in ?do you haveany others like that?
?, finished when the user isdone as in ?thanks goodbye?, and none when themethod can?t be determined.
At each turn, ex-actly one of the 5 methods is active, so we viewthe method component as a standard multinomialclassification task.
For features, we use the scorefor each method output by each of the 4 rule-based baselines, and whether each of the methodsis available according to the SLU results observedso far.
We also take conjunctions for each methodwith: whether each system dialog act is present inthe current turn, or has ever been used, and whatslots they mentioned; and whether each slot hasappeared in the SLU results from this turn, or anyturn.
In total there are 640 features for the methodclassifier (when using one SLU engine).The requested slots are the pieces of informa-tion the user wants to hear in that turn.
The usercan request to hear a restaurant?s area, food-type,name, price-range, address, phone number, post-code, and/or signature dish.
The user can ask tohear any combination of slots in a turn ?
e.g., ?tellme their address and phone number?.
Thereforewe view each requested slot as a binary classifica-tion task, and estimate 8 binary classifiers, one foreach requestable slot.
Each requested slot takesas features: whether the slot could logically be re-quested at this turn in the dialog; whether the SLUoutput contained a ?request?
act and which slotwas requested; the score output by each of the 4rule-based baselines; whether each system dialogact is present in the current turn, or has ever beenused, and what slots they mentioned; and whethereach slot has appeared in the SLU results from thisturn, or any turn.
For each requestable slot?s bi-nary classifier, this results in 187 features (withone SLU engine).For each of these tasks, we applied a maxent287Joint goal Search method Requested slotModel Dev.
set Test set Dev.
set Test set Dev.
set Test setRow SLU Fold Model comb.
Acc.
L2 Acc.
L2 Acc.
L2 Acc.
L2 Acc.
L2 Acc.
L21 0 ?
rules N 0.623 0.601 0.719 0.464 0.860 0.217 0.897 0.158 0.903 0.155 0.884 0.1962 0 all maxent N 0.649 0.532 0.692 0.480 0.890 0.177 0.909 0.143 0.952 0.078 0.967 0.0543 0 all * N 0.666 0.739 0.739 0.721 ?
?
?
?
?
?
?
?4 0+1 A * N 0.686 0.770 0.757 0.766 0.912 0.144 0.936 0.104 0.960 0.062 0.976 0.0395 0+1 AB * N 0.697 0.749 0.769 0.748 0.913 0.135 0.938 0.097 0.962 0.060 0.978 0.0376 0+1 AB ** Y 0.699 0.766 0.770 0.766 0.916 0.135 0.943 0.091 0.964 0.059 0.978 0.0367 0+2 A * N 0.697 0.731 0.765 0.727 0.910 0.146 0.939 0.099 0.966 0.058 0.979 0.0378 0+2 AB * N 0.711 0.725 0.778 0.721 0.913 0.133 0.943 0.092 0.967 0.058 0.980 0.0339 0+2 AB ** Y 0.710 0.742 0.781 0.739 0.915 0.132 0.948 0.085 0.967 0.057 0.980 0.033Table 2: Summary of accuracy and L2 for the three tracking tasks, trained on the ?train?
set.
In rowsmarked (*), joint goal accuracy used ranking, and the other two tasks used maxent.
In rows marked (**),several model classes/parameter settings were used and combined with score averaging.model; results for this and the best rule-basedbaseline are in the rows 1 and 2 of Table 2.
Wetried applying decision trees, but this did not im-prove performance (not shown) as it did for goaltracking.
Note that in the goal tracking task,one weight is learned for each feature for anyclass (goal), whereas in standard multiclass and bi-nary classification, one weight is learned for eachfeature,class pair.3Perhaps decision trees werenot effective in increasing accuracy for methodand requested slots because, compared to jointgoal tracking, some conjunctions are implicitly in-cluded in linear models.We then added a second SLU engine in the samemanner as for goal tracking.
This increased thenumber of features for the method task from 640to 840, and from 187 to 217 for each binary re-quested slot classifier.
Results are shown in Ta-ble 2; rows 4 and 7 show results with one fold,and rows 5 and 8 show results with both folds.Finally, we considered alternate model forms foreach classifier, and then combined them with scoreaveraging.
For the method task, we used a sec-ond maximum entropy model with different regu-larization weights, and a multi-class decision tree.For the requested slot binary classifiers, we addeda neural network classifier.
As above, score av-eraging across different model classes can yieldsmall gains (rows 6 and 9).Overall, as with goal tracking, adding a sec-ond SLU engine resulted in a substantial increasein accuracy.
Unlike goal tracking which useda ranker, the standard classification models usedhere are explicitly optimized for L2 performanceand as a result achieved very good L2 perfor-mance.3Plus a constant term per class.7 Blind evaluation resultsWhen preparing final entries for the DSTC2 blindevaluation, we not longer needed a separate devel-opment set, so our final models are trained on thecombined training and development sets.
In theDSTC2 results, we are team2.
Our entry 0 and 1use the process described above, including scoreaveraging across multiple models.
Entry0 usedSLU0+1, and entry1 used SLU0+2.
Entry3 useda maxent model on SLU0+2, but without modelaveraging since its parameters are set with cross-validation.4Results are summarized in Table 3.
For accu-racy for the joint goal and method tasks, our en-tries had highest accuracy.
After the evaluation,we learned that we were the only team to use fea-tures from the word confusion network (WCN).Comparing our entry0, which does not use WCNfeatures, to the other teams shows that, given thesame input data, our entries were still best for thejoint goal and method tasks.The blind evaluation results give a final oppor-tunity to compare the maxent model with the rank-ing model: entry1 and entry3 both use SLU0+2,and score an identical set of dialog states usingidentical features.
Joint goal accuracy is better forthe ranking model.
However, as noted above, L2performance for the ranking model was substan-tially worse than for the maxent model.After the blind evaluation, we realized that wehad inadvertently omitted a key feature from the?requested?
binary classifiers ?
whether the ?re-quest?
dialog act appeared in the SLU results.4The other entries team2.entry2 and team2.entry4 are notdescribed in this paper.
In brief, entry2 was based on a recur-rent neural network, and entry4 was a combination of entries1, 2, and 3.288Goal Method Requested Requested*model Acc.
L2 Acc.
L2 Acc.
L2 Acc.
L2Best baseline 0.719 0.464 0.897 0.158 0.884 0.196 0.884 0.196Best DSTC2 result from another team 0.768 0.346 0.940 0.095 0.978 0.035 0.978 0.035SLU0+1, AB, model comb.
(entry0) 0.775 0.758 0.944 0.092 0.954 0.073 0.977 0.037SLU0+2, AB, model comb.
(entry1) 0.784 0.735 0.947 0.087 0.957 0.068 0.980 0.034SLU0+2, AB, maxent (entry3) 0.771 0.354 0.947 0.093 0.941 0.090 0.979 0.040Table 3: Final DSTC2 evaluation results, training on the combined ?train?
and ?development?
sets.
Inthe results, we are team2.
?Model comb.?
indicates score averaging over several model instances.
For the?requested?
task, our entry in DSTC2 inadvertently omitted a key feature, which decreased performancesignificantly.
?Requested*?
columns indicate results with this feature included.
They were computedafter the blind evaluation and are not part of the official DSTC2 results.Therefore table 3 shows results with and withoutthis feature.
With the inclusion of this feature, therequested classifiers also achieved best accuracyand L2 scores, although we note that this is notpart of the official DSTC2 results.
(The results inthe preceding sections of this paper included thisfeature.
)8 ConclusionThis paper has introduced two new methods fordialog state tracking.
First, we have shown howto apply web-style ranking for scoring dialog statehypotheses.
Ranking is attractive because it canconstruct a forest of decision trees which computefeature conjunctions, and because it optimizes di-rectly for 1-best accuracy.
Second, we have in-troduced the usage of multiple SLU engines.
Us-ing additional SLU engines is attractive because itboth adds more possible dialog states to score (in-creasing recall), and adds features which help todiscriminate the best states (increasing precision).In experiments, using multiple SLU engines im-proved performance on all three of the tasks in thesecond dialog state tracking challenge.
Maximumentropy models scored best in the previous dia-log state tracking challenge; here we showed thatweb-style ranking improved accuracy over max-ent when using either a single or multiple SLUengines.
Thus, the two methods introduced hereare additive: they each yield gains separately, andfurther gains in combination.Comparing to other systems in the DSTC2 eval-uation, these two techniques yielded highest accu-racy in DSTC2 for 2 of 3 tasks.
If we include afeature accidentally omitted from the third task,our methods yield highest accuracy for all threetasks.
This experience highlights the importanceof the manual task of extracting a set of informa-tive features.
Also, ranking improved accuracy,but yielded poor probability quality.
For rank-ing, the L2 performance of ranking was amongthe worst in DSTC2.
By contrast, for the methodtask, where standard classification could be ap-plied, our entry yielded best L2 performance.
Therelative importance of L2 vs. accuracy in dialogstate tracking is an open question.In future work, we plan to investigate how toimprove the L2 performance of ranking.
One ap-proach is to train a maxent model on the output ofthe ranker.
On the test set, this yields an improve-ment in L2 score from 0.735 to 0.587, and simplyclamping ranker?s best guess to 1.0 and all othersto 0.0 improves L2 to 0.431.
This is a start, butnot competitive with the best result in DSTC2 of0.346.
Also, techniques which avoid the extrac-tion of manual features altogether would be ideal,particularly in light of experiences here.Even so, for the difficult and general task ofuser goal tracking, the techniques here yielded arelative error rate reduction of 23% over the bestbaseline, and exceeded the accuracy of any othertracker in the second dialog state tracking chal-lenge.AcknowledgementsThanks to Dan Bohus for making his maxent soft-ware available, to Andrzej Pastusiak for helpfulguidance with lambdaMART, and to Geoff Zweigfor several helpful conversations.ReferencesDan Bohus and Alex Rudnicky.
2006.
A ?K hypothe-ses + other?
belief updating model.
In Proc Amer-ican Association for Artificial Intelligence (AAAI)Workshop on Statistical and Empirical Approachesfor Spoken Dialogue Systems, Boston.289Christopher J.C. Burges.
2010.
From ranknet to lamb-darank to lambdamart: An overview.
Technical Re-port MSR-TR-2010-82, Microsoft Research.Olivier Chapelle and Yi Chang.
2011.
Yahoo!
learningto rank challenge.
JMLR Workshop and ConferenceProceedings, 14:1?24.Matthew Henderson, Milica Gasic, Blaise Thomson,Pirros Tsiakoulis, Kai Yu, and Steve Young.
2012.Discriminative spoken language understanding us-ing word confusion networks.
In Proc IEEE Work-shop on Spoken Language Technologies (SLT), Mi-ami, Florida, USA.Matthew Henderson, Blaise Thomson, and Jason D.Williams.
2013a.
Handbook for the dialog statetracking challenge 2 & 3.
Technical report, Cam-bridge University.Matthew Henderson, Blaise Thomson, and SteveYoung.
2013b.
Deep neural network approach forthe dialog state tracking challenge.
In Proceedingsof the SIGDIAL 2013 Conference, pages 467?471,Metz, France, August.
Association for Computa-tional Linguistics.Matthew Henderson, Blaise Thomson, and JasonWilliams.
2014.
The second dialog state trackingchallenge.
In Proceedings of the SIGdial 2014 Con-ference, Baltimore, U.S.A., June.Eric Horvitz and Tim Paek.
1999.
A computationalarchitecture for conversation.
In Proc 7th Interna-tional Conference on User Modeling (UM), Banff,Canada, pages 201?210.Sungjin Lee and Maxine Eskenazi.
2013.
Recipe forbuilding robust spoken dialog state trackers: Dialogstate tracking challenge system description.
In Pro-ceedings of the SIGDIAL 2013 Conference, pages414?422, Metz, France, August.
Association forComputational Linguistics.Sungjin Lee.
2013.
Structured discriminative modelfor dialog state tracking.
In Proceedings of theSIGDIAL 2013 Conference, pages 442?451, Metz,France, August.
Association for Computational Lin-guistics.Angeliki Metallinou, Dan Bohus, and Jason D.Williams.
2013.
Discriminative state tracking forspoken dialog systems.
In Proc Association forComputational Linguistics, Sofia.Blaise Thomson and Steve Young.
2010.
Bayesianupdate of dialogue state: A POMDP framework forspoken dialogue systems.
Computer Speech andLanguage, 24(4):562?588.B Thomson, K Yu, M Gasic, S Keizer, F Mairesse,J Schatzmann, and S Young.
2008.
Evaluatingsemantic-level confidence scores with multiple hy-potheses.
In Proc Intl Conf on Spoken LanguageProcessing (ICSLP), Brisbane, Australia.Zhuoran Wang and Oliver Lemon.
2013.
A sim-ple and generic belief tracking mechanism for thedialog state tracking challenge: On the believabil-ity of observed information.
In Proceedings of theSIGDIAL 2013 Conference, pages 423?432, Metz,France, August.
Association for Computational Lin-guistics.Jason D Williams and Steve Young.
2007.
Partiallyobservable Markov decision processes for spokendialog systems.
Computer Speech and Language,21(2):393?422.Jason D. Williams, Antoine Raux, Deepak Ramachan-dran, and Alan Black.
2013.
The dialog state track-ing challenge.
In Proc SIGdial Workshop on Dis-course and Dialogue, Metz, France.Jason D. Williams.
2012.
Challenges and oppor-tunities for state tracking in statistical spoken dia-log systems: Results from two public deployments.IEEE Journal of Selected Topics in Signal Process-ing, Special Issue on Advances in Spoken DialogueSystems and Mobile Interface, 6(8):959?970.Qiang Wu, Christopher J. C. Burges, Krysta M. Svore,and Jianfeng Gao.
2010.
Adapting boosting for in-formation retrieval measures.
Journal of Informa-tion Retrieval, 13(3):254?270.Steve Young, Milica Ga?si?c, Simon Keizer, Franc?oisMairesse, Jost Schatzmann, Blaise Thomson, andKai Yu.
2009.
The hidden information state model:a practical framework for POMDP-based spoken di-alogue management.
Computer Speech and Lan-guage, 24(2):150?174.Lukas Zilka, David Marek, Matej Korvas, and Filip Ju-rcicek.
2013.
Comparison of bayesian discrimina-tive and generative models for dialogue state track-ing.
In Proceedings of the SIGDIAL 2013 Confer-ence, pages 452?456, Metz, France, August.
Asso-ciation for Computational Linguistics.Appendix A: Example decision treeFigure 1 shows an example decision tree gener-ated by lambdaMART.
Note how the tree is able tocombine features across different slots ?
for exam-ple, following the right-most path tests the scoresof 3 different slots.
Also, note how generally morepositive evidence leads to higher scores.Appendix B: Schematic of approachFigure 2 shows a schematic diagram of ouroverall approach for training the state tracker(team2.entry0).290area.baseline1.score> 0.45 ?food.baseline1.score> 0.52 ?# values recognized forarea > 1 and food valueempty?price.baseline1.score> 0.61 ?+0.31-0.08area.best_conf_score >0.01+0.50Has system grounded price?-0.32 Is price value empty?+0.48+0.33-0.36 -0.47YNYNYNYNN YNYN YFigure 1: Appendix A: Example decision tree with 8 leaves generated by lambdaMART.
Each non-terminal node contains a binary test; each terminal node contains a real value that linearly contributes tothe score of the dialog state being evaluated.
?baseline1?
refers to the output of one of the rule-basedbaseline trackers, used in this classifier as an input feature.Final trackeroutputAll trainData, SLU0All test data,SLU0+1 / BAll test data,SLU0+1 / AScoreaveraging2nd half train,SLU0+1 / A1st half train,SLU0+1 / BTrain, Fold A,SLU0SLU 1Fold ASLU 1Fold BTrain, Fold B,SLU0Tracker ouputSLU0+1 / A / IITracker ouputSLU0+1 / A / ITracker ouputSLU0+1 / B / IITracker ouputSLU0+1 / B / IRanker,params IIRanker,params IRanker,params IIRanker,params IAll test data,SLU 0Figure 2: Appendix B: Schematic diagram of our overall approach for training the state tracker, usingSLU1 (team2.entry0).
Cylinders represent data, rectangles are models, and scripts are tracker output.Solid arrows are steps done at training time, and dotted arrows are steps done at test time.
Approach forSLU2 (team2.entry1) is identical except that additional features are used in training the SLU models.291
