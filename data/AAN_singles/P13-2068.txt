Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 382?386,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsBilingual Lexical Cohesion Trigger Model for Document-LevelMachine TranslationGuosheng Ben?
Deyi Xiong??
Zhiyang Teng?
Yajuan Lu??
Qun Liu??
?Key Laboratory of Intelligent Information ProcessingInstitute of Computing Technology, Chinese Academy of Sciences{benguosheng,tengzhiyang,lvyajuan,liuqun}@ict.ac.cn?School of Computer Science and Technology,Soochow University{dyxiong}@suda.edu.cn?Centre for Next Generation Localisation, Dublin City University{qliu}@computing.dcu.ieAbstractIn this paper, we propose a bilingual lexi-cal cohesion trigger model to capture lex-ical cohesion for document-level machinetranslation.
We integrate the model intohierarchical phrase-based machine trans-lation and achieve an absolute improve-ment of 0.85 BLEU points on average overthe baseline on NIST Chinese-English testsets.1 IntroductionCurrent statistical machine translation (SMT) sys-tems are mostly sentence-based.
The major draw-back of such a sentence-based translation fash-ion is the neglect of inter-sentential dependencies.As a linguistic means to establish inter-sententiallinks, lexical cohesion ties sentences together in-to a meaningfully interwoven structure throughwords with the same or related meanings (Wongand Kit, 2012).This paper studies lexical cohesion devices andincorporate them into document-level machinetranslation.
We propose a bilingual lexical cohe-sion trigger model to capture lexical cohesion fordocument-level SMT.
We consider a lexical co-hesion item in the source language and its corre-sponding counterpart in the target language as atrigger pair, in which we treat the source languagelexical cohesion item as the trigger and its targetlanguage counterpart as the triggered item.
Thenwe use mutual information to measure the strengthof the dependency between the trigger and trig-gered item.We integrate this model into a hierarchicalphrase-based SMT system.
Experiment results?Corresponding authorshow that it is able to achieve substantial improve-ments over the baseline.The remainder of this paper proceeds as fol-lows: Section 2 introduces the related work andhighlights the differences between previous meth-ods and our model.
Section 3 elaborates the pro-posed bilingual lexical cohesion trigger model, in-cluding the details of identifying lexical cohesiondevices, measuring dependency strength of bilin-gual lexical cohesion triggers and integrating themodel into SMT.
Section 4 presents experimentsto validate the effectiveness of our model.
Finally,Section 5 concludes with future work.2 Related WorkAs a linguistic means to establish inter-sententiallinks, cohesion has been explored in the literatureof both linguistics and computational linguistics.Cohesion is defined as relations of meaning thatexist within the text and divided into grammaticalcohesion that refers to the syntactic links betweentext items and lexical cohesion that is achievedthrough word choices in a text by Halliday andHasan (1976).
In order to improve the quality ofmachine translation output, cohesion has served asa high level quality criterion in post-editing (Vas-concellos, 1989).
As a part of COMTIS project,grammatical cohesion is integrated into machinetranslation models to capture inter-sentential links(Cartoni et al, 2011).
Wong and Kit (2012) in-corporate lexical cohesion to machine translationevaluation metrics to evaluate document-level ma-chine translation quality.
Xiong et al (2013) inte-grate various target-side lexical cohesion devicesinto document-level machine translation.
Lexicalcohesion is also partially explored in the cache-based translation models of Gong et al (2011) andtranslation consistency constraints of Xiao et al382(2011).All previous methods on lexical cohesion fordocument-level machine translation as mentionedabove have one thing in common, which is thatthey do not use any source language information.Our work is mostly related to the mutual infor-mation trigger based lexical cohesion model pro-posed by Xiong et al (2013).
However, we sig-nificantly extend their model to a bilingual lexicalcohesion trigger model that captures both sourceand target-side lexical cohesion items to improvetarget word selection in document-level machinetranslation.3 Bilingual Lexical Cohesion TriggerModel3.1 Identification of Lexical Cohesion DevicesLexical cohesion can be divided into reiterationand collocation (Wong and Kit, 2012).
Reitera-tion is a form of lexical cohesion which involvesthe repetition of a lexical item.
Collocation is apair of lexical items that have semantic relation-s, such as synonym, near-synonym, superordinate,subordinate, antonym, meronym and so on.
Inthe collocation, we focus on the synonym/near-synonym and super-subordinate semantic relation-s 1.
We define lexical cohesion devices as contentwords that have lexical cohesion relations, namelythe reiteration, synonym/near-synonym and super-subordinate.Reiteration is common in texts.
Take the fol-lowing two sentences extracted from a documentfor example (Halliday and Hasan, 1976).1.
There is a boy climbing the old elm.2.
That elm is not very safe.We see that word elm in the first sentence is re-peated in the second sentence.
Such reiteration de-vices are easy to identify in texts.
Synonym/near-synonym is a semantic relationship set.
We canuse WordNet (Fellbaum, 1998) to identify them.WordNet is a lexical resource that clusters wordswith the same sense into a semantic group calledsynset.
Synsets in WordNet are organized ac-cording to their semantic relations.
Let s(w) de-note a function that defines all synonym words ofw grouped in the same synset in WordNet.
Wecan use the function to compute all synonyms andnear-synonyms for word w. In order to represen-t conveniently, s0 denotes the set of synonyms in1Other collocations are not used frequently, such asantonyms.
So we we do not consider them in our study.s(w).
Near-synonym set s1 is defined as the unionof all synsets that are defined by the function s(w)where w?
s0.
It can be formulated as follows.s1 =?w?s0s(w) (1)s2 =?w?s1s(w) (2)s3 =?w?s2s(w) (3)Similarly sm can be defined recursively as follows.sm =?w?sm?1s(w) (4)Obviously, We can find synonyms and near-synonyms for word w according to formula (4).Superordinate and subordinate are formed bywords with an is-a semantic relation in WordNet.As the super-subordinate relation is also encodedin WordNet, we can define a function that is simi-lar to s(w) identify hypernyms and hyponyms.We use rep, syn and hyp to represent the lex-ical cohesion device reiteration, synonym/near-synonym and super-subordinate respectively here-after for convenience.3.2 Bilingual Lexical Cohesion TriggerModelIn a bilingual text, lexical cohesion is present inthe source and target language in a synchronousfashion.
We use a trigger model capture such abilingual lexical cohesion relation.
We define xRy(R?
{rep, syn, hyp}) as a trigger pair where x isthe trigger in the source language and y the trig-gered item in the target language.
In order to cap-ture these synchronous relations between lexicalcohesion items in the source language and theircounterparts in the target language, we use wordalignments.
First, we identify a monolingual lexi-cal cohesion relation in the target language in theform of tRy where t is the trigger, y the triggereditem that occurs in a sentence succeeding the sen-tence of t, and R?
{rep, syn, hyp}.
Second, wefind word x in the source language that is alignedto t in the target language.
We may find multiplewords xk1 in the source language that are alignedto t. We use all of them xiRt(1?i?k) to definebilingual lexical cohesion relations.
In this way,we can create bilingual lexical cohesion relationsxRy (R?
{rep, syn, hyp}): x being the trigger andy the triggered item.383The possibility that y will occur given x is equalto the chance that x triggers y.
Therefore we mea-sure the strength of dependency between the trig-ger and triggered item according to pointwise mu-tual information (PMI) (Church and Hanks, 1990;Xiong et al, 2011).The PMI for the trigger pair xRy where x is thetrigger, y the triggered item that occurs in a targetsentence succeeding the target sentence that alignsto the source sentence of x, and R?
{rep, syn, hyp}is calculated as follows.PMI(xRy) = log( p(x, y,R)p(x,R)p(y,R) ) (5)The joint probability p(x, y,R) is:p(x, y,R) = C(x, y,R)?x,y C(x, y,R)(6)where C(x, y,R) is the number of aligned bilin-gual documents where both x and y occurwith the relation R in different sentences, and?x,y C(x, y,R) is the number of bilingual docu-ments where this relation R occurs.
The marginalprobabilities of p(x,R) and p(y,R) can be calcu-lated as follows.p(x,R) =?yC(x, y,R) (7)p(y,R) =?xC(x, y,R) (8)Given a target sentence ym1 , our bilingual lexicalcohesion trigger model is defined as follows.MIR(ym1 ) =?yiexp(PMI(?Ryi)) (9)where yi are content words in the sentence ym1 andPMI(?Ryi)is the maximum PMI value among alltrigger words xq1 from source sentences that havebeen recently translated, where trigger words xq1have an R relation with word yi.PMI(?Ryi) = max1?j?qPMI(xjRyi) (10)Three models MIrep(ym1 ), MIsyn(ym1 ),MIhyp(ym1 ) for the reiteration device, thesynonym/near-synonym device and the super-subordinate device can be formulated as above.They are integrated into the log-linear model ofSMT as three different features.3.3 DecodingWe incorporate our bilingual lexical cohesion trig-ger model into a hierarchical phrase-based system(Chiang, 2007).
We add three features as follows.?
MIrep(ym1 )?
MIsyn(ym1 )?
MIhyp(ym1 )In order to quickly calculate the score of each fea-ture, we calculate PMI for each trigger pair be-fore decoding.
We translate document one by one.During translation, we maintain a cache to storesource language sentences of recently translatedtarget sentences and three sets Srep, Ssyn, Shypto store source language words that have the re-lation of {rep, syn, hyp} with content words gen-erated in target language.
During decoding, weupdate scores according to formula (9).
When onesentence is translated, we store the correspondingsource sentence into the cache.
When the wholedocument is translated, we clear the cache for thenext document.4 Experiments4.1 SetupOur experiments were conducted on the NISTChinese-English translation tasks with large-scaletraining data.
The bilingual training data contain-s 3.8M sentence pairs with 96.9M Chinese word-s and 109.5M English words from LDC2.
Themonolingual data for training data English lan-guage model includes the Xinhua portion of theGigaword corpus.
The development set is theNIST MT Evaluation test set of 2005 (MT05),which contains 100 documents.
We used the setsof MT06 and MT08 as test sets.
The numbers ofdocuments in MT06, MT08 are 79 and 109 respec-tively.
For the bilingual lexical cohesion triggermodel, we collected data with document bound-aries explicitly provided.
The corpora are select-ed from our bilingual training data and the wholeHong Kong parallel text corpus3, which contains103,236 documents with 2.80M sentences.2The corpora include LDC2002E18, LDC2003E07, LD-C2003E14,LDC2004E12,LDC2004T07,LDC2004T08(OnlyHong Kong News), LDC2005T06 and LDC2005T10.3They are LDC2003E14, LDC2004T07, LDC2005T06,LDC2005T10 and LDC2004T08 (Hong Kong Hansard-s/Laws/News).384We obtain the word alignments by runningGIZA++ (Och and Ney, 2003) in both direction-s and applying ?grow-diag-final-and?
refinemen-t (Koehn et al, 2003).
We apply SRI LanguageModeling Toolkit (Stolcke, 2002) to train a 4-gram language model with Kneser-Ney smooth-ing.
Case-insensitive NIST BLEU (Papineni etal., 2002) was used to measure translation per-formance.
We used minimum error rate trainingMERT (Och, 2003) for tuning the feature weights.4.2 Distribution of Lexical Cohesion Devicesin the Target LanguageCohesion Device Percentage(%)rep 30.85syn 17.58hyp 18.04Table 1: Distributions of lexical cohesion devicesin the target language.In this section we want to study how theselexical cohesion devices distribute in the train-ing data before conducting our experiments onthe bilingual lexical cohesion model.
Herewe study the distribution of lexical cohesion inthe target language (English).
Table 1 showsthe distribution of percentages that are countedbased on the content words in the training da-ta.
From Table 1, we can see that the reitera-tion cohesion device is nearly a third of all con-tent words (30.85%), synonym/near-synonym andsuper-subordinate devices account for 17.58% and18.04%.
Obviously, lexical cohesion devices arefrequently used in real-world texts.
Therefore cap-turing lexical cohesion devices is very useful fordocument-level machine translation.4.3 ResultsSystem MT06 MT08 AvgBase 30.43 23.32 26.88rep 31.24 23.70 27.47syn 30.92 23.71 27.32hyp 30.97 23.48 27.23rep+syn+hyp 31.47 23.98 27.73Table 2: BLEU scores with various lexical co-hesion devices on the test sets MT06 and MT08.?Base?
is the traditonal hierarchical system, ?Avg?is the average BLEU score on the two test sets.Results are shown in Table 2.
From the table,we can see that integrating a single lexical cohe-sion device into SMT, the model gains an improve-ment of up to 0.81 BLEU points on the MT06 testset.
Combining all three features rep+syn+hyp to-gether, the model gains an improvement of up to1.04 BLEU points on MT06 test set, and an av-erage improvement of 0.85 BLEU points on thetwo test sets of MT06 and MT08.
These stableimprovements strongly suggest that our bilinguallexical cohesion trigger model is able to substan-tially improve the translation quality.5 ConclusionsIn this paper we have presented a bilingual lex-ical cohesion trigger model to incorporate threeclasses of lexical cohesion devices, namely thereiteration, synonym/near-synonym and super-subordinate devices into a hierarchical phrase-based system.
Our experimental results showthat our model achieves a substantial improvementover the baseline.
This displays the advantage ofexploiting bilingual lexical cohesion.Grammatical and lexical cohesion have oftenbeen studied together in discourse analysis.
Inthe future, we plan to extend our model to cap-ture both grammatical and lexical cohesion indocument-level machine translation.AcknowledgmentsThis work was supported by 863 State Key Project(No.2011AA01A207) and National Key Technol-ogy R&D Program(No.2012BAH39B03).
QunLiu was also partially supported by Science Foun-dation Ireland (Grant No.07/CE/I1142) as part ofthe CNGL at Dublin City University.
We wouldlike to thank the anonymous reviewers for their in-sightful comments.ReferencesBruno Cartoni, Andrea Gesmundo, James Hender-son, Cristina Grisot, Paola Merlo, Thomas Mey-er, Jacques Moeschler, Sandrine Zufferey, AndreiPopescu-Belis, et al 2011.
Improving mt coher-ence through text-level processing of input texts:the comtis project.
http://webcast.
in2p3.
fr/videos-the comtis project.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
computational linguistics, 33(2):201?228.Kenneth Ward Church and Patrick Hanks.
1990.
Word385association norms, mutual information, and lexicog-raphy.
Computational linguistics, 16(1):22?29.Christine Fellbaum.
1998.
Wordnet: An electroniclexical database.Zhengxian Gong, Min Zhang, and Guodong Zhou.2011.
Cache-based document-level statistical ma-chine translation.
In Proceedings of the 2011 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 909?919, Edinburgh, Scotland,UK., July.
Association for Computational Linguis-tics.M.A.K Halliday and Ruqayia Hasan.
1976.
Cohesionin english.
English language series, 9.Philipp Koehn, Franz Josef Och, and Daniel Mar-cu.
2003.
Statistical phrase-based translation.
InProceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology-Volume 1, pages 48?54.
Association for Computa-tional Linguistics.Franz Josef Och and Hermann Ney.
2003.
A systemat-ic comparison of various statistical alignment mod-els.
Computational linguistics, 29(1):19?51.Franz Josef Och.
2003.
Minimum error rate train-ing in statistical machine translation.
In Proceed-ings of the 41st Annual Meeting of the Associationfor Computational Linguistics, pages 160?167, S-apporo, Japan, July.
Association for ComputationalLinguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic e-valuation of machine translation.
In Proceedings of40th Annual Meeting of the Association for Com-putational Linguistics, pages 311?318, Philadelphia,Pennsylvania, USA, July.
Association for Computa-tional Linguistics.Andreas Stolcke.
2002.
Srilm-an extensible languagemodeling toolkit.
In Proceedings of the internation-al conference on spoken language processing, vol-ume 2, pages 901?904.Muriel Vasconcellos.
1989.
Cohesion and coherencein the presentation of machine translation products.Georgetown University Round Table on Languagesand Linguistics, pages 89?105.Billy T. M. Wong and Chunyu Kit.
2012.
Extend-ing machine translation evaluation metrics with lex-ical cohesion to document level.
In Proceedings ofthe 2012 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning, pages 1060?1068, JejuIsland, Korea, July.
Association for ComputationalLinguistics.Tong Xiao, Jingbo Zhu, Shujie Yao, and Hao Zhang.2011.
Document-level consistency verification inmachine translation.
In Machine Translation Sum-mit, volume 13, pages 131?138.Deyi Xiong, Min Zhang, and Haizhou Li.
2011.Enhancing language models in statistical machinetranslation with backward n-grams and mutual in-formation triggers.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, pages1288?1297, Portland, Oregon, USA, June.
Associa-tion for Computational Linguistics.Deyi Xiong, Guosheng Ben, Min Zhang, Yajuan Lv,and Qun Liu.
2013.
Modeling lexical cohesion fordocument-level machine translation.
In Proceedingsof the Twenty-Third international joint conferenceon Artificial Intelligence, Beijing,China.386
