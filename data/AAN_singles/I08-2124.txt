Towards Automated Semantic Analysis on Biomedical Research ArticlesDonghui Feng         Gully Burns         Jingbo Zhu         Eduard HovyInformation Sciences InstituteUniversity of Southern CaliforniaMarina del Rey, CA, 90292{donghui, burns, jingboz, hovy}@isi.eduAbstractIn this paper, we present an empiricalstudy on adapting Conditional RandomFields (CRF) models to conduct semanticanalysis on biomedical articles using ac-tive learning.
We explore uncertainty-based active learning with the CRF modelto dynamically select the most informa-tive training examples.
This abridges thepower of the supervised methods and ex-pensive human annotation cost.1 IntroductionResearchers have experienced an increasing needfor automated/semi-automated knowledge acquisi-tion from the research literature.
This situation isespecially serious in the biomedical domain wherethe number of individual facts that need to bememorized is very high.Many successful information extraction (IE)systems, work in a supervised fashion, requiringhuman annotations for training.
However, humanannotations are either too expensive or not alwaysavailable and this has become a bottleneck to de-veloping supervised IE methods to new domains.Fortunately, active learning systems designstrategies to select the most informative trainingexamples.
This process can achieve certain levelsof performance faster and reduce human annota-tion (e.g., Thompson et al, 1999; Shen et al, 2004).In this paper, we present an empirical study onadapting CRF model to conduct semantic analysison biomedical research literature.
We integrate anuncertainty-based active learning framework withthe CRF model to dynamically select the most in-formative training examples and reduce humanannotation cost.
A systematic study with exhaus-tive experimental evaluations shows that it canachieve satisfactory performance on biomedicaldata while requiring less human annotation.Unlike direct estimation on target individuals intraditional active learning, we use two heuristiccertainty scores, peer comparison certainty and setcomparison certainty, to indirectly estimate se-quences labeling quality in CRF models.We partition biomedical research literature byexperimental types.
In this paper, our goal is toanalyze various aspects of useful knowledge abouttract-tracing experiments (TTE).
This type of ex-periments has prompted the development of sev-eral curated databases but they have only partialcoverage of the available literature (e.g., Stephan etal., 2001).2 Related WorkKnowledge Base Management Systems allowindividual users to construct personalizedrepositories of knowledge statements based ontheir own interaction with the research literature(Stephan et al, 2001; Burns and Cheng, 2006).
Butthis process of data entry and curation is manual.Current approaches on biomedical text mining (e.g.,Srinivas et al, 2005; OKanohara et al, 2006) tendto address the tasks of named entity recognition orrelation extraction, and our goal is more complex:to extract computational representations of theminimum information in a given experiment type.Pattern-based IE approaches employ seed datato learn useful patterns to pinpoint required fieldsvalues (e.g.
Ravichandran and Hovy, 2002; Mannand Yarowsky, 2005; Feng et al, 2006).
However,this only works if the data corpus is rich enough tolearn variant surface patterns and does not neces-sarily generalize to more complex situations, suchas our domain problem.
Within biomedical articles,sentences tend to be long and the prose structuretends to be more complex than newsprint.871The CRF model (Lafferty et al, 2001) providesa compact way to integrate different types of fea-tures for sequential labeling problems.
Reportedwork includes improved model variants (e.g., Jiaoet al, 2006) and applications such as web data ex-traction (Pinto et al, 2003), scientific citation ex-traction (Peng and McCallum, 2004), word align-ment (Blunsom and Cohn, 2006), and discourse-level chunking (Feng et al, 2007).Pool-based active learning was first successfullyapplied to language processing on text classifica-tion (Lewis and Gale, 1994; McCallum and Nigam,1998; Tong and Koller, 2000).
It was also gradu-ally applied to NLP tasks, such as information ex-traction (Thompson et al, 1999); semantic parsing(Thompson et al, 1999); statistical parsing (Tanget al, 2002); NER (Shen et al, 2004); and WordSense Disambiguation (Chen et al, 2006).
In thispaper, we use CRF models to perform a more com-plex task on the primary TTE experimental resultsand adapt it to process new biomedical data.3 Semantic Analysis with CRF Model3.1 What knowledge is of interest?The goal of TTE is to chart the interconnectivity ofthe brain by injecting tracer chemicals into a regionof the brain and then identifying correspondinglabeled regions where the tracer is transported to.A typical TTE paper may report experiments aboutone or many labeled regions.Name DescriptioninjectionLocation the named brain region where the injection was made.tracerChemical the tracer chemical used.labelingLocation the region/location where the labeling was found.labelingDescription a description of labeling, den-sity or label type.Table 1.
Minimum knowledge schema for a TTE.Figure 1.
An extraction example of TTE description.In order to construct the minimum informationrequired to interpret a TTE, we consider a set ofspecific components as shown in Table 1.Figure 1 gives an example of description of acomplete TTE in a single sentence.
In the researcharticles, this information is usually spread overmany such sentences.3.2 CRF LabelingWe use a plain text sentence for input and attemptto label each token with a field label.
In addition tothe four pre-defined fields, a default label, ?O?, isused to denote tokens beyond our concern.In this task, we consider five types of featuresbased on language analysis as shown in Table 2.Name Feature DescriptionTOPOGRAPHY Is word topog-raphic?BRAIN_REGION Is word a regionname?TRACER Is word a tracerchemical?DENSITY Is word a densityterm?LexicalKnowledgeLABELING_TYPE Does word denotea labeling type?Surface Word Word Current wordContextWindowCONT_INJ If current word ifwithin a windowof injection con-textPrev-word Previous word WindowWords Next-word Next wordRoot-form Root form of theword if differentGov-verb The governingverbSubj The sentencesubjectDependencyFeaturesObj The sentenceobjectTable 2.
The features for system labeling.Lexical Knowledge.
We define lexical items rep-resenting different aspects of prior knowledge.
Tothis end we use names of brain structures takenfrom brain atlases, standard terms to denote neuro-anatomical topographical spatial relationships, andcommon sense words for labeling descriptions.
Wecollect five separate lexicons as shown in Table 3.Lexicons # of terms # of wordsBRAIN_REGION 1123 5536DENSITY 8 10LABELING_TYPE 9 13TRACER 30 30TOPOGRAPHY 9 36Total 1179 5625Table 3.
The five lexicons.The NY injection ( Fig .
9B ) encompassedtracerChemicalmost of the pons and was very dense ininjectionLocationthe region of the MLF.labelingLocation872Surface word.
The word token is an importantindicator of the probable label for itself.Context Window.
The TTE is a description of theinject-label-findings context.
Whenever we find aword with a root form of ?injection?
or ?deposit?,we generate a context window around this wordand all the words falling into this window are as-signed a feature of ?CON_INJ?.
This means whenlabeling these words the system should considerthe very current context.Window Words.
We also use all the words occur-ring in the window around the current word.
Weset the window size to only include the previousand following words (window size = 1).Dependency Features.
To untangle word relation-ships within each sentence, we apply the depend-ency parser MiniPar (Lin, 1998) to parse each sen-tence, and then derive four types of features.
Thesefeatures are (a) root form of word, (b) the subjectin the sentence, (c) the object in the sentence, and(d) the governing verb for each word.4 Uncertainty-based Active LearningActive learning was initially introduced forclassification tasks.
The intuition is to always addthe most informative examples to the training set toimprove the system as much as possible.We apply an uncertainty/certainty score-basedapproach.
Unlike traditional classification tasks,where disagreement or uncertainty is easy to obtainon target individuals, information extraction tasksin our problem take a whole sequence of tokensthat might include several slots as processing units.We therefore need to make decisions on whether afull sequence should be returned for labeling.Estimations on confidence for single segmentsin the CRF model have been proposed by (Culottaand McCallum, 2004; Kristjannson et al, 2004).However as every processing unit in the data set isat the sentence level and we make decisions at thesentence level to train better sequential labelingmodels, we define heuristic scores at the sentencelevel.Symons et al (2006) presents multi-criterion foractive learning with CRF models, but our motiva-tion is from a different perspective.
The labelingresult for every sentence corresponds to a decodingpath in the state transition network.
Inspired by thedecoding and re-ranking approaches in statisticalmachine translation, we use two heuristic scores tomeasure the degree of correctness of the top label-ing path, namely, peer comparison certainty andset comparison certainty.Suppose a sentence S includes n words/tokensand a labeling path at position m in the ranked N-best list is represented by ),...,,( 110 ?= nm lllL .
Thenthe probability of this labeling path is representedby )( mLP , and we have the following two equa-tions to define the peer comparison certaintyscore, )(SScore peer  and set comparison certaintyscore, )(SScoreset :)()()(21LPLPSScorepeer =                                      (1)?==NkksetLPLPSScore11)()()(                                    (2)For peer comparison certainty (Eq.
1), we calcu-late the ratio of the top-scoring labeling path prob-ability to the second labeling path probability.
Ahigh ratio means there is a big jump from the toplabeling path to the second one.
The higher the ra-tio score, the higher the relative degree of correct-ness for the top labeling path, giving system higherconfidence for those with higher peer comparisoncertainty scores.
Sentences with lowest certaintyscore will be sent to the oracle for manual labeling.In the labeling path space, if a labeling path isstrong enough, its probability score should domi-nate all the other path scores.
In Equation 2, wecompute the set comparison certainty score by con-sidering the portion of the probability of the path inthe overall N-best labeling path space.
A largevalue means the top path dominates all the otherlabeling paths together giving the system a higherconfidence on the current path over others.We start with a seed training set including k la-beled sentences.
We then train a CRF model withthe training data and use it to label unlabeled data.The results are compared based on the certaintyscores and those sentences with the lowest cer-tainty scores are sent to an oracle for human label-ing.
The new labeled sentences are then added tothe training set for next iteration.5 Experimental ResultsWe first investigated how the active learning stepscould help for the task.
Second, we evaluated howthe CRF labeling system worked with different setsof features.
We finally applied the model to new873biomedical articles and examined its performanceon one of its subsets.5.1 Experimental SetupWe have obtained 9474 Journal of ComparativeNeurology (JCN)1 articles from 1982 to 2005.
Forsentence labeling, we collected 21 TTE articlesfrom the JCN corpus.
They were converted fromPDF files to XML files, and all of the article sec-tions were identified using a simple rule-based ap-proach.
As most of the meaningful descriptions ofTTEs appear in the Results section, we only proc-essed the Results section.
The 21 files in total in-clude 2009 sentences, in which 1029 sentences aremeaningful descriptions for TTEs and 980 sen-tences are not related to TTEs.We randomly split the sentences into a trainingpool and a testing pool, under a ratio 2:1.
Thetraining pool includes 1338 sentences, with 685 ofthem related to TTEs, while 653 not.
Testing wasbased on meaningful sentences in the testing pool.Table 4 gives the configurations in the data pools.# ofRelatedSentences# ofUnrelatedSentencesSumTraining Pool 685 653 1338Testing Pool 344 327 671Sum 1029 980 2009Table 4.
Training and testing pool configurations.5.2 Evaluation MetricsAs the label ?O?
dominates the data set (70% outof all tokens), a simple accuracy score would pro-vide an inappropriate high score for a baseline sys-tem that always chooses ?O?.
We used Precision,Recall, and F_Score to evaluate only meaningfullabels.5.3 How well does active learning work?For the active learning procedure, we initially se-lected a set of seed sentences related to TTEs fromthe training pool.
At every step we trained a CRFmodel and labeled sentences in the rest of the train-ing pool.
As described in section 4, those with thelowest rank on certainty scores were selected.
Ifthey are related to a TTE, human annotation willbe added to the training set.
Otherwise, the systemwill keep on selecting sentences until it findsenough related sentences.1 http://www3.interscience.wiley.com/cgi-bin/jhome/31248People have found active learning in batch modeis more efficient, as in some cases a single addi-tional training example will not improve a classi-fier/system that much.
In our task, we chose thebottom k related sentences with the lowest cer-tainty scores.
We conducted various experimentsfor k = 2, 5, and 10.
We also compared experi-ments with passive learning, where at every stepthe new k related sentences were randomly se-lected from the corpus.
Figures 2, 3, and 4 give thelearning curves for precision, recall, and F_Scoreswhen k = 10.Figure 2.
Learning curve for Precision.Figure 3.
Learning curve for Recall.Figure 4.
Learning curve for F_Score.From these figures, we can see active learningapproaches required fewer training examples toachieve the same level of performance.
As we it-eratively added new labeled sentences into thetraining set, the precision scores of active learningwere steadily better than that of passive learning asthe uncertain examples were added to strengthen874existing labels.
However, the recall curve isslightly different.
Before some point, the recallscore of passive learning was a little better thanactive learning.
The reason is that examples se-lected by active learning are mainly used to fosterexisting labels but have relatively weaker im-provements for new labels, while passive learninghas the freedom to add new knowledge for newlabels and improve recall scores faster.
As we keepon using more examples, the active learningcatches up with and overtakes passive learning onrecall score.These experiments demonstrate that under theframework of active learning, examples needed totrain a CRF model can be greatly reduced andtherefore make it feasible to adapt to other domains.5.4 How well does CRF labeling work?As we added selected annotated sentences, the sys-tem performance kept improving.
We investigatedsystem performance at the final step when all therelated sentences in the training pool are selectedinto the training set.
The testing set alo only in-cludes the related sentences.
This results in 685training sentences and 344 testing sentences.To establish a baseline for our labeling task, wesimply scanned every sentence for words orphrases from each lexicon.
If the term was present,then we labeled the word based on the lexicon inwhich it appeared.
If words appeared in multiplelexicons, we assigned labels randomly.System Features Prec.
Recall F_ScoreBaseline 0.4067 0.1761 0.2458Lexicon 0.5998 0.3734 0.4602Lexicon+ Surface Words0.7663 0.7302 0.7478Lexicon+ Surface Words+ Context Window0.7717 0.7279 0.7491Lexicon + SurfaceWords + ContextWindow + WindowWords0.8076 0.7451 0.7751Lexicon + SurfaceWords + ContextWindow + WindowWords + Depend-ency Features0.7991 0.7828 0.7909Table 5.
Precision, Recall, and F_Score for labeling.We tried exhaustive feature combinations.
Table5 shows system performance with different featurecombinations.
All systems performed significantlyhigher than the baseline.
The sole use of lexiconknowledge produced poor performance, and theinclusion of surface words produced significantimprovement.
The use of window words boostedprecision and recall.
The performance with all thefeatures generated an F_score of 0.7909.We explored how system performance reflectsdifferent labels.
Figure 5 and 6 depict the detaileddistribution of system labeling from the perspec-tive of precision and recall respectively for the sys-tem with the best performance.
Most errors oc-curred in the confusion of injectionLocation andlabelingLocation, or of the meaningful labels and?O?.0.000.100.200.300.400.500.600.700.800.901.00injLoc labelDesp labelLoc tracerOinjLoclabelDesplabelLoctracerFigure 5.
Precision confusion matrix distribution.0.000.100.200.300.400.500.600.700.800.901.00injLoc labelDesp labelLoc tracerOinjLoclabelDesplabelLoctracerFigure 6.
Recall confusion matrix distribution.The worst performance occurred for files thatdistinguish themselves from others by using fairlydifferent writing styles.
We believe given moretraining data with different writing styles, the sys-tem could achieve a better overall performance.5.5 On New Biomedical DataUnder this active learning framework, we haveshown a CRF model can be trained with less anno-tation cost than using traditional passive learning.We adapted the trained CRF model to new bio-medical research articles.Out of the 9474 collected JCN articles, morethan 230 research articles are on TTEs.
The wholeprocessing time for each document varies from 20seconds to 90 seconds.
We sent the new system-labeled files back to a biomedical knowledge ex-pert for manual annotation.
The time to correct oneautomatically labeled document is dramaticallyreduced, around 1/3 of that spent on raw text.We processed 214 new research articles and ex-amined a subset including 16 articles.
We evalu-875ated it in two aspects: the overall performance andthe performance averaged at the document level.Table 6 gives the performance on the whole newsubset and that averaged on 16 documents.
Theperformance is a little bit lower than reported inthe previous section as the new document set mightinclude different styles of documents.
We exam-ined system performance at each document.
Figure7 gives the detailed evaluation for each of the 16documents.
The average F_Score of the documentlevel is around 74%.
For those documents withreasonable TTE description, the system canachieve an F_Score of 87%.
The bad documentshad a different description style and usually mixedthe TTE descriptions with general discussion.Prec.
Recall F_ScoreOverall 0.7683 0.7155 0.7410Averaged per Doc.
0.7686 0.7209 0.7418Table 6.
Performance on the whole new subset andthe averaged performance per document.00.10.20.30.40.50.60.70.80.9111 4 12 0 6 2 14 7 8 15 3 1 13 10 9 5 Doc No.PrecisionRecallF-scoreFigure 7.
System performance per document.6 Conclusions and Future WorkIn this paper, we explored adapting a supervisedCRF model for semantic analysis on biomedicalarticles using an active learning framework.
Itabridges the power of the supervised approach andexpensive human costs.
We are also investigatingthe use of other certainty measures, such as aver-aged field confidence scores over each sentence.In the long run we wish to generalize the frame-work to be able to mine other types of experimentswithin the biomedical research literature and im-pact research in those domains.ReferencesBlunsom, P. and Cohn, T. 2006.
Discriminative word align-ment with conditional random fields.
In ACL-2006.Burns, G.A.
and Cheng, W.C. 2006.
Tools for knowledgeacquisition within the NeuroScholar system and their ap-plication to anatomical tract-tracing data.
In Journal ofBiomedical Discovery and Collaboration.Chen, J., Schein, A., Ungar, L., and Palmer, M. 2006.
An em-pirical study of the behavior of active learning for wordsense disambiguation.
In Proc.
of HLT-NAACL 2006.Culotta, A. and McCallum, A.
2004.
Confidence estimationfor information extraction.
In HLT-NAACL-2004, short pa-pers.Feng, D., Burns, G., and Hovy, E.H. 2007.
Extracting datarecords from unstructured biomedical full text.
InProc.
of EMNLP-CONLL-2007.Feng, D., Ravichandran, D., and Hovy, E.H. 2006.
Mining andre-ranking for answering biographical queries on the web.In Proc.
of AAAI-2006.Jiao, F., Wang, S., Lee, C., Greiner, R., and Schuurmans, D.2006.
Semi-supervised conditional random fields for im-proved sequence segmentation and labeling.
In Proc.
ofACL-2006.Kristjannson, T., Culotta, A., Viola, P., and McCallum, A.2004.
Interactive information extraction with constrainedconditional random fields.
In Proc.
of AAAI-2004.Lafferty, J., McCallum, A., and Pereira, F. 2001.
Conditionalrandom fields: probabilistic models for segmenting and la-beling sequence data.
In Proc.
of ICML-2001.Lewis, D.D.
and Gale, W.A.
1994.
A sequential algorithm fortraining text classifiers.
In Proc.
of SIGIR-1994.Lin, D. 1998.
Dependency-based evaluation of MINIPAR.
InWorkshop on the Evaluation of Parsing Systems.Mann, G.S.
and Yarowsky, D. 2005.
Multi-field informationextraction and cross-document fusion.
In Proc.
of ACL-2005.McCallum, A.K.
2002.
MALLET: a machine Learning forlanguage toolkit.
http://mallet.cs.umass.edu.McCallum, A. and Nigam, K. 1998.
Employing EM in pool-based active learning for text classification.
In Proc.
ofICML-98.OKanohara, D., Miyao, Y., Tsuruoka, Y., and Tsujii, J.
2006.Improving the scalability of semi-markov conditional ran-dom fields for named entity recognition.
In ACL-2006.Peng, F. and McCallum, A.
2004.
Accurate information ex-traction from research papers using conditional randomfields.
In Proc.
of HLT-NAACL-2004.Pinto, D., McCallum, A., Wei, X., and Croft, W.B.
2003.
Ta-ble extraction using conditional random fields.
In SIGIR-2003.Ravichandran, D. and Hovy, E.H. 2002.
Learning surface textpatterns for a question answering system.
In ACL-2002.Shen, D., Zhang, J., Su, J., Zhou, G., and Tan, C.L.
2004.Multi-criteria-based active learning for named entity rec-ognition.
In Proc.
of ACL-2004.Srinivas, et al, 2005.
Comparison of vector space modelmethodologies to reconcile cross-species neuroanatomicalconcepts.
Neuroinformatics, 3(2).Stephan, K.E., et al, 2001.
Advanced database methodologyfor the Collation of Connectivity data on the Macaquebrain (CoCoMac).
Philos Trans R Soc Lond B Biol Sci.Symons et al, 2006.
Multi-Criterion Active Learning in Con-ditional Random Fields.
In ICTAI-2006.Tang, M., Luo, X., and Roukos, S. 2002.
Active learning forstatistical natural language parsing.
In ACL-2002.Thompson, C.A., Califf, M.E., and Mooney, R.J. 1999.
Activelearning for natural language parsing and information ex-traction.
In Proc.
of ICML-99.Tong, S. and Koller, D. 2000.
Support vector machine activelearning with applications to text classification.
In Proc.
ofICML-2000.876
