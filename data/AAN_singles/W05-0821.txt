Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 125?128,Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005Improved Language Modeling for Statistical Machine TranslationKatrin Kirchhoff and Mei YangDepartment of Electrical EngineeringUniversity of Washington, Seattle, WA, 98195{katrin,yangmei}@ee.washington.eduAbstractStatistical machine translation systemsuse a combination of one or more transla-tion models and a language model.
Whilethere is a significant body of research ad-dressing the improvement of translationmodels, the problem of optimizing lan-guage models for a specific translationtask has not received much attention.
Typ-ically, standard word trigram models areused as an out-of-the-box component ina statistical machine translation system.In this paper we apply language model-ing techniques that have proved benefi-cial in automatic speech recognition to theACL05 machine translation shared datatask and demonstrate improvements over abaseline system with a standard languagemodel.1 IntroductionStatistical machine translation (SMT) makes use ofa noisy channel model where a sentence e?
in the de-sired language can be conceived of as originating asa sentence f?
in a source language.
The goal is tofind, for every input utterance f?
, the best hypothesise??
such thate??
= argmaxe?P (e?|f?)
= argmaxe?P (f?
|e?
)P (e?
)(1)P (f?
|e?)
is the translation model expressing proba-bilistic constraints on the association of source andtarget strings.
P (e?)
is a language model specifyingthe probability of target language strings.
Usually, astandard word trigram model of the formP (e1, ..., el) ?l?i=3P (ei|ei?1, ei?2) (2)is used, where e?
= e1, ..., el .
Each word is predictedbased on a history of two preceding words.Most work in SMT has concentrated on develop-ing better translation models, decoding algorithms,or minimum error rate training for SMT.
Compara-tively little effort has been spent on language mod-eling for machine translation.
In other fields, partic-ularly in automatic speech recognition (ASR), thereexists a large body of work on statistical languagemodeling, addressing e.g.
the use of word classes,language model adaptation, or alternative probabil-ity estimation techniques.
The goal of this study wasto use some of the language modeling techniquesthat have proved beneficial for ASR in the past andto investigate whether they transfer to statistical ma-chine translation.
In particular, this includes lan-guage models that make use of morphological andpart-of-speech information, so-called factored lan-guage models.2 Factored Language ModelsA factored language model (FLM) (Bilmes andKirchhoff, 2003) is based on a representation ofwords as feature vectors and can utilize a variety ofadditional information sources in addition to words,such as part-of-speech (POS) information, morpho-logical information, or semantic features, in a uni-fied and principled framework.
Assuming that each125word w can be decomposed into k features, i.e.
w ?f1:K , a trigram model can be defined asp(f1:K1 , f1:K2 , ..., f 1:KT ) ?T?t=3p(f1:Kt |f1:Kt?1 , f1:Kt?2 )(3)Each word is dependent not only on a single streamof temporally preceding words, but also on addi-tional parallel streams of features.
This represen-tation can be used to provide more robust probabil-ity estimates when a particular word n-gram has notbeen observed in the training data but its correspond-ing feature combinations (e.g.
stem or tag trigrams)has been observed.
FLMs are therefore designed toexploit sparse training data more effectively.
How-ever, even when a sufficient amount of training datais available, a language model utilizing morpholog-ical and POS information may bias the system to-wards selecting more fluent translations, by boost-ing the score of hypotheses with e.g.
frequent POScombinations.
In FLMs, word feature informationis integrated via a new generalized parallel back-off technique.
In standard Katz-style backoff, themaximum-likelihood estimate of an n-gram with toofew observations in the training data is replaced witha probability derived from the lower-order (n ?
1)-gram and a backoff weight as follows:pBO(wt|wt?1, wt?2) (4)={dcpML(wt|wt?1, wt?2) if c > ??
(wt?1, wt?2)pBO(wt|wt?1) otherwisewhere c is the count of (wt, wt?1, wt?2), pMLdenotes the maximum-likelihood estimate, ?
is acount threshold, dc is a discounting factor and?
(wt?1, wt?2) is a normalization factor.
Duringstandard backoff, the most distant conditioning vari-able (in this case wt?2) is dropped first, followedby the second most distant variable etc., until theunigram is reached.
This can be visualized as abackoff path (Figure 1(a)).
If additional condition-ing variables are used which do not form a tempo-ral sequence, it is not immediately obvious in whichorder they should be eliminated.
In this case, sev-eral backoff paths are possible, which can be sum-marized in a backoff graph (Figure 1(b)).
Paths inthis graph can be chosen in advance based on lin-guistic knowledge, or at run-time based on statis-tical criteria such as counts in the training set.
IttW 1tW?
2tW?
3tW?tW 1tW?
2tW?tW 1tW?tW(a)F 1F 2F 3FFF 1F 2F F 1F 3F F 2F 3FF 1F F 3FF 2F(b)Figure 1: Standard backoff path for a 4-gram lan-guage model over words (left) and backoff graphover word features (right).is also possible to choose multiple paths and com-bine their probability estimates.
This is achieved byreplacing the backed-off probability pBO in Equa-tion 2 by a general function g, which can be anynon-negative function applied to the counts of thelower-order n-gram.
Several different g functionscan be chosen, e.g.
the mean, weighted mean, prod-uct, minimum or maximum of the smoothed prob-ability distributions over all subsets of conditioningfactors.
In addition to different choices for g, dif-ferent discounting parameters can be selected at dif-ferent levels in the backoff graph.
One difficulty intraining FLMs is the choice of the best combinationof conditioning factors, backoff path(s) and smooth-ing options.
Since the space of different combina-tions is too large to be searched exhaustively, we usea guided search procedure based on Genetic Algo-rithms (Duh and Kirchhoff, 2004), which optimizesthe FLM structure with respect to the desired crite-rion.
In ASR, this is usually the perplexity of thelanguage model on a held-out dataset; here, we usethe BLEU scores of the oracle 1-best hypotheses onthe development set, as described below.
FLMs havepreviously shown significant improvements in per-plexity and word error rate on several ASR tasks(e.g.
(Vergyri et al, 2004)).3 Baseline SystemWe used a fairly simple baseline system trained us-ing standard tools, i.e.
GIZA++ (Och and Ney, 2000)for training word alignments and Pharaoh (Koehn,2004) for phrase-based decoding.
The training data126was that provided on the ACL05 Shared MT taskwebsite for 4 different language pairs (translationfrom Finnish, Spanish, French into English); noadditional data was used.
Preprocessing consistedof lowercasing the data and filtering out sentenceswith a length ratio greater than 9.
The total num-ber of training sentences and words per languagepair ranged between 11.3M words (Finnish-English)and 15.7M words (Spanish-English).
The develop-ment data consisted of the development sets pro-vided on the website (2000 sentences each).
Wetrained our own word alignments, phrase table, lan-guage model, and model combination weights.
Thelanguage model was a trigram model trained us-ing the SRILM toolkit, with modified Kneser-Neysmoothing and interpolation of higher- and lower-order ngrams.
Combination weights were trainedusing the minimum error weight optimization pro-cedure provided by Pharaoh.
We use a two-pass de-coding approach: in the first pass, Pharaoh is runin N-best mode to produce N-best lists with 2000hypotheses per sentence.
Seven different compo-nent model scores are collected from the outputs,including the distortion model score, the first-passlanguage model score, word and phrase penalties,and bidirectional phrase and word translation scores,as used in Pharaoh (Koehn, 2004).
In the secondpass, the N-best lists are rescored with additionallanguage models.
The resulting scores are then com-bined with the above scores in a log-linear fashion.The combination weights are optimized on the de-velopment set to maximize the BLEU score.
Theweighted combined scores are then used to selectthe final 1-best hypothesis.
The individual rescoringsteps are described in more detail below.4 Language ModelsWe trained two additional language models to beused in the second pass, one word-based 4-grammodel, and a factored trigram model.
Both weretrained on the same training set as the baseline sys-tem.
The 4-gram model uses modified Kneser-Ney smoothing and interpolation of higher-orderand lower-order n-gram probabilities.
The potentialadvantage of this model is that it models n-gramsup to length 4; since the BLEU score is a combina-tion of n-gram precision scores up to length 4, theintegration of a 4-gram language model might yieldbetter results.
Note that this can only be done in arescoring framework since the first-pass decoder canonly use a trigram language model.For the factored language models, a feature-basedword representation was obtained by tagging the textwith Rathnaparki?s maximum-entropy tagger (Rat-naparkhi, 1996) and by stemming words using thePorter stemmer (Porter, 1980).
Thus, the factoredlanguage models use two additional features perword.
A word history of up to 2 was considered (3-gram FLMs).
Rather than optimizing the FLMs onthe development set references, they were optimizedto achieve a low perplexity on the oracle 1-best hy-potheses (the hypotheses with the best individualBLEU scores) from the first decoding pass.
This isdone to avoid optimizing the model on word combi-nations that might never be hypothesized by the first-pass decoder, and to bias the model towards achiev-ing a high BLEU score.
Since N-best lists differ fordifferent language pairs, a separate FLM was trainedfor each language pair.
While both the 4-gram lan-guage model and the FLMs achieved a 8-10% reduc-tion in perplexity on the dev set references comparedto the baseline language model, their perplexities onthe oracle 1-best hypotheses were not significantlydifferent from that of the baseline model.5 N-best List RescoringFor N-best list rescoring, the original seven modelscores are combined with the scores of the second-pass language models using the framework of dis-criminative model combination (Beyerlein, 1998).This approach aims at an optimal (with respect toa given error criterion) integration of different infor-mation sources in a log-linear model, whose com-bination weights are trained discriminatively.
Thiscombination technique has been used successfullyin ASR, where weights are typically optimized tominimize the empirical word error count on a held-out set.
In this case, we use the BLEU score ofthe N-best hypothesis as an optimization criterion.Optimization is performed using a simplex downhillmethod known as amoeba search (Nelder and Mead,1965), which is available as part of the SRILMtoolkit.127Language pair 1st pass oracleFi-En 21.8 29.8Fr-En 28.9 34.4De-En 23.9 31.0Es-En 30.8 37.4Table 1: First-pass (left column) and oracle results(right column) on the dev set (% BLEU).Language pair 4-gram FLM bothFi-En 22.2 22.2 22.3Fr-En 30.2 30.2 30.4De-En 24.6 24.2 24.6Es-En 31.4 31.0 31.3Table 2: Second-pass rescoring results (% BLEU)on the dev set for 4-gram LM, 3-gram FLM, andtheir combination.6 ResultsThe results from the first decoding pass on the de-velopment set are shown in Table 1.
The secondcolumn in Table 1 lists the oracle BLEU scores forthe N-best lists, i.e.
the scores obtained by alwaysselecting the hypothesis known to have the highestindividual BLEU score.
We see that considerableimprovements can in principle be obtained by a bet-ter second-pass selection of hypotheses.
The lan-guage model rescoring results are shown in Table 2,for both types of second-pass language models indi-vidually, and for their combination.
In both cases weobtain small improvements in BLEU score, with the4-gram providing larger gains than the 3-gram FLM.Since their combination only yielded negligible ad-ditional improvements, only 4-grams were used forprocessing the final evaluation sets.
The evaluationresults are shown in Table 3.Language pair baseline 4-gramFi-En 21.6 22.0Fr-En 29.3 30.3De-En 24.2 24.8Es-En 30.5 31.0Table 3: Second-pass rescoring results (% BLEU)on the evaluation set.7 ConclusionsWe have demonstrated improvements in BLEUscore by utilizing more complex language modelsin the rescoring pass of a two-pass SMT system.We noticed that FLMs performed worse than word-based 4-gram models.
However, only trigram FLMwere used in the present experiments; larger im-provements might be obtained by 4-gram FLMs.The weights assigned to the second-pass languagemodels during weight optimization were larger thanthose assigned to the first-pass language model, sug-gesting that both the word-based model and the FLMprovide more useful scores than the baseline lan-guage model.
Finally, we observed that the overallimprovement represents only a small portion of thepossible increase in BLEU score as indicated by theoracle results, suggesting that better language mod-els do not have a significant effect on the overall sys-tem performance unless the translation model is im-proved as well.AcknowledgementsThis work was funded by the National ScienceFoundation, Grant no.
IIS-0308297.
We are grate-ful to Philip Koehn for assistance with Pharaoh.ReferencesP.
Beyerlein.
1998.
Discriminative model combination.
InProc.
ICASSP, pages 481?484.J.A.
Bilmes and K. Kirchhoff.
2003.
Factored language mod-els and generalized parallel backoff.
In Proceedings ofHLT/NAACL, pages 4?6.K.
Duh and K. Kirchhoff.
2004.
Automatic learning of lan-guage model structure.
In Proceedings of COLING.P.
Koehn.
2004.
Pharaoh: a beam search decoder for phrase-based statistical machine translation models.
In Proceedingsof AMTA.J.A.
Nelder and R. Mead.
1965.
A simplex method for functionminimization.
Computing Journal, 7(4):308?313.F.J.
Och and H. Ney.
2000.
Giza++: Training of sta-tistical translation models.
http://www-i6.informatik.rwth-aachen.de/ och/software/GIZA++.html.M.F.
Porter.
1980.
An algorithm for suffix stripping.
Program,14(3):130?137.A.
Ratnaparkhi.
1996.
A maximum entropy part-of-speech tag-ger.
In Proceedings EMNLP, pages 133?141.D.
Vergyri et al 2004.
Morphology-based language modelingfor Arabic speech recognition.
In Proceedings of ICSLP.128
