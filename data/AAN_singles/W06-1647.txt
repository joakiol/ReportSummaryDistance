Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 399?407,Sydney, July 2006. c?2006 Association for Computational LinguisticsLexicon Acquisition for Dialectal Arabic Using Transductive LearningKevin DuhDept.
of Electrical EngineeringUniversity of WashingtonSeattle, WA, USAduh@ee.washington.eduKatrin KirchhoffDept.
of Electrical EngineeringUniversity of WashingtonSeattle, WA, USAkatrin@ee.washington.eduAbstractWe investigate the problem of learn-ing a part-of-speech (POS) lexicon for aresource-poor language, dialectal Arabic.Developing a high-quality lexicon is oftenthe first step towards building a POS tag-ger, which is in turn the front-end to manyNLP systems.
We frame the lexicon ac-quisition problem as a transductive learn-ing problem, and perform comparisonson three transductive algorithms: Trans-ductive SVMs, Spectral Graph Transduc-ers, and a novel Transductive Clusteringmethod.
We demonstrate that lexiconlearning is an important task in resource-poor domains and leads to significant im-provements in tagging accuracy for dialec-tal Arabic.1 IntroductionDue to the rising importance of globalization andmultilingualism, there is a need to build natu-ral language processing (NLP) systems for an in-creasingly wider range of languages, includingthose languages that have traditionally not beenthe focus of NLP research.
The development ofNLP technologies for a new language is a chal-lenging task since one needs to deal not only withlanguage-specific phenomena but also with a po-tential lack of available resources (e.g.
lexicons,text, annotations).
In this study we investigate theproblem of learning a part-of-speech (POS) lexi-con for a resource-poor language, dialectal Arabic.Developing a high-quality POS lexicon is thefirst step towards training a POS tagger, which inturn is typically the front end for other NLP appli-cations such as parsing and language modeling.
Inthe case of resource-poor languages (and dialec-tal Arabic in particular), this step is much morecritical than is typically assumed: a lexicon withtoo few constraints on the possible POS tags fora given word can have disastrous effects on tag-ging accuracy.
Whereas such constraints can beobtained from large hand-labeled corpora or high-quality annotation tools in the case of resource-rich languages, no such resources are available fordialectal Arabic.
Instead, constraints on possiblePOS tags must be inferred from a small amountof tagged words, or imperfect analysis tools.
Thiscan be seen as the problem of learning complex,structured outputs (multi-class labels, with a dif-ferent number of classes for different words anddependencies among the individual labels) frompartially labeled data.Our focus is on investigating several machinelearning techniques for this problem.
In partic-ular, we argue that lexicon learning in resource-poor languages can be best viewed as transduc-tive learning.
The main contribution of this workare: (1) a comprehensive evaluation of three trans-ductive algorithms (Transductive SVM, SpectralGraph Transducer, and a new technique calledTransductive Clustering) as well as an inductiveSVM on this task; and (2) a demonstration thatlexicon learning is a worthwhile investment andleads to significant improvements in the taggingaccuracy for dialectal Arabic.The outline of the paper is as follows: Section 2describes the problem in more detail and discussesthe situation in dialectal Arabic.
The transductiveframework and algorithms for lexicon learning areelaborated in Section 3.
Sections 4 and 5 describethe data and system.
Experimental results are pre-sented in Section 6.
We discuss some related workin Section 7 before concluding in Section 8.3992 The Importance of Lexicons inResource-poor POS Tagging2.1 Unsupervised TaggingThe lack of annotated training data in resource-poor languages necessitates the use of unsuper-vised taggers.
One commonly-used unsuper-vised tagger is the Hidden Markov model (HMM),which models the joint distribution of a word se-quence w0:M and tag sequence t0:M as:P (t0:M , w0:M ) =M?i=0p(wi|ti)p(ti|ti?1, ti?2)(1)This is a trigram HMM.
Unsupervised learn-ing is performed by running the Expectation-Maximization (EM) algorithm on raw text.
In thisprocedure, the tag sequences are unknown, and theprobability tables p(wi|ti) and p(ti|ti?1, ti?2) areiteratively updated to maximize the likelihood ofthe observed word sequences.Although previous research in unsupervisedtagging have achieved high accuracies rivaling su-pervised methods (Kupiec, 1992; Brill, 1995),much of the success is due to the use of artifi-cially constrained lexicons.
Specifically, the lex-icon is a wordlist where each word is annotatedwith the set of all its possible tags.
(We will callthe set of possible tags of a given word the POS-set of that word; an example: POS-set of the En-glish word bank may be {NN,VB}.)
Banko andMoore (2004) showed that unsupervised tagger ac-curacies on English degrade from 96% to 77% ifthe lexicon is not constrained such that only highfrequency tags exist in the POS-set for each word.Why is the lexicon so critical in unsupervisedtagging?
The answer is that it provides addi-tional knowledge about word-tag distributions thatmay otherwise be difficult to glean from raw textalone.
In the case of unsupervised HMM taggers,the lexicon provides constraints on the probabilitytables p(wi|ti) and p(ti|ti?1, ti?2).
Specifically,the lexical probability table is initialized such thatp(wi|ti) = 0 if and only if tag ti is not included inthe POS-set of word wi.
The transition probabilitytable is initialized such that p(ti|ti?1, ti?2) = 0 ifand only if the tag sequence (ti, ti?1, ti?2) neveroccurs in the tag lattice induced by the lexicon onthe raw text.
The effect of these zero-probabilityinitialization is that they will always stay zerothroughout the EM procedure (modulo the effectsof smoothing).
This therefore acts as hard con-straints and biases the EM algorithm to avoid cer-tain solutions when maximizing likelihood.
If thelexicon is accurate, then the EM algorithm canlearn very good predictive distributions from rawtext only; conversely, if the lexicon is poor, EMwill be faced with more confusability during train-ing and may not produce a good tagger.
In general,the addition of rare tags, even if they are correct,creates a harder learning problem for EM.Thus, a critical aspect of resource-poor POStagging is the acquisition of a high-quality lexi-con.
This task is challenging because the lexiconlearning algorithm must not be resource-intensive.In practice, one may be able to find analysis toolsor incomplete annotations such that only a partiallexicon is available.
The focus is therefore on ef-fective machine learning algorithms for inferringa full high-quality lexicon from a partial, possiblynoisy initial lexicon.
We shall now discuss this sit-uation in the context of dialectal Arabic.2.2 Dialectal ArabicThe Arabic language consist of a collection ofspoken dialects and a standard written language(Modern Standard Arabic, or MSA).
The dialectsof Arabic are of considerable importance sincethey are used extensively in almost all everydayconversations.
NLP technology for dialectal Ara-bic is still in its infancy, however, due to the lackof data and resources.
Apart from small amountsof written dialectal material in e.g.
plays, novels,chat rooms, etc., data can only be obtained byrecording and manually transcribing actual con-versations.
Annotated corpora are scarce becauseannotation requires another stage of manual ef-fort beyond transcription work.
In addition, ba-sic resources such as lexicons, morphological an-alyzers, tokenizers, etc.
have been developed forMSA, but are virtually non-existent for dialectalArabic.In this study, we address lexicon learning forLevantine Colloquial Arabic.
We assume that onlytwo resources are available during training: (1)raw text transcriptions of Levantine speech and (2)a morphological analyzer developed for MSA.The lexicon learning task begins with a par-tial lexicon generated by applying the MSA ana-lyzer to the Levantine wordlist.
Since MSA dif-fers from Levantine considerably in terms of syn-tax, morphology, and lexical choice, not all Lev-antine words receive an analysis.
In our data,23% of the words are un-analyzable.
Thus, the400goal of lexicon learning is to infer the POS-setsof the un-analyzable words, given the partially-annotated lexicon and raw text.Details on the Levantine data and overall systemare provided in Sections 4 and 5.
We discuss thelearning algorithms in the next section.3 Learning Frameworks and AlgorithmsLet us formally define the lexicon learning prob-lem.
We have a wordlist of size m + u.
A portionof these words (m) are annotated with POS-set la-bels, which may be acquired by manual annotationor an automatic analysis tool.
The set of labeledwords {Xm} is the training set, also referred to asthe partial lexicon.
The task is to predict the POS-sets of the remaining u unlabeled words {Xu}, thetest set.
The goal of lexicon learning is to label{Xu} with low error.
The final result is a full lex-icon that contains POS-sets for all m + u words.3.1 Transductive Learning with StructuredOutputsWe argue that the above problem formulationlends itself to a transductive learning framework.Standard inductive learning uses a training set offully labeled samples in order to learn a classi-fication function.
After completion of the train-ing phase, the learned model is then used to clas-sify samples from a new, previously unseen testset.
Semi-supervised inductive learning exploitsunlabeled data in addition to labeled data to betterlearn a classification function.
Transductive learn-ing, first described by Vapnik (Vapnik, 1998) alsodescribes a setting where both labeled and unla-beled data are used jointly to decide on a label as-signment to the unlabeled data points.
However,the goal here is not to learn a general classifica-tion function that can be applied to new test setsmultiple times but to achieve a high-quality one-time labeling of a particular data set.
Transduc-tive learning and inductive semi-supervised learn-ing are sometimes confused in the literature.
Bothapproaches use unlabeled data in learning ?
thekey difference is that a transductive classifier onlyoptimizes the performance on the given unlabeleddata while an inductive semi-supervised classifieris trained to perform well on any new unlabeleddata.Lexicon learning fits in the transductive learn-ing framework as follows: The test set {Xu},i.e.
the unlabeled words, is static and known dur-NN?VB vs. ~NN?VBNN?JJ vs. ~NN?JJVB vs. ~VBNN vs. ~NNVB?JJ vs. ~VB?JJ..., etc.0.80.6?0.4?0.40.7 argmax NN?JJNN vs.~NNVB vs. ~VBJJ vs. ~JJsamplesampleK independent classifiers + 1 overall classifierSINGLE?LABEL FRAMWORKCOMPOUND?LABEL FRAMEWORK1 multi?class classifier(one?vs?rest implementation using N binary classifiers)0.9?0.8 {NN,JJ}0.1Classifier2nd StageFigure 1: Learning with Structured Outputs usingsingle or compound labelsing learning time; we are not interested in inferringPOS-sets for any words outside the word list.An additional characterization of the lexiconlearning problem is that it is a problem of learn-ing with complex, structured outputs.
The labelfor each word is its POS-set, which may containone to K POS tags (where K is the size of thetagset, K=20 in our case).
This differs from tra-ditional classification tasks where the output is asingle scalar variable.Structured output problems like lexicon learn-ing can be characterized by the granularity of thebasic unit of labels.
We define two cases: single-label and compound-label.
In the single-labelframework (see Figure 1), each individual POS tagis the target of classification and we have K binaryclassifiers each hypothesizing whether a word hasa POS tag k (k = 1, .
.
.
,K).
A second-stage clas-sifier takes the results of the K individual classi-fiers and outputs a POS-set.
This classifier cansimply take all POS tags hypothesized positive bythe individual binary classifiers to form the POS-set, or use a more sophisticated scheme for deter-mining the number of POS tags (Elisseeff and We-ston, 2002).The alternative compound-label frameworktreats each POS-set as an atomic label for clas-sification.
A POS-set such as {?NN?, ?VB?}
is?compounded?
into one label ?NN-VB?, which re-sults in a different label than, say, ?NN?
or ?NN-JJ?.
Suppose there exist N distinct POS-sets in the401training data; then we have N atomic units for la-beling.
Thus a (N -ary) multi-class classifier is em-ployed to directly predict the POS-set of a word.
Ifonly binary classifiers are available (i.e.
in the caseof Support Vector Machines), one can use one-vs-rest, pairwise, or error correcting code schemes toimplement the multi-class classification.The single-label framework is potentially ill-suited for capturing the dependencies betweenPOS tags.
Dependencies between POS tags arisesince some tags, such as ?NN?
and ?NNP?
can of-ten be tagged to the same word and therefore co-occur in the POS-set label.
The compound-labelframework implicitly captures tag co-occurrence,but potentially suffers from training data fragmen-tation as well as the inability to hypothesize POS-sets that do not already exist in the training data.In our initial experiments, the compound-labelframework gave better classification results; thuswe implemented all of our algorithms in the multi-class framework (using the one-vs-rest schemeand choosing the argmax as the final decision).3.2 Transductive ClusteringHow does a transductive algorithm effectively uti-lize unlabeled samples in the learning process?One popular approach is the application of the so-called cluster assumption, which intuitively statesthat samples close to each other (i.e.
samples thatform a cluster) should have similar labels.Transductive clustering (TC) is a simple algo-rithm that directly implements the cluster assump-tion.
The algorithm clusters labeled and unlabeledsamples jointly, then uses the labels of labeledsamples to infer the labels of unlabeled words inthe same cluster.
This idea is relatively straight-forward, yet what is needed is a principled wayof deciding the correct number of clusters and theprecise way of label transduction (e.g.
based onmajority vote vs. probability thresholds).
Typ-ically, such parameters are decided heuristically(e.g.
(Duh and Kirchhoff, 2005a)) or by tuning ona labeled development set; for resource-poor lan-guages, however, no such set may be available.As suggested by (El-Yaniv and Gerzon, 2005),the TC algorithm can utilize a theoretical errorbound as a principled way of determining the pa-rameters.
Let R?h(Xm) be the empirical risk of agiven hypothesis (i.e.
classifier) on the training set;let Rh(Xu) be the test risk.
(Derbeko et al, 2004)derive an error bound which states that, with prob-ability 1?
?, the risk on the test samples is boundedby:Rh(Xu) ?
R?h(Xm)+?
(m+uu)(u+1u)(ln 1p(h)+ln1?2m)(2)i.e.
the test risk is bounded by the empirical risk onthe labeled data, R?h(Xm), plus a term that varieswith the prior p(h) of the hypothesis or classifier.This is a PAC-Bayesian bound (McAllester, 1999).The prior p(h) indicates ones prior belief on thehypothesis h over the set of all possible hypothe-ses.
If the prior is low or the empirical risk is high,then the bound is large, implying that test risk maybe large.
A good hypothesis (i.e.
classifier) willideally have a small value for the bound, thus pre-dicting a small expected test risk.The PAC-Bayesian bound is important becauseit provides a theoretical guarantee on the qualityof a hypothesis.
Moreover, the bound in Eq.
2 isparticularly useful because it is easily computableon any hypothesis h, assuming that one is giventhe value of p(h).
Given two hypothesized label-ings of the test set, h1 and h2, the one with thelower PAC-Bayesian bound will achieve a lowerexpected test risk.
Therefore, one can use thebound as a principled way of choosing the pa-rameters in the Transductive Clustering algorithm:First, a large number of different clusterings is cre-ated; then the one that achieves the lowest PAC-Bayesian bound is chosen.
The pseudo-code isgiven in Figure 2.
(El-Yaniv and Gerzon, 2005) has applied theTransductive Clustering algorithm successfully tobinary classification problems and demonstratedimprovements over the current state-of-the-artSpectral Graph Transducers (Section 3.4).
We usethe algorithm as described in (Duh and Kirchhoff,2005b), which adapts the algorithm to structuredoutput problems.
In particular, the modificationinvolves a different estimate of the priors p(h),which was assumed to be uniform in (El-Yaniv andGerzon, 2005).
Since there are many possible h,adopting a uniform prior will lead to small valuesof p(h) and thus a loose bound for all h. Proba-bility mass should only be spent on POS-sets thatare possible, and as such, we calculate p(h) basedon frequencies of compound-labels in the trainingdata (i.e.
an empirical prior).3.3 Transductive SVMTransductive SVM (TSVM) (Joachims, 1999) isan algorithm that implicitly implements the cluster4021 For ?
= 2 : C (C is set arbitrarily to a large number)2 Apply a clustering algorithm to generate ?
clusters on Xm+u.3 Generate label hypothesis h?
(by labeling each cluster with the most frequent label among its labeled samples)4 Calculate the bound for h?
as defined in Eq.
2.5 Choose the hypothesis h?
with the lowest bound; output the corresponding classification of Xu.Figure 2: Pseudo-code for transductive clustering.assumption.
In standard inductive SVM (ISVM),the learning algorithm seeks to maximize the mar-gin subject to misclassification constraints on thetraining samples.
In TSVM, this optimization isgeneralized to include additional constraints onthe unlabeled samples.
The resulting optimiza-tion algorithm seeks to maximize the margin onboth labeled and unlabeled samples and creates ahyperplane that avoids high-density regions (e.g.clusters).3.4 Spectral Graph TransducerSpectral Graph Transducer (SGT) (Joachims,2003) achieves transduction via an extension ofthe normalized mincut clustering criterion.
First,a data graph is constructed where the vertices arelabeled or unlabeled samples and the edge weightsrepresent similarities between samples.
The min-cut criteria seeks to partition the graph such thatthe sum of cut edges is minimized.
SGT extendsthis idea to transductive learning by incorporatingconstraints that require samples of the same labelto be in the same cluster.
The resulting partitionsdecide the label of unlabeled samples.4 Data4.1 CorpusThe dialect addressed in this work is LevantineColloquial Arabic (LCA), primarily spoken in Jor-dan, Lebanon, Palestine, and Syria.
Our devel-opment/test data comes from the Levantine Ara-bic CTS Treebank provided by LDC.
The train-ing data comes from the Levantine CTS AudioTranscripts.
Both are from the Fisher collectionof conversational telephone speech between Lev-antine speakers previously unknown to each other.The LCA data was transcribed in standard MSAscript and transliterated into ASCII characters us-ing the Buckwalter transliteration scheme1.
No di-acritics are used in either the training or develop-ment/test data.
Speech effects such as disfluenciesand noises were removed prior to our experiments.1http://www.ldc.upenn.edu/myl/morph/buckwalter.htmlThe training set consists of 476k tokens and16.6k types.
It is not annotated with POS tags ?this is the raw text we use to train the unsuper-vised HMM tagger.
The test set consists of 15ktokens and 2.4k types, and is manually annotatedwith POS tags.
The development set is also POS-annotated, and contains 16k tokens and 2.4k types.We used the reduced tagset known as the Biestagset (Maamouri et al, 2004), which focuses onmajor part-of-speech and excludes detailed mor-phological information.Using the compound-label framework, weobserve 220 and 67 distinct compound-labels(i.e.
POS-sets) in the training and test sets, respec-tively.
As mentioned in Section 3.1, a classifierin the compound-label framework can never hy-pothesize POS-sets that do not exist in the trainingdata: 43% of the test vocabulary (and 8.5% by to-ken frequency) fall under this category.4.2 Morphological AnalyzerWe employ the LDC-distributed Buckwalter ana-lyzer for morphological analyses of Arabic words.For a given word, the analyzer outputs all possi-ble morphological analyses, including stems, POStags, and diacritizations.
The information regard-ing possible POS tags for a given word is crucialfor constraining the unsupervised learning processin HMM taggers.The Buckwalter analyzer is based on an internalstem lexicon combined with rules for affixation.
Itwas originally developed for the MSA, so only acertain percentage of Levantine words can be cor-rectly analyzed.
Table 1 shows the percentagesof words in the LCA training text that received Npossible POS tags from the Buckwalter analyzer.Roughly 23% of types and 28% of tokens receivedno tags (N=0) and are considered un-analyzable.5 SystemOur overall system looks as follows (see Figure3): In Step 1, the MSA (Buckwalter) analyzeris applied to the word list derived from the rawtraining text.
The result is a partial POS lexicon,403word2 JJ?NNword3 JJword4 ?word5 ?word1 NN?VBHMM TaggerFull POS LexiconPartial POS LexiconRAWTEXTBuckwalterAnalyzer (1)TransductiveLearning (2) Training (3)EMword2 JJ?NNword3 JJword1 NN?VBword4 NN?VBword5 JJFigure 3: Overall System: (1) Apply Buckwalter Analyzer to dialectal Arabic raw text, obtaining apartial POS lexicon.
(2) Use Transductive Learning to infer missing POS-sets.
(3) Unsupervised trainingof HMM Tagger using both raw text and inferred lexicon.N Type Token0 23.3 28.21 52.5 40.42 17.7 19.93 5.2 10.54 1.0 2.35 0.1 0.6Table 1: Percentage of word types/tokens with Npossible tags, as determined by the Buckwalter an-alyzer.
Words with 0 tags are un-analyzable.which lists the set of possible POS tags for thosewords for which the analyzer provided some out-put.
All possibilities suggested by the analyzer areincluded.The focus of Step 2 is to infer the POS-sets ofthe remaining, unannotated words using one of theautomatic learning procedures described in Sec-tion 3.
Finally, Step 3 involves training an HMMtagger using the learned lexicon.
This is the stan-dard unsupervised learning component of the sys-tem.
We use a trigram HMM, although modifica-tions such as the addition of affixes and variablesmodeling speech effects may improve tagging ac-curacy.
Our concern here is the evaluation of thelexicon learning component in Step 2.An important problem in this system setup isthe possibility of error propagation.
In Step 1, theMSA analyzer may give incorrect POS-sets to ana-lyzable words.
It may not posit the correct tag (lowrecall), or it may give too many tags (low preci-sion).
Both have a negative effect on lexicon learn-ing and EM training.
For lexicon learning, Step1 errors represent corrupt training data; For EMtraining, Step 1 error may cause the HMM taggerto never hypothesize the correct tag (low recall) orhave too much confusibility during training (lowprecision).
We attempted to measure the extent ofthis error by calculating the tag precision/recall onwords that occur in the test set: Among the 12kwords analyzed by the analyzer, 1483 words oc-cur in the test data.
We used the annotations inthe test data and collected all the ?oracle?
POS-sets for each of these 1483 words.2 The aver-age precision of the analyzer-generated POS-setsagainst the oracle is 56.46%.
The average recallis 81.25%.
Note that precision is low?this impliesthat the partial lexicon is not very constrained.
Therecall of 81.25% means that 18.75% of the wordsmay never receive the correct tag in tagging.
Inthe experiments, we will investigate to what ex-tent this kind of error affects lexicon learning andEM training.6 Experiments6.1 Lexicon learning experimentsWe seek to answer the following three questionsin our experiments:?
How useful is the lexicon learning step in anend-to-end POS tagging system?
Do the ma-chine learning algorithms produce lexiconsthat result in higher tagging accuracies, whencompared to a baseline lexicon that simplyhypothesizes all POS tags for un-analyzablewords?
The answer is a definitive yes.?
What machine learning algorithms performthe best on this task?
Do transductive learn-ing outperform inductive learning?
The em-pirical answer is that TSVM performs best,SGT performs worst, and TC and ISVM arein the middle.2Since the test set is small, these ?oracle?
POS-sets maybe missing some tags.
Thus the true precision may be higher(and recall may be lower) than measured.404Orthographic features:wi matches /?pre/, pre = {set of data-derived prefixes}wi matches /suf$/, suf = {set of data-derived suffixes}Contextual features:wi?1 = voc, voc = {set of words in lexicon}ti?1 = tag, tag = {set of POS tags}ti+1 = tag, tag = {set of POS tags}wi?1 is an un-analyzable wordwi+1 is an un-analyzable wordTable 2: Binary features used for predicting POS-sets of un-analyzable words.?
What is the relative impact of errors from theMSA analyzer on lexicon learning and EMtraining?
The answer is that Step 1 errors af-fect EM training more, and lexicon learningis comparably robust to these errors.In our problem, we have 12k labeled samplesand 3970 unlabeled samples.
We define the featureof each sample as listed in Table 2.
The contextualfeatures are generated by co-occurrence statisticsgleaned from the training data.
For instance, fora word foo, we collect all bigrams consisting offoo from the raw text; all features [wt?1 = voc]that correspond to the bigrams (voc, foo) are setto 1.
The idea is that words with similar ortho-graphic and/or contextual features should receivesimilar POS-sets.All results, unless otherwise noted, are taggingaccuracies on the test set given by training a HMMtagger on a specific lexicon.
Table 3 gives taggingaccuracies of the four machine learning methods(TSVM, TC, ISVM, SGT) as well as two base-line approaches for generating a lexicon: (all tags)gives all 20 possible tags to the un-analyzablewords, whereas (open class) gives only the sub-set of open-class POS tags.3 The results are givenin descending order of overall tagging accuracy.4With the exception of TSVM (63.54%) vs. TC(62.89%), all differences are statistically signifi-cant.
As seen in the table, applying a machinelearning step for lexicon learning is a worthwhileeffort since it always leads to better tagging accu-racies than the baseline methods.3Not all un-analyzable words are open-class.
Close-classwords may be un-analyzable due to dialectal spelling varia-tions.4Note that the unknown word accuracies do not followthe same trend and are generally quite low.
This might bedue to the fact that POS tags of unknown words are usuallybest predicted by the HMM?s transition probabilities, whichmay not be as robust due to the noisy lexicon.Method Accuracy UnkAccTSVM 63.54 26.19TC 62.89 26.71ISVM 61.53 27.68SGT 59.68 25.82open class 57.39 27.08all tags 55.64 25.00Table 3: Tagging Accuracies for lexicons derivedby machine learning (TSVM, TC, ISVM, SGT)and baseline methods.
Accuracy=Overall accu-racy; UnkAcc=Accuracy of unknown words.The poor performance of SGT is somewhat sur-prising since it is contrary to results presented inother papers.
We attributed this to the difficulty inconstructing the data graph.
For instance, we con-structed k-nearest-neighbor graphs based on thecosine distance between feature vectors, but it isdifficult to decide the best distance metric or num-ber of neighbors.
Finally, we note that besides theperformance of SGT, transductive learning meth-ods (TSVM, TC) outperform the inductive ISVM.We also compute precision/recall statistics ofthe final lexicon on the test set words (similar toSection 5) and measure the average size of thePOS-sets (?POSset?).
As seen in Table 4, POS-set sizes of machine-learned lexicon is a factor of2 or 3 smaller than that of the baseline lexicons.On the other hand, recall is better for the baselinelexicons.
These observations, combined with thefact that machine-learned lexicons gave better tag-ging accuracy, suggests that we have a constrainedlexicon effect here: i.e.
for EM training, it is betterto constrain the lexicon with small POS-sets thanto achieve high recall.Method Precision Recall ?POSset?TSVM 58.15 88.85 1.89TC 59.19 87.88 1.80ISVM 58.09 88.44 1.87SGT 53.98 82.60 1.87open class 54.03 96.77 3.39all tags 53.31 98.53 5.17Table 4: Statistics of the Lexicons in Table 3.Next, we examined the effects of error propa-gation from the MSA analyzer in Step 1.
We at-tempted to correct these errors by using POS-setsof words derived from the development data.
In405particular, of the 1562 partial lexicon words thatalso occur in the development set, we found 1044words without entirely matching POS-sets.
ThesePOS-sets are replaced with the oracle POS-sets de-rived from the development data, and the result istreated as the (corrected) partial lexicon of Step 1.In this procedure, the average POS-set size of thepartial lexicon decreased from 2.13 to 1.10, recallincreased from 82.44% to 100%, and precision in-creased from 57.15% to 64.31%.
We apply lexi-con learning to this corrected partial lexicon andevaluate tagging results, shown in Table 5.
Thefact that all numbers in Table 5 represent signifi-cant improvements over Table 3 implies that errorpropagation is not a trivial problem, and automaticerror correction methods may be desired.Method Accuracy UnkAccTSVM 66.54 27.38ISVM 65.08 26.86TC 64.05 28.20SGT 63.78 27.23all tags 62.96 27.91open class 61.26 27.83Table 5: Tag accuracies by correcting mistakes inthe partial lexicon prior to lexicon learning.
In-terestingly, we note ISVM outperforms TC here,which differs from Table 3.Finally, we determine whether error propaga-tion impacts lexicon learning (Step 2) or EM train-ing (Step 3) more.
Table 6 shows the results ofTSVM for four scenarios: correcting analyzer er-rors in the the lexicon: (A) prior to lexicon learn-ing, (B) prior to EM training, (C) both, or (D)none.
As seen in Table 6, correcting the lexiconat Step 3 (EM training) gives the most improve-ments, indicating that analyzer errors affects EMtraining more than lexicon learning.
This impliesthat lexicon learning is relatively robust to train-ing data corruption, and that one can mainly focuson improved estimation techniques for EM train-ing (Wang and Schuurmans, 2005) if the goal is toalleviate the impact of analyzer errors.
The sameevaluation on the other machine learning methods(TC, ISVM, SGT) show similar results.6.2 Comparison experiments: Expert lexiconand supervised learningOur approach to building a resource-poor POStagger involves (a) lexicon learning, and (b) un-Scenario Step2 Step3 TSVM(B) N Y 66.70(C) Y Y 66.54(A) Y N 64.93(D) N N 63.54Table 6: Effect of correcting the lexicon in differ-ent steps.
Y=yes, lexicon corrected; N=no, POS-set remains the same as analyzer?s output.supervised training.
In this section we examinecases where (a) an expert lexicon is available, sothat lexicon learning is not required, and (b) sen-tences are annotated with POS information, so thatsupervised training is possible.
The goal of theseexperiments is to determine when alternative ap-proaches involving additional human annotationsbecome worthwhile in this task.
(a) Expert lexicon: First, we build an expertlexicon by collecting all tags per word in the de-velopment set (i.e.
?oracle?
POS-sets).
Then, thetagger is trained using EM by treating the develop-ment set as raw text (i.e.
ignoring the POS anno-tations).
This achieves an accuracy of 74.45% onthe test set.
Note that this accuracy is significantlyhigher than the ones in Table 3, which representunsupervised training on more raw text (the train-ing set), but with non-expert lexicons derived fromthe MSA analyzer and a machine learner.
This re-sult further demonstrates the importance of obtain-ing an accurate lexicon in unsupervised training.
Ifone were to build this expert lexicon by hand, onewould need an annotator to label the POS-sets of2450 distinct lexicon items.
(b) Supervised training: We build a super-vised tagger by training on the POS annotations ofthe development set, which achieves 82.93% accu-racy.
This improved accuracy comes at the cost ofannotating 2.2k sentences (16k tokens) with com-plete POS information.Finally, we present the same results with re-duced data, taking first 50, 100, 200, etc.
sen-tences in the development set for lexicon or POSannotation.
The learning curve is shown in Table7.
One may be tempted to draw conclusions re-garding supervised vs. unsupervised approachesby directly comparing this table with the resultsin Section 6.1; we avoid doing so since taggers inSections 6.1 and 6.2 are trained on different datasets (training vs. development set) and the accu-racy differences are compounded by issues such406Supervised Unsupervised, Expert#Sentence Acc #Vocab Acc50 47.82 123 47.13100 55.32 188 54.65200 61.17 299 57.37400 69.17 497 64.36800 76.92 953 70.361600 81.73 1754 72.992200 82.93 2450 74.45Table 7: (1) Supervised training accuracies withvarying numbers of sentences.
(2) Accuracies ofunsupervised training using a expert lexicon ofdifferent vocabulary sizes.as ngram coverage, data-set selection, and the wayannotations are done.7 Related WorkThere is an increasing amount of work in NLPtools for Arabic.
In supervised POS tagging, (Diabet al, 2004) achieves high accuracy on MSA withthe direct application of SVM classifiers.
(Habashand Rambow, 2005) argue that the rich morphol-ogy of Arabic necessitates the use of a morpho-logical analyzer in combination with POS tag-ging.
This can be considered similar in spirit tothe learning of lexicons for unsupervised tagging.The work done at a recent JHU Workshop(Rambow and others, 2005) is very relevant in thatit investigates a method for improving LCA tag-ging that is orthogonal to our approach.
They donot use the raw LCA text as we have done.
Instead,they train a MSA supervised tagger and adapt it toLCA by a combination of methods, such using aMSA-LCA translation lexicon and redistributingthe probabibility mass of MSA words to LCA.8 ConclusionIn this study, we investigated several machinelearning algorithms on the task of lexicon learn-ing and demonstrated its impact on dialectal Ara-bic tagging.
We achieve a POS tagging accuracyof 63.54% using a transductively-learned lexicon(TSVM), outperforming the baseline (57.39%).This result brings us one step closer to the accu-racies of unsupervised training with expert lexi-con (74.45%) and supervised training (82.93%),both of which require significant annotation effort.Future work includes a more detailed analysis oftransductive learning in this domain and possiblesolutions to alleviating error propagation.AcknowledgmentsWe would like to thank Rebecca Hwa for discussions regard-ing the JHU project.
This work is funded in part by NSFGrant IIS-0326276 and an NSF Graduate Fellowship for the1st author.
Any opinions, findings, and conclusions expressedin this material are those of the authors and do not necessarilyreflect the views of these agencies.ReferencesM.
Banko and R. Moore.
2004.
Part-of-speech tagging incontext.
In Proc.
of COLING 2004.E.
Brill.
1995.
Unsupervised learning of disambiguationrules for part of speech tagging.
In Proc.
of the ThirdWorkshop on Very Large Corpora.P.
Derbeko, R. El-Yaniv, and R. Meir.
2004.
Explicit learningcurves for transduction and application to clustering andcompression algorithms.
Journal of Artificial IntelligenceResearch, 22:117-142.M.
Diab, K. Hacioglu, and D. Jurafsky.
2004.
Automatic tag-ging of Arabic text: from raw text to base phrase chunks.In Proceedings of HLT/NAACL.K.
Duh and K. Kirchhoff.
2005a.
POS tagging of dialectalarabic: a minimally-supervised approach.
In ACL 2005,Semitic Languages Workshop.K.
Duh and K. Kirchhoff.
2005b.
Structured multi-labeltransductive learning.
In NIPS Workshop on Advances inStructured Learning for Text/Speech Processing.R.
El-Yaniv and L. Gerzon.
2005.
Effective transductivelearning via objective model selection.
Pattern Recogni-tion Letters, 26(13):2104-2115.A.
Elisseeff and J. Weston.
2002.
Kernel methods for multi-labeled classification.
In NIPS.N.
Habash and O. Rambow.
2005.
Arabic tokenization, mor-phological analysis, and part-of-speech tagging in one fellswoop.
In ACL.T.
Joachims.
1999.
Transductive inference for text classifi-cation using support vector machines.
In ICML.T.
Joachims.
2003.
Transductive learning via spectral graphpartitioning.
In ICML.J.
Kupiec.
1992.
Robust part-of-speech tagging using a hid-den Markov model.
Computer Speech and Language, 6.M.
Maamouri, A. Bies, and T. Buckwalter.
2004.
The PennArabic Treebank: Building a large-scale annotated Arabiccorpus.
In NEMLAR Conf.
on Arabic Language Resourcesand Tools.D.
McAllester.
1999.
Some PAC-Bayesian theorems.
Ma-chine Learning, 37(3):255-36.O.
Rambow et al 2005.
Parsing Arabic dialects.
Technicalreport, Final Report, 2005 JHU Summer Workshop.V.
Vapnik.
1998.
Statistical Learning Theory.
Wiley Inter-science.Q.
Wang and D. Schuurmans.
2005.
Improved estimation forunsupervised part-of-speech tagging.
In IEEE NLP-KE.407
