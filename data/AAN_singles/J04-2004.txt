c?
2004 Association for Computational LinguisticsMachine Translation with InferredStochastic Finite-State TransducersFrancisco Casacuberta?
Enrique Vidal?Universidad Polite?cnica de Valencia Universidad Polite?cnica de ValenciaFinite-state transducers are models that are being used in different areas of pattern recognition andcomputational linguistics.
One of these areas is machine translation, in which the approaches thatare based on building models automatically from training examples are becoming more and moreattractive.
Finite-state transducers are very adequate for use in constrained tasks in which trainingsamples of pairs of sentences are available.
A technique for inferring finite-state transducers isproposed in this article.
This technique is based on formal relations between finite-state transducersand rational grammars.
Given a training corpus of source-target pairs of sentences, the proposedapproach uses statistical alignment methods to produce a set of conventional strings from whicha stochastic rational grammar (e.g., an n-gram) is inferred.
This grammar is finally convertedinto a finite-state transducer.
The proposed methods are assessed through a series of machinetranslation experiments within the framework of the EuTrans project.1.
IntroductionFormal transducers give rise to an important framework in syntactic-pattern recogni-tion (Fu 1982; Vidal, Casacuberta, and Garc?
?a 1995) and in language processing (Mohri1997).
Many tasks in automatic speech recognition can be viewed as simple translationsfrom acoustic sequences to sublexical or lexical sequences (acoustic-phonetic decod-ing) or from acoustic or lexical sequences to query strings (for database access) or(robot control) commands (semantic decoding) (Vidal, Casacuberta, and Garc?
?a 1995;Vidal 1997; Bangalore and Ricardi 2000a, 2000b; Hazen, Hetherington, and Park 2001;Mou, Seneff, and Zue 2001; Segarra et al 2001; Seward 2001).Another similar application is the recognition of continuous hand-written char-acters (Gonza?lez et al 2000).
Yet a more complex application of formal transducersis language translation, in which input and output can be text, speech, (continuous)handwritten text, etc.
(Mohri 1997; Vidal 1997; Bangalore and Ricardi 2000b, 2001;Amengual et al 2000).Rational transductions (Berstel 1979) constitute an important class within the fieldof formal translation.
These transductions are realized by the so-called finite-statetransducers.
Even though other, more powerful transduction models exist, finite-statetransducers generally entail much more affordable computational costs, thereby mak-ing these simpler models more interesting in practice.One of the main reasons for the interest in finite-state machines for languagetranslation comes from the fact that these machines can be learned automatically fromexamples (Vidal, Casacuberta, and Garc?
?a 1995).
Nowadays, only a few techniquesexist for inferring finite-state transducers (Vidal, Garc?
?a, and Segarra 1989; Oncina,?
Departamento de Sistemas Informa?ticos y Computacio?n, Instituto Tecnolo?gico de Informa?tica, 46071Valencia, Spain.
E-mail:{fcn, evidal}@iti.upv.es.206Computational Linguistics Volume 30, Number 2Garc?
?a, and Vidal 1993; Ma?kinen 1999; Knight and Al-Onaizan 1998; Bangalore andRicardi 2000b; Casacuberta 2000; Vilar 2000).
Nevertheless, there are many techniquesfor inferring regular grammars from finite sets of learning strings which have beenused successfully in a number of fields, including automatic speech recognition (Vi-dal, Casacuberta, and Garc?
?a 1995).
Some of these techniques are based on results fromformal language theory.
In particular, complex regular grammars can be built by infer-ring simple grammars that recognize local languages (Garc?
?a, Vidal, and Casacuberta1987).Here we explore this idea further and propose methods that use (simple) finite-state grammar learning techniques, such as n-gram modeling, to infer rational trans-ducers which prove adequate for language translation.The organization of the article is as follows.
Sections 2 and 3 give the basic defini-tions of a finite-state transducer and the corresponding stochastic extension, presentedwithin the statistical framework of language translation.
In Section 4, the proposedmethod for inferring stochastic finite-state transducers is presented.
The experimentsare described in Section 5.
Finally, Section 6 is devoted to general discussion andconclusions.2.
Finite-State TransducersA finite-state transducer, T , is a tuple ?
?, ?, Q, q0, F, ?
?, in which ?
is a finite set ofsource symbols, ?
is a finite set of target symbols (?
?
?
= ?
), Q is a finite set ofstates, q0 is the initial state, F ?
Q is a set of final states, and ?
?
Q ?
?
?
? ?
Q isa set of transitions.1 A translation form ?
of length I in T is defined as a sequence oftransitions:?
= (q?0 , s?1 , t?
?1 , q?1 )(q?1 , s?2 , t?
?2 , q?2 )(q?2 , s?3 , t?
?3 , q?3 ) .
.
.
(q?I?1, s?I , t?
?I , q?I ) (1)where (q?i?1, s?i , t?
?i , q?i ) ?
?, q?0 = q0, and q?I ?
F. A pair (s, t) ?
? ?
? is a translationpair if there is a translation form ?
of length I in T such that I =|s | and t = t?
?1 t?
?2 .
.
.
t?
?I .By d(s, t) we will denote the set of translation forms2 in T associated with the pair (s, t).A rational translation is the set of all translation pairs of some finite-state transducer T .This definition of a finite-state transducer is similar to the definition of a regularor finite-state grammar G. The main difference is that in a finite-state grammar, the setof target symbols ?
does not exist, and the transitions are defined on Q ?
?
?
Q. Atranslation form is the transducer counterpart of a derivation in a finite-state gram-mar, and the concept of rational translation is reminiscent of the concept of (regular)language, defined as the set of strings associated with the derivations in the grammarG.Rational translations exhibit many properties similar to those shown for regularlanguages (Berstel 1979).
One of these properties can be stated as follows (Berstel 1979):Theorem 1T ?
? ?
? is a rational translation if and only if there exist an alphabet ?, a regular languageL ?
?, and two morphisms h?
: ? ?
? and h?
: ? ?
?, such that T = {(h?
(w), h?
(w)) |w ?
L}.1 By ??
and ?
?, we denote the set of finite-length strings on ?
and ?, respectively.2 To simplify the notation, we will remove the superscript ?
from the components of a translation formif no confusion is induced.207Casacuberta and Vidal Translation with Finite-State TransducersAs will be discussed later, this theorem directly suggests the transducer inferencemethods proposed in this article.3.
Statistical Translation Using Finite-State TransducersIn the statistical translation framework, the translation of a given source string s in? is a string t?
?
?, such that3t?
= argmaxt?
?Pr(t | s) = argmaxt?
?Pr(s, t) (2)Pr(s, t) can be modeled by the stochastic extension of a finite-state transducer.
Astochastic finite-state transducer, TP, is defined as a tuple ?
?,?, Q, q0, p, f ?, in whichQ, q0, Q,?, and ?
are as in the definition of a finite-state transducer and p and f aretwo functions p : Q ?
??
? ?
Q ?
[0, 1] and f : Q ?
[0, 1] that satisfy, ?q ?
Q,f (q) +?(a,?,q?)???
??Qp(q, a,?, q?)
= 1In this context, T will denote the natural finite-state transducer associated with astochastic finite-state transducer TP (characteristic finite-state transducer).
The set oftransitions of T is the set of tuples (q, s, t, q?)
in TP with probabilities greater than zero,and the set of final states is the set of states with nonzero final-state probabilities.The probability of a translation pair (s, t) ?
? ?
? according to TP is the sumof the probabilities of all the translation forms of (s, t) in T :PTP(s, t) =???d(s,t)PTP(?
)where the probability of a translation form ?
(as defined in equation (1)) isPTP(?)
=I?i=0p(qi?1, si, t?i, qi) ?
f (qI) (3)that is, the product of the probabilities of all the transitions involved in ?.We are interested only in transducers without useless states, that is, those in whichfor every state in T , there is a path leading to a final state.
If we further assume thatPTP(s, t) is zero when no translation form exists for (s, t) in T , it can be easily verifiedthat?(s,t)???
?PTP(s, t) = 1That is, PTP is a joint distribution on ? ?
? which will be called the stochastictranslation defined by TP.
4Finally, the translation of a source string s ?
? by a stochastic finite-state trans-ducer TP ist?
= argmaxt?
?PTP(s, t) (4)3 For the sake of simplicity, we will denote Pr(X = x) as Pr(x) and Pr(X = x | Y = y) as Pr(x | y).4 This concept is similar to the stochastic regular language for a stochastic regular grammar.
In thatcase, the probability distribution is defined on the set of finite-length strings rather than on the set ofpairs of strings.208Computational Linguistics Volume 30, Number 2A stochastic finite-state transducer has stochastic source and target regular languagesembedded (Pi and Po, respectively.
):Pi(s) =?t?
?PTP(s, t), Po(t) =?s?
?PTP(s, t)In practice, these source or target regular languages are obtained, by dropping thetarget or the source symbols, respectively, from each transition of the finite-state trans-ducer.The following theorem naturally extends Theorem 1 to the stochastic framework(Casacuberta, Vidal, and Pico?
2004):Theorem 2A distribution PT : ? ?
? ?
[0, 1] is a stochastic rational translation if and only if thereexist an alphabet ?, two morphisms h?
: ? ?
? and h?
: ? ?
?, and a stochastic regularlanguage PL such that, ?
(s, t) ?
? ?
?,PT(s, t) =??
?
? :(h?(?
), h?(?))
= (s, t)PL(?)
(5)3.1 Search with Stochastic Finite-State TransducersThe search for an optimal t?
in Equation (4) has proved to be a difficult computationalproblem (Casacuberta and de la Higuera 2000).
In practice, an approximate solutioncan be obtained (Casacuberta 2000) on the basis of the following approximation to theprobability of a translation pair (Viterbi score of a translation):PTP(s, t) ?
VTP(s, t) = max??d(s,t)PTP(?)
(6)An approximate translation can now be computed ast?
= argmaxt?
?VTP(s, t) = argmaxt??max??d(s,t)PTP(?)
(7)This computation can be carried out efficiently (Casacuberta 1996) by solving thefollowing recurrence by means of dynamic programming:maxt??
?VTP(s, t) = maxq?Q(V(|s|, q) ?
f (q))(8)V(i, q) = maxq??Q,w?
?(V(i ?
1, q?)
?
p(q?, si, w, q))if i = 0, q = q0 (9)V(0, q0) = 1 (10)Finally, the approximate translation t?
is obtained as the concatenation of the targetstrings associated with the translation form??
= (q0, s1, t?1, q1)(q1, s2, t?2, q2) .
.
.
(qI?1, sI?1, t?I, qI),corresponding to the optimal sequence of states involved in the solution to Equa-tion (8); that is,t?
= t?1 t?2 .
.
.
t?I209Casacuberta and Vidal Translation with Finite-State Transducers0 1una / a  (1.0)2camera / double (0.3)4camera / ?
(0.3)6camera / room (0.4)3doppia / room (1.0)5doppia / double room (1.0)7doppia / with two beds (1.0)Figure 1Example of Viterbi score-based suboptimal result.
The probability PTP of the pair una cameradoppia/a double room is (1.0 ?
0.3 ?
1.0) + (1.0 ?
0.3 ?
1.0) = 0.6.
This is greater than the probabilityPTP of the pair una camera doppia/a room with two beds, 1.0 ?
0.4 ?
1.0 = 0.4.
However, the Viterbiscore VTP for the first pair is 1.0 ?
0.3 ?
1.0 = 0.3, which is lower than the Viterbi score VTP forthe second pair, 1.0 ?
0.4 ?
1.0 = 0.4.
Therefore this second pair will be the approximate resultgiven by equation (7).The computational cost of the iterative version of this algorithm is O(| s | ?
|Q| ?
B),where B is the (average) branching factor of the finite-state transducer.Figure 1 shows a simple example in which Viterbi score maximization (7) leads toa suboptimal result.4.
A Method for Inferring Finite-State TransducersTheorems 1 and 2 establish that any (stochastic) rational translation T can be obtainedas a homomorphic image of certain (stochastic) regular language L over an adequatealphabet ?.
The proofs of these theorems are constructive (Berstel 1979; Casacuberta,Vidal, and Pico?
2004) and are based on building a (stochastic) finite-state transducer Tfor T by applying certain morphisms h?
and h?
to the symbols of ?
that are associatedwith the rules of a (stochastic) regular grammar that generates L.This suggests the following general technique for learning a stochastic finite-statetransducer, given a finite sample A of string pairs (s, t) ?
??
? ( a parallel corpus):1.
Each training pair (s, t) from A is transformed into a string z from anextended alphabet ?
(strings of ?-symbols) yielding a sample S ofstrings S ?
?.2.
A (stochastic) regular grammar G is inferred from S.3.
The ?-symbols of the grammar rules are transformed back into pairs ofsource/target symbols/strings (from ? ?
?).This technique, which is very similar to that proposed in Garc?
?a, Vidal, and Casacu-berta (1987) for the inference of regular grammars, is illustrated in Figure 2.The first transformation is modeled by the labeling function L : ? ?
? ?
?,while the last transformation is carried out by an ?inverse labeling function?
?(?
), thatis, one such that ?
(L(A)) = A.
Following Theorems 1 and 2, ?(?)
consists of a coupleof morphisms, h?, h?, such that for a string z ?
?, ?
(z) = (h?
(z), h?
(z)).Without loss of generality, we assume that the method used in the second step ofthe proposed method consists of the inference of n-grams (Ney, Martin, and Wessel1997) with final states, which are particular cases of stochastic regular grammars.
Thissimple method automatically derives, from the strings in S, both the structure of G(i.e., the rules?states and transitions) and the associated probabilities.Since ?
is typically the inverse of L, the morphisms h?
and h?
needed in thethird step of the proposed approach are determined by the definition of L. So a key210Computational Linguistics Volume 30, Number 2Figure 2Basic scheme for the inference of finite-state transducers.
A is a finite sample of training pairs.S is the finite sample of strings obtained from A using L. G is a grammar inferred from S suchthat S is a subset of the language, L(G), generated by the grammar G. T is a finite-statetransducer whose translation (T(T )) includes the training sample A.point in this approach is its first step, that is, how to conveniently transform a parallelcorpus into a string corpus.
In general, there are many possible transformations, butif the source?target correspondences are complex, the design of an adequate transfor-mation can become difficult.
As a general rule, the labeling process must capture thesesource?target word correspondences and must allow for a simple implementation ofthe inverse labeling needed in the third step.A very preliminary, nonstochastic version of this finite-state transducer inferencetechnique was presented in Vidal, Garc?
?a, and Segarra (1989) An important drawbackof that early proposal was that the methods proposed for building the ? sentencesfrom the training pairs did not adequately cope with the dependencies between thewords of the source sentences and the words of the corresponding target sentences.
Inthe following section we show how this drawback can be overcome using statisticalalignments (Brown et al 1993).The resulting methodology is called grammatical inference and alignments fortransducer inference (GIATI).5 A related approach was proposed in Bangalore andRicardi (2000b).
In that case, the extended symbols were also built according to pre-viously computed alignments, but the order of target words was not preserved.
As aconsequence, that approach requires a postprocess to try to restore the target wordsto a proper order.4.1 Statistical AlignmentsThe statistical translation models introduced by Brown et al (1993) are based on theconcept of alignment between source and target words (statistical alignment mod-els).
Formally, an alignment of a translation pair (s, t) ?
? ?
? is a functiona : {1, .
.
.
, |t|} ?
{0, .
.
.
, | s |}.
The particular case a(j) = 0 means that the position jin t is not aligned with any position in s. All the possible alignments between t ands are denoted by A(s, t), and the probability of translating a given s into t by analignment a is Pr(t, a | s).Thus, an optimal alignment between s and t can be computed asa?
= argmaxa?A(s,t)Pr(t, a | s) (11)5 In previous work, this idea was often called morphic generator transducer inference.211Casacuberta and Vidal Translation with Finite-State TransducersDifferent approaches for estimating Pr(t, a | s) were proposed in Brown et al (1993).These approaches are known as models 1 through 5.
Adequate software packages arepublicly available for training these statistical models and for obtaining good align-ments between pairs of sentences (Al-Onaizan et al 1999; Och and Ney 2000).
Anexample of Spanish-English sentence alignment is given below:Example 1?
Cua?nto cuesta una habitacio?n individual por semana ?how (2) much (2) does (3) a (4) single (6) room (5) cost (3) per (7) week (8) ?
(9)Each number within parentheses in the example represents the position in the sourcesentence that is aligned with the (position of the) preceding target word.
A graphicalrepresentation of this alignment is shown in Figure 3.4.2 First Step of the GIATI Methodology: Transformation of Training Pairs intoStringsThe first step of the proposed method consists in a labeling process (L) that builds astring of certain extended symbols from each training string pair and its correspondingstatistical alignment.
The main idea is to assign each word from t to the correspondingword from s given by the alignment a.
But sometimes this assignment produces aviolation of the sequential order of the words in t. To illustrate the GIATI methodologywe will use example 2:Figure 3Graphical representation of the alignment between a source (Spanish) sentence (?
Cua?nto cuestauna habitacio?n individual por semana ?)
and a target (English) sentence (How much does a singleroom cost per week ?).
Note the correspondence between the Spanish cuesta and the English doesand cost.
Note also that the model does not allow for alignments between sets of two or moresource words and one target word.212Computational Linguistics Volume 30, Number 2Example 2Let A be a training sample composed by the following pairs (Italian/English):una camera doppia # a double roomuna camera # a roomla camera singola # the single roomla camera # the roomSuitable alignments for these pairs areuna camera doppia # a (1) double (3) room (2)una camera # a (1) room (2)la camera singola # the (1) single (3) room (2)la camera # the (1) room (2)In the first pair of this example, the English word double could be assigned tothe third Italian word (doppia) and the English word room to the second Italian word(camera).
This would imply a ?reordering?
of the words double and room, which is notappropriate in our finite-state framework.Given s, t, and a (source and target strings and associated alignment, respectively),the proposed transformation z = L1(s, t) avoids this problem as follows:| z | = | s |1 ?
i ?
| z |zi =???
(si , tj tj+1 .
.
.
tj+l) if ?j : a(j) = i and ?| j?
< j : a(j?)
> a(j)and for j??
: j ?
j??
?
j + l, a(j??)
?
a(j)(si , ?)
otherwiseEach word from t is joined with the corresponding word from s given by the alignmenta if the target word order is not violated.
Otherwise, the target word is joined withthe first source word that does not violate the target word order.The application of L1 to example 2 generates the following strings of extendedsymbols:(una , a) (camera , ?)
(doppia , double room)(una , a) (camera , room)(la , the) (camera , ?)
(singola , single room)(la , the) (camera , room)As a more complicated example, the application of this transformation to example 1generates the following string:(?
, ?)
(Cua?nto , how much) (cuesta , does) (una , a) (habitacio?n , ?
)(individual , single room cost) (por , per) (semana , week) (?
, ?
)In this case the unaligned token ?
has an associated empty target string, and thetarget word cost, which is aligned with the source word cuesta, is associated with thenearby source word individual.
This avoids a ?reordering?
of the target string andentails an (apparently) lower degree of nonmonotonicity.
This is achieved, however,at the expense of letting the method generalize from word associations which can beconsidered improper from a linguistic point of view (e.g., (cuesta, does), (individual, single213Casacuberta and Vidal Translation with Finite-State Transducersroom cost)).
While this would certainly be problematic for general language translation,it proves not to be so harmful when the sentences to be translated come from limited-domain languages.Obviously, other transformations are possible.
For example, after the application ofthe above procedure, successive isolated source words (without any target word) canbe joined to the first extended word which has target word(s) assigned.
Let z = L1(s, t)be a transformed string obtained from the above procedure and let(sk?1 , tj tj+1 .
.
.
tj+m)(sk , ?)
.
.
.
(sk+l?1 , ?
)(sk+l , tj+m+1 .
.
.
tj+n)be a subsequence within z.
Then the subsequence(sk , ?)
.
.
.
(sk+l?1 , ?
)(sk+l , tj+m+1 .
.
.
tj+n)is transformed by L2 into(sk .
.
.
sk+l?1 sk+l , tj+m+1 .
.
.
tj+n)The application of L2 to example 2 leads to(una , a) (camera doppia , double room)(una , a) (camera , room)(la , the) (camera singola , single room)(la , the) (camera , room)Although many other sophisticated transformations can be defined following theabove ideas, only the simple L1 will be used in the experiments reported in this article.4.3 Second Step of the GIATI Methodology: Inferring a Stochastic Regular Grammarfrom a Set of StringsMany grammatical inference techniques are available to implement the second stepof the proposed procedure.
In this work, (smoothed) n-grams are used.
These modelshave proven quite successful in many areas such as language modeling (Clarkson andRosenfeld 1997; Ney, Martin, and Wessel 1997).Figures 4 and 5 show the (nonsmoothed) bigram models inferred from the sampleobtained using L1 and L2, respectively, in example 2.
Note that the generalizationachieved by the first model is greater than that of the second.The probabilities of the n-grams are computed from the corresponding counts inthe training set of extended strings.
The probability of an extended word zj = (si, t?i)given the sequence of extended words zi?n+1, .
.
.
, zi?1 = (si?n+1, t?i?n+1) .
.
.
(si?1, t?i?1)01(una , a)4(la , the)2(camera , ?
)5(camera , room)3(doppia , double room)6(singola , single room)(camera , ?
)(camera , room)Figure 4Bigram model inferred from strings obtained by the transformation L1 in example 2.214Computational Linguistics Volume 30, Number 201(una , a)4(la , the)2(camera doppia , double room)3(camera , room)(camera , room)5(camera singola , single room)Figure 5Bigram model inferred from strings obtained by the transformation L2 in example 2.is estimated aspn(zi | zi?n+1 .
.
.
zi?1) =c(zi?n+1, .
.
.
, zi?1, zi)c(zi?n+1, .
.
.
, zi?1)(12)where c(?)
is the number of times that an event occurs in the training set.
To deal withunseen n-grams, the back-off smoothing technique from the CMU Statistical LanguageModeling (SLM) Toolkit (Rosenfeld 1995) has been used.The (smoothed) n-gram model obtained from the set of extended symbols is repre-sented as a stochastic finite-state automaton (Llorens, Vilar, and Casacuberta 2002).
Thestates of the automaton are the observed (n ?
1)-grams.
For the n-gram (zi?n+1 .
.
.
zi),there is a transition from state (zi?n+1 .
.
.
zi?1) to state (zi?n+2 .
.
.
zi) with the associ-ated extended word zi and a probability pn(zi | zi?n+1 .
.
.
zi?1).
The back-off smoothingmethod supplied by the SLM Toolkit is represented by the states corresponding to k-grams (k < n) and by special transitions between k-gram states and (k ?
1)-gramstates (Llorens, Vilar, and Casacuberta 2002).
The final-state probability is computedas the probability of a transition with an end-of-sentence mark.4.4 Third Step of the GIATI Methodology: Transforming a Stochastic Regular Gram-mar into a Stochastic Finite-State TransducerIn order to obtain a finite-state transducer from a grammar of L1?transformed symbols,an ?inverse transformation?
?(?)
is used which is based on two simple morphisms:if (a, b1b2 .
.
.
bk) ?
?
with a ?
?
and b1, b2, .
.
.
, bk ?
?,h?
((a, b1b2 .
.
.
bk)) = ah?
((a, b1b2 .
.
.
bk)) = b1 b2 .
.
.
bkIt can be verified that this constitutes a true inverse transformation; that is, for everytraining pair ?
(s, t) ?
As = h?
(L1(s, t)), t = h?
(L1(s, t))If zi is a transition of the inferred regular grammar, where zi = (a, b1b2 .
.
.
bk) ?
?, thecorresponding transition of the resulting finite-state transducer is (q, a, b1b2 ?
?
?
bk, q?
).This construction is illustrated in Figures 6 and 7 for the bigrams of Figures 4 and 5,respectively.
Note that in the second case, this construction entails the trivial addition ofa few states which did not exist in the corresponding bigram.
As previously discussed,the first transformation (L1) definitely leads to a greater translation generalization thanthe second (L2) (Casacuberta, Vidal, and Pico?
2004).
The probabilities associated with215Casacuberta and Vidal Translation with Finite-State Transducers01una / a4la / the2camera / ?5camera / room3doppia / double room6singola / single roomcamera / ?camera / roomFigure 6A finite-state transducer built from the n-gram of Figure 4.01una / a4la / thecamera  / ?3camera / room2doppia / double roomcamera / roomcamera  / ?5singola / single roomFigure 7A finite-state transducer built from the n-gram of Figure 5.the transitions and the final states of the finite-state transducer are the same as thoseof the original stochastic regular grammar.Since we are using n-grams in the second step, a transition (q, a, b1b2 ?
?
?
bk, q?)
is inthe finite-state transducer if the states q and q?
are (zi?n+1 .
.
.
zi?1), (zi?n+2 .
.
.
zi), respec-tively, and (a, b1b2 ?
?
?
bk) is zi.
The probability of the transition is pn(zi | zi?n+1 .
.
.
zi?1).The transitions associated with back-off are labeled with a special source symbol (notin the source vocabulary) and with an empty target string.
The number of states isthe overall number of k-grams (k < n) that appear in the training set of extendedstrings plus one (the unigram state).
The number of transitions is the overall num-ber of k-grams (k ?
n) plus the number of states (back-off transitions).
The actualnumber of these k-grams depends on the degree of nonmonotonicity of the originalbilingual training corpus.
If the corpus if completely monotone, this number would beapproximately the same as the number of k-grams in the source or target parts of thetraining corpus.
If the corpus in not monotone, the vocabulary of expanded stringsbecomes large, and the number of k-grams can be much larger than the number oftraining source or target k-grams.
As a consequence, an interesting property of thistype of transformations is that the source and target languages embedded in the finalfinite-state transducer are more constrained than the corresponding n-gram modelsobtained from either the source or the target strings, respectively, of the same trainingpairs (Casacuberta, Vidal, and Pico?
2004).While n-grams are deterministic (hence nonambiguous) models, the finite-statetransducers obtained after the third-step inverse transformations (h?, h?)
are oftennondeterministic and generally ambiguous; that is, there are source strings which canbe parsed through more than one path.
This is in fact a fundamental property, directlycoming from expression (5) of Theorem 2, on which the whole GIATI approach isessentially based.
As a consequence, all the search issues discussed in Section 3.1 doapply to GIATI-learned transducers.216Computational Linguistics Volume 30, Number 25.
Experimental ResultsDifferent translation tasks of different levels of difficulty were selected to assess thecapabilities of the proposed inference method in the framework of the EuTransproject (ITI et al 2000): two Spanish-English tasks (EuTrans-0 and EuTrans-I),an Italian-English task (EuTrans-II) and a Spanish-German task (EuTrans-Ia).
TheEuTrans-0 task, with a large semi-automatically generated training corpus, was usedfor studying the convergence of transducer learning algorithms for increasingly largetraining sets (Amengual et al 2000).
In this article it is used to get an estimation of per-formance limits of the GIATI technique by assuming an unbounded amount of trainingdata.
The EuTrans-I task was similar to EuTrans-0 but with a more realistically sizedtraining corpus.
This corpus was defined as a first benchmark in the EuTrans project,and therefore results with other techniques are available.
The EuTrans-II task, with aquite small and highly spontaneous natural training set, was a second benchmark ofthe project.
Finally, EuTrans-Ia was similar to EuTrans-I, but with a higher degreeof nonmonotonicity between corresponding words in input/output sentence pairs.Tables 1, 4, and 7 show some important features of these corpora.
As can be seen inthese tables, the training sets of EuTrans-0, EuTrans-I and EuTrans-Ia contain non-negligible amounts of repeated sentence pairs.
Most of these repetitions correspondto simple and/or usual sentences such as good morning, thank you, and do you have asingle room for tonight.
The repetition rate is quite significant for EuTrans-0, but it wasexplicitly reduced in the more realistic benchmark tasks EuTrans-I and EuTrans-Ia.It is worth noting, however, that no repetitions appear in any of the test sets of thesetasks.
While repetitions can be helpful for probability estimation, they are completelyuseless for inducing the transducer structure.
Moreover, since no repetitions appearin the test sets, the estimated probabilities will not be as useful as they could be iftest data repetitions exhibited the same patterns as those in the corresponding trainingmaterials.In all the experiments reported in this article, the approximate optimal translations(equation (7)) of the source test strings were computed and the word error rate (WER),the sentence error rate (SER), and the bilingual evaluation understudy (BLEU) metricfor the translations were used as assessment criteria.
The WER is the minimum numberof substitution, insertion, and deletion operations needed to convert the word stringhypothesized by the translation system into a given single reference word string (ITIet al 2000).
The SER is the result of a direct comparison between the hypothesized andreference word strings as a whole.
The BLEU metric is based on the n-grams of thehypothesized translation that occur in the reference translations (Papineni et al2001).The BLEU metric ranges from 0.0 (worst score) to 1.0 (best score).5.1 The Spanish-English Translation TasksA Spanish-English corpus was semi-automatically generated in the first phase of theEuTrans project (Vidal 1997).
The domain of the corpus involved typical human-to-human communication situations at a reception desk of a hotel.A summary of this corpus (EuTrans-0) is given in Table 1 (Amengual et al2000;Casacuberta et al 2001).
From this (large) corpus, a small subset of ten thousandtraining sentence pairs (EuTrans-I) was randomly selected in order to approach morerealistic training conditions (see also Table 1).
From these data, completely disjointtraining and test sets were defined.
It was guaranteed, however, that all the words inthe source test sentences were contained in both training sets (closed vocabulary).Results for the EuTrans-0 and EuTrans-I corpora are presented in Tables 2 and 3,respectively.
The best results obtained using the proposed technique were 3.1% WER217Casacuberta and Vidal Translation with Finite-State TransducersTable 1The Spanish-English corpus.
There was no overlap betweentraining and test sentences, and the test set did not containout-of-vocabulary words with respect to any of the training sets.Spanish EnglishEuTrans-0 Train: Sentence pairs 490,000Distinct pairs 168,629Running words 4,655,000 4,802,000Vocabulary 686 513EuTrans-I Train: Sentence pairs 10,000Distinct pairs 6,813Running words 97,131 99,292Vocabulary 683 513Test: Sentences 2,996Running words 35,023 35,590EuTrans-0 Bigram test perplexity 6.8 5.8EuTrans-I Bigram test perplexity 8.6 6.3Table 2Results with the standard corpus EuTrans-0.
Theunderlying regular models were smoothed n-grams fordifferent values of n.n-grams States Transitions WER % SER % BLEU2 4,056 67,235 8.8 50.0 0.863 33,619 173,500 4.7 27.2 0.944 110,321 364,373 4.2 23.2 0.945 147,790 492,840 3.8 20.5 0.956 201,319 663,447 3.6 19.0 0.967 264,868 857,275 3.4 18.0 0.968 331,598 1,050,949 3.3 17.4 0.969 391,812 1,218,367 3.3 17.2 0.9610 438,802 1,345,278 3.2 16.8 0.9611 471,733 1,432,027 3.1 16.4 0.9612 492,620 1,485,370 3.1 16.4 0.96Table 3Results with the standard corpus EuTrans-I.
Theunderlying regular models were smoothed n-grams fordifferent values of n.n-grams States Transitions WER % SER % BLEU2 1,696 17,121 9.0 53.7 0.863 8,562 36,763 6.7 38.9 0.904 21,338 64,856 6.7 37.9 0.915 23,879 72,006 6.6 37.1 0.916 25,947 77,531 6.6 37.0 0.917 27,336 81,076 6.6 37.0 0.91218Computational Linguistics Volume 30, Number 2for EuTrans-0 and 6.6% WER for EuTrans-I.
These results were achieved using thestatistical alignments provided by model 5 (Brown et al 1993; Och and Ney 2000) andsmoothed 11-grams and 6-grams, respectively.These results were obtained using the first type of transformation described inSection 4.2 (L1).
Similar experiments with the second type of transformation (L2) pro-duced slightly worse results.
However, L2 is interesting because many of the extendedsymbols obtained in the experiments involve very good relations between some sourceword groups and target word groups which could be useful by themselves.
Conse-quently, more research work has to be done with this second type of transformation.The results on the (benchmark) EuTrans-I corpus can be compared with thoseobtained using other approaches.
GIATI outperforms other finite-state techniques insimilar experimental conditions (with a best result of 8.3% WER, using another trans-ducer inference technique called OMEGA [ITI et al 2000]).
On the other hand, thebest result achieved by the statistical templates technique (Och and Ney 2000) was4.4% WER (ITI et al 2000).
However, this result cannot be exactly compared withthat achieved by GIATI, because the statistical templates approach used an explicit(automatic) categorization of the source and the target words, while only the rawword forms were used in GIATI.
Although GIATI is compatible with different formsof word categorization, the required finite-state expansion is not straightforward, andsome work is still needed in order to actually allow this technique to be taken advan-tage of.5.2 The Italian-English TaskThe Italian-English translation task of the EuTrans project (ITI et al 2000) consistedof spoken person-to-person telephone communications in the framework of a hotelreception desk.
A text corpus was collected with the transcriptions of dialogues of thistype, along with the corresponding (human-produced) translations.
A summary of thecorpus used in the experiments (EuTrans-II) is given in Table 4.
There was a smalloverlap of seven pairs between the training set and the test set, but in this case, thevocabulary was not closed (there were 107 words in the test set that did not exist inthe training-set vocabulary).
The processing of words out of the vocabulary was verysimple in this experiment: If the word started with a capital letter, the translation wasthe source word; otherwise it was the empty string.The same translation procedure and evaluation criteria used for EuTrans-0 andEuTrans-I were used for EuTrans-II.
The results are reported in Table 5.Table 4The EuTrans-II corpus.
There was a smalloverlap of seven pairs between the training andtest sets, but 107 source words in the test set werenot in the (training-set-derived) vocabulary.Italian EnglishTrain: Sentence pairs 3,038Running words 55,302 64,176Vocabulary 2,459 1,712Test: Sentences 300Running words 6,121 7,243Bigram test perplexity 31 25219Casacuberta and Vidal Translation with Finite-State TransducersTable 5Results with the standard EuTrans-II corpus.
Theunderlying regular models were smoothedn-grams (Rosenfeld 1995) for different values of n.n-grams States Transitions WER % SER % BLEU2 5,909 49,701 27.2 96.7 0.563 24,852 97,893 27.3 96.0 0.564 54,102 157,073 27.4 96.0 0.56Table 6Results with the standard EuTrans-II corpus.
Theunderlying regular models were smoothed n-grams(Rosenfeld 1994) for different values of n. The trainingset was (automatically) segmented using a prioriknowledge.
The statistical alignments were constrainedto be within each parallel segment.n-grams States Transitions WER % SER % BLEU2 6,300 52,385 24.9 93.0 0.623 26,194 102,941 25.5 93.3 0.614 56,856 164,972 25.5 93.3 0.61This corpus contained many long sentences, most of which were composed ofrather short segments connected by punctuation marks.
Typically, these segments canbe monotonically aligned with corresponding target segments using a simple dynamicprogramming procedure (prior segmentation) (ITI et al 2000).
We explored computingthe statistical alignments within each pair of segments rather than in the entire sen-tences.
Since the segments were shorter than the whole sentences, the alignment prob-ability distributions were better estimated.
In the training phase, extended symbolswere built from these alignments, and the strings of extended symbols correspondingto the segments of the same original string pair were concatenated.
Test sentenceswere directly used, without any kind of segmentation.The translation results using prior segmentation are reported in Table 6.
Theseresults were clearly better than those of the corresponding experiments with nonseg-mented training data.The accuracy of GIATI in the EuTrans-II experiments was significantly worsethan that achieved in EuTrans-I, and best performance is obtained with a lower-ordern-gram.
One obvious reason for this behavior is that this corpus is far more sponta-neous than the first one, and consequently, it has a much higher degree of variability.Moreover, the training data set is about three times smaller than the correspondingdata of EuTrans-I, while the vocabularies are three to four times larger.The best result achieved with the proposed technique on EuTrans-II was 24.9%WER, using prior segmentation of the training pairs and a smoothed bigram model.This result was comparable to the best among all those reported in RWTH Aachen andITI (1999).
The previously mentioned statistical templates technique achieved 25.1%WER in this case.
In this application, in which categories are not as important as inEuTrans-I, statistical templates and GIATI achieved similar results.220Computational Linguistics Volume 30, Number 2Table 7The Spanish-German corpus.
There was no overlapbetween training and test sets and noout-of-vocabulary words in the test set.Spanish GermanTrain: Sentence pairs 10,000Distinct pairs 6,636Running words 96,043 90,099Vocabulary 6,622 4,890Test: Sentences 2,862Running words 33,542 31,103Bigram test perplexity 8.3 6.6Table 8Results with the standard corpus EuTrans-Ia.
Theunderlying regular models were smoothed n-grams fordifferent values of n.n-grams States Transitions WER % SER % BLEU2 2,441 21,181 16.0 78.1 0.743 10,592 43,294 11.3 65.3 0.824 24,554 74,412 10.6 62.3 0.835 27,748 83,553 10.6 62.5 0.836 30,501 91,055 10.6 62.4 0.837 32,497 96,303 10.7 62.7 0.835.3 The Spanish-German TaskThe Spanish-German translation task is similar to EuTrans-I, but here the targetlanguage is German instead of English.
It should be noted that Spanish syntax issignificantly more different from that of German than it is from that of English, andtherefore, the corresponding corpus exhibited a higher degree of nonmonotonicity.
Thefeatures of this corpus (EuTrans-Ia) are summarized in Table 7.
There was no overlapbetween training and test sets, and the vocabulary was closed.The translation results are reported in Table 8.
As expected from the higher degreeof nonmonotonicity of the present task, these results were somewhat worse than thoseachieved with EuTrans-I.
This is consistent with the larger number of states andtransitions of the EuTrans-Ia models: The higher degree of word reordering of thesemodels is achieved at the expense of a larger number of extended words.The way GIATI transducers cope with these monotonicity differences can be moreexplicitly illustrated by estimating how many target words are produced after somedelay with respect to the source.
While directly determining (or even properly defin-ing) the actual production delay for each individual (test) word is not trivial, anapproximation can be indirectly derived from the number of target words that arepreceded by sequences of ?
symbols (from target-empty transitions) in the parsing ofa source test text with a given transducer.
This has been done for the EuTrans-I andEuTrans-Ia test sets with GIATI transducers learned with n = 6.
On the average, theEuTrans-I transducer needed to introduce delays ranging from one to five positionsfor approximately 15% of the English target words produced, while the transducerfor EuTrans-Ia had to introduce similar delays for about 20% of the German targetwords produced.221Casacuberta and Vidal Translation with Finite-State Transducers5.4 Error AnalysisThe errors reported in the previous sections can be attributed to four main factors:1.
Correct translations which differ from the given (single) reference2.
Wrong alignments of training pairs3.
Insufficient or improper generalization of n-gram-based GIATI learning4.
Wrong approximate Viterbi score?based search resultsAn informal inspection of the target sentences produced by GIATI in all the experi-ments reveals that the first three factors are responsible for the vast majority of errors.Table 9 shows typical examples for the results of the EuTrans-I experiment with6-gram-based GIATI transducers.The first three examples correspond to correct translations which have been wrong-ly counted as errors (factor 1).
Examples 4 and 5 are probably due to alignment prob-lems (factor 2).
In fact, more than half of the errors reported in the EuTrans-I experi-ments are due to misuse or misplacement of the English word please.
Examples 6?8 canalso be considered minor errors, probably resulting from factors 2 and 3.
Examples 9and 10 are clear undergeneralization errors (factor 3).
These errors could have beeneasily overcome through an adequate use of bilingual lexical categorization.
Examples11 and 12, finally, are more complex errors that can be attributed to (a combination of)factors 2, 3, and 4.6.
ConclusionsA method has been proposed in this article for inferring stochastic finite-state trans-ducers from stochastic regular grammars.
This method, GIATI, allowed us to achievegood results in several language translation tasks with different levels of difficulty.
Itworks better than other finite-state techniques when the training data are scarce andachieves similar results with sufficient training data.The GIATI approach produces transducers which generalize the information pro-vided by the (aligned) training pairs.
Thanks to the use of n-grams as a core learningprocedure, a wide range of generalization degrees can be achieved.
It is well-knownthat a 1-gram entails a maximum generalization, allowing (extended) words to followone another.
On the other hand, for sufficiently large m, a (nonsmoothed) m-gram is justan exact representation of the training strings (of extended words, in our case).
Such arepresentation can thus be considered a simple ?translation memory?
that just containsthe (aligned) training pairs.
For any new source sentence, this ?memory?
can be eas-ily and quite efficiently searched through finite-state parsing.
For other intermediatevalues of n, 1<n<m, GIATI obtains increasing degrees of generalization.
As in thecase of language modeling, the generalization degree (n) has to be tuned so as to takemaximum advantage of the available training data.
As training pairs become scarce,more generalization is needed to allow GIATI to adequately accept new test sentences.This behavior can be clearly observed throughout the results presented in this article.Another feature of the GIATI approach is the use of smoothed n-grams of extendedwords as the basic mechanism for producing smoothed transducers.
The combinationof this feature with the intrinsic generalization provided by the n-gram modeling itselfhas proved very adequate to deal with the problem of unseen source (sub)strings.Obviously, the overall quality of the generalizations achieved by GIATI stronglyrelies on the quality of the statistical alignments used and on the way word orderis preserved in the source-target strings of each training pair.
Taking into account the222Computational Linguistics Volume 30, Number 2Table 9Examples of typical errors produced by a 6-gram-based GIATItransducer in the the EuTrans-I task.
For each Spanish source sentence,the corresponding target reference and GIATI translations are shown insuccessive lines.1 ?
les importar?
?a bajarnos nuestras bolsas a recepcio?n ?would you mind sending our bags down to reception ?would you mind sending down our bags to reception ?2 explique la cuenta de la habitacio?n cuatro diecise?is .explain the bill for room number four one six .explain the bill for room number four sixteenth .3 ?
cua?nto vale una habitacio?n doble para cinco d?
?as incluyendo desayuno ?how much is a double room including breakfast for five days ?how much is a double room for five days including breakfast ?4 por favor , deseo una habitacio?n individual para esta semana .I want a single room for this week , please .I want a single room for this week .5 ?
le importar?
?a despertarnos a las cinco ?would you mind waking us up at five ?would you mind waking us up at five , please ?6 ?
hay televisio?n , aire acondicionado y caja fuerte en las habitaciones ?are there a tv , air conditioning and a safe in the rooms ?is there a tv , air conditioning and a safe in the rooms ?7 ?
tiene habitaciones libres con tele?fono ?do you have any rooms with a telephone available ?do you have any rooms with a telephone ?8 ?
querr?
?a llamar a mi taxi ?would you call my taxi , please ?would you call my taxi for me , please ?9 hemos de marcharnos el veintise?is de marzo por la tarde .we should leave on March the twenty-sixth in the afternoon .we should leave on March the twenty-seventh in the afternoon10 por favor , ?
nos podr?
?a dar usted la llave de la ochocientos ochenta y uno ?could you give us the key to room number eight eight one , please ?could you give us the key to room number eight oh eight one , please ?11 quiero cambiarme de habitacio?n .I want to change rooms .I want to move .12 ?
tiene televisio?n nuestra habitacio?n ?does our room have a tv ?does our room ?223Casacuberta and Vidal Translation with Finite-State Transducersfinite-state nature of GIATI transducers, certain heuristics have been needed in order toavoid a direct use of too-long-distance alignments (L1 in Section 4.2).
This has provedadequate for language pairs with not too different (syntactic) structure and more soif the domains are limited.
As we relax these restrictions, we might have to relax thenot-too-long-distance assumption correspondingly.
In this respect, the bilingual wordreordering ideas of Vilar, Vidal, and Amengual (1996), Vidal (1997), and Bangalore andRicardi (2000a) may certainly prove useful in future developments.AcknowledgmentsThis work has been partially supported bythe European Union under grantsIT-LTR-OS-30268, IST-2001-32091 andSpanish project TIC 2000-1599-C02-01.
Theauthors wish to thank the anonymousreviewers for their criticisms andsuggestions.ReferencesAl-Onaizan, Yaser, Jan Curin, Michael Jahr,Kevin Knight, John Lafferty, DanMelamed, Franz-Josef Och, David Purdy,Noah A. Smith, and David Yarowsky.1999.
Statistical machine translation.
FinalReport, JHU Workshop, Johns HopkinsUniversity, Baltimore.Amengual, Juan-Carlos, Jose-Miguel Bened?
?,Francisco Casacuberta, Asuncio?n Castano, Antonio Castellanos, V?
?ctor Jime?nez,David Llorens, Andre?s Marzal, Moise?sPastor, Federico Prat, Enrique Vidal, andJuan-Miguel Vilar.
2000.
The EUTRANS-Ispeech translation system.
MachineTranslation Journal, 15(1?2):75?103.Bangalore, Srinivas and Giuseppe Ricardi.2000a.
Finite-state models for lexicalreordering in spoken languagetranslation.
In Proceedings of theInternational Conference on Speech andLanguage Processing, Beijing, China,October.Bangalore, Srinivas and Giuseppe Ricardi.2000b.
Stochastic finite-state models forspoken language machine translation.
InProceedings of the Workshop on EmbeddedMachine Translation Systems, NorthAmerican Association for ComputationalLinguistics, pages 52?59, Seattle, May.Bangalore, Srinivas and Giuseppe Ricardi.2001.
A finite-state approach to machinetranslation.
In Proceedings of the SecondMeeting of the North American Chapter of theAssociation for Computational Linguistics2001, Pittsburgh, May.Berstel, Jean.
1979.
Transductions andcontext-free languages.
B. G. Teubner,Stuttgart.Brown, Peter F., Stephen A. Della Pietra,Vincent J. Della Pietra, and Robert L.Mercer.
1993.
The mathematics ofstatistical machine translation: Parameterestimation.
Computational Linguistics,19(2):263?310.Casacuberta, Francisco.
1996.
Maximummutual information and conditionalmaximum likelihood estimation ofstochastic regular syntax-directedtranslation schemes.
In GrammaticalInference: Learning Syntax from Sentences(volume 1147 of Lecture Notes onComputer Science).
Springer-Verlag,Berlin and Heidelberg, pages 282?291.Casacuberta, Francisco.
2000.
Inference offinite-state transducers by using regulargrammars and morphisms.
In GrammaticalInference: Algorithms and Applications(volume 1891 of Lecture Notes inArtificial Intelligence).
Springer-Verlag,Berlin and Heidelberg, pages 1?14.Casacuberta, Francisco and Colin de laHiguera.
2000.
Computational complexityof problems on probabilistic grammarsand transducers.
In Grammatical Inference:Algorithms and Applications (volume 1891of Lecture Notes in Artificial Intelligence).Springer-Verlag, Berlin and Heidelberg,pages 15?24.Casacuberta, Francisco, David Llorens,Carlos Mart?
?nez, Sirko Molau, FranciscoNevado, Hermann Ney, Moisee?s Pastor,David Pico?, Alberto Sanchis, EnriqueVidal, and Juan-Miguel Vilar.
2001.Speech-to-speech translation based onfinite-state transducers.
In Proceedings ofthe IEEE International Conference onAcoustic, Speech and Signal Processing,volume 1.
IEEE Press, Piscataway, NJ,pages 613?616.Casacuberta, Francisco, Enrique Vidal, andDavid Pico?.
2004.
Inference of finite-statetransducers from regular languages.Pattern Recognition, forthcoming.Clarkson, Philip and Ronald Rosenfeld.1997.
Statistical language modeling usingthe CMU-Cambridge toolkit.
InProceedings of EUROSPEECH, volume 5,pages 2707?2710, Rhodes, September.Fu, King-Sun.
1982.
Syntactic patternrecognition and applications.
Prentice-Hall,Englewood Cliffs, NJ.224Computational Linguistics Volume 30, Number 2Garc?
?a, Pedro, Enrique Vidal, and FranciscoCasacuberta.
1987.
Local languages, thesuccessor method, and a step towards ageneral methodology for the inference ofregular grammars.
IEEE Transactions onPattern Analysis and Machine Intelligence,9(6):841?845.Gonza?lez, Jorge, Ismael Salvador, AlejandroToselli, Alfons Juan, Enrique Vidal, andFrancisco Casacuberta.
2000.
Offlinerecognition of syntax-constrained cursivehandwritten text.
In Advances in PatternRecognition (volume 1876 of Lecture Notesin Computer Science).
Springer-Verlag,Berlin and Heidelberg, pages 143?153.Hazen, Timothy, I. Lee Hetherington, andAlex Park.
2001.
FST-based recognitiontechniques for multi-lingual andmulti-domain spontaneous speech.
InProceedings of EUROSPEECH2001, pages1591?1594, Aalborg, Denmark, September.ITI, FUB, RWTH Aachen, and ZERES.
2000.Example-based language translationsystems: Final report.
Technical ReportD0.1c, Instituto Tecnolo?gico deInforma?tica, Fondazione Ugo Bordoni,Rheinisch Westfa?lische TechnischeHochschule Aachen Lehrstuhl fu?rInformatik V and Zeres GmbH Bochum.Information Technology.
Long TermResearch Domain.
Open scheme.Knight, Kevin and Yaser Al-Onaizan.
1998.Translation with finite-state devices.
InProceedings of the Fourth.
AMTA Conference(volume 1529 of Lecture Notes inArtificial Intelligence).
Springer-Verlag,Berlin and Heidelberg, pages 421?437.Llorens, David, Juna-Miguel Vilar, andFrancisco Casacuberta.
2002.
Finite statelanguage models smoothed usingn-grams.
International Journal of PatternRecognition and Artificial Intelligence,16(3):275?289.Ma?kinen, Erkki.
1999.
Inferring finitetransducers.
Technical Report A-1999-3,University of Tampere, Tampere, Finland.Mohri, Mehryar.
1997.
Finite-statetransducers in language and speechprocessing.
Computational Linguistics,23(2):1?20.Mou, Xiaolong, Stephanie Seneff, and VictorZue.
2001.
Context-dependentprobabilistic hierarchical sub-lexicalmodelling using finite-state transducers.In Proceedings of EUROSPEECH2001, pages451?454, Aalborg, Denmark, September.Ney, Hermann, Sven Martin, andFrank Wessel.
1997.
Statistical languagemodeling using leaving-one-out.
InS.
Young and G. Bloothooft, editors,Corpus-Based Statiscal Methods in Speech andLanguage Processing.
Kluwer Academic,Dordrecht, the Netherlands, pages174?207.Och, Franz-Josef and Hermann Ney.
2000.Improved statistical alignment models.
InProceedings of the 38th Annual Meeting of theAssociation for Computational Linguistics,pages 440?447, Hong Kong, October.Oncina, Jose?, Pedro Garc?
?a, and EnriqueVidal.
1993.
Learning subsequentialtransducers for pattern recognitioninterpretation tasks.
IEEE Transactions onPattern Analysis and Machine Intelligence,15(5):448?458.Papineni, Kishore, Salim Roukos, ToddWard, and Wei-Jing Zhu.
2001.
Bleu: Amethod for automatic evaluation ofmachine translation.
Technical ReportRC22176(W?1?9-?22), IBM ResearchDivision, Yorktown Heights, NY,September.Rosenfeld, Ronald.
1995.
The CMUstatistical language modeling toolkit andits use in the 1994 ARPA CSR evaluation.In Proceedings of the ARPA Spoken LanguageTechnology Workshop, Princeton, NJ.Morgan Kaufmann, San Mateo, CA.RWTH Aachen and ITI.
1999.
Statisticalmodeling techniques and results andsearch techniques and results.
TechnicalReport D3.1a and D3.2a,Rheinisch Westfa?lische Technis-che HochschuleAachen Lehrstuhl fu?r Informatik VI andInstituto Tecnolo?gico de Informa?tica.Information Technology.
Long TermResearch Domain.
Open scheme.Segarra, Encarna, Mar?
?a-Isabel GalianoEmilio Sanchis, Fernando Garc?
?a, andLuis Hurtado.
2001.
Extracting semanticinformation through automatic learningtechniques.
In Proceedings of the SpanishSymposium on Pattern Recognition and ImageAnalysis, pages 177?182, Benicasim, Spain,May.Seward, Alexander.
2001.
Transduceroptimizations for tight-coupled decoding.In Proceedings of EUROSPEECH2001, pages1607?1610, Aalborg, Denmark, September.Vidal, Enrique.
1997.
Finite-statespeech-to-speech translation.
InProceedings of the International Conference onAcoustic Speech and Signal Processing,Munich.
IEEE Press, Piscataway, NJ,pages 111?114.Vidal, Enrique, Francisco Casacuberta, andPedro Garc??a.
1995.
Grammatical inferenceand automatic speech recognition.
InA.
Rubio, editor, New Advances and Trendsin Speech Recognition and Coding (volume147 of NATO-ASI Series F: Computer and225Casacuberta and Vidal Translation with Finite-State TransducersSystems Sciences).
Springer-Verlag, Berlinand Heidelberg, pages 174?191.Vidal, Enrique, Pedro Garc?
?a, and EncarnaSegarra.
1989.
Inductive learning offinite-state transducers for theinterpretation of unidimensional objects.In R. Mohr, T. Pavlidis, and A. Sanfeliu,editors, Structural Pattern Analysis.
WorldScientific, Singapore, pages 17?35.Vilar, Juan-Miguel.
2000.
Improve thelearning of subsequential transducers byusing alignments and dictionaries.
InGrammatical Inference: Algorithms andApplications (volume 1891 of Lecture Notesin Artificial Intelligence).
Springer-Verlag,Berlin and Heidelberg, pages 298?312.Vilar, Juan-Miguel, Enrique Vidal, andJuan-Carlos Amengual.
1996.
Learningextended finite state models for languagetranslation.
In Andra?s Kornai, editor,Proceedings of the Extended Finite StateModels of Language Workshop, pages 92?96,Budapest, August.
