Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 655?665,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsA Convolutional Neural Network for Modelling SentencesNal Kalchbrenner Edward Grefenstette{nal.kalchbrenner, edward.grefenstette, phil.blunsom}@cs.ox.ac.ukDepartment of Computer ScienceUniversity of OxfordPhil BlunsomAbstractThe ability to accurately represent sen-tences is central to language understand-ing.
We describe a convolutional architec-ture dubbed the Dynamic ConvolutionalNeural Network (DCNN) that we adoptfor the semantic modelling of sentences.The network uses Dynamic k-Max Pool-ing, a global pooling operation over lin-ear sequences.
The network handles inputsentences of varying length and inducesa feature graph over the sentence that iscapable of explicitly capturing short andlong-range relations.
The network doesnot rely on a parse tree and is easily ap-plicable to any language.
We test theDCNN in four experiments: small scalebinary and multi-class sentiment predic-tion, six-way question classification andTwitter sentiment prediction by distant su-pervision.
The network achieves excellentperformance in the first three tasks and agreater than 25% error reduction in the lasttask with respect to the strongest baseline.1 IntroductionThe aim of a sentence model is to analyse andrepresent the semantic content of a sentence forpurposes of classification or generation.
The sen-tence modelling problem is at the core of manytasks involving a degree of natural language com-prehension.
These tasks include sentiment analy-sis, paraphrase detection, entailment recognition,summarisation, discourse analysis, machine trans-lation, grounded language learning and image re-trieval.
Since individual sentences are rarely ob-served or not observed at all, one must representa sentence in terms of features that depend on thewords and short n-grams in the sentence that arefrequently observed.
The core of a sentence modelinvolves a feature function that defines the processThe  cat  sat  on  the  red  mat  The  cat  sat  on  the  red  matFigure 1: Subgraph of a feature graph inducedover an input sentence in a Dynamic Convolu-tional Neural Network.
The full induced graphhas multiple subgraphs of this kind with a distinctset of edges; subgraphs may merge at differentlayers.
The left diagram emphasises the poolednodes.
The width of the convolutional filters is 3and 2 respectively.
With dynamic pooling, a fil-ter with small width at the higher layers can relatephrases far apart in the input sentence.by which the features of the sentence are extractedfrom the features of the words or n-grams.Various types of models of meaning have beenproposed.
Composition based methods have beenapplied to vector representations of word meaningobtained from co-occurrence statistics to obtainvectors for longer phrases.
In some cases, com-position is defined by algebraic operations overword meaning vectors to produce sentence mean-ing vectors (Erk and Pad?o, 2008; Mitchell andLapata, 2008; Mitchell and Lapata, 2010; Tur-ney, 2012; Erk, 2012; Clarke, 2012).
In othercases, a composition function is learned and ei-ther tied to particular syntactic relations (Guevara,2010; Zanzotto et al, 2010) or to particular wordtypes (Baroni and Zamparelli, 2010; Coecke etal., 2010; Grefenstette and Sadrzadeh, 2011; Kart-saklis and Sadrzadeh, 2013; Grefenstette, 2013).Another approach represents the meaning of sen-tences by way of automatically extracted logicalforms (Zettlemoyer and Collins, 2005).655A central class of models are those based onneural networks.
These range from basic neu-ral bag-of-words or bag-of-n-grams models to themore structured recursive neural networks andto time-delay neural networks based on convo-lutional operations (Collobert and Weston, 2008;Socher et al, 2011; Kalchbrenner and Blunsom,2013b).
Neural sentence models have a num-ber of advantages.
They can be trained to obtaingeneric vectors for words and phrases by predict-ing, for instance, the contexts in which the wordsand phrases occur.
Through supervised training,neural sentence models can fine-tune these vec-tors to information that is specific to a certaintask.
Besides comprising powerful classifiers aspart of their architecture, neural sentence modelscan be used to condition a neural language modelto generate sentences word by word (Schwenk,2012; Mikolov and Zweig, 2012; Kalchbrennerand Blunsom, 2013a).We define a convolutional neural network archi-tecture and apply it to the semantic modelling ofsentences.
The network handles input sequencesof varying length.
The layers in the network in-terleave one-dimensional convolutional layers anddynamic k-max pooling layers.
Dynamic k-maxpooling is a generalisation of the max pooling op-erator.
The max pooling operator is a non-linearsubsampling function that returns the maximumof a set of values (LeCun et al, 1998).
The op-erator is generalised in two respects.
First, k-max pooling over a linear sequence of values re-turns the subsequence of k maximum values in thesequence, instead of the single maximum value.Secondly, the pooling parameter k can be dynam-ically chosen by making k a function of other as-pects of the network or the input.The convolutional layers apply one-dimensional filters across each row of features inthe sentence matrix.
Convolving the same filterwith the n-gram at every position in the sentenceallows the features to be extracted independentlyof their position in the sentence.
A convolutionallayer followed by a dynamic pooling layer anda non-linearity form a feature map.
Like in theconvolutional networks for object recognition(LeCun et al, 1998), we enrich the representationin the first layer by computing multiple featuremaps with different filters applied to the inputsentence.
Subsequent layers also have multiplefeature maps computed by convolving filters withall the maps from the layer below.
The weights atthese layers form an order-4 tensor.
The resultingarchitecture is dubbed a Dynamic ConvolutionalNeural Network.Multiple layers of convolutional and dynamicpooling operations induce a structured featuregraph over the input sentence.
Figure 1 illustratessuch a graph.
Small filters at higher layers can cap-ture syntactic or semantic relations between non-continuous phrases that are far apart in the inputsentence.
The feature graph induces a hierarchicalstructure somewhat akin to that in a syntactic parsetree.
The structure is not tied to purely syntacticrelations and is internal to the neural network.We experiment with the network in four set-tings.
The first two experiments involve predict-ing the sentiment of movie reviews (Socher etal., 2013b).
The network outperforms other ap-proaches in both the binary and the multi-class ex-periments.
The third experiment involves the cat-egorisation of questions in six question types inthe TREC dataset (Li and Roth, 2002).
The net-work matches the accuracy of other state-of-the-art methods that are based on large sets of en-gineered features and hand-coded knowledge re-sources.
The fourth experiment involves predict-ing the sentiment of Twitter posts using distant su-pervision (Go et al, 2009).
The network is trainedon 1.6 million tweets labelled automatically ac-cording to the emoticon that occurs in them.
Onthe hand-labelled test set, the network achieves agreater than 25% reduction in the prediction errorwith respect to the strongest unigram and bigrambaseline reported in Go et al (2009).The outline of the paper is as follows.
Section 2describes the background to the DCNN includingcentral concepts and related neural sentence mod-els.
Section 3 defines the relevant operators andthe layers of the network.
Section 4 treats of theinduced feature graph and other properties of thenetwork.
Section 5 discusses the experiments andinspects the learnt feature detectors.12 BackgroundThe layers of the DCNN are formed by a convo-lution operation followed by a pooling operation.We begin with a review of related neural sentencemodels.
Then we describe the operation of one-dimensional convolution and the classical Time-Delay Neural Network (TDNN) (Hinton, 1989;Waibel et al, 1990).
By adding a max pooling1Code available at www.nal.co656layer to the network, the TDNN can be adopted asa sentence model (Collobert and Weston, 2008).2.1 Related Neural Sentence ModelsVarious neural sentence models have been de-scribed.
A general class of basic sentence modelsis that of Neural Bag-of-Words (NBoW) models.These generally consist of a projection layer thatmaps words, sub-word units or n-grams to highdimensional embeddings; the latter are then com-bined component-wise with an operation such assummation.
The resulting combined vector is clas-sified through one or more fully connected layers.A model that adopts a more general structureprovided by an external parse tree is the RecursiveNeural Network (RecNN) (Pollack, 1990; K?uchlerand Goller, 1996; Socher et al, 2011; Hermannand Blunsom, 2013).
At every node in the tree thecontexts at the left and right children of the nodeare combined by a classical layer.
The weights ofthe layer are shared across all nodes in the tree.The layer computed at the top node gives a repre-sentation for the sentence.
The Recurrent NeuralNetwork (RNN) is a special case of the recursivenetwork where the structure that is followed is asimple linear chain (Gers and Schmidhuber, 2001;Mikolov et al, 2011).
The RNN is primarily usedas a language model, but may also be viewed as asentence model with a linear structure.
The layercomputed at the last word represents the sentence.Finally, a further class of neural sentence mod-els is based on the convolution operation and theTDNN architecture (Collobert and Weston, 2008;Kalchbrenner and Blunsom, 2013b).
Certain con-cepts used in these models are central to theDCNN and we describe them next.2.2 ConvolutionThe one-dimensional convolution is an operationbetween a vector of weights m ?
Rmand a vectorof inputs viewed as a sequence s ?
Rs.
The vectorm is the filter of the convolution.
Concretely, wethink of s as the input sentence and si?
R is a sin-gle feature value associated with the i-th word inthe sentence.
The idea behind the one-dimensionalconvolution is to take the dot product of the vectorm with each m-gram in the sentence s to obtainanother sequence c:cj= m?sj?m+1:j(1)Equation 1 gives rise to two types of convolutiondepending on the range of the index j.
The narrowtype of convolution requires that s ?
m and yieldss1 s1ss ssc1 c5c5Figure 2: Narrow and wide types of convolution.The filter m has size m = 5.a sequence c ?
Rs?m+1with j ranging from mto s. The wide type of convolution does not haverequirements on s or m and yields a sequence c ?Rs+m?1where the index j ranges from 1 to s +m ?
1.
Out-of-range input values siwhere i < 1or i > s are taken to be zero.
The result of thenarrow convolution is a subsequence of the resultof the wide convolution.
The two types of one-dimensional convolution are illustrated in Fig.
2.The trained weights in the filter m correspondto a linguistic feature detector that learns to recog-nise a specific class of n-grams.
These n-gramshave size n ?
m, where m is the width of thefilter.
Applying the weights m in a wide convo-lution has some advantages over applying them ina narrow one.
A wide convolution ensures that allweights in the filter reach the entire sentence, in-cluding the words at the margins.
This is particu-larly significant when m is set to a relatively largevalue such as 8 or 10.
In addition, a wide convo-lution guarantees that the application of the filterm to the input sentence s always produces a validnon-empty result c, independently of the width mand the sentence length s. We next describe theclassical convolutional layer of a TDNN.2.3 Time-Delay Neural NetworksA TDNN convolves a sequence of inputs s with aset of weights m. As in the TDNN for phonemerecognition (Waibel et al, 1990), the sequence sis viewed as having a time dimension and the con-volution is applied over the time dimension.
Eachsjis often not just a single value, but a vector ofd values so that s ?
Rd?s.
Likewise, m is a ma-trix of weights of size d?m.
Each row of m isconvolved with the corresponding row of s and theconvolution is usually of the narrow type.
Multi-ple convolutional layers may be stacked by takingthe resulting sequence c as input to the next layer.The Max-TDNN sentence model is based on thearchitecture of a TDNN (Collobert and Weston,2008).
In the model, a convolutional layer of thenarrow type is applied to the sentence matrix s,where each column corresponds to the feature vec-657tor wi?
Rdof a word in the sentence:s =??w1.
.
.
ws??
(2)To address the problem of varying sentencelengths, the Max-TDNN takes the maximum ofeach row in the resulting matrix c yielding a vectorof d values:cmax=???max(c1,:)...max(cd,:)???
(3)The aim is to capture the most relevant feature, i.e.the one with the highest value, for each of the drows of the resulting matrix c. The fixed-sizedvector cmaxis then used as input to a fully con-nected layer for classification.The Max-TDNN model has many desirableproperties.
It is sensitive to the order of the wordsin the sentence and it does not depend on externallanguage-specific features such as dependency orconstituency parse trees.
It also gives largely uni-form importance to the signal coming from eachof the words in the sentence, with the exceptionof words at the margins that are considered fewertimes in the computation of the narrow convolu-tion.
But the model also has some limiting as-pects.
The range of the feature detectors is lim-ited to the span m of the weights.
Increasing m orstacking multiple convolutional layers of the nar-row type makes the range of the feature detectorslarger; at the same time it also exacerbates the ne-glect of the margins of the sentence and increasesthe minimum size s of the input sentence requiredby the convolution.
For this reason higher-orderand long-range feature detectors cannot be easilyincorporated into the model.
The max pooling op-eration has some disadvantages too.
It cannot dis-tinguish whether a relevant feature in one of therows occurs just one or multiple times and it for-gets the order in which the features occur.
Moregenerally, the pooling factor by which the signalof the matrix is reduced at once corresponds tos?m+1; even for moderate values of s the pool-ing factor can be excessive.
The aim of the nextsection is to address these limitations while pre-serving the advantages.3 Convolutional Neural Networks withDynamic k-Max PoolingWe model sentences using a convolutional archi-tecture that alternates wide convolutional layersK-Max pooling(k=3)Fully connectedlayerFoldingWideconvolution(m=2)Dynamick-max pooling(k= f(s) =5)Projectedsentencematrix(s=7)Wideconvolution(m=3)The cat sat on the red matFigure 3: A DCNN for the seven word input sen-tence.
Word embeddings have size d = 4.
Thenetwork has two convolutional layers with twofeature maps each.
The widths of the filters at thetwo layers are respectively 3 and 2.
The (dynamic)k-max pooling layers have values k of 5 and 3.with dynamic pooling layers given by dynamic k-max pooling.
In the network the width of a featuremap at an intermediate layer varies depending onthe length of the input sentence; the resulting ar-chitecture is the Dynamic Convolutional NeuralNetwork.
Figure 3 represents a DCNN.
We pro-ceed to describe the network in detail.3.1 Wide ConvolutionGiven an input sentence, to obtain the first layer ofthe DCNN we take the embedding wi?
Rdforeach word in the sentence and construct the sen-tence matrix s ?
Rd?sas in Eq.
2.
The valuesin the embeddings wiare parameters that are op-timised during training.
A convolutional layer inthe network is obtained by convolving a matrix ofweights m ?
Rd?mwith the matrix of activationsat the layer below.
For example, the second layeris obtained by applying a convolution to the sen-tence matrix s itself.
Dimension d and filter widthm are hyper-parameters of the network.
We let theoperations be wide one-dimensional convolutionsas described in Sect.
2.2.
The resulting matrix chas dimensions d?
(s+m?
1).6583.2 k-Max PoolingWe next describe a pooling operation that is a gen-eralisation of the max pooling over the time di-mension used in the Max-TDNN sentence modeland different from the local max pooling opera-tions applied in a convolutional network for objectrecognition (LeCun et al, 1998).
Given a valuek and a sequence p ?
Rpof length p ?
k, k-max pooling selects the subsequence pkmaxof thek highest values of p. The order of the values inpkmaxcorresponds to their original order in p.The k-max pooling operation makes it possibleto pool the k most active features in p that may bea number of positions apart; it preserves the orderof the features, but is insensitive to their specificpositions.
It can also discern more finely the num-ber of times the feature is highly activated in pand the progression by which the high activationsof the feature change across p. The k-max poolingoperator is applied in the network after the topmostconvolutional layer.
This guarantees that the inputto the fully connected layers is independent of thelength of the input sentence.
But, as we see next, atintermediate convolutional layers the pooling pa-rameter k is not fixed, but is dynamically selectedin order to allow for a smooth extraction of higher-order and longer-range features.3.3 Dynamic k-Max PoolingA dynamic k-max pooling operation is a k-maxpooling operation where we let k be a function ofthe length of the sentence and the depth of the net-work.
Although many functions are possible, wesimply model the pooling parameter as follows:kl= max( ktop, dL?
lLse ) (4)where l is the number of the current convolutionallayer to which the pooling is applied and L is thetotal number of convolutional layers in the net-work; ktopis the fixed pooling parameter for thetopmost convolutional layer (Sect.
3.2).
For in-stance, in a network with three convolutional lay-ers and ktop= 3, for an input sentence of lengths = 18, the pooling parameter at the first layeris k1= 12 and the pooling parameter at the sec-ond layer is k2= 6; the third layer has the fixedpooling parameter k3= ktop= 3.
Equation 4is a model of the number of values needed to de-scribe the relevant parts of the progression of anl-th order feature over a sentence of length s. Foran example in sentiment prediction, according tothe equation a first order feature such as a posi-tive word occurs at most k1times in a sentence oflength s, whereas a second order feature such as anegated phrase or clause occurs at most k2times.3.4 Non-linear Feature FunctionAfter (dynamic) k-max pooling is applied to theresult of a convolution, a bias b ?
Rdand a non-linear function g are applied component-wise tothe pooled matrix.
There is a single bias value foreach row of the pooled matrix.If we temporarily ignore the pooling layer, wemay state how one computes each d-dimensionalcolumn a in the matrix a resulting after the convo-lutional and non-linear layers.
Define M to be thematrix of diagonals:M = [diag(m:,1), .
.
.
, diag(m:,m)] (5)where m are the weights of the d filters of the wideconvolution.
Then after the first pair of a convolu-tional and a non-linear layer, each column a in thematrix a is obtained as follows, for some index j:a = g???M???wj...wj+m?1??
?+ b???
(6)Here a is a column of first order features.
Sec-ond order features are similarly obtained by ap-plying Eq.
6 to a sequence of first order featuresaj, ..., aj+m?
?1with another weight matrix M?.Barring pooling, Eq.
6 represents a core aspectof the feature extraction function and has a rathergeneral form that we return to below.
Togetherwith pooling, the feature function induces positioninvariance and makes the range of higher-orderfeatures variable.3.5 Multiple Feature MapsSo far we have described how one applies a wideconvolution, a (dynamic) k-max pooling layer anda non-linear function to the input sentence ma-trix to obtain a first order feature map.
The threeoperations can be repeated to yield feature mapsof increasing order and a network of increasingdepth.
We denote a feature map of the i-th orderby Fi.
As in convolutional networks for objectrecognition, to increase the number of learnt fea-ture detectors of a certain order, multiple featuremaps Fi1, .
.
.
,Finmay be computed in parallel atthe same layer.
Each feature map Fijis computedby convolving a distinct set of filters arranged ina matrix mij,kwith each feature map Fi?1kof thelower order i?
1 and summing the results:659Fij=n?k=1mij,k?
Fi?1k(7)where ?
indicates the wide convolution.
Theweights mij,kform an order-4 tensor.
After thewide convolution, first dynamic k-max poolingand then the non-linear function are applied indi-vidually to each map.3.6 FoldingIn the formulation of the network so far, featuredetectors applied to an individual row of the sen-tence matrix s can have many orders and createcomplex dependencies across the same rows inmultiple feature maps.
Feature detectors in differ-ent rows, however, are independent of each otheruntil the top fully connected layer.
Full depen-dence between different rows could be achievedby making M in Eq.
5 a full matrix instead ofa sparse matrix of diagonals.
Here we explore asimpler method called folding that does not intro-duce any additional parameters.
After a convo-lutional layer and before (dynamic) k-max pool-ing, one just sums every two rows in a feature mapcomponent-wise.
For a map of d rows, folding re-turns a map of d/2 rows, thus halving the size ofthe representation.
With a folding layer, a featuredetector of the i-th order depends now on two rowsof feature values in the lower maps of order i?
1.This ends the description of the DCNN.4 Properties of the Sentence ModelWe describe some of the properties of the sentencemodel based on the DCNN.
We describe the no-tion of the feature graph induced over a sentenceby the succession of convolutional and poolinglayers.
We briefly relate the properties to those ofother neural sentence models.4.1 Word and n-Gram OrderOne of the basic properties is sensitivity to the or-der of the words in the input sentence.
For mostapplications and in order to learn fine-grained fea-ture detectors, it is beneficial for a model to be ableto discriminate whether a specific n-gram occursin the input.
Likewise, it is beneficial for a modelto be able to tell the relative position of the mostrelevant n-grams.
The network is designed to cap-ture these two aspects.
The filters m of the wideconvolution in the first layer can learn to recognisespecific n-grams that have size less or equal to thefilter width m; as we see in the experiments, m inthe first layer is often set to a relatively large valuesuch as 10.
The subsequence of n-grams extractedby the generalised pooling operation induces in-variance to absolute positions, but maintains theirorder and relative positions.As regards the other neural sentence models, theclass of NBoW models is by definition insensitiveto word order.
A sentence model based on a recur-rent neural network is sensitive to word order, butit has a bias towards the latest words that it takes asinput (Mikolov et al, 2011).
This gives the RNNexcellent performance at language modelling, butit is suboptimal for remembering at once the n-grams further back in the input sentence.
Sim-ilarly, a recursive neural network is sensitive toword order but has a bias towards the topmostnodes in the tree; shallower trees mitigate this ef-fect to some extent (Socher et al, 2013a).
As seenin Sect.
2.3, the Max-TDNN is sensitive to wordorder, but max pooling only picks out a single n-gram feature in each row of the sentence matrix.4.2 Induced Feature GraphSome sentence models use internal or externalstructure to compute the representation for the in-put sentence.
In a DCNN, the convolution andpooling layers induce an internal feature graphover the input.
A node from a layer is connectedto a node from the next higher layer if the lowernode is involved in the convolution that computesthe value of the higher node.
Nodes that are notselected by the pooling operation at a layer aredropped from the graph.
After the last poolinglayer, the remaining nodes connect to a single top-most root.
The induced graph is a connected, di-rected acyclic graph with weighted edges and aroot node; two equivalent representations of aninduced graph are given in Fig.
1.
In a DCNNwithout folding layers, each of the d rows of thesentence matrix induces a subgraph that joins theother subgraphs only at the root node.
Each sub-graph may have a different shape that reflects thekind of relations that are detected in that subgraph.The effect of folding layers is to join pairs of sub-graphs at lower layers before the top root node.Convolutional networks for object recognitionalso induce a feature graph over the input image.What makes the feature graph of a DCNN pecu-liar is the global range of the pooling operations.The (dynamic) k-max pooling operator can drawtogether features that correspond to words that aremany positions apart in the sentence.
Higher-orderfeatures have highly variable ranges that can be ei-660ther short and focused or global and long as theinput sentence.
Likewise, the edges of a subgraphin the induced graph reflect these varying ranges.The subgraphs can either be localised to one ormore parts of the sentence or spread more widelyacross the sentence.
This structure is internal tothe network and is defined by the forward propa-gation of the input through the network.Of the other sentence models, the NBoW is ashallow model and the RNN has a linear chainstructure.
The subgraphs induced in the Max-TDNN model have a single fixed-range feature ob-tained through max pooling.
The recursive neuralnetwork follows the structure of an external parsetree.
Features of variable range are computed ateach node of the tree combining one or more ofthe children of the tree.
Unlike in a DCNN, whereone learns a clear hierarchy of feature orders, ina RecNN low order features like those of sin-gle words can be directly combined with higherorder features computed from entire clauses.
ADCNN generalises many of the structural aspectsof a RecNN.
The feature extraction function asstated in Eq.
6 has a more general form than thatin a RecNN, where the value of m is generally 2.Likewise, the induced graph structure in a DCNNis more general than a parse tree in that it is notlimited to syntactically dictated phrases; the graphstructure can capture short or long-range seman-tic relations between words that do not necessar-ily correspond to the syntactic relations in a parsetree.
The DCNN has internal input-dependentstructure and does not rely on externally providedparse trees, which makes the DCNN directly ap-plicable to hard-to-parse sentences such as tweetsand to sentences from any language.5 ExperimentsWe test the network on four different experiments.We begin by specifying aspects of the implemen-tation and the training of the network.
We then re-late the results of the experiments and we inspectthe learnt feature detectors.5.1 TrainingIn each of the experiments, the top layer of thenetwork has a fully connected layer followed bya softmax non-linearity that predicts the probabil-ity distribution over classes given the input sen-tence.
The network is trained to minimise thecross-entropy of the predicted and true distribu-tions; the objective includes an L2regularisationClassifier Fine-grained (%) Binary (%)NB 41.0 81.8BINB 41.9 83.1SVM 40.7 79.4RECNTN 45.7 85.4MAX-TDNN 37.4 77.1NBOW 42.4 80.5DCNN 48.5 86.8Table 1: Accuracy of sentiment prediction in themovie reviews dataset.
The first four results arereported from Socher et al (2013b).
The baselinesNB and BINB are Naive Bayes classifiers with,respectively, unigram features and unigram and bi-gram features.
SVM is a support vector machinewith unigram and bigram features.
RECNTN is arecursive neural network with a tensor-based fea-ture function, which relies on external structuralfeatures given by a parse tree and performs bestamong the RecNNs.term over the parameters.
The set of parameterscomprises the word embeddings, the filter weightsand the weights from the fully connected layers.The network is trained with mini-batches by back-propagation and the gradient-based optimisation isperformed using the Adagrad update rule (Duchiet al, 2011).
Using the well-known convolutiontheorem, we can compute fast one-dimensionallinear convolutions at all rows of an input matrixby using Fast Fourier Transforms.
To exploit theparallelism of the operations, we train the networkon a GPU.
A Matlab implementation processesmultiple millions of input sentences per hour onone GPU, depending primarily on the number oflayers used in the network.5.2 Sentiment Prediction in Movie ReviewsThe first two experiments concern the predictionof the sentiment of movie reviews in the StanfordSentiment Treebank (Socher et al, 2013b).
Theoutput variable is binary in one experiment andcan have five possible outcomes in the other: neg-ative, somewhat negative, neutral, somewhat posi-tive, positive.
In the binary case, we use the givensplits of 6920 training, 872 development and 1821test sentences.
Likewise, in the fine-grained case,we use the standard 8544/1101/2210 splits.
La-belled phrases that occur as subparts of the train-ing sentences are treated as independent traininginstances.
The size of the vocabulary is 15448.Table 1 details the results of the experiments.661Classifier Features Acc.
(%)HIERunigram, POS, head chunks 91.0NE, semantic relationsMAXENTunigram, bigram, trigram 92.6POS, chunks, NE, supertagsCCG parser, WordNetMAXENTunigram, bigram, trigram 93.6POS, wh-word, head wordword shape, parserhypernyms, WordNetSVMunigram, POS, wh-word 95.0head word, parserhypernyms, WordNet60 hand-coded rulesMAX-TDNN unsupervised vectors 84.4NBOW unsupervised vectors 88.2DCNN unsupervised vectors 93.0Table 2: Accuracy of six-way question classifica-tion on the TREC questions dataset.
The secondcolumn details the external features used in thevarious approaches.
The first four results are re-spectively from Li and Roth (2002), Blunsom et al(2006), Huang et al (2008) and Silva et al (2011).In the three neural sentence models?the Max-TDNN, the NBoW and the DCNN?the word vec-tors are parameters of the models that are ran-domly initialised; their dimension d is set to 48.The Max-TDNN has a filter of width 6 in its nar-row convolution at the first layer; shorter phrasesare padded with zero vectors.
The convolu-tional layer is followed by a non-linearity, a max-pooling layer and a softmax classification layer.The NBoW sums the word vectors and applies anon-linearity followed by a softmax classificationlayer.
The adopted non-linearity is the tanh func-tion.
The hyper parameters of the DCNN are asfollows.
The binary result is based on a DCNNthat has a wide convolutional layer followed by afolding layer, a dynamic k-max pooling layer anda non-linearity; it has a second wide convolutionallayer followed by a folding layer, a k-max poolinglayer and a non-linearity.
The width of the convo-lutional filters is 7 and 5, respectively.
The valueof k for the top k-max pooling is 4.
The num-ber of feature maps at the first convolutional layeris 6; the number of maps at the second convolu-tional layer is 14.
The network is topped by a soft-max classification layer.
The DCNN for the fine-grained result has the same architecture, but thefilters have size 10 and 7, the top pooling parame-ter k is 5 and the number of maps is, respectively,6 and 12.
The networks use the tanh non-linearClassifier Accuracy (%)SVM 81.6BINB 82.7MAXENT 83.0MAX-TDNN 78.8NBOW 80.9DCNN 87.4Table 3: Accuracy on the Twitter sentimentdataset.
The three non-neural classifiers are basedon unigram and bigram features; the results are re-ported from (Go et al, 2009).function.
At training time we apply dropout to thepenultimate layer after the last tanh non-linearity(Hinton et al, 2012).We see that the DCNN significantly outper-forms the other neural and non-neural models.The NBoW performs similarly to the non-neuraln-gram based classifiers.
The Max-TDNN per-forms worse than the NBoW likely due to the ex-cessive pooling of the max pooling operation; thelatter discards most of the sentiment features of thewords in the input sentence.
Besides the RecNNthat uses an external parser to produce structuralfeatures for the model, the other models use n-gram based or neural features that do not requireexternal resources or additional annotations.
In thenext experiment we compare the performance ofthe DCNN with those of methods that use heavilyengineered resources.5.3 Question Type ClassificationAs an aid to question answering, a question maybe classified as belonging to one of many questiontypes.
The TREC questions dataset involves sixdifferent question types, e.g.
whether the questionis about a location, about a person or about somenumeric information (Li and Roth, 2002).
Thetraining dataset consists of 5452 labelled questionswhereas the test dataset consists of 500 questions.The results are reported in Tab.
2.
The non-neural approaches use a classifier over a largenumber of manually engineered features andhand-coded resources.
For instance, Blunsom etal.
(2006) present a Maximum Entropy model thatrelies on 26 sets of syntactic and semantic fea-tures including unigrams, bigrams, trigrams, POStags, named entity tags, structural relations froma CCG parse and WordNet synsets.
We evaluatethe three neural models on this dataset with mostlythe same hyper-parameters as in the binary senti-662POSITIVElovely	 	 	 	 	 comedic	 	 	 	 	 moments	 and	 	 	 	 several	 	 	 	 	 fine	 	 	 	 	 	 performancesgood	 	 	 	 	 	 	 script	 	 	 	 	 	 ,	 	 	 	 	 	 	 good	 	 	 dialogue	 	 	 	 ,	 	 	 	 	 	 	 	 	 funnysustains	 	 	 throughout	 	 is	 	 	 	 	 	 daring	 ,	 	 	 	 	 	 	 	 	 	 	 inventive	 andwell	 	 	 	 	 	 	 written	 	 	 	 	 ,	 	 	 	 	 	 	 nicely	 acted	 	 	 	 	 	 	 and	 	 	 	 	 	 	 beautifullyremarkably	 solid	 	 	 	 	 	 	 and	 	 	 	 	 subtly	 satirical	 	 	 tour	 	 	 	 	 	 deNEGATIVE,	 	 	 	 	 	 	 	 	 	 nonexistent	 plot	 	 	 	 and	 	 	 	 pretentious	 visual	 	 	 	 styleit	 	 	 	 	 	 	 	 	 fails	 	 	 	 	 	 	 the	 	 	 	 	 most	 	 	 basic	 	 	 	 	 	 	 test	 	 	 	 	 	 asso	 	 	 	 	 	 	 	 	 stupid	 	 	 	 	 	 ,	 	 	 	 	 	 	 so	 	 	 	 	 ill	 	 	 	 	 	 	 	 	 conceived	 ,,	 	 	 	 	 	 	 	 	 	 too	 	 	 	 	 	 	 	 	 dull	 	 	 	 and	 	 	 	 pretentious	 to	 	 	 	 	 	 	 	 behood	 	 	 	 	 	 	 rats	 	 	 	 	 	 	 	 butt	 	 	 	 their	 	 ugly	 	 	 	 	 	 	 	 heads	 	 	 	 	 in'NOT'n't	 	 	 	 have	 	 	 	 	 any	 	 	 	 	 	 	 	 	 huge	 laughs	 	 	 	 	 	 in	 	 	 	 	 	 	 	 	 	 	 itsno	 	 	 	 	 movement	 ,	 	 	 	 	 	 	 	 	 	 	 no	 	 	 ,	 	 	 	 	 	 	 	 	 	 	 not	 	 	 	 	 	 	 	 	 	 muchn't	 	 	 	 stop	 	 	 	 	 me	 	 	 	 	 	 	 	 	 	 from	 enjoying	 	 	 	 much	 	 	 	 	 	 	 	 	 ofnot	 	 	 	 that	 	 	 	 	 kung	 	 	 	 	 	 	 	 pow	 	 is	 	 	 	 	 	 	 	 	 	 n't	 	 	 	 	 	 	 	 	 	 funnynot	 	 	 	 a	 	 	 	 	 	 	 	 moment	 	 	 	 	 	 that	 is	 	 	 	 	 	 	 	 	 	 not	 	 	 	 	 	 	 	 	 	 false'TOO',	 	 	 	 	 	 too	 	 	 	 	 	 dull	 	 	 	 	 	 	 	 and	 	 pretentious	 to	 	 	 	 	 	 	 	 	 	 	 beeither	 too	 	 	 	 	 	 serious	 	 	 	 	 or	 	 	 too	 	 	 	 	 	 	 	 	 lighthearted	 ,too	 	 	 	 slow	 	 	 	 	 ,	 	 	 	 	 	 	 	 	 	 	 too	 	 long	 	 	 	 	 	 	 	 and	 	 	 	 	 	 	 	 	 	 toofeels	 	 too	 	 	 	 	 	 formulaic	 	 	 and	 	 too	 	 	 	 	 	 	 	 	 familiar	 	 	 	 	 tois	 	 	 	 	 too	 	 	 	 	 	 predictable	 and	 	 too	 	 	 	 	 	 	 	 	 self	 	 	 	 	 	 	 	 	 consciousFigure 4: Top five 7-grams at four feature detectors in the first layer of the network.ment experiment of Sect.
5.2.
As the dataset israther small, we use lower-dimensional word vec-tors with d = 32 that are initialised with embed-dings trained in an unsupervised way to predictcontexts of occurrence (Turian et al, 2010).
TheDCNN uses a single convolutional layer with fil-ters of size 8 and 5 feature maps.
The differencebetween the performance of the DCNN and that ofthe other high-performing methods in Tab.
2 is notsignificant (p < 0.09).
Given that the only labelledinformation used to train the network is the train-ing set itself, it is notable that the network matchesthe performance of state-of-the-art classifiers thatrely on large amounts of engineered features andrules and hand-coded resources.5.4 Twitter Sentiment Prediction withDistant SupervisionIn our final experiment, we train the models on alarge dataset of tweets, where a tweet is automat-ically labelled as positive or negative dependingon the emoticon that occurs in it.
The training setconsists of 1.6 million tweets with emoticon-basedlabels and the test set of about 400 hand-annotatedtweets.
We preprocess the tweets minimally fol-lowing the procedure described in Go et al (2009);in addition, we also lowercase all the tokens.
Thisresults in a vocabulary of 76643 word types.
Thearchitecture of the DCNN and of the other neuralmodels is the same as the one used in the binaryexperiment of Sect.
5.2.
The randomly initialisedword embeddings are increased in length to a di-mension of d = 60.
Table 3 reports the results ofthe experiments.
We see a significant increase inthe performance of the DCNN with respect to thenon-neural n-gram based classifiers; in the pres-ence of large amounts of training data these clas-sifiers constitute particularly strong baselines.
Wesee that the ability to train a sentiment classifier onautomatically extracted emoticon-based labels ex-tends to the DCNN and results in highly accurateperformance.
The difference in performance be-tween the DCNN and the NBoW further suggeststhat the ability of the DCNN to both capture fea-tures based on long n-grams and to hierarchicallycombine these features is highly beneficial.5.5 Visualising Feature DetectorsA filter in the DCNN is associated with a featuredetector or neuron that learns during training tobe particularly active when presented with a spe-cific sequence of input words.
In the first layer, thesequence is a continuous n-gram from the inputsentence; in higher layers, sequences can be madeof multiple separate n-grams.
We visualise thefeature detectors in the first layer of the networktrained on the binary sentiment task (Sect.
5.2).Since the filters have width 7, for each of the 288feature detectors we rank all 7-grams occurring inthe validation and test sets according to their ac-tivation of the detector.
Figure 5.2 presents thetop five 7-grams for four feature detectors.
Be-sides the expected detectors for positive and nega-tive sentiment, we find detectors for particles suchas ?not?
that negate sentiment and such as ?too?that potentiate sentiment.
We find detectors formultiple other notable constructs including ?all?,?or?, ?with...that?, ?as...as?.
The feature detectorslearn to recognise not just single n-grams, but pat-terns within n-grams that have syntactic, semanticor structural significance.6 ConclusionWe have described a dynamic convolutional neuralnetwork that uses the dynamic k-max pooling op-erator as a non-linear subsampling function.
Thefeature graph induced by the network is able tocapture word relations of varying size.
The net-work achieves high performance on question andsentiment classification without requiring externalfeatures as provided by parsers or other resources.AcknowledgementsWe thank Nando de Freitas and Yee Whye Tehfor great discussions on the paper.
This work wassupported by a Xerox Foundation Award, EPSRCgrant number EP/F042728/1, and EPSRC grantnumber EP/K036580/1.663ReferencesMarco Baroni and Roberto Zamparelli.
2010.
Nounsare vectors, adjectives are matrices: Representingadjective-noun constructions in semantic space.
InEMNLP, pages 1183?1193.
ACL.Phil Blunsom, Krystle Kocik, and James R. Curran.2006.
Question classification with log-linear mod-els.
In SIGIR ?06: Proceedings of the 29th an-nual international ACM SIGIR conference on Re-search and development in information retrieval,pages 615?616, New York, NY, USA.
ACM.Daoud Clarke.
2012.
A context-theoretic frame-work for compositionality in distributional seman-tics.
Computational Linguistics, 38(1):41?71.Bob Coecke, Mehrnoosh Sadrzadeh, and StephenClark.
2010.
Mathematical Foundations for a Com-positional Distributional Model of Meaning.
March.Ronan Collobert and Jason Weston.
2008.
A unifiedarchitecture for natural language processing: Deepneural networks with multitask learning.
In Interna-tional Conference on Machine Learning, ICML.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learningand stochastic optimization.
J. Mach.
Learn.
Res.,12:2121?2159, July.Katrin Erk and Sebastian Pad?o.
2008.
A structuredvector space model for word meaning in context.Proceedings of the Conference on Empirical Meth-ods in Natural Language Processing - EMNLP ?08,(October):897.Katrin Erk.
2012.
Vector space models of word mean-ing and phrase meaning: A survey.
Language andLinguistics Compass, 6(10):635?653.Felix A. Gers and Jrgen Schmidhuber.
2001.
Lstmrecurrent networks learn simple context-free andcontext-sensitive languages.
IEEE Transactions onNeural Networks, 12(6):1333?1340.Alec Go, Richa Bhayani, and Lei Huang.
2009.
Twit-ter sentiment classification using distant supervision.Processing, pages 1?6.Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011.Experimental support for a categorical composi-tional distributional model of meaning.
In Proceed-ings of the Conference on Empirical Methods in Nat-ural Language Processing, pages 1394?1404.
Asso-ciation for Computational Linguistics.Edward Grefenstette.
2013.
Category-theoreticquantitative compositional distributional modelsof natural language semantics.
arXiv preprintarXiv:1311.1539.Emiliano Guevara.
2010.
Modelling Adjective-NounCompositionality by Regression.
ESSLLI?10 Work-shop on Compositionality and Distributional Se-mantic Models.Karl Moritz Hermann and Phil Blunsom.
2013.
TheRole of Syntax in Vector Space Models of Composi-tional Semantics.
In Proceedings of the 51st AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), Sofia, Bulgaria,August.
Association for Computational Linguistics.Forthcoming.Geoffrey E. Hinton, Nitish Srivastava, AlexKrizhevsky, Ilya Sutskever, and Ruslan Salakhut-dinov.
2012.
Improving neural networks bypreventing co-adaptation of feature detectors.CoRR, abs/1207.0580.Geoffrey E. Hinton.
1989.
Connectionist learning pro-cedures.
Artif.
Intell., 40(1-3):185?234.Zhiheng Huang, Marcus Thint, and Zengchang Qin.2008.
Question classification using head words andtheir hypernyms.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Pro-cessing, EMNLP ?08, pages 927?936, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Nal Kalchbrenner and Phil Blunsom.
2013a.
Recur-rent continuous translation models.
In Proceedingsof the 2013 Conference on Empirical Methods inNatural Language Processing, Seattle, October.
As-sociation for Computational Linguistics.Nal Kalchbrenner and Phil Blunsom.
2013b.
Recur-rent Convolutional Neural Networks for DiscourseCompositionality.
In Proceedings of the Workshopon Continuous Vector Space Models and their Com-positionality, Sofia, Bulgaria, August.
Associationfor Computational Linguistics.Dimitri Kartsaklis and Mehrnoosh Sadrzadeh.
2013.Prior disambiguation of word tensors for construct-ing sentence vectors.
In Proceedings of the 2013Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP), Seattle, USA, October.Andreas K?uchler and Christoph Goller.
1996.
Induc-tive learning in symbolic domains using structure-driven recurrent neural networks.
In G?unther G?orzand Steffen H?olldobler, editors, KI, volume 1137 ofLecture Notes in Computer Science, pages 183?197.Springer.Yann LeCun, L?eon Bottou, Yoshua Bengio, and PatrickHaffner.
1998.
Gradient-based learning applied todocument recognition.
Proceedings of the IEEE,86(11):2278?2324, November.Xin Li and Dan Roth.
2002.
Learning question clas-sifiers.
In Proceedings of the 19th internationalconference on Computational linguistics-Volume 1,pages 1?7.
Association for Computational Linguis-tics.Tomas Mikolov and Geoffrey Zweig.
2012.
Contextdependent recurrent neural network language model.In SLT, pages 234?239.664Tomas Mikolov, Stefan Kombrink, Lukas Burget, JanCernock?y, and Sanjeev Khudanpur.
2011.
Exten-sions of recurrent neural network language model.In ICASSP, pages 5528?5531.
IEEE.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
In Proceedings ofACL, volume 8.Jeff Mitchell and Mirella Lapata.
2010.
Compositionin distributional models of semantics.
Cognitive Sci-ence, 34(8):1388?1429.Jordan B. Pollack.
1990.
Recursive distributed repre-sentations.
Artificial Intelligence, 46:77?105.Holger Schwenk.
2012.
Continuous space translationmodels for phrase-based statistical machine transla-tion.
In COLING (Posters), pages 1071?1080.Joo Silva, Lusa Coheur, AnaCristina Mendes, and An-dreas Wichert.
2011.
From symbolic to sub-symbolic information in question classification.
Ar-tificial Intelligence Review, 35(2):137?154.Richard Socher, Jeffrey Pennington, Eric H. Huang,Andrew Y. Ng, and Christopher D. Manning.
2011.Semi-Supervised Recursive Autoencoders for Pre-dicting Sentiment Distributions.
In Proceedings ofthe 2011 Conference on Empirical Methods in Nat-ural Language Processing (EMNLP).Richard Socher, Quoc V. Le, Christopher D. Manning,and Andrew Y. Ng.
2013a.
Grounded Composi-tional Semantics for Finding and Describing Imageswith Sentences.
In Transactions of the Associationfor Computational Linguistics (TACL).Richard Socher, Alex Perelygin, Jean Wu, JasonChuang, Christopher D. Manning, Andrew Y. Ng,and Christopher Potts.
2013b.
Recursive deep mod-els for semantic compositionality over a sentimenttreebank.
In Proceedings of the 2013 Conference onEmpirical Methods in Natural Language Process-ing, pages 1631?1642, Stroudsburg, PA, October.Association for Computational Linguistics.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: a simple and general methodfor semi-supervised learning.
In Proceedings of the48th Annual Meeting of the Association for Compu-tational Linguistics, pages 384?394.
Association forComputational Linguistics.Peter Turney.
2012.
Domain and function: A dual-space model of semantic relations and compositions.J.
Artif.
Intell.
Res.
(JAIR), 44:533?585.Alexander Waibel, Toshiyuki Hanazawa, Geofrey Hin-ton, Kiyohiro Shikano, and Kevin J. Lang.
1990.Readings in speech recognition.
chapter PhonemeRecognition Using Time-delay Neural Networks,pages 393?404.
Morgan Kaufmann Publishers Inc.,San Francisco, CA, USA.Fabio Massimo Zanzotto, Ioannis Korkontzelos,Francesca Fallucchi, and Suresh Manandhar.
2010.Estimating linear models for compositional distri-butional semantics.
In Proceedings of the 23rd In-ternational Conference on Computational Linguis-tics, pages 1263?1271.
Association for Computa-tional Linguistics.Luke S. Zettlemoyer and Michael Collins.
2005.Learning to map sentences to logical form: Struc-tured classification with probabilistic categorialgrammars.
In UAI, pages 658?666.
AUAI Press.665
