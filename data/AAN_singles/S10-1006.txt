Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 33?38,Uppsala, Sweden, 15-16 July 2010.c?2010 Association for Computational LinguisticsSemEval-2010 Task 8: Multi-Way Classificationof Semantic Relations Between Pairs of NominalsIris Hendrickx?, Su Nam Kim?, Zornitsa Kozareva?, Preslav Nakov?,Diarmuid?O S?eaghdha?, Sebastian Pad?o?, Marco Pennacchiotti?
?,Lorenza Romano?
?, Stan Szpakowicz?
?AbstractSemEval-2 Task 8 focuses on Multi-wayclassification of semantic relations betweenpairs of nominals.
The task was designedto compare different approaches to seman-tic relation classification and to provide astandard testbed for future research.
Thispaper defines the task, describes the train-ing and test data and the process of theircreation, lists the participating systems (10teams, 28 runs), and discusses their results.1 IntroductionSemEval-2010 Task 8 focused on semantic rela-tions between pairs of nominals.
For example, teaand ginseng are in an ENTITY-ORIGIN relation in?The cup contained tea from dried ginseng.?.
Theautomatic recognition of semantic relations hasmany applications, such as information extraction,document summarization, machine translation, orconstruction of thesauri and semantic networks.It can also facilitate auxiliary tasks such as wordsense disambiguation, language modeling, para-phrasing, and recognizing textual entailment.Our goal was to create a testbed for automaticclassification of semantic relations.
In developingthe task we met several challenges: selecting asuitable set of relations, specifying the annotationprocedure, and deciding on the details of the taskitself.
They are discussed briefly in Section 2; seealso Hendrickx et al (2009), which includes a sur-vey of related work.
The direct predecessor of Task8 was Classification of semantic relations betweennominals, Task 4 at SemEval-1 (Girju et al, 2009),?University of Lisbon, iris@clul.ul.pt?University of Melbourne, snkim@csse.unimelb.edu.au?Information Sciences Institute/University of SouthernCalifornia, kozareva@isi.edu?National University of Singapore, nakov@comp.nus.edu.sg?University of Cambridge, do242@cl.cam.ac.uk?University of Stuttgart, pado@ims.uni-stuttgart.de??Yahoo!
Inc., pennacc@yahoo-inc.com?
?Fondazione Bruno Kessler, romano@fbk.eu?
?University of Ottawa and Polish Academy of Sciences,szpak@site.uottawa.cawhich had a separate binary-labeled dataset foreach of seven relations.
We have defined SemEval-2010 Task 8 as a multi-way classification task inwhich the label for each example must be chosenfrom the complete set of ten relations and the map-ping from nouns to argument slots is not providedin advance.
We also provide more data: 10,717 an-notated examples, compared to 1,529 in SemEval-1Task 4.2 Dataset Creation2.1 The Inventory of Semantic RelationsWe first decided on an inventory of semantic rela-tions.
Ideally, it should be exhaustive (enable thedescription of relations between any pair of nomi-nals) and mutually exclusive (each pair of nominalsin context should map onto only one relation).
Theliterature, however, suggests that no relation inven-tory satisfies both needs, and, in practice, sometrade-off between them must be accepted.As a pragmatic compromise, we selected ninerelations with coverage sufficiently broad to be ofgeneral and practical interest.
We aimed at avoid-ing semantic overlap as much as possible.
Weincluded, however, two groups of strongly relatedrelations (ENTITY-ORIGIN / ENTITY-DESTINA-TION and CONTENT-CONTAINER / COMPONENT-WHOLE / MEMBER-COLLECTION) to assess mod-els?
ability to make such fine-grained distinctions.Our inventory is given below.
The first four werealso used in SemEval-1 Task 4, but the annotationguidelines have been revised, and thus no completecontinuity should be assumed.Cause-Effect (CE).
An event or object leads to aneffect.
Example: those cancers were causedby radiation exposuresInstrument-Agency (IA).
An agent uses an in-strument.
Example: phone operatorProduct-Producer (PP).
A producer causes aproduct to exist.
Example: a factory manu-factures suits33Content-Container (CC).
An object is physicallystored in a delineated area of space.
Example:a bottle full of honey was weighedEntity-Origin (EO).
An entity is coming or is de-rived from an origin (e.g., position or mate-rial).
Example: letters from foreign countriesEntity-Destination (ED).
An entity is moving to-wards a destination.
Example: the boy wentto bedComponent-Whole (CW).
An object is a com-ponent of a larger whole.
Example: myapartment has a large kitchenMember-Collection (MC).
A member forms anonfunctional part of a collection.
Example:there are many trees in the forestMessage-Topic (MT).
A message, written or spo-ken, is about a topic.
Example: the lecturewas about semantics2.2 Annotation GuidelinesWe defined a set of general annotation guidelinesas well as detailed guidelines for each semanticrelation.
Here, we describe the general guidelines,which delineate the scope of the data to be col-lected and state general principles relevant to theannotation of all relations.1Our objective is to annotate instances of seman-tic relations which are true in the sense of hold-ing in the most plausible truth-conditional inter-pretation of the sentence.
This is in the traditionof the Textual Entailment or Information Valida-tion paradigm (Dagan et al, 2009), and in con-trast to ?aboutness?
annotation such as semanticroles (Carreras and M`arquez, 2004) or the BioNLP2009 task (Kim et al, 2009) where negated rela-tions are also labelled as positive.
Similarly, weexclude instances of semantic relations which holdonly in speculative or counterfactural scenarios.
Inpractice, this means disallowing annotations withinthe scope of modals or negations, e.g., ?Smokingmay/may not have caused cancer in this case.
?We accept as relation arguments only nounphrases with common-noun heads.
This distin-guishes our task from much work in InformationExtraction, which tends to focus on specific classesof named entities and on considerably more fine-grained relations than we do.
Named entities are aspecific category of nominal expressions best dealt1The full task guidelines are available at http://docs.google.com/View?id=dfhkmm46_0f63mfvf7with using techniques which do not apply to com-mon nouns.
We only mark up the semantic heads ofnominals, which usually span a single word, exceptfor lexicalized terms such as science fiction.We also impose a syntactic locality requirementon example candidates, thus excluding instanceswhere the relation arguments occur in separate sen-tential clauses.
Permissible syntactic patterns in-clude simple and relative clauses, compounds, andpre- and post-nominal modification.
In addition,we did not annotate examples whose interpretationrelied on discourse knowledge, which led to theexclusion of pronouns as arguments.
Please seethe guidelines for details on other issues, includ-ing noun compounds, aspectual phenomena andtemporal relations.2.3 The Annotation ProcessThe annotation took place in three rounds.
First,we manually collected around 1,200 sentences foreach relation through pattern-based Web search.
Inorder to ensure a wide variety of example sentences,we used a substantial number of patterns for eachrelation, typically between one hundred and severalhundred.
Importantly, in the first round, the relationitself was not annotated: the goal was merely tocollect positive and near-miss candidate instances.A rough aim was to have 90% of candidates whichinstantiate the target relation (?positive instances?
).In the second round, the collected candidates foreach relation went to two independent annotatorsfor labeling.
Since we have a multi-way classifi-cation task, the annotators used the full inventoryof nine relations plus OTHER.
The annotation wasmade easier by the fact that the cases of overlapwere largely systematic, arising from general phe-nomena like metaphorical use and situations wheremore than one relation holds.
For example, there isa systematic potential overlap between CONTENT-CONTAINER and ENTITY-DESTINATION depend-ing on whether the situation described in the sen-tence is static or dynamic, e.g., ?When I came,the <e1>apples</e1> were already put in the<e2>basket</e2>.?
is CC(e1, e2), while ?Then,the <e1>apples</e1> were quickly put in the<e2>basket</e2>.?
is ED(e1, e2).In the third round, the remaining disagreementswere resolved, and, if no consensus could beachieved, the examples were removed.
Finally, wemerged all nine datasets to create a set of 10,717instances.
We released 8,000 for training and kept34the rest for testing.2Table 1 shows some statistics about the dataset.The first column (Freq) shows the absolute and rel-ative frequencies of each relation.
The second col-umn (Pos) shows that the average share of positiveinstances was closer to 75% than to 90%, indicatingthat the patterns catch a substantial amount of ?near-miss?
cases.
However, this effect varies a lot acrossrelations, causing the non-uniform relation distribu-tion in the dataset (first column).3After the secondround, we also computed inter-annotator agreement(third column, IAA).
Inter-annotator agreementwas computed on the sentence level, as the per-centage of sentences for which the two annotationswere identical.
That is, these figures can be inter-preted as exact-match accuracies.
We do not reportKappa, since chance agreement on preselected can-didates is difficult to estimate.4IAA is between60% and 95%, again with large relation-dependentvariation.
Some of the relations were particularlyeasy to annotate, notably CONTENT-CONTAINER,which can be resolved through relatively clear cri-teria, despite the systematic ambiguity mentionedabove.
ENTITY-ORIGIN was the hardest relation toannotate.
We encountered ontological difficultiesin defining both Entity (e.g., in contrast to Effect)and Origin (as opposed to Cause).
Our numbersare on average around 10% higher than those re-ported by Girju et al (2009).
This may be a sideeffect of our data collection method.
To gather1,200 examples in realistic time, we had to seekproductive search query patterns, which invitedcertain homogeneity.
For example, many queriesfor CONTENT-CONTAINER centered on ?usual sus-pect?
such as box or suitcase.
Many instances ofMEMBER-COLLECTION were collected on the ba-sis of from available lists of collective names.3 The TaskThe participating systems had to solve the follow-ing task: given a sentence and two tagged nominals,predict the relation between those nominals and thedirection of the relation.We released a detailed scorer which outputs (1) aconfusion matrix, (2) accuracy and coverage, (3)2This set includes 891 examples from SemEval-1 Task 4.We re-annotated them and assigned them as the last examplesof our training dataset to ensure that the test set was unseen.3To what extent our candidate selection produces a biasedsample is a question that we cannot address within this paper.4We do not report Pos or IAA for OTHER, since OTHER isa pseudo-relation that was not annotated in its own right.
Thenumbers would therefore not be comparable to other relations.Relation Freq Pos IAACause-Effect 1331 (12.4%) 91.2% 79.0%Component-Whole 1253 (11.7%) 84.3% 70.0%Entity-Destination 1137 (10.6%) 80.1% 75.2%Entity-Origin 974 (9.1%) 69.2% 58.2%Product-Producer 948 (8.8%) 66.3% 84.8%Member-Collection 923 (8.6%) 74.7% 68.2%Message-Topic 895 (8.4%) 74.4% 72.4%Content-Container 732 (6.8%) 59.3% 95.8%Instrument-Agency 660 (6.2%) 60.8% 65.0%Other 1864 (17.4%) N/A4N/A4Total 10717 (100%)Table 1: Annotation Statistics.
Freq: Absolute andrelative frequency in the dataset; Pos: percentageof ?positive?
relation instances in the candidate set;IAA: inter-annotator agreementprecision (P), recall (R), and F1-Score for eachrelation, (4) micro-averaged P, R, F1, (5) macro-averaged P, R, F1.
For (4) and (5), the calculationsignored the OTHER relation.
Our official scoringmetric is macro-averaged F1-Score for (9+1)-wayclassification, taking directionality into account.The teams were asked to submit test data pre-dictions for varying fractions of the training data.Specifically, we requested results for the first 1000,2000, 4000, and 8000 training instances, calledTD1 through TD4.
TD4 was the full training set.4 Participants and ResultsTable 2 lists the participants and provides a roughoverview of the system features.
Table 3 shows theresults.
Unless noted otherwise, all quoted numbersare F1-Scores.Overall Ranking and Training Data.
We rankthe teams by the performance of their best systemon TD4, since a per-system ranking would favorteams with many submitted runs.
UTD submit-ted the best system, with a performance of over82%, more than 4% better than the second-bestsystem.
FBK IRST places second, with 77.62%,a tiny margin ahead of ISI (77.57%).
Notably, theISI system outperforms the FBK IRST system forTD1 to TD3, where it was second-best.
The accu-racy numbers for TD4 (Acc TD4) lead to the sameoverall ranking: micro- versus macro-averagingdoes not appear to make much difference either.A random baseline gives an uninteresting score of6%.
Our competitive baseline system is a simpleNaive Bayes classifier which relies on words in thesentential context only; two systems scored belowthis baseline.35System Institution Team Description Res.
Class.Baseline Task organizers local context of 2 words only BNECNU-SR-1 East China NormalUniversityMan Lan, YuanChen, ZhiminZhou, Yu Xustem, POS, syntactic patterns S SVM(multi)ECNU-SR-2,3 features like ECNU-SR-1, dif-ferent prob.
thresholdsSVM(binary)ECNU-SR-4 stem, POS, syntactic patterns,hyponymy and meronymy rela-tionsWN,SSVM(multi)ECNU-SR-5,6 features like ECNU-SR-4, dif-ferent prob.
thresholdsSVM(binary)ECNU-SR-7 majority vote of ECNU-1,2,4,5FBK IRST-6C32 Fondazione BrunoKesslerClaudio Giu-liano, KaterynaTymoshenko3-word window context features(word form, part of speech, or-thography) + Cyc; parameterestimation by optimization ontraining setCyc SVMFBK IRST-12C32 FBK IRST-6C32 + distance fea-turesFBK IRST-12VBC32 FBK IRST-12C32 + verbsFBK IRST-6CA,-12CA, -12VBCAfeatures as above, parameter es-timation by cross-validationFBK NK-RES1 Fondazione BrunoKesslerMatteo Negri,Milen Kouylekovcollocations, glosses, semanticrelations of nominals + contextfeaturesWN BNFBK NK-RES 2,3,4 like FBK NK-RES1 with differ-ent context windows and collo-cation cutoffsISI Information Sci-ences Institute,University ofSouthern Califor-niaStephen Tratz features from different re-sources, a noun compoundrelation system, and variousfeature related to capitalization,affixes, closed-class wordsWN,RT, GMEISTI-1,2 Istituto di sci-enca e tecnologiedell?informazione?A.
Faedo?Andrea Esuli,Diego Marcheg-giani, FabrizioSebastianiBoosting-based classification.Runs differ in their initializa-tion.WN 2SJU Jadavpur Univer-sitySantanu Pal, ParthaPakray, DipankarDas, Sivaji Bandy-opadhyayVerbs, nouns, and prepositions;seed lists for semantic relations;parse features and NEsWN,SCRFSEKA HungarianAcademy ofSciencesEszter Simon, An-dras KornaiLevin and Roget classes, n-grams; other grammatical andformal featuresRT,LCMETUD-base Technische Univer-sit?at DarmstadtGy?orgy Szarvas,Iryna Gurevychword, POS n-grams, depen-dency path, distanceS METUD-wp TUD-base + ESA semantic re-latedness scores+WPTUD-comb TUD-base + own semantic relat-edness scores+WP,WNTUD-comb-threshold TUD-comb with higher thresh-old for OTHERUNITN University ofTrentoFabio Celli punctuation, context words,prepositional patterns, estima-tion of semantic relation?
DRUTD University of Texasat DallasBryan Rink, SandaHarabagiucontext wods, hypernyms, POS,dependencies, distance, seman-tic roles, Levin classes, para-phrasesWN,S, G,PB/NB,LCSVM,2STable 2: Participants of SemEval-2010 Task 8.
Res: Resources used (WN: WordNet data; WP:Wikipedia data; S: syntax; LC: Levin classes; G: Google n-grams, RT: Roget?s Thesaurus, PB/NB:PropBank/NomBank).
Class: Classification style (ME: Maximum Entropy; BN: Bayes Net; DR: DecisionRules/Trees; CRF: Conditional Random Fields; 2S: two-step classification)36System TD1 TD2 TD3 TD4 Acc TD4 Rank Best Cat Worst Cat-9Baseline 33.04 42.41 50.89 57.52 50.0 - MC (75.1) IA (28.0)ECNU-SR-1 52.13 56.58 58.16 60.08 57.14CE (79.7) IA (32.2)ECNU-SR-2 46.24 47.99 69.83 72.59 67.1 CE (84.4) IA (52.2)ECNU-SR-3 39.89 42.29 65.47 68.50 62.0 CE (83.4) IA (46.5)ECNU-SR-4 67.95 70.58 72.99 74.82 70.5 CE (84.6) IA (61.4)ECNU-SR-5 49.32 50.70 72.63 75.43 70.2 CE (85.1) IA (60.7)ECNU-SR-6 42.88 45.54 68.87 72.19 65.8 CE (85.2) IA (56.7)ECNU-SR-7 58.67 58.87 72.79 75.21 70.2 CE (86.1) IA (61.8)FBK IRST-6C32 60.19 67.31 71.78 76.81 72.42ED (82.6) IA (69.4)FBK IRST-12C32 60.66 67.91 72.04 76.91 72.4 MC (84.2) IA (68.8)FBK IRST-12VBC32 62.64 69.86 73.19 77.11 72.3 ED (85.9) PP (68.1)FBK IRST-6CA 60.58 67.14 71.63 76.28 71.4 CE (82.3) IA (67.7)FBK IRST-12CA 61.33 67.80 71.65 76.39 71.4 ED (81.8) IA (67.5)FBK IRST-12VBCA 63.61 70.20 73.40 77.62 72.8 ED (86.5) IA (67.3)FBK NK-RES1 55.71?64.06?67.80?68.02 62.17ED (77.6) IA (52.9)FBK NK-RES2 54.27?63.68?67.08?67.48 61.4 ED (77.4) PP (55.2)FBK NK-RES3 54.25?62.73?66.11?66.90 60.5 MC (76.7) IA (56.3)FBK NK-RES4 44.11?58.85?63.06?65.84 59.4 MC (76.1) IA/PP (58.0)ISI 66.68 71.01 75.51 77.57 72.7 3 CE (87.6) IA (61.5)ISTI-1 50.49?55.80?61.14?68.42 63.26ED (80.7) PP (53.8)ISTI-2 50.69?54.29?59.77?66.65 61.5 ED (80.2) IA (48.9)JU 41.62?44.98?47.81?52.16 50.2 9 CE (75.6) IA (27.8)SEKA 51.81 56.34 61.10 66.33 61.9 8 CE (84.0) PP (43.7)TUD-base 50.81 54.61 56.98 60.50 56.15CE (80.7) IA (31.1)TUD-wp 55.34 60.90 63.78 68.00 63.5 ED (82.9) IA (44.1)TUD-comb 57.84 62.52 66.41 68.88 64.6 CE (83.8) IA (46.8)TUD-comb-?
58.35 62.45 66.86 69.23 65.4 CE (83.4) IA (46.9)UNITN 16.57?18.56?22.45?26.67 27.4 10 ED (46.4) PP (0)UTD 73.08 77.02 79.93 82.19 77.9 1 CE (89.6) IA (68.5)Table 3: F1-Score of all submitted systems on the test dataset as a function of training data: TD1=1000,TD2=2000, TD3=4000, TD4=8000 training examples.
Official results are calculated on TD4.
The resultsmarked with?were submitted after the deadline.
The best-performing run for each participant is italicized.As for the amount of training data, we see a sub-stantial improvement for all systems between TD1and TD4, with diminishing returns for the transi-tion between TD3 and TD4 for many, but not all,systems.
Overall, the differences between systemsare smaller for TD4 than they are for TD1.
Thespread between the top three systems is around 10%at TD1, but below 5% at TD4.
Still, there are cleardifferences in the influence of training data sizeeven among systems with the same overall archi-tecture.
Notably, ECNU-SR-4 is the second-bestsystem at TD1 (67.95%), but gains only 7% fromthe eightfold increase of the size of the training data.At the same time, ECNU-SR-3 improves from lessthan 40% to almost 69%.
The difference betweenthe systems is that ECNU-SR-4 uses a multi-wayclassifier including the class OTHER, while ECNU-SR-3 uses binary classifiers and assigns OTHERif no other relation was assigned with p>0.5.
Itappears that these probability estimates for classesare only reliable enough for TD3 and TD4.The Influence of System Architecture.
Almostall systems used either MaxEnt or SVM classifiers,with no clear advantage for either.
Similarly, twosystems, UTD and ISTI (rank 1 and 6) split the taskinto two classification steps (relation and direction),but the 2nd- and 3rd-ranked systems do not.
Theuse of a sequence model such as a CRF did notshow a benefit either.The systems use a variety of resources.
Gener-ally, richer feature sets lead to better performance(although the differences are often small ?
comparethe different FBK IRST systems).
This improve-ment can be explained by the need for semanticgeneralization from training to test data.
This needcan be addressed using WordNet (contrast ECNU-1to -3 with ECNU-4 to -6), the Google n-gram col-lection (see ISI and UTD), or a ?deep?
semanticresource (FBK IRST uses Cyc).
Yet, most of theseresources are also included in the less successfulsystems, so beneficial integration of knowledgesources into semantic relation classification seemsto be difficult.System Combination.
The differences betweenthe systems suggest that it might be possible toachieve improvements by building an ensemble37system.
When we combine the top three systems(UTD, FBK IRST-12VBCA, and ISI) by predict-ing their majority vote, or OTHER if there was none,we obtain a small improvement over the UTD sys-tem with an F1-Score of 82.79%.
A combination ofthe top five systems using the same method showsa worse performance, however (80.42%).
This sug-gests that the best system outperforms the rest bya margin that cannot be compensated with systemcombination, at least not with a crude majority vote.We see a similar pattern among the ECNU systems,where the ECNU-SR-7 combination system is out-performed by ECNU-SR-5, presumably since itincorporates the inferior ECNU-SR-1 system.Relation-specific Analysis.
We also analyze theperformance on individual relations, especially theextremes.
There are very stable patterns across allsystems.
The best relation (presumably the eas-iest to classify) is CE, far ahead of ED and MC.Notably, the performance for the best relation is75% or above for almost all systems, with compar-atively small differences between the systems.
Thehardest relation is generally IA, followed by PP.5Here, the spread among the systems is much larger:the highest-ranking systems outperform others onthe difficult relations.
Recall was the main prob-lem for both IA and PP: many examples of thesetwo relations are misclassified, most frequently asOTHER.
Even at TD4, these datasets seem to beless homogeneous than the others.
Intriguingly, PPshows a very high inter-annotator agreement (Ta-ble 1).
Its difficulty may therefore be due not toquestionable annotation, but to genuine variability,or at least the selection of difficult patterns by thedataset creator.
Conversely, MC, among the easiestrelations to model, shows only a modest IAA.Difficult Instances.
There were 152 examplesthat are classified incorrectly by all systems.
Weanalyze them, looking for sources of errors.
In ad-dition to a handful of annotation errors and someborderline cases, they are made up of instanceswhich illustrate the limits of current shallow mod-eling approaches in that they require more lexicalknowledge and complex reasoning.
A case in point:The bottle carrier converts your <e1>bottle</e1>into a <e2>canteen</e2>.
This instance ofOTHER is misclassified either as CC (due to the5The relation OTHER, which we ignore in the overall F1-score, does even worse, often below 40%.
This is to be ex-pected, since the OTHER examples in our datasets are nearmisses for other relations, thus making a very incoherent class.nominals) or as ED (because of the prepositioninto).
Another example: [...] <e1>Rudders</e1>are used by <e2>towboats</e2> and other ves-sels that require a high degree of manoeuvrability.This is an instance of CW misclassified as IA, prob-ably on account of the verb use which is a frequentindicator of an agentive relation.5 Discussion and ConclusionThere is little doubt that 19-way classification is anon-trivial challenge.
It is even harder when thedomain is lexical semantics, with its idiosyncrasies,and when the classes are not necessarily disjoint,despite our best intentions.
It speaks to the successof the exercise that the participating systems?
per-formance was generally high, well over an orderof magnitude above random guessing.
This maybe due to the impressive array of tools and lexical-semantic resources deployed by the participants.Section 4 suggests a few ways of interpretingand analyzing the results.
Long-term lessons willundoubtedly emerge from the workshop discussion.One optimistic-pessimistic conclusion concerns thesize of the training data.
The notable gain TD3?TD4 suggests that even more data would be helpful,but that is so much easier said than done: it tookthe organizers well in excess of 1000 person-hoursto pin down the problem, hone the guidelines andrelation definitions, construct sufficient amounts oftrustworthy training data, and run the task.ReferencesX.
Carreras and L. M`arquez.
2004.
Introduction tothe CoNLL-2004 shared task: Semantic role label-ing.
In Proc.
CoNLL-04, Boston, MA.I.
Dagan, B. Dolan, B. Magnini, and D. Roth.
2009.Recognizing textual entailment: Rational, evalua-tion and approaches.
Natural Language Engineer-ing, 15(4):i?xvii.R.
Girju, P. Nakov, V. Nastase, S. Szpakowicz, P. Tur-ney, and D. Yuret.
2009.
Classification of semanticrelations between nominals.
Language Resourcesand Evaluation, 43(2):105?121.I.
Hendrickx, S. Kim, Z. Kozareva, P. Nakov, D.?OS?eaghdha, S. Pad?o, M. Pennacchiotti, L. Romano,and S. Szpakowicz.
2009.
SemEval-2010 Task8: Multi-way classification of semantic relations be-tween pairs of nominals.
In Proc.
NAACL Workshopon Semantic Evaluations, Boulder, CO.J.
Kim, T. Ohta, S. Pyysalo, Y. Kano, and J. Tsujii.2009.
Overview of BioNLP?09 shared task on eventextraction.
In Proc.
BioNLP-09, Boulder, CO.38
