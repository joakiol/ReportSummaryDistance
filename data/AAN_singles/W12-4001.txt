Proceedings of the 3rd Workshop on the People?s Web Meets NLP, ACL 2012, pages 1?9,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsSentiment Analysis Using a Novel Human Computation GameClaudiu-Cristian Musat THISONE Alireza GhasemiArtificial Intelligence Laboratory (LIA)Ecole Polytechnique Fe?de?rale de Lausanne (EPFL)IN-Ecublens, 1015 Lausanne, Switzerlandfirstname.lastname@epfl.chBoi FaltingsAbstractIn this paper, we propose a novel human com-putation game for sentiment analysis.
Ourgame aims at annotating sentiments of a col-lection of text documents and simultaneouslyconstructing a highly discriminative lexicon ofpositive and negative phrases.Human computation games have been widelyused in recent years to acquire human knowl-edge and use it to solve problems which areinfeasible to solve by machine intelligence.We package the problems of lexicon construc-tion and sentiment detection as a single hu-man computation game.
We compare the re-sults obtained by the game with that of otherwell-known sentiment detection approaches.Obtained results are promising and show im-provements over traditional approaches.1 IntroductionWe propose a novel solution for the analysis of sen-timent expressed in text media.
Novel corpus basedand lexicon based sentiment analysis methods arecreated each year.
The continual emergence of con-ceptually similar methods for this known problemshows that a satisfactory solution has still not beenfound.
We believe that the lack of suitable labelleddata that could be used in machine learning tech-niques to train sentiment classifiers is one of the ma-jor reasons the field of sentiment analysis is not ad-vancing more rapidly.Recognizing that knowledge for understandingsentiment is common sense and does not require ex-perts, we plan to take a new approach where labelleddata is obtained from people using human computa-tion platforms and games.
We also prove that themethod can provide not only labelled texts, but peo-ple also help by selecting sentiment-expressing fea-tures that can generalize well.Human computation is a newly emergingparadigm.
It tries to solve large-scale problems byutilizing human knowledge and has proven usefulin solving various problems (Von Ahn and Dabbish,2004; Von Ahn, 2006; Von Ahn et al, 2006a).To obtain high quality solution from human com-putation, people should be motivated to make theirbest effort.
One way to incentivize people for sub-mitting high-quality results is to package the prob-lem at hand as a game and request people to playit.
This process is called gamification.
The gamedesign should be such that the solution to the mainproblems can be formed by appropriately aggregat-ing results of played games.In this work, we propose a cooperative humancomputation game for sentiment analysis calledGuesstiment.
It aims at annotating sentiment ofa collection of text documents, and simultaneouslyconstructing a lexicon of highly polarized (positiveand negative) words which can further be used forsentiment detection tasks.
By playing a collabora-tive game, people rate hotel reviews as positive andnegative and select words and phrases within the re-views that best express the chosen polarity.We compare these annotations with those ob-tained during a former crowd-sourcing survey andprove that packaging the problem as a game can im-prove the quality of the responses.
We also com-pare our approach with the state-of-the-art machine1learning techniques and prove the superiority of hu-man cognition for this task.
In a third experimentwe use the same annotations in a multi faceted opin-ion classification problem and find that results aresuperior to those obtained using known linguistic re-sources.In (section 2) we review the literature related toour work.
We then outline the game and its rules(section 3).
We compare the Guesstiment resultsto the state-of-the-art machine learning, standardcrowd-sourcing methods and sentiment dictionar-ies(section 4) and conclude the paper with ideas forfuture work (section 5).2 Related WorkIn this section we review the important literature re-lated and similar to our work.
Sine we propose ahuman computation approach for sentiment analy-sis, we start by reviewing the literature on humancomputation and the closely related field of crowd-sourcing.
Then we move on by having a brief lookon the human computation and knowledge acquisi-tion games proposed so far by the researchers.
Fi-nally, we briefly review major sentiment analysismethods utilized by the researchers.2.1 Human Computation and Crowd-SourcingThe literature on human computation is highly over-lapping with that of crowd-sourcing, as they areclosely connected.
The two terms are sometimesused interchangeably although they are slightly dif-ferent.
Crowd-sourcing in its broadest form, ?is theact of taking a job traditionally performed by a des-ignated agent (usually an employee) and outsourc-ing it to an undefined, generally large group of peo-ple in the form of an open call?
(Quinn and Bederson,2011; Howe, 2006).
Since the first use of the wordcrowd-sourcing by J. Howe (Howe, 2006), there hasbeen a lot of interest in this field due to the wide ac-cessibility of anonymous crowd workers across theweb.The work described in (Rumshisky, 2011) usescrowd-sourcing to perform word sense disambigua-tion on a corpus.
In (Vondrick et al, 2010), crowd-sourcing is used for video annotation.
Moreover,(Christophe et al, 2010) has used crowd-sourcingfor satellite image analysis.
(Settles, 2011a) is another approach which aimsat combining active learning with crowd-sourcingfor text classification.
The principal contribution oftheir work is that as well as document annotation,they use human computation also to perform fea-ture selection.
(Law et al, ) is another recent workwhich proposes a game for acquisition of attribute-value pairs from images.2.2 Human Computation GamesLuis Von Ahn, the pioneer of the field of humancomputation, designed a game to encourage play-ers to semantically annotate a large corpus of images(Von Ahn and Dabbish, 2004).
It was the first humancomputation game.Following Von Ahn?s work, more researcherswere encouraged to package computational prob-lems as joyful games and have a group of non-expert users play them (Von Ahn, 2006).
Verbosity(Von Ahn et al, 2006a) was designed with the goalof gathering common sense knowledge about words.KissKissBan (Ho et al, 2009) was another game forimage annotation.Peekaboom(Von Ahn et al, 2006b) aimed at im-age segmentation and ?Phrase Detectives?
(Cham-berlain et al, 2008) was used to help constructing ananamorphic corpus for NLP tasks.
Another humancomputation game is described in (Riek et al, 2011)whose purpose is semantic annotation of video data.2.3 Sentiment AnalysisThe field of sentiment analysis and classificationcurrently mostly deals with extracting sentimentfrom text data.
Various methods (Turney, 2002;Esuli and Sebastiani, 2006; Taboada et al, 2011;Pang et al, 2002) have been proposed for effec-tive and efficient sentiment extraction of large col-lections of text documents.Sentiment classification methods are usually di-vided into two main categories: Lexicon based tech-niques and methods based on machine learning.
Inlexicon-based methods, a rich lexicon of polarizedwords is used to find key sentences and phrases intext documents which can be used to describe senti-ment of the whole text (Taboada et al, 2011).
Ma-chine learning methods, on the other hand, treat thesentiment detection problem as a text classificationtask (Pang et al, 2002).2Most of the research has been oriented towardsfinding the overall polarity of whole documents.
Theproblem was broken down even more by using ofthe faceted opinion concept (Liu, 2010).
The goal ofthis attempt was to determine precisely what aspectsof the concepts the expressed opinions should belinked to.
We will use this distinction to assess ourmethod?s viability in both overall and multi facetedopinion analysis.The work in (Brew et al, 2010) is an attempt touse crowd-sourcing for sentiment analysis.
The au-thors use a crowd of volunteers for the analysis ofsentiments of economical news items.
Users provideannotations which are then used to learn a classifierto discriminate positive articles from negatives.
Ituses active learning to select a diverse set of arti-cles for annotations so that a generalizable, preciseclassifier can be learned from annotated data.
Thework in (Zhou et al, 2010) is another approach touse active learning to improve sentiment classifica-tion.
It uses a deep network to learn a sentimentclassifier in a semi-supervised manner.
Moreover,this method uses active learning to learn from unla-beled data which are the most informative samplesthat need to be labeled.Two more recent works that have focused on sen-timent classification by designing human compu-tation games are (Weichselbraun et al, 2011) and(Al-Subaihin et al, 2011).
In (Weichselbraun etal., 2011) the game ?Sentiment Quiz?
has been pro-posed that aims at finding the degree of polarity ofwords in a lexicon.
In each round of the game, theplayer is asked to vote about polarity of a givenwords from most negative to most positive.
Theplayer is score based on the agreement between hisvote and the votes of previous players.
?SentimentQuiz?
demands annotation in the word level andtherefore can only be used to construct a sentimentlexicon.Another work which aims at sentiment classifica-tion is (Al-Subaihin et al, 2011).
In this work, amulti-player game is proposed which aims at find-ing the sentiment of individual sentences.
The gameis played by three groups of two players each.
Eachteam is shown a sentence and its members are askedto highlight the sentiment carrying terms of the sen-tence separately and quickly.
The first team whoseplayers?
votes match wins and the current gameround finishes.
The game continues by introducingdifferent sentences to the teams and hence gathersinformation about polarity of terms and their corre-sponding context.3 The Proposed GameIn this section we propose a novel human compu-tation game called Guesstiment.
We use the infor-mation provided while playing this game to obtain areliable dataset of sentiment annotated data as wellas a lexicon of highly polarized positive and negativewords.Having two by-products as the result of playinginstead of merely trying to obtain document annota-tions is the most important contribution of Guessti-ment.
The idea of using crowd-sourcing for featureextraction has already been used in (Settles, 2011b),but not as a human computation game.
In the rest ofthe following section, we will discuss the game playand rules of Guesstiment.3.1 Rules of GuesstimentGuesstiment is a two-player asynchronous game.
Itaims at annotating a large corpus of text documents,similar to the goal of the ESP game in (Von Ahnand Dabbish, 2004) for images.
However, Guessti-ment does this in a different way because of itsrules and asynchronous approach.
The differencesallow Guesstiment to obtain more useful informa-tion from played game rounds than ESP does, sinceeach player contributes in providing a different typeof information.The two players of the game are called ?Sug-gester?
and ?Guesser?.
These roles are initializedrandomly and interchanged between the two playersafter each round of the game.The Suggester, who starts each round will begiven the whole text of a review document andhe/she is supposed to:1.
Decide whether the whole text is positive ornegative, i.e.
the author is praising about a sub-ject or criticising it.2.
Select a single word (or a sequence of words,as short as possible) which best describes thepolarity (positive or negative) he has selectedin part (1).
For example, when the negative po-larity is chosen, the word ?terrible?
would be3a good choice for the representative word (pro-vided that it is present in the text).The Guesser, on the other hand, will be givenonly the word (or word sequence) suggested by theSuggester (he won?t see the whole text) and he hasto guess polarity of the whole text just based onthat single word.
If the polarities suggested by thetwo players agree, they are both given some posi-tive score (based on factors described below) oth-erwise 0.
Then the roles are interchanged and thegame continues with a new document.
The guessercan also refuse to make a guess about polarity of thetext (when for example the suggested word is am-biguous or not very discriminative) in which casethe suggester has two more opportunities to suggestanother word from the text.Guesstiment is a cooperative game.
It means thatthe two players are not opponent and they both re-ceive equal score after each round (Not high scorefor one player and low score for the other).
There-fore, the Suggester should make his best efforts toselect the most polarized word from the test whichbest describes the selected sentiment or polarity.
TheUI screens for Suggester and Guesser are depicted infigures 1a and 1b respectively.3.1.1 ScoringThe score of each suggested word (or word se-quence) depends on a variety of factors, includ-ing the length of the sequence and its novelty, i.e.how many times it has already been selected byother players.
Suppose that the word sequence wis present in the current text document and also ithas been present in text documents of nw of previ-ously played game rounds.
Assuming w has beenselected kw time before current game round, the po-tential score of w, PSw is defined as:PSw =[1length(w)?
kwnw](1)In (1), length(w) is the length (number of words)of phrase w. Using this scoring strategy, players areencouraged to select as shortest phrases as possible.Single words that are not already selected by otherplayers will yield the highest score.Moreover, some words are not allowed as sug-gestions and will yield zero score regardless of theagreement in polarity judgments.
They are selectedby putting a threshold on the potential score ofwords and placing those with a score lower than thethreshold on the forbidden list.
These words are col-ored red in the text and are separately displayed inthe forbidden list.The cooperation between the Suggester and theGuesser requires an agreement between them.
Thisallows the game to collect precise annotations andsimultaneously build a good quality lexicon ofwords which are most important in detecting polar-ity of the text.The total score of each player is displayed onthe scoreboard at the bottom of the Suggest/Guessgraphical user interface.
The potential score of aword is also displayed while typing which allowsusers to avoid selecting words with low score.4 Experiments4.1 Implementation DetailsThe game was implemented as a traditional three-tier web application.
For data storage, we used theH2 embedded database which has proven to be fastenough for scientific information retrieval applica-tions.
For the server side of the application we usedthe Play!
framework, which is a lightweight easyto use framework for Java MVC web applicationframework.The client side is a Java Applet.
We used a serviceoriented approach to define interactions between theclient and the server so that the game play is de-fined as a sequence of HTTP requests between clientand server.
Using this approach,, client and serverare maximally separate and therefore various clientapplications can be written, for instance to run onsmart phones.4.2 Experimentation EnvironmentA total of 80000 review texts were extracted alongwith their corresponding ratings from the TripAdvi-sor website 1.
Among these, 1000 articles were ran-domly selected and inserted in the game databaseto be used in game rounds.
More than 20 playersplayed the game over the course of one month and697 annotations were collected during this period,1http://www.trip-advisor.com4(a) Suggester Role (b) Guesser RoleFigure 1: The Suggest/Guess UI.of which 312 are distinct.
The players were volun-teer students who played the game after an initialadvertisement in Facebook.
Almost all of them werenon-native English speakers, both graduate and un-dergraduate.For selecting articles for each round, a combina-tion of strategies were used.
From the set of docu-ments which have not already been labelled by anyof the players, we select the article with the leastdifference between number of positive and negative(as collected in the lexicon constructed so far) wordsso that we get the most information from annota-tions.
If all document have been annotated at leasttwo times, we make the selection among documentsfor which the two annotations disagree, so that wesolve disagreements by majority vote.4.3 Quality of AnnotationsFor each review text in the TripAdvisor website,there is a corresponding rating score given by thevery writer of the text.
These ratings, score the qual-ity of services of the mentioned hotel from the pointof view of the review writer.They give a score of 1(most negative) to 5 (most positive) to the describedhotel which is presumably correlated with the inher-ent sentiment of the review text written by the sameauthor for the same hotel.We used these review ratings as the ground truthto assess the quality of player annotations.
We con-sidered review ratings higher than 3 as having posi-tive sentiment and those with rating lower than 3 ashaving negative sentiment.
Review with rating equalto 3 were considered neutral and excluded from fur-ther experiments.
Let Ratei be the numerical ratingof document i, according to the above criteria, weaccept document i if and only if:Ratei ?
{1, 2, 4, 5} (2)As well as annotations provided by players of thegame, we also compared the ground truth to the re-sults of state-of-the-art machine learning techniquesadapted for sentiment analysis.
We considered senti-ment analysis as a typical binary classification prob-lem and used a simple bag of words approach forfeature extraction.For the learning algorithm, we used Support Vec-tor Machines (SVM) and the Na?
?ve Bayes meth-ods which are two well-known learning algorithmsfor text classification problems (Brew et al, 2010).In the SVM method, document feature vectors aretransformed to high-dimensional kernel space andthen a maximum margin separating hyperplane issought in the new kernel space.
Training of SVMis quadratic in the amount of training data and there-fore it is hardly feasible for large-scale problems.The Na?
?ve Bayes approach is another learningalgorithm which is simpler and faster to run thanSVM.
In this statistical method, feature words areconsidered independent random variables and Bayesrule is used to derive posterior probabilities of hav-ing positive and negatives sentiment for each docu-ment.
The sentiment with maximum posterior prob-ability is the predicted sentiment for the given docu-ment.Results of comparison between the ground truthand various annotation approaches are depicted intable 1.
For the game results, we aggregated differ-ent annotations for individual documents by major-5ity voting.
Moreover, for the machine learning algo-rithms we used cross-validation to adjust the param-eters.
Moreover, the results were computed by aver-aging 10-fold cross-validation results over all folds.Accuracy of each method is defined as:Accuracy = NcorrectNtotal (3).In equation (3), Ncorrect is the number of docu-ment with computed sentiment equal to the groundtruth and Ntotal is the total number of documents.
Itcan be seen in table 1 that our method outperformsmachine learning.4.4 Comparison with ClassicalCrowd-SourcingWe also made a comparison between our approachand simple crowd-sourcing.
For this goal, we usedthe results of a survey conducted in summer 2011.40 of the review texts were selected randomly fromthe whole dataset and given to a crowd of 27 studentto be annotated based on their sentiment.
Individualannotations for each document were aggregated us-ing majority voting.
The ground truth was computedin the same way as the previous section.We re-executed the Guesstiment in a period of oneweek using only those 40 reviews and compared thequality of the obtained annotations to that of the sur-vey (aggregated using majority voting).
Similar tothe survey, we aggregated annotations for individualdocuments by majority voting.The results, depicted in table 2, are quite promis-ing.
Accuracy of the simple crowd-sourcing was82.5% whereas gamification acquired an accuracy of100%.
We can see that merely packaging the prob-lem as a game significantly improves accuracy of theresults.We can infer from tables 1 and 2 that gamificationactually helps in obtaining good quality annotationresults.
Therefore, annotations derived from play-ers?
effort are highly reliable and can be used forfurther studies, discussed below.4.5 Comparison with Sentiment DictionaryPerformanceThe previous experiments proved the viability ofhuman computation for detecting the polarities ofTable 1: Comparison of Game Annotation Accura-cies With that of Automatic ClassifiersMethod AccuracyGame Collected Annotations 90.4Na?
?ve Bayes 80.5Logistic Regression 83.6SVM 82.8Table 2: Comparison between Quality of the Resultsof Gamification and Crowd-SourcingMethod AccuracyGame Collected Annotations 100Aggregated Crowd Votes 82.5whole documents.
Manual classification is howeverexpensive, even if it takes the form of a game.
Wetake a step further and use the result of the game,in the form of a sentiment dictionary, in a subse-quent automated classification task.
We comparethe Guesstiment dictionary with an established re-source, OpinionFinder (Wilson et al, 2005) in amulti faceted opinion classification problem.The OpinionFinder dictionary (OF) contains 8220entries representing tuples of English words, eitherin the original form or stemmed, and their mostlikely parts of speech.
Each tuple has an associatedpolarity which can be positive, negative or neutral.There are 5276 words in the original form in the dic-tionary that have a positive or negative polarity.
Bycontrast, the Guesstiment dictionary GS only con-tains 312 terms, nearly 17 times less than Opinion-Finder.
Of these, 175 words are negative and 137positive.
Each of the words within the two dictio-naries has an intrinsic polarity P (w),?w ?
D ={OF,GS}.The opinion extraction task is topic oriented.
Weextract faceted opinions (Liu, 2010) - occurrencesof sentiment that can be attached to a given topic orclass within a topic model zi ?
?, i ?
{1..k} wherek is the number of independent topics.
We used twosets of topics: the first is a statistical topic model ob-tained with Latent Dirichlet Allocation (Blei et al,2003) with k = 50 topics from which we retainedthe most probable 5 words for each topic and createdsets of topic relevant terms P{zi}.
The second set of6topic terms contains the most common 90 nouns inall available hotel reviews, which were afterwardsmanually separated into 11 classes.Many Guesstiment dictionary words, such as?value?
and ?gentleman?
bear meaning by them-selves (i.e.
are nouns) and are not useful in this anal-ysis.
However the great majority of the words areadjectives or adverbs.
This makes them useful forfaceted sentiment analysis.
We only consider combi-nations of topic words and opinion dictionary termsand the allowed combinations are based on gram-matical dependency chains:w1???
w2, w1 ?
P{zi}, i = {1..k}, w2 ?
D (4)obtained using the Stanford parser (De Marneffeand Manning, 2008).This binding brings confidence to the model andprevents the accidental misinterpretation of uni-grams.
Also, the higher granularity of the opiniondescription allows clustering users based on theirpreferences.We define a construct c relevant to a topic ziwithin a review r asczi ?
zi ?Dc = (w1, w2|w1 ?
P{zi}, w2 ?
D,w1???
w2)(5)The polarity of the said relevant construct is givenby the orientation of the contained dictionary word:P (c) = P (w2) (6)The polarity P of the opinion expressed within areview r ?
R with respect to a topic zi is defined asthe sum of the polarities of constructs relevant to zi.This allows us to assess the strength of the opinionexpressed with regard to a topic.P : R?
?
7?
RP (r, zi) =?r P (czi), i = {1..k}(7)while the overall polarity of the review is the sumof all topic dependent polaritiesP : R 7?
RP (r) =?ki=1 P (r, zi)(8)We test whether the method assigns positive over-all polarities to reviews which have high (4 and 5)numeric ratings nr(r) and negative to those with lowones (1 and 2).
We compare the precision and recallof the method using both dictionaries and both topicsets.
The dataset consists of 2881 reviews regardingthe Las Vegas Bellagio hotel.
Table 3 summarizesthe results.
We confine our analysis to a subset of2594 reviews from the initial 2881 for which the nu-meric rating is greater or smaller than 3.We notice that the recall is consistently lower forthe frequent noun topics, which was expected be-cause of the significantly smaller number of topicterms.
However the recall does not depend on thechosen dictionary.
This is relevant because with amuch smaller pool of dictionary terms, similar re-sults are obtained.
Precision is constant in all fourcases, which also shows that results similar to thoseof OpinionFinder can be obtained with our muchsmaller dictionary.The precision and recall values in Table 3 donot reflect the capacity of the higher grained opin-ion analysis to extract targeted user preferences.The overall variance of the hotel?s numeric ratingsV ar(nr(r)) shows how much the reviewers dis-agree on the quality of the stay.
Generally thisdisagreement comes from the different sets of val-ues the reviewers have.
For example some considercleanliness the most important aspect while othersare interested in a busy nightlife.We cluster users based on the faceted opinionswe retrieved, using the k-Means algorithm (Mac-Queen, 1967).
Each reviewer is represented by afeature vector and each feature i within the vec-tor is the cumulative opinion expressed by the re-viewer with regard to topic zi.
The reviews withinthe same cluster j have a similar representation fromthe mined opinion perspective.
If the quality of theopinion mining process is high, the numeric rat-ings associated to the reviews within a cluster willalso be similar, thus their variance V arj(nr(r)) willbe lower than the overall variance.
We study thedifference between the mean intra cluster varianceavgV arj(nr(r)) and overall variance V ar(nr(r))7and the results are shown in table 4 for differentnumbers of clusters, using both topic models andboth dictionaries.The results show that we succeeded in decreasingthe variance by more than 20% using the Guessti-ment dictionary and the frequent noun topics.
A17% decrease is obtained by using the same topicset and the OpinionFinder dictionary, while the de-creases for the LDA topics with dictionaries arethree times lower.
This proves that the dictionaryresulted from playing the Guesstiment game is bet-ter suited for faceted opinion analysis than an estab-lished resource like OpinionFinder.5 ConclusionIn this paper we introduced Guesstiment, a humancomputation game for simultaneous feature extrac-tion and sentiment annotation.
By conducting vari-ous experiments, we showed that quality of the an-notations obtained using our approach outperformsthose obtained by classic crowd-sourcing.
This is anindicator of the fact that packaging a crowd-sourcingproblem as a game can improve the quality of theobtained results.
It?s mostly because that games at-tract more attention from people than simple ques-tions which are common ways of crowd-sourcing.We also showed that our approach outperformsstate-of-the-art machine learning methods which il-lustrates that human computation power is still su-perior to machine intelligence in this problem.The idea of the game could be further extendedby testing other more complicated scoring func-tions which could better motivate players to submithigh quality results.
Also other document selectionstrategies can be created to make a better trade-offbetween informativeness and interestingness, or ex-ploration and exploitation.
Moreover, a computerplayer could be designed to perform active learningon feature extraction and direct the word suggestionprocess toward selecting more informative features,hereby obtaining a more discriminative high-qualitylexicon.ReferencesA.A.
Al-Subaihin, H.S.
Al-Khalifa, and A.M.S.
Al-Salman.
2011.
A proposed sentiment analysis toolfor modern arabic using human-based computing.
InProceedings of the 13th International Conference onInformation Integration and Web-based Applicationsand Services, pages 543?546.
ACM.David M Blei, Andrew Y Ng, and Michael I Jordan.2003.
Latent dirichlet alocation.
Journal of MachineLearning Research, 3(4-5):993?1022.Anthony Brew, Derek Greene, and Padraig Cunningham.2010.
Using crowdsourcing and active learning totrack sentiment in online media.
In ECAI, volume 215of Frontiers in Artificial Intelligence and Applications,pages 145?150.
IOS Press.J.
Chamberlain, M. Poesio, and U. Kruschwitz.
2008.Phrase detectives: A web-based collaborative annota-tion game.
Proceedings of I-Semantics, Graz.Emmanuel Christophe, Jordi Inglada, and JeromeMaudlin.
2010.
Crowd-sourcing satellite image anal-ysis.
In IGARSS, pages 1430?1433.
IEEE.Marie-Catherine De Marneffe and Christopher D Man-ning.
2008.
The stanford typed dependencies repre-sentation.
Coling 2008 Proceedings of the workshopon CrossFramework and CrossDomain Parser Evalu-ation CrossParser 08, 1(ii):1?8.Andrea Esuli and Fabrizio Sebastiani, 2006.
SentiWord-Net: A publicly available lexical resource for opinionmining, volume 6, page 417422.
Citeseer.C.J.
Ho, T.H.
Chang, J.C. Lee, J.Y.
Hsu, and K.T.
Chen.2009.
Kisskissban: a competitive human computa-tion game for image annotation.
In Proceedings ofthe ACM SIGKDD Workshop on Human Computation,pages 11?14.
ACM.J.
Howe.
2006.
The rise of crowdsourcing.
Wired maga-zine, 14(14):1?5.E.
Law, B.
Settles, A. Snook, H. Surana, L. von Ahn,and T. Mitchell.
Human computation for attribute andattribute value acquisition.Bing Liu.
2010.
Sentiment analysis : A multi-facetedproblem.
Science, 25(1):76?80.J.
B. MacQueen.
1967.
Some methods for classifica-tion and analysis of multivariate observations.
In Pro-ceedings of 5th Berkeley Symposium on MathematicalStatistics and Probability, pages 281?297.B.
Pang, L. Lee, and S. Vaithyanathan.
2002.
Thumbsup?
: sentiment classification using machine learningtechniques.
In Proceedings of the ACL-02 conferenceon Empirical methods in natural language processing-Volume 10, pages 79?86.
Association for Computa-tional Linguistics.A.J.
Quinn and B.B.
Bederson.
2011.
Human computa-tion: a survey and taxonomy of a growing field.
InProceedings of the 2011 annual conference on Hu-man factors in computing systems, pages 1403?1412.ACM.8Table 3: Precision and Recall of Overall Review Polarity DetectionTotal TP FP FN Precision RecallLDA GS 2594 1399 456 739 0.75 0.65OF 2594 1434 461 699 0.75 0.67Top Frequency GS 2594 1275 405 914 0.75 0.58OF 2594 1362 371 861 0.78 0.61Table 4: Weighted Average of Intra Cluster Variancesk 1 10 11 12 13 14 15 % DecreaseVariance LDA GS 1.03 1.01 0.99 0.99 0.98 0.96 1.01 6.79OF 1.03 0.99 0.95 0.99 0.99 1.01 0.96 7.761Top Frequency GS 0.62 0.52 0.52 0.51 0.49 0.52 0.5 20.964OF 0.68 0.57 0.59 0.56 0.57 0.56 0.56 17.64Laurel D. Riek, Maria F. O?Connor, and Peter Robinson.2011.
Guess what?
a game for affective annotation ofvideo using crowd sourcing.
In Sidney K. D?Mello,Arthur C. Graesser, Bjo?rn Schuller, and Jean-ClaudeMartin, editors, ACII (1), volume 6974 of LectureNotes in Computer Science, pages 277?285.
Springer.Anna Rumshisky.
2011.
Crowdsourcing word sense def-inition.
In Linguistic Annotation Workshop, pages 74?81.
The Association for Computer Linguistics.B.
Settles.
2011a.
Closing the loop: Fast, interactivesemi-supervised annotation with queries on featuresand instances.
In Conference on Empirical Methods inNatural Language Processing (EMNLP), Edinburgh,Scotland, July.B.
Settles.
2011b.
Closing the loop: Fast, interactivesemi-supervised annotation with queries on featuresand instances.
Conference on Empirical Methods inNatural Language Processing (EMNLP), Edinburgh,Scotland, July.Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-berly D. Voll, and Manfred Stede.
2011.
Lexicon-based methods for sentiment analysis.
ComputationalLinguistics, 37(2):267?307.Peter D. Turney.
2002.
Thumbs up or thumbs down?
:semantic orientation applied to unsupervised classifi-cation of reviews.
In Proceedings of the 40th AnnualMeeting on Association for Computational Linguis-tics, ACL ?02, pages 417?424, Stroudsburg, PA, USA.Association for Computational Linguistics.L.
Von Ahn and L. Dabbish.
2004.
Labeling imageswith a computer game.
In Proceedings of the SIGCHIconference on Human factors in computing systems,pages 319?326.
ACM.L.
Von Ahn, M. Kedia, and M. Blum.
2006a.
Verbosity:a game for collecting common-sense facts.
In Pro-ceedings of the SIGCHI conference on Human Factorsin computing systems, pages 75?78.
ACM.L.
Von Ahn, R. Liu, and M. Blum.
2006b.
Peekaboom:a game for locating objects in images.
In Proceedingsof the SIGCHI conference on Human Factors in com-puting systems, pages 55?64.
ACM.L.
Von Ahn.
2006.
Games with a purpose.
Computer,39(6):92?94.Carl Vondrick, Deva Ramanan, and Donald Patterson.2010.
Efficiently scaling up video annotation withcrowdsourced marketplaces.
In Kostas Daniilidis, Pet-ros Maragos, and Nikos Paragios, editors, ECCV (4),volume 6314 of Lecture Notes in Computer Science,pages 610?623.
Springer.Albert Weichselbraun, Stefan Gindl, and Arno Scharl.2011.
Using games with a purpose and bootstrap-ping to create domain-specific sentiment lexicons.
InCraig Macdonald, Iadh Ounis, and Ian Ruthven, edi-tors, CIKM, pages 1053?1060.
ACM.Theresa Wilson, Paul Hoffmann, Swapna Somasun-daran, Jason Kessler, Janyce Wiebe, Yejin Choi, ClaireCardie, Ellen Riloff, and Siddharth Patwardhan.
2005.Opinionfinder : A system for subjectivity analysis.Learning, (October):34?35.Shusen Zhou, Qingcai Chen, and Xiaolong Wang.
2010.Active deep networks for semi-supervised sentimentclassification.
In Chu-Ren Huang and Dan Jurafsky,editors, COLING (Posters), pages 1515?1523.
Chi-nese Information Processing Society of China.9
