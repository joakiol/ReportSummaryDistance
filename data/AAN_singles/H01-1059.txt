Portability Issues for Speech Recognition Technologies Lori Lamel, Fabrice Lefevre, Jean-Luc Gauvain and Gilles AddaSpoken Language Processing Group,CNRS-LIMSI, 91403 Orsay, Franceflamel,lefevre,gauvain,gaddag@limsi.frABSTRACTAlthough there has been regular improvement in speech recog-nition technology over the past decade, speech recognition is farfrom being a solved problem.
Most recognition systems are tunedto a particular task and porting the system to a new task (or lan-guage) still requires substantial investment of time and money, aswell as expertise.
Todays state-of-the-art systems rely on the avail-ability of large amounts of manually transcribed data for acous-tic model training and large normalized text corpora for languagemodel training.
Obtaining such data is both time-consuming andexpensive, requiring trained human annotators with substantial a-mounts of supervision.In this paper we address issues in speech recognizer portabil-ity and activities aimed at developing generic core speech recogni-tion technology, in order to reduce the manual effort required forsystem development.
Three main axes are pursued: assessing thegenericity of wide domain models by evaluating performance underseveral tasks; investigating techniques for lightly supervised acous-tic model training; and exploring transparent methods for adaptinggeneric models to a specific task so as to achieve a higher degree ofgenericity.1.
INTRODUCTIONThe last decade has seen impressive advances in the capabilityand performance of speech recognizers.
Todays state-of-the-artsystems are able to transcribe unrestricted continuous speech frombroadcast data with acceptable performance.
The advances arisefrom the increased accuracy and complexity of the models, whichare closely related to the availability of large spoken and text cor-pora for training, and the wide availability of faster and cheapercomputational means which have enabled the development and im-plementation of better training and decoding algorithms.
Despitethe extent of progress over the recent years, recognition accuracy isstill extremely sensitive to the environmental conditions and speak-ing style: channel quality, speaker characteristics, and backgroundThis work was partially financed by the European Commissionunder the IST-1999 Human Language Technologies project 11876Coretex..noise have an important impact on the acoustic component of thespeech recognizer, whereas the speaking style and the discoursedomain have a large impact on the linguistic component.In the context of the EC IST-1999 11876 project CORETEX weare investigating methods for fast system development, as well asdevelopment of systems with high genericity and adaptability.
Byfast system development we refer to: language support, i.e., thecapability of porting technology to different languages at a reason-able cost; and task portability, i.e.
the capability to easily adapt atechnology to a new task by exploiting limited amounts of domain-specific knowledge.
Genericity and adaptability refer to the capac-ity of the technology to work properly on a wide range of tasks andto dynamically keep models up to date using contemporary data.The more robust the initial generic system is, the less there is aneed for adaptation.
Concerning the acoustic modeling component,genericity implies that it is robust to the type and bandwidth of thechannel, the acoustic environment, the speaker type and the speak-ing style.
Unsupervised normalization and adaptation techniquesevidently should be used to enhance performance further when thesystem is exposed to data of a particular type.With today?s technology, the adaptation of a recognition systemto a new task or new language requires the availability of suffi-cient amount of transcribed training data.
When changing to newdomains, usually no exact transcriptions of acoustic data are avail-able, and the generation of such transcribed data is an expensiveprocess in terms of manpower and time.
On the other hand, thereoften exist incomplete information such as approximate transcrip-tions, summaries or at least key words, which can be used to pro-vide supervision in what can be referred to as ?informed speechrecognition?.
Depending on the level of completeness, this infor-mation can be used to develop confidence measures with adapted ortrigger language models or by approximate alignments to automatictranscriptions.
Another approach is to use existing recognizer com-ponents (developed for other tasks or languages) to automaticallytranscribe task-specific training data.
Although in the beginning theerror rate on new data is likely to be rather high, this speech datacan be used to re-train a recognition system.
If carried out in aniterative manner, the speech data base for the new domain can becumulatively extended over time without direct manual transcrip-tion.The overall objective of the work presented here is to reducethe speech recognition development cost.
One aspect is to develop?generic?
core speech recognition technology, where by ?generic?we mean a transcription engine that will work reasonably well on awide range of speech transcription tasks, ranging from digit recog-nition to large vocabulary conversational telephony speech, with-out the need for costly task-specific training data.
To start with weassess the genericity of wide domain models under cross-task con-Table 1: Brief descriptions and best reported error rates for the corpora used in this work.Corpus Test Year Task Train (#spkr) Test (#spkr) Textual Resources Best WERBN 98 TV & Radio News 200h 3h Closed-captions, commercial transcripts,manual transcripts of audio data13.5TI-digits 93 Small Vocabulary 3.5h (112) 4h (113) - 0.2ATIS 93 H-M Dialog 40h (137) 5h (24) Transcriptions 2.5WSJ 95 News Dictation 100h (355) 45mn (20) Newspaper, newswire 6.6S9 WSJ 93 Spontaneous Dictation 43mn (10) Newspaper, newswire 19.1ditions, i.e., by recognizing task-specific data with a recognizer de-veloped for a different task.
We chose to evaluate the performanceof broadcast news acoustic and language models, on three com-monly used tasks: small vocabulary recognition (TI-digits), readand spontaneous text dictation (WSJ), and goal-oriented spoken di-alog (ATIS).
The broadcast news task is quite general, covering awide variety of linguistic and acoustic events in the language, en-suring reasonable coverage of the target task.
In addition, there aresufficient acoustic and linguistic training data available for this taskthat accurate models covering a wide range of speaker and languagecharacteristics can be estimated.Another research area is the investigation of lightly supervisedtechniques for acoustic model training.
The strategy taken is touse a speech recognizer to transcribe unannotated data, which arethen used to estimate more accurate acoustic models.
The lightsupervision is applied to the broadcast news task, where unlim-ited amounts of acoustic training data are potentially available.
Fi-nally we apply the lightly supervised training idea as a transpar-ent method for adapting the generic models to a specific task, thusachieving a higher degree of genericity.
In this work we focus onreducing training costs and task portability, and do not address lan-guage transfer.We selected the LIMSI broadcast news (BN) transcription sys-tem as the generic reference system.
The BN task covers a largenumber of different acoustic and linguistic situations: planned tospontaneous speech; native and non-native speakers with differentaccents; close-talking microphones and telephone channels; quietstudio, on-site reports in noisy places to musical background; anda variety of topics.
In addition, a lot of training resources are avail-able including a large corpus of annotated audio data and a hugeamount of raw audio data for the acoustic modeling; and largecollections of closed-captions, commercial transcripts, newspapersand newswires texts for linguistic modeling.
The next section pro-vides an overview of the LIMSI broadcast news transcription sys-tem used as our generic system.2.
SYSTEM DESCRIPTIONThe LIMSI broadcast news transcription system has two maincomponents, the audio partitioner and the word recognizer.
Datapartitioning [6] serves to divide the continuous audio stream intohomogeneous segments, associating appropriate labels for cluster,gender and bandwidth with the segments.
The speech recognizeruses continuous density HMMs with Gaussian mixture for acous-tic modeling and n-gram statistics estimated on large text corporafor language modeling.
Each context-dependent phone model is atied-state left-to-right CD-HMM with Gaussian mixture observa-tion densities where the tied states are obtained by means of a de-cision tree.
Word recognition is performed in three steps: 1) initialhypothesis generation, 2) word graph generation, 3) final hypoth-esis generation.
The initial hypotheses are used for cluster-basedacoustic model adaptation using the MLLR technique [13] prior toword graph generation.
A 3-gram LM is used in the first two de-coding steps.
The final hypotheses are generated with a 4-gram LMand acoustic models adapted with the hypotheses of step 2.In the baseline system used in DARPA evaluation tests, the acous-tic models were trained on about 150 hours of audio data from theDARPA Hub4 Broadcast News corpus (the LDC 1996 and 1997Broadcast News Speech collections) [9].
Gender-dependent acous-tic models were built using MAP adaptation of SI seed models forwide-band and telephone band speech [7].
The models contain28000 position-dependent, cross-word triphone models with 11700tied states and approximately 360k Gaussians [8].The baseline language models are obtained by interpolation ofmodels trained on 3 different data sets (excluding the test epochs):about 790M words of newspaper and newswire texts; 240M wordof commercial broadcast news transcripts; and the transcriptions ofthe Hub4 acoustic data.
The recognition vocabulary contains 65120words and has a lexical coverage of over 99% on all evaluation testsets from the years 1996-1999.
A pronunciation graph is associatedwith each word so as to allow for alternate pronunciations.
Thepronunciations make use of a set of 48 phones set, where 3 phoneunits represent silence, filler words, and breath noises.
The lexiconcontains compound words for about 300 frequent word sequences,as well as word entries for common acronyms, providing an easyway to allow for reduced pronunciations [6].The LIMSI 10x system obtained a word error of 17.1% on the1999 DARPA/NIST evaluation set and can transcribe unrestrictedbroadcast data with a word error of about 20% [8].3.
TASK INDEPENDENCEOur first step in developing a ?generic?
speech transcription en-gine is to assess the most generic system we have under cross-task conditions, i.e., by recognizing task-specific data with a rec-ognizer developed for a different task.
Three representative taskshave been retained as target tasks: small vocabulary recognition(TI-digits), goal-oriented human-machine spoken dialog (ATIS),and dictation of texts (WSJ).
The broadcast news transcription task(Hub4E) serves as the baseline.
The main criteria for the task se-lection were that they are realistic enough and task-specific datashould be available.
The characteristics of these four tasks and theavailable corpora are summarized in Table 1.For the small vocabulary recognition task, experiments are car-ried out on the adult speaker portion of the TI-digits corpus [14],containing over 17k utterances from a total of 225 speakers.
Thevocabulary contains 11 words, the digits ?1?
to ?9?, plus ?zero?
and?oh?.
Each speaker uttered two versions of each digit in isolationand 55 digit strings.
The database is divided into training and testsets (roughly 3.5 hours each, corresponding to 9k strings).
Thespeech is of high quality, having been collected in a quiet environ-ment.
The best reported WERs on this task are around 0.2-0.3%.The digit phonemic coverage being very low, only 108 context-dependent models are used in our recognition system.
The task-Table 2: Word error rates (%) for BN98, TI-digits, ATIS94,WSJ95 and S9 WSJ93 test sets after recognition with three dif-ferent configurations: (left) BN acoustic and language models;(center) BN acoustic models combined with task-specific lex-ica and LMs and (right) task-dependent acoustic and languagemodels.Test Set BN models Task LMs Task modelsBN98 13.6 13.6 13.6TI-digits 17.5 1.7 0.4ATIS94 22.7 4.7 4.4WSJ95 11.6 9.0 7.6S9 WSJ93 12.1 13.6 15.3specific LM for the TI-digits is a simple grammar allowing any se-quence of up to 7 digits.
Our task-dependent system performanceis 0.4% WER.The DARPA Air Travel Information System (ATIS) task is cho-sen as being representative of a goal-oriented human-machine di-alog task, and the ARPA 1994 Spontaneous Speech Recognition(SPREC) ATIS-3 data (ATIS94) [4] is used for testing purposes.The test data amounts for nearly 5 hours of speech from 24 speakersrecorded with a close-talking microphone.
Around 40h of speechdata are available for training.
The word error rates for this task inthe 1994 evaluation were mainly in the range of 2.5% to 5%, whichwe take as state-of-the-art for this task.
The acoustic models usedin our task-specific system include 1641 context-dependent phoneswith 4k independent HMM states.
A back-off trigram languagemodel has been estimated on the transcriptions of the training ut-terances.
The lexicon contains 1300 words, with compounds wordsfor multi-word entities in the air-travel database (city and airportnames, services etc.).
The WER obtained with our task-dependentsystem is 4.4%.For the dictation task, the Wall Street Journal continuous speechrecognition corpus [17] is used, abiding by the ARPA 1995 Hub3test (WSJ95) conditions.
The acoustic training data consist of 100hours of speech from a total of 355 speakers taken from the WSJ0and WSJ1 corpora.
The Hub3 baseline test data consist of stu-dio quality read speech from 20 speakers with a total duration of45 minutes.
The best result reported at the time of the evaluationwas 6.6%.
A contrastive experiment is carried out with the WSJ93Spoke 9 data comprised of 200 spontaneous sentences spoken byjournalists [11].
The best performance reported in the 1993 evalua-tion on the spontaneous data was 19.1% [18], however lower worderror rates have since been reported on comparable test sets (14.1%on the WSJ94 Spoke 9 test data).
21000 context and position-dependent models have been trained for the WSJ system, with 9kindependent HMM states.
A 65k-word vocabulary was selectedand a back-off trigram model obtained by interpolating models trainedon different data sets (training utterance transcriptions and newspa-pers data).
The task-dependent WSJ system has a WER of 7.6% onthe read speech test data and 15.3% on the spontaneous data.For the BN transcription task, we follow the conditions of the1998 ARPA Hub4E evaluation (BN98) [15].
The acoustic trainingdata is comprised of 150 hours of North-American TV and radioshows.
The best overall result on the 1998 baseline test was 13.5%.Three sets of experiments are reported.
The first are cross-taskrecognition experiments carried out using the BN acoustic and lan-guage models to decode the test data for the other tasks.
The secondset of experiments made use of mixed models, that is the BN acous-tic models and task-specific LMs.
Due to the different evaluationparadigms, some minor modifications were made in the transcrip-tion procedure.
First of all, in contrast with the BN data, the datafor the 3 tasks is already segmented into individual utterances sothe partitioning step was eliminated.
With this exception, the de-coding process for the WSJ task is exactly the same as described inthe previous section.
For the TI-digits and ATIS tasks, word decod-ing is carried out in a single trigram pass, and no speaker adaptationwas performed.The WERs obtained for the three recognition experiments arereported in Table 2.
A comparison with Table 1 shows that theperformances of the task-dependent models are close to the best re-ported results even though we did not devote too much effort in op-timizing these models.
We can also observe by comparing the task-dependent (Table 2, right) and mixed (Table 2, middle) conditions,that the BN acoustic models are relatively generic.
These mod-els seem to be a good start towards truly task-independent acousticmodels.
By using task-specific language models For the TI-digitsand ATIS we can see that the gap in performance is mainly duea linguistic mismatch.
For WSJ the language models are moreclosely matched to BN and only a small 1.6% WER reduction isobtained.
On the spontaneous journalist dictation (WSJ S9 spoke)test data there is even an increase in WER using the WSJ LMs,which can be attributed to a better modelization of spontaneousspeech effects (such as breath and filler words) in the BN models.Prior to introducing our approach for lightly supervised acousticmodel training, we describe our standard training procedure in thenext section.4.
ACOUSTIC MODEL TRAININGHMM training requires an alignment between the audio signaland the phone models, which usually relies on a perfect ortho-graphic transcription of the speech data and a good phonetic lex-icon.
In general it is easier to deal with relatively short speech seg-ments so that transcription errors will not propagate and jeopardizethe alignment.
The orthographic transcription is usually consideredas ground truth and training is done in a closely supervised man-ner.
For each speech segment the training algorithm is providedwith the exact orthographic transcription of what was spoken, i.e.,the word sequence that the speech recognizer should hypothesizewhen confronted with the same speech segment.Training acoustic models for a new corpus (which could also re-flect a change of task and/or language), usually entails the follow-ing sequence of operations once the audio data and transcriptionfiles have been loaded:1.
Normalize the transcriptions to a common format (some ad-justment is always needed as different text sources make useof different conventions).2.
Produce a word list from the transcriptions and correct blatanterrors (these include typographical errors and inconsistencies).3.
Produce a phonemic transcription for all words not in our mas-ter lexicon (these are manually verified).4.
Align the orthographic transcriptions with the signal using ex-isting models and the pronunciation lexicon (or bootstrap mod-els from another task or language).
This procedure often re-jects a substantial portion of the data, particularly for long seg-ments.5.
Eventually correct transcription errors and realign (or just ig-nore these if enough audio data is available)6.
Run the standard EM training procedure.This sequence of operations is usually iterated several times torefine the acoustic models.
In general each iteration recovers a por-tion of the rejected data.5.
LIGHTLY SUPERVISED ACOUSTICMODEL TRAININGOne can imagine training acoustic models in a less supervisedmanner, by using an iterative procedure where instead of usingmanual transcriptions for alignment, at each iteration the most likelyword transcription given the current models and all the informationavailable about the audio sample is used.
This approach still fitswithin the EM training framework, which is well-suited for miss-ing data training problems.
A completely unsupervised trainingprocedure is to use the current best models to produce an ortho-graphic transcription of the training data, keeping only words thathave a high confidence measure.
Such an approach, while very en-ticing, is limited since the only supervision is provided by the con-fidence measure estimator.
This estimator must in turn be trainedon development data, which needs to be small to keep the approachinteresting.Between using carefully annotated data such as the detailed tran-scriptions provided by the LDC and no transcription at all, there isa wide spectrum of possibilities.
What is really important is thecost of producing the associated annotations.
Detailed annotationrequires on the order of 20-40 times real-time of manual effort, andeven after manual verification the final transcriptions are not ex-empt from errors [2].
Orthographic transcriptions such as closed-captions can be done in a few times real-time, and therefore arequite a bit less costly.
These transcriptions have the advantage thatthey are already available for some television channels, and there-fore do not have to be produced specifically for training speechrecognizers.
However, closed-captions are a close, but not exacttranscription of what is being spoken, and are only coarsely time-aligned with the audio signal.
Hesitations and repetitions are notmarked and there may be word insertions, deletions and changesin the word order.
They also are missing some of the additionalinformation provided in the detailed speech transcriptions such asthe indication of acoustic conditions, speaker turns, speaker identi-ties and gender and the annotation of non-speech segments such asmusic.
NIST found the disagreement between the closed-captionsand manual transcripts on a 10 hour subset of the TDT-2 data usedfor the SDR evaluation to be on the order of 12% [5].Another approach is to make use of other possible sources ofcontemporaneous texts from newspapers, newswires, summariesand the Internet.
However, since these sources have only an indirectcorrespondence with the audio data, they provide less supervision.The basic idea is of light supervision is to use a speech recog-nizer to automatically transcribe unannotated data, thus generat-ing ?approximate?
labeled training data.
By iteratively increasingthe amount of training data, more accurate acoustic models are ob-tained, which can then be used to transcribe another set of unanno-tated data.
The modified training procedure used in this work is:1.
Train a language model on all texts and closed captions afternormalization2.
Partition each show into homogeneous segments and label theacoustic attributes (speaker, gender, bandwidth) [6]3.
Train acoustic models on a very small amount of manuallyannotated data (1h)4.
Automatically transcribe a large amount of training data5.
(Optional) Align the closed-captions and the automatic tran-scriptions (using a standard dynamic programming algorithm)6.
Run the standard acoustic model training procedure on thespeech segments (in the case of alignment with the closedcaptions only keep segments where the two transcripts are inagreement)7.
Reiterate from step 4.It is easy to see that the manual work is considerably reduced, notonly in generating the annotated corpus but also during the trainingprocedure, since we no longer need to extend the pronunciation lex-icon to cover all words and word fragments occurring in the trainingdata and we do not need to correct transcription errors.
This ba-sic idea was used to train acoustic models using the automaticallygenerated word transcriptions of the 500 hours of audio broadcastsused in the spoken document retrieval task (part of the DARPATDT-2 corpus used in the SDR?99 and SDR?00 evaluations) [3].This corpus is comprised of 902 shows from 6 sources broadcastbetween January and June 1998: CNN Headline News (550 30-minute shows), ABC World News Tonight (139 30-minute shows),Public Radio International The World (122 1-hour shows), Voice ofAmerica VOA Today and World Report (111 1-hour shows).
Theseshows contain about 22k stories with time-codes identifying thebeginning and end of each story.First, the recognition performance as a function of the availableacoustic and language model training data was assessed.
Then weinvestigated the accuracy of the acoustic models obtained after rec-ognizing the audio data using different levels of supervision viathe language model.
With the exception of the baseline Hub4 lan-guage models, none of the language models include a componentestimated on the transcriptions of the Hub4 acoustic training data.The language model training texts come from contemporaneoussources such as newspapers and newswires, and commercial sum-maries and transcripts, and closed-captions.
The former sourceshave only an indirect correspondence with the audio data and pro-vide less supervision than the closed captions.
For each set of LMtraining texts, a new word list was selected based on the word fre-quencies in the training data.
All language models are formed byinterpolating individual LMs built on each text source.
The interpo-lation coefficients were chosen in order to minimize the perplexityon a development set composed of the second set of the Nov98evaluation data (3h) and a 2h portion of the TDT2 data from Jun98(not included in the LM training data).
The following combinationswere investigated: LMa (baseline Hub4 LM): newspaper+newswire (NEWS), com-mercial transcripts (COM) predating Jun98, acoustic transcripts LMn t c: NEWS, COM, closed-captions through May98 LMn t: NEWS, COM through May98 LMn c: NEWS, closed-captions through May98 LMn: NEWS through May98 LMn to: NEWS through May98, COM through Dec97 LMno: NEWS through Dec97Table 3: Word error rate for various conditions using acous-tic models trained on the HUB4 training data with detailedmanual transcriptions.
All runs were done in less than 10xRT,except the last row.
?1S?
designates one set of gender-independent acoustic models, whereas ?4S?
designates four setsof gender and bandwidth dependent acoustic models.Training Conditions bn99 1 bn99 2 Average1h 1S, LMn t c 35.2 31.9 33.369h 1S, LMn t c 20.2 18.0 18.9123h 1S, LMn t c 19.3 17.1 18.0123h 4S, LMn t c 18.5 16.1 17.1123h 4S, LMa 18.3 16.3 17.1123h 4S, LMa, 50x 17.1 14.5 15.6Table 4: Word error rate for different language models and increasing quantities of automatically labeled training data on the 1999evaluation test sets using gender and bandwidth independent acoustic models.
LMn t c: NEWS, COM, closed-captions throughMay98 LMn t: NEWS, COM through May98 LMn c: NEWS, closed-captions through May98 LMn: NEWS through May98LMn to: NEWS through May98, COM through Dec97 LMno: NEWS through Dec97.Amount of training data %WERraw unfiltered LMn t c LMn t LMn c LMn LMn to LMno150h 123h 18.0 18.6 19.1 20.6 18.7 20.91h 1h 33.3 33.7 34.4 35.9 33.9 36.114h 8h 26.4 27.6 27.4 29.0 27.6 30.628h 17h 25.2 25.7 25.6 28.1 25.7 28.958h 28h 24.3 25.2 25.7 27.4 25.1 27.9It should be noted that all of the conditions include newspaperand newswire texts from the same epoch as the audio data.
Theseprovide an important source of knowledge particularly with re-spect to the vocabulary items.
Conditions which include the closedcaptions in the LM training data provide additional supervision inthe decoding process when transcribing audio data from the sameepoch.For testing purposes we use the 1999 Hub4 evaluation data, whichis comprised of two 90 minute data sets selected by NIST.
The firstset was extracted from 10 hours of data broadcast in June 1998,and the second set from a set of broadcasts recorded in August-September 1998 [16].
All recognition runs were carried out in un-der 10xRT unless stated otherwise.
The LIMSI 10x system ob-tained a word error of 17.1% on the evaluation set (the combinedscores in the penultimate row in Table 3 4S, LMa) [8].
The worderror can be reduced to 15.6% for a system running at 50xRT (lastentry in Table 3).As can be seen in Table 3, the word error rates with our orig-inal Hub4 language model (LMa) and the one without the tran-scriptions of the acoustic data (LMn t c) give comparable resultsusing the 1999 acoustic models trained on 123 hours of manuallyannotated data (123h, 4S).
The quality of the different languagemodels listed above are compared in the first row of Table 3 us-ing speaker-independent (1S) acoustic models trained on the sameHub4 data (123h).
As can be observed, removing any text sourceleads to a degradation in recognition performance.
It appears it ismore important to include commercial transcripts (LMn t), evenif they are old (LMn to) than the closed captions (LMn c).
Thissuggests that the commercial transcripts more accurately representspoken language than closed-captioning.
Even if only newspaperand newswire texts are available (LMn), the word error increasesby only 14% over the best configuration (LMn t c), and even usingolder newspaper and newswire texts (LMno) does not substantiallyincrease the word error rate.
The second row of Table 3 gives theword error rates with acoustic models trained on only 1 hour ofmanually transcribed data.
These are the models used to initializethe process of automatically transcribing large quantities of data.These word error rates range from 33% to 36% across the languagemodels.We compared a straightforward approach of training on all theautomatically annotated data with one in which the closed-captionsare used to filter the hypothesized transcriptions, removing wordsthat are ?incorrect?.
In the filtered case, the hypothesized transcrip-tions are aligned with the closed captions story by story, and onlyregions where the automatic transcripts agreed with the closed cap-tions were kept for training purposes.
To our surprise, somewhatcomparable recognition results were obtained both with and with-out filtering, suggesting that inclusion of the closed-captions in thelanguage model training material provided sufficient supervision(see Table 5).1 It should be noted that in both cases the closed-caption story boundaries are used to delimit the audio segmentsafter automatic transcription.To investigate this further we are assessing the effects of reduc-ing the amount of supervision provided by the language modeltraining texts on the acoustic model accuracy (see Table 4).
With14 hours (raw) of approximately labeled training data, the word er-ror is reduced by about 20% for all LMs compared with training on1h of data which has carefully manual transcriptions.
Using largeramounts of data transcribed with the same initial acoustic modelsgives smaller improvements, as seen by the entries for 28h and 58h.The commercial transcripts (LMn+t and LMn+to), even if predat-ing the data epoch, are seen to be more important than the closed-captions (LMn+c), supporting the earlier observation that they arecloser to spoken language.
Even if only news texts from the sameperiod (LMn) are available, these provide adequate supervision forlightly supervised acoustic model training.Table 5: Word error rates for increasing quantities of auto-matically label training data on the 1999 evaluation test setsusing gender and bandwidth independent acoustic models withthe language model LMn t c (trained on NEWS, COM, closed-captions through May98).Amount of training data %WERraw unfiltered filtered unfiltered filtered14h 8h 6h 26.4 25.728h 17h 13h 25.2 23.758h 28h 21h 24.3 22.5140h 76h 57h 22.4 21.1287h 140h 108h 21.0 19.9503h 238h 188h 20.2 19.46.
TASK ADAPTATIONThe experiments reported in the section 3 show that while directrecognition with the reference BN acoustic models gives relatively1The difference in the amounts of data transcribed and actuallyused for training is due to three factors.
The first is that the total du-ration includes non-speech segments which are eliminated prior torecognition during partitioning.
Secondly, the story boundaries inthe closed captions are used to eliminate irrelevant portions, suchas commercials.
Thirdly, since there are many remaining silenceframes, only a portion of these are retained for training.Table 6: Word error rates (%) for TI-digits, ATIS94, WSJ95 and S9 WSJ93 test sets after recognition with three different configura-tions, all including task-specific lexica and LMs: (left) BN acoustic models, (middle left) unsupervised adaptation of the BN acousticmodels, (middle right) supervised adaptation of the BN acoustic models and (right) task-dependent acoustic models.Test Set BN models Unsupervised Adaptation Supervised Adaptation Task-dep.
modelsBN models BN modelsTI-digits 1.7 0.8 0.5 0.4ATIS94 4.7 4.7 3.2 4.4WSJ95 9.0 6.9 6.7 7.6S9 WSJ93 13.6 12.6 11.4 15.3competitive results, the WER on the targeted tasks can still be im-proved.
Since we want to minimize the cost and effort involved intuning to a target task, we are investigating methods to transpar-ently adapt the reference acoustic models.
By transparent we meanthat the procedure is automatic and can be carried out without anyhuman expertise.
We therefore apply the approach presented in theprevious section, that is the reference BN system is used to tran-scribe the training data of the destination task.
This supposes ofcourse that audio data have been collected.
However, this can becarried out with an operational system and the cost of collectingtask-specific training data is greatly reduced since no manual tran-scriptions are needed.
The performance of the BN models undercross task conditions is well within the range for which the approx-imate transcriptions can be used for acoustic model adaptation.The reference acoustic models are then adapted by means of aconventional adaptation technique such as MLLR and MAP.
Thusthere is no need to design a new set of models based on the trainingdata characteristics.
Adaptation is also preferred to the trainingof new models as it is likely that the new training data will havea lower phonemic contextual coverage than the original referencemodels.The cross-task unsupervisedadaptation is evaluated for the tasks:TI-digits, ATIS and WSJ.
The 100 hours of the WSJ data were tran-scribed using the BN acoustic and language models.
For ATIS, only26 of the 40 hours of training data from 276 speakers were tran-scribed, due to time constraints.
For TI-digits, the training data wastranscribed using a mixed configuration, combining the BN acous-tic models with the simple digit loop grammar.2 For completenesswe also used the task-specific audio data and the associated tran-scriptions to carry out supervised adaptation of the BN models.Gender-dependent acoustic models were estimated using the cor-responding gender-dependent BN models as seeds and the gender-specific training utterances as adaptation data.
For WSJ and ATIS,the speaker ids were directly used for gender identification sincein previous experiments with this test set there were no genderclassification errors.
Only the acoustic models used in the sec-ond and third word decoding passes have been adapted.
For theTI-digits, the gender of each training utterance was automaticallyclassified by decoding each utterance twice, once with each set ofgender-dependent models.
Then, the utterance gender was deter-mined based on the best global score between the male and femalemodels (99.0% correct classification).Both the MLLR and MAP adaptation techniques were applied.The recognition tests were carried out under mixed conditions (i.e.,with the adapted acoustic models and the task-dependent LM).
The2In order to assess the quality of the automatic transcription, wecompared the system hypotheses to the manually provided trainingtranscriptions.
For resulting word error rates on the training dataare 11.8% for WSJ, 29.1% for ATIS and 1.2% for TI-digits.BN models are first adapted using MLLR with a global transforma-tion, followed by MAP adaptation.The word error rates obtained with the task-adapted BN mod-els are given in Table 6 for the four test sets.
Using unsupervisedadaptation the performance is improved for TIdigits (53% relative),WSJ (19% relative) and S9 (7% relative).The manual transcriptions for the targeted tasks were used tocarry out supervised model adaptation.
The results (see the 4th col-umn of Table 6) show a clear improvement over unsupervisedadap-tation for both the TI-digits (60% relative) and ATIS (47% relative)tasks.
A smaller gain of about 10% relative is obtained for the spon-taneous dictation task, and only 3% relative for read WSJ data.
Thegain appears to be correlated with the WER of the transcribed data:the difference between BN and task specific models is smaller forWSJ than ATIS and TI-digits.
The TI-digit task is the only task forwhich the best performance is obtained using task-dependent mod-els rather than BN models adapted with supervised.
For the othertasks, the lowest WER is obtained when the supervised adapted BNacoustic models are used: 3.2% for ATIS, 6.7% for WSJ and 11.4%for S9.
This result confirms our hypothesis that better performancecan be achieved by adapting generic models with task-specific datathan by directly training task-specific models.7.
CONCLUSIONSThis paper has explored methods to reduce the cost of developingmodels for speech recognizers.
Two main axes have been explored:developing generic acoustic models and the use of low cost data foracoustic model training.We have explored the genericity of state-of-the-art speech recog-nition systems, by testing a relatively wide-domain system on datafrom three tasks ranging in complexity.
The generic models weretaken from the broadcast news task which covers a wide range ofacoustic and linguistic conditions.
These acoustic models are rel-atively task-independent as there is only a small increase in worderror relative to the word error obtained with task-dependent acous-tic models, when a task-dependent language model is used.
Thereremains a large difference in performance on the digit recogni-tion task which can be attributed to the limited phonetic coverageof this task.
On a spontaneous WSJ dictation task, the broadcastnews acoustic and language are more robust to deviations in speak-ing style than the read-speech WSJ models.
We also have shownthat unsupervised acoustic model adaptation can reduce the perfor-mance gap between task-independent and task-dependent acousticmodels, and that supervised adaptation of generic models can leadto better performance than that achieved with task-specific models.Both supervised and unsupervised adaptation are less effective forthe digits task indicating that these may be a special case.We have investigated the use of low cost data to train acousticmodels for broadcast news transcription, with supervision providedthe language models.
Recognition results obtained with acousticmodels trained on large quantities of automatically annotated dataare comparable (under a 10% relative increase in word error) toresults obtained with acoustic models trained on large quantitiesof manually annotated data.
Given the significantly higher cost ofdetailed manual transcription (substantially more time consumingthan producing commercial transcripts, and more expensive sinceclosed captions and commercial transcripts are produced for otherpurposes), such approaches are very promising as they require sub-stantial computation time, but little manual effort.
Another advan-tage offered by this approach is that there is no need to extend thepronunciation lexicon to cover all words and word fragments oc-curring in the training data.
By eliminating the need for manualtranscription, automated training can be applied to essentially un-limited quantities of task-specific training data.
While the focus ofour work has been on reducing training costs and task portability,we have been exploring these in a multi-lingual context.REFERENCES[1] G. Adda, M. Jardino, J.L.
Gauvain, ?Language Modeling for Broad-cast News Transcription,?
ESCA Eurospeech?99, Budapest, 4, pp.1759-1760, Sept.
1999.
[2] C. Barras, E. Geoffrois et al,?Transcriber: development and use of atool for assisting speech corpora production,?
SpeechCommunication,33(1-2), pp.
5-22, Jan.
2001.
[3] C. Cieri, D. Graff, M. Liberman, ?The TDT-2 Text and SpeechCorpus,?
DARPA Broadcast News Workshop, Herndon.
(see alsohttp://morph.ldc.upenn.edu/TDT).
[4] D. Dahl, M. Bates et al, ?Expanding the Scope of the ATIS Task : TheATIS-3 Corpus,?
Proc.
ARPA Spoken Language Systems TechnologyWorkshop, Plainsboro, NJ, pp.
3-8, 1994.
[5] J. Garofolo, C. Auzanne, E. Voorhees, W. Fisher, ?1999 TREC-8 Spo-ken Document Retrieval Track Overview and Results,?
8th Text Re-trieval Conference TREC-8, Nov.
1999.
[6] J.L.
Gauvain, G. Adda, et al, ?Transcribing Broadcast News: TheLIMSI Nov96 Hub4 System,?
Proc.
ARPA Speech Recognition Work-shop, pp.
56-63, Chantilly, Feb.
1997.
[7] J.L.
Gauvain, C.H.
Lee, ?Maximum a Posteriori Estimation for Mul-tivariate Gaussian Mixture Observation of Markov Chains,?
IEEETrans.
on SAP, 2(2), pp.
291-298, April 1994.
[8] J.L.
Gauvain, L. Lamel, ?Fast Decoding for Indexation of BroadcastData,?
ICSLP?2000, 3, pp.
794-798, Beijing, Oct.
2000.
[9] D. Graff, ?The 1996 Broadcast News Speech and Language-ModelCorpus,?
Proc.
DARPA Speech Recognition Workshop, Chantilly, VA,pp.
11-14, Feb.
1999.
[10] T. Kemp, A. Waibel, ?UnsupervisedTraining of a Speech Recognizer:Recent Experiments,?
Eurospeech?99, 6, Budapest, pp.
2725-2728,Sept.
1999.
[11] F. Kubala, J. Cohen et al, ?The Hub and Spoke Paradigm for CSREvaluation,?
Proc.
ARPA SpokenLanguageSystems TechnologyWork-shop, Plainsboro, NJ, pp.
9-14, 1994.
[12] L. Lamel, J.L.
Gauvain, G. Adda, ?Lightly Supervised AcousticModel Training,?
Proc.
ISCA ITRW ASR2000, pp.
150-154, Paris,Sept.
2000.
[13] C.J.
Leggetter, P.C.
Woodland, ?Maximum likelihood linear regres-sion for speaker adaptation of continuous density hidden Markovmodels,?
Computer Speech & Language, 9(2), pp.
171-185, 1995.
[14] R.G.
Leonard, ?A Database for speaker-independent digit recogni-tion,?
Proc.
ICASSP, 1984.
[15] D.S.
Pallett, J.G.
Fiscus, et al ?1998 Broadcast News Benchmark TestResults,?
Proc.
DARPA Broadcast News Workshop, pp.
5-12, Hern-don, VA, Feb.
1999.
[16] D. Pallett, J. Fiscus, M. Przybocki, ?Broadcast News 1999 Test Re-sults,?
NIST/NSA Speech Transcription Workshop, College Park, May2000.
[17] D.B.
Paul, J.M.
Baker, ?The Design for the Wall Street Journal-basedCSR Corpus,?
Proc.
ICSLP, Kobe, Nov.
1992.
[18] G. Zavaliagkos, T. Anastsakos et al, ?ImprovedSearch, Acoustic, andLanguage Modeling in the BBN BYBLOS Large Vocabulary CSRSystems,?
Proc.
ARPA Spoken Language Systems Technology Work-shop, Plainsboro, NJ, pp.
81-88, 1994.
[19] G. Zavaliagkos, T. Colthurst, ?Utilizing Untranscribed Training Datato Improve Performance,?
DARPA Broadcast News Transcription andUnderstanding Workshop, Landsdowne, pp.
301-305, Feb. 1998.
