MindNet: acquiring and structuring semanticinformation from textStephen D. R ichardson,  Wi l l i am B. Dolan,  Lucy  VanderwendeMicrosoft ResearchOne Microsoft WayRedmond, WA 98052U.S.A.AbstractAs a lexical knowledge base constructedautomatically from the definitions andexample sentences in two machine-readabledictionaries (MRDs), MindNet embodiesseveral features that distinguish it from priorwork with MRDs.
It is, however, more thanthis static resource alone.
MindNet representsa general methodology for acquiring,structuring, accessing, and exploiting semanticinformation from natural language text.
Thispaper provides an overview of thedistinguishing characteristics of MindNet, thesteps involved in its creation, and its extensionbeyond ictionary text.1 IntroductionIn this paper, we provide a description of the salientcharacteristics and functionality of MindNet as it existstoday, together with comparisons to related work.
Weconclude with a discussion on extending the MindNetmethodology to the processing of other corpora(specifically, to the text of the Microsoft Encarta?
98Encyclopedia) and on future plans for MindNet.
Foradditional details and background on the creation anduse of MindNet, readers are referred to Richardson(1997), Vanderwende (1996), and Dolan et al (1993).2 Full automationMindNet is produced by a fully automatic process,based on the use of a broad-coverage NL parser.
Afresh version of MindNet is built regularly as part of anormal regression process.
Problems introduced bydaily changes to the underlying system or parsinggrammar are quickly identified and fixed.Although there has been much research on the useof automatic methods for extracting information fromdictionary definitions (e.g., Vossen 1995, Wilks et al1996), hand-coded knowledge bases, e.g.
WordNet(Miller et al 1990), continue to be the focus of ongoingresearch.
The Euro WordNet project (Vossen 1996),although continuing in the WordNet tradition, includesa focus on semi-automated procedures for acquiringlexical content.Outside the realm of NLP, we believe thatautomatic procedures uch as MindNet's provide theonly credible prospect for acquiring world knowledgeon the scale needed to support common-sensereasoning.
At the same time, we acknowledge thepotential need for the hand vetting of such informationto insure accuracy and consistency in production levelsystems.3 Broad-coverage parsingThe extraction of the semantic informationcontained in MindNet exploits the very same broad-coverage parser used in the Microsoft Word 97grammar checker.
This parser produces yntactic parsetrees and deeper logical forms, to which rules areapplied that generate corresponding structures ofsemantic relations.
The parser has not  been speciallytuned to process dictionary definitions.
Allenhancements to the parser are geared to handle theimmense variety of general text, of which dictionarydefinitions are simply a modest subset.There have been many other attempts to processdictionary definitions using heuristic pattern matching(e.g., Chodorow et al 1985), specially constructeddefinition parsers (e.g., Wilks et al 1996, Vossen1995), and even general coverage syntactic parsers(e.g., Briscoe and Carroll 1993).
However, none ofthese has succeeded in producing the breadth ofsemantic relations across entire dictionaries that hasbeen produced for MindNet.Vanderwende (1996) describes in detail themethodology used in the extraction of the semanticrelations comprising MindNet.
A truly broad-coverageparser is an essential component of this process, and itis the basis for extending it to other sources ofinformation such as encyclopedias nd text corpora.4 Labeled, semantic relationsThe different ypes of labeled, semantic relationsextracted by parsing for inclusion in MindNet are givenin the table below:1098Attribute GoalCauseCo-AgentColorDeep_ObjectDeep.
SubjectDomainHypernymLocationMannerMaterialMeansPossessorPurposeSizeSourceSubclassModifierEquivalent Part UserSynonymTimeTable 1.
Current set of semantic relation types mMindNetThese relation types may be contrasted with simpleco-occurrence statistics used to create networkstructures from dictionaries by researchers includingVeronis and Ide (1990), Kozima and Furugori (1993),and Wilks et al (1996).
Labeled relations, while moredifficult to obtain, provide greater power for resolvingboth structural attachment and word sense ambiguities.While many researchers have acknowledged theutility of labeled relations, they have been at timeseither unable (e.g., for lack of a sufficiently powerfulparser) or unwilling (e.g., focused on purely statisticalmethods) to make the effort to obtain them.
Thisdeficiency limits the characterization f word pairs suchas river~bank (Wilks et al 1996) and write~pen(Veronis and Ide 1990) to simple relatedness, whereasthe labeled relations of MindNet specify precisely therelations river---Part-->bank and write---Means--->pen.5 Semantic relation structuresThe automatic extraction of semantic relations (orsemrels) from a definition or example sentence forMindNet produces a hierarchical structure of theserelations, representing the entire definition or sentencefrom which they came.
Such structures are stored intheir entirety in MindNet and provide crucial contextfor some of the procedures described in later sections ofthis paper.
The semrel structure for a definition of caris given in the figure below.car:"a veh ic le  w i th  3 or usu.
4 whee lsand  dr iven  by  a motor ,  esp.
oneone for car ry ing  peop le"car  ~ HMp> veh ic le  Part> whee l  Tobj  dr ive~Means>- -Purp> car ry~Tob j>motorpeopleFigure 1.
Semrel structure for a definition of car.Early dictionary-based work focused on theextraction of paradigmatic relations, in particularHypernym relations (e.g., car--Hypernym--->vehicle).Almost exclusively, these relations, as well as othersyntagmatic ones, have continued to take the form ofrelational triples (see Wilks et al 1996).
The largercontexts from which these relations have been takenhave generally not been retained.
For labeled relations,only a few researchers (recently, Barri~re and Popowich1996), have appeared to be interested in entire semanticstructures extracted from dictionary definitions, thoughthey have not reported extracting a significant numberof them.6 Full inversion of structuresAfter semrel structures are created, they are fullyinverted and propagated throughout the entire MindNetdatabase, being linked to every word that appears inthem.
Such an inverted structure, produced from adefinition for motorist and linked to the entry for car(appearing as the root of the inverted structure), isshown in the figure below:motorist:'a person who drives, and usu.
owns, a car"'inverted)car~<TobJ--drive~Tsub>--motorist ~ nYP>--person~sub- -own~Tob~>- -  carFigure 2.
Inverted semrel structure from a definition ofmotoristResearchers who produced spreading activationnetworks from MRDs, including Veronis and Ide(1990) and Kozima and Furugori (1993), typically onlyimplemented forward links (from headwords to theirdefinition words) in those networks.
Words were notrelated backward to any of the headwords whosedefinitions mentioned them, and words co-occurring inthe same definition were not related directly.
In thefully inverted structures tored in MindNet, however,all words are cross-linked, no matter where they appear.The massive network of inverted semrel structurescontained in MindNet invalidates the criticism leveledagainst dictionary-based methods by Yarowsky (1992)and Ide and Veronis (1993) that LKBs created fromMRDs provide spotty coverage of a language at best.Experiments described elsewhere (Richardson 1997)demonstrate the comprehensive coverage of theinformation contained in MindNet.Some statistics indicating the size (rounded to thenearest thousand) of the current version of MindNet andthe processing time required to create it are provided inthe table below.
The definitions and example sentencesare from the Longman Dictionary of ContemporaryEnglish (LDOCE) and the American HeritageDictionary, 3 ra Edition (AHD3).1099Dictionaries used LDOCE & AHD 3Time to create (on a P2/266) 7 hoursHeadwords 159,000Definitions (N, V, ADJ) .191,000Example sentences (N, V, ADJ)Unique semantic relations58,000713,000Inverted structures 1,047,000Linked headwords 91,000Table 2.
Statistics on the current version of MindNet7 Weighted pathsInverted semrel structures facilitate the access todirect and indirect relationships between the root wordof each structure, which is the headword for theMindNet entry containing it, and every other wordcontained in the structures.
These relationships,consisting of one or more semantic relations connectedtogether, constitute semrel paths between two words.For example, the semrel path between car and personin Figure 2 above is:car~--Tobj---drive--Tsub--)motorist--Hyp--~person.An extended semrel path is a path created from sub-paths in two different inverted semrel structures.
Forexample, car and truck are not related directly by asemantic relation or by a semrel path from any singlesemrel structure.
However, if one allows the joining ofthe semantic relations car--Hyp--->vehicle andvehicle6--Hyp---Cruck, each from a different semrelstructure, at the word vehicle, the semrel pathcar--Hyp-~vehicle6--Hyp--Cruck results.
Adequatelyconstrained, extended semrel paths have proveninvaluable in determining the relationship betweenwords in MindNet that would not otherwise beconnected.Semrel paths are automatically assigned weightsthat reflect their salience.
The weights in MindNet arebased on the computation of averaged vertexprobability, which gives preference to semanticrelations occurring with middle frequency, and aredescribed in detail in Richardson (1997).
Weightingschemes with similar goals are found in work byBraden-Harder (1993) and Bookman (1994).8 Similarity and inferenceMany researchers, both in the dictionary- andcorpus-based camps, have worked extensively ondeveloping methods to identify similarity betweenwords, since similarity determination is crucial to manyword sense disambiguation and parameter-smoothing/inference procedures.
However, someresearchers have failed to distinguish betweensubstitutional similarity and general relatedness.
Thesimilarity procedure of MindNet focuses on measuringsubstitutional similarity, but a function is also providedfor producing clusters of generally related words.Two general strategies have been described in theliterature for identifying substitutional similarity.
Oneis based on identifying direct, paradigmatic relationsbetween the words, such as Hypernym or Synonym.For example, paradigmatic relations in WordNet havebeen used by many to determine similarity, including Liet al (1995) and Agirre and Rigau (1996).
The otherstrategy is based on identifying syntagmatic relationswith other words that similar words have in common.Syntagmatic strategies for determining similarity haveoften been based on statistical analyses of large corporathat yield clusters of words occurring in similar bigramand trigram contexts (e.g., Brown et al 1992,Yarowsky 1992), as well as in similar predicate-argument structure contexts (e.g., Grishman andSterling 1994).There have been a number of attempts to combineparadigmatic and syntagmatic similarity strategies (e.g.,Hearst and Grefenstette 1992, Resnik 1995).
However,none of these has completely integrated bothsyntagmatic and paradigmatic nformation into a singlerepository, as is the case with MindNet.The MindNet similarity procedure is based on thetop-ranked (by weight) semrel paths between words.For example, some of the top semrel paths in MindNetbetween pen and pencil, are shown below:pen6-Means---draw--Means-->pencilpen<--Means--write--Means--~pencilpen--Hyp-->instrument~--Hyp---pencilpen--Hyp-->write--Means---~pencilpen6-Means--write6--Hyp--pencilTable 3.
Highly weighted semrel paths between pen andpencilIn the above example, a pattern of semrel symmetryclearly emerges in many of the paths.
This observationof symmetry led to the hypothesis that similar wordsare typically connected in MindNet by semrel paths thatfrequently exhibit certain patterns of relations(exclusive of the words they actually connect), manypatterns being symmetrical, but others not.Several experiments were performed in which wordpairs from a thesaurus and an anti-thesaurus (the lattercontaining dissimilar words) were used in a trainingphase to identify semrel path patterns that indicatesimilarity.
These path patterns were then used in atesting phase to determine the substitutional similarityor dissimilarity of unseen word pairs (algorithms aredescribed in Richardson 1997).
The results,summarized in the table below, demonstrate thestrength of this integrated approach, which uniquelyexploits both the paradigmatic and the syntagmaticrelations in MindNet.1100Training: over 100,000 word pairs from a thesaurusand anti-thesaurus produced 285,000 semrel pathscontaining approx.
13,500 unique path patterns.Testing: over 100,000 (different) word pairs from athesaurus and anti-thesaurus were evaluated using thepath patterns.
Similar correct Dissimilar correct84% 82%Human benchmark: random sample of 200 similarand dissimilar word pairs were evaluated by 5 humansand by MindNet: Similar correct Dissimilar correctHumans: 83% 93%MindNet: 82% 80%Table 4.
Results of similari O, experimentThis powerful similarity procedure may also beused to extend the coverage of the relations in MindNet.Equivalent o the use of similarity determination icorpus-based approaches to infer absent n-grams ortriples (e.g., Dagan et al 1994, Grishman and Sterling1994), an inference procedure has been developedwhich allows semantic relations not presently inMindNet to be inferred from those that are.
It alsoexploits the top-ranked paths between the words in therelation to be inferred.
For example, if the relationwatch--Means-->telescope were not in MindNet, itcould be inferred by first finding the semrel pathsbetween watch and telescope, examining those paths tosee if another word appears in a Means relation withtelescope, and then checking the similarity between thatword and watch.
As it turns out, the word observesatisfies these conditions in the path:watch--Hyp-->observe--Means->telescopeand therefore, it may be inferred that one can watch byMeans of a telescope.
The seamless integration of theinference and similarity procedures, both utilizing theweighted, extended paths derived from inverted semrelstructures in MindNet, is a unique strength of thisapproach.9 Disambiguating MindNetAn additional level of processing during thecreation of MindNet seeks to provide sense identifierson the words of semrel structures.
Typically, wordsense disambiguation (WSD) occurs during the parsingof definitions and example sentences, following theconstruction of logical forms (see Braden-Harder,1993).
Detailed information from the parse, bothmorphological nd syntactic, sharply reduces the rangeof senses that can be plausibly assigned to each word.Other aspects of dictionary structure are also exploited,including domain information associated with particularsenses (e.g., Baseball).In processing normal input text outside of thecontext of MindNet creation, WSD relies crucially oninformation from MindNet about how word senses arelinked to one another.
To help mitigate thisbootstrapping problem during the initial construction ofMindNet, we have experimented with a two-passapproach to WSD.During a first pass, a version of MindNet that doesnot include WSD is constructed.
The result is asemantic network that nonetheless contains a great dealof "ambient" information about sense assignments.
Forinstance, processing the definition spin 101: (of aspider or silkworm) to produce thread.., yields asemrel structure in which the sense node spinlO1 islinked by a DeepSubject  relation to theundisambiguated form spider.
On the subsequent pass,this information can be exploited by WSD in assigningsense 101 to the word spin in unrelated efinitions:wolf_spider I00: any of various spiders...that...do n tspin webs.
This kind of bootstrapping reflects thebroader nature of our approach, as discussed in the nextsection: a fully and accurately disambiguated MindNetallows us to bootstrap senses onto words encountered infree text outside the dictionary domain.10 MindNet as a methodologyThe creation of MindNet was never intended to bean end unto itself.
Instead, our emphasis has been onbuilding a broad-coverage NLP understanding system.We consider the methodology for creating MindNet toconsist of a set of general tools for acquiring,structuring, accessing, and exploiting semanticinformation from NL text.Our techniques for building MindNet are largelyrule-based.
However we arrive at these representations,though, the overall structure of MindNet can beregarded as crucially dependent on statistics.
We havemuch more in common with traditional corpus-basedapproaches than a first glance might suggest.
Anadvantage we have over these approaches, however, isthe rich structure imposed by the parse, logical form,and word sense disambiguation components of oursystem.
The statistics we use in the context of MindNetallow richer metrics because the data themselves arericher.Our first foray into the realm of processing free textwith our methods has already been accomplished; Table2 showed that some 58,000 example sentences fromLDOCE and AHD3 were processed in the creation ofour current MindNet.
To put our hypothesis to a muchmore rigorous test, we have recently embarked on theassimilation of the entire text of the Microsoft Encarta?98 Encyclopedia.
While this has presented several newchallenges in terms of volume alone, we havenevertheless successfully completed a first pass andhave produced and added semrel structures from theEncarta?
98 text to MindNet.
Statistics on that passare given below:1101Processin\[g time (on a P2/266)SentencesWordsAverage words/sentenceNew headwords inMindlqet34 hours497,00010,900,00022220,000New inverted structures inMindNet 5,600,000Table 5.
Statistics for Microsoft Encarta?
98Besides our venture into additional English data, wefully intend to apply the same methodologies totext inother languages as well.
We are currently developingNLP systems for 3 European and 3 Asian languages:French, German, and Spanish; Chinese, Japanese, andKorean.
The syntactic parsers for some of theselanguages are already quite advanced and have beendemonstrated publicly.
As the systems for theselanguages mature, we will create correspondingMindNets, beginning, as we did in English, with theprocessing of machine-readable reference materials andthen adding information gleaned from corpora.11 References:Agirre, E., and G. Rigau.
1996.
Word sensedisambiguation using conceptual density.
InProceedings of COLING96, 16-22.Barri~re, C., and F. Popowich.
1996.
Conceptclustering and knowledge integration from a children'sdictionary.
In Proceedings of COLING96, 65-70.Bookman, L. 1994.
Trajectories through knowledgespace: A dynamic framework for machinecomprehension.
Boston, MA: Kluwer AcademicPublishers.Braden-Harder, L. 1993.
Sense disambiguationusing an online dictionary.
In Natural anguageprocessing: The PLNLP approach, ed.
K. Jensen, G.Heidorn, and S. Richardson, 247-261.
Boston, MA:Kluwer Academic Publishers.Briscoe, T., and J. Carroll.
Generalized probabilisticLR parsing of natural language (corpora) withunification-based grammars.
Computational Linguistics19, no.
1:25-59.Brown, P., V. Della Pietra, P. deSouza, J. Lai, andR.
Mercer.
1992.
Class-based n-gram models of naturallanguage.
Computational Linguistics 18, no.
4:467-479.Chodorow, M., R. Byrd, and G. Heidorn.
1985.Extracting semantic hierarchies from a large on-linedictionary.
In Proceedings of the 23 rd Annual Meetingof the ACL, 299-304.Dagan, I., F. Pereira, and L. Lee.
1994.
Similarity-based estimation of word cooccurrence probabilities.
InProceedings of the 32 nd Annual Meeting of the A CL,272-278.Dolan, W., L. Vanderwende, and S. Richardson.1993.
Automatically deriving structured knowledgebases from on-line dictionaries.
In Proceedings of theFirst Conference of the Pacific Association forComputational Linguistics (Vancouver, Canada), 5-14.Grishman, R., and J.
Sterling.
1994.
Generalizingautomatically generated selectional patterns.
InProceedings of COLING94, 742-747.Hearst, M., and G. Grefenstette.
1992.
Refiningautomatically-discovered lexical relations: Combiningweak techniques for stronger results.
In Statistically-Based Natural Language Programming Techniques,Papers from the 1992 AAAI Workshop (Menlo Park,CA), 64-72.Ide, N., and J. Veronis.
1993.
Extracting knowledgebases from machine-readable dictionaries: Have wewasted our time?
In Proceedings of KB&KS '93(Tokyo), 257-266.Kozima, H., and T. Furugori.
1993.
Similaritybetween words computed by spreading activation on anEnglish dictionary.
In Proceedings of the 6 thConference of the European Chapter of the ACL, 232-239.Li, X., S. Szpakowicz, and S. Matwin.
1995.
AWordNet-based algorithm for word sensedisambiguation.
I  Proceedings oflJCAI'95, 1368-1374.Miller, G., R. Beckwith, C. Fellbaum, D. Gross, andK.
Miller.
1990.
Introduction to WordNet: an on-linelexical database.
In International Journal ofLexicography 3, no.
4:235-244.Resnik, P. 1995.
Disambiguating oun groupingswith respect to WordNet senses.
In Proceedings of theThird Workshop on Very Large Corpora, 54-68.Richardson, S. 1997.
Determining similarity andinferring relations in a lexical knowledge base.
PhD.dissertation, City University of New York.Vanderwende, L. 1996.
The analysis of nounsequences using semantic information extracted fromon-line dictionaries.
Ph.D. dissertation, GeorgetownUniversity, Washington, DC.Veronis, J., and N. Ide.
1990.
Word sensedisambiguation with very large neural networksextracted from machine readable dictionaries.
InProceedings of COLING90, 289-295.Vossen, P. 1995.
Grammatical nd conceptualindividuation i the lexicon.
PhD.
diss.
University ofAmsterdam.Vossen, P. 1996: Right or Wrong.
Combininglexical resources in the EuroWordNet project.
In: M.Gellerstam, J. Jarborg, S. Malmgren, K. Noren, L.Rogstrom, C.R.
Papmehl, Proceedings of Euralex-96,Goetheborg, 1996, 715-728Wilks, Y., B. Slator, and L. Guthrie.
1996.
Electricwords: Dictionaries, computers, and meanings.Cambridge, MA: The MIT Press.Yarowsky, D. 1992.
Word-sense disambiguationusing statistical models of Roget's categories trained onlarge corpora.
In Proceedings of COLING92, 454-460.1102
