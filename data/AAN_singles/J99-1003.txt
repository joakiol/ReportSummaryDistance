Bitext Maps and Alignment via PatternRecognitionI.
Dan  Me lamed*West GroupTexts that are available in two languages (bitexts) are becoming more and more plentiful, both inprivate data warehouses and on publicly accessible sites on the World Wide Web.
As with otherkinds of data, the value ofbitexts largely depends on the efficacy of the available data mining tools.The first step in extracting useful information from bitexts is to find corresponding words and~ortext segment boundaries in their two halves (bitext maps).This article advances the state of the art ofbitext mapping by formulating the problem in termsof pattern recognition.
From this point of view, the success of a bitext mapping algorithm hingeson how well it performs three tasks: signal generation, oise filtering, and search.
The SmoothInjective Map Recognizer (SIMR) algorithm presented here integrates innovative approaches toeach of these tasks.
Objective valuation has shown that SIMR's accuracy is consistently high forlanguage pairs as diverse as French~English and Korean~English.
If necessary, S IMR's bitext mapscan be efficiently converted into segment alignments using the Geometric Segment Alignment(GSA) algorithm, which is also presented here.SIMR has produced bitext maps for over 200 megabytes of French-English bitexts.
GSA hasconverted these maps into alignments.
Both the maps and the alignments are available from theLinguistic Data Consortium}1.
IntroductionExisting translations contain more solutions to more translation prob-lems than any other existing resource (Isabelle 1992).Although the above statement was made about translation problems faced by humantranslators, recent research (Brown et al 1993; Melamed 1996b) suggests that it alsoapplies to problems in machine translation.
Texts that are available in two languages(bitexts) (Harris 1988) also play a pivotal role in various less automated applications.For example, bilingual exicographers can use bitexts to discover new cross-languagelexicalization patterns (Catizone, Russell, and Warwick 1993; Gale and Church 1991b);students of foreign languages can use one half of a bitext to practice their readingskills, referring to the other half for translation when they get stuck (Nerbonne t al.1997).
Bitexts are of little use, however, without an automatic method for matchingcorresponding text units in their two halves.The bitext mapping problem can be formulated in terms of pattern recognition.From this point of view, the success of a bitext mapping algorithm hinges on threetasks: signal generation, noise filtering, and search.
This article presents the SmoothInjective Map Recognizer (SIMR), a generic pattern recognition algorithm that is partic-* 610 Opperman Drive, #D1-66F, Eagan, MN,  551231 See http ://www.
idc.
upenn, edu/idc/catalog/html/text_html/hansfreng, html(~) 1999 Association for Computational LinguisticsComputational Linguistics Volume 25, Number 1terminusX.~_._o..,~0 .
o.. ~ "  diagonalo0IIorigin x = character  posit ion in text 1Figure 1A bitext space.ularly well suited to mapping bitext correspondence.
SIMR demonstrates that, giveneffective signal generators and noise filters, it is possible to map bitext correspon-dence with high accuracy in linear space and time.
If necessary, SIMR can be usedwith the Geometric Segment Alignment (GSA) algorithm, which uses segment bound-ary information to reduce general bitext maps to segment alignments.
Evaluations onpreexisting gold standards have shown that SIMR's bitext maps and GSA's alignmentsare more accurate than those of comparable algorithms in the literature.The article begins with a geometric interpretation f the bitext mapping problemand a discussion of previous work.
SIMR is detailed in Section 4 and evaluated inSection 6.
Section 7 discusses the formal relationship between bitext maps and seg-ment alignments.
The GSA algorithm for converting from the former to the latter ispresented in Section 7 and evaluated in Section 8.2.
Bitext GeometryEach bitext defines a rectangular bitext space, as illustrated in Figure 1.
The lower leftcorner of the rectangle is the origin of the bitext space and represents he two texts'beginnings.
The upper right corner is the terminus and represents he texts' ends.
Theline between the origin and the terminus is the main diagonal.
The slope of the maindiagonal is the bitext slope.Each bitext space is spanned by a pair of axes.
The lengths of the axes are thelengths of the two component texts.
The axes of a bitext space are measured in char-acters, because text lengths measured in characters correlate better than text lengthsmeasured in tokens (Gale and Church 1991a).
This correlation is important for geo-metric bitext mapping heuristics, uch as those described in Section 4.4.
Although theaxes are measured in characters, I will argue that word tokens are the optimum levelof analysis for bitext mapping.
By convention, each token is assigned the position ofits median character.Each bitext space contains a number of true points of correspondence (TPCs),other than the origin and the terminus.
TPCs exist both at the coordinates ofmatching108Melamed Bitext Maps and Alignmenttext units and at the coordinates of matching text unit boundaries.
If a token at positionp on the x-axis and a token at position q on the y-axis are translations of each other,then the coordinate (p, q) in the bitext space is a TPC.
If a sentence on the x-axis endsat character r and the corresponding sentence on the y-axis ends at character s, thenthe coordinate (r + .5, s + .5) is a TPC.
The .5 is added because it is the inter-sentenceboundaries that correspond, rather than the last characters of the sentences.
Similarly,TPCs arise from corresponding boundaries between paragraphs, chapters, list items,etc.
Groups of TPCs with a roughly linear arrangement in the bitext space are calledchains.Bitext maps are injective (1-to-1) partial functions in bitext spaces.
A complete setof TPCs for a particular bitext is the true bitext map (TBM).
The purpose of a bitextmapping algorithm is to produce bitext maps that are the best possible approximationsof each bitext's TBM.3.
Previous WorkEarly bitext mapping algorithms focused on finding corresponding sentences (De-bifi and San~nouda 1992; Kay and R6scheisen 1993).
Although sentence maps aretoo coarse for some bitext applications (Melamed 1996a; Macklovitch 1996), sentenceswere a relatively easy starting point, because their order rarely changes during trans-lation.
Therefore, most sentence mapping algorithms ignore the possibility of crossingcorrespondences and aim to produce only an alignment.
Given parallel texts U andV, an alignment is a segmentation of U and V into n segments each, so that for eachi, 1 < i < n, ui and vi are mutual translations.
An aligned segment pair ai is an or-dered pair (ui, vi).
Thus, an alignment A can also be defined as a sequence of alignedsegments: A =- (al .
.
.
.
,an).
In 1991, two teams of researchers independently discov-ered that sentences from bitexts involving clean translations can be aligned with highaccuracy just by matching sentence sequences with similar lengths (Brown, Lai, andMercer 1991; Gale and Church 1991a).
Both teams approached the alignment problemvia maximum-likelihood estimation, but using different models.Brown, Lai, and Mercer (1991) formulated the problem as a hidden Markov model(HMM), based on a two-stage generative process.
Stage one generated some numberof aligned segment pairs; stage two decided how many segments from each half ofthe bitext to put in each aligned segment pair.
Brown, Lai, and Mercer (1991) tookadvantage of various lexical "anchors" in the bitext that they were experimentingwith.
These anchors were also generated by the HMM, according to their respectiveprobability functions.
All the hidden variables were estimated using the EM algorithm(Dempster, Laird, and Rubin 1977).Gale and Church (1991a) began with a less structured model and proceeded toestimate its parameters through a series of approximations.
Given the set A of allpossible alignments, the maximum-likelihood alignment isAm~x = arg ~cax Pr(A I U, V).
(1)Gale and Church first assumed that the probability of any aligned segment pair isindependent of any other segment pair:IAIAmax = argmax \ ]1  Pr(ailui, vi).
(2)V AE,A  a .a .i=1Next, they assumed that the only feature of b/i and v i that influences the probability of109Computational Linguistics Volume 25, Number 1Table 1Alignment algorithms that don't look at the words can fumble in bitext regions like this voterecord.
Source: Chen (1996)English FrenchMr.
McInnis?
M. McInnis?Yes.
Oui.Mr.
Saunders?
M. Saunders?No.
Non.Mr.
Cossitt?
M. Cossitt?Yes.
Oui.their alignment is a function d(ui, Vi) of the difference in their lengths, in characters:IAIAmax ~- arg max \ ]1  Pr(aild(ui, vi)).
(3)AC.A ~a.i=1By Bayes' rule,Ial Pr(d(ui, vi)lai) Pr(ai)Amax -- arg max \ ]1  (4) - AE~4 ~'L Pr(d(ui, vi))i=1Ignoring the normalizing constant Pr(d(ui, vi)) and taking the logarithm, Gale andChurch arrived atIAIArea x = arg max ~ log Pr(d(ui, Vi)lai) Pr (a i ) .
(5)v AGAi=1Gale and Church empirically estimated the distributions Pr(d(ui, vi)lai) and Pr(ai) froma hand-aligned training bitext and then used dynamic programming to solve Equa-tion 5.The length-based alignment algorithms work remarkably well on language pairslike French/English and German/English, considering how little information they use.However, length correlations are not as high when either of the languages involveddoes not use a phonetically based alphabet (e.g., Chinese).
Even in language pairswhere the length correlation is high, length-based algorithms can fumble in bitextregions that contain many segments of similar length, like the vote record in Table 1.The only way to ensure a correct alignment in such cases is to look at the words.
Forthis reason, Chen (1996) added a statistical translation model to the Brown, Lai, andMercer alignment algorithm, and Wu (1994) added a translation lexicon to the Galeand Church alignment algorithm.A translation lexicon T can be represented asa sequence of t entries, where eachentry is a pair of words: T ~ /(xl,yl) .
.
.
.
.
(xt, yt)l .
Roughly speaking, Wu (1994) ex-tended Gale and Church's (1991a) method with a matching function m(u, v,j), whichwas equal to one whenever xj E u and yj E v for lexicon entry (xj, yj), and zerootherwise?
The information in the matching function was then used along with theinformation i  d(ui, Vi) to condition the probability of alignments in Equation 3:IAIAmax = arg max H Pr(aild( ui" vi ) ; m( ui, vi, 1) .
.
.
.
.
m( ui, vi, t ) ).
(6)A .Ai=1110Melamed Bitext Maps and AlignmentFrom this point, Wu proceeded along the lines of Equations 4 and 5 and the dynamicprogramming solution.Another interesting approach is possible when part-of-speech taggers are availablefor both languages.
The insight that parts of speech are usually preserved in translationenabled Papageorgiou, Cranias, and Piperidis (1994) to design an alignment algorithmthat maximizes the number of matching parts of speech in aligned segments.
It isdifficult to compare this algorithm's performance to that of other algorithms in theliterature, because results were only reported for a relatively easy bitext.
On this bitext,the algorithm's performance was nearly perfect.
A translation model between parts ofspeech would not help on bitext regions like the one in Table 1.The alignment algorithms described above work nearly perfectly given clean bi-texts that have easily detectable sentence boundaries.
However bitext mapping at thesentence level is not an option for many bitexts (Church 1993).
Sentences are oftendifficult to detect, especially when punctuation is missing due to OCR errors.
Moreimportantly, bitexts often contain lists, tables, titles, footnotes, citations and/or mark-up codes that foil sentence alignment methods.
Church's solution was to map bitextcorrespondence at the level of the smallest ext units--characters.
Characters matchacross languages to the extent hat they participate in orthographic ognates--wordswith similar meanings and spellings in different languages.
Since there are far morecharacters than sentences in any bitext, the quadratic omputational complexity of thisapproach presented an efficiency problem.
Church showed how to use a high-bandfilter to find a rough bitext map quickly.Church's rough bitext maps were intended for input into Dagan, Church, andGale's (1993) slower algorithm for refinement.
Dagan, Church, and Gale used the roughbitext map to define a distance-based model of co-occurrence.
Then they adaptedBrown et al's (1993) statistical translation Model 2 to work with this model of co-occurrence.
2 The information in the translation model was more reliable than character-level cognate information, so it produced a higher signal-to-noise ratio in the bitextspace.
Therefore, Dagan, Church, and Gale were able to filter out many of the imper-fections of the initial bitext map.A limitation of Church's method, and therefore also of Dagan, Church, and Gale'smethod, is that orthographic ognates exist only among languages with similar al-phabets (Church et al 1993).
Fung investigated ways to make these methods usefulwhen cognates cannot be found.
First, working with Church, she introduced the K-Vec algorithm (Fung and Church 1994), which used a rough model of co-occurrence tobootstrap a small translation lexicon.
The translation lexicon indicated points of cor-respondence in the bitext map, much the same way as matching character n-grams.These points of correspondence ould then be further refined using the methods pre-viously developed by Church (1993) and Dagan, Church, and Gale (1993).
Later, Fungand McKeown (1994) improved on K-Vec by employing relative position offsets, in-stead of a fixed model of co-occurrence.
This strategy made the algorithm more robustfor noisier bitexts.4.
The Smooth Injective Map Recognizer (SIMR)4.1 OverviewSIMR borrows several insights from previous work.
Like the algorithms ofGale and Church (1991a) and Brown, Lai, and Mercer (1991), SIMR exploits the cor-2 See Melamed (1998a) for a general discussion ofmodels of co-occurrence.111Computational Linguistics Volume 25, Number 1relation between the lengths of mutual translations.
Like char_align (Church 1993),SIMR infers bitext maps from likely points of correspondence b tween the two texts,points that are plotted in a two-dimensional space of possibilities.
Unlike previousmethods, SIMR greedily searches for only a small chain of correspondence points ata time.The search begins in a small search rectangle in the bitext space, whose diagonal isparallel to the main diagonal.
The search for each chain alternates between a generationphase and a recognition phase.
In the generation phase, SIMR generates candidatepoints of correspondence within the search rectangle that satisfy the supplied matchingpredicate, as explained in Section 4.2.
In the recognition phase, SIMR invokes the chainrecognition heuristic to select the most likely chain of true points of correspondence(TPCs) among the generated points.
The most likely chain of TPCs is the set of pointswhose geometric arrangement most resembles the typical arrangement of TPCs.
Theparameters of the chain recognition heuristic are optimized on a small training bitext.If no suitable chains are found, the search rectangle is proportionally expanded bythe minimum possible amount and the generation-recognition cycle is repeated.
Therectangle keeps expanding until at least one acceptable chain is found.
If more thanone acceptable chain is found in the same cycle, SIMR accepts the chain whose pointsare least dispersed around its least-squares line.
Each time SIMR accepts a chain, itmoves the search rectangle to another egion of the bitext space to search for the nextchain.SIMR employs a simple heuristic to select regions of the bitext space to search.
Toa first approximation, true bitext maps are monotonically increasing functions.
Thismeans that if SIMR accepts one chain, it should look for others either above and tothe right or below and to the left of the one it has just found.
All SIMR needs is aplace to start the trace, and a good place to start is at the beginning.
Since the originof the bitext space is always a TPC, the first search rectangle is anchored at the origin.Subsequent search rectangles are anchored at the top right corner of the previouslyfound chain, as shown in Figure 2.The expanding rectangle search strategy makes SIMR robust in the face of TBMdiscontinuities.
Figure 2 shows a segment of the TBM that contains a vertical gap (anomission in the text on the x-axis).
As the search rectangle grows, it will eventuallyintersect with the TBM, even if the discontinuity is quite large (Melamed 1996a).
Thenoise filter described in Section 4.3 reduces the chances that SIMR will be led astrayby false points of correspondence.4.2 Point GenerationBefore SIMR can decide where to generate candidate points of correspondence, itmust be told which pairs of words have coordinates within the boundaries of thecurrent search rectangle.
The mapping from tokens to axis positions is performed bya language-specific axis generator (Melamed 1998b).
SIMR calls one of its matchingpredicates on each pair of tokens whose coordinate falls within the search rectangle.A matching predicate is a heuristic for deciding whether two given tokens might bemutual translations.
Two kinds of information that a matching predicate can rely onmost often are cognates and translation lexicons.Two words are orthographic ognates if they have the same meaning and similarspellings.
Similarity of spelling can be measured in more or less complicated ways.The first published attempt o exploit cognates for bitext mapping purposes (Simard,Foster, and Isabelle 1992) deemed two alphabetic tokens cognates if their first fourcharacters were identical.
This criterion proved surprisingly effective, given its sim-plicity.
However, like all heuristics, it produced some false positives and some false112Melamed Bitext Maps and Alignment?
discovered TPC next ~ oo undiscovered me( meccha in / J~ o '~~ noise ~ ~J search ~ ~ ntier fro i r ~main ~searchV.~ searchdiag/~,,~" ~ t frontier rectangle~ oO O O .
?
?
previous chainFigure 2SIMR's "expanding rectangle" search strategy.
The search rectangle is anchored at the top rightcorner of the previously accepted chain.
Its diagonal remains parallel to the main diagonal.negatives.
An example of a false negative is the word pair government and gouverne-ment.
The false positives were often words with a big difference in length, like conseiland conservative.
These examples suggest that a more accurate cognate criterion canbe driven by approximate string matching.
For example, McEnery and Oakes (1995)threshold the Dice coefficient of matching character bigrams in each pair of candidatecognates.
The matching predicates in SIMR's current implementation threshold theLongest Common Subsequence Ratio (LCSR).The LCSR of two tokens is the ratio of the length of their longest (not necessar-ily contiguous) common subsequence (LCS) and the length of the longer token.
Insymbols,LCSR(A, B) = length\[LCS(A, B)\]max\[length(A), length(B)\] " (7)For example, gouvernement, which is 12 characters long, has 10 characters that appearin the same order in government.
So, the LCSR for these two words is 10/12.
On theother hand, the LCSR for conseil and conservative is only 6/12.
A simple dynamicprogramming algorithm (Bellman 1957) can compute the LCS in O(n2).
A rather morecomplicated algorithm can compute it in O(n log logn) time on average (Hunt andSzymanski 1977).When dealing with language pairs that have different alphabets, the matchingpredicate can employ phonetic ognates.
When language L1 borrows a word fromlanguage L2, the word is usually written in L1 similarly to the way it sounds in L2.Thus, French and Russian /portm0n0/ are cognates, as are English /sIstom/ andJapanese/~isutemu/.
For many languages, it is not difficult o construct an approxi-mate mapping from the orthography toits underlying phonological form.
Given sucha mapping for L1 and L2, it is possible to identify cognates despite incomparableorthographies.113Computational Linguistics Volume 25, Number 1Knight and Graehl (1997) have shown that it is possible to find phonetic ognateseven between languages whose writing systems are as different as those of Englishand Japanese.
They have built a weighted finite-state automaton (WFSA), based onempirically estimated probability distributions, for back-transliterating English loanwords written in katakana into their original English form.
The WFSA efficiently rep-resents a large number of transliteration probabilities between words written in thekatakana nd Roman alphabets.
Standard finite-state techniques can efficiently find themost likely path through the WFSA from a Japanese word written in katakana to anEnglish word.
The weight of the most likely path is an estimate of the probability thatthe former is a transliteration of the latter.
Thresholding this probability would leadto a phonetic ognate matching predicate for English/Japanese bitexts.
The thresholdwould need to be optimized together with SIMR's other parameters, the same waythe LCSR threshold is currently optimized (see Section 5).Cognates are more common in bitexts from more similar language pairs, and fromtext genres where more word borrowing occurs, such as technical texts.
In the non-technical Canadian Hansards (parliamentary debate transcripts published in Englishand in French), an LCSR cutoff of .58 finds cognates for roughly one quarter of alltext tokens.
Even distantly related languages like English and Czech will share a largenumber of orthographic cognates in the form of proper nouns, numerals, and punctu-ation.
When one or both of the languages involved is written in pictographs, cognatescan still be found among punctuation and numerals.
However, these kinds of cognatesare usually too sparse to build an accurate bitext map from.When the matching predicate cannot generate nough candidate correspondencepoints based on cognates, its signal can be strengthened by a seed translation lexicon--a simple list of word pairs that are believed to be mutual translations.
Seed translationlexicons can be extracted from machine-readable bilingual dictionaries (MRBDs) inthe rare cases where MRBDs are available.
In other cases, they can be constructedautomatically or semiautomatically using any of several published methods (Fungand Church 1994; Fung 1995; Melamed 1996b; Resnik & Melamed 1997).
3A matchingpredicate based on a seed translation lexicon deems two candidate tokens to be mutualtranslations if the token pair appears in the lexicon.
Since the matching predicate neednot be perfectly accurate, the seed translation lexicons need not be perfectly accurateeither.All the matching predicates described above can be fine-tuned with stop lists forone or both languages.
For example, closed-class words are unlikely to have cog-nates.
Indeed, French/English words like a, an, on, and par often produce spuriouspoints of correspondence.
The same problem is caused by faux amis ("false friends")(Macklovitch 1996).
These are words with similar spellings but different meanings indifferent languages.
For example, the French word librarie means 'bookstore,' not'library,' and actuel means 'current,' not 'actual.'
A matching predicate can use alist of closed-class words and/or  a list of pairs of faux amis to filter out spuriousmatches.3 Most published methods for automatically constructing translation lexicons require a preexisting bitextmap, which seems to render them useless for the purposes of bitext mapping algorithms.
Fortunately,only one seed translation lexicon is required for each language pair, or at worst for each sublanguage.If we expect to map many bitexts in the same language pair, then it becomes feasible to spend a fewhours creating one bitext map by hand.
Melamed (1996c) explains how to do so quickly and efficiently.Better yet, Fung (1995) shows how it may be possible to extract asmall translation lexicon and a roughbitext map simultaneously.114Melamed Bitext Maps and Alignmentaa la ax"~ a~o aUJah h h hFrench  textFigure 3Frequent word types cause false points of correspondence that line up in rows and columns.4.3 Noise FilterInspection of several bitext spaces has revealed a common oise pattern, illustrated inFigure 3.
It consists of correspondence points that line up in rows or columns associatedwith frequent word types.
Word types like the English article a can produce one ormore correspondence points for almost every sentence in the opposite text.
Only onepoint of correspondence in each row and column can be correct; the rest are noise.
Itis difficult to measure xactly how much noise is generated by frequent tokens, andthe proportion is different for every bitext.
Informal inspection of some bitext spacesindicated that frequent tokens are often responsible for the lion's share of the noise.Reducing this source of noise makes it much easier for SIMR to stay on track.Other bitext mapping algorithms mitigate this source of noise either by assigninglower weights to correspondence points associated with frequent word types (Church1993) or by deleting frequent word types from the bitext altogether (Dagan, Church,and Gale 1993).
However, a word type that is relatively frequent overall can be rarein some parts of the text.
In those parts, the word type can provide valuable cluesto correspondence.
On the other hand, many tokens of a relatively rare type can beconcentrated in a short segment of the text, resulting in many false correspondencepoints.
The varying concentration f identical tokens uggests that more localized noisefilters would be more effective.
SIMR's localized search strategy provides a vehicle fora localized noise filter.The filter is based on the maximum point ambiguity level parameter.
For eachpoint p = (x, y), let X be the number of points in column x within the search rectangle,and let Y be the number of points in row y within the search rectangle.
The ambiguitylevel of p is defined as X + Y - 2.
In particular, if p is the only point in its row andin its column, then its ambiguity level is zero.
The chain recognition heuristic ignores115Computational Linguistics Volume 25, Number IOO~OOQ!o Ifalse .chain ?!
?off track"Figure 4SIMR's noise filter makes an important contribution to the signal-to-noise ratio in the bitextspace.
Even if one chain of false points of correspondence slips by the chain recognitionheuristic, the expanding rectangle is likely to find its way back to the TBM trace before thechain recognition heuristic accepts another chain.points whose ambiguity level is too high.
What makes this a localized filter is thatonly points within the search rectangle count toward each other's ambiguity level.The ambiguity level of a given point can change when the search rectangle xpandsor moves.The noise filter ensures that false points of correspondence are relatively sparse,as illustrated in Figure 4.
Even if one chain of false points of correspondence slips bythe chain recognition heuristic, the expanding rectangle is likely to find its way backto the TBM trace before the chain recognition heuristic accepts another chain.
If thematching predicate generates a reasonably strong signal then the signal-to-noise ratiowill be high and SIMR is not likely to get lost, even though it is a greedy algorithmwith no ability to look ahead.4.4 Point SelectionAfter noise tering, most TPC chains conform to the pattern illustrated in Figure 5.The pattern can be characterized by three properties:?
Injectivity: No two points in a chain of TPCs can have the same x- ory-coordinates.?
Linearity: TPCs tend to line up straight.
Recall that sets of points with aroughly linear arrangement are called chains.?
Low Variance of Slope: The slope of a TPC chain is rarely muchdifferent from the bitext slope.SIMR exploits these properties to decide which chains might be TPC chains.
First,chains that lack the injectivity property are rejected outright.
The remaining chains arefiltered using two threshold parameters: maximum point dispersal and maximumangle deviation.
The linearity of each chain is measured as the root mean squared116Melamed Bitext Maps and Alignment98400c ._o(D0Ci..9820098000978009760097400972009700096800109000o o ?oo o %00 G'00 0oOo 0 0o oO/~ooo ooooo oi i i109500 110000 110500 111000position in text 1Figure 5Typical pattern of candidate points of correspondence in a bitext space, after noise filtering.The true points of correspondence trace the true bitext map parallel to the main diagonal.distance of the chain's points from the chain's least-squares line.
If this distance xceedsthe maximum point dispersal threshold, the chain is rejected.
The angle of each chain'sleast-squares line is compared to the arctangent of the bitext slope.
If the differenceexceeds the maximum angle deviation threshold, the chain is rejected.4.5 Reduction of the Search SpaceIn a search rectangle containing n points, there are 2 n possible chains--too manyto search by brute force.
The properties of TPCs listed above provide two ways toconstrain the search.The Linearity property leads to a constraint on the chain size.
Chains of only afew points are unreliable, because they often line up straight by coincidence.
Chainsthat are too big will span too long a segment of the TBM to be well approximated bya line.
SIMR uses a fixed chain size k, 6 < k < 11.
The exact value of k is optimizedtogether with the other parameters, asdescribed in Section 5.
Fixing the chain size atk reduces the number of candidate chains to (~) - n: (n-k)!k!
"For typical values of n and k, (~) can still reach into the millions.
The Low Varianceof Slope property suggests another constraint: SIMR should consider only chains thatare roughly parallel to the main diagonal.
Two lines are parallel if the perpendiculardisplacement between them is constant.
So, chains that are roughly parallel to the maindiagonal will consist of points that all have roughly the same displacement from themain diagonal.
4 Points with similar displacement can be grouped together by sorting,as illustrated in Figure 6.
Then, chains that are most parallel to the main diagonal willbe contiguous ubsequences of the sorted point sequence.
In a region of the bitext4 Displacement can be negative.117Computational Linguistics Volume 25, Number 1subsequence 1 maine~subsequence 8(points 1 thru 6)?1 o2L47 J (points 5 thru 10) (points 8 thru 13)Figure 6The chain recognition heuristic exploits the Low Variance of Slope property of TPC chains.
Thecandidate points of correspondence are numbered according to their displacement from themain diagonal.
The chain most parallel to the main diagonal is always one of the contiguoussubsequences of this ordering.
For a fixed chain size of 6, there are 13 - 6 + 1 = 8 contiguoussubsequences in this region of 13 points.
Of these 8, the fifth subsequence is the best chain.space containing n points, there will be only n - k + 1 such subsequences of lengthk.
The most computationally expensive step in the chain recognition process is theinsertion of candidate points into the sorted point sequence.4.6 EnhancementsThe following subsections describe two of the more interesting enhancements in thecurrent SIMR implementation.4.6.1 Overlapping Chains.
SIMR's fixed chain size imposes a rather arbitrary fragmen-tation on the TBM trace.
Each chain starts at the top-right corner of the previouslyfound chain, but these chain boundaries are independent of discontinuities or anglevariations in the TBM trace.
Therefore, SIMR is likely to miss TPCs wherever the TBMis not linear.
One way to make SIMR more robust is to start the search rectangle justabove the lowest point of the previously found chain, instead of just above the highestpoint.
If the chain size is fixed at k, then each linear stretch of s TPCs will result ins - k + 1 overlapping chains.Unfortunately, this solution introduces another problem: Two overlapping chainscan be inconsistent.
The injective property of TBMs implies that whenever two (inter-polated) chains overlap in the x or y dimensions, but are not identical in the regionof overlap, then one of the chains must be wrong.
To resolve such conflicts, SIMRemploys a postprocessing algorithm to eliminate conflicting chains one at a time, untilall remaining chains are pairwise consistent.
The conflict resolution algorithm is basedon the heuristic that chains that conflict with a larger number of other chains are morelikely to be wrong.
The algorithm sorts all chains with respect o how many otherchains they conflict with, and eliminates them in this sort order, one at a time, until no118Melamed Bitext Maps and Alignmentmaximum angle mare /e > deviation threshold d iagonalso O .--- 'o2nd passsearch spaceFigure 7Chain X is perfectly valid, even though it has a highly deviant slope.
Such chains can berecovered by re-searching regions between accepted chains.
The slope of the local maindiagonal can be quite different from the slope of the global main diagonal.conflicts remain.
Whenever two or more chains are fled in the sort order, the conflictresolution algorithm eliminates all but the chain with the least point dispersal.4.6.2 Additional Search Passes.
To ensure that SIMR rejects purious chains, the max-imum angle deviation threshold must be set low.
However, like any heuristic filter,this one will reject some perfectly valid candidates?
If a more precise bitext map isdesired, some of these valid chains can be recovered uring an extra sweep throughthe bitext space.
Since bitext maps are mostly injective, valid chains that are rejectedby the angle deviation filter usually occur between two accepted chains, as shown inFigure 7.
If Chains C and D are accepted as valid, then the slope of the TBM betweenthe end of Chain C and the start of Chain D must be much closer to the slope ofChain X than to the slope of the main diagonal?
Chain X should be accepted?
Dur-ing a second pass through the bitext space, SIMR searches for sandwiched chains inany space between two accepted chains that is large enough to accommodate anotherchain.
This subspace of the bitext space will have its own main diagonal?
The slopeof this local main diagonal can be quite different from the slope of the global maindiagonal.An additional search through the bitext space also enables SIMR to recover chainsthat were missed because of an inversion in the translation.
Nonmonotonic TBM seg-ments result in a characteristic map pattern, as a consequence of the injectivity of bitextmaps.
SIMR has no problem with small nonmonotonic segments inside chains.
How-ever, the expanding rectangle search strategy can miss larger nonmonotonic segmentsthat do not fit inside one chain.
In Figure 8, the vertical range of segment j corre-sponds to a vertical gap in SIMR's first-pass map.
The horizontal range of segment jcorresponds toa horizontal gap in SIMR's first-pass map.
'Similarly, any nonmonotonicsegment of the TBM will occupy the intersection ofa vertical gap and a horizontal gapin the monotonic first-pass map.
Furthermore, switched segments are usually adjacentand relatively short?
Therefore, to recover nonmonotonic segments of the TBM, SIMRneeds only to search gap intersections that are close to the first-pass map.
There areusually very few such intersections that are large enough to accommodate n w chains,119Computational Linguistics Volume 25, Number 1first a TPC 4- SIMR's ;Z i -a: !
-~- II I :e n -- .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
, : , i  .
.
.
.
r :  .
.
.
.
.
.
.
.
.
.
tj: :=  : .
.
.
.
_i .
.
.
.
.
.
.
...... i~i ~s~_g_ ~i_i i ii ~_ iiiiii~_e i '~= .....L ..... ~_ .........Figure 8Segments i and j switched places during translation.
Any nonmonotonic segment of the TBMwill occupy the intersection ofa vertical gap and a horizontal gap in the monotonic first-passmap.
These larger nonmonotonic segments can be recovered uring a second sweep throughthe bitext space.so the second-pass earch requires only a small fraction of the computational effort ofthe first pass.5.
Parameter OptimizationSIMR's parameters--the fixed chain size; the LCSR threshold used in the matchingpredicate; and the thresholds for maximum point dispersal, maximum angle devi-ation, and maximum point ambiguity--interact in complicated ways.
Ideally, SIMRshould be reparameterized so that its parameters are pairwise independent.
Then itmay be possible to optimize the parameters analytically, or at least in a probabilisticframework.
For now, the easiest way to optimize these parameters i  via simulatedannealing (Vidal 1993), a simple general framework for optimizing highly interdepen-dent parameter sets.Simulated annealing requires an objective function to optimize.
The objective func-tion for bitext mapping should measure the difference between the TBM and the in-terpolated bitext maps produced with the current parameter set.
In geometric terms,the difference is a distance.
The TBM consists of a set of TPCs.
The distance betweena bitext map and each TPC can be defined in a number of ways.
The simplest metricsare the horizontal distance or the vertical distance, but these metrics measure the errorwith respect to only one language or the other.
A more robust average is the distanceperpendicular to the main diagonal.
In order to penalize large errors more heavily,root mean squared (RMS) distance, rather than mean distance, should be minimized.There is a slight complication i  the computation of distances between two par-tial functions, in that linear interpolation is not well-defined for nonmonotonic sets ofpoints.
It would be incorrect to simply connect the dots left to right, because the result-120Melamed Bitext Maps and Alignment0"AIIiIm09oc0ccOII,0"IIM2 I?, :e?
,0  ?
? '
"  . '
i,, ?..
?-" M1MI."
"?,0"" ?''
L, ,  ,o ?
M20MER.
.
.
.
.
.
.
.
.
Sentence  A .
.
.
.
.
.
.
.
.
.
.
..OFigure 9Two text segments at the end of Sentence A were switched uring translation, resulting in anonmonotonic segment?
To interpolate injective bitext maps, nonmonotonic segments must beencapsulated in Minimum Enclosing Rectangles (MERs).
A unique bitext map can then beinterpolated by using the lower left and upper ight comers of the MER (map M2), instead ofusing the nonmonotonic correspondence points (function M1).Table 2SIMR accuracy on training bitexts for three language pairs.Language Pair Number of Training TPCs Training GenreRMS Errorin CharactersFrench / English 598 marketing report 6.6Spanish / English 562 software manuals 5.5Korean / English 615 military manuals 3.9ing function may not be injective.
To interpolate injective bitext maps, nonmonotonicsegments must be encapsulated in Minimum Enclosing Rectangles (MERs), as shownin Figure 9.
A unique bitext map results from interpolating between the lower left andupper right comers of the MER, instead of using the nonmonotonic correspondencepoints.6.
Evaluation of SIMRSIMR's parameters were optimized by simulated annealing, as described in the pre-vious section?
A separate optimization was performed on separate training bitexts foreach of three language pairs.
SIMR was then evaluated on previously unseen testbitexts in the three language pairs?
The objective function for optimization and theevaluation metric were the root mean squared istance, in characters, between eachTPC and the interpolated bitext map produced by SIMR, where the distance was mea-sured perpendicular to the main diagonal?
Tables 2 and 3 report SIMR's errors on thetraining and test bitexts, respectively?The TBM samples used for training and testing were derived from segment align-ments.
All the bitexts had been manually aligned by bilingual annotators (Melamed121Computational Linguistics Volume 25, Number 1Table 3SIMR error estimates on different text genres in three language pairs.Language Pair Bitext or Genre Number of RMS ErrorTest TPCs in CharactersFrench / English parliamentary debates 7,123 5.7CITI technical reports 365, 305, 176 4.4, 2.6, 9.9other technical reports 561, 1,393 21, 14court transcripts 1,377 3.9U.N.
annual report 2,049 12I.L.O.
report 7,129 6.4Spanish / English software manuals 376, 151, 100, 349 4.6, 0.67, 5.2, 4.7Korean / English military manuals 40, 88, 186, 299 2.6, 7.1, 25, 7.8military messages 192 0.531997).
The alignments were converted into sets of coordinates in the bitext space bypairing the character positions at the ends of aligned segment pairs.
This TBM sam-pling method artificially reduced the error estimates.
Most of the aligned segmentswere sentences, which ended with a period.
Whenever SIMR matched the periodscorrectly, the interpolated bitext map was pulled close to the TPC, even though itmay have been much farther off in the middle of the sentence.
Thus, the results inTable 3 should be considered only relative to each other and to other results obtainedunder the same experimental conditions.
It would be impressive indeed if any bitextmapping algorithm's actual RMS error were less than 1 character on bitexts involvinglanguages with different word order, such as English/Korean.The matching predicates for French/English and Spanish/English relied on anLCSR threshold to find cognates.
The Korean text contained some Roman characterstrings, so the matching predicate for Korean/English generated candidate points ofcorrespondence whenever one of these strings coordinated in the search rectanglewith an identical string in the English half of the bitext.
A seed translation lexiconwas also used to strengthen the Korean/English signal.
In addition, English, French,Spanish and Korean stop lists were used to prevent matches of closed-class words.The translation lexicon and stop lists had been previously developed independentlyof the training and test bitexts.The French/English part of the evaluation was performed on bitexts from the pub-licly available corpus de bi-texte anglais-franfais (BAF) (Simard and Plamondon 1996).SIMR's error distribution on the "parliamentary debates" bitext in this collection isgiven in Table 4.
This distribution can be compared to the error distributions reportedfor the same test set by Dagan, Church, and Gale (1993), who reported parts of theirerror distribution in words, rather than in characters: "In 55% of the cases, there isno error in word_align's output (distance of 0), in 73% the distance from the correctalignment is at most 1, and in 84% the distance is at most 3" (Dagan, Church, andGale 1993, 7).
These distances were measured horizontally from the bitext map ratherthan perpendicularly to the main diagonal.
Given the bitext slope for that bitext and aconservative estimate of 6 characters per word (including the space between words),each horizontal word of error corresponds tojust over 4 characters of error perpendic-ular to the main diagonal.
Thus, Dagan, Church, and Gale's "no error" is the same as122Melamed Bitext Maps and AlignmentTable 4SIMR's error distribution on the French/English "parliamentary debates" bitext.
Errors weremeasured perpendicular to the main diagonal.Number of Test Points Error Range in Characters Fraction of Test Points1 -101 .00012 -80 to -70 .00031 -70 to -60 .00015 -60 to -50 .00074 -50 to -40 .00066 -40 to -30 .00089 -30 to -20 .001329 -20 to -10 .00413,057 -10 to 0 .42923,902 0 to 10 .547843 10 to 20 .006028 20 to 30 .0039 ~17 30 to 40 .00245 40 to 50 .00078 50 to 60 .00111 60 to 70 .00011 70 to 80 .00011 80 to 90 .00011 90 to 100 .00011 110 to 120 .00011 185 .00017,123 -101 to 185 1.000Table 5Comparison of error distributions for SIMR and word_align on the parl iamentary debatesbitext.Error of at Most Error of at Most Error of at MostAlgorithm 2 Characters 6 Characters 14 Charactersword_align 55% 73% 84%SIMR 93% 97% 98%2 characters  of error  or less, i.e., less than hal f  a word .
One word  of error  is the sameas an error  of up  to 6 characters  and  3 words  are equ iva lent  o 4 .3?
= 14 characters.On  this basis,  Table 5 compares  the accuracy of SIMR and word_a l ign .
5Another  interest ing compar i son  is in terms of max imum error.
Certa in  app l icat ionsof b i text  maps ,  such as the one descr ibed  by  Me lamed (1996a), can to lerate many smal lerrors but  no large ones.
As shown in Table 4, S IMR's  bi text  map was  never  off bymore  than 185 characters  f rom any  of the 7,123 segment  boundar ies .
185 charactersis about  1.5 t imes the length of an average sentence (Me lamed 1996a).
The input  toword_a l ign  is the output  of char_a l ign  and  Dagan,  Church,  and  Gale (1993) haverepor ted  that word_a l ign  cannot  escape f rom char_a l ign 's  wors t  errors.
An  indepen-dent  imp lementat ion  of char_a l ign  (Michel S imard,  persona l  communicat ion)  er redby  more  than one thousand characters  on the same bitext.5 Error measurements at the character level are less susceptible torandom variation than measurementsat the word level.
Character-level measurements al o have the advantage ofbeing universallyapplicable to all languages, including those in which words are difficult o identify automatically.123Computational Linguistics Volume 25, Number 1The Spanish/English and Korean/English bitexts were hand-aligned when SIMRwas being ported to these language pairs.
6 The Spanish/English bitexts were drawnfrom the Sun Solaris AnswerBooks and hand-aligned by Philip Resnik.
The Korean/English bitexts were provided by MIT's Lincoln Laboratories and hand-aligned byYoung-Suk Lee.
Table 3 shows that SIMR's performance on Spanish/English and Ko-rean/English bitexts is no worse than its performance on French/English bitexts.The results in Table 3 were obtained using a version of SIMR that included allthe enhancements described in Section 4.6.
It is interesting to consider the degreeto which each enhancement improves performance.
I remapped the French/Englishbitexts listed in Table 3 with two stripped-down versions of SIMR.
One version wasbasic SIMR without any enhancements.
The other version incorporated overlappingchains, but performed only one search pass.
The deterioration i performance variedwidely.
For example, on the parliamentary debates bitext, the RMS error rose from 5.7to 16 when only one search pass was allowed, but rose only another 2points to 18 usingnon-overlapping chains.
In contrast, on the U.N. annual report bitext, the extra searchpasses made no difference at all but non-overlapping chains increased the RMS errorfrom 12 to 40.
For most of the Other bitexts, each enhancement reduced the RMS errorby a few characters, compared to the basic version.
However, the improvement wasnot universal: the RMS error of the basic SIMR was 19 for the "other technical report"on which the enhanced SIMR scored 21.
The expected value of the enhancements isdifficult o predict, because ach enhancement is aimed at solving a particular patternrecognition problem, and each problem may or may not occur in a given bitext.
Therelationship between geometric patterns in TPC chains and syntactic properties ofbitexts is a ripe research topic.7.
AlignmentSIMR has no idea that words are often used to make sentences.
It just outputs a seriesof corresponding token positions, leaving users free to draw their own conclusionsabout how the texts' larger units correspond.
However, many existing translators' toolsand machine translation strategies depend on aligned sentences or other aligned textsegments.
What can SIMR do for them?
Formally, an alignment is a correspondencerelation that does not permit crossing correspondences.
The rest of this article presentsthe Geometric Segment Alignment (GSA) algorithm, which uses segment boundaryinformation to reduce the correspondence r lation in SIMR's output to a segmentalignment.
The GSA algorithm can be applied equally well to sentences, paragraphs,lists of items, or any other text units for which boundary information is available.7.1 Correspondence is Richer than AlignmentA set of correspondence points, supplemented with segment boundary information,expresses egment correspondence, which is a richer representation than segmentalignment.
Figure 10 illustrates how segment boundaries form a grid over the bitextspace.
Each cell in the grid represents he intersection of two segments, one from eachhalf of the bitext.
A point of correspondence inside cell (X,y) indicates that some tokenin segment X corresponds with some token in segment y; i.e., segments X and y corre-spond.
For example, Figure 10 indicates that segment e corresponds with segments Gand H.In contrast to a correspondence relation, "an alignment is a segmentation f the6 The porting method isdetailed elsewhere (Melamed 1996c, 1997, 1998b).124Melamed Bitext Maps and Alignment._~ hX: kgo fif)o e~ dfflCbaI l l l  II l l l  II o ei 'i ?i ?I ?
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
i .
.
.
.
.
.
.iiI.
.
.
.
I - - T - - I - - T  .
.
.
.
~ - - /  I O .
.
.
.
.
.
T .
.
.
.
.
.
.
I .
.
.
.
.I ~ 1 1  t i o I Ii I l l  I I I i. .
.
.
i - - T - - i - -  T .
.
.
.
T .
.
.
.
.
.
i~  .
.
.
.
.
.
.
.
.
.
.
.
.
.
T .
.
.
.
.
.
.
i .
.
.
.
.i I 1 ~  I I I Ii l l l  I O i  II I I I  i O ~  r I. .
.
.
i - - T - - l - - l l - - i  i l .
.
.
.
.
, .
.
.
.
.
.
T .
.
.
.
.
.
.
i .
.
.
.
.~ I I  JOe .
~ i , i  , I ,i l l  i l J , I ,i l l  I 1 1  r I ,.
.
.
.
i I - ~ _ _ l _ _ +  .
.
.
.
# _ _ 4  .
.
.
.
i i  .
.
.
.
.
.
.
.
.
i .
.
.
.
.
.
~ .
.
.
.
.
.
.
i .
.
.
.
.
.i i l l  i i  i i  i ii l l l  i i  i i i ii i i i  i i  I i i i ,, .
.
.
.
.
.
.
.
.
.
,,' .
.
.
.
.
.
.
.
.
,: ,,' .
.
.
.
.
.
.
.
.
.
.
.
.
:, ,,' .
.
.
.
.
.I i i  i i  I I  ~ I II I I  I i  i i  t i IABCD E F G H I J K Lsentences on x-axisFigure 10Segment boundaries form a grid over the bitext space.
Each cell in the grid represents heproduct of two segments, one from each half of the bitext.
A point of correspondence insidecell (X,y) indicates that some token in segment X corresponds with some token in segment y;i.e., the segments X and y correspond.
So, for example, segment E corresponds withsegment d. The aligned blocks are outlined with solid lines.two texts such that the nth segment of one text is the translation of the nth segment ofthe other" (Simard, Foster, and Isabelle 1992, 68).
For example, given the token corre-spondences in Figure 10, the segment (G,H / should be aligned with the segment (e,f/.
Ifsegments (X1 , .
.
.
,  Xnl align with segments/Yl .
.
.
.
.
Yn)" then ( (X  1 .
.
.
.
.
Xnl" (Yl .
.
.
.
.
Yn/)is an aligned block.
In geometric terms, aligned blocks are rectangular regions of thebitext space, such that the sides of the rectangles coincide with segment boundaries,and such that no two rectangles overlap either vertically or horizontally.
The alignedblocks in Figure 10 are outlined with solid lines.SIMR's initial output has more expressive power than the alignment that can bederived from it.
One illustration of this difference is that segment correspondence canrepresent order inversions, but segment alignment cannot.
Inversions occur surpris-ingly often in real bitexts, even for sentence-size s gments (Church 1993).
Figure 10provides another illustration.
If, instead of the point in cell (H,e), there was a pointin cell (G,tf), the correct alignment for that region would still be ((G,H/, (e,f/).
If therewere points of correspondence in both (H,e) and (G,f), the correct alignment wouldstill be the same.
Yet, the three cases are clearly different.
If a lexicographer wanted tosee a word in segment G in its bilingual context, it would be useful to know whethersegment f is relevant.7.2 The Geometric Segment Alignment (GSA) AlgorithmGiven a sequence of segment boundaries for each half of a bitext, the Geometric Seg-ment Alignment (GSA) algorithm reduces ets of correspondence points to segmentalignments.
The algorithm's first step is to perform a transitive closure over the in-put correspondence relation.
For instance, if the input contains (G,e), (H,e), and (H,f),125Computational Linguistics Volume 25, Number 1then GSA adds the pairing (G,f).
Next, GSA forces all segments to be contiguous: Ifsegment Y corresponds with segments x and z, but not y, the pairing (Y,y) is added.In geometric terms, these two operations arrange all cells that contain points of corre-spondence into nonoverlapping rectangles, while adding as few cells as possible.
Theresult is an alignment relation.A complete set of TPCs, together with appropriate boundary information, guar-antees a perfect alignment.
Alas, the points of correspondence postulated by SIMR areneither complete nor noise-free.
SIMR makes errors of omission and errors of commis-sion.
Fortunately, the noise in SIMR's output causes alignment errors in predictableways.
GSA employs everal backing-off heuristics to reduce the number of errors.Typical errors of commission are stray points of correspondence like the one incell (H,e) in Figure 10.
This point indicates that /G,H/ and ~e,f / should form a 2x2aligned block, whereas the lengths of the component segments uggest hat a pair oflx  I blocks is more likely.
In a separate development bitext, I have found that SIMRis usually wrong in these cases.
To reduce such errors, GSA asks Gale & Church'slength-based alignment algorithm (Gale and Church 1991a; Michel Simard, personalcommunication) for a second opinion on any aligned block that is not lx  1.
When-ever the length-based algorithm prefers a more fine-grained alignment, its judgementoverrules SIMR's.Typical errors of omission are illustrated in Figure 10 by the complete absenceof correspondence points between segments /B,C,D/ and ~b,c/.
This empty block ofsegments i  sandwiched between aligned blocks.
It is highly likely that at least someof these segments are mutual translations, despite SIMR's failure to find any points ofcorrespondence b tween them.
Therefore, GSA treats all sandwiched empty blocks asaligned blocks.
If an empty block is not I x 1, GSA realigns it using Gale and Church'slength-based algorithm, just as it would realign any other many-to-many alignedblock.The most problematic ases involve an error of omission adjacent o an error ofcommission, as in blocks (//,/hi) and (/J,K/,/i/).
If the point in cell (J,i) should re-ally be in cell (J,h), then realignment inside the erroneous blocks would not solve theproblem.
A naive solution is to merge these blocks and then to realign them using alength-based method.
Unfortunately, this kind of alignment pattern, i.e., 0 x I followedby 2x 1, is surprisingly often correct.
Length-based methods assign low probabilitiesto such pattern sequences and usually get them wrong.
Therefore, GSA also con-siders the confidence level with which the length-based alignment algorithm reportsits realignment.
If this confidence level is sufficiently high, GSA accepts the length-based realignment; otherwise, the alignment indicated by SIMR's points of correspon-dence is retained.
The minimum confidence at which GSA trusts the length-based re-alignment is a GSA parameter, which has been optimized on a separate developmentbitext.8.
Evaluat ion of GSAGSA processed two bitext maps produced by SIMR using two different matchingpredicates.
The first matching predicate relied only on cognates that pass a certainLCSR threshold, as described in Section 4.2.
The second matching predicate was likethe first, except hat it also generated a point of correspondence whenever the inputtoken pair appeared as an entry in a translation lexicon.
The translation lexicon wasautomatically extracted from an MRBD (Cousin et al 1991).Bitexts involving millions of segments are becoming more and more common.Before comparing bitext alignment algorithms in terms of accuracy, it is important126Melamed Bitext Maps and AlignmentTable 6Comparison of bitext alignment algorithms' accuracy.
One error is counted for each alignedblock in the reference alignment that is missing from the test alignment.Errors, Given Errors, Not GivenBitext Algorithm Aligned Paragraphs % Aligned Paragraphs %"easy"Hansard(n = 7,123)Gale and Church (1991a) not available 128 1.8Simard, Foster, and Isabelle (1992) 114 1.6 171 2.4S1MR/GSA 104 1.5 115 1.6SIMR/GSA with MRBD 80 1.1 90 1.3"hard"Hansard(n = 2,693)Gale and Church (1991a) not available 80 3.0Simard, Foster, and Isabelle (1992) 50 1.9 102 3.8SIMR/GSA 50 1.9 61 2.3SIMR/GSA with MRBD 45 1.7 48 1.8to compare their asymptotic running times.
In order to run a quadratic-time align-ment algorithm in a reasonable amount of time on a large bitext, the bitext must bepresegmented into a set of smaller bitexts.
When a bitext contains no easily recogniz-able "anchors," such as paragraphs or sections, this first-pass alignment must be donemanually.Given a reasonably good bitext map, GSA's expected running time is linear in thenumber of input segment boundaries.
In all the bitexts on which GSA was trainedand tested, the points of correspondence in SIMR's output were sufficiently denseand accurate that GSA backed off to a quadratic-time alignment algorithm only forvery small aligned blocks.
For example, when the seed translation lexicon was used inSIMR's matching predicate, the largest aligned block that needed to be realigned was5x5 segments.
Without the seed translation lexicon, the largest realigned block was7x7 segments.
Thus, GSA can obviate the need to manually prealign large bitexts.Table 6 compares GSA's accuracy on the "easy" and "hard" French/English bi-texts with the accuracy of two other alignment algorithms, as reported by Simard,Foster, and Isabelle (1992).
The error metric counts one error for each aligned blockin the reference alignment that is missing from the test alignment.
To account for thepossibility of modularizing the overall alignment task into paragraph alignment fol-lowed by sentence alignment, Simard, Foster, and Isabelle (1992) have reported theaccuracy of their sentence alignment algorithm when a perfect alignment at the para-graph level is given.
SIMR/GSA was also tested in this manner, to enable the secondset of comparisons in Table 6.Due to the scarcity of hand-aligned training bitexts at my disposal, GSA's backing-off heuristics are somewhat ad hoc.
Even so, GSA performs at least as well as, andusually better than, other alignment algorithms for which comparable results havebeen published.
Chen (1996) has also published a quantitative evaluation of his align-ment algorithm on these reference bitexts, but his evaluation was done post hoc.
Sincethe results in this article are based on a gold standard, they are not comparable toChen's results.
Among other reasons, error rates based on a gold standard are some-times inflated by errors in the gold standard and this was indeed the case for the goldstandard used here (see Melamed \[1996a\]).
It is also an open question whether GSAperforms better than the algorithm proposed by Wu (1994).
The two algorithms havenot yet been evaluated on the same test data.
For now, I can offer only a theoreticalreason why SIMR+GSA should be more accurate than the algorithms of Chen andWu: Bitext maps lead to alignment more directly than a translation model (Chen 1996)127Computational Linguistics Volume 25, Number 1or a translation lexicon (Wu 1994), because both segment alignments and bitext mapsare relations between token instances, rather than between token types.More important han GSA's current accuracy is GSA's potential accuracy.
Witha bigger development bitext, more effective backing-off heuristics can be developed.Better input can also make a difference: GSA's accuracy will improve in lockstep withSIMR's accuracy.9.
ConclusionThe Smooth Injective Map Recognizer (SIMR) is based on innovative approaches toeach of the three main components of a bitext mapping algorithm: signal generation,noise tering, and search.
The advances in signal generation stemmed from the use ofword-based matching predicates.
When word-pair coordinates are plotted in a Carte-sian bitext space, the geometric heuristics of existing sentence alignment algorithmscan be exploited just as easily and to a greater extent at the word level.
The cognateheuristic of character-based bitext mapping algorithms also works better at the wordlevel, because cognateness can be defined more precisely in terms of words, e.g., usingthe Longest Common Subsequence Ratio.
Most importantly, matching heuristics basedon existing translation lexicons can be defined only at the word level.
When neithercognates nor sentence boundaries can be found, we can still map bitexts in any pair oflanguages using a small hand-constructed translation lexicon.
To complement word-based matching predicates, I have proposed localized noise filtering.
Localized noisefilters are more accurate than global ones because they are sensitive to local variationsin noise distributions.
The combination of a strong signal and an accurate noise filterenables localized search heuristics.
Localized search heuristics can directly exploit thegeometric tendencies of TPC chains in order to search the bitext space in linear spaceand time.
This level of efficiency is particularly important for large bitexts.SIMR also advances the state of the art of bitext mapping on several other crite-ria.
Evaluation on preexisting old standards has shown that SIMR can map bitextswith high accuracy in a variety of language pairs and text genres, without gettinglost.
SIMR is robust in the face of translation irregularities like omissions and allowscrossing correspondences to account for word-order differences.
SIMR encapsulatesits language-specific heuristics, so that it can be ported to any language pair with aminimal effort (Melamed 1997).
These features make SIMR one of the most widelyapplicable bitext mapping algorithms published to date.For applications that require it, SIMR's bitext maps can be efficiently reducedto segment alignments, using the Geometric Segment Alignment (GSA) algorithmpresented here.
Admittedly, GSA is only useful when a good bitext map is available.In such cases, there are three reasons to favor GSA over other options for alignment:One, it is simply more accurate.
Two, its expected running time is linear in the sizeof the bitext.
Therefore, three, it is not necessary to manually prealign large bitextsbefore input to GSA.There are numerous ways to improve on the methods presented here.
If SIMR canbe reparameterized so that its parameters are pairwise independent, hen it may bepossible to optimize these parameters analytically, or at least within a well-foundedprobabilistic framework.
Likewise, the parameters in GSA's backing-off heuristics andthe heuristics themselves were partially dictated by the scarcity of suitable trainingdata at the time that GSA was being developed.
All of this is to say that the detailsof the current implementations of SIMR and GSA are less important han the generalapproach to bitext mapping advocated here.128Melamed Bitext Maps and AlignmentAcknowledgmentsThis research began while I was a visitor atthe Centre d'Innovation en Technologies del'Information i  Laval, Canada.
I amindebted to Pierre Isabelle for informing methat the bitext mapping problem was farfrom being solved.
Thanks are due toeveryone at CITI for letting me use theirsoftware.
SIMR was ported toSpanish/English while I was visiting SunMicroSystems Laboratories.
Thanks to GaryAdams, Cookie Callahan, Bob Kuhns, andPhilip Resnik for their help with thatproject.
Thanks also to Philip Resnik forwriting the Spanish tokenizer, and for,hand-aligning the Spanish/English trainingbitexts.
Porting SIMR to Korean/Englishwould not have been possible withoutYoung-Suk Lee of MIT's LincolnLaboratories, who provided the seedtranslation lexicon, and aligned all thetraining and test bitexts.
This paper hasbenefited tremendously from the insightsand comments of Stan Chen, Ken Church,Mike Collins, Ido Dagan, Jason Eisner,George Foster, Pierre Isabelle, ElliottMacklovitch, Mitch Marcus, AdwaitRatnaparkhi, Michel Simard, EeroSimoncelli, Matthew Stone, Lyle Ungar,Bonnie Webber, and four anonymousreviewers.
The majority of this work wasdone at the Department of Computer andInformation Science of the University ofPennsylvania, where it was supported by anequipment grant from Sun MicroSystemsand partially funded by ARO grantDAAL03-89-C0031 PRIME and by ARPAgrants N00014-90-J-1863 andN66001-94C-6043.ReferencesBellman, Richard.
1957.
DynamicProgramming.
Princeton University Press,Princeton, NJ.Brown, Peter F., Stephen Della Pietra,Vincent Della Pietra, and Robert L.Mercer.
1993.
The mathematics ofstatistical machine translation: Parameterestimation.
Computational Linguistics, 9(2):263-311.Brown, Peter F., Jennifer C. Lai, and RobertL.
Mercer.
1991.
Aligning sentences inparallel corpora.
In Proceedings ofthe 29thAnnual Meeting, pages 169-176, Berkeley,CA.
Association for ComputationalLinguistics.Catizone, Roberta, Graham Russell, andSusan Warwick.
1993.
Derivingtranslation data from bilingual texts.
InProceedings ofthe First International LexicalAcquisition Workshop, Detroit, MI.Chen, Stanley.
1996.
Building ProbabilisticModels for Natural Language.
Ph.D.dissertation, Harvard University,Cambridge, MA.Church, Kenneth W. 1993.
Char_align: Aprogram for aligning parallel texts at thecharacter level.
In Proceedings ofthe 31stAnnual Meeting, pages 1-8, Columbus,OH.
Association for ComputationalLinguistics.Church, Kenneth W., Ido Dagan, WilliamGale, Pascale Fung, J. Helfman, and B.Satish.
1993.
Aligning parallel texts: Domethods developed for English-Frenchgeneralize to Asian languages?
InProceedings ofPacfoCol'93.
Taipei, Taiwan.Cousin, Pierre-Henri, Lorna Sinclair,Jean-Francois Allain, and Catherine E.Love.
1991.
The Collins Paperback FrenchDictionary.
Harper Collins Publishers,Glasgow, Scotland.Dagan, Ido, Kenneth W. Church, andWilliam Gale.
1993.
Robust wordalignment for machine aided translation.In Proceedings ofthe Workshop on Very LargeCorpora: Academic and IndustrialPerspectives, pages 1-8, Columbus, OH.Debili, Fathi and Ely@s Sammouda.
1992.Appariement des phrases de textesbilingues.
In Proceedings ofthe 14thInternational Conference on ComputationalLinguistics, pages 517-538, Nantes, France.Dempster, A. P., N. M. Laird, and D. B.Rubin.
1977.
Maximum likelihood fromincomplete data via the EM algorithm.Journal of the Royal Statistical Society, 34(B):1-38.Fung, Pascale.
1995.
A pattern matchingmethod for finding noun and propernoun translations from noisy parallelcorpora.
In Proceedings ofthe 33rd AnnualMeeting, pages 236-243, Boston, MA.Association for ComputationalLinguistics.Fung, Pascale and Kenneth W. Church.1994.
K-vec: A new approach for aligningparallel texts.
In Proceedings ofthe 15thInternational Conference on ComputationalLinguistics, pages 1,096-1,102, Kyoto,Japan.Fung, Pascale and Kathleen McKeown.1994.
Aligning noisy parallel corporaacross language groups: Word pairfeature matching by dynamic timewarping.
In Proceedings ofthe Conference ofthe Association for Machine Translation in the129Computational Linguistics Volume 25, Number 1Americas, pages 81-88, Columbia, MD.Gale, William and Kenneth W. Church.1991a.
A program for aligning sentencesin bilingual corpora.
In Proceedings ofthe29th Annual Meeting, pages 177-184,Berkeley, CA.
Association forComputational Linguistics.Gale, William and Kenneth W. Church.1991b.
Identifying word correspondencesin parallel texts.
In Proceedings oftheDARPA SNL Workshop, pages 152-157.Harris, Brian.
1988.
Bi-text, a new concept intranslation theory.
Language Monthly, 54:8-10.J.
W. Hunt and T. G. Szymanski.
1977.
Afast algorithm for computing longestcommon subsequences.
Communications ofthe ACM, 20(5): 350-353.Isabelle, Pierre.
1992.
Bi-textual aids fortranslators.
In Proceedings ofthe 8th AnnualConference ofthe UW Centre for the NewOED and Text Research, pages 1-15,Waterloo, Canada.Kay, Martin and Martin R6scheisen.
1993.Text-translation alignment.
ComputationalLinguistics, 19(1): 121-142.Knight, Kevin and Jonathan Graehi.
1997.Machine transliteration.
In Proceedings ofthe 35th Annual Meeting, pages 128-135,Madrid, Spain.
Association forComputational Linguistics.Macklovitch, Elliott.
1996.
Peut-on v4rifierautomatiquement la coh4renceterminologique?
META 41(3).
.McEnery, Tony and Michael Oakes.
1995.Cognate xtraction in the CRATERproject: Methods and assessment.
InProceedings ofFrom Texts to Tags: Issues inMultilingual Language Analysis,pages 77-86, Dublin, Ireland.Melamed, I. Dan.
1996a.
Automaticdetection of omissions in translations.
InProceedings ofthe 16th InternationalConference on Computational Linguistics,pages 764-769, Copenhagen, Denmark.Melamed, I. Dan.
1996b.
Automaticconstruction of clean broad-coveragetranslation lexicons.
In Proceedings ofthe2nd Conference ofthe Association for MachineTranslation in the Americas, pages 125-134,Montreal, Canada.Melamed, I. Dan.
1996c.
Porting SIMR tonew language pairs.
Institute for Researchin Cognitive Science Technical Report96-26, University of Pennsylvania,Philadelphia, PA.Melamed, I. Dan.
1997.
A portablealgorithm for mapping bitextcorrespondence.
In Proceedings ofthe 35thAnnual Meeting, pages 305-312, Madrid,Spain.
Association for ComputationalLinguistics.Melamed, I. Dan.
1998a.
Models ofco-occurrence.
Institute for Research inCognitive Science Technical Report 98-05,University of Pennsylvania, Philadelphia,PA.Melamed, I. Dan.
1998b.
Empirical Methodsfor Exploiting Parallel Texts.
Ph.D.dissertation, University of Pennsylvania,Philadelphia, PA.Nerbonne, John, Lauri Karttunen, ElenaPaskaleva, Gabor Proszeky, and TiitRoosmaa.
1997.
Reading more intoforeign languages.
In Proceedings ofthe 5thACL Conference on Applied Natural LanguageProcessing, pages 135-138, Washington,DC.Papageorgiou, Harris, Lambros Cranias, andStelios Piperidis.
1994.
Automaticalignment in parallel corpora.
InProceedings ofthe 32nd Annual Meeting(Student Session), pages 334-336, LasCruces, NM.
Association forComputational Linguistics.Resnik, Philip and I. Dan Melamed.
1997.Semi-automatic a quisition ofdomain-specific translation lexicons.
InProceedings ofthe 5th ACL Conference onApplied Natural Language Processing,pages 340-347, Washington, DC.Simard, Michel, George F. Foster, and PierreIsabelle.
1992.
Using cognates to alignsentences in bilingual corpora.
InProceedings ofthe Fourth InternationalConference on Theoretical nd MethodologicalIssues in Machine Translation, pages 67-81.Montreal, Canada.Simard, Michel and Pierre Plamondon.
1996.Bilingual sentence alignment: Balancingrobustness and accuracy.
In Proceedings ofthe 2nd Conference ofthe Association forMachine Translation in the Americas,pages 135-144, Montreal, Canada.Ren4 V. V. Vidal, editor.
1993.
AppliedSimulated Annealing.
Springer-Verlag,Heidelberg, Germany.Wu, Dekai.
1994.
Aligning a parallelEnglish-Chinese corpus statistically withlexical criteria.
In Proceedings ofthe 32ndAnnual Meeting, pages 80-87, Las Cruces,NM.
Association for ComputationalLinguistics.130
