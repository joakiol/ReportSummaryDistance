Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1025?1032,Sydney, July 2006. c?2006 Association for Computational LinguisticsExploring Distributional Similarity Based Modelsfor Query Spelling CorrectionMu LiMicrosoft Research Asia5F Sigma CenterZhichun Road, Haidian DistrictBeijing, China, 100080muli@microsoft.comMuhua ZhuSchool ofInformation Science and EngineeringNortheastern UniversityShenyang, Liaoning, China, 110004zhumh@ics.neu.edu.cnYang ZhangSchool ofComputer Science and TechnologyTianjin UniversityTianjin, China, 300072yangzhang@tju.edu.cnMing ZhouMicrosoft Research Asia5F Sigma CenterZhichun Road, Haidian DistrictBeijing, China, 100080mingzhou@microsoft.comAbstractA query speller is crucial to search en-gine in improving web search relevance.This paper describes novel methods foruse of distributional similarity estimatedfrom query logs in learning improvedquery spelling correction models.
Thekey to our methods is the property of dis-tributional similarity between two terms:it is high between a frequently occurringmisspelling and its correction, and lowbetween two irrelevant terms only withsimilar spellings.
We present two modelsthat are able to take advantage of thisproperty.
Experimental results demon-strate that the distributional similaritybased models can significantly outper-form their baseline systems in the webquery spelling correction task.1 IntroductionInvestigations into query log data reveal thatmore than 10% of queries sent to search enginescontain misspelled terms (Cucerzan and Brill,2004).
Such statistics indicate that a good queryspeller is crucial to search engine in improvingweb search relevance, because there is little op-portunity that a search engine can retrieve manyrelevant contents with misspelled terms.The problem of designing a spelling correctionprogram for web search queries, however, posesspecial technical challenges and cannot be wellsolved by general purpose spelling correctionmethods.
Cucerzan and Brill (2004) discussed indetail specialties and difficulties of a query spellchecker, and illustrated why the existing methodscould not work for query spelling correction.They also identified that no single evidence, ei-ther a conventional spelling lexicon or term fre-quency in the query logs, can serve as criteria forvalidate queries.To address these challenges, we concentrateon the problem of learning improved query spell-ing correction model by integrating distributionalsimilarity information automatically derivedfrom query logs.
The key contribution of ourwork is identifying that we can successfully usethe evidence of distributional similarity toachieve better spelling correction accuracy.
Wepresent two methods that are able to take advan-tage of distributional similarity information.
Thefirst method extends a string edit-based errormodel with confusion probabilities within a gen-erative source channel model.
The secondmethod explores the effectiveness of our ap-proach within a discriminative maximum entropymodel framework by integrating distributionalsimilarity-based features.
Experimental resultsdemonstrate that both methods can significantlyoutperform their baseline systems in the spellingcorrection task for web search queries.1025The rest of the paper is structured as follows:after a brief overview of the related work in Sec-tion 2, we discuss the motivations for our ap-proach, and describe two methods that can makeuse of distributional similarity information inSection 3.
Experiments and results are presentedin Section 4.
The last section contains summariesand outlines promising future work.2 Related WorkThe method for web query spelling correctionproposed by Cucerzan and Brill (2004) isessentially based on a source channel model, butit requires iterative running to derive suggestionsfor very-difficult-to-correct spelling errors.
Wordbigram model trained from search query logs isused as the source model, and the error model isapproximated by inverse weighted edit distanceof a correction candidate from its original term.The weights of edit operations are interactivelyoptimized based on statistics from the query logs.They observed that an edit distance-based errormodel only has less impact on the overallaccuracy than the source model.
The paperreports that un-weighted edit distance will causethe overall accuracy of their speller?s output todrop by around 2%.
The work of Ahmad andKondrak (2005) tried to employ an unsupervisedapproach to error model estimation.
Theydesigned an EM (Expectation Maximization)algorithm to optimize the probabilities of editoperations over a set of search queries from thequery logs, by exploiting the fact that there aremore than 10% misspelled queries scatteredthroughout the query logs.
Their method isconcerned with single character edit operations,and evaluation was performed on an isolatedword spelling correction task.There are two lines of research in conventionalspelling correction, which deal with non-worderrors and real-word errors respectively.
Non-word error spelling correction is concerned withthe task of generating and ranking a list of possi-ble spelling corrections for each query word notfound in a lexicon.
While traditionally candidateranking is based on manually tuned scores suchas assigning weights to different edit operationsor leveraging candidate frequencies, some statis-tical models have been proposed for this rankingtask in recent years.
Brill and Moore (2000) pre-sented an improved error model over the oneproposed by Kernigham et al (1990) by allowinggeneric string-to-string edit operations, whichhelps with modeling major cognitive errors suchas the confusion between le and al.
Toutanovaand Moore (2002) further explored this via ex-plicit modeling of phonetic information of Eng-lish words.
Both these two methods require mis-spelled/correct word pairs for training, and thelatter also needs a pronunciation lexicon.
Real-word spelling correction is also referred to ascontext sensitive spelling correction, which triesto detect incorrect usage of valid words in certaincontexts (Golding and Roth, 1996; Mangu andBrill, 1997).Distributional similarity between words hasbeen investigated and successfully applied inmany natural language tasks such as automaticsemantic knowledge acquisition (Dekang Lin,1998) and language model smoothing (Essen andSteinbiss, 1992; Dagan et al, 1997).
An investi-gation on distributional similarity functions canbe found in (Lillian Lee, 1999).3 Distributional Similarity-Based Mod-els for Query Spelling Correction3.1 MotivationMost of the previous work on spelling correctionconcentrates on the problem of designing bettererror models based on properties of characterstrings.
This direction ever evolves from simpleDamerau-Levenshtein distance (Damerau, 1964;Levenshtein, 1966) to probabilistic models thatestimate string edit probabilities from corpus(Church and Gale, 1991; Mayes et al 1991; Ris-tad and Yianilos, 1997; Brill and Moore, 2000;and Ahmad and Kondrak, 2005).
In the men-tioned methods, however, the similarities be-tween two strings are modeled on the average ofmany misspelling-correction pairs, which maycause many idiosyncratic spelling errors to beignored.
Some of those are typical word-levelcognitive errors.
For instance, given the queryterm adventura, a character string-based errormodel usually assigns similar similarities to itstwo most probable corrections adventure andaventura.
Taking into account that adventure hasa much higher frequency of occurring, it is mostlikely that adventure would be generated as asuggestion.
However, our observation into thequery logs reveals that adventura in most cases isactually a common misspelling of aventura.
Twoannotators were asked to judge 36 randomlysampled queries that contain more than one term,and they agreed upon that 35 of them should beaventura.To solve this problem, we consider alternativemethods to make use of the information beyond a1026term?s character strings.
Distributional similarityprovides such a dimension to view the possibilitythat one word can be replaced by another basedon the statistics of words co-occuring with them.Distributional similarity has been proposed toperform tasks such as language model smoothingand word clustering, but to the best of ourknowledge, it has not been explored in estimat-ing similarities between misspellings and theircorrections.
In this section, we will only involvethe consine metric for illustration purpose.Query logs can serve as an excellent corpusfor distributional similarity estimation.
This isbecause query logs are not only an up-to-dateterm base, but also a comprehensive spelling er-ror repository (Cucerzan and Brill, 2004; Ahmadand Kondrak, 2005).
Given enough size of querylogs, some misspellings, such as adventura, willoccur so frequently that we can obtain reliablestatistics of their typical usage.
Essential to ourmethod is the observation of high distributionalsimilarity between frequently occurring spellingerrors and their corrections, but low between ir-relevant terms.
For example, we observe thatadventura occurred more than 3,300 times in aset of logged queries that spanned three months,and its context was similar to that of aventura.Both of them usually appeared after words likepeurto and lyrics, and were followed by mall,palace and resort.
Further computation showsthat, in the tf (term frequency) vector space basedon surrounding words, the cosine value betweenthem is approximately 0.8, which indicates thesetwo terms are used in a very similar way amongall the users trying to search aventura.
The co-sine between adventura and adventure is lessthan 0.03 and basically we can conclude thatthey are two irrelevant terms, although theirspellings are similar.Distributional similarity is also helpful to ad-dress another challenge for query spelling correc-tion: differentiating valid OOV terms from fre-quently occurring misspellings.InLex  Freq Cosinevaccum No 18,430vacuum Yes 158,428 0.99seraphin No 1,718seraphim Yes 14,407 0.30Table 1.
Statistics of two word pairswith similar spellingsTable 1 lists detailed statistics of two wordpairs, each of pair of words have similar spelling,lexicon and frequency properties.
But the distri-butional similarity between each pair of wordsprovides the necessary information to make cor-rection classification that vacuum is a spellingerror while seraphin is a valid OOV term.3.2 Problem FormulationIn this work, we view the query spelling correc-tion task as a statistical sequence inference prob-lem.
Under the probabilistic model framework, itcan be conceptually formulated as follows.Given a correction candidate set C for a querystring q:}),(|{ ?<= cqEditDistcCin which each correction candidate c satisfies theconstraint that the edit distance between c and qis less than a given threshold ?, the model is tofind c* in C with the highest probability:)|(maxarg* qcPcCc?=  (1)In practice, the correction candidate set C isnot generated from the entire query string di-rectly.
Correction candidates are generated foreach term of a query first, and then C is con-structed by composing the candidates of individ-ual terms.
The edit distance threshold ?
is set foreach term proportionally to the length of the term.3.3 Source Channel ModelSource channel model has been widely used forspelling correction (Kernigham et al, 1990;Mayes, Damerau et al, 1991; Brill and More,2000; Ahmad and Kondrak, 2005).
Instead ofdirectly optimize (1), source channel model triesto solve an equivalent problem by applyingBayes?s rule and dropping the constant denomi-nator:)()|(maxarg* cPcqPcCc?=  (2)In this approach, two component generativemodels are involved: source model P(c) that gen-erates the user?s intended query c and errormodel P(q|c) that generates the real query qgiven c. These two component models can beindependently estimated.In practice, for a multi-term query, the sourcemodel can be approximated with an n-gram sta-tistical language model, which is estimated withtokenized query logs.
Taking bigram model forexample, c is a correction candidate containing nterms, ncccc ?21= , then P(c) can be written asthe product of consecutive bigram probabilities:?
?= )|()( 1ii ccPcP1027Similarly, the error model probability of aquery is decomposed into generation probabili-ties of individual terms which are assumed to beindependently generated:?= )|()|( ii cqPcqPPrevious proposed methods for error modelestimation are all based on the similarity betweenthe character strings of qi and ci as described in3.1.
Here we describe a distributional similarity-based method for this problem.
Essentially thereare different ways to estimate distributional simi-larity between two words (Dagan et al, 1997),and the one we propose to use is confusion prob-ability (Essen and Steinbiss, 1992).
Formally,confusion probability cP  estimates the possibil-ity that one word w1 can be replaced by anotherword w2:?=wc wPwwPwPwwPwwP )()|()()|()|( 22112  (3)where w belongs to the set of words that co-occur with both w1 and w2.From the spelling correction point of view,given w1 to be a valid word and w2 one of itsspelling errors, )|( 12 wwPc  actually estimatesopportunity that w1 is misspelled as w2 in querylogs.
Compared to other similarity measures suchas cosine or Euclidean distance, confusion prob-ability is of interest because it defines a probabil-istic distribution rather than a generic measure.This property makes it more theoretically soundto be used as error model probability in theBayesian framework of the source channel model.Thus it can be applied and evaluated independ-ently.
However, before using confusion probabil-ity as our error model, we have to solve twoproblems: probability renormalization andsmoothing.Unlike string edit-based error models, whichdistribute a major portion of probability overterms with similar spellings, confusion probabil-ity distributes probability over the entire vocabu-lary in the training data.
This property may causethe problem of unfair comparison between dif-ferent correction candidates if we directly use (3)as the error model probability.
This is becausethe synonyms of different candidates may sharedifferent portion of confusion probabilities.
Thisproblem can be solved by re-normalizing theprobabilities only over a term?s possible correc-tion candidates and itself.
To obtain better esti-mation, here we also require that the frequencyof a correction candidate should be higher thanthat of the query term, based on the observationthat correct spellings generally occur more oftenin query logs.
Formally, given a word w and itscorrection candidate set C, the confusion prob-ability of a word w?
conditioned on w can beredefined as????????????=?
?
?CwCwwcPwwPwwPCc ccc0)|()|()|(  (4)where )|( wwPc ??
is the original definition of con-fusion probability.In addition, we might also have the zero-probability problem when the query term has notappeared or there are few context words for it inthe query logs.
In such cases there is no distribu-tional similarity information available to anyknown terms.
To solve this problem, we definethe final error model probability as the linearcombination of confusion probability and a stringedit-based error model probability )|( cqPed :)|()1()|()|( cqPcqPcqP edc ??
?+=  (5)where ?
is the interpolation parameter between 0and 1 that can be experimentally optimized on adevelopment data set.3.4 Maximum Entropy ModelTheoretically we are more interested in buildinga unified probabilistic spelling correction modelthat is able to leverage all available features,which could include (but not limited to) tradi-tional character string-based typographical simi-larity, phonetic similarity and distributional simi-larity proposed in this work.
The maximum en-tropy model (Berger et al, 1996) provides uswith a well-founded framework for this purpose,which has been extensively used in natural language processing tasks ranging from part-of-speech tagging to machine translation.For our task, the maximum entropy modeldefines a posterior probabilistic distribution)|( qcP  over a set of feature functions fi (q, c)defined on an input query q and its correctioncandidate c:?
?
?===cNi iiNi iiqcfqcfqcP11),(exp),(exp)|(??
(6)1028where ?s are feature weights, which can be opti-mized by maximizing the posterior probabilityon the training set:?
?=TDqtqtP),()|(logmaxarg* ??
?where TD denotes the set of training samples inthe form of query-truth pairs presented to thetraining algorithm.We use the Generalized Iterative Scaling (GIS)algorithm (Darroch and Ratcliff, 1972) to learnthe model parameter ?s of the maximum entropymodel.
GIS training requires normalization overall possible prediction classes as shown in thedenominator in equation (6).
Since the potentialnumber of correction candidates may be huge formulti-term queries, it would not be practical toperform the normalization over the entire searchspace.
Instead, we use a method to approximatethe sum over the n-best list (a list of most prob-able correction candidates).
This is similar towhat Och and Ney (2002) used for their maxi-mum entropy-based statistical machine transla-tion training.3.4.1 FeaturesFeatures used in our maximum entropy modelare classified into two categories I) baseline fea-tures and II) features supported by distributionalsimilarity evidence.
Below we list the featuretemplates.Category I:1.
Language model probability feature.
Thisis the only real-valued feature with feature valueset to the logarithm of source model probability:)(log),( cPcqf prob =2.
Edit distance-based features, which aregenerated by checking whether the weightedLevenshtein edit distance between a query termand its correction is in certain range;All the following features, including this one,are binary features, and have the feature functionof the following form:??
?=otherwisesatisfiedconstraintcqfn 01),(in which the feature value is set to 1 when theconstraints described in the template are satisfied;otherwise the feature value is set to 0.3.
Frequency-based features, which are gen-erated by checking whether the frequencies of aquery term and its correction candidate are abovecertain thresholds;4.
Lexicon-based features, which are gener-ated by checking whether a query term and itscorrection candidate are in a conventional spell-ing lexicon;5.
Phonetic similarity-based features, whichare generated by checking whether the edit dis-tance between the metaphones (Philips, 1990) ofa query term and its correction candidate is be-low certain thresholds.Category II:6.
Distributional similarity based term fea-tures, which are generated by checking whether aquery term?s frequency is higher than certainthresholds but there are no candidates for it withhigher frequency and high enough distributionalsimilarity.
This is usually an indicator that thequery term is valid and not covered by the spell-ing lexicon.
The frequency thresholds are enu-merated from 10,000 to 50,000 with the interval5,000.7.
Distributional similarity based correctioncandidate features, which are generated bychecking whether a correction candidate?s fre-quency is higher than the query term or the cor-rection candidate is in the lexicon, and at thesame time the distributional similarity is higherthan certain thresholds.
This generally gives theevidence that the query term may be a commonmisspelling of the current candidate.
The distri-butional similarity thresholds are enumeratedfrom 0.6 to 1 with the interval 0.1.4 Experimental Results4.1 DatasetWe randomly sampled 7,000 queries from dailyquery logs of MSN Search and they were manu-ally labeled by two annotators.
For each queryidentified to contain spelling errors, correctionswere given by the annotators independently.From the annotation results that both annotatorsagreed upon 3,061 queries were extracted, whichwere further divided into a test set containing1,031 queries and a training set containing 2,030queries.
In the test set there are 171 queries iden-tified containing spelling errors with an error rateof 16.6%.
The numbers on the training set is 312and 15.3%, respectively.
The average length ofqueries on training set is 2.8 terms and on test setit is 2.6.1029In our experiments, a term bigram model isused as the source model.
The bigram model istrained with query log data of MSN Search dur-ing the period from October 2004 to June 2005.Correction candidates are generated from a termbase extracted from the same set of query logs.For each of the experiments, the performanceis evaluated by the following metrics:Accuracy: The number of correct outputs gen-erated by the system divided by the total numberof queries in the test set;Recall: The number of correct suggestions formisspelled queries generated by the system di-vided by the total number of misspelled queriesin the test set;Precision: The number of correct suggestionsfor misspelled queries generated by the systemdivided by the total number of suggestions madeby the system.4.2 ResultsWe first investigated the impact of the interpola-tion parameter ?
in equation (5) by applying theconfusion probability-based error model on train-ing set.
For the string edit-based error modelprobability )|( cqPed , we used a heuristic scorecomputed as the inverse of weighted edit dis-tance, which is similar to the one used by Cucer-zan and Brill (2004).Figure 1 shows the accuracy metric at differ-ent settings of ?.
The accuracy generally gainsimprovements before ?
reaches 0.9.
This showsthat confusion probability plays a more importantrole in the combination.
As a result, we empiri-cally set ?= 0.9 in the following experiments.88%89%89%90%90%91%91%0.05 0.15 0.25 0.35 0.45 0.55 0.65 0.75 0.85 0.95lambdaaccuracyFigure 1.
Accuracy with different ?sTo evaluate whether the distributional similar-ity can contribute to performance improvements,we conducted the following experiments.
Forsource channel model, we compared the confu-sion probability-based error model (SC-SimCM)against two baseline error model settings, whichare source model only (SC-NoCM) and the heu-ristic string edit-based error model (SC-EdCM)we just described.
Two maximum entropy mod-els were trained with different feature sets.
ME-NoSim is the model trained only with baselinefeatures.
It serves as the baseline for ME-Full,which is trained with all the features described in3.4.1.
In training ME-Full, cosine distance isused as the similarity measure examined by fea-ture functions.In all the experiments we used the standardviterbi algorithm to search for the best output ofsource channel model.
The n-best list for maxi-mum entropy model training and testing is gen-erated based on language model scores of cor-rection candidates, which can be easily obtainedby running the forward-viterbi backward-A* al-gorithm.
On a 3.0GHZ Pentium4 personal com-puter, the system can process 110 queries persecond for source channel model and 86 queriesper second for maximum entropy model, inwhich 20 best correction candidates are used.Model Accuracy Recall PrecisionSC-NoCM 79.7% 63.3% 40.2%SC-EdCM 84.1% 62.7% 47.4%SC-SimCM 88.2% 57.4% 58.8%ME-NoSim 87.8% 52.0% 60.0%ME-Full 89.0% 60.4% 62.6%Table 2.
Performance results for different modelsTable 2 details the performance scores for theexperiments, which shows that both of the twodistributional similarity-based models boost ac-curacy over their baseline settings.
SC-SimCMachieves 26.3% reduction in error rate over SC-EdCM, which is significant to the 0.001 level(paired t-test).
ME-Full outperforms ME-NoSimin all three evaluation measures, with 9.8% re-duction in error rate and 16.2% improvement inrecall, which is significant to the 0.01 level.It is interesting to note that the accuracy ofSC-SimCM is slightly better than ME-NoSim,although ME-NoSim makes use of a rich set offeatures.
ME-NoSim tends to keep queries withfrequently misspelled terms unchanged (e.g.
caf-fine extractions from soda) to reduce false alarms(e.g.
bicycle suggested for biocycle).We also investigated the performance of themodels discussed above at different recall.
Fig-ure 2 and Figure 3 show the precision-recallcurves and accuracy-recall curves of differentmodels.
We observed that the performance ofSC-SimCM and ME-NoSim are very close toeach other and ME-Full consistently yields betterperformance over the entire P-R curve.103040%45%50%55%60%65%70%75%80%85%35% 40% 45% 50% 55% 60%recallprecisionME-FullME-NoSimSC-EdCMSC-SimCMSC-NoCMFigure 2.
Precision-recall curve of different models82%83%84%85%86%87%88%89%90%91%35% 40% 45% 50% 55% 60%recallaccuracyME-FullME-NoSimSC-EdCMSC-SimCMSC-NoCMFigure 3.
Accuracy-recall curve of different modelsWe performed a study on the impact of train-ing size to ensure all models are trained withenough data.40%50%60%70%80%90%200 400 600 800 1000 1600 2000ME-Full RecallME-Full AccuracyME-NoSim RecallME-NoSim AccuracyFigure 4.
Accuracy of maximum entropy modelstrained with different number of samplesFigure 4 shows the accuracy of the two maxi-mum entropy models as functions of number oftraining samples.
From the results we can seethat after the number of training samples reaches600 there are only subtle changes in accuracyand recall.
Therefore basically it can be con-cluded that 2,000 samples are sufficient to train amaximum entropy model with the current featuresets.5 Conclusions and Future WorkWe have presented novel methods to learn betterstatistical models for the query spelling correc-tion task by exploiting distributional similarityinformation.
We explained the motivation of ourmethods with the statistical evidence distilledfrom query log data.
To evaluate our proposedmethods, two probabilistic models that can takeadvantage of such information are investigated.Experimental results show that both methods canachieve significant improvements over theirbaseline settings.A subject of future research is exploring moreeffective ways to utilize distributional similarityeven beyond query logs.
Currently for low-frequency terms in query logs there are no reli-able distribution similarity evidence available forthem.
A promising method of dealing with this innext steps is to explore information in the result-ing page of a search engine, since the snippets inthe resulting page can provide far greater de-tailed information about terms in a query.ReferencesFarooq Ahmad and Grzegorz Kondrak.
2005.
Learn-ing a spelling error model from search query logs.Proceedings of EMNLP 2005, pages 955-962.Adam L. Beger, Stephen A. Della Pietra, and VincentJ.
Della Pietra.
1996.
A maximum entropy ap-proach to natural language processing.
Computa-tion Linguistics, 22(1):39-72.Eric Brill and Robert C. Moore.
2000.
An improvederror model for noisy channel spelling correction.Proceedings of 38th annual meeting of the ACL,pages 286-293.Kenneth W. Church and William A. Gale.
1991.Probability scoring for spelling correction.
In Sta-tistics and Computing, volume 1, pages 93-103.Silviu Cucerzan and Eric Brill.
2004.
Spelling correc-tion as an iterative process that exploits the collec-tive knowledge of web users.
Proceedings ofEMNLP?04, pages 293-300.Ido Dagan, Lillian Lee and Fernando Pereira.
1997.Similarity-Based Methods for Word Sense Disam-biguation.
Proceedings of the 35th annual meetingof ACL, pages 56-63.Fred Damerau.
1964.
A technique for computer detec-tion and correction of spelling errors.
Communica-tion of the ACM 7(3):659-664.J.
N. Darroch and D. Ratcliff.
1972.
Generalized itera-tive scaling for long-linear models.
Annals of Ma-thematical Statistics, 43:1470-1480.Ute Essen and Volker Steinbiss.
1992.
Co-occurrencesmoothing for stochastic language modeling.
Pro-ceedings of ICASSP, volume 1, pages 161-164.Andrew R. Golding and Dan Roth.
1996.
Applyingwinnow to context-sensitive spelling correction.Proceedings of ICML 1996, pages 182-190.Mark D. Kernighan, Kenneth W. Church and WilliamA.
Gale.
1990.
A spelling correction program1031based on a noisy channel model.
Proceedings ofCOLING 1990, pages 205-210.Karen Kukich.
1992.
Techniques for automaticallycorrecting words in text.
ACM Computing Surveys.24(4): 377-439Lillian Lee.
1999.
Measures of distributional similar-ity.
Proceedings of the 37th annual meeting of ACL,pages 25-32.V.
Levenshtein.
1966.
Binary codes capable of cor-recting deletions, insertions and reversals.
SovietPhysice ?
Doklady 10: 707-710.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
Proceedings of COLING-ACL1998, pages 768-774.Lidia Mangu and Eric Brill.
1997.
Automatic ruleacquisition for spelling correction.
Proceedings ofICML 1997, pages 734-741.Eric Mayes, Fred Damerau and Robert Mercer.
1991.Context based spelling correction.
Informationprocessing and management 27(5): 517-522.Franz Och and Hermann Ney.
2002.
Discriminativetraining and maimum entropy models for statisticalmachine translation.
Proceedings of the 40th an-nual meeting of ACL, pages 295-302.Lawrence Philips.
1990.
Hanging on the metaphone.Computer Language Magazine, 7(12): 39.Eric S. Ristad and Peter N. Yianilos.
1997.
Learningstring edit distance.
Proceedings of ICML 1997.pages 287-295Kristina Toutanova and Robert Moore.
2002.
Pronun-ciation modeling for improved spelling correction.Proceedings of the 40th annual meeting of ACL,pages 144-151.1032
