Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 65?68,Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLPFast decoding and Easy Implementation:Transliteration as Sequential LabelingEiji ARAMAKIThe University of Tokyoeiji.aramaki@gmail.comTakeshi ABEKAWWANational Institute of Informaticsabekawa@nii.ac.jpAbstractAlthough most of previous translitera-tion methods are based on a generativemodel, this paper presents a discrimi-native transliteration model using condi-tional random fields.
We regard charac-ter(s) as a kind of label, which enablesus to consider a transliteration process asa sequential labeling process.
This ap-proach has two advantages: (1) fast decod-ing and (2) easy implementation.
Experi-mental results yielded competitive perfor-mance, demonstrating the feasibility of theproposed approach.1 IntroductionTo date, most transliteration methods have reliedon a generative model which resembles a statisti-cal machine translation (SMT) model.
Althoughthe generative approach has appealing feasibility,it usually suffers from parameter settings, lengthbiases and decoding time.We assume a transliteration process as a kindof sequential labeling that is widely employed forvarious tasks, such as Named Entity Recognition(NER), part-of-speech (POS) labeling, and so on.Figure 1 shows a lattice of both the transliterationand POS labeling.
As shown in that figure, bothtasks share a similar work frame: (1) an input se-quence is decomposed into several segments; then(2) each segments produces a label.
Although thelabel represents a POS in POS labeling, it repre-sents a character (or a character sequence) in thetransliteration task.The proposed approach entails three risks.1.
Numerous Label Variation: Although POSrequires only 10?20 labels at most, a translit-eration process requires numerous labels.
Infact, Japanese katakana requires more than260 labels in the following experiment (weFigure 1: (i) Part-of-Speech Lattice and (ii)Transliteration Lattice.consider combinations of characters as a la-bel).
Such a huge label set might require ex-tremely heavy calculation.2.
No Gold Standard Data: We build the goldstandard label from character alignment us-ing GIZA++ 1.
Of course, such gold standarddata contain alignment errors, which mightdecrease labeling performance.3.
No Language Model: The proposed ap-proach cannot incorporate the target languagemodel.In spite of the disadvantages listed above, theproposed method offers two strong advantages.1.
Fast Decoding: Decoding (more pre-cisely labeling) is extremely fast (0.12?0.58s/input).
Such rapid decoding is useful forvarious applications, for example, a query ex-pansion for a search engine and so on 2.1http://www.fjoch.com/GIZA++.html2A fast transliteration demonstration is available at theweb site; http://akebia.hcc.h.u-tokyo.ac.jp/NEWS/65Figure 2: Conversion from Training set to GoldStandard Labels2.
Easy Implementation: Because sequentiallabeling is a traditional research topic, vari-ous algorithms and tools are available.
Usingthem, we can easily realize various transliter-ation systems in any language pairs.The experimental results empirically demon-strate that the proposed method is competitivein several language directions (e.g.
English?Chinese).2 MethodWe developed a two-stage labeling system.
First,an input term is decomposed into several segments(STEP1).
Next, each segmentation produces sym-bol(s) (STEP2).2.1 STEP1: ChunkingFor a given noun phrase, consisting n characters,the system gave a label (L1...Ln) that representssegmentations.The segmentation is expressed as two types oflabels (label B and I), where B signifies a begin-ning of the segmentation, and I signifies the endof segmentation.
This representation is similar tothe IOB representation, which is used in NamedEntity Recognition (NER) or chunking.For label prediction, we used Conditional Ran-dom Fields (CRFs), which is a state-of-the-art la-beling algorithm.
We regard a source character it-self as a CRF feature.
The window size is three(the current character and previous/next charac-ter).2.2 STEP2: Symbol productionNext, the system estimates labels (T1...Tm) foreach segmentation, where m is the number of seg-Table 1: Corpora and SizesNotation Language Train TestEN-CH English?Chinese 31,961 2,896EN-JA English?Japanese 27,993 1,489EN-KO English?Korean 4,840 989EN-HI English?Hindi 10,014 1,000EN-TA English?Tamil 8,037 1,000EN-KA English?Kannada 8,065 1,000EN-RU English?Russian 5,977 1,000* EN-CH is provided by (Li et al, 2004); EN-TA, EN-KA, EN-HI and EN-RU are from (Kumaranand Kellner, 2007); EN-JA and EN-KO are fromhttp://www.cjk.org/.mentations (the number of B labels in STEP1).The label of this step directly represents a targetlanguage character(s).
The method of building agold standard label is described in the next sub-section.Like STEP1, we use CRFs, and regard sourcecharacters as a feature (window size=3).2.3 Conversion from Alignment to LabelsFirst, character alignment is estimated usingGIZA++ as shown at the top of Fig.
2.
The align-ment direction is a target- language-to-English, as-suming that n English characters correspond to atarget language character.The STEP1 label is generated for each Englishcharacter.
If the alignment is 1:1, we give the char-acter aB label.
If the alignment is n : 1, we assignthe first character a B label, and give the others I .Note that we regard null alignment as a continu-ance of the last segmentation (I).The STEP2 label is generated for each Englishsegmentation (B or BI?).
If a segmentation cor-responds to two or more characters in the targetside, we regard the entire sequence as a label (seeT5 in Fig.
2).3 Experiments3.1 Corpus, Evaluation, and SettingTo evaluate the performance of our system,we used a training-set and test-set provided byNEWS3(Table 1).We used the following six metrics (Table 2) us-ing 10 output candidates.
A white paper4 presentsthe detailed definitions.
For learning, we usedCRF++5 with standard parameters (f=20, c=.5).3http://www.acl-ijcnlp-2009.org/workshops/NEWS2009/4https://translit.i2r.a-star.edu.sg/news2009/whitepaper/5http://crfpp.sourceforge.net/66Table 3: Results in Test-setACC MeanF MRR MAPref MAP10 MAPsysEN?CH 0.580 0.826 0.653 0.580 0.199 0.199EN?RU 0.531 0.912 0.635 0.531 0.219 0.219EN?JA 0.457 0.828 0.576 0.445 0.194 0.194EN?TA 0.365 0.884 0.504 0.360 0.172 0.172EN?HI 0.363 0.864 0.503 0.360 0.170 0.170EN?KA 0.324 0.856 0.438 0.315 0.148 0.148EN?KO 0.170 0.512 0.218 0.170 0.069 0.069Table 2: Evaluation MetricsACC Word Accuracy in Top 1.MeanFThe meanF measures the fuzzy accu-racy that is defined by the edit dis-tance and Longest Common Subse-quence (LCS).MRRMean Reciprocal Rank.
1/MRR tellsapproximately the average rank of thecorrect transliteration.MAPrefMeasures the precision in the n?bestcandidates tightly for each reference.MAP10 Measures the precision in the 10-bestcandidates.MAPsysMeasures the precision in the top Ki-best candidates produced by the system.3.2 Results and DiscussionTable 3 presents the performance.
As shown in thetable, a significant difference was found betweenlanguages (from low (0.17) to high (0.58)).The high accuracy results(EN-CH or EN-RU)are competitive with other systems (the middlerank among the NEWS participating systems).However, several language results (such as EN-KO) were found to have poor performance.We investigated the difference between high-performance languages and the others.
Table 4shows the training/test times and the number oflabels.
As shown in the table, wide divergence isapparent in the number of labels.
For example,although EN?KO requires numerous labels (536labels), EN?RU needs only 131 labels.
This diver-gence roughly corresponds to both training-timeand accuracy as follows: (1) EN?KO requires longtraining time (11 minutes) which gave poor per-formance (0.17 ACC), and (2) EN?RU requiresshort training (only 26.3 seconds) which gave highperformance (0.53 ACC).
This suggests that if thenumber of labels is small, we successfully converttransliteration into a sequential labeling task.The test time seemed to have no relation toTable 4: Average Test time, Training Time, andthe number of labels (label variation).Language Test Train # of labelsEN?KO 0.436s 11m09.5s 536EN?CH 0.201s 6m18.9s 283EN?JA 0.247s 4m44.3s 269EN?KA 0.190s 2m26.6s 231EN?HI 0.302s 1m55.6s 268EN?TA 0.124s 1m32.9s 207EN?RU 0.580s 0m26.3s 131* Test time is the average labeling time for an input.
Trainingtime is the average training time for 1000 labels.both training time and performance.
To investi-gate what gave effects on test time is a subject forour future work.4 Related WorksMost previous transliteration studies have re-lied on a generative model resembling the IBMmodel(Brown et al, 1993).
This approach is ap-plicable to various languages: for Japanese (Gotoet al, 2004; Knight and Graehl, 1998), Korean(Ohand Choi, 2002; Oh and Choi, 2005; Oh andIsahara, 2007), Arabic(Stalls and Knight, 1998;Sherif and Kondrak, 2007), Chinese(Li et al,2007), and Persian(Karimi et al, 2007).
As de-scribed previously, the proposed discriminativeapproach differs from them.Another perspective is that of how to repre-sent transliteration phenomena.
Methods can beclassified into three main types: (1) grapheme-based (Li et al, 2004), (2) phoneme-based (Knightand Graehl, 1998), and (3) combinations of thesemethods (hybrid-model(Bilac and Tanaka, 2004),and a correspondence-based model(Oh and Choi,2002; Oh and Choi, 2005) re-ranking model (Ohand Isahara, 2007)).
Our proposed method em-ploys a grapheme-based approach.
Employingphonemes is a challenge reserved for future stud-ies.Aramaki et al (2008) proposed a discrimina-67tive transliteration approach using Support VectorMachines (SVMs).
However, their goal, which isto judge whether two terms come from the sameEnglish words or not, differs from this paper goal.5 ConclusionsThis paper presents a discriminative translitera-tion model using a sequential labeling technique.Experimental results yielded competitive perfor-mance, demonstrating the feasibility of the pro-posed approach.
In the future, how to incorporatemore rich information, such as language modeland phoneme, is remaining problem.
We believethis task conversion, from generation to sequentiallabeling, can be useful for several practical appli-cations.ACKNOWLEDGMENTPart of this research is supported by JapaneseGrant-in-Aid for Scientific Research (A) Num-ber:20680006.ReferencesEiji Aramaki, Takeshi Imai, Kengo Miyo, andKazuhiko Ohe.
2008.
Orthographic disambiguationincorporating transliterated probability.
In Proceed-ings of International Joint Conference on NaturalLanguage Processing (IJCNLP2008), pages 48?55.Slaven Bilac and Hozumi Tanaka.
2004.
A hybridback-transliteration system for Japanese.
In Pro-ceedings of The 20th International Conference onComputational Linguistics (COLING2004), pages597?603.Peter F. Brown, Stephen A. Della Pietra, Vi centJ.
Della Pietra, and Robert L. Mercer.
1993.The mathematics of statistical machine translation:Parameter estimation.
Computational Linguistics,19(2).Isao Goto, Naoto Kato, Terumasa Ehara, and HidekiTanaka.
2004.
Back transliteration from Japaneseto English using target English context.
In Proceed-ings of The 20th International Conference on Com-putational Linguistics (COLING2004), pages 827?833.Sarvnaz Karimi, Falk Scholer, and Andrew Turpin.2007.
Collapsed consonant and vowel models: Newapproaches for English-Persian transliteration andback-transliteration.
In Proceedings of the AnnualMeeting of the Association of Computational Lin-guistics (ACL2007), pages 648?655.Kevin Knight and Jonathan Graehl.
1998.
Ma-chine transliteration.
Computational Linguistics,24(4):599?612.A.
Kumaran and Tobias Kellner.
2007.
A genericframework for machine transliteration.
In SIGIR?07: Proceedings of the 30th annual internationalACM SIGIR conference on Research and develop-ment in information retrieval, pages 721?722.Haizhou Li, Min Zhang, and Jian Su.
2004.
A jointsource-channel model for machine transliteration.In Proceedings of the Meeting of the Association forComputational Linguistics (ACL2004), pages 159?166.Haizhou Li, Khe Chai Sim, Jin-Shea Kuo, andMinghuiDong.
2007.
Semantic transliteration of per-sonal names.
In Proceedings of the Annual Meet-ing of the Association of Computational Linguistics(ACL2007), pages 120?127.Jong-Hoon Oh and Key-Sun Choi.
2002.
An English-Korean transliteration model using pronunciationand contextual rules.
In Proceedings of The 19th In-ternational Conference on Computational Linguis-tics (COLING2002), pages 758?764.Jong-HoonOh and Key-Sun Choi.
2005.
An ensembleof grapheme and phoneme for machine translitera-tion.
In Proceedings of Second International JointConference on Natural Language Processing (IJC-NLP2005), pages 450?461.Jong-Hoon Oh and Hitoshi Isahara.
2007.
Machinetransliteration using multiple transliteration enginesand hypothesis re-ranking.
In Proceedings of MTSummit XI, pages 353?360.Tarek Sherif and Grzegorz Kondrak.
2007.
Substring-based transliteration.
In Proceedings of the 45th An-nual Meeting of the Association of ComputationalLinguistics (ACL2007), pages 944?951.Bonnie Glover Stalls and Kevin Knight.
1998.
Trans-lating names and technical terms in arabic text.In Proceedings of The International Conferenceon Computational Linguistics and the 36th AnnualMeeting of the Association of Computational Lin-guistics (COLING-ACL1998) Workshop on Compu-tational Approaches to Semitic Languages.68
