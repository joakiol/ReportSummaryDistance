Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 341?350, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsSemEval-2013 Task 9 : Extraction of Drug-Drug Interactions fromBiomedical Texts (DDIExtraction 2013)Isabel Segura-Bedmar, Paloma Mart?
?nez, Mar?
?a Herrero-ZazoUniversidad Carlos III de MadridAv.
Universidad, 30, Legane?s 28911, Spain{isegura,pmf}@inf.uc3m.es, mhzazo@pa.uc3m.esAbstractThe DDIExtraction 2013 task concerns therecognition of drugs and extraction of drug-drug interactions that appear in biomedicalliterature.
We propose two subtasks for theDDIExtraction 2013 Shared Task challenge:1) the recognition and classification of drugnames and 2) the extraction and classificationof their interactions.
Both subtasks have beenvery successful in participation and results.There were 14 teams who submitted a total of38 runs.
The best result reported for the firstsubtask was F1 of 71.5% and 65.1% for thesecond one.1 IntroductionThe definition of drug-drug interaction (DDI) isbroadly described as a change in the effects of onedrug by the presence of another drug (Baxter andStockely, 2010).
The detection of DDIs is an im-portant research area in patient safety since these in-teractions can become very dangerous and increasehealth care costs.
Drug interactions are frequentlyreported in journals, making medical literature themost effective source for their detection (Aronson,2007).
Therefore, Information Extraction (IE) canbe of great benefit in the pharmaceutical industry al-lowing identification and extraction of relevant in-formation on DDIs and providing an interesting wayof reducing the time spent by health care profession-als on reviewing the literature.The DDIExtraction 2013 follows up on afirst event organized in 2011, DDIExtraction2011 (Segura-Bedmar et al 2011b) whose maingoal was the detection of drug-drug interactionsfrom biomedical texts.
The new edition includes inaddition to DDI extraction also a supporting task,the recognition and classification of pharmacologi-cal substances.
DDIExtraction 2013 is designed toaddress the extraction of DDIs as a whole, but di-vided into two subtasks to allow separate evaluationof the performance for different aspects of the prob-lem.
The shared task includes two challenges:?
Task 9.1: Recognition and classification ofpharmacological substances.?
Task 9.2: Extraction of drug-drug interactions.Additionally, while the datasets used forthe DDIExtraction 2011 task were composedby texts describing DDIs from the DrugBankdatabase(Wishart et al 2006), the new datasets forDDIExtraction 2013 also include MedLine abstractsin order to deal with different types of texts andlanguage styles.This shared task has been conceived with a dualobjective: advancing the state-of-the-art of text-mining techniques applied to the pharmacologicaldomain, and providing a common framework forevaluation of the participating systems and other re-searchers interested in the task.In the next section we describe the DDI corpusused in this task.
Sections 3 and 4 focus on the de-scription of the task 9.1 and 9.2 respectively.
Finally,Section 5 draws the conclusions and future work.2 The DDI CorpusThe DDIExtraction 2013 task relies on the DDI cor-pus, which is a semantically annotated corpus of341documents describing drug-drug interactions fromthe DrugBank database and MedLine abstracts onthe subject of drug-drug interactions.The DDI corpus consists of 1,017 texts (784DrugBank texts and 233 MedLine abstracts) andwas manually annotated with a total of 18,491 phar-macological substances and 5,021 drug-drug inter-actions (see Table 1).
A detailed description of themethod used to collect and process documents canbe found in (Segura-Bedmar et al 2011a).
The cor-pus is distributed in XML documents following theunified format for PPI corpora proposed by Pyysaloet al (2008) (see Figure 1).
A detailed descriptionand analysis of the DDI corpus and its methodologyare included in an article currently under review byBioInformatics journal.1The corpus was split in order to build the datasetsfor the training and evaluation of the different par-ticipating systems.
Approximately 77% of the DDIcorpus documents were randomly selected for thetraining dataset and the remaining (142 DrugBanktexts and 91MedLine abstracts) was used for the testdataset.
The training dataset is the same for bothsubtasks since it contains entity and DDI annota-tions.
The test dataset for the task 9.1 was formed bydiscarding documents which contained DDI annota-tions.
Entity annotations were removed from thisdataset to be used by participants.
The remainingdocuments (that is, those containing some interac-tion) were used to create the test dataset for task 9.2.Since entity annotations are not removed from thesedocuments, the test dataset for the task 9.2 can alsobe used as additional training data for the task 9.1.3 Task 9.1: Recognition and classificationof pharmacological substances.This task concerns the named entity extraction ofpharmacological substances in text.
This named en-tity task is a crucial first step for information ex-traction of drug-drug interactions.
In this task, fourtypes of pharmacological substances are defined:drug (generic drug names), brand (branded drugnames), group (drug group names) and drug-n (ac-tive substances not approved for human use).
For a1M.
Herrero-Zazo, I. Segura-Bedmar, P.
Mart??nez.
2013.The DDI Corpus: an annotated corpus with pharmacologicalsubstances and drug-drug interactions, submitted to BioInfor-maticsTraining Test for task 9.1 Test for task 9.2DDI-DrugBankdocuments 572 54 158sentences 5675 145 973drug 8197 180 1518group 3206 65 626brand 1423 53 347drug n 103 5 21mechanism 1260 0 279effect 1548 0 301advice 819 0 215int 178 0 94DDI-MedLinedocuments 142 58 33sentences 1301 520 326drug 1228 171 346group 193 90 41brand 14 6 22drug n 401 115 119mechanism 62 0 24effect 152 0 62advice 8 0 7int 10 0 2Table 1: Basic statistics on the DDI corpus.more detailed description, the reader is directed toour annotation guidelines.2For evaluation, a part of the DDI corpus consist-ing of 52 documents from DrugBank and 58 Med-Line abstracts, is provided with the gold annota-tion hidden.
The goal for participating systems is torecreate the gold annotation.
Each participant sys-tem must output an ASCII list of reported entities,one per line, and formatted as:IdSentence|startOffset-endOffset|text|typeThus, for each recognized entity, each line mustcontain the id of the sentence where this entity ap-pears, the position of the first character and the oneof the last character of the entity in the sentence, thetext of the entity, and its type.
When the entity is adiscontinuous name (eg.
aluminum and magnesiumhydroxide), this second field must contain the startand end positions of all parts of the entity separatedby semicolon.
Multiple mentions from the same sen-tence should appear on separate lines.3.1 Evaluation MetricsThis section describes the methodology that is usedto evaluate the performance of the participating sys-tems in task 9.1.The major forums of the Named Entity Recogni-tion and Classification (NERC) research community(such as MUC-7 (Chinchor and Robinson, 1997),CoNLL 2003 (Tjong Kim Sang and De Meulder,2003) or ACE07 have proposed several techniquesto assess the performance of NERC systems.
While2http://www.cs.york.ac.uk/semeval-2013/task9/342Figure 1: Example of an annotated document of the DDI corpus.Team Affiliation DescriptionTask9.1LASIGE(Grego et al 2013) University of Lisbon, Portugal Conditional random fieldsNLM LHC National Library of Medicine, USA Dictionary-based approachUEM UC3M(Sanchez-Cisneros and Aparicio, 2013) European U. of Madrid, Carlos III University of Madrid, Spain Ontology-based approachUMCC DLSI(Collazo et al 2013) Matanzas University, Cuba j48 classifierUTurku(Bjo?rne et al 2013) University of Turku, Finland SVM classifier (TEES system)WBI NER(Rockta?schel et al 2013) Humboldt University of Berlin, Germany Conditional random fieldsTask9.2FBK-irst (Chowdhury and Lavelli, 2013c) FBK-irst, Italy hybrid kernel + scope of negations and semantic rolesNIL UCM(Bokharaeian, 2013) Complutense University of Madrid, Spain SVM classifier (Weka SMO)SCAI(Bobic?
et al 2013) Fraunhofer SCAI, Germany SVM classifier (LibLINEAR)UC3M(Sanchez-Cisneros, 2013) Carlos III University of Madrid, Spain Shallow Linguistic KernelUCOLORADO SOM(Hailu et al 2013) University of Colorado School of Medicine, USA SVM classifier (LIBSVM)UTurku(Bjo?rne et al 2013) University of Turku, Finland SVM classifier (TEES system)UWM-TRIADS(Rastegar-Mojarad et al 2013) University of Wisconsin-Milwaukee, USA Two-stage SVMWBI DDI(Thomas et al 2013) Humboldt University of Berlin, Germany Ensemble of SVMsTable 2: Short description of the teams.ACE evaluation is very complex because its scoresare not intuitive, MUC and CoNLL 2003 used thestandard precision/recall/f-score metrics to comparetheir participating systems.
The main shared tasks inthe biomedical domain have continued using thesemetrics to evaluate the outputs of their participantteams.System performance should be scored automat-ically by how well the generated pharmacologicalsubstance list corresponds to the gold-standard an-notations.
In our task, we evaluate the results ofthe participating systems according to several evalu-ation criteria.
Firstly, we propose a strict evaluation,which does not only demand exact boundary match,but also requires that both mentions have the sameentity type.
We are aware that this strict criterionmay be too restrictive for our overall goal (extrac-tion of drug interactions) because it misses partialmatches, which can provide useful information fora DDI extraction system.
Our evaluation metricsshould score if a system is able to identify the ex-act span of an entity (regardless of the type) and ifit is able to assign the correct entity type (regardlessof the boundaries).
Thus, our evaluation script willoutput four sets of scores according to:1.
Strict evaluation (exact-boundary and typematching).2.
Exact boundary matching (regardless to thetype).3.
Partial boundary matching (regardless to thetype).4.
Type matching (some overlap between thetagged entity and the gold entitity is required).Evaluation results are reported using the standardprecision/recall/f-score metrics.
We refer the readerto (Chinchor and Sundheim, 1993) for a more de-tailed description of these metrics.These metrics are calculated over all entities andon both axes (type and span) in order to evaluatethe performance of each axe separately.
The finalscore is the micro-averaged F-measure, which is cal-culated over all entity types without distinction.
Themain advantage of the micro-average F1 is that it343takes into account all possible types of errors madeby a NERC system.Additionally, we calculate precision, recall and f-measure for each entity type and then their macro-average measures are provided.
Calculating thesemetrics for each entity type allows us to evalu-ate the level of difficulty of recognizing each en-tity type.
In addition to this, since not all entitytypes have the same frequency, we can better as-sess the performance of the algorithms proposed bythe participating systems.
This is mainly becausethe results achieved on the most frequent entity typehave a much greater impact on overall performancethan those obtained on the entity types with few in-stances.3.2 Results and DiscussionParticipants could send a maximum of three systemruns.
After downloading the test datasets, they hada maximum of two weeks to upload the results.
Atotal of 6 teams participated, submitting 16 systemruns.
Table 2 lists the teams, their affiliations anda brief description of their approaches.
Due to thelack of space we cannot describe them in this paper.Tables 3, 4 and 5 show the F1 scores for each run inalphabetic order.
The reader can find the full rankinginformation on the SemEval-2013 Task 9 website3.The best results were achieved by the WBIteam with a conditional random field.
They em-ployed a domain-independent feature set algwith features generated from the output ofChemSpot (Rockta?schel et al 2012), an existingchemical named entity recognition tool, as well asa collection of domain-specific resources.
Its modelwas trained on the training dataset as well as on en-tities of the test dataset for task 9.2.
The secondtop best performing team developed a dictionary-based approach combining biomedical resourcessuch as DrugBank, the ATC classification system,4or MeSH,5 among others.
Regarding the classifi-cation of each entity type, we observed that branddrugs were easier to recognize than the other types.This could be due to the fact that when a drug is mar-keted by a pharmaceutical company, its brand nameis carefully selected to be short, unique and easy to3http://www.cs.york.ac.uk/semeval-2013/task9/4http://www.whocc.no/atc ddd index/5http://www.ncbi.nlm.nih.gov/meshremember (Boring, 1997).
On the other hand, sub-stances not approved for human use (drug-n) weremore difficult, due to the greater variation and com-plexity in their naming.
In fact, the UEM UC3Mteam was the only team who obtained an F1 measuregreater than 0 on the DDI-DrugBank dataset.
Also,this may indicate that this type is less clearly definedthan the others in the annotation guidelines.
Anotherpossible reason is that the presence of such sub-stances in this dataset is very scarce (less than 1%).It is interesting that almost every participating sys-temwas better in detecting and classifying entities ofa particular class compared to all other systems.
Forinstance, on the whole dataset the dictionary-basedsystem from NLM LHC had it strengths at drug en-tities, UEM UC3M at drug N entities, UTurku atbrand entities and WBI NER at group entities.Finally, the results on the DDI-DrugBank datasetare much better than those obtained on the DDI-MedLine dataset.
While DDI-DrugBank texts focuson the description of drugs and their interactions, themain topic of DDI-MedLine texts would not neces-sarily be on DDIs.
Coupled with this, it is not al-ways trivial to distinguish between substances thatshould be classified as pharmacological substancesand those who should not.
This is due to the ambi-guity of some pharmacological terms.
For example,insulin is a hormone produced by the pancreas, butcan also be synthesized in the laboratory and usedas drug to treat insulin-dependent diabetes mellitus.The participating systems should be able to deter-mine if the text is describing a substance originatedwithin the organism or, on the contrary, it describes aprocess in which the substance is used for a specificpurpose and thus should be identified as pharmaco-logical substance.4 Task 9.2: Extraction of drug-druginteractions.The goal of this subtask is the extraction of drug-drug interactions from biomedical texts.
However,while the previous DDIExtraction 2011 task focusedon the identification of all possible pairs of inter-acting drugs, DDIExtraction 2013 also pursues theclassification of each drug-drug interaction accord-ing to one of the following four types: advice, ef-fect, mechanism, int.
A detailed description of these344Team Run Rank STRICT EXACT PARTIAL TYPE DRUG BRAND GROUP DRUG N MAVGLASIGE1 6 0,656 0,781 0,808 0,69 0,741 0,581 0,712 0,171 0,5772 9 0,639 0,775 0,801 0,672 0,716 0,541 0,696 0,182 0,5713 10 0,612 0,715 0,741 0,647 0,728 0,354 0,647 0,16 0,498NLM LHC1 4 0,698 0,784 0,801 0,722 0,803 0,809 0,646 0 0,572 3 0,704 0,792 0,807 0,726 0,81 0,846 0,643 0 0,581UMCC DLSI 1,2,3 14,15,16 0,275 0,3049 0,367 0,334 0,297 0,313 0,257 0,124 0,311UEM UC3M1 13 0,458 0,528 0,585 0,51 0,718 0,075 0,291 0,185 0,3512 12 0,529 0,609 0,669 0,589 0,752 0,094 0,291 0,264 0,38UTurku1 11 0,579 0,639 0,719 0,701 0,721 0,603 0,478 0,016 0,4682 8 0,641 0,659 0,731 0,766 0,784 0,901 0,495 0,015 0,5573 7 0,648 0,666 0,743 0,777 0,783 0,912 0,485 0,076 0,604WBI1 5 0,692 0,772 0,807 0,729 0,768 0,787 0,761 0,071 0,6152 2 0,708 0,831 0,855 0,741 0,786 0,803 0,757 0,134 0,6433 1 0,715 0,833 0,856 0,748 0,79 0,836 0,776 0,141 0,652Table 3: F1 scores for task 9.1 on the whole test dataset (DDI-MedLine + DDI-DrugBank).
(MAVG for macro-average).
Each run is ranked by STRICT performance.Team Run Rank STRICT EXACT PARTIAL TYPE DRUG BRAND GROUP DRUG N MAVGLASIGE1 8 0,771 0,834 0,855 0,799 0,817 0,571 0,833 0 0,5632 9 0,771 0,831 0,852 0,799 0,823 0,553 0,824 0 0,5683 11 0,682 0,744 0,764 0,713 0,757 0,314 0,756 0 0,47NLM LHC1 2 0,869 0,902 0,922 0,902 0,909 0,907 0,766 0 0,6462 3 0,869 0,903 0,919 0,896 0,911 0,907 0,754 0 0,644UMCC DLSI 1,2,3 14,15,16 0,424 0,4447 0,504 0,487 0,456 0,429 0,371 0 0,351UEM UC3M1 13 0,561 0,632 0,69 0,632 0,827 0,056 0,362 0,022 0,3542 12 0,595 0,667 0,721 0,667 0,842 0,063 0,366 0,028 0,37UTurku1 10 0,739 0,753 0,827 0,864 0,829 0,735 0,553 0 0,5312 6 0,785 0,795 0,863 0,908 0,858 0,898 0,559 0 0,5813 7 0,781 0,787 0,858 0,905 0,847 0,911 0,551 0 0,578WBI1 5 0,86 0,877 0,9 0,89 0,905 0,857 0,782 0 0,6362 4 0,868 0,894 0,914 0,897 0,909 0,865 0,794 0 0,6423 1 0,878 0,901 0,917 0,908 0,912 0,904 0,806 0 0,656Table 4: F1 scores for task 9.1 on the DDI-DrugBank test data.
(MAVG for macro-average).
Each run is ranked bySTRICT performance.Team Run Rank STRICT EXACT PARTIAL TYPE DRUG BRAND GROUP DRUG N MAVGLASIGE1 4 0,567 0,74 0,772 0,605 0,678 0,667 0,612 0,183 0,5772 8 0,54 0,733 0,763 0,576 0,631 0,444 0,595 0,196 0,5123 6 0,557 0,693 0,723 0,596 0,702 0,667 0,56 0,171 0,554NLM LHC1 5 0,559 0,688 0,702 0,575 0,717 0,429 0,548 0 0,4622 3 0,569 0,702 0,715 0,586 0,726 0,545 0,555 0 0,486UMCC DLSI 1,2,3 14,15,16 0,187 0,2228 0,287 0,245 0,2 0,091 0,191 0,13 0,23UEM UC3M1 13 0,39 0,461 0,516 0,431 0,618 0,111 0,238 0,222 0,3412 11 0,479 0,564 0,628 0,529 0,665 0,182 0,233 0,329 0,387UTurku1 12 0,435 0,538 0,623 0,556 0,614 0,143 0,413 0,016 0,3282 10 0,502 0,528 0,604 0,628 0,703 0,923 0,436 0,016 0,5333 9 0,522 0,551 0,634 0,656 0,716 0,923 0,426 0,08 0,582WBI1 7 0,545 0,681 0,726 0,589 0,634 0,353 0,744 0,074 0,4792 2 0,576 0,779 0,807 0,612 0,673 0,444 0,729 0,14 0,5343 1 0,581 0,778 0,805 0,617 0,678 0,444 0,753 0,147 0,537Table 5: F1 scores for task 9.1 on the DDI-MedLine test data.
(MAVG for macro-average).
Each run is ranked bySTRICT performance.345types can be found in our annotation guidelines6.Gold standard annotations (correct, human-created annotations) of pharmacological substancesare provided to participants both for training and testdata.
The test data for this subtask consists of 158DrugBank documents and 33 MedLine abstracts.Each participant system must output an ASCII listincluding all pairs of drugs in each sentence, one perline (multiple DDIs from the same sentence shouldappear on separate lines), its prediction (1 if the pairis a DDI and 0 otherwise) and its type (label nullwhen the prediction value is 0), and formatted as:IdSentence|IdDrug1|IdDrug2|prediction|type4.1 Evaluation MetricsEvaluation is relation-oriented and based on thestandard precision, recall and F-score metrics.
ADDI is correctly detected only if the system is ableto assign the correct prediction label and the correcttype to it.
In other words, a pair is correct only ifboth prediction and type are correct.
The perfor-mance of systems to identify those pairs of drugsinteracting (regardless of the type) is also evaluated.This allows us to assess the progress made with re-gard to the previous edition, which only dealt withthe detection of DDIs.Additionally, we are interested in assessing whichdrug interaction types are most difficult to detect.Thus, we calculate precision, recall and F1 for eachDDI type and then their macro-average measures areprovided.
While micro-averaged F1 is calculatedby constructing a global contingency table and thencalculating precision and recall, macro-averaged F-score is calculated by first calculating precision andrecall for each type and then taking the average ofthese results.Evaluating each DDI type separately allows us toassess the level of difficulty of detecting and classi-fying each type of interaction.
Additionally, it is im-portant to note that the scores achieved on the mostfrequent DDI type have a much greater impact onoverall performance than those achieved on the DDItypes with few instances.
Therefore, by calculatingscores for each type of DDI, we can better assessthe performance of the algorithms proposed by the6http://www.cs.york.ac.uk/semeval-2013/task9/participating systems.4.2 Results and DiscussionThe task of extracting drug-drug interactions frombiomedical texts has attracted the participation of 8teams (see Table 2) who submitted 22 runs.
Tables 6,7 and 8 show the results for each run in alphabeticorder.
Due to the lack of space, the performanceinformation is only shown in terms of F1 score.
Thereader can find the full ranking information on theSemEval-2013 Task 9 website7.Most of the participating systems were built onsupport vector machines.
In general, approachesbased on non-linear kernels methods achieved betterresults than linear SVMs.
As in the previous editionof DDIExtraction, most systems have used primarilysyntactic information.
However, semantic informa-tion has been poorly used.The best results were submitted by the team fromFBK-irst.
They applied a novel hybrid kernel basedRE approach described in Chowdhury (2013a).They also exploited the scope of negations andsemantic roles for negative instance filtering asproposed in (Chowdhury and Lavelli, 2013b) and(Chowdhury and Lavelli, 2012).
The second bestresults were obtained by the WBI team from theHumboldt University of Berlin.
Its system com-bines several kernel methods (APG (Airola et al2008) and Shallow Linguistic Kernel (SL) (Giulianoet al 2006) among others), the Turku Event Ex-traction system (TEES) (Bjo?rne et al 2011)8 andthe Moara system (Neves et al 2009).
These twoteams were also the top two ranked teams in DDIEx-traction 2011.
For a more detailed description, thereader is encouraged to read the papers of the partic-ipants in the proceedings book.While the DDIExtraction 2011 shared task con-centrated efforts on the detection of DDIs, this newDDIExtraction 2013 task involved not only the de-tection of DDIs, but also their classification.
Al-though the results of DDIExtraction 2011 are not di-rectly comparable with the ones reported in DDIEx-traction 2013 due to the use of different training andtest datasets in each edition, it should be noted thatthere has been a significant improvement in the de-7http://www.cs.york.ac.uk/semeval-2013/task9/8http://jbjorne.github.io/TEES/346Team Run Rank CLA DEC MEC EFF ADV INT MAVGFBK-irst1 3 0.638 0.8 0.679 0.662 0.692 0.363 0.6022 1 0.651 0.8 0.679 0.628 0.692 0.547 0.6483 2 0.648 0.8 0.627 0.662 0.692 0.547 0.644NIL UCM1 12 0.517 0.588 0.515 0.489 0.613 0.427 0.5352 10 0.548 0.656 0.531 0.556 0.61 0.393 0.526SCAI1 14 0.46 0.69 0.446 0.459 0.562 0.02 0.4232 16 0.452 0.683 0.441 0.44 0.559 0.021 0.4483 15 0.458 0.704 0.45 0.462 0.54 0.02 0.411UC3M1 11 0.529 0.676 0.48 0.547 0.575 0.5 0.5342 21 0.294 0.537 0.268 0.286 0.325 0.402 0.335UCOLORADO SOM1 22 0.214 0.492 0.109 0.25 0.219 0.097 0.2152 20 0.334 0.504 0.361 0.311 0.381 0.333 0.4073 19 0.336 0.491 0.335 0.313 0.42 0.329 0.38UTurku1 9 0.581 0.684 0.578 0.585 0.606 0.503 0.5722 7 0.594 0.696 0.582 0.6 0.63 0.507 0.5873 8 0.582 0.699 0.569 0.593 0.608 0.511 0.577UWM-TRIADS1 17 0.449 0.581 0.413 0.446 0.502 0.397 0.4512 13 0.47 0.599 0.446 0.449 0.532 0.421 0.4723 18 0.432 0.564 0.442 0.383 0.537 0.292 0.444WBI1 6 0.599 0.736 0.602 0.604 0.618 0.516 0.5882 5 0.601 0.745 0.616 0.595 0.637 0.49 0.5883 4 0.609 0.759 0.618 0.61 0.632 0.51 0.597Table 6: F1 scores for Task 9.2 on the whole test dataset (DDI-MedLine + DDI-DrugBank).
DEC for Detection, CLAfor detection and classification, MEC for mechanism type, EFF for effect type, ADV for advice type, INT for int typeand MAVG for macro-average.
Each run is ranked by CLA performance.Team Run Rank CLA DEC MEC EFF ADV INT MAVGFBK-irst1 3 0.663 0.827 0.705 0.699 0.705 0.376 0.6242 1 0.676 0.827 0.705 0.664 0.705 0.545 0.6723 2 0.673 0.827 0.655 0.699 0.705 0.545 0.667NIL UCM1 12 0.54 0.615 0.527 0.525 0.625 0.444 0.5652 10 0.573 0.68 0.552 0.597 0.619 0.408 0.55SCAI1 15 0.464 0.711 0.449 0.459 0.57 0.021 0.4612 16 0.463 0.71 0.445 0.458 0.569 0.021 0.463 14 0.473 0.734 0.468 0.482 0.551 0.021 0.439UC3M1 11 0.555 0.703 0.493 0.593 0.59 0.51 0.5612 21 0.306 0.549 0.274 0.302 0.334 0.426 0.352UCOLORADO SOM1 22 0.218 0.508 0.115 0.251 0.24 0.098 0.2282 20 0.341 0.518 0.373 0.313 0.398 0.344 0.4253 19 0.349 0.511 0.353 0.324 0.429 0.327 0.394UTurku1 8 0.608 0.712 0.6 0.63 0.617 0.522 0.62 7 0.62 0.724 0.605 0.644 0.638 0.522 0.6143 9 0.608 0.726 0.591 0.635 0.617 0.522 0.601UWM-TRIADS1 17 0.462 0.596 0.43 0.459 0.509 0.405 0.4632 13 0.485 0.616 0.467 0.466 0.536 0.425 0.4863 18 0.445 0.573 0.469 0.39 0.544 0.29 0.46WBI1 6 0.624 0.762 0.621 0.645 0.634 0.52 0.612 5 0.627 0.775 0.636 0.636 0.652 0.5 0.6113 4 0.632 0.783 0.629 0.652 0.65 0.513 0.617Table 7: F1 scores for task 9.2 on the DDI-DrugBank test dataset.
Each run is ranked by CLA performance.Team Run Rank CLA DEC MEC EFF ADV INT MAVGFBK-irst1 4 0.387 0.53 0.383 0.436 0.286 0.211 0.4062 3 0.398 0.53 0.383 0.407 0.286 0.571 0.4363 2 0.398 0.53 0.339 0.436 0.286 0.571 0.44NIL UCM1 20 0.19 0.206 0.286 0.186 0 0 0.1212 19 0.219 0.336 0.143 0.271 0 0 0.11SCAI1 1 0.42 0.462 0.412 0.458 0.2 0 0.2692 8 0.323 0.369 0.389 0.333 0 0 0.1823 6 0.341 0.474 0.31 0.379 0.222 0 0.229UC3M1 15 0.274 0.406 0.333 0.267 0 0.364 0.2682 22 0.186 0.421 0.222 0.171 0.143 0 0.149UCOLORADO SOM1 21 0.188 0.37 0.042 0.241 0 0 0.0732 14 0.275 0.394 0.258 0.302 0.138 0 0.1773 17 0.244 0.356 0.194 0.255 0.222 0.4 0.272UTurku1 18 0.242 0.339 0.258 0.256 0.2 0 0.182 16 0.262 0.344 0.214 0.278 0.364 0 0.2243 13 0.286 0.376 0.286 0.289 0.333 0 0.232UWM-TRIADS1 10 0.312 0.419 0.233 0.36 0.267 0 0.2192 9 0.319 0.436 0.233 0.34 0.421 0.333 0.3453 11 0.306 0.479 0.247 0.326 0.381 0.333 0.33WBI1 7 0.336 0.456 0.368 0.344 0.154 0.4 0.3342 12 0.304 0.406 0.343 0.318 0.167 0 0.2093 5 0.365 0.503 0.476 0.347 0.143 0.4 0.353Table 8: F1 scores for task 9.2 on the DDI-MedLine test dataset.
Each run is ranked by CLA performance.347tection of DDIs: F1 has a remarkable increase from65.74% (the best F1-score in DDIExtraction 2011)to 80% (see DEC column of Table 6).
The increaseof the size of the corpus made for DDIExtraction2013 and of the quality of their annotations mayhave contributed significantly to this improvement.However, the results for the detection and classifi-cation for DDIs did not exceed an F1 of 65.1%.
Ta-ble 6 suggests that some type of DDIs are more diffi-cult to classify than others.
The best F1 ranges from69.2% for advice to 54.7% for int.
One possible ex-planation for this could be that recommendations oradvice regarding a drug interaction are typically de-scribed by very similar text patterns such as DRUGshould not be used in combination with DRUG orCaution should be observed when DRUG is admin-istered with DRUG.Regarding results for the int relationship, it shouldbe noted that the proportion of instances of this re-lationship (5.6%) in the DDI corpus is much smallerthan those of the rest of the relations (41.1% for ef-fect, 32.3% for mechanism and 20.9% for advice).As stated earlier, one of the differences fromthe previous edition is that the corpus developedfor DDIExtraction 2013 is made up of texts fromtwo different sources: MedLine and the DrugBankdatabase.
Thus, the different approaches can beevaluated on two different styles of biomedical texts.While MedLine abstracts are usually written in ex-tremely scientific language, texts from DrugBankare written in a less technical form of the language(similar to the language used in package inserts).
In-deed, this may be the reason why the results on theDDI-DrugBank dataset are much better than thoseobtained on the DDI-MedLine dataset (see Tables 7and 8).5 ConclusionsThe DDIExtraction 2011 task concentrated effortson the novel aspects of the DDI extraction task, thedrug recognition was assumed and the annotationsfor drugs were provided to the participants.
Thisnew DDIExtraction 2013 task pursues the detec-tion and classification of drug interactions as wellas the recognition and classification of pharmaco-logical substances.
The task attracted broad interestfrom the community.
A total of 14 teams from 7 dif-ferent countries participated, submitted a total of 38runs, exceeding the participation of DDIExtraction2011 (10 teams).
The participating systems demon-strated substantial progress at the established DDIextraction task on DrugBank texts and showed thattheir methods also obtain good results for MedLineabstracts.The results that the participating systems have re-ported show successful approaches to this difficulttask, and the advantages of non-linear kernel-basedmethods over linear SVMs for extraction of DDIs.In the named entity task, the participating systemsperform well in recognizing generic drugs, branddrugs and groups of drugs, but they fail in recogniz-ing active substances not approved for human use.Although the results are positive, there is still muchroom to improve in both subtasks.
We have ac-complished our goal of providing a framework anda benchmark data set to allow for comparisons ofmethods for the recognition of pharmacological sub-stances and detection and classification of drug-druginteractions from biomedical texts.We would like that our test dataset can still serveas the basis for fair and stable evaluation after thetask.
Thus, we have decided that the full gold an-notations for the test data are not available for themoment.
We plan to make available a web servicewhere researchers can test their methods on the testdataset and compare their results with the DDIEx-traction 2013 task participants.AcknowledgmentsThis research work has been supported by the Re-gional Government of Madrid under the ResearchNetwork MA2VICMR (S2009/TIC-1542), by theSpanish Ministry of Education under the projectMULTIMEDICA (TIN2010-20644-C03-01).
Addi-tionally, we would like to thank all participants fortheir efforts and to congratulate them to their inter-esting work.ReferencesA.
Airola, S. Pyysalo, J. Bjorne, T. Pahikkala, F. Gin-ter, and T. Salakoski.
2008.
All-paths graph kernelfor protein-protein interaction extraction with evalu-ation of cross-corpus learning.
BMC bioinformatics,9(Suppl 11):S2.348JK.
Aronson.
2007.
Communicating information aboutdrug interactions.
British Journal of Clinical Pharma-cology, 63(6):637?639, June.K.
Baxter and I.H.
Stockely.
2010.
Stockley?s drug inter-actions.8th ed.
London:Pharmaceutical Press.J.
Bjo?rne, J. Heimonen, F. Ginter, A. Airola, T. Pahikkala,and T. Salakoski.
2011.
Extracting contextualizedcomplex biological events with graph-based featuresets.
Computational Intelligence, 27(4):541?557.J.
Bjo?rne, S. Kaewphan, and T. Salakoski.
2013.UTurku: Drug Named Entity Detection and Drug-drugInteraction Extraction Using SVM Classification andDomain Knowledge.
In Proceedings of the 7th Inter-national Workshop on Semantic Evaluation (SemEval2013).T.
Bobic?, J. Fluck, and M. Hofmann-Apitius.
2013.SCAI: Extracting drug-drug interactions using a richfeature vector.
In Proceedings of the 7th InternationalWorkshop on Semantic Evaluation (SemEval 2013).A.
Bokharaeian, B.and D??az.
2013.
NIL UCM: Extract-ing Drug-Drug interactions from text through combi-nation of sequence and tree kernels.
In Proceedings ofthe 7th International Workshop on Semantic Evalua-tion (SemEval 2013).D.
Boring.
1997.
The development and adop-tion of nonproprietary, established, and proprietarynames for pharmaceuticals.
Drug information journal,31(3):621?634.N.
Chinchor and P. Robinson.
1997.
Muc-7 named entitytask definition.
In Proceedings of the 7th Conferenceon Message Understanding.N.
Chinchor and B. Sundheim.
1993.
Muc-5 evalua-tion metrics.
In Proceedings of the 5th conference onMessage understanding, pages 69?78.
Association forComputational Linguistics.MFM.
Chowdhury and A. Lavelli.
2012.
Impact ofless skewed distributions on efficiency and effective-ness of biomedical relation extraction.
In Proceedingsof COLING 2012.MFM.
Chowdhury and A. Lavelli.
2013b.
Exploitingthe scope of negations and heterogeneous features forrelation extraction: Case study drug-drug interactionextraction.
In Proceedings of NAACL 2013.M.F.M.
Chowdhury and A. Lavelli.
2013c.
FBK-irst: A Multi-Phase Kernel Based Approach for Drug-Drug Interaction Detection and Classification that Ex-ploits Linguistic Information.
In Proceedings of the7th International Workshop on Semantic Evaluation(SemEval 2013).MFM.
Chowdhury.
2013a.
Improving the Effectivenessof Information Extraction from Biomedical Text.
Ph.d.dissertation, University of Trento.A.
Collazo, A. Ceballo, D Puig, Y. Gutie?rrez, J. Abreu,J Pe?rez, A.
Ferna?ndez-Orqu?
?n, A. Montoyo, R. Mun?oz,and F. Camara.
2013.
UMCC DLSI-(DDI): Seman-tic and Lexical features for detection and classificationDrugs in biomedical texts.
In Proceedings of the 7thInternational Workshop on Semantic Evaluation (Se-mEval 2013).C.
Giuliano, A. Lavelli, and L. Romano.
2006.
Ex-ploiting shallow linguistic information for relation ex-traction from biomedical literature.
In Proceedings ofthe Eleventh Conference of the European Chapter ofthe Association for Computational Linguistics (EACL-2006), pages 401?408.T.
Grego, F. Pinto, and F.M.
Couto.
2013.
LASIGE: us-ing Conditional Random Fields and ChEBI ontology.In Proceedings of the 7th International Workshop onSemantic Evaluation (SemEval 2013).N.D.
Hailu, L.E.
Hunter, and K.B.
Cohen.
2013.UColorado SOM: Extraction of Drug-Drug Interac-tions from Biomedical Text using Knowledge-rich andKnowledge-poor Features.
In Proceedings of the 7thInternational Workshop on Semantic Evaluation (Se-mEval 2013).ML.
Neves, JM.
Carazo, and A. Pascual-Montano.
2009.Extraction of biomedical events using case-based rea-soning.
In Proceedings of the Workshop on BioNLP:Shared Task, pages 68?76.
Association for Computa-tional Linguistics.S.
Pyysalo, A. Airola, J. Heimonen, J. Bjorne, F. Gin-ter, and T. Salakoski.
2008.
Comparative analysis offive protein-protein interaction corpora.
BMC bioin-formatics, 9(Suppl 3):S6.M.
Rastegar-Mojarad, R. D. Boyce, and R. Prasad.
2013.UWM-TRIADS: Classifying Drug-Drug Interactionswith Two-Stage SVM and Post-Processing.
In Pro-ceedings of the 7th International Workshop on Seman-tic Evaluation (SemEval 2013).T.
Rockta?schel, M. Weidlich, and U. Leser.
2012.Chemspot: a hybrid system for chemical named entityrecognition.
Bioinformatics, 28(12):1633?1640.T.
Rockta?schel, T. Huber, M. Weidlich, and U. Leser.2013.
WBI-NER: The impact of domain-specific fea-tures on the performance of identifying and classifyingmentions of drugs.
In Proceedings of the 7th Inter-national Workshop on Semantic Evaluation (SemEval2013).D.
Sanchez-Cisneros and F. Aparicio.
2013.
UEM-UC3M: An Ontology-based named entity recognitionsystem for biomedical texts.
In Proceedings of the 7thInternational Workshop on Semantic Evaluation (Se-mEval 2013).D.
Sanchez-Cisneros.
2013.
UC3M: A kernel-based ap-proach for identify and classify DDIs in biomedical349texts.
In Proceedings of the 7th International Work-shop on Semantic Evaluation (SemEval 2013).I.
Segura-Bedmar, P.
Mart?
?nez, and C. de Pablo-Sa?nchez.2011a.
Using a shallow linguistic kernel for drug-druginteraction extraction.
Journal of Biomedical Infor-matics, 44(5):789 ?
804.I.
Segura-Bedmar, P. Mart?nez, and D. Sa?nchez-Cisneros.2011b.
The 1st ddiextraction-2011 challenge task:Extraction of drug-drug interactions from biomedicaltexts.
In Proceedings of DDIExtraction-2011 chal-lenge task, pages 1?9.P.
Thomas, M. Neves, T. Rockta?schel, and U. Leser.2013.
WBI-DDI: Drug-Drug Interaction ExtractionusingMajority Voting.
In Proceedings of the 7th Inter-national Workshop on Semantic Evaluation (SemEval2013).E.F.
Tjong Kim Sang and F. De Meulder.
2003.
Intro-duction to the CoNLL-2003 shared task: Language-independent named entity recognition.
In Proceedingsof the seventh conference on Natural language learn-ing at HLT-NAACL 2003-Volume 4, pages 142?147.Association for Computational Linguistics.D.S.
Wishart, C. Knox, A.C. Guo, S. Shrivastava,M.
Hassanali, P. Stothard, Z. Chang, and J. Woolsey.2006.
Drugbank: a comprehensive resource for in sil-ico drug discovery and exploration.
Nucleic acids re-search, 34(suppl 1):D668?D672.350
