First Joint Conference on Lexical and Computational Semantics (*SEM), pages 608?616,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsUMCC_DLSI: Multidimensional Lexical-Semantic Textual SimilarityAntonio Fern?ndez, Yoan Guti?rrez,Alexander Ch?vez, H?ctor D?vila, AndyGonz?lez, Rainel Estrada , Yenier Casta?edaDI, University of MatanzasAutopista a Varadero km 3 ?Matanzas, CubaSonia V?zquezAndr?s Montoyo, Rafael Mu?oz,DLSI, University of AlicanteCarretera de San Vicente S/NAlicante, SpainAbstractThis paper describes the specifications andresults of UMCC_DLSI system, whichparticipated in the first Semantic TextualSimilarity task (STS) of SemEval-2012.
Oursupervised system uses different kinds ofsemantic and lexical features to train classifiersand it uses a voting process to select the correctoption.
Related to the different features we canhighlight the resource ISR-WN1 used to extractsemantic relations among words and the use ofdifferent algorithms to establish semantic andlexical similarities.
In order to establish whichfeatures are the most appropriate to improveSTS results we participated with three runsusing different set of features.
Our bestapproach reached the position 18 of 89 runs,obtaining a general correlation coefficient up to0.72.1.
IntroductionSemEval 2012 competition for evaluating NaturalLanguage Processing (NLP) systems presents anew task called Semantic Textual Similarity (STS)(Agirre et al, 2012).
In STS the participatingsystems must examine the degree of semanticequivalence between two sentences.
The goal ofthis task is to create a unified framework for theevaluation of semantic textual similarity modulesand to characterize their impact on NLPapplications.STS is related to Textual Entailment (TE) andParaphrase tasks.
The main difference is that STS1 Integration of Semantic Resource based on WordNet.assumes bidirectional graded equivalence betweenthe pair of textual snippets.
In the case of TE theequivalence is directional (e.g.
a student is aperson, but a person is not necessarily a student).In addition, STS differs from TE and Paraphrase inthat, rather than being a binary yes/no decision,STS is a similarity-graded notion (e.g.
a studentand a person are more similar than a dog and aperson).
This bidirectional gradation is useful forNLP tasks such as Machine Translation,Information Extraction, Question Answering, andSummarization.
Several semantic tasks could beadded as modules in the STS framework, ?such asWord Sense Disambiguation and Induction,Lexical Substitution, Semantic Role Labeling,Multiword Expression detection and handling,Anaphora and Co-reference resolution, Time andDate resolution and Named Entity Recognition,among others?21.1.
Description of 2012 pilot taskIn STS, all systems were provided with a set ofsentence pairs obtained from a segmented corpus.For each sentence pair, s1 and s2, all participantshad to quantify how similar s1 and s2 were,providing a similarity score.
The output ofdifferent systems was compared to the manualscores provided by SemEval-2012 gold standardfile, which range from 5 to 0 according to the nextcriterions3:?
(5) ?The two sentences are equivalent, as theymean the same thing?.2http://www.cs.york.ac.uk/semeval-2012/task6/3http://www.cs.york.ac.uk/semeval-2012/task6/data/uploads/datasets/train-readme.txt608?
(4) ?The two sentences are mostly equivalent,but some unimportant details differ?.?
(3) ?The two sentences are roughly equivalent,but some important informationdiffers/missing?.?
(2) ?The two sentences are not equivalent, butshare some details?.?
(1) ?The two sentences are not equivalent, butare on the same topic?.?
(0) ?The two sentences are on different topics?.After this introduction, the rest of the paper isorganized as follows.
Section 2 shows thearchitecture of our system and a description of thedifferent runs.
In section 3 we describe thealgorithms and methods used to obtain the featuresfor our system, and Section 4 describe the trainingphase.
The obtained results and a discussion areprovided in Section 5, and finally the conclusionsand future works in Section 6.2.
System architecture and description ofthe runsAs we can see in Figure 1 our three runs beginwith the pre-processing of SemEval 2012?straining set.
Every sentence pair is tokenized,lemmatized and POS tagged using Freeling tool(Atserias et al, 2006).
Afterwards, severalmethods and algorithms are applied in order toextract all features for our Machine LearningSystem (MLS).
Each run uses a particular group offeatures.The Run 1 (MultiSemLex) is our main run.This takes into account all extracted features andtrains a model with a Voting classifier composedby the following techniques: Bagging (using M5P),Bagging (using REPTree), Random SubSpace(using REPTree) and MP5.
The training corpus hasbeen provided by SemEval-2012 competition, inconcrete by the Semantic Textual Similarity task.The Runs 2 and 3 use the same classifier, butincluding different features.
Run 2 (MultiLex) uses(see Figure 1) features extracted from Lexical-Semantic Metrics (LS-M) described in section 3.1,Lexical-Semantic Alignment (LS-A) described insection 3.2 and Sentiment Polarity (SP) describedin section 3.3.On the other hand, the Run 3 (MultiSem) usesfeatures extracted only from Semantic Alignment(SA) described in section 3.4 and the textual editdistances named QGram-Distances.Figure 1.
System Architecture.As a result, we obtain three trained modelscapable to estimate the similarity value betweentwo sentences.Finally, we test our system with the SemEval2012 test set (see Table 7 with the results of ourthree runs).
The following section describes thefeatures extraction process.Run 1VotingClassifierTraining set fromSemEval 2012Pre-Processing (using Freeling)Run 3Voting classifierRun 2Voting classifierSimilarity ScoresFeature extractionLexical-Semantic MetricsLexical-semanticalignmentSemanticalignmentSentimentPolarityJaro QGram Rel.
Concept .
.
.Tokenizing Lemmatizing POS taggingSemEval 2012Test setTraining Process (using Weka)6093.
Description of the features used in theMachine Learning SystemSometimes, when two sentences are very similar,one sentence is in a high degree lexicallyoverlapped by the other.
Inspired by this fact wedeveloped various algorithms, which measure thelevel of overlapping by computing a quantity ofmatching words (the quantity of lemmas thatcorrespond exactly by its morphology) in a pair ofsentences.
In our system, we used lexical andsemantic similarity measures as features for aMLS.
Other features were extracted from a lexical-semantic sentences alignment and a variant usingonly a semantic alignment.3.1.
Similarity measuresWe have used well-known string based similaritymeasures like: Needleman-Wunch (NW) (sequencealignment), Smith-Waterman (SW) (sequencealignment), Jaro, Jaro-Winkler (JaroW), Chapman-Mean-Length (CMLength), QGram-Distance(QGramD), Block-Distance (BD), JaccardSimilarity (JaccardS), Monge-Elkan (ME) andOverlap-Coefficient (OC).
These algorithms havebeen obtained from an API (Application ProgramInterface) SimMetrics library v1.54 for .NET 2.0.Copyright (c) 2006 by Chris Parkinson.
Weobtained 10 features for our MLS from thesesimilarity measures.Using Levenshtein?s edit distance (LED), wecomputed also two different algorithms in order toobtain the alignment of the phrases.
In the firstone, we considered a value of the alignment as theLED between two sentences and the normalizedvariant named NomLED.
Contrary to (Tatu et al,2006), we do not remove the punctuation or stopwords from the sentences, neither considerdifferent cost for transformation operation, and weused all the operations (deletion, insertion andsubstitution).
The second one is a variant that wenamed Double Levenshtein?s Edit Distance(DLED).
For this algorithm, we used LED tomeasure the distance between the sentences, but tocompare the similarity between the words, we usedLED again.
Another feature is the normalizedvariant of DLED named NomDLED.The unique difference between classic LEDalgorithm and DLED is the comparison of4http://sourceforge.net/projects/simmetrics/similitude between two words.
With LED shouldbe: ?[?]
= ?[?
], whereas for our DLED wecalculate words similarity also with LED (e.g.
????(?[?
], ?[?])
<= 2).
Values above a decisionthreshold (experimentally 2) mean unequal words.We obtain as result two new different featuresfrom these algorithms.Another distance we used is an extension ofLED named Extended Distance (EDx) (see(Fern?ndez Orqu?n et al, 2009) for details).
Thisalgorithm is an extension of the Levenshtein?salgorithm, with which penalties are applied byconsidering what kind of operation ortransformation is carried out (insertion, deletion,substitution, or non-operation) in what position,along with the character involved in the operation.In addition to the cost matrixes used byLevenshtein?s algorithm, EDx also obtains theLongest Common Subsequence (LCS)(Hirschberg, 1977) and other helpful attributes fordetermining similarity between strings in a singleiteration.
It is worth noting that the inclusion of allthese penalizations makes the EDx algorithm agood candidate for our approach.
In our previouswork (Fern?ndez Orqu?n et al, 2009), EDxdemonstrated excellent results when it wascompared with other distances as (Levenshtein,1965), (Needleman and Wunsch, 1970), (Winkler,1999).
How to calculate EDx is briefly describedas follows (we refer reader to (Fern?ndez Orqu?n etal., 2009) for a further description):EDx = ??
?????????????,???????(???????)?????????
??
; (1)Where: ?
- Transformations accomplished on the words (?, ?, ?, ?).
?
- Not operations at all, ?
- Insertion, ?
- Deletion, ?
- Substitution.We formalize ?
as a vector:?
=?????
(0,0)(1,0) :: ??
(0,1)(1,1) :: ???????
?1 and ?2 - The examined words ?1j - The j-th character of the word ?1610?2k - The k-th character of the word ?2 ?
- The weight of each characterWe can vary all this weights in order to make aflexible penalization to the interchangeablecharacters.
?
?1j - The weight of characters at ?1j ?
?2k - The weight of characters at ?2k ?
= ??
+ 1 ??
?i ?
??
??
?i = ?
?
; ?
= ??
+ 1 ??
?i ?
??
??
?i = ?
?
?
- The biggest word length of the language ?
- Edit operations length ?i - Operation at (?)
position ????
- Greatest value of ?
ranking?
= ?
2????(2????
+ 1)???????
(2)As we can see in the equation (1), the term ?(??)
?
??????
?, ?(???)?
is the Cartesian product thatanalyzes the importance of doing i-th operationbetween the characters at j-th and k-th positionThe term (2R???
+ 1)???
in equation (1) penalizesthe position of the operations.
The most to the lefthand the operation is the highest the penalizationis.
The term ?
(see equation (2) normalizes theEDx into [0,1] interval.
This measure is also usedas a feature for the system.We also used as a feature the MinimalSemantic Distances (Breadth First Search (BFS))obtained between the most relevant concepts ofboth sentences.
The relevant concepts pertain tosemantic resources ISR-WN (Guti?rrez et al,2011a; 2010b), as WordNet (Miller et al, 1990),WordNet Affect (Strapparava and Valitutti, 2004),SUMO (Niles and Pease, 2001) and SemanticClasses (Izquierdo et al, 2007).
Those conceptswere obtained after having applied the AssociationRatio (AR) measure between concepts and wordsover each sentence.
The obtained distances foreach resource SUMO, WordNet Affect, WordNetand Semantic Classes are named SDist, AffDist,WNDist and SCDist respectively.ISR-WN, takes into account different kind oflabels linked to WN: Level Upper Concepts(SUMO), Domains and Emotion labels.
In thiswork, our purpose is to use a semantic network,which links different semantic resources aligned toWN.
After several tests, we decided to apply ISR-WN.
Although others resources provide differentsemantic relations, ISR-WN has the highestquantity of semantic dimensions aligned, so it is asuitable resource to run our algorithm.Using ISR-WN we are able to extractimportant information from the interrelations offour ontological resources: WN, WND, WNA andSUMO.
ISR-WN resource is based on WN1.6 orWN2.0 versions.
In the last updated version,Semantic Classes and SentiWordNet were alsoincluded.
Furthermore, ISR-WN provides a toolthat allows the navigation across internal links.
Atthis point, we can discover the multidimensionalityof concepts that exists in each sentence.
In order toestablish the concepts associated to each sentencewe apply Relevant Semantic Trees (Guti?rrez etal., 2010a; Guti?rrez et al, 2011b) approach usingthe provided links of ISR-WN.
We refer reader to(Guti?rrez et al, 2010a) for a further description.3.2.
Lexical-Semantic alignmentAnother algorithm that we created is the Lexical-Semantic Alignment.
In this algorithm, we tried toalign the sentences by its lemmas.
If the lemmascoincide we look for coincidences among parts ofspeech, and then the phrase is realigned using both.If the words do not share the same part of speech,they will not be aligned.
Until here, we only havetaken into account a lexical alignment.
From nowon, we are going to apply a semantic variant.
Afterall the process, the non-aligned words will beanalyzed taking into account its WorldNet?srelations (synonymy, hyponymy, hyperonymy,derivationally ?
related ?
form, similar-to, verbalgroup, entailment and cause-to relation); and a setof equivalencies like abbreviations of months,countries, capitals, days and coins.
In the case ofthe relation of hyperonymy and hyponymy, thewords will be aligned if there is a word in the firstsentence that is in the same relation (hyperonymyor hyponymy) of another one in the secondsentence.
For the relations of ?cause-to?
and?implication?
the words will be aligned if there is aword in the first sentence that causes or implicatesanother one of the second sentence.
All the othertypes of relations will be carried out inbidirectional way, that is, there is an alignment if aword of the first sentence is a synonymous ofanother one belonging to the second one or viceversa.
Finally, we obtain a value we calledalignment relation.
This value is calculated as ???
=  ???
/ ????.
Where ???
is the final611alignment value, ???
is the number of alignedword and ????
is the number of words of theshorter phrase.
This value is also another featurefor our system.3.3.
Sentiment Polarity FeatureAnother feature is obtained calculatingSentiWordNet Polarities matches of the analyzedsentences (see (Guti?rrez et al, 2011c) for detail).This analysis has been applied from severaldimensions (WordNet, WordNet Domains,WordNet Affect, SUMO, and Semantic Classes)where the words with sentimental polarity offer tothe relevant concepts (for each conceptual resourcefrom ISR-WN (e.g.
WordNet, WordNet Domains,WordNet Affect, SUMO, and Semantic Classes))its polarity values.
Other analysis were theintegration of all results of polarity in a measureand further a voting process where all polaritiesoutput are involved (for more details see(Fern?ndez et al, 2012)).The final measure corresponds to ??
=?????
+ ????
?, where ???
?1 is a polarity value ofthe sentence ?1 and ?????
is a polarity value of thesentence ?2.
The negative, neutral, and positivevalues of polarities are represented as -1, 0 and 1respectively.3.4.
Semantic AlignmentThis alignment method depends on calculating thesemantic similarity between sentences based on ananalysis of the relations, in ISR-WN, of the wordsthat fix them.First, the two sentences are pre-processed withFreeling and the words are classified according totheir parts of speech (noun, verb, adjective, andadverbs.
).We take 30% of the most probable senses ofevery word and we treat them as a group.
Thedistance between two groups will be the minimaldistance between senses of any pair of wordsbelonging to the group.
For example:Figure 2.
Minimal Distance between ?Run?
and?Chase?.In the example of Figure 2 the ????
= 2 isselected for the pair ?Run-Chase?, because thispair has the minimal cost=2.For nouns and the words that are not found inWordNet like common nouns or Christian names,the distance is calculated in a different way.
In thiscase, we used LED.Let's see the following example:We could take the pair 99 of corpus MSRvid(from training set) with a litter of transformation inorder to a better explanation of our method.Original pairA: A polar bear is running towards a group ofwalruses.B: A polar bear is chasing a group of walruses.Transformed pair:A1: A polar bear runs towards a group of cats.B1: A wale chases a group of dogs.Later on, using the algorithm showed in theexample of Figure 2, a matrix with the distancesbetween all groups of both sentences is created(see Table 1).GROUPS polar bear runs towards group catswale Dist:=3 Dist:=2 Dist:=3 Dist:=5  Dist:=2chases Dist:=4 Dist:=3 Dist:=2 Dist:=4  Dist:=3group     Dist:=0dogs Dist:=3 Dist:=1 Dist:=4 Dist:=4  Dist:=1Table 1.
Distances between the groups.Using the Hungarian Algorithm (Kuhn, 1955)for Minimum Cost Assignment, each group of thesmaller sentence is checked with an element of thebiggest sentence and the rest is marked as wordsthat were not aligned.In the previous example the words ?toward?and ?polar?
are the words that were not aligned, sothe number of non-aligned words is 2.
There isonly one perfect match: ?group-group?
(matchwith ????
= 0).
The length of the shortest sentenceis 4.
The Table 2 shows the results of this analysis.Number ofexactcoincidences(Same)Total Distancesof optimalMatching(Cost)Number ofnon-alignedWords(Dif)Number oflemmas ofshortersentence(Min)1 5 2 4Table 2.
Features extracted from the analyzed sentences.This process has to be repeated for the verbs,nouns (see Table 3), adjectives, and adverbs.
Onthe contrary, the tables have to be created onlywith the similar groups of the sentences.
Table 3Lemma: ChaseLemma: RunDist=2235Sense 1Sense 2Sense 1Sense 24612shows features extracted from the analysis ofnouns.GROUPS bear group catswale Dist := 2  Dist := 2group  Dist := 0dogs Dist := 1  Dist := 1Table 3.
Distances between the groups of nouns.Number ofexactcoincidences(SameN)TotalDistances ofoptimalMatching(CostN)Number ofnon-alignedWords(DifN)Number oflemmas ofshortersentence(MinN)1 3 0 3Table 4.
Feature extracted the analysis of nouns.Several attributes are extracted from the pair ofsentences.
Four attributes from the entiresentences, four attributes considering only verbs,only nouns, only adjectives, and only adverbs.These attributes are:?
Number of exact coincidences (Same)?
Total distance of optimal matching (Cost).?
Number of words that do not match (Dif).?
Number of lemmas of the shortest sentence(Min).As a result, we finally obtain 20 attributes fromthis alignment method.
For each part-of-speech,the attributes are represented adding to its namesthe characters N, V, A and R to represent featuresfor nouns, verbs, adjectives, and adverbsrespectively.It is important to remark that this alignmentprocess searches to solve, for each word from therows (see Table 3) its respectively word from thecolumns.4.
Description of the training phaseFor the training process, we used a supervisedlearning framework, including all the training set(MSRpar, MSRvid and SMTeuroparl) as a trainingcorpus.
Using 10 fold cross validation with theclassifier mentioned in section 2 (experimentallyselected).As we can see in Table 5, the features: FAV,EDx, CMLength, QGramD, BD, Same, SameN,obtain values over 0.50 of correlation.
The morerelevant are EDx and QGramD, which wereselected as a lexical base for the experiment in Run3.
It is important to remark that feature SameN andSame only using number of exact coincidencesobtain an encourage value of correlation.Feature Correlation Feature Correlation Feature CorrelationCorrelation using allfeatures(correspond to Run 1)FAV 0.5064 ME 0.4971 CostV 0.15170.8519LED 0.4572 OC 0.4983 SameN 0.5307DLED 0.4782 SDist 0.4037 MinN 0.4149NormLED 0.4349 AffDist 0.4043 DifN 0.1132NormDLED 0.4457 WNDist 0.2098 CostN 0.1984EDx 0.596 SCDist  0.1532 SameA 0.4182NW 0.2431 PV 0.0342 MinA 0.4261SW 0.2803 Same 0.5753 DifA 0.3818Jaro 0.3611 Min 0.5398 CostA 0.3794JaroW 0.2366 Dif 0.2588 SameR 0.3586CMLength 0.5588 Cost 0.2568 MinR 0.362QGramD 0.5749 SameV 0.3004 DifR 0.3678BD 0.5259 MinV 0.4227 CostR 0.3461JaccardS 0.4849 DifV 0.2634Table 5.
Correlation of individual features over all training sets.We decide to include the Sentiment Polarity asa feature, because our previous results on TextualEntailment task in (Fern?ndez et al, 2012).
But,contrary to what we obtain in this paper, theinfluence of the polarity (PV) for this task is verylow, its contribution working together with otherfeatures is not remarkable, but neither negative(Table 6), So we decide remaining in our system.In oder to select the lexical base for Run 3(MultiSem, features marked in bold) we comparedthe individual influences of the best lexicalfeatures (EDx, QGramD, CMLength), obtaining613the 0.82, 0.83, 0.81 respectively (Table 6).
Finally,we decided to use QGramD.The conceptual features SDist, AffDist,WNDist, SCDist do not increase the similarityscore, this is due to the generality of the obtainedconcept, losing the essential characteristic betweenboth sentences.
Just like with PV we decide tokeep them in our system.As we can see in Table 5, when all features aretaking into account the system obtain the bestscore.Feature Pearson (MSRpar, MSRvid and SMTeuroparl)SDist0.8509AffDistWNDistSCDistEDx0.8507PVQGramD0.8491CMLength0.8075Same0.70430.795 0.829 0.8302 0.8228MinDifCostSameV0.576 MinV DifVCostVSameN0.5975 MinN DifNCostNSameA0.4285 MinA DifACostASameR0.3778 MinR DifRCostRTable 6.
Features influence.Note: Gray cells mean features that are not taking intoaccount.5.
Result and discussionSemantic Textual Similarity task of SemEval-2012offered three official measures to rank thesystems5:1.
ALL: Pearson correlation with the goldstandard for the five datasets, andcorresponding rank.2.
ALLnrm: Pearson correlation after the systemoutputs for each dataset are fitted to the gold5http://www.cs.york.ac.uk/semeval-2012/task6/index.php?id=results-updatestandard using least squares, andcorresponding rank.3.
Mean: Weighted mean across the five datasets,where the weight depends on the number ofpairs in the dataset.4.
Pearson for individual datasets.Using these measures, our main run (Run 1)obtained the best results (see Table 7).
Thisdemonstrates the importance of tackling thisproblem from a multidimensional lexical-semanticpoint of view.Run MSRpar MSRvid SMT-eur On-WN SMT-news1 0.6205 0.8104 0.4325 0.6256 0.43402 0.6022 0.7709 0.4435 0.4327 0.42643 0.5269 0.7756 0.4688 0.6539 0.5470Table 7.
Official SemEval 2012 results.Run ALL Rank ALLnrm RankNrm Mean RankMean1 0.7213 18 0.8239 14 0.6158 152 0.6630 26 0.7922 46 0.5560 493 0.6529 29 0.8115 23 0.6116 16Table 8.
Ranking position of our runs in SemEval 2012.The Run 2 uses a lot of lexical analysis and notmuch of semantic analysis.
For this reason, theresults for Run 2 is poorer (in comparison to theRun 3) (see Table 7) for the test sets: SMT-eur,On-WN and SMT-news.
Of course, these testshave more complex semantic structures than theothers.
However, for test MSRpar it function betterand for test MSRvid it functions very similar toRun 3.Otherwise, the Run 3 uses more semanticanalysis that Run 2 (it uses all features mentionedexcept feature marked in bold on Table 6) and onlyone lexical similarity measure (QGram-Distance).This makes it to work better for test sets SMT-eur,On-WN and SMT-news (see Table 7).
It isimportant to remark that this run obtains importantresults for the test SMT-news, positioning thisvariant in the fifth place of 89 runs.
Moreover, it isinteresting to notice (Table 7) that when mixing thesemantic features with the lexical one (creatingRun 1) it makes the system to improve its generalresults, except for the test: SMT-eur, On-WN andSMT-news in comparison with Run 3.
For thesetest sets seem to be necessary more semanticanalysis than lexical in order to improve similarityestimation.
We assume that Run 1 is non-balanceaccording to the quantity of lexical and semanticfeatures, because this run has a high quantity of614lexical and a few of semantic analysis.
For thatreason, Run 3 has a better performance than Run 1for these test sets.Even when the semantic measures demonstratesignificant results, we do not discard the lexicalhelp on Run 3.
After doing experimentalevaluations on the training phase, when lexicalfeature from QGram-Distance is not taken intoaccount, the Run 3 scores decrease.
Thisdemonstrates that at least a lexical base isnecessary for the Semantic Textual Similaritysystems.6.
Conclusion and future worksThis paper introduced a new framework forrecognizing Semantic Textual Similarity, whichdepends on the extraction of several features thatcan be inferred from a conventional interpretationof a text.As mentioned in section 2 we have conductedthree different runs, these runs only differ in thetype of attributes used.
We can see in Table 7 thatall runs obtained encouraging results.
Our best runwas placed between the first 18th positions of theranking of Semeval 2012 (from 89 Runs) in allcases.
Table 8 shows the reached positions for thethree different runs and the ranking according tothe rest of the teams.In our participation, we used a MLS that workswith features extracted from five differentstrategies: String Based Similarity Measures,Semantic Similarity Measures, Lexical-SemanticAlignment, Semantic Alignment, and SentimentPolarity Cross-checking.We have conducted the semantic featuresextraction in a multidimensional context using theresource ISR-WN, the one that allowed us tonavigate across several semantic resources(WordNet, WordNet Domains, WordNet Affect,SUMO, SentiWorNet and Semantic Classes).Finally, we can conclude that our systemperforms quite well.
In our current work, we showthat this approach can be used to correctly classifyseveral examples from the STS task of SemEval-2012.
Comparing with the best run (UKP_Run2(see Table 9)) of the ranking our main run has veryclosed results.
In two times we increased the bestUKP?s run (UKP_Run 2), for MSRvid test set in0.2824 points and for On-WN test set in 0.1319points (see Table 10).Run ALL Rank ALLnrm RankNrm Mean RankMean(UKP)Run 2 0.8239 1 0.8579 2 0.6773 1Table 9.
The best run of SemEval 2012.It is important to remark that we do not expandany corpus to train the classifier of our system.This fact locates us at disadvantage according toother teams that do it.Run ALL MSRpar MSRvid SMT-eurOn-WNSMT-news(UKP)Run 2 0.8239 0.8739 0.528 0.6641 0.4937 0.4937(Our)Run 1 0.721 0.6205 0.8104 0.4325 0.6256 0.434Table 10.
Comparison of our distance with the best.As future work we are planning to enrich oursemantic alignment method with ExtendedWordNet (Moldovan and Rus, 2001), we think thatwith this improvement we can increase the resultsobtained with texts like those in On-WN test set.AcknowledgmentsThis paper has been supported partially byMinisterio de Ciencia e Innovaci?n - SpanishGovernment (grant no.
TIN2009-13391-C04-01),and Conselleria d'Educaci?n - GeneralitatValenciana (grant no.
PROMETEO/2009/119 andACOMP/2010/288).ReferenceAntonio Fern?ndez, Yoan Guti?rrez, Rafael Mu?oz andAndr?s Montoyo.
2012.
Approaching TextualEntailment with Sentiment Polarity.
In  ICAI'12 - The2012 International Conference on ArtificialIntelligence, Las Vegas, Nevada, USA.Antonio Celso Fern?ndez Orqu?n, D?az Blanco Josval,Alfredo Fundora Rolo and Rafael Mu?oz Guillena.2009.
Un algoritmo para la extracci?n decaracter?sticas lexicogr?ficas en la comparaci?n depalabras.
In  IV Convenci?n Cient?fica InternacionalCIUM, Matanzas, Cuba.Carlo Strapparava and Alessandro Valitutti.
2004.WordNet-Affect: an affective extension of WordNet.In Proceedings of the 4th International Conference onLanguage Resources and Evaluation (LREC 2004),Lisbon,  1083-1086.Daniel S. Hirschberg.
1977.
Algorithms for the longestcommon subsequence problem Journal of the ACM,24: 664?675.615Dan I. Moldovan and Vasile Rus.
2001.
ExplainingAnswers with Extended WordNet ACL.Eneko Agirre, Daniel Cer, Mona Diab and AitorGonzalez-Agirre.
2012.
SemEval-2012 Task 6: APilot on Semantic Textual Similarity.
In Proceedingsof the 6th International Workshop on SemanticEvaluation (SemEval 2012), in conjunction with theFirst Joint Conference on Lexical and ComputationalSemantics (*SEM 2012), Montreal, Canada, ACL.George A. Miller, Richard Beckwith, ChristianeFellbaum, Derek Gross and Katherine Miller.
1990.Introduction to WordNet: An On-line LexicalDatabase International Journal of Lexicography,3(4):235-244.Harold W. Kuhn.
1955.
The Hungarian Method for theassignment problem Naval Research LogisticsQuarterly,  2: 83?97.Ian Niles and Adam Pease.
2001.
Origins of the IEEEStandard Upper Ontology.
In  Working Notes of theIJCAI-2001 Workshop on the IEEE Standard UpperOntology, Seattle, Washington, USA.Jordi Atserias, Bernardino Casas, Elisabet Comelles,Meritxell Gonz?lez, Llu?s Padr?
and Muntsa Padr?.2006.
FreeLing 1.3: Syntactic and semantic servicesin an open source NLP library.
In  Proceedings of thefifth international conference on Language Resourcesand Evaluation (LREC 2006), Genoa, Italy.Marta Tatu, Brandon Iles, John Slavick, NovischiAdrian and Dan Moldovan.
2006.
COGEX at theSecond Recognizing Textual Entailment Challenge.In Proceedings of the Second PASCAL RecognisingTextual Entailment Challenge Workshop, Venice,Italy,  104-109.Rub?n Izquierdo, Armando Su?rez and German Rigau.2007.
A Proposal of Automatic Selection of Coarse-grained Semantic Classes for WSD Procesamientodel Lenguaje Natural,  39: 189-196.Saul B. Needleman and Christian D. Wunsch.
1970.
Ageneral method applicable to the search forsimilarities in the amino acid sequence of twoproteins Journal of Molecular Biology,  48(3): 443-453.Vladimir Losifovich Levenshtein.
1965.
Binary codescapable of correcting spurious insertions anddeletions of ones.
Problems of informationTransmission.
pp.
8-17.William E. Winkler.
1999.
The state of record linkageand current research problems.
Technical Report.U.S.
Census Bureau, Statistical Research Division.Yoan Guti?rrez, Antonio Fern?ndez, And?s Montoyoand Sonia V?zquez.
2010a.
UMCC-DLSI: Integrativeresource for disambiguation task.
In  Proceedings ofthe 5th International Workshop on SemanticEvaluation, Uppsala, Sweden, Association forComputational Linguistics,  427-432.Yoan Guti?rrez, Antonio Fern?ndez, Andr?s Montoyoand Sonia V?zquez.
2010b.
Integration of semanticresources based on WordNet XXVI Congreso de laSociedad Espa?ola para el Procesamiento delLenguaje Natural,  45: 161-168.Yoan Guti?rrez, Antonio Fern?ndez, Andr?s Montoyoand Sonia V?zquez.
2011a.
Enriching the Integrationof Semantic Resources based on WordNetProcesamiento del Lenguaje Natural,  47: 249-257.Yoan Guti?rrez, Sonia V?zquez and Andr?s Montoyo.2011b.
Improving WSD using ISR-WN with RelevantSemantic Trees and SemCor Senses Frequency.
InProceedings of the International Conference RecentAdvances in Natural Language Processing 2011,Hissar, Bulgaria, RANLP 2011 OrganisingCommittee,  233--239.Yoan Guti?rrez, Sonia V?zquez and Andr?s Montoyo.2011c.
Sentiment Classification Using SemanticFeatures Extracted from WordNet-based Resources.In  Proceedings of the 2nd Workshop onComputational Approaches to Subjectivity andSentiment Analysis (WASSA 2.011), Portland,Oregon., Association for Computational Linguistics,139--145.616
