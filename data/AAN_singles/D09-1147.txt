Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1418?1427,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPConsensus Training for Consensus Decoding in Machine TranslationAdam Pauls, John DeNero and Dan KleinComputer Science DivisionUniversity of California at Berkeley{adpauls,denero,klein}@cs.berkeley.eduAbstractWe propose a novel objective function for dis-criminatively tuning log-linear machine trans-lation models.
Our objective explicitly op-timizes the BLEU score of expected n-gramcounts, the same quantities that arise in forest-based consensus and minimum Bayes risk de-coding methods.
Our continuous objectivecan be optimized using simple gradient as-cent.
However, computing critical quantitiesin the gradient necessitates a novel dynamicprogram, which we also present here.
As-suming BLEU as an evaluation measure, ourobjective function has two principle advan-tages over standard max BLEU tuning.
First,it specifically optimizes model weights fordownstream consensus decoding procedures.An unexpected second benefit is that it reducesoverfitting, which can improve test set BLEUscores when using standard Viterbi decoding.1 IntroductionIncreasing evidence suggests that machine trans-lation decoders should not search for a singletop scoring Viterbi derivation, but should insteadchoose a translation that is sensitive to the model?sentire predictive distribution.
Several recent con-sensus decoding methods leverage compact repre-sentations of this distribution by choosing transla-tions according to n-gram posteriors and expectedcounts (Tromble et al, 2008; DeNero et al, 2009;Li et al, 2009; Kumar et al, 2009).
This changein decoding objective suggests a complementarychange in tuning objective, to one that optimizesexpected n-gram counts directly.
The ubiquitousminimum error rate training (MERT) approach op-timizes Viterbi predictions, but does not explicitlyboost the aggregated posterior probability of de-sirable n-grams (Och, 2003).We therefore propose an alternative objectivefunction for parameter tuning, which we call con-sensus BLEU or CoBLEU, that is designed tomaximize the expected counts of the n-grams thatappear in reference translations.
To maintain con-sistency across the translation pipeline, we for-mulate CoBLEU to share the functional form ofBLEU used for evaluation.
As a result, CoBLEUoptimizes exactly the quantities that drive efficientconsensus decoding techniques and precisely mir-rors the objective used for fast consensus decodingin DeNero et al (2009).CoBLEU is a continuous and (mostly) differ-entiable function that we optimize using gradientascent.
We show that this function and its gradientare efficiently computable over packed forests oftranslations generated by machine translation sys-tems.
The gradient includes expectations of prod-ucts of features and n-gram counts, a quantity thathas not appeared in previous work.
We present anew dynamic program which allows the efficientcomputation of these quantities over translationforests.
The resulting gradient ascent proceduredoes not require any k-best approximations.
Op-timizing over translation forests gives similar sta-bility benefits to recent work on lattice-based min-imum error rate training (Macherey et al, 2008)and large-margin training (Chiang et al, 2008).We developed CoBLEU primarily to comple-ment consensus decoding, which it does; it pro-duces higher BLEU scores than coupling MERTwith consensus decoding.
However, we foundan additional empirical benefit: CoBLEU is lessprone to overfitting than MERT, even when usingViterbi decoding.
In experiments, models trainedto maximize tuning set BLEU using MERT con-sistently degraded in performance from tuning totest set, while CoBLEU-trained models general-ized more robustly.
As a result, we found that op-timizing CoBLEU improved test set performancereliably using consensus decoding and occasion-ally using Viterbi decoding.1418Once upon a rhymeH1) Once on a rhymeH3) Once upon a timeH2) Once upon a rhymeIl ?tait une rime(a) Tuning set sentence and translation(a) Hypotheses ranked by ?TM= ?LM= 1(a)  Model score as a function of ?LMReference r:Sentence f:TM LM-3 -7 0.67-5 -6 0.24-9 -3 0.09Pr(b)  Objectives as functions of ?LM(b) Computing Consensus Bigram Precision-18-12-600 2H3H1H2Parameter: ?LMModel:TM+?LM?LMViterbi&ConsensusObjectivesParameter: ?LME?
[c(?Once upon?, d)|f ] = 0.24 + 0.09 = 0.33E?
[c(?upon a?, d)|f ] = 0.24 + 0.09 = 0.33E?
[c(?a rhyme?, d)|f ] = 0.67 + 0.24 = 0.91?gE?
[c(g, d)|f ] = 3[0.67 + 0.24 + 0.09]?gmin{E?
[c(g, d)|f ], c(g, r)}?gE?
[c(g, d)|f ]=0.33 + 0.33 + 0.913Figure 1: (a) A simple hypothesis space of translationsfor a single sentence containing three alternatives, eachwith two features.
The hypotheses are scored under alog-linear model with parameters ?
equal to the identityvector.
(b) The expected counts of all bigrams that ap-pear in the computation of consensus bigram precision.2 Consensus Objective FunctionsOur proposed objective function maximizes n-gram precision by adapting the BLEU evaluationmetric as a tuning objective (Papineni et al, 2002).To simplify exposition, we begin by adapting asimpler metric: bigram precision.2.1 Bigram Precision TuningLet the tuning corpus consist of source sentencesF = f1.
.
.
fmand human-generated referencesR = r1.
.
.
rm, one reference for each sourcesentence.
Let eibe a translation of fi, and letE = e1.
.
.
embe a corpus of translations, one foreach source sentence.
A simple evaluation scorefor E is its bigram precision BP(R,E):BP(R,E) =?mi=1?g2min{c(g2, ei), c(g2, ri)}?mi=1?g2c(g2, ei)where g2iterates over the set of bigrams in the tar-get language, and c(g2, e) is the count of bigramg2in translation e. As in BLEU, we ?clip?
the bi-gram counts of e in the numerator using counts ofbigrams in the reference sentence.Modern machine translation systems are typi-cally tuned to maximize the evaluation score ofViterbi derivations1under a log-linear model withparameters ?.
Let d??
(fi) = arg maxdP?
(d|fi) bethe highest scoring derivation d of fi.
For a systememploying Viterbi decoding and evaluated by bi-gram precision, we would want to select ?
to max-imize MaxBP(R,F, ?
):?mi=1?g2min{c(g2, d??
(fi)), c(g2, ri)}?mi=1?g2c(g2, d??
(fi))On the other hand, for a system that uses ex-pected bigram counts for decoding, we would pre-fer to choose ?
such that expected bigram countsmatch bigrams in the reference sentence.
To thisend, we can evaluate an entire posterior distri-bution over derivations by computing the sameclipped precision for expected bigram counts us-ing CoBP(R,F, ?):?mi=1?g2min{E?
[c(g2, d)|fi], c(g2, ri)}?mi=1?g2E?
[c(g2, d)|fi](1)whereE?
[c(g2, d)|fi] =?dP?
(d|fi)c(g2, d)is the expected count of bigram g2in all deriva-tions d of fi.
We define the precise parametricform of P?
(d|fi) in Section 3.
Figure 1 shows pro-posed translations for a single sentence along withthe bigram expectations needed to compute CoBP.Equation 1 constitutes an objective function fortuning the parameters of a machine translationmodel.
Figure 2 contrasts the properties of CoBPand MaxBP as tuning objectives, using the simpleexample from Figure 1.Consensus bigram precision is an instance of ageneral recipe for converting n-gram based eval-uation metrics into consensus objective functionsfor model tuning.
For the remainder of this pa-per, we focus on consensus BLEU.
However, thetechniques herein, including the optimization ap-proach of Section 3, are applicable to many differ-entiable functions of expected n-gram counts.1By derivation, we mean a translation of a foreign sen-tence along with any latent structure assumed by the model.Each derivation corresponds to a particular English transla-tion, but many derivations may yield the same translation.14191.0 1.5 2.0 2.5 3.0-16-14-12-10?LMLog ModelScoreH1H2H3(a)0 2 4 6 8 100.00.20.40.60.81.0?LMValue of ObjectiveCoBPMaxBPH1H3H1H2H3(b)Figure 2: These plots illustrate two properties of the objectives max bigram precision (MaxBP) and consensusbigram precision (CoBP) on the simple example from Figure 1.
(a) MaxBP is only sensitive to the convex hull (thesolid line) of model scores.
When varying the single parameter ?LM, it entirely disregards the correct translationH2becauseH2never attains a maximal model score.
(b) A plot of both objectives shows their differing characteris-tics.
The horizontal segmented line at the top of the plot indicates the range over which consensus decoding wouldselect each hypothesis, while the segmented line at the bottom indicates the same for Viterbi decoding.
MaxBPis only sensitive to the single point of discontinuity between H1and H3, and disregards H2entirely.
CoBP peakswhen the distribution most heavily favorsH2while suppressingH1.
ThoughH2never has a maximal model score,if ?LMis in the indicated range, consensus decoding would select H2, the desired translation.2.2 CoBLEUThe logarithm of the single-reference2BLEU met-ric (Papineni et al, 2002) has the following form:ln BLEU(R,E) =(1?|R|?mi=1?g1c(g1, ei))?+144?n=1ln?mi=1?gnmin{c(gn, ei), c(gn, ri)}?mi=1?gnc(gn, ei)Above, |R| denotes the number of words in thereference corpus.
The notation (?
)?is shorthandfor min(?, 0).
In the inner sums, gniterates overall n-grams of order n. In order to adapt BLEUto be a consensus tuning objective, we follow therecipe of Section 2.1: we replace n-gram countsfrom a candidate translation with expected n-gramcounts under the model.CoBLEU(R,F, ?)=(1?|R|?mi=1?g1E?
[c(g1, d)|fi])?+144?n=1ln?mi=1?gnmin{E?
[c(gn, d)|fi], c(gn, ri)}?mi=1?gnE?
[c(gn, d)|fi]The brevity penalty term in BLEU is calculatedusing the expected length of the corpus, which2Throughout this paper, we use only a single reference,but our objective readily extends to multiple references.equals the sum of all expected unigram counts.We call this objective function consensus BLEU,or CoBLEU for short.3 Optimizing CoBLEUUnlike the more common MaxBLEU tuning ob-jective optimized by MERT, CoBLEU is con-tinuous.
For distributions P?
(d|fi) that factorover synchronous grammar rules and n-grams, weshow below that it is also analytically differen-tiable, permitting a straightforward gradient ascentoptimization procedure.3In order to perform gra-dient ascent, we require methods for efficientlycomputing the gradient of the objective functionfor a given parameter setting ?.
Once we have thegradient, we can perform an update at iteration tof the form?(t+1)?
?
(t)+ ?t?
?CoBLEU(R,F, ?
(t))where ?tis an adaptive step size.43Technically, CoBLEU is non-differentiable at somepoints because of clipping.
At these points, we must com-pute a sub-gradient, and so our optimization is formally sub-gradient ascent.
See the Appendix for details.4After each successful step, we grow the step size by aconstant factor.
Whenever the objective does not decreaseafter a step, we shrink the step size by a constant factor andtry again until a decrease is attained.1420head(h)tail(h)u=OnceSrhymev1=OnceRBOncev2=uponINuponv3=aNPrhymec(?Once upon?, h)c(?upon a?, h)= 1= 1!2(h) = 2Figure 3: A hyperedge h represents a ?rule?
used insyntactic machine translation.
tail(h) refers to the ?chil-dren?
of the rule, while head(h) refers to the ?head?
or?parent?.
A forest of translations is built by combiningthe nodes viusing h to form a new node u = head(h).Each forest node consists of a grammar symbol and tar-get language boundary words used to track n-grams.
Inthe above, we keep one boundary word for each node,which allows us to track bigrams.In this section, we develop an analytical expres-sion for the gradient of CoBLEU, then discusshow to efficiently compute the value of the objec-tive function and gradient.3.1 Translation Model FormWe first assume the general hypergraph setting ofHuang and Chiang (2007), namely, that deriva-tions under our translation model form a hyper-graph.
This framework allows us to speak aboutboth phrase-based and syntax-based translation ina unified framework.We define a probability distribution over deriva-tions d via ?
as:P?
(d|fi) =w(d)Z(fi)withZ(fi) =?d?w(d?
)where w(d) = exp(?>?
(d, fi)) is the weight of aderivation and ?
(d, fi) is a featurized representa-tion of the derivation d of fi.
We further assumethat these features decompose over hyperedges inthe hypergraph, like the one in Figure 3.
That is,?
(d, fi) =?h?d?
(h, fi).In this setting, we can analytically compute thegradient of CoBLEU.
We provide a sketch of thederivation of this gradient in the Appendix.
Incomputing this gradient, we must calculate the fol-lowing expectations:E?
[c(?k, d)|fi] (2)E?
[`n(d)|fi] (3)E?
[c(?k, d) ?
`n(d)|fi] (4)where `n(d) =?gnc(gn, d) is the sum of all n-grams on derivation d (its ?length?).
The first ex-pectation is an expected count of the kth feature?kover all derivations of fi.
The second is an ex-pected length, the total expected count of all n-grams in derivations of fi.
We call the final ex-pectation an expected product of counts.
We nowpresent the computation of each of these expecta-tions in turn.3.2 Computing Feature ExpectationsThe expected feature counts E?
[c(?k, d)|fi] can bewritten asE?
[c(?k, d)|fi] =?dP?
(d|fi)c(?k, d)=?hP?
(h|fi)c(?k, h)We can justify the second step since fea-ture counts are local to hyperedges, i.e.c(?k, d) =?h?dc(?k, h).
The posteriorprobability P?
(h|fi) can be efficiently computedwith inside-outside scores.
Let I(u) and O(u) bethe standard inside and outside scores for a nodeu in the forest.5P?
(h|fi) =1Z(f)w(h) O(head(h))?v?tail(h)I(v)where w(h) is the weight of hyperedge h, givenby exp(?>?
(h)), and Z(f) = I(root) is the in-side score of the root of the forest.
Computingthese inside-outside quantities takes time linear inthe number of hyperedges in the forest.3.3 Computing n-gram ExpectationsWe can compute the expectations of any specificn-grams, or of total n-gram counts `, in the sameway as feature expectations, provided that target-side n-grams are also localized to hyperedges (e.g.consider ` to be a feature of a hyperedge whosevalue is the number of n-grams on h).
If thenodes in our forests are annotated with target-side5Appendix Figure 7 gives recursions for I(u) and O(u).1421boundary words as in Figure 3, then this will be thecase.
Note that this is the same approach used bydecoders which integrate a target language model(e.g.
Chiang (2007)).
Other work has computedn-gram expectations in the same way (DeNero etal., 2009; Li et al, 2009).3.4 Computing Expectations of Products ofCountsWhile the previous two expectations can be com-puted using techniques known in the literature, theexpected product of counts E?
[c(?k, d) ?
`n(d)|fi]is a novel quantity.
Fortunately, an efficient dy-namic program exists for computing this expec-tation as well.
We present this dynamic programhere as one of the contributions of this paper,though we omit a full derivation due to space re-strictions.To see why this expectation cannot be computedin the same way as the expected feature or n-gramcounts, we expand the definition of the expectationabove to get?dP?
(d|fi) [c(?k, d)`n(d)]Unlike feature and n-gram counts, the product ofcounts in brackets above does not decompose overhyperedges, at least not in an obvious way.
Wecan, however, still decompose the feature countsc(?k, d) over hyperedges.
After this decomposi-tion and a little re-arranging, we get=?hc(?k, h)?d:h?dP?
(d|fi)`n(d)=1Z(fi)?hc(?k, h)[?d:h?dw(d)`n(d)]=1Z(fi)?hc(?k, h)?Dn?
(h|fi)The quantity?Dn?
(h|fi) =?d:h?dw(d)`n(d) is thesum of the weight-length products of all deriva-tions d containing hyperedge h. In the sameway that P?
(h|fi) can be efficiently computedfrom inside and outside probabilities, this quan-tity?Dn?
(h|fi) can be efficiently computed with twonew inside and outside quantities, which we call?In(u) and?On(u).
We provide recursions for thesequantities in Figure 4.
Like the standard inside andoutside computations, these recursions run in timelinear in the number of hyperedges in the forest.While a full exposition of the algorithm is notpossible in the available space, we give some briefintuition behind this dynamic program.
We firstdefine?In(u):?In(u) =?duw(du)`n(d)where duis a derivation rooted at node u.
This isa sum of weight-length products similar to?D.
Togive a recurrence for?I, we rewrite it:?In(u) =?du?h?du[w(du)`n(h)]Here, we have broken up the total value of `n(d)across hyperedges in d. The bracketed quantityis a score of a marked derivation pair (d, h) wherethe edge h is some specific element of d. The scoreof a marked derivation includes the weight of thederivation and the factor `n(h) for the marked hy-peredge.This sum over marked derivations gives the in-side recurrence in Figure 4 by the following de-composition.
For?In(u) to sum over all markedderivation pairs rooted at u, we must consider twocases.
First, the marked hyperedge could be at theroot, in which case we must choose child deriva-tions from regular inside scores and multiply in thelocal `n, giving the first summand of?In(u).
Alter-natively, the marked hyperedge is in exactly oneof the children; for each possibility we recursivelychoose a marked derivation for one child, whilethe other children choose regular derivations.
Thesecond summand of?In(u) compactly expressesa sum over instances of this case.
?On(u) de-composes similarly: the marked hyperedge couldbe local (first summand), under a sibling (secondsummand), or higher in the tree (third summand).Once we have these new inside-outside quanti-ties, we can compute?D as in Figure 5.
This com-bination states that marked derivations containingh are either marked at h, below h, or above h.As a final detail, computing the gradient?Cclipn(?)
(see the Appendix) involves a clippedversion of the expected product of counts, forwhich a clipped?D is required.
This quantity canbe computed with the same dynamic program witha slight modification.
In Figure 4, we show the dif-ference as a choice point when computing `n(h).3.5 Implementation DetailsAs stated, the runtime of computing the requiredexpectations for the objective and gradient is lin-ear in the number of hyperedges in the forest.
The1422?In(u) =?h?IN(u)w(h)?
?`n(h)?v?tail(h)I(v) +?v?tail(h)?In(v)?w 6=vI(w)??
?On(u) =?h?OUT(u)w(h)?????
?`n(h) O(head(h))?v?tail(h)v 6=uI(v) + O(head(h))?v?tail(h)v 6=u?In(v)?w?tail(h)w 6=vw 6=uI(w) +?On(head(h))?w?tail(h)w 6=uI(w)?????
?`n(h) ={?gnc(gn, h) computing unclipped counts?gnc(gn, h)1 [E?
[c(gn, d)] ?
c(gn, ri)] computing clipped countsFigure 4: Inside and Outside recursions for?In(u) and?On(u).
IN(u) and OUT(u) refer to the incoming andoutgoing hyperedges of u, respectively.
I(?)
and O(?)
refer to standard inside and outside quantities, defined inAppendix Figure 7.
We initialize with?In(u) = 0 for all terminal forest nodes u and?On(root) = 0 for the rootnode.
`n(h) computes the sum of all n-grams of order n on a hyperedge h.?Dn?
(h|fi) =w(h)???
?`n(h)O(head(h))?v?tail(h)I(v) + O(head(h))?v?tail(h)?In(v)?v?tail(h)w 6=vI(w) +?On(head(h))?w?tail(h)I(w)???
?Figure 5: Calculation of?Dn?
(h|fi) after?In(u) and?On(u) have been computed.number of hyperedges is very large, however, be-cause we must track n-gram contexts in the nodes,just as we would in an integrated language modeldecoder.
These contexts are required both to cor-rectly compute the model score of derivations andto compute clipped n-gram counts.
To speed ourcomputations, we use the cube pruning method ofHuang and Chiang (2007) with a fixed beam size.For regularization, we added an L2penalty onthe size of ?
to the CoBLEU objective, a simpleaddition for gradient ascent.
We did not find thatour performance varied very much for moderatelevels of regularization.3.6 Related WorkThe calculation of expected counts can be for-mulated using the expectation semiring frame-work of Eisner (2002), though that work doesnot show how to compute expected products ofcounts which are needed for our gradient calcu-lations.
Concurrently with this work, Li and Eis-ner (2009) have generalized Eisner (2002) to com-pute expected products of counts on translationforests.
The training algorithm of Kakade et al(2002) makes use of a dynamic program similar toours, though specialized to the case of sequencemodels.4 Consensus DecodingOnce model parameters ?
are learned, we mustselect an appropriate decoding objective.
Sev-eral new decoding approaches have been proposedrecently that leverage some notion of consensusover the many weighted derivations in a transla-tion forest.
In this paper, we adopt the fast consen-sus decoding procedure of DeNero et al (2009),which directly complements CoBLEU tuning.
Fora source sentence f , we first build a translationforest, then compute the expected count of eachn-gram in the translation of f under the model.We extract a k-best list from the forest, then selectthe translation that yields the highest BLEU scorerelative to the forest?s expected n-gram counts.Specifically, let BLEU(e; r) compute the simi-larity of a sentence e to a reference r based onthe n-gram counts of each.
When training withCoBLEU, we replace e with expected counts andmaximize ?.
In consensus decoding, we replace rwith expected counts and maximize e.Several other efficient consensus decoding pro-1423cedures would similarly benefit from a tuning pro-cedure that aggregates over derivations.
For in-stance, Blunsom and Osborne (2008) select thetranslation sentence with highest posterior proba-bility under the model, summing over derivations.Li et al (2009) propose a variational approxima-tion maximizing sentence probability that decom-poses over n-grams.
Tromble et al (2008) min-imize risk under a loss function based on the lin-ear Taylor approximation to BLEU, which decom-poses over n-gram posterior probabilities.5 ExperimentsWe compared CoBLEU training with an imple-mentation of minimum error rate training on twolanguage pairs.5.1 ModelOur optimization procedure is in principletractable for any syntactic translation system.
Forsimplicity, we evaluate the objective using an In-version Transduction Grammar (ITG) (Wu, 1997)that emits phrases as terminal productions, as in(Cherry and Lin, 2007).
Phrasal ITG models havebeen shown to perform comparably to the state-of-the art phrase-based system Moses (Koehn et al,2007) when using the same phrase table (Petrov etal., 2008).We extract a phrase table using the Mosespipeline, based on Model 4 word alignments gen-erated from GIZA++ (Och and Ney, 2003).
Our fi-nal ITG grammar includes the five standard Mosesfeatures, an n-gram language model, a length fea-ture that counts the number of target words, a fea-ture that counts the number of monotonic ITGrewrites, and a feature that counts the number ofinverted ITG rewrites.5.2 DataWe extracted phrase tables from the Spanish-English and French-English sections of the Eu-roparl corpus, which include approximately 8.5million words of bitext for each of the languagepairs (Koehn, 2002).
We used a trigram lan-guage model trained on the entire corpus of En-glish parliamentary proceedings provided with theEuroparl distribution and generated according tothe ACL 2008 SMT shared task specifications.6For tuning, we used all sentences from the 2007SMT shared task up to length 25 (880 sentences6See http://www.statmt.org/wmt08 for details.2 4 6 8 100.00.20.40.60.81.0IterationsFraction of Value at ConvergenceCoBLEUMERTFigure 6: Trajectories of MERT and CoBLEU dur-ing optimization show that MERT is initially unstable,while CoBLEU training follows a smooth path to con-vergence.
Because these two training procedures op-timize different functions, we have normalized eachtrajectory by the final objective value at convergence.Therefore, the absolute values of this plot do not re-flect the performance of either objective, but ratherthe smoothness with which the final objective is ap-proached.
The rates of convergence shown in this plotare not directly comparable.
Each iteration for MERTabove includes 10 iterations of coordinate ascent, fol-lowed by a decoding pass through the training set.
Eachiteration of CoBLEU training involves only one gradi-ent step.for Spanish and 923 for French), and we tested onthe subset of the first 1000 development set sen-tences which had length at most 25 words (447sentences for Spanish and 512 for French).5.3 Tuning OptimizationWe compared two techniques for tuning the ninelog-linear model parameters of our ITG grammar.We maximized CoBLEU using gradient ascent, asdescribed above.
As a baseline, we maximizedBLEU of the Viterbi translation derivations usingminimum error rate training.
To improve opti-mization stability, MERT used a cumulative k-bestlist that included all translations generated duringthe tuning process.One of the benefits of CoBLEU training is thatwe compute expectations efficiently over an entireforest of translations.
This has substantial stabil-ity benefits over methods based on k-best lists.
InFigure 6, we show the progress of CoBLEU ascompared to MERT.
Both models are initializedfrom 0 and use the same features.
This plot ex-hibits a known issue with MERT training: becausenew k-best lists are generated at each iteration,the objective function can change drastically be-tween iterations.
In contrast, CoBLEU converges1424Consensus DecodingSpanishTune Test ?
Br.MERT 32.5 30.2 -2.3 0.992CoBLEU 31.4 30.4 -1.0 0.992MERT?CoBLEU 31.7 30.8 -0.9 0.992FrenchTune Test ?
Br.MERT 32.5 31.1* -1.4 0.972CoBLEU 31.9 30.9 -1.0 0.954MERT?CoBLEU 32.4 31.2* -0.8 0.953Table 1: Performance measured by BLEU using a con-sensus decoding method over translation forests showsan improvement over MERT when using CoBLEUtraining.
The first two conditions were initialized by0 vectors.
The third condition was initialized by thefinal parameters of MERT training.
Br.
indicates thebrevity penalty on the test set.
The * indicates differ-ences which are not statistically significant.smoothly to its final objective because the forestsdo not change substantially between iterations, de-spite the pruning needed to track n-grams.
Similarstability benefits have been observed for lattice-based MERT (Macherey et al, 2008).5.4 ResultsWe performed experiments from both French andSpanish into English under three conditions.
In thefirst two, we initialized both MERT and CoBLEUtraining uniformly with zero weights and traineduntil convergence.
In the third condition, we ini-tialized CoBLEU with the final parameters fromMERT training, denoted MERT?CoBLEU in theresults tables.
We evaluated each of these condi-tions on both the tuning and test sets using the con-sensus decoding method of DeNero et al (2009).The results appear in Table 1.In Spanish-English, CoBLEU slightly outper-formed MERT under the same initialization, whilethe opposite pattern appears for French-English.The best test set performance in both languagepairs was the third condition, in which CoBLEUtraining was initialized with MERT.
This con-dition also gave the highest CoBLEU objectivevalue.
This pattern indicates that CoBLEU is auseful objective for translation with consensus de-coding, but that the gradient ascent optimization isgetting stuck in local maxima during tuning.
Thisissue can likely be addressed with annealing, asdescribed in (Smith and Eisner, 2006).Interestingly, the brevity penatly results inFrench indicate that, even though CoBLEU didViterbi DecodingSpanishTune Test ?MERT 32.5 30.2 -2.3MERT?CoBLEU 30.5 30.9 +0.4FrenchTune Test ?MERT 32.0 31.0 -1.0MERT?CoBLEU 31.7 30.9 -0.8Table 2: Performance measured by BLEU using Viterbidecoding indicates that CoBLEU is less prone to over-fitting than MERT.not outperform MERT in a statistically significantway, CoBLEU tends to find shorter sentences withhigher n-gram precision than MERT.Table 1 displays a second benefit of CoBLEUtraining: compared to MERT training, CoBLEUperformance degrades less from tuning to testset.
In Spanish, initializing with MERT-trainedweights and then training with CoBLEU actuallydecreases BLEU on the tuning set by 0.8 points.However, this drop in tuning performance comeswith a corresponding increase of 0.6 on the testset, relative to MERT training.
We see the samepattern in French, albeit to a smaller degree.While CoBLEU ought to outperform MERT us-ing consensus decoding, we expected that MERTwould give better performance under Viterbi de-coding.
Surprisingly, we found that CoBLEUtraining actually outperformed MERT in Spanish-English and performed equally well in French-English.
Table 2 shows the results.
In these ex-periments, we again see that CoBLEU overfit thetraining set to a lesser degree than MERT, as evi-denced by a smaller drop in performance from tun-ing to test set.
In fact, test set performance actuallyimproved for Spanish-English CoBLEU trainingwhile dropping by 2.3 BLEU for MERT.6 ConclusionCoBLEU takes a fundamental quantity used inconsensus decoding, expected n-grams, and trainsto optimize a function of those expectations.While CoBLEU can therefore be expected to in-crease test set BLEU under consensus decoding, itis more surprising that it seems to better regularizelearning even for the Viterbi decoding condition.It is also worth emphasizing that the CoBLEU ap-proach is applicable to functions of expected n-gram counts other than BLEU.1425Appendix: The Gradient of CoBLEUWe would like to compute the gradient of(1?|R|?mi=1?g1E?
[c(g1, d)|fi])?+144?n=1ln?mi=1?gnmin{E?
[c(gn, d)|fi], c(gn, ri)}?mi=1?gnE?
[c(gn, d)|fi]To simplify notation, we introduce the functionsCn(?)
=m?i=1?gnE?
[c(gn, e)|fi]Cclipn(?)
=m?i=1?gnmin{E?
[c(gn, d)|fi], c(r, gn)}Cn(?)
represents the sum of the expected countsof all n-grams or order n in all translations ofthe source corpus F , while Cclipn(?)
represents thesum of the same expected counts, but clipped withreference counts c(gn, ri).With this notation, we can write our objectivefunction CoBLEU(R,F, ?)
in three terms:(1?|R|C1(?))?+144?n=1lnCclipn(?)?144?n=1lnCn(?
)We first state an identity:?gn???kE?
[c(gn, d)|fi] =E?
[c(?k, d) ?
`n(d)|fi]?E?
[`n(d)|fi] ?
E?
[c(?k, d)|fi]which can be derived by expanding the expectation onthe left-hand side?gn?d???kP?
(d|fi)c(gn, d)and substituting???kP?
(d|fi) =P?
(d|fi)c(?k, d)?
P?(d|fi)?d?P?
(d?|fi)c(?k, d?
)Using this identity and some basic calculus, thegradient?Cn(?)
ism?i=1E?
[c(?k, d) ?
`n(d)|fi]?
Cn(?)E?
[c(?k, d)|fi]I(u) =?h?IN(u)w(h)???v?tail(h)I(v)?
?O(u) =?h?OUT (u)w(h)???
?O(head(h))?v?tail(h)v 6=uI(v)???
?Figure 7: Standard Inside-Outside recursions whichcompute I(u) and O(u).
IN(u) and OUT(u) refer to theincoming and outgoing hyperedges of u, respectively.We initialize with I(u) = 1 for all terminal forest nodesu and O(root) = 1 for the root node.
These quantitiesare referenced in Figure 4.and the gradient?Cclipn(?)
is given bym?i=1?gn[E?
[c(gn, d) ?
c(?k, d)|fi]?1[E?
[c(gn, d)|fi] ?
c(gn, ri)]]?Cclipn(?)E?
[c(?k, d) + fi]where 1 denotes an indicator function.
At the toplevel, the gradient of the first term (the brevitypenalty) is|R|?C1(?)C1(?)21[C1(?)
?
|R|]The gradient of the second term is144?n=1?Cclipn(?)Cclipn(?
)and the gradient of the third term is?144?n=1?Cn(?)Cn(?
)Note that, because of the indicator func-tions, CoBLEU is non-differentiable whenE?
[c(gn, d)|fi] = c(gn, ri) or Cn(?)
= |R|.Formally, we must compute a sub-gradient atthese points.
In practice, we can choose betweenthe gradients calculated assuming the indicatorfunction is 0 or 1; we always choose the latter.1426ReferencesPhil Blunsom and Miles Osborne.
2008.
Probabilisticinference for machine translation.
In Proceedingsof the Conference on Emprical Methods for NaturalLanguage Processing.Colin Cherry and Dekang Lin.
2007.
Inversion trans-duction grammar for joint phrasal translation mod-eling.
In The Annual Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics Workshop on Syntax and Structure inStatistical Translation.David Chiang, Yuval Marton, and Philip Resnik.
2008.Online large-margin training of syntactic and struc-tural translation features.
In The Conference on Em-pirical Methods in Natural Language Processing.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics.John DeNero, David Chiang, and Kevin Knight.
2009.Fast consensus decoding over translation forests.
InThe Annual Conference of the Association for Com-putational Linguistics.Jason Eisner.
2002.
Parameter estimation for prob-abilistic finite-state transducers.
In Proceedings ofthe 40th Annual Meeting on Association for Compu-tational Linguistics.Liang Huang and David Chiang.
2007.
Forest rescor-ing: Faster decoding with integrated language mod-els.
In The Annual Conference of the Association forComputational Linguistics.Sham Kakade, Yee Whye Teh, and Sam T. Roweis.2002.
An alternate objective function for markovianfields.
In Proceedings of ICML.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InThe Annual Conference of the Association for Com-putational Linguistics.Philipp Koehn.
2002.
Europarl: A multilingual corpusfor evaluation of machine translation.Shankar Kumar, Wolfgang Macherey, Chris Dyer,and Franz Och.
2009.
Efficient minimum errorrate training and minimum Bayes-risk decoding fortranslation hypergraphs and lattices.
In The AnnualConference of the Association for ComputationalLinguistics.Zhifei Li and Jason Eisner.
2009.
First- and second-order expectation semirings with applications tominimum-risk training on translation forests.
InProceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing.Zhifei Li, Jason Eisner, and Sanjeev Khudanpur.
2009.Variational decoding for statistical machine transla-tion.
In The Annual Conference of the Associationfor Computational Linguistics.W.
Macherey, F. Och, I. Thayer, and J. Uszkoreit.2008.
Lattice-based minimum error rate training forstatistical machine translation.
In In Proceedings ofEmpirical Methods in Natural Language Process-ing.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29:19?51.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofthe 41st Annual Meeting on Association for Compu-tational Linguistics (ACL), pages 160?167, Morris-town, NJ, USA.
Association for Computational Lin-guistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A method for automaticevaluation of machine translation.
In The AnnualConference of the Association for ComputationalLinguistics.Slav Petrov, Aria Haghighi, and Dan Klein.
2008.Coarse-to-fine syntactic machine translation us-ing language projections.
In Proceedings of the2008 Conference on Empirical Methods in Nat-ural Language Processing, pages 108?116, Hon-olulu, Hawaii, October.
Association for Computa-tional Linguistics.David Smith and Jason Eisner.
2006.
Minimum riskannealing for training log-linear models.
In In Pro-ceedings of the Association for Computational Lin-guistics.Roy Tromble, Shankar Kumar, Franz Och, and Wolf-gang Macherey.
2008.
Lattice minimum Bayes-riskdecoding for statistical machine translation.
In TheConference on Empirical Methods in Natural Lan-guage Processing.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23:377?404.1427
