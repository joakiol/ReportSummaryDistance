Proceedings of the First Workshop on Argumentation Mining, pages 39?48,Baltimore, Maryland USA, June 26, 2014.c?2014 Association for Computational LinguisticsAnalyzing Argumentative Discourse Units in Online InteractionsDebanjan Ghosh* Smaranda Muresan?
Nina Wacholder* Mark Aakhus* Matthew Mitsui***School of Communication and Information, Rutgers University?Center of Computational Learning Systems, Columbia University**Department of Computer Science, Rutgers Universitydebanjan.ghosh|ninwac|aakhus|mmitsui@rutgers.edu, smara@ccls.columbia.eduAbstractArgument mining of online interactions isin its infancy.
One reason is the lack ofannotated corpora in this genre.
To makeprogress, we need to develop a principledand scalable way of determining whichportions of texts are argumentative andwhat is the nature of argumentation.
Wepropose a two-tiered approach to achievethis goal and report on several initial stud-ies to assess its potential.1 IntroductionAn increasing portion of information and opin-ion exchange occurs in online interactions suchas discussion forums, blogs, and webpage com-ments.
This type of user-generated conversation-al data provides a wealth of naturally occurringarguments.
Argument mining of online interac-tions, however, is still in its infancy (Abbott et al.,2011; Biran and Rambow, 2011; Yin et al., 2012;Andreas et al., 2012; Misra and Walker, 2013).One reason is the lack of annotated corpora in thisgenre.
To make progress, we need to develop aprincipled and scalable way of determining whichportions of texts are argumentative and what is thenature of argumentation.We propose a multi-step coding approachgrounded in findings from argumentation re-search on managing the difficulties of coding ar-guments (Meyers and Brashers, 2010).
In the firststep, trained expert annotators identify basic ar-gumentative features (coarse-grained analysis) infull-length threads.
In the second step, we explorethe feasibility of using crowdsourcing and noviceannotators to identify finer details and nuances ofthe basic argumentative units focusing on limitedthread context.
Our coarse-grained scheme for ar-gumentation is based on Pragmatic ArgumentationTheory (PAT) (Van Eemeren et al., 1993; Hutchby,Figure 1: Argumentative annotation of an OnlineThread2013; Maynard, 1985).
PAT states that an argu-ment can arise at any point when two or moreactors engage in calling out and making prob-lematic some aspect of another actor?s prior con-tribution for what it (could have) said or meant(Van Eemeren et al., 1993).
The argumentativerelationships among contributions to a discussionare indicated through links between what is tar-geted and how it is called-out.
Figure 1 showsan example of two Callouts that refer back to thesame Target.The annotation task performed by the trainedannotators includes three subtasks that Peldszusand Stede (2013a) identify as part of the argu-ment mining problem: 1) Segmentation, 2) Seg-ment classification, and 3) Relationship identifi-cation.
In the language of Peldszus and Stede(2013a), Callouts and Targets are the basic Argu-ment Discourse Units (ADUs) that are segmented,classified, and linked.
There are two key advan-tages of our coarse-grained annotation scheme:1) It does not initially prescribe what constitutesan argumentative text; 2) It makes it possible forExpert Annotators (EAs) to find ADUs in long39threads.
Assigning finer grained (more complex)labels would have unduly increased the alreadyheavy cognitive load for the EAs.
In Section2 we present the corpus, describe the annotationscheme and task, calculate Inter Annotator Agree-ment (IAA), and propose a hierarchical clusteringapproach to identify text segments that the EAsfound easier or harder to annotate.In Section 3, we report on two AmazonMechanical Turk (MTurk) experiments, whichdemonstrate that crowdsourcing is a feasible wayto obtain finer grained annotations of basic ADUs,especially on the text segments that were easierfor the EAs to code.
In the first crowd sourc-ing study, the Turkers (the workers at MTurk,who we consider novice annotators) assigned la-bels (Agree/Disagree/Other) to the relations be-tween Callout and Target identified by the EAs.In the second study, Turkers labeled segments ofCallouts as Stance or Rationale.
Turkers saw onlya limited context of the threaded discussion, i.e.a particular Callout-Target pair identified by theEA(s) who had analyzed the entire thread.
In addi-tion we report on initial classification experimentsto detect agreement/disagreement, with the bestF1 of 66.9% for the Agree class and 62.6% for theDisagree class.2 Expert Annotation for Coarse-GrainedArgumentationWithin Pragmatic Argumentation Theory, argu-mentation refers to the ways in which people (seekto) make some prior action or antecedent eventdisputable by performing challenges, contradic-tions, negations, accusations, resistance, and otherbehaviors that call out a ?Target?, a prior actionor event.
In this section, we present the corpus,the annotation scheme based on PAT and the an-notation task, the inter-annotator agreement, and amethod to identify which pieces of text are easieror harder to annotate using a hierarchical cluster-ing approach.2.1 CorpusOur corpus consists of blog comments posted asresponses to four blog postings selected from adataset crawled from Technorati between 2008-20101.
We selected blog postings in the generaltopic of technology and considered only postings1http://technorati.com/blogs/directory/that had more than 200 comments.
For the an-notation we selected the first one hundred com-ments on each blog together with the original post-ing.
Each blog together with its comments con-stitutes a thread.
The topics of each thread are:Android (comparison of features of iPhone andAndroid phones), iPad (the usefulness of iPads),Twitter (the usefulness of Twitter as a microblog-ging platform), and Layoffs (downsizing and out-sourcing efforts of technology companies).
We re-fer to these threads as the argumentative corpus.We plan to make the corpus available to the re-search community.2.2 Annotation Scheme and ExpertAnnotation TaskThe coarse-grained annotation scheme for argu-mentation is based on the concept of Callout andTarget of Pragmatic Argumentation Theory.
Theexperts?
annotation task was to identify expres-sions of Callout and their Targets while also indi-cating the links between them.
We prepared a setof guidelines with careful definitions of all techni-cal terms.
The following is an abbreviated excerptfrom the guidelines:?
Callout: A Callout is a subsequent actionthat selects (i.e., refers back to) all or somepart of a prior action (i.e., Target) and com-ments on it in some way.
In addition to re-ferring back to the Target, a Callout explic-itly includes either one or both of the fol-lowing: Stance (indication of attitude or posi-tion relative to the Target) and Rationale (ar-gument/justification/explanation of the Stancetaken).?
Target: A Target is a part of a prior action thathas been called out by a subsequent action.Fig.
1 shows two examples of Callouts fromtwo comments referring back to the same Target.Annotators were instructed to mark any text seg-ment (from words to entire comments) that sat-isfied the definitions above.
A single text seg-ment could be a Target and a Callout.
To per-formthe expert annotation, we hired five graduate stu-dents who had a strong background in humanitiesand who received extensive training for the task.The EAs performed three annotation subtasksmentioned by Peldszus and Stede (2013a): Seg-mentation (identify the Argumentative Dis-course40Thread A1 A2 A3 A4 A5Android 73 99 97 118 110iPad 68 86 85 109 118Layoffs 71 83 74 109 117Twitter 76 102 70 113 119Avg.
72 92.5 81.5 112.3 116Table 1: Number of Callouts by threads and EAThread F1 EM F1 OM ?Android 54.4 87.8 0.64iPad 51.2 86.0 0.73Layoffs 51.9 87.5 0.87Twitter 53.8 88.5 0.82Table 2: IAA for 5 EA: F1 and alpha values perthreadUnits (ADUs) including their boundaries), Seg-ment classification (label the roles of the ADUs,in this case Callout and Target) and relation iden-tification (indicate the link between a Callout andthe most recent Target to which is a response).The segmentation task, which Artstein and Poe-sio (2008) refer to as the unitization problem, isparticularly challenging.
Table 1 shows extensivevariation in the number of ADUs (Callout in thiscase) identified by the EAs for each of the fourthreads.
Annotator A1 identified the fewest Call-outs (72) while A4 and A5 identified the most(112.3 and 116, respectively).
Although these dif-ferences could be due to the issues with training,we interpret the consistent variation among codersas an indication that judges can be characterizedas ?lumpers?
or ?splitters?.
What lumpers con-sidered a single long unit was treated as two (ormore) shorter units by splitters.
This is an exampleof the problem of annotator variability discussedin (Peldszus and Stede, 2013b).
Similar behaviorwas noticed for Targets.22.3 Inter Annotator AgreementSince the annotation task includes the segmen-tation step, to measure the IAA we have to ac-count for fuzzy boundaries.
Thus, we con-sidertwo IAA metrics usually used in literature forsuch cases: the information retrieval (IR) in-spiredprecision-recall (P/R/F1) measure (Wiebe et al.,2005) and Krippendorff?s ?
(Krippendorff, 2004).We present here the main results; a detailed dis-cussion of the IAA is left for a different paper.
Fol-lowing Wiebe et al.
(2005), to calculate P/R/F1 fortwo annotators, one annotator?s ADUs are selected2Due to space limitations, here and in the rest of this paperwe report only on Callouts.as the gold standard.
If more than two annotatorsare employed, the IAA is the average of the pair-wise P/R/F1.
To determine if two annotators haveselected the same text span to represent an ADU,we use the two methods of Somasundaran et al.
(2008): exact match (EM) - text spans that varyat the start or end point by five characters or less,and overlap match (OM) - text spans that have atleast 10% of same overlapping characters.
Table 2shows the F1 measure for EM and OM for the fiveEAs on each of the four threads.
As expected, theF1 measures are much lower for EM than for OM.For the second IAA metric, we implementKrippendorff?s ?
(Krippendorff, 2004), where thecharacter overlap between any two annotationsand the gap between them are utilized to mea-sure the expected disagreement and the observeddisagreement.
Table 2 shows ?
values for eachthread, which means significant agreement.While the above metrics show reasonable agree-ment across annotators, they do not tell us whatpieces of text are easier or harder to annotate.
Inthe next section we report on a hierarchical cluster-ing technique that makes it possible to assess howdifficult it is to identify individual text segments asCallouts.2.4 Clustering of Callout ADUsWe use a hierarchical clustering technique (Hastieet al., 2009) to cluster ADUs that are variants ofthe same Callout.
Each ADU starts in its own clus-ter.
The start and end points of each ADU are uti-lized to identify overlapping characters in pairs ofADUs.
Then, using a ?bottom up?
clustering ap-proach, two ADUs (in this case, pairs of Callouts)that share overlapping characters are merged intoa cluster.
This process continues until no moretext segments can be merged.
Clusters with fiveoverlapping ADUs include a text segment that allfive annotators have labeled as a Callout, whileclusters with one ADU indicates that only one an-notator classified the text segment as a Callout(see Table 3).
These numbers provide informationabout what segments of text are easier or harder tocode.
For instance, when a cluster contains onlytwo ADUs, it means that three of the five anno-tators did not label the text segment as a Callout.Our MTurk study of Stance/Rationale (Sec.
3.2)could highlight one reason for the variation ?
somecoders consider a segment of text as Callout whenan implicit Stance is present, while others do not.41# Of EAs Callout Target5 I disagree too.
some things they get right, somethings they do not.the iPhone is a truly great design.I disagree too .
.
.
they do not.
That happened because the iPhone is a trulygreat design.I disagree too.
But when we first tried the iPhone it felt naturalimmediately .
.
.
iPhone is a truly great design.Hi there, I disagree too .
.
.
they do not.
Same asOSX.
?Same as above-I disagree too.
.
.
Same as OSX .
.
.
no problem.
?Same as above-2 Like the reviewer said .
.
.
(Apple) the industryleader.. .
.
Good luck with that (iPhone clones).Many of these iPhone .
.
.
griping about issuesthat will only affect them once in a blue moonLike the reviewer said.
.
.
(Apple) the industryleader.Many of these iPhone.
.
.1 Do you know why the Pre .
.
.
various hand-set/builds/resolution issues?Except for games??
iPhone is clearly dominantthere.Table 3: Examples of Callouts lusters and their corresponding TargetsThread # of Clusters # of EA ADUs per cluster5 4 3 2 1Android 91 52 16 11 7 5Ipad 88 41 17 7 13 10Layoffs 86 41 18 11 6 10Twitter 84 44 17 14 4 5Table 4: Number of clusters for each cluster typeTable 4 shows the number of Callout clusters ineach thread.
The number of clusters with five andfour annotators shows that in each thread there areCallouts that are plausibly easier to identify.
Onthe other hand, the clusters selected by only oneor two annotators are harder to identify.3 Crowdsourcing for Fine-grainedArgumentationTo understand better the nature of the ADUs, weconducted two studies asking Turkers to performfiner grained analysis of Callouts and Targets.
Ourfirst study asked five Turkers to label the relationbetween a Callout and its corresponding Targetas Agree, Disagree, or Other.
The Other relationmay be selected in a situation where the Callouthas no relationship with the Target (e.g., a pos-sible digression) or is in a type of argumentativerelationship that is difficult to classify as eitherAgreement or Disagreement.
The second studyasked five Turkers to identify Stance and Ratio-nale in Callouts identified by EAs.
As discussedin Section 2, by definition, a Callout contains anexplicit instance of Stance, Rationale or both.
Inboth of these crowdsourcing studies the Turkerswere shown only a limited portion of the threadeddiscussion, i.e.
the Callout-Target pairs that theEAs had linked.Crowdsourcing is becoming a popular mecha-nism to collect annotations and other type of datafor natural language processing research (Wangand Callison-Burch, 2010; Snow et al., 2008;Chen and Dolan, 2011; Post et al., 2012).
Crowd-sourcing platforms such as Amazon MechanicalTurk (MTurk) provide a flexible framework to sub-mit various types of NLP tasks where novice anno-tators (Turkers) can generate content (e.g., transla-tions, paraphrases) or annotations (labeling) in aninexpensive way and with limited training.
MTurkalso provides researchers with the ability to con-trol the quality of the Turkers, based on their pastperformances.
Section 3.1 and 3.2 describe ourtwo crowdsourcing studies for fine grain argumen-tation annotation.3.1 Crowdsourcing Study 1: Labeling theRelation between Callout and TargetIn this study, the Turkers?
task was to assign a rela-tion type between a Callout and its associated Tar-get.
The choices were Agree, Disagree, or Other.Turkers were provided with detailed instructions,including multiple examples of Callout and Targetpairs and their relation type.
Each HIT (HumanIntelligence Task, in the language of MTurk) con-tained one Callout-Target pair and Turkers werepaid 2 cents per HIT.
To assure a level of qual-ity control, only qualified Turkers were allowedto perform the task (i.e., Master level with morethan 95% approval rate and at least 500 approvedHITs).For this experiment, we randomly selected aCallout from each cluster, along with its corre-sponding Target.
Our assumption is that all Call-out ADUs in a given cluster have the same relationtype to their Targets (see Table 3).
While this as-sumption is logical, we plan to fully investigate it42in future work by running an MTurk experimenton all the Callout ADUs and their correspondingTargets.We utilized Fleiss?
kappa (Fleiss, 1971) tocompute IAA between the Turkers (every HITwas completed by five Turkers).
Kappa is be-tween 0.45-0.55 for each thread showing moder-ate agreement between the Turkers (Landis et al.,1977).
These agreement results are in line with theagreement noticed in previous studies on agree-ment/disagreement annotations in online interac-tions (Bender et al., 2011; Abbott et al., 2011).To select a gold standard for the relation type, weused majority voting.
That is, if three or moreTurkers agreed on a label, we selected that labelas the gold standard.
In cases where there wasno majority, we assigned the label Other.
The to-tal number of Callouts that are in agreement andin disagreement with Targets are 143 and 153, re-spectively.Table 5 shows the percentage of eachtype of relation identified by Turkers(Agree/Disagree/Other) for clusters annotated bydifferent number of EAs.
The results suggestthat there is a correlation between text segmentsthat are easier or harder to annotate by EAs withthe ability of novice annotators to identify anAgree/Disagree relation type between Callout andTarget.
For example, Turkers generally discoveredAgree/Disagree relations between Callouts andtheir Targets when the Callouts are part of thoseclusters that are annotated by a higher numberof EAs.
Turkers identified 57% as showinga disagreement relation between Callout andTarget, and 39% as showing an agreement relation(clusters with 5 EAs).
For those clusters, only4% of the Callouts are labeled as having an Otherrelation with the Target.
For clusters selectedby fewer EAs, however, the number of Calloutshaving a relation with the Target labeled as Otheris much higher (39% for clusters with two EAsand 32% for clusters with one EA).
These resultsshow that those Callouts that are easier to discover(i.e., identified by all five EAs) mostly have arelation with the Target (Agree or Disagree) thatis clearly expressed and thus recognizable to theTurkers.
Table 5 also shows that in some caseseven if some EAs agreed on a piece of text to beconsidered as a Callout, the novice annotatorsassigned the Other relation to the Callout and Tar-get ADUs.
There are two possible explanations:Relation label # of EA ADUs per cluster5 4 3 2 1Agree 39.36 43.33 42.50 35.48 48.39Disagree 56.91 31.67 32.50 25.81 19.35Other 3.72 25.00 25.00 38.71 32.26Table 5: Percentage of Relation labels per EAcluster typeeither the novice annotators could not detect animplicit agreement or disagreement and thus theyselected Other, or there are other types of relationsbesides Agreement and Disagreement betweenCallouts and their corresponding Targets.
Weplan to extend this study to other fine grainedrelation types in future work.
In the next sectionwe discuss the results of building a supervisedclassifier to predict the Agree or Disagree relationtype between Callout/Target pairs.3.1.1 Predicting the Agree/Disagree RelationLabelWe propose a supervised learning setup to clas-sify the relation types of Callout-Target pairs.
Theclassification categories are the labels collectedfrom the MTurk experiment.
We only considerthe Agree and Disagree categories since the Othercategory has a very small number of instances(53).
Based on the annotations from the Turkers,we have 143 Agree and 153 Disagree training in-stances.We first conducted a simple baseline exper-iment to check whether participants use wordsor phrases to express explicit agreement or dis-agreement such as ?I agree?, ?I disagree?.
Wecollected two small lists (twenty words each)of words from Merriam-Webster dictionary thatexplicitly represent agreement and disagreementStances.
The agreement list contains the word?agree?
and its synonyms such as ?accept?, ?con-cur?, and ?accede?.
The disagreement list con-tains the word ?disagree?
and synonyms such as?differ?
and ?dissent?.
We then checked whetherthe text of the Callouts contains these explicitagreement/disagreement markers.
Note, that thesemarkers are utilized as rules and no statisticallearning is involved in this stage of experiment.The first row of the Table 6 represents the base-line results.
Though the precision is high foragreement category, the recall is quite low and thatresults in a poor overall F1 measure.
This showsthat even though markers like ?agree?
or ?disagree?43Features Category P R F1Baseline Agree 83.3 6.9 12.9Disagree 50.0 5.2 9.5Unigrams Agree 57.9 61.5 59.7Disagree 61.8 58.2 59.9MI-based unigram Agree 60.1 66.4 63.1Disagree 65.2 58.8 61.9LexF Agree 61.4 73.4 66.9Disagree 69.6 56.9 62.63Table 6: Classification of Agree/Disagreeare very precise, they occur in less than 15% ofall the Callouts expressing agreement or disagree-ment.For the next set of experiments we used a super-vised machine learning approach for the two-wayclassification (Agree/Disagree).
We use SupportVector Machines (SVM) as our machine-learningalgorithm for classification as implemented inWeka (Hall et al., 2009) and ran 10-fold cross val-idation.
As a SVM baseline, we first use all un-igrams in Callout and Target as features (Table6, Row 2).
We notice that the recall improvessignificantly when compared with the rule-basedmethod.
To further improve the classification ac-curacy, we use Mutual Information (MI) to se-lect the words in the Callouts and Targets that arelikely to be associated with the categories Agreeand Disagree, respectively.
Specifically, we sorteach word based on its MI value and then se-lect the first 180 words in each of the two cate-gories to represent our new vocabulary set of 360words.
The feature vector includes only wordspresent in the MI list.
Compared to the all uni-grams baseline, the MI-based unigrams improvethe F1 by 4% (Agree) and 2% (Disagree) (Table6).
The MI approach discovers the words thatare highly associated with Agree/Disagree cate-gories and these words turn to be useful featuresfor classification.
In addition, we consider severaltypes of lexical features (LexF) inspired by previ-ous work on agreement and disagreement (Galleyet al., 2004; Misra and Walker, 2013).?
Sentiment Lexicon (SL): Two features are de-signed using a sentiment lexicon (Hu and Liu,2004) where the first feature represents the num-ber of times the Callout and the Target contain apositive emotional word and the second featurerepresents the number of the negative emotionalwords.?
Initial unigrams in Callout (IU): Instead ofusing all unigrams in the Callout and Target,Features Category P R F1LexF Agree 61.4 73.4 66.9Disagree 69.6 56.9 62.6LexF-SL Agree 60.6 74.1 66.7Disagree 69.4 54.9 61.3LexF-IU Agree 58.1 69.9 63.5Disagree 65.3 52.9 58.5LexF-LO Agree 57.2 74.8 64.8Disagree 67.0 47.7 55.7Table 7: Importance of Lexical Featureswe only select the first words from the Call-out (maximum ten).
The assumption is that thestance is generally expressed at the beginningof a Callout.
We used the same MI-based tech-nique to filter any sparse words.?
Lexical Overlap and Length (LO): This set offeatures represents the lexical overlap betweenthe Callout and the Target and the length of eachADU.Table 6 shows that using all these types oflexical features improves the F1 score for bothcategories as compared to the MI-based unigramfeatures.
Table 7 shows the impact of remov-ing each type of lexical features.
From these re-sults it seems that initial unigrams of Callout (IU)and lexical overlap (LO) are useful features: re-moving each of them lowers the results for bothAgree/Disagree categories.
In future work, weplan to explore context-based features such as thethread structure, and semantic features such asWordNet-based semantic similarity.
We also hy-pothesize that with additional training instancesthe ML approaches will achieve better results.3.2 Crowdsourcing Study 2: Analysis ofStance and RationaleIn the second study aimed at identifying the ar-gumentative nature of the Callouts identified bythe expert annotators, we focus on identifying theStance and Rationale segments of a Callout.
Sincethe presence of at least an explicit Stance or Ra-tionale was part of the definition of a Callout, weselected these two argumentation categories as ourfiner-grained scheme for this experiment.Given a pair of Callout and Target ADUs, fiveTurkers were asked to identify the Stance and Ra-tionale segments in the Callout, including the ex-act boundaries of the text segments.
IdentifyingStance and Rationale is a difficult task and thus,we also asked Turkers to mark the level of diffi-culty in the identification task.
We provided the44Diff Number of EAs per cluster5 4 3 2 1VE 22.11 22.38 20.25 16.67 10.71E 28.55 24.00 24.02 28.23 20.00M 19.69 17.87 20.72 19.39 23.57D 11.50 10.34 11.46 9.52 12.86VD 7.02 5.61 4.55 4.42 6.43TD 11.13 19.79 19.00 21.77 26.33Table 8: Difficulty judgments by Turkers com-pared to number of EAs who selected a clusterTurkers with a scale of difficulty (similar to a Lik-ert scale), where the Turkers have to choose oneof the following: very easy (VE), easy (E), moder-ate (M), difficult (D), very difficult (VD), too diffi-cult to code (TD).
Turkers were instructed to selectthe too difficult to code choice only in cases wherethey felt it was impossible to detect a Stance orRationale in the Callout.The Turkers were provided with detailed in-structions including examples of Stance and Ra-tionale annotations for multiple Callouts and onlyhighly qualified Turkers were allowed to performthe task.
Unlike the previous study, we also ran apre-screening testing phase and only Turkers thatpassed the screening were allowed to complete thetasks.
Because of the difficult nature of the anno-tation task, we paid ten cents per HIT.For the Stance/Rationale study, we consideredall the Callouts in each cluster along with the asso-ciated Targets.
We selected all the Callouts fromeach cluster because of variability in the bound-aries of ADUs, i.e., in the segmentation process.One benefit of this crowdsourcing experiment isthat it helps us understand better what the variabil-ity means in terms of argumentative structure.
Forexample, one EA might mark a text segment as aCallout only when it expresses a Stance, while an-other EA might mark as Callout a larger piece oftext expressing both the Stance and Rationale (Seeexamples of Clusters in Table 3).
We leave thisdeeper analysis as future work.Table 8 shows there is a correlation betweenthe number of EAs who selected a cluster and thedifficulty level Turkers assigned to identifying theStance and Rationale elements of a Callout.
Thistable shows that for more than 50% of the Calloutsthat are identified by 5 EAs, the Stance and Ra-tionale can be easily identified (refer to the ?VE?and ?E?
rows), where as in the case of Calloutsthat are identified by only 1 EA, the number isjust 31%.
Similarly, more than 26% of the Call-Diff Number of EAs per cluster5 4 3 2 1E 81.04 70.76 60.98 63.64 25.00M 7.65 7.02 17.07 6.06 25.00D 5.91 5.85 7.32 9.09 12.50TD 5.39 16.37 14.63 21.21 37.50Table 9: Difficulty judgment (majority voting)outs in that same category (1 EA) were labeled as?Too difficult to code?, indicating that the Turk-ers could not identify either a Stance or Rationalein the Callout.
These numbers are comparable towhat our first crowdsourcing study showed for theAgree/Disagree/Other relation identification (Ta-ble 5).
Table 9 shows results where we selectedoverall difficulty level by majority voting.
Wecombined the easy and very easy categories to thecategory easy (E) and the difficult and very diffi-cult categories to the category difficult (D) for asimpler presentation.Table 9 also shows that more than 80% of thetime, Turkers could easily identify Stance and/orRationale in Callouts identified by 5 EAs, whilethey could perform the finer grained analysis eas-ily only 25% of time for Callouts identified by asingle EA.
Only 5% of Callouts identified by all5 EAs were considered too difficult to code by theTurkers (i.e., the novice annotators could not iden-tify a Stance or a Rationale).
In contrast, morethan 37% of Callouts annotated only by 1 EA wereconsidered too difficult to code by the novice an-notators.
Table 10 presents some of the examplesof Stance and Rationale pairs as selected by theTurkers along with the difficulty labels.4 Related WorkPrimary tasks for argument analysis are to seg-ment the text to identify ADUs, detect the rolesof each ADUs, and to establish the relationshipbetween the ADUs (Peldszus and Stede, 2013a).Similarly, Cohen (1987) presented a computa-tional model of argument analysis where the struc-ture of each argument is restricted to the claim andevidence relation.
Teufel et al.
(2009) introducethe argumentative zoning (AZ) idea that identifiesimportant sections of scientific articles and laterHachey and Grover (2005) applied similar idea ofAZ to summarize legal documents.
Wyner et al.
(2012) propose a rule-based tool that can high-light potential argumentative sections of text ac-cording to discourse cues like ?suppose?
or ?there-fore?.
They tested their system on product reviews45Target Callout Stance Rationale Difficultythe iPhone is a trulygreat design.I disagree too.
somethings they get right,some things they donot.I.
.
.
too Some things .
.
.
do not Easythe dedicated ?Back?buttonthat back button is key.navigation is actuallymuch easier on the an-droid.That back button is key Navigationis.
.
.
androidModerateIt?s more about the fea-tures and apps and An-droid seriously lacks onlatter.Just because the iPhonehas a huge amount ofapps, doesn?t meanthey?re all worthhaving.?
Just because the iPhonehas a huge amount ofapps, doesn?t meanthey?re all worthhaving.DifficultI feel like your com-ments about Nexus Oneis too positive .
.
.I feel like your poorgrammar are to obviousto be self thought...?
?
Too difficult tocodeTable 10: Examples of Callout/Target pairs with difficulty level (majority voting)(Canon Camera) from Amazon e-commerce site.Relatively little attention has so far been de-voted to the issue of building argumentative cor-pora from naturally occurring texts (Peldszus andStede, 2013a; Feng and Hirst, 2011).
However,(Reed et al., 2008; Reed and Rowe, 2004) havedeveloped the Araucaria project that maintainsan online repository of arguments (AraucariaDB),which recently has been used as research cor-pus for several automatic argumentation analyses(Palau and Moens, 2009; Wyner et al., 2010; Fengand Hirst, 2011).
Our work contributes a new prin-cipled method for building annotated corpora foronline interactions.
The corpus and guidelines willalso be shared with the research community.Another line of research that is correlated withours is recognition of agreement/disagreement(Misra and Walker, 2013; Yin et al., 2012; Ab-bott et al., 2011; Andreas et al., 2012; Galley etal., 2004; Hillard et al., 2003) and classification ofstances (Walker et al., 2012; Somasundaran andWiebe, 2010) in online forums.
For future work,we can utilize textual features (contextual, depen-dency, discourse markers), relevant multiword ex-pressions and topic modeling (Mukherjee and Liu,2013), and thread structure (Murakami and Ray-mond, 2010; Agrawal et al., 2003) to improve theAgree/Disagree classification accuracy.Recently, Cabrio and Villata (2013) proposeda new direction of argumentative analysis wherethe authors show how arguments are associatedwith Recognizing Textual Entailment (RTE) re-search.
They utilized RTE approach to detect therelation of support/attack among arguments (en-tailment expresses a ?support?
and contradictionexpresses an ?attack?)
on a dataset of argumentscollected from online debates (e.g., Debatepedia).5 Conclusion and Future WorkTo make progress in argument mining for onlineinteractions, we need to develop a principled andscalable way to determine which portions of textsare argumentative and what is the nature of argu-mentation.
We have proposed a two-tiered ap-proach to achieve this goal.
As a first step weadopted a coarse-grained annotation scheme basedon Pragmatic Argumentation Theory and askedexpert annotators to label entire threads using thisscheme.
Using a clustering technique we iden-tified which pieces of text were easier or harderfor the Expert Annotators to annotate.
Then weshowed that crowdsourcing is a feasible approachto obtain annotations based on a finer grained ar-gumentation scheme, especially on text segmentsthat were easier for the Expert Annotators to la-bel as being argumentative.
While more qualita-tive analysis of these results is still needed, theseresults are an example of the potential benefits ofour multi-step coding approach.Avenues for future research include but are notlimited to: 1) analyzing the differences betweenthe stance and rationale annotations among thenovice annotators; 2) improving the classificationaccuracies of the Agree/Disagree classifier usingmore training data; 3) using syntax and seman-tics inspired textual features and thread structure;and 4) developing computational models to detectStance and Rationale.46AcknowledgementsPart of this paper is based on work supported bythe DARPA-DEFT program for the first two au-thors.
The views expressed are those of the au-thors and do not reflect the official policy or po-sition of the Department of Defense or the U.S.Government.ReferencesRob Abbott, Marilyn Walker, Pranav Anand, Jean EFox Tree, Robeson Bowmani, and Joseph King.2011.
How can you say such things?!?
: Recogniz-ing disagreement in informal political argument.
InProceedings of the Workshop on Languages in So-cial Media, pages 2?11.
Association for Computa-tional Linguistics.Rakesh Agrawal, Sridhar Rajagopalan, RamakrishnanSrikant, and Yirong Xu.
2003.
Mining newsgroupsusing networks arising from social behavior.
InProceedings of the 12th international conference onWorld Wide Web, pages 529?535.
ACM.Jacob Andreas, Sara Rosenthal, and Kathleen McKe-own.
2012.
Annotating agreement and disagree-ment in threaded discussion.
In LREC, pages 818?822.Ron Artstein and Massimo Poesio.
2008.
Inter-coderagreement for computational linguistics.
Computa-tional Linguistics, 34(4):555?596.Emily M Bender, Jonathan T Morgan, Meghan Oxley,Mark Zachry, Brian Hutchinson, Alex Marin, BinZhang, and Mari Ostendorf.
2011.
Annotating so-cial acts: Authority claims and alignment moves inwikipedia talk pages.
In Proceedings of the Work-shop on Languages in Social Media, pages 48?57.Association for Computational Linguistics.Or Biran and Owen Rambow.
2011.
Identifying jus-tifications in written dialogs by classifying text asargumentative.
International Journal of SemanticComputing, 5(04):363?381.Elena Cabrio and Serena Villata.
2013.
A naturallanguage bipolar argumentation approach to supportusers in online debate interactions.
Argument &Computation, 4(3):209?230.David L Chen and William B Dolan.
2011.
Collect-ing highly parallel data for paraphrase evaluation.In Proceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics: HumanLanguage Technologies-Volume 1, pages 190?200.Association for Computational Linguistics.Robin Cohen.
1987.
Analyzing the structure of ar-gumentative discourse.
Computational linguistics,13(1-2):11?24.Vanessa Wei Feng and Graeme Hirst.
2011.
Clas-sifying arguments by scheme.
In Proceedingsof the 49th Annual Meeting of the Associationfor Computational Linguistics: Human LanguageTechnologies-Volume 1, pages 987?996.
Associationfor Computational Linguistics.Joseph L Fleiss.
1971.
Measuring nominal scaleagreement among many raters.
Psychological bul-letin, 76(5):378.Michel Galley, Kathleen McKeown, Julia Hirschberg,and Elizabeth Shriberg.
2004.
Identifying agree-ment and disagreement in conversational speech:Use of bayesian networks to model pragmatic de-pendencies.
In Proceedings of the 42nd AnnualMeeting on Association for Computational Linguis-tics, page 669.
Association for Computational Lin-guistics.Ben Hachey and Claire Grover.
2005.
Automatic le-gal text summarisation: experiments with summarystructuring.
In Proceedings of the 10th internationalconference on Artificial intelligence and law, pages75?84.
ACM.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H Witten.2009.
The weka data mining software: an update.ACM SIGKDD explorations newsletter, 11(1):10?18.Trevor Hastie, Robert Tibshirani, Jerome Friedman,T Hastie, J Friedman, and R Tibshirani.
2009.
Theelements of statistical learning, volume 2.
Springer.Dustin Hillard, Mari Ostendorf, and ElizabethShriberg.
2003.
Detection of agreement vs. dis-agreement in meetings: Training with unlabeleddata.
In Proceedings of the 2003 Conference ofthe North American Chapter of the Associationfor Computational Linguistics on Human LanguageTechnology: companion volume of the Proceedingsof HLT-NAACL 2003?short papers-Volume 2, pages34?36.
Association for Computational Linguistics.Minqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of the tenthACM SIGKDD international conference on Knowl-edge discovery and data mining, pages 168?177.ACM.Ian Hutchby.
2013.
Confrontation talk: Arguments,asymmetries, and power on talk radio.
Routledge.Klaus Krippendorff.
2004.
Measuring the reliabilityof qualitative text analysis data.
Quality & quantity,38:787?800.J Richard Landis, Gary G Koch, et al.
1977.
The mea-surement of observer agreement for categorical data.biometrics, 33(1):159?174.Douglas W Maynard.
1985.
How children start argu-ments.
Language in society, 14(01):1?29.47Renee A Meyers and Dale Brashers.
2010.
Extend-ing the conversational argument coding scheme: Ar-gument categories, units, and coding procedures.Communication Methods and Measures, 4(1-2):27?45.Amita Misra and Marilyn A Walker.
2013.
Topic in-dependent identification of agreement and disagree-ment in social media dialogue.
In Proceedings ofthe SIGDIAL 2013 Conference, pages 41?50.
Asso-ciation for Computational Linguistics.Arjun Mukherjee and Bing Liu.
2013.
Discoveringuser interactions in ideological discussions.
In Pro-ceedings of the 51st Annual Meeting on Associationfor Computational Linguistics, pages 671?681.
Cite-seer.Akiko Murakami and Rudy Raymond.
2010.
Supportor oppose?
: classifying positions in online debatesfrom reply activities and opinion expressions.
InProceedings of the 23rd International Conference onComputational Linguistics: Posters, pages 869?875.Association for Computational Linguistics.Raquel Mochales Palau and Marie-Francine Moens.2009.
Argumentation mining: the detection, clas-sification and structure of arguments in text.
In Pro-ceedings of the 12th international conference on ar-tificial intelligence and law, pages 98?107.
ACM.Andreas Peldszus and Manfred Stede.
2013a.
From ar-gument diagrams to argumentation mining in texts:A survey.
International Journal of Cognitive Infor-matics and Natural Intelligence (IJCINI), 7(1):1?31.Andreas Peldszus and Manfred Stede.
2013b.
Rankingthe annotators: An agreement study on argumenta-tion structure.
In Proceedings of the 7th linguisticannotation workshop and interoperability with dis-course, pages 196?204.Matt Post, Chris Callison-Burch, and Miles Osborne.2012.
Constructing parallel corpora for six indianlanguages via crowdsourcing.
In Proceedings of theSeventh Workshop on Statistical Machine Transla-tion, pages 401?409.
Association for ComputationalLinguistics.Chris Reed and Glenn Rowe.
2004.
Araucaria: Soft-ware for argument analysis, diagramming and repre-sentation.
International Journal on Artificial Intelli-gence Tools, 13(04):961?979.Chris Reed, Raquel Mochales Palau, Glenn Rowe, andMarie-Francine Moens.
2008.
Language resourcesfor studying argument.
In Proceedings of the 6thconference on language resources and evaluation-LREC 2008, pages 91?100.Rion Snow, Brendan O?Connor, Daniel Jurafsky, andAndrew Y Ng.
2008.
Cheap and fast?but is itgood?
: evaluating non-expert annotations for natu-ral language tasks.
In Proceedings of the conferenceon empirical methods in natural language process-ing, pages 254?263.
Association for ComputationalLinguistics.Swapna Somasundaran and Janyce Wiebe.
2010.
Rec-ognizing stances in ideological on-line debates.
InProceedings of the NAACL HLT 2010 Workshop onComputational Approaches to Analysis and Genera-tion of Emotion in Text, pages 116?124.
Associationfor Computational Linguistics.Swapna Somasundaran, Josef Ruppenhofer, and JanyceWiebe.
2008.
Discourse level opinion relations: Anannotation study.
In Proceedings of the 9th SIGdialWorkshop on Discourse and Dialogue, pages 129?137.
Association for Computational Linguistics.Simone Teufel, Advaith Siddharthan, and Colin Batch-elor.
2009.
Towards discipline-independent ar-gumentative zoning: evidence from chemistry andcomputational linguistics.
In Proceedings of the2009 Conference on Empirical Methods in NaturalLanguage Processing: Volume 3-Volume 3, pages1493?1502.
Association for Computational Linguis-tics.Frans H Van Eemeren, Rob Grootendorst, Sally Jack-son, and Scott Jacobs.
1993.
Reconstructing argu-mentative discourse.
University of Alabama Press.Marilyn A Walker, Pranav Anand, Rob Abbott, JeanE Fox Tree, Craig Martell, and Joseph King.
2012.That is your evidence?
: Classifying stance in on-line political debate.
Decision Support Systems,53(4):719?729.Rui Wang and Chris Callison-Burch.
2010.
Cheapfacts and counter-facts.
In Proceedings of theNAACL HLT 2010 Workshop on Creating Speechand Language Data with Amazon?s MechanicalTurk, pages 163?167.
Association for Computa-tional Linguistics.Janyce Wiebe, Theresa Wilson, and Claire Cardie.2005.
Annotating expressions of opinions and emo-tions in language.
Language resources and evalua-tion, 39(2-3):165?210.Adam Wyner, Raquel Mochales-Palau, Marie-FrancineMoens, and David Milward.
2010.
Approaches totext mining arguments from legal cases.
In Semanticprocessing of legal texts, pages 60?79.
Springer.Adam Wyner, Jodi Schneider, Katie Atkinson, andTrevor JM Bench-Capon.
2012.
Semi-automatedargumentative analysis of online product reviews.
InCOMMA, pages 43?50.Jie Yin, Paul Thomas, Nalin Narang, and Cecile Paris.2012.
Unifying local and global agreement anddisagreement classification in online debates.
InProceedings of the 3rd Workshop in ComputationalApproaches to Subjectivity and Sentiment Analysis,pages 61?69.
Association for Computational Lin-guistics.48
