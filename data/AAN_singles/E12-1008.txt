Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 66?76,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsDependency Parsing with Undirected GraphsCarlos Go?mez-Rodr?
?guezDepartamento de Computacio?nUniversidade da Corun?aCampus de Elvin?a, 15071A Corun?a, Spaincarlos.gomez@udc.esDaniel Ferna?ndez-Gonza?lezDepartamento de Informa?ticaUniversidade de VigoCampus As Lagoas, 32004Ourense, Spaindanifg@uvigo.esAbstractWe introduce a new approach to transition-based dependency parsing in which theparser does not directly construct a depen-dency structure, but rather an undirectedgraph, which is then converted into a di-rected dependency tree in a post-processingstep.
This alleviates error propagation,since undirected parsers do not need to ob-serve the single-head constraint.Undirected parsers can be obtained by sim-plifying existing transition-based parserssatisfying certain conditions.
We apply thisapproach to obtain undirected variants ofthe planar and 2-planar parsers and of Cov-ington?s non-projective parser.
We performexperiments on several datasets from theCoNLL-X shared task, showing that thesevariants outperform the original directed al-gorithms in most of the cases.1 IntroductionDependency parsing has proven to be very use-ful for natural language processing tasks.
Data-driven dependency parsers such as those by Nivreet al(2004), McDonald et al(2005), Titov andHenderson (2007), Martins et al(2009) or Huangand Sagae (2010) are accurate and efficient, theycan be trained from annotated data without theneed for a grammar, and they provide a simplerepresentation of syntax that maps to predicate-argument structure in a straightforward way.In particular, transition-based dependencyparsers (Nivre, 2008) are a type of dependencyparsing algorithms which use a model that scorestransitions between parser states.
Greedy deter-ministic search can be used to select the transitionto be taken at each state, thus achieving linear orquadratic time complexity.0          1          2          3Figure 1: An example dependency structure wheretransition-based parsers enforcing the single-head con-straint will incur in error propagation if they mistak-enly build a dependency link 1 ?
2 instead of 2 ?
1(dependency links are represented as arrows goingfrom head to dependent).It has been shown by McDonald and Nivre(2007) that such parsers suffer from error prop-agation: an early erroneous choice can place theparser in an incorrect state that will in turn lead tomore errors.
For instance, suppose that a sentencewhose correct analysis is the dependency graphin Figure 1 is analyzed by any bottom-up or left-to-right transition-based parser that outputs de-pendency trees, therefore obeying the single-headconstraint (only one incoming arc is allowed pernode).
If the parser chooses an erroneous transi-tion that leads it to build a dependency link from1 to 2 instead of the correct link from 2 to 1, thiswill lead it to a state where the single-head con-straint makes it illegal to create the link from 3 to2.
Therefore, a single erroneous choice will causetwo attachment errors in the output tree.With the goal of minimizing these sources oferrors, we obtain novel undirected variants ofseveral parsers; namely, of the planar and 2-planar parsers by Go?mez-Rodr?
?guez and Nivre(2010) and the non-projective list-based parserdescribed by Nivre (2008), which is based onCovington?s algorithm (Covington, 2001).
Thesevariants work by collapsing the LEFT-ARC and66RIGHT-ARC transitions in the original parsers,which create right-to-left and left-to-right depen-dency links, into a single ARC transition creatingan undirected link.
This has the advantage thatthe single-head constraint need not be observedduring the parsing process, since the directed no-tions of head and dependent are lost in undirectedgraphs.
This gives the parser more freedom andcan prevent situations where enforcing the con-straint leads to error propagation, as in Figure 1.On the other hand, these new algorithms havethe disadvantage that their output is an undirectedgraph, which has to be post-processed to recoverthe direction of the dependency links and generatea valid dependency tree.
Thus, some complexityis moved from the parsing process to this post-processing step; and each undirected parser willoutperform the directed version only if the simpli-fication of the parsing phase is able to avoid moreerrors than are generated by the post-processing.As will be seen in latter sections, experimental re-sults indicate that this is in fact the case.The rest of this paper is organized as follows:Section 2 introduces some notation and conceptsthat we will use throughout the paper.
In Sec-tion 3, we present the undirected versions of theparsers by Go?mez-Rodr?
?guez and Nivre (2010)and Nivre (2008), as well as some considerationsabout the feature models suitable to train them.
InSection 4, we discuss post-processing techniquesthat can be used to recover dependency trees fromundirected graphs.
Section 5 presents an empir-ical study of the performance obtained by theseparsers, and Section 6 contains a final discussion.2 Preliminaries2.1 Dependency GraphsLet w = w1 .
.
.
wn be an input string.
A de-pendency graph for w is a directed graph G =(Vw, E), where Vw = {0, .
.
.
, n} is the set ofnodes, and E ?
Vw ?
Vw is the set of directedarcs.
Each node in Vw encodes the position ofa token in w, and each arc in E encodes a de-pendency relation between two tokens.
We writei ?
j to denote a directed arc (i, j), which willalso be called a dependency link from i to j.1 We1In practice, dependency links are usually labeled, butto simplify the presentation we will ignore labels throughoutmost of the paper.
However, all the results and algorithmspresented can be applied to labeled dependency graphs andwill be so applied in the experimental evaluation.say that i is the head of j and, conversely, that jis a syntactic dependent of i.Given a dependency graph G = (Vw, E), wewrite i ??
j ?
E if there is a (possibly empty)directed path from i to j; and i ??
j ?
E ifthere is a (possibly empty) path between i and j inthe undirected graph underlying G (omitting thereferences to E when clear from the context).Most dependency-based representations of syn-tax do not allow arbitrary dependency graphs, in-stead, they are restricted to acyclic graphs thathave at most one head per node.
Dependencygraphs satisfying these constraints are called de-pendency forests.Definition 1 A dependency graph G is said to bea forest iff it satisfies:1.
Acyclicity constraint: if i ??
j, then notj ?
i.2.
Single-head constraint: if j ?
i, then thereis no k 6= j such that k ?
i.A node that has no head in a dependency for-est is called a root.
Some dependency frame-works add the additional constraint that depen-dency forests have only one root (or, equivalently,that they are connected).
Such a forest is called adependency tree.
A dependency tree can be ob-tained from any dependency forest by linking allof its root nodes as dependents of a dummy rootnode, conventionally located in position 0 of theinput.2.2 Transition SystemsIn the framework of Nivre (2008), transition-based parsers are described by means of a non-deterministic state machine called a transitionsystem.Definition 2 A transition system for dependencyparsing is a tuple S = (C, T, cs, Ct), where1.
C is a set of possible parser configurations,2.
T is a finite set of transitions, which are par-tial functions t : C ?
C,3.
cs is a total initialization function mappingeach input string to a unique initial configu-ration, and4.
Ct ?
C is a set of terminal configurations.To obtain a deterministic parser from a non-deterministic transition system, an oracle is usedto deterministically select a single transition at67each configuration.
An oracle for a transition sys-tem S = (C, T, cs, Ct) is a function o : C ?
T .Suitable oracles can be obtained in practice bytraining classifiers on treebank data (Nivre et al2004).2.3 The Planar, 2-Planar and CovingtonTransition SystemsOur undirected dependency parsers are basedon the planar and 2-planar transition systemsby Go?mez-Rodr?
?guez and Nivre (2010) and theversion of the Covington (2001) non-projectiveparser defined by Nivre (2008).
We now outlinethese directed parsers briefly, a more detailed de-scription can be found in the above references.2.3.1 PlanarThe planar transition system by Go?mez-Rodr?
?guez and Nivre (2010) is a linear-timetransition-based parser for planar dependencyforests, i.e., forests whose dependency arcs do notcross when drawn above the words.
The set ofplanar dependency structures is a very mild ex-tension of that of projective structures (Kuhlmannand Nivre, 2006).Configurations in this system are of the formc = ?
?, B,A?
where ?
and B are disjoint lists ofnodes from Vw (for some input w), and A is a setof dependency links over Vw.
The list B, calledthe buffer, holds the input words that are still tobe read.
The list ?, called the stack, is initiallyempty and is used to hold words that have depen-dency links pending to be created.
The systemis shown at the top in Figure 2, where the nota-tion ?
| i is used for a stack with top i and tail ?,and we invert the notation for the buffer for clarity(i.e., i | B as a buffer with top i and tail B).The system reads the input sentence and createslinks in a left-to-right order by executing its fourtransitions, until it gets to a terminal configura-tion.
A SHIFT transition moves the first (leftmost)node in the buffer to the top of the stack.
Transi-tions LEFT-ARC and RIGHT-ARC create leftwardor rightward link, respectively, involving the firstnode in the buffer and the topmost node in thestack.
Finally, REDUCE transition is used to popthe top word from the stack when we have fin-ished building arcs to or from it.2.3.2 2-PlanarThe 2-planar transition system by Go?mez-Rodr?
?guez and Nivre (2010) is an extension ofthe planar system that uses two stacks, allowingit to recognize 2-planar structures, a larger setof dependency structures that has been shown tocover the vast majority of non-projective struc-tures in a number of treebanks (Go?mez-Rodr?
?guezand Nivre, 2010).This transition system, shown in Figure 2, hasconfigurations of the form c = ?
?0,?1, B,A?
,where we call ?0 the active stack and ?1 the in-active stack.
Its SHIFT, LEFT-ARC, RIGHT-ARCand REDUCE transitions work similarly to thosein the planar parser, but while SHIFT pushes thefirst word in the buffer to both stacks; the otherthree transitions only work with the top of the ac-tive stack, ignoring the inactive one.
Finally, aSWITCH transition is added that makes the activestack inactive and vice versa.2.3.3 Covington Non-ProjectiveCovington (2001) proposes several incremen-tal parsing strategies for dependency representa-tions and one of them can recover non-projectivedependency graphs.
Nivre (2008) implements avariant of this strategy as a transition system withconfigurations of the form c = ?
?1, ?2, B,A?,where ?1 and ?2 are lists containing partially pro-cessed words and B is the buffer list of unpro-cessed words.The Covington non-projective transition sys-tem is shown at the bottom in Figure 2.
At eachconfiguration c = ?
?1, ?2, B,A?, the parser hasto consider whether any dependency arc shouldbe created involving the top of the buffer and thewords in ?1.
A LEFT-ARC transition adds a linkfrom the first node j in the buffer to the node in thehead of the list ?1, which is moved to the list ?2to signify that we have finished considering it as apossible head or dependent of j.
The RIGHT-ARCtransition does the same manipulation, but creat-ing the symmetric link.
A NO-ARC transition re-moves the head of the list ?1 and inserts it at thehead of the list ?2 without creating any arcs: thistransition is to be used where there is no depen-dency relation between the top node in the bufferand the head of ?1, but we still may want to cre-ate an arc involving the top of the buffer and othernodes in ?1.
Finally, if we do not want to createany such arcs at all, we can execute a SHIFT tran-sition, which advances the parsing process by re-moving the first node in the bufferB and insertingit at the head of a list obtained by concatenating68?1 and ?2.
This list becomes the new ?1, whereas?2 is empty in the resulting configuration.Note that the Covington parser has quadraticcomplexity with respect to input length, while theplanar and 2-planar parsers run in linear time.3 The Undirected ParsersThe transition systems defined in Section 2.3share the common property that their LEFT-ARCand RIGHT-ARC have exactly the same effects ex-cept for the direction of the links that they create.We can take advantage of this property to defineundirected versions of these transition systems, bytransforming them as follows:?
Configurations are changed so that the arc setA is a set of undirected arcs, instead of di-rected arcs.?
The LEFT-ARC and RIGHT-ARC transitionsin each parser are collapsed into a single ARCtransition that creates an undirected arc.?
The preconditions of transitions that guaran-tee the single-head constraint are removed,since the notions of head and dependent arelost in undirected graphs.By performing these transformations and leavingthe systems otherwise unchanged, we obtain theundirected variants of the planar, 2-planar andCovington algorithms that are shown in Figure 3.Note that the transformation can be appliedto any transition system having LEFT-ARC andRIGHT-ARC transitions that are equal except forthe direction of the created link, and thus col-lapsable into one.
The above three transition sys-tems fulfill this property, but not every transitionsystem does.
For example, the well-known arc-eager parser of Nivre (2003) pops a node from thestack when creating left arcs, and pushes a nodeto the stack when creating right arcs, so the trans-formation cannot be applied to it.22One might think that the arc-eager algorithm could stillbe transformed by converting each of its arc transitions intoan undirected transition, without collapsing them into one.However, this would result into a parser that violates theacyclicity constraint, since the algorithm is designed in sucha way that acyclicity is only guaranteed if the single-headconstraint is kept.
It is easy to see that this problem cannothappen in parsers where LEFT-ARC and RIGHT-ARC transi-tions have the same effect: in these, if a directed graph is notparsable in the original algorithm, its underlying undirectedgraph cannot not be parsable in the undirected variant.3.1 Feature modelsSome of the features that are typically used totrain transition-based dependency parsers dependon the direction of the arcs that have been built upto a certain point.
For example, two such featuresfor the planar parser could be the POS tag associ-ated with the head of the topmost stack node, orthe label of the arc going from the first node in thebuffer to its leftmost dependent.3As the notion of head and dependent is lost inundirected graphs, this kind of features cannot beused to train undirected parsers.
Instead, we usefeatures based on undirected relations betweennodes.
We found that the following kinds of fea-tures worked well in practice as a replacement forfeatures depending on arc direction:?
Information about the ith node linked to agiven node (topmost stack node, topmostbuffer node, etc.)
on the left or on the right,and about the associated undirected arc, typi-cally for i = 1, 2, 3,?
Information about whether two nodes arelinked or not in the undirected graph, andabout the label of the arc between them,?
Information about the first left and right?undirected siblings?
of a given node, i.e., thefirst node q located to the left of the given nodep such that p and q are linked to some commonnode r located to the right of both, and viceversa.
Note that this notion of undirected sib-lings does not correspond exclusively to sib-lings in the directed graph, since it can alsocapture other second-order interactions, suchas grandparents.4 Reconstructing the dependency forestThe modified transition systems presented in theprevious section generate undirected graphs.
Toobtain complete dependency parsers that are ableto produce directed dependency forests, we willneed a reconstruction step that will assign a direc-tion to the arcs in such a way that the single-headconstraint is obeyed.
This reconstruction step canbe implemented by building a directed graph withweighted arcs corresponding to both possible di-rections of each undirected edge, and then findingan optimum branching to reduce it to a directed3These example features are taken from the default modelfor the planar parser in version 1.5 of MaltParser (Nivre etal., 2006).69Planar initial/terminal configurations: cs(w1 .
.
.
wn) = ?
[], [1 .
.
.
n], ?
?, Cf = {?
?, [], A?
?
C}Transitions: SHIFT ?
?, i|B,A?
?
?
?|i, B,A?REDUCE ?
?|i, B,A?
?
?
?, B,A?LEFT-ARC ?
?|i, j|B,A?
?
?
?|i, j|B,A ?
{(j, i)}?only if @k | (k, i) ?
A (single-head) and i??
j 6?
A (acyclicity).RIGHT-ARC ?
?|i, j|B,A?
?
?
?|i, j|B,A ?
{(i, j)}?only if @k | (k, j) ?
A (single-head) and i??
j 6?
A (acyclicity).2-Planar initial/terminal configurations: cs(w1 .
.
.
wn) = ?
[], [], [1 .
.
.
n], ?
?, Cf = {?
?0,?1, [], A?
?
C}Transitions: SHIFT ?
?0,?1, i|B,A?
?
?
?0|i,?1|i, B,A?REDUCE ?
?0|i,?1, B,A?
?
?
?0,?1, B,A?LEFT-ARC ?
?0|i,?1, j|B,A?
?
?
?0|i,?1, j|B,A ?
{j, i)}?only if @k | (k, i) ?
A (single-head) and i??
j 6?
A (acyclicity).RIGHT-ARC ?
?0|i,?1, j|B,A?
?
?
?0|i,?1, j|B,A ?
{(i, j)}?only if @k | (k, j) ?
A (single-head) and i??
j 6?
A (acyclicity).SWITCH ?
?0,?1, B,A?
?
?
?1,?0, B,A?Covington initial/term.
configurations: cs(w1 .
.
.
wn) = ?
[], [], [1 .
.
.
n], ?
?, Cf = {?
?1, ?2, [], A?
?
C}Transitions: SHIFT ?
?1, ?2, i|B,A?
?
?
?1 ?
?2|i, [], B,A?NO-ARC ?
?1|i, ?2, B,A?
?
?
?1, i|?2, B,A?LEFT-ARC ?
?1|i, ?2, j|B,A?
?
?
?1, i|?2, j|B,A ?
{(j, i)}?only if @k | (k, i) ?
A (single-head) and i??
j 6?
A (acyclicity).RIGHT-ARC ?
?1|i, ?2, j|B,A?
?
?
?1, i|?2, j|B,A ?
{(i, j)}?only if @k | (k, j) ?
A (single-head) and i??
j 6?
A (acyclicity).Figure 2: Transition systems for planar, 2-planar and Covington non-projective dependency parsing.Undirected Planar initial/term.
conf.
: cs(w1 .
.
.
wn) = ?
[], [1 .
.
.
n], ?
?, Cf = {?
?, [], A?
?
C}Transitions: SHIFT ?
?, i|B,A?
?
?
?|i, B,A?REDUCE ?
?|i, B,A?
?
?
?, B,A?ARC ?
?|i, j|B,A?
?
?
?|i, j|B,A ?
{{i, j}}?only if i??
j 6?
A (acyclicity).Undirected 2-Planar initial/term.
conf.
: cs(w1 .
.
.
wn) = ?
[], [], [1 .
.
.
n], ?
?, Cf = {?
?0,?1, [], A?
?
C}Transitions: SHIFT ?
?0,?1, i|B,A?
?
?
?0|i,?1|i, B,A?REDUCE ?
?0|i,?1, B,A?
?
?
?0,?1, B,A?ARC ?
?0|i,?1, j|B,A?
?
?
?0|i,?1, j|B,A ?
{{i, j}}?only if i??
j 6?
A (acyclicity).SWITCH ?
?0,?1, B,A?
?
?
?1,?0, B,A?Undirected Covington init./term.
conf.
: cs(w1 .
.
.
wn) = ?
[], [], [1 .
.
.
n], ?
?, Cf = {?
?1, ?2, [], A?
?
C}Transitions: SHIFT ?
?1, ?2, i|B,A?
?
?
?1 ?
?2|i, [], B,A?NO-ARC ?
?1|i, ?2, B,A?
?
?
?1, i|?2, B,A?ARC ?
?1|i, ?2, j|B,A?
?
?
?1, i|?2, j|B,A ?
{{i, j}}?only if i??
j 6?
A (acyclicity).Figure 3: Transition systems for undirected planar, 2-planar and Covington non-projective dependency parsing.70tree.
Different criteria for assigning weights toarcs provide different variants of the reconstruc-tion technique.To describe these variants, we first introducepreliminary definitions.
Let U = (Vw, E) bean undirected graph produced by an undirectedparser for some string w. We define the follow-ing sets of arcs:A1(U) = {(i, j) | j 6= 0 ?
{i, j} ?
E},A2(U) = {(0, i) | i ?
Vw}.Note that A1(U) represents the set of arcs ob-tained from assigning an orientation to an edgein U , except arcs whose dependent is the dummyroot, which are disallowed.
On the other hand,A2(U) contains all the possible arcs originatingfrom the dummy root node, regardless of whethertheir underlying undirected edges are in U or not;this is so that reconstructions are allowed to linkunattached tokens to the dummy root.The reconstruction process consists of findinga minimum branching (i.e.
a directed minimumspanning tree) for a weighted directed graph ob-tained from assigning a cost c(i, j) to each arc(i, j) of the following directed graph:D(U) = {Vw, A(U) = A1(U) ?A2(U)}.That is, we will find a dependency tree T =(Vw, AT ?
A(U)) such that the sum of costs ofthe arcs in AT is minimal.
In general, such a min-imum branching can be calculated with the Chu-Liu-Edmonds algorithm (Chu and Liu, 1965; Ed-monds, 1967).
Since the graph D(U) has O(n)nodes and O(n) arcs for a string of length n, thiscan be done in O(n log n) if implemented as de-scribed by Tarjan (1977).However, applying these generic techniques isnot necessary in this case: since our graph U isacyclic, the problem of reconstructing the forestcan be reduced to choosing a root word for eachconnected component in the graph, linking it asa dependent of the dummy root and directing theother arcs in the component in the (unique) waythat makes them point away from the root.It remains to see how to assign the costs c(i, j)to the arcs of D(U): different criteria for assign-ing scores will lead to different reconstructions.4.1 Naive reconstructionA first, very simple reconstruction technique canbe obtained by assigning arc costs to the arcs inA(U) as follows:c(i, j){1 if (i, j) ?
A1(U),2 if (i, j) ?
A2(U) ?
(i, j) 6?
A1(U).This approach gives the same cost to all arcsobtained from the undirected graph U , while alsoallowing (at a higher cost) to attach any node tothe dummy root.
To obtain satisfactory resultswith this technique, we must train our parser toexplicitly build undirected arcs from the dummyroot node to the root word(s) of each sentence us-ing arc transitions (note that this implies that weneed to represent forests as trees, in the mannerdescribed at the end of Section 2.1).
Under thisassumption, it is easy to see that we can obtain thecorrect directed tree T for a sentence if it is pro-vided with its underlying undirected tree U : thetree is obtained in O(n) as the unique orientationof U that makes each of its edges point away fromthe dummy root.This approach to reconstruction has the advan-tage of being very simple and not adding any com-plications to the parsing process, while guarantee-ing that the correct directed tree will be recoveredif the undirected tree for a sentence is generatedcorrectly.
However, it is not very robust, since thedirection of all the arcs in the output depends onwhich node is chosen as sentence head and linkedto the dummy root.
Therefore, a parsing error af-fecting the undirected edge involving the dummyroot may result in many dependency links beingerroneous.4.2 Label-based reconstructionTo achieve a more robust reconstruction, we uselabels to encode a preferred direction for depen-dency arcs.
To do so, for each pre-existing labelX in the training set, we create two labels Xl andXr.
The parser is then trained on a modified ver-sion of the training set where leftward links orig-inally labelled X are labelled Xl, and rightwardlinks originally labelled X are labelled Xr.
Thus,the output of the parser on a new sentence will bean undirected graph where each edge has a labelwith an annotation indicating whether the recon-struction process should prefer to link the pair ofnodes with a leftward or a rightward arc.
We canthen assign costs to our minimum branching algo-rithm so that it will return a tree agreeing with asmany such annotations as possible.71To do this, we call A1+(U) ?
A1(U) the setof arcs in A1(U) that agree with the annotations,i.e., arcs (i, j) ?
A1(U) where either i < j andi, j is labelledXr inU , or i > j and i, j is labelledXl in U .
We callA1?
(U) the set of arcs inA1(U)that disagree with the annotations, i.e.,A1?
(U) =A1(U)\A1+(U).
And we assign costs as follows:c(i, j)??
?1 if (i, j) ?
A1+(U),2 if (i, j) ?
A1?
(U),2n if (i, j) ?
A2(U) ?
(i, j) 6?
A1(U).where n is the length of the string.With these costs, the minimum branching algo-rithm will find a tree which agrees with as manyannotations as possible.
Additional arcs from theroot not corresponding to any edge in the outputof the parser (i.e.
arcs inA2(U) but not inA1(U))will be used only if strictly necessary to guaranteeconnectedness, this is implemented by the highcost for these arcs.While this may be the simplest cost assignmentto implement label-based reconstruction, we havefound that better empirical results are obtained ifwe give the algorithm more freedom to create newarcs from the root, as follows:c(i, j)??
?1 if (i, j) ?
A1+(U) ?
(i, j) 6?
A2(U),2 if (i, j) ?
A1?
(U) ?
(i, j) 6?
A2(U),2n if (i, j) ?
A2(U).While the cost of arcs from the dummy root isstill 2n, this is now so even for arcs that are in theoutput of the undirected parser, which had cost 1before.
Informally, this means that with this con-figuration the postprocessor does not ?trust?
thelinks from the dummy root created by the parser,and may choose to change them if it is conve-nient to get a better agreement with label anno-tations (see Figure 4 for an example of the dif-ference between both cost assignments).
We be-lieve that the better accuracy obtained with thiscriterion probably stems from the fact that it is bi-ased towards changing links from the root, whichtend to be more problematic for transition-basedparsers, while respecting the parser output forlinks located deeper in the dependency structure,for which transition-based parsers tend to be moreaccurate (McDonald and Nivre, 2007).Note that both variants of label-based recon-struction have the property that, if the undirectedparser produces the correct edges and labels for a0        1       2        3       4        5RR L L L0        1       2        3       4        50        1       2        3       4        5a.b.c.Figure 4: a) An undirected graph obtained by theparser with the label-based transformation, b) and c)The dependency graph obtained by each of the variantsof the label-based reconstruction (note how the secondvariant moves an arc from the root).given sentence, then the obtained directed tree isguaranteed to be correct (as it will simply be thetree obtained by decoding the label annotations).5 ExperimentsIn this section, we evaluate the performance of theundirected planar, 2-planar and Covington parserson eight datasets from the CoNLL-X shared task(Buchholz and Marsi, 2006).Tables 1, 2 and 3 compare the accuracy of theundirected versions with naive and label-based re-construction to that of the directed versions ofthe planar, 2-planar and Covington parsers, re-spectively.
In addition, we provide a comparisonto well-known state-of-the-art projective and non-projective parsers: the planar parsers are com-pared to the arc-eager projective parser by Nivre(2003), which is also restricted to planar struc-tures; and the 2-planar parsers are compared withthe arc-eager parser with pseudo-projective trans-formation of Nivre and Nilsson (2005), capable ofhandling non-planar dependencies.We use SVM classifiers from the LIBSVMpackage (Chang and Lin, 2001) for all the lan-guages except Chinese, Czech and German.
Inthese, we use the LIBLINEAR package (Fan etal., 2008) for classification, which reduces train-ing time for these larger datasets; and featuremodels adapted to this system which, in the caseof German, result in higher accuracy than pub-lished results using LIBSVM.72The LIBSVM feature models for the arc-eagerprojective and pseudo-projective parsers are thesame used by these parsers in the CoNLL-Xshared task, where the pseudo-projective versionof MaltParser was one of the two top performingsystems (Buchholz and Marsi, 2006).
For the 2-planar parser, we took the feature models fromGo?mez-Rodr?
?guez and Nivre (2010) for the lan-guages included in that paper.
For all the algo-rithms and datasets, the feature models used forthe undirected parsers were adapted from those ofthe directed parsers as described in Section 3.1.4The results show that the use of undirectedparsing with label-based reconstruction clearlyimproves the performance in the vast majority ofthe datasets for the planar and Covington algo-rithms, where in many cases it also improves uponthe corresponding projective and non-projectivestate-of-the-art parsers provided for comparison.In the case of the 2-planar parser the results areless conclusive, with improvements over the di-rected versions in five out of the eight languages.The improvements in LAS obtained with label-based reconstruction over directed parsing are sta-tistically significant at the .05 level5 for Danish,German and Portuguese in the case of the pla-nar parser; and Czech, Danish and Turkish forCovington?s parser.
No statistically significant de-crease in accuracy was detected in any of the al-gorithm/dataset combinations.As expected, the good results obtained by theundirected parsers with label-based reconstruc-tion contrast with those obtained by the variantswith root-based reconstruction, which performedworse in all the experiments.6 DiscussionWe have presented novel variants of the planarand 2-planar transition-based parsers by Go?mez-Rodr?
?guez and Nivre (2010) and of Covington?snon-projective parser (Covington, 2001; Nivre,2008) which ignore the direction of dependencylinks, and reconstruction techniques that can beused to recover the direction of the arcs thus pro-duced.
The results obtained show that this ideaof undirected parsing, together with the label-4All the experimental settings and feature models usedare included in the supplementary material and also availableat http://www.grupolys.org/?cgomezr/exp/.5Statistical significance was assessed using Dan Bikel?srandomized comparator: http://www.cis.upenn.edu/?dbikel/software.htmlbased reconstruction technique of Section 4.2, im-proves parsing accuracy on most of the testeddataset/algorithm combinations, and it can out-perform state-of-the-art transition-based parsers.The accuracy improvements achieved by re-laxing the single-head constraint to mitigate er-ror propagation were able to overcome the er-rors generated in the reconstruction phase, whichwere few: we observed empirically that the dif-ferences between the undirected LAS obtainedfrom the undirected graph before the reconstruc-tion and the final directed LAS are typically be-low 0.20%.
This is true both for the naive andlabel-based transformations, indicating that bothtechniques are able to recover arc directions accu-rately, and the accuracy differences between themcome mainly from the differences in training (e.g.having tentative arc direction as part of featureinformation in the label-based reconstruction andnot in the naive one) rather than from the differ-ences in the reconstruction methods themselves.The reason why we can apply the undirectedsimplification to the three parsers that we haveused in this paper is that their LEFT-ARC andRIGHT-ARC transitions have the same effect ex-cept for the direction of the links they create.The same transformation and reconstruction tech-niques could be applied to any other transition-based dependency parsers sharing this property.The reconstruction techniques alone could po-tentially be applied to any dependency parser(transition-based or not) as long as it can be some-how converted to output undirected graphs.The idea of parsing with undirected relationsbetween words has been applied before in thework on Link Grammar (Sleator and Temperley,1991), but in that case the formalism itself workswith undirected graphs, which are the final out-put of the parser.
To our knowledge, the idea ofusing an undirected graph as an intermediate steptowards obtaining a dependency structure has notbeen explored before.AcknowledgmentsThis research has been partially funded by the SpanishMinistry of Economy and Competitiveness and FEDER(projects TIN2010-18552-C03-01 and TIN2010-18552-C03-02), Ministry of Education (FPU Grant Program) andXunta de Galicia (Rede Galega de Recursos Lingu??
?sticospara unha Soc.
do Con?ec.).
The experiments were conductedwith the help of computing resources provided by the Su-percomputing Center of Galicia (CESGA).
We thank JoakimNivre for helpful input in the early stages of this work.73Planar UPlanarN UPlanarL MaltPLang.
LAS(p) UAS(p) LAS(p) UAS(p) LAS(p) UAS(p) LAS(p) UAS(p)Arabic 66.93 (67.34) 77.56 (77.22) 65.91 (66.33) 77.03 (76.75) 66.75 (67.19) 77.45 (77.22) 66.43 (66.74) 77.19 (76.83)Chinese 84.23 (84.20) 88.37 (88.33) 83.14 (83.10) 87.00 (86.95) 84.51* (84.50*) 88.37 (88.35*) 86.42 (86.39) 90.06 (90.02)Czech 77.24 (77.70) 83.46 (83.24) 75.08 (75.60) 81.14 (81.14) 77.60* (77.93*) 83.56* (83.41*) 77.24 (77.57) 83.40 (83.19)Danish 83.31 (82.60) 88.02 (86.64) 82.65 (82.45) 87.58 (86.67*) 83.87* (83.83*) 88.94* (88.17*) 83.31 (82.64) 88.30 (86.91)German 84.66 (83.60) 87.02 (85.67) 83.33 (82.77) 85.78 (84.93) 86.32* (85.67*) 88.62* (87.69*) 86.12 (85.48) 88.52 (87.58)Portug.
86.22 (83.82) 89.80 (86.88) 85.89 (83.82) 89.68 (87.06*) 86.52* (84.83*) 90.28* (88.03*) 86.60 (84.66) 90.20 (87.73)Swedish 83.01 (82.44) 88.53 (87.36) 81.20 (81.10) 86.50 (85.86) 82.95 (82.66*) 88.29 (87.45*) 82.89 (82.44) 88.61 (87.55)Turkish 62.70 (71.27) 73.67 (78.57) 59.83 (68.31) 70.15 (75.17) 63.27* (71.63*) 73.93* (78.72*) 62.58 (70.96) 73.09 (77.95)Table 1: Parsing accuracy of the undirected planar parser with naive (UPlanarN) and label-based (UPlanarL)postprocessing in comparison to the directed planar (Planar) and the MaltParser arc-eager projective (MaltP)algorithms, on eight datasets from the CoNLL-X shared task (Buchholz and Marsi, 2006): Arabic (Hajic?
et al2004), Chinese (Chen et al 2003), Czech (Hajic?
et al 2006), Danish (Kromann, 2003), German (Brants etal., 2002), Portuguese (Afonso et al 2002), Swedish (Nilsson et al 2005) and Turkish (Oflazer et al 2003;Atalay et al 2003).
We show labelled (LAS) and unlabelled (UAS) attachment score excluding and includingpunctuation tokens in the scoring (the latter in brackets).
Best results for each language are shown in boldface,and results where the undirected parser outperforms the directed version are marked with an asterisk.2Planar U2PlanarN U2PlanarL MaltPPLang.
LAS(p) UAS(p) LAS(p) UAS(p) LAS(p) UAS(p) LAS(p) UAS(p)Arabic 66.73 (67.19) 77.33 (77.11) 66.37 (66.93) 77.15 (77.09) 66.13 (66.52) 76.97 (76.70) 65.93 (66.02) 76.79 (76.14)Chinese 84.35 (84.32) 88.31 (88.27) 83.02 (82.98) 86.86 (86.81) 84.45* (84.42*) 88.29 (88.25) 86.42 (86.39) 90.06 (90.02)Czech 77.72 (77.91) 83.76 (83.32) 74.44 (75.19) 80.68 (80.80) 78.00* (78.59*) 84.22* (84.21*) 78.86 (78.47) 84.54 (83.89)Danish 83.81 (83.61) 88.50 (87.63) 82.00 (81.63) 86.87 (85.80) 83.75 (83.65*) 88.62* (87.82*) 83.67 (83.54) 88.52 (87.70)German 86.28 (85.76) 88.68 (87.86) 82.93 (82.53) 85.52 (84.81) 86.52* (85.99*) 88.72* (87.92*) 86.94 (86.62) 89.30 (88.69)Portug.
87.04 (84.92) 90.82 (88.14) 85.61 (83.45) 89.36 (86.65) 86.70 (84.75) 90.38 (87.88) 87.08 (84.90) 90.66 (87.95)Swedish 83.13 (82.71) 88.57 (87.59) 81.00 (80.71) 86.54 (85.68) 82.59 (82.25) 88.19 (87.29) 83.39 (82.67) 88.59 (87.38)Turkish 61.80 (70.09) 72.75 (77.39) 58.10 (67.44) 68.03 (74.06) 61.92* (70.64*) 72.18 (77.46*) 62.80 (71.33) 73.49 (78.44)Table 2: Parsing accuracy of the undirected 2-planar parser with naive (U2PlanarN) and label-based (U2PlanarL)postprocessing in comparison to the directed 2-planar (2Planar) and MaltParser arc-eager pseudo-projective(MaltPP) algorithms.
The meaning of the scores shown is as in Table 1.Covington UCovingtonN UCovingtonLLang.
LAS(p) UAS(p) LAS(p) UAS(p) LAS(p) UAS(p)Arabic 65.17 (65.49) 75.99 (75.69) 63.49 (63.93) 74.41 (74.20) 65.61* (65.81*) 76.11* (75.66)Chinese 85.61 (85.61) 89.64 (89.62) 84.12 (84.02) 87.85 (87.73) 86.28* (86.17*) 90.16* (90.04*)Czech 78.26 (77.43) 84.04 (83.15) 74.02 (74.78) 79.80 (79.92) 78.42* (78.69*) 84.50* (84.16*)Danish 83.63 (82.89) 88.50 (87.06) 82.00 (81.61) 86.55 (85.51) 84.27* (83.85*) 88.82* (87.75*)German 86.70 (85.69) 89.08 (87.78) 84.03 (83.51) 86.16 (85.39) 86.50 (85.90*) 88.84 (87.95*)Portug.
84.73 (82.56) 89.10 (86.30) 83.83 (81.71) 87.88 (85.17) 84.95* (82.70*) 89.18* (86.31*)Swedish 83.53 (82.76) 88.91 (87.61) 81.78 (81.47) 86.78 (85.96) 83.09 (82.73) 88.11 (87.23)Turkish 64.25 (72.70) 74.85 (79.75) 63.51 (72.08) 74.07 (79.10) 64.91* (73.38*) 75.46* (80.40*)Table 3: Parsing accuracy of the undirected Covington non-projective parser with naive (UCovingtonN) andlabel-based (UCovingtonL) postprocessing in comparison to the directed algorithm (Covington).
The meaningof the scores shown is as in Table 1.74ReferencesSusana Afonso, Eckhard Bick, Renato Haber, and Di-ana Santos.
2002.
?Floresta sinta?(c)tica?
: a tree-bank for Portuguese.
In Proceedings of the 3rd In-ternational Conference on Language Resources andEvaluation (LREC 2002), pages 1968?1703, Paris,France.
ELRA.Nart B. Atalay, Kemal Oflazer, and Bilge Say.
2003.The annotation process in the Turkish treebank.In Proceedings of EACL Workshop on Linguisti-cally Interpreted Corpora (LINC-03), pages 243?246, Morristown, NJ, USA.
Association for Com-putational Linguistics.Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-gang Lezius, and George Smith.
2002.
The tigertreebank.
In Proceedings of the Workshop on Tree-banks and Linguistic Theories, September 20-21,Sozopol, Bulgaria.Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-Xshared task on multilingual dependency parsing.
InProceedings of the 10th Conference on Computa-tional Natural Language Learning (CoNLL), pages149?164.Chih-Chung Chang and Chih-Jen Lin, 2001.LIBSVM: A Library for Support Vec-tor Machines.
Software available athttp://www.csie.ntu.edu.tw/?cjlin/libsvm.K.
Chen, C. Luo, M. Chang, F. Chen, C. Chen,C.
Huang, and Z. Gao.
2003.
Sinica treebank: De-sign criteria, representational issues and implemen-tation.
In Anne Abeille?, editor, Treebanks: Buildingand Using Parsed Corpora, chapter 13, pages 231?248.
Kluwer.Y.
J. Chu and T. H. Liu.
1965.
On the shortest arbores-cence of a directed graph.
Science Sinica, 14:1396?1400.Michael A. Covington.
2001.
A fundamental algo-rithm for dependency parsing.
In Proceedings ofthe 39th Annual ACM Southeast Conference, pages95?102.Jack Edmonds.
1967.
Optimum branchings.
Journalof Research of the National Bureau of Standards,71B:233?240.R.-E.
Fan, K.-W. Chang, C.-J.
Hsieh, X.-R. Wang, andC.-J.
Lin.
2008.
LIBLINEAR: A library for largelinear classification.
Journal of Machine LearningResearch, 9:1871?1874.Carlos Go?mez-Rodr?
?guez and Joakim Nivre.
2010.A transition-based parser for 2-planar dependencystructures.
In Proceedings of the 48th Annual Meet-ing of the Association for Computational Linguis-tics, ACL ?10, pages 1492?1501, Stroudsburg, PA,USA.
Association for Computational Linguistics.Jan Hajic?, Otakar Smrz?, Petr Zema?nek, Jan S?naidauf,and Emanuel Bes?ka.
2004.
Prague Arabic Depen-dency Treebank: Development in data and tools.
InProceedings of the NEMLAR International Confer-ence on Arabic Language Resources and Tools.Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, JarmilaPanevova?, Petr Sgall, Petr Pajas, Jan S?te?pa?nek,Jir???
Havelka, and Marie Mikulova?.
2006.Prague Dependency Treebank 2.0.
CDROM CAT:LDC2006T01, ISBN 1-58563-370-4.
LinguisticData Consortium.Liang Huang and Kenji Sagae.
2010.
Dynamic pro-gramming for linear-time incremental parsing.
InProceedings of the 48th Annual Meeting of the As-sociation for Computational Linguistics, ACL ?10,pages 1077?1086, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.Matthias T. Kromann.
2003.
The Danish dependencytreebank and the underlying linguistic theory.
InProceedings of the 2nd Workshop on Treebanks andLinguistic Theories (TLT), pages 217?220, Va?xjo?,Sweden.
Va?xjo?
University Press.Marco Kuhlmann and Joakim Nivre.
2006.
Mildlynon-projective dependency structures.
In Proceed-ings of the COLING/ACL 2006 Main ConferencePoster Sessions, pages 507?514.Andre Martins, Noah Smith, and Eric Xing.
2009.Concise integer linear programming formulationsfor dependency parsing.
In Proceedings of theJoint Conference of the 47th Annual Meeting of theACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP (ACL-IJCNLP), pages 342?350.Ryan McDonald and Joakim Nivre.
2007.
Character-izing the errors of data-driven dependency parsingmodels.
In Proceedings of the 2007 Joint Confer-ence on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning (EMNLP-CoNLL), pages 122?131.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic?.
2005.
Non-projective dependency pars-ing using spanning tree algorithms.
In Proceedingsof the Human Language Technology Conferenceand the Conference on Empirical Methods in Nat-ural Language Processing (HLT/EMNLP), pages523?530.Jens Nilsson, Johan Hall, and Joakim Nivre.
2005.MAMBA meets TIGER: Reconstructing a Swedishtreebank from Antiquity.
In Peter Juel Henrichsen,editor, Proceedings of the NODALIDA Special Ses-sion on Treebanks.Joakim Nivre and Jens Nilsson.
2005.
Pseudo-projective dependency parsing.
In Proceedings ofthe 43rd Annual Meeting of the Association forComputational Linguistics (ACL), pages 99?106.Joakim Nivre, Johan Hall, and Jens Nilsson.
2004.Memory-based dependency parsing.
In Proceed-ings of the 8th Conference on Computational Nat-ural Language Learning (CoNLL-2004), pages 49?56, Morristown, NJ, USA.
Association for Compu-tational Linguistics.75Joakim Nivre, Johan Hall, and Jens Nilsson.
2006.MaltParser: A data-driven parser-generator for de-pendency parsing.
In Proceedings of the 5th In-ternational Conference on Language Resources andEvaluation (LREC), pages 2216?2219.Joakim Nivre.
2003.
An efficient algorithm for pro-jective dependency parsing.
In Proceedings of the8th International Workshop on Parsing Technolo-gies (IWPT), pages 149?160.Joakim Nivre.
2008.
Algorithms for DeterministicIncremental Dependency Parsing.
ComputationalLinguistics, 34(4):513?553.Kemal Oflazer, Bilge Say, Dilek Zeynep Hakkani-Tu?r,and Go?khan Tu?r.
2003.
Building a Turkish tree-bank.
In Anne Abeille?, editor, Treebanks: Build-ing and Using Parsed Corpora, pages 261?277.Kluwer.Daniel Sleator and Davy Temperley.
1991.
Pars-ing English with a link grammar.
Technical Re-port CMU-CS-91-196, Carnegie Mellon University,Computer Science.R.
E. Tarjan.
1977.
Finding optimum branchings.Networks, 7:25?35.Ivan Titov and James Henderson.
2007.
A latent vari-able model for generative dependency parsing.
InProceedings of the 10th International Conferenceon Parsing Technologies (IWPT), pages 144?155.76
