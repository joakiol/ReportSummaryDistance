Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 10?20,ACL HLT 2011, Portland, Oregon, USA, June 2011. c?2011 Association for Computational LinguisticsStructured vs. Flat Semantic Role Representationsfor Machine Translation EvaluationChi-kiu Lo and DekaiWuHKUSTHuman Language Technology CenterDept.
of Computer Science and EngineeringHong Kong University of Science and Technology{jackielo|dekai}@cs.ust.hkAbstractWe argue that failing to capture the degree ofcontribution of each semantic frame in a sen-tence explains puzzling results in recent workon the MEANT family of semantic MT eval-uation metrics, which have disturbingly in-dicated that dissociating semantic roles andfillers from their predicates actually improvescorrelation with human adequacy judgmentseven though, intuitively, properly segregat-ing event frames should more accurately re-flect the preservation of meaning.
Our anal-ysis finds that both properly structured andflattened representations fail to adequately ac-count for the contribution of each seman-tic frame to the overall sentence.
We thenshow that the correlation of HMEANT, the hu-man variant of MEANT, can be greatly im-proved by introducing a simple length-basedweighting scheme that approximates the de-gree of contribution of each semantic frameto the overall sentence.
The new resultsalso show that, without flattening the struc-ture of semantic frames, weighting the degreeof each frame?s contribution gives HMEANThigher correlations than the previously best-performing flattened model, as well as HTER.1 IntroductionIn this paper we provide a more concrete answerto the question: what would be a better represen-tation, structured or flat, of the roles in semanticframes to be used in a semantic machine transla-tion (MT) evaluation metric?
We compare recentstudies on the MEANT family of semantic role la-beling (SRL) based MT evaluation metrics (Lo andWu, 2010a,b, 2011a,b) by (1) contrasting their vari-ations in semantic role representation and observingdisturbing comparative results indicating that segre-gating the event frames in structured role representa-tion actually damages correlation against human ad-equacy judgments and (2) showing how SRL basedMT evaluation can be improved beyond the currentstate-of-the-art compared to previous MEANT vari-ants as well as HTER, through the introduction ofa simple weighting scheme that reflects the degreeof contribution of each semantic frame to the overallsentence.
The weighting scheme we propose usesa simple length-based heuristic that reflects the as-sumption that a semantic frame that covers more to-kens contributes more to the overall sentence transla-tion.
We demonstrate empirically that when the de-gree of each frame?s contribution to its sentence istaken into account, the properly structured role rep-resentation is more accurate and intuitive than theflattened role representation for SRL MT evaluationmetrics.For years, the task of measuring the performanceof MT systems has been dominated by lexical n-gram based machine translation evaluation met-rics, such as BLEU (Papineni et al, 2002), NIST(Doddington, 2002), METEOR (Banerjee and Lavie,2005), PER (Tillmann et al, 1997), CDER (Leuschet al, 2006) and WER (Nie?en et al, 2000).
Thesemetrics are excellent at ranking overall systems byaveraging their scores over entire documents.
How-ever, as MT systems improve, the shortcomings ofsuch metrics are becoming more apparent.
Thoughcontaining roughly the correct words, MT output atthe sentence remains often quite incomprehensible,and fails to preserve the meaning of the input.
Thisresults from the fact that n-gram based metrics arenot as reliable at ranking the adequacy of transla-tions of individual sentences, and are particularly10poor at reflecting translation quality improvementsinvolving more meaningful word sense or semanticframe decisions?which human judges have no trou-ble distinguishing.
Callison-Burch et al (2006) andKoehn and Monz (2006), for example, study situ-ations where BLEU strongly disagrees with humanjudgment of translation quality.Newer avenues of research seek substitutes forn-gram based MT evaluation metrics that are bet-ter at evaluating translation adequacy, particularly atthe sentence level.
One line of research emphasizesmore the structural correctness of translation.
Liuand Gildea (2005) propose STM, a metric based onsyntactic structure, that addresses the failure of lex-ical similarity based metrics to evaluate translationgrammaticality.
However, the problem remains thata grammatical translation can achieve a high syntax-based score yet still make significant errors arisingfrom confusion of semantic roles.
On the other hand,despite the fact that non-automatic, manually evalu-ated metrics, such as HTER (Snover et al, 2006), aremore adequacy oriented exhibit much higher correla-tion with human adequacy judgment, their high laborcost prohibits widespread use.
There has also beenwork on explicitly evaluating MT adequacy by ag-gregating over a very large set of linguistic features(Gime?nez and Ma`rquez, 2007, 2008) and textual en-tailment (Pado et al, 2009).2 SRL based MT evaluation metricsA blueprint for more direct assessment of mean-ing preservation across translation was outlined byLo and Wu (2010a), in which translation utility ismanually evaluated with respect to the accuracy ofsemantic role labels.
A good translation is one fromwhich human readers may successfully understandat least the basic event structure?
?who did whatto whom, when, where and why?
(Pradhan et al,2004)?which represents the most essential meaningof the source utterances.
Adopting this principle,the MEANT family of metrics compare the seman-tic frames in reference translations against those thatcan be reconstructed from machine translation out-put.Preliminary results reported in (Lo and Wu,2010b) confirm that the blueprint model outper-forms BLEU and similar n-gram oriented evalu-ation metrics in correlation against human ade-quacy judgments, but does not fare as well asHTER.
The more complete study of Lo and Wu(2011a) introduces MEANT and its human variantsHMEANT, which implement an extended version ofblueprint methodology.
Experimental results showthat HMEANT correlates against human adequacyjudgments as well as the more expensive HTER,even though HMEANT can be evaluated using low-cost untrained monolingual semantic role annotatorswhile still maintaining high inter-annotator agree-ment (both are far superior to BLEU or other sur-face oriented evaluation metrics).
The study alsoshows that replacing the human semantic role la-belers with an automatic shallow semantic parseryields an approximation that is still vastly superiorto BLEU while remaining about 80% as closely cor-related with human adequacy judgments as HTER.Along with additional improvements to the accu-racy of the MEANT family of metrics, Lo and Wu(2011b) study the impact of each individual seman-tic role to themetric?s correlation against human ade-quacy judgments, as well as the time cost for humansto reconstruct the semantic frames and compare thetranslation accuracy of the role fillers.In general, the MEANT family of SRL MT eval-uation metrics (Lo and Wu, 2011a,b) evaluate thetranslation utility as follows.
First, semantic rolelabeling is performed (either manually or automat-ically) on both the reference translation (REF) andthe machine translation output (MT) to obtain thesemantic frame structure.
Then, the semantic pred-icates, roles and fillers reconstructed from the MToutput are compared to those in the reference trans-lations.
The number of correctly and partially cor-rectly annotated arguments of each type in eachframe of the MT output are collected in this step:Ci,j ?
# correct ARG i of PRED i in MTPi,j ?
# partially correct ARG j of PRED i in MTMi,j ?
total # ARG j of PRED i in MTRi,j ?
total # ARG j of PRED i in REFIn the following three subsections, we describehow the translation utility is calculated using thesecounts in (a) the original blueprint model, (b) thefirst version of HMEANT and MEANT using struc-tured role representations, and (c) the more accu-11Figure 1: The structured role representation for theblueprint SRL-based MT evaluation metric as proposedin Lo and Wu (2010a,b), with arguments aggregated intocore and adjunct classes.rate flattened-role implementation of HMEANT andMEANT.2.1 Structured core vs. adjunct rolerepresentationFigure 1 depicts the semantic role representationin the blueprint model of SRL MT evaluation metricproposed by Lo and Wu (2010a,b).
Each sentenceconsists of a number of frames, and each frame con-sists of a predicate and two classes of arguments, ei-ther core or adjunct.
The frame precision/recall isthe weighted sum of the number of correctly trans-lated roles (where arguments are grouped into thecore and adjunct classes) in a frame normalized bythe weighted sum of the total number of all roles inthat frame in the MT/REF respectively.
The sen-tence precision/recall is the sum of the frame preci-sion/recall for all frames averaged by the total num-ber of frames in the MT/REF respectively.
The SRLevaluation metric is then defined in terms of f-scorein order to balance the sentence precision and recall.More precisely, assuming the above definitions ofCi,j , Pi,j , Mi,j and Ri,j , the sentence precision andrecall are defined as follows.precision =?iwpred+?twt(?j?t(Ci,j+wpartialPi,j))wpred+?twt(?j?tMi,j)# frames in MTrecall =?iwpred+?twt(?j?t(Ci,j+wpartialPi,j))wpred+?twt(?j?tRi,j)# frames in REFFigure 2: The structured role representation for theMEANT family of metrics as proposed in Lo and Wu(2011a).where wpred is the weight for predicates, and wtwhere t ?
{core, adj} is the weight for core argu-ments and adjunct arguments.
These weights rep-resent the degree of contribution of the predicateand different classes of arguments (either core or ad-junct) to the overall meaning of the semantic framethey attach to.
In addition,wpartial is a weight control-ling the degree to which ?partially correct?
transla-tions are penalized.
All the weights can be automat-ically estimated by optimizing the correlation withhuman adequacy judgments.We conjecture that the reason for the low correla-tion with human adequacy judgments of this modelas reported in Lo and Wu (2010b) is that the ab-straction of arguments actually reduces the repre-sentational power of the original predicate-argumentstructure in SRL.
Under this representation, all thearguments in the same class, e.g.
all adjunct argu-ments, are weighted uniformly.
The assumption thatall types of arguments in the same class have thesame degree of contribution to their frame is obvi-ously wrong, and the empirical results confirm thatthe assumption is too coarse.2.2 Structured role representationFigure 2 shows the structured role representationused in the MEANT family of metrics as proposedin Lo and Wu (2011a), which avoids aggregating ar-guments into core and adjunct classes.
The designof the MEANT family of metrics addresses the in-correct assumption in the blueprint model by assum-ing each type of argument has a unique weight repre-senting its degree of contribution to the overall sen-tence translation.
Thus, the number of dimensions of12the weight vector is increased to allow an indepen-dent weight to be assigned to each type of argument.Unlike the previous representation in the blueprintmodel, there is no aggregation of arguments intocore and adjunct classes.
Each sentence consists of anumber of frames, and each frame consists of a pred-icate and a number of arguments of type j.Under the new approach, the frame preci-sion/recall is the weighted sum of the number of cor-rectly translated roles in a frame normalized by theweighted sum of the total number of all roles in thatframe in the MT/REF respectively.
Similar to theprevious blueprint representation, the sentence pre-cision/recall is the sum of the frame precision/recallfor all frames averaged by the total number of framesin the MT/REF respectively.
More precisely, fol-lowing the previous definitions of Ci,j , Pi,j , Mi,j ,Ri,j ,wpred andwpartial, the sentence precision and re-call are redefined as follows.precision =?iwpred+?jwj(Ci,j+wpartialPi,j)wpred+?jwjMi,j#frames in MTrecall =?iwpred+?jwj(Ci,j+wpartialPi,j)wpred+?jwjRi,j# frames in REFwhere wj is the weight for the arguments of type j.Theseweights represent the degree of contribution ofdifferent types of arguments to the overall meaningof their semantic frame.2.3 Flat role representationFigure 3 depicts the flat role representation used inthemore accurate variants ofMEANT as proposed inLo andWu (2011b).
This representation ismotivatedby the studies of the impact of individual seman-tic role.
The highly significant difference betweenthis flat representation and both of the previous twostructured role representations is that the semanticframes in the sentence are no longer segregated.The flat role representation desegregates the framestructure, resulting in a flat, single level structure.Therefore, there is no frame precision/recall.
Thesentence precision/recall is the weighted sum of thenumber of correctly translated roles in all frames nor-malized by the weighted sum of the total number ofroles in all frames in theMT/REF respectively.
Moreprecisely, again assuming the previous definitions ofCi,j , Pi,j , Mi,j , Ri,j and wpartial, the sentence preci-sion and recall are redefined as follows.Cpred ?
total # correctly translated predicatesMpred ?
total # predicates in MTRpred ?
total # predicates in REFprecision =wpredCpred +?j wj(?i(Ci,j + wpartialPi,j))wpredMpred +?j wj(?i Mi,j)recall =wpredCpred +?j wj(?i(Ci,j + wpartialPi,j))wpredRpred +?j wj(?i Ri,j)Note that there is a small modification of the defini-tion of wpred and wj .
Instead of the degree of contri-bution to the overall meaning of the semantic framethat the roles attached to, wpredand wj now representthe degree of contribution of the predicate and the ar-guments of type j to the overall meaning of the entiresentence.It is worth noting that the semantic role features inthe ULC metric proposed by Gime?nez and Ma`rquez(2008) also employ a flat feature-based represen-tation of semantic roles.
However, the definitionof those semantic role features adopts a differentmethodology for determining the role fillers?
transla-tion accuracy, which prevents a controlled consistentenvironment for the comparative experiments thatthe present work focuses on.3 Experimental setupThe evaluation data for our experiments consistsof 40 sentences randomly drawn from the DARPAGALE program Phase 2.5 newswire evaluation cor-pus containing Chinese input sentence, English ref-erence translations, and themachine translation fromthree different state-of-the-art GALE systems.
TheChinese and the English reference translation haveboth been annotated with gold standard PropBank(Palmer et al, 2005) semantic role labels.
Theweightswpred, wcore, wadj, wj and wpartial can be esti-mated by optimizing correlation against human ade-quacy judgments, using any of themany standard op-timization search techniques.
In the work of Lo and13Figure 3: The flat role representation for the MEANT family of metrics as proposed in Lo and Wu (2011b) .Wu (2011b), the correlations of all individual roleswith the human adequacy judgments were found tobe non-negative, therefore we found grid search tobe quite adequate for estimating the weights.
We uselinear weighting because we would like to keep themetric?s interpretation simple and intuitive.Following the benchmark assessment in NISTMetricsMaTr 2010 (Callison-Burch et al, 2010), weassess the performance of the semantic MT evalua-tion metric at the sentence level using the summed-diagonal-of-confusion-matrix score.
The human ad-equacy judgments were obtained by showing allthree MT outputs together with the Chinese sourceinput to a human reader.
The human reader was in-structed to order the sentences from the three MTsystems according to the accuracy of meaning inthe translations.
For the MT output, we ranked thesentences from the three MT systems according totheir evaluation metric scores.
By comparing thetwo sets of rankings, a confusion matrix is formed.The summed diagonal of confusion matrix is the per-centage of the total count when a particular rank bythe metric?s score exactly matches the human judg-ments.
The range of possible values of summed di-agonal of confusion matrix is [0,1], where 1 meansall the systems?
ranks determined by the metric areidentical with that of the human judgments and 0means all the systems?
ranks determined by the met-ric are different from that of the human judgment.Since the summed diagonal of confusion matrixscores only assess the absolute ranking accuracy,we also report the Kendall?s ?
rank correlation co-efficients, which measure the correlation of the pro-posed metric against human judgments with respectto their relative ranking of translation adequacy.
Ahigher the value for ?
indicates the more similar theranking by the evaluation metric to the human judg-ment.
The range of possible values of correlationTable 1: Sentence-level correlations against human ade-quacy judgments as measured by Kendall?s ?
and summeddiagonal of confusion matrix as used in MetricsMaTr2010.
?SRL - blueprint?
is the blueprint model describedin section 2.1.
?HMEANT (structured)?
is HMEANT us-ing the structured role representation described in sec-tion 2.2.
?HMEANT (flat)?
is HMEANT using the flatrole representation described in section 2.3.Metric Kendall MetricsMaTrHMEANT (flat) 0.4685 0.5583HMEANT (structured) 0.4324 0.5083SRL - blueprint 0.3784 0.4667coefficient is [-1,1], where 1 means the systems areranked in the same order as the human judgment and-1 means the systems are ranked in the reverse orderas the human judgment.4 Round 1: Flat beats structuredOur first round of comparative results quantita-tively assess whether a structured role representation(that properly preserves the semantic frame struc-ture, which is typically hierarchically nested in com-positional fashion) outperforms the simpler (but lessintuitive, and certainly less linguistically satisfying)flat role representation.As shown in table 1, disturbingly, HMEANT us-ing flat role representations yields higher correla-tions against human adequacy judgments than us-ing structured role representations, regardless ofwhether role types are aggregated into core andadjunct classes.
The results are consistent forboth Kendall?s tau correlation coefficient and Met-ricsMaTr?s summed diagonal of confusion matrix.HMEANT using a flat role representation achieveda Kendall?s tau correlation coefficient and summeddiagonal of confusion matrix score of 0.4685 and0.5583 respectively, which is superior to both14Figure 4: The new proposed structured role representa-tion, incorporating a weighting scheme reflecting the de-gree of contribution of each semantic frame to the overallsentence.HMEANT using a structured role representation(0.4324 and 0.5083 respectively) and the blueprintmodel (0.3784 and 0.4667 respectively).Error analysis, in light of these surprising results,strongly suggests that the problem lies in the designwhich uniformly averages the frame precision/recallover all frames in a sentence when computing thesentence precision/recall.
This essentially assumesthat each frame in a sentence contributes equallyto the overall meaning in the sentence translation.Such an assumption is trivially wrong and could wellhugely degrade the advantages of using a structuredrole representation for semanticMT evaluation.
Thissuggests that the structured role representation couldbe improved by also capturing the degree of contri-bution of each frame to the overall sentence transla-tion.5 Capturing the importance of each frameTo address the problem in the previousmodels, weintroduce a weighting scheme to reflect the degreeof contribution of each semantic frame to the overallsentence.
However, unlike the contribution of eachrole to a frame, the contribution of each frame tothe overall sentence cannot be estimated across sen-tences.
This is because unlike semantic roles, whichcan be identified by their types, frames do not neces-sarily have easily defined types, and their construc-tion is also different from sentence to sentence so thatthe positions of their predicates in the sentence arethe only way to identify the frames.
However, thedegree of contribution of each frame does not dependon the position of the predicate in the sentence.
Forexample, the two sentences I met Tom when I was go-ing home andWhen I was walking home, I saw Tom havesimilar meanings.
The verbs met and saw are thepredicates of the key event frames which contributemore to the overall sentences, whereas going andwalking are the predicates of the minor nested eventframes (in locative manner roles of the key eventframes) and contribute less to the overall sentences.However, the two sentences are realized with differ-ent surface constructions, and the two key frames arein different positions.
Therefore, the weights learnedfrom one sentence cannot directly be applied to theother sentence.Instead of estimating the weight of each frame us-ing optimization techniques, wemake an assumptionthat a semantic frame filled with more word tokensexpresses more concepts and thus contributes moreto the overall sentence.
Following this assumption,we determine the weights of each semantic frame byits span coverage in the sentence.
In other words,the weight of each frame is the percentage of wordtokens it covers in the sentence.Figure 4 depicts the structured role representa-tion with the proposed new frame weighting scheme.The significant difference between this representa-tion and the structured role representation in theMEANT variants proposed in Lo and Wu (2011a)is that each frame is now assigned an independentweight, which is its span coverage in the MT/REFwhen obtaining the frame precision/recall respec-tively.As in Lo and Wu (2011a), each sentence consistsof a number of frames, and each frame consists ofa predicate and a number of arguments of type j.Each type of argument is assigned an independentweight to represent its degree of contribution to theoverall meaning of the semantic frame they attachedto.
The frame precision/recall is the weighted sumof the number of correctly translated roles in a framenormalized by the weighted sum of the number of allroles in that frame in the MT/REF.
The sentence pre-cision/recall is the weighted sum of the frame preci-sion/recall for all frames normalized by the weightedsum of the total number of frames in MT/REF re-spectively.
More precisely, again assuming the ear-15lier definitions of Ci,j , Pi,j , Mi,j , Ri,j , wpred andwpartial in section 2, the sentence precision and recallare redefined as follows.mi ?# tokens filled in frame i of MTtotal # tokens in MTri ?# tokens filled in frame i of REFtotal # tokens in REFprecision =?imiwpred+?jwj(Ci,j+wpartialPi,j)wpred+?jwjMi,j?imirecall =?i riwpred+?jwj(Ci,j+wpartialPi,j)wpred+?jwjRi,j?i riwhere mi and ri are the weights for frame i, in theMT/REF respectively.
These weights estimate thedegree of contribution of each frame to the overallmeaning of the sentence.6 Round 2: Structured beats flatWe now assess the performance of the new pro-posed structured role representation, by comparingit with the previous models under the same experi-mental setup as in section 4.
We have also run con-trastive experiments against BLEU and HTER un-der the same experimental conditions.
In addition,to investigate the consistency of results for the au-tomated variants of MEANT, we also include com-parative experiments where shallow semantic pars-ing (ASSERT) replaces human semantic role label-ers for each model of role representation.Figure 5 shows an example where HMEANTwiththe frame weighting scheme outperforms HMEANTusing other role representations in correlation againsthuman adequacy judgments.
IN is the Chinesesource input.
REF is the corresponding refer-ence translation.
MT1, MT2 and MT3 are thethree corresponding MT output.
The human ade-quacy judgments for this set of translation are thatMT1>MT3>MT2.
HMEANT with the proposedframe weighting predicts the same ranking orderas the human adequacy judgment, while HMEANTwith the flat role representation and HMEANTwith the structured role representation without frameweighting both predict MT3>MT1>MT2.
Thereare four semantic frames in IN while there are onlythree semantic frames in the REF.
This is becausethe predicate ??
in IN is translated in REF as hadwhich is not a predicate.
However, for the sameframe, both MT1 and MT2 translated ARG1????
into the predicate affect, while MT3 did not trans-late the predicate ??
and translated the ARG1 ????
into the noun phrase adverse impact.
There-fore, using the flat role representation or the previ-ous structured role representation which assume allframes have an identical degree of contribution to theoverall sentence translation, MT1?s and MT2?s sen-tence precision is greatly penalized for having onemore extra frame than the reference.
In contrast, ap-plying the frame weighting scheme, the degree ofcontribution of each frame is adjusted by its tokencoverage.
Therefore, the negative effect of the lessimportant extra frames is minimized, allowing thepositive effect of correctly translating more roles inmore important frames to be more appropriately re-flected.Table 2 shows that HMEANT with the proposednew frameweighting scheme correlatesmore closelywith human adequacy judgments than HMEANTusing the previous alternative role representations.The results from Kendall?s tau correlation coeffi-cient and MetricsMaTr?s summed diagonal of con-fusion matrix analysis are consistent.
HMEANTusing the frame-weighted structured role represen-tation achieved a Kendall?s tau correlation coef-ficient and summed diagonal of confusion matrixscore of 0.2865 and 0.575 respectively, betteringboth HMEANT using the flat role representation(0.4685 and 0.5583) and HMEANT using the pre-vious un-frame-weighted structured role representa-tion (0.4324 and 0.5083).HMEANT using the improved structured role rep-resentation also outperforms other commonly usedMT evaluation metrics.
It correlates with human ad-equacy judgments more closely than HTER (0.4324and 0.425 in Kendall?s tau correlation coefficient andsummed diagonal of confusionmatrix, respectively).It also correlates with human adequacy judgmentssignificantly more closely than BLEU (0.1982 and0.425).Turning to the variants that replace human SRLwith automated SRL, table 2 shows that MEANT16Figure 5: Example input sentence along with reference and machine translations, annotated with semantic frames inPropbank format.
The MT output is annotated with semantic frames by minimally trained humans.
HMEANT withthe new frame-weighted structured role representation successfully ranks the MT output in an order that matches withhuman adequacy judgments (MT1>MT3>MT2), whereas HMEANT with a flat role representation or the previousun-frame-weighted structured role representation fails to rank MT1 and MT3 in an order that matches with humanadequacy judgments.
See section 6 for details.17Table 2: Sentence-level correlations against human ade-quacy judgments as measured by Kendall?s ?
and summeddiagonal of confusion matrix as used in MetricsMaTr2010.
?SRL - blueprint?, ?HMEANT (structured)?
and?HMEANT (flat)?
are the same as in table 1.
?MEANT(structured)?
and ?MEANT (flat)?
use automatic ratherthan human SRL.
?MEANT (frame)?
and ?HMEANT(frame)?
are MEANT/HMEANT using the structuredrole representation with the frame weighting scheme de-scribed in section 5.Metric Kendall MetricsMaTrHMEANT (frame) 0.4865 0.575HMEANT (flat) 0.4685 0.5583HMEANT (structured) 0.4324 0.5083HTER 0.4324 0.425SRL - blueprint 0.3784 0.4667MEANT (frame) 0.3514 0.4333MEANT (structured) 0.3423 0.425MEANT (flat) 0.3333 0.425BLEU 0.1982 0.425using the new frame-weighted structured role repre-sentation yields an approximation that is about 81%as closely correlated with human adequacy judgmentas HTER, and is better than all previous MEANTvariants using alternative role representations.
Allresults consistently confirm that using a structuredrole representation with the new frame weightingscheme, which captures the event structure and anapproximate degree of contribution of each frame tothe overall sentence, outperforms using a flat rolerepresentation for SRL based MT evaluation met-rics.7 ConclusionWe have shown how the MEANT family of SRLbased MT evaluation metrics is significantly im-proved beyond the state-of-the-art for both HTERand previous variants of MEANT, through the in-troduction of a simple but well-motivated weight-ing scheme to reflect the degree of contribution ofeach semantic frame to the overall sentence trans-lation.
Following the assumption that a semanticframe filled with more word tokens tends to expressmore concepts, the new model weight each frameby its span coverage.
Consistent experimental re-sults have been demonstrated under conditions uti-lizing both human and automatic SRL.
Under thenew frame weighted representation, properly nestedstructured semantic frame representations regain anempirically preferred position over the less intuitiveand linguistically unsatisfying flat role representa-tions.One future direction of this work will be to com-pare MEANT against the feature based and stringbased representations of semantic relations in ULC.Such a comparison could yield a more completecredit/blame perspective on the representationmodelwhen operating under the condition of using auto-matic SRL.Another interesting extension of this work wouldbe to investigate the discriminative power of theMEANT family of metrics to distinguish distancesin translation adequacy.
In this paper we confirmedthat the MEANT family of metrics are stable in cor-relation with human ranking judgments of transla-tion adequacy.
Further studies could focus on thecorrelation of the MEANT family of metrics againsthuman scoring.
We also plan to experiment on meta-evaluating MEANT on a larger scale in other genresand for other language pairs.AcknowledgmentsThis material is based uponwork supported in partby the Defense Advanced Research Projects Agency(DARPA) under GALE Contract Nos.
HR0011-06-C-0022 and HR0011-06-C-0023 and by the HongKong Research Grants Council (RGC) researchgrants GRF621008, GRF612806, DAG03/04.EG09,RGC6256/00E, and RGC6083/99E.
Any opinions,findings and conclusions or recommendations ex-pressed in this material are those of the authors anddo not necessarily reflect the views of the DefenseAdvanced Research Projects Agency.ReferencesSatanjeev Banerjee and Alon Lavie.
METEOR: AnAutomatic Metric for MT Evaluation with Im-proved Correlation with Human Judgments.
In43th AnnualMeeting of the Association of Compu-tational Linguistics (ACL-05), pages 65?72, 2005.Chris Callison-Burch, Miles Osborne, and PhilippKoehn.
Re-evaluating the role of BLEU in Ma-chine Translation Research.
In 13th Confer-18ence of the European Chapter of the Associationfor Computational Linguistics (EACL-06), pages249?256, 2006.Chris Callison-Burch, Philipp Koehn, ChristofMonz, Kay Peterson, Mark Pryzbocki, and OmarZaidan.
Findings of the 2010 Joint Workshopon Statistical Machine Translation and Metricsfor Machine Translation.
In Joint 5th Workshopon Statistical Machine Translation and Metric-sMATR, pages 17?53, Uppsala, Sweden, 15-16July 2010.G.
Doddington.
Automatic Evaluation of MachineTranslation Quality using N-gram Co-occurrenceStatistics.
In 2nd International Conference on Hu-man Language Technology Research (HLT-02),pages 138?145, San Francisco, CA, USA, 2002.Morgan Kaufmann Publishers Inc.Jesu?s Gime?nez and Llu?is Ma`rquez.
Linguistic Fea-tures for Automatic Evaluation of HeterogenousMT Systems.
In 2nd Workshop on Statistical Ma-chine Translation, pages 256?264, Prague, CzechRepublic, June 2007.
Association for Computa-tional Linguistics.Jesu?s Gime?nez and Llu?is Ma`rquez.
A Smorgas-bord of Features for Automatic MT Evaluation.
In3rd Workshop on Statistical Machine Translation,pages 195?198, Columbus, OH, June 2008.
Asso-ciation for Computational Linguistics.Philipp Koehn and Christof Monz.
Manual andAutomatic Evaluation of Machine Translation be-tween European Languages.
In Workshop onStatistical Machine Translation, pages 102?121,2006.Gregor Leusch, Nicola Ueffing, and Hermann Ney.CDer: Efficient MT Evaluation Using BlockMovements.
In 13th Conference of the EuropeanChapter of the Association for Computational Lin-guistics (EACL-06), 2006.Ding Liu and Daniel Gildea.
Syntactic Features forEvaluation of Machine Translation.
In ACLWork-shop on Intrinsic and Extrinsic Evaluation Mea-sures for Machine Translation and/or Summariza-tion, page 25, 2005.Chi-Kiu Lo and Dekai Wu.
Evaluating MachineTranslation Utility via Semantic Role Labels.
In7th International Conference on Language Re-sources and Evaluation (LREC-2010), 2010.Chi-Kiu Lo and Dekai Wu.
Semantic vs. Syntac-tic vs. N-gram Structure for Machine TranslationEvaluation.
In Proceedings of the 4th Workshopon Syntax and Structure in Statistical Translation(SSST-4), 2010.Chi-Kiu Lo and Dekai Wu.
MEANT: An Inexpen-sive, High-Accuracy, Semi-Automatic Metric forEvaluating Translation Utility based on Seman-tic Roles.
In Joint conference of the 49th AnnualMeeting of the Association for Computational Lin-guistics : Human Language Technologies (ACLHLT 2011), 2011.Chi-Kiu Lo and Dekai Wu.
SMT vs. AI redux: Howsemantic frames evaluate MT more accurately.
InTo appear in 22nd International Joint Conferenceon Artificial Intelligence, 2011.Sonja Nie?en, Franz Josef Och, Gregor Leusch, andHermann Ney.
A Evaluation Tool for MachineTranslation: Fast Evaluation for MT Research.
In2nd International Conference on Language Re-sources and Evaluation (LREC-2000), 2000.Sebastian Pado, Michel Galley, Dan Jurafsky, andChris Manning.
Robust Machine TranslationEvaluation with Entailment Features.
In Jointconference of the 47th Annual Meeting of the As-sociation for Computational Linguistics and the4th International Joint Conference on NaturalLanguage Processing of the Asian Federation ofNatural Language Processing (ACL-IJCNLP-09),2009.Martha Palmer, Daniel Gildea, and Paul Kings-bury.
The Proposition Bank: an Annotated Cor-pus of Semantic Roles.
Computational Linguis-tics, 31(1):71?106, 2005.Kishore Papineni, Salim Roukos, Todd Ward, andWei-Jing Zhu.
BLEU: A Method for Auto-matic Evaluation of Machine Translation.
In40th Annual Meeting of the Association for Com-putational Linguistics (ACL-02), pages 311?318,2002.Sameer Pradhan, Wayne Ward, Kadri Hacioglu,James H. Martin, and Dan Jurafsky.
Shallow Se-mantic Parsing Using Support Vector Machines.19In 2004 Conference on Human Language Tech-nology and the North American Chapter of theAssociation for Computational Linguistics (HLT-NAACL-04), 2004.Matthew Snover, Bonnie J. Dorr, Richard Schwartz,Linnea Micciulla, and John Makhoul.
A Study ofTranslation Edit Rate with Targeted Human An-notation.
In 7th Conference of the Association forMachine Translation in the Americas (AMTA-06),pages 223?231, 2006.Christoph Tillmann, Stephan Vogel, Hermann Ney,Arkaitz Zubiaga, and Hassan Sawaf.
AcceleratedDP Based Search For Statistical Translation.
In5th European Conference on Speech Communica-tion and Technology (EUROSPEECH-97), 1997.20
