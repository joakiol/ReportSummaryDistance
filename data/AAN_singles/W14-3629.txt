Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 217?224,October 25, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsArabizi Detection and Conversion to ArabicKareem DarwishQatar Computing Research InstituteQatar Foundation, Doha, Qatarkdarwish@qf.org.qaAbstractArabizi is Arabic text that is written using Latincharacters.
Arabizi is used to present both Mod-ern Standard Arabic (MSA) or Arabic dialects.
Itis commonly used in informal settings such as so-cial networking sites and is often with mixed withEnglish.
In this paper we address the problems of:identifying Arabizi in text and converting it to Ara-bic characters.
We used word and sequence-levelfeatures to identify Arabizi that is mixed with En-glish.
We achieved an identification accuracy of98.5%.
As for conversion, we used transliterationmining with language modeling to generate equiva-lent Arabic text.
We achieved 88.7% conversion ac-curacy, with roughly a third of errors being spellingand morphological variants of the forms in groundtruth.1 IntroductionArabic is often written using Latin characters intransliterated form, which is often referred to as Ara-bizi, Arabish, Franco-Arab, and other names.
Ara-bizi uses numerals to represent Arabic letters forwhich there is no phonetic equivalent in English orto account for the fact that Arabic has more lettersthan English.
For example, ?2?
and ?3?
representthe letters@ (that sounds like ?a?
as in apple) and ?
(that is a guttural ?aa?)
respectively.
Arabizi is par-ticularly popular in Arabic social media.
Arabizi hasgrown out of a need to write Arabic on systems thatdo not support Arabic script natively.
For example,Internet Explorer 5.0, which was released in March1999, was the first version of the browser to sup-port Arabic display natively1.
Windows Mobile andAndroid did not support Arabic except through thirdparty support until versions 6.5x and 3.x respec-tively.
Despite the increasing support of Arabic inmany platforms, Arabizi continues to be popular dueto the familiarity of users with it and the higher profi-ciency of users to use an English keyboard comparedto an Arabic keyboard.
Arabizi is used to presentboth MSA as well as different Arabic dialects, whichlack commonly used spelling conventions and differmorphologically and phonetically from MSA.
Therehas been recent efforts to standardize the spellingof some Arabic dialects (Habash et al., 2012), butsuch standards are not widely adopted on social me-dia.
Additionally, due to the fact that many of theArabic speakers are bilingual (with their second lan-guage being either English or French), another com-monly observed phenomenon is the presence of En-glish (or French) and Arabizi mixed together withinsentences, where users code switch between bothlanguages.
In this paper we focus on performing twotasks, namely: detecting Arabizi even when juxta-posed with English; and converting Arabizi to Ara-bic script regardless of it being MSA or dialectal.Detecting and converting Arabizi to Arabic scriptwould help: ease the reading of the text, whereArabizi is difficult to read; allow for the process-ing of Arabizi (post conversion) using existing NLPtools; and normalize Arabic and Arabizi into a uni-fied form for text processing and search.
Detectingand converting Arabizi are complicated by the fol-lowing challenges:1http://en.wikipedia.org/wiki/Internet_Explorer2171.
Due to the lack of spelling conventions for Ara-bizi and Arabic dialectal text, which Arabizi of-ten encodes, building a comprehensive dictio-nary of Arabizi words is prohibitive.
Considerthe following examples:(a) The MSA word QKQm'(liberty) has the fol-lowing popular Arabizi spellings: ta7rir,t7rir, tahrir, ta7reer, tahreer, etc.
(b) The dialectal equivalents to the MSAI.?
?KB (he does not play) could be?.??JK.A?,?.??K.A?,?.??J?,?.??KA?etc.
The resultant Arabizi could be:mayel3absh, mabyelaabsh, mabyel3absh,etc.2.
Some Arabizi and English words share a com-mon spelling, making solely relying on an En-glish dictionary insufficient to identify Englishwords.
Consider the following examples (am-biguous words are bolded):(a) Ana 3awez aroo7 men America lehCanada (I want to go from America toCanada).
The word ?men?
meaning?from?
is also an English word.
(b) I called Mohamed last night.
?Mohamed?in this context is an English word, thoughit is a transliterated Arabic name.3.
Within social media, users often use creativespellings of English words to shorten text, em-phasize, or express emotion.
This can compli-cate the differentiation of English and Arabizi.Consider the following examples:(a) I want 2 go with u tmrw, cuz my car isbroken.
(b) Woooooow.
Ur car is cooooooool.Due to these factors, classifying a word as Ara-bizi or English has to be done in-context.
Thus, weemployed sequence labeling using Conditional Ran-dom Fields (CRF) to detect Arabizi in context.
TheCRF was trained using word-level and sequence-level features.
For converting Arabizi to Arabicscript, we used transliteration mining in combinationwith a large Arabic language model that covers bothMSA and other Arabic dialects to properly choosethe best transliterations in context.The contributions of this paper are:?
We employed sequence labeling that is trainedusing word-level and sequence-level featuresto identify in-sentence code-switching betweentwo languages that share a common alphabet.?
We used transliteration mining and languagemodeling to convert form Arabizi to Arabicscript.?
We plan to publicly release all our training andtest data.The remainder of the paper is organized as fol-lows: Section 2 provides related work; Section 3presents our Arabizi detection and reports on thedetection accuracy; Section 4 describes our Arabizito Arabic conversion approach and reports the ac-curacy of conversion; and Section 5 concludes thepaper.2 Related WorkThere are two aspects to this work: the first is lan-guage identification, and the second is translitera-tion.
There is much work on language identifica-tion including open source utilities, such as the Lan-guage Detection Library for Java2.
Murthy and Ku-mar (2006) surveyed many techniques for languageidentification.
Some of the more successful tech-niques use character n-gram models (Beesley, 1988;Dunning, 1994) in combination with a machinelearning technique such as hidden Markov mod-els (HMM) or Bayesian classification (Xafopouloset al., 2004; Dunning, 1994).
Murthy and Ku-mar (2006) used logistic regression-like classifica-tion that employed so-called ?aksharas?
which aresub-word character sequences as features for identi-fying different Indian languages.
Ehara and Tanaka-Ishii (2008) developed an online language detec-tion system that detects code switching during typ-ing, suggests the language to switch to the user,and interactively invokes the appropriate text entrymethod.
They used HMM based language iden-tification in conjunction with an n-gram characterlanguage model.
They reported up to 97% accu-racy when detecting between two languages on a2http://code.google.com/p/language-detection/218synthetic test set.
In our work, we performed of-fline word-level language identification using CRFsequence labeling, which conceptually combineslogistic regression-like discriminative classificationwith an HMM-like generative model (Lafferty et al.,2001).
We opted to use a CRF sequence labeling be-cause it allowed us to use both state and sequencefeatures, which in our case corresponded to word-and sequence-level features respectively.
One of thedownsides of using a CRF sequence labeler is thatmost implementations, including CRF++ which wasused in this work, only use nominal features.
Thisrequired us to quantize all real-valued features.Converting between from Arabizi to Arabic isakin to transliteration or Transliteration Mining(TM).
In transliteration, a sequence in a source al-phabet or writing system is used to generate a pho-netically similar sequence in a target alphabet orwriting system.
In TM, a sequence in a source al-phabet or writing system is used to find the mostsimilar sequence in a lexicon that is written in thetarget alphabet or writing system.
Both problemsare fairly well studied with multiple evaluation cam-paigns, particularly at the different editions of theNamed Entities Workshop (NEWS) (Zhang et al.,2011; Zhang et al., 2012).
In our work we reliedon TM from a large corpus of Arabic microblogs.TM typically involves using transliteration pairs intwo different writing systems or alphabets to learn-ing character (or character-sequence) level map-pings between them.
The learning can be done usingthe EM algorithm (Kuo et al., 2006) or HMM align-ment (Udupa et al., 2009).
Once these mappings arelearned, a common approach involves using a gen-erative model that attempts to generate all possibletransliterations of a source word, given the charac-ter mappings between two languages, and restrictingthe output to words in the target language (El-Kahkiet al., 2011; Noeman and Madkour, 2010).
Otherapproaches include the use of locality sensitive hash-ing (Udupa and Kumar, 2010) and classification (Ji-ampojamarn et al., 2010).
Another dramatically dif-ferent approaches involves the unsupervised learn-ing of transliteration mappings from a large paral-lel corpus instead of transliteration pairs (Sajjad etal., 2012).
In our work, we used the baseline sys-tem of El-Kahky et al.
(2011).
There are three com-mercial Input Method Editors (IMEs) that convertfrom Arabizi to Arabic, namely: Yamli3, MicrosoftMaren4, and Google t3reeb5.
Since they are IMEs,they only work in an interactive mode and don?t al-low for batch processing.
Thus they are difficultto compare against.
Also, from interactively usingArabic IMEs, it seems that they only use unigramlanguage modeling.3 Identifying ArabiziAs mentioned earlier, classifying words as En-glish or Arabizi requires the use of word-level andsequence-level features.
We opted to use CRF se-quence labeling to identify Arabizi words.
We usedthe CRF++ implementation with default parame-ters (Sha and Pereira, 2003).
We constructed train-ing and test sets for word-level language classifica-tion from tweets that contain English, Arabizi, or amixture of English and Arabizi.
We collected thetweets in the following manner:1.
We issued commonly used Arabizi words asqueries against Twitter multiple times.
Thesewords were ?e7na?
(we), ?3shan?
(because),and ?la2a?
(no).
We issued these queries ev-ery 30 seconds for roughly 1 hour.
We put largetime gaps between queries to ensure that the re-sults were refreshed.2.
We extracted the user IDs of all the authors ofthe tweets that we found, and used the IDs asqueries to Twitter to get the remaining tweetsthat they have authored.
Our intuition was thattweeps who authored once in Arabizi wouldlikely have more Arabizi tweets.
Doing sohelped us find Arabizi tweets that don?t neces-sarily have the aforementioned common wordsand helped us find significantly more Arabizitext.
In all we identified 265 tweeps who au-thored 16,507 tweets in the last 7 days, contain-ing 132,236 words.
Of the words in the tweets,some of them were English, but most of themwere Arabizi.We filtered tweets where most of the words con-tained Arabic letters.
As in Table 1, all the tokens in3http://www.yamli.com/editor/ar/4http://www.getmaren.com5http://www.google.com/ta3reeb219Label Explanationa Arabizie Englisho Other including URL?s, user mentions, hashtags,laughs (lol, , :P, xd), and none wordsTable 1: Used labels for wordsthe set were manually labeled as English (?e?
), Ara-bizi (?a?
), or other (?o?).
For training, we used 522tweets, containing 5,207 tokens.
The breakdown oftokens is: 3,307 English tokens; 1,203 Arabizi to-kens; and 697 other tokens.
For testing, we used101 tweets containing 3,491 tokens.
The breakdownof the tokens is: 797 English tokens; 1,926 Arabizitokens; and 768 other tokens.
Though there is somemismatch in the distribution of English and Arabizitokens between training and test sets, this mismatchhappened naturally and is unlikely to affect overallaccuracy numbers.
For language models, we trainedtwo character level language models: the first us-ing 9.4 million English words; and the second us-ing 1,000 Arabizi words (excluding words in the testset).
We used the BerkeleyLM language modelingtoolkit.We trained the CRF++ implementation of CRFsequence labeler using the features in Table 2 alongwith the previous word and next word.
The Tabledescribes each feature and shows the features valuesfor the word ?Yesss?.Table 3 reports on the language identification re-sults and breaks down the results per word typeand provides examples of mislabeling.
Overall weachieved a word-level language identification accu-racy of 98.5%.
As the examples in the table show,the few mislabeling mistakes included: ArabizedEnglish words, Arabizi words that happen to be En-glish words, single Arabizi words surrounded by En-glish words (or vice versa), and misspelled Englishwords.4 Arabizi to ArabicAs mentioned earlier, Arabizi is simply Arabic,whether MSA or dialectal, that is written using Latincharacters.
We were able to collect Arabizi textby searching for common Arabizi words on Twit-ter, identifying the authors of these tweets, and thenscraping their tweets to find more tweets writtenin Arabizi.
In all, we constructed a collection thatcontained 3,452 training pairs that have both Ara-bizi and Arabic equivalents.
All Arabizi words weremanually transliterated into Arabic by a native Ara-bic speaker.
Some example pairs are:?
3endek?
?YJ?
(meaning ?in your care?)?
bytl3?
???JK.
(meaning ?he ascends?
)For testing, we constructed a set of 127 randomArabizi tweets containing 1,385 word.
Again, wehad a native Arabic speaker transliterate all tweetsinto Arabic script.
An example sentences is:?
sa7el eih ?
howa ntii mesh hatigi bokra ??Q?K.?j.JJ???
??K @ ??
?
?K@ ?gA??
meaning: what coast ?
aren?t you coming to-morrowWe applied the following preprocessing steps onthe training and test data:?
We performed the following Arabic letter nor-malizations (Habash, 2010):?
?
(alef maqsoura)?
?
(ya)?@ (alef maad),@ (alef with hamza on top),and @(alef with hamza on the bottom)?
@(alef)??'
(hamza on w), and Z?'
(hamza on ya)?
Z (hamza)??
(taa marbouta)?
?
(haa)?
Since people often repeat letters in tweets toindicate stress or to express emotions, we re-moved any repetition of a letter beyond 2 repe-titions (Darwish et al., 2012).
For example, wetransformed the word ?salaaaam?
to ?salaam?.?
Many people tend to segment Arabic words inArabizi into separate constituent parts.
For ex-ample, you may find ?w el kitab?
(meaning?and the book?)
as 3 separate tokens, whilein Arabic they are concatenated into a singletoken, namely ?H.AJ??@??.
Thus, we concate-nated short tokens that represent coordinatingconjunctions and prepositions to the tokens thatfollow them.
These tokens are: w, l, el, ll, la,we, f, fel, fil, fl, lel, al, wel, and b.220Feature Explanation Ex.Word This would help label words that appear in the training examples.
This feature is particularly useful for frequentwords.yesssShort This would remove repeated characters in a word.
Colloquial text such as tweets and Facebook statuses containword elongations.yesIsLaugh This indicates if a word looks like a laugh or emoticon.
For example lol, J, :D, :P, xD, (ha)+, etc.
Smiles and laughsshould get an ?o?
label.0IsURL This indicates if a token resembles as URL of the form: http:/[a-zA-z0-9?]+/.
URLs should get an ?o?
label.
0IsNo This indicates if a token is composed of numbers only.
Numbers should get an ?o?
label 0Is!Latin This indicates if a word is composed of non-Latin letters.
If a word is composed on non-Latin characters, it is not?e?.0WordLength This feature is simply the token length.
Transliterated words are typically longer than native words 8IsHashtag This indicates if it is a hashtag.
Hashtags would get an ?e?
label.
0IsNameMention This indicates if it is a name mention.
Name mentions, which start with ?@?
sign, should get an ?o?
label.
0IsEmail This indicates if it is an email.
Emails, which match [\S\.\-_]+@[\S\.\-_]+ should get an ?o?
label.
0wordEnUni Unigram probability in English word-level language model.
The language model is built on English tweets.
If aword has a high probability of being English then it is likely English.-4wordEnBi Bigram probability in English word-level language model of the word with the word that precedes it.
If the proba-bility is high then it is likely that it is an English word that follows another English word.-4charEnNgram Trigram probability in English character-level language model of characters in a word.
This checks if it is likelysequence of characters in an English word.-2charArNgram Trigram probability in Arabizi character-level language model of characters in a word.
This checks if it is likelysequence of characters in an Arabizi word.-13Table 2: Used labels for wordsActual Tag Predicted Tag Count Percent Example (Misclassified Token High-lighted)Analysisa a 1909 99.1%a e 12 0.6% tfker b2y shy be relax, tab 3 3ashan eltalta tabtashy & tab: words that exist in English but areactually Arabic in contextal weekend eljaay ya5i weekend: Arabized English wordswow be7keelk the cloud covered bt7keelk: sudden context switch before and af-tera o 5 0.3% ya Yara ha call u @fjoooj eeeeeeeh ha & eeeeeeh: mistaken for smiles or laughse e 773 97.0%e a 21 2.6% el eye drope eh ya fara7 eye & drop: sudden context switchofftoschool offtoschool: misspelled English wordse o 3 0.4% 4 those going 2 tahrir 4 & 2: numbers used instead of wordso o 758 98.7%o e 3 0.4% URL?s and name mentions Could be fixed with either a simple rule or moretraining datao a 7 0.9%Table 3: Used labels for words?
We directly transliterated the words ?isA?and ?jAk?
to ?
?<?
@ Z A??@?
(meaning ?Godwelling?)
and to ?
@Qg ?<?
@ ?@Qk.?
(meaning?may God reward you?)
respectively.For training, we aligned the word-pairs at char-acter level.
The pairs were aligned using GIZA++and the phrase extractor and scorer from the Mosesmachine translation package (Koehn et al., 2007) .To apply a machine translation analogy, we treatedwords as sentences and the letters from which wereconstructed as tokens.
The alignment produced let-ter sequence mappings.
The alignment producedmappings between Latin letters sequences and Ara-bic letter sequences with associated mapping proba-bilities.
For example, here is a sample mapping:?
2r?
Q?
(p = 0.459)To generate Arabic words from Arabizi words, wemade the fundamental simplifying assumption thatany generated Arabic word should exist in a largeword list.
Though the assumption fundamentallylimits generation to previously seen words only, webuilt the word list from a large set of tweets.
Thus,the probability that a correctly generated word didnot exist in the word list would be negligible.
Thisassumption allowed us to treat the problem as a min-221ing problem instead of a generation problem whereour task was to find a correct transliteration in a listof words instead of generating an arbitrary word.
Webuilt the word list from a tweet set containing a lit-tle over 112 million Arabic tweets that we scrapedfrom Twitter between November 20, 2011 and Jan-uary 9, 2012.
We collected the tweets by issuingthe query ?lang:ar?
against Twitter.
We utilized thetweet4j package for collection.
The tweet set had5.1 million unique words, and nearly half of themappeared only once.Our method involved doing two steps:Producing candidate transliterations: We im-plemented transliteration in a manner that is akin tothe baseline system in El-Kahki et al.
(2011).
Givenan Arabizi word waz, we produced all its possiblesegmentations along with their associated mappingsinto Arabic characters.
Valid target sequences wereretained and sorted by the product of the constituentmapping probabilities.
The top n (we picked n = 10)candidates, war1..nwith the highest probability weregenerated.
Using Bayes rule, we computed:argmaxwari?1..np(wari|waz) = p(waz|wari)p(wari) (1)where p(waz|wari) is the posterior probability ofmapping, which is computed as the product of themappings required to generate wazfrom wari, andp(wari) is the prior probability of the word.Picking the best candidate in context: We uti-lized a large word language model to help pick thebest transliteration candidate in context.
We builta trigram language model using the IRSTLM lan-guage modeling toolkit (Federico et al., 2008).
Theadvantage of this language model was that it con-tained both MSA and dialectal text.
Given the toptransliteration candidates and the language modelwe trained, we wanted to find the transliteration thatwould maximize the transliteration probability andlanguage model probability.
Given a word wiwithcandidates wi1?10, we wanted to find wi?
wi1?10that maximizes the product of the transliterationprobabilities (for all the candidates for all the wordsin the path) and the path probability, where the prob-ability of the path is estimated using the trigram lan-guage model.rank count precentage1 1,068 77.1%2 129 9.3%3 49 3.5%4 30 2.2%5 19 1.4%6 12 0.9%7 5 0.04%8 2 0.01%9 1 0.01%10 3 0.02%Not found 68 4.9%Total 1385Table 4: Results of converting from Arabizi to Arabicwith rank of correct candidatesFor testing, we used the aforementioned set of127 random Arabizi tweets containing 1,385 word.We performed two evaluations as follows:Out of context evaluation.
In this evaluation wewanted to evaluate the quality of the generated listof candidates.
Intuitively, a higher rank for the cor-rect transliteration in the list of transliterations isdesirable.
Thus, we used Mean Reciprocal Rank(MRR) to evaluate the generated candidates.
Recip-rocal Rank (RR) is simply1rankof the correct candi-date.
If the correct candidate is not in the generatedlist, we assumed that the rank was very large and weset RR = 0.
MRR is the average across all test cases.Notice that RR is 1 if the correct candidate is at po-sition 1, 0.5 if correct is at position 2, etc.
Thus thepenalty for not being at rank 1 is quite severe.For out of context evaluation, we achieved an MRRof 0.84.
Table 4 shows the breakdown of the ranksof the correct transliterations in the test set.
As canbe seen, the correct candidate was at position one77.1% of the time.
No correct candidates were found4.9% of the time.
This meant that the best possibleaccuracy that we could achieve for in context evalua-tion was 95.1%.
Further, we examined the 68 wordsfor which we did not generate a correct candidate.Table 5 categorizes the 68 words (words are pre-sented using Arabic script and Buckwalter encod-ing).
Though there has been recent efforts to codifya standard spelling convention for some Arabic di-alects (Habash et al., 2012), there is no commonlyadopted standard that is widely used on social me-dia.
Thus, some of the words that we generated had avariant spelling from the ground truth.
Also in other222cases, the correct morphological form did not existin the word list or was infrequent.
In some of thesecases, we generated morphologically related candi-dates that have an affix added or removed.
Some ex-ample affixes including coordinating conjunctions,prepositions, and feminine markers.Type Count Examplesno correct candidate 23wbenla2a7 ?and we hint to?- truth i??JK.?
wbnqHoleely ?tell me?- truth??J???
qwlylyfsanya ?in a second?- truth?JK AK ??
fy vAnypspelling variant of word 17online ?online?- truth?KCK?
@ AwnlAyn-guess?KCK @AnlAynbetshoot ?you kick?- truth ?
??K.bt$wT-guessH??K.bt$wtmorphological variant 17bt7bii ?you (fm.)
like?- truth ?
?.jJK.btHby-guess?J.jK.btHbyntesharadeeni ?you kick me out?- truth ?
?KXQ??t$rdyny-guess?KXQ??t$rdynEnglish word 4 cute; mention; nation; TVno candidate generated 4belnesbalko ?for you?- truth ???J.?
?AK.bAlnsblkmfilente5abat ?in the election?- truthHAJ.jKBA?
fAlAntxbAtmixed Arabic & English 3felguc ?in the GUC?- truth ?A?GUC fAl-GUCellive ?the live?- truth ?
@live Al-liveTable 5: Analysis of words for which we did not generatecandidatesIn context evaluation.
In this evaluation, wecomputed accuracy of producing the correct translit-erated equivalent in context.
For in context evalua-tion, if we used a baseline that used the top out-of-context choice, we would achieve 77.1% accuracy.Adding a trigram language model, we achieved anaccuracy of 88.7% (157 wrong out of 1,385).
Of thewrong guesses, 91 were completely unrelated wordsand 46 were spelling or morphological variants.5 ConclusionIn this paper, we presented methods of detectingArabizi that is mixed with English text and convert-ing Arabizi to Arabic.
For language detection weused a sequence labeler that used word and characterlevel features.
Language detection was trained andtested on datasets that were constructed from tweets.We achieved an overall accuracy of 98.5%.
For con-verting from Arabizi to Arabic, we trained a translit-eration miner that attempted to find the most likelyArabic word that could have generated an Arabiziword.
We used both character transliteration proba-bilities as well as language modeling.
We achieved88.7% transliteration accuracy.For future work, we would like to experimentwith additional training data and improved languagemodels that account for the morphological complex-ities of Arabic.
Also, the lack of commonly usedspelling conventions for Arabic dialects may war-rant detecting variant spellings of individual dialec-tal words and perhaps converting from dialectal textto MSA.ReferencesB.
Alex, A. Dubey, and F. Keller.
2007.
Using foreigninclusion detection to improve parsing performance.
InProceedings of EMNLP-CoNLLK.
Beesley.
1988.
Language Identifier: A computer pro-gram for automatic natural-language identification ofon-line text.
Proceedings of the 29th Annual Confer-ence of the American Translators Association, 4754.Kareem Darwish, Walid Magdy, and Ahmed Mourad.2012.
Language processing for arabic microblog re-trieval.
Proceedings of the 21st ACM internationalconference on Information and knowledge manage-ment.
ACM, 2012.T.
Dunning.
1994.
Statistical identification of language.Technical Report, CRL MCCS-94-273, New MexicoState University.Y.
Ehara, K. Tanaka-Ishii.
2008.
Multilingual text entryusing automatic language detection.
In Proceedings ofIJCNLP-2008.Ali El-Kahky, Kareem Darwish, Ahmed Saad Aldein,Mohamed Abd El-Wahab, Ahmed Hefny, and WaleedAmmar.
2001.
Improved transliteration mining usinggraph reinforcement.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Process-ing, pp.
1384-1393.
Association for ComputationalLinguistics, 2011.Marcello Federico, Nicola Bertoldi, and Mauro Cettolo.2008.
IRSTLM: an open source toolkit for handlinglarge scale language models.
Proceedings of Inter-speech.
2008.Nizar Habash.
2010.
Introduction to Arabic natural lan-guage processing.
Synthesis Lectures on Human Lan-guage Technologies 3.1 (2010): 1-187.223Nizar Habash, Mona T. Diab, Owen Rambow.
2012.
Con-ventional Orthography for Dialectal Arabic.
LREC,pp.
711-718.
2012.Sittichai Jiampojamarn, Kenneth Dwyer, Shane Bergsma,Aditya Bhargava, Qing Dou, Mi-Young Kim andGrzegorz Kondrak.
2010.
Transliteration Generationand Mining with Limited Training Resources.
ACLNEWS workshop 2010.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, Evan Herbst, Moses: Open Source Toolkitfor Statistical Machine Translation, Annual Meeting ofthe Association for Computational Linguistics (ACL),demonstration session, Prague, Czech Republic, June2007.Jin-Shea Kuo, Haizhou Li, Ying-Kuei Yang.
2006.
Learn-ing Transliteration Lexicons from the Web.
COLING-ACL 2006, Sydney, Australia, 11291136.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data, In Proc.
of ICML,pp.282-289, 2001.Kavi Narayana Murthy and G. Bharadwaja Kumar.
2006.Language identification from small text samples.
Jour-nal of Quantitative Linguistics 13.01 (2006): 57-80.Sara Noeman and Amgad Madkour.
2010.
Language In-dependent Transliteration Mining System Using FiniteState Automata Framework.
ACL NEWS workshop2010.Hassan Sajjad, Alexander Fraser, Helmut Schmid.
2012.A Statistical Model for Unsupervised and Semi-supervised Transliteration Mining.
ACL (1) 2012:469-477F.
Sha and F. Pereira.
2003.
Shallow parsing with condi-tional random fields, In Proc.
of HLT/NAACL 2003Raghavendra Udupa, K. Saravanan, Anton Bakalov, Ab-hijit Bhole.
2009.
?They Are Out There, If You KnowWhere to Look?
: Mining Transliterations of OOVQuery Terms for Cross-Language Information Re-trieval.
ECIR-2009, Toulouse, France, 2009.Raghavendra Udupa, Shaishav Kumar.
2010.
Hashing-based Approaches to Spelling Correction of PersonalNames.
EMNLP 2010.A.
Xafopoulos, C. Kotropoulos, G. Almpanidis, I. Pitas.2004.
Language identification in web documents usingdiscrete hidden Markov models.
Pattern Recognition,37 (3), 583594Min Zhang, A Kumaran, Haizhou Li.
2011.
Whitepaperof NEWS 2012 Shared Task on Machine Translitera-tion.
IJCNLP-2011 NEWS workshop.Min Zhang, Haizhou Li, Ming Liu, A Kumaran.
2012.Whitepaper of NEWS 2012 Shared Task on MachineTransliteration.
ACL-2012 NEWS workshop.224
