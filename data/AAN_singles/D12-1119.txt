Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 1302?1312, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsMulti-Domain Learning: When Do Domains Matter?Mahesh JoshiSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA, 15213, USAmaheshj@cs.cmu.eduMark DredzeHuman Language Technology Center of ExcellenceJohns Hopkins UniversityBaltimore, Maryland 21211mdredze@cs.jhu.eduWilliam W. CohenSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA, 15213, USAwcohen@cs.cmu.eduCarolyn P. Rose?School of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA, 15213, USAcprose@cs.cmu.eduAbstractWe present a systematic analysis of exist-ing multi-domain learning approaches with re-spect to two questions.
First, many multi-domain learning algorithms resemble ensem-ble learning algorithms.
(1) Are multi-domainlearning improvements the result of ensemblelearning effects?
Second, these algorithms aretraditionally evaluated in a balanced class la-bel setting, although in practice many multi-domain settings have domain-specific classlabel biases.
When multi-domain learningis applied to these settings, (2) are multi-domain methods improving because they cap-ture domain-specific class biases?
An under-standing of these two issues presents a cleareridea about where the field has had success inmulti-domain learning, and it suggests someimportant open questions for improving be-yond the current state of the art.1 IntroductionResearch efforts in recent years have demonstratedthe importance of domains in statistical natural lan-guage processing.
A mismatch between training andtest domains can negatively impact system accuracyas it violates a core assumption in many machinelearning algorithms: that data points are indepen-dent and identically distributed (i.i.d.).
As a result,numerous domain adaptation methods (Chelba andAcero, 2004; Daume?
III and Marcu, 2006; Blitzer etal., 2007) target settings with a training set from onedomain and a test set from another.Often times the training set itself violates the i.i.d.assumption and contains multiple domains.
In thiscase, training a single model obscures domain dis-tinctions, and separating the dataset by domains re-duces training data.
Instead, multi-domain learn-ing (MDL) can take advantage of these domain la-bels to improve learning (Daume?
III, 2007; Dredzeand Crammer, 2008; Arnold et al 2008; Finkel andManning, 2009; Zhang and Yeung, 2010; Saha et al2011).
One such example is sentiment classificationof product reviews.
Training data is available frommany product categories and while all data shouldbe used to learn a model, there are important differ-ences between the categories (Blitzer et al 2007)1.While much prior research has shown improve-ments using MDL, this paper explores what prop-erties of an MDL setting matter.
Are previous im-provements from MDL algorithms discovering im-portant distinctions between features in different do-mains, as we would hope, or are other factors con-tributing to learning success?
The key question ofthis paper is: when do domains matter?Towards this goal we explore two issues.
First,we explore the question of whether domain distinc-tions are used by existing MDL algorithms in mean-ingful ways.
While differences in feature behaviorsbetween domains will hurt performance (Blitzer etal., 2008; Ben-David et al 2009), it is not clearif the improvements in MDL algorithms can be at-tributed to correcting these errors, or whether theyare benefiting from something else.
In particular,there are many similarities between MDL and en-semble methods, with connections to instance bag-1Blitzer et al(2007) do not consider the MDL setup, theyconsider a single source domain, and a single target domain,with little or no labeled data available for the target domain.1302ging, feature bagging and classifier combination.
Itmay be that gains in MDL are the usual ensemblelearning improvements.Second, one simple way in which domains canchange is the distribution of the prior over the la-bels.
For example, reviews of some products may bemore positive on average than reviews of other prod-uct types.
Simply capturing this bias may accountfor significant gains in accuracy, even though noth-ing is learned about the behavior of domain-specificfeatures.
Most prior work considers datasets withbalanced labels.
However, in real world applica-tions, where labels may be biased toward some val-ues, gains from MDL could be attributed to simplymodeling domain-specific bias.
A practical advan-tage of such a result is ease of implementation andthe ability to scale to many domains.Overall, irrespective of the answers to these ques-tions, a better understanding of the performance ofexisting MDL algorithms in different settings willprovide intuitions for improving the state of the art.2 Multi-Domain LearningIn the multi-domain learning (MDL) setting, exam-ples are accompanied by both a class label and a do-main indicator.
Examples are of the form (xi, y,di),where xi ?
RN , di is a domain indicator, xi isdrawn according to a fixed domain-specific distri-bution Ddi , and yi is the label (e.g.
yi ?
{?1,+1}for binary labels).
Standard learning ignores di, butMDL uses these to improve learning accuracy.Why should we care about the domain label?
Do-main differences can introduce errors in a numberof ways (Ben-David et al 2007; Ben-David et al2009).
First, the domain-specific distributions Ddican differ such that they favor different features, i.e.p(x) changes between domains.
As a result, somefeatures may only appear in one domain.
This aspectof domain difference is typically the focus of un-supervised domain adaptation (Blitzer et al 2006;Blitzer et al 2007).
Second, the features may be-have differently with respect to the label in each do-main, i.e.
p(y|x) changes between domains.
As aresult, a learning algorithm cannot generalize the be-havior of features from one domain to another.
Thekey idea behind many MDL algorithms is to targetone or both of these properties of domain differenceto improve performance.Prior approaches to MDL can be broadly catego-rized into two classes.
The first set of approaches(Daume?
III, 2007; Dredze et al 2008) introduce pa-rameters to capture domain-specific behaviors whilepreserving features that learn domain-general be-haviors.
A key of these methods is that they do notexplicitly model any relationship between the do-mains.
Daume?
III (2007) proposes a very simple?easy adapt?
approach, which was originally pro-posed in the context of adapting to a specific targetdomain, but easily generalizes to MDL.
Dredze et al(2008) consider the problem of learning how to com-bine different domain-specific classifiers such thatbehaviors common to several domains can be cap-tured by a shared classifier, while domain-specificbehavior is still captured by the individual classi-fiers.
We describe both of these approaches in ?
3.2.The second set of approaches to MDL introducean explicit notion of relationship between domains.For example, Cavallanti et al(2008) assume a fixedtask relationship matrix in the context of onlinemulti-task learning.
The key assumption is that in-stances from two different domains are half as muchrelated to each other as two instances from the samedomain.
Saha et al(2011) improve upon the ideaof simply using a fixed task relationship matrix byinstead learning it adaptively.
They derive an onlinealgorithm for updating the task interaction matrix.Zhang and Yeung (2010) derive a convex formu-lation for adaptively learning domain relationships.We describe their approach in ?
3.2.
Finally, Daume?III (2009) proposes a joint task clustering and multi-task/multi-domain learning setup, where instead ofjust learning pairwise domain relationships, a hier-archical structure among them is inferred.
Hierar-chical clustering of tasks is performed in a Bayesianframework, by imposing a hierarchical prior on thestructure of the task relationships.In all of these settings, the key idea is to learnboth domain-specific behaviors and behaviors thatgeneralize between (possibly related) domains.3 DataTo support our analysis we develop several empir-ical experiments.
We first summarize the datasetsand methods that we use in our experiments, then1303proceed to our exploration of MDL.3.1 DatasetsA variety of multi-domain datasets have been usedfor demonstrating MDL improvements.
In this pa-per, we focus on two datasets representative of manyof the properties of MDL.Amazon (AMAZON) Our first dataset is the Multi-Domain Amazon data (version 2.0), first introducedby Blitzer et al(2007).
The task is binary sentimentclassification, in which Amazon product reviews arelabeled as positive or negative.
Domains are definedby product categories.
We select the four domainsused in most studies: books, dvd, electronicsand kitchen appliances.The original dataset contained 2,000 reviews foreach of the four domains, with 1,000 positive and1,000 negative reviews per domain.
Feature extrac-tion follows Blitzer et al(2007): we use case insen-sitive unigrams and bigrams, although we removerare features (those that appear less than five timesin the training set).
The reduced feature set was se-lected given the sensitivity to feature size of some ofthe MDL methods.ConVote (CONVOTE) Our second dataset is takenfrom segments of speech from United StatesCongress floor debates, first introduced by Thomaset al(2006).
The binary classification task on thisdataset is that of predicting whether a given speechsegment supports or opposes a bill under discus-sion in the floor debate.
We select this dataset be-cause, unlike the AMAZON data, CONVOTE can bedivided into domains in several ways based on dif-ferent metadata attributes available with the dataset.We consider two types of domain divisions: the billidentifier and the political party of the speaker.
Di-vision based on the bill creates domain differencesin that each bill has its own topic.
Division based onpolitical party implies preference for different issuesand concerns, which manifest as different language.We refer to these datasets as BILL and PARTY.We use Version 1.1 of the CONVOTE dataset,available at http://www.cs.cornell.edu/home/llee/data/convote.html.
Morespecifically, we combine the training, developmentand test folds from the data stage three/ ver-sion, and sub-sample to generate different versionsof the dataset required for our experiments.
ForBILL we randomly sample speech segments fromthree different bills.
The three bills and the numberof instances for each were chosen such that we havesufficient data in each fold for every experiment.For PARTY we randomly sample speech segmentsfrom the two major political parties (Democrats andRepublicans).
Feature processing was identical toAMAZON, except that the threshold for feature re-moval was two.3.2 Learning Methods and FeaturesWe consider three MDL algorithms, two are repre-sentative of the first approach and one of the secondapproach (learning domain similarities) (?2).
We fa-vored algorithms with available code or that werestraightforward to implement, so as to ensure repro-ducibility of our results.FEDA Frustratingly easy domain adaptation(FEDA) (Daume?
III, 2007; Daume?
III et al 2010b;Daume?
III et al 2010a) is an example of a classifiercombination approach to MDL.
The feature spaceis a cross-product of the domain and input features,augmented with the original input features (sharedfeatures).
Prediction is effectively a linear combina-tion of a set of domain-specific weights and sharedweights.
We combine FEDA with both the SVMand logistic regression algorithms described belowto obtain FEDA-SVM and FEDA-LR.MDR Multi-domain regularization (MDR) (Dredzeand Crammer, 2008; Dredze et al 2009) extends theidea behind classifier combination by explicitly for-mulating a classifier combination scheme based onConfidence-Weighted learning (Dredze et al 2008).Additionally, classifier updates (which happen inan online framework) contain an explicit constraintthat the combined classifier should perform well onthe example.
Dredze et al(2009) consider severalvariants of MDR.
We select the two best perform-ing methods: MDR-L2, which uses the underlyingalgorithm of Crammer et al(2008), and MDR-KL,which uses the underlying algorithm of Dredze et al(2008).
We follow their approach to classifier train-ing and parameter optimization.MTRL The multi-task relationship learning(MTRL) approach proposed by Zhang and Yeung1304(2010) achieves states of the art performance onmany MDL tasks.
This method is representativeof methods that learn similarities between domainsand in turn regularize domain-specific parametersaccordingly.
The key idea in their work is the useof a matrix-normal distribution p(X|M ,?,?)
asa prior on the matrix W created by column-wisestacking of the domain-specific classifier weightvectors.
?
represents the covariance matrix for thevariables along the columns of X .
When used asa prior over W it models the covariance betweenthe domain-specific classifiers (and therefore thetasks).
?
is learned jointly with the domain-specificclassifiers.
This method has similar benefits toFEDA in terms of classifier combination, but alsoattempts to model domain relationships.
We usethe implementation of MTRL made available by theauthors2.
For parameter tuning, we perform a gridsearch over the parameters ?1 and ?2, using the fol-lowing values for each (a total of 36 combinations):{0.00001, 0.0001, 0.001, 0.01, 0.1, 1}.In addition to these multi-task learning methods,we consider a common baseline: ignoring the do-main distinctions and learning a single classifierover all the data.
This reflects single-domain learn-ing, in which no domain knowledge is used and willindicate baseline performance for all experiments.While some earlier research has included a sepa-rate one classifier per domain baseline, it almost al-ways performs worse, since splitting the domainsprovides much less data to each classifier (Dredzeet al 2009).
So we omit this baseline for simplicity.To obtain a single classifier we use two classifica-tion algorithms: SVMs and logistic regression.Support Vector Machines A single SVM runover all the training data, ignoring domain labels.We use the SVM implementation available in the LI-BLINEAR package (Fan et al 2008).
In particular,we use the L2-regularized L2-loss SVM (option -s1 in version 1.8 of LIBLINEAR, and also option -B1 for including a standard bias feature).
We tunethe SVM using five-fold stratified cross-validationon the training set, using the following values forthe trade-off parameterC: {0.0001, 0.001, 0.01, 0.1,0.2, 0.3, 0.5, 1}.2http://www.cse.ust.hk/?zhangyu/codes/MTRL.zipLogistic Regression (LR) A single logistic re-gression model run over all the training data, ignor-ing domain labels.
Again, we use the L2-regularizedLR implementation available in the LIBLINEARpackage (option -s 0, and also option -B 1).
Wetune the LR model using the same strategy as theone used for SVM above, including the values of thetrade-off parameter C.For all experiments, we measure average accu-racy overK-fold cross-validation, using 10 folds forAMAZON, and 5 folds for both BILL and PARTY.4 When Do Domains Matter?We now empirically explore two questions regardingthe behavior of MDL.4.1 Ensemble LearningQuestion: Are MDL improvements the result ofensemble learning effects?Many of the MDL approaches bear a strikingresemblance to ensemble learning.
Traditionally,ensemble learning combines the output from sev-eral different classifiers to obtain a single improvedmodel (Maclin and Opitz, 1999).
It is well estab-lished that ensemble learning, applied on top of adiverse array of quality classifiers, can improve re-sults for a variety of tasks.
The key idea behindensemble learning, that of combining a diverse ar-ray of models, has been applied to settings in whichdata preprocessing is used to create many differentclassifiers.
Examples include instance bagging andfeature bagging (Dietterich, 2000).The core idea of using diverse inputs in makingclassification decisions is common in the MDL liter-ature.
In fact, the top performing and only success-ful entry to the 2007 CoNLL shared task on domainadaptation for dependency parsing was a straightfor-ward implementation of ensemble learning by cre-ating variants of parsers (Sagae and Tsujii, 2007).Many MDL algorithms, among them Dredze andCrammer (2008), Daume?
III (2009), Zhang and Ye-ung (2010) and Saha et al(2011), all include somenotion of learning domain-specific classifiers on thetraining data, and combining them in the best waypossible.
To be clear, we do not claim that theseapproaches can be reduced to an existing ensem-ble learning algorithm.
There are crucial elements1305in each of these algorithms that separate them fromexisting ensemble learning algorithms.
One exam-ple of such a distinction is the learning of domainrelationships by both Zhang and Yeung (2010) andSaha et al(2011).
However, we argue that theircore approach, that of combining parameters that aretrained on variants of the data (all data or individualdomains), is an ensemble learning idea.Consider instance bagging, in which multipleclassifiers are each trained on random subsets of thedata.
The resulting classifiers are then combinedto form a final model.
In MDL, we can considereach domain a subset of the data, albeit non-randomand non-overlapping.
The final model combines thedomain-specific parameters and parameters trainedon other instances, which in the case of FEDA are theshared parameters.
In this light, these methods are acomplex form of instance bagging, and their devel-opment could be justified from this perspective.However, given this justification, are improve-ments from MDL simply the result of standard en-semble learning effects, or are these methods re-ally learning something about domain behavior?
Ifknowledge of domain was withheld from the algo-rithm, could we expect similar improvements?
Aswe will do in each empirical experiment, we proposea contrarian hypothesis:Hypothesis: Knowledge of domains is irrelevantfor MDL.Empirical Evaluation We evaluate this hypothe-sis as follows.
We begin by constructing a true MDLsetting, in which we attempt to improve accuracythrough knowledge of the domains.
We will applythree MDL algorithms (FEDA, MDR, and MTRL) toour three multi-domain datasets (AMAZON, BILL,and PARTY) and compare them against a single clas-sifier baseline.
We will then withhold knowledgeof the true domains from these algorithms and in-stead provide them with random ?pseudo-domains,?and then evaluate the change in their behavior.
Thequestion is whether we can obtain similar benefitsby ignoring domain labels and relying strictly on anensemble learning motivation (instance bagging).For the ?True Domain?
setting, we apply theMDL algorithms as normal.
For the ?Random Do-main?
setting, we randomly shuffle the domain la-bels within a given class label within each fold, thusmaintaining the same number of examples for eachdomain label, and also retaining the same class dis-tribution within each randomized domain.
The re-sulting ?pseudo-domains?
are then similar to ran-dom subsets of the data used in ensemble learning.Following the standard practice in previous work,for this experiment we use a balanced number ofexamples from each domain and a balanced num-ber of positive and negative labels (no class bias).For AMAZON (4 domains), we have 10 folds of 400examples per fold, for BILL (3 domains) 5 folds of60 examples per fold, and for PARTY (2 domains) 5folds of 80 examples per fold.
In the ?Random Do-main?
setting, since we are randomizing the domainlabels, we increase the number of trials.
We repeateach cross-validation experiment 5 times with differ-ent randomization of the domain labels each time.Results Results are shown in Table 1.
The firstrow shows absolute (average) accuracy for a singleclassifier trained on all data, ignoring domain dis-tinctions.
The remaining cells indicate absolute im-provements against the baseline.First, we note for the well-studied AMAZONdataset that our results with true domains are con-sistent with the previous literature.
FEDA is knownto not improve upon a single classifier baseline forthat dataset (Dredze et al 2009).
Both MDR-L2 andMDR-KL improve upon the single classifier baseline,again as per Dredze et al(2009).
And finally, MTRLalso improves upon the single classifier baseline.
Al-though the MTRL improvement is not as dramatic asin the original paper3, the average accuracy that weachieve for MTRL (84.2%) is better than the best av-erage accuracy in the original paper (83.65%).The main comparison to make in Table 1 is be-tween having knowledge of true domains or not.
?Random Domain?
in the table is the case where do-main identifiers are randomly shuffled within a givenfold.
Ignoring the significance test results for now,overall the results indicate that knowing the true do-mains is useful for MDL algorithms.
Randomiz-ing the domains does not work better than knowingtrue domains in any case.
However, in all exceptone case, the improvements of MDL algorithms are3This might be due to a different version of the dataset beingused in a cross-validation setup, rather than their train/test setup,and also because of differences in baseline approaches.1306AMAZON BILL PARTYSVM LR SVM LR SVM LRSingle Classifier83.93% 83.78% 66.67% 68.00% 62.75% 64.00%FEDATrue Domain -0.35 -0.10 +2.33 + 1.00 +4.25 N +1.25Random Domain -1.30 H -1.02 H -1.20 -2.07 -2.05 -2.10MDR-L2True Domain +1.87 N +2.02 N +0.00 -1.33 +2.25 +1.00Random Domain +0.91 N +1.07 N -2.67 -4.00 -2.80 -4.05MDR-KLTrue Domain +1.85 N +2.00 N +1.00 -0.33 +3.00 +1.75Random Domain +1.36 N +1.51 N +0.60 -0.73 -1.30 -2.55 HMTRLTrue Domain +0.27 +0.42 +0.67 -0.67 +1.50 +0.25Random Domain -0.37 -0.21 -1.47 -2.80 -3.55 -4.80Table 1: A comparison between MDL methods with access to the ?True Domain?
labels and methods thatuse ?Random Domain?
information, essentially ensemble learning.
The first row has raw accuracy numbers,whereas the remaining entries are absolute improvements over the baseline.
N: Significantly better than thecorresponding SVM or LR baseline, with p < 0.05, using a paired t-test.
H: Significantly worse thancorresponding baseline, with p < 0.05, using a paired t-test.significantly better only for the AMAZON dataset4.And interestingly, exactly in the same case, ran-domly shuffling the domains also gives significantimprovements compared to the baseline, showingthat there is an ensemble learning effect in operationfor MDR-L2 and MDR-KL on the AMAZON dataset.For FEDA, randomizing the domains significantlyhurts its performance on the AMAZON data, as isthe case for MDR-KL on the PARTY data.
Therefore,while our contrarian hypothesis about irrelevance ofdomains is not completely true, it is indeed the casethat some MDL methods benefit from the ensemblelearning effect.A second observation to be made from these re-sults is that, while all of empirical research on MDLassumes the definition of domains as a given, thequestion of how to split a dataset into domains givenvarious metadata attributes is still open.
For exam-ple, in our experiments, in general, using the po-litical party as a domain distinction gives us moreimprovements over the corresponding baseline ap-proach5.We provide a detailed comparison of using true4Some numbers in Table 1 might appear to be significant,but are not.
That is because of high variance in the performanceof the methods across the different folds.5The BILL and the PARTY datasets are not directly compa-rable to each other, although the prediction task is the same.vs.
randomized domains in Table 6, after presentingthe second set of experimental results.4.2 Domain-specific Class BiasQuestion: Are MDL methods improving becausethey capture domain-specific class biases?In previous work, and the above section, experi-ments have assumed a balanced dataset in terms ofclass labels.
It has been in these settings that MDLmethods improve.
However, this is an unrealistic as-sumption.
Even in our datasets, the original versionsdemonstrated class bias: Amazon product reviewsare generally positive, votes on bills are rarely tied,and political parties vote in blocs.
While it is com-mon to evaluate learning methods on balanced data,and then adjust for imbalanced real world datasets, itis unclear what effect domain-specific class bias willhave on MDL methods.
Domains can differ in theirproportion of examples of different classes.
For ex-ample, it is quite likely that less controversial bills inthe United States Congress will have more yes votesthan controversial bills.
Similarly, if instead of thecategory of a product, its brand is considered as a do-main, it is likely that some brands receive a higherproportion of positive reviews than others.Improvements from MDL in such settings maysimply be capturing domain-specific class biases.1307domain class cb1 cb2 cb3 cb4AMAZONb- 20 80 60 40+ 80 20 40 60d- 40 20 80 60+ 60 80 20 40e- 60 40 20 80+ 40 60 80 20k- 80 60 40 20+ 20 40 60 80BILL031N 16 4 8 12Y 4 16 12 8088N 12 16 4 8Y 8 4 16 12132N 8 12 16 4Y 12 8 4 16PARTYDN 10 30 15 25Y 30 10 25 15RN 30 10 25 15Y 10 30 15 25Table 2: The table shows the distribution of in-stances across domains and class labels within onefold of each of the datasets, for four different classbias trials.
These datasets with varying class biasacross domains were used for the experiments de-scribed in ?4.2Consider two domains, where each domain is biasedtowards the opposite label.
In this case, domain-specific parameters may simply be capturing the biastowards the class label, increasing the weight uni-formly of features predictive of the dominant class.Similarly, methods that learn domain similarity maybe learning class bias similarity.Why does the effectiveness of these domain-specific bias parameters matter?
First, if capturingdomain-specific class bias is the source of improve-ment, there are much simpler methods for learningthat can be just as effective.
This would be espe-cially important in settings where we have many do-mains, and learning domain-specific parameters foreach feature becomes infeasible.
Second, if classbias accounted for most of the improvement in learn-ing, it suggests that such settings could be amenableto unsupervised adaptation of the bias parameters.Hypothesis: MDL largely capitalizes ondomain-specific class bias.Empirical Evaluation To evaluate our hypothe-sis, for each of our three datasets we create 4 randomversions, each with some domain-specific class-bias.A summary of the dataset partitions is shown inTable 2.
For example, for the AMAZON dataset,we create 4 versions (cb1 .
.
.
cb4), where each do-main has 100 examples per fold and each domainhas a different balance between positive and nega-tive classes.
For each of these settings, we conducta 10-fold cross validation experiment, then averagethe CV results for each of the 4 settings.
The re-sulting accuracy numbers therefore reflect an aver-age across many types of bias, each evaluated manytimes.
We do a similar experiment for the BILL andPARTY datasets, except we use 5-fold CV.In addition to the multi-domain and baselinemethods, we add a new baseline: DOM-ID.
In thissetting, we augment the baseline classifier (whichignores domain labels) with a new feature that in-dicates the domain label.
While we already includea general bias feature, as is common in classifica-tion tasks, these new features will capture domain-specific bias.
This is the only change to the base-line classifier, so improvements over the baseline areindicative of the change in domain-bias that can becaptured using these simple features.Results Results are shown in Table 3.
The tablefollows the same structure as Table 1, with the ad-dition of the results for the DOM-ID approach.
Wefirst examine the efficacy of MDL in this setting.
Anobservation that is hard to miss is that MDL resultsin these experiments show significant improvementsin almost all cases, as compared to only a few casesin Table 1, despite the fact that even the baseline ap-proaches have a higher accuracy.
This shows thatMDL results can be highly influenced by systematicdifferences in class bias across domains.
Note thatthere is also a significant negative influence of classbias on MTRL for the AMAZON data.A comparison of the MDL results on true domainsto the DOM-ID baseline gives us an idea of howmuch MDL benefits purely from class bias differ-ences across domains.
We see that in most cases,about half of the improvement seen in MDL is ac-counted for by a simple baseline of using the do-main identifier as a feature, and all but one of theimprovements from DOM-ID are significant.
This1308AMAZON BILL PARTYSVM LR SVM LR SVM LRSingle Classifier85.52% 85.46% 70.50% 70.67% 65.44% 65.81%FEDATrue Domain +0.11 +0.31 +4.25 N +4.00 N +4.81 N +4.69 NRandom Domain +0.94 N +1.03 N +3.68 N +4.03 N +4.24 +3.73MDR-L2True Domain +0.92 N +0.98 N +4.42 N +4.25 N +1.31 +0.94Random Domain +1.86 N +1.92 N +3.93 N +3.77 N +0.65 +0.28MDR-KLTrue Domain +1.54 N +1.59 N +5.17 N +5.00 N +4.25 N +3.88 NRandom Domain +2.84 N +2.90 N +4.13 N +3.97 N +3.81 N +3.44MTRLTrue Domain -1.22 H -1.17 H +4.50 N +4.33 N +6.44 N +6.06 NRandom Domain -0.69 H -0.63 H +3.53 N +3.37 N +4.87 N +4.50 NDOM-IDTrue Domain +0.36 +0.38 N +2.83 N +2.75 N +3.75 N +4.00 NRandom Domain +1.73 N +1.76 N +4.50 N +4.98 N +5.24 N +5.31 NTable 3: A comparison between MDL methods with class biased data.
Similar to the setup where weevaluate the ensemble learning effect, we have a setting of using randomized domains.
N: Significantlybetter than the corresponding SVM or LR baseline, with p < 0.05, using a paired t-test.
H: Significantlyworse than corresponding baseline, with p < 0.05, using a paired t-test.suggests that in a real-world scenario where differ-ence in class bias across domains is quite likely, it isuseful to consider DOM-ID as a simple baseline thatgives good empirical performance.
To our knowl-edge, using this approach as a baseline is not stan-dard practice in MDL literature.Finally, we also include the ?Random Domain?evaluation in the our class biased version of exper-iments.
Each ?Random Domain?
result in Table 3is an average over 20 cross-validation runs (5 ran-domized trials for each of the four class biased tri-als cb1 .
.
.
cb4).
This setup combines the effectsof ensemble learning and bias difference across do-mains.
As seen in the table, for MDL algorithms theresults are consistently better as compared to know-ing the true domains for the AMAZON dataset.
Forthe other datasets, the performance after randomiz-ing the domains is still significantly better than thebaseline.
This evaluation on randomized domainsfurther strengthens the conclusion that differences inbias across domains play an important role, even inthe case of noisy domains.
Looking at the perfor-mance of DOM-ID with randomized domains, wesee that in all cases the DOM-ID baseline performsbetter with randomized domains.
While the dif-ference is significant mostly only on the AMAZONdomain class cb5 cb6 cb7 cb8AMAZONb- 20 40 60 80+ 80 60 40 20d- 20 40 60 80+ 80 60 40 20e- 20 40 60 80+ 80 60 40 20k- 20 40 60 80+ 20 40 60 80Table 4: The table shows the distribution of in-stances across domains and class labels within onefold of the AMAZON dataset, for four different classbias trials.
For the BILL and PARTY datasets, similarfolds with consistent bias were created (number ofexamples used was different).
These datasets withconsistent class bias across domains were used forthe experiments described in ?4.2.1dataset (details in Table 6, columns under ?VaryingClass Bias,?)
this trend is still counter-intuitive.
Wesuspect this might be because randomization createsa noisy version of the domain labels, which helpslearners to avoid over-fitting that single feature.13094.2.1 Consistent Class BiasWe also performed a set of experiments that ap-ply MDL algorithms to a setting where the datasetshave different class biases (unlike the experimentsreported in Table 1, where the classes are balanced),but, unlike the experiments reported in Table 3, theclass bias is the same within each of the domains.We refer to this as the case of consistent class biasacross domains.
The distribution of classes withineach domain within each fold is shown in Table 4.The results for this set of experiments are reportedin Table 5.
The structure of Table 5 is identical tothat of Table 1.
Comparing these results to thosein Table 1, we can see that in most cases the im-provements seen using MDL algorithms are lowerthan those seen in Table 1.
This is likely due tothe higher baseline performance in the consistentclass bias case.
A notable difference is in the per-formance of MTRL ?
it is significantly worse forthe AMAZON dataset, and significantly better for thePARTY dataset.
For the AMAZON dataset, we be-lieve that the domain distinctions are less meaning-ful, and hence forcing MTRL to learn the relation-ships results in lower performance.
For the PARTYdataset, in the case of a class-biased setup, know-ing the party is highly predictive of the vote (in theoriginal CONVOTE dataset, Democrats mostly vote?no?
and Republicans mostly vote ?yes?
), and thisis rightly exploited by MTRL.4.2.2 True vs. Randomized DomainsIn Table 6 we analyze the difference in perfor-mance of MDL methods when using true vs. ran-domized domain information.
For the three sets ofresults reported earlier, we evaluated whether usingtrue domains as compared to randomized domainsgives significantly better, significantly worse orequal performance.
Significance testing was doneusing a paired t-test with ?
= 0.05 as before.
As thetable shows, for the first set of results where the classlabels were balanced (overall, as well as within eachdomain), using true domains was significantly bettermostly only for the AMAZON dataset.
FEDA-SVMwas the only approach that was consistently betterwith true domains across all datasets.
Note, how-ever, that it was significantly better than the baselineapproach only for PARTY.For the second set of results (Table 3) where theclass bias varied across the different domains, us-ing true domains was either no different from usingrandomized domains, or it was significantly worse.In particular, it was consistently significantly worseto use true domains on the AMAZON dataset.
Thisquestions the utility of domains on the AMAZONdataset in the context of MDL in a domain-specificclass bias scenario.
Since randomizing the domainsworks better for all of the MDL methods on AMA-ZON, it suggests that an ensemble learning effectis primarily responsible for the significant improve-ments seen on the AMAZON data, when evaluated ina domain-specific class bias setting.Finally, for the case of consistent class bias acrossdomains, the trend is similar to the case of no classbias ?
using true domains is useful.
This tablefurther supports the conclusion that domain-specificclass bias highly influences multi-domain learning.5 Discussion and Open QuestionsOur analysis of MDL algorithms revealed newtrends that suggest further avenues of exploration.We suggest three open questions in response.Question: When are MDL methods most effective?Our empirical results suggest that MDL can be moreeffective in settings with domain-specific class bi-ases.
However, we also saw differences in im-provements for each method, and for different do-mains.
Differences emerge between the AMAZONand CONVOTE datasets in terms of the ensemblelearning hypothesis.
While there has been some the-oretical analyses on the topic of MDL (Ben-Davidet al 2007; Ben-David et al 2009; Mansour etal., 2009; Daume?
III et al 2010a), our results sug-gest performing new analyses that relate ensemblelearning results with the MDL setting.
These anal-yses could provide insights into new algorithms thatcan take advantage of the specific properties of eachmulti-domain setting.Question: What makes a good domain for MDL?To the best of our knowledge, previous work hasassumed that domain identities are provided to thelearning algorithm.
However, in reality, there maybe many ways to split a dataset into domains.
Forexample, consider the CONVOTE dataset, which wesplit both by BILL and PARTY.
The choice of splits1310AMAZON BILL PARTYSVM LR SVM LR SVM LRSingle Classifier86.06% 86.22% 76.42% 75.58% 69.31% 68.38%FEDATrue Domain -0.25 -0.33 -0.83 +0.25 +0.88 +1.25Random Domain -1.17 H -1.26 H -1.33 -0.82 -0.55 -0.04MDR-L2True Domain +0.39 N +0.23 -0.42 +0.42 -2.12 -1.19Random Domain -0.38 -0.53 H -3.57 -2.73 -4.30 H -3.36 HMDR-KLTrue Domain +0.81 N +0.65 N -0.83 +0.00 +1.31 +2.25 NRandom Domain +0.22 +0.06 -1.90 -1.07 -0.60 +0.34MTRLTrue Domain -1.52 H -1.68 H -1.92 -1.08 +3.12 N +4.06 NRandom Domain -2.12 H -2.28 H -0.95 -0.12 +0.19 +1.12 NTable 5: A comparison between MDL methods with data that have a consistent class bias across domains.Similar to the setup where we evaluate the ensemble learning effect, we have a setting of using randomizeddomains.
N: Significantly better than the corresponding SVM or LR baseline, with p < 0.05, using a pairedt-test.
H: Significantly worse than corresponding baseline, with p < 0.05, using a paired t-test.MDL Method No Class Bias (Tab.
1) Varying Class Bias (Tab.
3) Consistent Class Bias (Tab.
5)better worse equal better worse equal better worse equalFEDA-SVM AM, BI, PA AM BI, PA AM, PA BIFEDA-LR AM BI, PA AM BI, PA AM, BI PAMDR-L2 AM BI, PA AM BI, PA AM, BI PAMDR-KL PA AM, BI AM BI, PA AM, PA BIMTRL AM BI, PA AM BI, PA AM, PA BIDOM-ID-SVM ?
?
?
AM BI, PA ?
?
?DOM-ID-LR ?
?
?
AM, BI PA ?
?
?Table 6: The table shows the datasets (AM:AMAZON, BI:BILL, PA:PARTY) for which a given MDL methodusing true domain information was significantly better, significantly worse, or not significantly different(equal) as compared to using randomized domain information with the same MDL method.impacted MDL.
This poses new questions: whatmakes a good domain?
How should we choose to di-vide data along possible metadata properties?
If wecan gain improvements simply by randomly creat-ing new domains (?Random Domain?
setting in ourexperiments) then there may be better ways to takeadvantage of the provided metadata for MDL.Question: Can we learn class-bias forunsupervised domain adaptation?Experiments with domain-specific class biases re-vealed that a significant part of the improvementscould be achieved by adding domain-specific biasfeatures.
Limiting the multi-domain improvementsto a small set of parameters raises an interestingquestion: can these parameters be adapted to a newdomain without labeled data?
Traditionally, domainadaptation without target domain labeled data hasfocused on learning the behavior of new features;beliefs about existing feature behaviors could not becorrected without new training data.
However, bycollapsing the adaptation into a single bias parame-ter, we may be able to learn how to adjust this pa-rameter in a fully unsupervised way.
This wouldopen the door to improvements in this challengingsetting for real world problems where class bias wasa significant factor.AcknowledgmentsResearch presented here is supported by the Officeof Naval Research grant number N000141110221.1311ReferencesAndrew Arnold, Ramesh Nallapati, and William W. Co-hen.
2008.
Exploiting Feature Hierarchy for TransferLearning in Named Entity Recognition.
In Proceed-ings of ACL-08: HLT, pages 245?253.Shai Ben-David, John Blitzer, Koby Crammer, and Fer-nando Pereira.
2007.
Analysis of representations fordomain adaptation.
In Proceedings of NIPS 2006.Shai Ben-David, John Blitzer, Koby Crammer, AlexKulesza, Fernando Pereira, and Jennifer WortmanVaughan.
2009.
A theory of learning from differentdomains.
Machine Learning.John Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain Adaptation with Structural Correspon-dence Learning.
In Proceedings of the 2006 Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 120?128.John Blitzer, Mark Dredze, and Fernando Pereira.
2007.Biographies, Bollywood, Boom-boxes and Blenders:Domain Adaptation for Sentiment Classification.
InProceedings of the 45th Annual Meeting of the Associ-ation of Computational Linguistics, pages 440?447.John Blitzer, Koby Crammer, Alex Kulesza, FernandoPereira, and Jennifer Wortman.
2008.
LearningBounds for Domain Adaptation.
In Advances in Neu-ral Information Processing Systems (NIPS 2007).Giovanni Cavallanti, Nicolo` Cesa-Bianchi, and ClaudioGentile.
2008.
Linear Algorithms for Online Multi-task Classification.
In Proceedings of COLT.Ciprian Chelba and Alex Acero.
2004.
Adaptation ofMaximum Entropy Capitalizer: Little Data Can Helpa Lot.
In Dekang Lin and Dekai Wu, editors, Proceed-ings of EMNLP 2004, pages 285?292.Koby Crammer, Mark Dredze, and Fernando Pereira.2008.
Exact convex confidence-weighted learning.
InAdvances in Neural Information Processing Systems(NIPS).Hal Daume?
III and Daniel Marcu.
2006.
Domain adap-tation for statistical classifiers.
Journal of ArtificialIntelligence Research, 26(1):101?126.Hal Daume?
III, Abhishek Kumar, and Avishek Saha.2010a.
A Co-regularization Based Semi-supervisedDomain Adaptation.
In Neural Information Process-ing Systems.Hal Daume?
III, Abhishek Kumar, and Avishek Saha.2010b.
Frustratingly Easy Semi-Supervised DomainAdaptation.
In Proceedings of the ACL 2010 Work-shop on Domain Adaptation for Natural LanguageProcessing, pages 53?59.Hal Daume?
III.
2007.
Frustratingly Easy Domain Adap-tation.
In Proceedings of the 45th Annual Meeting ofthe Association of Computational Linguistics, pages256?263.Hal Daume?
III.
2009.
Bayesian multitask learning withlatent hierarchies.
In Proceedings of the Twenty-FifthConference on Uncertainty in Artificial Intelligence.Thomas G. Dietterich.
2000.
An experimental compar-ison of three methods for constructing ensembles ofdecision trees: Bagging, boosting, and randomization.Machine Learning, 40:139?157.Mark Dredze and Koby Crammer.
2008.
Online meth-ods for multi-domain learning and adaptation.
Pro-ceedings of the Conference on Empirical Methods inNatural Language Processing - EMNLP ?08.Mark Dredze, Koby Crammer, and Fernando Pereira.2008.
Confidence-weighted linear classification.
Pro-ceedings of the 25th international conference on Ma-chine learning - ICML ?08.Mark Dredze, Alex Kulesza, and Koby Crammer.
2009.Multi-domain learning by confidence-weighted pa-rameter combination.
Machine Learning, 79(1-2).Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-ruiWang, and Chih-Jen Lin.
2008.
LIBLINEAR : A Li-brary for Large Linear Classification.
Journal of Ma-chine Learning Research, 9:1871?1874.Jenny R. Finkel and Christopher D. Manning.
2009.
Hi-erarchical Bayesian Domain Adaptation.
In Proceed-ings of Human Language Technologies: The 2009 An-nual Conference of the North American Chapter ofthe Association for Computational Linguistics, pages602?610.Richard Maclin and David Opitz.
1999.
Popular Ensem-ble Methods: An Empirical Study.
Journal of Artifi-cial Intelligence Research, 11:169?198.Yishay Mansour, Mehryar Mohri, and Afshin Ros-tamizadeh.
2009.
Domain Adaptation with MultipleSources.
In Proceedings of NIPS 2008, pages 1041?1048.Kenji Sagae and Jun?ichi Tsujii.
2007.
Dependencyparsing and domain adaptation with lr models andparser ensembles.
In Conference on Natural LanguageLearning (Shared Task).Avishek Saha, Piyush Rai, Hal Daume?
III, and SureshVenkatasubramanian.
2011.
Online learning of mul-tiple tasks and their relationships.
In Proceedings ofAISTATS 2011.Matt Thomas, Bo Pang, and Lillian Lee.
2006.
Getout the vote: Determining support or opposition fromCongressional floor-debate transcripts.
In Proceed-ings of EMNLP, pages 327?335.Yu Zhang and Dit-Yan Yeung.
2010.
A Convex Formu-lation for Learning Task Relationships in Multi-TaskLearning.
In Proceedings of the Proceedings of theTwenty-Sixth Conference Annual Conference on Un-certainty in Artificial Intelligence (UAI-10).1312
