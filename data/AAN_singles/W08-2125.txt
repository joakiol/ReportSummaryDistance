CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 193?197Manchester, August 2008Collective Semantic Role Labelling with Markov LogicSebastian Riedel Ivan Meza-RuizInstitute for Communicating and Collaborative SystemsSchool of InformaticsUniversity of Edinburgh, Scotland{S.R.Riedel,I.V.Meza-Ruiz}@sms.ed.ac.ukAbstractThis paper presents our system for theOpen Track of the CoNLL 2008 SharedTask (Surdeanu et al, 2008) in Joint De-pendency Parsing1and Semantic Role La-belling.
We use Markov Logic to definea joint SRL model and achieve a semanticF-score of 74.59%, the second best in theOpen Track.1 IntroductionMany SRL systems use a two-stage pipeline thatfirst extracts possible argument candidates (argu-ment identification) and then assigns argumentlabels to these candidates (argument classifica-tion) (Xue and Palmer, 2004).
If we also con-sider the necessary previous step of identifyingthe predicates and their senses (predicate identi-fication) this yields a three-stage pipeline: predi-cate identification, argument identification and ar-gument classification.Our system, on the other hand, follows a jointapproach in the spirit of Toutanova et al (2005)and performs the above steps collectively .
We de-cided to use Markov Logic (ML, Richardson andDomingos, 2005), a First Order Probabilistic Lan-guage, to develop a global probabilistic model ofSRL.
By using ML we are able to incorporate thedependencies between the decisions of differentstages in the pipeline and the well-known globalcorrelations between the arguments of a predi-cate (Punyakanok et al, 2005).
And since learningc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.1Note that in this work we do not consider the parsingtask; instead we use the provided dependencies of the opentrack datatsets.and inference methods were already implementedin theML software we use, only minimal engineer-ing efforts had to be done.In contrast to the work of Toutanova et al (2005)our system applies online learning to train its pa-rameters and exact inference to predict a collectiverole labelling.
Moreover, we jointly label the argu-ments of all predicates in a sentence.
This allowsus, for example, to require that certain tokens haveto be an argument of some predicates in the sen-tence.In this paper we also investigate the impact ofdifferent levels of interaction between the layers ofthe joint SRL model.
We find that a probabilis-tic model which resembles a traditional bottom-uppipeline (though jointly trained and globally nor-malised) performs better than the complete jointmodel on the WSJ test set and worse on the Browntest set.
The worst performance is observed whenno interaction between SRL stages is allowed.In terms of semantic F-score (74.59%) our sub-mitted results are the second best in the OpenTrack of the Shared Task.
Our error analysis in-dicates that a) the training regime can be improvedand b) nominalizations are difficult to handle forthe model as it is.In the next sections we will first briefly intro-duce Markov Logic.
Then we present the MarkovLogic model we used in our final submission.
Wepresent and analyse our results in section 4 beforewe conclude in Section 5.2 Markov LogicMarkov Logic (ML, Richardson and Domingos,2005) is a Statistical Relational Learning languagebased on First Order Logic and Markov Networks.It can be seen as a formalism that extends FirstOrder Logic to allow formulae that can be vi-olated with some penalty.
From an alternative193point of view, it is an expressive template languagethat uses First Order Logic formulae to instantiateMarkov Networks of repetitive structure.Let us describe Markov Logic by consideringthe predicate identification task.
In Markov Logicwe can model this task by first introducing a setof logical predicates2such as isPredicate(Token)or word(Token,Word).
Then we specify a set ofweighted first order formulae that define a distribu-tion over sets of ground atoms of these predicates(or so-called possible worlds).Ideally, the distribution we define with theseweighted formulae assigns high probability to pos-sible worlds where SRL predicates are correctlyidentified and a low probability to worlds wherethis is not the case.
For example, a suitable set ofweighted formulae would assign a high probabilityto the world3{word (1,Haag) , word(2, plays),word(3, Elianti), isPredicate(2)}and a low one to{word (1,Haag) , word(2, plays),word(3, Elianti), isPredicate(3)}In Markov Logic a set M = {(?,w?
)}?ofweighted first order formulae is called a MarkovLogic Network (MLN).
It assigns the probabilityp (y) =1Zexp???(?,w)?Mw?c?Cn?f?c(y)??
(1)to the possible world y.
Here f?cis a feature func-tion that returns 1 if in the possible world y theground formula we get by replacing the free vari-ables in ?
by the constants in c is true and 0 oth-erwise.
Cn?is the set of all tuples of constants wecan replace the free variables in ?
with.
Z is a nor-malisation constant.
Note that this distribution cor-responds to a Markov Network where nodes rep-resent ground atoms and factors represent groundformulae.For example, if M contains the formula ?word(x,?take?)?
isPredicate (x)then its corresponding log-linear model has,among others, a feature f?t1for which x in ?
has2In the cases were is not obvious whether we refer to SRLor ML predicates we add the prefix SRL or ML, respectively.3?Haag plays Elianti?
is a segment of a sentence in train-ing corpus.been replaced by the constant t1and that returns 1ifword(1,?take?)?
isPredicate (1)is true in y and 0 otherwise.We will refer predicates such as word as ob-served because they are known in advance.
In con-trast, isPredicate is hidden because we need to in-fer it at test time.2.1 LearningAn MLN we use to model the collective SRL taskis presented in section 3.
We learn the weights as-sociated this MLN using 1-best MIRA (Crammerand Singer, 2003) Online Learning method.2.2 InferenceAssuming that we have an MLN, a set of weightsand a given sentence then we need to predictthe choice of predicates, frame types, argumentsand role labels with maximal a posteriori prob-ability.
To this end we apply a method thatis both exact and efficient: Cutting Plane Infer-ence (CPI, Riedel, 2008) with Integer Linear Pro-gramming (ILP) as base solver.
We use it for infer-ence at test time as well as during the MIRA onlinelearning process.3 ModelWe define five hidden predicates for the threestages of the task.
For predicate identification, weuse the predicates isPredicate and sense.
isPred-icate(p) indicates that the word in the position pis an SRL predicate while sense(p,e) signals thatpredicate in position p has the sense e.For argument identification, we use the predi-cates isArgument and hasRole.
The atom isArgu-ment(a) signals that the word in the position a isa SRL argument of some (unspecified) SRL predi-cate while hasRole(p,a) indicates that the token atposition a is an argument of the predicate in posi-tion p.Finally, for the argument classification stage wedefine the predicate role.
Here role(p,a,r) corre-sponds to the decision that the argument in the po-sition a has the role r with respect to the predicatein the position p.3.1 Local formulaeWe define a set of local formulae.
A formula is lo-cal if its groundings relate any number of observedground atoms to exactly one hidden ground atom.For example, a grounding of the local formulalemma(p,+l1)?lemma(a,+l2) ?
hasRole(p, a)194Figure 1: Factor graph for the local formula in sec-tion 3.1.can be seen in the Markov Network of Figure 1.
Itconnects a hidden hasRole ground atom to two ob-served lemma ground atoms.
Note that the ?+?
pre-fix for variables indicates that there is a differentweight for each possible pair of lemmas (l1, l2).For the hasRole and role predicates we definedlocal formulae that aimed to reproduce the stan-dard features used in previous work (Xue andPalmer, 2004).
This also required us to developdependency-based versions of the constituent-based features such as the syntactic path betweenpredicate and argument, as proposed by Xue andPalmer (2004).The remaining hidden predicates, isPredicate,isArgument and sense, have local formulae thatrelate their ground atoms to properties of a con-textual window around the token the atom corre-sponds to.
For this we used the information pro-vided in the closed track training corpus of theshared task (i.e.
both versions of lemma and POStags plus a coarse version of the POS tags).Instead of describing the local feature set inmore detail we refer the reader to our MLN modelfiles.4They can be used both as a reference andas input to our Markov Logic Engine5, and thus al-low the reader to easily reproduce our results.
Webelieve that this is another advantage of explicitlyseparating model and algorithms by using first or-der probabilistic logic languages.3.2 Global formulaeGlobal formulae relate several hidden groundatoms.
We use them for two purposes: to en-sure consistency between the decisions of all SRLstages and to capture some of our intuition aboutthe task.
We will refer to formulae that serve thefirst purpose as structural constraints.For example, a structural constraint is given by4http://thebeast.googlecode.com/svn/mlns/conll085http://thebeast.googlecode.comthe (deterministic) formularole(p, a, r) ?
hasRole(p, a)which ensures that, whenever the argument a isgiven a label r with respect to the predicate p, thisargument must be an argument of a as denoted byhasRole(p,a).
Note that this formula by itself mod-els the traditional ?bottom-up?
argument identifi-cation and classification pipeline: it is possible tonot assign a role r to an predicate-argument pair(p, a) proposed by the identification stage; how-ever, it is impossible to assign a role r to tokenpairs (p, a) that have not been proposed as poten-tial arguments.One example of another class of structural con-straints ishasRole(p, a) ?
?r.role(p, a, r)which, by itself, models an inverted or ?top-down?pipeline.
In this architecture the argument classi-fication stage can assign roles to tokens that havenot been proposed by the argument identificationstage.
However, it must assign a label to any tokenpair the previous stage proposes.
Figure 2 illus-trates the structural formulae we use in form of aMarkov Network.The formulae we use to ensure consistency be-tween the remaining hidden predicates are omittedfor brevity as they are very similar to the bottom-up and top-down formulae we presented above.For the SRL predicates that perform a labellingtask (role and sense) we also need a structural con-straint which ensures that not more than one labelis assigned.
For instance,(role(p, a, r1) ?
r16= r2?
?role(p, a, r2))forbids two different semantic roles for a pair ofwords.The global formulae that capture our intuitionabout the task itself can be further divided into twoclasses.
The first one uses deterministic or hardconstraints such asrole (p, a1, r) ?
?mod (r) ?
a16= a2?
?role (p, a2, r)which forbids cases where distinct arguments ofa predicate have the same role unless the role de-scribes a modifier.The second class of global formulae is soft ornondeterministic.
For instance, the formulalemma(p,+l) ?
ppos(a,+p)?hasRole(p, a) ?
sense(p,+f)195Figure 2: Markov Network that illustrates thestructural constraints we use.is a soft global formula.
It captures the observationthat the sense of a verb or noun depends on the typeof its arguments.
Here the type of an argumenttoken is represented by its POS tag.4 ResultsWe only submitted results for the Open Track ofthe Shared Task.
Moreover, we focused on SRLand did not infer dependencies; instead we usedthe MALT dependencies parses provided in theOpen Track dataset.
Our submission was rankedsecond out of five with a semantic F1-score of74.59%.6After submission we also set up additional ex-periments to evaluate different types and degreesof connectivity between the decisions made by ourmodel.
To this end we created four new models:a model that omits top-down structural constraintsand thus resembles a (globally trained) bottom-up pipeline (Up); a model that does not containbottom-up structural constraints and thus resem-bles a top-down architecture (Down); a modelin which stages are not connected at all (Iso-lated); and finally, a model in which additionalglobal formulae are omitted and the only remain-ing global formulae are structural (Structural).
Theresults we submitted were generated using the fullmodel (Full).Table 1 summarises the results for each of thesemodels.
We report the F-scores for the WSJ andBrown test corpora provided for the task.
In addi-tion we show training and test times for each sys-tem.There are four findings we take from this.
First,and somewhat surprisingly, the jointly trainedbottom-up model (Up) performs substantially bet-6While we did use information of the open dataset we dobelieve that it is possible to train a stacked parsing-SRL sys-tem that would perform similarily.
If so, our system wouldhave the 5th best semantic scores among the 20 participantsof the closed track.Model WSJ Brown Train TestTime TimeFull 75.72% 65.38% 25h 24mUp 76.96% 63.86% 11h 14mDown 73.48% 59.34% 22h 23mIsolated 60.49% 48.12% 11h 14mStructural 74.93% 64.23% 22h 33mTable 1: F-scores for different models.ter than the full model on the WSJ test corpus.
Wewill try to give an explanation for this result inthe next section.
Second, the bottom-up model istwice as fast compared to both the full and the top-down model.
This is due to the removal of formu-lae with existential quantifiers that would result inlarge clique sizes of the ground Markov Network.Third, the isolated model performs extremely poor,particularly for argument classification.
Here fea-tures defined for the role predicate can not makeany use of the information in previous stages.
Fi-nally, the additional global formulae do improveperformance, although not substantially.4.1 AnalysisA substantial amount of errors in our submitted re-sults (Full) can be attributed to the seemingly ran-dom assignment of the very low frequency label?R-AA?
(appears once in the training set) to tokenpairs that should either have a different role or norole at all.
Without these false positives, precisionwould increase by about 1%.
Interestingly, thistype of error completely disappears for the bottom-up model (Up) and thus seem to be crucial in orderunderstand why this model can outperform the fullmodel.We believe that this type of error is an artifact ofthe training regime.
For the full model the weightsof the role predicate only have ensure that the right(true positive) role is the relative winner amongall roles.
In the bottom-up model they also haveto make sure that their cumulative weight is non-negative ?
otherwise simply not assigning a roler for (p, a) would increase the score even if has-Role(p,a) is predicted with high confidence.
Thusmore weight is shifted towards the correct roles.This helps the right label to win more likely overthe ?R-AA?
label, whose weights have rarely beentouched and are closer to zero.Likewise, in the bottom-up model the totalweight of the hasRole features of a wrong (falsepositive) candidate token pair must be nonpositive.Otherwise picking the wrong candidate would in-crease overall score and no role features can re-196ject this decision because the corresponding struc-tural constraints are missing.
Thus more weightis shifted away from false positive candidates, re-sulting in a higher precision of the hasRole pred-icate.
This also means that less wrong candidatesare proposed, for which the ?R-AA?
role is morelikely to be picked because its weights have hardlybeen touched.However, it seems that by increasing precisionin this way, we decrease recall for out-of-domaindata.
This leads to a lower F1 score for the bottom-up model on the Brown test set.Another prominent type of errors appear fornominal predicates.
Our system only recovers onlyabout 80% of predicates with ?NN?, ?NNS?
and?NNP?
tags (and classifies about 90% of these withthe right predicate sense).
Argument identificationand classification performs equally bad.
For exam-ple, for the ?A0?
argument of ?VB?
predicates weget an F-score of 82.00%.
For the ?A0?
of ?NN?predicates F-score is 65.92%.
The features of oursystem are essentially taken from the work done onPropBank predicates and we did only little workto adapt these to the case of nominal predicates.Putting more effort into designing features specificto the case of nominal predicates might improvethis situation.5 ConclusionWe presented a Markov Logic Network that jointlyperforms predicate identification, argument identi-fication and argument classification for SRL.
Thisnetwork achieves the second best semantic F-scores in the Open Track of the CoNLL sharedtask.Experimentally we show that results can be fur-ther improved by using an MLN that resembles aconventional SRL bottom-up pipeline (but is stilljointly trained and globally normalised) insteadof a fully connected model.
We hypothesise thatwhen training this model more weight is shiftedaway from wrong argument candidates and moreweight is shifted towards correct role labels.
Thisresults in higher precision for argument identifica-tion and better accuracy for argument classifica-tion.Possible future work includes better treatmentof nominal predicates, for which we perform quitepoorly.
We would also like to investigate the im-pact of linguistically motivated global formulaemore thoroughly.
So far our model benefits fromthem, albeit not substantially.ReferencesKoby Crammer and Yoram Singer.
Ultraconserva-tive online algorithms for multiclass problems.Journal of Machine Learning Research, 2003.V.
Punyakanok, D. Roth, and W. Yih.
Generalizedinference with multiple semantic role labelingsystems.
In Proceedings of the Annual Con-ference on Computational Natural LanguageLearning, 2005.Matthew Richardson and Pedro Domingos.Markov logic networks.
Technical report,University of Washington, 2005.Sebastian Riedel.
Improving the accuracy and ef-ficiency of map inference for markov logic.
InProceedings of the Annual Conference on Un-certainty in AI, 2008.Mihai Surdeanu, Richard Johansson, Adam Mey-ers, Llu?
?s M`arquez, and Joakim Nivre.
TheCoNLL-2008 shared task on joint parsing ofsyntactic and semantic dependencies.
In Pro-ceedings of the 12th Conference on Compu-tational Natural Language Learning (CoNLL-2008), 2008.Kristina Toutanova, Aria Haghighi, and Christo-pher D. Manning.
Joint learning improves se-mantic role labeling.
In Proceedings of the 43rdAnnual Meeting on Association for Computa-tional Linguistics, 2005.Nianwen Xue and Martha Palmer.
Calibrating fea-tures for semantic role labeling.
In Proceedingsof the Annual Conference on Empirical Methodsin Natural Language Processing, 2004.197
