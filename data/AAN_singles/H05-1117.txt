Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 931?938, Vancouver, October 2005. c?2005 Association for Computational LinguisticsAutomatically Evaluating Answers to Definition QuestionsJimmy Lin1,3 and Dina Demner-Fushman2,31College of Information Studies2Department of Computer Science3Institute for Advanced Computer StudiesUniversity of MarylandCollege Park, MD 20742, USAjimmylin@umd.edu, demner@cs.umd.eduAbstractFollowing recent developments in the au-tomatic evaluation of machine translationand document summarization, we presenta similar approach, implemented in a mea-sure called POURPRE, for automaticallyevaluating answers to definition questions.Until now, the only way to assess the cor-rectness of answers to such questions in-volves manual determination of whetheran information nugget appears in a sys-tem?s response.
The lack of automaticmethods for scoring system output is animpediment to progress in the field, whichwe address with this work.
Experimentswith the TREC 2003 and TREC 2004 QAtracks indicate that rankings produced byour metric correlate highly with officialrankings, and that POURPRE outperformsdirect application of existing metrics.1 IntroductionRecent interest in question answering has shiftedaway from factoid questions such as ?What city isthe home to the Rock and Roll Hall of Fame?
?,which can typically be answered by a short nounphrase, to more complex and difficult questions.One interesting class of information needs con-cerns so-called definition questions such as ?Who isVlad the Impaler?
?, whose answers would include?nuggets?
of information about the 16th centurywarrior prince?s life, accomplishments, and legacy.Actually a misnomer, definition questions can bebetter paraphrased as ?Tell me interesting thingsabout X.?, where X can be a person, an organiza-tion, a common noun, etc.
Taken another way, defi-nition questions might be viewed as simultaneouslyasking a whole series of factoid questions about thesame entity (e.g., ?When was he born?
?, ?What washis occupation?
?, ?Where did he live?
?, etc.
), exceptthat these questions are not known in advance; seePrager et al (2004) for an implementation based onthis view of definition questions.Much progress in natural language processing andinformation retrieval has been driven by the creationof reusable test collections.
A test collection con-sists of a corpus, a series of well-defined tasks, anda set of judgments indicating the ?correct answers?.To complete the picture, there must exist meaning-ful metrics to evaluate progress, and ideally, a ma-chine should be able to compute these values auto-matically.
Although ?answers?
to definition ques-tions are known, there is no way to automaticallyand objectively determine if they are present in agiven system?s response (we will discuss why inSection 2).
The experimental cycle is thus tortuouslylong; to accurately assess the performance of newtechniques, one must essentially wait for expensive,large-scale evaluations that employ human assessorsto judge the runs (e.g., the TREC QA track).
Thissituation mirrors the state of machine translation anddocument summarization research a few years ago.Since then, however, automatic scoring metrics suchas BLEU and ROUGE have been introduced as stop-gap measures to facilitate experimentation.Following these recent developments in evalua-9311 vital 32 kilograms plutonium powered2 vital seven year journey3 vital Titan 4-B Rocket4 vital send Huygens to probe atmosphere of Titan, Saturn?s largest moon5 okay parachute instruments to planet?s surface6 okay oceans of ethane or other hydrocarbons, frozen methane or water7 vital carries 12 packages scientific instruments and a probe8 okay NASA primary responsible for Cassini orbiter9 vital explore remote planet and its rings and moons, Saturn10 okay European Space Agency ESA responsible for Huygens probe11 okay controversy, protest, launch failure, re-entry, lethal risk, humans, plutonium12 okay Radioisotope Thermoelectric Generators, RTG13 vital Cassini, NASA?S Biggest and most complex interplanetary probe14 okay find information on solar system formation15 okay Cassini Joint Project between NASA, ESA, and ASI (Italian Space Agency)16 vital four year study missionTable 1: The ?answer key?
to the question ?What is the Cassini space probe?
?tion research, we propose POURPRE, a technique forautomatically evaluating answers to definition ques-tions.
Like the abovementioned metrics, POURPREis based on n-gram co-occurrences, but has beenadapted for the unique characteristics of the questionanswering task.
This paper will show that POUR-PRE can accurately assess the quality of answersto definition questions without human intervention,allowing experiments to be performed with rapidturnaround.
We hope that this will enable faster ex-ploration of the solution space and lead to acceler-ated advances in the state of the art.This paper is organized as follows: In Section 2,we briefly describe how definition questions are cur-rently evaluated, drawing attention to many of theintricacies involved.
We discuss previous work inSection 3, relating POURPRE to evaluation metricsfor other language applications.
Section 4 discussesmetrics for evaluating the quality of an automaticscoring algorithm.
The POURPRE measure itself isoutlined in Section 5; POURPRE scores are corre-lated with official human-generated scores in Sec-tion 6, and also compared to existing metrics.
InSection 7, we explore the effect that judgment vari-ability has on the stability of definition questionevaluation, and its implications for automatic scor-ing algorithms.2 Evaluating Definition QuestionsTo date, NIST has conducted two formal evaluationsof definition questions, at TREC 2003 and TREC2004.1 In this section, we describe the setup of thetask and the evaluation methodology.Answers to definition questions are comprised ofan unordered set of [document-id, answer string]pairs, where the strings are presumed to providesome relevant information about the entity being?defined?, usually called the target.
Although noexplicit limit is placed on the length of the answerstring, the final scoring metric penalizes verbosity(discussed below).To evaluate system responses, NIST pools answerstrings from all systems, removes their associationwith the runs that produced them, and presents themto a human assessor.
Using these responses and re-search performed during the original development ofthe question, the assessor creates an ?answer key?
?a list of ?information nuggets?
about the target.
Aninformation nugget is defined as a fact for which theassessor could make a binary decision as to whethera response contained that nugget (Voorhees, 2003).The assessor also manually classifies each nugget as1TREC 2004 questions were arranged around ?topics?
; def-inition questions were implicit in the ?other?
questions.932[XIE19971012.0112] The Cassini space probe, due to be launched from Cape Canaveral in Florida ofthe United States tomorrow, has a 32 kilogram plutonium fuel payload to power its seven year journeyto Venus and Saturn.Nuggets assigned: 1, 2[NYT19990816.0266] Early in the Saturn visit, Cassini is to send a probe named Huygens into thesmog-shrouded atmosphere of Titan, the planet?s largest moon, and parachute instruments to its hiddensurface to see if it holds oceans of ethane or other hydrocarbons over frozen layers of methane or water.Nuggets assigned: 4, 5, 6Figure 1: Examples of judging actual system responses.either vital or okay.
Vital nuggets represent con-cepts that must be present in a ?good?
definition;on the other hand, okay nuggets contribute worth-while information about the target but are not essen-tial; cf.
(Hildebrandt et al, 2004).
As an example,nuggets for the question ?What is the Cassini spaceprobe??
are shown in Table 1.Once this answer key of vital/okay nuggets is cre-ated, the assessor then manually scores each run.
Foreach system response, he or she decides whether ornot each nugget is present.
Assessors do not sim-ply perform string matches in this decision process;rather, this matching occurs at the conceptual level,abstracting away from issues such as vocabularydifferences, syntactic divergences, paraphrases, etc.Two examples of this matching process are shownin Figure 1: nuggets 1 and 2 were found in the toppassage, while nuggets 4, 5, and 6 were found in thebottom passage.
It is exactly this process of concep-tually matching nuggets from the answer key withsystem responses that we attempt to capture with anautomatic scoring algorithm.The final F-score for an answer is calculated inthe manner described in Figure 2, and the final scoreof a run is simply the average across the scores of allquestions.
The metric is a harmonic mean betweennugget precision and nugget recall, where recall isheavily favored (controlled by the ?
parameter, setto five in 2003 and three in 2004).
Nugget recall iscalculated solely on vital nuggets, while nugget pre-cision is approximated by a length allowance givenbased on the number of both vital and okay nuggetsreturned.
Early on in a pilot study, researchers dis-covered that it was impossible for assessors to con-sistently enumerate the total set of nuggets containedLetr # of vital nuggets returned in a responsea # of okay nuggets returned in a responseR # of vital nuggets in the answer keyl # of non-whitespace characters in the entireanswer stringThenrecall (R) = r/Rallowance (?)
= 100?
(r + a)precision (P) ={1 if l < ?1?
l?
?l otherwiseFinally, the F (?)
= (?2 + 1)?
P ?R?2 ?
P +R?
= 5 in TREC 2003, ?
= 3 in TREC 2004.Figure 2: Official definition of F-measure.in a system response, given that they were usuallyextracted text fragments from documents (Voorhees,2003).
Thus, a penalty for verbosity serves as a sur-rogate for precision.3 Previous WorkThe idea of employing n-gram co-occurrence statis-tics to score the output of a computer system againstone or more desired reference outputs was first suc-cessfully implemented in the BLEU metric for ma-chine translation (Papineni et al, 2002).
Since then,the basic method for scoring translation quality hasbeen improved upon by others, e.g., (Babych andHartley, 2004; Lin and Och, 2004).
The basic ideahas been extended to evaluating document summa-rization with ROUGE (Lin and Hovy, 2003).933Recently, Soricut and Brill (2004) employed n-gram co-occurrences to evaluate question answer-ing in a FAQ domain; unfortunately, the task differsfrom definition question answering, making their re-sults not directly applicable.
Xu et al (2004) appliedROUGE to automatically evaluate answers to defi-nition questions, viewing the task as a variation ofdocument summarization.
Because TREC answernuggets were terse phrases, the authors found it nec-essary to rephrase them?two humans were askedto manually create ?reference answers?
based on theassessors?
nuggets and IR results, which was a labor-intensive process.
Furthermore, Xu et al did notperform a large-scale assessment of the reliability ofROUGE for evaluating definition answers.4 Criteria for SuccessBefore proceeding to our description of POURPRE, itis important to first define the basis for assessing thequality of an automatic evaluation algorithm.
Cor-relation between official scores and automatically-generated scores, as measured by the coefficient ofdetermination R2, seems like an obvious metric forquantifying the performance of a scoring algorithm.Indeed, this measure has been employed in the eval-uation of BLEU, ROUGE, and other related metrics.However, we believe that there are better mea-sures of performance.
In comparative evaluations,we ultimately want to determine if one techniqueis ?better?
than another.
Thus, the system rank-ings produced by a particular scoring method areoften more important than the actual scores them-selves.
Following the information retrieval litera-ture, we employ Kendall?s ?
to capture this insight.Kendall?s ?
computes the ?distance?
between tworankings as the minimum number of pairwise adja-cent swaps necessary to convert one ranking into theother.
This value is normalized by the number ofitems being ranked such that two identical rankingsproduce a correlation of 1.0; the correlation betweena ranking and its perfect inverse is ?1.0; and the ex-pected correlation of two rankings chosen at randomis 0.0.
Typically, a value of greater than 0.8 is con-sidered ?good?, although 0.9 represents a thresholdresearchers generally aim for.
In this study, we pri-marily focus on Kendall?s ?
, but also report R2 val-ues where appropriate.5 POURPREPreviously, it has been assumed that matchingnuggets from the assessors?
answer key with sys-tems?
responses must be performed manually be-cause it involves semantics (Voorhees, 2003).
Wewould like to challenge this assumption and hypoth-esize that term co-occurrence statistics can serve asa surrogate for this semantic matching process.
Ex-perience with the ROUGE metric has demonstratedthe effectiveness of matching unigrams, an idea weemploy in our POURPRE metric.
We hypothesizethat matching bigrams, trigrams, or any other longern-grams will not be beneficial, because they primar-ily account for the fluency of a response, more rele-vant in a machine translation task.
Since answers todefinition questions are usually document extracts,fluency is less important a concern.The idea behind POURPRE is relatively straight-forward: match nuggets by summing the unigramco-occurrences between terms from each nugget andterms from the system response.
We decided to startwith the simplest possible approach: count the wordoverlap and divide by the total number of terms inthe answer nugget.
The only additional wrinkle is toensure that all words appear within the same answerstring.
Since nuggets represent coherent concepts,they are unlikely to be spread across different an-swer strings (which are usually different extracts ofsource documents).
As a simple example, let?s saywe?re trying to determine if the nugget ?A B C D?
iscontained in the following system response:1.
A2.
B C D3.
D4.
A DThe match score assigned to this nugget would be3/4, from answer string 2; no other answer stringwould get credit for this nugget.
This provision re-duces the impact of coincidental term matches.Once we determine the match score for everynugget, the final F-score is calculated in the usualway, except that the automatically-derived matchscores are substituted where appropriate.
For exam-ple, nugget recall now becomes the sum of the matchscores for all vital nuggets divided by the total num-ber of vital nuggets.
In the official F-score calcula-934POURPRE ROUGERun micro, cnt macro, cnt micro, idf macro, idf +stop ?stopTREC 2004 (?
= 3) 0.785 0.833 0.806 0.812 0.780 0.786TREC 2003 (?
= 3) 0.846 0.886 0.848 0.876 0.780 0.816TREC 2003 (?
= 5) 0.890 0.878 0.859 0.875 0.807 0.843Table 2: Correlation (Kendall?s ? )
between rankings generated by POURPRE/ROUGE and official scores.POURPRE ROUGERun micro, cnt macro, cnt micro, idf macro, idf +stop ?stopTREC 2004 (?
= 3) 0.837 0.929 0.904 0.914 0.854 0.871TREC 2003 (?
= 3) 0.919 0.963 0.941 0.957 0.876 0.887TREC 2003 (?
= 5) 0.954 0.965 0.957 0.964 0.919 0.929Table 3: Correlation (R2) between values generated by POURPRE/ROUGE and official scores.tion, the length allowance?for the purposes of com-puting nugget precision?was 100 non-whitespacecharacters for every okay and vital nugget returned.Since nugget match scores are now fractional, thisrequired some adjustment.
We settled on an al-lowance of 100 non-whitespace characters for everynugget match that had non-zero score.A major drawback of this basic unigram over-lap approach is that all terms are considered equallyimportant?surely, matching ?year?
in a system?s re-sponse should count for less than matching ?Huy-gens?, in the example about the Cassini spaceprobe.
We decided to capture this intuition using in-verse document frequency, a commonly-used mea-sure in information retrieval; idf(ti) is defined aslog(N/ci), where N is the number of documents inthe collection, and ci is the number of documentsthat contain the term ti.
With scoring based on idf,term counts are simply replaced with idf sums incomputing the match score, i.e., the match score ofa particular nugget is the sum of the idfs of match-ing terms in the system response divided by the sumof all term idfs from the answer nugget.
Finally,we examined the effects of stemming, i.e., matchingstemmed terms derived from the Porter stemmer.In the next section, results of experiments withsubmissions to TREC 2003 and TREC 2004 are re-ported.
We attempted two different methods for ag-gregating results: microaveraging and macroaverag-ing.
For microaveraging, scores were calculated bycomputing the nugget match scores over all nuggetsfor all questions.
For macroaveraging, scores foreach question were first computed, and then aver-aged across all questions in the testset.
With mi-croaveraging, each nugget is given equal weight,while with macroaveraging, each question is givenequal weight.As a baseline, we revisited experiments by Xuet al (2004) in using ROUGE to evaluate definitionquestions.
What if we simply concatenated all theanswer nuggets together and used the result as the?reference summary?
(instead of using humans tocreate custom reference answers)?6 Evaluation of POURPREWe evaluated all definition question runs submittedto the TREC 20032 and TREC 2004 question an-swering tracks with different variants of our POUR-PRE metric, and then compared the results with theofficial F-scores generated by human assessors.
TheKendall?s ?
correlations between rankings producedby POURPRE and the official rankings are shown inTable 2.
The coefficients of determination (R2) be-tween the two sets of scores are shown in Table 3.We report four separate variants along two differentparameters: scoring by term counts only vs. scoringby term idf, and microaveraging vs. macroaveraging.Interestingly, scoring based on macroaveraged term2In TREC 2003, the value of ?
was arbitrarily set to five,which was later determined to favor recall too heavily.
As aresult, it was readjusted to three in TREC 2004.
In our experi-ments with TREC 2003, we report figures for both values.935Figure 3: Scatter graph of official scores plottedagainst the POURPRE scores (macro, count) forTREC 2003 (?
= 5).counts outperformed any of the idf variants.A scatter graph plotting official F-scores againstPOURPRE scores (macro, count) for TREC 2003(?
= 5) is shown in Figure 3.
Corresponding graphsfor other variants appear similar, and are not shownhere.
The effect of stemming on the Kendall?s ?
cor-relation between POURPRE (macro, count) and of-ficial scores in shown in Table 4.
Results from thesame stemming experiment on the other POURPREvariants are similarly inconclusive.For TREC 2003 (?
= 5), we performed an anal-ysis of rank swaps between official and POURPREscores.
A rank swap is said to have occurred if therelative ranking of two runs is different under dif-ferent conditions?they are significant because rankswaps might prevent researchers from confidentlydrawing conclusions about the relative effectivenessof different techniques.
We observed 81 rank swaps(out of a total of 1431 pairwise comparisons for 54runs).
A histogram of these rank swaps, binned bythe difference in official score, is shown in Figure 4.As can be seen, 48 rank swaps (59.3%) occurredwhen the difference in official score is less than0.02; there were no rank swaps observed for runsin which the official scores differed by more than0.061.
Since measurement error is an inescapablefact of evaluation, we need not be concerned withrank swaps that can be attributed to this factor.
ForTREC 2003, Voorhees (2003) calculated this valueto be approximately 0.1; that is, in order to concludewith 95% confidence that one run is better than an-Run unstemmed stemmedTREC 2004 (?
= 3) 0.833 0.825TREC 2003 (?
= 3) 0.886 0.897TREC 2003 (?
= 5) 0.878 0.895Table 4: The effect of stemming on Kendall?s ?
; allruns with (macro, count) variant of POURPRE.Figure 4: Histogram of rank swaps for TREC 2003(?
= 5), binned by difference in official score.other, an absolute F-score difference greater than 0.1must be observed.
As can be seen, all the rank swapsobserved can be attributed to error inherent in theevaluation process.From these results, we can see that evaluationof definition questions is relatively coarse-grained.However, TREC 2003 was the first formal evalua-tion of definition questions; as methodologies are re-fined, the margin of error should go down.
Althougha similar error analysis for TREC 2004 has not beenperformed, we expect a similar result.Given the simplicity of our POURPRE metric,the correlation between our automatically-derivedscores and the official scores is remarkable.
Startingfrom a set of questions and a list of relevant nuggets,POURPRE can accurately assess the performance ofa definition question answering system without anyhuman intervention.6.1 Comparison Against ROUGEWe choose ROUGE over BLEU as a baseline forcomparison because, conceptually, the task of an-swering definition questions is closer to summariza-tion than it is to machine translation, in that both arerecall-oriented.
Since the majority of question an-936swering systems employ extractive techniques, flu-ency (i.e., precision) is not usually an issue.How does POURPRE stack up against usingROUGE3 to directly evaluate definition questions?The Kendall?s ?
correlations between rankings pro-duced by ROUGE (with and without stopword re-moval) and the official rankings are shown in Ta-ble 2; R2 values are shown in Table 3.
In all cases,ROUGE does not perform as well.We believe that POURPRE better correlates withofficial scores because it takes into account specialcharacteristics of the task: the distinction betweenvital and okay nuggets, the length penalty, etc.
Otherthan a higher correlation, POURPRE offers an advan-tage over ROUGE in that it provides a better diag-nostic than a coarse-grained score, i.e., it can revealwhy an answer received a particular score.
This al-lows researchers to conduct failure analyses to iden-tify opportunities for improvement.7 The Effect of Variability in JudgmentsAs with many other information retrieval tasks,legitimate differences in opinion about relevanceare an inescapable fact of evaluating definitionquestions?systems are designed to satisfy real-world information needs, and users inevitably dis-agree on which nuggets are important or relevant.These disagreements manifest as scoring variationsin an evaluation setting.
The important issue, how-ever, is the degree to which variations in judgmentsaffect conclusions that can be drawn in a compar-ative evaluation, i.e., can we still confidently con-clude that one system is ?better?
than another?
Forthe ad hoc document retrieval task, research hasshown that system rankings are stable with respect todisagreements about document relevance (Voorhees,2000).
In this section, we explore the effect of judg-ment variability on the stability and reliability ofTREC definition question answering evaluations.The vital/okay distinction on nuggets is one majorsource of differences in opinion, as has been pointedout previously (Hildebrandt et al, 2004).
In theCassini space probe example, we disagree with theassessors?
assignment in many cases.
More impor-tantly, however, there does not appear to be any op-3We used ROUGE-1.4.2 with n set to 1, i.e.
unigram match-ing, and maximum matching score rating.Figure 5: Distribution of rank placement using ran-dom judgments (for top two runs from TREC 2004).erationalizable rules for classifying nuggets as eithervital or okay.
Without any guiding principles, howcan we expect our systems to automatically recog-nize this distinction?How do differences in opinion about vital/okaynuggets impact the stability of system rankings?
Toanswer this question, we measured the Kendall?s ?correlation between the official rankings and rank-ings produced by different variations of the answerkey.
Three separate variants were considered:?
all nuggets considered vital?
vital/okay flipped (all vital nuggets becomeokay, and all okay nuggets become vital)?
randomly assigned vital/okay labelsResults are shown in Table 5.
Note that this exper-iment was conducted with the manually-evaluatedsystem responses, not our POURPRE metric.
For thelast condition, we conducted one thousand randomtrials, taking into consideration the original distri-bution of the vital and okay nuggets for each ques-tion using a simplified version of the Metropolis-Hastings algorithm (Chib and Greenberg, 1995); thestandard deviations are reported.These results suggest that system rankings aresensitive to assessors?
opinion about what consti-tutes a vital or okay nugget.
In general, the Kendall?s?
values observed here are lower than values com-puted from corresponding experiments in ad hocdocument retrieval (Voorhees, 2000).
To illustrate,the distribution of ranks for the top two runs from937Run everything vital vital/okay flipped random judgmentsTREC 2004 (?
= 3) 0.919 0.859 0.841 ?
0.0195TREC 2003 (?
= 3) 0.927 0.802 0.822 ?
0.0215TREC 2003 (?
= 5) 0.920 0.796 0.808 ?
0.0219Table 5: Correlation (Kendall?s ? )
between scores under different variations of judgments and the officialscores.
The 95% confidence interval is presented for the random judgments case.TREC 2004 (RUN-12 and RUN-8) over the onethousand random trials is shown in Figure 5.
In 511trials, RUN-12 was ranked as the highest-scoringrun; however, in 463 trials, RUN-8 was ranked asthe highest-scoring run.
Factoring in differences ofopinion about the vital/okay distinction, one couldnot conclude with certainty which was the ?best?
runin the evaluation.It appears that differences between POURPRE andthe official scores are about the same as (or in somecases, smaller than) differences between the officialscores and scores based on variant answer keys (withthe exception of ?everything vital?).
This means thatfurther refinement of the metric to increase correla-tion with human-generated scores may not be par-ticularly meaningful; it might essentially amount toovertraining on the whims of a particular human as-sessor.
We believe that sources of judgment variabil-ity and techniques for managing it represent impor-tant areas for future study.8 ConclusionWe hope that POURPRE can accomplish for defini-tion question answering what BLEU has done formachine translation, and ROUGE for document sum-marization: allow laboratory experiments to be con-ducted with rapid turnaround.
A much shorter ex-perimental cycle will allow researchers to exploredifferent techniques and receive immediate feedbackon their effectiveness.
Hopefully, this will translateinto rapid progress in the state of the art.49 AcknowledgementsThis work was supported in part by ARDA?sAdvanced Question Answering for Intelligence(AQUAINT) Program.
We would like to thank4A toolkit implementing the POURPRE metric can be down-loaded at http://www.umiacs.umd.edu/?jimmylin/downloads/Donna Harman and Bonnie Dorr for comments onearlier drafts of this paper.
In addition, we wouldlike to thank Kiri for her kind support.ReferencesBogdan Babych and Anthony Hartley.
2004.
Extend-ing the BLEU MT evaluation method with frequencyweightings.
In Proc.
of ACL 2004.Siddhartha Chib and Edward Greenberg.
1995.
Under-standing the Metropolis-Hastings algorithm.
Ameri-can Statistician, 49(4):329?345.Wesley Hildebrandt, Boris Katz, and Jimmy Lin.
2004.Answering definition questions with multiple knowl-edge sources.
In Proc.
of HLT/NAACL 2004.Chin-Yew Lin and Eduard Hovy.
2003.
Automatic evalu-ation of summaries using n-gram co-occurrence statis-tics.
In Proc.
of HLT/NAACL 2003.Chin-Yew Lin and Franz Josef Och.
2004.
ORANGE: Amethod for evaluating automatic evaluation metrics formachine translation.
In Proc.
of COLING 2004.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In Proc.
of ACL 2002.John Prager, Jennifer Chu-Carroll, and Krzysztof Czuba.2004.
Question answering using constraint satisfac-tion: QA?by?Dossier?with?Constraints.
In Proc.
ofACL 2004.Radu Soricut and Eric Brill.
2004.
A unified frameworkfor automatic evaluation using n-gram co-occurrencestatistics.
In Proc.
of ACL 2004.Ellen M. Voorhees.
2000.
Variations in relevance judg-ments and the measurement of retrieval effectiveness.Information Processing and Management, 36(5):697?716.Ellen M. Voorhees.
2003.
Overview of the TREC 2003question answering track.
In Proc.
of TREC 2003.Jinxi Xu, Ralph Weischedel, and Ana Licuanan.
2004.Evaluation of an extraction-based approach to answer-ing definition questions.
In Proc.
of SIGIR 2004.938
