An English to Korean Transliteration Modelof Extended Markov WindowSungYoung Jung SungLim Hong*Information Technology Lab.LG Electronics Institute of TechnologySeoul, Koreae-mail : syjung, pack}@lg-elite.comEunok PackAbstractAutomatic transliteration problem 1s totranscribe foreign words in one's own alphabet.Machine generated transliteration can be usefulin various applications uch as indexing in aninformation retrieval system and pronunciationsynthesis in a text-to-speech system.
In thispaper we present a model for statistical English-to-Korean transliteration that generatestransliteration candidates with probability.
Themodel is designed to utilize various informationsources by extending a conventional Markovwindow.
Also, an efficient and accurate methodfor alignment and syllabification ofpronunciation units is described.
Theexperimental results show a recall of 0.939 fortrained words and 0.875 for untrained wordswhen the best 10 candidates are considered.IntroductionAs the amount of international communicationincreases, more foreign words arc flooding intothe Korean language.
Especially in the area ofcomlmter and information science, it has beenreported that 29.4% of index terms aretransliterated fiom or directly written in Englishin the case of a balanced corpus, KT-SET \[18\].The transliteration of l'oreign words isindispensable in Korean language processing.In information retrieval, a simple method ofprocessing foreign words is via query termtranslation based on a synonym dictionary offoreign words and their target ransliteration.
It isnecessary to automate the construction process ofa synonym dictionary since its maintenancerequires continuous efforts for ever-incomingforeign words.
Another area to whichtransliteration can be applied is a text-to-speechsystem where orthographic words are transcribedinto phonetic symbols, in such applications,maximum likelihood \[15\], decision tree \[1\],neural network \[10\] or weighted finited-stateacceptor \[19\] has been used for finding the bestfit.English-to-Korean transliteration problem is thatof generating an appropriate Korean word givenan English word.
In general, there can be variouspossible transliterations in Korean whichcorrespond to a single English word.
It isCOlnmon that the newly imported foreign word istransliterated into several possible candidatewords based on pronunciation, out of which onlya few survive in competition over a period oftime.
In tiffs respect, a statistical approach makessense where multiple transliteration variationsexist for one word, generating candidates inprobable order.In this paper, we present a statistical method totransliterate English words in Korean alphabet togenerate various candidates.
In the next section,we describe a phonetic mapping tableconstruction.
In Section 2, we describe alignmentand syllabification methods, and in Section 3,mathematical formulation for a statistical modelis presented.
Section 4 provides experimentalresults, and finally, we state our conclusions.~' Present a&h'ess: Sevice Engineering Team, Chollia, Service Development Division, DACOM Cotporation, Seoul,Korea (E-mail : syrup913@chollia,.net)3831 Phonetic mapping table constructionFirst of all, we generate a mapping betweenEnglish and Korean phonetic unit pairs (Table 6).In doing so, we use pronunciation sylnbols forEnglish words (Table 5) as defined in the Oxfordcomputer-usable dictionary \[12\].
The Englishand Korean phonetic unit can be a consonant, avowel or some composite of them so as to maketransliteration mapping unique and accurate.
Theorthography for foreign word transliteration toKorean provides a siml?le mapping from Englishto Korean phonetic units.
But in reality, there area lot of transliteration cases that do not followthe orthography.
Table 6-1 has been constructedby examining a significant amount of corpus sothat we can cover as many cases as possible.Table 6-2 shows complex cases where acombination of two or more English phonelnesare mapped to multiple candidates of acomposite Korean phonetic unit.
This phoneticmapping table is carefully constructed so as toproduce a unique candidate in syllabification andaligmnent in the training stage.
When a givenEnglish pronunciation can be syllabificated intoserveral milts or a single composite unit, weadopt a heuristic that only the composite unitconsisting of longer phonetic units is considered.For example, the English phonetic unit "u @" canbe mapped to a Korean phonetic unit ,,@o\]\[u@\]" or "w?q \[w@\]" even though thecolnposition of each unit mapping of "u" and"@" can result in other composite mappingssuch as "-?r-?\] \[juG\]", ,~o\] \[wI@\]", ,o_o\]\[wuja\]", etc.
This composite phonetic unitmapping is also useful for statistical taggingsince composite units provide more accuratestatistical information when they are welldevised.2 Alignment and syllabificationmethodThe alignment and syllabification process iscritical for probabilistic tagging as it is closelylinked to computational complexity.
There canbe combinatorial explosion of state sequencesbecause potential syllables may overlap the sameletter sequences.
A statistical approach called,Forward-Backward parameter estimationalgorithm, is used by Sharman in phonetictranscription problem \[2\].
But a statisticalapproach for syllabification requires expensivecomputatioual resources and a large amount oftraining corpus.
Moreover, it often results inmany improper candidates.
In this paper, wepropose a simple heuristic alignment andsyllabification method that is fast and efficient.The maiu principle in separating phonetic units isto manage a phonetic unit of English and that ofKorean to be mapped in a unique way.
Forexample, the pronunciation notation "@R" of thesuffix "-er" in "computer" is mapped to ,,cq\[@R\]" in Korean.
In this case, the complexpronunciation "@R" is treated as one phoneticunit.
There are many such examples in complexvowels, as in "we" to "~-\]\] \[we\]", 'jo" to ,,.~o\[ jo TM j ,  etc.
It is essential to come up with aphonetic unit mapping table that can reduce thetime complexity of a tagger and also contributeto accurate transliteration results.
Table 6 showsthe examples of phonetic units and their mappingto Korean.The alignment process in training consists of twostages.
The first is consonant alignment whichidentifies corresponding consonant pairs byscanning the English phonetic unit mad Koreannotation.
The second is vowel alignment whichseparates corresponding vowel pairs within theconsonant alignment results of stage 1.
Figure 1shows an aligmnent example in training.
Thealigned and syllabificated units are used toextract statistical inforination from the trainingcorpus.
The alignment process always producesone result.
This is possible because of thepredefined English to Korean phonetic unitmapping in Table 6.Input:English pronunciationand Korean notationFirst stage:consonant alignmentSecond stage:vowel alignmentk@mput@R=1 t ~ ~z-II -E?k @ lm/pu/t @ R/~l -\] I~  lsr.
'lFIt4 "l I?kl @lmlplultl @ RI<Figure l>AIignment example for training data input.
'/' mark: a segmentation position by a consonant'\]' mark: a segmentation position by a vowel.384Figure 1. shows an example of syllabificationand alignment.
To take the English word"computer" as an exalnple, the Englishpronunciation otation "k@mpu@R" is retrievedfroln the Oxford dictionary, in the first stage, itis segmented in flont of the consonants "k", "m","p" and "t" which are aligned with thecorresponding Korean consonants "=1 \[k\]", "rJ\[m\]", ~  \[p\]" and "E  It\]".
In the second stage,it is segmented in flont of the vowels "@", "u"and "@R" and aligned with the correspondingKorean vowels "-\] \[@R\]", "-lT \[ju\]" and"-\] \[@R\]".
The composite vowel "@R" is notdivided into two simple vowels "@" and "R"since it is aligned to Korean "-\] \[@R\]" inaccordance with entry in Table 6-2.
When it ispossible to syllabificate in more than one ways,only the longest phonetic unit is selected so thatan alignment always ends up being uniqueduring the training process.After the training stage, an input English wordmust be syllabificated automatically so that itcan be transliterated by our tagger.
During thisstage, all possible syllabil'ication candidates aregeuerated and are given as inputs to thestatistical tagger so that the proper Koreannotation can be found.3 Statistical transliteration modelA probabilistic tagger finds the most probable setof Korean notation candidates fl'om the possiblesyllabificated results of English pronunciationnotation.
Let \[7, 8, 9\] proposed a statisticaltransliteration model based on the statisticaltranslation model-I by Brown \[2\] that uses onlya simple information source of a word pair.Various kinds of information sources areinvolved in the English to Korean transliterationproblem.
But it is not easy to systematicallyexploit various information sources by extendingthe Markov window in a statistical model.
Thetagging model proposed in this paper exploitsnot only simple pronunciation unit-to-unitmapping froul English to Korean, but also morecomplex contextual information of multiple unitsmapping.
In what follows, we explain how thecontextual information is represented asconditional probabilities.An English word E's pronunciation S is fouud ina phonetic dictionary.
Suppose that S can besegmented into a sequence of syllabificated unitssis 2...s. where s~ is an English phonetic unitas in Table 6.
Also suppose thatKis a Koreanword, where lq is the i-th phonetic unit of K.S.~- SIS 2 " "  Sn~K = k 1 k2... k,,(l)Let us say P(E, K) is the probability that anEnglish word E is transliterated to a Koreanword K. What we have to find is K where P(E,K) is lnaximized given E. This probability can beapproximated by substituting the English word Ewith its prontmciation S. Thus, the followingformula holds.arg max P(E, K)Karg max P(S, K)K= arg max P(K I S)P(S)K(2)where P(S) is called language model and I'(K\]S)is called translation, model.
P(S) is not constantgiven a fixed input word because there can be anumber of syllabification candidates.In detwmining k~, four neighborhood variablesare taken into account, while conventionaltagging models use only two neighborhoodwuiables.
The extended Markov window ofinfolnlation source is defined as in Figure 2.
Italso shows a conventional Markov window usinga dashed line.
Mathematical fornmlation forMarkov window extension is not an easyprobleln since extended window aggravates datasparseness.
We will explain our solution in thenext step.Korean notation ikj 1 Icj.................... : ~..iExtended Markov window "...~..Conventional Markov window<Figure 2> Extended Markov window of informationsource t"01" k i385Now, the translation lnodel, P(K\[S) in equation(2) can be approximated by Markov assumptionas follows.p(/<71s) ~ l~p(k ,  I k,_,,s.,.s,s,+,) (3)iEquation(3) still has data sparseness problem ingathering information directly from the trainingcorpus.
So we expand it using Markov chain inorder to replace the conditional probability termin (3) with more fragmented probability terms.P(k,k,,_\],%,s;,~;+O~k,_,s;_,,s;,%)_ p(k,.
,)p(~ I ~_,)~(.~,.
I~_,k,.
)p(,~; I ~_,k,,~._,)z%., I k,.
y,_lk,.,))mZ~k,.
,)Z~.% I k,.
,)P(s, I k,.
,.%)P(.% I k,.
lS~_vs~)~P(~ Ik, ,)P("'-' j-k'-lk') P(,,.
I ~,,._,) ~,~.+, I ~,,.
)- P(.s,_, I ki_l) ~.':1.
':-,) r(.,,+, I.,,)(4)In Equation (4), there are two kinds ofapproximations in probability terms.
First,P(s~ \]kjs~ ) and P(sj \]sj_~) are substituted forp(s~lkj_4k~s~_l) and P(s, Ik, iSi_l)' respectively.This approxilnation is based on our heuristic thatkj_~and s~ ~ provide somewhat redundantinformation in deterlnining s,.
Secondly,P(s,~ Ik~s~) and P(s,~ Is~) are substituted forp(si+ I I ki_lsi_lkisi) and P(s, I I k,_,s,_,s,).respectively, based on a heuristic that k,_ts~_ ~isfarther off than k, sj.
and is redundant.
Equation(4) can be reduced to Equation (5) becausel'(s,, I k,,k,) of (4) is equivalent to ,'(k, Is, ,k, ,)P(Si_ , \[ ki_l) l'(k~ I k, ~)mathematically.P(ki I K_,s,_,,~',s,+,)P(k~ l s.,k~_,)P(s, I k, s,_,)P(s,+, I k,s,)P(si I s,_,)P(s,+, Is,)(5)The language model we use is a bigram languagemodel (Brown et al \[61)p(s)  ~ H p(s, I .~',-,) (6)iNow, our statistical tagging model can beformulated as Equation (7) when the translationmodel (5) and the language model (6) are appliedto the transliteration model (2).'.
argmaxP(S, K)Krnr P(k, l s.,k,_,)P(s, I k, s.OP(s.,  I k~s,)= arglnaxl 1x , P(si+, j si)(7)English Pron: o o o ~isi-1 @_  @si+lKorean Notatio% e o @ "N~ 0 0 0<Figure 3> Statistical information source used inthe extended Markov window(1)\]'(k,l.s.,_\]k, ,) (2) l ' (s,  lk,s,_,) (3)P(ss+ ,\]kr% )Figure 3 pictorially summarizes the finalinformation sources that our statistical taggerutilizes.
It can be thought of as a generalized caseof prevalent Part-of-Speech tagging model.When P(kj I s,-lk~-i) is approximated asP(kiJki_\]), P(siJkisi_,) as P(sjJkj).
andP(s,~ \[k~s,) as P(s,+ I \]s,).
then Equation (7) isreduced to a conventional bigram tagging model(Eq.
8).
that is a base model of Brown model-1\[2\].
Charniak \[4\], Merialdo \[11\] and Lee \[7, 8.9\].arg max P(S. K) --_ arg max I~  P(ki I k.,)P(s~ I ki) (8)K K iEquation (7) is the final tagging model proposedin this paper.
We use a back-off strategy \[10, 1 1\]as follows, because our tagging model may havea data sparseness problem.P(k, \] s,_,k,_l).~ P(k, I k, ,) ,  if Count(s,_,ki_l) = 0.,'(s, I k, s, ,) .~ p(.~, I k,).
i f  Counz(k,s,_,) = 0p(si+ 1 \ ]k is i )  w, F(s/+ 1 Is/), if Col t r l t (k is i )  = 0Each probability term in equation (7) is obtainedfi'om the training data.
The statistical taggermodeled here uses Viterbi algorithm \[12\] for itssearch to get N-Best candidates.3864.
Exper in ienta l  restl ltsFor the evahlation we constructed a trainingcorpus of 8368 English-Korean word pairs.
OneI{nglish word can have one or more Koreantransliteration entries in the corpus.
90% of thecorpus is used as trainirig data and 10% of thecorpus as test data.
For nlore objectiveexperiment owthiation, we estinmted word-levelaccuracy based Oil exact strillg lllatch OVOI1though many other papers are based oil lexical-level distance to the correct word.
We adopted arecall measure based on wordqevel accuracy.Recall 111easure is the average nuulber ofgenerated corrool words divided by the iohilword COtlllt el: prepared correct allswer set given{111 input word (Eq.
9).
Precision ll\]e{isul'e is tileaverage number o1' relrieved correct wordsdivided by the number of generated candidates(Eq.
10).Recall = cottnl(<generaled, correcl wos'dv) (9)count(possible, correct._ worUs)Precision= c(,zmt(x, enerated con'oct_: word, v) (10)comet(generated_ words)For words not found in the pronunciationdictionary, a transcription ailtonlata is used Iolransfornl the English alphabet to the Koreanalphabet, k transcription aulonlata can be helpfulbocatlsO it ilsos alphabetic i l lfornlalioll thai otirstatistical tag;ger does ilot llso.
\]'tie atltom;_ilaproduces one result aiid ailachos it at the end ofN-best results of the statistical tagger.
Thisautomata has about 500 lranscriplion rules, basedoil previous, current, and lleXt coulext windowand production alphabet.I0.90.80.70.6O 0.t~?1) 0.40.30.20.10 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~t ~t'candidates<Figure 4> Reoall value for each numberof candidatesAl l  experimental results are estimated by 10-foldcross validation for more accurate results.
Table1 shows the estimated recall wlhles for the 10-best results generated by the tagger and for tilecase when transcription automata used as well.Figure 4 shows recall wihies given a number ofcandidates.Pure suilistical agger (cq.
7)Transcription autonlala used aswellTrained Test0.925 0.8500.939 0.875<Table 1 > esthnaied recall result on 10-best resultsWe estimated recall values in tile sameenvironment for conventional tagging model (Eq.8) in order to compare accuracy improvement bylhe I;xicndod Markov window model (Eq.
7)without ranscription atltonlata (Table 2).I~xicnded Markovwindow model (Eq.
7)Conventional taggingmodel (Eq.
8)Trained \] untrained0.925 0.8500.878 0.796<Table 2> comparison of recall wilucs on 10-bestresults el' lhc proposed Extended Markov model withconventional taggillg inede\]It cannot be COmlmred irectly with tilt results ofother models since other rchttcd works arc on adifferent donmin and they adopt differentevahlalion lllOaStlros such as lexical-levolaccuracy \[1, 7, 10\].
There is a model that is ill thesame doumin, i.e., l~nglish-to-Koreantransliteration \[1, 8, 9\], but it adopts a lexical-level accuracy nloasule \[17, 9\], or a subjectiveevahlation measure such as human judgment \[8,9\].
Table 3 shows the comparison with Lee'smodel which adopted the average wthle oftrained and untrained results, even though theaverage of trained and untrairiod results makes nosense since a registered dictionary lookupmethod can make all experiments oil trained data100% accurate.
Accuracy measure on untraineddata should be tile measure for comparisonamong different experiments for fairness.Extended Markovwindow modell,ce - hybrid \[171Trained0.962Untrained0.888Averagc:(train+untraincd ) / 20.471387<Table 3> Comparison with Lee's model, word basedrecall measure on 20-best results.
Lee's result ofsingle frequency coverage \[17\] is the same as a recallmeasure in this paper.Lee's model is a fully statistical approach evenin pronunciation unit alignment andsyllabification that may cause inaccurate results,while we use a heuristic approach inpronunciation unit alignment.
Anothersignificant difference is that Lee's model usesonly conventional information sources such as abigram while our model use various informationsources fi'om extended Markov window.
Lee'smodel transliterates using English alphabetdirectly without pronounciation dictionary sothat it can be better for unknown words or properI1oun .Trained UntrainedExtended Markov 64.0% 54.9%windowtranscription (no training) 36.4%automataMBRtalk \[16\] (100%) 43%Neural Net \[ 10\] 37.7% 26.2%WFSA \[ 19\] ?
64%Lee's - modified Average:( train + untrained ) / 2Direct \[9\] 38.1%<Table 4> comparison with other models - wordaccuracy (precision) of 1-best candidate.Table 4 shows the comparison with MBRtalk\[ 16\], Neural Network \[ i 0\], Weighted finite-stateacceptor (WFSA) \[19\] and direct transliteration\[9\] even though they are based on differentproblem domains.
MBRtalk and Neural Networkmodels are based on English word'spronunciation generation.
WFSA is for English-to-Japanese transliteration.
Experiment fortraining data in MBRtalk makes no sense, sinceit finds the most similar word in a database thatstores all the training data; thus the result wouldalways produce the exact answer.
The resultsshow that the model we propose indicates thegood performance.ConclusionWe have proposed a statistical English-to-Korean transliteration model that exploitsvarious information sources.
This model is ageneralized model from a conventional statisticaltagging model by extending Markov windowwith some inathelnatical approximationtechniques.
An alignment and syllabificationmethod instead of a statistical method isproposed for accurate and fast operation.
Theexperimental results show that the modelproposed in this paper demonstrates significantimprovement in its performance.Phnne l ie  svmhnl  examr~le  Phone| ie  ~vmhnl  e?nmnlei b e a d S shhe dI b id  Z b e i,Q&e be_ed tS e tch& bad  dZ e _d~g~_A bard  p tk  b d g0 ( zero )  cod  m n f v s zO (cap  O) cord  r I w h jU good  ei day_u food  @ U go.V bud  a l3 b izd a U c o w@ about  ol by_N sinE& I@ beerT t_hhin e @ b a reD t h.en U @ to u r<Table 5: l?ronunciation symbol to Ascii mapping\[3\]>English Korean unit Korean unitphonetic unil (orthography) (extra)IW@OE .X E Ir -K~ 2 2(o )ot2,A EE :K:: E77X ~A ;~XE .Z.
:;K?o l  ~oot l  oF o4o\[2oH 011 YJ o~ o -?7-- ?
- r~o-?E- (fa ~-F) ol _9_0tl ?41 ol 04 o oi-o<Table 6-1: Examples of English to Korean phoneticunit mapping (simple cases)>388EnglishPhonetic unilI@@UU@@RAIelwaw3wew@w@UjajAKorean units Korean units(orthography) (extra)oloto-~ot qol-oloqolql-S~lqo:1ol-o lG ol ?
oI o ~rOo Ootolo\[otl(-q-) -S-o~ qq-'q- q qlo ot ~ olot o~ olo4oF<Table 6-2: Examl)les of English to Korean phoncticunit mapping (complex cases)>References\[1\] Bahl, L. R. and et al (1991) Decisiolt Trees forPhonological Rules itt Continuous Speech.
IEEEICASSP, pp.
125-138.\[12\] Brown, P. F. and et al (1990) The Mathematics ofStatistical Machi~te Trat~slation: ParameterEstimatio~t.
Computational I~inguistics,Computational Linguistics, V. 19, No.
2, June.\[3\] Brown, P. F. and et al (1992) Class-Based ~z-gram Models of Natural Language.
Associationfor Compulational Linguistics, V. 18, No.
4.\[4\] Charniak, E. (1993) Statistical Models in Natural-Language Processing.
MIT Press.\[15\] Jung, S. Y. and el al.
(1996) Markov mttdomfieldbased English Part-of Speech taggi~tg system.International conference of ComputationalLinguistics (COLING).116\] Katz, S. (1987) Estimation of probabilities from,~pame data for the language model component ofa .weech recognizer.
IEEE Transactions on ASSP,34(3), pp400-401.\[7\]Lce, J. S. and K. S. Choi (1997) AutomaticForeign Word Transliteratiott Model ./'orh!forrnation Retrieval.
4 '~' Korean Informationmanagement conference proceeding, Seoul, Korea,pp 17 -24.\[18\] Lee, J. S. and K. S. Choi.
(1997)A StatisticalMethod to Generate Various Foreign WordTransliterations in Multilingual lnfonnatiollRetrieval System.
Proceedings of the 2 '"tInternational Workshop on \]nforlnation Retrievalwith Asian languages (IRAL'97), pp123-128,Tsukuba-shi, Japan.\[19\] Lee, J. S. and K. S. Choi.
(1998) Ettglish toKorean Statistical Transliteration for htformationRetrieval.
Colnputer Processing of Orientallanguages.\[10\] Lucas, S.M.
and R.I. Damper.
(1991) Syntacticneural networks for bi-directional text-photteticstrattslation.
Talking Machines, North Holland,pp 127-141.\[11\] Merialdo, B.
(1994) Tagging E~tglish Text witha Pmbabilistic Model Association forComputational Linguistics, V. 20, No.
2, pp155-171.\[ 12\] Roger, M. (1992) A Description of a computer-usable dictionao, file based ott the Oafordadva~med lean~er's dictionao~ of current Ettglish.l)epartment of computer science, University ofLondon.\[13\] Rosen fold, R. (1994) Adaptive StatisticalLanguage Modelling: A Maximum E~ttropyApproach.
Technical report, CMU-CS-94-138,Carnegie Mellon University.\[14\] Salton, G. (1988) Automatic Text Processing.Addison Wesley.1115\] Sharman, R. A.
(1994) Syllable-based Phmtetic7)'anscription by Maximum Likelihood Methods.Computational Linguistics, pp.
1279.1116\] Slanfill, C. (1986) 7bward Memoo,-BasedReasoning.
Communication of the ACM, vol.
29,no.
12, pp 1213-1228, December.\[117\] Viterbi, A. J.
(1967) Error Bounds forCom,olutional Codes attd al~ A?,mptoticallyOptimum Decoding AIL, orithm.
IEEE Transactionson Information Theory, v. IT-13, no.
2, pp 260-269, April.1118\] Kim, J., Y. Kim and S. Kim (1994)Developmem of the Data collection (KTSET) forKorean In\[ormation Retrieval Studies.
The 6"'Hangul and Korean Inforlnation Processing, pp.378-385.\[119\] Knight, Kevin and Jonathan Graehl (1997)Machine Transliteration, Proc.
35"' Mtg, Assoc.for Computational Linguistics389
