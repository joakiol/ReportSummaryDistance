Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 54?62,Portland, OR, USA, 24 June 2011. c?2011 Association for Computational LinguisticsStructure-Preserving Pipelines for Digital LibrariesMassimo PoesioUniversity of Essex, UK andUniversit?
di Trento, ItalyEduard BarbuEgon W. StemleUniversit?
di Trento, Italy{massimo.poesio,eduard.barbu,egon.stemle}@unitn.itChristian GirardiFBK-irst, Trento, Italycgirardi@fbk.euAbstractMost existing HLT pipelines assume the inputis pure text or, at most, HTML and either ig-nore (logical) document structure or removeit.
We argue that identifying the structure ofdocuments is essential in digital library andother types of applications, and show that itis relatively straightforward to extend existingpipelines to achieve ones in which the struc-ture of a document is preserved.1 IntroductionMany off-the-shelf Human Language Technology(HLT) pipelines are now freely available (examplesinclude LingPipe,1 OpenNLP,2 GATE3 (Cunning-ham et al, 2002), TextPro4 (Pianta et al, 2008)),and although they support a variety of document for-mats as input, actual processing (mostly) takes noadvantage of structural information, i.e.
structuralinformation is not used, or stripped off during pre-processing.
Such processing can be considered safe,e.g.
in case of news wire snippets, when processingdoes not need to be aware of sentence or paragraphboundaries, or of text being part of a table or a fig-ure caption.
However, when processing large doc-uments, section or chapter boundaries may be con-sidered an important segmentation to use, and whenworking with the type of data typically found in dig-ital libraries or historical archives, such as whole1http://alias-i.com/lingpipe/2http://incubator.apache.org/opennlp/3http://http://gate.ac.uk/4http://textpro.fbk.eu/books, exhibition catalogs, scientific articles, con-tracts we should keep the structure.
At least threetypes of problems can be observed when trying touse a standard HLT pipeline for documents whosestructure cannot be easily ignored:?
techniques for extracting content from plaintext do not work on, say, bibliographic refer-ences, or lists;?
simply removing the parts of a document thatdo not contain plain text may not be the rightthing to do for all applications, as sometimesthe information contained in them may also beuseful (e.g., keywords are often useful for clas-sification, bibliographic references are useful ina variety of applications) or even the most im-portant parts of a text (e.g., in topic classifica-tion information provided by titles and othertypes of document structure is often the mostimportant part of a document);?
even for parts of a document that still can beconsidered as containing basically text?e.g.,titles?knowing that we are dealing with whatwe will call here non-paragraph text can beuseful to achieve good - or improve - perfor-mance as e.g., the syntactic conventions usedin those type of document elements may be dif-ferent - e.g., the syntax of NPs in titles can bepretty different from that in other sections oftext.In this paper we summarize several years of workon developing structure-preserving pipelines for dif-ferent applications.
We discuss the incorporation of54document structure parsers both in pipelines whichthe information is passed in BOI format (Ramshawand Marcus, 1995), such as the TEXTPRO pipeline(Pianta et al, 2008), and in pipelines based on astandoff XML (Ide, 1998).
We also present sev-eral distinct applications that require preserving doc-ument structure.The structure of the paper is as follows.
We firstdiscuss the notion of document structure and previ-ous work in extracting it.
We then introduce our ar-chitecture for a structure-preserving pipeline.
Next,we discuss two pipelines based on this general archi-tecture.
A discussion follows.2 The Logical Structure of a DocumentDocuments have at least two types of structure5.The term geometrical, or layout, structure, refersto the structuring of a document according to its vi-sual appearance, its graphical representation (pages,columns, etc).
The logical structure (Luong et al,2011) refers instead to the content?s organization tofulfill an intended overall communicative purpose(title, author list, chapter, section, bibliography, etc).Both of these structures can be represented as trees;however, these two tree structures may not be mu-tually compatible (i.e.
representable within a singletree structure with non-overlapping structural ele-ments): e.g.
a single page may contain the end ofone section and the beginning of the next, or a para-graph may just span part of a page or column.
In thispaper we will be exclusively concerned with logicalstructure.2.1 Proposals concerning logical structureEarly on the separation of presentation and content,i.e.
of layout and logical structure, was promoted bythe early adopters of computers within the typeset-ting community; well-known, still widely used, sys-tems include the LATEXmeta-package for electronictypesetting.
The importance of separating documentlogical structure from document content for elec-tronic document processing and for the documentcreators lead to the ISO 8613-1:1989(E) specifica-tion where logical structure is defined as the resultof dividing and subdividing the content of a docu-5other structure types include e.g.
(hyper)links, cross-references, citations, temporal and spatial relationshipsment into increasingly smaller parts, on the basis ofthe human-perceptible meaning of the content, forexample, into chapters, sections, subsections, andparagraphs.
The influential ISO 8879:1986 Stan-dard Generalized Markup Language (SGML) spec-ification fostered document format definitions likethe Open Document Architecture (ODA) and inter-change format, CCITT T.411-T.424 / ISO 8613.Even though the latter format never gainedwide-spread support, its technological ideas influ-enced many of today?s formats, like HTML andCSS as well as, the Extensible Markup Language(XML), today?s successor of SGML.
Today, the ISO26300:2006 Open Document Format for Office Ap-plications (ODF), and the ISO 29500:2008 OfficeOpen XML (OOXML) format are the importantXML-based document file formats.For the work on digital libraries the Text Encod-ing Initiative (TEI)6,most notably, developed guide-lines specifying encoding methods for machine-readable texts.
They have been widely used, e.g.
bylibraries, museums, and publishers.The most common logical elements in suchproposals?chapters, sections, paragraphs, foot-notes, etc.
?can all be found in HTML, LATEX, orany other modern text processor.
It should bepointed out however that many modern types of doc-uments found on the Web do not fit this pattern:e.g.
blog posts with comments, and the structure ofreply threads and inner-linkings to other commentscannot be captured; or much of wikipedia?s non-paragraph text.
(For an in depth comparison anddiscussion of logical formats, and formal characteri-zations thereof we suggest (Power et al, 2003; Sum-mers, 1998).
)2.2 Extracting logical structureTwo families of methods have been developed to ex-tract document structure.
Older systems tend to fol-low the template-matching paradigm.
In this ap-proach the assignment of the categories to parts ofthe string is done by matching a sequence of handcrafted templates against the input string S. Aninstance of this kind of systems is DeLos (Deriva-tion of Logical Structure) (Niyogi and Srihari, 1995)which uses control rules, strategy rules and knowl-6http://www.tei-c.org55edge rules to derive the logical document structurefrom a scanned image of the document.
A more elab-orate procedure for the same task is employed byIshitani (Ishitani, 1999).
He uses rules to classify thetext lines derived from scanned document image andthen employs a set of heuristics to assign the classi-fied lines to logical document components.
The tem-plate based approach is also used by the ParaTools,a set of Perl modules for parsing reference strings(Jewell, 2000).
The drawback of the template basedapproaches is that they are usually not portable tonew domains and are not flexible enough to accom-modate errors.
Domain adaptation requires the de-vising of new rules many of them from scratch.
Fur-ther the scanned documents or the text content ex-tracted from PDF have errors which are not easilydealt with by template based systems.Newer systems use supervised machine learningtechniques which are much more flexible but re-quire training data.
Extracting document structureis an instance of (hierarchical) sequence labeling,a well known problem which naturally arises in di-verse fields like speech recognition, digital signalprocessing or bioinformatics.
Two kinds of machinelearning techniques are most commonly used for thisproblem: Hidden Markov Models (HMM) and Con-ditional Random Fields (CRF).
A system for pars-ing reference strings based on HMMs was developedin (Hetzner, 2008) for the California Digital Library.The system implements a first order HMM where theset of states of the model are represented by the cat-egories in C; the alphabet is hand built and tailoredfor the task and the probabilities in the probabilitymatrix are derived empirically.
The system obtainsan average F1 measure of 93 for the Cora dataset.A better performance for sequence labeling is ob-tained if CRF replaces the traditional HMM.
Thereason for this is that CRF systems better tolerateerrors and they have good performance even whenricher features are not available.
A system whichuses CRF and a series of post-processing rules forboth document logical structure identification andreference string parsing is ParsCit (Councill et al,2008).
ParsCit comprises three sub-modules: Sect-Label and ParseHead for document logical structureidentification and ParsCit for reference string pars-ing.
The system is built on top of the well knownCRF++ package.The linguistic surface level, i.e.
the linear orderof words, sentences, and paragraphs, and the hi-erarchical, tree-like, logical structure also lends it-self to parsing-like methods for the structure analy-sis.
However, the complexity of fostering, maintain-ing, and augmenting document structure grammarsis challenging, and the notorious uncertainty of theinput demands for the whole set of stochastic tech-niques the field has to offer ?
this comes at a highcomputing price; cf.
e.g.,(Lee et al, 2003; Mao etal., 2003).
It is therefore not surprising that high-throughput internet sites like CiteSeerX7 use a flattext classifier (Day et al, 2007).83 Digital Libraries and DocumentStructure PreservationOur first example of application in which documentstructure preservation is essential are digital libraries(Witten et al, 2003).
In a digital library setting, HLTtechniques can be used for a variety of purposes,ranging from indexing the documents in the libraryfor search to classifying them to automatically ex-tracting metadata.
It is therefore becoming more andmore common for HLT techniques to be incorporatedin document management platforms and used to sup-port a librarian when he / she enters a new documentin the library.
Clearly, it would be beneficial if sucha pipeline could identify the logical structure of thedocuments being entered, and preserve it: this infor-mation could be used by the document managementplatform to, for instance, suggest the librarian themost important keywords, find the text to be indexedor even summarized, and produce citations lists, pos-sibly to be compared with the digital library?s list ofcitations to decide whether to add them.We are in the process of developing a Portalfor Research in the Humanities (Portale RicercaUmanistica-PRU).
This digital library will eventu-ally include research articles about the Trentino re-gion from Archeology, History, and History of Art.So far, the pipeline to be discussed next has beenused to include in the library texts from the Italianarcheology journal Preistoria Alpina.
One of ourgoals was to develop a pipeline that could be used7http://citeseerx.ist.psu.edu/8Still, especially multimedia documents with their possibletemporal and spatial relationships might need more sophisti-cated methods.56whenever a librarian uploads an article in this digitallibrary, to identify title, authors, abstract, keywords,content, and bibliographic references from the arti-cle.
The implemented portal already incorporates in-formation extraction techniques that are used to iden-tify in the ?content?
part of the output of the pipelinetemporal expressions, locations, and entities suchas archeological sites, cultures, and artifacts.
Thisinformation is used to allow spatial, temporal, andentity-based access to articles.We are in the process of enriching the portal sothat title and author information are also used to au-tomatically produce a bibliographical card for the ar-ticle that will be entered in the PRU Library Catalog,and bibliographical references are processed in or-der to link the article to related articles and to thecatalog as well.
The next step will be to modify thepipeline (in particular, to modify the Named EntityRecognition component) to include in the library ar-ticles from other areas of research in the Humanities,starting with History.
There are also plans to makeit possible for authors themselves to insert their re-search articles and books in the Portal, as done e.g.,in the Semantics Archive.9.We believe the functionalities offered by this por-tal are or will become pretty standard in digital li-braries, and therefore that the proposals discussed inthis paper could find an application beyond the usein our Portal.
We will also see below that a docu-ment structure-sensitive pipeline can find other ap-plications.4 Turning an Existing Pipeline into Onethat Extracts and Preserves DocumentStructureMost freely available HLT pipelines simply elimi-nate markup during the initial phases of processingin order to eliminate parts of the document struc-ture that cannot be easily processed by their mod-ules (e.g., bibliographic references), but this is notappropriate for the Portal described in the previoussection, where different parts of the output of thepipeline need to be processed in different ways.
Onthe other end, it was not really feasible to developa completely new pipeline from scratch.
The ap-proach we pursued in this work was to take an exist-9http://semanticsarchive.net/ing pipeline and turn it into one which extracts andoutputs document structure.
In this Section we dis-cuss the approach we followed.
In the next Sectionwe discuss the first pipeline we developed accordingto this approach; then we discuss how the approachwas adopted for other purposes, as well.Incorporating a document structure extractor in apipeline requires the solution of two basic problems:where to insert the module, and how to pass on doc-ument structure information.
Concerning the firstissue, we decided to insert the document structureparser after tokenization but before sentence process-ing.
In regards to the second issue, there are atpresent three main formats for exchanging informa-tion between elements of an HLT pipeline:?
inline, where each module inserts informationin a pre-defined fashion into the file received asinput;?
tabular format as done in CONLL, where to-kens occupy the first column and each newlayer of information is annotated in a separatenew column, using the so-called IOB formatto represent bracketing (Ramshaw and Marcus,1995);?
standoff format, where new layers of informa-tion are stored in separate files.The two main formats used by modern HLT pipelinesare tabular format, and inline or standoff XML for-mat.
Even though we will illustrate the problem ofpreserving document structure in a pipeline of theformer type the PRU pipeline itself supports tabularformat and inline XML (TEI compliant).The solution we adopted, illustrated in Figure 1,involves using sentence headers to preserve docu-ment structure information.
In most pipelines usinga tabular interchange information, the output of amodule consists of a number of sentences each ofwhich consists of?
a header: a series of lines with a hash character# at the beginning;?
a set of tab-delimited lines representing tokensand token annotations;?
an empty EOF line.57 # FILE: 11# PART: id1# SECTION: title# FIELDS: token tokenstart sentence pos lemma entity nerTypeSpondylus 0 - SPN Spondylus O B-SITEgaederopus 10 - YF gaederopus O O, 20 - XPW , O Ogioiello 22 - SS gioiello O Odell ' 31 - E dell ' O OEuropa 36 - SPN europa B-GPE B-SITEpreistorica 43 - AS preistorico O O.
55 <eos > XPS full_stop O O# FILE: 11# PART: id2# SECTION: author# FIELDS: token tokenstart sentence pos lemma entity nerTypeMARIA 0 - SPN maria B-PER OA 6 - E a I-PER OBORRELLO 8 - SPN BORRELLO I-PER O& 17 - XPO & O O.
19 <eos > XPS full_stop O O(TEI compliant inline XML snippet :)<text ><body ><div type=" section" xml:lang="it">[...]<p id="p2" type=" author"><s id="p2s1"><name key="PER1" type=" person">MARIA A BORRELLO </name >&.</s></p></div ></body ></text > Figure 1: Using sentence headers to preserve document structure information.
For illustration, the TEI compliantinline XML snippet of the second sentence has been added.58The header in such pipelines normally specifies onlythe file id (constant through the file), the number ofthe sentence within the file, and the columns (seeFigure 1).
This format however can also be usedto pass on document structure information providedthat the pipeline modules ignore all lines beginningwith a hash, as these lines can then be used to pro-vide additional meta information.
We introduce anadditional tag, SECTION, with the following mean-ing: a line beginning with # SECTION: specifies theposition in the document structure of the followingsentence.
Thus for instance, in Figure 1, the line# SECTION: titlespecifies that the following sentence is a title.5 An Pipeline for Research Articles inArcheologyThe pipeline currently in use in the PRU Portalwe are developing is based on the strategy just dis-cussed.
In this Section We discuss the pipeline inmore detail.5.1 ModulesThe pipeline for processing archaeological articlesintegrates three main modules: a module for recov-ering the logical structure of the documents, a mod-ule for Italian and English POS tagging and a gen-eral Name Entity Recognizer and finally, a GazetteerBased Name Entity Recognizer.
The architecture ofthe system is presented in figure 2.
Each moduleexcept the first one takes as input the output of theprevious module in the sequence.1.
Text Extraction.
This module extracts the textfrom PDF documents.
Text extraction fromPDF is a notoriously challenging task.
We ex-perimented with many software packages andobtained the best results with pdftotext.
This isa component of XPDF, an open source viewerfor PDF documents.
pdftotext allows the extrac-tion of the text content of PDF documents in avariety of encodings.
The main drawback of thetext extractor is that it does not always preservethe original text order.2.
Language Identification.
The archeologyrepository contains articles written in one ofthe two languages: Italian or English.
Thismodule uses the TextCat language guesser10 forguessing the language of sentences.
The lan-guage identification task is complicated by thefact that some articles contain text in both lan-guages: for example, an article written in En-glish may have an Italian abstract and/or an Ital-ian conclusion.3.
Logical Structure Identification.
This mod-ule extracts the logical structure of a document.For example, it identifies important parts likethe title, the authors, the main headers, tablesor figures.
For this task we train the SectLa-bel component of ParsCit on the articles in thearcheology repository.
Details on the trainingprocess, the tag set and the performance of themodule are provided in section 5.2.4.
Linguistic Processing.
A set of modules in thepipeline then perform linguistic processing onspecific parts of the document (the Bibliogra-phy Section is excluded for example).
First En-glish or Italian POS is carried out as appropri-ate, followed by English or Italian NER.
NERadaptation techniques have been developed toidentify non-standard types entities that are im-portant in the domain, such as ArcheologicalSites and Archeological Cultures.
(This workis discussed elsewhere.)5.
Reference Parsing.
This module relies onthe output of ParsCit software to update theArcheology Database Bibliography table withthe parsed references for each article.
First,each parsed reference is corrected in an auto-matic post processing step.
Then, the modulechecks, using a simple heuristic, if the entry al-ready exists in the table and updates the table,if appropriate, with the new record.Finally, the documents processed by the pipelineare indexed using the Lucene search engine.5.2 Training the Logical Document StructureIdentifierAs mentioned in Section 5, we use ParsCit to find thelogical structure of the documents in the archeology10http://odur.let.rug.nl/~vannoord/TextCat/59Figure 2: The pipeline of the system for PDF article processing in the Archeology Domaindomain.
ParsCit comes with general CRF trainedmodels; unfortunately, they do not perform well onarcheology documents.
There are some particulari-ties of archeology repository articles that require theretraining of the models.
First, as said before, thetext extracted from PDF is not perfect.
Second, thearcheology articles contain many figures with bilin-gual captions.
Third, the articles have portions ofthe texts in both languages: Italian and English.
Toimprove the parsing performance two models aretrained: the first model should capture the logicaldocuments structure for those documents that haveItalian as main language but might contain portionsin English (like the abstract or summary).
The sec-ond model is trained with documents that have En-glish as main language but might contain fragmentsin Italian (like abstract or summary).The document structure annotation was per-formed by a student in the archeology department,and was checked by one of the authors.
In total 55documents have been annotated (35 with Italian asmain language, 20 with English as main Language).The tagset used for the annotation was specificallydevised for archeology articles.
However, as it canbe seen below most of the devised tags can also befound in general scientific articles.
In Table 1 wepresent the tag set used for annotation.
The column"Tag Count" gives the number of each tag in the an-notated documents.In general the meaning of the tags is self-explanatory with the possible exception of thetag VolumeInfo, which reports information for vol-ume the article is part of.
An annotation exam-ple using this tag is: "<VolumeInfo> PreistoriaAlpina v. 38 (2002) Trento 2002 ISSN 0393-0157</VolumeInfo>".
The volume information can befurther processed by extracting the volume number,the year of the issue and the International StandardSerial Number (ISSN).
To asses the performance ofthe trained models we performed a five fold cross-validation.
The results are reported in the table 2and are obtained for each tag using the F1 measure(1):F1 =2?P?RP+R(1)The results obtained for the Archeology articlesare in line with those obtained by the authors ofParsCit and reported in (Luong et al, 2011).
Thetag categories for which the performance of the sys-tem is bad are the multilingual tags (e.g.
ItalianAb-stract or Italian Summary in articles where the mainlanguage is English).
We will address this issue inthe future by adapting the language identifier to labelmultilingual documents.
We also noticed that manymis-tagged titles, notes or section headers are spliton multiple lines after the text extraction stage.
Thesystem performance might be further improved if apre-processing step immediately after the text extrac-tion is introduced.60Tag Tag CountItalianFigureCaption 456ItalianBodyText 347EnglishFigureCaption 313SectionHeader 248EnglishTableCaption 58ItalianTableCaption 58Author 71AuthorEmail 71AuthorAddress 65SubsectionHeader 50VolumeInfo 57Bibliography 55English Summary 31ItalianKeywords 35EnglishKeywords 35Title 55ItalianSummary 29ItalianAbstract 10Table 25EnglishAbstract 13Note 18Table 1: The tag set used for Archeology Article Annota-tion.6 Additional Applications forStructure-Sensitive PipelinesThe pipeline discussed above can be used for a va-riety of other types of documents?archeology doc-uments from other collections, or documents fromother domains?by simply replacing the documentstructure extractor.
We also found however that thepipeline is useful for a variety of other text-analysistasks.
We briefly discuss these in turn.6.1 Blogs and Microblogging platformsContent creation platforms like blogs, microblogs,community QA sites, forums, etc., contain user gen-erated data.
This data may be emotional, opin-ionated, personal, and sentimental, and as such,makes it interesting for sentiment analysis, opinionretrieval, and mood detection.
In their survey onopinion mining and sentiment analysis Pang and Lee(2008) report that logical structure can be used to uti-lize the relationships between different units of con-tent, in order to achieve a more accurate labeling;Tag F1ItalianFigureCaption 70ItalianBodyText 90EnglishFigureCaption 71SectionHeader 90EnglishTableCaption 70ItalianTableCaption 75Author 72AuthorEmail 75AuthorAddress 73SubsectionHeader 65VolumeInfo 85Bibliography 98English Summary 40ItalianKeywords 55EnglishKeywords 56Title 73ItalianSummary 40ItalianAbstract 50Table 67EnglishAbstract 50Note 70Table 2: The Precision and Recall for the trained models.e.g.
the relationships between discourse participantsin discussions on controversial topics when respond-ing are more likely to be antagonistic than to be re-inforcing, or the way of quoting?a user can refer toanother post by quoting part of it or by addressingthe other user by name or user ID?in posts on politi-cal debates hints at the perceived opposite end of thepolitical spectrum of the quoted user.We are in the process of creating an annotated cor-pus of blogs; the pipeline discussed in this paperwas easily adapted to pre-process this type of dataas well.6.2 HTML pagesIn the IR literature it has often been observed thatcertain parts of document structure contain infor-mation that is particularly useful for document re-trieval.
For instance, Kruschwitz (2003) automati-cally builds domain models ?
simple trees of relatedterms ?
from documents marked up in HTML toassist users during search tasks by performing auto-matic query refinements, and improves users?
experi-61ence for browsing the document collection.
He usesterm counts in different markup contexts like non-paragraph text and running text, and markups likebold, italic, underline to identify concepts and thecorresponding shallow trees.
However, this domain-independent method is suited for all types of datawith logical structure annotation and similar datasources can be found in many places, e.g.
corporateintranets, electronic archives, etc.6.3 Processing Wikipedia pagesWikipedia, as a publicly available web knowledgebase, has been leveraged for semantic informationin much work, including from our lab.
Wikipediaarticles consist mostly of free text, but also con-tain different types of structured information, e.g.
in-foboxes, categorization and geo information, linksto other articles, to other wiki projects, and to exter-nal Web pages.
Preserving this information is there-fore useful for a variety of projects.7 Discussion and ConclusionsThe main point of this paper is to argue that the fieldshould switch to structure-sensitive pipelines.
Theseare particularly crucial in digital library applications,but novel type of documents require them as well.We showed that such extension can be achievedrather painlessly even in tabular-based pipelines pro-vided they allow for meta-lines.ReferencesIsaac G. Councill, C. Lee Giles, and Min-Yen Kan. 2008.Parscit: An open-source crf reference string parsingpackage.
In Proceedings of the Language Resourcesand Evaluation Conference (LREC 08), May.Hamish Cunningham, Diana Maynard, Kalina Bontcheva,and Valentin Tablan.
2002.
GATE: A framework andgraphical development environment for robust NLPtools and applications.
In Proceedings of the 40thAnniversary Meeting of the Association for Computa-tional Linguistics.Min-Yuh Day, Richard Tzong-Han Tsai, Cheng-LungSung, Chiu-Chen Hsieh, Cheng-Wei Lee, Shih-HungWu, Kun-Pin Wu, Chorng-Shyong Ong, and Wen-LianHsu.
2007.
Reference metadata extraction using a hi-erarchical knowledge representation framework.
Deci-sion Support Systems, 43(1):152?167, February.Erik Hetzner.
2008.
A simple method for citation meta-data extraction using hidden markov models.
In Pro-ceedings of the 8th ACM/IEEE-CS joint conferenceon Digital libraries, JCDL ?08, pages 280?284, NewYork, NY, USA.
ACM.Nancy Ide.
1998.
Corpus encoding standard: SGMLguidelines for encoding linguistic corpora.
In Proceed-ings of LREC, pages 463?70, Granada.Yasuto Ishitani.
1999.
Logical structure analysis of doc-ument images based on emergent computation.
InProceedings of International Conference on DocumentAnalysis and Recognition.Michael Jewell.
2000.
Paracite: An overview.Udo Kruschwitz.
2003.
An Adaptable Search System forCollections of Partially Structured Documents.
IEEEIntelligent Systems, 18(4):44?52, July.Kyong-Ho Lee, Yoon-Chul Choy, and Sung-Bae Cho.2003.
Logical structure analysis and generation forstructured documents: a syntactic approach.
IEEEtransactions on knowledge and data engineering,15(5):1277?1294, September.Minh-Thang Luong, Thuy Dung Nguyen, and Min-YenKan.
2011.
Logical structure recovery in scholarlyarticles with rich document feature.
Journal of DigitalLibrary Systems.
Forthcoming.Song Mao, Azriel Rosenfeld, and Tapas Kanungo.
2003.Document Structure Analysis Algorithms: A Litera-ture Survey.Debashish Niyogi and Sargur N. Srihari.
1995.Knowledge-based derivation of document logicalstructure.
In Proceedings of International Conferenceon Document Analysis and Recognition, pages 472?475.Bo Pang and Lillian Lee.
2008.
Opinion Mining andSentiment Analysis.
Foundations and Trends in Infor-mation Retrieval, 2(1-2):1?135, January.Emanuele Pianta, Christian Girardi, and Roberto Zanoli.2008.
The TextPro tool suite.
In LREC, 6th edition ofthe Language Resources and Evaluation Conference,Marrakech (Marocco).Richard Power, Donia Scott, and Nadjet Bouayad-Agha.2003.
Document Structure.
Computational Linguis-tics, 29(2):211?260, June.Lance A. Ramshaw and Mitchell P. Marcus.
1995.
Textchunking using tranformation-based learning.
In Pro-ceedings of Third ACL Workshop on Very Large Cor-pora, pages 82?94.Kristen M. Summers.
1998.
Automatic discovery of log-ical document structure.
Ph.D. thesis, Cornell Univer-sity.Ian H. Witten, David Bainbridge, and David M. Nichols.2003.
How to build a digital library.
Morgan Kauf-mann.62
