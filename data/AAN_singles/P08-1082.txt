Proceedings of ACL-08: HLT, pages 719?727,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsLearning to Rank Answers on Large Online QA CollectionsMihai Surdeanu, Massimiliano Ciaramita, Hugo ZaragozaBarcelona Media Innovation Center, Yahoo!
Research Barcelonamihai.surdeanu@barcelonamedia.org, {massi,hugo}@yahoo-inc.comAbstractThis work describes an answer ranking enginefor non-factoid questions built using a largeonline community-generated question-answercollection (Yahoo!
Answers).
We show howsuch collections may be used to effectivelyset up large supervised learning experiments.Furthermore we investigate a wide range offeature types, some exploiting NLP proces-sors, and demonstrate that using them in com-bination leads to considerable improvementsin accuracy.1 IntroductionThe problem of Question Answering (QA) has re-ceived considerable attention in the past few years.Nevertheless, most of the work has focused on thetask of factoid QA, where questions match short an-swers, usually in the form of named or numerical en-tities.
Thanks to international evaluations organizedby conferences such as the Text REtrieval Confer-ence (TREC)1 or the Cross Language Evaluation Fo-rum (CLEF) Workshop2, annotated corpora of ques-tions and answers have become available for severallanguages, which has facilitated the development ofrobust machine learning models for the task.The situation is different once one moves beyondthe task of factoid QA.
Comparatively little researchhas focused on QA models for non-factoid ques-tions such as causation, manner, or reason questions.Because virtually no training data is available forthis problem, most automated systems train either1http://trec.nist.gov2http://www.clef-campaign.orgQ: How do you quiet a squeaky door?A: Spray WD-40 directly onto the hingesof the door.
Open and close the doorseveral times.
Remove hinges if thedoor still squeaks.
Remove any rust,dirt or loose paint.
Apply WD-40 toHigh removed hinges.
Put the hinges back,Quality open and close door several times again.Q: How to extract html tags from an htmlLow documents with c++?Quality A: very carefullyTable 1: Sample content from Yahoo!
Answers.on small hand-annotated corpora built in house (Hi-gashinaka and Isozaki, 2008) or on question-answerpairs harvested from Frequently Asked Questions(FAQ) lists or similar resources (Soricut and Brill,2006).
None of these situations is ideal: the costof building the training corpus in the former setupis high; in the latter scenario the data tends to bedomain-specific, hence unsuitable for the learning ofopen-domain models.On the other hand, recent years have seen an ex-plosion of user-generated content (or social media).Of particular interest in our context are community-driven question-answering sites, such as Yahoo!
An-swers3, where users answer questions posed by otherusers and best answers are selected manually eitherby the asker or by all the participants in the thread.The data generated by these sites has significant ad-vantages over other web resources: (a) it has a highgrowth rate and it is already abundant; (b) it cov-ers a large number of topics, hence it offers a better3http://answers.yahoo.com719approximation of open-domain content; and (c) it isavailable for many languages.
Community QA sites,similar to FAQs, provide large number of question-answer pairs.
Nevertheless, this data has a signifi-cant drawback: it has high variance of quality, i.e.,answers range from very informative to completelyirrelevant or even abusive.
Table 1 shows some ex-amples of both high and low quality content.In this paper we address the problem of answerranking for non-factoid questions from social mediacontent.
Our research objectives focus on answeringthe following two questions:1.
Is it possible to learn an answer ranking modelfor complex questions from such noisy data?This is an interesting question because a posi-tive answer indicates that a plethora of trainingdata is readily available to QA researchers andsystem developers.2.
Which features are most useful in this sce-nario?
Are similarity models as effective asmodels that learn question-to-answer transfor-mations?
Does syntactic and semantic infor-mation help?
For generality, we focus only ontextual features extracted from the answer textand we ignore all meta data information that isnot generally available.Notice that we concentrate on one component of apossible social-media QA system.
In addition toanswer ranking, a complete system would have tosearch for similar questions already answered (Jeonet al, 2005), and rank content quality using ?social?features such as the authority of users (Jeon et al,2006; Agichtein et al, 2008).
This is not the focus ofour work: here we investigate the problem of learn-ing an answer ranking model capable of dealing withcomplex questions, using a large number of, possi-ble noisy, question-answer pairs.
By focusing exclu-sively on textual content we increase the portabilityof our approach to other collections where ?social?features might not available, e.g., Web search.The paper is organized as follows.
We describeour approach, including all the features explored foranswer modeling, in Section 2.
We introduce thecorpus used in our empirical analysis in Section 3.We detail our experiments and analyze the results inSection 4.
We overview related work in Section 5and conclude the paper in Section 6.AnswerCollectionAnswersTranslationFeaturesWeb CorrelationFeaturesFeaturesSimilarityAnswerRankingQ AnswerRetrieval(unsupervised)(discriminative learning)(class?conditional learning)FeaturesDensity/FrequencyFigure 1: System architecture.2 ApproachThe architecture of the QA system analyzed in thepaper, summarized in Figure 1, follows that of themost successful TREC systems.
The first com-ponent, answer retrieval, extracts a set of candi-date answers A for question Q from a large col-lection of answers, C, provided by a community-generated question-answering site.
The retrievalcomponent uses a state-of-the-art information re-trieval (IR) model to extract A given Q. Sinceour focus is on exploring the usability of the an-swer content, we do not perform retrieval by find-ing similar questions already answered (Jeon et al,2005), i.e., our answer collection C contains onlythe site?s answers without the corresponding ques-tions answered.The second component, answer ranking, assignsto each answer Ai ?
A a score that representsthe likelihood that Ai is a correct answer for Q,and ranks all answers in descending order of thesescores.
The scoring function is a linear combina-tion of four different classes of features (detailed inSection 2.2).
This function is the focus of the pa-per.
To answer our first research objective we willcompare the quality of the rankings provided by thiscomponent against the rankings generated by the IRmodel used for answer retrieval.
To answer the sec-ond research objective we will analyze the contri-bution of the proposed feature set to this function.Again, since our interest is in investigating the util-ity of the answer textual content, we use only infor-mation extracted from the answer text when learn-ing the scoring function.
We do not use any metainformation (e.g., answerer credibility, click counts,etc.)
(Agichtein et al, 2008; Jeon et al, 2006).Our QA approach combines three types of ma-chine learning methodologies (as highlighted in Fig-ure 1): the answer retrieval component uses un-720supervised IR models, the answer ranking is im-plemented using discriminative learning, and fi-nally, some of the ranking features are producedby question-to-answer translation models, which useclass-conditional learning.2.1 Ranking ModelLearning with user-generated content can involvearbitrarily large amounts of data.
For this reasonwe choose as a ranking algorithm the Perceptronwhich is both accurate and efficient and can betrained with online protocols.
Specifically, we im-plement the ranking Perceptron proposed by Shenand Joshi (2005), which reduces the ranking prob-lem to a binary classification problem.
The generalintuition is to exploit the pairwise preferences in-duced from the data by training on pairs of patterns,rather than independently on each pattern.
Given aweight vector ?, the score for a pattern x (a candi-date answer) is simply the inner product between thepattern and the weight vector:f?
(x) = ?x, ??
(1)However, the error function depends on pairwisescores.
In training, for each pair (xi,xj) ?
A,the score f?
(xi ?
xj) is computed; note that if fis an inner product f?
(xi?xj) = f?(xi)?
f?
(xj).Given a margin function g(i, j) and a positive rate ?
,if f?
(xi ?
xj) ?
g(i, j)?
, an update is performed:?t+1 = ?t + (xi ?
xj)?g(i, j) (2)By default we use g(i, j) = (1i ?
1j ), as a mar-gin function, as suggested in (Shen and Joshi, 2005),and find ?
empirically on development data.
Giventhat there are only two possible ranks in our set-ting, this function only generates two possible val-ues.
For regularization purposes, we use as a finalmodel the average of all Perceptron models positedduring training (Freund and Schapire, 1999).2.2 FeaturesIn the scoring model we explore a rich set of featuresinspired by several state-of-the-art QA systems.
Weinvestigate how such features can be adapted andcombined for non-factoid answer ranking, and per-form a comparative feature analysis using a signif-icant amount of real-world data.
For clarity, wegroup the features into four sets: features that modelthe similarity between questions and answers (FG1),features that encode question-to-answer transfor-mations using a translation model (FG2), featuresthat measure keyword density and frequency (FG3),and features that measure the correlation betweenquestion-answer pairs and other collections (FG4).Wherever applicable, we explore different syntacticand semantic representations of the textual content,e.g., extracting the dependency-based representationof the text or generalizing words to their WordNetsupersenses (WNSS) (Ciaramita and Altun, 2006).We detail each of these feature groups next.FG1: Similarity FeaturesWe measure the similarity between a questionQ and an answer A using the length-normalizedBM25 formula (Robertson and Walker, 1997).
Wechose this similarity formula because, out of all theIR models we tried, it provided the best ranking atthe output of the answer retrieval component.
Forcompleteness we also include in the feature set thevalue of the tf ?idf similarity measure.
For both for-mulas we use the implementations available in theTerrier IR platform4 with the default parameters.To understand the contribution of our syntacticand semantic processors we compute the above sim-ilarity features for five different representations ofthe question and answer content:Words (W) - this is the traditional IR view where thetext is seen as a bag of words.Dependencies (D) - the text is represented as a bagof binary syntactic dependencies.
The relative syn-tactic processor is detailed in Section 3.
Dependen-cies are fully lexicalized but unlabeled and we cur-rently extract dependency paths of length 1, i.e., di-rect head-modifier relations (this setup achieved thebest performance).Generalized dependencies (Dg) - same as above, butthe words in dependencies are generalized to theirWNSS, if detected.Bigrams (B) - the text is represented as a bag of bi-grams (larger n-grams did not help).
We added thisview for a fair analysis of the above syntactic views.Generalized bigrams (Bg) - same as above, but thewords are generalized to their WNSS.4http://ir.dcs.gla.ac.uk/terrier721In all these representations we skip stop wordsand normalize all words to their WordNet lemmas.FG2: Translation FeaturesBerger et al (2000) showed that similarity-basedmodels are doomed to perform poorly for QA be-cause they fail to ?bridge the lexical chasm?
be-tween questions and answers.
One way to addressthis problem is to learn question-to-answer trans-formations using a translation model (Berger et al,2000; Echihabi and Marcu, 2003; Soricut and Brill,2006; Riezler et al, 2007).
In our model, we in-corporate this approach by adding the probabilitythat the question Q is a translation of the answer A,P (Q|A), as a feature.
This probability is computedusing IBM?s Model 1 (Brown et al, 1993):P (Q|A) =?q?QP (q|A) (3)P (q|A) = (1?
?
)Pml(q|A) + ?Pml(q|C) (4)Pml(q|A) =?a?A(T (q|a)Pml(a|A)) (5)where the probability that the question term q isgenerated from answer A, P (q|A), is smoothed us-ing the prior probability that the term q is gen-erated from the entire collection of answers C,Pml(q|C).
?
is the smoothing parameter.
Pml(q|C)is computed using the maximum likelihood estima-tor.
Pml(q|A) is computed as the sum of the proba-bilities that the question term q is a translation of ananswer term a, T (q|a), weighted by the probabilitythat a is generated from A.
The translation table forT (q|a) is computed using the EM-based algorithmimplemented in the GIZA++ toolkit5.Similarly with the previous feature group, weadd translation-based features for the five differ-ent text representations introduced above.
Bymoving beyond the bag-of-word representation wehope to learn relevant transformations of structures,e.g., from the ?squeaky?
?
?door?
dependency to?spray?
?
?WD-40?
in the Table 1 example.FG3: Density and Frequency FeaturesThese features measure the density and frequencyof question terms in the answer text.
Variants ofthese features were used previously for either an-swer or passage ranking in factoid QA (Moldovanet al, 1999; Harabagiu et al, 2000).5http://www.fjoch.com/GIZA++.htmlSame word sequence - computes the number of non-stop question words that are recognized in the sameorder in the answer.Answer span - the largest distance (in words) be-tween two non-stop question words in the answer.Same sentence match - number of non-stop questionterms matched in a single sentence in the answer.Overall match - number of non-stop question termsmatched in the complete answer.These last two features are computed also for theother four text representations previously introduced(B, Bg, D, and Dg).
Counting the number ofmatched dependencies is essentially a simplifiedtree kernel for QA (e.g., see (Moschitti et al,2007)) matching only trees of depth 2.
Experimentswith full dependency tree kernels based on severalvariants of the convolution kernels of Collins andDuffy (2001) did not yield improvements.
We con-jecture that the mistakes of the syntactic parser maybe amplified in tree kernels, which consider an ex-ponential number of sub-trees.Informativeness - we model the amount of informa-tion contained in the answer by counting the num-ber of non-stop nouns, verbs, and adjectives in theanswer text that do not appear in the question.FG4: Web Correlation FeaturesPrevious work has shown that the redundancy ofa large collection (e.g., the web) can be used for an-swer validation (Brill et al, 2001; Magnini et al,2002).
In the same spirit, we add features that mea-sure the correlation between question-answer pairsand large external collections:Web correlation - we measure the correlation be-tween the question-answer pair and the web usingthe Corrected Conditional Probability (CCP) for-mula of Magnini et al (2002): CCP (Q,A) =hits(Q + A)/(hits(Q) hits(A)2/3) where hits re-turns the number of page hits from a search engine.When a query returns zero hits we iteratively relax itby dropping the keyword with the smallest priority.Keyword priorities are assigned using the heuristicsof Moldovan et al (1999).Query-log correlation - as in (Ciaramita et al, 2008)we also compute the correlation between question-answer pairs and a search-engine query-log cor-pus of more than 7.5 million queries, which shares722roughly the same time stamp with the community-generated question-answer corpus.
We compute thePointwise Mutual Information (PMI) and Chi square(?2) association measures between each question-answer word pair in the query-log corpus.
Thelargest and the average values are included as fea-tures, as well as the number of QA word pairs whichappear in the top 10, 5, and 1 percentile of the PMIand ?2 word pair rankings.3 The CorpusThe corpus is extracted from a sample of the U.S.Yahoo!
Answers logs.
In this paper we focus onthe subset of advice or ?how to?
questions due totheir frequency and importance in social communi-ties.6 To construct our corpus, we implemented thefollowing successive filtering steps:Step 1: from the full corpus we keep only questionsthat match the regular expression:how (to|do|did|does|can|would|could|should)and have an answer selected as best either bythe asker or by the participants in the thread.The outcome of this step is a set of 364,419question-answer pairs.Step 2: from the above corpus we remove the questionsand answers of obvious low quality.
We im-plement this filter with a simple heuristic bykeeping only questions and answers that haveat least 4 words each, out of which at least 1 isa noun and at least 1 is a verb.
This step filtersout questions like ?How to be excellent??
andanswers such as ?I don?t know?.
The outcomeof this step forms our answer collection C. Ccontains 142,627 question-answer pairs.7.Arguably, all these filters could be improved.
Forexample, the first step can be replaced by a questionclassifier (Li and Roth, 2005).
Similarly, the secondstep can be implemented with a statistical classifierthat ranks the quality of the content using both thetextual and non-textual information available in thedatabase (Jeon et al, 2006; Agichtein et al, 2008).We plan to further investigate these issues which arenot the main object of this work.6Nevertheless, the approach proposed here is independentof the question type.
We will explore answer ranking for othernon-factoid question types in future work.7The data will be available through the Yahoo!
Webscopeprogram (research-data-requests@yahoo-inc.com).The data was processed as follows.
The text wassplit at the sentence level, tokenized and PoS tagged,in the style of the Wall Street Journal Penn Tree-Bank (Marcus et al, 1993).
Each word was morpho-logically simplified using the morphological func-tions of the WordNet library8.
Sentences were an-notated with WNSS categories, using the tagger ofCiaramita and Altun (2006)9, which annotates textwith a 46-label tagset.
These tags, defined by Word-Net lexicographers, provide a broad semantic cat-egorization for nouns and verbs and include labelsfor nouns such as food, animal, body and feeling,and for verbs labels such as communication, con-tact, and possession.
Next, we parsed all sentenceswith the dependency parser of Attardi et al (2007)10.It is important to realize that the output of all men-tioned processing steps is noisy and contains plentyof mistakes, since the data has huge variability interms of quality, style, genres, domains etc., and do-main adaptation for the NLP tasks involved is stillan open problem (Dredze et al, 2007).We used 60% of the questions for training, 20%for development, and 20% for test.
The candidateanswer set for a given question is composed by onepositive example, i.e., its corresponding best answer,and as negative examples all the other answers re-trieved in the top N by the retrieval component.4 ExperimentsWe evaluate our results using two measures: meanPrecision at rank=1 (P@1) ?
i.e., the percentage ofquestions with the correct answer on the first posi-tion ?
and Mean Reciprocal Rank (MRR) ?
i.e., thescore of a question is 1/k, where k is the positionof the correct answer.
We use as baseline the outputof our answer retrieval component (Figure 1).
Thiscomponent uses the BM25 criterion, the highest per-forming IR model in our experiments.Table 2 lists the results obtained using this base-line and our best model (?Ranking?
in the table) onthe testing partition.
Since we are interested in theperformance of the ranking model, we evaluate onthe subset of questions where the correct answer isretrieved by answer retrieval in the top N answers(similar to Ko et al (2007)).
In the table we report8http://wordnet.princeton.edu9sourceforge.net/projects/supersensetag10http://sourceforge.net/projects/desr723MRR P@1N = 10 N = 15 N = 25 N = 50 N = 10 N = 15 N = 25 N = 50recall@N 26.25% 29.04% 32.81% 38.09% 26.25% 29.04% 32.81% 38.09%Baseline 61.33 56.12 50.31 43.74 45.94 41.48 36.74 31.66Ranking 68.72?0.01 63.84?0.01 57.76?0.07 50.72?0.01 54.22?0.01 49.59?0.03 43.98?0.09 37.99?0.01Improvement +12.04% +13.75% +14.80% +15.95% +18.02% +19.55% +19.70% +19.99%Table 2: Overall results for the test partition.results for several N values.
For completeness, weshow the percentage of questions that match this cri-terion in the ?recall@N?
row.Our ranking model was tuned strictly on the de-velopment set (i.e., feature selection and parame-ters of the translation models).
During training, thepresentation of the training instances is randomized,which generates a randomized ranking algorithm.We exploit this property to estimate the variance inthe results produced by each model and report theaverage result over 10 trials together with an esti-mate of the standard deviation.The baseline result shows that, for N = 15,BM25 alone can retrieve in first rank 41% of thecorrect answers, and MRR tells us that the correctanswer is often found within the first three answers(this is not so surprising if we remember that in thisconfiguration only questions with the correct answerin the first 15 were kept for the experiment).
Thebaseline results are interesting because they indicatethat the problem is not hopelessly hard, but it is farfrom trivial.
In principle, we see much room for im-provement over bag-of-word methods.Next we see that learning a weighted combina-tion of features yields consistently marked improve-ments: for example, for N = 15, the best modelyields a 19% relative improvement in P@1 and 14%in MRR.
More importantly, the results indicate thatthe model learned is stable: even though for themodel analyzed in Table 2 we used N = 15 in train-ing, we measure approximately the same relative im-provement as N increases during evaluation.These results provide robust evidence that: (a) wecan use publicly available online QA collections toinvestigate features for answer ranking without theneed for costly human evaluation, (b) we can exploitlarge and noisy online QA collections to improve theaccuracy of answer ranking systems and (c) readilyavailable and scalable NLP technology can be usedIter.
Feature Set MRR P@10 BM25(W) 56.06 41.12%1 + translation(Bg) 61.13 46.24%2 + overall match(D) 62.50 48.34%3 + translation(W) 63.00 49.08%4 + query-log avg(?2) 63.50 49.63%5 + answer spannormalized by A size 63.71 49.84%6 + query-log max(PMI) 63.87 50.09%7 + same word sequence 63.99 50.23%8 + translation(B) 64.03 50.30%9 + tfidf(W) 64.08 50.42%10 + same sentence match(W) 64.10 50.42%11 + informativeness:verb count 64.18 50.36%12 + tfidf(B) 64.22 50.36%13 + same word sequencenormalized by Q size 64.33 50.54%14 + query-log max(?2) 64.46 50.66%15 + same sentence match(W)normalized by Q size 64.55 50.78%16 + query-log avg(PMI) 64.60 50.88%17 + overall match(W) 64.65 50.91%Table 3: Summary of the model selection process.to improve lexical matching and translation models.In the remaining of this section we analyze the per-formance of the different features.Table 3 summarizes the outcome of our automaticgreedy feature selection process on the developmentset.
Where applicable, we show within parenthesesthe text representation for the corresponding feature.The process is initialized with a single feature thatreplicates the baseline model (BM25 applied to thebag-of-words (W) representation).
The algorithmincrementally adds to the feature set the feature thatprovides the highest MRR improvement in the de-velopment partition.
The process stops when no fea-tures yield any improvement.
The table shows that,while the features selected span all the four featuregroups introduced, the lion?s share is taken by thetranslation features: approximately 60% of the MRR724W B Bg D Dg W + W + W + B + W + B + BgB B + Bg Bg + D D + DgFG1 (Similarity) 0 +1.06 -2.01 +0.84 -1.75 +1.06 +1.06 +1.06 +1.06FG2 (Translation) +4.95 +4.73 +5.06 +4.63 +4.66 +5.80 +6.01 +6.36 +6.36FG3 (Frequency) +2.24 +2.33 +2.39 +2.27 +2.41 +3.56 +3.56 +3.62 +3.62Table 4: Contribution of NLP processors.
Scores are MRR improvements on the development set.improvement is achieved by these features.
The fre-quency/density features are responsible for approx-imately 23% of the improvement.
The rest is dueto the query-log correlation features.
This indicatesthat, even though translation models are the mostuseful, it is worth exploring approaches that com-bine several strategies for answer ranking.Note that if some features do not appear in Table 3it does not necessarily mean that they are useless.In some cases such features are highly correlatedwith features previously selected, which already ex-ploited their signal.
For example, most similarityfeatures (FG1) are correlated.
Because BM25(W)is part of the baseline model, the selection processchooses another FG1 feature only much later (iter-ation 9) when the model is significantly changed.On the other hand, some features do not provide auseful signal at all.
A notable example in this classis the web-based CCP feature, which was designedoriginally for factoid answer validation and does notadapt well to our problem.
Because the length ofnon-factoid answers is typically significantly largerthan in the factoid QA task, we have to discard alarge part of the query when computing hits(Q+A)to reach non-zero counts.
This means that the finalhit counts, hence the CCP value, are generally un-correlated with the original (Q,A) tuple.One interesting observation is that the first twofeatures chosen by our model selection process useinformation from the NLP processors.
The first cho-sen feature is the translation probability computedbetween the Bg question and answer representations(bigrams with words generalized to their WNSStags).
The second feature selected measures thenumber of syntactic dependencies from the questionthat are matched in the answer.
These results pro-vide empirical evidence that coarse semantic disam-biguation and syntactic parsing have a positive con-tribution to non-factoid QA, even in broad-coveragenoisy settings based on Web data.The above observation deserves a more detailedanalysis.
Table 4 shows the performance of our firstthree feature groups when they are applied to eachof the five text representations or incremental com-binations of representations.
For each model cor-responding to a table cell we use only the featuresfrom the corresponding feature group and represen-tation to avoid the correlation with features fromother groups.
We generate each best model usingthe same feature selection process described above.The left part of Table 4 shows that, generally, themodels using representations that include the outputof our NLP processors (Bg, D and Dg) improve overthe baseline (FG1 and W).11 However, comparableimprovements can be obtained with the simpler bi-gram representation (B).
This indicates that, in termsof individual contributions, our NLP processors canbe approximated with simpler n-gram models in thistask.
Hence, is it fair to say that syntactic and se-mantic analysis is useful for such Web QA tasks?While the above analysis seems to suggest a neg-ative answer, the right-hand side of Table 4 tells amore interesting story.
It shows that the NLP anal-ysis provides complementary information to the n-gram-based models.
The best models for the FG2and FG3 feature groups are obtained when combin-ing the n-gram representations with the representa-tions that use the output of the NLP processors (W +B + Bg + D).
The improvements are relatively small,but remarkable (e.g., see FG2) if we take into ac-count the significant scale of the evaluation.
Thisobservation correlates well with the analysis shownin Table 3, which shows that features using semantic(Bg) and syntactic (D) representations contribute themost on top of the IR model (BM25(W)).11The exception to this rule are the models FG1(Bg) andFG1(Dg).
This is caused by the fact that the BM25 formulais less forgiving with errors of the NLP processors (due to thehigh idf scores assigned to bigrams and dependencies), and theWNSS tagger is the least robust component in our pipeline.7255 Related WorkContent from community-built question-answersites can be retrieved by searching for similar ques-tions already answered (Jeon et al, 2005) andranked using meta-data information like answererauthority (Jeon et al, 2006; Agichtein et al, 2008).Here we show that the answer text can be success-fully used to improve answer ranking quality.
Ourmethod is complementary to the above approaches.In fact, it is likely that an optimal retrieval enginefrom social media should combine all these threemethodologies.
Moreover, our approach might haveapplications outside of social media (e.g., for open-domain web-based QA), because the ranking modelbuilt is based only on open-domain knowledge andthe analysis of textual content.In the QA literature, answer ranking for non-factoid questions has typically been performed bylearning question-to-answer transformations, eitherusing translation models (Berger et al, 2000; Sori-cut and Brill, 2006) or by exploiting the redundancyof the Web (Agichtein et al, 2001).
Girju (2003) ex-tracts non-factoid answers by searching for certainsemantic structures, e.g., causation relations as an-swers to causation questions.
In this paper we com-bine several methodologies, including the above,into a single model.
This approach allowed us to per-form a systematic feature analysis on a large-scalereal-world corpus and a comprehensive feature set.Recent work has showed that structured retrievalimproves answer ranking for factoid questions:Bilotti et al (2007) showed that matching predicate-argument frames constructed from the question andthe expected answer types improves answer ranking.Cui et al (2005) learned transformations of depen-dency paths from questions to answers to improvepassage ranking.
However, both approaches usesimilarity models at their core because they requirethe matching of the lexical elements in the searchstructures.
On the other hand, our approach al-lows the learning of full transformations from ques-tion structures to answer structures using translationmodels applied to different text representations.Our answer ranking framework is closest in spiritto the system of Ko et al (2007) or Higashinaka etal.
(2008).
However, the former was applied onlyto factoid QA and both are limited to similarity, re-dundancy and gazetteer-based features.
Our modeluses a larger feature set that includes correlation andtransformation-based features and five different con-tent representations.
Our evaluation is also carriedout on a larger scale.
Our work is also related to thatof Riezler et al (2007) where SMT-based query ex-pansion methods are used on data from FAQ pages.6 ConclusionsIn this work we described an answer ranking en-gine for non-factoid questions built using a largecommunity-generated question-answer collection.On one hand, this study shows that we can effec-tively exploit large amounts of available Web datato do research on NLP for non-factoid QA systems,without any annotation or evaluation cost.
This pro-vides an excellent framework for large-scale experi-mentation with various models that otherwise mightbe hard to understand or evaluate.
On the other hand,we expect the outcome of this process to help sev-eral applications, such as open-domain QA on theWeb and retrieval from social media.
For example,on the Web our ranking system could be combinedwith a passage retrieval system to form a QA systemfor complex questions.
On social media, our systemshould be combined with a component that searchesfor similar questions already answered; this outputcan possibly be filtered further by a content-qualitymodule that explores ?social?
features such as theauthority of users, etc.We show that the best ranking performanceis obtained when several strategies are combinedinto a single model.
We obtain the best resultswhen similarity models are aggregated with featuresthat model question-to-answer transformations, fre-quency and density of content, and correlation ofQA pairs with external collections.
While the fea-tures that model question-to-answer transformationsprovide most benefits, we show that the combinationis crucial for improvement.Lastly, we show that syntactic dependency pars-ing and coarse semantic disambiguation yield asmall, yet statistically significant performance in-crease on top of the traditional bag-of-words andn-gram representation.
We obtain these results us-ing only off-the-shelf NLP processors that were notadapted in any way for our task.726ReferencesG.
Attardi, F. Dell?Orletta, M. Simi, A. Chanev andM.
Ciaramita.
2007.
Multilingual Dependency Pars-ing and Domain Adaptation using DeSR.
Proc.
ofCoNLL Shared Task Session of EMNLP-CoNLL 2007.E.
Agichtein, C. Castillo, D. Donato, A. Gionis, and G.Mishne.
2008.
Finding High-Quality Content in So-cial Media, with an Application to Community-basedQuestion Answering.
Proc.
of WSDM.E.
Agichtein, S. Lawrence, and L. Gravano.
2001.Learning Search Engine Specific Query Transforma-tions for Question Answering.
Proc.
of WWW.A.
Berger, R. Caruana, D. Cohn, D. Freytag, and V. Mit-tal.
2000.
Bridging the Lexical Chasm: StatisticalApproaches to Answer Finding.
Proc.
of SIGIR.M.
Bilotti, P. Ogilvie, J. Callan, and E. Nyberg.
2007.Structured Retrieval for Question Answering.
Proc.
ofSIGIR.E.
Brill, J. Lin, M. Banko, S. Dumais, and A. Ng.
2001.Data-Intensive Question Answering.
Proc.
of TREC.P.
Brown, S. Della Pietra, V. Della Pietra, R. Mercer.1993.
The Mathematics of Statistical Machine Trans-lation: Parameter Estimation.
Computational Linguis-tics, 19(2).M.
Ciaramita and Y. Altun.
2006.
Broad Coverage SenseDisambiguation and Information Extraction with a Su-persense Sequence Tagger.
Proc.
of EMNLP.M.
Ciaramita, V. Murdock and V. Plachouras.
2008.
Se-mantic Associations for Contextual Advertising.
2008.Journal of Electronic Commerce Research - Special Is-sue on Online Advertising and Sponsored Search, 9(1),pp.1-15.M.
Collins and N. Duffy.
2001.
Convolution Kernels forNatural Language.
Proc.
of NIPS 2001.H.
Cui, R. Sun, K. Li, M. Kan, and T. Chua.
2005.
Ques-tion Answering Passage Retrieval Using DependencyRelations.
Proc.
of SIGIR.M.
Dredze, J. Blitzer, P. Pratim Talukdar, K. Ganchev,J.
Graca, and F. Pereira.
2007.
Frustratingly HardDomain Adaptation for Parsing.
In Proc.
of EMNLP-CoNLL 2007 Shared Task.A.
Echihabi and D. Marcu.
2003.
A Noisy-Channel Ap-proach to Question Answering.
Proc.
of ACL.Y.
Freund and R.E.
Schapire.
1999.
Large margin clas-sification using the perceptron algorithm.
MachineLearning, 37, pp.
277-296.R.
Girju.
2003.
Automatic Detection of Causal Relationsfor Question Answering.
Proc.
of ACL, Workshop onMultilingual Summarization and Question Answering.S.
Harabagiu, D. Moldovan, M. Pasca, R. Mihalcea,M.
Surdeanu, R. Bunescu, R. Girju, V. Rus, and P.Morarescu.
2000.
Falcon: Boosting Knowledge forAnswer Engines.
Proc.
of TREC.R.
Higashinaka and H. Isozaki.
2008.
Corpus-basedQuestion Answering for why-Questions.
Proc.
of IJC-NLP.J.
Jeon, W. B. Croft, and J. H. Lee.
2005.
Finding Simi-lar Questions in Large Question and Answer Archives.Proc.
of CIKM.J.
Jeon, W. B. Croft, J. H. Lee, and S. Park.
2006.
AFramework to Predict the Quality of Answers withNon-Textual Features.
Proc.
of SIGIR.J.
Ko, T. Mitamura, and E. Nyberg.
2007.
Language-independent Probabilistic Answer Ranking.
for Ques-tion Answering.
Proc.
of ACL.X.
Li and D. Roth.
2005.
Learning Question Classifiers:The Role of Semantic Information.
Natural LanguageEngineering.B.
Magnini, M. Negri, R. Prevete, and H. Tanev.
2002.Comparing Statistical and Content-Based Techniquesfor Answer Validation on the Web.
Proc.
of the VIIIConvegno AI*IA.M.P.
Marcus, B. Santorini and M.A.
Marcinkiewicz.1993.
Building a large annotated corpus of En-glish: The Penn TreeBank.
Computational Linguis-tics, 19(2), pp.
313-330.D.
Moldovan, S. Harabagiu, M. Pasca, R. Mihalcea, R.Goodrum, R. Girju, and V. Rus.
1999.
LASSO - ATool for Surfing the Answer Net.
Proc.
of TREC.A.
Moschitti, S. Quarteroni, R. Basili and S. Manand-har.
2007.
Exploiting Syntactic and Shallow SemanticKernels for Question/Answer Classification.
Proc.
ofACL.S.
Robertson and S. Walker.
1997.
On relevance Weightswith Little Relevance Information.
Proc.
of SIGIR.R.
Soricut and E. Brill.
2006.
Automatic Question An-swering Using the Web: Beyond the Factoid.
Journalof Information Retrieval - Special Issue on Web Infor-mation Retrieval, 9(2).L.
Shen and A. Joshi.
2005.
Ranking and Rerankingwith Perceptron, Machine Learning.
Special Issue onLearning in Speech and Language Technologies, 60(1-3), pp.
73-96.S.
Riezler, A. Vasserman, I. Tsochantaridis, V. Mittaland Y. Liu.
2007.
Statistical Machine Translationfor Query Expansion in Answer Retrieval.
In Proc.of ACL.727
