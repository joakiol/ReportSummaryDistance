Proceedings of the ACL 2010 Conference Short Papers, pages 184?188,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsLearning Common Grammar from Multilingual CorpusTomoharu Iwata Daichi MochihashiNTT Communication Science Laboratories2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan{iwata,daichi,sawada}@cslab.kecl.ntt.co.jpHiroshi SawadaAbstractWe propose a corpus-based probabilis-tic framework to extract hidden commonsyntax across languages from non-parallelmultilingual corpora in an unsupervisedfashion.
For this purpose, we assume agenerative model for multilingual corpora,where each sentence is generated from alanguage dependent probabilistic context-free grammar (PCFG), and these PCFGsare generated from a prior grammar thatis common across languages.
We also de-velop a variational method for efficient in-ference.
Experiments on a non-parallelmultilingual corpus of eleven languagesdemonstrate the feasibility of the proposedmethod.1 IntroductionLanguages share certain common proper-ties (Pinker, 1994).
For example, the word orderin most European languages is subject-verb-object(SVO), and some words with similar forms areused with similar meanings in different languages.The reasons for these common properties can beattributed to: 1) a common ancestor language,2) borrowing from nearby languages, and 3) theinnate abilities of humans (Chomsky, 1965).We assume hidden commonalities in syntaxacross languages, and try to extract a commongrammar from non-parallel multilingual corpora.For this purpose, we propose a generative modelfor multilingual grammars that is learned in anunsupervised fashion.
There are some computa-tional models for capturing commonalities at thephoneme and word level (Oakes, 2000; Bouchard-Co?te?
et al, 2008), but, as far as we know, no at-tempt has been made to extract commonalities insyntax level from non-parallel and non-annotatedmultilingual corpora.In our scenario, we use probabilistic context-free grammars (PCFGs) as our monolingual gram-mar model.
We assume that a PCFG for eachlanguage is generated from a general model thatare common across languages, and each sentencein multilingual corpora is generated from the lan-guage dependent PCFG.
The inference of the gen-eral model as well as the multilingual PCFGs canbe performed by using a variational method forefficiency.
Our approach is based on a Bayesianmultitask learning framework (Yu et al, 2005;Daume?
III, 2009).
Hierarchical Bayesian model-ing provides a natural way of obtaining a joint reg-ularization for individual models by assuming thatthe model parameters are drawn from a commonprior distribution (Yu et al, 2005).2 Related workThe unsupervised grammar induction task hasbeen extensively studied (Carroll and Charniak,1992; Stolcke and Omohundro, 1994; Klein andManning, 2002; Klein and Manning, 2004; Lianget al, 2007).
Recently, models have been pro-posed that outperform PCFG in the grammar in-duction task (Klein and Manning, 2002; Klein andManning, 2004).
We used PCFG as a first stepfor capturing commonalities in syntax across lan-guages because of its simplicity.
The proposedframework can be used for probabilistic grammarmodels other than PCFG.Grammar induction using bilingual parallel cor-pora has been studied mainly in machine transla-tion research (Wu, 1997; Melamed, 2003; Eisner,2003; Chiang, 2005; Blunsom et al, 2009; Sny-der et al, 2009).
These methods require sentence-aligned parallel data, which can be costly to obtainand difficult to scale to many languages.
On theother hand, our model does not require sentencesto be aligned.
Moreover, since the complexity ofour model increases linearly with the number oflanguages, our model is easily applicable to cor-184pora of more than two languages, as we will showin the experiments.
To our knowledge, the onlygrammar induction work on non-parallel corporais (Cohen and Smith, 2009), but their method doesnot model a common grammar, and requires priorinformation such as part-of-speech tags.
In con-trast, our method does not require any such priorinformation.3 Proposed Method3.1 ModelLet X = {X l}l?L be a non-parallel and non-annotated multilingual corpus, where X l is a setof sentences in language l, and L is a set of lan-guages.
The task is to learn multilingual PCFGsG = {Gl}l?L and a common grammar that gen-erates these PCFGs.
Here, Gl = (K,W l,?l)represents a PCFG of language l, where K is aset of nonterminals, W l is a set of terminals, and?l is a set of rule probabilities.
Note that a set ofnonterminals K is shared among languages, buta set of terminals W l and rule probabilities ?lare specific to the language.
For simplicity, weconsider Chomsky normal form grammars, whichhave two types of rules: emissions rewrite a non-terminal as a terminal A ?
w, and binary pro-ductions rewrite a nonterminal as two nontermi-nalsA?
BC, whereA,B,C ?K and w ?W l.The rule probabilities for each nonterminalA of PCFG Gl in language l consist of: 1)?Al = {?lAt}t?
{0,1}, where ?lA0 and ?lA1 repre-sent probabilities of choosing the emission ruleand the binary production rule, respectively, 2)?lA = {?lABC}B,C?K , where ?lABC repre-sents the probability of nonterminal productionA ?
BC, and 3) ?lA = {?lAw}w?W l , where?lAw represents the probability of terminal emis-sion A?
w. Note that ?lA0 + ?lA1 = 1, ?lAt ?
0,?B,C ?lABC = 1, ?lABC ?
0,?w ?lAw = 1,and ?lAw ?
0.
In the proposed model, multino-mial parameters ?lA and ?lA are generated fromDirichlet distributions that are common across lan-guages: ?lA ?
Dir(?
?A) and ?lA ?
Dir(?
?A),since we assume that languages share a commonsyntax structure.
?
?A and ?
?A represent the param-eters of a common grammar.
We use the Dirichletprior because it is the conjugate prior for the multi-nomial distribution.
In summary, the proposedmodel assumes the following generative processfor a multilingual corpus,1.
For each nonterminal A ?K :???Aa,ba,b|L|?A?
lAlA|K|?
???
lA |L|z 1z 2 z 3x2 x3???
?Figure 1: Graphical model.
(a) For each rule type t ?
{0, 1}:i.
Draw common rule type parameters?
?At ?
Gam(a?, b?
)(b) For each nonterminal pair (B,C):i.
Draw common production parameters?
?ABC ?
Gam(a?, b?)2.
For each language l ?
L:(a) For each nonterminal A ?K :i.
Draw rule type parameters?lA ?
Dir(??A)ii.
Draw binary production parameters?lA ?
Dir(??A)iii.
Draw emission parameters?lA ?
Dir(??
)(b) For each node i in the parse tree:i.
Choose rule typetli ?
Mult(?lzi)ii.
If tli = 0:A. Emit terminalxli ?
Mult(?lzi)iii.
Otherwise:A.
Generate children nonterminals(zlL(i), zlR(i)) ?
Mult(?lzi),where L(i) and R(i) represent the left and rightchildren of node i.
Figure 1 shows a graphi-cal model representation of the proposed model,where the shaded and unshaded nodes indicate ob-served and latent variables, respectively.3.2 InferenceThe inference of the proposed model can be ef-ficiently computed using a variational Bayesianmethod.
We extend the variational method tothe monolingual PCFG learning of Kurihara andSato (2004) for multilingual corpora.
The goalis to estimate posterior p(Z,?,?|X), where Zis a set of parse trees, ?
= {?l}l?L is aset of language dependent parameters, ?l ={?lA,?lA,?lA}A?K , and ?
= {??A,?
?A}A?Kis a set of common parameters.
In the variationalmethod, posterior p(Z,?,?|X) is approximatedby a tractable variational distribution q(Z,?,?
).185We use the following variational distribution,q(Z,?,?)
=?Aq(??A)q(??A)?l,dq(zld)?
?l,Aq(?lA)q(?lA)q(?lA), (1)where we assume that hyperparameters q(?
?A) andq(?
?A) are degenerated, or q(?)
= ???(?
), andinfer them by point estimation instead of distribu-tion estimation.
We find an approximate posteriordistribution that minimizes the Kullback-Leiblerdivergence from the true posterior.
The variationaldistribution of the parse tree of the dth sentence inlanguage l is obtained as follows,q(zld) ??A?BC(pi?lA1pi?lABC)C(A?BC;zld,l,d)?
?A?w(pi?lA0pi?lAw)C(A?w;zld,l,d), (2)where C(r; z, l, d) is the count of rule r that oc-curs in the dth sentence of language l with parsetree z.
The multinomial weights are calculated asfollows,pi?lAt = exp(Eq(?lA)[log ?lAt]), (3)pi?lABC = exp(Eq(?lA)[log ?lABC]), (4)pi?lAw = exp(Eq(?lA)[log?lAw]).
(5)The variational Dirichlet parameters for q(?lA) =Dir(?
?lA), q(?lA) = Dir(?
?lA), and q(?lA) =Dir(?
?lA), are obtained as follows,?
?lAt = ?
?At +?d,zldq(zld)C(A, t; zld, l, d), (6)?
?lABC = ?
?ABC+?d,zldq(zld)C(A?BC; zld, l, d),(7)?
?lAw = ??
+?d,zldq(zld)C(A?
w; zld, l, d),(8)where C(A, t; z, l, d) is the count of rule type tthat is selected in nonterminal A in the dth sen-tence of language l with parse tree z.The common rule type parameter ?
?At that min-imizes the KL divergence between the true pos-terior and the approximate posterior can be ob-tained by using the fixed-point iteration methoddescribed in (Minka, 2000).
The update rule is asfollows,??
(new)At ?a??1+??AtL(?(?t?
??At?)??(??At))b?
+?l(?(?t?
??lAt?)??(?
?lAt)) ,(9)where L is the number of languages, and ?
(x) =?
log ?
(x)?x is the digamma function.
Similarly, thecommon production parameter ?
?ABC can be up-dated as follows,??
(new)ABC ?a?
?
1 + ??ABCLJABCb?
+?l J ?lABC, (10)where JABC = ?(?B?,C?
??AB?C?)
?
?(?
?ABC),and J ?lABC = ?(?B?,C?
??lAB?C?)??(?
?lABC).Since factored variational distributions dependon each other, an optimal approximated posteriorcan be obtained by updating parameters by (2) -(10) alternatively until convergence.
The updat-ing of language dependent distributions by (2) -(8) is also described in (Kurihara and Sato, 2004;Liang et al, 2007) while the updating of commongrammar parameters by (9) and (10) is new.
Theinference can be carried out efficiently using theinside-outside algorithm based on dynamic pro-gramming (Lari and Young, 1990).After the inference, the probability of a com-mon grammar rule A ?
BC is calculated by?
?A?BC = ??1?
?ABC , where ?
?1 = ??1/(?
?0 + ?
?1)and ?
?ABC = ??ABC/?B?,C?
??AB?C?
representthe mean values of ?l0 and ?lABC , respectively.4 Experimental resultsWe evaluated our method by employing the Eu-roParl corpus (Koehn, 2005).
The corpus con-sists of the proceedings of the European Parlia-ment in eleven western European languages: Dan-ish (da), German (de), Greek (el), English (en),Spanish (es), Finnish (fi), French (fr), Italian (it),Dutch (nl), Portuguese (pt), and Swedish (sv), andit contains roughly 1,500,000 sentences in eachlanguage.
We set the number of nonterminals at|K| = 20, and omitted sentences with more thanten words for tractability.
We randomly sampled100,000 sentences for each language, and ana-lyzed them using our method.
It should be notedthat our random samples are not sentence-aligned.Figure 2 shows the most probable terminals ofemission for each language and nonterminal witha high probability of selecting the emission rule.1862: verb and auxiliary verb (V)5: noun (N)7: subject (SBJ)9: preposition (PR)11: punctuation (.
)13: determiner (DT)Figure 2: Probable terminals of emission for eachlanguage and nonterminal.0?
16 11 (R?
S . )
0.1116?
7 6 (S?
SBJ VP) 0.066?
2 12 (VP?
V NP) 0.0412?
13 5 (NP?
DT N) 0.1915?
17 19 (NP?
NP N) 0.0717?
5 9 (NP?
N PR) 0.0715?
13 5 (NP?
DT N) 0.06Figure 3: Examples of inferred common gram-mar rules in eleven languages, and their proba-bilities.
Hand-provided annotations have the fol-lowing meanings, R: root, S: sentence, NP: nounphrase, VP: verb phrase, and others appear in Fig-ure 2.We named nonterminals by using grammatical cat-egories after the inference.
We can see that wordsin the same grammatical category clustered acrosslanguages as well as within a language.
Fig-ure 3 shows examples of inferred common gram-mar rules with high probabilities.
Grammar rulesthat seem to be common to European languageshave been extracted.5 DiscussionWe have proposed a Bayesian hierarchical PCFGmodel for capturing commonalities at the syntaxlevel for non-parallel multilingual corpora.
Al-though our results have been encouraging, a num-ber of directions remain in which we must extendour approach.
First, we need to evaluate our modelquantitatively using corpora with a greater diver-sity of languages.
Measurement examples includethe perplexity, and machine translation score.
Sec-ond, we need to improve our model.
For ex-ample, we can infer the number of nonterminalswith a nonparametric Bayesian model (Liang etal., 2007), infer the model more robustly basedon a Markov chain Monte Carlo inference (John-son et al, 2007), and use probabilistic grammarmodels other than PCFGs.
In our model, all themultilingual grammars are generated from a gen-eral model.
We can extend it hierarchically usingthe coalescent (Kingman, 1982).
That model mayhelp to infer an evolutionary tree of languages interms of grammatical structure without the etymo-logical information that is generally used (Grayand Atkinson, 2003).
Finally, the proposed ap-proach may help to indicate the presence of a uni-versal grammar (Chomsky, 1965), or to find it.187ReferencesPhil Blunsom, Trevor Cohn, and Miles Osborne.
2009.Bayesian synchronous grammar induction.
In D. Koller,D.
Schuurmans, Y. Bengio, and L. Bottou, editors, Ad-vances in Neural Information Processing Systems 21,pages 161?168.Alexandre Bouchard-Co?te?, Percy Liang, Thomas Griffiths,and Dan Klein.
2008.
A probabilistic approach to lan-guage change.
In J.C. Platt, D. Koller, Y.
Singer, andS.
Roweis, editors, Advances in Neural Information Pro-cessing Systems 20, pages 169?176, Cambridge, MA.MIT Press.Glenn Carroll and Eugene Charniak.
1992.
Two experimentson learning probabilistic dependency grammars from cor-pora.
In Working Notes of the Workshop Statistically-Based NLP Techniques, pages 1?13.
AAAI.David Chiang.
2005.
A hierarchical phrase-based model forstatistical machine translation.
In ACL ?05: Proceedingsof the 43rd Annual Meeting on Association for Computa-tional Linguistics, pages 263?270, Morristown, NJ, USA.Association for Computational Linguistics.Norm Chomsky.
1965.
Aspects of the Theory of Syntax.
MITPress.Shay B. Cohen and Noah A. Smith.
2009.
Shared logisticnormal distributions for soft parameter tying in unsuper-vised grammar induction.
In NAACL ?09: Proceedings ofHuman Language Technologies: The 2009 Annual Con-ference of the North American Chapter of the Associationfor Computational Linguistics, pages 74?82, Morristown,NJ, USA.
Association for Computational Linguistics.Hal Daume?
III.
2009.
Bayesian multitask learning with la-tent hierarchies.
In Proceedings of the Twenty-Fifth An-nual Conference on Uncertainty in Artificial Intelligence(UAI-09), pages 135?142, Corvallis, Oregon.
AUAI Press.Jason Eisner.
2003.
Learning non-isomorphic tree mappingsfor machine translation.
In ACL ?03: Proceedings of the41st Annual Meeting on Association for ComputationalLinguistics, pages 205?208, Morristown, NJ, USA.
As-sociation for Computational Linguistics.Russell D. Gray and Quentin D. Atkinson.
2003.
Language-tree divergence times support the Anatolian theory ofIndo-European origin.
Nature, 426(6965):435?439,November.Mark Johnson, Thomas Griffiths, and Sharon Goldwater.2007.
Bayesian inference for PCFGs via Markov chainMonte Carlo.
In Human Language Technologies 2007:The Conference of the North American Chapter of theAssociation for Computational Linguistics; Proceedingsof the Main Conference, pages 139?146, Rochester, NewYork, April.
Association for Computational Linguistics.J.
F. C. Kingman.
1982.
The coalescent.
Stochastic Pro-cesses and their Applications, 13:235?248.Dan Klein and Christopher D. Manning.
2002.
A generativeconstituent-context model for improved grammar induc-tion.
In ACL ?02: Proceedings of the 40th Annual Meet-ing on Association for Computational Linguistics, pages128?135, Morristown, NJ, USA.
Association for Compu-tational Linguistics.Dan Klein and Christopher D. Manning.
2004.
Corpus-based induction of syntactic structure: models of depen-dency and constituency.
In ACL ?04: Proceedings of the42nd Annual Meeting on Association for ComputationalLinguistics, page 478, Morristown, NJ, USA.
Associationfor Computational Linguistics.Philipp Koehn.
2005.
Europarl: A parallel corpus for sta-tistical machine translation.
In Proceedings of the 10thMachine Translation Summit, pages 79?86.Kenichi Kurihara and Taisuke Sato.
2004.
An applica-tion of the variational Bayesian approach to probabilisticcontext-free grammars.
In International Joint Conferenceon Natural Language Processing Workshop Beyond Shal-low Analysis.K.
Lari and S.J.
Young.
1990.
The estimation of stochasticcontext-free grammars using the inside-outside algorithm.Computer Speech and Language, 4:35?56.Percy Liang, Slav Petrov, Michael I. Jordan, and Dan Klein.2007.
The infinite PCFG using hierarchical dirichlet pro-cesses.
In EMNLP ?07: Proceedings of the EmpiricalMethods on Natural Language Processing, pages 688?697.I.
Dan Melamed.
2003.
Multitext grammars and syn-chronous parsers.
In NAACL ?03: Proceedings of the 2003Conference of the North American Chapter of the Associ-ation for Computational Linguistics on Human LanguageTechnology, pages 79?86, Morristown, NJ, USA.
Associ-ation for Computational Linguistics.Thomas Minka.
2000.
Estimating a Dirichlet distribution.Technical report, M.I.T.Michael P. Oakes.
2000.
Computer estimation of vocabu-lary in a protolanguage from word lists in four daughterlanguages.
Journal of Quantitative Linguistics, 7(3):233?243.Steven Pinker.
1994.
The Language Instinct: How the MindCreates Language.
HarperCollins, New York.Benjamin Snyder, Tahira Naseem, and Regina Barzilay.2009.
Unsupervised multilingual grammar induction.
InProceedings of the Joint Conference of the 47th AnnualMeeting of the ACL and the 4th International Joint Con-ference on Natural Language Processing of the AFNLP,pages 73?81, Suntec, Singapore, August.
Association forComputational Linguistics.Andreas Stolcke and Stephen M. Omohundro.
1994.
In-ducing probabilistic grammars by Bayesian model merg-ing.
In ICGI ?94: Proceedings of the Second InternationalColloquium on Grammatical Inference and Applications,pages 106?118, London, UK.
Springer-Verlag.Dekai Wu.
1997.
Stochastic inversion transduction gram-mars and bilingual parsing of parallel corpora.
Comput.Linguist., 23(3):377?403.Kai Yu, Volker Tresp, and Anton Schwaighofer.
2005.Learning gaussian processes from multiple tasks.
InICML ?05: Proceedings of the 22nd International Confer-ence on Machine Learning, pages 1012?1019, New York,NY, USA.
ACM.188
