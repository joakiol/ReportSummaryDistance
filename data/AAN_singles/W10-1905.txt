Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 37?45,Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational LinguisticsA Comparative Study of Syntactic Parsers for Event ExtractionMakoto Miwa1 Sampo Pyysalo1 Tadayoshi Hara1 Jun?ichi Tsujii1,2,31Department of Computer Science, the University of Tokyo, JapanHongo 7-3-1, Bunkyo-ku, Tokyo, Japan.2School of Computer Science, University of Manchester, UK3National Center for Text Mining, UK{mmiwa,smp,harasan,tsujii}@is.s.u-tokyo.ac.jpAbstractThe extraction of bio-molecular eventsfrom text is an important task for a numberof domain applications such as pathwayconstruction.
Several syntactic parsershave been used in Biomedical NaturalLanguage Processing (BioNLP) applica-tions, and the BioNLP 2009 Shared Taskresults suggest that incorporation of syn-tactic analysis is important to achievingstate-of-the-art performance.
Direct com-parison of parsers is complicated by to dif-ferences in the such as the division be-tween phrase structure- and dependency-based analyses and the variety of outputformats, structures and representations ap-plied.
In this paper, we present a task-oriented comparison of five parsers, mea-suring their contribution to bio-molecularevent extraction using a state-of-the-artevent extraction system.
The results showthat the parsers with domain models usingdependency formats provide very similarperformance, and that an ensemble of dif-ferent parsers in different formats can im-prove the event extraction system.1 IntroductionBio-molecular events are useful for modeling andunderstanding biological systems, and their au-tomatic extraction from text is one of the keytasks in Biomedical Natural Language Process-ing (BioNLP).
In the BioNLP 2009 Shared Taskon event extraction, participants constructed eventextraction systems using a variety of differentparsers, and the results indicated that the use ofa parser was correlated with high ranking in thetask (Kim et al, 2009).
By contrast, the resultsdid not indicate a clear preference for a particularparser, and there has so far been no direct compar-ison of different parsers for event extraction.While the outputs of parsers applying the sameout format can be compared using a gold standardcorpus, it is difficult to perform meaningful com-parison of parsers applying different frameworks.Additionally, it is still an open question to what ex-tent high performance on a gold standard treebankcorrelates with usefulness at practical tasks.
Task-based comparisons of parsers provide not only away to asses parsers across frameworks but also anecessary measure of their practical applicability.In this paper, five different parsers are com-pared on the bio-molecular event extraction taskdefined in the BioNLP 2009 Shared Task using astate-of-the-art event extraction system.
The datasets share abstracts with GENIA treebank, and thetreebank is used as an evaluation standard.
Theoutputs of the parsers are converted into two de-pendency formats with the help of existing conver-sion methods, and the outputs are compared in thetwo dependency formats.
The evaluation resultsshow that different syntactic parsers with domainmodels in the same dependency format achieveclosely similar performance, and that an ensembleof different syntactic parsers in different formatscan improve the performance of an event extrac-tion system.2 Bio-molecular Event Extraction withSeveral Syntactic ParsersThis paper focuses on the comparison of severalsyntactic parsers on a bio-molecular event extrac-tion task with a state-of-the-art event extractionsystem.
This section explains the details of thecomparison.
Section 2.1 presents the event ex-37traction task setting, following that of the BioNLP2009 Shared Task.
Section 2.2 then summa-rizes the five syntactic parsers and three formatsadopted for the comparison.
Section 2.3 describedhow the state-of-the-art event extraction system ofMiwa et al (2010) is modified and used for thecomparison.2.1 Bio-molecular Event ExtractionThe bio-molecular event extraction task consid-ered in this study is that defined in the BioNLP2009 Shared Task (Kim et al, 2009)1.
The sharedtask provided common and consistent task defi-nitions, data sets for training and evaluation, andevaluation criteria.
The shared task consists ofthree subtasks: core event extraction (Task 1),augmenting events with secondary arguments(Task 2), and the recognition of speculation andnegation of the events (Task 3) (Kim et al, 2009).In this paper we consider Task 1 and Task 2.
Theshared task defined nine event types, which can bedivided into five simple events (Gene expression,Transcription, Protein catabolism, Phosphoryla-tion, and Localization) that take one core argu-ment, a multi-participant binding event (Bind-ing), and three regulation events (Regulation, Pos-itive regulation, and Negative regulation) that cantake other events as arguments.In the two tasks considered, events are repre-sented with a textual trigger, type, and arguments,where the trigger is a span of text that states theevent in text.
In Task 1 the event arguments thatneed to be extracted are restricted to the core ar-guments Theme and Cause, and secondary argu-ments (locations and sites) need to be attached inTask 2.2.2 Parsers and FormatsFive parsers and three formats are adopted forthe evaluation.
The parsers are GDep (Sagae andTsujii, 2007)2, the Bikel parser (Bikel) (Bikel,2004)3, the Charniak-Johnson reranking parser,using David McClosky?s self-trained biomedi-cal parsing model (MC) (McClosky, 2009)4, theC&C CCG parser, adapted to biomedical text1http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/SharedTask/2http://www.cs.cmu.edu/?sagae/parser/gdep/3http://www.cis.upenn.edu/?dbikel/software.html4http://www.cs.brown.edu/?dmcc/biomedical.html     Figure 1: Stanford basic dependency tree        	Figure 2: CoNLL-X dependency tree     Figure 3: Predicate Argument StructureFigure 4: Format conversion dependencies in fiveparsers.
Formats adopted for the evaluation isshown in solid boxes.
SD: Stanford Dependencyformat, CCG: Combinatory Categorial Grammaroutput format, PTB: Penn Treebank format, andPAS: Predicate Argument Structure in Enju for-mat.
(C&C) (Rimell and Clark, 2009)5, and the Enjuparser with the GENIA model (Miyao et al,2009)6.
The formats are Stanford Dependencies(SD) (Figure 1), the CoNLL-X dependency for-mat (Figure 2) and the predicate-argument struc-ture (PAS) format used by Enju (Figure 3).
Withthe exception of Enju, the analyses of these parserswere provided by the BioNLP 2009 Shared Taskorganizers.
Analysis of system features in the taskfound that the use of parser output with one of5http://svn.ask.it.usyd.edu.au/trac/candc/6http://www-tsujii.is.s.u-tokyo.ac.jp/enju/38the formats considered here correlated with highrank at the task (Kim et al, 2009).
A number ofthese parsers have also been shown to be effectivefor protein-protein interactions extraction (Miyaoet al, 2009).The five parsers operate in a number of differentframeworks, reflected in their analyses.
GDep is anative dependency parser that produces CoNLL-X-format dependency trees.
MC and Bikel arephrase-structure parsers, and they produce PennTreebank (PTB) format analyses.
C&C is a deepparser based on Combinatory Categorial Gram-mar (CCG), and its native output is in a CCG-specific format.
The output of C&C is convertedinto SD by a rule-based conversion script (Rimelland Clark, 2009).
Enju is deep parser based onHead-driven Phrase Structure Grammar (HPSG)and produces a format containing predicate argu-ment structures (PAS) along with a phrase struc-ture tree in Enju format.To study the contribution of the formats inwhich the five parsers output their analyses to taskperformance, we apply a number of conversionsbetween the outputs, shown in Figure 4.
The EnjuPAS output is converted into Penn Treebank for-mat using the method introduced by (Miyao et al,2009).
SD is generated from PTB by the Stan-ford tools (de Marneffe et al, 2006)7, and CoNLL-X dependencies are generated from PTB by us-ing Treebank Converter (Johansson and Nugues,2007)8.
We note that all of these conversions canintroduce some errors in the conversion process.With the exception of Bikel, all the appliedparsers have models specifically adapted forbiomedical text.
Further, all of the biomedical do-main models have been created with reference andfor many parsers with direct training on the dataof (a subset of) the GENIA treebank (Tateisi etal., 2005).
The results of parsing with these mod-els as provided for the BioNLP Shared Task areused in this comparison.
However, we note thatthe shared task data, drawn from the GENIA eventcorpus (Kim et al, 2008), contains abstracts thatare also in the GENIA treebank.
This implies thatthe parsers are likely to perform better on the textsused in the shared task than on other biomedicaldomain text, and similarly that systems buildingon their output are expected to achieve best per-7http://www-nlp.stanford.edu/software/lex-parser.shtml8http://nlp.cs.lth.se/software/treebank converter/formance on this data.
However, it does not in-validate comparison within the dataset.
We fur-ther note that the models do not incorporate anyknowledge of the event annotations of the sharedtask.2.3 Event Extraction SystemThe system by Miwa et al (2010) is adopted forthe evaluation.
The system was originally devel-oped for finding core events (Task 1 in the BioNLP2009 Shared Task) using Enju and GDep with thenative output of these parsers.
The system con-sists of three supervised classification-based mod-ules: a trigger detector, an event edge detector,and a complex event detector.
The trigger detec-tor classifies each word into the appropriate eventtypes, the event edge detector classifies each edgebetween an event and a protein into an argumenttype, and the complex event detector classifiesevent candidates constructed by all edge combina-tions, deciding between event and non-event.
Thesystem uses one-vs-all support vector machines(SVMs) for the classifications.The system operates on one sentence at a time,building features for classification based on thesyntactic analyses for the sentence provided bythe two parsers as well as the sequence of thewords in the sentence, including the target candi-date.
The features include the constituents/wordsaround entities (triggers and proteins), the depen-dencies, and the shortest paths among the enti-ties.
The feature generation is format-independentregarding the shared properties of different for-mats, but makes use also of format-specific infor-mation when available for extracting features, in-cluding the dependency tags, word-related infor-mation (e.g.
a lexical entry in Enju format), andthe constituents and their head information.The previously introduced base system is hereimproved with two modifications.
One modifica-tion is removing two classes of features from theoriginal features (for details of the original featurerepresentation, we refer to (Miwa et al, 2010));specifically the features representing governor-dependent relationships from the target word, andthe features representing each event edges in thecomplex event detector are removed.
The othermodification is to use head words in a trigger ex-pression as a gold trigger word.
This modificationis inspired by the part-of-speech (POS) based se-lection proposed by Kilicoglu and Bergler (2009).39The system uses a head word ?in?
as a triggerword in a trigger expression ?in the presence of?instead of using all the words of the expression.In cases where there is no head word informationin a parser output, head words are selected heuris-tically: if a word does not modify another wordin the trigger expression, the word is selected as ahead word.The system is also modified to find secondaryarguments (Task 2 in the BioNLP 2009 SharedTask).
The second arguments are treated as ad-ditional arguments in Task 1: the trigger detec-tor finds secondary argument candidates, the eventedge detector finds secondary argument edge can-didates, and the complex event detector findsevents including secondary arguments.
The fea-tures are extracted using the same feature extrac-tion method as for regulation events taking pro-teins as arguments.3 Evaluation SettingEvent extraction performance is evaluated usingthe evaluation script provided by the BioNLP?09shared task organizers9 for the development dataset, and the online evaluation system of the task10for the test data set.
Results are reported underthe official evaluation criterion of the task, i.e.
the?Approximate Span Matching/Approximate Re-cursive Matching?
criterion.
Task 1 and Task 2are solved at once for the evaluation.As discussed in Section 2.2, the texts of the GE-NIA treebank are shared with the shared task datasets, which allows the gold annotations of the tree-bank to be used for reference.
The GENIA tree-bank is converted into the Enju format with Enju.When the trees in the treebank cannot be convertedinto the Enju format, parse results are used in-stead.
The GENIA treebank is also converted intoPTB format11.
The treebank is then converted intothe dependency formats with the conversions de-scribed in Section 2.2.
While based on manuallyannotated gold data, the converted treebanks arenot always correct due to conversion errors.The event extraction system described in Sec-tion 2.3 is used with the default settings shown in(Miwa et al, 2010).
The positive and negative ex-9http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/SharedTask/downloads.shtml10http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/SharedTask/eval-test.shtml11http://categorizer.tmit.bme.hu/?illes/genia ptb/BD CD CDP CTDTask 1 55.60 54.35 54.59 54.42Task 2 53.94 52.65 52.88 52.76Table 1: Comparison of the F-score results withdifferent Stanford dependency variants on the de-velopment data set with the MC parser.
Results forbasic dependencies (BD), collapsed dependencies(CD), collapsed dependencies with propagation ofconjunct dependencies (CDP), and collapsed treedependencies (CTD) are shown.
The best score ineach task is shown in bold.      	Figure 5: Stanford collapsed dependencies withpropagation of conjunct dependenciesamples are balanced by placing more weight onthe positive examples.
The examples predictedwith confidence greater than 0.5, as well as theexamples with the most confident labels, are ex-tracted.
The C-values of SVMs are set to 1.0.Some of the parse results do not include wordbase forms or part-of-speech (POS) tags, whichare required by the event extraction system.
Toapply these parsers, the GENIA Tagger (Tsuruokaet al, 2005) output is adopted to add this informa-tion to the results.4 EvaluationResults of event extraction with the setting in Sec-tion 2.3 will be presented in this section.
Sec-tion 4.1 considers the effect of different variantsof the Stanford Dependency representation.
Sec-tion 4.2 presents the results of experiments withdifferent parsers, and Section 4.3 shows the per-formance with ensembles of multiple parsers.
Fi-nally, the performance of the event extraction sys-tem is discussed in context of other proposedmethods for the task in Section 4.4.4.1 Stanford Dependency SettingStanford dependencies have four different vari-ants: basic dependencies (BD), collapsed depen-dencies (CD), collapsed dependencies with prop-agation of conjunct dependencies (CDP), and col-lapsed tree dependencies (CTD) (de Marneffe and40BD CD CDP CTDTask 1 54.22 54.37 53.88 53.84(-1.38) (+0.02) (-0.71) (-0.58)Task 2 52.73 52.80 52.31 52.35(-1.21) (+0.15) (-0.57) (-0.41)Table 2: Comparison of the F-score results withdifferent Stanford dependency variants withoutdependency types.Manning, 2008).
Except for BD, these variants donot necessarily connect all the words in the sen-tence, and CD and CDP do not necessarily forma tree structure.
Figure 5 shows an example ofCDP converted from the tree in Figure 1.
To se-lect a suitable alternative for the comparative ex-periments, we first compared these variants as apreliminary experiment.
Table 1 shows the com-parison results with the MC parser.
Dependenciesare generalized by removing expressions after ?
?of the dependencies (e.g.
?
with?
in prep with) forbetter performance.
We find that basic dependen-cies give the best performance to event extraction,with little difference between the other variants.This result is surprising, as variants other than ba-sic have features such as the resolution of con-junctions that are specifically designed for prac-tical applications.
However, basic dependenden-cies were found to consistently provide best per-formance also for the other parsers12.The SD variants differ from each other in twokey aspects: the dependency structure and the de-pendency types.
To gain insight into why thebasic dependencies should provide better perfor-mance than other variants, we performed an ex-periment attempting to isolate these factors by re-peating the evaluation while eliminating the de-pendency types.
The results of this evaluation areshown in Table 2.
The results indicate that thecontribution of the dependency types to extractionperformance differs between the variants: the ex-pected performance drop is most notable for thebasic dependencies, and for the collapsed depen-dencies there is even a minute increase in per-formance, making results for collapsed dependen-cies best of the untyped results (by a very narrowmargin).
While this result doesn?t unambiguouslypoint to a specific explanation for why basic de-pendencies provide best performance when types12Collapsed tree dependencies are not evaluated on theC&C parser since the conversion is not provided.are not removed, possible explanations include er-rors in typing or sparseness issues causing prob-lems in generalization for the types of non-basicdependencies.
While achieving a clear resolutionof the results of the comparison between SD vari-ants requires more analysis, from a performanceoptimization perspective the results present an un-complicated choice.
Thus, in the following eval-uation, the basic dependencies are adopted for allSD results.4.2 Parser ComparisonResults with different parsers and different for-mats on the development data set are summarizedin Table 3.
Baseline results are produced by re-moving dependency (or PAS) information fromthe parse results.
The baseline results differ be-tween the represetations as the word base formsand POS tags produced by the GENIA tagger foruse with the Stanford dependency and CoNLL-X formats are different from those for Enju, andbecause head word information in Enju format isused.
The evaluation finds best results for bothtasks with Enju, using its native output format.However, as discussed in Section 2.3, the treat-ment of the Enju format and the other two formatsare slightly different, this result does not necessar-ily indicate that the Enju format is the best alter-native for event extraction.Unsurprisingly, we find that the Bikel parser,the only one in the comparison lacking a modeladapted to the biomedical domain, performs worsethan the other parsers.
For SD, we find best resultsfor C&C, which is notable as the parser output isprocessed into SD by a custom conversion, whileMC output uses the de facto conversion of theStanford tools.
Similarly, MC produces the bestresult for the CoNLL-X format, which is the na-tive output format of GDep.
Enju and GDep pro-duces comparable results to the best formats forboth tasks.
Overall, we find that event extractionresults for the parsers applying GENIA treebankmodels are largely comparable for the dependencyformats (SD and CoNLL-X).The results with the data derived from the GE-NIA treebank can be considered as upper boundsfor the parsers and formats at the task, althoughconversion errors are expected to lower thesebounds to some extent.
Even though trained onthe treebank, using the parsers does not provideperformance as high as that for using the GE-41Task 1 Task 2SD CoNLL PAS SD CoNLL PASBaseline 51.05 - 50.42 49.17 - 48.88GDep - 55.70 - - 54.37 -Bikel 53.29 53.22 - 51.40 51.27 -MC 55.60 56.01 - 53.94 54.51 -C&C 56.09 - - 54.27 - -Enju 55.48 55.74 56.57 54.06 54.37 55.31GENIA 56.34 56.09 57.94 55.04 54.57 56.40Table 3: Comparison of F-score results with five parsers in three different formats on the developmentdata set.
SD: Stanford basic Dependency format, CoNLL: CoNLL-X format, and PAS: Predicate Argu-ment Structure in Enju format.
Results without dependency (or PAS) information are shown as baselines.The results with the GENIA treebank (converted into PTB format and Enju format) are shown for com-parison (GENIA).
The best score in each task is shown in bold, and the best score in each task and formatis underlined.Task 1 Task 2C&C MC Enju C&C MC EnjuSD CoNLL CoNLL SD CoNLL CoNLLMC 57.44 - - 55.75 - -CoNLL (+1.35) - - (+1.24) - -Enju 56.47 56.24 - 54.85 54.70 -CoNLL (+0.38) (+0.23) - (+0.48) (+0.19) -Enju 57.20 57.78 56.59 55.75 56.39 55.12PAS (+0.63) (+1.21) (+0.02) (+0.44) (+1.08) (-0.19)Table 4: Comparison of the F-score results with parser ensembles on the development data set.
C&Cwith Stanford basic Dependency format, MC with CoNLL-X format, Enju with CoNLL-X format, andEnju with Predicate Argument Structure in Enju format are used for the parser ensemble.
The changesfrom single-parser results are shown in parentheses.
The best score in each task is shown in bold.NIA treebank, but in many cases results with theparsers are only slightly worse than results withthe treebank.
The results suggest that there is rela-tive little remaining benefit to be gained for eventextraction from improving parser performance.This supports the claim that most of the errors inevent extraction are not caused by the parse er-rors in (Miwa et al, 2010).
Experiments using theCoNLL-X format produce slightly worse resultsthan for SD with the gold treebank data, which isat variance with the indication from parser-basedresults with MC and Enju.
Thus, the results do notprovide any systematic indication suggesting thatone dependency format would be superior to theother in use for event extraction.4.3 Event Extraction with Parser EnsembleThe four parser outputs were selected for the eval-uation of a parser ensemble: C&C with Stan-ford basic Dependency format, MC with CoNLL-X format, Enju with CoNLL-X format, and Enjuwith Predicate Argument Structure in Enju format.Table 4 summarizes the parser ensemble results.We find that all ensembles of different parsers indifferent formats produce better results than thosefor single parser outputs (Table 3); by contrast, theresults indicate that ensembles of the same formats(MC + Enju in CoNLL-X format) or parsers (Enjuin CoNLL-X and Enju formats) produce relativelysmall improvements, may in some cases even re-duce performance.
The results thus indicate thatwhile a parser ensemble can be effective but that itis important to apply different parsers in differentformats.Table 5 shows detailed results with three parserswith three different formats.
The ensembles sys-tematically improve F-scores in regulation and theoverall performance (?All?
), but the ensemblescan degrade the performance for simple and bind-ing events.
Different parser outputs are shownto have their strengths and weaknesses in differ-ent event groups.
The use of Enju, for exam-42Simple Binding Regulation AllTask 1BL-E 75.85 / 71.09 / 73.39 40.32 / 38.17 / 39.22 30.65 / 48.16 / 37.46 46.12 / 55.60 / 50.42BL-G 76.03 / 73.48 / 74.73 40.32 / 38.17 / 39.22 33.50 / 45.95 / 38.75 47.74 / 54.86 / 51.05C 78.89 / 78.43 / 78.66 48.79 / 43.37 / 45.92 37.17 / 54.07 / 44.06 51.82 / 61.12 / 56.09M 79.79 / 77.12 / 78.43 43.95 / 41.13 / 42.50 39.41 / 52.94 / 45.18 52.66 / 59.82 / 56.01E 79.79 / 76.07 / 77.88 45.16 / 43.75 / 44.44 40.12 / 53.68 / 45.92 53.21 / 60.38 / 56.57C+M 80.50 / 79.05 / 79.77 48.39 / 42.25 / 45.11 41.85 / 53.17 / 46.84 54.84 / 60.31 / 57.44C+E 79.79 / 76.46 / 78.09 47.98 / 45.59 / 46.76 41.04 / 53.66 / 46.51 54.11 / 60.66 / 57.20E+M 80.50 / 77.15 / 78.79 44.35 / 42.97 / 43.65 42.26 / 55.63 / 48.03 54.50 / 61.49 / 57.78C+E+M 80.14 / 77.07 / 78.58 51.61 / 42.95 / 46.89 42.46 / 54.30 / 47.66 55.51 / 60.27 / 57.79Task 2BL-E 74.60 / 69.10 / 71.75 36.55 / 34.73 / 35.62 29.89 / 47.20 / 36.60 44.74 / 53.86 / 48.88BL-G 74.42 / 71.31 / 72.83 36.55 / 33.33 / 34.87 32.52 / 44.83 / 37.70 46.13 / 52.64 / 49.17C 77.64 / 76.77 / 77.20 43.78 / 38.79 / 41.13 36.17 / 52.89 / 42.96 50.14 / 59.14 / 54.27M 78.71 / 75.95 / 77.31 39.36 / 36.57 / 37.91 38.70 / 52.12 / 44.42 51.25 / 58.21 / 54.51E 79.07 / 75.26 / 77.12 41.37 / 40.08 / 40.71 39.31 / 52.86 / 45.09 51.98 / 59.10 / 55.31C+M 79.61 / 78.03 / 78.81 43.37 / 36.99 / 39.93 40.93 / 52.07 / 45.83 53.31 / 58.41 / 55.75C+E 78.89 / 75.34 / 77.08 44.18 / 40.89 / 42.47 40.22 / 52.86 / 45.68 52.81 / 59.04 / 55.75E+M 79.79 / 76.33 / 78.02 40.16 / 38.76 / 39.45 41.34 / 54.69 / 47.09 53.15 / 60.05 / 56.39C+E+M 79.43 / 76.25 / 77.81 46.18 / 37.46 / 41.37 41.54 / 53.39 / 46.72 53.98 / 58.45 / 56.13Table 5: Comparison of Recall / Precision / F-score results on the development data set.
C&C with Stan-ford basic Dependency format (C), MC with CoNLL-X format (M), and Enju with Predicate ArgumentStructure in Enju format (E) are used for the evaluation.
Results with Enju output without PAS informa-tion (BL-E) and the GENIA tagger output (BL-G) are shown as baselines.
Results on simple, binding,regulation, and all events are shown.
The best score in each result is shown in bold.Simple Binding Regulation AllTask 1Ours 67.09 / 77.59 / 71.96 49.57 / 51.65 / 50.59 38.42 / 53.95 / 44.88 50.28 / 63.19 / 56.00Miwa 65.31 / 76.44 / 70.44 52.16 / 53.08 / 52.62 35.93 / 46.66 / 40.60 48.62 / 58.96 / 53.29Bjo?rne 64.21 / 77.45 / 70.21 40.06 / 49.82 / 44.41 35.63 / 45.87 / 40.11 46.73 / 58.48 / 51.95Riedel N/A 23.05 / 48.19 / 31.19 26.32 / 41.81 / 32.30 36.90 / 55.59 / 44.35Task 2Ours 65.77 / 75.29 / 70.21 47.56 / 49.55 / 48.54 38.24 / 53.57 / 44.62 49.48 / 61.87 / 54.99Riedel N/A 22.35 / 46.99 / 30.29 25.75 / 40.75 / 31.56 35.86 / 54.08 / 43.12Table 6: Comparison of Recall / Precision / F-score results on the test data set.
MC with CoNLL-Xformat and Enju with Predicate Argument Structure in Enju format are used for the evaluation.
Resultson simple, binding, regulation, and all events are shown.
Results by Miwa et al (2010) (Miwa), Bjo?rneet al (2009) (Bjo?rne), and Riedel et al (2009) (Riedel) for Task 1 and Task 2 are shown for comparison.The best score in each result is shown in bold.ple, is good for extracting regulation events, butproduced weaker results for simple events.
Theensembles of two parser outputs inherit both thestrengths and weaknesses of the outputs in mostcases, and the strengths and weaknesses of the en-sembles vary depending on the combined parseroutputs.
The differences in performance betweenensembles of the outputs of two parsers to the en-semble of the three parser outputs are +0.01 forTask 1, and -0.26 for Task 2.
This result sug-gests that adding more different parsers does notalways improve the performance.
The ensembleof three parser outputs, however, shows stable per-formance across categories, scoring in the top twofor binding, regulation, and all events, in the topfour for simple events.434.4 Performance of Event Extraction SystemTable 6 shows a comparison of performance onthe shared task test data.
MC with CoNLL-X for-mat and Enju with Predicate Argument Structurein Enju format are used for the evaluation, select-ing one of the best performing ensemble settingsin Section 4.3.
The performance of the best sys-tems in the original shared task is shown for refer-ence ((Bjo?rne et al, 2009) in Task 1 and (Riedelet al, 2009) in Task 2).
The event extractionsystem with our modifications performed signifi-cantly better than the best systems in the sharedtask, further outperforming the original systemby Miwa et al (2010).
This result shows thatthe system applied for the comparison of syntac-tic parsers achieves state-of-the-art performance atevent extraction.
This result also shows that thesystem originally developed only for core eventsextraction can be easily extended for other argu-ments simply by treating the other arguments asadditional arguments.5 Related WorkMany approaches for parser comparison have beenproposed in the BioNLP field.
Most compar-isons have used gold treebanks with intermediateformats (Clegg and Shepherd, 2007; Pyysalo etal., 2007).
Application-oriented parser compari-son across several formats was first introduced byMiyao et al (2009), who compared eight parsersand five formats for the protein-protein interaction(PPI) extraction task.
PPI extraction, the recog-nition of binary relations of between proteins, isone of the most basic information extraction tasksin the BioNLP field.
Our findings do not con-flict with those of Miyao et al Event extractioncan be viewed as an additional extrinsic evalua-tion task for syntactic parsers, providing more reli-able and evaluation and a broader perspective intoparser performance.
An additional advantage ofapplication-oriented evaluation on BioNLP sharedtask data is the availability of a manually anno-tated gold standard treebank, the GENIA treebank,that covers the same set of abstracts as the taskdata.
This allows the gold treebank to be consid-ered as an evaluation standard, in addition to com-parison of performance in the primary task.6 ConclusionWe compared five parsers and three formats on abio-molecular event extraction task with a state-of-the-art event extraction system.
The specifictask considered was the BioNLP shared task, al-lowing the use of the GENIA treebank as a goldstandard parse reference.
The event extraction sys-tem, modified for a higher performance and an ad-ditional subtask, showed high performance on theshared task subtasks considered.
Four of the fiveconsidered parsers were applied using biomedi-cal models trained on the GENIA treebank, andthey were found to produce similar performance.Parser ensembles were further shown to allow im-provement of the performance of the event extrac-tion system.The contributions of this paper are 1) the com-parison of several commonly used parsers on theevent extraction task with a gold treebank, 2)demonstration of the usefulness of the parser en-semble on the task, and 3) the introduction of astate-of-the-art event extraction system.
One lim-itation of this study is that the comparison be-tween the parsers is not perfect, as the format con-versions miss some information from the origi-nal formats and results with different formats de-pend on the ability of the event extraction sys-tem to take advantage of their strengths.
To max-imize comparability, the system was designed toextract features identically from similar parts ofthe dependency-based formats, further adding in-formation provided by other formats, such as thelexical entries of the Enju format, from external re-sources.
The results of this paper are expected tobe useful as a guide not only for parser selectionfor biomedical information extraction but also forthe development of event extraction systems.The selection of compared parsers and formatsin the present evaluation is somewhat limited.
Asfuture work, it would be informative to extendthe comparison to other syntactic representations,such as the PTB format.
Finally, the evaluationshowed that the system fails to recover approxi-mately 40% of events even when provided withmanually annotated treebank data, showing thatother methods and resources need to be adoptedto further improve bio-molecular event extractionsystems.
Such improvement is left as future work.AcknowledgmentsThis work was partially supported by Grant-in-Aidfor Specially Promoted Research (MEXT, Japan),Genome Network Project (MEXT, Japan), andScientific Research (C) (General) (MEXT, Japan).44ReferencesDaniel M. Bikel.
2004.
A distributional analysis of alexicalized statistical parsing model.
In In EMNLP,pages 182?189.Jari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola,Tapio Pahikkala, and Tapio Salakoski.
2009.
Ex-tracting complex biological events with rich graph-based feature sets.
In Proceedings of the BioNLP?09Shared Task on Event Extraction, pages 10?18.Andrew B. Clegg and Adrian J. Shepherd.
2007.Benchmarking natural-language parsers for biolog-ical applications using dependency graphs.
BMCBioinformatics, 8.Marie-Catherine de Marneffe and Christopher D. Man-ning.
2008.
Stanford typed dependencies manual.Technical report, September.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InProceedings of the IEEE / ACL 2006 Workshop onSpoken Language Technology.Richard Johansson and Pierre Nugues.
2007.
Ex-tended constituent-to-dependency conversion forEnglish.
In Proceedings of NODALIDA 2007, Tartu,Estonia, May 25-26.Halil Kilicoglu and Sabine Bergler.
2009.
Syntacticdependency based heuristics for biological event ex-traction.
In Proceedings of the BioNLP 2009 Work-shop Companion Volume for Shared Task, pages119?127, Boulder, Colorado, June.
Association forComputational Linguistics.Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii.2008.
Corpus annotation for mining biomedicalevents from literature.
BMC Bioinformatics, 9:10.Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-nobu Kano, and Jun?ichi Tsujii.
2009.
Overviewof bionlp?09 shared task on event extraction.
InBioNLP ?09: Proceedings of the Workshop onBioNLP, pages 1?9.David McClosky.
2009.
Any Domain Parsing: Au-tomatic Domain Adaptation for Natural LanguageParsing.
Ph.D. thesis, Department of Computer Sci-ence, Brown University.Makoto Miwa, Rune S?tre, Jin-Dong Kim, andJun?ichi Tsujii.
2010.
Event extraction with com-plex event classification using rich features.
Jour-nal of Bioinformatics and Computational Biology(JBCB), 8(1):131?146, February.Yusuke Miyao, Kenji Sagae, Rune S?tre, TakuyaMatsuzaki, and Jun ichi Tsujii.
2009.
Evalu-ating contributions of natural language parsers toprotein-protein interaction extraction.
Bioinformat-ics, 25(3):394?400.Sampo Pyysalo, Filip Ginter, Veronika Laippala, Ka-tri Haverinen, Juho Heimonen, and Tapio Salakoski.2007.
On the unification of syntactic annotationsunder the stanford dependency scheme: A casestudy on bioinfer and genia.
In Biological, transla-tional, and clinical language processing, pages 25?32, Prague, Czech Republic, June.
Association forComputational Linguistics.Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi,and Jun?ichi Tsujii.
2009.
A markov logic approachto bio-molecular event extraction.
In BioNLP ?09:Proceedings of the Workshop on BioNLP, pages 41?49, Morristown, NJ, USA.
Association for Compu-tational Linguistics.Laura Rimell and Stephen Clark.
2009.
Porting alexicalized-grammar parser to the biomedical do-main.
J. of Biomedical Informatics, 42(5):852?865.Kenji Sagae and Jun?ichi Tsujii.
2007.
Dependencyparsing and domain adaptation with LR models andparser ensembles.
In EMNLP-CoNLL 2007.Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and Jun-fichi Tsujii.
2005.
Syntax annotation for the geniacorpus.
In Proceedings of the IJCNLP 2005, Com-panion volume, pages 222?227, Jeju Island, Korea,October.Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim,Tomoko Ohta, John McNaught, Sophia Ananiadou,and Jun?ichi Tsujii.
2005.
Developing a robust part-of-speech tagger for biomedical text.
In Panayio-tis Bozanis and Elias N. Houstis, editors, Panhel-lenic Conference on Informatics, volume 3746 ofLecture Notes in Computer Science, pages 382?392.Springer.45
