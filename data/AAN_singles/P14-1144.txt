Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1534?1543,Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational LinguisticsModeling Prompt Adherence in Student EssaysIsaac Persing and Vincent NgHuman Language Technology Research InstituteUniversity of Texas at DallasRichardson, TX 75083-0688{persingq,vince}@hlt.utdallas.eduAbstractRecently, researchers have begun explor-ing methods of scoring student essays withrespect to particular dimensions of qual-ity such as coherence, technical errors,and prompt adherence.
The work onmodeling prompt adherence, however, hasbeen focused mainly on whether individ-ual sentences adhere to the prompt.
Wepresent a new annotated corpus of essay-level prompt adherence scores and pro-pose a feature-rich approach to scoring es-says along the prompt adherence dimen-sion.
Our approach significantly outper-forms a knowledge-lean baseline promptadherence scoring system yielding im-provements of up to 16.6%.1 IntroductionAutomated essay scoring, the task of employingcomputer technology to evaluate and score writ-ten text, is one of the most important educationalapplications of natural language processing (NLP)(see Shermis and Burstein (2003) and Shermis etal.
(2010) for an overview of the state of the artin this task).
A major weakness of many ex-isting scoring engines such as the Intelligent Es-say AssessorTM(Landauer et al, 2003) is that theyadopt a holistic scoring scheme, which summa-rizes the quality of an essay with a single score andthus provides very limited feedback to the writer.In particular, it is not clear which dimension ofan essay (e.g., style, coherence, relevance) a scoreshould be attributed to.
Recent work addresses thisproblem by scoring a particular dimension of es-say quality such as coherence (Miltsakaki and Ku-kich, 2004), technical errors, organization (Pers-ing et al, 2010), and thesis clarity (Persing andNg, 2013).
Essay grading software that providesfeedback along multiple dimensions of essay qual-ity such as E-rater/Criterion (Attali and Burstein,2006) has also begun to emerge.Our goal in this paper is to develop a com-putational model for scoring an essay along anunder-investigated dimension ?
prompt adher-ence.
Prompt adherence refers to how related anessay?s content is to the prompt for which it waswritten.
An essay with a high prompt adherencescore consistently remains on the topic introducedby the prompt and is free of irrelevant digressions.To our knowledge, little work has been doneon scoring the prompt adherence of student essayssince Higgins et al (2004).
Nevertheless, there aremajor differences between Higgins et al?s workand our work with respect to both the way the taskis formulated and the approach.
Regarding taskformulation, while Higgins et al focus on classi-fying each sentence as having either good or badadherence to the prompt, we focus on assigninga prompt adherence score to the entire essay, al-lowing the score to range from one to four pointsat half-point increments.
As far as the approachis concerned, Higgins et al adopt a knowledge-lean approach to the task, where almost all ofthe features they employ are computed based ona word-based semantic similarity measure knownas Random Indexing (Kanerva et al, 2000).
Onthe other hand, we employ a large variety of fea-tures, including lexical and knowledge-based fea-tures that encode how well the concepts in an es-say match those in the prompt, LDA-based fea-tures that provide semantic generalizations of lex-ical features, and ?error type?
features that encodedifferent types of errors the writer made that arerelated to prompt adherence.In sum, our contributions in this paper are two-fold.
First, we develop a scoring model for theprompt adherence dimension on student essays us-ing a feature-rich approach.
Second, in order tostimulate further research on this task, we makeour data set consisting of prompt adherence an-1534Topic Languages EssaysMost university degrees are the-oretical and do not prepare stu-dents for the real world.
They aretherefore of very little value.13 131The prison system is outdated.No civilized society should pun-ish its criminals: it should reha-bilitate them.11 80In his novel Animal Farm,George Orwell wrote ?All menare equal but some are moreequal than others.?
How true isthis today?10 64Table 1: Some examples of writing topics.notations of 830 essays publicly available.
Sinceprogress in prompt adherence modeling is hin-dered in part by the lack of a publicly annotatedcorpus, we believe that our data set will be a valu-able resource to the NLP community.2 Corpus InformationWe use as our corpus the 4.5 million word Interna-tional Corpus of Learner English (ICLE) (Grangeret al, 2009), which consists of more than 6000 es-says written by university undergraduates from 16countries and 16 native languages who are learn-ers of English as a Foreign Language.
91% of theICLE texts are argumentative.
We select a subsetconsisting of 830 argumentative essays from theICLE to annotate for training and testing of ouressay prompt adherence scoring system.
Table 1shows three of the 13 topics selected for annota-tion.
Fifteen native languages are represented inthe set of annotated essays.3 Corpus AnnotationWe ask human annotators to score each of the 830argumentative essays along the prompt adherencedimension.
Our annotators were selected fromover 30 applicants who were familiarized with thescoring rubric and given sample essays to score.The six who were most consistent with the ex-pected scores were given additional essays to an-notate.
Annotators evaluated how well each es-say adheres to its prompt using a numerical scorefrom one to four at half-point increments (see Ta-ble 2 for a description of each score).
This con-trasts with previous work on prompt adherence es-say scoring, where the corpus is annotated with abinary decision (i.e., good or bad) (e.g., Higginset al (2004; 2006), Louis and Higgins (2010)).Hence, our annotation scheme not only providesScore Description of Prompt Adherence4 essay fully addresses the prompt and consis-tently stays on topic3 essay mostly addresses the prompt or occasion-ally wanders off topic2 essay does not fully address the prompt or con-sistently wanders off topic1 essay does not address the prompt at all or iscompletely off topicTable 2: Descriptions of the meaning of scores.a finer-grained distinction of prompt adherence(which can be important in practice), but alsomakes the prediction task more challenging.To ensure consistency in annotation, we ran-domly select 707 essays to have graded by mul-tiple annotators.
Analysis reveals that the Pear-son?s correlation coefficient computed over thesedoubly annotated essays is 0.243.
Though annota-tors exactly agree on the prompt adherence scoreof an essay only 38% of the time, the scores theyapply fall within 0.5 points in 66% of essays andwithin 1.0 point in 89% of essays.
For the sakeof our experiments, whenever annotators disagreeon an essay?s prompt adherence score, we assignthe essay the average of all annotations rounded tothe nearest half point.
Table 3 shows the numberof essays that receive each of the seven scores forprompt adherence.score 1.0 1.5 2.0 2.5 3.0 3.5 4.0essays 0 0 8 44 105 230 443Table 3: Distribution of prompt adherence scores.4 Score PredictionIn this section, we describe in detail our system forpredicting essays?
prompt adherence scores.4.1 Model Training and ApplicationWe cast the problem of predicting an essay?sprompt adherence score as 13 regression prob-lems, one for each prompt.
Each essay is repre-sented as an instance whose label is the essay?strue score (one of the values shown in Table 3)with up to seven types of features including base-line (Section 4.2) and six other feature types pro-posed by us (Section 4.3).
Our regressors may as-sign an essay any score in the range of 1.0?4.0.Using regression captures the fact that somepairs of scores are more similar than others (e.g.,an essay with a prompt adherence score of 3.5 ismore similar to an essay with a score of 4.0 than itis to one with a score of 1.0).
A classification sys-1535tem, by contrast, may sometimes believe that thescores 1.0 and 4.0 are most likely for a particu-lar essay, even though these scores are at oppositeends of the score range.Using a different regressor for each prompt cap-tures the fact that it may be easier for an essay toadhere to some prompts than to others, and com-mon problems students have writing essays forone prompt may not apply to essays written in re-sponse to another prompt.
For example, in essayswritten in response to the prompt ?Marx once saidthat religion was the opium of the masses.
If hewas alive at the end of the 20th century, he wouldreplace religion with television,?
students some-times write essays about all the evils of television,forgetting that their essay is only supposed to beabout whether it is ?the opium of the masses?.
Stu-dents are less likely to make an analogous mistakewhen writing for the prompt ?Crime does not pay.
?After creating training instances for prompt pi,we train a linear regressor, ri, with regularizationparameter cifor scoring test essays written in re-sponse to piusing the linear SVM regressor imple-mented in the LIBSVM software package (Changand Lin, 2001).
All SVM-specific learning param-eters are set to their default values except ci, whichwe tune to maximize performance on held-out val-idation data.After training the classifiers, we use them toclassify the test set essays.
The test instances arecreated in the same way as the training instances.4.2 Baseline FeaturesOur baseline system for score prediction employsvarious features based on Random Indexing.1.
Random Indexing Random Indexing (RI) is?an efficient, scalable and incremental alterna-tive?
(Sahlgren, 2005) to Latent Semantic Index-ing (Deerwester et al, 1990; Landauer and Dut-nais, 1997) which allows us to automatically gen-erate a semantic similarity measure between anytwo words.
We train our RI model on over 30 mil-lion words of the English Gigaword corpus (Parkeret al, 2009) using the S-Space package (Jurgensand Stevens, 2010).
We expect that features basedon RI will be useful for prompt adherence scor-ing because they may help us find text relatedto the prompt even if some of its concepts havehave been rephrased (e.g., an essay may talk about?jail?
rather than ?prison?, which is mentioned inone of the prompts), and because they have al-ready proven useful for the related task of deter-mining which sentences in an essay are related tothe prompt (Higgins et al, 2004).For each essay, we therefore attempt to adaptthe RI features used by Higgins et al (2004) toour problem of prompt adherence scoring.
We dothis by generating one feature encoding the entireessay?s similarity to the prompt, another encodingthe essay?s highest individual sentence?s similarityto the prompt, a third encoding the highest entireessay similarity to one of the prompt sentences,another encoding the highest individual sentencesimilarity to an individual prompt sentence, and fi-nally one encoding the entire essay?s similarity toa manually rewritten version of the prompt that ex-cludes extraneous material (such as ?In his novelAnimal Farm, George Orwell wrote,?
which is in-troductory material from the third prompt in Ta-ble 1).
Our RI feature set necessarily excludesthose features from Higgins et al that are noteasily translatable to our problem since we areconcerned with an entire essay?s adherence to itsprompt rather than with each of its sentences?
re-latedness to the prompt.
Since RI does not pro-vide a straightforward way to measure similar-ity between groups of words such as sentencesor essays, we use Higgins and Burstein?s (2007)method to generate these features.4.3 Novel FeaturesNext, we introduce six types of novel features.2.
N-grams As our first novel feature, we usethe 10,000 most important lemmatized unigram,bigram, and trigram features that occur in the es-say.
N-grams can be useful for prompt adherencescoring because they can capture useful words andphrases related to a prompt.
For example, wordsand phrases like ?university degree?, ?student?,and ?real world?
are relevant to the first prompt inTable 1, so it is more likely that an essay adheresto the prompt if they appear in the essay.We determine the ?most important?
n-gram fea-tures using information gain computed over thetraining data (Yang and Pedersen, 1997).
Since theessays vary greatly in length, we normalize eachessay?s set of n-gram features to unit length.3.
Thesis Clarity Keywords Our next set of fea-tures consists of the keyword features we intro-duced in our previous work on essay thesis clarityscoring (Persing and Ng, 2013).
Below we give anoverview of these keyword features and motivate1536why they are potentially useful for prompt adher-ence scoring.The keyword features were formed by first ex-amining the 13 essay prompts, splitting each intoits component pieces.
As an example of what ismeant by a ?component piece?, consider the firstprompt in Table 1.
The components of this promptwould be ?Most university degrees are theoreti-cal?, ?Most university degrees do not prepare stu-dents for the real world?, and ?Most university de-grees are of very little value.
?Then the most important (primary) and secondmost important (secondary) words were selectedfrom each prompt component, where a word wasconsidered ?important?
if it would be a good wordfor a student to use when stating her thesis aboutthe prompt.
So since the lemmatized version of thethird component of the second prompt in Table 1is ?it should rehabilitate they?, ?rehabilitate?
wasselected as a primary keyword and ?society?
as asecondary keyword.Features are then computed based on these key-words.
For instance, one thesis clarity keywordfeature is computed as follows.
The RI similaritymeasure is first taken between the essay and eachgroup of the prompt?s primary keywords.
The fea-ture then gets assigned the lowest of these values.If this feature has a low value, that suggests thatthe student ignored the prompt component fromwhich the value came when writing the essay.To compute another of the thesis clarity key-word features, the numbers of combined primaryand secondary keywords the essay contains fromeach component of its prompt are counted.
Thesenumbers are then divided by the total count of pri-mary and secondary features in their respectivecomponents.
The greatest of the fractions gener-ated in this way is encoded as a feature because ifit has a low value, that indicates the essay?s thesismay not be very relevant to the prompt.14.
Prompt Adherence Keywords The thesisclarity keyword features described above were in-tended for the task of determining how clear anessay?s thesis is, but since our goal is instead to de-termine how well an essay adheres to its prompt,it makes sense to adapt keyword features to ourtask rather than to adopt keyword features ex-1Space limitations preclude a complete listing of the the-sis clarity keyword features.
See our website at http://www.hlt.utdallas.edu/?persingq/ICLE/ forthe complete list.actly as they have been used before.
For thisreason, we construct a new list of keywords foreach prompt component, though since prompt ad-herence is more concerned with what the studentsays about the topics than it is with whether ornot what she says about them is stated clearly,our keyword lists look a little different than theones discussed above.
For an example, we ear-lier alluded to the problem of students merely dis-cussing all the evils of television for the prompt?Marx once said that religion was the opium of themasses.
If he was alive at the end of the 20th cen-tury, he would replace religion with television.
?Since the question suggests that students discusswhether television is analogous to religion in thisway, our set of prompt adherence keywords forthis prompt contains the word ?religion?
while thepreviously discussed keyword sets do not.
Thisis because a thesis like ?Television is bad?
can bestated very clearly without making any referenceto religion at all, and so an essay with a thesis likethis can potentially have a very high thesis clarityscore.
It should not, however, have a very highprompt adherence score, as the prompt asked thestudent to discuss whether television is like reli-gion in a particular way, so religion should be atleast briefly addressed for an essay to be awardeda high prompt adherence score.Additionally, our prompt adherence keywordsets do not adopt the notions of primary and sec-ondary groups of keywords for each prompt com-ponent, instead collecting all the keywords for acomponent into one set because ?secondary?
key-words tend to be things that are important when weare concerned with what a student is saying aboutthe topic rather than just how clearly she said it.We form two types of features from prompt ad-herence keywords.
While both types of featuresmeasure how much each prompt component wasdiscussed in an essay, they differ in how they en-code the information.
To obtain feature values ofthe first type, we take the RI similarities betweenthe whole essay and each set of prompt adherencekeywords from the prompt?s components.
Thisresults in one to three features, as some promptshave one component while others have up to three.We obtain feature values of the second type asfollows.
For each component, we count the num-ber of prompt adherence keywords the essay con-tains.
We divide this number by the number ofprompt adherence keywords we identified from1537the component.
This results in one to three fea-tures since a prompt has one to three components.5.
LDA Topics A problem with the features wehave introduced up to this point is that they havetrouble identifying topics that are not mentionedin the prompt, but are nevertheless related to theprompt.
These topics should not diminish the es-say?s prompt adherence score because they are atleast related to prompt concepts.
For example,consider the prompt ?All armies should consist en-tirely of professional soldiers: there is no value ina system of military service.?
An essay contain-ing words like ?peace?, ?patriotism?, or ?training?are probably not digressions from the prompt, andtherefore should not be penalized for discussingthese topics.
But the various measures of keywordsimilarities described above will at best not noticethat anything related to the prompt is being dis-cussed, and at worst, this might have effects likelowering some of the RI similarity scores, therebyprobably lowering the prompt adherence score theregressor assigns to the essay.
While n-gram fea-tures do not have exactly the same problem, theywould still only notice that these example wordsare related to the prompt if multiple essays use thesame words to discuss these concepts.
For thisreason, we introduce Latent Dirichlet Allocation(LDA) (Blei et al, 2003) features.In order to construct our LDA features, wefirst collect all essays written in response to eachprompt into its own set.
Note that this feature typeexploits unlabeled data: it includes all essays inthe ICLE responding to our prompts, not just thosein our smaller annotated 830 essay dataset.
Wethen use the MALLET (McCallum, 2002) imple-mentation of LDA to build a topic model of 1,000topics around each of these sets of essays.
Thisresults in what we can think of as a soft clusteringof words into 1,000 sets for each prompt, whereeach set of words represents one of the topics LDAidentified being discussed in the essays for thatprompt.
So for example, the five most impor-tant words in the most frequently discussed topicfor the military prompt we mentioned above are?man?, ?military?, ?service?, ?pay?, and ?war?.We also use the MALLET-generated topicmodel to tell us how much of each essay is spentdiscussing each of the 1,000 topics.
The modelmight tell us, for example, that a particular essaywritten on the military prompt spends 35% of thetime discussing the ?man?, ?military?, ?service?,?pay?, and ?war?
topic and 65% of the time dis-cussing a topic whose most important words are?fully?, ?count?, ?ordinary?, ?czech?, and ?day?.Since the latter topic is discussed so much in theessay and does not appear to have much to do withthe military prompt, this essay should probablyget a bad prompt adherence score.
We construct1,000 features from this topic model, one for eachtopic.
Each feature?s value is obtained by usingthe topic model to tell us how much of the essaywas spent discussing the feature?s correspondingtopic.
From these features, our regressor shouldbe able to learn which topics are important to agood prompt adherent essay.6.
Manually Annotated LDA Topics A weak-ness of the LDA topics feature type is that it mayresult in a regressor that has trouble distinguishingbetween an infrequent topic that is adherent to theprompt and one that just represents an irrelevantdigression.
This is because an infrequent topicmay not appear in the training set often enough forthe regressor to make this judgment.
We introducethe manually annotated LDA topics feature type toaddress this problem.In order to construct manually annotated LDAtopic features, we first build 13 topic models, onefor each prompt, just as described in the sectionon LDA topic features.
Rather than requestingmodels of 1,000 topics, however, we request mod-els of only 100 topics2.
We then go through all13 lists of 100 topics as represented by their topten words, manually annotating each topic with anumber from 0 to 5 representing how likely it isthat the topic is adherent to the prompt.
A topiclabeled 5 is very likely to be related to the prompt,where a topic labeled 0 appears totally unrelated.Using these annotations alongside the topic dis-tribution for each essay that the topic models pro-vide us, we construct ten features.
The first fivefeatures encode the sum of the contributions to anessay of topics annotated with a number ?
1, thesum of the contributions to an essay of topics an-notated with a number ?
2, and so on up to 5.The next five features are similar to the last,with one feature taking on the sum of the contri-butions to an essay of topics annotated with thenumber 0, another feature taking on the sum of the2We use 100 topics for each prompt in the manually an-notated version of LDA features rather than the 1,000 topicswe use in the regular version of LDA features because 1,300topics are not too costly to annotate, but manually annotating13,000 topics would take too much time.1538contributions to an essay of topics annotated withthe number 1, and so on up to 4.
We do not includea feature for topics annotated with the number 5because it would always have the same value asthe feature for topics ?
5.Features like these should give the regressor abetter idea how much of an essay is composed ofprompt-related arguments and discussion and howmuch of it is irrelevant to the prompt, even if someof the topics occurring in it are too infrequent tojudge just from training data.7.
Predicted Thesis Clarity Errors In our pre-vious work on essay thesis clarity scoring (Persingand Ng, 2013), we identified five classes of errorsthat detract from the clarity of an essay?s thesis:Confusing Phrasing.
The thesis is phrased oddly,making it hard to understand the writer?s point.Incomplete Prompt Response.
The thesis leavessome part of a multi-part prompt unaddressed.Relevance to Prompt.
The apparent thesis?s weakrelation to the prompt causes confusion.Missing Details.
The thesis leaves out an impor-tant detail needed to understand the writer?s point.Writer Position.
The thesis describes a positionon the topic without making it clear that this is theposition the writer supports.We hypothesize that these errors, though orig-inally intended for thesis clarity scoring, couldbe useful for prompt adherence scoring as well.For instance, an essay that has a Relevance toPrompt error or an Incomplete Prompt Responseerror should intuitively receive a low prompt ad-herence score.
For this reason, we introduce fea-tures based on these errors to our feature set forprompt adherence scoring3.While each of the essays in our data set was pre-viously annotated with these thesis clarity errors,in a realistic setting a prompt adherence scoringsystem will not have access to these manual errorlabels.
As a result, we first need to predict whichof these errors is present in each essay.
To do this,we train five maximum entropy classifiers for eachprompt, one for each of the five thesis clarity er-rors, using MALLET?s (McCallum, 2002) imple-mentation of maximum entropy classification.
In-stances are presented to classifier for prompt p forerror e in the following way.
If a training essayis written in response to p, it will be used to gen-3See our website at http://www.hlt.utdallas.edu/?persingq/ICLE/ for the complete list of error an-notations.erate a training instance whose label is 1 if e wasannotated for it or 0 otherwise.
Since error pre-diction and prompt adherence scoring are relatedproblems, the features we associate with this in-stance are features 1?6 which we have describedearlier in this section.
The classifier is then usedto generate probabilities telling us how likely it isthat each test essay has error e.Then, when training our regressor for promptadherence scoring, we add the following featuresto our instances.
We add a binary feature indicat-ing the presence or absence of each error.
Or inthe case of test essays, the feature takes on a realvalue from 0 to 1 indicating how likely the classi-fier thought it was that the essay had each of theerrors.
This results in five additional features, onefor each error.5 EvaluationIn this section, we evaluate our system for promptadherence scoring.
All the results we reportare obtained via five-fold cross-validation exper-iments.
In each experiment, we use 35of our la-beled essays for model training, another 15for pa-rameter tuning, and the final 15for testing.5.1 Experimental Setup5.1.1 Scoring MetricsWe employ four evaluation metrics.
As we will seebelow, S1, S2, and S3 are error metrics, so lowerscores imply better performance.
In contrast, PCis a correlation metric, so higher correlation im-plies better performance.The simplest metric, S1, measures the fre-quency at which a system predicts the wrong scoreout of the seven possible scores.
Hence, a systemthat predicts the right score only 25% of the timewould receive an S1 score of 0.75.The S2 metric measures the average distancebetween a system?s score and the actual score.This metric reflects the idea that a system that pre-dicts scores close to the annotator-assigned scoresshould be preferred over a system whose predic-tions are further off, even if both systems estimatethe correct score at the same frequency.The S3 metric measures the average squareof the distance between a system?s score predic-tions and the annotator-assigned scores.
The in-tuition behind this system is that not only shouldwe prefer a system whose predictions are closeto the annotator scores, but we should also prefer1539one whose predictions are not too frequently veryfar away from the annotator scores.
These threescores are given by:1N?Aj6=E?j1,1NN?i=1|Aj?
Ej|,1NN?i=1(Aj?
Ej)2where Aj, Ej, and E?jare the annotator assigned,system predicted, and rounded system predictedscores4 respectively for essay j, and N is the num-ber of essays.The last metric, PC , computes Pearson?s cor-relation coefficient between a system?s predictedscores and the annotator-assigned scores.
PCranges from ?1 to 1.
A positive (negative) PCimplies that the two sets of predictions are posi-tively (negatively) correlated.5.1.2 Parameter TuningAs mentioned earlier, for each prompt pi, we traina linear regressor riusing LIBSVM with regular-ization parameter ci.
To optimize our system?sperformance on the three error measures describedpreviously, we use held-out validation data to in-dependently tune each of the civalues5.
Note thateach of the civalues can be tuned independentlybecause a civalue that is optimal for predictingscores for piessays with respect to any of the errorperformance measures is necessarily also the opti-mal ciwhen measuring that error on essays fromall prompts.
However, this is not case with Pear-son?s correlation coefficient, as the PC value foressays from all 13 prompts cannot be simplified asa weighted sum of the PC values obtained on eachindividual prompt.
In order to obtain an optimalresult as measured by PC , we jointly tune the ciparameters to optimize the PC value achieved byour system on the same held-out validation data.However, an exact solution to this optimizationproblem is computationally expensive, as there aretoo many (713) possible combinations of c valuesto exhaustively search.
Consequently, we find alocal maximum by employing the simulated an-4Since our regressor assigns each essay a real value ratherthan an actual valid score, it would be difficult to obtain areasonable S1 score without rounding the system estimatedscore to one of the possible values.
For that reason, we roundthe estimated score to the nearest of the seven scores the hu-man annotators were permitted to assign (1.0, 1.5, 2.0, 2.5,3.0, 3.5, 4.0) only when calculating S1.
For other scoringmetrics, we only round the predictions to 1.0 or 4.0 if theyfall outside the 1.0?4.0 range.5For parameter tuning, we employ the following values.cimay be assigned any of the values 100 101, 102, 103, 104,105, or 106.System S1 S2 S3 PCBaseline .517 .368 .234 .233Our System .488 .348 .197 .360Table 4: Five-fold cross-validation results forprompt adherence scoring.nealing algorithm (Kirkpatrick et al, 1983), alter-ing one civalue at a time to optimize PC whileholding the remaining parameters fixed.5.2 Results and DiscussionFive-fold cross-validation results on prompt ad-herence score prediction are shown in Table 4.
Onthe first line, this table shows that our baseline sys-tem, which recall uses only various RI features,predicts the wrong score 51.7% of the time.
Itspredictions are off by an average of .368 points,and the average squared distance between its pre-dicted score and the actual score is .234.
In addi-tion, its predicted scores and the actual scores havea Pearson correlation coefficient of 0.233.The results from our system, which uses allseven feature types described in Section 4, areshown in row 2 of the table.
Our system obtainsS1, S2, S3, and PC scores of .488, .348, .197,and .360 respectively, yielding a significant im-provement over the baseline with respect to S2,S3, and PC with p < 0.05, p < 0.01, and p < 0.06respectively6 .
While our system yields improve-ments by all four measures, its improvement overthe baseline S1 score is not significant.
These re-sults mean that the greatest improvements our sys-tem makes are that it ensures that our score pre-dictions are not too often very far away from anessay?s actual score, as making such predictionswould tend to drive up S3, yielding a relative er-ror reduction in S3 of 15.8%, and it also ensuresa better correlation between predicted and actualscores, thus yielding the 16.6% improvement inPC .7 It also gives more modest improvements inhow frequently exactly the right score is predicted(S1) and is better at predicting scores closer to theactual scores (S2).5.3 Feature AblationTo gain insight into how much impact each of thefeature types has on our system, we perform fea-6All significance tests are paired t-tests.7These numbers are calculated B?OB?Pwhere B is the base-line system?s score, O is our system?s score, and P is a per-fect score.
Perfect scores for error measures and PC are 0and 1 respectively.1540ture ablation experiments in which we remove thefeature types from our system one-by-one.Results of the ablation experiments when per-formed using the four scoring metrics are shown inTable 5.
The top line of each subtable shows whatour system?s score would be if we removed justone of the feature types from our system.
So to seehow our system performs by the S1 metric if weremove only predicted thesis clarity error features,we would look at the first row of results of Ta-ble 5(a) under the column headed by the number 7since predicted thesis clarity errors are the seventhfeature type introduced in Section 4.
The numberhere tells us that our system?s S1 score withoutthis feature type is .502.
Since Table 4 shows thatwhen our system includes this feature type (alongwith all the other feature types), it obtains an S1score of .488, this feature type?s removal costs oursystem .014 S1 points, and thus its inclusion has abeneficial effect on the S1 score.From row 1 of Table 5(a), we can see that re-moving feature 4 yields a system with the best S1score in the presence of the other feature types inthis row.
For this reason, we permanently removefeature 4 from the system before we generate theresults on line 2.
Thus, we can see what happenswhen we remove both feature 4 and feature 5 bylooking at the second entry in row 2.
And sinceremoving feature 6 harms performance least in thepresence of row 2?s other feature types, we perma-nently remove both 4 and 6 from our feature setwhen we generate the third row of results.
We it-eratively remove the feature type that yields a sys-tem with the best performance in this way until weget to the last line, where only one feature type isused to generate each result.Since the feature type whose removal yields thebest system is always the rightmost entry in a line,the order of column headings indicates the rela-tive importance of the feature types, with the left-most feature types being most important to per-formance and the rightmost feature types beingleast important in the presence of the other fea-ture types.
This being the case, it is interesting tonote that while the relative importance of differ-ent feature types does not remain exactly the sameif we measure performance in different ways, wecan see that some feature types tend to be more im-portant than others in a majority of the four scor-ing metrics.
Features 2 (n-grams), 3 (thesis claritykeywords), and 6 (manually annotated LDA top-(a) Results using the S1 metric3 5 1 7 2 6 4.527 .502 .512 .502 .511 .500 .488.527 .502 .512 .501 .513 .500.525 .508 .505 .505 .504.513 .527 .520 .513.523 .520 .506.541 .527(b) Results using the S2 metric2 6 3 1 4 5 7.356 .350 .348 .350 .349 .348 .348.351 .349 .348 .348 .348 .347.351 .349 .348 .348 .347.350 .349 .348 .348.358 .351 .349.362 .352(c) Results using the S3 metric2 6 1 5 4 7 3.221 .201 .197 .197 .197 .197 .196.215 .201 .197 .196 .196 .196.212 .203 .199 .197 .196.212 .203 .199 .197.212 .203 .199.223 .204(d) Results using the PC metric6 3 2 1 7 5 4.326 .332 .303 .344 .348 .348 .361.326 .332 .304 .343 .348 .348.324 .337 .292 .345 .352.322 .337 .297 .346.316 .321 .323.218 .325Table 5: Feature ablation results.
In each subtable,the first row shows how our system would perform if eachfeature type was removed.
We remove the least importantfeature type, and show in the next row how the adjusted sys-tem would perform without each remaining type.
For brevity,a feature type is referred to by its feature number: (1) RI; (2)n-grams; (3) thesis clarity keywords; (4) prompt adherencekeywords; (5) LDA topics; (6) manually annotated LDA top-ics; and (7) predicted thesis clarity errors.ics) tend to be the most important feature types,as they tend to be the last feature types removedin the ablation subtables.
Features 1 (RI) and 5(LDA topics) are of middling importance, withneither ever being removed first or last, and eachtending to have a moderate effect on performance.Finally, while features 4 (prompt adherence key-words) and 7 (predicted thesis clarity errors) mayby themselves provide useful information to oursystem, in the presence of the other feature typesthey tend to be the least important to performanceas they are often the first feature types removed.While there is a tendency for some feature typesto always be important (or unimportant) regardlessof which scoring metric is used to measure per-1541S1 S2 S3 PCGold .25 .50 .75 .25 .50 .75 .25 .50 .75 .25 .50 .752.0 3.35 3.56 3.79 3.40 3.52 3.73 3.06 3.37 3.64 3.06 3.37 3.642.5 3.43 3.63 3.80 3.25 3.52 3.79 3.24 3.45 3.67 3.24 3.46 3.733.0 3.64 3.78 3.85 3.56 3.70 3.90 3.52 3.65 3.74 3.52 3.66 3.793.5 3.73 3.81 3.88 3.63 3.78 3.90 3.59 3.70 3.81 3.60 3.74 3.854.0 3.76 3.84 3.88 3.70 3.83 3.90 3.63 3.75 3.84 3.66 3.78 3.88Table 6: Regressor scores for our system.formance, the relative importance of different fea-ture types does not always remain consistent if wemeasure performance in different ways.
For ex-ample, while we identified feature 3 (thesis clar-ity keywords) as one of the most important fea-ture types generally due to its tendency to have alarge beneficial impact on performance, when weare measuring performance using S3, it is the leastuseful feature type.
Furthermore, its removal in-creases the S3 score by a small amount, meaningthat its inclusion actually makes our system per-form worse with respect to S3.
Though feature 3 isan extreme example, all feature types fluctuate inimportance, as we see when we compare their or-ders of removal among the four ablation subtables.Hence, it is important to know how performanceis measured when building a system for scoringprompt adherence.Feature 3 is not the only feature type whose re-moval sometimes has a beneficial impact on per-formance.
As we can see in Table 5(b), the re-moval of features 4, 5, and 7 improves our sys-tem?s S2 score by .001 points.
The same effectoccurs in Table 5(c) when we remove features 4,7, and 3.
These examples illustrate that undersome scoring metrics, the inclusion of some fea-ture types is actively harmful to performance.
For-tunately, this effect does not occur in any othercases than the two listed above, as most featuretypes usually have a beneficial or at least neutralimpact on our system?s performance.For those feature types whose effect on perfor-mance is neutral in the first lines of ablation results(feature 4 in S1, features 3, 5, and 7 in S2, and fea-tures 1, 4, 5, and 7 in S3), it is important to notethat their neutrality does not mean that they areunimportant.
It merely means that they do not im-prove performance in the presence of other featuretypes.
We can see this is the case by noting thatthey are not all the least important feature types intheir respective subtables as indicated by columnorder.
For example, by the time feature 1 gets per-manently removed in Table 5(c), its removal harmsperformance by .002 S3 points.5.4 Analysis of Predicted ScoresTo more closely examine the behavior of our sys-tem, in Table 6 we chart the distributions of scoresit predicts for essays having each gold standardscore.
As an example of how to read this table,consider the number 3.06 appearing in row 2.0 inthe .25 column of the S3 region.
This means that25% of the time, when our system with parameterstuned for optimizing S3 is presented with a test es-say having a gold standard score of 2.0, it predictsthat the essay has a score less than or equal to 3.06.From this table, we see that our system has astrong bias toward predicting more frequent scoresas there are no numbers less than 3.0 in the table,and about 93.7% of all essays have gold standardscores of 3.0 or above.
Nevertheless, our systemdoes not rely entirely on bias, as evidenced by thefact that each column in the table has a tendencyfor its scores to ascend as the gold standard scoreincreases, implying that our system has some suc-cess at predicting lower scores for essays withlower gold standard prompt adherence scores.Another interesting point to note about this ta-ble is that the difference in error weighting be-tween the S2 and S3 scoring metrics appears to behaving its desired effect, as every entry in the S3subtable is less than its corresponding entry in theS2 subtable due to the greater penalty the S3 met-ric imposes for predictions that are very far awayfrom the gold standard scores.6 ConclusionWe proposed a feature-rich approach to the under-investigated problem of predicting essay-levelprompt adherence scores on student essays.
In anevaluation on 830 argumentative essays selectedfrom the ICLE corpus, our system significantlyoutperformed a Random Indexing based baselineby several evaluation metrics.
To stimulate furtherresearch on this task, we make all our annotations,including our prompt adherence scores, the LDAtopic annotations, and the error annotations pub-licly available.1542ReferencesYigal Attali and Jill Burstein.
2006.
Automated essayscoring with E-rater v.2.0.
Journal of Technology,Learning, and Assessment, 4(3).David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet Allocation.
Journal of Ma-chine Learning Research, 3:993?1022.Chih-Chung Chang and Chih-Jen Lin, 2001.
LIB-SVM: A library for support vector machines.Software available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.Scott Deerwester, Susan T. Dumais, George W. Fur-nas, Thomas K. Landauer, and Richard Harshman.1990.
Indexing by latent smeantic analysis.
Jour-nal of American Society of Information Science,41(6):391?407.Sylviane Granger, Estelle Dagneaux, Fanny Meunier,and Magali Paquot.
2009. International Corpus ofLearner English (Version 2).
Presses universitairesde Louvain.Derrick Higgins and Jill Burstein.
2007.
Sentence sim-ilarity measures for essay coherence.
In Proceed-ings of the 7th International Workshop on Computa-tional Semantics.Derrick Higgins, Jill Burstein, Daniel Marcu, and Clau-dia Gentile.
2004.
Evaluating multiple aspectsof coherence in student essays.
In Human Lan-guage Technologies: The 2004 Annual Conferenceof the North American Chapter of the Associationfor Computational Linguistics, pages 185?192.Derrick Higgins, Jill Burstein, and Yigal Attali.
2006.Identifying off-topic student essays without topic-specific training data.
Natural Language Engineer-ing, 12(2):145?159.David Jurgens and Keith Stevens.
2010.
The S-Spacepackage: An open source package for word spacemodels.
In Proceedings of the ACL 2010 SystemDemonstrations, pages 30?35.Pentti Kanerva, Jan Kristoferson, and Anders Holst.2000.
Random indexing of text samples for latentsemantic analysis.
In Proceedings of the 22nd An-nual Conference of the Cognitive Science Society,pages 103?106.Scott Kirkpatrick, C. D. Gelatt, and Mario P. Vecchi.1983.
Optimization by simulated annealing.
Sci-ence, 220(4598):671?680.Thomas K. Landauer and Susan T. Dutnais.
1997.A solution to plato?s problem: The latent semanticanalysis theory of acquisition, induction, and rep-resentation of knowledge.
Psychological review,pages 211?240.Thomas K. Landauer, Darrell Laham, and Peter W.Foltz.
2003.
Automated scoring and annotation ofessays with the Intelligent Essay AssessorTM?
In Au-tomated Essay Scoring: A Cross-Disciplinary Per-spective, pages 87?112.
Lawrence Erlbaum Asso-ciates, Inc., Mahwah, NJ.Annie Louis and Derrick Higgins.
2010.
Off-topicessay detection using short prompt texts.
In Pro-ceedings of the NAACL HLT 2010 Fifth Workshopon Innovative Use of NLP for Building EducationalApplications, pages 92?95.Andrew Kachites McCallum.
2002.
MALLET: AMachine Learning for Language Toolkit.
http://mallet.cs.umass.edu.Eleni Miltsakaki and Karen Kukich.
2004.
Evaluationof text coherence for electronic essay scoring sys-tems.
Natural Language Engineering, 10(1):25?55.Robert Parker, David Graf, Junbo Kong, Ke Chen, andKazuaki Maeda.
2009.
English Gigaword FourthEdition.
Linguistic Data Consortium, Philadelphia.Isaac Persing and Vincent Ng.
2013.
Modeling the-sis clarity in student essays.
In Proceedings of the51st Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages260?269.Isaac Persing, Alan Davis, and Vincent Ng.
2010.Modeling organization in student essays.
In Pro-ceedings of the 2010 Conference on Empirical Meth-ods in Natural Language Processing, pages 229?239.Magnus Sahlgren.
2005.
An introduction to randomindexing.
In Methods and Applications of SemanticIndexing Workshop at the 7th International Confer-ence on Terminology and Knowledge Engineering.Mark D. Shermis and Jill C. Burstein.
2003.
Au-tomated Essay Scoring: A Cross-Disciplinary Per-spective.
Lawrence Erlbaum Associates, Inc., Mah-wah, NJ.Mark D. Shermis, Jill Burstein, Derrick Higgins, andKlaus Zechner.
2010.
Automated essay scoring:Writing assessment and instruction.
In InternationalEncyclopedia of Education (3rd edition).
Elsevier,Oxford, UK.Yiming Yang and Jan O. Pedersen.
1997.
A compara-tive study on feature selection in text categorization.In Proceedings of the 14th International Conferenceon Machine Learning, pages 412?420.1543
