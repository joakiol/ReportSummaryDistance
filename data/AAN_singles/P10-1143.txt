Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1412?1422,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsUnsupervised Event Coreference Resolution with Rich Linguistic FeaturesCosmin Adrian BejanInstitute for Creative TechnologiesUniversity of Southern CaliforniaMarina del Rey, CA 90292, USASanda HarabagiuHuman Language Technology InstituteUniversity of Texas at DallasRichardson, TX 75083, USAAbstractThis paper examines how a new class ofnonparametric Bayesian models can be ef-fectively applied to an open-domain eventcoreference task.
Designed with the pur-pose of clustering complex linguistic ob-jects, these models consider a potentiallyinfinite number of features and categoricaloutcomes.
The evaluation performed forsolving both within- and cross-documentevent coreference shows significant im-provements of the models when comparedagainst two baselines for this task.1 IntroductionThe event coreference task consists of findingclusters of event mentions that refer to the sameevent.
Although it has not been extensively stud-ied in comparison with the related problem of en-tity coreference resolution, solving event coref-erence has already proved its usefulness in vari-ous applications such as topic detection and track-ing (Allan et al, 1998), information extraction(Humphreys et al, 1997), question answering(Narayanan and Harabagiu, 2004), textual entail-ment (Haghighi et al, 2005), and contradiction de-tection (de Marneffe et al, 2008).Previous approaches for solving event corefer-ence relied on supervised learning methods thatexplore various linguistic properties in order to de-cide if a pair of event mentions is coreferentialor not (Humphreys et al, 1997; Bagga and Bald-win, 1999; Ahn, 2006; Chen and Ji, 2009).
Inspite of being successful for a particular labeledcorpus, these pairwise models are dependent onthe domain or language that they are trained on.Moreover, since event coreference resolution is acomplex task that involves exploring a rich set oflinguistic features, annotating a large corpus withevent coreference information for a new languageor domain of interest requires a substantial amountof manual effort.
Also, since these models are de-pendent on local pairwise decisions, they are un-able to capture a global event distribution at topicor document collection level.To address these limitations and to provide amore flexible representation for modeling observ-able data with rich properties, we present twonovel, fully generative, nonparametric Bayesianmodels for unsupervised within- and cross-document event coreference resolution.
The firstmodel extends the hierarchical Dirichlet process(Teh et al, 2006) to take into account additionalproperties associated with observable objects (i.e.,event mentions).
The second model overcomessome of the limitations of the first model.
Ituses the infinite factorial hidden Markov model(Van Gael et al, 2008b) coupled to the infinitehidden Markov model (Beal et al, 2002) in or-der to (1) consider a potentially infinite numberof features associated with observable objects, (2)perform an automatic selection of the most salientfeatures, and (3) capture the structural dependen-cies of observable objects at the discourse level.Furthermore, both models are designed to accountfor a potentially infinite number of categorical out-comes (i.e., events).
These models provide addi-tional details and experimental results to our pre-liminary work on unsupervised event coreferenceresolution (Bejan et al, 2009).2 Event CoreferenceThe problem of determining if two events are iden-tical was originally studied in philosophy.
Onerelevant theory on event identity was proposed byDavidson (1969) who argued that two events areidentical if they have the same causes and effects.Later on, a different theory was proposed by Quine(1985) who considered that each event refers toa physical object (which is well defined in spaceand time), and therefore, two events are identical1412if they have the same spatiotemporal location.
In(Davidson, 1985), Davidson abandoned his sug-gestion to embrace the Quinean theory on eventidentity (Malpas, 2009).2.1 An ExampleIn accordance with the Quinean theory, we con-sider that two event mentions are coreferential ifthey have the same event properties and share thesame event participants.
For instance, the sen-tences from Example 1 encode event mentions thatrefer to several individuated events.
These sen-tences are extracted from a newly annotated cor-pus with event coreference information (see Sec-tion 4).
In this corpus, we organize documentsthat describe the same seminal event into topics.In particular, the topics shown in this example de-scribe the seminal event of buying ATI by AMD(topic 43) and the seminal event of buying EDSby HP (topic 44).Although all the event mentions of interest em-phasized in boldface in Example 1 evoke the samegeneric event buy, they refer to three individu-ated events: e1 = {em1, em2}, e2 = {em3?6,em8}, and e3 = {em7}.
For example, em1(buy)and em3(buy) correspond to different individuatedevents since they have a different AGENT ([BU-YER(em1)=AMD] 6= [BUYER(em3)=HP]).
Thisorganization of event mentions leads to the idea ofcreating an event hierarchy which has on the firstlevel, event mentions, on the second level, individ-uated events, and on the third level, generic events.In particular, the event hierarchy corresponding tothe event mentions annotated in our example is il-lustrated in Figure 1.Solving the event coreference problem posesmany interesting challenges.
For instance, in or-der to solve the coreference chain of event men-tions that refer to the event e2, we need to takeinto account the following issues: (i) a coreferencechain can encode both within- and cross-documentcoreference information; (ii) two mentions fromthe same chain can have different word classes(e.g., em3(buy)?verb, em4(purchase)?noun); (iii)not all the mentions from the same chain are syn-onymous (e.g., em3(buy) and em8(acquire)), al-though a semantic relation might exist betweenthem (e.g., in WordNet (Fellbaum, 1998), thegenus of buy is acquire); (iv) partial (or all) prop-erties and participants of an event mention can beomitted in text (e.g., em4(purchase)).
In SectionTopic 43Document 3s4: AMD agreed to [buy]em1 Markham, Ontario-basedATI for around $5.4 billion in cash and stock, thecompanies announced Monday.s5: The [acquisition]em2 would turn AMD into one ofthe world?s largest providers of graphics chips.Topic 44Document 2s1: Hewlett-Packard is negotiating to [buy]em3 technol-ogy services provider Electronic Data Systems.s8: With a market value of about $115 billion, HPcould easily use its own stock to finance the [pur-chase]em4 .s9: If the [deal]em5 is completed, it would be HP?sbiggest [acquisition]em6 since it [bought]em7 Com-paq Computer Corp. for $19 billion in 2002.Document 5s2: Industry sources have confirmed to eWEEK thatHewlett-Packard will [acquire]em8 Electronic DataSystems for about $13 billion.Example 1: Examples of event mention annotations.buyem7e2 e3e1em5 em6em3em2em1 em4 em8Figure 1: Fragment from the event hierarchy.5, we discuss additional aspects of the event coref-erence problem that are not revealed in Example 1.2.2 Linguistic FeaturesThe events representing coreference clusters ofevent mentions are characterized by a large set oflinguistic features.
To compute an accurate eventdistribution for event coreference resolution, weassociate the following categories of linguistic fea-tures with each annotated event mention.Lexical Features (LF) We capture the lexical con-text of an event mention by extracting the follow-ing features: the head word (HW), the lemmatizedhead word (HL), the lemmatized left and rightwords surrounding the mention (LHL,RHL), andthe HL features corresponding to the left and rightmentions (LHE,RHE).
For instance, the lexical fea-tures extracted for the event mention em7(bought)from our example are HW:bought, HL:buy, LHL:it,RHL:Compaq, LHE:acquisition, and RHE:acquire.Class Features (CF) These features aim to groupmentions into several types of classes: the part-of-speech of the HW feature (POS), the word classof the HW feature (HWC), and the event class ofthe mention (EC).
The HWC feature can take oneof the following values: VERB, NOUN, ADJEC-1413TIVE, and OTHER.
As values for the EC feature,we consider the seven event classes defined inthe TimeML specification language (Pustejovskyet al, 2003a): OCCURRENCE, PERCEPTION, RE-PORTING, ASPECTUAL, STATE, I ACTION, andI STATE.
In order to extract the event classes cor-responding to the event mentions from a givendataset, we employed the event extractor describedin (Bejan, 2007).
This extractor is trained onthe TimeBank corpus (Pustejovsky et al, 2003b),which is a TimeML resource encoding temporalelements such as events, time expressions, andtemporal relations.WordNet Features (WF) In our efforts to createclusters of event mention attributes as close as pos-sible to the true attribute clusters of the individu-ated events, we build two sets of word clusters us-ing the entire lexical information from the Word-Net database.
After creating these sets of clusters,we then associate each event mention with onlyone cluster from each set.
The first set uses thetransitive closure of the WordNet SYNONYMOUSrelation to form clusters with all the words fromWordNet (WNS).
For instance, the verbs buy andpurchase correspond to the same cluster ID be-cause there exist a chain of SYNONYMOUS rela-tions between them in WordNet.
The second setconsiders as grouping criteria the categorizationof words from the WordNet lexicographer?s files(WNL).
In addition, for each word that is not cov-ered in WordNet, we create a new cluster ID ineach set of clusters.Semantic Features (SF) To extract features thatcharacterize participants and properties of eventmentions, we use the semantic parser describedin (Bejan and Hathaway, 2007).
One category ofsemantic features that we identify for event men-tions is the predicate argument structures encodedin PropBank annotations (Palmer et al, 2005).In PropBank, the predicate argument structuresare represented by events expressed as verbs intext and by the semantic roles, or predicate argu-ments, associated with these events.
For example,ARG0 annotates a specific type of semantic rolewhich represents the AGENT, DOER, or ACTORof a specific event.
Another argument is ARG1,which plays the role of the PATIENT, THEME,or EXPERIENCER of an event.
In particular, thepredicate arguments associated to the event men-tion em8(bought) from Example 1 are ARG0:[it],ARG1:[Compaq Computer Corp.], ARG3:[for $19billion], and ARG-TMP:[in 2002].Event mentions are not only expressed as verbsin text, but also as nouns and adjectives.
There-fore, for a better coverage of semantic features,we also employ the semantic annotations encodedin the FrameNet corpus (Baker et al, 1998).FrameNet annotates word expressions capable ofevoking conceptual structures, or semantic frames,which describe specific situations, objects, orevents (Fillmore, 1982).
The semantic roles as-sociated with a word in FrameNet, or frame ele-ments, are locally defined for the semantic frameevoked by the word.
In general, the words anno-tated in FrameNet are expressed as verbs, nouns,and adjectives.To preserve the consistency of semantic rolefeatures, we align frame elements to predicate ar-guments by running the PropBank semantic parseron the manual annotations from FrameNet; con-versely, we also run the FrameNet parser on themanual annotations from PropBank.
Moreover, toobtain a better alignment of semantic roles, werun both parsers on a large amount of unlabeledtext.
The result of this process is a map with allframe elements statistically aligned to all predi-cate arguments.
For instance, in 99.7% of thecases the frame element BUYER of the semanticframe COMMERCE BUY is mapped to ARG0, andin the remaining 0.3% of the cases to ARG1.
Ad-ditionally, we use this map to create a more gen-eral semantic feature which assigns to each predi-cate argument a frame element label.
In particular,the features for em8(acquire) are FEA0:BUYER,FEA1:GOODS, FEA3:MONEY, and FEATMP:TIME.Two additional semantic features used in our ex-periments are: (1) the semantic frame (FR) evokedby every mention;1 and (2) the WNS feature ap-plied to the head word of every semantic role (e.g.,WSA0, WSA1).Feature Combinations (FC) We also explore var-ious combinations of the features presented above.Examples include HW+HWC, HL+FR, FR+ARG1,LHL+RHL, etc.It is worth noting that there exist event mentionsfor which not all the features can be extracted.
Forexample, the LHE and RHE features are missingfor the first and last event mentions in a document,respectively.
Also, many semantic roles can be ab-sent for an event mention in a given context.1 The reason for extracting this feature is given by the factthat, in general, frames are able to capture properties ofgeneric events (Lowe et al, 1997).14143 Nonparametric Bayesian ModelsAs input for our models, we consider a collectionof I documents, where each document i has Jievent mentions.
For features, we make the dis-tinction between feature types and feature values(e.g., POS is a feature type and has values suchas NN and VB).
Each event mention is charac-terized by L feature types, FT, and each featuretype is represented by a finite vocabulary of fea-ture values, fv.
Thus, we can represent the ob-servable properties of an event mention as a vec-tor of L feature type ?
feature value pairs ?
(FT1 :fv1i), .
.
.
, (FTL : fvLi)?, where each feature valueindex i ranges in the feature value space associatedwith a feature type.3.1 A Finite Feature ModelWe present an extension of the hierarchical Dirich-let process (HDP)model which is able to representeach observable object (i.e., event mention) by afinite number of feature types L. Our HDP ex-tension is also inspired from the Bayesian modelproposed by Haghighi and Klein (2007).
How-ever, their model is strictly customized for entitycoreference resolution, and therefore, extending itto include additional features for each observableobject is a challenging task (Ng, 2008; Poon andDomingos, 2008).In the HDP model, a Dirichlet process (DP)(Ferguson, 1973) is associated with each docu-ment, and each mixture component (i.e., event) isshared across documents.
To describe its exten-sion, we consider Z the set of indicator randomvariables for indices of events, ?z the set of param-eters associated with an event z, ?
a notation forall model parameters, and X a notation for all ran-dom variables that represent observable features.2Given a document collection annotated with eventmentions, the goal is to find the best assignmentof event indices Z?, which maximize the poste-rior probability P (Z|X).
In a Bayesian approach,this probability is computed by integrating out allmodel parameters:P (Z|X)=?P (Z, ?|X)d?=?P (Z|X, ?
)P (?|X)d?Our HDP extension is depicted graphically inFigure 2(a).
Similar to the HDP model, the dis-tribution over events associated with each docu-ment, ?, is generated by a Dirichlet process with a2 In this subsection, the feature term is used in context of afeature type.concentration parameter ?> 0.
Since this settingenables a clustering of event mentions at the doc-ument level, it is desirable that events be sharedacross documents and the number of events K beinferred from data.
To ensure this flexibility, aglobal nonparametric DP prior with a hyperparam-eter ?
and a global base measure H can be consid-ered for ?
(Teh et al, 2006).
The global distri-bution drawn from this DP prior, denoted as ?0in Figure 2(a), encodes the event mixing weights.Thus, same global events are used for each docu-ment, but each event has a document specific dis-tribution ?i that is drawn from a DP prior centeredon the global weights ?0.To infer the true posterior probability ofP (Z|X), we follow (Teh et al, 2006) and usethe Gibbs sampling algorithm (Geman and Ge-man, 1984) based on the direct assignment sam-pling scheme.
In this sampling scheme, the pa-rameters ?
and ?
are integrated out analytically.Moreover, to reduce the complexity of comput-ing P (Z|X), we make the na?
?ve Bayes assump-tion that the feature variables X are conditionallyindependent given Z.
This allows us to factorizethe joint distribution of feature variables X condi-tioned on Z into product of marginals.
Thus, byBayes rule, the formula for sampling an event in-dex for mention j from document i, Zi,j , is:3P (Zi,j | Z?i,j,X) ?
P (Zi,j | Z?i,j)?X?XP (Xi,j |Z,X?i,j)whereXi,j represents the feature value of a featuretype corresponding to the event mention j from thedocument i.In the process of generating an event mention,an event index z is first sampled by using a mech-anism that facilitates sampling from a prior for in-finite mixture models called the Chinese restau-rant franchise (CRF) representation, as reported in(Teh et al, 2006):P (Zi,j = z | Z?i,j, ?0) ?{?
?u0 , if z = znewnz + ?
?z0 , otherwiseHere, nz is the number of event mentions withevent index z, znew is a new event index not usedalready in Z?i,j , ?z0 are the global mixing propor-tions associated with the K events, and ?u0 is theweight for the unknown mixture component.Next, to generate a feature value x (with the fea-ture type X) of the event mention, the event z is3 Z?i,j represents a notation for Z?
{Zi,j}.1415HZi????
?
?Xi(a)?0?Ji IL??HLiFRiPOSi?
?H ?F20 F21 F22 F2T??
?0?IJiZi(b)F10Y1F11Y2F12YTF1TS0FM0 FM1 FM2 FMTS1 S2 STPhase 1Phase 2(c)Figure 2: Graphical representation of our models: nodes correspond to random variables; shaded nodes denote observablevariables; a rectangle captures the replication of the structure it contains, where the number of replications is indicated in thebottom-right corner.
The model in (a) illustrates a flat representation of a limited number of features in a generalized framework(henceforth, HDPflat).
The model in (b) captures a simple example of structured network topology of three feature variables(henceforth, HDPstruct).
The dependencies involving parameters ?
and ?
in these models are omitted for clarity.
The modelfrom (c) shows the representation of the iFHMM-iHMM model as well as the main phases of its generative process.associated with a multinomial emission distribu-tion over the feature values of X having the pa-rameters ?= ??xZ?.
We assume that this emissiondistribution is drawn from a symmetric Dirichletdistribution with concentration ?X :P (Xi,j = x | Z,X?i,j) ?
nx,z + ?Xwhere Xi,j is the feature type of the mention jfrom the document i, and nx,z is the number oftimes the feature value x has been associated withthe event index z in (Z,X?i,j).
We also apply theLidstone?s smoothing method to this distribution.In cases when only a feature type is considered(e.g., X = ?HL?
), the HDPflat model is identicalwith the original HDP model.
We denote this onefeature model by HDP1f .When dependencies between feature variablesexist (e.g., in our case, frame elements are de-pendent on the semantic frames that define them,and frames are dependent on the words that evokethem), various global distributions are involved forcomputing P (Z|X).
For the model depicted inFigure 2(b), for instance, the posterior probabilityis given by:P (Zi,j)P (FRi,j |HLi,j,?
)?X?XP (Xi,j |Z)In this formula, P (FRi,j|HLi,j ,?)
is a global dis-tribution parameterized by ?, and X is a featurevariable from the set X = ?HL,POS,FR?.
Forthe sake of clarity, we omit the conditioning com-ponents of Z, HL, FR, and POS.3.2 An Infinite Feature ModelTo relax some of the restrictions of the first model,we devise an approach that combines the infinitefactorial hidden Markov model (iFHMM)with theinfinite hidden Markov model (iHMM) to formthe iFHMM-iHMM model.The iFHMM framework uses the Markov In-dian buffet process (mIBP) (Van Gael et al,2008b) in order to represent each object as a sparsesubset of a potentially unbounded set of latent fea-tures (Griffiths and Ghahramani, 2006; Ghahra-mani et al, 2007; Van Gael et al, 2008a).4 Specif-ically, the mIBP defines a distribution over an un-bounded set of binary Markov chains, where eachchain can be associated with a binary latent fea-ture that evolves over time according to Markovdynamics.
Therefore, if we denote by M the to-tal number of feature chains and by T the num-ber of observable components, the mIBP definesa probability distribution over a binary matrix Fwith T rows, which correspond to observations,and an unbounded number of columns M , whichcorrespond to features.
An observation yt con-tains a subset from the unbounded set of features{f1, f2, .
.
.
, fM} that is represented in the matrixby a binary vector Ft =?F 1t , F 2t , .
.
.
, FMt ?, whereF it = 1 indicates that f i is associated with yt.
Inother words, F decomposes the observations andrepresents them as feature factors, which can thenbe associated with hidden variables in an iFHMMmodel as depicted in Figure 2(c).4 In this subsection, a feature will be represented by a (fea-ture type:feature value) pair.1416Although the iFHMM allows a more flexiblerepresentation of the latent structure by letting thenumber of parallel Markov chains M be learnedfrom data, it cannot be used as a framework wherethe number of clustering components K is infi-nite.
On the other hand, the iHMM representsa nonparametric extension of the hidden Markovmodel (HMM) (Rabiner, 1989) that allows per-forming inference on an infinite number of statesK .
To further increase the representational powerfor modeling discrete time series data, we proposea nonparametric extension that combines the bestof the two models, and lets the parameters M andK be learned from data.As shown in Figure 2(c), each step in the newiHMM-iFHMM generative process is performedin two phases: (i) the latent feature variables fromthe iFHMM framework are sampled using themIBP mechanism; and (ii) the features sampled sofar, which become observable during this secondphase, are used in an adapted version of the beamsampling algorithm (Van Gael et al, 2008a) to in-fer the clustering components (i.e., latent events).In the first phase, the stochastic process for sam-pling features in F is defined as follows.
The firstcomponent samples a number of Poisson(??)
fea-tures.
In general, depending on the value that wassampled in the previous step (t?
1), a feature fmis sampled for the tth component according to theP (Fmt = 1 |Fmt?1 = 1) and P (Fmt = 1 |Fmt?1 = 0)probabilities.5 After all features are sampled forthe tth component, a number of Poisson(?
?/t)new features are assigned for this component, andM gets incremented accordingly.To describe the adapted beam sampler, whichis employed in the second phase of the generativeprocess, we introduce additional notations.
We de-note by (s1, .
.
.
, sT ) the sequence of hidden statescorresponding to the sequence of event mentions(y1, .
.
.
, yT ), where each state st belongs to oneof the K events, st ?
{1, .
.
.
,K}, and each men-tion yt is represented by a sequence of latent fea-tures ?F 1t , F 2t , .
.
.
, FMt ?.
One element of the tran-sition probability pi is defined as piij =P (st = j |st?1 = i), and a mention yt is generated accordingto a likelihood model F that is parameterized by astate-dependent parameter ?st (yt | st ?
F(?st)).The observation parameters ?
are drawn indepen-dently from an identical prior base distribution H .The beam sampling algorithm combines the5 Technical details for computing these probabilities are de-scribed in (Van Gael et al, 2008b).ideas of slice sampling and dynamic program-ming for an efficient sampling of state trajectories.Since in time series models the transition probabil-ities have independent priors (Beal et al, 2002),Van Gael and colleagues (2008a) also used theHDP mechanism to allow couplings across transi-tions.
For sampling the whole hidden state trajec-tory s, this algorithm employs a forward filtering-backward sampling technique.In the forward step of our adapted beam sam-pler, for each mention yt, we sample features us-ing the mIBP mechanism and the auxiliary vari-able ut ?
Uniform(0, pist?1st).
As explained in(Van Gael et al, 2008a), the auxiliary variables uare used to filter only those trajectories s for whichpist?1st ?
ut for all t. Also, in this step, we com-pute the probabilities P (st |y1:t, u1:t) for all t:P (st|y1:t,u1:t)?P (yt|st)?st?1:ut<pist?1stP (st?1|y1:t?1,u1:t?1)Here, the dependencies involving parameters piand ?
are omitted for clarity.In the backward step, we first sample theevent for the last state sT directly from P (sT |y1:T , u1:T ) and then, for all t : T?1 .
.
.
1, we sam-ple each state st given st+1 by using the formulaP (st | st+1, y1:T , u1:T) ?
P (st | y1:t, u1:t)P (st+1 |st, ut+1).
To sample the emission distribution?
efficiently, and to ensure that each mention ischaracterized by a finite set of representative fea-tures, we set the base distribution H to be con-jugate with the data distribution F in a Dirichlet-multinomial model with the multinomial parame-ters (o1, .
.
.
, oK) defined as:ok =T?t=1?fm?BtnmkIn this formula, nmk counts how many times thefeature fm was sampled for the event k, and Btstores a finite set of features for yt.The mechanism for building a finite set of rep-resentative features for the mention yt is based onslice sampling (Neal, 2003).
Letting qm be thenumber of times the feature fm was sampled in themIBP, and vt an auxiliary variable for yt such thatvt ?
Uniform(1,max{qm : Fmt = 1}), we definethe finite feature set Bt for the observation yt asBt = {fm : Fmt = 1?qm ?
vt}.
The finiteness ofthis feature set is based on the observation that, inthe generative process of the mIBP, only a finite set1417of features are sampled for a component.
We de-note this model as iFHMM-iHMMuniform.
Also,it is worth mentioning that, by using this type ofsampling, only the most representative features ofyt get selected in Bt.Furthermore, we explore the mechanism forselecting a finite set of features associated withan observation by: (1) considering all the ob-servation?s features whose corresponding featurecounter qm ?
1 (unfiltered); (2) selecting onlythe higher half of the feature distribution consist-ing of the observation?s features that were sampledat least once in the mIBP model (median); and(3) sampling vt from a discrete distribution of theobservation?s features that were sampled at leastonce in the mIBP (discrete).4 ExperimentsDatasets One dataset we employed is the au-tomatic content extraction (ACE) (ACE-Event,2005).
However, the utilization of the ACE corpusfor the task of solving event coreference is lim-ited because this resource provides only within-document event coreference annotations using arestricted set of event types such as LIFE, BUSI-NESS, CONFLICT, and JUSTICE.
Therefore, as asecond dataset, we created the EventCorefBank(ECB) corpus6 to increase the diversity of eventtypes and to be able to evaluate our models forboth within- and cross-document event corefer-ence resolution.
One important step in the cre-ation process of this corpus consists in finding setsof related documents that describe the same semi-nal event such that the annotation of coreferentialevent mentions across documents is possible.
Forthis purpose, we selected from the GoogleNewsarchive7 various topics whose description containskeywords such as commercial transaction, attack,death, sports, terrorist act, election, arrest, natu-ral disaster, etc.
The entire annotation process forcreating the ECB resource is described in (Bejanand Harabagiu, 2008).
Table 1 lists several basicstatistics extracted from these two corpora.Evaluation For a more realistic approach, we notonly trained the models on the manually annotatedevent mentions (i.e., true mentions), but also on allthe possible mentions encoded in the two datasets.To extract all event mentions, we ran the eventidentifier described in (Bejan, 2007).
The men-tions extracted by this system (i.e., system men-6 ECB is available at http://www.hlt.utdallas.edu/?ady.7 http://news.google.com/ACE ECBNumber of topics ?
43Number of documents 745 482Number of within-topic events ?
339Number of cross-document events ?
208Number of within-document events 4946 1302Number of true mentions 6553 1744Number of system mentions 45289 21175Number of distinct feature values 391798 237197Table 1: Statistics of the ACE and ECB corpora.tions) were able to cover all the true mentions fromboth datasets.
As shown in Table 1, we extractedfrom ACE and ECB corpora 45289 and 21175 sys-tem mentions, respectively.We report results in terms of recall (R), preci-sion (P), and F-score (F) by employing the men-tion-based B3 metric (Bagga and Baldwin, 1998),the entity-based CEAF metric (Luo, 2005), and thepairwise F1 (PW) metric.
All the results are av-eraged over 5 runs of the generative models.
Inthe evaluation process, we considered only thetrue mentions of the ACE test dataset, and theevent mentions of the test sets derived from a 5-fold cross validation scheme on the ECB dataset.For evaluating the cross-document coreference an-notations, we adopted the same approach as de-scribed in (Bagga and Baldwin, 1999) by merg-ing all the documents from the same topic into ameta-document and then scoring this document asperformed for within-document evaluation.
Forboth corpora, we considered a set of 132 featuretypes, where each feature type consists on averageof 3900 distinct feature values.Baselines We consider two baselines for eventcoreference resolution (rows 1&2 in Tables 2&3).One baseline groups each event mention by itsevent class (BLeclass).
Therefore, for this baseline,we cluster mentions according to their correspond-ing EC feature value.
Similarly, the second base-line uses as grouping criteria for event mentionstheir corresponding WNS feature value (BLsyn).HDP Extensions Due to memory limitations, weevaluated the HDP models on a restricted set ofmanually selected feature types.
In general, theHDP1f model with the feature type HL, whichplays the role of a baseline for the HDPflat andHDPstruct models, outperforms both baselines onthe ACE and ECB datasets.
For the HDPflat mod-els (rows 4?7 in Tables 2&3), we classified the ex-periments according to the set of feature types de-scribed in Section 2.
Our experiments reveal thatthe best configuration of features for this model1418Model configuration B3 CEAF PW B3 CEAF PWR P F R P F R P F R P F R P F R P FECB | WD ECB | CD1 BLeclass 97.7 55.8 71.0 44.5 80.1 57.2 93.7 25.4 39.8 93.8 49.6 64.9 36.6 72.7 48.7 90.7 28.6 43.32 BLsyn 91.5 57.4 70.5 45.7 75.9 57.0 65.3 21.9 32.6 84.6 48.1 61.3 32.8 63.6 43.3 66.2 26.0 37.33 HDP1f (HL) 84.3 89.0 86.5 83.4 79.6 81.4 36.6 53.4 42.6 67.0 86.2 75.3 76.2 57.1 65.2 34.9 58.9 43.54 HDPflat (LF) 81.4 98.2 89.0 92.7 77.2 84.2 24.7 82.8 37.7 63.8 97.3 77.0 84.9 54.3 66.1 27.2 88.5 41.55 (LF+CF) 81.5 98.0 89.0 92.8 77.9 84.7 24.6 80.7 37.4 64.6 97.3 77.6 85.3 55.6 67.2 27.6 88.7 42.06 (LF+CF+WF) 82.0 98.9 89.6 93.7 78.4 85.3 26.8 89.9 41.0 65.8 98.0 78.7 86.7 57.1 68.8 29.6 93.0 44.87 (LF+CF+WF+SF) 82.1 99.2 89.8 93.9 78.2 85.3 27.0 92.4 41.3 65.0 98.7 78.3 86.9 56.0 68.0 29.2 95.1 44.48 HDPstruct (HL?FR?FEA) 84.3 97.1 90.2 92.7 81.1 86.5 34.4 83.0 48.6 69.3 95.8 80.4 86.2 60.1 70.8 37.5 85.6 52.19 iFHMM-iHMMunfiltered 82.6 97.7 89.5 92.7 78.5 85.0 28.5 82.4 41.8 67.2 96.4 79.1 85.6 58.0 69.1 32.5 87.7 47.210 iFHMM-iHMMdiscrete 82.6 98.1 89.7 93.2 79.0 85.5 29.7 85.4 44.0 66.2 96.2 78.4 84.8 57.2 68.3 32.2 88.1 47.111 iFHMM-iHMMmedian 82.6 97.8 89.5 92.9 78.8 85.3 29.3 83.7 43.0 67.0 96.5 79.0 86.1 58.3 69.5 33.1 88.1 47.912 iFHMM-iHMMuniform 82.5 98.1 89.6 93.1 78.8 85.3 29.4 86.6 43.7 67.0 96.4 79.0 85.5 58.0 69.1 33.3 88.3 48.2Table 2: Results for within-document (WD) and cross-document (WD) coreference resolution on the ECB dataset.B3 CEAF PWR P F R P F R P FACE | WD1 97.9 25.0 39.9 14.7 64.4 24.0 93.5 8.2 15.22 89.3 36.7 52.1 25.1 64.8 36.2 63.8 10.5 18.13 86.0 70.6 77.5 62.3 76.4 68.6 50.5 27.7 35.84 82.9 82.6 82.7 74.9 75.8 75.3 42.4 41.9 42.15 82.0 84.9 83.4 77.8 75.3 76.6 37.9 45.1 41.26 83.3 83.6 83.4 76.3 76.2 76.3 42.2 43.9 43.07 83.4 84.2 83.8 76.9 76.5 76.7 43.3 47.1 45.18 86.2 76.9 81.3 69.0 77.5 73.0 53.2 38.1 44.49 82.8 83.6 83.2 75.8 75.0 75.4 41.4 42.6 42.010 83.1 81.5 82.3 73.7 75.1 74.4 41.9 40.1 41.011 83.0 81.3 82.1 73.2 75.2 74.2 40.7 39.0 39.812 81.9 82.2 82.1 74.6 74.5 74.5 37.2 39.0 38.1Table 3: Results for WD coreference resolution on ACE.consists of a combination of feature types fromall the categories of features (row 7).
For theHDPstruct experiments, we considered the set offeatures of the best HDPflat experiment as well asthe dependencies between HL, FR, and FEA.
Over-all, we can assert that HDPflat achieved the bestperformance results on the ACE test dataset (Ta-ble 3), whereas HDPstruct proved to be more ef-fective on the ECB dataset (Table 2).
Moreover,the results of the HDPflat and HDPstruct modelsshow an F-score increase by 4-10% over HDP1f ,and therefore, the results prove that the HDP ex-tension provides a more flexible representation forclustering objects with rich properties.We also plot the evolution of our generativeprocesses.
For instance, Figure 3(a) shows thatthe HDPflat model corresponding to row 7 in Ta-ble 3 converges in 350 iteration steps to a posteriordistribution over event mentions from ACE witharound 2000 latent events.
Additionally, our ex-periments with different values of the ?
parame-ter for the Lidstone?s smoothing method indicatethat this smoothing method is useful for improv-ing the performance of the HDP models.
How-ever, we could not find a ?
value in our experi-ments that brings a major improvement over thenon-smoothed HDPmodels.
Figure3(b) shows theperformances of HDPstruct on ECBwith various ?values.8 The HDP results from Tables 2&3 corre-spond to a ?
value of 10?4 and 10?2 for HDPflatand HDPstruct, respectively.iFHMM-iHMM In spite of the fact that theiFHMM-iHMM model employs automatic featureselection, its results remain competitive againstthe results of the HDP models, where the fea-ture types were manually tuned.
When compar-ing the strategies for filtering feature values in thisframework, we could not find a distinct separationbetween the results obtained by the unfiltered,discrete, median, and uniform models.
As ob-served from Tables 2&3, most of the iFHMM-iHMM results fall in between the HDPflat andHDPstruct results.
The results were obtained byautomatically selecting only up to 1.5% of distinctfeature values.
Figure 3(c) shows the percents offeatures employed by this model for various val-ues of the parameter ??
that controls the numberof sampled features.
The best results (also listedin Tables 2&3) were obtained for ??
= 10 (0.05%)on ACE and ??
= 150 (0.91%) on ECB.To show the usefulness of the sampling schemesconsidered for this model, we also compare inTable 4 the results obtained by an iFHMM-iHMM model that considers all the feature valuesassociated with an observable object (iFHMM-iHMMall) against the iFHMM-iHMMmodels thatemploy the mIBP sampling scheme together withthe unfiltered, discrete, median, and uniformfiltering schemes.
Because of the memory limi-tation constraints, we performed the experimentslisted in Table 4 by selecting only a subset from8 A configuration ?
= 0 in the Lidstone?s smoothing methodis equivalent with a non-smoothed version of the model onwhich it is applied.14191000150020002500HDPflat     |      ACE      |      WDNumber of categories0 50 100 150 200 250 300 350?4.5?4?3.5?3?2.5 x 105Number of iterationsLog?likelihood(a)3040506070809010090.2786.5348.620 10?7 10?6 10?4 10?3 10?2 101 102?F1?measureHDPstruct|      ECB      |      WDB3 CEAF PW(b)00.20.40.60.811.21.41.61.8210 50 100 150 200 2500.070.320.630.911.201.47?
?Number of featurevalues(%)iFHMM?iHMM      |      ECB      |      WD&CD(c)Figure 3: (a) Evolution of K and log-likelihood in the HDPflat model.
(b) Evaluation of the Lidstone?s smoothing method inthe HDPstruct model.
(c) Counts of features employed by the iFHMM-iHMM model for various ??
values.Model B3 CEAF PWR P F R P F R P FACE | WDall 89.3 39.8 55.0 30.2 68.8 42.0 62.7 9.1 15.9unfiltered 83.3 77.7 80.4 70.6 75.9 73.2 42.1 34.6 38.0discrete 83.8 80.7 82.2 73.0 75.8 74.4 43.9 39.1 41.4median 83.5 80.2 81.8 72.2 75.3 73.7 42.7 38.2 40.3uniform 82.8 80.7 81.7 72.8 75.2 73.9 41.4 39.3 40.3ECB | WDall 89.5 62.5 73.6 53.3 76.5 62.8 60.7 22.9 33.2unfiltered 82.6 96.6 89.0 92.0 79.1 85.1 28.4 75.6 41.0discrete 83.1 96.7 89.4 91.6 79.2 84.9 30.5 79.0 43.9median 82.5 97.3 89.3 92.8 78.9 85.3 29.2 78.8 42.0uniform 82.7 96.0 88.9 91.1 79.0 84.6 29.3 74.9 41.6ECB | CDall 79.3 54.4 64.5 43.3 61.3 50.7 59.6 26.2 36.4unfiltered 67.2 94.5 78.5 84.7 59.2 69.6 32.8 82.5 46.8discrete 67.6 94.8 78.9 83.8 58.3 68.8 34.3 85.3 48.9median 66.7 95.2 78.4 84.5 57.7 68.5 32.2 83.7 46.3uniform 67.7 93.6 78.4 83.6 59.2 69.2 33.6 79.5 46.9Table 4: Feature non-sampling vs. feature sampling in theiFHMM-iHMM model.the feature types which proved to be salient inthe HDP experiments.
As listed in Table 4,all the iFHMM-iHMM models that used a fea-ture sampling scheme significantly outperformthe iFHMM-iHMMall model; this proves that allthe sampling schemes considered in the iFHMM-iHMM framework are able to successfully filterout noisy and redundant feature values.The closest comparison to prior work is thesupervised approach described in (Chen and Ji,2009) that achieved a 92.2% B3 F-measure on theACE corpus.
However, for this result, ground truthevent mentions as well as a manually tuned coref-erence threshold were employed.5 Error AnalysisOne frequent error occurs when a more complexform of semantic inference is needed to find a cor-respondence between two event mentions of thesame individuated event.
For instance, since allproperties and participants of em3(deal) are omit-ted in our example and no common features ex-ist between em3(buy) and em1(buy) to indicate asimilarity between these mentions, they will mostprobably be assigned to different clusters.
This ex-ample also suggests the need for a better modelingof the discourse salience for event mentions.Another common error is made when match-ing the semantic roles corresponding to coref-erential event mentions.
Although we simu-lated entity coreference by using various seman-tic features, the task of matching participants ofcoreferential event mentions is not completelysolved.
This is because, in many coreferen-tial cases, partonomic relations between seman-tic roles need to be inferred.9 Examples ofsuch relations extracted from ECB are Israeliforces PART OF????
?Israel, an Indian warship PART OF????
?theIndian navy, his cell PART OF????
?Sicilian jail.
Simi-larly for event properties, many coreferential ex-amples do not specify a clear location and timeinterval (e.g., Jabaliya refugee camp PART OF????
?Gaza,Tuesday PART OF????
?this week).
In future work, weplan to build relevant clusters using partonomiesand taxonomies such as the WordNet hierarchiesbuilt from MERONYMY/HOLONYMY and HYPER-NYMY/HYPONYMY relations, respectively.106 ConclusionWe have presented two novel, nonparametricBayesian models that are designed to solve com-plex problems that require clustering objects char-acterized by a rich set of properties.
Our experi-ments for event coreference resolution proved thatthese models are able to solve real data applica-tions in which the feature and cluster numbers aretreated as free parameters, and the selection of fea-ture values is performed automatically.9 This observation was also reported in (Hasler and Orasan,2009).
10 This task is not trivial since, if applying the tran-sitive closure on these relations, all words will end up beingpart from the same cluster with entity for instance.1420ReferencesACE-Event.
2005.
ACE (Automatic Content Extrac-tion) English Annotation Guidelines for Events, ver-sion 5.4.3 2005.07.01.David Ahn.
2006.
The stages of event extraction.In Proceedings of the Workshop on Annotating andReasoning about Time and Events, pages 1?8.James Allan, Jaime Carbonell, George Doddington,Jonathan Yamron, and Yiming Yang.
1998.
TopicDetection and Tracking Pilot Study: Final Report.In Proceedings of the Broadcast News Understand-ing and Transcription Workshop, pages 194?218.Amit Bagga and Breck Baldwin.
1998.
Algorithmsfor Scoring Coreference Chains.
In Proceedings ofthe 1st International Conference on Language Re-sources and Evaluation (LREC-1998).Amit Bagga and Breck Baldwin.
1999.
Cross-Document Event Coreference: Annotations, Exper-iments, and Observations.
In Proceedings of theACL Workshop on Coreference and its Applications,pages 1?8.Collin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The Berkeley FrameNet project.
In Pro-ceedings of the 36th Annual Meeting of the Associ-ation for Computational Linguistics and 17th Inter-national Conference on Computational Linguistics(COLING-ACL).Matthew J. Beal, Zoubin Ghahramani, and Carl Ed-ward Rasmussen.
2002.
The Infinite HiddenMarkov Model.
In Advances in Neural InformationProcessing Systems 14 (NIPS).Cosmin Adrian Bejan and Sanda Harabagiu.
2008.A Linguistic Resource for Discovering Event Struc-tures and Resolving Event Coreference.
In Proceed-ings of the Sixth International Conference on Lan-guage Resources and Evaluation (LREC).Cosmin Adrian Bejan and Chris Hathaway.
2007.UTD-SRL: A Pipeline Architecture for ExtractingFrame Semantic Structures.
In Proceedings of theFourth International Workshop on Semantic Evalu-ations (SemEval), pages 460?463.Cosmin Adrian Bejan, Matthew Titsworth, AndrewHickl, and Sanda Harabagiu.
2009.
NonparametricBayesian Models for Unsupervised Event Corefer-ence Resolution.
In Advances in Neural InformationProcessing Systems 23 (NIPS).Cosmin Adrian Bejan.
2007.
Deriving Chronologi-cal Information from Texts through a Graph-basedAlgorithm.
In Proceedings of the 20th Florida Ar-tificial Intelligence Research Society InternationalConference (FLAIRS), Applied Natural LanguageProcessing track.Zheng Chen and Heng Ji.
2009.
Graph-based EventCoreference Resolution.
In Proceedings of the2009 Workshop on Graph-based Methods for Natu-ral Language Processing (TextGraphs-4), pages 54?57.Donald Davidson, 1969.
The Individuation of Events.In N. Rescher et al, eds., Essays in Honor of Carl G.Hempel, Dordrecht: Reidel.
Reprinted in D. David-son, ed., Essays on Actions and Events, 2001, Ox-ford: Clarendon Press.Donald Davidson, 1985.
Reply to Quine on Events,pages 172?176.
In E. LePore and B.
McLaughlin,eds., Actions and Events: Perspectives on the Phi-losophy of Donald Davidson, Oxford: Blackwell.Marie-Catherine de Marneffe, Anna N. Rafferty, andChristopher D. Manning.
2008.
Finding Contra-dictions in Text.
In Proceedings of the 46th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies (ACL-HLT), pages 1039?1047.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
MIT Press.Thomas S. Ferguson.
1973.
A Bayesian Analysisof Some Nonparametric Problems.
The Annals ofStatistics, 1(2):209?230.Charles J. Fillmore.
1982.
Frame Semantics.
In Lin-guistics in the Morning Calm.Stuart Geman and Donald Geman.
1984.
Stochas-tic relaxation, Gibbs distributions and the Bayesianrestoration of images.
IEEE Transactions on PatternAnalysis and Machine Intelligence, 6:721?741.Zoubin Ghahramani, T. L. Griffiths, and Peter Sollich,2007.
Bayesian Statistics 8, chapter Bayesian non-parametric latent feature models, pages 201?225.Oxford University Press.Tom Griffiths and Zoubin Ghahramani.
2006.
InfiniteLatent Feature Models and the Indian Buffet Pro-cess.
In Advances in Neural Information ProcessingSystems 18 (NIPS), pages 475?482.Aria Haghighi and Dan Klein.
2007.
Unsuper-vised Coreference Resolution in a NonparametricBayesian Model.
In Proceedings of the 45th An-nual Meeting of the Association of ComputationalLinguistics (ACL), pages 848?855.Aria Haghighi, Andrew Ng, and Christopher Man-ning.
2005.
Robust Textual Inference via GraphMatching.
In Proceedings of Human LanguageTechnology Conference and Conference on Empiri-cal Methods in Natural Language Processing (HLT-EMNLP), pages 387?394.Laura Hasler and Constantin Orasan.
2009.
Docoreferential arguments make event mentions coref-erential?
In Proceedings of the 7th DiscourseAnaphora and Anaphor Resolution Colloquium(DAARC 2009).1421Kevin Humphreys, Robert Gaizauskas, and Saliha Az-zam.
1997.
Event coreference for information ex-traction.
In Proceedings of the Workshop on Opera-tional Factors in Practical, Robust Anaphora Reso-lution for Unrestricted Texts, 35th Meeting of ACL,pages 75?81.John B. Lowe, Collin F. Baker, and Charles J. Fillmore.1997.
A frame-semantic approach to semantic an-notation.
In Proceedings of the SIGLEX Workshopon Tagging Text with Lexical Semantics: Why, What,and How?, pages 18?24.Xiaoqiang Luo.
2005.
On coreference resolution per-formance metrics.
In Proceedings of the HumanLanguage Technology Conference and Conferenceon Empirical Methods in Natural Language Pro-cessing (EMNLP-2005), pages 25?32.Jeff Malpas.
2009.
Donald Davidson.
In TheStanford Encyclopedia of Philosophy (Fall 2009Edition), Edward N. Zalta (ed.
), http://plato.stanford.edu/archives/fall2009/entries/davidson/.Srini Narayanan and Sanda Harabagiu.
2004.
Ques-tion Answering Based on Semantic Structures.
InProceedings of the 20th International Conference onComputational Linguistics (COLING), pages 693?701.Radford M. Neal.
2003.
Slice Sampling.
The Annalsof Statistics, 31:705?741.Vincent Ng.
2008.
Unsupervised Models for Corefer-ence Resolution.
In Proceedings of the 2008 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP), pages 640?649.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The Proposition Bank: An Annotated Cor-pus of Semantic Roles.
Computational Linguistics,31(1):71?105.Hoifung Poon and Pedro Domingos.
2008.
JointUnsupervised Coreference Resolution with MarkovLogic.
In Proceedings of the 2008 Conference onEmpirical Methods in Natural Language Processing(EMNLP), pages 650?659.James Pustejovsky, Jose Castano, Bob Ingria, RoserSauri, Rob Gaizauskas, Andrea Setzer, and Gra-ham Katz.
2003a.
TimeML: Robust Specificationof Event and Temporal Expressions in Text.
InProceedings of the Fifth International Workshop onComputational Semantics (IWCS).James Pustejovsky, Patrick Hanks, Roser Sauri, An-drew See, Robert Gaizauskas, Andrea Setzer,Dragomir Radev, Beth Sundheim, David Day, LisaFerro, and Marcia Lazo.
2003b.
The TimeBankCorpus.
In Corpus Linguistics, pages 647?656.W.
V. O. Quine, 1985.
Events and Reification, pages162?171.
In E. LePore and B. P. McLaughlin, eds.,Actions and Events: Perspectives on the philosophyof Donald Davidson, Oxford: Blackwell.
Reprintedin R. Casati and A. C. Varzi, eds., Events, 1996,pages 107?116, Aldershot: Dartmouth.Lawrence R. Rabiner.
1989.
A Tutorial on Hid-den Markov Models and Selected Applications inSpeech Recognition.
In Proceedings of the IEEE,pages 257?286.Yee Whye Teh, Michael Jordan, Matthew Beal, andDavid Blei.
2006.
Hierarchical Dirichlet Pro-cesses.
Journal of the American Statistical Associa-tion, 101(476):1566?1581.Jurgen Van Gael, Y. Saatci, Yee Whye Teh, and ZoubinGhahramani.
2008a.
Beam Sampling for the Infi-nite Hidden Markov Model.
In Proceedings of the25th Annual International Conference on MachineLearning (ICML), pages 1088?1095.Jurgen Van Gael, Yee Whye Teh, and Zoubin Ghahra-mani.
2008b.
The Infinite Factorial Hidden MarkovModel.
In Advances in Neural Information Process-ing Systems 21 (NIPS).1422
