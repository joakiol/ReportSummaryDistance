Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 443?454,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsAn I-vector Based Approach to Compact Multi-Granularity Topic SpacesRepresentation of Textual DocumentsMohamed Morchid?, Mohamed Bouallegue?, Richard Dufour?,Georges Linar`es?, Driss Matrouf?and Renato de Mori??
?LIA, University of Avignon, France?McGill University, School of Computer Science, Montreal, Quebec, Canada{firstname.lastname}@univ-avignon.frrdemori@cs.mcgill.caAbstractVarious studies highlighted that topic-based approaches give a powerful spo-ken content representation of documents.Nonetheless, these documents may con-tain more than one main theme, and theirautomatic transcription inevitably containserrors.
In this study, we propose an orig-inal and promising framework based on acompact representation of a textual docu-ment, to solve issues related to topic spacegranularity.
Firstly, various topic spacesare estimated with different numbers ofclasses from a Latent Dirichlet Allocation.Then, this multiple topic space representa-tion is compacted into an elementary seg-ment, called c-vector, originally developedin the context of speaker recognition.
Ex-periments are conducted on the DECODAcorpus of conversations.
Results show theeffectiveness of the proposed multi-viewcompact representation paradigm.
Ouridentification system reaches an accuracyof 85%, with a significant gain of 9 pointscompared to the baseline (best single topicspace configuration).1 IntroductionAutomatic Speech Recognition (ASR) systemsfrequently fail on noisy conditions and high WordError Rates (WER) make the analysis of the au-tomatic transcriptions difficult.
Speech analyt-ics suffer from these transcription issues that maybe overcome by improving the ASR robustnessand/or the tolerance of speech analytic systems toASR errors.
This paper proposes a new methodto improve the robustness of speech analytics bycombining a semantic multi-model approach anda noise reduction technique based on the i-vectorparadigm.This method is evaluated in the applicationframework of the RATP call centre (Paris PublicTransportation Authority), focusing on the themeidentification task (Bechet et al., 2012).Telephone conversations are a particular caseof human-human interaction whose automaticprocessing raises problems, especially due to thespeech recognition step required to obtain thetranscription of the speech contents.
First, thespeaker?s behavior may be unexpected and thetraining/test mismatch may be very large.
Second,the speech signal may be strongly impacted byvarious sources of variability: environment andchannel noises, acquisition devices, etc.Telephone conversation issuesTopics are related to the reason why the customercalled.
Various classes corresponding to themain customer?s requests are considered (lost andfounds, traffic state, timelines, etc).
In additionto classical issues in such adverse conditions,the topic-identification system should deal withproblems related to class proximity.
For example,a lost & found request is related to itinerary(where was the object lost?)
or timeline (when?
),that could appear in most of the classes.
In fact,these conversations involve a relatively smallset of basic concepts related to transportationissues.
Figure 1 shows an example of a dialoguewhich is manually labeled by the agent as anissue related to an infraction.
However, wordsin bold suggest that this conversation could berelated to a transportation card.
Thus, we assumethat a dialogue representation should be seen asa multi-view problem to substantiate the claimsregarding the multi-theme representation of agiven dialogue.On the other hand, multi-view approaches in-troduce additional variability due to the diversityof the views.
This variability is also due to thevocabulary used by both agent and customer443Agent: HelloCustomer: HelloAgent: Speaking ...Customer: I call you becauseI was fined today, but I stillhave an imagine cardsuitable for zone 1 [...] I forgotto use my navigo card forzone 2Agent: You did not useyour navigo card, that iswhy they give you a fine notfor a zone issue [...]Customer: Thanks, byeAgent: byeAgentCustomerTransportationcardsFigure 1: Example of a dialogue from the DE-CODA corpus labeled by the agent as an infractionissue which contains more than one theme (infrac-tion + transportation cards).during a telephone conversation.
Indeed, anagent have to follow an predefined scenario ofconversation.
Thus, the agent can find the mainreason for the call which corresponds to the theme.Proposed solutionsAn efficient way to tackle both ASR robustnessand class ambiguity could be to map dialoguesinto a topic space abstracting the ASR outputs.Then, dialogue categorization is achieved in thistopic space.
Numerous unsupervised methods fortopic-space estimation were proposed in the past.Latent Dirichlet Allocation (LDA) (Blei et al.,2003) has been largely used for speech analytics;one of its main drawbacks is the tuning of themodel, that involves various meta-parameterssuch as the number of classes (that determinesthe model granularity), word distribution meth-ods, temporal spans.
.
.
If the decision process ishighly dependent on these features, the system?sperformance could be quite unstable.Classically, this abstract representation involvesselecting the right number of classes composingthe topic space.
This decision is crucial sincetopic model perplexity, which expresses its qual-ity, is highly dependent on this feature.
Further-more, the multi-theme context of the study (seeFigure 1) involves a more complex dialogue rep-resentation.
In this paper, we propose to deal withthese two drawbacks by using a compact represen-tation from multiple topic spaces.
This model isbased on a robust multi-view representation of thetextual documents.A multi-view representation of a dialogue intro-duces both a relevant variability needed to repre-sent different contexts of the dialogue, and a noisyvariability related to topic space processing.
Thus,a topic-based representation of a dialogue is builtfrom the dialogue content itself.
For this reason,the mapping process of a dialogue into severaltopic spaces generates a noisy variability related tothe difference between the dialogue and the con-tent of each class.
In the same way, the relevantvariability comes from the common content be-tween the dialogue and the classes composing thetopic space.We propose to reduce the noisy variability byusing a factor analysis technique, which was ini-tially developed in the domain of speaker identifi-cation.
In this field, the factor analysis paradigmis used as a decomposition model that enables toseparate the representation space into two sub-spaces containing respectively useful and uselessinformation.
The general Joint Factor Analysis(JFA) paradigm (Kenny et al., 2008) considersmultiple variabilities that may be cross-dependent.Therefore, JFA representation allows us to com-pensate the variability within sessions of a samespeaker.
This representation is an extension of theGMM-UBM (Gaussian Mixture Model-UniversalBackground Model) models (Reynolds and Rose,1995).
(Dehak et al., 2011) extract a compactsuper-vector (called an i-vector) from the GMMsuper-vector.
The aim of the compression pro-cess (i-vector extraction) is to represent the super-vector variability in a low dimensional space.
Al-though this compact representation is widely usedin speaker recognition systems, this method hasnot been used yet in the field of text classification.In this paper, we propose to apply factor anal-ysis to compensate noisy variabilities due to themultiplication of LDA models.
Furthermore, anormalization approach to condition dialogue rep-resentations (multi-model and i-vector) is pre-sented.
The two methods showed improvementsfor speaker verification: within Class CovarianceNormalization (WCCN) (Dehak et al., 2011) andEigen Factor Radial (EFR) (Bousquet et al., 2011).The latter includes length normalization (Garcia-Romero and Espy-Wilson, 2011).
Both methodsdilate the total variability space as a means of re-ducing the within-class variability.
In our multi-model representation, the within class variabilityis redefined according to both dialogue content444(vocabulary) and topic space characteristics (worddistributions among the topics).
Thus, the speakeris represented by a theme, and the speaker sessionis a set of topic-based representations (frames) ofa dialogue (session).The paper is organized as follows.
Section 2presents previous related works.
The dialogue rep-resentation is described in Section 3.
Section 4 in-troduces the i-vector compact representation andpresents its application to text documents.
Sec-tions 5 and 6 report experiments and results.
Thelast section concludes and proposes some perspec-tives.2 Related workIn the past, several approaches considered atext document as a mixture of latent topics.These methods, such as Latent Semantic Analysis(LSA) (Deerwester et al., 1990; Bellegarda, 1997),Probabilistic LSA (PLSA) (Hofmann, 1999) orLatent Dirichlet Allocation (LDA) (Blei et al.,2003), build a higher-level representation of thedocument in a topic space.
?
Document isthen considered as a bag-of-words (Salton, 1989)where the word order is not taken into account.These methods have demonstrated their perfor-mance on various tasks, such as sentence (Belle-garda, 2000) or keyword (Suzuki et al., 1998) ex-traction.In opposition to a multinomial mixture model,LDA considers that a theme is associated to eachoccurrence of a word composing the document,rather than associate a topic to the complete doc-ument.
Therefore, a document can change topicsfrom a word to another one.
However, word oc-currences are connected by a latent variable whichcontrols the global match of the distribution ofthe topics in the document.
These latent topicsare characterized by a distribution of associatedword probabilities.
PLSA and LDA models havebeen shown to generally outperform LSA on IRtasks (Hofmann, 2001).
Moreover, LDA providesa direct estimate of the relevance of a topic givena word set.
In this paper, probabilities of hiddentopic features, estimated with LDA, are consideredfor possibly capturing word dependencies express-ing the semantic contents of a given conversation.Topic-based approaches involve defining anumber of topics composing the topic space.
Thechoice of the ?right?
number of topics is a crucialstep, especially when the documents may containmultiple themes.
Many studies have tried to finda relevant method to deal with this issue.
(Arun etal., 2010) proposed to use a Singular Value De-composition (SVD) to represent the separabilitybetween the words contained in the vocabulary.Then, if the singular values of the topic-word ma-trix M equal the norm of the rows of M, this meansthat the vocabulary is well separated among thetopics.
This method has to be evaluated with theKullback-Liebler divergence metric for each topicspace.
However, this process would be time con-suming for thousands of representations of a dia-logue.
(Teh et al., 2004) proposed the HierarchicalDirichlet Process (HDP) method to find the ?right?number of topics by assuming that the data hasa hierarchical structure.
The HDP models werethen compared to the LDA ones on the samedataset.
(Zavitsanos et al., 2008) presented amethod to learn the right depth of an ontology de-pending of the number of topics of LDA models.The study presented by (Cao et al., 2009) is quitesimilar to (Teh et al., 2004).
The authors considerthe average correlation between pairs of topics ateach stage as the right number of topics.All these methods assume that a document canhave only one representation since they considerthat finding the optimal topic model is the best so-lution.
Another solution would be to consider a setof topic models to represent a document.
Nonethe-less, a multi-topic-based representation of a dia-logue can involve a noisy variability due to themapping of a dialogue in each topic space.
Indeed,a dialogue does not share its content (i.e.
words)with each class composing the topic space.
Thus,a variability is added during the mapping pro-cess.
Another weakness of the multi-view repre-sentation is the relation between classes in a topicspace.
(Blei and Lafferty, 2006) show that classesinto a LDA topic space are correlated.
More-over, (Li and McCallum, 2006) consider a classas a node of an acyclic graph and as a distribu-tion over other classes contained in the same topicspace.3 Multi-view representation of automaticdialogue transcriptions in ahomogeneous spaceThe purpose of the considered application is theidentification of the major theme of a human-human telephone conversation in the customer445care service (CCS) of the RATP Paris transporta-tion system.
The approach considered in this pa-per focuses on modeling the variability betweendifferent dialogues expressing the same theme t.For this purpose, it is important to select relevantfeatures that represent semantic contents for thetheme of a dialogue.
An attractive set of featuresfor capturing possible semantically relevant worddependencies is obtained with Latent Dirichlet Al-location (LDA) (Blei et al., 2003), as described insection 2.Given a training set of conversations D, a hid-den topic space is derived and a conversation dis represented by its probability in each topic ofthe hidden space.
Estimation of these probabili-ties is affected by a variability inherent to the es-timation of the model parameters.
If many hiddenspaces are considered and features are computedfor each hidden space, it is possible to model theestimation variability together with the variabilityof the linguistic expression of a theme by differentspeakers in different real-life situations.
Even ifthe purpose of the application is theme identifica-tion and a training corpus annotated with themes isavailable, supervised LDA (Griffiths and Steyvers,2004) is not suitable for the proposed approach.LDA is used only for producing different featuresets used involved in statistical variability models.In order to estimate the parameters of differ-ent hidden spaces, a set of discriminative wordsV is constructed as described in (Morchid et al.,2014a).
Each theme t contains a set of specificwords.
Note that the same word may appear inseveral discriminative word sets.
All the selectedwords are then merged without repetition to formV .Several techniques, such as Variational Meth-ods (Blei et al., 2003), Expectation-propagation(Minka and Lafferty, 2002) or Gibbs Sam-pling (Griffiths and Steyvers, 2004), have beenproposed for estimating the parameters describ-ing a LDA hidden space.
Gibbs Sampling isa special case of Markov-chain Monte Carlo(MCMC) (Geman and Geman, 1984) and givesa simple algorithm for approximate inference inhigh-dimensional models such as LDA (Heinrich,2005).
This overcomes the difficulty to directlyand exactly estimate parameters that maximize thelikelihood of the whole data collection defined as:p(W |???
,??? )
=?w?Wp(?
?w |???
,??? )
for the wholedata collection W knowing the Dirichlet parame-ters???
and???
.Gibbs Sampling allows us both to estimate theLDA parameters in order to represent a new dia-logue d with the rthtopic space of size n, and toobtain a feature vector Vzrdof the topic representa-tion of d. The jthfeature Vzrjd= P (zrj|d) (where1 ?
j ?
n) is the probability of topic zrjto begenerated by the unseen dialogue d in the rthtopicspace of size n (see Figure 2) and Vwzrj= P (w|zrj)is the vector representation of a word into r.Agent: HelloCustomer: HelloAgent: Speaking ...Customer: I call you because Iwas fined today, but I still have animagine card suitable for zone 1[...] I forgot to use my navigo cardfor zone 2Agent: You did not use yournavigo card, that is why they giveyou a fine not for a zone issue [...]Customer: Thanks, byeAgent: byeAgentCustomerConversations agent/customercustomer care service of theParis transportation systemTOPIC 1P(w|z)           w0.03682338236708009   card0.026680126910873955 month0.026007114700509565 navigo0.01615229304874531   old0.015527353139121238 agency0.014229401019132776 euros0.013123738102105566 imagineTOPIC nP(w|z)           w0.06946564885496183   card0.04045801526717557  fine0.016793893129770993 transport0.01603053435114504   woman0.01450381679389313  fined0.013740458015267175 a?e0.012977099236641221 infraction...P(z |d) P(z |d)...1nAgent: HelloCustomer: HelloAgent: Speaking ...Customer: I call you because Iwas fined today, but I still have animagine card suitable for zone 1[...] I forgot to use my navigo cardfor zone 2Agent: You did not use yournavigo card, that is why they giveyou a fine not for a zone issue [...]Customer: Thanks, byeAgent: byeAgentCustomerConversations agent/customercustomer care service of theParis transportation systemTOPIC 1P(w|z)           w0.03682338236708009   card0.026680126910873955 month0.0260 7114700509565 navigo0.01615229304874531   old0.015527353139121238 agency0.014229401019132776 euros0.013123738102105566 imagineTOPIC nP(w|z)           w0.06946564885496183   card0.0404580152 717557  fine0.016793893129770993 transport0.01603053435114504   woman0.01450381679389313  fined0.013740458015267175 a?e0.012977099236641221 infraction...P(z |d) P(z |d)...1nFigure 2: Example of a dialogue d mapped into atopic space of size n.In the LDA technique, topic zj, j is drawnfrom a multinomial over ?
which is drawn froma Dirichlet distribution over???
.
Thus, a set ofp topic spaces are learned using LDA by varyingthe number of topics n to obtain p topic spaces ofsize n. The number of topics n varies from 10 to3, 010.
Thus, a set of 3, 000 topic spaces is esti-mated.
This is high enough to generate, for eachdialogue, many feature sets for estimating the pa-rameters of a variability model.The next process allows us to obtain a homo-geneous representation of transcription d for therthtopic space r. The feature vector Vzmdofd is mapped to the common vocabulary spaceV composed with a set of |V | discriminativewords (Morchid et al., 2014a) of size 166, to ob-tain a new feature vector Vwd,r= {P (w|d)r}w?V446of size |V | for the rthtopic space r of size nwherethe ith(0 ?
i ?
|V |) feature is:Vwid,r= P (wi|d)=n?j=1P (wi|zrj)P (zrj|d)=n?j=1Vwizrj?
Vzrjd=????Vwizr,??
?Vzrd?where ?
?, ??
is the inner product, ?
being the fre-quency of the term wiin d, Vwizrj= P (wi|zj) andVzrjd= P (zj|d) evaluated using Gibbs Samplingin the topic space r.4 Compact multi-view representationIn this section, an i-vector-based method torepresent automatic transcriptions is presented.Initially introduced for speaker recognition, i-vectors (Kenny et al., 2008) have become verypopular in the field of speech processing and re-cent publications show that they are also reli-able for language recognition (Mart?nez et al.,2011) and speaker diarization (Franco-Pedroso etal., 2010).
I-vectors are an elegant way of re-ducing the imput space dimensionality while re-taining most of the relevant information.
Thetechnique was originally inspired by the JointFactor Analysis framework (Kenny et al., 2007).Hence, i-vectors convey the speaker characteris-tics among other information such as transmissionchannel, acoustic environment or phonetic contentof speech segments.
The next sections describethe i-vector extraction process, the application ofthis compact representation to textual documents(called c-vector), and the vector transformationwith the EFR method and the Mahalanobis met-ric.4.1 Total variability space definitionI-vector extraction could be seen as a probabilisticcompression process that reduces the dimension-ality of speech super-vectors according to a linear-Gaussian model.
The speech (of a given speechrecording) super-vector msof concatenated GMMmeans is projected in a low dimensionality space,named Total Variability space, with:m(h,s)= m+ Tx(h,s), (1)where m is the mean super-vector of the UBM1.T is a low rank matrix (MD ?
R), where M isthe number of Gaussians in the UBM and D is thecepstral feature size, which represents a basis ofthe reduced total variability space.
T is named To-tal Variability matrix; the components of x(h,s)arethe total factors which represent the coordinates ofthe speech recording in the reduced total variabil-ity space called i-vector (i for identification).4.2 From i-vector speaker identification toc-vector textual document classificationThe proposed approach uses i-vectors to modeltranscription representation through each topicspace in a homogeneous vocabulary space.
Theseshort segments are considered as basic semantic-based representation units.
Indeed, vector Vwdrep-resents a segment or a session of a transcription d.In the following, (d, r) will indicate the dialoguerepresentation d in the topic space r. In our model,the segment super-vector m(d,r)of a transcriptiond knowing a topic space r is modeled:m(d,r)= m+ Tx(d,r)(2)where x(d,r)contains the coordinates of the topic-based representation of the dialogue in the re-duced total variability space called c-vector (c forclassification).Let N(d,r)and X(d,r)be two vectors containingthe zero order and first order dialogue statistics re-spectively.
The statistics are estimated against theUBM:Nr[g] =?t?r?g(t); {X(d,r)}[g]=?t?
(d,r)?g(t) ?
t(3)where ?g(t) is the a posteriori probability of Gaus-sian g for the observation t. In the equation,?t?
(d,r)represents the sum over all the frames be-longing to the dialogue d.Let X(d,r)be the state dependent statistics de-fined as follows:{X(d,r)}[g]={X(d,r)}[g]?m[g]??
(d,r)N(d,r)[g](4)Let L(d,r)be a R ?
R matrix, and B(d,r)a vector1The UBM is a GMM that represents all the possible ob-servations.447Algorithm 1: Estimation algorithm of T andlatent variable x.For each dialogue d mapped into the topicspace r: x(d,r)?
0, T?
random ;Estimate statistics: N(d,r), X(d,r)(eq.3);for i = 1 to nb iterations dofor all d and r doCenter statistics: X(d,r)(eq.4);Estimate L(d,r)and B(d,r)(eq.5);Estimate x(d,r)(eq.6);endEstimate matrix T (eq.
7 and 8) ;endof dimension R, both defined as:L(d,r)= I +?g?UBMN(d,r)[g] ?
{T}t[g]???1[g]?
{T}[g]B(d,r)=?g?UBM{T}t[g]???1g?
{X(d,r)}[g],(5)By using L(d,r)and B(d,r), x(d,r)can be obtainedusing the following equation:x(d,r)= L?1(d,r)?
B(d,r)(6)The matrix T can be estimated line by line, with{T}i[g]being the ithline of {T}[g]then:Ti[g]= LU?1g?
RUig, (7)where RUigand LUgare given by:LUg=?
(d,r)L?1(d,r)+ x(d,r)xt(d,r)?
N(d,r)[g]RUig=?(d,r){X(d,r)}[i][g]?
x(d,r)(8)Algorithm 1 presents the method adopted to es-timate the multi-view variability dialogue matrixwith the above developments where the standardlikelihood function can be used to assess the con-vergence.
One can refer to (Matrouf et al., 2007)to find out more about the implementation of thefactor analysis.C-vector representation suffers from 3 raised c-vector issues: (i) the c-vectors x of equation 2have to be theoretically distributed among the nor-mal distribution N (0, I), (ii) the ?radial?
effectshould be removed, and (iii) the full rank totalfactor space should be used to apply discriminanttransformations.
The next section presents a solu-tion to these 3 problems.4.3 C-vector standardizationA solution to standardize c-vectors has been de-veloped in (Bousquet et al., 2011).
The authorsproposed to apply transformations for training andtest transcription representations.
The first step isto evaluate the empirical mean x and covariancematrix V of the training c-vector.
Covariance ma-trix V is decomposed by diagonalization into:PDPT(9)where P is the eigenvector matrix of V and D is thediagonal version of V. A training i-vector x(d,r)istransformed in x?
(d,r)as follows:x?(d,r)=D?12PT(x(d,r)?
x)?(x(d,r)?
x)TV?1(x(d,r)?
x)(10)The numerator is equivalent by rotation toV?12(x(d,r)?
x) and the Euclidean norm of x?
(d,r)is equal to 1.
The same transformation is appliedto the test c-vectors, using the training set parame-ters x and mean covariance Vas estimations of thetest set of parameters.Figure 3 shows the transformation steps: Fig-ure 3-(a) is the original training set; Figure 3-(b) shows the rotation applied to the initial train-ing set around the principal axes of the total vari-ability when PTis applied; Figure 3-(c) showsthe standardization of c-vectors when D?12isapplied; and finally, Figure 3-(d) shows the c-vector x?
(d,r)on the surface area of the unit hyper-sphere after a length normalization by a divisionof?(x(d,r)?
x)TV?1(x(d,r)?
x).5 Experimental ProtocolThe proposed c-vector representation of automatictranscriptions is evaluated in the context of thetheme identification of a human-human telephoneconversation in the customer care service (CCS)of the RATP Paris transportation system.
The met-ric used to identify of the best theme is the Maha-lanobis metric.5.1 Theme identification taskThe DECODA project corpus (Bechet et al., 2012)was designed to perform experiments on the iden-tification of conversation themes.
It is composedof 1,514 telephone conversations, correspondingto about 74 hours of signal, split into a training448l llll llllllllllll llllll l lllllllllllll llll lllll l?3 ?2 ?1 0 1 2 3?2?1012a.Inital: Ml llllll lllllll ll l llllllllll ll lllllll l llll lll l?3 ?2 ?1 0 1 2 3?2?1012b.Rotation: Ptlllll ll lllllll ll l llllllllll ll lllllllllllll lll l?3 ?2 ?1 0 1 2 3?2?1012c.Standardization: D?1 2l l lllll ll ll lllll llll llll ll?3 ?2 ?1 0 1 2 3?2?1012d.Norm: (x ?
x)tV?1(x ?
x)Figure 3: Effect of the standardization with the EFR algorithm.set (740 dialogues), a development set (175 dia-logues) and a test set (327 dialogues), and manu-ally annotated with 8 conversation themes: prob-lems of itinerary, lost and found, time schedules,transportation cards, state of the traffic, fares, in-fractions and special offers.An LDA model allowed us to elaborate 3,000topics spaces by varying the number of topics from10 to 3,010.
A topic space having less than 10topics is not suitable for a corpus of more than 700dialogues (training set).
For each theme {Ci}8i=1,a set of 50 specific words is identified.
All theselected words are then merged without repetitionto compose V , which is made of 166 words.
Thetopic spaces are made with the LDA Mallet Javaimplementation2.The LIA-Speeral ASR system (Linar`es et al.,2007) is used for the experiments.
Acoustic modelparameters were estimated from 150 hours ofspeech in telephone conditions.
The vocabularycontains 5,782 words.
A 3-gram language model(LM) was obtained by adapting a basic LM withthe training set transcriptions.
A ?stop list?
of 126words3was used to remove unnecessary words(mainly function words), which results in a WordError Rate (WER) of 33.8% on the training, 45.2%on the development, and 49.5% on the test.
These2http://mallet.cs.umass.edu/3http://code.google.com/p/stop-words/high WER are mainly due to speech disfluenciesand to adverse acoustic environments (for exam-ple, calls from noisy streets with mobile phones).5.2 Mahalanobis metricGiven a new observation x, the goal of the task isto identify the theme belonging to x. Probabilisticapproaches ignore the process by which c-vectorswere extracted and they pretend instead they weregenerated by a prescribed generative model.
Oncea c-vector is obtained from a dialogue, its repre-sentation mechanism is ignored and it is regardedas an observation from a probabilistic generativemodel.
The Mahalanobis scoring metric assigns adialogue d with the most likely theme C. Givena training dataset of dialogues, let W denote thewithin dialogue covariance matrix defined by:W =K?k=1ntnWk=1nK?k=1nt?i=0(xki?
xk)(xki?
xk)t(11)where Wkis the covariance matrix of the kththeme Ck, ntis the number of utterances for thetheme Ck, n is the total number of dialogues, andxkis the centroid (mean) of all dialogues xkiofCk.449Each dialogue does not contribute to the co-variance in an equivalent way.
For this reason,the termntnis introduced in equation 11.
If ho-moscedasticity (equality of the class covariances)and Gaussian conditional density models are as-sumed, a new observation x from the test datasetcan be assigned to the most likely themeCkBayesus-ing the classifier based on the Bayes decision rule:CkBayes= argmaxk{N (x | xk,W)}= argmaxk{?12(x?
xk)tW?1(x?
xk) + ak}where W is the within theme covariance ma-trix defined in eq.
11; N denotes the normal dis-tribution and ak= log (P (Ck)).
It is noted that,with these assumptions, the Bayesian approach issimilar to Fisher?s geometric approach: x is as-signed to the class of the nearest centroid, accord-ing to the Mahalanobis metric (Xing et al., 2002)of W?1:CkBayes= argmaxk{?12||x?
xk||2W?1+ ak}6 Experiments and resultsThe proposed c-vector approach is applied tothe same classification task and corpus proposedin (Morchid et al., 2014a; Morchid et al., 2014b;Morchid et al., 2013) (state-of-the-art in text clas-sification in (Morchid et al., 2014a)).
Experimentsare conducted using the multiple topic spaces esti-mated with an LDA approach.
From these mul-tiple topic spaces, a classical way is to find theone that reaches the best performance.
Figure 4presents the theme classification performance ob-tained on the development and test sets using vari-ous topic-based representation configurations withthe EFR normalization algorithm (baseline).For sake of comparison, experiments are con-ducted using the automatic transcriptions only(ASR) only.
The conditions indicated by the ab-breviations between parentheses are consideredfor the development (Dev) and the test (Test) sets.Only homogenous conditions (ASR for bothtraining and validations sets) are considered in thisstudy.
Authors in (Morchid et al., 2014a) noticethat results collapse dramatically when heteroge-nous conditions are employed (TRS or TRS+ASRfor training set and ASR for validation set).First of all, we can see that this baseline ap-proach reached a classification accuracy of 83%and 76%, respectively on the development and thetest sets.
However, we note that the classifica-tion performance is rather unstable, and may com-pletely change from a topic space configuration toanother.
The gap between the lower and the higherclassification results is also important, with a dif-ference of 25 points on the development set (thesame trend is observed on the test set).
As a result,finding the best topic space size seems crucial forthis classification task, particularly in the contextof highly imperfect automatic dialogue transcrip-tions containing more than one theme.The topic space that yields the best accuracywith the baseline method (n = 15 topics) is pre-sented in Figure 5.
This figure presents each of the15 topics and their 10 most representative words(highest P (w|z)).
Several topics contain more orless the same representative words, such as topics3, 6 and 9.
This figure points out some interestingtopics that allow us to distinguish a theme from theothers.
For example:?
topics 2, 10 and 15 represent some words re-lated to itinerary problems,?
the transportation cards theme is mostly rep-resented in topic 4 and 15 (Imagine and Nav-igo are names of transportation cards),?
the words which represent the time schedulestheme are contained in topic 5,6,7 and less intopic 9,?
state of the traffic could be discussed withwords such as: departure, line, service, day.These words and others are contained in topic13,?
topics 4 and 12 are related to the infractionstheme with to words fine, pass, zone or ticket,?
but topic 12 could be related to theme faresor special offers as well .Table 1 presents results obtained with the pro-posed c-vector approach coupled with the EFR al-gorithm.
We can firstly note that this compact rep-resentation allows it to outperform the best topicspace configuration (baseline), with a gain of 9.4points on the development data and of 9 points onthe test data.
Moreover, if we consider the differ-ent c-vector configurations with the development45015 500 1500 2000 2497 30105060708090Max = 83.3Min = 58.6(a) Accuracy with the development setItems by varying the granularity 10 ?
n ?
3010Accuracy(%)15 500 825 1500 2000 30105060708090Max = 76.0Min = 56.8(b) Accuracy with the test setItems by varying the granularity 10 ?
n ?
3010Accuracy(%)Figure 4: Theme classification accuracies using various topic-based representations with EFR normal-ization (baseline) on the development and test sets (X-coordinates start at 10 indeed, but to show the bestconfiguration point (15), the origine (10) has been removed).Table 1: Theme classification accuracy (%) with different c-vectors and GMM-UBM sizes.DEV TESTc-vector Number of Gaussians in GMM-UBMsize 32 64 128 256 32 64 128 25660 88.8 86.5 91.2 90.6 85.0 82.6 83.5 84.7100 91.2 92.4 92.4 87.7 86.0 85.0 83.5 84.7120 89.5 92.2 89.5 87.7 85.0 83.5 85.4 84.1Table 2: Maximum (Max), minimum (Min) and Difference (Max ?Min) theme classification accu-racies (%) using the baseline and the proposed c-vector approaches.Max Min DifferenceMethod DEV TEST DEV TEST DEV TESTbaseline 83.3 76.0 58.6 56.8 14.7 20.8c-vector 92.4 85.0 86.5 82.6 5.9 2.4and test sets, the gap between accuracies is muchsmaller: classification accuracy does not go be-low 82.6%, while it reached 56% for the worsttopic-based configuration.
Indeed, as shown in Ta-ble 2, the difference between the maximum andthe minimum theme classification accuracies is of20% using the baseline approach while it is onlyof 2.4% using the c-vector method.We can conclude that this original c-vector ap-proach allows one to better handle the variabilities451w P(w|z)TOPIC 1linebagmetrolosthoursnamefoundthingobjectinstant0.0280.0270.0180.0170.0150.0130.0120.0120.0110.009w P(w|z)TOPIC 2busdirectionroadstopsixtyfivethreegohourstation0.0380.0270.0220.0210.0180.0170.0160.0130.0120.010w P(w|z)TOPIC 3bushourstwentyfourminutesontooldknowsundayline0.0240.0230.0190.0140.0140.0130.0100.0100.0100.008w P(w|z)TOPIC 4cardpassnavigomontheurogoagencymailfineaddress0.0400.0320.0240.0220.0210.0180.0160.0120.0100.009w P(w|z)TOPIC 5lineknowstationtraffichoursayleveltimetodayinstant0.0290.0270.0240.0210.0170.0150.0140.0140.0120.011w P(w|z)TOPIC 6bushourhundredlinetenoldmistermorninglostbag0.0300.0210.0200.0190.0180.0160.0150.0150.0140.012w P(w|z)TOPIC 7ticketsayoldbusissueneveralwaystimevalidatenormal0.0260.0170.0160.0150.0150.0140.0140.0130.0120.011w P(w|z)TOPIC 8saintplussayroadlevelstationtraincityfourfar0.0180.0170.0130.0130.0120.0110.0110.0100.0100.009w P(w|z)TOPIC 9hourfourtenbushundredmisszerolinefivesix0.0410.0390.0370.0360.0240.0240.0220.0200.0180.017w P(w|z)TOPIC 10stationsaintdirectionorlytakemadamemetrolinenorthbus0.0410.0360.0240.0200.0150.0150.0130.0120.0120.011w P(w|z)TOPIC 11madameserviceaddressmailmetroparisoldstoplacdock0.0280.0270.0220.0210.0200.0190.0180.0180.0160.015w P(w|z)TOPIC 12pariseurozoneticketfarecardbuystationnoisyweek0.0250.0250.0170.0150.0140.0140.0130.0130.0100.010w P(w|z)TOPIC 13serviceoldlinemadamemisteraskinternetdeparturedayclient0.0340.0270.0180.0170.0140.0140.0130.0130.0120.011w P(w|z)TOPIC 14busdirectionmetrolinestopmadamesaintoldroaddoor0.0400.0230.0220.0170.0160.0150.0150.0140.0140.014w P(w|z)TOPIC 15numberintegralcardagencyimaginesubscriptionnavigooldelevencall0.0400.0300.0240.0230.0180.0180.0170.0140.0130.012Figure 5: Topic space (15 topics) that obtains the best accuracy with the baseline system (see Fig.
4).contained in dialogue conversations: in a classi-fication context, better accuracy can be obtainedand the results can be more consistent when vary-ing the c-vector size and the number of Gaussians.7 ConclusionsThis paper presents an original multi-view repre-sentation of automatic speech dialogue transcrip-tions, and a fusion process with the use of a factoranalysis method called i-vector.
The first step ofthe proposed method is to represent a dialogue inmultiple topic spaces of different sizes (i.e.
num-ber of topics).
Then, a compact representationof the dialogue from the multiple views is pro-cessed to compensate the vocabulary and the vari-ability of the topic-based representations.
The ef-fectiveness of the proposed approach is evaluatedin a classification task of theme dialogue identifi-cation.
Thus, the architecture of the system iden-tifies conversation themes using the i-vector ap-proach.
This compact representation was initiallydeveloped for speaker recognition and we showedthat it can be successfully applied to a text clas-sification task.
Indeed, this solution allowed thesystem to obtain better classification accuracy thanwith the use of the classical best topic space con-figuration.
In fact, we highlighted that this originalcompact version of all topic-based representationsof dialogues, called c-vector in this work, coupledwith the EFR normalization algorithm, is a bettersolution to deal with dialogue variabilities (highword error rates, bad acoustic conditions, unusualword vocabulary, etc).
This promising compactrepresentation allows us to effectively solve boththe difficult choice of the right number of topicsand the multi-theme representation issue of partic-ular textual documents.
Finally, the classificationaccuracy reached 85% with a gain of 9 points com-pared to usual baseline (best topic space configu-ration).
In a future work, we plan to evaluate thisnew representation of textual documents in otherinformation retrieval tasks, such as keyword ex-traction or automatic summarization systems.AcknowledgementsWe thank the anonymous reviewers for their help-ful comments.
This work was funded by theSUMACC and ContNomina projects supported bythe French National Research Agency (ANR) un-der contracts ANR-10-CORD-007 and ANR-12-BS02-0009.452ReferencesR.
Arun, Venkatasubramaniyan Suresh, C.E.Veni Madhavan, and Musti Narasimha Murty.2010.
On finding the natural number of topicswith latent dirichlet allocation: Some observations.In Advances in Knowledge Discovery and DataMining, pages 391?402.
Springer.Frederic Bechet, Benjamin Maza, Nicolas Bigouroux,Thierry Bazillon, Marc El-Beze, Renato De Mori,and Eric Arbillot.
2012.
Decoda: a call-centre human-human spoken conversation corpus.LREC?12.Jerome R. Bellegarda.
1997.
A latent semantic analy-sis framework for large-span language modeling.
InFifth European Conference on Speech Communica-tion and Technology.Jerome R. Bellegarda.
2000.
Exploiting latent se-mantic information in statistical language modeling.Proceedings of the IEEE, 88(8):1279?1296.David M. Blei and John Lafferty.
2006.
Correlatedtopic models.
Advances in neural information pro-cessing systems, 18:147.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet allocation.
The Journal ofMachine Learning Research, 3:993?1022.Pierre-Michel Bousquet, Driss Matrouf, and Jean-Franc?ois Bonastre.
2011.
Intersession compensa-tion and scoring methods in the i-vectors space forspeaker recognition.
In Interspeech, pages 485?488.Juan Cao, Tian Xia, Jintao Li, Yongdong Zhang, andSheng Tang.
2009.
A density-based method foradaptive lda model selection.
Neurocomputing,72(7):1775?1781.Scott Deerwester, Susan T. Dumais, George W. Fur-nas, Thomas K. Landauer, and Richard Harshman.1990.
Indexing by latent semantic analysis.
Jour-nal of the American society for information science,41(6):391?407.Najim Dehak, Patrick J. Kenny, R?eda Dehak, PierreDumouchel, and Pierre Ouellet.
2011.
Front-endfactor analysis for speaker verification.
IEEE Trans-actions on Audio, Speech, and Language Process-ing, 19(4):788?798.Javier Franco-Pedroso, Ignacio Lopez-Moreno, Doro-teo T Toledano, and Joaquin Gonzalez-Rodriguez.2010.
Atvs-uam system description for the audiosegmentation and speaker diarization albayzin 2010evaluation.
In FALA VI Jornadas en Tecnologa delHabla and II Iberian SLTech Workshop, pages 415?418.Daniel Garcia-Romero and Carol Y Espy-Wilson.2011.
Analysis of i-vector length normalization inspeaker recognition systems.
In Interspeech, pages249?252.Stuart Geman and Donald Geman.
1984.
Stochas-tic relaxation, gibbs distributions, and the bayesianrestoration of images.
IEEE Transactions on PatternAnalysis and Machine Intelligence, (6):721?741.Thomas L. Griffiths and Mark Steyvers.
2004.
Find-ing scientific topics.
Proceedings of the Nationalacademy of Sciences of the United States of Amer-ica, 101(Suppl 1):5228?5235.Gregor Heinrich.
2005.
Parameter estimationfor text analysis.
Web: http://www.
arbylon.net/publications/text-est.
pdf.Thomas Hofmann.
1999.
Probabilistic latent semanticanalysis.
In Proc.
of Uncertainty in Artificial Intelli-gence, UAI ?
99, page 21.
Citeseer.Thomas Hofmann.
2001.
Unsupervised learningby probabilistic latent semantic analysis.
MachineLearning, 42(1):177?196.Patrick Kenny, Gilles Boulianne, Pierre Ouellet, andPierre Dumouchel.
2007.
Joint factor analysis ver-sus eigenchannels in speaker recognition.
IEEETransactions on Audio, Speech, and Language Pro-cessing, 15(4):1435?1447.Patrick Kenny, Pierre Ouellet, Najim Dehak, VishwaGupta, and Pierre Dumouchel.
2008.
A study of in-terspeaker variability in speaker verification.
IEEETransactions on Audio, Speech, and Language Pro-cessing, 16(5):980?988.Wei Li and Andrew McCallum.
2006.
Pachinko allo-cation: Dag-structured mixture models of topic cor-relations.Georges Linar`es, Pascal Noc?era, Dominique Massonie,and Driss Matrouf.
2007.
The lia speech recogni-tion system: from 10xrt to 1xrt.
In Text, Speech andDialogue, pages 302?308.
Springer.David Mart?nez, Oldrich Plchot, Luk?as Burget, On-drej Glembek, and Pavel Matejka.
2011.
Languagerecognition in ivectors space.
Interspeech, pages861?864.Driss Matrouf, Nicolas Scheffer, Benoit G.B.
Fauve,and Jean-Francois Bonastre.
2007.
A straightfor-ward and efficient implementation of the factor anal-ysis model for speaker verification.
In Interspeech,pages 1242?1245.Thomas Minka and John Lafferty.
2002.
Expectation-propagation for the generative aspect model.
InProceedings of the Eighteenth conference on Uncer-tainty in artificial intelligence, pages 352?359.
Mor-gan Kaufmann Publishers Inc.Mohamed Morchid, Georges Linar`es, Marc El-Beze,and Renato De Mori.
2013.
Theme identification intelephone service conversations using quaternions ofspeech features.
In Interspeech.
ISCA.453Mohamed Morchid, Richard Dufour, Pierre-MichelBousquet, Mohamed Bouallegue, Georges Linar`es,and Renato De Mori.
2014a.
Improving dialogueclassification using a topic space representation anda gaussian classifier based on the decision rule.
InICASSP.
IEEE.Mohamed Morchid, Richard Dufour, and GeorgesLinar`es.
2014b.
A LDA-Based Topic ClassificationApproach from Highly Imperfect Automatic Tran-scriptions.
In LREC.Douglas A. Reynolds and Richard C. Rose.
1995.Robust text-independent speaker identification usinggaussian mixture speaker models.
IEEE Transac-tions on Speech and Audio Processing, 3(1):72?83.Gerard Salton.
1989.
Automatic text processing: thetransformation.
Analysis and Retrieval of Informa-tion by Computer.Yoshimi Suzuki, Fumiyo Fukumoto, and YoshihiroSekiguchi.
1998.
Keyword extraction using term-domain interdependence for dictation of radio news.In 17th international conference on Computationallinguistics, volume 2, pages 1272?1276.
ACL.Yee Whye Teh, Michael I. Jordan, Matthew J. Beal,and David M. Blei.
2004.
Sharing clusters amongrelated groups: Hierarchical dirichlet processes.
InNIPS.Eric P. Xing, Michael I. Jordan, Stuart Russell, andAndrew Ng.
2002.
Distance metric learning withapplication to clustering with side-information.
InAdvances in neural information processing systems,pages 505?512.Elias Zavitsanos, Sergios Petridis, Georgios Paliouras,and George A. Vouros.
2008.
Determining auto-matically the size of learned ontologies.
In ECAI,volume 178, pages 775?776.454
