First Joint Conference on Lexical and Computational Semantics (*SEM), pages 591?596,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsUNIBA: Distributional Semantics for Textual SimilarityAnnalina Caputo Pierpaolo BasileDepartment of Computer ScienceUniversity of Bari ?Aldo Moro?Via E. Orabona, 4 - 70125 Bari, Italy{acaputo, basilepp, semeraro}@di.uniba.itGiovanni SemeraroAbstractWe report the results of UNIBA participationin the first SemEval-2012 Semantic TextualSimilarity task.
Our systems rely on distribu-tional models of words automatically inferredfrom a large corpus.
We exploit three differ-ent semantic word spaces: Random Indexing(RI), Latent Semantic Analysis (LSA) over RI,and vector permutations in RI.
Runs based onthese spaces consistently outperform the base-line on the proposed datasets.1 Background and Related ResearchSemEval-2012 Semantic Textual Similarity (STS)task (Agirre et al, 2012) aims at providing a gen-eral framework to ?examine the degree of semanticequivalence between two sentences.
?We propose an approach to Semantic TextualSimilarity based on distributional models of words,where the geometrical metaphor of meaning is ex-ploited.
Distributional models are grounded on thedistributional hypothesis (Harris, 1968), accordingto which the meaning of a word is determined bythe set of textual contexts in which it appears.
Thesemodels represent words as vectors in a high dimen-sional vector space.
Word vectors are built from alarge corpus in such a way that vector dimensionsreflect the different uses (or contexts) of a word inthe corpus.
Hence, the meaning of a word is de-fined by its use, and words used in similar contextsare represented by vectors near in the space.
In thisway, semantically related words like ?basketball?and ?volleyball?, which occur frequently in similarcontexts, say with words ?court, play, player?, willbe represented by near points.
Different definitionsof contexts give rise to different (semantic) spaces.A context can be a document, a sentence or a fixedwindow of surrounding words.
Contexts and wordscan be stored through a co-occurrence matrix, whosecolumns correspond to contexts, and rows to words.Therefore, the strength of the semantic associationbetween words can be computed as the cosine simi-larity of their vector representations.Latent Semantic Analysis (Deerwester et al,1990), BEAGLE (Jones and Mewhort, 2007),Random Indexing (Kanerva, 1988), HyperspaceAnalogue to Language (Burgess et al, 1998),WordSpace (Sch?tze and Pedersen, 1995) are alltechniques conceived to build up semantic spaces.However, all of them intend to represent semantics ata word scale.
Although vectors addition and multi-plication are two well defined operations suitable forcomposing words in semantic spaces, they miss tak-ing into account the underlying syntax, which regu-lates the compositionality of words.
Some efforts to-ward this direction are emerging (Clark and Pulman,2007; Clark et al, 2008; Mitchell and Lapata, 2010;Coecke et al, 2010; Basile et al, 2011; Clarke,2012), which resulted in theoretical work corrob-orated by empirical evaluation on how small frag-ments of text compose (e.g.
noun-noun, adjective-noun, and verb-noun pairs).2 MethodologyOur approach to STS is inspired by the latest devel-opments about semantic compositionality and distri-butional models.
The general methodology is basedon the construction of a semantic space endowed591with a vector addition operator.
The vector additionsums the word vectors of each pair of sentences in-volved in the evaluation.
The result consists of twovectors whose similarity can be computed by co-sine similarity.
However, this simple methodologytranslates a text into a mere bag-of-word representa-tion, depriving the text of its syntactic construction,which also influences the overall meaning of the sen-tence.
In order to deal with this limit, we experi-ment two classical methods for building a semanticspace, namely Random Indexing and Latent Seman-tic Analysis, along with a new method based on vec-tor permutations, which tries to encompass syntacticinformation directly into the resulting space.2.1 Random IndexingOur first method is based on Random Indexing (RI),introduced by Kanerva (Kanerva, 1988).
This tech-nique allows us to build a semantic space with noneed for (either term-document or term-term) ma-trix factorization, because vectors are inferred byusing an incremental strategy.
Moreover, it allowsus to solve efficiently the problem of reducing di-mensions, which is one of the key features used touncover the ?latent semantic dimensions?
of a worddistribution.RI1 (Widdows and Ferraro, 2008) is based onthe concept of Random Projection according towhich high dimensional vectors chosen randomlyare ?nearly orthogonal?.Formally, given an n ?
m matrix A and an m ?k matrix R made up of k m-dimensional randomvectors, we define a new n?
k matrix B as follows:Bn,k = An,m?Rm,k k << m (1)The new matrix B has the property to preserve thedistance between points scaled by a multiplicativefactor (Johnson and Lindenstrauss, 1984).Specifically, RI creates the semantic space Bn,kin two steps (we consider a fixed window w of termsas context):1.
A context vector is assigned to each term.
Thisvector is sparse, high-dimensional and ternary,which means that its elements can take values1An implementation of RI can be found at:http://code.google.com/p/semanticvectors/in {-1, 0, 1}.
A context vector contains a smallnumber of randomly distributed non-zero ele-ments, and the structure of this vector followsthe hypothesis behind the concept of RandomProjection;2.
Context vectors are accumulated by analyzingco-occurring terms in a window w. The seman-tic vector for a term is computed as the sum ofthe context vectors for terms which co-occur inw.2.2 Latent Semantic AnalysisLatent Semantic Analysis (Deerwester et al, 1990)relies on the Singular Value Decomposition (SVD)of a term-document co-occurrence matrix.
Givena matrix M, it can be decomposed in the productof three matrices U?V>, where U and V are theorthonormal matrices and ?
is the diagonal matrixof singular values of M placed in decreasing order.Computing the LSA on the co-occurrence matrix Mcan be a computationally expensive task, as a corpuscan contain thousands of terms.
Hence, we decidedto apply LSA to the reduced approximation gener-ated by RI.
It is important to point out that no trun-cation of singular values is performed.
Since com-puting the similarity between any two words is equalto taking the corresponding entry in the MM> ma-trix, we can exploit the relationMM> = U?V>V?>U> = U?
?>U> =(U?)(U?
)>Hence, the application of LSA to RI makes possibleto represent each word in the U?
space.A similar approach was investigated by Sellbergand J?nsson (2008) for retrieval of similar FAQs ina Question Answering system.
Authors showed thathalving the matrix dimension by applying the RI re-sulted in a drastic reduction of LSA computationtime.
Certainly there was also a performance priceto be paid, however general performance was bet-ter than VSM and RI respectively.
We also experi-mented LSA computed on RI versus LSA applied tothe original matrix during the tuning of our systems.Surprisingly, we found that LSA applied on the re-duced matrix gives better results than LSA.
How-ever, these results are not reported as they are notthe focus of this evaluation.5922.3 Vector Permutations in RIThe classical distributional models can handle onlyone definition of context at a time, such as the wholedocument or the window w. A method to add infor-mation about context in RI is proposed in (Sahlgrenet al, 2008).
The authors describe a strategy to en-code word order in RI by the permutation of coor-dinates in context vector.
When the coordinates areshuffled using a random permutation, the resultingvector is nearly orthogonal to the original one.
Thatoperation corresponds to the generation of a newrandom vector.
Moreover, by applying a predeter-mined mechanism to obtain random permutations,such as elements rotation, it is always possible toreconstruct the original vector using the reverse per-mutations.
By exploiting this strategy it is possibleto obtain different random vectors for each contextin which the term occurs.Our idea is to encode syntactic dependen-cies using vector permutations.
A syntacticdependency between two words is defined asdep(head, dependent), where dep is the syntac-tic link which connects the dependent word to thehead word.
Generally speaking, dependent is themodifier, object or complement, while head plays akey role in determining the behavior of the link.
Forexample, subj(eat, cat) means that ?cat?
is the sub-ject of ?eat?.
In that case the head word is ?eat?,which plays the role of verb.The key idea is to encode in the semantic space in-formation about syntactic dependencies which linkwords together.
Rather than representing the kindof dependency, our focus is to encompass informa-tion about the existence of such a relation betweenwords in the construction of the space.
The methodadopted to construct a semantic space that takes intoaccount both syntactic dependencies and RandomIndexing can be defined as follows:1. a context vector is assigned to each term, as de-scribed in Section 2.1 (Random Indexing);2. context vectors are accumulated by analyzingterms which are linked by a dependency.
Inparticular the semantic vector for each term tiis computed as the sum of the inverse-permutedcontext vectors for the terms tj which are de-pendents of ti, and the permuted vectors forthe terms tj which are heads of ti.
Moreover,the context vector of ti, and those of tj termswhich appears in a dependency relation withit, are sum to the final semantic vector in or-der to provide distributional evidence of co-occurrence.
Each permutation is computed asa forward/backward rotation of one element.
If?1 is a permutation of one element, the inverse-permutation is defined as ?
?1: the elementsrotation is performed by one left-shifting step.Formally, denoting with x the context vectorfor a term, we compute the semantic vector forthe term ti as follows:si = xi +?j?dep(ti,tj)(?
?1xj + xj)+?k?dep(tk,ti)(?1xk + xk)Adding permuted vectors to the head word andinverse-permuted vectors to the corresponding de-pendent word allows to encode the informationabout both heads and dependents into the space.This approach is similar to the one investigated by(Cohen et al, 2010) to encode relations betweenmedical terms.3 EvaluationDataset Description.
SemEval-2012 STS is a firstattempt to provide a ?unified framework for the eval-uation of modular semantic components.?
The taskconsists in computing the similarity between pairof texts, returning a similarity score.
Sentencesare extracted from five publicly available datasets:MSR (Paraphrase Microsoft Research ParaphraseCorpus, 750 pairs), MSR (Video Microsoft ResearchVideo Description Corpus, 750 pairs), SMTeuroparl(WMT2008 development dataset, Europarl section,459 pairs), SMTnews (news conversation sentencepairs from WMT, 399 pairs), and OnWN (pairs ofsentences from Ontonotes and WordNet definition,750 pairs).
Humans rated each pair with values from0 to 5.
The evaluation is performed by comparinghumans scores against systems performance throughPearson?s correlation.
The organizers propose threedifferent ways to aggregate values from the datasets:593ALL Rank-ALL ALLnrm Rank-ALLNrm Mean Rank-Meanbaseline .3110 87 .6732 85 .4356 70UNIBA-RI .6285 41 .7951 43 .5651 45UNIBA-LSARI .6221 44 .8079 30 .5728 40UNIBA-DEPRI .6141 46 .8027 38 .5891 31Table 1: Evaluation results of Pearson?s correlation.MSRpar MSRvid SMT-eur On-WN SMT-newsbaseline .4334 .2996 .4542 .5864 .3908UNIBA- RI .4128 .7612 .4531 .6306 .4887UNIBA- LSARI .3886 .7908 .4679 .6826 .4238UNIBA- DEPRI .4542 .7673 .5126 .6593 .4636Table 2: Evaluation results of Pearson?s correlation for individual datasets.ALL Pearson correlation with the gold standard forthe five datasets.ALLnrm Pearson correlation after the system out-puts for each dataset are fitted to the gold stan-dard using least squares.Mean Weighted mean across the five datasets,where the weight depends on the number ofpairs in the dataset.Experimental Setting.
For the evaluation, webuilt Distributional Spaces using the WaCkype-dia_EN corpus2.
WaCkypedia_EN is based on a2009 dump of the English Wikipedia (about 800 mil-lion tokens) and includes information about: part-of-speech, lemma and a full dependency parsing per-formed by MaltParser (Nivre et al, 2007).
The threespaces described in Section 2 are built exploitinginformation about term windows and dependencyparsing supplied by WaCkypedia.
The total numberof dependencies amounts to about 200 million.The RI system is implemented in Java and re-lies on some portions of code publicly available inthe Semantic Vectors package (Widdows and Fer-raro, 2008), while for LSA we exploited the publiclyavailable C library SVDLIBC3.We restricted the vocabulary to the 50,000 mostfrequent terms, with stop words removal and forc-ing the system to include terms which occur in thedataset.
Hence, the dimension of the original matrixwould have been 50,000?50,000.2http://wacky.sslmit.unibo.it/doku.php?id=corpora3http://tedlab.mit.edu/ dr/SVDLIBC/Our approach involves some parameters.
In par-ticular, each semantic space needs to set up the di-mension k of the space.
All spaces use a dimen-sion of 500 (resulting in a 50,000?500 matrix).
Thenumber of non-zero elements in the random vectoris set to 10.
When we apply LSA to the output spacegenerated by the Random Indexing we hold all the500 dimensions since during the tuning we observeda drop in performance when a lower dimension wasset.
The co-occurrence distance w between termswas set up to 4.In order to compute the similarity between thevector representations of sentences we used the co-sine similarity, and then we multiplied by 5 the ob-tained value.Results.
Table 1 shows the overall results obtainedexploiting the different semantic spaces.
We re-port the three proposed evaluation measures with thecorresponding overall ranks with respect to the 89runs submitted by participants.
We submitted threedifferent runs, each exploring a different semanticspace: UNIBA-RI (based on Random Indexing),UNIBA-LSARI (based on LSA performed over RIoutcome), and UNIBA-DEPRI (based on RandomIndexing and vector permutations).
Each proposedmeasure stresses different aspects.
ALL is the Pear-son?s correlation computed over the concatenateddataset.
As a consequence this measure ranks highersystems which obtain consistent better results.
Con-versely, ALLNrm normalizes results by scaling val-ues obtained from each dataset, in this way it triesto give emphasis to systems trained on each dataset.594The result of these different perspective is that ourthree spaces rank differently according to each mea-sure.
It seems that UNIBA-RI is able to work betteracross all datasets, while UNIBA-LSARI gives thebest results on specific datasets, even though all ourmethods are unsupervised and do not need trainingsteps.
A deeper analysis on each dataset is reportedon Table 2.
Here results seem to be at odds withTable 1.Considering individual datasets, UNIBA-RI givesonly once the best result, while UNIBA-LSARI andUNIBA-DEPRI are able to provide the best resultstwice.
Generally, all results outperform the base-line, based on a simple keyword overlap.
Lower re-sults are obtained in MSRpar, we ascribe this resultto the notably long sentences here involved.
In par-ticular, UNIBA-LSARI gives a result lower than thebaseline, and in line with the one obtained by LSAduring the tuning.
Hence, we ascribe this low per-formance to the application of LSA method to thisspecific dataset.
Only UNIBA-DEPRI was able tooutperform the baseline in this dataset.
This showsthe usefulness of encoding syntactic features in se-mantic word space where longer sentences are in-volved.
Generally, it is interesting to be noticed thatour spaces perform rather well on short and similarlystructured sentences, such as MSRvid and On-WN.4 ConclusionWe reported evaluation results of our participation inSemantic Textual Similarity task.
Our systems ex-ploit distributional models to represent the seman-tics of words.
Two of such spaces are based on aclassical definition of context, such as a fixed win-dow of surrounding words.
A third spaces tries toencompass more definitions of context at once, asthe syntactic structure that relates words in a cor-pus.
Although simple, our methods have achievedgenerally good results, outperforming the baselineprovided by the organizers.ReferencesEneko Agirre, Daniel Cer, Mona Diab, and AitorGonzalez-Agirre.
2012.
Semeval-2012 task 6: A piloton semantic textual similarity.
In Proceedings of the6th International Workshop on Semantic Evaluation(SemEval 2012), in conjunction with the First JointConference on Lexical and Computational Semantics(*SEM 2012).Pierpaolo Basile, Annalina Caputo, and Giovanni Semer-aro.
2011.
Encoding syntactic dependencies by vec-tor permutation.
In Proceedings of the EMNLP 2011Workshop on GEometrical Models of Natural Lan-guage Semantics, GEMS ?11, pages 43?51, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Curt Burgess, Kay Livesay, and Kevin Lund.
1998.
Ex-plorations in context space: Words, sentences, dis-course.
Discourse Processes, 25(2-3):211?257.Stephen Clark and Stephen Pulman.
2007.
Combiningsymbolic and distributional models of meaning.
InProceedings of the AAAI Spring Symposium on Quan-tum Interaction, pages 52?55.Stephen Clark, Bob Coecke, and Mehrnoosh Sadrzadeh.2008.
A compositional distributional model of mean-ing.
In Proceedings of the Second Quantum Interac-tion Symposium (QI-2008), pages 133?140.Daoud Clarke.
2012.
A context?theoretic framework forcompositionality in distributional semantics.
Compu-tational Linguistics, 38(1):41?71.Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark.2010.
Mathematical foundations for a composi-tional distributional model of meaning.
CoRR,abs/1003.4394.Trevor Cohen, Dominic Widdows, Roger W. Schvan-eveldt, and Thomas C. Rindflesch.
2010.
Logicalleaps and quantum connectives: Forging paths throughpredication space.
In AAAI-Fall 2010 Symposium onQuantum Informatics for Cognitive, Social, and Se-mantic Processes, pages 11?13.Scott Deerwester, Susan T. Dumais, George W. Furnas,Thomas K. Landauer, and Richard Harshman.
1990.Indexing by latent semantic analysis.
Journal of theAmerican Society for Information Science, 41(6):391?407.Zellig Harris.
1968.
Mathematical Structures of Lan-guage.
New York: Interscience.William B. Johnson and Joram Lindenstrauss.
1984.
Ex-tensions of Lipschitz mappings into a Hilbert space.Conference on Modern Analysis and Probability, Con-temporary Mathematics, 26:189?206.Michael N. Jones and Douglas J. K. Mewhort.
2007.Representing word meaning and order information ina composite holographic lexicon.
Psychological Re-view, 114(1):1?37.Pentti Kanerva.
1988.
Sparse Distributed Memory.
MITPress.Jeff Mitchell and Mirella Lapata.
2010.
Composition indistributional models of semantics.
Cognitive Science,34(8):1388?1429.595Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,G?lsen Eryigit, Sandra K?bler, Svetoslav Marinov,and Erwin Marsi.
2007.
Maltparser: A language-independent system for data-driven dependency pars-ing.
Natural Language Engineering, 13(02):95?135.Magnus Sahlgren, Anders Holst, and Pentti Kanerva.2008.
Permutations as a means to encode order inword space.
In V. Sloutsky, B.
Love, and K. Mcrae,editors, Proceedings of the 30th Annual Meeting ofthe Cognitive Science Society (CogSci?08), July 23-26,Washington D.C., USA, pages 1300?1305.
CognitiveScience Society, Austin, TX.Hinrich Sch?tze and Jan O. Pedersen.
1995.
Informa-tion retrieval based on word senses.
In Proceedings ofthe 4th Annual Symposium on Document Analysis andInformation Retrieval, pages 161?175.Linus Sellberg and Arne J?nsson.
2008.
Using randomindexing to improve singular value decompositionfor latent semantic analysis.
In Nicoletta Calzolari,Khalid Choukri, Bente Maegaard, Joseph Mariani, JanOdjik, Stelios Piperidis, and Daniel Tapias, editors,Proceedings of the Sixth International Conferenceon Language Resources and Evaluation (LREC2008),pages 2335?2338, Marrakech, Morocco.
EuropeanLanguage Resources Association (ELRA).Dominic Widdows and Kathleen Ferraro.
2008.
Se-mantic Vectors: A Scalable Open Source Packageand Online Technology Management Application.
InNicoletta Calzolari, Khalid Choukri, Bente Maegaard,Joseph Mariani, Jan Odjik, Stelios Piperidis, andDaniel Tapias, editors, Proceedings of the 6th Interna-tional Conference on Language Resources and Eval-uation (LREC2008), pages 1183?1190, Marrakech,Morocco.
European Language Resources Association(ELRA).596
