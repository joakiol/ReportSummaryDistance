Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 152?159,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsStatistical Machine Translation through Global Lexical Selection andSentence ReconstructionSrinivas Bangalore, Patrick Haffner, Stephan KanthakAT&T Labs - Research180 Park Ave, Florham Park, NJ 07932{srini,haffner,skanthak}@research.att.comAbstractMachine translation of a source languagesentence involves selecting appropriate tar-get language words and ordering the se-lected words to form a well-formed tar-get language sentence.
Most of the pre-vious work on statistical machine transla-tion relies on (local) associations of targetwords/phrases with source words/phrasesfor lexical selection.
In contrast, in this pa-per, we present a novel approach to lexicalselection where the target words are associ-ated with the entire source sentence (global)without the need to compute local associa-tions.
Further, we present a technique forreconstructing the target language sentencefrom the selected words.
We compare the re-sults of this approach against those obtainedfrom a finite-state based statistical machinetranslation system which relies on local lex-ical associations.1 IntroductionMachine translation can be viewed as consisting oftwo subproblems: (a) lexical selection, where appro-priate target language lexical items are chosen foreach source language lexical item and (b) lexical re-ordering, where the chosen target language lexicalitems are rearranged to produce a meaningful targetlanguage string.
Most of the previous work on statis-tical machine translation, as exemplified in (Brownet al, 1993), employs word-alignment algorithm(such as GIZA++ (Och and Ney, 2003)) that pro-vides local associations between source and targetwords.
The source-to-target word alignments aresometimes augmented with target-to-source wordalignments in order to improve precision.
Further,the word-level alignments are extended to phrase-level alignments in order to increase the extent oflocal associations.
The phrasal associations compilesome amount of (local) lexical reordering of the tar-get words ?
those permitted by the size of the phrase.Most of the state-of-the-art machine translation sys-tems use phrase-level associations in conjunctionwith a target language model to produce sentences.There is relatively little emphasis on (global) lexicalreordering other than the local reorderings permit-ted within the phrasal alignments.
A few exceptionsare the hierarchical (possibly syntax-based) trans-duction models (Wu, 1997; Alshawi et al, 1998;Yamada and Knight, 2001; Chiang, 2005) and thestring transduction models (Kanthak et al, 2005).In this paper, we present an alternate approach tolexical selection and lexical reordering.
For lexicalselection, in contrast to the local approaches of as-sociating target to source words, we associate tar-get words to the entire source sentence.
The intu-ition is that there may be lexico-syntactic features ofthe source sentence (not necessarily a single sourceword) that might trigger the presence of a targetword in the target sentence.
Furthermore, it might bedifficult to exactly associate a target word to a sourceword in many situations ?
(a) when the translationsare not exact but paraphrases (b) when the target lan-guage does not have one lexical item to express thesame concept that is expressed by a source word.Extending word to phrase alignments attempts to ad-dress some of these situations while alleviating thenoise in word-level alignments.As a consequence of this global lexical selectionapproach, we no longer have a tight association be-tween source and target language words.
The re-sult of lexical selection is simply a bag of words inthe target language and the sentence has to be recon-structed using this bag of words.
The words in thebag, however, might be enhanced with rich syntacticinformation that could aid in reconstructing the tar-get sentence.
This approach to lexical selection and152Translation modelWFSABilanguagePhrase SegmentedFSA to FSTBilanguageWFSTTransformationBilanguageReorderingLocal Phrase Joint LanguageModelingJoint LanguageAlignmentWordAlignmentSentence AlignedCorpusFigure 1: Training phases for our systemConstructionPermutationPermutation LatticeLexical ChoiceFST CompositionDecodingSourceSentence/WeightedLatticeTargetDecoding Lexical ReoderingCompositionFSA SentenceModelTranslation ModelLanguageTargetFigure 2: Decoding phases for our systemsentence reconstruction has the potential to circum-vent limitations of word-alignment based methodsfor translation between languages with significantlydifferent word order (e.g.
English-Japanese).In this paper, we present the details of traininga global lexical selection model using classifica-tion techniques and sentence reconstruction mod-els using permutation automata.
We also present astochastic finite-state transducer (SFST) as an exam-ple of an approach that relies on local associationsand use it to compare and contrast our approach.2 SFST Training and DecodingIn this section, we describe each of the componentsof our SFST system shown in Figure 1.
The SFSTapproach described here is similar to the one de-scribed in (Bangalore and Riccardi, 2000) which hassubsequently been adopted by (Banchs et al, 2005).2.1 Word AlignmentThe first stage in the process of training a lexical se-lection model is obtaining an alignment function (f )that given a pair of source (s1s2 .
.
.
sn) and target(t1t2 .
.
.
tm) language sentences, maps source lan-guage word subsequences into target language wordsubsequences, as shown below.
?i?j(f(si) = tj ?
f(si) = ?)
(1)For the work reported in this paper, we have usedthe GIZA++ tool (Och and Ney, 2003) which im-plements a string-alignment algorithm.
GIZA++alignment however is asymmetric in that the wordmappings are different depending on the directionof alignment ?
source-to-target or target-to-source.Hence in addition to the functions f as shown inEquation 1 we train another alignment function g :?j?i(g(tj) = si ?
g(tj) = ?)
(2)English: I need to make a collect callJapanese: ?H ????
?ff?k $*d ?^%ffcW2Alignment: 1 5 0 3 0 2 4Figure 3: Example bilingual texts with alignment in-formationI:?H need:?^%ffcW2 to:?
make:?ff?ka:?
collect ????
call $*dFigure 4: Bilanguage strings resulting from align-ments shown in Figure 3.2.2 Bilanguage RepresentationFrom the alignment information (see Figure 3), weconstruct a bilanguage representation of each sen-tence in the bilingual corpus.
The bilanguage stringconsists of source-target symbol pair sequences asshown in Equation 3.
Note that the tokens of a bilan-guage could be either ordered according to the wordorder of the source language or ordered according tothe word order of the target language.Bf = bf1 bf2 .
.
.
bfm (3)bfi = (si?1; si, f(si)) if f(si?1) = ?= (si, f(si?1); f(si)) if si?1 = ?= (si, f(si)) otherwiseFigure 4 shows an example alignment and thesource-word-ordered bilanguage strings correspond-ing to the alignment shown in Figure 3.We also construct a bilanguage using the align-ment function g similar to the bilanguage using thealignment function f as shown in Equation 3.Thus, the bilanguage corpus obtained by combin-ing the two alignment functions is B = Bf ?Bg.2.3 Bilingual Phrases and Local ReorderingWhile word-to-word translation only approximatesthe lexical selection process, phrase-to-phrase map-ping can greatly improve the translation of colloca-tions, recurrent strings, etc.
Using phrases also al-lows words within the phrase to be reordered into thecorrect target language order, thus partially solvingthe reordering problem.
Additionally, SFSTs cantake advantage of phrasal correlations to improve thecomputation of the probability P (WS ,WT ).The bilanguage representation could result insome source language phrases to be mapped to ?153(empty target phrase).
In addition to these phrases,we compute subsequences of a given length k on thebilanguage string and for each subsequence we re-order the target words of the subsequence to be inthe same order as they are in the target language sen-tence corresponding to that bilanguage string.
Thisresults in a retokenization of the bilanguage into to-kens of source-target phrase pairs.2.4 SFST ModelFrom the bilanguage corpus B, we train an n-gramlanguage model using standard tools (Goffin et al,2005).
The resulting language model is representedas a weighted finite-state automaton (S ?
T ?
[0, 1]).
The symbols on the arcs of this automaton(si ti) are interpreted as having the source and targetsymbols (si:ti), making it into a weighted finite-statetransducer (S ?
T?
[0, 1]) that provides a weightedstring-to-string transduction from S into T :T ?
= argmaxTP (si, ti|si?1, ti?1 .
.
.
si?n?1, ti?n?1)2.5 DecodingSince we represent the translation model as aweighted finite-state transducer (TransFST ), thedecoding process of translating a new source in-put (sentence or weighted lattice (Is)) amounts toa transducer composition (?)
and selection of thebest probability path (BestPath) resulting from thecomposition and projecting the target sequence (pi1).T ?
= pi1(BestPath(Is ?
TransFST )) (4)However, we have noticed that on the develop-ment corpus, the decoded target sentence is typicallyshorter than the intended target sentence.
This mis-match may be due to the incorrect estimation of theback-off events and their probabilities in the train-ing phase of the transducer.
In order to alleviatethis mismatch, we introduce a negative word inser-tion penalty model as a mechanism to produce morewords in the target sentence.2.6 Word Insertion ModelThe word insertion model is also encoded as aweighted finite-state automaton and is included inthe decoding sequence as shown in Equation 5.
Theword insertion FST has one state and |?T | numberof arcs each weighted with a ?
weight representingthe word insertion cost.
On composition as shownin Equation 5, the word insertion model penalizes orrewards paths which have more words depending onwhether ?
is positive or negative value.T ?
= pi1(BestPath(Is?TransFST?WIP )) (5)00001000101002 110021010311110311014 1111432Figure 5: Locally constraint permutation automatonfor a sentence with 4 words and window size of 2.2.7 Global ReorderingLocal reordering as described in Section 2.3 is re-stricted by the window size k and accounts only fordifferent word order within phrases.
As permutingnon-linear automata is too complex, we apply globalreordering by permuting the words of the best trans-lation and weighting the result by an n-gram lan-guage model (see also Figure 2):T ?
= BestPath(perm(T ?)
?
LMt) (6)Even the size of the minimal permutation automa-ton of a linear automaton grows exponentially withthe length of the input sequence.
While decoding bycomposition simply resembles the principle of mem-oization (i.e.
here: all state hypotheses of a wholesentence are kept in memory), it is necessary to ei-ther use heuristic forward pruning or constrain per-mutations to be within a local window of adjustablesize (also see (Kanthak et al, 2005)).
We have cho-sen to constrain permutations here.
Figure 5 showsthe resulting minimal permutation automaton for aninput sequence of 4 words and a window size of 2.Decoding ASR output in combination with globalreordering uses n-best lists or extracts them from lat-tices first.
Each entry of the n-best list is decodedseparately and the best target sentence is pickedfrom the union of the n intermediate results.3 Discriminant Models for LexicalSelectionThe approach from the previous section is a genera-tive model for statistical machine translation relyingon local associations between source and target sen-tences.
Now, we present our approach for a globallexical selection model based on discriminativelytrained classification techniques.
Discriminant mod-eling techniques have become the dominant methodfor resolving ambiguity in speech and other NLPtasks, outperforming generative models.
Discrimi-native training has been used mainly for translationmodel combination (Och and Ney, 2002) and withthe exception of (Wellington et al, 2006; Tillmannand Zhang, 2006), has not been used to directly trainparameters of a translation model.
We expect dis-criminatively trained global lexical selection models154to outperform generatively trained local lexical se-lection models as well as provide a framework forincorporating rich morpho-syntactic information.Statistical machine translation can be formulatedas a search for the best target sequence that maxi-mizes P (T |S), where S is the source sentence andT is the target sentence.
Ideally, P (T |S) shouldbe estimated directly to maximize the conditionallikelihood on the training data (discriminant model).However, T corresponds to a sequence with a ex-ponentially large combination of possible labels,and traditional classification approaches cannot beused directly.
Although Conditional Random Fields(CRF) (Lafferty et al, 2001) train an exponentialmodel at the sequence level, in translation tasks suchas ours the computational requirements of trainingsuch models are prohibitively expensive.We investigate two approaches to approximatingthe string level global classification problem, usingdifferent independence assumptions.
A comparisonof the two approaches is summarized in Table 1.3.1 Sequential Lexical Choice ModelIn the first approach, we formulate a sequential lo-cal classification problem as shown in Equations 7.This approach is similar to the SFST approach inthat it relies on local associations between the sourceand target words(phrases).
We can use a conditionalmodel (instead of a joint model as before) and theparameters are determined using discriminant train-ing which allows for richer conditioning context.P (T |S) =?Ni=1 P (ti|?
(S, i)) (7)where ?
(S, i) is a set of features extracted from thesource string S (shortened as ?
in the rest of thesection).3.2 Bag-of-Words Lexical Choice ModelThe sequential lexical choice model described inthe previous section treats the selection of a lexicalchoice for a source word in the local lexical contextas a classification task.
The data for training suchmodels is derived from word alignments obtainedby e.g.
GIZA++.
The decoded target lexical itemshave to be further reordered, but for closely relatedlanguages the reordering could be incorporated intocorrectly ordered target phrases as discussed previ-ously.For pairs of languages with radically differentword order (e.g.
English-Japanese), there needs tobe a global reordering of words similar to the casein the SFST-based translation system.
Also, for suchdiffering language pairs, the alignment algorithmssuch as GIZA++ perform poorly.These observations prompted us to formulate thelexical choice problem without the need for wordalignment information.
We require a sentencealigned corpus as before, but we treat the target sen-tence as a bag-of-words or BOW assigned to thesource sentence.
The goal is, given a source sen-tence, to estimate the probability that we find a givenword in the target sentence.
This is why, instead ofproducing a target sentence, what we initially obtainis a target bag of words.
Each word in the target vo-cabulary is detected independently, so we have herea very simple use of binary static classifiers.
Train-ing sentence pairs are considered as positive exam-ples when the word appears in the target, and neg-ative otherwise.
Thus, the number of training ex-amples equals the number of sentence pairs, in con-trast to the sequential lexical choice model whichhas one training example for each token in the bilin-gual training corpus.
The classifier is trained with n-gram features (BOgrams(S)) from the source sen-tence.
During decoding the words with conditionalprobability greater than a threshold ?
are consideredas the result of lexical choice decoding.BOW ?T = {t|P (t|BOgrams(S)) > ?}
(8)For reconstructing the proper order of words inthe target sentence we consider all permutations ofwords in BOW ?T and weight them by a target lan-guage model.
This step is similar to the one de-scribed in Section 2.7.
The BOW approach can alsobe modified to allow for length adjustments of tar-get sentences, if we add optional deletions in the fi-nal step of permutation decoding.
The parameter ?and an additional word deletion penalty can then beused to adjust the length of translated outputs.
InSection 6, we discuss several issues regarding thismodel.4 Choosing the classifierThis section addresses the choice of the classifi-cation technique, and argues that one techniquethat yields excellent performance while scaling wellis binary maximum entropy (Maxent) with L1-regularization.4.1 Multiclass vs. Binary ClassificationThe Sequential and BOW models represent two dif-ferent classification problems.
In the sequentialmodel, we have a multiclass problem where eachclass ti is exclusive, therefore, all the classifier out-puts P (ti|?)
must be jointly optimized such that155Table 1: A comparison of the sequential and bag-of-words lexical choice modelsSequential Lexical Model Bag-of-Words Lexical ModelOutput target Target word for each source position i Target word given a source sentenceInput features BOgram(S, i?
d, i+ d) : bag of n-grams BOgram(S, 0, |S|): bag of n-gramsin source sentence in the interval [i?
d, i+ d] in source sentenceProbabilities P (ti|BOgram(S, i?
d, i+ d)) P (BOW (T )|BOgram(S, 0, |S|))Independence assumption between the labelsNumber of classes One per target word or phraseTraining samples One per source token One per sentencePreprocessing Source/Target word alignment Source/Target sentence alignment?i P (ti|?)
= 1.
This can be problematic: withone classifier per word in the vocabulary, even allo-cating the memory during training may exceed thememory capacity of current computers.In the BOW model, each class can be detectedindependently, and two different classes can be de-tected at the same time.
This is known as the 1-vs-other scheme.
The key advantage over the multiclassscheme is that not all classifiers have to reside inmemory at the same time during training which al-lows for parallelization.
Fortunately for the sequen-tial model, we can decompose a multiclass classifi-cation problem into separate 1-vs-other problems.
Intheory, one has to make an additional independenceassumption and the problem statement becomes dif-ferent.
Each output label t is projected into a bitstring with components bj(t) where probability ofeach component is estimated independently:P (bj(t)|?)
= 1?
P (b?j(t)|?)
= 11 + e?(?j??j?)?
?In practice, despite the approximation, the 1-vs-other scheme has been shown to perform as well asthe multiclass scheme (Rifkin and Klautau, 2004).As a consequence, we use the same type of binaryclassifier for the sequential and the BOW models.The excellent results recently obtained with theSEARN algorithm (Daume et al, 2007) also sug-gest that binary classifiers, when properly trainedand combined, seem to be capable of matching morecomplex structured output approaches.4.2 Geometric vs. Probabilistic InterpretationWe separate the most popular classification tech-niques into two broad categories:?
Geometric approaches maximize the width ofa separation margin between the classes.
Themost popular method is the Support Vector Ma-chine (SVM) (Vapnik, 1998).?
Probabilistic approaches maximize the con-ditional likelihood of the output class giventhe input features.
This logistic regression isalso called Maxent as it finds the distributionwith maximum entropy that properly estimatesthe average of each feature over the trainingdata (Berger et al, 1996).In previous studies, we found that the best accuracyis achieved with non-linear (or kernel) SVMs, at theexpense of a high test time complexity, which is un-acceptable for machine translation.
Linear SVMsand regularized Maxent yield similar performance.In theory, Maxent training, which scales linearlywith the number of examples, is faster than SVMtraining, which scales quadratically with the num-ber of examples.
In our first experiments with lexi-cal choice models, we observed that Maxent slightlyoutperformed SVMs.
Using a single threshold withSVMs, some classes of words were over-detected.This suggests that, as theory predicts, SVMs do notproperly approximate the posterior probability.
Wetherefore chose to use Maxent as the best probabilityapproximator.4.3 L1 vs. L2 regularizationTraditionally, Maxent is regularized by imposing aGaussian prior on each weight: this L2 regulariza-tion finds the solution with the smallest possibleweights.
However, on tasks like machine translationwith a very large number of input features, a Lapla-cian L1 regularization that also attempts to maxi-mize the number of zero weights is highly desirable.A new L1-regularized Maxent algorithms wasproposed for density estimation (Dudik et al, 2004)and we adapted it to classification.
We found this al-gorithm to converge faster than the current state-of-the-art in Maxent training, which is L2-regularizedL-BFGS (Malouf, 2002)1.
Moreover, the number oftrained parameters is considerably smaller.5 Data and ExperimentsWe have performed experiments on the IWSLT06Chinese-English training and development sets from1We used the implementation available athttp://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html156Table 2: Statistics of training and development data from 2005/2006 (?
= first of multiple translations only).Training (2005) Dev 2005 Dev 2006Chinese English Chinese English Chinese EnglishSentences 46,311 506 489Running Words 351,060 376,615 3,826 3,897 5,214 6,362?Vocabulary 11,178 11,232 931 898 1,136 1,134?Singletons 4,348 4,866 600 538 619 574?OOVs [%] - - 0.6 0.3 0.9 1.0ASR WER [%] - - - - 25.2 -Perplexity - - 33 - 86 -# References - - 16 72005 and 2006.
The data are traveler task ex-pressions such as seeking directions, expressions inrestaurants and travel reservations.
Table 2 presentssome statistics on the data sets.
It must be notedthat while the 2005 development set matches thetraining data closely, the 2006 development set hasbeen collected separately and shows slightly differ-ent statistics for average sentence length, vocabularysize and out-of-vocabulary words.
Also the 2006development set contains no punctuation marks inChinese, but the corresponding English translationshave punctuation marks.
We also evaluated ourmodels on the Chinese speech recognition outputand we report results using 1-best with a word er-ror rate of 25.2%.For the experiments, we tokenized the Chinesesentences into character strings and trained the mod-els discussed in the previous sections.
Also, wetrained a punctuation prediction model using Max-ent framework on the Chinese character strings inorder to insert punctuation marks into the 2006 de-velopment data set.
The resulting character stringwith punctuation marks is used as input to the trans-lation decoder.
For the 2005 development set, punc-tuation insertion was not needed since the Chinesesentences already had the true punctuation marks.In Table 3 we present the results of the three dif-ferent translation models ?
FST, Sequential Maxentand BOW Maxent.
There are a few interesting ob-servations that can be made based on these results.First, on the 2005 development set, the sequentialMaxent model outperforms the FST model, eventhough the two models were trained starting fromthe same GIZA++ alignment.
The difference, how-ever, is due to the fact that Maxent models can copewith increased lexical context2 and the parametersof the model are discriminatively trained.
The moresurprising result is that the BOW Maxent model sig-nificantly outperforms the sequential Maxent model.2We use 6 words to the left and right of a source word forsequential Maxent, but only 2 preceding source and target wordsfor FST approach.The reason is that the sequential Maxent model re-lies on the word alignment, which, if erroneous, re-sults in incorrect predictions by the sequential Max-ent model.
The BOW model does not rely on theword-level alignment and can be interpreted as a dis-criminatively trained model of dictionary lookup fora target word in the context of a source sentence.Table 3: Results (mBLEU) scores for the three dif-ferent models on the transcriptions for developmentset 2005 and 2006 and ASR 1-best for developmentset 2006.Dev 2005 Dev 2006Text Text ASR 1-bestFST 51.8 19.5 16.5Seq.
Maxent 53.5 19.4 16.3BOW Maxent 59.9 19.3 16.6As indicated in the data release document, the2006 development set was collected differently com-pared to the one from 2005.
Due to this mis-match, the performance of the Maxent models arenot very different from the FST model, indicatingthe lack of good generalization across different gen-res.
However, we believe that the Maxent frame-work allows for incorporation of linguistic featuresthat could potentially help in generalization acrossgenres.
For translation of ASR 1-best, we see a sys-tematic degradation of about 3% in mBLEU scorecompared to translating the transcription.In order to compensate for the mismatch betweenthe 2005 and 2006 data sets, we computed a 10-foldaverage mBLEU score by including 90% of the 2006development set into the training set and using 10%of the 2006 development set for testing, each time.The average mBLEU score across these 10 runs in-creased to 22.8.In Figure 6 we show the improvement of mBLEUscores with the increase in permutation window size.We had to limit to a permutation window size of 10due to memory limitations, even though the curvehas not plateaued.
We anticipate using pruning tech-niques we can increase the window size further.1570.460.480.50.520.540.560.580.66  6.5  7  7.5  8  8.5  9  9.5  10Permutation Window SizeFigure 6: Improvement in mBLEU score with theincrease in size of the permutation window5.1 United Nations and Hansard CorporaIn order to test the scalability of the global lexicalselection approach, we also performed lexical se-lection experiments on the United Nations (Arabic-English) corpus and the Hansard (French-English)corpus using the SFST model and the BOW Maxentmodel.
We used 1,000,000 training sentence pairsand tested on 994 test sentences for the UN corpus.For the Hansard corpus we used the same trainingand test split as in (Zens and Ney, 2004): 1.4 milliontraining sentence pairs and 5432 test sentences.
Thevocabulary sizes for the two corpora are mentionedin Table 4.
Also in Table 4, are the results in terms ofF-measure between the words in the reference sen-tence and the decoded sentences.
We can see that theBOW model outperforms the SFST model on bothcorpora significantly.
This is due to a systematic10% relative improvement for open class words, asthey benefit from a much wider context.
BOW per-formance on close class words is higher for the UNcorpus but lower for the Hansard corpus.Table 4: Lexical Selection results (F-measure) onthe Arabic-English UN Corpus and the French-English Hansard Corpus.
In parenthesis are F-measures for open and closed class lexical items.Corpus Vocabulary SFST BOWSource TargetUN 252,571 53,005 64.6 69.5(60.5/69.1) (66.2/72.6)Hansard 100,270 78,333 57.4 60.8(50.6/67.7) (56.5/63.4)6 DiscussionThe BOW approach is promising as it performs rea-sonably well despite considerable losses in the trans-fer of information between source and target lan-guage.
The first and most obvious loss is about wordposition.
The only information we currently use torestore the target word position is the target languagemodel.
Information about the grammatical role of aword in the source sentence is completely lost.
Thelanguage model might fortuitously recover this in-formation if the sentence with the correct grammat-ical role for the word happens to be the maximumlikelihood sentence in the permutation automaton.We are currently working toward incorporatingsyntactic information on the target words so as to beable to recover some of the grammatical role infor-mation lost in the classification process.
In prelimi-nary experiments, we have associated the target lex-ical items with supertag information (Bangalore andJoshi, 1999).
Supertags are labels that provide linearordering constraints as well as grammatical relationinformation.
Although associating supertags to tar-get words increases the class set for the classifier, wehave noticed that the degradation in the F-score ison the order of 3% across different corpora.
The su-pertag information can then be exploited in the sen-tence construction process.
The use of supertags inphrase-based SMT system has been shown to im-prove results (Hassan et al, 2006).A less obvious loss is the number of times a wordor concept appears in the target sentence.
Func-tion words like ?the?
and ?of?
can appear manytimes in an English sentence.
In the model dis-cussed in this paper, we index each occurrence of thefunction word with a counter.
In order to improvethis method, we are currently exploring a techniquewhere the function words serve as attributes (e.g.definiteness, tense, case) on the contentful lexicalitems, thus enriching the lexical item with morpho-syntactic information.A third issue concerning the BOW model is theproblem of synonyms ?
target words which translatethe same source word.
Suppose that in the trainingdata, target words t1 and t2 are, with equal probabil-ity, translations of the same source word.
Then, inthe presence of this source word, the probability todetect the corresponding target word, which we as-sume is 0.8, will be, because of discriminant learn-ing, split equally between t1 and t2, that is 0.4 and0.4.
Because of this synonym problem, the BOWthreshold ?
has to be set lower than 0.5, which isobserved experimentally.
However, if we set thethreshold to 0.3, both t1 and t2 will be detected inthe target sentence, and we found this to be a majorsource of undesirable insertions.The BOW approach is different from the pars-ing based approaches (Melamed, 2004; Zhang andGildea, 2005; Cowan et al, 2006) where the transla-tion model tightly couples the syntactic and lexicalitems of the two languages.
The decoupling of the158two steps in our model has the potential for gener-ating paraphrased sentences not necessarily isomor-phic to the structure of the source sentence.7 ConclusionsWe view machine translation as consisting of lexi-cal selection and lexical reordering steps.
These twosteps need not necessarily be sequential and could betightly integrated.
We have presented the weightedfinite-state transducer model of machine translationwhere lexical choice and a limited amount of lexicalreordering are tightly integrated into a single trans-duction.
We have also presented a novel approachto translation where these two steps are loosely cou-pled and the parameters of the lexical choice modelare discriminatively trained using a maximum en-tropy model.
The lexical reordering model in thisapproach is achieved using a permutation automa-ton.
We have evaluated these two approaches on the2005 and 2006 IWSLT development sets and shownthat the techniques scale well to Hansard and UNcorpora.ReferencesH.
Alshawi, S. Bangalore, and S. Douglas.
1998.
Automaticacquisition of hierarchical transduction models for machinetranslation.
In ACL, Montreal, Canada.R.E.
Banchs, J.M.
Crego, A. Gispert, P. Lambert, and J.B.Marino.
2005.
Statistical machine translation of euparl databy using bilingual n-grams.
In Workshop on Building andUsing Parallel Texts.
ACL.S.
Bangalore and A. K. Joshi.
1999.
Supertagging: An ap-proach to almost parsing.
Computational Linguistics, 25(2).S.
Bangalore and G. Riccardi.
2000.
Stochastic finite-statemodels for spoken language machine translation.
In Pro-ceedings of the Workshop on Embedded Machine Transla-tion Systems, pages 52?59.A.L.
Berger, Stephen A. D. Pietra, D. Pietra, and J. Vincent.1996.
A Maximum Entropy Approach to Natural LanguageProcessing.
Computational Linguistics, 22(1):39?71.P.
Brown, S.D.
Pietra, V.D.
Pietra, and R. Mercer.
1993.
TheMathematics of Machine Translation: Parameter Estimation.Computational Linguistics, 16(2):263?312.D.
Chiang.
2005.
A hierarchical phrase-based model for statis-tical machine translation.
In Proceedings of the ACL Con-ference, Ann Arbor, MI.B.
Cowan, I. Kucerova, and M. Collins.
2006.
A discrimi-native model for tree-to-tree translation.
In Proceedings ofEMNLP.H.
Daume, J. Langford, and D. Marcu.
2007.
Search-basedstructure prediction.
submitted to Machine Learning Jour-nal.M.
Dudik, S. Phillips, and R.E.
Schapire.
2004.
Perfor-mance Guarantees for Regularized Maximum Entropy Den-sity Estimation.
In Proceedings of COLT?04, Banff, Canada.Springer Verlag.V.
Goffin, C. Allauzen, E. Bocchieri, D. Hakkani-Tur, A. Ljolje,S.
Parthasarathy, M. Rahim, G. Riccardi, and M. Saraclar.2005.
The AT&T WATSON Speech Recognizer.
In Pro-ceedings of ICASSP, Philadelphia, PA.H.
Hassan, M. Hearne, K. Sima?an, and A.
Way.
2006.
Syntac-tic phrase-based statistical machine translation.
In Proceed-ings of IEEE/ACL first International Workshop on SpokenLanguage Technology (SLT), Aruba, December.S.
Kanthak, D. Vilar, E. Matusov, R. Zens, and H. Ney.
2005.Novel reordering approaches in phrase-based statistical ma-chine translation.
In Proceedings of the ACL Workshop onBuilding and Using Parallel Texts, pages 167?174, Ann Ar-bor, Michigan.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Conditionalrandom fields: Probabilistic models for segmenting and la-beling sequence data.
In Proceedings of ICML, San Fran-cisco, CA.R.
Malouf.
2002.
A comparison of algorithms for maximumentropy parameter estimation.
In Proceedings of CoNLL-2002, pages 49?55.
Taipei, Taiwan.I.
D. Melamed.
2004.
Statistical machine translation by pars-ing.
In Proceedings of ACL.F.
J. Och and H. Ney.
2002.
Discriminative training and max-imum entropy models for statistical machine translation.
InProceedings of ACL.F.J.
Och and H. Ney.
2003.
A systematic comparison of vari-ous statistical alignment models.
Computational Linguistics,29(1):19?51.Ryan Rifkin and Aldebaro Klautau.
2004.
In defense of one-vs-all classification.
Journal of Machine Learning Research,pages 101?141.C.
Tillmann and T. Zhang.
2006.
A discriminative global train-ing algorithm for statistical mt.
In COLING-ACL.V.N.
Vapnik.
1998.
Statistical Learning Theory.
John Wiley &Sons.B.
Wellington, J. Turian, C. Pike, and D. Melamed.
2006.
Scal-able purely-discriminative training for word and tree trans-ducers.
In AMTA.D.
Wu.
1997.
Stochastic Inversion Transduction Grammarsand Bilingual Parsing of Parallel Corpora.
ComputationalLinguistics, 23(3):377?404.K.
Yamada and K. Knight.
2001.
A syntax-based statisticaltranslation model.
In Proceedings of 39th ACL.R.
Zens and H. Ney.
2004.
Improvements in phrase-based sta-tistical machine translation.
In Proceedings of HLT-NAACL,pages 257?264, Boston, MA.H.
Zhang and D. Gildea.
2005.
Stochastic lexicalized inver-sion transduction grammar for alignment.
In Proceedings ofACL.159
