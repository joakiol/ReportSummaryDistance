Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 76?80,Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational LinguisticsMining Transliterations from Wikipedia using Pair HMMsPeter NabendeAlfa-Informatica, University of GroningenThe Netherlandsp.nabende@rug.nlAbstractThis paper describes the use of a pairHidden Markov Model (pair HMM) sys-tem in mining transliteration pairs fromnoisy Wikipedia data.
A pair HMM vari-ant that uses nine transition parameters,and emission parameters associated withsingle character mappings between sourceand target language alphabets is identifiedand used in estimating transliteration sim-ilarity.
The system resulted in a precisionof 78% and recall of 83% when evaluatedon a random selection of English-RussianWikipedia topics.1 IntroductionThe transliteration mining task as defined in theNEWS 2010 White paper (Kumaran et al, 2010)required identifying single word transliterationpairs from a set of candidate transliteration pairs.In the case of Wikipedia data, we have a collectionof corresponding source and target language topicsthat can be used for extracting candidate translit-erations.
We apply a pair HMM edit-distancebased method to obtain transliteration similarityestimates.
The similarity estimates for a given setof source and target language words are then com-pared with the aim of identifying potential translit-eration pairs.
Generally, the pair HMM methoduses the notion of transforming a source stringto a target string through a series of edit opera-tions.
The three edit operations that we considerfor use in transliteration similarity estimation in-clude: substitution, insertion, and deletion.
Theseedit operations are represented as hidden states ofa pair HMM.
Depending on the source and targetlanguage alphabets, it is possible to design or use aspecific pair HMM algorithm for estimating pairedcharacter emission parameters in the edit opera-tion states, and transition parameters for a givendesign of transitions between the pair HMM?sstates.
Before applying the pair HMM method, weuse external datasets to identify a pair HMM vari-ant that we consider as suitable for application totransliteration similarity estimation.
We then usethe shared task datasets to train the selected pairHMM variant, and finally apply an algorithm thatis specific to the trained pair HMM for comput-ing transliteration similarity estimates.
In section2, we discuss transliteration similarity estimationwith regard to applying the pair HMM method;section 3 describes the experimental setup and re-sults; section 4 concludes the paper with pointersto future work.2 Transliteration Similarity Estimationusing Pair HMMsTo describe the transliteration similarity estima-tion process, consider examples of correspondingEnglish (as source language) and Russian (as tar-get language) Wikipedia topics as shown in Table1.
Across languages, Wikipedia topics are writtenin different ways and all words in a topic could beimportant for mining transliterations.
One mainstep in the transliteration mining task is to identifya set of words in each topic for consideration ascandidate transliterations.
As seen in Table 1, it isvery likely that some words will not be selected asid English topic Russian topic1 Johnston Atoll ????????
(?????
)2 Oleksandr ???????
?, ????????
?Palyanytya ?????????
?3 Ministers for ?????????:???????
?Foreign Affairs of ???????????
??
?Luxembourg ???????????...
...Table 1: Example of corresponding English Rus-sian Wikipedia topics76candidate transliterations depending on the criteriafor selection.
For example, if a criterion is suchthat we consider only words starting with upper-case characters for English and Russian datasets,then the Russian word ???????
in the topic pair 1in Table 1 will not be used as a candidate translit-eration and that in turn makes the system loosethe likely pair of ?Atoll, ??????.
After extractingcandidate transliterations, the approach we use inthis paper takes each candidate word on the sourcelanguage side and determines a transliteration es-timate with each candidate word on the target lan-guage side.
Consider the example for topic id 1 inTable 1 where we expect to have ?Johnston?
and?Atoll?
as candidate source language translitera-tions, and ??????????
and ???????
as candidatetarget language transliterations.
The method usedis expected to compare ?Johnston?
against ?????-?????
and ??????
?, and then compare ?Atoll?
tothe Russian candidate transliterations.
We expectthe output to be ?Johston, ?????????
and ?Atoll,??????
as the most likely single word transliter-ations from topic pair 1 after sorting out all thefour transliteration similarity estimates in this par-ticular case.
We employ the pair HMM approachto estimate transliteration similarity for candidatesource-target language words.A pair HMM has an emission state or states thatgenerate two observation sequences instead of oneobservation sequence as is the case in standardHMMs.
Pair HMMs originate from work in Bi-ological sequence analysis (Durbin et al, 1998;Rivas and Eddy, 2001) from which variants werecreated and successfully applied in cognate identi-fication (Mackay and Kondrak, 2005), Dutch di-alect comparison (Wieling et al, 2007), translit-eration identification (Nabende et al, 2010),and transliteration generation (Nabende, 2009).As mentioned earlier, we have first, tested twopair HMM variants on manually verified English-Russian datasets which we obtain from the previ-ous shared task on machine transliteration (NEWS2009) (Kumaran and Kellner, 2007).
This pre-liminary test is aimed at determining the effect ofpair HMM parameter changes on the quality of thetransliteration similarity estimates.
For the firstpair HMM variant, no transitions are modeled be-tween edit states; we only use transtion parame-ters associated with transiting from a start state toeach of the edit operation states, and from eachof the edit operation states to an end state.
TheIMEnd1-?
D-?
I-?
M1-?
D - ?D -?
D1 - ?
I-?
I - ?
I?
I  ?
D?
I?
D?
M?
D ?
I?
D ?
IDFigure 1: Pair HMM with nine distinct transi-tion parameters.
Emission parameters are speci-fied with emitting states and their size is dependenton the characters used in the source and target lan-guagessecond pair HMM variant uses nine distinct tran-sition parameters between the pair HMM?s statesas shown in Figure 1.
The node M in Figure 1 rep-resents the substitution state in which emission pa-rameters encode relationships between each of thesource and target language characters.
D denotesthe deletion state where emission parameters spec-ify relationships between source language charac-ters and a target language gap.
I denotes the inser-tion state where emission parameters encode rela-tionships between target language characters anda source language gap.
Starting parameters for thepair HMM in Figure 1 are assoicated with transit-ing from the M state to one of the edit operationstates including transiting back to M.The pair HMM parameters are estimated usingthe well-known Baum-Welch Expectation Maxi-mization (EM) algorithm (Baum et al, 1970).For each pair HMM variant, the training algorithmstarts with a uniform distribution for substitution,deletion, insertion, and transition parameters, anditerates through the data until a local maximum.A method referred to as stratified ten fold crossvalidation (Olson and Delen, 2008) is used to eval-uate the two pair HMM variants.
In each fold,7056 pairs of English-Russian names from the pre-vious shared task on machine transliteration (Ku-77Pair HMM Model CVA CVMRRphmm00edtransViterbi 0.788 0.809Forward 0.927 0.954phmm09edtransViterbi 0.943 0.952Forward 0.987 0.991Table 2: CVA and CVMRR results two pair HMMvariants on a preliminary transliteration identifica-tio experiment.
phmm00edtrans is the pair HMMvariant with no transition parameters between theedit states while phmm09edtrans is the pair HMMvariant with nine distinct transition parameters.maran and Kellner, 2007) are used for training and784 name pairs for testing.
The Cross Valida-tion Accuracy (CVA) and Cross Validation MeanReciprocal Rank (CVMRR) results obtained fromapplying the Forward and Viterbi algorithms of thetwo pair HMM variants on this particular datasetare shown in Table 2.The CVA and CVMRR values in Table 2 sug-gest that it is necessary to model for transition pa-rameters when using pair HMMs for translitera-tion similarity estimation.
Table 2 also suggeststhat it is better to use the Forward algorithm for agiven pair HMM variant.
Based on the results inTable 2, the pair HMM variant illustrated in Figure1 is chosen for application in estimating transliter-ation similarity for the mining task.3 Experimental setup and ResultsTo simplify the analysis of the source and tar-get strings, the pair HMM system requires uniquewhole number representations for each characterin the source and target language data.
This is notsuitable for all the different types of writing sys-tems.
In this paper, we look at only the Englishand Russian languages where many characters areassociated with a phonemic alphabet and wherenumbered representations are hardly expected tocontribute to errors from loss of information in-herent in the original orthography.
A preliminaryrun on Chinese-English1 datasets from the previ-ous shared task on machine transliteration (NEWS2009) resulted in an accuracy of 0.213 and MRRof 0.327 using the pair HMM variant in Figure1.
In the following subsection we discuss somedata preprocessing steps on the English-Russian1In this case Chinese is the source language while Englishis the target languageWikipedia dataset.3.1 English and Russian candidatetransliteration extractionThe English-Russian Wikipedia dataset that wasprovided for the transliteration mining task is verynoisy meaning that it has various types of other en-tities in addition to words for each language?s or-thography.
A first step in simplifying the translit-eration mining process was to remove any unnec-essary entities.We observed the overlap of writing systems inboth the English and Russian Wikipedia datasets.We therefore made sure that there is no topicwhere the same writing system is used in both theEnglish and Russian data.
Any strings that containcharacters that are not associated with the writ-ing systems for English and Russian were also re-moved.We also observed the presence of many tempo-ral and numerical expressions that are not neces-sary on both the English and Russian Wikipediadatasets.
We applied different sets of rules to re-move such expressions while leaving any neces-sary words.Using knowledge about the initial formattingof strings in both the English and Russian data,a set of rules was applied to split most of thestrings based on different characters.
For ex-ample almost all strings in the English side hadthe underscore ?
?
character as a string separa-tor.
We also removed characters such as: colons,semi-colons, commas, question marks, exclama-tion marks, dashes, hyphens, forward and backslashes, mathematical operator symbols, currencysymbols, etc.
Some strings were also split basedon string patterns, for example where differentwords are joined into one string and it was easyto identify that the uppercase character for eachword still remained in the combined string just likewhen it is alone.
We also removed many abbrevia-tions and titles in the datasets that were not neces-sary for analysis during the transliteration miningprocess.After selecting candidate words based on mostof the criteria above, we determine all charactersin our extracted candidate transliteration data andcompare against those in the shared task?s seeddata (Kumaran et al, 2010) with the aim of find-ing all characters that are missing in the seed data.Matching transliteration pairs with the the miss-78ing characters are then hand picked from the can-didate words dataset and added to the seed databefore training the pair HMM variant that is se-lected from the previous section.
The process foridentifying missing characters and words that havethem is carried out seperately for each language.However, a matching word in the other languageis identified to constitute a transliteration pair thatcan be added to the seed dataset.
For the English-Russian dataset, we use 142 transliteration pairs inaddition to the 1000 transliteration pairs in the ini-tial seed data.
We hence apply the Baum-Welchalgorithm for the selected pair HMM specificationfrom section 2 on a total of 1142 transliterationpairs.
The algorithm performed 182 iterations be-fore converging for this particular dataset.3.2 ResultsTo obtain transliteration similarity measures, weapply the Forward algorithm of the trained pairHMM from section 3.1 to all the remainingWikipedia topics.
For each word in an Englishtopic, the algorithm computes transliteration simi-larity estimates for all words in the Russian topic.After observing transliteration similarity estimatesfor a subset of candidate transliteration words, wespecify a single threshold value (th) and use itfor identifying potential transliteration pairs.
Athreshold value of 1 ?
10?13 was chosen afterobserving that many of the pairs that had a sim-ilarity estimate above this threshold were indeedtransliteration pairs.
Therefore, a pair of wordswas taken as a potential transliteration pair onlywhen its transliteration estimate (tr sim) was suchthat tr sim > th.
This resulted in a total of299389 potential English-Russian transliterationpairs.
This collection of potential transliterationpairs has been evaluated using a random set of cor-responding English and Russian Wikipedia topicsas specified in the NEWS 2010 White paper forthe transliteration mining task (Kumaran et al,2010).
Table 3 shows the precision, recall, andf-score results2 that were obtained after applyingthe Forward algorithm for the pair HMM of Fig-ure 1.Despite using the pair HMM method with itsbasic probabilistic one-to-one mapping for each2The numbers in Table 3 were obtained from a post eval-uation after correcting a number of processing errors in thepair HMM transliteration mining system.
The errors initiallyled to relatively lower values associated with the measures inthis Table.
The values in this Table are therefore not part ofthe initial shared task resultsModel precision recall f-scorephmm09edtrans 0.780 0.834 0.806Table 3: Evaluation results for the Pair HMM ofFigure 1 on a random selection of 1000 corre-sponding English Russian Wikipedia topics.of the source target character representations, theresult in Table 3 suggests a promising applica-tion of pair HMMs in mining transliterations fromWikipedia.4 Conclusions and Future WorkWe have described the application of Pair HMMsto mining transliterations from Wikipedia.
Thetransliteration mining evaluation results suggesta valuable application of Pair HMMs to miningtransliterations.
Currently, the pair HMM systemis considered to be best applicable to languageswhose writing system mostly uses a phonemic al-phabet.
Although an experimental test run wasdone for Chinese-English data, a conclusion aboutthe general applicability of the pair HMM neces-sitates additional tests using other language pairssuch as Hindi and Tamil which were also part ofthe shared task.As future work, we would like to investigatethe performance of Pair HMMs on additional writ-ing systems.
This may require additional modi-fications to a pair HMM system to minimize oninput formatting errors for other types of writ-ing systems.
It is also necessary to determine thetransliteration mining performance of pair HMMswhen more tolerant criteria are used on the noisyWikipedia data.
Currently, the pair HMM is ap-plied in its most basic form, that is, no complexmodifications have been implemented for examplemodeling for context in source and target languagewords, and other factors that may affect the qualityof a transliteration similarity estimate; it should beinteresting to investigate perfromance of complexpair HMM variants in transliteration mining.AcknowledgmentsResearch in this paper is funded through a secondNPT (Uganda) Project.ReferencesA Kumaran, Mitesh Khapra, and Haizhou Li.
2010.Whitepaper on NEWS 2010 Shared Task on79Transliteration Mining.A Kumaran and Tobias Kellner.
2007.
A GenericFramework for Machine Transliteration.
Proceed-ings of the 30th Annual International ACM SIGIRConference on Research and Development in Infor-mation Retrieval (SIGIR 2007), pp 721?722, Ams-terdam, The Netherlands.Leonard E. Baum, Ted Petrie, George Soules, and Nor-man Weiss.
1970.
A Maximization Technique Oc-curring in the Statistical Analysis of ProbabilisticFunctions of Markov Chains.
Annals of Mathemati-cal Statistics, 41(1):164?171.David L. Olson and Dursun Delen.
2008.
AdvancedData Mining Techniques.
Springer.Elena Rivas and Sean R. Eddy.
2001.
Noncoding RNAGene Detection using Comparative Sequence Anal-ysis.
BMC Bioinformatics 2001, 2:8.Martijn Wieling, Therese Leinonen, and John Ner-bonne.
2007.
Inducing Sound Segment Differencesusing Pair Hidden Markov Models.
In John Ner-bonne, Mark Ellison, and Grzegorz Kondrak (eds.
)Computing Historical Phonology: 9th Meeting ofthe ACL Special Interest Group for ComputationalMorphology and Phonology Workshop, pp 48?56,Prague, Czech Republic.Peter Nabende.
2009.
Transliteration System usingPair HMMs with Weighted FSTs.
Proceedings ofthe Named Entities Workshop, NEWS?09, pp 100?103, Suntec, Singapore.Peter Nabende, Jorg Tiedemann, and John Nerbonne.2010.
Pair Hidden Markov Model for Named EntityMatching.
In Tarek Sobh (ed.)
Innovations and Ad-vances in Computer Sciences and Engineering, pp497?502, Springer, Heidelberg.Richard Durbin, Sean R. Eddy, Anders Krogh, GraemeMitchison.
1998.
Biological Sequence Analysis:Probabilistic Models of Proteins and Nucleic Acids.Cambridge University Press, Cambridge, UK.Wesley Mackay and Grzegorz Kondrak.
2005.
Com-puting Word Similarity and Identifying Cognateswith Pair Hidden Markov Models.
Proceedingsof the ninth Conference on Computational NaturalLanguage Learning (CoNLL 2005), pp 40?47, AnnArbor, Michigan.80
