Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 69?72,Sydney, July 2006. c?2006 Association for Computational LinguisticsNLTK: The Natural Language ToolkitSteven BirdDepartment of Computer Science and Software EngineeringUniversity of Melbourne, Victoria 3010, AUSTRALIALinguistic Data Consortium, University of Pennsylvania,Philadelphia PA 19104-2653, USAAbstractThe Natural Language Toolkit is a suite ofprogram modules, data sets and tutorialssupporting research and teaching in com-putational linguistics and natural languageprocessing.
NLTK is written in Pythonand distributed under the GPL open sourcelicense.
Over the past year the toolkit hasbeen rewritten, simplifying many linguis-tic data structures and taking advantageof recent enhancements in the Python lan-guage.
This paper reports on the simpli-fied toolkit and explains how it is used inteaching NLP.1 IntroductionNLTK, the Natural Language Toolkit, is a suiteof Python modules providing many NLP datatypes, processing tasks, corpus samples andreaders, together with animated algorithms,tutorials, and problem sets (Loper and Bird,2002).
Data types include tokens, tags, chunks,trees, and feature structures.
Interface definitionsand reference implementations are provided fortokenizers, stemmers, taggers (regexp, ngram,Brill), chunkers, parsers (recursive-descent,shift-reduce, chart, probabilistic), clusterers, andclassifiers.
Corpus samples and readers include:Brown Corpus, CoNLL-2000 Chunking Corpus,CMU Pronunciation Dictionary, NIST IEERCorpus, PP Attachment Corpus, Penn Treebank,and the SIL Shoebox corpus format.NLTK is ideally suited to students who arelearning NLP or conducting research in NLPor closely related areas.
NLTK has been usedsuccessfully as a teaching tool, as an individualstudy tool, and as a platform for prototyping andbuilding research systems (Liddy and McCracken,2005; S?tre et al, 2005).We chose Python for its shallow learning curve,transparent syntax, and good string-handling.Python permits exploration via its interactiveinterpreter.
As an object-oriented language,Python permits data and code to be encapsulatedand re-used easily.
Python comes with anextensive library, including tools for graphicalprogramming and numerical processing (Beasley,2006).Over the past four years the toolkit grew rapidlyand the data structures became significantly morecomplex.
Each new processing task added newrequirements on input and output representations.It was not clear how to generalize tasks so theycould be applied independently of each other.As a simple example, consider the independenttasks of tagging and stemming, which both oper-ate on sequences of tokens.
If stemming is donefirst, we lose information required for tagging.
Iftagging is done first, the stemming must be ableto skip over the tags.
If both are done indepen-dently, we need to be able to align the results.As task combinations multiply, managing the databecomes extremely difficult.To address this problem, NLTK 1.4 introduceda blackboard architecture for tokens, unifyingmany data types, and permitting distinct tasksto be run independently.
Unfortunately thisarchitecture also came with a significant overheadfor programmers, who were often forced to use?rather awkward code structures?
(Hearst, 2005).It was clear that the re-engineering done in NLTK1.4 unduly complicated the programmer?s task.This paper presents a brief overview and tutorialon a new, simplified toolkit, and describes how itis used in teaching.692 Simple Processing Tasks2.1 Tokenization and StemmingThe following three-line program imports thetokenize package, defines a text string, andtokenizes the string on whitespace to create a listof tokens.
(NB.
?>>>?
is Python?s interactiveprompt; ?...?
is the continuation prompt.
)>>> text = ?This is a test.
?>>> list(tokenize.whitespace(text))[?This?, ?is?, ?a?, ?test.?
]Several other tokenizers are provided.
We canstem the output of tokenization using the PorterStemmer as follows:>>> text = ?stemming is exciting?>>> tokens = tokenize.whitespace(text)>>> porter = stem.Porter()>>> for token in tokens:... print porter.stem(token),stem is excitThe corpora included with NLTK come withcorpus readers that understand the file structureof the corpus, and load the data into Python datastructures.
For example, the following code readspart a of the Brown Corpus.
It prints a list oftuples, where each tuple consists of a word andits tag.>>> for sent in brown.tagged(?a?):...
print sent[(?The?, ?at?
), (?Fulton?, ?np-tl?
),(?County?, ?nn-tl?
), (?Grand?, ?jj-tl?
),(?Jury?, ?nn-tl?
), (?said?, ?vbd?
), ...]NLTK provides support for conditionalfrequency distributions, making it easy to countup items of interest in specified contexts.
Suchinformation may be useful for studies in stylisticsor in text categorization.2.2 TaggingThe simplest possible tagger assigns the same tagto each token:>>> my_tagger = tag.Default(?nn?
)>>> list(my_tagger.tag(tokens))[(?John?, ?nn?
), (?saw?, ?nn?
),(?3?, ?nn?
), (?polar?, ?nn?
),(?bears?, ?nn?
), (?.
?, ?nn?
)]On its own, this will tag only 10?20% of thetokens correctly.
However, it is a reasonable tag-ger to use as a default if a more advanced taggerfails to determine a token?s tag.The regular expression tagger assigns a tag to atoken according to a series of string patterns.
Forinstance, the following tagger assigns cd to cardi-nal numbers, nns to words ending in the letter s,and nn to everything else:>>> patterns = [...
(r?\d+(.\d+)?$?, ?cd?),...
(r?\.
*s$?, ?nns?),...
(r?.
*?, ?nn?
)]>>> simple_tagger = tag.Regexp(patterns)>>> list(simple_tagger.tag(tokens))[(?John?, ?nn?
), (?saw?, ?nn?
),(?3?, ?cd?
), (?polar?, ?nn?
),(?bears?, ?nns?
), (?.
?, ?nn?
)]The tag.Unigram class implements a sim-ple statistical tagging algorithm: for each token,it assigns the tag that is most likely for that token.For example, it will assign the tag jj to any occur-rence of the word frequent, since frequent is usedas an adjective (e.g.
a frequent word) more oftenthan it is used as a verb (e.g.
I frequent this cafe).Before a unigram tagger can be used, it must betrained on a corpus, as shown below for the firstsection of the Brown Corpus.>>> unigram_tagger = tag.Unigram()>>> unigram_tagger.train(brown(?a?
))Once a unigram tagger has been trained, it canbe used to tag new text.
Note that it assignsthe default tag None to any token that was notencountered during training.>>> text = "John saw the books on the table">>> tokens = list(tokenize.whitespace(text))>>> list(unigram_tagger.tag(tokens))[(?John?, ?np?
), (?saw?, ?vbd?
),(?the?, ?at?
), (?books?, None),(?on?, ?in?
), (?the?, ?at?
),(?table?, None)]We can instruct the unigram tagger to back offto our default simple_tagger when it cannotassign a tag itself.
Now all the words are guaran-teed to be tagged:>>> unigram_tagger =... tag.Unigram(backoff=simple_tagger)>>> unigram_tagger.train(train_sents)>>> list(unigram_tagger.tag(tokens))[(?John?, ?np?
), (?saw?, ?vbd?
),(?the?, ?at?
), (?books?, ?nns?
),(?on?, ?in?
), (?the?, ?at?
),(?table?, ?nn?
)]We can go on to define and train a bigram tagger,as shown below:>>> bigram_tagger =\... tag.Bigram(backoff=unigram_tagger)>>> bigram_tagger.train(brown.tagged(?a?
))We can easily evaluate this tagger againstsome gold-standard tagged text, using thetag.accuracy() function.NLTK also includes a Brill tagger (contributedby Christopher Maloof) and an HMM tagger (con-tributed by Trevor Cohn).703 Chunking and ParsingChunking is a technique for shallow syntacticanalysis of (tagged) text.
Chunk data can beloaded from files that use the common bracket orIOB notations.
We can define a regular-expressionbased chunk parser for use in chunking taggedtext.
NLTK also supports simple cascading ofchunk parsers.
Corpus readers for chunked datain Penn Treebank and CoNLL-2000 are provided,along with comprehensive support for evaluationand error analysis.NLTK provides several parsers for context-freephrase-structure grammars.
Grammars can bedefined using a series of productions as follows:>>> grammar = cfg.parse_grammar(???...
S -> NP VP... VP -> V NP | V NP PP... V -> "saw" | "ate"... NP -> "John" | Det N | Det N PP... Det -> "a" | "an" | "the" | "my"... N -> "dog" | "cat" | "ball"... PP -> P NP... P -> "on" | "by" | "with"...
???
)Now we can tokenize and parse a sentence witha recursive descent parser.
Note that we avoidedleft-recursive productions in the above grammar,so that this parser does not get into an infinite loop.>>> text = "John saw a cat with my ball">>> sent = list(tokenize.whitespace(text))>>> rd = parse.RecursiveDescent(grammar)Now we apply it to our sentence, and iterateover all the parses that it generates.
Observethat two parses are possible, due to prepositionalphrase attachment ambiguity.>>> for p in rd.get_parse_list(sent):... print p(S:(NP: ?John?
)(VP:(V: ?saw?
)(NP:(Det: ?a?
)(N: ?cat?
)(PP: (P: ?with?
)(NP: (Det: ?my?)
(N: ?ball?
))))))(S:(NP: ?John?
)(VP:(V: ?saw?
)(NP: (Det: ?a?)
(N: ?cat?
))(PP: (P: ?with?
)(NP: (Det: ?my?)
(N: ?ball?
)))))The same sentence can be parsed using a grammarwith left-recursive productions, so long as weuse a chart parser.
We can invoke NLTK?s chartparser with a bottom-up rule-invocation strategywith chart.ChartParse(grammar,chart.BU STRATEGY).
Tracing can be turnedon in order to display each step of the process.NLTK also supports probabilistic context freegrammars, and provides a Viterbi-style PCFGparser, together with a suite of bottom-upprobabilistic chart parsers.4 Teaching with NLTKNatural language processing is often taught withinthe confines of a single-semester course, eitherat advanced undergraduate level or at postgradu-ate level.
Unfortunately, it turns out to be ratherdifficult to cover both the theoretical and practi-cal sides of the subject in such a short span oftime.
Some courses focus on theory to the exclu-sion of practical exercises, and deprive students ofthe challenge and excitement of writing programsto automatically process natural language.
Othercourses are simply designed to teach programmingfor linguists, and do not manage to cover any sig-nificant NLP content.
NLTK was developed toaddress this problem, making it feasible to covera substantial amount of theory and practice withina single-semester course.A significant fraction of any NLP course ismade up of fundamental data structures andalgorithms.
These are usually taught with thehelp of formal notations and complex diagrams.Large trees and charts are copied onto the boardand edited in tedious slow motion, or laboriouslyprepared for presentation slides.
A moreeffective method is to use live demonstrationsin which those diagrams are generated andupdated automatically.
NLTK provides interactivegraphical user interfaces, making it possibleto view program state and to study programexecution step-by-step (e.g.
see Figure 1).Most NLTK components have a demonstrationmode, and will perform an interesting task withoutrequiring any special input from the user.
It is evenpossible to make minor modifications to programsin response to ?what if?
questions.
In this way,students learn the mechanics of NLP quickly,gain deeper insights into the data structures andalgorithms, and acquire new problem-solvingskills.
Since these demonstrations are distributedwith the toolkit, students can experiment on theirown with the algorithms that they have seenpresented in class.71Figure 1: Two Parser Demonstrations: Shift-Reduce and Recursive Descent ParsersNLTK can be used to create student assign-ments of varying difficulty and scope.
In the sim-plest assignments, students experiment with one ofthe existing modules.
Once students become morefamiliar with the toolkit, they can be asked to makeminor changes or extensions to an existing module(e.g.
build a left-corner parser by modifying therecursive descent parser).
A bigger challenge is todevelop one or more new modules and integratethem with existing modules to perform a sophis-ticated NLP task.
Here, NLTK provides a usefulstarting point with its existing components and itsextensive tutorials and API documentation.NLTK is a unique framework for teaching nat-ural language processing.
NLTK provides com-prehensive support for a first course in NLP whichtightly couples theory and practice.
Its extensivedocumentation maximizes the potential for inde-pendent learning.
For more information, includingdocumentation, download pointers, and links todozens of courses that have adopted NLTK, pleasesee: http://nltk.sourceforge.net/ .AcknowledgementsI am grateful to Edward Loper, co-developer ofNLTK, and to dozens of people who have con-tributed code and provided helpful feedback.ReferencesMarti Hearst.
2005.
Teaching applied natural languageprocessing: Triumphs and tribulations.
In Proc 2ndACL Workshop on Effective Tools and Methodolo-gies for Teaching NLP and CL, pages 1?8, ACLElizabeth Liddy and Nancy McCracken.
2005.
Hands-on NLP for an interdisciplinary audience.
In Proc2nd ACL Workshop on Effective Tools and Method-ologies for Teaching NLP and CL, pages 62?68,ACLEdward Loper and Steven Bird.
2002.
NLTK: TheNatural Language Toolkit.
In Proc ACL Workshopon Effective Tools and Methodologies for TeachingNatural Language Processing and ComputationalLinguistics, pages 62?69.
ACL.David Beasley.
2006.
Python Essential Reference, 3rdEdition.
Sams.Rune S?tre, Amund Tveit, Tonje S. Steigedal, andAstrid L?greid.
2005.
Semantic annotation ofbiomedical literature using Google.
In Data Min-ing and Bioinformatics Workshop, volume 3482 ofLecture Notes in Computer Science.
Springer.72
