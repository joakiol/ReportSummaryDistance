Workshop on TextGraphs, at HLT-NAACL 2006, pages 61?64,New York City, June 2006. c?2006 Association for Computational LinguisticsGraph-based Generalized Latent Semantic Analysisfor Document RepresentationIrina MatveevaDept.
of Computer ScienceUniversity of ChicagoChicago, IL 60637matveeva@cs.uchicago.eduGina-Anne LevowDept.
of Computer ScienceUniversity of ChicagoChicago, IL 60637levow@cs.uchicago.eduAbstractDocument indexing and representation ofterm-document relations are very impor-tant for document clustering and retrieval.In this paper, we combine a graph-baseddimensionality reduction method with acorpus-based association measure withinthe Generalized Latent Semantic Analysisframework.
We evaluate the graph-basedGLSA on the document clustering task.1 IntroductionDocument indexing and representation of term-document relations are very important issues fordocument clustering and retrieval.
Although thevocabulary space is very large, content bearingwords are often combined into semantic classes thatcontain synonyms and semantically related words.Hence there has been a considerable interest in low-dimensional term and document representations.Latent Semantic Analysis (LSA) (Deerwester etal., 1990) is one of the best known dimensionalityreduction algorithms.
The dimensions of the LSAvector space can be interpreted as latent semanticconcepts.
The cosine similarity between the LSAdocument vectors corresponds to documents?
sim-ilarity in the input space.
LSA preserves the docu-ments similarities which are based on the inner prod-ucts of the input bag-of-word documents and it pre-serves these similarities globally.More recently, a number of graph-based dimen-sionality reduction techniques were successfully ap-plied to document clustering and retrieval (Belkinand Niyogi, 2003; He et al, 2004).
The main ad-vantage of the graph-based approaches over LSA isthe notion of locality.
Laplacian Eigenmaps Embed-ding (Belkin and Niyogi, 2003) and Locality Pre-serving Indexing (LPI) (He et al, 2004) discover thelocal structure of the term and document space andcompute a semantic subspace with a stronger dis-criminative power.
Laplacian Eigenmaps Embed-ding and LPI preserve the input similarities onlylocally, because this information is most reliable.Laplacian Eigenmaps Embedding does not providea fold-in procedure for unseen documents.
LPIis a linear approximation to Laplacian EigenmapsEmbedding that eliminates this problem.
Similarto LSA, the input similarities to LPI are based onthe inner products of the bag-of-word documents.Laplacian Eigenmaps Embedding can use any kindof similarity in the original space.Generalized Latent Semantic Analysis(GLSA) (Matveeva et al, 2005) is a frame-work for computing semantically motivated termand document vectors.
It extends the LSA approachby focusing on term vectors instead of the dualdocument-term representation.
GLSA requires ameasure of semantic association between terms anda method of dimensionality reduction.In this paper, we use GLSA with point-wise mu-tual information as a term association measure.
Weintroduce the notion of locality into this frameworkand propose to use Laplacian Eigenmaps Embed-ding as a dimensionality reduction algorithm.
Weevaluate the importance of locality for documentrepresentation in document clustering experiments.The rest of the paper is organized as follows.
Sec-61tion 2 contains the outline of the graph-based GLSAalgorithm.
Section 3 presents our experiments, fol-lowed by conclusion in section 4.2 Graph-based GLSA2.1 GLSA FrameworkThe GLSA algorithm (Matveeva et al, 2005) has thefollowing setup.
The input is a document collectionC with vocabulary V and a large corpus W .1.
For the vocabulary in V , obtain a matrix ofpair-wise similarities, S, using the corpus W2.
Obtain the matrix UT of a low dimensionalvector space representation of terms that pre-serves the similarities in S, UT ?
Rk?|V |3.
Construct the term document matrix D for C4.
Compute document vectors by taking linearcombinations of term vectors D?
= UTDThe columns of D?
are documents in the k-dimensional space.GLSA approach can combine any kind of simi-larity measure on the space of terms with any suit-able method of dimensionality reduction.
The innerproduct between the term and document vectors inthe GLSA space preserves the semantic associationin the input space.
The traditional term-documentmatrix is used in the last step to provide the weightsin the linear combination of term vectors.
LSA isa special case of GLSA that uses inner product instep 1 and singular value decomposition in step 2,see (Bartell et al, 1992).2.2 Singular Value DecompositionGiven any matrix S, its singular value decompo-sition (SVD) is S = U?V T .
The matrix Sk =U?kV T is obtained by setting all but the first k di-agonal elements in ?
to zero.
If S is symmetric, asin the GLSA case, U = V and Sk = U?kUT .
Theinner product between the GLSA term vectors com-puted as U?1/2k optimally preserves the similaritiesin S wrt square loss.The basic GLSA computes the SVD of S and usesk eigenvectors corresponding to the largest eigenval-ues as a representation for term vectors.
We will re-fer to this approach as GLSA.
As for LSA, the simi-larities are preserved globally.2.3 Laplacian Eigenmaps EmbeddingWe used the Laplacian Embedding algo-rithm (Belkin and Niyogi, 2003) in step 2 ofthe GLSA algorithm to compute low-dimensionalterm vectors.
Laplacian Eigenmaps Embeddingpreserves the similarities in S only locally sincelocal information is often more reliable.
We willrefer to this variant of GLSA as GLSAL.The Laplacian Eigenmaps Embedding algorithmcomputes the low dimensional vectors y to minimizeunder certain constraints?ij||yi ?
yj||2Wij .W is the weight matrix based on the graph adjacencymatrix.
Wij is large if terms i and j are similar ac-cording to S. Wij can be interpreted as the penaltyof mapping similar terms far apart in the LaplacianEmbedding space, see (Belkin and Niyogi, 2003)for details.
In our experiments we used a binary ad-jacency matrix W .
Wij = 1 if terms i and j areamong the k nearest neighbors of each other and iszero otherwise.2.4 Measure of Semantic AssociationFollowing (Matveeva et al, 2005), we primarilyused point-wise mutual information (PMI) as a mea-sures of semantic association in step 1 of GLSA.PMI between random variables representing twowords, w1 and w2, is computed asPMI(w1, w2) = logP (W1 = 1,W2 = 1)P (W1 = 1)P (W2 = 1).2.5 GLSA SpaceGLSA offers a greater flexibility in exploring thenotion of semantic relatedness between terms.
Inour preliminary experiments, we obtained the matrixof semantic associations in step 1 of GLSA usingpoint-wise mutual information (PMI), likelihood ra-tio and ?2 test.
Although PMI showed the best per-formance, other measures are particularly interest-ing in combination with the Laplacian Embedding.Related approaches, such as LSA, the Word SpaceModel (WS) (Schu?tze, 1998) and Latent RelationalAnalysis (LRA) (Turney, 2004) are limited to onlyone measure of semantic association and preservethe similarities globally.62Assuming that the vocabulary space has some un-derlying low dimensional semantic manifold.
Lapla-cian Embedding algorithm tries to approximate thismanifold by relying only on the local similarity in-formation.
It uses the nearest neighbors graph con-structed using the pair-wise term similarities.
Thecomputations of the Laplacian Embedding uses thegraph adjacency matrix W .
This matrix can be bi-nary or use weighted similarities.
The advantageof the binary adjacency matrix is that it conveysthe neighborhood information without relying on in-dividual similarity values.
It is important for co-occurrence based similarity measures, see discus-sion in (Manning and Schu?tze, 1999).The Locality Preserving Indexing (He et al,2004) has a similar notion of locality but has to usebag-of-words document vectors.3 Document Clustering ExperimentsWe conducted a document clustering experiment forthe Reuters-21578 collection.
To collect the co-occurrence statistics for the similarities matrix Swe used a subset of the English Gigaword collec-tion (LDC), containing New York Times articles la-beled as ?story?.
We had 1,119,364 documents with771,451 terms.
We used the Lemur toolkit1 to tok-enize and index all document collections used in ourexperiments, with stemming and a list of stop words.Since Locality Preserving Indexing algorithm(LPI) is most related to the graph-based GLSAL, weran experiments similar to those reported in (He etal., 2004).
We computed the GLSA document vec-tors for the 20 largest categories from the Reuters-21578 document collection.
We had 8564 docu-ments and 7173 terms.
We used the same list of 30TREC words as in (He et al, 2004) which are listedin table 12.
For each word on this list, we generateda cluster as a subset of Reuters documents that con-tained this word.
Clusters are not disjoint and con-tain documents from different Reuters categories.We computed GLSA, GLSAL, LSA and LPI rep-resentations.
We report the results for k = 5 forthe k nearest neighbors graph for LPI and LaplacianEmbedding, and binary weights for the adjacency1http://www.lemurproject.org/2We used 28 words because we used stemming whereas (Heet al, 2004) did not, so that in two cases, two words were re-duces to the same stem.matrix.
We report results for 300 embedding dimen-sions for GLSA, LPI and LSA and 500 dimensionsfor GLSAL.We evaluate these representations in terms of howwell the cosine similarity between the documentvectors within each cluster corresponds to the truesemantic similarity.
We expect documents from thesame Reuters category to have higher similarity.For each cluster we computed all pair-wise doc-ument similarities.
All pair-wise similarities weresorted in decreasing order.
The term ?inter-pair?
de-scribes a pair of documents that have the same label.For the kth inter-pair, we computed precision at k as:precision(pk) =#inter ?
pairs pj, s.t.
j < kk ,where pj refers to the jth inter-pair.
The averageof the precision values for each of the inter-pairs wasused as the average precision for the particular doc-ument cluster.Table 1 summarizes the results.
The first columnshows the words according to which document clus-ters were generated and the entropy of the categorydistribution within that cluster.
The baseline was touse the tf document vectors.
We report results forGLSA, GLSAL, LSA and LPI.
The LSA and LPIcomputations were based solely on the Reuters col-lection.
For GLSA and GLSALwe used the term as-sociations computed for the Gigaword collection, asdescribed above.
Therefore, the similarities that arepreserved are quite different.
For LSA and LPI theyreflect the term distribution specific for the Reuterscollection whereas for GLSA they are more general.By paired 2-tailed t-test, at p ?
0.05, GLSA outper-formed all other approaches.
There was no signifi-cant difference in performance of GLSAL, LSA andthe baseline.
Disappointingly, we could not achievegood performance with LPI.
Its performance variesover clusters similar to that of other approaches butthe average is significantly lower.
We would liketo stress that the comparison of our results to thosepresented in (He et al, 2004) are only suggestivesince (He et al, 2004) applied LPI to each clusterseparately and used PCA as preprocessing.
We com-puted the LPI representation for the full collectionand did not use PCA.63word tf glsa glsaL lsa lpiagreement(1) 0.74 0.73 0.73 0.75 0.46american(0.8) 0.63 0.72 0.59 0.64 0.36bank(1.4) 0.45 0.52 0.40 0.48 0.28control(0.7) 0.78 0.82 0.80 0.80 0.58domestic(0.8) 0.64 0.68 0.66 0.68 0.35export(0.8) 0.64 0.65 0.70 0.67 0.37five(1.3) 0.74 0.77 0.71 0.70 0.40foreign(1.2) 0.51 0.58 0.55 0.56 0.28growth(1) 0.51 0.58 0.48 0.54 0.32income(0.5) 0.84 0.86 0.83 0.80 0.69increase(1.3) 0.51 0.61 0.53 0.53 0.29industrial(1.2) 0.59 0.66 0.58 0.61 0.34internat.
(1.1) 0.58 0.59 0.54 0.61 0.34investment(1) 0.68 0.77 0.70 0.72 0.46loss(0.3) 0.98 0.99 0.98 0.98 0.88money(1.1) 0.70 0.62 0.71 0.65 0.38national(1.3) 0.49 0.58 0.49 0.55 0.27price(1.2) 0.53 0.63 0.57 0.57 0.29production(1) 0.56 0.66 0.58 0.59 0.29public(1.2) 0.58 0.60 0.57 0.57 0.31rate(1.1) 0.61 0.62 0.64 0.60 0.35report(1.2) 0.66 0.72 0.62 0.65 0.35service(0.9) 0.59 0.66 0.56 0.61 0.39source(1.2) 0.56 0.54 0.59 0.60 0.27talk(0.9) 0.74 0.67 0.73 0.74 0.39tax(0.7) 0.91 0.93 0.90 0.89 0.67trade(1) 0.85 0.74 0.82 0.60 0.33world(1.1) 0.63 0.65 0.68 0.66 0.33Av.
Acc 0.65 0.68 0.65 0.66 0.40Table 1: Average inter-pairs accuracy.The inter-pair accuracy depended on the cate-gories distribution within clusters.
For more homo-geneous clusters, e.g.
?loss?, all methods (exceptLPI) achieve similar precision.
For less homoge-neous clusters, e.g.
?national?, ?industrial?, ?bank?,GLSA and LSA outperformed the tf document vec-tors more significantly.4 Conclusion and Future WorkWe introduced a graph-based method of dimension-ality reduction into the GLSA framework.
Lapla-cian Eigenmaps Embedding preserves the similar-ities only locally, thus providing a potentially bet-ter approximation to the low dimensional semanticspace.
We explored the role of locality in the GLSArepresentation and used binary adjacency matrix assimilarity which was preserved and compared it toGLSA with unnormalized PMI scores.Our results did not show an advantage of GLSAL.GLSAL and LPI seem to be very sensitive to the pa-rameters of the neighborhood graph.
We tried dif-ferent parameter settings but more experiments arerequired for a thorough analysis.
We are also plan-ning to use a different document collection to elimi-nate the possible effect of the specific term distribu-tion in the Reuters collection.
Further experimentsare needed to make conclusions about the geometryof the vocabulary space and the appropriateness ofthese methods for term and document embedding.ReferencesBrian T. Bartell, Garrison W. Cottrell, and Richard K.Belew.
1992.
Latent semantic indexing is an optimalspecial case of multidimensional scaling.
In Proc.
ofthe 15th ACM SIGIR, pages 161?167.
ACM Press.Mikhail Belkin and Partha Niyogi.
2003.
Laplacianeigenmaps for dimensionality reduction and data rep-resentation.
Neural Computation, 15(6):1373?1396.Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-dauer, George W. Furnas, and Richard A. Harshman.1990.
Indexing by latent semantic analysis.
Jour-nal of the American Society of Information Science,41(6):391?407.Xiaofei He, Deng Cai, Haifeng Liu, and Wei-Ying Ma.2004.
Locality preserving indexing for document rep-resentation.
In Proc.
of the 27rd ACM SIGIR, pages96?103.
ACM Press.Chris Manning and Hinrich Schu?tze.
1999.
Founda-tions of Statistical Natural Language Processing.
MITPress.
Cambridge, MA.Irina Matveeva, Gina-Anne Levow, Ayman Farahat, andChristian Royer.
2005.
Generalized latent semanticanalysis for term representation.
In Proc.
of RANLP.Hinrich Schu?tze.
1998.
Automatic word sense discrimi-nation.
Computational Linguistics, 24(21):97?124.Peter D. Turney.
2004.
Human-level performance onword analogy questions by latent relational analysis.Technical report, Technical Report ERB-1118, NRC-47422.64
