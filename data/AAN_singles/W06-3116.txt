Proceedings of the Workshop on Statistical Machine Translation, pages 126?129,New York City, June 2006. c?2006 Association for Computational LinguisticsMood at work: Ramses versus PharaohAlexandre Patry, Fabrizio Gotti and Philippe LanglaisRALI ?
DIROUniversite?
de Montre?al{patryale,gottif,felipe}@iro.umontreal.caAbstractWe present here the translation system weused in this year?s WMT shared task.
Themain objective of our participation wasto test RAMSES, an open source phrase-based decoder.
For that purpose, we usedthe baseline system made available by theorganizers of the shared task1 to build thenecessary models.
We then carried out apair-to-pair comparison of RAMSES withPHARAOH on the six different translationdirections that we were asked to perform.We present this comparison in this paper.1 IntroductionPhrase-based (PB) machine translation (MT) is nowa popular paradigm, partly because of the relativeease with which we can automatically create an ac-ceptable translation engine from a bitext.
As a mat-ter of fact, deriving such an engine from a bitext con-sists in (more or less) gluing together dedicated soft-ware modules, often freely available.
Word-basedmodels, or the so-called IBM models, can be trainedusing the GIZA or GIZA++ toolkits (Och and Ney,2000).
One can then train phrase-based models us-ing the THOT toolkit (Ortiz-Mart?
?nez et al, 2005).For their part, language models currently in use inSMT systems can be trained using packages such asSRILM (Stolcke, 2002) and the CMU-SLM toolkit(Clarkson and Rosenfeld, 1997).1www.statmt.org/wmt06/shared-task/baseline.htmlOnce all the models are built, one can chooseto use PHARAOH (Koehn, 2004), an efficient full-fledged phrase-based decoder.
We only know ofone major drawback when using PHARAOH: itslicensing policy.
Indeed, it is available for non-commercial use in its binary form only.
Thisseverely limits its use, both commercially and sci-entifically (Walker, 2005).For this reason, we undertook the design of ageneric architecture called MOOD (Modular Object-Oriented Decoder), especially suited for instantiat-ing SMT decoders.
Two major goals directed ourdesign of this package: offering open source, state-of-the-art decoders and providing an architecture toeasily build these decoders.
This effort is describedin (Patry et al, 2006).As a proof of concept that our framework (MOOD)is viable, we attempted to use its functionalities toimplement a clone of PHARAOH, based on the com-prehensive user manual of the latter.
This clone,called RAMSES, is now part of the MOOD distribu-tion, which can be downloaded freely from the pagehttp://smtmood.sourceforge.net.We conducted a pair-to-pair comparison betweenthe two engines that we describe in this paper.
Weprovide an overview of the MOOD architecture inSection 2.
Then we describe briefly RAMSES in Sec-tion 3.
The comparison between the two decoders interms of automatic metrics is analyzed in Section 4.We confirm this comparison by presenting a man-ual evaluation we conducted on an random sampleof the translations produced by both decoders.
Thisis reported in Section 5.
We conclude in Section 6.1262 The MOOD FrameworkA decoder must implement a specific combination oftwo elements: a model representation and a searchspace exploration strategy.
MOOD is a frameworkdesigned precisely to allow such a combination, byclearly separating its two elements.
The design ofthe framework is described in (Patry et al, 2006).MOOD is implemented with the C++ program-ming language and is licensed under the Gnu Gen-eral Public License (GPL)2.
This license grants theright to anybody to use, modify and distribute theprogram and its source code, provided that any mod-ified version be licensed under the GPL as well.As explained in (Walker, 2005), this kind of licensestimulates new ideas and research.3 MOOD at work: RAMSESAs we said above, in order to test our design, wereproduced the most popular phrase-based decoder,PHARAOH (Koehn, 2004), by following as faithfullyas possible its detailed user manual.
The command-line syntax RAMSES recognizes is compatible withthat of PHARAOH.
The output produced by bothdecoders are compatible as well and RAMSES canalso output its n-best lists in the same format asPHARAOH does, i.e.
in a format that the CARMELtoolkit can parse (Knight and Al-Onaizan, 1999).Switching decoders is therefore straightforward.4 RAMSES versus PHARAOHTo compare the translation performances of bothdecoders in a meaningful manner, RAMSES andPHARAOH were given the exact same languagemodel and translation table for each translation ex-periment.
Both models were produced with thescripts provided by the organizers.
This means inpractice that the language model was trained usingthe SRILM toolkit (Stolcke, 2002).
The word align-ment required to build the phrase table was pro-duced with the GIZA++ package.
A Viterbi align-ment computed from an IBM model 4 (Brown et al,1993) was computed for each translation direction.Both alignments were then combined in a heuristicway (Koehn et al, ).
Each pair of phrases in the2http://www.gnu.org/copyleft/gpl.htmlmodel is given 5 scores, described in the PHARAOHtraining manual.3To tune the coefficients of the log-linearcombination that both PHARAOH and RAMSESuse when decoding, we used the organizers?minimum-error-rate-training.perlscript.
This tuning step was performed on thefirst 500 sentences of the dedicated developmentcorpora.
Inevitably, RAMSES differs slightlyfrom PHARAOH, because of some undocumentedembedded heuristics.
Thus, we found appropriateto tune each decoder separately (although withthe same material).
In effect, each decoder doesslightly better (with BLEU) when it uses its own bestparameters obtained from tuning, than when it usesthe parameters of its counterpart.Eight coefficents were adjusted this way: five forthe translation table (one for each score associatedto each pair of phrases), and one for each of the fol-lowing models: the language model, the so-calledword penalty model and the distortion model (wordreordering model).
Each parameter is given a start-ing value and a range within which it is allowed tovary.
For instance, the language model coefficient?sstarting value is 1.0 and the coefficient is in the range[0.5?1.5].
Eventually, we obtained two optimal con-figurations (one for each decoder) with which wetranslated the TEST material.We evaluated the translations produced by bothdecoders with the organizers?
multi-bleu.perlscript, which computes a BLEU score (and displaysthe n-gram precisions and brevity penalty used).
Wereport the scores we gathered on the test corpus of2000 pairs of sentences in Table 1.
Overall, bothdecoders offer similar performances, down to then-gram precisions.
To assess the statistical signifi-cance of the observed differences in BLEU, we usedthe bootstrapping technique described in (Zhangand Vogel, 2004), randomly selecting 500 sentencesfrom each test set, 1000 times.
Using a 95% con-fidence interval, we determined that the small dif-ferences between the two decoders are not statis-tically significant, except for two tests.
For thedirection English to French, RAMSES outperformsPHARAOH, while in the German to English direc-3http://www.statmt.org/wmt06/shared-task/training-release-1.3.tgz127tion, PHARAOH is better.
Whenever a decoder isbetter than the other, Table 1 shows that it is at-tributable to higher n-gram precisions; not to thebrevity penalty.We further investigated these two cases by calcu-lating BLEU for subsets of the test corpus sharingsimilar sentence lengths (Table 2).
We see that bothdecoders have similar performances on short sen-tences, but can differ by as much as 1% in BLEU onlonger ones.
In contrast, on the Spanish-to-Englishtranslation direction, where the two decoders offersimilar performances, the difference between BLEUscores never exceeds 0.23%.Expectedly, Spanish and French are much easierto translate than German.
This is because, in thisstudy, we did not apply any pre-processing strat-egy that we know can improve performances, suchas clause reordering or compound-word splitting(Collins et al, 2005; Langlais et al, 2005).Table 2 shows that it does not seem much moredifficult to translate into English than from English.This is surprising: translating into a morphologicallyricher language should be more challenging.
Theopposite is true for German here: without doing any-thing specific for this language, it is much easier totranslate from German to English than the other wayaround.
This may be attributed in part to the lan-guage model: for the test corpus, the perplexity ofthe language models provided is 105.5 for German,compared to 59.7 for English.5 Human EvaluationIn an effort to correlate the objective metrics withhuman reviews, we undertook the blind evaluationof a sample of 100 pairwise translations for the threeForeign language-to-English translation tasks.
Thepairs were randomly selected from the 3064 trans-lations produced by each engine.
They had to bedifferent for each decoder and be no more than 25words long.Each evaluator was presented with a source sen-tence, its reference translation and the translationproduced by each decoder.
The last two were in ran-dom order, so the evaluator did not know which en-gine produced the translation.
The evaluator?s taskwas two-fold.
(1) He decided whether one transla-tion was better than the other.
(2) If he replied ?yes?D BLEU p1 p2 p3 p4 BPes ?
enP 30.65 64.10 36.52 23.70 15.91 1.00R 30.48 64.08 36.30 23.52 15.76 1.00fr ?
enP 30.42 64.28 36.45 23.39 15.64 1.00R 30.43 64.58 36.59 23.54 15.73 0.99de ?
enP 25.15 61.19 31.32 18.53 11.61 0.99R 24.49 61.06 30.75 17.73 10.81 1.00en ?
esP 29.40 61.86 35.32 22.77 15.02 1.00R 28.75 62.23 35.03 22.32 14.58 0.99en ?
frP 30.96 61.10 36.56 24.49 16.80 1.00R 31.79 61.57 37.38 25.30 17.53 1.00en ?
deP 18.03 52.77 22.70 12.45 7.25 0.99R 18.14 53.38 23.15 12.75 7.47 0.98Table 1: Performance of RAMSES and PHARAOHon the provided test set of 2000 pairs of sentencesper language pair.
P stands for PHARAOH, R forRAMSES.
All scores are percentages.
pn is the n-gram precision and BP is the brevity penalty usedwhen computing BLEU.in test (1), he stated whether the best translation wassatisfactory while the other was not.
Two evalua-tors went through the 3 ?
100 sentence pairs.
Noneof them understands German; subject B understandsSpanish, and both understand French and English.The results of this informal, yet informative exerciseare reported in Table 3.Overall, in many cases (64% and 48% for subjectA and B respectively), the evaluators did not pre-fer one translation over the other.
On the Spanish-and French-to-English tasks, both subjects slightlypreferred the translations produced by RAMSES.
Inabout one fourth of the cases where one translationwas preferred did the evaluators actually flag the se-lected translation as significantly better.6 DiscussionWe presented a pairwise comparison of two de-coders, RAMSES and PHARAOH.
Although RAM-SES is roughly twice as slow as PHARAOH, both de-128Test set [0,15] [16,25] [26,?
[en ?
fr (P) 33.52 30.65 30.39en ?
fr (R) 33.78 31.19 31.35de ?
en (P) 29.74 24.30 24.76de ?
en (R) 29.85 23.92 23.78es ?
en (P) 34.23 28.32 30.60es ?
en (R) 34.46 28.39 30.40Table 2: BLEU scores on subsets of the test corpusfiltered by sentence length ([min words, max words]intervals), for Pharaoh and Ramses.Preferred ImprovedP R No P Res ?
ensubject A 13 16 71 6 1subject B 23 31 46 3 8fr ?
ensubject A 18 19 63 5 3subject B 20 21 59 8 8de ?
ensubject A 24 18 58 5 9subject B 30 31 39 3 3Total 128 136 336 30 32Table 3: Human evaluation figures.
The columnPreferred indicates the preference of the subject(Pharaoh, Ramses or No preference).
The columnImproved shows when a subject did prefer a trans-lation and also said that the preferred translation wascorrect while the other one was not.coders offer comparable performances, according toautomatic and informal human evaluations.Moreover, RAMSES is the product of clean frame-work: MOOD, a solid tool for research projects.
Itscode is open source and the architecture is modular,making it easier for researchers to experiment withSMT.
We hope that the availability of the sourcecode and the clean design of MOOD will make it auseful platform to implement new decoders.AcknowledgmentsWe warmly thanks Elliott Macklovitch for his par-ticipation in the manual annotation task.
This workhas been partially funded by an NSERC grant.ReferencesP.F.
Brown, S.A. Della Pietra, V.J.
Della Pietra, and R.L.Mercer.
1993.
The Mathematics of Statistical Ma-chine Translation: Parameter Estimation.
Computa-tional Linguistics, 19(2):263?311.P.
Clarkson and R. Rosenfeld.
1997.
Statistical languagemodeling using the CMU-cambridge toolkit.
In Proc.of Eurospeech, pages 2707?2710, Rhodes, Greece.M.
Collins, P. Koehn, and I. Kucerova.
2005.
Clause re-structuring for statistical machine translation.
In Proc.of the 43rd ACL, pages 531?540, Ann Arbor, MI.K.
Knight and Y. Al-Onaizan, 1999.
A Primer onFinite-State Software for Natural Language Process-ing.
www.isi.edu/licensed-sw/carmel.P.
Koehn, F. Joseph Och, and D. Marcu.
StatisticalPhrase-Based Translation.
In Proc.
of HLT, Edmon-ton, Canada.P.
Koehn.
2004.
Pharaoh: a Beam Search Decoder forPhrase-Based SMT.
In Proc.
of the 6th AMTA, pages115?124, Washington, DC.P.
Langlais, G. Cao, and F. Gotti.
2005.
RALI: SMTshared task system description.
In 2nd ACL workshopon Building and Using Parallel Texts, pages 137?140,Ann Arbor, MI.F.J.
Och and H. Ney.
2000.
Improved Statistical Align-ment Models.
In Proc.
of ACL, pages 440?447,Hongkong, China.D.
Ortiz-Mart?
?nez, I. Garcia?-Varea, and F. Casacuberta.2005.
Thot: a toolkit to train phrase-based statisticaltranslation models.
In Proc.
of MT Summit X, pages141?148, Phuket, Thailand.A.
Patry, F. Gotti, and P. Langlais.
2006.
MOODa modular object-oriented decoder for statistical ma-chine translation.
In Proc.
of LREC, Genoa, Italy.A.
Stolcke.
2002.
SRILM - an Extensible LanguageModeling Toolkit.
In Proc.
of ICSLP, Denver, USA.D.J.
Walker.
2005.
The open ?a.i.?
kitTM: General ma-chine learning modules from statistical machine trans-lation.
In Workshop of MT Summit X, ?Open-SourceMachine Translation?, Phuket, Thailand.Ying Zhang and Stephan Vogel.
2004.
Measuring confi-dence intervals for the machine translation evaluationmetrics.
In Proc.
of the 10th TMI, Baltimore, MD.129
