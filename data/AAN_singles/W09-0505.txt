Proceedings of EACL 2009 Workshop on Semantic Representation of Spoken Language - SRSL 2009, pages 34?41,Athens, Greece, 30 March 2009. c?2009 Association for Computational LinguisticsAnnotating Spoken Dialogs: from Speech Segments to Dialog Acts andFrame SemanticsMarco Dinarelli, Silvia Quarteroni, Sara Tonelli, Alessandro Moschitti, Giuseppe Riccardi?University of Trento38050 Povo - Trento, Italy{dinarelli,silviaq,moschitti,riccardi}@disi.unitn.it, satonelli@fbk.euAbstractWe are interested in extracting semanticstructures from spoken utterances gener-ated within conversational systems.
Cur-rent Spoken Language Understanding sys-tems rely either on hand-written seman-tic grammars or on flat attribute-value se-quence labeling.
While the former ap-proach is known to be limited in coverageand robustness, the latter lacks detailed re-lations amongst attribute-value pairs.
Inthis paper, we describe and analyze the hu-man annotation process of rich semanticstructures in order to train semantic statis-tical parsers.
We have annotated spokenconversations from both a human-machineand a human-human spoken dialog cor-pus.
Given a sentence of the transcribedcorpora, domain concepts and other lin-guistic features are annotated, rangingfrom e.g.
part-of-speech tagging and con-stituent chunking, to more advanced anno-tations, such as syntactic, dialog act andpredicate argument structure.
In particu-lar, the two latter annotation layers appearto be promising for the design of complexdialog systems.
Statistics and mutual in-formation estimates amongst such featuresare reported and compared across corpora.1 IntroductionSpoken language understanding (SLU) addressesthe problem of extracting and annotating themeaning structure from spoken utterances in thecontext of human dialogs (De Mori et al, 2008).In spoken dialog systems (SDS) most used modelsof SLU are based on the identification of slots (en-?This work was partially funded by the European Com-mission projects LUNA (contract 33549) and ADAMACH(contract 022593).tities) within one or more frames (frame-slot se-mantics) that is defined by the application.
Whilethis model is simple and clearly insufficient tocope with interpretation and reasoning, it has sup-ported the first generation of spoken dialog sys-tems.
Such dialog systems are thus limited by theability to parse semantic features such as predi-cates and to perform logical computation in thecontext of a specific dialog act (Bechet et al,2004).
This limitation is reflected in the type ofhuman-machine interactions which are mostly di-rected at querying the user for specific slots (e.g.
?What is the departure city??)
or implementingsimple dialog acts (e.g.
confirmation).
We believethat an important step in overcoming such limita-tion relies on the study of models of human-humandialogs at different levels of representation: lexi-cal, syntactic, semantic and discourse.In this paper, we present our results in address-ing the above issues in the context of the LUNAresearch project for next-generation spoken dialoginterfaces (De Mori et al, 2008).
We proposemodels for different levels of annotation of theLUNA spoken dialog corpus, including attribute-value, predicate argument structures and dialogacts.
We describe the tools and the adaptation ofoff-the-shelf resources to carry out annotation ofthe predicate argument structures (PAS) of spokenutterances.
We present a quantitative analysis ofsuch semantic structures for both human-machineand human-human conversations.To the best of our knowledge this is the first(human-machine and human-human) SDS corpusdenoting a multilayer approach to the annotationof lexical, semantic and dialog features, which al-lows us to investigate statistical relations betweenthe layers such as shallow semantic and discoursefeatures used by humans or machines.
In the fol-lowing sections we describe the corpus, as well asa quantitative analysis and statistical correlationsbetween annotation layers.342 Annotation modelOur corpus is planned to contain 1000 equallypartitioned Human-Human (HH) and Human-Machine (HM) dialogs.
These are recorded bythe customer care and technical support center ofan Italian company.
While HH dialogs refer toreal conversations of users engaged in a problemsolving task in the domain of software/hardwaretroubleshooting, HM dialogs are acquired with aWizard of Oz approach (WOZ).
The human agent(wizard) reacts to user?s spontaneous spoken re-quests following one of ten possible dialog scenar-ios inspired by the services provided by the com-pany.The above data is organized in transcrip-tions and annotations of speech based on a newmulti-level protocol studied specifically within theproject, i.e.
the annotation levels of words, turns1,attribute-value pairs, dialog acts, predicate argu-ment structures.
The annotation at word levelis made with part-of-speech and morphosyntac-tic information following the recommendations ofEAGLES corpora annotation (Leech and Wilson,2006).
The attribute-value annotation uses a pre-defined domain ontology to specify concepts andtheir relations.
Dialog acts are used to annotate in-tention in an utterance and can be useful to findrelations between different utterances as the nextsection will show.
For predicate structure annota-tion, we followed the FrameNet model (Baker etal., 1998) (see Section 2.2).2.1 Dialog Act annotationDialog act annotation is the task of identifyingthe function or goal of a given utterance (Sinclairand Coulthard, 1975): thus, it provides a comple-mentary information to the identification of do-main concepts in the utterance, and a domain-independent dialog act scheme can be applied.For our corpus, we used a dialog act taxonomywhich follows initiatives such as DAMSL (Coreand Allen, 1997), TRAINS (Traum, 1996) andDIT++ (Bunt, 2005).
Although the level of granu-larity and coverage varies across such taxonomies,a careful analysis leads to identifying three maingroups of dialog acts:1.
Core acts, which represent the fundamen-tal actions performed in the dialog, e.g.
re-1A turn is defined as the interval when a speaker is active,between two pauses in his/her speech flow.questing and providing information, or exe-cuting a task.
These include initiatives (oftencalled forward-looking acts) and responses(backward-looking acts);2.
Conventional/Discourse management acts,which maintain dialog cohesion and delimitspecific phases, such as opening, continua-tion, closing, and apologizing;3.
Feedback/Grounding acts,used to elicit andprovide feedback in order to establish or re-store a common ground in the conversation.Our taxonomy, following the same three-foldpartition, is summarized in Table 1.Table 1: Dialog act taxonomyCore dialog actsInfo-request Speaker wants information from ad-dresseeAction-request Speaker wants addressee to performan actionYes-answer Affirmative answerNo-answer Negative answerAnswer Other kinds of answerOffer Speaker offers or commits to performan actionReportOnAction Speaker notifies an action is being/hasbeen performedInform Speaker provides addressee with in-formation not explicitly required (viaan Info-request)Conventional dialog actsGreet Conversation openingQuit Conversation closingApology ApologyThank Thanking (and down-playing)Feedback/turn management dialog actsClarif-request Speaker asks addressee for confirma-tion/repetition of previous utterancefor clarification.Ack Speaker expresses agreement withprevious utterance, or provides feed-back to signal understanding of whatthe addressee saidFiller Utterance whose main goal is to man-age conversational time (i.e.
dpeakertaking time while keeping the turn)Non-interpretable/non-classifiable dialog actsOther Default tag for non-interpretable andnon-classifiable utterancesIt can be noted that we have decided to retainonly the most frequent dialog act types from theschemes that inspired our work.
Rather than as-piring to the full discriminative power of possibleconversational situations, we have opted for a sim-ple taxonomy that would cover the vast majority35of utterances and at the same time would be ableto generalize them.
Its small number of classes ismeant to allow a supervised classification methodto achieve reasonable performance with limiteddata.
The taxonomy is currently used by the sta-tistical Dialogue Manager in the ADAMACH EUproject (Varges et al, 2008); the limited numberof classes allows to reduce the number of hypoth-esized current dialogue acts, thus reducing the di-alogue state space.Dialog act annotation was performed manuallyby a linguist on speech transcriptions previouslysegmented into turns as mentioned above.
The an-notation unit for dialog acts, is the utterance; how-ever, utterances are complex semantic entities thatdo not necessarily correspond to turns.
Hence, asegmentation of the dialog transcription into ut-terances was performed by the annotator beforedialog act labeling.
Both utterance segmentationand dialog act labeling were performed throughthe MMAX tool (Mu?ller and Strube, 2003).The annotator proceeded according to the fol-lowing guidelines:1. by default, a turn is also an utterance;2. if more than one tag is applicable to an ut-terance, choose the tag corresponding to itsmain function;3. in case of doubt among several tags, give pri-ority to tags in core dialog acts group;4. when needed, split the turn into several utter-ances or merge several turns into one utter-ance.Utterance segmentation provides the basis notonly for dialog act labeling but also for the othersemantic annotations.
See Fig.
1 for a dialog sam-ple where each line represents an utterance anno-tated according to the three levels.2.2 Predicate Argument annotationWe carried out predicate argument structure an-notation applying the FrameNet paradigm as de-scribed in (Baker et al, 1998).
This modelcomprises a set of prototypical situations calledframes, the frame-evoking words or expressionscalled lexical units and the roles or participants in-volved in these situations, called frame elements.The latter are typically the syntactic dependents ofthe lexical units.
All lexical units belonging tothe same frame have similar semantics and showPERSON-NAMEInfo: Buongiorno, sono   Paola.GREETING    B._NAMED NameGood morning, this is Paola.Info-req: Come la posso aiutare?Benefitted_party   ASSISTANCEHow may I help you?CONCEPT         HARDWARE-COMPONENTInfo: Buongiorno.
Ho un problema con la stampante.GREETING            PR._DESCRIPTION     Affected_deviceGood morning.
I have a problem with the printer.PART-OF-DAY   NEGAT.
ACTION                ACTIONInfo: Da stamattina non   riesco pi?
a  stampareProblemSince this morning I can?t print.Info-req:   Mi  pu?
dire   nome e cognome per favore?Addressee      TELLING               MessageCan you tell me your name and surname, please?PERSON-NAME  PERSON-SURNAMEAnswer: Mi chiamo  Alessandro  Manzoni.Entity B._NAMED                   NameMy name is Alessandro Manzoni.Figure 1: Annotated dialog extract.
Each utteranceis preceded by dialog act annotation.
Attribute-value annotation appears above the text, PAS an-notation below the text.the same valence.
A particular feature of theFrameNet project both for English and for otherlanguages is its corpus-based nature, i.e.
every el-ement described in the resource has to be instanti-ated in a corpus.
To annotate our SDS corpus, weadopted where possible the already existing frameand frame element descriptions defined for the En-glish FrameNet project, and introduced new def-initions only in case of missing elements in theoriginal model.Figure 1 shows a dialog sample with PAS an-notation reported below the utterance.
All lexi-cal units are underlined and the frame is written incapitals, while the other labels refer to frame el-ements.
In particular, ASSISTANCE is evoked bythe lexical unit aiutare and has one attested frameelement (Benefitted party), GREETING has noframe element, and PROBLEM DESCRIPTIONand TELLING have two frame elements each.Figure 2 gives a comprehensive view of the an-notation process, from audio file transcription tothe annotation of three semantic layers.
Whereas36Figure 2: The annotation processAudio fileTurn segmentation &TranscriptionUtterance segmentationPOS tagging Domain attributeannotationPAS annotationDialog ActannotationSyntactic parsingattribute-value and DA annotation are carriedout on the segmented dialogs at utterance level,PAS annotation requires POS-tagging and syntac-tic parsing (via Bikel?s parser trained for Italian(Corazza et al, 2007)).
Finally, a shallow manualcorrection is carried out to make sure that the treenodes that may carry semantic information havecorrect constituent boundaries.
For the annotationof frame information, we used the Salto tool (Bur-chardt et al, 2006), that stores the dialog file inTIGER-XML format and allows to easily intro-duce word tags and frame flags.
Frame informa-tion is recorded on top of parse trees, with targetinformation pointing to terminal words and frameelements pointing to tree nodes.3 Quantitative comparison of theAnnotationWe evaluated the outcome of dialog act andPAS annotation levels on both the human-human(henceforth HH) and human-machine (HM) cor-pora by not only analyzing frequencies and occur-rences in the separate levels, but also their interac-tion, as discussed in the following sections.3.1 Dialog Act annotationAnalyzing the annotation of 50 HM and 50 HHdialogs at the dialog act level, we note that anHH dialog is composed in average by 48.9?17.4(standard deviation) dialog acts, whereas a HMdialog is composed of 18.9?4.4.
The differencebetween average lengths shows how HH sponta-neous speech can be redundant, while HM dialogsare more limited to an exchange of essential infor-mation.
The standard deviation of a conversationin terms of dialog acts is considerably higher inthe HH corpus than in the HM one.
This can be ex-plained by the fact that the WOZ follows a unique,previously defined task-solving strategy that doesnot allow for digressions.
Utterance segmentationwas also performed differently on the two corpora.In HH we performed 167 turn mergings and 225turn splittings; in HM dialogs, only turn splittings(158) but no turn mergings were performed.Tables 2 and 3 report the dialog acts occurringin the HM and HH corpora, respectively, rankedby their frequencies.Table 2: Dialog acts ranked by frequency in thehuman-machine (HM) corpushuman-machine (HM)DA count rel.
freq.Info-request 249 26.3%Answer 171 18.1%Inform 163 17.2%Yes-answer 70 7.4%Quit 60 6.3%Thank 56 5.9%Greet 50 5.3%Offer 49 5.2%Clarification-request 26 2.7%Action-request 25 2.6%Ack 12 1.3%Filler 6 0.6%No-answer 5 0.5%Other, ReportOnAction 2 0.2%Apology 1 0.1%TOTAL 947From a comparative analysis, we note that:1. info-request is by far the most common dia-log act in HM, whereas in HH ack and infoshare the top ranking position;2. the most frequently occurring dialog act inHH, i.e.
ack, is only ranked 11th in HM;3. the relative frequency of clarification-request(4,7%) is considerably higher in HH than inHM.We also analyzed the ranking of the most fre-quent dialog act bigrams in the two corpora.
Wecan summarize our comparative analysis, reportedin Table 4, to the following: in both corpora,most bigram types contain info and info-request,37Table 3: Dialog acts ranked by frequency in thehuman-human (HH) corpushuman-human (HH)DA count rel.
freq.Ack 582 23.8%Inform 562 23.0%Info-request 303 12.4%Answer 192 7.8%Clarification-request 116 4.7%Offer 114 4.7%Yes-answer 112 4.6%Quit 101 4.1%ReportOnAction 91 3.7%Other 70 2.9%Action-request 69 2.8%Filler 61 2.5%Thank 33 1.3%No-answer 26 1.1%Greet, Apology 7 0.3%TOTAL 2446as expected in a troubleshooting system.
How-ever, the bigram info-request answer, which weexpected to form the core of a task-solving dia-log, is only ranked 5th in the HH corpus, while 5out of the top 10 bigram types contain ack.
Webelieve that this is because HH dialogs primarilycontain spontaneous information-providing turns(e.g.
several info info by the same speaker) andacknowledgements for the purpose of backchan-nel.
Instead, HM dialogs, structured as sequencesof info-request answers pairs, are more minimaland brittle, showing how users tend to avoid re-dundancy when addressing a machine.Table 4: The 10 most frequent dialog act bigramshuman-machine (HM) human-human (HH)info-req answer ack infoanswer info-req info ackinfo info-req info infoinfo-req y-answer ack acksentence beginning greet info-req answergreet info info info-reqinfo quit info-req y-answeroffer info ack info-reqthank info answer acky-answer thank quit sentence end3.2 Predicate Argument annotationWe annotated 50 HM and 50 HH dialogs withframe information.
Differently from the EnglishFrameNet database, we didn?t annotate one frameper sentence.
On the contrary, we identified alllexical units corresponding to ?semantically rele-vant?
verbs, nouns and adjectives with a syntac-tic subcategorization pattern, eventually skippingthe utterances with empty semantics (e.g.
dis-fluencies).
In particular, we annotated all lexicalunits that imply an action, introduce the speaker?sopinion or describe the office environment.
Weintroduced 20 new frames out of the 174 iden-tified in the corpus because the original defini-tion of frames related to hardware/software, data-handling and customer assistance was sometimestoo coarse-grained.
Few new frame elements wereintroduced as well, mostly expressing syntactic re-alizations that are typical of spoken Italian.Table 5 shows some statistics about the cor-pus dimension and the results of our annotation.The human-human dialogs contain less frame in-stances in average than the human-machine group,meaning that speech disfluencies, not present inturns uttered by the WOZ, negatively affect the se-mantic density of a turn.
For the same reason, thepercentage of turns in HH dialogs that were manu-ally corrected in the pre-processing step (see Sec-tion 2.2) is lower than for HM turns, since HH di-alogs have more turns that are semantically emptyand that were skipped in the correction phase.
Be-sides, HH dialogs show a higher frame variabil-ity than HM, which can be explained by the factthat spontaneous conversation may concern mi-nor topics, whereas HM dialogs follow a previ-ously defined structure, designed to solve soft-ware/hardware problems.Tables 6 and 7 report the 10 most frequentframes occurring in the human-machine resp.human-human dialogs.
The relative frame fre-quency in HH dialogs is more sparse than in HMdialogs, meaning that the task-solving strategy fol-lowed by the WOZ limits the number of digres-sions, whereas the semantics of HH dialogs isricher and more variable.As mentioned above, we had to introduce anddefine new frames which were not present in theoriginal FrameNet database for English in order tocapture all relevant situations described in the di-alogs.
A number of these frames appear in bothtables, suggesting that the latter are indeed rel-38Table 5: Dialog turn and frame statistics for thehuman-machine (HM) resp.
human-human (HH)corpusHM HHTotal number of turns 662 1,997Mean dialog length (turns) 13.2 39.9Mean turn length (tokens) 11.4 10.8Corrected turns (%) 50 39Total number of annotations 923 1951Mean number of frame annota-tions per dialog18.5 39.0Mean number of frame elementsper frame annotation1.6 1.7evant to model the general semantics of the di-alogs we are approaching.
The most frequentframe group comprises frames relating to infor-mation exchange that is typical of the help-deskactivity, including Telling, Greeting, Contacting,Statement, Recording, Communication.
Anotherrelevant group encompasses frames related to theoperational state of a device, for example Be-ing operational, Change operational state, Oper-ational testing, Being in operation.The two groups also show high variability oflexical units.
Telling, Change operational stateand Greeting have the richest lexical unit set,with 11 verbs/nouns/adjectives each.
Arrivingand Awareness are expressed by 10 different lexi-cal units, while Statement, Being operational, Re-moving and Undergo change of operational statehave 9 different lexical units each.
The informalnature of the spoken dialogs influences the com-position of the lexical unit sets.
In fact, they arerich in verbs and multiwords used only in collo-quial contexts, for which there are generally fewattestations in the English FrameNet database.Similarly to the dialog act statistics, we alsoanalyzed the most frequent frame bigrams andtrigrams in HM and HH dialogs.
Results arereported in Tables 8 and 9.
Both HH bigramsand trigrams show a more sparse distribution andlower relative frequency than HM ones, implyingthat HH dialogs follow a more flexible structurewith a richer set of topics, thus the sequence ofthemes is less predictable.
In particular, 79%of HH bigrams and 97% of HH trigrams occuronly once (vs. 68% HM bigrams and 82% HMtrigrams).
On the contrary, HM dialogs deal withTable 6: The 10 most frequent frames in the HMcorpus (* =newly introduced)HM corpusFrame count freq-%Greeting* 146 15.8Telling 134 14.5Recording 83 8.9Being named 74 8.0Contacting 52 5.6Usefulness 50 5.4Being operational 28 3.0Problem description* 24 2.6Inspecting 24 2.6Perception experience 21 2.3Table 7: The 10 most frequent frames in the HHcorpus (* =newly introduced)HH corpusFrame count freq-%Telling 143 7.3Greeting* 124 6.3Awareness 74 3.8Contacting 63 3.2Giving 62 3.2Navigation* 61 3.1Change operational state 51 2.6Perception experience 46 2.3Insert data* 46 2.3Come to sight* 38 1.9a fix sequence of topics driven by the turns utteredby the WOZ.
For instance, the most frequentHM bigram and trigram both correspond to theopening utterance of the WOZ:Help desk buongiornoGREETING, sonoBEING NAMEDPaola, in cosa posso esserti utileUSEFULNESS?
(Good morning, help-desk service, Paola speaking, how canI help you?
)3.3 Mutual information between PAS anddialog actsA unique feature of our corpus is the availabil-ity of both a semantic and a dialog act annota-tion level: it is intuitive to seek relationships inthe purpose of improving the recognition and un-derstanding of each level by using features fromthe other.
We considered a subset of 20 HH and50 HM dialogs and computed an initial analysis39Table 8: The 5 most frequent frame bigramshuman-machine (HM) freq-%Greeting Being named 17.1Being named Usefulness 15.3Telling Recording 12.9Recording Contacting 10.9Contacting Greeting 10.6human-human (HH) freq-%Greeting Greeting 4.7Navigation Navigation 1.2Telling Telling 1.0Change op.
state Change op.
state 0.9Telling Problem description 0.8Table 9: The 5 most frequent frame trigramshuman-machine (HM) freq-%Greeting Being named Usefulness 9.5Recording Contacting Greeting 5.7Being named Usefulness Greeting 3.7Telling Recording Contacting 3.5Telling Recording Recording 2.2human-human (HH) freq-%Greeting Greeting Greeting 1.6Greeting Being named Greeting 0.5Contacting Greeting Greeting 0.3Navigation Navigation Navigation 0.2Working on Greeting Greeting 0.2of the co-occurrences of dialog acts and PAS.
Wenoted that each PAS tended to co-occur only with alimited subset of the available dialog act tags, andmoreover in most cases the co-occurrence hap-pened with only one dialog act.
For a more thor-ough analysis, we computed the weighted condi-tional entropy between PAS and dialog acts, whichyields a direct estimate of the mutual informationbetween the two levels of annotation2.2Let H(yj |xi) be the weighted conditional entropy of ob-servation yj of variable Y given observation xi of variableX:H(yj |xi) = ?p(xi; yj)logp(xi; yj)p(xi),where p(xi; yj) is the probability of co-occurrence of xi andyj , and p(xi) and p(yj) are the marginal probabilities of oc-currence of xi resp.
yj in the corpus.
There is an obvious re-lation with the weighted mutual information between xi andyj , defined following e.g.
(Bechet et al, 2004) as:wMI(xi; yj) = p(xi; yj)logp(xi; yj)p(xi)p(yj).
(a) human-machine dialogs (filtering co-occurrences below 3)(b) human-human dialogs (filtering co-occurrences below 5)Figure 3: Weighted conditional entropy betweenPAS and dialog acts in the HM (a) and HH corpus(b).
To lower entropies correspond higher valuesof mutual information (darker color in the scale)Our results are illustrated in Figure 3.
In theHM corpus (Fig.
3(a)), we noted some interestingassociations between dialog acts and PAS.
First,info-req has the maximal MI with PAS like Be-ing in operation and Being attached, as requestsare typically used by the operator to get informa-tion about the status of device.
Several PAS de-note a high MI with the info dialog act, includ-ing Activity resume, Information, Being named,Contacting, and Resolve problem.
Contactingrefers to the description of the situation and of thespeaker?s point of view (usually the caller).
Be-ing named is primarily employed when the callerintroduces himself, while Activity resume usuallyrefers to the operator?s description of the sched-Indeed, the higher is H(yj |xi), the lower is wMI(xi; yj).We approximate all probabilities using frequency of occur-rence.40uled interventions.As for the remaining acts, clarif has the high-est MI with Perception experience and Statement,used to warn the addressee about understandingproblems and asking him to repeat/rephrase an ut-terance, respectively.
The two strategies can becombined in the same utterance, as in the utter-ance: Non ho sentito bene: per favore ripeti cer-cando di parlare piu` forte.
(I haven?t quite heardthat, please repeat trying to speak up.
).The answer tag is highly informative with Suc-cessful action, Change operational state, Becom-ing nonfunctional, Being detached, Read data.These PAS refer to the exchange of infor-mation (Read data) or to actions performedby the user after a suggestion of the system(Change operational state).
Action requests (act-req) seem to be correlated to Replacing as it usu-ally occurs when the operator requests the callerto carry out an action to solve a problem, typicallyto replace a component with another.
Another fre-quent request may refer to some device that theoperator has to test.In the HH corpus (Fig.
3(b)), most of the PASare highly mutually informative with info: in-deed, as shown in Table 3, this is the most fre-quently occurring act in HH except for ack, whichrarely contain verbs that can be annotated by aframe.
As for the remaining acts, there is an easilyexplainable high MI between quit and Greeting;moreover, info-req denote its highest MI withGiving, as in requests to give information, whilerep-action denotes a strong co-occurrence withInchoative attaching: indeed, interlocutors oftenreport on the action of connecting a device.These results corroborate our initial observationthat for most PAS, the mutual information tendsto be very high in correspondence of one dialogact type: this suggests the beneficial effect of in-cluding shallow semantic information as featuresfor dialog act classification.
The converse is lessclear as the same dialog act can relate to a spanof words covered by multiple PAS and generally,several PAS co-occur with the same dialog act.4 ConclusionsIn this paper we have proposed an approach tothe annotation of spoken dialogs using seman-tic and discourse features.
Such effort is crucialto investigate the complex dependencies betweenthe layers of semantic processing.
We have de-signed the annotation model to incorporate fea-tures and models developed both in the speechand language research community and bridgingthe gap between the two communities.
Our multi-layer annotation corpus allows the investigationof cross-layer dependencies and across human-machine and human-human dialogs as well astraining of semantic models which accounts forpredicate interpretation.ReferencesC.
F. Baker, C. J. Fillmore, and J.
B. Lowe.
1998.The Berkeley FrameNet Project.
In Proceedings ofACL/Coling?98, pages 86?90.F.
Bechet, G. Riccardi, and D. Hakkani-Tur.
2004.Mining spoken dialogue corpora for system evalu-ation and modeling.
In Proceedings of EMNLP?04,pages 134?141.H.
Bunt.
2005.
A framework for dialogue act specica-tion.
In Proceedings of SIGSEM WG on Represen-tation of Multimodal Semantic Information.A.
Burchardt, K. Erk, A. Frank, A. Kowalski, S. Pado?,and M. Pinkal.
2006.
Salto - a versatile multi-level annotation tool.
In Proceedings of LREC 2006,pages 517?520, Genoa, Italy.A.
Corazza, A. Lavelli, and G. Satta.
2007.
Anal-isi sintattica-statistica basata su costituenti.
Intelli-genza Artificiale, 4(2):38?39.M.
G. Core and J. F. Allen.
1997.
Coding dialogswith the DAMSL annotation scheme.
In Proceed-ings of the AAAI Fall Symposium on CommunicativeActions in Humans and Machines.R.
De Mori, F. Bechet, D. Hakkani-Tur, M. McTear,G.
Riccardi, and G. Tur.
2008.
Spoken languageunderstanding: A survey.
IEEE Signal Processingmagazine, 25(3):50?58.G.
Leech and A. Wilson.
2006.
EAGLES recommen-dations for the morphosyntactic annotation of cor-pora.
Technical report, ILC-CNR.C.
Mu?ller and M. Strube.
2003.
Multi-level annotationin MMAX.
In Proceedings of SIGDIAL?03.J.
M. Sinclair and R. M. Coulthard.
1975.
Towards anAnalysis of Discourse: The English Used by Teach-ers and Pupils.
Oxford University Press, Oxford.D.
Traum.
1996.
Conversational agency: TheTRAINS-93 dialogue manager.
In Proceedings ofTWLT 11: Dialogue Management in Natural Lan-guage Systems, pages 1?11, June.S.
Varges, G. Riccardi, and S. Quarteroni.
2008.
Per-sistent information state in a data-centric architec-ture.
In Proceedings of SIGDIAL?08.41
