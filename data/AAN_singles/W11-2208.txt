Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 64?71,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsUnsupervised Bilingual POS Tagging with Markov Random FieldsDesai Chen Chris Dyer Shay B. Cohen Noah A. SmithSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213, USAdesaic@andrew.cmu.edu, {cdyer,scohen,nasmith}@cs.cmu.eduAbstractIn this paper, we give a treatment to the prob-lem of bilingual part-of-speech induction withparallel data.
We demonstrate that na?
?ve op-timization of log-likelihood with joint MRFssuffers from a severe problem of local max-ima, and suggest an alternative ?
using con-trastive estimation for estimation of the pa-rameters.
Our experiments show that estimat-ing the parameters this way, using overlappingfeatures with joint MRFs performs better thanprevious work on the 1984 dataset.1 IntroductionThis paper considers unsupervised learning of lin-guistic structure?specifically, parts of speech?inparallel text data.
This setting, and more gener-ally the multilingual learning scenario, has beenfound advantageous for a variety of unsupervisedNLP tasks (Snyder et al, 2008; Cohen and Smith,2010; Berg-Kirkpatrick et al, 2010; Das and Petrov,2011).We consider globally normalized Markov randomfields (MRFs) as an alternative to directed modelsbased on multinomial distributions or locally nor-malized log-linear distributions.
This alternate pa-rameterization allows us to introduce correlated fea-tures that, at least in principle, depend on any partsof the hidden structure.
Such models, sometimescalled ?undirected,?
are widespread in supervisedNLP; the most notable instances are conditional ran-dom fields (Lafferty et al, 2001), which have en-abled rich feature engineering to incorporate knowl-edge and improve performance.
We conjecture thatthe ?features view?
of NLP problems is also moreappropriate in unsupervised settings than the con-trived, acyclic causal stories required by directedmodels.
Indeed, as we will discuss below, previouswork on multilingual POS induction has had to re-sort to objectionable independence assumptions toavoid introducing cyclic dependencies in the causalnetwork.While undirected models are formally attractive,they are computationally demanding, particularlywhen they are used generatively, i.e., as joint dis-tributions over input and output spaces.
Inferenceand learning algorithms for these models are usuallyintractable on realistic datasets, so we must resort toapproximations.
Our emphasis here is primarily onthe machinery required to support overlapping fea-tures, not on weakening independence assumptions,although we weaken them slightly.
Specifically, ourparameterization permits us to model the relation-ship between aligned words in any configuration,rather than just those that conform to an acyclic gen-erative process, as previous work in this area hasdone (?2).
We incorporate word prefix and suffixfeatures (up to four characters) in an undirected ver-sion of a model designed by Snyder et al (2008).Our experiments suggest that feature-based MRFsoffer advantages over the previous approach.2 Related WorkThe task of unsupervised bilingual POS inductionwas originally suggested and explored by Snyder etal.
(2008).
Their work proposes a joint model overpairs of tag sequences and words that can be under-stood as a pair of hidden Markov models (HMMs)64in which aligned words share states (a fixed andobservable word alignment is assumed).
Figure 1gives an example for a French-English sentence pair.Following Goldwater and Griffiths (2007), the tran-sition, emission and coupling parameters are gov-erned by Dirichlet priors, and a token-level col-lapsed Gibbs sampler is used for inference.
The hy-perparameters of the prior distributions are inferredfrom data in an empirical Bayesian fashion.Why repeat that catastrophe ?Pourquoi r?p?ter la m?me ?catastrophex1/y1 X2/y2 y3 y4 x5/y6x4/y5x3Figure 1: Bilingual Directed POS induction modelWhen word alignments are monotonic (i.e., thereare no crossing links in the alignment graph), themodel of Snyder et al is straightforward to con-struct.
However, crossing alignment links pose aproblem: they induce cycles in the tag sequencegraph, which corresponds to an ill-defined probabil-ity model.
Their solution is to eliminate such align-ment pairs (their algorithm for doing so is discussedbelow).
Unfortunately, this is a potentially a seri-ous loss of information.
Crossing alignments oftencorrespond to systematic word order differences be-tween languages (e.g., SVO vs. SOV languages).
Assuch, leaving them out prevents useful informationabout entire subsets of POS types from exploiting ofbilingual context.In the monolingual setting, Smith and Eisner(2005) showed similarly that a POS induction modelcan be improved with spelling features (prefixes andsuffixes of words), and Haghighi and Klein (2006)describe an MRF-based monolingual POS inductionmodel that uses features.
An example of such amonolingual model is shown in Figure 2.
Both pa-pers developed different approximations of the com-putationally expensive partition function.
Haghighiand Klein (2006) approximated by ignoring all sen-tences of length greater than some maximum, andthe ?contrastive estimation?
of Smith and Eisner(2005) approximates the partition function with a setEconomicdiscrepanciesANareVgrowingVFigure 2: Monolingual MRF tag model (Haghighiand Klein, 2006)of automatically distorted training examples whichare compactly represented in WFSTs.Das and Petrov (2011) also consider the prob-lem of unsupervised bilingual POS induction.
Theymake use of independent conventional HMM mono-lingual tagging models that are parameterized withfeature-rich log-linear models (Berg-Kirkpatrick etal., 2010).
However, training is constrained with tagdictionaries inferred using bilingual contexts derivedfrom aligned parallel data.
In this way, the complexinference and modeling challenges associated with abilingual tagging model are avoided.Finally, multilingual POS induction has also beenconsidered without using parallel data.
Cohen et al(2011) present a multilingual estimation techniquefor part-of-speech tagging (and grammar induction),where the lack of parallel data is compensated bythe use of labeled data for some languages and unla-beled data for other languages.3 ModelOur model is a Markov random field whose ran-dom variables correspond to words in two parallelsentences and POS tags for those words.
Let s =?s1, .
.
.
, sNs?
and t = ?t1, .
.
.
, tNt?
denote the twoword sequences; these correspond to Ns + Nt ob-served random variables.1 Let x and y denote the se-quences of POS tags for s and t, respectively.
Theseare the hidden variables whose values we seek to in-fer.
We assume that a word alignment is provided forthe sentences.
Let A ?
{1, .
.
.
, Ns} ?
{1, .
.
.
Nt}denote the word correspondences specified by thealignment.
The MRF?s unnormalized probability S1We use ?source?
and ?target?
but the two are completelysymmetric in our undirected framework.65assigns:S(s, t,x,y | A,w) =expw>(Ns?i=1fs-emit(si, xi) +Ns?i=2fs-tran(xi?1, xi)+Nt?i=1ft-emit(ti, yi) +Nt?i=2ft-tran(yi?1, yi)+?
(i,j)?Afalign-POS(xi, yj)?
?where w is a numerical vector of feature weightsthat parameterizes the model.
Each f?
corre-sponds to features on pairs of random variables;a source POS tag and word, two adjacent sourcePOS tags, similarly for the target side, and alignedsource/target POS pairs.
For simplicity, we let f de-note the sum of these five feature vectors.
(In mostsettings, each feature/coordinate will be specific toone of the five addends.)
In this paper, the featuresare indicators for each possible value of the pair ofrandom variables, plus prefix and suffix features forwords (up to four characters).
These features encodeinformation similar to the Bayesian bilingual HMMdiscussed in ?2.
Future work might explore exten-sions to this basic feature set.The marginal probability of the words is given by:p(s, t | A,w) =?x,y S(x,y, s, t | A,w)?s?,t?
?x,y S(s?, t?,x,y | A,w).Maximum likelihood estimation would chooseweights w to optimize a product of quantities likethe above, across the training data.A key advantage of this representation is that anyalignments may be present.
In directed models,crossing links create forbidden cycles in the graph-ical model.
For example, Figure 3 shows a cross-ing link between ?Economic discrepancies?
and ?di-vergences economiques.?
Snyder et al (2008) dealtwith this problem by deleting word correspondencesthat created cycles.
The authors deleted crossinglinks by considering each alignment link in the orderof the source sentence, deleting it if it crossed pre-vious links.
Deleting crossing links removes someinformation about word correspondence.divergences?conomiquesEconomicdiscrepanciesNAANLes ARTvont areV Vcroissant growingV VFigure 3: Bilingual tag model.4 Inference and Parameter LearningWhen using traditional generative models, such ashidden Markov models, the unsupervised settinglends itself well to maximizing joint log-likelihood,leading to a model that performs well (Snyder etal., 2008).
However, as we show in the followinganalysis, maximizing joint log-likelihood for a jointMarkov random field with arbitrary features suffersfrom serious issues which are related to the com-plexity of the optimized objective surface.4.1 MLE with Gradient DescentFor notational simplicity, we assume a single pair ofsentences s and t; generalizing to multiple traininginstances is straightforward.
The marginalized log-likelihood of the data given w isL(w) = log p(s, t | w)= log?x,y S(x,y, s, t | w)?s?,t?
?x,y S(x,y, s?, t?
| w).In general, maximizing marginalized log-likelihood is a non-concave optimization problem.Iterative hill-climbing methods (e.g., expectation-maximization and gradient-based optimization) willlead only to local maxima, and these may be quiteshallow.
Our analysis suggests that the problemis exacerbated when we move from directed toundirected models.
We next describe a simpleexperiment that gives insight into the problem.We created a small synthetic monolingual data setfor sequence labeling.
Our synthetic data consists ofthe following five sequences of observations: {(0 1 23) , (1 2 3 0) , (2 3 0 1) , (3 0 1 2) , (0 1 2 3)}.
We then66maximized the marginalized log-likelihood for twomodels: a hidden Markov model and an MRF.
Bothuse the same set features, only the MRF is globallynormalized.
The number of hidden states in bothmodels is 4.The global maximium in both cases would beachieved when the emission probabilities (or featureweights, in the case of MRF) map each observationsymbol to a single state.
When we tested whetherthis happens in practice, we noticed that it indeedhappens for hidden Markov models.
The MRF, how-ever, tended to use fewer than four tags in the emis-sion feature weights, i.e., for half of the tags, allemission feature weights were close to 0.
This ef-fect also appeared in our real data experiments.The reason for this problem with the MRF, we be-lieve, is that the parameter space of the MRF is un-derconstrained.
HMMs locally normalize the emis-sion probabilities, which implies that a tag cannot?disappear?
?a total probability mass of 1 must al-ways be allocated to the observation symbols.
WithMRFs, however, there is no such constraint.
Fur-ther, effective deletion of a state y requires zeroingout transition probabilities from all other states toy, a large number of parameters that are completelydecoupled within the model.Wh Wy rh ry yh yy eh ey ph py ah ayhthrhehahchhcthcrhceh p e y r W(a) likelihoodWhyyWhyr WhyeWhypWhyaWhyt WhycWhysWhyo e p a tWrWpWtWsWyWWyrWypWytWysW a p?c p?ae?a(b) contrastive objectiveFigure 4: Histograms of local optima found by opti-mizing the length neighborhood objective (a) and thecontrastive objective (b) on a synthetic dataset with8 sentences of length 7.
The weights are initializeduniformly at random in the interval [?1, 1].
We plotfrequency versus negated log-likelihood (lower hor-izontal values are better).
An HMM always finds asolution that uses all available tags.
The numbers atthe top are numbers of tags used by each local opti-mum.Our bilingual model is more complex than theabove example, and we found in preliminary exper-iments that the effect persists there, as well.
In thefollowing section, we propose a remedy to this prob-lem based on contrastive estimation (Smith and Eis-ner, 2005).4.2 Contrastive EstimationContrastive estimation maximizes a modified ver-sion of the log-likelihood.
In the modified version,it is the normalization constant of the log-likelihoodthat changes: it is limited to a sum over possible ele-ments in a neighborhood of the observed instances.More specifically, in our bilingual tagging model,we would define a neighborhood function for sen-tences, N(s, t) which maps a pair of sentences toa set of pairs of sentences.
Using this neighborhoodfunction, we maximize the following objective func-tion:Lce(w)= log p(S = s,T = t | S ?
N1(s),T ?
N2(t),w)= log?x,y S(s, t,x,y | w)?s?,t?
?N(s,t)?x,yS(s?, t?,x,y | w).
(1)We define the neighborhood function usinga cross-product of monolingual neighborhoods:N(s, t) = N1(s) ?
N1(t).
N1 is the ?dynasearch?neighborhood function (Potts and van de Velde,1995; Congram et al, 2002), used for contrastiveestimation previously by Smith (2006).
This neigh-borhood defines a subset of permutations of a se-quence s, based on local transpositions.
Specifically,a permutation of s is in N1(s) if it can be derivedfrom s through swaps of any adjacent pairs of words,with the constraint that each word only be movedonce.
This neighborhood can be compactly repre-sented with a finite-state machine of size O(Ns) butencodes a number of sequences equal to the NsthFibonacci number.Monolingual Analysis To show that contrastiveestimation indeed gives a remedy to the local max-imum problem, we return to the monolingual syn-thetic data example from ?4.1 and apply contrastiveestimation on this problem.
The neighborhood weuse is the dynasearch neighborhood.
In Figure 4b67we compare the maxima identified using MLE withthe monolingual MRF model to the maxima identi-fied by contrastive estimation.
The results are con-clusive: MLE tends to get stuck much more often inlocal maxima than contrastive estimation.Following an analysis of the feature weightsfound by contrastive estimation, we found that con-trastive estimation puts more weight on the transi-tion features than emission features, i.e., the tran-sition features weights have larger absolute valuesthan emission feature weights.
We believe that thiscould explain why contrastive estimation finds betterlocal maximum that plain MLE, but we leave explo-ration of this effect for future work.It is interesting to note that even though the con-trastive objective tends to use more tags available inthe dictionary than the likelihood objective does, themaximum objective that we were able to find doesnot correspond to the tagging that uses all availabletags, unlike with HMM, where the maximum thatachieved highest likelihood also uses all availabletags.4.3 Optimizing the Contrastive ObjectiveTo optimize the objective in Eq.
1 we use a genericoptimization technique based on the gradient.
Usingthe chain rule for derivatives, we can derive the par-tial derivative of the log-likelihood with respect to aweight wi:?Lce(w)?wi= Ep(X,Y|s,t,w)[fi]?
Ep(S,T,X,Y|S?N1(s),T?N1(t),w)[fi]The second term corresponds to a computationallyexpensive inference problem, because of the loopsin the graphical model.
This situation is differ-ent from previous work on linear chain-structuredMRFs (Smith and Eisner, 2005; Haghighi and Klein,2006), where exact inference is possible.
To over-come this problem, we use Gibbs sampling to obtainthe two expectations needed by the gradient.
Thistechnique is closely related to methods like stochas-tic expectation-maximization (Andrieu et al, 2003)and to contrastive divergence (Hinton, 2000).The training algorithm iterates between sam-pling part-of-speech tags and sampling permutationsof words to compute the expected value of fea-tures.
To sample permutations, the sampler iteratesthrough the sentences and decides, for each sen-tence, whether to swap a pair of adjacent tags andwords or not.
The Markov blanket for computingthe probability of swapping a pair of tags and wordsis shown in Figure 5.
We run the algorithm for afixed number (50) of iterations.
By testing on a de-velopment set, we observed that the accuracy mayincrease after 50 iterations, but we chose this smallnumber of iterations for speed.N Adivergences ?conomiquesA NEconomic discrepanciesVvontN Adivergences ?conomiquesA NEconomic discrepanciesAVt?s?von?are?Figure 5: Markov blanket of a tag (left) and of a pairof adjacent tags and words (right).In preliminary experiments we consideredstochastic gradient descent, with online updating.We found this led to low-accuracy local optima,and opted for gradient descent with batch updatesin our implementation.
The step size was chosen tolimit the maximum absolute value of the update inany weight to 0.1.
Preliminary experiments showedonly harmful effects from regularization, so we didnot use it.
These issues deserve further analysis andexperimentation in future research.5 ExperimentsWe next describe experiments using our undirectedmodel to unsupervisedly learn POS tags.With unsupervised part-of-speech tagging, it iscommon practice to use a full or partial dictionarythat maps words to possible part-of-speech tags.
Thegoal of the learner is then to discern which tag aword should take among the tags available for thatword.
Indeed, in all of our experiments we makeuse of a tag dictionary.
We consider both a com-plete tag dictionary, where all of the POS tags for allwords in the data are known,2 and a smaller tag dic-tionary that only provides possible tags for the 1002Of course, additional POS tags may be possible for a givenword that were not in evidence in our finite dataset.68most frequent words in each language, leaving theother words completely ambiguous.
The former dic-tionary makes the problem easier by reducing ambi-guity; it also speeds up inference.Our experiments focus on the Orwell novel 1984dataset for our experiments, the same data used bySnyder et al (2008).
It consists of parallel text ofthe 1984 novel in English, Bulgarian, Slovene andSerbian (Erjavec, 2004), totalling 5,969 sentences ineach language.
The 1984 datset uses fourteen part-of-speech tags, two of which denote punctuation.The tag sets for English and other languages haveminor differences in determiners and particles.We use the last 25% of sentences in the datasetas a test set, following previous work.
The datasetis manually annotated with part-of-speech tags.
Weuse automatically induced word alignments usingGiza++ (Och and Ney, 2003).
The data show veryregular patterns of tags that are aligned together:words with the same tag in two languages tend tobe aligned with each other.When a complete tag dictionary derived from theSlavic language data is available, the level of ambi-guity is very low.
The baseline of choosing randomtags for each word gives an accuracy in the low 80s.For English, we use an extended tag dictionary builtfrom the Wall Street Journal and the 1984 data.
TheEnglish tag dictionary is much more ambiguous be-cause it is obtained from a much larger dataset.
Therandom baseline gives an accuracy of around 56%.
(See Table 1.
)In our first set of experiments (?5.1), we performa ?sanity check?
with a monolingual version of theMRF that we described in earlier sections.
We com-pare it against plain HMM to assure that the MRFsbehave well in the unsupervised setting.In our second set of experiments (?5.2), we com-pare the bilingual HMM model from Snyder et al(2008) to the joint MRF model.
We show that usingan MRF has an advantage over an HMM model inthe partial tag dictionary setting.5.1 Monolingual ExperimentsWe turn now to two monolingual experiments thatverify our model?s suitability for the tagging prob-lem.Language Random HMM MRFBulgarian 82.7 88.9 93.5English 56.2 90.7 87.0Serbian 83.4 85.1 89.3Slovene 84.7 87.4 94.5Table 1: Unsupervised monolingual tagging accura-cies with complete tag dictionary on 1984 data.Supervised Learning As a very primitive com-parison, we trained a monolingual supervised MRFmodel to compare to the results of supervisedHMMs.
The training procedure is based on sam-pling, just like the unsupervised estimation methoddescribed in ?4.3.
The only difference is that there isno need to sample the words because the tags are theonly random variables to be marginalized over.
Ourmodel and HMM give very close performance withdifference in accuracy less than 0.1%.
This showsthat the MRF is capable of representing an equiva-lent model represented by the HMM.
It also showsthat gradient descent with MCMC approximate in-ference is capable of finding a good model with theweights initialized to all 0s.Unsupervised Learning We trained our modelunder the monolingual setting as a sanity check forour approximate training algorithm.
Our model un-der monolingual mode is exactly the same as themodels introduced in ?2.
We ran our model on the1984 data with the complete tag dictionary.
A com-parison between our result and monolingual directedmodel is shown in Table 1.
?Random?
is obtained bychoosing a random tag for each word according tothe tag dictionary.
?HMM?
is a Bayesian HMM im-plemented by (Snyder et al, 2008).
We also imple-mented a basic (non-Bayesian) HMM.
We trainedthe HMM with EM and obtained rsults similar to theBayesian HMM (not shown).5.2 Billingual ResultsTable 2 gives the full results in the bilingual settingfor the 1984 dataset with a partial tag dictionary.
Ingeneral, MRFs do better than their directed counter-parts, the HMMs.
Interestingly enough, removingcrossing links from the data has only a slight adverseeffect.
It appears like the prefix and suffix featuresare more important than having crossing links.
Re-69Language pair HMM MRF MRF w/o cross.
MRF w/o spell.English 71.3 73.3?
0.6 73.4?
0.6 67.4?
0.9Bulgarian 62.6 62.3?
0.3 63.8?
0.4 55.2?
0.5Serbian 54.1 55.7?
0.2 54.6?
0.3 47.7?
0.5Slovene 59.7 61.4?
0.3 60.4?
0.3 56.7?
0.4English 66.5 73.3?
0.3 73.4?
0.2 62.3?
0.5Slovene 53.8 59.7?
2.5 57.6?
2.0 52.1?
1.3Bulgarian 54.2 58.1?
0.1 56.3?
1.3 58.0?
0.2Serbian 56.9 58.6?
0.3 59.0?
1.2 55.1?
0.3English 68.2 72.8?
0.6 72.7?
0.6 65.7?
0.4Serbian 54.7 58.5?
0.6 57.7?
0.3 54.2?
0.3Bulgarian 55.9 59.8?
0.1 60.3?
0.5 55.0?
0.4Slovene 58.5 61.4?
0.3 61.6?
0.4 58.1?
0.6Average 59.7 62.9 62.5 56.5Table 2: Unsupervised bilingual tagging accuracies with tag dictionary only for the top 100 frequent words.?HMM?
is the result reported by (Snyder et al, 2008).
?MRF?
is our contrastive model averaged over tenruns.
?MRF w/o cross.?
is our model trained without crossing links, like Snyder et al?s HMM.
?MRFw/o spell.?
is our model without prefix and suffix features.
Numbers appearing next to results are standarddeviations over the ten runs.Language w/ cross.
w/o cross.French 73.8 70.3English 56.0 59.2Table 3: Effect of removing crossing links whenlearning French and English in a bilingual setting.moving the prefix and suffix features gives substan-tially lower results on average, results even belowplain HMMs.The reason that crossing links do not change theresults much could be related to fact that most ofthe sentence pairs in the 1984 dataset do not containmany crossing links (only 5% of links cross anotherlink).
To see whether crossing links do have an ef-fect when they come in larger number, we tested ourmodel on French-English data.
We aligned 10,000sentences from the Europarl corpus (Koehn, 2005),resulting in 87K crossing links out of a total of 673Klinks.
Using the Penn treebank (Marcus et al, 1993)and the French treebank (Abeille?
et al, 2003) toevaluate the model, results are given in Table 3.
It isevident that crossing links have a larger effect here,but it is mixed: crossing links improve performancefor French while harming it for English.6 ConclusionIn this paper, we explored the capabilities of jointMRFs for modeling bilingual part-of-speech mod-els.
Exact inference with dynamic programming isnot applicable, forcing us to experiment with ap-proximate inference techniques.
We demonstratedthat using contrastive estimation together with Gibbssampling for the calculation of the gradient of theobjective function leads to better results in unsuper-vised bilingual POS induction.Our experiments also show that the advantage ofusing MRFs does not necessarily come from the factthat we can use non-monotonic alignments in ourmodel, but instead from the ability to use overlap-ping features such as prefix and suffix features forthe vocabulary in the data.AcknowledgmentsWe thank the reviewers and members of the ARKgroup for helpful comments on this work.
This re-search was supported in part by the NSF throughgrant IIS-0915187 and the U. S. Army ResearchLaboratory and the U. S. Army Research Office un-der contract/grant number W911NF-10-1-0533.70ReferencesA.
Abeille?, L. Cle?ment, and F. Toussenel.
2003.
Buildinga treebank for French.
In A.
Abeille?, editor, Treebanks.Kluwer, Dordrecht.C.
Andrieu, N. de Freitas, A. Doucet, and M. I. Jordan.2003.
An introduction to MCMC for machine learn-ing.
Machine Learning, 50:5?43.T.
Berg-Kirkpatrick, A. Bouchard-Cote, J. DeNero, andD.
Klein.
2010.
Unsupervised learning with features.In Proceedings of NAACL.S.
B. Cohen and N. A. Smith.
2010.
Covariance in unsu-pervised learning of probabilistic grammars.
Journalof Machine Learning Research, 11:3017?3051.S.
B. Cohen, D. Das, and N. A. Smith.
2011.
Unsuper-vised structure prediction with non-parallel multilin-gual guidance.
In Proceedings of EMNLP.R.
K. Congram, C. N. Potts, and S. L. van de Velde.2002.
An iterated Dynasearch algorithm for thesingle-machine total weighted tardiness schedulingproblem.
Informs Journal On Computing, 14(1):52?67.D.
Das and S. Petrov.
2011.
Unsupervised part-of-speech tagging with bilingual graph-based projections.In Procedings of ACL.T.
Erjavec.
2004.
MULTEXT-East version 3: Multilin-gual morphosyntactic specifications, lexicons and cor-pora.
In Proceedings of LREC.S.
Goldwater and T. Griffiths.
2007.
A fully Bayesianapproach to unsupervised part-of-speech tagging.
InProc.
of ACL.A.
Haghighi and D. Klein.
2006.
Prototype-driven learn-ing for sequence models.
In Proceedings of HLT-NAACL.G.
E. Hinton.
2000.
Training products of experts byminimizing contrastive divergence.
Technical ReportGCNU TR 2000-004, University College London.P.
Koehn.
2005.
Europarl: A parallel corpus for statisti-cal machine translation.
In MT Summit 2005.J.
D. Lafferty, A. McCallum, and F. C. N. Pereira.
2001.Conditional random fields: Probabilistic models forsegmenting and labeling sequence data.
In Proceed-ings of ICML.M.
P. Marcus, B. Santorini, and M. A. Marcinkiewicz.1993.
Building a large annotated corpus of En-glish: The Penn treebank.
Computational Linguistics,19:313?330.F.
Och and H. Ney.
2003.
A systematic comparison ofvarious statistical alignment models.
ComputationalLinguistics, 29(1):19?51.C.
N. Potts and S. L. van de Velde.
1995.
Dynasearch?iterative local improvement by dynamic programming.Part I: The traveling salesman problem.
Technical re-port.N.
A. Smith and J. Eisner.
2005.
Contrastive estimation:training log-linear models on unlabeled data.
In Proc.of ACL.N.
A. Smith.
2006.
Novel Estimation Methods for Unsu-pervised Discovery of Latent Structure in Natural Lan-guage Text.
Ph.D. thesis, Johns Hopkins University.B.
Snyder, T. Naseem, J. Eisenstein, and R. Barzilay.2008.
Unsupervised multilingual learning for POStagging.
In Proceedings of EMNLP.71
