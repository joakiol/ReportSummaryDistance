Language Without A Central Pushdown StackCarson T. Schtitze and Peter A. ReichDepartment of LinguisticsUniversity of TorontoToronto, Canada M5S 1A1E-mail: carsontr@ dgp.toronto.eduAbstractWe will attempt to show how human performancelimitations on various types of syntactic embeddingconstructions in Germanic languages can be modelledin a relational network linguistic framework.
Afterarguing against centralized ata stores such aspushdown stacks and queues, we will demonstrate howinterconnections among levels of linguistic structurecan account for many of the psycholinguistic facts.1.
IntroductionThe long-range goal of our research project is todevelop and implement in computer simulation a uni-fied, psycholinguistically realistic model of languagebehaviour--specifically, language production, languagecomprehension, and language development.
Our modelis constructed in the framework of relational networklinguistics (also known as cognitive-stratificationalgrammar (Copeland & Davis 1980)), while alsoincorporating features of spreading activation (Collins& Loftus 1975), competition (Bates & MacWhinney1987), and other models.
In this paradigm, linguisticinformation is represented in the form of anasynchronous, massively parallel network (in the senseof Schnelle t al.
1988) whose nature can be seen asintermediate between that of mainstream connectionistnetworks and traditional generative grammars,incorporating aspects of both formalisms.
The basics ofrelational network grammars will be set forth in ?3.Our specific goal here is to describe our attempts osimulate the psychological facts about he production ofsyntactic embedding in a non-ad hoc way within theabove framework.
We will show that adequately ac-counting for human performance requires an ex-amination of at least two other epresentational levels(or strata) in addition to the syntax, namely the eventlevel and a level we call the lexotacticso In the processwe also hope to demonstrate the unified nature of ourtheory across linguistic levels.Particular consideration will be given to thecognitive data structures and processing mechanismsrequired.
Our claim in this regard is that temporarymemory in the form of a centralized ata store, whetherit be a pushdown stack, a queue, a collection of stacks,or whatever, is an inappropriate component formodelling human linguistic processing.
We will argueinstead for a large collection of very simple, localizedprocessing units which possess small amounts ofstorage by virtue of the possible states which they maybe in; in short, a network of finite-state devices.
Theseprocessors will be general, in the sense that they areuseful for cognitive processes outside the domain oflanguage.642.
The PhenomenaThe types of sentences we want to account forinclude centrally embedded and crossed serial structures(see below), in contrast o right- and left-branchingstructures.
This is one area where computationallinguists find it useful to look at human limitations,because if there were strict limits on the former forms ofembedding, they could allow us to build simpler lan-guage processors.
Many linguists continue to claim thatbecause of potentially indefinite mbedding, somethingmore powerful than a finite-state device is needed.
Butin some sense all computational linguistics is done onmachines which have a limited number of states, andsome of us believe that the human brain also has thisproperty.The matter first arose when Chomsky (1957)argued that natural language cannot be produced by afinite-state device, because of sentences with centralembedding toan arbitrary depth.
Chomsky's argumentwas that such sentences are all grammatical becausethey are "formed by processes of sentence constn~ctionso simple that even the most rudimentary Englishgrammar would contain them" (1957: 23).
WhatChomsky meant by rudimentary processes wasrecursion, and within the generative framework therewas no way to account for people's performancelimitations in terms of recursive phrase structure rules.The issue of whether embedding toarbitrary depthis part of natural anguage has been debated in thepsychological nd linguistic literature ver since (e.g.,Miller 1962; Labov 1973; de Roeck et al 1982).
Whileit is still a point of contention, recent carefullycontrolled experiments suggest that the human syntacticmechanism, without semantic or pragmatic ues, andwithout the aid of pencil and paper to constructsentences, does have a sharp limit of one or two levelsof embedding (Bruner & Cromer 1967; Reich & Dell1977).
In this paper we make no claim as to how manylevels people are able to process, only that here is somesmall, finite bound.
See Reich & Schiitze 1990 forfurther discussion.The following are some examples of the types ofsentences we are particularly concerned with.
Althoughsome of these are judged to be marginal or unacceptableby some informants, none of the subsequent discussiondepends on precisely where the limits of acceptabilityare drawn.?
Centrally embedded relative clause constructions inEnglish.
A clause B is centrally embedded within aclause A if and only if part of A occurs before B and theremainder ofA occurs after B.1.
The man that the dog chased ate the malt.2.
The man that the dog that bit the cat chased ated~e malt.. Verb-final complement constructions in German,which involve nested dependencies.
Nesteddependencies occur when the verbs at the end of thesentence are produced in reverse order from theirassociated arguments at the beginning.
Thus, they mustbe paired up by working from the edges in toward themiddle.3.
Die Manner haben Hans die Pferde fiitternlehren.\[the men have Hans the horses to feed taught\]"The men taught Hans to feed the horses."4.
Johanna hat die Manner tlans die Pferdefiitternlehren helfen.\[Joanna has the men Hans the horses to feed toteach helped\]"Joanna helped the men to teach tlans to feedthe horses."?
Verb-final complement constructions in Dutch,which involve crossed serial dependencies.
Crossedserial dependencies occur when the verbs at the end ofthe sentence are produced in the same sequence as theirassociated arguments at the beginning.
Sentences 5 and6 are synonymous with 3 and 4 respectively.5.
De mannen hebben Hans de paarden lerenvoeren.\[the men have Hans the horses taught o feed\]6.
Jeanine heeft de mannen Hans de paardenhelpen leren voeren.\[Joanna has the men Hans the horses helped toteach feed\] (Bach et al 1986)- Additional variations which are claimed to occur inSwiss-German (Shieber 1985): scntence 7 is in crossedserial order, 8 is in nested order, and 9 shows a variationof crossed serial order with the upper subject and dativeobject ransposed; all three are synonymous.
Analogousvariations on 10 are also claimed lo be possible.7.
Jan s',iit, das mer em ttans es huus hgilfed aastri-iehe.\[Jan says that we Hans the house helped to paint\]8.
Jan sait, das mer em tlans es huus aastriiehehi~lfed.\[Jan says that we Hans the house to paint helped\]9.
Jan ~sait, das em llans meres huus hfilfed aastri-iche.\[Jan says that Hans we the house helped to paint\]"Jan says that we helped llans to paint thehouse."10.
Jan ~t ,  das met d'ehind em ltans es huus 16ridhdlfe aastriiche.\[Jan says that we the children Hans the house lethelp paint\]"Jan says that we let the children help Hanspaint the house."3.
The Basics of Relationai NetworksA relational network consists of a collection ofnodes of various types, and connections between them~known as wires.
Language processing consists of ac-tivity flowing through the network or, in the case of ac-quisition, growing new wires and nodes in the network.Signals move in both directions along the wires fromone node to the next.
Each node is an independentlyoperating processing unit.
All nodes of the same typehave the ~me finite-state definition.
Their behaviourconsists of sending signals out along the wires to whichthey are connected, and possibly changing state.
Signalsare one of a small number of possible types, e.g.production (also called positive),feedback,failure (onegative) and anticipation.There are currently approximately 25 types ofnodes required in our system, expressing various ex-plicit and implicit features found in context-free phrasestructure grammars and other formalisms.
Each type ofnode is represented graphicaUy by a distinct shape.
Onebasic building block is the concatenation ode,equivalent to the phrase structure rule a ~' b c (seeFigure 1).
When a positive signal comes into the nodevia its top wire, which we label the a-wire, a positivesignal will be sent down the b-wire to produce the firstelement, and the node will change state to 'remember'this fact.
If the production of that element succeeds (asindicated by a positive feedback signal returning on theb-wire), a positive signal will be sent down the c-wire toproduce the second element in the concatenatedsequence, and the node will change skate again.
Uponthe c-wire's successful completion, positive feedback issent up the a-wire and the node returns to the initialstate, known as stale zero.FIGURE 1: Concatenation NodeOther major types of nodes include: disjunction,which allows a choice of one alternative among manypaths of production (e.g., a verb may be realized as"sing", "think", "walk", etc.
); precedence disjunction,which also allows a choice of alternatives, but triesthem one at a time, slopping as soon as one succeeds;conjunction (used in the Boolean sense), which simplyfires all its downward w~res at once when a productionsignal comes in the top; and inverse conjunction, whichrequires that two or more separate conditions must besignalling for a path to be followed (e.g., the pronoun"we" can only be produced if plural, first person, andnominative case are all being signalled).In its gross structure, the network we proposeconnects a general memory for events to a semotacticlevel, a lexotact ic  level, a syntact ic  level, aphonological evel, an articulatory level, and anauditory level.
The same type of structure is found in allthe levels, and except for the last two, all strata re usedin both production and understanding.
The syntaxdefines the sequence in which morPhemes arc'.
built intowords, phrases, and clauses.
The phonotactics definesthe sequence in which the sounds are combined into265clusters, syllables, and phonological feet.
Thesemotactics defines which concepts can be associatedwith which types of participants.
(For example, theconcept "fill" allows an agent and an affected partici-pant, among others, and the affected must be a type ofcontainer.)
The lexotactics constrains the choice ofvocabulary and its syntactic position on the basis ofwhich elements of an event are to be expressed.
Thelexicon of the language, itself in the form of a network,is connected to all major areas of language structure,and is in part what binds the levels to one another.
Eachword, morpheme, or idiom connects to meaning, syntax,and phonological representation.
I  addition, there aresome wires which pass control information betweenstrata.Thus, the major differences between relationalnetworks and connectionist ones h la Rummelhart andMcClelland (1986) are that the former use several typesof nodes with different behaviour, and the actions ofeach node depend on an internal state in addition to theincoming signals.
Furthermore, output signals can be amore complex function of the input signals than simpleweighted sums.4.
Right Branching: Iteration with Clean-UpWe now focus our attention on the production ofembedded clauses in relational networks.
Relationalnetwork syntax makes a strong distinction betweenright-branching clauses and centrally embedded ones.
(Left-branching is handled similarly to right-branching.
)In a right-branching structure, once an embeddedclause is complete, the superordinate clause will also becomplete (as in "This is the cat \[that killed the rat \[thatate the malt \[that was stored in the house \[that Jackbuilt\]\]\]\]").
In such sentences there is no need topreserve any information about he superordinate clauseonce an embedded one has begun; in fact, from apsychological point of view, it is undesirable--we donot wish to posit more demands on working memorythan are actually required to do the job.
Hence, in ourmodel, the superordinate clause is explicitly cleaned upbefore a right-branching embedded clause is begun.
Bycleaned up, we mean that the nodes involved in itsproduction are returned to state zero by sending apositive feedback signal up through the syntacticnetwork; that signal eventually reaches the top of theclause structure, at which point the embedded clausemay begin.For this clean-up to happen when it does, namelybefore the start of an embedded clause, the syntax must'know' whether the current syntactic onstituent is thefinal element of its superordinate clause.
There is noindependent  way for the syntax to make thisdetermination, since a direct object, say, might befollowed by an indirect object, or by any number ofprepositional phrases.
Therefore, we must addsomething elsewhere in the network to allow thiscondition to be recognized.
What we add is a controlwire from the lexotactics to the clause-heading ode inthe syntax, which will signal when the final participant(defined broadly) is underway.
(The lexotactics alreadyhas access to all participants of a clause right from itsstart, and is notified when each participant begins to berealized, for independent reasons.)
We note that in some66sense the syntax is no longer completely autonomous.Whether this is actually a drawback is partly an empir~ical psycholinguistic question; studies of Wernicke'saphasics could be relevant.5.
From Iteration to Recursion: CentralEmbeddingWe have now described how, in cases of right-branching, each clause starts with an essentially pristinesyntactic network, and therefore little more needs to besaid about how indefinite iteration is possible in a finite-state device.
The more difficult cases are centrallyembedded and crossed serial constructions.
In suchstructures, it is clear that a portion of some clauses isdelayed, i.e.
prevented from being realized, until sometime after its usual (simplex clause) position.
In mostcomputational approaches, a centralized (thoughpossibly implicit) data structure, be it a stack or aqueue, is used to store these elements until it comestime to real~e them.
We see a number of problems withthis approach.First of all, there is a certain intuitive appeal to thesuggestion that in people, currently active informationis distributed in shallow storage across the cognitivenetwork, rather than localized in a single, deepdata store.
(For instance, parking your car multipletimes leads you to forget previous parking spots, but nothow much money is in your wallet.)
Secondly, it is notclear what a central store should look like in order for itto account for both 'queue' and 'stack' types oflanguages, i.e.
crossed serial and strictly nested-orderones, especially since both orders may occur in a singlelanguage.
Models which have been proposed to handleboth cases have typically involved powerfulformalisms, uch as a sequence of stacks in Joshi's case(1985, 1990).
The resulting processor is more powerfulthan a pushdown automaton (it can recognize somestrictly context-sensitive languages), and it is our beliefthat structures in the brain are simply not this powerful.These two arguments are independent ofwhat has oftenseemed to be the central quarrel relational networktheorists have with other computational models, namelythat they allow for nesting to unlimited epth.
It mightbe simple enough, if somewhat arbitrary, to impose afinite limit on the size of stacks or queues in othermodels, and thus limit their generative power to thatwhich we believe humans are capable of.
However, thiswould not address our other objections.Our proposal is as follows.
When the cognitiverepresentation of an event becomes active, all itscomposing elements, i.e.
the action and all theparticipants in it, are fired simultaneously, However, therealization of those elements i held 'in check:' until thesyntax allows them to come out, one at a time.
Therealization of any given participant may involveproducing one or more clauses which describe it (e.g.,relative clauses, sentential complements), and theseexpansions may be produced before all the elements oftheir superordinate clause (in particular, the verb) havecome out.
However, since all aspects of an event firesimultaneously, the superordinate verb will havealready been signalled.
This is necessary because thechoice of verb may affect the realization of itsassociated participants; in particular, it may determinein which syntactic position they must be realized.
Forexample, the sentences "George borrowed the bookti'om Sue" and "Sue loaned the book to George" bothdescribe the same event, but the choice of verb hasforced George into subject position in the former case,and indirect object in the latter.The already-signalled superordinate v rb will havegenerated an anticipation signal up towards the verb-completion wire of the syntax.
(German and Dutchsyntax allow only one verb immediately after thesubject.
Any remaining verbal elements are placed atthe end of a clause, in what we will call the verb-completion position.)
It is the handling of this signal,and any subsequent verb anticipations which come up,which determines the eventual order of production.
(Weare assuming for simplicity that verbs come out only inverb or verb-completion positions, although possibly aspart of the 'wrong' clause.)
The limit on how manynested clauses are possible turns out to be totallyunrelated to this structure, deriving instead from thefinite-state definitions of the nodes themselves ( ee ?
7).How does the network keep track of which orderverbs should come out in?
The dashed box of Figure 2shows the relevant structure.
This structure contains nplaceholder nodes, where n = 1 + the number ofpossible embeddings in the language.
(In thisdiscussion, we will assume n = 3.)
The placeholders arelabelled pl, p2 and p3 in the figure.
Each is connectedto every verb of the language, and acts as a 'slot' forremembering one verb.
Whenever a verbocompletion isrequired by the syntax, the network attempts to realizethe first verb slot, then the second if the first was empty,and ,so on.
These realizations will succeed if verbs havealready been signalled from the events which theydescribe, the E's in the diagram.
A verb signalling inthis way tries to 'turn on' one of the positions in thesequence of possible verb-completions, and succeeds atdoing so if and only if no other verb has already filledthat position.
In the case of crossed serial orders uch asDutch, verbs try to occupy the first slot first, exactly asto syntaxerb-completion(Dc03c0 (Dc-o_fl)d2:tactics to mo rphologyFIGURE 2: Fragment of the network for Dutchshown in Figure 2.
For nested-order languages such asGerman and English, the wiring from r's to p's isexactly reversed, so that a verb first tales to fill the lastavailable slot, then works its way forward to em'lierslots until it finds one unoccupied (see Figure 3, whichwould replace the dashed box of Figure 2).,/r3 i r2 Ir lFIGURE 3: GetTnan network fragment6.
A Detailed ExampleAs an example, let us consider the production ofthe Dutch sentence 6, assuming sentences of suchcomplexity to be possible under some circumstances.The sentence involves three events, represented by theconjunction odes El, E2 and E3 in Figure 2.
Theevents are: <doanna helping the men>> ~1), <<the menteaching Hans>> (E2), and <4~ans feeding the horses>>(E3).
These events will fire in the order just stated, sincethis is how they are hierarchically arranged in the eventstructure (E2 and E3 each modify or constitute a par-ticipant of the next higher event).
As production begins,"Jeanine", "helpen" and E2 fire simultaneously.
"Jeanine" is realized immediately, but "helpen" cannotbe immediately realized, for the tollowing reason.
Thefirst verb position of the clause has been filled by theauxiliary verb "heeft", which is the realization of asemantic element which marks this scenario as havingtaken place in the past.
Since "helpen" could not berealized in this position, it must come out in the verb-completion position.
Therefore, itcauses an anticipationto fire up from inverse conjunction ode il towards thatposition.
That anticipation will be directed up to thefirst verb position, namely placeholder node p 1, by therouting node rl.
(A routing node is an invertedprecedence disjunction which attempts to send a signalup its leftmost wire to the placeholder at the other end.If that fails, it tries to send the signal up its remainingwires in order from left to right.)
Since "helpen" is thefirst verb to signal in this sentence, pl will accept heanticipation and remember which of its wires the signalcame in on.While all this is taking place, the syntax has begunrealizing the subordinate vent E2, <<the men teachttans>>.
"Mannen" can be immediately realized.
"Leren"cannot, but it will again send an anticipation signal, thistime via i2 up into the verb structure.
This will berouted by r2 to the first position (pl) once again, butthis time the anticipation will be rejected, because thereis already a verb pending for this position.
A failuresignal is sent down by pl to signal this fact, and r2, see-ing that cancellation, ow tries sending the anticipationup to the second position, p2.
This time the anticipationwill be accepted, since nothing has previously come up467to this point, and the verb's wire will be remembered.Similarly the third verb, "voeren", will be routed to thetlfird slot when E3 fires, and remembered byp3.Now, as E3 is realized by the syntax, the syntaxwill license a verb-completion following the object"paarden", since there are no more participants in thelowest clause.
As the verb-completion signal comesdown, it passes through the precedence disjunction odedl, which tries each of its output wires in turn from leftto right until one succeeds.
Its first output leads to pl,which will succeed (since an anticipation has previouslycome up to it), and finally permit he first verb, namely"helpen", to be phonetically realized.
(Node plremembered the wire which led to the morphologicalrepresentation f that verb.)
Positive feedback from thisproduction will trigger pl to return to state zero, andpass the feedback on up.
Since the first alternative wireof the precedence disjunction dl succeeded, none of theothers will be touched.
The end of the verb-completionis signalled by dl, to which the syntax responds by'unwinding' the complement clause loop (not shown inFigure 2) once.We are now at the point of having finished all theparticipants in the middle clattse (the ~the men teachingHans>~ clause), so all that remains at this level is theverb.
Again the syntax signals the verb-completionwire, again the precedence disjunction dl tries the firstpath, but this time it will fail, because no verb is waitingat pl.
In state zero, this placeholder node sends anegative signal up to the precedence disjunction, whichmust therefore try its next wire.
This one will succeed,producing the verb which was remembered by p2,namely "leren".
Similarly, as the syntax unwinds oncemore and signals for a verb-completion ce more, theprecedence disjunction will eventually find the verbheld by p3 at the third position, namely "voeren", andthe sentence will be complete.Incidentally, a close analogue to this method can beused to account for the order of appearance of nounphrases across embedded clauses as well.
In the case ofEnglish, we can use a structure like Figure 3 to holdonto the direct object of a superordinate clause untilafter the direct object of an embedded relative clausehas come out.
For exanrple, in "The man who liked thedog hated the cat", "the cat" is the first direct object obe made available by the event structure, since it is aparticipant in the superordinate event, but the first directobject to come out is "the dog", so object NPs arerealized in last-in, first-out order.
Thus the handling ofparticipants provides additional motivation for the typesof nodes and structure tlmt we have posited to handleverbs.7.
Some Theoretical IssuesIn a syntax in which nodes are finite state devices,the job of remembering the status of a clause falls oneach and every node in the network, as follows.Suppose a node requires a set of s states to handleprocessing within one clause.
Then in the worst case,for each of those states it will need a copy of the entireset to use for processing an embedded clause.
Each setcorresponds toremembering a different place where thesuperordinate clause was left off.
The total number ofstates in the node will be s n, where n once again is thenumber of possible pending clauses.
This approach isanalogous to a programming language that does notallow subroutines.
In such a language, a copy of arecurring block of code must appear at each place whereit could be needed.
This tendency to exponential growthcould account for why languages seem to impose suchsevere restrictions on the amount of central embeddingor crossed serial dependency.The interesting and crucial thing about he way theprocess described in ?6 was carried out is that themechanism for remembering verbs was totallyindependent of the mechanism which ordered them foroutput.
That is, while a particular set of nodes eachremembered which one verb was associated with itsslot, the connections between nodes determined theorder of output relative to the order of signalling.
Thismeans that all the various orderings which occur cross-linguistically can be accounted for by the sameinventory of nodes.
No additional data structure is re-quired; all that we must do to 'convert' from Dutch toGerman word order is to rewire the connectionsbetween the upward routing nodes and the placeholdernodes, so that slots are tried in exactly the oppositeorder.
To handle the fact that a single language (likeSwiss-German) may use different orders depending onsyntactic ontext (or even stylistic factors), all we needto do is have the verb-completion wire branch into allthe options, each of which will have its own precedencedisjunction and set of placeholder nodes.
The upwardanticipation from a particular verb will be sentsimultaneously to all the different orderings, and thesyntax will choose the appropriate one and cancel theothers.The close symmetry between German and Dutch inour model would seem to be a psycholinguisticshortcoming, given Bach et al's (1986) result thatDutch is easier to process than German.
However, webelieve that, to the extent that their results aremeaningful, they are not  attributable toa queue versusstack difference, but rather to something along the linesof Joshi's (1990) proposed Principle of PartialInterpretation, whereby the syntax can't forget about aclause unless an argument slot to place it in has alreadybeen processed.One could argue that, viewed somewhat abstractly,our collection of nodes and wires in fact implements afinite-sized convertible queue/stack.
Our basic responseto this is to point out once again that that is essentiallyan artifact, having been built up out of independent,lower-level components.
As for the particular size('depth') of the data structure being stipulated, thisreally is not troubling.
Note that such a structure couldbe any size--nothing in the node definitions would limitit to size three or four, since expanding it only requiresadding more nodes.
However, more than some smallfixed number of verbs can never be realized nestedly,because the syntax simply will not be able to call forthem.
As described above, the definitions of the nodeswhich the syntax makes use of simply break down aftera couple of nestings.
It is therefore reasonable topostu-late that he acquisition process would have no reason tobuild the verb structure any larger than the syntax hadever called for.
And with regard to the node definitions68themselves being arbitrary in their maximum nestinglimitations, this is certainly true in the sense that wedefine them to Ix'.
precisely powerful enough to do whathumans can do with syntax.
(It is possible to imaginethat humans could have evolved with the capacity for,say, one fewer or one more nesting; we would notexpect hat number to follow from mlything else.)
Thepoint once again is that this limitation is distributedthroughout the network, rather than being a function ofthe total amount of storage available.8.
Areas for Further Research"Fhrough detailed computational modelling we havemade significant progress in analyzing our theory,finding flaws and oversights, and making it morerigorous.
We believe that, with the complexity of lin-guistic ruodels as it is today, no theory can lay strongclaims to adequacy, completeness, correcmess, etc.unless it has been tested in a computer simulation.Having reworked the theory several times over a periodof only a few months, we cannot stress this pointvigorously enough.There are Several important questions which ourresearch as not yet addressed.
One m~or issue involv-ing high-level control between strata is that of preciselywhere and how the decision is made that a subordinateclause is to be I~oduced.
in a highly interconnectedsemantic network of events, there will almost always be'extra' information available which could be used toexpand the desc~ption of any participant in the tbrrn ofa relative claus.
We believe that ninny factors go intothe decision as to whether or not to carry out thisexpansion.
The~ would include pragmatic issues suchas the purpose of communication, urgency of theconversation, amount of relevant knowledge believed tobe possessed by the audience, etc.
Even assuming wecan wire in the relevant decision criteria, it still remainsto show how the lexotactic and syntactic strata arenotified ttmt an additional clause is being produced.
Onepossibility is that the tiring of a new action (and/or theassociated verb) is the trigger.Additionally, if we look back at the stated goals ofthe theory in our introduction, it is evident hat only oneof the three main areas of language behaviour has beenexplored, namely production.
The whole question ofhow this system works for comprehension has barelybeen addressed for relational network models ingeneral.
'I\]~e specific issue of embo~ding is sure to addmore wrinkles.
Furthermore, accounting for theacquisition of both iteration and recursion is a serioushurdle for any connectionist model of language toovercome.
In our case, it will involve the networkgrowing new structure, in addition to modifyingconnection weights.
So far we have concentrated onconvincing ourselves that a viable language processorcan be created in network form, whereas connectionistsmore often are concerned with exploring how muchinformation can be acquired when starting from a tabularasa.
While we have no clear ideas on how acquisitionshould proceed in our framework, we believe we haveat least come up with a possible structure as an end-goalfor future acquisition models to strive towards.AcknowledgementsWe would like to thank Elizabeth Cowper, JanWiebe and Graeme Hirst for their comments on a draftof this paper.
This research was supported by a grant tothe second author from the Social Sciences andHumanities Research Council of Canada.ReferencesBach, Emmon, Colin Brown & William Marslen-Wilson(1986) Crossed and nested ependencies in German andDutch: A psycholinguistic study.
Language andCognitive Processes 1:4, 249-262.Bates, Elizabeth & Brian MacWhirmey (1987) Competition,variation, and language learning.
In B. MaeWhinney,Mechanisms of language acquisition, Hillsdale, N.J.:Lawrence Erlbaum, 157-193.Bruner, Jerome S. & R. Cromer (1967) An unpunished studyof eye movements reported in Harvard Center forCognitive Studies Seventh Annual Report, p. 7.Chomsky, Noanl (1957) Syntact& Structures.
The Hague:Mouton.Collins, Allan M. & Elizabeth Lofms (1975) A spreadingactivation model of semantic processing.
PsychologicalReview 82, 407-428.Copeland, James E. & Philip W. Davis, Eds.
(1980) Papers inCognitive-Stratificational Linguistics.
Rice UniversityStudies, Vol.
66.
Houston, TX: Rice University.Joshi, Aravind K. (1985) Tree adjoining grammars: Howmuch context-sensitivity is required to providereasonable structural descriptions?
In D. Dowry, L.Karttunen & A. Zwicky, eds., Natural LanguageParsing: Pn2cchologieal, computational and theoreticalperspectives, New York: Cambridge University Press,206-250.Joshi, Aravind K. (1990) Processing crossed and nesteddependencies: An automaton perspective on thepsycholinguistic results.
Language and CognitiveProcesses, to appear.Labov, William (1973) The place of linguistics research inAmerican society.
In Eric Hamp, ed., Themes inlinguistics: The 1970s, The Hague: Mouton.Miller, George A.
(1962) Some psychological studies ofgrammar.
American Psychologist 17, 748-762.Reich, P.A.
& G.S.
Dell (1977) Finiteness and embedding.
InR.J.
DiPietro & E.L. Blansett, Jr., eds., The thirdLACUSforum, Columbia, S.C.: Hornbeam Press, 438-447.Reich, Peter A.
& Carson T. Schiltze (1990) SyntacticEmbedding: What Can People Really Do?
Workingpaper in the Computer Applications Group, Departmentof Linguistics, University of Toronto.de Roeck, Anne, Roderick Johnson, Margaret King, MichaelRosner, Geoffrey Sampson & Nino Varile (1982) AMyth About Centre-Embexlding.
Lingua 58, 327-340.Rumelhart, David E. & McClelland, James L. (1986) Paralleldistributed processing: Explorations in themicrostructures of cognition.
Cambridge, MA: MITPress.Schnelle, Helmut (moderator), with (alphabetically) G.Cottrell, P. Dey, J. Diederich, P. A. Reich, L. Shastri &A. Yonezawa (panelists) (1988) Panel: ParallelProcessing in Computational Linguistics.
In DrnesVargha, ed., Proceedings of Coling Budapest,Association for Computational Linguistics, 595-598.Shieber, Stuart M. (1985) Evidence against the context-freeness of natural language.
Linguistics and Philosophy8, 333-343.669
