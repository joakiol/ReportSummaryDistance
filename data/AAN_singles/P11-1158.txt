Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1577?1585,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsEfficient CCG Parsing: A* versus Adaptive SupertaggingMichael AuliSchool of InformaticsUniversity of Edinburghm.auli@sms.ed.ac.ukAdam LopezHLTCOEJohns Hopkins Universityalopez@cs.jhu.eduAbstractWe present a systematic comparison and com-bination of two orthogonal techniques forefficient parsing of Combinatory CategorialGrammar (CCG).
First we consider adap-tive supertagging, a widely used approximatesearch technique that prunes most lexical cat-egories from the parser?s search space usinga separate sequence model.
Next we con-sider several variants on A*, a classic exactsearch technique which to our knowledge hasnot been applied to more expressive grammarformalisms like CCG.
In addition to standardhardware-independent measures of parser ef-fort we also present what we believe is the firstevaluation of A* parsing on the more realisticbut more stringent metric of CPU time.
By it-self, A* substantially reduces parser effort asmeasured by the number of edges consideredduring parsing, but we show that for CCG thisdoes not always correspond to improvementsin CPU time over a CKY baseline.
CombiningA* with adaptive supertagging decreases CPUtime by 15% for our best model.1 IntroductionEfficient parsing of Combinatorial Categorial Gram-mar (CCG; Steedman, 2000) is a longstanding prob-lem in computational linguistics.
Even with practi-cal CCG that are strongly context-free (Fowler andPenn, 2010), parsing can be much harder than withPenn Treebank-style context-free grammars, sincethe number of nonterminal categories is generallymuch larger, leading to increased grammar con-stants.
Where a typical Penn Treebank grammarmay have fewer than 100 nonterminals (Hocken-maier and Steedman, 2002), we found that a CCGgrammar derived from CCGbank contained nearly1600.
The same grammar assigns an average of 26lexical categories per word, resulting in a very largespace of possible derivations.The most successful strategy to date for efficientparsing of CCG is to first prune the set of lexi-cal categories considered for each word, using theoutput of a supertagger, a sequence model overthese categories (Bangalore and Joshi, 1999; Clark,2002).
Variations on this approach drive the widely-used, broad coverage C&C parser (Clark and Cur-ran, 2004; Clark and Curran, 2007).
However, prun-ing means approximate search: if a lexical categoryused by the highest probability derivation is pruned,the parser will not find that derivation (?2).
Since thesupertagger enforces no grammaticality constraintsit may even prefer a sequence of lexical categoriesthat cannot be combined into any derivation (Fig-ure 1).
Empirically, we show that supertagging im-proves efficiency by an order of magnitude, but thetradeoff is a significant loss in accuracy (?3).Can we improve on this tradeoff?
The line of in-vestigation we pursue in this paper is to considermore efficient exact algorithms.
In particular, wetest different variants of the classical A* algorithm(Hart et al, 1968), which has met with success inPenn Treebank parsing with context-free grammars(Klein and Manning, 2003; Pauls and Klein, 2009a;Pauls and Klein, 2009b).
We can substitute A* forstandard CKY on either the unpruned set of lexi-cal categories, or the pruned set resulting from su-1577Valid supertag-sequencesValid parsesHigh scoringsupertagsHigh scoringparsesDesirable parsesAttainable parsesFigure 1: The relationship between supertagger andparser search spaces based on the intersection of their cor-responding tag sequences.pertagging.
Our empirical results show that on theunpruned set of lexical categories, heuristics em-ployed for context-free grammars show substantialspeedups in hardware-independent metrics of parsereffort (?4).
To understand how this compares to theCKY baseline, we conduct a carefully controlled setof timing experiments.
Although our results showthat improvements on hardware-independent met-rics do not always translate into improvements inCPU time due to increased processing costs that arehidden by these metrics, they also show that whenthe lexical categories are pruned using the output ofa supertagger, then we can still improve efficiencyby 15% with A* techniques (?5).2 CCG and Parsing AlgorithmsCCG is a lexicalized grammar formalism encodingfor each word lexical categories which are eitherbasic (eg.
NN, JJ) or complex.
Complex lexicalcategories specify the number and directionality ofarguments.
For example, one lexical category (ofover 100 in our model) for the transitive verb like is(S\NP2)/NP1, specifying the first argument as anNP to the right and the second as an NP to the left.
Inparsing, adjacent spans are combined using a smallnumber of binary combinatory rules like forward ap-plication or composition on the spanning categories(Steedman, 2000; Fowler and Penn, 2010).
In thefirst derivation below, (S\NP )/NP and NP com-bine to form the spanning category S\NP , whichonly requires an NP to its left to form a completesentence-spanning S. The second derivation usestype-raising to change the category type of I.I like teaNP (S\NP)/NP NP>S\NP<SI like teaNP (S\NP)/NP NP>TS/(S\NP)>BS/NP>SBecause of the number of lexical categories and theircomplexity, a key difficulty in parsing CCG is thatthe number of analyses for each span of the sentencequickly becomes extremely large, even with efficientdynamic programming.2.1 Adaptive SupertaggingSupertagging (Bangalore and Joshi, 1999) treats theassignment of lexical categories (or supertags) as asequence tagging problem.
Once the supertaggerhas been run, lexical categories that apply to eachword in the input sentence are pruned to contain onlythose with high posterior probability (or other figureof merit) under the supertagging model (Clark andCurran, 2004).
The posterior probabilities are thendiscarded; it is the extensive pruning of lexical cate-gories that leads to substantially faster parsing times.Pruning the categories in advance this way has aspecific failure mode: sometimes it is not possibleto produce a sentence-spanning derivation from thetag sequences preferred by the supertagger, since itdoes not enforce grammaticality.
A workaround forthis problem is the adaptive supertagging (AST) ap-proach of Clark and Curran (2004).
It is based on astep function over supertagger beam ratios, relaxingthe pruning threshold for lexical categories when-ever the parser fails to find an analysis.
The pro-cess either succeeds and returns a parse after someiteration or gives up after a predefined number ofiterations.
As Clark and Curran (2004) show, mostsentences can be parsed with a very small number ofsupertags per word.
However, the technique is inher-ently approximate: it will return a lower probabilityparse under the parsing model if a higher probabil-ity parse can only be constructed from a supertagsequence returned by a subsequent iteration.
In thisway it prioritizes speed over accuracy, although thetradeoff can be modified by adjusting the beam stepfunction.2.2 A* ParsingIrrespective of whether lexical categories are prunedin advance using the output of a supertagger, theCCG parsers we are aware of all use some vari-1578ant of the CKY algorithm.
Although CKY is easyto implement, it is exhaustive: it explores all pos-sible analyses of all possible spans, irrespective ofwhether such analyses are likely to be part of thehighest probability derivation.
Hence it seems nat-ural to consider exact algorithms that are more effi-cient than CKY.A* search is an agenda-based best-first graphsearch algorithm which finds the lowest cost parseexactly without necessarily traversing the entiresearch space (Klein and Manning, 2003).
In contrastto CKY, items are not processed in topological orderusing a simple control loop.
Instead, they are pro-cessed from a priority queue, which orders them bythe product of their inside probability and a heuris-tic estimate of their outside probability.
Providedthat the heuristic never underestimates the true out-side probability (i.e.
it is admissible) the solution isguaranteed to be exact.
Heuristics are model specificand we consider several variants in our experimentsbased on the CFG heuristics developed by Klein andManning (2003) and Pauls and Klein (2009a).3 Adaptive Supertagging ExperimentsParser.
For our experiments we used the generativeCCG parser of Hockenmaier and Steedman (2002).Generative parsers have the property that all edgeweights are non-negative, which is required for A*techniques.1 Although not quite as accurate as thediscriminative parser of Clark and Curran (2007) inour preliminary experiments, this parser is still quitecompetitive.
It is written in Java and implementsthe CKY algorithm with a global pruning thresholdof 10?4 for the models we use.
We focus on twoparsing models: PCFG, the baseline of Hockenmaierand Steedman (2002) which treats the grammar as aPCFG (Table 1); and HWDep, a headword depen-dency model which is the best performing model ofthe parser.
The PCFG model simply generates a treetop down and uses very simple structural probabili-ties while the HWDep model conditions node expan-sions on headwords and their lexical categories.Supertagger.
For supertagging we used Den-nis Mehay?s implementation, which follows Clark1Indeed, all of the past work on A* parsing that we are aware ofuses generative parsers (Pauls and Klein, 2009b, inter alia).
(2002).2 Due to differences in smoothing of thesupertagging and parsing models, we occasionallydrop supertags returned by the supertagger becausethey do not appear in the parsing model 3.Evaluation.
All experiments were conducted onCCGBank (Hockenmaier and Steedman, 2007), aright-most normal-form CCG version of the PennTreebank.
Models were trained on sections 2-21,tuned on section 00, and tested on section 23.
Pars-ing accuracy is measured using labelled and unla-belled predicate argument structure recovery (Clarkand Hockenmaier, 2002); we evaluate on all sen-tences and thus penalise lower coverage.
All tim-ing experiments reported in the paper were run on a2.5 GHz Xeon machine with 32 GB memory and areaveraged over ten runs4.3.1 ResultsSupertagging has been shown to improve the speedof a generative parser, although little analysis hasbeen reported beyond the speedups (Clark, 2002)We ran experiments to understand the time/accuracytradeoff of adaptive supertagging, and to serve asbaselines.Adaptive supertagging is parametrized by a beamsize ?
and a dictionary cutoff k that bounds thenumber of lexical categories considered for eachword (Clark and Curran, 2007).
Table 3 shows boththe standard beam levels (AST) used for the C&Cparser and looser beam levels: AST-covA, a sim-ple extension of AST with increased coverage andAST-covB, also increasing coverage but with bet-ter performance for the HWDep model.Parsing results for the AST settings (Tables 4and 5) confirm that it improves speed by an order ofmagnitude over a baseline parser without AST.
Per-haps surprisingly, the number of parse failures de-creases with AST in some cases.
This is because theparser prunes more aggressively as the search spaceincreases.52http://code.google.com/p/statopenccg3Less than 2% of supertags are affected by this.4The timing results reported differ from an earlier draft sincewe used a different machine5Hockenmaier and Steedman (2002) saw a similar effect.1579Expansion probability p(exp|P ) exp ?
{leaf, unary, left-head, right-head}Head probability p(H|P, exp) H is the head daughterNon-head probability p(S|P, exp,H) S is the non-head daughterLexical probability p(w|P )Table 1: Factorisation of the PCFG model.
H ,P , and S are categories, and w is a word.Expansion probability p(exp|P, cP#wP ) exp ?
{leaf, unary, left-head, right-head}Head probability p(H|P, exp, cP#wP ) H is the head daughterNon-head probability p(S|P, exp,H#cP#wP ) S is the non-head daughterLexcat probability p(cS |S#P,H, S) p(cTOP |P=TOP )Headword probability p(wS |cS#P,H, S,wP ) p(wTOP |cTOP )Table 2: Headword dependency model factorisation, backoff levels are denoted by ?#?
between conditioning variables:A # B # C indicates that P?
(.
.
.
|A,B,C) is interpolated with P?
(.
.
.
|A,B), which is an interpolation of P?
.
.
.
|A,B)and P?
(.
.
.
|A).
Variables cP and wP represent, respectively, the head lexical category and headword of category P .Condition Parameter Iteration 1 2 3 4 5 6AST?
(beam width) 0.075 0.03 0.01 0.005 0.001k (dictionary cutoff) 20 20 20 20 150AST-covA?
0.075 0.03 0.01 0.005 0.001 0.0001k 20 20 20 20 150 150AST-covB?
0.03 0.01 0.005 0.001 0.0001 0.0001k 20 20 20 20 20 150Table 3: Beam step function used for standard (AST) and high-coverage (AST-covA and AST-covB) supertagging.Time(sec) Sent/sec Cats/word Fail UP UR UF LP LR LFPCFG 290 6.6 26.2 5 86.4 86.5 86.5 77.2 77.3 77.3PCFG (AST) 65 29.5 1.5 14 87.4 85.9 86.6 79.5 78.0 78.8PCFG (AST-covA) 67 28.6 1.5 6 87.3 86.9 87.1 79.1 78.8 78.9PCFG (AST-covB) 69 27.7 1.7 5 87.3 86.2 86.7 79.1 78.1 78.6HWDep 1512 1.3 26.2 5 90.2 90.1 90.2 83.2 83.0 83.1HWDep (AST) 133 14.4 1.5 16 89.8 88.0 88.9 82.6 80.9 81.8HWDep (AST-covA) 139 13.7 1.5 9 89.8 88.3 89.0 82.6 81.1 81.9HWDep (AST-covB) 155 12.3 1.7 7 90.1 88.7 89.4 83.0 81.8 82.4Table 4: Results on CCGbank section 00 when applying adaptive supertagging (AST) to two models of a generativeCCG parser.
Performance is measured in terms of parse failures, labelled and unlabelled precision (LP/UP), recall(LR/UR) and F-score (LF/UF).
Evaluation is based only on sentences for which each parser returned an analysis.3.2 Efficiency versus AccuracyThe most interesting result is the effect of thespeedup on accuracy.
As shown in Table 6, thevast majority of sentences are actually parsed witha very tight supertagger beam, raising the questionof whether many higher-scoring parses are pruned.66Similar results are reported by Clark and Curran (2007).Despite this, labeled F-score improves by up to 1.6F-measure for the PCFG model, although it harmsaccuracy for HWDep as expected.In order to understand this effect, we filtered sec-tion 00 to include only sentences of between 18and 26 words (resulting in 610 sentences) for which1580Time(sec) Sent/sec Cats/word Fail UP UR UF LP LR LFPCFG 326 7.4 25.7 29 85.9 85.4 85.7 76.6 76.2 76.4PCFG (AST) 82 29.4 1.5 34 86.7 84.8 85.7 78.6 76.9 77.7PCFG (AST-covA) 85 28.3 1.6 15 86.6 85.5 86.0 78.5 77.5 78.0PCFG (AST-covB) 86 27.9 1.7 14 86.6 85.6 86.1 78.1 77.3 77.7HWDep 1754 1.4 25.7 30 90.2 89.3 89.8 83.5 82.7 83.1HWDep (AST) 167 14.4 1.5 27 89.5 87.6 88.5 82.3 80.6 81.5HWDep (AST-covA) 177 13.6 1.6 14 89.4 88.1 88.8 82.2 81.1 81.7HWDep (AST-covB) 188 12.8 1.7 14 89.7 88.5 89.1 82.5 81.4 82.0Table 5: Results on CCGbank section 23 when applying adaptive supertagging (AST) to two models of a CCG parser.?
Cats/word Parses %0.075 1.33 1676 87.60.03 1.56 114 6.00.01 1.97 60 3.10.005 2.36 15 0.80.001k=150 3.84 32 1.7Fail 16 0.9Table 6: Breakdown of the number of sentences parsedfor the HWDep (AST) model (see Table 4) at each ofthe supertagger beam levels from the most to the leastrestrictive setting.we can perform exhaustive search without pruning7,and for which we could parse without failure at allof the tested beam settings.
We then measured thelog probability of the highest probability parse foundunder a variety of beam settings, relative to the logprobability of the unpruned exact parse, along withthe labeled F-Score of the Viterbi parse under thesesettings (Figure 2).
The results show that PCFG ac-tually finds worse results as it considers more of thesearch space.
In other words, the supertagger can ac-tually ?fix?
a bad parsing model by restricting it toa small portion of the search space.
With the moreaccurate HWDep model, this does not appear to bea problem and there is a clear opportunity for im-provement by considering the larger search space.The next question is whether we can exploit thislarger search space without paying as high a cost inefficiency.7The fact that only a subset of short sentences could be exhaus-tively parsed demonstrates the need for efficient search algo-rithms.79?80?81?82?83?84?85?86?87?88?95.0?95.5?96.0?96.5?97.0?97.5?98.0?98.5?99.0?99.5?100.0?0.075?0.030?0.010?0.005?0.001?0.001????
0.0001?0.0001????exact?Labeled?F-??score?%?of?op?mal?Log?Probability?Supertagger?beam?PCFG?Log?Probability?HWDep?Log?Probability?PCFG?F-??score?HWDep?F-?
?score?Figure 2: Log-probability of parses relative to exact solu-tion vs. labelled F-score at each supertagging beam-level.4 A* Parsing ExperimentsTo compare approaches, we extended our baselineparser to support A* search.
Following (Klein andManning, 2003) we restrict our experiments to sen-tences on which we can perform exact search via us-ing the same subset of section 00 as in ?3.2.
Beforeconsidering CPU time, we first evaluate the amountof work done by the parser using three hardware-independent metrics.
We measure the number ofedges pushed (Pauls and Klein, 2009a) and edgespopped, corresponding to the insert/decrease-keyoperations and remove operation of the priorityqueue, respectively.
Finally, we measure the num-ber of traversals, which counts the number of edgeweights computed, regardless of whether the weightis discarded due to the prior existence of a betterweight.
This latter metric seems to be the most ac-curate account of the work done by the parser.Due to differences in the PCFG and HWDep mod-els, we considered different A* variants: for thePCFG model we use a simple A* with a precom-1581puted heuristic, while for the the more complexHWDep model, we used a hierarchical A* algo-rithm (Pauls and Klein, 2009a; Felzenszwalb andMcAllester, 2007) based on a simple grammar pro-jection that we designed.4.1 Hardware-Independent Results: PCFGFor the PCFG model, we compared three agenda-based parsers: EXH prioritizes edges by their spanlength, thereby simulating the exhaustive CKY algo-rithm; NULL prioritizes edges by their inside proba-bility; and SX is an A* parser that prioritizes edgesby their inside probability times an admissible out-side probability estimate.8 We use the SX estimatedevised by Klein and Manning (2003) for CFG pars-ing, where they found it offered very good perfor-mance for relatively little computation.
It gives abound on the outside probability of a nonterminal Pwith i words to the right and j words to the left, andcan be computed from a grammar using a simple dy-namic program.The parsers are tested with and without adap-tive supertagging where the former can be seen asperforming exact search (via A*) over the prunedsearch space created by AST.Table 7 shows that A* with the SX heuristic de-creases the number of edges pushed by up to 39%on the unpruned search space.
Although encourag-ing, this is not as impressive as the 95% speedupobtained by Klein and Manning (2003) with thisheuristic on their CFG.
On the other hand, the NULLheuristic works better for CCG than for CFG, withspeedups of 29% and 11%, respectively.
These re-sults carry over to the AST setting which shows thatA* can improve search even on the highly prunedsearch graph.
Note that A* only saves work in thefinal iteration of AST, since for earlier iterations itmust process the entire agenda to determine thatthere is no spanning analysis.Since there are many more categories in the CCGgrammar we might have expected the SX heuristic towork better than for a CFG.
Why doesn?t it?
We canmeasure how well a heuristic bounds the true cost in8The NULL parser is a special case of A*, also called uni-form cost search, which in the case of parsing corresponds toKnuth?s algorithm (Knuth, 1977; Klein and Manning, 2001),the extension of Dijkstra?s algorithm to hypergraphs.0?0.1?0.2?0.3?0.4?0.5?0.6?0.7?0.8?1?
4?
7?
10?
13?
16?
19?
22?
25?Average?Slack?Outside?Span?Figure 3: Average slack of the SX heuristic.
The figureaggregates the ratio of the difference between the esti-mated outside cost and true outside costs relative to thetrue cost across the development set.terms of slack: the difference between the true andestimated outside cost.
Lower slack means that theheuristic bounds the true cost better and guides us tothe exact solution more quickly.
Figure 3 plots theaverage slack for the SX heuristic against the num-ber of words in the outside context.
Comparing thiswith an analysis of the same heuristic when appliedto a CFG by Klein and Manning (2003), we find thatit is less effective in our setting9.
There is a steepincrease in slack for outside contexts with size morethan one.
The main reason for this is because a sin-gle word in the outside context is in many cases thefull stop at the end of the sentence, which is very pre-dictable.
However for longer spans the flexibility ofCCG to analyze spans in many different ways meansthat the outside estimate for a nonterminal can bebased on many high probability outside derivationswhich do not bound the true probability very well.4.2 Hardware-Independent Results: HWDepLexicalization in the HWDep model makes the pre-computed SX estimate impractical, so for this modelwe designed two hierarchical A* (HA*) variantsbased on simple grammar projections of the model.The basic idea of HA* is to compute Viterbi in-side probabilities using the easier-to-parse projected9Specifically, we refer to Figure 9 of their paper which uses aslightly different representation of estimate sharpness1582Parser Edges pushed Edges popped TraversalsStd % AST % Std % AST % Std % AST %EXH 34 100 6.3 100 15.7 100 4.2 100 133.4 100 13.3 100NULL 24.3 71 4.9 78 13.5 86 3.5 83 113.8 85 11.1 83SX 20.9 61 4.3 68 10.0 64 2.6 62 96.5 72 9.7 73Table 7: Exhaustive search (EXH), A* with no heuristic (NULL) and with the SX heuristic in terms of millions of edgespushed, popped and traversals computed using the PCFG grammar with and without adaptive supertagging.grammar, use these to compute Viterbi outside prob-abilities for the simple grammar, and then use theseas outside estimates for the true grammar; all com-putations are prioritized in a single agenda follow-ing the algorithm of Felzenszwalb and McAllester(2007) and Pauls and Klein (2009a).
We designedtwo simple grammar projections, each simplifyingthe HWDep model: PCFGProj completely re-moves lexicalization and projects the grammar toa PCFG, while as LexcatProj removes only theheadwords but retains the lexical categories.Figure 4 compares exhaustive search, A* with noheuristic (NULL), and HA*.
For HA*, parsing ef-fort is broken down into the different edge typescomputed at each stage: We distinguish between thework carried out to compute the inside and outsideedges of the projection, where the latter representthe heuristic estimates, and finally, the work to com-pute the edges of the target grammar.
We find thatA* NULL saves about 44% of edges pushed whichmakes it slightly more effective than for the PCFGmodel.
However, the effort to compute the gram-mar projections outweighs their benefit.
We suspectthat this is due to the large difference between thetarget grammar and the projection: The PCFG pro-jection is a simple grammar and so we improve theprobability of a traversal less often than in the targetgrammar.The Lexcat projection performs worst, for tworeasons.
First, the projection requires about as muchwork to compute as the target grammar without aheuristic (NULL).
Second, the projection itself doesnot save a large amount of work as can be seen inthe statistics for the target grammar.5 CPU Timing ExperimentsHardware-independent metrics are useful for under-standing agenda-based algorithms, but what we ac-tually care about is CPU time.
We were not aware ofany past work that measures A* parsers in terms ofCPU time, but as this is the real objective we feel thatexperiments of this type are important.
This is espe-cially true in real implementations because the sav-ings in edges processed by an agenda parser come ata cost: operations on the priority queue data struc-ture can add significant runtime.Timing experiments of this type are veryimplementation-dependent, so we took care to im-plement the algorithms as cleanly as possible andto reuse as much of the existing parser code as wecould.
An important implementation decision foragenda-based algorithms is the data structure usedto implement the priority queue.
Preliminary experi-ments showed that a Fibonacci heap implementationoutperformed several alternatives: Brodal queues(Brodal, 1996), binary heaps, binomial heaps, andpairing heaps.10We carried out timing experiments on the best A*parsers for each model (SX and NULL for PCFG andHWDep, respectively), comparing them with ourCKY implementation and the agenda-based CKYsimulation EXH; we used the same data as in ?3.2.Table 8 presents the cumulative running times withand without adaptive supertagging average over tenruns, while Table 9 reports F-scores.The results (Table 8) are striking.
Although thetiming results of the agenda-based parsers track thehardware-independent metrics, they start at a signif-icant disadvantage to exhaustive CKY with a sim-ple control loop.
This is most evident when lookingat the timing results for EXH, which in the case ofthe full PCFG model requires more than twice thetime than the CKY algorithm that it simulates.
A*10We used the Fibonacci heap implementation athttp://www.jgrapht.org1583Figure 4: Comparsion between a CKY simulation (EXH), A* with no heuristic (NULL), hierarchical A* (HA*) usingtwo grammar projections for standard search (left) and AST (right).
The breakdown of the inside/outside edges for thegrammar projection as well as the target grammar shows that the projections, serving as the heuristic estimates for thetarget grammar, are costly to compute.Standard ASTPCFG HWDep PCFG HWDepCKY 536 24489 34 143EXH 1251 26889 41 155A* NULL 1032 21830 36 121A* SX 889 - 34 -Table 8: Parsing time in seconds of CKY and agenda-based parsers with and without adaptive supertagging.Standard ASTPCFG HWDep PCFG HWDepCKY 80.4 85.5 81.7 83.8EXH 79.4 85.5 80.3 83.8A* NULL 79.6 85.5 80.7 83.8A* SX 79.4 - 80.4 -Table 9: Labelled F-score of exact CKY and agenda-based parsers with/without adaptive supertagging.
Allparses have the same probabilities, thus variances are dueto implementation-dependent differences in tiebreaking.makes modest CPU-time improvements in parsingthe full space of the HWDep model.
Although thisdecreases the time required to obtain the highest ac-curacy, it is still a substantial tradeoff in speed com-pared with AST.On the other hand, the AST tradeoff improves sig-nificantly: by combining AST with A* we observea decrease in running time of 15% for the A* NULLparser of the HWDep model over CKY.
As the CKYbaseline with AST is very strong, this result showsthat A* holds real promise for CCG parsing.6 Conclusion and Future WorkAdaptive supertagging is a strong technique for ef-ficient CCG parsing.
Our analysis confirms tremen-dous speedups, and shows that for weak models, itcan even result in improved accuracy.
However, forbetter models, the efficiency gains of adaptive su-pertagging come at the cost of accuracy.
One way tolook at this is that the supertagger has good precisionwith respect to the parser?s search space, but low re-call.
For instance, we might combine both parsingand supertagging models in a principled way to ex-ploit these observations, eg.
by making the supertag-ger output a soft constraint on the parser rather thana hard constraint.
Principled, efficient search algo-rithms will be crucial to such an approach.To our knowledge, we are the first to measureA* parsing speed both in terms of running time andcommonly used hardware-independent metrics.
Itis clear from our results that the gains from A* donot come as easily for CCG as for CFG, and thatagenda-based algorithms like A* must make verylarge reductions in the number of edges processedto result in realtime savings, due to the added ex-pense of keeping a priority queue.
However, we1584have shown that A* can yield real improvementseven over the highly optimized technique of adaptivesupertagging: in this pruned search space, a 44%reduction in the number of edges pushed results ina 15% speedup in CPU time.
Furthermore, just asA* can be combined with adaptive supertagging, itshould also combine easily with other search-spacepruning methods, such as those of Djordjevic etal.
(2007), Kummerfeld et al (2010), Zhang et al(2010) and Roark and Hollingshead (2009).
In fu-ture work we plan to examine better A* heuristicsfor CCG, and to look at principled approaches tocombine the strengths of A*, adaptive supertagging,and other techniques to the best advantage.AcknowledgementsWe would like to thank Prachya Boonkwan, JuriGanitkevitch, Philipp Koehn, Tom Kwiatkowski,Matt Post, Mark Steedman, Emily Thomforde, andLuke Zettlemoyer for helpful discussion related tothis work and comments on previous drafts; JuliaHockenmaier for furnishing us with her parser; andthe anonymous reviewers for helpful commentary.We also acknowledge funding from EPSRC grantEP/P504171/1 (Auli); the EuroMatrixPlus projectfunded by the European Commission, 7th Frame-work Programme (Lopez); and the resources pro-vided by the Edinburgh Compute and Data Fa-cility (http://www.ecdf.ed.ac.uk/).
TheECDF is partially supported by the eDIKT initiative(http://www.edikt.org.uk/).ReferencesS.
Bangalore and A. K. Joshi.
1999.
Supertagging: AnApproach to Almost Parsing.
Computational Linguis-tics, 25(2):238?265, June.G.
S. Brodal.
1996.
Worst-case efficient priority queues.In Proc.
of SODA, pages 52?58.S.
Clark and J. R. Curran.
2004.
The importance of su-pertagging for wide-coverage CCG parsing.
In Proc.of COLING.S.
Clark and J. R. Curran.
2007.
Wide-coverage effi-cient statistical parsing with CCG and log-linear mod-els.
Computational Linguistics, 33(4):493?552.S.
Clark and J. Hockenmaier.
2002.
Evaluating a wide-coverage CCG parser.
In Proc.
of LREC Beyond Par-seval Workshop, pages 60?66.S.
Clark.
2002.
Supertagging for Combinatory Catego-rial Grammar.
In Proc.
of TAG+6, pages 19?24.B.
Djordjevic, J. R. Curran, and S. Clark.
2007.
Improv-ing the efficiency of a wide-coverage CCG parser.
InProc.
of IWPT.P.
F. Felzenszwalb and D. McAllester.
2007.
The Gener-alized A* Architecture.
In Journal of Artificial Intelli-gence Research, volume 29, pages 153?190.T.
A. D. Fowler and G. Penn.
2010.
Accurate context-free parsing with combinatory categorial grammar.
InProc.
of ACL.P.
Hart, N. Nilsson, and B. Raphael.
1968.
A formalbasis for the heuristic determination of minimum costpaths.
Transactions on Systems Science and Cybernet-ics, 4, Jul.J.
Hockenmaier and M. Steedman.
2002.
Generativemodels for statistical parsing with Combinatory Cat-egorial Grammar.
In Proc.
of ACL, pages 335?342.Association for Computational Linguistics.J.
Hockenmaier and M. Steedman.
2007.
CCGbank:A corpus of CCG derivations and dependency struc-tures extracted from the Penn Treebank.
Computa-tional Linguistics, 33(3):355?396.D.
Klein and C. D. Manning.
2001.
Parsing and hyper-graphs.
In Proc.
of IWPT.D.
Klein and C. D. Manning.
2003.
A* parsing: Fastexact Viterbi parse selection.
In Proc.
of HLT-NAACL,pages 119?126, May.D.
E. Knuth.
1977.
A generalization of Dijkstra?s algo-rithm.
Information Processing Letters, 6:1?5.J.
K. Kummerfeld, J. Roesner, T. Dawborn, J. Haggerty,J.
R. Curran, and S. Clark.
2010.
Faster parsing bysupertagger adaptation.
In Proc.
of ACL.A.
Pauls and D. Klein.
2009a.
Hierarchical search forparsing.
In Proc.
of HLT-NAACL, pages 557?565,June.A.
Pauls and D. Klein.
2009b.
k-best A* Parsing.
InProc.
of ACL-IJCNLP, ACL-IJCNLP ?09, pages 958?966.B.
Roark and K. Hollingshead.
2009.
Linear complexitycontext-free parsing pipelines via chart constraints.
InProc.
of HLT-NAACL.M.
Steedman.
2000.
The syntactic process.
MIT Press,Cambridge, MA.Y.
Zhang, B.-G. Ahn, S. Clark, C. V. Wyk, J. R. Cur-ran, and L. Rimell.
2010.
Chart pruning for fastlexicalised-grammar parsing.
In Proc.
of COLING.1585
