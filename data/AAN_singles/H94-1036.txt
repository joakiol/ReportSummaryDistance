SESSION 6: SPOKEN LANGUAGE SYSTEMSMadeleine Bates, ChairSpeech & Language DepartmentBBN Systems & Technologies70 Fawcett StreetCambridge, MA 02138What are spoken language systems?
How do they differfrom the speech reeoguition systems that are on the vergeof becoming common in everything from consumer goodsto military systems?In the way that this community uses the term, a spokenlanguage system (SLS) is one that incorporates bothspeech recognition and a large amount of languageunderstanding, enerally in the context of a specific taskthat is being carried out by the user.
A simple "voicecommand" system would not qualify as an SLS, since littleor no language processing is needed to translate therecognized word(s) into the appropriate action(s).A system that is capable of understanding a wide range ofvery natural utterances must of necessity use some form oflanguage processing in addition to speech recognition.
Thegoals of research in the area of SLS are to allow fluent,more "natural" communication between people andcomputers, particularly when they are engaged inperforming non-trivial tasks, such as planning orinformation retrieval.SLSs combine the power (and the problems) of recognitionand understanding.
But SLS is more than the sum of itstwo parts.
Attempts to develop SLSs inevitably inspirethe need to make advances in other disciplines as well,ranging fxom human factors of user interfaces to prosodicanalysis and language generation.Effective SLSs, even in the laboratory, must meet ademanding set of requirements imposed by users who areaccustomed tohaving what they say understood by highlyintelligent agents (other people).
Some of theserequirements are: allowing fluent, natural speech,including a wide range of disfluencies; real-timeperformance; ase of use, particularly for new users; andhigh performance.The focus of effort in developing spoken language systemshas shifted during the course of the ARPA program thathas supported some of the work reported here.
Just lastyear at this workshop, most of the SLS papers wereconcerned with data collection for the domain known asATIS3, the evaluation methodology that was being used toevaluate SLS systems in that domain, and the languageprocessing techniques used in those systems.This year, the papers in this session show that althoughsome effort has been devoted to bettering existing SLSsystems, research attention is beginning to turn beyondATIS3, to focus more on interfaces and on the developmentof dialogue systems.In the area of bettering existing SLS systems, the paper byWayne Ward and Sunil Issar of Carnegie Mellon Universitydiscusses improvements hat have been made to CMU'stop-performing ATIS SLS system.
They descnbe howthey made maximum use of the limited amount of trainingdata, generalized the lexicon and parser, and improved theresolution of ambiguous parses by using context.Many SLS systems use a list of the N best-scoringhypotheses produced by the speech recognizer (typically Nis 20 or fewer) as the interface between speech and languageprocessing.
The simplest ype of integration has been tohave the language processor t y the sentences from the N-best list one at a time, stopping at the first one that isacceptable (understandable) to the language processor.At SRI, Rayner, Carter, Digalakis, and Price improvedtheir ATIS SLS system by concentrating on improving theN-best the interface between the speech recognition andlanguage understanding components.
SRrs innovation wasto experiment with re-ordering the N-best list usingmultiple knowledge sources, such as whether a completelinguistic analysis was possible or not, and discriminantsbased on semantic lasses, grammar ules, and semantictriples that embody the linguistic analysis of the utterance.The fact that this method can be generalized to incorporatenew knowledge sources, and can be automatically trained,makes it an important addition to our methods fordeveloping SLSs.A third system that saw improvement was the Gistingsystem, reported in the paper by Marie Meteer and RobinRohlicek at BBN.
This system is not an ATIS system, oreven a human-computer dialogue system.
It attempts toextract particular types of phrases from off-the-airrecordings of the speech of air traffic controllers and pilots.199In this task, the recognition performance is almostnecessarily poor (given the poor quality of the input), butit is possible nonetheless toachieve respectable precisionand recall results based on extracting phrases from thenoisy speech.
The innovation reported on here is that thesame parser is used for both speech training and theinterpretation of the recognized text.In the area of dialogue, we know that we do not knownearly enough about what kind of dialogues occur betweenpeople and machines; they are certainly different fromhuman-lhuman dialogues in interesting ways, but theirstructure is not well understood.The PEGASUS system, described in the paper by VictorZue and others at MIT, is being used for research focusedon dialogue management.
This system, which is an ATIS-like system connected to a live airline reservation system(EAASY SABRE) accessible over the telephone, features aSystem Manager that monitors the user's dialogue state andthe state of completeness of the booking the user is tryingto complete, as well as the state of the underlyingapplication system PEGASUS has been used by severalpeople in that laboratory during the last year to plan theiractual trips and make reservations for them.The WAXHOLM system described in the paper by RolfCarlson of KTH is another dialogue-based system that goesbeyond ATIS3.
Its domain, although travel related, istravel by boat in the Stockholm archipelago.
The goal ofWAXHOLM to provide a set of tools for generic dialoguesystems which inc ludespeech synthesis, speechrecognition, language understanding, and graphical output.WAXHOLM uses a dialogue grammar that employsprobabilities for topic selection, and artificial neuralnetworks for speech recognition..
Both WAXHOLM andPEGASUS use speech as an important output modality inaddition to the screen display.The problem of disfluencies in speech has long beenpointed to as one of the factors that makes understandinglanguage that comes from speech a more difficult problemthan understanding language that originates in text form.In a paper that is an interesting change from those thatdescribe existing SLS systems, Sharon Oviatt of SRIdiscusses laboratory experiments aimed at improvinghuman's dialogues with computers by preventing orminimizing disfluencies instead of simply coping withthem after they occur.Certainly, ff we can design systems that result in fewerhesitations, filled pauses, restarts, mid-utterancecorrections, mispronunciations, and other speech errors, thesuccess rate of the systems will go up, and with it, useracceptance.
The predictive model that Oviatt presentsshows, for example, that the rate of disfluencies i a linearfunction of the length of the utterance.
Another esult hatshould be useful to SLS builders everywhere is thatsystems that allow unconstrained input have to deal withmore disfluencies than systems that present a morestructured interface to the user.
Reducing the amount ofplanning the speaker must accomplish before (or while)speaking reduces the number of disfluencies produced.Taken as a whole, the papers in this session represent awidening interest in SLSs, and a continued commitment todeveloping techniques that will make high-performing SLSsystems broadly applicable and usable.200
