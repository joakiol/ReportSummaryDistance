Finding Structural Correspondences from Bilingual Parsed Corpusfor Corpus-b sed TranslationHideo Watanabe*, Sadao Kurohashi** and Eiji Aramaki*** IBM Researdt, Tokyo Research Laboratory1623-14 Shimotsuruma, Yamato,Kanagawa 242-8502, Japanwatanabe@trl.ibm.co.jp** Graduate School of Inforlnatics, Kyoto UniversityYoshida-homnachi, Sakyo,Kyoto 606-8501, .JaI)ankuro@i.kyoto-u.ac.jp,aramaki@pine.kuee.kyoto-u.ac.jpAbstractIn this paper, we describe a system and meth-ods for finding structural correspondences from thepaired dependency structures of a source sentenceand its translation in a target language.
The sys-tem we have developed finds word correspondencesfirst, then finds phrasal correspon(tences based onword correspondences.
We have also developed aGUI system with which a user can check and cor-rect tile correspondences retrieved by the system.These structural correspondences will be used asraw translation I)atterns in a corpus-based transla-tion system.1 IntroductionSo far, a number of methodologies and systelnsfor machine trauslation using large corpora exist.They include example-based at)proaches \[7, 8, 9,12\], pattern-based approaches \[10, 11, 14\], and sta-tistical approaches.
For instance, example-basedapproaches use a large set of translation patternseach of which is a pair of parsed structures of asource-language fragment and its target-languagetranslation fragment.
Figure 1 shows an exanl-ple of translation by an example-based method, illwhich translation patterns (pl) and (p2) are se-lected as similar to a (left hand) Japanese depen-dency structure, and an (right hand) English de-pendency structure is constructed by merging thetarget parts of these translation patterns 1.In this kind of system, it is very important ocollect a large set of translatiou patterns easily andefficiently.
Previous systems, however, collect suchtranslation patterns mostly manually.
Therefore,they have problems in terms of the developmentcost.1Words in parenthesis at the nodes of the Japanese de-pendency structure are representative English translations,and are for explanation.This paper tries to provide solutions for this is-sue by proposing methods for finding structuralcorrespondences of parsed trees of a translationpair.
These structural correspondences are used asbases of translation patterns in corpus-based ap-proaches.Figure 2 shows an example of extracting struc-tural correspondences.
In this figure, tile left treeis a Japanese dependency tree, the right tree is adependency tree of its English translation, dottedarrows represent word correspondence, and a pairof boxes connected by a solid line represent phrasalcorrespondence.
We would like to extract these,ook \"4" - .
.~  ...," ~.. a movie ~Figure 2: An Example of Finding Structural Cor-respoudencesword and phrasal correspondeuces automatically.In what follows, we will describe details of proce-dures for finding these structural correspondences.2 Finding Structural CorrespondencesThis sectiou describes methods for finding struc-tural correspondences for a paired parsed trees.2.1 Data  St ructureBefore going into the details of finding structuralcorrespondences, we describe the data format of a906verb - -9anoun- -noun ,n0mu--drinkl l 0un  - -  n0unverb !i,,t .
"', dl lkhe  .__medicinel1 ~ #\[.--I(p2)Figure 1: Translation Example by Examt)le-based ~li'anslationdependency structure.
A det)endeney stru('ture asused in this pat)er is a tree consisting of nodes andlinks (or m:cs), wh('.re a node represents a contentword, while a link rel)resents a fllnctional word ora relation between content words.
For instance, asshown in Figure 2, a t)reposition "at;" is representedas a l ink  in l~,nglish.2.2 F ind ing  Word  Cor respondencesThe  tirst task for finding stru('tm:al corresI)On-den(:c's is to lind word (:orro, sl)ondenccs t)et;ween (;henodes of a sour(:e parsed tree and the nodes of at;wget parsed tree.Word correspondences are tkmn(1 by eonsull;ing asource-to-target translation dictionary.
Most wordscan find a unique 1;ranslation candidate in a targettree, but there are cases such that there are manytranslation candidates in a target parsed tree fora source word.
Theretbre, the main task of tind-ing word correspondences is to determine the mostplausible l;ranslation word mnong can(tidates.
Wecall a pair of a source word and its translationcandidate word in a target tree a word correspon-dence candidate denoted by WC(s,/,), where s is asource word and t is a target word.
If 17\[TC(s,/,) ix aword correspondence andida.te such that there isrto other WC originating h'om s, then it is calledWA word correspondence.The basic idea to select the most plausil)le wordcorrespondence candidate ix to select a candidatewhich is near to another word correspondence whosesource is also near to a sour(:e word in question.Suppose a source word s has multiple candidatetranslation target words t~ (i = 1,...,7~,), that  is,there are multiple 17FCs originating h'om .s'.
We,denote these multiple word corresl)ondence candi-dates by WC(s, tl).
For each I'VC of s, this proce-dure finds the neighbor WA correspondence whosedistance to WC ix below a threshold.
The distancebetween WC(sl,/,~) and WA(s.2,/,2) is defined asthe distance between sl and .s2 plus the distmmebetween s2 and 1,2 where a distance between twonodes is defined as the number of nodes in the t)athwhoso, ends are the two nodes.
Among I~VCs of.s for which neighbor H/A ix tound, the one withthe smallest (listan(:(~ is chosen as the word corre-Sl)ondenee of s, and I/VCs whMl are not chosenare invalidated (or deleted).
We call a word corre-spondence found t)y this procedure WX.
We use3 as t;he distance threshold of the above procedurecurrently.
This procedure ix applied to all sourcenodes which have multii)le WCs.
Figure 3 showsan example of WX word correspondence.
In thisexamt)le, since the Japanese word "ki" has two En-glish l;ranslation word candidates "time" and "pe-riod," there are two WCs  (~7C 1 and WC2).
Thedirect parent node "ymlryo" of "ki" has a WA cor-respondence (I/VA1) to "concern," and the directchild node "ikou" has also a WA correspondc'nee(WA2) to "transition."
In this ease, since the dis-tance between I'VC2 and WA2 is smaller than thedistan(:e between I.VC1 and WA1, I'VC~ in clmngedto a 1/l/X, and I~ITC1 is adandoned.In addition to WX correspondences, weconsidera special case such that given a word correspon-dence l'lZ(s,/,), if s has only one child node which ix907.
.
,  ........ be ....... ",,,.. -~%omp t. .
.WAI at / /  concern."
timeyuuryo ,.."(concern) -"ni ..,Wc1 same.... accompanyki..,*\[__(time) .............. W_..G2 ............'"-- periodikou ......... VVA2 oftransition (transition)Figure 3: An Exmnt)le of WX Word Correst)on-(lencea leaf and t has also only one child node which ix aleaf, th(;n we COllStrllet a lleW word correspondencecalled 1US from these two leaf nodes.
This WEprocedure is al)plied to all word correspondences.Note tlmt this word correst)ondence is not to se.le, ctone of candidates, rather it is a new finding of wordcorre, spondence by utilizing a special structm:e. Forinstance, in Figure 3, if there is a word eorrespol>dence 1)etween "ki" and "period" and there is noword correst)ondence between "ikou" and "transi-tion," then I<V,g(iko'u~ transition) will be found 1)3'this 1)roeedure.These WX and WS t)rocedures are continuouslyal)plied until no new word correspondences arc t'(mnd.Aft;er al)l)lying the above WX and I'VS pro(:e-dures, there are some target words t such that t is adestination of a l,l/C(.s ", t) and there ix no other 1,176 ,whose destination ix t:.
In this case, the lUG(s,t)correspondence andidate is chosen as a valid wordcorrespondence b tween s and/,,  and it; is called aHzZ word eorrest)ondence.We call a source node or a target node of a wordcorrespondence an anchor node in what tbllows.The above t)rocedures for finding word corre-sI)ondences are summarized as follows:Find WCs by consulting translation dictionary;Find WAs;whi le  (true) {find WXs;find WSs;i f  no new word corresp, is found, then  break;}find WZs;2.3 F ind ing  Phrasa l  Co l ' res l )ondencesThe next step is to tind phrasal correspondencesbased on word eorl'eSl)ondences t'(mnd t) 3, 1)roce.-dures described in tim previous section.
What  wewould like to retrieve here, is a set of phrasal cor-respondences which (:overs all elements of a paireddependency trees.In what follows, we (:all a portion of a tree whichconsists of nodes in a 1)att~ from a node ?t I (;o all-oth(;r node nu which is a descen(lanl; of n:l a lin-.ear tree denoted by LT(v,1, n~), and we denote aminimal sul)tree including st)coiffed nodes hi,  ..., n.~,l)y T (n l , .
.
.
,n , ) .
For instan(:(,~ in the English treestructure (the right tree) in Figure 4, LT(tcch, nology ,science) is a rectangular area covering %eclmol-?
tg "e e ~ ogy," and SOl ,no ,, anti .T(J'acl;or, cou'ntrjl ) is a1)olygonal area covering "factor,""atDcl,, .
.
.
.
t)ol-icy," and "country.
"The tirst step is to find a 1)air of word correst)on-dences W, (.~'~, t ) and ~4q(.,.~, t ~) such that .,, a.,ts2 constructs a linear tree LT(si ,  s2) and there is noanchor node in th(' 1)al;h from s~ to s2 other than .s'~and .s2, where 1UI and H~ denote any tyi)e of word('orrest)on(lences 2 and we assmne there is a wordcorresI)ondence t)etwee, n roots of source and (;argettrees by defmflt.
We construct a t)hrasal correspon-dence fi'om source nodes in LT(s , , s2)  and targetl/o(les itl r \ ] ' ( t : l , / '2 ) ,  (l()llote(t by \];'(l~,~F'(.q'l, .
";2), 5\].n(tl, t2)).For illstall('e~ ill F ig l l re 41~ \]"11~ \]~12~ 1)'2~ 1)3 and\])4 tu.
'e source portions of phrasal et)rrespondencesfound in this step.The next stel) checks, for ea(:h 1', if all anchorl lo(les of wor(1 eorres1)Oll(leile(?s wllose SOUlT(;e o1 ~;al-get node is included in P are al,eo included in P.If a t)hrasal correst)ondenee satisiies this condition,then it is called closed, otherwise it ix called open.Further, nodes which are not included in the I ) inquestion are called open nodes.
If a l ) ix ot)en, thenit ix merged with other 1)hrasal correspondenceshaving ol)en nodes of P so that the merged 1)hrasalcorrespondence b comes (-losed.Next, each P~,, is checked if there is another l)qwhich shares any nodes ottmr than anchor nodeswith P.,,.
If this is the case, these P:., and 1~ arelnerged into one phrasal correspondence.
In Figure4, t)hrasal correspondences i 11 and P12 are mergedinto P1, since their source I)ortions LT (haikei, koku)and LT (haikci, seisaku) share "doukou" which isnot an anchor node.Finally, any path whose nodes other than theroot are not included in any 1)s but the root nodeix included in a 1 ) is searched for.
This procedure2Since WC is not a word correspondence (it is a candi-date, of word corresi)ondence), it is llOi; conside, red here.908is apl)lied I;o 1)oth source a.nd (;arget trees.
A im.thfound 1)y this 1)ro(:(xlur(~ is called an open pal, h,, m~(tits root no(le is called a pivot.
If such an Ol)en pathis found, it is t)rocessed as follows: l, br each 1)ivotnode, (a) if the t)ivot is not an mmhor nod(;, thenopen lmths originating fl:om the pivot is mergedinto a 1 ) having I;he pivot, (b) if the pivot is an~LIlChOf l lo(lo~ {;hOll 3_ llOW t)hl'~lS~L1 c()rFos1)oII(|(~IlC( ~, iScreated from Ol)(m 1)ai;hs originating from the m>thor nodes of the word (:orrcsl)on(l(:ncc.In Figure 4, w(: get tinally four phrasal (:orr(:-Sl)on(lences l~, f~, l~, an(l l~t.!
haikei.
!,,: I - ................... { -~ factor',,,l ',i /~0 :: a ect ", ,  ',,(tre, nd) i f \  ~ ~-"  l i ;k .
'/ ;~oy, v __:~, koku ( seisak~'~{t - - - - .~-  - -~~' (C0UrlttV)_l , (p0%,~ :t' .. technology .lrltly~< :::>i--- -::-:~= 7 :: TLI io.
; I(major) ~,_/_- X-~ /' giutu"\]\](technolo~ly)l' .?
sciencekagaku "(scie, nce) .
-P4/tt /  ffiFigm:e d: An l~;xaml)le of Finding Phrasal  Corr(>S\])Olld(~,IIC(',SThe above 1)ro(:edures fl)r finding l)hrasal (:orr(>Ht)oIIdoIICOS ~-LF(~ SlllIllIl?~riz(Kl gtS fo l lows :Find initial Ps;Mea'ge a.n Ol)Cn 1>~ with other i ' s  havingopen nodes of 1};Create new Ps 1)y merging \])swhich have more tlmn 2 (:ommon nodes;Find ot)en path, alldif the t)ivot is ml mmhor, | ;henmerge the path to P having the anchor,o therwise  create new l ) by mergingall open t)ai,hs having l;lm pivot;3 Exper iments3.1 C, o r lms  and  D ic t ionaryWe used (l()(;lllil(~'ll|;s t'rolil White Papers on S(:i-en(-e and Technology (1.994 to \ ]996) pul)lished bythe S(:ience mid Technology Agency (STA) of tim.\]al)mmse govcrlim(~nl;.
STA lmblished th(;se WhitePaI)ers in both Jat)mmse and English.
The Com-mmfications l{esea.rch Laboratory of" the Ministryof Posts and Telecommuni(:a.tion of the .\]al)mmsegoverlmmnt supl)lied us with the l)ilingual corpuswtfich is already roughly aligned.
We made a bilin-gual cortms consisting of pa.rs(;d dependency struc-tures by using the KNP\[2\] .\]al)mmso, 1)arser ((l(wel-Ol)ed by Kyoto (hfive)sity) for .Jal)anes(~ sentencesand the ESG\[5\] English 1)arser (developed by IBMWatson i{e, sear(:h Center) for English s(~nl;(!nces.We mad(} al)oul; 500 senl;(m(:e l)airs, each of whi(:h11~1,'4 ;I, OIlC-I;O-OII(', 80,11|;(',11(;0 (-orresl)onden(:(~,, fl'OI\[l (,\]lOraw (t~tta of l;he, White l)al)crs, mid s(',l(;(;i;(xl rm>domly aboul; 130 s('aH;en(:c pairs for (',Xl)(Mm(;nts.ilow(wer, since a 1)nrser does not always \])ro(hwe(;orl'c(;\[; 1);~l"s(t t;re(}s~ wo (~x(:lude(1 some, ~(~ii|;(Hic(~ p;Lil'swlfich have severe 1)arse errors, and tinally got i\[15S(~,II\[;OIlC(; pairs as a, to, st s(%.As a trm~slation wor(1 dictionary/)etw(',(m .l at)ml(',s(;and English, we, tirsl; used ,l-to-l~; trmlslati()n (li(:-l, ionary which has mot(,' t lmn 100,000 (,ifl;l'i(;~, butwe, fi)un(l l;}l~/{; l ller(?
are som(~ word ('orr(~sl)Oll(l(~,llt;(~snot (:()v(ued in this di(:ti()nary.
Tlmref()rG we merged(retries fi:om \]';-t;o-.I translatioll dictionary in orderto get; much broad (:ov(wag(,'.
The l;oDd nulnl)(}r ()fentries a.re now more I;ha.n \[50,000.3.2 Exper inmnta l  Resu l tsTd)le i shows l;he result of (~Xl)c, rimeni; fl)r tind-ing word correspond(nm(~s.
A row with ALL in th(',l:yl)e cohmm shows Llle total  ~CClll'~lcy of WOI'(1 cor-r(Lqpolld('31c(~s and ol;\]l{~r rows sh()\v Llle .~iCClll'ktcy ofeach t, yt)e. It is clear that WA (:orr(~sl)Olld(~ll(;(',shave a very high a('cura(:y.
Other word (:orresl)On--do, nc(,,s also ha.ve a roJatively high ac(:ura(:y.Table 2 shows tim remflt of exl)erimenl,s for find~ing 1)hrasal correspondences.
The row with ALL inI;he l;yt)c cohlmn shows l;he l;ol;al accuracy of phrasal(:ol'r(~sl)ondo, n(:(~s found by the 1)rol)osed 1)rocedure.This ac(:macy level is not I)romising and it is not;useful for later 1)ro(:e, sses since it needs human (:he(:k-ing ml(l (:orrec?ion.
Therefore, we sul)categoriz(~each phrasal corl'eSpond(m('es, and check l;he a('-(:uracy for each subca.tegory.We consider the following sut)catcgories for 1)hrasal('x)rl'(}Sl)olidell(-(~s:?
MIN ...
The minimal  t)hrasal correst)ondence,that is, I'(1Zl'(.s'l, .s2), LT(t l ,  t2)) such that  (;herc909typeALLWAWXWSWZ1111111.
nunl .
of SUCCESSof correct ratio found corresp.
(%) corresp.771 745 96.63612 600 98.03131 118 90.0713 12 92.315 15 100Table h Experimental Result of Word Correspon-dencesare word correspondences W(s1,  t l )  and W(s2,t2), s2 is a direct child of St and t2 is adirect child of tl.?
LTX ... P(LT(.s'I,S2),LT(tl,t2)) such thatall nodes other titan s2 and t2 have only onechild node.?
LTY ... P(LT(sl,.S2), LT(tl, t2)) such thatall nodes other than Sl, s2,1':1 and t.2 have onlyone child node.LTX is a special case of LTY, since Sl and tl ofLTX must have only one child node, on the otherhand, ones of LTY may have more than two childnodes.
A subcategory test tbr a phrasal correspon-dence is done in the above order.
Exmnples of thesesubcategories are shown in Fig 5.Tlm result of these subcategories are also shownin Table 2.
Subcategories MIN and LTX have veryhigh accuracy and this result is very promising,since we can avoid nmnual checking for ttmse phrasalcorrest)ondences , or we would check only these typesof t)hrasal correspondences mmmally and discardother types.As stated earlier, since we removed only sen-tences with severe parsing errors from the test set,please note that the above mtmbers of experimentalresults are calculated for a bilingual parsed corpusincluding parsing errors.4 D iscuss ionThere have been some studies on structural align-Inent of bilingual texts such as \[1, 4, 13, 3, 6\].
Ourwork is similar to these previous tudies at the con-ceptual level, but different in some aspects.
\[1\]reported a method for extracting translation tem-plates by CKY parsing of bilingual sentences.
Thiswork is to get phrase-structure level phrasal cor-respondences, but our work is to get dependency-structure level phrasal correspondences.
\[4\] pro-posed a method for extracting structural matclfing(pairs of dependency trees) by calculating matchingsimilarities of two dependency structures.
Theirwork focuses on tile parsing ambiguity resolutionby calculating structural matching.
Further, \[3, 6\]proposed structural alignnmnt of dependency struc-tures.
Their work assuined tha.t least common an-cestors of each fragment of a structural correspon-dence are preserved, but our work does not havesuch structural restriction.
\[13\] is different o oth-ers in that it tries to find phrasal correspondencesby comt)aring a MT result and its manual correc-tion.In addition to these differences, the main differ-ence is to find classes (or categories) of phrasal cor-respondences which have high accuracy.
In general,since bilingual structural alignment is very compli-cated and difficult task, it; is very hard to get morethan 90% accuracy in total.
If we get only suchan accuracy rate, the result is not useful, since weneed manual clmcks tbr the all correspondences re-trieved.
But, if we can get some classes of phrasalcorrespondence with, for instance, more than 90%accuracy rate, then we can reduce manual clmck-ing for phrasal correspondences in such classes, andthis reduces the development cost of translationpatterns used in later corpus-based translation pro-tess.
As shown in the previous section, we couldfind ttmt all (:lasses of word correspondences andtwo subclasses of phrasal correspondences are morethan 90% accurate.When actually using this automatically retrievedstructural correspondence data, we must considerhow to manually correct the incomplete parts andhow to reuse mamlal correction data if the parserresults are ctmnged.As for the tbrlner issue, we need an easy-to-usetool to modify correspondences to reduce the costof mmmal operation.
We have developed a GUItool as shown in Figure 6.
In this figure, the bot-tom half presents a pair of source and target depen-dency structures with word correspondences (solidlines) and phrasal correspondences ( equences ofslmded circles).
You can easily correct correspon-dences by looking at this graplfical presentation.As for tlm latter issue, we must develop meth-ods for reusing the manual correction data as muchas possible even if tim parser outputs are changed.We have developed a tool for attaching phrasalcorrespondences by using existing phrasal corm-spondence data.
This is implemented as follows:Each phrasal correspondence is assigned a signa-ture which is a pair of source and target, sentences,each of which tins bracketed segments which are in-cluded in the phrasal correspondence.
For instance,910Itmihatu((~uebiomeR),~10gijutu -,,-(tedr, do?.t?
)I--,<Jeveloprned0f--,'.-tect'lrlOlogyILl Zl.l~l.l ,~\[<dime}goseityou(i, otldl}Iya t~.a d 0ute ki.
.
.
.
.
.
,.corlti nue*o- -  shob'o-.
~obj" "k 9 Kl t;dh/ ' / \economicfkaga t,lJ"~.%unparallelled.
?tij d u - \ [ - -~e  c;h n 010 ?tl~<~o~l I "-)/. '
,  ,,o  ol oy- .
x ,,0,,,ka nte n{a~laedlkorera .,-science.#gzee(a) MIN (b) LTX (c) LTYp Urp osesPec I-,qhisFigure 5: Examples of Categories of Phrasal Correst)ondencesA:5115511. oftype foundCOl; l 'es i ) .ALL 678MIN 223LTX 17LTY 27B:I515151.
of(:orrectco5-5"(~Sl).431215(~:SllC(;(~SSratio~/A (%)63.5696A1D:nunL of nodescovered t)y A72481234E:nunl.
of nodescovered by B42781194F:Sl lCCeSSratioE/D (%)59.0296.7617 100 153 153 10020 I 74.07 253 191 75A9ITal)le 2: lgxperinmntal Fh',sult ot' Phrasal Correst)on(len :esthe following signature is made h)r a i)hrasal corre-Sl)on(lence (c) in Figure 5:(.~i:j)... \[korer~ no kanten karmlo\] kagaku \[gi-j u tu \ ]  ...... science and \[technology fl:om thislmrl)ose\] ...(/.~io)In the above e, xample, segments betwee, n '\[' and '\]'represent a phrasal correspondence.If new parsed dqmndency structures for a sen-tence pair is given, for each phrasal correspondencesignature of the sentence pair, nodes in the struc-tures wtfich are inside 1)rackets of the signature aremarked, mid if there is a minimal sul)tree consist-ing of only marked nodes, then a phrasal corre-Sl)ondence is reconstructed from the phrasal corre-spondence signature.
By using this tool, we canefficiently reuse the manual efforts as much as pos-sible even if parsers are updated.5 Conc lus ionIll this I)al)er, we have t)rol)osed methods forfinding structural correspondences (word correst)on-dences and i)hrasal corr(;spondences) of bilingualparsed corpus.
Further, we showed that the t)reci-sion of word correst)ond(mces and some catc'goriesof t)hrasal corresl)ondences found 1)y our methodsare highly accurate, and these correst)ondences canreduce the cost of trm~slation pattern accumula-tion.In addition to these results, we showed a GUItool for mmmal correction and a tool for reusingprevious correspondence data.As fld;ure directions, we will find more subclasseswith high accuracy to reduce the cost for transla-tion pattern preparation.We believe that these methods and tools can ac-celerate the collection of a large set of translationpatterns and the developlnent of a corlms-basedtranslation system.911~rel id="28" type="P4" src="3.4,9,10,11,12.13" tgt="1,2.3,4,8,9,12" eval="T~R UE" score="O" geoeratlon='' subtype="orff' org=" con'lment='"'=~rel id="29" type="P5" src="1,2,3" tg~"l 0,11,12" BvaI="TRUE" score="0" generation="" subtype="org" org=" comment:="'>~rel ld="3O" type="P5" src="5.6.7" \[g1="5.6,7" eva~"TRUE" score="0" generation=" sublype="org" org="" cornmen~''~<tel id="31" type="P5" src="7.8,9" tg~"7.F' evaI="TRUE" ecore="O" generation=" subtype="org" erg=" cerumen .t=""~.
'~rel id="32" type="P5" src="3,4.9.10,11.12.13" tgt="1,2,3,4.e,g,12" evaI="TRUE" score="0" generation="' subtype="org" org='' comment=-"'-'.
!\L4\h6 ac len ;~aad tactiilC:;Id6i' g,0tiCid;~ ; f  rn~\[,~r.c~un~les.. .
.
.
.
.
, .
.
.
.
?
~- .,.
?
L:i : :  ?
: i %Figure 6: An GUI tool for presenting/manipulating structural correspondencesReferences\[1\] Kaji, H., Kids, Y., and Morimoto, Y., "Learning Trans-lation Templates from Bilingual Texts," Proc.
of Coling92, pp.
672-678, I992.\[2\] Kurohashi, S., and Nagao, M., "A Syntactic Analy~sis Method of Long Japanese Sentences based on theDetection of Conjunctive Structures," ComputationalLinguisties~ Voh 20, No.
4, 1994.\[3\] Grishman, R., "Iterative Alignment of Syntactic Struc-tures for a Bilingual Corpus," Proe.
of 2nd Workshopfor Very Large Corpora, pp.
57-68, 1994.\[4\] Matsumoto, Y., Ishimoto, H., and Utsuro, T., "Struc-tural Matching of Parallel Texts," Proc.
of the 31st ofACL,  pp.
23-30, 1993.\[5\] MeCord, C. M., "Slot Grammars," Computational Lin-guistics, Voh 6, pp.
31-43, 1980.\[6\] Meyers, A., Yanharber, R., and Grishman, R., "Align-ment of Shared Forests for Bilingual Corpora," Proc.
ofthe 16th of COLING, pp.
460-465, June 1996.\[7\] Nagao, M., "A Framework of a Mechanical Translationbetween Japanese and English by Analogy Principle,"Elithorn, A. and Banerji, R.
(eds.)
: Artificial and Hu-man Intelligence , NATO 1984.\[8\] Sato, S., and Nagao, M. "Toward Memory-based Trans-lation," Proc.
of 13th COLING, August 1990.\[19\] Sumita, E., Iida, II., and Kohyama, H. "'Translatingwith Examples: A New Approach to Machine 3Yanslaotion," Proc.
of" Info Japan 90, 1990.\[10\] Takeda, K., "Pattern-Based Context-Free Grammarsfor Machine ~l~anslation," Proc.
of 34th ACL, pp.
144--15I, June 1996.\[11\] Takeda, K., "Pattern-Based Machine ~lYanslation,"Proc.
of 16th COLING, Vol.
2, pp.
1155-1158, August1996.\[12\] Watanabe, H. "A Similarity-Driven Transfer System,"Proc.
of the 14th COLING, Vol.
2, pp.
770.-776, 1992.\[13\] ~Vatanabe, H. "A Method for Extracting ~IYanslationPatterns from ~lS'anstation Examples," Proc.
of the 5thInt.
Conf.
on Theoretical and Methodological Issues inMachine Translation, pp.
292-301, 1993.\[14\] Watanabe, H., and Takeda, K., "A Pattern-based Ma.-chine Translation System Extended by Example-basedProcessing," Proc.
of the 36th ACL & 17th COLING,Vol.
2, pp.
1369o1373, 1998.912
