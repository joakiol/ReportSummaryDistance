Memory-Efficient and Thread-Safe Quasi-Destructive GraphUnificationMarcel P. van LohuizenDepartment of Information Technology and SystemsDelft University of Technologympvl@acm.orgAbstractIn terms of both speed and mem-ory consumption, graph unificationremains the most expensive com-ponent of unification-based gram-mar parsing.
We present a tech-nique to reduce the memory usageof unification algorithms consider-ably, without increasing executiontimes.
Also, the proposed algorithmis thread-safe, providing an efficientalgorithm for parallel processing aswell.1 IntroductionBoth in terms of speed and memory consump-tion, graph unification remains the most ex-pensive component in unification-based gram-mar parsing.
Unification is a well known algo-rithm.
Prolog, for example, makes extensiveuse of term unification.
Graph unification isslightly different.
Two different graph nota-tions and an example unification are shown inFigure 1 and 2, respectively.In typical unification-based grammarparsers, roughly 90% of the unificationsfail.
Any processing to create, or copy, theresult graph before the point of failure isbeA CFD?
?A = bC = 1[D = e]F = 1?
?Figure 1: Two ways to represent an identicalgraph.redundant.
As copying is the most expensivepart of unification, a great deal of researchhas gone in eliminating superfluous copying.Examples of these approaches are given in(Tomabechi, 1991) and (Wroblewski, 1987).In order to avoid superfluous copying, thesealgorithms incorporate control data in thegraphs.
This has several drawbacks, as wewill discuss next.Memory Consumption To achieve thegoal of eliminating superfluous copying, theaforementioned algorithms include adminis-trative fields?which we will call scratchfields?in the node structure.
These fieldsdo not attribute to the definition of the graph,but are used to efficiently guide the unifica-tion and copying process.
Before a graph isused in unification, or after a result graph hasbeen copied, these fields just take up space.This is undesirable, because memory usageis of great concern in many unification-basedgrammar parsers.
This problem is especiallyof concern in Tomabechi?s algorithm, as it in-creases the node size by at least 60% for typ-ical implementations.In the ideal case, scratch fields would bestored in a separate buffer allowing them to bereused for each unification.
The size of such abuffer would be proportional to the maximumnumber of nodes that are involved in a singleunification.
Although this technique reducesmemory usage considerably, it does not re-duce the amount of data involved in a singleunification.
Nevertheless, storing and loadingnodes without scratch fields will be faster, be-cause they are smaller.
Because scratch fieldsare reused, there is a high probability thatthey will remain in cache.
As the difference[A =[B = c]D =[E = f]]unionsq?
?A = 1[B = c]D = 1G =[H = j]??????
?A = 1[B = cE = f]D = 1G =[H = j]???
?Figure 2: An example unification in attribute value matrix notation.in speed between processor and memory con-tinues to grow, caching is an important con-sideration (Ghosh et al, 1997).1A straightforward approach to separate thescratch fields from the nodes would be to usea hash table to associate scratch structureswith the addresses of nodes.
The overheadof a hash table, however, may be significant.In general, any binding mechanism is boundto require some extra work.
Nevertheless,considering the difference in speed betweenprocessors and memory, reducing the mem-ory footprint may compensate for the loss ofperformance to some extent.Symmetric Multi Processing Small-scale desktop multiprocessor systems (e.g.dual or even quad Pentium machines) are be-coming more commonplace and affordable.
Ifwe focus on graph unification, there are twoways to exploit their capabilities.
First, it ispossible to parallelize a single graph unifica-tion, as proposed by e.g.
(Tomabechi, 1991).Suppose we are unifying graph a with graph b,then we could allow multiple processors towork on the unification of a and b simulta-neously.
We will call this parallel unifica-tion.
Another approach is to allow multiplegraph unifications to run concurrently.
Sup-pose we are unifying graph a and b in addi-tion to unifying graph a and c. By assigninga different processor to each operation we ob-tain what we will call concurrent unifica-tion.
Parallel unification exploits parallelisminherent of graph unification itself, whereasconcurrent unification exploits parallelism atthe context-free grammar backbone.
As longas the number of unification operations in1Most of today?s computers load and store data inlarge chunks (called cache lines), causing even unini-tialized fields to be transported.one parse is large, we believe it is preferableto choose concurrent unification.
Especiallywhen a large number of unifications termi-nates quickly (e.g.
due to failure), the over-head of more finely grained parallelism can beconsiderable.In the example of concurrent unification,graph a was used in both unifications.
Thissuggests that in order for concurrent unifica-tion to work, the input graphs need to beread only.
With destructive unification al-gorithms this does not pose a problem, asthe source graphs are copied before unifica-tion.
However, including scratch fields in thenode structure (as Tomabechi?s and Wrob-lewski?s algorithms do) thwarts the imple-mentation of concurrent unification, as differ-ent processors will need to write different val-ues in these fields.
One way to solve this prob-lem is to disallow a single graph to be usedin multiple unification operations simultane-ously.
In (van Lohuizen, 2000) it is shown,however, that this will greatly impair the abil-ity to achieve speedup.
Another solution is toduplicate the scratch fields in the nodes foreach processor.
This, however, will enlargethe node size even further.
In other words,Tomabechi?s and Wroblewski?s algorithms arenot suited for concurrent unification.2 AlgorithmThe key to the solution of all of the above-mentioned issues is to separate the scratchfields from the fields that actually make upthe definition of the graph.
The result-ing data structures are shown in Figure 3.We have taken Tomabechi?s quasi-destructivegraph unification algorithm as the startingpoint (Tomabechi, 1995), because it is oftenconsidered to be the fastest unification algo-arc listtypeArcNodeUnification data Copy dataReusable scratchstructurescopyforwardcomp-arc listvaluelabeloffsetindexindexonly structuresPermanent, read-Figure 3: Node and Arc structures and thereusable scratch fields.
In the permanentstructures we use offsets.
Scratch structuresuse index values (including arcs recorded incomp-arc list).
Our implementation derivesoffsets from index values stored in nodes.rithm for unification-based grammar parsing(see e.g.
(op den Akker et al, 1995)).
Wehave separated the scratch fields needed forunification from the scratch fields needed forcopying.2We propose the following technique to asso-ciate scratch structures with nodes.
We takean array of scratch structures.
In addition,for each graph we assign each node a uniqueindex number that corresponds to an elementin the array.
Different graphs typically sharethe same indexes.
Since unification involvestwo graphs, we need to ensure that two nodeswill not be assigned the same scratch struc-ture.
We solve this by interleaving the indexpositions of the two graphs.
This mapping isshown in Figure 4.
Obviously, the minimumnumber of elements in the table is two timesthe number of nodes of the largest graph.
Toreduce the table size, we allow certain nodesto be deprived of scratch structures.
(For ex-ample, we do not forward atoms.)
We denotethis with a valuation function v, which re-turns 1 if the node is assigned an index and 0otherwise.We can associate the index with a node byincluding it in the node structure.
For struc-ture sharing, however, we have to use offsetsbetween nodes (see Figure 4), because other-wise different nodes in a graph may end uphaving the same index (see Section 3).
Off-2The arc-list field could be used for permanent for-ward links, if required.c_Left graphoffset: 0g4e3 f _Right graphoffset: 12jh0_l3k1b 1i2 x 0 + 0a h b ji k0 1 2 3 4 5 6 7 8 9 10 11 12d e ga0d2+1+1 +12 x 1 + 1+1 -2 +0+3+12 x 4 + 0+4-2+1+0Figure 4: The mechanism to associate indexnumbers with nodes.
The numbers in thenodes represent the index number.
Arcs areassociated with offsets.
Negative offsets indi-cate a reentrancy.sets can be easily derived from index valuesin nodes.
As storing offsets in arcs consumesmore memory than storing indexes in nodes(more arcs may point to the same node), westore index values and use them to computethe offsets.
For ease of reading, we present ouralgorithm as if the offsets were stored insteadof computed.
Note that the small index val-ues consume much less space than the scratchfields they replace.The resulting algorithm is shown in Fig-ure 5.
It is very similar to the algorithm in(Tomabechi, 1991), but incorporates our in-dexing technique.
Each reference to a nodenow not only consists of the address of thenode structure, but also its index in the ta-ble.
This is required because we cannot deriveits table index from its node structure alone.The second argument of Copy indicatesthe next free index number.
Copy returnsreferences with an offset, allowing them tobe directly stored in arcs.
These offsets willbe negative when Copy exits at line 2.2,resembling a reentrancy.
Note that onlyAbsArc explicitly defines operations on off-sets.
AbsArc computes a node?s index usingits parent node?s index and an offset.Unify(dg1, dg2)1. try Unify1((dg1, 0), (dg2, 1))a1.1.
(copy, n)?
Copy((dg1, 0), 0)1.2.
Clear the fwtab and cptab table.b1.3.
return copy2.
catch2.1.
Clear the fwtab table.b2.2.
return nilUnify1(ref in1, ref in2)1. ref1?
(dg1, idx1)?
Dereference(ref in1)2. ref2?
(dg2, idx2)?
Dereference(ref in2)3. if dg1 ?addr dg2 and idx1 = idx2c then3.1.
return4.
if dg1.type = bottom then4.1.
Forward(ref1, ref2)5. elseif dg2.type = bottom then5.1.
Forward(ref2, ref1)6. elseif both dg1 and dg2 are atomic then6.1.
if dg1.arcs 6= dg2.arcs thenthrow UnificationFailedException6.2.
Forward(ref2, ref1)7. elseif either dg1 or dg2 is atomic then7.1.
throw UnificationFailedException8.
else8.1.
Forward(ref2, ref1)8.2. shared?
IntersectArcs(ref1, ref2)8.3. for each (( , r1), ( , r2)) in shared doUnify1(r1, r2)8.4. new?
ComplementArcs(ref1, ref2)8.5. for each arc in new doPush arc to fwtab[idx1].comp arcsForward((dg1, idx1), (dg2, idx2))1. if v(dg1) = 1 thenfwtab[idx1].forward?
(dg2, idx2)AbsArc((label, (dg, off)), current idx)return (label, (dg, current idx + 2 ?
off))dDereference((dg, idx))1. if v(dg1) = 1 then1.1.
(fwd-dg, fwd-idx)?
fwtab[idx].forward1.2.
if fwd-dg 6= nil thenDereference(fwd-dg, fwd-idx)1.3. elsereturn (dg, idx)IntersectArcs(ref1, ref2)Returns pairs of arcs with index values for each pairof arcs in ref1 resp.
ref2 that have the same label.To obtain index values, arcs from arc-list must beconverted with AbsArc.ComplementArcs(ref1, ref2)Returns node references for all arcs with labels thatexist in ref2, but not in ref1.
The references are com-puted as with IntersectArcs.Copy(ref in, new idx)1.
(dg, idx)?
Dereference(ref in)2. if v(dg) = 1 and cptab[idx].copy 6= nil then2.1.
(dg1, idx1)?
cptab[idx].copy2.2.
return (dg1, idx1?
new idx + 1)3. newcopy?
new Node4.
newcopy.type?
dg.type5.
if v(dg) = 1 thencptab[idx].copy?
(newcopy, new idx)6. count?
v(newcopy)e7.
if dg.type = atomic then7.1.
newcopy.arcs?
dg.arcs8.
elseif dg.type = complex then8.1.
arcs?
{AbsArc(a, idx) | a ?
dg.arcs}?
fwtab[idx].comp arcs8.2.
for each (label, ref) in arcs doref1?
Copy(ref, count + new idx)fPush (label, ref1) into newcopy.arcsif ref1.offset > 0g thencount?
count + ref1.offset9.
return (newcopy, count)aWe assign even and odd indexes to the nodes of dg1 and dg2, respectively.bTables only needs to be cleared up to point where unification failed.cCompare indexes to allow more powerful structure sharing.
Note that indexes uniquely identify a node inthe case that for all nodes n holds v(n) = 1.dNote that we are multiplying the offset by 2 to account for the interleaved offsets of the left and right graph.eWe assume it is known at this point whether the new node requires an index number.fNote that ref contains an index, whereas ref1 contains an offset.gIf the node was already copied (in which case it is < 0), we need not reserve indexes.Figure 5: The memory-efficient and thread-safe unification algorithm.
Note that the arraysfwtab and cptab?which represent the forward table and copy table, respectively?are definedas global variables.
In order to be thread safe, each thread needs to have its own copy of thesetables.Contrary to Tomabechi?s implementation,we invalidate scratch fields by simply reset-ting them after a unification completes.
Thissimplifies the algorithm.
We only reset thetable up to the highest index in use.
As tableentries are roughly filled in increasing order,there is little overhead for clearing unused el-ements.A nice property of the algorithm is thatindexes identify from which input graph anode originates (even=left, odd=right).
Thisinformation can be used, for example, toselectively share nodes in a structure shar-ing scheme.
We can also specify additionalscratch fields or additional arrays at hardlyany cost.
Some of these abilities will be usedin the enhancements of the algorithm we willdiscuss next.3 EnhancementsStructure Sharing Structure sharing is animportant technique to reduce memory us-age.
We will adopt the same terminology asTomabechi in (Tomabechi, 1992).
That is,we will use the term feature-structure sharingwhen two arcs in one graph converge to thesame node in that graph (also refered to asreentrancy) and data-structure sharing whenarcs from two different graphs converge to thesame node.The conditions for sharing mentioned in(Tomabechi, 1992) are: (1) bottom andatomic nodes can be shared; (2) complexnodes can be shared unless they are modified.We need to add the following condition: (3)all arcs in the shared subgraph must have thesame offsets as the subgraph that would haveresulted from copying.
A possible violationof this constraint is shown in Figure 6.
Aslong as arcs are processed in increasing orderof index number,3 this condition can only beviolated in case of reentrancy.
Basically, thecondition can be violated when a reentrancypoints past a node that is bound to a largersubgraph.3This can easily be accomplished by fixing the or-der in which arcs are stored in memory.
This is a goodidea anyway, as it can speedup the ComplementArcsand IntersectArcs operations.h0a01i3ks6tG +17Node could be shared Node violates condition 31b j 4+3+1 +2FK +1G Hc2 de4 f 5g6+4+1 +1+5FF G +1HG+1K Lb 2j13o2 p3+4+1 +1+5F HG+1K LF0q4+11nmr 5result without sharing result with sharingF0m+1F G +4s6-3+6HG +1KSpecialized sharing arc-3-23d g74lFigure 6: Sharing mechanism.
Node f cannotbe shared, as this would cause the arc labeledF to derive an index colliding with node q.Contrary to many other structure sharingschemes (like (Malouf et al, 2000)), our algo-rithm allows sharing of nodes that are part ofthe grammar.
As nodes from the different in-put graphs are never assigned the same tableentry, they are always bound independentlyof each other.
(See the footnote for line 3 ofUnify1.
)The sharing version of Copy is similar tothe variant in (Tomabechi, 1992).
The extracheck can be implemented straightforwardlyby comparing the old offset with the offset forthe new nodes.
Because we derive the offsetsfrom index values associated with nodes, weneed to compensate for a difference betweenthe index of the shared node and the index itshould have in the new graph.
We store thisinformation in a specialized share arc.
Weneed to adjust Unify1 to handle share arcsaccordingly.Deferred Copying Just as we use a tablefor unification and copying, we also use a ta-ble for subsumption checking.
Tomabechi?salgorithm requires that the graph resulting01234564 5 6 7 8 9 10 11 12 13 14 15 16 17Time (seconds) Sentence length (no.
words)"basic""tomabechi""packed""pack+deferred_copy""pack+share""packed_on_dual_proc"Figure 7: Execution time (seconds).from unification be copied before it can beused for further processing.
This can resultin superfluous copying when the graph is sub-sumed by an existing graph.
Our techniqueallows subsumption to use the bindings gener-ated by Unify1 in addition to its own table.This allows us to defer copying until we com-pleted subsumption checking.Packed Nodes With a straightforward im-plementation of our algorithm, we obtain anode size of 8 bytes.4 By dropping the con-cept of a fixed node size, we can reduce thesize of atom and bottom nodes to 4 bytes.Type information can be stored in two bits.We use the two least significant bits of point-ers (which otherwise are 0) to store this typeinformation.
Instead of using a pointer forthe value field, we store nodes in place.
Onlyfor reentrancies we still need pointers.
Com-plex nodes require 8 bytes, as they includea pointer to the first node past its children(necessary for unification).
This scheme re-quires some extra logic to decode nodes, butsignificantly reduces memory consumption.4We do not have a type hierarchy.05101520253035404 5 6 7 8 9 10 11 12 13 14 15 16 17Heapsize(MB) Sentence length (no.
words)"basic""tomabechi""packed""pack+share"Figure 8: Memory used by graph heap (MB).4 ExperimentsWe have tested our algorithm with a medium-sized grammar for Dutch.
The system wasimplemented in Objective-C using a fixed ar-ity graph representation.
We used a test setof 22 sentences of varying length.
Usually, ap-proximately 90% of the unifications fails.
Onaverage, graphs consist of 60 nodes.
The ex-periments were run on a Pentium III 600EB(256 KB L2 cache) box, with 128 MB mem-ory, running Linux.We tested both memory usage and execu-tion time for various configurations.
The re-sults are shown in Figure 7 and 8.
It includesa version of Tomabechi?s algorithm.
Thenode size for this implementation is 20 bytes.For the proposed algorithm we have includedseveral versions: a basic implementation, apacked version, a version with deferred copy-ing, and a version with structure sharing.The basic implementation has a node size of8 bytes, the others have a variable node size.Whenever applicable, we applied the same op-timizations to all algorithms.
We also testedthe speedup on a dual Pentium II 266 Mhz.5Each processor was assigned its own scratchtables.
Apart from that, no changes to the5These results are scaled to reflect the speedup rel-ative to the tests run on the other machine.algorithm were required.
For more details onthe multi-processor implementation, see (vanLohuizen, 1999).The memory utilization results show signif-icant improvements for our approach.6 Pack-ing decreased memory utilization by almost40%.
Structure sharing roughly halved thisonce more.7 The third condition prohibitedsharing in less than 2% of the cases where itwould be possible in Tomabechi?s approach.Figure 7 shows that our algorithm does notincrease execution times.
Our algorithm evenscrapes off roughly 7% of the total parsingtime.
This speedup can be attributed to im-proved cache utilization.
We verified this byrunning the same tests with cache disabled.This made our algorithm actually run slowerthan Tomabechi?s algorithm.
Deferred copy-ing did not improve performance.
The addi-tional overhead of dereferencing during sub-sumption was not compensated by the savingson copying.
Structure sharing did not sig-nificantly alter the performance as well.
Al-though, this version uses less memory, it hasto perform additional work.Running the same tests on machines withless memory showed a clear performance ad-vantage for the algorithms using less memory,because paging could be avoided.5 Related WorkWe reduce memory consumption of graph uni-fication as presented in (Tomabechi, 1991)(or (Wroblewski, 1987)) by separating scratchfields from node structures.
Pereira?s(Pereira, 1985) algorithm also stores changesto nodes separate from the graph.
However,Pereira?s mechanism incurs a log(n) overheadfor accessing the changes (where n is thenumber of nodes in a graph), resulting inan O(n logn) time algorithm.
Our algorithmruns in O(n) time.6The results do not include the space consumedby the scratch tables.
However, these tables do notconsume more than 10 KB in total, and hence haveno significant impact on the results.7Because the packed version has a variable nodesize, structure sharing yielded less relative improve-ments than when applied to the basic version.
Interms of number of nodes, though, the two resultswere identical.With respect to over and early copying (asdefined in (Tomabechi, 1991)), our algorithmhas the same characteristics as Tomabechi?salgorithm.
In addition, our algorithm allowsto postpone the copying of graphs until aftersubsumption checks complete.
This would re-quire additional fields in the node structurefor Tomabechi?s algorithm.Our algorithm allows sharing of grammarnodes, which is usually impossible in otherimplementations (Malouf et al, 2000).
Aweak point of our structure sharing schemeis its extra condition.
However, our experi-ments showed that this condition can have aminor impact on the amount of sharing.We showed that compressing node struc-tures allowed us to reduce memory consump-tion by another 40% without sacrificing per-formance.
Applying the same technique toTomabechi?s algorithm would yield smallerrelative improvements (max.
20%), becausethe scratch fields cannot be compressed to thesame extent.One of the design goals of Tomabechi?s al-gorithm was to come to an efficient imple-mentation of parallel unification (Tomabechi,1991).
Although theoretically parallel uni-fication is hard (Vitter and Simons, 1986),Tomabechi?s algorithm provides an elegantsolution to achieve limited scale parallelism(Fujioka et al, 1990).
Since our algorithm isbased on the same principles, it allows paral-lel unification as well.
Tomabechi?s algorithm,however, is not thread-safe, and hence cannotbe used for concurrent unification.6 ConclusionsWe have presented a technique to reducememory usage by separating scratch fieldsfrom nodes.
We showed that compressingnode structures can further reduce the mem-ory footprint.
Although these techniques re-quire extra computation, the algorithms stillrun faster.
The main reason for this was thedifference between cache and memory speed.As current developments indicate that thisdifference will only get larger, this effect is notjust an artifact of the current architectures.We showed how to incoporate data-structure sharing.
For our grammar, the ad-ditional constraint for sharing did not posea problem.
If it does pose a problem, thereare several techniques to mitigate its effect.For example, one could reserve additional in-dexes at critical positions in a subgraph (e.g.based on type information).
These can thenbe assigned to nodes in later unifications with-out introducing conflicts elsewhere.
Anothertechnique is to include a tiny table with re-pair information in each share arc to allow asmall number of conflicts to be resolved.For certain grammars, data-structure shar-ing can also significantly reduce executiontimes, because the equality check (see line 3 ofUnify1) can intercept shared nodes with thesame address more frequently.
We did not ex-ploit this benefit, but rather included an offsetcheck to allow grammar nodes to be shared aswell.
One could still choose, however, not toshare grammar nodes.Finally, we introduced deferred copying.Although this technique did not improve per-formance, we suspect that it might be benefi-cial for systems that use more expensive mem-ory allocation and deallocation models (likegarbage collection).Since memory consumption is a major con-cern with many of the current unification-based grammar parsers, our approach pro-vides a fast and memory-efficient alternativeto Tomabechi?s algorithm.
In addition, weshowed that our algorithm is well suited forconcurrent unification, allowing to reduce ex-ecution times as well.References[Fujioka et al1990] T. Fujioka, H. Tomabechi,O.
Furuse, and H. Iida.
1990.
Parallelizationtechnique for quasi-destructive graph unifica-tion algorithm.
In Information Processing So-ciety of Japan SIG Notes 90-NL-80.
[Ghosh et al1997] S. Ghosh, M. Martonosi, andS.
Malik.
1997.
Cache miss equations: Ananalytical representation of cache misses.
InProceedings of the 11th International Confer-ence on Supercomputing (ICS-97), pages 317?324, New York, July 7?11.
ACM Press.
[Malouf et al2000] Robert Malouf, John Carroll,and Ann Copestake.
2000.
Efficient featurestructure operations witout compilation.
Nat-ural Language Engineering, 1(1):1?18.
[op den Akker et al1995] R. op den Akker, H. terDoest, M. Moll, and A. Nijholt.
1995.
Parsingin dialogue systems using typed feature struc-tures.
Technical Report 95-25, Dept.
of Com-puter Science, University of Twente, Enschede,The Netherlands, September.
Extended versionof an article published in E...[Pereira1985] Fernando C. N. Pereira.
1985.
Astructure-sharing representation for unification-based grammar formalisms.
In Proc.
of the23 rd Annual Meeting of the Association forComputational Linguistics.
Chicago, IL, 8?12Jul 1985, pages 137?144.
[Tomabechi1991] H. Tomabechi.
1991.
Quasi-destructive graph unifications.
In Proceedingsof the 29th Annual Meeting of the ACL, Berke-ley, CA.
[Tomabechi1992] Hideto Tomabechi.
1992.
Quasi-destructive graph unifications with structure-sharing.
In Proceedings of the 15th Interna-tional Conference on Computational Linguis-tics (COLING-92), Nantes, France.
[Tomabechi1995] Hideto Tomabechi.
1995.
De-sign of efficient unification for natural lan-guage.
Journal of Natural Language Process-ing, 2(2):23?58.
[van Lohuizen1999] Marcel van Lohuizen.
1999.Parallel processing of natural language parsers.In PARCO ?99.
Paper accepted (8 pages), toappear soon.
[van Lohuizen2000] Marcel P. van Lohuizen.
2000.Exploiting parallelism in unification-basedparsing.
In Proc.
of the Sixth InternationalWorkshop on Parsing Technologies (IWPT2000), Trento, Italy.
[Vitter and Simons1986] Jeffrey Scott Vitter andRoger A. Simons.
1986.
New classes for paral-lel complexity: A study of unification and othercomplete problems for P. IEEE Transactionson Computers, C-35(5):403?418, May.
[Wroblewski1987] David A. Wroblewski.
1987.Nondestructive graph unification.
In HowardForbus, Kenneth; Shrobe, editor, Proceedingsof the 6th National Conference on Artificial In-telligence (AAAI-87), pages 582?589, Seattle,WA, July.
Morgan Kaufmann.
