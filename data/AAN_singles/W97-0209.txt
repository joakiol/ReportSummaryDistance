Selectional Preference and Sense DisambiguationPh i l ip  Resn ikDepar tment  of Linguistics andInst i tute for Advanced Computer  StudiesUniversity of Mary landCollege Park, MD 20742 USAre snik@um?acs,  umd.
eduAbst ractThe absence of training data is a real prob-lem for corpus-based approaches to sensedisambiguation, one that is unlikely to besolved soon.
Selectional preference is tra-ditionally connected with sense ambigu-ity; this paper explores how a statisticalmodel of selectional preference, requiringneither manual annotation of selection re-strictions nor supervised training, can beused in sense disambiguation.1 In t roduct ionIt has long been observed that selectional con-straints and word sense disambiguation are closelylinked.
Indeed, the exemplar for sense disambigua-tion in most computational settings (e.g., see Allen's(1995) discussion) is Katz and Fodor's (1964) use ofBoolean selection restrictions to constrain semanticinterpretation.
For example, Mthough burgundy canbe interpreted as either a color or a beverage, onlythe latter sense is available in the context of Marydrank burgundy, because the verb drink specifies theselection restriction +LIQUID for its direct objects.Problems with this approach arise, however, assoon as the domain of interest becomes too large ortoo rich to specify semantic features and selection re-strictions accurately by hand.
This paper concernsthe use of selectional constraints for automatic sensedisambiguation in such broad-coverage settings.
Theapproach combines tatistical and knowledge-basedmethods, but unlike many recent corpus-based ap-proaches to sense disambiguation (?arowsky, 1993;Bruce and Wiebe, 1994; Miller et al, 1994), ittakes as its starting point the assumption that sense-annotated training text is not available.
Motivat-ing this assumption is not only the limited avail-ability of such text at present, but skepticism thatthe situation will change any time soon.
In markedcontrast to annotated training material for part-of-speech tagging, (a) there is no coarse-level setof sense distinctions widely agreed upon (whereaspart-of-speech tag sets tend to differ in the details);(b) sense annotation has a comparatively high er-ror rate (Miller, personal communication, reports anupper bound for human annotators of around 90%for ambiguous cases, using a non-blind evaluationmethod that may make even this estimate overlyoptimistic); and (c) no fully automatic method pro-vides high enough quality output to support the "an-notate automatically, correct manually" methodol-ogy used to provide high volume annotation by dataproviders like the Penn Treebank project (Marcus etal., 1993).2 Selectional Pre ference asStatistical Associat ionThe treatment of selectional preference used here isthat proposed by Resnik (1993a; 1996), combiningstatistical and knowledge-based methods.
The basisof the approach is a probabilistic model capturingthe co-occurrence behavior of predicates and con-ceptual classes in the taxonomy.
The intuition is il-lustrated in Figure 1.
The prior distribution PrR(c)captures the probability of a class occurring as theargument in predicate-argument relation R, regard-less of the identity of the predicate.
For example,given the verb-subject relationship, the prior prob-ability for (person) tends to be significantly higherthan the prior probability for (insect).
However,once the identity of the predicate is taken into ac-count, the probabilities can change -- if the verb isbuzz, then the probability for (insect) Can be ex-pected to be higher than its prior, and (person) willlikely be lower.
In probabilistic terms, it is the dif-ference between this conditional or posterior distri-bution and the prior distribution that determinesselectional preference.Information theory provides an appropriate wayto quantify the difference between the prior and pos-terior distributions, in the form of relative entropy(Kullback and Leibler, 1951).
The model definesthe selectional preference strength of a predicate as: ?SR(p) = D(er(clp)\[I Pr(c))= E pr(clp)log Pr(clp)Pr(c) "Cmmmmmmmmmmmmmmmmmm\[\]52mm.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
!I 0 Pr c) 0 \[7 iPr(c'bu  )n | - ~!L person insect ... , person insect ...Prior PosteriorFigure I: Prior and posterior distributions over argument classes.Intuitively, SR(p) measures how much information,in bits, predicate p provides about the conceptualclass of its argument.
The better Pr(c) approximatesPr(cip), the leas influence p is having on its argu-ment, and therefore the less strong its selectionalpreference.Given this definition, a natural way to character-ize the "semantic fit" of a particular class as theargument to a predicate is by its relative contribu-tion to the overall selectional preference strength.
Inparticular, classes that fit very well can be expectedto have higher posterior probabilities, compared totheir priors, as is the case for (insect) in Figure 1.Formally, selectional association is defined as:Am(p, c) -- 1 Pr(c\[p) Pr(c\[p) log Pr(c) "This model of selectional preference has turnedout to make reasonable predictions about humanjudgments of argument plausibility obtained by psy-cholinguistic methods (Resnik, 1993a).
Closely re-lated proposals have been applied in syntactic dis-ambiguation (Resnik, 1993b; Lauer, 1994) and toautomatic acquisition of more KatzFodoresque se-lection restrictions in the form of weighted disjunc-tions (Ribas, 1994).
The selectional association hasalso been used recently to explore apparent cases ofsyntactic optionality (Paola Merlo, personal commu-nication).3 Estimation IssuesIf taxonomic classes were labeled explicitly in atraining corpus, estimation of probabilities in themodel would be fairly straightforward.
But sincetext corpora contain words, not classes, it is neces-sary to treat each occurrence of a word in an ar-gument position as if it might represent any of theconceptual classes to which it belongs, and assignfrequency counts accordingly.
At present, this isdone by distributing the "credit" for an observa-tion uniformly across all the conceptual classes con-taining an observed argument.
Formally, given apredicate-argument relationship R (for example, theverb-object relationship), a predicate p, and a con-ceptual class c,~'~ count(p, w) freqR(p,c) ~ ~ ~ 'tvEcwhere countR(p, w) is the number of times word wwas observed as the argument of p with respect oR, and classes(w) is the number of taxonomic classesto which w belongs.
Given the frequencies, proba-bilities are currently estimated using max imum like-lihood; the use of word classes is itself a form ofsmoothing (cf.
Pereira et al (1993)).
IThis estimation method is similar to that used byYarowsky (1992) for Roget's thesaurus categories,and works for similar reasons.
As an example, con-sider two instances of the verb-object relationship ina training corpus, drink coffee and drink wine.
Cof-fee has 2 senses in the WordNet 1.4 noun taxonomy,and belongs to 13 classes in all, and wine has 2 sensesand belongs to a total of 16 classes.
This meansthat the observed countverb_obj(drink , coffee) = 1will be distributed by adding 1-~ to the joint fre-quency with drink for each of the 13 classes con-taining coffee.
Similarly, the joint frequency withdrink will be incremented by ~ for each of the 16classes containing wine.
Crucially, although each ofthe two words is ambiguous, only those taxonomicclasses containing both words - -  e.g., (beverage) - -receive credit for both observed instances.
In gen-eral, because different words are ambiguous in dif-ferent ways, credit tends to accumulate in the tax-onomy only in those classes for which there is realevidence of co-occurrence; the rest tends to disperseunsystematically, resulting primarily in noise.
Thus,despite the absence of class annotation i  the train-ing text, it is still possible to arrive at a usable esti-mate of class-based probabilities.4 An Unsupervised Method forSense DisambiguationTable 1 presents a selected sample of Resnik's(1993a) comparison with argument plausibility judg-ments made by human subjects.
What is most in-teresting here is the way in which strongly selecting1Word w is typically the head of a noun phrase, whichcould lead the model astray - -  for example, toy sol-diers behave differently from soldiers (McCawley, 1968).In principle, addressing this issue requires that nounphrases be mapped to taxonomic lasses based on theircompositional interpretation; however, such complica-tions rarely axise in practice.53Verb I Object \[ A(Verb, Object) Classwrite letter 7.26 /irritinglread article 6.80 ~writing~warn driver 4.73 (person)hear story 1.89 (communication)remember reply 1.31 (statement)expect visit 0.59 (act)Table I: Selectional ratings for plausible objectsverbs "choose" the sense of their arguments.
For ex-ample, letter has 3 senses in WordNet, 2and belongsto 19 classes in all.
In order to approximate its plau-sibility as the object of wrfle, the selectional ssocia-tion with wrote was computed for all I9 classes, andthe highest value returned ~ in this case, (writing)("anything expressed in letters; reading matter").Since only one sense of letter has this class as an an-cestor, this method of determining argument plausi-bility has, in essence, performed sense disambigua-tion as a side effect.This observation suggests the following simple al-gorithm for disambignation by selectional prefer-ence.
Let n be a noun that stands in relationshipR to predicate p, and let {sl, ..., st} be its possiblesenses.
For i from 1 to h, compute:C, = {clc is an ancestor ofsi}as = max AR(p,c) cEC~and assign as as the score for sense st.
The simplestway to use the resulting scores, following Miller etal.
(1994), is as follows: if n has only one sense,select it; otherwise select the sense st for which at isgreatest, breaking ties by random choice.5 Eva luat ionTask and materials.
Test and training materialswere derived from the Brown corpus of AmericanEnglish, all of which has been parsed and manuallyverified by the Penn T~eebank project (Marcus etal., 1993) and parts of which have been manuallysense-tagged by the WordNet group (Miller et al,1993).
A parsed, sense-tagged corpus was obtainedby mergingthe WordNet sense-tagged corpus (ap-proximately 200,000 words of source text from theBrown corpus, distributed across genres) with thecorresponding Penn Treebank parses, a The rest ofthe Brown corpus (approximately 800,000 words ofsource text) remained as a parsed, but not sense-tagged, training set.3(1) Written message, (2) varsity letter, (3) alpha-betic character.3The merge was mostly automatic, requiring manualintervention for only 3 of 103 files.The test set for the verb-object relationship wasconstructed by first training a selectional preferencemodel on the training corpus, using the T~eebank'stgrep utility to extract verb-object pairs from parsetrees.
The 100 verbs that select most strongly fortheir objects were identified, excluding verbs appear-ing only once in the training corpus; test instances ofthe form (verb, object, correct sense) were then ex-tracted from the merged test corpus, including alltriples where verb was one of the 100 test verbs.
4Evaluation materials were obtained in the samemanner for several other surface syntactic reia-tionships, including verb-subject (John ~ admires),adjective-noun (tall =~ building), modifier-head(river =~ bank), and head-modifier (river ~= bank).Basel ine.
Following Miller et al (1994), disam-biguation by random choice was used as a baseline:if a noun has one sense, use it; otherwise select atrandom among its senses.Results .
Since both the algorithm and the base-line may involve random choices, evaluation i volvedmultiple runs with different random seeds.
Table 2summarizes the results, taken over I0 runs, consid-ering only ambiguous test cases.
All differences be-tween the means for algorithm and baseline were sta-tistically significant.Discussion.
The results of the experiment showthat disambignation using automatically acquiredselectional constraints leads to performance signifi-cantly better than random choice.
Not surprisingly,though, the results are far from what one might ex-pect to obtain with supervised training.
In that re-spect, the most direct point of comparison is the per-formance of Miller et al's (1994) frequency heuristicalways choose the most frequent sense of a wordas evaluated using the full sense-tagged corpus,including nouns, verbs, adjectives, and adverbs.
Forambiguous words, they report 58.2% correct, as com-pared to a random baseline of 26.8%.Crucially, however, the frequency heuristic re-quires sense-tagged training data (Miller et al eval-uated via cross-validation), and this paper startsfrom the assumption that such data are unavail-able.
A fairer comparison, therefore, considers al-4 Excluded were some inapplicable cases, e.g.
whereobject was a proper noun tagged as (person).54Relationshipverb-objectverb-subjecthead-modifiermodifier-headadjective-noun% Correctmean o" I min max(baseline) 28.5 5.91 18.0 35.0(sel.
pref.)
44.3 4.90 36.0 51.0(baseline) 29.1 5.23 20.0 38.0(sel.
pref.)
40.8 2.86 36.0 44.0(baseline) 32.8 7.00 23.0 JJ.0(sel.
pref.)
40.2 5.99 33.0 51.0(baseline) 30.8 6.25 24.0 JO.Osel.
pref.)
39.9 2.60 35.0 43.0baseline) 29.1 8.40 16.0 38.0(sel.
pref.)
35.3 3.95 31.0 40.0Table 2: Experimental resultsternative unsupervised a lgor i thms-  though unfor-tunately the literature contains more proposed algo-rithms than quantitative evaluations of those algo-rithms.
One experiment where results were reportedwas conducted by Cowie et al (1992); their methodinvolved using a stochastic search procedure to max-imize the overlap in dictionary definitions (LDOCE)for alternative senses of words co-occurring in a sen-tence.
They report an accuracy of 72% for dis-ambiguation to the homograph level, and 47% fordisambiguation to the sense level.
Since the taskhere involved WordNet sense distinctions, which arerather fine grained, the latter value is more appro-priate for comparison.
Their experiment was moregeneral in that they did not restrict themselves tonouns; on the other hand, their test set involved is-ambiguating words taken from full sentences, o thepercentage correct may have been improved by thepresence of unambiguous words.Sussna (1993) has also looked at unsupervised dis-ambiguation of nouns using WordNet.
Like Cowieet al, his algorithm optimizes a measure of semanticcoherence over an entire sentence, in this case pair-wise semantic distance between nouns in the sen-tence as measured using the noun taxonomy.
Com-parison of results is somewhat difficult, however, fortwo reasons.
First, Sussna used an earlier version ofWordNet (version 1.2) having a significantly smallernoun taxonomy (35K nodes vs. 49K nodes).
Sec-ond, and more significant, in creating the test data,Sussna's human sense-taggers (tagging articles fromthe Time IR test collection) were permitted to tag anoun with as many senses as they felt were "good,"rather than making a forced choice; Sussna developsa scoring metric based on that fact rather than re-quiring exact matches to a single best sense.
Thisis quite a reasonable move (see discussion below),but unfortunately not an option in the present ex-periment.
Nonetheless, ome comparison is possible,since he reports a "% correct," apparently treatinga sense assignment as correct if any of the "good"senses is chosen - -  his experiments have a lowerbound (chance) of about 40% correct, with his algo-rithm performing at 53-55%, considering only am-biguous cases.The best results reported for an unsupervisedsense disambiguation method are those of Yarowsky(1992), who uses evidence from a wider context (awindow of 100 surrounding words) to build up aco-occurrence model using classes from Roget's the-saurus.
He reports accuracy figures in the 72-99%range (mean 92%) in disambiguating test instancesinvolving twelve "interesting" polysemons words.
Asin the experiments by Cowie et al, the choice ofcoarser distinctions presumably accounts in part forthe high accuracy.
By way of comparison, somewords in Yarowsky's test set would require choos-ing among ten senses in WordNet, as compared toa maximum of six using the Roget's thesaurus cat-egories; the mean level of polysemy for the testedwords is a six-way distinction in WordNet as com-pared to a three-way distinction in Roget's the-saurus.As an aside, a rich taxonomy like WordNet per-mits a more continuous view of the sense vs. ho-mograph distinction.
For example, town has threesenses in WordNet, corresponding to an administra-tive district, a geographical rea, and a group of peo-ple.
Given town as the object of leave, selectionalpreference will produce a tie between the first twosenses, since both inherit their score from a com-mon ancestor, (location).
In effect, the automaticselection of a class higher in the taxonomy as hav-ing the highest score provides the same coarse cate-gory that might be provided by a homograph/sensedistinction in another setting.
The choice of coarsercategory varies dynamically with the context: as theargument in rural town, the same two senses till tie,but with (region) (a subclass of (location)) as thecommon ancestor that determines the score.In other work, Yarowsky (1993) has shown thatlocal collocational information, including selectionalconstraints, can be used to great effect in sense dis-ambiguation, though his algorithm requires super-55vised training.
The present work can be viewed asan attempt o take advantage of the same kind ofinformation, but in an unsupervised setting.6 Conc lus ions  and  Future  WorkAlthough the definition of selectional preferencestrength is motivated by the use of relative en-tropy in information theory, selectional associationis not; the approach would benefit from experimen-tation with alternative statistical association mea-sures, particularly a comparison with simple mutualinformation and with the likelihood ratio.
Combin-ing information about selectional preference couldalso be helpful, e.g., where a noun is both the objectof a verb and modified by an adjective, though suchcases are rarer than one might expect.More important is information beyond selectionalpreference, notably the wider context utilized byYarowsky (1992).
Performance of the method ex-plored here is limited at present, though not sur-prisingly so when taken in the context of previousattempts at unsupervised disambiguation using fine-grained senses.
One main message to take awayfrom this experiment is the observation that, al-though selectional preferences are widely viewed asan important factor in disambiguation, their practi-cal broad-coverage application appears limited - -  atleast when disambiguating nouns - -  because manyverbs and modifiers simply do not select stronglyenough to make a significant difference.
They mayprovide some evidence, but most likely only as acomplement to other sources of information such asfrequency-based priors, topical context, and the like.AcknowledgementsMuch of this work was conducted at Sun Microsys-tems Laboratories in Chelmsford, Massachusetts.56\[\]mmmnmmmmmmmmmmmmmmmmmmmmmmReferencesJames Allen.
1995.
Natural Language Understand-ing.
The Benjamin/Cummings Publishing Com-pany.Rebecca Bruce and Janyce Wiebe.
1994.
Word-sense disambiguation using decomposable mod-els.
In Proceedings ofthe 32nd Annual Conferenceof the Association for Computational Linguistics,Las Cruces, New Mexico, June.Jim Cowie, Joe Guthrie, and Louise Guthrie.
1992.Lexical disambiguation using simulated anneal-ing.
In Proceedings ofthe 14th International Con-terence on Computational Linguistics (COLING-g2), pages 359-365, Nantes, France, August.J.
J. Katz and J.
A. Fodor.
1964.
The structure ofa semantic theory.
In J.
A. Fodor and J. J. Katz,editors, The Structure of Language, chapter 19,pages 479-518.
Prentice Hall.S.
Kullback and R. A. Leibler.
1951.
On informationand sufficiency.
Annals of Mathematical Statis-tics, 22:79-86.Mark Laner.
1994.
Conceptual ssociation for com-pound noun analysis.
In Proceedings ofthe 32ndAnnual Meeting of the Association for Computa-tional Linguistics, Las Cruces, New Mexico, June.Student Session.Mitchell P. Marcus, Beatrice Santorini, and MaryAnn Marcinkiewicz.
1993.
Building a large an-notated corpus of English: the Penn Treebank.Computational Linguistics, 19:313-330.James McCawley.
1968.
The role of semantics in agrammar.
In Emmon Bach and Robert Harms,editors, Universals in Linguistic Theory, pages124-169.
Holt, Rinehart and Winston.George Miller, Claudia Leacock, Randee Tengi, andRoss Bunker.
1993.
A semantic oncordance.
InARPA Workshop on Human Language Technol-ogy.
Morgan Kanfmann, March.George Miller, Martin Chodorow, Shari Landes,Claudia Leacock, and Robert Thomas.
1994.
Us-ing a semantic oncordance for sense identifica-tion.
In ARPA Workshop on human LanguageTechnology, Plainsboro, N J, March.Fernando Pereira, Naftali Tishby, and Lillian Lee.1993.
Distributional clustering of English words.In Proceedings of the 3Ist Annual Meeting of theAssociation for Computational Linguistics (A CL-93), Morristown, New Jersey, June.
Associationfor Computational Linguistics.Philip Resnik.
1993a.
Selection and Information:A Class-Based, Approach to Lexical Relationships.Ph.D.
thesis, University of Pennsylvania, Decem-ber.Philip Resnik.
1993b.
Semantic lasses and syntac-tic ambiguity.
In Proceedings ofthe 1993 ARPAHuman Language Technology Workshop.
MorganKanfmann, March.Philip Resnik.
1996.
Selectional constraints: Aninformation-theoretic model and its computa-tional realization.
Cognition, 61:127-159.Francesc Ribas.
1994.
An experiment on learningappropriate selectional restrictions from a parsedcorpus.
In Proceedings ofCOLING 1994.Michael Sussna.
1993.
Word sense disambiguationfor free-text indexing using a massive semanticnetwork.
In Proceedings of the Second Interna-tional Conference on Information and KnowledgeManagement (CIKM-93), Arlington, Virginia.David Yarowsky.
1992.
Word-sense disambigua-tion using statistical models of Roger's cate-gories trained on large corpora.
In Proceedings ofthe 1Jth International Conference on Computa-tional Linguistics (COLING-92), pages 454-460,Nantes, France, July.David Yarowsky.
1993.
One sense per collocation.ARPA Workshop on Human Language Technol-ogy, March.
Princeton.57
