Experiments in Constructing a Corpus of Discourse TreesDaniel MarcuInformation Sciences Institute andDepartment of Computer ScienceUniversity of Southern California4676 Admiralty Way, Suite 1001Marina del Rey, CA 90292marcu @isi.
eduEstibaliz AmorrortuDepartment of LinguisticsUniversity of Southern CaliforniaLos Angeles, CA 90089amorrort@usc.eduMagdalena RomeraDepartment of LinguisticsUniversity of Southern CaliforniaLos Angeles, CA 90089romera@usc.eduAbstractWe discuss a tagging schema nd a tagging tool forlabeling the rhetorical structure of texts.
We alsopropose a statistical method for measuring agree-ment of hierarchical structure annotations and wediscuss its strengths and weaknesses.
The statis-tical measure we use suggests that annotators canachieve good levels of agreement on the task ofdetermining the high-level, rhetorical structure oftexts.
Our empirical experiments also suggest thatbuilding discourse parsers that incrementally de-rive correct rhetorical structures of unrestricted textswithout applying any form of backtracking is unfea-sible.1 IntroductionEmpirical studies of discourse structure have pri-marily focused on identifying discourse segmentboundaries and their linguistic correlates.
Verylittle attention has been paid so far to the high-level, rhetorical relations that hold between dis-course segments.
In some cases, the role of theserelations was considered to fall outside the scope ofa study (Flammia and Zue, 1995); in other cases,judgements were made with respect o a taxon-omy of very few intention-based relations (usuallydominance and satisfaction-precedence) (Grosz andHirschberg, 1992; Nakatani et al, 1995; Hirschbergand Litman, 1987; Passonneau and Litman, 1997;Carletta et al, 1997).
And in the only case in whicha rich taxonomy of 29 relations was used (Moserand Moore, 1997), the corpus was small and spe-cific to a very restricted genre: written interactionsbetween a student and tutor on the subject of faultlocation and repair in electronic ircuitry.In spite of many influential proposals in the lin-guistic of discourse structures and relations (Ballardet al, 1971; Grimes, 1975; Halliday and Hasan,1976; Martin, 1992; Mann and Thompson, 1988;Sanders et al, 1992; Sanders et al, 1993; Asher,1993; Lascarides and Asher, 1993; Knott, 1995;Hovy and Maier, 1993), a number of empiricalquestions remain to be answered.?
Can human judges construct rich discoursestructures in a manner that ensures inter-judgeagreement that is statistically significant??
How can one measure the agreement?#" How should judges (and programs) constructthe discourse structure of texts; should theyfollow a top-down, bottom-up, or an incremen-tal procedure??
How does the genre of a text influence the de-gree to which judges achieve agreement on thetask of rhetorical tagging?In this paper, we describe an experiment designedto answer these questions.2 The exper iment2.1 ToolsWe used as starting point O'Donnell's discourse an-notation tool (1997), which we improved signifi-cantly.
The original tool constrains human judges toconstruct rhetorical structures in a bottom-up fash-ion: as a first step, judges determine the elementarydiscourse units (edu) of a text; subsequently, theyrecursively assemble the units into discourse trees,in a bottom-up fashion.
As texts get larger, the an-notation process becomes impractical.We modified O'Donnell's tool in order to enableannotators to construct discourse structures in an in-cremental fashion as well.
At any time t during theannotation process, annotators have access to twopanels (see figure 1 for an example):?
The upper panel displays in the style of Mannand Thompson (1988) the discourse structurebuilt up to time t. The discourse structure isNNNN\[ \ ]NImmmm\ [ \ ]lINNNNNNNnmmNNmmNNUNmmIllU48Mars<l>With its distant orbit<p> - 50 percent farther from the sun than Earth -</p>and slim atmospheric blanket, <2>Mars experiences frigid weather:;onditions.<3> Surface temperatures typically ave:rage about -G0 degreesZaleius<p> (.-7~ degrees Fahrenheit) </p>at the equator<4> and can dip to -123degree~ C near the poles.<5> Only the midday sun at tropical latitudes is~arm enough<5> to thaw ice on occasion, <7>but any liquid water formed inthis way would evaporate almost instantly<8> because of ~he lowatmos~hericpress~re,<9>Although the atmosphere holds a small amount of water, and water-lceclouds sometimes develop, most Martian weather involves blowing dustor carbon dioxide.Figure 1: A snapshot of our annotation toola tree whose leaves correspond to edus andwhose internal nodes correspond to contiguoustext spans.
Each internal node is characterizedby a rhetorical relation, which is a relation thatholds between two non-overlapping text spanscalled NUCLEUS and SATELLITE.
(There area few exceptions to this rule: some relations,such as the LIST relation that holds betweenunits 4 and 5 and the CONTRAST relation thatholds between spans \[6,7\] and \[8,9\] in figure 1,are multinuclear.)
The distinction between u-clei and satellites comes from the empiricalobservation that the nucleus expresses what ismore essential to the writer's purpose/intentionthan the satellite; and that the nucleus of arhetorical relation is comprehensible indepen-dent of the satellite, but not vice versa.
Someedus may contain parenthetical units, i.e., em-bedded units whose deletion does not affect heunderstanding ofthe edu to which they belong.For example, the unit shown in italics in (1) isparenthetic.This book, which 1 have received (1)from John, is the best book that Ihave read in a while.?
The lower panel displays the text read bythe annotator up to time t and only the firstsentence that immediately follows the labelededus.Annotators can create elementary and parentheticalunits by clicking on their boundaries; immediately49add a newly created unit to a partial discourse struc-ture using operations specific to tree-adjoining andbottom-up arsers; postpone the construction of apartial discourse structure until the understandingof the text enables them to do so; take discoursestructures apart and re-connect them; change rela-tion names and nuclearity assignments; undo anynumber of steps; etc.
In other words, annotatorshave complete control over the discourse construc-tion strategy that hey employ.All actions taken by annotators are automaticallylogged.2.2 Annotation protocolOne of us initially prepared a manual that containedinstructions pertaining to the functionality of thetool, definitions of edus and rhetorical relations, anda protocol that was supposed to be followed duringthe annotation process (Marcu, 1998).Edus were defined functionally as clauses orclause-like units that are unequivocally the NU-CLEUS or SATELLITE of a rhetorical relation thatadds some significant information to the text.
Forexample, because of the low atmospheric pressurein text (2) is not a fully fleshed clause.
However,since it is the SATELLITE of an EXPLANATION rela-tion, it should be treated as elementary.\[Only the midday sun at tropical atitudes (2)is warm enough\] [to thaw ice on occa-sion,\] \[but any liquid water formed inthis way would evaporate almost instantly\]\[because ofthe low atmospheric pressure.\]A total of 70 rhetorical relations were partitionedinto clusters, each cluster containing a subset of re-lations that shared some rhetorical meaning.
Forexample, one cluster contained the contrast-likerhetorical relations of ANTITHESIS, CONTRAST,and CONCESSION.
Another cluster contained REA-SON, EVIDENCE, and EXPLANATION.
Each rela-tion was paired with an informal definition given inthe style of Mann and Thompson (1988) and Moserand Moore (1997) and one or more examples.
Noexplicit distinction was made between intentionaland informational relations.
In addition, we alsomarked two constituency relations that were ubiqui-tous in our corpora nd that often subsumed com-plex rhetorical constituents, and one textual rela-tion.
The constituency relations were ATTRIBUTION,which was used to label the relation between a re-porting and a reported clause, and APPOSITION.
Thetextual relation was TEXTUALORGANIZATION; itwas used to connect in an RST-like manner the tex-tual spans that corresponded to the title, author, andtextual body of each document in the corpus.
Wealso enabled the annotators touse the label OTHER-RELATION whenever they felt that no relation in themanual captured sufficiently well the meaning of arhetorical relation that held between two text spans.In an attempt to manage the inherent rhetoricalambiguity of texts, we also devised a protocol thatlisted the clusters of relations in decreasing order ofspecificity.
Hence, the relations at the beginning ofthe protocol were more specific than the relationsat the end of the protocol.
The protocol specifiedthat in assigning rhetorical relations judges shouldchoose the first relation in the protocol whose def-inition was consistent with the case under consid-eration.
For example, it is often the case that whenan EVIDENCE relation holds between two segments,an ELABORATION relation holds as well.
BecauseEVII~ENCE is more specific than ELABORATION, itcomes first in the protocol, and hence, wheneverboth of these relations hold, only EVIDENCE is sup-posed to be used for tagging.The protocol specified that the rhetorical taggingshould be performed incrementally.
That is, if anannotator c eated an edu at time t and if she knewhow to attach that edu to the discourse structure, shewas supposed to do so at time t + 1.
If the text readup to time t did not warrant such a decision, theannotator was supposed to determine the edus of thesubsequent text and complete the construction ofthediscourse structure as soon as sufficient informationbecame available.2_3 Materials and methodSince we were aware of no previous tudy that in-vestigated thoroughly the coverage of any set ofrhetorical relations or any protocol, we felt neces-sary to divide the experiment into a training and anannotation stage.
During the training stage, eachof us built the discourse structures of 10 texts thatvaried in size from 162 to 1924 words.
The textsbelonged to the news story, editorial, and scientificgenres.
We had extensive discussions after the tag-ging of each text.
During these discussions, werefined the definition of edu, the definitions andnumber of rhetorical relations that we used, andthe order of the relations in the protocol.
Eventu-ally, our protocol comprised 50 mononuclear rela-tions and 23 multinuclear relations.
All relationswere divided into 23 clusters of rhetorical similarity50mmmmmmmmmmmmMUC Corpus WSJ Corpus Brown CorpusRelation Percent Relation Percent Relation Percent .
.
.
.
., ,,,ELABORATION-ADDITIONAL ELABORATIONoADDITIONAL 13.80ATTR1B UTION 12.07LIST 9.99TEXTUALORGANIZATION 6.23APPOSITION 5.02TOPICSHIFT 4.76JOINT 4.19CONTRAST 3.99ELAB ORATION-OB JECT-ATTI B UTE 2,88EVIDENCE 2.54BACKGROUND 2.42PURPOSE 2.26ELABORATION-GENERAL-SPECIFIC 2.21TOPIC-DRIFT 1.85SEQUENCE 1.59OTHER-RELAT ION 0.38ELABORATION-ADDITIONAL 17.41ATTRIBUTION 14.78LIST I 1.25CONTRAST 6.84JOINT 4.35EVIDENCE 3.82APPOSITION 3.31TOPIC-SHIFT 2.96BACKGROUND 2.41ELAB OR ATI ON -OB JECT- ATTI BUTE 2.37PU R POSE 2.2 iELABORATION-GENERAL-SPECIFIC 2.19TOPIC-DRIFT 1.88CONDITION !.77SEQUENCE 1.31o..OTHER-RELATION 0.3221.64LIST 16.29JOINT 6.58CONTRAST 5.60TEXTUALORGANIZATION 3.22PURPOSE 2.88EXPLANATION-ARGUMENTATIVE 2.68SEQUENCE 2.57ELABORATION-GENERAL-SPECIFIC 2.23TOPIC-SHIFT 2.12BACKGROUND 1.96CONCESSION 1.84ELAB OR ATI ON - OBJECT- ATTIBUTE !
.76CONDITION 1.76EVIDENCE 1.62OTHER-RELATION 0.19Table 1: Distribution of the most frequent rhetorical relations in each of the three corpora.
(see (Marcu, 1998) for the complete list of rhetori-cal relations and protocol).During the annotation stage, we independentlybuilt the discourse structures of 90 texts by follow-ing the instructions in the protocol; 30 texts weretaken from the MUC7 co-reference corpus, 30 textsfrom the Brown-Learned corpus, and 30 texts fromthe Wall Street Journal (WSJ) corpus.
The MUCcorpus contained news stories about changes in cor-porate xecutive management personnel; the Browncorpus contained long, highly elaborate scientificarticles; and the WSJ corpus contained editorials.The average number of words for each text was 405in the MUC corpus, 2029 in the Brown corpus, and878 in the WSJ corpus.
The average number ofedusin each text was 52 in the MUC corpus, 170 in theBrown corpus, and 95 in the WSJ corpus.
Each ofthe MUC texts was tagged by all three of us; eachof the Brown and WSJ texts was tagged by only twoof us.
Table 1 shows the 15 relations that were usedmost frequently by annotators in each of the threecorpora; the associated percentages reflect averagescomputed over all annotators.
The table also showsthe percentage ofcases in which the annotators usedthe label OTHER-RELAT1 ON.Problems with the method.
It has been arguedthat the reliability of a coding schema can be as-sessed only on the basis of judgments made bynaive coders (Carletta, 1996).
Although we agreewith this, we believe that more experiments of thekind reported here will have to be carried out beforewe can produce a tagging manual that is usable bynaive coders.
In our experiment, it is not clear howmuch of the agreement came from the manual andhow much from the common understanding that wereached uring the training session.
For our annota-tion task, we felt that it was more important to arriveat a common understanding instead of tightly con-trolling how this understanding was reached.
Thisposition was taken by other computational linguistsas well (Carletta et al, 1997, p. 25).3 Computing agreement among judgesWe computed agreement figures with respect to theway we set up edu boundaries and the way we builthierarchical discourse structures of texts.3.1 Reliability of tagging the edu andparenthetical unit boundariesIn order to compute how well we agreed on deter-mining the edu and parenthetical unit boundaries,we used the kappa coefficient k (Siegel and Castel-lan, 1988), a statistic used extensively in previousempirical studies of discourse.
The kappa coeffi-cient measures palrwise agreement among a set ofcoders who make category judgements, correctingfor chance xpected agreement (see equation (3) he-low, where P(A) is the proportion of times a set ofcoders agree and P(E) is the proportion of times aset of coders are expected to agree by chance).k -~ P(A) - P(E) (3)1 -  P (E )Carletta (1996) suggests that the units over whichthe kappa statistic is computed affects the outcome.To account for this, we computed the kappa statis-tics in two ways:511.
The first statistic, kw, reflects inter-annotatoragreement under the assumption that edu andparenthetical unit boundaries can be insertedafter any word in a text.
Because many ofthe words occur within units and not at theirboundaries, the chance agreement is very high,and therefore, k~ tends to be higher than thestatistic discussed below.2.
The second statistic, ku, reflects inter-annotator agreement under the assumption thatedu and parenthetical unit boundaries can oc-cur only at locations judged to be boundariesby at least one annotator.
This statistic offersthe most conservative measure of agreement.3.2 Reliability of tagging the discoursestructure of texts3.2.1 Previous workWe are aware of only one proposal for computingagreement with respect o the way human judgesconstruct hierarchical structures, that of Flammiaand Zue (1995).
This proposal appears to be ade-quate for computing the observed agreement, but itprovides only a lower bound on the chance agree-ment, and hence, only an upper bound on the kappacoefficient.
With the exception of Flammia andZue, other esearchers elied primarily on cascadedschemata for computing agreement among hierar-chical structures.
For example, Carletta et al (1997)computed agreement on a coarse segmentation levelthat was constructed on the top of finer segments,by determining how well coders agreed on wherethe coarse segments started, and, for agreed starts,by computing how coders agreed on where coarsesegments ended.
Moser and Moore (1997) deter-mined first the kappa coefficient with respect to theway judges assigned boundaries at the highest levelof segmentation.
Then judges met and agreed on aparticular segmentation.
Each high-level segmentwas then independently broken into smaller seg-ments and the process was repeated recursively un-til the elementary unit level was reached.
AlthoughMoser and Moore's approach accommodates read-ily the traditional computation of kappa, it is im-practical for large texts.
In addition, since judgesmeet and agree on every level, it is likely that theagreement at finer levels of detail is influenced byjudges' interaction.3.2.2 Our approachIn order to compute the kappa statistics we deviseda new method whose core idea is to map hierar-chical structures into sets of units that are labeledwith categorial judgments (see (Marcu and Hovy,1999) for details).
Consider, for example, the twohierarchical structures shown in figure 2.a, in whichfor simplicity, we focus only on the nuclear statusof each segment (Nucleus or Satellite).
In order toenable the computation of the kappa agreement wetake as elementary all textual units found betweentwo consecutive t xtual boundaries, independent ofwhether one or multiple judges chose those bound-aries.
Hence, for the segmentations in figure 2.a weconsider that the text is made of 7 units;judge 1tookas elementary segments \[0,1 \], \[2,2\], \[3,3\], \[4,5\] and\[6,6\], while judge 2 took as elementary segments\[0,0\], \[1,1\], \[2,2\], \[3,3\], \[4,4\] and \[5,6\].The mapping between the hierarchical structureand a set of units labeled with categorial judgmentsis straightforward if we consider not only the seg-ments that play an active role in the structure (thenuclei and the satellites) but also the segments thatare hot explicitly represented.
For example, for seg-mentation 1, there is no active segment across units\[2,4\], \[2,5\], and \[2,6\].
Similarly, for segmentation2, there is no active segment across units \[4,5\] and\[6,6\].
By associating the label NONE to the textualunits that do not play an active role in a hierarchi-cal representation, each discourse structure can bemapped into a set that explicitly enumerates all pos-sible spans that range over the units in the text.
Fora text of n units there are n spans of length 1, n - 1spans of length 2 .
.
.
.
.
and 1 span of length n. Hence,each hierarchical structure of n units can be mappedintoa set of n+ (n -  1 )+.
.
.
+ 1 = n(n+ 1)/2 units,each labeled with a categorial judgment.
And com-puting the kappa statistic for such sets is a prob-lem with a textbook solution (Siegel and Castellan,1988).In the example in figure 2, we therefore computethe kappa statistics between the two hierarchies bycomputing the kappa statistics between the two setsthat are represented in figure 2.b.The hierarchical structures in figure 2 correspondto nuclearity judgments.
However, the schema weuse here is general, since it can accommodate hecomputation of kappa statistic for judgments at thesegmentation a d rhetorical levels as well.
In fact,the schema can be applied to any discourse, syntac-tic, or semantic hierarchical labeling.In our experiment, we computed the kappa statis-tic with respect to four categorial judgments:1. ks reflects the agreement with respect to the hi-52Segmentation 1SNN So I 2IN NN Ss NN S3 4 5 6N SNN NSSegmentation 2 Na)Segment\[0,0\] none\[0,1\] N\[0,2\] N\[0.3\] s\[0,4\] none\[0,5\] none\[0,6\] N\[ I , I I  none\[1,21 none\[1,3\] none\[1,4\] none\[4,4\] none\[4,51 N\[4,61 N\[5,5\] none\[5,6l none\[6,6\] SSegmentation I Segmentation 2NNNsnonenoneNNnonenonenoneNnon~Nnonesnoneb)Figure 2: Computing the kappa statistic among hierarchical structureserarchical ssignment of discourse segments;2. kn reflects the agreement with respect o thehierarchical ssignment ofnuclearity;3. k r reflects the agreement with respect o thehierarchical ssignment of rhetorical relations;4.
/err reflects the agreement with respect o thehierarchical ssignment of rhetorical relationsunder the assumption that rhetorical relationsare chosen from a reduced set of only 18 re-lations, each relation representing one or moreclusters of rhetorical similarity.lWe chose to compute krr in order to estimatewhether the confusion in assigning rhetorical rela-tions lay within the clusters or across them.Problems with the proposed method.
In inter-preting the statistical Significance of the results re-ported in this paper, the readers hould bear in mindthat the method we propose and use here is not per-fect.
The biggest problem we perceive concerns theviolation of the independence assumption betweenthe categorial judgments hat are usually associatedwith the computation f the kappa statistics.
Obvi-ously, in our proposal, since the decisions taken at2Some relations were used so rarely in our  corpus that wedecided to cluster them into only one group  in spite o f  not be-longing to the same cluster o f  rhetorical  s imi lar i ty.one level in the tree affect decisions taken at otherlevels in the tree, the data points over which thekappa coefficient is computed are not independent.However, it is not clear what the effect on kappathis interdependence has: if two judges agree, forexample, on the labels associated with two largespans, they automatically agree on many other spansthat do not play an active role in the representa-tion.
When this happens, it is likely that the value ofkappa increases.
However, if two judges disagree ontwo high-level spans, they automatically disagree onother spans that play an active role in the representa-tion.
When this happens, it is likely that the value ofkappa decreases.
Therefore, it is not very clear howmuch the final value of kappa is skewed in one di-rection or another.
Most likely, if two judges agreesignificantly, the kappa coefficient will be skewed tohigher values; if two judges disagree significantly,the kappa coefficient will be skewed to smaller val-t ies .Another problem concerns the effect of NONE-agreements onthe computation ofkappa.
Althoughthe kappa statistics makes corrections for chanceagreement, it is likely that the kappa coefficientis "artificially" high, because of the large numbersof non-active spans that are labelled NONE.
Typi-cally, a hierarchical structure with n leaves will bemapped into n(n + 1)/2 categorial judgments, of53which only 2n - 1 have values different than NONE.Hence, it is possible the kappa coefficient to be "ar-tificially" high because of many agreements onnon-active spans.
However, the interdependence effectdiscussed above may equally well "artificially" de-crease the value of the kappa coefficient.
One mayimagine variants of our method in which all NONE-NONE agreements are eliminated, or in which only2n - 1 are preserved.
The first variant may be in-felicitous because its adoption may artificially pre-vent judges to agree on NONE labels.
Adoptingthe second variant is problematic because we don'tknow exactly how many NONE labels to keep in themapped representation.Another potential problem stems from assigningthe same importance to agreements atall levels inthe hierarchy.
For some classes of problems, onemay argue that achieving agreement a higher lev-els in the hierarchy should be more important thanachieving agreement a lower levels.
Obviously, themethod we described here does not enable such anintuition to be properly accounted for.
However,for the discourse annotation task, we are quite am-bivalent about his intuition.
It is not clear to uswhether we should consider the annotations thathave high agreements with respect o large textualsegments and low agreements with respect to smallsegments better than the annotations that have lowagreements with respect o large textual segmentsand high agreements with respect to small segments.The first group of annotations would correspond toan ability to deal properly with global discoursephenomena, but no ability to deal with local dis-course phenomena.
The second group of annota-tions would correspond to an ability to deal properlywith local discourse phenomena, but no ability todeal with global discourse phenomena.
Which oneis "better"?
The method we propose treats all spansequally.
It is similar in this respect to the labeled re-call and precision methods used to evaluate parsers,for example, which also do not consider that it ismore important to agree on high level constituentsthan low level constituents.The method we propose does not enable one toassess agreement a different levels of granularity; itproduces one number, which cannot be used to di-agnose where the disagreements are coming from.Although we believe that cascade techniques thatwere used to measure agreement between hierar-chies (Moser and Moore, 1997; Carletta et al, 1997)are more adequate for diagnosing problems in theannotation, we found these techniques difficult toapply on our data.
Some of our trees have morethan 200 elementary units; and carrying out and in-terpreting a cascade analysis at potentially 200 lev-els of granularity is not straightforward either.Another choice for computing agreement of hi-erarchical annotations would be to devise a methodsimilar to that used in the Kendall's 7- statistic, inwhich one computes the minimal number of op-erations that can map one annotation i to another.Since the problem of finding the minimal numberof operations that rewrite a tree into another treeis NP-complete, devising an operational method forcomputing agreement does not seem computation-ally feasible.
After all, the number of possible treesthat can be built for a text with 200 units is a numberlarger than 1 followed by 110 zeroes.3.3 Tagging consistencyFor each corpus, table 2 displays the numbers ofcoders that annotated each text in the corpus (#c)and the average numbers of data points (N~ andN~) over which the kappas were computed for eachtext in the corpus.
In the first three rows, the tablealso shows the average kappa statistics computedfor each text in the corpus with respect o judges'ability to agree on elementary discourse boundaries(k~ and k~,) and the average value of the corre-sponding z statistics (zw and zu) that were com-puted to test the significance of kappa (Siegel andCastellan, 1988, p. 289).
The last three rows showthe same statistics computed over all data points ineach corpus.The field of content analysis uggests that valuesof k higher than 0.6 indicate good agreement.
Val-ues of z that are higher than 2.32 correspond to sig-nificance levels that are higher than a = 0.01.
Theresults in table 2 indicate that high, statistically sig-nificant agreement was achieved for all three cor-pora with respect to the task of determining the ele-mentary discourse units.For each corpus, table 3 displays in its first threerows the number of coders (#c) that annotated thetexts and the average number of points (N) overwhich the agreements were computed for each textin the corpus.
In the first three rows, the table dis-plays the average kappa statistics with respect to thejudges' ability to agree on each text on discoursesegmentation, ks, nuclearity assignments, kn, andrhetorical relation assignments, kr and kr~.
In thelast three rows, the table displays the kappa statisticscomputed over all the data points in each corpus.
If54MUC-avg/text 3WSJ-avg/text 2Brown-avg/text 2MUC-all 3WSJ-all 2Brown-all 2Word level, Unit levelNw k~/z~ N~ ~lz~408 0.930/11.1909 0.906/9.52062 0.894/10.652 0.799/9.295 0.722/70.6170 0.685/92.912242 0.919/66.0 1528 0.769/57.127283 0.905/52.4 2836 0.717/419.361888 0.895/58.1 5100 0.688/841.8Table 2: Inter-annotator agreement - - edu boundaries.Corpus #cMUC-avg/text 3WSJ-avg/text 2Brown-avg/text 2MUC-all 3WSJ-all 2Brown-all 2N Spans Nuclei Relations Fewer relationsks/zs1326 0.792/11.3 0.744/10.6 0.646/8.9 0.689/9.53654 0.753/5.9 0.691/5.5 0.588/4.7 0.626/5.011634 0.733/5.4 0.658/4.9 0.539/4.0 0.586/4.339807 0.778/61.8 0.722/56.4 0.617/48.3 0.659/51.7109649 0.751/29.8 0.688/27.6 0.565/27.6 0.623/25.1349039 0.736/29.7 0.661/26.8 0.543/22.1 0.589/23.9Table 3: Inter-annotator agreem6nt - - discourse trees.the statistical method we proposed oes not skewthe values of k - -  a fact that we have not demon-strated - -  the data in table 3 suggest that reliableagreement is obtained across all three corpora withrespect to the assignment ofdiscourse segments andnuclear statuses.
Reliable agreement is obtainedwith respect to the rhetorical labeling only for theMUC corpus.
The results in table 3 also show thata significant reduction in the size of the taxonomyof relations may not have a significant impact onagreement (k~T is only about 4% higher than k~).This suggests that choosing one relation from a setof rhetorically similar relations produces ome, butnot too much, confusion.
However, it may also sug-gest hat it is more difficult o assess where to attachan edu in a discourse tree than what relation to use.The results in tables 2 and 3 also show that theagreement figures vary significantly from one cor-pus to another: the news story genre of the MUCtexts yields higher agreement figures than the edi-torial genre of the WSJ texts, which yields higheragreement figures than the scientific genre of theBrown texts.
One possible xplanation is that someof the Brown texts, which dealt with advanced top-ics in mathematics, physics, and chemistry, weredifficult o understand.Overall, if our method for computing the kappastatistic is not skewed towards higher values, ourexperiment suggests that even simple, intuitive def-initions of rhetorical relations, textual saliency, anddiscourse structure can lead to reliable annotationschemata.
However, the results do not exclude thatbetter definitions of edu and parenthetical units andrhetorical relations can lead to significant improve-ments in the reliability scores.4 Tagging styleThe vast majority of the computational pproachesto discourse parsing rely on models that implic-itly or explicitly assume that parsing is incremen-tal (Polanyi, 1988; Lascarides and Asher, 1993;Gardent, 1997; Schilder, 1997; van den Berg, 1996;Cristea and Webber, 1997).
That is, as edus areprocessed, they are immediately added to one par-tial discourse structure that subsumes all previoustext.
However, the logs of our experiment show that,quite often, annotators axe unable to decide whereto attach a newly created edu.
The annotation stylevaries significantly among annotators; but neverthe-less, even the most aggressive annotator still needsto postpone 9.2% of the time the decision of whereto attach anewly created edu (see table 4).
Note thatthis percentage does not reflect UNDO steps, whichmay also correlate with attachment decisions thatare eventually proven to be incorrect.
2We noticed that managing multiple partial dis-2UNDO operations may reflect ypo-like rrors'as well.55Annotator 1 Annotator 2 Annotator 3# % # % # %#incremental operations 2834 79.54 3938 58.41 6034 86.43# non-incremental ops.
670 18.80 2509 37.21 642 9.20# change-relation ps.
42 1.18 191 2.83 156 2.23# change-tree-structure ops.
17 0.48 104 1.54 149 2.13# operations 3563 6742 6981# undo cycles 247 515 466# undo operations/cycle 2.38 3.60 2.29Table 4: Distribution of tagging operations.course trees during the annotation process is thenorm rather than the exception.
In fact it is notthat edus are attached incrementally to one partialdiscourse structure, although the annotators wereasked to do so, but rather that multiple partial dis-course structures are created and then assembledusing a rich variety of operations, which are spe-cific to tree-adjoining and bottom-up arsers.
More-over, even this strategy proves to be somewhat inad-equate, since annotators need from time to time tochange rhetorical relation labels (2-3% of the op-erations) and re-structure completely the discourse(1-2% of the operations).This data suggests that it is unlikely that we willbe able to build perfect discourse parsers that can in-crementally derive discourse trees without applyingany form of backtracking.
If humans are unable todecide incrementally, in 100% of the cases, where toattach the edus, it is unlikely we can build computerprograms that are.Note.
* Estibaliz Amorrortu and MagdalenaRomera cotributed equally to this paper.Note.
** The tool described in this paper can beobtained by emailing the first author or by down-loading it from http://www.isi.edu/,,~marcu/.Acknowledgements.
We are grateful to MickO'Donnell for making publically available his dis-course annotation tool and to Benjamin Libermanand Ulrich Germann for contributing to the devel-opment of the annotation tool described in this pa-per.
We also thank Eduard Hovy, Kevin Knight, andthree anonymous reviewers for extensive commentson a previous version of this paper.ReferencesNicholas Asher.
1993.
Reference to Abstract Ob-jects in Discourse.
Kluwer Academic Publishers,Dordrecht.D.
Lee Ballard, Robert Conrad, and Robert E. Lon-gacre.
1971.
The deep and surface grammar ofinterclausal relations.
Foundations of language,4:70--118.Jean Carletta.
1996.
Assessing agreement on clas-sification tasks: The kappa statistic.
Computa-tional Linguistics, 22(2):249-254, June.Jeala Carletta, Amy Isard, Stephen Isard, Jacque-line C. Kowtko, Gwyneth Doherty-Sneddon, andAnne H. Anderson.
1997.
The reliability of a di-alogue structure coding scheme.
ComputationalLinguistics, 23(1): 13-32, March.Dan Cristea and Bonnie L. Webber.
1997.
Ex-pectations in incremental discourse processing.In Proceedings of the 35th Annual Meeting ofthe Association for Computational Linguistics(ACL/EACL-97), pages 88-95, Madrid, Spain,July 7-12.Giovanni Flammia and Victor Zue.
1995.
Empiri-cal evaluation of human performance and agree-ment in parsing discourse constituents in spokendialogue.
In Proceedings of the 4th EuropeanConference on Speech Communication a d Tech-nology, volume 3, pages 1965-1968, Madrid,Spain, September.Claire Gardent.
1997.
Discourse TAG.
TechnicalReport CLAUS-Report Nr.
89, Universit~it desSaarlandes, Saarbriicken, April.J.E.
Grimes.
1975.
The Thread of Discourse.
Mou-ton, The Hague, Pads.Barbara Grosz and Julia Hirschberg.
1992.
Someintonational characteristics of discourse structure.In Proceedings of the International Conferenceon Spoken Language Processing.Michael A.K.
Halliday and Ruqaiya Hasan.
1976.Cohesion in English.
Longman.Julia B. Hirschberg and Diane Litman.
1987.
Nowlet's talk about now: Identifying cue phrases in-tonationally.
In Proceedings of the 25th Annual56Meeting of the Association for ComputationalLinguistics (ACL-87), pages 163-171.Eduard H. Hovy and Elisabeth Maier.
1993.
Par-simonious or profligate: How many and whichdiscourse structure relations?
UnpublishedManuscript.Alistair Knott.
1995.
A Data-Driven Methodol-ogy for Motivating aSet of Coherence Relations.Ph.D.
thesis, University of Edinburgh.Alex Lascarides and Nicholas Asher.
1993.
Tem-poral interpretation, discourse relations, andcommon sense ntailment.
Linguistics and Phi-losophy, 16(5):437--493.William C. Mann and Sandra A. Thompson.
1988.Rhetorical structure theory: Toward a functionaltheory of text organization.
Text, 8(3):243-281.Daniel Marcu, 1998.
Instructions for Manually An-notating the Discourse Structures of Texts.Daniel Marcu and Eduard Hovy.
1999.
Computingthe kappa statistic for hierarchical structures.
Inpreparation.James R. Martin.
1992.
English Text.
System andStructure.
John Benjamin Publishing Company,Philadelphia/Amsterdam.Megan Moser and Johanna D. Moore.
1997.
Onthe correlation of cues with discourse structure:Results from a corpus tudy.
Forthcoming.Christine H. Nakatani, Julia Hirschberg, and Bar-bara J. Grosz.
1995.
Discourse structure in spo-ken language: Studies on speech corpora.
InWorking Notes of the AAAI Spring Symposiumon Empirical Methods in Discourse Interpreta-tion and Generation, pages 106-112, Stanford,CA, March.Mick O'Donnell.
1997.
RST-TooI: An RST anal-ysis tool.
In Proceedings of the 6th Euro-pean Workshop on Natural Language Genera-tion, Duisburg, Germany, March 24-26.Rebbeca J. Passonneau nd Diane J. Litman.
1997.Discourse segmentation by human and automatedmeans.
Computational Linguistics, 23(1):103-140, March.Livia Polanyi.
1988.
A formal model of thestructure of discourse.
Journal of Pragmatics,12:601-638.Ted J.M.
Sanders, Wilbert P.M. Spooren, and LeoG.M.
Noordman.
1992.
Toward a taxonomy ofcoherence r lations.
Discourse Processes, 15:1-35.Ted J.M.
Sanders, Wilbert P.M. Spooren, and LeoG.M.
Noordman.
1993.
Coherence relations ina cognitive theory of discourse representation.Cognitive Linguistics, 4(2):93-133.Frank Schilder.
1997.
Tree discourse grammar, orhow to get attached a discourse.
In Proceedingsof the Second International Workshop on Com-putational Semantics (IWCS-II), pages 261-273,Tilburg, The Netherlands, January.Sidney Siegel and N.J. Castellan.
1988.
Non-parametric Statistics for the Behavioral Sciences.McGraw-Hill, second edition.Martin H. van den Berg.
1996.
Discourse grammarand dynamic logic.
In P. Dekker and M. Stokhof,editors, Proceedings of the Tenth Amsterdam Col-loquium, pages 93-112.
Department of Philoso-phy, University of Amsterdam.57
