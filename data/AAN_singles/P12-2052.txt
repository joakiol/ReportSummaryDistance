Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 264?269,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsAuthorship Attribution with Author-aware Topic ModelsYanir Seroussi Fabian BohnertFaculty of Information Technology, Monash UniversityClayton, Victoria 3800, Australiafirstname.lastname@monash.eduIngrid ZukermanAbstractAuthorship attribution deals with identifyingthe authors of anonymous texts.
Building onour earlier finding that the Latent Dirichlet Al-location (LDA) topic model can be used toimprove authorship attribution accuracy, weshow that employing a previously-suggestedAuthor-Topic (AT) model outperforms LDAwhen applied to scenarios with many authors.In addition, we define a model that combinesLDA and AT by representing authors and doc-uments over two disjoint topic sets, and showthat our model outperforms LDA, AT and sup-port vector machines on datasets with manyauthors.1 IntroductionAuthorship attribution (AA) has attracted much at-tention due to its many applications in, e.g., com-puter forensics, criminal law, military intelligence,and humanities research (Stamatatos, 2009).
Thetraditional problem, which is the focus of our work,is to attribute test texts of unknown authorship toone of a set of known authors, whose training textsare supplied in advance (i.e., a supervised classifi-cation problem).
While most of the early work onAA focused on formal texts with only a few pos-sible authors, researchers have recently turned theirattention to informal texts and tens to thousands ofauthors (Koppel et al, 2011).
In parallel, topic mod-els have gained popularity as a means of analysingsuch large text corpora (Blei, 2012).
In (Seroussi etal., 2011), we showed that methods based on LatentDirichlet Allocation (LDA) ?
a popular topic modelby Blei et al (2003) ?
yield good AA performance.However, LDA does not model authors explicitly,and we are not aware of any previous studies thatapply author-aware topic models to traditional AA.This paper aims to address this gap.In addition to being the first (to the best ofour knowledge) to apply Rosen-Zvi et al?s (2004)Author-Topic Model (AT) to traditional AA, themain contribution of this paper is our DisjointAuthor-Document Topic Model (DADT), which ad-dresses AT?s limitations in the context of AA.
Weshow that DADT outperforms AT, LDA, and linearsupport vector machines on AA with many authors.2 Disjoint Author-Document Topic ModelBackground.
Our definition of DADT is motivatedby the observation that when authors write texts onthe same issue, specific words must be used (e.g.,texts about LDA are likely to contain the words?topic?
and ?prior?
), while other words vary in fre-quency according to author style.
Also, texts by thesame author share similar style markers, indepen-dently of content (Koppel et al, 2009).
DADT aimsto separate document words from author words bygenerating them from two disjoint topic sets of T (D)document topics and T (A) author topics.Lacoste-Julien et al (2008) and Ramage et al(2009) (among others) also used disjoint topic setsto represent document labels, and Chemuduguntaet al (2006) separated corpus-level topics fromdocument-specific words.
However, we are unawareof any applications of these ideas to AA.
The clos-est work we know of is by Mimno and McCallum(2008), whose DMR model outperformed AT in AA264(D) (A)wdi?
T DNdzdi?t?
D?dydiADad ???d?
(D)(D)(D)(D) (D)??(A)?
(A)T?t (A)(A)?a(A)Figure 1: The Disjoint Author-Document Topic Modelof multi-authored texts (DMR does not use disjointtopic sets).
We use AT rather than DMR, since wefound that AT outperforms DMR in AA of single-authored texts, which are the focus of this paper.The Model.
Figure 1 shows DADT?s graphical rep-resentation, with document-related parameters onthe left (the LDA component), and author-relatedparameters on the right (the AT component).
We de-fine the model for single-authored texts, but it can beeasily extended to multi-authored texts.The generative process for DADT is described be-low.
We use D and C to denote the Dirichlet andcategorical distributions respectively, and A, D andV to denote the number of authors, documents, andunique vocabulary words respectively.
In addition,we mark each step as coming from either LDA orAT, or as new in DADT.Global level:L. For each document topic t, draw a word dis-tribution ?
(D)t ?
D(?
(D)), where ?
(D) is alength-V vector.A.
For each author topic t, draw a word distribu-tion ?
(A)t ?
D(?
(A)), where ?
(A) is a length-V vector.A.
For each author a, draw the author topic dis-tribution ?
(A)a ?
D(?
(A)), where ?
(A) is alength-T (A) vector.D.
Draw a distribution over authors ?
?
D (?
),where ?
is a length-A vector.Document level: For each document d:L. Draw d?s topic distribution ?
(D)d ?
D(?
(D)),where ?
(D) is a length-T (D) vector.D.
Draw d?s author ad ?
C (?).D.
Draw d?s topic ratio pid ?
Beta(?
(A), ?
(D)),where ?
(A) and ?
(D) are scalars.Word level: For each word index i in document d:D. Draw di?s topic indicator ydi ?
Bernoulli(pid).L.
If ydi = 0, draw a document topic zdi ?C(?
(D)d)and word wdi ?
C(?(D)zdi).A.
If ydi = 1, draw an author topic zdi ?
C(?
(A)ad)and word wdi ?
C(?
(A)zdi).DADT versus AT.
DADT might seem similar toAT with ?fictitious?
authors, as described by Rosen-Zvi et al (2010) (i.e., AT trained with an additionalunique ?fictitious?
author for each document, allow-ing it to adapt to individual documents and not onlyto authors).
However, there are several key differ-ences between DADT and AT.First, in DADT author topics are disjoint fromdocument topics, with different priors for each topicset.
Thus, the number of author topics can be differ-ent from the number of document topics, enablingus to vary the number of author topics according tothe number of authors in the corpus.Second, DADT places different priors on theword distributions for author topics and documenttopics (?
(A) and ?
(D) respectively).
Stopwords areknown to be strong indicators of authorship (Kop-pel et al, 2009), and DADT allows us to use thisknowledge by assigning higher weights to the ele-ments of ?
(A) that correspond to stopwords than tosuch elements in ?
(D).Third, DADT learns the ratio between documentwords and author words on a per-document basis,and makes it possible to specify a prior belief ofwhat this ratio should be.
We found that specify-ing a prior belief that about 80% of each documentis composed of author words yielded better resultsthan using AT?s approach, which evenly splits eachdocument into author and document words.Fourth, DADT defines the process that generatesauthors.
This allows us to consider the numberof texts by each author when performing AA.
Thisalso enables the potential use of DADT in a semi-supervised setting by training on unlabelled texts,which we plan to explore in the future.3 Authorship Attribution MethodsWe experimented with the following AA methods,using token frequency features, which are good pre-dictors of authorship (Koppel et al, 2009).265Baseline: Support Vector Machines (SVMs).Koppel et al (2009) showed that SVMs yieldgood AA performance.
We use linear SVMs in aone-versus-all setup, as implemented in LIBLIN-EAR (Fan et al, 2008), reporting results obtainedwith the best cost parameter values.Baseline: LDA + Hellinger (LDA-H).
This ap-proach uses the Hellinger distances of topic dis-tributions to assign test texts to the closest author.In (Seroussi et al, 2011), we experimented with twovariants: (1) each author?s texts are concatenated be-fore building the LDA model; and (2) no concate-nation is performed.
We found that the latter ap-proach performs poorly in cases with many candi-date authors.
Hence, we use only the former ap-proach in this paper.
Note that when dealing withsingle-authored texts, concatenating each author?stexts yields an LDA model that is equivalent to AT.AT.
Given an inferred AT model (Rosen-Zvi et al,2004), we calculate the probability of the test textwords for each author a, assuming it was writtenby a, and return the most probable author.
We do notknow of any other studies that used AT in this man-ner for single-authored AA.
We expect this methodto outperform LDA-H as it employs AT directly,rather than relying on an external distance measure.AT-FA.
Same as AT, but built with an additionalunique ?fictitious?
author for each document.DADT.
Given our DADT model, we assume that thetest text was written by a ?new?
author, and inferthis author?s topic distribution, the author/documenttopic ratio, and the document topic distribution.
Wethen calculate the probability of each author giventhe model?s parameters, the test text words, and theinferred author/document topic ratio and documenttopic distribution.
The most probable author is re-turned.
We use this method to avoid inferring thedocument-dependent parameters separately for eachauthor, which is infeasible when many authors ex-ist.
A version that marginalises over these parame-ters will be explored in future work.4 EvaluationWe compare the performance of the methods ontwo publicly-available datasets: (1) PAN?11: emailswith 72 authors (Argamon and Juola, 2011);and (2) Blog: blogs with 19,320 authors (Schler etal., 2006).
These datasets represent realistic scenar-ios of AA of user-generated texts with many can-didate authors.
For example, Chaski (2005) notesa case where an employee who was terminated forsending a racist email claimed that any person withaccess to his computer could have sent the email.Experimental Setup.
Experiments on the PAN?11dataset followed the setup of the PAN?11 competi-tion (Argamon and Juola, 2011): We trained all themethods on the given training subset, tuned the pa-rameters according to the results on the given valida-tion subset, and ran the tuned methods on the giventesting subset.
In the Blog experiments, we used ten-fold cross validation as in (Seroussi et al, 2011).We used collapsed Gibbs sampling to train all thetopic models (Griffiths and Steyvers, 2004), run-ning 4 chains with a burn-in of 1,000 iterations.
Inthe PAN?11 experiments, we retained 8 samples perchain with spacing of 100 iterations.
In the Blogexperiments, we retained 1 sample per chain due toruntime constraints.
Since we cannot average topicdistribution estimates obtained from training sam-ples due to topic exchangeability (Steyvers and Grif-fiths, 2007), we averaged the distances and probabil-ities calculated from the retained samples.
For testtext sampling, we used a burn-in of 100 iterationsand averaged the parameter estimates over the next100 iterations in a similar manner to Rosen-Zvi etal.
(2010).
We found that these settings yield stableresults across different random seed values.We found that the number of topics has a largerimpact on accuracy than other configurable pa-rameters.
Hence, we used symmetric topic pri-ors, setting all the elements of ?
(D) and ?
(A)to min{0.1, 5/T (D)} and min{0.1, 5/T (A)} respec-tively.1 For all models, we set ?w = 0.01 for eachword w as the base measure for the prior of words intopics.
Since DADT allows us to encode our priorknowledge that stopword use is indicative of author-ship, we set ?
(D)w = 0.01 ?
 and ?
(A)w = 0.01 + for all w, where w is a stopword.2 We set  = 0.009,which improved accuracy by up to one percentagepoint over using  = 0.
Finally, we set ?
(A) = 4.889and ?
(D) = 1.222 for DADT.
This encodes our prior1We tested Wallach et al?s (2009) method of obtainingasymmetric priors, but found that it did not improve accuracy.2We used the stopword list from www.lextek.com/manuals/onix/stopwords2.html.266PAN?11 PAN?11 Blog BlogMethod Validation Testing Prolific FullSVM 48.61% 53.31% 33.31% 24.13%LDA-H 34.95% 42.62% 21.61% 7.94%AT 46.68% 53.08% 37.56% 23.03%AT-FA 20.68% 24.23% ?
?DADT 54.24% 59.08% 42.51% 27.63%Table 1: Experiment resultsbelief that 0.8 ?
0.15 of each document is com-posed of author words.
We found that this yieldsbetter results than an uninformed uniform prior of?
(A) = ?
(D) = 1 (Seroussi et al, 2012).
In addition,we set ?a = 1 for each author a, yielding smoothedestimates for the corpus distribution of authors ?.To fairly compare the topic-based methods, weused the same overall number of topics for all thetopic models.
We present only the results obtainedwith the best topic settings: 100 for PAN?11 and 400for Blog, with DADT?s author/document topic splitsbeing 90/10 for PAN?11, and 390/10 for Blog.
Thesesplits allow DADT to de-noise the author represen-tations by allocating document words to a relativelysmall number of document topics.
It is worth not-ing that AT can be seen as an extreme version ofDADT, where all the topics are author topics.
A fu-ture extension is to learn the topic balance automat-ically, e.g., in a similar manner to Teh et al?s (2006)method of inferring the number of topics in LDA.Results.
Table 1 shows the results of our experi-ments in terms of classification accuracy (i.e., thepercentage of test texts correctly attributed to theirauthor).
The PAN?11 results are shown for the val-idation and testing subsets, and the Blog results areshown for a subset containing the 1,000 most prolificauthors and for the full dataset of 19,320 authors.Our DADT model yielded the best results in allcases (the differences between DADT and the othermethods are statistically significant according to apaired two-tailed t-test with p < 0.05).
We attributeDADT?s superior performance to the de-noising ef-fect of the disjoint topic sets, which appear to yieldauthor representations of higher predictive qualitythan those of the other models.As expected, AT significantly outperformedLDA-H. On the other hand, AT-FA performed muchworse than all the other methods on PAN?11, prob-ably because of the inherent noisiness in using thesame topics to model both authors and documents.Hence, we did not run AT-FA on the Blog dataset.DADT?s PAN?11 testing result is close to thethird-best accuracy from the PAN?11 competi-tion (Argamon and Juola, 2011).
However, to thebest of our knowledge, DADT obtained the bestaccuracy for a fully-supervised method that usesonly unigram features.
Specifically, Kourtis andStamatatos (2011), who obtained the highest accu-racy (65.8%), assumed that all the test texts aregiven to the classifier at the same time, and usedthis additional information with a semi-supervisedmethod; while Kern et al (2011) and Tanguy et al(2011), who obtained the second-best (64.2%) andthird-best (59.4%) accuracies respectively, used var-ious feature types (e.g., features obtained from parsetrees).
Further, preprocessing differences make ithard to compare the methods on a level playingfield.
Nonetheless, we note that extending DADT toenable semi-supervised classification and additionalfeature types are promising future work directions.While all the methods yielded relatively low accu-racies on Blog due to its size, topic-based methodswere more strongly affected than SVM by the transi-tion from the 1,000 author subset to the full dataset.This is probably because topic-based methods use asingle model, making them more sensitive to corpussize than SVM?s one-versus-all setup that uses onemodel per author.
Notably, an oracle that choosesthe correct answer between SVM and DADT whenthey disagree yields an accuracy of 37.15% on thefull dataset, suggesting it is worthwhile to exploreensembles that combine the outputs of SVM andDADT (we tried using DADT topics as additionalSVM features, but this did not outperform DADT).5 ConclusionThis paper demonstrated the utility of using author-aware topic models for AA: AT outperformed LDA,and our DADT model outperformed LDA, AT andSVMs in cases with noisy texts and many authors.We hope that these results will inspire further re-search into the application of topic models to AA.AcknowledgementsThis research was supported in part by AustralianResearch Council grant LP0883416.
We thank MarkCarman for fruitful discussions on topic modelling.267ReferencesShlomo Argamon and Patrick Juola.
2011.
Overview ofthe international authorship identification competitionat PAN-2011.
In CLEF 2011: Proceedings of the 2011Conference on Multilingual and Multimodal Informa-tion Access Evaluation (Lab and Workshop NotebookPapers), Amsterdam, The Netherlands.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet alocation.
Journal of MachineLearning Research, 3(Jan):993?1022.David M. Blei.
2012.
Probabilistic topic models.
Com-munications of the ACM, 55(4):77?84.Carole E. Chaski.
2005. Who?s at the keyboard?
Au-thorship attribution in digital evidence investigations.International Journal of Digital Evidence, 4(1).Chaitanya Chemudugunta, Padhraic Smyth, and MarkSteyvers.
2006.
Modeling general and specific as-pects of documents with a probabilistic topic model.In NIPS 2006: Proceedings of the 20th Annual Confer-ence on Neural Information Processing Systems, pages241?248, Vancouver, BC, Canada.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-RuiWang, and Chih-Jen Lin.
2008.
LIBLINEAR: A li-brary for large linear classification.
Journal of Ma-chine Learning Research, 9(Aug):1871?1874.Thomas L. Griffiths and Mark Steyvers.
2004.
Find-ing scientific topics.
Proceedings of the NationalAcademy of Sciences, 101(Suppl.
1):5228?5235.Roman Kern, Christin Seifert, Mario Zechner, andMichael Granitzer.
2011.
Vote/veto meta-classifierfor authorship identification.
In CLEF 2011: Pro-ceedings of the 2011 Conference on Multilingual andMultimodal Information Access Evaluation (Lab andWorkshop Notebook Papers), Amsterdam, The Nether-lands.Moshe Koppel, Jonathan Schler, and Shlomo Argamon.2009.
Computational methods in authorship attribu-tion.
Journal of the American Society for InformationScience and Technology, 60(1):9?26.Moshe Koppel, Jonathan Schler, and Shlomo Argamon.2011.
Authorship attribution in the wild.
LanguageResources and Evaluation, 45(1):83?94.Ioannis Kourtis and Efstathios Stamatatos.
2011.
Au-thor identification using semi-supervised learning.
InCLEF 2011: Proceedings of the 2011 Conferenceon Multilingual and Multimodal Information AccessEvaluation (Lab and Workshop Notebook Papers),Amsterdam, The Netherlands.Simon Lacoste-Julien, Fei Sha, and Michael I. Jordan.2008.
DiscLDA: Discriminative learning for dimen-sionality reduction and classification.
In NIPS 2008:Proceedings of the 22nd Annual Conference on Neu-ral Information Processing Systems, pages 897?904,Vancouver, BC, Canada.David Mimno and Andrew McCallum.
2008.Topic models conditioned on arbitrary features withDirichlet-multinomial regression.
In UAI 2008: Pro-ceedings of the 24th Conference on Uncertainty in Ar-tificial Intelligence, pages 411?418, Helsinki, Finland.Daniel Ramage, David Hall, Ramesh Nallapati, andChristopher D. Manning.
2009.
Labeled LDA: Asupervised topic model for credit attribution in multi-labeled corpora.
In EMNLP 2009: Proceedings ofthe 2009 Conference on Empirical Methods in Natu-ral Language Processing, pages 248?256, Singapore.Michal Rosen-Zvi, Thomas Griffiths, Mark Steyvers, andPadhraic Smyth.
2004.
The author-topic model forauthors and documents.
In UAI 2004: Proceedings ofthe 20th Conference on Uncertainty in Artificial Intel-ligence, pages 487?494, Banff, AB, Canada.Michal Rosen-Zvi, Chaitanya Chemudugunta, ThomasGriffiths, Padhraic Smyth, and Mark Steyvers.
2010.Learning author-topic models from text corpora.
ACMTransactions on Information Systems, 28(1):1?38.Jonathan Schler, Moshe Koppel, Shlomo Argamon, andJames W. Pennebaker.
2006.
Effects of age and gen-der on blogging.
In Proceedings of AAAI Spring Sym-posium on Computational Approaches for AnalyzingWeblogs, pages 199?205, Stanford, CA, USA.Yanir Seroussi, Ingrid Zukerman, and Fabian Bohnert.2011.
Authorship attribution with latent Dirichlet alo-cation.
In CoNLL 2011: Proceedings of the 15th Inter-national Conference on Computational Natural Lan-guage Learning, pages 181?189, Portland, OR, USA.Yanir Seroussi, Fabian Bohnert, and Ingrid Zukerman.2012.
Authorship attribution with author-aware topicmodels.
Technical Report 2012/268, Faculty of Infor-mation Technology, Monash University, Clayton, VIC,Australia.Efstathios Stamatatos.
2009.
A survey of modern au-thorship attribution methods.
Journal of the Ameri-can Society for Information Science and Technology,60(3):538?556.Mark Steyvers and Tom Griffiths.
2007.
Probabilistictopic models.
In Thomas K. Landauer, Danielle S.McNamara, Simon Dennis, and Walter Kintsch, ed-itors, Handbook of Latent Semantic Analysis, pages427?448.
Lawrence Erlbaum Associates.Ludovic Tanguy, Assaf Urieli, Basilio Calderone, NabilHathout, and Franck Sajous.
2011.
A multitudeof linguistically-rich features for authorship attribu-tion.
In CLEF 2011: Proceedings of the 2011 Con-ference on Multilingual and Multimodal InformationAccess Evaluation (Lab and Workshop Notebook Pa-pers), Amsterdam, The Netherlands.Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, andDavid M. Blei.
2006.
Hierarchical Dirichlet pro-268cesses.
Journal of the American Statistical Associa-tion, 101(476):1566?1581.Hanna M. Wallach, David Mimno, and Andrew McCal-lum.
2009.
Rethinking LDA: Why priors matter.
InNIPS 2009: Proceedings of the 23rd Annual Confer-ence on Neural Information Processing Systems, pages1973?1981, Vancouver, BC, Canada.269
