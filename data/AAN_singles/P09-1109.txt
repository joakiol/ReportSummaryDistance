Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 967?975,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPCoordinate Structure Analysis with Global Structural Constraints andAlignment-Based Local FeaturesKazuo Hara Masashi Shimbo Hideharu Okuma Yuji MatsumotoGraduate School of Information ScienceNara Institute of Science and TechnologyIkoma, Nara 630-0192, Japan{kazuo-h,shimbo,hideharu-o,matsu}@is.naist.jpAbstractWe propose a hybrid approach to coor-dinate structure analysis that combinesa simple grammar to ensure consistentglobal structure of coordinations in a sen-tence, and features based on sequencealignment to capture local symmetry ofconjuncts.
The weight of the alignment-based features, which in turn determinesthe score of coordinate structures, is op-timized by perceptron training on a givencorpus.
A bottom-up chart parsing al-gorithm efficiently finds the best scor-ing structure, taking both nested or non-overlapping flat coordinations into ac-count.
We demonstrate that our approachoutperforms existing parsers in coordina-tion scope detection on the Genia corpus.1 IntroductionCoordinate structures are common in life sci-ence literature.
In Genia Treebank Beta (Kim etal., 2003), the number of coordinate structures isnearly equal to that of sentences.
In clinical pa-pers, the outcome of clinical trials is typically de-scribed with coordination, as inMedian times to progression and mediansurvival times were 6.1 months and 8.9months in arm A and 7.2 months and 9.5months in arm B.
(Schuette et al, 2006)Despite the frequency and implied importanceof coordinate structures, coordination disambigua-tion remains a difficult problem even for state-of-the-art parsers.
Figure 1(a) shows the coordinatestructure extracted from the output of Charniakand Johnson?s (2005) parser on the above exam-ple.
This is somewhat surprising, given that thesymmetry of conjuncts in the sentence is obviousto human eyes, and its correct coordinate structureshown in Figure 1(b) can be readily observed.6.1months and8.9monthsinarm A7.2monthsand 9.5monthsinarm Band6.1monthsand 8.9monthsinarm A7.2monthsand 9.5monthsinarm Band(b)(a)Figure 1: (a) Output from the Charniak-Johnsonparser and (b) the correct coordinate structure.Structural and semantic symmetry of conjunctsis one of the frequently observed features of coor-dination.
This feature has been explored by previ-ous studies on coordination, but these studies oftendealt with a restricted form of coordination withapparently too much information provided fromoutside.
Sometimes it was assumed that the co-ordinate structure contained two conjuncts eachsolely composed of a few nouns; and in manycases, the longest span of coordination (e.g., outernoun phrase scopes) was given a priori.
Such richinformation might be given by parsers, but this isstill an unfounded assumption.In this paper, we approach coordination by tak-ing an extreme stance, and assume that the input isa whole sentence with no subsidiary informationexcept for the parts-of-speech of words.As it assumes minimal information about syn-tactic constructs, our method provides a baselinefor future work exploiting deeper syntactic infor-mation for coordinate structure analysis.
More-over, this stand-alone approach has its own meritsas well:1.
Even apart from parsing, the output coordi-nate structure alone may provide valuable in-formation for higher-level applications, in thesame vein as the recent success of namedentity recognition and other shallow parsing967technologies.
One such potential applicationis extracting the outcome of clinical tests asillustrated above.2.
As the system is designed independentlyfrom parsers, it can be combined with anytypes of parsers (e.g., phrase structure or de-pendency parsers), if necessary.3.
Because coordination bracketing is some-times inconsistent with phrase structurebracketing, processing coordinations apartfrom phrase structures might be beneficial.Consider, for example,John likes, and Bill adores, Sue.
(Carston and Blakemore, 2005)This kind of structure might be treated by as-suming the presence of null elements, but thecurrent parsers have limited ability to detectthem.
On the other hand, the symmetry ofconjuncts, John likes and Bill adores, is ratherobvious and should be easy to detect.The method proposed in this paper builds atree-like coordinate structure from the input sen-tence annotated with parts-of-speech.
Each treeis associated with a score, which is defined interms of features based on sequence alignment be-tween conjuncts occurring in the tree.
The featureweights are optimized with a perceptron algorithmon a training corpus annotated with the scopes ofconjuncts.The reason we build a tree of coordinations is tocope with nested coordinations, which are in factquite common.
In Genia Treebank Beta, for ex-ample, about 1/3 of the whole coordinations arenested.
The method proposed in this paper im-proves upon our previous work (Shimbo and Hara,2007) which also takes a sentence as input but isrestricted to flat coordinations.
Our new method,on the other hand, can successfully output the cor-rect nested structure of Figure 1(b).2 Related workResnik (1999) disambiguated coordinations of theform [n1 and n2 n3], where ni are all nouns.
Thistype of phrase has two possible readings: [(n1)and (n2 n3)] and [((n1) and (n2)) n3].
He demon-strated the effectiveness of semantic similarity cal-culated from a large text collection, and agreementof numbers between n1 and n2 and between n1 andn3.
Nakov and Hearst (2005) collected web-basedstatistics with search engines and applied them toa task similar to Resnik?s.Hogan (2007) improved the parsing accuracyof sentences in which coordinated noun phrasesare known to exist.
She presented a generativemodel incorporating symmetry in conjunct struc-tures and dependencies between coordinated headwords.
The model was then used to rerank the n-best outputs of the Bikel parser (2005).Recently, Buyko et al (2007; 2008) andShimbo and Hara (2007) applied discriminativelearning methods to coordinate structure analysis.Buyko et al used a linear-chain CRF, whereasShimbo and Hara proposed an approach based onperceptron learning of edit distance between con-juncts.Shimbo and Hara?s approach has its root inKurohashi and Nagao?s (1994) rule-based methodfor Japanese coordinations.
Other studies on co-ordination include (Agarwal and Boggess, 1992;Chantree et al, 2005; Goldberg, 1999; Okumuraand Muraki, 1994).3 Proposed methodWe propose a method for learning and detectingthe scopes of coordinations.
It makes no assump-tion about the number of coordinations in a sen-tence, and the sentence can contain either nestedcoordinations, multiple flat coordinations, or both.The method consists of (i) a simple gram-mar tailored for coordinate structure, and (ii) aperceptron-based algorithm for learning featureweights.
The features are defined in terms of se-quence alignment between conjuncts.We thus use the grammar to filter out incon-sistent nested coordinations and non-valid (over-lapping) conjunct scopes, and the alignment-basedfeatures to evaluate the similarity of conjuncts.3.1 Grammar for coordinationsThe sole objective of the grammar we present be-low is to ensure the consistency of two or morecoordinations in a sentence; i.e., for any two co-ordinations, either (i) they must be totally non-overlapping (non-nested coordinations), or (ii) onecoordination must be embedded within the scopeof a conjunct of the other coordination (nested co-ordinations).Below, we call a parse tree built from the gram-mar a coordination tree.968Table 1: Non-terminalsCOORD Complete coordination.COORD?
Partially-built coordination.CJT Conjunct.N Non-coordination.CC Coordinate conjunction like ?and,??or,?
and ?but?.SEP Connector of conjuncts other than CC:e.g., punctuations like ?,?
and ?
;?.W Any word.Table 2: Production rules for coordination trees.(.
.
.
| .
.
.
| .
.
.)
denotes a disjunction (matches anyone of the elements).
A ?*?
matches any word.Rules for coordinations:(i) COORDi,m ?
CJTi, j CC j+1,k?1 CJTk,m(ii) COORDi,n ?
CJTi, j SEP j+1,k?1 COORD?k,n[m](iii) COORD?i,m[ j]?
CJTi, j CC j+1,k?1 CJTk,m(iv) COORD?i,n[ j]?
CJTi, j SEP j+1,k?1 COORD?k,n[m]Rules for conjuncts:(v) CJTi, j ?
(COORD | N)i, jRules for non-coordinations:(vi) Ni,k ?
COORDi, j N j+1,k(vii) Ni, j ?
Wi,i (COORD|N)i+1, j(viii) Ni,i ?
Wi,iRules for pre-terminals:(ix) CCi,i ?
(and | or | but )i(x) CCi,i+1 ?
( , | ; )i (and | or | but )i+1(xi) SEPi,i ?
( , | ; )i(xii) Wi,i ?
?i3.1.1 Non-terminalsThe grammar is composed of non-terminal sym-bols listed in Table 1.
The distinction betweenCOORD and COORD?
is made to cope with three ormore conjuncts in a coordination.
For example?a , b and c?
is treated as a tree of the form (a ,(b and c))), and the inner tree (b and c) is not acomplete coordination, until it is conjoined withthe first conjunct a.
We represent this inner treeby a COORD?
(partial coordination), to distinguish itfrom a complete coordination represented by CO-ORD.
Compare Figures 2(a) and (b), which respec-tively depict the coordination tree for this exam-ple, and a tree for nested coordination with a sim-ilar structure.3.1.2 Production rulesTable 2 lists the production rules.
Rules are shownwith explicit subscripts indicating the span of theirproduction.
The subscript to a terminal word(shown in a box) specifies its position within a sen-tence (word index).
Non-terminals have two sub-script indices denoting the span of the production.COORD?
in rules (iii) and (iv) has an extra in-dex j shown in brackets.
This bracketed indexmaintains the end of the first conjunct (CJT) onthe right-hand side.
After a COORD?
is producedby these rules, it may later constitute a larger CO-ORD or COORD?
through the application of produc-tions (ii) or (iv).
At this point, the bracketed in-dex of the constituent COORD?
allows us to identifythe scope of the first conjunct immediately under-neath.
As we describe in Section 3.2.4, the scopeof this conjunct is necessary to compute the scoreof coordination trees.These grammar rules are admittedly minimaland need further elaboration to cover all real usecases of coordination (e.g., conjunctive phraseslike ?as well as?, etc.).
Yet they are sufficient togenerate the basic trees illustrated in Figure 2.
Theexperiments of Section 5 will apply this grammaron a real biomedical corpus.Note that although non-conjunction cue expres-sions, such as ?both?
and ?either,?
are not thepart of this grammar, such cues can be learned(through perceptron training) from training exam-ples if appropriate features are introduced.
Indeed,in Section 5 we use features indicating whichwords precede coordinations.3.2 Score of a coordination treeGiven a sentence, our system outputs the coordina-tion tree with the highest score among all possibletrees for the sentence.
The score of a coordinationtree is simply the sum of the scores of all its nodes,and the node scores are computed independentlyfrom each other.
Hence a bottom-up chart parsingalgorithm can be designed to efficiently computethe highest scoring tree.While scores can be assigned to any nodes, wehave chosen to assign a non-zero score only to twotypes of coordination nodes, namely COORD andCOORD?, in the experiment of Section 5; all othernodes are ignored in score computation.
The scoreof a coordination node is defined via sequencealignment (Gusfield, 1997) between conjuncts be-low the node, to capture the symmetry of these969(a) a , b and cW W WCOORDCOORD?N SEP N CC N(b) a or b and cWCCWCCWN N NCOORDCOORD(c) aWbWcWNNNFigure 2: Coordination trees for (a) a coordination with three conjuncts, (b) nested coordinations, and(c) a non-coordination.
The CJT nodes in (a) and (b) are omitted for brevity.W W CC W W W W W CC W W CC W W W W WNNNNNNNNNNNNCOORDNNCOORDNNCOORD6.1months8.9months9.5months7.2months6.1monthsand8.9monthsinarmA7.2monthsand9.5monthsin armBMediantimestoprogressionandmediansurvivaltimeswere6.1monthsand8.9months inarm Aand7.2monthsand9.5months inarm BW W W W CC W W WNNNNNNNCOORDWNNMediantimestoprogressionmediansurvivaltimesFigure 3: A coordination tree for the example sen-tence presented in Section 1, with the edit graphsattached to COORD nodes.mediansurvivaltimesMediantimestoprogressioninitial vertexterminal vertexFigure 4: An edit graph and an alignment path(bold line).conjuncts.Figure 3 schematically illustrates the relationbetween a coordination tree and alignment-basedcomputation of the coordination nodes.
The scoreof this tree is given by the sum of the scores of thefour COORD nodes, and the score of a COORD nodeis computed with the edit graph shown above thenode.3.2.1 Edit graphThe edit graph is a basic data structure for comput-ing sequence alignment.
An example edit graph isdepicted in Figure 4 for word sequences ?Mediantimes to progression?
and ?median survival times.
?A diagonal edge represents alignment (or sub-stitution) between the word at the top of the edgeand the one on the left, while horizontal and ver-tical edges represent skipping (or deletion) of re-spective word.
With this representation, a pathstarting from the top-left corner (initial vertex) andarriving at the bottom-right corner (terminal ver-tex) corresponds one-to-one to a sequence of editoperations transforming one word sequence to theother.In standard sequence alignment, each edge of anedit graph is associated with a score representingthe merit of the corresponding edit operation.
Bydefining the score of a path as the total score of itscomponent edges, we can assess the similarity ofa pair of sequences as the maximum score over allpaths in its edit graph.3.2.2 FeaturesIn our model, instead of assigning a score inde-pendently to edges of an edit graph, we assign avector of features to edges.
The score of an edgeis the inner product of this feature vector and an-other vector w, called global weight vector.
Fea-ture vectors may differ from one edge to another,but the vector w is unique in the entire system andconsistently determines the relative importance ofindividual features.In parallel to the definition of a path score, thefeature vector of a path can be defined as the sumof the feature vectors assigned to its componentedges.
Then the score of a path is equal to theinner product ?w, f?
of w and the feature vector fof the path.A feature assigned to an edge can be an arbi-trary indicator of edge directions (horizontal, ver-tical, or diagonal), edge coordinates in the editgraph, attributes (such as the surface form, part-of-speech, and the location in the sentence) of thecurrent or surrounding words, or their combina-tion.
Section 5.3 will describe the exact featuresused in our experiments.9703.2.3 Averaged path score as the score of acoordination nodeFinally, we define the score of a COORD (or COORD?
)node in a coordination tree as the average scoreof all paths in its associated edit graph.
This is an-other deviation from standard sequence alignment,in that we do not take the maximum scoring pathsas representing the similarity of conjuncts, but in-stead use the average over all paths.Notice that the average is taken over paths, andnot edges.
In this way, a natural bias is incurredtowards features occurring near the diagonal con-necting the initial vertex and the terminal vertex.For instance, in an edit graph of size 8?
8, thereis only one path that goes through the vertex at thetop-right corner, while more than 3,600 paths passthrough the vertex at the center of the graph.
Inother words, the features associated with the cen-ter vertex receives 3,600 times more weights thanthose at the top-right corner after averaging.The major benefit of this averaging is the re-duced computation during training.
During theperceptron training, the global weight vector wchanges and the score of individual paths changesaccordingly.
On the other hand, the average fea-ture vector f (as opposed to the average score?w, f?)
over all paths in the edit graph remainsconstant.
This means that f can be pre-computedonce before the training starts, and the score com-putation during training reduces to simply takingthe inner product of the current w and the pre-computed f.Alternatively, the alignment score could be de-fined as that of the best scoring path with respectto the current w, following the standard sequencealignment computation.
However, it would requirerunning the Viterbi algorithm in each iteration ofthe perceptron training, for all possible spans ofconjuncts.
While we first pursued this direction,it was abandoned as the training was intolerablyslow.3.2.4 Coordination with three or moreconjunctsFor a coordination with three or more conjuncts,we define its score as the sum of the similarityscores of all pairwise consecutive conjuncts; i.e.,for a coordination ?a, b, c, and d?
with four con-juncts, the score is the sum of the similarity scoresfor conjunct pairs (a, b), (b, c), and (c, d).
Ide-ally, we should take all combinations of conjunctsinto account, but it would lead to a combinatoriala , b , c and dW W W WCOORDCOORD?COORD?N SEP N SEP N CC NFigure 5: A coordination tree with four conjuncts.All CJT nodes are omitted.explosion and is impractical.Recall that in the grammar introduced in Sec-tion 3.1, we attached a bracketed index to COORD?.This bracketed index was introduced for the com-putation of this pairwise similarity.Figure 5 shows the coordination tree for ?a, b,c, and d.?
The pairwise similarity scores for (a,b), (b, c), and (c, d) are respectively computed atthe top COORD, left COORD?, and right COORD?
nodes,using the scheme described in Section 3.2.3.
Tocompute the similarity of a and b, we need to liftthe information about the end position of b upwardto the COORD node.
The same applies to computingthe similarity of b and c; the end position of c isneeded at the left COORD?.
The bracketed index ofCOORD?
exactly maintains this information, i.e., theend of the first conjunct below the COORD?.
Seeproduction rules (iii) and (iv) in Table 2.3.3 Perceptron learning of feature weightsAs we saw above, our model is a linear model withthe global weight vector w acting as the coefficientvector, and hence various existing techniques canbe exploited to optimize w.In this paper, we use the averaged perceptronlearning (Collins, 2002; Freund and Schapire,1999) to optimize w on a training corpus, so thatthe system assigns the highest score to the correctcoordination tree among all possible trees for eachtraining sentence.4 Discussion4.1 Computational complexityGiven an input sentence of N words, finding itsmaximum scoring coordination tree by a bottom-up chart parsing algorithm incurs a time complex-ity of O(N3).While the right-hand side of rules (i)?
(iv) in-volves more than three variables and thus appearsto increase complexity, this is not the case since971some of the variables ( j and k in rules (i) and (iii),and j, k, and m in rules (ii) and (iv)) are con-strained by the location of conjunct connectors (CCand SEP), whose number in a sentence is negligi-ble compared to the sentence length N. As a result,these rules can be processed in O(N2) time.
Hencethe run-time complexity is dominated by rule (vi),which has three variables and leads to O(N3).Each iteration of the perceptron algorithm fora sentence of length N also incurs O(N3) for thesame reason.Our method also requires pre-processing in thebeginning of perceptron training, to compute theaverage feature vectors f for all possible spans(i, j) and (k,m) of conjuncts in a sentence.
With areasoning similar to the complexity analysis of thechart parsing algorithm above, we can show thatthe pre-processing takes O(N4) time.4.2 Difference from Shimbo and Hara?smethodThe method proposed in this paper extends thework of Shimbo and Hara (2007).
Both take awhole sentence as input and use perceptron learn-ing, and the difference lies in how hypothesis co-ordination(s) are encoded as a feature vector.Unlike our new method which constructs a treeof coordinations, Shimbo and Hara used a chain-able partial paths (representing non-overlappingseries of local alignments; see (Shimbo and Hara,2007, Figure 5)) in a global triangular edit graph.In our method, we compute many edit graphs ofsmaller size, one for each possible conjunct pair ina sentence.
We use global alignment (a completepath) in these smaller graphs, as opposed to chain-able local alignment (partial paths) in a global editgraph used by Shimbo and Hara.Since nested coordinations cannot be encodedas chainable partial paths (Shimbo and Hara,2007), their method cannot cope with nested coor-dinations such as those illustrated in Figure 2(b).4.3 Integration with parsersCharniak and Johnson (2005) reported an im-proved parsing accuracy by reranking n-best parsetrees, using features based on similarity of coor-dinated phrases, among others.
It should be inter-esting to investigate whether alignment-based fea-tures like ours can be built into their reranker, ormore generally, whether the coordination scopesoutput by our method help improving parsing ac-curacy.The combinatory categorial grammar (CCG)(Steedman, 2000) provides an account for vari-ous coordination constructs in an elegant manner,and incorporating alignment-based features intothe CCG parser (Clark and Curran, 2007) is alsoa viable possibility.5 EvaluationWe evaluated the performance of our method1 onthe Genia corpus (Kim et al, 2003).5.1 DatasetGenia Treebank Beta is a collection of PennTreebank-like phrase structure trees for 4529 sen-tences from Medline abstracts.In this corpus, each scope of coordinate struc-tures is annotated with an explicit tag, and theconjuncts are always placed inside brackets.
Notmany treebanks explicitly mark the scope of con-juncts; for example, the Penn Treebank frequentlyomits bracketing of coordination and conjunctscopes, leaving them as a flat structure.Genia contains a total of 4129 occurrences ofCOOD tags indicating coordination.
These tags arefurther subcategorized into phrase types such asNP-COOD and VP-COOD.
Among coordinations anno-tated with COOD tags, we selected those surround-ing ?and,?
?or,?
and ?but.?
This yielded 3598 co-ordinations (2997, 355, and 246 for ?and,?
?or,?and ?but,?
respectively) in 2508 sentences.
Thesecoordinations constitute nearly 90% of all coordi-nations in Genia, and we used them as the evalua-tion dataset.
The length of these sentences is 30.0words on average.5.2 Evaluation methodWe tested the proposed method in two tasks:(i) identify the scope of coordinations regardlessof phrase types, and(ii) detect noun phrase (NP) coordinations andidentify their scopes.While the goal of task (i) is to determine the scopesof 3598 coordinations, task (ii) demands both tojudge whether each of the coordinations constructsan NP, and if it does, to determine its scope.1A C++ implementation of our method can be foundat http://cl.naist.jp/project/coordination/, along with supple-mentary materials including the preliminary experimental re-sults of the CCG parser on the same dataset.972Table 3: Features in the edit graph for conjuncts wkwk+1 ?
?
?wm and wlwl+1 ?
?
?wn.edge/vertex type vertical edge horizontal edge diagonal edge initial vertex terminal vertex?
?
?w j?1 w j w j+1?
?
?...wi?1wiwi+1...?
?
?w j?1 w j w j+1?
?
?...wi?1wiwi+1...?
?
?w j?1 w j w j+1?
?
?...wi?1wiwi+1...wl wl+1 ?
?
?wkwk+1...?
?
?wn?1 wn...wm?1wmvertical bigrams wi?1wiwiwi+1wi?1wi wi?1wiwiwi+1wk?2wk?1wk?1wkwkwk+1wm?2wm?1wm?1wmwmwm+1horizontal bigrams wj?1wj w j?1wjw jw j+1wj?1wjw jw j+1wl?2wl?1wl?1wlwlwl+1wn?2wn?1wn?1wnwnwn+1orthogonal bigrams wiw j wk?1wl?1wk?1wlwkwl?1wkwlwm?1wn?1wm?1wnwmwn?1wmwnFor comparison, two parsers, the Bikel-Collinsparser (Bikel, 2005)2 and Charniak-Johnsonreranking parser3, were applied in both tasks.Task (ii) imitates the evaluation reported byShimbo and Hara (2007), and to compare ourmethod with their coordination analysis method.Because their method can only process flat coordi-nations, in task (ii) we only used 1613 sentences inwhich ?and?
occurs just once, following (Shimboand Hara, 2007).
Note however that the split ofdata is different from their experiments.We evaluate the performance of the tested meth-ods by the accuracy of coordination-level brack-eting (Shimbo and Hara, 2007); i.e., we counteach of the coordination (as opposed to conjunct)scopes as one output of the system, and the systemoutput is deemed correct if the beginning of thefirst output conjunct and the end of the last con-junct both match annotations in the Genia Tree-bank.In both tasks, we report the micro-averaged re-sults of five-fold cross validation.The Bikel-Collins and Charniak-Johnsonparsers were trained on Genia, using all the phrasestructure trees in the corpus except the test set;i.e., the training set alo contains (in addition tothe four folds) 2021(= 4129 ?
2508) sentenceswhich are not in the five folds.
Since the twoparsers were also trained on Genia, we interpretthe bracketing above each conjunction in theparse tree output by them as the coordinationscope output by the parsers, in accordance withhow coordinations are annotated in Genia.
In2http://www.cis.upenn.edu/?dbikel/software.html3ftp://ftp.cs.brown.edu/pub/nlparser/reranking-parserAug06.tar.gztesting, the Bikel-Collins parser and Shimbo-Haramethod were given the gold parts-of-speech(POS) of the test sentences in Genia.
We trainedthe proposed method twice, once with the goldPOS tags and once with the POS tags output bythe Charniak-Johnson parser.
This is because theCharniak-Johnson parser does not accept POStags of the test sentences.5.3 FeaturesTo compute features for our method, each wordin a sentence was represented as a list of at-tributes.
The attributes include the surface word,part-of-speech, suffix, prefix, and the indicatorsof whether the word is capitalized, whether it iscomposed of all uppercase letters or digits, andwhether it contains digits or hyphens.
All fea-tures are defined as an indicator of an attribute intwo words coming from either a single conjunct(either horizontal or vertical word sequences asso-ciated with the edit graph) or two conjuncts (onefrom the horizontal word sequence and one fromthe vertical sequence).
We call the first type hori-zontal/vertical bigrams and the second orthogonalbigrams.Table 3 summarizes the features in an editgraph for two conjuncts (wkwk+1 ?
?
?wm) and(wlwl+1 ?
?
?wn), where wi denotes the ith word inthe sentence.As seen from the table, features are assignedto the initial and terminal vertices as well as toedges.
A wiwj in the table indicates that for eachattribute (e.g., part-of-speech, etc.
), an indicatorfunction for the combination of the attribute val-ues in wi and wj is assigned to the vertex or edgeshown in the figure above.
Note that the features973Table 4: Results of Task (i).
The number of coor-dinations of each type (#), and the recall (%) forthe proposed method, Bikel-Collins parser (BC),and Charniak-Johnson parser (CJ).gold POS CJ POSCOOD # Proposed BC Proposed CJOverall 3598 61.5 52.1 57.5 52.9NP 2317 64.2 45.5 62.5 50.1VP 465 54.2 67.7 42.6 61.9ADJP 321 80.4 66.4 76.3 48.6S 188 22.9 67.0 15.4 63.3PP 167 59.9 53.3 53.9 58.1UCP 60 36.7 18.3 38.3 26.7SBAR 56 51.8 85.7 33.9 83.9ADVP 21 85.7 90.5 85.7 90.5Others 3 66.7 33.3 33.3 0.0assigned to different types of vertex or edge aretreated as distinct even if the word indices i and jare identical; i.e., all features are conditioned onedge/vertex types to which they are assigned.5.4 ResultsTask (i) Table 4 shows the results of task (i).
Weonly list the recall score in the table, as precision(and hence F1-measure, too) was equal to recallfor all methods in this task; this is not surpris-ing given that in this data set, conjunctions ?and?,?or?, and ?but?
always indicate the existence of acoordination, and all methods successfully learnedthis trend from the training data.The proposed method outperformed parsers onthe coordination scope identification overall.
Thetable also indicates that our method considerablyoutperformed two parsers on NP-COOD, ADJP-COOD,and UCP-COOD categories, but it did not work wellon VP-COOD, S-COOD, and SBAR-COOD.
In contrast,the parsers performed quite well in the latter cate-gories.Task (ii) Table 5 lists the results of task (ii).The proposed method outperformed Shimbo-Haramethod in this task, although the setting of thistask is mostly identical to (Shimbo and Hara,2007) and does not include nested coordinations.Note also that both methods use roughly equiva-lent features.One reason should be that our grammar rulescan strictly enforce the scope consistency of con-juncts in coordinations with three or more con-juncts.
Because the Shimbo-Hara method repre-sents such coordinations as a series of sub-pathsin an edit graph which are output independentlyof each other without enforcing consistency, theirTable 5: Results of Task (ii).
Proposed method,BC: Bikel-Collins, CJ: Charniak-Johnson, SH:Shimbo-Hara.gold POS CJ POSProposed BC SH Proposed CJPrecision 61.7 45.6 55.9 60.2 49.0Recall 57.9 46.1 53.7 55.6 46.8F1 59.7 45.8 54.8 57.8 47.9method can produce inconsistent scopes of con-juncts in the middle.In fact, the advantage of the proposed method intask (ii) is noticeable especially in coordinationswith three or more conjuncts; if we restrict the testset only to coordinations with three or more con-juncts, the F-measures in the proposed method andShimbo-Hara become 53.0 and 42.3, respectively;i.e., the margin increases to 10.7 from 4.9 points.6 Conclusion and outlookWe have proposed a method for learning andanalyzing generic coordinate structures includingnested coordinations.
It consists of a simple gram-mar for coordination and perceptron learning ofalignment-based features.The method performed well overall and on co-ordinated noun and adjective phrases, but not oncoordinated verb phrases and sentences.
The lat-ter coordination types are in fact easy for parsers,as the experimental results show.The proposed method failing in verbal and sen-tential coordinations is as expected, since con-juncts in these coordinations are not necessarilysimilar, if they are viewed as a sequence of words.We will investigate similarity measures differentfrom sequence alignment, to better capture thesymmetry of these conjuncts.We will also pursue integration of our methodwith parsers.
Because they have advantages in dif-ferent coordination phrase types, their integrationlooks promising.AcknowledgmentsWe thank anonymous reviewers for helpful com-ments and the pointer to the combinatory catego-rial grammar.ReferencesRajeev Agarwal and Lois Boggess.
1992.
A simple butuseful approach to conjunct identification.
In Pro-ceedings of the 30th Annual Meeting of the Associa-974tion for Computational Linguistics (ACL?92), pages15?21.Daniel M. Bikel.
2005.
Multilingual statistical pars-ing engine version 0.9.9c.
http://www.cis.upenn.edu/?dbikel/software.html.Ekaterina Buyko and Udo Hahn.
2008.
Are morpho-syntactic features more predicative for the resolutionof noun phrase coordination ambiguity than lexico-semantic similarity scores.
In Proceedings of the22nd International Conference on ComputationalLinguistics (COLING 2008), pages 89?96, Manch-ester, UK.Ekaterina Buyko, Katrin Tomanek, and Udo Hahn.2007.
Resolution of coordination ellipses in bi-ological named entities using conditional randomfields.
In Proceedings of the Pacific Associationfor Computational Linguistics (PACLIC?07), pages163?171.Robyn Carston and Diane Blakemore.
2005.
Editorial:Introduction to coordination: syntax, semantics andpragmatics.
Lingua, 115:353?358.Francis Chantree, Adam Kilgarriff, Anne de Roeck,and Alistair Willis.
2005.
Disambiguating coor-dinations using word distribution information.
InProceedings of the Int?l Conference on Recent Ad-vances in Natural Language Processing, Borovets,Bulgaria.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and MaxEnt discriminativereranking.
In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguistics(ACL 2005), pages 173?180, Ann Arbor, Michigan,USA.Stephen Clark and James R. Curran.
2007.
Wide-coverage efficient statistical parsing with CCGand log-linear models.
Computational Linguistics,33(4):493?552.Michael Collins.
2002.
Discriminative training meth-ods for hidden Markov models: theory and experi-ments with perceptron algorithms.
In Proceedingsof the Conference on Empirical Methods in Natu-ral Language Processing (EMNLP 2002), pages 1?8, Philadelphia, PA, USA.Yoav Freund and Robert E. Schapire.
1999.
Largemargin classification using the perceptron algorithm.Machine Learning, 37(3):277?296.Miriam Goldberg.
1999.
An unsupervised modelfor statistically determining coordinate phrase at-tachment.
In Proceedings of the Annual Meetingof the Association for Computational Linguistics(ACL 1999), pages 610?614, College Park, Mary-land, USA.Dan Gusfield.
1997.
Algorithms on Strings, Trees, andSequences.
Cambridge University Press.Deirdre Hogan.
2007.
Coordinate noun phrase disam-biguation in a generative parsing model.
In Proceed-ings of the 45th Annual Meeting of the Association ofComputational Linguistics (ACL 2007), pages 680?687, Prague, Czech Republic.J.-D. Kim, T. Ohta, Y. Tateisi, and J. Tsujii.
2003.GENIA corpus: a semantically annotated corpus forbio-textmining.
Bioinformatics, 19(Suppl.
1):i180?i182.Sadao Kurohashi and Makoto Nagao.
1994.
A syn-tactic analysis method of long Japanese sentencesbased on the detection of conjunctive structures.Computational Linguistics, 20:507?534.Preslav Nakov and Marti Hearst.
2005.
Using the webas an implicit training set: application to structuralambiguity resolution.
In Proceedings of the HumanLanguage Technology Conference and Conferenceon Empirical Methods in Natural Language (HLT-EMNLP 2005), pages 835?842, Vancouver, Canada.Akitoshi Okumura and Kazunori Muraki.
1994.
Sym-metric pattern matching analysis for English coordi-nate structures.
In Proceedings of the Fourth Con-ference on Applied Natural Language Processing,pages 41?46.Philip Resnik.
1999.
Semantic similarity in a tax-onomy.
Journal of Artificial Intelligence Research,11:95?130.Wolfgang Schuette, Thomas Blankenburg, WolfGuschall, Ina Dittrich, Michael Schroeder, HansSchweisfurth, Assaad Chemaissani, Christian Schu-mann, Nikolas Dickgreber, Tabea Appel, and Di-eter Ukena.
2006.
Multicenter randomized trial forstage iiib/iv non-small-cell lung cancer using every-3-week versus weekly paclitaxel/carboplatin.
Clini-cal Lung Cancer, 7:338?343.Masashi Shimbo and Kazuo Hara.
2007.
A discrimi-native learning model for coordinate conjunctions.In Proceedings of Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning (EMNLP-CoNLL 2007), pages 610?619, Prague, Czech Re-public.Mark Steedman.
2000.
The Syntactic Process.
MITPress, Cambridge, MA, USA.975
