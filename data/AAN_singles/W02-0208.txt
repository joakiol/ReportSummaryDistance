DialogueView: An Annotation Tool for DialoguePeter A. Heeman and Fan Yang and Susan E. StrayerComputer Science and EngineeringOGI School of Science and EngineeringOregon Health & Science University20000 NW Walker Rd., Beaverton OR, 97006heeman@cse.ogi.edu yangf@cse.ogi.edu susan strayer@yahoo.comAbstractThis paper describes DialogueView, atool for annotating dialogues with utter-ance boundaries, speech repairs, speechact tags, and discourse segments.
Thetool provides several views of the data,including a word view that is time-aligned with the audio signal, and an ut-terance view that shows the dialogue asif it were a script for a play.
The ut-terance view abstracts away from lowerlevel details that are coded in the wordview.
This allows the annotator to havea simpler view of the dialogue when cod-ing speech act tags and discourse struc-ture, but still have access to the detailswhen needed.1 IntroductionThere is a growing interest in annotating human-human dialogue.
Annotated dialogues are usefulfor formulating and verifying theories of dialogueand for building statistical models.
In addition toorthographic word transcription, one might wantthe following dialogue annotations. Annotation of the speech repairs.
Speech re-pairs are a type of disuency where speakersgo back and change or repeat something theyjust said. Segmentation of the speech of each speakerinto utterance units, with a single ordering ofthe utterances.
We refer to this as linearizingthe dialogue (Heeman and Allen, 1995a). Each utterance tagged with its speech actfunction. The utterances grouped into hierarchical dis-course segments.There are tools that address subsets of the abovetasks.
However, we feel that doing dialogue an-notation is very di?cult.
Part of this di?cultyis due to the interactions between the annotationtasks.
An error at a lower level can have a largeimpact on the higher level annotations.
For in-stance, there can be ambiguity as to whether an\okay" is part of a speech repair; this will im-pact the segmentation of the speech into utter-ance units and the speech act coding.
Sometimes,it is only by considering the higher level annota-tions that one can make sense of what is going onat the lower levels, especially when there is over-lapping speech.
Hence, a tool is needed that letsusers examine and code the dialogue at all lev-els.
The second reason why dialogue annotationis di?cult is because it is di?cult to follow whatis occurring in the dialogue, especially for codingdiscourse structure.
A dialogue annotation toolneeds to help the user deal with this overload.In this paper, we describe our dialogue anno-tation tool, DialogueView.
This tool displays thedialogue at three dierent levels of abstraction.The word level shows the words time-aligned withthe audio signal.
The utterance level shows thedialogue as a sequence of utterance, as if it were ascript for a play.
It abstracts away from the exacttiming of the words and even skips words that donot impact the progression of the dialogue.
Theblock level shows the dialogue as a hierarchy ofdiscourse segment purposes, and abstracts awayfrom the exact utterances that were said.
Anno-tations are done at the view that is most appropri-ate for what is being annotated.
The tool allowsthe user to easily navigate between the three viewsand automatically updates the higher level viewswhen changes are made in the lower level views.Because the higher levels abstract away lower leveldetails, it is easier for the user to understand whatis happening in the dialogue.1Yet, the user caneasily refer to the lower level views to see what1This approach bears resemblance to the work ofJonsson and Dahlback (2000) in which they distillhuman-human dialogues by removing those parts thatwould not occur in human-computer dialogue.
Theydo this to create training data for their spoken dia-logue systems.Philadelphia, July 2002, pp.
50-59.
Association for Computational Linguistics.Proceedings of the Third SIGdial Workshop on Discourse and Dialogue,actually occurred when necessary.In the rest of this paper, we rst discuss otherannotation tools.
We then describe the three lev-els of abstraction in our annotation tool.
We thendiscuss audio playback.
Next, we discuss how thetool can be customized for dierent annotationschemes.
Finally, we discuss the implementationof the tool.2 Existing ToolsThere are a number of tools that let users annotatespeech audio les.
These include Emu (Cassidyand Harrington, 2001), SpeechView (Sutton et al,1998) and Waves+ (Ent, 1993).
These tools oftenallow multiple text annotation les to be associ-ated with the waveform and allow users an easy fa-cility to capture time alignments.
For instance, forthe ToBI annotation scheme (Pitrelli et al, 1994),one can have the word alignment in one text le,intonation features in a second, break indices in athird, and miscellaneous features in a fourth.
Theaudio annotation tools often have powerful signalanalysis packages for displaying such phenomenaas spectrograms and voicing.
These tools, how-ever, do not have any built-in facility to groupwords into utterances nor group utterances intohierarchical discourse segments.Several tools allow users to annotate higherlevel structure in dialogue.
The annotation toolDAT from the University of Rochester (Fergu-son, 1998) allows users to annotate utterances orgroups of utterances with a set of hard-coded an-notation tags for the DAMSL annotation scheme(Allen and Core, 1997; Core and Allen, 1997).The tool Nb from MIT (Flammia, 1995; Flammia,1998) allows users to impose a hierarchical group-ing on a sequence of utterances, and hence anno-tate discourse structure.
Both DAT and Nb takeas input a linearization of the speaker utterances.Mistakes in this input cannot be xed.
Whetherthere are errors or not, users cannot see the exacttiming interactions between the speakers' words orthe length of pauses.
This simplication can makeit di?cult for the annotator to truly understandwhat is happening in the dialogue, especially foroverlapping speech, where speakers ght over theturn or make back-channels.The tools Transcriber (Barras et al, 2001) andMate (McKelvie et al, 2001) allow multiple viewsof the data: a word view with words time-alignedto the audio signal and an utterance view.
How-ever, Transcriber is geared to single channel dataand has a weak ability to handle overlappingspeech and Mate only allows one view to be shownat a time.
The author of a competing tool hasremarked that \speed and stability of [Mate] areboth still problematic for real annotation.
Also,the highly generic approach increases the initialeort to set up the tool since you basically haveto write your own tool using the Mate style-sheetlanguage" (Kipp, 2001).
Hence, he developed anew tool Anvil that he claims is a better trade-oamong generality, functionality, and complexity.This tool oers multi-modal annotation support,but like Transcriber, does not allow detailed an-notation of dialogue phenomena, such as overlap-ping speech and abandoned speech, and has noabstraction mechanism.3 Word ViewOur dialogue annotation tool, DialogueView,gives the user three views of the data.
The low-est level view is called WordView and takes asinput the words said by each speaker and theirstart and stop times and shows them time-alignedwith the audio signal.
Figure 1 shows an ex-cerpt from the Trains corpus (Heeman and Allen,1995b) in WordView.
This view is ideal for view-ing the exact timing of speech, especially overlap-ping speech.
As will be discussed below, we useit for segmenting the speech into utterances, andannotating communicative status and speech re-pairs.2These annotations will allow us to build asimpler representation of what is happening in thespeech for the utterance view, which is discussedin the next section.3.1 Utterance SegmentationAs can be seen in Figure 1, WordView shows thewords segmented into utterances, which for ourpurpose is simply a grouping of consecutive wordsby a single speaker, in which there is minimaleect from the other speaker.
Consider the ex-change in Figure 1, where the upper speaker says\and then take the remaining boxcar down to" fol-lowed by the lower speaker saying \right, to du-",followed by the upper speaker saying \Corning".Although one could consider the upper speaker'sspeech as one utterance, we preclude that due tothe interaction from the lower speaker.
A full def-inition of `utterance' is beyond the scope of thispaper, and is left to the users to specify in theirannotation scheme.Utterance boundaries in WordView are onlyshown by their start times.
The end of the utter-ance is either the start of the next utterance by the2There currently is no support for changing theword transcription.
There are already a number oftools that do an excellent job at this task.
Hence,adding this capability is a low priority for us.Figure 1: Utterance Segmentation in WordViewsame speaker or the end of the le.
With Word-View, the user can easily move, insert or deleteutterance boundaries.
The tool ensures that theboundaries never fall in the middle of a word bythe speaker.The start times of the utterances are used toderive a single ordering of the utterances of thetwo speakers.
This linearization of the dialoguecaptures how the speakers are sequentially addingto the dialogue.
The linearization is used by thenext view to give an abstraction away from theexact timing of the speech.3.2 Communicative StatusWordView allows features of the utterances to beannotated.
In practice, however, we only anno-tate features related to its communicative status,based on the DAMSL annotation scheme (Allenand Core, 1997).
Below, we give the utterancetags that we assign in WordView.Abandoned: The speaker abandoned whatthey were saying and it had no impact on the restof the dialogue.
This mainly happens where onespeaker loses the ght over who speaks next.
Fig-ure 2 gives an example where the upper speakertries to take the turn by saying \takes," and thanabandons this eort.Incomplete: The speaker did not make a com-plete utterance, either prosodically, syntactically,or semantically.
Unlike the previous category, thepartial utterance did impact the dialogue behav-ior.
Figure 1 gives two examples.
The rst iswhere the upper speaker says \and than take theremaining boxcar down to", and the second iswhere the lower speaker said \to du-," which wasthan completed by the upper speaker.Overlap: The speech in the utterance overlapswith the speech from the prior utterance.
Forinstance, in Figure 1, the lower speaker utters\okay" during the middle of an utterance, perhapsto tell the upper speaker that they are understand-ing everything so far.
However, a simple lineariza-tion would make it seem that the \okay" is anacknowledgment of the entire utterance, which isnot the case.
Hence, we tag the \okay" utterancewith the overlap tag.
The next view, Utterance-View, will take the overlap tag into account indisplaying the utterances, as will be discussed inSection 4.3.Not all overlapping speech needs to be anno-tated with the overlap tag.
In Figure 1, the sec-ond instance of \Corning" overlaps slightly withthe rst instance of \Corning".
However, viewingit sequentially does not alter the analysis of theexchange.3.3 Reordering Overlapping SpeechConsider the example in Figure 2.
Before the ex-cerpt shown, the lower speaker had just nishedan utterance and then paused for over a second.The upper speaker then acknowledged the utter-ance with \okay", but this happened a fraction ofa second after the bottom speaker started speak-ing again.
A simple linearization of the dialoguewould have the \okay" following the wrong stretchof speech|\and than th(at)- takes w- what threehours."
A solution to this would be to anno-Figure 2: Utterance Ordering and Speech Repairs in WordViewtate the \okay" with the overlap tag.
However,this \Okay" is more similar to the overlapping in-stances of \Corning" in Figure 1.
The fact that\Okay" overlaps with the start of the next utter-ance is not critical to understanding what is oc-curring in the dialogue, as long as we linearizethe \Okay" to occur before the other utterance.WordView allows the user to change the lineariza-tion by moving the start times of utterances.
Thiscan be done provided that the speaker was silentin the time interval preceding where the other per-son started talking.In summary, overlapping speech can be handledin three ways.
The utterance can be explicitlytagged as overlapping; the overlap can be ignoredif it is not critical in understanding what is goingon in the dialogue; or the start time of the utter-ance can be changed so that the overlap does notneed to be tagged.3.4 Speech RepairsWordView also allows users to annotate speechrepairs.
A speech repair is where a user goes backand repeats or changes something that was justsaid (Heeman and Allen, 1999).
Below we give anexample of a speech repair and show its principlecomponents: reparandum, interruption point, andediting term.Example 1why don't we take|{z}reparandum"ipum|{z}ettake two boxcarsThe reparandum is the speech that is being re-placed, the interruption point is the end of thereparandum, and the editing term consists ofwords such as \um", \uh", \okay", \let's see" thathelp signal the repair.To annotate a repair, the user highlights a se-quence of words and then tags it as a reparandumor an editing term of a repair.
The user can alsospecify the type of repair.
Figure 2 shows howspeech repairs are displayed in WordView.
Thewords in the reparandum and editing term are un-derlined and displayed in a special color.3.5 Speech Repairs and UtterancesSome phenomena can be marked as either a speechrepair, or could be marked using the utterancetags of incomplete or abandon.
This is especiallytrue for fresh starts (Heeman and Allen, 1999),where a speaker abandons the current utteranceand starts over.
To avoid having multiple ways ofannotating the same phenomena, we impose thefollowing restrictions in our annotation scheme. There cannot be an utterance boundary in-side of a reparandum, inside of an editingterm, at the interruption point, nor at theend of the editing term.
Hence, somethingannotated as a reparandum cannot also beannotated as an abandoned utterance. Abandoned or incomplete utterances cannotbe followed by an utterance by the samespeaker. All word fragments must either be the lastword of a reparandum or the last word of anutterance that is marked as abandoned or in-complete.Figure 3: UtteranceView: Segmented Utterances in UtteranceView Abandoned or incomplete utterances can endwith an editing term, which would be markedas the editing term of an abridged repair.3.6 SummaryThere are a number of reasons why we annotateutterance boundaries, speech repairs, and commu-nicative status in WordView.
Annotating utter-ance boundaries and overlapping speech requiresthe user to take into account the exact timing ofthe utterances, which is best done in this view.Speech repairs also require ne tuned listening tothe speech and have strong interactions with ut-terance boundaries.
Furthermore, all three typesof annotations can be used to build a simpler viewof what is happening in the dialogue, as will be ex-plained in the next section.4 Utterance ViewThe annotations from the word view are used tobuild the next view, which we refer to as Utter-anceView.
The dialogue excerpts from Figures 1and 2 are shown in the utterance view in Figures 3and 4, respectively.
The utterance view abstractsaway from the detailed timing information and in-dividual words that were spoken.
Instead, it fo-cuses on the sequence of utterances between thespeakers.
By removing details that were anno-tated in the word view, we still preserve the im-portant details that are needed to annotate speechact types for the utterances and to annotate dis-course segments.
Of course, if the user wants tosee the exact timing of the words in the utter-ances, they can examine the word view, as it isFigure 4: UtteranceView: Reordered and Abandoned Utterances in UtteranceViewdisplayed alongside the utterance view.
There arealso navigation buttons on each view that allowthe user to easily reposition the portion of the di-alogue in the other view.
Furthermore, changesmade in the word view are immediately propa-gated into the utterance view, and hence the userwill immediately see the impact of their annota-tions.4.1 Utterance OrderingUtterance ordering in the utterance view is deter-mined by the start times of the utterances as spec-ied in WordView.
As was explained earlier, alter-ing the start time of an utterance can be used tosimplify some cases of overlapping speech, wherethe overlap is not critical to understanding therole of the utterance in the dialogue.
Figure 2gave the word view of such an example.
Ratherthan code it as an overlap, we moved the starttime of the \okay" utterance so that it precedesthe overlapping speech by the other speaker.
Fig-ure 4 shows how this looks in the utterance view.Here, the annotator would view the \okay" as anacknowledgment that occurred between the twoutterances of the lower speaker.4.2 Speech RepairsIn the word view, the user annotates the reparan-dum and editing term of speech repairs.
If thereparandum and editing term are removed, theresulting utterance reects what the speaker in-tended to say.
Speech repairs do carry informa-tion. Their occurrence can signal a lack of certaintyof the speaker. The reparandum of a repair can have ananaphoric reference, as in \Peter was, wellhe was red.
"However, removing the reparandum and editingterm of speech repairs from utterances in the ut-terance view leads to a simpler representation ofwhat is occurring in the dialogue.
Hence, in theutterance view, we clean up the speech repairs, asshown in Figures 2 and 4.
Figure 2, which showsthe word view, contains the utterance \and thenth(at) that takes w- what three hours"; whereasFigure 4, which shows the utterance view, con-tains \and then that takes what three hours."
Ofcourse, a user can always refer to the word viewwhen annotating in the utterance view if theywant to see the exact speech that was said.
Inmost cases, we feel that this will not be neces-sary for annotating speech act tags and discoursesegments.4.3 Communicative StatusThe communicative status coded in the word viewis used in formatting the utterance view.
Utter-ances tagged as overlapping are indented and dis-played with `+' on either side, as shown in Fig-ure 3.
Utterances tagged as abandoned are notshown, as can be seen in Figure 4, in which theabandoned utterance \takes" made by the upperspeaker is not included.
Utterances tagged as in-complete are shown with a trailing \..." as shownin Figure 3.4.4 Annotating Utterance TagsIn the utterance view, one can also annotate theutterances with various tags.
For our work, weuse a subset of the DAMSL tags correspondingto forward and backward functions (Allen andCore, 1997).
Forward functions include state-ment, request information, and suggestion.
Back-ward functions include answer, acknowledgment,and agreement.
Although these utterance tagscould be annotated in the word view, doing it inthe utterance view allows us to see more context,which is needed to give the utterance the propertags.
When necessary, the annotator can easilyrefer to the word view to see the exact local con-text.4.5 Annotating Blocks of UtterancesIn the utterance view, the user can also annotatehierarchical groupings of utterances.3We use theutterance blocks to annotate discourse structure(Grosz and Sidner, 1986).
This is similar to whatFlammia's tool allows (Flammia, 1995).
Ratherthan showing it with indentation and color, wedraw boxes around segments.
Figure 3 shows adialogue excerpt with three utterance blocks in-side of a larger block.
To create a segment, theuser highlights a sequence of utterances and thenpresses the \make segment" button.
The user canchange the boundaries of the blocks by simplydragging either the top or bottom edge of the box.Blocks can also be deleted.
The tool ensures thatif two blocks have utterances in common then oneblock is entirely contained in the other.Tags can also be assigned to the blocks.
Wehave just started using the tool for discourse seg-mentation, and so we are still rening these tags.In Grosz and Sidner's theory of discourse struc-ture (1986), the speaker who initiates the blockdoes so to accomplish a certain purpose.
We havea tag for annotating this purpose.
We also havetags to categorize the block as a greeting, spec-ify goal, construct plan, summarize plan, verifyplan, give info, request info, or other (Strayer andHeeman, 2001).The utterance view also allows the user to openor close a block.
When a block is open (the de-fault), all of its utterances and sub-blocks are dis-3We do not allow segments to be interleaved.
It isunclear if such phenomena actually occur.played.
When it is closed, its utterances and sub-blocks are replaced by the single line purpose.Opening and closing blocks is useful as it allowsthe user to control the level of detail that is shown.Consider the third embedded block shown in Fig-ure 3, in which the conversants take seven utter-ances to jointly make a suggestion.
After we haveanalyzed it, we can close it and just see the pur-pose.
This will make it easier to determine thesegmentation of the blocks that contain it.We are experimenting with a special type ofdialogue block.
Consider the example from theprevious paragraph, in which the conversantstake seven utterances to jointly make a sugges-tion.
This is related to the shared turns ofSchirin (1987), the co-operative completions ofLinell (1998), and the grounding units of Traumand Nakatani (1999).
We are experimenting withhow to support the annotation of such phenom-ena.
We have added a tag to indicate whether theutterances in the block are being used to build asingle contribution.
For these single contributions,we also supply a concise paraphrase of what wassaid.
We have found that this paraphrase can bebuilt from a sequential subset of the words in theutterances of the block.
For instance, the para-phrase of our example block is \and then take theremaining boxcar down to Corning.
"5 Block ViewWe are experimenting with a third view of thedialogue.
This view, which we refer to as Block-View, abstracts away from the individual utter-ances, and shows the hierarchical structure of thediscourse segments.
This gives a very concise viewof the dialogue.
The block view is also convenientfor it provides an index to the whole dialogue.This allows the user to quickly move around thedialogue.6 Audio PlaybackEach view gives the user the ability to select a re-gion of the dialogue and to play it.
In the wordview, the user can play each speaker channel indi-vidually or both combined.4This ability is espe-cially useful for overlapping speech, where the an-notator would want to listen to what each speakersaid individually, as well as hear the timing be-tween the speaker utterances.Just as each view provides a visual abstractionfrom the previous one, we also do the same withaudio playback.
In the word view, which has4In order to play each speaker individually, we re-quire a separate audio channel for each speaker.wordViewUtt => atmostoneof abandoned incomplete overlaputtViewUtt => anyof forward backward commentuttViewUtt.forward => oneof statement question suggestion otheruttViewUtt.backward => oneof agreement understanding answer otheruttViewUtt.comment => otherFigure 5: Sample Specication of Utterance TagsFigure 6: Sample Utterance Annotation Panelthe speech repairs annotated, the user can playback the speech cleanly of either speaker, wherethe stretches of speech annotated as the reparan-dum or editing term of a repair are skipped.
Wehave found this to be of great assistance in verify-ing if something should be annotated as a repairor not.
It gives us an audio means to verify thespeech repair annotations.
If we have annotatedthe repair correctly, the edited audio signal shouldsound fairly natural.In formatting the utterance view, we take intoaccount whether utterances have been marked asabandoned or overlapped.
We provide a specialplayback in the utterance view that takes thisinto account.
We build an audio le in which weskip over repairs, skip over abandoned speech, andshorten large silences.
If there is overlap that isnot marked as signicant, we linearize it by con-catenating the utterances together.
If the overlapis marked as signicant, we keep the overlap.
Weare nding that this gives us an audio means toensure that our markings of abandonment, over-lap and our linearization is correct.We are also experimenting with even furthersimplifying the audio output.
For blocks that havea paraphrase, and the block is closed, we play theparaphrase by constructing it from the words saidin the block.
For blocks that are closed that donot have a paraphrase, we use the text-to-speechengine in the CSLU toolkit (Colton et al, 1996;Sutton et al, 1997) to say the purpose, as if therewas a narrator.7 CustomizationsSome aspects of the tool are built in, such as thenotion of utterances, speech repairs, and hierar-chical grouping of utterances into blocks.
How-ever, the annotations of these phenomena and howthey are displayed can be customized through aconguration le.
This allows us to easily exper-iment as we revise our annotation scheme; to usedomain specic tags; and to make the tool usefulfor other researchers who might use dierent tags.Speech repair tags, utterance tags, and blocktags are specied in the conguration le.
Fig-ure 5 gives a sample of how the annotationtags for an utterance are specied.
The two toplevel entries in the gure are \wordViewUtt" and\uttViewUtt", which specify the utterance anno-tation tags in WordView and UtteranceView, re-spectively.
The decomposition can be of one ofthree types.atmostoneof: at most one of the attributes canbe speciedoneof: exactly one of the attributes must be spec-iedanyof: there is no restriction on which attributescan be speciedThe subcomponents can either be terminals as isthe case for the decomposition of \wordViewUtt",or can be non-terminals, as is the case for eachof the three subcomponents of \uttViewUtt".Hence, hierarchical tags are supported.
Termi-nals are assumed to be of binary type, except for\other", which is assumed to be a string.
Theconguration le determines how the annotationpanel is generated.
For the annotation schemespecied in Figure 5, Figure 6 shows the annota-tion panel that would be automatically generatedfor the utterance view.As we explained earlier, some of the utterancetags aect how the word view and utterance vieware formatted.
Rather than hard code this func-tionality, it is specied in the conguration le.We are still experimenting with the best way tocode this functionality.
Figure 7 gives an exam-ple of how we code the utterance tag function-ality.
The rst line indicates that the utterancewordViewUtt.abandoned do wordView color redwordViewUtt.abandoned uttView ignorewordViewUtt.incomplete wordView color yellowwordViewUtt.incomplete uttView trailsoffwordViewUtt.overlap wordView color bluewordViewUtt.overlap uttView overlapFigure 7: Sample Utterance Display Specicationtag of \abandoned" coded in WordView shouldbe displayed in red in WordView.
The second lineindicates that it should not be displayed in Utter-anceView.8 ImplementationDialogueView is written in Tcl/Tk.
We also useutilities from the CSLU Speech Toolkit (Coltonet al, 1996; Sutton et al, 1997), including audioand wave handling and speech synthesis.
We haverewritten the tool to use an object-oriented exten-sion of Tcl called Tclpp, designed and developedby Stefan Simnige.
This is allowing us to bettermanage the growing complexity of the tool as wellas reuse pieces of the software in our annotationcomparison tool (Yang et al, 2002).
It should alsohelp in expanding the tool so that it can handleany number of speakers.9 ConclusionIn this paper, we described a dialogue annotationtool that we are developing for segmenting dia-logue into utterances, annotating speech repairs,tagging speech acts, and segmenting dialogue intohierarchical discourse segments.
The tool presentsthe dialogue at dierent levels of abstraction al-lowing the user to both see in detail what is go-ing on and see the higher level structure that isbeing built.
The higher levels not only abstractaway from the exact timing, but also can skip overwords, whole utterances, and even simplify seg-ments to a single line paraphrase.
Along with thevisual presentation, the audio can also be playedat these dierent levels of abstraction.
We feelthat these capabilities should help annotators bet-ter code dialogue.This tool is still under active development.
Inparticular, we are currently rening how blocksare displayed, improving the ability to customizethe tool for dierent tagsets, and improving theaudio playback facilities.
As we develop this tool,we are also doing dialogue annotation, and ren-ing our scheme for annotating dialogue in order tobetter capture the salient features of dialogue andimprove the inter-coder reliability.10 AcknowledgmentsThe authors acknowledgment funding from the In-tel Research Council.ReferencesJames F. Allen and Mark G. Core.
1997.
Damsl:Dialog annotation markup in several layers.Unpublished Manuscript.Claude Barras, Edouard Georois, Zhibiao Wu,and Mark Liberman.
2001.
Transcriber: devel-opment and use of a tool for assisting speechcorpora production.
Speech Communications,33:5{22.Steve Cassidy and Jonathan Harrington.
2001.Multi-level annotation in the Emu speechdatabase management system.
Speech Commu-nications, 33:61{77.Don Colton, Ron Cole, David G. Novick, andStephen Sutton.
1996.
A laboratory coursefor designing and testing spoken dialogue sys-tems.
In Proceedings of the International Con-ference on Audio, Speech and Signal Processing(ICASSP), pages 1129{1132.Mark G. Core and James F. Allen.
1997.
Codingdialogs with the DAMSL annotation scheme.In Working notes of the AAAI Fall Symposiumon Communicative Action in Humans and Ma-chines.Entropic Research Laboratory, Inc., 1993.WAVES+ Reference Manual.
Version 5.0.George Ferguson.
1998.
DAT: Dialogue annota-tion tool.
Available from www.cs.rochester.eduin the subdirectory research/speech/damsl.Giovanni Flammia.
1995.
N.b.
: A graphical userinterface for annotating spoken dialogue.
InAAAI Spring Symposium on Empirical Meth-ods in Discourse Interpretation and Generation,pages pages 40{46, Stanford, CA.Giovanni Flammia.
1998.
Discourse segmenta-tion of spoken dialogue: an empirical approach.Doctoral dissertation, Department of Electricaland Computer Science, Massachusetts Instituteof Technology.Barbara J. Grosz and Candace L. Sidner.
1986.Attention, intentions, and the structure of dis-course.
Computational Linguistics, 12(3):175{204.Peter A. Heeman and James Allen.
1995a.
Dia-logue transcription tools.
Trains Technical Note94-1, Department of Computer Science, Univer-sity of Rochester, March.
Revised.Peter A. Heeman and James F. Allen.
1995b.
TheTrains spoken dialog corpus.
CD-ROM, Lin-guistics Data Consortium, April.Peter A. Heeman and James F. Allen.
1999.Speech repairs, intonational phrases and dis-course markers: Modeling speakers' utterancesin spoken dialog.
Computational Linguistics,25(4):527{572.Arne Jonsson and Nils Dahlback.
2000.
Distill-ing dialogues | a method using natural di-alogue corpora for dialogue systems develop-ment.
In Proceedings of the 6th Applied NaturalLanguage Processing Conference, pages 44{51,Seattle.Michael Kipp.
2001.
Anvil: A generic annotationtool for multimodal dialogue.
In Proceedings ofthe 7th European Conference on Speech Com-munication and Technology (Eurospeech).Per Linell.
1998.
Approaching Dialogue: Talk,Interaction and Contexts in Dialogical Perspec-tives.
John Benjamins Publishing.David McKelvie, Amy Isard, Andreas Mengel,Morten Baun Muller, Michael Grosse, and Mar-ion Klein.
2001.
The MATE workbench | anannotation tool for XML coded speech corpora.Speech Communications, 33:97{112.John F. Pitrelli, Mary E. Beckman, and Ju-lia Hirschberg.
1994.
Evaluation of prosodictranscription labeling reliability in the ToBIframework.
In Proceedings of the 3rd Interna-tional Conference on Spoken Language Process-ing (ICSLP-94), Yokohama, September.Deborah Schirin.
1987.
Discourse Markers.Cambridge University Press, New York.Susan E. Strayer and Peter A. Heeman.
2001.Dialogue structure and mixed initiative.
InSecond workshop of the Special Interest Groupon Dialogue, pages 153{161, Aalborg Denmark,September.Stephen Sutton, Ed Kaiser, Andrew Cronk, andRonald Cole.
1997.
Bringing spoken languagesystems to the classroom.
In Proceedings of the5th European Conference on Speech Commu-nication and Technology (Eurospeech), Rhodes,Greece.S.
Sutton, R. Cole, J. de Villiers, J. Schalkwyk,P.
Vermeulen, M. Macon, Y. Yan, E. Kaiser,R.
Rundle, K. Shobaki, P. Hosom, A. Kain,J.
Wouters, M. Massaro, and M. Cohen.
1998.Universal speech tools: the cslu toolkit.
In Pro-ceedings of the 5th International Conference onSpoken Language Processing (ICSLP-98), pages3221{3224, Sydney Australia, November.David R. Traum and Christine H. Nakatani.
1999.A two-level approach to coding dialogue for dis-course structure: Activities of the 1998 workinggroup on higher-level structures.
In Proceed-ings of the ACL'99 Workshop Towards Stan-dards and Tools for Discourse Tagging, pages101{108, June.Fan Yang, Susan E. Strayer, and Peter A. Hee-man.
2002.
ACT: a graphical dialogue anno-tation comparison tool.
Submitted for publica-tion.
