Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 325?334,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsFine-grained Tree-to-String Translation Rule ExtractionXianchao Wu?
Takuya Matsuzaki?
Jun?ichi Tsujii???
?Department of Computer Science, The University of Tokyo7-3-1 Hongo, Bunkyo-ku, Tokyo 113-0033, Japan?School of Computer Science, University of Manchester?National Centre for Text Mining (NaCTeM)Manchester Interdisciplinary Biocentre, 131 Princess Street, Manchester M1 7DN, UK{wxc, matuzaki, tsujii}@is.s.u-tokyo.ac.jpAbstractTree-to-string translation rules are widelyused in linguistically syntax-based statis-tical machine translation systems.
In thispaper, we propose to use deep syntac-tic information for obtaining fine-grainedtranslation rules.
A head-driven phrasestructure grammar (HPSG) parser is usedto obtain the deep syntactic information,which includes a fine-grained descriptionof the syntactic property and a semanticrepresentation of a sentence.
We extractfine-grained rules from aligned HPSGtree/forest-string pairs and use them inour tree-to-string and string-to-tree sys-tems.
Extensive experiments on large-scale bidirectional Japanese-English trans-lations testified the effectiveness of our ap-proach.1 IntroductionTree-to-string translation rules are generic and ap-plicable to numerous linguistically syntax-basedStatistical Machine Translation (SMT) systems,such as string-to-tree translation (Galley et al,2004; Galley et al, 2006; Chiang et al, 2009),tree-to-string translation (Liu et al, 2006; Huanget al, 2006), and forest-to-string translation (Mi etal., 2008; Mi and Huang, 2008).
The algorithmsproposed by Galley et al (2004; 2006) are fre-quently used for extracting minimal and composedrules from aligned 1-best tree-string pairs.
Deal-ing with the parse error problem and rule sparse-ness problem, Mi and Huang (2008) replaced the1-best parse tree with a packed forest which com-pactly encodes exponentially many parses for tree-to-string rule extraction.However, current tree-to-string rules only makeuse of Probabilistic Context-Free Grammar treefragments, in which part-of-speech (POS) orkoroshita korosareta(active) (passive)VBN(killed) 6 (6/10,6/6) 4 (4/10,4/4)VBN(killed:active) 5 (5/6,5/6) 1 (1/6,1/4)VBN(killed:passive) 1 (1/4,1/6) 3 (3/4,3/4)Table 1: Bidirectional translation probabilities ofrules, denoted in the brackets, change when voiceis attached to ?killed?.phrasal tags are used as the tree node labels.
Aswill be testified by our experiments, we argue thatthe simple POS/phrasal tags are too coarse to re-flect the accurate translation probabilities of thetranslation rules.For example, as shown in Table 1, sup-pose a simple tree fragment ?VBN(killed)?
ap-pears 6 times with ?koroshita?, which is aJapanese translation of an active form of ?killed?,and 4 times with ?korosareta?, which is aJapanese translation of a passive form of ?killed?.Then, without larger tree fragments, we willmore frequently translate ?VBN(killed)?
into ?ko-roshita?
(with a probability of 0.6).
But,?VBN(killed)?
is indeed separable into two fine-grained tree fragments of ?VBN(killed:active)?and ?VBN(killed:passive)?1.
Consequently,?VBN(killed:active)?
appears 5 times with ?ko-roshita?
and 1 time with ?korosareta?
; and?VBN(killed:passive)?
appears 1 time with ?ko-roshita?
and 3 times with ?korosareta?.
Now, byattaching the voice information to ?killed?, we aregaining a rule set that is more appropriate to reflectthe real translation situations.This motivates our proposal of using deep syn-tactic information to obtain a fine-grained trans-lation rule set.
We name the information such asthe voice of a verb in a tree fragment as deep syn-tactic information.
We use a head-driven phrasestructure grammar (HPSG) parser to obtain the1For example, ?John has killed Mary.?
versus ?John waskilled by Mary.
?325deep syntactic information of an English sentence,which includes a fine-grained description of thesyntactic property and a semantic representationof the sentence.
We extract fine-grained trans-lation rules from aligned HPSG tree/forest-stringpairs.
We localize an HPSG tree/forest to makeit segmentable at any nodes to fit the extractionalgorithms described in (Galley et al, 2006; Miand Huang, 2008).
We also propose a linear-timealgorithm for extracting composed rules guidedby predicate-argument structures.
The effective-ness of the rules are testified in our tree-to-stringand string-to-tree systems, taking bidirectionalJapanese-English translations as our test cases.This paper is organized as follows.
In Section 2,we briefly review the tree-to-string and string-to-tree translation frameworks, tree-to-string rule ex-traction algorithms, and rich syntactic informationpreviously used for SMT.
The HPSG grammar andour proposal of fine-grained rule extraction algo-rithms are described in Section 3.
Section 4 givesthe experiments for applying fine-grained transla-tion rules to large-scale Japanese-English transla-tion tasks.
Finally, we conclude in Section 5.2 Related Work2.1 Tree-to-string and string-to-treetranslationsTree-to-string translation (Liu et al, 2006; Huanget al, 2006) first uses a parser to parse a sourcesentence into a 1-best tree and then searches forthe best derivation that segments and converts thetree into a target string.
In contrast, string-to-treetranslation (Galley et al, 2004; Galley et al, 2006;Chiang et al, 2009) is like bilingual parsing.
Thatis, giving a (bilingual) translation grammar and asource sentence, we are trying to construct a parseforest in the target language.
Consequently, thetranslation results can be collected from the leavesof the parse forest.Figure 1 illustrates the training and decodingprocesses of bidirectional Japanese-English trans-lations.
The English sentence is ?John killedMary?
and the Japanese sentence is ?jyon ha mariwo koroshita?, in which the function words ?ha?and ?wo?
are not aligned with any English word.2.2 Tree/forest-based rule extractionGalley et al (2004) proposed the GHKM algo-rithm for extracting (minimal) tree-to-string trans-lation rules from a tuple of ?F,Et, A?, where F =x0 ?
x1x0 x1x1 ?
x0NPJohn??
?Vkilled ??
?NPMary ??
?NPV NPVPSJohn killed Mary???
?
???
?
??
?NP VPSV NPVPx0 x1TrainingAligned tree-string pair:ExtractrulesJohn killed Mary???
?
???
?
??
?CKY decodingTestingNP V NPVPSJohn killed MaryNPVPV NPApplyrules?
?jyon   ha    mari  wo  koroshitaparsingBottom-updecodingtree-to-string string-to-treeFigure 1: Illustration of the training and decod-ing processes for tree-to-string and string-to-treetranslations.fJ1 is a sentence of a foreign language other thanEnglish,Et is a 1-best parse tree of an English sen-tence E = eI1, and A = {(j, i)} is an alignmentbetween the words in F and E.The basic idea of GHKM algorithm is to de-compose Et into a series of tree fragments, eachof which will form a rule with its correspondingtranslation in the foreign language.
A is used as aconstraint to guide the segmentation procedure, sothat the root node of every tree fragment of Et ex-actly corresponds to a contiguous span on the for-eign language side.
Based on this consideration, afrontier set (fs) is defined to be a set of nodes n inEt that satisfies the following constraint:fs = {n|span(n) ?
comp span(n) = ?}.
(1)Here, span(n) is defined by the indices of the firstand last word in F that are reachable from a noden, and comp span(n) is defined to be the comple-ment set of span(n), i.e., the union of the spansof all nodes n?
in Et that are neither descendantsnor ancestors of n. span(n) and comp span(n)of each n can be computed by first a bottom-upexploration and then a top-down traversal of Et.By restricting each fragment so that it only takes326JohnCAT       NPOS      NNPBASE    johnLEXENTRY [D<N.3sg>]_lxmPRED  noun_arg0t0HEAD           t0SEM_HEAD t0CAT           NXXCATc2killedCAT     VPOS     VBDBASE   killLEXENTRY [NP.nom<V.bse> NP.acc]_lxm-past_verb_rulePRED  verb_arg12TENSE     pastASPECT   noneVOICE      activeAUX          minusARG1       c1ARG2       c5t1HEAD           t1SEM_HEAD t1CAT           VXXCATc4HEAD           c6SEM_HEAD c6CAT              NPXCATSCHEMA empty_spec_headc5HEAD           t2SEM_HEAD t2CAT          NXXCATc6HEAD           c3SEM_HEAD c3CAT              SXCATSCHEMA subj_headc0HEAD           c2SEM_HEAD c2CAT              NPXCATSCHEMA empty_spec_headc1HEAD           c4SEM_HEAD c4CAT              VPXCATSCHEMA  head_compc3MaryCAT       NPOS      NNPBASE    maryLEXENTRY[D<N.3sg>]_lxmPRED    noun_arg0t2???
?
???
?
???1.
c0(x0:c1, x1:c3)  x0 ?
x12.
c1(x0:c2)  x03.
c2(t0)  ???4.
c3(x0:c4, x1:c5)  x1 ?
x05.
c4(t1)  ???6.
c5(x0:c6)  x07.
c6(t2)  ??
?c0c1 c3c4 c5t1minimumcovering treex0 ?
x1 ?
??
?An HPSG-tree based minimal rule set A PAS-based composed ruleJohn killed MaryHEAD           c8SEM_HEAD c8CAT              SXCATSCHEMA head_modc7HEAD           c9SEM_HEAD c9CAT              SXCATSCHEMA  subj_headc8killedCAT     VPOS     VBDBASE   killLEXENTRY [NP.nom<V.bse>]_lxm-past_verb_rulePRED  verb_arg1TENSE     pastASPECT   noneVOICE      activeAUX          minusARG1       c1t3HEAD           t3SEM_HEAD t3CAT           VPXCATc9HEAD           c11SEM_HEAD c11CAT              NPXCATSCHEMA empty_spec_headc10HEAD           t4SEM_HEAD t4CAT          NXXCATc11MaryCAT       NPOS      NNPBASE    maryLEXENTRYV[D<N.3sg>]PRED    noun_arg0t42.77 4.520.81 2.2500.00-3.47 -0.030-2.82-0.07 -0.001Figure 2: Illustration of an aligned HPSG forest-string pair.
The forest includes two parse trees by taking?Mary?
as a modifier (t3, t4) or an argument (t1, t2) of ?killed?.
Arrows with broken lines denote the PASdependencies from the terminal node t1 to its argument nodes (c1 and c5).
The scores of the hyperedgesare attached to the forest as well.the nodes in fs as the root and leaf nodes, a well-formed fragmentation of Et is generated.
Withfs computed, rules are extracted through a depth-first traversal of Et: we cut Et at all nodes in fsto form tree fragments and extract a rule for eachfragment.
These extracted rules are calledminimalrules (Galley et al, 2004).
For example, the 1-best tree (with gray nodes) in Figure 2 is cut into 7pieces, each of which corresponds to the tree frag-ment in a rule (bottom-left corner of the figure).In order to include richer context informationand account for multiple interpretations of un-aligned words of foreign language, minimal ruleswhich share adjacent tree fragments are connectedtogether to form composed rules (Galley et al,2006).
For each aligned tree-string pair, Gal-ley et al (2006) constructed a derivation-forest,in which composed rules were generated, un-aligned words of foreign language were consis-tently attached, and the translation probabilitiesof rules were estimated by using Expectation-Maximization (EM) (Dempster et al, 1977) train-ing.
For example, by combining the minimal rulesof 1, 4, and 5, we obtain a composed rule, asshown in the bottom-right corner of Figure 2.Considering the parse error problem in the1-best or k-best parse trees, Mi and Huang(2008) extracted tree-to-string translation rulesfrom aligned packed forest-string pairs.
A for-est compactly encodes exponentially many trees327rather than the 1-best tree used by Galley et al(2004; 2006).
Two problems were managed tobe tackled during extracting rules from an alignedforest-string pair: where to cut and how to cut.Equation 1 was used again to compute a frontiernode set to determine where to cut the packedforest into a number of tree-fragments.
The dif-ference with tree-based rule extraction is that thenodes in a packed forest (which is a hypergraph)now are hypernodes, which can take a set of in-coming hyperedges.
Then, by limiting each frag-ment to be a tree and whose root/leaf hypernodesall appearing in the frontier set, the packed forestcan be segmented properly into a set of tree frag-ments, each of which can be used to generate atree-to-string translation rule.2.3 Rich syntactic information for SMTBefore describing our approaches of applyingdeep syntactic information yielded by an HPSGparser for fine-grained rule extraction, we wouldlike to briefly review what kinds of deep syntacticinformation have been employed for SMT.Two kinds of supertags, from Lexicalized Tree-Adjoining Grammar and Combinatory CategorialGrammar (CCG), have been used as lexical syn-tactic descriptions (Hassan et al, 2007) for phrase-based SMT (Koehn et al, 2007).
By introduc-ing supertags into the target language side, i.e.,the target language model and the target sideof the phrase table, significant improvement wasachieved for Arabic-to-English translation.
Birchet al (2007) also reported a significant improve-ment for Dutch-English translation by applyingCCG supertags at a word level to a factorized SMTsystem (Koehn et al, 2007).In this paper, we also make use of supertagson the English language side.
In an HPSGparse tree, these lexical syntactic descriptionsare included in the LEXENTRY feature (re-fer to Table 2) of a lexical node (Matsuzakiet al, 2007).
For example, the LEXEN-TRY feature of ?t1:killed?
takes the value of[NP.nom<V.bse>NP.acc]_lxm-past_verb_rule in Figure 2.
In which,[NP.nom<V.bse>NP.acc] is an HPSGstyle supertag, which tells us that the base formof ?killed?
needs a nominative NP in the left handside and an accessorial NP in the right hand side.The major differences are that, we use a largerfeature set (Table 2) including the supertags forfine-grained tree-to-string rule extraction, ratherthan string-to-string translation (Hassan et al,2007; Birch et al, 2007).The Logon project2 (Oepen et al, 2007) forNorwegian-English translation integrates in-depthgrammatical analysis of Norwegian (using lexi-cal functional grammar, similar to (Riezler andMaxwell, 2006)) with semantic representations inthe minimal recursion semantics framework, andfully grammar-based generation for English usingHPSG.
A hybrid (of rule-based and data-driven)architecture with a semantic transfer backbone istaken as the vantage point of this project.
Incontrast, the fine-grained tree-to-string translationrule extraction approaches in this paper are to-tally data-driven, and easily applicable to numer-ous language pairs by taking English as the sourceor target language.3 Fine-grained rule extractionWe now introduce the deep syntactic informa-tion generated by an HPSG parser and then de-scribe our approaches for fine-grained tree-to-string rule extraction.
Especially, we localize anHPSG tree/forest to fit the extraction algorithmsdescribed in (Galley et al, 2006; Mi and Huang,2008).
Also, we propose a linear-time com-posed rule extraction algorithm by making use ofpredicate-argument structures.3.1 Deep syntactic information by HPSGparsingHead-driven phrase structure grammar (HPSG) isa lexicalist grammar framework.
In HPSG, lin-guistic entities such as words and phrases are rep-resented by a data structure called a sign.
A signgives a factored representation of the syntactic fea-tures of a word/phrase, as well as a representationof their semantic content.
Phrases and words rep-resented by signs are composed into larger phrasesby applications of schemata.
The semantic rep-resentation of the new phrase is calculated at thesame time.
As such, an HPSG parse tree/forestcan be considered as a tree/forest of signs (c.f.
theHPSG forest in Figure 2).An HPSG parse tree/forest has two attractiveproperties as a representation of an English sen-tence in syntax-based SMT.
First, we can carefullycontrol the condition of the application of a trans-lation rule by exploiting the fine-grained syntactic2http://www.emmtee.net/328Feature DescriptionCAT phrasal categoryXCAT fine-grained phrasal categorySCHEMA name of the schema applied in the nodeHEAD pointer to the head daughterSEM HEAD pointer to the semantic head daughterCAT syntactic categoryPOS Penn Treebank-style part-of-speech tagBASE base formTENSE tense of a verb (past, present, untensed)ASPECT aspect of a verb (none, perfect,progressive, perfect-progressive)VOICE voice of a verb (passive, active)AUX auxiliary verb or not (minus, modal,have, be, do, to, copular)LEXENTRY lexical entry, with supertags embeddedPRED type of a predicateARG?x?
pointer to semantic arguments, x = 1..4Table 2: Syntactic/semantic features extractedfrom HPSG signs that are included in the outputof Enju.
Features in phrasal nodes (top) and lexi-cal nodes (bottom) are listed separately.description in the English parse tree/forest, as wellas those in the translation rules.
Second, we canidentify sub-trees in a parse tree/forest that cor-respond to basic units of the semantics, namelysub-trees covering a predicate and its arguments,by using the semantic representation given in thesigns.
We expect that extraction of translationrules based on such semantically-connected sub-trees will give a compact and effective set of trans-lation rules.A sign in the HPSG tree/forest is represented bya typed feature structure (TFS) (Carpenter, 1992).A TFS is a directed-acyclic graph (DAG) whereinthe edges are labeled with feature names and thenodes (feature values) are typed.
In the originalHPSG formalism, the types are defined in a hierar-chy and the DAG can have arbitrary shape (e.g., itcan be of any depth).
We however use a simplifiedform of TFS, for simplicity of the algorithms.
Inthe simplified form, a TFS is converted to a (flat)set of pairs of feature names and their values.
Ta-ble 2 lists the features used in this paper, whichare a subset of those in the original output from anHPSG parser, Enju3.
The HPSG forest shown inFigure 2 is in this simplified format.
An impor-tant detail is that we allow a feature value to be apointer to another (simplified) TFS.
Such pointer-valued features are necessary for denoting the se-mantics, as explained shortly.In the Enju English HPSG grammar (Miyao et3http://www-tsujii.is.s.u-tokyo.ac.jp/enju/index.htmlSheignorefactwantIdisputeARG1ARG2ARG1 ARG1ARG2ARG2JohnkillMary ARG2ARG1Figure 3: Predicate argument structures for thesentences of ?John killed Mary?
and ?She ignoredthe fact that I wanted to dispute?.al., 2003) used in this paper, the semantic contentof a sentence/phrase is represented by a predicate-argument structure (PAS).
Figure 3 shows the PASof the example sentence in Figure 2, ?John killedMary?, and a more complex PAS for another sen-tence, ?She ignored the fact that I wanted to dis-pute?, which is adopted from (Miyao et al, 2003).In an HPSG tree/forest, each leaf node generallyintroduces a predicate, which is represented bythe pair of LEXENTRY (lexical entry) feature andPRED (predicate type) feature.
The arguments ofa predicate are designated by the pointers from theARG?x?
features in a leaf node to non-terminalnodes.3.2 Localize HPSG forestOur fine-grained translation rule extraction algo-rithm is sketched in Algorithm 1.
Considering thata parse tree is a trivial packed forest, we only usethe term forest to expand our discussion, hereafter.Recall that there are pointer-valued features in theTFSs (Table 2) which prevent arbitrary segmenta-tion of a packed forest.
Hence, we have to localizean HPSG forest.For example, there are ARG pointers from t1 toc1 and c5 in the HPSG forest of Figure 2.
How-ever, the three nodes are not included in one (min-imal) translation rule.
This problem is causedby not considering the predicate argument depen-dency among t1, c1, and c5 while performing theGHKM algorithm.
We can combine several min-imal rules (Galley et al, 2006) together to ad-dress this dependency.
Yet we have a faster wayto tackle PASs, as will be described in the nextsubsection.Even if we omit ARG, there are still two kindsof pointer-valued features in TFSs, HEAD andSEM HEAD.
Localizing these pointer-valued fea-tures is straightforward, since during parsing, theHEAD and SEM HEAD of a node are automati-cally transferred to its mother node.
That is, thesyntactic and semantic head of a node only take329Algorithm 1 Fine-grained rule extractionInput: HPSG tree/forest Ef , foreign sentence F , and align-ment AOutput: a PAS-based rule set R1 and/or a tree-rule set R21: if Ef is an HPSG tree then2: E?f = localize Tree(Ef )3: R1 = PASR extraction(E?f , F , A) ?
Algorithm 24: E?
?f = ignore PAS(E?f )5: R2 = TR extraction(E?
?f , F , A) ?
composed rule ex-traction algorithm in (Galley et al, 2006)6: else if Ef is an HPSG forest then7: E?f = localize Forest(Ef );8: R2 = forest based rule extraction(E?f , F , A) ?
Algo-rithm 1 in (Mi and Huang, 2008)9: end ifthe identifier of the daughter node as the values.For example, HEAD and SEM HEAD of node c0take the identical value to be c3 in Figure 2.To extract tree-to-string rules from the treestructures of an HPSG forest, our solution is topre-process an HPSG forest in the following way:?
for a phrasal hypernode, replace its HEADand SEM HEAD value with L, R, or S,which respectively represent left daughter,right daughter, or single daughter (line 2 and7); and,?
for a lexical node, ARG?x?
and PRED fea-tures are ignored (line 4).A pure syntactic-based HPSG forest without anypointer-valued features can be yielded through thispre-processing for the consequent execution of theextraction algorithms (Galley et al, 2006; Mi andHuang, 2008).3.3 Predicate-argument structuresIn order to extract translation rules from PASs,we want to localize a predicate word and its ar-guments into one tree fragment.
For example, inFigure 2, we can use a tree fragment which takesc0 as its root node and c1, t1, and c5 on its yield (=leaf nodes of a tree fragment) to cover ?killed?
andits subject and direct object arguments.
We definethis kind of tree fragment to be a minimum cov-ering tree.
For example, the minimum coveringtree of {t1, c1, c5} is shown in the bottom-rightcorner of Figure 2.
The definition supplies us alinear-time algorithm to directly find the tree frag-ment that covers a PAS during both rule extractingand rule matching when decoding an HPSG tree.Algorithm 2 PASR extractionInput: HPSG tree Et, foreign sentence F , and alignment AOutput: a PAS-based rule set R1: R = {}2: for node n ?
Leaves(Et) do3: if Open(n.ARG) then4: Tc = MinimumCoveringTree(Et, n, n.ARGs)5: if root and leaf nodes of Tc are in fs then6: generate a rule r using fragment Tc7: R.append(r)8: end if9: end if10: end forSee (Wu, 2010) for more examples of minimumcovering trees.Taking a minimum covering tree as the treefragment, we can easily build a tree-to-stringtranslation rule that reflects the semantic depen-dency of a PAS.
The algorithm of PAS-basedrule (PASR) extraction is sketched in Algorithm2.
Suppose we are given a tuple of ?F,Et, A?.Et is pre-processed by replacing HEAD andSEM HEAD to be L, R, or S, and computing thespan and comp span of each node.We extract PAS-based rules through one-timetraversal of the leaf nodes in Et (line 2).
For eachleaf node n, we extract a minimum covering treeTc if n contains at least one argument.
That is, atleast one ARG?x?
takes the value of some nodeidentifier, where x ranges 1 over 4 (line 3).
Then,we require the root and yield nodes of Tc being inthe frontier set of Et (line 5).
Based on Tc, we caneasily build a tree-to-string translation rule by fur-ther completing the right-hand-side string by sort-ing the spans of Tc?s leaf nodes, lexicalizing theterminal node?s span(s), and assigning a variableto each non-terminal node?s span.
Maximum like-lihood estimation is used to calculate the transla-tion probabilities of each rule.An example of PAS-based rule is shown in thebottom-right corner of Figure 2.
In the rule, thesubject and direct-object of ?killed?
are general-ized into two variables, x0 and x1.4 Experiments4.1 Translation modelsWe use a tree-to-string model and a string-to-treemodel for bidirectional Japanese-English transla-tions.
Both models use a phrase translation table(PTT), an HPSG tree-based rule set (TRS), anda PAS-based rule set (PRS).
Since the three rulesets are independently extracted and estimated, we330use Minimum Error Rate Training (MERT) (Och,2003) to tune the weights of the features from thethree rule sets on the development set.Given a 1-best (localized) HPSG tree Et, thetree-to-string decoder searches for the optimalderivation d?
that transforms Et into a Japanesestring among the set of all possible derivations D:d?
=argmaxd?D{?1 log pLM (?
(d)) + ?2|?
(d)|+ log s(d|Et)}.
(2)Here, the first item is the language model (LM)probability where ?
(d) is the target string ofderivation d; the second item is the translationlength penalty; and the third item is the transla-tion score, which is decomposed into a product offeature values of rules:s(d|Et) =?r?df(r?PTT )f(r?TRS)f(r?PRS).This equation reflects that the translation rules inone d come from three sets.
Inspired by (Liu etal., 2009b), it is appealing to combine these rulesets together in one decoder because PTT providesexcellent rule coverages while TRS and PRS offerlinguistically motivated phrase selections and non-local reorderings.
Each f(r) is in turn a product offive features:f(r) = p(s|t)?3 ?
p(t|s)?4 ?
l(s|t)?5 ?
l(t|s)?6 ?
e?7 .Here, s/t represent the source/target part of a rulein PTT, TRS, or PRS; p(?|?)
and l(?|?)
are transla-tion probabilities and lexical weights of rules fromPTT, TRS, and PRS.
The derivation length penaltyis controlled by ?7.In our string-to-tree model, for efficient decod-ing with integrated n-gram LM, we follow (Zhanget al, 2006) and inversely binarize all translationrules into Chomsky Normal Forms that containat most two variables and can be incrementallyscored by LM.
In order to make use of the bina-rized rules in the CKY decoding, we add two kindsof glues rules:S ?
Xm(1), Xm(1);S ?
S(1)Xm(2), S(1)Xm(2).Here Xm ranges over the nonterminals appearingin a binarized rule set.
These glue rules can beseen as an extension from X to {Xm}of the twoglue rules described in (Chiang, 2007).The string-to-tree decoder searches for the op-timal derivation d?
that parses a Japanese stringF into a packed forest of the set of all possiblederivations D:d?
=argmaxd?D{?1 log pLM (?
(d)) + ?2|?
(d)|+ ?3g(d) + log s(d|F )}.
(3)This formula differs from Equation 2 by replacingEt with F in s(d|?)
and adding g(d), which is thenumber of glue rules used in d. Further definitionsof s(d|F ) and f(r) are identical with those usedin Equation 2.4.2 Decoding algorithmsIn our translation models, we have made useof three kinds of translation rule sets which aretrained separately.
We perform derivation-levelcombination as described in (Liu et al, 2009b) formixing different types of translation rules withinone derivation.For tree-to-string translation, we use a bottom-up beam search algorithm (Liu et al, 2006) fordecoding an HPSG tree Et.
We keep at most 10best derivations with distinct ?
(d)s at each node.Recall the definition of minimum covering tree,which supports a faster way to retrieve availablerules from PRS without generating all the sub-trees.
That is, when node n fortunately to be theroot of someminimum covering tree(s), we use thetree(s) to seek available PAS-based rules in PRS.We keep a hash-table with the key to be the nodeidentifier of n and the value to be a priority queueof available PAS-based rules.
The hash-table iseasy to be filled by one-time traversal of the termi-nal nodes in Et.
At each terminal node, we seekits minimum covering tree, retrieve PRS, and up-date the hash-table.
For example, suppose we aredecoding an HPSG tree (with gray nodes) shownin Figure 2.
At t1, we can extract its minimumcovering tree with the root node to be c0, then takethis tree fragment as the key to retrieve PRS, andconsequently put c0 and the available rules in thehash-table.
When decoding at c0, we can directlyaccess the hash-table looking for available PAS-based rules.In contrast, we use a CKY-style algorithm withbeam-pruning and cube-pruning (Chiang, 2007)to decode Japanese sentences.
For each Japanesesentence F , the output of the chart-parsing algo-rithm is expressed as a hypergraph representing aset of derivations.
Given such a hypergraph, we331Train Dev.
Test# of sentences 994K 2K 2K# of Jp words 28.2M 57.4K 57.1K# of En words 24.7M 50.3K 49.9KTable 3: Statistics of the JST corpus.use the Algorithm 3 described in (Huang and Chi-ang, 2005) to extract its k-best (k = 500 in ourexperiments) derivations.
Since different deriva-tions may lead to the same target language string,we further adopt Algorithm 3?s modification, i.e.,keep a hash-table to maintain the unique targetsentences (Huang et al, 2006), to efficiently gen-erate the unique k-best translations.4.3 SetupsThe JST Japanese-English paper abstract corpus4,which consists of one million parallel sentences,was used for training and testing.
This corpuswas constructed from a Japanese-English paperabstract corpus by using the method of Utiyamaand Isahara (2007).
Table 3 shows the statisticsof this corpus.
Making use of Enju 2.3.1, we suc-cessfully parsed 987,401 English sentences in thetraining set, with a parse rate of 99.3%.
We mod-ified this parser to output a packed forest for eachEnglish sentence.We executed GIZA++ (Och and Ney, 2003) andgrow-diag-final-and balancing strategy (Koehn etal., 2007) on the training set to obtain a phrase-aligned parallel corpus, from which bidirectionalphrase translation tables were estimated.
SRI Lan-guage Modeling Toolkit (Stolcke, 2002) was em-ployed to train 5-gram English and Japanese LMson the training set.
We evaluated the translationquality using the case-insensitive BLEU-4 metric(Papineni et al, 2002).
The MERT toolkit we usedis Z-mert5 (Zaidan, 2009).The baseline system for comparison is Joshua(Li et al, 2009), a freely available decoder for hi-erarchical phrase-based SMT (Chiang, 2005).
Werespectively extracted 4.5M and 5.3M translationrules from the training set for the 4K English andJapanese sentences in the development and testsets.
We used the default configuration of Joshua,expect setting the maximum number of items/rulesand the k of k-best outputs to be the identical4http://www.jst.go.jp.
The corpus can be conditionallyobtained from NTCIR-7 patent translation workshop home-page: http://research.nii.ac.jp/ntcir/permission/ntcir-7/perm-en-PATMT.html.5http://www.cs.jhu.edu/ ozaidan/zmert/PRS CS3 C3 FS Ftree nodes TFS POS TFS POS TFS# rules 0.9 62.1 83.9 92.5 103.7# tree types 0.4 23.5 34.7 40.6 45.2extract time 3.5 - 98.6 - 121.2Table 4: Statistics of several kinds of tree-to-stringrules.
Here, the number is in million level and thetime is in hour.200 for English-to-Japanese translation and 500for Japanese-to-English translation.We used four dual core Xeon machines(4?3.0GHz?2CPU, 4?64GB memory) to run allthe experiments.4.4 ResultsTable 4 illustrates the statistics of several transla-tion rule sets, which are classified by:?
using TFSs or simple POS/phrasal tags (an-notated by a superscript S) to represent treenodes;?
composed rules (PRS) extracted from thePAS of 1-best HPSG trees;?
composed rules (C3), extracted from the treestructures of 1-best HPSG trees, and 3 is themaximum number of internal nodes in thetree fragments; and?
forest-based rules (F ), where the packedforests are pre-pruned by the marginalprobability-based inside-outside algorithmused in (Mi and Huang, 2008).Table 5 reports the BLEU-4 scores achieved bydecoding the test set making use of Joshua and oursystems (t2s = tree-to-string and s2t = string-to-tree) under numerous rule sets.
We analyze thistable in terms of several aspects to prove the effec-tiveness of deep syntactic information for SMT.Let?s first look at the performance of TFSs.
Wetake CS3 and FS as approximations of CFG-basedtranslation rules.
Comparing the BLEU-4 scoresof PTT+CS3 and PTT+C3, we gained 0.56 (t2s)and 0.57 (s2t) BLEU-4 points which are signifi-cant improvements (p < 0.05).
Furthermore, wegained 0.50 (t2s) and 0.62 (s2t) BLEU-4 pointsfrom PTT+FS to PTT+F , which are also signif-icant improvements (p < 0.05).
The rich fea-tures included in TFSs contribute to these im-provements.332Systems BLEU-t2s Decoding BLEU-s2tJoshua 21.79 0.486 19.73PTT 18.40 0.013 17.21PTT+PRS 22.12 0.031 19.33PTT+CS3 23.56 2.686 20.59PTT+C3 24.12 2.753 21.16PTT+C3+PRS 24.13 2.930 21.20PTT+FS 24.25 3.241 22.05PTT+F 24.75 3.470 22.67Table 5: BLEU-4 scores (%) achieved by Joshuaand our systems under numerous rule configura-tions.
The decoding time (seconds per sentence)of tree-to-string translation is listed as well.Also, BLEU-4 scores were inspiringly in-creased 3.72 (t2s) and 2.12 (s2t) points by append-ing PRS to PTT, comparing PTT with PTT+PRS.Furthermore, in Table 5, the decoding time (sec-onds per sentence) of tree-to-string translation byusing PTT+PRS is more than 86 times faster thanusing the other tree-to-string rule sets.
This sug-gests that the direct generation of minimum cover-ing trees for rule matching is extremely faster thangenerating all subtrees of a tree node.
Note thatPTT performed extremely bad compared with allother systems or tree-based rule sets.
The majorreason is that we did not perform any reorderingor distorting during decoding with PTT.However, in both t2s and s2t systems, theBLEU-4 score benefits of PRS were covered bythe composed rules: both PTT+CS3 and PTT+C3performed significant better (p < 0.01) thanPTT+PRS, and there are no significant differenceswhen appending PRS to PTT+C3.
The reason isobvious: PRS is only a small subset of the com-posed rules, and the probabilities of rules in PRSwere estimated by maximum likelihood, which isfast but biased compared with EM based estima-tion (Galley et al, 2006).Finally, by using PTT+F , our systems achievedthe best BLEU-4 scores of 24.75% (t2s) and22.67% (s2t), both are significantly better (p <0.01) than that achieved by Joshua.5 ConclusionWe have proposed approaches of using deep syn-tactic information for extracting fine-grained tree-to-string translation rules from aligned HPSGforest-string pairs.
The main contributions are theapplications of GHKM-related algorithms (Galleyet al, 2006; Mi and Huang, 2008) to HPSG forestsand a linear-time algorithm for extracting com-posed rules from predicate-argument structures.We applied our fine-grained translation rules to atree-to-string system and an Hiero-style string-to-tree system.
Extensive experiments on large-scalebidirectional Japanese-English translations testi-fied the significant improvements on BLEU score.We argue the fine-grained translation rules aregeneric and applicable to many syntax-based SMTframeworks such as the forest-to-string model (Miet al, 2008).
Furthermore, it will be interestingto extract fine-grained tree-to-tree translation rulesby integrating deep syntactic information in thesource and/or target language side(s).
These tree-to-tree rules are applicable for forest-to-tree trans-lation models (Liu et al, 2009a).AcknowledgmentsThis work was partially supported by Grant-in-Aid for Specially Promoted Research (MEXT,Japan) and Japanese/Chinese Machine TranslationProject in Special Coordination Funds for Pro-moting Science and Technology (MEXT, Japan),and Microsoft Research Asia Machine TranslationTheme.
The first author thanks Naoaki Okazakiand Yusuke Miyao for their help and the anony-mous reviewers for improving the earlier version.ReferencesAlexandra Birch, Miles Osborne, and Philipp Koehn.2007.
Ccg supertags in factored statistical machinetranslation.
In Proceedings of the Second Work-shop on Statistical Machine Translation, pages 9?16, June.Bob Carpenter.
1992.
The Logic of Typed FeatureStructures.
Cambridge University Press.David Chiang, Kevin Knight, and Wei Wang.
2009.11,001 new features for statistical machine transla-tion.
In Proceedings of HLT-NAACL, pages 218?226, June.David Chiang.
2005.
A hierarchical phrase-basedmodel for statistical machine translation.
In Pro-ceedings of ACL, pages 263?270, Ann Arbor, MI.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Lingustics, 33(2):201?228.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.Maximum likelihood from incomplete data via theem algorithm.
Journal of the Royal Statistical Soci-ety, 39:1?38.Michel Galley, Mark Hopkins, Kevin Knight, andDaniel Marcu.
2004.
What?s in a translation rule?In Proceedings of HLT-NAACL.333Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Pro-ceedings of COLING-ACL, pages 961?968, Sydney.Hany Hassan, Khalil Sima?an, and Andy Way.
2007.Supertagged phrase-based statistical machine trans-lation.
In Proceedings of ACL, pages 288?295, June.Liang Huang and David Chiang.
2005.
Better k-bestparsing.
In Proceedings of IWPT.Liang Huang, Kevin Knight, and Aravind Joshi.
2006.Statistical syntax-directed translation with extendeddomain of locality.
In Proceedings of 7th AMTA.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondr?ej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InProceedings of the ACL 2007 Demo and Poster Ses-sions, pages 177?180.Zhifei Li, Chris Callison-Burch, Chris Dyery, JuriGanitkevitch, Sanjeev Khudanpur, Lane Schwartz,Wren N. G. Thornton, Jonathan Weese, and Omar F.Zaidan.
2009.
Demonstration of joshua: An opensource toolkit for parsing-based machine translation.In Proceedings of the ACL-IJCNLP 2009 SoftwareDemonstrations, pages 25?28, August.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment templates for statistical machinetransaltion.
In Proceedings of COLING-ACL, pages609?616, Sydney, Australia.Yang Liu, Yajuan Lu?, and Qun Liu.
2009a.
Improvingtree-to-tree translation with packed forests.
In Pro-ceedings of ACL-IJCNLP, pages 558?566, August.Yang Liu, Haitao Mi, Yang Feng, and Qun Liu.
2009b.Joint decoding with multiple translation models.
InProceedings of ACL-IJCNLP, pages 576?584, Au-gust.Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.2007.
Efficient hpsg parsing with supertagging andcfg-filtering.
In Proceedings of IJCAI, pages 1671?1676, January.Haitao Mi and Liang Huang.
2008.
Forest-based trans-lation rule extraction.
In Proceedings of the 2008Conference on Empirical Methods in Natural Lan-guage Processing, pages 206?214, October.Haitao Mi, Liang Huang, and Qun Liu.
2008.
Forest-based translation.
In Proceedings of ACL-08:HLT,pages 192?199, Columbus, Ohio.Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsu-jii.
2003.
Probabilistic modeling of argument struc-tures including non-local dependencies.
In Proceed-ings of the International Conference on Recent Ad-vances in Natural Language Processing, pages 285?291, Borovets.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofACL, pages 160?167.Stephan Oepen, Erik Velldal, Jan Tore L?nning, PaulMeurer, and Victoria Rose?n.
2007.
Towards hy-brid quality-oriented machine translation - on lin-guistics and probabilities in mt.
In Proceedingsof the 11th International Conference on Theoreticaland Methodological Issues in Machine Translation(TMI-07), September.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automaticevaluation of machine translation.
In Proceedingsof ACL, pages 311?318.Stefan Riezler and John T. Maxwell, III.
2006.
Gram-matical machine translation.
In Proceedings of HLT-NAACL, pages 248?255.Andreas Stolcke.
2002.
Srilm-an extensible languagemodeling toolkit.
In Proceedings of InternationalConference on Spoken Language Processing, pages901?904.Masao Utiyama and Hitoshi Isahara.
2007.
Ajapanese-english patent parallel corpus.
In Proceed-ings of MT Summit XI, pages 475?482, Copenhagen.Xianchao Wu.
2010.
Statistical Machine Transla-tion Using Large-Scale Lexicon and Deep SyntacticStructures.
Ph.D. dissertation.
Department of Com-puter Science, The University of Tokyo.Omar F. Zaidan.
2009.
Z-MERT: A fully configurableopen source tool for minimum error rate training ofmachine translation systems.
The Prague Bulletin ofMathematical Linguistics, 91:79?88.Hao Zhang, Liang Huang, Daniel Gildea, and KevinKnight.
2006.
Synchronous binarization for ma-chine translation.
In Proceedings of HLT-NAACL,pages 256?263, June.334
