Adaptive Multilingual Sentence BoundaryDisambiguationDav id  D. Palmer*The MITRE Corporat ionMart i  A. HearstXerox PARCThe sentence is a standard textual unit in natural anguage processing applications.
In manylanguages the punctuation mark that indicates the end-of-sentence boundary is ambiguous; thusthe tokenizers of most NLP systems must be equipped with special sentence boundary recognitionrules for every new text collection.As an alternative, this article presents an efficient, trainable system for sentence boundarydisambiguation.
The system, called Satz, makes imple estimates of the parts of speech of thetokens immediately preceding and following each punctuation mark, and uses these stimates asinput to a machine learning algorithm that then classifies the punctuation mark.
Satz is veryfast both in training and sentence analysis, and its combined robustness and accuracy surpassexisting techniques.
The system needs only a small lexicon and training corpus, and has beenshown to transfer quickly and easily from English to other languages, as demonstrated onFrenchand German.1.
IntroductionRecent years have seen a dramatic increase in the availabil ity of on-l ine text collections,which are useful in many  areas of computat ional  l inguistics research.
One active areaof research is the deve lopment  of a lgor i thms for al igning sentences in parallel corpora.The success of most  natural  language processing (NLP) algorithms, including multi-l ingual sentence al ignment algor i thms (Kay and R6scheisen 1993; Gale and Church1993), 1 part-of-speech taggers (Cutting et al 1991), and parsers, depends  on pr iorknowledge of the location of sentence boundaries.Segment ing a text into sentences is a nontrivial  task, however,  since in Engl ishand many other languages the end-of-sentence punctuat ion marks are ambiguous.
2A period, for example, can denote a decimal point, an abbreviation, the end of asentence, or even an abbreviat ion at the end of a sentence.
Exclamation points andquest ion marks can occur within quotat ion marks or parentheses as well as at the endof a sentence.
Ellipsis, a series of per iods (..
.)
,  can occur both within sentences and at* 202 Burlington Road, Bedford, MA 01730.
E-maih palmer@mitre.org.
Some of the work reported herewas done while the author was at the University of California, Berkeley.
The views and opinions inthis paper are those of the authors and do not reflect he MITRE Corporation's current work position.t 3333 Coyote Hill Rd., Palo Alto, CA 94304.
E-marl: hearst@parc.xerox.com1 There is some recent research in aligning bilingual corpora without relying on sentence boundaries(Fung and Church 1994; Fung and McKeown 1994).2 In this article, we will consider only the period, the exclamation point, and the question mark to bepossible "end-of-sentence punctuation marks," and all references to "ptmctuation marks" will refer tothese three.
Although the colon, the semicolon, and conceivably the comma can also delimitgrammatical sentences, their usage is beyond the scope of this work.
(~) 1997 Association for Computational LinguisticsComputational Linguistics Volume 23, Number 2sentence boundaries.
The ambiguity of these punctuation marks is illustrated in thefollowing difficult cases:(1) The group included Dr. J. M. Freeman and T. Boone Pickens Jr.(2) "This issue crosses party lines and crosses philosophical lines!"
said Rep. JohnRowland (R., Conn.).
(3) Somit entsprach ein ECU am 17.
9.
1984 0.73016 US$ (vgl.
Tab.
1).
(4) Crdd au ddbut des ann~es 60 .
.
.
.
.
.
par un gouvernement conservateur : .
.
.
cetOffice s'~tait vu accorder six ans ...The existence of punctuation i  grammatical subsentences suggests the possibilityof a further decomposition of the sentence boundary problem into types of sentenceboundaries, one of which would be "embedded sentence boundary."
Such a distinctionmight be useful for certain applications that analyze the grammatical structure ofthe sentence.
However, in this work we will only address the simpler problem ofdetermining boundaries between sentences, finding that which Nunberg (1990) callsthe "text-sentence.
"In examples (1-4), the word immediately preceding and the word immediatelyfollowing a punctuation mark provide important information about its role in thesentence.
However, more context may be necessary, such as when punctuation occursin a subsentence within quotation marks or parentheses, as seen in example (2), orwhen an abbreviation appears at the end of a sentence, as seen in (5a):(5)a.
It was due Friday by 5 p.m. Saturday would be too late.(5)b.
She has an appointment a 5 p.m. Saturday to get her car fixed.Examples (5a-b) also show some problems inherent in relying on brittle features,such as capitalization, when determining sentence boundaries.
The initial capital inSaturday does not necessarily indicate that Saturday is the first word in the sentence.As a more dramatic example, some important kinds of text consist only of upper-caseletters, thus thwarting any system that relies on capitalization rules.
Another obstacleto systems that rely on brittle features is that many texts are not well-formed.
Onesuch class of texts are those that are the output of optical character recognition (OCR);typically these texts contain many extraneous or incorrect characters.This article presents an efficient, trainable system for sentence boundary disam-biguation that circumvents these obstacles.
The system, called Satz, makes simpleestimates of the parts of speech of the tokens immediately preceding and followingeach punctuation mark, and uses these estimates as input to a machine learning al-gorithm that determines whether the punctuation mark is a sentence boundary orserves another purpose in the sentence.
Satz is very fast in both training and sentenceanalysis; training is accomplished in less than one minute on a workstation, and it canprocess 10,000 sentences per minute.
The combined robustness and accuracy of thesystem surpasses existing techniques, consistently producing an error rate less than1.5% on a range of corpora and languages.
It requires only a small lexicon (which canbe less than 5,000 words) and a training corpus of 300-500 sentences.The following sections discuss related work and the criteria used to evaluate suchwork, describe our system in detail, and present he results of applying the system toa variety of texts.
The transferability of the system from English to other languages ialso demonstrated on French and German text.
Finally, the learning-based system is242Palmer and Hearst Multilingual Sentence Boundaryshown to be able to improve the results of a more conventional system on especiallydifficult cases.2.
Related Work2.1 Evaluation of Related WorkAn important consideration when discussing related work is the mode of evaluation.To aid our evaluation, we define a lower bound, an objective score which any reason-able algorithm should be able to match or better.
In our test collections, the ambiguouspunctuation mark is used much more often as a sentence boundary marker than forany other purpose.
Therefore, a very simple, successful algorithm is one in which ev-ery potential boundary marker is labeled as the end-of-sentence.
Thus, for the task ofsentence boundary disambiguation, we define the lower bound of a text collection asthe percentage of possible sentence-ending punctuation marks in the text that indeeddenote sentence boundaries.Since the use of abbreviations in a text depends on the particular text and textgenre, the number of ambiguous punctuation marks, and the corresponding lowerbound, will vary dramatically depending on text genre.
For example, Liberman andChurch (1992) report on a Wall Street Journal corpus containing 14,153 periods permillion tokens, whereas in the Tagged Brown corpus (Francis and Kucera 1982), thefigure is only 10,910 periods per million tokens.
Liberman and Church also report hat47% of the periods in the WSJ corpus denote abbreviations (thus a lower bound of53%), compared to only 10% in the Brown corpus (lower bound 90%) (Riley 1989).
Incontrast, Mfiller, Amerl, and Natalis (1980) reports lower bound statistics ranging from54.7% to 92.8% within a corpus of scientific abstracts.
Such a range of lower boundfigures uggests the need for a robust approach that can adapt rapidly to different textrequirements.Another useful evaluation technique is the comparison of a new algorithm againsta strong baseline algorithm.
The baseline algorithm should perform better than thelower bound and should represent a strong effort or a standard method for solvingthe problem at hand.Although sentence boundary disambiguation is an essential preprocessing step ofmany natural language processing systems, it is a topic rarely addressed in the litera-ture and there are few public-domain programs for performing the segmentation task.For our studies we compared our system against he results of the UNIX STYLE pro-gram (Cherry and Vesterman 1991).
3The STYLE program, which attempts to providea stylistic profile of writing at the word and sentence level, reports the length andstructure for all sentences in a document, hereby indicating the sentence boundaries.STYLE defines a sentence as a string of words ending in one of: period, exclamationpoint, question mark, or backslash-period (the latter of which can be used by an au-thor to mark an imperative sentence nding).
The program handles numbers withembedded ecimal points and commas and makes use of an abbreviation list with 48entries.
It also uses the following heuristic: initials cause a sentence break only if thenext word begins with a capital etter and is found in a dictionary of function words.In an evaluation on a sample of 20 documents, the developers of the program foundit to incorrectly classify sentence boundaries 204 times out of 3287 possible (an errorrate of 6.3%).3 Comparison against the STYLE program was suggested to us by Mickey Chandrasekar.243Computational Linguistics Volume 23, Number 22.2 Regular Expressions and Heuristic RulesThe method currently widely used for determining sentence boundaries i  a regu-lar grammar, usually with limited lookahead.
In the simplest implementation f thismethod, the grammar ules attempt to find patterns of characters, uch as "period-space-capital letter," which usually occur at the end of a sentence.
More elaborateimplementations, such as the STYLE program discussed above, consider the entireword preceding and following the punctuation mark and include extensive word listsand exception lists to attempt to recognize abbreviations and proper nouns.
There area few examples of rule-based and heuristic systems for which performance numbersare available, discussed in the remainder of this subsection.The Alembic information extraction system (Aberdeen et al 1995) contains a veryextensive regular-expression-based sentence boundary disambiguation module, cre-ated using the lexical scanner generator Flex (Nicol 1993).
The boundary disambigua-tion module is part of a comprehensive preprocess pipeline that utilizes a list of 75abbreviations and a series of over 100 hand-crafted rules to identify sentence bound-aries, as well as titles, date and time expressions, and abbreviations.
The sentenceboundary module was developed over the course of more than six staff months.
Onthe Wall Street Journal corpus described in Section 4, Alembic achieved an error rateof 0.9%.Christiane Hoffmann (1994) used a regular expression approach to classify punctu-ation marks in a corpus of the German ewspaper die tageszeitung with a lower bound(as defined above) of 92%.
She used the UNIX tool LEX (Lesk and Schmidt 1975)and a large abbreviation list to classify occurrences ofperiods.
Her method incorrectlyclassified less than 2% of the sentence boundaries when tested on 2,827 periods fromthe corpus.
The method was developed specifically for the tageszeitung corpus, andHoffmann reports that success in applying her method to other corpora would be de-pendent on the quality of the available abbreviation lists.
Her work would thereforeprobably not be easily transportable to other corpora or languages.Mark Wasson and colleagues invested nine staff months developing a system thatrecognizes special tokens (e.g., nondictionary terms uch as proper names, legal statutecitations, etc.)
as well as sentence boundaries.
From this, Wasson built a stand-aloneboundary recognizer in the form of a grammar converted into finite automata with1,419 states and 18,002 transitions (excluding the lexicon).
The resulting system, whentested on 20 megabytes of news and case law text, achieved an error rate of 0.3% atspeeds of 80,000 characters per CPU second on a mainframe computer.
When testedagainst upper-case legal text the system still performed very well, achieving errorrates of 0.3% and 1.8% on test data of 5,305 and 9,396 punctuation marks, respectively.According to Wasson, it is not likely, however, that the results would be this strongon lower-case-only data.
4Although the regular grammar approach can be successful, it requires a largemanual effort to compile the individual rules used to recognize the sentence bound-aries.
Such efforts are usually developed specifically for a text corpus (Liberman andChurch 1992; Hoffmann 1994) and would probably not be portable to other text gen-res.
Because of their reliance on special anguage-specific word lists, they are also notportable to other natural anguages without repeating the effort of compiling exten-sive lists and rewriting rules.
In addition, heuristic approaches depend on having a4 This work has not been published.
All information about this system iscourtesy ofa personalcommunication with Mark Wasson.
Wasson's reported processing time cannot be compared directly tothe other systems since it was obtained from a mainframe computer and was estimated in terms ofcharacters rather than sentences.244Palmer and Hearst Multilingual Sentence Boundarywell-behaved corpus with regular punctuation and few extraneous characters, andthey would probably not be very successful with texts obtained via optical characterrecognition (OCR).Miiller, Amerl, and Natalis (1980) provides an exhaustive analysis of sentenceboundary disambiguation as it relates to lexical endings and the identification of ab-breviations and words surrounding a punctuation mark, focusing on text written inEnglish.
This approach makes multiple passes through the data to find recognizablesuffixes and thereby filters out words that are not likely to be abbreviations.
The mor-phological analysis makes it possible to identify words not otherwise present in theextensive word lists used to identify abbreviations.
Error rates of 2-5% are reportedfor this method tested on over 75,000 scientific abstracts, with lower bounds rangingfrom 54.7% to 92.8%.2.3 Approaches Using Machine LearningThere have been two other published attempts to apply machine-learning techniquesto the sentence boundary disambiguation task.
Both make use of the words in thecontext found around the punctuation mark.2.3.1 Regression Trees.
Riley (1989) describes an approach that uses regression trees(Breiman et al 1984) to classify periods according to the following features:?
Probability\[word preceding "."
occurs at end of sentence\]?
Probability\[word following "."
occurs at beginning of sentence\]?
Length of word preceding ....?
Length of word after "."?
Case of word preceding ".
': Upper, Lower, Cap, Numbers?
Case of word following ".
": Upper, Lower, Cap, Numbers?
Punctuation after "."
(if any)?
Abbreviation class of words with ".
"The method uses information about one word of context on either side of thepunctuation mark and thus must record, for every word in the lexicon, the probabilitythat it occurs next to a sentence boundary.
Probabilities were compiled from 25 millionwords of prelabeled training data from a corpus of AP newswire.
The probabilitieswere actually estimated for the beginning and end of paragraphs rather than for allsentences, ince paragraph boundaries were explicitly marked in the AP corpus, whilethe sentence boundaries were not.
The resulting classification tree was used to identifywhether a word ending in a period is at the end of a declarative sentence in the Browncorpus, and achieved an error rate of 0.2%.
5 Although this is an impressive rror rate,the amount of training data (25 million words) required is prohibitive for a problemthat acts as a preprocessing step to other natural anguage processing tasks; it wouldbe impractical to expect this amount of data to be available for every corpus andlanguage to be tagged.5 Time for training was not reported, nor was the amount of the Brown corpus against which testingwas performed; we assume the entire Brown corpus was used.
Furthermore, no estimates of scalabilitywere given, so we are unable to report results with a smaller set.245Computational Linguistics Volume 23, Number 22.3.2 Feed-forward Neural Networks.
Humphrey and Zhou (1989) report using a feed-forward neural network to disambiguate periods, and achieve an error rate averaging7%.
They use a regular grammar to tokenize the text before training the neural nets,but no further details of their approach are available.
62.4 Our ApproachEach of the approaches described above has disadvantages to overcome.
In the follow-ing sections we present an approach that avoids the problems of previous approaches,yielding a very low error rate and behaving more robustly than solutions that requiremanually designed rules.
We present results of testing our system on several corporain three languages: English, German, and French.3.
The Satz SystemThis section describes the structure of our adaptive sentence boundary disambiguationsystem, known as Satz.
7 The Satz system represents the context surrounding a punc-tuation mark as a sequence of vectors, where the vector constructed for each contextword represents an estimate of the part-of-speech distribution for the word, obtainedfrom a lexicon containing part-of-speech frequency data.
This use of part-of-speechestimates of the context words, rather than the words themselves, is a unique aspectof the Satz system, and is responsible in large part for its efficiency and effectiveness.The context vectors, which we call descriptor arrays, are input to a machine learn-ing algorithm trained to disambiguate s ntence boundaries.
The output of the learningalgorithm is then used to determine the role of the punctuation mark in the sentence.The architecture of the system is shown in Figure 1.
The Satz system works in twomodes--learning mode and disambiguation mode.
In learning mode, the input textis a training text with all sentence boundaries manually labeled, and the parametersin the learning algorithm are dynamically adjusted during training.
Once learningmode is completed, the parameters in the learning algorithm remain fixed.
Trainingof the learning algorithm is therefore necessary only once for each language, althoughtraining can be repeated for a new corpus or genre within a language, if desired.
Indisambiguation mode, the input is the text whose sentence boundaries have not beenmarked up yet and need to be disambiguated.The essence of the Satz system lies in how machine learning is used, rather thanin which particular method is used.
In this article we report results using two differentlearning methods: neural networks and decision trees.
The two methods are almostequally effective for this task, and both train and run quickly using small resources.For some applications, one may be more appropriate than another, (e.g., the scoresproduced by a neural net may be useful for another processing step in a naturallanguage program), so we do not consider either learning algorithm to be the "correct"one to use.
Therefore, when we refer to the Satz system, we refer to the use of machinelearning with a small training corpus, representing the word context surrounding eachpunctuation mark in terms of estimates of the parts of speech of those words, wherethese estimates are derived from a very small lexicon.6 Results were obtained courtesy of a personal communication with Joe Zhou.7 "Satz" is the German word for "sentence.
"246Palmer and Hearst Multilingual Sentence BoundaryFigure 1The Satz architecture.Input TextTokenizationPart-of-speech LookupDescriptor array constructionClassification by learning algorithmText withsentence boundaries disambiguated3.1 TokenizationThe first stage of the process is lexical analysis, which breaks the input text (a streamof characters) into tokens.
The Satz tokenizer is implemented using the UNIX toolLEX (Lesk and Schmidt 1975) and is modeled on the tokenizer used by the PARTSpart-of-speech tagger (Church 1988).
The tokens returned by the LEX program canbe a sequence of alphabetic haracters, a sequence of digits, 8 or a sequence of one ormore non-alphanumeric characters such as periods or quotation marks.3.2 Part-of-Speech LookupThe individual tokens are next assigned a series of possible parts of speech, based ona lexicon and simple heuristics described below.3.2.1 Representing Context.
The context surrounding a punctuation mark can be rep-resented in various ways.
The most straightforward is to use the individual wordspreceding and following the punctuation mark, as in this example:at the plant.
He had thoughtUsing this approach, a representation of an individual word's position in a contextmust be made for every word in the language.
Compiling these representations foreach word is undesirable due to the large amount of training data, training time, andstorage overhead required, especially since it is unlikely that such information will beuseful to later stages of processing.As an alternative, the context could be approximated by using a single part ofspeech for each word.
The above context would then be represented by the followingpart-of-speech sequence:preposition article nounpronoun verb verbHowever, requiring a single part-of-speech assignment for each word introduces aprocessing circularity: because most part-of-speech taggers require predetermined sen-tence boundaries, the boundary disambiguation must be done before tagging.
But if8 Numbers containing periods acting as decimal points are considered a single token.
This eliminatesone possible ambiguity of the period at the lexical analysis tage.247Computational Linguistics Volume 23, Number 2the disambiguation is done before tagging, no part-of-speech assignments are avail-able for the boundary-determination system.
To avoid this circularity, we approximateeach word's part of speech in one of two ways: (1) by the prior probabilities of allparts of speech for that word, or (2) by a binary value for each possible part of speechfor that word.In the case of prior probabilities, each word in the context is represented by theprobability that the word occurs as each part of speech, with all part-of-speech prob-abilities in the vector summing to 1.0.
Continuing the example, the context becomes(and for simplicity, suppressing the parts of speech with value 0.0):preposition(1.0) article(1.0) noun(O.8)/verb(0.2)pronoun(1.0) verb(1.0) noun(O.1)/verb(0.9)This denotes that at and the have a probability of 1.0 of occurring as a prepositionand article respectively, plant has a probability of 0.8 of occurring as a noun and aprobability of 0.2 of occurring as a verb, and so on.
These probabilities, which aremore accurately "scaled frequencies," are based on occurrences of the words in apretagged corpus, and are therefore corpus dependent.
9In the case of binary part-of-speech assignment, for each possible part of speech,the vector is assigned the value 1 if the word can ever occur as that part of speech(according to the lexicon), and the value 0 if it cannot.
In this case the sum of all itemsin the vector is not predefined, as it is with probabilities.
Continuing the example withbinary POS vectors (and, for simplicity, suppressing the parts of speech with value 0),the context becomes:preposition(I) article(I) noun(1)/verb(1)pronoun(I) verb(l) noun(1)/verb(1)The part-of-speech data necessary to construct probabilistic and binary vectorsis often present in the lexicon of a part-of-speech tagger or other existing NLP tool,or it can easily be obtained from word lists; the data would thus be readily avail-able and would not require excessive storage overhead.
It is also possible to estimatepart-of-speech data for new or unknown words.
For these reasons, we chose to ap-proximate the context in our system by using the prior part-of-speech information.
InSection 4.7 we give the results of a comparative study of system performance withboth probabilistic and binary part-of-speech vectors.3.2.2 The Lexicon.
An important component of the Satz system is the lexicon contain-ing part-of-speech frequency data from which the descriptor arrays are constructed.Words in the lexicon are followed by a series of part-of-speech tags and associatedfrequencies, representing the possible parts of speech for that word and the frequencywith which the word occurs as each part of speech.
The frequency information canbe obtained in various ways, as discussed in the previous ection.
The lexical lookupstage of the Satz system finds a word in the lexicon (if it is present) and returns thepossible parts of speech.
For the English word well, for example, the lookup modulemight return the tagsJJ/15 NN/18 QL/68 RB/634 UH/22 VB/59 The frequencies can be obtained from an existing corpus tagged manually or automatically; the corpusdoes not need to be tagged specifically for this task.248Palmer and Hearst Multilingual Sentence Boundaryindicating that, in the corpus on which the lexicon is based, the word well occurred15 times as an adjective, 18 as a singular noun, 68 as a qualifier, 634 as an adverb, 22as an interjection, and 5 as a singular verb) ?3.2.3 Heuristics for Unknown Words.
If a word is not present in the lexicon, the Satzsystem contains a set of heuristics that attempt o assign the most reasonable parts ofspeech to the word.
A summary of these heuristics is listed below.Unknown tokens containing a digit (0-9) are assumed to be numbers.Any token beginning with a period, exclamation point, or question markis assigned a "possible nd-of-sentence punctuation" tag.
This catchescommon sequences like "?!"
and "... ".Common morphological endings are recognized and the appropriatepart(s)-of-speech is assigned to the entire word.Words containing a hyphen are assigned a series of tags and frequenciesequally distributed between adjective, common noun, and proper noun.Words containing an internal period are assumed to be abbreviations.A capitalized word is not always a proper noun, even when it appearssomewhere other than in a sentence's initial position (e.g., the wordAmerican is often used as an adjective).
Those words not present in thelexicon are assigned a certain language-dependent probability (0.9 forEnglish) of being a proper noun, and the remainder is distributeduniformly among adjective, common noun, verb, and abbreviation, themost likely tags for unknown words, nCapitalized words appearing in the lexicon but not registered as propernouns can nevertheless till be proper nouns.
In addition to thepart-of-speech frequencies present in the lexicon, these words areassigned a certain probability of being a proper noun (0.5 for English)with the probabilities already assigned to that word redistributedproportionally in the remaining 0.5.
The proportion of words falling intothis category varies greatly depending on the style of the text and theuniformity of capitalization.As a last resort, the word is assigned the tags for common noun, verb,adjective, and abbreviation with a uniform frequency distribution.These heuristics can be easily modified and adapted to the specific needs of a newlanguage, 12although we obtained low error rates without changing the heuristics.3.3 Descriptor Array ConstructionA vector, or descriptor array, is constructed for each token in the input text.
The lexiconmay contain as many as several hundred very specific tags, which we first need to mapinto more general categories.
For example, the Brown corpus tags of present tense verb,10 In this example, the frequencies are derived from the Brown corpus (Francis and Kucera 1982).11 Note that in the case of binary vectors, all probabilities receive the value 1.12 For example, the probability of a capitalized word being a proper noun is higher in English than inGerman, where all nouns are also capitalized.249Computational Linguistics Volume 23, Number 2Figure 2nounarticleconjunctionprepositionnumberleft parenthesesnon-punctuation charactercolon or dashsentence-ending punctuationverbmodifierpronounproper nouncomma or semicolonright parenthesespossessiveabbreviationothersElements of the descriptor array assigned to each incoming token.past participle, and modal verb are all mapped into the more general "verb" category.
Theparts of speech returned by the lookup module are thus mapped into the 18 generalcategories given in Figure 2, and the frequencies for each category are summed.
Inthe case of a probabilistic vector described in Section 3.2.1, the 18 category frequenciesfor the word are then converted to probabilities by dividing the frequencies for eachby the total frequency for the word.
For a binary vector, all categories with a nonzerofrequency count are assigned a value of 1, and all others are assigned a value of0.
In addition to the 18 category frequencies, the descriptor array also contains twoadditional f ags that indicate if the word begins with a capital etter and if it follows apunctuation mark, for a total of 20 items in each descriptor array.
These last two flagsallow the system to include capitalization information when it is available withouthaving to require that this information be present.3.4 Classification by a Learning AlgorithmThe descriptor arrays representing the tokens in the context are used as the input toa machine learning algorithm.
To disambiguate a punctuation mark given a contextof k surrounding words (referred to in this article as k-context), a window of k + 1tokens and their descriptor arrays is maintained as the input text is read.
The firstk/2 and final k/2 tokens of this sequence represent the context in which the middletoken appears.
If the middle token is a potential end-of-sentence punctuation mark,the descriptor arrays for the context tokens are input to the learning algorithm and theoutput result indicates whether the punctuation mark serves as a sentence boundaryor not.
In learning mode, the descriptor arrays are used to train the parameters ofthe learning algorithm.
We investigated the effectiveness of two separate algorithms:(1) back-propagation training of neural networks, and (2) decision tree induction.
Thelearning algorithms are described in the next two sections, and the results obtainedwith the algorithms are presented in Section 4.3.4.1 Neural Network.
Artificial neural networks have been successfully applied formany years in speech recognition applications (Bourland and Morgan 1994; Lippmann1989), and more recently in NLP tasks such as word category prediction (Nakamuraet al 1990) and part-of-speech tagging (Schmid 1994).
Neural networks in the contextof machine learning provide a well-tested training algorithm (back-propagation) thathas achieved high success rates in pattern-recognition problems imilar to the problemposed by sentence boundary disambiguation (Hertz, Krogh, and Palmer 1991).For Satz, we used a fully-connected feed-forward neural network, as shown inFigure 3.
The network accepts k ?
20 input values, where k is the size of the contextand 20 is the number of elements in the descriptor array described in Section 3.3.
The250Palmer and Hearst Multilingual Sentence BoundaryDA DA.
?
?
DA DAOutput (0 < x < 1)Figure 3Neural network architecture (DA = descriptor array of 20 items).Input LayerHidden LayerOutput Layerinput layer is fully connected to a hidden layer consisting of j hidden units; the hiddenunits in turn feed into one output unit that indicates the results of the function.
In atraditional back-propagation network, the input to a node is the sum of the outputsof the nodes in the previous layer multiplied by the weights between the layers.
Thissum is then passed through a "squashing" function to produce a node output between0 and 1.
A commonly-used squashing function--due to its mathematical properties,which assist in network training--is the sigrnoidal function, given byf(hi) = ~ '1where hi is the node input and T is a constant o adjust the slope of the sigmoid.In the Satz system we use a sigrnoidal squashing function on all hidden nodes andthe single output node of the neural network.
The output of the network is thus a singlevalue between 0 and 1, and represents the strength of the evidence that a punctuationmark occurring in its context is indeed the end of a sentence.
Two adjustable sensitivitythresholds, to and tl, are used to classify the results of the disambiguation.
If theoutput is less than to, the punctuation mark is not a sentence boundary; if the outputis greater than or equal to tl, it is a sentence boundary.
Outputs which fall betweenthe thresholds cannot be disambiguated by the network (which may indicate that themark is inherently ambiguous) and are marked accordingly, so they can be treatedspecially in later processing.
13For example, the sentence alignment algorithm in Galeand Church (1993) allows a distinction between hard and soft boundaries, where softboundaries are movable by the alignment program.
In our case, punctuation marksremaining ambiguous after processing by Satz can be treated as soft boundaries whileunambiguous punctuation marks (as well as paragraph boundaries) can be treated ashard boundaries, thus allowing the alignment program greater flexibility.A neural network is trained by presenting it with input data paired with the de-sired output.
For Satz, the input is the context surrounding the punctuation mark to bedisambiguated, and the output is a score indicating how much evidence there is thatthe punctuation mark is acting as an end-of-sentence boundary.
The nodes are con-nected via links that have weights assigned to them, and if the network produces anincorrect score, the weights are adjusted using an algorithm called back-propagation(Hertz, Krogh, and Palmer 1991) so that the next time the same input is presentedto the network, the output should more closely match the desired score.
This train-ing procedure is often iterated many times in order to allow the weights to adjust13 When to ---- tl, no punctuation mark is left ambiguous.251Computational Linguistics Volume 23, Number 2appropriately, and the same input data is presented multiple times.
Each round ofpresenting the same input data is called an epoch; of course, it is desirable to requireas few training epochs and as little training data as possible.
If one trains the networktoo often on the same data, overfitting can occur, meaning that the weights becometoo closely aligned with the particular training data that has been presented to thenetwork, and so may not correspond well to new examples that will come later.
Forthis reason, training should be accompanied by cross-validation (Bourland and Mor-gan 1994), a check against a held-out set of data to be sure that the weights are nottoo closely tailored to the training text.
This will be described in more detail below.Training data for the neural network consist of two sets of text in which all sen-tence boundaries have been manually disambiguated.
The first text, the training text,contains 300-600 test cases, where a test case is an ambiguous punctuation mark.
Theweights of the neural network are trained on the training text using the standard back-propagation algorithm (Hertz, Krogh, and Palmer 1991).
The second set of texts usedin training is the cross-validation set, whose contents are separate from the trainingtext and which contains roughly half as many test cases as the training text.
Trainingof the weights is not performed on this text; the cross-validation text is instead usedto increase the generalization of the training, such that when the total training errorover the cross-validation text reaches a minimum, training is halted.
TM Testing is thenperformed on texts independent of the training and cross-validation texts.
We measurethe speed of training by the number of training epochs required to complete training,where an epoch is a single pass through all the training data.
Training times for allexperiments reported in this article were less than one minute and were obtained ona DEC Alpha 3000 workstation, unless otherwise noted.In Sections 4.1-4.9 we present results of testing the Satz system with a neuralnetwork, including investigations of the effects of varying network parameters suchas hidden layer size, threshold values, and amount of training data.3.4.2 Decision Tree.
Algorithms for decision tree induction (Quinlan 1986; Bahl etal.
1989) have been successfully applied to NLP problems uch as parsing (Resnik1993; Magerman 1995) and discourse analysis (Siegel and McKeown 1994; Soderlandand Lehnert 1994).
We tested the Satz system using the c4.5 (Quinlan 1993) decisiontree induction program as the learning algorithm and compared the results to thoseobtained previously with the neural network.
These results are discussed in Section4.10.The induction algorithm proceeds by e;caluating the information content of a seriesof binary attributes and iteratively building a tree from the attribute values, with theleaves of the decision tree being the values of the goal attributes.
At each step in thelearning procedure, the evolving tree is branched on the attribute that divides the dataitems with the highest gain in information.
Branches are added to the tree until thedecision tree can classify all items in the training set.
Overfitting is also possible indecision tree induction, resulting in a tree that can very accurately classify the trainingdata but may not be able to accurately classify new examples.
To reduce the effects ofoverfitting, the c4.5 learning algorithm prunes the tree after the entire decision tree hasbeen constructed.
It recursively examines each subtree to determine whether eplacingit with a leaf or a branch would reduce the number of errors.
This pruning producesa decision tree better able to classify data different from the training data.14 The training error is the least mean squares error, one-half the sum of the squares of all the errors,where the error of a particular item is the difference between the desired output  and the actual outputof the neural  net.252Palmer and Hearst Multilingual Sentence BoundaryTable 1Results of comparing context sizes.Context Size Training Epochs Testing Errors Error (%)4-context 1,731 1,424 5.2%6-context 218 409 1.5%8-context 831 877 3.2%Integrating the decision tree induction algorithm into the Satz system was simplya matter of defining the input attributes as the k descriptor arrays in the context,with a single goal attribute representing whether the punctuation mark is a sentenceboundary or not.
Training data for the induction of the decision tree were identical tothe training set used to train the neural network.4.
Experiments with English TextsWe first tested the Satz system using English texts from the Wall Street Journal portionof the ACL/DCI collection (Church and Liberman 1991).
We constructed a training textof 573 test cases and a cross-validation text of 258 test cases.
15 We then constructeda separate test text consisting of 27,294 test cases, with a lower bound of 75.0%.
Thebaseline system (UNIX STYLE) achieved an error rate of 8.3% on the sentence bound-aries in the test set.
The lexicon and thus the frequency counts used to calculate thedescriptor arrays were derived from the Brown corpus (Francis and Kucera 1982).
Ininitial experiments we used the extensive l xicon from the PARTS part-of-speech tag-ger (Church 1988), which contains 30,000 words.
We later experimented with a muchsmaller lexicon, and these results are discussed in Section 4.4.
In Sections 4.1-4.9 wedescribe the results of our experiments with the Satz system using the neural networkas the learning algorithm.
Section 4.10 describes results using decision tree induction.4.1 Context SizeIn order to determine how much context is necessary to accurately disambiguate s n-tence boundaries in a text, we varied the size of the context from which the neuralnetwork inputs were constructed and obtained the results in Table 1.
The number inthe Training Epochs column is the number of passes through, the training data requiredto learn the training set; the number in the Testing Errors cohnnn is the number oferrors on the 27,294 item test set the system made after training with the correspond-ing context size.
From these data we concluded that a 6-token context, 3 preceding thepunctuation mark and 3 following, produces the best results.4.2 Hidden UnitsThe number of hidden units in a neural network can affect its performance.
To de-termine the size of the hidden layer in the neural network that produced the lowestoutput error rate, we experimented with various hidden layer sizes and obtained theresults in Table 2.
From these data we concluded that the lowest error rate in this caseis possible using a neural network with two nodes in its hidden layer.15 Note that "constructing" a training, cross-validation, r test ext simply involves manuallydisambiguating the sentence boundaries by inserting a unique character sequence at the end of eachsentence.253Computational Linguistics Volume 23, Number 2bTable 2Results of comparing hidden layer sizes (6-context).# Hidden Units Training Epochs Testing Errors Error (%)1 623 721 2.6%2 216 409 1.5%3 239 435 1.6%4 350 1,343 4.9%4.3 Sources of ErrorsAs described in Sections 4.1 and 4.2, the best results were obtained with a context sizeof 6 tokens and a hidden layer with 2 units.
This configuration produced a total of 409errors out of 27,294 test cases, for an error rate of 1.5%.
These errors fall into two majorcategories: (i) false positive, i.e., a punctuation mark the method erroneously abeledas a sentence boundary, and (ii) false negative, i.e., an actual sentence boundary thatthe method id not label as such.
Table 3 contains a summary of these errors.These errors can be decomposed into the following groups:(37.6%) false positive at an abbreviation within a title or name, usuallybecause the word following the period exists in the lexicon with otherparts of speech (Mr. Gray, Col. North, Mr. Major, Dr. Carpenter, Mr.
Sharp).
(22.5%) false negative due to an abbreviation at the end of a sentence,most frequently Inc., Co., Corp., or U.S., which all occur within sentencesas well.
(11.0%) false positive or negative due to a sequence of charactersincluding a period and quotation marks, as this sequence can occur bothwithin and at the end of sentences.
(9.2%) false negative resulting from an abbreviation followed byquotation marks; related to the previous two types.
(9.8%) false positive or false negative resulting from presence of ellipsis(...), which can occur at the end of or within a sentence.
(9.9%) miscellaneous errors, including extraneous characters (dashes,asterisks, etc.
), ungrammatical sentences, misspellings, and parentheticalsentences.The first two items indicate that the system is having difficulty recognizing thefunction of abbreviations.
We attempted tocounter this by dividing the abbreviationsin the lexicon into two distinct categories, title abbreviations such as Mr. and Dr.,which almost never occur at the end of a sentence, and all other abbreviations.
Thisnew classification, however, significantly increased the training time and eliminatedonly 12 of the 409 errors (2.9%).The third and fourth items demonstrate he difficulty of distinguishing subsen-tences within a sentence.
This problem may be addressed by creating a new classifi-cation for punctuation marks, the "embedded end-of-sentence," as suggested in Sec-tion 1.
The fifth class of error may similarly be addressed by creating a new classifica-tion for ellipses, and then attempting todetermine the role of the ellipses independentof the sentence boundaries.254Palmer and Hearst Multilingual Sentence BoundaryTable 3Results of testing on 27,294mixed-case items; to = tl = 0.5,6-context, 2 hidden units.224 (54.8%) false positives185 (45.2%) false negatives409 total errors out of 27,294 test casesTable 4Results of comparing lexicon size (27,294 potential sentenceboundaries).Words in Lexicon Training Epochs Testing Errors Error (%)30,000 218 411 1.5%5,000 372 483 1.8%3,000 1,056 551 2.0%4.4 Lexicon SizeThe results in Sections 4.1-4.3 depended on a very large lexicon with more than 30,000words.
It is not always possible to obtain or build a large lexicon, so it is importantto understand the impact of a smaller lexicon on the training time and error rate ofthe system.
We altered the size of the English lexicon used in training and testingby removing large sections of the original exicon and obtained the results in Table 4.These data demonstrate that a larger lexicon provides faster training and a lower errorrate, although the performance with the smaller lexica was still almost as accurate.
Inthe experiments describecl in Sections 4.5--4.10, we used a 5,000 word lexicon.It is important to note, however, that in reducing the size of the lexicon as a whole,the number of abbreviations remained constant (at 206).
Recognizing abbreviationsgives important evidence as to the location of sentence boundaries, and reducing thenumber of abbreviations in the lexicon naturally reduces the accuracy of the system.Most existing boundary disambiguation systems, uch as the STYLE program, dependheavily on abbreviation lists and would be relatively ineffective without informationabout abbreviations.
However, the robustness of the Satz system allows it to stillproduce a relatively high accuracy without relying on extensive abbreviation lists.To demonstrate his robustness, we removed all abbreviations from the lexicon afterreducing it in size to 5,000 words.
The resulting Satz error rate was 4.9%, which wasstill significantly better than the STYLE baseline rror rate of 8.3%, which was obtainedwith a 48 entry abbreviation list.4.5 Single-Case ResultsA major advantage ofthe Satz approach to sentence boundary recognition is its robust-ness.
In contrast to many existing systems, which depend on brittle parameters suchas capitalization and spacing, Satz is able to adapt to texts that are not well-formed,such as single-case t xts.
The two descriptor array flags for capitalization, discussed inSection 3.3, allow the system to include capitalization i formation when it is available.When this information is not available, the system is nevertheless able to adapt andproduce a low error rate.
To demonstrate his robustness, we converted the training,cross-validation, and test texts used in previous testing to a lower-case-only format,255Computational Linguistics Volume 23, Number 2with no capital etters.
After retraining the neural network with the lower-case-onlytexts, the Satz system was able to correctly disambiguate all but 3.3% of the sentenceboundaries.
After converting the texts to an upper-case-only format, with all capitalletters, and retraining the network on the texts in this format, the system was able tocorrectly label all but 3.5%.
164.6 Results on OCR TextsA large and ever-increasing source of on-line texts is texts obtained via optical characterrecognition (OCR).
These texts require very robust processing methods, as they containa large number of extraneous and incorrect characters.
The robustness results of theSatz system in the absence of an abbreviation list and capitalization suggest hat itwould be well suited for processing OCR texts as well.
To test this, we prepareda small corpus of raw OCR data containing 1,157 punctuation marks.
The STYLEprogram produced an error rate of 11.7% over the OCR texts; the Satz system, usinga neural network trained on mixed-case WSJ texts, produced an error rate of 4.2%.In analyzing the sources of the errors produced by Satz over the raw OCR data, itwas clear that many errors came from areas of high noise in the texts, such as the linein example (6), which contains an extraneous question mark and three periods.
Theseareas probably represented charts or tables in the source text and would most likelyneed to be eliminated anyway, as it is doubtful any text-processing program wouldbe able to productively process them.
We therefore applied a simple filter to the rawOCR data to locate areas of high noise and remove them from the text.
In the resultingtext of 1,115 punctuation marks, the STYLE program had an error rate of 9.6% whilethe Satz system improved to 1.9%.
(6) e:)i.
i)'e;y',?
;.i#i TCE grades' are'(7) newsprint.
Furthermore, shoe presses have Using rock for granite rollTwo years ago we reported on advances inWhile the low error rate on OCR texts is encouraging, it should not be viewed asan absolute figure.
One problem with OCR texts is that periods in the original textmay be scanned as commas or dropped from the text completely.
Our system is unableto detect hese cases.
Similarly, the definition of a sentence boundary is not necessarilyabsolute, as large parts of texts may be incorrectly or incompletely scanned by theOCR program.
The resulting "sentences" may not correspond to those in the originaltext, as can be seen in example (7).
Such problems cause a low error rate to have lesssignificance in OCR texts than in more well-formed texts such as the WSJ corpus.4.7 Probabilistic vs. Binary InputsIn the discussion of methods of representing context in Section 3.2.1, we suggested twoways of approximating the part-of-speech distribution of a word, using prior prob-abilities and binary features.
The results reported in the previous sections were allobtained using the prior probabilities in the descriptor arrays for all tokens.
Our ex-periments in comparing probabilisfic inputs to binary feature inputs, given in Table 5,indicate that using binary feature inputs significantly improves the performance of thesystem on both mixed-case and single-case texts, as well as decreasing the training16 The difference in results with upper-case-only and lower-case-only formats can probably be attributedto the capitalization flags in the descriptor a rays, as these flags would always be on in one case andoff in the other.256Palmer and Hearst Multilingual Sentence BoundaryTable 5Results of comparing probabilistic to binary feature inputs (5,000 word lexicon, 27,294 Englishtest cases).Probabilistic BinaryTraining Testing Error (%) Training Testing Error (%)Epochs Errors Epochs ErrorsMixed case 368 483 1.8% 312 474 1.7%Lower case 182 890 3.3% 148 813 3.0%Upper case 542 956 3.5% 190 744 2.7%Table 6Results of varying the sensitivity thresholds (27,294 test cases, 6-context, 2 hidden units).Lower Upper False False Not Were % Not Error (%)Threshold Threshold Positive Negative Labeled Correct Labeled0.5 0.5 209 200 0 0 0.0 1.50.4 0.6 173 174 145 83 0.50 1.30.3 0.7 140 148 326 205 1.20 1.10.2 0.8 111 133 541 376 1.98 0.90.1 0.9 79 94 1,021 785 3.74 0.6times.
The lower error rate and faster training time suggest hat the simpler approachof using binary feature inputs to the neural network is better than the frequency-basedinputs previously used.4.8 ThresholdsAs described in Section 3.4.1, the output of the neural network (after passing throughthe sigmoidal squashing function) is used to determine the function of a punctuationmark based on its value relative to two sensitivity thresholds, with outputs that fallbetween the thresholds denoting that the function of the punctuation mark is stillambiguous.
These are shown in the Not Labeled column of Table 6, which gives theresults of a systematic experiment with the sensitivity thresholds.
As the thresholdswere moved from the initial values of 0.5 and 0.5, certain items that had been classi-fied as False Pos or False Neg fell between the thresholds and became Not Labeled.At the same time, however, items that had been correctly labeled also fell betweenthe thresholds, and these are shown in the Were Correct co lumn.
17 There is thus atradeoff: decreasing the error percentage by adjusting the thresholds also decreasesthe percentage of cases correctly labeled and increases the percentage of items leftambiguous.4.9 Amount of Training DataTo obtain the results in Sections 4.1-4.8, we used very small training and cross-validation sets of 573 and 258 items, respectively.
The training and cross-validationsets could thus be constructed in a few minutes, and the resulting system error ratewas very low.
To determine the system improvement with more training data, we17 Note that the number of items in the Were Correct column is a subset of those in the Not Labeledcolumn.257Computational Linguistics Volume 23, Number 2Table 7Results of varying size of training and cross-validation sets (19,034 testitems).Training Items Cross-validation Items Training Epochs Error (%)573 258 85 1.5%622 587 84 1.8%1,174 587 135 2.0%1,858 1,266 172 1.2%2,514 1,266 222 1.1%3,179 1,952 316 1.3%removed a portion of the test data and incrementally added it to the training andcross-validation sets.
We found that, after an initial increase in the error rate, whichcan probably be accounted for by the fact that the new training data came from adifferent part of the corpus, increasing the size of the training and cross-validationsets to 2,514/1,266 reduced the error percentage to 1.1%, as can be seen in Table 7.The trade-off or this decreased error rate is a longer training time (often more than10 minutes) as well as the extra time required to construct he larger sets.4.10 Decision TreesWe next compared the Satz system error rate obtained using the neural network withresults using a decision tree.
We were able to use some of the previous results, specifi-cally the optimality of a 6-context and the effectiveness of a smaller lexicon and binaryfeature vectors, to obtain a direct comparison with the neural net results.
We used thec4.5 decision tree induction program (Quinlan 1993) and a 5,000 word lexicon to pro-duce all decision tree results.4.10.1 Size of Training Set.
As we showed in Section 4.9, the size of the training setused for the neural network affected the overall system error rate.
The same was truewith the decision tree induction algorithm, as seen in Figure 4.
The lowest error rate(1.0%) was obtained with a training set of 6,373 items.4.10.2 Mixed-Case Results.
One advantage of decision tree induction is that the al-gorithm clearly indicates which of the input attributes are most important.
While the6-context descriptor arrays present 120 input attributes to the algorithm, c4.5 induceda decision tree utilizing only 10 of the attributes, when trained on the same mixed-caseWSJ text used to train the neural network.
The 10 attributes for mixed-case Englishtexts, as seen in the induced decision tree in Figure 5, are (where t - 1 is the tokenpreceding the punctuation mark, t + 1 is the token following, and so on):t - 1 can be an abbreviationt + 1 is a comma or semicolont + 1 can be a sentence-ending punctuation markt + 1 can be a pronount + 1 is capitalizedt + 1 can be a conjunction258Palmer and Hearst Multilingual Sentence Boundary400 i i i i i i i380360g 34o~ 320*6 300i 280*6 260240220200180 i i i I i i i1000 2000 3000 4000 5000 6000 7000 8000size of training setFigure 4Results of increasing training set size for decision tree induction (19,034 item test set).t + 1 can be a proper nount + 2 can be a nount - 3 can be a modifiert - 3 can be a proper nounThe decision tree created from the small training set of 622 items resulted in anerror rate of 1.6%.
This result was slightly higher than the lowest error rate (1.5%)obtained with the neural network trained with a similar training set and a 5,000 wordlexicon.
The lowest error rate obtained using a larger training set to induce the decisiontree (1.0%), however, is better than the lowest error rate (1.1%) for the neural networktrained on a larger set.4.10.3 Single-Case Results.
Running the induction algorithm on upper-case-only andlower-case-only texts both produced the same decision tree, shown in Figure 6.
Aninteresting feature of this tree is that it reduced the 120 input attributes to just 4important ones.
Note the similarity of this tree to the algorithm used by the STYLEprogram as discussed in Section 2.Trained on 622 items, this tree produced 527 errors over the 27,294 item test set,an error rate of 1.9%, for both upper-case-only and lower-case-only texts.
This errorrate is lower than the best result for the neural network (3.3%) on single-case texts,despite the small size of the training set used.5.
Adaptation to Other LanguagesSince the disambiguation component of the sentence boundary recognition system, thelearning algorithm, is language independent, the Satz system can be easily adapted tonatural languages with punctuation systems imilar to English.
Adaptation to other259Computational Linguistics Volume 23, Number 2t-3 t-2 t-1 .
t+l t+2 t+3?
t-1 can be an abbreviation?t+l can be a pronoun?
t+l can be a sent-endpunctuation?yes ot+l is 0 t+t is1 capitalized?
, or ;?Y ~  yes / " " ~ n  ot+l can be a 0 0 1conjunction?yes /~~no1 t+l can be aproper noun?0 t+2 can be a noun?yes f /~no1 t-3 can be amodifier?.0 t-3 can be aproper noun?yes ~~no1 0Figure 5Decision tree induced for mixed-case English texts.
Leaf nodes labeled with 1 indicate that thepunctuation mark is determined to be a sentence boundary.languages involves obtaining (or building) a small lexicon containing the necessarypart-of-speech data and constructing small training and cross-validation texts.
We havesuccessfully adapted the Satz system to German and French, and the results are de-scribed below.5.1 GermanThe German lexicon was built from a series of public-domain word lists obtainedfrom the Consortium for Lexical Research.
In the resulting lexicon of 17,000 Germanadjectives, verbs, prepositions, articles, and 156 abbreviations, each word was assignedonly the parts of speech for the lists from which it came, with a frequency of I for eachpart of speech.
As the lack of actual frequency data in the lexicon made constructionof a probabilistic descriptor array impossible, we performed all German experimentsusing binary vectors.
The part-of-speech tags used were identical to those from theEnglish lexicon, and the descriptor array mapping remained unchanged.
This lexiconwas used in testing with two separate corpora.
The total development time required to260Palmer and Hearst Mulffiingual Sentence Boundaryt-3 t-2 t-1 .
t+l t+2 t+3t -1 can be an abbreviation?t+l can be a pronoun?
t+l can be a sent-endpunctuation?1 0 0 t+l is, or ;?0 1Figure 6Decision tree induced for single-case English texts.adapt Satz to German, including building the lexicon and constructing training texts,was less than one day.
We tested the system with two separate German corpora.5.1.1 Siiddeutsche Zeitung Corpus.
The Siiddeutsche Zeitung corpus consists of sev-eral megabytes of on-line texts from the German newspaper.
TM We constructed a train-ing text of 520 items from the Sfiddeutsche Zeitung corpus, and a cross-validation textof 268 items.
Training was performed in less than five minutes on a Next workstation.
19Testing on a sample of 3,184 separate items from the same corpus resulted in error ratesless than 1.3%, as summarized in Table 8.
A direct comparison to the UNIX STYLEprogram is not possible for German texts, as the STYLE program is only effective forEnglish texts.
The SZ corpus did have a lower bound of 79.1%, which was similar tothe 75.0% lower bound of the WSJ corpus.5.1.2 German News Corpus.
The German News corpus was constructed from a seriesof public-domain German articles distributed internationally by the University of Ulm.We constructed a training text of 268 potential sentence boundaries from the corpus,as well as a cross-validation text of 150 potential sentence boundaries, and the trainingtime was less than one minute in all cases.
A separate portion of the corpus was usedfor testing the system and contained over 5,037 potential sentence boundaries fromthe months July-October 1994, with a "baseline system performance of 96.7%.
Resultsof testing on the German News corpus are given in Table 8 and show a very low errorrate for both mixed-case and single-case texts.
Repeating the testing with a smallerlexicon containing less than 2,000 words still produced an error rate lower than 1%with a slightly higher training time.5.1.3 Decision Tree Induction for German Texts.
Using the c4.5 program to inducedecision trees from a 788 item German training set (from the SZ corpus) resulted in atree utilizing 11 of the 120 attributes.
The error rates over the SZ test set were 2.3% formixed-case texts and 1.9% for single-case texts, both noticeably higher than the besterror rate (1.3%) achieved with the neural network on the SZ corpus.
The decision tree18 All work with the Siiddeutsche Z itung corpus was performed in collaboration with the University ofMunich.19 The Next workstation issignificantly slower than the DEC Alpha workstation used in other tests,which accounts for the slower training time.261Computational Linguistics Volume 23, Number 2Table 8German results (17,000 word lexicon).Training Epochs Testing Errors Error (%)Siiddeutsche Zeitung mixed case 204 42 1.3%Lower case 141 44 1.4%Upper case 274 43 1.4%Decision tree mixed 72 2.3%Single 61 1.9%German News mixed case 732 37 0.7%Lower case 678 25 0.5%Upper case 611 36 0.7%Decision tree mixed 36 0.7%Single 36 0.7%Table 9French results (1,000 word lexicon, 3,766 potential sentence boundaries).Probabilistic BinaryTraining Testing Error (%) Training Testing Error (%)Epochs Errors Epochs ErrorsMixed case 273 39 1.0% 302 22 0.6%Lower case 243 24 0.6% 181 29 0.8%Upper case 328 25 0.7% 223 21 0.6%Decision tree (all cases) 17 0.4%induced for the German News corpus utilized only three attributes (t - 1 can be anabbreviation, t - 1 can be a number, t + 1 can be a noun) and produced a 0.7% errorrate in all cases.5.2 FrenchThe French lexicon was compiled from the part-of-speech data obtained by runningthe Xerox PARC part-of-speech tagger (Cutting et al 1991) on a portion of the Frenchhalf of the Canadian Hansards corpus.
The lexicon consisted of less than 1,000 wordsassigned parts of speech by the tagger, including 20 French abbreviations appended tothe 206 English abbreviations available from the lexicon used in obtaining the resultsdescribed in Section 4.
The part-of-speech tags in the lexicon were different fromthose used in the English implementation, so the descriptor array mapping had tobe adjusted accordingly.
The development time required to adapt Satz to French wastwo days.
A training text of 361 potential sentence boundaries was constructed fromthe Hansards corpus, and a cross-validation text of 137 potential sentence boundaries,and the training time was less than one minute in all cases.
The results of testingthe system on a separate set of 3,766 potential sentence boundaries (also from theHansards corpus) with a baseline algorithm performance of 80.1% are given in Table9, including a comparison of results with both probabilistic and binary feature inputs.These data show a very low system error rate on both mixed and single-case texts.
Inaddition, the decision tree induced from 498 French training items by the c4.5 programproduced a lower error rate (0.4%) in all cases.262Palmer and Hearst Multilingual Sentence BoundaryTable 10Summary of best results.Corpus Size Lower Baseline SatzBound (STYLE)Neural Net Decision TreeWall Street Journal 27,294 75.0 8.3% 1.1% 1.0%Si~ddeutsche Z itung 3,184 79.1 1.3% 1.9%German News 5,037 96.7 0.7% 0.7%Hansards (French) 3,766 80.1 0.6% 0.4%6.
Improving Performance on the Difficult CasesTo demonstrate he performance of our system within a large-scale NLP application,we integrated the Satz system into an existing information extraction system, the Alem-bic system (Aberdeen et al 1995) described in Section 2.2.
On the same WSJ corpusused to test Satz in Section 4, the Alembic system alone achieved an error rate of only0.9% (the best error rate achieved by Satz was 1.0%).
A large percentage of the errorsmade by the Alembic module fell into the second category described in Section 4.3,where one of the five abbreviations Co., Corp., Ltd., Inc., or U.S. occurred at the end ofa sentence.
We decided to see if Satz could be applied in such a way that it improvedthe results on the hard cases on which the hand-written rules were unable to performas well as desired.We trained Satz on 320 of the problematic examples described above, taken fromthe WSJ training corpus.
The remaining 19,034 items were used as test data.
TheAlembic module was used to disambiguate he sentence boundaries in all cases exceptwhen one of the five problematic abbreviations was encountered; in these cases, Satz(in neural network mode) was used to determine the role of the period following theabbreviation.
The hybrid disambiguation system reduced the total number of sentenceboundary errors by 46% and the error rate for the whole corpus fell from 0.9% to 0.5%.We trained Satz again, this time using the decision tree learning method, in orderto see what types of rules were acquired for the problematic sentences.
The decisiontree shown in Figure 7 is the result; the performance of Satz with this decision treewas nearly identical to the performance with the neural network.
From this tree itcan be seen that context farther away from the punctuation mark is important, andextensive use is made of part-of-speech information.7.
SummaryThis paper has presented Satz, a sentence boundary disambiguation system that pos-sesses the following desirable characteristics:?
It is robust, and does not require a hand-built grammar or specializedrules that depend heavily on capitalization, multiple spaces betweensentences, etc.?
It adapts easily to new text collections.?
It adapts easily to new languages.?
It trains and executes quickly without requiring large resources.263Computational Linguistics Volume 23, Number 2t -3  t -2  t -1  .
t+l t+2 t+3t+l begins with a capital letter?.t -3 can be a sent-end punctuation?0 t+l is a conjunction?0 t+2 can be a sent-end punctuation?t+l is a preposition?1 t -3  can be an abbreviation?1 t+l can be a verb?t+l is a right paren?0 t -2  is a conjunction?1 0t -2  can be a modifier?1 01 t+3 can be a sent-end punctuation?0 t+2 is a preposition?0 t -2  can be a modifier?Figure 7Decision tree induced when training on difficult cases exclusively.1 0?
It produces very accurate results.?
It is efficient enough that it does not noticeably slow down textpreprocessing.?
It is able to specify "no opinion" on cases that are too difficult todisambiguate, rather than making under-informed guesses.The Satz system offers a robust, rapidly trainable alternative to existing systems,which usually require extensive manual effort and are tailored to a specific text genreor a particular language.
By using part-of-speech frequency data to represent the con-text in which the punctuation mark appears, the system offers significant savings inparameter estimation and training time over word-based methods, while at the same264Palmer and Hearst Multilingual Sentence Boundarytime producing a very low error rate (see Table 10 for a summary of the best resultsfor each language).The systems of Riley (1989) and Wasson report what seem to be slightly bettererror rates, but these results are not directly comparable since they were evaluated onother collections.
Furthermore, the Wasson system required nine staff months of devel-opment, and the Riley system required 25 million word tokens for training and storageof probabilities for all corresponding word types.
By comparison, the Satz approachhas the advantages of flexibility for application to new text genres, small trainingsets (and thereby fast training times), relatively small storage requirements, and littlemanual effort.
The training time on a workstation (in our case a DEC Alpha 3000) isless than one minute, and the system can perform the sentence boundary disambigua-tion at a rate exceeding 10,000 sentences/minute.
B cause the system is lightweight, itcan be incorporated into the tokenization stage of many natural language processingsystems without substantial penalty.
For example, combining our system with a fastsentence alignment program such as that of Gale and Church (1993), which performsalignment at a rate of up to 1,000 sentences/minute, would make it possible to rapidlyand accurately create a bilingual aligned corpus from raw parallel texts.
Because thesystem is adaptive, it can be focused on especially difficult cases and combined withexisting systems to achieve still better error rates, as shown in Section 6.The system was designed to be easily portable to new natural languages, assumingthe accessibility of lexical part-of-speech information.
The lexicon itself need not beexhaustive, as shown by the success of adapting Satz to German and French withlimited lexica, and by the experiments in English lexicon size described in Section 4.4.The heuristics used within the system to classify unknown words can compensate forinadequacies in the lexicon, and these heuristics can be easily adjusted.It is interesting that the system performs o well using only estimates of the partsof speech of the tokens surrounding the punctuation mark and using very roughestimates at that.
In the future it may be fruitful to apply a technique that uses suchsimple information to more complex problems.AcknowledgmentsThe authors would like to acknowledgevaluable advice, assistance, andencouragement provided by ManuelF~ihndrich, Haym Hirsh, Dan Jurafsky, andTerry Regier.
We would also like to thankKen Church for making the PARTS dataavailable, and Ido Dagan, ChristianeHoffmann, Mark Liberman, Jan Pedersen,Martin R6scheisen, Mark Wasson, and JoeZhou for assistance in finding referencesand determining the status of related work.ReferencesAberdeen, John, John Burger, David Day,Lynette Hirschman, Patricia Robinson,and Marc Vilain.
1995.
MITRE:Description of the Alembic system usedfor MUC-6.
In Proceedings ofthe SixthMessage Understanding Conference (MUC-6),Columbia, MD, November.Bahl, L. R., P. E Brown, P. V. deSouza, andR.
L. Mercer.
1989.
A tree-based statisticallanguage model for natural languagespeech recognition.
IEEE Transactions onAcoustics, Speech, and Signal Processing,36(7):1001-1008.Bourland, Herv~ and Nelson Morgan.
1994.Connectionist Speech Recognition: A HybridApproach.
Kluwer Academic Publishers,Norwell, MA.Breiman, Leo, Jerome H. Friedman, RichardOlshen, and Charles J.
Stone.
1984.Classification and Regression Trees.Wadsworth International Group, Belmont,CA.Cherry, L. L. and W. Vesterman.
1991.Writing tools--the STYLE and DICTIONprograms.
In 4.3BSD UNIX SystemDocumentation.
University of California,Berkeley.Church, Kenneth W. 1988.
A stochastic partsprogram and noun phrase parser forunrestricted text.
In Second Conference onApplied Natural Language Processing, pages136-143, Austin, TX.Church, Kenneth W. and Mark Y. Liberman.1991.
A status report on the ACL/DCI.
In265Computational Linguistics Volume 23, Number 2Proceedings ofthe 7th Annual Conference ofthe UW Centre for the New OED and TextResearch: Using Corpora, pages 84-91,Oxford.Cutting, Doug, Julian Kupiec, Jan Pedersen,and Penelope Sibun.
1991.
A practicalpart-of-speech tagger.
In The 3rdConference on Applied Natural LanguageProcessing, Trento, Italy.Francis, W. Nelson and Henry Kucera.
1982.Frequency Analysis of English Usage.Houghton Mifflin Co., New York.Fung, Pascale and Kenneth Ward Church.1994.
K-vec: A new approach to aligningparallel texts.
In Proceedings ofCOLING-94.Fung, Pascale and Kathleen McKeown.1994.
Aligning noisy parallel corporaacross language groups: Word pairfeature matching by dynamic timewarping.
In Proceedings ofthe FirstConference ofthe Association for MachineTranslation i  the Americas AMTA-94,pages 81-88, Columbia, MD.Gale, William A. and Kenneth W. Church.1993.
A program for aligning sentences inbilingual corpora.
ComputationalLinguistics, 19(1):75-102.Hertz, John, Anders Krogh, and Richard G.Palmer.
1991.
Introduction to the Theory ofNeural Computation.
Santa Fe InstituteStudies in the Sciences of Complexity.Addison-Wesley Publishing Co.,Redwood City, CA.Hoffmann, Christiane.
1994.
AutomatischeDisambiguierung von Satzgrenzen ieinem maschinenlesbaren d utschenKorpus.
Manuscript, University of Trier,Germany.Humphrey, T. L. and E-q.
Zhou.
1989.Period disambiguation using a neuralnetwork.
In IJCNN: International JointConference on Neural Networks, page 606,Washington, D.C.Kay, Martin and Martin ROscheisen.
1993.Text-translation alignment.
ComputationalLinguistics, 19(1):121-142.Lesk, M. E. and E. Schmidt.
1975.
Lex--alexical analyzer generator.
ComputingScience Technical Report 39, AT&T BellLaboratories, Murray Hill, NJ.Liberman, Mark Y. and Kenneth W. Church.1992.
Text analysis and wordpronunciation i text-to-speech synthesis.In Sadaoki Furui and Man MohanSondhi, editors, Advances in Speech SignalProcessing.
Marcel Dekker, Inc., pages791-831.Lippmann, R. P. 1989. Review of neuralnetworks for speech recognition.
NeuralComputation, 1:1-38.Magerman, David M. 1995.
Statisticaldecision-tree models for parsing.
InProceedings ofthe 33rd Annual Meeting.Association for ComputationalLinguistics.Mtiller, Hans, V. Amerl, and G. Natalis.1980.
Worterkennungsverfahren alsGrundlage iner Universalmethode zurautomatischen Segmentierung von Textenin S~itze.
Ein Verfahren zur maschinellenSatzgrenzenbestimmung im Englischen.Sprache und Datenverarbeitung, 1.Nakamura, Masami, Katsuteru Maruyama,Takeshi Kawabata, and Kiyohiro Shikano.1990.
Neural network approach to wordcategory prediction for English texts.
InProceedings ofCOLING-90, volume 3,pages 213-218.Nicol, G. T. 1993.
Flex--The Lexical ScannerGenerator.
The Free Software Foundation,Cambridge, MA.Nunberg, Geoffrey.
1990.
The Linguistics ofPunctuation.
C.S.L.I.
Lecture Notes,Number 18.
Center for the Study ofLanguage and Information, Stanford, CA.Palmer, David D. 1995.
Experiments inmultilingual sentence boundaryrecognition.
In Proceedings ofRecentAdvances in Natural Language Processing,Velingrad, Bulgaria, September 1995.Palmer, David D. and Marti A. Hearst.
1995.Adaptive sentence boundarydisambiguation.
I  Proceedings oftheFourth ACL Conference on Applied NaturalLanguage Processing, pages 78-83.
MorganKaufmann, October 1994.Quinlan, J. Ross.
1986.
Induction of decisiontrees.
Machine Learning, 1(1):81-106.Quinlan, J. Ross.
1993. c4.5: Programs forMachine Learning.
Morgan Kaufman, SanMateo, CA.Resnik, Philip.
1993.
Semantic lasses andsyntactic ambiguity.
In Proceedings oftheARPA Workshop on Human LanguageTechnology (March 1993).Riley, Michael D. 1989.
Some applications oftree-based modelling to speech andlanguage indexing.
In Proceedings oftheDARPA Speech and Natural LanguageWorkshop, pages 339-352.
MorganKaufmann.Schmid, Helmut.
1994.
Part-of-speechtagging with neural networks.
InProceedings ofCOLING-94.Siegel, Eric V. and Kathleen R. McKeown.1994.
Emergent linguistic rules frominducing decision trees: Disambiguatingdiscourse clue words.
In Proceedings ofAAA11994, pages 820-826.
AAAIPress/MIT Press, July.266Palmer and Hearst Multilingual Sentence BoundarySoderland, Stephen and Wendy Lehnert.1994.
Corpus-driven knowledgeacquisition for discourse analysis.
InProceedings of AAA11994, pages 827-832.AAAI Press/MIT Press, July.267
