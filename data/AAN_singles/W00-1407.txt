A Strategy for Generating Evaluative Arguments ..Giuseppe CareniniIntelligent Systems ProgramUniversity of Pittsburgh,Pittsburgh, PA 15260, USAcarenini@cs.pitt.eduAbstractWe propose an argumentation strategy forgenerating evaluative arguments that can beapplied in systems erving as personal assistantsor advisors.
By following guidelines fromargumentation theory and by employing aquantitative model of the user's preferences, thestrategy generates arguments hat are tailored tothe user, properly arranged and concise.
Ourproposal extends the scope of previousapproaches both in terms of types of argumentsgenerated, and in terms of compliance withprinciples from argumentation theory.IntroductionArguing involves an intentional communicativeact that attempts to create, change or reinforcethe beliefs and attitudes of another person.Factual and causal arguments attempt to affectbeliefs (i.e.
assessments that something is or isnot the case), whereas evaluative argumentsattempt to affect attitudes (i.e., evaluativetendencies typically phrased in terms of like anddislike or favor and disfavor).With the ever growing use of the Web, anincreasing number of systems that serve aspersonal assistants, advisors, or sales assistantsare becoming available online ~.
These systemsfrequently need to generate evaluativearguments for domain entities.
For instance, areal-estate assistant may need to compare twohouses, arguing that one would be a betterchoice than the other for its user.Argumentation theory (Mayberry and Golden1996; Miller and Levine 1996; Corbett andConnors 1999) indicates that effectivearguments should be constructed tbllowing threeJohanna D. MooreThe Human Communication Research Centre,University of Edinburgh,2 Buccleuch Place, Edinburgh EH8 9LW, UK.jmoore@cogsci.ed.ac.ukgeneral principles.
First, arguments hould beconstructed considering the dispositions of theaudience towards the information presented.Second, sub-arguments supporting or opposingthe main argument claim should be carefullyarranged by considering their strength of supportor opposition.
Third, effective arguments shouldbe concise, presenting only pertinent and cogentinformation.In this paper, we propose an argumentationstrategy for generating evaluative arguments hatcan be applied in systems erving as personalassistants or advisors.
By following principlesand guidelines from argumentation theory andby employing a quantitative model of the user'spreference, our strategy generates evaluativearguments hat are tailored to the user, properlyarranged and concise.Although a preliminary version of ourargumentative strategy was cursorily describedin a previous short paper (Carenini and Moore1999), this paper includes several additionalcontributions.
First, we discuss how the strategyis grounded in the argumentation literature.Then, we provide details on the measures ofargument strength and importance used inselecting and ordering argument support.
Next,we generalize the argumentative strategy andcorrect some errors in its preliminary version.Finally, we discuss how our strategy extends thescope of previous approaches to generatingevaluative arguments in terms of coverage (i.e.,types of arguments), and in terms of compliancewith principles from argumentation theory.Because of  space limitations, we only discuss'previous work on generating evaluativearguments, rather than previous work ongenerating arguments in general.See llbr instance www.activebuyersguide.com471 Guidelines from Argumentation TheoryAn argumentation strategy specifies whatcontent should be included in the argument andhow it should be arranged.
This comprisesseveral decisions: what represents supporting (oropposing) evidence for the main claim, where toposition the main claim of the argument; whatsupporting (or opposing) evidence to includeandhow to order it, and.how to order supp6rfingand opposing evidence with respect to eachother.Argumentation theory has developed guidelinesspecifying how these decisions can beeffectively made (see (Mayberry and Golden1996; Miller and Levine 1996; Corbett andConnors 1999; McGuire 1968) for details; seealso (Marcu 1996) for an alternative discussionof some of the same guidelines).
(a) What represents supporting (or opposing)evidence for  a claim - Guidelines for thisdecision vary depending on the argument type.Limiting our analysis to evaluative arguments,argumentation theory indicates that supportingand opposing evidence should be identifiedaccording to a model of the reader's values andpreferences.
For instance, the risk involved in agame can be used as evidence for why yourreader should like the game, only if the readerlikes risky situations.
(b) Posit ioning the main claim - Claims areoften presented up front, usually for the sake ofclarity.
Placing the claim early helps readersfollow the line of reasoning.
However, delayingthe claim until the end of the argument can beeffective, particularly when readers are likely tofind the claim objectionable or emotionallyshattering.
(c) Selecting supporting (and opposing)evidence - Often an argument cannot mention allthe available evidence, usually for the sake ofbrevity.
Only strong evidence should, bepresented in detail, whereas weak evidenceshould be either briefly mentioned or omittedentirely.
(d) Arranging/Ordering~supporiing evicleiTce -Typically the strongest support should bepresented first, in order to get at least provisionalagreement from the reader early on.
If at allpossible, at least one very effective piece ofsupporting evidence should be saved for the endof the argument, inorder to leave the reader witha final impression of the argument's strength.This guideline proposed in (Mayberry andGolden 1996) is a compromise between theclimax and the anti-climax approaches discussedin (McGuire 1968).
(e) Addressing and ordering thecounterarguments (opposing evidence) - There........ ?
~ar~ .
~three,.~options.
~ .for, :Ihis~ :.ateeision: not ~to .
.
.
.mention any counterarguments, to acknowledgethem without directly refuting them, toacknowledge them and directly refuting them.Weak counterarguments may be omitted.Stronger counterarguments should be brieflyacknowledged, because that shows the readerthat you are aware of the issue's complexity; andit also contributes to the impression that you arereasonable and broad-minded.
You may need torefute a counterargument once you haveacknowledged it, if the reader agrees with aposition substantially different from yours.Counterarguments should be ordered tominimize their effectiveness: strong ones shouldbe placed in the middle, weak ones upfront andat the end.
(09 Ordering supporting and opposing evidence- A preferred ordering between supporting andopposing evidence appears to depend onwhether the reader is aware of the opposingevidence.
If so, the preferred ordering isopposing before supporting, and the reverseotherwise.Although these guidelines provide usefulinformation on the types of content to include inan evaluative argument and how to arrange it,the design of a computational rgumentativestrategy based on these guidelines requires thatthe concepts mentioned in the guidelines beformalized in a coherent computationalframework.
This includes: explicitlyrepresenting the reader's values and preferences(used in guideline a); operationally defining theterm "objectionable claim v (used in guideline b)through a measure of the discrepancy betweenthe readerrs-initial positionand-the argument'smain claim2; providing a measure of evidencestrength (needed in guidelines c, d, and e); and3 An operational definition for "emotionallyshattering" isoutside the scope of this paper.48HouseValueOBJECTIVES ~OMPONENT VALUE FUNCTIONSATTRIBUTESLocation ?.y.Size 0.80.2~.
bTeighborhoo dDistance-from- park"---- t-of-roomStorage-spacexl=nl  0xl=n2 0.3xl=n3 10=<:x2<:5 1-(1/5" X2)X~5 0Figure 1 Sample additive multiattribute value function (AMVF)representing whether the reader is or is notaware of  certain facts (needed in guideline tO.2 From Guidelines to the ArgumentationStrategyWe assume that the reader's values andpreferences are represented as an additivemultiattribute value function (AMVF), aconceptualization based on multiattribute utilitytheory (MAUT)(Clemen 1996).
Besides beingwidely used in decision theory (where they wereoriginally developed), conceptualizations basedon MAUT have recently become a commonchoice in the field of  user modelling (Jameson,Schafer et al 1995).
Similar models are alsoused in Psychology, in the study of consumerbehaviour (Solomon 1998).2.1 Background on AMVFAn AMVF is a model of a person's values andpreferences with respect o entities in a certainclass.
It comprises a value tree and a set ofcomponent  value funct ions,  one for eachattribute of the entity.
A value tree is adecomposition of the value of an entity into ahierarchy of aspects of the entity 3, in which theleaves correspond to the entity primitivea~ributes (see Figure 1 for a simple value tree inthe real estate domain).
The arcs of the tree areweighted to represent he importance of thevalue of  an objective in contributing to the value3 In decision theory these aspects are calledobjectives.
For consistency with previous work, wewill follow this terminology in the remainder of thepaper.of its parent in the tree (e.g., in Figure 1 locationis more than twice as important as size indetermining the value of a house).
Note that thesum of the weights at each level is equal to 1.
Acomponent value function for an attributeexpresses the preferability of each attributevalue as a number in the \[0,1\] interval.
Forinstance, in Figure 1, neighborhood n2 haspreferability 0.3, and a distance-from-park of 1mile has preferability (1 - (1/5" 1))=0.8.Formally, an AMVF predicts the value v(e) of anentity e as follows:v(e) = v(xl ..... x,) = Y~w, v /x9,  where- (x/ ..... x,,) is the vector of attribute values foran entity e- Vattribute i, v, is the component valuefunction, which maps the least preferable x,to 0, the most preferable to I, and the otherx, to values in \[0,1\]- w, is the weight for attribute i, with 0_< w, _<1and Zw, =1- w, is equal to the product of all the weightsfrom the root of the value tree to theattribute iA function vo(e) can also be defined for eachobjective.
When applied to an entity, this?
- function "returns ~the value o f  the entity withrespect o that objective.
For instance, assumingthe value tree shown in Figure 1, we have:v,.
.
.
.
.
.
.
.
.
(e )  == (0.4 * V~,,h~orhooa (e)) + (0.6 * vl~,~,_/,~,,,_r~rk (e))Thus, given someone's AMVF, it is possible tocompute how valuable an entity is to that49individual.
Furthermore, it is possible tocompute how valuable any objective (i.e., anyaspect of that entity) is for that person.
All ofthese values are expressed as a number in theinterval \[0, i \].2.2 Computational Definition of ConceptsMentioned in GuidelinesPresenting an evaluative argument is an attemptto persuade the reader that a value judgmentapplies to a subject.
The value judgement, alsocalled the argumentative intent, can either bepositive (in favour of  the subject), or negative(against the subject) 4.
The subject can be asingle entity (e.g., "This book is very good"), thedifference between two entities (e.g., "City-a issomewhat better than city-b'), or any other formof comparison among entities in a set (e.g.,"This city is the best in North America").Guideline (a) - Given the reader's AMVF, it isstraightforward to establish what representsupporting or opposing evidence for anargument with a given argumentative intent anda given subject.
In fact, if the argumentativeintent is positive, objectives for which thesubject has positive value can be used assupporting evidence, whereas objectives forwhich the subject has a negative value can beused as opposing evidence (the opposite holdswhen the argumentative intent is negative).
Thevalue of different subjects is measured asfollows.
If the subject is a single entity e, thevalue of the subject for an objective o is vo(e),and it is positive when it is greater than 0.5, themidpoint of \[0,1\] (negative otherwise).
Incontrast, if the subject is a comparison betweentwo entities (e.g., v(ed > v(e_,)), the value of thesubject for an objective o is \[vo(e9 - Vo(e,)\], andit is positive when it is greater than 0 (negativeotherwise).Guidelines (b) - Since argumentative intent is avalue judgment, we canreasonab\[y assume thatinstead of  being simply positive or negative, itmay be specified more precisely as a number inthe interval \[0,1\] (or as a specification that canbe normalized in this interval), Then, the term4 Arguments can also be neutral.
However, in thispaper we do not discuss arguments with a neutralargumentative intent.
"objectionable claim" can be operationallydefined.
If we introduce a measure-of-discrepancy(MD) as the absolute value of thedifference between the argumentative intent andthe reader's expected value of the subject beforethe argument is presented (based on her AMVF),a claim becomes more and more "objectionable!
'for a reader as MD moves from 0 to 1.,~,.
,:,_.~.uidelin~;,(c) ~(d), (e).
~,:~The,,~strength o?
the ....evidence in support of (or opposition to) themain argument claim is critical in selecting andorganizing the argument content.
To define ameasure of the strength of support (oropposition), we adopt and extend previous workon explaining decision theoretic advice based onan AMVF.
(Klein 1994) presents explanationstrategies (not based on argumentation theory) tojustify the preference of one alternative from apair.
In these strategies, the compellingness of anobjective measures the objective's strength indetermining the overall value difference betweenthe two alternatives, other things being equal.And an objective is notably-compell ing?
(i.e.,worth mentioning) if it is an outlier in apopulation of objectives with respect tocompeilingness.
The formal definitions are:compellingness(o, al a2, refo) == w(o, refo)\[vo(at) - Vo(a2)\], where- o is an objective, a /and a2 are alternatives,refo is an ancestor of o in the value tree- w(o, refo) is the product of the weights of allthe links from o to refo- vo is the component value function for leafobjectives (i.e., attributes), and it is therecursive evaluation over children(o) fornonleaf objectivesnotably-compelling?
(o, opop.
al, a2, refo) -\[ compellingness(o, al a2, refo) \[ >px+ko'x, where- o, al, a2 and refo are defined as in theprevious Def; opop is an objectivepopulation (e.g., siblings(o)), and I opopl >2- pe  opop; xeX = \[compellingness(p, al, a_~,refo) l- gx is the mean of X, ~x is the standarddeviation and k is a user-defined constantWe have defined similar measures for arguingthe value of a single entity and we named thems-compellingness and s-notably-compell ing?.50An objective can be s-compelling either becauseof its strength or because of its weakness incontributing to the value of an alternative.
So, ifm~ measures how much the value of an objectivecontributes to the overall value difference of analternative from the worst possible case 5and m2measures how much the value of an objectivecontributes to the overall value difference of theis either a single entity or a pair of entities in thedomain of interest.
Root can be any objective inthe value tree for the evaluation (e.g., the overallvalue of a house, its location, its amenities).ArgInt is the argumentative intent of theargument, a number in \[0,1 \].
The constant k, partof the definitions of notably-compelling?
and s-notably-compelling?, determines the degree of:, .,,alternative ,from., th~_b~st:,possible:~ease,:.~e-: :,~ eoneisenessofithe;argument,,, The~Express-Valuedefine s-compellingness a  the greatest of thetwo quantities m~ and m2.
Following theterminology introduced in the two previousEquations we have:s-compellingness(o, a, refo) == w(o, refo)\[max\[vo(a) - 0\],'\[1 - vo(a)\]\]We give to s-notably-compelling?
a definitionanalogous to the one for notably-compelling?s-notably-compelling?
(o,opop, a, refo) -\] s-compellingness(o,a, refo) \[ >~+k~x,Guideline 09 - An AMVF does not representwhether the reader is or is not aware of certainfacts.
We assume this information is representedseparately.2.3 The Argumentation StrategyWe have applied the formal definitionsdescribed in the previous ection to develop theargumentative strategy shown in Figure 2.
Thestrategy is designed for generating honest andbalanced arguments, which present anevaluation of the subject equivalent to the oneyou would expect he reader to hold according toher model of preferences (i.e., the argumentativeintent is equal to the expected value, so MD=0) 6.We now examine the strategy in detail, afterintroducing necessary, terminology.
The subject5 a,.or~, is an alternative such that Vo v~,(a,,,,r~,)=O,whereas abL., is an alternative suchthat Vo vo(abe.
?~)=l6 An alternative strategy, for generating argumentswhose argumentative intent was-greater (or lower)than the expected value, could also be defined in ourframework.
However, this strategy should boost theevaluation of supporting evidence and include onlyweak counterarguments, or hide them overall (theopposite if the target value was lower than theexpected value)function, used at the end of the strategy,indicates that the objective applied to the subjectmust be realized in natural language with acertain argumentative intent.In the first part of the strategy, depending on thenature of the subject, an appropriate measure ofevidence strength is assigned, along with theappropriate predicate that determines whether apiece of evidence is worth mentioning.
Afterthat, only evidence that is worth mentioning isassigned as supporting or opposing evidence bycomparing its value to the argument intent.
Inthe second part, ordering constraints fromargumentation theory are applied 7.
Notice thatwe assume a predicate Aware that is true whenthe user is aware of a certain fact, falseotherwise.
Finally, in the third part of thestrategy, the argument claim is expressed innatural language.
The opposing evidence (i.e.,ContrastingSubObjectives), that must beconsidered, but not in detail, is also expressed innatural language.
In contrast, supportingevidence is presented in detail, by recursivelycalling the strategy on each supporting piece ofevidence.2.4 Implementation and ApplicationThe argumentation strategy has beenimplemented as a set of plan operators.
Usingthese operators the Longbow discourse planner(Young and Moore 1994) selects and arrangesthe content of the argument.
We have appliedour strategy in a system that serves as a real-estate personal assistant (Carenini 2000a).
Thesystem presents information about housesavailable on the market in graphical format.
Theuser explores this information by means ofinteractive techniques, and can request a natural7 The steps in the strategy are marked with theguideline they are based on.51Argue(subject, Root, Argint, k );; ass ignments  and content  select ionI f  subject = single-entity = e then SVo, = Vol (e)Measure-of-strength = s-compel!ingness" Worth-mention?
= s-notably-compelling?Else I f  subject = e~,e 2 then SVo, = \[%, (e,) - vo, (e2)\]Measure-of-strength = compellingnessWorth-mention?
= notably-compelling?Eliminate all objectives oil ~ Worth-mention?
(o,, siblings(o,), subject, Root) ;guideline(c)AllEvidence ~- ehildren(RooOAlllnFavor~-- all o \] o e AllEvidence/x (SVo ..~ArglnO ;guideline(a)SecondBestObjlnFavor~-second most compelling objective o lo E AlllnFavorRemainingObjectiveslnFavor ~- AlllnFavor - SecondBestObjlnFavorContrastingObjectives ~- AllEvidence - AlllnFavor ;guideline(a);; ordering the selected contentAddOrdering(Root -~AllEvidence) ;; we assume MD=0, so claim is not objectionable ;guideline(b)I f  Aware(User, ContrastingObjectives) then ;guideline(f)AddOrdering( ContrastingObjectives -~ AlllnFavor)Else AddOrdering(ContrastingObjectives ~- A lllnFavor );A ddOrdering( RemainingObjectiveslnFavor -~ SecondBestObjlnFavor ) ;guideline(d)Sort(RemainingObjectiveslnFavor," decreasing order according to Measure-of-strength) ;guideline(d)Sort(ContrastingObjectives," strong ones in the middle, weak ones upfront and at the end) ;guideline(e);; steps for expressing or further argue the contentExpress-Value(subject, Root, Arglnt)For all o ~ AlllnFavor, I f  ~leaffo) then Argue(subject, o SVo, k)Else Express-Value(subject, o, SVo)For all o E ContrastingObjectives, Express-Value(subject, o, SVo) ;guideline(e)Legend: (a -~ b) ~ a preceeds b(v~ ~- v 2) ~ vl and v 2 are both positive or negative values(see Section O for what this means for d~erent subjects)-, .
-= Figure 2 The,Argumentation strategy52language evaluation of any house just bydragging the graphical representation of thehouse to a query button.
The evaluativearguments generated by the system are concise,properly arranged and tailored to the user'spreferences s. For sample arguments generatedby our strategy see (Carenini 2000b) in thisproceedings.
(Elzer, Chu-Carroli et al 1994; Chu-Carroll andCarberry 1998) studied the generation ofevaluative arguments in the context ofcollaborative planning dialogues.
Although theyalso adopt a qualitative measure of evidencestrength, when an evaluation is needed thismeasure is mapped into numerical values so thatpreferences can be compared and combined.
.
.
.
.
...= :- .
.
.
.
.
.
.
:,, ~- .-: :-;.~ ~,xnore:.
:e:ffeeti~ely:,Rl~ve.~t?,~ittr,,respeet =-~tO:our3 Previous WorkAlthough considerable research has beendevoted to study the generation of evaluativearguments, all approaches proposed so far arelimited in the type of evaluative argumentsgenerated, and in the extent to which theycomply with guidelines from argumentationliterature.
(Elhadad 1992) investigated a generalcomputational framework that covers all aspectsof generating evaluative arguments of singleentities, from content selection and structuring tofine-grained realization decisions.
However, hiswork concentrates on the linguistic aspects.
Hisapproach to content selection and structuringdoes not provide a measure of evidence strength,which is necessary to implement several of theguidelines from argumentation literature wehave examined.Other studies have focused more on the processof content selection and structuring.
However,with respect o our proposal, they still sufferfrom some limitations.
(Morik 1989) describes asystem that uses a measure of evidence strengthto tailor evaluations of hotel rooms to its users.However, her system adopts a qualitativemeasure of evidence strength (an ordinal scalethat appears to range from very-important tonot-important).
This limits the ability of the systemto select and arrange argument evidence,because qualitative measures only supportapproximate comparisons and are ~ notoriouslydifficult to combine (e.g., how many"somewhat-important" pieces of evidence areequivalent to.
:an #important" .
:.piece of..evidence?
).s The generation of fluent English also required thedevelopment of microplanning and realizationcomponents.
For lack of space, we do not discussthem in this paper.approach, this work makes two strongsimplifying assumptions.
It only considers thedecomposition of the preference for an entityinto preferences for its primitive attributes (notconsidering that complex preferences frequentlyhave a hierarchical structure).
Additionally, itassumes that the same dialogue turn cannotprovide both supporting and opposing evidence.
(Kolln 1995) proposes a framework forgenerating evaluative arguments which is basedon a quantitative measure of evidence strength.Evidence strength is computed on a ~zzyhierarchical representation of user preferences.Although this fuzzy representation mayrepresent a viable alternative to the AMVF wehave discussed in this paper, Kolln's proposal israther sketchy in describing how his measure ofstrength can be used to select and arrange theargument content.Finally, (Klein 1994) is the previous work mostrelevant to our proposal.
Klein developed aframework for generating explanations to justifythe preference of an entity out of a pair.
Thesestrategies were not based on argumentationtheory.
As described in Section 2.2, from thiswork, we have adapted a measure of evidencestrength (i.e., compellingness), and a measurethat defines when a piece of evidence is worthmentioning (i.e., notably-compelling?
).Conclusions and Future WorkIn this paper, we propose.an argumentationstrategy that extends?
previous research ongenerating evaluative arguments in two ways.Our .
strategy -covers ~ the: <generation.
: :ofevaluations of a single entity, as well ascomparisons between two entities.
Furthermore,our strategy generates arguments, which areconcise, properly arranged and tailored to ahierarchical model of user's preferences, by53following a comprehensive set of guidelinesfrom argumentation theory.Several issues require further investigation.First, we plan to generalize our approach tomore complex models of user preferences.Second, although our strategy is based oninsights from argumentation theory, the ultimatearbiter for effectiveness is empirical evaluation.Clemen, R. T. (1'996).
Making Hard Decisions: anintroduction to decision analysis.
Duxbury PressCorbett, E. P. J. and R. J. Connors (1999).
ClassicalRhetoric for the Modern Student, OxfordUniversity Press.Elhadad, M. (1992).
Using Argumentation toControlLexical Choice: A Functional UnificationImplementation.
PhD Thesis, CS.
Columbia.
NY.Therefore, we have~..developed~an+~v.atuation .........
Elzer,.S.,..I_Giatt.-.Carrolk..et.al.
(.1994).Recogn&ingenvironment o verify whether argumentsgenerated by our strategy actually affect userattitudes in the intended irection (Carenini2000b).
A third area for future work is theexploration of techniques to improve thecoherence of arguments generated by ourstrategy.
In the short term, we intend to integratethe ordering heuristics uggested in (Reed andLong 1997).
In the long term, by modelling userattention and retention, we intend to enable ourstrategy to assess in a principled way whenrepeating the same information can strengthenargument force.
Finally, we plan to extend ourstrategy to evaluative arguments forcomparisons between mixtures of entities andset of entities.AcknowledgementsOur thanks go to the members of the Autobriefproject: S. Roth, N. Green, S. Kerpedjiev and J.Mattis.
We also thank C. Conati for commentson drafts of this paper.
This work was supportedby grant number DAA-1593K0005 from theAdvanced Research Projects Agency (ARPA).Its contents are solely responsibility of theauthors.ReferencesCarenini, G. (2000a).
Evaluating MultimediaInteractive Arguments in the Context of DataExploration Tasks.
PhD Thesis, Intelligent SystemProgram, University of Pittsburgh.Carenini, G. (2000b).
A Framework to EvaluateEvaluative Arguments.
Int.
Conference on NaturalLanguage-Generations.
Mitzpe~,Ramon, Israel.Carenini, G. and J. Moore (1999).
TailoringEvaluative Arguments to User's Preferences.
UserModelling, Banff; Canada : 299-301.Chu-Carroll, J. and S, Carberry (1998).
CollaborativeResponse Generation in Planning Dialogues.Computational Linguistics 24(2): 355-400.and Utilizing User Preferences in CollaborativeConsultation Dialogues.
Proceedings of FourthInt.
Conf.
of User Modeling.
Hyannis, MA: 19-24.Jameson, A., R. Schafer, et al (1995).
Adaptiveprovision of Evaluation-Oriented Information:Tasks and techniques.
Proc.
of 14th IJCAI.Montreal, Canada.Klein, D. (1994).
Decision Analytic IntelligentSystems: Automated Explanation and KnowledgeAcquisition, Lawrence Erlbaum Associates.Kolln, M. E. (1995).
Employing User Attitudes inText Planning.
5th European Workshop on NaturalLanguage Generation, Leiden, The Netherlands.Marcu, D. (1996).
The Conceptual and LinguisticFacets of Persuasive Arguments.
ECAI workshop -Gaps and Bridges: New Directions in Planning andNatural Language Generation.Mayberry, K. J. and R. E. Golden (1996).
ForArgument's Sake: A Guide to Writing EffectiveArguments, Harper Collins, College Publisher.McGuire, W. J.
(1968).
The Nature of Attitudes andAttitudes Change.
The Handbook of SocialPsychology.
G. Lindzey and E. Aronson, Addison-Wesley.
3: 136-314.Miller, M. D. and T. R. Levine (1996).
Persuasion.An Integrated Approach to Communication Theot Tand Research.
M. B. Salwen and D. W. Stack.Mahwah, New Jersey: 261-276.Morik, K. (1989).
User Models and ConversationalSettings: Modeling the User's Wants.
User Modelsin Dialog Systems.
A. Kobsa and W. Wahlster,Springer-Verlag: 364-385.Reed, C. and D. Long (1997).
Content Ordering inthe Generation of Persuasive Discourse.
Proc, ofthe 15th IJCAI, Nagoya; Japan.Solomon, M. R. (1998).
Consumer Behavior: Bzo,ing,Having.
and Being.
~ Prentice Hall.Young, M. R. and J. D. Moore (1994).
DoesDiscourse Planning Require a Special-PurposePlanner?
Proc.
of the AAAI-94 Workshop onplanning for lnteragent Communication.
Seattle,WA.54
