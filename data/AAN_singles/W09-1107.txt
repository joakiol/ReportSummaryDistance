Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 39?47,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsA Method for Stopping Active Learning Based on Stabilizing Predictionsand the Need for User-Adjustable StoppingMichael Bloodgood?Human Language TechnologyCenter of ExcellenceJohns Hopkins UniversityBaltimore, MD 21211 USAbloodgood@jhu.eduK.
Vijay-ShankerComputer and InformationSciences DepartmentUniversity of DelawareNewark, DE 19716 USAvijay@cis.udel.eduAbstractA survey of existing methods for stopping ac-tive learning (AL) reveals the needs for meth-ods that are: more widely applicable; more ag-gressive in saving annotations; and more sta-ble across changing datasets.
A new methodfor stopping AL based on stabilizing predic-tions is presented that addresses these needs.Furthermore, stopping methods are requiredto handle a broad range of different annota-tion/performance tradeoff valuations.
Despitethis, the existing body of work is dominatedby conservative methods with little (if any) at-tention paid to providing users with controlover the behavior of stopping methods.
Theproposed method is shown to fill a gap in thelevel of aggressiveness available for stoppingAL and supports providing users with controlover stopping behavior.1 IntroductionThe use of Active Learning (AL) to reduce NLP an-notation costs has generated considerable interest re-cently (e.g.
(Bloodgood and Vijay-Shanker, 2009;Baldridge and Osborne, 2008; Zhu et al, 2008a)).To realize the savings in annotation efforts that ALenables, we must have a mechanism for knowingwhen to stop the annotation process.Figure 1 is intended to motivate the value of stop-ping at the right time.
The x-axis measures the num-ber of human annotations that have been requestedand ranges from 0 to 70,000.
The y-axis measures?
This research was conducted while the first author was aPhD student at the University of Delaware.0 1 2 3 4 5 6 7x 104657075808590Active Learning Curve (F Measure vs Number of Annotations)Number of Points for whichAnnotations Have Been RequestedPerformance(FMeasure)stop point 1:stops too early;results in lowerperforming modelstop point 2:good place to stopstop point 3:stops too late;wastes around30,000 humanannotationsFigure 1: Hypothetical Active Learning Curve with hy-pothetical stopping points.performance in terms of F-Measure.
As can be seenfrom the figure, the issue is that if we stop too earlywhile useful generalizations are still being made, wewind up with a lower performing system but if westop too late after all the useful generalizations havebeen made, we just wind up wasting human annota-tion effort.The terms aggressive and conservative will beused throughout the rest of this paper to describe thebehavior of stopping methods.
Conservative meth-ods tend to stop further to the right in Figure 1.They are conservative in the sense that they?re verycareful not to risk losing significant amounts of F-measure, even if it means annotating many more ex-amples than necessary.
Aggressive methods, on theother hand, tend to stop further to the left in Figure 1.They are aggressively trying to reduce unnecessaryannotations.There has been a flurry of recent work tackling the39problem of automatically determining when to stopAL (see Section 2).
There are three areas where thisbody of work can be improved:applicability Several of the leading methods are re-stricted to only being used in certain situations,e.g., they can?t be used with some base learn-ers, they have to select points in certain batchsizes during AL, etc.
(See Section 2 for dis-cussion of the exact applicability constraints ofexisting methods.
)lack of aggressive stopping The leading methodstend to find stop points that are too far to theright in Figure 1.
(See Section 4 for empiricalconfirmation of this.
)instability Some of the leading methods work wellon some datasets but then can completely breakdown on other datasets, either stopping way toolate and wasting enormous amounts of annota-tion effort or stopping way too early and losinglarge amounts of F-measure.
(See Section 4 forempirical confirmation of this.
)This paper presents a new stopping method basedon stabilizing predictions that addresses each ofthese areas and provides user-adjustable stoppingbehavior.
The essential idea behind the new methodis to test the predictions of the recently learned mod-els (during AL) on examples which don?t have tobe labeled and stop when the predictions have sta-bilized.
Some of the main advantages of the newmethod are that: it requires no additional labeleddata, it?s widely applicable, it fills a need for amethod which can aggressively save annotations, ithas stable performance, and it provides users withcontrol over how aggressively/conservatively to stopAL.Section 2 discusses related work.
Section 3 ex-plains our Stabilizing Predictions (SP) stopping cri-terion in detail.
Section 4 evaluates the SP methodand discusses results.
Section 5 concludes.2 Related WorkLaws and Schu?tze (2008) present stopping criteriabased on the gradient of performance estimates andthe gradient of confidence estimates.
Their tech-nique with gradient of performance estimates is onlyapplicable when probabilistic base learners are used.The gradient of confidence estimates method is moregenerally applicable (e.g., it can be applied withour experiments where we use SVMs as the baselearner).
This method, denoted by LS2008 in Tablesand Figures, measures the rate of change of modelconfidence over a window of recent points and whenthe gradient falls below a threshold, AL is stopped.The margin exhaustion stopping criterion was de-veloped for AL with SVMs (AL-SVM).
It says tostop when all of the remaining unlabeled examplesare outside of the current model?s margin (Schohnand Cohn, 2000) and is denoted as SC2000 in Ta-bles and Figures.
Ertekin et al (2007) developed asimilar technique that stops when the number of sup-port vectors saturates.
This is equivalent to marginexhaustion in all of our experiments so this methodis not shown explicitly in Tables and Figures.
Sincewe use AL with SVMs, we will compare with mar-gin exhaustion in our evaluation section.
Unlike ourSP method, margin exhaustion is only applicable foruse with margin-based methods such as SVMs andcan?t be used with other base learners such as Maxi-mum Entropy, Naive Bayes, and others.
Schohn andCohn (2000) show in their experiments that marginexhaustion has a tendency to stop late.
This is fur-ther confirmed in our experiments in Section 4.The confidence-based stopping criterion (here-after, V2008) in (Vlachos, 2008) says to stop whenmodel confidence consistently drops.
As pointed outby (Vlachos, 2008), this stopping criterion is basedon the assumption that the learner/feature represen-tation is incapable of fully explaining all the exam-ples.
However, this assumption is often violated andthen the performance of the method suffers (see Sec-tion 4).Two stopping criteria (max-conf and min-err) arereported in (Zhu and Hovy, 2007).
The max-confmethod indicates to stop when the confidence of themodel on each unlabeled example exceeds a thresh-old.
In the context of margin-based methods, max-conf boils down to be simply a generalization of themargin exhaustion method.
Min-err, reported to besuperior to max-conf, says to stop when the accu-racy of the most recent model on the current batch ofqueried examples exceeds some threshold (they use0.9).
Zhu et al (2008b) proposes the use of multi-criteria-based stopping to handle setting the thresh-40old for min-err.
They refuse to stop and they raisethe min-err threshold if there have been any classi-fication changes on the remaining unlabeled data byconsecutive actively learned models when the cur-rent min-err threshold is satisfied.
We denote thismulti-criteria-based strategy, reported to work betterthan min-err in isolation, by ZWH2008.
As seen in(Zhu et al, 2008a), sometimes min-err indeed stopslater than desired and ZWH2008 must (by natureof how it operates) stop at least as late as min-errdoes.
The susceptibility of ZWH2008 to stoppinglate is further shown emprically in Section 4.
Also,ZWH2008 is not applicable for use with AL setupsthat select examples in small batches.3 A Method for Stopping Active LearningBased on Stabilizing PredictionsTo stop active learning at the point when annotationsstop providing increases in performance, perhaps themost straightforward way is to use a separate set oflabeled data and stop when performance begins tolevel off on that set.
But the problem with this is thatit requires additional labeled data which is counterto our original reason for using AL in the first place.Our hypothesis is that we can sense when to stop ALby looking at (only) the predictions of consecutivelylearned models on examples that don?t have to belabeled.
We won?t know if the predictions are cor-rect or not but we can see if they have stabilized.
Ifthe predictions have stabilized, we hypothesize thatthe performance of the models will have stabilizedand vice-versa, which will ensure a (much-needed)aggressive approach to saving annotations.SP checks for stabilization of predictions on a setof examples, called the stop set, that don?t have tobe labeled.
Since stabilizing predictions on the stopset is going to be used as an indication that modelstabilization has occurred, the stop set ought to berepresentative of the types of examples that will beencountered at application time.
There are two con-flicting factors in deciding upon the size of the stopset to use.
On the one hand, a small set is desir-able because then SP can be checked quickly.
Onthe other hand, a large set is desired to ensure wedon?t make a decision based on a set that isn?t repre-sentative of the application space.
As a compromisebetween these factors, we chose a size of 2000.
InSection 4, sensitivity analysis to stop set size is per-formed and more principled methods for determin-ing stop set size and makeup are discussed.It?s important to allow the examples in the stopset to be queried if the active learner selects thembecause they may be highly informative and rulingthem out could hurt performance.
In preliminary ex-periments we had made the stop set distinct from theset of unlabeled points made available for queryingand we saw performance was qualitatively the samebut the AL curve was translated down by a few F-measure points.
Therefore, we allow the points inthe stop set to be selected during AL.1The essential idea is to compare successive mod-els?
predictions on the stop set to see if they havestabilized.
A simple way to define agreement be-tween two models would be to measure the percent-age of points on which the models make the samepredictions.
However, experimental results on a sep-arate development dataset show then that the cutoffagreement at which to stop is sensitive to the datasetbeing used.
This is because different datasets havedifferent levels of agreement that can be expected bychance and simple percent agreement doesn?t adjustfor this.Measurement of agreement between human anno-tators has received significant attention and in thatcontext, the drawbacks of using percent agreementhave been recognized (Artstein and Poesio, 2008).Alternative metrics have been proposed that takechance agreement into account.
In (Artstein andPoesio, 2008), a survey of several agreement met-rics is presented.
Most of the agreement metrics areof the form:agreement = Ao ?
Ae1 ?
Ae , (1)where Ao = observed agreement, and Ae = agree-ment expected by chance.
The different metrics dif-fer in how they compute Ae.The Kappa statistic (Cohen, 1960) measuresagreement expected by chance by modeling eachcoder (in our case model) with a separate distribu-tion governing their likelihood of assigning a partic-ular category.
Formally, Kappa is defined by Equa-1They remain in the stop set if they?re selected.41tion 1 with Ae computed as follows:Ae =?k?
{+1,?1}P (k|c1) ?
P (k|c2), (2)where each ci is one of the coders (in our case,models), and P (k|ci) is the probability that coder(model) ci labels an instance as being in category k.Kappa estimates P (k|ci) based on the proportion ofobserved instances that coder (model) ci labeled asbeing in category k.We have found Kappa to be a robust parameterthat doesn?t require tuning when moving to a newdataset.
On a separate development dataset, a Kappacutoff of 0.99 worked well.
All of the experiments(except those in Table 2) in the current paper used anagreement cutoff of Kappa = 0.99 with zero tuningperformed.
We will see in Section 4 that this cutoffdelivers robust results across all of the folds for allof the datasets.The Kappa cutoff captures the intensity of theagreement that must occur before SP will concludeto stop.
Though an intensity cutoff of K=0.99 isan excellent default (as seen by the results in Sec-tion 4), one of the advantages of the SP method isthat by giving users the option to vary the intensitycutoff, users can control how aggressive SP will be-have.
This is explored further in Section 4.Another way to give users control over stoppingbehavior is to give them control over the longevityfor which agreement (at the specified intensity) mustbe maintained before SP concludes to stop.
The sim-plest implementation would be to check the mostrecent model with the previous model and stop iftheir agreement exceeds the intensity cutoff.
How-ever, independent of wanting to provide users witha longevity control, this is not an ideal approach be-cause there?s a risk that these two models could hap-pen to highly agree but then the next model will nothighly agree with them.
Therefore, we propose us-ing the average of the agreements from a windowof the k most recent pairs of models.
If we call themost recent model Mn, the previous model Mn?1and so on, with a window size of 3, we average theagreements between Mn and Mn?1, between Mn?1and Mn?2, and between Mn?2 and Mn?3.
On sepa-rate development data a window size of k=3 workedwell.
All of the experiments (except those in Ta-ble 3) in the current paper used a longevity windowsize of k=3 with zero tuning performed.
We willsee in Section 4 that this longevity default deliversrobust results across all of the folds for all of thedatasets.
Furthermore, Section 4 shows that varyingthe longevity requirement provides users with an-other lever for controlling how aggressively SP willbehave.4 Evaluation and Discussion4.1 Experimental SetupWe evaluate the Stabilizing Predictions (SP) stop-ping method on multiple datasets for Text Classifi-cation (TC) and Named Entity Recognition (NER)tasks.
All of the datasets are freely and publiclyavailable and have been used in many past works.For Text Classification, we use two publicly avail-able spam corpora: the spamassassin corpus used in(Sculley, 2007) and the TREC spam corpus trec05p-1/ham25 described in (Cormack and Lynam, 2005).For both of these corpora, the task is a binary clas-sification task and we perform 10-fold cross valida-tion.
We also use the Reuters dataset, in particularthe Reuters-21578 Distribution 1.0 ModApte split2.Since a document may belong to more than one cat-egory, each category is treated as a separate binaryclassification problem, as in (Joachims, 1998; Du-mais et al, 1998).
Consistent with (Joachims, 1998;Dumais et al, 1998), results are reported for the tenlargest categories.
Other TC datasets we use are the20Newsgroups3 newsgroup article classification andthe WebKB web page classification datasets.
ForWebKB, as in (McCallum and Nigam, 1998; Zhu etal., 2008a; Zhu et al, 2008b) we use the four largestcategories.
For all of our TC datasets, we use binaryfeatures for every word that occurs in the trainingdata at least three times.For NER, we use the publicly available GENIAcorpus4.
Our features, based on those from (Lee etal., 2004), are surface features such as the words in2http://www.daviddlewis.com/resources/testcollections/reuters215783We used the ?bydate?
version of the dataset downloadedfrom http://people.csail.mit.edu/jrennie/20Newsgroups/.
Thisversion is recommended since it makes cross-experiment com-parison easier since there is no randomness in the selection oftrain/test splits.4http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/home/wiki.cgi?page=GENIA+Project42the named entity and two words on each side, suf-fix information, and positional information.
We as-sume a two-phase model where boundary identifica-tion has already been performed, as in (Lee et al,2004).SVMs deliver high performance for the datasetswe use so we employ SVMs as our base learnerin the bulk of our experiments (maximum entropymodels are used in Subsection 4.3).
For selection ofpoints to query, we use the approach that was usedin (Tong and Koller, 2002; Schohn and Cohn, 2000;Campbell et al, 2000) of selecting the points that areclosest to the current hyperplane.
We use SVMlight(Joachims, 1999) for training the SVMs.
For thesmaller datasets (less than 50,000 examples in total),a batch size of 20 was used with an initial trainingset of size 100 and for the larger datasets (greaterthan 50,000 examples in total), a batch size of 200was used with an initial training set of size 1000.4.2 Main ResultsTable 1 shows the results for all of our datasets.
Foreach dataset, we report the average number of anno-tations5 requested by each of the stopping methodsas well as the average F-measure achieved by eachof the stopping methods.6There are two facts worth keeping in mind.
First,the numbers in Table 1 are averages and therefore,sometimes two methods could have very similaraverage numbers of annotations but wildly differ-ent average F-measures (because one of the meth-ods was consistently stopping around its averagewhereas the other was stopping way too early andway too late).
Second, sometimes a method with ahigher average number of annotations has a lower5Better evaluation metrics would use more refined measuresof annotation effort than the number of annotations because notall annotations require the same amount of effort to annotate butlacking such a refined model for our datasets, we use number ofannotations in these experiments.6Tests of statistical significance are performed usingmatched pairs t tests at a 95% confidence level.7(Vlachos, 2008) suggests using three drops in a row to de-tect a consistent drop in confidence so we do the same in ourimplementation of the method from (Vlachos, 2008).8Following (Zhu et al, 2008b), we set the starting accuracythreshold to 0.9 when reimplementing their method.9(Laws and Schu?tze, 2008) uses a window of size 100and a threshold of 0.00005 so we do the same in our re-implementation of their method.average F-measure than a method with a lower aver-age number of annotations.
This can be caused be-cause of the first fact just mentioned about the num-bers being averages and/or this can also be causedby the ?less is more?
phenomenon in active learn-ing where often with less data, a higher-performingmodel is learned than with all the data; this wasfirst reported in (Schohn and Cohn, 2000) and sub-sequently observed by many others (e.g., (Vlachos,2008; Laws and Schu?tze, 2008)).There are a few observations to highlight regard-ing the performance of the various stopping meth-ods:?
SP is the most parsimonious method in termsof annotations.
It stops the earliest and remark-ably it is able to do so largely without sacrific-ing F-measure.?
All the methods except for SP and SC2000 areunstable in the sense that on at least one datasetthey have a major failure, either stopping waytoo late and wasting large numbers of anno-tations (e.g.
ZWH2008 and V2008 on TRECSpam) or stopping way too early and losinglarge amounts of F-measure (e.g.
LS2008 onNER-Protein) .?
It?s not always clear how to evaluate stoppingmethods because the tradeoff between the valueof extra F-measure versus saving annotations isnot clearly known and will be different for dif-ferent applications and users.This last point deserves some more discussion.
Insome cases it is clear that one stopping method isthe best.
For example, on WKB-Project, the SPmethod saves the most annotations and has the high-est F-measure.
But which method performs thebest on NER-DNA?
Arguments can reasonably bemade for SP, SC2000, or ZWH2008 being the bestin this case depending on what exactly the anno-tation/performance tradeoff is.
A promising direc-tion for research on AL stopping methods is to de-velop user-adjustable stopping methods that stop asaggressively as the user?s annotation/performancepreferences dictate.One avenue of providing user-adjustable stoppingis that if some methods are known to perform con-sistently in an aggressive manner against annotating43Task-Dataset SP V20087 SC2000 ZWH20088 LS20089 AllTREC-SPAM 2100 56000 3900 29220 3160 56000(10-fold AVG) 98.33 98.47 98.41 98.44 96.63 98.4720Newsgroups 678 181 1984 1340 1669 11280(20-cat AVG) 60.85 18.06 55.43 60.72 54.79 54.81Spamassassin 326 4362 862 398 1176 5400(10-fold AVG) 94.57 95.00 95.53 95.94 95.62 95.63NER-protein 8720 67220 17680 18580 2360 67220(10-fold AVG) 89.48 90.28 90.38 90.31 76.47 90.28NER-DNA 4020 67220 10640 7200 3900 67220(10-fold AVG) 82.40 84.31 84.73 84.51 74.74 84.31NER-cellType 3840 29600 5540 11580 4580 67220(10-fold AVG) 86.15 86.87 87.19 87.32 85.65 87.83Reuters 484 6762 1196 650 1272 9580(10-cat AVG) 74.29 65.81 73.88 76.77 74.00 75.64WKB-Course 790 184 1752 912 1740 7420(10-fold AVG) 83.12 30.34 80.47 83.16 80.55 80.19WKB-Faculty 808 892 1932 1062 1818 7420(10-fold AVG) 81.53 40.14 81.79 81.64 81.99 82.36WKB-Project 646 916 1358 794 1482 7420(10-fold AVG) 63.30 25.33 58.11 61.82 59.30 61.19WKB-Student 1258 894 2400 1468 2150 7420(10-fold AVG) 84.70 50.66 83.46 84.39 83.19 83.30Average 2152 21294 4477 6655 2301 28509(macro-avg) 81.70 62.30 80.85 82.27 78.45 81.27Table 1: Methods for stopping AL.
For each dataset, the average number of annotations at the automatically determinedstopping points and the average F-measure at the automatically determined stopping points are displayed.
Bold entriesare statistically significantly different than SP (and non-bold entries are not).
The Average row is simply an unweightedmacro-average over all the datasets.
The final column (labeled ?All?)
represents standard fully supervised passivelearning with the entire set of training data.too much while others are known to perform consis-tently in a conservative manner, then users can pickthe stopping criterion that?s more suitable for theirparticular annotation/performance valuation.
Forthis purpose, SP fills a gap as the other stopping cri-teria seem to be conservative in the sense definedin Section 1.
SP, on the other hand, is more of anaggressive stopping criterion and is less likely to an-notate data that is not needed.A second avenue for providing user-adjustablestopping is a single stopping method that is itself ad-justable.
To this end, Section 4.3 shows how inten-sity and longevity provide levers that can be used tocontrol the behavior of SP in a controlled fashion.Sometimes viewing the stopping points of the var-ious criteria on a graph with the active learning curvecan help one visualize how the methods perform.Figure 2 shows the graph for a representative fold.10The x-axis measures the number of human annota-tions that have been requested so far.
The y-axismeasures performance in terms of F-Measure.
Thevertical lines are where the various stopping meth-ods would have stopped AL if we hadn?t continuedthe simulation.
The figure reinforces and illustrateswhat we have seen in Table 1, namely that SP stopsmore aggressively than existing criteria and is able10It doesn?t make sense to show a graph for the average overcross validation because the average number of annotations atthe stopping point may cross the learning curve at a completelymisleading point.
Consider a method that stops way too earlyand way too late at times.440 1 2 3 4 5 6 7x 10460657075808590Number of Human Annotations RequestedPerformance(F?Measure)DNA Fold 1SC2000LS2008ZWH2008V2008SPFigure 2: Graphic with stopping criteria in action for fold1 of NER of DNA from the GENIA corpus.
The x-axisranges from 0 to 70,000.to do so without sacrificing performance.4.3 Additional ExperimentsAll of the additional experiments in this subsectionwere conducted on our least computationally de-manding dataset, Spamassassin.
The results in Ta-bles 2 and 3 show how varying the intensity cut-off and the longevity requirement, respectively, ofSP enable a user to control stopping behavior.
Bothmethods enable a user to adjust stopping in a con-trolled fashion (without radical changes in behav-ior).
Areas of future work include: combining theintensity and longevity methods for controlling be-havior; and developing precise expectations on thechange in behavior corresponding to changes in theintensity and longevity settings.The results in Table 4 show results for differentstop set sizes.
Even with random selection of a stopset as small as 500, SP?s performance holds fairlysteady.
This plus the fact that random selection ofstop sets of size 2000 worked across all the folds ofall the datasets in Table 1 show that in practice per-haps the simple heuristic of choosing a fairly largerandom set of points works well.
Nonetheless, wethink the size necessary will depend on the datasetand other factors such as the feature representationso more principled methods of determining the sizeand/or the makeup of the stop set are an area forfuture work.
For example, construction techniquesIntensity Annotations F-MeasureK=99.5 364 96.01K=99.0 326 94.57K=98.5 304 95.59K=98.0 262 93.75K=97.5 242 93.35K=97.0 224 90.91Table 2: Controlling the behavior of stopping through theuse of intensity.
For Kappa intensity levels in {97.0, 97.5,98.0, 98.5, 99.0, 99.5}, the 10-fold average number of an-notations at the automatically determined stopping pointsand the 10-fold average F-measure at the automaticallydetermined stopping points are displayed for the Spamas-sassin dataset.Longevity Annotations F-Measurek=1 284 95.17k=2 318 94.95k=3 326 94.57k=4 336 95.40k=5 346 96.41k=6 366 94.53Table 3: Controlling the behavior of stopping through theuse of longevity.
For window length k longevity levels in{1, 2, 3, 4, 5, 6}, the 10-fold average number of annota-tions at the automatically determined stopping points andthe 10-fold average F-measure at the automatically deter-mined stopping points are displayed for the Spamassassindataset.could be developed to create stop sets with high rep-resentativeness (in terms of feature space) density(meaning representativeness of stop set divided bysize of stop set).
For example, a possibility is tocluster examples before AL begins and then makesure the stop set contains examples from each of theclusters.
Another possibility is to use a greedy algo-rithm where the stop set is iteratively grown whereon each iteration the center of mass of the stop setin feature space is computed and an example in theunlabeled pool that is maximally far in feature spacefrom this center of mass is selected for inclusion inthe stop set.
This could be useful for efficiency (interms of getting the same stopping performance witha smaller stop set as could be achieved with a largerstop set) and also as a way to ensure adequate repre-sentation of the task space.
The latter can be accom-45Task-Dataset SP V2008 ZWH2008 LS2008 AllSpamassassin 286 1208 386 756 5400(10-fold AVG) 94.92 89.89 95.31 96.40 91.74Table 5: Methods for stopping AL with maximum entropy as the base learner.
For each stopping method, the averagenumber of annotations at the automatically determined stopping point and the average F-measure at the automaticallydetermined stopping point are displayed.
Bold entries are statistically significantly different than SP (and non-boldentries are not).
SC2000, the margin exhaustion method, is not shown since it can?t be used with a non-margin-basedlearner.
The final column (labeled ?All?)
represents standard fully supervised passive learning with the entire set oftraining data.Stop Set Size Annotations F-Measure2500 326 95.582000 326 94.571500 314 95.001000 328 95.73500 314 94.57Table 4: Investigating the sensitivity to stop set size.
Forstop set sizes in {2500, 2000, 1500, 1000, 500}, the 10-fold average number of annotations at the automaticallydetermined stopping points and the 10-fold average F-measure at the automatically determined stopping pointsare displayed for the Spamassassin dataset.plished by perhaps continuing to add examples tothe stop set until adding new examples is no longerincreasing the representativeness of the stop set.As one of the advantages of SP is that it?s widelyapplicable, Table 5 shows the results when usingmaximum entropy models as the base learner dur-ing AL (the query points selected are those whichthe model is most uncertain about).
The results re-inforce our conclusions from the SVM experiments,with SP performing aggressively and all statisticallysignificant differences in performance being in SP?sfavor.
Figure 3 shows the graph for a representativefold.5 ConclusionsEffective methods for stopping AL are crucial for re-alizing the potential annotation savings enabled byAL.
A survey of existing stopping methods identi-fied three areas where improvements are called for.The new stopping method based on Stabilizing Pre-dictions (SP) addresses all three areas: SP is widelyapplicable, stable, and aggressive in saving annota-tions.0 1000 2000 3000 4000 5000 60005060708090100Number of Human Annotations RequestedPerformance(F?Measure)AL?MaxEnt: Spamassassin Fold 5SPZWH2008LS2008V2008Figure 3: Graphic with stopping criteria in action for fold5 of TC of the spamassassin corpus.
The x-axis rangesfrom 0 to 6,000.The empirical evaluation of SP and the existingmethods was informative for evaluating the crite-ria but it was also informative for demonstrating thedifficulties for rigorous objective evaluation of stop-ping criteria due to different annotation/performancetradeoff valuations.
This opens up a future area forwork on user-adjustable stopping.
Two potentialavenues for enabling user-adjustable stopping are asingle criterion that is itself adjustable or a suite ofmethods with consistent differing levels of aggres-siveness/conservativeness from which users can pickthe one(s) that suit their annotation/performancetradeoff valuation.
SP substantially widens the rangeof behaviors of existing methods that users canchoose from.
Also, SP?s behavior itself can be ad-justed through user-controllable parameters.46ReferencesRon Artstein and Massimo Poesio.
2008.
Inter-coderagreement for computational linguistics.
Computa-tional Linguistics, 34(4):555?596.Jason Baldridge and Miles Osborne.
2008.
Active learn-ing and logarithmic opinion pools for hpsg parse se-lection.
Nat.
Lang.
Eng., 14(2):191?222.Michael Bloodgood and K. Vijay-Shanker.
2009.
Takinginto account the differences between actively and pas-sively acquired data: The case of active learning withsupport vector machines for imbalanced datasets.
InNAACL.Colin Campbell, Nello Cristianini, and Alex J. Smola.2000.
Query learning with large margin classifiers.In ICML ?00: Proceedings of the Seventeenth Interna-tional Conference on Machine Learning, pages 111?118, San Francisco, CA, USA.
Morgan KaufmannPublishers Inc.J.
Cohen.
1960.
A coefficient of agreement for nominalscales.
Educational and Psychological Measurement,20:37?46.Gordon Cormack and Thomas Lynam.
2005.
Trec 2005spam track overview.
In TREC-14.Susan Dumais, John Platt, David Heckerman, andMehran Sahami.
1998.
Inductive learning algorithmsand representations for text categorization.
In CIKM?98: Proceedings of the seventh international con-ference on Information and knowledge management,pages 148?155, New York, NY, USA.
ACM.Seyda Ertekin, Jian Huang, Le?on Bottou, and C. LeeGiles.
2007.
Learning on the border: active learn-ing in imbalanced data classification.
In Ma?rio J.Silva, Alberto H. F. Laender, Ricardo A. Baeza-Yates,Deborah L. McGuinness, Bj?rn Olstad, ?ystein HaugOlsen, and Andre?
O. Falca?o, editors, Proceedings ofthe Sixteenth ACM Conference on Information andKnowledge Management, CIKM 2007, Lisbon, Portu-gal, November 6-10, 2007, pages 127?136.
ACM.Thorsten Joachims.
1998.
Text categorization with su-port vector machines: Learning with many relevantfeatures.
In ECML, pages 137?142.Thorsten Joachims.
1999.
Making large-scale SVMlearning practical.
In Advances in Kernel Methods ?Support Vector Learning, pages 169?184.Florian Laws and Hinrich Schu?tze.
2008.
Stopping crite-ria for active learning of named entity recognition.
InProceedings of the 22nd International Conference onComputational Linguistics (Coling 2008), pages 465?472, Manchester, UK, August.
Coling 2008 Organiz-ing Committee.Ki-Joong Lee, Young-Sook Hwang, Seonho Kim, andHae-Chang Rim.
2004.
Biomedical named entityrecognition using two-phase model based on svms.Journal of Biomedical Informatics, 37(6):436?447.Andrew McCallum and Kamal Nigam.
1998.
A compar-ison of event models for naive bayes text classification.In Proceedings of AAAI-98, Workshop on Learning forText Categorization.Greg Schohn and David Cohn.
2000.
Less is more: Ac-tive learning with support vector machines.
In Proc.17th International Conf.
on Machine Learning, pages839?846.
Morgan Kaufmann, San Francisco, CA.D.
Sculley.
2007.
Online active learning methods for fastlabel-efficient spam filtering.
In Conference on Emailand Anti-Spam (CEAS), Mountain View, CA, USA.Simon Tong and Daphne Koller.
2002.
Support vec-tor machine active learning with applications to textclassification.
Journal of Machine Learning Research(JMLR), 2:45?66.Andreas Vlachos.
2008.
A stopping criterion for activelearning.
Computer Speech and Language, 22(3):295?312.Jingbo Zhu and Eduard Hovy.
2007.
Active learningfor word sense disambiguation with methods for ad-dressing the class imbalance problem.
In Proceedingsof the 2007 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning (EMNLP-CoNLL), pages783?790.Jingbo Zhu, Huizhen Wang, and Eduard Hovy.
2008a.Learning a stopping criterion for active learning forword sense disambiguation and text classification.
InIJCNLP.Jingbo Zhu, Huizhen Wang, and Eduard Hovy.
2008b.Multi-criteria-based strategy to stop active learning fordata annotation.
In Proceedings of the 22nd Interna-tional Conference on Computational Linguistics (Col-ing 2008), pages 1129?1136, Manchester, UK, Au-gust.
Coling 2008 Organizing Committee.47
