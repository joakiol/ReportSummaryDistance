Word Sense Disambiguation for Cross-Language InformationRetrievalMary Xiaoyong Liu, Ted Diamond, and Anne R. DiekemaSchool of  Information StudiesSyracuse UniversitySyracuse, NY 13244xliu03@mailbox.syr.edut..diar0onl @twcny.rr.comdiekemar@griailbox, syr.
eduAbstractWe have developed a word sensedisambiguation algorithm, following Cheng andWilensky (1997), to disambiguate amongWordNet synsets.
This algorithm is to be used ina cross-language information retrieval system,CINDOR, which indexes queries and documentsin a language-neutral concept representationbased on WordNet synsets.
Our goal is toimprove retrieval precision through word sensedisambiguation.
An evaluation against humandisambiguation judgements suggests promise forour approach.1 IntroductionThe CINDOR cross-language informationretrieval system (Diekema et al, 1998) uses aninformation structure known as "conceptualinterlingua" for query and documentrepresentation.
This conceptual interlingua is ahierarchically organized multilingual conceptlexicon, which is structured following WordNet(Miller, 1990).
By representing query anddocument terms by their WordNet synsetnumbers we arrive at essentially a languageneutral representation consisting of synsetnumbers representing concepts.
Thisrepresentation facilitates cross-language retrievalby matching tea-m synonyms in English as well asacross languages.
However, many terms arepolysemous and belong to multiple synsets,resulting in spurious matches in retrieval.
Thenounfigure for example appears in 13 synsets inWordNet 1.6.
This research paper describes theearly stages I of our efforts to develop a wordsense disambiguation (WSD) algorithm aimed atimproving the precision of our cross-languageretrieval system.2 Related WorkTo determine the sense of a word, a WSDalgorithm typically uses the context of theambiguous word, external resources such asmachine-readable dictionaries, or a combinationof both.
Although dictionaries provide usefulword sense information and thesauri provideadditional information about relatiomhipsbetween words, they lack pragmatic informationas can be found in corpora.
Corpora containexamples of words that enable the development ofstatistical models of word senses and theircontexts (Ide and Veronis, 1998; Leacock andChodorow, 1998).There are two general problems with usingcorpora however; 1) corpora typically do notcome pre-tagged with manually disambiguatedsenses, and 2) corpora are often not large nordiverse nough for all senses of a word to appearoften enough for reliable statistical models (datasparseness).
Although researchers have triedsense-tagging corpora automatically by usingeither supervised or unsupervised trainingmethods, we have adopted a WSD algorithmwhich avoids the necessity for a sense-taggedtraining corpus.l Please note that he disambiguation researchdescribed inthis paper has not yet been extended tomultiple language areas.35P(synsetlcontext(w)) =P(context(w) I synset) P(synset)P(context(w))(I)The problem of data sparseness i usuallysolved by using either smoothing methods, class-based methods, or by relying on similarity-basedmethods between words and co-occurrence data.Since we are using a WordNet-based resource forretrieval, using class-based methods seems anatural choice.
Appropriate word classes can beformed by synsets or groups of synsets.
Theevidence of a certain sense (synset) is then nolonger dependent on one word but on all themembers of a particular synset.Yarowsky (1992) used Rogets Thesauruscategories as classes for WSD.
His approach wasbased on selecting the most likely Roget categoryfor nouns given their context of 50 words oneither side.
When any of the category indicatorwords appeared in the context of an ambiguousword, the indicator weights for each categorywere summed to determine the most likelycategory.
The category with the largest sum wasthen selected.A similar approach to that of Yarowsky wasfollowed by Cheng and Willensky (1997) whoused a training matrix of associations of wordswith a certain category.
Their algorithm wasappealing to us because it requires no humanintervention, and more importantly, it avoids theuse of sense-tagged ata.
Our methodologydescribed in the next section is therefore based onCheng and Wilensky's approach.Methods to reduce (translation) ambiguity incross-language information retrieval haveincluded using part-of-speech taggers to restrictthe translation options (Davis 1997), applyingpseudo-relevance feedback loops to expand thequery with better terms aiding translation(Ballesteros and Croft 1997), using corpora forterm translation disambiguation (Ballesteros andCroft, 1998), and weighted Boolean modelswhich tend to have a self-disambiguating quality(Hull, 1997; Diekema et al, 1999; Hiemstra ndKraaij, 1999).3 MethodologyTo disambiguate a given word, we would liketo know the probability that a sense occurs in agiven context, i.e., P(semse\[context).
In this study,WordNet synsets are used to represent wordsenses, so P(senselcontext) can be rewritten asP(synsetlcontext), for each synset of which thatword is a member.
For nouns, we define thecontext of word w to be the occurrence of wordsin a moving window of I00 words (50 words oneach side) around w 2.By Bayes Theorem, we can obtain the desiredprobability by inversion (see equation (I)).
Sincewe are not specifically concerned with gettingaccurate probabilities but rather relative rankorder for sense selection, we ignore P(context(w))and focus on estimatingP(context(w)lsymet)P(synset).
The event spacel~om which "context(w)" is drawn is the set ofsets of words that ever appear with each other inthe window around w. In other words, w inducesa partition on the set of words.
We define"context(w)" to be true whenever any of thewords in the set appears in the window around w,and conversely to be false whenever none of thewords in the set appears around w. If we assumeindependence of appearance of any two words ina given context, then we get:P(synset)?
(1- l-I(1-P(wilsynset))) (2)wiecontextDue to the lack of sense-tagged corpora, weare not able to directly estimate P(synset) andP(wilsymet).
Instead, we introduce "noisyestimators" (Pdsymet) and Pdwl\]symet)) toapproximate hese probabilities.
In doing so, wemake two assumptions: l) The presence of anyword Wk that belongs to synset si signals thepresence of si; 2) Any word Wk belongs to all itssynsets simultaneously, and with equalprobability.
Although the assumptions underlyingthe "noisy estimators" are not strictly true, it isour belief that the "noisy estimators" hould workreasonably well if:?
The words that belong to symet sitend toappear in similar contexts when si is theirintended sense;?
These words do not completely overlapwith the words belonging to some synsetsj ( i ~ j ) that partially overlaps with si;2 For other parts of speech, the window size shouldbe much smaller as suggested by previous research.36The common words between si and sjappear in different contexts when si andsj are their intended senses.4 The WSD AlgorithmWe chose as a basis the algorithms describedby Yan'owsky (1992) and by Cheng andWilensky (1997).
In our variation, we use thesynset numbers in WordNet to represent hesenses of a word.
Our algorithm learnsassociations of WordNet synsets with words in asurrounding context o determine a word sense.
Itconsists of two phases.During the training phase, the algorithmreads in all training documents in collection andcomputes the distance-adjusted weight of co-occurrence of each word with each correspondingsynset.
This is done by establishing a 100-wordwindow around a target word (50 words on eachside), and correlating each synset to which thetarget word belongs with each word in thesurrounding window.
The result of the trainingphase is a matrix of associations of words withsynsets.In the sense prediction phase, the algorithmtakes as input randomly selected testingdocuments or sentences that contain thepolysemous words we want to disambiguate andexploits the context vectors built in the trainingphase by adding up the weighted "votes".
It thenreturns a ranked list of probability valuesassociated with each synset, and chooses thesynset with the highest probability as the sense ofthe ambiguous word.Figure 1 and Figure 2 show an outline of thealgorithm.In this algorithm, "noisy estimators" areemployed in the sense prediction phase.
They arecalculated using following formulas:M\[w, Ix\]Po(wilx)-- LwM\[wIx\] (3)where wi is a stem, x is a given synset,M\[w\]\[x\] is a cell in the correlation matrix thatcorresponds to word w and synset x, andZ.,,,,M\[wlx\]P~(x)= Z,,~w.y~rM\[wly \] (4)where w is any stem in the collection, xis a given symet, y is any synset ever occurred incollection.For each document d in collectionread in a noun stem w from dfor each synset s in which w occursget the column b in the association matrix M that corresponds tos if the column alreadyexists; create anew column for s otherwisefor each word stem j appearing in the 100-word window around wget the row a in M that corresponds toj if the row already exists; create anewrow for j otherwiseadd a distance-adjusted weight o M\[a\]\[b\]Figure 1: WSD Algorithm: the training phaseSet value = 1For each word w to be disambiguatedget synsets of wfor each synset x ofwfor each wi in the context ofw (within the 100-window around w)calculate Pc(wilx)value *= ( 1 - Pc(wilx))P(context(w)lx) = 1 - valueCalculate pc(x)P(xlcontext(w)) =p~(x)* P(eontext(w)lx)display a ranked list of the synsets arranged according to their P(xlcontext(w)) in decreasingorderFigure 2: WSD Algorithm: the sense prediction phase375 EvaluationAs suggested by the WSD literature,evaluation of word sense disambiguation systemsis not yet standardized (Resnik and Yarowsky,1997).
Some WSD evaluations have been doneusing the Brown Corpus as training and testingresources and comparing the results againstSemCor 3, the sense-tagged version of the BrownCorpus (Agirre and Rigau, 1996; Gonzalo et al,1998).
Others have used common test suites suchas the 2094-word line data of Leacock et al(1993).
Still others have tended to use their ownmetrics.
We chose an evaluation with a user-based component that allowed a ranked list ofsense selection for each target word and enableda comprehensive comparison between automaticand manual WSD results.
In addition we wantedto base the disambiguation matrix on a corpusthat we use for retrieval.
This approach allowsfor a much richer evaluation than a simple hit-or-miss test.
For vahdation purpose, we will conducta fully automatic evaluation against SemCor inour future efforts.We use in vitro evaluation in this study, i.e.the WSD algorithm is tested independent of theretrieval system.
The population consists of allthe nouns in WordNet, after removal ofmonoseanous nouns, and after removal of aproblematic lass of polysemous nouns.
4 Wedrew a random sample of 87 polysemous nouns 5from this population.In preparation, for each noun in our samplewe identified all the documents containing thatnoun from the Associated Press (AP) newspapercorpus.
The testing document set was thenformed by randomly selecting 10 documents fromthe set of identified ocuments for each of the 87nouns.
In total, there are 867 documents in the3 SemCor is a semantically sense-tagged corpuscomprising approximately 250, 000 words.
Thereported error rate is around 10% for polysemouswords.4 This class of nouns refers to nouns that are insynsets in which they are the sole word, or in synsetswhose words were subsets of other synsets for thatnoun.
This situation makes disambiguationextremely problematic.
This class of noun will bedealt with in a future version of our algorithm but fornow it is beyond the scope of this evaluation.5 A polysemous noun is defined as a noun thatbelongs to two or more synsets.testing set.
The training document set consists ofall the documents in the AP corpus excluding theabove-mentioned 867 documents.
For each nounin our sample, we selected all its correspondingWordNet noun synsets and randomly selected 10sentence occurrences with each from one of the10 random documents.After collecting 87 polysemous nouns with10 noun sentences each, we had 870 sentences fordisambiguation.
Four human judges wererandomly assigned to two groups with two judgeseach, and each judge was asked to disambiguate275 word occurrences out of which 160 wereunique and 115 were shared with the other judgein the same group.
For each word occurrence, thejudge put the target word's possible senses inrank order according to their appropriatenessgiven the context (ties are allowed).Our WSD algorithm was also fed with theidentical set of 870 word occurrences in the senseprediction phase and produced a ranked hst ofsenses for each word occurrence.Since our study has a matched-group designin which the subjects (word occurrences) receiveboth the treatments and control, the measurementof variables is on an ordinal scale, and there is noapparently applicable parametric statisticalprocedure available, two nonparametricprocedures -the Friedman two-way analysis ofvariance and the Spearman rank correlationcoefficient -were originally chosen as candidatesfor the statistical analysis of our results.However, the number of ties in our resultsrenders the Spearman coefficient unreliable.
Wehave therefore concentrated on the Friedmananalysis of our experimental results.
We use thetwo-alternative test with o~=0.05.The first tests of interest were aimed atestabhshing inter-judge reliability across the 115shared sentenees by each pair of judges.
The nullhypothesis can be generalized as "There is nodifference in judgments on the same wordoccurrences between two judges in the samegroup".
Following general steps of conducting aFriedman test as described by Siegel (1956), wecast raw ranks in a two-way table having 2conditions/columns (K = 2) with each of thehuman judges in the pair serving as one conditionand 365 subjects/rows (N = 365) which are allthe senses of the 115 word occurrences that werejudged by both human judges.
We then ranked38N K Xr 2 df Rejection region Reject H0?First pair of judges 365 2 .003 1 3.84 NoSecond pair of judges 380 2 2.5289 1 3.84 NoFigure 3."
Statistics for significance tests of inter-judge reliability (ct=.05, 2-alt.
Test)Auto WSD vs man.
WSDVS sense poo,UngAuto WSD vs man.
WSDAuto WSD vs sense poolingMan.
WSD vs sense poolingN2840284028402840K x?3 73.2172 3.73562 5.95072 126.338df2Rejection region Reject H9?5.99 Yes3.84 No3.84 Yes3.84 YesFigure 4: Statistics for significance tests among automatic WSD, manual WSD,and sense pooling (ct=.05, 2-alt.
TesOthe scores in each row from 1 to K (in this case Kis 2), summed the derived ranks in each column,and calculated X\[ which is .003.
For ct=0.05,degrees of freedom df = 1 (df = K -1), therejection region starts at 3.84.
Since .003 issmaller than 3.84, the null hypothesis is notrejected.
Similar steps were used for analyzingreliability between the second pair of judges.
Inboth cases, we did not find significant differencebetween judges (see Figure 3).Our second area of interest was thecomparison of automatic WSD, manual WSD,and "sense pooling".
Sense pooling equates to nodisambiguation, where each sense of a word isconsidered equally likely (a tie).
The nullhypothesis (H0) is "There is no difference amongmanual WSD, automatic WSD, and sensepooling (all the conditions come from the samepopulation)".
The steps for Friedman analysiswere similar to what we did for the inter-judgereliability test while the conditions and subjectswere changed in each test according to what wewould like to compare.
Test results aresummarized in Figure 4.
In the three-waycomparison shown in the first row of the table,we rejected H0 so there was at least one conditionthat was from a different population.
By furtherconducting tests which examined each two of theabove three conditions at a time we found that itwas sense pooling that came from a differentpopulation while manual and automatic WSDwere not significantly different.
We can thereforeconclude that our WSD algorithm is better thanno disambiguation.6 Concluding RemarksThe ambiguity of words may negativelyimpact the retrieval performance of a concept-based information retrieval system like CINDOR.We have developed a WSD algorithm that usesall the words in a WordNet symet as evidence ofa given sense and builds an association matrix tolearn the co-occurrence between words andsenses.
An evaluation of our algorithm againsthuman judgements of a small sample of nounsdemonstrated no significant difference betweenour automatic ranking of senses and the humanjudgements.
There was, however, a significantdifference between human judgement andrankings produced with no disambiguation whereall senses were tied.These early results are such as to encourageus to continue our research in this area.
In ourfuture work we must tackle issues associatedwith the fine granularity of some WordNet sensedistinctions, synsets which are proper subsets ofother synsets and are therefore impossible todistinguish, and also extend our evaluation tomultiple languages and to other parts of speech.The next step in our work will be to evaluate ourWSD algorithm against the manually sense-tagged SemCor Corpus for validation, and thenintegrate our WSD algorithm into CINDOR'sprocessing and evaluate directly the impact onretrieval performance.
We hope to verify thatword sense disambiguation leads to improvedprecision in cross-language retrieval.AcknowledgementsThis work was completed under a researchpracticum at MNIS-TextWise Labs, Syracuse,NY.
We thank Paraie Sheridan for many usefuldiscussions and the anonymous reviewers forconstructive comments on the manuscript.39ReferencesAgirre, E., and Rigau, G. (1996).
Word sensedisambiguation using conceptual density.
In:Proceedings of the 16th InternationalConference on Computational Linguistics,Copenhagen, 1996.Ballesteros, L., and Croft, B.
(1997).
PhrasalTranslation and Query Expansion Techniquesfor Cross-Language Information Retrieval.
In:Proceedings of the Association for ComputingMachinery Special lnterest Group onInformation Retrieval (ACM/SIGIR) 20thInternational Conference on Research andDevelopment in Information Retrieval; 1997July 25-31; Philadelphia, PA. New York, NY:ACM, 1997.84-91.Ballesteros, L., and CroR, B.
(1998).
ResolvingAmbiguity for Cross-language Retrieval.
In:Proceedings of the Association for ComputingMachinery Special lnterest Group onInformation Retrieval (ACM/SIGIR) 21stInternational Conference on Research andDevelopment in Information Retrieval; 1998August 24-28; Melbourne, Australia.
New York,NY: ACM, 1998.
64-71.Cheng, I., and Wilensky, R. (1997).
An Experimentin Enhancing Information Access by NaturalLanguage Processing.
UC Berkeley ComputerScience Technical Report UCB/CSDUCB//CSD-97-963.Davis, M. (1997).
New Experiments in Cross-language Text Retrieval at NMSU's ComputingResearch Lab.
In: D.K.
Harman, Ed.
The FifthText Retrieval Conference (TREC-5).
1996,November.
National Institute of Standards andTechnology (NIST), Gaithersburg, MD.Diekema, A., Oroumchian, F., Sheridan, P., andLiddy, E. D. (1999).
TREC-7 Evaluation ofConceptual Interlingua Document Retrieval(CINDOR) in English and French.
In: E.M.Voorhees and D.K.
Harman (Eds.)
The SeventhText REtrieval Conference (TREC-7).
1998,November 9-11; National Institute of Standardsand Technology 0NIST), Gaithersburg, MD.169-180.Gonzalo, J., Verdejo, F., Chugur, I., and Cigarran, J.(1998).
Indexing with WordNet synsets canimprove text retrieval.
In: Proceedings of theCOLING/ACL Workshop on Usage of WordNetin Natural Language Processing Systems,Montreal, 1998.Hiemstra, D., and Kraaij, W. (1999).
Twenty-One atTREC-7: Ad-hoc and Cross-language Track.
In:E.M. Voorhees and D.K.
Harman (Eds.)
TheSeventh Text REtrieval Conference (TREC-7).1998, November 9-11; National Institute ofStandards and Technology (NIST),Gaithersburg, MD.
227-238.Hull, D. A.
(1997).
Using Structured Queries forDisambiguation i  Cross-Language InformationRetrieval.
In: American Association forArtificial Intelligence (AAA1) Symposium onCross-Language Text and Speech Retrieval;1997 March 24-26; Palo Alto, CA 1997.84-98.Ide, N., and Veronis, J.
(1998).
Introduction to theSpecial Issue on Word Sense Disambiguation:The State of the Art.
Computational Linguistics,Vol.
24, No.
1, 1-40.Leacock, C., and Chodorow, M. (1998).
CombiningLocal Context and WordNet Similarity for WordSense Identification.
In: Christiane FellbaumrEds.)
WordNet: An Electronic LexicalDatabase.
Cambridge, MA: MIT Press.l_e, acock, C., ToweU, G., and Voorhees, E. (1993).Corpus-based Statistical Sense Resolution.
In:Proceedings, ARPA Human LanguageTechnology Workshop, Plainsboro, NJ.
260-265.Miller, G. (1990).
WordNet: An On-line LexicalDatabase.
International Journal ofLexicography, Vol.
3, No.
4, Special Issue.Resnik, P., and Yarowsky, D. (1997).
A Perspectiveon Word Sense Disambiguation Methods andTheir Evaluation, position paper presented attheACL SIGLEX Workshop on Tagging Text withLexical Semantics: Why, What, and Howl heldApril 4-5, 1997 in Washington, D.C., USA inconjunction with ANLP-97.Siegel, S. (1956).
Nonparametric Statistics for theBehavioral Sciences.
New York: McGraw-Hill,1956.Yarowsky, D. (1992).
Word-Sense DisambiguationUsing Statistical Models of Roger's CategoriesTrained on Large Corpora.
In: Proceedings ofthe Fourteenth International Conference onComputational Linguistics.
Nantes, France.
454-460.40
