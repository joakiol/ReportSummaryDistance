Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conferenceand the Shared Task, pages 85?89, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational LinguisticsUCAM-CORE: Incorporating structured distributional similarity into STSTamara Polajnar Laura Rimell Douwe KielaComputer LaboratoryUniversity of CambridgeCambridge CB3 0FD, UK{tamara.polajnar,laura.rimell,douwe.kiela}@cl.cam.ac.ukAbstractThis paper describes methods that were sub-mitted as part of the *SEM shared task onSemantic Textual Similarity.
Multiple kernelsprovide different views of syntactic structure,from both tree and dependency parses.
Thekernels are then combined with simple lex-ical features using Gaussian process regres-sion, which is trained on different subsets oftraining data for each run.
We found that thesimplest combination has the highest consis-tency across the different data sets, while in-troduction of more training data and modelsrequires training and test data with matchingqualities.1 IntroductionThe Semantic Textual Similarity (STS) shared taskconsists of several data sets of paired passages oftext.
The aim is to predict the similarity that hu-man annotators have assigned to these aligned pairs.Text length and grammatical quality vary betweenthe data sets, so our submissions to the task aimed toinvestigate whether models that incorporate syntac-tic structure in similarity calculation can be consis-tently applied to diverse and noisy data.We model the problem as a combination of ker-nels (Shawe-Taylor and Cristianini, 2004), each ofwhich calculates similarity based on a different viewof the text.
State-of-the-art results on text classifi-cation have been achieved with kernel-based classi-fication algorithms, such as the support vector ma-chine (SVM) (Joachims, 1998), and the methodshere can be adapted for use in multiple kernel classi-fication, as in Polajnar et al(2011).
The kernels arecombined using Gaussian process regression (GPR)(Rasmussen and Williams, 2006).
It is importantto note that the combination strategy described hereis only a different way of viewing the regression-combined mixture of similarity measures approachthat is already popular in STS systems, includingseveral that participated in previous SemEval tasks(Croce et al 2012; Ba?r et al 2012).
Likewise, oth-ers, such as Croce et al(2012), have used tree anddependency parse information as part of their sys-tems; however, we use a tree kernel approach basedon a novel encoding method introduced by Zanzottoet al(2011) and from there derive two dependency-based methods.In the rest of this paper we will describe our sys-tem, which consists of distributional similarity (Sec-tion 2.1), several kernel measures (Section 2.2), anda combination method (Section 2.3).
This will befollowed by the description of our three submissions(Section 3), and a discussion of the results (Sec-tion 4).2 MethodsAt the core of all the kernel methods is either sur-face, distributional, or syntactic similarity betweensentence constituents.
The methods themselves en-code sentences into vectors or sets of vectors, whilethe similarity between any two vectors is calculatedusing cosine.2.1 Distributional SimilarityTarget words are the non-stopwords that occurwithin our training and test data.
The two distri-butional methods we use here both represent target85words as vectors that encode word occurrence withina set of contexts.
The first method is a variation onBEAGLE (Jones and Mewhort, 2007), which con-siders contexts to be words that surround targets.The second method is based on ESA (Gabrilovichand Markovitch, 2007), which considers contexts tobe Wikipedia documents that contain target words.To gather the distributional data with both ofthese approaches we used 316,305 documents fromthe September 2012 snapshot of Wikipedia.
Thetraining corpus for BEAGLE is generated by pool-ing the top 20 documents retrieved by querying theWikipedia snapshot index for each target word in thetraining and test data sets.2.1.1 BEAGLERandom indexing (Kaski, 1998) is a technique fordimensionality reduction where pseudo-orthogonalbases are generated by randomly sampling a distri-bution.
BEAGLE is a model where random indexingis used to represent word co-occurrence vectors in adistributional model.Each context word is represented as a D-dimensional vector of normally distributed randomvalues drawn from the Gaussian distributionN (0, ?2), where ?
=1?Dand D = 4096 (1)A target word is represented as the sum of thevectors of all the context words that occur within acertain context window around the target word.
InBEAGLE this window is considered to be the sen-tence in which the target word occurs; however, toavoid segmenting the entire corpus, we assume thewindow to include 5 words to either side of the tar-get.
This method has the advantage of keeping thedimensionality of the context space constant evenif more context words are added, but we limit thecontext words to the top 10,000 most frequent non-stopwords in the corpus.2.1.2 ESAESA represents a target word as a weightedranked list of the top N documents that contain theword, retrieved from a high quality collection.
Weused the BM25F (Robertson et al 2004) weightingfunction and the topN = 700 documents.
These pa-rameters were chosen by testing on the WordSim353dataset.1 The list of retrieved documents can be rep-resented as a very sparse vector whose dimensionsmatch the number of documents in the collection,or in a more computationally efficient manner asa hash map linking document identifiers to the re-trieval weights.
Similarity between lists was calcu-lated using the cosine measure augmented to workon the hash map data type.2.2 Kernel MeasuresIn our experiments we use six basic kernel types,which are described below.
Effectively we haveeight kernels, because we also use the tree and de-pendency kernels with and without distributional in-formation.
Each kernel is a function which is passeda pair of short texts, which it then encodes into a spe-cific format and compares using a defined similarityfunction.
LK uses the regular cosine similarity func-tion, but LEK, TK, DK, MDK, DGK use the follow-ing cosine similarity redefined for sets of vectors.
Ifthe texts are represented as sets of vectors X and Y ,the set similarity kernel function is:?set(X,Y ) =?i?jcos(~xi, ~yj) (2)and normalisation is accomplished in the standardway for kernels by:?set?n(X,Y ) =?set(X,Y )?
(?set(X,X)?set(Y, Y ))(3)LK - The lexical kernel calculates the overlap be-tween the tokens that occur in each of the pairedtexts, where the tokens consist of Porter stemmed(Porter, 1980) non-stopwords.
Each text is repre-sented as a frequency vector of tokens that occurwithin it and the similarity between the pair is cal-culated using cosine.LEK - The lexical ESA kernel represents eachexample in the pair as the set of words that do notoccur in the intersection of the two texts.
The simi-larity is calculated as in Equation (3) with X and Ybeing the ESA vectors of each word from the firstand second text representations, respectively.TK - The tree kernel representation is based onthe definition by Zanzotto et al(2011).
Briefly,1http://www.cs.technion.ac.il/?gabr/resources/data/wordsim353/86each piece of text is parsed2; the non-terminalnodes of the parse tree, stopwords, and out-of-dictionary terms are all assigned a new random vec-tor (Equation 1); while the leaves that occurredin the BEAGLE training corpus are assigned theirlearned distributional vectors (Section 2.1.1).Each subtree of a tree is encoded recursively asa vector, where the distributional vectors represent-ing each node are combined using the circular con-volution operator (Plate, 1994; Jones and Mewhort,2007).
The whole tree is represented as a set of vec-tors, one for each subtree.DK - The dependency kernel representation en-codes each dependency pair as a separate vector, dis-counting the labels.
The non-stopword terminals arerepresented as their distributional vectors, while thestopwords and out-of-dictionary terms are given aunique random vector.
The vector for the depen-dency pair is obtained via a circular convolution ofthe individual word vectors.MDK - The multiple dependency kernel is con-structed like the dependency kernel, but similarity iscalculated separately between all the the pairs thatshare the same dependency label.
The combinedsimilarity for all dependency labels in the parse isthen calculated using least squares linear regression.While at the later stage we use GPR to combine allof the different kernels, for MDK we found that lin-ear regression provided better performance.DGK - The depgram kernel represents each de-pendency pair as an ESA vector obtained by search-ing the ESA collection for the two words in thedependency pair joined by the AND operator.
TheDGK representation only contains the dependenciesthat occur in one similarity text or the other, but notin both.2.3 RegressionEach of the kernel measures above is used to calcu-late a similarity score between a pair of texts.
Thedifferent similarity scores are then combined using2Because many of the datasets contained incomplete or un-grammatical sentences, we had to approximate some parses.The parsing was done using the Stanford parser (Klein andManning, 2003), which failed on some overly long sentences,which we therefore segmented at conjunctions or commas.Since our methods only compared subtrees of parses, we simplytook the union of all the partial parses for a given sentence.Gaussian process regression (GPR) (Rasmussen andWilliams, 2006).
GPR is a probabilistic regressionmethod where the weights are modelled as Gaussianrandom variables.
GPR is defined by a covariancefunction, which is akin to the kernel function in thesupport vector machine.
We used the squared expo-nential isotropic covariance function (also known asthe radial basis function):cov(xi, xj) = p21e(xi?xj)T ?(p2?I)?1?
(xi?xj)2 + p23?ijwith parameters p1 = 1, p2 = 1, and p3 = 0.01.
Wefound that training for parameters increased overfit-ting and produced worse results in validation exper-iments.3 Submitted RunsWe submitted three runs.
This is not sufficient fora full evaluation of the new methods we proposedhere, but it gives us an inkling of general trends.
Tochoose the composition of the submissions, we usedSTS 2012 training data for training, and STS 2012test data for validation (Agirre et al 2012).
Thefinal submitted runs also used some of the STS 2012test data for training.Basic - With this run we were examining if a sim-ple introduction of syntactic structure can improveover the baseline performance.
We trained a GPRcombination of the linear and tree kernels (LK-TK)on the MSRpar training data.
In validation experi-ments we found that this data set in general gave themost consistent performance for regression training.Custom - Here we tried to approximate the besttraining setup for each type of data.
We only hadtraining data for OnWN and for this dataset we wereable to improve over the LK-TK setup; however, thesettings for the rest of the data sets were guessesbased on observations from the validation experi-ments and overall performed poorly.
OnWN wastrained on MSRpar train with LK and DK.
The head-lines model was trained on MSRpar train and Eu-roparl test, with LK-LEK-TK-DK-TKND-DKND-MDK (trained on Europarl).3 FNWN was trained onMSRpar train and OnWN test with LK-LEK-DGK-TK-DK-TKND-DKND.
Finally, the SMT model3TKND and DKND are the versions of the tree and depen-dency kernels where no distributional vectors were used.8701020304050600 1 2 3 4 5Numberof STS pairsScoreGold Standard05101520250 1 2 3 4 5Numberof STS pairsScoreBasic051015202530350 1 2 3 4 5Numberof STS pairsScoreCustom051015202 2.5 3 3.5 4 4.5 5Numberof STS pairsScoreAllFigure 1: Score distributions of different runs on the OnWN datasetwas trained on MSRpar train and Europarl test withLK-LEK-TK-DK-TKND-DKND-MDK (trained onMSRpar).All - As in the LK-TK experiment, we usedthe same model on all of the data sets.
It wastrained on all of the training data except MSRvid,using all eight kernel types defined above.
In sum-mary we used the LK-LEK-TK-TKND-DK-DKND-MDK-DGK kernel combination.
MDK was trainedon the 2012 training portion of MSRpar.4 DiscussionFrom the shared task results in Table 1, we can seethat Basic is our highest ranked run.
It has alsoachieved the best performance on all data sets.
TheLK on its own improves slightly on the task baselineby removing stop words and using stemming, whilethe introduction of TK contributes syntactic and dis-tributional information.
With the Custom run, wewere trying to manually estimate which training datawould best reflect properties of particular test data,and to customise the kernel combination throughvalidation experiments.
The only data set for whichthis led to an improvement is OnWN, indicatingthat customised settings can be beneficial, but thata more scientific method for matching of trainingand test data properties is required.
In the All run,we were examining the effects that maximising theamount of training data and the number of kernelhdlns OnWN FNWN SMT mean rankBL 0.5399 0.2828 0.2146 0.2861 0.3639 71Basic 0.6399 0.4440 0.3995 0.3400 0.4709 51Cstm 0.4962 0.5639 0.1724 0.3006 0.4207 60All 0.5510 0.3099 0.2385 0.1171 0.3200 78Table 1: Shared task results: Pearson correlation with thegold standardmeasures has on the output predictions.
The resultsshow that swamping the regression with models andtraining data leads to overly normalised output anda decrease in performance.While the evaluation measure, Pearson correla-tion, does not take into account the shape of the out-put distribution, Figure 1 shows that this informa-tion may be a useful indicator of model quality andbehaviour.
In particular, the role of the regressioncomponent in our approach is to learn a transforma-tion from the output distributions of the models tothe distribution of the training data gold standard.This makes it sensitive to the choice of training data,which ideally would have similar characteristics tothe individual kernels, as well as a similar gold stan-dard distribution to the test data.
We can see in Fig-ure 1 that the training data and choice of kernels in-fluence the output distribution.Analysis of the minimum, first quartile, median,third quartile, and maximum statistics of the distri-butions in Figure 1 demonstrates that, while it is dif-ficult to visually evaluate the similarities of the dif-ferent distributions, the smallest squared error is be-tween the gold standard and the Custom run.
Thissuggests that properties other than the rank ordermay also be good indicators in training and testingof STS methods.AcknowledgmentsTamara Polajnar is supported by the ERC StartingGrant, DisCoTex, awarded to Stephen Clark, andLaura Rimell and Douwe Kiela by EPSRC grantEP/I037512/1: A Unified Model of Compositionaland Distributional Semantics: Theory and Applica-tions.88ReferencesEneko Agirre, Daniel Cer, Mona Diab, and AitorGonzalez-Agirre.
2012.
Semeval-2012 task 6: Apilot on semantic textual similarity.
In *SEM 2012:The First Joint Conference on Lexical and Computa-tional Semantics ?
Volume 1: Proceedings of the mainconference and the shared task, and Volume 2: Pro-ceedings of the Sixth International Workshop on Se-mantic Evaluation (SemEval 2012), pages 385?393,Montre?al, Canada, 7-8 June.
Association for Compu-tational Linguistics.Daniel Ba?r, Chris Biemann, Iryna Gurevych, and TorstenZesch.
2012.
UKP: Computing semantic textual sim-ilarity by combining multiple content similarity mea-sures.
In Proceedings of the 6th International Work-shop on Semantic Evaluation, held in conjunction withthe 1st Joint Conference on Lexical and Computa-tional Semantics, pages 435?440, Montre?al, Canada,June.
Association for Computational Linguistics.Danilo Croce, Paolo Annesi, Valerio Storch, and RobertoBasili.
2012.
UNITOR: Combining semantic textsimilarity functions through sv regression.
In Pro-ceedings of the 6th International Workshop on Seman-tic Evaluation, held in conjunction with the 1st JointConference on Lexical and Computational Semantics,pages 597?602, Montre?al, Canada, 7-8 June.
Associa-tion for Computational Linguistics.Evgeniy Gabrilovich and Shaul Markovitch.
2007.
Com-puting semantic relatedness using wikipedia-basedexplicit semantic analysis.
In Proceedings of the20th international joint conference on Artifical intel-ligence, IJCAI?07, pages 1606?1611, San Francisco,CA, USA.
Morgan Kaufmann Publishers Inc.Thorsten Joachims.
1998.
Text categorization with su-port vector machines: Learning with many relevantfeatures.
In Proceedings of the 10th European Con-ference on Machine Learning, ECML ?98, pages 137?142, London, UK, UK.
Springer-Verlag.Michael N. Jones and Douglas J. K. Mewhort.
2007.Representing word meaning and order information ina composite holographic lexicon.
Psychological Re-view, 114:1?37.S.
Kaski.
1998.
Dimensionality reduction by randommapping: fast similarity computation for clustering.In Proceedings of the 1998 IEEE International JointConference on Neural Networks, volume 1, pages413?418 vol.1, May.Dan Klein and Christopher D. Manning.
2003.
Accurateunlexicalized parsing.
In Proceedings of the 41st An-nual Meeting on Association for Computational Lin-guistics - Volume 1, ACL ?03, pages 423?430, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.T.
A.
Plate.
1994.
Distributed Representations andNested Compositional Structure.
Ph.D. thesis, Univer-sity of Toronto.T Polajnar, T Damoulas, and M Girolami.
2011.
Proteininteraction sentence detection using multiple semantickernels.
J Biomed Semantics, 2(1):1?1.M.
F. Porter.
1980.
An algorithm for suffix stripping.Program, 14(3):130?137, July.C.
E. Rasmussen and C. K. I. Williams.
2006.
GaussianProcesses for Machine Learning.
MIT Press.Stephen Robertson, Hugo Zaragoza, and Michael Taylor.2004.
Simple BM25 extension to multiple weightedfields.
In Proceedings of the thirteenth ACM interna-tional conference on Information and knowledge man-agement, CIKM ?04, pages 42?49, New York, NY,USA.
ACM.John Shawe-Taylor and Nello Cristianini.
2004.
KernelMethods for Pattern Analysis.
Cambridge UniversityPress, New York, NY, USA.Fabio Massimo Zanzotto and Lorenzo Dell?Arciprete.2011.
Distributed structures and distributional mean-ing.
In Proceedings of the Workshop on DistributionalSemantics and Compositionality, DiSCo ?11, pages10?15, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.89
