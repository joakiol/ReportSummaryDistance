Coling 2008: Proceedings of the workshop on Grammar Engineering Across Frameworks, pages 17?24Manchester, August 2008A More Precise Analysis of Punctuation forBroad-Coverage Surface Realization with CCGMichael White and Rajakrishnan RajkumarDepartment of LinguisticsThe Ohio State UniversityColumbus, OH, USA{mwhite,raja}@ling.osu.eduAbstractThis paper describes a more precise anal-ysis of punctuation for a bi-directional,broad coverage English grammar extractedfrom the CCGbank (Hockenmaier andSteedman, 2007).
We discuss various ap-proaches which have been proposed inthe literature to constrain overgenerationwith punctuation, and illustrate how as-pects of Briscoe?s (1994) influential ap-proach, which relies on syntactic featuresto constrain the appearance of balancedand unbalanced commas and dashes to ap-propriate sentential contexts, is unattrac-tive for CCG.
As an interim solutionto constrain overgeneration, we proposea rule-based filter which bars illicit se-quences of punctuation and cases of im-properly unbalanced apposition.
Usingthe OpenCCG toolkit, we demonstratethat our punctuation-augmented grammaryields substantial increases in surface re-alization coverage and quality, helping toachieve state-of-the-art BLEU scores.1 IntroductionIn his pioneering monograph, Nunberg (1990) ar-gues that punctuation is a systematic module of thegrammar of written text and is governed by princi-ples and constraints like other sub-systems such assyntax or phonology.
Since then, others includingBriscoe (1994) and Doran (1998) have exploredways of including rules and representations forpunctuation marks in broad coverage grammars.
Inc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.computational systems, punctuation provides dis-ambiguation cues which can help parsers arrive atthe correct parse.
From a natural language gener-ation standpoint, text without punctuation can bedifficult to comprehend, or even misleading.In this paper, we describe a more precise analy-sis of punctuation for a bi-directional, broad cover-age English grammar extracted from the CCGbank(Hockenmaier and Steedman, 2007).
In contrast toprevious work, which has been primarily orientedtowards parsing, our goal has been to develop ananalysis of punctuation that is well suited for bothparsing and surface realization.
In addition, whileBriscoe and Doran have simply included punctu-ation rules in their manually written grammars,our approach has been to revise the CCGbank it-self with punctuation categories and more preciselinguistic analyses, and then to extract a grammarfrom the enhanced corpus.In developing our analysis, we illustrate how as-pects of Briscoe?s (1994) approach, which relieson syntactic features to constrain the appearanceof balanced and unbalanced commas and dashes toappropriate sentential contexts, is unattractive forCCG, with its more flexible handling of word or-der.
Consequently, as an interim solution, we havechosen to identify and filter undesirable configu-rations when scoring alternative realizations.
Wealso point to other ways in which punctuation con-straints could be incorporated into the grammar,for exploration in future work.Using the OpenCCG toolkit, we demonstratethat our punctuation-enhanced grammar yieldssubstantial increases in surface realization quality,helping to achieve state-of-the-art BLEU scores.We use non-blind testing to evaluate the efficacyof the grammar, and blind-testing to evaluate itsperformance on unseen data.
The baseline models17are (1) a grammar which has lexicalized punctu-ation categories only for conjunction and apposi-tion, and (2) one which has punctuation categoriescorresponding to the existing treatment of punctua-tion in the corpus.
Non-blind testing results showna nearly 9-point increase in BLEU scores com-pared to the best baseline model using oracle n-grams, as well as a 40% increase in exact matches.Blind testing results show a more than 5.5-pointincrease in BLEU scores, contributing to an all-sentences score of 0.7323 on Section 23 with over96% coverage.2 BackgroundCCG (Steedman, 2000) is a unification-based cat-egorial grammar formalism which is defined al-most entirely in terms of lexical entries that encodesub-categorization information as well as syntacticfeature information (e.g.
number and agreement).Complementing function application as the stan-dard means of combining a head with its argument,type-raising and composition support transparentanalyses for a wide range of phenomena, includ-ing right-node raising and long distance dependen-cies.
Semantic composition happens in parallelwith syntactic composition, which makes it attrac-tive for generation.OpenCCG is a parsing/generation library whichworks by combining lexical categories for wordsusing CCG rules and multi-modal extensions onrules (Baldridge, 2002) to produce derivations.Surface realization is the process by which logicalforms are transduced to strings.
OpenCCG usesa hybrid symbolic-statistical chart realizer (White,2006) which takes logical forms as input and pro-duces sentences by using CCG combinators tocombine signs.
Alternative realizations are rankedusing integrated n-gram scoring.To illustrate the input to OpenCCG, considerthe semantic dependency graph in Figure 1.
Inthe graph, each node has a lexical predication(e.g.
make.03) and a set of semantic features (e.g.
?NUM?sg); nodes are connected via dependencyrelations (e.g.
?ARG0?).
Internally, such graphsare represented using Hybrid Logic DependencySemantics (HLDS), a dependency-based approachto representing linguistic meaning (Baldridge andKruijff, 2002).
In HLDS, each semantic head (cor-responding to a node in the graph) is associatedwith a nominal that identifies its discourse referent,and relations between heads and their dependentshe h2aa1heh3<Det><Arg0> <Arg1><TENSE>pres<NUM>sg<Arg0>w1 want.01m1<Arg1><GenRel><Arg1><TENSE>presp1pointh1have.03make.03<Arg0>Figure 1: Semantic dependency graph from theCCGbank for He has a point he wants to make[.
.
.
]are modeled as modal relations.3 The need for an OpenCCG analysis ofpunctuationThe linguistic analysis aims to make a broad cover-age OpenCCG grammar extracted from the CCG-bank (White et al, 2007) more precise by addinglexicalized punctuation categories to deal withconstructions involving punctuation.
The origi-nal CCGbank corpus does not have lexical cate-gories for punctuation; instead, punctuation markscarry categories derived from their part of speechtags and form part of a binary rule.
It is as-sumed that there are no dependencies betweenwords and punctuation marks and that the re-sult of punctuation rules is the same as the non-punctuation category.
OpenCCG does not supportnon-combinatory binary rules, as they can be re-placed by equivalent lexicalized categories withapplication-only slashes.
For example, a binaryrule of the form , s ?
s can be replaced by theequivalent category s?1?/?s?1?for the comma.
Infact, this would work reasonably well for parsing,but is inadequate for generation.
To illustrate, con-sider (1):(1) Despite recent declines in yields, in-vestors continue to pour cash intomoney funds.
(wsj 0004.10)A comma category like the one shown abovewould end up overgenerating, as sentences and18sentential complements would be generated witha comma preceding them.
Also, the result of theabove function application rule could act as its ownargument, producing a string of commas.
Moregenerally, binary rules miss out on many linguis-tic generalizations, such as the presence of manda-tory balancing marks in sentence-medial commaor dash adjuncts.The literature discusses various means to ad-dress the issue of overgeneration: absorption rules(Nunberg, 1990), syntactic features (Doran, 1998)and (Briscoe, 1994) and semantic features (White,2006).
Section 5 explains these approaches in de-tail, and considers a possible system of syntacticfeatures for a multi-modal CCG grammar imple-mentation.
We show how such a system is inade-quate to constrain all possible cases of overgener-ation, motivating our decision to employ semanticfeatures in our bi-directional grammar.4 Integrating an analysis of punctuationinto the grammarAs our starting point, we used an XML repre-sentation of an enhanced version of the CCGbankwith Propbank roles projected onto it (Boxwell andWhite, 2008).
Contexts and constructions in whichpunctuation marks occur were isolated and the cor-pus was then restructured by inserting new cate-gories and modified derivations using XSL trans-forms.
In many cases this also involved modify-ing the gold standard derivations substantially andadding semantic representations to syntactic cat-egories using logical form templates.
Currently,the algorithm succeeds in creating logical formsfor 98.01% of the sentences in the developmentsection (Sect.
00) of the converted CCGbank, and96.46% of the sentences in the test section (Sect.23).
Of these, 92.10% of the development LFsare semantic dependency graphs with a single root,while 92.12% of the test LFs have a single root.The remaining cases, with multiple roots, are miss-ing one or more dependencies required to form afully connected graph.
These missing dependen-cies usually reflect inadequacies in the current log-ical form templates.
In Section 00, 89 punctuationcategories were created (66 commas, 14 dashesand 3 each for the rest) out of 54 classes of binaryrules (37 comma, 8 dash, 3 apiece of colon, paren-thesis and dots).
Three high frequency comma cat-egories are explained below.4.1 Sentential AdjunctsThe comma in example (1) has been analysedas selecting a sentential modifier to its left,Despite recent declines in yields, to result in asentential modifier which then selects the rest ofthe sentence.
This results in the following lexicalcategory and semantics for the comma category:(2) , ` s?1?ind=X1 ,mod=M/s?1?\?(s?1?/s?1?
): @M(?EMPH-INTRO?+)Syntactic categories and their semantics are linkedby index variables in the feature structures of cat-egories.
Index variables for semantic heads (e.g.X1) are conventionally named X plus the numberof the feature structure.
To support modifier modi-fiers, as in (2), semantic heads of modifiers are alsomade available through a modifier index feature,with a variable conventionally named M .1Here,the effect of combining the comma with the phraseheaded by despite is to add the ?EMPH-INTRO?+feature to the despite-phrase?s semantics.
Follow-ing (Bayraktar et al, 1998), this feature indicatesthat the comma has the discourse function of em-phasizing an introductory clause or phrase.
Dur-ing realization, the feature triggers the look-up ofthe category in (2), and prevents the re-applicationof the category to its own output (as the featureshould only be realized once).The category in (2) illustrates our approach,which is to assign to every punctuation mark (otherthan balancing marks) a category whose LF in-cludes a feature or relation which represents itsdiscourse semantic function in broad-brush termssuch as emphasis, elaboration and apposition.4.2 Verbs of reported speechIn (3), the comma which follows Neverthless andsets off the phrase headed by said has the categoryin (4):(3) Nevertheless, said Brenda Malizia Ne-gus, editor of Money Fund Report,yields may blip up again before theyblip down because of recent rises inshort-term interest rates.
(wsj 0004.8)(4) , ` s?2?/s?2?/?punct[, ]/?
(s?1?dcl\s?2?dcl): @X2(?ELABREL?
?X1)1A limited form of default unification is used in the im-plementation to keep multiple modifiers from conflicting.
Asthe names of index variables are entirely predictable, they aresuppressed in the remainder of the paper.19In the genre of newswire text, this constructionoccurs frequently with verbs of reported speech.The CCGbank derivation of (3) assigns the cate-gory s?1?dcl\s?2?dclto the phrase headed by said,the same category that is used when the phrasefollows the missing sentential complement.
Thecomma category in (4) selects for this categoryand a balancing comma and then converts it toa pre-sentential modifier, s?2?/s?2?.
Semantically,an elaboration relation is added between the mainclause and the reported speech phrase.Category (4) overgenerates to some extent inthat it will allow a comma at the beginning of thesentence.
To prevent this, an alternative would beto make the comma explicitly select for lexical ma-terial to its left (in this case for the category of Nev-erthless).
Another possibility would be to followDoran (1998) in analyzing the above constructionby using the verb itself to select for the comma.However, since our method involves changing thegold standard derivations, and since making theverb select extra commas or having the comma se-lect leftward material would entail substantial fur-ther changes to the derivations, we have opted togo with (4), balancing adequacy and convenience.4.3 NP appositivesNeither the Penn Tree Bank nor the CCGbankdistinguishes between NP appositives and NPconjunctions.
We wrote a set of simple heuristicrules to enforce this distinction, which is vitalto generation.
Appositives can occur sentencemedially or finally.
The conventions of writingmandate that sentence medial appositives shouldbe balanced?i.e., the appositive NP shouldbe surrounded by commas or dashes on bothsides?while sentence final appositives shouldbe unbalanced?i.e., they should only have onepreceding comma or dash.
The categories andsemantics for unbalanced and balanced appositivecommas are, respectively:(5) a. , ` np?1?\np?1?/?np?3?
: @X1(?APPOSREL?
?X3)b. , ` np?1?\np?1?/?punct[, ]/?np?3?
: @X1(?APPOSREL?
?X3)Here, the unbalanced appositive has a categorywhere the comma selects as argument the apposi-tive NP and converts it to a nominal modifier.
Forbalanced appositives, the comma selects the ap-positive NP and the balancing comma to form anominal modifier (examples are given in the nextsection).5 Constraining overgeneration inbi-directional grammarsA complex issue that arises in the design of bi-directional grammars is ensuring the proper pre-sentation of punctuation.
Among other things, thisinvolves the task of ensuring the correct realizationof commas introducing noun phrase appositives?in our case, choosing when to use (5a) vs. (5b).
Inthis section, we consider and ultimately reject a so-lution that follows Briscoe (1994) in using syntac-tic features.
As an alternative, interim solution, wethen describe a rule-based filter which bars illicitpunctuation sequences and improperly unbalancedapposition.
The paradigm below helps illustratethe issues:(6) John, CEO of ABC, loves Mary.
(7) * John, CEO of ABC loves Mary.
(8) Mary loves John, CEO of ABC.
(9) * Mary loves John, CEO of ABC,.
(10) Mary loves John, CEO of ABC, madly.
(11) * Mary loves John, CEO of ABC madly.5.1 Absorption vs. syntactic featuresNunberg (1990) argues that text adjuncts intro-duced by punctuation marks have an underlyingrepresentation where these adjuncts have marks oneither side.
They attain their surface form whena set of presentation rules are applied.
This ap-proach ensures that all sentence medial cases like(6) and (10) above are generated correctly, whileunacceptable examples (7) and (11) would not begenerated at all.
Example (8) would at first begenerated as (9): to deal with such sentences,where two points happen to coincide, Nunbergposits an implicit point which is absorbed by theadjacent point.
Absorption occurs according tothe ?strength?
of the two points.
Strength is de-termined according to the Point Absorption Hi-erarchy, which ranks commas lower than dashes,semi-colons, colons and periods.
As White (1995)observes, from a generation-only perspective, itmakes sense to generate text adjuncts which arealways balanced and post-process the output todelete lower ranked points, as absorption uses rel-atively simple rules that operate independently of20the hierarchy of the constituents.
However, us-ing this approach for parsing would involve a pre-processing step which inserts commas into possi-ble edges of possible constituents, as described in(Forst and Kaplan, 2006).
To avoid this consider-able complication, Briscoe (1994) has argued fordeveloping declarative approaches involving syn-tactic features, with no deletions or insertions ofpunctuation marks.5.2 Features for punctuation in CCG?Unfortunately, the feature-based approach appearsto be inadequate for dealing with the class of ex-amples presented above in CCG.
This approach in-volves the incorporation of syntactic features forpunctuation into atomic categories so that certaincombinations are blocked.
To ensure proper ap-positive balancing sentence finally, the rightmostelement in the sentence should transmit a relevantfeature to the clause level, which the sentence-finalperiod can then check for the presence of right-edge punctuation.
Possible categories for a tran-sitive verb and the full stop appear below:(12) loves ` s?1?bal=BAL,end=PE\np?2?bal=+/np?3?bal=BAL,end=PE(13) .
` sent\?send=nilHere the feature variables BAL and PE of the right-most argument of the verb would unify with thecorresponding result category feature values to re-alize the main clauses of (8) and (9) with the fol-lowing feature values:(14) Mary loves John, CEO of ABC `s?1?bal=?,end=nil(15) Mary loves John, CEO of ABC, `s?1?bal=+,end=commaThus, in (15), the sentence-final period would notcombine with s?1?bal=+,end=commaand the deriva-tion would be blocked.25.2.1 Issue: Extraction casesThe solution sketched above is not adequate todeal with extraction involving ditransitive verbs incases like (16) and (17):2It is worth noting than an n-gram scorer would highlydisprefer example (9), as a comma period sequence would notbe attested in the training data.
However, an n-gram modelcannot be relied upon to eliminate examples like (11), whichwould likely be favored as they are shorter than their balancedcounterparts.
(16) Mary loves a book that John gave Bill,his brother.
(17) * Mary loves a book that John gave Bill,his brother,.As Figure 2 shows, an unacceptable case like (17)is not blocked.
Even when the sentence final NP isbalanced, the end=comma value is not propagatedto the root level.
This is because the end featurefor the relative clause should depend on the first(indirect) object of gave, rather than the second(direct) object as in a full ditransitive clause.
Apossible solution would be to introduce more fea-tures which record the presence of punctuation inthe leftward and rightward arguments of complexcategories; this would be rather baroque, however.5.2.2 Issue: Crossing compositionAnother issue is how crossing composition, usedwith adverbs in heavy NP shift contructions, inter-acts with appositives, as in the following examples:(18) Mary loves madly John, CEO of ABC.
(19) * Mary loves madly John, CEO of ABC,.For examples (10) and (11), which do not involvecrossing composition, the category for the adverbshould be the one in (20):(20) madly ` s?1?end=nil\np?2?\(s?1?bal=+\np?2?
)Here the bal=+ feature on the argument of the ad-verb madly ensures that the direct object of theverb is balanced, as in (10); otherwise, the deriva-tion fails, as in (11).
Irrespective of the valueof the end feature of the argument, the result ofthe adverb has the feature end=nil as the post-modifier is lexical material which occurs after theVP.
With crossing composition, however, category(20) would licence an erroneous derivation for ex-ample (19), as the end=nil feature on the result ofthe adverb category would prevent the percolationof the end feature at the edge of the phrase to theclausal root, as Figure 3 shows.To block such derivations, one might considergiving the adverb another category for use withcrossing composition:(21) madly ` s?1?\np?2?\?(s?1?\np?2?
)The use of the non-associative, permutative modal-ity ?
on the main slash allows the crossing com-position rule to be applied, and feature inheritance21that John gave Bill, his brother,(nend=PE\n)/(send=PE/np) np send=PE\np/npend=PE/np npend=comma>T >s/(s\np) send=PE\np/npend=PE>Bsend=PE/npend=PE>nend=PE\nFigure 2: Object extractionMary loves madly John, CEO, .np send=PE\np/npend=PEs 1end=nil\np 1\(s 1bal=+\np 1) npbal=+,end=commasent\?send=nil<B?send=nil\np/npend=PE>send=nil\np<send=nil<sentFigure 3: Crossing compositionensures that the end feature from the verb lovesis also copied over.
Thus, in example (19), thepunctuation at the edge of the phrase would bepercolated to the clausal root, where the sentence-final period would block the derivation.
However,in the slash modality inheritance hierarchy pro-posed by Baldridge (2002), the ?
modality inher-its the properties of function application.
Conse-quently, this category could also lead to the erro-neous derivation of example (11).
In such a deriva-tion, category (21) will not require the direct ob-ject to have a balanced appositive; meanwhile, theend=nil feature on the direct object will propagateto the clausal root, where it will happily combinewith the category for the full stop.
Finally, havingtwo distinct categories for the adverb would off-set the advantage of multi-modal categorial gram-mar in dealing with word order variation, where itis possible to use one category in situations whereotherwise several categories would be required.5.3 A rule-based filter to constrainovergenerationFor the reasons discussed in the preceding section,we decided not to use syntactic features to con-strain overgeneration.
Instead, we have employedsemantic features in the logical form together witha rule-based filter, as an interim solution.
Dur-ing realization, the generated output is examinedand fragments where two marks appear in a roware eliminated.
Additionally, to handle improp-erly unbalanced punctuation, we modified the re-sult categories of unbalanced appositive commasand dashes to include a feature marking unbal-anced punctuation, as follows:(22) , ` np?1?unbal=comma\?np?1?/?np?2?Then, during realization, a filter on derivationslooks for categories such as npunbal=comma, andchecks to make sure this NP is followed by a an-other punctuation mark in the string.
We report onthe effects of the filter in our results section.6 EvaluationWe extracted a grammar from the restructured cor-pus and created testbeds of logical forms under thefollowing conditions:1.
Baseline 1: A CCGbank version which has nolexicalized categories corresponding to anyof the punctuation marks except sentence fi-nal marks and commas which conjoin ele-ments or introduce NP appositives.
Conjunc-tion and apposition are frequent in the corpusand if excluded, logical forms for many sen-tences are not produced, weakening the base-line considerably.2.
Baseline 2: A CCGbank version whereall punctuation marks (except conjunc-tion/apposition commas and sentence-finalmarks, which have proper categories) havelexicalized MMCCG categories with no se-mantics, corresponding to binary rules in theoriginal CCGbank.3.
The CCGbank augmented with punctuationcategories.22Testing was done under four conditions:1.
Non-blind testing with oracle n-gram scoring.This condition tests the grammar most di-rectly, as it avoids the issue of lexical smooth-ing and keeps the combinatorial search man-ageable.
A grammar extracted from the de-velopment section (Section 00) of the CCG-bank was applied to the LF testbed of thatsection, using oracle n-gram scoring (alongwith FLMs, see next) to generate the sen-tences back.
For each logical form, the gener-ated output sentence was compared with theactual gold standard sentence correspondingto that logical form.2.
Blind testing with factored language mod-els (FLM) and lexical smoothing, following(White et al, 2007).
Blind testing naturallyprovides a more realistic test of performanceon unseen data.
Here logical forms of Sec-tions 00 and 23 were created using gram-mars of those sections respectively and thena grammar was extracted from the standardtraining sections (02-21).
This grammar wasused to generate from the LFs of the develop-ment and test sections; for space reasons, weonly report the results on the test section.3.
Blind testing with hypertagging.
Hypertag-ging (Espinosa et al, 2008) is supertaggingfor surface realization; it improves realizerspeed and coverage with large grammars bypredicting lexical category assignments witha maximum entropy model.4.
The punctuation-enhanced grammars weretested in the three conditions above with andwithout the balanced punctuation filter.7 ResultsNon-blind testing results in Table 1 indicate thatboth exact match figures as well BLEU scores in-crease substantially in comparison to the baselineswhen a punctuation augmented grammar is used.The difference is especially notable when oraclen-gram scoring is used.
The punctuation filter im-proves performance as exact matches increase by1.66% and BLEU scores also show a slight in-crease.
Complete realizations are slightly worsefor the augmented grammar than Baseline 1, butthe coverage of the baseline grammar is lower.Table 1: Non-blind testing on Section 00 (Gram-mar coverage: Baseline 1, 95.8%; Baseline 2,95.03%; Punct grammar, 98.0%)N-grams Grammar Exact Complete BLEUOracle Baseline 1 35.8% 86.2% 0.8613Baseline 2 39.10% 53.58% 0.8053Punct 75.9% 85.3% 0.9503FLM w/o Baseline 1 17.7% 83.0% 0.7293filter Baseline 2 5.72% 4.18% 0.4470Punct 29.7% 80.6% 0.7984FLM w/ filt.
Punct 31.3% 80.6% 0.8062Table 2: Blind testing on Section 23 with FLM(Grammar coverage: Baseline 1, 94.8%; Base-line 2, 95.06%; Punct grammar, 96.5%)Hyp., Filt.
Grammar Exact Complete BLEUno, w/o Baseline 1 11.1% 46.4% 0.6297Baseline 2 2.97% 3.97% 0.3104Punct 18.0% 43.2% 0.6815no, w/ Punct 19.3% 43.3% 0.6868yes, w/o Punct 20.4% 61.5% 0.7270yes, w/ Punct 21.6% 61.5% 0.7323Blind testing results shown in Table 2 also demon-strate that the augmented grammar does better thanthe baseline in terms of BLEU scores and ex-act matches, with the hypertagger further boostingBLEU scores and the number of complete realiza-tions.
The use of the filter yields a further 1.2?1.3% increase in exact match figures as well as ahalf a BLEU point improvement; a planned col-lection of human judgments may reveal that theseimprovements are more meaningful than the scoreswould indicate.Baseline 2, which models all punctuation, per-forms very badly with FLM scoring though it doesbetter than the minimal punctuation Baseline 1with oracle scoring.
The main reason for this isthat, without any semantic or syntactic features toconstrain punctuation categories, they tend to re-apply to their own output, clogging up the chart.This results in a low number of complete realiza-tions as well as exact matches.While direct comparisons cannot really be madeacross grammar frameworks, as inputs vary intheir semantic depth and specificity, we observethat our all-sentences BLEU score of 0.7323 ex-ceeds that of Hogan et al (2007), who report atop score of 0.6882 including special treatment ofmulti-word units (though their coverage is near100%).
Nakanishi et al (2005) and Langkilde-23Geary (2002) report scores several points higher,though the former is limited to sentences of length20 or less, and the latter?s coverage is much lower.8 ConclusionWe have shown that incorporating a more pre-cise analysis of punctuation into a broad-coveragereversible grammar extracted from the CCGbankyields substantial increases in the number of ex-act matches and BLEU scores when performingsurface realization with OpenCCG, contributing tostate-of-the-art results.
Our discussion has alsohighlighted the inadequacy of using syntactic fea-tures to control punctuation placement in CCG,leading us to develop a filter to ensure appro-priately balanced commas and dashes.
In fu-ture work, we plan to investigate a more satisfac-tory grammatical treatment involving constraintsin independent orthographic derivations, perhapsalong the lines of the autonomous prosodic deriva-tions which Steedman and Prevost (1994) discuss.An evaluation of parsing side performance is alsoplanned.AcknowledgmentsWe thank the anonymous reviewers, Detmar Meur-ers and the Clippers and Synners groups at OSUfor helpful comments and discussion.ReferencesBaldridge, Jason and Geert-Jan Kruijff.
2002.
Cou-pling CCG and Hybrid Logic Dependency Seman-tics.
In Proc.
ACL-02.Baldridge, Jason.
2002.
Lexically Specified Deriva-tional Control in Combinatory Categorial Grammar.Ph.D.
thesis, University of Edinburgh.Bayraktar, Murat, Bilge Say, and Varol Akman.
1998.An Analysis of English Punctuation: The SpecialCase of Comma.
International Journal of CorpusLinguistics, 3(1):33?58.Boxwell, Stephen and Michael White.
2008.
Pro-jecting Propbank roles onto the CCGbank.
In Proc.LREC-08.
To appear.Briscoe, Ted.
1994.
Parsing (with) punctuation.
Tech-nical report, Xerox, Grenoble, France.Doran, Christine.
1998.
Incorporating Punctuationinto the Sentence Grammar: A Lexicalized Tree Ad-joining Grammar Perspective.
Ph.D. thesis, Univer-sity of Pennsylvania.Espinosa, Dominic, Michael White, and Dennis Mehay.2008.
Hypertagging: Supertagging for surface real-ization with CCG.
In Proc.
ACL-08:HLT.
To appear.Forst, Martin and Ronald M. Kaplan.
2006.
The im-portance of precise tokenizing for deep grammars.In Proc.
LREC-06.Hockenmaier, Julia and Mark Steedman.
2007.
CCG-bank: A Corpus of CCG Derivations and Depen-dency Structures Extracted from the Penn Treebank.Computational Linguistics, 33(3):355?396.Hogan, Deirdre, Conor Cafferkey, Aoife Cahill, andJosef van Genabith.
2007.
Exploiting multi-wordunits in history-based probabilistic generation.
InProc.
EMNLP-CoNLL-07.Langkilde-Geary, Irene.
2002.
An empirical veri-fication of coverage and correctness for a general-purpose sentence generator.
In Proc.
INLG-02.Nakanishi, Hiroko, Yusuke Miyao, and Jun?ichi Tsujii.2005.
Probabilistic methods for disambiguation ofan HPSG-based chart generator.
In Proc.
IWPT-05.Nunberg, Geoffrey.
1990.
The Linguistics of Punctua-tion.
CSLI Publications, Stanford, CA.Steedman, Mark and S. Prevost.
1994.
Specifying in-tonation from context for speech synthesis.
SpeechCommunication, 15(1?2):139?153.Steedman, Mark.
2000.
The syntactic process.
MITPress, Cambridge, MA, USA.White, Michael, Rajakrishnan Rajkumar, and ScottMartin.
2007.
Towards broad coverage surface re-alization with CCG.
In Proc.
of the Workshop onUsing Corpora for NLG: Language Generation andMachine Translation (UCNLG+MT).White, Michael.
1995.
Presenting punctuation.
In Pro-ceedings of the Fifth European Workshop on NaturalLanguage Generation, pages 107?125.White, Michael.
2006.
Efficient realization of coordi-nate structures in combinatory categorial grammar.Research on Language and Computation, 4(1):39?75.24
