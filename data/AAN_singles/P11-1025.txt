Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 239?247,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsBayesian Inference for Zodiac and Other Homophonic CiphersSujith Ravi and Kevin KnightUniversity of Southern CaliforniaInformation Sciences InstituteMarina del Rey, California 90292{sravi,knight}@isi.eduAbstractWe introduce a novel Bayesian approach fordeciphering complex substitution ciphers.
Ourmethod uses a decipherment model whichcombines information from letter n-gram lan-guage models as well as word dictionaries.Bayesian inference is performed on our modelusing an efficient sampling technique.
Weevaluate the quality of the Bayesian deci-pherment output on simple and homophonicletter substitution ciphers and show that un-like a previous approach, our method consis-tently produces almost 100% accurate deci-pherments.
The new method can be appliedon more complex substitution ciphers and wedemonstrate its utility by cracking the famousZodiac-408 cipher in a fully automated fash-ion, which has never been done before.1 IntroductionSubstitution ciphers have been used widely in thepast to encrypt secrets behind messages.
Theseciphers replace (English) plaintext letters with ci-pher symbols in order to generate the ciphertext se-quence.There exist many published works on automaticdecipherment methods for solving simple letter-substitution ciphers.
Many existing methods usedictionary-based attacks employing huge word dic-tionaries to find plaintext patterns within the ci-phertext (Peleg and Rosenfeld, 1979; Ganesan andSherman, 1993; Jakobsen, 1995; Olson, 2007).Most of these methods are heuristic in nature andsearch for the best deterministic key during deci-pherment.
Others follow a probabilistic decipher-ment approach.
Knight et al (2006) use the Expec-tation Maximization (EM) algorithm (Dempster etal., 1977) to search for the best probabilistic key us-ing letter n-gram models.
Ravi and Knight (2008)formulate decipherment as an integer programmingproblem and provide an exact method to solve sim-ple substitution ciphers by using letter n-gram mod-els along with deterministic key constraints.
Corlettand Penn (2010) work with large ciphertexts con-taining thousands of characters and provide anotherexact decipherment method using an A* search al-gorithm.
Diaconis (2008) presents an analysis ofMarkov Chain Monte Carlo (MCMC) sampling al-gorithms and shows an example application for solv-ing simple substitution ciphers.Most work in this area has focused on solvingsimple substitution ciphers.
But there are variantsof substitution ciphers, such as homophonic ciphers,which display increasing levels of difficulty andpresent significant challenges for decipherment.
Thefamous Zodiac serial killer used one such cipher sys-tem for communication.
In 1969, the killer sent athree-part cipher message to newspapers claimingcredit for recent shootings and crimes committednear the San Francisco area.
The 408-character mes-sage (Zodiac-408) was manually decoded by hand inthe 1960?s.
Oranchak (2008) presents a method forsolving the Zodiac-408 cipher automatically with adictionary-based attack using a genetic algorithm.However, his method relies on using plaintext wordsfrom the known solution to solve the cipher, whichdeparts from a strict decipherment scenario.In this paper, we introduce a novel method for239solving substitution ciphers using Bayesian learn-ing.
Our novel contributions are as follows:?
We present a new probabilistic deciphermentapproach using Bayesian inference with sparsepriors, which can be used to solve differenttypes of substitution ciphers.?
Our new method combines information fromword dictionaries along with letter n-grammodels, providing a robust deciphermentmodel which offsets the disadvantages faced byprevious approaches.?
We evaluate the Bayesian decipherment outputon three different types of substitution ciphersand show that unlike a previous approach, ournew method solves all the ciphers completely.?
Using the Bayesian decipherment, we show forthe first time a truly automated system that suc-cessfully solves the Zodiac-408 cipher.2 Letter Substitution CiphersWe use natural language processing techniques toattack letter substitution ciphers.
In a letter substi-tution cipher, every letter p in the natural language(plaintext) sequence is replaced by a cipher token c,according to some substitution key.For example, an English plaintext?H E L L O W O R L D ...?may be enciphered as:?N O E E I T I M E L ...?according to the key:p: ABCDEFGHIJKLMNOPQRSTUVWXYZc: XYZLOHANBCDEFGIJKMPQRSTUVWwhere, ?
?
represents the space character (wordboundary) in the English and ciphertext messages.If the recipients of the ciphertext message havethe substitution key, they can use it (in reverse) torecover the original plaintext.
The sender can en-crypt the message using one of many different ci-pher systems.
The particular type of cipher systemchosen determines the properties of the key.
For ex-ample, the substitution key can be deterministic inboth the encipherment and decipherment directionsas shown in the above example?i.e., there is a 1-to-1 correspondence between the plaintext letters andciphertext symbols.
Other types of keys exhibit non-determinism either in the encipherment (or decipher-ment) or both directions.2.1 Simple Substitution CiphersThe key used in a simple substitution cipher is deter-ministic in both the encipherment and deciphermentdirections, i.e., there is a 1-to-1 mapping betweenplaintext letters and ciphertext symbols.
The exam-ple shown earlier depicts how a simple substitutioncipher works.Data: In our experiments, we work with a 414-letter simple substitution cipher.
We encrypt anoriginal English plaintext message using a randomlygenerated simple substitution key to create the ci-phertext.
During the encipherment process, we pre-serve spaces between words and use this informationfor decipherment?i.e., plaintext character ?
?
mapsto ciphertext character ?
?.
Figure 1 (top) showsa portion of the ciphertext along with the originalplaintext used to create the cipher.2.2 Homophonic CiphersA homophonic cipher uses a substitution key thatmaps a plaintext letter to more than one cipher sym-bol.For example, the English plaintext:?H E L L O W O R L D ...?may be enciphered as:?65 82 51 84 05 60 54 42 51 45 ...?according to the key:A: 09 12 33 47 53 67 78 92B: 48 81...E: 14 16 24 44 46 55 57 64 74 82 87...L: 51 84...Z: 02Here, ?
?
represents the space character in bothEnglish and ciphertext.
Notice the non-determinisminvolved in the enciphering direction?the English240letter ?L?
is substituted using different symbols (51,84) at different positions in the ciphertext.These ciphers are more complex than simple sub-stitution ciphers.
Homophonic ciphers are generatedvia a non-deterministic encipherment process?thekey is 1-to-many in the enciphering direction.
Thenumber of potential cipher symbol substitutes for aparticular plaintext letter is often proportional to thefrequency of that letter in the plaintext language?for example, the English letter ?E?
is assigned morecipher symbols than ?Z?.
The objective of this isto flatten out the frequency distribution of cipher-text symbols, making a frequency-based cryptanaly-sis attack difficult.The substitution key is, however, deterministic inthe decipherment direction?each ciphertext symbolmaps to a single plaintext letter.
Since the ciphertextcan contain more than 26 types, we need a largeralphabet system?we use a numeric substitution al-phabet in our experiments.Data: For our decipherment experimentson homophonic ciphers, we use the same414-letter English plaintext used in Sec-tion 2.1.
We encrypt this message using ahomophonic substitution key (available fromhttp://www.simonsingh.net/The Black Chamber/homophoniccipher.htm).
As before, we preservespaces between words in the ciphertext.
Figure 1(middle) displays a section of the homophoniccipher (with spaces) and the original plaintextmessage used in our experiments.2.3 Homophonic Ciphers without spaces(Zodiac-408 cipher)In the previous two cipher systems, the word-boundary information was preserved in the cipher.We now consider a more difficult homophonic ci-pher by removing space characters from the originalplaintext.The English plaintext from the previous examplenow looks like this:?HELLOWORLD ...?and the corresponding ciphertext is:?65 82 51 84 05 60 54 42 51 45 ...?Without the word boundary information, typicaldictionary-based decipherment attacks fail on suchciphers.Zodiac-408 cipher: Homophonic ciphers with-out spaces have been used extensively in the past toencrypt secret messages.
One of the most famoushomophonic ciphers in history was used by the in-famous Zodiac serial killer in the 1960?s.
The killersent a series of encrypted messages to newspapersand claimed that solving the ciphers would revealclues to his identity.
The identity of the Zodiac killerremains unknown to date.
However, the mysterysurrounding this has sparked much interest amongcryptanalysis experts and amateur enthusiasts.The Zodiac messages include two interesting ci-phers: (1) a 408-symbol homophonic cipher withoutspaces (which was solved manually by hand), and(2) a similar looking 340-symbol cipher that has yetto be solved.Here is a sample of the Zodiac-408 cipher mes-sage:...and the corresponding section from the originalEnglish plaintext message:I L I K E K I L L I N G P E O P LE B E C A U S E I T I S S O M U CH F U N I T I S M O R E F U N T HA N K I L L I N G W I L D G A M EI N T H E F O R R E S T B E C A US E M A N I S T H E M O S T D A NG E R O U E A N A M A L O F A L LT O K I L L S O M E T H I N G G I...Besides the difficulty with missing word bound-aries and non-determinism associated with the key,the Zodiac-408 cipher poses several additional chal-lenges which makes it harder to solve than anystandard homophonic cipher.
There are spellingmistakes in the original message (for example,the English word ?PARADISE?
is misspelt as241?PARADICE?)
which can divert a dictionary-basedattack.
Also, the last 18 characters of the plaintextmessage does not seem to make any sense (?EBE-ORIETEMETHHPITI?
).Data: Figure 1 (bottom) displays the Zodiac-408cipher (consisting of 408 tokens, 54 symbol types)along with the original plaintext message.
We runthe new decipherment method (described in Sec-tion 3.1) and show that our approach can success-fully solve the Zodiac-408 cipher.3 DeciphermentGiven a ciphertext message c1...cn, the goal of de-cipherment is to uncover the hidden plaintext mes-sage p1...pn.
The size of the keyspace (i.e., num-ber of possible key mappings) that we have to navi-gate during decipherment is huge?a simple substi-tution cipher has a keyspace size of 26!, whereas ahomophonic cipher such as the Zodiac-408 cipherhas 2654 possible key mappings.Next, we describe a new Bayesian deciphermentapproach for tackling substitution ciphers.3.1 Bayesian DeciphermentBayesian inference methods have become popularin natural language processing (Goldwater and Grif-fiths, 2007; Finkel et al, 2005; Blunsom et al, 2009;Chiang et al, 2010).
Snyder et al (2010) proposeda Bayesian approach in an archaeological decipher-ment scenario.
These methods are attractive for theirability to manage uncertainty about model parame-ters and allow one to incorporate prior knowledgeduring inference.
A common phenomenon observedwhile modeling natural language problems is spar-sity.
For simple letter substitution ciphers, the origi-nal substitution key exhibits a 1-to-1 correspondencebetween the plaintext letters and cipher types.
It isnot easy to model such information using conven-tional methods like EM.
But we can easily spec-ify priors that favor sparse distributions within theBayesian framework.Here, we propose a novel approach for decipher-ing substitution ciphers using Bayesian inference.Rather than enumerating all possible keys (26!
fora simple substitution cipher), our Bayesian frame-work requires us to sample only a small number ofkeys during the decipherment process.Probabilistic Decipherment: Our deciphermentmethod follows a noisy-channel approach.
We arefaced with a ciphertext sequence c = c1...cn andwe want to find the (English) letter sequence p =p1...pn that maximizes the probability P (p|c).We first formulate a generative story to model theprocess by which the ciphertext sequence is gener-ated.1.
Generate an English plaintext sequence p =p1...pn, with probability P (p).2.
Substitute each plaintext letter pi with a cipher-text token ci, with probability P (ci|pi) in orderto generate the ciphertext sequence c = c1...cn.We build a statistical English language model(LM) for the plaintext source model P (p), whichassigns a probability to any English letter sequence.Our goal is to estimate the channel model param-eters ?
in order to maximize the probability of theobserved ciphertext c:argmax?P (c) = argmax??pP?
(p, c) (1)= argmax?
?pP (p) ?
P?
(c|p) (2)= argmax?
?pP (p) ?n?i=1P?
(ci|pi) (3)We estimate the parameters ?
using Bayesianlearning.
In our decipherment framework, a ChineseRestaurant Process formulation is used to modelboth the source and channel.
The detailed genera-tive story using CRPs is shown below:1. i?
12.
Generate the English plaintext letter p1, withprobability P0(p1)3.
Substitute p1 with cipher token c1, with proba-bility P0(c1|p1)4. i?
i+ 15.
Generate English plaintext letter pi, with prob-ability?
?
P0(pi|pi?1) + Ci?11 (pi?1, pi)?+ Ci?11 (pi?1)242Plaintext: D E C I P H E R M E N T I S T H E A N A L Y S I S O F D O C U M E N T SW R I T T E N I N A N C I E N T L A N G U A G E S W H E R E T H E ...Ciphertext: i n g c m p n q s n w f c v f p n o w o k t v c v h u i h g z s n w f vr q c f f n w c w o w g c n w f k o w a z o a n v r p n q n f p n ...Bayesian solution: D E C I P H E R M E N T I S T H E A N A L Y S I S O F D O C U M E N T SW R I T T E N I N A N C I E N T L A N G U A G E S W H E R E T H E ...Plaintext: D E C I P H E R M E N T I S T H E A N A L Y S I SO F D O C U M E N T S W R I T T E N I N ...Ciphertext: 79 57 62 93 95 68 44 77 22 74 59 97 32 86 85 56 82 67 59 67 84 52 86 73 1199 10 45 90 13 61 27 98 71 49 19 60 80 88 85 20 55 59 32 91 ...Bayesian solution: D E C I P H E R M E N T I S T H E A N A L Y S I SO F D O C U M E N T S W R I T T E N I N ...Ciphertext:Plaintext:Bayesian solution (final decoding): I L I K E K I L L I N G P E O P L E B E C A U S EI T I S S O M U C H F U N I T I A M O R E F U N TH A N K I L L I N G W I L D G A M E I N T H E F OR R E S T B E C A U S E M A N I S T H E M O A T DA N G E R T U E A N A M A L O F A L L ...(with spaces shown): I L I K E K I L L I N G P E O P L E B E C A U S EI T I S S O M U C H F U N I T I A M O R EF U N T H A N K I L L I N G W I L D G A M E I NT H E F O R R E S T B E C A U S E M A N I S T H EM O A T D A N G E R T U E A N A M A L O F A L L ...Figure 1: Samples from the ciphertext sequence, corresponding English plaintext message and output from Bayesiandecipherment (using word+3-gram LM) for three different ciphers: (a) Simple Substitution Cipher (top), (b) Homo-phonic Substitution Cipher with spaces (middle), and (c) Zodiac-408 Cipher (bottom).2436.
Substitute pi with cipher token ci, with proba-bility?
?
P0(ci|pi) + Ci?11 (pi, ci)?
+ Ci?11 (pi)7.
With probability Pquit, quit; else go to Step 4.This defines the probability of any given deriva-tion, i.e., any plaintext hypothesis corresponding tothe given ciphertext sequence.
The base distribu-tion P0 represents prior knowledge about the modelparameter distributions.
For the plaintext sourcemodel, we use probabilities from an English lan-guage model and for the channel model, we spec-ify a uniform distribution (i.e., a plaintext letter canbe substituted with any given cipher type with equalprobability).
Ci?11 represents the count of eventsoccurring before plaintext letter pi in the derivation(we call this the ?cache?).
?
and ?
represent Dirich-let prior hyperparameters over the source and chan-nel models respectively.
A large prior value impliesthat characters are generated from the base distribu-tion P0, whereas a smaller value biases charactersto be generated with reference to previous decisionsinside the cache (favoring sparser distributions).Efficient inference via type sampling: We use aGibbs sampling (Geman and Geman, 1984) methodfor performing inference on our model.
We couldfollow a point-wise sampling strategy, where wesample plaintext letter choices for every cipher to-ken, one at a time.
But we already know that thesubstitution ciphers described here exhibit determin-ism in the deciphering direction,1 i.e., although wehave no idea about the key mappings themselves,we do know that there exists only a single plaintextletter mapping for every cipher symbol type in thetrue key.
So sampling plaintext choices for everycipher token separately is not an efficient strategy?our sampler may spend too much time exploring in-valid keys (which map the same cipher symbol todifferent plaintext letters).Instead, we use a type sampling technique similarto the one proposed by Liang et al (2010).
Under1This assumption does not strictly apply to the Zodiac-408cipher where a few cipher symbols exhibit non-determinism inthe decipherment direction as well.this scheme, we sample plaintext letter choices foreach cipher symbol type.
In every step, we samplea new plaintext letter for a cipher type and updatethe entire plaintext hypothesis (i.e., plaintext lettersat all corresponding positions) to reflect this change.For example, if we sample a new choice pnew fora cipher symbol which occurs at positions 4, 10, 18,then we update plaintext letters p4, p10 and p18 withthe new choice pnew.Using the property of exchangeability, we derivean incremental formula for re-scoring the probabil-ity of a new derivation based on the probability ofthe old derivation?when sampling at position i, wepretend that the area affected (within a context win-dow around i) in the current plaintext hypothesis oc-curs at the end of the corpus, so that both the oldand new derivations share the same cache.2 Whilewe may make corpus-wide changes to a derivationin every sampling step, exchangeability allows us toperform scoring in an efficient manner.Combining letter n-gram language models withword dictionaries: Many existing probabilistic ap-proaches use statistical letter n-gram language mod-els of English to assign P (p) probabilities to plain-text hypotheses during decipherment.
Other de-cryption techniques rely on word dictionaries (usingwords from an English dictionary) for attacking sub-stitution ciphers.Unlike previous approaches, our deciphermentmethod combines information from both sources?letter n-grams and word dictionaries.
We build aninterpolated word+n-gram LM and use it to assignP (p) probabilities to any plaintext letter sequencep1...pn.3 The advantage is that it helps direct thesampler towards plaintext hypotheses that resemblenatural language?high probability letter sequenceswhich form valid words such as ?H E L L O?
in-stead of sequences like ?
?T X H R T?.
But in ad-dition to this, using letter n-gram information makes2The relevant context window that is affected when sam-pling at position i is determined by the word boundaries to theleft and right of i.3We set the interpolation weights for the word and n-gramLM as (0.9, 0.1).
The word-based LM is constructed from adictionary consisting of 9,881 frequently occurring words col-lected from Wikipedia articles.
We train the letter n-gram LMon 50 million words of English text available from the Linguis-tic Data Consortium.244our model robust against variations in the origi-nal plaintext (for example, unseen words or mis-spellings as in the case of Zodiac-408 cipher) whichcan easily throw off dictionary-based attacks.
Also,it is hard for a point-wise (or type) sampler to ?findwords?
starting from a random initial sample, buteasier to ?find n-grams?.Sampling for ciphers without spaces: For cipherswithout spaces, dictionaries are hard to use becausewe do not know where words start and end.
We in-troduce a new sampling operator which counters thisproblem and allows us to perform inference usingthe same decipherment model described earlier.
Ina first sampling pass, we sample from 26 plaintextletter choices (e.g., ?A?, ?B?, ?C?, ...) for every ci-pher symbol type as before.
We then run a secondpass using a new sampling operator that iterates overadjacent plaintext letter pairs pi?1, pi in the currenthypothesis and samples from two choices?
(1) adda word boundary (space character ?
?)
between pi?1and pi, or (2) remove an existing space character be-tween pi?1 and pi.For example, given the English plaintext hypoth-esis ?...
A B O Y ...?, there are two sam-pling choices for the letter pair A,B in the secondstep.
If we decide to add a word boundary, our newplaintext hypothesis becomes ?...
A B O Y...?.We compute the derivation probability of the newsample using the same efficient scoring proceduredescribed earlier.
The new strategy allows us to ap-ply Bayesian decipherment even to ciphers withoutspaces.
As a result, we now have a new decipher-ment method that consistently works for a range ofdifferent types of substitution ciphers.Decoding the ciphertext: After the sampling runhas finished,4 we choose the final sample as our En-glish plaintext decipherment output.4For letter substitution decipherment we want to keep thelanguage model probabilities fixed during training, and hencewe set the prior on that model to be high (?
= 104).
We usea sparse prior for the channel (?
= 0.01).
We instantiate a keywhich matches frequently occurring plaintext letters to frequentcipher symbols and use this to generate an initial sample for thegiven ciphertext and run the sampler for 5000 iterations.
Weuse a linear annealing schedule during sampling decreasing thetemperature from 10?
1.4 Experiments and ResultsWe run decipherment experiments on different typesof letter substitution ciphers (described in Sec-tion 2).
In particular, we work with the followingthree ciphers:(a) 414-letter Simple Substitution Cipher(b) 414-letter Homophonic Cipher (with spaces)(c) Zodiac-408 CipherMethods: For each cipher, we run and compare theoutput from two different decipherment approaches:1.
EM Method using letter n-gram LMs follow-ing the approach of Knight et al (2006).
Theyuse the EM algorithm to estimate the chan-nel parameters ?
during decipherment training.The given ciphertext c is then decoded by us-ing the Viterbi algorithm to choose the plain-text decoding p that maximizes P (p)?P?
(c|p)3,stretching the channel probabilities.2.
Bayesian Decipherment method usingword+n-gram LMs (novel approach describedin Section 3.1).Evaluation: We evaluate the quality of a particulardecipherment as the percentage of cipher tokens thatare decoded correctly.Results: Figure 2 compares the decipherment per-formance for the EM method with Bayesian deci-pherment (using type sampling and sparse priors)on three different types of substitution ciphers.
Re-sults show that our new approach (Bayesian) out-performs the EM method on all three ciphers, solv-ing them completely.
Even with a 3-gram letter LM,our method yields a +63% improvement in decipher-ment accuracy over EM on the homophonic cipherwith spaces.
We observe that the word+3-gram LMproves highly effective when tackling more complexciphers and cracks the Zodiac-408 cipher.
Figure 1shows samples from the Bayesian decipherment out-put for all three ciphers.
For ciphers without spaces,our method automatically guesses the word bound-aries for the plaintext hypothesis.245Method LM Accuracy (%) on 414-letterSimple Substitution CipherAccuracy (%) on 414-letterHomophonic SubstitutionCipher (with spaces)Accuracy (%) on Zodiac-408 Cipher1.
EM 2-gram 83.6 30.93-gram 99.3 32.6 0.3?
(?28.8 with 100 restarts)2.
Bayesian 3-gram 100.0 95.2 23.0word+2-gram 100.0 100.0word+3-gram 100.0 100.0 97.8Figure 2: Comparison of decipherment accuracies for EM versus Bayesian method when using different languagemodels of English on the three substitution ciphers: (a) 414-letter Simple Substitution Cipher, (b) 414-letter Homo-phonic Substitution Cipher (with spaces), and (c) the famous Zodiac-408 Cipher.For the Zodiac-408 cipher, we compare the per-formance achieved by Bayesian decipherment underdifferent settings:?
Letter n-gram versus Word+n-gram LMs?Figure 2 shows that using a word+3-gram LMinstead of a 3-gram LM results in +75% im-provement in decipherment accuracy.?
Sparse versus Non-sparse priors?We find thatusing a sparse prior for the channel model (?
=0.01 versus 1.0) helps for such problems andproduces better decipherment results (97.8%versus 24.0% accuracy).?
Type versus Point-wise sampling?Unlikepoint-wise sampling, type sampling quicklyconverges to better decipherment solutions.After 5000 sampling passes over the entiredata, decipherment output from type samplingscores 97.8% accuracy compared to 14.5% forthe point-wise sampling run.5We also perform experiments on shorter substitu-tion ciphers.
On a 98-letter simple substitution ci-pher, EM using 3-gram LM achieves 41% accuracy,whereas the method from Ravi and Knight (2009)scores 84% accuracy.
Our Bayesian method per-forms the best in this case, achieving 100% withword+3-gram LM.5 ConclusionIn this work, we presented a novel Bayesian deci-pherment approach that can effectively solve a va-5Both sampling runs were seeded with the same random ini-tial sample.riety of substitution ciphers.
Unlike previous ap-proaches, our method combines information fromletter n-gram language models and word dictionar-ies and provides a robust decipherment model.
Weempirically evaluated the method on different substi-tution ciphers and achieve perfect decipherments onall of them.
Using Bayesian decipherment, we cansuccessfully solve the Zodiac-408 cipher?the firsttime this is achieved by a fully automatic method ina strict decipherment scenario.For future work, there are other interesting deci-pherment tasks where our method can be applied.One challenge is to crack the unsolved Zodiac-340cipher, which presents a much harder problem thanthe solved version.AcknowledgementsThe authors would like to thank the reviewers fortheir comments.
This research was supported byNSF grant IIS-0904684.ReferencesPhil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-borne.
2009.
A Gibbs sampler for phrasal syn-chronous grammar induction.
In Proceedings of theJoint Conference of the 47th Annual Meeting of theACL and the 4th International Joint Conference onNatural Language Processing of the Asian Federa-tion of Natural Language Processing (ACL-IJCNLP),pages 782?790.David Chiang, Jonathan Graehl, Kevin Knight, AdamPauls, and Sujith Ravi.
2010.
Bayesian inference forfinite-state transducers.
In Proceedings of the Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics - Human LanguageTechnologies (NAACL/HLT), pages 447?455.246Eric Corlett and Gerald Penn.
2010.
An exact A* methodfor deciphering letter-substitution ciphers.
In Proceed-ings of the 48th Annual Meeting of the Association forComputational Linguistics, pages 1040?1047.Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-bin.
1977.
Maximum likelihood from incomplete datavia the EM algorithm.
Journal of the Royal StatisticalSociety, Series B, 39(1):1?38.Persi Diaconis.
2008.
The Markov Chain Monte Carlorevolution.
Bulletin of the American Mathematical So-ciety, 46(2):179?205.Jenny Finkel, Trond Grenager, and Christopher Manning.2005.
Incorporating non-local information into infor-mation extraction systems by Gibbs sampling.
In Pro-ceedings of the 43rd Annual Meeting of the Associa-tion for Computational Linguistics (ACL), pages 363?370.Ravi Ganesan and Alan T. Sherman.
1993.
Statisticaltechniques for language recognition: An introductionand guide for cryptanalysts.
Cryptologia, 17(4):321?366.Stuart Geman and Donald Geman.
1984.
Stochastic re-laxation, Gibbs distributions and the Bayesian restora-tion of images.
IEEE Transactions on Pattern Analysisand Machine Intelligence, 6(6):721?741.Sharon Goldwater and Thomas Griffiths.
2007.
A fullyBayesian approach to unsupervised part-of-speech tag-ging.
In Proceedings of the 45th Annual Meeting of theAssociation of Computational Linguistics, pages 744?751.Thomas Jakobsen.
1995.
A fast method for cryptanalysisof substitution ciphers.
Cryptologia, 19(3):265?274.Kevin Knight, Anish Nair, Nishit Rathod, and Kenji Ya-mada.
2006.
Unsupervised analysis for deciphermentproblems.
In Proceedings of the Joint Conference ofthe International Committee on Computational Lin-guistics and the Association for Computational Lin-guistics, pages 499?506.Percy Liang, Michael I. Jordan, and Dan Klein.
2010.Type-based MCMC.
In Proceedings of the Conferenceon Human Language Technologies: The 2010 AnnualConference of the North American Chapter of the As-sociation for Computational Linguistics, pages 573?581.Edwin Olson.
2007.
Robust dictionary attack of shortsimple substitution ciphers.
Cryptologia, 31(4):332?342.David Oranchak.
2008.
Evolutionary algorithm for de-cryption of monoalphabetic homophonic substitutionciphers encoded as constraint satisfaction problems.
InProceedings of the 10th Annual Conference on Geneticand Evolutionary Computation, pages 1717?1718.Shmuel Peleg and Azriel Rosenfeld.
1979.
Break-ing substitution ciphers using a relaxation algorithm.Comm.
ACM, 22(11):598?605.Sujith Ravi and Kevin Knight.
2008.
Attacking deci-pherment problems optimally with low-order n-grammodels.
In Proceedings of the Empirical Methods inNatural Language Processing (EMNLP), pages 812?819.Sujith Ravi and Kevin Knight.
2009.
Probabilistic meth-ods for a Japanese syllable cipher.
In Proceedingsof the International Conference on the Computer Pro-cessing of Oriental Languages (ICCPOL), pages 270?281.Benjamin Snyder, Regina Barzilay, and Kevin Knight.2010.
A statistical model for lost language decipher-ment.
In Proceedings of the 48th Annual Meeting ofthe Association for Computational Linguistics, pages1048?1057.247
