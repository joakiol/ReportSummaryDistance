Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 47?50,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsAutomated Suggestions for MiscollocationsAnne Li-E Liu David Wible Nai-Lung TsaoResearch Centre for Englishand Applied LinguisticsGraduate Institute of Learning andInstructionGraduate Institute of Learning andInstructionUniversity of Cambridge National Central University National Central UniversityCambridge, CB3 9DP,United KingdomJhongli City, Taoyuan County32001, TaiwanJhongli City, Taoyuan County32001, Taiwanlel29@cam.ac.uk wible45@yahoo.com beaktsao@gmail.comAbstractOne of the most common and persistent errortypes in second language writing is colloca-tion errors, such as learn knowledge instead ofgain or acquire knowledge, or make damagerather than cause damage.
In this work-in-progress report, we propose a probabilisticmodel for suggesting corrections to lexicalcollocation errors.
The probabilistic model in-corporates three features: word associationstrength (MI), semantic similarity (via Word-Net) and the notion of shared collocations (orintercollocability).
The results suggest thatthe combination of all three features outper-forms any single feature or any combinationof two features.1 Collocation in Language LearningThe importance and difficulty of collocations forsecond language users has been widely acknowl-edged and various sources of the difficulty putforth (Granger 1998, Nesselhauf 2004, Howarth1998, Liu 2002, inter alia).
Liu?s study of a 4-million-word learner corpus reveals that verb-noun(VN) miscollocations make up the bulk of the lexi-cal collocation errors in learners?
essays.
Our studyfocuses, therefore, on VN miscollocation correc-tion.2 Error Detection and Correction in NLPError detection and correction have been twomajor issues in NLP research in the past decade.Projects involving learner corpora in analyzing andcategorizing learner errors include NICT JapaneseLearners of English (JLE), the Chinese Learners ofEnglish Corpus (Gamon et al, 2008) and EnglishTaiwan Learner Corpus (or TLC) (Wible et al,2003).
Studies that focus on providing automaticcorrection, however, mainly deal with errors thatderive from closed-class words, such as articles(Han et al, 2004) and prepositions (Chodorow etal., 2007).
One goal of this work-in-progress is toaddress the less studied issue of open class lexicalerrors, specifically lexical collocation errors.3 The Present StudyWe focus on providing correct collocation sug-gestions for lexical miscollocations.
Three featuresare employed to identify the correct collocationsubstitute for a miscollocation: word associationmeasurement, semantic similarity between the cor-rection candidate and the misused word to be re-placed, and intercollocability (i.e., the concept ofshared collocates in collocation clusters proposedby Cowie and Howarth, 1995).
NLP research onlearner errors includes work on error detection anderror correction.
While we are working on both,here we report specifically on our work on lexicalmiscollocation correction.4 MethodWe incorporate both linguistic and computa-tional perspectives in our approach.
84 VN miscol-locations from Liu?s (2002) study were employedas the training and the testing data in that eachcomprised 42 randomly chosen miscollocations.Two experienced English teachers1 manually wentthrough the 84 miscollocations and provided a listof correction suggestions.
Only when the systemoutput matches to any of the suggestions offered1 One native speaker and one experienced non-native English teacher.47by the two annotators would the data be includedin the result.
The two main knowledge resourcesthat we incorporated are British National Corpus2and WordNet (Miller, 1990).
BNC was utilized tomeasure word association strength and to extractshared collocates while WordNet was used in de-termining semantic similarity.
Our probabilisticmodel that combines the features is described insub-section 4.4.
Note that all the 84 VN miscollo-cations are combination of incorrect verbs and fo-cal nouns, our approach is therefore aimed to findthe correct verb replacements.4.1 Word Association MeasurementThe role of word association in miscollocationsuggestions are twofold: 1. all suggested correctcollocations in any case have to be identified ascollocations; thus, we assume candidate replace-ments for the miscollocate verbs must exceed athreshold word association strength with the focalnoun; 2. we examine the possibility that the higherthe word association score the more likely it is tobe a correct substitute for the wrong collocate.
Weadopt Mutual Information (Church et al 1991) asour association measurement.4.2 Semantic SimilarityBoth Gitsaki et al (2000) and Liu (2002) sug-gest a semantic relation holds between a miscollo-cate and its correct counterpart.
Following this, weassume that in the 84 miscollocations, the miscol-locates should stand in more or less a semantic re-lation with the corrections.
For example, say in anattested learner miscollocation say story is found tobe a synonym of the correct verb tell in WordNet.Based on this assumption, words that show somedegree of semantic similarity with the miscollocateare considered possible candidates for replacing it.To measure similarity we take the synsets ofWordNet to be nodes in a graph.
We quantify thesemantic similarity of the incorrect verb in a mis-collocation with other possible substitute verbs bymeasuring graph-theoretic distance between thesynset containing the miscollocate verb and thesynset containing candidate substitutes.
In cases ofpolysemy, we take the closest synsets for the dis-tance measure.
If the miscollocate and the candi-2 The British National Corpus, version 3 (BNC XML Edition).
2007.URL: http://www.natcorp.ox.ac.uk/date substitute occur in the same synset, then thedistance between them is zero.The similarity measurement function is as fol-lows (Tsao et al, 2003):)),max(2),(1(max),()(),(21 21jiji ssjiwsynsetswsynsets LLssdiswwsim ?
?= ?
?,where  means the node path length be-tween the synset  and  in WordNet hy-per/hypo tree.
means the level number of s inhyper/hypo tree and the level of top node is 1.Multiplying  by 2 ensures the simi-larity is less than 1.
If   and  are synonymous,the similarity will be 1.
),( ji ssdisis jssL),max(ji ssLLis js4.3 Shared Collocates in Collocation ClustersFutagi et al(2008) review several studies whichadopt computational approaches in tackling collo-cation errors; yet none of them, including Futagi etal., include the notion of collocation cluster.
Weborrow the cluster idea from Cowie & Howarth(1995) who propose ?overlapping cluster?
to denotesets of collocations that carry similar meaning andshared collocates.
Figure 1 represents a collocationcluster that expresses the concept of ?bringingsomething into actuality.?
The key here is that notall VN combinations in Figure 1 are acceptable.While fulfill and achieve collocate with the fournouns on the right, realize does not collocate withpurpose, as indicated by the dotted line.
Cowie andHowarth?s point is that collocations that can beclustered via overlapping collocates can be thesource of collocation errors for language learners.That both fulfill and reach collocate with goal andthe further collocability of fulfill with ambition andpurpose plausibly lead learners to assume thatreach shares this collocability as well, leading byovergeneralization to the miscollocations reach anambition or reach a purpose.Figure 1.
Collocation cluster of ?bringing somethinginto actuality?48We employ the ideas of ?collocation cluster?
and?shared collocates?
in identifying correct counter-parts to the miscollocations.
Specifically, takingthe miscollocation reach their purpose as a startingpoint, our system generates a collocation cluster byfinding the verbs that collocate with purpose andnouns that reach collocates with.
We consider thisformed cluster the source that contains the possiblecorrect replacement for reach in reach their pur-pose.
By finding verbs that not only collocate withpurpose but also share the most other collocatingnouns with the wrong verb reach, successfully, weidentified candidate substitutes fulfill and achievefor the incorrect verb reach.4.4 Our Probabilistic ModelThe three features we described above are inte-grated into a probabilistic model.
Each feature isused to look up the correct collocation suggestionfor a miscollocation.
For instance, cause damage,one of the possible suggestions for the miscolloca-tion make damage, is found to be ranked the 5thcorrection candidate by using word associationmeasurement merely, the 2nd by semantic similarityand the 14th by using shared collocates.
If we com-bine the three features, however, cause damage isranked first.The conditional probability of the case wherethe candidate is a correct one can be presented as:)( ,mcFverbcorrectaiscPwhere c means a candidate for a specific miscollo-cation and Fc, m means the features values betweenm (misused words) and c (candidates).
Accordingto Bayes theorem and Bayes assumption, whichassume that these features are independent, theprobability can be computed by:( ) ( ) ( )( )( ) ( )( )????
?=mcmcFfcFfcmcccmcmcc fPSPSfPFPSPSFPFSP,,,,,where  means the situation ?c is a correct verb?,as described above and f is one of the three particu-lar features.
We use probability values to chooseand rank the K-best suggestions.cS5 Experimental ResultsAny found VN combination via our probabilisticapproach was compared to the suggestions madeby the two human experts.
A match would becounted as a true positive.
A discrete probabilitydistribution is produced for each feature.
We di-vided feature value into five levels and obtainedprior predicting value for each level of the threefeatures.
For example, we divided MI value to fivelevels (<1.5, 1.5~3.0, 3.0~4.5, 4.5~6, >6).
The fiveranks for semantic similarity and normalizedshared collocates number are 0.0~0.2, 0.2~0.4,0.4~0.6, 0.6~0.8 and 0.8 ~1.0.
For every feature,we obtain a predicting value for each level after thetraining process.
The predicting value is shownas ( )( )fP SfP c .
In line with that, P(MI>6)  means theprobability of all VN collocations retrieved fromBNC in which the MI value is higher than 6whereas P(MI>6| ) shows the probability of  allcorrect VN collocations with the MI value higherthan 6.cSDifferent combinations of the three features aremade on the basis of the probabilistic model de-scribed in Section 4.4.
Seven models derive fromsuch combinations (See Table 1).
Table 2 showsthe precision of k-best suggestions for each model.Models Feature(s) consideredM 1 MI (Mutual Information)M 2 SS (Semantic Similarity)M 3 SC (Shared Collocates)M 4 MI + SSM 5 MI + SCM 6 SS + SCM 7 MI + SS + SCTable 1.
Models of feature combinations.K-Best M1 M2 M3 M4 M5 M6 M71 16.67 40.48 22.62 48.81 29.76 55.95 53.572 36.90 53.57 38.10 60.71 44.05 63.1 67.863 47.62 64.29 50.00 71.43 59.52 77.38 78.574 52.38 67.86 63.10 77.38 72.62 80.95 82.145 64.29 75.00 72.62 83.33 78.57 83.33 85.716 65.48 77.38 75.00 85.71 83.33 84.52 88.107 67.86 80.95 77.38 86.90 86.90 86.9 89.298 70.24 83.33 82.14 86.90 89.29 88.1 91.679 72.62 86.90 85.71 88.10 92.86 90.48 92.8610 76.19 86.90 88.10 88.10 94.05 90.48 94.05Table 2.
The precision rate of Model 1- 7.K-Best M2 M6 M71 aim *obtain *acquire2 generate share share3 draw *develop *obtain4 *obtain generate *develop5 *develop *acquire *gainTable 3.
The K-Best suggestions for getknowledge.49Table 2 shows that, considering the results foreach feature run separately (M1-M3), the feature?semantic similarity?
(M2) outperforms the othertwo.
Among combined feature models (M4-M7),M7 (MI + SS+ SC), provides the highest propor-tion of true positives at every value of k except k =1.
The full hybrid of all three features (M7) outper-forms any single feature.
The best results areachieved when taking into account both statisticaland semantic features.
This is illustrated with re-sults for the example get knowledge in Table 3 (theasterisks (*) indicate the true positives.
)6 ConclusionIn this report of work in progress, we present aprobabilistic model that adopts word associationmeasurement, semantic similarity and shared col-locates in looking for corrections for learners?
mis-collocations.
Although only VN miscollocationsare examined, the model is designed to be applica-ble to other types of miscollocations.
Applyingsuch mechanisms to other types of miscollocationsas well as detecting miscollocations will be thenext steps of this research.
Further, a larger amountof miscollocations should be included in order toverify our approach and to address the issue of thesmall drop of the full-hybrid M7 at k=1.AcknowledgmentsThe work reported in this paper was partiallysupported by the grants from the National ScienceCouncil, Taiwan (Project Nos.
96-2524-S-008-003- and 98-2511-S-008-002-MY2)ReferencesAnne.
Li-E Liu 2002.
A Corpus-based Lexical SemanticInvestigation of VN Miscollocations in TaiwanLearners?
English.
Master Thesis, Tamkang Univer-sity, Taiwan.Anthony P Cowie and Peter Howarth.
1995.
Phrase-ological Competence and Written Proficiency, PaperPresented at the British Association of Applied Lin-guistics Conference (BAAL), Southampton, England,September.Christina Gitsaki, Nagoya Shoka Daigaku, and RichardP.
Taylor.
2000.
English Collocations and TheirPlace in the EFL Classroom, Available at:http://www.hum.nagoya-cu.ac.jp/~taylor/publications/collocations.html.Claudia Leacock and Martin Chodorow.
2003.
Auto-mated Grammatical Error Detection, In MD Shermis& JC Burstein (Eds.
), Automated Essay Scoring: ACross-disciplinary, Mahwah, NJ: Lawrence ErlbaumAssociates.David Wible, Chin-Hwa Kuo, Nai-Lung Tsao, Anne Li-E Liu, and Hsiu-Lin Lin.
2003.
Bootstrapping in aLanguage Learning Environment.
Journal of Com-puter Assisted Learning, 19, 90-102.George Miller.
1990.
WordNet: An On-line LexicalDatabase, International Journal of Lexicography.Kenji Kita and Hiroaki Ogata.
1997.
Collocations inLanguage Learning: Corpus-based Automatic compi-lation of Collocations and Bilingual CollocationConcordancer, Computer Assisted Language Learn-ing.
Vol.10, No.
3, 229-238.Kenneth Church, William Gale, Patrick Hanks andDonald Hindle.
1991.
Using Statistics in LexicalAnalysis, in Zernik (ed), Lexical Acquisition: UsingOn-line Resources to Build a Lexicon, Lawrence Erl-baum, pp.
115-164.Martin Chodorow, Joel R. Tetreault and Na-Rae Han.2007.
Detection of Grammatical Errors InvolvingPrepositions, Proceedings of the 45th Annual Meet-ing of the Association for Computational Linguistics,Special Interest Group on Semantics, Workshop onPrepositions, 25-30.Michael Gamon, Jianfeng Gao, Chris Brockett, Alexan-dre Klementiev, William B. Dolan, Dmitriy Belenko,Lucy Vanderwende.
2008.
Using Contextual SpellerTechniques and Language Modeling for ESL ErrorCorrection, Proceedings of The Third InternationalJoint Conference on Natural Language Processing,Hyderabad, India.Na-Rae Han, Martin Chodorow and Claudia Leacock.2004.
Detecting Errors in English Article Usage witha Maximum Entropy Classifier Trained on a Large,Diverse Corpus, Proceedings of the 4th InternationalConference on Language Resources and Evaluation,Lisbon, Portugal.Nai-Lung Tsao, David Wible and Chin-Hwa Kuo.
2003.Feature Expansion for Word Sense Disambiguation,Proceedings of the International Conference onNatural Language Processing and Knowledge Engi-neering, 126-131.Peter Howarth.
1998.
Phraseology and Second Lan-guage Acquisition.
Applied Linguistics.
19/1, 24-44.Yoko Futagi, Paul Deane, Martin Chodorow & JoelTetreault.
2008.
A computational approach to detectingcollocation errors in the writing of non-native speakers ofEnglish.
Computer Assisted Language Learn-ing,21:4,353 ?
36750
