TIPSTER/MUC-5INFORMATION EXTRACTION SYSTEM EVALUATIONBeth M. SundheimNaval Command, Control, and Ocean Surveillance CenterRDT&E Division (NRaD)Information Access Technology Project Team, Code 44208San Diego, CA 92152-7420sundheim @nose.railINTRODUCTIONThree information extraction systemevaluations using Tipster data were conducted in thecontext of Phase 1 of the Tipster Text program.Interim evaluations were conducted in September,1992, and February, 1993; the final evaluation wasconducted in July, 1993.
The final evaluationincluded not only the Tipster-supported inform~onextraction contractors but thirteen other participantsas well.
This evaluation was the topic of the FifthMessage Understanding Conference (MUC-5) inAugust, 1993.
With particular espect o theresearch and development tasks of the Tipstercontractors, the goal of these evaluations has beento assess uccess in terms of the development ofsystems to work in both English and Japanese(BBN, GE/CMU, and NMSU/Brandeis) and/or inboth the joint ventures and microelectronicsdomains (BBN, GE/CMU, NMSU/Brandeis, andUMass/Hughes).The methodology associated with theseevaluations has been under development since 1987,when the series of Message UnderstandingConferences began.
The evaluations have pushedtechnology to handle the recurring languageproblems found in sizeable samples of naturally-occuring text.
Designing the evaluations around aninformation extraction application of text processingtechnology has made it possible to discuss NLPtechniques at a practical level and to gain insightinto the capabilities of complex systems.However, any such evaluation testbedapplication will undoubtedly differ in importantrespects from a real-life application.
Thus, there isonly an indirect connection between the evaluationresults for a system and the suitability of applyingthe system to performance of a task in anoperational setting.
A fairly large number ofmetrics have been defined that respond to the varietyof subtasks inherent in information extraction andthe varying perspectives of evaluation consumers.The evaluations measure coverage, accuracy,and classes of error on each language-domain pair,independently of all other language-domain pairsthat the system may be tested on.
With its duallanguage and domain requirements and challengingtask definition, Tipster Phase 1 pushed especiallyhard on issues such as portability tools, language-and domain- independent architectures andalgorithms, and system efficiency.
These aspects ofsoftware were not directly evaluated, althoughinformation concerning some or all of them may befound in the papers prepared by the evaluationparticipants,THE EVALUATION PROCESSThe Tipster contractors were allowed access tothe training corpus (articles and hand-codedtemplates for a given language-domain pair) andassociated materials (documentation, softwareresources, lexical resources) as they were beingprepared over the course of Phase 1.
The articlesand corresponding hand-coded templates from thetest corpus were held in reserve for use as blind-testmaterials during evaluation periods; new test setswere used for each evaluation.
A description of thetraining and test corpora is contained in \[1\].
ThoseMUC-5 evaluation participants who were notTipster contractors were allowed access to trainingmaterials in March, 1993, when major updatesresulting from decisions made at the Tipster interimevaluation in February had been completed andpermission for MUC-5 participants to use most ofthe copyrighted articles had been obtained.
Table 1identifies the MUC-5 evaluation participants and thelanguage-domain pairs on which their systems wereevaluated.The evaluation participants (Tipster and non-Tipster) were also provided with evaluationsoftware, prepared via NRaD contract o SAIC, tohelp them monitor the performance benefits ofalternative software solutions they were exploring intheir research \[9\].
The evaluation software, corpora,147documentation, and miscellaneous other resourceswere distributed primarily through electronic mailand electronic fde transfer.
Virtually every item wasupdated numerous times, and updates continued onsome of them right up to the start of final testing.Personnel at the Consortium for Lexical Research(New Mexico State University) and the Institute forDefense Analyses played critical roles in makingthese materials available for electronic transfer,At the start of the test week for eachevaluation, the participants were suppliedelectronically with encoded test sets of articles,which they were to decode only when they wereready to begin testing.
Testing was conducted bythe participants at their own sites in accordance witha strict test protocol.
After their systems processedthe texts and produced the extracted information inthe expected template format, the participantsMUC-$ CLASS OFPARTICIPANT PARTICIPATIONBBN TipsterGE/CMU TipsterLanguage Sys., Inc. non-TipsterMITRE non-TipsterNEC (Japan) non-TipsterNMSU/Brandeis TipsterNYU non-TipsterPRC, Inc. non-TipsterSRA Corp. non-TipsterSRI International non-TipsterTRW non-TipsterUnisys-Paramax non-TipsterUManitoba (Canada) non-TipsterUMass/Hughes TipsterUMichigan non-TipsterUSouthern California non-TipsterUSussex (UK) non-TipsterSYSTEMPLUMSHOGUN~EXTRACTDBGALEMBICVENIEXDIDEROTPROTEUSPAKTUSSOLOMONFASTUSDEFFCBASNUBACIRCUSLINKSNAPSUSSEXTable 1.
MUC-5 ParticipationEJV EME J JV JMEX X X XX X X XX XXXXXXXXXXXXXXX X XXXXXXCHARAC-TERISTICTest setsS i tesPrimarymetr ics12-MO TIPSTERTEST (SEP92)EJV core 1, fullJJV core 1, fullTipster 4Recall-precision18-MO TIPSTER MUC-5 DRY-RUN 24-MO/MUC-$TEST ~FEB93) TEST (MAY93) FINAL TEST (JUL93)E.IV core l, full EJV ~ core l, full EJV core t, fullJJV core 1, full JJV core 1, full JJV core 1, fullEME full EME full EME fullJME partial 2 JME partial 2 JME fullTipster ~ Non-Tipster 6 Tipster 5Non-Tipster 6Recall-precision Error ErrorTable 2.
Tipster Phase i extraction system evaluationsl,,Core- refers to a core set of JV template slots: the <template>content slo , the <tie-up-relationship>status, entity, and Joint-venture slots, the <entity> name, aliases, location, national ity,  type, andent i ty - re la t ionsh ip  slots, and the <entity-relat ionship> ent i ty l ,  ent i ty2,  re l -ent2 - to -ent l ,  andstatus slots.2"Partial" means that all slots except hose in the <packaging> object were part of the evaluation.3The EJV test set for the MUC dry run contained fewer articles than the 18-month Tipster evaluation, due to restrictionson the right to use articles from some sources for MUC-5.4The UMass/Hughes team was not yet under contract and did not participate in this evaluation.5The UMass/Hughes team was tasked to work only in English (EJV, EME).6All thirteen non-Tipster MUC-5 sites worked in just one domain; two worked in t;oth languages, one worked i:Japanese only, and ten worked in English only.
See table 1.148electronically transfered the templates to theGovernment for scoring.Using the evaluation software prepared bySAIC, evaluators may score templates fullyautomatically (batch mode) or partially interactively(interactive mode).
Since interactive scoringproduces a slightly more accurate performanceevaluation, scoring for the formal evaluations iusually done in that way.
Some of the sameanalysts who hand-coded the answer-key templatesprepared written guidelines for conducting theinteractive scoring and did the scoring.
SAICconducted statistical significance tests on the 24-month/MUC-5 final test scores for the overallmetrics of performance \[3\].Table 2 summarizes the Phase 1 evaluations interms of the test sets, participating sites, andprimary evaluation metrics.
7 Since the JV templatewas especially complex, JV testing was done in twoways for each evaluation: (1) the core portion of thetemplate, including the identification of tie-ups,entities, and relationships of entities within tie-ups,(2) the full template.
The first microelectronics testwas conducted at the 18-month point; up until a fewweeks prior to that test, the Tipster contractors hadhad only a small portion of the EME and JMEcorpora vailable to them.
The first JME evaluation(at 18 months) was conducted using all but the<packaging> objects.The same test sets used for the Tipster 18-month evaluation were used for the MUC-5 dry run,with one exception: certain articles in the F.JV testset had to be omitted because permission for non-Tipster MUC-5 participants to use thosecopyrighted articles had not been obtained.
(Permission to use all but two of these sources wereobtained in time for the MUC-5 final test.)
TheTipster contractors did not participate in the dry run.The primary performance metrics changed inthe course of Phase 1.
These are discussed below.Evaluation Criteria and MetricsIn assessing the performance of theinformation extraction systems, we are interested inknowing the classes of errors made and the7This tabulation ignores the fact that the period of timecovered by Tipster Phase 1 also included MUC-4.
TheTipster Phase 1 contractors were evaluated for MUC-4in the terrorism domain even as they were beginningtheir Tipster research and development in the Tipsterdomains \[MUC-4\].circumstances in which those errors were made.
Weare also interested in performance from anapplications perspective in terms of thecompleteness and accuracy of the database fillsgenerated by the system.
The criteria re limited tothose that can be measured without access toanything more than the templates that the systemsgenerated.
Using these criteria, we attempt to assessthe current state of the art, measure progress relativeto previous evaluations, and compare the taskperformance ofmachines with that of humans.The scoring software classifies each piece ofextracted information into one of the followingscoring categories: correct, partial, incorrect,spurious, missing, and noncommittal.
Systems arepenalized for having missed pertinent information,for having "hallucinated" more information than wasactually pertinent, and for having otherwise xtractedmismatching pieces of information.
In order toreveal information about he circumstances affectingperformance, the scoring software calculates scoresat the following levels of granularity: for each slotin each template, for each object type in eachtemplate, and overall for each template; for each slotin the test set, for each object type in the test set,and overall for the test set.Two sets of metrics were in force for MUC-5\[4\].
The first set of metrics is based on theclassification error rate and includes an overallmetric (error per response fill) and three secondary,Oiagnostic metrics (undergeneration, vergeneration,and substitution).
These secondary metricscorrespond to the three penalty situations describedabove.
The error per response fill metric and thesecondary metrics are together referred to as theerror-based metrics.The second set of metrics measures thecompleteness (recall) and accuracy (precision) of theextracted information.
These are supplemented bythe undergeneration a d overgeneration metricsmentioned above, which serve to isolate thesystem's hortfall in recall due to undergenerationand the system's shortfall in precision due toovergeneration.
Recall and precision are combinedinto a weighted overall measure called the F-measure.
Recall, precision, and F-measure aretogether referred to as the recall-precision-basedmetrics.The error-based metrics erved as the officialmetrics for the MUC-5 evaluation, meaningessentially that any ranking of systems by overallperformance would be done on the basis of error perresponse fill rather than F-measure.
However, as it149turned out, statistical significance tests showed thatsystem ranldngs on the basis of error per responsefill are very consistent with those made on the basisof F-measure (see discussion below and \[3\]).
Bothsets of metrics play important roles in thediscussion found in this paper.An appendix to this volume contains ummarytallies and scores for each of the Tipster systems.The rightmost columns in the tables contain thescores for the error-based and recall-precision-basedmetrics; other columns contain the raw tallies.
Therows in the top portion of the tables containsummary statistics for each slot and object; the rowsat the bottom contain overall statistics.
See thepreface to the appendix and \[4\] for furtherinformation on reading the sc~e reports.Updates to the Test Design and DataEach of the interim evaluations resulted insignificant updates to the evaluation design.
For the12-month test, the evaluation software that had beenused for MUC-4 was rewritten by SAIC toaccomodate the object-oriented Tipster templates.Issues that were addressed in the interim between the12-month and the 18-month tests include JVtemplate formating (especially in the Japanesetemplate), performance metrics (probabifity of falsealarm as alternative toprecision, system-independentversion of  recall), object alignment by theevaluation software 8 (content-based as well asthreshold-based alignment options, alignmentoptimization based on score rather than on numbercorrec0, and evaluation software support for humanperformance studies (scoring of one set of hand-coded templates versus another).At the 18-month meeting, decisions were maderegarding scoring for the 24-month/MUC-59evaluation.
The principal decision was to supplantrecall and precision with a modified formulation ofthe error rate metric that had been in experimentalusage for the 18-month test.
The revised metric wasnamed error per response fill because it is system-dependent (i.e., the denominator in the formulavaries across systems according to the number ofspurious fills generated, and it also varies becausethe answer keys allow for a somewhat variablenumber of expected fills).
Error per response fill8Object alignment as implemented for MUC-5 isdiscussed in the next section.9Since the MUC-5 evaluation was the 24-monthevaluation for the Tipster contractors, the evaluationwill hereafter be referred to simply as the MUC-5evaluation.became the primary measure of performance.However, a less system-dependent rror rate metricwas also implemented; this metric was termed therichness-normalized rror.
These changes to themetrics necessitated significant reprogramming ofthe evaluation software.
In addition, the decisionwas made to convert portions of the JV templatefrom objects to complex slots 10, and this resultedin significant updates to the evaluation software, JVcorpora, and JV documentation.
Another decisionresulting from this meeting was to ease the objectalignment criteria, largely because of the difficultyof setting valid threshold values given thesparseness of the fills in many of the objects in theanswer key.The MUC-5 dry run was conducted after allthese updates h~ been completed.
Between the dryrun and the final test two months later, furtherupdates were made to the evaluation software,including a revised way of scoring two-part(complex) slots in the JV template.
The newmethod gives separate scores to each part of the two-part fill, rather than giving one score to the complexfill as a whole.
Another update was theimplementation of a limited two-pass objectalignment strategy, which results in slightlyimproved object alignments because more of theinformation on the interrelationships among entitiesis present when objects that reference the entities arealigned.The intention had been to eliminate someevaluation criteria before the MUC-5 effort began inearnest in March, 1993; however, some of thedecisions made at the 18-month meeting weretentative and, in the end, few simplifications weremade at that time.
The net result was that thenumber of performance measures has increased sinceMUC-4, and it is clear that there is still no clearanswer as to the single most appropriate criterion toapply to assessing performance on an informationextraction task.
The good news is that the error perresponse fill and the F-measure provide consistentviews of the relative performance of systems, andtherefore technology consumers may choose to usewhichever set of metrics they feel is mostappropriate for their purposes.
All thisexperimentation resulted in other useful informationas well about system-independent metrics, objectalignment approaches, and template design, amongother things.l OThis conversion affected three parts of the JVtemplate: ownership percent, product/service,and activity site.
After the conversion, each ofthese was represented in the template as a two-part slotrather than as an object with two slots.150Al ignment  and Scor ingSystem-generated (r sponse) templates must bealigned with the answer-key (key) templates forscoring.
Alignment akes place at all levels wherethere exist more than one response instance of agiven kind and/or more than one key instance.These levels include the template level, the objectlevel, and the slot-fill level.
In each case, the intentis to fmd the alignment that will provide the bestcontent match between the key and the response.
Atthe object level, there is also the intent o determinewhether a response object should be rejected foralignment purposes for falling to show anysubstantial degree of match with a key object.Alignment at the template l vel is trivial; it isdone on the basis of matching the <template>doc nr fills in the key and response.
At the slot-fill level, when there are more than one key and/orresponse slot-fill for a given instance of a slot type,alignment is done on the basis of the degree ofmatch between key and response fills.
Slot-fillalignments that the alignment program can onlyguess at may be revised inter'actively during thescoring stage.Alignment at the object level is the mostcompficated and controversial spect of alignment.It takes place prior to scoring, and it is normallydone fully automatically because to do itinteractively would be so time-consuming as to bevirtually impossible.
The criteria for estabfishingwhether an object ought o be allowed to align at allare defined in a file external to the alignmentprocess.
The criteria re defined to apply across allinstances of a given object type.
However, it isdifficult o specify the criteria in this manner sincemany instances in the keys contain little fill onwhich to base a comparison.Various object alignment schemes andminimal alignment criteria (also called the minimalmapping requirements) have been tried; for MUC-5,an alignment scheme called threshold-based wasused, and the alignment criteria were loose.
As usedfor MUC-5, this scheme allows nearly anymatching fill in a given object type to enable anobject alignment.
The only exception concernscertain slots for which an overwheiming default fillexists, e.g., <ent i ty>type .
Such slots areignored in the alignment process.If there is no content match at all between aresponse object and a key object or if the onlymatch is on a slot that is excluded from thethreshold-based alignment criteria, the responseobject is marked as spurious.
In such cases, theobject's alignment status is termed connected,meaning that the object did not align but thereexisted a key object o which it could have mappedhad it met the minimal alignment criteria.
As aconnected object under the official All-Objectsscoring method (see \[4\] and preface to extractionscore-report appendix), response fills with nocorresponding key fill are scored as spurious andthose with a corresponding key fill (whether the fillsthemselves are a correct match or not) are scored asincotrecLFor a given object type in a template, theremay be more than one possible alignment of objectinstances that meet the alignment criteria.
Suchobjects are aligned on the basis of the degree of slot-fill match, as coarsely determined by the alignmentprogram.
The program determines an approximateerror per response fill score, which will beoverridden during the actual scoring processfollowing alignment.The alignment of objects in one pass results insuboptimal mappings of some object types,especially <entity> in the JV template, becauseadvantage cannot be taken of useful informationabout the dependency between <ent i ty> (or<person>) and <entity-relationship>.
Thesolution implemented for MUC-5 was to alignobjects in two passes, with a few of the object ypeshandled in both passes.
However, despite thetheoretical dvantages of two-pass alignment, it isbelieved that the adopted solution results in onlyslightly improved object mappings over what can bedone in a single pass.
Two-pass alignment is onlya partial solution to the problem, but the problemitself appears to be relatively minor.MEASURING TASK D IFF ICULTYWith each new MUC, the evaluators havechallenged technology to deal with a broader varietyof texts and to do more with them.
One of the waysin which MUC-5 distinguishes itself from previousevaluations i in the increased task realism, whichmanifests itself in a greater variety of data extractionrequirements, in the requirement for translation ofextracted information into entries from standardreference sources (unabridged gazetteers, the StandardIndustrial Code manual, etc.
), and in a richertemplate structure.
However, the most distinctivefeature of Tipster Phase 1 for extraction is therequirement tohandle more than one language andmore than one domain.
This requirement generateda strong push in the direction of language- anddomain-independence, while the task realism151generated a strong push for maximizing taskcoverage with minimum time and effort.Major changes have been made to theevaluation design over the years, which complicatesthe issue of progress assessment.
Not only have themetrics and scoring and alignment algorithmsevolved and been replaced, but new exlraction taskshave been defined \[10\].
The first two tasks were inthe naval tactical domain, the next two were in theterrorist domain, and the Tipster/MUC-5 evaluationwas conducted in the joint ventures andmieroelectronics domains.
One could conceive oftrying to compare the difficulty of these tasks interms of human performance; however, at this pointthere exists reasonably sound performance data onlyfor MUC-5 \[11\].
One could also imagine trying tomeasure relative difficulty in some atheoretic orpolytheoretic way in terms of the number ofsemantic patterns, inference rules, etc., required tocarry out the task, but that idea is not a practicalone.In a preliminary attempt to compare thedifficulty of different extraction tasks, quantitativecriteria were developed in support of MUC-3 thatenable comparison i terms of superficial features ofthe texts, template definition, and template fill rules\[5\].
Comparison of the complexity of the terroristtask with the naval task in light of these criteriashows at least an order-of-magnitude increase forseveral of the criteria.
Once allowances are made forchanges to the scoring methods and the earlierevaluation results are recomputed, it is clear for theresults of the top systems in each evaluation thatMUC-3 system performance r presents significantprogress for extraction systems as a group over theprevious evaluation.The criteria can be adapted to allow roughestimation of the relative difficulty of the MUC-5joint ventures and microelectronics tasks comparedto the MUC-3/MUC-4 terrorism task.
Most of theadaptations reflect the shift from a flat-formattemplate to an object-oriented template.
Table 3summarizes the comparison, using EJV as theMUC-5 point of comparison.DIMENSION FACTORText corpus complexityText corpus dimensionsTemplate fill characteristicsNature of task- Ix-3x~l-2x-2xTable 3.
Difficulty of EJV task compared toterrorism taskThe summarized data indicate that the EJV taskis a somewhat more difficult task than the terrorismtask along three of the four major dimensions.
Thedimensions measure difficulty in the followingterms:Text corpus complexity measures difficulty interms of coverage of language features that may beencountered during testing.
Measurement lakes thefollowing statistics from the training corpus intoaccount:?
number of tegt types?
vocabtd~ry size?
average sentence l ngth?
average number of sentences per textText corpus dimensions measures difficulty interms of the volume of material to be processed inorder to achieve coverage and monitor systemprogress.
Measurement akes the followingstatistics from the training corpus into account:?
number of texts?
number of sentences?
total number of wordsTemplate fill characteristics measures difficultyin terms of features of the template structure and theamount of information to be extracted from a giventest set.
Measurement takes the following statisticsfrom the training corpus into account 11:?
number of object ypes 12?
number of slots?
?
overall difficulty of slot typesThis measurement also takes into account thefollowing statistics from the MUC-5 test setl3:?
percent nonrelevant texts?
average number of relevant events per relevanttext?
average number of f'dls per slot11On e other statistic that was used in comparing thenaval and terrorism tasks, the number of templatetypes, was not used in this comparison because thestatistic is not pertinent to the way the Tipstertemplates are designed.12In the MUC-4 template, there were no objects, butthere were groupings of slots into those that containeddata on the perpetrator f the terrorist act, the physicaltarget of the terrorist act, the human target of theterrorist act, and on the terrorist act itself.
These fourslot groupings were referred to as pseudo-objects.13Two other statistics that could be used if two object-oriented tasks were being compared--average numberof objects per template and average number of slots perobject--were not used in this comparison because therewere no formal object ypes in the terrorist emplate.152Nature of task measures difficulty in terms ofthe extraction task in general -- the elaborateness ofthe rules that the system must incorporate in orderto conform to the template definition and fill rules,including relevance rules at the template, object, andslot level and the formating specifications at theslot-fill level.
Measurement takes into account hefollowing statistics from the training corpus andMUC-5 test set:?
percent nonrelevant textsThis measurement also takes the following statisticsdrawn from the task documentation into account:?
number of pages of relevance rules?
number of pages of template definition andtemplate f'fll rulesThe numerical factor corresponding to eachdimension in table 3 represents a rough average ofthe factors assigned to the component criteriaidentified above.
Some of the assumptions inherentin this approach to assessing relative difficulty arethat longer sentences will be processed lessaccurately than shorter ones, that relevant texts witha greater amount of relevant information presentmore opporttmities for error, that a greater variety ofextraction requirements makes a task harder, and thatextraction is harder when it goes beyondcategorization f information i to a set f'dl.The EJV m~k is harder by a factor of two oncriteria such as the following:?
vocabulary size;?
average number of sentences per text;?
number of slots in the template;?
one of the types of slot (numeric/complexslots).Among the ways in which EJV is easier thanthe terrorism task are the following:?
sentences in the EJV corpus are shorter onaverage (18 words versus 27 words);?
there are so few nonrelevant EJV texts thatrelevance filtering plays a negligible role (~10%nonrelevant versus -50% nonrelevant);?
there is a sparser amount of information i  theEJV templates (-1 filler per slot versus -1.5 perslot).The greatest difference between the EJV andterrorism tasks concerns the text corpus dimensions.This dimension, which treats the volume of text asa measure of difficulty, could be viewed as less of anissue now than it was for MUC-3.
In fact, with theincreasing popularity of statistical techniques, largeamounts of training data are sometimes required.Nonetheless, the challenge of making effective useof text increases with the quantity of text, since alarge amout of text implies a broad domain, andmost kinds of domain knowledge cannot currentlybe captured using automated training methods.The percent nonrelevant texts criterion, whichfigures in two of the dimensions, is based on theview that the more a system's performance wouldsuffer as a consequence of ignoring the text filtering(document detection) subtask, the harder the task.The percentage of nonrelevant texts in EJV is solow (approximately 5% in the training corpus and10% in the MUC-5 test sets) that a system canalmost ignore the text filtering subtask withoutsuffering a serious degradation i performance; thesystem can be optimized in favor of generating tie-ups even when it is not sure there is sufficientinformation in the text.
This is not true of theterrorism task, where the percentage of nonrelevantand relevant exts is about equal.
In conclusion,either a task such as EJV that places extremely littleemphasis on text filtering or a task that placesextremely high emphasis on text filtering isconsidered to be less difficult than one such as theterrorism task, which places significant emphasisboth on text faltering and information extraction.Within the context of the extraction subtaskindependent of the text filtering subtask, the moreinformation there is to be extracted, the moredifficult the task is judged to be.
This is becausericher texts present more opportunities to missinformation and to confuse information about onereportable item with another.The most difficult comparison to makeconcerns the template fill characteristics, because ofthe switch to the object-oriented template.Furthermore, the overall difficulty of slot fillcriterion is itself composed of several features.
It isbased on the number and distribution of the varioustypes of slots: set-fill slots with no more thantwelve possible fills, set-fill slots with more thantwelve possible fills (for MUC-5, these were slotsthat referenced the gazetteer), numeric/complex slots(which includes some normalized fills and, in thecase of MUC-5, some two-part fills), string-fillslots (and normalized strings such as corporationnames), and pointer-fill slots (in the case of MUC-4, these are slots that require cross-references).
Themore open-ended the extraction task, the harder it isjudged to be.
The E/V task is judged to be harderwith regard to the numeric/complex slots inparticular.In summary, the generalization may be thatthe EJV task is harder than the terrorism task interms of the template (number and nature of slots),153the sheer volume of text (vocabulary size), and thediscourse demands (number of sentences per text),but a little easier in terms of the shorter sentencelength, lesser proportion of relevant information inrelevant exts (number of fills per sit0, and verysmall proportion of nonrelevant texts.OVERALL  RESULTSThe discussion of the MUC-5 evaluationresults will be presented from various perspectives,using the metrics that are most appropriate in eachcase.
This paper presents ome general views onthe results.
Results for individual Tipster sites arepresented and analyzed in the papers in this volumethat were prepared by the extraction contractors.Progress Assessment from MUC-4to  MUC-$  (EJV)Since the F-measure was in force for bothMUC-4 (as an official metric) and for MUC-5 (as anunofficial metric), a rough measure of progress canbe obtained with that metric, using EJV as therepresentative MUC-5 task.
The purpose of thecomparison is to gauge whether the field of NLP asa whole has progressed in terms of overallperformance achievable on extraction tasks.
To thatend, only the top-scoring systems are included in thecomparison, namely those that were in one of thetop two ranks statistically according to the F-measure.
14There were four systems in the top two ranksfor MUC-4 (TST3 and TST4 test sets) \[2\] and threein the top two ranks for MUC-5 (EJV test set) \[3\].These systems are GE, GE/CMU, UMass/Hughes,and SRI for MUC-4, and GE/CMU, BBN, and SRIfor FEJV MUC-5.
The average F-measure score ofthe MUC-4 systems is 51.68; the average for theMUC-5 EJV systems is 47.12.
If the one non-Tipster EJV system (SRI) is excluded from the EJVMUC-5 average, the average rises to 49.35.The greater level of difficulty of the MUC-5EJV task and the fact that the F-measure scores areclose to being as high as the MUC-4 F-measurescores indicate that performance of top MUC-5 EJVsystems i  at least comparable toperformance of topMUC4 systems.
It is important to remember thatthe Tipster systems were achieving that level ofperformance for MUC-5 on EJV while working alsoin the microelectronics domain and, in most cases,also in Japanese.
In that regard, it is notable that14The "P&R" F-measure value is used.
This valueweights precision and recall equally.the GE/CMU system scored in the top rank in eachlanguage and domain pair; on the F-measure theirscores were 52.75 for EJV, 60.07 for JJV, 49.18 forEME, and 56.31 for JME.
It is also notable thatSRI, which was a non-Tipster MUC-5 participant inboth EJV and JJV, achieved F-measure scores of42.67 and 44.21, respectively.The fact that relative task difficulty can beassessed only roughly together with the fact thatseveral MUC-5 sites worked on more than one taskmean that too much importance should not beplaced on comparison of scores between MUC-4 andMUC-5.
However, whether or not difficulty factorsand evaluation design changes are taken intoaccount, there is at least one MUC-5 task on whichperformance can only be said to be outstanding,namely the JJV core-template ask.
Two systemsachieved an F-measure score on the JJV core-template test in the 70-80 range -- 73.54(corresponding toerror per response fill of 39) forthe GE/CMU Shogun system and 77.94 (error perresponse fill of 34) for the GE/CMU optional testrun with the TEXTRACT system.
Topperformance on the EJV core-template st wasabout 20 points worse.
The relatively highperformance on the JJV core-template ask may beindicative not only of the relative simplicity of thecore-template ask compared to the full-templatetask but also to the relative simplicity of the JJVtexts compared to the EIV texts.
(Some of theselanguage differences are discussed further in a latersection.)
Nonetheless, taken on the task's ownterms, these JJV scores reflect strong performance.Comparison of Machine Performancewith Human PerformanceApplication perspective.
The F-measureis a weighted combination of recall and precision.Recall and precision give an indication of systemperformance relative to the application goals ofextracting all and only the information that shouldbe extracted.
Despite the fact that humans aresubject to human factors limitations that inhibittheir performance, the performance limits of humanson an information extraction task represent a goodtarget for automated systems as well, since theshortfall of human performance from perfection isdue not only to human factors but also to otherfactors, such as deficiencies in the task definition.As reported in \[11\], human performance andmachine performance on 120 articles in the MUC-5EME test set was measured.
As part of the study,the performance of the four well-trained analysts andthe top three MUC-5 systems (GE/CMU, BBN, andUManitoba) was compared.154The four human analysts were able to extractup to 79% of the information expected (recallmetric), and of all the information they extracted, atbest 82% of it was judged to be correct (precisionmetric).
Performance of the top systems fell farbelow human performance; the three systems usedin the comparison were able to extract up to 53% ofthe information expected, and of all the informationthey extracted, at best 57% of it was judged to becorrect.
In terms of performance shortfall, themachines fell 19-38 points of human performanceon the recall measure and 18-31 points short ofhuman performance on the precision measure.Increasing system recall and precision byanother 20 points or so may not seem to be adifficult task -- after all, since systems managed toobtain an F-measure score in the 70s on the JJVcore-template t st, why not also on the EME task?But it may not be easy to increase both recall andprecision by that amount simultaneously on arelatively difficult task such as FAME, since themetrics are in tension with each other.
The harder asystem tries to extract all the expected information(i.e., the more aggressively configured it is), themore likely it is to extract erroneous information.The tension is reduced if the texts are easier tointerpret, as the JJV texts apparently are (see sectionbelow on handling two languages) and if the task issimpler, as the JJV core-template task undoubtedlyis in comparison to the EME (full-template) task.The overall recall and precision scores hide thefact that there were not only slots on which humanperformance was relatively strong but also slots onwhich human performance was relatively weak.
Astudy reported in \[11\] measured the degree ofdifference between human and machine performancefor frequently-filled slots in a portion of the EMEtest set.
The author's general conclusion was thatmachines did comparatively well on slots that maylend themselves tokeyword analysis and that are tobe filled with a set-fill category from a relativelylong list; examples include the <layering> typeand film slots.Speed.
Another respect in which systemsshowed an advantage over humans is in terms ofspeed.
On average, the time required for a human tofill a template (using software tools tailored for theTipster tasks) was between 15 minutes (for an EMEtemplate) to over 60 minutes (for a JJV template).In contrast, timing information collected for theBBN PLUM system, the GE/CMU Shogun system,and the NMSU/Brandeis Diderot system shows thatthe average time required to process an article in theEME test set was between 75.0 seconds (Shogun ona Sparcl0 with 64 mb RAM) and 211.2 secon6s(Diderot, which was not optimized for speed inEnglish, on a Spare2 with 32 mb RAM) and thatthe average time required to process an article in theJJV test set was between 39.0 seconds (Diderot on aSparc2 with 2.32 mb RAM) and 140.8 seconds(PLUM on a Sparcl0 with 128 mb RAM).Predominant  C lasses o f  E r ro rThe most frequent type of error committed bynearly all of the MUC-5 systems was to misspertinent information.
This class of error iscaptured by the undergenemtion metric.
The testresults show that performance on this metric is agood indicator of performance on the overall metricof error per response fill.
The effect ofundergenemtion n relation to the overgenerafion a dsubstitution metrics as well as to the error perresponse fill metric can be seen in figures 1 and 2,which graph the results of all MUC-5 systems forI10l07OI0so4o302OlO0BBN GE/CMU LSl ~ ~ UVASS/t.ltJFigure 1.
Classes of error and overall error perresponse fill for all EME MUC-5 systems10090807060SO403020100BBN GE/CMU GE/CMU NB~ NM/BR(OPT)Figure 2.
Classes of error and overall error perresponse fill for all JME MUC-5 systems155the EME and JME tests.
From these graphs it isclear that undergeneration (UND) generally correlateswith the overall metric of error per response fill(ERR), overgeneration (OVG) does not correlatewith it, and substitution (SUB) correlates with itonly to a limited extent.Substitution is a lesser source of error thanundergeneration and overgeneration, lesser even thanovergenerafion.
Examination of the template-fillspecifications sheds light on these data.
Some slotsand objects in the JV and ME templates haveessentially fixed number, requiting one fill orallowing zero or one fill; others have a highlyvariable number, some requiring one or more fillsand some allowing zero or more fills.
Thus, for theslots having a highly variable number of fills, thereis no absolute bound on the number of fills asystem could potentially spuriously generate.
Thismeans that overgeneration those slots could bequite high.
Substitution errors, on the other hand,are accrued only when there exists a pairing betweena fill in the key and a fill in the response, and theresponse is judged to be incorrect.
Thus, thesubstitution score has as its upper bound thenumber of fills in the key.If the extraction of relatively little targetinformation is indicative of poor overallperformance, how and to what extent does theextraction of relatively much information -- good orbad -- correlate with overall performance?
Are theaggressive systems just wildly guessing, or is theiraggressiveness paying off form them on the overallmetric?
The data show that there is a correlationbetween generating lots of data and obtaining arelatively good (i.e., low) error per response fillscore.
This can be seen by computing the numberof fight and wrong fills generated by a system (thisnumber is called the actual (ACT)) as a percentageof the total number of fills expected (termed thepossible (POS)) and comparing that percentage withthe overall error per response f'dl score.In figure 3, the EME results are sorted byincreasing error per response fill on the vertical axis.It is evident hat the more fills generated by thesystem, the better its error per response fill score,even to the extent hat the number of fills generatedby the GE/CMU system exceeds the numberexpected, i.e., the system clearly generated a highproportion of spurious fills (as figure 1 bears ou0.The only clear exception to the generalization is theUMichigan system, which had a relatively higherror per response fill score despite having generatedrelatively many fillers (more than the LanguageSystems, Inc. (LSI) system or NMSU/Brandeissystem).
Figure 1 shows that the UMichigansystem suffered from relatively high overgenerationas well as relatively high undergeneration.UM~S~Ht~Lla~H'NM~R 'LSI'BBN'GEICMU'.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.IImEmI u ?~r~?s)'~ I........ II ............................................................................................................................................| | !
| |I~'EACI~IYAGEFigure 3.
MUC-5 EME system aggressiveness incomparison with performance on the error perresponse fill metricThe results of the comparative performancestudy of machines (the GE/CMU, BBN, andUManitoba systems) and humans on part of theMUC-5 EME task show how far short of humanperformance the machines' performance f ll.
Well-trained humans are being compared with the best-performing MUC-5 systems.
Since the MUCevaluations are designed to challenge researchtechnology as well as to show a practical use oftechnology, it would probably be unreasonable toexpect that any information extraction systemparticipating in a MUC evaluation would perform ata level close to humans, and it is unlikely that anyof the MUC-5 participants had comparability withhumans as their primary development goal.Nonetheless, there may be evaluation data to helpsupport speculation about how likely it would bethat these systems could be developed to make upthe shortfall.Figure 1 shows that all EME systems otherthan GE/CMU incurred more errors as a result ofmissing information than as a result of committingother types of error, and figure 3 shows thatgenerating more data was generally beneficial interms of improving overall performance.
The factthat the BBN and UManitoba systems' overallperformance is very close to GE/CMU's -- in fact,the differences among the three are statisticallyinsignificant \[3\] -- provides evidence that relativelygood performance does not necessarily come at theexpense of high overgeneration 15 and therefore that15This is not to fault the GE/CMU system forovergenerafing.
There are other systems with an equalor worse overgeneration score that come nowhere nearmatching the GE/CMU system in error per responsefill.
The GE/CMU system had undergeneration a d!120156greater task coverage could make up for some of theshortfall from human performance.
Further evidenceof the room left for improvement of most, if notall, MUC-5 systems is found in the fact that thereare slots which systems never filled in during thefinal test.
16 Even in the case of Tipster systems,these unattempted slots can account for a sizeableproportion of the total number of missed pieces ofinformation, t7Measuring the Performance ofSystems at Different Levels ofMaturityScoring of unfilled slots.
An object thatis instantiated in the answer key may not be fullyfilled; the corresponding text may not provideinformation to fill some of the slots defined for thatobject type.
Cases where a template slot iscorrectly left unfilled by the system under evaluationare scored as noncommittal by the scoring software.Noncommittals are not included in the standardformulation of any of the performance measures.This is reasonable from a research perspective, if notfrom an applications perspective.
The questioncomes down to whether systems normally leave aslot unfilled out of knowledge or whether they do soout of a lack of knowledge.
Highly immaturesystems tend either to overgenerate o an extreme,leaving few slots unfilled, or to undergenerate to anextreme, leaving many slots unfilled.
The lattertype of immature system was very common at theMUC-5 evaluation and could have benefited unfairlyfrom a metric that considers a noncommittal fall tobe a correct fill, especially since there are manyunfilled slots in the key templates.overgeneration i close balance for the MUC-5evaluation and evidently was optimized on both.
Ofthe four language-pair systems they were required tofield for MUC-5, three came out slightly better onbalance on recall (which emphasizes minimizingundergeneration) and one, JJV, came out slightly betteron precision (which emphasizes minimizingovergeneration).16Count of unattempted slots (i.e., those where thesystem's "actual" equals zero) excludes those slots thatwere never filled in the key (i.e., those where the"possible" equals zero).17For example, BBN's JJV system made no attempt tofill 17 of the slots in the JJV template, which accountsfor 25% of the total missing, and their JME systemmade no attempt to fill 12 of the JME slots, accountingfor 24% of the total missing; the UMass/Hughessystem made no attempt to fill 13 of the EYV slots and11 of the EME slots, and in each case this accounts for15% of the total missing.The effect of scoring noncommittal fills ascorrect fills is to give an inflated estimate ofperformance, at least for the systems thatundergenerate to a relatively large extent.
It also hasthe potential effect of giving a distorted cross-system view, since very imlmature systems couldend up being ranked higher than is intuitivelysensible.
18The latter effect was not evident, however, forMUC-5, despite the relatively large number ofunfilled slots in the answer keys (FJV compared toMUC-4).
Apparently, the potential effect on theMUC-5 evaluation was eliminated through theobject structure.
Since the MUC-5 templatesconsist of objects that arc aligned separately, thescoring impact of producing an object that fails tomeet the minimal alignment criteria is limited tojust that one object.
Such an object, whichcontains an insufficient amount of correct fill towarrant alignment, is not given credit for any"correct" fills.
19 Thus, even though the objectalignment criteria were loose for MUC-5, there werestill objects that failed to align, and systems got nocredit for any correct inftrmation that they may havecontained.For MUC-4, on the other hand, there was noobject alignment, only template alignment, and thetemplate alignment criteria were fairly strict.
Thus,although no credit would be gained for correct fillsin an unaligned template, the amount of credit thatwould be obtained for noncommittal fills in analigned template would be fairly high on average,since the MUC-4 template is a larger structure thanany of the objects in the MUC-5 template.Figures 4 and 5 provide examples of thedifference the treatment of the noncommittal scoringcategory can make in the MUC-5 results.
They18When applied to MUC-4 systems, the standardformulation of error per response fill results in nosignificant reranking of the 17 systems.
But aformulation that includes noncommittals would resultin rerankings of all 17 systems.
The most radicalchanges would be for immature systems whose numberof noncommittals grealy outweighs all other categoriesof response.19Major differences between MUC-5 and MUC-4 in thealignment process do not play a role in thisinvestigation of the scoring of noncommittal fills,since the investigation with respect o both MUC-5and MUC-4 treated such fills as correct only in thescoring stage, not in the alignment stage.
As far asscoring method goes, the two evaluations are not verydifferent; both used the All-Objects method, which forMUC-4 was called All-Templates.157show the error per response fill scores for theTipster systems on EME and JME MUC-5 usingtwo formulations of the metric: the standardformulation, which disregards noncommittal fills,and the alternative formulation, which treatsnoncommittal fills as correct.
The alternativeformulation and the standard formulation provideconsistent cross-system views of performance; asdiscussed above, the alternative formulation does notdistort he cross-system perspective on the results.3OBBN CG,F./CM U NMSUIBR UMAS~/H UFigure 4.
Tipster system MUC-5 EME scoresfor two formulations of error per response fill80706050403OBml  GF.X:UJ G E.'CMU (OPT) ~SLUB~:IFigure $.
Tipster system MUC-5 J/viE scores fortwo formulations of error per response fillViewed in terms of the impact on the actualscores, the difference between the two formulationsranges from 14 to 18 points.
2?
As mentionedearlier, the alternative formulation inflates the scoresof systems that greatly undergenerate.
It is quitelikely that such systems leave slots unfilledignorantly more often than they do so knowingly.Nonetheless, actual performance of the systems maybe estimated to lie somewhere between the twovalues, closer to the standard value for lesserdeveloped systems and closer to the alternative valuefor more highly developed systems.20For FJV and JJV, the difference is somewhat less,ranging from 9-13 points.Richness-Normalized Er ror .
Thealternative error per response fill formulationdescribed above may provide better insight than thestandard formulation into the potential performancelevel of systems that miss relatively little of thepertinent information in the texts.
Similarly, therichness-normalized rror, either in its standardformulation or in an alternative formulation, mayprovide better insight han the error per response fillinto the potential performance l vel of systems thatgenerate relatively little spurious information, i.e.,that have a relatively low overgeneration score.This metric views documents as streams of data ofvarying richness according to the number of f'flls inthe key.
21The richness-normalized error metric is closeto being a system-independent metric, meaning thatthe denominator disregards spurious responsesbecause the number of such responses varies greatlyfrom one system to the next.
22 Sinceovergeneration was a significant problem forvirtually all MUC-5 systems, this measure tends todistort cross-system comparisons by treatingsystems relatively harshly that overgenerate o arelatively large extent.
For this reason, thismeasure does not appear to offer a useful way ofviewing the MUC-5 test results; however, it may beuseful when the performance of systems underevaluation is uniformly higher.Handling Two LanguagesFour of the five sites that were evaluated inboth Japanese and English (see table 1) performed atleast as well in Japanese as in English.
Averagedacross all the MUC-5 systems, JME error perresponse fill is better than EME by eight points,and JJV is better than EJV by eleven points.Averaged across all the sites and the two domains,21Thus, data extraction is viewed as analogous tospeech recognition.
Just as in speech, where there aredetectable and classifiable signals coming in, in dataextraction there is extractable information coming in.The slot-fill count for a document is analagous to theword count for a stream of speech, and the slot fills inthe key templates are analagous to the known words inthe spoken sentences.22However, it is not entirely system-independenL Asmall amount of system dependence r mains because ofvariability in the key templates, which capture sometextual ambiguity by representing alternative correctanswers, which may include an alternative number ofslot fills or objects in a particular instance \[4\].
Thissituation may arise in speech as well, where bearersdisagree on which words and how many words wereuttered.158there is a ten-point difference between Japanese andEnglish.
23 These findings are presented andanalyzed in \[71.System performance differences between thetwo languages in the JV domain are attributedlargely to differences between the EJV and JJV textcorpora in terms of overall text structure and style.Analysis of the Japanese text characteristics andtheir impact on extraction performance is presentedin \[6\].
The JJV corpus is more nearlyhomogeneous and the texts and sentences morepattern-like, which reduces the discourse demandsand generally facilitates extraction.At one level of analysis, the ME difference inscores may be attributed to the fact that there wasone-third less data to extract in JME than EME(average of 17 flUs per template in JME, 25 fills pertemplate in EME), including only about onemicroelectronics-capability per template in JME asopposed to about wo per template in EME.
Thus,the problem of object splitting and merging(discourse-related mplate ffects) is lesser in JME.Analyzed in more detail, the differing richnessof the JME and EME corpora is seen to be related inpart to a significant difference in the amount of dataabout he packaging process.
It appears much morecommonly in the EME corpus than in the JMEcorpus; in the MUC-5 test set, about one-third ofthe EME templates contain one or more<packaging> objects, versus about one-tenth ofthe JME.
However, it does not seem to be the casethat this type of object caused particular problemsfor most systems; the error per response fill scorefor three of the four Tipster EME systems for thetest set overall is virtually the same as for thesubset of templates that they generated eontaining<packaging> objects.
The performance impact ofthe differing quantities and types of information ithe EME and JME corpora is being furtherinvestigated.Handling Two DomainsThe four Tipster sites (five systems, includingthe GE/CMU optional JJV and JME optional test23These statistics are based on the results for all MUC-5 sites.
Consequently, Engfish JV and ME averages arelow because of the number of relativelyunderdeveloped, non-Tipster systems that wereevaluated in English only.
If the statistics are limitedto those sites that worked in both languages (five .IV,three ME), there is still a five-point difference betweenJapanese and English (six-point difference for JV andfour-point difference for ME).runs using the CMU TEXTRACT system) wereevaluated in both extraction domains.
Althoughthey had more time to work on JV than ME, theirsystem performance was comparable acrossdomains.
Overall error per response fill scores forthe UMass/Hughes system are the same for EJV andEME; the BBN system performed a little better onME than JV (two points difference in bothlanguages); the GE/CMU system scored worse onME than JV (four points difference in bo/hlanguages); and results for the NMSU/Brandeissystem are mixed -- better on ME than JV forEnglish (five points difference) and a little worse forJapanese (two points difference).
The biggestdifference was shown on the GE/CMU optional testrun (nine points worse on JME than JJV).It would appear that the comparable resultsachieved by most of the systems are attributableprimarily to factors that kept JV performance down.Parts of the JV template underwent many changes,which may have caused sites to do less developmenton those parts.
Some sites may have also skippedparts that represented a very small proportion of theoverall task in terms of number of fills in thetraining corpus keys, especially skipping deeplyembedded slots and/or objects.
In addition, the factthat testing was conducted on a core portion of thetemplate as well as on the full template may havecaused sites to focus less development effort on thenon-core poaions of the template.The net effect of these factors is that the sitesessentially reduced the task to a manageable size andas a consequence, incurred errors by missingrelatively more information in JV than ME.Although this generalization holds for most of theMUC-5 systems, among the Tipster systems it doesnot apply to the GE/CMU Shogun English system,the GE/CMU TEXTRACT (optional) Japanesesystem, or the NMSU/Brandeis Japanese system.Statistics on the average degree of task reduction bythe MUC-5 sites in each language-domain pair canbe found in \[7\].RESULTS FOR L IMITED JV  TASKOverall PerformanceMUC-5 English and Japanese joint venturestesting was conducted in two configurations.
In oneconfiguration, the entire template was scored; in theother, only the core portion of the template wasscored (see footnotes to table 2).
Figures 6-9 grapherror per response fill together with the diagnosticsecondary metr ics of  undergenerat ion,overgeneration, and substitution for the Tipster159systems for each of the two configurations.
Acrossthe EJV systems, the error per response fill scoreson the core-template test range between seven andnine points beuer (lower) than on the full-templatetest; for the JJV systems, the error per response fillscores on the core-template test range betweenfifteen and sixteen points lower than on the full-template test.The source of most of the difference in errorper response fill is in the number of missed fills,which is reflected in better undergeneration scores onthe core-template test; the range across Tipstersystems is 6-15 points lower for EJV and 11-24points lower for JJV.
The only other sizeabledifferences (i.e., differences of more than five points)are the overgeneration score for the GEICMU EJVand JJV systems (nine points lower on the core-template test for EJV and seven points lower forJJV) and both the overgeneration and substitutionscores for the GEICMU optional JJV run using theCMU TEXTRACT system (overgeneration ninepoints lower on the core-template test andsubstitution seven points lower).
Thus, for allsystems except GEICMU's, the only score amongthe secondary metrics that differs considerablyBBN GECMU NWBRFigure 6.
Tipster system scores for the EJVfull-template testFigure 7.
Tipster system scores for the FJVcore-template testBW GCaV GVULJ(OPT1 NMBRFigure 8.
Tipster system scores for the JJVfull-template testFigure 9.
Tipster system scores for the JJVcoretemplate testbetween the two test configurations is theundergeneration score.The difference in scores on the twoconfigurations is more marked for Japanese than forEnglish, with the best error per response fill scoresposted for the whole evaluation by the GEICMUShogun system and the GEICMU optional test runwith the TEXTRACT system on the JJV core-template test (scores of 39 and 34, respectively).On the EJV and JJV full-template tests, most of theerror per response fill scores are in the 50-70 range.As a point of reference, the error per response fillscore of 61 posted by the GEICMU system on theEJV full-template test corresponds to a recall of 57and precision of 49 (F-measure of 52.75).Slot-Level PerformanceThe JV core template includes fourteen slots,one-third as many slots as the full template; yet forthe EJV MUC-5 full-template test, the slot fillsfrom the core slots account for nearly two-thirds(around 63%) of the total slot fills.
Thisdistribution reflects the fact that the core-templateslots cover some of the less idiosyncratic portionsof the task.
Since the MUCJ test set is fairlyrepresentative of data seen in the training corpus, itis not surprising that participants would havededicated more development effort to the core slotsin the template and would have been able to leverageprevious work that is applicable across a range oftasks.Therefore, it is not surprising that scores onthe core slots are relatively goad compared to otherslots in the template.
At least one of the fourTipster EJV systems had an error per response fillscore of less than or equal to 50 on six of the 43scored slots24; five out of the six slots are in thecore part of the template.
At least one of thesystems scored between 51 and 75 on twenty otherslots; nine of the twenty are in the core part of thetemplate.
Scores over 75 were obtained for manynon-core slots but not for any core slots.
Statisticsfor the Tipster EIV system that scored best on eachslot and for the average across Tipster EJV systemsare summarized in table 4.ERR I #Slots: Best #Slots: Average-Range I Slot Score Slot Score0-25 I 0 0Table 4.
Tipster ETV performance on slots (bestand average score) by range in error per response fill.Numbers in parentheses are for core-template slots.The fact that performance on the core slots isrelatively good is evident if the template slots aredivided into categories roughly according to theirtype: pointer, set fill, string fill, numeric fill,geographic place-name fill, temporal fill, two-part(complex) fill.
The core template contains slots ofthe following types: pointer, set fill, string fill, andgeographic place-name fill.
For each of the TipsterEJV systems it is generally the case thatperformance on the core slots of a given type isbetter than performance of any other slots of thattype.
Thus, for example, performance by each ofthe Tipster EJV systems on the four set-fill slots inthe core set ( < e n t i t y > t y p e ,  c t i e - u p -relationship>status, <entity-relationship>status, <entity-relationship>rel-ent2-to-entl is better than performance on any of the four2 4 ~ h e  <rate>eta slot is excluded from the total slotcount, since there were no fills for it in the key for theE N  MUC-5 test.set-fill slots that are not in the core set(cindustry>type, <facility>type, <person>position, <revenue>type).There are three minor exceptions, which affectonly the NMSU/Brandeis and UMassIHughessystems.
Two of the exceptions show performanceon a non-core set-fill slot slightly better (twopoints) than centity-relationship>rel-ent2-to-ent l .
The third exception is that theUMasdHughes system performed four points worseon <entity>aliases, a core string-fill slot, thanon <person>name, which is a non-core string-fillslot.However, in addition to these minorexceptions, there are two core slots that representmajor exceptions that affect all four of the systems:<tie-up-relationship>joint-venture and<entity-relationship>entity2.
These are bothpointer slots to an <entity> object.
For eachsystem, there is at least one non-core pointer slot(and as many as five) that the system scored betteron than on than these two core slots.
Furthermore,there is a gap between the scores for these two corepointer slots and the scores for the other core pointerslots of at least nine points (and as many asseventeen).The joint-venture and entity2 slots havesimilarities that indicate why performance on themis not as good as on the other core pointer slots:they both require making two-way role distinctionsamong entities found in the texts, and they bothcapture the less frequent of the two entity roles.
Inthe case of the <tie-up-relationship> object,both the joint-venture and the entity slots pointto an <entity> object, but the joint-venture ismeant to be filled only when a tie-up results in theformation of a joint venture company, which isoften not the case.
In the case of the <entity-relationship> object, both the entityl andentity2 slots point to an <entity> object, butthe entity2 slot is meant to be filled only if arelationship exists other than partnership, which isthe most common type of relationship.
The lowerscores on joint-venture and entity2 are thereforeattributed in part to the relative difficulty ofidentifying specific roles of entities.The restricted use of the joint-venture andentity2 slots is reflected in the template definition:joint-venture and entity2 are constrained tocontain either zero or one filler while the entityand entityl slots must contain at least one fillerand may contain two or more.
The system mustdecide not only what to fill the joint-venture andentity2 slots with but also whether to fill them atall.
Thus, the system is likely to fill them only ifit has found clear evidence in order to avoidgenerating spurious data, and this can result in theopposing type of performance problem, namelymissing relevant information.Apart from the jo in t -venture  and ent i ty2slots, the only core slots that appear to have sufferedrelatively poor performance for all four systemscompared to other core slots are the <ent i ty>locat ion  and <ent i ty>nat iona l i ty  slots, two ofthe three geographic place-name slots in thetemplate.
Although the systems cored better onthese two place-name slots than on the non-coreone, <fac i l i ty>locat ion ,  the fact that allsystems appeared to have relative difficulty withthose two core slots is notable, as it may reflect apractical difficulty of selecting the correct entry foran ambiguous place name from the large Englishgazetteer as well as the linguistic difficulty ofdetermining whether a mention of a place inassociation with an entity reflects the entity'slocation or its nationality.However, there is a problem with attributingrelatively low performance of the four core slotsunder discussion solely to the difficulty ofdetermining the correct role of an entity or of ageographic place-name.
The problem is that thelower performance may also be partially explainedby the fact that those slots are less frequently filledthan any of the other core slots in the full-templatetest.
25 All other core slots account for at least 3%each of the fills in the full-template test, with sixcore slots in the 3--4% range and five slots in the 5-10% range.
Thus, even among the core slots, it canbe expected that development efforts were notfocused equally on all slots and that lowerperformance on some core slots may be aconsequence not only of their relative difficulty butalso of their lesser impact on the tolal evaluation.SUMMARYThe evaluations conducted uring Phase 1 ofthe Tipster extraction program have measured thecompleteness and accuracy of systems and have usedan examination of the role of missing, spurious and25However, these four core slots are more frequentlyfilled than many of the non-core slots.
Of the 30 non-core slots, 24 account for less than 3% each of the totalfills (13 account for less than 1% each, and 11 accountfor 1-2% each); only six of the non-core slots accountfor a sizeable proportion of the total fills (four accountfor 3-4% each, and only two account for 5-10% each).otherwise erroneous output as a means ofdiagnosing the state of the art.
Viewed as a set ofperformance benchmarks for the state of the art ininformation extraction, the MUC-5 evaluationyielded EJV results that are at least as good as theMUC-4 level of performance.
This comparisontakes into account some of the measurabledifferences in difficulty between the EJV task andthe MUC-3 and MUC-4 terrorism task.However, even a superficial comparison oftask difficulty is hard to make because of the changefrom the fiat-format design of the earlier MUCtemplates to the object-oriented design of the MUC-5 templates.
Comparison is also made difficult bythe many changes that have been made to thealignment and scoring processes and to theperformance metrics.
Therefore, it is more useful toview performance of the MUC-5 systems on theirown terms rather than in comparison to previousMUC evaluations.From this independent vantage point, MUC-5yielded very impressive results for some systems onsome tasks.
Error per response fill scores as low as34 (GE/CMU optional test run using the CMUTEXTRACT system) and 39 (GE/CMU Shogunsystem) were obtained on the JJV core-template test.The only other error per response fill scores in the30-40 range were achieved by humans, who weretested on the EME task; however, machineperformance on that EME test was only half as goodas human performance.
Thus, while the JJV core-template test results how that machine performanceon a constrained test can be quite high, the EMEresults show that a similar level of machineperformance on a more extensive task could not beachieved, at least not in the relatively shortdevelopment period allowed for ME.Not only do results uch as those cited for theJJV core-template test show how well someapproaches toinformation extraction work for sometasks, they also show how manageable anguagesother than English can be.
A cross-languagecomparison of results showed fairly consistentadvantage in favor of Japanese over English.Comparison of results across domains does notshow an advantage in favor of one domain over theother, and it is quite likely that differences in thenature of the texts, the nature and evolution of theextraction tasks, and the amount of time allowed fordevelopment allhad an impact on the results.The quantity and variety of material on whichsystems were trained and tested presented challengesfar beyond those posed by earlier MUC evaluations.162The scope of the evaluations was broad enough tocause most MUC-5 sites to skip parts of theextraction task, especially types of information thatappear relatively rarely in the corpus.
Since no typeof information is weighted in the scoring moreheavily than any other, the biases that exist in theevaluation reflect the distribution of relevantinformation in the text corpus and result in a naturalemphasis on handling the most frequently-occurringslot-tilting tasks.
These tasks turn out to be theones that are less idiosyncratic and therefore moreimportant to the development of generally usefultechnology.Examination of the slot-level results in theappendix to this volume shows which systems arefilling which slots and how aggressively they aregenerating fills.
For those slots where a system isgenerating a substantial number of fills, analysis atthe level of the individual templates andcorresponding texts would provide insight into theparticular circumstances under which the systemextracted cor r~ or incorrect information.
In otherwords, the quantitative performance measures mayyield information on aspects of performance thatdeserve further analysis, but a deeper investigationneeds to include examination of the actual fills andthe actual texts.
The discussion in this paper ofslot-level performance on the JV core-template askdoes not go as far as that; the discussion is basedonly on frequency of slot fill and on the slotdefinitions.
Some of the deeper analysis can becarried out only by the authors of the systems.Such an analysis would relate the circumstancesunder which correct or incorrect system behaviorwas seen with the strengths and weaknessses ofparticular algorithms and modules of the system.ACKNOWLEDGEMENTSThe author would like to acknowledge theextensive support hroughout Tipster Phase 1 fromNancy Chinchor and Gary Dungca at SAIC.
Shewould also like to acknowledge the activeparticipation of the ARPA/SISTO sponsors,Thomas Crystal and George Doddington, and thelong-suffering assistance of the MUC-5 programcommittee, which included Nancy Chinchor ofSAIC, representatives of the Tipster contractors --Sean Boisen of BBN, Jim Cowie of NMSU, JoeMcCarthy of UMass, and Lisa Ran of GE --,representatives of other MUC-5 sites -- RalphGrishman of NYU, Jerry Hobbs of SRI, and CarlWeir of Unisys --, and representatives of the DoDTipster management -- Lynn Carlson, Mary EllenOkurowski, and Boyan Onyshkevych.REFERENCES\[1\] Carlson, L., et al, Corpora and DataPreparation, in this volume.\[2\] Chinchor, N., The Statistical Significance of theMUC-4 Results, in Proceedings of the FourthMessage Understanding Conference (MUC-4), June1992, San Mateo: Morgan Kanfmann, pp.
30-50.\[3\] Chinchor, N., The Statistical Significance of theMUC-5 Results, in Proceedings of the FifthMessage Understanding Conference (MUC-5),August 1993, San Mateo: Morgan Kanfmann (toappear).\[4\] Chinchor, N., and Sundheim, B., MUC-5Evaluation Metrics, in Proceedings of the FifthMessage Understanding Conference (MUC-5),August 1993, San Mateo: Morgan Kaufmann (toappear).\[5\] Hirschman, L., Comparing MUCK-II and MUC-3: Assessing the Difficulty of Different Tasks, inProceedings ofthe Third Message UnderstandingConference (MUC-3), May 1991, San Mateo:Morgan Kauffmann, pp.
25-30.\[6\], Malorano, S., An Analysis of the Joint VentureJapanese Text Prototype and Its Effect On SystemPerformance, in this volume.\[7\] Okurowski, M.E., Domain and LanguageEvaluation Results, in Proceedings of the FifthMessage Understanding Conference (MUC-5),August 1993, San Mateo: Morgan Kanfmann (toappear).\[8\] Proceedings of the Fourth MessageUnderstanding Conference (MUC-4), June 1992,San Matco: Morgan Kaufmann.\[9\] Science Appfications International Corporation,Tipster/MUC-5 Scoring System User's Manual,version 4.3, August 1993.\[10\] Sundheim, B., and Chinchor, N., Survey of theMessage Understanding Conferences, inProceedingsof the ARPA Human Language TechnologyWorkshop, March 1993, San Mateo: MorganKaufmann (to appear).\[11\] Will, C., Comparing Human and MachinePerformance for Natural Language Inform~itionExtraction: Results from the Tipster TextEvaluation, in this volume.163
