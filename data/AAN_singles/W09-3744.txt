Proceedings of the 8th International Conference on Computational Semantics, pages 359?370,Tilburg, January 2009.c?2009 International Conference on Computational SemanticsSemantic Normalisation : a Framework and an ExperimentPaul Bedaride Claire GardentINRIA/LORIA CNRS/LORIAUniversit?e Henri Poincar?e, Nancy Nancypaul.bedaride@loria.fr claire.gardent@loria.frAbstractWe present a normalisation framework for linguistic representations and illustrate its useby normalising the Stanford Dependency graphs (SDs) produced by the Stanford parser intoLabelled Stanford Dependency graphs (LSDs).
The normalised representations are evaluatedboth on a testsuite of constructed examples and on free text.
The resulting representationsimprove on standard Predicate/Argument structures produced by SRL by combining role la-belling with the semantically oriented features of SDs.
Furthermore, the proposed normalisa-tion framework opens the way to stronger normalisation processes which should be useful inreducing the burden on inference.1 IntroductionIn automated text understanding, there is a tradeoff between the degree of abstraction providedby the semantic representations used and the complexity of the logical or probabilistic reasoninginvolved.
Thus, a system that normalises syntactic passives as actives avoids having to reasonabout equivalences between grammatical dependencies.
Similarly, normalising phrasal synonymsinto their one word equivalent (e.g., take a turn for the worse/worsen) or converting the semanticrepresentation of deverbal nominals into their equivalent verbal representations (Caesar?s destruc-tion of the city/Caesar destroyed the city) avoids having to reason with the corresponding lexicalaxioms.
In short, the better, semantic representations abstract away from semantically irrelevantdistinctions, the less reasoning needs to be performed.In this paper, we investigate a normalisation approach and present a framework for normalisinglinguistic representations which we apply to converting the dependency structures output by theStanford parser (henceforth, Stanford Dependencies or SDs) into labelled SD graphs (LSD) thatis, dependency graphs where grammatical relations have been converted to roles.The LSD graphs we produce and the normalisation framework we present, provide an inter-esting alternative both for the shallow Predicate/Argument structures produced by semantic rolelabelling (SRL) systems and for the complex logical formulae produced by deep parsers.Thus as we shall see in Section 2, labelled SDs are richer than the standard Predicate/Argumentstructures produced by SRL in that (i) they indicate dependencies between all parts of a sentence,1359not just the verb and its arguments1and (ii) they inherit the semantically oriented features ofSDs namely, a detailed set of dependencies, a precise account of noun phrases and a semanticallyoriented treatment of role marking prepositions, of heads and of conjunctions.Furthermore, the normalisation framework (formal system and methodology) we present, canbe extended to model and implement more advanced normalisation steps (e.g., deverbal/verbal andphrasal/lexical synonym normalisation) thereby potentially supporting a stronger normalisationprocess than the semantic role labelling already supported by SRL systems and by deep parsers.In sum, although the normalised SDs presented in this paper, do not exhibit a stronger normal-isation than that available in the Predicate/Argument structures already produced by deep parsersand by SRL systems, we believe that they are interesting in their own right in that they combinesemantic role labelling with the semantic features of SDs.
Moreover, the proposed normalisationframework opens the way for a stronger normalisation process.The paper is structured as follows.
Section 2 presents the representations handled by the systemnamely, the SD graphs and their labelled versions, the LSDs.
Section 3 presents the rewritingsystem used and explains how SDs are converted to LSDs.
Section 4 reports on evaluation.
Section5 discusses related work and concludes with pointers for further research.2 (Normalised) Stanford Dependency graphsStanford Dependency graphs.
SD graphs are syntactic dependency graphs where nodes arewords and edges are labelled with syntactic relations.
As detailed in [dMM06, dM08], SD graphsdiffer from other dependency graphs in several ways.
First, they involve an extensive set of 56dependency relations.
These relations are organised in a hierarchy thereby permitting underspec-ifying the relation between a head and its dependent (by using a very generic relation such asdependent).
Second, in contrast to other relational schemes such as the GR [CMB99] and PARC[KCR+03], NP-internal dependency relations are relatively fine-grained2thereby permitting a de-tailed description of NPs internal structure and providing better support for an accurate definitionof their semantics.
Third, heads are constrained to be content words i.e., noun, verbs, adjectives,adverbs but also conjunctions.
In particular, contrary to the GR scheme, SD graphs take copulabe to be a dependent rather than a head.
Fourth, SD graphs are further simplified in that somenodes may be collapsed.
for instance, role marking prepositions are omitted and a trace kept ofthat preposition in the dependency name (e.g., prep-on).The practical adequacy of SD graphs and their ability to support shallow semantic reasoningis attested by a relatively high number of experiments.
Thus, in 2007, 5 out of the 21 systemssubmitted to the RTE (Recognising Textual Entailment) challenge used the SD representations.SDs have been used in bioinformatics for extracting relations between genes and proteins [EOR07,CS07].
It has furthermore been used for opinion extraction [ZJZ06], sentence-level sentimentanalysis [MP07] and information extraction [BCS+07].1In the CoNLL 2008 shared task on joint parsing of syntactic and semantic dependencies [SJM+08], the aim is toproduce dependency structures labelled with predicate/argument relations.
Although such structures are similar to theLSD graphs we produce, there are differences both in the precise type of structures built and in how these are built.
Wediscuss this point in more detail in section 5.2e.g., appos for apposition, nn for noun-noun compounds, num for a numeric modifier and number for an element ina compound number.360loveJohnMarynsubj dobjloveMarybe JohnnsubjpassagentauxpassloveJohnMaryarg0 arg1Figure 1: SDs and LSDs for ?John loves Mary?
and ?Mary is loved by John?Normalised Stanford Dependency graphs.
From the SDs produced by the Stanford parser, weproduce labelled SDs where the syntactic relations between a verb and its arguments are replacedby the roles.
For instance, the SDs and LSDs for the sentences ?john loves mary?
and ?mary isloved by john?
are as given in Figure 1.
The roles used in LSDs are those used in the PropBank forcore- and adjunct-like arguments namely, A0, A1, A2, A3, A4, AM where AM covers all PropBankadjunct tags such as AM-TMP, AL-LOC, etc..As mentioned in the introduction, LSD graphs combine the advantages of SD graphs with se-mantic role labelling.
From semantic role labelling, they take the more semantic predicate/argumentrelations.
From SD graphs, they inherit the semantic oriented features such as the deletion of con-tent poor function words, the rich hierarchy of NP internal relations and the detailed description ofthe relations holding between words other than the verb and its arguments.In short, LSD graphs are both more semantic than SD graphs and richer than SRL Predi-cate/Argument structures.3 Normalising dependency treesTo normalise the SD graphs, we extend the Stanford parser with a normalisation module de-signed to translate the grammatical relations between a verb and its arguments into roles.
Thisnormalisation module consists of an ordered set of rewrite rules and is defined semi-automaticallyin a two-step procedure as follows.First, the rewrite rules for transitive verbs are defined.
This first step is done manually and isbased on the XTAG [Gro01] inventory of possible syntactic contexts for verbs of this type.Second, further rewrite rules for verbs of other classes (ditransitive, verbs with sentential ar-gument, verbs with one prepositional argument, etc.)
are automatically derived from the set ofrewrite rules for transitive verbs and from a small set of ?base-form rewrite rules?
manually de-fined for each class.
The rules are then lexicalised using the information contained in the PropBankFrames3.3The PropBank Frames specify for each verb sense in PropBank, the arguments it accepts and the correspondingsemantic roles.361xyzxyznsubj dobjarg0 arg1xyztxzynsubjpassagentauxpassarg0 arg1Figure 2: Rewriting rules for active and passive3.1 Defining basic rewrite rulesIn the first phase, we manually define a set of rewrite rules for each possible syntactic variation ofa transitive verb.Using the XTAG Tree Adjoining Grammar [Gro01], we start by listing these variations.
In-deed a Tree Adjoining Grammar (TAG) lists the set of all possible syntactic configurations forbasic clauses and groups them into so-called (tree) families.
Thus the Tnx0Vnx1 family is a setof trees describing the possible syntactic contexts in which a transitive verb can occur.
Further,W1nx0Vnx1 names a tree in that family which describes a syntactic context in which a transitiveverb (nx0Vnx1) occurs together with a canonical nominal subject (nx0) and a questioned object(W1).
We use the XTAG families to produce a list of basic clauses illustrating the possible syntac-tic variations of each verb type.
For instance, using the Tnx0Vnx1 XTAG family, we create a ?listof Tnx0Vnx1 sentences?
i.e.,(1) ?John loves Mary?,?Mary is loved by John?, ?Mary, John loves?, ?It is Mary who is lovedby John?, ?It is John who loves Mary?, ?Mary who is loved by John?, ?John who lovesMary?,etc.We then parse these sentences using the Stanford parser and retrieve the correct dependency struc-ture from the output thus gathering the set of dependency structures associated by the Stanfordparser with the various syntactic realisations of a given verb type.Finally, for each distinct dependency structure found, we define a rewrite rule which maps thisdependency structure onto a unique (canonical) semantic representation.
For instance, the rewriterules for the active and passive form of a sentence featuring a transitive verb are as sketched inFigure 2 (see below for the exact content of these rules).To define our rewrite rules, we resort to a standard rewriting system namely GrGen [KG07].Used in multiple domains (e.g., formal calculus, combinatoric algebra, operational semantics),rewriting is a technique for modelling reduction and simplification.
For instance, the rewriting ruler1: x ?
y+ x ?
z ?
x ?
(y+ z) permits factorising 5 ?
6+ 5 ?
7+ 5 ?
8 to 5 ?
((6+ 7)+ 8).
Moregenerally, a rewriting system consists of a set of rewriting rules of the form l ?
r where l and rare filtering and rewriting patterns respectively.
Given an object o, such a rule will apply to o if o362rule nx0Vnx1 {pattern{verb:element;if{verb.verb != "None";}np0:element;np1:element;verb -:nsubj-> np0;verb -:dobj-> np1;}replace {verb -:arg0-> np0;verb -:arg1-> np1;}}rule nx1Vbynx0 {pattern{verb:element;if{verb.verb != "None";}np1:element;be:element;np0:element;verb -:nsubjpass-> np1;verb -:auxpass-> be;verb -:agent-> np0;}replace {verb -:arg0-> np0;verb -:arg1-> np1;}}Figure 3: Two rewrite rules in the GrGen formatsatisfies the filtering pattern l. The result of applying a rule to an object o is o where the sub-part ofo matched by l is rewritten according to the rewriting pattern r. Matching consists in looking fora homograph homomorphism between the pattern graph l and the host graph h while the allowedrewriting operations include information duplication, deletion and addition4.In GrGen, the objects handled by rewriting are attributed typed directed multigraphs.
Theseare directed graphs with typed nodes and edges, where between two nodes more than one edge ofthe same type and direction is permitted.
According to its type, each node or edge has a defined setof attributes associated with it.
Moreover, the type system suppports multiple inheritance on nodeand edge types.Expressive and efficient, GrGen5is well suited to specify our normalisation rules.
For instance,the rewrite rule sketched in figure 2 can be specified as given in Figure 3.
The left handside (lhs) ofthe rule specifies a pattern in terms of nodes, node attributes, edge labels and conditions on nodes.The right handside specifies how to rewrite the subgraphs matched by the lhs.More generally, the SD graphs can be seen as attributed typed directed multigraphs wherenode attributes are words and edge labels are grammatical relations.
Rewrite rules can then beused to modify, add or duplicate information present in the dependency graphs to create predicate-argument structures.Typically, rewriting is not confluent (different rule application orders yield different results)andGrGen supports various sophisticated control strategies.
So far however, we simply used rulesequencing : rules are tested and fired in the order in which they are listed.
They are ordered byspecificity with the most specific rules listed first.
For instance, the rule rewriting a long passivewill precede that for a short passive thereby preventing the short passive rule from applying to a4For a more precise definition of satisfaction, matching and replacement, we refer the reader to [EHK+99].5There are other rewriting systems available such as in particular, the Tsurgen system used in the Stanford Parser tomap parse trees into dependency graphs.
We opted for GrGen instead because it fitted our requirements best.
GrGenis efficient, notationally expressive (for specifying graphs but also rules and rule application strategies) and comes witha sophisticated debugging environment.
Importantly, GrGen developers are also quick to react to questions and tointegrate proposed modifications.363long passive sentence.We also use GrGen ?global rewriting mode?.
This ensures that whenever the rule filtering pat-tern matches several subgraphs in the input structures, the rewriting operates on each of the filteredsubgraph.
As we shall see in section 3, our rewrite rules are applied on not one but 5 dependencygraphs at a time.
Moreover the same rewrite rules may be applicable to several subgraphs in asentence analysis (typically when the sentence contains 2 or more verbs occurring in the samesyntactic configuration).
Global rewriting thereby avoids having to iterate over the rule set.3.2 Deriving new rewrite rulesManually specifying the normalisation rules is time consuming and error prone.
To extend theapproach to all types of verbs and syntactic configurations, we semi-automatically derive newrewrite rules from existing ones.Let us call source class, the syntactic class from which we derive new rules, target class, thesyntactic class for which rewrite rules are being derived and base-form rewrite rule, a rewriterule operating on a ?base-form?
that is, either on an active, a passive or a short passive formsubcategorising for canonical (i.e., non extracted) arguments.Now, let us define the set of primitive rewrite rules used to bootstrap the process as the set ofall rewrite rules defined for the source class together with the set of base-form rewrite rules definedfor the target class.To derive new rules from the set of primitive rewrite rules, we start by computing the differ-ences (in terms of edges, node and labels) between a source base-form rewrite rule (RR) and eithera target, base-form RR (DIFF+arg) or a source non base-form RR (DIFF+movt).
We then use theresulting DIFFs to compute new rewrite rules which differ either from a source RR by a DIFF+argpatch or from a target base-form RR by a DIFF+movt.
Figure 4 illustrates the idea on a specificexample.
The RR for a ditransitive verb with questioned object (?What does John put on the ta-ble?
?, W1nx0Vnx1pnx2) is derived both by applying a DIFF+W1patch to the nx0Vnx1pnx2active base-form RR (?John put a book on the table.?)
and by applying a DIFF+pnx2patch tothe source RR operating on W1nx0Vnx1 verbs with questioned object (?Who does Mary love??
).Note that in this way, the same rewrite rule (W1nx0Vnx2nx1) is derived in two different waysnamely, from the W1nx0Vnx1 RR by applying a DIFF+pnx2patch and from the nx0Vnx1pnx2RR by applying a DIFF+W1one.
We use this double derivation process to check the approachconsistency and found that in all cases, the same rule is derived by both possible paths.Using the method just sketched, we derived 377 rules from a set of 352 primitive rewrite rules.Although the ratio might seem weak, automating the derivation of rewrite rules facilitates systemmaintenance and extension.
This is because whenever a correction in the set of primitive rewriterules is carried out, the change automatically propagates to the related derived rules.
In practice,we found that a real feature when adapting the system to the Propbank data.
We believe that it willalso be useful when extending the system to deal with nominalisations.4 Evaluation and discussionWe evaluated our normalisation method both on a testsuite of constructed examples and on realworld data namely, the Propbank corpus.364nx0Vnx1nx0Vnx1pnx2W1nx0Vnx1W1nx0Vnx1pnx2+pnx2+pnx2DIFFarg+W1 +W1DIFFmvtSource RRTarget RRBase Form RRFigure 4: Deriving new rules from existing ones4.1 Evaluation on a testsuite of constructed examplesThis first evaluation aims to provide a systematic, fine grained assessment of how well the systemnormalises each of the several syntactic configurations assigned by XTAG to distinct verb types.The emphasis is here in covering the most exhaustive set of possible syntactic configurations possi-ble.
Because constructing the examples was intricate and time consuming, we did not cover all thepossibilities described by XTAG however.
Instead we concentrated on listing all the configurationsspecified by XTAG for 4 very distinct families namely, Tnx0Vnx1, Tnx0Vnx2nx1,Tnx0Vplnx1and Tnx0Vnx1pnx2.
The first class is the class for transitive verbs.
Because of passive, this classpermits many distinct variations.
The second class is the class of verbs with 3 nominal arguments.This class is difficult for role labelling as the distinction between the complements often relies onsemantic rather than syntactic grounds.
The third class is the class of verbs with a particle and 2nominal arguments (ring up) and the fourth, the class of ditransitive.For these constructed sentences, we had no gold standard i.e., no role annotation.
Hence weused logical inference to check normalisation.
We proceeded by grouping the test items in (non)entailment pairs and then checked whether the associated LSDs supported the detection of thecorrect entailment relation (i.e., true or false).The testsuite.
Using a restricted lexicon, a set of clauses covering the possible syntactic patternsof the four verb classes and regular expressions describing sentence-semantics pairs, we developa script generating (sentence,semantics) pairs where sentences contain one or more clauses.
Afterhaving manually verified the correctness of the generated pairs, we used them to construct textualentailment testsuite items that is, pairs of sentences annotated with TRUE or FALSE dependendingon whether the two sentences are related by entailment (TRUE) or not (FALSE).
The resultingtestsuite6contains 4 976 items of which 2 335 are entailments between a sentence and a clause(1V+TE, example 2), 1 019 between two complex sentences (2V+TE, example 3) and 1 622 arenon-entailments (V-TE, example 4).
(2) T1: John likes the book that Mary put on the table.T2: John likes a bookAnnotations: 1V+TE, TRUE6Available at http://www.loria.fr/?bedaride/publications/taln08-bedgar/index.html.365(3) T1: John likes the book that Mary put on the table.T2: The book which is put on the table by Mary, is liked by JohnAnnotations: 2V+TE, TRUE(4) T1: John likes the book that Mary put on the table.T2: John likes a tableAnnotations: V-TE, FALSEChecking for entailment.
For each testsuite item, we then checked for entailment by translatingLSDs into FOL formulae and checking entailment between the first five LSDs derived from theparser output for the sentences contained in the testsuite item.The translation of a LSD into a FOL formula is done as follows.
Each node is associated withan existentially quantified variable and a predication over that variable where the predicate used isthe word labelling the node.
Each edge translates to a binary relation between the source and thetarget node variables.
The overall formula associated with an LSD is then the conjunction of thepredications introduced by each node.
For instance, for the LSD given in Figure 1, the resultingformula is ?x, y, z : love(x) ?
john(y) ?mary(z) ?
arg0(x, y) ?
arg1(x, z).This translation procedure is of course very basic.
Nonetheless, because the testsuite buildson a restricted syntax and vocabulary7, it suffices to check how well the normalisation processsucceeds in assigning syntactic variants the same semantic representation.Results.
The test procedure just described is applied to the LSD graphs produced by the normal-isation module on the testsuite items.
Table 5 gives the results.
For each class of testsuite items(1V+TE, 2V+TE, V-TE), we list the percentage of cases recognised by the system as entailment(+TE) and non entailment (-TE).
Because FOL is only semi-decidable, the reasoners do not alwaysreturn an answer.
The Failure line gives the number of cases for which the reasoners fail.The results on positive entailments (1V+TE,2V+TE) show that the proposed normalisationmethod is generally successful in recognising syntax based entailments with an overall averageprecision of 86.3% (and a breakdown of 94.9% for 1V+TE and 66.6% for 2V+TE cases).
Impor-tantly, the results on negative entailments (99.2% overall precision) show that the method is notoverly permissive and does not conflate semantically distinct structures.
Finally, it can be seen thatthe results degrade for the Tnx0Vnx2nx1 class (John gave Mary a book).
This is due mainly togenuine syntactic ambiguities which cannot be resolved without further semantic (usually ontolog-ical) knowledge.
For instance, both The book which John gave the woman and The woman whomJohn gave the book are assigned the same dependency structures by the Stanford parser.
Hencethe same rewrite rule applies to both structures and necessarily assigns one of them the wronglabelling.
Other sources of errors are cases where the DIFF patch used to derive a new rule failto adequately generalise to the target structure.
In such cases, the erroneous rewrite rule can bemodified manually.7In particular, the testsuite contains no quantifiers.366family ans 1V+TE 2V+TE V-TE+TE 585 (98.2%) 212 (72.4%) 0 (0.0%)Tnx0Vnx1 -TE 11 (1.8%) 79 (27.0%) 57 (100.0%)Failure 0 (0.0%) 2 (0.6%) 0 (0.0%)+TE 513 (89.2%) 131 (55.7%) 3 (0.4%)Tnx0Vnx2nx1 -TE 61 (10.6%) 103 (43.8%) 703 (99.6%)Failure 1 (0.2%) 1 (0.5%) 0 (0.0%)+TE 567 (95.5%) 169 (67.9%) 0 (0.0%)Tnx0Vplnx1 -TE 27 (4.5%) 79 (31.7%) 198 (100.0%)Failure 0 (0.0%) 1 (0.4%) 0 (0.0%)+TE 550 (96.5%) 167 (69.0%) 10 (1.5%)Tnx0Vnx1pnx2 -TE 16 (2.8%) 69 (28.5%) 651 (98.5%)Failure 4 (0.7%) 6 (2.5%) 0 (0.0%)+TE 2215 (94.9%) 679 (66.6%) 13 (0.8%)all -TE 115 (4.9%) 330 (32.4%) 1609 (99.2%)Failure 5 (0.2%) 10 (1.0%) 0 (0.0%)Figure 5: Precision on constructed examples.
Each cell gives the proportion of cases recognised asentailment by the system.
Bold face figures give the precision i.e., the proportion of answers givenby the system that are correct.4.2 Evaluation on the PropBankThe PropBank (Proposition Bank) was created by semantic annotation of the Wall Street Journalsection of Treebank-2.
Each verb occurring in the Treebank has been treated as a semantic pred-icate and the surrounding text has been annotated for arguments and adjuncts of the predicate asillustrated in (5).
(5) [A0 He ] [AM-MOD would ] [AM-NEG n?t ] [V accept ] [A1 anything of value ] from [A2those he was writing about ] .The labels used for the core and adjunct-like arguments are the following8.
The labels A0 .. A5designate arguments associated with a verb predicate as defined in the PropBank Frames scheme.A0 is the agent, A1 the patient or the theme.
For A2 to A5 no consistent generalisation can bemade and the annotation reflects the decisions made when defining the PropBank Frames scheme.Further, the AM-T label describes adjunct like arguments of various sorts, where T is the type ofthe adjunct.
Types include locative, temporal, manner, etc.We used the PropBank to evaluate our normalisation procedure on free text.
As in the CoNLL(Conference on Natural Language Learning) shared task for SRL, the evaluation metrics usedare precision, recall and F measure.
An argument is said to be correctly recognised if the wordsspanning the argument as well as its semantic role match the PropBank annotation.
Precision isthe proportion of arguments predicted by a system which are correct.
Recall is the proportion ofcorrect arguments which are predicted by a system.
F-measure is the harmonic mean of precisionand recall.
The results are given below.8This is in fact simplified.
The PropBank corpus additionally provide information about R-* arguments (a referencesuch as a trace to some other argument of A* type) and C-* arguments (a continuation phrase in a split argument).367args 0 1 2 3 4 5 a m totalrecall 68.4% 68.2% 62.4% 47.2% 57.6% 5.3% 0.0% 64.4% 66.1%precision 88.0% 80.2% 76.4% 83.1% 83.3% 50.0% ?
75.0% 80.6%f-mesure 77.0% 73.7% 68.7% 60.2% 68.1% 9.5% ?
69.3% 72.6%Precision (80.6%) is comparable to the results obtained in the ConLL 2005 SRL shared taskwhere the top 8 systems have an average precision ranging from 76.55% to 82.28%.
Recall isgenerally a little low (the ConLL05 recall ranged from 64.99% to 76.78%) for mainly two reasons:either the Stanford parser, did not deliver the correct analysis or the required rewrite rule was notpresent.5 ConclusionOur approach is akin to so-called semantic role labelling (SRL) approaches [CM05] and to sev-eral rewriting approaches developed to modify parsing output in RTE systems [Ass07].
It differsfrom the SRL approaches in that unlike most SRLs systems, it is based on a hybrid, statistic andsymbolic, framework.
As a result, improving or extending the system can be done independentlyof the availability of an appropriately annotated corpus.
However, the quality, performance andcoverage of the system remains dependent on those of the Stanford parser9,Our approach also differs from approaches that use the lambda calculus to normalise syntacticvariation.
In such approaches, a compositional semantics module associates words and grammarrules or derivation structures with lambda terms which in effect normalise variations such as forinstance, the active/passive variation.
One important advantage of lambda based approaches is thatthe rewriting system is confluent.
The drawback however is that the specification of the appropriatelambda terms requires expert linguistic skills.
In contrast, the rewrite rule approach is compara-tively easier to handle (the rules presented here were developed by a computer scientist) and itsuse is supported by sophisticated developing environments such as GrGen which provides strongnotational expressivity (the rewrite rules can include conditions, can operate on graphs of arbi-trary depth, etc.
), a good debugging environment and good processing times.
In short although,the lambda calculus approach is undoubtly more principled, the rewrite rule approach is arguablyeasier to handle and easier to understand.Normalisation of linguistic representations is not new.
It is used in particular, in [BCC+07,DBBT07, RTF07] for dealing with entailment detection in the RTE (Recognising Textual Entail-ment) challenge.
The approach we present here differs from these approaches both by its sys-tematic treatment of syntactic variation and by its use of GrGen as a framework for specifyingtransformations.
More generally, our approach emphasises the following three points namely (i)the systematic testing of all possible syntactic variations (based on the information contained inXTAG); (ii) the use of an expressive, efficient and well-understood graph rewriting system fordefining transformations; and (iii) the development of a methodology for automatically derivingnew rewrite rules from existing ones.By providing a well-defined framework for specifying, deriving and evaluating rewrite rules,we strive to develop a system that normalises NLP representations in a way that best supports9[KM03] report a label F-mesure of 86.3% on section 23 of the Penn Treebank.368semantic processing.
The emphasis is on aligning Predicate/Argument structures that diverge inthe surface text but that are semantically similar (e.g., John buy a car from Peter/Peter sells a carto John).
In particular, we plan to extend the system to normalise nominal dependencies (usingNomBank) and converse constructions.References[Ass07] Association for Computational Linguistics.
Proceedings of the ACL-PASCAL Work-shop on Textual Entailment and Paraphrasing, Prague, Czech Republic, June 2007.
[BCC+07] D. G. Bobrow, C. Condoravdi, R. S. Crouch, V. de Paiva, L. Karttunen, T. H. King,R.
Nairn, L. Price, and A. Zaenen.
Precision-focused textual inference.
In ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pages 16?21, Prague,Czech Republic, June 2007.
[BCS+07] M. Banko, M. J. Cafarella, S. Soderland, M. Broadhead, and O. Etzioni.
Open in-formation extraction from the web.
In IJCAI ?07: Proceedings of International JointConference on Artificial Intelligence, pages 2670?2676, Hyderabad, India, January2007.
[CM05] X. Carreras and L. Marquez.
Introduction to the conll-2005 shared task: Semantic rolelabeling.
In Proceedings of the CoNLL-2005 Shared Task: Semantic Role Labeling,pages 152?164, Ann Arbor, Michigan, June 2005.
[CMB99] J. Carroll, G. Minnen, and T. Briscoe.
Corpus annotation for parser evaluation.
InEACL Workshop on Linguistically Interpreted Corpora, Bergen, Norway, June 1999.
[CS07] A.
B. Clegg and A. J. Shepherd.
Benchmarking natural-language parsers for biologicalapplications using dependency graphs.
BMC Bioinformatics, 8:24, January 2007.
[DBBT07] R. Delmonte, A. Bristot, M. A. P. Boniforti, and S. Tonelli.
Entailment and anaphoraresolution in rte3.
In ACL-PASCAL Workshop on Textual Entailment and Paraphras-ing, pages 48?53, Prague, Czech Republic, June 2007.
[dM08] M.-C. de Marneffe and C. D. Manning.
The stanford typed dependencies representa-tions.
In COLING?08 Workshop on Cross-framework and Cross-domain Parser Eval-uation, Manchester, England, August 2008.
[dMM06] Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning.
Gener-ating typed dependency parses from phrase structure parses.
In LREC ?06: Proceed-ings of 5th International Conference on Language Resources and Evaluation, pages449?454, Genoa, Italy, May 2006.
[EHK+99] H. Ehrig, R. Heckel, M. Korff, Loewe M., L. Ribeiro, A. Wagner, and A. Corradini.Handbook of Graph Grammars and Computing by Graph Transformation., volume 1,chapter Algebraic Approaches to Graph Transformation - Part II: Single Pushout A.and Comparison with Double Pushout A, pages 247?312.
World Scientific, 1999.369[EOR07] G. Erkan, A. Ozgur, and D. R. Radev.
Semi-supervised classification for extractingprotein interaction sentences using dependency parsing.
In EMNLP-CoNLL ?07: Pro-ceedings of the 2007 Joint Conference on Empirical Methods in Natural LanguageProcessing and Computational Natural Language Learning, pages 228?237, Prague,Czech Republic, June 2007.
Association for Computational Linguistics.
[Gro01] XTAG Research Group.
A lexicalized tree adjoining grammar for english.
TechnicalReport IRCS-01-03, IRCS, University of Pennsylvania, 2001.
[KCR+03] T. King, R. Crouch, S. Riezler, M. Dalrymple, and R. Kaplan.
The parc 700 depen-dency bank.
In EACL workshop on Linguistically Interpreted Corpora, Budapest,Hungary, April 2003.
[KG07] M. Kroll and R. Gei?.
Developing graph transformations with grgen.net.
Technicalreport, October 2007. preliminary version, submitted to AGTIVE 2007.
[KM03] D. Klein and C. D. Manning.
Accurate unlexicalized parsing.
In ACL ?03: Proceedingsof the 41st Annual Meeting of the Association for Computational Linguistics, pages423?430, Sapporo, Japan, July 2003.
Association for Computational Linguistics.
[MP07] A. Meena and T. V. Prabhakar.
Sentence level sentiment analysis in the presenceof conjuncts using linguistic analysis.
In Ecir ?07: Proceedings of 29th EuropeanConference on Information Retrieval, pages 573?580, Rome, Italy, April 2007.
[RTF07] A.
B. N. Reiter, S. Thater, and A. Frank.
A semantic approach to textual entailment:System evaluation and task analysis.
In ACL-PASCAL Workshop on Textual Entailmentand Paraphrasing, pages 10?15, Prague, Czech Republic, June 2007.
[SJM+08] M. Surdeanu, R. Johansson, A. Meyers, L. M`arquez, and J. Nivre.
The CoNLL-2008shared task on joint parsing of syntactic and semantic dependencies.
In CoNLL ?08:Proceedings of the 12th Conference on Computational Natural Language Learning,pages 159?177, Manchester, UK, August 2008.
[ZJZ06] L. Zhuang, F. Jing, and X.-Y.
Zhu.
Movie review mining and summarization.
InCIKM ?06: Proceedings of the 15th ACM international conference on Informationand knowledge management, pages 43?50, Arlington, Virginia, USA, November 2006.ACM.370
