Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 395?404,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsLearning from Post-Editing:Online Model Adaptation for Statistical Machine TranslationMichael Denkowski Chris Dyer Alon LavieLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213 USA{mdenkows,cdyer,alavie}@cs.cmu.eduAbstractUsing machine translation output as astarting point for human translation hasbecome an increasingly common applica-tion of MT.
We propose and evaluate threecomputationally efficient online methodsfor updating statistical MT systems in ascenario where post-edited MT output isconstantly being returned to the system:(1) adding new rules to the translationmodel from the post-edited content, (2)updating a Bayesian language model ofthe target language that is used by theMT system, and (3) updating the MTsystem?s discriminative parameters witha MIRA step.
Individually, these tech-niques can substantially improve MT qual-ity, even over strong baselines.
Moreover,we see super-additive improvements whenall three techniques are used in tandem.1 IntroductionUsing machine translation outputs as a startingpoint for human translators is becoming increas-ingly common and is now arguably one of the mostcommercially important applications of MT.
Con-siderable evidence has accumulated showing thathuman translators are more productive and accu-rate when post-editing MT output than when trans-lating from scratch (Guerberof, 2009; Carl et al.,2011; Koehn, 2012; Zhechev, 2012, inter alia).An important (if unsurprising) insight from priorresearch in this area is that translators becomemore productive as MT quality improves (Tat-sumi, 2009).
While general improvements to MTcontinue to lead to further productivity gains, weexplore how MT quality can be improved specifi-cally in an online post-editing scenario in whichsentence-level MT outputs are constantly beingpresented to human experts, edited, and then re-turned to the system for immediate learning.
Thistask is challenging in two regards.
First, from atechnical perspective, post-edited outputs must beprocessed rapidly: a productive post-editor cannotwait for a standard batch MT training pipeline tobe rerun after each sentence is corrected!
Sec-ond, from a methodological perspective, it is ex-pensive to run many human subject experiments,in particular when the human subjects must havetranslation expertise.
We therefore use a sim-ulated post-editing paradigm in which eithernon-post-edited reference translations or manuallypost-edited translations from a similar MT systemare used in lieu of human post-editors (?2).
Thisparadigm allows us to efficiently develop and eval-uate systems that can go on to function in real-timepost-editing scenarios without modification.We present and evaluate three online methodsfor improving translation models using feedbackfrom editors: adding new translations rules tothe translation grammar (?3), updating a Bayesianlanguage model with observations of the post-edited output (?4), and using an online discrimi-native parameter update to minimize model error(?5).
These techniques are computationally effi-cient and make minimal use of approximation orheuristics, handling initial and incremental data ina uniform way.
We evaluate these techniques in avariety of language and data scenarios that mimicthe demands of real-world translation tasks.
Com-pared to a competitive baseline, we show substan-tial improvement from updating the translationgrammar or language model independently andsuper-additive gains from combining these tech-niques with a MIRA update (?6).
We then discusshow our techniques relate to prior work (?7) andconclude (?8).2 Simulated Post-Editing ParadigmIn post-editing scenarios, humans continuouslyedit machine translation outputs into production-quality translations, providing an additional, con-395stant stream of data absent in batch translation.This data consists of highly domain-relevant ref-erence translations that are minimally differentfrom MT outputs, making them ideal for learn-ing.
However, true post-editing data is infeasi-ble to collect during system development and in-ternal testing as standard MT pipelines requiretens of thousands of sentences to be translatedwith low latency.
To address this problem, Hardtand Elming (2010) formulate the task of sim-ulated post-editing, wherein pre-generated refer-ence translations are used as a stand-in for actualpost-editing.
This approximation is equivalent tothe case where humans edit each translation hy-pothesis to be identical to the reference rather thansimply correcting the MT output to be grammat-ical and meaning-equivalent to the source.
Ourwork uses this approximation for tuning and eval-uation.
We also introduce a more accurate approx-imation wherein MT output from the target sys-tem (or a similar system) is post-edited in advance,creating ?offline?
post-edited data that is similarto expected system outputs and should thus min-imize unnecessary edits.
An experiment in ?6.4compares the two approximations.In our simulated post-editing tasks, decoding(for both the test corpus and each pass over thedevelopment corpus during optimization) beginswith baseline models trained on standard bilin-gual and monolingual data.
After each sentenceis translated, the following take place in order:First, MIRA uses the new source?reference pairto update weights for the current models.
Second,the source is aligned to the reference and used toupdate the translation grammar.
Third, the refer-ence is added to the Bayesian language model.
Assentences are translated, the models gain valuablecontext information, allowing them to zero in onthe target document and translator.
Context is re-set at the start of each development or test corpus.1This setup, which allows a uniform approach totuning and decoding, is visualized in Figure 1.3 Translation Grammar AdaptationTranslation models (either phrase tables or syn-chronous grammars) are typically generated of-fline from large bilingual text.
This is reasonablein scenarios where available training data is fixedover long periods of time.
However, this approach1Initial experiments show this to outperform resettingmodels on more fine-grained document boundaries, althoughfurther investigation is warranted.Hola contestadora ...Hello voicemail, ...He llamado a servicio ...
I?ve called for tech ...Ignor?e la advertencia ...
I ignored my boss?
...Ahora anochece, ...Now it?s evening, and ...Todav?
?a sigo en espera ...I?m still on hold ...No creo que me hayas ...
I don?t think you ...Ya he presionado cada ...
I punched every touch ...Incremental training dataSourceTarget (Reference)Figure 1: Context when translating an input sen-tence (bold) with simulated post-editing.
Previ-ous sentences and references (shaded) are addedto the training data.
After the current sentence istranslated, it is aligned to the reference (italic) andadded to the context for the next sentence.does not allow adding new data without repeatingmodel estimation in its entirety, which may takehours or days.
In this section, we describe a simpletechnique for incorporating new bilingual trainingdata as soon as it is available.
Our approach isan extension of the on-demand grammar extractordescribed by Lopez (2008a).
We extend the workinitially designed for on-the-fly grammar extrac-tion from static data (to mitigate the expense ofstoring large translation grammars), to specificallyhandle incremental data from post-editing.3.1 Suffix Array Grammar ExtractionLopez (2008a) introduces an alternative to tradi-tional model estimation for hierarchical phrase-based statistical machine translation (Chiang,2007).
Rather than estimating a single grammarfrom all training data, the aligned bitext is indexedusing a source-side suffix array (Manber and My-ers, 1993).
When an input sentence is to be trans-lated, a grammar extraction program samples in-stances of aligned phrase pairs from the suffix ar-ray that match the source side of the sentence.Using statistics from these samples rather thanthe entire bitext, a sentence-specific grammar israpidly generated.
In addition to speed gains fromsampling, indexing the source side of the bitext fa-cilitates a more powerful feature set.
Rules in on-demand grammars are generated using a sample Sfor each source phrase f in the input sentence.
Thesample, containing pairs ?f, e?, is used to calculatethe following statistics:396Feature Baseline Adaptivecoherentp(e|f)CS(f, e)|S|CS(f, e) + CL(f, e)|S|+ |L|sample size |S| |S|+ |L|co-occur-rence ?f, e?CS(f, e) CS(f, e)+CL(f, e)singleton fCS(f)= 1CS(f) + CL(f) =1singleton?f, e?CS(f, e)= 1CS(f, e) + CL(f, e)= 1post-edit sup-port ?f, e?0 CL(f, e) > 0Table 1: Phrase feature definitions for baseline andadaptive translation models.?
CS(f, e): count of instances in S where faligns to e (phrase co-occurrence count).?
CS(f): count of instances in S where f alignsto any target phrase.?
|S|: total number of instances in S, equal tonumber of occurrences of f in training data,capped by the sample size limit.These statistics are used to instantiate translationrules X?
?f, e?
and calculate scores for the phrasefeature set shown in the ?Baseline?
column of Ta-ble 1.
Notably, the coherent phrase translationprobability that conditions on f occurring in thedata (|S|) rather than f being extracted as part of aphrase pair (CS(f)) is shown by Lopez (2008b) toyield significant improvement over the traditionaltranslation probability.3.2 Online Grammar ExtractionWhen a human translator post-edits MT output, anew bilingual sentence pair is created.
However,in typical settings, it can be weeks or months be-fore these training instances are incorporated intobilingual data and models retrained.
Our exten-sion to on-demand grammar extraction incorpo-rates these new training instances into the modelimmediately.
In addition to a static suffix arraythat indexes initial data, our system maintains adynamic lookup table.
Each new sentence pair isword-aligned with the model estimated from theinitial data (a process often called forced align-ment).
This makes a generally insignificant ap-proximation with respect to the original alignmentmodel.
Extractable phrase pairs are stored in thelookup table and phrase occurrences are countedon the source side.
When subsequent grammarsare extracted, the suffix array sample S for eachf is accompanied by an exhaustive lookup L fromthe lookup table.
Matching statistics are calculatedfrom L:?
CL(f, e): count of instances in L where faligns to e.?
CL(f): count of instances inLwhere f alignsto any target phrase.?
|L|: total number of instances of f in post-editing data (no size limit).We use combined statistics from S and L to calcu-late scores for the ?Adaptive?
feature set defined inTable 1.
In addition to updating existing features,we introduce a new indicator feature that identi-fies rules supported by post-editor feedback.
Fur-ther, our approach allows us to extract rules thatencode translations (phrase mappings and reorder-ings) only observed in the incremental post-editingdata.
This process, which can be seen as influ-encing the distribution from which grammars aresampled over time, produces comparable resultsto the infeasible process of rebuilding the transla-tion model after every sentence is translated withthe added benefit of allowing an optimizer to learna weight for the post-edited data via the post-editsupport feature.
The simple aggregation of statis-tics allows our model to handle initial and incre-mental data in a formally consistent way.
Further,any additional features that can be calculated on asuffix array sample can be matched by an incre-mental data lookup, making our translation modela viable platform for further exploration in onlinelearning for MT.4 Language Model AdaptationAdapting language models in an online mannerbased on the content they are generating has longbeen seen as a promising technique for improvingautomatic speech recognition and machine transla-tion (Kuhn and de Mori, 1990; Zhao et al., 2004;Sanchis-Trilles, 2012, inter alia).
The post-editingscenario we are considering simplifies this processsomewhat since rather than only having a poste-rior distribution over machine-generated outputs(any of which may be ungrammatical), the out-puts, once edited by human translators, may bepresumed to be grammatical.We thus take a novel approach to languagemodel adaptation, building on recent work show-ing that state-of-the-art language models can be397inferred as the posterior predictive distributionof a Bayesian language model with hierarchi-cal Pitman-Yor process priors, conditioned on thetraining corpus (Teh, 2006).
The Bayesian formu-lation provides a natural way to incorporate pro-gressively more data: by updating the posteriordistribution given subsequent observations.
Fur-thermore, the nonparametric nature of the modelmeans that the model is well suited to poten-tially unbounded growth of vocabulary.
Unfortu-nately, in general, Bayesian techniques are com-putationally difficult to work with.
However, hi-erarchical Pitman-Yor process language models(HPYPLMs) are convenient in this regard since(1) inference can be carried out efficiently in aconvenient collapsed representation (the ?Chineserestaurant franchise?)
and (2) the posterior predic-tive distribution from a single sample provides ahigh quality language model.We thus use the following procedure.
Usingthe target side of the bitext as observations, werun the Gibbs sampling procedure described byTeh (2006) for 100 iterations in a 3-gram HPY-PLM.
The inferred ?seating configuration?
definesa posterior predictive distribution over words in 2-gram contexts (as with any 3-gram LM) as wellas a posterior distribution over how the model willgenerate subsequent observations.
We use the for-mer as a language model component of a transla-tion model.
And, as post-edited sentences becomeavailable, we add their n-grams to the model us-ing the later.
We do not run any Gibbs sampling.Just updating the language model in this way, weobtain the results shown in Table 2 for the experi-mental conditions described in ?6.5 Learning Feature WeightsMT system parameter optimization (learning fea-ture weights for the decoder) is also typically con-ducted as a batch process.
Discriminative learn-ing techniques such as minimum error rate train-ing (Och, 2003) are used to find feature weightsthat maximize automatic metric score on a smalldevelopment corpus.
The resulting weight vectoris then used to decode given input sentences.
Us-ing this approach with post-editing tasks presentstwo major issues.
First, reference translation areonly considered after all sentences are translated,a mismatch with post-editing where references areavailable incrementally.
Second, despite the factthat adaptive feature sets become more powerfulas post-editing data increases, an optimizer mustSpanish?English WMT10 WMT11 TED1 TED2HPYPLM 25.5 24.8 29.4 26.6+data 25.8 25.2 29.5 27.0English?Spanish WMT10 WMT11 TED1 TED2HPYPLM 25.1 26.8 26.0 24.3+data 25.4 27.2 26.2 25.0Arabic?English MT08 MT09 TED1 TED2HPYPLM 19.3 24.7 9.5 10.0+data 19.6 24.9 9.8 10.5Table 2: BLEU scores for systems with trigramHPYPLM (no large language model), with andwithout incremental updates from simulated post-editing data.
Scores are averages over 3 optimizerruns.
Bold scores indicate statistically significantimprovement.
Tuning set scores are italicized.learn a single corpus-level weight for each fea-ture.
This forces an averaging effect that can leadto decoding individual sentences with suboptimalweights.
We address the first issue by using ref-erence translations to simulate post-editing (Hardtand Elming, 2010) at tuning time and the secondby using a version of the margin-infused relaxedalgorithm (Crammer et al., 2006; Eidelman, 2012)to make online parameter updates during decod-ing.
The result is a consistent approach to tuningand decoding that brings out the potential of adap-tive models.5.1 Parameter OptimizationIn order to make our decoding process fully con-sistent with tuning, we introduce an online dis-criminative parameter update that allows our adap-tive translation and language models be weightedappropriately as more data is available.
This re-quires an optimization algorithm that can func-tion as an online learner during decoding as wellas a batch optimizer during tuning.
Popular opti-mizers such as MERT (Och, 2003) and pairwiserank optimization (Hopkins and May, 2011) can-not be used due to their reliance on corpus-leveloptimization.
We select the cutting-plane variantof the margin-infused relaxed algorithm (Chiang,2012; Crammer et al., 2006) with additional exten-sions described by Eidelman (2012).
MIRA is anonline large-margin learner that makes a param-eter update after each model prediction with theobjective of choosing the correct output over theincorrect output by a margin at least as large as thecost of predicting the incorrect output.
Applied398to MT system optimization on a development cor-pus, MIRA proceeds as follows.
The MT systemgenerates a list of the k best translations for a sin-gle input sentence.
From the list, a ?hope?
hy-pothesis is selected as a translation with both highmodel score and high automatic metric score.
A?fear?
hypothesis is selected as a translation withhigh model score but low metric score.
Parametersare updated away from the fear hypothesis, towardthe hope hypothesis, and the system processes thenext input sentence.
This process continues for aset number of passes over the development corpus.All adaptive systems used in our work are opti-mized with this variant of MIRA using the param-eter settings described by Eidelman (2012).
Foreach pass over the data, translation and languagemodels have incremental access to reference trans-lations (simulated post-editing data) as input sen-tences are translated.
Translation and languagemodels reset to using background data only at thebeginning of each MIRA iteration.25.2 Online Parameter UpdatesOur optimization strategy allows us to treat de-coding as if it were simply the next iteration ofMIRA (or alternatively that MIRA makes a singlepass over an input corpus that consists of the de-velopment data concatenated n times followed byunseen input data).
After each sentence is trans-lated, a reference translation (resulting from ac-tual human post-editing in production or simulatedpost-editing for our experiments) is provided tothe models and MIRA makes a parameter update.In the only departure from our optimization setup,we decrease the maximum step size for MIRA (de-scribed in ?6.2), effectively increasing regulariza-tion strength.
This allows us to prefer small ad-justments to already optimized decoding parame-ters over the large changes needed during tuning.It is also important to note that by using MIRAfor updating weights during both tuning and de-coding, we avoid scaling issues between multipleoptimizers (such as when tuning with MERT andupdating with a passive-aggressive algorithm).6 ExperimentsWe evaluate our online extensions to standardmachine translation systems in a series of sim-2Resetting translation and language models prevents con-tamination.
If models retained state from previous passesover the development set, they would include data for inputsentences before they were translated, rather than after as inpost-editing.Spanish?English WMT10 WMT11 TED1 TED2Base MERT 29.1 27.9 32.8 29.6Base MIRA 29.2 28.0 32.7 29.7G 29.8 28.3 34.2 30.7L 29.2 28.1 33.0 29.8M 29.2 28.1 33.1 29.8G+L+M 30.0 28.8 35.2 31.3English?Spanish WMT10 WMT11 TED1 TED2Base MERT 27.8 29.4 26.5 25.7Base MIRA 27.7 29.6 26.8 26.7G 28.1 29.8 27.9 27.5L 27.9 29.7 26.8 26.5M 27.9 29.7 27.2 26.6G+L+M 28.4 30.4 28.6 27.9Arabic?English MT08 MT09 TED1 TED2Base MERT 21.5 25.0 10.4 10.5Base MIRA 21.2 25.9 10.6 10.9G 21.8 26.2 11.0 11.7L 20.6 25.7 10.6 10.9M 21.3 25.7 10.8 11.0G+L+M 21.8 26.5 11.4 11.8Table 3: BLEU scores for baseline and adap-tive systems.
Scores are averages over three opti-mizer runs.
Highest scores are bold and tuning setscores are italicized.
All fully adaptive systems(G+L+M) show statistically significant improve-ment over both MERT and MIRA baselines.ulated post-editing experiments that cover high-traffic languages and challenging domains.
Weshow incremental improvement from our adaptivemodels and significantly larger gains when pair-ing our models with an online parameter update.We finally validate our adaptive system on actualpost-edited data.6.1 DataWe conduct a series of simulated post-editingexperiments in three full scale language sce-narios: Spanish?English, English?Spanish, andArabic?English.
Spanish?English and English?Spanish systems are trained on the 2012 NAACLWMT (Callison-Burch et al., 2012) constrainedresources (2 million bilingual sentences, 300 mil-lion words of monolingual Spanish, and 1.1 billionwords of monolingual English).
Arabic?Englishsystems are trained on the 2012 NIST OpenMT(Przybocki, 2012) constrained bilingual resourcesplus a selection from the English Gigaword cor-pus (Parker et al., 2011) (5 million bilingual sen-tences and 650 million words of monolingual En-399glish).
We tune and evaluate on standard newssets: WMT10 and WMT11 for Spanish?Englishand English?Spanish, and MT08 and MT09 forArabic?English.
To simulate real-world post edit-ing where one translator works on a document at atime, we use only one of the four available refer-ence translation sets for MT08 and MT09.We also evaluate on a blind domain adapta-tion scenario that mimics the demands placedon MT systems in real-world translation tasks.The Web Inventory of Transcribed and TranslatedTalks (WIT3) corpus (Cettolo et al., 2012) makestranscriptions of TED talks3available in severallanguages, including English, Spanish, and Ara-bic.
For each language pair, we select two sets of10 talk transcripts each (2000-3000 sentences) asblind evaluation sets.
These sets consist of spokenlanguage covering a broad range of topics.
Sys-tems have no access to any training or develop-ment data in this domain prior to translation.6.2 Translation SystemsFor each language scenario, we first construct acompetitive baseline system.
Bilingual data isword aligned using the model described by Dyeret al.
(2013) and suffix array-backed transla-tion grammars are extracted using the methoddescribed by Lopez (2008a).
We add the stan-dard lexical and derivation features4from Lopez(2008b) and Dyer et al.
(2010).
An unpruned,modified Kneser-Ney-smoothed 4-gram languagemodel is estimated using the KenLM toolkit(Heafield et al., 2013).
Feature weights are op-timized using the lattice-based variant of MERT(Macherey et al., 2008; Och, 2003) on eitherWMT10 or MT08.
Evaluation sets are translatedusing the cdec decoder (Dyer et al., 2010) andevaluated with the BLEU metric (Papineni et al.,2002).
These results are listed as ?Base MERT?in Table 3.
To establish a baseline for our adap-tive systems, we tune the same baseline systemusing cutting-plane MIRA with 500-best lists, thepseudo-document approximation described by Ei-delman (2012), and a maximum update size of0.01.
We begin with uniform weights and make20 passes over the development corpus.
Resultsfor this system are listed as ?Base MIRA?.To evaluate the impact of each online modeladaptation technique, we report the results for the3http://www.ted.com/talks4Derivation features consist of word count, discretizedrule-level non-terminal count (0, 1, or 2), glue rule count,and out-of-vocabulary pass-through count.News TED TalksNew Supp New SuppSpanish?English 15% 19% 14% 18%English?Spanish 12% 16% 9% 13%Arabic?English 9% 12% 23% 28%Table 5: Percentages of new rules (only seenin incremental data) and post-edit supported rules(Rules from all data for which the ?post-edit sup-port ?f, e??
feature fires) in grammars by domain.following systems in Table 3:?
G: Baseline MIRA system with online gram-mar extraction, including incrementally up-dating existing phrase features plus an addi-tional indicator feature for post-edit support.?
L: Baseline MIRA with a trigram hierarchi-cal Pitman-Yor process language model thatis incrementally updated, including a sepa-rate out-of-vocabulary feature.?
M: Baseline MIRA with online featureweight updates from cutting-plane MIRA.Finally, we report results for a fully adaptivesystem that includes online grammar, languagemodel, and feature weight updates.
This systemis reported as ?G+L+M?.
To account for optimizerinstability, all systems are tuned (consisting ofrunning either MERT or MIRA) and evaluated 3times.
We report average scores over optimizerruns and conduct statistical significance tests us-ing the methods described by Clark et al.
(2011).6.3 ResultsOur simulated translation post-editing experi-ments are summarized in Table 3.
Simply mov-ing from MERT to cutting-plane MIRA for pa-rameter optimization yields improvement in mostcases, corroborating existing work (Eidelman,2012).
Using incremental post-editing data to up-date translation grammars (G) yields further im-provement in all cases evaluated.
Gains are signif-icantly larger for TED talks where translator feed-back can bridge the gap between domains.
Table 5shows the aggregate percentages of rules in onlinegrammars that are entirely new (extracted frompost-editing instances only) or post-edit supported(superset of new rules).
While percentages varyby data set, the overall trend is a combination oflearning new vocabulary and reordering and dis-ambiguating existing translation choices.The introduction of a trigram Bayesian lan-guage model (L) yields mixed results: in some400Base MERT and changing the definition of what the Zona Cero is .G+L+M and the changing definition of what the Ground Zero is .Reference and the changing definition of what Ground Zero is .Base MERT was that when we side by side comparisons with coal , timberG+L+M was that when we did side-by-side comparisons with wood charcoal ,Reference was when we did side-by-side comparisons with wood charcoal ,Base MERT There was a way ?
there was one ?G+L+M There was a way ?
there had to be a way ?Reference There was a way ?
there had to be a way ?Table 4: Translation examples from baseline and fully adaptive systems of Spanish TED talks into En-glish.
Examples illustrate (from top to bottom) learning translations for new vocabulary items, selectingcorrect translation candidates for the domain, and learning domain-appropriate phrasing.cases it leads to slight improvement and in oth-ers, degradation.
It appears that a static but large4-gram language model often outperforms an in-crementally updated but smaller trigram model.Further, learning a single weight for the Bayesianmodel can lead to a harmful mismatch.
As a tun-ing pass over the development corpus proceeds,the model incorporates additional data and MIRAlearns a weight corresponding to its predictiveability at the end of the corpus.
During decod-ing, all sentences are translated with this languagemodel weight, even before the model can ade-quately adapt itself to the target domain.
Thisproblem is alleviated in our fully adaptive system.Using cutting-plane MIRA to incrementally up-date weights during decoding (M) also leads tomixed results, frequently resulting in both smallincreases and decreases in score.
This could bedue to the noise incurred when making small ad-justments to static features after each sentence:depending on the similarity between the previousand current sentence and the limit of the step size(regularization strength), a parameter update mayslightly improve or degrade translation.Finally, we see significantly larger gains forour fully adaptive system (G+L+M) that com-bines adaptive translation grammars and languagemodels with online parameter updates.
In manycases, the difference between the baseline sys-tems and our adaptive system is greater than thesum of the differences from our individual tech-niques, demonstrating the effectiveness of com-bining online learning methods.
Our final sys-tem has two key advantages over any individualextension.
First, incremental updates from MIRAcan rescale weights for features that change overtime, keeping the model consistent.
Second, theBayesian language model?s out-of-vocabulary fea-ture can discriminate between true OOV itemsand vocabulary items in the post-editing data notpresent in the monolingual data.
By contrast, theonly OOVs in the baseline system are untranslateditems, as the target side of the bitext is included inthe language model training data.
This interplaybetween the adaptive components in our transla-tion system leads to significant gains over MERTand MIRA baselines.
Table 4 contains examplesfrom our system?s output that exemplify key im-provements in translation quality.
With respect toperformance, our fully adaptive system translatesan average of 1.5 sentences per second per CPUcore.
The additional cost incurred updating trans-lation grammars and language models is less thanone second per sentence (though the baseline costof on-demand grammar extraction can be up to afew seconds).
In total, the system is well withinthe acceptable speed range needed to function inreal-time human translation scenarios.6.4 Evaluation Using Post-Edited ReferencesThe 2012 ACL Workshop on Machine Translation(Callison-Burch et al., 2012) makes available a setof 1832 English?Spanish parallel news source sen-tences, independent references, initial MT outputs,and post-edited MT outputs.
The employed MTsystem is trained on largely the same resources asour own English?Spanish system, granting the op-portunity for a much closer approximation to anactual post-editing task; our system configurationsscore between 54 and 56 BLEU against the sam-ple MT, indicating that humans post-edited trans-lations similar but not identical to our own.
Wesplit the data into development and test sets, each916 sentences, and run 3 iterations of optimizingon the development set and evaluating on the testset with both the MERT baseline and our G+L+M401system on both types of references.
Using inde-pendent references for tuning and evaluation (asbefore), our system yields an improvement of 0.6BLEU (23.3 to 23.9).
With post-edited references,our system yields an improvement of 1.3 BLEU(43.0 to 44.3).
This provides strong evidence thatour adaptive systems would provide better trans-lations (both in terms of absolute quality and im-provement over a standard baseline) for real-worldpost-editing scenarios.7 Related WorkPrior work has led to the extension of standardphrase-based translation systems to make use ofincrementally available data.5Approaches gen-erally fall into categories of adding new data totranslation models and of using incremental datato adjust model parameters (feature weights).
Inthe first case, Nepveu et al.
(2004) use cache-basedtranslation and language models to incorporatedata from the current document into a computer-aided translation scenario.
Ortiz-Mart?
?nez et al.
(2010) augment a standard translation model bystoring sufficient statistics in addition to featurescores for phrase pairs, allowing feature values tobe incrementally updated as new sentence pairsare available for phrase extraction.
Hardt and Elm-ing (2010) demonstrate the benefit of maintain-ing a distinction between background and post-editing data in an adaptive model with simulatedpost-editing.
Though not targeted at post-editingapplications, the most similar work to our onlinegrammar adaptation is the stream-based transla-tion model described by Levenberg et al.
(2010).The authors introduce a dynamic suffix array thatcan incorporate new training text as it becomesavailable.
Sanchis-Trilles (2012) proposes a strat-egy for online language model adaptation whereinseveral smaller domain-specific models are builtand their scores interpolated for each sentencetranslated based on the target domain.Focusing on incrementally updating model pa-rameters with post-editing data, Mart?
?nez-G?omezet al.
(2012) and L?opez-Salcedo et al.
(2012)show improvement under some conditions whenusing techniques including passive-aggressive al-gorithms, perceptron, and discriminative ridge re-gression to adapt feature weights for systems ini-tially tuned using MERT.
This work also uses ref-erence translations to simulate post-editing.
Saluja5Prior to phrase-based systems, NISHIDA et al.
(1988)use post-editing data to correct errors in transfer-based MT.et al.
(2012) introduce a support vector machine-based algorithm capable of learning from binary-labeled examples.
This learning algorithm is usedto incrementally adjust feature weights given userfeedback on whether a translation is ?good?
or?bad?.
As with our work, this strategy can be usedduring both optimization and decoding.Finally, Simard and Foster (2013) apply apipeline solution to the post-editing task whereina second stage automatic post-editor (APE) sys-tem learns to replicate the corrections made to ini-tial MT output by human translators.
As incre-mental data accumulates, the APE (itself a statisti-cal phrase-based system) attempts to ?correct?
theMT output before it is shown to humans.8 ConclusionCasting machine translation for post-editing asan online learning task, we have presented threemethods for incremental model adaptation: addingdata to the indexed bitext from which gram-mars are extracted, updating a Bayesian languagemodel with incremental data, and using an on-line discriminative parameter update during de-coding.
These methods, which allow the sys-tem to handle all data in a uniform way, are ap-plied to a strong baseline system optimized usingMIRA in conjunction with simulated post-editing.In addition to showing gains for individual meth-ods under various circumstances, we report super-additive improvement from combining our tech-niques to produce a fully adaptive system.
Im-provements generalize over language and data sce-narios, with the greatest gains realized in blindout-of-domain tasks where the system must relyheavily on post-editor feedback to improve qual-ity.
Gains are also more significant when using of-fline post-edited references, showing promise forapplying our techniques to real-world post-editingtasks.
All software used for our online modeladaptation experiments is freely available under anopen source license as part of the cdec toolkit.6AcknowledgementsThis work is supported in part by the National Sci-ence Foundation under grant IIS-0915327, by theQatar National Research Fund (a member of theQatar Foundation) under grant NPRP 09-1140-1-177, and by the NSF-sponsored XSEDE programunder grant TG-CCR110017.6http://www.cs.cmu.edu/?mdenkows/cdec-realtime.html402References[Callison-Burch et al.2012] Chris Callison-Burch,Philipp Koehn, Christof Monz, Matt Post, RaduSoricut, and Lucia Specia.
2012.
Findings of the2012 workshop on statistical machine translation.In Proceedings of the Seventh Workshop on Statis-tical Machine Translation, pages 10?51, Montr?eal,Canada, June.
Association for ComputationalLinguistics.
[Carl et al.2011] Michael Carl, Barbara Dragsted,Jakob Elming, Daniel Hardt, and Arnt LykkeJakobsen.
2011.
The process of post-editing: Apilot study.
Copenhagen Studies in Language,41:131?142.
[Cettolo et al.2012] Mauro Cettolo, Christian Girardi,and Marcello Federico.
2012.
Wit3: Web inventoryof transcribed and translated talks.
In Proceedingsof the Sixteenth Annual Conference of the EuropeanAssociation for Machine Translation.
[Chiang2007] David Chiang.
2007.
Hierarchicalphrase-based translation.
Computational Linguis-tics, 33.
[Chiang2012] David Chiang.
2012.
Hope and fear fordiscriminative training of statistical translation mod-els.
Journal of Machine Learning Research, pages1159?1187, April.
[Clark et al.2011] Jonathan H. Clark, Chris Dyer, AlonLavie, and Noah A. Smith.
2011.
Better hypothe-sis testing for statistical machine translation: Con-trolling for optimizer instability.
In Proceedings ofthe 49th Annual Meeting of the Association for Com-putational Linguistics: Human Language Technolo-gies, pages 176?181, Portland, Oregon, USA, June.Association for Computational Linguistics.
[Crammer et al.2006] Koby Crammer, Ofer Dekel,Joseph Keshet, Shai Shalev-Shwartz, and YoramSinger.
2006.
Online passive-aggressive algo-rithms.
Journal of Machine Learning Research,pages 551?558, March.
[Dyer et al.2010] Chris Dyer, Adam Lopez, Juri Gan-itkevitch, Jonathan Weese, Ferhan Ture, Phil Blun-som, Hendra Setiawan, Vladimir Eidelman, andPhilip Resnik.
2010. cdec: A decoder, alignment,and learning framework for finite-state and context-free translation models.
In Proceedings of the ACL2010 System Demonstrations, pages 7?12, Uppsala,Sweden, July.
Association for Computational Lin-guistics.
[Dyer et al.2013] Chris Dyer, Victor Chahuneau, andNoah A. Smith.
2013.
A simple, fast, and effectivereparameterization of IBM model 2.
In The 2013Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies.
[Eidelman2012] Vladimir Eidelman.
2012.
Optimiza-tion strategies for online large-margin learning inmachine translation.
In Proceedings of the SeventhWorkshop on Statistical Machine Translation, pages480?489, Montr?eal, Canada, June.
Association forComputational Linguistics.
[Guerberof2009] Ana Guerberof.
2009.
Productivityand quality in mt post-editing.
In Proceedings of MTSummit XII - Workshop: Beyond Translation Memo-ries: New Tools for Translators MT.
[Hardt and Elming2010] Daniel Hardt and Jakob Elm-ing.
2010.
Incremental re-training for post-editingsmt.
In Proceedings of the Ninth Conference of theAssociation for Machine Translation in the Ameri-cas.
[Heafield et al.2013] Kenneth Heafield, IvanPouzyrevsky, Jonathan H. Clark, and PhilippKoehn.
2013.
Scalable modified Kneser-Neylanguage model estimation.
In Proceedings ofthe 51st Annual Meeting of the Association forComputational Linguistics, Sofia, Bulgaria, August.
[Hopkins and May2011] Mark Hopkins and JonathanMay.
2011.
Tuning as ranking.
In Proceedings ofthe 2011 Conference on Empirical Methods in Nat-ural Language Processing, pages 1352?1362, Edin-burgh, Scotland, UK., July.
Association for Compu-tational Linguistics.
[Koehn2012] Philipp Koehn.
2012.
Computer-aidedtranslation.
Machine Translation Marathon.
[Kuhn and de Mori1990] Roland Kuhn and Renatode Mori.
1990.
A cache-based natural languagemodel for speech recognition.
IEEE Transac-tions on Pattern Analysis and Machine Intelligence,12(6).
[Levenberg et al.2010] Abby Levenberg, ChrisCallison-Burch, and Miles Osborne.
2010.Stream-based translation models for statisticalmachine translation.
In Human Language Tech-nologies: The 2010 Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics, pages 394?402, Los Angeles,California, June.
Association for ComputationalLinguistics.
[Lopez2008a] Adam Lopez.
2008a.
Machine transla-tion by pattern matching.
In Dissertation, Univer-sity of Maryland, March.
[Lopez2008b] Adam Lopez.
2008b.
Tera-scale transla-tion models via pattern matching.
In Proceedingsof the 22nd International Conference on Compu-tational Linguistics (Coling 2008), pages 505?512,Manchester, UK, August.
Coling 2008 OrganizingCommittee.
[L?opez-Salcedo et al.2012] Francisco-Javier L?opez-Salcedo, Germ?an Sanchis-Trilles, and FranciscoCasacuberta.
2012.
Online learning of log-linearweights in interactive machine translation.
Ad-vances in Speech and Language Technologies forIberian Languages, pages 277?286.403[Macherey et al.2008] Wolfgang Macherey, Franz Och,Ignacio Thayer, and Jakob Uszkoreit.
2008.
Lattice-based minimum error rate training for statistical ma-chine translation.
In Proceedings of the 2008 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 725?734, Honolulu, Hawaii, Oc-tober.
Association for Computational Linguistics.
[Manber and Myers1993] Udi Manber and Gene My-ers.
1993.
Suffix arrays: A new method for on-line string searches.
SIAM Journal of Computing,22:935?948.[Mart?
?nez-G?omez et al.2012] Pascual Mart?
?nez-G?omez, Germ?an Sanchis-Trilles, and FranciscoCasacuberta.
2012.
Online adaptation strategiesfor statistical machine translation in post-editingscenarios.
Pattern Recognition, 45:3193?3203.
[Nepveu et al.2004] Laurent Nepveu, Guy Lapalme,Philippe Langlais, and George Foster.
2004.
Adap-tive language and translation models for interactivemachine translation.
In Dekang Lin and Dekai Wu,editors, Proceedings of EMNLP 2004, pages 190?197, Barcelona, Spain, July.
Association for Com-putational Linguistics.
[NISHIDA et al.1988] Fujio NISHIDA, ShinobuTAKAMATSU, Tadaaki TANI, and TsunehisaDOI.
1988.
Feedback of correcting informationin postediting to a machine translation system.
InProc.
of COLING.
[Och2003] Franz Josef Och.
2003.
Minimum error ratetraining in statistical machine translation.
In Pro-ceedings of the 41st Annual Meeting of the Associa-tion for Computational Linguistics, pages 160?167,Sapporo, Japan, July.
Association for ComputationalLinguistics.[Ortiz-Mart?
?nez et al.2010] Daniel Ortiz-Mart?
?nez, Is-mael Garc?
?a-Varea, and Francisco Casacuberta.2010.
Online learning for interactive statistical ma-chine translation.
In Human Language Technolo-gies: The 2010 Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics, pages 546?554, Los Ange-les, California, June.
Association for ComputationalLinguistics.
[Papineni et al.2002] Kishore Papineni, Salim Roukos,Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: amethod for automatic evaluation of machine trans-lation.
In Proceedings of 40th Annual Meetingof the Association for Computational Linguistics,pages 311?318, Philadelphia, Pennsylvania, USA,July.
Association for Computational Linguistics.
[Parker et al.2011] Robert Parker, David Graff, JunboKong, Ke Chen, and Kazuaki Maeda.
2011.
En-glish Gigaword Fifth Edition, June.
Linguistic DataConsortium, LDC2011T07.
[Przybocki2012] Mark Przybocki.
2012.
Nist openmachine translation 2012 evaluation (openmt12).http://www.nist.gov/itl/iad/mig/openmt12.cfm.
[Saluja et al.2012] Avneesh Saluja, Ian Lane, and YingZhang.
2012.
Machine translation with binary feed-back: a large-margin approach.
In Proceedings ofthe Tenth Biennial Conference of the Association forMachine Translation in the Americas.
[Sanchis-Trilles2012] Germ?an Sanchis-Trilles.
2012.Building task-oriented machine translation systems.In Ph.D. Thesis, Universitat Politcnica de Valncia.
[Simard and Foster2013] Michel Simard and GeorgeFoster.
2013.
PEPr: Post-edit propagation usingphrase-based statistical machine translation.
In Pro-ceedings of the XIV Machine Translation Summit,pages 191?198,, September.
[Tatsumi2009] Midori Tatsumi.
2009.
Correlationbetween automatic evaluation metric scores, post-editing speed, and some other factors.
In Proceed-ings of the Twelfth Machine Translation Summit.
[Teh2006] Yee Whye Teh.
2006.
A hierarchicalBayesian language model based on Pitman-Yor pro-cesses.
In Proc.
of ACL.
[Zhao et al.2004] Bing Zhao, Matthias Eck, andStephan Vogel.
2004.
Language model adaptationfor statistical machine translation with structuredquery models.
In Proc.
of COLING.
[Zhechev2012] Ventsislav Zhechev.
2012.
MachineTranslation Infrastructure and Post-editing Perfor-mance at Autodesk.
In AMTA 2012 Workshopon Post-Editing Technology and Practice (WPTP2012), pages 87?96, San Diego, USA, October.
As-sociation for Machine Translation in the Americas(AMTA).404
