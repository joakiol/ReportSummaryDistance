Proceedings of the ACL-HLT 2011 Student Session, pages 81?87,Portland, OR, USA 19-24 June 2011. c?2011 Association for Computational LinguisticsSentiment Analysis of Citations using Sentence Structure-Based FeaturesAwais AtharUniversity of CambridgeComputer Laboratory15 JJ Thompson AvenueCambridge, CB3 0FD, U.K.awais.athar@cl.cam.ac.ukAbstractSentiment analysis of citations in scientific pa-pers and articles is a new and interesting prob-lem due to the many linguistic differences be-tween scientific texts and other genres.
Inthis paper, we focus on the problem of auto-matic identification of positive and negativesentiment polarity in citations to scientific pa-pers.
Using a newly constructed annotated ci-tation sentiment corpus, we explore the effec-tiveness of existing and novel features, includ-ing n-grams, specialised science-specific lex-ical features, dependency relations, sentencesplitting and negation features.
Our resultsshow that 3-grams and dependencies performbest in this task; they outperform the sentencesplitting, science lexicon and negation basedfeatures.1 IntroductionSentiment analysis is the task of identifying positiveand negative opinions, sentiments, emotions and at-titudes expressed in text.
Although there has been inthe past few years a growing interest in this field fordifferent text genres such as newspaper text, reviewsand narrative text, relatively less emphasis has beenplaced on extraction of opinions from scientific liter-ature, more specifically, citations.
Analysis of cita-tion sentiment would open up many exciting new ap-plications in bibliographic search and in bibliomet-rics, i.e., the automatic evaluation the influence andimpact of individuals and journals via citations.Existing bibliometric measures like H-Index(Hirsch, 2005) and adapted graph ranking algo-rithms like PageRank (Radev et al, 2009) treat all ci-tations as equal.
However, Bonzi (1982) argued thatif a cited work is criticised, it should consequentlycarry lower or even negative weight for bibliometricmeasures.
Automatic citation sentiment detection isa prerequisite for such a treatment.Moreover, citation sentiment detection can alsohelp researchers during search, by detecting prob-lems with a particular approach.
It can be used asa first step to scientific summarisation, enable usersto recognise unaddressed issues and possible gapsin the current research, and thus help them set theirresearch directions.For other genres a rich literature on sentiment de-tection exists and researchers have used a numberof features such as n-grams, presence of adjectives,adverbs and other parts-of-speech (POS), negation,grammatical and dependency relations as well asspecialised lexicons in order to detect sentimentsfrom phrases, words, sentences and documents.State-of-the-art systems report around 85-90% ac-curacy for different genres of text (Nakagawa et al,2010; Yessenalina et al, 2010; Ta?ckstro?m and Mc-Donald, 2011).Given such good results, one might think that asentence-based sentiment detection system trainedon a different genre could be used equally well toclassify citations.
We argue that this might not bethe case; our citation sentiment recogniser uses spe-cialised training data and tests the performance ofspecialised features against current state-of-the-artfeatures.
The reasons for this are based on the fol-lowing observations:?
Sentiment in citations is often hidden.
This might81be because of the general strategy to avoid overtcriticism due to the sociological aspect of cit-ing (MacRoberts and MacRoberts, 1984; Thomp-son and Yiyun, 1991).
Ziman (1968) states thatmany works are cited out of ?politeness, policy orpiety?.
Negative sentiment, while still present anddetectable for humans, is expressed in subtle waysand might be hedged, especially when it cannot bequantitatively justified (Hyland, 1995).While SCL has been successfully applied to POS tag-ging and Sentiment Analysis (Blitzer et al, 2006), itseffectiveness for parsing was rather unexplored.?
Citation sentences are often neutral with respectto sentiment, either because they describe an al-gorithm, approach or methodology objectively, orbecause they are used to support a fact or state-ment.There are five different IBM translation models (Brownet al , 1993).This gives rise to a far higher proportion of objec-tive sentences than in other genres.?
Negative polarity is often expressed in contrastiveterms, e.g.
in evaluation sections.
Although thesentiment is indirect in these cases, its negativityis implied by the fact that the authors?
own workis clearly evaluated positively in comparison.This method was shown to outperform the class basedmodel proposed in (Brown et al, 1992) .
.
.?
There is also much variation between scientifictexts and other genres concerning the lexicalitems chosen to convey sentiment.
Sentiment car-rying science-specific terms exist and are rela-tively frequent, which motivates the use of a sen-timent lexicon specialised to science.Similarity-based smoothing (Dagan, Lee, and Pereira1999) provides an intuitively appealing approach tolanguage modeling.?
Technical terms play a large role overall in scien-tific text (Justeson and Katz, 1995).
Some of thesecarry sentiment as well.Current state of the art machine translation systems(Och, 2003) use phrasal (n-gram) features .
.
.For this reason, using higher order n-grams mightprove to be useful in sentiment detection.?
The scope of influence of citations varies widelyfrom a single clause (as in the example below) toseveral paragraphs:As reported in Table 3, small increases in METEOR(Banerjee and Lavie, 2005), BLEU (Papineni et al,2002) and NIST scores (Doddington, 2002) suggestthat .
.
.This affects lexical features directly since therecould be ?sentiment overlap?
associated withneighbouring citations.
Ritchie et al (2008)showed that assuming larger citation scopes hasa positive effect in retrieval.
We will test the op-posite direction here, i.e., we assume short scopesand use a parser to split sentences, so that the fea-tures associated with the clauses not directly con-nected to the citation are disregarded.We created a new sentiment-annotated corpus ofscientific text in the form of a sentence-based col-lection of over 8700 citations.
Our experimentsuse a supervised classifier with the state-of-the-artfeatures from the literature, as well as new fea-tures based on the observations above.
Our resultsshow that the most successful feature combinationincludes dependency features and n-grams longerthan for other genres (n = 3), but the assumptionof a smaller scope (sentence splitting) decreased re-sults.2 Training and Test CorpusWe manually annotated 8736 citations from 310 re-search papers taken from the ACL Anthology (Birdet al, 2008).
The citation summary data from theACL Anthology Network1 (Radev et al, 2009) wasused.
We identified the actual text of the citationsby regular expressions and replaced it with a specialtoken <CIT> in order to remove any lexical biasassociated with proper names of researchers.
We la-belled each sentence as positive, negative or objec-tive, and separated 1472 citations for developmentand training.
The rest were used as the test set con-taining 244 negative, 743 positive and 6277 objec-tive citations.
Thus our dataset is heavily skewed,with subjective citations accounting for only around14% of the corpus.1http://www.aclweb.org823 FeaturesWe represent each citation as a feature set in a Sup-port Vector Machine (SVM) (Cortes and Vapnik,1995) framework which has been shown to producegood results for sentiment classification (Pang etal., 2002).
The corpus is processed using WEKA(Hall et al, 2008) and the Weka LibSVM library(EL-Manzalawy and Honavar, 2005; Chang and Lin,2001) with the following features.3.1 Word Level FeaturesIn accordance with Pang et al (2002), we use uni-grams and bigrams as features and also add 3-gramsas new features to capture longer technical terms.POS tags are also included using two approaches:attaching the tag to the word by a delimiter, and ap-pending all tags at the end of the sentence.
This mayhelp in distinguishing between homonyms with dif-ferent POS tags and signalling the presence of ad-jectives (e.g., JJ) respectively.
Name of the primaryauthor of the cited paper is also used as a feature.A science-specific sentiment lexicon is also addedto the feature set.
This lexicon consists of 83 polarphrases which have been manually extracted fromthe development set of 736 citations.
Some of themost frequently occurring polar phrases in this setconsists of adjectives such as efficient, popular, suc-cessful, state-of-the-art and effective.3.2 Contextual Polarity FeaturesFeatures previously found to be useful for detect-ing phrase-level contextual polarity (Wilson et al,2009) are also included.
Since the task at hand issentence-based, we use only the sentence-based fea-tures from the literature e.g., presence of subjectiv-ity clues which have been compiled from severalsources2 along with the number of adjectives, ad-verbs, pronouns, modals and cardinals.To handle negation, we include the count of nega-tion phrases found within the citation sentence.
Sim-ilarly, the number of valance shifters (Polanyi andZaenen, 2006) in the sentence are also used.
Thepolarity shifter and negation phrase lists have beentaken from the OpinionFinder system (Wilson et al,2005).2Available for download at http://www.cs.pitt.edu/mpqa/3.3 Sentence Structure Based FeaturesWe explore three different feature sets which focuson the lexical and grammatical structure of a sen-tence and have not been explored previously for thetask of sentiment analysis of scientific text.3.3.1 Dependency StructuresThe first set of these features include typed depen-dency structures (de Marneffe and Manning, 2008)which describe the grammatical relationships be-tween words.
We aim to capture the long distancerelationships between words.
For instance in thesentence below, the relationship between results andcompetitive will be missed by trigrams but the de-pendency representation captures it in a single fea-ture nsubj competitive results.<CIT> showed that the results for French-Englishwere competitive to state-of-the-art alignment systems.A variation we experimented with, but gave upon as it did not show any improvements, concernsbacking-off the dependent and governor to their POStags (Joshi and Penstein-Rose?, 2009).3.3.2 Sentence SplittingRemoving irrelevant polar phrases around a ci-tation might improve results.
For this purpose, wesplit each sentence by trimming its parse tree.
Walk-ing from the citation node (<CIT>) towards theroot, we select the subtree rooted at the first sentencenode (S) and ignore the rest.
For example, in Figure1, the cited paper is not included in the scope of thediscarded polar phrase significant improvements.Figure 1: An example of parse tree trimming833.3.3 NegationDependencies and parse trees attach negationnodes, such as not, to the clause subtree and thisshows no interaction with other nodes with respectto valence shifting.
To handle this effect, we takea simple window-based inversion approach.
Allwords inside a k-word window of any negation termare suffixed with a token neg to distinguish themfrom their non-polar versions.
For example, a 2-word negation window inverts the polarity of thepositive phrase work well in the sentence below.Turney?s method did not work neg well neg althoughthey reported 80% accuracy in <CIT>.The negation term list has been taken from theOpinionFinder system.
Khan (2007) has shown thatthis approach produces results comparable to gram-matical relations based negation models.4 ResultsBecause of our skewed dataset, we report boththe macro-F and the micro-F scores using 10-foldcross-validation (Lewis, 1991).
The bold values inTable 1 show the best results.Features macro-F micro-F1 grams 0.581 0.8631-2 grams 0.592 0.8641-3 grams 0.597 0.862??
+ POS 0.535 0.859??
+ POS (tokenised) 0.596 0.859??
+ scilex 0.597 0.860??
+ wlev 0.535 0.859??
+ cpol 0.418 0.859??
+ dep 0.760 0.897??
+ dep + split + neg 0.683 0.872??
+ dep + split 0.642 0.866??
+ dep + neg 0.764 0.898Table 1: Results using science lexicon (scilex), contex-tual polarity (cpol), dependencies (dep), negation (neg),sentence splitting (split) and word-level (wlev) features.The selection of the features is on the basis of im-provements over a baseline of 1-3 grams i.e.
if afeature (e.g.
scilex) did not shown any improvement,it is has been excluded from the subsequent experi-ments.The results show that contextual polarity featuresdo not work well on citation text.
Adding a science-specific lexicon does not help either.
This may indi-cate that n-grams are sufficient to capture discrim-inating lexical structures.
We find that word leveland contextual polarity features are surpassed by de-pendency features.
Sentence splitting does not help,possibly due to longer citation scope.
Adding anegation window (k=15) improves the performancebut the improvement was not found to be statisticallysignificant.
This might be due to skewed class dis-tribution and a larger dataset may prove to be useful.5 Related WorkWhile different schemes have been proposed forannotating citations according to their function(Spiegel-Ro?sing, 1977; Nanba and Okumura, 1999;Garzone and Mercer, 2000), there have been no at-tempts on citation sentiment detection in a large cor-pus.Teufel et al (2006) worked on a 2829 sentence ci-tation corpus using a 12-class classification scheme.However, this corpus has been annotated for the taskof determining the author?s reason for citing a givenpaper and is thus built on top of sentiment of cita-tion.
It considers usage, modification and similar-ity with a cited paper as positive even when there isno sentiment attributed to it.
Moreover, contrast be-tween two cited methods (CoCoXY) is categorizedas objective in the annotation scheme even if the textindicates that one method performs better than theother.
For example, the sentence below talks abouta positive attribute but is marked as neutral in thescheme.Lexical transducers are more efficient for analysis andgeneration than the classical two-level systems (Kosken-niemi,1983) because .
.
.Using this corpus is thus more likely to lead toinconsistent representation of sentiment in any sys-tem which relies on lexical features.
Teufel et al(2006) group the 12 categories into 3 in an at-tempt to perform a rough approximation of senti-ment analysis over the classifications and report a0.710 macro-F score.
Unfortunately, we have ac-84cess to only a subset3 of this citation function cor-pus.
We have extracted 1-3 grams, dependencies andnegation features from the reduced citation functiondataset and used them in our system with 10-foldcross-validation.
This results in an improved macro-F score of 0.797 for the subset.
This shows thatour system is comparable to Teufel et al (2006).When this subset is used to test the system trained onour newly annotated corpus, a low macro-F score of0.484 is achieved.
This indicates that there is a mis-match in the annotated class labels.
Therefore, wecan infer that citation sentiment classification is dif-ferent from citation function classification.Other approaches to citation annotation and clas-sification include Wilbur et al (2006) who annotateda small 101 sentence corpus on focus, polarity, cer-tainty, evidence and directionality.
Piao et al (2007)proposed a system to attach sentiment informationto the citation links between biomedical papers.Different dependency relations have been ex-plored by Dave et al (2003), Wilson et al (2004)and Ng et al (2006) for sentiment detection.
Nak-agawa et al (2010) report that using dependencieson conditional random fields with lexicon based po-larity reversal results in improvements over n-gramsfor news and reviews corpora.A common approach is to use a sentiment la-belled lexicon to score sentences (Hatzivassiloglouand McKeown, 1997; Turney, 2002; Yu and Hatzi-vassiloglou, 2003).
Research suggests that creatinga general sentiment classifier is a difficult task andexisting approaches are highly topic dependent (En-gstro?m, 2004; Gamon and Aue, 2005; Blitzer et al,2007).6 ConclusionIn this paper, we focus on automatic identificationof sentiment polarity in citations.
Using a newlyconstructed annotated citation sentiment corpus, weexamine the effectiveness of existing and novel fea-tures, including n-grams, scientific lexicon, depen-dency relations and sentence splitting.
Our resultsshow that 3-grams and dependencies perform bestin this task; they outperform the scientific lexiconand the sentence splitting features.
Future direc-3This subset contains 591 positive, 59 negative and 1259objective citations.tions include trying to improve the performance bymodelling negations using a more sophisticated ap-proach.
New techniques for detection of the nega-tion scope such as the one proposed by Councill etal.
(2010) might also be helpful in citations.
Explor-ing longer citation scopes by including citation con-texts might also improve citation sentiment detec-tion.ReferencesS.
Bird, R. Dale, B.J.
Dorr, B. Gibson, M.T.
Joseph,M.Y.
Kan, D. Lee, B. Powley, D.R.
Radev, and Y.F.Tan.
2008.
The acl anthology reference corpus: Areference dataset for bibliographic research in compu-tational linguistics.
In Proc.
of the 6th InternationalConference on Language Resources and EvaluationConference (LREC08), pages 1755?1759.
Citeseer.J.
Blitzer, M. Dredze, and F. Pereira.
2007.
Biographies,bollywood, boom-boxes and blenders: Domain adap-tation for sentiment classification.
In ACL, volume 45,page 440.S.
Bonzi.
1982.
Characteristics of a literature as pre-dictors of relatedness between cited and citing works.Journal of the American Society for Information Sci-ence, 33(4):208?216.C.C.
Chang and C.J.
Lin.
2001.
LIBSVM: a li-brary for support vector machines, 2001.
Softwareavailable at http://www.csie.ntu.edu.tw/cjlin/libsvm.C.
Cortes and V. Vapnik.
1995.
Support-vector networks.Machine learning, 20(3):273?297.I.G.
Councill, R. McDonald, and L. Velikovich.
2010.What?s great and what?s not: learning to classify thescope of negation for improved sentiment analysis.
InProceedings of the Workshop on Negation and Specu-lation in Natural Language Processing, pages 51?59.Association for Computational Linguistics.K.
Dave, S. Lawrence, and D.M.
Pennock.
2003.
Miningthe peanut gallery: Opinion extraction and semanticclassification of product reviews.
In Proceedings ofthe 12th international conference on World Wide Web,pages 519?528.
ACM.M.C.
de Marneffe and C.D.
Manning.
2008.
The Stan-ford typed dependencies representation.
In COLING,pages 1?8.
Association for Computational Linguistics.Y.
EL-Manzalawy and V. Honavar, 2005.
WLSVM:Integrating LibSVM into Weka Environment.
Soft-ware available at http://www.cs.iastate.edu/?yasser/wlsvm.C.
Engstro?m.
2004.
Topic dependence in sentiment clas-sification.
Unpublished MPhil Dissertation.
Univer-sity of Cambridge.85M.
Gamon and A. Aue.
2005.
Automatic identificationof sentiment vocabulary: exploiting low associationwith known sentiment terms.
In Proceedings of theACL Workshop on Feature Engineering for MachineLearning in Natural Language Processing, pages 57?64.
Association for Computational Linguistics.M.
Garzone and R. Mercer.
2000.
Towards an automatedcitation classifier.
Advances in Artificial Intelligence,pages 337?346.D.
Hall, D. Jurafsky, and C.D.
Manning.
2008.
Studyingthe history of ideas using topic models.
In EMNLP,pages 363?371.V.
Hatzivassiloglou and K.R.
McKeown.
1997.
Predict-ing the semantic orientation of adjectives.
In Proceed-ings of EACL, pages 174?181.
Association for Com-putational Linguistics.J.E.
Hirsch.
2005.
An index to quantify an individual?sscientific research output.
Proceedings of the NationalAcademy of Sciences of the United States of America,102(46):16569.K.
Hyland.
1995.
The Author in the Text: Hedging Sci-entific Writing.
Hong Kong papers in linguistics andlanguage teaching, 18:11.M.
Joshi and C. Penstein-Rose?.
2009.
Generalizing de-pendency features for opinion mining.
In Proceed-ings of the ACL-IJCNLP 2009 Conference Short Pa-pers, pages 313?316.
Association for ComputationalLinguistics.J.S.
Justeson and S.M.
Katz.
1995.
Technical terminol-ogy: some linguistic properties and an algorithm foridentification in text.
Natural language engineering,1(01):9?27.S.
Khan.
2007.
Negation and Antonymy in SentimentClassification.
Ph.D. thesis, Computer Lab, Univer-sity of Cambridge.D.D.
Lewis.
1991.
Evaluating text categorization.
InProceedings of Speech and Natural Language Work-shop, pages 312?318.M.H.
MacRoberts and B.R.
MacRoberts.
1984.
Thenegational reference: Or the art of dissembling.
So-cial Studies of Science, 14(1):91?94.T.
Nakagawa, K. Inui, and S. Kurohashi.
2010.
Depen-dency tree-based sentiment classification using CRFswith hidden variables.
In NAACL HLT, pages 786?794.
Association for Computational Linguistics.H.
Nanba and M. Okumura.
1999.
Towards multi-papersummarization using reference information.
In IJCAI,volume 16, pages 926?931.
Citeseer.V.
Ng, S. Dasgupta, and SM Arifin.
2006.
Examiningthe role of linguistic knowledge sources in the auto-matic identification and classification of reviews.
InProceedings of the COLING/ACL on Main conferenceposter sessions, pages 611?618.
Association for Com-putational Linguistics.B.
Pang, L. Lee, and S. Vaithyanathan.
2002.
Thumbsup?
: sentiment classification using machine learningtechniques.
In EMNLP, pages 79?86.
Association forComputational Linguistics.S.
Piao, S. Ananiadou, Y. Tsuruoka, Y. Sasaki, and J. Mc-Naught.
2007.
Mining opinion polarity relations of ci-tations.
In International Workshop on ComputationalSemantics (IWCS), pages 366?371.
Citeseer.L.
Polanyi and A. Zaenen.
2006.
Contextual valenceshifters.
Computing attitude and affect in text: Theoryand applications, pages 1?10.D.R.
Radev, M.T.
Joseph, B. Gibson, and P. Muthukrish-nan.
2009.
A Bibliometric and Network Analysis ofthe field of Computational Linguistics.
Journal of theAmerican Society for Information Science and Tech-nology, 1001:48109?1092.A.
Ritchie, S. Robertson, and S. Teufel.
2008.
Compar-ing citation contexts for information retrieval.
In Pro-ceeding of the 17th ACM Conference on Informationand Knowledge Management, pages 213?222.
ACM.I.
Spiegel-Ro?sing.
1977.
Science studies: Bibliomet-ric and content analysis.
Social Studies of Science,7(1):97?113.O.
Ta?ckstro?m and R. McDonald.
2011.
Discoveringfine-grained sentiment with latent variable structuredprediction models.
In Proceedings of the ECIR.S.
Teufel, A. Siddharthan, and D. Tidhar.
2006.
Auto-matic classification of citation function.
In EMNLP,pages 103?110.
Association for Computational Lin-guistics.G.
Thompson and Y. Yiyun.
1991.
Evaluation in thereporting verbs used in academic papers.
Applied lin-guistics, 12(4):365.P.D.
Turney.
2002.
Thumbs up or thumbs down?
: seman-tic orientation applied to unsupervised classification ofreviews.
In Proceedings of the 40th Annual Meetingon Association for Computational Linguistics, pages417?424.
Association for Computational Linguistics.W.J.
Wilbur, A. Rzhetsky, and H. Shatkay.
2006.
Newdirections in biomedical text annotation: definitions,guidelines and corpus construction.
BMC bioinfor-matics, 7(1):356.T.
Wilson, J. Wiebe, and R. Hwa.
2004.
Just how madare you?
Finding strong and weak opinion clauses.
InProceedings of the National Conference on ArtificialIntelligence, pages 761?769.
Menlo Park, CA; Cam-bridge, MA; London; AAAI Press; MIT Press; 1999.T.
Wilson, J. Wiebe, and P. Hoffmann.
2005.
Rec-ognizing contextual polarity in phrase-level sentimentanalysis.
In EMNLP, pages 347?354.
Association forComputational Linguistics.T.
Wilson, J. Wiebe, and P. Hoffmann.
2009.
Recogniz-ing Contextual Polarity: an exploration of features for86phrase-level sentiment analysis.
Computational Lin-guistics, 35(3):399?433.A.
Yessenalina, Y. Yue, and C. Cardie.
2010.
Multi-level structured models for document-level sentimentclassification.
In Proceedings of EMNLP, pages 1046?1056, Cambridge, MA, October.
Association for Com-putational Linguistics.H.
Yu and V. Hatzivassiloglou.
2003.
Towards answer-ing opinion questions: Separating facts from opinionsand identifying the polarity of opinion sentences.
InProceedings of EMNLP, pages 129?136.
Associationfor Computational Linguistics.J.M.
Ziman.
1968.
Public Knowledge: An essay con-cerning the social dimension of science.
CambridgeUniv.
Press, College Station, Texas.87
