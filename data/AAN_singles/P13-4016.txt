Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 91?96,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsTravatar: A Forest-to-String Machine Translation Enginebased on Tree TransducersGraham NeubigGraduate School of Information ScienceNara Institute of Science and Technology8916-5 Takayama-cho, Ikoma-shi, Nara, Japanneubig@is.naist.jpAbstractIn this paper we describe Travatar, aforest-to-string machine translation (MT)engine based on tree transducers.
It pro-vides an open-source C++ implementationfor the entire forest-to-string MT pipeline,including rule extraction, tuning, decod-ing, and evaluation.
There are a numberof options for model training, and tuningincludes advanced options such as hyper-graph MERT, and training of sparse fea-tures through online learning.
The train-ing pipeline is modeled after that of thepopular Moses decoder, so users famil-iar with Moses should be able to getstarted quickly.
We perform a valida-tion experiment of the decoder on English-Japanese machine translation, and find thatit is possible to achieve greater accuracythan translation using phrase-based andhierarchical-phrase-based translation.
Asauxiliary results, we also compare differ-ent syntactic parsers and alignment tech-niques that we tested in the process of de-veloping the decoder.Travatar is available under the LGPL athttp://phontron.com/travatar1 IntroductionOne of the recent trends in statistical machinetranslation (SMT) is the popularity of models thatuse syntactic information to help solve problemsof long-distance reordering between the sourceand target language text.
These techniques canbe broadly divided into pre-ordering techniques,which first parse and reorder the source sentenceinto the target order before translating (Xia andMcCord, 2004; Isozaki et al 2010b), and tree-based decoding techniques, which take a tree orforest as input and choose the reordering andtranslation jointly (Yamada and Knight, 2001; Liuet al 2006; Mi et al 2008).
While pre-ordering isnot able to consider both translation and reorder-ing in a joint model, it is useful in that it is donebefore the actual translation process, so it can beperformed with a conventional translation pipelineusing a standard phrase-based decoder such asMoses (Koehn et al 2007).
For tree-to-string sys-tems, on the other hand, it is necessary to haveavailable or create a decoder that is equipped withthis functionality, which becomes a bottleneck inthe research and development process.In this demo paper, we describe Travatar, anopen-source tree-to-string or forest-to-string trans-lation system that can be used as a tool for transla-tion using source-side syntax, and as a platformfor research into syntax-based translation meth-ods.
In particular, compared to other decoderswhich mainly implement syntax-based translationin the synchronous context-free grammar (SCFG)framework (Chiang, 2007), Travatar is built uponthe tree transducer framework (Graehl and Knight,2004), a richer formalism that can help captureimportant distinctions between parse trees, as weshow in Section 2.
Travatar includes a fully docu-mented training and testing regimen that was mod-eled around that of Moses, making it possible forusers familiar with Moses to get started with Tra-vatar quickly.
The framework of the software isalso designed to be extensible, so the toolkit is ap-plicable for other tree-to-string transduction tasks.In the evaluation of the decoder on English-Japanese machine translation, we perform a com-parison to Moses?s phrase-based, hierarchical-phrase-based, and SCFG-based tree-to-string91Figure 1: Tree-to-string translation rules forSCFGs and tree transducers.translation.
Based on the results, we find that tree-to-string, and particularly forest-to-string, transla-tion using Travatar provides competitive or supe-rior accuracy to all of these techniques.
As aux-iliary results, we also compare different syntacticparsers and alignment techniques that we tested inthe process of developing the decoder.2 Tree-to-String Translation2.1 OverviewTree-to-string translation uses syntactic informa-tion to improve translation by first parsing thesource sentence, then using this source-side parsetree to decide the translation and reordering of theinput.
This method has several advantages, includ-ing efficiency of decoding, relatively easy han-dling of global reordering, and an intuitive repre-sentation of de-lexicalized rules that express gen-eral differences in order between the source andtarget languages.
Within tree-to-string translationthere are two major methodologies, synchronouscontext-free grammars (Chiang, 2007), and treetransducers (Graehl and Knight, 2004).An example of tree-to-string translation rulessupported by SCFGs and tree transducers is shownin Figure 1.
In this example, the first rule is asimple multi-word noun phrase, the second exam-ple is an example of a delexicalized rule express-ing translation from English SVO word order toJapanese SOV word order.
The third and fourthexamples are translations of a verb, noun phrase,and prepositional phrase, where the third rule hasthe preposition attatched to the verb, and the fourthhas the preposition attached to the noun.For the SCFGs, it can be seen that on the sourceside of the rule, there are placeholders correspond-ing to syntactic phrases, and on the target side ofthe rule there corresponding placeholders that donot have a syntactic label.
On the other hand in theexample of the translation rules using tree trans-ducers, it can be seen that similar rules can be ex-pressed, but the source rules are richer than simpleSCFG rules, also including the internal structureof the parse tree.
This internal structure is im-portant for achieving translation results faithful tothe input parse.
In particular, the third and fourthrules show an intuitive example in which this in-ternal structure can be important for translation.Here the full tree structures demonstrate importantdifferences in the attachment of the prepositionalphrase to the verb or noun.
While this is one ofthe most difficult and important problems in syn-tactic parsing, the source side in the SCFG is iden-tical, losing the ability to distinguish between thevery information that parsers are designed to dis-ambiguate.In traditional tree-to-string translation methods,the translator uses a single one-best parse tree out-put by a syntactic parser, but parse errors have thepotential to degrade the quality of translation.
Animportant advance in tree-to-string translation thathelps ameliorate this difficulity is forest-to-stringtranslation, which represents a large number ofpotential parses as a packed forest, allowing thetranslator to choose between these parses duringthe process of translation (Mi et al 2008).2.2 The State of Open Source SoftwareThere are a number of open-source software pack-ages that support tree-to-string translation in theSCFG framework.
For example, Moses (Koehn etal., 2007) and NiuTrans (Xiao et al 2012) sup-port the annotation of source-side syntactic labels,and taking parse trees (or in the case of NiuTrans,forests) as input.There are also a few other decoders that sup-port other varieties of using source-side syntaxto help improve translation or global reorder-ing.
For example, the cdec decoder (Dyer et al2010) supports the context-free-reordering/finite-state-translation framework described by Dyer andResnik (2010).
The Akamon decoder (Wu etal., 2012) supports translation using head-driven92phrase structure grammars as described by Wu etal.
(2010).However, to our knowledge, while there is ageneral-purpose tool for tree automata in general(May and Knight, 2006), there is no open-sourcetoolkit implementing the SMT pipeline in the treetransducer framework, despite it being a target ofactive research (Graehl and Knight, 2004; Liu etal., 2006; Huang et al 2006; Mi et al 2008).3 The Travatar Machine TranslationToolkitIn this section, we describe the overall frameworkof the Travatar decoder, following the order of thetraining pipeline.3.1 Data PreprocessingThis consists of parsing the source side sentenceand tokenizing the target side sentences.
Travatarcan decode input in the bracketed format of thePenn Treebank, or also in forest format.
There isdocumentation and scripts for using Travatar withseveral parsers for English, Chinese, and Japaneseincluded with the toolkit.3.2 TrainingOnce the data has been pre-processed, a tree-to-string model can be trained with the trainingpipeline included in the toolkit.
Like the train-ing pipeline for Moses, there is a single script thatperforms alignment, rule extraction, scoring, andparameter initialization.
Language model trainingcan be performed using a separate toolkit, and in-structions are provided in the documentation.For word alignment, the Travatar trainingpipeline is integrated with GIZA++ (Och and Ney,2003) by default, but can also use alignments fromany other aligner.Rule extraction is performed using the GHKMalgorithm (Galley et al 2006) and its extension torule extraction from forests (Mi and Huang, 2008).There are also a number of options implemented,including rule composition, attachment of null-aligned target words at either the highest point inthe tree, or at every possible position, and left andright binarization (Galley et al 2006; Wang et al2007).Rule scoring uses a standard set of forwardand backward conditional probabilities, lexical-ized translation probabilities, phrase frequency,and word and phrase counts.
Rule scores arestored as sparse vectors by default, which allowsfor scoring using an arbitrarily large number offeature functions.3.3 DecodingGiven a translation model Travatar is able to de-code parsed input sentences to generate transla-tions.
The decoding itself is performed using thebottom-up forest-to-string decoding algorithm ofMi et al(2008).
Beam-search implemented us-ing cube pruning (Chiang, 2007) is used to adjustthe trade-off between search speed and translationaccuracy.The source side of the translation model isstored using a space-efficient trie data structure(Yata, 2012) implemented using the marisa-trietoolkit.1 Rule lookup is performed using left-to-right depth-first search, which can be implementedas prefix lookup in the trie for efficient search.The language model storage uses the implemen-tation in KenLM (Heafield, 2011), and particu-larly the implementation that maintains left andright language model states for syntax-based MT(Heafield et al 2011).3.4 Tuning and EvaluationFor tuning the parameters of the model, Travatarnatively supports minimum error rate training(MERT) (Och, 2003) and is extension to hyper-graphs (Kumar et al 2009).
This tuning canbe performed for evaluation measures includingBLEU (Papineni et al 2002) and RIBES (Isozakiet al 2010a), with an easily extendable interfacethat makes it simple to support other measures.There is also a preliminary implementation ofonline learning methods such as the structured per-ceptron algorithm (Collins, 2002), and regularizedstructured SVMs trained using FOBOS (Duchiand Singer, 2009).
There are plans to implementmore algorithms such as MIRA or AROW (Chi-ang, 2012) in the near future.The Travatar toolkit also provides an evaluationprogram that can calculate the scores of transla-tion output according to various evaluation mea-sures, and calculate the significance of differ-ences between systems using bootstrap resampling(Koehn, 2004).1http://marisa-trie.googlecode.com934 Experiments4.1 Experimental SetupIn our experiments, we validated the performanceof the translation toolkit on English-Japanesetranslation of Wikipedia articles, as specified bythe Kyoto Free Translation Task (KFTT) (Neubig,2011).
Training used the 405k sentences of train-ing data of length under 60, tuning was performedon the development set, and testing was performedon the test set using the BLEU and RIBES mea-sures.
As baseline systems we use the Moses2 im-plementation of phrase-based (MOSES-PBMT), hi-erarchical phrase-based (MOSES-HIER), and tree-to-string translation (MOSES-T2S).
The phrase-based and hierarchical phrase-based models weretrained with the default settings according to tuto-rials on each web site.For all systems, we use a 5-gram Kneser-Neysmoothed language model.
Alignment for eachsystem was performed using either GIZA++3 orNile4 with main results reported for the alignerthat achieved the best accuracy on the dev set, anda further comparison shown in the auxiliary exper-iments in Section 4.3.
Tuning was performed withminimum error rate training to maximize BLEUover 200-best lists.
Tokenization was performedwith the Stanford tokenizer for English, and theKyTea word segmenter (Neubig et al 2011) forJapanese.For all tree-to-string systems we use Egret5 asan English parser, as we found it to achieve highaccuracy, and it allows for the simple output offorests.
Rule extraction was performed using one-best trees, which were right-binarized, and lower-cased post-parsing.
For Travatar, composed rulesof up to size 4 and a maximum of 2 non-terminalsand 7 terminals for each rule were used.
Null-aligned words were only attached to the top node,and no count normalization was performed, incontrast to Moses, which performs count normal-ization and exhaustive null word attachment.
De-coding was performed over either one-best trees(TRAV-T2S), or over forests including all edges in-cluded in the parser 200-best list (TRAV-F2S), anda pop limit of 1000 hypotheses was used for cube2http://statmt.org/moses/3http://code.google.com/p/giza-pp/4http://code.google.com/p/nile/ As Nile isa supervised aligner, we trained it on the alignments providedwith the KFTT.5http://code.google.com/p/egret-parser/BLEU RIBES Rules Sent/s.MOSES-PBMT 22.27 68.37 10.1M 5.69MOSES-HIER 22.04 70.29 34.2M 1.36MOSES-T2S 23.81 72.01 52.3M 1.71TRAV-T2S 23.15 72.32 9.57M 3.29TRAV-F2S 23.97 73.27 9.57M 1.11Table 1: Translation results (BLEU, RIBES), ruletable size, and speed in sentences per second foreach system.
Bold numbers indicate a statisticallysignificant difference over all other systems (boot-strap resampling with p > 0.05) (Koehn, 2004).pruning.4.2 System ComparisonThe comparison between the systems is shown inTable 1.
From these results we can see that thesystems utilizing source-side syntax significantlyoutperform the PBMT and Hiero, validating theusefulness of source side syntax on the English-to-Japanese task.
Comparing the two tree-to-stringsytems, we can see that TRAV-T2S has slightlyhigher RIBES and slightly lower BLEU thanMOSES-T2S.
One reason for the slightly higherBLEU of MOSES-T2S is because Moses?s rule ex-traction algorithm is more liberal in its attachmentof null-aligned words, resulting in a much largerrule table (52.3M rules vs. 9.57M rules) and mem-ory footprint.
In this setting, TRAV-T2S is approx-imately two times faster than MOSES-T2S.
Whenusing forest based decoding in TRAV-F2S, we seesignificant gains in accuracy over TRAV-T2S, withBLEU slightly and RIBES greatly exceeding thatof MOSES-T2S.4.3 Effect of Alignment/ParsingIn addition, as auxiliary results, we present a com-parison of Travatar?s tree-to-string and forest-to-string systems using different alignment methodsand syntactic parsers to examine the results ontranslation (Table 2).For parsers, we compared Egret with the Stan-ford parser.6 While we do not have labeled datato calculate parse accuracies with, Egret is a cloneof the Berkeley parser, which has been reported toachieve higher accuracy than the Stanford parseron several domains (Kummerfeld et al 2012).From the translation results, we can see that STAN-6http://nlp.stanford.edu/software/lex-parser.shtml94GIZA++ NileBLEU RIBES BLEU RIBESPBMT 22.28 68.37 22.37 68.43HIER 22.05 70.29 21.77 69.31STAN-T2S 21.47 70.94 22.44 72.02EGRET-T2S 22.82 71.90 23.15 72.32EGRET-F2S 23.35 71.77 23.97 73.27Table 2: Translation results (BLEU, RIBES), forseveral translation models (PBMT, Hiero, T2S,F2S), aligners (GIZA++, Nile), and parsers (Stan-ford, Egret).T2S significantly underperforms EGRET-T2S, con-firming that the effectiveness of the parser plays alarge effect on the translation accuracy.Next, we compared the unsupervised alignerGIZA++, with the supervised aligner Nile, whichuses syntactic information to improve alignmentaccuracy (Riesa and Marcu, 2010).
We held out10% of the hand aligned data provided with theKFTT, and found that GIZA++ achieves 58.32%alignment F-measure, while Nile achieves 64.22%F-measure.
With respect to translation accuracy,we found that for translation that does not use syn-tactic information, improvements in alignment donot necessarily increase translation accuracy, ashas been noted by Ganchev et al(2008).
How-ever, for all tree-to-string systems, the improvedalignments result in significant improvements inaccuracy, showing that alignments are, in fact, im-portant in our syntax-driven translation setup.5 Conclusion and Future DirectionsIn this paper, we introduced Travatar, an open-source toolkit for forest-to-string translation usingtree transducers.
We hope this decoder will beuseful to the research community as a test-bed forforest-to-string systems.
The software is alreadysufficiently mature to be used as is, as evidencedby the competitive, if not superior, results in ourEnglish-Japanese evaluation.We have a number of plans for future devel-opment.
First, we plan to support advanced ruleextraction techniques, such as fuller support forcount regularization and forest-based rule extrac-tion (Mi and Huang, 2008), and using the EMalgorithm to choose attachments for null-alignedwords (Galley et al 2006) or the direction of rulebinarization (Wang et al 2007).
We also planto incorporate advances in decoding to improvesearch speed (Huang and Mi, 2010).
In addition,there is a preliminary implementation of the abil-ity to introduce target-side syntactic information,either through hard constraints as in tree-to-treetranslation systems (Graehl and Knight, 2004), orthrough soft constraints, as in syntax-augmentedmachine translation (Zollmann and Venugopal,2006).
Finally, we will provide better support ofparallelization through the entire pipeline to in-crease the efficiency of training and decoding.Acknowledgements: We thank Kevin Duh and ananonymous reviewer for helpful comments.
Partof this work was supported by JSPS KAKENHIGrant Number 25730136.ReferencesDavid Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2).David Chiang.
2012.
Hope and fear for discriminativetraining of statistical translation models.
Journal ofMachine Learning Research, pages 1159?1187.Michael Collins.
2002.
Discriminative training meth-ods for hidden Markov models: Theory and experi-ments with perceptron algorithms.
In Proc.
EMNLP,pages 1?8.John Duchi and Yoram Singer.
2009.
Efficient onlineand batch learning using forward backward splitting.Journal of Machine Learning Research, 10:2899?2934.Chris Dyer and Philip Resnik.
2010.
Context-freereordering, finite-state translation.
In Proc.
HLT-NAACL.Chris Dyer, Adam Lopez, Juri Ganitkevitch, JonathanWeese, Ferhan Ture, Phil Blunsom, Hendra Seti-awan, Vladimir Eidelman, and Philip Resnik.
2010.cdec: A decoder, alignment, and learning frameworkfor finite-state and context-free translation models.In Proceedings of the ACL 2010 System Demonstra-tions, pages 7?12.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Proc.ACL, pages 961?968.Kuzman Ganchev, Joa?o V. Grac?a, and Ben Taskar.2008.
Better alignments = better translations?
InProc.
ACL.Jonathan Graehl and Kevin Knight.
2004.
Trainingtree transducers.
In Proc.
HLT, pages 105?112.Kenneth Heafield, Hieu Hoang, Philipp Koehn, TetsuoKiso, and Marcello Federico.
2011.
Left language95model state for syntactic machine translation.
InProc.
IWSLT.Kenneth Heafield.
2011.
Kenlm: Faster and smallerlanguage model queries.
In Proc.
WMT, pages 187?197.Liang Huang and Haitao Mi.
2010.
Efficient incre-mental decoding for tree-to-string translation.
InProc.
EMNLP, pages 273?283.Liang Huang, Kevin Knight, and Aravind Joshi.
2006.Statistical syntax-directed translation with extendeddomain of locality.
In Proceedings of AMTA, pages66?73.Hideki Isozaki, Tsutomu Hirao, Kevin Duh, KatsuhitoSudoh, and Hajime Tsukada.
2010a.
Automaticevaluation of translation quality for distant languagepairs.
In Proc.
EMNLP, pages 944?952.Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, andKevin Duh.
2010b.
Head finalization: A simplereordering rule for SOV languages.
In Proc.
WMTand MetricsMATR.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InProc.
ACL, pages 177?180, Prague, Czech Republic.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proc.
EMNLP.Shankar Kumar, Wolfgang Macherey, Chris Dyer,and Franz Och.
2009.
Efficient minimum errorrate training and minimum Bayes-risk decoding fortranslation hypergraphs and lattices.
In Proc.
ACL,pages 163?171.Jonathan K Kummerfeld, David Hall, James R Cur-ran, and Dan Klein.
2012.
Parser showdown at thewall street corral: an empirical investigation of er-ror types in parser output.
In Proc.
EMNLP, pages1048?1059.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machinetranslation.
In Proc.
ACL.Jonathan May and Kevin Knight.
2006.
Tiburon:A weighted tree automata toolkit.
In Implementa-tion and Application of Automata, pages 102?113.Springer.Haitao Mi and Liang Huang.
2008.
Forest-based trans-lation rule extraction.
In Proc.
EMNLP, pages 206?214.Haitao Mi, Liang Huang, and Qun Liu.
2008.
Forest-based translation.
In Proc.
ACL, pages 192?199.Graham Neubig, Yosuke Nakata, and Shinsuke Mori.2011.
Pointwise prediction for robust, adaptableJapanese morphological analysis.
In Proc.
ACL,pages 529?533, Portland, USA, June.Graham Neubig.
2011.
The Kyoto free translationtask.
http://www.phontron.com/kftt.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proc.
ACL.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proc.
ACL,pages 311?318, Philadelphia, USA.Jason Riesa and Daniel Marcu.
2010.
Hierarchicalsearch for word alignment.
In Proc.
ACL, pages157?166.Wei Wang, Kevin Knight, and Daniel Marcu.
2007.Binarizing syntax trees to improve syntax-based ma-chine translation accuracy.
In Proc.
EMNLP, pages746?754.Xianchao Wu, Takuya Matsuzaki, and Jun?ichi Tsujii.2010.
Fine-grained tree-to-string translation rule ex-traction.
In Proc.
ACL, pages 325?334.Xianchao Wu, Takuya Matsuzaki, and Jun?ichi Tsu-jii.
2012.
Akamon: An open source toolkit fortree/forest-based statistical machine translation.
InProceedings of the ACL 2012 System Demonstra-tions, pages 127?132.Fei Xia and Michael McCord.
2004.
Improvinga statistical MT system with automatically learnedrewrite patterns.
In Proc.
COLING.Tong Xiao, Jingbo Zhu, Hao Zhang, and Qiang Li.2012.
Niutrans: An open source toolkit for phrase-based and syntax-based machine translation.
In Pro-ceedings of the ACL 2012 System Demonstrations,pages 19?24.Kenji Yamada and Kevin Knight.
2001.
A syntax-based statistical translation model.
In Proc.
ACL.Susumu Yata.
2012.
Dictionary compression usingnested prefix/Patricia tries (in Japanese).
In Proc.17th NLP.Andreas Zollmann and Ashish Venugopal.
2006.
Syn-tax augmented machine translation via chart parsing.In Proc.
WMT.96
