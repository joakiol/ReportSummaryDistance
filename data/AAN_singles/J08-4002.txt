Hybrid Reinforcement/Supervised Learningof Dialogue Policies from Fixed Data SetsJames Henderson?University of GenevaOliver Lemon?
?University of EdinburghKallirroi Georgila?
?University of EdinburghWe propose a method for learning dialogue management policies from a fixed data set.
The methodaddresses the challenges posed by Information State Update (ISU)-based dialogue systems, whichrepresent the state of a dialogue as a large set of features, resulting in a very large state spaceand a huge policy space.
To address the problem that any fixed data set will only provideinformation about small portions of these state and policy spaces, we propose a hybrid model thatcombines reinforcement learning with supervised learning.
The reinforcement learning is usedto optimize a measure of dialogue reward, while the supervised learning is used to restrict thelearned policy to the portions of these spaces for which we have data.
We also use linear functionapproximation to address the need to generalize from a fixed amount of data to large state spaces.To demonstrate the effectiveness of this method on this challenging task, we trained this modelon the COMMUNICATOR corpus, to which we have added annotations for user actions and In-formation States.
When tested with a user simulation trained on a different part of the same dataset, our hybrid model outperforms a pure supervised learning model and a pure reinforcementlearning model.
It also outperforms the hand-crafted systems on the COMMUNICATOR data,according to automatic evaluation measures, improving over the average COMMUNICATORsystem policy by 10%.
The proposed method will improve techniques for bootstrapping andautomatic optimization of dialogue management policies from limited initial data sets.1.
IntroductionIn the practical development of dialogue systems it is often the case that an initialcorpus of task-oriented dialogues is collected, either using ?Wizard of Oz?
methodsor a prototype system deployment.
This data is usually used to motivate and inspirea new hand-built dialogue system or to modify an existing one.
However, given the?
Universite?
de Gene`ve, De?partement d?Informatique, Battelle-ba?timent A, 7 route de Drize, 1227 Carouge,Switzerland.
E-mail: james.henderson@cui.unige.ch.??
University of Edinburgh, 2 Buccleuch Place, Edinburgh EH8 9LW, UK.
E-mail: {olemon, kgeorgil}@inf.ed.ac.uk.Submission received: 18 November 2005; revised submission received: 18 October 2006; accepted forpublication: 21 September 2007.?
2008 Association for Computational LinguisticsComputational Linguistics Volume 34, Number 4existence of such data, it should be possible to exploit machine learning methods toautomatically build and optimize a new dialogue system.
This objective poses twoquestions: what machine learning methods are effective for this problem?
and howcan we encode the task in a way which is appropriate for these methods?
For thelatter challenge, we exploit the Information State Update (ISU) approach to dialoguesystems (Bohlin et al 1999; Larsson and Traum 2000), which provides the kind of richand flexible feature-based representations of context that are used with many recentmachine learning methods, including the linear function approximation method weuse here.
For the former challenge, we propose a novel hybrid method that combinesreinforcement learning (RL) with supervised learning (SL).The focus of this article is to establish effective methods for using fixed corporaof dialogues to automatically optimize complex dialogue systems.
To avoid the needfor extensive hand-crafting, we allow rich representations of context that include allthe features that might be relevant to dialogue management decisions, and we allowa broad set of dialogue management decisions with very few constraints on when adecision is applicable.
This flexibility simplifies system design, but it leads to a hugespace of possible dialogue management policies, which poses severe difficulties forexisting approaches to machine learning for dialogue systems (see Section 1.1).
Our pro-posed method addresses these difficulties without the use of user simulations, featureengineering, or further data collections.We demonstrate the effectiveness of the proposed method on the COMMUNICA-TOR corpora of flight-booking dialogues.
Our method (?hybrid learning?
with linearfunction approximation) can learn dialogue strategies that are better than those learnedby standard learning methods, and that are better than the (in this case hand-coded)strategies present in the original corpora, according to a variety of metrics.
To evaluatelearned strategies we run them with simulated users that are also trained on (differ-ent parts of) the COMMUNICATOR corpora, and automatically score the simulateddialogues based on how many information ?slots?
they manage to collect from users(?filled slots?
), whether those slots were confirmed (?confirmed slots?
), and how manydialogue turns were required to do so.
Later work has shown these metrics to correlatestrongly with task completion for real users of the different policies (Lemon, Georgila,and Henderson 2006).The main contributions of the work are therefore in empirically demonstrating that: limited initial data sets can be used to train complex dialogue policies,using a novel combination of supervised and reinforcement learning; and large, feature-based representations of dialogue context can be usedin tractable learning of dialogue policies, using linear functionapproximation.In this article, after a discussion of related work, we outline the annotations we haveadded to the COMMUNICATOR data, then present the proposed learning method, anddescribe our evaluation method.
Finally, we present the evaluation results and discusstheir implications.1.1 Related WorkAs in previous work on learning for dialogue systems, in this article we focus onlearning dialogue management policies.
Formally, a dialogue management policy is a488Henderson, Lemon, and Georgila Hybrid Reinforcement/Supervised Learningmapping from a dialogue context (a.k.a.
a state) to an action that the system shouldtake in that context.
Because most previous work on dialogue systems has been done inthe context of hand-crafted systems, we use representations of the dialogue context andthe action set based on previous work on hand-crafted dialogue systems.
Our mainnovel contribution is in the area of learning, where we build on previous work onautomatically learning dialogue management policies, discussed subsequently.The ISU approach to dialogue (Bohlin et al 1999; Larsson and Traum 2000) employsrich representations of dialogue context for flexible dialogue management.
InformationStates are feature structures intended to record all the information about the precedingportion of the dialogue that is relevant to making dialogue management decisions.
Anexample of some of the types of information recorded in our Information States is shownin Figure 1, including filled slots, confirmed slots, and previous speech acts.
Previouswork has raised the question of whether dialogue management policies can be learned(Levin and Pieraccini 1997) for systems that have only a limited view of the dialoguecontext, for example, not including prior speech act history (see the following).One prominent representation of the set of possible system actions is the DATEscheme (Walker and Passonneau 2001).
In particular, this representation is used in theCOMMUNICATOR corpus annotation (Walker, Passonneau, and Boland 2001), discussedherein.
The DATE scheme classifies system actions in terms of their ConversationalDomain, Speech Act, and Task.
For example, one possible system action is ?about task,Figure 1Example fields from an Information State annotation.
User-provided information is insquare brackets.489Computational Linguistics Volume 34, Number 4request info, dest city?, which corresponds to a system utterance such as What isyour destination city?
The specific instantiation of this scheme, and our extensions to it,are discussed in Section 1.2.Machine-learning approaches to dialogue management attempt to learn optimaldialogue policies from corpora of simulated or real dialogues, or by generating suchdata during automatic trial-and-error exploration of possible policies.
Automatic op-timization is desirable because of the high cost of developing and maintaining hand-coded dialogue managers, and because there is no guarantee that hand-coded dialoguemanagement strategies are good.
Several research groups have developed reinforce-ment learning approaches to dialogue management, starting with Levin and Pieraccini(1997) and Walker, Fromer, and Narayanan (1998).
Previous work has been restrictedto limited dialogue context representations and limited sets of actions to choose among(Walker, Fromer, and Narayanan 1998; Goddeau and Pineau 2000; Levin, Pieraccini, andEckert 2000; Roy, Pineau, and Thrun 2000; Scheffler and Young 2002; Singh et al 2002;Williams and Young 2005; Williams, Poupart, and Young 2005a).Much of the prior work in RL for dialogue management focuses on the problemof choosing among a particular limited set of actions (e.g., confirm, don?t confirm)in specific problematic states (see, e.g., Singh et al 2000a).
This approach augments,rather than replaces, hand-crafted dialogue systems, because the vast majority of deci-sions, which are not learned, need to be specified by hand.
In contrast, we tackle theproblem of learning to choose among any possible dialogue actions for almost everypossible state.In addition, all prior work has used only a limited representation of the dialoguecontext, often consisting only of the states of information slots (e.g., destination cityfilled with high confidence) in the application (Goddeau and Pineau 2000; Levin,Pieraccini, and Eckert 2000; Singh et al 2000a, 2000b, 2002; Young 2000; Scheffler andYoung 2002; Williams, Poupart, and Young 2005a, 2005b; Williams and Young 2005;Pietquin and Dutoit 2006b), with perhaps some additional low-level information (suchas acoustic features [Pietquin 2004]).
Only recently have researchers experimented withusing enriched representations of dialogue context (Gabsdil and Lemon 2004; Lemonet al 2005; Frampton and Lemon 2006; Rieser and Lemon 2006c), as we do in thisarticle.
From this work it is known that adding context features leads to better dialoguestrategies, compared to, for example, simply using the status of filled or confirmedinformation slots as has been studied in all prior work (Frampton and Lemon 2006).In this article we explore methods for scalable, tractable learning when using all theavailable context features.Reinforcement Learning requires estimating how good different actions will bein different dialogue contexts.
Because most previous work has only differentiatedbetween a small number of possible dialogue contexts, they have been able to per-form these estimates for each state independently (e.g., Singh et al 2002; Pietquin2004).
In contrast, we use function approximation to allow generalization to statesthat were not in the training data.
Function approximation was also applied to RL byDenecke, Dohsaka, and Nakano (2005), but they still use a relatively small state space(6 features, 972 possible states).
They also only exploit data for the 50 most frequentstates, using what is in effect a Gaussian kernel to compute estimates for the remainingstates from these 50 states.
This is a serious limitation to their method, because a largepercentage of the data is likely to be from less frequent states, and thus would beignored.
In our data set, we found that state frequencies followed a Zipfian (i.e., large-tailed) distribution, with 61% of the system turns having states that only occurred oncein the data.490Henderson, Lemon, and Georgila Hybrid Reinforcement/Supervised LearningAnother source of variation between learning approaches is the extent to which theytrain on data from simulated users of different kinds, rather than train on data gatheredfrom real user interactions (as is done in this article).
Simulated users are generallypreferred due to the much smaller development effort involved, and the fact that trial-and-error training with humans is tedious for the users.
However, the issues of howto construct and then evaluate simulated users are open problems.
Clearly there is adependency between the accuracy of the simulation used for training and the eventualdialogue policy that is learned (Schatzmann et al 2005).
Current research attempts todevelop metrics for user simulation that are predictive of the overall quality of thefinal learned dialogue policy (Schatzmann, Georgila, and Young 2005; Schatzmannet al 2005; Georgila, Henderson, and Lemon 2005; Georgila, Henderson, and Lemon2006; Rieser and Lemon 2006a; Schatzmann et al 2006; Williams 2007).
Furthermore,several approaches use simple probabilistic simulations encoded by hand, using intu-itions about reasonable user behaviors (e.g., Pietquin 2004; Frampton and Lemon 2005;Pietquin and Dutoit 2006a), whereas other work (e.g., Scheffler and Young 2001, 2002;Georgila, Henderson, and Lemon 2005; Georgila, Henderson, and Lemon 2006; Rieserand Lemon 2006a) builds simulated users from dialogue corpora.
We use the latterapproach, but only in the evaluation of our learned policies.No matter which method is chosen for user simulation, a simulated user is stillclearly different from a human user.
Therefore, it is important to learn as much as possi-ble from the data we have from human users.
In addition, the huge policy space makespolicy exploration with simulated users intractable, unless we can initialize the systemwith a good policy and constrain the policy exploration.
This also requires learning asmuch as possible from the initial set of data.
Therefore, in this article we investigateusing a fixed corpus of dialogues to automatically optimize dialogue systems.
No usersimulation is involved in training, thus avoiding the issue of dependency on the qualityand availability of user simulations.Previous work on RL has made use of policy exploration (Sutton and Barto 1998),where new data is generated for each policy that is considered during the courseof learning (for example using simulated users).
Indeed, this is often considered anintegral part of RL.
In contrast, we choose to learn from a fixed data set, withoutpolicy exploration.
This is motivated by the fact that real dialogue corpora are veryexpensive to produce, and it is often not practical to produce new real dialogues duringthe course of learning.
Singh et al (2002) manage to perform one iteration of policyexploration with real data, but most work on RL requires many thousands of iterations.As discussed previously, this motivates using simulated data for training, but even ifaccurate dialogues can be automatically generated with simulated users, training onsimulated dialogues does not replace the need to fully exploit the real data, and doesnot solve the sparse data problems that we address here.
With a very large state space,it will never be tractable for policy exploration to test a new policy on even a reasonableproportion of the states.
Thus we will inevitably need to stop policy exploration witha policy that has not been sufficiently tested.
In this sense, we will be in a very similarsituation to learning from a fixed data set, where we don?t have the option of generatingnew data for new states.
For this reason, the solution we propose for learning from fixeddata sets is also useful for learning with policy exploration.There have been some proposals in RL for learning a policy that is different fromthat used to generate the data (called ?off-policy?
learning), but these methods havebeen found not to work well with linear function approximation (Sutton and Barto1998).
They also do not solve the problem of straying from the region of state spacethat has been observed in the data, discussed subsequently.491Computational Linguistics Volume 34, Number 41.2 The COMMUNICATOR Domain and Data AnnotationTo empirically evaluate our proposed learning method, we apply it to the COMMU-NICATOR domain using the COMMUNICATOR corpora.
The COMMUNICATOR corpora(2000 [Walker et al 2001] and 2001 [Walker et al 2002b]) consist of human?machinedialogues (approximately 2,300 dialogues in total).
The users always try to book a flight,but they may also try to select a hotel or car rental.
The dialogues are primarily ?slot-filling?
dialogues, with some information being presented to the user after the systemthinks it has filled (or confirmed) the relevant slots.
These corpora have been previouslyannotated using the DATE scheme, for the Conversational Domain, Speech Act, andTask of each system utterance (Walker and Passonneau 2001; Walker, Passonneau, andBoland 2001).
In addition, the results of user questionnaires are available, but only forthe 2001 corpus.Table 1 shows some statistics for the two collections.
In the 2000 collection each turncontains only one utterance but in the 2001 corpus a turn may contain more than oneutterance.
More details about the COMMUNICATOR corpora can be found in Walker,Passonneau, and Boland (2001) and Walker et al (2001, 2002a).We used a hand-crafted automatic system (Georgila, Lemon, and Henderson 2005;Georgila et al, submitted) to assign Speech Acts and Tasks to the user utterances, and tocompute state representations for each point in the dialogue (i.e., after every utterance).Although we annotated the whole 2000 and 2001 corpora, because we need the resultsof user questionnaires (as discussed subsequently), we only make use of the 2001 datafor the experiments reported here.
The 2001 data has eight systems, 1,683 dialogues, and125,388 total states, two thirds of which result from system actions and one third fromuser actions.
The annotation system is implemented using DIPPER (Bos et al 2003) andOAA (Cheyer and Martin 2001), using several OAA agents (see Georgila, Lemon, andHenderson, 2005, and Georgila et al, submitted, for more details).
Following the ISUapproach, we represented states using Information States, which are feature structuresintended to record all the information about the preceding portion of the dialogue thatis relevant to making dialogue management decisions.
An example of some of the typesof information recorded in an Information State is shown in Figure 1, including filledslots, confirmed slots, and previous speech acts.Given this corpus, we need to learn a dialogue management policy that maps thesestate representations to effective system actions.
As the example in Figure 1 illustrates,there are a large number of features in dialogue states that are potentially relevant toTable 1Statistics for the 2000 and 2001 COMMUNICATOR data.Year2000 2001 TotalNumber of dialogues 648 1683 2331Number of turns 24,728 78,718 103,446Number of system turns 13,013 39,419 52,432Number of user turns 11,715 39,299 51,014Number of utterances 24,728 89,666 114,394Number of system utterances 13,013 50,159 63,172Number of user utterances 11,715 39,507 51,222Number of system dialogue acts 22,752 85,881 108,633492Henderson, Lemon, and Georgila Hybrid Reinforcement/Supervised Learningdialogue management, and thus should not be excluded from the state representationswe use in learning.
This leads to a very large space of possible states (over 10386 states aretheoretically possible in our model), with a very high chance that a state encounteredin testing will not be exactly the same as any state encountered in training.
This factmotivates, if not requires, the use of approximation methods.The complexity of the COMMUNICATOR domain is alsomanifested in the large num-ber of system actions that the dialogue management policy needs to choose between.The DATE scheme representation of system actions implies that each possible triple ofvalues for the Conversational Domain, Speech Act, and Task is a different action.
Inaddition, we have added release turn and end dialogue actions.
There are a total of74 system actions that occur in the annotated COMMUNICATOR data.2.
Reinforcement Learning with a Fixed Data SetWe use the annotated COMMUNICATOR data to train a Reinforcement Learning system.In RL, the objective of the system is to maximize the reward it gets during entire dia-logues.
Rewards are defined to reflect how successful a dialogue was, so by maximizingthe total reward the system optimizes the quality of dialogues.
The difficulty is that, atany point in the dialogue, the system cannot be sure what will happen in the remainderof the dialogue, and thus cannot be sure what effect its actions will have on the totalreward at the end of the dialogue.
Thus the system must choose an action based on theaverage reward it has observed previously after it has performed that action in statessimilar to the current one.
This average is the expected future reward.The core component of any RL system is the estimation of the expected futurereward (called the Q-function).
Given a state and an action that could be taken in thatstate, the Q-function tells us what total reward, on average, we can expect betweentaking that action and the end of the dialogue.1 Once we have this function, the optimaldialogue management policy reduces to simply choosing the action that maximizes theexpected future reward for the current state.Our proposal for RL with fixed data sets uses two main techniques.
The first is theuse of function approximation to estimate the expected future reward.
We claim thatlinear function approximation is an effective way to generalize from a limited data setto a large space of state?action pairs.
The second technique is a novel hybrid learningmethod that combines RL with supervised learning (SL).
SL is used to characterize howmuch data we have for each area of the state?action space (also using linear functionapproximation).
Our hybrid policy uses SL to avoid state?action pairs for which we donot have enough data, while using RL to maximize reward within the parts of the spacewhere we do have enough data.
We claim that this is an effective solution to the problemof learning complex tasks from fixed data sets.2.1 Defining Dialogue RewardTo apply RL to the COMMUNICATOR data, we first have to define a mapping r(d, i) froma dialogue d and a position in that dialogue i to a reward value.
This reward function iscomputed using the reward level of annotation in the COMMUNICATOR data, which was1 The expected future reward also depends on the dialogue management policy that the system will use inthe future.
This self-referential nature of RL is the topic of much RL research, and will be discussed morein the following.493Computational Linguistics Volume 34, Number 4extracted from user questionnaires and task completion measures.
For all states otherthan the final state, we provide a reward of ?1 if the state follows a system action, and0 otherwise.
This encodes the idea that, all other things being equal, short dialoguesare better than long ones.
For the final state we provide a reward that is the sum ofthe rewards for each feature in the reward annotation.
?Actual Task Completion?
and?Perceived Task Completion?
are both worth a reward of 100 if they are non-zero (i.e.,true), and 0 otherwise.
The remaining reward features have values ranging from 1 to 5in the annotation (where 5 is the best), which we rescale to the range 0 to 1 (1 convertsto 0, 5 converts to 1).
Their reward is their rescaled value times the weight shown inTable 2.
The relative values of these later weights were determined by the empiricalanalysis reported in Walker et al (2001) within the PARADISE evaluation framework(Walker, Kamm, and Litman 2000).2.2 Estimating the Expected Future RewardGiven this definition of reward, we want to find an estimate Q(si, a) of the expectedfuture reward, which is the expected value (?E[ ]?
in Equation 1) of the total rewardbetween taking action a in state si and the end of the dialogue.
This expectation is a sumover all possible future dialogues d, weighted by the probability of the dialogue giventhat we have performed action a in state si.Q(si, a) ?
Ed|si,a[?j>ir(d, j)] =?d(P(d|si, a)?j>ir(d, j)) (1)Given that the number of possible future dialogues d = ?si+1, .
.
.
, snd?
is exponential inthe length of the sequences, it is not surprising that estimating the expected reward overthese sequences can be very difficult.The ISU framework is significantly different from the frameworks used in previouswork on reinforcement learning for dialogue management, in that the rich contextrepresentation makes the number of possible states extremely large.
Having a largenumber of states is a more realistic scenario for practical, flexible, and generic dialoguesystems, but it also makes many RL approaches intractable.
In particular, with a largenumber of states it is not possible to learn estimates of the expected future reward forevery state, unless we can exploit commonalities between different states.
The feature-based nature of ISU state representations expresses exactly these commonalities be-tween states through the features that the states share.
There are a number of techniquesthat could be used for RL with feature-based representations of states, but the simplestand most efficient is linear function approximation.Table 2The weights used to compute a dialogue?s final reward value, multiplied by values between 0and 1 computed from user responses.Actual task completion 100Perceived task completion 100Task ease 36Comprehension ease 28System behaved as expected 32Future use 36494Henderson, Lemon, and Georgila Hybrid Reinforcement/Supervised LearningWe use linear function approximation to map from a vector of real valued featuresf (s) for the state s to a vector of estimates Q(s, a), one estimate for each a.
The trainedparameters of the linear function are a vector of weights wa for each action a. Givenweights trained on a given data set, an estimate Qdata(s, a) of the expected future rewardgiven a state s and an action a is the inner product of the state vector f (s) and the weightvector wa.2Qdata(s, a) = f (s)Twa =?ifi(s)wai (2)This approximation method has the effect of treating two states as similar if theyshare features.
During learning, updating the estimate Qdata(s, a) for one observed states will also update the estimate Qdata(s?, a) for all other states s?
to the extent that s?shares features with s. This updating happens via the weights wa; if s has feature i thenupdating the estimate Qdata(s, a) will change wai, which will in turn change Qdata(s?, a) forany s?
that also has feature i.
Thus each feature represents a dimension with respect towhich two states can be similar or different.
This similarity measure is known as a linearkernel.This is the first time that linear function approximation has been used for learningdialogue strategies.
Denecke, Dohsaka, and Nakano (2005) also use function approxi-mation, but there the notion of similarity used during learning is Euclidean distance,rather than shared features.
In effect, Denecke, Dohsaka, and Nakano use a Gaussiankernel, whereas we use a linear kernel.To train the weights of the linear approximation Qdata(s, a), we employed a standardRL learning method called SARSA(?)
(Sutton and Barto 1998).
This method learnsbased on two criteria, with a parameter ?
used to weight their relative influence.
Thefirst criterion comes from temporal-difference learning: the current estimate for theQ-function should (on average) equal the reward from the next state plus the estimatefor the expected future reward at the next state.
The second criterion comes directlyfrom the observed reward: The current estimate for the Q-function should (on average)equal the reward observed for the remainder of the dialogue.
The combination of thesetwo criteria makes learning faster than using either one alone.
Gradient descent learningis applied to the weights; at each step of learning, the weights are updated so as to makethe Q-function better fit this combined criterion.Whereas the weights wa are learned from data, the mapping f (s) from states to vec-tors must be specified beforehand.
Because each value fi(s) in these vectors represents apossible commonality between states, it is through the definition of f (s) that we controlthe notion of similarity that will be used by the linear function approximation.
Thedefinition of f (s) we are currently using is a straightforward mapping from attribute?value pairs in the Information State s to values in the vector f (s).The state vector mapping f (s) was computed using the first four levels of our stateannotations for the COMMUNICATOR data (i.e., the Dialogue, Task, Low, and Historylevels shown in Figure 1).
The values of the attributes in these annotations were con-verted to features of three types.
For attributes that take numbers as values, we useda simple function to map these numbers to a real number between 0 and 1, with theabsence of any value being mapped to 0 (resulting in six features, e.g., StateNumber).2 We will use the notation xTy to denote the inner product between vectors x and y (i.e., ?x transposetimes y?).
wai is the ith element of the vector wa.495Computational Linguistics Volume 34, Number 4For attributes that can have arbitrary text as their values, we used 1 to represent thepresence of text and 0 to represent no value (resulting in two features, e.g., AsrInput).The remaining attributes all have either a finite set of possible values, or a list of suchvalues.The vast majority of our features are constructed from this third set of attributes.First, to reflect the importance of speech act?task pairs (which we use to define bothsystem and user actions), we construct a new SpeechAct-Task attribute whose valueis the concatenation of the values for the SpeechAct and Task attributes.
The same isdone for the SpeechActsHist and TasksHist attributes.
Second, attributes with a listvalue (i.e., the .
.
.Hist and .
.
.Status attributes, plus user actions3) are converted to aset of attribute?value pairs consisting of the attribute and each value in the list (result-ing in 509 features, e.g., FilledSlotsStatus:[orig city]).
Note that this conversionloses the ordering between the values in the list.
In the case of SpeechAct, Task, andSpeechAct-Task attributes that have list values (which result from turns in which auser performs more than one action), we also include the whole list as a value forthe attribute4 (resulting in 364 features, e.g., SpeechAct:[no answer,provide info]).Finally, attributes with single values are assigned features (which result in 401 features,e.g., Speaker:user).From this set of potential features, we only use those that occur in the data at leastfive times.5 (Only these features are included in the feature counts given previously.
)Each feature is assigned an element of the vector f (s) that is 1 if that feature is present inthe state and 0 if it is not.
In total there are 1,282 features.One advantage of using linear function approximation is that the learning methodcan be kept fairly simple, while still incorporating domain knowledge in the designof the mapping to feature vectors.
One area of future research is to investigate morecomplicated mappings to feature vectors f (s).
This would involve making use of kernel-based methods.
Kernels are used to compensate for the oversimplicity of linear func-tions, and can be used to express more complicated notions of commonality betweenstates (Shawe-Taylor and Cristianini 2004).2.3 Pure RL and SL PoliciesGiven the estimate of the expected future reward Qdata(s, a) discussed in the previoussection, one obvious approach would use this estimate to define the dialogue policy.This ?pure RL?
policy simply selects the action a with the highest Qdata(s, a) given thestate s. As demonstrated by the evaluation in Section 3, this policy performs very badly.Inspection of the actions chosen by the pure RL policy indicates that this policy isvery different from the policy observed in the COMMUNICATOR data; the pure RL policyalmost never chose the same action as was in the data.
This means that the actions thathave been learned to have the best future reward for a state are not the ones that were3 Because in the 2001 COMMUNICATOR data users may perform more than one action in a single turn, auser?s action is potentially a list of speech act?task pairs.
These are annotated as lists of speech acts pluslists of tasks, to which we add lists of speech act?task pairs.
Histories of these lists (i.e., lists of lists) arefirst flattened and then treated like other lists.4 These ?list?
values are more accurately described as set values, because we do not encode the orderingof the values in the list.5 We also do not include the .
.
.Value.
.
.
attributes, such as FilledSlotValue, which specify the actualfillers for slots.496Henderson, Lemon, and Georgila Hybrid Reinforcement/Supervised Learningtypically chosen by the COMMUNICATOR systems in that state.
This difference results intwo problems: such atypical actions then lead to states unlike anything observed in thedata, the policy that the system will use for future actions is different from thatobserved in the data, andThe first problem makes the Qdata(s, a) estimates for the visited states highly unreliable,because we don?t have data for these states.
Because the future reward depends on thepolicy that the systemwill use in the future, the second problemmeans that the estimateQdata(s, a) is not even relevant to the expected future reward of the pure RL policy.Wewillreturn to these problems when we develop our proposed method in Section 2.4.These problems are a result of the fact that we are training on a fixed data set,and therefore cannot generate new data that is appropriate for the new policy.
Thesolution to these problems that is typically used in RL research is to generate newdata as learning progresses and the policy changes, as discussed in Section 1.1.
The RLsystem can thus explore the space of possible policies and states, generating new datathat is relevant to each explored policy and its states.
The problem with learning withpolicy exploration, even when using simulated users, is that it is not tractable with alarge state space and action set.
Consider that with 10386 states and 74 actions, there are7410386possible policies.
If we were able to explore policies at a rate of 1 policy a second,after 1 year we would have visited only one policy in every 7410385.6policies.
Policyexploration algorithms are only partially random, so to some extent they can makeaccurate choices about which parts of the policy space to explore and which to ignore,but these numbers are indicative of the scale of the problem faced by policy exploration.In addition, experiments with a random policy achieved an average score of ?66,showing that the vast majority of policies are very bad.
This indicates that startingpolicy exploration with a random policy would require an extremely large amount ofexploration to move from there to a policy which is as good as the policy found withthe proposal discussed herein (which achieved a score of 140, out of a maximum 197).Therefore it is crucial that exploratory learning at least be initialized with a policy thatwe already know to be good.
The method proposed in this article for learning a policyfrom a pre-existing corpus of dialogues can be used to find such an initial policy.Given these problems with using RL with a fixed data set, an obvious alternativewould be to simply train a policy to mimic the policies of the systems used to generatethe data.
One reason for training a policy, rather than using one of the original policies,is that learning allows us to merge the policies from all the different systems, whichcan lead to a better policy than any one system (as we will show in Section 3).
Anotherreason is that learning results in a policy that generalizes from the original policies ininteresting ways.
Most notably, our learning method can be used to define a probabilis-tic policy, not just the (presumably) deterministic policies used to generate the data.
Athird reason could be (as in our case) that we do not have access to any of the originalsystems that generated the data.
In some sense we can use learning to reverse engineerthe systems.We train a policy to mimic the policy observed in the data using supervised learningwith linear function approximation.
This ?pure SL?
policy simply selects the actiona with the highest probability P(a|s) of being chosen given the state s. We estimateP(a|s) with linear function approximation, just as for Qdata(s, a), except that a normalized497Computational Linguistics Volume 34, Number 4exponential function (a.k.a.
?softmax?)
is used so that the result is a probability distri-bution over actions a.P(a|s) ?
Sdata(s, a) =exp( f (s)Tw?a)?a?
exp( f (s)Tw?a?
)(3)This gives us a log-linear model, also known as a maximum entropy model.
Theparameters of this model (thew?a) are trained using supervised learning on the COMMU-NICATOR data.
As with the Q-function, the use of linear function approximation meansthat we have estimates for P(a|s) even for states s that have never occurred in the data,based on similar states that did occur.2.4 A Hybrid Approach to RLIn this work we focus on solving the first of the two problems we have discussed,namely, preventing the system from straying into portions of the state space for whichwe do not have sufficient data.
To do this, we propose a novel hybrid approach thatcombines RL with supervised learning.
SL is used to model which actions will take thesystem into a portion of the state space for which we don?t have sufficient data.
RLis used to choose between the remaining actions.
A discriminant function Qhybrid(s, a) isderived that combines these two criteria in a principled way.
The resulting policy can beadjusted to be as similar as necessary to the policy in the data, thereby also addressingthe second problem discussed previously.As with the pure SL policy, supervised learning is used to model the policy thatthe systems in the data actually use.
Because in general multiple policies were used,we model the data?s policy as a probabilistic policy, using the estimate Sdata(s, a) of P(a|s)presented in the previous section.
Sdata(s, a) is an estimate of the probability that a randomsystem selected from those that generated the data would choose action a given that it isin state s. Because we are using function approximation to learn Sdata(s, a) from the data,it will generalize (or ?smooth?)
the policies actually used to generate the data so thatsimilar states will allow similar sets of actions.6The hybrid approach we have investigated is based on the assumption that the Q-function trained on the data is a poor model of the expected future reward for statesin the portion of the state space not covered by the data.
Thus we need an alternativemethod for estimating the future reward for these unobserved states.
We have exper-imented with two such methods.
The first method simply specifies a fixed reward Ufor these states.
By setting this fixed reward to a low value, it amounts to a penalty forstraying from the observed portion of the state space.The second method estimated the reward for unobserved states by adding a fixedreward offset UO to the reward estimates for ending the dialogue immediately.
Thismethod compensates for the use of a dialogue-final reward scheme, where many thingsthat the dialogue has already accomplished aren?t reflected in the reward given so far.For example, in our reward scheme, filling a slot does not result in immediate reward,but instead results in reward at the end of the dialogue if it leads to a successfuldialogue.
The estimated reward for ending the dialogue immediately reflects howmuch6 For this reason, we will get a probabilistic policy even if only a single deterministic policy is usedto generate the data.
This makes this method applicable even for data sets generated with a singledeterministic prototype system.498Henderson, Lemon, and Georgila Hybrid Reinforcement/Supervised Learningreward is stored up in the state in this way.
If the fixed reward added to this estimate isset to negative, then we can be sure that the reward estimated for unobserved states isalways less than that for the best observed state, so this method also results in a penaltyfor straying from the observed portion of the state space.Given an estimated reward u for unobserved states, the expected future reward isthen the average between u for the cases where performing a in s leads to an unobservedstate and the expected reward Qdata(s, a) for the cases where it leads to an observed state.Formally, this average is a mixture of the estimate u with the estimate Qdata(s, a), wherethe mixture coefficient is the probability Pobserved(s, a) that performing a in swill lead to anobserved state.Ed|si,a[?j>i r(d, j)]?
Qdata(s, a)Pobserved(s, a)+ u(1?
Pobserved(s, a))(4)Because this estimate of the expected future reward is only needed for choosingthe next action given the current state s, we only need to estimate a function that dis-criminates between different actions in the same way as this estimate.
To derive such adiscriminant function, we first approximate Pobserved(s, a) with a first-order approximationin terms of the probability distribution in the data P(s, a) and the size of the data set N,under the assumption that the number of possible state?action pairs is much larger thanthe size of the data set (so P(s, a)N 	 1).Pobserved(s, a) = 1?
(1?
P(s, a))N?
P(s, a)N ?
Sdata(s, a)P(s)N(5)Given this approximation, the discriminant function needs to order two actions a1, a2 inthe same way as this estimate of the expected future reward.Qdata(s, a1)Sdata(s, a1)P(s)N + u(1?
Sdata(s, a1)P(s)N)?
Qdata(s, a2)Sdata(s, a2)P(s)N + u(1?
Sdata(s, a2)P(s)N)if and only ifSdata(s, a1)(Qdata(s, a1)?
u) ?
Sdata(s, a2)(Qdata(s, a2)?
u)(6)We call this discriminant function Qhybrid(s, a).Qhybrid(s, a) = Sdata(s, a)(Qdata(s, a)?
u) (7)We use thisQhybrid(s, a) function to choose the actions for our hybrid policy.
By adjust-ing the value of the unobserved state penalty u, we can adjust the extent to which thismodel follows the supervised policy defined by Sdata(s, a) or the reinforcement learningpolicy defined by Qdata(s, a).
In particular, if u is very low, then maximizing Qhybrid(s, a) isequivalent to maximizing Sdata(s, a).
Thus a very low u is equivalent to the policy thatalways chooses the most probable action, which we will call the ?SL policy.
?The procedure for trainingQhybrid(s, a) is simply to trainQdata(s, a) with RL and Sdata(s, a)with SL.
These two models are then combined using Equation (7), given a value for ucomputed with one of the two methods presented previously.
Both of these methodsinvolve setting a constant that determines the relative importance of RL versus SL.
Inthe next section we will empirically investigate good values for these constants.499Computational Linguistics Volume 34, Number 43.
Empirical EvaluationWe evaluate the trained dialogue management policies by running them against traineduser simulations.
The policies and the user simulations were trained using differentparts of the annotated COMMUNICATOR data (using two-fold and five-fold cross val-idation).
We compare our results against each other and against the performance ofthe eight COMMUNICATOR systems, using an evaluation metric discussed subsequently.The Information States for the simulated dialogues were computed with the same rulesused to compute the Information States for the annotated data.3.1 The Testing SetupFor these experiments, we restrict our attention to users who only want single-legand return flight bookings.
This allows us to do the evaluation using only the fouressential slots included in both these types of bookings: origin city, destination city,departure date, and departure time.
To achieve this restriction, we first selected all thoseCOMMUNICATOR dialogues that consisted only of single-leg or return flight bookings.This subset contained 217 ATT dialogues, 116 BBN dialogues, 126 CMU dialogues, 159Colorado dialogues, 77 IBM dialogues, 192 Lucent dialogues, 180 MIT dialogues, and185 SRI dialogues, for a total of 1,252 dialogues (out of 1,683).
This subset was used forevaluating the COMMUNICATOR systems and for training the user models.
The systemmodels were trained on the full set of dialogues, because they should not know theuser?s goals in advance.
So, for each fold of the data, the user model was trained on onlythe single-leg and return dialogues from that fold and the system model was trained onthe full set of dialogues from a subset of the remaining folds (one fold for the two-foldexperiments and three folds for the five-fold experiment, as discussed subsequently).The user models were trained in the same way as the Sdata(s, a) function for thepure SL model discussed in Section 2.3, using linear function approximation and anormalized exponential output function.
The states that precede user actions are inputas vectors of features virtually identical to those used for the system.
However, unlikethe action set for the system, the user only chooses one action per turn, and that actioncan include multiple ?Speech Act, Task?
pairs.
The output of the model is a probabilitydistribution over these actions.
The user simulation selects an action randomly accord-ing to this distribution.
We also trained a user model based on n-grams of user andsystem actions, which produced similar results in our testing (Georgila, Henderson,and Lemon 2006).In our initial experiments with the hybrid policy, we found that it never closedthe dialogue.
We think that this was due to the system action (annotated in DATE)meta greeting goodbye, which is used both as the first action and as the last actionof a dialogue.
The hybrid policy expects this action to be chosen before it will closethe dialogue, but the system never chooses this action at the end of a dialogue becauseit is so strongly associated with the beginning of the dialogue.
This is an example ofthe limitations of linear function approximation, and our dependence on the previousCOMMUNICATOR annotations.
We could address this problem by splitting this actioninto two actions, one for ?greeting?
and one for ?goodbye.?
But because we do not wantto embark on the task of feature engineering at this stage, we have instead augmentedthe hybrid policy with a rule that closes the dialogue after the system chooses the actionoffer, to offer the user a flight.
After this first flight offer, the user has one turn toreply, and then the dialogue is ended.
For practical reasons we have also added rules500Henderson, Lemon, and Georgila Hybrid Reinforcement/Supervised Learningthat close the dialogue after 100 states (i.e., total of user and system actions), and thatrelease the turn if the system has done 10 actions in a row without releasing the turn.3.2 The Evaluation MetricsTo evaluate the success of a dialogue, we take the final state of the dialogue and useit to compute a scoring function.
We want the scoring function to be similar to thereward we compute from the quality measures provided with the COMMUNICATORdata (e.g., the user questionnaires), but because we do not have these quality measuresfor the simulated dialogues, we cannot use the exact same reward function.
When wecompare the hybrid policy against the COMMUNICATOR systems, we apply the samescoring function to both types of dialogues so that we have a comparable evaluationmetric for both.Because currently we are only considering users who only want single-leg or returnflight bookings, the scoring function only looks at the four essential slots for thesebookings: origin city, destination city, departure date, and departure time.
We give25 points for each slot that is filled, plus another 25 points for each slot that is alsoconfirmed.
We also deduct 1 point for each action performed by the system, to penalizelonger dialogues.
Thus the maximum possible score is 197 (i.e., 200 minus 3 systemactions: ask for all the user information in one action, then confirm all the four slots inone action and offer a flight).The motivation behind this evaluation metric is that confirmed slots are more likelyto be correct than slots that are just filled.
If we view the score as proportional to theprobability that a slot is filled correctly, then this scoring assumes that confirmed slotsare twice as likely to be correct.
Although other scoring metrics are clearly possible, thisone is a simple and reasonable approximation of the relative expected correctness ofconfirmed versus non-confirmed information in dialogue systems.
On the other hand,none of our conclusions depend on this exact scoring function, as indicated by results forthe ?no-conf?
version of our scoring function (discussed subsequently), which ignoresconfirmations.When combining the scores for different slots, we do not try to model the all-or-nothing nature of the COMMUNICATOR task-completion quality measures, but insteadsum the scores for the individual slots.
This summakes our scoring metric value partialcompletions more highly, but inspection of the distributions of scores indicates thatthis difference does not favor either the hybrid policy or the original COMMUNICATORsystems.Although this evaluation metric could reflect the relative quality of individualdialogues more accurately, we believe it provides a good measure of the relative qualityof the systems we wish to compare.
First, the exact same metric is applied to everysystem.
Additional information that we have for some systems, but not all, is not used(e.g., the COMMUNICATOR user questionnaires, which we do not have for simulateddialogues).
Second, the systems are being run against approximately equivalent users.The user simulation is trained on exactly the same user actions that are used to evaluatethe COMMUNICATOR systems, so the user simulations mimic exactly these users.
Inparticular, the simulation is able to mimic the effects of speech recognition errors,because it is just as likely as the real users to disagree with a confirmation or provide anew value for a previously filled slot.
The nature of the simulation model may make itsystematically different from real users in some way, but we know of no argument forwhy this would bias our results in favor of one system or another.501Computational Linguistics Volume 34, Number 4One concern about this evaluation metric is that it does not reflect the quality of thespeech recognizer being used by the system.
If a system has a good speech recognizer,then it may not be necessary for it to confirm a slot value, but our scoring function willstill penalize it for not confirming.
This would certainly be a problem if this metric wereto be used to compare different systems within the COMMUNICATOR data set.
However,the intention of the metric is simply to facilitate comparisons between different versionsof our proposed system, and between our proposed systems and those in the data.Because the user simulations are trained on the COMMUNICATOR data, they simulatespeech recognition errors at the same rate as the data, thereby controlling for the qualityof the speech recognizer.Nonetheless, it is worth considering another evaluation metric that does not penal-ize for missing confirmations.
For this reason we also evaluate the different systemsbased on their scores for only filled slots and length, which we call the ?no-conf?
score.3.3 The Influence of Reinforcement LearningIn our first set of experiments, we evaluated the success of our hybrid policy relativeto the performance of the pure reinforcement learning policy and the pure supervisedlearning policy.
We also investigated how to best set the parameters for combining thesupervised and reinforcement learning policies in a hybrid policy.We first compared the two proposed hybrid methods using two-fold cross valida-tion.
We trained models of both Qdata(s, a) and Sdata(s, a), and then used them to definepolicies.
We trained both models for 100 iterations through the training portion of thedata, at which point there was little change in the training error.
We trained Qdata(s, a)using SARSA(?)
with ?
= 0.9.
This training was repeated twice, once for each fold of thecomplete data set.
The reinforcement learning policy uses only Qdata(s, a), the SL policyuses only Sdata(s, a), and the hybrid policies combine the two using Equation (7).
For thehybrid policies, we used the two methods for estimating the unobserved state penaltyu and various values for the fixed reward U or reward offset UO.During testing, each policy was run for 2,000 dialogues against a linear functionapproximation user model trained on the opposite half of the data.
The final state foreach one of these dialogues was then fed through the scoring function and averagedacross dialogues and across data halves.
The results are plotted in Figure 2.
To allowdirect comparisons between the different values of U and UO, these scores are plottedagainst the proportion of decisions that are different from that which the pure SL policywould choose.
Thus the SL policy (average reward 139.8) is plotted at 0 (which isFigure 2Average dialogue score plotted against the proportion of decisions that diverge from the SLpolicy, for different values of the unobservable state reward U and reward offset UO.
Averagesover two folds, 2,000 dialogues per fold.502Henderson, Lemon, and Georgila Hybrid Reinforcement/Supervised LearningFigure 3Average dialogue score plotted against the proportion of decisions that diverge from the SLpolicy, for different values of the unobservable state reward offset UO.
Averages over five folds,2,000 dialogues per fold.equivalent to a large negative U or UO).
Additional points for the hybrid policies areshown for (from left to right, respectively) U = 0, 40, 80, and 100, and UO = ?300, ?100,?60, ?40, ?20, ?10, and 0.
The pure reinforcement learning policy is not shown becauseits average score falls well below the bottom of the graph, at 44.4.Figure 2 indicates that, for both hybrid methods, adding some influence from RLincreases performance over pure SL, but too much RL results in degradation.
Using areward offsetUO for u generally does better than a fixed rewardU, and allows a greaterinfluence from RL before degradation.We found that the results for our two folds were very different,7 so we repeatedthe experiments using five-fold cross validation, where the dialogues from each systemwere split randomly (rather than chronologically).8 For each fold, we trained modelsof both Qdata(s, a) and Sdata(s, a) on three of the folds, using a fourth fold to decide whento stop training.
The fifth fold was then used to train a linear function approximationuser model, which was used to generate 2,000 simulated dialogues.
Combining the fivefolds, this gave us 10,000 dialogues per model.
Because, in the previous experiments,using a reward offset UO performed better than using a fixed reward U, we only testedmodels using different values of the reward offset UO.The validation performance of the trained models for Qdata(s, a) and Sdata(s, a) per-formed similarly across the different splits.
Taken together, the models of Sdata(s, a) had aperplexity of 4.4.
Intuitively, this means that the supervised models were able to narrowdown the list of possible actions from 74 to about 4 choices, on average.
This suggeststhat the ISU representation of state is doing a good job of representing the informationbeing used by the systems to make dialogue management decisions, but that there isstill a good amount of uncaptured variability.
Presumably most of this variability is dueto differences between the policies for the different systems.
The models of Qdata(s, a)had a mean squared error of 8,242, whose square root is 91.
This measure is harder tointerpret because it is dominated by large errors, but suggests that the expected futurereward is rather hard to predict, as is to be expected.Figure 3 shows the average scores for the pure SL policy (at 0) and for hybridpolicies (from left to right) with UO = ?300, ?100, ?60, ?40, ?20, ?10, ?5 and 0.
The7 For the two-fold experiments, the data were split by putting the first half of the dialogues for each systemin one fold and the second half in the other, under the constraint that no user had dialogues in more thanone fold.
It appears that the users that were run at the beginning of the 2001 COMMUNICATOR datacollection were very different from those run at the end.8 To be more precise, for each system we split the set of users randomly into five groups.
Then all thedialogues for a given group of users were put in the same fold.503Computational Linguistics Volume 34, Number 4hybrid policies perform consistently better than the SL policy.
The difference betweenthe hybrid policy and the SL policy is statistically significant at the 5% level for thethree best hybrid policies tested (p < 0.01 for UO = ?40, p < 0.001 for UO = ?10, andp < 0.007 for UO = ?5).
If we combine all the tested hybrid policies together, then theiraverage score (139.4) is also significantly better than the SL policy (p < 0.014).
All theseresults are significantly better than the average score of the pure RL policy (34.9).3.4 Comparisons with COMMUNICATOR SystemsIn our second set of experiments, we evaluated the success of our learned policiesrelative to the performance of the COMMUNICATOR systems that they were trained on.To evaluate the performance of the COMMUNICATOR systems, we extracted final statesfrom all the dialogues that only contain single-leg or return flight bookings and fedthem through the scoring function.
The average scores are shown in Tables 3 and 4,along with the average scores for the pure SL policy, the pure RL policy, and the besthybrid policy (UO = ?10).
The total score, the score excluding confirmations, and thethree components of the total score are shown.Table 3 shows the results computed from the complete dialogues.
These resultsshow a clear advantage for the hybrid policy over the average across the COMMUNI-CATOR systems, as well as over each individual COMMUNICATOR system.
In particular,the hybrid policy uses fewer steps.
Because the number of steps is doubtless affectedby the hybrid policy?s built-in strategy of stopping the dialogue after the first flightoffer, we also evaluated the performance of the COMMUNICATOR systems if we alsostopped these dialogues after the first flight offer, shown in Table 4.
The COMMUNI-CATOR systems do better when stopped at the first flight offer, but still their average(?all COMMUNICATOR?)
is not nearly as good as the hybrid or SL policies, under allmeasures.Although the average score of the COMMUNICATOR systems in Table 4 is well belowthose of the hybrid and SL policies, under this measure the single best system (BBN)beats our proposed system.
Also, if we ignore confirmations (the ?no-conf?
measure),Table 3The average scores from the different systems for single-leg and return dialogues, the scoreexcluding confirmations, and the three components of these scores.System Total score No-conf Filled Confirmed Lengthscore slots slots penaltyhybrid RL/SL 140.3 70.3 88.0 70.0 ?17.7pure SL 138.3 69.2 89.2 69.1 ?20.0pure RL 34.9 25.6 56.9 8.3 ?31.3all COMMUNICATOR 103.6 40.6 85.0 63.0 ?44.4SRI 115.3 50.5 83.4 64.9 ?32.9MIT 114.3 43.2 87.1 71.1 ?43.9LUC 110.3 36.1 91.1 74.1 ?55.0COL 105.9 47.0 90.6 59.0 ?43.6BBN 102.4 27.1 82.5 75.2 ?55.4ATT 94.0 38.7 78.3 55.3 ?39.6CMU 92.1 24.0 81.7 68.1 ?57.7IBM 77.0 61.8 85.4 15.3 ?23.6504Henderson, Lemon, and Georgila Hybrid Reinforcement/Supervised LearningTable 4The average scores after the first flight offer for single-leg and return dialogues, the scoreexcluding confirmations, and the three components of these scores.System Total score No-conf Filled Confirmed Lengthscore slots slots penaltyhybrid RL/SL 140.3 70.3 88.0 70.0 ?17.7pure SL 138.3 69.2 89.2 69.1 ?20.0pure RL 34.9 25.6 56.9 8.3 ?31.3all COMMUNICATOR 127.1 63.2 84.5 63.9 ?21.3BBN 148.9 73.2 88.6 75.6 ?15.4LUC 138.5 59.1 91.1 79.4 ?32.1MIT 136.4 66.9 82.8 69.4 ?15.9COL 132.9 71.4 89.9 61.5 ?18.6SRI 128.2 61.7 84.2 66.5 ?22.5CMU 123.8 58.7 77.2 65.1 ?18.5ATT 109.1 53.6 78.3 55.4 ?24.7IBM 86.4 71.2 85.1 15.3 ?13.9then three of the individual systems beat our proposed system by small amounts.
How-ever, as discussed in Section 3.2, our evaluation methodology is not really appropriatefor comparing against individual COMMUNICATOR systems, due to likely differences inspeech recognition performance across systems.
To test this explanation, we looked atthe word error rates for the speech recognition outputs for the different systems.
BBNhas the highest percentage of user utterances with no speech recognition errors (79%,versus an average of 66%), and the second lowest average word error rate (12.1 versusan average of 22.1).
Because our simulated users simulate speech recognition errors atthe average rate, the difference in performance between BBN and our systems couldeasily be explained simply by differences in the speech recognizers, and not differencesin the dialogue management policies.3.5 DiscussionThe most obvious conclusion to draw from these results is not a surprising one: Purereinforcement learning with such a huge state space and such limited data does notperformwell.
Given the pure RL policy?s score of 34.9, all the policies in Figure 3 and allthe COMMUNICATOR systems in Tables 3 and 4 perform better by quite a large margin.Inspection of the dialogues indicates that the pure RL policy does not result in a coherentsequence of actions.
This policy tends to choose actions that are associated with the endof the dialogue, even at the beginning of the dialogue.
Perhaps this is because theseactions are only chosen by the COMMUNICATOR systems during relatively successfuldialogues.
This policy also tends to repeat the same actions many times, for examplerepeatedly requesting information even after the user has supplied this information.These phenomena are examples of the problemwe used tomotivate our hybrid learningmethod, in that they both involve state?action pairs that the learner would never haveseen in the COMMUNICATOR training data.Given the disappointing performance of the pure RL policy, it is surprising that ourhybrid policies outperform the pure SL policy, as shown in Figures 2 and 3.
Though theincrease in performance is small, it is statistically significant, and consistent across the505Computational Linguistics Volume 34, Number 4two hybrid methods and across a range of degrees of influence from RL.9 This indicatesthat our hybrid policies are succeeding in getting useful information from the results ofreinforcement learning, even under these extremely difficult circumstances.
Perhaps,under less severe circumstances for RL, a greater gain can be achieved with hybridpolicies.
For the second hybrid policy (unobserved state reward offset), the fact that thebest result was achievedwith aUO value (UO = ?10) that is very close to the theoreticallimit of this method (UO = 0) suggests that future improvements to this method couldresult in even more useful information being extracted from the RL policy.The different components of the scoring function give some indication of how thehybrid policies differ from the SL policy.
As indicated in the top two rows of Table 4,the hybrid policies mostly improve over the SL policy in dialogue length, with a slightincrease in confirmed slots and a slight decrease in filled slots.One striking conclusion from the results comparing the learned policies to the poli-cies of the COMMUNICATOR systems, shown in Tables 3 and 4, is that the learned policiesscore better than the policies they were trained on.
This is particularly surprising forthe pure SL policy, given that this policy is simply trying to mimic the behavior ofthese same systems.
This can be explained by the fact that the SL policy is the resultof merging all policies of the COMMUNICATOR systems.
Thus it can be thought of asa form of multi-version system, where decisions are made based on what the majorityof systems would do.10 Multi-version systems are well known to perform better thantheir component systems, because the mistakes tend to be different across the differentcomponent systems.
They remove errors made by any one system that are not sharedby most of the other systems.The good performance of the SL policy compared to the COMMUNICATOR systemsmakes the better performance of the hybrid policies even more impressive.
As shownon the x axis of Figure 3, the best hybrid systems choose a different action from theSL policy about one action out of four.
Despite the good performance of the actionchosen by the SL policy, RL is able to (on average) find a better action by looking at therewards achieved by the systems in the data when they chose those actions in similarstates.
By following different systems?
choices at different points in the dialogue, thelearned policy can potentially perform better than any individual system.
Although ourcurrent evaluation methodology is not fine-grained enough to determine if this is beingachieved, the most promising aspect of applying RL to fixed data sets is in learning tocombine the best aspects of each system in the data set.Although we believe that these results provide an accurate picture of the relativestrengths of the different types of systems we compare, it should be noted that thereliance on evaluation with simulated dialogues inevitably leads to some lack of pre-cision in the evaluation.
All these results are computed with users who have the samegoal (booking a return flight) and with an evaluation metric that only looks at dialoguelength and whether the four main slots were filled and (optionally) confirmed.
On the9 We previously reported results that showed that adding influence from reinforcement learning alwaysdegraded performance slightly compared to the pure SL policy (Henderson, Lemon, and Georgila 2005).However, these results were obtained with a preliminary version of the data annotation, which gavea less accurate indication of when slots were filled and confirmed.
The scores we are achieving withthe new data annotation (Georgila et al submitted) are all higher than those reported in Henderson,Lemon, and Geogila (2005), including the scores calculated from the data for the COMMUNICATORsystems themselves.10 To be more technically accurate, we can think of the SL policy as in effect asking each COMMUNICATORsystem for a probability distribution over state?action pairs for the current state, summing theseprobabilities across systems, and choosing the action with the highest probability.506Henderson, Lemon, and Georgila Hybrid Reinforcement/Supervised Learningother hand, all the systems were trained to handle a more complicated task than this,including multi-leg flights, hotel bookings, and rental-car bookings.
They were alsodesigned or trained to complete the task, rather than to fill the slots.
Therefore theevaluation does not reflect all the capabilities or behaviors of the systems.
However,there is no apparent reason to believe that this fact biases our results towards one typeof system or another.
This claim is easiest to support for the comparisons betweenthe hybrid method and the two trained baselines, pure RL and pure SL.
For all thesesystems, the same data was used to train the systems, the same user models wereused to generate simulated dialogues, and the same evaluation metric was applied tothese simulated dialogues.
For the comparisons between the hybrid method and theCOMMUNICATOR systems, only the evaluation metric is exactly the same, but the usermodels used for testing the hybrid method were trained to mimic exactly the usersin the dialogues used to evaluate the COMMUNICATOR systems.
Because we know ofno evaluation bias introduced when moving from real users to their simulation, weconclude that this comparison is also indicative of the relative performance of these twotypes of systems (particularly given the size of the improvement).A more general methodological objection could be raised against any evaluationthat uses simulated users.
Despite the substantial amount of dialogue system work thathas relied on simulated users (e.g., Scheffler and Young 2002; Pietquin 2004; Georgila,Henderson, and Lemon 2006; Schatzmann et al 2006), to date there has not been asystematic experiment that validates this methodology against results from humanusers.
However, in related work (Lemon, Georgila, and Henderson 2006), we havedemonstrated that a hybrid policy learned as proposed in this article performs betterthan a state-of-the-art hand-coded system in experiments with human users.
The exper-iments were done using the ?Town Information?multimodal dialogue system of Lemonet at.
(2006) and Lemon, Georgila, and Stuttle (2005).
The hybrid policy reported here(trained on the COMMUNICATOR data) was ported to this domain, and then evaluatedwith human subjects.
The learned policy achieved an average gain in perceived taskcompletion of 14.2% (from 67.6% to 81.8% at p < 0.03) compared to a state-of-the-arthand-coded system (Lemon, Georgila, and Henderson 2006).
This demonstrates that apolicy that performs well in simulation also performs well in real dialogues.11These experiments demonstrate improvements given an initial fixed data set whichhas been generated from existing systems.
For applications where there are no existingsystems, an alternative would be to generate the initial data with a Wizard-of-Ozexperiment, where a human plays the part of the system, as explored by Williams andYoung (2003) and Rieser and Lemon (2006b).
The methods proposed in this article canbe used to train a policy from such data without having to first build an initial system.4.
ConclusionsIn this article, we have investigated how reinforcement learning can be applied to learndialogue management policies with large action sets and very large state spaces givenonly a fixed data set of dialogues.
Under a variety of metrics, our proposed hybrid re-inforcement learning method outperforms both a policy trained with standard RL and a11 Future work is to port the hand-coded policy back to the COMMUNICATOR domain for use in simulation.This will investigate whether a relative improvement in simulated dialogues translates into a relativeimprovement in real dialogues.507Computational Linguistics Volume 34, Number 4policy trained with supervised learning, as well as the COMMUNICATOR systems whichgenerated the data it was trained on.
This performance is achieved despite the extremelychallenging task, with 74 actions to choose between, over 10386 possible states, andvery few hand-coded policy decisions.
The two main features of our model that makethis possible are the incorporation of supervised learning into a reinforcement learningmodel, and the use of linear function approximation with state features provided by theInformation State Update approach to dialogue management.
The supervised learningis used to avoid states not covered by the data set, and the linear function approximationis used to handle the very large state spaces.With such a large space of possible state?action pairs, and therefore a huge policyspace, pure reinforcement learning would require an enormous amount of data to findgood policies.
We have succeeded in using RL with fairly small data sets of only around1,000 dialogues (in the portion used for training).
This is achieved by using supervisedlearning to model when an action would lead to a state for which we do not haveenough data.
We proposed two methods for estimating a default value for these unseenstates, and derived a principled way to combine this value with the value estimated byRL, using the probability provided by SL to weight this combination.
This gave us twohybrid RL/SL methods, both of which outperform both the RL and SL policies alone.The best hybrid policy performs 302% better than the standard RL policy, and 1.4%better than the SL policy, according to our automatic evaluation method.
In addition,according to our automatic evaluation method, the hybrid RL/SL policy outperformsthe systems used to generate the data.
The best hybrid policy improves over the averageCOMMUNICATOR system policy by 10% on our metric.
This good performance hasbeen corroborated in separate experiments with human subjects (Lemon, Georgila, andHenderson 2006), where the learned policy outperforms a state-of-the-art hand-codedsystem.The success of the hybrid method (and of pure supervised learning) on this chal-lenging task indicates that linear function approximation is a viable approach to the verylarge state spaces produced by the ISU framework.
It also demonstrates the utility of afeature-based representation of states, such as that used in the ISU approach.
Furtherimprovement should be possible by tailoring the representation of states and actionsbased on our experience so far (e.g., by including information about specific sequencesof moves), and by using automatic feature selection techniques.
We should also be ableto get some improvement from more sophisticated function approximation methods,such as kernel-based methods.The next step is to better exploit the advantages of reinforcement learning.
Onepromising approach is to apply RL while running the learned policy against simulatedusers, thereby allowing RL to explore parts of the policy and state spaces that arenot included in the COMMUNICATOR data.
The hybrid policy we have learned on theCOMMUNICATOR data is a good starting point for this exploration.
Also, the supervisedcomponent within the hybrid system can be used to constrain the range of policiesthat need to be explored when training the RL component.
All of these advances willimprove techniques for bootstrapping and automatic optimization of dialogue manage-ment policies from limited initial data sets.AcknowledgmentsThis work was partially supported by theEuropean Commission under the FP6 project?TALK: Talk and Look, Tools for AmbientLinguistic Knowledge?
(507802) and the FP7project ?CLASSIC: ComputationalLearning in Adaptive Systems for SpokenConversation?
(216594), by the EPSRCunder grant EP/E019501/1, and by SHEFCHR04016?Wellcome Trust VIP Award.508Henderson, Lemon, and Georgila Hybrid Reinforcement/Supervised LearningWe thank Johanna Moore for proposingthe use of the COMMUNICATOR data set forthis work.ReferencesBohlin, Peter, Robin Cooper, ElisabetEngdahl, and Staffan Larsson.
1999.Information states and dialog moveengines.
Electronic Transactions in AI,3(9).
Available at www.ep.liu.se/ej/etai/1999/D/.Bos, Johan, Ewan Klein, Oliver Lemon, andTetsushi Oka.
2003.
DIPPER: Descriptionand formalisation of an information-stateupdate dialogue system architecture.
InProceedings of the 4th SIGdial Workshop onDiscourse and Dialogue, pages 115?124,Sapporo.Cheyer, Adam and David Martin.
2001.The open agent architecture.
Journal ofAutonomous Agents and Multi-AgentSystems, 4(1/2):143?148.Denecke, Matthias, Kohji Dohsaka, andMikio Nakano, 2005.
Fast reinforcementlearning of dialogue policies using stablefunction approximation.
In K. Y. Su,J.
Tsujii, J.-H. Lee, and O. Y. Kwong,Natural Language Processing, IJCNLP 2004.Springer, Berlin, pages 1?11.Frampton, Matthew and Oliver Lemon.2005.
Reinforcement learning of dialoguestrategies using the user?s last dialogueact.
In Proceedings of the 4th Workshop onKnowledge and Reasoning in Practical DialogSystems, International Joint Conference onArtificial Intelligence (IJCAI), pages 83?90,Edinburgh.Frampton, Matthew and Oliver Lemon.2006.
Learning more effective dialoguestrategies using limited dialogue movefeatures.
In Proceedings of the 44th Meetingof the Association for ComputationalLinguistics, pages 185?192, Sydney.Gabsdil, Malte and Oliver Lemon.
2004.Combining acoustic and pragmaticfeatures to predict recognitionperformance in spoken dialogue systems.In Proceedings of the 42nd Meeting of theAssociation for Computational Linguistics,pages 344?351, Barcelona.Georgila, Kallirroi, James Henderson, andOliver Lemon.
2005.
Learning usersimulations for Information StateUpdate dialogue systems.
In Proceedingsof the 9th European Conference onSpeech Communication and Technology(Interspeech ?
Eurospeech), pages 893?896,Lisbon.Georgila, Kallirroi, James Henderson, andOliver Lemon.
2006.
User simulationfor spoken dialogue systems: Learningand evaluation.
In Proceedings of the9th International Conference on SpokenLanguage Processing (Interspeech ?
ICSLP),pages 1065?1068, Pittsburgh, PA.Georgila, Kallirroi, Oliver Lemon, andJames Henderson.
2005.
Automaticannotation of COMMUNICATORdialogue data for learning dialoguestrategies and user simulations.In Proceedings of the Ninth Workshopon the Semantics and Pragmatics ofDialogue (SEMDIAL), pages 61?68,Nancy.Georgila, Kallirroi, Oliver Lemon, JamesHenderson, and Johanna Moore.(submitted).
Automatic annotation ofcontext and speech acts for dialoguecorpora.Goddeau, D. and J. Pineau.
2000.
Fastreinforcement learning of dialog strategies.In Proceedings of the IEEE InternationalConference on Acoustics Speech and SignalProcessing (ICASSP), pages II?1233?1236,Istanbul.Henderson, James, Oliver Lemon, andKallirroi Georgila.
2005.
Hybridreinforcement/supervised learning fordialogue policies from COMMUNICATORdata.
In Proceedings of the 4th Workshop onKnowledge and Reasoning in Practical DialogSystems, International Joint Conference onArtificial Intelligence (IJCAI), pages 68?75,Edinburgh.Larsson, Staffan and David Traum.
2000.Information state and dialoguemanagement in the TRINDI DialogueMove Engine Toolkit.
Natural LanguageEngineering, 6(3?4):323?340.Lemon, Oliver, Kallirroi Georgila, and JamesHenderson.
2006.
Evaluating effectivenessand portability of reinforcement learneddialogue strategies with real users: theTALK TownInfo evaluation.
In Proceedingsof the IEEE/ACL 2006 Workshop on SpokenLanguage Technology, pages 178?181,Aruba.Lemon, Oliver, Kallirroi Georgila, JamesHenderson, Malte Gabsdil, IvanMeza-Ruiz, and Steve Young.
2005.Integration of learning and adaptivity withthe ISU approach.
Technical Report D4.1,TALK Project.Lemon, Oliver, Kallirroi Georgila, JamesHenderson, and Matthew Stuttle.
2006.An ISU dialogue system exhibitingreinforcement learning of dialoguepolicies: generic slot-filling in the TALK509Computational Linguistics Volume 34, Number 4in-car system.
In Proceedings of theDemonstrations of EACL, pages 119?122,Trento.Lemon, Oliver, Kallirroi Georgila, andMatthew Stuttle.
2005.
Showcaseexhibiting reinforcement learning fordialogue strategies in the in-car domain.Technical Report D4.2, TALK Project.Levin, Esther and Roberto Pieraccini.
1997.A stochastic model of computer-humaninteraction for learning dialogue strategies.In Proceedings of the 5th European Conferenceon Speech Communication and Technology(Interspeech ?
Eurospeech), pages 1883?1886,Rhodes.Levin, Esther, Roberto Pieraccini, andWieland Eckert.
2000.
A stochastic modelof human-machine interaction for learningdialog strategies.
IEEE Transactions onSpeech and Audio Processing, 8(1):11?23.Pietquin, Olivier.
2004.
A Framework forUnsupervised Learning of Dialogue Strategies.Presses Universitaires de Louvain,SIMILAR Collection.Pietquin, Olivier and Thierry Dutoit.
2006a.Dynamic Bayesian networks for NLUsimulation with application to dialogoptimal strategy learning.
In Proceedingsof the IEEE International Conference onAcoustics Speech and Signal Processing(ICASSP), pages 49?52, Toulouse.Pietquin, Olivier and Thierry Dutoit.
2006b.A probabilistic framework for dialogsimulation and optimal strategy learning.IEEE Transactions on Speech and AudioProcessing, 14(2):589?599.Rieser, Verena and Oliver Lemon.
2006a.Cluster-based user simulations forlearning dialogue strategies and theSUPER evaluation metric.
In Proceedingsof the 9th International Conference on SpokenLanguage Processing (Interspeech ?
ICSLP),pages 1766?1769, Pittsburgh, PA.Rieser, Verena and Oliver Lemon.
2006b.Using logistic regression to initialisereinforcement-learning-based dialoguesystems.
In Proceedings of the IEEE/ACL2006 Workshop on Spoken LanguageTechnology, pages 190?193, Aruba.Rieser, Verena and Oliver Lemon.
2006c.Using machine learning to explorehuman multimodal clarificationstrategies.
In Proceedings of the PosterSession of the 44th Meeting of theAssociation for Computational Linguistics,pages 659?666, Sydney.Roy, Nicholas, Joelle Pineau, and SebastianThrun.
2000.
Spoken dialog managementfor robots.
In Proceedings of the 38th Meetingof the Association for ComputationalLinguistics, pages 93?100, Hong Kong.Schatzmann, Jost, Kallirroi Georgila,and Steve Young.
2005.
Quantitativeevaluation of user simulation techniquesfor spoken dialogue systems.
InProceedings of the 6th SIGdial Workshopon Discourse and Dialogue, pages 45?54,Lisbon.Schatzmann, Jost, Matthew N. Stuttle,Karl Weilhammer, and Steve Young.2005.
Effects of the user model onsimulation-based learning of dialoguestrategies.
In Proceedings of the IEEEAutomatic Speech Recognition andUnderstanding Workshop, pages 220?225,San Juan, Puerto Rico.Schatzmann, Jost, Karl Weilhammer,Matthew N. Stuttle, and Steve Young.2006.
A survey of statistical usersimulation techniques forreinforcement-learning of dialoguemanagement strategies.
The KnowledgeEngineering Review, 21:97?126.Scheffler, Konrad and Steve Young.
2001.Corpus-based dialogue simulationfor automatic strategy learning andevaluation.
In Proceedings of the NAACLWorkshop on Adaptation in DialogueSystems, pages 64?70, Pittsburgh, PA.Scheffler, Konrad and Steve Young.2002.
Automatic learning of dialoguestrategy using dialogue simulationand reinforcement learning.
In Proceedingsof the Human Language TechnologyConference, pages 12?19, San Diego, CA.Shawe-Taylor, John and Nello Cristianini.2004.
Kernel Methods for Pattern Analysis.Cambridge University Press.Singh, Satinder, Michael Kearns, DianeLitman, and Marilyn Walker.
2000a.Empirical evaluation of a reinforcementlearning dialogue system.
In Proceedingsof the AAAI, pages 645?651, Whistler.Singh, Satinder, Michael Kearns, DianeLitman, and Marilyn Walker.
2000b.Reinforcement learning for spokendialogue systems.
In Advances in NeuralInformation Processing Systems, 12:956?962.Singh, Satinder, Diane Litman, MichaelKearns, and Marilyn Walker.
2002.Optimizing dialogue management withreinforcement learning: Experimentswith the NJFun system.
Journal ofArtificial Intelligence Research (JAIR),16:105?133.Sutton, Richard and Andrew Barto.
1998.Reinforcement Learning.
MIT Press,Cambridge, MA.510Henderson, Lemon, and Georgila Hybrid Reinforcement/Supervised LearningWalker, M., J. Aberdeen, J. Boland, E. Bratt,J.
Garofolo, L. Hirschman, A.
Le, S. Lee,S.
Narayanan, K. Papineni, B. Pellom,B.
Polifroni, A. Potamianos, P. Prabhu,A.
Rudnicky, G. Sanders, S. Seneff,D.
Stallard, and S. Whittaker.
2001.DARPA communicator dialog travelplanning systems: The June 2000 datacollection.
In Proceedings of the 7th EuropeanConference on Speech Communication andTechnology (Interspeech ?
Eurospeech),pages 1371?1374, Aalborg.Walker, M. and R. Passonneau.
2001.
DATE:A dialogue act tagging scheme forevaluation of spoken dialogue systems.In Proceedings of the Human LanguageTechnology Conference, pages 1?8, SanDiego, CA.Walker, M., A. Rudnicky, J. Aberdeen,E.
Bratt, J. Garofolo, H. Hastie, A. Le,B.
Pellom, A. Potamianos, R. Passonneau,R.
Prasad, S. Roukos, G. Sanders, S. Seneff,D.
Stallard, and S. Whittaker.
2002a.DARPA Communicator Evaluation:Progress from 2000 to 2001.
In Proceedingsof the 7th International Conference on SpokenLanguage Processing (Interspeech ?
ICSLP),pages 273?276, Denver, CO.Walker, M., A. Rudnicky, R. Prasad,J.
Aberdeen, E. Bratt, J. Garofolo,H.
Hastie, A.
Le, B. Pellom, A. Potamianos,R.
Passonneau, S. Roukos, G. Sanders,S.
Seneff, and D. Stallard.
2002b.
DARPACommunicator: Cross-system resultsfor the 2001 evaluation.
In Proceedings ofthe 7th International Conference on SpokenLanguage Processing (Interspeech ?
ICSLP),pages 269?272, Denver, CO.Walker, Marilyn A., Jeanne C. Fromer, andShrikanth Narayanan.
1998.
Learningoptimal dialogue strategies: A case studyof a spoken dialogue agent for email.In Proceedings of the 17th InternationalConference on Computational Linguistics,pages 1345?1351, Montreal.Walker, Marilyn A., Candace A. Kamm,and Diane J. Litman.
2000.
Towardsdeveloping general models of usabilitywith PARADISE.
Natural LanguageEngineering, 6(3):363?377.Walker, Marilyn A., Rebecca J. Passonneau,and Julie E. Boland.
2001.
Quantitativeand qualitative evaluation of DARPACommunicator spoken dialogue systems.In Proceedings of the 39th Meeting of theAssociation for Computational Linguistics,pages 515?522, Toulouse.Williams, Jason.
2007.
A method forevaluating and comparing usersimulations: The Cramer-von Misesdivergence.
In Proceedings of the IEEEAutomatic Speech Recognition andUnderstanding Workshop, pages 508?513,Kyoto.Williams, Jason, Pascal Poupart, andSteve Young.
2005a.
Factored partiallyobservable Markov decision processes fordialogue management.
In Proceedings of the4th Workshop on Knowledge and Reasoning inPractical Dialog Systems, International JointConference on Artificial Intelligence (IJCAI),pages 76?82, Edinburgh.Williams, Jason, Pascal Poupart, and SteveYoung.
2005b.
Partially observable Markovdecision processes with continuousobservations for dialogue management.In Proceedings of the 6th SIGdial Workshopon Discourse and Dialogue, pages 25?34,Lisbon.Williams, Jason and Steve Young.
2003.
UsingWizard-of-Oz simulations to bootstrapreinforcement-learning-based dialogmanagement systems.
In Proceedings of the4th SIGdial Workshop on Discourse andDialogue, pages 135?139, Sapporo.Williams, Jason and Steve Young.
2005.Scaling up POMDPs for dialogmanagement: The ?Summary POMDP?method.
In Proceedings of the IEEEAutomatic Speech Recognition andUnderstanding Workshop, pages 177?182,San Juan, Puerto Rico.Young, Steve.
2000.
Probabilistic methods inspoken dialogue systems.
PhilosophicalTransactions of the Royal Society (Series A),358(1769):1389?1402.511
