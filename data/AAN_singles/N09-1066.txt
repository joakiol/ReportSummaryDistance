Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 584?592,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsUsing Citations to Generate Surveys of Scientific ParadigmsSaif Mohammad?
?, Bonnie Dorr??
?, Melissa Egan?
?, Ahmed Hassan?,Pradeep Muthukrishan?, Vahed Qazvinian?, Dragomir Radev?
?, David Zajic?Institute for Advanced Computer Studies?
and Computer Science?, University of Maryland.Human Language Technology Center of Excellence?.
Center for Advanced Study of Language.
{saif,bonnie,mkegan,dmzajic}@umiacs.umd.eduDepartment of Electrical Engineering and Computer Science?School of Information?, University of Michigan.
{hassanam,mpradeep,vahed,radev}@umich.eduAbstractThe number of research publications in var-ious disciplines is growing exponentially.Researchers and scientists are increasinglyfinding themselves in the position of havingto quickly understand large amounts of tech-nical material.
In this paper we present thefirst steps in producing an automatically gen-erated, readily consumable, technical survey.Specifically we explore the combination ofcitation information and summarization tech-niques.
Even though prior work (Teufel etal., 2006) argues that citation text is unsuitablefor summarization, we show that in the frame-work of multi-document survey creation, cita-tion texts can play a crucial role.1 IntroductionIn today?s rapidly expanding disciplines, scientistsand scholars are constantly faced with the dauntingtask of keeping up with knowledge in their field.
Inaddition, the increasingly interconnected nature ofreal-world tasks often requires experts in one dis-cipline to rapidly learn about other areas in a shortamount of time.Cross-disciplinary research requires scientists inareas such as linguistics, biology, and sociologyto learn about computational approaches and appli-cations, e.g., computational linguistics, biologicalmodeling, social networks.
Authors of journal ar-ticles and books must write accurate surveys of pre-vious work, ranging from short summaries of relatedresearch to in-depth historical notes.Interdisciplinary review panels are often calledupon to review proposals in a wide range of areas,some of which may be unfamiliar to panelists.
Thus,they must learn about a new discipline ?on the fly?in order to relate their own expertise to the proposal.Our goal is to effectively serve these needs bycombining two currently available technologies: (1)bibliometric lexical link mining that exploits thestructure of citations and relations among citations;and (2) summarization techniques that exploit thecontent of the material in both the citing and citedpapers.It is generally agreed upon that manually writtenabstracts are good summaries of individual papers.More recently, Qazvinian and Radev (2008) arguethat citation texts are useful in creating a summaryof the important contributions of a research paper.The citation text of a target paper is the set of sen-tences in other technical papers that explicitly referto it (Elkiss et al, 2008a).
However, Teufel (2005)argues that using citation text directly is not suitablefor document summarization.In this paper, we compare and contrast the use-fulness of abstracts and of citation text in automati-cally generating a technical survey on a given topicfrom multiple research papers.
The next section pro-vides the background for this work, including theprimary features of a technical survey and also thetypes of input that are used in our study (full pa-pers, abstracts, and citation texts).
Following this,we describe related work and point out the advancesof our work over previous work.
We then describehow citation texts are used as a new input for multi-document summarization to produce surveys of agiven technical area.
We apply four different sum-marization techniques to data in the ACL Anthol-584ogy and evaluate our results using both automatic(ROUGE) and human-mediated (nugget-based pyra-mid) measures.
We observe that, as expected, ab-stracts are useful in survey creation, but, notably, wealso conclude that citation texts have crucial survey-worthy information not present in (or at least, noteasily extractable from) abstracts.
We further dis-cover that abstracts are author-biased and thus com-plementary to the broader perspective inherent in ci-tation texts; these differences enable the use of arange of different levels and types of information inthe survey?the extent of which is subject to surveylength restrictions (if any).2 BackgroundAutomatically creating technical surveys is sig-nificantly distinct from that of traditional multi-document summarization.
Below we describe pri-mary characteristics of a technical survey and wepresent three types of input texts that we used forthe production of surveys.2.1 Technical SurveyIn the case of multi-document summarization, thegoal is to produce a readable presentation of mul-tiple documents, whereas in the case of technicalsurvey creation, the goal is to convey the key fea-tures of a particular field, basic underpinnings of thefield, early and late developments, important con-tributions and findings, contradicting positions thatmay reverse trends or start new sub-fields, and ba-sic definitions and examples that enable rapid un-derstanding of a field by non-experts.A prototypical example of a technical survey isthat of ?chapter notes,?
i.e., short (50?500 word)descriptions of sub-areas found at the end of chap-ters of textbook, such as Jurafsky and Martin (2008).One might imagine producing such descriptions au-tomatically, then hand-editing them and refiningthem for use in an actual textbook.We conducted a human analysis of these chapternotes that revealed a set of conventions, an outlineof which is provided here (with example sentencesin italics):1.
Introductory/opening statement: The earliestcomputational use of X was in Y, considered bymany to be the foundational work in this area.2.
Definitional follow up: X is def ined as Y.3.
Elaboration of definition (e.g., with an exam-ple): Most early algorithms were based on Z.4.
Deeper elaboration, e.g., pointing out issueswith initial approaches: Unfortunately, thismodel seems to be wrong.5.
Contrasting definition: Algorithms since then...6.
Introduction of additional specific instances /historical background with citations: Two clas-sic approaches are described in Q.7.
References to other summaries: R provides acomprehensive guide to the details behind X.The notion of text level categories or zoningof technical papers?related to the survey compo-nents enumerated above?has been investigated pre-viously in the work of Nanba and Kan (2004b) andTeufel (2002).
These earlier works focused on theanalysis of scientific papers based on their rhetori-cal structure and on determining the portions of pa-pers that contain new results, comparisons to ear-lier work, etc.
The work described in this paper fo-cuses on the synthesis of technical surveys based onknowledge gleaned from rhetorical structure not un-like that of the work of these earlier researchers, butperhaps guided by structural patterns along the linesof the conventions listed above.Although our current approach to survey creationdoes not yet incorporate a fully pattern-based com-ponent, our ultimate objective is to apply these pat-terns to guide the creation and refinement of the finaloutput.
As a first step toward this goal, we use cita-tion texts (closest in structure to the patterns iden-tified by convention 7 above) to pick out the mostimportant content for survey creation.2.2 Full papers, abstracts, and citation textsPublished research on a particular topic can be sum-marized from two different kinds of sources: (1)where an author describes her own work and (2)where others describe an author?s work (usually inrelation to their own work).
The author?s descrip-tion of her own work can be found in her paper.
Howothers perceive her work is spread across other pa-pers that cite her work.
We will refer to the set ofsentences that explicitly mention a target paper Y asthe citation text of Y.585Traditionally, technical survey generation hasbeen tackled by summarizing a set of research pa-pers pertaining to the topic.
However, individual re-search papers usually come with manually-created?summaries?
?their abstracts.
The abstract of a pa-per may have sentences that set the context, state theproblem statement, mention how the problem is ap-proached, and the bottom-line results?all in 200 to500 words.
Thus, using only the abstracts (insteadof full papers) as input to a summarization system isworth exploring.Whereas the abstract of a paper presents what theauthors think to be the important contributions of apaper, the citation text of a paper captures what oth-ers in the field perceive as the contributions of thepaper.
The two perspectives are expected to havesome overlap in their content, but the citation textalso contains additional information not found in ab-stracts (Elkiss et al, 2008a).
For example, how aparticular methodology (described in one paper) wascombined with another (described in a different pa-per) to overcome some of the drawbacks of each.A citation text is also an indicator of what contri-butions described in a paper were more influentialover time.
Another distinguishing feature of citationtexts in contrast to abstracts is that a citation texttends to have a certain amount of redundant informa-tion.
This is because multiple papers may describethe same contributions of a target paper.
This redun-dancy can be exploited to determine the importantcontributions of the target paper.Our goal is to test the hypothesis that an ef-fective technical survey will reflect information onresearch not only from the perspective of its au-thors but also from the perspective of others whouse/commend/discredit/add to it.
Before describ-ing our experiments with technical papers, abstracts,and citation texts, we first summarize relevant priorwork that used these sources of information as input.3 Related workPrevious work has focused on the analysis of cita-tion and collaboration networks (Teufel et al, 2006;Newman, 2001) and scientific article summarization(Teufel and Moens, 2002).
Bradshaw (2003) usedcitation texts to determine the content of articles andimprove the results of a search engine.
Citationtexts have also been used to create summaries of sin-gle scientific articles in Qazvinian and Radev (2008)and Mei and Zhai (2008).
However, there is no pre-vious work that uses the text of the citations to pro-duce a multi-document survey of scientific articles.Furthermore, there is no study contrasting the qual-ity of surveys generated from citation summaries?both automatically and manually produced?to sur-veys generated from other forms of input such as theabstracts or full texts of the source articles.Nanba and Okumura (1999) discuss citation cate-gorization to support a system for writing a survey.Nanba et al (2004a) automatically categorize cita-tion sentences into three groups using pre-definedphrase-based rules.
Based on this categorization asurvey generation tool is introduced in Nanba et al(2004b).
They report that co-citation (where bothpapers are cited by many other papers) implies sim-ilarity by showing that the textual similarity of co-cited papers is proportional to the proximity of theircitations in the citing article.Elkiss et al (2008b) conducted several exper-iments on a set of 2,497 articles from the freePubMed Central (PMC) repository.1 Results fromthis experiment confirmed that the cohesion of a ci-tation text of an article is consistently higher thanthe that of its abstract.
They also concluded that ci-tation texts contain additional information are morefocused than abstracts.Nakov et al (2004) use sentences surrounding ci-tations to create training and testing data for seman-tic analysis, synonym set creation, database cura-tion, document summarization, and information re-trieval.
Kan et al (2002) use annotated bibliogra-phies to cover certain aspects of summarization andsuggest using metadata and critical document fea-tures as well as the prominent content-based featuresto summarize documents.
Kupiec et al (1995) use astatistical method and show how extracts can be usedto create summaries but use no annotated metadatain summarization.Siddharthan and Teufel (2007) describe a newreference task and show high human agreement aswell as an improvement in the performance of ar-gumentative zoning (Teufel, 2005).
In argumenta-tive zoning?a rhetorical classification task?seven1http://www.pubmedcentral.gov586classes (Own, Other, Background, Textual, Aim,Basis, and Contrast) are used to label sentences ac-cording to their role in the author?s argument.Our aim is not only to determine the utility of cita-tion texts for survey creation, but also to examine thequality distinctions between this form of input andothers such as abstracts and full texts?comparingthe results to human-generated surveys using bothautomatic and nugget-based pyramid evaluation(Lin and Demner-Fushman, 2006; Nenkova and Pas-sonneau, 2004; Lin, 2004).4 Summarization systemsWe used four summarization systems for oursurvey-creation approach: Trimmer, LexRank, C-LexRank, and C-RR.
Trimmer is a syntactically-motivated parse-and-trim approach.
LexRank is agraph-based similarity approach.
C-LexRank and C-RR use graph clustering (?C?
stands for clustering).We describe each of these, in turn, below.4.1 TrimmerTrimmer is a sentence-compression tool that extendsthe scope of an extractive summarization system bygenerating multiple alternative sentence compres-sions of the most important sentences in target doc-uments (Zajic et al, 2007).
Trimmer compressionsare generated by applying linguistically-motivatedrules to mask syntactic components of a parse of asource sentence.
The rules can be applied iterativelyto compress sentences below a configurable lengththreshold, or can be applied in all combinations togenerate the full space of compressions.Trimmer can leverage the output of any con-stituency parser that uses the Penn Treebank con-ventions.
At present, the Stanford Parser (Klein andManning, 2003) is used.
The set of compressionsis ranked according to a set of features that may in-clude metadata about the source sentences, details ofthe compression process that generated the compres-sion, and externally calculated features of the com-pression.Summaries are constructed from the highest scor-ing compressions, using the metadata and maximalmarginal relevance (Carbonell and Goldstein, 1998)to avoid redundancy and over-representation of asingle source.4.2 LexRankWe also used LexRank (Erkan and Radev, 2004), astate-of-the-art multidocument summarization sys-tem, to generate summaries.
LexRank first builds agraph of all the candidate sentences.
Two candidatesentences are connected with an edge if the similar-ity between them is above a threshold.
We used co-sine as the similarity metric with a threshold of 0.15.Once the network is built, the system finds the mostcentral sentences by performing a random walk onthe graph.The salience of a node is recursively defined onthe salience of adjacent nodes.
This is similar tothe concept of prestige in social networks, where theprestige of a person is dependent on the prestige ofthe people he/she knows.
However, since randomwalk may get caught in cycles or in disconnectedcomponents, we reserve a low probability to jumpto random nodes instead of neighbors (a techniquesuggested by Langville and Meyer (2006)).Note also that unlike the original PageRankmethod, the graph of sentences is undirected.
Thisupdated measure of sentence salience is called asLexRank.
The sentences with the highest LexRankscores form the summary.4.3 Cluster Summarizers: C-LexRank, C-RRTwo clustering methods proposed by Qazvinian andRadev (2008)?C-RR and C-LexRank?were usedto create summaries.
Both create a fully connectednetwork in which nodes are sentences and edges arecosine similarities.
A cutoff value of 0.1 is appliedto prune the graph and make a binary network.
Thelargest connected component of the network is thenextracted and clustered.Both of the mentioned summarizers cluster thenetwork similarly but use different approaches to se-lect sentences from different communities.
In C-RR sentences are picked from different clusters ina round robin (RR) fashion.
C-LexRank first calcu-lates LexRank within each cluster to find the mostsalient sentences of each community.
Then it picksthe most salient sentence of each cluster, and thenthe second most salient and so forth until the sum-mary length limit is reached.587Most of work in QA and paraphrasing focused on folding paraphrasing knowledge into question analyzer or answerlocator Rinaldi et al 2003; Tomuro, 2003.
In addition, number of researchers have built systems to take readingcomprehension examinations designed to evaluate children?s reading levels Charniak et al 2000; Hirschman et al1999; Ng et al 2000; Riloff and Thelen, 2000; Wang et al 2000. so-called ?
definition ?
or ?
other ?questions at recent TREC evalua - tions Voorhees, 2005 serve as good examples.
To better facilitate userinformation needs, recent trends in QA research have shifted towards complex, context-based, and interactivequestion answering Voorhees, 2001; Small et al 2003; Harabagiu et al 2005.
[And so on.
]Table 1: First few sentences of the QA citation texts survey generated by Trimmer.5 DataThe ACL Anthology is a collection of papers fromthe Computational Linguistics journal, and proceed-ings of ACL conferences and workshops.
It hasalmost 11,000 papers.
To produce the ACL An-thology Network (AAN), Joseph and Radev (2007)manually parsed the references before automaticallycompiling the network metadata, and generating ci-tation and author collaboration networks.
The AANincludes all citation and collaboration data withinthe ACL papers, with the citation network consist-ing of 11,773 nodes and 38,765 directed edges.Our evaluation experiments are on a set of papersin the research area of Question Answering (QA)and another set of papers on Dependency parsing(DP).
The two sets of papers were compiled by se-lecting all the papers in AAN that had the wordsQuestion Answering and Dependency Parsing, re-spectively, in the title and the content.
There were10 papers in the QA set and 16 papers in the DP set.We also compiled the citation texts for the 10 QApapers and the citation texts for the 16 DP papers.6 ExperimentsWe automatically generated surveys for both QAand DP from three different types of documents: (1)full papers from the QA and DP sets?QA and DPfull papers (PA), (2) only the abstracts of the QAand DP papers?QA and DP abstracts (AB), and(3) the citation texts corresponding to the QA andDP papers?QA and DP citations texts (CT).We generated twenty four (4x3x2) surveys,each of length 250 words, by applying Trimmer,LexRank, C-LexRank and C-RR on the three datatypes (citation texts, abstracts, and full papers) forboth QA and DP.
(Table 1 shows a fragment of oneof the surveys automatically generated from QA ci-tation texts.)
We created six (3x2) additional 250-word surveys by randomly choosing sentences fromthe citation texts, abstracts, and full papers of QAand DP.
We will refer to them as random surveys.6.1 EvaluationOur goal was to determine if citation texts do in-deed have useful information that one will want toput in a survey and if so, how much of this infor-mation is not available in the original papers andtheir abstracts.
For this we evaluated each of theautomatically generated surveys using two separateapproaches: nugget-based pyramid evaluation andROUGE (described in the two subsections below).Two sets of gold standard data were manually cre-ated from the QA and DP citation texts and abstracts,respectively:2 (1) We asked two impartial judges toidentify important nuggets of information worth in-cluding in a survey.
(2) We asked four fluent speak-ers of English to create 250-word surveys of thedatasets.
Then we determined how well the differ-ent automatically generated surveys perform againstthese gold standards.
If the citation texts have onlyredundant information with respect to the abstractsand original papers, then the surveys of citation textswill not perform better than others.6.1.1 Nugget-Based Pyramid EvaluationFor our first approach we used a nugget-basedevaluation methodology (Lin and Demner-Fushman,2006; Nenkova and Passonneau, 2004; Hildebrandtet al, 2004; Voorhees, 2003).
We asked three impar-tial annotators (knowledgeable in NLP but not affil-iated with the project) to review the citation textsand/or abstract sets for each of the papers in the QAand DP sets and manually extract prioritized lists2Creating gold standard data from complete papers is fairlyarduous, and was not pursued.588of 2?8 ?nuggets,?
or main contributions, suppliedby each paper.
Each nugget was assigned a weightbased on the frequency with which it was listed byannotators as well as the priority it was assignedin each case.
Our automatically generated surveyswere then scored based on the number and weightof the nuggets that they covered.
This evaluation ap-proach is similar to the one adopted by Qazvinianand Radev (2008), but adapted here for use in themulti-document case.The annotators had two distinct tasks for the QAset, and one for the DP set: (1) extract nuggets foreach of the 10 QA papers, based only on the citationtexts for those papers; (2) extract nuggets for eachof the 10 QA papers, based only on the abstracts ofthose papers; and (3) extract nuggets for each of the16 DP papers, based only on the citation texts forthose papers.3We obtained a weight for each nugget by revers-ing its priority out of 8 (e.g., a nugget listed withpriority 1 was assigned a weight of 8) and summingthe weights over each listing of that nugget.4To evaluate a given survey, we counted the num-ber and weight of nuggets that it covered.
Nuggetswere detected via the combined use of annotator-provided regular expressions and careful human re-view.
Recall was calculated by dividing the com-bined weight of covered nuggets by the combinedweight of all nuggets in the nugget set.
Precisionwas calculated by dividing the number of distinctnuggets covered in a survey by the number of sen-tences constituting that survey, with a cap of 1.
F-measure, the weighted harmonic mean of precisionand recall, was calculated with a beta value of 3 inorder to assign the greatest weight to recall.
Recallis favored because it rewards surveys that includehighly weighted (important) facts, rather than just a3We first experimented using only the QA set.
Then to showthat the results apply to other datasets, we asked human anno-tators for gold standard data on the DP citation texts.
Addi-tional experiments on DP abstracts were not pursued becausethis would have required additional human annotation effort toestablish a point we had already made with the QA set, i.e., thatabstracts are useful for survey creation.4Results obtained with other weighting schemes that ig-nored priority ratings and multiple mentions of a nugget by asingle annotator showed the same trends as the ones shown bythe selected weighting scheme, but the latter was a stronger dis-tinguisher among the four systems.Human Performance: Pyramid F-measureHuman1 Human2 Human3 Human4 AverageInput: QA citation surveysQA?CT nuggets 0.524 0.711 0.468 0.695 0.599QA?AB nuggets 0.495 0.606 0.423 0.608 0.533Input: QA abstract surveysQA?CT nuggets 0.542 0.675 0.581 0.669 0.617QA?AB nuggets 0.646 0.841 0.673 0.790 0.738Input: DP citation surveysDP?CT nuggets 0.245 0.475 0.378 0.555 0.413Table 2: Pyramid F-measure scores of human-createdsurveys of QA and DP data.
The surveys are evaluatedusing nuggets drawn from QA citation texts (QA?CT),QA abstracts (QA?AB), and DP citation texts (DP?CT).great number of facts.Table 2 gives the F-measure values of the 250-word surveys manually generated by humans.
Thesurveys were evaluated using the nuggets drawnfrom the QA citation texts, QA abstracts, and DP ci-tation texts.
The average of their scores (listed in therightmost column) may be considered a good scoreto aim for by the automatic summarization methods.Table 3 gives the F-measure values of the surveysgenerated by the four automatic summarizers, evalu-ated using nuggets drawn from the QA citation texts,QA abstracts, and DP citation texts.
The table alsoincludes results for the baseline random summaries.When we used the nuggets from the abstractsset for evaluation, the surveys created from ab-stracts scored higher than the corresponding surveyscreated from citation texts and papers.
Further, thebest surveys generated from citation texts outscoredthe best surveys generated from papers.
When weused the nuggets from citation sets for evaluation,the best automatic surveys generated from citationtexts outperform those generated from abstracts andfull papers.
All these pyramid results demonstratethat citation texts can contain useful information thatis not available in the abstracts or the original papers,and that abstracts can contain useful information thatis not available in the citation texts or full papers.Among the various automatic summarizers, Trim-mer performed best at this task, in two cases ex-ceeding the average human performance.
Note alsothat the random summarizer outscored the automaticsummarizers in cases where the nuggets were takenfrom a source different from that used to generatethe survey.
However, one or two summarizers stilltended to do well.
This indicates a difficulty in ex-589System Performance: Pyramid F-measureRandom C-LexRank C-RR LexRank TrimmerInput: QA citation surveysQA?CT nuggets 0.321 0.434 0.268 0.295 0.616QA?AB nuggets 0.305 0.388 0.349 0.320 0.543Input: QA abstract surveysQA?CT nuggets 0.452 0.383 0.480 0.441 0.404QA?AB nuggets 0.623 0.484 0.574 0.606 0.622Input: QA full paper surveysQA?CT nuggets 0.239 0.446 0.299 0.190 0.199QA?AB nuggets 0.294 0.520 0.387 0.301 0.290Input: DP citation surveysDP?CT nuggets 0.219 0.231 0.170 0.372 0.136Input: DP abstract surveysDP?CT nuggets 0.321 0.301 0.263 0.311 0.312Input: DP full paper surveysDP?CT nuggets 0.032 0.000 0.144 * 0.280Table 3: Pyramid F-measure scores of automatic surveys of QA and DP data.
The surveys are evaluated using nuggetsdrawn from QA citation texts (QA?CT), QA abstracts (QA?AB), and DP citation texts (DP?CT).
* LexRank is computationally intensive and so was not run on the DP-PA dataset (about 4000 sentences).Human Performance: ROUGE-2human1 human2 human3 human4 averageInput: QA citation surveysQA?CT refs.
0.1807 0.1956 0.0756 0.2019 0.1635QA?AB refs.
0.1116 0.1399 0.0711 0.1576 0.1201Input: QA abstract surveysQA?CT refs.
0.1315 0.1104 0.1216 0.1151 0.1197QA-AB refs.
0.2648 0.1977 0.1802 0.2544 0.2243Input: DP citation surveysDP?CT refs.
0.1550 0.1259 0.1200 0.1654 0.1416Table 4: ROUGE-2 scores obtained for each of the manu-ally created surveys by using the other three as reference.ROUGE-1 and ROUGE-L followed similar patterns.tracting the overlapping survey-worthy informationacross the two sources.6.1.2 ROUGE evaluationTable 4 presents ROUGE scores (Lin, 2004) ofeach of human-generated 250-word surveys againsteach other.
The average (last column) is what the au-tomatic surveys can aim for.
We then evaluated eachof the random surveys and those generated by thefour summarization systems against the references.Table 5 lists ROUGE scores of surveys when themanually created 250-word survey of the QA cita-tion texts, survey of the QA abstracts, and the surveyof the DP citation texts, were used as gold standard.When we use manually created citation textsurveys as reference, then the surveys gener-ated from citation texts obtained significantly bet-ter ROUGE scores than the surveys generated fromabstracts and full papers (p < 0.05) [RESULT 1].This shows that crucial survey-worthy informationpresent in citation texts is not available, or hard toextract, from abstracts and papers alone.
Further,the surveys generated from abstracts performed sig-nificantly better than those generated from the fullpapers (p < 0.05) [RESULT 2].
This shows that ab-stracts and citation texts are generally denser in sur-vey worthy information than full papers.When we use manually created abstract sur-veys as reference, then the surveys generatedfrom abstracts obtained significantly better ROUGEscores than the surveys generated from citation textsand full papers (p < 0.05) [RESULT 3].
Further, andmore importantly, the surveys generated from cita-tion texts performed significantly better than thosegenerated from the full papers (p < 0.05) [RESULT4].
Again, this shows that abstracts and citation textsare richer in survey-worthy information.
These re-sults also show that abstracts of papers and citationtexts have some overlapping information (RESULT2 and RESULT 4), but they also have a signifi-cant amount of unique survey-worthy information(RESULT 1 and RESULT 3).Among the automatic summarizers, C-LexRankand LexRank perform best.
This is unlike the resultsfound through the nugget-evaluation method, whereTrimmer performed best.
This suggests that Trim-590System Performance: ROUGE-2Random C-LexRank C-RR LexRank TrimmerInput: QA citation surveysQA?CT refs.
0.11561 0.17013 0.09522 0.13501 0.16984QA?AB refs.
0.08264 0.11653 0.07600 0.07013 0.10336Input: QA abstract surveysQA?CT refs.
0.04516 0.05892 0.06149 0.05369 0.04114QA?AB refs.
0.12085 0.13634 0.12190 0.20311 0.13357Input: QA full paper surveysQA?CT refs.
0.03042 0.03606 0.03599 0.28244 0.03986QA?AB refs.
0.04621 0.05901 0.04976 0.10540 0.07505Input: DP citation surveysDP?CT refs.
0.10690 0.13164 0.08748 0.04901 0.10052Input: DP abstract surveysDP?CT refs.
0.07027 0.07321 0.05318 0.20311 0.07176Input: DP full paper surveysDP?CT refs.
0.03770 0.02511 0.03433 * 0.04554Table 5: ROUGE-2 scores of automatic surveys of QA and DP data.
The surveys are evaluated by using humanreferences created from QA citation texts (QA?CT), QA abstracts (QA?AB), and DP citation texts (DP?CT).
Theseresults are obtained after Jack-knifing the human references so that the values can be compared to those in Table 4.
* LexRank is computationally intensive and so was not run on the DP full papers set (about 4000 sentences).mer is better at identifying more useful nuggets ofinformation, but C-LexRank and LexRank are bet-ter at producing unigrams and bigrams expected ina survey.
To some extent this may be due to the factthat Trimmer uses smaller (trimmed) fragments ofsource sentences in its summaries.7 ConclusionIn this paper, we investigated the usefulness of di-rectly summarizing citation texts (sentences that citeother papers) in the automatic creation of technicalsurveys.
We generated surveys of a set of Ques-tion Answering (QA) and Dependency Parsing (DP)papers, their abstracts, and their citation texts us-ing four state-of-the-art summarization systems (C-LexRank, C-RR, LexRank, and Trimmer).
We thenused two separate approaches, nugget-based pyra-mid and ROUGE, to evaluate the surveys.
The re-sults from both approaches and all four summa-rization systems show that both citation texts andabstracts have unique survey-worthy information.These results also demonstrate that, unlike singledocument summarization (where citing sentenceshave been suggested to be inappropriate (Teufelet al, 2006)), multidocument summarization?especially technical survey creation?benefits con-siderably from citation texts.We next plan to generate surveys using both cita-tion texts and abstracts together as input.
Given theoverlapping content of abstracts and citation texts,discovered in the current study, it is clear that re-dundancy detection will be an integral component ofthis future work.
Creating readily consumable sur-veys is a hard task, especially when using only rawtext and simple summarization techniques.
There-fore we intend to combine these summarization andbibliometric techniques with suitable visualizationmethods towards the creation of iterative technicalsurvey tools?systems that present surveys and bib-liometric links in a visually convenient manner andwhich incorporate user feedback to produce evenbetter surveys.AcknowledgmentsThis work was supported, in part, by the NationalScience Foundation under Grant No.
IIS-0705832(iOPENER: Information Organization for PENningExpositions on Research) and Grant No.
0534323(Collaborative Research: BlogoCenter - Infrastruc-ture for Collecting, Mining and Accessing Blogs),in part, by the Human Language Technology Cen-ter of Excellence, and in part, by the Center for Ad-vanced Study of Language (CASL).
Any opinions,findings, and conclusions or recommendations ex-pressed in this material are those of the authors anddo not necessarily reflect the views of the sponsors.591ReferencesShannon Bradshaw.
2003.
Reference directed indexing:Redeeming relevance for subject search in citation in-dexes.
In Proceedings of the 7th European Conferenceon Research and Advanced Technology for Digital Li-braries.Jaime G. Carbonell and Jade Goldstein.
1998.
The useof mmr, diversity-based reranking for reordering doc-uments and producing summaries.
In Proceedings of21st Annual International ACM SIGIR Conference onResearch and Development in Information Retrieval,pages 335?336, Melbourne, Australia.Aaron Elkiss, Siwei Shen, Anthony Fader, Gu?nes?
Erkan,David States, and Dragomir R. Radev.
2008a.
Blindmen and elephants: What do citation summaries tellus about a research article?
Journal of the Ameri-can Society for Information Science and Technology,59(1):51?62.Aaron Elkiss, Siwei Shen, Anthony Fader, Gu?nes?
Erkan,David States, and Dragomir R. Radev.
2008b.
Blindmen and elephants: What do citation summaries tellus about a research article?
Journal of the Ameri-can Society for Information Science and Technology,59(1):51?62.Gu?nes?
Erkan and Dragomir R. Radev.
2004.
Lexrank:Graph-based centrality as salience in text summariza-tion.
Journal of Artificial Intelligence Research.Wesley Hildebrandt, Boris Katz, and Jimmy Lin.
2004.Overview of the trec 2003 question-answering track.In Proceedings of the 2004 Human Language Tech-nology Conference and the North American Chapterof the Association for Computational Linguistics An-nual Meeting (HLT/NAACL 2004).Mark Joseph and Dragomir Radev.
2007.
Citation analy-sis, centrality, and the ACL Anthology.
Technical Re-port CSE-TR-535-07, University of Michigan.
Dept.of Electrical Engineering and Computer Science.Daniel Jurafsky and James H. Martin.
2008.
Speechand Language Processing: An Introduction to NaturalLanguage Processing, Speech Recognition, and Com-putational Linguistics (2nd edition).
Prentice-Hall.Min-Yen Kan, Judith L. Klavans, and Kathleen R. McK-eown.
2002.
Using the Annotated Bibliography as aResource for Indicative Summarization.
In Proceed-ings of LREC 2002, Las Palmas, Spain.Dan Klein and Christopher D. Manning.
2003.
Accu-rate unlexicalized parsing.
In Proceedings of the 41stMeeting of ACL, pages 423?430.Julian Kupiec, Jan Pedersen, and Francine Chen.
1995.A trainable document summarizer.
In SIGIR ?95,pages 68?73, New York, NY, USA.
ACM.Amy Langville and Carl Meyer.
2006.
Google?s PageR-ank and Beyond: The Science of Search Engine Rank-ings.
Princeton University Press.Jimmy J. Lin and Dina Demner-Fushman.
2006.
Meth-ods for automatically evaluating answers to complexquestions.
Information Retrieval, 9(5):565?587.Chin-Yew Lin.
2004.
Rouge: A package for automaticevaluation of summaries.
In Proceedings of the ACLworkshop on Text Summarization Branches Out.Qiaozhu Mei and ChengXiang Zhai.
2008.
Generatingimpact-based summaries for scientific literature.
InProceedings of ACL ?08, pages 816?824.Preslav I. Nakov, Schwartz S. Ariel, and Hearst A. Marti.2004.
Citances: Citation sentences for semantic anal-ysis of bioscience text.
In Workshop on Search andDiscovery in Bioinformatics.Hidetsugu Nanba and Manabu Okumura.
1999.
Towardsmulti-paper summarization using reference informa-tion.
In IJCAI1999, pages 926?931.Hidetsugu Nanba, Takeshi Abekawa, Manabu Okumura,and Suguru Saito.
2004a.
Bilingual presri: Integrationof multiple research paper databases.
In Proceedingsof RIAO 2004, pages 195?211, Avignon, France.Hidetsugu Nanba, Noriko Kando, and Manabu Okumura.2004b.
Classification of research papers using cita-tion links and citation types: Towards automatic re-view article generation.
In Proceedings of the 11thSIG Classification Research Workshop, pages 117?134, Chicago, USA.Ani Nenkova and Rebecca Passonneau.
2004.
Evaluat-ing content selection in summarization: The pyramidmethod.
Proceedings of the HLT-NAACL conference.Mark E. J. Newman.
2001.
The structure of scientificcollaboration networks.
PNAS, 98(2):404?409.Vahed Qazvinian and Dragomir R. Radev.
2008.
Scien-tific paper summarization using citation summary net-works.
In COLING 2008, Manchester, UK.Advaith Siddharthan and Simone Teufel.
2007.
Whoseidea was this, and why does it matter?
attribut-ing scientific work to citations.
In Proceedings ofNAACL/HLT-07.Simone Teufel and Marc Moens.
2002.
Summariz-ing scientific articles: experiments with relevance andrhetorical status.
Comput.
Linguist., 28(4):409?445.Simone Teufel, Advaith Siddharthan, and Dan Tidhar.2006.
Automatic classification of citation function.
InProceedings of EMNLP, pages 103?110, Australia.Simone Teufel.
2005.
Argumentative Zoning for Im-proved Citation Indexing.
Computing Attitude and Af-fect in Text: Theory and Applications, pages 159?170.Ellen M. Voorhees.
2003.
Overview of the trec 2003question answering track.
In Proceedings of theTwelfth Text Retrieval Conference (TREC 2003).David M. Zajic, Bonnie J. Dorr, Jimmy Lin, and RichardSchwartz.
2007.
Multi-candidate reduction: Sentencecompression as a tool for document summarizationtasks.
Information Processing and Management (Spe-cial Issue on Summarization).592
