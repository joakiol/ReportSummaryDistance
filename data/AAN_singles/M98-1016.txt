DESCRIPTION OF THE KENT RIDGE DIGITAL LABS SYSTEMUSED FOR MUC-7Shihong Yu, Shuanhu Bai and Paul WuKent Ridge Digital Labs21 Heng Mui Keng TerraceSingapore 119613Email: shyu@krdl.org.sg, bai@krdl.org.sg, paulwu@krdl.org.sgBASIC OF THE SYSTEMWe aim to build a single simple framework for tasks in text information extraction, forwhich, to a certain extent, the required information can be resolved locally.Our system is statistics-based.
As usual, language model is built from training corpus.This is the so-called learning process.
Much eort has been spent to absorb domain knowl-edge in the language model in a systematic and generic way, because the system is designednot for one particular task, but for general local information extraction.For the information extraction part (tagging), the system consists of the following mod-ules: Sentence segmentor and tokenizer.
This module accepts a stream of characters asinput, and transforms it into a sequence of sentences and tokens.
The way of tok-enization can vary with dierent tasks and domains.
For example, most English textis tokenized in the same way, while tokenization in Chinese itself is a research topic. Text analyzer.
This module provides analysis necessary for the particular task, beit semantic, syntactic, orthographic, etc.
This same analyzer is also applied in thelearning process. Hypothesis generator.
The possibilities for each word (token) are determined.
Rulescan be captured by letting one word have one choice, as is the case in the recognitionof time, date, money and percentage terms for the Chinese Named Entity (NE) task.These are identied by pattern matching rules. Disambiguation module.
This is essentially implementation of Viterbi algorithm.All the above modules will be described in detail in the following sections.TEXT INFORMATION EXTRACTION TO TAGGINGFirst of all, a brief of the modeling of the problem is in order.
Each word in text isassigned a tag, information can then be obtained from tags of all words.
For example, forthe English NE task,Example 1:The/- British/- balloon/- ,/- called/- the/- Virgin/- Global/- Challenger/- ,/- is/- to/-be/-own/- by/- Richard/PERSON Branson/PERSON ,/- chairman/- of/- Virgin/ORGAtlantic/ORG Airways/ORG ;/-Grouping all adjacent words with tag PERSON gives a person name, grouping thosewith tag ORG gives an organization name, etc.The problem becomes, for any given sequence of words w = w1w2: : : wn, nding thetags t = t1t2: : : tncorrespondingly.Note that there are dierent ways of assigning tags.
For the above example, tags canalso be:Example 1:The/- British/- balloon/- ,/- called/- the/- Virgin/- Global/- Challenger/- ,/- is/- to/- be/-own/- by/- Richard/PERSON-start Branson/PERSON-end ,/- chairman/- of/- Virgin/ORG-start Atlantic/ORG-continue Airways/ORG-end ;/-This way, extra information such as common surnames, rst names, organization endings(Corp., Inc. etc) and so on can be obtained.
It is observed that dierent tags for a sametask make dierence.
We feel that choosing an appropriate tag set is a problem worthy ofcareful investigation.
Intuitively, a tag set for a particular task must be: sucient, meaningthat the information extracted must be sucient for the task; and ecient, meaning thatthere should be no redundant and nonrelevant information.LEARNING PROCESS: INFORMATION DISTILLATION OF TRAININGCORPUSLearning Process in GeneralCareful consideration has been given to study how to absorb domain knowledge inlanguage model(s) in a generic and systematic way.
The basic idea is, as much as possiblerelevant and signicant information (to the task) contained in the original corpus shouldretain in back-o corpora where back-o features are stored, so that correct decisions canbe made from the statistics generated from the back-o corpora when they can not be donefrom the statistics from the original training corpus.The original training corpus is in the form of word/tag, statistics about words and tagsincluding local contextual information can be obtained.
Each word in the corpus is given aback-o feature by the principle that the back-o features of all words should extract themost information from the corpus relevant to the particular task.
The information loss iscompensated by gain of generosity.
A back-o corpus in the form of back-o feature/tagis then generated, and statistics can be obtained in the same manner.
The original corpusis processed this way for a certain number of times.
Every time, a less descriptive back-ocorpus which gains more in generosity is generated, and thus the corresponding statistics.For example, semantic classes can be used as back-o features for all the words inExample 1, which gives the back-o corpus of the following form:seman1/- seman2/- ... semanM-1/PERSON semanM/PERSON ... semanN-3 /ORGsemanN-2/ORG semanN-1/ORG semanN/-or part-of-speech as back-o features, which gives1st backoffcorpusoriginaltraining corpus2nd backoffcorpusNth backoffcorpus...Information I          >           Information I1   >        Information I2   > ...     Information INGenerosity G          <           Generosity G1   <        Generosity G2   < ...
Generosity GNFigure 1: Information Distillation of Training Corpuspos1/- pos2/- ... posM-1/PERSON - posM/PERSON ... posN-3 /ORG posN-2/ORGposN-1/ORG posn-1/-The generation of back-o corpora is described by Figure 1.
The total number of back-ocorpora therein is a controllable parameter.Learning Process for Chinese NE Training Corpus and Supporting ResourcesWe have a text corpus of about 500,000 words from People Daily and Xinhua NewsAgency, all of which were manually checked for both word segmentation and part ofspeech tagging.In addition, we have a lexicon of 89,777 words, in which 5351 words are labeled asgeographic names, 304 words are people's name and 183 are organization names.
1167words consist of more than 4 characters.
The longest word (meaning \Great Britainand North Ireland United Kingdom") contains 13 characters.About 50,000 dierent words appeared in the 500,000 words corpus.We also have three entity name lists: people name list (67,616 entries), location namelist (6,451 entries) and organization name list (6190 entries). Observation: Problems and Solutions1.
Intuitively, case information of proper names in English writing system pro-vides good indication about locations and boundaries of entity names.
Thereare successful systems [2] which are built upon this intuition.
Unfortunately, theuniformity of character string in Chinese writing system does not contain suchinformation.One should look for such analogous indicative characteristics which may beunique in Chinese language.2.
Word in Chinese is a vague concept and there is no clear denition for it.
Thereare boundary ambiguities between words in texts for even human being under-standing, and inevitably machine processing.
Tokenization, or word segmenta-tion is still a problem in Chinese NLP.
Word boundary ambiguities exist not onlybetween commonly used words which are not in entity names, but also betweencommonly used words and entity names.3.
Besides the uniformity appearance of characters, proper names in Chinese canconsist of commonly used words.
As a matter of fact, almost all Chinese charac-ters can be a commonly used words themselves, including those in entity namessuch as people's names, location names, etc.Therefore, unlike English, the problem of Chinese entity recognition should notbe isolated from the problem of tokenization, or word segmentation. Building Language ModelsOne level of back-o features, which are also called word classes, are obtained by thefollowing way:We extend the idea in the new word detection engine of the integrated model ofChinese word segmentor and part of speech tagger [1].
The idea is to extend thescope of an interested word class of new word, the proper names, into named entitiesby looking into broader range of constituents.
Under this framework, we believecontextual statistics plays important rules in deciding word boundary and predictingthe categories of named entities, while local statistics, or information resides withinwords or entities, can provide evidence for suggesting the appearance of named entityand deciding the validity of these entities.
We need to make full use of both contextualand local statistics to recognize these named entities, thus contextual language modeland entity models are created.The basic process to build the model is like this:1.
Change the tag set of the part-of-speech tagger by splitting the tag NOUN intomore detailed tags related to the particular task, which include the symbolicnotions of person, location, organization, date, time, money and percentage.2.
Replace the tag NOUN in the training corpus with the above extended new tags.Only ambiguous words are manually checked.3.
Build contextual language model with the training corpus with the new tag set.4.
Build entity models from the entity name lists.
Each entity has its own model.Learning Process for English NE Training Corpus and Supporting ResourcesSGML marked up (for NE task only) Brown corpus and corpus from Wall StreetJournal.
In total the size of words is 7.2MB, words with SGML-markup is 9.5MB.Supporting resources include the location list, country list, corporation reference listand the people's surname list provided by MUC.
Only the single-word entries in theselists are in actual use. Observation: Problems and SolutionsCase information, or more generally, orthographic information, gives good evidence ofnames, as was observed in [2].
Although things get muddled up when one really getsdeep into it: e.g.
rst words of sentences, words which do not have all normal (lower)case form (e.g.
\I"), or words whose cases are changed due to other reasons such asformatting (e.g.
titles), being artifacts, etc.
Nevertheless, this is an very importantinformation for identifying entity names.Prepositions are also helpful, so are common suxes and prexes of the entities, suchas Corp., Mr., and so on.
In general, all such useful information should be somehowsorted out.
Word classes tailored for this particular purpose will be ideal. Building Language ModelsThere are two levels of back-o features represented by word classes.For the following words, the two back-o features are the same:{ Hand-crafted special words for NE task.
Each possesses a dierent word class(represented by word itself).
These special words include \I", \the", \past",\pound", \following", \of", \in", \May", etc.
In total there are about 100 suchwords;{ Words from the supporting resources (as stated in the beginning of this section).Words from a same list possess a same word class.
{ Hand-crafted lists of words, which include week words (Monday, Tuesday, ...),month words (January, February, ...), cardinal numbers (one, two, 1  31, ...),ordinal numbers (1st, rst, 2nd, second, ...), etc.For the rest of words, the rst level features are word classes provided by a machineauto classication of words, while the second level of features include:word class exampleoneDigitNum 1containsDigitAndColon 2:34containsAlphaDigit A4allCaps KRDLcapPeriod M.rstCommonWordInitCaprstNonCommonWordICCommonWordInitCap DepartmentinitCapNotCommonWord DavidmixedCasesWord ValueJetcharApos O'clockallLowerCase cancompoundWord ad-hocIn total, the number of orthographic features is about 30.To give a sense what information is extracted from the original training corpus, forexample, the two back-o sentences for Example 1 are:Level 1:the/- COUN ADJ/- WordClass1/- ,/- WordClass2/- the/- WordClass3/- WordClass4/- WordClass5/- ,/- WordClass6/- to/- WordClass7/- WordClass8/- by/- WordClass9/PERSON WordClass10/PERSON ,/- WordClass11/- of/- WordClass12/ORG Loc/ORG WordClass13/slash ORG ;/-Level 2:the/- COUN ADJ/- LowerCaseWord/- ,/- LowerCaseWord/- the/- CommonWor-dInitCap/- CommonWordInitCap/- CommonWordInitCap/- ,/- LowerCaseWord/-to/- LowerCaseWord/- LowerCaseWord/- by/- initCapNotCommonWord/PERSONinitCapNotCommonWord/PERSON ,/- LowerCaseWord/- of/- CommonWordInit-Cap/ORG Loc/ORG CommonWordInitCap/ORG ;/-Statistics such as the possibilities of CommonWordInitCap (which are NOT rst wordsof sentences) and the corresponding frequencies can be obtained from the second back-o corpus.
From our corpus, these are:Organization 7525None of the named entities 8493Location 896Person 195Date 8Money 2From the above statistics, it's interesting to notice that non-rst common words whichare initial capitalized have a far more chance to be organization than person (frequen-cies 7525 vs 195) and location (frequencies 7525 vs 896).
This agrees with generalobservations.
Also interesting is that such words have a higher chance not to be anyof the seven entities.
This comes as a bit surprise.
For NLP researchers, though, itmay not be a surprise at all.
This example also gives a sense how general observationsare represented in a precise way.Further research is to be carried out to justify quantitively the merits of this learningprocess.
Its full potential has yet to be exploited.
So far, our experimentation has provedthat:1.
Various kinds of text analysis (syntactic, semantic, orthographic, etc) can be incorpo-rated into the same framework in a precise way, which will be used in the informationextraction (tagging) stage in the same way;2.
It provides an easy way to absorb human knowledge as well as domain knowledge,and thus customization can be done easily;3.
It gives greatexibility as how to optimize the system.1 and 2 are somehow clear from the above discussion.
Details on the disambiguation modulewill reveal 3.DETAILS OF THE SYSTEM MODULES1.
Sentence segmentor and tokenizer: initial tokenization by looking up dictionary forChinese, standard way for English.2.
Text analyzer.
What has been done for training corpus in the learning stage is donehere.
After the analysis, each word possesses a given number of back-o features.3.
Hypothesis generator. Chinese: based on entities' prexes, suxes, trigger words and local context in-formation, guesses are made about possible boundaries of entities and categoriesof entities.
Time, date, money, and percentage are extracted by pattern-matchingrules. English: for each word basically look for all the possibilities from the databaserst.
If the word is not found, look for the possibilities of its back-o features.4.
Disambiguation module.
Recall that information extraction from word sequencew becomes nding the corresponding tag sequence t. In the paradigm of maxi-mum likelihood estimation, the best set of tags t is the one such that prob(tjw) =maxt0prob(t0jw).
This is equivalently to nd t such that prob(tw) = maxt0prob(t`w)because prob(t0jw) = prob(tw0)=prob(w) and prob(w) is a constant for any given w.The following equality is well-known:prob(tw) = prob(t1) prob(w1jt1) prob(t2jt1w1) prob(w2jt1w1t2)   prob(tnjt1w1: : : tn 1wn 1) prob(wnjt1w1: : : tn 1wn 1tn): (1)Computationally, it is only feasible when some (actually most) dependencies aredropped, for example,prob(tkjt1w1: : : tk 1wk 1)  prob(tkjtk 1tk 2); (2)prob(wkjt1w1: : : tk 1wk 1tk)  prob(wkjtktk 1): (3)(2) and (3) can be justied by Hidden Markov Modeling for the generation of wordsequences.As always, Viterbi algorithm is employed to compute the probability (1), given anyapproximations like (2) and (3).
When sparse data problem is encountered, back-oand smoothing strategy can be adopted, e.g.prob(wkjtktk 1) backoff to !
prob(wkjtk); (4)or for unknown words, substitute word in (4) with its back-o features, e.g.prob(wkjtktk 1) backoff to !
prob(bof1kjtktk 1)backoff to !
prob(bof2kjtktk 1) : : :backoff to !
prob(bofNkjtktk 1)backoff to !
prob(bof1kjtk) : : : backoff to !
prob(bofNkjtk);where N is the total number of back-o features for the word.Note that no smoothing is employed in the above scheme.
From this scheme one cansee that there exist various ways of back-o and smoothing.
This characteristics, aswell as the free choices of back-o features, is where theexibility of the system lies.Remark.
In the actual system, back-o and smoothing schemes are dierent from theabove.
The actual schemes are not included because they are more complicated, andyet no systematic experimentation has been done to show that they are better thanother options.PERFORMANCE ANALYSISThe system currently processes one sentence at a time, and no memory is kept once thesentence is done.
Furthermore, due to limitation of time, the guidelines for both Chineseand English NE are not entirely followed, as we didn't have time to read the guidelinescarefully!The F-measures of formal run for Chinese and English are 86.38% and 77.74%, respec-tively.
Given the limited time (less than six months) and resources (three persons, all halftime), we are satisfactory with the performance.
* * * CHINESE NE SUMMARY SCORES * * *P&R 2P&R P&2RF-MEASURES 86.38 84.39 88.46* * * ENGLISH SUMMARY SCORES * * *P&R 2P&R P&2RF-MEASURES 77.74 79.06 76.46FUTURE RESEARCH DIRECTIONOur brief experimentation in Chinese and English Named Entity recognition shows thatthe system has great potential that deserves further investigation.1.
Modeling of the problem: currently information and knowledge is represented in theform of word/tag.
This may pose too much restriction.
A better way of representinginformation and knowledge, in other words, a better modeling of the problem, shouldbe studied.2.
Quantitive justication of the learning process (knowledge distillation) should also bestudied.
The system should be able to compare dierent set of back-o features andthus the best one can be chosen.3.
The system provides greatexibility as how to optimize it.
The optimization shouldbe done systematicly, rather than trial by trial as is the case for the time being.References[1] S. Bai, An Integrated Model of Chinese Word Segmentation and Part of Speech Tagging,Advances and Applications on Computational Linguistics (1995), Tsinghua UniversityPress.
[2] D.M.
Bikel, S. Miller, R. Schwartz and R. Weischedel, Nymble: a High-PerformanceLearning Name-nder.
