AN LR CATEGORY-NEUTRAL PARSER WITH LEFT CORNERPREDICTIONPao la  Mer loUnivers i ty  of  Mary land/Un ivers i t~  de Gen~veFscult~ des LettresCH-1211 Gen~ve 4mer lo@divsun.
,n ige.chAbst rac tIn this paper we present a new parsing model oflinguistic and computational interest.
Linguisti-cally, the relation between the paxsez and the the-ory of grammar adopted (Government and Bind-ing (GB) theory as presented in Chomsky(1981,1986a,b) is clearly specified.
Computationally,this model adopts a mixed parsing procedure,by using left corner prediction in a modified LRparser.ON L INGUIST IC  THEORYFor a parser to be linguistically motivated, it mustbe transparent to a linguistic theory, under someprecise notion of transparency (see Abney 1987)~GB theory is a modular theory of abstract prin-ciples.
A parser which encodes a modular theoryof grammax must fulfill apparently contradictorydemands: for the parser to be explanatory it mustmaintain the modularity of the theory, while forthe paxser to be efficient, modularization must beminimized so that all potentially necessary infor-mation is available at all times, x We explore apossible solution to this contradiction.
We observethat linguistic information can be classified into 5different classes, as shown in (1), on the basis oftheir informational content.
These we will ca\]\] ICClasses.
(1) a. Configurations: isterhood, c-command,m-command, :t:maximal projection .. .b.
Lexical features: ~N, ?V, ?Funct,?c-selected, :t:Strong Agr .
.
.c.
Syntactic features: ?Case, ~8, ?7,~baxrier.d.
Locality information: minimality, binding,antecedent government.e.
Referential information: +D-linked,?anaphor, ?pronominal.IOn efficiency of GB-based systemstad(1990), Kashkett(1991).see RJs-288This classification can be used to specify pre-cisely the amount of modularity in the parser.Berwick(1982:400ff) shows that a modulax systemis efficient only if modules that depend on eachother axe compiled, while independent modulesaxe not.
We take the notion of dependent andindependent to correspond to IC Classes, in thatprimitives that belong to the same IC Class axedependent on each other, while primitives that be-long to different IC Classes axe independent fromeach other.
We impose a modularity requirementthat makes precise predictions for the design of theparser.Modular i ty  Requirement  (MR) Only primi-tives that belong to the same IC Class can becompiled in the parser.RECOVERING PHRASESTRUCTUREAccording to the MR, notions uch as headedness,directionality, sisterhood, and maximal projectioncan be compiled and stored in a data structure, be-cause these notions belong to the same IC Class,configurations.
These features are compiled intocontext-free rules in our parser.
These basic Xrules axe augmented by A rules licensed by thepart of Trace theory that deals with configura-tions.
The crucial feature of this grammar is thatnontermina\]s specify only the X projection level,and not the category.
The full context-free gram-max is shown in Figure 1.The recovery of phrase structure is a crucialcomponent of a parser, as it builds the skeletonwhich is needed for feature annotation.
It mustbe efficient and it must fail as soon as an error isencountered, in order to limit backtracking.
AnLR(k) parser (Knuth 1965) has these properties,since it is deterministic on unambiguous input,and it has been proved to recognize only validprefixes.
In our parser, we compile the grammarshown above into an LALR(1) (Aho and Ullma~n1972) parse table.
The table has been modifiedX" ~ Y" X'X" --' X' Y"X' --' X Y"X' --+ ?"
XX' --* Y" X'X' --' X' Y"X" --~ Y"  X"X"  --' X"  Y"X --, emptyX" --, emptyFigure 1:specificationcomplementationmodificationadjunctionempty headsempty XmaxsCategory-Neutral Grammarin order to have more than one action for eachtable entry.
2 Three stacks are used: a stack forthe states traversed so far; a stack for the seman-tic attributes associated with each of the nodes;a tree stack of partial trees.
The LR algorithmis encoded in a parse  predicate, which establishesa relation between two sets of 5-tuples, as shownin (2).
s(2) Tix$ixA~xCixPT~--* T~xSjxA.~xCjxPT~Our parser is more elaborate and less restric-tive than a standard LR parser, because it im-poses conditions on the attr ibutes of the statesand it is nondeterministic.
In order to reduce theamount of nondeterminism, some predictive powerhas been introduced.
The cooccurenee restrictionsbetween categories, and subcategorization i for-mation of verbs is compiled in a table, which wecall Left Corner Prediction Table (LC Table).
Bylooking at the current token, at its category la-bel, and its subcategorization frame, the numberof choices of possible next states can be restricted.For instance, if the current token is a verb, andthe LR table allows the parser either to project onelevel up to V ~, or it requires to create an empty ob-ject NP, then, on consulting the subcategorizationinformation, the parser can eliminate the secondoption as incorrect if the verb is intransitive.RESULTS AND COMMENTSThe design presented so far embodies the MR,since it compiles only dependent features in twotables off-line.
Compared to the use of partiallyor fully instantiated context-free grammars, this2This modification is necessary because the gram-mar compiled into the LR table is not an LR grammar.Sin (2) T~ is an element of the set of input tokens,Ss is an element of the set of states in the LR table, Atis an element of the set of attributes associated witheach state in the table, C~ iS an element of the set ofchains, i.e.
displaced element, and PTk  iS an elementof the set of tokens predicted by the left corner table(see below).289Grammar  InstantiatedNumber of Rules 5146224Number of StatesShift/reduce conflictsReduce/reduce conflicts 270X16142436Figure 2: Numbersorganization of the parsing algorithms has beenfound to be better on several grounds.Consider again the X grammar that we use inthe parser, shown in Figure 1.
One of the crucialfeatures of this grammar is that the nonterminalsare specified only for level and headedness.
Thisversion of the grammar is a recent result.
In previ-ous implementations of the parser, the projectionsof the head in a rule were instantiated: for in-stance NP- -~ YP  IV' .
Empirically, we find thaton compiling the partially instantiated grammarthe number of rules is increased proportionatelyto the number of categories, and so is the num-ber of conflicts in the table.
Figure 2 shows therelative sizes of the LALR(1) tables and the num-ber of conflicts.
Moreover, on closer inspectionof the entries in the table, categories that belongto the same level of projection show the same re-duce/reduce conflicts.
This means that introduc-ing unrestricted categoriM information increasesthe size of the table without  decreasing the num-ber of conflicts in each entry, i.e.
without reducingthe nondeterminism in the table.These findings confirm that categorial infor-mation can be factored out of the compiled table,as predicted by the MR.
The information aboutcooccurrenee r strictions, category and subcatego-rization frame is compiled in the Left Corner (LC)table, as described above.
Using two compiled ta-bles that interact on-line is better than compilingall the information into a fully instantiated, stan-dard context-free grammar for several reasons.
4Computational\]y, it is more efllcient, s Practically,manipulating a small, highly abstract grammar is4Fully iustantiated grammars have been used,among others, by Tomita(1985) in an LR parser, andby Doff(1990), Fong(1991) in GB-based parsers.sit has been argued elsewhere that for context-freeparsing algorithms, the size of the graxrtrnsr (which iSa constant factor) can easily become the predominantfactor for a11 useful inputs (see Berwick and Weinberg1982).
Work on compilation of parsers that use GPSGseems to point in the same direction.
The separation ofstrnctu~al information from cooccttrence r strictions iSadvocated in Kilbury(1986); both Shieber(1986) andPhi\]Hps(1987) argue that the combinatorial explosion(Barton 1985) of a fully expanded ID/LP formalismcan be avoided by using feature variables in the com-piled gxammar.
See also Thompson 1982.much easier.
It is easy to maintain and to embedin a full-fledged parsing system.
Linguistically, afully-instantiated paxser would not be transpaxentto the theory and it would be language dependent.Finally, it could not model some experimental psy-cholingnistic evidence, which we present below.PSYCHOLINGUIST IC  SUPPORTA reading task is presented in F~azier and Rayner1987 where eye movements are monitored: theyfind that in locally ambiguous contexts, the am-biguous region takes less time than an unambigu-ous eounterpaxt, while a slow down in process-ing time is registered in the disambiguating re-gion.
This suggests that selection of major catego-rial information in lexically ambiguous sentences idelayed, e This delay means that the parser mustbe able to operate in absence of categorial infor-mation, making use of a set of category-neutralphrase structure rules.
This separation of item-dependent and item-independent information isencoded in the grammax used in our paxser.
Aparser that uses instantiated categories would haveto store categorial cooccurence r strictions in a dif-ferent data structure, to be consulted in case oflexically ambiguous inputs.
Such design would beredundant, because categorial information wouldbe encoded twice.CONCLUSIONThe module described in this paper is imple-mented and embedded in a parser for English oflimited coverage, but it has some shortcomings,which axe currently under investigation.
Refine-ments axe needed to compile the LC table auto-matically, to define IC Classes predictively insteadof by exhaustive listing.
Finally, a formal proofis needed to show that our definition of indepen-dent and dependent is always going to increaseefficiency.ACKNOWLEDGEMENTSThis work has benefited from suggestions by Bon-nie Doff, Paul Gorrell, Eric Wehrli and AmyWeinberg.
The author is supported by a Fellow-ship from the Swiss-Italian Foundation.eFor instance, in the sentences in (3), (from F~azierand Rayner 1987) the ambiguous target item, shownin capitals in (3)a, takes less time than the unambigu-ous control in (3)b, while there is a slow down in thedisambiguating material (in italics).
(3) a.
The warehouse FIRES numerous employeeseach year.b.
That warehouse fixes numerous employees achyear.REFERENCESAbney Steven 1987, "GB Paxsing and Psycholog-ical Reality" in MIT Paxsing Volume, CognitiveScience Center.Aho A.V.
and J.D.
Ullman 1972, The Theoryof Parsing, Translation and Compiling, Prentice-Hall, Englewood Cliffs, NJ.Barton Edward 1985, "The ComputationalDifficulty of ID/LP Parsing" in Proc.
of the ACL.Berwick Robert 1982, Locality Principles andthe Acquisition of Syntactic Knowledge, Ph.DDiss., MIT.Berwick Robert and Amy Weinberg 1982," Paxsing Efficiency, Computational Complexityand the Evaluation of Grammatical Theories ",Linguistic Inquiry, 13:165-191.Chomsky Noam 1981, Lectures on Govern-ment and Binding, Foris, Dordrecht.Chomsky Noam 1986a, Knowledge of Lan-guage: Its Nature, Origin and Use, Praeger, NewYork.Chomsky Noam 1986b, Barriers,MIT Press,Cambridge MA.Dorr Bonnie J.
1990,Lezical Conceptual Struc-ture and Machine Translation, Ph.D Diss., MIT.Fong Sandiway 1991, Computational Prop-erties of Principle-based Grammatical Theories,Ph.D Diss., MIT.Frazier Lyn and Keith Rayner 1987, "Res-olution of Syntactic Category Ambiguities: EyeMovements in Parsing Lexically Ambiguous Sen-tences" in Journal of Memory and Language,26:505-526.Kashkett Michael 1991, A ParameterisedParser for English and Warlpiri, Ph.D Diss.,MIT.Kilbury James 1986, "Category CooccurrenceRestrictions and the Elimination of Metaxules", inProc.
of COLING, 50-55.Knuth Donald 1965, "On the 'I~anslation ofLanguages from Left to Right", Information andControl, 8.Phillips John 1987, "A Computational Repre-sentation for GPSG", DAI Research Paper 316.Ristad Eric 1990 , Computational Strnc~ure ofHuman Language, MIT AI Lab, TR 1260.Shieber Stuart 1986, "A Simple Reconstruc-tion of GPSG" in Proc.
of COLING, 211-215.Thompson Henry 1982, "Handling Metaxulesin a Parser for GPSG" in Proc.
of COLING.Tomita Masaru 1985, E~cien~ Parsing forNatural Language, KluweI, Hingham, MA.290
