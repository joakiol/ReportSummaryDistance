Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 124?127,Queen Mary University of London, September 2009. c?2009 Association for Computational LinguisticsAnalysis of Listening-oriented Dialogue for Building Listening AgentsToyomi Meguro, Ryuichiro Higashinaka, Kohji Dohsaka, Yasuhiro Minami, Hideki IsozakiNTT Communication Science Laboratories, NTT Corporation2-4, Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0237, Japan{meguro,rh,dohsaka,minami,isozaki}@cslab.kecl.ntt.co.jpAbstractOur aim is to build listening agents thatcan attentively listen to the user and sat-isfy his/her desire to speak and have him-self/herself heard.
This paper investigatesthe characteristics of such listening-orienteddialogues so that such a listening processcan be achieved by automated dialogue sys-tems.
We collected both listening-orienteddialogues and casual conversation, and ana-lyzed them by comparing the frequency ofdialogue acts, as well as the dialogue flowsusing HiddenMarkovModels (HMMs).
Theanalysis revealed that listening-oriented dia-logues and casual conversation have charac-teristically different dialogue flows and thatit is important for listening agents to self-disclose before asking questions and to uttermore questions and acknowledgment than incasual conversation to be good listeners.1 IntroductionAlthough task-oriented dialogue systems have beenactively researched over the years (Walker et al,2001), systems that perform more flexible (less task-oriented) dialogues such as chats are beginning to beactively investigated from their social and entertain-ment aspects (Bickmore and Cassell, 2001; Higuchiet al, 2008).This paper deals with dialogues in which one con-versational participant attentively listens to the other(hereafter, listening-oriented dialogue).
Our aim isto build listening agents that can implement such alistening process so that a user can satisfy his/herdesire to speak and have him/herself heard.
Suchagents would lead the user?s state of mind for thebetter as in a therapy session, although we want ourlistening agents to help users mentally in everydayconversation.
It should also be noted that the pur-pose of the listening-oriented dialogue is to simplylisten to users, not to elicit information as in inter-views.L: The topic is ?travel?, so did youtravel during summer vacation?
(QUESTION)S: I like traveling.
(SELF-DISCLOSURE)L: Oh!
I see!
(SYMPATHY)Why do you like to travel?
(QUESTION)S: This summer, I just went backto my hometown.
(SELF-DISCLOSURE)I was busy at work, but I?mplanning to go to KawaguchiLake this weekend.
(SELF-DISCLOSURE)I like traveling because it isstimulating.
(SELF-DISCLOSURE)L: Going to unusual placeschanges one?s perspective,doesn?t it?
(SYMPATHY)You said you?re going to go toKawaguchi Lake this weekend.Is this travel?
(QUESTION)Will you go by car or train?
(QUESTION)Figure 1: Excerpt of a typical listening-oriented di-alogue.
Dialogue acts corresponding to utterancesare shown in parentheses (See Section 3.1 for theirmeanings).
The dialogue was originally in Japaneseand was translated by the authors.There has been little research on listening agents.One exception is (Maatman et al, 2005), whichshowed that systems can make the user have thesense of being heard by using gestures, such as nod-ding and shaking of the head.
Although our work issimilar to theirs, the difference is that we focus moreon verbal communication instead of non-verbal one.For the purpose of gaining insight into how tobuild our listening agents, we collected listening-oriented dialogues as well as casual conversation,and compared them in order to reveal the charac-teristics of the listening-oriented dialogue.
Figure 1shows an example of a typical listening-oriented di-alogue.
In the figure, the conversational participantstalk about travel with the listener (L), repeatedly ask-ing the speaker (S) to make self-disclosure.2 ApproachWe analyze the characteristics of listening-orienteddialogues by comparing them with casual conversa-tion.
Here, casual conversation means a dialoguewhere conversational participants have no prede-fined roles (i.e., listeners and speakers).
In this124study, we collect dialogues in texts because we wantto avoid the particular problems of voice, such asfilled pauses and interruptions, although we plan todeal with speech input in the future.As a procedure, we first collect listening-orienteddialogues and casual conversation using human sub-jects.
Then, we label the collected dialogues withdialogue act tags (see Section 3.1 for details of thetags) to facilitate the analysis of the data.
In the anal-ysis, we examine the frequency of the tags in eachtype of dialogue.
We also look into the difference ofdialogue flows by modeling each type of dialogue byHidden Markov Models (HMMs) and comparing theobtained models.
We employ HMMs because theyare useful for modeling sequential data especiallywhen the number of states is unknown.
We checkwhether the HMMs for the listening-oriented dia-logue and casual conversation can be successfullydistinguished from each other to see if the listen-ing process can be successfully modeled.
We alsoanalyze the transitions between states in the createdHMMs to examine the dialogue flows.
We note thatHMMs have been used to model task-oriented dia-logues (Shirai, 1996) and casual conversation (Iso-mura et al, 2006).
In this study, we use HMMs tomodel and analyze listening-oriented dialogues.3 Data collectionWe recruited 16 participants.
Eight participated aslisteners and the other eight as speakers.
The male-to-female ratio was even.
The participants were 21to 29 years old.
Each participant engaged in four di-alogues: two casual conversations followed by twolistening-oriented dialogues with a fixed role of lis-tener/speaker.
In listening-oriented dialogue, the lis-teners were instructed to make it easy for the speak-ers to say what they wanted to say.
When col-lecting the casual conversation, listeners were notaware that they would be listeners afterwards.
Lis-teners had never met nor talked to the speakers priorto the data collection.
The listeners and speakerstalked over Microsoft Live MessengerTMin differentrooms; therefore, they could not see each other.In each conversation, participants chatted for 30minutes about their favorite topic that they selectedfrom the topic list we prepared.
The topics werefood, travel, movies, music, entertainers, sports,health, housework and childcare, personal comput-ers and the Internet, animals, fashion and games.
Ta-ble 1 shows the number of collected dialogues, utter-ances and words in each utterance of listeners andListening Casual# dialogues 16 16# utterances 850 720# words Listener 20.60 17.92per utt.
Speaker 26.46 21.44Table 1: Statistics of collected dialogues.speakers.
Generally, utterances in listening-orienteddialogue were longer than those in casual conversa-tion, probably because the subjects explained them-selves in detail to make themselves better under-stood.At the end of each dialogue, the participantsfilled out questionnaires that asked for their sat-isfaction levels of dialogue, as well as how wellthey could talk about themselves to their conver-sational partners on the 10-point Likert scale.
Theanalysis of the questionnaire results showed that, inlistening-oriented dialogue, speakers were having abetter sense of making themselves heard than in ca-sual conversation (Welch?s pairwise t-test; p=0.016)without any degradation in the satisfaction level ofdialogue.
This indicates that the subjects were suc-cessfully performing attentive listening and that it ismeaningful to investigate the characteristics of thecollected listening-oriented dialogues.3.1 Dialogue actWe labeled the collected dialogues using the dia-logue act tag set: (1) SELF-DISCLOSURE (disclo-sure of one?s preferences and feelings), (2) INFOR-MATION (delivery of objective information), (3) AC-KNOWLEDGMENT (encourages the conversationalpartner to speak), (4) QUESTION (utterances that ex-pect answers), (5) SYMPATHY (sympathetic utter-ances and praises) and, (6) GREETING (social cuesto begin/end a dialogue).We selected these tags from the DAMSL tag set(Jurafsky et al, 1997) that deals with general con-versation and also from those used to label therapyconversation (Ivey and Ivey, 2002).
Since our workis still preliminary, we selected only a small num-ber of labels that we thought were important formodeling utterances in our collected dialogues, al-though we plan to incorporate other tags in the fu-ture.
We expected that self-disclosure would occurquite often in our data because the subjects were totalk about their favorite topics and the participantswould be willing to communicate about their expe-riences and feelings.
We also expected that the lis-teners would sympathize often to make others talkwith ease.
Note that sympathy has been found usefulto increase closeness between conversational partic-125Listener SpeakerCasual Listening Casual ListeningDISC 66.6% 44.5% 53.3% 57.3%INFO 6.5% 1.4% 5.6% 5.2%ACK 8.0% 12.3% 6.6% 6.9%QUES 4.1% 25.8% 21.3% 14.0%SYM 2.6% 3.7% 3.2% 3.3%GR 10.9% 9.8% 7.2% 9.6%OTHER 1.3% 2.5% 2.9% 3.7%Table 2: Rates of dialogue act tags.DISC INFO ACK QUES SYM GRIncrease 0 0 8 8 5 4Decrease 8 8 0 0 3 4Table 3: Number of listeners whose tags in-creased/decreased in listening-oriented dialogue.ipants (Reis and Shaver, 1998).A single annotator, who is not one of the authors,labeled each utterance using the seven tags (six di-alogue act tags plus OTHER).
As a result, 1,177tags were labeled to the utterances in the listening-oriented dialogues and 1,312 tags to those in casualconversation.
The numbers of tags and utterances donot match because, in text dialogue, an utterance canbe long and may be annotated with several tags.4 Analysis4.1 Comparing the frequency of dialogue actsWe compared the frequency of the dialogue act tagsin listening-oriented dialogues and casual conversa-tion.
Table 2 shows the rates of the tags in each typeof dialogue.
In the table, OTHER means the expres-sions that did not fall into any of our six dialogueacts, such as facial expressions and mistypes.
Table3 shows the number of listeners whose rates of tagsincreased or decreased from casual conversation tolistening-oriented dialogue.Compared to casual conversation, the rates ofSELF-DISCLOSURE and INFORMATION decreasedin the listening-oriented dialogue.
On the otherhand, the rates of ACKNOWLEDGMENT and QUES-TION increased.
This means that the listeners tendedto hold the transmission of information and focusedon letting speakers self-disclose or deliver informa-tion.
It can also be seen that the speakers decreasedQUESTION to increase self-disclosure.4.2 Modeling dialogue act sequences by HMMWe analyzed the flow of listening-oriented dialogueand casual conversation by modeling their dialogueact sequences using HMMs.
We defined 14 obser-vation symbols, corresponding to the seven tags fora listener and the same number of tags for a speaker.L:Greeting:0.483S:Greeting:0.39L:Self-disclosure:0.107L:Question:0.456S:Ack:0.224 S:Self-disclosure:0.828L:Self-disclosure:0.579L:Ack:0.1320.10.580.130.380.830.410.550.51??
?
?Figure 2: Ergodic HMM for listening-oriented dia-logue.
Circled numbers represent state IDs.We trained the following two types of HMMs foreach type of dialogue.Ergodic HMM: Each state emits all 14 observationsymbols.
All states are connected to each other.Speaker HMM: Half the states in this HMM onlyemit one speaker?s dialogue acts and the otherhalf emit other speaker?s dialogue acts.
Allstates are connected to each other.The EM algorithm was used to train the HMMs.To find the best fitting HMM with minimal states,we trained 1,000 HMMs for each type of HMM byincreasing the number of states from one to ten andtraining 100 HMMs for each number of states.
Thiswas necessary because the HMMs severely dependon the initial probabilities.
From the 1,000 HMMs,we chose the most fitting model using the MDL(Minimum Description Length) criterion.4.2.1 Distinguishing Dialogue TypesWe performed an experiment to examine whetherthe trained HMMs can distinguish listening-orienteddialogues and casual conversation.
For this exper-iment, we used eight listening-oriented dialoguesand eight casual conversations to train HMMs andmade them classify the remaining 16 dialogues.
Wefound that Ergodic HMM can distinguish the dia-logues with an accuracy of 87.5%, and the SpeakerHMM achieved 100% accuracy.
This indicates thatwe can successfully train HMMs for each type ofdialogue and that investigating the trained HMMswould show the characteristics of each type of di-alogue.
In the following sections, we analyze theHMMs trained using all 16 dialogues of each type.4.2.2 Analysis of Ergodic HMMFigure 2 shows the Ergodic HMM for listening-oriented dialogue.
It can be seen that the major flow126L:Greeting:0.888L:Self-disclosure:0.445L:Question:0.492 S:Self-disclosure:0.835L:Self-disclosure:0.556L:Ack:0.27S:Greeting:0.98S:Self-disclosure:0.125S:Ack:0.6610.42 0.370.430.380.110.56 0.510.18 0.920.470.630.25?
??
??
?Figure 3: Speaker HMM for listening-oriented dia-logue.S2:Greeting:0.775S2:Self-disclosure:0.523S2:Question:0.414S1:Self-disclosure:0.644S1:Question:0.26S2:Self-disclosure:0.629S2:Ack:0.12S1:Greeting:0.848S1:Self-disclosure:0.662S1:Ack:0.1350.45 0.350.450.45 0.110.160.32 0.420.430.10.740.510.120.76 0.15?
??
??
?Figure 4: Speaker HMM for casual conversation.of dialogue acts are: 2?
L?s question ?
3?
S?s self-disclosure ?
4?
L?s self-disclosure ?
2?
L?s ques-tion.
This flow indicates that listeners tend to self-disclose before the next question, showing the cycleof reciprocal self-disclosure.
This indicates that lis-tening agents would need to have the capability ofself-disclosure in order to become human-like lis-teners.4.2.3 Analysis of Speaker HMMFigures 3 and 4 show the Speaker HMMs forlistening-oriented dialogue and casual conversation,respectively.
Here, L and S correspond to S1 andS2.
It can be clearly seen that the two HMMshave very similar structures.
From the probabili-ties, states with the same IDs seem to correspond toeach other.
When we compare state IDs 3 and 5, itcan be seen that, when speakers take the role of lis-teners, they reduce self-disclosure while increasingquestions and acknowledgment.
Questions seem tohave more importance in listening-oriented dialoguethan in casual conversation, indicating that listeningagents need to have a good capability of generatingquestions.
The agents would also need to explicitlyincrease acknowledgment in their utterances.
Notethat, compared to spoken dialogue, acknowledgmenthas to be performed consciously in text-based dia-logue.
When we compare state ID 4, we see thatthe speaker starts questioning in casual conversation,whereas the speaker only self-discloses in listening-oriented dialogue.
This shows that, in our data, thespeakers are successfully concentrating on makingself-disclosure in listening-oriented dialogue.5 Conclusion and Future workWe collected listening-oriented dialogue and ca-sual conversation, and compared them to find thecharacteristics of listening-oriented dialogues thatare useful for building automated listening agents.Our analysis found that it is important for listen-ing agents to self-disclose before asking questionsand that it is necessary to utter more questions andacknowledgment than in casual conversation to begood listeners.
As future work, we plan to use amore elaborate tag set to further analyze the dia-logue flows.
We also plan to extend the HMMsto Partially Observable Markov Decision Processes(POMDPs) (Williams and Young, 2007) to achievedialogue management of listening agents from data.ReferencesTimothy Bickmore and Justine Cassell.
2001.
Relationalagents: A model and implementation of building user trust.In Proc.
ACM CHI, pages 396?403.Shinsuke Higuchi, Rafal Rzepka, and Kenji Araki.
2008.
Acasual conversation system using modality and word associ-ations retrieved from the web?.
In EMNLP, pages 382?390.Naoki Isomura, Fujio Toriumi, and Kenichiro Ishii.
2006.Evaluation method of non-task-oriented dialogue system byHMM.
In Proc.
the 4th Symposium on Intelligent Media In-tegration for Social Information Infrastructure, pages 149?152.Allen E. Ivey and Mary Bradford Ivey.
2002.
Intentional Inter-viewing and Counseling: Facilitating Client Development ina Multicultural Society.
Brooks/Cole Publishing Company.Dan Jurafsky, Liz Shriberg, and Debra Biasca, 1997.
Switch-board SWBD-DAMSL Shallow-Discourse-Function Annota-tion Coders Manual.Martijn Maatman, Jonathan Gratch, and Stacy Marsella.
2005.Natural behavior of a listening agent.
Lecture Notes in Com-puter Science, 3661:25?36.Harry T. Reis and Phillip Shaver.
1998.
Intimacy as an inter-personal process.
In S. Duck, editor, Handbook of personalrelationships, pages 367?398.
John Wiley & Sons Ltd.Katsuhiko Shirai.
1996.
Modeling of spoken dialogue with andwithout visual information.
In Proc.
ICSLP, volume 1, pages188?191.Marilyn A. Walker, Rebecca Passonneau, and Julie E. Boland.2001.
Quantitative and qualitative evaluation of darpa com-municator spoken dialogue systems.
In Proc.
ACL, pages515?522.Jason D. Williams and Steve Young.
2007.
Partially observ-able Markov decision processes for spoken dialog systems.Computer Speech and Language, 21(2):393?422.127
