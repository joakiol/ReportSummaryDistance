Trainable, Scalable SummarizationUsing Robust NLP and Machine Learning*Chinatsu  Aone~,  Mary  E l len  Okurowsk i  +, James  Gor l insky~tSRA In ternat iona l  +Depar tment  of Defense4300 Fair  Lakes Cour t  9800 Savage RoadFair fax,  VA 22033 Fort  Meade,  MD 20755-6000{aonec,  gor l insk}@sra .com meokuro@af ter l i fe .ncsc .mi lAbst rac tWe describe a trainable and scalable sum-marization system which utilizes featuresderived from information retrieval, infor-mation extraction, and NLP techniquesand on-line resources.
The system com-bines these features using a trainable fea-ture combiner learned from summary ex-amples through a machine learning algo-rithm.
We demonstrate system scalabilityby reporting results on the best combina-tion of summarization features for differentdocument sources.
We also present prelim-inary results from a task-based evaluationon summarization output usability.1 In t roduct ionFrequency-based (Edmundson, 1969; Kupiec, Ped-ersen, and Chen, 1995; Brandow, Mitze, andRau, 1995), knowledge-based (Reimer and Hahn,1988; McKeown and l:Ladev, 1995), and discourse-based (Johnson et al, 1993; Miike et al, 1994; Jones,1995) approaches to automated summarization cor-respond to a continuum of increasing understandingof the text and increasing complexity in text pro-cessing.
Given the goal of machine-generated sum-maries, these approaches attempt to answer threecentral questions:?
How does the system count words to calculateworthiness for summarization??
How does the system incorporate the knowledgeof the domain represented in the text??
How does the system create a coherent and co-hesive summary?Our work leverages off of research in these threeapproaches and attempts to remedy some of the dif-ficulties encountered in each by applying a combina-tion of information retrieval, information extraction,"We would like to thank Jamie Callan for his helpwith the INQUERY experiments.and NLP techniques and on-line resources with ma-chine learning to generate summaries.
Our DimSumsystem follows a common paradigm of sentence x-traction, but automates acquiring candidate knowl-edge and learns what knowledge is necessary to sum-marize.We present how we automatically acquire candi-date features in Section 2.
Section 3 describes ourtraining methodology for combining features to gen-erate summaries, and discusses evaluation results ofboth batch and machine learning methods.
Section 4reports our task-based evaluation.2 Ext ract ing  FeaturesIn this section, we describe how the sys-tem counts linguistically-motivated, automatically-derived words and multi-words in calculating wor-thiness for summarization.
We show how the sys-tem uses an external corpus to incorporate domainknowledge in contrast to text-only statistics.
Fi-nally, we explain how we attempt o increase the co-hesiveness of our summaries by using name aliasing,WordNet synonyms, and morphological variants.2.1 Defining Single and Multi-word TermsFrequency-based summarization systems typicallyuse a single word string as the unit for counting fre-quency.
Though robust, such a method ignores thesemantic ontent of words and their potential mem-bership in multi-word phrases and may introducenoise in frequency counting by treating the samestrings uniformly regardless of context.Our approach, similar to (Tzoukerman, Klavans,and Jacquemin, 1997), is to apply NLP tools to ex-tract multi-word phrases automatically with high ac-curacy and use them as the basic unit in the sum-marization process, including frequency calculation.Our system uses both text statistics (term frequency,or t\]) and corpus statistics (inverse docmnent fre-quency, or id\]) (Salton and McGill, 1983) to derivesignature words as one of the summarization fea-tures.
If single words were the sole basis of countingfor our summarization application, noise would be62introduced both in term frequency and inverse doc-ument frequency.First, we extracted two-word noun collo-cations by pre-processing about 800 MB ofL.A.
Times/Washington Post newspaper articles us-ing a POS tagger and deriving two-word noun collo-cations using mutual information.
Secondly, we em-ployed SRA's NameTag TM system to tag the afore-mentioned corpus with names of people, entities, andplaces, and derived a baseline database for t\]*idfcal-culation.
Multi-word names (e.g., "Bill Clinton")are treated as single tokens and disambiguated bysemantic types in the database.2.2 Acqu i r ing  Knowledge  of  the  DomainKnowledge-based summarization approaches oftenhave difficulty acquiring enough domain knowledgeto create conceptual representations for a text.
Wehave automated the acquisition of some domainknowledge from a large corpus by calculating idfval-ues for selecting signature words, deriving colloca-tions statistically, and creating a word associationindex (Jing aim Croft, 1994).2.3 Recogn iz ing  Sources of  D iscourseKnowledge  through Lexlcal  Cohes ionOur approach to acquiring sources of discourseknowledge is much shallower than those of discourse-based approaches.
For a target text for summariza-tion, we tried to capture lexical cohesion of signa-ture words through name aliasing with the NameTagtool, synonyms with WordNet, and morphologicalvariants with morphological pre-processing.3 Combin ing  FeaturesWe experinaented with combining summarizationfeatures in two stages.
In the first batch stage, weexperimented to identify what features are most ef-fective for signature words.
In the second stage, wetook the best combination of features determined bythe first stage and used it to define "high scoring sig-nature words."
Then, we trained DimSum over high-score signature word feature, along with conven-tional length and positional information, to deter-mine which training features are most useful in ren-dering useful summaries.
We also experimented withthe effect of training and different corpora types.3.1 Batch  Feature  Combiner3.1.1 MethodIn DirnSum, sentences are selected for a summarybased upon a score calculated from the differentcombinations of signature word features and theirexpansion with the discourse features of aliases, syn-onyms, and morphological variants.
Every token ina document is assigned a score based on its tf*idfvalue.
The token score is used, in turn, to calculatethe score of each sentence in the document.
Thescore of a sentence is calculated as the average ofthe scores of the tokens contained in that sentence.To obtain the best combination of features for sen-tence extraction, we experimented extensively.The summarizer allows us to experiment withboth how we count and what we count for both in-verse document frequency and term frequency val-ues.
Because different baseline databases can affectidfvalues, we examined the effect on summarizationof multiple baseline databases based upon nmltipledefinitions of the signature words.
Similarly, the dis-course features, i.e., synonyms, morphological vari-ants, or name aliases, for signature words, can affecttf values.
Since these discourse features boost theterm frequency score within a text when they aretreated as variants of signature words, we also ex-amined their impact upon summarization.After every sentence is assigned a score, the top nhighest scoring sentences are chosen as a summaryof the content of the document.
Currently, the Dim-Sum system chooses the number of sentences equalto a power k (between zero and one) of the totalnumber of sentences.
This scheme has an advantageover choosing a given percentage of document sizeas it yields more information for longer documentswhile keeping summary size manageable.3.1.2 Eva luat ionOver 135,000 combinations of the above pa-rameters were performed using 70 texts fromL.A.
Times/Washington Post.
We evaluated thesummary results against the human-generated x-tracts for these 70 texts in terms of F-Measures.
Asthe results in Table 1 indicate, name recognition,alias recognition and WordNet (for synonyms) allmake positive contributions to the system summaryperformance.The most significant result of the batch testswas the dramatic improvement in performance fromwithholding person names from the feature combi-nation algorithm.The most probable reason for thisis that personal names usually have high idf values,but they are generally not good indicators of topicsof articles.
Even when names of people are associ-ated with certain key events, documents are not usu-ally about these people.
Not only do personal namesappear to be very misleading in terms of signatureword identification, they also tend to mask synonymgroup performance.
WordNet synonyms appear tobe effective only when names are suppressed.3.2 ' IYainable Feature  Combiner3.2.1 MethodWith our second method, we developed a train-able feature combiner using Bayes' rule.
Oncewe had defined the best feature combination forhigh scoring tf*idf signature words in a sentencein the first round, we tested the inclusion of coin-lnonly acknowledged positional and length informa-63I Entity I Place I Person \[Alias \[Syn.
II F-M I+ ++ ++ ++ ++ + ++ + ++ + ++ + ++ + 41.3+ 40.7+ 40.439.6?
39.539.037.4+ + 37.4+ 37.2q- 36.7Text.
Set Training?
I F-M Lead,latwp-devl NO 41.3latwp-devl YES 49.9 48.2latwp-testl NO I 31.9latwp-testl YES I 44.6 42.0pi-test 1 NO t 40.5pi-testl YES I 49.7 47.7Table 2: Results on Different Test Sets with or with-out TrainingTable h Results for Different Feature Combinationstion.
From manually extracted summaries, the sys-tem automatically learns to combine the followingextracted features for summarization:?
short sentence length (less than 5 words)?
inclusion high-score tJaidfsignature words in asentence?
sentence position in a document (lst, 2nd, 3rdor 4th quarter)?
sentence position in a paragraph (initial.
me-dial, final)Inclusion in the high scoring t\]* idf signature wordset was determined by a variable system parameter(identical to that used in the pre-trainable version ofthe system).
Unlike Kupiec et al's experiment, wedid not use the cue word feature.
Possible values ofthe paragraph feature are identical to how Kupiec etal.
used this feature, but applied to all paragraphsbecause of the short length of the newspaper articles.3.2.2 Eva luat ionWe performed two different rounds of experi-ments, the first with newspaper sets and the secondwith a broader set from the TREC-5 collection (Har-man and Voorhees, 1996).
In both rounds we exper-imented with* different feature sets?
different data sources?
the effects of training.In the first round, we trained our system on 70texts from the L.A. Times/Washington Post (latwp-devl) and then tested it against 50 new texts fromthe L.A. Times/Washington Post (latwp-testl) and50 texts from the Philadelphia Inquirer (pi-testl).The results are shown in Table 2.
In both cases, wefound that the effects of training increased systemscores by as much as 10% F-Measure or greater.
Ourresults are similar to those of Mitra (Mitra, Sing-hal, and Buckley, 1997), but our system with thetrainable combiner was able to outperform the leadsentence summaries.F-M Sentence\] High I DocumentLength Score Position24.624.6 +39.2 +39.739.739.7 +39.739.7 +43.845.145.5 +45.7 +46.646.6 +48.449.9 +++++++++ParagraphPosition++++++ ++ +++++ ++ +Table 3: Effects of Different Training FeaturesTable 3 summarizes the results of using dif-ferent training features on the 70 texts fromL.A.
Times/Washington Post (latwp-devl).
It is ev-ident that positional information is the most valu-able.
while the sentence length feature introducesthe most noise.
High scoring signature word sen-tences contribute, especially in conjunction with thepositional information and the paragraph feature.High Score refers to using ant\]* idfmetric with Word-Net synonyms and name aliases enabled, personnames suppressed, but all other name types active.The second round of experiments were conductedusing 100 training and 100 test texts for each of sixsources from the the TREC 5 corpora (i.e., Associ-ated Press, Congressional Records, Federal Registry,Financial Times, Wall Street Journal, and Ziff).Each corpus was trained and tested on a large base-line database created by using multiple text sources.Results on the test sets are shown in Table 4.
Thediscrepancy in results among data sources suggeststhat summarization may not be equally viable forall data types.
This squares with results reportedin (Nomoto and Matsumoto, 1997) where learnedattributes varied in effectiveness by text type.64Text Setap-testlcr-testlfr-testlft-testlwsj-testlzf-testl\] F-M I Precision \] Recall \] Short \[ High Score \[ Doc.
Position49.7 47.5 52.1 YES YES YES36.1 35.1 37.0 YES NO YES38.4 33.8 44.5 YES NO YES46.5 41.8 52.3 YES YES YES51.5 48.5 54.8 YES NO YES46.6 45.0 48.3 NO YES YESPara.
PositionYESYESYESNOTable 4: Results of Summaries for Different Corpora4 Task -based  Eva luat ionThe goal of our task-based evaluation was to de-termine whether it was possible to retrieve auto-matically generated summaries with similar preci-sion to that of retrieving the full texts.
Underpin-ning this was the intention to examine whether ageneric summary could substitute for a full-text doc-ument given that a common application for summa-rization is assumed to be browsing/scanning sum-marized versions of retrieved documents.
The as-sumption is that summaries help to accelerate thebrowsing/scanning without information loss.Miike et al (1994) described preliminary experi-ments comparing browsing of original full texts withbrowsing of dynamically generated abstracts and re-ported that abstract browsing was about 80% ofthe original browsing function with precision andrecall about the same.
There is also an assumptionthat summaries, as encapsulated views of texts, mayactually improve retrieval effectiveness.
(Brandow,Mitze, and Rau, 1995) reported that using program-matically generated sulnmaries improved precisionsignificantly, but with a dramatic loss in recall.We identified 30 TREC-5 topics, classified by theeasy/hard retrieval schema of (Voorhees and Har-man, 1996), five as hard, five as easy, and the re-maining twenty were randomly selected.
In our eval-uation, INQUERY (Allan et al, 1996) retrieved andranked 50 documents for these 30 TREC-5 topics.Our summary system summarized these 1500 textsat 10%.reduction, 20%, 30%, and at what our sys-tem considers the BEST reduction.
For each levelof reduction, a new index database was built for IN-QUERY, replacing the full texts with summaries.The 30 queries were run against he new database,retrieving 10,000 documents per query.
At thispoint, some of the summarized versions weredropped as these documents no longer ranked in the10,000 per topic, as shown in Table 5.
For eachquery, all results except for the documents umma-rized were thrown away.
New rankings were com-puted with the remaining summarized ocuments.Precision for the INQUERY baseline (INQ.base) wasthen compared against each level of the reduction.Table 6 shows that at each level of reduction theoverall precision dropped for the summarized ver-sions.
With more reduction, the drop was more dra-Precision at INQ.BEST I5 docs .8000 .800010 docs .8000 .780015 docs .7465 .720020 docs .7600 .720030 docs .7067 .6733Table 7: Precision for 5 High Recall Queriesmatic.
However, the BEST summary version per-formed better than the percentage methods.We examined in more detail document-level aver-ages for five "easy" topics for which the INQUERYsystem had retrieved a high number of texts.
Ta-ble 7 reveals that for topics with a high INQUERYretrieval rate the precision is comparable.
We positthat when queries have a high number of relevantdocuments retrieved, the summary system is morelikely to reduce information rather than lose infor-mation.
Query topics with a high retrieval rate arelikely to have documents on the subject matter andtherefore the summary just reduces the information,possibly alleviating the browsing/scanning load.We are currently examining documents lost in there-ranking process and are cautious in interpretingresults because of the difficulty of closely correlatingthe term selection and ranking algorithms of auto-matic IR systems with human performance.
Our ex-perimental results do indicate, however, that genericsummarization is more useful when there are manydocuments of interest o the user and the user wantsto scan summaries and weed out less relevant docu-ment quickly.5 SummaryOur summarization system leverages off research ininformation retrieval, information extraction, andNLP.
Our experiments indicate that automatic sum-marization performance can be enhanced by discov-ering different combinations of features through amachine learning technique, and that it can exceedlead summary performance and is affected by datasource type.
Our task-based evaluation reveals thatgeneric summaries may be more effectively appliedto high-recall document, retrievals.65Run INQ.base I INQ.10% \] INQ.20% I INQ.30% \[ INQ.BEST IRetrieved 1500 1500 1500 1500 1500Relevant 4551Rel-ret 4154551 4551 4551 4551294 (-29.2%) 332 (-20.0%) 335 (-19.3%) 345 (-16.9%)Table 5: INQUERY Baseline Recall vs. Summarized VersionsPrecision at5 docs 0.413310 docs 0.370015 docs 0.35110.338330 docs 0.30670.3267 (-21.0)0.2600 (-29.7)0.2400 (-31.6)0.2217 (-34.5)0.2056 (-33.0)INQ.2O% I INQ.30%0.3800 (- 8.1) 0.3067 (-25.8)0.2800 (-24.3) 0.2933 (-20.7)0.2800 (-20.3) 0.2867 (-18.3)0.2600 (-23.1) 0.2733 (-19.2)0.2400 (-21.7) 0.2522 (-17.8)INQ.BEST0.3333 (-19.4)0.3100 (-16.2)0.2867 (-18.3)0.2717 (-19.7)0.2556 (-16.7)Table 6: INQUERY Baseline Precision vs. Summarized VersionsReferencesAllan, J., J. Callan, B. Croft, L. Ballesteros,J.
Broglio, J. Xu, and H. Shu Ellen.
1996.
In-query at trec-5.
In Proceedings of The Fifth TextREtrieval Conference (TREC-5).Brandow, Ron, Karl Mitze, and Lisa Rau.
1995.Automatic ondensation ofelectronic publicationsby sentence selection.
Information Processing andManagement, 31:675-685.Edmundson, H. P. 1969.
New methods in automaticabstracting.
Journal of the Association for Com-puting Machinery, 16(2):264-228.Harman, Donna and Ellen M. Voorhees, editors.1996.
Proceedings of The Fifth Text REtrievalConference (TREC-5).
National Institute of Stan-dards and Technology, Department of Commerce.Jing, Y. and B. Croft.
1994.
An Association The-saurus for Information Retrieval.
Technical Re-port 94-17.
Center for Intelligent Information Re-trieval, University of Massachusetts.Johnson, F. C., C. D. Paice, W. J.
Black, and A. P.Neal.
1993.
The application of linguistic process-ing to automatic abstract generation.
Journal ofDocumentation and Text Management, 1(3):215-241.Jones, Karen Sparck.
1995.
Discourse modeling forautomatic summaries.
In E. Hajicova, M. Cer-venka, O. Leska, and P. Sgall, editors, Prague Lin-guistic Circle Papers, volume 1, pages 201-227.Kupiec, Julian, Jan Pedersen, and Francine Chen.1995.
A trainable document summarizer.
In Pro-ceedings of the 18th Annual International SIGIRConference on Research and Development in In-formation Retrieval, pages 68-73.McKeown, Kathleen and Dragomir Radev.
1995.Generating summaries of multiple news articles.In Proceedings of the 18th Annual InternationalSIGIR Conference on Research and Developmentin Information, pages 74-78.Miike, Seiji, Etsuo Itho, Kenji Ono, and KazuoSumita.
1994.
A full text retrieval system witha dynamic abstract generation function.
In Pro-ceedings of 17th Annual International ACM SI-GIR Conference on Research and Development inInformation Retrieval, pages 152-161.Mitra, Mandar, Amit Singhal, and Chris Buckley.1997.
An Automatic Text Summarization andText Extraction.
In Proceedings of IntelligentScalable Text Summarization Workshop, Associa-tion for Computational Linguistics (ACL), pages39-46.Nomoto, T. and Y. Matsumoto.
1997.
Data relia-bility and its effects on automatic abstraction.
InProceedings of the Fifth Workshop on Very LargeCorpora.Reimer, Ulrich and Udo Hahn.
1988.
Text con-densation as knowledge base abstraction.
In Pro-ceedings of the 4th Conference on Artificial Intel-ligence Applications (CAIA), pages 338-344.Salton, G. and M. McGill, editors.
1983. hdroduc-lion to Modern Information Retrieval.
McGraw-Hill Book Co., New York, New York.Tzoukerman, E., J. Klavans, and C. Jacquemin.1997.
Effective use of naural language processingtechniques for automatic onflation of multi-wordterms: the role of derivational morphology, partof speech tagging and shallow parsing.
In Pro-ceedings of the Annual International ACM SIGIRConference on Research and Development of In-formation Retrieval, pages 148-155.Voorhees, Ellen M. and Donna Harman.
1996.Overview of the fifth text retrieval conference(tree-5).
In Proceedings of The Fifth Text RE-trieval Conference (TREC-5).66
