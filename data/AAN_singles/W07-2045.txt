Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 215?218,Prague, June 2007. c?2007 Association for Computational LinguisticsLCC-SRN: LCC?s SRN System for SemEval 2007 Task 4Adriana BadulescuLanguage Computer Corporation1701 N Collins Blvd #2000Richardson, TX, 75080adriana@languagecomputer.comMunirathnam SrikanthLanguage Computer Corporation1701 N Collins Blvd #2000Richardson, TX, 75080srikanth@languagecomputer.comAbstractThis document provides a description ofthe Language Computer Corporation (LCC)SRN System that participated in the SemE-val 2007 Semantic Relation betweenNominals task.
The system combines theoutputs of different binary and multi-classclassifiers build using machine learning al-gorithms like Decision Trees, SemanticScattering, Iterative Semantic Specializa-tion, and Support Vector Machines.1 IntroductionThe Semantic Relations between Nominals taskfrom SemEval 2007 focuses on identifying the se-mantic relations that hold between two argumentsmanually annotated with word senses (Girju et al2007).The previous work in identifying semantic rela-tions between nominals focuses on finding one ormore relations in text for specific syntactic patternsor constructions (like genitives and noun com-pounds) using semi-automated and automated sys-tems.
An overview of some of these methods canbe found in (Badulescu, 2004).The LCC SRN system, developed during theSRN training period, was for us, the beginning of adifferent approach to semantic relations detection:detecting semantic relations in text without using asyntactic pattern.
Our existing work on semanticrelation detection was on detecting semantic rela-tions in text (one or more at a time) at differentlevels in the sentence using different syntactic pat-terns like genitives, noun compounds, verb-arguments, etc.For SRN, we built a new system that combinesthe output of the pattern dependent classifiers withthe new pattern-independent classifiers for betterresults.The remainder of this paper is organized as fol-lows:  Section 2 describes our system, Section 3details the experimental results, and Section 4summarizes the conclusions.2 System descriptionThe system consists of two types of classifiers:classifiers that do not use the syntactic parsed treeand that were built specifically for the SemEval2007 Task 4(SRN) and classifiers that use specificsyntactic pattern to determine the semantic rela-tions and there were previously developed at LCCand then adapted to the SRN task (SRNPAT).The classifiers for each type were built from an-notated examples using supervised machine learn-ing algorithms like Decision Trees (DT)1, SupportVector Machines (SVM) 2 , Semantic Scattering(SS) (Moldovan and Badulescu, 2005) , IterativeSemantic Specialization (ISS) (Girju, Badulescu,and Moldovan, 2006), Na?ve Bayes (NB) 3  andMaximum Entropy (ME)4.The outputs of different classifiers (built usingdifferent types of machine learning algorithmswere combined and ranked using predefined rules.Figure 1 shows the architecture of our SRNsystem.1C5.0., http://www.rulequest.com/see5-info.html2LIBSVM, www.csie.ntu.edu.tw/~cjlin/libsvm/3jBNC, http://jbnc.sourceforge.net4http://homepages.inf.ed.ac.uk/s0450736/maxent_tool-kit.html215Pre-processAnnotated treeText Processing ToolsClassification[Arg1, Arg2, Relation,Score,Classifier][NPArg1, NPArg2, Relation,Score, Classifier]SRNREL=1, ..7, AllREL1, ?, REL7, or NULDT SV MESRNPATPAT=1-M REL=1, ..,7, AllREL1, ?, REL7, or NULDT SVSS ISSFeatures Feature  ExtractionFeatureSetsSRN[Arg1Arg2Pattern, Arg1, Arg2]FeatureSetsSRNPAT[Pattern, NPArg1, NPArg2]Selection Relation Selection[Arg1, Arg2, Relation, Score]Sentences AnnotationsPatterns[Pattern, NPArg1, NPArg2]Pattern Matching[Arg1Arg2Pattern, Arg1, Arg2]Argument DetectionOutput Generate OutputREL1: SENTID Value REL7: SENTID Value?Pre-processPre-processClassificationClassificationFeaturesFeaturesSelectionSelectionPatternsPatternsOutputFigure 1.
The architecture of our SRN system.2.1 Text PreprocessingThe sentences were processed using an in-housetext tokenizer, Brill?s part-of-speech tagger, an in-house WordNet?based concept detector, an in-house Named Entity Recognizer, and an in-housesyntactic parser.Then, the syntactic and semantic informationobtained using these tools (concepts, part ofspeech, named entities, etc) or obtained from thesensekeys for the arguments as provided by theTask 4 organizers (e.g.
word senses, lemmas, etc)were mapped into the syntactic trees.
If an argu-ment corresponds to more than one tree node, theannotation was mapped to the phrase containingthe two nodes.2.2 Learning and Classification MethodsThe core of our system is the learning and clas-sification module.We used two types of methods: pattern-dependent that uses the syntactic parsed trees forextracting and assigning a label to the argumentsand pattern-independent that creates classifiersform all the examples disregarding the pattern inthe tree.2.2.1 Pattern-independent Methods (SRN)Considering the limited number of examples foreach pattern, we developed pattern-independentmethods for classifying the semantic relations us-ing the provided argument annotations and thecontext from the sentence.We built two types of classifiers: binary thatfocuses on building a classifier for a specific rela-tion (SRNREL) and multi-class methods that buildclassifiers for all the SRN relations (SRN).
Table 1presents the accuracy of the classifiers built usingdifferent machine learning algorithms.Relation DT SVM ME1 52.10 46.15 46.672 41.40 30.76 60.003 61.70 51.61 63.334 59.30 52.17 53.335 58.60 39.99 50.006 71.70 24.99 73.337 50.00 57.13 43.33Avg 56.40 43.26 55.71Table 1.
The accuracy of the SRNREL classifiersbuilt using different machine learning algorithms.The classifiers were built using lexical, seman-tic, and syntactic features of the arguments, theirphrases, their clauses, their common phrase/clause,and their modifier or head phrase.
The system usesWordNet, an in-house Named Entity Recognizer,and an in-house Syntactic Parser for determiningthe values of some of these features.
Table 2 pre-sents the list of features used by the SRN classifi-ers.Argument?s lexical, semantic, and syntactic features: thesurface form, the label (POS tag or phrase label), the namedentity (human, group, location, etc), the WordNet hierarchy(entity, group, abstraction, etc), the Semantic Scatteringclass (e.g.
object, substance, etc), the grammatical role (sub-ject or object of the clause), the syntactic parser structure,the POS Pattern (the sequence of POS of the words from theargument), and the phrase pattern (the sequence of labels ofthe phrases, words from the argument);Argument's phrase features:  surface form, label, gram-matical role, named entity, POS pattern, Phrase patterns;Argument's Modifier/Head features: the label, surfaceforms, NE, and WN Hierarchy  for the first modifier, postmodifier, pre-modifier, and head;Arguments' common tree node features: label, namedentity, grammatical role, POS pattern, and phrase pattern,the tree path between arguments, and their order in tree;Arguments' clause: label, verb, voice, POS pattern, phrasepattern.Table 2.
The list of features used for the SRN classi-fiers.2.2.2 Pattern-dependent Methods (SRNPAT)The second type of methods we used, were forparticular patterns frequent in the training corpus.Table 3 shows the list of most frequent patterns inthe training corpus.
For having general pattern andcovering the arguments that correspond to more216than one node in a tree, we considered as argumentthe noun phrase that contains the nominal insteadof the node for the nominal.Pattern name ExampleNoun compounds:NN1 NN2If you are cleaning a <e1>coffee</e1><e2>maker</e2> that hasn't beencleaned regularl271076ady, repeat this step again with a freshvinegar and water mixture.Of-genitives:NP1 of NP2The incoming <e1>chairman</e1> ofthe <e2>committee</e2> is promisingan array of oversight investigationsthat could provoke sharp disagreementwith Republicans and the WhiteHouse.S-genitives:NP1 ?s NP2This is the <e1>government</e1>'s<e2>effort</e2> to encourage moreemployers to open up childcare centresat the respective ministries and gov-ernment departments.Prepositionalconstructions:NP1 IN NP2I believe that unless we take this issueseriously, the red squirrel is facingeventual <e1>extinction</e1> fromthe <e2>woods</e2> of Scotland.Verbal construc-tions:NP1 VB NP2On both of my systems, the<e1>reboot</e1> produced the omi-nous <e2>message</e2> 'Missingoperating system'.Verbal preposi-tional construc-tions:NP1 VB IN NP2Manila radio station DZMM quotedsurvivors as saying that the<e1>fire</e1> started with an<e2>explosion</e2> in the cargo holdand spread across the ship within min-utes.Table 3.
The most frequent patterns found in thetraining corpus.For the pattern-dependent methods we adaptedsome of our existing binary and multi-class classi-fiers to work with the SRN relations.For the SRN system we used only one binaryclassifier built for the Part-Whole relation (relation6) using the ISS learning algorithm andtrained/tested on the examples used in (Girju,Badulescu, and Moldovan, 2006) and differentmulti-class classifiers for the first 4 patterns fromTable 3 built using DT, SVM, SS, and NB learningalgorithms trained on a corpus annotated with 40semantic relations (extracted from Wall StreetJournal articles from the TreeBank collection andLATimes articles from TREC 9 collection) thatincludes the 7 SRN relations (or equivalents).
(Badulescu, 2004) gives more details on this list ofrelations (definitions, examples, distribution oncorpus, etc).
Table 4 shows the accuracy of theseclassifiers on other WSJ and LAT articles for the40 LCC relations and respectively Part-Whole rela-tion for the most frequent patterns from the SRNcorpus (Table 3).Pattern cluster SS DT NB SVM ISSNoun compounds 52.54 47.8 53.45 74.79 73.59S-genitives 62.27 56.2 58.27 72.66 87.26Of-genitives 67.55 53.1 54.63 72 87.26Prepositional con-structions43.48 43.3 41.92 64.52 75.97Table 4.
The accuracy of the SRNPAT classifiers forthe list of 40 LCC relations and the Part-Whole Rela-tion.2.3 Relation SelectionAny of the SRN or SRNPAT classifiers can returna relation for a pair of arguments.
The best relationis selected by weighting them using the followingpredefined rules: The relations returned by the SRN classifiersweight more than the ones returned by SRNPATclassifiers because they were trained on the taskannotated examples The relations returned by the binary classifiersweight more than the ones returned by multi-classclassifiers because they focus on one relation andtherefore are more precise.3 Experimental Results3.1 Experiments on TestingDuring the competition we performed severalexperiments to assess the correct combination ofclassifiers that leads to the best results.The organizer provided 140 examples for eachof the 7 relations.
For testing the classifiers wetrained the system on the first 110 examples andtested it on the last 30 of them.We performed different sets of experiments. Experiments with one type of classifiers.These experiments showed that ME has a best per-formance (55.1) 10.05 more than DT and 8.05more than SV.
ME also got the highest score forCause-Effect, while DT obtained the best score forProduct-Producer. Experiments with multiple classifiers.
Theseexperiments showed that DT+SV+SS+ISS has thebest score (66.72) followed by DT+SS+ISS with55.66.
Also by adding the SS and ISS classifiersthe DT score increased with 10.51, the SV scorewith 5.81 and the DT+SV with 20.57.217 Experiments with types of methods.
Theseexperiments showed that the SRN methods (with ascore 0.44) are better than the SRNPAT methods(with a score of 0.41) with 0.03 which was ex-pected since SRN were trained on provided exam-ples.Table 5 shows the results of our SRN systemwhen using specific classifiers or a combination ofclassifiers.
The time did not permit us to do anyexperiments with the ME and NB classifiers.Classifier Combination Average F-measureDT 45.05SV 47.05ME 55.10DT+SV 46.15DT+SS+ISS 55.66SV+SS+ISS 52.96DT+SV+SS+ISS 66.72SRN 44.31SRNPAT 41.15Table 5.
The results of some of our experiments withthe different classifiers on the testing corpus.We submitted the DT+SS+ISS version becauseof its closeness to the normal distribution ratherthan DT+SV+SS+ISS that had a better f-measurebut it was closer to All-True.
The evaluation re-sults showed that the testing examples we usedwere representative and the DT+SV+SS+ISS pro-duce better results.3.2 ResultsTable 6 shows the results obtained by our sys-tem on the evaluation corpus for the B4 case (usingWordNet but not the query and all the training ex-amples.Relation Precision Recall F-measure Accuracy1 50.8 73.2 60.0 50.02 54.5 31.6 40.0 53.83 66.7 100.0 80.0 66.74 80.0 22.2 34.8 63.05 42.2 65.5 51.4 42.36 39.6 80.8 53.2 48.67 57.1 31.6 40.7 51.4Avg 55.9 57.8 51.4 53.7Table 6.
The results of our system on the evaluationcorpus.Table 7 shows a comparison of our results withthe following baseline systems: All-True, a systemthat always returns true, Majority, a system thatalways returns the majority value from the training,and Prob-Match, a system that randomly generatethe value.
We have obtained a larger precision andaccuracy than the All-True and the Prob-Matchsystems.
However, we obtained a lower recall andtherefore an F-measure.System Preci-sionRecall F-measureAccuracyAll-True 48.5 100.0 64.8 48.5Majority 81.3 42.9 30.8 57.0Prob-Match 48.5 48.5 48.5 51.7LCC-SRN 55.9 57.8 51.4 53.7Table 7.
Comparison with the baselines.3.3 DiscussionsThe results are promising.
However, there is stillroom for improvement.
The system was developedin a limited time, and therefore it could have beenbenefited from more features, feature selection,more experiments, a more complex relation selec-tion scheme (using learning), more patterns, andmore types of machine learning algorithms (espe-cially unsupervised ones).4 ConclusionWe presented a system for classifying the semanticrelations between nominals that combines the re-sults of different methods (pattern-dependent orpattern-independent) and machine learning algo-rithms (decision tree, support vector machines, se-mantic scattering, maximum entropy, na?ve bayes,etc).
The classifiers use lexical, semantic, and syn-tactic features and external resources like WordNetand an in-house Named Entity dictionary.ReferencesAdriana Badulescu.
2004.
Classification of SemanticRelations between Nouns.
PhD Dissertation.
Univer-sity of Texas at Dallas.Dan Moldovan and Adriana Badulescu.
2005.
A Seman-tic Scattering Model for the Automatic Interpretationof Genitives.
In Proceedings of HLT/EMNLP 2005.Roxana Girju, Adriana Badulescu, and Dan Moldovan.2006.
Automatic Discovery of Part-Whole Relations.Computation Linguistics, 32:1.Roxana Girju et al 2007.
Classification of SemanticRelations between Nominals: Description of Task 4in SemEval-1, In Proceedings of ACL-2007, SemE-val-1 Workshop.218
