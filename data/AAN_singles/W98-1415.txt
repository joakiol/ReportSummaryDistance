Clause Aggregation Using Linguistic KnowledgeJames ShawDept.
of Computer ScienceColumbia UniversityNew York, NY 10027, USAshaw@cs.columbia.eduAbstractBy combining multiple clauses into one single sentence, a text generation system can expressthe same amount of information i  fewer words and at the same time, produce a great varietyof complex constructions.
In this paper, we describe hypotactic and paratactic operators forgenerating complex sentences from clause-sized semantic representations.
These two types ofoperators are portable and reusable because they are based on general resources such as thelexicon and the grammar.1 In t roduct ionAn expression is more concise than another expression if it conveys the same amount of informa-tion in fewer words.
Complex sentences generated by combining clauses are more concise thancorresponding simple sentences because multiple references to the recurring entities are removed.For example, clauses like "Jones is a patient" and "Jones has hypertension" can be combined intoa more concise sentence "Jones is a hypertensive patient.
'~ To illustrate the common occurrenceof such repeated entities in generation, let us take a shipping company's database as an example.Each database tuple being conveyed is transformed into one or multiple propositions or clauses(we use these terms interchangeably throughout the paper).
Each proposition refers to a piece ofinformation which usually corresponds to a simple sentence.
The database might Contain multipleshipments to the same location possibly on the same day.
Generating a sentence for each tuple sep-arately would containrepetitive and potentially redundant references to the same location Or date.Though we used a relational database as the example, the observation about recurring entities inthe input is also valid for other types of input, such as execution traces from expert systems.CASPER (Clause Aggregation in Sentence PlannER) is a sentence planner which focuses ongenerating concise sentences.
Clause aggregation can happen at three levels: inferential, rhetori-cal, and linguistic.
At the inferential level, user modeling, domain knowledge, and common sensereasoning are used to reduce the number of concepts to convey.
Such operations are implementedin the content planner and clauses are combined without consulting lexical resources.
Text sum-marization is an application which uses inferential operators extensively.
For example, the twosentences "John hit Mary" and "Mary kicked John" might imply that "John and Mary fought.
"To define a set of inferential operators for unrestricted text is beyond the state-of-art.
Becauseit is unlikely that the inferential operators for our domains (medical briefings and telephone net-work plan descriptions ) will be reusable for other applications, we have directed our effort intoaggregation operations at other levels.
At the rhetorical level, clauses are combined based on theirrhetorical relationships \[Mann and Thompson, 1986\], such as CONTRAST and CONDITION.
Wewill take advantage of such information in future aggregation work.
At the linguistic level, lexicaland Syntactic information are used to combine clauses.
In this paper, we concentrate on two types138IIII!II!1iii lI!1I!
!1ilIIThe patient's past medical history is significant for bladder carcinoma1, status post cystectomy witha urostomy tube insertion2, left nephrolithiasis~, status post surgery4, recurrent syncopes, questionablevagovagal6, a neurological workup was negativer, and the EPS was negatives, abdominal aortic aneurysmapproximately 5 cmg, high cholesterol10, exertional nginan, past tobacco smoker, quit about one year ago12.Figure 1: The sentence with maximum number of propositions in the corpusOf linguistic aggregation operators: hypotactic and paratactic.
The term, hypotaxis, describes therelation between a dependent element and its dominant element.
Hypotactic operators transformone clause into a modifier and attach the modifier to the dominant clause.
In  contrast, parat-actic aggregation operators combine clauses together using constructions of equal status, such as.coordination.CASPER is used in two separate projects, MAGIC (Multimedia Abstract Generation forIntensive Care) and PLANDoc ,  to increase the fluency of the generated text.
MAGIC\[Dalal et al, 1996, MeKeown et al, 1997\] automatically generates multimedia briefings to describethe post-operative status of a patient after undergoing Coronary Artery Bypass Graft (CABG)surgery.
It uses the existing computerized information infrastructure in the operating rooms atColumbia Presbyterian Medical Center.
PLANDoc\[Kukich et al,  1994, McKeown et al, 1994\]generates English summaries based on somewhat cryptic traces of the interaction between planningengineers and LEIS-PLAN TM.
It documents the timing, placement and cost of new facilities forroutes in telephone networks.In Section 2, we present a corpus analysis to identify the complexity of the target output inMAGIC.
Section 3 describes the semantic representation used in CASPER.
Details of hypotacticoperators are presented in Section 4.
Paratactic operators are described in Section 5.
Section 6describes related work.2 Corpus AnalysisWe conducted a corpus analysis to study various styles and types of aggregation.
The corpusconsists of the first few sentences in the discharge summaries for 54 patients in the medical domain.These sentences describe patients' demographics and medical conditions pertinent o patient carein the Intensive Care Unit.
In our study, the first step was to find out how many propositions werecombined in each sentence.
A proposition is defined as a piece of information that the physician(the speaker) might choose to convey in a stand-alone sentence to the  nurses in the Intensive CareUnit (the hearer).
For example; a sentence "The patient is a 40 year old female admitted for heartsurgery:' contains three propositions: "The patient is a female.
", "The patient is 40 years old.
",and "The patient was admitted for heart surgery.
"The small corpus contained 121 sentences with 2262 words.
From the 121 sentences, we obtained418 propositions after manual decomposition, with a maximum 12 propositions in a single sentenceas shown in Figure 1.
On  average, there are 3.5 propositions per sentence.
Out of 54 summarysentences (the first sentence in each discharge summary) for each patient, doctors prefer to useprepositional phrases (PPs) ('%vith aortic stenosis") rather than relative clauses ("who likely hasendocarditis...';) to insert medical conditions into a sentence (35 occurrences vs. 4).
In only twocases, both PPs and relative clauses were used; all others have neither.
Our studies revealed thefollowing:?
Physicians produce very complex sentences.?
Coordinate constructions are the most popular aggregation operations, followed by PPs, andthen adjectives.
Present and past participle clauses are less common; relative clauses are rare.?
These aggregation operations result in long distance dependencies and non-constituent coor-dinations (conjoin'ing constituents with different syntactic types):139The analysis also indicates that people prefer using linguistic devices that are simpler (e.g., wordsover phrases over clauses) \[Scott and de Souza, 1990, Hovy, 1993\].We encountered sentences from the corpus which could be formulated more concisely.
Thedoctors did very little editing to the discharge summaries.
In this respect, the summaries aresomewhat similar to speech.
As a result, doctors prefer to use more flexible linguistic onstructions,such as PPs, instead of producing the most concise sentences.
Concepts uch as "hypertension"and "diabetes" have both noun and adjective forms.
Even though the noun form is longer (itis always used together with other words as in "patient with hypertension", or "patient who hashypertension"),  the shorter adjective form ("hypertensive patient") did not appear in the corpus.In only one case, an adjective "obese" is used instead of the PP "with obesity" to indicate medicalconditions.
Since many medical conditions have no adjective forms, such as "peptic ulcers", thespeaker is more likely to use noun forms to group together all medical conditions.
In addition,more information can be attached to nouns but not adjectives.
In  the noun form, the medicalcondition "diabetes" might be modified in the corpus, as in "type 1 diabetes with extensive ndorgan damage" and "borderline diabetes": Such flexibility with nouns explains the popularity ofits usage over adjectives.In summary, our analysis hows that a high level of aggregation is typical in the domain.
Judgingfrom the number of the PPs in comparison to relative clauses used, clause aggregation using simplersyntactic onstituents i  preferred.
DoCtors generate summaries in real-time without examining allthe information right in front of them.
As a result, they might not generate the most concisesentences.
MAGIC,  on the other hand, generates text off-line, with all the conveying informationavailable.
This would allow MAGIC  to generate more concise text by taking advantage of linguisticopportunities.3 Semantic RepresentationCASPER uses a representation influenced by Lexical-Functional Grammar (LFG)\[Kaplan and Bresnan, 1982\] and Semantic Structures \[Jackendoff, 1990\].
An example of the se-mantic representation is provided inF igure  2.
In our representation, the roles for each event orstate are PRED, ARG1, ARG2, ARG3, and MOD.
The slot PRED stores the verb concept.
Depend-ing on the concept in PRED, ARG1, ARG2, and ARG3 can take on different hematic roles, suchas Actor, Goal, and Beneficiary, respectively, as in "John gave a red book to Mary yesterday.
"The optional slot MOD stores modifiers of the PRED.
It can have one or multiple circumstantialelements, including MANNER, PLACE, or TIME.
Inside each argument slot, it too has a MOD slotto store information such as adjectives or PPs.4 Hypotactic OperatorsWe will use an example from MAGIC  to demonstrate how hypotactic operators work.
The surfaceforms of the propositions from the content planner are shown in Figure 3.
In addition to thepropositions, the content planner also indicates that the focus of the discourse is "the patient",with an entity-id, ID1.
CASPER picks the first proposition, la, as the dominant proposition becauseit contains the focus entity, and it has C-NAME entity.
Since, the entity in focus should appearas early as possible to provide a context, the proposition l a  is transformed from "The patienthas name - Jones" into the semantic representation for "Jones is a patient".
The PRED of theproposition is changed from C-IttS-ATTRIBtrrE to C-IS-INSTANCE, in addition to swapping of ARG1and ARG2.
Each proposition is represented similarly to the one shown in Figure 2.
We use theconcept C-HAS-ATTRIBUTE to denote that the entity in ARG1 has the attribute stored in ARG2.Depending on the lexical properties of the attribute in ARG2, the proposition l e  in Figure 3, canbe realized as "the patient has diabetesnou~" or "the patient is diabeticaaj".140II((pred ((pred c-has-attribute) (type EVENT) (tense present)))(argl ((pred c-doctor) (type THING)(mod ((pred c-patient) (type THING)(modify-type POSSESSOR) (entity-id IDI)))))(arg2 ((pred c-name) (type THING)(last-name "Smith"))))Figure 2: Semantic representation for lf: 'q'he patient's doctor is Smith."la.
Thelb.
Thelc.
TheId.
Thele.
TheIf.
TheIg.
Thepatient has name - Jones.patient has gender - female.patient has age - 80 year.patient has hypertension.patient has diabetes.patient's doctor has name - Smith,patient is undergoing CABG.Figure 3: input propositions for "Ms. Jones is an 80 year old hypertensive diabetic female patient of DoctorSmith undergoing CABG.
'"IIIIIIIIIIITo aggregate two propositions using hypotactic operators, the proposit ionsmust share someentities in common.
When they do, hypotactic operators try to transform one of the clauses intoa modifier.
Since the goal is to generate concise text, CASPER prefers transforming a propositioninto an adjective if possible, then a PP, a participle clause, and if i l  else fails, a relative clause.This preference of syntactically simple expressions over more complex ones was also proposed in\[Scott and de Souza, 1990\].
In the future, we plan to incorporate constraints from the corpus todetermine which aggregation operators to apply and in what order.To transform a proposition into an adjective, a propositions must satisfy the following twopreconditions.
First, the slot PLIED of the proposition being transformed must be C-HAS-ATTRIBUTE(the patient has age - 80 years).
The  other requirement is that the ARG2 of the proposition (age80 years) can be mapped to an adjective, as permitted in the lexicon.
Using the algorithm,propositions lb, lc, ld, le can all be transformed into adjectives and attached to proposition laresulting in "Jones is an 80 year old hypertensive diabetic female patient."
There are two interestingthings to note here.
First, because of the PRED of the dominant proposition is C-IS-INSTANCE, thetransformed modifiers (age, gender, etc) are attached to the ARG2 slot of the dominant proposition('% patient") instead of ARG1 ("Jones").
Second, the  sequential order of the modifiers is notdetermined yet at this stage.
The goal of CASPER is to produce a concise semantic representationfor a set of propositions and to guarantee that there is at least one way to express the result in thelater generation modules.
To guarantee expressibility \[Meteer, 1991\], CASPER looks ahead into thelexicon, but it does not make detailed lexical decisions for efficiency reasons.
The exact lexical andsyntactic decisions, including the ordering between modifiers, are made later in the lexical chooser.Consider another proposition: "the patient has peptic ulcers".
This proposition cannot betransformed into an adjective because there is no adjective form for C-PEPTIC-ULCER in the lexicon.A proposition can be transformed into a PP with a general preposition '%vith" if the PRED of theproposition is C-HAS-ATTRIBUTE and the concept in its ARG2 can be mapped into a noun phrase.
Ifwe apply the PP operator to the proposition, we would have "Jones is an 80 year old hypertensivediabetic female patient with peptic ulcers."
CASPER currently uses an ontology which can identifythat C-PEPTIC-ULCER, C-HYPERTENSION, and C=DIABETES are all medical disorders and group themtogether for cohesion.
Since all these medical conditions can be mapped to nouns but not toadjectives, they will all be realized as PPs: "Jones is an 80 year old female patient with hypertension,diabetes and peptic ulcers\]'141II((pred ((pred c - ins ta l l )  (type EVENT) (tense past)))(argl ((pred c-name) (TYPE THING)(first-name "Alice") ))(arg2 ((pred c-MS-0ffice) (type THING)))(rood "(((pred "on") (type TIME)(argl ((pred "Monday") (type TIME-THING))))((pred "for") (type BENEFICIARY)(argl ?
((pred c-name) (type THING)(first-name "John")))))))Figure 4: The attribute-value pair representation for "Alice installed MS Office for John on Monday.
I' ~().= a list.IIIIIn If in Figure 3, "The patient's doctor has name - Smith", is transformed into a PP ("ofSmith").
The POSSESSOR modifier in ARC1, as shown in Figure 2, can be transformed intoa PP using of-genitive\[Quirk et al, 1985\].
This phenomenon holds for relationships imilar topatient/doctor, such as advisor/advisee, and boss/employee.All propositions can be transformed into a relative clause of another as long as they: share acommon entity.
In the example, proposition lg does not satisfy the precondition s of the previoushypotactic operators.
In this case, it is combined as a present participle clause because presentparticiple clause is simpler and shorter.
The result of the hypotactic operators is a semanticrepresentation for "Jones is an 80 year old hypertensive diabetic female patient of Smith undergoingCABG.
"Similar to parsing long sentences, efficiency is an important issue in generating long and complexsentences.
Search space grows exponentially in respect o the length in both cases.
CASPER is ableto generate complex sentences efficiently because it delays the difficult detailed lexical decisions untilabsolutely needed.
At the sentence planning level, CASPER looks ahead into the lexicon and mergesthose propositions that satisfy the required lexical constraints.
This prevents the lexical chooserfrom ?trying to combine incompatible clauses later.
By determining sentence boundaries beforecarrying out detailed lexical decisions, CASPER cuts down the search space of the lexical chooserdrastically.
In STREAK \[Robin, 1995\], a generation system which also implements hypotacticaggregation, detailed lexical decisions are made whenever a proposition is aggregated.
This iscostly because the best lexical decisions?
for n propositions might not be useful or correct forn + !
propositions.
The strategy generates impressive complex sentences, but for some complexsentences, STREAK took more than half an hour.
Since ?CASPER does not use detailed lexicalinformation when it makes sentence boundary determination, it traded some optimal aggregationfor efficiency.
Even though the lexicon is accessed twice in our system, CASPER prunes the searchspace drastically by delaying expensive detailed lexical decisions after it knows?
about how manyconcepts are involved in the desired sentence.
Efficiency issues in generation were also addressedin \[McDonald et al, 1987, Elhadad et al, ?1997\].5 Paratactic OperatorsWe will use an imaginary human resource report system for a technical support eam as an exampleto illustrate our paratactic algorithm.
The example shown in Figure 4 has the following slots: PRED,ARC1, ARC2, MOD-BENEFICIARY,.
MOD-TIME.
We Currently have two approaches to combinepropositions using coordinate constructions.
In the first approach, adjacent propositions that haveonly 1 slot containing distinct elements are collapsed into one proposition with one conjoined slotcontaining the distinct elements.
For example, the following sentence is the result of collapsing?
two propositions with distinct elements in their MOD-BENEFICIARY slot: "Alice installed Quickenfor Mary and Peter on Tuesday."
\[McCawley, 198!\] described this ?phenomenon as ?Conjunction142i lI!IIIIIII Alice installed Excel for John onBob removed WordPerfect for John onAlice installed Powerpoint for John onCindy removed Access for John onMonday.Tuezday.Monday.Monday.Figure 5: A sample of input propositions in surface form.Alice insta~.le~_Excel for John onAlice installed Powerpoint for John onCindy removed Access for John onBob removed WordPerfect for John onMonday.Monday.Monday.Tuesday.Figure 6: The propositions in surface form after Stage 1.Reduction.
In the second approach, the conjoined propositions have distinct elements in more thanIIIIIiiIIiIIone slot.
To combine them, each conjoined proposition is generated, but deletion rules (describedlater in Section 5.4) are used to ensure the resulting sentence has the correct ellipsis.
In  the followingsentence, the two propositions are distinct at both PRED and ARG2: "John finished his work and\[John\] went home.
''1 The ARG1 in second proposition "John" is deleted.Due to limited space, we only describe the algorithm used in CASPER to produce sentenceswith coordinations.
For a more detailed discussion with relevant linguistic motivations, please see\[Shaw, 1998\].
We have divided the algorithm into four stages~ where the first three stages takeplace in the sentence planner and the last stage takes place in the lexical chooser:Stage 1: group propositions and order them according to their similarities whilesatisfying pragmatic and contextual constraints.Stage 2: determine recurring elements in the ordered propositions that will becombined.Stage 3: create a sentence boundary when the combined clause reaches pre-determined thresholds.Stage 4: determine which recurring elements are redundant and should be deleted.We will go into detail of each Stage in the following 4 sections.5.1 Group  and  Order  P ropos i t ionsCoordination allows the deletion of recurring entities at the surface level, but only if they areadjacent; that is, the propositions containing the entities are sequentially next to each other.
As aresult, the sequential order of the propositions being coordinated affects the length of the outputtext.
In Step 1, CASPER sequentializes the propositions to allow the maximum number of adjacentrecurring entities to produce concise text.For the proposition in Figure 5, the semantic representations have the following slots: PRED,ARG1, ARG2, MOD-BENEFICIARY, and MOD-TIME.
To identify which slot has the most similarityamong its elements, we calculate the number of distinct elements (NDE) in each slot across thepropositions.
For the purpose of generating concise text, CASPER prefers to group propositionswhich result in as many slots with NDE = 1 as possible.
For the propositions in Figure 5, the NDEof MOD-BENEFICIARY is 1 because all the beneficiaries are "John"; the NDEs for both PREDand MOD-TIME are2  because there are two actions, "install" and "remove", which occurred oneither "Monday" or "Tuesday"; the NDE for ARG2 is 4 because it contains "Excel", "WordPerfect","Powerpoint", and "Access"; similarly, the NDE of ARG1, the agent, is 3.1The string enclosed in symbols \[ and \] are deleted from the surface xpression, but these concepts exist in thesemantic representation.143II-((pred c-and) (type(e l ts-(((pred ((pred(argl ((pred(arg2 ((pred(mod ((pred(arg l((pred ((pred(argl ((pred(arg2 ((pred(mod ((pred(arglLIST)"installed") (type EVENT) (status RECURRING)))"Alice") (TYPE THING) (status RECURRING)))"Excel") (type THING)))"on") (type TIME)((pred "Monday") (type TIME-THING))))))"installed") (type EVENT) (status RECURRING)))"Alice") (TYPE THING) (status RECURRING)))"Outlook") (type THING)))"on") (type TIME)( (pred "Friday" ) (type TIME-THING) ) ) ) ) ) ) ) )Figure 7: The simplified representation for "Alice installed Excel on Monday and Outlook on Friday.
"IIIIThe algorithm re-orders the propositions by sorting the elements in each slots using compar-ison operators which can determine that Monday is Smaller than Tuesday, or "Alice" issmal lerthan "Bob" alphabetically.
Starting from the slots with highest NDE to the lowest, the algorithmre-orders the propositions based on the elements of each particular slot.
In this case, proposi-tions will ordered according to their ARG2 first, followed by ARG1, MOD-TIME, PRED, and MOD-BENEFICIARY.
The sorting process will put similar propositions adjacent o each other as Shownin Figur e 6.5.2 Ident i fy  Recur r ing  E lementsThe current algorithm tries to combine only two propositions at once.
In Stage 2, CASPER isconcerned with how many slots have distinct values and ?which slots they are.
When multiple?
adjacent propositions have only one slot with distinct elements, these propositions are 1-distinct.Propositions that are 1-distinct can be replaced with one proposition with one slot conjoining thedistinct elements of that slot.
In our example, the first and second propositions are 1-distinct atARG2, and they are combined into a semantic structure representing "Alice installed Excel andPowerpoint for John on Monday.
"When propositions have more than one distinct slot or their 1-distinct slot is different fromthe previous 1-distinct slot, the two propositions are said to be multiple-distinct.
Our approachin combining multiple-distinct propositions i different from previous linguistic analysis.
Insteadof removing recurring entities immediately based on transformation or substitution, the currentsystem generates every conjoined multiple-distinct proposition.
During the lexicalization of theconjoined sentence, the lexical chooser prevents the realization component from generating anystring for the redundant elements.
Our multiple-distinct oordination produces what linguistsdescribe as ?ellipsis and gapping.
Figure 7 shows the result combining two propositions that willresult in !
'Alice installed Excel on Monday and Outlook on Friday."
Some readers might noticethat PRED and ARG1 in both propositions are marked as RECURRING.
The process to delete onlysubsequent recurring elements at surface level will be explained in Section 5.4.?
5 .3  Determine  Sentence  BoundaryUnless combining the next proposition into the result proposition will exceed the pre-determinedparameters for the complexity of a sentence, the algorithm will keep on combining more propositionsinto the result proposition using 1-distinct or multiple-distinct oordination.
Based on looking atPLANDoc  output, we limit the number of propositions conjoined by multiple-distinct oordinationto two in normal cases.
Higher threshold renders ome of the sentences difficult to comprehend.In special cases where the same slots across nmltiple propositions are multiple-distinct, the pre-'-IIIIIIIIII I144II!IiiIII!IIIII!II!II!determined limit is ignored.
By taking advantage of parallel structures, these propositions can becombined using multiple-distinct procedures without making the coordinate structure more difficultto understand.
For example, the sentence "John took aspirin on Monday, penicillin on Tuesday,and Tylenol on Wednesday."
is long but quite understandable.
Similarly, conjoining a long listof 3-distinct propositions produces understandable s ntences too: "John played tennis on Monday,drove to school on Tuesday, and won the lottery on Wednesday."
These constraints allow CASP~,Rto produce asily understandable complex sentences containing a lot of information.5.4 Delete  Redundant  E lementsStage 4 handles ellipsis.
In the previous tages, adjacent elements that occur more than once amongthe propositions are marked as RECURRING, but the actual deletion decisions have not been madebecause CASPER lacks the necessary information.
T15e importance of the surface sequential ordercan be demonstrated by the following example.
In the sentence "On Monday, Alice installed Exceland \[on Monday,\] \]Alice\] removed Lotus 123.
", the elements in MOD-TIME delete forward (i.e.the subsequent occurrence of the identical constituent disappears).
When MOD-TIME elementsare realized at the end of the clause, the same elements in MOD-TIME delete backward (i.e.
theantecedent occurrence of the identical constituent disappears): "Alice installed Excel \[on Monday,\]and \[Alice\] removed Lotus 123 on Monday."
In general, if a slot is realized at the front or medialof a clause, the recurring elements in that slot delete forward.
In the first example, MOD-TIMEis realized as the front adverbial while ARG1, "Alice", appears in the middle of the clause, soelements in both slots delete forward.
On the other hand, if a slot is realized at the end position ofa clause: the recurring elements in such slot delete backward, as the MOD-TIME in second exanlple.Our extended irectionality constraint, an extension of \[Tai, 1969\]'s Directionality Constraint, alsoapplies to conjoined premodifiers and postmodifiers as well, as demonstrated by ':in Aisle 3 and \[inAisle\] 4;', and "at 3 \[PM\] and \[at\] 9 PM".Using the algorithm just described, the result is concise and easily understandable: "On Monday:Alice installed Excel and Powerpoint and Cindy removed Word for John.
Bob removed WordPerfectfor John on Tuesday."
Further discourse processing can replace the beneficiary"John" in the secondsentence with a pronoun "him".6 Re lated WorkBoth hypotactic and paratactic onstructions described in this paper have received a lot of attention ?in linguistics \[Quirk et al, 1985, Halliday, 19941 Carpenter, 1998\].
Much generation literature onaggregation was disguised under the topic "revision" \[Meteer, 1991, Robin, 1995\]\[Callaway and Lester, 1997\].
We consider clause aggregation as an integral part of a text gen-eration system, not as a revision.
The term "revision" implies that something has been generatedand then improved upon, which is not the case in these systems.
We prefer the term optimizationused by \[Dale, 1992\], which describes the phenomenon of aggregation more appropriately - it usefewer words to convey the same amount of information.In earlier systems, clause aggregations are implemented in strategic component\[Mann and Moore, 1980, Dale, 1992, Horacek, 1992\].
Logical derivations were used to combineclauses and remove easily inferable clauses in \[Mann and Moore, 1980\].
In such systems, ag-gregation decisions are made without lexical information.
Newer systems, such as \[Shaw, 1995,Wanner and Hovy, 1996, Huang and Fiedler, 1997\]: use a sentence planner to make decisions atclause level between the strategic and tactical component.With the exception of \[Scott and de Souza, 1990\] and \[Robin, 1995\], most research in aggrega-tion did not transform clauses into modifiers, such as adjectives, PP, or relative clauses, in a sys-tematic manner.
\[Scott.
and de Souza, 1990\] proposed heuristics for carrying out clause combiningbased on RST and specifically identified which rhetorical relations are appropriate for "embedding" :145which corresponds to our hypotactic operators.
We will incorporate rhetorical aggregation i  thefuture.
Robin's work on revision operators ?is similar to ours.
We have describe his work earlier inSection 4.Because sentences with coordination constructions can express a lot of information with fewwords, many text generation systems have implemented the generation of coordination expres-sions with various ?complexities \[Dale, 1992, Dalianis and Hovy, 1993, Huang and Fiedler, 1997,Shaw, 1995, Callaway and Lester, 1997\].
Most systems handles simple coordination which con-tains only one conjoined syntactic onstituents, uch as subject, verb, or object.
None of themhandles ellipsis as CASPER does.
CASPER tries to  systematically find the most concise way to ex-press the propositions by looking through all the ?
propositions.
In contrast, aggregation operatorsproposed in other work are local and does handle complex cases.
In addition, the possibility ?of toomuch information in a sentence has not received much attention.
Most research simply ignores thispossibility because the input to their sentence planners never exceeds a few clauses.7 ConclusionWe describe how hypotactic ?operators combine clauses using lexical information and how paratacticoperators produce sentences with coordination.
Through the use of look-ahead into the lexiconduring the aggregation process to guarantee xpressibility and by performing the task of sentencedelimitation before lexical choice, the system can generate complex sentences efficiently.
Sincehypotactic, and paratactic operators are reusable, further speed-up in future generation systemdevelopment is expected.8 AcknowledgmentsThe author would like to thank Kathleen McKeown for her valuable advice and encouragement.This work is supported by DARPA Contract DAAL01-94-K-0119, the Columbia University Centerfor Advanced Technology in High Performance Computing and Comnmnications in Healthcare(funded by the New York State Science and Technology ?
Foundation) and NSF Grants GER-90-2406.References\[Callaway and Lester, 1997\] Callaway, C. B. and Lester, J. C. Dynamically improving explanations: Arevision-based approach to ?explanation generation.
In Proc.
of the 15th IJCAI, pages 952-958, Nagoya,Japan.
.
- .....\[Carpenter, 1998\] Carpenter, B.
Distribution, collection and quantification: A type-logical account.
Toappear in Linguistics and Philosophy.\[Dalai et al, 1996\] Dalai, M., Feiner, S., McKeown, K., Jordan, D., Allen, B., ar/d alSafadi, Y.
MAGIC: Anexperimental system for generating multimedia briefings about post-bypass patient status.
In Proc.
1996AMIA Annual Fall Syrup, pages 684--688, Washington, DC.\[Dale, 1992\] Dale, R. Generating Referring Expressions: Constructing Descriptions in a Domain of Objectsand Processes.
MIT Press, Cambridge, MA.\[Dalianis and Hovy, 1993\] Dalianis, H. and Hovyl E. Aggregation i natural language generation.
In Proc.of the th European Workshop on Natural Language Generation, Pisa, Italy.\[Elhadad et al, 1997\] Elhadad, M., McKeown, K., and Robin, J.
Floating constraints in lexical choice.Computational Linguistics, 23(2):195-239~\[Halliday, 1994\] Halliday , M. A. K. An Introduction to Functional Grammar.
Edward Arnold, London, 2ndedition.146IIIIi l:,1i iI!1!1!1II!l\[Ii.III\[Horacek, 1992\] Horacek, H: An integrated view of text planning.
In Aspects of Automated Natural LanguageGeneration, Lecture Notes in Artificial Intelligence, 587, pages 29-44.
Springer-Verlag, Berlin.\[Hovy, 1993\] Hovy, E. H. Automated iscourse generation using discourse structure relations.
ArtificialIntelligence, 63.
Special Issue on NLP.\[Huang and Fiedler, 1997\] Huang, X. and Fiedler, A.
Proof verbalization as an application of NLG.
In Proc.of the 15th IJCAI, pages 965-970, Nagoya, Japan.\[Jackendoff, 1990\] Jackendoff, R. Semantic Structures.
MIT Press, Cambridge, MA.\[Kaplan and Bresnan, 1982\] Kaplan, R. M. and Bresnan, J. Lexical-functional grammar: A formal sys-tem for grammatical representation.
I  Bresnan, J., editor, Th_e Mental Representation f GrammaticalRelations, chapter 4.
MIT Press.\[Kukich et al, 1994\] Kukich, K., McKeown, K., Shaw, J., Robin, J., Lim, J., Morgan, N., and Phillips,J.
User-needs analysis and design methodology for an automated document generator.
In Zampolli, A.,Calzolari, N., and Palmer, M., editors, Linguistica Computazionale, Vol.
IX-X, pages 109-115.
KluwerAcademic Publishers, Norwell, MA.\[Mann and Moore, 1980\] Mann, W. C. and Moore, J.
A.
Computer as author - results and prospects.Technical Report RR-79-82, USC Information Science Institute, Marina del Rey, CA.\[Mann and Thompson, 1986\] Mann, \V.
C. and Thompson, S. A.
Rhetorical?structure th ory: Descriptionand construction of text structures.
Technical Report RS-86-174, USC Information Sciences Institute,Marina Del Rey, CA.\[McCawley, 1981\] McCawley, J. D. Everything that linguists have always wanted to know about logic (butwere ashamed to ask).
University of Chicago Press.\[McDonald et al, 1987\] McDonald, D. D., Meteer, M.
h'i., and Pustejovsky, J. D. Factors contributing toefficiency in natural anguage generation.
In Kempen, G., editor, Natural Language Generation: NewResults in Artificial Intelligence, Psychology and Linguistics, NATO ASI Series - 135, pages !59-182.Martinus Nijhoff Publishers, Boston.\[McKeown et al, 1994\] McKeown, K., Kukich, K., and Shaw, J.
Practical issues in automaticdocumentation?
generation.
In Proc.
of the 4th ACL Conference on Applied Natural Language Processing, pages 7-14,Stuttgart.\[McKeown et al, 1997\] McKeown, K., Pan, S., Shaw, J ,  Jordan, D., and Allen, B.
Language generation formultimedia healthcare briefings.
In Proc.
of the Fifth ACL Conf.
on ANLP, pages 277-282.\[Meteer, 1991\] Meteer, M. The implications of revisions for natural anguage generation.
In Paris, C. L.,Swartout, W. R., and Mann, W. C., editors, Natural Language Generation in Artificial Intelligence andComputational Linguistics, pages 155-178.
Kluwer Academic Publishers, Boston.\[Quirk et al, 1985\] Quirk, R., Greebaum, S., Leech, G., and Svartvik, J.
A Comprehensive Grammar of theEnglish Language.
Longman Publishers, London.\[Robin, 1995\] Robin, J. Revision-Based Generation of Natural Language Summaries Providing HistoricalBackground.
PhD thesis, Columbia University.\[Scott and de Souza, 1990\] Scott, D. R. and de Souza, C. S. Getting the message across in RST-basedtext generation.
In Dale, R., Mellish, C., and Zock, M., editors, Current Research in Natural LanguageGeneration, pages 47-73.
Academic Press, New York.\[Shaw, 1995\] Shaw, J. Conciseness through aggregation in text generation: In Proc.
of the 33rd A CL (StudentSession), pages 329-331.\[Shaw, 1998\] Shaw, J. Segregatory coordination and ellipsis in text generation.
In To appear in Proc.
of the17th COLING and the 36th Annual Meeting of the ACL.\[Tai, 1969\] Tai, J. H.-Y.
Coordination Reduction.
PhD thesis, Indiana University.\[Wanner and Hovy, 1996\] V~ranner, L. and Hovy, E. The HealthDoc sentence planner.
In Proc.
of the 8thInternational Natural Language Generation Workshop, pages 1-10: Sussex, UK.147
