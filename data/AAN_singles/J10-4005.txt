String-to-Dependency StatisticalMachine TranslationLibin Shen?Raytheon BBN TechnologiesJinxi Xu?
?Raytheon BBN TechnologiesRalph Weischedel?Raytheon BBN TechnologiesWe propose a novel string-to-dependency algorithm for statistical machine translation.
Thisalgorithm employs a target dependency language model during decoding to exploit long distanceword relations, which cannot be modeled with a traditional n-gram language model.
Experimentsshow that the algorithm achieves significant improvement in MT performance over a state-of-the-art hierarchical string-to-string system on NIST MT06 and MT08 newswire evaluation sets.1.
Introductionn-gram Language Models (LMs) have been widely used in current Statistical MachineTranslation (SMT) systems.
Because they treat a sentence as a flat string of tokens, adrawback of traditional n-gram LMs is that they cannot model long range word rela-tions, such as predicate?argument attachments, that are critical to translation quality.We propose a hierarchical string-to-dependency translation model that exploitsa dependency LM while decoding (as opposed to during reranking n-best output) toscore alternative translations based on their structural soundness.
In order to generatethe structured output (dependency trees) required for dependency LM scoring, transla-tion rules in our system represent the target side as dependency structures.
We restrictthe target side of the rules to well-formed dependency structures to weed out badtranslation rules and enable efficient decoding through dynamic programming.
Due tothe flexibility of well-formed dependency structures, such structures can cover a largeset of non-constituent transfer rules (Marcu et al 2006) that have been shown usefulfor MT.For comparison purposes, as our baseline, we replicated the Hiero decoder (Chiang2005), a state-of-the-art hierarchical string-to-string model.
Our experiments show thatthe string-to-dependency decoder significantly improves MT performance.
Overall, the?
10 Moulton Street, Cambridge, MA 02138.
E-mail: libinshen@gmail.com.??
10 Moulton Street, Cambridge, MA 02138.
E-mail: jxu@bbn.com.?
10 Moulton Street, Cambridge, MA 02138.
E-mail: weisched@bbn.com.Submission received: 6 March 2009; revised submission received: 1 December 2009; accepted for publication:18 March 2010.?
2010 Association for Computational LinguisticsComputational Linguistics Volume 36, Number 4improvement in BLEU score is around 2 BLEU points on NIST Arabic-to-English andChinese-to-English newswire test sets.Section 2 briefly discusses previous approaches to SMT in order to motivate ourwork.
Section 3 provides an overview of our string-to-dependency translation system.Section 4 provides a complete description of our system, including formal definitionsof well-formed dependency structures and their operations, as well as proofs abouttheir key properties.
Section 5 describes the implementation details, which includerule extraction, decoding, using dependency LM scores, and using labels in translationrules.
We discuss experimental results in Section 6, compare our work with relatedwork in Section 7, and draw conclusions in Section 8.2.
Previous Approaches to SMTPhrase-based systems (Koehn, Och, and Marcu 2003; Och 2003) had dominated SMTuntil recently.
Such systems typically treat the input as a sequence of phrases (wordn-grams), reorder them, and produce a translation for the reordered sentence based ontranslation options of each source phrase.
A prominent feature of such systems is theuse of an n-gram LM to measure the quality of translation hypotheses.
A drawback ofsuch systems is that the lack of structural information in the output makes it impossibleto score translation hypotheses based on their structural soundness.The Hiero system (Chiang 2007) was a major breakthrough in SMT.
Translationrules in Hiero contain non-terminals (NTs), as well as words, which allow the inputto be translated in a hierarchical manner.
Because both the source and target sides ofits translation rules are strings with NTs, Hiero can be viewed as a hierarchical string-to-string model.
Despite the hierarchical nature of its decoder, Hiero lacks the ability tomeasure translation quality based on structural relations such as predicate?argumentagreement.Yamada and Knight (2001) proposed a syntax-based translation model that transfersa source parse tree into a target string.
This method depends on the quality of sourceside parsing, and ignores target information during source side analysis.
Mi, Huang,and Liu (2008) later proposed a translation model that takes the source parse forest asMT input to reduce translation errors due to imperfect source side analysis.Galley et al (2004) proposed an MT model which produces target parse trees forstring inputs in order to exploit the syntactic structure of the target language.
Galleyet al (2006) formalized this approach with tree transducers (Graehl and Knight 2004)by using context-free parse trees to represent the target side.
However, it was latershown by Marcu et al (2006) and Wang, Knight, and Marcu (2007) that coverage couldbe a big issue for the constituent based rules, even though the translation rule set wasalready very large.Carreras and Collins (2009) introduced a string-to-tree MT model based on spinalTree Adjoining Grammar (TAG) (Joshi and Schabes 1997; Shen, Champollion, and Joshi2008).
In this model, a translation rule is composed of a source string and a targetelementary tree.
Target hypothesis trees are combined with the adjoining operation,and there are no NT slots for substitution as in LTAG-spinal parsing (Shen and Joshi2005, 2008; Carreras, Collins, and Koo 2008).
Without the constraint of NT slots, theadjoining operation allows very flexible composition, so that the search space becomesmuch larger.
One has to carefully prune the search space.DeNeefe and Knight (2009) proposed another TAG-based MT model.
In theirimplementation, a TAG grammar was transformed to an equivalent Tree Insertion650Shen, Xu, and Weischedel String-to-Dependency Statistical Machine TranslationGrammar (TIG).
In this way, they do not have an explicit adjoining operation in theirsystem, and as such reduce the search space in decoding.
Sub-trees are combined withNT substitution.Many researchers followed the tree-to-tree approach (Shieber and Schabes 1990) totake advantage of structural knowledge on both sides?for example, as in the papersby Hajic?
et al (2002), Eisner (2003), Ding and Palmer (2005), and Quirk, Menezes, andCherry (2005).
Although tree-to-tree models can represent rich structural informationof the input and the output, they have not significantly improved MT performance,possibly due to a much larger grammar and search space.
On the other hand, Smith andEisner (2006) showed the necessity of allowing loose transformations between the trees,which made tree-to-tree models even more complicated.3.
Overview of String-to-Dependency TranslationOur system is designed to address problems with existing SMT approaches.
It is novelin two respects.
First, it uses a dependency LM to model long-distance relations.
Sec-ond, it uses well-formed dependency structures to represent translation hypotheses toachieve an effective trade-off between model coverage and decoding complexity.3.1 Dependency-Based Translation and Language ModelsOur system generates target dependency trees as output and exploits a dependency LMin scoring translation hypotheses.
As described before, the goal of using a dependencyLM is to exploit long-distance word dependencies and as such model the quality of theoutput more accurately.Figure 1 shows an example dependency tree.
Each arrow points from a child to itsparent.
In this example, the word find is the root.For the purpose of comparison, we first show how a simplified SMT system uses ann-gram LM to score translation hypotheses:S?e = argmaxSeP(Se|Sf )w1P(Sf |Se)w2P(Se)w3 (1)where w1,w2, and w3 are feature weights.
Sf is the input and Se?s are outputs.
P(Se|Sf )is the probability of the target string given the source, and P(Sf |Ss) is the probability ofthe source given the target.
P(Se) is the prior probability of the target string Se using ann-gram LM.Figure 1The dependency tree for sentence the boy will find it interesting.651Computational Linguistics Volume 36, Number 4In comparison, the scoring function in our system is:D?
= argmaxDP(D|Sf )w1P(Sf |D)w2P(D)w3 (2)where P(D) is the dependency LM score of target dependency tree D. We will show howto compute P(D) in Section 5.4.We can rewrite Equation (2) with a linear model:D?
= argmaxDn?i=1wiFi(Sf ,D) (3)where n = 3,F1 = log P(D|Sf ),F2 = log P(Sf |D), and F3 = log P(D).
In practice, we useboth a dependency LM and a traditional n-gram LM (also known as a string LM), aswell as several other features, in our decoder.
Section 5.6 lists all the features used inour decoder.3.2 Well-Formed Dependency StructuresA central question in our system design is: What kinds of dependency structures areallowed in translation rules?
One extreme would be to allow any arbitrary multiplelevel treelets, as in Ding and Palmer (2005) and Quirk, Menezes, and Cherry (2005).One can define translation rules on any fragment of a parse/dependency tree.
It offersmaximum coverage of translation patterns, but suffers from data sparseness and a largesearch space.The other extreme would be to allow only complete (CFG) constituents.
This offersa more robust model and a small search space, but excludes many useful transfer rules.In our system, the target side hypotheses are restricted to well-formed dependencystructures (see Section 4 for formal definitions) for a trade-off between rule coverage,model robustness, and decoding complexity.
In short, a well-formed dependency struc-ture is either (1) a single rooted tree, with each child being a complete sub-tree, or (2) asequence of siblings, each being a complete sub-tree.Well-formed dependency structures are very flexible and can represent a variety ofnon-constituent rules in addition to rules that are complete constituents.
For example,the following translationhongChinese-to-English??????????????
the redis obviously useful for Chinese-to-English MT, but cannot be represented in some tree-based translation systems since the red is a partial constituent.
However, it is a validdependency structure in our system.4.
FormalismWe first formally define the well-formed dependency structures, which are used torepresent target hypotheses.
Then, we define the operations to build well-formed de-pendency structures from the bottom up in decoding.652Shen, Xu, and Weischedel String-to-Dependency Statistical Machine Translation4.1 Well-Formed Dependency Structures and CategoriesIn order to exclude undesirable structures and reduce the search space, we only allowSe whose dependency structure D is well formed, which we will define subsequently.The well-formedness requirement will be applied to partial decoding results.Based on the results of previous work (DeNeefe et al 2007), we keep two kinds ofdependency structures, fixed and floating.
Fixed structures consist of a sub-root withchildren, each of which must be a complete constituent.
We call them fixed dependencystructures because the head is known or fixed.
Floating structures consist of a numberof consecutive sibling nodes of a common head, but the head itself is unspecified.
Eachof the siblings must be a complete constituent.
Floating structures can represent manylinguistically meaningful non-constituent structures: for example, like the red, a modifierof a noun.
Only those two kinds of dependency structures are well-formed structures inour system.In the rest of this section, we will provide formal definitions of well-formedstructures and combinatory operations over them, so that we can easily manipulatethem in decoding.
Examples will be provided along with the formal definitions to aidunderstanding.Consider a sentence S = w1w2...wn.
Let d1d2...dn represent the parent word IDs foreach word.
For example, d4 = 2 means that w4 depends on w2.
If wi is a root, we definedi = 0.Definition 1A dependency structure didi+1...dj, or di..j for short, is fixed on head h, where h ?
[i, j],or fixed for short, if and only if it meets the following conditions1.
dh /?
[i, j]2.
?k ?
[i, j] and k = h, dk ?
[i, j]3.
?k /?
[i, j], dk = h or dk /?
[i, j]We say the category of di..j is (?, h,?
), where ?
means this field is undefined.Definition 2A dependency structure di...dj is floating with children C, for a non-empty set C ?
{i, ..., j}, or floating for short, if and only if it meets the following conditions1.
?h /?
[i, j], s.t.
?k ?
C, dk = h2.
?k ?
[i, j] and k /?
C, dk ?
[i, j]3.
?k /?
[i, j], dk /?
[i, j]We say the category of di..j is (C,?,?)
if j < h, which means that children are on the leftside of the head, or (?,?,C) otherwise.A category is composed of the three fields (A, h,B), where h is used to represent thehead, and fields A and B represent left and right dependents of the head, respectively.A dependency structure is well-formed if and only if it is either fixed or floating.653Computational Linguistics Volume 36, Number 4ExamplesWe represent dependency structures with graphs.
Figure 2 shows examples of fixedstructures, Figure 3 shows examples of floating structures, and Figure 4 shows ill-formed dependency structures.The structures in Figures 2 and 3 are well-formed.
Figure 4(a) is ill-formed becauseboy does not have its child word the in the tree.
Figure 4(b) is ill-formed because it is nota continuous segment due to the missing it.As for the example the red mentioned earlier, it is a well-formed floating dependencystructure.It is easy to see that a floating structure whose child set C has only one element isalso a fixed structure.
Actually, this is a desirable property on which we will introducemeta category operations later.
However, for the sake of convenience, we would like toassign a single category to each well-formed structure.Figure 2Fixed dependency structures.Figure 3Floating dependency structures.Figure 4Ill-formed dependency structures.654Shen, Xu, and Weischedel String-to-Dependency Statistical Machine TranslationDefinition 3Let structure T be well formed.
Category cat of T is defined as followscat(T) =???
(?, h,?)
if T is fixed on h(C,?,?)
if T is floating with children C on the left side AND |C| > 1(?,?,C) if T is floating with children C on the right side AND |C| > 1cat is well-defined according to Definitions 1 and 2.4.2 OperationsOne of the purposes of introducing floating dependency structures is that siblings hav-ing a common parent will become a well-defined entity, although they are not con-sidered a constituent.
We always build well-formed partial structures on the target sidein decoding.
Furthermore, we combine partial dependency structures in a way suchthat we can obtain all possible well-formed dependency structures (but no ill-formedones) during bottom?up decoding.The solution is to employ categories (introduced earlier).
Each well-formed depen-dency structure has a category.
We can apply four combinatory operations over thecategories.
If we can combine two categories with a certain category operation, we canuse a corresponding tree operation to combine two dependency structures.
The cate-gory of the combined dependency structure is the result of the combinatory categoryoperations.Operations on Well-Formed Dependency StructuresThere are four types of operations on well-formed dependency structures.
Instead ofproviding formal definitions, we use figures to illustrate these operations to make themeasy to understand.
Figure 1 shows a traditional dependency tree.
Figure 5 shows thefour operations for combining partial dependency structures, which are left adjoining(LA), right adjoining (RA), left concatenation (LC), and right concatenation (RC).
Wealways combine two well-formed structures in one of the four ways, and obtain a largerwell-formed structure.Two structures can be combined by adjoining, which is similar to the traditionaldependency formalism.
We can adjoin either a fixed structure or a floating structure tothe head of a fixed structure.Completed siblings can be combined via concatenation.
We can concatenate twofixed structures, one fixed structure with one floating structure, or two floating struc-tures in the same direction.The flexibility of the order of operation allows us to take advantage of varioustranslation fragments encoded in transfer rules.
Figure 6 shows alternative ways ofapplying operations on well-formed structures to build larger structures in a bottom?up style.
Numbers represent the order of operation.
The fact that the same dependencystructure can have multiple derivations means that we can utilize various rules learnedfrom different training samples.
Such flexibility is important for MT.655Computational Linguistics Volume 36, Number 4Figure 5Operations over well-formed structures.Figure 6Two alternative derivations of an example dependency tree.Meta Operations on CategoriesWe first introduce three meta category operations, which will later be used to definecategory operations.
Two of the meta operations are unary operations, left raising (LR)and right raising (RR), and one is the binary operation unification (UF).Definition 4Meta Category Operations LR((?, h,?))
= ({h},?,?
) RR((?, h,?))
= (?,?, {h}) UF((A1, h1,B1), (A2, h2,B2)) = NORM((A1 unionsq A2, h1 unionsq h2,B1 unionsq B2))First, the raising operations are used to turn a completed fixed structure into afloating structure, according to Theorem 1.Theorem 1A fixed structure with category (?, h,?)
for span [i, j] is also a floating structure withchildren {h} if there are no outside words depending on word h, which means that?k /?
[i, j], dk = h (4)ProofIt suffices to show that all the three conditions of floating structures hold.
Conditions 1and 2 immediately follow from conditions 1 and 2 of the fixed structure, respectively.Condition 3 is met according to Equation (4) and condition 3 of the fixed structure.
656Shen, Xu, and Weischedel String-to-Dependency Statistical Machine TranslationTherefore, we can always raise a fixed structure if we assume it is complete, that is,Equation (4) holds.Unification is well-defined if and only if we can unify all three elements and theresult is a valid fixed or floating category.
For example, we can unify a fixed structurewith a floating structure or two floating structures in the same direction, but we cannotunify two fixed structures.h1 unionsq h2 =??
?h1 if h2 = ?h2 if h1 = ?undefined otherwiseA1 unionsq A2 =??
?A1 if A2 = ?A2 if A1 = ?A1 ?
A2 otherwiseNORM((A, h,B)) =???????
(?, h,?)
if h = ?(A,?,?)
if h = ?,B = ?
(?,?,B) if h = ?,A = ?undefined otherwiseOperations on CategoriesNow we define category operations.
For the sake of convenience, we use the samenames for category operations and dependency structure operations.
We can easily usethe meta category operations to define the four combinatory category operations.
Thedefinition of the operations is as follows.Definition 5Combinatory category operationsLA((A1,?,?
), (?, h2,?))
= UF((A1,?,?
), (?, h2,?
))LA((?, h1,?
), (?, h2,?))
= UF(LR((?, h1,?
)), (?, h2,?))LC((A1,?,?
), (A2,?,?))
= UF((A1,?,?
), (A2,?,?))LC((A1,?,?
), (?, h2,?))
= UF((A1,?,?
), LR((?, h2,?
)))LC((?, h1,?
), (A2,?,?))
= UF(LR((?, h1,?
)), (A2,?,?
))LC((?, h1,?
), (?, h2,?))
= UF(LR((?, h1,?
)), LR((?, h2,?
)))RA((?, h1,?
), (?,?,B2)) = UF((?, h1,?
), (?,?,B2))RA((?, h1,?
), (?, h2,?))
= UF((?, h1,?
), RR((?, h2,?
)))RC((?,?,B1), (?,?,B2)) = UF((?,?,B1), (?,?,B2))RC((?, h1,?
), (?,?,B2)) = UF(RR((?, h1,?
)), (?,?,B2))RC((?,?,B1), (?, h2,?))
= UF((?,?,B1), RR((?, h2,?
)))RC((?, h1,?
), (?, h2,?))
= UF(RR((?, h1,?
)), RR((?, h2,?
)))657Computational Linguistics Volume 36, Number 4Based on the definitions of dependency structure operations and category op-erations, one can verify the one-to-one correspondence.
This correspondence can beformally stated in the following theorem.Theorem 2Suppose X and Y are well-formed dependency structures and OP(cat(X), cat(Y)) is well-defined.
We havecat(OP(X,Y)) = OP(cat(X), cat(Y)) (5)ProofThe proof of the theorem is rather routine, so we just give a sketch here.
One can showit by induction on the number of nodes in dependency structures.
It suffices to showthat Equation (5) holds for all the operations.
Actually, the category operations aredesigned to meet this requirement; the three fields of a category represent the headand the children on both sides.
With category operations, we can easily track the types of dependency structuresand constrain operations in decoding.Soundness and CompletenessNow we show the soundness and completeness of the operations on dependencystructures.
If we follow the operations defined herein, we will build all the well-formedstructures and only the well-formed structures.Theorem 3 (Soundness)Let X and Y be two well-defined dependency structures, and OP an operation over Xand Y.
It can be shown that OP(X,Y) is also a well-defined dependency structure.ProofTheorem 3 immediately follows Theorem 2.
Theorem 4 (Completeness)Let Z be a well-defined dependency structure with at least two nodes.
It can beshown that there exist well-formed structures X,Y and an operation OP, such thatZ = OP(X,Y).ProofIf Z is fixed on h, without losing generality, we assume g is the leftmost child (orrightmost if there is no left child) of h. We detach g from h, and obtain two sub-treesX and Y which are rooted on g and h respectively.
It can be verified that X and Y arewell-formed, and Z = LA(X,Y).If Z is floating with children {c1, c2, ..., cn}, where n > 1, we can split it into twofloating structures with children {c1} and {c2, ..., cn}, respectively.
It is easy to verifythat they are the sub-structures X and Y that we are looking for.
658Shen, Xu, and Weischedel String-to-Dependency Statistical Machine Translation5.
Implementation5.1 Translation RulesTranslation rules are central to an MT system.
In our system, each rule translates asource sub-string into a target dependency structure.
The target side of the translationrules constitutes a tree grammar.One way to define a tree grammar is in the way we described earlier.
Two well-formed structures can be combined into a larger one with adjoining or concatenation,and there is no non-terminal slot for substitution.
This is similar to tree grammarswithout substitution, such as the original TAG (Joshi, Levy, and Takahashi 1975) andLTAG-spinal (Shen, Champollion, and Joshi 2008).
A corresponding MT model wasproposed in Carreras and Collins (2009).
Search space is a major problem for such anapproach, as we described earlier.In our system, we introduced NT substitution to combat the search problem.
TheNT slots for substitution come from what we have observed in training data.
Combina-tion of well-formed dependency structures can only happen on NT slots.
By replacingNT slots with well-formed structures, we implicitly adjoin or concatenate sub-structuresbased on the dependency information stored in rules.
We extract the translation rulesfrom the training data containing word-to-word alignment and target parse trees, whichwe will explain in the next section.
A similar strategy was employed by DeNeefe andKnight (2009).
They turned a TAG into an equivalent TIG.In addition to these extracted rules, we also have special rules to adjoin or concate-nate two neighboring hypotheses.
Each of the special rules has two NT slots, but theyvary on target dependency structures.
They are comparable to the glue rules in Chiang(2005).To formalize translation rules and grammars, a string-to-dependency grammar G isa 4-tuple G = ?R,X,Tf ,Te?
where R is a set of transfer rules.
X is the only non-terminaltype.1 Tf is a set of terminals (words) in the source language, and Te is a set of terminalsin the target language.A string-to-dependency transfer rule R ?
R is a 4-tuple R = ?Sf ,Se,D,A?
whereSf ?
(Tf ?
{X})+ is a source string, Se ?
(Te ?
{X})+ is a target string, D represents thedependency structure for Se, and A is the alignment between Sf and Se.
Non-terminalalignments in A must be one-to-one.
We ignore the left hand side for both source andtarget, since there is only one NT type.5.2 Rule ExtractionNow we explain how we extract string-to-dependency rules from parallel training data.The procedure is similar to Chiang (2007) except that we maintain tree structures on thetarget side, instead of strings.Given sentence-aligned bilingual training data, we first use GIZA++ (Och andNey 2003) to generate word level alignment.
We use a statistical CFG parser to parse1 Later in the article, we will introduce label information for NTs.
However, labels are treated as softfeatures, and there is still a single NT type.
In fact, other useful information can also be treated as softfeatures, for example, length distribution for each NT observed in the training data.
Details are providedin Shen et al (2009).659Computational Linguistics Volume 36, Number 4Figure 7An example to show the rule extraction procedure.
In this example, the word it is replaced witha non-terminal X, which generates a hierarchical translation rule.the English side of the training data, and extract dependency trees with Magerman?srules (1995).
Then we use heuristic rules to extract transfer rules recursively based onword alignments and the target dependency trees.
The rule extraction procedure is asfollows.1.
Initialization:All the 4-tuples ?Pi,jf,Pm,ne ,D,A?
are valid span templates, where sourcephrase Pi,jfis aligned to target phrase Pm,ne under alignment2 A.
D is awell-formed dependency structure for Pm,ne .
All valid span templates arevalid rule templates.2.
Inference:Let ?Pi,jf,Pm,ne ,D1,A?
be a valid rule template, and ?Pp,qf,Ps,te ,D2,A?
avalid span template, where range [p, q] ?
[i, j], [s, t] ?
[m,n], D2 is asub-structure of D1, and at least one word in Pi,jfbut not in Pp,qfis aligned.We create a new valid rule template ?P?f ,P?e,D?,A?, where we obtain P?fby replacing Pp,qfwith label X in Pi,jf, and obtain P?e by replacing Ps,te withX in Pm,ne .
Furthermore, we obtain D?
by replacing sub-structure D2with X in D1.3 An example is shown in Figure 7.By applying the inference rule recursively, we can generate rules with arbitraryaligned NT slots if there are enough words and alignments.
In order to make the sizeof the grammar manageable, we keep only rules with at most two NT slots and at mostseven source elements.Following previous work (Och and Ney 2003; Chiang 2007), we have three featuresfor each rule, which are P(source|target), P(target|source), and the lexical translationprobability given by GIZA.
The two conditional probabilities are simply estimated bycounting in all the extracted rules.2 Pi,jfrepresents the ith to the jth words on the source side, and Pm,ne represents the mth to the nth wordson the target side.
By Pi,jfaligned to Pm,ne , we mean all words in Pi,jfare either aligned to words inPm,ne or unaligned, and vice versa.
Furthermore, at least one word in Pi,jfis aligned to a word in Pm,ne .3 If D2 is a floating structure, we need to merge several dependency links into one.660Shen, Xu, and Weischedel String-to-Dependency Statistical Machine Translation5.3 DecodingFollowing previous work on hierarchical MT (Chiang 2005; Galley et al 2006), we solvethe decoding problem with chart parsing.
We view the target dependency trees ashidden structures in the input.
The task of decoding is then to find the best hiddenstructure for the input given the transfer grammar and the language models (a stringn-gram LM and a dependency LM).The parser scans all source cells in a bottom?up style, and checks matched transferrules according to the source side.
Once there is a completed rule, we build a larger de-pendency structure by substituting component dependency structures for correspond-ing NTs in the target dependency structure of rules.Hypotheses, that is, candidate dependency structures, are organized in a sharedforest, or AND?OR structures.
An AND-structure represents an application of a ruleover component OR-structures, and an OR-structure represents a set of alternativeAND-structures with the same state.
A state keeps the necessary information abouthypotheses under it, which is needed for computing scores for higher level hypothesesfor dynamic programming.
For example, with an n-gram string LM in decoding, a statekeeps the leftmost n ?
1 words and the rightmost n ?
1 words shared by hypotheses inthat state.
Because of the use of a dependency LM in decoding, the state informationalso includes boundary information about dependency structures for the purpose ofcomputing dependency LM scores for larger structures.In the next section, we will explain how to extend categories and states to exploit adependency language model during decoding.5.4 Using Dependency LM ScoresFor the dependency tree in Figure 1, we calculate the probability of the tree as followsP = PT(find)?PL(will | find-as-head)?PL(boy | will, find-as-head)?PL(the | boy-as-head)?PR(it | find-as-head)?PR(interesting | it, find-as-head)Here PT(x) is the probability that word x is the root of a dependency tree.
PL and PRare left and right side generative probabilities respectively.
Let wh be the head, andwL1wL2 ...wLn be all the children on the left side, from the nearest to the farthest.
Weuse a tri-gram dependency LM,PL(wL1wL2 ...wLn |wh-as-head) = PL(wL1 |wh-as-head) ?
PL(wL2 |wL1 ,wh-as-head) ?
...?PL(wLn |wLn?1 ,wLn?2 ) ?
PL(STOP|wLn ,wLn?1 ) (6)In this formula, wh-as-head represents the event that w is used as the head, and wLirepresents the event that wLi is a sibling word.
The computation of STOP probabilitiesgreatly complicates the implementation of inside dependency LM probabilities, so weignored it in practice.
Right side probability PR is defined in a similar way.661Computational Linguistics Volume 36, Number 4We should note that other orders of dependency LMs (e.g., bi-gram or 4-gram) canbe used by changing the independence assumptions in the above formulas.
The choiceof using a tri-gram model in our experiments is a trade-off between model robustnessand sharpness given the training data available.In order to calculate the dependency language model score, or depLM score forshort, on the fly for partial hypotheses in a bottom?up decoding, we need to save moreinformation in categories and states.We use a 5-tuple ?LF,LN, h,RN,RF?
to represent the category of a dependencystructure.
h represents the head.
Relative to the head, LF is the farthest children onthe left side and RF the farthest children on the right side.
Similarly, LN is the nearestchildren on the left side and RN the nearest children on the right.
The three types ofcategories are as follows. fixed: ?LF,?, h,?,RF? floating left: ?LF,LN,?,?,?
? floating right: ?
?,?,?,RN,RF?Furthermore, operations similar to those described in Section 4.2 are used to keep trackof the head and boundary child nodes, which are then used to compute depLM scoresin decoding.5.5 Using Labels in Transfer RulesIn the formalism introduced in the previous section, there is only a single non-terminaltype X.
This may result in loss of information in the training data.
For example, there isa rule whose target dependency structure is X1 ?
says ?
X2, where X1 and X2 dependon says.
In the training data, X1 comes from a tree rooted on a noun and X2 comes from atree rooted on a verb.
Without this information in the rule, any structure could be placedin either of these two slots in the decoding phase.We alleviate this problem by associating a label with each non-terminal in the rules.Specifically, each non-terminal has a label, and the whole target structure side also hasa label.
When we replace an NT with a sub-structure, we check if the label of the sub-structure is the same as the NT label.
If they do not match, we assign a penalty to thisreplacement.An obvious choice of the label is the POS tag of the head word, if it is a fixed tree.
Inthe previous example, the target structure would generate X1(NN) ?
says ?
X2(VBP),where NN means noun (singular or mass) and VBP means verb (non-3rd person sin-gular present), and the whole target structure has a label of VBZ, which means verb(3rd person singular present).
If we replace NN with a sub-tree rooted at, for example,a preposition, there will be a penalty.In our system, we use the POS tag of the head word as the label of a fixed structure.We always use the generic label for floating structures.
Any NT substitution with thislabel involved is regarded as a mismatch.
In other words, there is a penalty for insertingany floating structure during decoding.This extension does not affect the basic formalism of dependency structures de-scribed in the previous section.
Instead, we modify the representation of translationrules and states in the decoder.
For each rule, if its dependency structure is of a fixedtype, the whole structure has a label which is the POS tag of the head word.
Otherwise,the label is X.
Similarly, each NT slot has a label which is defined in the same way,662Shen, Xu, and Weischedel String-to-Dependency Statistical Machine Translationbased on the dependency structure from which the rule is extracted.
In decoding,each state has an extra field representing the label for the dependency structure of thehypothesis.5.6 Other DetailsWe have nine features in our system.1.
Log probability of the source side given the target side of a rule2.
Log probability of the target side given the source side of a rule3.
Log probability of word alignment4.
Number of target words5.
Number of special rules (see Section 5.1) used6.
Log probability of string LM7.
Log probability of dependency LM8.
Discount on ill-formed dependency structures9.
Discount on unmatched labelsThe values of the first four features are accumulated on the rules used in a transla-tion.
The fifth feature counts the number of times the adjoining and concatenation rulesare used in a translation.
The string LM score and dependency LM score are the nexttwo features.In practice, we also allow hypotheses that do not have well-formed structures inderivation, but they are penalized.
For this purpose, we introduce the null dependencystructure e. For any operation OP and dependency structure X,Y, we haveOP(X,Y) = e if this operation is not defined in Definition 5OP(X, e) = XOP(e,X) = XBecause part of a hypothesis may have a null dependency structure, we cannot calculatedependency LM scores on some of the related words.
Therefore, we give a discount foreach of these words.
This is the eighth feature.There are two sources for null dependency structures.
One is the use of an unde-fined operation, for example, left-adjoining a right floating structure to a fixed structure.The other source is a lack of target structure information in translation rules.
The parserthat we used may fail to generate parse trees for short segments?for example, dictio-nary items.
In these cases, we extracted the so-called phrasal rules with null dependencystructures.
We limited phrasal rules to at most three lexical items for each side.The last feature counts the number of substitutions with unmatched labels.In decoding, partial hypotheses are mapped into states.
The states maintain suf-ficient statistics for feature calculation.
For example, each state should memorize theleftmost two words and rightmost two words for LM score calculation.
Similar exten-sions are required for dependency LM score and NT labels.
Therefore, we use beamsearch with cube pruning as in Chiang (2005) for speedup.
Like chart parsing, the663Computational Linguistics Volume 36, Number 4computational complexity of decoding time is O(n3 ?
B ?
|G|), where n is the lengthof the source sentence, B is beam width, and |G| is the maximal number of transfer rulesapplicable to a span with translation grammar G. This number agrees with the empiricalresults.We tune the weights with several rounds of decoding and optimization.
FollowingOch (2003), the k-best results are accumulated as the input to the optimizer.
Powell?smethod is used for optimization with 20 random starting points around the weightvector of the last iteration.
For improved results, we rescore 1,000-best translations,generated using the technique described by Huang and Chiang (2005), by replacingtri-gram string LM scores in the output with 5-gram string LM scores.
The algorithm totune the rescoring weights is similar to the one to tune the decoder weights.6.
ExperimentsWe experimented with four models: baseline: hierarchical string to string translation, using our own replicationof the Hiero system (Chiang 2007) filtered: like the baseline, it uses string to string rules, except that ruleswhose target side does not correspond to a well-formed structure in ruleextraction are excluded.
No dependency LM is used in decoding str-dep: string-to-dependency system.
It uses rules with target dependencystructures and a dependency LM in decoding labeled: an enhanced str-dep model with POS tags as labelsWe use the Hiero model as our baseline because it is the closest to our string-to-dependency model.
They use similar rule extraction and decoding algorithms.
Themajor difference is in the representation of target structures.
We use dependencystructures instead of strings; thus, the comparison will show the contribution of usingdependency information in decoding.All models were tuned on BLEU (Papineni, Roukos, and Ward 2001), and evaluatedon BLEU, TER (Snover et al 2006), and METEOR (Banerjee and Lavie 2005).
It is wellknown that all automatic scores are crude approximations of translation quality.
It is notuncommon for a technique to improve the metric that is used for tuning but hurt othermetrics.
The use of multiple metrics helps us avoid drawing false conclusions based onmetric-specific improvements.
For both Arabic-to-English and Chinese-to-English MT,we tuned on NIST MT02-05 and tested on MT06 and MT08 newswire sets.The training data for Arabic-to-English MT contains around 1.9 million pairs ofbi-lingual sentences from ten corpora: LDC2004T17, LDC2004T18, LDC2005E46, LDC-2006E25, LDC2006G05, LDC2005E85, LDC2006E36, LDC2006E82, LDC2006E95, andSSUSAC27 (Sakhr Arabic-English Parallel Corpus).
The training data for Chinese-to-English MT contains around 1.0 million pairs of bi-lingual sentences from eight corpora:LDC2002E18, LDC2005T06, LDC2005T10, LDC2006E26, LDC2006G05, LDC2002L27,LDC2005T34, and LDC2003E07.The dependency LMs were trained on the same parallel training data.
For that pur-pose, we parsed the English side of the parallel data.
Two separate models weretrained: one for Arabic from the Arabic training data and the other for Chinese fromthe Chinese training data.
Traditional tri-gram and 5-gram string LMs were trained on664Shen, Xu, and Weischedel String-to-Dependency Statistical Machine TranslationTable 1Number of transfer rules.Model Arabic-to-English Chinese-to-Englishbaseline 337,542,137 193,922,173filtered 32,057,337 39,005,696str-dep 35,801,341 41,013,346labeled 41,201,100 43,705,510the English side of the parallel data as well as the English Gigaword corpus V3.0 in away described by Bulyko et al (2007).Table 1 shows the number of transfer rules extracted from the training data forthe tuning and test sets.
The constraint of well-formed dependency structures greatlyreduced the size of the rule set.
Although the rule size increased a little bit after incorpo-rating dependency structures and labels in rules, the size of string-to-dependency ruleset is about 10% to 20% of the baseline.Tables 2 and 3 show the BLEU, TER, and METEOR scores on MT06 and MT08 forArabic-to-English MT.
Tables 4 and 5 show the scores for Chinese-to-English MT.For system comparison, we primarily rely on the lower-cased BLEU score of thedecoding output because it is the metric on which all systems were tuned.
We measuredthe significance of BLEU, TER, and METEOR with paired bootstrap resampling asproposed by Koehn (2004).
In Tables 2 through 5, (+/-) represent being better/worsethan the baseline at 95% confidence level, respectively, and (*) represents insignificantdifference from the baseline.For Arabic-to-English MT, the str-dep model decoder improved BLEU by 1.3 onMT06 and 1.2 on MT08 before 5-gram rescoring.
For Chinese-to-English MT, the im-provements in BLEU were 1.0 on MT06 and 1.4 on MT08.
After rescoring, the improve-ments became smaller, ranging from 0.8 to 1.3.
All the BLEU improvements on 5-gramscores are statistically significant.The use of POS labels in transfer rules further improves the BLEU score by about0.7 points on average.
The overall BLEU improvement on lower-cased decoding outputTable 2BLEU, TER, and METEOR percentage scores on MT06 Arabic-to-English newswire set.Model BLEU TER METEORlower mixed lower mixedDecoding (3-gram LM)baseline 47.50 45.48 44.79 46.97 66.17filtered 46.64 (-) 44.47 (-) 45.38 (*) 47.96 (-) 66.64 (*)str-dep 48.75 (+) 46.74 (+) 43.43 (+) 45.79 (+) 67.18 (+)labeled 49.33 (+) 47.07 (+) 43.09 (+) 45.53 (+) 67.04 (+)Rescoring (5-gram LM)baseline 50.38 48.33 42.64 44.87 67.25filtered 49.60 (-) 47.51 (-) 43.50 (-) 45.81 (-) 67.44 (*)str-dep 51.24 (+) 49.23 (+) 42.08 (*) 44.42 (*) 67.89 (+)labeled 51.80 (+) 49.69 (+) 41.54 (+) 43.76 (+) 67.97 (+)665Computational Linguistics Volume 36, Number 4Table 3BLEU, TER, and METEOR percentage scores on MT08 Arabic-to-English newswire set.Model BLEU TER METEORlower mixed lower mixedDecoding (3-gram LM)baseline 48.41 46.13 43.83 46.18 67.45filtered 47.37 (-) 45.24 (-) 44.39 (-) 46.83 (-) 67.17 (*)str-dep 49.58 (+) 47.46 (+) 42.80 (+) 45.08 (+) 68.08 (+)labeled 50.46 (+) 48.19 (+) 42.27 (+) 44.57 (+) 67.78 (+)Rescoring (5-gram LM)baseline 50.50 48.35 42.78 44.92 67.98filtered 49.56 (-) 47.49 (-) 43.20 (*) 45.44 (*) 67.79 (*)str-dep 51.23 (+) 49.11 (+) 42.01 (+) 44.15 (+) 68.65 (+)labeled 51.93 (+) 49.86 (+) 41.27 (+) 43.33 (+) 68.40 (+)is 1.8 points on MT06 and 2.1 points on MT08 for Arabic-to-English translation, and2.0 points on MT06 and 1.6 points on MT08 for Chinese-to-English translation.METEOR scores became significantly better for all conditions.
TER improved sig-nificantly for Arabic-to-English but marginally on Chinese-to-English tasks.
The resultson METEOR and TER suggested that the new model did improve translation accuracy.The filtered string-to-string rules can be viewed as the string projection of string-to-dependency rules.
It shows the performance of using dependency structure for rulefiltering only.
The results are very interesting.
On Arabic-to-English, the filtered modelwas significantly worse, which means that many useful rules were lost due to thestructural constraints.
On Chinese-to-English, the tri-gram scores of the filtered modelwere a little bit worse.
However, after 5-gram rescoring, the BLEU scores became higherthan the baseline, and METEOR scores were even significantly better.
We suspect thatthe different performance that we observed is due to the difference in source languagesand their tokenization methods.Table 4BLEU, TER, and METEOR percentage scores on MT06 Chinese-to-English newswire set.Model BLEU TER METEORlower mixed lower mixedDecoding (3-gram LM)baseline 36.40 34.79 54.98 56.53 57.25filtered 36.02 (*) 34.23 (*) 55.29 (*) 57.03 (*) 57.60 (+)str-dep 37.44 (+) 35.62 (+) 54.64 (*) 56.47 (*) 57.42 (+)labeled 38.37 (+) 36.53 (+) 54.14 (+) 55.99 (*) 58.42 (+)Rescoring (5-gram LM)baseline 37.88 36.18 53.80 55.45 57.44filtered 38.52 (*) 36.74 (*) 54.09 (*) 55.69 (*) 58.16 (+)str-dep 38.91 (+) 37.04 (+) 53.65 (*) 55.45 (*) 57.99 (+)labeled 39.11 (+) 37.30 (+) 53.61 (*) 55.29 (*) 58.69 (+)666Shen, Xu, and Weischedel String-to-Dependency Statistical Machine TranslationTable 5BLEU, TER, and METEOR percentage scores on MT08 Chinese-to-English newswire set.Model BLEU TER METEORlower mixed lower mixedDecoding (3-gram LM)baseline 31.64 29.56 57.35 59.37 54.93filtered 31.26 (*) 29.42 (*) 57.46 (*) 59.28 (*) 55.16(+)str-dep 33.05 (+) 31.26 (+) 56.79 (*) 58.69 (+) 55.18(+)labeled 33.25 (+) 31.34 (+) 56.60 (+) 58.49 (+) 56.01(+)Rescoring (5-gram LM)baseline 33.06 31.21 55.84 57.71 55.18filtered 33.25 (*) 31.22 (*) 56.53 (*) 58.39 (-) 56.08 (+)str-dep 34.34 (+) 32.32 (+) 55.60 (*) 57.60 (*) 55.91 (+)labeled 35.02 (+) 33.00 (+) 55.39 (*) 57.48 (*) 56.46 (+)In any case, the purpose of the filtered model is not to propose the use of structuralconstraints for rule filtering, although it greatly reduced the rule size and allowedthe use of more useful training data potentially.
The use of structural constraints iscompulsory for the introduction of dependency LMs and non-terminal labels, whichcompensated for the loss of rule filtering, and led to significant overall improvement.7.
Comparison to Related WorkFox (2002), Ding and Palmer (2005), and Quirk, Menezes, and Cherry (2005) showedthat, for the purpose of representing word relations, dependency structures are ad-vantageous over CFG structures because they do not require complete constituents.
Anumber of techniques have been proposed to improve rule coverage.
Marcu et al (2006)and Galley et al (2006) introduced artificial constituent nodes dominating the phrase ofinterest.
The binarization method used by Wang, Knight, and Marcu (2007) can covermany non-constituent rules also, but not all of them.
DeNeefe et al (2007) showed thatthe best results were obtained by combining these methods.Charniak, Knight, and Yamada (2003) described a two-step string-to-CFG-treetranslation model which employed a syntax-based language model to select the besttranslation from a target parse forest built in the first step.
A crucial difference from ourwork is that they only used the tree-based LM in rescoring, possibly due to the com-plexity of the syntax-based LM.
In contrast, our system uses a dependency LM directlyin decoding and as such can prune out unpromising hypotheses as soon as possible.The use of a dependency LM in MT is similar to the use of a structured LM inASR (Chelba and Jelinek 2000; Xu, Chelba, and Jelinek 2002), with the same motivationof exploiting long-distance relations.
A difference is that the dependency LM is usedbottom?up in our MT system, whereas the structured LM is used left-to-right in ASR.Another difference is that long-distance relations are more important in MT due toword re-orderings.The well-formed dependency structures defined here are similar to the data struc-tures in previous work on monolingual parsing (Eisner and Satta 1999; McDonald,Crammer, and Pereira 2005), which allowed floating structures as well defined statesin derivation, too.
However, as for monolingual parsing, one usually wants exactly667Computational Linguistics Volume 36, Number 4one derivation for each parse tree, so as to avoid spurious ambiguity of derivationsfor the same parse.
The derivation model proposed by Eisner and Satta (1999) satisfiedthis prerequisite, and had O(n3) complexity with a bi-lexical probability model, whichwas O(n4) in many other derivation models.
In our MT model, the motivation is toexploit various translation fragments learned from the training data, and the opera-tions in monolingual parsing were designed to avoid artificial ambiguity of derivation.Another difference is that we have fixed structures growing on both sides, whereasfixed structures in (Eisner and Satta 1999) can only grow in one direction.The formalism for well-formed structures and the operations over them wereinspired by the well-known approach of Combinatory Categorial Grammar (CCG)(Steedman 2000).
In fact, the names of left raising and right raising stem from theraising operation in CCG.The string-to-dependency formalism can be viewed as a special case of Synchro-nous Tree Adjoining Grammar (STAG) (Shieber and Schabes 1990).
Trees on the sourceside are weakened to strings, and multi-rooted structures are employed on the tar-get side.
The adjoining operation in our model is similar to attachment in LTAG-spinal (Shen, Champollion, and Joshi 2008) and sister adjunction in variants (Rambow,Shanker, and Weir 1995; Chiang 2000; Carreras, Collins, and Koo 2008) of TAG (Joshi andSchabes 1997).
Translation rules can be viewed as constraints on the tree operations.8.
Conclusions and Future WorkIn this article, we propose a novel string-to-dependency algorithm for statistical ma-chine translation.
It employs a target dependency language model to exploit long dis-tance word relations in decoding, which cannot be captured with a traditional n-gramlanguage model.Compared with a state-of-the-art hierarchical string-to-string system, our string-to-dependency system generates about 80% fewer rules.
The overall gain in BLEU scoreon lower-cased decoding output is about two points.Dependency structures provide a desirable platform for employing linguisticknowledge in MT.
We will extend our approach with deeper linguistic features such aspropositional structures (Palmer, Gildea, and Kingsbury 2005).
The fixed and floatingstructures proposed in this article can be extended to model predicates and arguments.AcknowledgmentsThis work4 was supported by DARPA/IPTO Contract No.
HR0011-06-C-0022under the GALE program.
The views,opinions, and/or findings contained inthis article/presentation are those of theauthor/presenter and should not beinterpreted as representing the official viewsor policies, either expressed or implied, ofthe Defense Advanced Research ProjectsAgency or the Department of Defense.We are grateful to our colleagues Roger Bock,Ivan Bulyko, Mike Kayser, Jeff Ma, John4 Distribution Statement ?A?
(Approved forPublic Release, Distribution Unlimited).Makhoul, Spyros Matsoukas, Antti-VeikkoRosti, Rich Schwartz, Bing Zhang, andRabih Zbib for their help in running theexperiments and constructive commentsto improve this article.
Mike Kayser helpedto proofread the manuscript.
We also thankZhifei Li and the anonymous reviewers fortheir suggestions to improve this article.ReferencesBanerjee, Satanjeev and Alon Lavie.
2005.Meteor: An automatic metric for MTevaluation with improved correlationwith human judgments.
In Proceedingsof the 43th Annual Meeting of the Associationfor Computational Linguistics (ACL),pages 101?104, Ann Arbor, MI.668Shen, Xu, and Weischedel String-to-Dependency Statistical Machine TranslationBulyko, Ivan, Spyros Matsoukas, RichardSchwartz, Long Nguyen, and JohnMakhoul.
2007.
Language modeladaptation in machine translation fromspeech.
In Proceedings of the 32nd IEEEInternational Conference on Acoustics,Speech, and Signal Processing (ICASSP),pages 117?120, Honolulu, HI.Carreras, Xavier and Michael Collins.
2009.Non-projective parsing for statisticalmachine translation.
In Proceedings ofthe 2009 Conference of Empirical Methodsin Natural Language Processing,pages 200?209, Singapore.Carreras, Xavier, Michael Collins, andTerry Koo.
2008.
TAG, dynamicprogramming, and the perceptron forefficient, feature-rich parsing.
InProceedings of the 12th Conference onComputational Natural Language Learning,pages 9?16, Manchester.Charniak, Eugene, Kevin Knight, and KnightYamada.
2003.
Syntax-based languagemodels for statistical machine translation.In Proceedings of MT Summit IX,pages 40?46, New Orleans, LA.Chelba, Ciprian and Frederick Jelinek.
2000.Structured language modeling.
ComputerSpeech and Language, 14(4):283?332.Chiang, David.
2000.
Statistical parsingwith an automatically extracted treeadjoining grammar.
In Proceedings ofthe 38th Annual Meeting of the Associationfor Computational Linguistics (ACL),pages 456?463, Hong Kong.Chiang, David.
2005.
A hierarchical phrase-based model for statistical machinetranslation.
In Proceedings of the 43thAnnual Meeting of the Association forComputational Linguistics (ACL),pages 263?270, Ann Arbor, MI.Chiang, David.
2007.
Hierarchicalphrase-based translation.
ComputationalLinguistics, 33(2):201?228.DeNeefe, Steve and Kevin Knight.
2009.Synchronous tree adjoining machinetranslation.
In Proceedings of the 2009Conference of Empirical Methods in NaturalLanguage Processing, pages 727?736,Singapore.DeNeefe, Steve, Kevin Knight, Wei Wang,and Daniel Marcu.
2007.
What cansyntax-based MT learn from phrase-basedMT?
In Proceedings of the 2007 Conference ofEmpirical Methods in Natural LanguageProcessing, pages 755?763, Prague.Ding, Yuan and Martha Palmer.
2005.Machine translation using probabilisticsynchronous dependency insertiongrammars.
In Proceedings of the 43th AnnualMeeting of the Association for ComputationalLinguistics (ACL), pages 541?548, AnnArbor, MI.Eisner, Jason.
2003.
Learning non-isomorphictree mappings for machine translation.In Proceedings of the 41st Annual Meeting ofthe Association for Computational Linguistics(ACL), pages 205?208, Sapporo.Eisner, Jason and Giorgio Satta.
1999.Efficient parsing for bilexical context-freegrammars and head automatongrammars.
In Proceedings of the 37thAnnual Meeting of the Association forComputational Linguistics (ACL),pages 457?464, College Park, MD.Fox, Heidi.
2002.
Phrasal cohesion andstatistical machine translation.In Proceedings of the 2002 Conferenceof Empirical Methods in NaturalLanguage Processing, pages 304?311,Philadelphia, PA.Galley, Michel, Jonathan Graehl, KevinKnight, Daniel Marcu, Steve DeNeefe,Wei Wang, and Ignacio Thayer.
2006.Scalable inference and training ofcontext-rich syntactic models.
InCOLING-ACL ?06: Proceedings of 44thAnnual Meeting of the Association forComputational Linguistics and 21stInternational Conference on ComputationalLinguistics, pages 961?968, Sydney.Galley, Michel, Mark Hopkins, Kevin Knight,and Daniel Marcu.
2004.
What?s in atranslation rule?
In Proceedings of the 2004Human Language Technology Conferenceof the North American Chapter of theAssociation for Computational Linguistics,pages 273?280, Boston, MA.Graehl, Jonathan and Kevin Knight.
2004.Training tree transducers.
In Proceedings ofthe 2004 Human Language TechnologyConference of the North American Chapter ofthe Association for Computational Linguistics,pages 105?112, Boston, MA.Hajic?, Jan, Martin C?mejrek, Jason Eisner,Gerald Penn, Owen Rambow, DragomirRadev, Yuan Ding, Terry Koo, and KristenParton.
2002.
Natural language generationin the context of machine translation.Final report of JHU Summer Workshopproject, Johns Hopkins University,Baltimore, MD.Huang, Liang and David Chiang.
2005.Better k-best parsing.
In Proceedings of the9th International Workshop on ParsingTechnologies, pages 53?64, Vancouver.Joshi, Aravind K., Leon S. Levy, and MasakoTakahashi.
1975.
Tree adjunct grammars.669Computational Linguistics Volume 36, Number 4Journal of Computer and System Sciences,10(1):136?163.Joshi, Aravind K. and Yves Schabes.
1997.Tree-adjoining grammars.
In G. Rozenbergand A. Salomaa, editors, Handbook of FormalLanguages, volume 3.
Springer-Verlag,Berlin, pages 69?124.Koehn, Philipp.
2004.
Statistical significancetests for machine translation evaluation.In Proceedings of the 2004 Conferenceof Empirical Methods in Natural LanguageProcessing, pages 388?395, Barcelona.Koehn, Philipp, Franz J. Och, and DanielMarcu.
2003.
Statistical phrase basedtranslation.
In Proceedings of the 2003 HumanLanguage Technology Conference of the NorthAmerican Chapter of the Association forComputational Linguistics, pages 48?54,Edmonton.Magerman, David.
1995.
Statistical decision-tree models for parsing.
In Proceedings ofthe 33rd Annual Meeting of the Associationfor Computational Linguistics,pages 276?283, Cambridge, MA.Marcu, Daniel, Wei Wang, AbdessamadEchihabi, and Kevin Knight.
2006.
SPMT:Statistical machine translation withsyntactified target language phrases.In Proceedings of the 2006 Conference ofEmpirical Methods in Natural LanguageProcessing, pages 44?52, Sydney.McDonald, Ryan, Koby Crammer, andFernando Pereira.
2005.
Online large-margin training of dependency parsers.In Proceedings of the 43th Annual Meeting ofthe Association for Computational Linguistics(ACL), pages 91?98, Ann Arbor, MI.Mi, Haitao, Liang Huang, and Qun Liu.
2008.Forest-based translation.
In Proceedings ofthe 46th Annual Meeting of the Associationfor Computational Linguistics (ACL),pages 192?199, Columbus, OH.Och, Franz J.
2003.
Minimum error ratetraining for statistical machine translation.In Proceedings of the 41st Annual Meeting ofthe Association for Computational Linguistics(ACL), pages 160?167, Sapporo.Och, Franz J. and Hermann Ney.
2003.A systematic comparison of variousstatistical alignment models.
ComputationalLinguistics, 29(1):19?52.Palmer, Martha, Daniel Gildea, andPaul Kingsbury.
2005.
The propositionbank: An annotated corpus of semanticroles.
Computational Linguistics,31(1):71?106.Papineni, Kishore, Salim Roukos, andTodd Ward.
2001.
BLEU: A method forautomatic evaluation of machinetranslation.
IBM Research Report No.RC22176, Armonk, NY.Quirk, Chris, Arul Menezes, and ColinCherry.
2005.
Dependency treelettranslation: Syntactically informed phrasalSMT.
In Proceedings of the 43th AnnualMeeting of the Association for ComputationalLinguistics (ACL), pages 271?279,Ann Arbor, MI.Rambow, Owen, Vijay K. Shanker, andDavid Weir.
1995.
D-tree grammars.In Proceedings of the 33rd Annual Meetingof the Association for ComputationalLinguistics, pages 151?158,Cambridge, MA.Shen, Libin, Lucas Champollion, andAravind K. Joshi.
2008.
LTAG-spinaland the Treebank: A new resource forincremental, dependency and semanticparsing.
Language Resources and Evaluation,42(1):1?19.Shen, Libin and Aravind K. Joshi.
2005.Incremental LTAG Parsing.
In Proceedingsof Human Language Technology Conferenceand Conference on Empirical Methods inNatural Language Processing, pages 811?818,Vancouver.Shen, Libin and Aravind K. Joshi.
2008.LTAG dependency parsing withbidirectional incremental construction.In Proceedings of the 2008 Conferenceof Empirical Methods in Natural LanguageProcessing, pages 495?504, Honolulu, HI.Shen, Libin, Jinxi Xu, Bing Zhang, SpyrosMatsoukas, and Ralph Weischedel.2009.
Effective use of linguistic andcontextual information for statisticalmachine translation.
In Proceedings of the2009 Conference of Empirical Methods inNatural Language Processing, pages 72?80,Singapore.Shieber, Stuart and Yves Schabes.
1990.Synchronous tree adjoining grammars.In Proceedings of COLING ?90: The 13thInternational Conference on ComputationalLinguistics, pages 253?258, Helsinki.Smith, David A. and Jason Eisner.
2006.Quasi-synchronous grammars:Alignment by soft projection of syntacticdependencies.
In Proceedings of the HLT-NAACL Workshop on Statistical MachineTranslation, pages 23?30, New York, NY.Snover, Matthew, Bonnie Dorr, RichardSchwartz, Linnea Micciulla, and JohnMakhoul.
2006.
A study of translationedit rate with targeted human annotation.In Proceedings of Association for MachineTranslation in the Americas, pages 223?231,Cambridge, MA.670Shen, Xu, and Weischedel String-to-Dependency Statistical Machine TranslationSteedman, Mark.
2000.
The Syntactic Process.The MIT Press, Cambridge, MA.Wang, Wei, Kevin Knight, and Daniel Marcu.2007.
Binarizing syntax trees to improvesyntax-based machine translation accuracy.In Proceedings of the 2007 Conferenceof Empirical Methods in Natural LanguageProcessing, pages 746?754, Prague.Xu, Peng, Ciprian Chelba, and FrederickJelinek.
2002.
A study on richer syntacticdependencies for structured languagemodeling.
In Proceedings of the 40th AnnualMeeting of the Association for ComputationalLinguistics (ACL), pages 191?198,Philadelphia, PA.Yamada, Kenji and Kevin Knight.
2001.
Asyntax-based statistical translation model.In Proceedings of the 39th Annual Meeting ofthe Association for Computational Linguistics(ACL), pages 523?530, Toulouse.671
