Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1073?1082,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsJoint Word Alignment and Bilingual Named Entity RecognitionUsing Dual DecompositionMengqiu WangStanford UniversityStanford, CA 94305mengqiu@cs.stanford.eduWanxiang CheHarbin Institute of TechnologyHarbin, China, 150001car@ir.hit.edu.cnChristopher D. ManningStanford UniversityStanford, CA 94305manning@cs.stanford.eduAbstractTranslated bi-texts contain complemen-tary language cues, and previous workon Named Entity Recognition (NER)has demonstrated improvements in perfor-mance over monolingual taggers by pro-moting agreement of tagging decisions be-tween the two languages.
However, mostprevious approaches to bilingual taggingassume word alignments are given as fixedinput, which can cause cascading errors.We observe that NER label informationcan be used to correct alignment mis-takes, and present a graphical model thatperforms bilingual NER tagging jointlywith word alignment, by combining twomonolingual tagging models with two uni-directional alignment models.
We intro-duce additional cross-lingual edge factorsthat encourage agreements between tag-ging and alignment decisions.
We designa dual decomposition inference algorithmto perform joint decoding over the com-bined alignment and NER output space.Experiments on the OntoNotes datasetdemonstrate that our method yields signif-icant improvements in both NER and wordalignment over state-of-the-art monolin-gual baselines.1 IntroductionWe study the problem of Named Entity Recogni-tion (NER) in a bilingual context, where the goalis to annotate parallel bi-texts with named entitytags.
This is a particularly important problem formachine translation (MT) since entities such asperson names, locations, organizations, etc.
carrymuch of the information expressed in the sourcesentence.
Recognizing them provides useful in-formation for phrase detection and word sense dis-ambiguation (e.g., ?melody?
as in a female namehas a different translation from the word ?melody?in a musical sense), and can be directly leveragedto improve translation quality (Babych and Hart-ley, 2003).
We can also automatically construct anamed entity translation lexicon by annotating andextracting entities from bi-texts, and use it to im-prove MT performance (Huang and Vogel, 2002;Al-Onaizan and Knight, 2002).
Previous worksuch as Burkett et al (2010b), Li et al (2012) andKim et al (2012) have also demonstrated that bi-texts annotated with NER tags can provide usefuladditional training sources for improving the per-formance of standalone monolingual taggers.Because human translation in general preservessemantic equivalence, bi-texts represent two per-spectives on the same semantic content (Burkett etal., 2010b).
As a result, we can find complemen-tary cues in the two languages that help to dis-ambiguate named entity mentions (Brown et al,1991).
For example, the English word ?Jordan?can be either a last name or a country.
Withoutsufficient context it can be difficult to distinguishthe two; however, in Chinese, these two senses aredisambiguated: ????
as a last name, and ???
?as a country name.In this work, we first develop a bilingual NERmodel (denoted as BI-NER) by embedding twomonolingual CRF-based NER models into a largerundirected graphical model, and introduce addi-tional edge factors based on word alignment (WA).Because the new bilingual model contains manycyclic cliques, exact inference is intractable.
Weemploy a dual decomposition (DD) inference al-gorithm (Bertsekas, 1999; Rush et al, 2010) forperforming approximate inference.
Unlike most1073f1 f2 f3 f4 f5 f6e1 e2 e3 e4 e5 e6Xinhua News Agency Beijing Feb 16B-ORG I-ORG I-ORG [O] B-LOC O O???
?
??
?
??
?
?B-ORG O B-GPE O O OFigure 1: Example of NER labels between two word-aligned bilingual parallel sentences.
The [O] tag isan example of a wrong tag assignment.
The dashed alignment link between e3 and f2 is an example ofalignment error.previous applications of the DD method in NLP,where the model typically factors over two com-ponents and agreement is to be sought between thetwo (Rush et al, 2010; Koo et al, 2010; DeNeroand Macherey, 2011; Chieu and Teow, 2012), ourmethod decomposes the larger graphical modelinto many overlapping components where eachalignment edge forms a separate factor.
We designclique potentials over the alignment-based edgesto encourage entity tag agreements.
Our methoddoes not require any manual annotation of wordalignments or named entities over the bilingualtraining data.The aforementioned BI-NER model assumesfixed alignment input given by an underlying wordaligner.
But the entity span and type predictionsgiven by the NER models contain complementaryinformation for correcting alignment errors.
Tocapture this source of information, we present anovel extension that combines the BI-NER modelwith two uni-directional HMM-based alignmentmodels, and perform joint decoding of NER andword alignments.
The new model (denoted asBI-NER-WA) factors over five components: oneNER model and one word alignment model foreach language, plus a joint NER-alignment modelwhich not only enforces NER label agreements butalso facilitates message passing among the otherfour components.
An extended DD decoding algo-rithm is again employed to perform approximateinference.We give a formal definition of the Bi-NERmodel in Section 2, and then move to present theBi-NER-WA model in Section 3.2 Bilingual NER by AgreementThe inputs to our models are parallel sentencepairs (see Figure 1 for an example in English andChinese).
We denote the sentences as e (for En-glish) and f (for Chinese).
We assume accessto two monolingual linear-chain CRF-based NERmodels that are already trained.
The English-sideCRF model assigns the following probability for atag sequence ye:PCRFe (ye|e) =?vi?Ve?(vi)?(vi,vj)?De?
(vi, vj)Ze(e)where Ve is the set of vertices in the CRF andDe is the set of edges.
?
(vi) and ?
(vi, vj) arethe node and edge clique potentials, and Ze(e)is the partition function for input sequence e un-der the English CRF model.
We let k(ye) be theun-normalized log-probability of tag sequence ye,defined as:k(ye) = log??
?vi?Ve?(vi)?(vi,vj)?De?
(vi, vj)?
?Similarly, we define model PCRFf and un-normalized log-probability l(yf) for Chinese.We also assume that a set of word alignments(A = {(i, j) : ei ?
fj}) is given by a wordaligner and remain fixed in our model.For clarity, we assume ye and yf are binary vari-ables in the description of our algorithms.
The ex-tension to the multi-class case is straight-forwardand does not affect the core algorithms.2.1 Hard AgreementWe define a BI-NER model which imposes hardagreement of entity labels over aligned word pairs.At inference time, we solve the following opti-1074mization problem:maxye,yflog (PCRFe (ye)) + log(PCRFf(yf))=maxye,yfk(ye) + l(yf)?
logZe(e)?
logZf (f)'maxye,yfk(ye) + l(yf)3 yei = yfj ?
(i, j) ?
AWe dropped the Ze(e) and Zf(f) terms becausethey remain constant at inference time.The Lagrangian relaxation of this term is:L(ye,yf,U)=k (ye) + l(yf)+?
(i,j)?Au(i, j)(yei ?
yfj)where u(i, j) are the Lagrangian multipliers.Instead of solving the Lagrangian directly, wecan form the dual of this problem and solve it us-ing dual decomposition (Rush et al, 2010):minU(maxye?
?k (ye) +?
(i,j)?Au(i, j)yei??+maxyf??l(yf)??
(i,j)?Au(i, j)yfj??
)Similar to previous work, we solve this DDproblem by iteratively updating the sub-gradientas depicted in Algorithm 1.
T is the maximumnumber of iterations before early stopping, and ?tis the learning rate at time t. We adopt a learningrate update rule from Koo et al (2010) where ?t isdefined as 1N , where N is the number of times weobserved a consecutive dual value increase fromiteration 1 to t.A thorough introduction to the theoretical foun-dations of dual decomposition algorithms is be-yond the scope of this paper; we encourage un-familiar readers to read Rush and Collins (2012)for a full tutorial.2.2 Soft AgreementThe previously discussed hard agreement modelrests on the core assumption that aligned wordsmust have identical entity tags.
In reality, however,this assumption does not always hold.
Firstly, as-suming words are correctly aligned, their entitytags may not agree due to inconsistency in anno-tation standards.
In Figure 1, for example, theAlgorithm 1 DD inference algorithm for hardagreement model.?
(i, j) ?
A : u(i, j) = 0for t?
1 to T doye?
?
argmax k (ye) + ?
(i,j)?Au(i, j)yeiyf?
?
argmax l(yf)?
?
(i,j)?Au(i, j)yfjif ?
(i, j) ?
A : ye?i = yf?j thenreturn (ye?,yf?
)end iffor all (i, j) ?
A dou(i, j)?
u(i, j) + ?t(yf?j ?
ye?i)end forend forreturn (ye?(T),yf?
(T))word ?Beijing?
can be either a Geo-Political En-tity (GPE) or a location.
The Chinese annotationstandard may enforce that ?Beijing?
should alwaysbe tagged as GPE when it is mentioned in isola-tion, while the English standard may require theannotator to judge based on word usage context.The assumption in the hard agreement model canalso be violated if there are word alignment errors.In order to model this uncertainty, we extendthe two previously independent CRF models into alarger undirected graphical model, by introducinga cross-lingual edge factor ?
(i, j) for every pair ofword positions (i, j) ?
A.
We associate a cliquepotential function h(i,j)(yei , yfj) for ?
(i, j):h(i,j)(yei , yfj)= pmi(yei , yfj)P?
(ei,fj)where pmi(yei , yfj) is the point-wise mutual in-formation (PMI) of the tag pair, and we raise itto the power of a posterior alignment probabilityP?
(ei, fj).
For a pair of NEs that are aligned withlow probability, we cannot be too sure about theassociation of the two NEs, therefore the modelshould not impose too much influence from thebilingual agreement model; instead, we will let themonolingual NE models make their decisions, andtrust that those are the best estimates we can comeup with when we do not have much confidence intheir bilingual association.
The use of the poste-rior alignment probability facilitates this purpose.Initially, each of the cross-lingual edge factorswill attempt to assign a pair of tags that has thehighest PMI score, but if the monolingual taggersdo not agree, a penalty will start accumulatingover this pair, until some other pair that agrees bet-ter with the monolingual models takes the top spot.1075Simultaneously, the monolingual models will alsobe encouraged to agree with the cross-lingual edgefactors.
This way, the various components effec-tively trade penalties indirectly through the cross-lingual edges, until a tag sequence that maximizesthe joint probability is achieved.Since we assume no bilingually annotated NERcorpus is available, in order to get an estimate ofthe PMI scores, we first tag a collection of unan-notated bilingual sentence pairs using the mono-lingual CRF taggers, and collect counts of alignedentity pairs from this auto-generated tagged data.Each of the ?
(i, j) edge factors (e.g., the edgebetween node f3 and e4 in Figure 1) overlaps witheach of the two CRF models over one vertex (e.g.,f3 on Chinese side and e4 on English side), andwe seek agreement with the Chinese CRF modelover tag assignment of fj , and similarly for ei onEnglish side.
In other words, no direct agreementbetween the two CRF models is enforced, but theyboth need to agree with the bilingual edge factors.The updated optimization problem becomes:maxye(k)yf(l)ye(h)yf(h)k(ye(k))+ l(yf (l))+?
(i,j)?Ah(i,j)(ye(h)i , yf(h)j)3 ?
(i, j) ?
A :(ye(k)i = ye(h)i)?
(yf (l)j = yf (h)j)where the notation ye(k)i denotes tag assignment toword ei by the English CRF and ye(h)i denotes as-signment to word ei by the bilingual factor; yf (l)jdenotes the tag assignment to word fj by the Chi-nese CRF and yf (h)j denotes assignment to wordfj by the bilingual factor.The updated DD algorithm is illustrated in Al-gorithm 2 (case 2).
We introduce two separatesets of dual constraints we and wf, which rangeover the set of vertices on their respective halfof the graph.
Decoding the edge factor modelh(i,j)(yei , yfj) simply involves finding the pair oftag assignments that gives the highest PMI score,subject to the dual constraints.The way DD algorithms work in decomposingundirected graphical models is analogous to othermessage passing algorithms such as loopy beliefpropagation, but DD gives a stronger optimalityguarantee upon convergence (Rush et al, 2010).3 Joint Alignment and NER DecodingIn this section we develop an extended model inwhich NER information can in turn be used toimprove alignment accuracy.
Although we haveseen more than a handful of recent papers that ap-ply the dual decomposition method for joint in-ference problems, all of the past work deals withcases where the various model components havethe same inference output space (e.g., dependencyparsing (Koo et al, 2010), POS tagging (Rush etal., 2012), etc.).
In our case the output space isthe much more complex joint alignment and NERtagging space.
We propose a novel dual decom-position variant for performing inference over thisjoint space.Most commonly used alignment models, suchas the IBM models and HMM-based aligner areunsupervised learners, and can only capture sim-ple distortion features and lexical translational fea-tures due to the high complexity of the structureprediction space.
On the other hand, the CRF-based NER models are trained on manually anno-tated data, and admit richer sequence and lexicalfeatures.
The entity label predictions made by theNER model can potentially be leveraged to correctalignment mistakes.
For example, in Figure 1, ifthe tagger knows that the word ?Agency?
is taggedI-ORG, and if it also knows that the first commain the Chinese sentence is not part of any entity,then we can infer it is very unlikely that there ex-ists an alignment link between ?Agency?
and thecomma.To capture this intuition, we extend the BI-NERmodel to jointly perform word alignment and NERdecoding, and call the resulting model BI-NER-WA.
As a first step, instead of taking the outputfrom an aligner as fixed input, we incorporate twouni-directional aligners into our model.
We namethe Chinese-to-English aligner model as m(Be)and the reverse directional model n(Bf ).
Be isa matrix that holds the output of the Chinese-to-English aligner.
Each be(i, j) binary variable inBe indicates whether fj is aligned to ei; similarlywe define output matrix Bf and bf (i, j) for Chi-nese.
In our experiments, we used two HMM-based alignment models.
But in principle we canadopt any alignment model as long as we can per-form efficient inference over it.We introduce a cross-lingual edge factor ?
(i, j)in the undirected graphical model for every pair ofword indices (i, j), which predicts a binary vari-1076Algorithm 2 DD inference algorithm for jointalignment and NER model.
A line marked with (2)means it applies to the BI-NER model; a line marked with(3) means it applies to the BI-NER-WA model.S ?
A (2)S ?
{(i, j) : ?i ?
|e|, ?j ?
|f |} (3)?i ?
|e| : wei = 0; ?j ?
|f | : wfj = 0 (2,3)?
(i, j) ?
S : de(i, j) = 0, df (i, j) = 0 (3)for t?
1 to T doye(k)?
?
argmax k(ye(k))+?i?|e|wei ye(k)i (2,3)yf(l)?
?
argmax l(yf(l))+?i?|f |wfj yf(l)j (2,3)Be?
?argmax m (Be) + ?
(i,j)de(i, j)be(i, j) (3)Bf?
?argmax n(Bf)+?
(i,j)df(i, j)bf(i, j) (3)for all (i, j) ?
S do(ye(h)?i yf(h)?j )?
?wei ye(h)i ?
wfj yf(h)j+ argmax h(i,j)(ye(q)i yf(q)j ) (2)(ye(q)?i yf(q)?j a(i, j)?)?
?wei ye(q)i ?
wfj yf(q)j+ argmax q(i,j)(ye(q)i yf(q)j a(i, j))?
de(i, j)a(i, j)?
df(i, j)a(i, j) (3)end forConv = (ye(k)=ye(q) ?
yf(l)=yf(q)) (2)Conv = (Be=A=Bf ?
ye(k)=ye(q)?
yf(l)=yf(q)) (3)if Conv = true , thenreturn(ye(k)?
,yf(l)?)(2)return(ye(k)?
,yf(l)?
,A)(3)elsefor all i ?
|e| dowei ?
wei + ?t(ye(q|h)?i ?
ye(k)?i)(2,3)end forfor all j ?
|f | dowfj ?
wfj + ?t(yf(q|h)?j ?
yf(l)?j)(2,3)end forfor all (i, j) ?
S dode(i, j)?
de(i, j) + ?t (ae?
(i, j)?
be?
(i, j)) (3)df(i, j)?
df(i, j) + ?t(af?
(i, j)?
bf?
(i, j)) (3)end forend ifend forreturn(ye(k)?
(T) ,yf(l)?(T))(2)return(ye(k)?
(T) ,yf(l)?
(T) ,A(T ))(3)able a(i, j) for an alignment link between ei andfj .
The edge factor also predicts the entity tags forei and fj .The new edge potential q is defined as:q(i,j)(yei , yfj , a(i, j))=log(P (a(i, j) = 1)) + S(yei , yfj |a(i, j))P (a(i,j)=1)S(yei , yfj |a(i, j))={pmi(yei , yfj), if a(i, j) = 10, elseP (a(i, j) = 1) is the alignment probability as-signed by the bilingual edge factor between nodeei and fj .
We initialize this value to P?
(ei, fj) =12(Pm(ei, fj) + Pn(ei, fj)), where Pm(ei, fj) andPn(ei, fj) are the posterior probabilities assignedby the HMM-aligners.The joint optimization problem is defined as:maxye(k)yf(l)ye(h)yf(h)BeBfAk(ye(k)) + l(yf (l))+m(Be) + n(Bf) +?
(i?|e|,j?|f |)q(i,j)(yehi , yf(h)j , a(i, j))3 ?
(i, j) :(be(i, j)=a(i, j))?
(bf (i, j)=a(i, j))?
if a(i, j) = 1 then(ye(k)i =ye(h)i)?
(yf (l)j =yf (h)j)We include two dual constraints de(i, j) anddf (i, j) over alignments for every bilingual edgefactor ?
(i, j), which are applied to the English andChinese sides of the alignment space, respectively.The DD algorithm used for this model is givenin Algorithm 2 (case 3).
One special note is thatafter each iteration when we consider updates tothe dual constraint for entity tags, we only checktag agreements for cross-lingual edge factors thathave an alignment assignment value of 1.
In otherwords, cross-lingual edges that are not aligned donot affect bilingual NER tagging.Similar to ?
(i, j), ?
(i, j) factors do not providethat much additional information other than someselectional preferences via PMI score.
But thereal power of these cross-language edge cliquesis that they act as a liaison between the NERand alignment models on each language side, andencourage these models to indirectly agree witheach other by having them all agree with the edgecliques.It is also worth noting that since we decodethe alignment models with Viterbi inference, ad-ditional constraints such as the neighborhood con-straint proposed by DeNero and Macherey (2011)can be easily integrated into our model.
Theneighborhood constraint enforces that if fj isaligned to ei, then fj can only be aligned to ei+1or ei?1 (with a small penalty), but not any otherword position.
We report results of adding neigh-borhood constraints to our model in Section 6.4 Experimental SetupWe evaluate on the large OntoNotes (v4.0) cor-pus (Hovy et al, 2006) which contains manually1077annotated NER tags for both Chinese and En-glish.
Document pairs are sentence aligned us-ing the Champollion Tool Kit (Ma, 2006).
Af-ter discarding sentences with no aligned counter-part, a total of 402 documents and 8,249 paral-lel sentence pairs were used for evaluation.
Wewill refer to this evaluation set as full-set.
We useodd-numbered documents as the dev set and even-numbered documents as the blind test set.
Wedid not perform parameter tuning on the dev setto optimize performance, instead we fix the ini-tial learning rate to 0.5 and maximum iterations to1,000 in all DD experiments.
We only use the devset for model development.The Stanford CRF-based NER tagger was usedas the monolingual component in our models(Finkel et al, 2005).
It also serves as a state-of-the-art monolingual baseline for both Englishand Chinese.
For English, we use the default tag-ger setting from Finkel et al (2005).
For Chi-nese, we use an improved set of features over thedefault tagger, which includes distributional sim-ilarity features trained on large amounts of non-overlapping data.1We train the two CRF models on all portionsof the OntoNotes corpus that are annotated withnamed entity tags, except the parallel-aligned por-tion which we reserve for development and testpurposes.
In total, there are about 660 train-ing documents (?16k sentences) for Chinese and1,400 documents (?39k sentences) for English.Out of the 18 named entity types that are an-notated in OntoNotes, which include person, lo-cation, date, money, and so on, we select the fourmost commonly seen named entity types for evalu-ation.
They are person, location, organization andGPE.
All entities of these four types are convertedto the standard BIO format, and background to-kens and all other entity types are marked withtag O.
When we consider label agreements overaligned word pairs in all bilingual agreement mod-els, we ignore the distinction between B- and I-tags.We report standard NER measures (entity pre-cision (P), recall (R) and F1 score) on the testset.
Statistical significance tests are done using thepaired bootstrap resampling method (Efron andTibshirani, 1993).For alignment experiments, we train two uni-1The exact feature set and the CRF implementationcan be found here: http://nlp.stanford.edu/software/CRF-NER.shtmldirectional HMM models as our baseline andmonolingual alignment models.
The parametersof the HMM were initialized by IBM Model 1 us-ing the agreement-based EM training algorithmsfrom Liang et al (2006).
Each model is trainedfor 2 iterations over a parallel corpus of 12 mil-lion English words and Chinese words, almosttwice as much data as used in previous work thatyields state-of-the-art unsupervised alignment re-sults (DeNero and Klein, 2008; Haghighi et al,2009; DeNero and Macherey, 2011).Word alignment evaluation is done over thesections of OntoNotes that have matching gold-standard word alignment annotations from GALEY1Q4 dataset.2 This subset contains 288 docu-ments and 3,391 sentence pairs.
We will referto this subset as wa-subset.
This evaluation setis over 20 times larger than the 150 sentencesset used in most past evaluations (DeNero andKlein, 2008; Haghighi et al, 2009; DeNero andMacherey, 2011).Alignments input to the BI-NER model areproduced by thresholding the averaged posteriorprobability at 0.5.
In joint NER and alignment ex-periments, instead of posterior thresholding, wetake the direct intersection of the Viterbi-bestalignment of the two directional models.
We re-port the standard P, R, F1 and Alignment ErrorRate (AER) measures for alignment experiments.An important past work to make comparisonswith is Burkett et al (2010b).
Their methodis similar to ours in that they also model bilin-gual agreement in conjunction with two CRF-based monolingual models.
But instead of usingjust the PMI scores of bilingual NE pairs, as inour work, they employed a feature-rich log-linearmodel to capture bilingual correlations.
Parame-ters in their log-linear model require training withbilingually annotated data, which is not readilyavailable.
To counter this problem, they proposedan ?up-training?
method which simulates a super-vised learning environment by pairing a weak clas-sifier with strong classifiers, and train the bilin-gual model to rank the output of the strong classi-fier highly among the N-best outputs of the weakclassifier.
In order to compare directly with theirmethod, we obtained the code behind Burkett etal.
(2010b) and reproduced their experimental set-ting for the OntoNotes data.
An extra set of 5,000unannotated parallel sentence pairs are used for2LDC Catalog No.
LDC2006E86.1078Chinese EnglishP R F1 P R F1Mono 76.89 61.64 68.42 81.98 74.59 78.11Burkett 77.52 65.84 71.20 82.28 76.64 79.36Bi-soft 79.14 71.55 75.15 82.58 77.96 80.20Table 1: NER results on bilingual parallel test set.Best numbers on each measure that are statisticallysignificantly better than the monolingual baselineand Burkett et al (2010b) are highlighted in bold.training the reranker, and the reranker model se-lection was performed on the development dataset.5 Bilingual NER ResultsThe main results on bilingual NER over the testportion of full-set are shown in Table 1.
Weinitially experimented with the hard agreementmodel, but it performs quite poorly for reasons wediscussed in Section 2.2.
The BI-NER model withsoft agreement constraints, however, significantlyoutperforms all baselines.
In particular, it achievesan absolute F1 improvement of 6.7% in Chineseand 2.1% in English over the CRF monolingualbaselines.A well-known issue with the DD method isthat when the model does not necessarily con-verge, then the procedure could be very sensi-tive to hyper-parameters such as initial step sizeand early termination criteria.
If a model onlygives good performance with well-tuned hyper-parameters, then we must have manually anno-tated data for tuning, which would significantlyreduce the applicability and portability of thismethod to other language pairs and tasks.
To eval-uate the parameter sensitivity of our model, werun the model from 50 to 3000 iterations beforeearly stopping, and with 6 different initial stepsizes from 0.01 to 1.
The results are shown in Fig-ure 2.
The soft agreement model does not seem tobe sensitive to initial step size and almost alwaysconverges to a superior solution than the baseline.6 Joint NER and Alignment ResultsWe present results for the BI-NER-WA modelin Table 2.
By jointly decoding NER with wordalignment, our model not only maintains signifi-cant improvements in NER performance, but alsoyields significant improvements to alignment per-formance.
Overall, joint decoding with NER aloneyields a 10.8% error reduction in AER over thebaseline HMM-aligners, and also gives improve-0 0.01 0.050.1 0.2 0.51 230001000800500300100507374757677787980initial step sizemax no.
of iterationsF1 scoreFigure 2: Performance variance of the soft agree-ment models on the Chinese dev dataset, as a func-tion of step size (x-axis) and maximum number ofiterations before early stopping (y-axis).ment over BI-NER in NER.
Adding additionalneighborhood constraints gives a further 6% er-ror reduction in AER, at the cost of a small lossin Chinese NER.
In terms of word alignment re-sults, we see great increases in F1 and recall, butprecision goes down significantly.
This is be-cause the joint decoding algorithm promotes an ef-fect of ?soft-union?, by encouraging the two uni-directional aligners to agree more often.
Addingthe neighborhood constraints further enhances thisunion effect.7 Error Analysis and DiscussionWe can examine the example in Figure 3 to gainan understanding of the model?s performance.
Inthis example, a snippet of a longer sentence pair isshown with NER and word alignment results.
Themonolingual Chinese tagger provides a strong cuethat word f6 is a person name because the unique4-character word pattern is commonly associatedwith foreign names in Chinese, and also the wordis immediately preceded by the word ?president?.The English monolingual tagger, however, con-fuses the aligned word e0 with a GPE.Our bilingual NER model is able to correct thiserror as expected.
Similarly, the bilingual modelcorrects the error over e11.
However, the modelalso propagates labeling errors from the Englishside over the entity ?Tibet Autonomous Region?
tothe Chinese side.
Nevertheless, the resulting Chi-nese tags are arguably more useful than the origi-nal tags assigned by the baseline model.In terms of word alignment, the HMM modelsfailed badly on this example because of the long1079NER-Chinese NER-English word alignmentP R F1 P R F1 P R F1 AERHMM-WA - - - - - - 90.43 40.95 56.38 43.62Mono-CRF 82.50 66.58 73.69 84.24 78.70 81.38 - - - -Bi-NER 84.87 75.30 79.80 84.47 81.45 82.93 - - - -Bi-NER-WA 84.42 76.34 80.18 84.25 82.20 83.21 77.45 50.43 61.09 38.91Bi-NER-WA+NC 84.25 75.09 79.41 84.28 82.17 83.21 76.67 54.44 63.67 36.33Table 2: Joint alignment and NER test results.
+NC means incorporating additional neighbor constraintsfrom DeNero and Macherey (2011) to the model.
Best number in each column is highlighted in bold.f0 f1 f2 f3 f4 f5 f6e0 e1 e2 e3 e4 e5 e6 e7 e8 e9 e10 e11Suolangdaji , president of Tibet Auto.
Region branch of Bank of ChinaB-PER O O O B-GPE I-GPE I-GPE O O B-ORG I-ORG I-ORGB-PER O O O [B-LOC] [I-LOC] [I-LOC] O O B-ORG I-ORG I-ORG[B-GPE] O O O [B-LOC] [I-LOC] [I-LOC] O O [O] [O] [B-GPE]??
??
??
???
??
??
???
?B-ORG I-ORG B-GPE O O O B-PERB-ORG I-ORG [B-LOC] [I-LOC] O O B-PERB-ORG I-ORG [O] O O O B-PERFigure 3: An example output of our BI-NER-WA model.
Dotted alignment links are the oracle, dashedlinks are alignments from HMM baseline, and solid links are outputs of our model.
Entity tags in thegold line (closest to nodes ei and fj) are the gold-standard tags; in the green line (second closest tonodes) are output from our model; and in the crimson line (furthest from nodes) are baseline output.distance swapping phenomena.
The two unidirec-tional HMMs also have strong disagreements overthe alignments, and the resulting baseline aligneroutput only recovers two links.
If we were to takethis alignment as fixed input, most likely we wouldnot be able to recover the error over e11, but thejoint decoding method successfully recovered 4more links, and indirectly resulted in the NER tag-ging improvement discussed above.8 Related WorkThe idea of employing bilingual resources to im-prove over monolingual systems has been ex-plored by much previous work.
For example,Huang et al (2009) improved parsing performanceusing a bilingual parallel corpus.
In the NERdomain, Li et al (2012) presented a cyclic CRFmodel very similar to our BI-NER model, andperformed approximate inference using loopy be-lief propagation.
The feature-rich CRF formula-tion of bilingual edge potentials in their model ismuch more powerful than our simple PMI-basedbilingual edge model.
Adding a richer bilingualedge model might well further improve our results,and this is a possible direction for further experi-mentation.
However, a big drawback of this ap-proach is that training such a feature-rich modelrequires manually annotated bilingual NER data,which can be prohibitively expensive to generate.How and where to obtain training signals with-out manual supervision is an interesting and openquestion.
One of the most interesting papers in thisregard is Burkett et al (2010b), which exploredan ?up-training?
mechanism by using the outputsfrom a strong monolingual model as ground-truth,and simulated a learning environment where abilingual model is trained to help a ?weakened?monolingual model to recover the results of thestrong model.
It is worth mentioning that sinceour method does not require additional trainingand can take pretty much any existing model as?black-box?
during decoding, the richer and moreaccurate bilingual model learned from Burkett etal.
(2010b) can be directly plugged into our model.A similar dual decomposition algorithm to ourswas proposed by Riedel and McCallum (2011)for biomedical event detection.
In their Model3, the trigger and argument extraction modelsare reminiscent of the two monolingual CRFs inour model; additional binding agreements are en-forced over every protein pair, similar to how weenforce agreement between every aligned word1080pair.
Martins et al (2011b) presented a new DDmethod that combines the power of DD with theaugmented Lagrangian method.
They showedthat their method can achieve faster convergencethan traditional sub-gradient methods in modelswith many overlapping components (Martins etal., 2011a).
This method is directly applicable toour work.Another promising direction for improvingNER performance is in enforcing global labelconsistency across documents, which is an ideathat has been greatly explored in the past (Sut-ton and McCallum, 2004; Bunescu and Mooney,2004; Finkel et al, 2005).
More recently, Rushet al (2012) and Chieu and Teow (2012) haveshown that combining local prediction modelswith global consistency models, and enforcingagreement via DD is very effective.
It is straight-forward to incorporate an additional global consis-tency model into our model for further improve-ments.Our joint alignment and NER decoding ap-proach is inspired by prior work on improvingalignment quality through encouraging agreementbetween bi-directional models (Liang et al, 2006;DeNero and Macherey, 2011).
Instead of enforc-ing agreement in the alignment space based onbest sequences found by Viterbi, we could optto encourage agreement between posterior prob-ability distributions, which is related to the pos-terior regularization work by Grac?a et al (2008).Cromie`res and Kurohashi (2009) proposed an ap-proach that takes phrasal bracketing constraintsfrom parsing outputs, and uses them to enforcephrasal alignments.
This idea is similar to our jointalignment and NER approach, but in our case thephrasal constraints are indirectly imposed by en-tity spans.
We also differ in the implementationdetails, where in their case belief propagation isused in both training and Viterbi inference.Burkett et al (2010a) presented a supervisedlearning method for performing joint parsing andword alignment using log-linear models over parsetrees and an ITG model over alignment.
Themodel demonstrates performance improvementsin both parsing and alignment, but shares the com-mon limitations of other supervised work in that itrequires manually annotated bilingual joint pars-ing and word alignment data.Chen et al (2010) also tackled the problem ofjoint alignment and NER.
Their method employs aset of heuristic rules to expand a candidate namedentity set generated by monolingual taggers, andthen rank those candidates using a bilingual namedentity dictionary.
Our approach differs in that weprovide a probabilistic formulation of the problemand do not require pre-existing NE dictionaries.9 ConclusionWe introduced a graphical model that combinestwo HMM word aligners and two CRF NER tag-gers into a joint model, and presented a dual de-composition inference method for performing ef-ficient decoding over this model.
Results fromNER and word alignment experiments suggest thatour method gives significant improvements in bothNER and word alignment.
Our techniques makeminimal assumptions about the underlying mono-lingual components, and can be adapted for manyother tasks such as parsing.AcknowledgmentsThe authors would like to thank Rob Voigt andthe three anonymous reviewers for their valuablecomments and suggestions.
We gratefully ac-knowledge the support of the National NaturalScience Foundation of China (NSFC) via grant61133012, the National ?863?
Project via grant2011AA01A207 and 2012AA011102, the Min-istry of Education Research of Social SciencesYouth funded projects via grant 12YJCZH304,and the support of the U.S. Defense AdvancedResearch Projects Agency (DARPA) Broad Op-erational Language Translation (BOLT) programthrough IBM.Any opinions, findings, and conclusion or rec-ommendations expressed in this material are thoseof the authors and do not necessarily reflect theview of DARPA, or the US government.ReferencesYaser Al-Onaizan and Kevin Knight.
2002.
Translat-ing named entities using monolingual and bilingualresources.
In Proceedings of ACL.Bogdan Babych and Anthony Hartley.
2003.
Im-proving machine translation quality with automaticnamed entity recognition.
In Proceedings of the7th International EAMT workshop on MT and otherLanguage Technology Tools, Improving MT throughother Language Technology Tools: Resources andTools for Building MT.1081Dimitri P. Bertsekas.
1999.
Nonlinear Programming.Athena Scientific, New York.Peter F. Brown, Stephen A. Della Pietra, VincentJ.
Della Pietra, and Robert L. Mercer.
1991.
Word-sense disambiguation using statistical methods.
InProceedings of ACL.Razvan Bunescu and Raymond J. Mooney.
2004.Collective information extraction with relationalMarkov networks.
In Proceedings of ACL.David Burkett, John Blitzer, and Dan Klein.
2010a.Joint parsing and alignment with weakly synchro-nized grammars.
In Proceedings of NAACL-HLT.David Burkett, Slav Petrov, John Blitzer, and DanKlein.
2010b.
Learning better monolingual mod-els with unannotated bilingual text.
In Proceedingsof CoNLL.Yufeng Chen, Chengqing Zong, and Keh-Yih Su.2010.
On jointly recognizing and aligning bilingualnamed entities.
In Proceedings of ACL.Hai Leong Chieu and Loo-Nin Teow.
2012.
Com-bining local and non-local information with dual de-composition for named entity recognition from text.In Proceedings of 15th International Conference onInformation Fusion (FUSION).Fabien Cromie`res and Sadao Kurohashi.
2009.
Analignment algorithm using belief propagation and astructure-based distortion model.
In Proceedings ofEACL/ IJCNLP.John DeNero and Dan Klein.
2008.
The complexity ofphrase alignment problems.
In Proceedings of ACL.John DeNero and Klaus Macherey.
2011.
Model-based aligner combination using dual decomposi-tion.
In Proceedings of ACL.Brad Efron and Robert Tibshirani.
1993.
An Introduc-tion to the Bootstrap.
Chapman & Hall, New York.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by Gibbssampling.
In Proceedings of ACL.Joao Grac?a, Kuzman Ganchev, and Ben Taskar.
2008.Expectation maximization and posterior constraints.In Proceedings of NIPS.Aria Haghighi, John Blitzer, John DeNero, and DanKlein.
2009.
Better word alignments with super-vised ITG models.
In Proceedings of ACL.Eduard Hovy, Mitchell Marcus, Martha Palmer,Lance Ramshaw, and Ralph Weischedel.
2006.OntoNotes: the 90% solution.
In Proceedings ofNAACL-HLT.Fei Huang and Stephan Vogel.
2002.
Improved namedentity translation and bilingual named entity extrac-tion.
In Proceedings of the 2002 International Con-ference on Multimodal Interfaces (ICMI).Liang Huang, Wenbin Jiang, and Qun Liu.
2009.Bilingually-constrained (monolingual) shift-reduceparsing.
In Proceedings of EMNLP.Sungchul Kim, Kristina Toutanova, and Hwanjo Yu.2012.
Multilingual named entity recognition usingparallel data and metadata from Wikipedia.
In Pro-ceedings of ACL.Terry Koo, Alexander M. Rush, Michael Collins,Tommi Jaakkola, and David Sontag.
2010.
Dualdecomposition for parsing with non-projective headautomata.
In Proceedings of EMNLP.Qi Li, Haibo Li, Heng Ji, Wen Wang, Jing Zheng, andFei Huang.
2012.
Joint bilingual name tagging forparallel corpora.
In Proceedings of CIKM.Percy Liang, Ben Taskar, and Dan Klein.
2006.
Align-ment by agreement.
In Proceedings of HLT-NAACL.Xiaoyi Ma.
2006.
Champollion: A robust parallel textsentence aligner.
In Proceedings of LREC.Andre?
F. T. Martins, Noah A. Smith, Pedro M. Q.Aguiar, and Ma?rio A. T. Figueiredo.
2011a.
Dualdecomposition with many overlapping components.In Proceedings of EMNLP.Andre F. T. Martins, Noah A. Smith, Eric P. Xing,Pedro M. Q. Aguiar, and Ma?rio A. T. Figueiredo.2011b.
Augmenting dual decomposition for map in-ference.
In Proceedings of the International Work-shop on Optimization for Machine Learning (OPT2010).Sebastian Riedel and Andrew McCallum.
2011.
Fastand robust joint models for biomedical event extrac-tion.
In Proceedings of EMNLP.Alexander M. Rush and Michael Collins.
2012.
A tu-torial on dual decomposition and Lagrangian relax-ation for inference in natural language processing.JAIR, 45:305?362.Alexander M. Rush, David Sontag, Michael Collins,and Tommi Jaakkola.
2010.
On dual decomposi-tion and linear programming relaxations for naturallanguage processing.
In Proceedings of EMNLP.Alexander M. Rush, Roi Reichert, Michael Collins, andAmir Globerson.
2012.
Improved parsing and POStagging using inter-sentence consistency constraints.In Proceedings of EMNLP.Charles Sutton and Andrew McCallum.
2004.
Col-lective segmentation and labeling of distant entitiesin information extraction.
In Proceedings of ICMLWorkshop on Statistical Relational Learning and Itsconnections to Other Fields.1082
