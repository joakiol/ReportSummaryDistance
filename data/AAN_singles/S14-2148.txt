Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 833?837,Dublin, Ireland, August 23-24, 2014.V3: Unsupervised Generation of Domain Aspect Terms forAspect Based Sentiment AnalysisAitor Garc?
?a-Pablos,Montse Cuadros, Se?an GainesVicomtech-IK4 research centreMikeletegi 57, San Sebastian, Spain{agarciap,mcuadros}@vicomtech.orgGerman RigauIXA GroupEuskal Herriko Unibertsitatea,San Sebastian, Spaingerman.rigau@ehu.esAbstractThis paper presents V3, an unsupervisedsystem for aspect-based Sentiment Analy-sis when evaluated on the SemEval 2014Task 4.
V3 focuses on generating a listof aspect terms for a new domain using acollection of raw texts from the domain.We also implement a very basic approachto classify the aspect terms into categoriesand assign polarities to them.1 IntroductionThe automatic analysis of opinions, within theframework of opinion mining or sentiment anal-ysis, has gained a huge importance during the lastdecade due to the amount of review web sites,blogs and social networks producing everyday amassive amount of new content (Pang and Lee,2008; Liu, 2012; Zhang and Liu, 2014).
This con-tent usually contains opinions about different enti-ties, products or services.
Trying to cope with thislarge amounts of textual data is unfeasible with-out the help of automatic Opinion Mining toolswhich try to detect, identify, classify, aggregateand summarize the opinions expressed about dif-ferent topics (Hu and Liu, 2004) (Popescu and Et-zioni, 2005) (Wu et al., 2009) (Zhang et al., 2010).In this framework, aspect based opinion miningsystems aim to detect the sentiment at ?aspect?level (i.e.
the precise feature being opinionated ina clause or sentence).In this paper we describe our system presentedin the SemEval 2014 task 41Aspect Based Senti-ment Analysis (Pontiki et al., 2014), which focuseson detecting opinionated aspect terms (e.g.
wineThis work is licensed under a Creative Commons Attribution4.0 International Licence.
Page numbers and proceedingsfooter are added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/1http://alt.qcri.org/semeval2014/task4/list and menu in restaurant domain, and hard diskand battery life in laptop domain), their categoriesand polarities in customer review sentences.The task provides two training datasets, one ofrestaurant reviews and other of laptop reviews.The restaurant review dataset consists of over3,000 English sentences from restaurant reviewsborrowed from (Ganu et al., 2009).
The laptopreview dataset consists of over 3,000 English sen-tences extracted from customer reviews.
The taskis divided in four different subtasks: subtask 1 as-pect term extraction, subtask 2 aspect term polar-ity detection, subtask 3 aspect category detection,subtask 4 aspect category polarity detection.
Oursystem mainly focused on subtask 1, but we havealso participated in the other subtasks.The paper is organized as follows: section 2presents our approach, section 3 details the im-provement methods used for the aspects term se-lection and section 4 focus on category and polar-ity tagging.
Finally section 5 presents the resultsobtained and section 6 draws the conclusions andfuture work.2 Our approachWe have adapted the double-propagation tech-nique described in (Qiu et al., 2009; Qiu et al.,2011).
This method consists of using a minimalseed list of aspect terms and opinion words andpropagate them through an unlabelled domain-related corpus using a set of propagation rules.The goal is to obtain an extended aspect term andopinion word lists.
(Qiu et al., 2009) define opin-ion words as words that convey some positive ornegative sentiment polarities.
They only extractnouns as aspect terms and adjectives as opinionwords, and we assume the same restriction.The propagation rules have the form of depen-dency relations and some part-of-speech restric-tions.
Some rules extract new aspect terms, andothers extract new opinion words.
Table 1 shows833the rules used in our approach, similar to those de-tailed in (Qiu et al., 2011) with some modifica-tions.
In this table, T stands for aspect term (i.e.a word already in the aspect terms set) and O foropinion word (i.e.
a word already in the opinionwords set).
W means any word.
The dependencytypes used are amod, dobj, subj and conj, whichstand for adjectival modifier, direct object, subjectand conjunction respectively.
Additional restric-tions on the Part-Of-Speech (POS) of the wordspresent in the rule are shown in the third columnof the table.
The last column indicates to whichset (aspect terms or opinion words) the new wordis added.To obtain the dependency trees and word lem-mas and POS tags, we use the Stanford NLP tools2(De Marneffe et al., 2006).
Our initial seed wordsare just the adjectives good and bad, which areadded to the initial opinion words set.
The ini-tial aspect terms set starts empty.
Each sentencein the dataset is analysed to obtain its dependencytree and the rules are checked sequentially.
If ruleis triggered, then the word indicated by that ruleis added to the corresponding set (aspect termsor opinion words, depending on the rule).
Thesenew words can be then used to trigger the propa-gation rules later.
After the last sentence the pro-cess starts from the beginning to check the ruleswith the newly added words.
The process stopswhen no more words have been added during afull dataset iteration.3 Selecting aspect term candidatesThe double-propagation process populates bothsets of domain aspect terms and domain opinionwords, but we focus our attention in the aspectterms set.
Due to the nature of the process it tendsto generate hundreds of different potential aspectterms, many of them being incorrect.
We applysome additional processes to improve the list.3.1 Ranking the aspect termsOne way to reduce the undesired terms is to rankthem, pushing the incorrect aspect terms to thebottom of the list and using only a certain subsetof top ranked terms.
In order to rank this list wehave modelled the double-propagation process asa undirected graph population process.
Each newaspect term or opinion word discovered by apply-2http://nlp.stanford.edu/software/lex-parser.shtmling a propagation rule is added as a vertex to thegraph.
The rule used to extract the new word isadded as an edge to the graph, connecting the orig-inal word and the newly discovered word.We have applied the well-known PageRank al-gorithm (Brin and Page, 1998) to score the verticesof the graph.
To calculate the PageRank scoreswe have used the JUNG framework3(OMadad-hain et al., 2005), a set of Java libraries to workwith graphs.
The value of the alpha parameter thatrepresents the probability of a random jump to anynode of the graph has been left at 0.15 (in the lit-erature it is recommended an alpha value between0.1 and 0.2).
The aspect terms are then ordered us-ing their associated score, being the most relevantaspect term, the one with the highest score.
Thenthe list can be trimmed to a certain amount of topranked terms, trying to balance the precision andrecall of the resulting list.3.2 Filtering undesired wordsThe double-propagation method always intro-duces many undesired words.
Some of these un-desired words appear very frequently and are com-bined with a large number of words.
So, they tendto also appear in high positions in the ranking.Many of these words are easy to identify, and theyare not likely to be useful aspect terms in any do-main.
Examples of these words are: nothing, ev-erything, thing, anyone, someone, somebody, etc.In this work we use a domain agnostic stop wordlist to deal with this kind of words.
The authorsof the original double-propagation approach usesome clause and frequency based heuristics thatwe do not employ here.3.3 Detecting multiword termsMany aspect terms are not just single words, butcompounds and multiword terms (e.g.
wine list,hard disk drive, battery life, etc.).
In the origi-nal double-propagation paper, the authors consideradjacent nouns to a given aspect term as multiwordterms and perform an a posteriori pruning basedon the frequency of the combination.
We havetried to add multiword terms without increasingthe amount of noise in the resulting list.
One of theapproaches included in the system exploits Word-Net4(Fellbaum, 1999), and the following simplerules:3http://jung.sourceforge.net4http://wordnet.princeton.edu/834Rule Observations Constraints ActionR11 O?
amod?W W is a noun W?TR12 O?dobj?W1?subj?W2 W2 is a noun W2?TR21 T?
amod?W W is an adjective W?OR22 T?
subj?W1?
dobj?W2 W2 is an adjective W2?
OR31 T?
conj?W W is a noun W?
TR32 T?
subj?W1?
dobj?W2 W2 is a noun W?
TR41 O?
conj?W W is an adjective W?
OR42 O?
Dep1?W1?
Dep2?W2 Dep1==Dep2, W2 is an adjective W2?
OTable 1: Propagation rules.?
If word N and word N+1 are nouns, and thecombination is an entry in WordNet (or inWikipedia, see below).
E.g.
: battery life?
If word N is an adjective and word N+1 isa noun, and the combination is an entry inWordNet.
E.g.
: hot dog, happy hour?
If word N is an adjective, word N+1 is a noun,and word n is a relational adjective in Word-Net (lexical file 01).
E.g.
: Thai foodIn order to improve the coverage of the Word-Net approach, we also check if a combination oftwo consecutive nouns appears as a Wikipedia ar-ticle title.
Wikipedia articles refer to real wordconcepts and entities, so if a combination of wordsis a title of a Wikipedia article it is very likelythat this word combination is also meaningful (e.g.DVD player, USB port, goat cheese, pepperonipizza).
We limit the lookup in Wikipedia titles justto combination of nouns to avoid the inclusion ofincorrect aspect terms.4 Assigning categories and polaritiesDespite we have focused our attention on acquir-ing aspect terms from a domain, we have also par-ticipated in the rest of subtasks: grouping aspectterms into a fixed set of categories, and assigningpolarities to both aspect terms and categories.To group the aspect terms into categories, wehave employed WordNet similarities.
The ideais to compare the detected aspect terms against aterm or group of terms representative of the tar-get categories.
In this case the categories (onlyfor restaurants) were food, service, price, ambi-ence and anecdotes/miscellaneous.Initially, the representative word for each cate-gory (except for the anecdotes/miscellaneous) wasthe name of the category itself.
We use the similar-ity measure described by (Wu and Palmer, 1994).Detected aspect terms are compared to the set ofrepresentative words on each category, and theyare assigned to the category with a higher similar-ity result.
For example using this approach, thesimilarity between food and cheese is 0.8, whilesimilarity between service and cheese is 0.25, andbetween price and cheese is 0.266.
Thus, in thiscase cheese is assigned to the category food.If the similarity does not surpass a given min-imum threshold (manually set to 0.7), the currentaspect term is not assigned to the category to avoidassigning a wrong category just because the otherwere even less similar.
After classifying the as-pect terms of a given sentence into categories, weassign those categories to the sentence.
If no cat-egory has been assigned, then we use the anec-dotes/miscellaneous category as the default one.This approach is quite naive and it has manylimitations.
It works quite well for the categoryfood, classifying ingredients and meals, but it failswhen the category or the aspect terms are morevague or abstract.
In addition, we do not performany kind of word sense disambiguation or sensepruning, which probably would discard unrelatedsenses.For detecting the polarity we have used theSentiWords (Guerini et al., 2013; Warriner et al.,2013) as a polarity lexicon.
Using direct depen-dency relations between aspect terms and polaritybearing words we assign the polarity value fromthe lexicon to the aspect term.
We make a simplecount of the polarities of the aspect terms classi-fied under a certain category to assign the polarityof that category in a particular sentence.5 EvaluationThe run submitted to the SemEval task 4 compe-tition was based on 25k unlabelled sentences ex-tracted from domain related reviews (for restau-rants and laptops) obtained by scraping differentwebsites.
We used these unlabelled sentences toexecute our unsupervised system to generate and835Restaur.
aspect terms Precision Recall F-scoreSemEval Baseline 0.525 0.427 0.471V3 (S) 0.656 0.562 0.605V3 (W) 0.571 0.641 0.604V3 (W+S) 0.575 0.645 0.608Table 2: Results on the restaurant review test set.Laptops aspect terms Precision Recall F-scoreSemEval Baseline 0.443 0.298 0.356V3 (S) 0.265 0.276 0.271V3 (W) 0.321 0.425 0.366V3 (W+S) 0.279 0.444 0.343Table 3: Results on the laptop review test set.rank the aspect term lists.
Then we used thoseaspect term lists to annotate the sentences usinga simple lemma matching approach between thewords.
The generated aspect term lists were lim-ited to the first ranked 550 items after some initialexperiments with the SemEval training sets.The SemEval test datasets (restaurants and lap-tops) contain about 800 sentences each.
Therestaurant dataset contains 1,134 labelled gold as-pect term spans, and the laptop dataset contains634 labelled gold aspect term spans.
We comparethe results against the SemEval baseline which iscalculated using the scripts provided by the Se-mEval organizers.
This baseline splits the datasetinto train and test subsets, and uses all the labelledaspect terms in the train subset to build a dictio-nary of aspect terms.
Then it simply uses that dic-tionary to label the test subset for evaluation.Tables 2 and 3 show the performance of our sys-tem with respect to the baselines in both datasets.
?V3 (S)?
stands for our system only using the Se-mEval test data (as our approach is unsupervisedit learns from the available texts for the task).
(W)refers to the results using our own dataset scrapedfrom the Web.
Finally (W+S) refers to the resultsusing both SemEval and our Web dataset mixedtogether.
The best results are highlighted in bold.For subtask 1, although our system outperformsthe baseline in terms of F-score in both datasets, inthe competition our system obtained quite modestresults ranking 24th and 26th out of 29 participantsRestaur.
categories Precision Recall F-scoreSemEval Baseline 0.671 0.602 0.638V3 0.638 0.569 0.602Table 4: Results on restaurant category detectionusing the test set.Polarity detection accuracy Baseline V3Restaur.
aspect terms 0.642 0.597Restaur.
categories 0.656 0.472Laptop aspect terms 0.510 0.538Table 5: Results for the polarity classification sub-tasks (subtasks 2 and 4).for restaurants and laptops respectively.One of the most important source of errors arethe multiword aspect term detection.
In the Se-mEval datasets, about the 25% of the gold aspectterms are multiword terms.
In both datasets wefind a large number of names of recipes and meals,composed by two, three or even more words,which cannot appear in our aspect term lists be-cause we limit the multiword length up to twowords.As mentioned in the introduction our approachfocuses mainly in the aspects so the approach fordetecting categories and polarities needs more at-tention.
Table 4 presents our results on categorydetection and table 5 our results on polarities.
Theresults are quite poor so we do not comment onthem here.
We will address these subtasks in fu-ture work.6 Conclusions and future workIn this paper we propose a simple and unsuper-vised system able to bootstrap and rank a listof domain aspect terms from a set of unlabelleddomain texts.
We use a double-propagation ap-proach, and we model the obtained terms and theirrelations as a graph.
Then, we apply the PageRankalgorithm to score the obtained terms.
Despite themodest results, our unsupervised system for de-tecting aspect terms performs better than the su-pervised baseline.
In our future work we will tryto improve the way we deal with multiword termsto reduce the amount of incorrect aspect terms andgenerate a better ranking.
We also plan to trydifferent methods for the category grouping, andexplore knowledge-based word sense disambigua-tion methods for improving the current system.AcknowledgementsThis work has been partially funded by SKaTer(TIN2012-38584-C06-02) and OpeNER (FP7-ICT-2011-SME- DCL-296451).836ReferencesSergey Brin and Lawrence Page.
1998.
The anatomyof a large-scale hypertextual web search engine.Computer networks and ISDN systems, 30(1):107?117.Marie-Catherine De Marneffe, Bill MacCartney,Christopher D Manning, et al.
2006.
Generat-ing typed dependency parses from phrase structureparses.
In Proceedings of LREC, volume 6, pages449?454.Christiane Fellbaum.
1999.
WordNet.
Wiley OnlineLibrary.Gayatree Ganu, N Elhadad, and A Marian.
2009.
Be-yond the Stars: Improving Rating Predictions usingReview Text Content.
WebDB, (WebDB):1?6.Marco Guerini, Lorenzo Gatti, and Marco Turchi.2013.
Sentiment analysis: How to derive priorpolarities from sentiwordnet.
arXiv preprintarXiv:1309.5843.Minqing Hu and Bing Liu.
2004.
Mining opinion fea-tures in customer reviews.
AAAI.Bing Liu.
2012.
Sentiment analysis and opinion min-ing.
Synthesis Lectures on Human Language Tech-nologies, 5(1):1?167.Joshua OMadadhain, Danyel Fisher, Padhraic Smyth,Scott White, and Yan-Biao Boey.
2005.
Analysisand visualization of network data using jung.
Jour-nal of Statistical Software, 10(2):1?35.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and trends in infor-mation retrieval, 2(1-2):1?135.Maria Pontiki, Dimitrios Galanis, John Pavlopou-los, Harris Papageorgiou, Ion Androutsopoulos, andSuresh Manandhar.
2014.
Semeval-2014 task 4:Aspect based sentiment analysis.
Proceedings ofthe International Workshop on Semantic Evaluation(SemEval).AM Popescu and Oren Etzioni.
2005.
Extracting prod-uct features and opinions from reviews.
Natural lan-guage processing and text mining, (October):339?346.Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.2009.
Expanding Domain Sentiment Lexiconthrough Double Propagation.
IJCAI.Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.2011.
Opinion word expansion and target extrac-tion through double propagation.
Computationallinguistics, (July 2010).Amy Beth Warriner, Victor Kuperman, and Marc Brys-baert.
2013.
Norms of valence, arousal, and dom-inance for 13,915 english lemmas.
Behavior re-search methods, 45(4):1191?1207.Zhibiao Wu and Martha Palmer.
1994.
Verbs seman-tics and lexical selection.
In Proceedings of the 32ndannual meeting on Association for ComputationalLinguistics, pages 133?138.
Association for Com-putational Linguistics.Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.2009.
Phrase dependency parsing for opinion min-ing.
In Proceedings of the 2009 Conference on Em-pirical Methods in Natural Language Processing:Volume 3-Volume 3, pages 1533?1541.
Associationfor Computational Linguistics.Lei Zhang and Bing Liu.
2014.
Aspect and EntityExtraction for Opinion Mining.
Data Mining andKnowledge Discovery for Big Data.L Zhang, Bing Liu, SH Lim, and E O?Brien-Strain.2010.
Extracting and ranking product features inopinion documents.
Proceedings of the 23rd Inter-national Conference on Computational Linguistics,(August):1462?1470.837
