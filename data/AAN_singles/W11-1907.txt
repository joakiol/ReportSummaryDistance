Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 56?60,Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational LinguisticsUnrestricted Coreference Resolution via Global Hypergraph PartitioningJie Cai and ?Eva Mu?jdricza-Maydt and Michael StrubeNatural Language Processing GroupHeidelberg Institute for Theoretical Studies gGmbHHeidelberg, Germany(jie.cai|eva.mujdriczamaydt|michael.strube)@h-its.orgAbstractWe present our end-to-end coreference res-olution system, COPA, which implements aglobal decision via hypergraph partitioning.In constrast to almost all previous approaches,we do not rely on separate classification andclustering steps, but perform coreference res-olution globally in one step.
COPA representseach document as a hypergraph and partitionsit with a spectral clustering algorithm.
Varioustypes of relational features can be easily incor-porated in this framwork.
COPA has partici-pated in the open setting of the CoNLL sharedtask on modeling unrestricted coreference.1 IntroductionCoreference resolution is the task of grouping men-tions of entities into sets so that all mentions inone set refer to the same entity.
Most recent ap-proaches to coreference resolution divide this taskinto two steps: (1) a classification step which de-termines whether a pair of mentions is coreferent orwhich outputs a confidence value, and (2) a cluster-ing step which groups mentions into entities basedon the output of step 1.In this paper we present an end-to-end corefer-ence resolution system, COPA, which avoids the di-vision into two steps and instead performs a globaldecision in one step.
The system presents a doc-ument as a hypergraph, where the vertices denotementions and the edges denote relational featuresbetween mentions.
Coreference resolution is thenperformed globally in one step by partitioning thehypergraph into subhypergraphs so that all mentionsin one subhypergraph refer to the same entity (Caiand Strube, 2010).
COPA assigns edge weights byapplying simple descriptive statistics on the tranin-ing data.
Since COPA does not need to learn anexplicit model, we used only 30% of the CoNLLshared task training data.
We did this not for effi-ciency reasons, only for convenience.While COPA has been developed originally toperform coreference resolution on MUC and ACEdata (Cai and Strube, 2010), the move to theOntoNotes data (Weischedel et al, 2011) requiredmainly to update the mention detector and the fea-ture set.
Since several off-the-shelf preprocessingcomponents are used, COPA participated in the opensetting of the CoNLL shared task on modeling unre-stricted coreference (Pradhan et al, 2011).
We didnot make extensive use of information beyond infor-mation from the closed class setting.2 PreprocessingCOPA is implemented on top of the BART-toolkit(Versley et al, 2008).
Documents are transformedinto the MMAX2-format (Mu?ller and Strube, 2006)which allows for easy visualization and (linguis-tic) debugging.
Each document is stored in severalXML-files representing different layers of annota-tions.
These annotations are created by a pipelineof preprocessing components.
We use the Stan-ford MaxentTagger (Toutanova et al, 2003) for part-of-speech tagging, and the Stanford Named En-tity Recognizer (Finkel et al, 2005) for annotat-ing named entities.
In order to derive syntacticinformation, we use the Charniak/Johnson rerank-ing parser (Charniak and Johnson, 2005) com-56bined with a constituent-to-dependency conversionTool (http://nlp.cs.lth.se/software/treebank_converter).
The preprocessingmodels are not trained on CoNLL data, so we onlyparticipated in the open task.We have implemented an in-house mention detec-tor, which makes use of the parsing output, the part-of-speech tags, as well as the chunks from the Yam-cha Chunker (Kudoh and Matsumoto, 2000).
Forthe OntoNotes data, the mention detector annotatesthe biggest noun phrase spans.3 COPA: Coreference PartitionerThe COPA system consists of modules which derivehyperedges from features and assign edge weightsindicating a positive correlation with the coreferencerelation, and resolution modules which create a hy-pergraph representation for the testing data and per-form partitioning to produce subhypergraphs, eachof which represents an entity.3.1 HyperEdgeCreatorCOPA needs training data only for computing thehyperedge weights.
Hyperedges represent features.Each hyperedge corresponds to a feature instancemodeling a simple relation between two or morementions.
This leads to initially overlapping sets ofmentions.
Hyperedges are assigned weights whichare calculated on the training data as the percentageof the initial edges being in fact coreferent.
Due tothe simple strategy of assigning edge weights, onlya reasonable size of training data is needed.3.2 Coreference Resolution ModulesUnlike pairwise models, COPA processes a docu-ment globally in one step, taking care of the pref-erence information among all the mentions simul-taneously and clustering them into sets directly.
Adocument is represented as a single hypergraph withmultiple edges.
The hypergraph resolver partitionsthe hypergraph into several sub-hypergraphs, eachcorresponding to one set of coreferent mentions.3.2.1 HGModelBuilderA single document is represented in a hypergraphwith basic relational features.
Each hyperedge in agraph corresponds to an instance of one of those fea-tures with the weight assigned by the HyperEdge-Learner.
Instead of connecting nodes with the tar-get relation as usually done in graph models, COPAbuilds the graph directly out of low dimensional fea-tures without assuming a distance metric.3.2.2 HGResolverIn order to partition the hypergraph we adopt aspectral clustering algorithm (Agarwal et al, 2005).All experimental results are obtained using symmet-ric Laplacians (Lsym) (von Luxburg, 2007).We apply the recursive variant of spectral clus-tering, recursive 2-way partitioning (R2 partitioner)(Cai and Strube, 2010).
This method does not needany information about the number of target sets (thenumber k of clusters).
Instead a stopping criterion??
has to be provided which is adjusted on develop-ment data.3.3 Complexity of HGResolverSince edge weights are assigned using simple de-scriptive statistics, the time HGResolver needs forbuilding the graph Laplacian matrix is not substan-tial.
For eigensolving, we use an open source libraryprovided by the Colt project1which implements aHouseholder-QL algorithm to solve the eigenvaluedecomposition.
When applied to the symmetricgraph Laplacian, the complexity of the eigensolv-ing is given by O(n3), where n is the number ofmentions in a hypergraph.
Since there are only afew hundred mentions per document in our data, thiscomplexity is not an issue.
Spectral clustering getsproblematic when applied to millions of data points.4 FeaturesIn our system, features are represented as types ofhyperedges.
Any realized edge is an instance of thecorresponding edge type.
All instances derived fromthe same type have the same weight, but they mayget reweighed by the distance feature (see Cai andStrube (2010)).
We use three types of features:negative: prevent edges between mentions;positive: generate strong edges between mentions;weak: add edges to an existing graph without intro-ducing new vertices;1http://acs.lbl.gov/?hoschek/colt/57In the following subsections we describe the fea-tures used in our experiments.
Some of the fea-tures described in Cai and Strube (2010) had to bechanged to cope with the OntoNotes data.
We alsointroduced a few more features (in particular in or-der to deal with the dialogue section in the data).4.1 Negative FeaturesNegative features describe pairwise relations whichare most likely not coreferent.
While we imple-mented this information as weak positive features inCai and Strube (2010), here we apply these featuresbefore graph construction as global variables.When two mentions are connected by a negativerelation, no edges will be built between them in thegraph.
For instance, no edges are allowed betweenthe mention Hillary Clinton and the mention he dueto incompatible gender.
(1) N Gender, (2) N Number: Two mentions donot agree in gender or number.
(3) N SemanticClass: Two mentions do notagree in semantic class (only the Object, Date andPerson top categories derived from WordNet (Fell-baum, 1998) are used).
(4) N Mod: Two mentions have the same syntac-tic heads, and the anaphor has a pre-modifier whichdoes not occur in the antecedent and does not con-tradict the antecedent.
(5) N DSPrn: Two first person pronouns in directspeeches assigned to different speakers.
(6) N ContraSubjObj: Two mentions are in thesubject and object positions of the same verb, andthe anaphor is a non-possesive pronoun.4.2 Positive FeaturesThe majority of well studied coreference features(e.g.
Stoyanov et al (2009)) are actually positivecoreference indicators.
In our system, the mentionswhich participate in positive relations are includedin the graph representation.
(7) StrMatch Npron & (8) StrMatch Pron: Af-ter discarding stop words, if the strings of mentionscompletely match and are not pronouns, they are putinto edges of the StrMatch Npron type.
When thematched mentions are pronouns, they are put intothe StrMatch Pron type edges.
(9) Alias: After discarding stop words, if men-tions are aliases of each other (i.e.
proper names withpartial match, full names and acronyms, etc.).
(10) HeadMatch: If the syntactic heads of men-tions match.
(11) Nprn Prn: If the antecedent is not a pro-noun and the anaphor is a pronoun.
This feature isrestricted to a sentence distance of 2.
Though it isnot highly weighted, it is crucial for integrating pro-nouns into the graph.
(12) Speaker12Prn: If the speaker of the secondperson pronoun is talking to the speaker of the firstperson pronoun.
The mentions contain only first orsecond person pronouns.
(13) DSPrn: If one of the mentions is the subjectof a speak verb, and other mentions are first personpronouns within the corresponding direct speech.
(14) ReflexivePrn: If the anaphor is a reflexivepronoun, and the antecedent is subject of the sen-tence.
(15) PossPrn: If the anaphor is a possesive pro-noun, and the antecedent is the subject of the sen-tence or the subclause.
(16) GPEIsA: If the antecedent is a Named Entityof GPE entity type (i.e.
one of the ACE entity type(NIST, 2004)), and the anaphor is a definite expres-sion of the same type.
(17) OrgIsA: If the antecedent is a Named En-tity of Organization entity type, and the anaphor is adefinite expression of the same type.4.3 Weak FeaturesWeak features are weak coreference indicators.
Us-ing them as positive features would introduce toomuch noise to the graph (i.e.
a graph with too manysingletons).
We apply weak features only to men-tions already integrated in the graph, so that weakinformation provides it with a richer structure.
(18) W Speak: If mentions occur with a wordmeaning to say in a window size of two words.
(19) W Subject: If mentions are subjects.
(20) W Synonym: If mentions are synonymousas indicated by WordNet.5 ResultsWe submitted COPA?s results to the open settingin the CoNLL shared task on modeling unrestrictedcoreference.
We used only 30% of the training data58(randomly selected) and the 20 features described inSection 4.The stopping criterion ??
(see Section 3) is tunedon development data to optimize the final corefer-ence scores.
A value of 0.06 is chosen for testing.COPA?s results on development set (which con-sists of 202 files) and on testing set are displayed inTable 1 and Table 2 respectively.
The Overall num-bers in both tables are the average scores of MUC,BCUBED and CEAF (E).Metric R P F1MUC 52.69 57.94 55.19BCUBED 64.26 73.39 68.52CEAF (M) 54.44 54.44 54.44CEAF (E) 45.73 40.92 43.19BLANC 69.78 75.26 72.13Overall 55.63Table 1: COPA?s results on CoNLL development setMetric R P F1MUC 56.73 58.90 57.80BCUBED 64.60 71.03 67.66CEAF (M) 53.37 53.37 53.37CEAF (E) 42.71 40.68 41.67BLANC 69.77 73.96 71.62Overall 55.71Table 2: COPA?s results on CoNLL testing set6 Mention Detection ErrorsAs described in Section 2, our mention detection isbased on automatically extracted information, suchas syntactic parses and basic noun phrase chunks.Since there is no minimum span information pro-vided in the OntoNotes data (in constrast to the pre-vious standard corpus, ACE), exact mention bound-ary detection is required.
A lot of the spuriousmentions in our system are generated due to mis-matches of ending or starting punctuations, and theOntoNotes annotation is also not consistent in thisregard.
Our current mention detector does not ex-tract verb phrases.
Therefore it misses all the Eventmentions in the OntoNotes corpus.We are planning to include idiomatic expressionidentification into our mention detector, which willhelp to avoid detecting a lot of spurious mentions,such as God in the phrase for God?s sake.7 COPA ErrorsBesides the fact that the current COPA is not resolv-ing any event coreferences, our in-house mention de-tector performs weakly in extracting date mentionstoo.
As a result, the system outputs several spuri-ous coreference sets, for instance a set containingthe September from the mention 15th September.A large amount of the recall loss in our system isdue to the lack of the world knowledge.
For exam-ple, COPA does not resolve the mention the Europestation correctly into the entity Radio Free Europe,for it has no knowledge that the entity is a station.Some more difficult coreference phenomena inOntoNotes data might require a reasoning mecha-nism.
To be able to connect the mention the vic-tim with the mention the groom?s brother, the eventof the brother being killed needs to be intepreted bythe system.We also observed from the experiments that theresolution of the it mentions are quite inaccurate.Although our mention detector takes care of dis-carding pleonastic it?s, there are still a lot of themleft which introduce wrong coreference sets.
Sincethe it?s do not contain enough information by them-selves, more features exploring their local syntax arenecessary.8 ConclusionsIn this paper we described a coreference resolutionsystem, COPA, which implements a global decisionin one step via hypergraph partitioning.
COPA?shypergraph-based strategy is a general preferencemodel, where the preference for one mention de-pends on information on all other mentions.The system implements three types of relationalfeatures ?
negative, positive and weak features, andassigns the edge weights according to the statiticsfrom the training data.
Since the weights are robustwith respect to the amount of training data we usedonly 30% of the training data.Acknowledgements.
This work has been fundedby the Klaus Tschira Foundation, Heidelberg, Ger-many.
The first author has been supported by a HITSPhD.
scholarship.59ReferencesSameer Agarwal, Jonwoo Lim, Lihi Zelnik-Manor, PietroPerona, David Kriegman, and Serge Belongie.
2005.Beyond pairwise clustering.
In Proceedings of theIEEE Computer Society Conference on Computer Vi-sion and Pattern Recognition (CVPR?05), volume 2,pages 838?845.Jie Cai and Michael Strube.
2010.
End-to-end coref-erence resolution via hypergraph partitioning.
InProceedings of the 23rd International Conference onComputational Linguistics, Beijing, China, 23?27 Au-gust 2010, pages 143?151.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and MaxEent discriminativereranking.
In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguistics,Ann Arbor, Mich., 25?30 June 2005, pages 173?180.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press, Cambridge,Mass.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by Gibbs sam-pling.
In Proceedings of the 43rd Annual Meetingof the Association for Computational Linguistics, AnnArbor, Mich., 25?30 June 2005, pages 363?370.Taku Kudoh and Yuji Matsumoto.
2000.
Use of SupportVector Machines for chunk identification.
In Proceed-ings of the 4th Conference on Computational NaturalLanguage Learning, Lisbon, Portugal, 13?14 Septem-ber 2000, pages 142?144.Christoph Mu?ller and Michael Strube.
2006.
Multi-levelannotation of linguistic data with MMAX2.
In SabineBraun, Kurt Kohn, and Joybrato Mukherjee, editors,Corpus Technology and Language Pedagogy: New Re-sources, New Tools, New Methods, pages 197?214.
Pe-ter Lang: Frankfurt a.M., Germany.NIST.
2004.
The ACE evaluation plan:Evaluation of the recognition of ACE en-tities, ACE relations and ACE events.http://www.itl.nist.gov/iad/mig//tests/ace/2004/doc/ace04-evalplan-v7.pdf.Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,Martha Palmer, Ralph Weischedel, and Nianwen Xue.2011.
CoNLL-2011 Shared Task: Modeling unre-stricted coreference in OntoNotes.
In Proceedings ofthe Shared Task of 15th Conference on ComputationalNatural Language Learning, Portland, Oreg., 23?24June 2011.Veselin Stoyanov, Nathan Gilbert, Claire Cardie, andEllen Riloff.
2009.
Conundrums in noun phrase coref-erence resolution: Making sense of the state-of-the-art.
In Proceedings of the Joint Conference of the 47thAnnual Meeting of the Association for ComputationalLinguistics and the 4th International Joint Conferenceon Natural Language Processing, Singapore, 2?7 Au-gust 2009, pages 656?664.Kristina Toutanova, Dan Klein, Christopher D. Manning,and Yoram Singer.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In Pro-ceedings of the Human Language Technology Confer-ence of the North American Chapter of the Associ-ation for Computational Linguistics, Edmonton, Al-berta, Canada, 27 May ?1 June 2003, pages 252?259.Yannick Versley, Simone Paolo Ponzetto, Massimo Poe-sio, Vladimir Eidelman, Alan Jern, Jason Smith,Xiaofeng Yang, and Alessandro Moschitti.
2008.BART: A modular toolkit for coreference resolution.In Companion Volume to the Proceedings of the 46thAnnual Meeting of the Association for ComputationalLinguistics, Columbus, Ohio, 15?20 June 2008, pages9?12.Ulrike von Luxburg.
2007.
A tutorial on spectral cluster-ing.
Statistics and Computing, 17(4):395?416.Ralph Weischedel, Martha Palmer, Mitchell Marcus, Ed-uard Hovy, Sameer Pradhan, Lance Ramshaw, Ni-anwen Xue, Ann Taylor, Jeff Kaufman, MichelleFranchini, Mohammed El-Bachouti, Robert Belvin,and Ann Houston.
2011.
OntoNotes release 4.0.LDC2011T03, Philadelphia, Penn.
: Linguistic DataConsortium.60
