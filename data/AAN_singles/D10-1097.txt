Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 993?1001,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsEnhancing Mention Detectionusing Projection via Aligned CorporaYassine BenajibaCenter for Computational Learning SystemsColumbia University, NYybenajiba@ccls.columbia.eduImed ZitouniIBM T.J. Watson Research CenterYorktown Heights, NYizitouni@us.ibm.comAbstractThe research question treated in this paperis centered on the idea of exploiting rich re-sources of one language to enhance the per-formance of a mention detection system of an-other one.
We successfully achieve this goalby projecting information from one languageto another via a parallel corpus.
We exam-ine the potential improvement using variousdegrees of linguistic information in a statisti-cal framework and we show that the proposedtechnique is effective even when the targetlanguage model has access to a significantlyrich feature set.
Experimental results showup to 2.4F improvement in performance whenthe system has access to information obtainedby projecting mentions from a resource-rich-language mention detection system via a par-allel corpus.1 IntroductionThe task of identifying and classifying entity textualreferences in open-domain texts, i.e.
the MentionDetection (MD) task, has become one of the mostimportant subtasks of Information Extraction (IE).It might intervene both as one step to structure nat-ural language texts or as a text enrichment prepro-cessing step to help other Natural Language Process-ing (NLP) applications reach higher accuracy.
Simi-larly to the Automatic Content Extraction (ACE) 1nomenclature, we consider that a mention can beeither named (e.g., John, Chicago), nominal (e.g.,president, activist) or pronominal (e.g., he, she).
Ithas also a specific class which describes the type ofthe entity it refers to.
For instance, in the sentence:1http://www.itl.nist.gov/iad/mig/tests/ace/2007/doc/ace07-evalplan.v1.3a.pdfMichael Bloomberg, the Mayor of NYC, declaredhis war on tobacco and sugary drinks in the city.we find the mentions ?Michael Bloomberg?, ?Mayor?and ?his?
of the same person entity.
Their typesare named, nominal and pronominal, respectively.?NYC?
and ?city?, on the other hand, are mentionsof the same geopolitical (GPE) entity of type namedand nominal, respectively.
Consequently, MD is amore general and complex task than the well knownNamed Entity Recognition (NER) task which aimssolely at the identification and classification of thenamed mentions.The difficulty of the MD task is directly relatedto the nature of the language and the linguistic re-sources available, i.e.
it is easier to build accu-rate MD systems for languages with a simple mor-phology and a high amount of linguistic resources.For this reason, we explore the idea of using anMD system, which has been designed and built fora resource-rich language (RRL), to help enhancethe performance of an MD system in a target lan-guage (TL).
More specifically, the goal of the re-search work we present in this paper is to employthe richness of English, in terms of natural lan-guage resources, to raise the accuracy of MD sys-tems in other languages.
For instance, an EnglishMD system might achieve a performance of F?=1-measure=82.7 (Zitouni and Florian, 2009) when itresorts to a rich set of features extracted from di-verse resources, namely: part-of-speech, chunk in-formation, syntactic parse trees, word sense infor-mation, WordNet information and information fromthe output of other mention detection classifiers.
Inthis paper, our research question revolves around in-vestigating an adequate approach to use such a sys-tem to the benefit of other languages such as Arabic,Chinese, French or Spanish MD systems, which also993have annotated resources but not of the same quan-tity and/or quality as English.In this paper, we have targeted English and Arabicas the RRL and TL, respectively, because:1.
We have a very competitive English MD system;2.
The linguistic resources available for the Arabiclanguage allow a simulation of different TL richnesslevels; and3.
The use of two languages of an utterly differentnature makes the extrapolation of the results to otherlanguages possible.Our hypothesis might be expressed as follows: us-ing an MD system resorting to a rich feature set (i.e.the RRL MD system) to boost a MD system perfor-mance in a TL can be very beneficial if the ?donor?system surpasses its TL counterpart in terms of re-sources.
To test this hypothesis, we have projectedMD tags from RRL to TL via a parallel corpus, andthen extracted several linguistic features about theautomatically tagged words.
Thereafter, we haveconducted experiments adding these new features tothe TL baseline MD system.
In order to have a com-plete picture on the impact of these new features, wehave used TL baseline systems resorting to a variedamount of features, starting with a case employingonly lexical information to a case where we use allthe resources we could gather for the TL.
Experi-ments show that the gain is always statistically sig-nificant and it reaches its maximum when only verybasic features are used in the baseline TL MD sys-tem.2 Mention DetectionSimilarly to classical NLP tasks, such as BasePhrase Chunking (Ramshaw and Marcus, 1999)(BPC) or NER (Tjong Kim Sang, 2002), we formu-late the MD task as a sequence classification prob-lem, i.e.
the classifier assigns to each token in thetext a label indicating whether it starts a specificmention, is inside a specific mention, or is outsideany mentions.
It also assigns to every non outsidemention a class to specify its type: e.g., person, or-ganization, location, etc.
In this study, we chose theMaximum Entropy Markov Model (MEMM hence-forth) approach because it can easily integrate arbi-trary types of information in order to make a clas-sification decision.
To train our models, we haveused the Sequential Conditional Generalized Itera-tive Scaling (SCGIS) technique (Goodman, 2002).This techniques uses a Gaussian prior for regular-ization (Chen and Rosenfeld, 2000).
The featuresused by our MD systems can be divided into the fol-lowing categories:1- Lexical: these are token n-grams directly neigh-boring the current token on both sides, i.e.
left andright.
Empirical results have shown that the optimalspan is n = 3.2- Syntactic: they consist of the outcomes of severalPart-Of-Speech (POS) taggers and BPCs trained ondifferent corpora and different tag-sets in order toprovide the MD system with a wider variety of in-formation.
Our model uses the POS and BPC in-formation appearing in window of 5 (current, twoprevious, and two next) jointly with the tokens.Both the English and the Arabic MD systems haveaccess to lexical and syntactic features.
The formerone, however, also employs a set of features ob-tained from the output of other MD classifiers.
Inorder to provide the MD system with complemen-tary information, these classifiers are trained on dif-ferent datasets annotated for different mention types,e.g.
dates or occupation references (not used in ourtask).3 Annotation, Projection and FeatureExtractionWe remind the reader that our main goal is to usean RRL MD system to enhance the performance ofan MD system in another language, i.e.
the TL.
Inorder to achieve this goal, we propose an approachthat uses an RRL-to-TL parallel corpus to bridge be-tween these two languages.
This approach performsin three main steps, namely: annotation, projectionand feature extraction.
In this section, we describein details each of these steps.3.1 AnnotationThis first step consists of MD tagging of the RRLside of the parallel corpus.
Because in our case studywe have chosen English as the RRL, we have usedan accurate English MD system to perform the an-notation step.
Our English MD system achieves anF-measure of 82.7 (Zitouni and Florian, 2009) andhas achieved significantly competitive results at theACE evaluation campaign.3.2 ProjectionOnce the RRL side of the parallel corpus is accu-rately augmented with MD tags, the projection stepcomes to transfer those tags to the TL side, Arabicin our case study, using the word alignment informa-tion.
We illustrate the projection step with a relevantexample.
Let consider the following MD tagged En-glish sentence:994Bill/B-PER-NAM Clinton/I-PER-NAM is visitingNorth/B-GPE-NAM Korea/I-GPE-NAM todaywhere ?Bill Clinton?
is a named person mention and?North Korea?
is a named geopolitical entity (GPE)one.
A potential Arabic translation of this sentencewould be:??J?
@?J?A???
@ AKP??
P?QK??JJ??
?JK.which might be transliterated as:byl klyntwn yzwr kwryA Al$mAlyA AlywmAfter projecting the English mentions to the Ara-bic text, we obtain the following:byl/B-PER-NAM klyntwn/I-PER-NAM yzwrkwryA/B-GPE-NAM Al$mAlyp/I-GPE-NAMAlywmThis tagged version of the Arabic text is provided tothe third module of the process responsible on fea-ture extraction (see Subsection 3.3).
It is, however,pertinent to point out that the example we have usedfor illustration is relatively simple in the sense thatalmost all English and Arabic words have a 1-to-1mapping.
In real world translation (both human andautomatic), one should expect to see 1-to-n, n-to-1mappings as well as unmapped words on both sidesof the parallel corpus rather frequently.As stated by (Klementiev and Roth, 2006), the pro-jection of NER tags is easier in comparison to pro-jecting other types of annotations such as POS-tagsand BPC2, mainly because:1.
Not all the words are mentions: once we have pro-jected the tags of the mentions from the RRL to TLside, the rest of tokens are simply considered as out-side any mentions.
This is different from the POS-tag and BPC where all the words are assigned a tagand thus when a word is unmapped, further process-ing is required (Yarowsky et al, 2001);2.
In case of a 1-to-n mapping, the target nwords are assigned the same class: for instance, letconsider the English GPE named mention ?North-Korea?.
The segmented version of its Arabic transla-tion would be ??J?A???
?
@ AKP???
(kwrya Al $mAlyp).The projection process consists in simply assigningthe same class, i.e.
GPE, to all Arabic tokens.
Theproblem takes another dimension, however, in thecase of propagating the POS-tags, because ?North?is a NNP aligned with the determinant (DET) ?Al?and the NNP ?$mAlyp?.
Additional processing isneeded to handle this difference of tags on the two2The claim is also valid for MD because it is the same typeof annotation.sides.3.
In case of n-to-1 mapping, the TL side word issimply assigned the class propagated from the RRLside.
For instance, if on the English side we have thenamed person multi-word mention ?Ben Moussa?,translated into the one-word mention ?????
JK.
(bn-mwsY) on the Arabic side, then projection consistsof simply assigning the person named tag to the Ara-bic word.However, in our research study, new challengesarose because our RRL data are automatically an-notated, which is different from what has been re-ported in the research works we have mentioned be-fore, i.e.
(Yarowsky et al, 2001) and (Klementievand Roth, 2006), where gold annotated data wereused.
In order to relax the impact of the noise intro-duced by the English MD system, we :1. use mention ?splits?
to filter annotation errors:We assume that when a sequence of tokens is taggedas a mention on the RRL side, its TL counterpartshould be an uninterrupted sequence of tokens aswell.
When the RRL MD system captures incor-rectly the span of a mention, e.g.
in the sentence?Dona Karan international reputation of ...?, theRRL MD system might mistakenly tag ?Dona Karaninternational?
as an organization mention instead oftagging ?Dona Karan?
as a person mention.
It is pos-sible to detect this type of errors on the TL side be-cause ?dwnA kArAn?
(Dona Karan) is distant from?Al EAlmyp?
(international), i.e.
they do not forman uninterrupted token sequence.
We use this ?split?in the mentions as information in order to not usethese mentions in the feature extraction step (seeSubsection 3.3).2. do not use the projected mentions directly fortraining: Instead, we use these tags as additionalfeatures to our TL baseline model and allow ourMEMM classifier to weigh them according to theirrelevance to each mention type.3.3 Feature ExtractionAt this point, the parallel corpus should be anno-tated with mentions on both of its sides.
Wherethe RRL side is tagged using the English MDsystem during the annotation step (c.f section 3.1)while the TL side is annotated by the propagationof these MD tags via the parallel corpus in theprojection step (c.f.
section 3.2).
In this third step,the goal is to extract pertinent linguistic featuresof the automatically tagged TL corpus to enhanceMD model in the TL.
The explored features are asfollows:9951.
Gazetteers: we group mentions by class indifferent dictionaries.
During both training anddecoding, when we encounter a token or a sequenceof tokens that is part of a dictionary, we fire itscorresponding class; the feature is fired only whenwe find a complete match between sequence oftokens in the text and in the dictionary.2.
Model-based features: it consists of building amodel on the automatically tagged TL side of theparallel corpus.
The output of this model is usedas a feature to enhance MD model in the targetlanguage.
However, it is also possible to use thismodel to directly tag text in the TL.
This wouldbe useful in cases where we do not have any TLannotated data.3.
n-gram context features: it consists of usingthe annotated corpus in the TL to collect n-gramtokens surrounding a mention.
We organize thosecontexts by mention type and we use them totag tokens which appear in the same contextin both the training and decoding sets.
Thesetags will be used as additional feature in theMD model.
For instance, if we consider thatthe person mention 	?
?k ?@Y?
(SdAm Hsyn -Sadam Husein) appears in the following sentence:C ?A?
A?A??@QK?
?k ?@Y??
@ ??
@ hQ?
?which might be transliterated as: SrH Ams An SdAmHsyn ytrAs nZAmA fA$lA and translated to Englishas: declared yesterday that Sadam Husein governsa failed systemthe context n-grams that would be extracted are:.
Left n-grams: W?1=?
@ (An - that),W?2=?
@ ??
@ (Ams An - yesterday that), etc..
Right n-grams: W+1=?
@QK(ystrAs - governs),W+2= A?A??
@QK(ytrAs nZAmA - governs a sys-tem), etc.. Left and right n-grams: a joint of the two previ-ous features, W?i and W+i.For both training and test data we create a newfeature stream where we indicate that a token se-quence is a mention if it appears in the same n-gramcontext.4.
Head-word based features: it considers thatthe lexical context in which the mention appearedis the sequence of the parent sub-trees head wordsin a parse-tree.
For instance, if we consider the sen-tence which we have used in the previous example,the corresponding parse tree is shown in Figure 1.The parent sub-tree heads of ?SdAm Hsyn?
areSVPp3hhhhhhh(((((((SrHh3 NPAmsSBARp2hhhhhhhh((((((((Anh2 Sp1PPPPNPaaa!!
!SdAm HsynVPQQytrAsh1 NPT?
?
?Figure 1: Parse treemarked with hi on the tree.
Similarly to the otherfeatures, in both training and decoding sets, wecreate a new feature stream where we tag thosetoken sequences which appear with the same n firstparent sub-tree head words as a person mention inthe annotated TL data.5.
Parser-based features: it attempts to use thesyntactic environment in which a mention might ap-pear.
In order to do so, for each mention in the tar-get language corpus we consider only labels of theparent non-terminals .We mark parent non-terminallabels of ?SdAm Hsyn?
on the tree with pi.
Simi-larly to the features described above, we create dur-ing both training and test a new feature stream wherewe indicate the token sequences which appear in thesame parent non-terminal labels.Gazetteers and model-based features are the mostnatural and expected kind of features that one wouldextract from the automatically MD tagged version ofthe TL text.
Our motivation of using n-gram contextfeatures, on one hand, and the head-word based andparse-based features on the other is to: (i) contrastthe impact of local and global context features; and(ii) experiment the possibility of employing both ofthem jointly in order to test their complementarity.4 The Target Language Mention DetectionSystem- The Arabic language: In our research study, wehave intentionally chosen a TL which is differs fromEnglish in its strategy in forming words and sen-tences.
By doing so, we are seeking to avoid ob-taining results which are biased by the similarity ofthe employed languages.
For this reason, we have996chosen Arabic as a TL.Due to its Semitic origins, the Arabic language isboth derivational, i.e.
it uses a templatic strategyto form a word, and highly inflectional, i.e.
addi-tional affixes might be added to a word in order toobtain further meaning.
Whereas the former char-acteristic is common in most languages, the latter,however, results in increasing sparseness in dataand consequently forming an obstacle to achieve ahigh performance for most of the NLP tasks (Diabet al, 2004; Benajiba et al, 2008; Zitouni et al,2005; Zitouni and Florian, 2008).
From a NLPviewpoint, especially the supervised tasks such asthe one we are dealing with in this paper, this im-plies that a huge amount of training data is nec-essary in order to build a robust model.
In ourstudy, to tackle the data sparseness problem, we haveperformed the word segmentation.
This segmenta-tion pre-processing step consists of separating thenormal white-space delimited words into prefixes,stems, and suffixes.
Thus, from a modeling view-point, the unit of analysis becomes the segments.
Weuse a technique similar to the one introduced in (Leeet al, 2003) for segmentation with an accuracy of98%.- The Arabic MD system: Our Arabic MD systememploys the same technique presented in Section 2.Compared to English MD model, Arabic MD sys-tem has access to morphological information (Stem)as we will explain next.
Features used by the ArabicMD system are divided in three categories:1.
Lexical: Similar to the lexical features used byour English MD system (c.f.
section 2);2.
Stem: This feature has been introduced in (Zitouniet al, 2005) as stem n-grams spanning the currentstem; both preceding and following it.
If the currenttoken xi is a stem, stem n-gram features contain theprevious n?
1 stems and the following n?
1 stems.Stem n-gram features represent a lexical generaliza-tion that reduce data sparseness;3.
Syntactic: it consists of the output of POS taggersand the BPCs.As we describe with more details in the experimentssection (see Section 6), once we have extracted thenew features from the parallel corpus, we contrasttheir impact with the level of richness in features ofthe TL MD system, i.e.
we measure the impact ofeach feature fi when the TL MD system uses: (i)only lexical features; (ii) both lexical and stem fea-tures; and (iii) lexical, stem and syntactic features.5 Evaluation DataExperiments are conducted on the Arabic ACE 2007data.
There are 379 Arabic documents and al-most 98, 000 words.
We find seven classes of men-tions: Person (PER), Organization (ORG), Geo-Political Entity (GPE), Location (LOC), Facility(FAC), Vehicle (VEH) and Weapon (WEA).
Sincethe evaluation test sets are not publicly available,we have split the publicly available training cor-pus into an 85%/15% data split.
We use 323 doc-uments (80, 000 words) for training and 56 docu-ments (18, 000 words) as a test set.
This resultsin 17, 634 mentions (7, 816 named, 8, 831 nominaland 987 pronominal) for training and 3, 566 for test(1, 673 named, 1, 682 nominal and 211 pronominal).To facilitate future comparisons with work presentedhere, and to simulate a realistic scenario, the splitsare created based on article dates: the test data is se-lected as the latest 15% of the data in chronologicalorder, in each of the covered genres (newswire andwebblog).
Performance on the ACE data is usuallyevaluated using a special-purpose measure, i.e.
theACE value metric.
However, given that we are inter-ested in the mention detection task only, we decidedto use the more intuitive and popular (un-weighted)F-measure, the harmonic mean of precision and re-call.6 Experiments and ResultsAs we have stated earlier, our main goal is to in-vestigate how an MD model of a TL might bene-fit from additional information about the mentionsobtained by propagation from an RRL.
In our re-search study we have chosen Arabic as the TL andEnglish as the RRL.
The English MD system we usehas access to a large set of information (Zitouni andFlorian, 2009) and has achieved a performance of82.7F on ACE?07 data.
In order to simulate differ-ent levels of resource-richness for the TL, we haveemployed four baseline systems which use differentfeature-sets.
Following we present these feature-setsranked from the resource-poorest to the resource-richest one: 1- Lex.
: lexical features; 2- Stem.:Lex.
+ stem features; and 3- Syntac.
: Stem.
+ syn-tactic features.For each of these baseline systems, we study the im-pact of features extracted from the parallel corpus(c.f.
Section 3) separately.
We report the followingresults:1- Base.
: baseline system without the use ofparallel-data extracted features;2- n?
Lex.
: Base.
+ n-gram context features;997Lex.
Stem SyntacBase.
74.14 74.47 75.53n?
Lex.
74.71 75.25 76.20n?Head 74.63 75.29 75.93n?
Pars.
75.32 75.19 75.74Gaz 74.90 74.79 75.66Model 74.60 75.50 76.22Comb.
76.01 76.74 77.18Table 1: Obtained results when the features were ex-tracted from a hand-aligned parallel corpus3- n?Head: Base.
+ head-word based features;4- n?
Pars.
: Base.
+ parser-related features;5- Gaz.
: Base.
+ automatically extractedgazetteers from the parallel corpus;6- Model: Base.
+ output of model trained on theArabic part of the parallel corpus;7- Comb.
: combination of all the above.In the rest of the paper, to measure whether the im-provement in performance of a system using fea-tures from parallel data over baseline is statisticallysignificant or not, we use the stratified bootstrap re-sampling significance test (Noreen, 1989) used inthe NER shared task of CoNLL-20023.
We considerresults as statistically significant when p < 0.02.6.1 Hand-aligned DataIn our first experiment-set, we use a hand-alignedEnglish-to-Arabic parallel corpus of approximatelyone million words.
After tagging the Arabic sideby projection we obtain 86.5K mentions.
As wehave previously mentioned, in order to generatethe model-based feature, Model, we have trained amodel on the Arabic side of the parallel corpus.
Thismodel achieved an F-measure of 57.7F.
This showsthe performance that might be achieved when no hu-man annotated data is available in the TL.Results in Table 1 show that a significant improve-ment is obtained when the TL is poor in resources;for instance an improvement of ?1.9 points wasachieved when the TL used only lexical features.The use of n ?
Pars.
features alone yielded 1.2points of improvement.
when the TL model uses arich feature-set, we still can obtain ?1.7 points im-provement.
When the TL baseline model employsthe Syntac feature-set, the greatest improvementis obtained when we add the model-based feature.Improvement obtained by the system using Comb.3http://www.cnts.ua.ac.be/conll2002/ner/features is statistically significant compared to thebaseline model.
This system also outperforms sys-tems using the new feature set separately across theboard.
According to our error-analysis, the signif-icant amount of Arabic mentions observed in theparallel corpus, where many of them do not appearin the training corpus, has significantly helped theLex., Stem and SyntacMD models to capture newmentions and/or correct the type assigned.
Some ofthe relevant examples in our data are: (i) the facilitymention P???K.
??J.?
(mbnY blfwr - Belvoir Build-ing); (ii) the GPE mention ??K.
A?
(kAbwl - Kabul);and (iii) the person mention 	?J?J.
?
@ (AlbEvyyn - theBaathists).
These mentions have only been taggedcorrectly when we have added the new extracted fea-tures to our model.In other words, the error-analysis clearly points outthat one possible way to get further improvement isto increase the parallel data in order to increase thenumber of matches between (1) the number of men-tions which are wrongly tagged by the TL MD modeland (2) the number of mentions in the TL side of theparallel corpus.
The second parameter can be, indi-rectly, increased by increasing the size of the paral-lel data.
Getting 10 or 20 times more of parallel datathat is hand-aligned is expensive and requires sev-eral months of human/hours work.
For this reasonwe opted for using an unsupervised approach by se-lecting a parallel corpus that is automatically alignedas we discuss in the next section.6.2 Automatically-aligned DataWe have used for this experiment-set an Arabic-to-English parallel data of 22 million words.
The datain this corpus is automatically aligned using a tech-nique presented in (Ittycheriah and Roukos, 2005).The alignment is one-to-many with a performancearound 87 F-measure.Because we are dealing with a large amount ofdata and the word alignment is done automatically,meaning more noise, we have used the English MDmodel confidence for additional filtering.
Such fil-tering consists in keeping, from the parallel corpus,only sentences which have all tokens tagged with aconfidence greater than ?.
In this paper, we use avalue of ?
= 0.94, which results in a corpus of 17million words.
We notice that a lower value of ?
re-sults in a radical increase in noise.
Because of spacelimitation, we will report results only with this valueof ?.Table 2 shows the obtained results for parallel-998Lex.
Stem SyntacBase.
74.14 74.47 75.53n?
Lex.
74.27 74.74 75.24n?Head.
74.07 74.95 75.33n?
Pars.
75.62 75.22 76.02Gaz 73.96 74.11 74.94Model 74.87 75.12 75.76Comb.
75.56 75.93 76.46Table 2: Obtained results when the features were ex-tracted from a automatically-aligned parallel corpusdata based features using the 17M subset.
Differ-ently from experiments using hand-aligned data, thebest results have been obtained when we have usedthe parser-based feature, i.e.
n ?
Pars.
On onehand, the overall behavior is comparable to the oneobtained when using the 1M hand-aligned paralleldata (see Table 1), i.e.
(i) the greatest improve-ment has been obtained when the TL uses a poorfeature-set; and (ii) when the TL baseline model isrich in resources, we still obtain 0.45 points absoluteimprovement when using n ?
Pars.
On the otherhand, features extracted from automatically-aligneddata, in comparison with the ones extracted from thehand aligned data, have helped the MD model to cor-rect many of the TL baseline model false negatives.This has been observed when the TL baseline sys-tem uses a rich feature set as well.
A side effect ofthe noisy word alignment, however, was an increasein the number of false positives.
For instance, theword H@Q?
?jJ??
(mstHDrAt - preparations) whichappeared in the following sentence:?Qk@ H@Q??jJ???
hA???
@ ?Y?which might be transliterated as:Edm AlsmAH lmstHDrAt AxrYand translated to English as:not to allow other preparationshas been tagged as an organization mention becauseit has been mistakenly aligned, in the parallel cor-pus, with the word ?A?, KO, in the sentence:?J?J?j.J?
@ H@Q??jJ????
?Q.??
@ ?A??
?Q??meaning:The big cosmetics company KO.In order to validate our results, we run our exper-iments on a blind test-set.
We have selected thelatest 5% of each genre of the hand-aligned dataClass Num.
of mentionsFAC 285GPE 2,145LOC 239ORG 1,135PER 2,474VEH 65WEA 138Table 3: Distribution over the classes of the blind testmentionsLex.
Stem SyntacBase.
74.26 73.54 73.61n?
Lex.
74.04 73.72 73.83n?Head 74.14 73.64 73.83n?
Pars.
74.32 74.18 74.32Gaz 71.49 72.13 73.39Model 75.01 74.66 74.78Table 4: Obtained results on blind testand they have been manually annotated by a hu-man.
The blind test-set consists of 51,781 tokens ofwhich 6,481 are mentions.
Table 3 shows the distri-bution of these mentions over the different classes.The results are shown in Table 4.
These results con-firm the conclusions we have deduced from the onespreviously presented in Table 2, i.e.
: (i) the highestimprovement is obtained when the TL is resource-scarce.6.3 Combining Hand-aligned andAutomatically-aligned DataTable 5 shows that combining both featuresextracted from hand-aligned and automatically-aligned corpora has led to better results.
The im-Lex.
Stem SyntacBase.
74.14 74.47 75.53n?
Lex.
74.60 75.08 75.58n?Head 74.51 75.32 75.56n?
Pars.
75.46 75.90 76.22Gaz 74.85 74.83 75.92Model 74.83 75.59 75.40Comb.
76.39 76.85 77.23Table 5: Obtained results when the features wereextracted from both hand-aligned and automatically-aligned parallel corpora999provement of using Comb.
compared to baseline isstatistically significant.
We notice again that whenthe TL baseline MD model uses a richer feature set,the obtained improvement from using RRL becomessmaller.
We also observed that automatically aligneddata helped capture most of the unseen mentionswhereas the hand-aligned features helped decreasethe number of false-alarms.
It is important to noticethat when features Comb.
is used with Stem base-line model, the obtained F-measure (76.85) is 1.3higher than the baseline model which uses lexical,stem and syntactic features ?
Syntac (75.53).
Thetype of errors which mostly occur and has not beenfixed neither by using hand-aligned data, automati-cally aligned data nor the combination of both arethe nominal mentions whose class depends fully onthe context.
For instance, the word????
(mwZf -employee) which was considered as O by the MDmodel because it has not been seen in any of the par-allel data in a context such as the following:.
.
.
?A?
?Q????
@?????
@ ??
?
?KQ?Ktransliterated as:tEryf $kl AlmwZf AlmSry ...and translated as: ?defining the life of the Egyptianemployee ...?7 Previous WorksSeveral research works, in different NLP tasks, haveshown that the use of an RRL to achieve a betterperformance in a resource-challenged languageyields to successful results.
In (Rogati et al, 2003),authors used a statistical machine translation (MT)system to build an Arabic stemmer.
The obtainedstemmer has a performance of 87.5%.
In (Ide et al,2002), authors use the aligned versions of GeorgeOrwell?s Nineteen Eighty-Four in seven languagesin order to determine sense distinctions which canbe used in the Word Sense Disambiguation (WSD)task.
They report that the automatically obtainedtags are at least as reliable as the one made by hu-man annotators.
Similarly, (Ng et al, 2003) report aresearch study which uses an English-Chinese par-allel corpus in order to extract sense-tagged trainingdata.
In (Hwa et al, 2002), authors report promisingresults of inducing Chinese dependency trees fromEnglish.
The obtained model outperformed thebaseline.One of the significant differences between theseworks and the one we present in this paper is thatinstead of using the propagated annotation directlyas training data we use it as an additional feature andthus allow the MEMM model to weigh each one ofthem.
By doing so, the model is able to distinguishbetween the relevant and the irrelevant informationpropagated from the RRL.Authors in (Zitouni and Florian, 2008) attempt toenhance an MD model of a foreign language by us-ing an English MD system.
They have used an MTsystem to (i) translate the text to English; (ii) run theEnglish model on the translated text; (iii) and prop-agate outcome to the original text.
The approachin (Zitouni and Florian, 2008) requires a MT systemthat needs more effort and resources to build whencompared to a parallel corpus (used in our experi-ments); not all institutions may have access to MTand MD systems in plenty of language pairs.8 Conclusions and Future WorksIn this paper, we presented a novel approach that al-lows to exploit the richness, in terms of resources, ofone language (English) to the benefit of a target lan-guage (Arabic).
We achieved successful results byadopting a novel approach performing in three mainsteps, namely: (i) Annotate the English side of anEnglish-to-Arabic parallel corpus automatically; (ii)Project the obtained annotation from English to Ara-bic via the parallel corpus; and (iii) Extract featuresof different linguistic motivations of the automati-cally tagged Arabic tokens.
Thereafter, each of theextracted features is used to bootstrap Arabic MDsystem.
We use different Arabic baseline MD mod-els which employ different feature sets representingdifferent levels of richness in resources.
We also useboth a one million word hand-aligned parallel cor-pus and a 22 million word automatically aligned onein order to study size vs. noise trade-off.Results show that a statistically significant improve-ment is always observed even when the Arabic base-line MD model uses all the available resources.When we use the hand-aligned parallel corpus, weobtain up to 2.2 points improvement when the Ara-bic MD model has access to very limited resources.It decreases to 1.7 points when we use all the re-sources we could gather for the Arabic language.When no human-annotated data is available in theTL, we show that we can obtain a performance of57.6 using only mention propagation from RRL.The results also show that a greater improvementis achieved when using a small hand-aligned corpusthan using a 20 times bigger automatically aligneddata.
However, in case both of them are available,combining them leads to even higher results.1000ReferencesYassine Benajiba, Mona Diab, and Paolo Rosso.
2008.Arabic named entity recognition using optimized fea-ture sets.
In Proc.
of EMNLP?08, pages 284?293.Stanley Chen and Ronald Rosenfeld.
2000.
A survey ofsmoothing techniques for ME models.
IEEE Transac-tion on Speech and Audio Processing.Mona Diab, Kadri Hacioglu, and Dan Jurafsky.
2004.Automatic tagging of arabic text: from raw text to basephrase chunks.
In Proc.
of HLT/NAACL?04.Joshua Goodman.
2002.
Sequential conditional general-ized iterative scaling.
In Proceedings of ACL?02.Rebecca Hwa, Philip Resnik, and Amy Weinberg.
2002.Breaking the resource bottleneck for multilingual pars-ing.
In Proceedings of LREC.Nancy Ide, Tomaz Erjavec, and Dan Tufis.
2002.
Sensediscrimination with parallel corpora.
In Proceedingsof the SIGLEX/SENSEVAL Workshop on Word SenseDisambiguation, pages 54?60.Abe Ittycheriah and Salim Roukos.
2005.
A maximumentropy word aligner for arabic-english machine trans-lation.
In Proceedings of HLT/EMNLP?05, pages 89?96.Alexandre Klementiev and Dan Roth.
2006.
Weaklysupervised named entity transliteration and discoveryfrom multilingual comparable corpora.
In Proceed-ings of ACL?06, pages 817?824, Sydney, Australia.Association for Computational Linguistics.Young-Suk Lee, Kishore Papineni, Salim Roukos, Os-sama Emam, and Hany Hassan.
2003.
Languagemodel based Arabic word segmentation.
In Proc.
ofthe ACL?03, pages 399?406.Hwee Tou Ng, Bin Wang, and Yee Seng Chan.
2003.
Ex-ploiting parallel texts for word sense disambiguation:An empirical study.
In Proceedings of ACL?03, pages455?462.Eric W. Noreen.
1989.
Computer-Intensive Methods forTesting Hypotheses.
John Wiley Sons.Lance Ramshaw and Mitchell Marcus.
1999.
Textchunking using transformation-based learning.
InS.
Armstrong, K.W.
Church, P. Isabelle, S. Manzi,E.
Tzoukermann, and D. Yarowsky, editors, Natu-ral Language Processing Using Very Large Corpora,pages 157?176.
Kluwer.Monica Rogati, Scott McCarley, and Yiming Yang.
2003.Unsupervised learning of arabic stemming using a par-allel corpus.
In Proceedings of ACL?03, pages 391?398.Eric.
F. Tjong Kim Sang.
2002.
Introduction to the conll-2002 shared task: Language-independent named entityrecognition.
In Proceedings of CoNLL-2002, pages155?158.
Taipei, Taiwan.David Yarowsky, Grace Ngai, and Richard Wicentowski.2001.
Inducing multilingual text analysis tools via ro-bust projection across aligned corpora.
In Proceedingsof HLT?01, pages 1?8.Imed Zitouni and Radu Florian.
2008.
Mention detectioncrossing the language barrier.
In Proc.
of the Confer-ence on Empirical Methods in Natural Language Pro-cessing (EMNLP), Honolulu, Hawaii, October.Imed Zitouni and Radu Florian.
2009.
Cross-languageinformation propagation for arabic mention detection.ACM Transactions on Asian Language InformationProcessing (TALIP), 8(4):1?21.Imed Zitouni, Jeff Sorensen, Xiaoqiang Luo, and RaduFlorian.
2005.
The impact of morphological stem-ming on arabic mention detection and coreference res-olution.
In Proc.
of the ACL Workshop on Computa-tional Approaches to Semitic Languages, pages 63?70.1001
