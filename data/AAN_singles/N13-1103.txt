Proceedings of NAACL-HLT 2013, pages 826?836,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsUsing Semantic Unification to GenerateRegular Expressions from Natural LanguageNate Kushman Regina BarzilayComputer Science and Artificial Intelligence LaboratoryMassachusetts Institute of Technology{nkushman, regina}@csail.mit.eduAbstractWe consider the problem of translating natu-ral language text queries into regular expres-sions which represent their meaning.
The mis-match in the level of abstraction between thenatural language representation and the regu-lar expression representation make this a noveland challenging problem.
However, a givenregular expression can be written in many se-mantically equivalent forms, and we exploitthis flexibility to facilitate translation by find-ing a form which more directly corresponds tothe natural language.
We evaluate our tech-nique on a set of natural language queriesand their associated regular expressions whichwe gathered from Amazon Mechanical Turk.Our model substantially outperforms a state-of-the-art semantic parsing baseline, yieldinga 29% absolute improvement in accuracy.11 IntroductionRegular expressions (regexps) have proven them-selves to be an extremely powerful and versatile for-malism that has made its way into everything fromspreadsheets to databases.
However, despite theirusefulness and wide availability, they are still con-sidered a dark art that even many programmers donot fully understand (Friedl, 2006).
Thus, the abilityto automatically generate regular expressions fromnatural language would be useful in many contexts.Our goal is to learn to generate regexps from nat-ural language, using a training set of natural lan-guage and regular expression pairs such as the onein Figure 1.
We do not assume that the data includesan alignment between fragments of the natural lan-guage and fragments of the regular expression.
In-1The dataset used in this work is available athttp://groups.csail.mit.edu/rbg/code/regexp/Text Description Regular Expressionthree letter word starting with ?X?
\bX[A-Za-z]{2}\bFigure 1: An example text description and its associatedregular expression.3ducing such an alignment during learning is partic-ularly challenging because oftentimes even humansare unable to perform a fragment-by-fragment align-ment.We can think of this task as an instance ofgrounded semantic parsing, similar to the workdone in the domain of database queries (Kate andMooney, 2006; Zettlemoyer and Collins, 2005;Kwiatkowski et al 2010).
However, the currentsuccess in semantic parsing relies on two impor-tant properties of the data.
First, while the pastwork did not assume the alignment was given, theydid assume that finding a fine grained fragment-by-fragment alignment was possible.
Secondly,the semantic domains considered in the past werestrongly typed.
This typing provides constraintswhich significantly reduce the space of possibleparses, thereby greatly reducing the ambiguity.However, in many interesting domains these twoproperties may not hold.
In our domain, the align-ment between the natural language and the regu-lar expressions often happens at the level of thewhole phrase, making fragment-by-fragment align-ment impossible.
For example, in Figure 1 no frag-ment of the regexp maps clearly to the phrase ?threeletter?.
Instead, the regexp explicitly represents thefact that there is only two characters after X, which isnot stated explicitly by the text description and mustbe inferred.
Furthermore, regular expressions have3Our regular expression syntax supports Perl regular expres-sion shorthand which utilizes \b to represent a break (i.e.
aspace or the start or end of the line).
Our regular expressionsyntax also supports intersection (&) and complement(?).826([A-Za-z]{3})&(\b[A-Za-z]+\b)&(X.
*)(a)three letter [A-Za-z]{3}word \b[A-Za-z]+\bstarting with ?X?
X.
*(b)Figure 2: (a) shows a regexp which is semanticallyequivalent to that in Figure 1, yet admits a fragment-by-fragment mapping to the natural language.
(b) shows thismapping.relatively few type constraints.The key idea of our work is to utilize semanticunification in the logical domain to disambiguate themeaning of the natural language.
Semantic unifi-cation utilizes an inference engine to determine thesemantic equality of two syntactically divergent ex-pressions.
This is a departure from past work on se-mantic parsing which has largely focused on the syn-tactic interface between the natural language and thelogical form, and on example-based semantic equal-ity, neither of which utilize the inference power in-herent in many symbolic domains.To see how we can take advantage of semanticunification, consider the regular expression in Fig-ure 2(a).
This regular expression is semanticallyequivalent to the regular expression in Figure 1.
Fur-thermore, it admits a fragment-by-fragment map-ping as can be seen in Figure 2(b).
In contrast, aswe noted earlier, the regexp in Figure 1 does not ad-mit such a mapping.
In fact, learning can be quitedifficult if our training data contains only the regexpin Figure 1.
We can, nonetheless, use the regexp inFigure 2 as a stepping-stone for learning if we canuse semantic inference to determine the equivalencebetween the two regular expressions.
More gener-ally, whenever the regexp in the training data doesnot factorize in a way that facilitates a direct map-ping to the natural language description, we mustfind a regexp which does factorize and be able tocompute its equivalence to the regexp we see in thetraining data.
We compute this equivalence by con-verting each regexp to a minimal deterministic finiteautomaton (DFA) and leveraging the fact that mini-mal DFAs are guaranteed to be the same for seman-tically equivalent regexps (Hopcroft et al 1979).We handle the additional ambiguity stemmingfrom the weak typing in our domain through the useof a more effective parsing algorithm.
The state ofthe art semantic parsers (Kwiatkowski et al 2011;Liang et al 2011) utilize a pruned chart parsingalgorithm which fails to represent many of the topparses and is prohibitively slow in the face of weaktyping.
In contrast, we use an n-best parser whichalways represents the most likely parses, and can bemade very efficient through the use of the parsingalgorithm from Jimenez and Marzal (2000).Our approach works by inducing a combinatorycategorial grammar (CCG) (Steedman, 2001).
Thisgrammar consists of a lexicon which pairs wordsor phrases with regular expression functions.
Thelearning process initializes the lexicon by pairingeach sentence in the training data with the full reg-ular expression associated with it.
These lexical en-tries are iteratively refined by considering all possi-ble ways to split the regular expression and all pos-sible ways to split the phrase.
At each iteration wefind the n-best parses with the current lexicon, andfind the subset of these parses which are correct us-ing DFA equivalence.
We update the weights of alog-linear model based on these parses and the cal-culated DFA equivalence.We evaluate our technique using a dataset of sen-tence/regular expression pairs which we generatedusing Amazon Mechanical Turk (Turk, 2013).
Wefind that our model generates the correct regexpfor 66% of sentences, while the state-of-the-art se-mantic parsing technique from Kwiatkowski et al(2010) generates correct regexps for only 37% ofsentences.
The results confirm our hypothesis thatleveraging the inference capabilities of the seman-tic domain can help disambiguate natural languagemeaning.2 Related WorkGenerating Regular Expressions Past work haslooked at generating regular expressions from nat-ural language using rule based techniques (Ranta,1998), and also at automatically generating regularexpressions from examples (Angluin, 1987).
To thebest of our knowledge, however, our work is the firstto use training data to learn to automatically gener-ate regular expressions from natural language.Language Grounding There is a large body of re-search mapping natural language to some form ofmeaning representation (Kate and Mooney, 2006;Kate et al 2005; Raymond and Mooney, 2006;Thompson and Mooney, 2003; Wong and Mooney,8272006; Wong and Mooney, 2007; Zelle and Mooney,1996; Branavan et al 2009; Mihalcea et al 2006;Poon and Domingos, 2009).
In some of the consid-ered domains the issue of semantic equivalence doesnot arise because of the way the data is generated.The most directly related work in these domains, isthat by Kwiatkowski et al(2010 and 2011) which isan extension of earlier work on CCG-based semanticparsing by Zettlemoyer and Collins (2005).
Similarto our work, Kwiatkowski et alutilize unification tofind possible ways to decompose the logical form.However, they perform only syntactic unification.Syntactic unification determines equality using onlyvariable substitutions and does not take advantage ofthe inference capabilities available in many semanticdomains.
Thus, syntactic unification is unable to de-termine the equivalence of two logical expressionswhich use different lexical items, such as ?.*?
and?.*.*?.
In contrast, our DFA based technique candetermine the equivalence of such expressions.
Itdoes this by leveraging the equational inference ca-pabilities of the regular expression domain, makingit a form of semantic unification.
Thus, the contribu-tion of our work is to show that using semantic uni-fication to find a deeper level of equivalence helps todisambiguate language meanings.In many other domains of interest, determiningsemantic equivalence is important to the learningprocess.
Previous work on such domains has fo-cused on either heuristic or example-driven mea-sures of semantic equivalence.
For example, Artziand Zettlemoyer (2011) estimate semantic equiva-lence using a heuristic loss function.
Other pastwork has executed the logical form on an exampleworld or in a situated context and then compared theoutputs.
This provides a very weak form of semanticequivalence valid only in that world/context (Clarkeet al 2010; Liang et al 2009; Liang et al 2011;Chen and Mooney, 2011; Artzi and Zettlemoyer,2013).
In contrast, our work uses an exact, theoret-ically sound measure of semantic equivalence thatdetermines whether two logical representations areequivalent in any context, i.e.
on any input string.3 Background3.1 Finding Regexp Equivalence Using DFAsRegular expressions can be equivalently representedas minimal DFAs, which are guaranteed to be equalfunction sig.
regexp function signature regexpcons(R,R,...) ab rep*(R) a*and(R,R,...) [a-b]&[b-c] repminmax(I,I,R) a{3,5}or(R,R,...) a|b repmin(I,R) a{3,}not(R) ?
(a) repexact(I,R) a{3}Figure 3: This shows the signatures of all functions in ourlambda calculus along with their regexp syntax.for the same regular language (Hopcroft et al1979).
The DFA representation of a regular expres-sion may be exponentially larger than the the orig-inal regular expression.
However, past work hasshown that most regular expressions do not exhibitthis exponential behavior (Tabakov and Vardi, 2005;Moreira and Reis, 2012), and the conversion pro-cess is renowned for its good performance in prac-tice (Moreira and Reis, 2012).
Hence, we comparethe equivalence of two regular expressions by con-verting them to minimal DFAs and comparing theDFAs.
We do this using a modified version of M?ller(2010).43.2 Lambda Calculus RepresentationTo take advantage of the inherent structure of reg-ular expressions, we deterministically convert themfrom a flat string representation into simply typedlambda calculus expressions.
The full set of func-tions available in our lambda calculus can be seenin Figure 3.
As can be seen from the figures, ourlambda calculus is very weakly typed.
It has onlytwo primitive types, integer (I) and regexp (R), withmost arguments being of type R.3.3 ParsingOur parsing model is based on a Combinatory Cate-gorial Grammar.
In CCG parsing most of the gram-mar complexity is contained in the lexicon, ?, whilethe parser itself contains only a few simple rewriterules called combinators.Lexicon The lexicon, ?, consists of a set of lexicalentries that couple natural language with a lambdacalculus expression.
Our lexical entries containwords or phrases, each of which is associated witha function from the lambda calculus we describedin ?3.2.
For example:4We set a timeout on this process to catch any cases wherethe resulting DFA might be prohibitively large.
We use a onesecond timeout in our experiments, which results in timeoutson less than 0.25% of the regular expressions.828with ?bob?
after ?joe???????
??
???????
?
?R/R R R\R/R R?x(.*x.
*) bob ?xy.(x.
*y) joe?????????(f)R\R?y.joe.*y????????????(b)Rjoe.*bob????????????????????(f)R.*joe.*bob.
*Figure 4: This shows an example parse.?
after, R\R/R:?xy.(x.
*y) ??
at least, R/I/R:?xy.
((x){y,}) ?Note that the lambda expressions contain type infor-mation indicating the number of arguments and thetype of those arguments as described in ?3.2.
How-ever, this information is augmented with a (/) or a(\) for each argument indicating whether that argu-ment comes from the left or the right, in sentenceorder.
Thus R\R/R can be read as a function whichfirst takes an argument of type R on the right thentakes another argument of type R on the left, andreturns an expression of type R.Combinators Parses are built by combining lexicalentries through the use of a set of combinators.
Ourparser uses only the two most basic combinators,forward function application and backward functionapplication.5 These combinators work as follows:R/R:f R:g ?
R:f(g) (forward)R:f R\R:g ?
R:g(f) (backward)The forward combinator applies a function to an ar-gument on its right when the type of the argumentmatches the type of the function?s first argument.The backward combinator works analogously.
Fig-ure 4 shows an example parse.4 Parsing ModelFor a given lexicon, ?, and sentence, ~w, there will ingeneral be many valid parse trees, t ?
T (~w; ?).
Weassign probabilities to these parses using a standardlog-linear parsing model with parameters ?
:p(t|~w; ?,?)
=e???
(t, ~w)?t?
e???
(t?, ~w)Our training data, however, includes only the cor-rect regular expression, r, and not the correct parse,5Technically, this choice of combinators makes our modeljust a Categorial Grammar instead of a CCG.t.
The training objective used by the past work insuch circumstances, is to maximize the probabilityof the correct regular expression by marginalizingover all parses which generate that exact regular ex-pression.
Such an objective is limited, however, be-cause it does not allow parses that generate seman-tically correct regexps which are not syntacticallyequivalent to r, such as those in Figure 2.
The maindeparture of our work is to use an objective which al-lows such parses through the use of the DFA-EQUALprocedure.
DFA-EQUAL uses the process describedin ?3.1 to determine whether parse t evaluates to aregexp which is semantically equivalent to r, lead-ing to the following objective:O =?ilog?t|DFA-EQUAL(t,ri)p(t|~wi; ?,?)
(1)At testing time, for efficiency reasons, we calcu-late only the top parse.
Specifically, if r = eval(t)is the regexp which results from evaluating parse t,then we generate t?
= arg maxt?T (~w) p(t|~w; ?,?
),and return r?
= eval(t?
).5 LearningOur learning algorithm starts by generating a singlelexical entry for each training sample which pairsthe full sentence, ~wi, with the associated regular ex-pression, ri.
Formally, we initialize the lexicon as?
= {?~wi, R : ri?
|i = 1 .
.
.
n}.
We then run an iter-ative process where in each iteration we update both?
and ?
for each training sample.
Our initial ?
willperfectly parse the training data.
However it won?tgeneralize at all to the test data since the lexical en-tries contain only full sentences.
Hence, in eachiteration we refine the lexicon by splitting existinglexical entries to generate more granular lexical en-tries which will generalize better.
The candidates forsplitting are all lexical entries used by parses whichgenerate the correct regular expression, ri, for thecurrent training sample.
We consider all possibleways to factorize each lexical entry, and we add to?
a new lexical entry for each possible factorization,as discussed in ?5.2.
Finally, we update ?
by per-forming a single stochastic gradient ascent updatestep for each training sample, as discussed in ?5.1.See Algorithm 1 for details.This learning approach follows the structureof the previous work on CCG based seman-tic parsers (Zettlemoyer and Collins, 2005;829Inputs: Training set of sentence regular expression pairs.
{?~wi, ri?
|i = 1 .
.
.
n}Functions:?
N-BEST(~w; ?,?)
n-best parse trees for ~w using thealgorithm from ?5.1?
DFA-EQUAL(t, r) calculates the equality of the regexpfrom parse t and regexp r using the algorithm from ?3.1?
SPLIT-LEX(T ) splits all lexical entries used by anyparse tree in set T , using the process described in ?5.2Initialization: ?
= {?~wi, R : ri?
|i = 1 .
.
.
n}For k = 1 .
.
.K, i = 1 .
.
.
nUpdate Lexicon: ??
T = N-BEST(~wi; ?,?)?
C = {t|t ?
T ?
DFA-EQUAL(t, ri)}?
?
= ?
?
SPLIT-LEX(C)Update Parameters: ??
T = N-BEST(~wi; ?,?)?
C = {t|t ?
T ?
DFA-EQUAL(t, ri)}?
?
= Ep(t|t?C)[?
(t, ~w)]?
Ep(t|t?T )[?
(t, ~w)]?
?
= ?
+ ?
?Output: The lexicon and the parameters, ?
?, ?
?Algorithm 1: The full learning algorithm.Kwiatkowski et al 2010).
However, our domainhas distinct properties that led to three importantdepartures from this past work.First, we use the DFA based semantic unifica-tion process described in ?3.1 to determine the setof correct parses when performing parameter up-dates.
This is in contrast to the syntactic unificationtechnique, used by Kwiatkowski et al(2010), andthe example based unification used by other seman-tic parsers, e.g.
Artzi and Zettlemoyer (2011).
Us-ing semantic unification allows us to handle trainingdata which does not admit a fragment-by-fragmentmapping between the natural language and the reg-ular expression, such as the example in Figure 2.Second, our parser is based on the efficient n-bestparsing algorithm of Jimenez and Marzal (2000) in-stead of the pruned chart parsing algorithm usedby the past work (Zettlemoyer and Collins, 2005;Kwiatkowski et al 2010).
As we show in ?8.2, thisresults in a parser which more effectively representsthe most likely parses.
This allows our parser to bet-ter handle the large number of potential parses thatexist in our domain due to the weak typing.Third, we consider splitting lexical entries used inany correct parse, while the past work (Zettlemoyerand Collins, 2005; Kwiatkowski et al 2010) con-siders splitting only those used in the best parse.
Wemust utilize a less constrictive splitting policy sinceour domain does not admit the feature weight ini-tialization technique used in the domains of the pastwork.
We discuss this in ?5.2.1.
In the remainderof this section we discuss the process for learning ?and for generating the lexicon, ?.5.1 Estimating ThetaTo estimate ?
we will use stochastic gradient ascent,updating the parameters based on one training exam-ple at a time.
Hence, we can differentiate the objec-tive from equation 1 to get the gradient of parameter?j for training example i, as follows:?Oi??j=Ep(t|DFA-EQUAL(t,ri),?)
[?j(t, ~wi)]?
Ep(t|?)
[?j(t, ~wi)](2)This gives us the standard log-linear gradient, whichrequires calculating expected feature counts.
We de-fine the features in our model over individual parseproductions, admitting the use of dynamic program-ming to efficiently calculate the unconditioned ex-pected counts.
However, when we condition on gen-erating the correct regular expression, as in the firstterm in (2), the calculation no longer factorizes, ren-dering exact algorithms computationally infeasible.To handle this, we use an approximate gradientcalculation based on the n-best parses.
Our n-bestparser uses an efficient algorithm developed orig-inally by (Jimenez and Marzal, 2000), and subse-quently improved by (Huang and Chiang, 2005).This algorithm utilizes the fact that the first bestparse, t1, makes the optimal choice at each deci-sion point, and the 2nd best parse, t2 must make thesame optimal choice at every decision point, exceptfor one.
To execute on this intuition, the algorithmfirst calculates t1 by generating an unpruned CKY-style parse forest which includes a priority queueof possible subparses for each constituent.
The setof possible 2nd best parses T are those that choosethe 2nd best subparse for exactly one constituent oft1 but are otherwise identical to t1.
The algorithmchooses t2 = arg maxt?T p(t).
More generally,T is maintained as a priority queue of possible nthbest parses.
At each iteration, i, the algorithm setsti = arg maxt?T p(t) and augments T by all parseswhich both differ from ti at exactly one constituentci and choose the next best possible subparse for ci.We use the n-best parses to calculate an approxi-mate version of the gradient.
Specifically, Ti is the830set of n-best parses for training sample i, and Ci in-cludes all parses t in Ti such that DFA-EQUAL(t, ri).We calculate the approximate gradient as:?
= Ep(t|t?Ci;?,?)[?
(t, ~wi)]?
Ep(t|t?Ti;?,?)[?
(t, ~wi)](3)In contrast to our n-best technique, the pastwork has calculated equation (2) using a beamsearch approximation of the full inside-outside algo-rithm (Zettlemoyer and Collins, 2005; Kwiatkowskiet al 2010; Liang et al 2011).
Specifically, sincethe conditional probability of t given r does not fac-torize, a standard chart parser would need to main-tain the full logical form (i.e.
regular expression)for each subparse, and there may be an exponentialnumber of such subparses at each chart cell.
Thus,they approximate this full computation using beamsearch, maintaining only the m-best logical forms ateach chart cell.Qualitatively, our n-best approximation alwaysrepresents the most likely parses in the approxima-tion, but the number of represented parses scalesonly linearly with n. In contrast, the number ofparses represented by the beam search algorithmof the past work can potentially scale exponentiallywith the beam size,m, due to its use of dynamic pro-gramming.
However, since the beam search prunesmyopically at each chart cell, it often prunes outthe highest probability parses.
In fact, we find thatthe single most likely parse is pruned out almost20% of the time.
Furthermore, our results in ?8show that the beam search?s inability to representthe likely parses significantly impacts the overallperformance.
It is also important to note that theruntime of the n-best algorithm scales much better.Specifically, as n increases, the n-best runtime in-creases as O(n|~w| log(|~w||P | + n), where P is theset of possible parse productions.
In contrast, asm is increased, the beam search runtime scales asO(|~w|5m2), where the |~w|5 factor comes from ouruse of headwords, as discussed in ?6.
In practice,we find that even with n set to 10, 000 and m set to200, our algorithm still runs almost 20 times faster.5.2 Lexical Entry SplittingEach lexical entry consists of a sequence of nwords aligned to a typed regular expression func-tion, ?w0:l, T : r?.
Our splitting algorithm considersall possible ways to split a lexical entry into two newcons Parent Tree Child Treeb .rep*o b.rep* cons .rep*x.rep* b o bconsOriginal Tree(a) (b) (c)Figure 5: The tree in (a) represents the lambda expressionfrom the lexical entry ?with bob, R:.*bob.*?.
One pos-sible split of this lexical entry generates the parent lexicalentry ?with, R/R:?x.(.*x.*)?
and the child lexical en-try, ?bob, R:bob?, whose lambda expressions are repre-sented by (b) and (c), respectively.lexical entries such that they can be recombined viafunction application to obtain the original lexical en-try.
This process is analogous to the syntactic unifi-cation process done by Kwiatkowski et al(2010).We first consider all possible ways to split thelambda expression r. The splitting process is mosteasily explained using a tree representation for r, asshown in Figure 5(a).
This tree format is simply aconvenient visual representation of a lambda calcu-lus function, with each node representing one of thefunction type constants from Figure 3.
Each split,s ?
S(r), generates a child expression sc and a par-ent expression sp such that r = sp(sc).
For eachnode, n, in r besides the root node, we generate asplit where sc is the subtree rooted at node n. Forsuch splits, sp is the lambda expression r with thesub-expression sc replaced with a bound variable,say x.
In addition to these simple splits, we also con-sider a set of more complicated splits at each nodewhose associated function type constant can takeany number of arguments, i.e.
or, and, or cons.
IfC(n) are the children of node n, then we generate asplit for each possible subset, {V |V ?
C(n)}.
Notethat for cons nodes V must be contiguous.
In ?6 wediscuss additional restrictions placed on the splittingprocess to avoid generating an exponential numberof splits.
For the split with subset V , the child tree,sc, is a version of the tree rooted at node n prunedto contain only the children in V .
Additionally, theparent tree, sp, is generated from r by replacing allthe children in V with a single bound variable, sayx.
Figure 5 shows an example of such a split.
Weonly consider splits in which sc does not have anybound variables, so its type, Tc, is always either Ror I .
The type of sp is then type of the original ex-pression, T augmented by an additional argument ofthe child type, i.e.
either T/Tc or T\Tc.Each split s generates two pairs of lexical entries,831one for forward application, and one for backwardapplication.
The set of such pairs of pairs is:{( ?w0:j , T/Tc : sp?
, ?wj:l, Tc : sc?
),(?w0:j , Tc : sc?
, ?wj:l, T\Tc : sp?
)|(0 ?
j ?
l) ?
(s ?
S(r))}5.2.1 Adding New Lexical EntriesOur model splits all lexical entries used in parseswhich generate correct regular expressions, i.e.those in Ci, and adds all of the generated lexicalentries to ?.
In contrast, the previous work (Zettle-moyer and Collins, 2005; Kwiatkowski et al 2010)has a very conservative process for adding new lex-ical entries.
This process relies on a good initial-ization of the feature weights associated with a newlexical entry.
They perform this initialization usinga Giza++ alignment of the words in the training sen-tences with the names of functions in the associatedlambda calculus expression.
Such an initializationis ineffective in our domain since it has very fewprimitive functions and most of the training exam-ples use more than half of these functions.
Instead,we add new lexical entries more aggressively, andrely on the n-best parser to effectively ignore anylexicon entries which do not generate high probabil-ity parses.6 Applying the ModelFeatures To allow inclusion of head words in ourfeatures, our chart cells are indexed by start word,end word, and head word.
Thus for each parse pro-duction we have a set of features that combine thehead word and CCG type, of the two children andthe newly generated parent.
Additionally, for eachlexical entry ?~wi, R : ri?
?
?, we have four types offeatures: (1) a feature for ?~wi, R : ri?, (2) a featurefor ~wi, (3) a feature for R : ri, and (4) a set of fea-tures indicating whether ~wi contains a string literaland whether the leaves of ri contain any exact char-acter matches (rather than character range matches).Initialization In addition to the sentence level ini-tialization discussed in ?5 we also initialize the lex-icon, ?, with two other sets of lexical entries.
Thefirst set is all of the quoted string literals in the natu-ral language phrases from the training set.
Thus forthe phrase, ?lines with ?bob?
twice?
we would addthe lexical entry ?
?bob?, R:bob ?.
We also add lex-ical entries for both numeric and word representa-tions of numbers, such as ?
1, R:1?
and ?
one, R:1?.We add these last two types of lexical entries be-cause learning them from the data is almost impos-sible due to data sparsity.
Lastly, for every individualword in our training set vocabulary, we add an iden-tity lexical entry whose lambda expression is just afunction which takes one argument and returns thatargument.
This allows our parser to learn to skipsemantically unimportant words in the natural lan-guage description, and ensures that it generates atleast one parse for every example in the dataset.
Attest time we also add both identity lexical entries forevery word in the test set vocabulary as well as lex-ical entries for every quoted string literal seen in thetest queries.
Note that the addition of these lexicalentries requires only access to the test queries anddoes not make use of the regular expressions (i.e.labels) in the test data in any way.Parameters We initialize the weight of all lexicalentry features except the identity features to a defaultvalue of 1 and initialize all other features to a defaultweight of 0.
We regularize our log-linear model us-ing the L2-norm and a ?
value of 0.001.
We use alearning rate of ?
= 1.0, set n = 10, 000 in our n-best parser, and run each experiment with 5 randomrestarts and K = 50 iterations.
We report resultsusing the pocket alrithm technique originated byGallant (1990).Constraints on Lexical Entry Splitting To preventthe generation of an exponential number of splits,we constrain the lexical entry splitting process asfollows:?
We only consider splits at nodes which are at mosta depth of 2 from the root of the original tree.?
We limit lambda expressions to 2 arguments.?
In unordered node splits (and and or) the result-ing child can contain at most 4 of the arguments.These restrictions ensure the number of splits isat most an M-degree polynomial of the regexp size.The unification process used by Kwiatowski et al(2010) bounded the number of splits similarly.7 Experimental SetupDataset Our dataset consists of 824 natural languageand regular expression pairs gathered using AmazonMechanical Turk (Turk, 2013) and oDesk (oDesk,2013).6 On Mechanical Turk we asked workers to6This is similar to the size of the datasets used by past work.832generate their own original natural language queriesto capture a subset of the lines in a file (similar toUNIX grep).
In order to compare to example basedtechniques we also ask the Mechanical Turk work-ers to generate 5 positive and 5 negative examplesfor each query.
On oDesk we hired a set of pro-grammers to generate regular expressions for eachof these natural language queries.
We split our datainto 3 sets of 275 queries each and tested using 3-fold cross validation.
We tuned our parameters sep-arately on each development set but ended up withthe same values in each case.Evaluation Metrics We evaluate by comparing thegenerated regular expression for each sentence withthe correct regular expression using our DFA equiv-alence technique.
As discussed in ?3.1 this met-ric is exact, indicating whether the generated regu-lar expression is semantically equivalent to the cor-rect regular expression.
Additionally, as discussedin ?6, our identity lexical entries ensure we generatea valid parse for every sentence, so we report onlyaccuracy instead of precision and recall.Baselines We compared against six different base-lines.
The UBL baseline uses the published codefrom Kwiatkowski et al(2010) after configuringit to handle the lambda calculus format of our reg-ular expressions.7 The other baselines are ablatedand/or modified versions of our model.
The Beam-Parse baselines replace the N-BEST procedure fromAlgorithm 1 with the beam search algorithm usedfor parsing by past CCG parsers (Zettlemoyer andCollins, 2005; Kwiatkowski et al 2010).8 TheStringUnify baseline replaces the DFA-EQUAL proce-dure from Algorithm 1 with exact regular expres-sion string equality.
The HeuristicUnify baselinesstrengthen this by replacing DFA-EQUAL with a smartheuristic form of semantic unification.
Our heuristicunification procedure first flattens the regexp treesby merging all children into the parent node if theyare both of the same type and of type or, and, orcons.
It then sorts all children of the and and oroperators.
Finally, it converts both regexps back toa flat string and compares these strings for equiva-lence.
This process should more effective than any7This was done in consultation with the original authors.8we set the beam size to 200, which is equivalent to the pastwork.
With this setting, the slow runtime of this algorithm al-lowed us to run only two random restarts.Model Percent CorrectUBL 36.5%BeamParse-HeuristicUnify 9.4%BeamParse-HeuristicUnify-TopParse 22.1%NBestParse-StringUnify 31.1%NBestParse-ExampleUnify 52.3%NBestParse-HeuristicUnify 56.8%Our Full Model 65.5%Table 1: Accuracy of our model and the baselines.form of syntactic unification and any simpler heuris-tics.
The ExampleUnify baseline represents the per-formance of the example based semantic unificationtechniques.
It replaces DFA-EQUAL with a procedurethat evaluates the regexp on all the positive and neg-ative examples associated with the given query andreturns true if all 10 are correctly classified.
Finally,BeamParse-HeuristicUnify-TopParse uses the samealgorithm as that for BeamParse-HeuristicUnify ex-cept that it only generates lexical entries from thetop parse instead of all parses.
This more closelyresembles the conservative lexical entry splitting al-gorithm used by Kwiatkowski et al8 ResultsOur model outperforms all of the baselines, asshown in Table 1.
The first three baselines ?UBL, BeamParse-HeuristicUnify, and BeamParse-HeuristicUnify-TopParse?
represent the algorithmused by Kwiatkowski et alOur model outperformsthe best of these by over 30% in absolute terms and180% in relative terms.The improvement in performance of our modelover the NBestParse-StringUnify, NBestParse-ExampleUnify and NBestParse-HeuristicUnifybaselines highlights the importance of our DFAbased semantic unification technique.
Specifi-cally, our model outperforms exact string basedunification by over 30%, example based semanticunification by over 13% and our smart heuristicunification procedure by 9%.
These improvementsconfirm that leveraging exact semantic unificationduring the learning process helps to disambiguatelanguage meanings.8.1 Effect of Additional Training DataTable 2 shows the change in performance as we in-crease the amount of training data.
We see that ourmodel provides particularly large gains when there833%age of Data 15% 30% 50% 75%NBestParse-HeuristicUnify12.4% 26.4% 39.0% 45.4%Our Model 29.0% 50.3% 58.7% 65.2%Relative Gain 2.34x 1.91x 1.51x 1.43xTable 2: Results for varying amounts of training data.0204060801001 2000 4000 6000 8000 10000%Represented# of Top Parses1st Iter10th IterFigure 6: This graph compares the set of parses repre-sented by the n-best algorithm used in our model to theset of parses represented by the beam search algorithmused by the past work.
Note that our n-best algorithmrepresents 100% of the top 10000 parses.is a small amount of training data.
These gains de-crease as the amount of training data increases be-cause the additional data allows the baseline to learnnew lexical entries for every special case.
This re-duces the need for the fine grained lexicon decom-position which is enabled by our DFA based unifica-tion.
For example, our DFA based model will learnseparate lexical entries for ?line?, ?word?, ?startingwith?, and ?ending with?.
The baseline instead willjust learn separate lexical entries for every possiblecombination such as ?line starting with?, ?word end-ing with?, etc.
Our model?s ability to decompose,however, allows it to provide equivalent accuracy toeven the best baseline with less than half the amountof training data.
Furthermore, we would expect thisgain to be even larger for domains with more com-plex mappings and a larger number of different com-binations.8.2 Beam Search vs. N-BestA critical step in the training process is calculatingthe expected feature counts over all parses that gen-erate the correct regular expression.
In ?4 we dis-cussed the trade-off between approximating this cal-culation using the n-best parses, as our model does,verses the beam search model used by the past work.The effect of this trade-off can be seen clearly in Fig-ure 6.
The n-best parser always represents the n-bestparses, which is set to 10,000 in our experiments.
Incontrast, on the first iteration, the beam search algo-rithm fails to represent the top parse almost 20% ofthe time and represents less than 15% of the 10,000most likely parses.
Even after 10 iterations it stillonly represents 70% of the top parses and fails torepresent the top parse almost 10% of the time.
Thisdifference in representation ability is what providesthe more than 30% difference in accuracy betweenthe BeamParse-HeuristicUnify version of our modeland the NBestParse-HeuristicUnify version of ourmodel.9 Conclusions and Future WorkIn this paper, we present a technique for learninga probabilistic CCG which can parse a natural lan-guage text search into the regular expression thatperforms that search.
The key idea behind our ap-proach is to use a DFA based form of semantic uni-fication to disambiguate the meaning of the naturallanguage descriptions.
Experiments on a dataset ofnatural language regular expression pairs show thatour model significantly outperforms baselines basedon a state-of-the-art model.We performed our work on the domain of reg-ular expressions, for which semantic unification istractable.
In more general domains, semantic uni-fication is undecidable.
Nevertheless, we believeour work motivates the use of semantic inferencetechniques for language grounding in more generaldomains, potentially through the use of some formof approximation or by restricting those domains insome way.
For example, SAT and SMT solvers haveseen significant success in performing semantic in-ference for program induction and hardware veri-fication despite the computational intractability ofthese problems in the general case.10 AcknowledgmentsThe authors acknowledge the support of BattelleMemorial Institute (PO#300662) and NSF (grantIIS-0835652).
We thank Luke Zettlemoyer, TomKwiatkowski, Yoav Artzi, Mirella Lapata, the MITNLP group, and the ACL reviewers for their sugges-tions and comments.
Any opinions, findings, con-clusions, or recommendations expressed in this pa-per are those of the authors, and do not necessarilyreflect the views of the funding organizations.834ReferencesDana Angluin.
1987.
Learning regular sets from queriesand counterexamples.
Information and computation,75(2):87?106.Yoav Artzi and Luke Zettlemoyer.
2011.
Bootstrappingsemantic parsers from conversations.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing, pages 421?432.
Association forComputational Linguistics.Yoav Artzi and Luke Zettlemoyer.
2013.
Weakly su-pervised learning of semantic parsers for mapping in-structions to actions.
Transactions of the Associationfor Computational Linguistics.S.R.K Branavan, Harr Chen, Luke Zettlemoyer, andRegina Barzilay.
2009.
Reinforcement learning formapping instructions to actions.
In Proceedings ofACL, pages 82?90.David L Chen and Raymond J Mooney.
2011.
Learn-ing to interpret natural language navigation instruc-tions from observations.
In Proceedings of the 25thAAAI Conference on Artificial Intelligence (AAAI-2011), pages 859?865.J.
Clarke, D. Goldwasser, M.W.
Chang, and D. Roth.2010.
Driving semantic parsing from the world?s re-sponse.
In Proceedings of the Fourteenth Conferenceon Computational Natural Language Learning, pages18?27.
Association for Computational Linguistics.Jeffrey Friedl.
2006.
Mastering Regular Expressions.OReilly.Steven I Gallant.
1990.
Perceptron-based learning al-gorithms.
Neural Networks, IEEE Transactions on,1(2):179?191.J.E.
Hopcroft, R. Motwani, and J.D.
Ullman.
1979.
In-troduction to automata theory, languages, and compu-tation, volume 2.
Addison-wesley Reading, MA.Liang Huang and David Chiang.
2005.
Better k-bestparsing.
In Proceedings of the Ninth InternationalWorkshop on Parsing Technology, pages 53?64.
As-sociation for Computational Linguistics.Victor M. Jimenez and Andres Marzal.
2000.
Com-putation of the n best parse trees for weighted andstochastic context-free grammars.
Advances in Pat-tern Recognition, pages 183?192.R.J.
Kate and R.J. Mooney.
2006.
Using string-kernelsfor learning semantic parsers.
In ASSOCIATION FORCOMPUTATIONAL LINGUISTICS, volume 44, page913.R.J.
Kate, Y.W.
Wong, and R.J. Mooney.
2005.
Learningto transform natural to formal languages.
In Proceed-ings of the National Conference on Artificial Intelli-gence, volume 20, page 1062.
Menlo Park, CA; Cam-bridge, MA; London; AAAI Press; MIT Press; 1999.Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwater,and Mark Steedman.
2010.
Inducing probabilistic ccggrammars from logical form with higher-order unifica-tion.
In Proceedings of EMNLP.T.
Kwiatkowski, L. Zettlemoyer, S. Goldwater, andM.
Steedman.
2011.
Lexical generalization in ccggrammar induction for semantic parsing.
In Proceed-ings of the Conference on Empirical Methods in Natu-ral Language Processing, pages 1512?1523.
Associa-tion for Computational Linguistics.Percy Liang, Michael I. Jordan, and Dan Klein.
2009.Learning semantic correspondences with less supervi-sion.
In Proceedings of ACL, pages 91?99.P.
Liang, M.I.
Jordan, and D. Klein.
2011.
Learningdependency-based compositional semantics.
Compu-tational Linguistics, pages 1?94.R.
Mihalcea, H. Liu, and H. Lieberman.
2006.
Nlp (natu-ral language processing) for nlp (natural language pro-gramming).
Computational Linguistics and IntelligentText Processing, pages 319?330.Anders M?ller.
2010. dk.brics.automaton ?
finite-state automata and regular expressions for Java.http://www.brics.dk/automaton/.N.
Moreira and R. Reis.
2012.
Implementation and ap-plication of automata.oDesk.
2013. http://odesk.com/.H.
Poon and P. Domingos.
2009.
Unsupervised seman-tic parsing.
In Proceedings of the 2009 Conferenceon Empirical Methods in Natural Language Process-ing: Volume 1-Volume 1, pages 1?10.
Association forComputational Linguistics.Aarne Ranta.
1998.
A multilingual natural-languageinterface to regular expressions.
In Proceedings ofthe International Workshop on Finite State Methods inNatural Language Processing, pages 79?90.
Associa-tion for Computational Linguistics.R.G.
Raymond and J. Mooney.
2006.
Discriminativereranking for semantic parsing.
In Proceedings ofthe COLING/ACL on Main conference poster sessions,pages 263?270.
Association for Computational Lin-guistics.M.
Steedman.
2001.
The syntactic process.
MIT press.D.
Tabakov and M. Vardi.
2005.
Experimental evalua-tion of classical automata constructions.
In Logic forProgramming, Artificial Intelligence, and Reasoning,pages 396?411.
Springer.C.A.
Thompson and R.J. Mooney.
2003.
Acquiringword-meaning mappings for natural language inter-faces.
Journal of Artificial Intelligence Research,18(1):1?44.Mechanical Turk.
2013. http://mturk.com/.Y.W.
Wong and R.J. Mooney.
2006.
Learning for se-mantic parsing with statistical machine translation.
In835Proceedings of the main conference on Human Lan-guage Technology Conference of the North AmericanChapter of the Association of Computational Linguis-tics, pages 439?446.
Association for ComputationalLinguistics.Y.W.
Wong and R. Mooney.
2007.
Learning syn-chronous grammars for semantic parsing with lambdacalculus.
In ANNUAL MEETING-ASSOCIATIONFOR COMPUTATIONAL LINGUISTICS, volume 45,page 960.J.M.
Zelle and R.J. Mooney.
1996.
Learning to parsedatabase queries using inductive logic programming.In Proceedings of the National Conference on Artifi-cial Intelligence, pages 1050?1055.L.S.
Zettlemoyer and M. Collins.
2005.
Learning tomap sentences to logical form: Structured classifica-tion with probabilistic categorial grammars.L.S.
Zettlemoyer and M. Collins.
2007.
Online learningof relaxed ccg grammars for parsing to logical form.In In Proceedings of the 2007 Joint Conference on Em-pirical Methods in Natural Language Processing andComputational Natural Language Learning (EMNLP-CoNLL-2007.
Citeseer.L.S.
Zettlemoyer and M. Collins.
2009.
Learningcontext-dependent mappings from sentences to logi-cal form.
In Proceedings of the Joint Conference ofthe 47th Annual Meeting of the ACL and the 4th Inter-national Joint Conference on Natural Language Pro-cessing of the AFNLP: Volume 2-Volume 2, pages 976?984.
Association for Computational Linguistics.836
