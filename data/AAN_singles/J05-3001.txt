Squibs and DiscussionsEvaluating Discourse and DialogueCoding SchemesRichard Craggs?University of ManchesterMary McGee Wood?University of ManchesterAgreement statistics play an important role in the evaluation of coding schemes for discourse anddialogue.
Unfortunately there is a lack of understanding regarding appropriate agreement mea-sures and how their results should be interpreted.
In this article we describe the role of agreementmeasures and argue that only chance-corrected measures that assume a common distribution oflabels for all coders are suitable for measuring agreement in reliability studies.
We then providerecommendations for how reliability should be inferred from the results of agreement statistics.Since Jean Carletta (1996) exposed computational linguists to the desirability of usingchance-corrected agreement statistics to infer the reliability of data generated by apply-ing coding schemes, there has been a general acceptance of their use within the field.However, there are prevailing misunderstandings concerning agreement statistics andthe meaning of reliability.Investigation of new dialogue types and genres has been shown to reveal newphenomena in dialogue that are ill suited to annotation by current methods and alsonew annotation schemes that are qualitatively different from those commonly usedin dialogue analysis.
Previously prescribed practices for evaluating coding schemesbecome less applicable as annotation schemes become more sophisticated.
To compen-sate, we need a greater understanding of reliability statistics and how they should beinterpreted.
In this article we discuss the purpose of reliability testing, address certainmisunderstandings, and make recommendations regarding the way in which codingschemes should be evaluated.1.
Agreement, Reliability, and Coding SchemesAfter developing schemes for annotating discourse or dialogue, it is necessary toassess their suitability for the purpose for which they are designed.
Although nostatistical test can determine whether any form of annotation is worthwhile or howapplications will benefit from it, we at least need to show that coders are capable ofperforming the annotation.
This often means assessing reliability based on agreementbetween annotators applying the scheme.
Agreement measures are discussed in detailin section 2.Much of the confusion regarding which agreement measures to apply and how theirresults should be interpreted stems from a lack of understanding of what it means to?
School of Computer Science, University of Manchester, Manchester, M13 9PL, U.K.E-mail: richard craggs@yahoo.co.uk; mary mcgee.wood@manchester.ac.uk.?
2005 Association for Computational LinguisticsComputational Linguistics Volume 31, Number 3assess reliability.
For example, the coding manual for the Switchboard DAMSLdialogue act annotation scheme (Jurafsky, Shriberg, and Biasca 1997, page 2) states thatkappa is used to ?assess labelling accuracy,?
and Di Eugenio and Glass (2004) relatereliability to ?the objectivity of decisions,?
whereas Carletta (1996) regards reliability asthe degree to which we understand the judgments that annotators are asked to make.Although most researchers recognize that reporting agreement statistics is an importantpart of evaluating coding schemes, there is frequently a lack of understanding of whatthe figures actually mean.The intended meaning of reliability should refer to the degree to which the datagenerated by coders applying a scheme can be relied upon.
If we consider the codingprocess to involve mapping units of analysis onto categories, data are reliable if codersagree on the category onto which each unit should be mapped.
The further from perfectagreement that coders stray, the less we can rely on the resulting annotation.If data produced by applying a scheme are shown to be reliable, then we haveestablished two important properties of those data:1.
The categories onto which the units are mapped are not inordinatelydependent on the idiosyncratic judgments of any individual coder.2.
There is a shared understanding of the meaning of the categories and howdata are mapped onto them.The first of these is important for ensuring the reproducibility of the coding.
To be able totrust the analysis of annotated corpora, we need to be confident that the categorizationof the units of data is not dependent on which individual performed the annotation.
Thesecond governs the value of data resulting from the coding process.
For an annotatedcorpus or the analysis thereof to be valuable, the phenomenon being annotated mustrepresent some notion in which we can enjoy a shared understanding.2.
Agreement MeasuresThere are many ways in which the level of agreement between coders can be evaluated,and the choice of which to apply in order to assess reliability is the source of much con-fusion.
An appropriate statistic for this purpose must measure agreement as a functionof the coding process and not of the coders, data, or categories.
Only if the results ofa test are solely dependent on the degree to which there is a shared understanding ofhow the phenomena to be described are mapped to the given categories can we infer thereliability of the resulting data.
Some agreement measures do not behave in this mannerand are therefore unsuitable for evaluating reliability.A great deal of importance is placed on domain specificity in discourse and dialoguestudies and as such, researchers are often encouraged to evaluate schemes using corporafrom more than one domain.
Concerning agreement, this encouragement is misplaced.Since an appropriate agreement measure is a function of only the coding process, if theoriginal agreement test is performed in a scientifically sound manner, little more canbe proved by applying it again to different data.
Any differences in the results betweencorpora are a function of the variance between samples and not of the reliability of thecoding scheme.Di Eugenio and Glass (2004) identify three general classes of agreement statisticsand suggest that all three should be used in conjunction in order to accurately evaluatecoding schemes.
However, this suggestion is founded on some misunderstandings of290Craggs and Wood Evaluating Discourse and Dialogue Coding Schemesthe role of agreement measure in reliability studies.
We shall now rectify these andconclude that only one class of agreement measure is suitable.2.1 Percentage AgreementThe first of the recommended agreement tests, percentage agreement, measures theproportion of agreements between coders.
This is an unsuitable measure for inferringreliability, and it was the use of this measure that prompted Carletta (1996) to recom-mend chance-corrected measures.Percentage agreement is inappropriate for inferring reliability because it excludesany notion of the level of agreement that we could expect to achieve by chance.
Reliabil-ity should be inferred by locating the achieved level of agreement on a scale between thebest possible (coders agree perfectly) and the worst possible (coders do not understandor cannot perform the mapping and behave randomly).
Without any indication of theagreement that coders would achieve by behaving randomly, any deviation from perfectagreement is uninterpretable (Krippendorff 2004b).The justification given for using percentage agreement is that it does not suffer fromwhat Di Eugenio and Glass (2004) referred to as the ?prevalence problem.?
Prevalencerefers to the unequal distribution of label use by coders.
For example, Table 1 showsan example taken from Di Eugenio and Glass (2004) showing the classification of theutterance Okay as an acceptance or acknowledgment.
It represents a confusion matrixdescribing the number of occasions that coders used pairs of labels for a given turn.
Thistable shows that the two coders favored the use of accept strongly over acknowledge.
Theycorrectly state that this skew in the distribution of categories increases the expectedchance agreement, thus lowering the overall agreement in chance-corrected tests.
Thereason for this is that since one category is more popular than others, the likelihood ofcoders?
agreeing by chance by choosing this category increases.
We therefore require acomparable increase in observed agreement to accommodate this.Di Eugenio and Glass (2004) perceive this as an ?unpleasant behavior?
of chance-corrected tests, one that prevents us from concluding that the example given in Table 1shows satisfactory levels of agreement.
Instead they use percentage agreement toarrive at this conclusion.
By examining the data, it is clear that this conclusion wouldbe false.In Table 1, the coders agree 90 out of 100 times, but all agreements occur when bothcoders choose accept.
There is not a single case in which they agree on Okay?s being usedas an acknowledgment.
The only conclusion one may justifiably draw is that the coderscannot distinguish the use of Okay as an acceptance from its use as an acknowledgment.Rather than being an unpleasant behavior, accounting for prevalence in the data is anTable 1Prevalence in coding.Coder 2Coder 1 Accept AckAccept 90 5 95Acknowledge 5 0 595 5 100291Computational Linguistics Volume 31, Number 3important part of accurately reporting the level of agreement.
This helps us to avoidarriving at incorrect conclusions such as believing that the data shown in Table 1 suggestreliable coding.2.2 Chance-Corrected Agreement: Unequal Coder Category DistributionThe second class of agreement measure recommended in Di Eugenio and Glass (2004)is that of chance-corrected tests that do not assume an equal distribution of categoriesbetween coders.
Chance-corrected tests compute agreement according to the ratio ofobserved (dis)agreement to that which we could expect by chance, estimated from thedata.
The measures differ in the way in which this expected (dis)agreement is estimated.Those that do not assume an equal distribution between coders calculate expected(dis)agreement based on the individual distribution of each coder.The concern that in discourse and dialogue coding, coders will differ in the fre-quency with which they apply labels leads Di Eugenio and Glass to conclude thatCohen?s (1960) kappa is the best chance-corrected test to apply.
To clarify, by unequaldistribution of categories, we do not refer to the disparity in the frequency with whichcategories occur (e.g., verbs are more common than pronouns) but rather to the differ-ence in proclivity between coders (e.g., coder A is more likely to label something a nounthan coder B).Cohen?s kappa calculates expected chance agreement, based on the individualcoders?
distributions, in a manner similar to association measures, such as chi?square.This means that its results are dependent on the preferences of the individual coderstaking part in the tests.
This violates the condition set out at the beginning of this sectionwhereby agreement must be a function of the coding process, with coders being viewedas interchangeable.
The purpose of assessing the reliability of coding schemes is not tojudge the performance of the small number of individuals participating in the trial, butrather to predict the performance of the schemes in general.
The proposal that in mostdiscourse and dialogue studies, the assumption of equal distribution between codersdoes not hold is, in fact, an argument against the use of Cohen?s kappa.
Assessing theagreement between coders and accounting for their idiosyncratic proclivity toward oragainst certain labels tells us little about how the coding scheme will perform when ap-plied by others.
The solution is not to apply a test that panders to individual differences,but rather to increase the number of coders so that the influence of any individual onthe final result becomes less pronounced.1Another reason provided for using Cohen?s kappa is that its sensitivity to bias (dif-ferences in coders?
category distribution) can be exploited to improve coding schemes.However, there is no need to calculate kappa in order to observe bias, since it willbe evident in a contingency table of the data in question.
Even if it were necessary tocompute kappa for this purpose, however, this would not justify its use as a reliabilitytest.2.3 Chance-Corrected Agreement: Assumed Equal Coder Category DistributionThe remaining class of agreement measure assumes an equal distribution of categoriesfor all coders.
Once we have accepted that this assumption is necessary in order to1 When there is a single correct label that should be used, such as part-of-speech tags used to describe thesyntactic function of a word or group of words, then training coders may mitigate coder preference.292Craggs and Wood Evaluating Discourse and Dialogue Coding Schemespredict the performance of the scheme in general, there appears to be no objection tousing this type of statistical test for assessing agreement in discourse and dialogue work.Tests that fall into this class include Siegel and Castellan?s (1988) extension of Scott?s(1955) pi, confusingly called kappa, and Krippendorff?s (2004a) alpha.
Both of thesemeasures calculate expected (dis)agreement based on the frequency with which eachcategory is used, estimated from the overall usage by the coders.Kappa is more frequently described in statistics textbooks and more commonlyimplemented in statistical software.
In circumstances in which mechanisms other thannominal labels are used to annotate data, alpha has the benefit of being able to deal withdifferent degrees of disagreement between pairs of interval, ordinal, and ratio values,among others.Di Eugenio and Glass (2004) conclude with the proposal that these three forms ofagreement measure collectively provide better means with which to judge agreementthan any individual test.
We would argue, to the contrary, that applying three differentmetrics to measure the same property suggests a lack of confidence in any of them.Percentage agreement and Cohen?s kappa do not provide an insight into a scheme?sreliability, so reporting their results is potentially misleading.3.
Inferring ReliabilityTo reiterate, when testing reliability we are assessing whether the data that a schemegenerates can be relied on.
This may be inferred from the level of agreement betweencoders applying the scheme.
In section 1 we described two properties of reliable datathat are important to establish in discourse and dialogue analysis.
In this section weexplain how the gap between agreement and reliability may be bridged.When inferring reliability from agreement, a common error is to believe that thereare a number of thresholds against which agreement scores can be measured in order togauge whether or not a coding scheme produces reliable data.
Most commonly thisis Krippendorff?s decision criterion, in which scores greater than 0.8 are consideredsatisfactory and scores greater than 0.667 allow tentative conclusions to be drawn(Krippendorff 2004a).
The prevalent use of this criterion despite repeated advice thatit is unlikely to be suitable for all studies (Carletta 1996; Di Eugenio and Glass 2004;Krippendorff 2004a) is probably due to a desire for a simple system that can be easilyapplied to a scheme.
Unfortunately, because of the diversity of both the phenomenabeing coded and the applications of the results, it is impossible to prescribe a scaleagainst which all coding schemes can be judged.Instead we provide discussion and some recommendations, all founded on thepremise that reliability must ?correlate with the conditions under which one is willing torely on imperfect data?
(Krippendorff 2004b, page 6).
A common concern regarding theapplication of standards from other fields, such as the one described above, to discourseand dialogue research is that the subjectivity of the phenomena being coded maymean that we never obtain the necessary agreement levels.
In this context, subjectivitydescribes the absence of an obvious mapping for each unit of analysis onto categoriesthat describe the phenomenon in question.
However, the fact that we consider thesesubjective phenomena worthy of study shows that we are, in fact, ?willing to relyon imperfect data,?
which is fine as long as we recognize the limitations of a schemethat delivers less-than-ideal levels of reliability and use the resulting annotated corporaaccordingly.In order to discuss the acceptable levels of agreement for discourse and dialoguecoding, let us consider two popular uses of coded data: to train systems to perform293Computational Linguistics Volume 31, Number 3some automated task and to study the relationship between the coded phenomena andsome other feature of the data.3.1 Reliability and Training for Automatic AnnotationConsidering the effort involved in manually annotating linguistic data, it is un-surprising that attempts are often made to train a system to perform such annotationautomatically (Mast et al 1996; Wrede and Shriberg 2003).
The reliability of manuallyannotated data is clearly a concern when they are used to train a system.
If the levelof agreement for the annotation scheme is low, then the system is going to replicatethe inconsistent behavior of human annotators.
Any deviant behavior by the systemresulting in less than 100% accuracy in comparison with the manual annotation willcompound the problem, possibly leading to meaningless data.
Worse still, if a systemis to learn how to annotate from manually annotated data, it will do so based on thepatterns observed in those data.
If the manual annotation is not reliable, then thosepatterns may be nonexistent or misleading.Returning to our original premise, we would suggest that if a coding scheme is tobe used to generate data from which a system will learn to perform similar coding, thenwe should be ?unwilling to rely on imperfect data.
?3.2 Reliability and Corpus AnalysisManually annotated corpora can also be used to infer a relationship between the phe-nomena in question and some other facet of the data.
When performing this sort ofanalysis, we may be more willing to work with imperfect data and therefore acceptlower levels of agreement.
However, the conclusions that are gleaned from the analysismust be tempered according to the level of agreement achieved.
For example, when it issuggested that a correlation exists between the occurrence of one phenomenon and thatof another, less agreement observed in the sample annotation requires stronger evidenceof the correlation in order for the conclusion to be valid.To summarize, there are no magic thresholds that, once crossed, entitle us to claimthat a coding scheme is reliable.
One must decide for oneself, based on the intended useof a scheme, whether the observed level of agreement is sufficient and conduct one?sanalysis accordingly.4.
ConclusionThe application of agreement statistics has done much to improve the scientific rigorof discourse and dialogue research.
However, unless we understand what we areattempting to prove and which tests are appropriate, the results of evaluation can beunsatisfactory or, worse still, misleading.
In this article we have encouraged researchersto clarify their reasons for assessing agreement and have suggested that in many casesthe most suitable test for this purpose is one that corrects for expected agreement, basedon an assumed equal distribution between coders.AcknowledgmentsThe authors acknowledge the help of KlausKrippendorff for patiently aiding ourunderstanding of reliability and Universityof Manchester for funding Richard Craggs.ReferencesCarletta, Jean.
1996.
Assessing agreementon classification tasks: The kappastatistic.
Computational Linguistics,22(2):249?254.294Craggs and Wood Evaluating Discourse and Dialogue Coding SchemesCohen, J.
1960.
A coefficient of agreement fornominal scales.
Education and PsychologicalMeasurement, 43(6):37?46.Di Eugenio, Barbara and Michael Glass.2004.
The kappa statistic: A second look.Computational Linguistics, 30(1):95?101.Jurafsky, Daniel, Elizabeth Shriberg, andDebra Biasca.
1997.
SwitchboardSWBD-DAMSL shallow-discourse-functionannotation coders manual.
Technical Report(Draft 13), University of Colorado.Krippendorff, Klaus.
2004a.
Content Analysis:An Introduction to Its Methodology.
2nd ed.Sage, Beverly Hills, CA.Krippendorff, Klaus.
2004b.
Reliability incontent analysis: Some commonmisconceptions and recommendations.Human Communication Research,30(3):411?437.Mast, Marion, Heinrich Niemann, ElmarNoth, and Ernst GunterSchukat-Talamazzini.
1996.
Automaticclassification of dialog acts with semanticclassification trees and polygrams.
InLearning for Natural Language Processing,edited by Stefan Wermter, Ellen Riloff,and Gabriele Scheler.
Springer,pages 217?229.Scott, W. A.
1955.
Reliability of contentanalysis: The case of nominal scale coding.Public Opinion Quarterly, 19:127?141.Siegel, Sidney and John N. Castellan, Jr. 1988.Nonparametric Statistics.
2nd ed.McGraw-Hill.Wrede, Britta and Elizabeth Shriberg.
2003.Spotting ?hot spots?
in meetings: Humanjudgments and prosodic cues.
InProceedings of EUROSPEECH, Geneva.295
