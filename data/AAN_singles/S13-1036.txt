Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conferenceand the Shared Task, pages 248?253, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational LinguisticsUnsupervised Word Usage Similarity in Social Media TextsSpandana Gella,?
Paul Cook,?
and Bo Han???
NICTA Victoria Research Laboratory?
Department of Computing and Information Systems, The University of Melbournesgella@student.unimelb.edu.au, paulcook@unimelb.edu.au,hanb@student.unimelb.edu.auAbstractWe propose an unsupervised method for au-tomatically calculating word usage similar-ity in social media data based on topic mod-elling, which we contrast with a baseline dis-tributional method and Weighted Textual Ma-trix Factorization.
We evaluate these meth-ods against a novel dataset made up of humanratings over 550 Twitter message pairs anno-tated for usage similarity for a set of 10 nouns.The results show that our topic modelling ap-proach outperforms the other two methods.1 IntroductionIn recent years, with the growing popularity of so-cial media applications, there has been a steep risein the amount of ?post?-based user-generated text(including microblog posts, status updates and com-ments) (Bennett, 2012).
This data has been iden-tified as having potential for applications rangingfrom trend analysis (Lau et al 2012a) and event de-tection (Osborne et al 2012) to election outcomeprediction (O?Connor et al 2010).
However, giventhat posts are generally very short, noisy and lack-ing in context, traditional NLP approaches tend toperform poorly over social media data (Hong andDavison, 2010; Ritter et al 2011; Han et al 2012).This is the first paper to address the task of lexi-cal semantic interpretation in microblog data basedon word usage similarity.
Word usage similar-ity (USIM: Erk et al(2009)) is a relatively newparadigm for capturing similarity in the usages ofa given word independently of any lexicon or senseinventory.
The task is to rate on an ordinal scale thesimilarity in usage between two different usages ofthe same word.
In doing so, it avoids common issuesin conventional word sense disambiguation, relatingto sense underspecification, the appropriateness of astatic sense inventory to a given domain, and the in-ability to capture similarities/overlaps between wordsenses.
As an example of USIM, consider the fol-lowing pairing of Twitter posts containing the targetword paper:1.
Deportation of Afghan Asylum Seekers fromAustralia : This paper aims to critically evalu-ate a newly signed agree.2.
@USER has his number on a piece of paperand I walkd off!The task is to predict a real-valued number in therange [1, 5] for the similarity in the respective us-ages of paper, where 1 indicates the usages are com-pletely different and 5 indicates they are identical.In this paper we develop a new USIM datasetbased on Twitter data.
In experiments on this datasetwe demonstrate that an LDA-based topic modellingapproach outperforms a baseline distributional se-mantic approach and Weighted Textual Matrix Fac-torization (WTMF: Guo and Diab (2012a)).
Wefurther show that context expansion using a novelhashtag-based strategy improves both the LDA-based method and WTMF.2 Related WorkWord sense disambiguation (WSD) is the task ofdetermining the particular sense of a word from agiven set of pre-defined senses (Navigli, 2009).
It248contrasts with word sense induction (WSI), wherethe senses of a given target word are induced froman unannotated corpus of usages, and the inducedsenses are then used to disambiguate each token us-age of the word (Manandhar et al 2010; Lau etal., 2012b).
WSD and WSI have been the predomi-nant paradigms for capturing and evaluating lexicalsemantics, and both assume that each usage corre-sponds to exactly one of a set of discrete senses ofthe target word, and that any prediction other thanthe ?correct?
sense is equally wrong.Erk et al(2009) showed that, given a sense in-ventory, there is a high likelihood of multiple sensesbeing compatible with a given usage, and proposedUSIM as a means of capturing the similarity in us-age between a pairing of usages of a given word.As part of their work, they released a dataset, whichLui et al(2012) recently developed a topic mod-elling approach over.
Based on extensive experi-mentation, they demonstrated the best results witha single topic model for all target words based onfull document context.
Our topic modelling-basedapproach to USIM builds off the approach of Luiet al(2012).
Guo and Diab (2012a) observed that,when applied to short texts, the effectiveness of la-tent semantic approaches can be boosted by expand-ing the text to include ?missing?
words.
Based onthis, they proposed Weighted Textual Matrix Factor-ization (WTMF), based on weighted matrix factor-ization (Srebro and Jaakkola, 2003).
Here we ex-periment with both LDA based topic modeling andWTMF to estimate word similarities in twitter data.LDA based topic modeling has been earlier studiedon Twitter data for tweet classification (Ramage etal., 2010) and tweet clustering (Jin et al 2011).3 Data PreparationThis section describes the construction of the USIM-tweet dataset based on microblog posts (?tweets?
)from Twitter.
We describe the pre-processing stepstaken to sample the tweets in our datasets, outlinethe annotation process, and then describe the back-ground corpora used in our experiments.3.1 Data preprocessingAround half of Twitter is non-English (Hong et al2011), so our first step was to automatically identifyEnglish tweets using langid.py (Lui and Bald-win, 2012).
We next performed lexical normaliza-tion using the dictionary of Han et al(2012) to con-vert lexical variants (e.g., tmrw) to their standardforms (e.g., tomorrow) and reduce data sparseness.As our target words, we chose the 10 nouns fromthe original USIM dataset of Erk et al(2009) (bar,charge, execution, field, figure, function, investiga-tor, match, paper, post), and identified tweets con-taining the target words as nouns using the CMUTwitter POS tagger (Owoputi et al 2012).3.2 Annotation Settings and DataTo collect word usage similarity scores for Twittermessage pairs, we used a setup similar to that ofErk et al(2009) using Amazon Mechanical Turk:we asked the annotators to rate each sentence pairwith an integer score in the range [1, 5] using sim-ilar annotation guidelines to Erk et alWe ran-domly sampled twitter messages from the TREC2011 microblog dataset,1 and for each of our 10nouns, we collected 55 pairs of messages satisfyingthe preprocessing described in Section 3.1.
These55 pairs are chosen such that each tweet has at least4 content words (nouns, verbs, adjectives and ad-verbs) and at least 70+% of its post-normalized to-kens in the Aspell dictionary (v6.06)2; these restric-tions were included in an effort to ensure the tweetswould contain sufficient linguistic content to be in-terpretable.3 We created 110 Mechanical Turk jobs(referred to as HITs), with each HIT containing 5randomly-selected message pairs.
For this annota-tion the tweets were presented in their original form,i.e., without lexical normalisation applied.
Each HITwas completed by 10 ?turkers?, resulting in a totalof 5500 annotations.
The annotation was restrictedto turkers based in the United States having had atleast 95% of their previous HITs accepted.
In total,the annotation was carried out by 68 turkers, eachcompleting between 1 and 100 HITs.To detect outlier annotators, we calculated the av-erage Spearman correlation score (?)
of every an-notator by correlating their annotation values withevery other annotator and taking the average.
We1http://trec.nist.gov/data/tweets/2http://aspell.net/3In future analyses we intend to explore the potential impactof these restrictions on the resulting dataset.249Word Orig Exp Word Orig Expbar 180k 186k function 26k 27kcharge 41k 43k investigator 17k 19kexecution 28k 30k field 72k 75kfigure 28k 29k match 126k 133kpaper 210k 218k post 299k 310kTable 1: The number of tweets for each word in eachbackground corpus (?Orig?
= ORIGINAL; ?Exp?= EXPANDED; RANDEXPANDED, not shown, con-tains the same number of tweets as EXPANDED).accepted all the annotations of annotators whose av-erage ?
is greater than 0.6; this corresponded to 95%of the annotators.
Two annotators had a negativeaverage ?
and their annotations (only 4 HITs to-tal) were discarded.
For the other annotators (i.e.,0 ?
?
?
0.6), we accepted each of their HITs ona case by case basis; a HIT was accepted only ifat least 2 out of 5 of the annotations for that HITwere within ?2.0 of the mean for that annotationbased on the judgments of the other turkers.
(21HITS were discarded using this heuristic.)
We fur-ther eliminated 7 HITS which have incomplete judg-ments.
In total only 32 HITs (of the 1100 HITs com-pleted) were discarded through these heuristics.
Theweighted average Spearman correlation over all an-notators after this filtering is 0.681, which is some-what higher than the inter-annotator agreement of0.548 reported by Erk et al(2009).
This dataset isavailable for download.3.3 Background CorpusWe created three background corpora based on datafrom the Twitter Streaming API in February 2012(only tweets satisfying the preprocessing steps inSection 3.1 were chosen).ORIGINAL: 1 million tweets which contain at leastone of the 10 target nouns;EXPANDED: ORIGINAL plus an additional 40ktweets containing at least 1 hashtag attested inORIGINAL with an average frequency of use of10?35 times/hour (medium frequency);RANDEXPANDED: ORIGINAL plus 40k randomlysampled tweets containing the same targetnouns.We select medium-frequency hashtags because low-frequency hashtags tend to be ad hoc and non-thematic in nature, while high-frequency hash-tags are potentially too general to capture us-age similarity.
Statistics for ORIGINAL and EX-PANDED/RANDEXPANDED are shown in Table 1.RANDEXPANDED is sampled such that it has thesame number of tweets as EXPANDED.4 MethodologyWe propose an LDA topic modelling-based ap-proach to the USIM task, which we contrast witha baseline distributional model and WTMF.
In allthese methods, the similarity between two word us-ages is measured using cosine similarity between thevector representation of each word usage.4.1 BaselineWe represent each target word usage in a tweet as asecond-order co-occurrence vector (Schu?tze, 1998).A second-order co-occurrence vector is built fromthe centroid (summation) of all the first-order co-occurrence vectors of the context words in the sametweet as the target word.The first-order co-occurrence vector for a giventarget word represents the frequency with which thatword co-occurs in a tweet with other context words.Each first-order vector is built from all tweets whichcontain a context word and the target word catego-rized as noun in the background corpus, thus sensi-tizing the first-order vector to the target word.
Weuse the most frequent 10000 words (excluding stop-words) in the background corpus as our first-ordervector dimensions/context words.
Context words(dimensions) in the first-order vectors are weightedby mutual information.Second-order co-occurrence is used as the contextrepresentation to reduce the effects of data sparse-ness in the tweets (which cannot be more than 140codepoints in length).4.2 Weighted Textual Matrix FactorizationWTMF (Guo and Diab, 2012b) addresses the datasparsity problem suffered by many latent variable250Model ORIGINAL EXPANDED RANDEXPANDEDBaseline 0.09 0.08 0.09WTMF 0.02 0.09 0.06LDA 0.20 0.29 0.18Table 2: Spearman rank correlation (?)
for eachmethod based on each background corpus.
The bestresult for each corpus is shown in bold.models by predicting ?missing?
words on the ba-sis of the message content, and including them inthe vector representation.
Guo and Diab showedWTMF to outperform LDA on the SemEval-2012semantic textual similarity task (STS) (Agirre et al2012).
The semantic space required for this modelas applied here is built from the background tweetscorresponding to the target word.
We experimentedwith the missing weight parameter wm of WTMFin the range [0.05, 0.01, 0.005, 0.0005] and with di-mensions K=100 and report the best results (wm =0.0005).4.3 Topic ModellingLatent Dirichlet Allocation (LDA) (Blei et al 2003)is a generative model in which a document is mod-eled as a finite mixture of topics, where each topic isrepresented as a multinomial distribution of words.We treat each tweet as a document.
Topics sensi-tive to each target word are generated from its corre-sponding background tweets.
We topic model eachtarget word individually,4 and create a topic vectorfor each word usage based on the topic allocations ofthe context words in that usage.
We use Gibbs sam-pling in Mallet (McCallum, 2002) for training andinference of the LDA model.
We experimented withthe number of topics T for each target word rangingfrom 2 to 500.
We optimized the hyper parametersby choosing those which best fit the data every 20 it-erations over a total of 800 iterations, following 200burn-in iterations.4Unlike Lui et al(2012) we found a single topic model forall target words to perform very poorly.ll lll l lll ll l lll l l2 3 5 8 10 20 30 50 100 150 200 250 300 350 400 450 500T?0.10.00.10.20.3Spearmancorrelation?
l OriginalExpandedRandExpandedFigure 1: Spearman rank correlation (?)
for LDA forvarying numbers of topics (T ) using different back-ground corpora.5 ResultsWe evaluate the above methods for word usage sim-ilarity on the dataset constructed in Section 3.2.
Weevaluate our models against the mean human ratingsusing Spearman?s rank correlation.
Table 2 presentsresults for each method using each background cor-pus.
The results for LDA are for the optimal set-ting for T (8, 5, and 20 for ORIGINAL, EXPANDED,and RANDEXPANDED, respectively).
LDA is su-perior to both the baseline and WTMF using eachbackground corpus.
The performance of LDA im-proves for EXPANDED but not RANDEXPANDED,over ORIGINAL, demonstrating the effectiveness ofour hashtag based corpus expansion strategy.In Figure 1 we plot the rank correlation of LDAacross all words against the number of topics (T ).As the number of topics increases beyond a certainnumber, the rank correlation decreases.
LDA trainedon EXPANDED consistently outperforms ORIGINALand RANDEXPANDED for lower values of T (i.e.,T <= 20).In Table 3, we show results for LDA over each tar-get word, for ORIGINAL and EXPANDED.
(Resultsfor RANDEXPANDED are not shown but are similarto ORIGINAL.)
Results are shown for the optimalT for each lemma, and the optimal T over all lem-mas.
Optimizing T for each lemma gives an indica-tion of the upperbound of the performance of LDA,and unsurprisingly gives better performance than us-251LemmaORIGINAL EXPANDEDPer lemma Global Per lemma Global?
(T ) ?
(T=8) ?
(T ) ?
(T=5)bar 0.39 (10) 0.28 0.35 (50) 0.1charge 0.27 (30) 0.04 0.33 (20) ?0.08execution 0.43 (8) 0.43 0.58 (5) 0.58field 0.46 (5) 0.33 0.53 (10) 0.32figure 0.24 (150) 0.06 0.24 (250) 0.14function 0.44 (8) 0.44 0.40 (10) 0.27investigator 0.3 (30) 0.05 0.50 (5) 0.50match 0.28 (5) 0.26 0.45 (5) 0.45paper 0.29 (30) 0.20 0.32 (30) 0.22post 0.1 (3) ?0.13 0.2 (30) ?0.01Table 3: Spearman?s ?
using LDA for the optimal Tfor each lemma (Per lemma) and the best T over alllemmas (Global) using ORIGINAL and EXPANDED.?
values that are significant at the 0.05 level areshown in bold.ing a fixed T for all lemmas.
This suggests that ap-proaches that learn an appropriate number of topics(e.g., HDP, (Teh et al 2006)) could give further im-provements; however, given the size of the dataset,the computational cost of HDP could be a limitation.Contrasting our results with a fixed number oftopics to those of Lui et al(2012), our highest rankcorrelation of 0.29 (T = 5 using EXPANDED) ishigher than the 0.11 they achieved over the origi-nal USIM dataset (where the documents offer an or-der of magnitude more context).
The higher inter-annotator agreement for USIM-tweet compared tothe original USIM dataset (Section 3.2), combinedwith this finding, demonstrates that USIM over mi-croblog data is indeed a viable task.Returning to the performance of LDA relativeto WTMF in Table 2, the poor performance ofWTMF is somewhat surprising here given WTMF?sencouraging performance on the somewhat similarSemEval-2012 STS task.
This difference is possi-bly due to the differences in the tasks: usage simi-larity measures the similarity of the usage of a tar-get word while STS measures the similarity of twotexts.
Differences in domain ?
i.e., Twitter hereand more standard text for STS ?
could also be afactor.
WTMF attempts to alleviate the data spar-sity problem by adding information from ?missing?words in a text by assigning a small weight to thesemissing words.
Because of the prevalence of lexicalvariation on Twitter, some missing words might becounted multiple times (e.g., coool, kool, and kewlall meaning roughly cool) thus indirectly assigninghigher weights to the missing words leading to thelower performance of WTMF compared to LDA.6 SummaryWe have analysed word usage similarity in mi-croblog data.
We developed a new dataset (USIM-tweet) for usage similarity of nouns over Twitter.We applied a topic modelling approach to this task,and contrasted it with baseline and benchmark meth-ods.
Our results show that the LDA-based approachoutperforms the other methods over microblog data.Moreover, our novel hashtag-based corpus expan-sion strategy substantially improves the results.In future work, we plan to expand our annotateddataset, experiment with larger background corpora,and explore alternative corpus expansion strategies.We also intend to further analyse the difference inperformance LDA and WTMF on similar data.AcknowledgementsWe are very grateful to Timothy Baldwin for histremendous help with this work.
We additionallythank Diana McCarthy for her insightful commentson this paper.
We also acknowledge the EuropeanErasmus Mundus Masters Program in Language andCommunication Technologies from the EuropeanCommission.NICTA is funded by the Australian governmentas represented by Department of Broadband, Com-munication and Digital Economy, and the AustralianResearch Council through the ICT Centre of Excel-lence programme.ReferencesEneko Agirre, Daniel Cer, Mona Diab, and AitorGonzalez-Agirre.
2012.
Semeval-2012 task 6: A piloton semantic textual similarity.
In Proceedings of theSixth International Workshop on Semantic Evaluation(SemEval 2012), pages 385?393, Montreal, Canada.Shea Bennett.
2012.
Twitter on track for500 million total users by March, 250 mil-lion active users by end of 2012. http://www.mediabistro.com/alltwitter/twitter-active-total-users_b17655.252David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet alcation.
Journal of MachineLearning Research, 3:993?1022.Katrin Erk, Diana McCarthy, and Nicholas Gaylord.2009.
Investigations on word senses and word usages.In Proceedings of the Joint conference of the 47th An-nual Meeting of the Association for ComputationalLinguistics and the 4th International Joint Conferenceon Natural Language Processing of the Asian Feder-ation of Natural Language Processing (ACL-IJCNLP2009), pages 10?18, Singapore.Weiwei Guo and Mona Diab.
2012a.
Modeling sen-tences in the latent space.
In Proc.
of the 50th AnnualMeeting of the Association for Computational Linguis-tics, pages 864?872, Jeju, Republic of Korea.Weiwei Guo and Mona Diab.
2012b.
Weiwei: A sim-ple unsupervised latent semantics based approach forsentence similarity.
In Proceedings of the First JointConference on Lexical and Computational Semantics(*SEM 2012), pages 586?590, Montreal, Canada.Bo Han, Paul Cook, and Timothy Baldwin.
2012.
Au-tomatically constructing a normalisation dictionary formicroblogs.
In Proceedings of the Joint Conference onEmpirical Methods in Natural Language Processingand Computational Natural Language Learning 2012,pages 421?432, Jeju, Republic of Korea.Liangjie Hong and Brian D Davison.
2010.
Empiricalstudy of topic modeling in twitter.
In Proc.
of the FirstWorkshop on Social Media Analytics, pages 80?88.Lichan Hong, Gregoria Convertino, and Ed H. Chi.
2011.Language matters in Twitter: A large scale study.
InProceedings of the 5th International Conference onWeblogs and Social Media (ICWSM 2011), pages 518?521, Barcelona, Spain.Ou Jin, Nathan N Liu, Kai Zhao, Yong Yu, and QiangYang.
2011.
Transferring topical knowledge fromauxiliary long texts for short text clustering.
In Proc.of the 20th ACM International Conference on Informa-tion and Knowledge Management, pages 775?784.Jey Han Lau, Nigel Collier, and Timothy Baldwin.2012a.
On-line trend analysis with topic models:#twitter trends detection topic model online.
InProceedings of the 24th International Conference onComputational Linguistics (COLING 2012), pages1519?1534, Mumbai, India.Jey Han Lau, Paul Cook, Diana McCarthy, David New-man, and Timothy Baldwin.
2012b.
Word sense in-duction for novel sense detection.
In Proceedingsof the 13th Conference of the European Chapter ofthe Association for Computational Linguistics (EACL2012), pages 591?601, Avignon, France.Marco Lui and Timothy Baldwin.
2012. langid.py: Anoff-the-shelf language identification tool.
In Proceed-ings of the 50th Annual Meeting of the Association forComputational Linguistics (ACL 2012) Demo Session,pages 25?30, Jeju, Republic of Korea.Marco Lui, Timothy Baldwin, and Diana McCarthy.2012.
Unsupervised estimation of word usage simi-larity.
In Proceedings of the Australasian LanguageTechnology Workshop 2012 (ALTW 2012), pages 33?41, Dunedin, New Zealand.Suresh Manandhar, Ioannis Klapaftis, Dmitriy Dligach,and Sameer Pradhan.
2010.
SemEval-2010 Task 14:Word sense induction & disambiguation.
In Proceed-ings of the 5th International Workshop on SemanticEvaluation, pages 63?68, Uppsala, Sweden.Andrew Kachites McCallum.
2002.
Mallet: A machinelearning for language toolkit.
http://mallet.cs.umass.edu.Roberto Navigli.
2009.
Word sense disambiguation: Asurvey.
ACM Computing Surveys, 41(2).Brendan O?Connor, Ramnath Balasubramanyan,Bryan R. Routledge, and Noah A. Smith.
2010.From tweets to polls: Linking text sentiment topublic opinion time series.
In Proceedings of the4th International Conference on Weblogs and SocialMedia, pages 122?129, Washington, USA.Miles Osborne, Sasa Petrovic?, Richard McCreadie, CraigMacdonald, and Iadh Ounis.
2012.
Bieber no more:First story detection using Twitter and Wikipedia.
InProceedings of the SIGIR 2012 Workshop on Time-aware Information Access, Portland, USA.Olutobi Owoputi, Brendan O?Connor, Chris Dyer, KevinGimpel, and Nathan Schneider.
2012.
Part-of-speechtagging for Twitter: Word clusters and other advances.Technical Report CMU-ML-12-107, Carnegie MellonUniversity.Daniel Ramage, Susan Dumais, and Dan Liebling.
2010.Characterizing microblogs with topic models.
In In-ternational AAAI Conference on Weblogs and SocialMedia, volume 5, pages 130?137.Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.2011.
Named entity recognition in tweets: An experi-mental study.
In Proceedings of the 2011 Conferenceon Empirical Methods in Natural Language Process-ing, pages 1524?1534, Edinburgh, UK.Hinrich Schu?tze.
1998.
Automatic word sense discrimi-nation.
Computational Linguistics, 24(1):97?123.Nathan Srebro and Tommi Jaakkola.
2003.
Weightedlow-rank approximations.
In Proceedings of the20th International Conference on Machine Learning,Washington, USA.Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, andDavid M. Blei.
2006.
Hierarchical Dirichlet pro-cesses.
Journal of the American Statistical Associa-tion, 101:1566?1581.253
