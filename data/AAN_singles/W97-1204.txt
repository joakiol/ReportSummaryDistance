I n tegrat ing  Language Generat ion  w i th  Speech  Synthes isConcept  to Speech  SystemShimei Pan and Kathleen R. McKeownDepartment of Computer Science450 Computer Science BuildingColumbia UniversityNew York, N.Y. 10027{pan, kathy} @cs.
columbia, eduAbstractConcept To Speech (CTS) systems areclosely related to two other types ofsystems: Natural Language Generation(NLG) and Speech Synthesis (SS).
In thispaper, we propose a new architecture for aCTS system.
A Speech Integrating MarkupLanguage (SIML) is designed as an generalinterface for integrating NLG and SS.
Wealso present a CTS system for a multimediapresentation generation application.
Wediscuss how to extend the current CTS sys-tem based on the new architecture.
Cur-rently, only limited semantic, syntactic andprosodic features are covered inour proto-type system.1 In t roduct ionCurrently, there are two ways to develop a Concept-To-Speech (CTS) system.
The first is to design amonolithic CTS system for a specific application.This design involves a specific NLG module andan SS module, often developed for the application,where discourse, semantic and syntactic informa-tion produced by the NLG module can be used di-rectly by CTS algorithms to determine ither sys-tem specific parameters for a Text- To-Speech system,or phonological parameters for a vocal tract model(e.g., (Young and Fallside, 1979)).
One advantage ofthis design is its efficiency, but features from the twosystems are usually so intertwined that the interfaceof the CTS algorithms are system dependent.
An-other design is to keep NLG and SS as independentas possible, thus allowing reuse of the current NLGtools and TTS systems for other applications.
Thetypical design is equivalent to "NLG plus Text-To-Speech( TTS)" where the common interface betweenNLG and TTS is plain text.
One advantage ofthis isin aits simplicity and adaptability.
No change is neces-sary for existing NLG tools and TTS systems, but itsuffers from a serious problem in that it loses usefulinformation.
All discourse, semantic and syntacticinformation is lost when the internal representationof NLG is converted to the text output and clearlythis could be useful in determining prosody.In this paper, we want to maintain the autonomyof NLG and SS so that they are reusable for differ-ent applications, yet flexible enough to easily inte-grate without losing useful information.
We proposea new architecture in which the common interface isnot plain text, but a Speech Integrating Markup Lan-guage (SIML).
We show how this architecture can beused in a multimedia presentation application wherea prototype SIML was designed for this purpose.2 Related WorkRecently, people have become more interested in de-veloping CTS algorithms to improve the quality ofsynthesized speech.
In (Prevost, 1995) and (Steed-man, 1996), theme, rheme and contrast are used asimportant knowledge sources in determining accen-tual patterns.
In (Davis and Hirschberg, 1988),given/new and topic structure are used to controlintonational variation.
Other CTS related researchincludes (Young and Fallside, 1979) and (Danlos etal., 1986).
Most of the CTS systems developed todate have a closely integrated architecture.
Becauseof this, CTS algorithms which map information fromNLG to TTS parameters are system dependent.There is some related research in developingmarkup languages for TTS and speech transcription.The Speech Synthesis Markup Language( SSML) (Is-ard, 1995) is used as an interface for TTS.
The mo-tivation behind SSML is to overcome the difficultythat different TTS systems require different inputformat.
No additional information isprovided as in-put to TTS, but SSML provides a straightforwardrepresentation f existing prosodic features.
This23representation is too simple for the purpose of inte-grating NLG and SS for CTS.
There is almost nodiscourse, semantic or syntactic information i  theirrepresentation, yet these are features one would ex-pect as output from NLG and which should influencethe prosody of speech.The Text Encoding Initiative (TEl) (Sperberg-McQueen and Burnard, 1993) provides a generalguideline for transcribing spoken language usingStandard Generalized Markup Language (SGML).SGML is an international standard for encoding elec-tronic document for data interchange.
Integratingtwo components in CTS is a specific SGML appli-cation.
Therefore, it can't be addressed irectly inSGML.
But the design of SIML can be guided byTEI standards.3 System ArchitectureThe main new feature of the architecture (see Fig.
1)is the introduction of SIML.
The system has threemajor components: the NLG component, the SIMLTo Prosody Component(STP) and the TTS compo-nent.
Each can be designed and implemented in-dependently.
The NLG-SIML component first con-verts the input concepts into grammatical sentenceswith associated iscourse, semantic, and syntacticinformation.
Then the SIML converter transformsthe system specific NLG representation into stan-dard SIML format.
The STP component computesthe prosodic features based on the discourse, seman-tic and syntactic information encoded in the SIMLformat.
The STP component has three modules: theSIML parser, the STP algorithms and the SIML gen-erator.
First the SIML parser analyzes the informa-tion in SIML.
The STP algorithms predict prosodicparameters based on the information derived fromthe markup language.
Then the SIML generator en-codes the prosodic features in SIML format.
TheTTS component first extracts the prosodic parame-ters from the SIML representation a d translates itinto a specific, system dependent TTS input.
In thisway various NLG tools, STP algorithms and TTScan be integrated through the standard interfaces,SIML.4 MA GIC CTS systemOur CTS system is a component of the MAGIC sys-tem (Multimedia Abstract Generation for IntensiveCare).
(Dalal et ah, 1996) (Pan and McKeown,1996).
MAGICs goal is to provide a temporallycoordinated multimedia presentation of data in anonline medical database.
The graphics and speechgenerators communicate hrough a media coordina-tor to produce a synchronized multimedia presen-tation.
Given that CTS takes place within a mul-timedia context, many of the parameters our CTSsystem address are those needed for aiding coordina-tion between media.
Currently, there are three com-ponents in the CTS system: an NLG component, aset of CTS algorithms and a TTS.
The NLG toolsand TTS are application independent.
We use theFUF/SURGE package (Elhadad, 1993) for gener-ation.
The speech synthesis ystem is AT&T BellLabs' TTS system.
The concept o speech algo-rithms, however, are not system independent.
Theinput of these algorithms are in FUF/SURGE rep-resentation and the output is designed specificly forAT~T TTS input.
In this section we describe ourcurrent CTS system and in the following section dis-cuss extensions that we plan to adapt it to the gen-eral, proposed architecture.NLG component  in MAGICThe NLG component in MAGIC consists of 4 mod-ules: a general content planner, a micro planner,a lexical chooser and a surface realizer.
The gen-eral content planner groups and organizes the dataitems from the medical database into topic segments;each segment may consist of several sentences.
Thenthe micro planner plans the content of the sentenceswithin each topic segment.
One or several sentencescan be used to convey the information within a topicsegment.
The lexical chooser makes decisions onword selection and the semantic structure of thesentence.
The output of the lexical chooser is aninternal semantic representation f the intended ut-terance.For example, the internal semantic structure of"The patient is hypertensive" is represented in:( (cat clause)(proc( (type ascriptive)(mode attributive)))(partic((carrier ((cat common)(head ( (lex "patient") ) ) ))(attribute ((cat ap)(lex "hypertensive" ) ) ) ) ) )In a semantic representation, a clause is defined byprocess type, participant and circumstance.
Processtype could be simple, as in the example, or com-posite (e.g., using conjunction).
Each participant orcircumstance may consist of a head and one or morepre-modifiers or qualifiers.
Words and phrases areused to realize each semantic unit.The surface realizer maps the lexicalized, semanticrepresentation to its corresponding syntactic struc-24I ?LG Component 1 TFS Component \]Input .
~ NLGSystem ) \[ \['lTrS System ~ ~td~trepresentation I t  J /NLG Output 4 .
.
.
.
~ \[ " ?
"~=- ~" b - - - -SP~Y?
inRepresentation I r LG.SIML a \[ \ [ \ [  ~on~erter \ ] \ [  ( SIML-TTS ~\[ TTS Format t Co~e~ter j j NLG Output in ~ " " ,,JSIML Format .
.
.
.
.
\[ S iML ~ STP 1_~ SIML } Parser1 , STPAlgorithms C mponent , Generator l "a"i .
.
.
.
.
_Prosody format in SIML' !semantic, syntactic Prosody discourse info.Figure 1: CTS System Architectureture.
Aft~er linearizing the syntactic structure, whichusually is the last step in a written language gen-eration system, the internal semantic and syntac-tic structure as well as the words of the sentenceare used as a rich and reliable knowledge source forspeech synthesis.CTS Algor i thms in MAGICDue to the synchronization requirements, we arespecifically interested in two features: pause andspeaking rate.
We want to increase or decrease thelength of pauses or the speaking rate in such a wayttiat speech actions begin and end at the same timeas corresponding graphical actions.
Even a smalldrift can be noticed by human eyes and cause un-comfortable visual effects.
In MAGIC, only pauseand speaking rate are set by our CTS algorithms;all other prosodic features are set by the default val-ues predicted by AT&T Bell Labs' TTS system.Currently, we use a simple strategy in adjustingthe speeda rate.
We define the relative speaking rateas the ratio of the real speaking rate to the defaultspeaking rate.
Through experiments, we determinedthat the relative speaking rate can vary from 0.5 to1 without significantly affecting the speech quality.In the future, we plan to develop an algorithm wherethe adjustable range is not uniform everywhere butdecided by the underlying discourse, semantic andsyntactic structures.In the following, we give more detail on the CTSalgorithm which is used to predict prosodic phraseboundary.
It provides a reliable indication on wherepauses can be inserted and how long the pause couldbe.We use semantic structures to derive the prosodicphrase boundaries.
In our algorithm, we first iden-tify the basic semantic unit (BSU), which is thesmallest, complete information unit in the semanticstructure.
Then we define the closeness measure-ment between two adjacent BSUs.
If two adjacentBSUs are loosely connected, then we have reason tobelieve that it won't hurt the intelligibility signifi-cantly if we speak them separately.
Therefore, se-mantic closeness is an important knowledge sourcefor prosodic phrase boundary prediction.
Otherfactors which also affect the placement of prosodicphrase boundary are breath length, and the distanceto the end of the utterance.A Basic Semantic Unit(BSU) is a leaf node in asemantic hierarchy.
In the semantic hierarchy (seeFig.
2), the BSU is indicated by dark blocks.We define the closeness between two adjacentBSUs as the level of the lowest common ancestorin the semantic hierarchy.
If a node has only onechild, then both parent and the child are consideredat the same level.
The closeness indicates the seman-tic distance of two adjacent BSUs.
1 means they aresemanticly far apart, while higher numbers indicatethey are semantically dose.Breath length is defined as the typical numberof words a human can speak comfortably with-out breathing.
The value used in the algorithm islearned automatically from a corpus.
The distancefrom the current place to the end of an utterance issimply defined by the number of words.Now we have 3 factors working together determin-ing the prosodic phrase boundary.
Basically, therewon't be any prosodic phrase boundary within aBSU.
For each place between two adjacent BSUs,we measure the possibility of inserting a prosodicphrase boundary using the combination of the 3 fac-tors:1.
The larger the closeness measurement, the lessthe possibility of a boundary.25ClauseIPar P roc lE'a.r t i.c ?pan.t(carriQr) (ascriptivo) (at t r lbuto)I Np.
BQmNp.
sereIHeadII I JH..o Class i f ie r l  C lasml f le~3I Classiflor2 J Cl~smlfler4I I Adj,.
sere AaJp.mem AdJp.
sore AdJp.semH.~d Hdad H I d HoJadJ Quallfier2IClause  Qumlifierlj II | I\[ \[ Ag.nt Af fe~.dpp.s6m ~.sem J J .~  re_Figure 2: Semantic Structure and BSU2.
The closer the current breath length to the com-fortable breath length, the more the possibilityof a boundary.3.
The closer the current place to the end of theutterance, the less the possibility of a boundary.4.
The above factors are weighted, using a learningalgorithm we trained automatically on a smallcorpus (40 sentences).The result is encouraging.
When we test thison the set provided in (Bachenko and Fitzpatrick,1990), we got a 90% accuracy for primary phraseboundary and we get an 82% accuracy for the ut-terances in (Gee and Grosjean, 1983).
We didnot formally measure the algorithm for secondaryphrase boundaries, because we only consider insert-ing pauses at primary phrase boundary.TTS in MAGICBasically, we treat TTS as a black box in MAGIC.We use the escape sequence of TTS to override theTTS default value.5 Extens ions  to  MAGIC CTS Basedon  the  New Arch i tec tureThe cm'rent MAGIC CTS uses CTS algorithms thatare closely integrated with both the NLG tools andTTS.
This will make it difficult to experiment withnew tools, requiring changes in all the input andoutput format for the CTS algorithms.
In the spiritof developing a portable language generation systemsuch as FUF/SURGE, we are working on a portablespoken language generation system by using the newarchitecture.Extens ion 1: Design SIML for MAGICIn order to extend the current CTS, we must define aprototype SIML.
As a first step, we have designed aprototype SIML that covers the information eededfor CTS in the multimedia context.
For our CTS al-gorithms, only semantic and syntactic structure areused in predicting prosodic phrase boundary and arerepresented in the SIML.
Speaking rate and pauseare also included in SIML.We first describe how this information is repre-sented in SIML, giving examples howing how touse SIML to tag pauses, speaking rate, semanticand syntactic structure.
Then part of the formalDocument Type Definition (DTD) of the proto-type SIML is presented, providing a grammar forSIML.
See (Sperberg-McQueen a d Burnard, 1993)for more information about SGML and DTD.Example 1: Using SIML to tag speaking rate andpauses:<u.pro>Ms.
Jones <pause dur=5 durunit=ms> isan <phrase rate=0.9> 80 year old </phrase>hypertensive, diabetic female patient ofdoctor Smith undergoing CABG.
</u.pro><u.pro> and </u.pro> above indicate the start andend of an utterance.
<phrase> and </phrase>is the front and end tag of a phrase.
Rate is anattribute associated with <phrase>, indicating thespeaking rate of the phrase.
<pause> is a tag with26two associated attributes: dur and durunit.
Theyindicate the length of the pause.Example 2: using SIML to tag semantic structure:<clause><part ic ipant  ro le=carr ier><np.sera>The<head>patient</head></np.sem></part ic ipant><proc type=ascr ipt ive l x=be> is  </prec><participant role=attribute><adjp.sem><head>hypertensive</head></adjp.sem></participant></clause>Example 3: using SIML to tag syntactic structure:<sentence><np> <art> The <noun> patient</np><vp> <verb>is<adjp> hypertensive.
</adjp><Ivp></sentence>Part of the formal definition of SIML, using DTD.<!-- DTD specifying speaking rate and pause --><!
DOCTYPE utterance.pro \[<!
ELEMENT u .pro - -  ((#PCDATA~ phrase~pause)*)><!
ATTLIST u.prorate NUMBER i ><!
ELEMENT phrase- -  ((#PCDATAI pause)*) ><!
ATTLIST phraserate NUMBER 1 ><!
ELEMENT pause - o #EMPTY><!
ATTLIST pausedur NUMBER $CUB/tENTdurunit CDATA ms >\]>In the above DTD specification, three elements andtheir associated attributes are defined:?
u.pro and its attribute, rate;?
phrase and its attribute, rate;?
pause and its attributes, dur and durunit.The following is the element definition for "u.pro":<!
ELEMENT u .pro - -  ((#PCDATAI phrase\]pause)*)>ELEMENT is a reserved word for the element defini-tion.
"u.pro" is the element name.
% -" is an omit-ted tag minimization which means both the startand end tags are mandatory.
The last part is thecontent model specification.
(#PCDATA I phrase \]pause)* means only parsed character data, phrasesand pauses may appear between the start and endtags of "u.pro".The associated attributes are defined in<!
ATTLIST u.prorate  NUMBER 1 >where the ATTLIST is the reserved word for at-tribute list definition.
"u.pro" is the element name,"rate" is the attribute name, the type of "rate" isNUMBER and the default value is "1".Extension 2: Design the STP componentThe STP component is the core part in the architec-ture and deserves more explanation.
There are threetasks for this component: parsing of the input SIML,generation ofprosodic parameters from the informa-tion produced by NLG, and transformation f theparameters into the SIML format.
The SIML pars-ing is straight forward.
It can be done either by de-veloping an SIML specific parser for better efficiencyor by using an SGML parser (there are several whichare publicly available).
The output of this compo-nent is the semantic and syntactic information ex-tracted from SIML.
Generation of prosodic param-eters must be done using a set of CTS algorithms;we need to change the input and output of our ex-isting CTS algorithms and make it system indepen-dent.
Since the performance of these algorithms di-rectly affects the quality of the synthesized speech,much effort is required to develop good CTS algo-rithms.
The good news is that the proposed esignensures that the markup to prosody algorithms aresystem independent.
Therefore, they can be reusedin other applications.
The output of the STP al-gorithms then converts to the SIML format by theSIML generator.
The procedure is straight forwardand it can be done very efficiently.6 Genera l i ze  SIMLSince the current prototype SIML is designed specif-ically for multimedia pplication, it includes verylimited semantic, syntactic and prosodic informa-tion.
Thus, it is currently too primitive to be usedas a standard interface for other CTS applications.For the future, we must include other forms informa-tion that are needed for speech synthesis and thatcan be generated by an NLG system.
Some types ofknowledge that we have identified include:1.
Discourse information (e.g.
discourse structure,focus, rhetoric relations etc.
), semantic struc-ture and its associated features (such as in theprototype SIML), and syntactic structure.2.
Pragmatic information such as speaker-hearergoals, hearer background, hearer type, speaker27type, emotions.3.
Morphology information, such as root, prefix,suffix.4.
Speech features, such as pronunci-ation, prosodic features, temporal information(such as duration, start, end), and non-lexicalfeatures (such as click, cough).7 Conc lus ion  and  Future  workIn this paper, a new CTS architecture is presented.The key idea is to integrate current NLG and TTSsystems in a standard way so that the CTS systemdeveloped isable to use any existing NLG tools, STPalgorithms and TTS systems and benefit from the in-formation available from NLG.
A Speech IntegratingMarkup Language is designed for this purpose.In the future, we will extend our STP algorithms,to predict an adjustable range of speaking rate andstress placement based on discourse, semantic andsyntactic information.
As a result, we need to ex-tend our SIML so that new information can be in-corporated easily.8 AcknowledgementsMAGIC is a system involving a large number ofpeople at Columbia University.
In addition to theauthors, who are responsible for the text and spo-ken language generator, the MAGIC team includes.lames Shaw (media-independent content planningand text organization); Steve Feiner, MichelleZhou (graphics generation); Mukesh Dalal, Li Yang(knowledge representation); and Tobias Hollerer(media coordination).
We thank Becky Passonneaufor providing the SGML developing environment.This research is supported in part by DARPA Con-tract DAAL01-94-K-0119, the Columbia UniversityCenter for Advanced Technology in High Perfor-mance Computing and Communications in Health-care (funded by the New York State Science andTechnology Foundation) and GER-90-2406.temporal multimedia presentations.
In Proceed-ings of ACM Multimedia 1996.L.
Danlos, E. LaPort, and F. Emerard.
1986.
Syn-thesis of spoken messages from semantic represen-tations.
In Proceedings of the 11th InternationalConference on Computational Linguistics, pages599-604.Dams and J. Hirschberg.
1988.
Assigning into-national features in synthesized spoken discourse.In Proceedings of the 26th Annual Meeting of theAssociation for Computational Linguistics, pages187-193, Buffalo, New York.M.
Elhadad.
1993.
Using Argumentation to ControlLexical Choice: A Functional Unification Imple-mentation.
Ph.D. thesis, Columbia University.J.
P. Gee and F. Grosjean.
1983.
Performance struc-ture: A psycholinguistic and linguistic appraisal.Cognitive Psychology, 15:411-458.A.
Isard.
1995.
SSML: A markup language forspeech synthesis.
Master's thesis, University ofEdinburgh.Shimei Pan and Kathleen McKeown.
1996.
Spokenlanguage generation i  a multimedia system.
InProceedings of ICSLP, volume 1, Philadelphia.S.
Prevost.
1995.
A Semantics of Contrast and In-formation Structure for Specifying Intonation inSpoken Language Generation.
Ph.D. thesis, Uni-versity of Pennsylvania.C.M.
Sperberg-McQueen a d L. Burnard.
1993.Guidelines for Electronic Text Encoding and In-terchange.
ACH, ACL and ALLC.M.
Steedman.
1996.
Representing discourse infor-mation for spoken dialogue generation.
In Pro-ceedings of the International Symposium on Spo-ken Dialogue, pages 89-92, Philadelphia.S.
Young and F. Fallside.
1979.
Speech synthesisfrom concept: a method for speech output frominformation systems.
Journal of the AcousticalSociety of America, 66:685-695.J.Re ferencesJ.
Bachenko and E. Fitzpatrick.
1990.
A com-putational grammar of discourse-neutral prosodicphrasing in English.
Computational Linguistics,16(3):155-170.Mukesh Dalai, Steven Feiner, Kathleen McKe-own, Shirnei Pan, Michelle Zhou, Tobias Hollerer,.lames Shaw, Yong Feng, and Jeanne Fromer.1996.
Negotiation for automated generation of28
