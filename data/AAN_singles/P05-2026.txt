Proceedings of the ACL Student Research Workshop, pages 151?156,Ann Arbor, Michigan, June 2005. c?2005 Association for Computational LinguisticsA Domain-Specific Statistical Surface RealizerJeffrey T. RussellCenter for the Study of Language and InformationStanford Universityjefe@stanford.eduAbstractWe present a search-based approach to au-tomatic surface realization given a cor-pus of domain sentences.
Using heuris-tic search based on a statistical languagemodel and a structure we introduce calledan inheritance table we overgenerate aset of complete syntactic-semantic treesthat are consistent with the given seman-tic structure and have high likelihood rela-tive to the language model.
These trees arethen lexicalized, linearized, scored, andranked.
This model is being developed togenerate real-time navigation instructions.1 IntroductionThe target application for this work is real-time, in-teractive navigation instructions.
Good direction-givers respond actively to a driver?s actions andquestions, and express instructions relative to a largevariety of landmarks, times, and distances.
Thesetraits require robust, real-time natural language gen-eration.
This can be broken into three steps: (1) gen-erating a route plan, (2) reasoning about the routeand the user to produce an abstract representationof individual instructions, and (3) realizing these in-structions as sentences in natural language (in ourcase, English).
We focus on the last of these steps:given a structure that represents the semantic contentof a sentence, we want to produce an English sen-tence that expresses this content.
According to thetraditional division of content determination, sen-tence planning, and surface realization, our workis primarily concerned with surface realization, butalso includes aspects of sentence planning.
Ourapplication requires robust flexibility within a re-stricted domain that is not well represented in thetraditional corpora or tools.
These requirements sug-gest using trainable stochastic generation.A number of statistical surface realizers have beendescribed, notably the FERGUS (Bangalore andRambow, 2000) and HALogen systems (Langkilde-Geary, 2002), as well as experiments in (Rat-naparkhi, 2000).
FERGUS (Flexible Empiri-cist/Rationalist Generation Using Syntax) takes asinput a dependency tree whose nodes are markedwith lexemes only.
The generator automatically ?su-pertags?
each input node with a TAG tree, then pro-duces a lattice of all possible linearizations consis-tent with the supertagged dependency tree.
Finallyit selects the most likely traversal of this lattice,conditioned on a domain-trained language model.The HALogen system is a broad-coverage genera-tor that uses a combination of statistical and sym-bolic techniques.
The input, a structure of feature-value pairs (see Section 3.1), is symbolically trans-formed into a forest of possible expressions, whichare then ranked using a corpus-trained statistical lan-guage model.
Ratnaparkhi also uses an overgener-ation approach, using search to generate candidatesentences which are then scored and ranked.
Hispaper outlines experiments with an n-gram model,a trained dependency grammar, and finally a hand-built grammar including content-driven conditionsfor applying rules.
The last of these systems outper-formed the n-gram and trained grammar in testingbased on human judgments.151The basic idea of our system fits in theovergenerate-and-rank paradigm.
Our approach ispartly motivated by the idea of ?softening?
Ratna-parkhi?s third system, replacing the hand-built gram-mar rules with a combination of a trained statisticallanguage model and a structure called an inheritancetable, which captures long-run dependency informa-tion.
This allows us to overgenerate based on rulesthat are sensitive to structured content without incur-ring the cost of designing such rules by hand.2 AlgorithmWe use dependency tree representations for both thesemantics and syntax of a sentence; we introducethe syntactic-semantic (SS) tree to combine infor-mation from both of these structures.
An SS treeis constructed by ?attaching?
some of the nodes of asentence?s semantic tree to the nodes of its syntactictree, obeying two rules:?
Each node in the semantic tree is attached toat most one node of the syntactic tree.?
Semantic and syntactic hierarchical order-ings are consistent.
That is to say, if two se-mantic nodes x1and x2are attached to two syn-tactic nodes y1and y2, respectively, then x1isa descendant of x2in the semantic tree if andonly if y1is a descendant of y2in the syntactictree.The nodes of an SS tree are either unattached se-mantic or syntactic nodes, or else pairs of attachednodes.
The SS tree?s hierarchy is consistent with thehierarchies in the syntactic and semantic trees.
Wesay that an SS tree T satisfies a semantic structureS if S is embedded in T .
This serves as formaliza-tion of the idea of a sentence expressing a certaincontent.2.1 OutlineThe core of our method is a heuristic search of thespace of possible SS trees.
Our search goal is to findthe N best complete SS trees that express the givensemantic structure.
We take ?best?
here to be thetrees which have the highest conditional likelihoodgiven that they express the right semantic structure.If S is our semantic structure and LM is our statis-tical language model, we want to find syntactic treesT that maximize PLM (T |S).In order to search the space of trees, we buildup trees by expanding one node at a time.
Duringthe search, then, we deal with incomplete trees; thatis, trees with some nodes not fully expanded.
Thismeans that we need a way to determine how promis-ing an incomplete tree T is: i.e., how good the bestcomplete trees are that can be built up by expandingT .
As it turns out (Section 2.2), we can efficientlyapproximate the function 1 PLM (T |S) for an incom-plete tree, and this function is a good heuristic forthe maximum likelihood of a complete tree extendedfrom T .Here is an outline of the algorithm:?
Start with a root tree.?
Take the top N trees and expand one nodein each.?
Score each expanded tree for PLM (T |S),and put in the search order accordingly.?
Repeat until we find enough trees that sat-isfy S.?
Complete the trees.?
Linearize and lexicalize the trees.?
Rank the complete trees according to somescoring function.2.2 HeuristicOur search goal is to maximize PLM (T |S).
(Hence-forth we abbreviate PLM as just P .)
Ideally,then, we would at each step expand the incompletetree that can be extended to the highest-likelihoodcomplete tree, i.e.
that has the highest value ofmaxT ?
P (T ?|S) over all complete trees T ?
that ex-tend T .
We use the notation T ?
> T when T ?
is acomplete tree that extends an incomplete tree T , andthe notation T ?
 S when T ?
satisfies S. Then the?goodness?
of a tree T is given bymaxT ?>TP (T ?|S) = maxT ?>T ;T ?SP (T ?
)/P (S) (1)1This probability is defined to be the sum of the probabilitiesPLM(T |T ?
)PLM(T ?|S) for all complete trees T ?152Since finding this maximum explicitly is not fea-sible, we use the heuristic P (T |S).
By Bayes?
rule,P (T |S) = P (S|T )P (T )/P (S), where P (S) is anormalizing factor, P (T ) can be easily calculatedusing the language model (as the product of theprobabilities of the node expansions that appear inT), andP (S|T ) =?T ?P (S|T ?
)P (T ?|T ) =?T ?SP (T ?|T )Since P (T ?|T ) = P (T |T ?
)P (T ?
)/P (T ), and sinceP (T |T ?)
is 1 if T ?
> T and 0 otherwise, we haveP (T |S) =1P (S)?T ?SP (T |T ?
)P (T ?
)=1P (S)?T ?>T ;T ?SP (T ?
)Together with Equation 1 this shows thatP (T |S) ?
maxT ?>T P (T ?|S), since the maximumis one of the terms in the sum.
This fact is analogousto showing that P (T |S) is an admissible heuristic(in the sense of A* search).We can see how to calculate P (T |S) in practiceby decomposing the structure of a tree T?
such thatT ?
> T and T ?S.
Since T ?
extends T , the top of T ?is identical to T .
The semantic tree S will have someof its nodes in T , and some in the part of T?
thatextends beyond T .
Let ?
(S, T ) be the set containingthe highest nodes in S that are not in T .
Each nodes ?
?
(S, T ) is the root node of a subtree in T?.
Eachof these subtrees can be considered separately.First we consider how the these subtrees arejoined to the nodes in T .
The condition of consis-tent ordering requires that each node in ?
(S, T ) bea descendant in T ?
of its parent in S, and moreoverit should not be a descendant of any of its siblingsin S. Let sib be a set of siblings in ?
(S, T ), and letp be their semantic parent.
Then p is the root nodeof a subtree of T , called Tp.
We will designate theT-set of sib as the set of leaves of Tp that are notdescended from any nodes in S below p?in particu-lar, that are not descended from any other siblings ofthe nodes in sib.
Then in T ?
all of the nodes in sibmust descend from the T-set of sib.
In other words,there is a set of subtrees of T?
which are rooted atthe nodes in the T-set of sib, and all of the nodes insib appear in these subtrees such that none of themare descended from each other.This analysis sets us up to rewrite P (T |S) interms of sums over these various subtrees.
Weuse the notation P ({x1, ..., xk} ?
{y1, ..., yl})to denote the probability that the nodes y1, ..., yleventually descend from x1, ..., xk without domi-nating each other; this probability is the sum ofP (T1, ..., Tk) over all sets of trees T1 > x1, ..., Tk >xk such that each node y1, ..., yl appears in some Tiand no yi descends from any yj .
Then we can rewriteP (T |S) asP (T )P (S)?sibP (T-set(sib) ?
sib)?x??
(S,T )P (x ?
Sx)(2)Sx denotes the subtree of S whose root node is x.P (x ?
Sx) is 1 if Sx contains only the node x, andotherwise isP (x ?
childrenS(x))???
?y?childrenS(x)P (y ?
Sy)??
?Rather than calculating the value of formula 2 ex-actly, we now introduce an approximation to ourheuristic function.
For sets X, Y , we approximateP (X ?
Y ) with?y?Y P (X ?
y).
This amountsto two simplifications: first, we drop the restrictionthat no node be descended from its semantic sib-ling; second, we assume that the probabilities ofeach node descending from X are independent fromone another.P (X ?
y) is the probability that at least onex ?
X has y as a descendant, i.e.
P (X ?
y) =AL1x?XP (x ?
y), where AL1 is the ?At-least-one?
function.2 This means that we can approximateP (T |S) asP (T )P (S)?y??
(S,T )AL1x?T-set(y)P (x ?
y)P (y ?
Sy)(3)2That is, given the probabilities of a set of events, the At-least-one function gives the probability of at least one of theevents occuring.
For independent events, AL1{} = 0 andAL1{p1, ..., pn} = pn+ (1 ?
pn)AL1{p1, ..., pn?1}.153The calculation of P (T |S) has been reduced tofinding P (x ?
y) for individual nodes.
Thesevalues are retrieved from the inheritance table, de-scribed below.Note that when we expand a single node of anincomplete tree, only a few factors in Equation 3change.
Rather than recalculating each tree?s scorefrom scratch, then, by caching intermediate resultswe can recompute only the terms that change.
Thisallows for efficient calculation of the heuristic func-tion.2.3 Inheritance TableThe inheritance table (IT) allows us to predict thepotential descendants of an incomplete tree.
Foreach pair of SS nodes x and y, the IT stores P (x ?y), the probability that y will eventually appear asa descendant of x.
The IT is precomputed oncefrom the language model; the same IT is used forall queries.We can compute the IT using an iterative process.Consider the transformation T that takes a distribu-tion Q(x ?
y) to a new distribution T(Q) such thatT(Q)(x ?
y) is equal to 1 when x = y, and other-wise is equal to??
?Exp(x)PLM (?|x)AL1z?
?Q(z ?
y) (4)Here Exp(x) is the set of possible expansions of x,and PLM (?|x) is the probability of the expansion ?according to the language model.The defining property of the IT?s distribution Pis that T(P ) = P .
We can use this propertyto compute the table iteratively.
Begin by settingP0(x ?
y) to 1 when x = y and 0 otherwise.
Thenat each step let Pk+1 = T(Pk).
When this processconverges, the limiting function is the correct inher-itance distribution.2.4 Completing TreesA final important issue is termination.
Ordinarily, itwould be sensible to remove a tree from the searchorder only when it is a goal state?that is, if it is acomplete tree that satisfies S. However, this turnsout to be not the best approach in this case due to aquirk of our heuristic.
P (T |S) has two non-constantfactors, P (S|T ) and P (T ).
Once all of the nodesin S appear in an incomplete tree T , P (S|T ) = 1,and so it won?t increase as the tree is expanded fur-ther.
Moreover, with each node expanded, P (T ) de-creases.
This means that we are unlikely to makeprogress beyond the point where all of the semanticcontent appears in a tree.An effective way to deal with this is to removetrees from the search order as soon as P (S|T )reaches 1.
When the search terminates by findingenough of these ?almost complete?
trees, these treesare completed: we find the optimal complete treesby repeatedly expanding the N most likely almost-complete trees (ranked by P (T )) until sufficientlymany complete trees are found.3 Implementation3.1 RepresentationOur semantic representation is based on the HALo-gen input structure (Langkilde-Geary, 2002).
Themeaning of a sentence is represented by a tree whosenodes are each marked with a concept and a seman-tic role.
For example, the meaning of the sentence?Turn left at the second traffic light?
is representedby the following structure:(maketurn:direction (left):spatial-locating(trafficlight:modifier (second)))The syntax model we use is statistical dependencygrammar.
As we outlined in Section 2, the semanticand syntactic structures are attached to one anotherin an SS tree.
In order to accomodate the require-ment that each semantic node is attached to no morethan one syntactic node, collocations like ?trafficlight?
or ?John Hancock Tower?, are treated as sin-gle syntactic nodes.
It can also be convenient to ex-tend this idea, treating phrases like ?turn around?or ?thank you very much?
as atomic.
In the casewhere a concept attaches to multi-word expression,but where it is inconvenient to treat the expressionas a syntactic atom, we adopt the convention of at-taching the concept to the hierarchically dominantword in the expression.
For instance, the concept ofturning can be attached to the expression ?make a154turn?
; in this case we attach the concept to the word?make?, and not to ?turn?.The nodes of an SS tree are (word, part of speech,concept, semantic role) 4-tuples, where the conceptand role are left empty for function words, and theword and part of speech are left empty for conceptswith no direct syntactic correlate.
Generally we omitthe word itself from the tree in order to mitigate spar-sity issues; these are added to the final full tree by alexical choice module.We use a domain-trained language model basedon the same dependency structure as our syntactic-semantic representations.
The currently imple-mented model calculates the probability of expan-sions given a parent node based on an explicit tabu-lar representation of the distribution P (?|x) for eachx.
This language model is also used to score andrank generated sentences.3.2 Corpus and AnnotationTraining this language model requires an annotatedcorpus of in-domain text.
Our main corpus comesfrom transcripts of direction-giving in a simulationcontext, collected using the ?Wizard of Oz?
set-updescribed in (Cheng et al, 2004).
For developmentand testing, we extracted approximately 600 instruc-tions, divided into training and test sets.
The trainingset was used to train the language model used forsearch, the lexical choice module, and the scoringfunction.
Both sets both underwent four partially-automated stages of annotation.First we tag words with their part of speech, usingthe Brill tagger with manually modified lexicon andtransformation rules for our domain (Brill, 1995).Second, the words are disambiguated and assigneda concept tag.
For this we construct a domain on-tology, which is used to automatically tag the unam-biguous words and prompt for human disambigua-tion in the remaining cases.
The third step is to as-sign semantic roles.
This is accomplished by usinga list of contextual rules, similar to the rules used bythe Brill tagger.
For example, the ruleCON intersection PREV1OR2OR3WD at: spatial-locatingassigns the role ?spatial-locating?
to a word whoseconcept is ?intersection?
if the word ?at?
appearsone, two, or three words before it.
A segment ofthe corpus was automatically annotated using suchrules, then a human annotater made corrections andadded new rules, repeating these steps until the cor-pus was fully annotated with semantic roles.After the first three stages, the sentence, ?Turn leftat the next intersection?
is annotated as follows:turn/VB/maketurn left/RB/$leftright/direction at/IN the/DT next/JJ/first/modifierintersection/NN/intersection/spatial-locatingThe final annotation step is parsing.
For thiswe use an approach similar to Pereira and Sch-abes?
grammar induction from partially bracketedtext (Pereira and Schabes, 1992).
First we annotatea segment of the corpus.
Then we use the inside-outside algorithm to simultaneously train a depen-dency grammar and complete the annotation.
Wethen manually correct a further segment of the an-notation, and repeat until acceptable parses are ob-tained.3.3 RenderingLinearizing an SS tree amounts to deciding the or-der of the branches and whether each appears on theleft or the right side of the head.
We built this infor-mation into our language model, so a grammar rulefor expanding a node includes full ordering informa-tion.
This makes the linearization step trivial at thecost of adding sparsity to the language model.Lexicalization could be relegated to the languagemodel in the same way, by including lexemes in therepresentation of each node, but again this would in-cur sparsity costs.
The other option is to delegatelexical choice to a separate module, which takes aSS tree and assigns a word to each node.
We use ahybrid approach: content words are assigned usinga lexical choice module, while most function wordsare included explicitly in the language model.
Thecurrent lexical choice module simply assigns eachunlabeled node the most likely word conditioned onits (POS, concept, role) triple, as observed in thetraining corpus.4 ExampleWe take the semantic structure presented in Sec-tion 3.1 as an example generation query.
The search155stage terminates when 100 trees that embed this se-mantic structure have been found.
The best-scoringsentence has the following lexicalized tree:turn/VB/maketurn+left/RB/$leftright/direction+at/IN+traffic_light/NN/trafficlight/spatial-locating-the/DT+next/JJ/first/modifierThis is finally rendered thus:turn left at the second traffic light.5 Preliminary ResultsFor initial testing, we separated the annotated corpusinto a 565-sentence training set and a 57-sentencetest set.
We automatically extracted semantic struc-tures from the test set, then used these structuresas generation queries, returning only the highest-ranked sentence for each query.
The generated re-sults were then evaluated by three independent hu-man annotaters along two dimensions: (1) Is thegenerated sentence grammatical?
(2) Does the gen-erated sentence have the same meaning as the origi-nal sentence?For 11 of the 57 sentences (19%), the query ex-traction failed due to inadequate grammar cover-age.3 Of the 46 instances where a query was suc-cessfully extracted, 3 queries (7%) timed out with-out producing output.
Averaging the annotaters?judgments, 1 generated sentence (2%) was ungram-matical, and 3 generated sentences (7%) had dif-ferent meanings from their originals.
39 queries(85%) produced output that was both grammaticaland faithful to the original sentence?s meaning.6 Future WorkStatistically-driven search offers a means of effi-ciently overgenerating sentences to express a givensemantic structure.
This is well-suited not only toour navigation domain, but also to other domains3The corpus was partially annotated for parse data, the fullparses being automatically generated from the domain-trainedlanguage model.
It was at this step that query extraction some-times failed.with a relatively small vocabulary but variable andcomplex content structure.
Our implementation ofthe idea of this paper is under development in a num-ber of directions.A better option for robust language modelingis to use maximum entropy techniques to train afeature-based model.
For instance, we can deter-mine the probability of each child using such fea-tures as the POS, concept, and role of the parent andprevious siblings.
It may also be more effective toisolate linear precedence from the language model,introducing a non-trivial linearization step.
Simi-larly, the lexicalization module can be improved onby using a more context-sensitive model.Using only a tree-based scoring function is likelyto produce inferior results to one that incorporates alinear score.
A weighted average of the dependencyscore with an n-gram model would already offer im-provement.
To further improve fluency, these couldalso be combined with a scoring function that takeslonger-range dependencies into account, as well aspenalizing extraneous content.ReferencesSrinivas Bangalore and O. Rambow.
2000.
Using TAG,a Tree Model, and a Language Model for Genera-tion.
5th Int?l Workshop on Tree-Adjoining Grammars(TAG+), TALANA, Paris.Eric Brill.
1995.
Transformation-Based Error-DrivenLearning and Natural Language Processing: A CaseStudy in Part of Speech Tagging.
Computational Lin-guistics, 21 (4).Hua Cheng, H. Bratt, R. Mishra, E. Shriberg, S. Upson, J.Chen, F. Weng, S. Peters, L. Cavedon and J. Niekrasz.2004.
A Wizard Of OZ Framework for CollectingSpoken Human-Computer Dialogs.
Proc.
8th ICSLP,Jeju Island, Korea.Irene Langkilde-Geary.
2002.
An empirical verificationof coverage and correctness for a general-purpose sen-tence generator.
Proc.
2nd INLG, Harriman, NY.Fernando Pereira and Y. Schabes.
1992.
Inside-outsidereestimation from partially bracketed corpora.
Proc.30th ACL, p.128-135, Newark.Adwait Ratnaparkhi.
2000.
Trainable methods for sur-face natural language generation.
Proc.
1st NAACL,Seattle.156
