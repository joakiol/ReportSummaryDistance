Proceedings of the UCNLG+Eval: Language Generation and Evaluation Workshop, pages 54?63,Edinburgh, Scotland, UK, July 31, 2011. c?2011 Association for Computational LinguisticsDiscriminative features in reversible stochastic attribute-value grammarsDanie?l de KokUniversity of Groningend.j.a.de.kok@rug.nlAbstractReversible stochastic attribute-value gram-mars (de Kok et al, 2011) use one modelfor parse disambiguation and fluency rank-ing.
Such a model encodes preferences withrespect to syntax, fluency, and appropriate-ness of logical forms, as weighted features.Reversible models are built on the premisethat syntactic preferences are shared betweenparse disambiguation and fluency ranking.Given that reversible models also use fea-tures that are specific to parsing or genera-tion, there is the possibility that the model istrained to rely on these directional features.
Ifthis is true, the premise that preferences areshared between parse disambiguation and flu-ency ranking does not hold.In this work, we compare and apply feature se-lection techniques to extract the most discrim-inative features from directional and reversiblemodels.
We then analyse the contributions ofdifferent classes of features, and show that re-versible models do rely on task-independentfeatures.1 IntroductionReversible stochastic attribute-value grammars (deKok et al, 2011) provide an elegant framework thatfully integrates parsing and generation.
The mostimportant contribution of this framework is that ituses one conditional maximum entropy model forfluency ranking and parse disambiguation.
In sucha model, the probability of a derivation d is con-ditioned on a set of input constraints c that restrictthe set of derivations allowed by a grammar to thosecorresponding to a particular sentence (parsing) orlogical form (generation):p(d|c) =1Z(c)exp?iwifi(c, d) (1)Z(c) =?d???
(c)exp?iwifi(c, d?)
(2)Here, ?
(c) is the set of derivations for input c,fi(c, d) the value of feature fi in derivation d of c,and wi is the weight of fi.
Reversibility is opera-tionalized during training by imposing a constrainton a given feature fi with respect to the sentencesT in the parse disambiguation treebank and logicalforms L in the fluency ranking treebank.
This con-straint is:?c?C?d??(c)p?
(c)p(d|c)fi(c, d) ?
(3)p?
(c, d)fi(c, d) = 0Where C = T ?
L, p?
(c) is the empirical proba-bility of a set of constraints c, and p?
(c, d) the jointprobability of a set of constraints c and a derivationd.Reversible stochastic-attribute grammars rest onthe premise that preferences are shared between lan-guage comprehension and production.
For instance,in Dutch, subject fronting is preferred over directobject fronting.
If models for parse disambiguationand fluency ranking do not share preferences withrespect to fronting, it would be difficult for a parser54to recover the logical form that was the input to agenerator.Reversible models incorporate features that arespecific to parse disambiguation and fluency rank-ing, as well as features that are used for bothtasks.
Previous work (Cahill et al, 2007; de Kok,2010) has shown through feature analysis that task-independent features are indeed useful in directionalmodels.
However, since reversible models assignjust one weight to each feature regardless the task,one particular concern is that much of their discrim-inatory power is provided by task-specific features.If this is true, the premise that similar preferencesare used in parsing and generation does not hold.In this work, we will isolate the most discrimina-tive features of reversible models through feature se-lection, and make a quantitative and qualitative anal-ysis of these features.
Our aim is to to verify thatreversible models do rely on features used both inparsing and generation.To find the most effective features of a model, weneed an effective feature selection method.
Section 2describes three such methods: grafting, grafting-light, and gain-informed selection.
These methodsare compared empirically in Section 4 using the ex-perimental setup described in Section 3.
We then usethe best feature selection method to perform quanti-tative and qualitative analyses of reversible modelsin Sections 5 and 6.2 Feature selectionFeature selection is a procedure that attempts to ex-tract a subset of discriminative features S ?
Ffrom a set of features F , such that a model usingS performs comparable to a model using F and|S|  |F |.As discussed in De Kok (2010), a good feature se-lection method should handle three kinds of redun-dancies in feature sets: features that rarely changevalue; features that overlap; and noisy features.Also, for a qualitative evaluation of fluency ranking,it is necessary to have a ranking of features by dis-criminative power.De Kok (2010) compares frequency-based selec-tion, correlation selection, and a gain-informed se-lection method.
In that work, it was found thatthe gain-informed selection method outperformsfrequency-based and correlation selection.
For thisreason we exclude the latter two methods from ourexperiments.
Other commonly used selection meth-ods for maximum entropy models include `1 regu-larization (Tibshirani, 1996), grafting (Perkins et al,2003; Riezler and Vasserman, 2004), and grafting-light (Zhu et al, 2010).
In the following sections,we will give a description of these selection meth-ods.2.1 `1 regularizationDuring the training of maximum entropy mod-els, regularization is often applied to avoid uncon-strained feature weights and overfitting.
If L(w) isthe objective function that is minimized during train-ing, a regularizer ?q(w) is added as a penalty forextreme weights (Tibshirani, 1996):C(w) = L(w) + ?q(w) (4)Given that the maximum entropy training pro-cedure attempts to minimize the negative log-likelihood of the model, the penalized objectivefunction is:C(w) = ??c,dp?
(c, d)log(p(d|c)) + ?q(w) (5)The regularizer has the following form:?q(w) = ?n?i=1|wi|qSetting q = 1 in the regularizer function gives aso-called `1 regularizer and amounts to applying adouble-exponential prior distribution with ?
= 0.Since the double-exponential puts much of its prob-ability mass near its mean, the `1 regularizer has atendency to force weights towards zero, providingintegral feature selection and avoiding unboundedweights.
Increasing ?
strengthens the regularizer,and forces more feature weights to be zero.Given an appropriate value for ?, `1 regulariza-tion can exclude features that change value infre-quently, as well as noisy features.
However, it doesnot guarantee to exclude overlapping features, since55the weight mass can be distributed among overlap-ping features.
`1 regularization also does not fulfill anecessary characteristic for the present task, in thatit does not provide a ranking based on the discrimi-native power of features.2.2 GraftingGrafting (Perkins et al, 2003) adds incremental fea-ture selection during the training of a maximum en-tropy model.
The selection process is a repetition oftwo steps: 1. a gradient-based heuristic selects themost promising feature from the set of unselectedfeatures Z, adding it to the set of selected featuresS, and 2. a full optimization of weights is performedover all features in S. These steps are repeated untila stopping condition is triggered.During the first step, the gradient of each unse-lected feature fi ?
Z is calculated with respect tothe model pS , that was trained with the set of se-lected features, S:?????L(wS)?wi????
= pS(fi)?
p?
(fi) (6)The feature with the largest gradient is removedfrom Z and added to S.The stopping condition for grafting integrates the`1 regularizer in the grafting method.
Note thatwhen `1 regularization is applied, a feature is onlyincluded (has a non-zero weight) if its penalty is out-weighted by its contribution to the reduction of theobjective function.
Consequently, only features forwhich????L(wS)?wi???
> ?
holds are eligible for selection.This is enforced by stopping selection if for all fi inZ?????L(wS)?wi????
?
?
(7)Although grafting uses `1 regularization, its iter-ative nature avoids selecting overlapping features.For instance, if f1 and f2 are identical, and f1 isadded to the model pS ,????L(wS)?w2??
?will amount to zero.Performing a full optimization after each selectedfeature is computationally expensive.
Riezler andVasserman (2004) observe that during the featurestep selection a larger number of features can beadded to the model (n-best selection) without a lossof accuracy in the resulting model.
However, thisso-called n-best grafting may introduce overlappingfeatures.2.3 Grafting-lightThe grafting-light method (Zhu et al, 2010) oper-ates using the same selection step as grafting, butimproves performance over grafting by applying oneiteration of gradient-descent during the optimizationstep rather than performing a full gradient-descent.As such, grafting-light gradually works towards theoptimal weights, while grafting always finds the op-timal weights for the features in S during each iter-ation.Since grafting-light does not perform a fullgradient-descent, an additional stopping condition isrequired, since the model may still not be optimaleven though no more features can be selected.
Thisadditional condition requires that change in value ofthe objective function incurred by the last gradient-descent is smaller than a predefined threshold.2.4 Gain-informed selectionGain-informed feature selection methods calculatethe gain ?L(S, fi) of adding a feature fi ?
Z tothe model.
If L(wS) is the negative log-likelihoodof pS , ?L(S, fi) is defined as:?L(S, fi) ?
L(wS)?
L(wS?fi) (8)During each selection step, the feature that givesthe highest gain is selected.
The calculationof L(pS?fi) requires a full optimization over theweights of the features in S ?
fi.
Since it is com-putationally intractable to do this for every fi in Z,Berger et al (1996) propose to estimate the weightwi of the candidate feature fi, while assuming thatthe weights of features in S stay constant.
Underthis assumption, wi can be estimated using a simpleline search method.However, Zhou et al (2003) observe that, de-spite this simplification, the gain-informed selectionmethod proposed by Berger et al (1996) still recal-culates the weights of all the candidate features dur-ing every cycle.
They observe that the gains of can-didate features rarely increase.
If it is assumed thatthe gain of adding a feature does indeed never in-crease as a result of adding another feature, the gainsof features during the previous iteration can be kept.56To account for features that become ineffective, thegain of the highest ranked feature is recalculated.The highest ranked feature is selected if it remainsthe best feature after this recalculation.
Otherwise,the same procedure is repeated for the next best fea-ture.De Kok (2010) modifies the method of Zhou etal.
(2003) for ranking tasks.
In the present work, wealso apply this method, but perform a full optimiza-tion of feature weights in pS every n cycles.Since this selection method uses the gain of a fea-ture in its selection criterion, it excludes noisy andredundant features.
Overlapping features are alsoexcluded since their gain diminishes after selectingone of the overlapping features.3 Experimental setup and evaluation3.1 TreebanksWe carry out our experiments using the Alpino de-pendency parser and generator for Dutch (van No-ord, 2006; de Kok and van Noord, 2010).
Twonewspaper corpora are used in the experiments.The training data consists of the cdbl part of theEindhoven corpus1 (7,154 sentences).
Syntacticannotations are available from the Alpino Tree-bank2 (van der Beek et al, 2002).
Part of the Trouwnewspaper of 2001 is used for evaluation3.
Syntac-tic annotations are part of LASSY4 (van Noord etal., 2010), part WR-P-P-H (2,267 sentences).3.2 FeaturesIn our experiments, we use the features described inDe Kok et al (2011).
In this section, we provide ashort summarization of the types of features that areused.Word adjacency.
Word and Alpino part-of-speech tag trigram models are used as auxiliary dis-tributions (Johnson and Riezler, 2000).
In bothmodels, linear interpolation smoothing is applied tohandle unknown trigrams, and Laplacian smoothingfor unknown unigrams.
The trigram models have1http://www.inl.nl/corpora/eindhoven-corpus2http://www.let.rug.nl/vannoord/trees/3http://hmi.ewi.utwente.nl/TwNC4http://www.inl.nl/corpora/lassy-corpusbeen trained on the Twente Nieuws Corpus (approx-imately 100 million words), excluding the Trouw2001 corpus.
In parsing, the value of the word tri-gram model is constant across derivations of a giveninput sentence.Lexical frames.
The parser applies lexical analy-sis to find all possible subcategorization frames fortokens in the input sentence.
Since some frames oc-cur more frequently in good parses than others, twofeature templates record the use of frames in deriva-tions.
An additional feature implements an auxil-iary distribution of frames, trained on a large cor-pus of automatically annotated sentences (436 mil-lion words).
The values of lexical frame featuresare constant for all derivations in sentence realiza-tion, unless the frame is underspecified in the logicalform.Dependency relations.
Several templates de-scribe aspects of the dependency structure.
For eachdependency relation multiple dependency featuresare extracted.
These features list the dependencyrelation, and characteristics of the head and depen-dent, such as their roots or part of speech tags.
Ad-ditionally, features are used to implement auxiliarydistributions for selectional preferences (van Noord,2007).
In generation, the values of these features areconstant across derivations corresponding to a givenlogical form.Syntactic features.
Syntactic features include fea-tures that record the application of grammar rules,as well as the application of a rule in the contextof another rule.
Additionally, there are features de-scribing more complex syntactic patterns, such asfronting of subjects and other noun phrases, order-ings in the middle field, long-distance dependencies,and parallelism of conjuncts in coordinations.3.3 Parse disambiguationTo create training and evaluation data for parse dis-ambiguation, the treebanks described in section 3.1are parsed, extracting the first 3000 derivations.
Onaverage, there are about 649 derivations for the sen-tences in the training data, and 402 derivations forthe sentences in the test data.Since the parser does not always yield the cor-rect parse, the concept accuracy (CA) (van Noord,572006) of each derivation is calculated to estimate itsquality.
The highest scoring derivations for each in-put are marked as correct, all other derivations aremarked as incorrect.
Features are then extractedfrom each derivation.The concept accuracy is calculated based on thenamed dependency relations of the candidate andcorrect parses.
If Dp(t) is the bag of dependen-cies produced by the parser for sentence t andDg(t)is the bag of dependencies of the correct (gold-standard) parse, concept accuracy is defined as:CA =?t?T |Dp(t) ?Dg(t)|?t?T max(|Dp(t)|, |Dg(t)|)(9)The procedure outlined above gives examples ofcorrect and incorrect derivations to train the model,and derivations to test the resulting model.3.4 Fluency rankingFor training and evaluation of the fluency ranker, weuse the same treebanks as in parse disambiguation.We assume that the sentence that corresponds to adependency structure in the treebank is the correctrealization of that dependency structure.
We parseeach sentence in the treebank, extracting the depen-dency structure that is the most similar to that inthe treebank.
We perform this step to assure that itis possible to generate from the given dependencystructure.
We then use the Alpino chart genera-tor to make all possible derivations and realizationsconforming to that dependency structure.
Due to alimit on generation time, some longer sentences andcorresponding dependency structures are excludedfrom the data.
The average sentence length was 15.7tokens, with a maximum of 26 tokens.Since the sentence in the treebank cannot alwaysbe produced exactly, we estimate the quality of eachrealization using the General Text Matcher (GTM)method (Melamed et al, 2003).
The best-scoringderivations are marked as correct, the other deriva-tions are marked as incorrect.
Finally, features areextracted from these derivations.The General Text Matcher method marks all cor-responding tokens of a candidate realization and thecorrect realization in a grid, and finds the maximummatching (the largest subset of marks, such that nomarks are in the same row or column).
The size ofthe matchingM is then determined using the lengthsof runs r in the matching (a run is a diagonal ofmarks), rewarding longer runs:size(M) =?
?r?Mlength(r)2 (10)This method has been shown to have the highestcorrelation with human judgments in a related lan-guage (German), using a comparable system (Cahill,2009).3.5 TrainingModels are trained by extracting an informativesample of ?
(c) for each c in the training data (Os-borne, 2000).
This informative sample consists of atmost 100 randomly selected derivations.We then apply feature selection on the trainingdata.
We let each method select 1711 features.
Thisnumber is derived from the number of non-zero fea-tures that training a model with a `1 norm coefficientof 0.0002 gives.
Grafting and grafting-light selec-tion are applied using TinyEst5.
For gain-informedselection, we use FeatureSqueeze6.
For all threemethods, we add 10 features to the model duringeach selection step.3.6 EvaluationWe evaluate each selection method stepwise.
Wetrain and evaluate a model on the best-n features ac-cording to each selection method, for n = [0..1711].In each case, the feature weights are estimated withTinyEst using a `1 norm coefficient of 0.0002.
Thisstepwise evaluation allows us to capture the effec-tiveness of each method.Parse disambiguation and fluency ranking modelsare evaluated on the WR-P-P-H corpus that was de-scribed in Section 3.1, using CA and GTM scoresrespectively.4 Evaluation of feature selection methods4.1 Incremental feature selectionFigure 1 shows the performance of the feature selec-tion methods for parse disambiguation.
This graphshows that that both grafting methods are far more5http://github.com/danieldk/tinyest6https://github.com/rug-compling/featuresqueeze58effective than gain-informed selection.
We can alsosee that only a small number of features is requiredto construct a competitive model.
Selecting morefeatures improves the model only gradually.Figure 2 shows the performance of the featureselection methods in fluency ranking.
Again, wesee the same trend as in parse disambiguation.The grafting and grafting-light methods outperformgain-informed selection, with the grafting methodcoming out on top.
In feature selection, even asmaller number of features is required to train aneffective model.
After selecting more than approx-imately 50 features, adding features only improvesthe model very gradually.Figure 1: Application of feature selection methods toparse disambiguation4.2 Selection using an `1 priorDuring our experiments, we also evaluated the effectof using an `1 prior in Alpino to see if it is worthwileto replace feature selection using a frequency cut-off (Malouf and van Noord, 2004).
Using Alpino?sdefault configuration with a frequency cut-off of 2and an `2 prior with ?2 = 1000 the system had aCA-score of 90.94% using 25237 features.
We thentrained a model, applying an `1 prior with a normcoefficient of 0.0002.
With this model, the systemhad a CA-score of 90.90% using 2346 features.In generation, Alpino uses a model with the samefrequency cut-off and `2 prior.
This model has1734 features features and achieves a GTM score of0.7187.
Applying the `1 prior reduces the numberFigure 2: Effectiveness of feature selection methods influency ranking.
Both grafting methods outperform gain-based rankingof features to 607, while mildly increasing the GTMscore to 0.7188.These experiments show that the use of `1 priorscan compress models enormously, even comparedto frequency-based feature selection, while retainingthe same levels of accuracy.5 Quantitative analysis of reversiblemodelsFor a quantitative analysis of highly discriminativefeatures, we extract the 300 most effective featuresof the fluency ranking, parse disambiguation, and re-versible models using grafting.
We then divide fea-tures into five classes: dependency (enumeration ofdependency triples), lexical (readings of words), n-gram (word and tag trigram auxiliary distributions),rule (identifiers of grammar rules), and syntactic(abstract syntactic features).
Of these classes, ruleand syntactic features are active during both parsedisambiguation and fluency ranking.In the quantitative analyses, we train a model foreach selection step.
The models contain the 1 to 300best features.
Using these models, we can calculatethe contribution of feature fi to the improvement ac-cording to some evaluation function ec(fi) =e(p0..i)?
e(p0..i?1)e(p0..n)?
e(p0)(11)where p0..i is a model trained with the i most dis-59criminative features, p0 is the uniform model, andn = 300.5.1 Parse disambiguationTable 1 provides class-based counts of the 300 mostdiscriminative features for the parse disambiguationand reversible models.
Since the n-gram features arenot active during parse disambiguation, they are notselected for the parse disambiguation model.
Allother classes of features are used in the parse dis-ambiguation model.
The reversible model uses allclasses of features.Class Directional ReversibleDependency 93 84Lexical 24 24N-gram 0 2Rule 156 154Syntactic 27 36Table 1: Per-class counts of the best 300 features accord-ing to the grafting method.Contributions per feature class in parse disam-biguation are shown in table 2.
In the directionalparse disambiguation model, parsing-specific fea-tures (dependency and lexical) account for 55% ofthe improvement over the uniform model.In the reversible model, there is a shift of con-tribution towards task-independent features.
Whenapplying this model, the contribution of parsing-specific features to the improvement over the uni-form model is reduced to 45.79%.We can conclude from the per-class feature con-tributions in the directional parse disambiguationmodel and the reversible model, that the reversiblemodel does not put more emphasis on parsing-specific features.
Instead, the opposite is true: task-independent features are more important in the re-versible model than the directional model.5.2 Fluency rankingTable 3 provides class-based counts of the 300 mostdiscriminative features of the fluency ranking andreversible models.
During fluency ranking, depen-dency features and lexical features are not active.Table 4 shows the per-class contribution to theimprovement in accuracy for the directional and re-versible models.
Since the dependency and lexicalClass Directional ReversibleDependency 21.53 13.35Lexical 33.68 32.62N-gram 0.00 0.00Rule 37.61 47.35Syntactic 7.04 6.26Table 2: Per-class contribution to the improvement of themodel over the base baseline in parse disambiguation.Class Directional ReversibleDependency 0 84Lexical 0 24N-gram 2 2Rule 181 154Syntactic 117 36Table 3: Per-class counts of the best 300 features accord-ing to the grafting method.features are not active during fluency ranking, it maycome as a surprise that their contribution is nega-tive in the reversible model.
Since they are used forparse disambiguation, they have an effect on weightsof task-independent features.
This phenomenon didnot occur when using the reversible model for parsedisambiguation, because the features specific to flu-ency ranking (n-gram features) were selected as themost discriminative features in the reversible model.Consequently, the reversible models with one andtwo features were uniform models from the perspec-tive of parse disambiguation.Class Directional ReversibleDependency 0.00 -4.21Lexical 0.00 -1.49N-gram 81.39 83.41Rule 14.15 16.45Syntactic 3.66 4.59Table 4: Per-class contribution to the improvement of themodel over the baseline in fluency ranking.Since active features compensate for this loss inthe reversible model, we cannot directly compareper-class contributions.
To this end, we normalizethe contribution of all positively contributing fea-tures, leading to table 5.
Here, we can see that thereversible model does not shift more weight towardstask-specific features.
On the contrary, there is a60mild effect in the opposite direction here as well.Class Directional ReversibleN-gram 81.39 79.89Rule 14.15 15.75Syntactic 3.66 4.39Table 5: Classes giving a net positive distribution, withnormalized contributions.6 Qualitative analysis of reversible modelsWhile the quantitative evaluation shows that task-independent features remain important in reversiblemodels, we also want to get an insight into the ac-tual features that were used.
Since it is unfeasible tostudy the 300 best features in detail, we extract the20 best features.Grafting-10 is too course-grained for this task,since it selects the first 10 features solely by theirgradients, while there may be overlap in those fea-tures.
To get the most accurate list possible, we per-form grafting-1 selection to extract the 20 most ef-fective features.
We show these features in table 6with their polarities.
The polarity indicates whethera feature is an indicator for a good (+) or bad (-)derivation.We now provide a description of these features bycategory.Word/tag trigrams.
The most effective featuresin fluency ranking are the n-gram auxiliary distribu-tions (1, 3).
The word n-gram model settles prefer-ences with respect to fixed expressions and commonword orders.
It also functions as a (probabilistic)filter of archaic inflections and incorrect inflectionsthat are not known to the Alpino lexicon.
The tagn-gram model help picking a sequence of part-of-speech tags that is plausible.Frame selection.
Various features assist in theselection of proper subcategorization frames forwords.
This currently affects parse disambiguationmostly.
There is virtually no ambiguity of framesduring generation, and a stem/frame combinationnormally only selects one inflection.
The most ef-fective feature for frame selection is (2), which isan auxiliary distribution of words and correspond-ing frames based on a large automatically annotatedRank Polarity Feature1 + ngram lm2 + z f23 + ngram tag4 - r1(np n)5 + r2(np det n,2,n n pps)6 - p1(pardepth)7 + r2(vp mod v,3,vproj vc)8 - r2(vp arg v(np),2,vproj vc)9 - f1(adj)10 + r2(vp mod v,2,optpunct(e))11 - s1(non subj np topic)12 + r1(n adj n)13 + dep23(prep,hd/pc,verb)14 + r1(optpunct(e))15 + dep34(van,prep,hd/mod,noun)16 + dep23(noun,hd/su,verb)17 + p1(par)18 - r1(vp v mod)19 + dep23(prep,hd/mod,verb)20 - f1(verb(intransitive))Table 6: The twenty most discriminative features of thereversible model, and their polarities.corpus.
Other effective features indicate that read-ings as an adjective (9) and as an intransitive verb(20) are not preferred.Modifiers.
Feature 5 indicates the preference toattach prepositional phrases to noun phrases.
How-ever, if a modifier is attached to a verb, we preferreadings and realizations where the modifier is left-adjoining rather than right-adjoining (7, 18, 19).
Forinstance, zij heeft met de hond gelopen (she has withthe dog walked) is more fluent than zij heeft gelopenmet de hond (she has walked with the dog).
Finally,feature 15 gives preference to analyses where thepreposition van is a modifier of a noun.Conjunctions.
Two of the twenty most discrimi-native features involve conjunctions.
The first (6)is a dispreference for conjunctions where conjunctshave a varying depth.
In conjunctions, the modelprefers derivations where all conjuncts in a con-junctions have an equal depth.
The other feature(17) gives a preferences to conjunctions with paral-lel conjuncts ?
conjunctions where every conjunctis constructed using the same grammar rule.61Punctuation.
The Alpino grammar is very gen-erous in allowing optional punctuation.
An emptypunctuation sign is used to fill grammar rule slotswhen no punctuation is used or realized.
Two fea-tures indicate preferences with respect to optionalpunctuation.
The first (10) gives preference to fillingthe second daughter slot of the vp mod v with theempty punctuation sign.
This implies that deriva-tions are preferred where a modifier and a verb arenot separated by punctuation.
The second feature(14) indicates a general preference for the occur-rence of empty optional punctuation in the deriva-tion tree.Subjects/objects.
In Dutch, subject fronting ispreferred over object fronting.
For instance, Spanjewon de wereldbeker (Spain won the World Cup)is preferred over de wereldbeker won Spanje (theWorld Cup won spain).
Feature 8 will in many casescontribute to the preference of having topicalizednoun phrase subjects.
It disprefers having a nounphrase left of the verb.
For example, zij heeft met dehond gelopen (she has with the dog walked) is pre-ferred over met de hond heeft zij gelopen (with thedog she has walked).
Feature 11 encodes the prefer-ence for subject fronting, by penalizing derivationswhere the topic is a non-subject noun phrase.Other syntactic preferences.
The remainingfeatures are syntactic preferences that do notbelong to any of the previous categories.
Feature4 indicates a dispreference for derivations wherebare nouns occur.
Feature 12 indicates a preferencefor derivations where a noun occurs along withan adjective.
Finally, feature 13 gives preferenceto the prepositional complement (pc) relation if apreposition is a dependent of a verb and lexicalanalysis shows that the verb can combine with thatprepositional complement.We can conclude from this description of fea-tures that many of the features that are paramountto parse disambiguation and fluency ranking aretask-independent, modeling phenomena such assubject/object fronting, modifier adjoining, paral-lelism and depth in conjunctions, and the use ofpunctuation.7 ConclusionIn this work we have used feature selection tech-niques for maximum entropy modeling to analyzethe hypothesis that the models in reversible stochas-tic attribute-value grammars use task-independentfeatures.
To this end, we have first comparedthree feature selection techniques, namely gain-informed selection, grafting, and grafting-light.
Inthis comparison we see that grafting outperformsboth grafting-light and gain-informed selection inparse disambiguation and fluency ranking tasks.We then used grafting to select the most effectivefeatures for parse disambiguation, fluency ranking,and reversible models.
In the quantitative analysiswe have shown that the reversible model does notput more emphasis on task-specific features.
In fact,the opposite is true: in the reversible model task-independent features become more defining than inthe directional models.We have also provided a qualitative analysis of thetwenty most effective features, showing that many ofthese features are relevant to both parsing and gener-ation.
Effective task-independent features for Dutchmodel phenomena such as subject/object fronting,modifier adjoining, parallelism and depth in con-junctions, and the use of punctuation.8 Future workAn approach for testing the reversibility of mod-els that we have not touched upon in this work, isto evaluate such models using tasks that combineparsing and generation.
For instance, a good wordgraph parser should choose a fluent sentence with asyntactically plausible reading.
If reversible modelsintegrate parsing-specific, generation-specific, andtask-independent features properly, they should becompetitive to models specifically trained for thattask.
In the future, we hope to evaluate reversiblestochastic attribute-value grammars in the light ofsuch tasks.9 AcknowledgmentsThis work was funded by the DAISY project of theSTEVIN program.
The author would also like tothank Yan Zhao, Barbara Plank, and Gertjan van No-ord for the many valuable discussions on maximumentropy modeling and feature selection.62ReferencesAdam L. Berger, Stephen A. Della Pietra, and Vincent J.Della Pietra.
1996.
A maximum entropy approach tonatural language processing.
Computational linguis-tics, 22(1):71.Aoife Cahill, Martin Forst, and Christian Rohrer.
2007.Designing features for parse disambiguation and real-isation ranking.
In The Proceedings of the LFG ?07Conference, pages 128?147.
CSLI Publications.Aoife Cahill.
2009.
Correlating human and automaticevaluation of a german surface realiser.
In Proceed-ings of the ACL-IJCNLP 2009 Conference - Short Pa-pers, pages 97?100.Danie?l de Kok and Gertjan van Noord.
2010.
A sentencegenerator for Dutch.
In Proceedings of the 20th Com-putational Linguistics in the Netherlands conference(CLIN), pages 75?90.Danie?l de Kok, Barbara Plank, and Gertjan van Noord.2011.
Reversible stochastic attribute-value grammars.In Proceedings of the ACL HLT 2011 Conference -Short Papers.Danie?l de Kok.
2010.
Feature selection for fluency rank-ing.
In Proceedings of the 6th International NaturalLanguage Generation Conference (INLG), pages 155?163.Mark Johnson and Stefan Riezler.
2000.
Exploitingauxiliary distributions in stochastic unification-basedgrammars.
In Proceedings of the 1st Meeting of theNorth American Chapter of the Association for Com-putational Linguistics, pages 154?161, Seattle, Wash-ington.Robert Malouf and Gertjan van Noord.
2004.
Widecoverage parsing with stochastic attribute value gram-mars.
In IJCNLP-04 Workshop: Beyond shallow anal-yses - Formalisms and statistical modeling for deepanalyses.
JST CREST, March.I.
Dan Melamed, Ryan Green, and Joseph Turian.
2003.Precision and recall of machine translation.
In HLT-NAACL.Miles Osborne.
2000.
Estimation of stochastic attribute-value grammars using an informative sample.
In Pro-ceedings of the 18th conference on Computational lin-guistics (COLING), pages 586?592.Simon Perkins, Kevin Lacker, and James Theiler.
2003.Grafting: Fast, incremental feature selection by gradi-ent descent in function space.
The Journal of MachineLearning Research, 3:1333?1356.Stefan Riezler and Alexander Vasserman.
2004.
Incre-mental feature selection and l1 regularization for re-laxed maximum-entropy modeling.
In Proceedings ofthe 2004 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP04), Barcelona, Spain.Robert Tibshirani.
1996.
Regression shrinkage and se-lection via the Lasso.
Journal of the Royal StatisticalSociety.
Series B (Methodological), pages 267?288.Leonoor van der Beek, Gosse Bouma, Robert Malouf,and Gertjan van Noord.
2002.
The Alpino depen-dency treebank.
In Computational Linguistics in theNetherlands (CLIN).Gertjan van Noord, Ineke Schuurman, and GosseBouma.
2010.
Lassy syntactische annotatie,revision 19053. http://www.let.rug.nl/vannoord/Lassy/sa-man_lassy.pdf.Gertjan van Noord.
2006.
At Last Parsing Is NowOperational.
In TALN 2006 Verbum Ex Machina,Actes De La 13e Conference sur Le Traitement Au-tomatique des Langues naturelles, pages 20?42, Leu-ven.Gertjan van Noord.
2007.
Using self-trained bilexicalpreferences to improve disambiguation accuracy.
InProceedings of the International Workshop on Pars-ing Technology (IWPT), ACL 2007 Workshop, pages1?10, Prague.
Association for Computational Linguis-tics, ACL.Yaqian Zhou, Lide Wu, Fuliang Weng, and HaukeSchmidt.
2003.
A fast algorithm for feature se-lection in conditional maximum entropy modeling.In Proceedings of the 2003 conference on Empiricalmethods in natural language processing, EMNLP ?03,pages 153?159, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Jun Zhu, Ni Lao, and Eric P. Xing.
2010.
Grafting-light: fast, incremental feature selection and structurelearning of Markov random fields.
In Proceedings ofthe 16th ACM SIGKDD international conference onKnowledge discovery and data mining, pages 303?312.
ACM.63
