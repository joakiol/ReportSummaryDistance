Proceedings of the 2014 Joint Meeting of SIGMORPHON and SIGFSM, pages 64?68,Baltimore, Maryland USA, June 27 2014. c?2014 Association for Computational Linguistics10 Open Questions in Computational MorphonologyGrzegorz KondrakDepartment of Computing ScienceUniversity of Albertagkondrak@ualberta.caAbstractThe objective of this paper is to initi-ate discussion within the SIGMORPHONcommunity around several issues that in-volve computational morphology, phonol-ogy, phonetics, orthography, syllabifica-tion, transliteration, machine translation,inflection generation, and native languageidentification.1 Morphology in Machine TranslationIn contrast with English, which is a morpho-logically simple language, many languages havedozens of different wordforms for any givenlemma, some of which are unattested even inlarge monolingual corpora.
In Statistical MachineTranslation (SMT), lexical sparsity in such lan-guages is often addressed by performing morpho-logical segmentation, which simplifies the cor-respondence between the tokens in the sourceand target language.
When translating into En-glish from a morphologically complex language,the segmentation is a form of preprocessing per-formed before the the translation process.
Sincethe English words are not segmented, the outputof the decoder can be directly compared to thereference translation.
However, when translatingin the opposite direction, the segmentation mustbe reversed to make the generated text readable.Desegmentation is typically performed as a post-processing step that is independent from the de-coding process.
Unfortunately, the pipeline ap-proach may prevent the desegmenter from recov-ering from errors made by the decoder, includingoutput morpheme sequences that cannot be com-bined into valid words.Salameh et al.
(2014) propose to replace thepipeline approach with a solution inspired byfinite-state methods.
They perform desegmenta-tion directly on the search graph of a phrase-baseddecoder, which is represented as a lattice encodinga large set of possible decoder outputs.
The lattice,which can be interpreted as a finite?state accep-tor over target strings, is composed with a deseg-menting transducer which consumes morphemesand outputs desegmented words.
The desegment-ing transducer, in turn, is constructed from a ta-ble that maps morpheme sequences to words.
Thelattice desegmentation algorithm effectively com-bines both segmented and desegmented views ofthe target language, and allows for inclusion offeatures related to the desegmentation process, aswell as an unsegmented language model.
The re-sults on English-to-Arabic indicate significant im-provements in translation quality.
However, themorphology of Arabic is largely concatenative,with relatively simple morpheme-boundary adap-tations.
In contrast, many European languages areclassified as inflecting, with affixes that representseveral rather than a single morpheme.
The ques-tion remains whether a morphologically-aware ap-proach can be developed to improve translationinto inflecting languages as well.2 Inflection GenerationAn alternative to the morphological segmentationapproach is to reduce the diverse forms in thetraining bitext to lemmas, and, at test time, re-construct the wordforms in the target language di-rectly from lemmas annotated with morphologicalfeatures.
Note that the wordforms that have notbeen seen in training pose a problem for languagemodels, and are typically shunned by the currentSMT systems.Although complex morphology leads to a hightype-to-token ratio, words tend fo fall into certaininflectional paradigms.
Individual inflections areobtained by combining a specific affix with a stem.These combinations are rarely concatenative, of-ten affecting characters at the end or even in themiddle of a stem.64For languages without hand-built morphologi-cal analyzers and generators, automated learningof morphological paradigms is the only option.Dreyer and Eisner (2011) propose a Dirichlet pro-cess mixture model and loopy belief propagationto learn complete paradigms starting from an ini-tial small set of seed paradigms.
An unannotatedcorpus is utilized to guide the predictions of themodel by reducing the likelihood of generatingunseen wordforms.
Durrett and DeNero (2013)align the lemmas with inflected forms to identifyspans that change for the inflections, and learn ex-plicit rules for applying those changes in contextsin which they appear.
Their joint model is aware ofcomplete paradigms, and is able to correct errorsmade on individual inflections.Nicolai et al.
(2014) train a discriminativestring transducer on lemma-inflection pairs, andapply a separate re-ranking step to take advan-tage of the paradigmatic constraints.
In spite ofits relative simplicity, their string transduction ap-proach outperforms the previous approaches tolearning morphological paradigms on several Eu-ropean languages.
The question remains whetherthe string transduction approach is also superior tomore complex methods on languages with differ-ent morphological systems.3 From Syntax to MorphologyIn some languages, syntactic function of phrases ismainly marked by word position and prepositions,while other languages rely on morphology to agreater degree.
Similarly, verbal attributes such astense, person, and gender, can be either encodedmorphologically or lexically.
Chahuneau et al.
(2013) propose a discriminative model for trans-lating into morphologically rich languages thatpredicts inflections of target words from source-side annotations that include POS tags, depen-dency parses, and semantic clusters.
In otherwords, they exploit the syntax of the source lan-guage to select the most likely wordforms in thetarget language,The open question in this case is whether in-stead of learning a prediction model separatelyfor each language pair, the morphological featurescould be mapped directly on the source words.
Forexample, in the phrase she would have asked, theactual morphological marking is minimal, but thecontext disambiguates the person, number, gender,and aspect of the verb.
Explicit morphological an-notation could not only help machine translation,but also provide a rich source of information in themonolingual context, which would go well beyondPOS tagging.4 Transliteration and MorphologyTransliteration is sometimes defined as ?phonetictranslation?
(Knight and Graehl, 1997).
In fact, itis straightforward to train a transliteration modelusing SMT toolkits by treating individual char-acters as words, and words as sentences.
How-ever, unless substantial modifications are made,the accuracy of such a system will be mediocre.Transliteration needs a dedicated approach in or-der to fully exploit the source-side context andother constraints.The way we define tasks in NLP is important,because the definitions (and shared tasks) tend toguide research in a particular direction.
New pa-pers are expected to show improvement over pre-viously published results, preferably on alreadyestablished benchmarks.
Redefining a task car-ries the risk of being interpreted as an attempt toavoid a fair experimental comparison, or as a mis-directed effort to investigate irrelevant problems.The NEWS Shared Task on Machine Translit-eration was held four times between 2009 and2012 (Zhang et al., 2012).
With the exceptionof the 2010 edition that included a transliterationmining task, the shared task was invariably de-fined in terms of learning transliteration modelsfrom the training sets of word pairs.
This frame-work seems to ignore the fact that many of thetransliteration target words can be found in mono-lingual corpora, in a marked contrast with theprevalent SMT practice of avoiding unseen words.Cherry and Suzuki (2009) show that the inclusionof a target lexicon dramatically improves translit-eration accuracy.
Unfortunately, the paper haslargely been ignored by the transliteration commu-nity (perhaps because it strays from the standardtask formulation), as well as the SMT community(perhaps because it shows only modest gains interms of BLEU score).Another drawback of limiting the training datato a list of name pairs is the lack of the con-text that is required to account for morphologi-cal alterations.
For example, the title of the Rus-sian Wikipedia page that corresponds to Pres-idency of Barack Obama back-transliterates asPresidentstvo Baraka Obamy, where the personal65name appears in the genetive case.
Simply in-cluding morphological variants in the training datawithout their context is likely to confuse a translit-eration model.
How to best combine translitera-tion with morphology remains an open question.5 Transliteration and OrthographyTransliteration is more than just phonetic transla-tion.
In the idealized model of Knight and Graehl(1997) a human transliterator pronounces a namein the source language, modifies the pronunciationto fit the target language phonology, and writesit down using the orthographic rules of the targetscript.
In reality, however, the source orthographystrongly influences the form of the transliteration.For example, the Russian transliteration of thename Dickens on Wikipedia back-transliterates asDikkens, although Dykynz would be much closerto the original pronunciation.
For less well-knownnames that first appear in English-language news,human transliterators are often in the dark becausethe correct pronunciation may be difficult to guessfrom the spelling.Al-Onaizan and Knight (2002) report that aspelling-based model outperforms a phonetic-based model even when pronunciations are ex-tracted from a pronunciation dictionary.
Bhargavaand Kondrak (2012) present a re-ranking approachthat is able to improve spelling-based models byconsulting the supplied pronunciations.
It remainsan open question how to design a superior jointmodel that would generate transliterations directlyfrom both spelling and pronunciation.6 Transliteration and DeciphermentAlthough transliteration is typically defined asconversion between writing scripts, the properform strongly depends on the particular target lan-guage with its phonological and orthographic con-straints.
For example, the name of the city thathosted the recent Winter Olympics is representedin various European languages as Sochi, Sotchi,Sotschi, Sotsji, Sotji, Sots?i, Soc?i, Soczi, Szocsi, etc.In order to derive language-specific transliterationmodels, we would need to collect training data forthousands of possible language pairs.Ravi and Knight (2009) introduce the task ofunsupervised transliteration without parallel re-sources.
They formulate the problem as decipher-ment, and reconstruct cross-lingual phoneme map-ping tables from Japanese words of English origin,achieving approximately 50% character accuracyon U.S. names written in the Katakana script.Hauer et al.
(2014) frame transliteration asa substitution cipher, and apply a mixture ofcharacter- and word-level language models to thedecipherment of a known language written in anunknown script.
The authors treat a short text inSerbian as enciphered Croatian, and attempt to re-cover the ?key?, which is the mapping between thecharacters in the two writing scripts.
In reality,Croatian and Serbian are distinct but closely re-lated languages, that are written in different scriptsand exhibit differences in both lexicon and gram-mar.
In particular, 30 Serbian Cyrillic characterscorrespond to 27 letters in Croatian Latin, withthree of the characters represented in the otherscript as digraphs (e.g., nj).
The deciphermenterror rate plateaus at about 3% at the ciphertextlength of 50 words.
In contrast, a pure frequency-based approach fails on this task with a mappingerror rate close to 90%.
The question remainswhether a more flexible approach could be appliedsuccessfully to unsupervised transliteration of lan-guages that are less closely related.7 Phonetic Similarity of TranslationsWords that are phonetically similar across differ-ent languages tend to be transliterations, or at leastshare the same origin.
For this reason, wordson two sides of a bitext are more likely to corre-spond to each other if they exhibit phonetic simi-larity (Kondrak, 2005).
This is true even for com-pletely unrelated languages because of the preva-lence of loanwords, proper names, and techni-cal terms.
Orthographic similarity, which reflectsphonetic similarity, has been exploited in the pastto improve word and sentence alignment in SMT,and other NLP tasks.Surprisingly, the correlation with phonetic sim-ilarity appears to hold for any translations, definedas words that express the same meaning in somecontext.
Kondrak (2013) observes that even afterall cognates and loanwords are removed from con-sideration, the similarity between the words fromdifferent languages for the same concept is signif-icantly higher on average than the similarity be-tween the words for different concepts (as mea-sured by the Longest Common Subsequence Ra-tio).
This seems to contradict the Saussurean prin-ciple of the arbitrariness of the linguistic sign.Kondrak (2013) proposes to explain this phe-66nomenon by positing a chain of correlations be-tween the following word characteristics: trans-latability, frequency, length, and similarity.
Thekey observation is that translations are on aver-age closer in terms of their length than randomwords.
First, pairs of cross-lingual translations ex-hibit a correlation with respect to the logarithm oftheir frequencies.
Intuitively, translations refer tothe same semantic concepts, which tend to be ex-pressed with similar frequency across languages.Second, the connection between word frequencyand length is well established (Zipf, 1936).
Fi-nally, pairs of words that differ in length are lesslikely to be considered similar, which is reflectedby word similarity measures.
In summary, the rea-son for the greater phonetic similarity of trans-lations lies in the similarity of their frequencies,which is reflected by the similarity of their lengths.This hypothesis remains to be verified on otherlanguages and data sets.8 L1 Phonology in L2The task of Native Language Identification (NLI)is to determine the first language (L1) of the writerof a text in another language (L2) (Tetreault etal., 2013).
Koppel et al.
(2005) report 80% ac-curacy in classifying a set of English texts intofive L1 languages using a multi-class linear SVMwith features including function words, POS bi-grams, and character n-grams.
Tsur and Rap-poport (2007) observe that limiting the set of fea-tures to the relative frequency of the 200 most fre-quent character bigrams yields a respectable ac-curacy of about 65%.
They interpret this as evi-dence that the choice of words in L2 is stronglyinfluenced by the phonology of L1.
As the orthog-raphy of alphabetic languages is representative oftheir phonology, character bigrams appear to cap-ture these phonetic preferences.In order to test the above hypothesis, Nico-lai and Kondrak (2014) design an algorithm toidentify the most discriminative words and thecorresponding character bigrams.
They find thatthe removal of such words results in a substan-tial drop in the accuracy of the classifier that isbased exclusively on character bigrams, and thatthe majority of the most indicative character bi-grams are common among different language sets.They conclude that the effectiveness of a bigram-based classifier in identifying the native languageof a writer is primarily driven by the relative fre-quency of words rather than by the influence ofthe phonology of L1.
Although this provides ev-idence against the hypothesis of Tsur and Rap-poport (2007), the question to what degree the L1phonology affects L2 writing remains open.9 English OrthographyThe English spelling system is notorious for itsirregularity.
Kominek and Black (2006) estimatethat it is about 3 times more complex than German,and 40 times more complex than Spanish.
This isconfirmed by lower accuracy of letter-to-phonemesystems on English (Bisani and Ney, 2008).
Asurvey of English spelling (Carney, 1994) devotes120 pages to describe phoneme-to-letter corre-spondences, and lists 226 letter-to-phoneme rules,almost all of which admit exceptions.In view of this, the claim of Chomsky and Halle(1968) that English orthography is ?close to opti-mal?
could be interpreted as facetious.
The ques-tion is how we could validate the accuracy of thisstatement from the computational perspective.
Itwould seem to require answering at least the fol-lowing three questions: (a) what is the optimal or-thography for English, (b) how to measure the dis-tance between alternative orthographies, and (c)what distance should be considered ?close?.10 Syllabification and MorphologyOrthographic syllabification of words is some-times referred to as hyphenation.
Bartlett et al.
(2008) propose a sequence prediction approach tosyllabify out-of-dictionary words based on lettern-gram features.
Despite its high accuracy, theirsystem suffers from the lack of awareness of com-pound nouns and other morphological phenom-ena.
For example, hold-o-ver is incorrectly syl-labified as hol-dov-er.Yao and Kondrak (2014) demonstrate that theaccuracy of orthographic syllabification can beimproved by using morphological information.In particular, incorporating oracle morphologicalsegmentation substantially reduces the syllabifica-tion error rate on English and German.
If unsu-pervised segmentation is used instead, the errorreduction is smaller but still significant.
How-ever, they are unable to achieve any error reductionusing a supervised segmentation approach, eventhough it is much more accurate than the unsuper-vised approach.
The confirmation and explanationof this surprising result remains an open question.67ReferencesYaser Al-Onaizan and Kevin Knight.
2002.
Machinetransliteration of names in Arabic texts.
In Work-shop on Computational Approaches to Semitic Lan-guages.Susan Bartlett, Grzegorz Kondrak, and Colin Cherry.2008.
Automatic syllabification with structuredSVMs for letter-to-phoneme conversion.
In ACL,pages 568?576.Aditya Bhargava and Grzegorz Kondrak.
2012.
Lever-aging supplemental representations for sequentialtransduction.
In NAACL-HLT, pages 396?406.Maximilian Bisani and Hermann Ney.
2008.
Joint-sequence models for grapheme-to-phoneme conver-sion.
Speech Communication, 50(5):434?451.Edward Carney.
1994.
A Survey of English Spelling.Routledge.Victor Chahuneau, Eva Schlinger, Noah A. Smith, andChris Dyer.
2013.
Translating into morphologicallyrich languages with synthetic phrases.
In EMNLP,pages 1677?1687.Colin Cherry and Hisami Suzuki.
2009.
Discrim-inative substring decoding for transliteration.
InEMNLP, pages 1066?1075.Noam Chomsky and Morris Halle.
1968.
The SoundPattern of English.
Harper & Row.Markus Dreyer and Jason Eisner.
2011.
Discover-ing morphological paradigms from plain text using aDirichlet process mixture model.
In EMNLP, pages616?627.Greg Durrett and John DeNero.
2013.
Supervisedlearning of complete morphological paradigms.
InNAACL-HLT, pages 1185?1195.Bradley Hauer, Ryan Hayward, and Grzegorz Kondrak.2014.
Solving substitution ciphers with combinedlanguage models.
Submitted for publication.Kevin Knight and Jonathan Graehl.
1997.
Machinetransliteration.
In ACL, pages 128?135.John Kominek and Alan W. Black.
2006.
Learn-ing pronunciation dictionaries: Language complex-ity and word selection strategies.
In HLT-NAACL,pages 232?239.Grzegorz Kondrak.
2005.
Cognates and word align-ment in bitexts.
In MT Summit, pages 305?312.Grzegorz Kondrak.
2013.
Word similarity, cogna-tion, and translational equivalence.
In Lars Borinand Anju Saxena, editors, Approaches to MeasuringLinguistic Differences, pages 375?386.
De GruyterMouton.Moshe Koppel, Jonathan Schler, and Kfir Zigdon.2005.
Determining an author?s native language bymining a text for errors.
In SIGKDD, pages 624?628.Garrett Nicolai and Grzegorz Kondrak.
2014.
Doesthe phonology of L1 show up in L2 texts?
In ACL.Garret Nicolai et al.
2014.
In preparation.Sujith Ravi and Kevin Knight.
2009.
Learningphoneme mappings for transliteration without par-allel data.
In NAACL, pages 37?45.Mohammad Salameh, Colin Cherry, and GrzegorzKondrak.
2014.
Lattice desegmentation for statis-tical machine translation.
In ACL.Joel Tetreault, Daniel Blanchard, and Aoife Cahill.2013.
A Report on the First Native Language Iden-tification Shared Task.
In Workshop on InnovativeUse of NLP for Building Educational Applications(BEA8).Oren Tsur and Ari Rappoport.
2007.
Using classifierfeatures for studying the effect of native languageon the choice of written second language words.
InWorkshop on Cognitive Aspects of ComputationalLanguage Acquisition, pages 9?16.Lei Yao and Grzegorz Kondrak.
2014.
In preparation.Min Zhang, Haizhou Li, A Kumaran, and Ming Liu.2012.
Report of NEWS 2012 machine transliter-ation shared task.
In 4th Named Entity Workshop,pages 10?20.George Zipf.
1936.
The Psychobiology of Language.Routledge.68
