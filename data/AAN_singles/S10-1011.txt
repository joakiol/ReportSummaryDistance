Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 63?68,Uppsala, Sweden, 15-16 July 2010.c?2010 Association for Computational LinguisticsSemEval-2010 Task 14: Word Sense Induction & DisambiguationSuresh ManandharDepartment of Computer ScienceUniversity of York, UKIoannis P. KlapaftisDepartment of Computer ScienceUniversity of York, UKDmitriy DligachDepartment of Computer ScienceUniversity of Colorado, USASameer S. PradhanBBN TechnologiesCambridge, USAAbstractThis paper presents the description andevaluation framework of SemEval-2010Word Sense Induction & Disambiguationtask, as well as the evaluation results of 26participating systems.
In this task, partici-pants were required to induce the senses of100 target words using a training set, andthen disambiguate unseen instances of thesame words using the induced senses.
Sys-tems?
answers were evaluated in: (1) anunsupervised manner by using two clus-tering evaluation measures, and (2) a su-pervised manner in a WSD task.1 IntroductionWord senses are more beneficial than simple wordforms for a variety of tasks including InformationRetrieval, Machine Translation and others (Panteland Lin, 2002).
However, word senses are usuallyrepresented as a fixed-list of definitions of a manu-ally constructed lexical database.
Several deficien-cies are caused by this representation, e.g.
lexicaldatabases miss main domain-specific senses (Pan-tel and Lin, 2002), they often contain general defi-nitions and suffer from the lack of explicit seman-tic or contextual links between concepts (Agirreet al, 2001).
More importantly, the definitions ofhand-crafted lexical databases often do not reflectthe exact meaning of a target word in a given con-text (V?eronis, 2004).Unsupervised Word Sense Induction (WSI)aims to overcome these limitations of hand-constructed lexicons by learning the senses of atarget word directly from text without relying onany hand-crafted resources.
The primary aim ofSemEval-2010 WSI task is to allow comparisonof unsupervised word sense induction and disam-biguation systems.The target word dataset consists of 100 words,50 nouns and 50 verbs.
For each target word, par-ticipants were provided with a training set in or-der to learn the senses of that word.
In the nextstep, participating systems were asked to disam-biguate unseen instances of the same words usingtheir learned senses.
The answers of the systemswere then sent to organisers for evaluation.2 Task descriptionFigure 1 provides an overview of the task.
Ascan be observed, the task consisted of threeseparate phases.
In the first phase, train-ing phase, participating systems were providedwith a training dataset that consisted of aset of target word (noun/verb) instances (sen-tences/paragraphs).
Participants were then askedto use this training dataset to induce the sensesof the target word.
No other resources were al-lowed with the exception of NLP components formorphology and syntax.
In the second phase,testing phase, participating systems were pro-vided with a testing dataset that consisted of aset of target word (noun/verb) instances (sen-tences/paragraphs).
Participants were then askedto tag (disambiguate) each testing instance withthe senses induced during the training phase.
Inthe third and final phase, the tagged test instanceswere received by the organisers in order to evalu-ate the answers of the systems in a supervised andan unsupervised framework.
Table 1 shows the to-tal number of target word instances in the trainingand testing set, as well as the average number ofsenses in the gold standard.The main difference of the SemEval-2010 ascompared to the SemEval-2007 sense inductiontask is that the training and testing data are treatedseparately, i.e the testing data are only used forsense tagging, while the training data are only used63Figure 1: Training, testing and evaluation phases of SemEval-2010 Task 14Training set Testing set Senses (#)All 879807 8915 3.79Nouns 716945 5285 4.46Verbs 162862 3630 3.12Table 1: Training & testing set detailsfor sense induction.
Treating the testing data asnew unseen instances ensures a realistic evalua-tion that allows to evaluate the clustering modelsof each participating system.The evaluation framework of SemEval-2010WSI task considered two types of evaluation.In the first one, unsupervised evaluation, sys-tems?
answers were evaluated according to: (1) V-Measure (Rosenberg and Hirschberg, 2007), and(2) paired F-Score (Artiles et al, 2009).
Nei-ther of these measures were used in the SemEval-2007 WSI task.
Manandhar & Klapaftis (2009)provide more details on the choice of this evalu-ation setting and its differences with the previousevaluation.
The second type of evaluation, super-vised evaluation, follows the supervised evalua-tion of the SemEval-2007 WSI task (Agirre andSoroa, 2007).
In this evaluation, induced sensesare mapped to gold standard senses using a map-ping corpus, and systems are then evaluated in astandard WSD task.2.1 Training datasetThe target word dataset consisted of 100 words,i.e.
50 nouns and 50 verbs.
The training datasetfor each target noun or verb was created by follow-ing a web-based semi-automatic method, similarto the method for the construction of Topic Signa-tures (Agirre et al, 2001).
Specifically, for eachWordNet (Fellbaum, 1998) sense of a target word,we created a query of the following form:<Target Word> AND <Relative Set>The <Target Word> consisted of the targetword stem.
The <Relative Set> consisted of adisjunctive set of word lemmas that were relatedWord QuerySenseSense 1 failure AND (loss OR nonconformity OR testOR surrender OR ?force play?
OR ...)Sense 2 failure AND (ruination OR flop OR bustOR stall OR ruin OR walloping OR ...)Table 2: Training set creation: example queries fortarget word failureto the target word sense for which the query wascreated.
The relations considered were WordNet?shypernyms, hyponyms, synonyms, meronyms andholonyms.
Each query was manually checked byone of the organisers to remove ambiguous words.The following example shows the query createdfor the first1and second2WordNet sense of thetarget noun failure.The created queries were issued to Yahoo!search API3and for each query a maximum of1000 pages were downloaded.
For each page weextracted fragments of text that occurred in <p></p> html tags and contained the target wordstem.
In the final stage, each extracted fragment oftext was POS-tagged using the Genia tagger (Tsu-ruoka and Tsujii, 2005) and was only retained, ifthe POS of the target word in the extracted textmatched the POS of the target word in our dataset.2.2 Testing datasetThe testing dataset consisted of instances of thesame target words from the training dataset.
Thisdataset is part of OntoNotes (Hovy et al, 2006).We used the sense-tagged dataset in which sen-tences containing target word instances are taggedwith OntoNotes (Hovy et al, 2006) senses.
Thetexts come from various news sources includingCNN, ABC and others.1An act that fails2An event that does not accomplish its intended purpose3http://developer.yahoo.com/search/ [Access:10/04/2010]64G1G2G3C110 10 15C220 50 0C31 10 60C45 0 0Table 3: Clusters & GS senses matrix.3 Evaluation frameworkFor the purposes of this section we provide an ex-ample (Table 3) in which a target word has 181instances and 3 GS senses.
A system has gener-ated a clustering solution with 4 clusters coveringall instances.
Table 3 shows the number of com-mon instances between clusters and GS senses.3.1 Unsupervised evaluationThis section presents the measures of unsuper-vised evaluation, i.e V-Measure (Rosenberg andHirschberg, 2007) and (2) paired F-Score (Artileset al, 2009).3.1.1 V-Measure evaluationLet w be a target word with N instances (datapoints) in the testing dataset.
Let K = {Cj|j =1 .
.
.
n} be a set of automatically generated clus-ters grouping these instances, and S = {Gi|i =1 .
.
.m} the set of gold standard classes contain-ing the desirable groupings of w instances.V-Measure (Rosenberg and Hirschberg, 2007)assesses the quality of a clustering solution by ex-plicitly measuring its homogeneity and its com-pleteness.
Homogeneity refers to the degree thateach cluster consists of data points primarily be-longing to a single GS class, while completenessrefers to the degree that each GS class consists ofdata points primarily assigned to a single cluster(Rosenberg and Hirschberg, 2007).
Let h be ho-mogeneity and c completeness.
V-Measure is theharmonic mean of h and c, i.e.
VM =2?h?ch+c.Homogeneity.
The homogeneity, h, of a clus-tering solution is defined in Formula 1, whereH(S|K) is the conditional entropy of the classdistribution given the proposed clustering andH(S) is the class entropy.h ={1, if H(S) = 01?H(S|K)H(S), otherwise(1)H(S) = ?|S|?i=1?|K|j=1aijNlog?|K|j=1aijN(2)H(S|K) = ?|K|?j=1|S|?i=1aijNlogaij?|S|k=1akj(3)When H(S|K) is 0, the solution is perfectlyhomogeneous, because each cluster only containsdata points that belong to a single class.
How-ever in an imperfect situation, H(S|K) dependson the size of the dataset and the distribution ofclass sizes.
Hence, instead of taking the raw con-ditional entropy, V-Measure normalises it by themaximum reduction in entropy the clustering in-formation could provide, i.e.
H(S).
When thereis only a single class (H(S) = 0), any clusteringwould produce a perfectly homogeneous solution.Completeness.
Symmetrically to homogeneity,the completeness, c, of a clustering solution is de-fined in Formula 4, where H(K|S) is the condi-tional entropy of the cluster distribution given theclass distribution and H(K) is the clustering en-tropy.
When H(K|S) is 0, the solution is perfectlycomplete, because all data points of a class belongto the same cluster.For the clustering example in Table 3, homo-geneity is equal to 0.404, completeness is equal to0.37 and V-Measure is equal to 0.386.c ={1, if H(K) = 01?H(K|S)H(K), otherwise(4)H(K) = ?|K|?j=1?|S|i=1aijNlog?|S|i=1aijN(5)H(K|S) = ?|S|?i=1|K|?j=1aijNlogaij?|K|k=1aik(6)3.1.2 Paired F-Score evaluationIn this evaluation, the clustering problem is trans-formed into a classification problem.
For eachcluster Ciwe generate(|Ci|2)instance pairs, where|Ci| is the total number of instances that belong tocluster Ci.
Similarly, for each GS class Giwe gen-erate(|Gi|2)instance pairs, where |Gi| is the totalnumber of instances that belong to GS class Gi.Let F (K) be the set of instance pairs that ex-ist in the automatically induced clusters and F (S)be the set of instance pairs that exist in the goldstandard.
Precision can be defined as the numberof common instance pairs between the two sets tothe total number of pairs in the clustering solu-tion (Equation 7), while recall can be defined asthe number of common instance pairs between thetwo sets to the total number of pairs in the gold65standard (Equation 8).
Finally, precision and re-call are combined to produce the harmonic mean(FS =2?P ?RP+R).P =|F (K) ?
F (S)||F (K)|(7)R =|F (K) ?
F (S)||F (S)|(8)For example in Table 3, we can generate(352)in-stance pairs for C1,(702)for C2,(712)for C3and(52)for C4, resulting in a total of 5505 instancepairs.
In the same vein, we can generate(362)in-stance pairs for G1,(702)for G2and(752)for G3.
Intotal, the GS classes contain 5820 instance pairs.There are 3435 common instance pairs, hence pre-cision is equal to 62.39%, recall is equal to 59.09%and paired F-Score is equal to 60.69%.3.2 Supervised evaluationIn this evaluation, the testing dataset is split into amapping and an evaluation corpus.
The first oneis used to map the automatically induced clustersto GS senses, while the second is used to evaluatemethods in a WSD setting.
This evaluation fol-lows the supervised evaluation of SemEval-2007WSI task (Agirre and Soroa, 2007), with the dif-ference that the reported results are an averageof 5 random splits.
This repeated random sam-pling was performed to avoid the problems of theSemEval-2007 WSI challenge, in which differentsplits were providing different system rankings.Let us consider the example in Table 3 and as-sume that this matrix has been created by using themapping corpus.
Table 3 shows that C1is morelikely to be associated with G3, C2is more likelyto be associated with G2, C3is more likely to beassociated with G3and C4is more likely to be as-sociated with G1.
This information can be utilisedto map the clusters to GS senses.Particularly, the matrix shown in Table 3 is nor-malised to produce a matrix M , in which eachentry depicts the estimated conditional probabil-ity P (Gi|Cj).
Given an instance I of tw fromthe evaluation corpus, a row cluster vector IC iscreated, in which each entry k corresponds to thescore assigned to Ckto be the winning cluster forinstance I .
The product of IC and M provides arow sense vector, IG, in which the highest scor-ing entry a denotes that Gais the winning sense.For example, if we produce the row cluster vector[C1= 0.8, C2= 0.1, C3= 0.1, C4= 0.0], andSystem VM (%) VM (%) VM (%) #Cl(All) (Nouns) (Verbs)Hermit 16.2 16.7 15.6 10.78UoY 15.7 20.6 8.5 11.54KSU KDD 15.7 18 12.4 17.5Duluth-WSI 9 11.4 5.7 4.15Duluth-WSI-SVD 9 11.4 5.7 4.15Duluth-R-110 8.6 8.6 8.5 9.71Duluth-WSI-Co 7.9 9.2 6 2.49KCDC-PCGD 7.8 7.3 8.4 2.9KCDC-PC 7.5 7.7 7.3 2.92KCDC-PC-2 7.1 7.7 6.1 2.93Duluth-Mix-Narrow-Gap 6.9 8 5.1 2.42KCDC-GD-2 6.9 6.1 8 2.82KCDC-GD 6.9 5.9 8.5 2.78Duluth-Mix-Narrow-PK2 6.8 7.8 5.5 2.68Duluth-MIX-PK2 5.6 5.8 5.2 2.66Duluth-R-15 5.3 5.4 5.1 4.97Duluth-WSI-Co-Gap 4.8 5.6 3.6 1.6Random 4.4 4.2 4.6 4Duluth-R-13 3.6 3.5 3.7 3Duluth-WSI-Gap 3.1 4.2 1.5 1.4Duluth-Mix-Gap 3 2.9 3 1.61Duluth-Mix-Uni-PK2 2.4 0.8 4.7 2.04Duluth-R-12 2.3 2.2 2.5 2KCDC-PT 1.9 1 3.1 1.5Duluth-Mix-Uni-Gap 1.4 0.2 3 1.39KCDC-GDC 7 6.2 7.8 2.83MFS 0 0 0 1Duluth-WSI-SVD-Gap 0 0 0.1 1.02Table 4: V-Measure unsupervised evaluationmultiply it with the normalised matrix of Table 3,then we would get a row sense vector in which G3would be the winning sense with a score equal to0.43.4 Evaluation resultsIn this section, we present the results of the 26systems along with two baselines.
The first base-line, Most Frequent Sense (MFS), groups all test-ing instances of a target word into one cluster.
Thesecond baseline, Random, randomly assigns an in-stance to one out of four clusters.
The numberof clusters of Random was chosen to be roughlyequal to the average number of senses in the GS.This baseline is executed five times and the resultsare averaged.4.1 Unsupervised evaluationTable 4 shows the V-Measure (VM) performanceof the 26 systems participating in the task.
The lastcolumn shows the number of induced clusters ofeach system in the test set.The MFS baseline has aV-Measure equal to 0, since by definition its com-pleteness is 1 and homogeneity is 0.
All systemsoutperform this baseline, apart from one, whoseV-Measure is equal to 0.
Regarding the Randombaseline, we observe that 17 perform better, whichindicates that they have learned useful informationbetter than chance.Table 4 also shows that V-Measure tends tofavour systems producing a higher number of clus-66System FS (%) FS (%) FS (%) #Cl(All) (Nouns) (Verbs)MFS 63.5 57.0 72.7 1Duluth-WSI-SVD-Gap 63.3 57.0 72.4 1.02KCDC-PT 61.8 56.4 69.7 1.5KCDC-GD 59.2 51.6 70.0 2.78Duluth-Mix-Gap 59.1 54.5 65.8 1.61Duluth-Mix-Uni-Gap 58.7 57.0 61.2 1.39KCDC-GD-2 58.2 50.4 69.3 2.82KCDC-GDC 57.3 48.5 70.0 2.83Duluth-Mix-Uni-PK2 56.6 57.1 55.9 2.04KCDC-PC 55.5 50.4 62.9 2.92KCDC-PC-2 54.7 49.7 61.7 2.93Duluth-WSI-Gap 53.7 53.4 53.9 1.4KCDC-PCGD 53.3 44.8 65.6 2.9Duluth-WSI-Co-Gap 52.6 53.3 51.5 1.6Duluth-MIX-PK2 50.4 51.7 48.3 2.66UoY 49.8 38.2 66.6 11.54Duluth-Mix-Narrow-Gap 49.7 47.4 51.3 2.42Duluth-WSI-Co 49.5 50.2 48.2 2.49Duluth-Mix-Narrow-PK2 47.8 37.1 48.2 2.68Duluth-R-12 47.8 44.3 52.6 2Duluth-WSI-SVD 41.1 37.1 46.7 4.15Duluth-WSI 41.1 37.1 46.7 4.15Duluth-R-13 38.4 36.2 41.5 3KSU KDD 36.9 24.6 54.7 17.5Random 31.9 30.4 34.1 4Duluth-R-15 27.6 26.7 28.9 4.97Hermit 26.7 24.4 30.1 10.78Duluth-R-110 16.1 15.8 16.4 9.71Table 5: Paired F-Score unsupervised evaluationters than the number of GS senses, although V-Measure does not increase monotonically with thenumber of clusters increasing.
For that reason,we introduced the second unsupervised evaluationmeasure (paired F-Score) that penalises systemswhen they produce: (1) a higher number of clus-ters (low recall) or (2) a lower number of clusters(low precision), than the GS number of senses.Table 5 shows the performance of systems us-ing the second unsupervised evaluation measure.In this evaluation, we observe that most of the sys-tems perform better than Random.
Despite that,none of the systems outperform the MFS baseline.It seems that systems generating a smaller numberof clusters than the GS number of senses are bi-ased towards the MFS, hence they are not able toperform better.
On the other hand, systems gen-erating a higher number of clusters are penalisedby this measure.
Systems generating a number ofclusters roughly the same as the GS tend to con-flate the GS senses lot more than the MFS.4.2 Supervised evaluation resultsTable 6 shows the results of this evaluation for a80-20 test set split, i.e.
80% for mapping and 20%for evaluation.
The last columns shows the aver-age number of GS senses identified by each sys-tem in the five splits of the evaluation datasets.Overall, 14 systems outperform the MFS, while 17of them perform better than Random.
The rankingof systems in nouns and verbs is different.
For in-System SR (%) SR (%) SR (%) #S(All) (Nouns) (Verbs)UoY 62.4 59.4 66.8 1.51Duluth-WSI 60.5 54.7 68.9 1.66Duluth-WSI-SVD 60.5 54.7 68.9 1.66Duluth-WSI-Co-Gap 60.3 54.1 68.6 1.19Duluth-WSI-Co 60.8 54.7 67.6 1.51Duluth-WSI-Gap 59.8 54.4 67.8 1.11KCDC-PC-2 59.8 54.1 68.0 1.21KCDC-PC 59.7 54.6 67.3 1.39KCDC-PCGD 59.5 53.3 68.6 1.47KCDC-GDC 59.1 53.4 67.4 1.34KCDC-GD 59.0 53.0 67.9 1.33KCDC-PT 58.9 53.1 67.4 1.08KCDC-GD-2 58.7 52.8 67.4 1.33Duluth-WSI-SVD-Gap 58.7 53.2 66.7 1.01MFS 58.7 53.2 66.6 1Duluth-R-12 58.5 53.1 66.4 1.25Hermit 58.3 53.6 65.3 2.06Duluth-R-13 58.0 52.3 66.4 1.46Random 57.3 51.5 65.7 1.53Duluth-R-15 56.8 50.9 65.3 1.61Duluth-Mix-Narrow-Gap 56.6 48.1 69.1 1.43Duluth-Mix-Narrow-PK2 56.1 47.5 68.7 1.41Duluth-R-110 54.8 48.3 64.2 1.94KSU KDD 52.2 46.6 60.3 1.69Duluth-MIX-PK2 51.6 41.1 67.0 1.23Duluth-Mix-Gap 50.6 40.0 66.0 1.01Duluth-Mix-Uni-PK2 19.3 1.8 44.8 0.62Duluth-Mix-Uni-Gap 18.7 1.6 43.8 0.56Table 6: Supervised recall (SR) (test set split:80%mapping, 20% evaluation)stance, the highest ranked system in nouns is UoY,while in verbs Duluth-Mix-Narrow-Gap.
It seemsthat depending on the part-of-speech of the targetword, different algorithms, features and parame-ters?
tuning have different impact.The supervised evaluation changes the distri-bution of clusters by mapping each cluster to aweighted vector of senses.
Hence, it can poten-tially favour systems generating a high number ofhomogeneous clusters.
For that reason, we applieda second testing set split, where 60% of the testingcorpus was used for mapping and 40% for eval-uation.
Reducing the size of the mapping corpusallows us to observe, whether the above statementis correct, since systems with a high number ofclusters would suffer from unreliable mapping.Table 7 shows the results of the second super-vised evaluation.
The ranking of participants didnot change significantly, i.e.
we observe only dif-ferent rankings among systems belonging to thesame participant.
Despite that, Table 7 also showsthat the reduction of the mapping corpus has a dif-ferent impact on systems generating a larger num-ber of clusters than the GS number of senses.For instance, UoY that generates 11.54 clustersoutperformed the MFS by 3.77% in the 80-20 splitand by 3.71% in the 60-40 split.
The reduction ofthe mapping corpus had a minimal impact on itsperformance.
In contrast, KSU KDD that gener-ates 17.5 clusters was below the MFS by 6.49%67System SR (%) SR (%) SR (%) #S(All) (Nouns) (Verbs)UoY 62.0 58.6 66.8 1.66Duluth-WSI-Co 60.1 54.6 68.1 1.56Duluth-WSI-Co-Gap 59.5 53.5 68.3 1.2Duluth-WSI-SVD 59.5 53.5 68.3 1.73Duluth-WSI 59.5 53.5 68.3 1.73Duluth-WSI-Gap 59.3 53.2 68.2 1.11KCDC-PCGD 59.1 52.6 68.6 1.54KCDC-PC-2 58.9 53.4 67.0 1.25KCDC-PC 58.9 53.6 66.6 1.44KCDC-GDC 58.3 52.1 67.3 1.41KCDC-GD 58.3 51.9 67.6 1.42MFS 58.3 52.5 66.7 1KCDC-PT 58.3 52.2 67.1 1.11Duluth-WSI-SVD-Gap 58.2 52.5 66.7 1.01KCDC-GD-2 57.9 51.7 67.0 1.44Duluth-R-12 57.7 51.7 66.4 1.27Duluth-R-13 57.6 51.1 67.0 1.48Hermit 57.3 52.5 64.2 2.27Duluth-R-15 56.5 50.0 66.1 1.76Random 56.5 50.2 65.7 1.65Duluth-Mix-Narrow-Gap 56.2 47.7 68.6 1.51Duluth-Mix-Narrow-PK2 55.7 46.9 68.5 1.51Duluth-R-110 53.6 46.7 63.6 2.18Duluth-MIX-PK2 50.5 39.7 66.1 1.31KSU KDD 50.4 44.3 59.4 1.92Duluth-Mix-Gap 49.8 38.9 65.6 1.04Duluth-Mix-Uni-PK2 19.1 1.8 44.4 0.63Duluth-Mix-Uni-Gap 18.9 1.5 44.2 0.56Table 7: Supervised recall (SR) (test set split:60%mapping, 40% evaluation)in the 80-20 split and by 7.83% in the 60-40 split.The reduction of the mapping corpus had a largerimpact in this case.
This result indicates that theperformance in this evaluation also depends on thedistribution of instances within the clusters.
Sys-tems generating a skewed distribution, in which asmall number of homogeneous clusters tag the ma-jority of instances and a larger number of clusterstag only a few instances, are likely to have a bet-ter performance than systems that produce a moreuniform distribution.5 ConclusionWe presented the description, evaluation frame-work and assessment of systems participating inthe SemEval-2010 sense induction task.
The eval-uation has shown that the current state-of-the-artlacks unbiased measures that objectively evaluateclustering.The results of systems have shown that theirperformance in the unsupervised and supervisedevaluation settings depends on cluster granularityalong with the distribution of instances within theclusters.
Our future work will focus on the assess-ment of sense induction on a task-oriented basis aswell as on clustering evaluation.AcknowledgementsWe gratefully acknowledge the support of the EUFP7 INDECT project, Grant No.
218086, the Na-tional Science Foundation Grant NSF-0715078,Consistent Criteria for Word Sense Disambigua-tion, and the GALE program of the Defense Ad-vanced Research Projects Agency, Contract No.HR0011-06-C-0022, a subcontract from the BBN-AGILE Team.ReferencesEneko Agirre and Aitor Soroa.
2007.
SemEval-2007Task 02: Evaluating Word Sense Induction and Dis-crimination Systems.
In Proceedings of SemEval-2007, pages 7?12, Prague, Czech Republic.
ACL.Eneko Agirre, Olatz Ansa, David Martinez, and EduardHovy.
2001.
Enriching Wordnet Concepts WithTopic Signatures.
ArXiv Computer Science e-prints.Javier Artiles, Enrique Amig?o, and Julio Gonzalo.2009.
The role of named entities in web peoplesearch.
In Proceedings of EMNLP, pages 534?542.ACL.Christiane Fellbaum.
1998.
Wordnet: An ElectronicLexical Database.
MIT Press, Cambridge, Mas-sachusetts, USA.Eduard Hovy, Mitchell Marcus, Martha Palmer, LanceRamshaw, and Ralph Weischedel.
2006.
Ontonotes:the 90% solution.
In Proceedings of NAACL, Com-panion Volume: Short Papers on XX, pages 57?60.ACL.Suresh Manandhar and Ioannis P. Klapaftis.
2009.Semeval-2010 Task 14: Evaluation Setting for WordSense Induction & Disambiguation Systems.
InDEW ?09: Proceedings of the Workshop on Se-mantic Evaluations: Recent Achievements and Fu-ture Directions, pages 117?122, Boulder, Colorado,USA.
ACL.Patrick Pantel and Dekang Lin.
2002.
DiscoveringWord Senses from Text.
In KDD ?02: Proceedingsof the 8th ACM SIGKDD Conference, pages 613?619, New York, NY, USA.
ACM.Andrew Rosenberg and Julia Hirschberg.
2007.
V-measure: A Conditional Entropy-based ExternalCluster Evaluation Measure.
In Proceedings of the2007 EMNLP-CoNLL Joint Conference, pages 410?420, Prague, Czech Republic.Yoshimasa Tsuruoka and Jun?
?chi Tsujii.
2005.
Bidi-rectional Inference With the Easiest-first Strategyfor Tagging Sequence Data.
In Proceedings ofthe HLT-EMNLP Joint Conference, pages 467?474,Morristown, NJ, USA.Jean V?eronis.
2004.
Hyperlex: Lexical Cartographyfor Information Retrieval.
Computer Speech & Lan-guage, 18(3):223?252.68
