Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 150?154,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsSimple extensions for a reparameterised IBM Model 2Douwe GellingDepartment of Computer ScienceThe University of Sheffieldd.gelling@shef.ac.ukTrevor CohnComputing and Information SystemsThe University of Melbournet.cohn@unimelb.edu.auAbstractA modification of a reparameterisation ofIBM Model 2 is presented, which makesthe model more flexible, and able to modela preference for aligning to words to eitherthe right or left, and take into account POStags on the target side of the corpus.
Weshow that this extension has a very smallimpact on training times, while obtain-ing better alignments in terms of BLEUscores.1 IntroductionWord alignment is at the basis of most statisticalmachine translation.
The models that are gener-ally used are often slow to train, and have a largenumber of parameters.
Dyer et al (2013) presenta simple reparameterization of IBM Model 2 thatis very fast to train, and achieves results similar toIBM Model 4.While this model is very effective, it also hasa very low number of parameters, and as suchdoesn?t have a large amount of expressive power.For one thing, it forces the model to consideralignments on both sides of the diagonal equallylikely.
However, it isn?t clear that this is the case,as for some languages an alignment to earlier orlater in the sentence (above or below the diagonal)could be common, due to word order differences.For example, when aligning to Dutch, it may becommon for one verb to be aligned near the end ofthe sentence that would be at the beginning in En-glish.
This would mean most of the other words inthe sentence would also align slightly away fromthe diagonal in one direction.
Figure 1 shows anexample sentence in which this happens.
Here, acircle denotes an alignment, and darker squaresare more likely under the alignment model.
Inthis case the modified Model 2 would simply makeboth directions equally likely, where we would re-ally like for only one direction to be more likely.Hij had de man gezienHehadseenthemanFigure 1: Visualization of aligned sentence pair inDutch and English, darker shaded squares have ahigher alignment probability under the model, acircle indicates a correct alignment.
The Englishsentence runs from bottom to top, the Dutch sen-tence left to Right.In some cases it could be that the prior probabilityfor a word alignment should be off the diagonal.Furthermore, it is common in word alignment totake word classes into account.
This is commonlyimplemented for the HMM alignment model aswell as Models 4 and 5.
Och and Ney (2003) showthat for larger corpora, using word classes leadsto lower Alignment Error Rate (AER).
This is notimplemented for Model 2, as it already has analignment model that is dependent on both sourceand target length, and the position in both sen-tences, and adding a dependency to word classeswould make the the Model even more prone tooverfitting than it already is.
However, using thereparameterization in (Dyer et al, 2013) wouldleave the model simple enough even with a rela-tively large amount of word classes.Figure 2 shows an example of how the modelextensions could benefit word alignment.
In theexample, all the Dutch words have a different150Hij had de man gezienHehadseenthemanFigure 2: Visualization of aligned sentence pair inDutch and English, darker shaded squares have ahigher alignment probability under the model, acircle indicates a correct alignment.
The Englishsentence runs from bottom to top, the Dutch sen-tence left to Right.word class, and so can have different gradients foralignment probability over the english words.
Ifthe model has learned that prepositions and nounsare more likely to align to words later in the sen-tence, it could have a lower lambda for both wordclasses, resulting in a less steep slope.
If we alsosplit lambda into two variables, we can get algn-ment probabilities as shown above for the Dutchword ?de?, where aligning to one side of the diag-onal is made more likely for some word classes.Finally, instead of just having one side of the di-agonal less steep than the other, it may be usefulto instead move the peak of the alignment prob-ability function off the diagonal, while keeping itequally likely.
In Figure 2, this is done for the pastparticiple ?gezien?.We will present a simple model for adding theabove extensions to achieve the above (splittingthe parameter, adding an offset and conditioningthe parameters on the POS tag of the target word)in section 2, results on a set of experiments in sec-tion 3 and present our conclusions in section 4.2 MethodsWe make use of a modified version of Model 2,from Dyer et al (2013), which has an alignmentmodel that is parameterised in its original formsolely on the variable ?.
Specifically, the proba-bility of a sentence e given a sentence f is givenas:m?i=1n?j=0?
(ai|i,m, n) ?
?
(ei|fai)here, m is the length of the target sentence e, nthe same for source sentence f , ?
is the alignmentmodel and ?
is the translation model.
In this pa-per we are mainly concerned with the alignmentmodel ?.
In the original formulation (with a minortweak to ensure symmetry through the center), thisfunction is defined as:?
(ai= j|i,m, n) =??????
?p0j = 0(1?
p0) ?eh(i,j,m,n)Z(i,m,n)0 < j ?
n0 otherwisewhere, h(?)
is defined ash(i, j,m, n) = ?????
?im+ 1?jn+ 1???
?and Z?
(i,m, n) is?nj?=1e?h(i,j?,m,n), i.e.
anormalising function.
Like the original Model 2(Brown et al, 1993), this model is trained us-ing Expectation-Maximisation.
However, it is notpossible to directly update the ?
parameter duringtraining, as it cannot be computed analytically.
In-stead, a gradient-based approach is used during theM-step.Two different optimisations are employed, thefirst of which is used for calculating Z?.
Thisfunction forms a geometric series away from thediagonal (for each target word), which can becomputed efficiently for each of the directionsfrom the diagonal.
The second is used during theM-step when computing the derivative, and is verysimilar, but instead of using a geometric series, anarithmetico-geometric series is used.In order to allow the model to have a differentparameter above and below the diagonal, the onlychange needed is to redefine h(?)
to use a differentparameter for ?
above and below the diagonal.
Wedenote these parameters as ?
and ?
for below andabove the diagonal respectively.
Further, the offsetis denoted as ?.we change the definition of h(?)
to the followinginstead:151h(i, j,m, n) =??????????????
?im+ 1?jn+ 1+ ????
?j <= j??????
?im+ 1?jn+ 1+ ????
?otherwisej?is the point closest to or on the diagonal here,calculated as:max(min(bi ?
(n+ 1)m+ 1+ ?
?
(n+ 1)c, n), 0)Here, ?
can range from ?1 to 1, and thus thecalculation for the diagonal j?is clamped to be ina valid range for alignments.As the partition function (Z(?))
used in (Dyer etal., 2013) consists of 2 calculations for each tar-get position i, one for above and one for below thediagonal, we can simply substitute ?
for the geo-metric series calculations in order to use differentparameters for each:s?
(e?h(i,j?,m,n), r) + sn??
(e?h(i,j?,m,n), r)where j?is j?+ 1.2.1 Optimizing the ParametersAs in the original formulation, we need to usegradient-based optimisation in order to find goodvalues for ?, ?
and ?.
Unfortunately, optimizing?
would require taking the derivative of h(?
), andthus the derivative of the absolute value.
This isunfortunately undefined when the argument is 0,however we work around this by choosing a sub-gradient of 0 at that point.
This means the steps wetake do not always improve the objective function,but in practice the method works well.The first derivative of L with respect to ?
at asingle target word becomes:?
?L =j?
?k=1p(ai= k|ei, f,m, n)h(i, k,m, n)?j??l=1?
(l|i,m, n)h(i, l,m, n)And similar for finding the first derivative withrespect to ?, but summing from j?to n instead.The first derivative with respect to ?
then, is:?
?L =n?k=1p(ai= k|ei, f,m, n)h?
(i, k,m, n)?j??l=1?
(l|i,m, n)h?
(i, l,m, n)Where h?(?)
is the first derivative of h(?)
withrespect to ?.
For obtaining this derivative, thearithmetico-geometric series (Fernandez et al,2006) was originally used as an optimization, andfor the gradient with respect to omega a geometricseries should suffice, as an optimization, as thereis no conditioning on the source words.
This isnot done in the current work however, so timingresults will not be directly comparable to thosefound in (Dyer et al, 2013).Conditioning on the POS of the target wordsthen becomes as simple as using a different ?, ?,and ?
for each POS tag in the input, and calculat-ing a separate derivative for each of them, usingonly the derivatives at those target words that usethe POS tag.
A minor detail is to keep a count ofalignment positions used for finding the derivativefor each different parameter, and normalizing theresulting derivatives with those counts, so the stepsize can be kept constant across POS tags.3 Empirical resultsThe above described model is evaluated with ex-periments on a set of 3 language pairs, on whichAER scores and BLEU scores are computed.
Weuse similar corpora as used in (Dyer et al, 2013):a French-English corpus made up of Europarl ver-sion 7 and news-commentary corpora, the Arabic-English parallel data consisting of the non-UNportions of the NIST training corpora, and theFBIS Chinese-English corpora.The models that are compared are the originalreparameterization of Model 2, a version where ?is split around the diagonal (split), one where postags are used, but ?
is not split around the diagonal(pos), one where an offset is used, but parametersaren?t split about the diagonal (offset), one that?ssplit about the diagonal and uses pos tags (pos &split) and finally one with all three (pos & split &offset).
All are trained for 5 iterations, with uni-form initialisation, where the first iteration onlythe translation probabilities are updated, and theother parameters are updated as well in the sub-sequent iterations.
The same hyperparameters are152Model Fr-En Ar-En Zh-EnTokens 111M 46M 17.3M(after) 110M 29.0M 10.4Maverage 1.64 0.76 0.27Model 4 15.5 6.3 2.2Table 1: Token counts and average amount of timeto train models (and separately training time forModel 4) on original corpora in one direction inhours, by corpus.used as in (Dyer et al, 2013), with stepsize for up-dates to ?
and ?
during gradient ascent is 1000,and that for ?
is 0.03, decaying after every gradi-ent descent step by 0.9, using 8 steps every iter-ation.
Both ?
and ?
are initialised to 6, and ?
isinitialised to 0.
For these experiments the pos andpos & split use POS tags generated using the Stan-ford POS tagger (Toutanova and Manning, 2000),using the supplied models for all of the languagesused in the experiments.
For comparison, Model4 is trained for 5 iterations using 5 iterations eachof Model 1 and Model 3 as initialization, usingGIZA++ (Och and Ney, 2003).For the comparisons in AER, the corpora areused as-is, but for the BLEU comparisons, sen-tences longer than 50 words are filtered out.
InTable 2 the sizes of the corpora before filtering arelisted, as well as the time taken in hours to alignthe corpora for AER.
As the training times forthe different versions barely differ, only the aver-age is displayed for the models here described andModel 4 training times are given for comparison.Note that the times for the models optimizing only?
and ?, and the model only optimizing ?
still cal-culate the derivatives for the other parameters, andso could be made to be faster than here displayed.For both the BLEU and AER results, the align-ments are generated in both directions, and sym-metrised using the grow-diag-final-and heuristic,which in preliminary tests had shown to do best interms of AER.The results are given in Table 2.
These scoreswere computed using the WMT2012 data as goldstandard.
The different extensions to the modelmake no difference to the AER scores for Chinese-English, and actually do slightly worse for French-English.
In both cases, Model 4 does better thanthe models introduced here.Model Fr-En Zh-EnOriginal 16.3 42.5Split 16.8 42.5Pos 16.6 42.5Offset 16.8 42.5Pos & Split 16.8 42.5Pos & Split & Offset 16.7 42.5Model 4 11.2 40.5Table 2: AER results on Chinese-English andFrench-English data setsModel Fr-En Ar-En Zh-EnOriginal 25.9 43.8 32.8Split 25.9 43.2 32.8Pos 25.9 43.9 32.9Offset 26.0 43.9 32.8Pos & Split 26.0 44.1 33.2Pos & Split & Offset 26.0 44.2 33.3Model 4 26.8 43.9 32.4Table 3: BLEU results on Chinese-English andFrench-English data setsFor the comparisons of translation quality, themodels are trained up using a phrase-based trans-lation system (Koehn et al, 2007) that used theabove listed models to align the data.
Languagemodels were augmented with data outside of thecorpora for Chinese-English (200M words total)and Arabic-English (100M words total).
Test setsfor Chinese are MT02, MT03, MT06 and MT08,for Arabic they were MT05, MT06 and MT08, andfor French they were the newssyscomb2009 dataand the newstest 2009-2012 data.The results are listed in Table 31.
BLEU scoresfor Arabic-English and Chinese-English are com-puted with multiple references, while those forFrench-English are against a single reference.
Al-though the different models made little differencein AER, there is quite a bit of variation in theBLEU scores between the different models.
Inall cases, the models conditioned on POS tagsdid better than the original model, by as muchas 0.5 BLEU points.
For Arabic-English as wellas Chinese-English, the full model outperformed1The difference in these results compared to those re-ported in Dyer et al (2013) is due to differences in corpussize, and the fact that a different translation model is used.153Model 4, in the case of Chinese-English by 0.9BLEU points.The low impact of the split and offset modelsare most likely due to the need to model all align-ments in the corpus.
The distributions can?t skewtoo far to aligning to one direction, as that wouldlower the probability of a large amount of align-ments.
This is reflected in the resulting parame-ters ?, ?
and ?
that are estimated, as the first twodo not differ much from the parameters estimatedwhen both are kept the same, and the second tendsto be very small.As for the Pos model, it seems that only vary-ing the symmetrical slope for the different POStags doesn?t capture the differences between dis-tributions for POS tags.
For example, the ?
and?
parameters can differ quite a lot in the Pos &Split model when compared to the Pos model, withone side having a much smaller parameter and theother a much larger parameter for a given POS tagin the first model, and the single parameter beingcloser to the model average for the same POS tagin the second model.The low variation in results between the differ-ent models for French-English might be explainedby less word movement when translating betweenthese languages, which could mean the originalmodel is sufficient to capture this behaviour.4 ConclusionWe have shown some extensions to a reparame-terized IBM Model 2, allowing it to model wordreordering better.
Although these models don?timprove on the baseline in terms of AER, theydo better than the original in all three languagestested, and outperform M4 in two of these lan-guages, at little cost in terms of training time.
Fu-ture directions for this work include allowing formore expressivity of the alignment model by usinga Beta distribution instead of the current exponen-tial model.5 AcknowledgmentsDr Cohn is the recipient of an Australian Re-search Council Future Fellowship (project numberFT130101105).ReferencesPeter F. Brown, Vincent J. Della Pietra, StephenA.
Della Pietra, and Robert.
L. Mercer.
1993.The mathematics of statistical machine translation:Parameter estimation.
Computational Linguistics,19:263?311.Chris Dyer, Victor Chahuneau, and Noah A Smith.2013.
A simple, fast, and effective reparameteri-zation of ibm model 2.
In Proceedings of NAACL-HLT, pages 644?648.P.
A. Fernandez, T. Foregger, and J. Pahikkala.
2006.Arithmetico-geometric series.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ond?rej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InProceedings of the 45th Annual Meeting of the ACLon Interactive Poster and Demonstration Sessions,ACL ?07, pages 177?180, Stroudsburg, PA, USA.Association for Computational Linguistics.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Comput.
Linguist., 29:19?51, March.Kristina Toutanova and Christopher D. Manning.2000.
Enriching the knowledge sources used in amaximum entropy part-of-speech tagger.
In Pro-ceedings of the 2000 Joint SIGDAT Conference onEmpirical Methods in Natural Language Process-ing and Very Large Corpora: Held in Conjunctionwith the 38th Annual Meeting of the Associationfor Computational Linguistics - Volume 13, EMNLP?00, pages 63?70, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.154
