Proceedings of the 43rd Annual Meeting of the ACL, pages 549?556,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsContext-dependent SMT Model using Bilingual Verb-Noun CollocationYoung-Sook HwangATR SLT Research Labs2-2-2 Hikaridai Seika-choSoraku-gun Kyoto, 619-0288, JAPANyoungsook.hwang@atr.jpYutaka SasakiATR SLT Research Labs2-2-2 Hikaridai Seika-choSoraku-gun Kyoto, 619-0288, JAPANyutaka.sasaki@atr.jpAbstractIn this paper, we propose a new context-dependent SMT model that is tightly cou-pled with a language model.
It is de-signed to decrease the translation ambi-guities and efficiently search for an opti-mal hypothesis by reducing the hypothe-sis search space.
It works through recipro-cal incorporation between source and tar-get context: a source word is determinedby the context of previous and correspond-ing target words and the next target wordis predicted by the pair consisting of theprevious target word and its correspond-ing source word.
In order to alleviatethe data sparseness in chunk-based trans-lation, we take a stepwise back-off trans-lation strategy.
Moreover, in order to ob-tain more semantically plausible transla-tion results, we use bilingual verb-nouncollocations; these are automatically ex-tracted by using chunk alignment and amonolingual dependency parser.
As a casestudy, we experimented on the languagepair of Japanese and Korean.
As a result,we could not only reduce the search spacebut also improve the performance.1 IntroductionFor decades, many research efforts have contributedto the advance of statistical machine translation.Recently, various works have improved the qualityof statistical machine translation systems by usingphrase translation (Koehn et al, 2003; Marcu et al,2002; Och et al, 1999; Och and Ney, 2000; Zenset al, 2004).
Most of the phrase-based translationmodels have adopted the noisy-channel based IBMstyle models (Brown et al, 1993):  (1)In these model, we have two types of knowledge:translation model,  and language model,.
The translation model links the source lan-guage sentence to the target language sentence.
Thelanguage model describes the well-formedness ofthe target language sentence and might play a rolein restricting hypothesis expansion during decoding.To recover the word order difference between twolanguages, it also allows modeling the reordering byintroducing a relative distortion probability distribu-tion.
However, in spite of using such a languagemodel and a distortion model, the translation outputsmay not be fluent or in fact may produce nonsense.To make things worse, the huge hypothesis searchspace is much too large for an exhaustive search.
Ifarbitrary reorderings are allowed, the search prob-lem is NP-complete (Knight, 1999).
Accordingto a previous analysis (Koehn et al, 2004) of howmany hypotheses are generated during an exhaustivesearch using the IBM models, the upper bound forthe number of states is estimated by 	   ,where  is the number of source words and  isthe size of the target vocabulary.
Even though thenumber of possible translations of the last two wordsis much smaller than , we still need to makefurther improvement.
The main concern is the ex-549ponential explosion from the possible configurationsof source words covered by a hypothesis.
In orderto reduce the number of possible configurations ofsource words, decoding algorithms based on  aswell as the beam search algorithm have been pro-posed (Koehn et al, 2004; Och et al, 2001).
(Koehnet al, 2004; Och et al, 2001) used heuristics forpruning implausible hypotheses.Our approach to this problem examines the pos-sibility of utilizing context information in a givenlanguage pair.
Under a given target context, the cor-responding source word of a given target word is al-most deterministic.
Conversely, if a translation pairis given, then the related target or source context ispredictable.
This implies that if we considered bilin-gual context information in a given language pairduring decoding, we can reduce the computationalcomplexity of the hypothesis search; specifically, wecould reduce the possible configurations of sourcewords as well as the number of possible target trans-lations.In this study, we present a statistical machinetranslation model as an alternative to the classicalIBM-style model.
This model is tightly coupledwith target language model and utilizes bilingualcontext information.
It is designed to not only re-duce the hypothesis search space by decreasing thetranslation ambiguities but also improve translationperformance.
It works through reciprocal incorpo-ration between source and target context: sourcewords are determined by the context of previousand corresponding target words, and the next targetwords are predicted by the current translation pair.Accordingly, we do not need to consider any dis-tortion model or language model as is the case withIBM-style models.Under this framework, we propose a chunk-basedtranslation model for more grammatical, fluent andaccurate output.
In order to alleviate the data sparse-ness problem in chunk-based translation, we use astepwise back-off method in the order of a chunk,sub-parts of the chunk, and word level.
Moreover,we utilize verb-noun collocations in dealing withlong-distance dependency which are automaticallyextracted by using chunk alignment and a monolin-gual dependency parser.As a case study, we developed a Japanese-to-Korean translation model and performed some ex-periments on the BTEC corpus.2 Overview of Translation ModelThe goal of machine translation is to transfer themeaning of a source language sentence, , into a target language sentence, .
In most types of statistical machine trans-lation, conditional probability  is used todescribe the correspondence between two sentences.This model is used directly for translation by solvingthe following maximization problem:  (2) (3)   (4)Since a source language sentence is given and the probability is applied to all possible corre-sponding target sentences, we can ignore the denom-inator in equation (3).
As a result, the joint proba-bility model can be used to describe the correspon-dence between two sentences.
We apply Markovchain rules to the joint probability model and obtainthe following decomposed model:   (5)where is the index of the source word that isaligned to the word under the assumption of thefixed one-to-one alignment.
In this model, we havetwo probabilities: source word prediction probability under agiven target language context,   target word prediction probability under thepreceding translation pair,  The probability of target word prediction is used forselecting the target word that follows the previoustarget words.
In order to make this more determin-istic, we use bilingual context, i.e.
the translationpair of the preceding target word.
For a given targetword, the corresponding source word is predicted bysource word prediction probability based on the cur-rent and preceding target words.550Since a target and a source word are predictedthrough reciprocal incorporation between sourceand target context from the beginning of a targetsentence, the word order in the target sentence isautomatically determined and the number of pos-sible configurations of source words is decreased.Thus, we do not need to perform any computationfor word re-ordering.
Moreover, since correspon-dences are provided based on bilingual contextualevidence, translation ambiguities can be decreased.As a result, the proposed model is expected to re-duce computational complexity during the decodingas well as improve performance.Furthermore, since a word-based translation ap-proach is often incapable of handling complicatedexpressions such as an idiomatic expressions orcomplicated verb phrases, it often outputs nonsensetranslations.
To avoid nonsense translations and toincrease explanatory power, we incorporate struc-tural aspects of the language into the chunk-basedtranslation model.
In our model, one source chunkis translated by exactly one target chunk, i.e., one-to-one chunk alignment.
Thus we obtain:  (6)  (7)where  is the number of chunks in a source and atarget sentence.3 Chunk-based J/K Translation Modelwith Back-OffWith the translation framework described above, webuilt a chunk-based J/K translation model as a casestudy.
Since a chunk-based translation model causessevere data sparseness, it is often impossible to ob-tain any translation of a given source chunk.
In orderto alleviate this problem, we apply back-off trans-lation models while giving the consideration to lin-guistic characteristics.Japanese and Korean is a very close language pair.Both are agglutinative and inflected languages in theword formation of a bunsetsu and an eojeol.
A bun-setsu/eojeol consists of two sub parts: the head partcomposed of content words and the tail part com-posed of functional words agglutinated at the end ofthe head part.
The head part is related to the mean-ing of a given segment, while the tail part indicatesa grammatical role of the head in a given sentence.By putting this linguistic knowledge to practicaluse, we build a head-tail based translation modelas a back-off version of the chunk-based translationmodel.
We place several constraints on this head-tailbased translation model as follows: The head of a given source chunk correspondsto the head of a target chunk.
The tail of thesource chunk corresponds to the tail of a targetchunk.
If a chunk does not have a tail part, weassign NUL to the tail of the chunk. The head of a given chunk follows the tail of thepreceding chunk and the tail follows the head ofthe given chunk.The constraints are designed to maintain the struc-tural consistency of a chunk.
Under these con-straints, the head-tail based translation can be for-mulated as the following equation:   (8)  where denotes the head of the  chunk and means the tail of the chunk.In the worst case, even the head-tail based modelmay fail to obtain translations.
In this case, weback it off into a word-based translation model.
Inthe word-based translation model, the constraintson the head-tail based translation model are not ap-plied.
The concept of the chunk-based J/K transla-tion framework with back-off scheme can be sum-marized as follows:1.
Input a dependency-parsed sentence at thechunk level,2.
Apply the chunk-based translation model to thegiven sentence,3.
If one of chunks does not have any correspond-ing translation: divide the failed chunk into a head and atail part,551Figure 1: An example of (a) chunk alignment for chunk-based, head-tail based translation and (b) bilingualverb-noun collocation by using the chunk alignment and a monolingual dependency parser back-off the translation into the head-tailbased translation model, if the head or tail does not have any corre-sponding translation, apply a word-basedtranslation model to the chunk.Here, the back-off model is applied only to the partthat failed to get translation candidates.3.1 Learning Chunk-based TranslationWe learn chunk alignments from a corpus that hasbeen word-aligned by a training toolkit for word-based translation models: the Giza++ (Och andNey, 2000) toolkit for the IBM models (Brownet al, 1993).
For aligning chunk pairs, we con-sider word(bunsetsu/eojeol) sequences to be chunksif they are in an immediate dependency relationshipin a dependency tree.
To identify chunks, we usea word-aligned corpus, in which source languagesentences are annotated with dependency parse treesby a dependency parser (Kudo et al, 2002) and tar-get language sentences are annotated with POS tagsby a part-of-speech tagger (Rim, 2003).
If a se-quence of target words is aligned with the words ina single source chunk, the target word sequence isregarded as one chunk corresponding to the givensource chunk.
By applying this method to the cor-pus, we obtain a word- and chunk-aligned corpus(see Figure 1).From the aligned corpus, we directly estimatethe phrase translation probabilities,  ,and the model parameters,   ,.
These estimation are madebased on relative frequencies.3.2 DecodingFor efficient decoding, we implement a multi-stackdecoder and a beam search with  algorithm.
Ateach search level, the beam search moves through atmost -best translation candidates, and a multi-stackis used for partial translations according to the trans-lation cardinality.
The output sentence is generatedfrom left to right in the form of partial translations.Initially, we get  translation candidates for eachsource chunk with the beam size .
Every possibletranslation is sorted according to its translation prob-ability.
We start the decoding with the initializedbeams and initial stack , the top of which has theinformation of the initial hypothesis,  .
The decoding algorithm is described in Table 1.In the decoding algorithm, estimating the back-ward score is so complicated that the computationalcomplexity becomes too high because of the contextconsideration.
Thus, in order to simplify this prob-lem, we assume the context-independence of onlythe backward score estimation.
The backward scoreis estimated by the translation probability and lan-guage model score of the uncovered segments.
Foreach uncovered segment, we select the best transla-tion with the highest score by multiplying the trans-lation probability of the segment by its languagemodel score.
The translation probability and lan-guage model score are computed without givingconsideration to context.After estimating the forward and backward scoreof each partial translation on stack , we try to5521.
Push the initial hypothesis    on the initialstack 2.
for i=1 to K Pop the previous state information of from stack  Get next target and corresponding source  for all pairs of ?
Check the head-tail consistency?
Mark the source segment as a covered one?
Estimate forward and backward score?
Push the state of pair  onto stack  Sort all translations on stack by the scores Prune the hypotheses3.
while (stack is not empty) Pop the state of the pair  Compose translation output,    4.
Output the best  translationsTable 1:  multi-stack decoding algorithmprune the hypotheses.
In pruning, we first sort thepartial translations on stack according to theirscores.
If the gradient of scores steeply decreasesover the given threshold at the  translation, weprune the translations of lower scores than the one.
Moreover, if the number of filtered translationsis larger than 	 , we only take the top 	 transla-tions.
As a final translation, we output the singlebest translation.4 Resolving Long-distance DependencySince most of the current translation models takeonly the local context into account, they cannotaccount for long-distance dependency.
This oftencauses syntactically or semantically incorrect trans-lation to be output.
In this section, we describehow this problem can be solved.
For handling thelong-distance dependency problem, we utilize bilin-gual verb-noun collocations that are automaticallyacquired from the chunk-aligned bilingual corpora.4.1 Automatic Extraction of BilingualVerb-Noun Collocation(BiVN)To automatically extract the bilingual verb-nouncollocations, we utilize a monolingual dependencyparser and the chunk alignment result.
The basicconcept is the same as that used in (Hwang et al,2004): bilingual dependency parses are obtained bysharing the dependency relations of a monolingualdependency parser among the aligned chunks.
Thenbilingual verb sub-categorization patterns are ac-quired by navigating the bilingual dependency trees.A verb sub-categorization is the collocation of a verband all of its argument/adjunct nouns, i.e.
verb-nouncollocation(see Figure 1).To acquire more reliable and general knowledge,we apply the following filtering method with statis-tical  test and unification operation: step 1.
Filter out the reliable translation corre-spondences from all of the alignment pairs by test at a probability level of  step 2.
Filter out reliable bilingual verb-nouncollocations BiVN by a unification and  testat a probability level of : Here, we assumethat two bilingual pairs,   and  are unifiable into a frame     iffboth of them are reliable pairs filtered in step 1.and they share the same verb pair  .4.2 Application of BiVNThe acquired BiVN is used to evaluate the bilingualcorrespondence of a verb-noun pair dependent oneach other and to select the correct translation.
Itcan be applied to any verb-noun pair regardless ofthe distance between them in a sentence.
Moreover,since the verb-noun relation in BiVN is bilingualknowledge, the sense of each corresponding verband noun can be almost completely disambiguatedby each other.In our translation system, we apply this BiVNduring decoding as follows:1.
Pivot verbs and their dependents in a givendependency-parsed source sentence2.
When extending a hypothesis, if one of the piv-oted verb and noun pairs is covered and its cor-responding translation pair is in BiVN, we givepositive weight   	 to the hypothesis. if  otherwise553where      and is a function that indicates whether the bilingualtranslation pair is in BiVN.
By adding the weightof the  function, we refine our model asfollows:   (10)where is a function indicating whether thepair of a verb and its argument   is coveredwith  or  and     is a bilingual translation pair in the hy-pothesis.5 Experiments5.1 CorpusThe corpus for the experiment was extracted fromthe Basic Travel Expression Corpus (BTEC), a col-lection of conversational travel phrases for Japaneseand Korean (see Table 2).
The entire corpus wassplit into two parts: 162,320 sentences in parallel fortraining and 10,150 sentences for test.
The Japanesesentences were automatically dependency-parsed byCaboCha (Kudo et al, 2002) and the Korean sen-tences were automatically POS tagged by KUTag-ger (Rim, 2003)5.2 Translation SystemsFour translation systems were implemented forevaluation: 1) Word based IBM-style SMT Sys-tem(WBIBM), 2) Chunk based IBM-style SMT Sys-tem(CBIBM), 3) Word based LM tightly CoupledSMT System(WBLMC), and 4) Chunk based LMtightly Coupled SMT System(CBLMC).
To exam-ine the effect of BiVN, BiVN was optionally usedfor each system.The word-based IBM-style (WBIBM) system1consisted of a word translation model and a bi-gram language model.
The bi-gram languagemodel was generated by using CMU LM toolkit(Clarkson et al, 1997).
Instead of using a fer-tility model, we allowed a multi-word target ofa given source word if it aligned with more thanone word.
We didn?t use any distortion model forword re-ordering.
And we used a log-linear model1In this experiment, a word denotes a morpheme    for weighting thelanguage model and the translation model.
For de-coding, we used a multi-stack decoder based on the algorithm, which is almost the same as that de-scribed in Section 3.
The difference is the use ofthe language model for controlling the generation oftarget translations.The chunk-based IBM-style (CBIBM) systemconsisted of a chunk translation model and a bi-gram language model.
To alleviate the data sparse-ness problem of the chunk translation model, we ap-plied the back-off method at the head-tail or mor-pheme level.
The remaining conditions are the sameas those for WBIBM.The word-based LM tightly coupled (WBLMC)system was implemented for comparison with thechunk-based systems.
Except for setting the transla-tion unit as a morpheme, the other conditions are thesame as those for the proposed chunk-based transla-tion system.The chunk-based LM tightly coupled (CBLMC)system is the proposed translation system.
A bi-gram language model was used for estimating thebackward score.5.3 EvaluationTranslation evaluations were carried out on 510 sen-tences selected randomly from the test set.
The met-rics for the evaluations are as follows:PER(Position independent WER), which pe-nalizes without considering positional dis-fluencies(Niesen et al, 2000).mWER(multi-reference Word Error Rate), which isbased on the minimum edit distance betweenthe target sentence and the sentences in the ref-erence set (Niesen et al, 2000).BLEU, which is the ratio of the n-gram forthe translation results found in the referencetranslations with a penalty for too short sen-tences (Papineni et al, 2001).NIST which is a weighted n-gram precision incombination with a penalty for too short sen-tences.For this evaluation, we made 10 multiple referencesavailable.
We computed all of the above criteria withrespect to these multiple references.554Training TestJapanese Korean Japanese Korean# of sentences 162,320 10,150# of total morphemes 1,153,954 1,179,753 74,366 76,540# of bunsetsu/eojeol 448,438 587,503 28,882 38,386vocabulary size 15,682 15,726 5,144 4,594Table 2: Statistics of Basic Travel Expression CorpusPER mWER BLEU NISTWBIBM 0.3415 / 0.3318 0.3668 / 0.3591 0.5747 / 0.5837 6.9075 / 7.1110WBLMC 0.2667 / 0.2666 0.2998 / 0.2994 0.5681 / 0.5690 9.0149 / 9.0360CBIBM 0.2677 / 0.2383 0.2992 / 0.2700 0.6347 / 0.6741 8.0900 / 8.6981CBLMC 0.1954 / 0.1896 0.2176 / 0.2129 0.7060 / 0.7166 9.9167 / 10.027Table 3: Evaluation Results of Translation Systems: without BiVN/with BiVNWBIBM WBLMC CBIBM CBLMC0.8110 / 0.8330 2.5585 / 2.5547 0.3345 / 0.3399 0.9039 / 0.9052Table 4: Translation Speed of Each Translation Systems(sec./sentence): without BiVN/with BiVN5.4 Analysis and DiscussionTable 3 shows the performance evaluation of eachsystem.
CBLMC outperformed CBIBM in overallevaluation criteria.
WBLMC showed much betterperformance than WBIBM in most of the evalua-tion criteria except for BLEU score.
The interestingpoint is that the performance of WBLMC is close tothat of CBIBM in PER and mWER.
The BLEU scoreof WBLMC is lower than that of CBIBM, but theNIST score of WBLMC is much better than that ofCBIBM.The reason the proposed model provided betterperformance than the IBM-style models is becausethe use of contextual information in CBLMC andWBLMC enabled the system to reduce the transla-tion ambiguities, which not only reduced the compu-tational complexity during decoding, but also madethe translation accurate and deterministic.
In addi-tion, chunk-based translation systems outperformedword-based systems.
This is also strong evidence ofthe advantage of contextual information.To evaluate the effectiveness of bilingual verb-noun collocations, we used the BiVN filtered with	   , where coverage ison the test set and average ambiguity is .
Wesuffered a slight loss in the speed by using theBiVN(see Table 4), but we could improve perfor-mance in all of the translation systems(see Table3).
In particular, the performance improvement inCBIBM with BiVN was remarkable.
This is a pos-itive sign that the BiVN is useful for handling theproblem of long-distance dependency.
From this re-sult, we believe that if we increased the coverage ofBiVN and its accuracy, we could improve the per-formance much more.Table 4 shows the translation speed of each sys-tem.
For the evaluation of processing time, we usedthe same machine, with a Xeon 2.8 GHz CPU and4GB memory , and checked the time of the best per-formance of each system.
The chunk-based trans-lation systems are much faster than the word-basedsystems.
It may be because the translation ambi-guities of the chunk-based models are lower thanthose of the word-based models.
However, the pro-cessing speed of the IBM-style models is faster thanthe proposed model.
This tendency can be analyzedfrom two viewpoints: decoding algorithm and DBsystem for parameter retrieval.
Theoretically, thecomputational complexity of the proposed model islower than that of the IBM models.
The use of a555sorting and pruning algorithm for partial translationsprovides shorter search times in all system.
Sincethe number of parameters for the proposed model ismuch more than for the IBM-style models, it took alonger time to retrieve parameters.
To decrease theprocessing time, we need to construct a more effi-cient DB system.6 ConclusionIn this paper, we proposed a new chunk-based statis-tical machine translation model that is tightly cou-pled with a language model.
In order to alleviatethe data sparseness in chunk-based translation, weapplied the back-off translation method at the head-tail and morpheme levels.
Moreover, in order toget more semantically plausible translation resultsby considering long-distance dependency, we uti-lized verb-noun collocations which were automat-ically extracted by using chunk alignment and amonolingual dependency parser.
As a case study,we experimented on the language pair of Japaneseand Korean.
Experimental results showed that theproposed translation model is very effective in im-proving performance.
The use of bilingual verb-noun collocations is also useful for improving theperformance.However, we still have some problems of the datasparseness and the low coverage of bilingual verb-noun collocation.
In the near future, we will try tosolve the data sparseness problem and to increase thecoverage and accuracy of verb-noun collocations.ReferencesPeter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and R. L. Mercer.
1993.
The mathematics ofstatistical machine translation: Parameter estimation,Computational Linguistics, 19(2):263-311.P.R.
Clarkson and R. Rosenfeld.
1997.
Statistical Lan-guage Modeling Using the CMU-Cambridge Toolkit,Proc.
of ESCA Eurospeech.Young-Sook Hwang, Kyonghee Paik, and Yutaka Sasaki.2004.
Bilingual Knowledge Extraction Using ChunkAlignment, Proc.
of the 18th Pacific Asia Con-ference on Language, Information and Computation(PACLIC-18), pp.
127-137, Tokyo.Kevin Knight.
1999.
Decoding Complexity in Word-Replacement Translation Models, Computational Lin-guistics, Squibs Discussion, 25(4).Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003 Statistical Phrase-Based Translation, Proc.of the Human Language Technology Confer-ence(HLT/NAACL)Philipp Koehn.
2004 Pharaoh: a Beam Search De-coder for Phrase-Based Statistical Machine Transla-tion Models, Proc.
of AMTA?04Taku Kudo, Yuji Matsumoto.
2002.
Japanese Depen-dency Analyisis using Cascaded Chunking, Proc.
ofCoNLL-2002Daniel Marcu and William Wong.
2002.
A phrase-based,joint probability model for statistical machine transla-tion , Proc.
of EMNLP.Sonja Niesen, Franz Josef Och, Gregor Leusch, HermannNey.
2000.
An Evaluation Tool for Machine Transla-tion: Fast Evaluation for MT Research, Proc.
of the2nd International Conference on Language Resourcesand Evaluation, pp.
39-45, Athens, Greece.Franz Josef Och, Christoph Tillmann, Hermann Ney.1999.
Improved alignment models for statistical ma-chine translation, Proc.
of EMNLP/WVLC.Franz Josef Och and Hermann Ney.
2000.
Improved Sta-tistical Alignment Models , Proc.
of the 38th AnnualMeeting of the Association for Computational Lin-guistics, pp.
440-447, Hongkong, China.Franz Josef Och, Nicola Ueffing, Hermann Ney.
2001.An Efficient A* Search Algorithm for Statistical Ma-chine Translation , Data-Driven Machine TranslationWorkshop, pp.
55-62, Toulouse, France.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
Bleu: a method for automatic evalu-ation of machine translation , IBM Research Report,RC22176.Toshiyuki Takezawa, Eiichiro Sumita, Fumiaki Sugaya,Hirofumi Yamamoto, and Seiichi Yamamoto.
2002.Toward a broad-coverage bilingual corpus for speechtranslation of travel conversations in the real world,Proc.
of LREC 2002, pp.
147-152, Spain.Richard Zens and Hermann Ney.
2004.
Improve-ments in Phrase-Based Statistical Machine Transla-tion, Proc.
of the Human Language Technology Con-ference (HLT-NAACL) , Boston, MA, pp.
257-264.Hae-Chang Rim.
2003.
Korean Morphological Analyzerand Part-of-Speech Tagger, Technical Report, NLPLab.
Dept.
of Computer Science and Engineering, Ko-rea University556
