Proceedings of the NAACL HLT Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 74?77,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsIntegrating High Precision Rules with Statistical Sequence Classifiers forAccuracy and SpeedWenhui Liao, Marc Light, and Sriharsha VeeramachaneniResearch and Development,Thomson Reuters610 Opperman Drive, Eagan MN 55123AbstractIntegrating rules and statistical systems is achallenge often faced by natural language pro-cessing system builders.
A common sub-class is integrating high precision rules with aMarkov statistical sequence classifier.
In thispaper we suggest that using such rules to con-strain the sequence classifier decoder resultsin superior accuracy and efficiency.
In a casestudy of a named entity tagging system, weprovide evidence that this method of combina-tion does prove efficient than other methods.The accuracy was the same.1 IntroductionSequence classification lies at the core of severalnatural language processing applications, such asnamed entity extraction, Asian language segmen-tation, Germanic language noun decompounding,and event identification.
Statistical models with aMarkov dependency have been successful employedto perform these tasks, e.g., hidden Markov mod-els (HMMs)(Rabiner, 1989) and conditional randomfields (CRFs)(Lafferty et al, 2001).
These statisticalsystems employ a Viterbi (Forney, 1973) decoder atruntime to efficiently calculate the most likely la-bel sequence based on the observed sequence andmodel.
Statistical machine translation systems makeuse of similar decoders.In many situations it is beneficial, and some-times required, for these systems to respect con-straints from high precision rules.
And thus whenbuilding working sequence labeling systems, re-searchers/software engineers are often faced withthe task of combining these two approaches.
Inthis paper we argue for a particular method of com-bining statistical models with Markov dependenciesand high precision rules.
We outline a number ofways to do this and then argue that guiding the de-coder of the statistical system has many advantagesover other methods of combination.But first, does the problem of combining multi-ple approaches really happen?
In our experience theneed arises in the following way: a statistical ap-proach with a Markov component is chosen becauseit has the best precision/recall characteristics and hasreasonable speed.
However, a number of rules arisefor varied reasons.
For example, the customer pro-vides domain knowledge not present in the trainingdata or a particular output characteristic is more im-portant that accuracy.
Consider the following ficti-tious but plausible situation: A named entity taggingsystem is built using a CRF.
The customer then pro-vides a number of company names that cannot bemissed, i.e., false negatives for these companies arecatastrophic but false positives can be tolerated.
Inaddition, it is known that, unlike in the training data,the runtime data will have a company name immedi-ately before every ticker symbol.
The question fac-ing the builder of the system is how to combine theCRF with rules based on the must-find company listand the company-name-before-every-ticker-symbolfact.Similar situations arise for the other sequence tag-ging situations mentioned above and for machinetranslation.
We suspect that even for non-languageapplications, such as gene sequence labeling, similarsituations arise.74In the next section we will discuss a number ofmethods for combining statistical systems and highprecision rules and argue for guiding the decoderof the statistical model.
Then in section 3, we de-scribe an implementation of the approach and giveevidence that the speed benefits are substantial.2 Methods for Combining a MarkovStatistical System and High PrecisionRulesOne method of combination is to encode high preci-sion rules as features and then train a new model thatincludes these features.
One advantage is that thesystem stays a straightforward statistical system.
Inaddition, the rules are fully integrated into the sys-tem allowing the statistical model weigh the rulesagainst other evidence.
However, the model maynot give the rules high weight if training data doesnot bear out their high precision or if the rule trig-ger does not occur often enough in the training data.Thus, despite a ?rule?
feature being on, the systemmay not ?follow?
the rule in its result labeling.
Also,addition or modification of a rule would require aretraining of the model for optimal accuracy.
Theretraining process may be costly and/or may not bepossible in the operational environment.Another method is to run both the statistical sys-tem and the rules and then merge the resulting labelsgiving preference to the labels resulting from thehigh precision rules.
The benefits are that the rulesare always followed.
However, the statistical systemdoes not have the information needed to give an op-timal solution based on the results of the high preci-sion rules.
In other words, the results will be incon-sistent from the view of the statistical system; i.e., ifit had know what the rules were going to say, then itwould have calculated the remaining part of the labelsequence differently.
In addition, the decoder con-siders part of the label sequence search space that isonly going to be ruled out, pun intended, later.Now for the preferred method: run the rules first,then use their output to guide the decoder for thestatistical model.
The benefits of this method arethat the rules are followed, the statistical system isinformed of constraints imposed by the rules andthus the statistical system calculates optimal pathsgiven these constraints.
In addition, the decoderconsiders only those label sequences consistent withthese constraints, resulting in a smaller search space.Thus, we would expect this method to produce botha more accurate and a faster implementation.Consider Figure 1 which shows a lattice that rep-resents all the labeling sequences for the input ...Microsoft on Monday announced a ...
The possiblelabels are O (out), P (person), C (company), L (lo-cation) .
Assume Microsoft is in a list of must-findcompanies and that on and Monday are part of a rulethat makes them NOT names in this context.
Thebold points are constraints from the high-precisionrules.
In other words, only sequences that includethese bold points need to be considered.Figure 1: Guiding decoding with high-precision rulesFigure 1 also illustrates how the constraints re-duce the search space.
Without constraints, thesearch space includes 46 = 4096 sequences, whilewith constraints, it includes only 43 = 64.It should also be noted that we do not claim tohave invented the idea of constraining the decoder.For example, in the context of active learning, wherea human corrects some of the errors made by a CRFsequence classifier, (Culota et al, 2006) proposed aconstrained Viterbi algorithm that finds the path withmaximum probability that passes through the labelsassigned by the human.
They showed that constrain-ing the path to respect the human labeling consider-ably improves the accuracy on the remaining tokensin the sequence.
Our contribution is noticing thatconstraining the decoder is a good way to integraterule output.3 A Case Study: Named EntityRecognitionIn this section, we flesh out the discussion of namedentity (NE) tagging started above.
Since the entitytype of a word is determined mostly by the contextof the word, NE tagging is often posed as a sequence75classification problem and solved by Markov statis-tical systems.3.1 A Named Entity Recognition SystemThe system described here starts with a CRF whichwas chosen because it allows for the use of numer-ous and arbitrary features of the input sequence andit can be efficiently trained and decoded.
We usedthe Mallet toolkit (McCallum, 2002) for training theCRF but implemented our own feature extractionand runtime system.
We used standard features suchas the current word, the word to the right/left, ortho-graphic shape of the word, membership in word sets(e.g., common last names), features of neighboringwords, etc.The system was designed to run on news wire textand based on this data?s characteristics, we designeda handful of high precision rules including:Rule 1: if a token is in a must-tag list, this tokenshould be marked as Company no matter what thecontext is.Rule 2: if a capitalized word is followed by cer-tain company suffix such as Ltd, Inc, Corp, etc., la-bel both as Company.Rule 3: if a token sequence is in a company listand the length of the sequence is larger than 3, labelthem as Company.Rule 4: if a token does not include any uppercaseletters, is not pure number, and is not in an excep-tions list, label it as not part of a name.
(The ex-ceptions list includes around 70 words that are notcapitalized but still could be an NE, such as al, at,in, -, etc.
)Rule 5: if a token does not satisfy rule 4 but itsneighboring tokens satisfy rule 4, then if this tokenis a time related word, label it as not part of a name.
(Example time tokens are January and Monday.
)The first three rules aim to find company namesand the last two to find tokens that are not part of aname.These rules are integrated into the system as de-scribed in section 2: we apply the rules to the inputtoken sequence and then use the resulting labels, ifany, to constrain the Viterbi decoder for the CRF.A further optimization of the system is based onthe following observation: features need not be cal-culated for tokens that have already received labelsfrom the rules.
(An exception to this is when fea-tures are copied to a neighbor, e.g., the token to myleft is a number.)
Thus, we do not calculate manyfeatures of rule-labeled tokens.
Note that feature ex-traction can often be a major portion of the compu-tational cost of sequence labeling systems (see Table1(b))3.2 Evidence of Computational SavingsResulting from Our Proposed Method ofIntegrationWe compare the results when high-precision rulesare integrated into CRF for name entity extraction(company, person, and location) in terms of both ac-curacy and speed for different corpora.
Three cor-pora are used, CoNLL (CoNLL 2003 English sharedtask official test set), MUC (Message UnderstandingConference), and TF (includes around 1000 news ar-ticles from Thomson Financial).Table 1(a) shows the results for each corpora re-spectively.
The baseline method does not use anyhigh-precision rules, the Post-corr uses the high-precision rules to correct the labeling from the CRF,and Constr-viti uses the rules to constrain the labelsequences considered by the Viterbi decoder.
In gen-eral, Constr-viti achieves slightly better precisionand recall.
(a)(b)Figure 2: (b) A test example : (a) without constraints; (b)with constraintsTo better understand how our strategy could im-prove the accuracy, we did some analysis on the76Table 1: Experiment ResultsDatabase Methods Precision Recall F1CoNLL Baseline 84.38 83.02 83.69Post-corr 85.87 84.86 85.36Constr-viti 85.98 85.55 85.76TF Baseline 88.39 82.42 85.30Post-corr 87.69 88.30 87.99Constr-viti 88.02 88.54 88.28MUC Baseline 92.22 88.72 90.43Post-Corr 91.28 88.87 90.06Constr-viti 90.86 89.37 90.11(a)Precision and RecallMethods Rules Features Viterbi OverallBaseline 0 0.78 0.22 1Post-corr 0.08 0.78 0.22 1.08Constr-vite 0.08 0.35 0.13 0.56Baseline 0 0.85 0.15 1Post-Corr 0.14 0.85 0.15 1.14Constr-vite 0.14 0.38 0.1 0.62Baseline 0 0.79 0.21 1Post-corr 0.12 0.79 0.21 1.12Constr-vite 0.12 0.36 0.12 0.60(b)Time Efficiencytesting data.
In one example as shown in Figure 2,Steel works as an attorney, without high-precisionrules, Steel works is tagged as a company since it isin our company list.
Post-correction changes the la-bel of works to O, but it is unable to fix Steel.
Withour strategy, since works is pinned as O in the Vert-ibi algorithm, Steel is tagged as Per.
Thus, com-pared to post-correction, the advantage of constrain-ing Viterbi is that it is able to affect the whole pathwhere the token is, instead a token itself.
However,the improvements were not significant in our casestudy.
We have not done an error analysis.
We canonly speculate that the high precision rules do nothave perfect precision and thus create a number oferrors that the statistical model would not have madeon its own.We also measured how much the constrainedViterbi method improves efficiency.
We divide thecomputational time to three parts: time in applyingrules, time in feature extraction, and time in Viterbicomputation.
Table 1(b) lists the time efficiency.
In-stead using specific time unit (e.g.
second), we useratio instead by assuming the overall time for thebaseline method is 1.
As shown in the table, forthe three data sets, the overall time of our methodis 0.56, 0.62, and 0.60 of the time of the baselinealgorithm respectively.
The post-correction methodis the most expensive one because of the extra timespending in rules.
Overall, the constrained Viterbimethod is substantially faster than the Baseline andPost-corr methods in addition to being more accu-rate.4 ConclusionsThe contribution of this paper is the repurposing ofthe idea of constraining a decoder: we constrain thedecoder as a way to integrate high precision ruleswith a statistical sequence classifier.
In a case studyof named entity tagging, we show that this methodof combination does in fact increase efficiency morethan competing methods without any lose of ac-curacy.
We believe analogous situations exist forother sequence classifying tasks such as Asian lan-guage segmentation, Germanic language noun de-compounding, and event identification.ReferencesAron Culota, Trausti Kristjansson, Andrew McCallum,and Paul Viola.
2006.
Corrective feedback and per-sistent learning for information extraction.
ArtificialIntelligence Journal, 170:1101?1122.G.
D. Forney.
1973.
The viterbi algorithm.
Proceedingsof the IEEE, 61(3):268?278.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional random fields: Probabilistic modelsfor segmenting and labeling sequence data.
In Proc.18th International Conf.
on Machine Learning, pages282?289.A.K.
McCallum.
2002.
Mallet: A machine learning forlanguage toolkit.
http://mallet.cs.umass.edu.Lawrence R. Rabiner.
1989.
A tutorial on hidden markovmodels and selected applications in speech recogni-tion.
Proceedings of the IEEE, pages 257?286.77
