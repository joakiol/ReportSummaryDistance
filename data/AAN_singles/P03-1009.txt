Clustering Polysemic Subcategorization Frame Distributions SemanticallyAnna Korhonen?Computer LaboratoryUniversity of Cambridge15 JJ Thomson AvenueCambridge CB3 0FD, UKalk23@cl.cam.ac.ukYuval KrymolowskiDivision of InformaticsUniversity of Edinburgh2 Buccleuch PlaceEdinburgh EH8 9LWScotland, UKykrymolo@inf.ed.ac.ukZvika MarxInterdisciplinary Centerfor Neural Computation,The Hebrew UniversityJerusalem, Israelzvim@cs.huji.ac.ilAbstractPrevious research has demonstrated theutility of clustering in inducing semanticverb classes from undisambiguated cor-pus data.
We describe a new approachwhich involves clustering subcategoriza-tion frame (SCF) distributions using theInformation Bottleneck and nearest neigh-bour methods.
In contrast to previouswork, we particularly focus on cluster-ing polysemic verbs.
A novel evaluationscheme is proposed which accounts forthe effect of polysemy on the clusters, of-fering us a good insight into the potentialand limitations of semantically classifyingundisambiguated SCF data.1 IntroductionClassifications which aim to capture the close rela-tion between the syntax and semantics of verbs haveattracted a considerable research interest in both lin-guistics and computational linguistics (e.g.
(Jack-endoff, 1990; Levin, 1993; Pinker, 1989; Dang etal., 1998; Dorr, 1997; Merlo and Stevenson, 2001)).While such classifications may not provide a meansfor full semantic inferencing, they can capture gen-eralizations over a range of linguistic properties, andcan therefore be used as a means of reducing redun-dancy in the lexicon and for filling gaps in lexicalknowledge.
?This work was partly supported by UK EPSRC projectGR/N36462/93: ?Robust Accurate Statistical Parsing (RASP)?.Verb classifications have, in fact, been used tosupport many natural language processing (NLP)tasks, such as language generation, machine transla-tion (Dorr, 1997), document classification (Klavansand Kan, 1998), word sense disambiguation (Dorrand Jones, 1996) and subcategorization acquisition(Korhonen, 2002).One attractive property of these classifications isthat they make it possible, to a certain extent, to in-fer the semantics of a verb on the basis of its syn-tactic behaviour.
In recent years several attemptshave been made to automatically induce semanticverb classes from (mainly) syntactic informationin corpus data (Joanis, 2002; Merlo et al, 2002;Schulte im Walde and Brew, 2002).In this paper, we focus on the particular taskof classifying subcategorization frame (SCF) distri-butions in a semantically motivated manner.
Pre-vious research has demonstrated that clusteringcan be useful in inferring Levin-style semanticclasses (Levin, 1993) from both English and Ger-man verb subcategorization information (Brew andSchulte im Walde, 2002; Schulte im Walde, 2000;Schulte im Walde and Brew, 2002).We propose a novel approach, which involves: (i)obtaining SCF frequency information from a lexi-con extracted automatically using the comprehen-sive system of Briscoe and Carroll (1997) and (ii)applying a clustering mechanism to this informa-tion.
We use clustering methods that process rawdistributional data directly, avoiding complex pre-processing steps required by many advanced meth-ods (e.g.
Brew and Schulte im Walde (2002)).In contrast to earlier work, we give special empha-sis to polysemy.
Earlier work has largely ignoredthis issue by assuming a single gold standard classfor each verb (whether polysemic or not).
The rel-atively good clustering results obtained suggest thatmany polysemic verbs do have some predominatingsense in corpus data.
However, this sense can varyacross corpora (Roland et al, 2000), and assuming asingle sense is inadequate for an important group ofmedium and high frequency verbs whose distribu-tion of senses in balanced corpus data is flat ratherthan zipfian (Preiss and Korhonen, 2002).To allow for sense variation, we introduce a newevaluation scheme against a polysemic gold stan-dard.
This helps to explain the results and offersa better insight into the potential and limitations ofclustering undisambiguated SCF data semantically.We discuss our gold standards and the choice oftest verbs in section 2.
Section 3 describes themethod for subcategorization acquisition and sec-tion 4 presents the approach to clustering.
Detailsof the experimental evaluation are supplied in sec-tion 5.
Section 6 concludes with directions for futurework.2 Semantic Verb Classes and Test VerbsLevin?s taxonomy of verbs and their classes (Levin,1993) is the largest syntactic-semantic verb classifi-cation in English, employed widely in evaluation ofautomatic classifications.
It provides a classificationof 3,024 verbs (4,186 senses) into 48 broad / 192fine grained classes.
Although it is quite extensive,it is not exhaustive.
As it primarily concentrates onverbs taking NP and PP complements and does notprovide a comprehensive set of senses for verbs, itis not suitable for evaluation of polysemic classifi-cations.We employed as a gold standard a substan-tially extended version of Levin?s classificationconstructed by Korhonen (2003).
This incorpo-rates Levin?s classes, 26 additional classes byDorr (1997)1, and 57 new classes for verb types notcovered comprehensively by Levin or Dorr.110 test verbs were chosen from this gold stan-dard, 78 polysemic and 32 monosemous ones.
Somelow frequency verbs were included to investigate the1These classes are incorporated in the ?LCS database?
(http://www.umiacs.umd.edu/?bonnie/verbs-English.lcs).effect of sparse data on clustering performance.
Toensure that our gold standard covers all (or most)senses of these verbs, we looked into WordNet(Miller, 1990) and assigned all the WordNet sensesof the verbs to gold standard classes.2Two versions of the gold standard were created:monosemous and polysemic.
The monosemous onelists only a single sense for each test verb, that cor-responding to its predominant (most frequent) sensein WordNet.
The polysemic one provides a compre-hensive list of senses for each verb.
The test verbsand their classes are shown in table 1.
The classesare indicated by number codes from the classifica-tions of Levin, Dorr (the classes starting with 0) andKorhonen (the classes starting with A).3 The pre-dominant sense is indicated by bold font.3 Subcategorization InformationWe obtain our SCF data using the subcategorizationacquisition system of Briscoe and Carroll (1997).We expect the use of this system to be benefi-cial: it employs a robust statistical parser (Briscoeand Carroll, 2002) which yields complete thoughshallow parses, and a comprehensive SCF classifier,which incorporates 163 SCF distinctions, a super-set of those found in the ANLT (Boguraev et al,1987) and COMLEX (Grishman et al, 1994) dictio-naries.
The SCFs abstract over specific lexically-governed particles and prepositions and specificpredicate selectional preferences but include somederived semi-predictable bounded dependency con-structions, such as particle and dative movement.78 of these ?coarse-grained?
SCFs appeared in ourdata.
In addition, a set of 160 fine grained frameswere employed.
These were obtained by parameter-izing two high frequency SCFs for prepositions: thesimple PP and NP + PP frames.
The scope was re-stricted to these two frames to prevent sparse dataproblems in clustering.A SCF lexicon was acquired using this systemfrom the British National Corpus (Leech, 1992,BNC) so that the maximum of 7000 citations were2As WordNet incorporates particularly fine grained sensedistinctions, some senses were found which did not appear inour gold standard.
As many of them appeared marginal and/orlow in frequency, we did not consider these additional senses inour experiment.3The gold standard assumes Levin?s broad classes (e.g.
class10) instead of possible fine-grained ones (e.g.
class 10.1).TEST GOLD STANDARD TEST GOLD STANDARD TEST GOLD STANDARD TEST GOLD STANDARDVERB CLASSES VERB CLASSES VERB CLASSES VERB CLASSESplace 9 dye 24, 21, 41 focus 31, 45 stare 30lay 9 build 26, 45 force 002, 11 glow 43drop 9, 45, 004, 47, bake 26, 45 persuade 002 sparkle 4351, A54, A30pour 9, 43, 26, 57, 13, 31 invent 26, 27 urge 002, 37 dry 45load 9 publish 26, 25 want 002, 005, 29, 32 shut 45settle 9, 46, A16, 36, 55 cause 27, 002 need 002, 005, 29, 32 hang 47, 9, 42, 40fill 9, 45, 47 generate 27, 13, 26 grasp 30, 15 sit 47, 9remove 10, 11, 42 induce 27, 002, 26 understand 30 disappear 48withdraw 10, A30 acknowledge 29, A25, A35 conceive 30, 29, A56 vanish 48wipe 10, 9 proclaim 29, 37, A25 consider 30, 29 march 51brush 10, 9, 41, 18 remember 29, 30 perceive 30 walk 51filter 10 imagine 29, 30 analyse 34, 35 travel 51send 11, A55 specify 29 evaluate 34, 35 hurry 53, 51ship 11, A58 establish 29, A56 explore 35, 34 rush 53, 51transport 11, 31 suppose 29, 37 investigate 35, 34 begin 55carry 11, 54 assume 29, A35, A57 agree 36, 22, A42 continue 55, 47, 51drag 11, 35, 51, 002 think 29, 005 communicate 36, 11 snow 57, 002push 11, 12, 23, 9, 002 confirm 29 shout 37 rain 57pull 11, 12, 13, 23, 40, 016 believe 29, 31, 33 whisper 37 sin 003give 13 admit 29, 024, 045, 37 talk 37 rebel 003lend 13 allow 29, 024, 13, 002 speak 37 risk 008, A7study 14, 30, 34, 35 act 29 say 37, 002 gamble 008, 009hit 18, 17, 47, A56, 31, 42 behave 29 mention 37 beg 015, 32bang 18, 43, 9, 47, 36 feel 30, 31, 35, 29 eat 39 pray 015, 32carve 21, 25, 26 see 30, 29 drink 39 seem 020add 22, 37, A56 hear 30, A32 laugh 40, 37 appear 020, 48, 29mix 22, 26, 36 notice 30, A32 smile 40, 37colour 24, 31, 45 concentrate 31, 45 look 30, 35Table 1: Test verbs and their monosemous/polysemic gold standard sensesused per test verb.
The lexicon was evaluated againstmanually analysed corpus data after an empiricallydefined threshold of 0.025 was set on relative fre-quencies of SCFs to remove noisy SCFs.
The methodyielded 71.8% precision and 34.5% recall.
When weremoved the filtering threshold, and evaluated thenoisy distribution, F-measure4 dropped from 44.9 to38.51.54 Clustering MethodData clustering is a process which aims to partition agiven set into subsets (clusters) of elements that aresimilar to one another, while ensuring that elementsthat are not similar are assigned to different clusters.We use clustering for partitioning a set of verbs.
Ourhypothesis is that information about SCFs and theirassociated frequencies is relevant for identifying se-mantically related verbs.
Hence, we use SCFs as rel-evance features to guide the clustering process.64F = 2?precision?recallprecision+recall5These figures are not particularly impressive because ourevaluation is exceptionally hard.
We use 1) highly polysemictest verbs, 2) a high number of SCFs and 3) evaluate againstmanually analysed data rather than dictionaries (the latter havehigh precision but low recall).6The relevance of the features to the task is evident whencomparing the probability of a randomly chosen pair of verbsverbi and verbj to share the same predominant sense (4.5%)with the probability obtained when verbj is the JS-divergenceWe chose two clustering methods which do not in-volve task-oriented tuning (such as pre-fixed thresh-olds or restricted cluster sizes) and which approachdata straightforwardly, in its distributional form: (i)a simple hard method that collects the nearest neigh-bours (NN) of each verb (figure 1), and (ii) the In-formation Bottleneck (IB), an iterative soft method(Tishby et al, 1999) based on information-theoreticgrounds.The NN method is very simple, but it has somedisadvantages.
It outputs only one clustering config-uration, and therefore does not allow examinationof different cluster granularities.
It is also highlysensitive to noise.
Few exceptional neighbourhoodrelations contradicting the typical trends in the dataare enough to cause the formation of a single clusterwhich encompasses all elements.Therefore we employed the more sophisticatedIB method as well.
The IB quantifies the rele-vance information of a SCF distribution with re-spect to output clusters, through their mutual infor-mation I(Clusters; SCFs).
The relevance informa-tion is maximized, while the compression informa-tion I(Clusters;V erbs) is minimized.
This en-sures optimal compression of data through clusters.The tradeoff between the two constraints is realizednearest neighbour of verbi (36%).NN Clustering:1.
For each verb v:2.
Calculate the JS divergence between the SCFdistributions of v and all other verbs:JS(p, q) = 12[D(p??
?p+q2)+ D(q???p+q2)]3.
Connect v with the most similar verb;4.
Find all the connected componentsFigure 1: Connected components nearest neighbour (NN)clustering.
D is the Kullback-Leibler distance.through minimizing the cost term:L = I(Clusters;V erbs)?
?I(Clusters; SCFs) ,where ?
is a parameter that balances the constraints.The IB iterative algorithm finds a local minimumof the above cost term.
It takes three inputs: (i) SCF-verb distributions, (ii) the desired number of clustersK, and (iii) the value of ?.Starting from a random configuration, the algo-rithm repeatedly calculates, for each cluster K, verbV and SCF S, the following probabilities: (i) themarginal proportion of the cluster p(K); (ii) theprobability p(S|K) for a SCF to occur with mem-bers of the cluster; and (iii) the probability p(K|V )for a verb to be assigned to the cluster.
These prob-abilities are used, each in its turn, for calculating theother probabilities (figure 2).
The collection of allp(S|K)?s for a fixed cluster K can be regarded as aprobabilistic center (centroid) of that cluster in theSCF space.The IB method gives an indication of themost informative values of K.7 Intensifying theweight ?
attached to the relevance informationI(Clusters; SCFs) allows us to increase the num-ber K of distinct clusters being produced (while toosmall ?
would cause some of the output clusters tobe identical to one another).
Hence, the relevance in-formation grows with K. Accordingly, we consideras the most informative output configurations thosefor which the relevance information increases moresharply between K?
1 and K clusters than betweenK and K + 1.7Most works on clustering ignore this issue and refer to anarbitrarily chosen number of clusters, or to the number of goldstandard classes, which cannot be assumed in realistic applica-tions.IB Clustering (fixed ?
):Perform till convergence, for each time stept = 1, 2, .
.
.
:1. zt(K,V ) = pt?1(K) e?
?D[p(S|V )?pt?1(S|K)](When t = 1, initialize zt(K,V ) arbitrarily)2. pt(K|V ) = zt(K,V )?K?
zt(K?,V )3. pt(K) =?V p(V )pt(K|V )4. pt(S|K) =?V p(S|V )pt(V |K)Figure 2: Information Bottleneck (IB) iterative clustering.
Dis the Kullback-Leibler distance.When the weight of relevance grows, the assign-ment to clusters is more constrained and p(K|V ) be-comes more similar to hard clustering.
LetK(V ) = argmaxKp(K|V )denote the most probable cluster of a verb V .For K ?
30, more than 85% of the verbs havep(K(V )|V ) > 90% which makes the output cluster-ing approximately hard.
For this reason, we decidedto use only K(V ) as output and defer a further ex-ploration of the soft output to future work.5 Experimental Evaluation5.1 DataThe input data to clustering was obtained from theautomatically acquired SCF lexicon for our 110 testverbs (section 2).
The counts were extracted fromunfiltered (noisy) SCF distributions in this lexicon.8The NN algorithm produced 24 clusters on this in-put.
From the IB algorithm, we requested K = 2to 60 clusters.
The upper limit was chosen so asto slightly exceed the case when the average clus-ter size 110/K = 2.
We chose for evaluation theIB results for K = 25, 35 and 42.
For these val-ues, the SCF relevance satisfies our criterion for anotable improvement in cluster quality (section 4).The value K=35 is very close to the actual number(34) of predominant senses in the gold standard.
Inthis way, the IB yields structural information beyondclustering.8This yielded better results, which might indicate that theunfiltered ?noisy?
SCFs contain information which is valuablefor the task.5.2 MethodA number of different strategies have been proposedfor evaluation of clustering.
We concentrate here onthose which deliver a numerical value which is easyto interpret, and do not introduce biases towards spe-cific numbers of classes or class sizes.
As we cur-rently assign a single sense to each polysemic verb(sec.
5.4) the measures we use are also applicablefor evaluation against a polysemous gold standard.Our first measure, the adjusted pairwise preci-sion (APP), evaluates clusters in terms of verb pairs(Schulte im Walde and Brew, 2002) 9:APP = 1KK?i=1num.
of correct pairs in kinum.
of pairs in ki ?|ki|?1|ki|+1.APP is the average proportion of all within-clusterpairs that are correctly co-assigned.
It is multipliedby a factor that increases with cluster size.
This fac-tor compensates for a bias towards small clusters.Our second measure is derived from purity, aglobal measure which evaluates the mean precisionof the clusters, weighted according to the cluster size(Stevenson and Joanis, 2003).
We associate witheach cluster its most prevalent semantic class, anddenote the number of verbs in a cluster K that takeits prevalent class by nprevalent(K).
Verbs that donot take this class are considered as errors.
Givenour task, we are only interested in classes which con-tain two or more verbs.
We therefore disregard thoseclusters where nprevalent(K) = 1.
This leads us todefine modified purity:mPUR =?nprevalent(ki)?2nprevalent(ki)number of verbs .The modification we introduce to purity removes thebias towards the trivial configuration comprised ofonly singletons.5.3 Evaluation Against the Predominant SenseWe first evaluated the clusters against the predom-inant sense, i.e.
using the monosemous gold stan-dard.
The results, shown in Table 2, demonstratethat both clustering methods perform significantly9Our definition differs by a factor of 2 from that ofSchulte im Walde and Brew (2002).Alg.
K +PP ?PP +PP ?PPAPP: mPUR:NN (24) 21% 19% 48% 45%25 12% 9% 39% 32%IB 35 14% 9% 48% 38%42 15% 9% 50% 39%RAND 25 3% 15%Table 2: Clustering performance on the predominant senses,with and without prepositions.
The last entry presents the per-formance of random clustering with K = 25, which yielded thebest results among the three values K=25, 35 and 42.better on the task than our random clustering base-line.
Both methods show clearly better performancewith fine-grained SCFs (with prepositions, +PP) thanwith coarse-grained ones (-PP).Surprisingly, the simple NN method performsvery similarly to the more sophisticated IB.
Beingbased on pairwise similarities, it shows better per-formance than IB on the pairwise measure.
The IBis, however, slightly better according to the globalmeasure (2% with K = 42).
The fact that the NNmethod performs better than the IB with similar Kvalues (NN K = 24 vs. IB K = 25) seems to suggestthat the JS divergence provides a better model forthe predominant class than the compression modelof the IB.
However, it is likely that the IB perfor-mance suffered due to our choice of test data.
As themethod is global, it performs better when the targetclasses are represented by a high number of verbs.In our experiment, many semantic classes were rep-resented by two verbs only (section 2).Nevertheless, the IB method has the clear advan-tage that it allows for more clusters to be produced.At best it classified half of the verbs correctly ac-cording to their predominant sense (mPUR = 50%).Although this leaves room for improvement, the re-sult compares favourably to previously published re-sults10.
We argue, however, that evaluation against amonosemous gold standard reveals only part of thepicture.10Due to differences in task definition and experimentalsetup, a direct comparison with earlier results is impossible.For example, Stevenson and Joanis (2003) report an accuracyof 29% (which implies mPUR ?
29%), but their task involvesclassifying 841 verbs to 14 classes based on differences in thepredicate-argument structure.K Pred.
Multiple Pred.
Multiplesense senses sense sensesAPP: mPUR:NN:(24) 21% 29% (23% + 5?)
48% 60% (46%+ 2?
)IB:25 12% 18% (14% + 5?)
39% 48% (43%+ 3?
)35 14% 20% (16% + 6?)
47% 59% (50%+ 4?
)42 15% 19% (16% + 3?)
50% 59% (54%+ 2?
)Table 3: Evaluation against the monosemous (Pred.)
and pol-ysemous (Multiple) gold standards.
The figures in parenthesesare results of evaluation on randomly polysemous data + sig-nificance of the actual figure.
Results were obtained with fine-grained SCFs (including prepositions).5.4 Evaluation Against Multiple SensesIn evaluation against the polysemic gold standard,we assume that a verb which is polysemous in ourcorpus data may appear in a cluster with verbs thatshare any of its senses.
In order to evaluate the clus-ters against polysemous data, we assigned each pol-ysemic verb V a single sense: the one it shares withthe highest number of verbs in the cluster K(V ).Table 3 shows the results against polysemic andmonosemous gold standards.
The former are notice-ably better than the latter (e.g.
IB with K = 42 is 9%better).
Clearly, allowing for multiple gold standardclasses makes it easier to obtain better results withevaluation.In order to show that polysemy makes a non-trivial contribution in shaping the clusters, we mea-sured the improvement that can be due to purechance by creating randomly polysemous gold stan-dards.
We constructed 100 sets of random gold stan-dards.
In each iteration, the verbs kept their originalpredominant senses, but the set of additional senseswas taken entirely from another verb - chosen at ran-dom.
By doing so, we preserved the dominant senseof each verb, the total frequency of all senses and thecorrelations between the additional senses.The results included in table 3 indicate, with99.5% confidence (3?
and above), that the improve-ment obtained with the polysemous gold standard isnot artificial (except in two cases with 95% confi-dence).5.5 Qualitative Analysis of PolysemyWe performed qualitative analysis to further inves-tigate the effect of polysemy on clustering perfor-Different Pairs FractionSenses in cluster0 39 51%1 85 10%2 625 7%3 1284 3%4 1437 3%Table 4: The fraction of verb pairs clustered together, as afunction of the number of different senses between pair mem-bers (results of the NN algorithm)Common one irregular no irregularSenses Pairs in cluster Pairs in cluster0 2180 3% 3018 3%1 388 9% 331 12%2 44 20% 31 35%Table 5: The fraction of verb pairs clustered together, as afunction of the number of shared senses (results of the NN algo-rithm)mance.
The results in table 4 demonstrate that themore two verbs differ in their senses, the lower theirchance of ending up in the same cluster.
From thefigures in table 5 we see that the probability of twoverbs to appear in the same cluster increases withthe number of senses they share.
Interestingly, it isnot only the degree of polysemy which influencesthe results, but also the type.
For verb pairs where atleast one of the members displays ?irregular?
poly-semy (i.e.
it does not share its full set of senses withany other verb), the probability of co-occurrence inthe same cluster is far lower than for verbs which arepolysemic in a ?regular?
manner (Table 5).Manual cluster analysis against the polysemicgold standard revealed a yet more comprehensivepicture.
Consider the following clusters (the IB out-put with K = 42):A1: talk (37), speak (37)A2: look (30, 35), stare (30)A3: focus (31, 45), concentrate (31, 45)A4: add (22, 37, A56)We identified a close relation between the clus-tering performance and the following patterns of se-mantic behaviour:1) Monosemy: We had 32 monosemous testverbs.
10 gold standard classes included 2 or moreor these.
7 classes were correctly acquired us-ing clustering (e.g.
A1), indicating that clusteringmonosemous verbs is fairly ?easy?.2) Predominant sense: 10 clusters were exam-ined by hand whose members got correctly classi-fied together, despite one of them being polysemous(e.g.
A2).
In 8 cases there was a clear indication inthe data (when examining SCFs and the selectionalpreferences on argument heads) that the polysemousverb indeed had its predominant sense in the rele-vant class and that the co-occurrence was not due tonoise.3) Regular Polysemy: Several clusters were pro-duced which represent linguistically plausible inter-sective classes (e.g.
A3) (Dang et al, 1998) ratherthan single classes.4) Irregular Polysemy: Verbs with irregular pol-ysemy11 were frequently assigned to singleton clus-ters.
For example, add (A4) has a ?combining andattaching?
sense in class 22 which involves NP andPP SCFs and another ?communication?
sense in 37which takes sentential SCFs.
Irregular polysemy wasnot a marginal phenomenon: it explains 5 of the 10singletons in our data.These observations confirm that evaluationagainst a polysemic gold standard is necessary inorder to fully explain the results from clustering.5.6 Qualitative Analysis of ErrorsFinally, to provide feedback for further developmentof our verb classification approach, we performed aqualitative analysis of errors not resulting from poly-semy.
Consider the following clusters (the IB outputfor K = 42):B1: place (9), build (26, 45),publish (26, 25), carve (21, 25, 26)B2: sin (003), rain (57), snow (57, 002)B3: agree (36, 22, A42), appear (020, 48, 29),begin (55), continue (55, 47, 51)B4: beg (015, 32)Three main error types were identified:1) Syntactic idiosyncracy: This was the most fre-quent error type, exemplified in B1, where place isincorrectly clustered with build, publish and carvemerely because it takes similar prepositions to theseverbs (e.g.
in, on, into).2) Sparse data: Many of the low frequency verbs(we had 12 with frequency less than 300) performed11Recall our definition of irregular polysemy, section 5.4.poorly.
In B2, sin (which had 53 occurrences) isclassified with rain and snow because it does notoccur in our data with the preposition against -the ?hallmark?
of its gold standard class (?ConspireVerbs?
).3) Problems in SCF acquisition: These were notnumerous but occurred e.g.
when the system couldnot distinguish between different control (e.g.
sub-ject/object equi/raising) constructions (B3).6 Discussion and ConclusionsThis paper has presented a novel approach to auto-matic semantic classification of verbs.
This involvedapplying the NN and IB methods to cluster polysemicSCF distributions extracted from corpus data usingBriscoe and Carroll?s (1997) system.
A principledevaluation scheme was introduced which enabled usto investigate the effect of polysemy on the resultingclassification.Our investigation revealed that polysemy has aconsiderable impact on the clusters formed: pol-ysemic verbs with a clear predominant sense andthose with similar regular polysemy are frequentlyclassified together.
Homonymic verbs or verbs withstrong irregular polysemy tend to resist any classifi-cation.While it is clear that evaluation should accountfor these cases rather than ignore them, the issue ofpolysemy is related to another, bigger issue: the po-tential and limitations of clustering in inducing se-mantic information from polysemic SCF data.
Ourresults show that it is unrealistic to expect that the?important?
(high frequency) verbs in language fallinto classes corresponding to single senses.
How-ever, they also suggest that clustering can be usedfor novel, previously unexplored purposes: to de-tect from corpus data general patterns of seman-tic behaviour (monosemy, predominant sense, reg-ular/irregular polysemy).In the future, we plan to investigate the use of softclustering (without hardening the output) and de-velop methods for evaluating the soft output againstpolysemous gold standards.
We also plan to workon improving the accuracy of subcategorization ac-quisition, investigating the role of noise (irregular /regular) in clustering, examining whether differentsyntactic/semantic verb types require different ap-proaches in clustering, developing our gold standardclassification further, and extending our experimentsto a larger number of verbs and verb classes.ReferencesB.
Boguraev, E. J. Briscoe, J. Carroll, D. Carter, andC.
Grover.
1987.
The derivation of a grammatically-indexed lexicon from the longman dictionary of con-temporary english.
In Proc.
of the 25th ACL, pages193?200, Stanford, CA.C.
Brew and S. Schulte im Walde.
2002.
Spectral clus-tering for german verbs.
In Conference on EmpiricalMethods in Natural Language Processing, Philadel-phia, USA.E.
J. Briscoe and J. Carroll.
1997.
Automatic extractionof subcategorization from corpora.
In 5th ACL Confer-ence on Applied Natural Language Processing, pages356?363, Washington DC.E.
J. Briscoe and J. Carroll.
2002.
Robust accurate sta-tistical annotation of general text.
In 3rd InternationalConference on Language Resources and Evaluation,pages 1499?1504, Las Palmas, Gran Canaria.H.
T. Dang, K. Kipper, M. Palmer, and J. Rosenzweig.1998.
Investigating regular sense extensions based onintersective Levin classes.
In Proc.
of COLING/ACL,pages 293?299, Montreal, Canada.B.
Dorr and D. Jones.
1996.
Role of word sense disam-biguation in lexical acquisition: predicting semanticsfrom syntactic cues.
In 16th International Conferenceon Computational Linguistics, pages 322?333, Copen-hagen, Denmark.B.
Dorr.
1997.
Large-scale dictionary constructionfor foreign language tutoring and interlingual machinetranslation.
Machine Translation, 12(4):271?325.R.
Grishman, C. Macleod, and A. Meyers.
1994.
Com-lex syntax: building a computational lexicon.
In In-ternational Conference on Computational Linguistics,pages 268?272, Kyoto, Japan.R.
Jackendoff.
1990.
Semantic Structures.
MIT Press,Cambridge, Massachusetts.E.
Joanis.
2002.
Automatic verb classification using ageneral feature space.
Master?s thesis, University ofToronto.J.
L. Klavans and M. Kan. 1998.
Role of verbs in docu-ment analysis.
In Proc.
of COLING/ACL, pages 680?686, Montreal, Canada.A.
Korhonen.
2002.
Subcategorization Acquisition.Ph.D.
thesis, University of Cambridge, UK.A.
Korhonen.
2003.
Extending Levin?s Classificationwith New Verb Classes.
Unpublished manuscript, Uni-versity of Cambridge Computer Laboratory.G.
Leech.
1992.
100 million words of english: the britishnational corpus.
Language Research, 28(1):1?13.B.
Levin.
1993.
English Verb Classes and Alternations.Chicago University Press, Chicago.P.
Merlo and S. Stevenson.
2001.
Automatic verb clas-sification based on statistical distributions of argumentstructure.
Computational Linguistics, 27(3):373?408.P.
Merlo, S. Stevenson, V. Tsang, and G. Allaria.
2002.A multilingual paradigm for automatic verb classifica-tion.
In Proc.
of the 40th ACL, Pennsylvania, USA.G.
A. Miller.
1990.
WordNet: An on-line lexi-cal database.
International Journal of Lexicography,3(4):235?312.S.
Pinker.
1989.
Learnability and Cognition: The Acqui-sition of Argument Structure.
MIT Press, Cambridge,Massachusetts.J.
Preiss and A. Korhonen.
2002.
Improving subcate-gorization acquisition with WSD.
In ACL Workshopon Word Sense Disambiguation: Recent Successes andFuture Directions, Philadelphia, USA.D.
Roland, D. Jurafsky, L. Menn, S. Gahl, E. Elder, andC.
Riddoch.
2000.
Verb subcatecorization frequencydifferences between business-news and balanced cor-pora.
In ACL Workshop on Comparing Corpora, pages28?34.S.
Schulte im Walde and C. Brew.
2002.
Inducing ger-man semantic verb classes from purely syntactic sub-categorisation information.
In Proc.
of the 40th ACL,Philadephia, USA.S.
Schulte im Walde.
2000.
Clustering verbs seman-tically according to their alternation behaviour.
InProc.
of COLING-2000, pages 747?753, Saarbru?cken,Germany.S.
Stevenson and E. Joanis.
2003.
Semi-supervisedverb-class discovery using noisy features.
In Proc.
ofCoNLL-2003, Edmonton, Canada.N.
Tishby, F. C. Pereira, and W. Bialek.
1999.
The infor-mation bottleneck method.
In Proc.
of the 37th AnnualAllerton Conference on Communication, Control andComputing, pages 368?377.
