Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 128?131,Queen Mary University of London, September 2009. c?2009 Association for Computational LinguisticsOn NoMatchs, NoInputs and BargeIns:Do Non-Acoustic Features Support Anger Detection?Alexander Schmitt, Tobias HeinrothDialogue Systems Research GroupInstitute for Information TechnologyUlm University, Germanyalexander.schmitt@uni-ulm.detobias.heinroth@uni-ulm.deJackson LiscombeSpeechCycle, Inc.Broadway 26New York City, USAjackson@speechcycle.comAbstractMost studies on speech-based emotionrecognition are based on prosodic andacoustic features, only employing artifi-cial acted corpora where the results cannotbe generalized to telephone-based speechapplications.
In contrast, we present anapproach based on utterances from 1,911calls from a deployed telephone-basedspeech application, taking advantage ofadditional dialogue features, NLU featuresand ASR features that are incorporatedinto the emotion recognition process.
De-pending on the task, non-acoustic featuresadd 2.3% in classification accuracy com-pared to using only acoustic features.1 IntroductionCertainly, the most relevant employment ofspeech-based emotion recognition is that of atelephone-based Interactive Voice Response Sys-tem (IVR).Emotion recognition for IVR differs insofarto ?traditional?
emotion recognition, that it canbe reduced to a binary classification problem,namely the distinction between angry and non-angry whereas studies on speech-based emotionrecognition analyze complete and relatively longsentences covering the full bandwidth of humanemotions.
In a way, emotion recognition in thetelephone domain is less challenging since a dis-tinction between two different emotion classes,angry and non-angry, is sufficient.
We don?t haveto expect callers talking to IVRs in a sad, anxious,happy, disgusted or bored manner.
I.e., even if acaller is happy, the effect on the dialogue will bethe same as if he is neutral.
However, there stillremain challenges for the system developer suchas varying speech quality caused by, e.g., vary-ing distance to the receiver during the call lead-ing to loudness variations (which emotion recog-nizers might mistakenly interpret as anger).
Butalso bandwidth limitation introduced by the tele-phone channel and a strongly unbalanced distribu-tion of non-angry and angry utterances with morethan 80% non-angry utterances make a reliabledistinction of the caller emotion difficult.
Whilehot anger with studio quality conditions can be de-termined with over 90% (Pittermann et al, 2009)studies on IVR anger recognition report lower ac-curacies due to these limitations.
However, thereis one advantage of anger recognition in IVR sys-tems that can be exploited: additional informationis available from the dialogue context, the speechrecognizer and the natural language parser.This contribution is organized as follows: first,we introduce related work and describe our cor-pus.
In Section 4 we outline our employed featureswith emphasis on the non-acoustic ones.
Experi-ments are shown in Section 5 where we analyzethe impact of the newly developed features beforewe summarize our work in Section 6.2 Related WorkSpeech-based emotion research regarding tele-phone applications has been increasingly dis-cussed in the speech community.
While in earlystudies acted corpora were used, such as in (Ya-coub et al, 2003), training and testing data in laterstudies has been more and more based on real-life data, see (Burkhardt et al, 2008),(Burkhardtet al, 2009).
Most studies are limited to acous-tic/prosodic features that have been extracted outof the audio data.
Linguistic information was ad-ditionaly exploited in (Lee et al, 2002) resulting in128a 45.7% accuracy improvement compared to usingonly acoustic features.
In (Liscombe et al, 2005)the lexical and prosodic features were additionalyenriched with dialogue act features leading to anincrease in accuracy of 2.3%.3 Corpus DescriptionFor our studies we employed a corpus of 1,911calls from an automated agent helping to resolveinternet-related problems comprising 22,724 utter-ances.
Three labelers divided the corpus into an-gry, annoyed and non-angry utterances (Cohen?s?
= 0.70 on whole corpus; L1 vs. L2 ?
= 0.8,L1 vs. L3 ?
= 0.71, L2 vs. L3 ?
= 0.59).
Thereason for choosing three emotion classes insteadof a binary classification lies in the hope to findclearer patterns for strong anger.
A distinction be-tween non-angry and somewhat annoyed callersis rather difficult even for humans.
The final la-bel was defined based on majority voting resultingin 90.2% non-angry, 5.1% garbage, 3.4% annoyedand 0.7% angry utterances.
0.6% of the samples inthe corpus were sorted out since all three raters haddifferent opinions.
The raters were asked to label?garbage?
when the utterance is incomprehensibleor consists of non-speech events.
While the num-ber of angry and annoyed utterances seems verylow, 429 calls (i.e.
22.4%) contained annoyed orangry utterances.4 FeaturesWe created two different feature sets: one basedon typical acoustic/prosodic features and anotherone to which we will refer as ?non-acoustic?
fea-tures consisting of features from the AutomatichSpeech Recognition (ASR), Natural LanguageUnderstanding (NLU), Dialogue Manager (DM)and Context features.4.1 Acoustic FeaturesThe acoustic/prosodic features were extractedwith the aid of Praat (Boersma, 2001) and con-sist of power, mean, rms, mean harmonicity, pitch(mean, deviation, voiced frames, time step, meanslope, minimum, maximum, range), voiced pitch(mean, minimum mean, maximum mean, range),intensity (mean, maximum, minimum, deviation,range), jitter points, formants 1-5, MFCC 1-12.The extraction was performed on the completeshort utterance.4.2 Non-Acoustic FeaturesThe second, i.e.
non-acoustic, feature set is basedon features logged with the aid of the speech plat-form hosting the IVR application and is presentedhere in more detail.
They include:ASR features: raw ASR transcription ofcaller?s utterance (Utterance) (unigram bag-of-words); ASR confidence of returned utterancetranscription, as floating point number between 0(least confident) and 1 (most confident) (Confi-dence); names of all grammars active (Grammar-Name); name of the grammar that returned theparse (TriggeredGrammarName); did the callerbegin speaking before the prompt completed?
(?yes?, ?no?)
(BargedIn); did the caller communi-cate with speech (?voice?)
or keypad (?dtmf?)
(In-putModeName); was the speech recognizer suc-cessful (?Complete?)
or not and if it was not suc-cessful, an error message is recorded such as?NoInput?
or ?NoMatch?
(RecognitionStatus)NLU-Features: the semantic parse of the callerutterance as returned by the activated grammar inthe current dialog module (Interpretation); givencaller speech input, we need to try and recognizethe semantic meaning.
The first time we try to dothis, this is indicated with a value of ?Initial?.
Ifwe were not returned a parse then we have to re-prompt (?Retry1?
or ?Timeout1?).
Similar for if thecaller asks for help or a repetition of the prompt.Etc.
(LoopName)DM-Features: the text of what the auto-mated agent said prior to recording the user input(PromptName); the number of tries to elicit a de-sired response.
Integer values range from 0 (firsttry) to 7 (6th try) (RoleIndex); an activity may re-quest substantive user input (?Collection?)
or con-firm previous substantive input (?Confirmation?
)(RoleName); within a call each event is sequen-tially organized by these numbers (SequenceID);the name of the activity (aka dialog module) thatis active (ActivityName); type of activity.
Possiblevalues are: Question, PlatformValue, Announce-ment, Wait, Escalate (ActivityType)Context-Features: We further developed addi-tional cumulative features based on the previousones in order to keep track of the NoMatch, NoIn-puts and similar parameters serving as an indicatorfor the call quality: number of non-empty NLUparses (CumUserTurns); number of statementsand questions by the system (CumSysTurns); num-ber of questions (CumSysQuestions); number of129help requests by the user (CumHelpReq); num-ber of operator requests (CumOperatorReq); num-ber of NoInput events (CumNoInputs); numberof NoMatch events (CumNoMatchs) number ofBargeIns (CumBargeIns).5 ExperimentsIn order to prevent an adaption of the anger modelto specific callers we seperated the corpus ran-domly into 75% training and 25% testing materialand ensured that no speaker contained in trainingwas used for testing.
To exclude that we receive agood classification result by chance, we performed50 iterations in each test and calculated the per-formance?s mean and standard deviation over alliterations.Note, that our aim in this study is less findingan optimum classifer, than finding additional fea-tures that support the distinction between angryand non-angry callers.
Support Vector Machinesand Artificial Neural Networks are thus not con-sidered, although the best performances are re-ported with those learning algorithms.
A simi-lar performance, i.e.
only slightly poorer, can bereached with Rule Learners.
They enable a thor-ough study of the features, leading to the decisionfor one or the other class, since they produce ahuman readable set of if-then-else rules.
Our hy-potheses on a perfect feature set can thus easily beconfirmed or rejected.We performed experiments with two differ-ent classes: ?angry?
vs. ?non-angry?
and ?an-gry+annoyed?
vs. ?non-angry?.
Merging angryand annoyed utterances aims on finding all callers,where the customer satisfaction is endangered.
Inboth tasks, we employ a) only acoustic featuresb) only ASR/NLU/DM/Context features and c) acombination of both feature sets.
The number ofutterances used for training and testing is shown inTable 1.As result we expect acoustic features to per-form better than non-acoustic features.
Amongthe relevant non-acoustic features we assume asan indicator for angry utterances low ASR confi-dences and high barge-in rates, which we consideras signal for the caller?s impatience.
All tests havebeen performed with the machine learning frame-work RapidMiner (Mierswa et al, 2006) featuringall common supervised and unsupervised learningschemes.Results are listed in Table 2, including preci-Test A Test Bangry+annoyed non-a.
angry non-a.Training ?
320 ?
320 ?
80 ?
80Testing ?
140 ?
140 ?
40 ?
40Table 1: Number of utterances employed for bothtests per iteration.
Since the samples are selectedrandomly and the corpus was separated by speak-ers before training and testing, the numbers mayvary in each iteration.sion and recall values.
As expected, Test B (an-gry vs. non-angry) has the highest accuracy with87.23% since the patterns are more clearly sep-arable compared to Test A (annoyed vs. non-angry, 72.57%).
Obviously, adding non-acousticfeatures increases classification accuracy signifi-cantly, but only where the acoustic features arenot expressive enough.
While the additional in-formation increases the accuracy of the combinedangry+annoyed task by 2.3 % (Test A), it doesnot advance the distinction between only angry vs.non-angry (Test B).5.1 Emotional HistoryOne could expect, that the probability ofan angry/annoyed turn following another an-gry/annoyed turn is rather high and that this in-formation could be exploited.
Thus, we furtherincluded two features PrevEmotion and PrevPre-vEmotion, taking into account the two previoushand-labeled emotions in the dialogue discourse.If they would contribute to the recognition pro-cess, we would replace them by automatically la-belled ones.
All test results, however, did not im-prove.5.2 Ruleset AnalysisFor a determination of the relevant features in thenon-acoustic feature set, we analyzed the rulesetgenerated by the RuleLearner in Test A. Interest-ingly, a dominant feature in the resulting ruleset is?AudioDuration?.
While shorter utterances wereassigned to non-angry (about <2s), longer utter-ances tended to be assigned to angry/annoyed.
Afollowing analysis of the utterance length confirmsthis rule: utterances labeled as angry averaged2.07 (+/-0.73) seconds, annoyed utterances lasted1.82 (+/-0.57) s and non-angry samples were 1.57(+/- 0.66) s in average.
The number of NoMatch130Test A: Angry/Annoyed vs. Non-angry only Acoustic only Non-Acoustic bothAccuracy 70.29 (+-2.94) % 61.43 (+-2.75) % 72.57 (+-2.37) %Precision/Recall Class ?Ang./Ann.?
71.51% / 61.57% 68.35% / 42.57% 73.67% / 70.14%Precision/Recall Class ?Non-angry?
69.19% / 73.00% 58.30% / 80.29% 71.57% / 75.00%Test B: Angry vs. Non-angry only Acoustic only Non-Acoustic bothAccuracy 87.06 (+-3.76) % 64.29 (+-1.32) % 87.23 (+-3.72) %Precision/Recall Class ?Angry?
87.13% / 86.55% 66.0% / 58.9% 86.88% / 87.11%Precision/Recall Class ?Non-angry?
86.97% / 87.53% 62.9% 69.9% 87.55% / 87.33%Table 2: Classification results for angry+annoyed vs. non-angry and angry vs. non-angry utterances.events (CumNoMatch) up to the angry turn playeda less dominant role than expected: only 8 sampleswere assigned to angry/annoyed due to reoccur-ring NoMatch events (>5 NoMatchs).
Utterancesthat contained ?Operator?, ?Agent?
or ?Help?
were,as expected, assigned to angry/annoyed, however,in combination with high AudioDuration values(>2s).
Non-angry utterances were typically betterrecognized: average ASR confidence values are0.82 (+/-0.288) (non-angry), 0.71 (+/- 0.36) (an-noyed) and 0.56 (+/- 0.41) (angry).6 Conclusion and DiscussionIn IVR systems, we can take advantage of non-acoustic information, that comes from the dia-logue context.
As demonstrated in this work,ASR, NLU, DM and contextual features sup-port the distinction between angry and non-angrycallers.
However, where the samples can be sepa-rated into clear patterns, such as in Test B, no ben-efit from the additional feature set can be expected.In what sense a late fusion of linguistic, dialogueand context features would improve the classifier,i.e.
by building various subsystems whose opin-ions are subject to a voting mechanism, will beevaluated in future work.
We will also analyzewhy the linguistic features did not have any vis-ible impact on the classifier.
Presumably a combi-nation of n-grams, bag-of-words and bag of emo-tional salience will improve classification.7 AcknowledgementsWe would like to take the opportunity to thank thefollowing colleagues for contributing to the devel-opment of our emotion recognizer: Ulrich Tschaf-fon, Shu Ding and Alexey Indiryakov.ReferencesPaul Boersma.
2001.
Praat, a System for Do-ing Phonetics by Computer.
Glot International,5(9/10):341?345.Felix Burkhardt, Richard Huber, and JoachimStegmann.
2008.
Advances in anger detection withreal life data.Felix Burkhardt, Tim Polzehl, Joachim Stegmann, Flo-rian Metze, and Richard Huber.
2009.
Detectingreal life anger.
In Proc.
of ICASSP, April.Chul Min Lee, Shrikanth Narayanan, and Roberto Pier-accini.
2002.
Combining Acoustic and LanguageInformation for Emotion Recognition.
In Interna-tional Conference on Speech and Language Process-ing (ICSLP), Denver, USA, October.Jackson Liscombe, Guiseppe Riccardi, and DilekHakkani-Tu?r.
2005.
Using Context to ImproveEmotion Detection in Spoken Dialog Systems.
InInternational Conference on Speech and LanguageProcessing (ICSLP), Lisbon, Portugal, September.Ingo Mierswa, Michael Wurst, Ralf Klinkenberg, Mar-tin Scholz, and Timm Euler.
2006.
Yale: Rapidprototyping for complex data mining tasks.
In KDD?06: Proceedings of the 12th ACM SIGKDD interna-tional conference on Knowledge discovery and datamining, New York, NY, USA, August.Johannes Pittermann, A. Pittermann, and WolfgangMinker.
2009.
Handling Emotions in Human-Computer Dialogues.
Text, Speech and LanguageTechnology.
Springer, Dordrecht (The Netherlands).Sherif Yacoub, Steven Simske, Xiaofan Lin, and JohnBurns.
2003.
Recognition of emotions in interac-tive voice response systems.
In Proc.
Eurospeech,Geneva, pages 1?4.131
