Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 9?14,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsSemEval-2010 Task 2: Cross-Lingual Lexical SubstitutionRada MihalceaUniversity of North Texasrada@cs.unt.eduRavi SinhaUniversity of North Texasravisinha@unt.eduDiana McCarthyLexical Computing Ltd.diana@dianamccarthy.co.ukAbstractIn this paper we describe the SemEval-2010 Cross-Lingual Lexical Substitutiontask, where given an English target wordin context, participating systems had tofind an alternative substitute word orphrase in Spanish.
The task is based onthe English Lexical Substitution task runat SemEval-2007.
In this paper we pro-vide background and motivation for thetask, we describe the data annotation pro-cess and the scoring system, and presentthe results of the participating systems.1 IntroductionIn the Cross-Lingual Lexical Substitution task, an-notators and systems had to find an alternativesubstitute word or phrase in Spanish for an En-glish target word in context.
The task is basedon the English Lexical Substitution task run atSemEval-2007, where both target words and sub-stitutes were in English.An automatic system for cross-lingual lexicalsubstitution would be useful for a number of ap-plications.
For instance, such a system could beused to assist human translators in their work, byproviding a number of correct translations that thehuman translator can choose from.
Similarly, thesystem could be used to assist language learners,by providing them with the interpretation of theunknown words in a text written in the languagethey are learning.
Last but not least, the outputof a cross-lingual lexical substitution system couldbe used as input to existing systems for cross-language information retrieval or automatic ma-chine translation.2 Motivation and Related WorkWhile there has been a lot of discussion on the rel-evant sense distinctions for monolingual WSD sys-tems, for machine translation applications there isa consensus that the relevant sense distinctions arethose that reflect different translations.
One earlyand notable work was the SENSEVAL-2 JapaneseTranslation task (Kurohashi, 2001) that obtainedalternative translation records of typical usages ofa test word, also referred to as a translation mem-ory.
Systems could either select the most appro-priate translation memory record for each instanceand were scored against a gold-standard set of an-notations, or they could provide a translation thatwas scored by translation experts after the resultswere submitted.
In contrast to this work, in ourtask we provided actual translations for target in-stances in advance, rather than predetermine trans-lations using lexicographers or rely on post-hocevaluation, which does not permit evaluation ofnew systems after the competition.Previous standalone WSD tasks based on par-allel data have obtained distinct translations forsenses as listed in a dictionary (Ng and Chan,2007).
In this way fine-grained senses with thesame translations can be lumped together, how-ever this does not fully allow for the fact that somesenses for the same words may have some transla-tions in common but also others that are not (Sinhaet al, 2009).In our task, we collected a dataset which al-lows instances of the same word to have sometranslations in common, while not necessitatinga clustering of translations from a specific re-source into senses (in comparison to Lefever andHoste (2010)).
1 Resnik and Yarowsky (2000) also1Though in that task note that it is possible for a transla-tion to occur in more than one cluster.
It will be interesting to9conducted experiments using words in context,rather than a predefined sense-inventory howeverin these experiments the annotators were asked fora single preferred translation.
In our case, we al-lowed annotators to supply as many translationsas they felt were equally valid.
This allows usto examine more subtle relationships between us-ages and to allow partial credit to systems thatget a close approximation to the annotators?
trans-lations.
Unlike a full blown machine translationtask (Carpuat and Wu, 2007), annotators and sys-tems are not required to translate the whole contextbut just the target word.3 Background: The English LexicalSubstitution TaskThe English Lexical substitution task (hereafterreferred to as LEXSUB) was run at SemEval-2007 (McCarthy and Navigli, 2007; McCarthy andNavigli, 2009).
LEXSUB was proposed as a taskwhich, while requiring contextual disambiguation,did not presuppose a specific sense inventory.
Infact, it is quite possible to use alternative rep-resentations of meaning, such as those proposedby Schu?tze (1998) and Pantel and Lin (2002).The motivation for a substitution task was thatit would reflect capabilities that might be usefulfor natural language processing tasks such as para-phrasing and textual entailment, while not requir-ing a complete system that might mask system ca-pabilities at a lexical level and make participationin the task difficult for small research teams.The task required systems to produce a substi-tute word for a word in context.
The data wascollected for 201 words from open class parts-of-speech (PoS) (i.e.
nouns, verbs, adjectives and ad-verbs).
Words were selected that have more thanone meaning with at least one near synonym.
Tensentences for each word were extracted from theEnglish Internet Corpus (Sharoff, 2006).
Therewere five annotators who annotated each targetword as it occurred in the context of a sentence.The annotators were each allowed to provide up tothree substitutes, though they could also providea NIL response if they could not come up with asubstitute.
They had to indicate if the target wordwas an integral part of a multiword.see the extent that this actually occurred in their data and theextent that the translations that our annotators provided mightbe clustered.4 The Cross-Lingual LexicalSubstitution TaskThe Cross-Lingual Lexical Substitution task fol-lows LEXSUB except that the annotations aretranslations rather than paraphrases.
Given a tar-get word in context, the task is to provide severalcorrect translations for that word in a given lan-guage.
We used English as the source languageand Spanish as the target language.We provided both development and test sets, butno training data.
As for LEXSUB, any systems re-quiring training data had to obtain it from othersources.
We included nouns, verbs, adjectives andadverbs in both development and test data.
Weused the same set of 30 development words as inLEXSUB, and a subset of 100 words from the LEX-SUB test set, selected so that they exhibit a widevariety of substitutes.
For each word, the same ex-ample sentences were used as in LEXSUB.4.1 AnnotationWe used four annotators for the task, all nativeSpanish speakers from Mexico, with a high levelof proficiency in English.
As in LEXSUB, the an-notators were allowed to use any resources theywanted to, and were required to provide as manysubstitutes as they could think of.The inter-tagger agreement (ITA) was calcu-lated as pairwise agreement between sets of sub-stitutes from annotators, as done in LEXSUB.
TheITA without mode was determined as 0.2777,which is comparable with the ITA of 0.2775 de-termined for LEXSUB.4.2 An ExampleOne significant outcome of this task is that thereare not necessarily clear divisions between usagesand senses because we do not use a predefinedsense inventory, or restrict the annotations to dis-tinctive translations.
This means that there can beusages that overlap to different extents with eachother but do not have identical translations.
Anexample is the target adverb severely.
Four sen-tences are shown in Figure 1 with the translationsprovided by one annotator marked in italics and{} braces.
Here, all the token occurrences seemrelated to each other in that they share some trans-lations, but not all.
There are sentences like 1and 2 that appear not to have anything in com-mon.
However 1, 3, and 4 seem to be partly re-lated (they share severamente), and 2, 3, and 4 arealso partly related (they share seriamente).
When10we look again, sentences 1 and 2, though not di-rectly related, both have translations in commonwith sentences 3 and 4.4.3 ScoringWe adopted the best and out-of-ten precision andrecall scores from LEXSUB (oot in the equationsbelow).
The systems were allowed to supply asmany translations as they feel fit the context.
Thesystem translations are then given credit depend-ing on the number of annotators that picked eachtranslation.
The credit is divided by the numberof annotator responses for the item and since forthe best score the credit for the system answersfor an item is also divided by the number of an-swers the system provides, this allows more creditto be given to instances where there is less varia-tion.
For that reason, a system is better guessingthe translation that is most frequent unless it re-ally wants to hedge its bets.
Thus if i is an itemin the set of instances I , and Tiis the multiset ofgold standard translations from the human annota-tors for i, and a system provides a set of answersSifor i, then the best score for item i is2:best score(i) =?s?Sifrequency(s ?
Ti)|Si| ?
|Ti|(1)Precision is calculated by summing the scoresfor each item and dividing by the number of itemsthat the system attempted whereas recall dividesthe sum of scores for each item by |I|.
Thus:best precision =?ibest score(i)|i ?
I : defined(Si)|(2)best recall =?ibest score(i)|I|(3)The out-of-ten scorer allows up to ten systemresponses and does not divide the credit attributedto each answer by the number of system responses.This allows a system to be less cautious and forthe fact that there is considerable variation on thetask and there may be cases where systems selecta perfectly good translation that the annotators hadnot thought of.
By allowing up to ten translationsin the out-of-ten task the systems can hedge theirbets to find the translations that the annotators sup-plied.2NB scores are multiplied by 100, though for out-of-tenthis is not strictly a percentage.oot score(i) =?s?Sifrequency(s ?
Ti)|Ti|(4)oot precision =?ioot score(i)|i ?
I : defined(Si)|(5)oot recall =?ioot score(i)|I|(6)We note that there was an issue that the origi-nal LEXSUB out-of-ten scorer allowed duplicates(McCarthy and Navigli, 2009).
The effect of du-plicates is that systems can get inflated scores be-cause the credit for each item is not divided by thenumber of substitutes and because the frequencyof each annotator response is used.
McCarthy andNavigli (2009) describe this oversight, identify thesystems that had included duplicates and explainthe implications.
For our task, we decided to con-tinue to allow for duplicates, so that systems canboost their scores with duplicates on translationswith higher probability.For both the best and out-of-ten measures, wealso report a mode score, which is calculatedagainst the mode from the annotators responses aswas done in LEXSUB.
Unlike the LEXSUB task,we did not run a separate multi-word subtask andevaluation.5 Baselines and Upper boundTo place results in perspective, several baselines aswell as the upper bound were calculated.5.1 BaselinesWe calculated two baselines, one dictionary-basedand one dictionary and corpus-based.
The base-lines were produced with the help of an on-line Spanish-English dictionary3 and the SpanishWikipedia.
For the first baseline, denoted by DICT,for all target words, we collected all the Spanishtranslations provided by the dictionary, in the or-der returned on the online query page.
The bestbaseline was produced by taking the first transla-tion provided by the online dictionary, while theout-of-ten baseline was produced by taking thefirst 10 translations provided.The second baseline, DICTCORP, also ac-counted for the frequency of the translationswithin a Spanish dictionary.
All the translations3www.spanishdict.com111.
Perhaps the effect of West Nile Virus is sufficient to extinguish endemic birds already severelystressed by habitat losses.
{fuertemente, severamente, duramente, exageradamente}2.
She looked as severely as she could muster at Draco.
{rigurosamente, seriamente}3.
A day before he was due to return to the United States Patton was severely injured in a road accident.
{seriamente, duramente, severamente}4.
Use market tools to address environmental issues , such as eliminating subsidies for industries thatseverely harm the environment, like coal.
{peligrosamente, seriamente, severamente}5.
This picture was severely damaged in the flood of 1913 and has rarely been seen until now.
{altamente, seriamente, exageradamente}Figure 1: Translations from one annotator for the adverb severelyprovided by the online dictionary for a given targetword were ranked according to their frequencies inthe Spanish Wikipedia, producing the DICTCORPbaseline.5.2 Upper boundThe results for the best task reflect the inherentvariability as less credit is given where annotatorsexpress differences.
The theoretical upper boundfor the best recall (and precision if all items areattempted) score is calculated as:bestub=?i?Ifreqmost freq substitutei|Ti||I|?
100= 40.57 (7)Note of course that this upper bound is theoreticaland assumes a human could find the most frequentsubstitute selected by all annotators.
Performanceof annotators will undoubtedly be lower than thetheoretical upper bound because of human vari-ability on this task.
Since we allow for duplicates,the out-of-ten upper bound assumes the most fre-quent word type in Tiis selected for all ten an-swers.
Thus we would obtain ten times the bestupper bound (equation 7).ootub=?i?Ifreqmost freq substitutei?10|Ti||I|?
100= 405.78 (8)If we had not allowed duplicates then the out-of-ten upper bound would have been just less than100% (99.97).
This is calculated by assuming thetop 10 most frequent responses from the annota-tors are picked in every case.
There are only a cou-ple of cases where there are more than 10 transla-tions from the annotators.6 SystemsNine teams participated in the task, and severalof them entered two systems.
The systems usedvarious resources, including bilingual dictionar-ies, parallel corpora such as Europarl or corporabuilt from Wikipedia, monolingual corpora suchas Web1T or newswire collections, and transla-tion software such as Moses, GIZA or Google.Some systems attempted to select the substituteson the English side, using a lexical substitu-tion framework or word sense disambiguation,whereas some systems made the selection on theSpanish side using lexical substitution in Spanish.In the following, we briefly describe each par-ticipating system.CU-SMT relies on a phrase-based statistical ma-chine translation system, trained on the EuroparlEnglish-Spanish parallel corpora.The UvT-v and UvT-g systems make use of k-nearest neighbour classifiers to build one word ex-pert for each target word, and select translationson the basis of a GIZA alignment of the Europarlparallel corpus.The UBA-T and UBA-W systems both use can-didates from Google dictionary, SpanishDict.comand Babylon, which are then confirmed using par-allel texts.
UBA-T relies on the automatic trans-lation of the source sentence using the GoogleTranslation API, combined with several heuristics.The UBA-W system uses a parallel corpus auto-matically constructed from DBpedia.SWAT-E and SWAT-S use a lexical substitutionframework applied to either English or Spanish.The SWAT-E system first performs lexical sub-12stitution in English, and then each substitute istranslated into Spanish.
SWAT-S translates thesource sentences into Spanish, identifies the Span-ish word corresponding to the target word, andthen it performs lexical substitution in Spanish.TYO uses an English monolingual substitutionmodule, and then it translates the substitution can-didates into Spanish using the Freedict and theGoogle English-Spanish dictionary.FCC-LS uses the probability of a word to betranslated into a candidate based on estimates ob-tained from the GIZA alignment of the Europarlcorpus.
These translations are subsequently fil-tered to include only those that appear in a trans-lation of the target word using Google translate.WLVUSP determines candidates using the bestN translations of the test sentences obtained withthe Moses system, which are further filtered us-ing an English-Spanish dictionary.
USPWLV usescandidates from an alignment of Europarl, whichare then selected using various features and a clas-sifier tuned on the development data.IRST-1 generates the best substitute using a PoSconstrained alignment of Moses translations of thesource sentences, with a back-off to a bilingualdictionary.
For out-of-ten, dictionary translationsare filtered using the LSA similarity between can-didates and the sentence translation into Spanish.IRSTbs is intended as a baseline, and it uses onlythe PoS constrained Moses translation for best,and the dictionary translations for out-of-ten.ColEur and ColSlm use a supervised word sensedisambiguation algorithm to distinguish betweensenses in the English source sentences.
Trans-lations are then assigned by using GIZA align-ments from a parallel corpus, collected for theword senses of interest.7 ResultsTables 1 and 2 show the precision P and recallR for the best and out-of-ten tasks respectively,for normal and mode.
The rows are ordered byR.
The out-of-ten systems were allowed to pro-vide up to 10 substitutes and did not have any ad-vantage by providing less.
Since duplicates wereallowed so that a system can put more emphasison items it is more confident of, this means thatout-of-ten R and P scores might exceed 100%because the credit for each of the human answersis used for each of the duplicates (McCarthy andNavigli, 2009).
Duplicates will not help the modescores, and can be detrimental as valuable guesseswhich would not be penalised are taken up withSystems R P Mode R Mode PUBA-T 27.15 27.15 57.20 57.20USPWLV 26.81 26.81 58.85 58.85ColSlm 25.99 27.59 56.24 59.16WLVUSP 25.27 25.27 52.81 52.81SWAT-E 21.46 21.46 43.21 43.21UvT-v 21.09 21.09 43.76 43.76CU-SMT 20.56 21.62 44.58 45.01UBA-W 19.68 19.68 39.09 39.09UvT-g 19.59 19.59 41.02 41.02SWAT-S 18.87 18.87 36.63 36.63ColEur 18.15 19.47 37.72 40.03IRST-1 15.38 22.16 33.47 45.95IRSTbs 13.21 22.51 28.26 45.27TYO 8.39 8.62 14.95 15.31DICT 24.34 24.34 50.34 50.34DICTCORP 15.09 15.09 29.22 29.22Table 1: best resultsduplicates.
In table 2, in the column marked dups,we display the number of test items for which atleast one duplicate answer was provided.
4 Al-though systems were perfectly free to use dupli-cates, some may not have realised this.
5 Dupli-cates help when a system is fairly confident of asubset of its 10 answers.We had anticipated a practical issue to come upwith all participants, which is the issue of differentcharacter encodings, especially when using bilin-gual dictionaries from the Web.
While we werecounting on the participants to clean their files andprovide us with clean characters only, we ended upwith result files following different encodings (e.g,UTF-8, ANSI), some of them including diacrit-ics, and some of them containing malformed char-acters.
We were able to perform a basic cleaningof the files, and transform the diacritics into theirdiacriticless counterparts, however it was not pos-sible to clean all the malformed characters withouta significant manual effort that was not possibledue to time constraints.
As a result, a few of theparticipants ended up losing a few points becausetheir translations, while being correct, containedan invalid, malformed character that was not rec-ognized as correct by the scorer.There is some variation in rank order of the sys-tems depending on which measures are used.
64Please note that any residual character encoding issueswere not considered by the scorer and so the number of du-plicates may be slightly higher than if diacritics/different en-codings had been considered.5Also, note that some systems did not supply 10 transla-tions.
Their scores would possibly have improved if they haddone so.6There is not a big difference between P and R because13Systems R P Mode R Mode P dupsSWAT-E 174.59 174.59 66.94 66.94 968SWAT-S 97.98 97.98 79.01 79.01 872UvT-v 58.91 58.91 62.96 62.96 345UvT-g 55.29 55.29 73.94 73.94 146UBA-W 52.75 52.75 83.54 83.54 -WLVUSP 48.48 48.48 77.91 77.91 64UBA-T 47.99 47.99 81.07 81.07 -USPWLV 47.60 47.60 79.84 79.84 30ColSlm 43.91 46.61 65.98 69.41 509ColEur 41.72 44.77 67.35 71.47 125TYO 34.54 35.46 58.02 59.16 -IRST-1 31.48 33.14 55.42 58.30 -FCC-LS 23.90 23.90 31.96 31.96 308IRSTbs 8.33 29.74 19.89 64.44 -DICT 44.04 44.04 73.53 73.53 30DICTCORP 42.65 42.65 71.60 71.60 -Table 2: out-of-ten resultsUBA-T has the highest ranking on R for best.
US-PWLV is best at finding the mode, for best how-ever the UBA-W and UBA-T systems (particularlythe former) both have exceptional performance forfinding the mode in the out-of-ten task, thoughnote that SWAT-S performs competitively giventhat its duplicate responses will reduce its chanceson this metric.
SWAT-E is the best system for out-of-ten, as several of the items that were empha-sized through duplication were also correct.The results are much higher than for LEX-SUB (McCarthy and Navigli, 2007).
There are sev-eral possible causes for this.
It is perhaps easierfor humans, and machines to come up with trans-lations compared to paraphrases.
Though the ITAfigures are comparable on both tasks, our task con-tained only a subset of the data in LEXSUB and wespecifically avoided data where the LEXSUB an-notators had not been able to come up with a sub-stitute or had labelled the instance as a name e.g.measurements such as pound, yard or terms suchas mad in mad cow disease.
Another reason forthis difference may be that there are many parallelcorpora available for training a system for this taskwhereas that was not the case for LEXSUB.8 ConclusionsIn this paper we described the SemEval-2010cross-lingual lexical substitution task, includingthe motivation behind the task, the annotation pro-cess and the scoring system, as well as the partic-ipating systems.
Nine different teams with a totalsystems typically supplied answers for most items.
However,IRST-1 and IRSTbs did considerably better on precision com-pared to recall since they did not cover all test items.of 15 different systems participated in the task, us-ing a variety of resources and approaches.
Com-parative evaluations using different metrics helpeddetermine what works well for the selection ofcross-lingual lexical substitutes.9 AcknowledgementsThe work of the first and second authors has been partiallysupported by a National Science Foundation CAREER award#0747340.
The work of the third author has been supportedby a Royal Society UK Dorothy Hodgkin Fellowship.
Theauthors are grateful to Samer Hassan for his help with theannotation interface.ReferencesMarine Carpuat and Dekai Wu.
2007.
Improving statis-tical machine translation using word sense disambigua-tion.
In Proceedings of the Joint Conference on Empir-ical Methods in Natural Language Processing and Com-putational Natural Language Learning (EMNLP-CoNLL2007), pages 61?72, Prague, Czech Republic, June.
Asso-ciation for Computational Linguistics.Sadao Kurohashi.
2001.
SENSEVAL-2 japanese translationtask.
In Proceedings of the SENSEVAL-2 workshop, pages37?44.Els Lefever and Veronique Hoste.
2010.
SemEval-2007 task3: Cross-lingual word sense disambiguation.
In Proceed-ings of the 5th International Workshop on Semantic Eval-uations (SemEval-2010), Uppsala, Sweden.Diana McCarthy and Roberto Navigli.
2007.
SemEval-2007task 10: English lexical substitution task.
In Proceedingsof the 4th International Workshop on Semantic Evalua-tions (SemEval-2007), pages 48?53, Prague, Czech Re-public.Diana McCarthy and Roberto Navigli.
2009.
The Englishlexical substitution task.
Language Resources and Eval-uation Special Issue on Computational Semantic Analysisof Language: SemEval-2007 and Beyond, 43(2):139?159.Hwee Tou Ng and Yee Seng Chan.
2007.
SemEval-2007 task11: English lexical sample task via English-Chinese paral-lel text.
In Proceedings of the 4th International Workshopon Semantic Evaluations (SemEval-2007), pages 54?58,Prague, Czech Republic.Patrick Pantel and Dekang Lin.
2002.
Discovering wordsenses from text.
In Proceedings of ACM SIGKDD Con-ference on Knowledge Discovery and Data Mining, pages613?619, Edmonton, Canada.Philip Resnik and David Yarowsky.
2000.
Distinguishingsystems and distinguishing senses: New evaluation meth-ods for word sense disambiguation.
Natural LanguageEngineering, 5(3):113?133.Hinrich Schu?tze.
1998.
Automatic word sense discrimina-tion.
Computational Linguistics, 24(1):97?123.Serge Sharoff.
2006.
Open-source corpora: Using the net tofish for linguistic data.
International Journal of CorpusLinguistics, 11(4):435?462.Ravi Sinha, Diana McCarthy, and Rada Mihalcea.
2009.Semeval-2010 task 2: Cross-lingual lexical substitution.In Proceedings of the NAACL-HLT Workshop SEW-2009- Semantic Evaluations: Recent Achievements and FutureDirections, Boulder, Colorado, USA.14
