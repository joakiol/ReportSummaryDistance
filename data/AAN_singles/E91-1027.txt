THE RECOGNITION CAPACITY OF LOCAL SYNTACTIC CONSTRAINTSMori Rimon'Jacky Herz ~The Computer Science DepartmentThe Hebrew University of Jerusalem,Giv'at Ram, Jerusalem 91904, ISRAELE-mail: r imon@hujics.BITNETAbstractGivcn a grammar for a language, it is possible tocreate finite state mechanisms that approximateits recognition capacity.
These simple automataconsider only short context information~ drawnfrom local syntactic constraints which thegrammar hnposes.
While it is short of providingthe strong generative capacity of the grammar,such an approximation is useful for removingmost word tagging ambiguities, identifying manycases of iU-fonncd input, and assisting efficientlyin othcr natural language processing tasks.
Ourbasic approach to the acquisition and usage oflocal syntactic constraints was presented clse-whcre; in this papcr we present some formal andempiric-,d results pertaining to properties of theapproximating automata.1.
IntroductionParsing is a process by which an input sentenceis not only recognized as belonging to the lan-guage, but is also assigned a structure.
As\[l\]erwick/Wcinbcrg 84\] commcnt, recognitionper se (i.e.
a weak generative capacity analysis) isnot of much value for a theory of languageunderstanding, but it can be useful "as a diag-nostic".
We claim that if an cfficient recognitionprocedure is availat~le, it can be tnost valuable asa prc-parsing reducer of lcxical ambiguity (espe-cially, as \[Milne 86\] points out, for detcnninisticparsers), and cvcn more useful in applicationswhere full parsing is not absolutely required -e.g.
identification of iU-formed inputs in a textcritique program.
Still weaker than recognitionprocedures are 'methods which approximate therecognition capacity.
This is the kind of methodsthat we discuss in this paper.More specifically, we analyze the recognitioncapacity of automata based on local (shortcontext) considerations.
In \[Herz/Rimon 91\] weprescnted our approach to the acquisition andusage of local syntactic onstraints, focusing onits use for reduction of word-level ambiguity.After briefly reviewing this method in section 2below, we examine in more detail various char-acteristics of the approximating automata, andsuggest several applications.2.
Background: Local SyntacticConstraintsLet S = Wi,..., W be a sentence of length N,{Wi} being the words composing the sentence.And let ti ..... t be a tag image corresponding tothe sentence S, {ti} belonging to the tag set T -the set of word-class tags used as terminalsymbols in a given grammar G. Typically,M=N, but in a more general environment weallow M > N .
This is useful when dealing withlanguages where morphology allows cliticization,concatenation of conjunctions, prepositions, Ordeterminers to a verb or a noun, etc.
; in gram-mars for l lebrew, for example, it is convenientJ M. Rimon's main atfiliafion is the IBM Scientific Center, i laifa, Israel, E-mail: rimon@haifasc3.iinusl.ibm.com2 j. I Icrz was partly supported by the I.eihniz ('enter for R.esearch in Computer Science, the !
lebrew University,and by the Rau foundation of the Open University.155 -to assume that a preliminary morphologicalphase separated word-forms to basic sequencesof tags, and then state syntactic rules in terms ofstandard word classes.In any case, it is reasonable to assume that thetag image it ..... IM cannot be uniquely assigned.Fven with a coarse tag set (e.g.
parts of speechwith no features) many words have more thanone interpretation, thus giving rise to exponen-tially many tag images for a sentence.
3Following \[Karlsson 90\], we use the term cohortto refer to the set of lcxicaUy valid readings of agiven word.
We use the term path to refer to asequence of M tags (M~ N) which is a tag-image corresponding to the words W,..., WN ofa given sentence S. This is motivated by a viewof lexical mnbiguity as a graph problem: we tryto reduce the number of tentative paths inambiguous cases by removing arcs from the Sen-tence Graph (SG) - a directed graph with ver-tices for all tags in all cohorts of the words inthe given sentence, and arcs connecting each tagto ~dl tags in the cohort which follows it.The removal of arcs and the testing of paths forvalidity as complete sentence interpretations aredone using local constraints.
A local constraintof length k on a given tag t is a rule allowing ordisaUowing a sequence of k tags from being inits right (or left) neighborhood in any tag imageof a sentence.
In our approach, the local con-straints are extractcd from the grammar (and thisis the major aspect distinguishing it from someother short context methods uch as \[Beale 881,\[DeRose 88\], \[Karlsson 90\], \[Katz 851,\[Marcus 80\], \[Marshall 831, and \[Milnc 861).For technical convenience we add the symbol"$ <" at the beginning of tag images and "> $~ atthe etad.
Given a grammar G (wlfich for the timebeing we assume to be an unrestricted context-free phrase structure grammar), with a:set T ofterminal symbols (tag set), a set V of variables(non-terminals, among which S is the root vail-able for derivations), and a set P of productionrules of the form A --.
a, where A is in V and ais in (VUT)*  , we define the Right ShortContext of length k of a terminal t (tag):SCr (t,k) for t in T and for k = 0,1,2,3...tz I z ~ T* ,  Izl=k or Izl < k if"> $' is the last tag in z,and there exists a derivationS = > atz// (a, / /~ (V U T)* )The l.eft Short Context of length k of a tag t rel-ative to the grammar G is denoted by SCI (t,k)and defined in; a similar way.It is sometimes useful to define Positional ShortContexts.
The definition is similar to the above,with a restriction that t may start only in a givenposition in a tag image of a sentence.The basis for the automaton Which checks a tagstream (path) for validity as a tag-image relativeto the local constraints, is the function next(t),which for any t in T defines a set, as follows: :next  (t) = { z I tz  E SCr  ( t , l )  }In \[ I lerz/Rimon 911 we gave a procedure forcomputing next(t) from a given context freegrammar, using standard practices of parsing offormal languages (see \[Aho/Ulhnan 72\]).3.
Local Constraints AutomataWe denote by LCA(I) the simple finite stateautomaton which uses the pre-processed{next(t)} sets to check if a given tag stream(path) satisfies the SCr(t,l) constraints.In a similar: manner it is possible to defineLCA(k), relative to the short context of length k.We denote by L the language generated by the3 Our studies of modern written !
lebrew suggest that about 60% of the word-forms in running texts are ambiguouswith respect o a basic tag set, and the :average number of possible readings of such word-forms is 2.4.
Evenwhen counting only "natural readings', i.e.
interpretations which are likely to occur in typical corpora, thisnumber is quite large, around 1.8 (it is somewhat larger for the small subset of the most common words).156 -underlying grammar, and by L(k) the languageaccepted by the automaton LCA(k).
The fol-lowing relations hold for the family of automata(LCA(i)}:L(I) _~ L(2) _~ ... ~ L"llfis guarantees a security feature: If for some i,I.CA(i) does not recognize (accept) a string oftags, then this string is sure to be illegM (i.e.
notin 1.).
On the other hand, any LCA(k) may rec-ognize sentences not in L (or, from a dual pointof view, will reject only part of the illegal tagimages).
The important question is how tight arethe inclusion relations above - i.e.
how wellLCA(k) approximates the language I.. in partic-ular we are interestcd in LCA(I).There is no simple analytic answer to tiffs ques-tion.
Contradictory forces play here: the natureof the language -- c.g a rigid word order andconstituent order yield stronger constraints; thegrain of the tag set -- better refined tags (dif-ferent languages may require different tag sets)help express refined syntactic laims, hence morespecific constraints, but they "also create a greaterlevel of tagging ambiguity; the size of thegrammar -- a larger grammar offers more infor-mation, but, covering a richer set of structures, it?
allows more tag-pairs to co-occur; etc.It is interesting to note that for l lebrew, shortcontext methods are most needed because of theconsiderable ambiguity at the lexical level, buttheir cll~:ctiveness suffers from the rather freeword/constituent order.Finally, a comment about the computationalefficiency of the LCA(k) automaton.
The timecomplexity of checking a tag string of length nusing I,CA(k) is at most O(n x k x loglTI),while a non-deterministic parser for a contextfree grmntnar may require O(n3x IGI2).
(IT\] isthe size of the tag set, IGI is the size of thegrammar).
The space complexity of l,CA(k) isproportionM to \]7\] k?~ ; this is why otfly trulyshort contexts hould be used.Note that for a sentence of length k, the powerof LCA(k) is idcnticM to the weak generativecapacity of the full underlying grammar.
Butsince the size of sentences (tag sequences) in L isunbounded, there is no fixed k which suffices.4.
A Sample GrammarTo illustrate claims made in the sections below,we will use the following toy grammar of a smallfragment of English.
Statements about the cor-rectness of sentences etc., are of course relativeto this toy grammar.The tag set T includes: n (noun), v (verb), det(determiner), adj ( adjective ) and prep (preposi-tion).
The context free grammar G is:S --> $< NP VP >$NP--> (det) (adj) nNP --> NP PPPP --> prep NPVP --> v NPVP --> VP PPTo extract the local constraints from thisgrammar, we first compute the function next(t)for every tag t in T, and from the resulting setswe obtain the graph below, showing valid pairsin the short context of length 1 (again, validity isrelative to the given toy grammar):>$This graph, or more conveniently the table of"valid neighbors" below, define the LCA(I)automaton.
The table is actually the union ofthe SCr(t,l) sets for all t in T, and it is deriveddirectly from the graph:$< det adj n prep adj$< adj v det prep n$< n v adj n prepdet adj v n n vdet n prep det n >$- 157  -5.
A "Lucky Bag" ExperimentConsider the following sentence, which is in thelanguage gcncratcd by grammar G of section 4:(1) Thc channing princess kissed a frog.The unique tag image corresponding to this sen-tence is: \[ $ <, dot, adi, n, v, det, n, > $ \].Now let us look at the 720 "random inputs" gen-erated by permutations of the six words in (i),and the set of corresponding tag images.Applying I.CA(I), only two tag images arerccog~.ed as valid: \[ $ <, det, adj, n, v, det, n,>$ \], and \[ $<,  dct, n, v, dot, adj, n, >$ \].These are exactly the images corresponding tothe eight syntactically correct sentences (relativeto G),(la-b) The/a charming princess kissed a/the frog.
(lc-d) The/a chamfing frog kissed a/the princess.
(lc-t') The/a princess kissed a/the charming frog.
(lg-h) The/a frog kissed a/the charming princess.This result is not surprising, given the simplescntence and toy grammar.
(In general, agrammar with a small number of rules relative tothe size of the tag set cannot produce too manyvalid short contexts).
It is therefore interestingto examine another example, where each word isassociated with a cohort of several interpreta-tions.
We borrow from \[llcrz/Rimon 9.1\]:(2) All old people like books about fish.Assuming the word tagging shown in section 6,there are 256 (2 x 2 x 2 x 4 x 2 x 2 x 2) tentativetag hnages (paths) for this sentence and for eachof its 5040 permutations.
This generates a veryhtrge number of rather random tag images.Applying LCA(I), only a small number ofhnages are rccogtfizcd as potentially valid.Among them are syntactically correct sentencessuch as:(2a) Fish like old books about all people.,and only less than 0.1% sentences which arelocally valid but globally incorrect, such as:(2b) * Old tish all about books like people.
(tagged as \[$ <, n, v, n, prep, n, v, n, > $\]).These two examples do not suggest any kind ofproof, but they well illustrate the recognitionpower of even the least powerful automaton inthe {LeA(i)} family.
To get another point ofview, one may consider the simple formal lan-guage L consisting of the strings {ar"b m} forI < rn, which can be generated by a context-freegrammar (} over T = {a, b}.
I.CA(I) based on(; will recognize all strings of the form (a'b ~} for1 <j,k, but none of the very many other stringsover T. It can be shown that, given arbitrarystrings of length n over T, the probability thatLeA(I)  will not reject strings not belonging to Lis proportional to n/2", a term which tendsrapidly to 0.
This is the over-recognition margin.6.
Use of LeA in Conjunction with aParserThe number of potentially valid tag images(paths) for a given sentence can be exponentialin the length of the sentence if all words areambiguous.
It is therefore desirable to filter outinvalid tag images before (or during) parsing.To examine the power of LCAs as a pre-parsingfdter, we use example (2) again, demonstratinglexical ambiguities as shown in the chart below.The chart shows the Reduced Sentence Graph(RSG) - the original SG from which invalid arcs(relative to the SCr(t,l) table) were removed.ALL OLD PEOPLE LIKE BOOKS ABOUT FISHdet--~adj--~n ~v - ~ n---~prep--->nn n ) v__prep j  --e v >$nWe are left with four valid paths through thesentence, out of the 256 tentative paths in SG.Two paths represent legal syntactic interpreta-tions (of which one is "the intended" meaning).The other two are locally valid but globallyincorrect, having either two verbs or no verb at- 158  -all, in contrast o the grammar.
SCr(t,2) wouldhave rejected one of the wrong two.Note that in this particular example the methodwas quite effective in reducing sentence-wideinterpretations (leaving an easy job even for adeterministic parser), but it was not very good inindividual word tagging disambiguation.
Thesetwo sub-goals of raging disambiguationreducing the number of paths and reducingword-level possibilities - are not identical.
It ispossible to construct sentences in which allwords are two-way ambiguous and only two dis-joint paths out of the 2 N possible paths are legal,thus preserving all word-level ambiguity.We demonstrated the potential of efficient pathreduction for a pre-parsing filter.
But short-con-text techniques can also be integrated into theparsing process itself.
In this mode, when theparser hypothesizes the existence of a constit-uent, it will first check if local constraints do notrule out that hypothesis.
In the example above,a more sophisticated method could have usedthe fact that our grammar does not allow verbsin constituents other than VP, or that it requiresone and only one verb in the whole sentence.The motiwttion for this method, and its princi-ples of operation, are similar to those behind dif-ferent tecimiques combining top-down andbottom-up considerations.
The performancegains depend on the parsing technique; ingeneral, allowing early decisions regarding incon-sistent tag assignments, based on informationWhich may be only implicit in the grammar,offers considerable savings.7.
Educated Guess of Unknown WordsAnother interesting aid Which local syntacticconstraints can provide for practical parsers is"an oracle" which makes "educated guesses ~about unknown words.
It is typical for languageanalysis systems to assume a noun whenever anunknown word is encountered.
There is sense intiffs strategy, but the use of LCA, even LCA(I),can do much better.To illustrate this feature, we go back to the prin-cess and the frog.
Suppose that an adjectiveunknown to the system, say 'q'ransylvanian" wasused rather than "charming" in example (1),yielding the input sentence:(3) The Transylvanian princess kissed a frog.Checking out all tags in T in the second positionof the tag image of this sentence, the only tagthat satisfies the constraints of LCA(1) is adj.8.
"Context Sensitive" SpellingVerificationA related application of local syntactic con-straints is spelling verification beyond the basicword level (which is, in fact, SCr(t,0) ).Suppose that while typing sentence (1), a usermade a typing error and instead of the adjective"charming u wrote "charm" (or "arming", or anyother legal word which is interpreted as a noun):(4) The charm princess kissed a frog.This is the kind of errors* that a full parserwould recognize but a word-based spell-checkerwould not.
But in many such cases there is noneed for the "full power (and complexity) of aparser; even LCA(I)  can detect the error.
Ingeneral, an LCA which is based on a detailedgrammar, offers cheap and effective means forinvalidation of a large set of ill-formed inputs.Here too, one may want to get another point ofview by considering the simple formal languageL = {ambm}.
A single typo results in a stringwith one "a', changed for a "W, or vice versa.Since LCA(i) recognizes strings of the form{aJb ~} for 1 <_j,k, given arbitrary strings of lengthn over T = (a, b}, LCA(I) will detect "all buttwo of the n single typos possible - those on theborderline between the a's and b's.Remember that everything is relative to ~ the toy grammar  used  throughout this paper.
Hence, although "thecharm princess" may be a perfect noun phrase, it is illegal relative to our grammar.- 159  -9.
Assistance to Tagging SystemsTaggcd corpora are important resources formany applications.
Since manual tagging is aslow and expensive process, it is a commonapproach to try automatic hcuristics and resortto user interaction only when there is no dccisiveinformation.
A well-built tagging system can"learn" and improve its performance as moretext is processed (e.g.
by using the already taggedcorpus as a statistical knowledge base).Arguments uch as those given in sections 7 and8 above suggest hat the use of local constraintscan resolve many tagging ambiguities, thusincrcasing the "specd of convergence" of an auto-matic tagging system?
This seems to be true evenfor the rather simple and inexpensive I,CA(I) forlaaaguagcs with a relatively rigid word order.
Forrelated work cf.
\[Grccne/Rubin 71\], I~Church88\], \[ l)cRose 88\], and \[Marshall 83\].10.
Final RemarksTo make our presentation simpler, we havelimited thc discussion to straightforward contextfree grammars.
But the method is more gcnerzd.It can, for example, he extended to Ci:Gs aug-mented with conditional equations on features(such as agrccmcnt)- cither by translathag suchgrammars to equivalent CFGs with a moredetailed tag set (assuming a finite range offeature values), or by augmenting our a:utomatawith conditions on arcs.
It can also be extendedfor a probabilistic language model, generatingprobabilistic onstraints on tag sequences from aprobabilistic CFG (such as of \[Fujisaki et ",3.1.89\]).Perhaps more interestingly, the method can beused even without an underlying grammar, if alarge corpus and a lexical analyzer (which sug-gests prc-disambiguatcd cohorts) are available.This variant is based on a tcchnique of invali-dation of tag pairs (or longer sequences) whichsatisfy certain conditions over the whole lan-guage L, and the fact that L can be approxi-matcd by a large corpus.
We cannot elaborateon this extcnsion here.References\[ Aho/UIIman 72\] Alfred V. Aho and Jeffrey D. Jllman.
7"he Theory of Parsing, Translation andCompiling.
Prentice-!
lall, 1972-3.f Bcalc 88\] Andrew David 13eale.
I~exicon and ;rammar in Probabilistic Tagging of WrittenFnglish.
Proc.
of the 26th Annual Meeting of theACL, Buffalo NY, 1988.\[Berwick/Wcinberg 84\] Robert C. Berwick andAmy S. Weinberg.
"/'he Grammatical Basis ofLinguistic Performance, The M IT Press, 1984.\[Church 88\] Kenneth W. Church.
A Sto-chastic Parts Program and Noun Phrase Parserfor Running Text.
Proc.
of the 2nd A CL conf.on Applied Natural Language Processing.
1988.\[DcRose 88\] Steven J. l)eRose.
GrammaticalCategory Dnsambiguation by Statistical Opti-mization.
Computational Linguistics, vol.
14, no.1, 1988.Fujisaki et al 89\] T. Fujisaki, F. Jelinek, J.~'ocke, E. Black, T. Nishimo.
A ProbabilisticParsing Method for Sentence l)isambiguation.Proc.
of the Ist International Parsing Workshop,Pittsburgh, June 1989.~ ;rcene/Rubin 71\] Barbara Greene and Gerald ubin.
Automated Grammatical Tagging ofll:~ish.
Technical Report, Brown Umversity,llerz/Rinnon 91\] Jacky llerz and Mori Rimon.,ocal Syntactic Constraints.
Proc.
of the 2ndInternational Workshop on Parsing Technologies,Cancun, February 1991.Karlsson 90\] Fred Karlsson.
Constraintrammar as a Framework for Parsing RunningText.
The 13th COLING Conference, Helsinki,1990.\[Katz 85\] Slava Katz.
Recursive M-gram l_,an-guage Model Via Smoothing of Turing Formula.IBM Technical Disclosure Bulletin, 1985.~ larcus 80\] Mitchell P. Marcus.
A Theo~ of ntactic Recognition for Natural Language, l'heIT Press, 1980.\[Marshall 83\] lan Marshall.
Choice of Gram-matical Word-Class Without Global SyntacticAnalysis: Tagging Words in the LOB Corpus.Computers in the llumanities, vol.
17, pp.139-150, 1983.Mi lne  86\] Robert Milne.
Resolving Lexicalmbiguity in a Deterministic Parser.
Computa-tionalLinguistics, vol.
12, no.
1, pp.
1-12, 1986?- 160-
