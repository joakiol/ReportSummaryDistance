Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 2217?2226, Dublin, Ireland, August 23-29 2014.Augment Dependency-to-String Translation with Fixed and FloatingStructuresJun Xie?Jinan Xu?Qun Liu??
?Key Laboratory of Intelligent Information Processing,Institute of Computing Technology,Chinese Academy of Sciencesxiejun@ict.ac.cn?School of Computer and Information Technology, Beijing Jiaotong Universityxja2010@gmail.com?School of Computing, Dublin City Universityqliu@computing.dcu.ieAbstractIn this paper, we propose an augmented dependency-to-string model to combine the merits ofboth the head-dependents relations at handling long distance reordering and the fixed and floatingstructures at handling local reordering.
For this purpose, we first compactly represent both thehead-dependent relation and the fixed and floating structures into translation rules; second, indecoding we build ?on-the-fly?
new translation rules from the compact translation rules thatcan incorporate non-syntactic phrases into translations, thus alleviate the non-syntactic phrasecoverage problem of dependency-to-string translation (Xie et al., 2011).
Large-scale experimentson Chinese-to-English translation show that our augmented dependency-to-string model gainssignificant improvement of averaged +0.85 BLEU scores on three test sets over the dependency-to-string model.1 IntroductionAs a representation holding both syntactic and semantic information, dependency grammar has beenattracting more and more attention in statistical machine translation.
Lin (2004) took paths as the el-ementary structures and proposed a path-based transfer model.
Quirk et al.
(2005) extended path totreelets (connected subgraphs of dependency trees) and put forward dependency treelet translation.
Dingand Palmer (2005) proposed a model on the basis of dependency insertion grammar.
Shen et al.
(2008)employed the fixed and floating structures as elementary structures and proposed a string-to-dependencymodel with state-of-the-art performance.
Xie et al.
(2011) employs head-dependents relations as elemen-tary structures and proposed a dependency-to-string model with good long distance reordering property.A head-dependents relation (HDR) is composed of a head and all its dependents, which can be viewedas an instance of a sentence pattern or phrase pattern.However, since dependency trees are much flatter than constituency trees, the dependency-to-stringmodel suffers more severe non-syntactic phrase coverage problem (Meng et al., 2013) than constituency-based models (Galley et al., 2004; Liu et al., 2006; Huang et al., 2006).
Non-syntactic phrases are thosephrases that can not be covered by whole subtrees.
To address this problem, Meng et al.
(2013) proposedto translate with both constituency and dependency trees, which can incorporate non-syntactic phrasescovered by the constituents of the constituency trees.
This model requires both constituency and depen-dency trees, thus may suffer from both constituency and dependency parse errors.
Additionally, there areonly few languages that have both constituency and dependency parsers, which limits its practical use.In this paper, we propose to address non-syntactic phrase coverage problem of the dependency-to-string model without resort to extra resources (Section 3).
To this end, we augment the dependency-to-string model at two aspects.
First, we combine the merits of both the head-dependent relations and thefixed and floating structures (Shen et al., 2008), and compactly represent these two kinds of knowlegeinto augmented HDR rules (Section 3.1).
We acquire the augmented HDR rules automatically from theThis work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/2217X4:VVX5:NNX3:!
*(a)X3 X2X5X2:NTX1:PNX4X1X4:VVX5:NNX3:!
*X3 X2X5X2:NTX1:PNX4X1(b)Figure 1: Examples of an HDR rule (a) and an augmented HDR rule (b).
Where each ?*?
denotesa substitute site which is a compact representation of a whole subtree.
The shadow with line borderindicates a fixed structure and the shadow with dash line border indicates a floating structure.word-aligned source dependency tree and target string paris (Section 3.2).
In decoding we propose an?on-the-fly?
rule building strategy, which builds new translation rules from the augmented HDR rulesand incorporates non-syntactic phrases into translations (Section 3.4).
Large-scale experiments (Section4) on Chinese-to-English translation show that our augmented model gains significant improvement ofaveraged +0.85 BLEU points on three test sets over the dependency-to-string model.2 BackgroundFor convenience of the description of our augmented dependency-to-string model, we first briefly re-view the dependency-to-string model and the fixed and floating structures of string-to-dependency model(Shen et al., 2008).2.1 Dependency-to-String TranslationThe dependency-to-string model (Xie et al., 2011) takes head-dependents relations as the elementarystructures of dependency trees, and represents the translation rules with the source side as HDRs andthe target side as string.
Since the HDRs in essence relate to phrase patterns and sentence patterns,the HDR rules specify the reordering of these patterns.
For example, Figure 1 (a) is an example HDRrule, which represents a reordering manner of a sentence pattern composed of a proper noun (X1:PN),a temporal noun (X2:NT), an prepositional phrase relate to ??
(give)?
(X3:?
), a verb (X4:VV) and anoun (X5:NN).With the HDR rules, the dependency-to-string model gets rid of the extra reordering heuristics andreordering models of the previous models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005).
Moreimportantly, the model shows state-of-the-art performance and exhibits good long distance reorderingproperty.2.2 Fixed and Floating StructuresThe fixed structures and floating structures are fundamental structures of the string-to-dependency model(Shen et al., 2008), which are introduced to handle the coverage of non-constituent rules.
Given thedependency tree d1d2...dnof a sentence f1f2...fn, where diindicates the parent word index of word fi.Definition 1.
A dependency structure di...jis fixed on the head h, where h ?
[i, j], if and only if it meetsthe following conditions:- dh/?
[i, j]- ?k ?
[i, j] and k 6= h, dk?
[i, j]- ?k /?
[i, j], dk= h or dk/?
[i, j]A fixed structure describes a fragment with a sub-root, where all the children of the sub-root arecomplete.2218Definition 2.
A dependency tree di...jis floating with children C, for a non-empty set C ?
i, ..., j, if andonly if it meets the following conditions:- ?h /?
[i, j], s.t.
?k ?
C, dk= h- ?k ?
[i, j] and k /?
C, dk?
[i, j]- ?k /?
[i, j], dk/?
[i, j]A floating structure consists of sibling nodes of a common head, but the head itself is unspecified.In nature, the fixed and floating structures represent the phrases under the structural constraint ofdependency trees, most of them are non-syntactic phrases.The HDRs are good at handling long distance dependencies, while the fixed and floating structuresexcels at handling local reordering.
This encourages us to address the non-syntactic phrase coverageproblem of dependency-to-string model by exploiting these two kinds of structures.3 Augmented Dependency-to-String TranslationIn the following, we will describe our augmented dependency-to-string model in detail, including theaugmented HDR rules (Section 3.1), rule acquisition (Section 3.2) and ?on-the-fly?
rule building indecoding (Section 3.4).3.1 Augmented HDR rulesOur augmented HDR rules aim at combining the merits of both the HDRs at handling long distance re-ordering and the fixed and floating structures at handling local reordering.
For this purpose, we augmentthe HDR rules (Xie et al., 2011) by labelling the HDRs with the fixed and floating structures.Figure 1 (b) shows an example augmented HDR rule.
Which is an augmented version of the HDRrule Figure 1 (a) by labelling it with a fixed structure (shadow with line border) and a floating structure(shadow with dash line border).
The labeled fixed and floating structures indicate the bilingual phrasesthat we can incorporate in this sentence pattern.3.2 Rule AcquisitionGiven a word-aligned parallel corpus defined as a set of triples ?T, e, A?, where T is a dependency treeof source sentence fJ1, eI1is the target sentence and A is an alignment relation between fJ1and eI1, weacquire the augmented HDR rules by three steps: tree annotation, acceptable HDR identification and ruleinduction.
The process is similar with that of Xie et al.
(2011).
However, we make some extensions sothat we can take the fixed and floating structures into account.3.2.1 Tree AnnotationBesides annotating each node of T with head span and dependency span as Xie et al.
(2011), we alsolabel the tree with consistent fixed and floating structures.Definition 3.
The head span hsp(n) of a node n is the closure of the set taking the index of the targetwords aligned to n as its elements.The closure of a set contains all the elements between the minimum and the maximum of the set andeach element has only one copy.
For example, the closure of set {1, 3} is {1, 2, 3}.We say a head span is consistent with alignment if the bilingual phrase it covers is consistent with thealignment (Koehn et al., 2003).Definition 4.
Given a subtree T?rooted at n, the dependency span dsp(n) of n is the closure of the unionof the consistent head spans of all the nodes of T?.dsp(n) = closure(?n??T?hsp(n?)
is consistenthsp(n?
))If no head spans of all the nodes of T?are consistent, dsp(n) = ?.2219!"##$%&%'$(&)'*+",,$-&-'$-&-'.
"/$0&0'$0&1'2"/,$1&1'$1&1'3445 647 849:;<8I=*",>$)&)'$)&)'?
"/,$(&('$(&('@:AA B4CD:99E7.
!?
=* *+2% 0 )1 F 1-!"#X3:.
*X3 tonightdinner$"%cookI will!
"# !$#%&'()*+()'()*Figure 2: An example annotated dependency tree (a) and an example lexicalized augmented HDR rule(b) induced from the top-level HDR of (a).
Each node of the dependency tree is annotated with twospans: head span (the former) and dependency span (the latter).
The shadows denote a consistent fixedstructure (shadow with line border) and a floating structure (shadow with dash line border).
The ?
*?denotes a substitute site.Definition 5.
A fixed or floating structure is consistent with alignment if the phrase it covers is consistentwith alignment.Tree annotation can be readily accomplished by a single post-order traversal of dependency tree T .For each accessed node n, annotate it with head span and dependency span according to A.
If n is aninternal node, enumerate all the fixed and floating structures relate to n, and label those consistent oneson T .
Repeat the above process till the root is accessed.Figure 2 (a) shows an example annotated dependency tree.
Where each node is annotated with twospans: head span (the former) and dependency span (the latter).
Moreover, the dependency tree is alsolabeled with two consistent fixed and floating structures that cover phrases ??
??
and ???
?
?
?respectively.3.2.2 Acceptable HDR IdentificationFrom the annotated dependency tree, we identify the HDRs that are suitable for rule induction.
TheseHDRs are called as acceptable HDRs.
To this end, we traverse the annotated dependency tree in post-order and identify the HDRs with the following properties:- for the head, its head span is consistent;- for the dependents, the dependency span of each dependent should not be ?
unless the dependentis a leaf node;- the intersection of the head span of the head and the dependency spans of the dependents is ?
(ordo not overlap).Different from those acceptable HDRs of Xie et al.
(2011), the acceptable HDRs here may be labeledwith fixed and floating structures.
For example, the top level of Figure 2 (a) is an acceptable HDR, whichis labeled with a fixed structure and a floating structures.
Typically an acceptable HDR has three typesof nodes: leaf node (of the dependency tree), internal node (of the dependency tree) and head node (aninternal node function as the head of the HDR).3.2.3 Rule InductionFrom each acceptable HDR, we induce a set of lexicalized and unlexicalized augmented HDR rules.
Thisprocess is similar with that of Xie et al.
(2011) except that here we have to consider the consistent fixed2220!"#X3:!
*X3 tonightdinner$"%cookI will!&'())X3:!
*X3 X2X5&*()+&,(-)cookX1 will!
"#X3:"*X3 tonightdinner$"%cookI will!&'())X3:"*X3 X2&'&*()+&,(-)cookX1 will&.(//"#X3:!
*X3 tonightdinner$"%X4I will&.(//&'())X3:!
*X3 X2X5&*()+&,(-)X4X1 will&.
(//"#X3:"*X3 tonightdinner$"%X4I will&.
(//&'())X3:"*X3 X2&'&*()+&,(-)X4X1 will!
"# !$#!%# !&#!
'# !(#!
)# !
*#+,+, +,+,+-+-UHFigure 3: Lexicalized augmented HDR rule (a) and unlexicalized augmented HDR rules (b)?
(h) inducedfrom the top level HDR of the annotated dependency tree in Figure 2.
Where ?UH?, ?UI?
and ?UL?denotes ?unlexicalize head?, ?unlexicalize internal?
and ?unlexicalize leaf?
, respectively.
The shadowswith line border denote fixed structures and the shadows with dash line border denotes floating structures.and floating structures.First, we induce a lexicalized augmented HDR rule with the following principles:1. extract the HDR, mark each internal node as a variable, and label the HDR with the floating struc-tures that cover only variables.
This forms the input of a lexicalized rule.2.
generate the target string according to head span of the head and the dependency spans of the relateddependents, and turn the word sequences covered by the dependency spans of the internal nodes intovariables.
This forms the output of a lexicalized rule.Figure 2 (b) illustrates a lexicalized augmented HDR rule induced from the top-level HDR of theannotated dependency tree Figure 2 (a).From each lexicalized augmented HDR rule (along with the acceptable HDR), we then induce a set ofunlexicalized augmented HDR rules with the following principles:1. turn each type (leaf, internal or head) of nodes simultaneously into variables;2. when turning a head or leaf node into a variable, change the counterpart of the target side into thevariable; label the unlexicalized HDR with the fixed and floating structures that cover only variables.3.
when turing an internal node into a variable, keep the counterpart of the target side unchanged.Totally, we will obtain eight types of augmented HDR rules from an acceptable HDR.
In this paper,we call the lexicalized and unlexicalized HDRs generated by the above process as instances of the HDR.Figure 3 illustrates the rule induction of seven unlexicalized augmented HDR rules (b)-(h) from lexi-calized augmented HDR rule (a).
Where ?UH?, ?UI?
and ?UL?
on the dash arrows indicate ?unlexicalizehead?, ?unlexicalize internal?
and ?unlexicalized leaf?, respectively.3.2.4 Probability EstimationWe take the augmented HDR rules acquired from word-aligned parallel corpus as the observed data, andemploy relative frequency estimation to calculate the translation probabilities of the rules.
Note that, herewe take the labeled fixed and floating structures of the augmented HDR rules as indicators of bilingualphrases that can be incorporated in the sentence patterns and phrases patterns represented by the HDRs.So we consider only the HDRs when counting the augmented HDR rules.2221X2:NT!
"# callcalled...$%  &  'him yesterday!
"#$%&'$()*+)#,-(...X45:VV_NNX3:&*X3 X2X2:NTX1:PNX45X1X4:VVX5:NNX23X5X23:NT_P*X1:PNX4X1X45:VV_NNX23X23:NT_P*X1:PNX45X1(relate to X4 X5)(relate to X2 X3)(d) (e) (f)X4:VVX5:NNX3:&*X3 X2X5X1:PNX4X1(a) (b) (c)!
{ }!X4:VVX5:NNX4:VVX5:NNX2:NT X3:&*X2:NT X3:&*Figure 4: Illustration of ?on-the-fly?
translation rule building.3.3 The modelFollowing Och and Ney (2002), we adopt a general log-linear model for our augmented dependency-to-string model.
Let d be a derivation that converts a source dependency tree T into a target string e. Theprobability of derivation d is defined as:P (d) ?
?i?i(d)?i(1)where ?iare features defined on derivation and ?iare feature weights.In our implementation, we make use of eleven features, including seven features inherited from thedependency-to-string model:- translation probabilities P (f |e) and P (e|f) and lexical translation probabilities Plex(f |e) andPlex(e|f) of augmented HDR rules- rule penalty exp(?1)- language model Plm(e)- word penalty exp(|e|), where |e| is the length of the generated target stringand four extra features for bilingual phrases relate to fixed and floating structures:- translation probabilities Pbp(f |e) and Pbp(e|f) and lexical translation probabilities Pbp lex(f |e) andPbp lex(e|f) of bilingual phrases3.4 ?On-the-Fly?
DecodingThe task of the decoder is to find the best derivation from all possible derivations.
Our decoder is basedon bottom-up chart parsing, which characterizes at ?on-the-fly?
translation rule building.Given an input dependency tree T , the decoder traverses it in post-order.
For each accessed node n,the decoder first enumerates all instances of the HDR rooted at n as we do in rule induction, and checksfor matched augmented HDR rules.
If a matched rule is labeled with fixed and floating structures, thedecoder builds new translation rules ?on the fly?
with the following principles:22221. check the phrases covered by the labeled fixed and floating structures for matched bilingual phrases;2. if there are no matched bilingual phrases for all labeled fixed and floating structures, take the aug-mented HDR rule as a HDR rule of dependency-to-string model; otherwise,- enumerate all combinations of the fixed and floating structures with matched bilingual phrases;- for each combination, build a new translation rule by turning the variable sequences coveredby the fixed and floating structures into new variables;- the new-built rule inherits the translation probabilities of the deriving augmented HDR rule,and the new variables take the matched bilingual phrases as their translation hypothesis.Figure 4 illustrates the ?on-the-fly?
rule building process.
Suppose augmented HDR rule (a) is thematched rule, and bilingual phrases (b) and (c) match the phrases covered by the labeled fixed andfloating structures of (a).
There will be three combinations of the labeled fixed and floating structuresas shown in the middle of Figure 4.
For each combination, the decoder builds a new translation rule byturning variable sequences ?X2:NT X3:?*?
and/or ?X4:VV X5:NN?
into new variables ?X23:NT P*?and/or ?X45:VV NN?.
And we will obtain three new translation rules (d)-(f) that can incorporate non-syntactic phrases into translations.If there are no matched rules, the decoder builds a pseudo translation rule with monotonic reordering.The decoder then employs cube pruning (Chiang, 2007; Huang and Chiang, 2007) to generate k-besthypothesis with integrated language model for node n.Repeat the above process till the root of T is accessed.
The hypothesis with the highest score is outputas translation.4 ExperimentsWe evaluated our augmented model by comparison with dependency-to-string model and hierarchicalphrase-based model on Chinese-to-English translation in terms of BLEU (Papineni et al., 2002).4.1 Experimental SetupThe parallel training corpus include 1.25M Chinese-English sentence pairs.1We parse the Chinesesentences with Stanford Parser (Klein and Manning, 2003) into projective dependency trees, obtain wordalignment by running GIZA++ (Och and Ney, 2003) in both directions and applying ?grow-diag-final?refinement (Koehn et al., 2003), and train a 4-gram language model by SRI Language Modeling Toolkit(Stolcke, 2002) with Kneser-Ney smoothing on the Xinhua portion of the Gigaword corpus.We take NIST MT Evaluation test set 2002 as our development set, 2003 (MT03), 2004 (MT04)and 2005 (MT05) as our test sets, evaluate the quality of translations by case insensitive NIST BLEU-4 metric2, tune the feature weights by Max-BLEU strategy with MERT (Och, 2003), and check thestatistical difference between the systems with significance test (Collins et al., 2005).4.2 SystemsWe take ?Moses-Chart?
of Moses3(Koehn et al., 2007) as hierarchical phrase-based model baseline.
Inour experiments, we use the default settings.Both the dependency-to-string baseline and our augmented model employ the same settings as thoseof Xie et al.
(2011), with the beam threshold, beam size and rule size are set to 10?3, 200 and 100respectively.
And both systems employ bilingual phrases with length ?
7 extracted by Moses.4.3 Experiment resultsTable 1 shows the results of the BLEU scores of the three systems.
Where ?dep2str?
and ?dep2str-aug?denote dependency-to-string model baseline and our augmented dependency-to-string model, respec-tively.
As we can see, ?dep2str?
shows better performance (+0.31 BLEU on average) than ?Moses-Chart?1From LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06.2ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl3http://www.statmt.org/moses/2223System Rule# MT03 MT04 MT05 AverageMoses-Chart 116.4M 34.65 36.47 34.39 35.17dep2str 37M+32.5M 34.92 36.82 34.71 35.48dep2str-aug 37M+32.5M 35.66*(+0.74) 37.61*(+0.79) 35.74*(+1.03) 36.33 (+0.85)Table 1: Statistics of the extracted rules and BLEU scores (%) on the test sets of the three systems.Where ?37M+32.5M?
denotes 37M rules and 32.5M bilingual phrases.
And ?*?
indicates dep2str-augare statistically better than dep2str with p < 0.01.Source: !
"# $ % & ' ( ) *+, -.
/0 1 23 45 67 8Reference 1: Sampaio has placed  high hopes on the Portuguese-Sinocooperation in the World Expo.Moses-Chart: Sampaio on cooperation between the two countries in theworld expo affairs Portugal and China places great .Dep2Str: President placed great  cooperation between Portugal and China ,the two countries in the World Expo affairs .Dep2Str-aug: Sampaio placed high expectations of the Portuguese - Chinesecooperation in World Expo affairs .!
"#9:;$9<%& '( )*+,-=/0 123459>>679::89<?Reference 2: Sampaio expressed his high expectations on the Sino-Portuguese cooperation in the work of the world exposition.Figure 5: Translation examples of ?Moses-Chart?, ?dep2str?
and ?dep2str-aug?.
The line border shadowdenotes the phrases successfully captured by ?dep2str-aug?.and is a strong baseline.?Dep2str-aug?
gains significant improvements of +0.74, +0.79 and +1.03 BLEUpoints over ?dep2str?
on the test sets, respectively.Additionally, we compare the actual translations generated by ?Moses-Chart?, ?dep2str?
and ?dep2str-aug?.
Figure 5 shows the translations of these three systems on a sentence of MT05.
The source sentenceholds a common sentence pattern in Chinese, which is composed of a proper noun, a verb, a noun anda prepositional phrases (corresponding to the top level of the dependency tree on the right).
However,the preposition phrase related to ???
holds nine words, thus the simple pattern becomes a long distancedependency that challenges SMT systems.
Limited by the phrase-based rules, ?Moses-Chart?
fails tocapture the sentence pattern and outputs a messy translation with little sense.
?Dep2str?, resorting toHDR rules, successfully captures the pattern and outputs a translation with correct reordering, but it isstill hard to understand.
With the help of augmented HDR rules, ?dep2str-aug?
captures both the sentencepattern and non-syntactic phrase ??????
and gives an translation with good adequacy and fluency.These results reveal the merits of our augmented dependency-to-string model at handling both longdistance reordering (with HDR) and local reordering (with fixed and floating structures), which is promis-ing for translating language pairs that are syntactically divergent.5 Conclusion and Future WorkIn this paper, we propose an augmented dependency-to-string model to address the non-syntactic phrasecoverage problem for dependency-to-string model.
To this purpose, we make two important augmen-tations to the dependency-to-string model.
First, we propose an compact representation to combineboth head-dependent relation and the fixed and floating structures into translation rules.
Second, in de-coding we build ?on the fly?
new translation rules from the compact translation rules and incorporatenon-syntactic phrases into translations.
By this way, we can combine the merits of both head-dependentsrelation at handling long distance reordering and bilingual phrases at handling local reordering.
Large-2224scale experiments show that our augmented dependency-to-string model gains significant improvementsover the dependency-to-string model.In the future work, we would like to incorporate semantic knowledge such as typed dependencies andWordNet4(Miller, 1995) so as to better direct the process of translation.AcknowledgmentsThe authors were supported by National Nature Science Foundation of China ( Contract 61370130 and61379086).
Liu was partially supported by the Science Foundation Ireland (Grant No.
07/CE/I1142)as part of the CNGL at Dublin City University.
We sincerely thank the anonymous reviewers for theircareful review and insightful suggestions.ReferencesDavid Chiang.
2007.
Hierarchical phrase-based translation.
Computational Linguistics, 33.Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005.
Clause restructuring for statistical machine transla-tion.
In Proceedings of the ACL 2005, pages 531?540, Ann Arbor, Michigan, June.Yuan Ding and Martha Palmer.
2005.
Machine translation using probabilistic synchronous dependency insertiongrammars.
In Proceedings of ACL 2005, pages 271?279.Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu.
2004.
What?s in a translation rule?
In HLT-NAACL 2004: Main Proceedings, pages 273?280, Boston, Massachusetts, USA, May 2 - May 7.Liang Huang and David Chiang.
2007.
Forest rescoring: Faster decoding with integrated language models.
InProceedings of ACL 2007, pages 144?151, Prague, Czech Republic, June.Liang Huang, Kevin Knight, and Aravind Joshi.
2006.
Statistical syntax-directed translation with extended domainof locality.
In Proceedings of AMTA, pages 66?73.Dan Klein and Christopher D. Manning.
2003.
Fast exact inference with a factored model for natural languageparsing.
In In Advances in Neural Information Processing Systems 15 (NIPS), pages 3?10.
MIT Press.Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.
Statistical phrase-based translation.
In Proceedingsof the 2003 Human Language Technology Conference of the North American Chapter of the Association forComputational Linguistics, Edmonton, Canada, July.Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, BrookeCowan, Wade Shen, Christine Moran, Richard Zens, et al.
2007.
Moses: Open source toolkit for statisticalmachine translation.
In Proceedings of ACL 2005: Interactive Poster and Demonstration Sessions, pages 177?180.Dekang Lin.
2004.
A path-based transfer model for machine translation.
In Proceedings of Coling 2004, pages625?630, Geneva, Switzerland, Aug 23?Aug 27.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machine translation.In Proceedings of ACL 2006, pages 609?616, Sydney, Australia, July.Fandong Meng, Jun Xie, Linfeng Song, Yajuan L?u, and Qun Liu.
2013.
Translation with source constituency anddependency trees.
In Proceedings of EMNLP 2013, pages 1066?1076, Seattle, Washington, USA, October.George A Miller.
1995.
Wordnet: a lexical database for english.
Communications of the ACM, 38(11):39?41.Franz Josef Och and Hermann Ney.
2002.
Discriminative training and maximum entropy models for statisticalmachine translation.
In Proceedings of ACL 2002, pages 295?302, Philadelphia, Pennsylvania, USA, July.Franz Josef Och and Hermann Ney.
2003.
A systematic comparison of various statistical alignment models.Computational Linguistics, 29(1):19?51.Franz Josef Och.
2003.
Minimum error rate training in statistical machine translation.
In Proceedings of ACL-2003, pages 160?167, Sapporo, Japan, July.4http://wordnet.princeton.edu2225Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic evaluationof machine translation.
In Proceedings of ACL 2002, pages 311?318, Philadelphia, Pennsylvania, USA, July.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.
Dependency treelet translation: Syntactically informedphrasal smt.
In Proceedings of ACL 2005, pages 271?279.Libin Shen, Jinxi Xu, and Ralph Weischedel.
2008.
A new string-to-dependency machine translation algorithmwith a target dependency language model.
In Proceedings of ACL 2008: HLT, pages 577?585, Columbus, Ohio,June.Andreas Stolcke.
2002.
Srilm - an extensible language modeling toolkit.
In Proceedings of ICSLP, volume 30,pages 901?904.Jun Xie, Haitao Mi, and Qun Liu.
2011.
A novel dependency-to-string model for statistical machine translation.In Proceedings of EMNLP 2011, pages 216?226, Edinburgh, Scotland, UK., July.2226
