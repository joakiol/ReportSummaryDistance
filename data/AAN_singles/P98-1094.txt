A concurrent approach to the automatic extraction ofsubsegmental primes and phonological constituents from speechMichael INGLEBYSchool of Computing and Mathematics,University of Huddersfield, Queensgate,Huddersfield HD1 3DH, UKM.Ingleby@hud.ac.ukAbstractWe demonstrate the feasibility of using unary primesin speech-driven language processing.
Proponents ofGovernment Phonology (one of several phonologicalframeworks in which speech segments arerepresented as combinations of relatively fewsubsegmental primes) claim that primes areacoustically realisable.
This claim is examinedcritically searching out signatures for primes in multi-speaker speech signal data.
In response to a widevariation in the ease of detection of primes, it isproposed that the computational approach tophonology-based, speech-driven software should beorganised in stages.
After each stage, computationalprocesses like segmentation a d lexical access can belaunched to run concurrently with later stages ofprime detection.Introduction and overviewIn ?
1, the subsegmental primes and phonologicalconstituents u ed in Government Phonology (GP) aredescribed, and the acoustic realisability claims whichmake GP primes seem particularly attractive todevelopers of speech-driven software aresummarised.
We then outline an approach to definingidentification signatures for primes (?
2).
Ourapproach is based on cluster analysis using a set ofacoustic cues chosen to reflect familiar events inspectrograms: plosion, frication, excitation,resonance... We note that cues indicating manner ofarticulation, which change abruptly at segmentboundaries, are computationaUy simple, while thosefor voicing state and resonance quality are complexand calculable only after signal segmentation.
Also,Wiebke BROCKHAUSDepartment of German,University of Manchester,Oxford Rd, Manchester M13 9PL,UKWiebke.Brockhaus@man.ac.ukthe regions of cue space where the primes cluster (andwhich serve as their signatures) are disconnected,with separate sub-regions corresponding to theoccurrence of a prime in nuclear or non-nuclearsegmental positions.A further complication is that GP primes combineasymmetrically in segments: one prime - the HEAD -of the combination being more dominant, while theother element(s) - the OPERATORS(S)  - tend to berecessive.
This is handled by establishing incue spacea central location and within-cluster variance for eachprime.
The training sample needed for this consists ofsegments in which the prime suffers modificationonly by minimal combination with others, i.e on itsown, or with as few other primes as possible.
Then,when a segment containing the prime in less thanminimal combination is presented for identification,its location in cue space lies within a restrictednumber of units of within-cluster variance of thecentral location of the prime cluster.
The number ofsuch distance units determines headedness in thesegment, with separate thresholds for occurrence ashead and as operator.In ?
3 we describe in more detail the stagewiseprocedure for identifying via quadratic discriminantsthe primes present in segments.
At each stage, wedetail the computational processes which are drivenby the partial identification achieved by theend of thestage.
The processes include segmentation, selectionof lexical cohort by manner class, detection ofconstituent structure, detection and repair of theeffects of phonological processes on the speechsignal.
The prototype, speaker-independent, isolated-word automatic speech recognition (ASR) system isdescribed in ?
4.
Called 'PhonMaster', it is578implemented in C++ using objects which performseparate stages of lexical access and process repairconcurrently.1 Phonological primes and constituentsMuch of the phonological research work of the pasttwenty years has focussed on phonologicalrepresentations: on the make-up of individualsegments and on the prosodic hierarchy bindingskeletal positions together.Some researchers (e.g.
Anderson and Ewen 1987and Kaye et al 1985) have proposed a small set ofsubsegmental primes which may occur in isolationbut can also he compounded to model the manyphonologically significant sounds of the world'slanguages.
To give an example, in one version of GP(see Brockhaus et al 1996), nine primes or ELEMENTSare recognised, viz.
the .manner elements h (noise)and ?
(occlusion), the source elements H(voicelessness), L (non-spontaneous voicing) and N(nasality), and the resonance lements A (low), I(palatal), U (labial) and R (coronal).
These elementsare phonologically active - they can spread toneighbouring segments, be lenited etc..The skeletal positions to which elements may beattached (alone or in combination) enter intoasymmetric binary relations with each other, so-calledGOVERNING relations.
A CONSTITUENT is defined asan ordered pair, governor first on the left andgovernee second on the right.
Words are composed ofwell-formed sequences of constituents.
Whichskeletal positions may enter into governing relationswith each other is mainly determined bythe elementswhich occupy a particular skeletal slot, so elementalmake-up is an important factor in the construction ofphonological constituents.GP proponents have claimed that elements, whichwere originally described in articulatory terms, haveaudible acoustic identities.
As we shall see in ?
2, it ispossible to define the acoustic signatures of individualelements, o that the presence of an element can bedetected by analysis of the speech signal.Picking out elements from the signal is muchmore straightforward than identifying phonemes.Firstly, elements are subject to less variation due thecontextual effects (e.g.
place assimilation) ofpreceding and following segments than phonemes.Secondly, elements are much smaller in number thanphonemes (nine elements compared to c. 44phonemes in English) and, thirdly, elements, unlikephonemes, have been shown to participate inthe kindof phonological processes which lead to variation inpronunciation (see references in Harris 1994).Fourthly, although there is much variation ofphoneme inventory from language to language, theelement inventory is universal.These four characteristics of its elements, plus theavailability of reliable element detection, make aphonological framework such as GP a highlyattractive basis for multi-speaker speech-drivensoftware.
This includes not only traditional ASRapplications (e.g.
dictation, database access), but alsoembraces multilingual speech input, medical (speechtherapy) and teaching (computer-assisted languagelearning) applications.2 Signatures of GP elementsTable 1 below details the acoustic cues used inPhonMaster.
Using training data from five speakers,male and female, synthetic and real with differentregional accents, these cues discriminate between thesimplest speech segments containing an element in aminimal combination with others.
In the case of aresonance lement, say, U, the minimal state ofcombination corresponds to isolated occurrence inavowel such as \[U\], as in RP English hood or GermanBus.The accuracy of cues such as those in Table 1 fordiscrimination f simplest speech segments has beentested by different researchers u ing ratios of within-class to between-class variance-covariance anddendrograms (Brockhaus et al 1996, Williams 1997),as described in PhonMaster's documentation.The cues are calculated from fast Fouriertransforms (FFTs) of speech signals in terms of totalamplitude or energy distribution ED across low,middle and high frequency parts of the vocal rangeand the angular frequencies to(F) and amplitudes a(F)of formants.
The first four cues dp, to {h areproperties of a single spectral slice, and the change inthese four from slice to slice is logged as t} 5, whichpeaks at segment boundaries.
The duration cue #p6 issegment-based, computable only after segmentationfrom the length in slices from boundary to boundary,579normalising this length using the JSRU database ofthe relative durations of segments in different mannerclasses (see Chalfont 97).
The normalisation is asimple form of time-warping without thecomputational complexity of dynamic time-warpingor Hidden Markov Models (HMMs).Cue Label Definitiondpl Energy qbl = EDIo / ED~ratio~dp 2 Energy qb 2 -= EDmi d / ED~ratio 2dp 3 Width (~3 = (to(F2) - (o(F l)) /(to(F3) - to(F2))~4 Fall dP4 - a(F1) /(a(F3)+a(F2))dP5 Change If6qb = (I)next.sliee -- ~)current-slice,- + + 6q% +8 4dP6 Duration l~6 operates with reference!to a durations databasedp7 F1 \]q b7 = o(F 1)bo~.d/~o(F 1),t,,dyTrajectoryqbs ' I fA~ = dPsteady - ~bound,~bs = (Aco(F3) +Aco(F2))/FormantTransitionTable 1.
Cues used to define signaturesThe other segment-based cues contrast steady-state formant values at the centre of a segment withvalues at entrance and exit boundary.
They describethe context of a segment without going to thecomputational complexity of triphone HMMs (e.g.Young 1996).
The PhonMaster approach is not tiedto a particular set of cues, so long as the members ofthe set are concerned with ratios which vary muchless from speaker to speaker than absolutefrequencies and intensities.
Nor is the approachbound to FFTs - linear predictive coding wouldextract energy density and formants just as well.Signatures are defined from cues by locating incue space cluster centres and defining a quadraticdiscriminant based on the variance-covariancematrix of the cluster.
When elements occur in higherdegrees of combination than those selected for thetraining sample, separate detection thresholds fordistance from cluster centre are set for occurrence ashead and occurrence as operator.3 Stagewise lement recognitionThe detection of dements in the signal proceeds inthree stages, with concurrent processes (lexicalaccess, phonological process repair...) beinglaunched after each stage and before the full identityof a segment has been established.The overall architecture of the recognition task isshown in Figure 1.
At Stage 1, the recogniser checksfor the presence of the manner elements h and ?.1.
Maalte?2.
Pbentt le l tFigure 1.
Stagewise cue invocation strategyThis launches the calculation of cues 4)5 (for theautomatic segmentation process) and 4)6 (todistinguish vowels from approximants, and todetermine vowel length).
The ensuing manner classassignment process produces the classes:Occ Occlusion (i.e.
?
present as head, as inplosives and affricates)Sfr Strong fricative (i.e.
h present as head, asin \[s\], \[z\], IS\] and \[Z\])Wfr Weak fricative (i.e.
h present as operator,as in plosives and non-sibilant fricatives)580PloNasAppSvoLVoVowPlosion (as for Wfr, but interpreted asplosion when directly following Occ-except word-initially)Nasal (i.e.
?
present as operator)ApproximantShort vowelLong vowel or diphthongVowel (not readily identifiable as beingeither long or short).the words can be identified uniquely by manner classalone.
This is the case for languages such as English,German, French and Italian, so the accessing of anindividual word may be successful as early as Stage1, and no further data processing need be carried out.If, however, as in Figure 3, the manner-classsequence identified is a common one, shared byseveral words, then the recognition process movesFigure 2.
Representation of potential after Stage 1As soon as such a sequence of manner classesbecomes available, repair processes and lexicalsearches can be launched concurrently.
The repairobject refers to the constituent structure which canbe built on the basis of manner-class informationalone and checks its conformance to the universalprinciples of grammar in GP as well as to language-specific constraints.
In cases of conflict with either,a new structure is created to resolve the conflictFor example, the word potential is often realisedwithout a vowel between the first two consonants.This elided vowel would be restored automaticallyby the repair object, as illustra'~d in Figure 2, wherea nuclear position (N) has been inserted between thetwo onset (O) positions occupied by the plosives.Constituent s ructure is less specific than mannerclasses (in certain cases, different manner-classsequences are assigned the same constituentstructure), so manner classes form the key for lexicalaccess at Stage 1.
Zue (1985) reports that, even in alarge lexicon of c. 20, 000 words, around a third ofFigure 3.
Lexical search screen for a common mannerclass sequence (Stage 1)on to Stage 2, where the phonatory properties of thesegments identified at Stage 1 are determined.Continuing with the example in Figure 3, thelexical access object would now discard words suchas seed or shade, as neither of them contains theelement H (voicelessness in obstruents), whosepresence has been detected in both the initialfricative and the final plosive at Stage 2.
Again, itmay be possible to identify aunique word candidateat the end of Stage 2, but if several candidates areavailable, recognition moves on to Stage 3.Here, the focus is on the four resonanceelements.
As the manifestations of U, R, I and Avary between voiced vs. voiceless obstruents vs.sonorants, appropriate cues are invoked for each ofthese three broad classes (some of the cues reusinginformation gathered at Stage 1).
The detection ofcertain resonance lements then provides all thenecessary information for a final lexical search.
Inour example, only one word, seep, contains all theelements detected at Stages 1 to 3, as illustrated in581Figure 4.
Only in cases of homophony will morethan one word be accessed at Stage 3.Figure 4.
Lexical search screen for a common mannerclass sequence (Stage 3)Concurrently with this lexical search, repairprocesses check for the effects of assimilation,allowing for adjacent segments (especially inclusters involving nasals and plosives)to share oneor more resonance elements, thus resolving possibleaccess problems arising from words such as input/'InpUt/being realised as ['IrnpUt].4 PhonMaster  and its successorsThe PhonMaster prototype was implemented in C++by a PhD student educated inobject-oriented designand Windows application programming.
It usesstandard object-class libraries for screenmanagement, s andard relational database tools forcontrol of the lexicon and standard code for FFT asin a spectrogram display object.
Users may addwords using a keypad labelled with IPA symbols.Manner class sequences and constituent s ructure aregenerated automatically.
The objects concerned wilhthe extraction of cues from spectra, segmentation,manner-class equencing and display of constituentstructure, repairing effects of lenition andassimilation are custom built.PhonMaster does not use corpus trigram statistics(e.g.
Young 1996) to disambiguate word lattices, andthere is no speaker-adaptation.
Without thesestandard ways of enhancing pure pattern-recognitionaccuracy, its success rate for pure word recognitionis around 75%.
We are contemplating the addition d"pitch cues, which, with duration, would allowdetection of stress, which may further increaseaccuracy.Object orientation makes the task ofincorporating currently popular pattern recognitionmethods fairly straightforward.
HMMs whosehidden states have cues like ours as observables areobvious things to try.
Artificial Neural Nets (ANNs)also fit into the task architecture in various places.Vector quantisation ANNs could be used to learn thebest choice of thresholds for head-operator detectionand discrimination.
ANNs with output nodes basedon our quadratic discriminants in place of the morecommon linear discriminants are also an option, andtheir output node strengths would be direct measuresof presence of elements.ReferencesAnderson J.M.
and Ewen C.J.
(1987) Principles ofDependency Phonology.
Cambridge UniversityPress, Cambridge, England, 312 pp.Brockhaus W.G., Ingleby M. and Chalfont C.R.
(1996) Acoustic signatures of phonologicalprimes.
Internal report.
Universities ofManchester and Huddersfield, England.Chalfont C.R.
(1997) University of HuddersfieldPhD Dissertation 'Automatic Speech Recogni-tion: a Government Phonology perspective'Harris J.
(1994) English Sound Structure.
Blackwell,Oxford, England.Kaye J.D., Lowenstamm J. and Vergnaud J.-R.(1985) The internal structure of phonologicalelements: a theory of charm and government.Phonology Yearbook, 2, pp.
305-328.Williams G. (1997)A pattern recognition model f rthe phonetic interpretation of elements.
SOASWorking Papers in Linguistics and Phonetics, 7,pp.
275-297.Young S. (1997) A Review of Large-vocabularyContinuous-speech Recognition.
IEEE SignalProcessing Magazine, S ptember Issue.Zue V.W.
(1985) The use of speech knowledge inAutomatic Speech Recognition, Proc.
ICASSP,73/11, pp.
1602-1615.582
