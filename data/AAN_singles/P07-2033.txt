Proceedings of the ACL 2007 Demo and Poster Sessions, pages 129?132,Prague, June 2007. c?2007 Association for Computational LinguisticsWordNet-based Semantic Relatedness Measures in Automatic SpeechRecognition for MeetingsMichael PucherTelecommunications Research Center ViennaVienna, AustriaSpeech and Signal Processing Lab, TU GrazGraz, Austriapucher@ftw.atAbstractThis paper presents the application ofWordNet-based semantic relatedness mea-sures to Automatic Speech Recognition(ASR) in multi-party meetings.
Differ-ent word-utterance context relatedness mea-sures and utterance-coherence measures aredefined and applied to the rescoring of N -best lists.
No significant improvementsin terms of Word-Error-Rate (WER) areachieved compared to a large word-based n-gram baseline model.
We discuss our resultsand the relation to other work that achievedan improvement with such models for sim-pler tasks.1 IntroductionAs (Pucher, 2005) has shown different WordNet-based measures and contexts are best for word pre-diction in conversational speech.
The JCN (Sec-tion 2.1) measure performs best for nouns using thenoun-context.
The LESK (Section 2.1) measure per-forms best for verbs and adjectives using a mixedword-context.Text-based semantic relatedness measures canimprove word prediction on simulated speech recog-nition hypotheses as (Demetriou et al, 2000) haveshown.
(Demetriou et al, 2000) generated N -bestlists from phoneme confusion data acquired froma speech recognizer, and a pronunciation lexicon.Then sentence hypotheses of varying Word-Error-Rate (WER) were generated based on sentencesfrom different genres from the British National Cor-pus (BNC).
It was shown by them that the semanticmodel can improve recognition, where the amountof improvement varies with context length and sen-tence length.
Thereby it was shown that these mod-els can make use of long-term information.In this paper the best performing measuresfrom (Pucher, 2005), which outperform baselinemodels on word prediction for conversational tele-phone speech are used for Automatic Speech Recog-nition (ASR) in multi-party meetings.
Thereby wewant to investigate if WordNet-based models can beused for rescoring of ?real?
N -best lists in a difficulttask.1.1 Word prediction by semantic similarityThe standard n-gram approach in language mod-eling for speech recognition cannot cope withlong-term dependencies.
Therefore (Bellegarda,2000) proposed combining n-gram language mod-els, which are effective for predicting local de-pendencies, with Latent Semantic Analysis (LSA)based models for covering long-term dependencies.WordNet-based semantic relatedness measures canbe used for word prediction using long-term depen-dencies, as in this example from the CallHome En-glish telephone speech corpus:(1) B: I I well, you should see what thebstudentscB: after they torture them for six byearsc inmiddle bschoolc and high bschoolc theydon?t want to do anything in bcollegecparticular.In Example 1 college can be predicted from thenoun context using semantic relatedness measures,129here between students and college.
A 3-gram modelgives a ranking of college in the context of anythingin.
An 8-gram predicts college from they don?t wantto do anything in, but the strongest predictor is stu-dents.1.2 Test dataThe JCN and LESK measure that are defined in thenext section are used for N -best list rescoring.
Forthe WER experiments N -best lists generated fromthe decoding of conference room meeting test dataof the NIST Rich Transcription 2005 Spring (RT-05S) meeting evaluation (Fiscus et al, 2005) areused.
The 4-gram that has to be improved by theWordNet-based models is trained on various corporafrom conversational telephone speech to web datathat together contain approximately 1 billion words.2 WordNet-based semantic relatednessmeasures2.1 Basic measuresTwo similarity/distance measures from the Perlpackage WordNet-Similarity written by (Pedersen etal., 2004) are used.
The measures are named af-ter their respective authors.
All measures are im-plemented as similarity measures.
JCN (Jiang andConrath, 1997) is based on the information content,and LESK (Banerjee and Pedersen, 2003) allowsfor comparison across Part-of-Speech (POS) bound-aries.2.2 Word context relatednessFirst the relatedness between words is defined basedon the relatedness between senses.
S(w) are thesenses of word w. Definition 2 also performs word-sense disambiguation.rel(w,w?)
= maxci?S(w) cj?S(w?
)rel(ci, cj) (2)The relatedness of a word and a context (relW) isdefined as the average of the relatedness of the wordand all words in the context.relW(w,C) =1| C |?wi?Crel(w,wi) (3)2.3 Word utterance (context) relatednessThe performance of the word-context relatedness(Definition 3) shows how well the measures workfor algorithms that proceed in a left-to-right manner,since the context is restricted to words that have al-ready been seen.
For the rescoring of N -best listsit is not necessary to proceed in a left-to-right man-ner.
The word-utterance-context relatedness can beused for the rescoring of N -best lists.
This related-ness does not only use the context of the precedingwords, but the whole utterance.Suppose U = ?w1, .
.
.
, wn?
is an utterance.
Letpre(wi, U) be the set?j<i wj and post(wi, U) bethe set?j>i wj .
Then the word-utterance-contextrelatedness is defined asrelU1(wi, U, C) =relW(wi,pre(wi, U) ?
post(wi, U) ?
C) .
(4)In this case there are two types of context.
Thefirst context comes from the respective meeting, andthe second context comes from the actual utterance.Another definition is obtained if the context C iseliminated (C = ?)
and just the utterance context Uis taken into account.relU2(wi, U) =relW(wi,pre(wi, U) ?
post(wi, U)) (5)Both definitions can be modified for usage withrescoring in a left-to-right manner by restricting thecontexts only to the preceding words.relU3(wi, U, C) = relW(wi,pre(wi, U) ?
C) (6)relU4(wi, U) = relW(wi,pre(wi, U)) (7)2.4 Defining utterance coherenceUsing Definitions 4-7 different concepts of utterancecoherence can be defined.
For rescoring the utter-ance coherence is used, when a score for each el-ement of an N -best list is needed.
U is again anutterance U = ?w1, .
.
.
, wn?.130cohU1(U,C) =1| U |?w?UrelU1(w,U,C) (8)The first semantic utterance coherence measure(Definition 8) is based on all words in the utteranceas well as in the context.
It takes the mean of therelatedness of all words.
It is based on the word-utterance-context relatedness (Definition 4).cohU2(U) =1| U |?w?UrelU2(w,U) (9)The second coherence measure (Definition 9) isa pure inner-utterance-coherence, which means thatno history apart from the utterance is needed.
Sucha measure is very useful for rescoring, since the his-tory is often not known or because there are speechrecognition errors in the history.
It is based on Defi-nition 5.cohU3(U,C) =1| U |?w?UrelU3(w,U,C) (10)The third (Definition 10) and fourth (Defini-tion 11) definition are based on Definition 6 and 7,that do not take future words into account.cohU4(U) =1| U |?w?UrelU4(w,U) (11)3 Word-error-rate (WER) experimentsFor the rescoring experiments the first-best elementof the previous N -best list is added to the context.Before applying the WordNet-based measures, theN -best lists are POS tagged with a decision treetagger (Schmid, 1994).
The WordNet measures arethen applied to verbs, nouns and adjectives.
Thenthe similarity values are used as scores, which haveto be combined with the language model scores ofthe N -best list elements.The JCN measure is used for computing a nounscore based on the noun context, and the LESK mea-sure is used for computing a verb/adjective scorebased on the noun/verb/adjective context.
In the endthere is a lesk score and a jcn score for each N -bestlist.
The final WordNet score is the sum of the twoscores.The log-linear interpolation method used for therescoring is defined asp(S) ?
pwordnet(S)?
pn-gram(S)1??
(12)where ?
denotes normalization.
Based on all Word-Net scores of an N -best list a probability is esti-mated, which is then interpolated with the n-grammodel probability.
If only the elements in an N -best list are considered, log-linear interpolation canbe used since it is not necessary to normalize overall sentences.
Then there is only one parameter ?
tooptimize, which is done with a brute force approach.For this optimization a small part of the test data istaken and the WER is computed for different valuesof ?.As a baseline the n-gram mixture model trainedon all available training data (?
1 billion words) isused.
It is log-linearly interpolated with the Word-Net probabilities.
Additionally to this sophisticatedinterpolation, solely the WordNet scores are usedwithout the n-gram scores.3.1 WER experiments for inner-utterancecoherenceIn this first group of experiments Definitions 8 and 9are applied to the rescoring task.
Similarity scoresfor each element in an N -best list are derived ac-cording to the definitions.
The first-best element ofthe last list is always added to the context.
The con-text size is constrained to the last 20 words.
Def-inition 8 includes context apart from the utterancecontext, Definition 9 only uses the utterance context.No improvement over the n-gram baseline isachieved for these two measures.
Neither with thelog-linearly interpolated models nor with the Word-Net scores alone.
The differences between the meth-ods in terms of WER are not significant.3.2 WER experiments for utterance coherenceIn the second group of experiments Definitions 10and 11 are applied to the rescoring task.
There isagain one measure that uses dialog context (10) andone that only uses utterance context (11).Also for these experiments no improvement overthe n-gram baseline is achieved.
Neither with the131log-linearly interpolated models nor with the Word-Net scores alone.
The differences between the meth-ods in terms of WER are also not significant.
Thereare also no significant differences in performancebetween the second group and the first group of ex-periments.4 Summary and discussionWe showed how to define more and more complexrelatedness measures on top of the basic relatednessmeasures between word senses.The LESK and JCN measures were used for therescoring of N -best lists.
It was shown that speechrecognition of multi-party meetings cannot be im-proved compared to a 4-gram baseline model, whenusing WordNet models.One reason for the poor performance of the mod-els could be that the task of rescoring simulated N -best lists, as presented in (Demetriou et al, 2000), issignificantly easier than the rescoring of ?real?
N -best lists.
(Pucher, 2005) has shown that Word-Net models can outperform simple random mod-els on the task of word prediction, in spite of thenoise that is introduced through word-sense disam-biguation and POS tagging.
To improve the word-sense disambiguation one could use the approachproposed by (Basili et al, 2004).In the above WER experiments a 4-gram baselinemodel was used, which was trained on nearly 1 bil-lion words.
In (Demetriou et al, 2000) a simplerbaseline has been used.
650 sentences were usedthere to generate sentence hypotheses with differentWER using phoneme confusion data and a pronun-ciation lexicon.
Experiments with simpler baselinemodels ignore that these simpler models are not usedin today?s recognition systems.We think that these prediction models can still beuseful for other tasks where only small amounts oftraining data are available.
Another possibility ofimprovement is to use other interpolation techniqueslike the maximum entropy framework.
WordNet-based models could also be improved by using atrigger-based approach.
This could be done by notusing the whole WordNet and its similarities, butdefining word-trigger pairs that are used for rescor-ing.5 AcknowledgementsThis work was supported by the European Union 6thFP IST Integrated Project AMI (Augmented Multi-party Interaction, and by Kapsch Carrier-Com AGand Mobilkom Austria AG together with the Aus-trian competence centre programme Kplus.ReferencesSatanjeev Banerjee and Ted Pedersen.
2003.
Extendedgloss overlaps as a measure of semantic relatedness.In Proceedings of the 18th Int.
Joint Conf.
on ArtificialIntelligence, pages 805?810, Acapulco.Roberto Basili, Marco Cammisa, and Fabio MassimoZanzotto.
2004.
A semantic similarity measure forunsupervised semantic tagging.
In Proc.
of the FourthInternational Conference on Language Resources andEvaluation (LREC2004), Lisbon, Portugal.Jerome Bellegarda.
2000.
Large vocabulary speechrecognition with multispan statistical language mod-els.
IEEE Transactions on Speech and Audio Process-ing, 8(1), January.G.
Demetriou, E. Atwell, and C. Souter.
2000.
Usinglexical semantic knowledge from machine readabledictionaries for domain independent language mod-elling.
In Proc.
of LREC 2000, 2nd International Con-ference on Language Resources and Evaluation.Jonathan G. Fiscus, Nicolas Radde, John S. Garofolo,Audrey Le, Jerome Ajot, and Christophe Laprun.2005.
The rich transcription 2005 spring meetingrecognition evaluation.
In Rich Transcription 2005Spring Meeting Recognition Evaluation Workshop,Edinburgh, UK.Jay J. Jiang and David W. Conrath.
1997.
Semantic sim-ilarity based on corpus statistics and lexical taxonomy.In Proceedings of the International Conference on Re-search in Computational Linguistics, Taiwan.Ted Pedersen, S. Patwardhan, and J. Michelizzi.
2004.WordNet::Similarity - Measuring the relatedness ofconcepts.
In Proc.
of Fifth Annual Meeting of theNorth American Chapter of the ACL (NAACL-04),Boston, MA.Michael Pucher.
2005.
Performance evaluation ofWordNet-based semantic relatedness measures forword prediction in conversational speech.
In IWCS6, Sixth International Workshop on Computational Se-mantics, Tilburg, Netherlands.H Schmid.
1994.
Probabilistic part-of-speech taggingusing decision trees.
In Proceedings of InternationalConference on New Methods in Language Processing,Manchester, UK, September.132
