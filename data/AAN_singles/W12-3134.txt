Proceedings of the 7th Workshop on Statistical Machine Translation, pages 283?291,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsJoshua 4.0: Packing, PRO, and ParaphrasesJuri Ganitkevitch1, Yuan Cao1, Jonathan Weese1, Matt Post2, and Chris Callison-Burch11Center for Language and Speech Processing2Human Language Technology Center of ExcellenceJohns Hopkins UniversityAbstractWe present Joshua 4.0, the newest versionof our open-source decoder for parsing-basedstatistical machine translation.
The main con-tributions in this release are the introductionof a compact grammar representation basedon packed tries, and the integration of ourimplementation of pairwise ranking optimiza-tion, J-PRO.
We further present the exten-sion of the Thrax SCFG grammar extractorto pivot-based extraction of syntactically in-formed sentential paraphrases.1 IntroductionJoshua is an open-source toolkit1 for parsing-basedstatistical machine translation of human languages.The original version of Joshua (Li et al, 2009) wasa reimplementation of the Python-based Hiero ma-chine translation system (Chiang, 2007).
It was laterextended to support grammars with rich syntacticlabels (Li et al, 2010a).
More recent efforts in-troduced the Thrax module, an extensible Hadoop-based extraction toolkit for synchronous context-free grammars (Weese et al, 2011).In this paper we describe a set of recent exten-sions to the Joshua system.
We present a new com-pact grammar representation format that leveragessparse features, quantization, and data redundanciesto store grammars in a dense binary format.
This al-lows for both near-instantaneous start-up times anddecoding with extremely large grammars.
In Sec-tion 2 we outline our packed grammar format and1joshua-decoder.orgpresent experimental results regarding its impact ondecoding speed, memory use and translation quality.Additionally, we present Joshua?s implementationof the pairwise ranking optimization (Hopkins andMay, 2011) approach to translation model tuning.J-PRO, like Z-MERT, makes it easy to implementnew metrics and comes with both a built-in percep-tron classifier and out-of-the-box support for widelyused binary classifiers such as MegaM and Max-Ent (Daume?
III and Marcu, 2006; Manning andKlein, 2003).
We describe our implementation inSection 3, presenting experimental results on perfor-mance, classifier convergence, and tuning speed.Finally, we introduce the inclusion of bilingualpivoting-based paraphrase extraction into Thrax,Joshua?s grammar extractor.
Thrax?s paraphrase ex-traction mode is simple to use, and yields state-of-the-art syntactically informed sentential paraphrases(Ganitkevitch et al, 2011).
The full feature set ofThrax (Weese et al, 2011) is supported for para-phrase grammars.
An easily configured feature-levelpruning mechanism allows to keep the paraphrasegrammar size manageable.
Section 4 presents de-tails on our paraphrase extraction module.2 Compact Grammar RepresentationStatistical machine translation systems tend to per-form better when trained on larger amounts of bilin-gual parallel data.
Using tools such as Thrax, trans-lation models and their parameters are extractedand estimated from the data.
In Joshua, translationmodels are represented as synchronous context-freegrammars (SCFGs).
An SCFG is a collection of283rules {ri} that take the form:ri = Ci ?
?
?i, ?i,?i, ~?i?, (1)where left-hand side Ci is a nonterminal symbol, thesource side ?i and the target side ?i are sequencesof both nonterminal and terminal symbols.
Further,?i is a one-to-one correspondence between the non-terminal symbols of ?i and ?i, and ~?i is a vector offeatures quantifying the probability of ?i translat-ing to ?i, as well as other characteristics of the rule(Weese et al, 2011).
At decoding time, Joshua loadsthe grammar rules into memory in their entirety, andstores them in a trie data structure indexed by therules?
source side.
This allows the decoder to effi-ciently look up rules that are applicable to a particu-lar span of the (partially translated) input.As the size of the training corpus grows, so doesthe resulting translation grammar.
Using more di-verse sets of nonterminal labels ?
which can signifi-cantly improve translation performance ?
further ag-gravates this problem.
As a consequence, the spacerequirements for storing the grammar in memoryduring decoding quickly grow impractical.
In somecases grammars may become too large to fit into thememory on a single machine.As an alternative to the commonly used trie struc-tures based on hash maps, we propose a packed trierepresentation for SCFGs.
The approach we take issimilar to work on efficiently storing large phrasetables by Zens and Ney (2007) and language mod-els by Heafield (2011) and Pauls and Klein (2011) ?both language model implementations are now inte-grated with Joshua.2.1 Packed Synchronous TriesFor our grammar representation, we break the SCFGup into three distinct structures.
As Figure 1 in-dicates, we store the grammar rules?
source sides{?i}, target sides {?i}, and feature data {~?i} in sep-arate formats of their own.
Each of the structuresis packed into a flat array, and can thus be quicklyread into memory.
All terminal and nonterminalsymbols in the grammar are mapped to integer sym-bol id?s using a globally accessible vocabulary map.We will now describe the implementation details foreach representation and their interactions in turn.2.1.1 Source-Side TrieThe source-side trie (or source trie) is designedto facilitate efficient lookup of grammar rules bysource side, and to allow us to completely specify amatching set of rule with a single integer index intothe trie.
We store the source sides {?i} of a grammarin a downward-linking trie, i.e.
each trie node main-tains a record of its children.
The trie is packed intoan array of 32-bit integers.
Figure 1 illustrates thecomposition of a node in the source-side trie.
Allinformation regarding the node is stored in a con-tiguous block of integers, and decomposes into twoparts: a linking block and a rule block.The linking block stores the links to the child trienodes.
It consists of an integer n, the number of chil-dren, and n blocks of two integers each, containingthe symbol id aj leading to the child and the childnode?s address sj (as an index into the source-sidearray).
The children in the link block are sorted bysymbol id, allowing for a lookup via binary or inter-polation search.The rule block stores all information necessary toreconstruct the rules that share the source side thatled to the current source trie node.
It stores the num-ber of rules, m, and then a tuple of three integersfor each of the m rules: we store the symbol id ofthe left-hand side, an index into the target-side trieand a data block id.
The rules in the data block areinitially in an arbitrary order, but are sorted by ap-plication cost upon loading.2.1.2 Target-Side TrieThe target-side trie (or target trie) is designed toenable us to uniquely identify a target side ?i with asingle pointer into the trie, as well as to exploit re-dundancies in the target side string.
Like the sourcetrie, it is stored as an array of integers.
However,the target trie is a reversed, or upward-linking trie:a trie node retains a link to its parent, as well as thesymbol id labeling said link.As illustrated in Figure 1, the target trie is ac-cessed by reading an array index from the sourcetrie, pointing to a trie node at depth d. We then fol-low the parent links to the trie root, accumulatingtarget side symbols gj into a target side string gd1 aswe go along.
In order to match this traversal, the tar-get strings are entered into the trie in reverse order,i.e.
last word first.
In order to determine d from a284# children# ruleschild symbolchild addressrule left-hand sidetarget addressdata block idn ?m ?ajsj+1Cjtjbj......nm......parent symbolparent addressgjtj-1......# featuresfeature idfeature valuen ?fjvj...n.....Feature blockindexFeature bytebufferTarget triearraySource triearrayfj......Quantizationbj......fjqjFigure 1: An illustration of our packed grammar data structures.
The source sides of the grammar rules arestored in a packed trie.
Each node may contain n children and the symbols linking to them, and m entriesfor rules that share the same source side.
Each rule entry links to a node in the target-side trie, where the fulltarget string can be retrieved by walking up the trie until the root is reached.
The rule entries also containa data block id, which identifies feature data attached to the rule.
The features are encoded according to atype/quantization specification and stored as variable-length blocks of data in a byte buffer.pointer into the target trie, we maintain an offset ta-ble in which we keep track of where each new trielevel begins in the array.
By first searching the offsettable, we can determine d, and thus know how muchspace to allocate for the complete target side string.To further benefit from the overlap there may beamong the target sides in the grammar, we drop thenonterminal labels from the target string prior to in-serting them into the trie.
For richly labeled gram-mars, this collapses all lexically identical target sidesthat share the same nonterminal reordering behavior,but vary in nonterminal labels into a single path inthe trie.
Since the nonterminal labels are retained inthe rules?
source sides, we do not lose any informa-tion by doing this.2.1.3 Features and Other DataWe designed the data format for the grammarrules?
feature values to be easily extended to includeother information that we may want to attach to arule, such as word alignments, or locations of occur-rences in the training data.
In order to that, each ruleri has a unique block id bi associated with it.
Thisblock id identifies the information associated withthe rule in every attached data store.
All data storesare implemented as memory-mapped byte buffersthat are only loaded into memory when actually re-quested by the decoder.
The format for the featuredata is detailed in the following.The rules?
feature values are stored as sparse fea-tures in contiguous blocks of variable length in abyte buffer.
As shown in Figure 1, a lookup tableis used to map the bi to the index of the block in thebuffer.
Each block is structured as follows: a sin-gle integer, n, for the number of features, followedby n feature entries.
Each feature entry is led by aninteger for the feature id fj , and followed by a fieldof variable length for the feature value vj .
The sizeof the value is determined by the type of the feature.Joshua maintains a quantization configuration whichmaps each feature id to a type handler or quantizer.After reading a feature id from the byte buffer, weretrieve the responsible quantizer and use it to readthe value from the byte buffer.Joshua?s packed grammar format supports Java?sstandard primitive types, as well as an 8-bit quan-tizer.
We chose 8 bit as a compromise betweencompression, value decoding speed and transla-285Grammar Format MemoryHiero (43M rules)Baseline 13.6GPacked 1.8GSyntax (200M rules)Baseline 99.5GPacked 9.8GPacked 8-bit 5.8GTable 1: Decoding-time memory use for the packedgrammar versus the standard grammar format.
Evenwithout lossy quantization the packed grammar rep-resentation yields significant savings in memoryconsumption.
Adding 8-bit quantization for the real-valued features in the grammar reduces even largesyntactic grammars to a manageable size.tion performance (Federico and Bertoldi, 2006).Our quantization approach follows Federico andBertoldi (2006) and Heafield (2011) in partitioningthe value histogram into 256 equal-sized buckets.We quantize by mapping each feature value onto theweighted average of its bucket.
Joshua allows for aneasily per-feature specification of type.
Quantizerscan be share statistics across multiple features withsimilar value distributions.2.2 ExperimentsWe assess the packed grammar representation?smemory efficiency and impact on the decodingspeed on the WMT12 French-English task.
Ta-ble 1 shows a comparison of the memory neededto store our WMT12 French-English grammars atruntime.
We can observe a substantial decrease inmemory consumption for both Hiero-style gram-mars and the much larger syntactically annotatedgrammars.
Even without any feature value quantiza-tion, the packed format achieves an 80% reductionin space requirements.
Adding 8-bit quantizationfor the log-probability features yields even smallergrammar sizes, in this case a reduction of over 94%.In order to avoid costly repeated retrievals of indi-vidual feature values of rules, we compute and cachethe stateless application cost for each grammar ruleat grammar loading time.
This, alongside with a lazyapproach to rule lookup allows us to largely avoidlosses in decoding speed.Figure shows a translation progress graph for theWMT12 French-English development set.
Both sys-0 500 1000 1500 2000 25000  500  1000  1500  2000  2500Sentences Translated Seconds PassedStandardPackedFigure 2: A visualization of the loading and decod-ing speed on the WMT12 French-English develop-ment set contrasting the packed grammar represen-tation with the standard format.
Grammar loadingfor the packed grammar representation is substan-tially faster than that for the baseline setup.
Evenwith a slightly slower decoding speed (note the dif-ference in the slopes) the packed grammar finishesin less than half the time, compared to the standardformat.tems load a Hiero-style grammar with 43 millionrules, and use 16 threads for parallel decoding.
Theinitial loading time for the packed grammar repre-sentation is dramatically shorter than that for thebaseline setup (a total of 176 seconds for loading andsorting the grammar, versus 1897 for the standardformat).
Even though decoding speed is slightlyslower with the packed grammars (an average of 5.3seconds per sentence versus 4.2 for the baseline), theeffective translation speed is more than twice that ofthe baseline (1004 seconds to complete decoding the2489 sentences, versus 2551 seconds with the stan-dard setup).3 J-PRO: Pairwise Ranking Optimizationin JoshuaPairwise ranking optimization (PRO) proposed by(Hopkins and May, 2011) is a new method for dis-criminative parameter tuning in statistical machinetranslation.
It is reported to be more stable than thepopular MERT algorithm (Och, 2003) and is morescalable with regard to the number of features.
PROtreats parameter tuning as an n-best list rerankingproblem, and the idea is similar to other pairwiseranking techniques like ranking SVM and IR SVMs286(Li, 2011).
The algorithm can be described thusly:Let h(c) = ?w,?(c)?
be the linear model scoreof a candidate translation c, in which ?
(c) is thefeature vector of c and w is the parameter vector.Also let g(c) be the metric score of c (without lossof generality, we assume a higher score indicates abetter translation).
We aim to find a parameter vectorw such that for a pair of candidates {ci, cj} in an n-best list,(h(ci)?
h(cj))(g(ci)?
g(cj)) =?w,?(ci)??(cj)?(g(ci)?
g(cj)) > 0,namely the order of the model score is consistentwith that of the metric score.
This can be turned intoa binary classification problem, by adding instance?
?ij = ?(ci)??
(cj)with class label sign(g(ci) ?
g(cj)) to the trainingdata (and symmetrically add instance?
?ji = ?(cj)??
(ci)with class label sign(g(cj) ?
g(ci)) at the sametime), then using any binary classifier to find the wwhich determines a hyperplane separating the twoclasses (therefore the performance of PRO dependson the choice of classifier to a large extent).
Givena training set with T sentences, there are O(Tn2)pairs of candidates that can be added to the trainingset, this number is usually much too large for effi-cient training.
To make the task more tractable, PROsamples a subset of the candidate pairs so that onlythose pairs whose metric score difference is largeenough are qualified as training instances.
This fol-lows the intuition that high score differential makesit easier to separate good translations from bad ones.3.1 ImplementationPRO is implemented in Joshua 4.0 named J-PRO.In order to ensure compatibility with the decoderand the parameter tuning module Z-MERT (Zaidan,2009) included in all versions of Joshua, J-PRO isbuilt upon the architecture of Z-MERT with sim-ilar usage and configuration files(with a few extralines specifying PRO-related parameters).
J-PRO in-herits Z-MERT?s ability to easily plug in new met-rics.
Since PRO allows using any off-the-shelf bi-nary classifiers, J-PRO provides a Java interface thatenables easy plug-in of any classifier.
Currently, J-PRO supports three classifiers:?
Perceptron (Rosenblatt, 1958): the percep-tron is self-contained in J-PRO, no external re-sources required.?
MegaM (Daume?
III and Marcu, 2006): the clas-sifier used by Hopkins and May (2011).2?
Maximum entropy classifier (Manning andKlein, 2003): the Stanford toolkit for maxi-mum entropy classification.3The user may specify which classifier he wants touse and the classifier-specific parameters in the J-PRO configuration file.The PRO approach is capable of handling a largenumber of features, allowing the use of sparse dis-criminative features for machine translation.
How-ever, Hollingshead and Roark (2008) demonstratedthat naively tuning weights for a heterogeneous fea-ture set composed of both dense and sparse featurescan yield subpar results.
Thus, to better handle therelation between dense and sparse features and pro-vide a flexible selection of training schemes, J-PROsupports the following four training modes.
We as-sume M dense features and N sparse features areused:1.
Tune the dense feature parameters only, justlike Z-MERT (M parameters to tune).2.
Tune the dense + sparse feature parameters to-gether (M +N parameters to tune).3.
Tune the sparse feature parameters only withthe dense feature parameters fixed, and sparsefeature parameters scaled by a manually speci-fied constant (N parameters to tune).4.
Tune the dense feature parameters and the scal-ing factor for sparse features, with the sparsefeature parameters fixed (M+1 parameters totune).J-PRO supports n-best list input with a sparse fea-ture format which enumerates only the firing fea-tures together with their values.
This enables a morecompact feature representation when numerous fea-tures are involved in training.2hal3.name/megam3nlp.stanford.edu/software2870 10 20 30010203040IterationBLEUDev set MT03 (10 features)PercepMegaMMax?Ent0 10 20 30010203040IterationBLEUTest set MT04(10 features)PercepMegaMMax?Ent0 10 20 30010203040IterationBLEUTest set MT05(10 features)PercepMegaMMax?Ent0 10 20 30010203040IterationBLEUDev set MT03 (1026 features)PercepMegaMMax?Ent0 10 20 30010203040IterationBLEUTest set MT04(1026 features)PercepMegaMMax?Ent0 10 20 30010203040IterationBLEUTest set MT05(1026 features)PercepMegaMMax?EntFigure 3: Experimental results on the development and test sets.
The x-axis is the number of iterations (up to30) and the y-axis is the BLEU score.
The three curves in each figure correspond to three classifiers.
Upperrow: results trained using only dense features (10 features); Lower row: results trained using dense+sparsefeatures (1026 features).
Left column: development set (MT03); Middle column: test set (MT04); Rightcolumn: test set (MT05).Datasets Z-MERTJ-PROPercep MegaM Max-EntDev (MT03) 32.2 31.9 32.0 32.0Test (MT04) 32.6 32.7 32.7 32.6Test (MT05) 30.7 30.9 31.0 30.9Table 2: Comparison between the results given by Z-MERT and J-PRO (trained with 10 features).3.2 ExperimentsWe did our experiments using J-PRO on the NISTChinese-English data, and BLEU score was used asthe quality metric for experiments reported in thissection.4 The experimental settings are as the fol-lowing:Datasets: MT03 dataset (998 sentences) as devel-opment set for parameter tuning, MT04 (1788 sen-tences) and MT05 (1082 sentences) as test sets.Features: Dense feature set include the 10 regularfeatures used in the Hiero system; Sparse feature set4We also experimented with other metrics including TER,METEOR and TER-BLEU.
Similar trends as reported in thissection were observed.
These results are omitted here due tolimited space.includes 1016 target-side rule POS bi-gram featuresas used in (Li et al, 2010b).Classifiers: Perceptron, MegaM and Maximumentropy.PRO parameters: ?
= 8000 (number of candidatepairs sampled uniformly from the n-best list), ?
= 1(sample acceptance probability), ?
= 50 (number oftop candidates to be added to the training set).Figure 3 shows the BLEU score curves on thedevelopment and test sets as a function of itera-tions.
The upper and lower rows correspond tothe results trained with 10 dense features and 1026dense+sparse features respectively.
We intentionallyselected very bad initial parameter vectors to verifythe robustness of the algorithm.
It can be seen that288with each iteration, the BLEU score increases mono-tonically on both development and test sets, and be-gins to converge after a few iterations.
When only 10features are involved, all classifiers give almost thesame performance.
However, when scaled to over athousand features, the maximum entropy classifierbecomes unstable and the curve fluctuates signifi-cantly.
In this situation MegaM behaves well, butthe J-PRO built-in perceptron gives the most robustperformance.Table 2 compares the results of running Z-MERTand J-PRO.
Since MERT is not able to handle nu-merous sparse features, we only report results forthe 10-feature setup.
The scores for both setupsare quite close to each other, with Z-MERT doingslightly better on the development set but J-PROyielding slightly better performance on the test set.4 Thrax: Grammar Extraction at Scale4.1 Translation GrammarsIn previous years, our grammar extraction methodswere limited by either memory-bounded extractors.Moving towards a parallelized grammar extractionprocess, we switched from Joshua?s formerly built-in extraction module to Thrax for WMT11.
How-ever, we were limited to a simple pseudo-distributedHadoop setup.
In a pseudo-distributed cluster, alltasks run on separate cores on the same machineand access the local file system simultaneously, in-stead of being distributed over different physical ma-chines and harddrives.
This setup proved unreliablefor larger extractions, and we were forced to reducethe amount of data that we used to train our transla-tion models.For this year, however, we had a permanent clus-ter at our disposal, which made it easy to extractgrammars from all of the available WMT12 data.We found that on a properly distributed Hadoopsetup Thrax was able to extract both Hiero gram-mars and the much larger SAMT grammars on thecomplete WMT12 training data for all tested lan-guage pairs.
The runtimes and resulting (unfiltered)grammar sizes for each language pair are shown inTable 3 (for Hiero) and Table 4 (for SAMT).Language Pair Time RulesCs ?
En 4h41m 133MDe ?
En 5h20m 219MFr ?
En 16h47m 374MEs ?
En 16h22m 413MTable 3: Extraction times and grammar sizes for Hi-ero grammars using the Europarl and News Com-mentary training data for each listed language pair.Language Pair Time RulesCs ?
En 7h59m 223MDe ?
En 9h18m 328MFr ?
En 25h46m 654MEs ?
En 28h10m 716MTable 4: Extraction times and grammar sizes forthe SAMT grammars using the Europarl and NewsCommentary training data for each listed languagepair.4.2 Paraphrase ExtractionRecently English-to-English text generation taskshave seen renewed interest in the NLP commu-nity.
Paraphrases are a key component in large-scale state-of-the-art text-to-text generation systems.We present an extended version of Thrax that im-plements distributed, Hadoop-based paraphrase ex-traction via the pivoting approach (Bannard andCallison-Burch, 2005).
Our toolkit is capable ofextracting syntactically informed paraphrase gram-mars at scale.
The paraphrase grammars obtainedwith Thrax have been shown to achieve state-of-the-art results on text-to-text generation tasks (Ganitke-vitch et al, 2011).For every supported translation feature, Thrax im-plements a corresponding pivoted feature for para-phrases.
The pivoted features are set up to be awareof the prerequisite translation features they are de-rived from.
This allows Thrax to automatically de-tect the needed translation features and spawn thecorresponding map-reduce passes before the pivot-ing stage takes place.
In addition to features use-ful for translation, Thrax also offers a number offeatures geared towards text-to-text generation taskssuch as sentence compression or text simplification.Due to the long tail of translations in unpruned289Source Bitext Sentences Words Pruning RulesFr ?
En 1.6M 45M p(e1|e2), p(e2|e1) > 0.001 49M{Da + Sv + Cs + De + Es + Fr} ?
En 9.5M 100Mp(e1|e2), p(e2|e1) > 0.02 31Mp(e1|e2), p(e2|e1) > 0.001 91MTable 5: Large paraphrase grammars extracted from EuroParl data using Thrax.
The sentence and wordcounts refer to the English side of the bitexts used.translation grammars and the combinatorial effectof pivoting, paraphrase grammars can easily growvery large.
We implement a simple feature-levelpruning approach that allows the user to specify up-per or lower bounds for any pivoted feature.
If aparaphrase rule is not within these bounds, it is dis-carded.
Additionally, pivoted features are aware ofthe bounding relationship between their value andthe value of their prerequisite translation features(i.e.
whether the pivoted feature?s value can be guar-anteed to never be larger than the value of the trans-lation feature).
Thrax uses this knowledge to dis-card overly weak translation rules before the pivot-ing stage, leading to a substantial speedup in the ex-traction process.Table 5 gives a few examples of large paraphrasegrammars extracted from WMT training data.
Withappropriate pruning settings, we are able to obtainparaphrase grammars estimated over bitexts withmore than 100 million words.5 Additional New Features?
With the help of the respective original au-thors, the language model implementations byHeafield (2011) and Pauls and Klein (2011)have been integrated with Joshua, droppingsupport for the slower and more difficult tocompile SRILM toolkit (Stolcke, 2002).?
We modified Joshua so that it can be used asa parser to analyze pairs of sentences using asynchronous context-free grammar.
We imple-mented the two-pass parsing algorithm of Dyer(2010).6 ConclusionWe present a new iteration of the Joshua machinetranslation toolkit.
Our system has been extended to-wards efficiently supporting large-scale experimentsin parsing-based machine translation and text-to-textgeneration: Joshua 4.0 supports compactly repre-sented large grammars with its packed grammars,as well as large language models via KenLM andBerkeleyLM.We include an implementation of PRO,allowing for stable and fast tuning of large featuresets, and extend our toolkit beyond pure translationapplications by extending Thrax with a large-scaleparaphrase extraction module.Acknowledgements This research was supportedby in part by the EuroMatrixPlus project fundedby the European Commission (7th Framework Pro-gramme), and by the NSF under grant IIS-0713448.Opinions, interpretations, and conclusions are theauthors?
alone.ReferencesColin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with bilingual parallel corpora.
In Proceed-ings of ACL.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?228.Hal Daume?
III and Daniel Marcu.
2006.
Domain adap-tation for statistical classifiers.
Journal of ArtificialIntelligence Research, 26(1):101?126.Chris Dyer.
2010.
Two monolingual parses are bet-ter than one (synchronous parse).
In Proceedings ofHLT/NAACL, pages 263?266.
Association for Compu-tational Linguistics.Marcello Federico and Nicola Bertoldi.
2006.
Howmany bits are needed to store probabilities for phrase-based translation?
In Proceedings of WMT06, pages94?101.
Association for Computational Linguistics.Juri Ganitkevitch, Chris Callison-Burch, CourtneyNapoles, and Benjamin Van Durme.
2011.
Learningsentential paraphrases from bilingual parallel corporafor text-to-text generation.
In Proceedings of EMNLP.Kenneth Heafield.
2011.
Kenlm: Faster and smallerlanguage model queries.
In Proceedings of the SixthWorkshop on Statistical Machine Translation, pages187?197.
Association for Computational Linguistics.290Kristy Hollingshead and Brian Roark.
2008.
Rerank-ing with baseline system scores and ranks as features.Technical report, Center for Spoken Language Under-standing, Oregon Health & Science University.Mark Hopkins and Jonathan May.
2011.
Tuning as rank-ing.
In Proceedings of EMNLP.Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-itkevitch, Sanjeev Khudanpur, Lane Schwartz, WrenThornton, Jonathan Weese, and Omar Zaidan.
2009.Joshua: An open source toolkit for parsing-based ma-chine translation.
In Proc.
WMT, Athens, Greece,March.Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-itkevitch, Ann Irvine, Sanjeev Khudanpur, LaneSchwartz, Wren N.G.
Thornton, Ziyuan Wang,Jonathan Weese, and Omar F. Zaidan.
2010a.
Joshua2.0: a toolkit for parsing-based machine translationwith syntax, semirings, discriminative training andother goodies.
In Proc.
WMT.Zhifei Li, Ziyuan Wang, and Sanjeev Khudanpur.
2010b.Unsupervised discriminative language model trainingfor machine translation using simulated confusion sets.In Proceedings of COLING, Beijing, China, August.Hang Li.
2011.
Learning to Rank for Information Re-trieval and Natural Language Processing.
Morgan &Claypool Publishers.Chris Manning and Dan Klein.
2003.
Optimization,maxent models, and conditional estimation withoutmagic.
In Proceedings of HLT/NAACL, pages 8?8.
As-sociation for Computational Linguistics.Franz Och.
2003.
Minimum error rate training in statis-tical machine translation.
In Proceedings of the 41rdAnnual Meeting of the Association for ComputationalLinguistics (ACL-2003), Sapporo, Japan.Adam Pauls and Dan Klein.
2011.
Faster and smaller n-gram language models.
In Proceedings of ACL, pages258?267, Portland, Oregon, USA, June.
Associationfor Computational Linguistics.Frank Rosenblatt.
1958.
The perceptron: A probabilisticmodel for information storage and organization in thebrain.
Psychological Review, 65(6):386?408.Andreas Stolcke.
2002.
Srilm - an extensible languagemodeling toolkit.
In Seventh International Conferenceon Spoken Language Processing.Jonathan Weese, Juri Ganitkevitch, Chris Callison-Burch, Matt Post, and Adam Lopez.
2011.
Joshua3.0: Syntax-based machine translation with the Thraxgrammar extractor.
In Proceedings of WMT11.Omar F. Zaidan.
2009.
Z-MERT: A fully configurableopen source tool for minimum error rate training ofmachine translation systems.
The Prague Bulletin ofMathematical Linguistics, 91:79?88.Richard Zens and Hermann Ney.
2007.
Efficient phrase-table representation for machine translation with appli-cations to online MT and speech translation.
In Pro-ceedings of HLT/NAACL, pages 492?499, Rochester,New York, April.
Association for Computational Lin-guistics.291
