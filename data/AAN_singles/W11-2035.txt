Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 307?311,Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational LinguisticsImproving Pronominal and Deictic Co-Reference Resolution withMulti-Modal FeaturesLin Chen, Anruo Wang, Barbara Di EugenioDepartment of Computer ScienceUniversity of Illinois at Chicago851 S Morgan ST, Chicago, IL 60607, USA{lchen43,awang28,bdieugen}@uic.eduAbstractWithin our ongoing effort to develop a com-putational model to understand multi-modalhuman dialogue in the field of elderly care,this paper focuses on pronominal and deicticco-reference resolution.
After describing ourdata collection effort, we discuss our anno-tation scheme.
We developed a co-referencemodel that employs both a simple notion ofmarkable type, and multiple statistical mod-els.
Our results show that knowing the typeof the markable, and the presence of simulta-neous pointing gestures improve co-referenceresolution for personal and deictic pronouns.1 IntroductionOur ongoing research project, called RoboHelper,focuses on developing an interface for older peopleto effectively communicate with a robotic assistantthat can help them perform Activities of Daily Liv-ing (ADLs) (Krapp, 2002), so that they can safely re-main living in their home (Di Eugenio et al, 2010).We are devising a multi-modal interface since peo-ple communicate with one another using a variety ofverbal and non-verbal signals, including haptics, i.e.,force exchange (as when one person hands a bowl toanother person, and lets go only when s/he sensesthat the other is holding it).
We have collected amid size multi-modal human-human dialogue cor-pus, that we are currently processing and analyz-ing.
Meanwhile, we have started developing onecore component of our multi-modal interface, a co-reference resolution system.
In this paper, we willpresent the component of the system that resolvespronouns, both personal (I, you, it, they), and deictic(this, that, these, those, here, there).
Hence, this pa-per presents our first steps toward a full co-referenceresolution module, and ultimately, the multi-modalinterface.Co-reference resolution is likely the discourseand dialogue processing task that has received themost attention.
However, as Eisenstein and Davis(2006) notes, research on co-reference resolutionhas mostly been applied to written text; this taskis more difficult in dialogue.
First, utterances maybe informal, ungrammatical or disfluent; second,people spontaneously use hand gestures, body ges-tures and gaze.
Pointing gestures are the eas-iest gestures to identify, and vision researchers inour project are working on recognizing pointing andother hand gestures (Di Eugenio et al, 2010).
In thispaper, we replicate the results from (Eisenstein andDavis, 2006), that pointing gestures help improveco-reference, in a very different domain.
Other workhas shown that gestures can help detect sentenceboundaries (Chen and Harper, 2010) or user inten-tions (Qu and Chai, 2008).The rest of the paper is organized as follows.
InSection 2 we describe the data collection and the on-going annotation.
In Section 3 we discuss our co-reference resolution system, and we present experi-ments and results in Section 4.2 The ELDERLY-AT-HOME corpusDue to the absence of multi-modal collaborativehuman-human dialogue corpora that include hapticdata beyond what can be acquired via point-and-touch interfaces, and in the population of interest,307Figure 1: Experiment Excerptswe undertook a new data collection effort.
Our ex-periments were conducted in a fully functional stu-dio apartment at Rush University in Chicago ?
Fig-ure 1 shows two screen-shots from our recorded ex-periments.
We equipped the room with 7 web cam-eras to ensure multiple points of view.
Each of thetwo participants in the experiments wears a micro-phone, and a data glove on their dominant hand tocollect haptics data.
The ADLs we focused on in-clude ambulating, getting up from a bed or a chair,finding pots, opening cans and containers, puttingpots on a stove, setting the table etc.
Two studentsin gerontological nursing play the role of the helper(HEL), both in pilot studies and with real subjects.In 5 pilot dialogues, two faculty members played therole of the elderly person (ELD).
In the 15 real ex-periments, ELD resides in an assisted living facil-ity and was transported to the apartment mentionedabove.
All elderly subjects are highly functioning ata cognitive level and do not have any major physicalimpairment.The size of our collected video data is shownin Table 1.
The number of subjects refers to thenumber of different ELD?s and does not include thehelpers; we do include our 5 pilot dialogues though,since those pilot interactions do not measurably dif-fer from those with the real subjects.
Usually oneexperiment lasts about 50?
(recording starts after in-formed consent and after the microphones and datagloves have been put on).
Further, we eliminatedirrelevant content such as interruptions, e.g.
by theperson who accompanied the elderly subjects, andfurther explanations of the tasks.
This resulted inabout 15 minutes of what we call effective data foreach subject; the effective data comprises 4782 turns(see Table 1).Subjects Raw(Mins) Effective(Mins) Turns20 482 301 4782Table 1: ELDERLY-AT-HOME Corpus SizeThe effective portion of the data was transcribedby the first two authors using the Anvil video anno-tation tool (Kipp, 2001).
A subset of the transcribeddata was annotated for co-reference, yielding 114sub-dialogues corresponding to the tasks subjectsperform, such as finding bowls, filling a pot with wa-ter, etc.
(see Table 2).An annotation excerpt is shown in Figure 2.Markable tokens are classified into PLC(Place),PERS(Person), OBJ(Object) types, and numberedby type, e.g., PLC#5.
Accordingly, we mark pro-nouns with types as well, RPLC, RPERS, ROBJ, e.g.RPLC#5.
If a subject produced a pointing gesture,we generate a markable token to mark what is beingpointed to at the end of the utterance (see Utt.
4 and 5in Figure 2).
Within the same task, if two markableshave the same type and the same markable index,they are taken to co-refer (hence, longer chains ofreference across tasks are cut into shorter spans).Haptics annotation is at the beginning.
We haveidentified grab, hold, give and receive as high-levelhaptics phonemes that may be useful from the lan-guage point of view.
We have recently started anno-tating our corpus with those labels.Subjects Tasks Utterances Gestures Pronouns12 114 1920 896 1635Table 2: Annotated Corpus SizeIn order to test the reliability of our annotation,we double coded about 18% of the data, namely 21sub-dialogues comprising 213 pronouns, on whichwe computed the Kappa coefficient (Carletta, 1996).Similar to (Rodr?guez et al, 2010), we measured thereliability of markable annotations, and of link tothe antecedent annotations.
As concerns the mark-able level, we obtained ?=0.945, which is high butno surprisingly for such a simple task.
At the link tothe antecedent level, we compared the links frompronouns to antecedents in a specified context of 4utterances, obtaining a reasonable ?=0.723.3083: PERS#1(HEL/NNP) : RPERS#1(I/PRP) do/VBP n?t/RB see/VB any/DT OBJ#3(pasta/NN) ./.4: PERS#2(ELD/NNP) : Try/VB over/IN RPLC#5(there/RB) ./.
{PLC#5(cabinet/NN)}5: PERS#1(HEL/NNP) : This/DT RPLC#5(one/NN) ?/.
{PLC#5(cabinet/NN)}6: PERS#2(ELD/NNP) : Oh/UH ,/, yes/RB ./.Figure 2: Annotation Excerpt3 Our approachUtterances and GesturesFind Markables Generate CandidatesCoreference PairsPreprocessingMarkable Model Coreference ModelFigure 3: Co-reference System ArchitectureThe architecture of our co-reference resolutionsystem is shown in Figure 3.We first pre-process a dialogue by splitting turnsinto sentences, tokenizing sentences into tokens,POS tagging tokens.
The Markable model is usedto classify whether a token can be referred to andwhat type of markable it is.
The Markable model?sfeature set includes the POS tag of the token, theword, the surrounding tokens?
POS tags in a win-dow size of 3.
The model outputs markable classes:Place/Object/Person, or None, which means the to-ken is not markable.
A pointed-to entity serves as amarkable by default.To perform resolution, each pronoun to be re-solved ( I, you, it, they; this, that, these, those, here,there) is paired with markables in the context of theprevious 2 utterances, the current utterance and theutterance that follows, by using {pronoun, markabletype} compatibility rules.
For example, let?s con-sider the excerpt in Figure 2.
To resolve one inutterance 5, the system will generate 3 candidatetoken pairs: <one(5,2), pasta(3,6)>, <one(5,2),cabinet(4,-1)>, <one(5,2), cabinet(5,-1)> (includ-ing the pointed-to markable is a way of roughly ap-proximating information that will be returned by thevision component).
The elements in those pairsare tokens with their coordinates in the format (Sen-tenceIndex, TokenIndex); markables pointed to aregiven negative token indices.The Co-reference model will filter out the pairs<pronoun, markable> that it judges to be incor-rect.
For the Co-reference model, we adopted asubset of features which are commonly used in co-reference resolution in written text.
These featuresapply to each <pronoun, markable> pair and in-clude: Lexical features, i.e.
words and POS tags forboth anaphora and antecedent; Syntactic features,i.e.
syntactic constraints such as number and per-son agreement; Distance features, i.e.
sentence dis-tance, token distance and markable distance.
Addi-tionally, the Co-reference model uses pointing ges-ture information.
If the antecedent in the <pronoun,markable> was pointed to, the pair is tagged as Is-Pointed.
In our data, people often use pronounsand hand gestures instead of nouns when introduc-ing new entities.
It is not possible to map thesepronouns to a textual antecedent since none exists.This confirms the findings from (Kehler, 2000): ina multi-modal corpus, he found that no pronoun isused without a gesture when it refers to a referentwhich is not in focus.4 Experiments and DiscussionThe classification models described above were im-plemented using the Weka package (Hall et al,2009).
Specifically, for each model, we experi-mented with J48 (a decision tree implementation)and LibSVM (a Support Vector Machine implemen-tation).
All the results reported below are calculatedusing 10 fold cross-validation.We evaluated the performances of individualmodels separately (Tables 3 and 4), and of the sys-tem as a whole (Table 5).Algorithm Precision Recall F-MeasureJ48 0.984 0.984 0.984LibSVM 0.979 0.936 0.954Baseline 0.971 0.971 0.971Table 3: Markable Model PerformanceThe results in Table 3 are not surprising, since de-tecting the type of markables is a simple task.
In-deed the results of the baseline model are extremely309Method J48 LibSVMPrecision Recall F-Measure Precision Recall F-MeasureText + Gesture 0.700 0.684 0.686 0.672 0.669 0.670Text Only 0.655 0.656 0.656 0.624 0.624 0.624Table 4: Co-reference Model PerformanceWords Method Features Precision Recall F-MeasureAll PronounsJ48 Text Only 0.544 0.332 0.412Text + Gesture 0.482 0.783 0.596LibSVM Text Only 0.56 0.27 0.364Text + Gesture 0.522 0.6 0.559Baseline Text Only 0.367 0.254 0.300Text + Gesture 0.376 0.392 0.3843rd Person + DeicticJ48 Text Only 0.264 0.028 0.05Text + Gesture 0.438 0.902 0.589LibSVM Text Only 0.6 0.009 0.017Text + Gesture 0.525 0.695 0.598Baseline Text Only 0.172 0.114 0.137Text + Gesture 0.301 0.431 0.354Table 5: Co-reference System Performance (Markable + Co-reference Models)high as well.
We compute the baseline by assigningto the potential markable (i.e., each word) its mostfrequent class in the training set (recall that the fourclasses include None as well).For the Co-reference model, we conducted 2 setsof experiments to ascertain the effect of includingGesture in the model.
As shown in Table 4, both J48and LibSVM obtain better results when we includegestures in the model.
?2 shows that differences inprecision and recall 1 are significant at the p ?
0.01level, though the absolute improvement is not high.As concerns the evaluation of the whole system,we ran a 4-way experiment, where we examine theperformance of the system on all pronouns, and onthose pronouns left after eliminating first and secondperson pronouns, without and with Gesture informa-tion.
We also ran two sets of baseline experiments.In the baseline experiments, we link each pronounwe want to resolve, to the most recent utterance-markable token and to a pointed-to markable token(if applicable).
Markables are filtered by the samecompatibility rules mentioned above.Regarding the metrics we used for evaluation, weused the same method as Strube and Mu?ller (2003),which is also similar to MUC standard (Hirschman,1?2 does not apply to the F-Measure.1997).
As the golden set, we used the human an-notated links from the pronouns to markables in thesame context of four utterances used by the system.Then, we compared the co-reference links found bythe system against the golden set, and we finally cal-culated precision, recall and F-Measure.Table 5 shows that the F-measure is higher whenincluding gestures, no matter the type of pronouns.When we include gestures, there is no difference be-tween ?All Pronouns?
and ?3rd Person + Deictic?.In the ?3rd Person + Deictic?
experiments, we ob-served huge drops in recall, from 0.902 to 0.028 forJ48, and from 0.695 to 0.009 for LibSVM algorithm.This confirms the point we made earlier, that 3rdperson pronouns/deictic words (Kehler, 2000) oftendo not have textual antecedents, since when accom-panied by simultaneous pointing they introduce newentities in a dialogue.Comparison to previous work is feasible only at ahigh level, because of the usage of different corporaand/or measurement metrics.
This said, our modelwith gestures outperforms Strube andMu?ller (2003),who did not use gesture information to resolve pro-nouns in spoken dialogue.
Strube and Mu?ller (2003)used the 20 Switchboard dialogues as their experi-ment dataset, and used the MUC metrics.
Our re-310sults are similar to Eisenstein and Davis (2006), butthere are two main differences.
First, the corpusthey used is smaller than what we used in this pa-per.
Their corpus was collected by themselves andconsisted of 16 videos, each video was 2-3 minutesin length.
Second, they used a difference measure-ment metrics called CEAF (Luo, 2005).5 ConclusionsIn this paper, we presented the new ELDERLY-AT-HOME multi-modal corpus we collected.
A co-reference resolution system for personal and deic-tic pronouns has been developed on the basis of theannotated corpus.
Our results confirm that gesturesimprove co-reference resolution; a simple notion oftype also helps.
The Markable and Co-referencemodules we presented are a first start in developinga full multi-modal co-reference resolution module.Apart from completing the annotation of our cor-pus, we will develop an annotation scheme for hap-tics, and investigate how haptics information affectsco-reference and other dialogue phenomena.
Ulti-mately, both pointing gestures and haptic informa-tion will automatically be recognized by the collab-orators in the project we are members of.AcknowledgmentsThis work is supported by award IIS 0905593 fromthe National Science Foundation.
Thanks to theother members of the RoboHelper project, for theirmany contributions, especially to the data collectioneffort.ReferencesJean Carletta.
1996.
Assessing agreement on classifi-cation tasks: The kappa statistic.
Computational Lin-guistics, 22:249?254.Lei Chen and Mary P. Harper.
2010.
Utilizing gesturesto improve sentence boundary detection.
MultimediaTools and Applications, pages 1?33.Barbara Di Eugenio, Milos?
Z?efran, Jezekiel Ben-Arie, Mark Foreman, Lin Chen, Simone Franzini,Shankaranand Jagadeesan, Maria Javaid, and KaiMa.
2010.
Towards Effective Communication withRobotic Assistants for the Elderly: Integrating Speech,Vision and Haptics.
InDialog with Robots, AAAI 2010Fall Symposium, Arlington, VA, USA, November.Jacob Eisenstein and Randall Davis.
2006.
GestureImproves Coreference Resolution.
In Proceedings ofthe Human Language Technology Conference of theNAACL, Companion Volume: Short Papers, pages 37?40.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA data mining software: An update.SIGKDD Explorations, 11(1).Lynette Hirschman.
1997.
Muc-7 coreference task defi-nition.Andrew Kehler.
2000.
Cognitive Status and Form ofReference in Multimodal Human-Computer Interac-tion.
In AAAI 00, The 15th Annual Conference of theAmerican Association for Artificial Intelligence, pages685?689.Michael Kipp.
2001.
Anvil-a generic annotation toolfor multimodal dialogue.
In Proceedings of the 7thEuropean Conference on Speech Communication andTechnology, pages 1367?1370.Kristine M. Krapp.
2002.
The Gale Encyclopedia ofNursing & Allied Health.
Gale Group, Inc. ChapterActivities of Daily Living Evaluation.Xiaoqiang Luo.
2005.
On coreference resolution perfor-mance metrics.
In Proceedings of the conference onHuman Language Technology and Empirical Methodsin Natural Language Processing, HLT ?05, pages 25?32, Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Shaolin Qu and Joyce Y. Chai.
2008.
Beyond attention:the role of deictic gesture in intention recognition inmultimodal conversational interfaces.
In Proceedingsof the 13th international conference on Intelligent userinterfaces, pages 237?246.Kepa Joseba Rodr?guez, Francesca Delogu, Yannick Ver-sley, Egon Stemle, and Massimo Poesio.
2010.Anaphoric annotation of wikipedia and blogs in thelive memories corpus.
In Proceedings of the 7th In-ternational Conference on Language Ressources andEvaluation (LREC 2010), pages 157?163.Michael Strube and Christoph Mu?ller.
2003.
A machinelearning approach to pronoun resolution in spoken di-alogue.
In Proceedings of the 41st Annual Meeting onAssociation for Computational Linguistics-Volume 1.311
