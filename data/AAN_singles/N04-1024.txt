Evaluating Multiple Aspects of Coherence in Student EssaysDerrick HigginsEducationalTesting ServiceJill BursteinEducationalTesting ServiceDaniel MarcuUniversity of SouthernCalifornia/ Information SciencesInstituteClaudia GentileEducationalTesting ServiceAbstractCriterionSM Online Essay Evaluation Serviceincludes a capability that labels sentences instudent writing with essay-based discourse el-ements (e.g., thesis statements).
We describea new system that enhances Criterion?s capa-bility, by evaluating multiple aspects of co-herence in essays.
This system identifies fea-tures of sentences based on semantic similaritymeasures and discourse structure.
A supportvector machine uses these features to capturebreakdowns in coherence due to relatednessto the essay question and relatedness betweendiscourse elements.
Intra-sentential quality isevaluated with rule-based heuristics.
Resultsindicate that the system yields higher perfor-mance than a baseline on all three aspects.1 OverviewThis work is motivated by a need for advanced discourseanalysis capabilities for writing instruction applications.CriterionSM Online Essay Evaluation Service is an appli-cation for writing instruction which includes a capabilityto annotate sentences in student essays with discourse el-ement labels.
These labels include the categories ThesisStatement, Main Idea, Supporting Idea, and Conclusion(Burstein et al, 2003b).
Though it accurately annotatessentences with essay-based discourse labels, Criteriondoes not provide an evaluation of the expressive qualityof the sentences that comprise a discourse segment.
Thesystem might accurately label a student?s essay as hav-ing all of the typically expected discourse elements: the-sis statement, 3 main ideas, supporting evidence linkedto each main idea, and a conclusion.
As teachers havepointed out, however, an essay may have all of these or-ganizational elements, but the quality of individual ele-ments may need improvement.In this paper, we present a capability that captures ex-pressive quality of sentences in the discourse segmentsof an essay.
For this work, we have defined expressivequality in terms of four aspects related to global and lo-cal essay coherence.
The first two dimensions captureglobal coherence, and the latter two relate to local coher-ence: a) relatedness to the essay question (topic), b) re-latedness between discourse elements, c) intra-sententialquality, and d) sentence-relatedness within a discoursesegment.
Each dimension represents a different aspectof coherence.Essentially, the goal of the system is to be able to pre-dict whether a sentence in a discourse segment has highor low expressive quality with regard to a particular co-herence dimension.
We have deliberately developed anapproach to essay coherence that is comprised of multi-ple dimensions, so that an instructional application mayprovide appropriate feedback to student writers, based onthe system?s prediction of high or low for each dimen-sion.
For instance, sentences in the student?s thesis state-ment may have a strong relationship to the essay topic,but may have a number of serious grammatical errors thatmake it hard to follow.
For this student, we may want topoint out that on the one hand, the sentences in the thesisaddress the topic, but the thesis statement as a discoursesegment might be more clearly stated if the grammar er-rors were fixed.
By contrast, the sentences that comprisethe student?s thesis statement may be grammatically cor-rect, but only loosely related to the essay topic.
For thisstudent, we would also want the system to provide ap-propriate feedback to, so that the student could revise thethesis statement text appropriately.In earlier work, Foltz, Kintsch & Landauer (1998),and Wiemer-Hastings & Graesser (2000) have devel-oped systems that also examine coherence in studentwriting.
Their systems measure lexical relatedness be-tween text segments by using vector-based similaritybetween adjacent sentences.
This linear approach tosimilarity scoring is in line with the TextTiling scheme(Hearst and Plaunt, 1993; Hearst, 1997), which maybe used to identify the subtopic structure of a text.Miltsakaki and Kukich (2000) have also addressed the is-sue of establishing the coherence of student essays, usingthe Rough Shift element of Centering Theory.
Again, thisprevious work looks at the relatedness of adjacent textsegments, and does not explore global aspects of text co-herence.Hierarchical models of discourse have been applied tothe question of coherence (Mann and Thompson, 1986),but so far these have been more useful in language gen-eration than in determining how coherent a given text is,or in identifying the specific problem, such as the break-down of coherence in a document.Our approach differs in fundamental ways from thisearlier work that deals with student writing.
First, Foltzet al (1998), Wiemer-Hastings and Graesser (2000),and Miltsakaki and Kukich (2000) assume that text co-herence is linear.
They calculate the similarity betweenadjacent segments of text.
By contrast, our approachconsiders the discourse structure in the text, followingBurstein et al (2003b).
Our method considers sentenceswith regard to their discourse segments, and how the sen-tences relate to other text segments both inside (such asthe essay thesis) and outside (such as the essay topic) of adocument.
This allows us to identify cases in which theremay be a breakdown in coherence due to more global as-pects of essay-based discourse structure.
Second, previ-ous work has used Latent Semantic Analysis as a seman-tic similarity measure (Landauer and Dumais, 1997).
Wehave adapted another vector-based method of semanticrepresentation: Random Indexing (Kanerva et al, 2000;Sahlgren, 2001).
Another difference between our sys-tem and earlier systems is that we use essays manuallyannotated on the four coherence dimensions to train oursystem.The final system employs a hybrid approach to classifythe first two of the four coherence dimensions with a highor low quality rank.
For these dimensions, a support vec-tor machine is used to model features derived from Ran-dom Indexing and from essay-based discourse structureinformation.
A third local coherence dimension compo-nent is driven by rule-based heuristics.
A fourth dimen-sion related to coherence within a discourse segment can-not be classified due to a lack of data characterizing lowexpressive quality.
This is fully explained later in the pa-per.2 Protocol Development and HumanAnnotation2.1 Protocol DevelopmentThe development of this system required a corpus of hu-man annotated essay data for modeling purposes.
In theend, the goal is to have the system make judgments sim-ilar to those made by a human with regard to ranking thecoherence of an essay on four dimensions.
Therefore, wecreated a detailed protocol for annotating the expressivequality of essay-based discourse elements in essays withregard to four aspects related to global and local essaycoherence.
This protocol was designed for the followingpurposes:1.
To yield annotations that are useful for the purposeof providing students with feedback about the ex-pressive relatedness of discourse elements in theiressays, given four relatedness dimensions;2.
To permit human annotators to achieve high levelsof consistency during the annotation process;3.
To produce annotations that have the potential of be-ing derivable by computer programs through train-ing on corpora annotated by humans.2.1.1 Expressive Quality of Discourse Segments:Protocol DescriptionAccording to writing experts who collaborated in thiswork, the expressive relatedness of a sentence discourseelement may be characterized in terms of four dimen-sions: a) relationship to prompt (essay question topic),b) relationship to other discourse elements, c) relevancewith discourse segment, and d) errors in grammar, us-age, and mechanics.
For the sake of brevity, we refer tothese four dimensions as DimP (relatedness to prompt),DimT (typically, relatedness to thesis), DimS (related-ness within a discourse segment), and DimERR.The two annotators were required to label each sen-tence of an essay for expressive quality on the four di-mensions (above).
For the 989 essays used in this study,each sentence had already been manually annotated withthese discourse labels: background material, thesis, mainidea, supporting idea, and conclusion (Burstein et al,2003b).1 An assignment of high (1) or low (0) was givento each sentence, on the dimensions relevant to the dis-course element.
Not all dimensions apply to all discourseelements.
The protocol is extremely specific as to howannotators should label the expressive quality for eachsentence in a discourse element with regard to the fourdimensions.
In this paper, we provide a brief descriptionof the labeling protocol, so that the purpose of each di-mension is clear.Figure 1 shows a sample essay and prompt.
A hu-man judge has assigned a label to each sentence in theessay, resulting in the illustrated division into discoursesegments.
In addition, the figure indicates human annota-tors?
ratings for two of our coherence dimensions (DimPand DimT , discussed below).
By and large, the essayconsistently follows up on the ideas of the essay thesis,and so most sentences get a high relatedness score onDimT .
However, much of the essay fails to directly ad-dress the question posed in the essay prompt, and so manysentences are assigned low relatedness on DimP .Dimension 1: DimP (Relatedness to Prompt)The text of the discourse element and the prompt (textof the essay question) must be related.
Specifically, thethesis statement, main ideas, and conclusion statementshould all contain text that is strongly related to the essaytopic.
If this relationship does not exist, this is perhapsevidence that the student has written an off-topic essay.For this dimension, a high rank is assigned to each sen-tence from background material, thesis, main idea andconclusion statement that is related to the prompt text;otherwise a low rank is assigned.1The annotated data from the Burstein et al (2003b) studywere used to develop a commercial application that automati-cally assigns these discourse labels to student essays.Discourse Sentence DimP DimTSegmentPrompt Images of beauty?both male and female?are promoted in magazines, in movies, onbillboards, and on television.
Explain the extent to which you think these images canbe beneficial or harmful.BackgroundA lot of people really care about how they look or how other people look.
Low HighA lot of people like reading magazines or watch t.v about how you can fix your looks ifyou don?t like the way your looks are.
High HighThesisPeople that care about how they look is because they have problems at home, their parentsdon?t pay attention to them or even that they have a high self-steem which that is not good.
Low N/AA lot of people get to the extent of killing themselfs just because they?re not happy withthere looks.
Low N/ASupport Many people go thru make-overs to experiment how they will look but, some people stilldon?t like themself.
N/A HighMain PointThe people that don?t like themselfs need some helps and they probably feel like that be-cause they have told them oh!
your ugly , you look like Blank!
or maybe a guy never ask aher out.Low LowSupportIn case of a guy probably the same comments but he won?t dare to ask a girl out becausehe feels that the girl is going to say no because of the way he looks.
N/A HighThings like this make people don?t like each other.
N/A HighConclusion I suggest that a those people out here that are not happy with their looks get some help.
Low HighTheirs alot of programs that you can get help.
Low LowFigure 1: Student essay with discourse segments and two coherence dimensions as annotated by human judgeDimension 2: DimT (Relatedness to Thesis)The relationship between a discourse element andother discourse elements in the text governs the globalcoherence of the essay text.
For a text to hold together,certain discourse elements must be related or the text willappear choppy and will be difficult to follow.
Specifi-cally, a high rank is assigned to each sentence in the back-ground material, main ideas and conclusion that is relatedto the thesis, and supporting idea sentences that relate tothe relevant main idea.
A conclusion sentence may alsobe given a high rank if it is related to a main idea or back-ground information.
Low ranks are assigned to sentencesthat do not have these relationships.Dimension 3: DimS (Relatedness within Segment)This dimension indicates the cohesiveness of the mul-tiple sentences in a discourse segment of a text.
Thisdimension distinguishes a text segment that may go offtask within a discourse segment.
For this dimension, ahigh rank was assigned to each sentence in a discoursesegment that related to at least one other sentence in thesegment; otherwise the sentence received a low rank.
Ifthe discourse segment contained only one sentence, thenthe DimT label was assigned as the default.Dimension 4: DimERR (Technical Errors)Dimension 4 measures a sentence?s relatedness of ex-pression with regard to grammar, mechanics and wordusage.
More specifically, a sentence is considered to below on this dimension if it contains frequent patterns oferror, defined as follows: (a) contains 2 errors in gram-mar, word usage or mechanics (i.e., spelling, capitaliza-tion or punctuation), (b) is an incomplete sentence, or (c)is a run-on sentence (i.e., 4 or more independent clauseswithin a sentence).2.2 Topics, Human Annotation, and HumanAgreement2.2.1 Topics & Writing GenreEssays written to two genres were used: five of the top-ics were persuasive, and one was expository.
Persuasivewriting requires the reader to state an opinion on a par-ticular topic, support the stated opinion, and convince thereader that the perspective is valid and well-supported.An expository topic requires the writer only to state anopinion on a topic.
This typically elicits more personaland descriptive writing.
Four of the five sets of persua-sive essay responses were written by college freshman,and the fifth by 12th graders.
The set of expository re-sponses were also written by 12th graders.2.2.2 Human AnnotationTwo human judges participated in this study.
Thejudges were instructed to assign relevant dimension la-bels to each sentence.
Pre-training of the judges was doneusing a set of approximately 50 essays across the six top-ics in the study.
During this phase, the authors and thejudges discussed and labeled the essays together.
Duringthe next training phase, the judges labeled a total of 292essays across six topics.
They labeled the identical set ofessays, and were allowed to discuss their decisions.
In thenext annotation phase, the judges did not discuss their an-notations.
In this post-training phase (annotation phase),each judge labeled an average of about 278 unique es-says for each of four prompts (556 essays together).
Eachjudge also labeled an additional set of 141 essays that wasoverlapping.
So, about 20 percent of the data annotatedby each judge in the annotation phase was overlapping,Agreement ?DimP (N=779) 99% .99DimT (N=1890) 100% .99DimS (N=2119) 100% .99DimERR (N=2170) 99% .98Table 1: Annotator agreement across coherencedimensions?data from annotation phaseand 80 percent was unique.
The 20 percent is used to ob-tain human agreement.2 During both the training and an-notation phases, Kappa statistics were run on their judg-ments regularly, and if the Kappa for any particular cate-gory fell below 0.8, then the judges were asked to reviewthe protocol until their agreement was acceptable.
At theend of the annotation phase, we had a total of 989 labeledessays: 292 (training phase) + 278 ?
2 (unique essaysfrom annotator 1 + annotator 2, annotation phase) + 141(overlapping set, annotation phase).Human Judge AgreementIt is critical that the annotation process yields agree-ment that is high enough between human judges, suchthat it suggests that people can agree on how to categorizethe discourse elements.
As is stated in the above section,during the training of the judges for this study, Kappastatistics were computed on a regular basis.
Kappa be-tween the judges for each category had to be maintainedat least 0.8, since this is believed to represent strongagreement (Krippendorff, 1980).
In Table 1 we reporthuman agreement for overlapping data from the four top-ics on all four dimensions.
Clearly, the level of humanagreement is quite high across all four coherence dimen-sions.
In addition, if we look at kappas of sentences basedon discourse category, no kappa falls below 0.9.3 MethodOur final system uses a hybrid approach to label three ofthe four coherence dimensions.
For DimP and DimT ,assigning coherence judgments to sentences in an essayproceeds in three stages 1) identifying the discourse labelassociated with each sentence in an essay, 2) computingfeatures that quantify the semantic similarity between dif-ferent discourse segments of the essay, and 3) applying aclassifier to make a coherence judgment on a dimension.Consistent with the human annotated data, a coherencejudgment on any dimension is either ?high?
or ?low.?
Themethod for DimERR is rule-based, and is discussed later.3.1 Discourse element feature identificationAs noted earlier, the two human judges in this study anno-tated the four coherence dimensions according to the hu-2For the annotation phase, we were unable to collect datafor two essay prompts because of our annotators?
availability.This means that we only have inter-annotator agreement statis-tics on 4 prompts, although some data from all six prompts wasavailable for training and testing our models (with the extra twoprompts being represented in the training phase of annotation).man discourse label assignments.
Accordingly, we alsoused the human assigned discourse labels as features forpredicting coherence judgments.
In a deployed system,however, we would use discourse element labels gener-ated from Criterion?s discourse analysis system (Bursteinet al, 2003b).
Further evaluation is, of course, necessaryin order to determine the effect of using these automat-ically assigned labels in place of the gold standard dis-course labels.3.2 Semantic similarity featuresGiven the partition of an essay into discourse segments,we then derive a set of features from the essay in orderto predict how closely related each sentence is to variousimportant text segments, such as the essay topic, and dis-course elements, such as thesis statement.
As describedin Section 4, the features that are most useful for clas-sifying sentences according to coherence are semanticsimilarity features derived from Random Indexing (Kan-erva et al, 2000; Sahlgren, 2001).
Random Indexing isa vector-based semantic representation system similar toLatent Semantic Analysis.
Our Random Indexing (RI)semantic space is trained on about 30 million words ofnewswire text.When we extract a feature such as ?RI similarity toprompt?
for a sentence, this essentially measures to whatextent the sentence contains terms in the same semanticdomain as compared to those found in the prompt.
Withinany discourse segment, any semantic information that isword-order dependent is lost.3.3 Support vector classificationFinally, for each sentence in the essay we use the fea-tures derived from the essay to make a determination asto whether it meets our criteria for coherence in thesedimensions (DimP and DimT ).
To make this determi-nation, we use a support vector machine (SVM) classi-fier (Vapnik, 1995; Christianini and Shawe-Taylor, 2000).Specifically, we use an SVM with a radial basis functionkernel, which exhibited good performance on a subset ofabout 30 essays from the pre-training data.4 ResultsIn each of the experiments below, the results are re-ported for the entire set of 989 essays annotated for thisproject.
We performed ten-fold cross-validation, trainingour SVM classifier on 910 of the data at a time, and testingon the remaining 110 .
We report the results on the cross-validation set for all runs combined.For each dimension, we also report the performanceof a simple baseline measure, which assumes that all ofour essay coherence criteria are satisfied.
That is, ourbaseline assigns category 1 (high relevance) to everysentence, on every dimension.These essays were written in response to six differentprompts, and had an average (human-assigned) score ofScore DimP DimT DimS DimERR1?2 64.1% 71.2% 94.8% 61.1%5?6 72.0% 70.9% 97.2% 92.9%Table 2: Baseline performance on each coherence dimen-sion, broken down by essay score point4.0 on a six-point scale.
Therefore, a priori, it seems pos-sible that we could build a better baseline model by con-ditioning its predictions on the overall score of the essay(assigning 1?s to sentences from better-scoring essays,and 0?s to sentences from lower-scoring essays).
How-ever, the coherence requirements of each of our dimen-sions are usually met even in the lowest-scoring essays,as shown in Table 2, which lists the percentage of sen-tences in different essay score ranges which our humanannotators assigned category 1.
Looking at the highestand lowest score points on our six-point scale, it is clearthat higher-scoring essays do tend to have fewer problemswith coherence, but this effect is not overwhelming.
(Thelargest gap between the highest- and lowest-scoring es-says is on DimERR, which deals with errors in grammar,usage, and mechanics.
)4.1 DimPAccording to the protocol, there are four discourse ele-ments for which DimP , the degree of relatedness to theessay prompt, is relevant: Background, Conclusion, MainPoint, and Thesis.
The Supporting Idea category of sen-tence is not required to be related to the prompt, becauseit may express an elaboration of one of the main points ofthe essay, and has a more tenuous and mediated logicalconnection to the essay prompt text.The features which we provide to the SVM for predict-ing a sentence?s relatedness to the prompt are:1.
The RI similarity score of the target sentence withthe entire essay prompt,2.
The maximum RI similarity score of the target sen-tence with any sentence in the essay prompt,3.
The RI similarity score of the target sentence withthe required task sentence (a designated portion ofthe prompt text which contains an explicit directiveto the student to write about a specific topic),4.
The RI similarity score of the target sentence withthe entire thesis of the essay,5.
The maximum RI similarity score of the target sen-tence with any sentence in the thesis,6.
The maximum RI similarity score of the target sen-tence with any sentence in the preceding discoursechunk,7.
The number of sentences in the current chunk,8.
The offset of the target sentence (sentence number)from the beginning of the current discourse chunk,9.
The number of sentences in the current chunk whosesimilarity with the prompt is greater than .2,10.
The number of sentences in the current chunk whosesimilarity with the required task sentence is greaterthan .2,11.
The number of sentences in the current chunk whosesimilarity with the essay thesis is greater than .2,12.
The number of sentences in the current chunk whosesimilarity with the prompt is greater than .4,13.
The number of sentences in the current chunk whosesimilarity with the required task sentence is greaterthan .4,14.
The number of sentences in the current chunk whosesimilarity with the essay thesis is greater than .4,15.
The length of the target sentence in words,16.
A Boolean feature indicating whether the target sen-tence contains a transition word, such as ?however?,or ?although?,17.
A Boolean feature indicating whether the target sen-tence contains an anaphoric element, and18.
The category of the current chunk.
(This is encodedas five Boolean features: one bit for each of ?Back-ground?, ?Conclusion?, ?Main Point?, ?SupportingIdea?, and ?Thesis?.
)In calculating features 2, 5, and 6, we use the maximumsimilarity score of the sentence with any other sentence inthe relevant discourse segment, rather than simply usingthe similarity score of the sentence with the entire textchunk.
We add this feature based on the intuition that fora sentence to be relevant to another discourse segment, itneed only be connected to some part of that segment.It is perhaps surprising that we include features whichmeasure the degree of similarity between the sentenceand the thesis, since we are trying to predict its related-ness to the prompt, rather than the thesis.
However, thereare two reasons we believe this is fruitful.
First, since weare dealing with a relatively small amount of text, com-paring a single sentence to a short essay prompt, lookingat the thesis as well helps to overcome data sparsity is-sues.
Second, it may be that the relevance of the currentsentence to the prompt is mediated by the student?s thesisstatement.
For example, the prompt may ask the studentto take a position on some topic.
They may state this po-sition in the thesis, and provide an example to support itas one of their Main Points.
In such a case, the examplewould be more clearly linked to the Thesis, but this wouldsuffice for it to be related to the prompt.Considering the similarity scores of sentences in thecurrent discourse segment is also, in part, an attempt toovercome data sparsity issues, but is also motivated bythe idea that it may be an entire discourse segment whichcan properly be said to be (ir)relevant to the essay prompt.The sentence length and transition word features donot directly reflect the relatedness of a sentence to theprompt, but they are likely to be useful correlates.Finally, the feature (#17) indicating the presence ofa pronoun is to help the system deal with cases inwhich a sentence contains very few content words, butis still linked to other material in the essay by means ofanaphoric elements, such as ?This is shown by my argu-ment.?
In such as case, the sentence would normally geta low similarity score with the prompt (and other parts ofthe essay), but the information that it contains a pronounmight still allow the system to classify it correctly.Table 3 shows results using the baseline algorithm toclassify sentences according to their relatedness to theprompt.
Table 4 presents the results using the SVM clas-sifier.
We provide precision, recall, and f-measure for theassignment of the labels 1 and 0, and an overall accuracymeasure in the far right column.
(The accuracy measureis the value for precision and recall when 1 and 0 ranksare collapsed.
Precision and recall will be the same, sincethe number of labels assigned by the model is equal to thenumber of labels in the target assignment.
)The SVM model outperforms the baseline on everysubcategory, with the largest gains on Background sen-tences, most of which are, in fact, unrelated to the promptaccording to our human judges.
This low baseline resulton Background sentences could indicate that many stu-dents have a problem with providing unnecessary and ir-relevant prefaces to the important points in their essays.Note that the trained SVM has around .9 recall on theclass of sentences which according to our human annota-tors have high relevance to the prompt.
This means thatour system is less likely to incorrectly assign a low rankto a sentence that is high.
So, the system will tend to erron the side of the student, which is a preferable trade-off.In part, this is due to the nature of the semantic similaritymeasure we are using, which does not take word orderinto account.
While RI does allow us to capture a richermeaning component than simply matching words whichco-occur in the target sentence and prompt, it still doesnot encompass all that goes into determining whether asentence ?relates?
to another chunk of text.
Students of-ten write something which bears a loose topical connec-tion with the essay prompt, but does not directly addressthe question.
This sort of problem is hard to address witha tool such as LSA or RI; the vocabulary of the sentenceon its own will not provide a clue to the sentence?s failureto address the task.4.2 DimTThe annotation protocol states that these four discourseelements come into play for DimT : Background, Con-clusion, Main Point, and Supporting Idea.
Because thisdimension indicates the degree of relatedness to the the-sis of the essay (and also other discourse segments in thecase of Supporting Idea and Conclusion sentences; seeSection 2.1.1 above), we do not consider thesis sentenceswith regard to this aspect of coherence.The features which we provide to the SVM for pre-dicting whether or not a given sentence is related to thethesis are almost the same ones used for DimP .
The onlydifference is that we omit features #12 and #13 in ourmodel of DimT .
These are the features which evaluatehow many sentences in the current chunk have a simi-larity score with the prompt and required task sentencegreater than 0.4.
While DimP is to some degree sensitiveto the similarity of a sentence to the thesis, and DimT canlikewise benefit from the information about a sentence?ssimilarity to the prompt, it seems that the latter link is lessimportant, so a single cutoff suffices for this model.Tables 5?6 present the results for our SVM model andfor a baseline which assigns all sentences ?high?
rele-vance.
The improvements on DimT are smaller than theones reported for DimP , but we still record an overallgain of four percentage points in accuracy.
Only on con-clusion sentences were we unable to produce an improve-ment over the baseline; we need to investigate this further.Again, the system achieves high recall on sentenceswith high relatedness.
It outperforms the baseline by cor-rectly identifying a modest percentage of the sentenceslabeled as having low relatedness with the thesis.4.3 DimSDimS , which concerns whether the target sentence re-lates to another sentence within the same discourse seg-ment, seems another good candidate for applying our se-mantic similarity score to the task of establishing coher-ence.
At present, however we have not made substan-tial progress on this task.
The baselines for DimS aresubstantially higher than those for dimensions DimP andDimT ?
98.1% of all sentences in our data were anno-tated as ?highly related?
with respect to this dimension.This indicates that it is relatively rare to find a sentencewhich is not related to anything in the same discoursesegment.
This makes our task, to characterize those sen-tences which are not related to the discourse segment,much more difficult, since there are so few examples ofsentences with low-ranking coherence.4.4 DimERRDimERR is clearly a different kind of problem.
Here, weare looking for clarity of expression, or coherence withina sentence.
We base this solely on technical correctness.We are able to automatically assign high and low ranks toessay sentences using a set of rules based on the numberof grammar, usage and mechanics errors.
The rules usedfor DimERR are as follows: a) assign a low label if thesentence is a fragment, if the sentence contains 2 or moregrammar, usage, and mechanics errors, or if the sentenceis a run-on, b) assign a high label if no criteria in (a) apply.Criterion?s discourse analysis system also providesan essay score with e-rater?, and qualitative feedbackabout grammar, usage, mechanics, and style (LeacockHigh Low TotalPrecision Recall F-measure Precision Recall F-measure AccuracyBackground (N = 1077) 0.486 1.000 0.654 0.000 0.000 0.000 0.486Conclusion (N = 1830) 0.757 1.000 0.862 0.000 0.000 0.000 0.757Main Point (N = 1566) 0.663 1.000 0.797 0.000 0.000 0.000 0.663Thesis (N = 1899) 0.712 1.000 0.832 0.000 0.000 0.000 0.712All sentence types (N = 6372) 0.675 1.000 0.806 0.000 0.000 0.000 0.675Table 3: Baseline performance on DimPHigh Low TotalPrecision Recall F-measure Precision Recall F-measure AccuracyBackground (N = 1077) 0.714 0.702 0.708 0.723 0.735 0.729 0.719Conclusion (N = 1830) 0.784 0.959 0.863 0.578 0.175 0.269 0.768Main Point (N = 1566) 0.729 0.888 0.801 0.616 0.352 0.448 0.708Thesis (N = 1899) 0.771 0.929 0.843 0.644 0.318 0.426 0.753All sentence types (N = 6372) 0.759 0.901 0.824 0.665 0.407 0.505 0.740Table 4: SVM performance on DimPHigh Low TotalPrecision Recall F-measure Precision Recall F-measure AccuracyBackground (N = 1060) 0.793 1.000 0.885 0.000 0.000 0.000 0.793Conclusion (N = 1829) 0.834 1.000 0.909 0.000 0.000 0.000 0.834Main Point (N = 1556) 0.742 1.000 0.852 0.000 0.000 0.000 0.742Support (N = 10332) 0.664 1.000 0.798 0.000 0.000 0.000 0.664All sentence types (N = 14777) 0.702 1.000 0.825 0.000 0.000 0.000 0.702Table 5: Baseline performance on DimTHigh Low TotalPrecision Recall F-measure Precision Recall F-measure AccuracyBackground (N = 1060) 0.856 0.980 0.914 0.827 0.368 0.509 0.853Conclusion (N = 1829) 0.834 1.000 0.910 0.000 0.000 0.000 0.834Main Point (N = 1556) 0.776 0.997 0.873 0.958 0.172 0.292 0.785Support (N = 10332) 0.709 0.945 0.810 0.684 0.237 0.352 0.706All sentence types (N = 14777) 0.744 0.962 0.839 0.709 0.221 0.337 0.741Table 6: SVM performance on DimTand Chodorow, 2000; Burstein et al, 2003a).
We caneasily use Criterion?s outputs about grammar, usage, andmechanics errors to assign high and low ranks to essaysentences, using the rules described in the previous sec-tion.The performance of the module that does the DimERRassignments is in Table 7.
We used half of the 292 essaysfrom the training phase of annotation for development,and the remaining data from the training and post-trainingphases of annotation for cross-validation.
Results are re-ported for the cross-validation set.
Text labeled as titles,or opening or closing salutations, are not included in theresults.
The baselines were computed by assigning allsentences a high rank label.
The baseline is high; how-ever, the algorithm outperforms the baseline.5 Discussion and ConclusionsThere were multiple goals in this work.
We wanted to in-troduce a concept of essay coherence comprising multi-ple aspects, and investigate what linguistic features driveeach aspect in student essay writing.
Further, we wantedSentence N Precision Recall F-measureBaselineHigh 11789 0.83 1.00 0.91Low 2351 0.00 0.00 0.00Overall 14140 0.83 0.83 0.83AlgorithmHigh 11789 0.88 0.96 0.92Low 2351 0.63 0.34 0.44Overall 14140 0.86 0.86 0.86Table 7: Performance on DimERRto build a system to automatically evaluate these multipleaspects of coherence, so that appropriate feedback can beprovided through a writing instruction application.To accomplish these goals, we have worked with writ-ing experts to develop a comprehensive protocol that de-tails how coherence in writing can be evaluated, eithermanually or automatically.
Using this protocol, humanannotators labeled a corpus of student essays, using thecoherence dimensions.
These annotations built on a pre-vious set of annotations for these data, whereby discourseelement labels were assigned.
The result is a richly anno-tated data set with information about discourse elements,as well as their coherence in the context of the discoursestructure.
Using this data set, we were able to learn whatlinguistic features can be used to evaluate various aspectsof coherence in student writing.
We then developed aprototype system that ranks global and local aspects ofcoherence in an essay.
This capability shows promise inranking three aspects of coherence in essays: a) relation-ship to essay topic, b) relationship between discourse ele-ments, and c) intra-sentential technical quality.
More lowranking data on a fourth dimension, coherence within adiscourse segment, needs to be identified and annotatedbefore this dimension can be modeled.The approach used is innovative, since it moves beyondearlier methods of evaluating coherence in student writ-ing that capture only local information between adjacentsentences.
Two methods are used to model the aspectsof coherence handled by the system.
For the two globalcoherence dimensions, DimP and DimT , a support vec-tor machine provides a coherence ranking of sentencesbased on features related to essay-based discourse infor-mation, and semantic similarity values derived from theRI algorithm.
Using this classification method, we areable to rank the expressive quality of sentences in essay-based discourse segments, with regard to relatedness tothe text of the prompt, and also as they relate to the thesisstatement.
With regard to the local coherence dimension,DimERR, we use a rule-based heuristic to rank intra-sentential quality.
This addresses the issue of sentences inessays that have serious grammatical problems that mayinterfere with a reader?s comprehension.
We take advan-tage of Criterion?s identification of grammar, usage, andmechanics errors to design the rules for ranking this localcoherence dimension.We hope that in further investigation of this richly an-notated data set, we will be able to build on the currentprototype and develop a full-scale writing instruction ca-pability that provides feedback on the coherence dimen-sions described in this paper.AcknowledgementsWe would like to thank Irma Lorenz and Shauna Cooperfor advice on protocol development and for the annota-tion work, and Martin Chodorow for discussions aboutRandom Indexing.
We thank the anonymous reviewersfor their helpful feedback.Any opinions expressed here are those of the authorsand not necessarily of the Educational Testing Service.ReferencesJill Burstein, Martin Chodorow, and Claudia Leacock.2003a.
CriterionSM: Online essay evaluation: An ap-plication for automated evaluation of student essays.In Proceedings of the Fifteenth Annual Conference onInnovative Applications of Artificial Intelligence, Aca-pulco, Mexico.Jill Burstein, Daniel Marcu, and Kevin Knight.
2003b.Finding the WRITE stuff: Automatic identification ofdiscourse structure in student essays.
IEEE Trans-actions on Intelligent Systems: Special Issue on Ad-vances in Natural Language Processing, 181:32?39.Nello Christianini and John Shawe-Taylor.
2000.
Sup-port Vector Machines and other Kernel-based Learn-ing Methods.
Cambridge University Press, Cam-bridge, UK.Peter Foltz, Walter Kintsch, and Thomas K. Landauer.1998.
The measurement of textual coherence withLatent Semantic Analysis.
Discourse Processes,25(2&3):285?307.Marti A. Hearst and Christian Plaunt.
1993.
Subtopicstructuring for full-length document access.
In Pro-ceedings of ACM SIGIR, pages 59?68.Marti A. Hearst.
1997.
TextTiling: Segmenting textinto multi-paragraph subtopic passages.
Computa-tional Linguistics, 23(1):33?64.P.
Kanerva, J. Kristoferson, and A. Holst.
2000.
Randomindexing of text samples for Latent Semantic Analysis.In L. R. Gleitman and A. K. Josh, editors, Proc.
22ndAnnual Conference of the Cognitive Science Society.Klaus Krippendorff.
1980.
Content Analysis: An Intro-duction to Its Methodology.
Sage Publications.Thomas K. Landauer and Susan T. Dumais.
1997.
A so-lution to Plato?s problem: The Latent Semantic Analy-sis theory of acquisition, induction, and representationof knowledge.
Psychological Review, 104:211?240.Claudia Leacock and Martin Chodorow.
2000.
An unsu-pervised method for detecting grammatical errors.
InProceedings of NAACL 2000, pages 140?147.William Mann and Sandra Thompson.
1986.
Relationalprocesses in discourse.
Discourse Processes, 9:57?90.Eleni Miltsakaki and Karen Kukich.
2000.
Automatedevaluation of coherence in student essays.
In Proceed-ings of LREC 2000, Athens, Greece.Magnus Sahlgren.
2001.
Vector based semantic analy-sis: Representing word meanings based on random la-bels.
In Proceedings of the ESSLLI 2001 Workshop onSemantic Knowledge Acquisition and Categorisation.Helsinki, Finland.Vladimir Vapnik.
1995.
The Nature of Statistical Learn-ing Theory.
Springer Verlag, New York.Peter Wiemer-Hastings and Arthur Graesser.
2000.Select-a-Kibitzer: A computer tool that gives mean-ingful feedback on student compositions.
InteractiveLearning Environments, 8(2):149?169.
