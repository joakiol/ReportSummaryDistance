Proceedings of NAACL-HLT 2013, pages 391?400,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsParser lexicalisation through self-learningMarek ReiComputer LabratoryUniversity of CambridgeUnited KingdomMarek.Rei@cl.cam.ac.ukTed BriscoeComputer LaboratoryUniversity of CambridgeUnited KingdomTed.Briscoe@cl.cam.ac.ukAbstractWe describe a new self-learning frameworkfor parser lexicalisation that requires only aplain-text corpus of in-domain text.
Themethod first creates augmented versions of de-pendency graphs by applying a series of mod-ifications designed to directly capture higher-order lexical path dependencies.
Scores areassigned to each edge in the graph using statis-tics from an automatically parsed backgroundcorpus.
As bilexical dependencies are sparse,a novel directed distributional word similar-ity measure is used to smooth edge score es-timates.
Edge scores are then combined intograph scores and used for reranking the top-n analyses found by the unlexicalised parser.The approach achieves significant improve-ments on WSJ and biomedical text over theunlexicalised baseline parser, which is origi-nally trained on a subset of the Brown corpus.1 IntroductionMost parsers exploit supervised machine learningmethods and a syntactically annotated dataset (i.e.treebank), incorporating a wide range of features inthe training process to deliver competitive perfor-mance.
The use of lexically-conditioned features,such as relations between lemmas or word forms,is often critical when choosing the correct syntac-tic analysis in ambiguous contexts.
However, util-ising such features leads the parser to learn infor-mation that is often specific to the domain and/orgenre of the training data.
Several experiments havedemonstrated that many lexical features learnt inone domain provide little if any benefit when pars-ing text from different domains and genres (Sekine,1997; Gildea, 2001).
Furthermore, manual creationof in-domain treebanks is an expensive and time-consuming process, which can only be performed byexperts with sufficient linguistic and domain knowl-edge.In contrast, unlexicalised parsers avoid using lex-ical information and select a syntactic analysis us-ing only more general features, such as POS tags.While they cannot be expected to achieve optimalperformance when trained and tested in a single do-main, unlexicalised parsers can be surprisingly com-petitive with their lexicalised counterparts (Kleinand Manning, 2003; Petrov et al 2006).
In thiswork, instead of trying to adapt a lexicalised parserto new domains, we explore how bilexical featurescan be integrated effectively with any unlexicalisedparser.
As our novel self-learning framework re-quires only a large unannotated corpus, lexical fea-tures can be easily tuned to a specific domain orgenre by selecting a suitable dataset.
In addition,we describe a graph expansion process that capturesselected bilexical relations which improve perfor-mance but would otherwise require sparse higher-order dependency path feature types in most ap-proaches to dependency parsing.
As many bilex-ical features will still be sparse, we also developan approach to estimating confidence scores for de-pendency relations using a directional distributionalword similarity measure.
The final framework in-tegrates easily with any unlexicalised (and thereforepotentially less domain/genre-biased) parser capableof returning ranked dependency analyses.3912 BackgroundWe hypothesise that a large corpus will often containexamples of dependency relations in non-ambiguouscontexts, and these will mostly be correctly parsedby an unlexicalised parser.
Lexical statistics derivedfrom the corpus can then be used to select the cor-rect parse in a more difficult context.
For example,consider the following sentences:(1) a.
Government projects interest researchersb.
Government raises interest ratesc.
Government projects receive fundingd.
Interest rates are increasingNoun-verb ambiguities over projects and interestmight erroneously result in the unlexicalised parserreturning similar dependency graphs for both a andb.
However, sentences c and d contain less ambigu-ous instances of the same phrases and can provideclues to correctly parsing the first two examples.
Ina large in-domain corpus we are likely to find morecases of researchers being the object for interest andfewer cases where it is the object of project.
In con-trast, rates is more likely to have interest as a mod-ifier than as a head in an object relation.
Exploitingthis lexical information, we can assign the correctderivation to each of the more ambiguous sentences.Similar intuitions have been used to motivate theacquisition of bilexical features from backgroundcorpora for improving parser accuracy.
However,previous work has focused on including these statis-tics as auxiliary features during supervised training.For example, van Noord (2007) incorporated bilex-ical preferences as features via self-training to im-prove the Alpino parser for Dutch.
Plank and vanNoord (2008) investigated the application of aux-iliary distributions for domain adaptation.
Theyincorporated information from both in-domain andout-of-domain sources into their maximum entropymodel and found that the out-of-domain auxiliarydistributions did not contribute to parsing accuracyin the target domain.
Zhou et al(2011) extracted n-gram counts from Google queries and a large corpusto improve the MSTParser.
In contrast to previouswork, we refer to our approach as self-learning be-cause it differs from self-training by utilising statis-tics found using an initial parse ranking model tocreate a separate unsupervised reranking compo-nent, without retraining the baseline unlexicalisedmodel.We formulate our self-learning framework as areranking process that assigns new scores to the top-n ranked analyses found by the original parser.
Parsereranking has been successfully used in previouswork as a method of including a wider range of fea-tures to rescore a smaller selection of highly-rankedcandidate parses.
Collins (2000) was one of the firstto propose supervised reranking as an additional stepto increase parser accuracy and achieved 1.55% ac-curacy improvement for his parser.
Charniak andJohnson (2005) utilise a discriminative reranker andshow a 1.3% improvement for the Charniak parser.McClosky et al(2006) extend their work by addingnew features and further increase the performanceby 0.3%.
Ng et al(2010) implemented a dis-criminative maximum entropy reranker for the C&Cparser and showed a 0.23% improvement over thebaseline.
Bansal and Klein (2011) discriminativelyrerank derivations from the Berkeley unlexicalisedparser (Petrov et al 2006) demonstrating that lex-ical features derived from the Google n-gram cor-pus improve accuracy even when used in conjunc-tion with other reranking features.
They have alltreated reranking as a supervised task and trained adiscriminative classifier using parse tree features andannotated in-domain data.
In contrast, our rerankeronly uses statistics from an unlabelled source andrequires no manual annotation or training of thereranking component.
As we utilise an unlexicalisedparser, our baseline performance on WSJ text islower compared to some fully-lexicalised parsers.However, an unlexicalised parser is also likely to beless biased to domains or genres manifested in thetext used to train its original ranking model.
Thismay allow the reranker to adapt it to a new domainand/or genre more effectively.3 Reordering dependency graphsFor our experiments, we make use of the unlexi-calised RASP parser (Briscoe et al 2006) as thebaseline system.
For every sentence s the parserreturns a list of dependency graphs Gs, ranked bythe log probability of the associated derivation in thestructural ranking model.
Our goal is to reorder this392list to improve ranking accuracy and, most impor-tantly, to improve the quality of the highest-rankeddependency graph.
This is done by assigning a con-fidence score to every graph gs,r ?
Gs where r is therank of gs for sentence s. The method treats eachsentence independently, therefore we can omit thesentence identifiers and refer to gs,r as gr.We first calculate confidence scores for all the in-dividual edges and then combine them into an over-all score for the dependency graph.
In the followingsections, we describe a series of graph modificationsthat incorporates selected higher-order dependencypath relations, without introducing unwanted noiseor complexity into the reranker.
Next, we outlinedifferent approaches for calculating and smoothingthe confidence scores for bilexical relations.
Finally,we describe methods for combining together thesescores and calculating an overall score for a depen-dency graph.
We make publically available all thecode developed for performing these steps in theparse reranking system.13.1 Graph modificationsFor every dependency graph gr the graph expan-sion procedure creates a modified representation g?rwhich contains a wider range of bilexical relations.The motivation for this graph expansion step is sim-ilar to that motivating the move from first-order tohigher-order dependency path feature types (e.g.,Carreras (2007)).
However, compared to using allnth-order paths, these rules are chosen to maximisethe utility and minimise the sparsity of the result-ing bilexical features.
In addition, the cascading na-ture of the expansion steps means in some cases theexpansion captures useful 3rd and 4th order depen-dencies.
Similar approaches to graph modificationshave been successfully used for several NLP tasks(van Noord, 2007; Arora et al 2010).For any edge e we also use notation (rel, w1, w2),referring to an edge from w1 to w2 with the labelrel.
We perform the following modifications on ev-ery dependency graph:1.
Normalising lemmas.
All lemmas are convertedto lowercase.
Numerical lemmas are replacedwith more generic tags to reduce sparsity.1www.marekrei.com/projects/lexicalisation2.
Bypassing conjunctions.
For every edge pair(rel1, w1, w2) and (rel2, w2, w3) where w2 istagged as a conjunction, we create an additionaledge (rel1, w1, w3).
This bypasses the conjunc-tion node and creates direct edges between thehead and dependents of the conjunctive lemma.3.
Bypassing prepositions.
For every edge pair(rel1, w1, w2) and (rel2, w2, w3) where w2 istagged as a preposition, we create an additionaledge (rel3, w1, w3).
rel3 = rel1 +?
prep?, where?
prep?
is added as a marker to indicate that therelation originally contained a preposition.4.
Bypassing verbs.
For every edge pair(rel1, w1, w2) and (rel2, w1, w3) where w1 istagged as a verb, w2 and w3 are both taggedas open-class lemmas, rel1 starts with a subjectrelation, and rel2 starts with an object relation,we create an additional edge (rel3, w2, w3) whererel3 = rel1 + ?-?
+ rel2.
This creates an additionaledge between the subject and the object, with thenew edge label containing both of the original la-bels.5.
Duplicating nodes.
For every existing node inthe graph, containing the lemma and POS foreach token (lemma pos), we create a parallel nodewithout the POS information (lemma).
Then, foreach existing edge, we create three correspond-ing edges, interconnecting the parallel nodes toeach other and the original graph.
This allows thereranker to exploit both specific and more genericinstantiations of each lemma.Figure 1 illustrates the graph modification pro-cess.
It is important to note that each of these mod-ifications gets applied in the order that they are de-scribed above.
For example, when creating edges forbypassing verbs, the new edges for prepositions andconjunctions have already been created and also par-ticipate in this step.
We performed ablation tests onthe development data and verified that each of thesemodifications contributes positively to the final per-formance.3.2 Edge scoring methodsWe start the scoring process by assigning individualconfidence scores to every bilexical relation in the393italian pm meet with cabinet member and senior officialJJ NP1 VVZ IW NN1 NN2 CC JJ NN2ncmod ncsubj iobjdobjncmod conjconjncmodncsubj-iobj prepncsubj-iobj prepiobj prepiobj prepiobj prepdobjdobjFigure 1: Modified graph for the sentence ?Italian PM meets with Cabinet members and senior officials?
after steps1-4.
Edges above the text are created by the parser, edges below the text are automatically created using the operationsdescribed in Section 3.1.
The 5th step will create 9 new nodes and 45 additional edges (not shown).modified graph.
In this section we give an overviewof some possible strategies for performing this task.The parser returns a ranked list of graphs and thiscan be used to derive an edge score without requir-ing any additional information.
We estimate that thelikelihood of a parse being the best possible parse fora given sentence is roughly inversely proportionalto the rank that it is assigned by the parser.
Thesevalues can be summed for all graphs that contain aspecific edge, normalised to approximate a proba-bility.
We then calculate the score for edge e as theReciprocal Edge Score (RES) ?
the probability of ebelonging to the best possible parse:RES(e) =?Rr=1[ 1r ?
contains(g?r, e)]?Rr=11rwhereR is the total number of parses for a sentence,and contains(g?r, e) returns 1 if graph g?r containsedge e, and 0 otherwise.
The value is normalised,so that an edge which is found in all parses will havea score of 1.0, but occurrences at higher ranks willhave a considerably larger contribution.The score of an edge can also be assigned by es-timating the probability of that edge using a parsedreference corpus.
van Noord (2007) improved over-all parsing performance in a supervised self-trainingframework using feature weights based on pointwisemutual information:I(e) = log P(rel, w1, w2)P(rel, w1, ?)?
P(?, ?, w2)where P(rel, w1, w2) is the probability of seeing anedge from w1 to w2 with label rel, P(rel, w1, ?)
isthe probability of seeing an edge from w1 to anynode with label rel, and P(?, ?, w2) is the prob-ability of seeing any type of edge linking to w2.Plank and van Noord (2008) used the same approachfor semi-supervised domain adaptation but were notable to achieve similar performance benefits.
In ourimplementation we omit the logarithm in the equa-tion, as this improves performance and avoids prob-lems with log(0) for unseen edges.I(e) compares the probability of the completeedge to the probabilities of partially specified edges,but it assumes that w2 will have an incoming rela-tion, and that w1 will have an outgoing relation oftype rel to some unknown node.
These assumptionsmay or may not be true ?
given the input sentence,we have observed w1 and w2 but do not know whatrelations they are involved in.
Therefore, we createa more general version of the measure that comparesthe probability of the complete edge to the individualprobabilities of the two lemmas ?
the ConditionalEdge Score (CES1):CES1(e) =P(rel, w1, w2)P(w1)?
P(w2)where P(w1) is the probability of seeing w1 in text,estimated from a background corpus using maxi-mum likelihood.Finally, we know that w1 and w2 are in a sen-tence together but cannot assume that there is a de-pendency relation between them.
However, we canchoose to think of each sentence as a fully connectedgraph, with an edge going from every lemma to ev-ery other lemma in the same sentence.
If there exists394ECES1(rel, w1, w2) =12 ?
(?c1?C1sim(c1, w1)?
P(rel,c1,w2)P(c1)?P(w2)?c1?C1sim(c1, w1)+?c2?C2sim(c2, w2)?
P(rel,w1,c2)P(w1)?P(c2)?c2?C2sim(c2, w2))ECES2(rel, w1, w2) =12 ?
(?c1?C1sim(c1, w1)?
P(rel,c1,w2)P(?,c1,w2)?c1?C1sim(c1, w1)+?c2?C2sim(c2, w2)?
P(rel,w1,c2)P(?,w1,c2)?c2?C2sim(c2, w2))Figure 2: Expanded edge score calculation methods using the list of distributionally similar lemmasno genuine relation between the lemmas, the edge issimply considered a null edge.
We can then find theconditional probability of the relation type given thetwo lemmas:CES2(e) =P(rel, w1, w2)P(?, w1, w2)where P(rel, w1, w2) is the probability of the fully-specified relation, and P(?, w1, w2) is the probabilityof there being an edge of any type fromw1 tow2, in-cluding a null edge.
Using fully connected graphs,the latter is equivalent to the probability of w1 andw2 appearing in a sentence together, which again canbe calculated from the background corpus.3.3 Smoothing edge scoresApart from RES, all the scoring methods fromthe previous section rely on correctly estimat-ing the probability of the fully-specified edge,P(rel, w1, w2).
Even in a large background corpusthese triples will be very sparse, and it can be usefulto find approximate methods for estimating the edgescores.Using smoothing techniques derived from workon language modelling, we could back-off to a moregeneral version of the relation.
For example, if(dobj, read, publication) is not frequent enough, thevalue could be approximated using the probabilitiesof (dobj, read, *) and (dobj, *, publication).
How-ever, this can lead to unexpected results due to com-positionality ?
while (dobj, read, *) and (dobj, *,rugby) can be fairly common, (dobj, read, rugby) isan unlikely relation.Instead, we can consider looking at other lemmaswhich are similar to the rare lemmas in the relation.If (dobj, read, publication) is infrequent in the data,the system might predict that book is a reasonablesubstitute for publication and use (dobj, read, book)to estimate the original probability.Given that we have a reliable way of finding likelysubstitutes for a given lemma, we can create ex-panded versions of CES1 and CES2, as shown inFigure 2.
C1 is the list of substitute lemmas for w1,and sim(c1, w1) is a measure showing how similarc1 is to w1.
The methods iterate over the list of sub-stitutes and calculate the CES score for each of themodified relations.
The values are then combined byusing the similarity score as a weight ?
more similarlemmas will have a higher contribution to the finalresult.
This is done for both the head and the depen-dent in the original relation, and the scores are thennormalised and averaged.Experiments with a wide variety of distributionalword similarity measures revealed that WeightedCo-sine (Rei, 2013), a directional similarity measuredesigned to better capture hyponymy relations, per-formed best.
Hyponyms are more specific versionsof a word and normally include the general proper-ties of the hypernym, making them well-suited forlexical substitution.
The WeightedCosine measureincorporates an additional directional weight intothe standard cosine similarity, assigning differentimportance to individual features for the hyponymyrelation.
We retain the 10 most distributionally simi-lar putative hyponyms for each lemma and substitutethem in the relation.
The original lemma is also in-cluded with similarity 1.0, thereby assigning it thehighest weight.
The lemma vectors are built fromthe same vector space model that is used for cal-culating edge probabilities, which includes all thegraph modifications described in Section 3.1.3.4 Combining edge scoresWhile the CES and ECES measures calculate con-fidence scores for bilexical relations using statisticsfrom a large background corpus, they do not includeany knowledge about grammar, syntax, or the con-395CMB1(e) = 3?RES(e) ?
CES1(e) ?
CES2(e) CMB2(e) = 3?RES(e) ?
ECES1(e) ?
ECES2(e)Figure 3: Edge score combination methodstext in a specific sentence.
In contrast, the RES scoreimplicitly includes some of this information, as it iscalculated based on the original parser ranking.
Inorder to take advantage of both information sources,we combine these scores into CMB1 and CMB2, asshown in Figure 3.3.5 Graph scoringEvery edge in graph g?r is assigned a score indicat-ing the reranker?s confidence in that edge belongingto the best parse.
We investigated different strate-gies for combining these values together into a con-fidence score for the whole graph.
The simplest so-lution is to sum together individual edge scores, butthis would lead to always preferring graphs that havea larger number of edges.
Interestingly, averagingthe edge scores does not produce good results eitherbecause it is biased towards smaller graph fragmentscontaining only highly-confident edges.We created a new scoring method which prefersgraphs that cover all the nodes, but does not createbias for a higher number of edges.
For every nodein the graph, it finds the average score of all edgeswhich have that node as a dependent.
These scoresare then averaged again over all nodes:NScore(n) =?e?EgEdgeScore(e)?
isDep(e, n)?e?EgisDep(e, n)GraphScore(g) =?n?NgNScore(n)|Ng|where g is the graph being scored, n ?
Ng is anode in graph g, e ?
Eg is an edge in graph g,isDep(e, n) is a function returning 1.0 if n is the de-pendent in edge e, and 0.0 otherwise.
NScore(n) isset to 0 if the node does not appear as a dependent inany edges.
We found this metric performs well, asit prefers graphs that connect together many nodeswithout simply rewarding a larger number of edges.While the score calculation is done using themodified graph g?r, the resulting score is directly as-signed to the corresponding original graph gr, andthe reordering of the original dependency graphs isused for evaluation.4 Experiments4.1 Evaluation methodsIn order to evaluate how much the reranker improvesthe highest-ranked dependency graph, we calculatethe microaveraged precision, recall and F-score overall dependencies from the top-ranking parses forthe test set.
Following the official RASP evalua-tion (Briscoe et al 2006) we employ the hierarchi-cal edge matching scheme which aggregates countsup the dependency relation subsumption hierarchyand thus rewards the parser for making more fine-grained distinctions.2 Statistical significance of thechange in F-score is calculated by using the Approx-imate Randomisation Test (Noreen, 1989; Cohen,1995) with 106 iterations.We also wish to measure how well the rerankerdoes at the overall task of ordering dependencygraphs.
For this we make use of an oracle that cre-ates the perfect ranking for a set of graphs by calcu-lating their individual F-scores; this ideal ranking isthen compared to the output of our system.
Spear-man?s rank correlation coefficient between the tworankings is calculated for each sentence and then av-eraged over all sentences.
If the scores for all of thereturned analyses are equal, this coefficient cannotbe calculated and is set to 0.4.2 DepBankWe evaluated our self-learning framework usingthe DepBank/GR reannotation (Briscoe and Carroll,2006) of the PARC 700 Dependency Bank (Kinget al 2003).
The dataset is provided with theopen-source RASP distribution3 and has been usedfor evaluating different parsers, including RASP(Briscoe and Carroll, 2006; Watson et al 2007) and2Slight changes in the performance of the baseline parsercompared to previous publications are due to using a more re-cent version of the parser and minor corrections to the gold stan-dard annotation.3ilexir.co.uk/2012/open-source-rasp-3-1/396C&C (Clark and Curran, 2007).
It contains 700 sen-tences, randomly chosen from section 23 of the WSJPenn Treebank (Marcus et al 1993), divided intodevelopment (140 sentences) and test data (560 sen-tences).
We made use of the development data toexperiment with a wider selection of edge and graphscoring methods, and report the final results on thetest data.For reranking we collect up to 1000 top-rankedanalyses for each sentence.
The actual number ofanalyses that the RASP parser outputs depends onthe sentence and can be smaller.
As the parser firstconstructs parse trees and converts them to depen-dency graphs, several parse trees may result in iden-tical graphs; we remove any duplicates to obtain aranking of unique dependency graphs.Our approach relies on a large unannotated corpusof in-domain text, and for this we used the BLLIPcorpus containing 50M words of in-domain WSJ ar-ticles.
Our version of this corpus excludes texts thatare found in the Penn Treebank, thereby also exclud-ing the section that we use for evaluation.The baseline system is the unlexicalised RASPparser with default settings.
In order to constructthe upper bound, we use an oracle to calculate the F-score for each dependency graph individually, andthen create the best possible ranking using thesescores.Table 1 contains evaluation results on the Dep-Bank/GR test set.
The baseline system achieves76.41% F-score on the test data, with 32.70% av-erage correlation.
I and RES scoring methods givecomparable results, with RES improving correlationby 9.56%.
The CES and ECES scores all make useof corpus-based statistics and all significantly im-prove over the baseline system, with absolute in-creases in F-score of more than 2% for the fully-connected edge score variants.Finally, we combine the RES score with thecorpus-based methods and the fully-connectedCMB2 variant again delivers the best overall results.The final F-score is 79.21%, an absolute improve-ment of 2.8%, corresponding to 33.65% relative er-ror reduction with respect to the upper bound.
Cor-relation is also increased by 16.32%; this means themethods not only improve the chances of finding thebest dependency graph, but also manage to createa better overall ranking.
The F-scores for all thecorpus-based scoring methods are statistically sig-nificant when compared to the baseline (p < 0.05).By using our self-learning framework, we wereable to significantly improve the original unlexi-calised parser.
To put the overall result in a widerperspective, Clark and Curran (2007) achieve anF-score of 81.86% on the DepBank/GR test sen-tences using the C&C lexicalised parser, trainedon 40,000 manually-treebanked sentences from theWSJ.
The unlexicalised RASP parser, using amanually-developed grammar and a parse rankingcomponent trained on 4,000 partially-bracketed un-labelled sentences from a domain/genre balancedsubset of Brown (Watson et al 2007), achieves anF-score of 76.41% on the same test set.
The methodintroduced here improves this to 79.21% F-scorewithout using any further manually-annotated data,closing more than half of the gap between the perfor-mance of a fully-supervised in-domain parser and amore weakly-supervised more domain-neutral one.We also performed an additional detailed analysisof the results and found that, with the exception ofthe auxiliary dependency relation, the reranking pro-cess was able to improve the F-score of all other in-dividual dependency types.
Complements and mod-ifiers are attached with much higher accuracy, result-ing in 3.34% and 3.15% increase in the correspond-ing F-scores.
The non-clausal modifier relation (nc-mod), which is the most frequent label in the dataset,increases by 3.16%.4.3 GeniaOne advantage of our reranking framework is thatit does not rely on any domain-dependent manuallyannotated resources.
Therefore, we are interested inseeing how it performs on text from a completelydifferent domain and genre.The GENIA-GR dataset (Tateisi et al 2008) isa collection of 492 sentences taken from biomedi-cal research papers in the GENIA corpus (Kim etal., 2003).
The sentences have been manually anno-tated with dependency-based grammatical relationsidentical to those output by the RASP parser.
How-ever, it does not contain dependencies for all tokensand many multi-word phrases are treated as singleunits.
For example, the tokens ?intracellular redoxstatus?
are annotated as one node with label intra-cellular redox status.
We retain this annotation and397DepBank/GR GENIA-GRPrec Rec F ?
Prec Rec F ?Baseline 77.91 74.97 76.41 32.70 79.91 78.86 79.38 36.54Upper Bound 86.74 82.82 84.73 75.36 86.33 84.71 85.51 78.66I 77.77 75.00 76.36 33.32 77.18 76.21 76.69 30.23RES 78.13 74.94 76.50 42.26 80.06 78.89 79.47 47.52CES1 79.68 76.40 78.01 41.95 78.64 77.50 78.07 36.06CES2 80.48 77.28 78.85 48.43 79.92 78.92 79.42 43.09ECES1 79.96 76.68 78.29 42.41 79.09 78.11 78.60 38.02ECES2 80.71 77.52 79.08 49.05 79.84 78.95 79.39 43.64CMB1 80.64 77.31 78.94 48.25 80.60 79.51 80.05 44.96CMB2 80.88 77.60 79.21 49.02 80.69 79.64 80.16 46.24Table 1: Performance of different edge scoring methods on the test data.
For each measure we report precision,recall, F-score, and average Spearman?s correlation (?).
The highest results for each measure are marked in bold.
Theunderlined F-scores are significantly better compared to the baseline.allow the unlexicalised parser to treat these nodes asatomic unseen words during POS tagging and pars-ing.
However, we use the last lemma in each multi-word phrase for calculating the edge score statistics.In order to initialise our parse reranking frame-work, we also need a background corpus that closelymatches the evaluation domain.
The annotated sen-tences in GENIA-GR were chosen from abstractsthat are labelled with the MeSH term ?NF-kappa B?.Following this method, we created our backgroundcorpus by extracting 7,100 full-text articles (1.6Msentences) from the PubMed Central Open Accesscollection, containing any of the following termswith any capitalisation: ?nf-kappa b?, ?nf-kappab?,?nf kappa b?, ?nf-kappa b?, ?nf-kb?, ?nf-?b?.
Sincewe retain all texts from matching documents, thiskeyword search acts as a broad indicator that the sen-tences contain topics which correspond to the evalu-ation dataset.
This focussed corpus was then parsedwith the unlexicalised parser and used to create astatistical model for the reranking system, followingthe same methods as described in Sections 3 and 4.2.Table 1 also contains the results for experimentsin the biomedical domain.
The first thing to noticeis that while the upper bound for the unlexicalisedparser is similar to that for the DepBank experimentsin Section 4.2, the baseline results are considerablyhigher.
This is largely due to the nature of the dataset?
since many complicated multi-word phrases aretreated as single nodes, the parser is not evaluated onedges within these nodes.
In addition, treating thesenodes as unseen words eliminates many incorrectderivations that would otherwise split the phrases.This results in a naturally higher baseline of 79.38%,and also makes it more difficult to further improvethe performance.The edge scoring methods I, CES1 and ECES1deliver F-scores lower than the baseline in this ex-periment.
RES, CES2 and ECES2 yield a modestimprovement in both F-score and Spearman?s cor-relation.
Finally, the combination methods againgive the best performance, with CMB2 delivering anF-score of 80.16%, an absolute increase of 0.78%,which is statistically significant (p < 0.05).
Theexperiment shows that our self-learning frameworkworks on very different domains, and it can be usedto significantly increase the accuracy of an unlexi-calised parser without requiring any annotated data.5 ConclusionWe developed a new self-learning framework for de-pendency graph reranking that requires only a plain-text corpus from a suitable domain.
We automati-cally parse this corpus and use the highest rankedanalyses to estimate maximum likelihood probabili-ties for bilexical relations.
Every dependency graphis first modified to incorporate additional edges thatmodel selected higher-order dependency path rela-tionships.
Each edge in the graph is then assigned aconfidence score based on statistics from the back-ground corpus and ranking preferences from the un-398lexicalised parser.
We also described a novel methodfor smoothing these scores using directional dis-tributional similarity measures.
Finally, the edgescores are combined into an overall graph score byfirst averaging them over individual nodes.As the method requires no annotated data, it canbe easily adapted to different domains and genres.Our experiments showed that the reranking processsignificantly improved performance on both WSJand biomedical data.ReferencesShilpa Arora, Elijah Mayfield, Carolyn Penstein-Rose?,and Eric Nyberg.
2010.
Sentiment Classification us-ing Automatically Extracted Subgraph Features.
InProceedings of the NAACL HLT 2010 Workshop onComputational Approaches to Analysis and Genera-tion of Emotion in Text.Mohit Bansal and Dan Klein.
2011.
Web-scale fea-tures for full-scale parsing.
In Proceedings of the 49thAnnual Meeting of the Association for ComputationalLinguistics, pages 693?702.Ted Briscoe and John Carroll.
2006.
Evaluating theaccuracy of an unlexicalized statistical parser on thePARC DepBank.
In Proceedings of the COLING/ACLon Main conference poster sessions, number July,pages 41?48, Morristown, NJ, USA.
Association forComputational Linguistics.Ted Briscoe, John Carroll, and Rebecca Watson.
2006.The second release of the RASP system.
In Proceed-ings of the COLING/ACL 2006 Interactive Presenta-tion Sessions, number July, pages 77?80, Sydney, Aus-tralia.
Association for Computational Linguistics.Xavier Carreras.
2007.
Experiments with a higher-orderprojective dependency parser.
In Proceedings of theCoNLL Shared Task Session of EMNLP-CoNLL, vol-ume 7, pages 957?961.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and MaxEnt discriminative rerank-ing.
Proceedings of the 43rd Annual Meeting on As-sociation for Computational Linguistics - ACL ?05,1(June):173?180.Stephen Clark and James R. Curran.
2007.
Formalism-independent parser evaluation with CCG and Dep-Bank.
In Proceedings of the 45th Annual Meetingof the Association of Computational Linguistics, vol-ume 45, pages 248?255.Paul R Cohen.
1995.
Empirical Methods for ArtificialIntelligence.
The MIT Press, Cambridge, MA.Michael Collins.
2000.
Discriminative reranking for nat-ural language parsing.
In The 17th International Con-ference on Machine Learning (ICML).Daniel Gildea.
2001.
Corpus variation and parser per-formance.
In Proceedings of the 2001 Conference onEmpirical Methods in Natural Language Processing,pages 167?202.Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, and Jun?ichiTsujii.
2003.
GENIA corpus - a semantically an-notated corpus for bio-textmining.
Bioinformatics,19(1):180?182.Tracy H. King, Richard Crouch, Stefan Riezler, MaryDalrymple, and Ronald M. Kaplan.
2003.
The PARC700 dependency bank.
In Proceedings of the EACL03:4th International Workshop on Linguistically Inter-preted Corpora (LINC-03), pages 1?8.Dan Klein and Christopher D. Manning.
2003.
Accurateunlexicalized parsing.
In Proceedings of the 41st An-nual Meeting on Association for Computational Lin-guistics, number July, pages 423?430.
Association forComputational Linguistics Morristown, NJ, USA.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: The Penn Treebank.
Computationallinguistics, pages 1?22.David McClosky, Eugene Charniak, and Mark Johnson.2006.
Effective self-training for parsing.
In Proceed-ings of the Human Language Technology Conferenceof the North American Chapter of the Association ofComputational Linguistics, number June, pages 152?159, Morristown, NJ, USA.
Association for Computa-tional Linguistics.Dominick Ng, Matthew Honnibal, and James R. Curran.2010.
Reranking a wide-coverage CCG parser.
InAustralasian Language Technology Association Work-shop 2010, page 90.Eric W. Noreen.
1989.
Computer Intensive Methodsfor Testing Hypotheses: An Introduction.
Wiley, NewYork.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In Proceedings of the 21stInternational Conference on Computational Linguis-tics and the 44th annual meeting of the ACL (ACL ?06),pages 433?440, Morristown, NJ, USA.
Association forComputational Linguistics.Barbara Plank and Gertjan van Noord.
2008.
Explor-ing an auxiliary distribution based approach to domainadaptation of a syntactic disambiguation model.
InColing 2008: Proceedings of the Workshop on Cross-Framework and Cross- Domain Parser Evaluation,pages 9?16, Manchester, UK.
Association for Com-putational Linguistics.399Marek Rei.
2013.
Minimally supervised dependency-based methods for natural language processing.
Ph.D.thesis, University of Cambridge.Satoshi Sekine.
1997.
The domain dependence of pars-ing.
In Proceedings of the fifth conference on Appliednatural language processing, volume 1, pages 96?102,Morristown, NJ, USA.
Association for ComputationalLinguistics.Yuka Tateisi, Yusuke Miyao, Kenji Sagae, and Jun?ichiTsujii.
2008.
GENIA-GR: a Grammatical RelationCorpus for Parser Evaluation in the Biomedical Do-main.
In Proceedings of LREC, pages 1942?1948.Gertjan van Noord.
2007.
Using self-trained bilexicalpreferences to improve disambiguation accuracy.
InProceedings of the 10th International Conference onParsing Technologies, number June, pages 1?10, Mor-ristown, NJ, USA.
Association for Computational Lin-guistics.Rebecca Watson, Ted Briscoe, and John Carroll.
2007.Semi-supervised training of a statistical parser fromunlabeled partially-bracketed data.
Proceedings of the10th International Conference on Parsing Technolo-gies - IWPT ?07, (June):23?32.Guangyou Zhou, Jun Zhao, Kang Liu, and Li Cai.
2011.Exploiting Web-Derived Selectional Preference to Im-prove Statistical Dependency Parsing.
In 49th AnnualMeeting of the Association for Computational Linguis-tics, pages 1556?1565.400
