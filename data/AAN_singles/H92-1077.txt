SESS ION 12: CONTINUOUS SPEECHRECOGNIT ION AND EVALUATION II*Clifford J. Weinstein, ChairLincoln Laboratory, M.I.T.Lexington, MA 02173-9108This session featured a summary of the first dry runbenchmark tests on the new Wall Street Journal (WSJ)continuous peech recognition (CSR) pilot corpus, and adescription of the techniques used and lessons learned bythe four sites who conducted the large vocabulary CSRtests.For the first presentation, Dave Pallett distributed ahandout with system descriptions and results.
Hecredited the people involved, and indicated the tightschedule which was met.
The tests included threetraining paradigms: speaker-dependent (SD); longitudi-nal speaker-dependent (LSD), with much more trainingspeech; and speaker-independent (SI).
Tests included 5Kand 20K vocabularies, bigram and trigram language mod-els, and recognition on speech collected with verbalizedpunctuation (VP) and with no verbalized punctuation(NVP).
Data was shown indicating special difficulties witha few of the speakers.
Results were presented on signal-to-noise ratios for both the primary and secondary mi-crophones.
The data are summarized in Pallett's Pro-ceedings paper.
Some comments on the results are givenbelow.The next four papers, on recognition of the WSJ data atDragon, CMU, Lincoln Laboratory, and SKI, included thecommon theme that extending a CSR system to a muchlarger vocabulary and more general task domain requiredmore than a new dictionary and language model.
In par-ticular, major increases in search time, computation formatching, and memory utilization required each site tomake compromises or revise strategies in acoustic mod-elling, search, and matching strategies.
Despite the pre-liminary nature of the work on this corpus, encouragingresults were obtMned and important issues were rMsed.The Dragon paper was presented by Francesco Scattone,and described two recognition approaches that were de-veloped and tested.
The first method utilized unimodalphonetic elements (PELs), and the second a variation oftied mixtures very recently implemented at Dragon, which*This work was sponsored by the Defense Advanced lq.e-search Projects Agency.
The views expressed axe those of theauthor and do not reflect he official policy or position of theU.S.
Government.was used in Dragon's dry run evaluation test on the 5,000word SD portion of the corpus.
The tied-mixture mod-els proved very effective in modelling the multi-modalityof parameter distributions, and generally ielded betterrecognition results.
Scattone indicated that future workwill focus on further development of the tied-mixturetechniques, including efforts to develop high-performancespeaker-independent recognition techniques.Next, Fil Alleva discussed the application of CMU'sSPHINX-II system to the WSJ CSR task.
An importantchange to SPHINX-II which was made to reduce runningtime was to use only left-context-dependent cross wordmodels; in addition, a number of changes were made tothe Viterbi search to reduce running time.
Tests wererun on a variety of conditions, including the spontaneousspeech, and results are summarized in the paper.The next paper, by Doug Paul, described substantialchanges made to the Lincoln Tied-Mixture HMM CSR,to achieve effective operation for the large-vocabularyCSR task.
The recognizer, which had previously useda time-synchronous beam-pruned search, was convertedto a stack-decoder-based arch strategy with an acousticfast match.
Cross-word models had not yet been included.The stack decoder strategy was shown to perform effec-tively for the larger vocabularies, and a variety of develop-ment test and evaluation test results were presented.
Inaddition, a rapid speaker enrollment procedure was de-scribed, and positive (but preliminary) results on rapidadaptation (using the standard WSJ 40 adaptation sen-tences) were presented.
A discussion followed, focusingon the language modelling, and on perplexity for closedand open vocabularies.Hy Murveit described the application of SRI's DECI-PHER system to the WSJ CSR task.
He focused primar-ily on performance, since the CSR system used was essen-tially identical to the system used in ATIS.
He acknowl-edged help from Dragon (Lexicon) and Lincoln (LanguageModels) in porting to the WSJ task.
He described howDECIPHER was stripped down to reduce computationfor the task.
Tests on the secondary microphone were de-scribed, with about 40% increase in error rate.
An experi-ment was described to investigate the effects of additional379SI training data.
The experiment indicated that substan-tial increases in SI training data could produce significantreductions in error rate relative to those reported in thedry run evaluation tests.The chairman initiated the discussion period which fol-lowed this final presentation by presenting a plot of errorrate vs perplexity for the WSJ dry run tests, the pre-vious best resource management (RM) results, and CSRdictation results which had been presented by IBM atICASSP-89 (Bahl, et.al., Large Vocabulary Natural Lan-guage Continuous Speech Recognition).
For perplexity-80, the WSJ error rates ranged from 9.0% (LSD) to 12.9%(best SO) to 16.6% (best SI).
These error rates were con-siderably higher than the most recent perplexity 60 RMresults (1.8% for SD) and (3.8% for SI), but not as muchhigher than the perplexity-90 SD IBM results (an 11% er-ror rate was reported in the ICASSP-89 Proceedings pa-per, and an improved error rate of about 5% was presentedat the ICASSP-89 talk).
With the understanding thatresults obtained in these different tests are not directlycomparable, still a fair conclusion which could be drawnis that the WSJ corpus is a sufficiently-chailenging one(especially when 20K vocabularies, pontaneous speech,and secondary microphones are considered), and that theresults of the first dry run test were quite encouraging.Most of the ensuing discussion focused on the WSJ corpusand evaluation issues which George Doddington had listedin his earlier CCCC talk.
These are summarized below bytopic.In summarizing the discussion, an attempt is made tosample the range of comments and issues raised.MULT IPLE  EVALUATION CONDIT IONSThe issue was raised of whether the large number of eval-uation conditions was a good idea.Doug Paul :  The multiple conditions added richness andwere appropriate for a pilot and for covering the variedgoals of different sites.F ranc is  Kuba la :  The sampling of conditions had workedout well; the number of conditions hould probably benarrowed a bit over the next few months.V ic tor  Zue: There is a concern over too much splittingof the corpus into multiple parts to support the differenttests.Pat t i  Pr ice :  Expressed concern about too many base-lines.R ich Stern :  Suggested settling on a few common condi-tions.VERBAL IZED PUNCTUATIONS VSNON-VERBAL IZED PUNCTUATIONFirst, a sampling of the comments in favor of continuingto collect data with a split between VP and NVP.Janet  Baker :  People using real dictation systems useVP, so any recognition system for dictation must handleVP.Doug Paul :  Both NVP and VP are needed to supportboth general recognition and dictation; reading with VPmay be awkward at first, but not hard to get used to.M ichae l  P icheny:  Might as well use VP since it is easierfor the recognizer, and people who dictate do not seem tomind.
Emphasized his strong support for VP.Second, a sampling of comments generally against a lotmore collection of VP data.R ich Schwartz :  Given recording problems with VP,would be happier with NVP for general recognition.Dave Pa l le t t :  Doesn't like the split; would like to reducehandling costs.R ich Stern :  Doesn't see value in perpetuating VP.V ic tor  Zue: The VP speech, based on his listening ex-perience, is highly distorted; also people hate to read it.General follow-up comments.George  Dodd ington :  SRI (Jaret Bernstein) is going toask people to dictate naturally; let's see what they do.Pat t i  Pr ice:  All test data should be spontaneous speech.John  Makhou l :  Would the recognition techniques wedevelop depend on whether we collect VP or NVP speech?If not, who cares.PROMPTING TEXTS:  PREPROCESSED VSNATURALFrancis Kubala: Questioned the idea that the acoustictraining data must match the language model.Doug Paul :  Acoustic modelling is a priority of this CSReffort, so it's very important scientifically to have a correctlanguage model, as shown in paper by Paul, Baker, andBaker at the 1990 Speech and Natural Language Work-shop.
Prompting texts are a pragmatic way to do this.Jo rdan  Cohen:  Prompting should be an empirical issue- -  do real dictation experiment and see what people do.Bob  Moore :  Preprocessing is a small effect in the 20Klanguage model, so it should be possible to generate lan-guage models from text without constrMning the prompts.380Vic tor  Zue:  Cited the MIT study which showed the vari-ability of responses from unpreprocessed prompts; alsoraised the issue (not discussed further) of selection of lim-ited vocabulary.SPONTANEOUS VS READ SPEECHMany agree that real interactive data and testing isneeded, but expensive.
Many also agree that it is im-portant o continue collecting read speech.Roger  Moore :  Relates study showing that 2% errordrives dictation users to isolated words.Mike  Picheny:  Concurs - -  accuracy  is more importantfor CSR.George  Dodd ington :  Suggests simulating error-freedictation system.John  Makhou l :  Let's concentrate on read speech now,while keeping alive the effort on exploring paradigms forspontaneous speech collection.Janet  Baker :  Emphasizes that interactive simulationsare expensive, and that collection of read speech is valu-able and cost-effective.V ic tor  Zue: Agrees with John Makhoul.F INAL  REMARKSAt this point, Charles Wayne noted that two great land-marks had been achieved: the collection of the pilot cor-pus and the dry run evaluation, and that both were majoraccomplishments for the Spoken Language Program.381
