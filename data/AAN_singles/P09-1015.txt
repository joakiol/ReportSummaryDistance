Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 127?135,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPReducing the Annotation Effort for Letter-to-Phoneme ConversionKenneth Dwyer and Grzegorz KondrakDepartment of Computing ScienceUniversity of AlbertaEdmonton, AB, Canada, T6G 2E8{dwyer,kondrak}@cs.ualberta.caAbstractLetter-to-phoneme (L2P) conversion is theprocess of producing a correct phonemesequence for a word, given its letters.
Itis often desirable to reduce the quantity oftraining data ?
and hence human anno-tation ?
that is needed to train an L2Pclassifier for a new language.
In this pa-per, we confront the challenge of buildingan accurate L2P classifier with a minimalamount of training data by combining sev-eral diverse techniques: context ordering,letter clustering, active learning, and pho-netic L2P alignment.
Experiments on sixlanguages show up to 75% reduction in an-notation effort.1 IntroductionThe task of letter-to-phoneme (L2P) conversionis to produce a correct sequence of phonemes,given the letters that comprise a word.
An ac-curate L2P converter is an important componentof a text-to-speech system.
In general, a lookuptable does not suffice for L2P conversion, sinceout-of-vocabulary words (e.g., proper names) areinevitably encountered.
This motivates the needfor classification techniques that can predict thephonemes for an unseen word.Numerous studies have contributed to the de-velopment of increasingly accurate L2P sys-tems (Black et al, 1998; Kienappel and Kneser,2001; Bisani and Ney, 2002; Demberg et al, 2007;Jiampojamarn et al, 2008).
A common assump-tion made in these works is that ample amounts oflabelled data are available for training a classifier.Yet, in practice, this is the case for only a smallnumber of languages.
In order to train an L2P clas-sifier for a new language, we must first annotatewords in that language with their correct phonemesequences.
As annotation is expensive, we wouldlike to minimize the amount of effort that is re-quired to build an adequate training set.
The ob-jective of this work is not necessarily to achievestate-of-the-art performance when presented withlarge amounts of training data, but to outperformother approaches when training data is limited.This paper proposes a system for training an ac-curate L2P classifier while requiring as few an-notated words as possible.
We employ decisiontrees as our supervised learning method because oftheir transparency and flexibility.
We incorporatecontext ordering into a decision tree learner thatguides its tree-growing procedure towards gener-ating more intuitive rules.
A clustering over lettersserves as a back-off model in cases where individ-ual letter counts are unreliable.
An active learningtechnique is employed to request the phonemes(labels) for the words that are expected to be themost informative.
Finally, we apply a novel L2Palignment technique based on phonetic similarity,which results in impressive gains in accuracy with-out relying on any training data.Our empirical evaluation on several L2Pdatasets demonstrates that significant reductionsin annotation effort are indeed possible in this do-main.
Individually, all four enhancements improvethe accuracy of our decision tree learner.
The com-bined system yields savings of up to 75% in thenumber of words that have to be labelled, and re-ductions of at least 52% are observed on all thedatasets.
This is achieved without any additionaltuning for the various languages.The paper is organized as follows.
Section 2 ex-plains how supervised learning for L2P conversionis carried out with decision trees, our classifier ofchoice.
Sections 3 through 6 describe our fourmain contributions towards reducing the annota-tion effort for L2P: context ordering (Section 3),clustering letters (Section 4), active learning (Sec-tion 5), and phonetic alignment (Section 6).
Ourexperimental setup and results are discussed in127Sections 7 and 8, respectively.
Finally, Section 9offers some concluding remarks.2 Decision tree learning of L2P classifiersIn this work, we employ a decision tree modelto learn the mapping from words to phoneme se-quences.
Decision tree learners are attractive be-cause they are relatively fast to train, require littleor no parameter tuning, and the resulting classifiercan be interpreted by the user.
A number of priorstudies have applied decision trees to L2P data andhave reported good generalization accuracy (An-dersen et al, 1996; Black et al, 1998; Kienappeland Kneser, 2001).
Also, the widely-used Festi-val Speech Synthesis System (Taylor et al, 1998)relies on decision trees for L2P conversion.We adopt the standard approach of using theletter context as features.
The decision tree pre-dicts the phoneme for the focus letter based onthe m letters that appear before and after it inthe word (including the focus letter itself, and be-ginning/end of word markers, where applicable).The model predicts a phoneme independently foreach letter in a given word.
In order to keep ourmodel simple and transparent, we do not explorethe possibility of conditioning on adjacent (pre-dicted) phonemes.
Any improvement in accuracyresulting from the inclusion of phoneme featureswould also be realized by the baseline that wecompare against, and thus would not materially in-fluence our findings.We employ binary decision trees because theysubstantially outperformed n-ary trees in our pre-liminary experiments.
In L2P, there are manyunique values for each attribute, namely, the let-ters of a given alphabet.
In a n-ary tree each de-cision node partitions the data into n subsets, oneper letter, that are potentially sparse.
By contrast,a binary tree creates one branch for the nominatedletter, and one branch grouping the remaining let-ters into a single subset.
In the forthcoming exper-iments, we use binary decision trees exclusively.3 Context orderingIn the L2P task, context letters that are adjacentto the focus letter tend to be more important thancontext letters that are further away.
For exam-ple, the English letter c is usually pronounced as[s] if the following letter is e or i.
The generaltree-growing algorithm has no notion of the letterdistance, but instead chooses the letters on the ba-sis of their estimated information gain (Manningand Sch?tze, 1999).
As a result, it will sometimesquery a letter at position +3 (denoted l3), for ex-ample, before examining the letters that are closerto the center of the context window.We propose to modify the tree-growing proce-dure to encourage the selection of letters near thefocus letter before those at greater offsets are ex-amined.
In its strictest form, which resemblesthe ?dynamically expanding context?
search strat-egy of Davel and Barnard (2004), li can only bequeried after l0, .
.
.
, li?1 have been queried.
How-ever, this approach seems overly rigid for L2P.
InEnglish, for example, l2 can directly influence thepronunciation of a vowel regardless of the value ofl1 (c.f., the difference between rid and ride).Instead, we adopt a less intrusive strategy,which we refer to as ?context ordering,?
that biasesthe decision tree toward letters that are closer tothe focus, but permits gaps when the informationgain for a distant letter is relatively high.
Specif-ically, the ordering constraint described above isstill applied, but only to letters that have above-average information gain (where the average iscalculated across all letters/attributes).
This meansthat a letter with above-average gain that is eligi-ble with respect to the ordering will take prece-dence over an ineligible letter that has an evenhigher gain.
However, if all the eligible lettershave below-average gain, the ineligible letter withthe highest gain is selected irrespective of its posi-tion.
Our only strict requirement is that the focusletter must always be queried first, unless its infor-mation gain is zero.Kienappel and Kneser (2001) also worked onimproving decision tree performance for L2P, anddevised tie-breaking rules in the event that the tree-growing procedure ranked two or more questionsas being equally informative.
In our experiencewith L2P datasets, exact ties are rare; our contextordering mechanism will have more opportunitiesto guide the tree-growing process.
We expect thischange to improve accuracy, especially when theamount of training data is very limited.
By biasingthe decision tree learner toward questions that areintuitively of greater utility, we make it less proneto overfitting on small data samples.4 Clustering lettersA decision tree trained on L2P data bases its pho-netic predictions on the surrounding letter context.128Yet, when making predictions for unseen words,contexts will inevitably be encountered that didnot appear in the training data.
Instead of rely-ing solely on the particular letters that surroundthe focus letter, we postulate that the learner couldachieve better generalization if it had access toinformation about the types of letters that appearbefore and after.
That is, instead of treating let-ters as abstract symbols, we would like to encodeknowledge of the similarity between certain lettersas features.
One way of achieving this goal is togroup the letters into classes or clusters based ontheir contextual similarity.
Then, when a predic-tion has to be made for an unseen (or low probabil-ity) letter sequence, the letter classes can provideadditional information.Kienappel and Kneser (2001) report accuracygains when applying letter clustering to the L2Ptask.
However, their decision tree learner incorpo-rates neighboring phoneme predictions, and em-ploys a variety of different pruning strategies; theportion of the gains attributable to letter clusteringare not evident.
In addition to exploring the effectof letter clustering on a wider range of languages,we are particularly concerned with the impact thatclustering has on decision tree performance whenthe training set is small.
The addition of letter classfeatures to the data may enable the active learnerto better evaluate candidate words in the pool, andtherefore make more informed selections.To group the letters into classes, we employa hierarchical clustering algorithm (Brown et al,1992).
One advantage of inducing a hierarchy isthat we need not commit to a particular level ofgranularity; in other words, we are not required tospecify the number of classes beforehand, as is thecase with some other clustering algorithms.1The clustering algorithm is initialized by plac-ing each letter in its own class, and then pro-ceeds in a bottom-up manner.
At each step, thepair of classes is merged that leads to the small-est loss in the average mutual information (Man-ning and Sch?tze, 1999) between adjacent classes.The merging process repeats until a single classremains that contains all the letters in the alpha-bet.
Recall that in our problem setting we haveaccess to a (presumably) large pool of unanno-tated words.
The unigram and bigram frequen-cies required by the clustering algorithm are cal-1This approach is inspired by the work of Miller et al(2004), who clustered words for a named-entity tagging task.Letter Bit String Letter Bit Stringa 01000 n 1111b 10000000 o 01001c 10100 p 10001d 11000 q 1000001e 0101 r 111010f 100001 s 11010g 11001 t 101010h 10110 u 0111i 0110 v 100110j 10000001 w 100111k 10111 x 111011l 11100 y 11011m 10010 z 101011# 00Table 1: Hierarchical clustering of English lettersculated from these words; hence, the letters canbe grouped into classes prior to annotation.
Theletter classes only need to be computed once fora given language.
We implemented a brute-forceversion of the algorithm that examines all the pos-sible merges at each step, and generates a hierar-chy within a few hours.
However, when dealingwith a larger number of unique tokens (e.g., whenclustering words instead of letters), additional op-timizations are needed in order to make the proce-dure tractable.The resulting hierarchy takes the form of a bi-nary tree, where the root node/cluster contains allthe letters, and each leaf contains a single let-ter.
Hence, each letter can be represented by a bitstring that describes the path from the root to itsleaf.
As an illustration, the clustering in Table 1was automatically generated from the words in theEnglish CMU Pronouncing Dictionary (CarnegieMellon University, 1998).
It is interesting to notethat the first bit distinguishes vowels from con-sonants, meaning that these were the last twogroups that were merged by the clustering algo-rithm.
Note also that the beginning/end of wordmarker (#) is included in the hierarchy, and is thelast character to be absorbed into a larger clus-ter.
This indicates that # carries more informa-tion than most letters, as is to be expected, in lightof its distinct status.
We also experimented witha manually-constructed letter hierarchy, but ob-served no significant differences in accuracy vis-?-vis the automatic clustering.1295 Active learningWhereas a passive supervised learning algorithmis provided with a collection of training exam-ples that are typically drawn at random, an activelearner has control over the labelled data that it ob-tains (Cohn et al, 1992).
The latter attempts to se-lect its training set intelligently by requesting thelabels of only those examples that are judged to bethe most useful or informative.
Numerous studieshave demonstrated that active learners can makemore efficient use of unlabelled data than do pas-sive learners (Abe and Mamitsuka, 1998; Milleret al, 2004; Culotta and McCallum, 2005).
How-ever, relatively few researchers have applied activelearning techniques to the L2P domain.
This isdespite the fact that annotated data for training anL2P classifier is not available in most languages.We briefly review two relevant studies before pro-ceeding to describe our active learning strategy.Maskey et al (2004) propose a bootstrappingtechnique that iteratively requests the labels of then most frequent words in a corpus.
A classifier istrained on the words that have been annotated thusfar, and then predicts the phonemes for each of then words being considered.
Words for which theprediction confidence is above a certain thresholdare immediately added to the lexicon, while the re-maining words must be verified (and corrected, ifnecessary) by a human annotator.
The main draw-back of such an approach lies in the risk of addingerroneous entries to the lexicon when the classifieris overly confident in a prediction.Kominek and Black (2006) devise a word se-lection strategy based on letter n-gram coverageand word length.
Their method slightly outper-forms random selection, thereby establishing pas-sive learning as a strong baseline.
However, only asingle Italian dataset was used, and the results donot necessarily generalize to other languages.In this paper, we propose to apply an ac-tive learning technique known as Query-by-Bagging (Abe and Mamitsuka, 1998).
We con-sider a pool-based active learning setting, wherebythe learner has access to a pool of unlabelled ex-amples (words), and may obtain labels (phonemesequences) at a cost.
This is an iterative proce-dure in which the learner trains a classifier on thecurrent set of labelled training data, then selectsone or more new examples to label, according tothe classifier?s predictions on the pool data.
Oncelabelled, these examples are added to the trainingset, the classifier is re-trained, and the process re-peats until some stopping criterion is met (e.g., an-notation resources are exhausted).Query-by-Bagging (QBB) is an instance of theQuery-by-Committee algorithm (Freund et al,1997), which selects examples that have high clas-sification variance.
At each iteration, QBB em-ploys the bagging procedure (Breiman, 1996) tocreate a committee of classifiers C. Given a train-ing set T containing k examples (in our setting,k is the total number of letters that have been la-belled), bagging creates each committee memberby sampling k times from T (with replacement),and then training a classifier Ci on the resultingdata.
The example in the pool that maximizes thedisagreement among the predictions of the com-mittee members is selected.A crucial question is how to calculate thedisagreement among the predicted phoneme se-quences for a word in the pool.
In the L2P domain,we assume that a human annotator specifies thephonemes for an entire word, and that the activelearner cannot query individual letters.
We requirea measure of confidence at the word level; yet, ourclassifiers make predictions at the letter level.
Thisis analogous to the task of estimating record confi-dence using field confidence scores in informationextraction (Culotta and McCallum, 2004).Our solution is as follows.
Let w be a word inthe pool.
Each classifier Ci predicts the phonemefor each letter l ?
w. These ?votes?
are aggre-gated to produce a vector vl for letter l that indi-cates the distribution of the |C| predictions over itspossible phonemes.
We then compute the marginfor each letter: If {p, p?}
?
vl are the two highestvote totals, then the margin is M(vl) = |p ?
p?|.A small margin indicates disagreement among theconstituent classifiers.
We define the disagreementscore for the entire word as the minimum margin:score(w) = minl?w{M(vl)} (1)We also experimented with maximum vote en-tropy and average margin/entropy, where the av-erage is taken over all the letters in a word.
Theminimum margin exhibited the best performanceon our development data; hence, we do not pro-vide a detailed evaluation of the other measures.6 L2P alignmentBefore supervised learning can take place, theletters in each word need to be aligned with130phonemes.
However, a lexicon typically providesjust the letter and phoneme sequences for eachword, without specifying the specific phoneme(s)that each letter elicits.
The sub-task of L2P thatpairs letters with phonemes in the training data isreferred to as alignment.
The L2P alignments thatare specified in the training data can influence theaccuracy of the resulting L2P classifier.
In our set-ting, we are interested in mapping each letter toeither a single phoneme or the ?null?
phoneme.The standard approach to L2P alignment is de-scribed by Damper et al (2005).
It performs anExpectation-Maximization (EM) procedure thattakes a (preferably large) collection of words asinput and computes alignments for them simul-taneously.
However, since in our active learningsetting the data is acquired incrementally, we can-not count on the initial availability of a substantialset of words accompanied by their phonemic tran-scriptions.In this paper, we apply the ALINE algorithmto the task of L2P alignment (Kondrak, 2000;Inkpen et al, 2007).
ALINE, which performsphonetically-informed alignment of two strings ofphonemes, requires no training data, and so isideal for our purposes.
Since our task requires thealignment of phonemes with letters, we wish to re-place every letter with a phoneme that is the mostlikely to be produced by that letter.
On the otherhand, we would like our approach to be language-independent.
Our solution is to simply treat ev-ery letter as an IPA symbol (International PhoneticAssociation, 1999).
The IPA is based on the Ro-man alphabet, but also includes a number of othersymbols.
The 26 IPA letter symbols tend to cor-respond to the usual phonetic value that the letterrepresents in the Latin script.2 For example, theIPA symbol [m] denotes ?voiced bilabial nasal,?which is the phoneme represented by the letter min most languages that utilize Latin script.The alignments produced by ALINE are of highquality.
The example below shows the alignmentof the Italian word scianchi to its phonetic tran-scription [SaNki].
ALINE correctly aligns not onlyidentical IPA symbols (i:i), but also IPA symbolsthat represent similar sounds (s:S, n:N, c:k).s c i a n c h i| | | | |S a N k i2ALINE can also be applied to non-Latin scripts by re-placing every grapheme with the IPA symbol that is phoneti-cally closest to it (Jiampojamarn et al, 2009).7 Experimental setupWe performed experiments on six datasets, whichwere obtained from the PRONALSYL letter-to-phoneme conversion challenge.3 They are:English CMUDict (Carnegie Mellon University,1998); French BRULEX (Content et al, 1990),Dutch and German CELEX (Baayen et al, 1996),the Italian Festival dictionary (Cosi et al, 2000),and the Spanish lexicon.
Duplicate words andwords containing punctuation or numerals wereremoved, as were abbreviations and acronyms.The resulting datasets range in size from 31,491to 111,897 words.
The PRONALSYL datasets arealready divided into 10 folds; we used the first foldas our test set, and the other folds were merged to-gether to form the learning set.
In our preliminaryexperiments, we randomly set aside 10 percent ofthis learning set to serve as our development set.Since the focus of our work is on algorithmicenhancements, we simulate the annotator with anoracle and do not address the potential human in-terface factors.
During an experiment, 100 wordswere drawn at random from the learning set; theseconstituted the data on which an initial classifierwas trained.
The rest of the words in the learningset formed the unlabelled pool for active learning;their phonemes were hidden, and a given word?sphonemes were revealed if the word was selectedfor labelling.
After training a classifier on the100 annotated words, we performed 190 iterationsof active learning.
On each iteration, 10 wordswere selected according to Equation 1, labelled byan oracle, and added to the training set.
In or-der to speed up the experiments, a random sam-ple of 2000 words was drawn from the pool andpresented to the active learner each time.
Hence,QBB selected 10 words from the 2000 candidates.We set the QBB committee size |C| to 10.At each step, we measured word accuracy withrespect to the holdout set as the percentage of testwords that yielded no erroneous phoneme predic-tions.
Henceforth, we use accuracy to refer toword accuracy.
Note that although we query ex-amples using a committee, we train a single tree onthese examples in order to produce an intelligiblemodel.
Prior work has demonstrated that this con-figuration performs well in practice (Dwyer andHolte, 2007).
Our results report the accuracy ofthe single tree grown on each iteration, averaged3Available at http://pascallin.ecs.soton.ac.uk/Challenges/PRONALSYL/Datasets/131over 10 random draws of the initial training set.For our decision tree learner, we utilized the J48algorithm provided by Weka (Witten and Frank,2005).
We also experimented with Wagon (Tayloret al, 1998), an implementation of CART, but J48performed better during preliminary trials.
We ranJ48 with default parameter settings, except that bi-nary trees were grown (see Section 2), and subtreeraising was disabled.4Our feature template was established during de-velopment set experiments with the English CMUdata; the data from the other five languages did notinfluence these choices.
The letter context con-sisted of the focus letter and the 3 letters appear-ing before and after the focus (or beginning/end ofword markers, where applicable).
For letter classfeatures, bit strings of length 1 through 6 wereused for the focus letter and its immediate neigh-bors.
Bit strings of length at most 3 were usedat positions +2 and ?2, and no such features wereadded at?3.5 We experimented with other config-urations, including using bit strings of up to length6 at all positions, but they did not produce consis-tent improvements over the selected scheme.8 ResultsWe first examine the contributions of the indi-vidual system components, and then compare ourcomplete system to the baseline.
The dashedcurves in Figure 1 represent the baseline perfor-mance with no clustering, no context ordering,random sampling, and ALINE, unless otherwisenoted.
In all plots, the error bars show the 99%confidence interval for the mean.
Because the av-erage word length differs across languages, we re-port the number of words along the x-axis.
Wehave verified that our system does not substantiallyalter the average number of letters per word in thetraining set for any of these languages.
Hence, thenumber of words reported here is representative ofthe true annotation effort.4Subtree raising is an expensive pruning operation thathad a negligible impact on accuracy during preliminary ex-periments.
Our pruning performs subtree replacement only.5The idea of lowering the specificity of letter class ques-tions as the context length increases is due to Kienappel andKneser (2001), and is intended to avoid overfitting.
However,their configuration differs from ours in that they use longercontext lengths (4 for German and 5 for English) and ask let-ter class questions at every position.
Essentially, the authorstuned the feature set in order to optimize performance on eachproblem, whereas we seek a more general representation thatwill perform well on a variety of languages.8.1 Context orderingOur context ordering strategy improved the ac-curacy of the decision tree learner on every lan-guage (see Figure 1a).
Statistically significant im-provements were realized on Dutch, French, andGerman.
Our expectation was that context order-ing would be particularly helpful during the earlyrounds of active learning, when there is a greaterrisk of overfitting on the small training sets.
Forsome languages (notably, German and Spanish)this was indeed the case; yet, for Dutch, contextordering became more effective as the training setincreased in size.It should be noted that our context orderingstrategy is sufficiently general that it can be im-plemented in other decision tree learners that growbinary trees, such as Wagon/CART (Taylor et al,1998).
An n-ary implementation is also feasible,although we have not tried this variation.8.2 Clustering lettersAs can be seen in Figure 1b, clustering letters intoclasses tended to produce a steady increase in ac-curacy.
The only case where it had no statisticallysignificant effect was on English.
Another benefitof clustering is that it reduces variance.
The confi-dence intervals are generally wider when cluster-ing is disabled, meaning that the system?s perfor-mance was less sensitive to changes in the initialtraining set when letter classes were used.8.3 Active learningOn five of the six datasets, Query-by-Bagging re-quired significantly fewer labelled examples toreach the maximum level of performance achievedby the passive learner (see Figure 1c).
For in-stance, on the Spanish dataset, random samplingreached 97% word accuracy after 1420 words hadbeen annotated, whereas QBB did so with only510 words ?
a 64% reduction in labelling ef-fort.
Similarly, savings ranging from 30% to 63%were observed for the other languages, with theexception of English, where a statistically insignif-icant 4% reduction was recorded.
Since English ishighly irregular in comparison with the other fivelanguages, the active learner tends to query exam-ples that are difficult to classify, but which are un-helpful in terms of generalization.It is important to note that empirical compar-isons of different active learning techniques haveshown that random sampling establishes a very1320 5 10 15 20Number of training words (x100)102030405060708090100Word accuracy (%)Context OrderingNo Context Ordering(a) Context Ordering0 5 10 15 20Number of training words (x100)102030405060708090100Word accuracy (%)ClusteringNo Clustering(b) Clustering0 5 10 15 20Number of training words (x100)102030405060708090100Word accuracy (%)Query-by-BaggingRandom Sampling(c) Active learning0 5 10 15 20Number of training words (x100)102030405060708090100Word accuracy (%)ALINEEM(d) L2P alignmentSpanish Italian +French Dutch + German EnglishFigure 1: Performance of the individual system componentsstrong baseline on some datasets (Schein and Un-gar, 2007; Settles and Craven, 2008).
It is rarelythe case that a given active learning strategy isable to unanimously outperform random samplingacross a range of datasets.
From this perspective,to achieve statistically significant improvementson five of six L2P datasets (without ever beingbeaten by random) is an excellent result for QBB.8.4 L2P alignmentThe ALINE method for L2P alignment outper-formed EM on all six datasets (see Figure 1d).
Aswas mentioned in Section 6, the EM aligner de-pends on all the available training data, whereasALINE processes words individually.
Only onSpanish and Italian, languages which have highlyregular spelling systems, was the EM aligner com-petitive with ALINE.
The accuracy gains on theremaining four datasets are remarkable, consider-ing that better alignments do not necessarily trans-late into improved classification.We hypothesized that EM?s inferior perfor-mance was due to the limited quantities of datathat were available in the early stages of activelearning.
In a follow-up experiment, we allowedEM to align the entire learning set in advance,and these aligned entries were revealed when re-quested by the learner.
We compared this with theusual procedure whereby EM is applied to the la-belled training data at each iteration of learning.The learning curves (not shown) were virtually in-distinguishable, and there were no statistically sig-nificant differences on any of the languages.
EMappears to produce poor alignments regardless ofthe amount of available data.1330 5 10 15 20Number of training words (x100)102030405060708090100Word accuracy (%)Complete SystemBaselineSpanish Italian +French Dutch + German EnglishFigure 2: Performance of the complete system8.5 Complete systemThe complete system consists of context order-ing, clustering, Query-by-Bagging, and ALINE;the baseline represents random sampling with EMalignment and no additional enhancements.
Fig-ure 2 plots the word accuracies for all six datasets.Although the absolute word accuracies variedconsiderably across the different languages, oursystem significantly outperformed the baseline inevery instance.
On the French dataset, for ex-ample, the baseline labelled 1850 words beforereaching its maximum accuracy of 64%, whereasthe complete system required only 480 queries toreach 64% accuracy.
This represents a reductionof 74% in the labelling effort.
The savings for theother languages are: Spanish, 75%; Dutch, 68%;English, 59%; German, 59%; and Italian, 52%.6Interestingly, the savings are the highest on Span-ish, even though the corresponding accuracy gainsare the smallest.
This demonstrates that our ap-proach is also effective on languages with rela-tively transparent orthography.At first glance, the performance of both sys-tems appears to be rather poor on the Englishdataset.
To put our results into perspective, Blacket al (1998) report 57.8% accuracy on this datasetwith a similar alignment method and decision treelearner.
Our baseline system achieves 57.3% ac-curacy when 90,000 words have been labelled.Hence, the low values in Figure 2 simply reflectthe fact that many more examples are required to6The average savings in the number of labelled wordswith respect to the entire learning curve are similar, rangingfrom 50% on Italian to 73% on Spanish.learn an accurate classifier for the English data.9 ConclusionsWe have presented a system for learning a letter-to-phoneme classifier that combines four distinctenhancements in order to minimize the amountof data that must be annotated.
Our experimentsinvolving datasets from several languages clearlydemonstrate that unlabelled data can be used moreefficiently, resulting in greater accuracy for a giventraining set size, without any additional tuningfor the different languages.
The experiments alsoshow that a phonetically-based aligner may bepreferable to the widely-used EM alignment tech-nique, a discovery that could lead to the improve-ment of L2P accuracy in general.While this work represents an important stepin reducing the cost of constructing an L2P train-ing set, we intend to explore other active learnersand classification algorithms, including sequencelabelling strategies (Settles and Craven, 2008).We also plan to incorporate user-centric enhance-ments (Davel and Barnard, 2004; Culotta and Mc-Callum, 2005) with the aim of reducing both theeffort and expertise that is required to annotatewords with their phoneme sequences.AcknowledgmentsWe would like to thank Sittichai Jiampojamarn forhelpful discussions and for providing an imple-mentation of the Expectation-Maximization align-ment algorithm.
This research was supported bythe Natural Sciences and Engineering ResearchCouncil of Canada (NSERC) and the InformaticsCircle of Research Excellence (iCORE).ReferencesNaoki Abe and Hiroshi Mamitsuka.
1998.
Querylearning strategies using boosting and bagging.
InProc.
International Conference on Machine Learn-ing, pages 1?9.Ove Andersen, Ronald Kuhn, Ariane Lazarid?s, PaulDalsgaard, J?rgen Haas, and Elmar N?th.
1996.Comparison of two tree-structured approaches forgrapheme-to-phoneme conversion.
In Proc.
Inter-national Conference on Spoken Language Process-ing, volume 3, pages 1700?1703.R.
Harald Baayen, Richard Piepenbrock, and Leon Gu-likers, 1996.
The CELEX2 lexical database.
Lin-guistic Data Consortium, Univ.
of Pennsylvania.134Maximilian Bisani and Hermann Ney.
2002.
Investi-gations on joint-multigram models for grapheme-to-phoneme conversion.
In Proc.
International Confer-ence on Spoken Language Processing, pages 105?108.Alan W. Black, Kevin Lenzo, and Vincent Pagel.
1998.Issues in building general letter to sound rules.
InESCA Workshop on Speech Synthesis, pages 77?80.Leo Breiman.
1996.
Bagging predictors.
MachineLearning, 24(2):123?140.Peter F. Brown, Vincent J. Della Pietra, Peter V. deS-ouza, Jennifer C. Lai, and Robert L. Mercer.
1992.Class-based n-gram models of natural language.Computational Linguistics, 18(4):467?479.Carnegie Mellon University.
1998.
The Carnegie Mel-lon pronouncing dictionary.David A. Cohn, Les E. Atlas, and Richard E. Ladner.1992.
Improving generalization with active learn-ing.
Machine Learning, 15(2):201?221.Alain Content, Phillppe Mousty, and Monique Radeau.1990.
Brulex: Une base de donn?es lexicales in-formatis?e pour le fran?ais ?crit et parl?.
L?ann?ePsychologique, 90:551?566.Piero Cosi, Roberto Gretter, and Fabio Tesser.
2000.Festival parla Italiano.
In Proc.
Giornate delGruppo di Fonetica Sperimentale.Aron Culotta and Andrew McCallum.
2004.
Con-fidence estimation for information extraction.
InProc.
HLT-NAACL, pages 109?114.Aron Culotta and Andrew McCallum.
2005.
Reduc-ing labeling effort for structured prediction tasks.
InProc.
National Conference on Artificial Intelligence,pages 746?751.Robert I. Damper, Yannick Marchand, John-David S.Marsters, and Alexander I. Bazin.
2005.
Align-ing text and phonemes for speech technology appli-cations using an EM-like algorithm.
InternationalJournal of Speech Technology, 8(2):147?160.Marelie Davel and Etienne Barnard.
2004.
The effi-cient generation of pronunciation dictionaries: Hu-man factors during bootstrapping.
In Proc.
Interna-tional Conference on Spoken Language Processing,pages 2797?2800.Vera Demberg, Helmut Schmid, and Gregor M?hler.2007.
Phonological constraints and morphologi-cal preprocessing for grapheme-to-phoneme conver-sion.
In Proc.
ACL, pages 96?103.Kenneth Dwyer and Robert Holte.
2007.
Decision treeinstability and active learning.
In Proc.
EuropeanConference on Machine Learning, pages 128?139.Yoav Freund, H. Sebastian Seung, Eli Shamir, and Naf-tali Tishby.
1997.
Selective sampling using thequery by committee algorithm.
Machine Learning,28(2-3):133?168.Diana Inkpen, Rapha?lle Martin, and AlainDesrochers.
2007.
Graphon: un outil pourla transcription phon?tique des mots fran?ais.Unpublished manuscript.International Phonetic Association.
1999.
Handbookof the International Phonetic Association: A Guideto the Use of the International Phonetic Alphabet.Cambridge University Press.Sittichai Jiampojamarn, Colin Cherry, and GrzegorzKondrak.
2008.
Joint processing and discriminativetraining for letter-to-phoneme conversion.
In Proc.ACL, pages 905?913.Sittichai Jiampojamarn, Aditya Bhargava, Qing Dou,Kenneth Dwyer, and Grzegorz Kondrak.
2009.
Di-recTL: a language-independent approach to translit-eration.
In Named Entities Workshop (NEWS):Shared Task on Transliteration.
Submitted.Anne K. Kienappel and Reinhard Kneser.
2001.
De-signing very compact decision trees for grapheme-to-phoneme transcription.
In Proc.
European Con-ference on Speech Communication and Technology,pages 1911?1914.John Kominek and Alan W. Black.
2006.
Learn-ing pronunciation dictionaries: Language complex-ity and word selection strategies.
In Proc.
HLT-NAACL, pages 232?239.Grzegorz Kondrak.
2000.
A new algorithm for thealignment of phonetic sequences.
In Proc.
NAACL,pages 288?295.Christopher D. Manning and Hinrich Sch?tze.
1999.Foundations of Statistical Natural Language Pro-cessing.
MIT Press.Sameer R. Maskey, Alan W. Black, and Laura M.Tomokiya.
2004.
Boostrapping phonetic lexiconsfor new languages.
In Proc.
International Confer-ence on Spoken Language Processing, pages 69?72.Scott Miller, Jethran Guinness, and Alex Zamanian.2004.
Name tagging with word clusters and dis-criminative training.
In Proc.
HLT-NAACL, pages337?342.Andrew I. Schein and Lyle H. Ungar.
2007.
Activelearning for logistic regression: an evaluation.
Ma-chine Learning, 68(3):235?265.Burr Settles and Mark Craven.
2008.
An analysisof active learning strategies for sequence labelingtasks.
In Proc.
Conference on Empirical Methodsin Natural Language Processing, pages 1069?1078.Paul A. Taylor, Alan Black, and Richard Caley.
1998.The architecture of the Festival Speech SynthesisSystem.
In ESCA Workshop on Speech Synthesis,pages 147?151.Ian H. Witten and Eibe Frank.
2005.
Data Mining:Practical Machine Learning Tools and Techniques.Morgan Kaufmann, 2nd edition.135
