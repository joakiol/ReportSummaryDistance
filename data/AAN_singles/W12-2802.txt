Workshop on Semantic Interpretation in an Actionable Context, NAACL-HLT 2012, pages 7?14,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsToward Learning Perceptually Grounded Word Meanings from UnalignedParallel DataStefanie Tellex and Pratiksha Thaker and Josh Joseph and Matthew R. Walter and Nicholas RoyMIT Computer Science and Artificial Intelligence LaboratoryAbstractIn order for robots to effectively understandnatural language commands, they must be ableto acquire a large vocabulary of meaning rep-resentations that can be mapped to perceptualfeatures in the external world.
Previous ap-proaches to learning these grounded meaningrepresentations require detailed annotations attraining time.
In this paper, we present anapproach which is capable of jointly learninga policy for following natural language com-mands such as ?Pick up the tire pallet,?
as wellas a mapping between specific phrases in thelanguage and aspects of the external world;for example the mapping between the words?the tire pallet?
and a specific object in theenvironment.
We assume the action policytakes a parametric form that factors based onthe structure of the language, based on the G3framework and use stochastic gradient ascentto optimize policy parameters.
Our prelimi-nary evaluation demonstrates the effectivenessof the model on a corpus of ?pick up?
com-mands given to a robotic forklift by untrainedusers.1 IntroductionIn order for robots to robustly understand humanlanguage, they must have access to meaning rep-resentations capable of mapping between symbolsin the language and aspects of the external worldwhich are accessible via the robot?s perception sys-tem.
Previous approaches have represented wordmeanings as symbols in some specific symboliclanguage, either programmed by hand [Winograd,1971, MacMahon et al, 2006] or learned [Matuszeket al, 2010, Chen and Mooney, 2011, Liang et al,2011, Branavan et al, 2009].
Because word mean-ings are represented as symbols, rather than percep-tually grounded features, the mapping between thesesymbols and the external world must still be de-fined.
Furthermore, the uncertainty of the mappingbetween constituents in the language and aspects ofthe external world cannot be explicitly representedby the model.Language grounding approaches, in contrast, mapwords in the language to groundings in the externalworld [Mavridis and Roy, 2006, Hsiao et al, 2008,Kollar et al, 2010, Tellex et al, 2011].
Groundingsare the specific physical concept that is referred toby the language and can be objects (e.g., a truckor a door), places (e.g., a particular location in theworld), paths (e.g., a trajectory through the envi-ronment), or events (e.g., a sequence of robot ac-tions).
This symbol grounding approach [Harnad,1990] represents word meanings as functions whichtake as input a perceptual representation of a ground-ing and return whether it matches words in the lan-guage.
Recent work has demonstrated how to learngrounded word meanings from a parallel corpus ofnatural language commands paired with groundingsin the external world [Tellex et al, 2011].
How-ever, learning model parameters required that theparallel corpus be augmented with additional an-notations specifying the alignment between specificphrases in the language and corresponding ground-ings in the external world.
Figure 1 shows an ex-ample command from the training set paired withthese alignment annotations, represented as arrows7pointing from each linguistic constituent to a corre-sponding grounding.Our approach in this paper relaxes these annota-tion requirements and learns perceptually groundedword meanings from an unaligned parallel corpusthat only provides supervision for the top-level ac-tion that corresponds to a natural language com-mand.
Our system takes as input a state/action spacefor the robot defining a space of possible groundingsand available actions in the external world.
In addi-tion it requires a corpus of natural language com-mands paired with the correct action executed inthe environment.
For example, an entry in the cor-pus consists of a natural language command such as?Pick up the tire pallet?
given to a robotic forklift,paired with an action sequence of the robot as drivesto the tire pallet, inserts its forks, and raises it off theground, drives to the truck, and sets it down.To learn from an unaligned corpus, we derivea new training algorithm that combines the Gen-eralized Grounding Graph (G3) framework intro-duced by Tellex et al [2011] with the policy gra-dient method described by Branavan et al [2009].We assume a specific parametric form for the actionpolicy that is defined by the linguistic structure ofthe natural language command.
The system learnsa policy parameters that maximize expected rewardusing stochastic gradient ascent.
By factoring thepolicy according to the structure of language, we canpropagate the error signal to each term, allowing thesystem to infer groundings for each linguistic con-stituent even without direct supervision.
We eval-uate our model using a corpus of natural languagecommands collected from untrained users on the in-ternet, commanding the robot to pick up objects ordrive to locations in the environment.
The evalua-tion demonstrates that the model is able to predictboth robot actions and noun phrase groundings withhigh accuracy, despite having no direct supervisionfor noun phrase groundings.2 BackgroundWe briefly review the G3 framework, introduced byTellex et al [2011].
In order for a robot to un-derstand natural language, it must be able to mapbetween words in the language and correspondinggroundings in the external world.
The aim is to findFigure 1: Sample entry from an aligned corpus,where mappings between phrases in the languageand groundings in the external world are explicitlyspecified as arrows.
Learning the meaning of ?thetruck?
and ?the pallet?
is challenging when align-ment annotations are not known.
?r1?Pick up?
?1 =?1?f2?the pallet.?
?2?2 =Figure 2: Grounding graph for ?Pick up the tire pal-let.8the most probable groundings ?1 .
.
.
?N given thelanguage ?
and the robot?s model of the environ-ment M :argmax?1...?Np(?1 .
.
.
?N |?,M) (1)M consists of the robot?s location, the loca-tions, geometries, and perceptual tags of objects, andavailable actions the robot can take.
For brevity, weomit M from future equations in this section.To learn this distribution, one standard approachis to factor it based on certain independence assump-tions, then train models for each factor.
Naturallanguage has a well-known compositional, hierar-chical argument structure [Jackendoff, 1983], and apromising approach is to exploit this structure in or-der to factor the model.
However, if we define a di-rected model over these variables, we must assumea possibly arbitrary order to the conditional ?i fac-tors.
For example, for a phrase such as ?the tire pal-let near the other skid,?
we could factorize in eitherof the following ways:p(?tires, ?skid|?)
= p(?skid|?tires,?)?
p(?tires|?)
(2)p(?tires, ?skid|?)
= p(?tires|?skid,?)?
p(?skid|?)
(3)Depending on the order of factorization, we willneed different conditional probability tables that cor-respond to the meanings of words in the language.To resolve this issue, another approach is to useBayes?
Rule to estimate the p(?|?1 .
.
.
?N ), but thisapproach would require normalizing over all possi-ble words in the language ?.
Another alternativeis to use an undirected model, but this approach isintractable because it requires normalizing over allpossible values of all ?i variables in the model, in-cluding continuous attributes such as location andsize.To address these problems, the G3 framework in-troduced a correspondence vector ?
to capture thedependency between ?1 .
.
.
?N and ?.
Each entryin ?i ?
?
corresponds to whether linguistic con-stituent ?i ?
?
corresponds to grounding ?i.
Weassume that ?1 .
.
.
?N are independent of ?
unless?
is known.
Introducing ?
enables factorization ac-cording to the structure of language with local nor-malization at each factor over a space of just the twopossible values for ?i.2.1 InferenceIn order to use the G3 framework for inference, wewant to infer the groundings ?1 .
.
.
?N that maxi-mize the distributionargmax?1...?Np(?1 .
.
.
?N |?,?)
(4)which is equivalent to maximizing the joint distribu-tion of all groundings ?1 .
.
.
?N , ?
and ?,argmax?1...?Np(?1 .
.
.
?N ,?,?).
(5)We assume that ?
and ?1 .
.
.
?N are independentwhen ?
is not known, yielding:argmax?1...?Np(?|?, ?1 .
.
.
?N )p(?
)p(?1 .
.
.
?N ) (6)This independence assumption may seem unintu-itive, but it is justified because the correspondencevariable ?
breaks the dependency between ?
and?1 .
.
.
?N .
If we do not know whether ?1 .
.
.
?N cor-respond to ?, we assume that the language does nottell us anything about the groundings.Finally, for simplicity, we assume that any objectin the environment is equally likely to be referencedby the language, which amounts to a constant prioron ?1 .
.
.
?N .
In the future, we plan to incorporatemodels of attention and salience into this prior.
Weignore p(?)
since it does not depend on ?1 .
.
.
?N ,leading to:argmax?1...?Np(?|?, ?1 .
.
.
?N ) (7)To compute the maximum value of the objectivein Equation 7, the system performs beam searchover ?1 .
.
.
?N , computing the probability of eachassignment from Equation 7 to find the maximumprobability assignment.
Although we are usingp(?|?, ?1 .
.
.
?N ) as the objective function, ?
isfixed, and the ?1 .
.
.
?N are unknown.
This approachis valid because, given our independence assump-tions, p(?|?, ?1 .
.
.
?N ) corresponds to the joint dis-tribution over all the variables given in Equation 5.In order to perform beam search, we factor themodel according to the hierarchical, compositionallinguistic structure of the command:p(?|?, ?1 .
.
.
?N ) =?ip(?i|?i, ?i1 .
.
.
?ik) (8)9This factorization can be represented graphically;we call the resulting graphical model the groundinggraph for a natural language command.
The directedmodel for the command ?Pick up the pallet?
appearsin Figure 2.
The ?
variables correspond to language;the ?
variables correspond to groundings in the ex-ternal world, and the ?
variables are True if thegroundings correspond to the language, and Falseotherwise.In the fully supervised case, we fit model param-eters ?
using an aligned parallel corpus of labeledpositive and negative examples for each linguisticconstituent.
The G3 framework assumes a log-linearparametrization with feature functions fj and fea-ture weights ?j :p(?|?, ?1 .
.
.
?N ) = (9)?i1Zexp(?j?jfj(?i,?i, ?i1 .
.
.
?ik)) (10)This function is convex and can be optimized withgradient-based methods [McCallum, 2002].Features correspond to the degree to which each?
correctly grounds ?i.
For a relation such as ?on,?a natural feature is whether the grounding corre-sponding to the head noun phrase is supported bythe grounding corresponding to the argument nounphrases.
However, the feature supports(?i, ?j)alone is not enough to enable the model to learn that?on?
corresponds to supports(?i, ?j).
Instead weneed a feature that also takes into account the word?on:?supports(?i, ?j) ?
(?on?
?
?i) (11)3 ApproachOur goal is to learn model parameters for the G3framework from an unaligned corpus of natural lan-guage commands paired with robot actions.
Previ-ously, the system learned model parameters ?
usingan aligned corpus in which values for all ground-ing variables are known at training time, and anno-tators provided both positive and negative examplesfor each factor.
In this paper we describe how torelax this annotation requirement so that only thetop-level action needs to be observed in order totrain the model.
It is easy and fast to collect datawith these annotations, whereas annotating the val-ues of all the variables, including negative examplesis time-consuming and error prone.
Once we knowthe model parameters we can use existing inferenceto find groundings corresponding to word meanings,as in Equation 4.We are given a corpus of D training examples.Each example d consists of a natural language com-mand ?d with an associated grounding graph withgrounding variables ?d.
Values for the groundingvariables are not known, except for an observedvalue gda for the top-level action random variable, ?da .Finally, an example has an associated environmentalcontext or semantic map Md.
Each environmentalcontext will have a different set of objects and avail-able actions for the robot.
For example, one trainingexample might contain a command given in an envi-ronment with a single tire pallet; another might con-tain a command given in an environment with twobox pallets and a truck.We define a sampling distribution to choose val-ues for the ?
variables in the model using the G3framework with parameters ?:p(?d|?d,?d,Md,?)
(12)Next, we define a reward function for choosing thecorrect grounding ga for a training example:r(?d, gda) =?1 if ?da = gda?1 otherwise(13)Here ?a is a grounding variable for the top-level ac-tion corresponding to the command; it is one of thevariables in the vector ?.
Our aim is to find modelparameters that maximize expected reward whendrawing values for ?d from p(?d|?d,?d,Md,?
)over the training set:argmax??dEp(?d|?d,?d,Md,?
)r(?d, gda) (14)Expanding the expectation we have:argmax??dd?
?r(?d, gda)p(?d|?d,?d,Md,?
)(15)We use stochastic gradient descent to find model pa-rameters that maximize reward.
First, we take the10derivative of the expectation with respect to ?.
(Wedrop the d subscripts for brevity.)???kEp(?|?,?,M,?
)r(?, ga) =?
?r(?, ga)???kp(?|?,?,M,?)
(16)Focusing on the inner term, we expand it withBayes?
rule:???kp(?|?,?,M,?)
=???kp(?|?,?,M,?)p(?|?,M,?)p(?|?,M,?
)(17)We assume the priors do not depend on ?:p(?|M)p(?|?)???kp(?|?,?,M,?)
(18)For brevity, we compress ?, ?, and M in the vari-able X .
Next, we take the partial derivative of thelikelihood of ?.
First we assume each factor is inde-pendent.???kp(?|X,?)
=???k?ip(?i|X,?)
(19)=?ip(?i|X,?)????j??
?kp(?j |X,?
)p(?j |X,?)??
(20)Finally, we assume the distribution over ?j takes alog-linear form with feature functions fk and param-eters ?k, as in the G3 framework.??
?kp(?j |X,?)
= (21)p(?|X,?)?
?fk(?, X)?
Ep(??|X,?)?fk(?
?, X)?
?We substitute back into the overall expression for thepartial derivative of the expectation:???kEp(?|X,?
)r(?, ga) =Ep(?|X,?
)r(?, ga)???
?jfk(?j , X)?
Ep(??|X,?)?fk(?
?, X)???
(22)Input:1: Initial values for parameters, ?0.2: Training dataset, D.3: Number of iterations, T .4: Step size, ?.5:6: ??
?07: for t ?
T do8: for d ?
D do9: ??
?
???kEp(?d|?d,?d,Md,?
)r(?d, gda)10: end for11: ??
?+ ??
?12: end forOutput: Estimate of parameters ?Figure 3: Training algorithm.We approximate the expectation over ?
withhighly probable bindings for the ?
and update thegradient incrementally for each example.
The train-ing algorithm is given in Figure 3.4 ResultsWe present preliminary results for the learning algo-rithm using a corpus of natural language commandsgiven to a robotic forklift.
We collected a corpusof natural language commands paired with robot ac-tions by showing annotators on Amazon Mechani-cal Turk a video of the robot executing an action andasking them to describe in words they would use tocommand an expert human operator to carry out thecommands in the video.
Frames from a video in ourcorpus, together with commands for that video ap-pear in Figure 4.
Since commands often spannedmultiple parts of the video, we annotated the align-ment between each top-level clause in the commandand the robot?s motion in the video.
Our initial eval-uation uses only commands from the corpus thatcontain the words ?pick up?
due to scaling issueswhen running on the entire corpus.We report results using a random cost function asa baseline as well as the learned parameters on atraining set and a held-out test set.
Table 1 showsperformance on a training set using small environ-ments (with one or two other objects) and a test setof small and large environments (with up to six otherobjects).11(a) t=0 (b) t=20 (c) t=30Pick up pallet with refridgerator [sic] and place on truck tothe left.A distance away you should see a rectangular box.
Approach itslowly and load it up onto your forklift.
Slowly proceed toback out and then make a sharp turn and approach the truck.Raise your forklift and drop the rectangular box on the back ofthe truck.Go to the pallet with the refrigerator on it and pick itup.
Move the pallet to the truck trailer.
Place the pallet onthe trailer.Pick up the pallet with the refrigerator and place it onthe trailer.
(d) CommandsFigure 4: Frames from a video in our dataset, pairedwith natural language commands.As expected, on the training set the system learnsa good policy, since it is directly rewarded for act-ing correctly.
Because the environments are small,the chance of correctly grounding concrete nounphrases with a random cost function is high.
How-ever after training performance at grounding nounphrases increases to 92% even though the systemhad no access to alignment annotations for nounphrases at training time; it only observes rewardbased on whether it has acted correctly.Next, we report performance on a test set to assessgeneralization to novel commands given in novel en-vironments.
Since the test set includes larger en-vironments with up to six objects, baseline perfor-mance is lower.
However the trained system is ableto achieve high performance at both inferring cor-rect actions as well as correct object groundings, de-spite having no access to a reward signal of any kindduring inference.
This result shows the system haslearned general word meanings that apply in novelcontexts not seen at training time.5 Related WorkBeginning with SHRDLU [Winograd, 1971], manysystems have exploited the compositional structureof language to statically generate a plan correspond-% CorrectActions Concrete Noun PhrasesBefore Training 31% 61%After Training 100% 92%(a) Training (small environments)% CorrectActions Concrete Noun PhrasesBefore Training 3% 26%After Training 84% 77%(b) Testing (small and large environments)Table 1: Results on the training set and test set.ing to a natural language command [Hsiao et al,2008, MacMahon et al, 2006, Skubic et al, 2004,Dzifcak et al, 2009].
Our work moves beyondthis framework by defining a probabilistic graphicalmodel according to the structure of the natural lan-guage command, inducing a distribution over plansand groundings.Models that learned word meanings [Tellex et al,2011, Kollar et al, 2010] require detailed align-ment annotations between constituents in the lan-guage and objects, places, paths, or events in the ex-ternal world.
Previous approaches capable of learn-ing from unaligned data [Vogel and Jurafsky, 2010,Branavan et al, 2009] used sequential models thatcould not capture the hierarchical structure of lan-guage.
Matuszek et al [2010], Liang et al [2011]and Chen and Mooney [2011] describe models thatlearn compositional semantics, but word meaningsare symbolic structures rather than patterns of fea-tures in the external world.There has been a variety of work in transferringaction policies between a human and a robot.
In imi-tation learning, the goal is to create a system that canwatch a teacher perform an action, and then repro-duce that action [Kruger et al, 2007, Chernova andVeloso, 2009, Schaal et al, 2003, Ekvall and Kragic,2008].
Rybski et al [2007] developed an imitationlearning system that learns from a combination ofimitation of the human teacher, as well as naturallanguage input.
Our work differs in that the systemmust infer an action from the natural language com-mands, rather than from watching the teacher per-12form an action.
The system is trained off-line, andthe task of the robot is to respond on-line to the nat-ural language command.6 ConclusionIn this paper we described an approach for learningperceptually grounded word meanings from an un-aligned parallel corpus of language paired with robotactions.
The training algorithm jointly infers poli-cies that correspond to natural language commandsas well as alignments between noun phrases in thecommand and groundings in the external world.
Inaddition, our approach learns grounded word mean-ings or distributions corresponding to words in thelanguage, that the system can use to follow novelcommands that it may have never encountered dur-ing training.
We presented a preliminary evaluationon a small corpus, demonstrating that the system isable to infer meanings for concrete noun phrases de-spite having no direct supervision for these values.There are many directions for improvement.
Weplan to train our system using a large dataset of lan-guage paired with robot actions in more complex en-vironments, and on more than one robotic platform.Our approach points the way towards a frameworkthat can learn a large vocabulary of general groundedword meanings, enabling systems that flexibly re-spond to a wide variety of natural language com-mands given by untrained users.ReferencesS.
R. K. Branavan, H. Chen, L. S. Zettlemoyer, andR.
Barzilay.
Reinforcement learning for mappinginstructions to actions.
In Proceedings of ACL,page 82?90, 2009.D.
L. Chen and R. J. Mooney.
Learning to interpretnatural language navigation instructions from ob-servations.
In Proc.
AAAI, 2011.S.
Chernova and M. Veloso.
Interactive policy learn-ing through confidence-based autonomy.
JAIR, 34(1):1?25, 2009.J.
Dzifcak, M. Scheutz, C. Baral, and P. Schermer-horn.
What to do and how to do it: Translatingnatural language directives into temporal and dy-namic logic representation for goal managementand action execution.
In Proc.
IEEE Int?l Conf.on Robotics and Automation (ICRA), pages 4163?4168, 2009.S.
Ekvall and D. Kragic.
Robot learning fromdemonstration: a task-level planning approach.International Journal of Advanced Robotic Sys-tems, 5(3), 2008.S.
Harnad.
The symbol grounding problem.
PhysicaD, 43:335?346, 1990.K.
Hsiao, S. Tellex, S. Vosoughi, R. Kubat, andD.
Roy.
Object schemas for grounding languagein a responsive robot.
Connection Science, 20(4):253?276, 2008.R.
S. Jackendoff.
Semantics and Cognition, pages161?187.
MIT Press, 1983.T.
Kollar, S. Tellex, D. Roy, and N. Roy.
Toward un-derstanding natural language directions.
In Proc.ACM/IEEE Int?l Conf.
on Human-Robot Interac-tion (HRI), pages 259?266, 2010.V.
Kruger, D. Kragic, A. Ude, and C. Geib.
Themeaning of action: A review on action recogni-tion and mapping.
Advanced Robotics, 21(13),2007.P.
Liang, M. I. Jordan, and D. Klein.
Learningdependency-based compositional semantics.
InProc.
Association for Computational Linguistics(ACL), 2011.M.
MacMahon, B. Stankiewicz, and B. Kuipers.Walk the talk: Connecting language, knowledge,and action in route instructions.
In Proc.
Nat?lConf.
on Artificial Intelligence (AAAI), pages1475?1482, 2006.C.
Matuszek, D. Fox, and K. Koscher.
Followingdirections using statistical machine translation.
InProc.
ACM/IEEE Int?l Conf.
on Human-Robot In-teraction (HRI), pages 251?258, 2010.N.
Mavridis and D. Roy.
Grounded situation modelsfor robots: Where words and percepts meet.
In2006 IEEE/RSJ International Conference on In-telligent Robots and Systems, pages 4690?4697.IEEE, Oct. 2006.
ISBN 1-4244-0258-1.A.
K. McCallum.
MALLET: A machine learningfor language toolkit.
http://mallet.cs.umass.edu,2002.P.
Rybski, K. Yoon, J. Stolarz, and M. Veloso.
In-teractive robot task training through dialog and13demonstration.
In Proceedings of HRI, page 56.ACM, 2007.S.
Schaal, A. Ijspeert, and A. Billard.
Computationalapproaches to motor learning by imitation.
Phil.Trans.
R. Soc.
Lond.
B, (358), 2003.M.
Skubic, D. Perzanowski, S. Blisard, A. Schultz,W.
Adams, M. Bugajska, and D. Brock.
Spatiallanguage for human-robot dialogs.
IEEE Trans.on Systems, Man, and Cybernetics, Part C: Appli-cations and Reviews, 34(2):154?167, 2004.
ISSN1094-6977.S.
Tellex, T. Kollar, S. Dickerson, M. Walter,A.
Banerjee, S. Teller, and N. Roy.
Understand-ing natural language commands for robotic navi-gation and mobile manipulation.
In Proc.
AAAI,2011.A.
Vogel and D. Jurafsky.
Learning to follow naviga-tional directions.
In Proc.
Association for Compu-tational Linguistics (ACL), pages 806?814, 2010.T.
Winograd.
Procedures as a Representation forData in a Computer Program for UnderstandingNatural Language.
PhD thesis, Massachusetts In-stitute of Technology, 1971.
Ph.D. thesis.14
