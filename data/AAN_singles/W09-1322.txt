Proceedings of the Workshop on BioNLP, pages 171?178,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsEvaluation of the Clinical Question Answering PresentationYong-Gang Cao John Ely Lamont Antieau Hong YuCollege of Health Sci-encesCarver College ofMedicineCollege of Health Sci-encesCollege of Health Sci-encesUniversity of WisconsinMilwaukeeUniversity of Iowa University of WisconsinMilwaukeeUniversity of WisconsinMilwaukeeMilwaukee, WI53211,USAIowa, IA 52242,USA Milwaukee, WI53211,USAMilwaukee, WI53211,USAyonggang@uwm.edu john-ely@uiowa.eduantieau@uwm.edu hongyu@uwm.eduAbstractQuestion answering is different from infor-mation retrieval in that it attempts to an-swer questions by providing summariesfrom numerous retrieved documents ratherthan by simply providing a list of docu-ments that requires users to do additionalwork.
However, the quality of answers thatquestion answering provides has not beeninvestigated extensively, and the practicalapproach to presenting question answersstill needs more study.
In addition to fac-toid answering using phrases or entities,most question answering systems use a sen-tence-based approach for generating an-swers.
However, many sentences are oftenonly meaningful or understandable in theircontext, and a passage-based presentationcan often provide richer, more coherentcontext.
However, passage-based presenta-tions may introduce additional noise thatplaces greater burden on users.
In thisstudy, we performed a quantitative evalua-tion on the two kinds of presentation pro-duced by our online clinical questionanswering system, AskHERMES(http://www.AskHERMES.org).
The over-all finding is that, although irrelevant con-text can hurt the quality of an answer, thepassage-based approach is generally moreeffective in that it provides richer contextand matching across sentences.1 IntroductionQuestion answering is different from informa-tion retrieval in that it attempts to answer ques-tions by providing summaries from numerousretrieved documents rather than by simply pro-viding a list of documents for preparing the userto do even more exploration.
The presentation ofanswers to questions is a key factor in its effi-ciently meeting the information needs of infor-mation users.While different systems have adopted a varietyof approaches for presenting the results of ques-tion answering, the efficacy of the use of thesedifferent approaches in extracting, summarizing,and presenting results from the biomedical lit-erature has not been adequately investigated.
Inthis paper, we compare the sentence-based ap-proach and the passage-based approach by usingour own system, AskHERMES, which is de-signed to retrieve passages of text from the bio-medical literature in response to ad hoc clinicalquestions.2 Background2.1 Clinical Question CollectionThe National Library of Medicine (NLM) haspublished a collection of 4,653 questions thatcan be freely downloaded from the ClinicalQuestions Collection website1 and includes thequestions below:1http://clinques.nlm.nih.gov/JitSearch.html171Question 1: ?The maximum dose of estradiolvalerate is 20 milligrams every 2 weeks.
Weuse 25 milligrams every month which seems tocontrol her hot flashes.
But is that ade-quate for osteoporosis and cardiovasculardisease prevention?
?Question 2: ?Child has pectus carinatum.
Ra-diologist told Dr. X sometimes there are as-sociated congenital heart problems.
Dr. Xwants to study up on this.
Does the patienthave these associated problems?
?Such examples show that clinicians pose com-plex questions of a far greater sophisticationthan the simple term searches that typical infor-mation retrieval systems require as input.
Ask-HERMES, however, has been designed tohandle such complexity as it encounters it.2.2 Result PresentationIn recent years, there has been an emergence ofnumerous search engines ?
both open domainand domain-specific ?
as well as question an-swering systems, and these systems have em-ployed a variety of methods for presenting theirresults, including the use of metadata, sentences,snippets, and passages.
PubMed (Anon 2009a)and EAGLi (Anon 2009b), for example, use ar-ticle metadata to present their results, and thecombination of title, author name and publica-tion name that they use works like the citation atthe end of a paper to provide users with a gen-eral idea of what the listed article is about.
Onthe other hand, AnswerBus (Anon 2009c) andAnswerEngine (Anon 2009d) extract sentencesfrom relevant articles, then rank and list themone by one to answer the questions that usershave.
In response to a query, Google and othergeneral search engines provide the title of awork plus a snippet of text to provide metadataas well as multiple matching hints from articles.In response to user questions, Start (Anon2009e), Powerset(Anon 2009f) and Ask (Anon2009g) provide a single passage as output, mak-ing them ideal for answering simple questionsbecause they do not require users to access andread extra articles in order to answer the ques-tions they have.Each of these methods of presentation hasstrengths and weaknesses.
First, a strength ofusing metadata is that it provides a way for dis-covering the general idea of an article, but itdoes not explain to a user why the article is rele-vant to the query or question, making it difficultto decide whether it is worth the time and effortto access the listed article to read more.
An ap-proach presenting a single sentence in responseto a query can result in a good answer if the useris lucky but typically provides a limited idea ofwhat the target article contains and demands thatusers access the source of the item to learn more.A snippet-based approach can provide a hint asto why the target article is relevant, but snippetsare limited in that they are composed of seg-ments and usually cannot be read at all; evenpresenting a snippet with metadata as Googledoes is not suitable for adequately answeringmany questions.We propose a passage-based approach in whicheach passage is constructed by coherent sen-tences.
The approach we propose is similar tothat used by Start and Ask, but these systemshave limited knowledge bases and require que-ries to be written using very specific questiontypes.
On the other hand, our system will be ableto answer ad hoc questions (that is, questions notlimited to specific types).
Furthermore, the sys-tem we propose will be oriented toward answer-ing questions in the biomedical community, afield in which automated question answeringand information retrieval and extraction are instrong demand.3 Passage-Based Approach versus Sen-tence-Based ApproachWe define as sentence-based approaches thoseapproaches that return a list of independentlyretrieved and ranked sentences.
Although all thesentences are assumed to be relevant to the ques-tion, there are no assumptions of their relation-ship with each other.
On the other hand, apassage-based approach is defined as one thatreturns a list of independently retrieved andranked passages, each of which can comprisemultiple tightly coupled sentences.The passage-based approach has two benefits:1.
It provides richer context for readingand understanding.2.
It provides greater evidence for relevantranking of the passage by matchingacross sentences.For example, in Figure 1, the passage-based out-put of the top results of AskHERMES pertainsto the question ?What is the difference betweenthe Denver ii and the regular Denver develop-mental screening test??
The first answer is apassage with two sentences; the first sentence inthe passage informs users that there have been172criticisms of the ?Denver DevelopmentalScreening Test,?
and the second sentence showsthat ?Denver II?
addressed several concerns ofthe ?Denver Developmental Screening Test.
?The two sentences indicate that the article willmention several issues that answer the question.And the second passage directly shows the an-swer to the question: The criteria to select Den-ver II and the difference between the two tests.If we use the sentence-based approach (see Fig-ure 2), the sentences in the first passage will beranked very low and might not appear in the re-sults because both of them contain only one ofthe screening tests mentioned in the question.The second passage will be reduced to only thesecond sentence, which is an incomplete answerto the question; consequently, the user may re-main uninformed of the selection criteria be-tween the two screening tests without furtherexamination of the article.
Figure 2 shows thesentence-based output of the same question.
Acomparison of the examples in the figure clearlyshows how the results of the query are affectedby the two approaches.
The first result is incom-plete, and the second and third results are irrele-vant to the question although they have manymatched terms.Figure 1.
AskHERMES?
passage-based output for the question ?What is the difference between the Den-ver ii and the regular Denver developmental screening test?
?Figure 2.
AskHERMES?
sentence-based output for the question ?What is the difference betweenthe Denver ii and the regular Denver developmental screening test?
?173While the results shown in Figures 1 and 2 suggestthat a passage-based approach might be better thana sentence-based approach for question answering,this is not to say that passage-based approaches areinfallible.
Most importantly, a passage-based ap-proach can introduce noisy sentences that place anadditional burden on users as they search for themost informative answers to their questions.
InFigure 3, the first sentence in the output of sen-tence-based approach answers the question.
How-ever, the passage-based approach does not answerthe question until the fourth passage, and when itdoes, it outputs the same core answer sentence thatwas provided in the sentence-based approach.
Ad-ditionally, the core sentence is nested within agroup of sentences that on their own are only mar-ginally relevant to the query and in effect bury theanswer.Figure 3.
An example comparing the sentence-based approach and passage-based approach4 Evaluation DesignTo evaluate whether the passage-based presenta-tion improves question answering, we plugged twodifferent approaches into our real system by mak-ing use of either the passage-based or the sentence-based ranking and presentation unit constructor.Both of them share the same document retrievalcomponent, and they share the same ranking andclustering strategies.
In our system, we used a den-sity-based passage retrieval strategy (Tellex et al2003) and a sequence sensitive ranking strategysimilar to ROUGE (F. Liu and Y. Liu 2008).
Anin-house query-oriented clustering algorithm wasused to construct the order and structure of the fi-nal hierarchical presentation.
The difference be-tween the two approaches is the unit for rankingand presentation.
A passage-based approach takesthe passage as its primary unit, with each passageconsisting of one or more sentences.
Those sen-tences in the passage are extracted from the adja-cent matching sentences in the original article.174To evaluate the difference between the passage-based presentation and sentence-based presenta-tion, we randomly selected 20 questions from4,653 clinical questions.
A physician (Dr. JohnEly) was shown the corresponding passage-basedand sentence-based outputs of every question andwas then asked to judge the relevance of the outputand which output had the higher quality answer.Because physicians have little time in clinical set-tings to be sifting through data, we presented onlythe top five units (sentences or passages) of outputfor every question.Figure 4.
A partial screenshot of AskHERMESillustrating hierarchical clustering based on thequestion ?What is the dose of sporanox?
?For answer extraction, we built a hierarchicalweighted-keyword grouping model (Yu and Cao2008;Yu and Cao 2009).
More specifically, in us-ing this model we group units based on the pres-ence of expanded query-term categories:keywords, keyword synonyms, UMLS concepts,UMLS synonyms, and original words, and we thenprioritize the groups based on their ranking.
Forexample, units that incorporate keywords aregrouped into the first cluster, followed by the clus-ter of units that incorporate keyword synonyms,UMLS concepts, etc.
The units that appear syn-onymous are in the clusters with the same parentcluster.
Figure 4 shows an example of the topbranch of the clusters for the question ?What is thedose of sporanox??
in which the answers are or-ganized by sporanox and dose as well as theirsynonyms.5 Evaluation Result and DiscussionWe classify physician evaluations as being of thefollowing four types and plot their distribution inFigure 5:?
Hard Question: The question is considereddifficult because it is patient-specific orunclear (that is, it is a poorly formed ques-tion), e.g., ?Multiple small ulcers on anklesand buttocks.
No history of bites.
I senthim for a complete blood count (cbc) andblood sugar but I don't know what theseare.??
Failed Question: Neither approach can findany relevant information for the question.?
Passage Better: Passage-based approachpresents more useful information for an-swering the question.?
Sentence Better: Sentence-based approachprovides the same amount of useful infor-mation while reducing the effort requiredby the passage-based approach.FailedQuestion25%PassageBetter40%SentenceBetter15%HardQuestion20%Figure 5.
Distribution of the defined Evaluationcategories175The evaluation data is shown in Table 1.
In ourstudy, the score range is set from 0 to 5 with thevalue 0 referring to answers that are totally irrele-vant to the question and the value 5 meaning thereis enough information to fully answer the question.Our results show that the passage-based approachis better than the sentence-based approach (p-value< 0.05).Table 1.
Quantitative measurement of the answersgenerated by both approaches to the 20 questionsNo.
Passage-basedapproach scoreSentence-basedapproach score1234567891011121314151617181920means.deviation322001330011301201001.151.18100000100022400100000.551.05p-value 0.01Through further analysis of the results, we foundthat 70% of the sentences yielded by the sentence-based approach did not answer the question at all(the score is zero), while this was true for only40% of the output of the passage-based approach.This indicates that the passage-based approach pro-vides more evidence for answering questions byproviding richer context and matching across sen-tences.On the other hand, if the question was too generaland included a plethora of detail and little focus,both approaches failed.
For example, in the ques-tion ?One year and 10-month-old boy removedfrom his home because of parental neglect.
Care-taker says he often cries like he's in pain, possiblyabdominal pain.
Not eating, just drinking liquids,not sleeping.
The big question with him: "is itsomething physical or all adjustment disorder?
"?there is a great deal of description of the boy, and avariety of common symptoms are also provided.AskHERMES found a passage containing all of thefollowing extracted words: ?availability, because,before, between, changes, children, decrease, dis-order/disorders, drug, eating, going, increase, indi-cations/reasons, intake, laboratory, level, may,often, one, patient/patients, physical, recom-mended, routinely, specific, still, symp-tom/symptoms, two, urine, used, women,treat/treated/treating/therapy/treatment/treatments,and work.?
But since these words are so commonlyused in a variety of scenarios, the output passage isoff-topic.For very simple questions, the sentence-based ap-proach works well for providing answers in a veryconcise form.
For example, the question ?what isthe dose of zyrtec for a 3-year-old??
can be an-swered by the dosage amount for the target agegroup, and the query resulted in this answer:?
?children of both sexes aged between 2 to 6years with allergy rhinitis (AR) were included inthis study, who were randomly selected to betreated with Zyrtec (Cetirizine 2 HCL) drops 5 mgdaily for 3 weeks.?
From a literal view, this lookslike an answer to the question because it discussesthe dosage of Zyrtec for the specific age group;however, it actually describes an experiment anddoes not necessarily provide the suggested dosagethat the user is seeking.
This leads to an interestingproblem for clinical question answering: howshould experimental data be distinguished fromsuggestion data for recommended daily usage?People tend to ask for the best answer instead ofthe possible answers.
This is one of the main rea-sons why in Table 1, there is no perfect score (5).Our result looks similar to the conclusion of Lin etal (Jimmy Lin et al 2003), whose study on open-domain factoid question answering indicates apreference among users for the answer-in-paragraph approach rather than  the three othertypes of presentation: exact-answer (that is, answerentity), answer-in-sentence, and answer-in-176document.
The results of both Lin?s research andour own indicate the usefulness of context, butLin?s work focuses on how surrounding contexthelps users to understand and become confident inanswers retrieved by simple open-domain queries,while our research reveals that adjacent sentencescan improve the quality of answers retrieved usingcomplex clinical questions.
Our results also indi-cate that context is important for relevance rank-ing, which has not been thoroughly investigated inprevious research.
Furthermore, our work placesemphasis on proper passage extraction from thedocument or paragraph because irrelevant contextcan also be a burden to users, especially for physi-cians who have limited time for reading throughirrelevant text.
Our continuous sentence-based pas-sage extraction method works well for our study,but other approaches should be investigated to im-prove the passage-based approach.With respect to the quality of the answer, the con-tent of the output is not the only important issue.Rather, the question itself and the organization ofcontent are also important issues to consider.
Luoand Tang (Luo and Tang 2008) proposed an itera-tive user interface to capture the information needsof users to form structured queries with the assis-tance of a knowledge base, and this kind of ap-proach guides users toward a clearer and moreformal representation of their questions.
DynaCat(Pratt and Fagan 2000) also uses a knowledge-based approach to organize search results.
Thus,applying domain-specific knowledge is promisingfor improving the quality of an answer, but the dif-ficulty of the knowledge-based approach is thatbuilding and updating such knowledge bases ishuman labor intensive, and furthermore, a knowl-edge-based approach restricts the usage of the sys-tem.6 Conclusion and Future WorkIn this study, we performed a quantitative evalua-tion on the two kinds of presentation produced byour online clinical question answering system,AskHERMES.
Although there is some indicationthat sentence-based passages are more effective forsome question types, the overall finding is that byproviding richer context and matching across sen-tences, the passage-based approach is generally amore effective approach for answering questions.Compared to Lin?s study on open-domain factoidquestions (Jimmy Lin et al 2003), our study ad-dresses the usefulness of context for answeringcomplex clinical questions and its ability to im-prove answer quality instead of just adding sur-rounding context to the specific answer.While conducting this investigation, we noticedthat simple continuous sentence-based passageconstructions have limitations in that they have nosemantic boundary and will form too long a pas-sage if the question contains many common words.Therefore, we will take advantage of recent ad-vances we have made in HTML page analysiscomponents to split documents into paragraphs anduse the paragraph as the maximum passage, that is,a passage will only group sentences that appear inthe same paragraph.
Furthermore, by setting theboundary at a single paragraph, we can loosen theadjacency criterion of our current approach, whichrequires that the sentences in a passage be next toeach other in the original source, and instead adopta requirement that they only be in the same para-graph.
This will enable us to build a model consist-ing of one or more core sentences as well asseveral satellite sentences that could be used tomake the answer more complete or understandable.AcknowledgmentsThe authors acknowledge support from the Na-tional Library of Medicine to Hong Yu, grantnumber 1R01LM009836-01A1.
Any opinions,findings, or recommendations are those of the au-thors and do not necessarily reflect the views of theNIH.ReferencesAnon.
2009a.
PubMed Home.http://www.ncbi.nlm.nih.gov/pubmed/ (Ac-cessed: 10.
March 2009).Anon.
2009b.
EAGLi: the EAGL project's biomedicalquestion answering and information retrievalinterface.
http://eagl.unige.ch/EAGLi/ (Ac-cessed: 6.
March 2009).Anon.
2009c.
AnswerBus Question Answering System.http://www.answerbus.com/index.shtml (Ac-cessed: 6.
March 2009).Anon.
2009d.
Question Answering Engine.http://www.answers.com/bb/ (Accessed: 6.March 2009).177Anon.
2009e.
The START Natural Language QuestionAnswering System.
http://start.csail.mit.edu/(Accessed: 6.
March 2009).Anon.
2009f.
Powerset.
http://www.powerset.com/ (Ac-cessed: 19.
April 2009).Anon.
2009g.
Ask.com Search Engine - Better WebSearch.
http://www.ask.com/ (Accessed: 6.March 2009).Lin, Jimmy, Dennis Quan, Vineet Sinha, Karun Bakshi,David Huynh Boris, Boris Katz and David RKarger.
2003.
What Makes a Good Answer?The Role of Context in Question AnsweringJimmy Lin, Dennis Quan, Vineet Sinha, KarunBakshi, PROCEEDINGS OF INTERACT 2003:25--32. doi:10.1.1.4.7644, .Liu, F. and Y. Liu.
2008.
Correlation between rouge andhuman evaluation of extractive meeting sum-maries.
In: The 46th Annual Meeting of the As-sociation for Computational Linguistics:Human Language Technologies (ACL-HLT2008).Luo, Gang and Chunqiang Tang.
2008.
On iterativeintelligent medical search.
In: Proceedings ofthe 31st annual international ACM SIGIR con-ference on Research and development in in-formation retrieval, 3-10.
Singapore,Singapore: ACM.doi:10.1145/1390334.1390338,http://portal.acm.org/citation.cfm?id=1390338(Accessed: 13.
March 2009).Pratt, Wanda and Lawrence Fagan.
2000.
The Useful-ness of Dynamically Categorizing Search Re-sults.
Journal of the American MedicalInformatics Association 7, Nr.
6 (December):605?617.Tellex, S., B. Katz, J. Lin, A. Fernandes and G. Marton.2003.
Quantitative evaluation of passage re-trieval algorithms for question answering.
In:Proceedings of the 26th annual internationalACM SIGIR conference on Research and de-velopment in informaion retrieval, 41-47.ACM New York, NY, USA.Yu, Hong and Yong-Gang Cao.
2008.
Automaticallyextracting information needs from ad hoc clini-cal questions.
AMIA ...
Annual SymposiumProceedings / AMIA Symposium.
AMIA Sym-posium: 96-100.Yu, Hong and Yong-Gang Cao.
2009.
Using theweighted keyword models to improve informa-tion retrieval for answering biomedical ques-tions.
In: To appear in AMIA Summit onTranslational Bioinformatics.178
