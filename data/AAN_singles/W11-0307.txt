Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 49?57,Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational LinguisticsSearch-based Structured Prediction applied to Biomedical Event ExtractionAndreas Vlachos and Mark CravenDepartment of Biostatistics and Medical InformaticsUniversity of Wisconsin-Madison{vlachos,craven}@biostat.wisc.eduAbstractWe develop an approach to biomedical eventextraction using a search-based structured pre-diction framework, SEARN, which convertsthe task into cost-sensitive classification taskswhose models are learned jointly.
We showthat SEARN improves on a simple yet strongpipeline by 8.6 points in F-score on theBioNLP 2009 shared task, while achieving thebest reported performance by a joint inferencemethod.
Additionally, we consider the issue ofcost estimation during learning and present anapproach called focused costing that improvesimproves efficiency and predictive accuracy.1 IntroductionThe term biomedical event extraction is used to re-fer to the task of extracting descriptions of actionsand relations involving one or more entities fromthe biomedical literature.
The recent BioNLP 2009shared task (BioNLP09ST) on event extraction (Kimet al, 2009) focused on event types of varying com-plexity.
Each event consists of a trigger and one ormore arguments, the latter being proteins or otherevents.
Any token in a sentence can be a trigger forone of the nine event types and, depending on theirassociated event types, triggers are assigned appro-priate arguments.
Thus, the task can be viewed asa structured prediction problem in which the outputfor a given instance is a (possibly disconnected) di-rected acyclic graph (not necessarily a tree) in whichvertices correspond to triggers or protein arguments,and edges represent relations between them.Despite being a structured prediction task, most ofthe systems that have been applied to BioNLP09STto date are pipelines that decompose event extrac-tion into a set of simpler classification tasks.
Clas-sifiers for these tasks are typically learned indepen-dently, thereby ignoring event structure during train-ing.
Typically in such systems, the relationshipsamong these tasks are taken into account by incor-porating post-processing rules that enforce certainconstraints when combining their predictions, andby tuning classification thresholds to improve the ac-curacy of joint predictions.
Pipelines are appealingas they are relatively easy to implement and they of-ten achieve state-of-the-art performance (Bjorne etal., 2009; Miwa et al, 2010).Because of the nature of the output space, the taskis not amenable to sequential or grammar-based ap-proaches (e.g.
linear CRFs, HMMs, PCFGs) whichemploy dynamic programming in order to do ef-ficient inference.
The only joint inference frame-work that has been applied to BioNLP09ST to dateis Markov Logic Networks (MLNs) (Riedel et al,2009; Poon and Vanderwende, 2010).
However,MLNs require task-dependent approximate infer-ence and substantial computational resources in or-der to achieve state-of-the-art performance.In this work we explore an alternative joint in-ference approach to biomedical event extraction us-ing a search-based structured prediction framework,SEARN (Daume?
III et al, 2009).
SEARN is analgorithm that converts the problem of learning amodel for structured prediction into learning a setof models for cost-sensitive classification (CSC).CSC is a task in which each training instance hasa vector of misclassification costs associated with it,thus rendering some mistakes on some instances tobe more expensive than others (Domingos, 1999).Compared to a standard pipeline, SEARN is able to49achieve better performance because its models arelearned jointly.
Thus, each of them is able to use fea-tures representing the predictions made by the oth-ers, while taking into account possible mistakes.In this paper, we make the following contribu-tions.
Using the SEARN framework, we develop ajoint inference approach to biomedical event extrac-tion.
We evaluate our approach on the BioNLP09STdataset and show that SEARN improves on a simpleyet strong pipeline by 8.6 points in F-score, whileachieving the best reported performance on the taskby a joint inference method.
Additionally, we con-sider the issue of cost estimation and present an ap-proach called focused costing that improves perfor-mance.
We believe that these contributions are likelyto be relevant to applications of SEARN to othernatural language processing tasks that involve struc-tured prediction in complex output spaces.2 BioNLP 2009 shared task descriptionBioNLP09ST focused on the extraction of eventsinvolving proteins whose names are annotated inadvance.
Each event has two types of arguments,Theme and Cause, which correspond respectively tothe Agent and Patient roles in semantic role label-ing (Gildea and Jurafsky, 2002).
Nine event typesare defined which can be broadly classified in threecategories, namely Simple, Binding and Regulation.Simple events include Gene expression, Transcrip-tion, Protein catabolism, Phosphorylation, and Lo-calization events.
These have only one Theme ar-gument which is a protein.
Binding events haveone or more protein Themes.
Finally, Regulationevents, which include Positive regulation, Nega-tive regulation and Regulation, have one obligatoryTheme and one optional Cause, each of which canbe either a protein or another event.
Each event hasa trigger which is a contiguous string that can spanover one or more tokens.
Triggers and argumentscan be shared across events.
In an example demon-strating the complexity of the task, given the passage?.
.
.
SQ 22536 suppressed gp41-induced IL-10 pro-duction in monocytes?, systems should extract thethree appropriately nested events listed in Fig.
1d.Performance is measured using Recall, Precisionand F-score over complete events, i.e.
the trigger,the event type and the arguments all must be correctin order to obtain a true positive.
It is important tonote that if either the trigger, the type, or an argu-ment of a predicted event is incorrect then this eventwill result in one false positive and one false nega-tive.
In the example of Fig.
1, if ?suppressed?
is rec-ognized incorrectly as a Regulation trigger then it isbetter to not assign a Theme to it so that we avoida false positive due to extracting an event with in-correct type.
Finally, the evaluation ignores triggersthat do not form events.3 Event extraction decompositionFigure 1 describes the event extraction decomposi-tion that we use throughout the paper.
We assumethat the sentences to be processed are parsed intosyntactic dependencies and lemmatized.
Each stagehas its own module, which is either a learned classi-fier (trigger recognition, Theme/Cause assignment)or a rule-based component (event construction).3.1 Trigger recognitionIn trigger recognition the system decides whether atoken acts as a trigger for one of the nine event typesor not.
Thus it is a 10-way classification task.
Weonly consider tokens that are tagged as nouns, verbsor adjectives by the parser, as they cover the majorityof the triggers in the BioNLP09ST data.
The mainfeatures used in the classifier represent the lemmaof the token which is sufficient to predict the eventtype correctly in most cases.
In addition, we includefeatures that conjoin each lemma with its part-of-speech tag.
This allows us to handle words withthe same nominal and verbal form that have differ-ent meanings, such as ?lead?.
While the domainrestricts most lemmas to one event type, there aresome whose event type is determined by the context,e.g.
?regulation?
on its own denotes a Regulationevent but in ?positive regulation?
it denotes a Posi-tive regulation event instead.
In order to capture thisphenomenon, we add as features the conjunction ofeach lemma with the lemma of the tokens immedi-ately surrounding it, as well as with the lemmas ofthe tokens with which it has syntactic dependencies.3.2 Theme and Cause assignmentIn Theme assignment, we form an agenda of can-didate trigger-argument pairs for all trigger-proteincombinations in the sentence and classify them as50SQ 22536 suppressedNeg reggp41-inducedPos regIL-10 productionGene exp(a) Trigger recognitionSQ 22536 suppressedNeg reggp41-inducedPos regIL-10 productionGene expThemeThemeTheme(b) Theme assignmentSQ 22536 suppressedNeg reggp41-inducedPos regIL-10 productionGene expThemeThemeCauseTheme(c) Cause assignmentID type Trigger Theme CauseE1 Neg reg suppressed E2E2 Pos reg induced E3 gp41E3 Gene exp production IL-10(d) Event constructionFigure 1: The stages of our event extraction decomposition.
Protein names are shown in bold.Themes or not.
Whenever a trigger is predicted to beassociated with a Theme, we form candidate pairsbetween all the Regulation triggers in the sentenceand that trigger as the argument, thus allowing theprediction of nested events.
Also, we remove candi-date pairs that could result in directed cycles, as theyare not allowed by the task.The features used to predict whether a trigger-argument pair should be classified as a Theme areextracted from the syntactic dependency path andthe textual string between them.
In particular, weextract the shortest unlexicalized dependency pathconnecting each trigger-argument pair, allowing thepaths to follow either dependency direction.
One setof features represents these paths, and in addition,we have sets of features representing each path con-joined with the lemma, the PoS tag and the eventtype of the trigger, the type of the argument andthe first and last lemmas in the dependency path.The latter help by providing some mild lexicaliza-tion.
We also add features representing the textualstring between the trigger and the argument, com-bined with the event type of the trigger.
While not asinformative as dependency paths, such features helpin sentences where the parse is incorrect, as triggersand their arguments tend to appear near each other.In Cause assignment, we form an agenda of can-didate trigger-argument pairs using only the Regu-lation class triggers that were assigned at least oneTheme.
These are combined with protein names andother triggers that were assigned a Theme.
We ex-tract features as in Theme assignment, further fea-tures representing the conjunction of the dependencypath of the candidate pair with the path(s) from thetrigger to its Theme(s).3.3 Event constructionIn event construction, we convert the predictions ofthe previous stages into a set of legal events.
Ifa Binding trigger is assigned multiple Themes, wechoose to form either one event per Theme or oneevent with multiple Themes.
Following Bjorne etal.
(2009), we group the arguments of each Bindingtrigger according to the first label in their syntac-tic dependency path and generate events using thecross-product of these groups.
For example, assum-ing the parse was correct and all the Themes recog-nized, ?interactions of A and B with C?
results intwo Binding events with two Themes each, A withC, and B with C respectively.
We add the exceptionthat if two Themes are in the same token (e.g.
?A/Binteractions?)
or the lemma of the trigger is ?bind?then they form one Binding event with two Themes.4 Structured prediction with SEARNSEARN (Daume?
III et al, 2009) forms the struc-tured output prediction for an instance s as a se-quence of T multiclass predictions y?1:T made by ahypothesis h. The latter consists of a set of classi-fiers that are learned jointly.
Each prediction y?t canuse features from s as well as from all the previouspredictions y?1:t?1.
These predictions are referred to51as actions and we adopt this term in order to distin-guish them from the structured output predictions.The SEARN algorithm is presented in Alg.
1.
Itinitializes hypothesis h to the optimal policy pi (step2) which predicts the optimal action in each stept according to the gold standard.
The optimal ac-tion at step t is the one that minimizes the overallloss over s assuming that all future actions y?t+1:Tare also made optimally.
The loss function ` is de-fined by the structured prediction task considered.Each iteration begins by making predictions for allinstances s in the training data S (step 6).
For eachs and each action y?t, a cost-sensitive classification(CSC) example is generated (steps 8-12).
The fea-tures are extracted from s and the previous actionsy?1:t?1 (step 8).
The cost for each possible actionyit is estimated by predicting the remaining actionsy?t+1:T in s using h (step 10) and evaluating the costincurred given that action (step 11).
Using a CSClearning algorithm, a new hypothesis is learned (step13) which is combined with the current one accord-ing to the interpolation parameter ?.Algorithm 1 SEARN1: Input: labeled instances S , optimal policy pi, CSClearning algorithm CSCL, loss function `2: current policy h = pi3: while h depends significantly on pi do4: Examples E = ?5: for s in S do6: Predict h(s) = y?1:T7: for y?t in h(s) do8: Extract features ?t = f(s, y?1:t?1)9: for each possible action yit do10: Predict y?t+1:T = h(s|y?1:t?1, yit)11: Estimate cit = `(y?1:t?1, yit, y?t+1:T )12: Add (?t, ct) to E13: Learn a hypothesis hnew = CSCL(E)14: h = ?hnew + (1?
?
)h15: Output: policy h without piIn each iteration, SEARN moves away from theoptimal policy and uses the learned hypotheses in-stead when predicting (steps 6 and 10).
Thus, eachhnew is adapted to the actions chosen by h insteadof those of the optimal policy.
When the depen-dence on the latter becomes insignificant, the algo-rithm terminates and returns the weighted ensembleof learned hypotheses without the optimal policy.Note though that the estimation of the costs in step11 is always performed using the gold standard.The interpolation parameter ?
determines howfast SEARN moves away from the optimal policyand as a result how many iterations will be needed tominimize the dependence on it.
Dependence in thiscontext refers to the probability of using the optimalpolicy instead of the learned hypothesis in choos-ing an action during prediction.
In each iteration,the features extracted ?t are progressively corruptedwith the actions chosen by the learned hypothesesinstead of those of the optimal policy.Structural information under SEARN is incorpo-rated in two ways.
First, via the costs that are es-timated using the loss over the instance rather thanisolated actions (e.g.
in PoS tagging, the loss wouldbe the number of incorrect PoS tags predicted ina sentence if a token is tagged as noun).
Second,via the features extracted from the previous actions(y?1:t?1) (e.g.
the PoS tag predicted for the previ-ous token can be a feature).
These types of featuresare possible in a standard pipeline as well, but dur-ing training they would have to be extracted usingthe gold standard instead of the actual predictionsmade by the learned hypotheses, as during testing.Since the prediction for each instance (y?1:T in step6) changes in every iteration, the structure featuresused to predict the actions have to be extracted anew.The extraction of features from previous actionsimplies a search order.
For some tasks, such as PoStagging, there is a natural left-to-right order in whichthe tokens are treated, however for many tasks thisis not the case.Finally, SEARN can be used to learn a pipeline ofindependently trained classifiers.
This is achievedusing only one iteration in which the cost for eachaction is set to 0 if it follows from the gold standardand to 1 otherwise.
This adaptation allows for a faircomparison between SEARN and a pipeline.5 SEARN for biomedical event extractionIn this section we discuss how we learn the eventextraction decomposition described in Sec.
3 underSEARN.
Each instance is a sentence consisting ofthe tokens, the protein names and the syntactic pars-ing output.
The hypothesis learned in each iterationconsists of a classifier for each stage of the pipeline,52excluding event construction which is rule-based.Unlike PoS tagging, there is no natural orderingof the actions in event extraction.
Ideally, the ac-tions predicted earlier should be less dependent onstructural features and/or easier so that they can in-form the more structure dependent/harder ones.
Intrigger recognition, we process the tokens from leftto right since modifiers appearing before nouns tendto affect the meaning of the latter, e.g.
?binding ac-tivity?.
In Theme and Cause assignment, we predicttrigger-argument pairs in order of increasing depen-dency path length, assuming that since dependencypaths are the main source of features at this stage andshorter paths are less sparse, pairs containing shorterones should be more reliable to predict.In addition to the features mentioned in Sec.
3,SEARN allows us to extract and learn weights forstructural features for each action from the previousones.
During trigger recognition, we add as featuresthe combination of the lemma of the current tokencombined with the event type (if any) assigned tothe previous and the next token, as well as to the to-kens that have syntactic dependencies with it.
Dur-ing Theme assignment, when considering a trigger-argument pair, we add features based on whether itforms an undirected cycle with previously predictedThemes, whether the trigger has been assigned a pro-tein as a Theme and the candidate Theme is an eventtrigger (and the reverse) and whether the argumenthas become the Theme of a trigger with the sameevent type.
We also add a feature indicating whetherthe trigger has three Themes predicted already.
Dur-ing Cause assignment, we add features representingwhether the trigger has been assigned a protein as aCause and the candidate Cause is an event trigger.The loss function ` sums the number of false pos-itive and false negative events, which is the evalua-tion measure of BioNLP09ST.
The optimal policy isderived from the gold standard and returns the ac-tion that minimizes this loss over the sentence giventhe previous actions and assuming that all future ac-tions are optimal.
In trigger recognition, it returnseither the event type for tokens that are triggers or a?notrigger?
label otherwise.
In Theme assignment,for a given trigger-argument pair the optimal policyreturns Theme only if the trigger is recognized cor-rectly and the argument is indeed a Theme for thattrigger according to the gold standard.
In case the ar-gument is another event, we require that at least oneof its Themes to be recognized correctly as well.
InCause assignment, the requirements are the same asthose for the Themes, but we also require that at leastone Theme of the trigger in the trigger-argument pairto be considered correct.
These additional checksfollow from the task definition, under which eventsmust have all their elements identified correctly.5.1 Cost estimationCost estimation (steps 5-12 in Alg.
1) is crucial tothe successful application of SEARN.
In order tohighlight its importance, consider the example ofFig.
2 focusing on trigger recognition.In the first iteration (Fig.
2a), the actions for thesentence will be made using the optimal policy only,thus replicating the gold standard.
During costing,if a token is not a trigger according to the gold stan-dard (e.g.
?SQ?
), then the cost for incorrectly pre-dicting that it is a trigger is 0, as the optimal policywill not assign Themes to a trigger with incorrectevent type.
Such instances are ignored by the cost-sensitive learner.
If a token is a trigger according tothe gold standard, then the cost for not predicting itas such or predicting its type incorrectly is equal tothe number of the events that are dependent on it, asthey will become false negatives.
False positives areavoided as we are using the optimal policy in thisiteration.In the second iteration (Fig.
2b), the optimal pol-icy is interpolated with the learned hypothesis, thussome of the actions are likely to be incorrect.
As-sume that ?SQ?
is incorrectly predicted to be aNeg reg trigger and assigned a Theme.
During cost-ing, the action of labeling ?SQ?
as Neg reg has acost of 1, as it would result in a false positive event.Thus the learned hypothesis will be informed that itshould not label ?SQ?
as a trigger as it would assignThemes to it incorrectly and it is adapted to handleits own mistakes.
Similarly, the action of labeling?production?
as Neg reg in this iteration would in-cur a cost of 6, as the learned hypothesis would as-sign a Theme incorrectly, thus resulting in 3 falsenegative and 3 false positive events.
Therefore, thelearned hypothesis will be informed that assigningthe wrong event type to ?production?
is worse thannot predicting a trigger.By evaluating the cost of each action according to53SQ 22536 suppressedNeg reggp41-inducedPos regIL-10 productionGene expThemeThemeCauseThemetoken No Gene exp Pos reg Neg regSQ 0 0 0 0suppressed 1 1 1 0-induced 2 2 0 2production 3 0 3 3(a) First iteration (optimal policy only)SQNeg reg22536 suppressedNeg reggp41-inducedPos regIL-10 productionNeg regThemeThemeCauseThemeThemetoken No Gene exp Pos reg Neg regSQ 0 0 0 1suppressed 1 1 1 0-induced 2 2 0 2production 3 0 3 6(b) Second iteration (interpolation)Figure 2: Prediction (top) and CSC examples for trigger recognition actions (bottom) in the first two SEARNiterations.
Each CSC example has its own vector of misclassification costs.its effect on the prediction for the whole sentence,we are able to take into account steps in the pre-diction process that are not learned as actions.
Forexample, if the Binding event construction heuris-tic described in Sec.
3.3 cannot produce the correctevents for a token that is a Binding trigger despitethe Themes being assigned correctly, then this willincrease the cost for tagging that trigger as Binding.The interpolation between the optimal policy andthe learned hypothesis is stochastic, thus affectingthe cost estimates obtained.
In order to obtain morereliable estimates, one can average multiple sam-ples for each action by repeating steps 10 and 11of Alg.
1.
However, the computational cost is effec-tively multiplied by the number of samples.In step 11 of Alg.
1, the cost of each action is esti-mated over the whole sentence.
While this allows usto take structure into account, it can result in costsbeing affected by a part of the output that is not re-lated to that action.
This is likely to occur in eventextraction, as sentences can often be long and con-tain disconnected event components in their outputgraphs.
For this reason, we refine the cost estimationof each action to take into account only the eventsthat are connected to it through either gold standardor predicted events.
For example, in Fig.
2 the costestimation for ?SQ?
will ignore the predicted eventsin the first iteration and the gold standard, while itwill take them into account in the second one.
Werefer to this refinement as focused costing.A different approach proposed by Daume?
III etal.
(2009) is to assume that all actions following theone we are costing are going to be optimal and usethe optimal policy to approximate the prediction ofthe learned hypothesis in step 10 of Alg.
1.
In taskswhere the learned hypothesis is accurate enough,this has no performance loss and it is computation-ally efficient as the optimal policy is deterministic.However, in event extraction the learned hypothesisis likely to make mistakes, thus the optimal policydoes not provide a good approximation for it.5.2 CSC learning with passive-aggressivealgorithmsThe SEARN framework requires a multiclass CSCalgorithm to learn how to predict actions.
This algo-rithm must be computationally fast during parameterlearning and prediction, as in every iteration we needto learn a new hypothesis and to consider each pos-sible action for each instance in order to constructthe cost-sensitive examples.
Daume?
III et al (2009)showed that any binary classification algorithm canbe used to perform multiclass CSC by employing anappropriate conversion between the tasks.
The maindrawback of this approach is its reliance on multi-ple subsamplings of the training data, which can beinefficient for large datasets and many classes.With these considerations in mind, we implementa multiclass CSC learning algorithm using the gen-eralization of the online passive-aggressive (PA) al-gorithm for binary classification proposed by Cram-mer et al (2006).
For each training example xt,the K-class linear classifier with K weight vectorsw(k)t makes a prediction y?t and suffers a loss `t. In54the case of multiclass CSC learning, each examplehas its own cost vector ct.
If the loss is 0 then theweight vectors of the classifier are not updated (pas-sive).
Otherwise, the weight vectors are updatedminimally so that the prediction on example xt iscorrected (aggressive).
The update takes into ac-count the loss and the aggressiveness parameter C.Crammer et al (2006) describe three variants to per-form the updates which differ in how the learningrate ?t is set.
In our experiments we use the variantnamed PA-II with prediction-based updates (Alg.
2).Since we are operating in a batch learning setting(i.e.
we have access to all the training examples andtheir order is not meaningful), we perform multiplerounds over the training examples shuffling their or-der, and average the weight vectors obtained.Algorithm 2 Passive-aggressive CSC learning1: Input: training examples X = x1 .
.
.
xT , cost vec-tors c1 .
.
.
cT ?
0, rounds R, aggressiveness C2: Initialize weights w(k)0 = (0, ..., 0)3: for r = 1, ..., R do4: Shuffle X5: for xt ?
X do6: Predict y?t = argmaxk(w(k)t ?
xt)7: Receive cost vector ct ?
08: if c(y?t)t > 0 then9: Suffer loss `t = w(y?t)t ?xt?w(yt)t ?xt+?c(y?t)t10: Set learning rate ?t =`t||xt||2+ 12C11: Update w(yt)t+1 = wt + ?txt12: Update w(y?t)t+1 = wt ?
?txt13: Average wavg = 1T?R?T?Ri=0 wi6 ExperimentsBioNLP09ST comprises three datasets ?
training,development and test ?
which consist of 800, 150and 260 abstracts respectively.
After the endof the shared task, an on-line evaluation serverwas activated in order to allow the evaluation onthe test data once per day, without allowing ac-cess to the data itself.
We report results usingRecall/Precision/F-score over complete events usingthe approximate span matching/approximate recur-sive matching variant which was the primary perfor-mance criterion in BioNLP09ST.
This variant countsa predicted event as a true positive if its trigger isextracted within a one-token extension of the gold-standard trigger.
Also, in the case of nested events,those events below the top-level need their trigger,event type and Theme but not their Cause to be cor-rectly identified for the top-level event to be consid-ered correct.
The same event matching variant wasused in defining the loss as described in Sec.
5.A pre-processing step we perform on the train-ing data is to reduce the multi-token triggers in thegold standard to their syntactic heads.
This proce-dure simplifies the task of assigning arguments totriggers and, as the evaluation variant used allowsapproximate trigger matching, it does not result ina performance loss.
For syntactic parsing, we usethe output of the BLLIP re-ranking parser adapted tothe biomedical domain by McClosky and Charniak(2008), as provided by the shared task organizersin the Stanford collapsed dependency format withconjunct dependency propagation.
Lemmatizationis performed using morpha (Minnen et al, 2001).In all our experiments, for CSC learning with PA,the C parameter is set by tuning on 10% of the train-ing data and the number of rounds is fixed to 10.
ForSEARN, we set the interpolation parameter ?
to 0.3and the number of iterations to 12.
The costs foreach action are obtained by averaging three samplesas described in Sec.
5.1. ?
and the number of sam-ples are the only parameters that need tuning and weuse the development data for this purpose.First we compare against a pipeline of indepen-dently learned classifiers obtained as described inSec.
4 in order to assess the benefits of joint learningunder SEARN using focused costing.
The resultsshown in Table 1 demonstrate that SEARN obtainsbetter event extraction performance on both the de-velopment and test sets by 7.7 and 8.6 F-score pointsrespectively.
The pipeline baseline employed in ourexperiments is a strong one: it would have rankedfifth in BioNLP09ST and it is 20 F-score points bet-ter than the baseline MLN employed by Poon andVanderwende (2010).
Nevertheless, the indepen-dently learned classifier for triggers misses almosthalf of the event triggers, from which the subsequentstages cannot recover.
On the other hand, the trig-ger classifier learned with SEARN overpredicts, butsince the Theme and Cause classifiers are learnedjointly with it they maintain relatively high precisionwith substantially higher recall compared to their in-55pipeline SEARN focus SEARN defaultR P F R P F R P Ftriggerdev 53.0 61.1 56.8 81.8 34.2 48.2 84.9 12.0 21.0Themedev 44.2 79.6 56.9 62.0 69.1 65.4 59.0 65.1 61.9Causedev 18.1 59.2 27.8 30.6 45.0 36.4 31.9 45.5 37.5Eventdev 35.8 68.9 47.1 50.8 59.5 54.8 47.4 54.3 50.6Eventtest 30.8 67.4 42.2 44.5 59.1 50.8 41.3 53.6 46.6Table 1: Recall / Precision / F-score on BioNLP09ST development and test data.
Left-to-right: pipeline ofindependently learned classifiers, SEARN with focused costing, SEARN with default costing.dependently learned counterparts.
The benefits ofSEARN are more pronounced in Regulation eventswhich are more complex.
For these events, it im-proves on the pipeline on both the development andtest sets by 11 and 14.2 F-score points respectively.The focused costing approach we proposed con-tributes to the success of SEARN.
If we replace itwith the default costing approach which uses thewhole sentence, the F-score drops by 4.2 points onboth development and test datasets.
The defaultcosting approach mainly affects the trigger recog-nition stage, which takes place first.
Trigger over-prediction is more extreme in this case and rendersthe Theme assignment stage harder to learn.
Whilethe joint learning of the classifiers ameliorates thisissue and the event extraction performance is even-tually higher than that of the pipeline, the use of fo-cused costing improves the performance even fur-ther.
Note that trigger overprediction also makestraining slower, as it results in evaluating more ac-tions for each sentence.
Finally, using one insteadof three samples per action decreases the F-score by1.3 points on the development data.Compared with the MLN approaches applied toBioNLP09ST, our predictive accuracy is better thanthat of Poon and Vanderwende (2010) which is thebest joint inference performance to date and substan-tially better than that of Riedel et al (2009) (50 and44.4 in F-score respectively).
Recently, McCloskyet al (2011) combined multiple decoders for a de-pendency parser with a reranker, achieving 48.6 inF-score.
While they also extracted structure fea-tures for Theme and Cause assignment, their modelis restricted to trees (ours can output directed acyclicgraphs) and their trigger recognizer is learned inde-pendently.When we train SEARN combining the trainingand the development sets, we reach 52.3 in F-score,which is better than the performance of the topsystem in BioNLP09ST (51.95) by Bjorne et al(2009) which was trained in the same way.
Thebest performance to date is reported by Miwa et al(2010) (56.3 in F-score), who experimented with sixparsers, three dependency representations and vari-ous combinations of these.
They found that differentparser/dependency combinations provided the bestresults on the development and test sets.A direct comparison between learning frame-works is difficult due to the differences in task de-composition and feature extraction.
In particular,event extraction results depend substantially on thequality of the syntactic parsing.
For example, Poonand Vanderwende (2010) heuristically correct thesyntactic parsing used and report that this improvedtheir performance by four F-score points.7 ConclusionsWe developed a joint inference approach to biomed-ical event extraction using the SEARN frameworkwhich converts a structured prediction task into a setof CSC tasks whose models are learned jointly.
Ourapproach employs the PA algorithm for CSC learn-ing and a focused cost estimation procedure whichimproves the efficiency and accuracy of the standardcost estimation method.
Our approach provides thebest reported results for a joint inference method onthe BioNLP09ST task.
With respect to the experi-ments presented by Daume?
III et al (2009), we em-pirically demonstrate the gains of using SEARN ona problem harder than sequential tagging.AcknowledgmentsThe authors were funded by NIH/NLM grant R01 /LM07050.56ReferencesJari Bjorne, Juho Heimonen, Filip Ginter, Antti Airola,Tapio Pahikkala, and Tapio Salakoski.
2009.
Extract-ing complex biological events with rich graph-basedfeature sets.
In Proceedings of the BioNLP 2009Work-shop Companion Volume for Shared Task, pages 10?18.Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer.
2006.
Online passive-aggressive algorithms.
Journal of Machine LearningResearch, 7:551?585.Hal Daume?
III, John Langford, and Daniel Marcu.
2009.Search-based structured prediction.
Machine Learn-ing, 75:297?325.Pedro Domingos.
1999.
Metacost: a general method formaking classifiers cost-sensitive.
In Proceedings ofthe 5th International Conference on Knowledge Dis-covery and Data Mining, pages 155?164.Daniel Gildea and Daniel Jurafsky.
2002.
Automatic la-beling of semantic roles.
Computational Linguistics,28:245?288.Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-nobu Kano, and Jun?ichi Tsujii.
2009.
Overview ofBioNLP?09 shared task on event extraction.
In Pro-ceedings of the BioNLP 2009 Workshop CompanionVolume for Shared Task, pages 1?9.David McClosky and Eugene Charniak.
2008.
Self-training for biomedical parsing.
In Proceedings ofthe 46th Annual Meeting of the Association of Compu-tational Linguistics: Human Language Technologies,pages 101?104.David McClosky, Mihai Surdeanu, and Christopher D.Manning.
2011.
Event extraction as dependency pars-ing.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies.Guido Minnen, John Carroll, and Darren Pearce.
2001.Applied morphological processing of English.
Natu-ral Language Engineering, 7(3):207?223.Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, andJun?ichi Tsujii.
2010.
Evaluating dependency repre-sentation for event extraction.
In Proceedings of the23rd International Conference on Computational Lin-guistics, pages 779?787.Hoifung Poon and Lucy Vanderwende.
2010.
Joint in-ference for knowledge extraction from biomedical lit-erature.
In Proceedings of the Human Language Tech-nologies: The 2010 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, pages 813?821.Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi,and Jun?ichi Tsujii.
2009.
A Markov logic approachto bio-molecular event extraction.
In Proceedings ofthe BioNLP 2009 Workshop Companion Volume forShared Task, pages 41?49.57
