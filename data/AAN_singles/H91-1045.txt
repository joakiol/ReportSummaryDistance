Ca lcu la t ing  the  P robab i l i tyo f  a Par t ia l  Parse  o f  a Sentencebred Kochman and Joseph KupinCenter for Communications ResearchInstitute for Defense AnalysesPrinceton, New Jersey 08540ABSTRACTA standard problem in parsing algorithms i  Lhe organizaLion o~branched searches to deal with an~iguous sentences.
We discuss hift-reduce parsing o?
stochastic ontext-Gee granlnmrs and show how Loconstruct a probabilistic s ore for tanking compeLing parse hypotheses.The score we use is the likelihood Lhag the collection of subtrees canbe completed into a l~ull parse tree by means of the steps the parser isconstrained to \[oUow.INTRODUCTIONStochastic ontext-free grammars have been suggested for arole in speech-recognition algorithms, e.g.
\[1, 4, 9\].
In order tohe fully effective as an adjunct o speech recognition, the powerof the probability apparatus needs to be applied to the problemof controlling the branched search for parses of ambiguous input.The method we suggest for doing this employs hift-reduce(LR) parsing of context-free grammars together with a probabil-ity based score for ranking competing parse hypotheses.
Shift-reduce parsers can be made very efficient for unambiguous gram-mars (and unambiguous inputs) and Tomita \[7\] shows how muchof this efficiency can be maintained in the face of ambiguity.
Thismakes this class of parsers a good candidate for many speechproblems.
The structural simplicity of shift-reduce parsers makesthe analysis of the interaction of the parser with the stochasticproperties of the language particularly clean.The score we calculate is the likelihood that the collection ofsubtrees constructed by the parser so far can he completed into afull parse tree by means of the steps that the parser is constrainedto follow, taking into account all possibilities for the unscannedpart of the input.
This score is the same as that suggested byWright \[9\], who also studied shift-reduce parsers.
We provide anexact method for calculating the desired quantity, while Wright'scalculation requires everal approximations.Why do we care about this particular quantity?
As a firstrough answer, note that when this quantity is zero, then the hy-pothesis hould be abandoned; there is no possibility that theparse tree can he completed.
Furthermore, the bigger this quan-tity is, the larger the mass of the probability space that can beexplored by pursuing that particular hypothesis.For a more detailed answer, consider a breadth first searchof candidate hypotheses.
For each one we would like to knowwhich is the correct one, given the grammar and the text seg-ment we have observed: a,, .
.
.
,at.
We would like to calculateP(Hla,, .
.
.
, a~).This quantity is equal toP(H&al .
.
.
.
.
a~)/P(al,..., a~).The denominator in the above expression P (a , , .
.
.
,  at) is thegrand probability of seeing the observations al , .
.
.
,  at given thegrammar.
This is some fixed quantity.
We might not know whatit is, but as long as we are only comparing hypotheses that allexplain the same string ah .
- - ,  at, this quantity is a scaling factorthat can safely be ignored.
The numerator is the quantity weintend to calculate.For a depth-first or best-first search, as employed by \[1\], thequantity P (a l , .
.
.
,  at) cannot be ignored.
This makes the depth-first approach significantly more complicated.In the rest of this paper we will restrict our attention to gram-mars in Chomsky-normal form.
A similar probability analysis canbe made for arbitrary context-free grammars, but the notation be-comes cumbersome and the formulae more complicated.
We notethat all the topics in this paper are treated in considerably moredetail, including proofs, in \[3\].SHIFT-REDUCE PARSINGA bottom-up arser is one which reconstructs parsing trees byfirst constructing parsing subtrees over short disjoint segmentsof the text, then linking these together into a smaller number oflarger trees, and so on recursively until a single parse tree emerges,covering the entire text.
In this section we study a particular classof bottom-up arsers, called shift-reduce parsers, which conformto the following rules, leading to the reconstruction of a right-most-first derivation of the sentence being parsed.The parser receives ymbols one at a time from left to rightand at each stage of the process, the parser's memory contains asequence of disjoint parsing subtrees which completely cover thecurrent input.
Roughly speaking, as each new symbol is accepted(or shifted-in) the parser decides how to incorporate it into asubtree and perhaps how to link several existing subtrees together(i.e.
reduce).
The sequence of subtrees in the parser's memory ata given instant is called a parse hypothesis, or a parser stack.237To be more precise, here is how a shift-reduce parser updatesthe current hypothesis into a new one.
Consider a parse hy-pothesis consisting of n subtrees: 7"1...
I",,, having root symbolsB1.
.
.
B, ,  respectively.The three possible "moves" for reacting to the next input sym-bol 'z '  are listed below.1.
'z' can be shifted in and declared to be Tn+l.2.
If there is a rule A ~ B ,  in the grammar, then r ,  can bereplaced by a new ~-, having A as a root and old ~-, as theleft child of A.
(Note that 'z '  has not yet been shifted in.)3.
If there is a rule A ~ Bn- iB ,  in the grammar, then r , -1and 7, can be removed from the hypothesis and a new sub-tree 7",,-1 is added, having A as a root and old ~',-1 as theleft child of A and old ~-, as the right child of A.
Again notethat z remains to be shifted in.The "input cycle" of a shift-reduce parser is typically to shiftin a new symbol via move 1, use move 2 to give that symbol anonterminal root, and then to perform some number of moves oftype three.
Choosing which (if any) of the allowable type-twoand type-three rules should be used next in the parse can bequite difficult, but doing so cleverly makes the difference betweenefficient and inefficient parsing algorithms.
When faced with achoice among possible moves some parsers make a breadth-firstsearch among the possibilities.
Others use a depth-first scheme, oreven something intermediate between these two extremes.
We willnot be concerned with such schemes here.
We concern ourselvesonly with a probabilistic score for the plausibihty of availablechoices.
(The best use of that score is a study in its own right.
)The important fact about shift-reduce parsers from our pointof view is that they are quite limited in the kind of superstructurethey can build above a given set of subtrees.
Since new parentnodes can only be generated over the final few subtrees in thehypothesis, one can not "go back" and make non-final pairs ofsubtrees into siblings.
(A precise result is proved in \[3\]).
Figure 1shows the necessary superstructure for an n-subtree hypothesis.In this figure, the ellipses represent sequences of zero or morenodes in which each node is the left child of its parent.
Thediagram is also meant to admit the possibihty that Ai is thesame node as Ci - l .
The right children of the nodes labeled C(labeled X) as well as those in the ellipses are all to be found inthe remaining input.THE LEFT-EDGE PROCESSIn order to calculate the sum of the probabilities of all com-plete parse trees that could result from the parser's further pro-cessing of a given hypothesis, we must sum across all the possi-bilities for the As and the Cs in figure 1 (which is a finite set)as well as summing over the potentially infinite set of sequencesof nodes that could be lurking behind the ellipses.
This soundsprohibitive, but we are saved by the fact that the sequences ofnodes along the left-edge of a tree can be analyzed as the outputof a Markov process.
This fact is implicit in the work on treesand regular sets by \[6\], and was discovered independently by \[2\].Happily, this observation leads to a closed form solution tothe problem of calculating all the necessary probabilities for theb c .
.
.
d .
.
.
w .
.
.
x y .
.
.
zXOX1X2Xn-iFigure 1: A parse hypothesis, wish implied superstrucLureinfinite set of sequences.
To begin, construct he matrix M whichis the transition matrix of a Markov chain in which nonterminalsymbols are transient states and terminal symbols are absorbingstates.M(A,  B) = ~P(A  ~ BC)  for nonterminals BOM(A,  b) -- P (A  ~ b) for terminals bRows of M indexed by terminal symbols are identically zero.To illustrate the use of Markov chain theory for the left-edgesof trees, we compute the probability of the event hat the left edgeof a randomly generated subtree terminates in a specified terminalsymbol a, given that the root is a specified nonterminal symbolA.
This event is the disjoint union of the events that a is the n thsymbol in the left-edge sequence, for all n > 1.
Correspondingly,we want the sumP(the n th left-edge symbol is a I the root is A)n>lwhich is the sum of the (A, a) th entries in the sequence M, M 2,M 3, etc., which is in turn the (A, a) th entry in M+M2+M3+ ...  .As it turns out, this matrix sum converges.
The sum is equal toM( I  - M)  -1.
Thus the number we seek is the (A, a) th entry ofM( I  - M)  -1.
Note our convention that the root is the 0 th symbolalong the left edge of the tree.As another illustration we compute the probability that theleft edge of a subtree T terminates in some specific subtree ~',again given that the root of T is A.
More precisely, we computethe conditional probability that the subtree ~- appears as a subtreeof T, with its root B somewhere in the left-edge of T, given thatthe root of T is A.
This is the disjoint union of the events, as nvaries, that B is the n th symbol in the left edge of T and that Tthen appears rooted at this B.238If (just for a moment) we exclude the possibility that v isidentical to T, then n must be at least 1.
For each n > 1, theconditionM probability that ~- appears rooted at the n m symbolB, is P(r\]B) multiplied by the (A, B) th entry of the n th power ofM.
In this case we can find, much as in the preceding illustration,that the sum from 1 to infinity of these l~obabilities iP(rlB ) x the (A, B) th e~try of M( I -  M) -1.To include the possibility that ~ is identical to T, then wemust add the term:~'(~IB) x P(A = BS.Since the second factor is one or zero depending on whether B =A, the sum of probabilities for all n > 0 isP(v\[B) x the (A, B) ` h entry of \[I + M(I  - M5 -I\]which simplifies to:P( r lB  5 x the (A, B) th entry of (I - M5 -1.
(15In order to calculate the probability of the set of parse treeswhich might complete a given parse hypothesis we will need for-mulas like these, but with the proviso that we need to specify therule that is used to generate the root of ~ from its parent.So to calculate all the probabilities that could ever arise due tothe ellipses, we have work of inverting a rather large, but rathersparse, matrix.
This work is done when the rule probabilitiesare decided upon, and before any sentences are parsed.
The sizeof the matrix depends on the number of symbols (terminals andnonterminals 5 in the grammar.THE PROBABIL ITY  CALCULAT IONThe probability calculation must be divided into two cases.
Inone ease we are in the midst of processing input and do not knowhow many input symbols (if any 5 remain to be processed.
Thesecond situation is that we know that all input symbols have beenprocessed.
This second ease is special because it implies that theonly unknown events are which rules are to be used to link upthe subtrees to the root.
In this case, the summation down theleft edges of subtrees is no longer needed.When in  the  M ids t  o f  the  InputWhen there may be more input to be processed, the calcu-lation of the probability of a parser hypothesis with only onesubtree is exactly the equation (15 in which the start symbol ofthe grammar, S, takes the place of the symbol A in the formula.For hypotheses with n > 1 subtrees we need to take the A andC nodes from figure 1 into account.
To calculate the probabilityof a parser hypothesis with n subtrees ~-, .
.
.
r,, with root nodesB1 .. .Bn, we keep track of what rule is used to generate achBi.
This defines the necessary relationships among the variousAi Bi and Ci in figure 1.
To perform our calculation we need thefollowing matrices:Q(A,r) = p= zeroz(r, c5 = I=0Mo = (I- M)  -~if rule ~" is A .L BC for some B, Cotherwiseif rule r is A .L BC for some A, Botherwisefor M as defined aboveThe probability1.2.3.4.calculation requires the following four steps:Compute l.q* = the S th row of the product MoQ.
Zero outall entries except those corresponding to rules which haveBi as a left child and call the result t~.For i = 2 , .
.
.
,n  compute the product ~* = I,~-iZMoQ.Zero out all entries except those corresponding to ruleswhich have Bi as a left child and call the result t~.Construct a final vector Vii, by zeroing out all entries oft~j-i except hose corresponding to rules which have B,  asa right child.The desired probability is the sum of the entries in Vn andVii n multiplied by the conditional probability of the sub-trees already constructed:\[I e(~ln~)i=1When at  the  End  o f  the  InputIf there is no more input and the hypothesis has only onesubtree, then either the root of the subtree is the start symbolof the grammar, and hence the hypothesis has yielded a well-formed sentence with probability P(rlSS ) or the hypothesis mustbe abandoned since it has not yielded a sentence and no furtherchanges to it are possible.Things are more interesting if the hypothesis contains morethan one subtree.
Consider a parser hypothesis H, consistingof n > 1 subtrees rl through rn with root symbols B, throughBn respectively, with all of B1 through B,  being nonterminalsymbols.
Suppose that the leaves of these subtrees exhaust heinput, so no further shift operations are possible for the parser.For each nonterminal B let MB be the {symbols) x {symbols)matrix whose AC th entry is the probability P(A --* BC), if A --~BC is a rule of the grammar while otherwise the AC th entryis zero.
Also, for each pair of nonterminals BC, let FBc be thecolumn vector indexed by nonterminals whose A th entry is P(A --~BC) if A ~ BC is a rule of the grammar; otherwise the A th entryis zero.
Let Vs be a row vector indexed by nonterminals with a 1in the entry for S and zeros elsewhere.Then, for n > 1, the probability of the hypothesis i equal toVS,%IB~MB2... ,~IB.-2FB.-IB.
x f i  P(rilBi)i=1Programming Cons iderat ionsThere are several problems in making a practical parser basedon the probabilities eMeulated above.
First we must invert therather large matrix I - M and then for each parse hypothesis wemust perform two or three matrix operations for each subtree ofthe hypothesis.
This is not actually as bad as it seems.239First note that we can absorb two matrix operations for eachsubtree into one operation by precomputing MoQ.
If we use thisas our "in-core" matrix, we can reproduce Mo when needed (forn = I computations) by summing across the relevant rules.Next we note that the vector by which we are premultiplyingis very sparse.
This is true since the preceding step was to zero-out all entries in the vector that have the "wrong" left child.
Thismeans that there are only a few rows of the big MoQ matrix thatconcern us.Also note that immediately after we calculate the vector re-sult, we will again zero out entries with the "wrong" left child.This means that we really only need calculate those few entriesin the result vector that have the desired left child.
This reducesthe matrix operation to much lower order, say 5 ?
5.
The sizeof the calculation is determined by how many rules have a givennonterminal as left child.
A grammar will be easy to parse withthis method if each nonterminal only appears as the left child ina few rules.Finally, we note that each parse step can only create one newsubtree, and that at the end of the hypothesis.
So, ff we remem-ber the vector associated with each subtree as we make it, weonly need to do one of these order 5 x 5 calculations to get theprobability of the new hypothesis.BU ILD ING A PARSEROne might consider implementing the above probability calcu-lation in conjunction with some conventional shift-reduce parser.In this case one would let the LR0 parser suggest possibilities forupdating a given parse hypothesis and use the above scheme tocompute probabilities and discard unpromising hypotheses.It is worth pointing out that all the information eeded forLR0 parsing can in fact be reproduced from the probability vec-tors we calculate.
Hence we do not really need to construct sucha parser at all!
The point is that starting from a particular hy-pothesis, a given proposal for a next move leads to a nonzeroprobability if and only ff there is some completion of the inputthat would not "crash" for the conventional parser.
The vectors?~, and ~ i ,  contain all the information we could desire about thenext step for the parser.Finally, let us remark that our matrix calculations can beadapted to yield a shift-reduce parser even when no probabilitiesare initially present.
We simply replace the transition matrix Mwith a suitably scaled incidence matrix M',  in which M'(A, B) =?
if B is the left child of A via sorae rule.
Otherwise M'(A, B) = O.A similar replacement is made for the matrix Q.
The specificvalues of the "probabilities" then arising from our calculations donot matter, only whether or not they are zero.
Thus, the off-lineconstruction of parser tables could be accomplished via a matrixinversion, rather than the conventional recursive calculations.CONCLUSIONSWith the addition of this score, there are now a number ofdifferent methods for controlling the parsing of sentences froma stochastic grammar, each with its own kind of parser and ex-pected form of the grammar.
The four we know of are: \[1, 5, 8, 9\].It is possible to find "expensive" grammars for each of these240scores.
For our score, a "cheap" grammar is one in which eachsymbol is the left child in relatively few rules.The goal, then, must be to find a parser, score and grammarthat meet the needs of a particular application.
We take at leastsome small comfort from the fact that our score has a Bayesian"maximum likelihood" interpretation, even though the superior-ity of that approach depends on the shaky assumption that theinput being parsed really is the randomly-generated output of thestochastic grammar under consideration.REFE I~ENCES\[1\] Chitrao, M. V. and R.
Grishlimn., "Statistical Parsing of Mes-sages," Proc.
DARPA Speech and \]f alawl Language Workshop, p.263-266, June 1990.\[2\] Jelinek, F., "Computation of the Probability off Initial SubstringGeneration by Stochastic Context Free Graznmars', InlefTtal Re-porl, Continuous Speech Recognition Group, IBM Research, T.J.Watson Research Center, $%rktown Heights, ~"  10598, 10 pages.\[3\] Koch,ran, F. and J. Kupin, "Sequential Processing off Input UsingStochastic Graannars" to appear.\[4\] Lari, K and S. J.
$%ung., "The Estimation off Stochastic Context-free Gra\[mnars Using the Inside-Outside Algorithm," CompulerSpeech and Language vol.
4, pp.
35-56, 1990\[5\] Lee, H. C. and K. S.
Fu., "A Stochastic Syntax Analysis Procedureand its Application to Pattern Classification," IEEE T,~n,.
Vol.C-21, pp.
660-666, July 1972.\[6\] Thatcher, J. W., "CharacLerizing Derivation Trees off Context FreeGranunars through a Generalization of Finite Automata Theory"Journal of Compaler and System Sciences, VoI1A Dec. 1967.\[7\] Tomita, M., Efficient Parsing for Araluf~l Language, Kluwer Aca-denfic Publishers, Boston, 1986.\[8\] Velaseo, F. R. and C. R. Souza, '~Sequential Syntactic Decoding,"InL J. CompaL Inform.
Sci.
Vol.
3.4, pp.
273-287, 1974.\[9\] Wright, J. H., "LR parsing off Probabilistie Gra, unars with InputUncertainty for Speech Recognition."
Compgler Speech and Lan-guage ~,%1.
4, pp.
297-323, 1990.
