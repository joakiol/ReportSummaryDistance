Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 58?67,Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational LinguisticsUsing Sequence Kernels to identify Opinion Entities in UrduSmruthi Mukund?
and Debanjan Ghosh* Rohini K Srihari?SUNY at Buffalo, NYsmukund@buffalo.edu*Thomson Reuters Corporate R&Ddebanjan.ghosh@thomsonreuters.comSUNY at Buffalo, NYrohini@cedar.buffalo.eduAbstractAutomatic extraction of opinion holdersand targets (together referred to as opinionentities) is an important subtask of senti-ment analysis.
In this work, we attempt toaccurately extract opinion entities fromUrdu newswire.
Due to the lack of re-sources required for training role labelersand dependency parsers (as in English) forUrdu, a more robust approach based on (i)generating candidate word sequencescorresponding to opinion entities, and (ii)subsequently disambiguating these se-quences as opinion holders or targets ispresented.
Detecting the boundaries of suchcandidate sequences in Urdu is very differ-ent than in English since in Urdu,grammatical categories such as tense,gender and case are captured in wordinflections.
In this work, we exploit themorphological inflections associated withnouns and verbs to correctly identifysequence boundaries.
Different levels ofinformation that capture context areencoded to train standard linear and se-quence kernels.
To this end the best per-formance obtained for opinion entitydetection for Urdu sentiment analysis is58.06% F-Score using sequence kernelsand 61.55% F-Score using a combinationof sequence and linear kernels.1 IntroductionPerforming sentiment analysis on newswire da-ta facilitates the development of systems capableof answering perspective questions like ?How didpeople react to the latest presidential speech??
and?Does General Musharraf support the Indo-Pakpeace treaty??.
The components involved in de-veloping such systems require accurate identifica-tion of opinion expressions and opinion entities.Several of the approaches proposed in the literatureto automatically extract the opinion entities rely onthe use of thematic role labels and dependencyparsers to provide new lexical features for opinionwords (Bethard et al, 2004).
Semantic roles (SRL)also help to mark the semantic constituents (agent,theme, proposition) of a sentence.
Such featuresare extremely valuable for a task like opinion en-tity detection.English is a privileged language when it comesto the availability of resources needed to contributefeatures for opinion entity detection.
There areother widely spoken, resource poor languages,which are still in the infantile stage of automaticnatural language processing (NLP).
Urdu is onesuch language.
The main objective of our researchis to provide a solution for opinion entity detectionin the Urdu language.
Despite Urdu lacking NLPresources required to contribute features similar towhat works for the English language, the perform-ance of our approach is comparable with Englishfor this task (compared with the work of Weigandand Klalow, 2010 ~ 62.61% F1).
The morphologi-cal richness of the Urdu language enables us toextract features based on noun and verb inflectionsthat effectively contribute to the opinion entity ex-traction task.
Most importantly, these features canbe generalized to other Indic languages (Hindi,Bengali etc.)
owing to the grammatical similaritybetween the languages.58English has seen extensive use of sequence ker-nels (string and tree kernels) for tasks such as rela-tion extraction (Culotta and Sorensen, 2004) andsemantic role labeling (Moschitti et al, 2008).
But,the application of these kernels to a task like opin-ion entity detection is scarcely explored (Weigandand Klalow, 2010).
Moreover, existing works inEnglish perform only opinion holder identificationusing these kernels.
What makes our approachunique is that we use the power of sequence ker-nels to simultaneously identify opinion holders andtargets in the Urdu language.Sequence kernels allow efficient use of thelearning algorithm exploiting massive number offeatures without the traditional explicit feature rep-resentation (such as, Bag of Words).
Often, in caseof sequence kernels, the challenge lies in choosingmeaningful subsequences as training samples in-stead of utilizing the whole sequence.
In Urdunewswire data, generating candidate sequencesusable for training is complicated.
Not only are theopinion entities diverse in that they can be con-tained within noun phrases or clauses, the cluesthat help to identify these components can be con-tained within any word group - speech events,opinion words, predicates and connectors.1 Pakistan ke swaat sarhad ke janoobi shahar Bannoka havayi adda zarayye ablaagk tavvju ka markazban gaya hai.
[ Pakistan?s provincial border?s south city?s airbasehas become the center of attraction for all reporters.
]Here, the opinion target spans across four nounchunks, ?
Pakistan?s | provincial border?s | southcity?s | airbase ?.
The case markers (connectors)?ke?and?ka ?
indicate the span.2  Habib miyan ka ghussa bad gaya aur wo apne auratko maara.
[Habib miya?s anger increased and he hit his ownwife.
]Here, the gender (Masculine) inflection of the verb?maara?
(hit) indicates that the agent performingthis action is ?
Habib miya?
(Masculine)3  Ansari ne kaha ?
mere rayee mein Aamir Sohail eekbadimaak aur Ziddi insaan hai?.
[Ansari said, ?
according to me Aamir Sohail is onecrazy and stubborn man?
]Here, cues similar to English such as ?mere rayeemein ?
(according to)  indicate the opinion holder.Another interesting behavior here is the presence ofnested opinion holders.
?kaha?
(said)  indicates thatthis statement was made by Ansari only.4  Sutlan bahut khush tha, naseer key kaam se.
[Sultan was very happy with Naseer?s work ]Here, the target of the expression ?khush?
is after theverb ?khush tha?
(was happy) ?
SV O structureTable 1: Examples to outline the complexity of the taskAnother contributing factor is the free word or-der of the Urdu language.
Although the acceptedform is SOV, there are several instances where theobject comes after the verb or the object is beforethe subject.
In Urdu newswire data, the averagenumber of words in a sentence is 42 (Table 3).This generates a large number of candidate se-quences that are not opinion entities, on account ofwhich the data used for training is highly unbal-anced.
The lack of tools such as dependency pars-ers makes boundary detection for Urdu differentfrom English, which in turn makes opinion entityextraction a much harder task.
Examples shown intable 1 illustrate the complexity of the task.One safe assumption that can be made for opin-ion entities is that they are always contained in aphrase (or clause) that contains a noun (commonnoun, proper noun or pronoun), which is either thesubject or the object of the predicate.
Based onthis, we generate candidate sequences by consider-ing contextual information around noun phrases.
Inexample 1 of Table 1, the subsequence that is gen-erated will consider all four noun phrases ?
Paki-stan?s | provincial border?s | south city?s |airbase?
as a single group for opinion entity.We demonstrate that investigating postpositionsto capture semantic relations between nouns andpredicates is crucial in opinion entity identifica-tion.
Our approach shows encouraging perform-ance.2  Related WorkChoi et al, (2005) consider opinion entity iden-tification as an information extraction task and theopinion holders are identified using a conditionalrandom field (Lafferty et al, 2001) based se-quence-labeling approach.
Patterns are extractedusing AutoSlog (Riloff et al, 2003).
Bloom et al,(2006) use hand built lexicons for opinion entityidentification.
Their method is dependent on acombination of heuristic shallow parsing and de-pendency parsing information.
Kim and Hovy59(2006) map the semantic frames of FrameNet(Baker et al, 1998) into opinion holder and targetfor adjectives and verbs to identify these compo-nents.
Stoyanov and Cardie (2008) treat the task ofidentifying opinion holders and targets as a co-reference resolution problem.
Kim et al, (2008)used a set of communication words, appraisalwords from Senti-WordNet (Esuli and Sebastiani,2006) and NLP tools such as NE taggers and syn-tactic parsers to identify opinion holders accu-rately.
Kim and Hovy (2006) use structuralfeatures of the language to identify opinion enti-ties.
Their technique is based on syntactic path anddependency features along with heuristic featuressuch as topic words and named entities.
Weigandand Klalow (2010) use convolution kernels thatuse predicate argument structure and parse trees.For Urdu specifically, work in the area of clas-sifying subjective and objective sentences is at-tempted by Mukund and Srihari, (2010) using avector space model.
NLP tools that include POStaggers, shallow parser, NE tagger and morpho-logical analyzer for Urdu is provided by Mukundet al, (2010).
This is the only extensive work donefor automating Urdu NLP, although other efforts togenerate semantic role labels and dependencyparsers are underway.3  Linguistic Analysis for Opinion EntitiesIn this section we introduce the different cuesused to capture the contextual information for cre-ating candidate sequences in Urdu by exploitingthe morphological richness of the language.Table 2: Case Inflections on NounsUrdu is a head final language with post-positional case markers.
Some post-positions areassociated with grammatical functions and somewith specific roles associated with the meaning ofverbs (Davison, 1999).
Case markers play a veryimportant role in determining the case inflectionsof nouns.
The case inflections that are useful in thecontext of opinion entity detection are ?ergative?,?dative?, ?genitive?, ?instrumental?
and ?loca-tive?
.
Table 2 outlines the constructs.Consider example 1 below.
(a) is a case where?
Ali ?
is nominative.
However, in (b) ?
Ali?
is da-tive.
The case marker ?ko?
helps to identify sub-jects of certain experiential and psychologicalpredicates: sensations, psychological or mentalstates and obligation or compulsion.
Such predi-cates clearly require the subject to be sentient, andfurther, indicate that they are aected in somemanner, correlating with the semantic propertiesascribed to the dative?s primary use (Grimm,2007).E xample (1):(a)  Ali khush hua  (Ali became happy)(b)  Ali ko khushi hui (Ali became happy)E xample (2):(a) Sadaf kaam karne ki koshish karti hai ( Sadaftries to do work)Semantic information in Urdu is encoded in away that is very different from English.
Aspect,tense and gender depend on the noun that a verbgoverns.
Example 2 shows the dependency thatverbs have on nouns without addressing the lin-guistic details associated with complex predicates.In example 2, the verb ?karti?
(do)  is feminineand the noun it governs ~ Sadaf  is also feminine.The doer for the predicate ?karti hai?
(does)  is?
Sadaf?
and there exists a gender match.
Thisshows that we can obtain strong features if we areable to accurately (i) identify the predicates, (ii)find the governing noun, and (iii) determine thegender.In this work, for the purpose of generating can-didate sequences, we encompass the post-positionresponsible for case inflection in nouns, into thenoun phrase and group the entire chunk as one sin-gle candidate.
In example 1, the dative inflectionon ?
Ali?
is due to the case marker ?
ko?.
Here, ?
Aliko?
will always be considered together in all candi-date sequences that this sentence generates.
ThisCase CliticFormExamplesErgative (ne) Ali  ne ghussa dikhaya ~Ali showed angerAccusa-tive(ko) Ali ko mainey maara ~I hit AliDative (ko,ke) Similar to accusativeInstru-mental(se) Yeh kaam Ali  se hua ~This work was done byAliGenitive (ka, ke, ki) Ali ka ghussa, baap rebaap!
~ Ali?s anger, ohmy God!Locative (mein, par,tak, tale,talak)Ali mein ghussa zyaadahai ~ there is a lot ofanger in Ali60behavior can also be observed in example 1 of ta-ble 1.We use SemantexTM (Srihari et al, 2008) - anend to end NLP framework for Urdu that providesPOS, NE, shallow parser and morphological ana-lyzer, to mark tense, mood, aspect, gender andnumber inflections of verbs and case inflections ofnouns.
For ease of parsing, we enclose dative andaccusative inflected nouns and the respective casemarkers in a tag called POSSE S S .
We also encloselocative, genitive and ergative inflections and casemarkers in a tag called DOER.4  MethodologySequence boundaries are first constructed basedon the POSSESS, DOER and NP (noun chunk)tags prioritized by the position of the tag whileparsing.
We refer to these chunks as ?candidates?as they are the possible opinion entity candidates.We generate candidate sequences by combiningthese candidates with opinion expressions (Mu-kund and Srihari, 2010) and the predicates thatcontain or follow the expression words (~khushi in(b) of example 1 above).We evaluate our approach in two steps:(i) Boundary Detection - detecting opinionentities that contain both holders and tar-gets(ii) Entity Disambiguation - disambiguatingopinion holders from opinion targetsIn the following sections, we briefly describeour research methodology including sequencecreation, choice of kernels and the challenges thusencountered.4.1  Data SetThe data used for the experiments are newswirearticles from BBC Urdu1 that are manually anno-tated to reflect opinion holders, targets, and ex-pressions (emotion bearing words).Number of  subjective sentences 824Average word length of each sentence 42Number of opinion holders  974Number of opinion targets 833Number of opinion expressions 894Table 3: Corpus Statistics1 www.bbc.co.uk/urdu/Table 3 summarizes the corpus statistics.
The interannotator agreement established between two an-notators over 30 documents was found to be 0.85using Cohen?s Kappa score (averaged over alltags).
The agreement is acceptable as tagging emo-tions is a difficult and a personalized task.4.2  Support Vector Machines (SVM) andKernel MethodsSVMs belong to a class of supervised machinelearning techniques that merge the nuances of sta-tistical learning theory, kernel mapping and opti-mization techniques to discover separatinghyperplanes.
Given a set of positive and negativedata points, based on structural risk minimization,SVMs attempt to find not only a separating hyper-plane that separates two categories (Vapnik andKotz, 2006) but also maximize the boundary be-tween them (maximal margin separation tech-nique).
In this work, we propose to use a variationof sequence kernels for opinion entity detection.4.3  Sequence KernelsThe lack of parsers that capture dependencies inUrdu sentences inhibit the use of ?tree kernels?
(Weigand and Klalow, 2010).
In this work, we ex-ploit the power of a set of sequence kernels knownas ?gap sequence string kernels?
(Lodhi et al,2002).
These kernels provide numerical compari-son of phrases as entire sequences rather than aprobability at the chunk level.
Gap sequence ker-nels measure the similarity between two sequences(in this case a sequence of Urdu words) by count-ing the number of common subsequences.
Gapsbetween words are penalized with suitable use ofdecay factor to compensate formatches between lengthy word sequences.Formally, let  be the feature space overwords.
Consequently, we declare other disjointfeature spaces  (stem words, POS,chunks, gender inflections, etc.
)and.For any two-featurevectors  let  compute the numberof common features between s and t. Table 5 liststhe features used to compute .Given two sequences, s and t and the kernelfunction  that calculates the number of61weighted sparse subsequences of length n (say,n =2: bigram) common to both s and t, thenis as shown in eq 1 (Bunescu andMooney, 2005).
(i,j,k are dimensions)                                ------ Eq 1.Generating correct sequences is a prior require-ment for sequence kernels.
For example, in the taskof relation extraction, features included in theshortest path between the mentions of the two se-quences (which hold the relation) play a decisiverole (Bunescu and Mooney, 2005).
Similarly, inthe task of role labeling (SRL - Moschitti et al,2008), syntactic sub-trees containing the argumentsare crucial in finding the correct associations.
Ourapproach to create candidate sequences for opinionentity detection in Urdu is explained in the nextsection.4.4  Candidate Sequence GenerationEach subjective sentence in Urdu contains sev-eral noun phrases with one or more opinion ex-pressions.
The words that express opinions(expression words) can be contained within a verbpredicate (if the predicate is complex) or precedethe verb predicate.
These subjective sentences arefirst pre-processed to mark the morphological in-flections as mentioned in ?3.Table 4: Candidate Sequence GenerationWe define training candidate sequences as theshortest substring t which is a tuple that containsthe candidate noun phrase (POSSESS, DOER orNP), an emotion expression and the closest predi-cate.
Table 4 outlines the steps taken to create thecandidate sequences and figure 1 illustrates thedifferent tuples for a sample sentence.Experiments conducted by Weigand andKlakow (2010) consider <candidate, predicate>and <candidate, expression> tuples.
However, inUrdu the sense of expression and predicate are sotightly coupled (in many examples they subsumeeach other and hence inseparable), that specificallytrying to gauge the influence of predicate andexpression separately on candidates is impossible.There are three advantages in our approach tocreating candidate sequences: (i) by pairing ex-pressions with their nearest predicates, several un-necessary candidate sequences are eliminated, (ii)phrases that do not contain nouns are automaticallynot considered (see RBP chunk in figure 1), and(iii) by considering only one candidate chunk at atime in generating the candidate sequence, we en-sure that the sequence that is generated is short forbetter sequence kernel performance.4 .
4 .1  Linear Kernel featuresFor linear kernels we define features explicitlybased on the lexical relationship between the can-didate and its context.
Table 5 outlines the featuresused.Feature Sets and DescriptionSet 1Baseline1.
head word of candidate2.
case marker contained within candidate?3.
expression words4.
head word of predicate5.
POS sequence of predicate words6.
# of NPs between candidate and emotionSet 2 7. the DOER8.
expression right after candidate?Set 3  9. gender match between candidate andpredicate10.
predicate contains emotion words?Set 4  11.
POS sequence of candidateSet 5   12.
?kah?
feature in the predicate13.
locative feature?14.
genitive feature on noun?Table 5: Linear Kernel Features1 A sentence is parsed to extract all likely candi-date chunks ?
POSSESS, DOER, NP in thatorder.2 <expression, predicate> t uples are first selectedbased on nearest neighbor rule :1.
Predicates that are paired with the expres-sion words either contain the expressions orfollow the expressions.2.
Stand alone predicates are simply ignored asthey do not contribute to the holder identifi-cation task (they contribute to either the sen-tence topic or the reason for the emotion).3 For each candidate,<candidate, expression, predicate> tuples aregenerated without changing the word order.(Fig.
1 ?
example candidates maintain the sameword order)62Figure 1: Illustration of candidate sequences4 .
4 .1  Sequence Kernel featuresFeatures commonly used for sequence kernelsare based on words (such as character-based orword-based sequence kernels).
In this work, weconsider to be a feature space over Urdu wordsalong with other disjoint features such as POS,gender, case inflections.
In the kernel, however, foreach combination (see table 6) the similaritymatching function that computes the num-ber of similar features remains the same.Table 6: Disjoint feature set for sequence kernelsSequence kernels are robust and can deal withcomplex structures.
There are several overlappingfeatures between the feature sets used for linearkernel and sequence kernel.
Consider the POSpath information feature.
This is an important fea-ture for the linear kernel.
However this featureneed not be explicitly mentioned for the sequencekernel as the model internally learns the path in-formation.
In addition, several Boolean featuresexplicitly described for the linear kernel (2 and 13in table 5) are also learned automatically in thesequence kernel by matching subsequences.5  ExperimentsThe data used for our experiments is explainedin ?4.1.
Figure 2 gives a flow diagram of the entireprocess.
LIBSVM?s (Cha ng and Lin, 2001) linearkernel is trained using the manually coded featuresmentioned in table 5.
We integrated our proposedsequence kernel with the same toolkit.
This se-quence kernel uses the features mentioned in table6 and the decay factor is set to 0.5.Figure 2: Overall ProcessKID Kernel Type1 word based kernel (baseline)2 word + POS (parts of speech)3 word + POS + chunk4 word + POS + chunk + gender inflection63The candidate sequence generation algorithm gen-erated 8,329 candidate sequences (contains all opi-nion holders and targets ?
table 3) that are used fortraining both the kernels.
The data is parsed usingSemantexTM to apply POS, chunk and morphologyinformation.
Our evaluation is based on the exactcandidate boundary (whether the candidate is en-closed in a POSSESS, DOER or NP chunk).Allscores are averaged over a 5-fold cross validationset.5.1  Comparison of KernelsWe apply both linear kernels (LK) and se-quence kernels (SK) to identify the entities as wellas disambiguate between the opinion holders andtargets.
Table 7 illustrates the baselines and thebest results for boundary detection of opinion enti-ties.
ID 1 of table 7 represents the result of usingLK with feature set 1 (table 5).
We interpret this asour baseline result.
The best F1 score for this clas-sifier is 50.17%.Table 7: Boundary detection of Opinion EntitiesTable 8 compares various kernels and combina-tions.
Set 1 of table 8 shows the relative effect offeature sets for LK and how each set contributes todetecting opinion entity boundaries.
Although sev-eral features are inspired by similar classificationtechniques (features used for SRL and opinionmining by Choi et al, (2005) ~ set 1, table 5), thefree word nature of Urdu language renders thesefeatures futile.
Moreover, due to larger averagelength of each sentence and high occurrences ofNPs (candidates) in each sentence, the number ofcandidate instances (our algorithm creates 10 se-quences per sentence on average) is also very highas compared to any English corpus.
This makesthe training corpus highly imbalanced.
Interest-ingly, when features like ?
occurrence of postposi-tions, ?kah?
predicate, gender inflections etc.
areused, classification improves (set 1, Feature set1,2,3,4,5, table 8).Table 8: Kernel PerformanceID 3 of table 7 displays the baseline result for SK.Interestingly enough, the baseline F1 for SK isvery close to the best LK performance.
This showsthe robustness of SK and its capability to learncomplex substructures with only words.
A se-quence kernel considers all possible subsequencematching and therefore implements a concept ofpartial (fuzzy) matching.
Because of its tendencyto learn all fuzzy matches while penalizing thegaps between words intelligently, the performanceof SK in general has better recall (Wang, 2008).
Toexplain the recall situation, consider set 2 of table8.
This illustrates the effect of disjoint featurescopes of each feature (POS, chunk, gender).
Eachfeature adds up and expands the feature space ofsequence kernel and allows fuzzy matching there-by improving the recall.
Hence KID 4 has almost20% recall gain over the baseline (SK baseline).However, in many cases, this fuzzy matchingaccumulates in wrong classification and lowersprecision.
A fairly straightforward approach toovercome this problem is to employ a high preci-sion kernel in addition to sequence kernel.
Anotherlimitation of SK is its inability to capture complexID KernelFeatures(table5/6 )Prec.
(% )Rec.
(% )F1(% )1 LK Baseline (Set 1) 39.58 51.49 44.752 LK(best) Set 1, 2, 3, 4, 5 44.20 57.99 50.173  SK Baseline (KID 1) 58.22 42.75 49.304 SK (best) KID 4 54.00 62.79 58.065Best LK+ bestSKKID 4,Set 1, 2,3, 4, 55 8 .
4 3  65 .
0 4  61.55Set Kernel KID Prec.
(% )Rec.
(% )F1(% )Baseline(Set 1)39.58  51.49  44.75Set 1,2 39.91  52.57  45.38Set 1, 2, 3 43.55  57.72  49.65Set 1,2,3,4 44.10  56.90  49.681LKFeature set1,2,3,4,54 4 .
2 0  57 .
9 9  50 .17Baseline -KID 158.22 42.75  49.30KID 2 5 8 .
9 8  47.55  52.65KID 3 58.18  49.62  53.592SKKID 4 54.00 6 2 .
7 9  58 .
0 6KID 1 +best LK51.44 6 8 .
8 9  58.90KID 2 +best LK5 9 .18  62.98  61.02KID 3 +best LK55.18 68.38  61.073SK  +LKKID 4 +best LK58.43  65.04 61.5564grammatical structure and dependencies making ithighly dependent on only the order of the stringsequence that is supplied.We also combine the similarity scores of SKand LK to obtain the benefits of both kernels.
Thispermits SK to expand the feature space by natu-rally adding structural features (POS, chunk) re-sulting in high recall.
At the same time, LK withstrict features (such as the use of ?kah?
verb) orrigid word orders (several Boolean features) willhelp maintain acceptable precision.
By summingthe contribution of both kernels, we achieve an F1of 61.55% (Set 3, table 8), which is 17.8%, more(relative gain ?
around 40%) than the LK baselineresults (ID 1, table 7).Table 9: Opinion entity disa mbiguation for best featuresOur next sets of experiments are conducted to dis-ambiguate opinion holders and targets.
A largenumber of candidate sequences that are created arenot candidates for opinion entities.
This results in ahuge imbalance in the data set.
Jointly classifyopinion holders, opinion targets and false candi-dates with one model can be attempted if this im-balance in the data set due to false candidates canbe reduced.
However, this has not been attemptedin this work.
In order to showcase the feasibility ofour method, we train our model only on the goldstandard candidate sequences that contain opinionentities for entity disambiguation.The two kernels are applied on just the twoclasses (opinion holder vs. opinion target).
Com-bined kernels identify holders with a 65.26% F1(table 9).
However, LK performs best for targetidentification (61.23%).
We believe that this is dueto opinion holders and targets sharing similar syn-tactic structures.
Hence, the sequence informationthat SK learns affects accuracy but improves recall.6  ChallengesBased on the error analysis, we observe somecommon mistakes and provide some examples.1.
Mistakes resulting due to POS tagger and shal-low chunker errors.2.
Errors due to heuristic rules for morphologicalanalysis.3.
Mistakes due to inaccurate identification of ex-pression words by the subjectivity classifier.4.
Errors due to complex and unusual sentencestructures which the kernels failed to capture.Example (3):Is na-insaafi ka badla hamein zaroor layna chahiye.
[ we  have to certainly take revenge for this injustice.
]E xample (4):Kya hum dayshadgardi ka shikar banna chahateinhai?
[Do we  want to become victims of terrorism ?
]E xample (5):Jab secretary kisi aur say baat karke husthi hai, thoPinto ko ghussa aata hai.
[When the secretary talks to someone and laughs,Pinto  gets angry.
]Example 3 is a false positive.
The emotion is ?an-ger?, indicated by ?na-insaafi ka badla?
(revengefor injustice) and ?zaroor?
(certainly) .
But onlythe second expression word is identified accu-rately.
The sequence kernel model determines na-insaafi (injustice) to be the opinion holder when itis actually the reason for the emotion.
However, italso identifies the correct opinion holder - hamein(we) .
Emotions associated with interrogative sen-tences are not marked (example 4) as there existsno one word that captures the overall emotion.However, the subjectivity classifier identifies suchsentences as subjective candidates.
This results infalse negatives for opinion entity detection.
Thetarget (secretary) in example 5, fails to be detectedas no candidate sequence that we generate indi-cates the noun ?secretary?
to be the target.
Wepropose to address these issues in our future work.7  ConclusionWe describe an approach to identify opinion en-tities in Urdu using a combination of kernels.
Tothe best of our knowledge this is the first attemptwhere such an approach is used to identify opinionentities in a language lacking the availability ofresources for automatic text processing.
The per-formance for this task for Urdu is equivalent to thestate of the art performance for English (Weigandand Klakow, 2010) on the same task.Kernel OpinionEntityPrec.
(% )Rec.
(% )F1(% )Holder 58.71 66.67 62.44 LK(best) Target 6 5 .
5 3 57.48 61.23Holder 60.26 69.46 64.54 SKTarget 59.75 49.73 54.28Holder 62.90 6 9 .
81 65.
2 6 Bothkernels Target 60.71 55.44 57.9665ReferencesCollin F. Baker, Charles J. Fillmore, John B. Lowe.1998.
The Berkeley FrameN et Project, Proceedingsof the 17th international conference on Computa-tional linguistics, August 10-14.
Montreal, Quebec,CanadaSteven Bethard, Hong Yu, Ashley Thornton, VasileiosHatzivassiloglou, and Dan Jurafsky.
2004.
AutomaticExtraction of Opinion Propositions and their Holders,AAAI Spring Symposium on Exploring Attitude andAffect in Text: Theories and Applications.Kenneth Bloom, Sterling Stein, and Shlomo Argamon.2007.
Appraisal Extraction for News Opinion Analy-sis at NTCIR-6.
In Proceedings of NTCIR-6 Work-shop Meeting, Tokyo, Japan.R.
C. Bunescu and R. J. M ooney.
2005.
A shortest pathdependency kernel for relation extraction.
In Pro-ceedings of HLT/EMNLP.R.
C. Bunescu and R. J.  Mooney.
2005.
SubsequenceKernels for Relation Extraction.
NIPS.
Vancouver.December.Chih-Chung Chang and Chih-Jen Lin.
2001.
LIBSVM:a library for support vector machines.
Softwareavailable at http://www.cs ie.ntu.edu.tw/~cjlin/libsvmYejin Choi, Claire Cardie, Ellen Riloff, and SiddharthPatwardhan.
2005.
Identifying Sources of Opinionswith Conditional Random Fields and Extraction Pat-terns.
In Proceedings of the Conference on HumanLanguage Technology and Empirical Methods inNatural Language Processing (HLT/EMNLP), Van-couver, Canada.Aaron Culotta and Jeffery Sorensen.
2004.
Dependencytree kernels for relation extraction.
In Proceedings ofthe 42rd Annual Meeting of the Association forComputational Linguistics.
pp.
423-429.Alice Davison.
1999.
Syntax  and Morphology in Hindiand Urdu: A Lexical Resource.
University of Iowa.Andrea Esuli and Fabrizio Sebastiani.
2006.
Sen-tiWordNet: A publicly available lexical resource foropinion mining.
In Proc of LREC.
Vol 6, pp 417-422.Scott Gimm.
2007.
Subject Ma rking in Hindi/Urdu: AStudy in Case and Agency.
ESSLLI Student Session.Malaga, Spain.Youngho Kim, Seaongchan Kim and Sun-HyonMyaeng.
2008.
Extracting Topic-related Opinionsand their Targets in NTCIR-7.
In Proceedings of the7th NTCIR Workshop Meeting.
Tokyo.
Japan.John Lafferty, Andrew McCa llum and F. Pereira.
2001.Conditional random fields: Probabilistic models forsegmenting and labeling sequence data.
In: Proc.18th International Conf.
on Machine Learning, Mor-gan Kaufmann, San Francisco, CA .
pp.
282?289Huma Lodhi, Craig Saunders, John Shawe-Taylor,Nello Cristianini, Chris Watkins.
2002.
Textclassification using string kernels.
J. Mach.
Learn.Res.
2 (March 2002), 419-44.Kim, Soo-Min.
and Eduard Hovy.
2006.
ExtractingOpinions, Opinion Holders, and Topics Expressed inOnline News Media Text.
In ACL Workshop on Sen-timent and Subjectivity in Text.Alessandro Moschitti, Daniele Pighin, Roberto Basili.2008.
Tree kernels for semantic role labeling.
Com-putational Linguistics.
Vol 34, num 2, pp 193-224.Smruthi Mukund and Rohini K. Srihari.
2010.
A VectorSpace Model for Subjectivity Classification in Urduaided by Co-Training, In Proceedings of the 23rd In-ternational Conference on Computational Linguistics,Beijing, China.Smruthi Mukund, Rohini K. Srihari and Erik Peterson.2010.
An Information Extraction System for Urdu ?A Resource Poor Language.
Special Issue on Infor-mation Retrieval for Indian Languages.Ellen Riloff, Janyce Wiebe and Theresa Wilson.
2003.Learning subjective nouns using extraction patternbootstrapping.
In Proceedings of the Seventh Confer-ence on Natural Language Learning (CoNLL-03).Rohini K. Srihari, W. Li, C. Niu, and T. Cornell.
2008.InfoXtract: A Customizable Intermediate Level In-formation Extraction Engine, Journal of Natural Lan-guage Engineering, Cambridge U.
Press, 14(1), pp.33-69.Veselin Stoyanov and Claire Cardie.
2008.
AnnotatingTopic Opinions.
In Proceedings of the Sixth Interna-tional Conference on Language Resources and Eval-uation (LREC 2008), Marrakech, Morocco.John Shawe-Taylor and Nello  Cristianni.
2004.
Kernelmethods for pattern analysis.
Cambridge UniversityPress.Mengqiu Wang.
2008.
A Re-examination of Depend-ency Path Kernels for Relation Extraction, InProceedings of IJCNLP 2008.Michael Wiegand and Dietrich Klalow.
2010.
Convolu-tion kernels for opinion holder extraction.
In Proc.
ofHuman Language Technologies: The 2010 AnnualConference of the North American Chapter of theAssociation for Computational Linguistics.
pp 795-803, ACL66Vladimir Vapnik, S.Kotz .
2006.
Estimation of De-pendences Based on Empirical Data.
Springer,  510pages.67
