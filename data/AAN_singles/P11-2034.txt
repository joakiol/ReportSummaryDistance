Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 194?199,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsReversible Stochastic Attribute-Value GrammarsDanie?l de KokUniversity of Groningend.j.a.de.kok@rug.nlBarbara PlankUniversity of Groningenb.plank@rug.nlGertjan van NoordUniversity of Groningeng.j.m.van.noord@rug.nlAbstractAn attractive property of attribute-value gram-mars is their reversibility.
Attribute-valuegrammars are usually coupled with sepa-rate statistical components for parse selectionand fluency ranking.
We propose reversiblestochastic attribute-value grammars, in whicha single statistical model is employed both forparse selection and fluency ranking.1 IntroductionReversible grammars were introduced as early as1975 by Martin Kay (1975).
In the eighties, thepopularity of attribute-value grammars (AVG) wasin part motivated by their inherent reversible na-ture.
Later, AVG were enriched with a statisticalcomponent (Abney, 1997): stochastic AVG (SAVG).Training a SAVG is feasible if a stochastic modelis assumed which is conditioned on the input sen-tences (Johnson et al, 1999).
Various parsers basedon this approach now exist for various languages(Toutanova et al, 2002; Riezler et al, 2002; vanNoord and Malouf, 2005; Miyao and Tsujii, 2005;Clark and Curran, 2004; Forst, 2007).
SAVG can beapplied for generation to select the most fluent real-ization from the set of possible realizations (Velldalet al, 2004).
In this case, the stochastic model isconditioned on the input logical forms.
Such gener-ators exist for various languages as well (Velldal andOepen, 2006; Nakanishi and Miyao, 2005; Cahill etal., 2007; de Kok and van Noord, 2010).If an AVG is applied both to parsing and gen-eration, two distinct stochastic components are re-quired, one for parsing, and one for generation.
Tosome extent this is reasonable, because some fea-tures are only relevant in a certain direction.
Forinstance, features that represent aspects of the sur-face word order are important for generation, but ir-relevant for parsing.
Similarly, features which de-scribe aspects of the logical form are important forparsing, but irrelevant for generation.
Yet, there arealso many features that are relevant in both direc-tions.
For instance, for Dutch, a very effective fea-ture signals a direct object NP in fronted position inmain clauses.
If a main clause is parsed which startswith a NP, the disambiguation component will fa-vor a subject reading of that NP.
In generation, thefluency component will favor subject fronting overobject fronting.
Clearly, such shared preferences arenot accidental.In this paper we propose reversible SAVG inwhich a single stochastic component is applied bothin parsing and generation.
We provide experimen-tal evidence that such reversible SAVG achieve sim-ilar performance as their directional counterparts.A single, reversible model is to be preferred overtwo distinct models because it explains why pref-erences in a disambiguation component and a flu-ency component, such as the preference for subjectfronting over object fronting, are shared.
A single,reversible model is furthermore of practical inter-est for its simplicity, compactness, and maintainabil-ity.
As an important additional advantage, reversiblemodels are applicable for tasks which combine as-pects of parsing and generation, such as word-graphparsing and paraphrasing.
In situations where only asmall amount of training data is available for parsingor generation, cross-pollination improves the perfor-194mance of a model.
If preferences are shared betweenparsing and generation, it follows that a generatorcould benefit from parsing data and vice versa.
Wepresent experimental results indicating that in such abootstrap scenario a reversible model achieves betterperformance.2 Reversible SAVGAs Abney (1997) shows, we cannot use relativelysimple techniques such as relative frequencies toobtain a model for estimating derivation probabili-ties in attribute-value grammars.
As an alternative,he proposes a maximum entropy model, where theprobability of a derivation d is defined as:p(d) =1Zexp?i?ifi(d) (1)fi(d) is the frequency of feature fi in derivationd.
A weight ?i is associated with each feature fi.In (1), Z is a normalizer which is defined as fol-lows, where ?
is the set of derivations defined bythe grammar:Z =?d???exp?i?ifi(d?)
(2)Training this model requires access to all derivations?
allowed by the grammar, which makes it hard toimplement the model in practice.Johnson et al (1999) alleviate this problem byproposing a model which conditions on the inputsentence s: p(d|s).
Since the number of derivationsfor a given sentence s is usually finite, the calcula-tion of the normalizer is much more practical.
Con-versely, in generation the model is conditioned onthe input logical form l, p(d|l) (Velldal et al, 2004).In such directional stochastic attribute-value gram-mars, the probability of a derivation d given an inputx (a sentence or a logical form) is defined as:p(d|x) =1Z(x)exp?i?ifi(x, d) (3)with Z(x) as (?
(x) are all derivations for input x):Z(x) =?d???
(x)exp?i?ifi(x, d?)
(4)Consequently, the constraint put on feature valuesduring training only refers to derivations with thesame input.
If X is the set of inputs (for parsing,all sentences in the treebank; for generation, all log-ical forms), then we have:Ep(fi)?
Ep?
(fi) = 0 ?
(5)?x?X?d??(x)p?
(x)p(d|x)fi(x, d)?
p?
(x, d)fi(x, d) = 0Here we assume a uniform distribution for p?
(x).Let j(d) be a function which returns 0 if the deriva-tion d is inconsistent with the treebank, and 1 in casethe derivation is correct.
p?
(x, d) is now defined insuch a way that it is 0 for incorrect derivations, anduniform for correct derivations for a given input:p?
(x, d) = p?(x)j(d)?d???(x)j(d?
)(6)Directional SAVG make parsing and generationpractically feasible, but require separate models forparse disambiguation and fluency ranking.Since parsing and generation both create deriva-tions that are in agreement with the constraints im-plied by the input, a single model can accompanythe attribute-value grammar.
Such a model estimatesthe probability of a derivation d given a set of con-straints c, p(d|c).
We use conditional maximum en-tropy models to estimate p(d|c):p(d|c) =1Z(c)exp?i?ifi(c, d) (7)Z(c) =?d???
(c)exp?i?ifi(c, d?)
(8)We derive a reversible model by training on datafor parse disambiguation and fluency ranking simul-taneously.
In contrast to directional models, we im-pose the two constraints per feature given in figure 1:one on the feature value with respect to the sentencesS in the parse disambiguation treebank and the otheron the feature value with respect to logical forms Lin the fluency ranking treebank.
As a result of theconstraints on training defined in figure 1, the fea-ture weights in the reversible model distinguish, atthe same time, good parses from bad parses as wellas good realizations from bad realizations.3 Experimental setup and evaluationTo evaluate reversible SAVG, we conduct experi-ments in the context of the Alpino system for Dutch.195?s?S?d??(s)p?
(s)p(d|c = s)fi(s, d)?
p?
(c = s, d)fi(s, d) = 0?l?L?d??(l)p?
(l)p(d|c = l)fi(l, d)?
p?
(c = l, d)fi(l, d) = 0Figure 1: Constraints imposed on feature values for training reversible models p(d|c).Alpino provides a wide-coverage grammar, lexiconand parser (van Noord, 2006).
Recently, a sentencerealizer has been added that uses the same grammarand lexicon (de Kok and van Noord, 2010).In the experiments, the cdbl part of the AlpinoTreebank (van der Beek et al, 2002) is used as train-ing data (7,154 sentences).
The WR-P-P-H part(2,267 sentences) of the LASSY corpus (van Noordet al, 2010), which consists of text from the Trouw2001 newspaper, is used for testing.3.1 FeaturesThe features that we use in the experiment are thesame features which are available in the Alpinoparser and generator.
In the following section, thesefeatures are described in some detail.Word adjacency.
Two word adjacency featuresare used as auxiliary distributions (Johnson and Rie-zler, 2000).
The first feature is the probability of thesentence according to a word trigram model.
Thesecond feature is the probability of the sentence ac-cording to a tag trigram model that uses the part-of-speech tags assigned by the Alpino system.
Inboth models, linear interpolation smoothing for un-known trigrams, and Laplacian smoothing for un-known words and tags is applied.
The trigram mod-els have been trained on the Twente Nieuws Corpuscorpus (approximately 110 million words), exclud-ing the Trouw 2001 corpus.
In conventional pars-ing tasks, the value of the word trigram model is thesame for all derivations of a given input sentence.Lexical frames.
Lexical analysis is applied dur-ing parsing to find all possible subcategorizationframes for the tokens in the input sentence.
Sincesome frames occur more frequently in good parsesthan others, we use feature templates that record theframes that were used in a parse.
An example ofsuch a feature is: ?
?to play?
serves as an intransi-tive verb?.
We also use an auxiliary distribution ofword and frame combinations that was trained ona large corpus of automatically annotated sentences(436 million words).
The values of lexical framefeatures are constant for all derivations in sentencerealization, unless the frame is not specified in thelogical form.Dependency relations.
There are also featuretemplates which describe aspects of the dependencystructure.
For each dependency, three types of de-pendency features are extracted.
Examples of suchfeatures are ?a pronoun is used as the subject ofa verb?, ?the pronoun ?she?
is used as the sub-ject of a verb?, ?the noun ?beer?
is used as theobject of the verb ?drink??.
In addition, featuresare used which implement auxiliary distributionsfor selectional preferences, as described in Van No-ord (2007).
In conventional realization tasks, thevalues of these features are constant for all deriva-tions for a given input representation.Syntactic features.
Syntactic features include fea-tures which record the application of each grammarrule, as well as features which record the applicationof a rule in the context of another rule.
An exam-ple of the latter is ?rule 167 is used to construct thesecond daughter of a derivation constructed by rule233?.
In addition, there are features describing morecomplex syntactic patterns such as: fronting of sub-jects and other noun phrases, orderings in the middlefield, long-distance dependencies, and parallelism ofconjuncts in coordination.3.2 Parse disambiguationEarlier we assumed that a treebank is a set of cor-rect derivations.
In practice, however, a treebankonly contains an abstraction of such derivations (in196our case sentences with corresponding dependencystructures), thus abstracting away from syntactic de-tails needed in a parse disambiguation model.
As inOsborne (2000), the derivations for the parse disam-biguation model are created by parsing the trainingcorpus.
In the current setting, up to at most 3000derivations are created for every sentence.
Thesederivations are then compared to the gold standarddependency structure to judge the quality of theparses.
For a given sentence, the parses with thehighest concept accuracy (van Noord, 2006) are con-sidered correct, the rest is treated as incorrect.3.3 Fluency rankingFor fluency ranking we also need access to fullderivations.
To ensure that the system is able togenerate from the dependency structures in the tree-bank, we parse the corresponding sentence, and se-lect the parse with the dependency structure thatcorresponds most closely to the dependency struc-ture in the treebank.
The resulting dependencystructures are fed into the Alpino chart generatorto construct derivations for each dependency struc-ture.
The derivations for which the correspondingsentences are closest to the original sentence in thetreebank are marked correct.
Due to a limit on gen-eration time, some longer sentences and correspond-ing dependency structures were excluded from thedata.
As a result, the average sentence length was15.7 tokens, with a maximum of 26 tokens.
To com-pare a realization to the correct sentence, we use theGeneral Text Matcher (GTM) method (Melamed etal., 2003; Cahill, 2009).3.4 Training the modelsModels are trained by taking an informative sam-ple of ?
(c) for each c in the training data (Osborne,2000).
This sample consists of at most 100 ran-domly selected derivations.
Frequency-based fea-ture selection is applied (Ratnaparkhi, 1999).
A fea-ture f partitions ?
(c), if there are derivations d andd?
in ?
(c) such that f(c, d) 6= f(c, d?).
A feature isused if it partitions the informative sample of ?
(c)for at least two c. Table 1 lists the resulting charac-teristics of the training data for each model.We estimate the parameters of the conditionalFeatures Inputs DerivationsGeneration 1727 3688 141808Parse 25299 7133 376420Reversible 25578 10811 518228Table 1: Size of the training data for each modelmaximum entropy models using TinyEst,1 with aGaussian (`2) prior distribution (?
= 0, ?2 = 1000)to reduce overfitting (Chen and Rosenfeld, 1999).4 Results4.1 Parse disambiguationTable 2 shows the results for parse disambiguation.The table also provides lower and upper bounds: thebaseline model selects an arbitrary parse per sen-tence; the oracle chooses the best available parse.Figure 2 shows the learning curves for the direc-tional parsing model and the reversible model.Model CA (%) f-score (%)Baseline 75.88 76.28Oracle 94.86 95.09Parse model 90.93 91.28Reversible 90.87 91.21Table 2: Concept Accuracy scores and f-scores in termsof named dependency relations for the parsing-specificmodel versus the reversible model.The results show that the general, reversible,model comes very close to the accuracy obtainedby the dedicated, parsing specific, model.
Indeed,the tiny difference is not statistically significant.
Wecompute statistical significance using the Approxi-mate Randomization Test (Noreen, 1989).4.2 Fluency rankingTable 3 compares the reversible model with a di-rectional fluency ranking model.
Figure 3 showsthe learning curves for the directional generationmodel and the reversible model.
The reversiblemodel achieves similar performance as the direc-tional model (the difference is not significant).To show that a reversible model can actually profitfrom mutually shared features, we report on an ex-periment where only a small amount of generation1http://github.com/danieldk/tinyest1970.0 0.1 0.2 0.3 0.4 0.57678808284868890Proportion parse training dataCA (%)parse modelreversible modelFigure 2: Learning curve for directional and reversiblemodels for parsing.
The reversible model uses all trainingdata for generation.Model GTMRandom 55.72Oracle 86.63Fluency 71.82Reversible 71.69Table 3: General Text Matcher scores for fluency rankingusing various models.training data is available.
In this experiment, wemanually annotated 234 dependency structures fromthe cdbl part of the Alpino Treebank, by adding cor-rect realizations.
In many instances, there is morethan one fluent realization.
We then used this data totrain a directional fluency ranking model and a re-versible model.
The results for this experiment areshown in Table 4.
Since the reversible model outper-forms the directional model we conclude that indeedfluency ranking benefits from parse disambiguationdata.Model GTMFluency 70.54Reversible 71.20Table 4: Fluency ranking using a small amount of anno-tated fluency ranking training data (difference is signifi-cant at p < 0.05).0.0 0.1 0.2 0.3 0.4 0.5606570Proportion generation training dataGTMscoregeneration modelreversible modelFigure 3: Learning curves for directional and reversiblemodels for generation.
The reversible models uses alltraining data for parsing.5 ConclusionWe proposed reversible SAVG as an alternative todirectional SAVG, based on the observation thatsyntactic preferences are shared between parse dis-ambiguation and fluency ranking.
This frameworkis not purely of theoretical interest, since the exper-iments show that reversible models achieve accura-cies that are similar to those of directional models.Moreover, we showed that a fluency ranking modeltrained on a small data set can be improved by com-plementing it with parse disambiguation data.The integration of knowledge from parse disam-biguation and fluency ranking could be beneficial fortasks which combine aspects of parsing and genera-tion, such as word-graph parsing or paraphrasing.198ReferencesSteven Abney.
1997.
Stochastic attribute-value gram-mars.
Computational Linguistics, 23(4):597?618.Aoife Cahill, Martin Forst, and Christian Rohrer.
2007.Stochastic realisation ranking for a free word orderlanguage.
In ENLG ?07: Proceedings of the EleventhEuropean Workshop on Natural Language Genera-tion, pages 17?24, Morristown, NJ, USA.Aoife Cahill.
2009.
Correlating human and automaticevaluation of a german surface realiser.
In Proceed-ings of the ACL-IJCNLP 2009 Conference - Short Pa-pers, pages 97?100.Stanley F. Chen and Ronald Rosenfeld.
1999.
A gaussianprior for smoothing maximum entropy models.
Tech-nical report, Carnegie Mellon University, Pittsburg.Stephen Clark and James R. Curran.
2004.
Parsing theWSJ using CCG and log-linear models.
In Proceed-ings of the 42nd Annual Meeting of the ACL, pages103?110, Morristown, NJ, USA.Danie?l de Kok and Gertjan van Noord.
2010.
A sentencegenerator for Dutch.
In Proceedings of the 20th Com-putational Linguistics in the Netherlands conference(CLIN).Martin Forst.
2007.
Filling statistics with linguistics:property design for the disambiguation of german lfgparses.
In DeepLP ?07: Proceedings of the Workshopon Deep Linguistic Processing, pages 17?24, Morris-town, NJ, USA.Mark Johnson and Stefan Riezler.
2000.
Exploitingauxiliary distributions in stochastic unification-basedgrammars.
In Proceedings of the 1st Meeting of theNAACL, pages 154?161, Seattle, Washington.Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi,and Stefan Riezler.
1999.
Estimators for stochastic?unification-based?
grammars.
In Proceedings of the37th Annual Meeting of the ACL.Martin Kay.
1975.
Syntactic processing and functionalsentence perspective.
In TINLAP ?75: Proceedings ofthe 1975 workshop on Theoretical issues in naturallanguage processing, pages 12?15, Morristown, NJ,USA.I.
Dan Melamed, Ryan Green, and Joseph Turian.
2003.Precision and recall of machine translation.
In HLT-NAACL.Yusuke Miyao and Jun?ichi Tsujii.
2005.
Probabilisticdisambiguation models for wide-coverage hpsg pars-ing.
In Proceedings of the 43rd Annual Meeting of theACL, pages 83?90, Morristown, NJ, USA.Hiroko Nakanishi and Yusuke Miyao.
2005.
Probabilis-tic models for disambiguation of an hpsg-based chartgenerator.
In Proceedings of the 9th InternationalWorkshop on Parsing Technologies (IWPT), pages 93?102.Eric W. Noreen.
1989.
Computer-Intensive Methodsfor Testing Hypotheses: An Introduction.
Wiley-Interscience.Miles Osborne.
2000.
Estimation of stochastic attribute-value grammars using an informative sample.
In Pro-ceedings of the 18th conference on Computational lin-guistics (COLING), pages 586?592.Adwait Ratnaparkhi.
1999.
Learning to parse naturallanguage with maximum entropy models.
MachineLearning, 34(1):151?175.Stefan Riezler, Tracy H. King, Ronald M. Kaplan,Richard Crouch, John T. Maxwell III, and Mark John-son.
2002.
Parsing the wall street journal using alexical-functional grammar and discriminative estima-tion techniques.
In Proceedings of the 40th AnnualMeeting of the ACL, pages 271?278, Morristown, NJ,USA.Kristina Toutanova, Christopher D. Manning, Stuart M.Shieber, Dan Flickinger, and Stephan Oepen.
2002.Parse disambiguation for a rich hpsg grammar.
InFirst Workshop on Treebanks and Linguistic Theories(TLT), pages 253?263, Sozopol.Leonoor van der Beek, Gosse Bouma, Robert Malouf,and Gertjan van Noord.
2002.
The Alpino depen-dency treebank.
In Computational Linguistics in theNetherlands (CLIN).Gertjan van Noord and Robert Malouf.
2005.
Widecoverage parsing with stochastic attribute value gram-mars.
Draft available from the authors.
A preliminaryversion of this paper was published in the Proceedingsof the IJCNLP workshop Beyond Shallow Analyses,Hainan China, 2004.Gertjan van Noord, Ineke Schuurman, and Gosse Bouma.2010.
Lassy syntactische annotatie, revision 19053.Gertjan van Noord.
2006.
At Last Parsing Is NowOperational.
In TALN 2006 Verbum Ex Machina,Actes De La 13e Conference sur Le Traitement Au-tomatique des Langues naturelles, pages 20?42, Leu-ven.Gertjan van Noord.
2007.
Using self-trained bilexicalpreferences to improve disambiguation accuracy.
InProceedings of the International Workshop on ParsingTechnology (IWPT), ACL 2007 Workshop, pages 1?10, Prague.Erik Velldal and Stephan Oepen.
2006.
Statistical rank-ing in tactical generation.
In Proceedings of the 2006Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP), pages 517?525, Sydney,Australia, July.
ACL.Erik Velldal, Stephan Oepen, and Dan Flickinger.
2004.Paraphrasing treebanks for stochastic realization rank-ing.
In Proceedings of the 3rd Workshop on Treebanksand Linguistic Theories (TLT), pages 149?160.199
