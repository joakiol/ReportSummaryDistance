SESS ION 6: LEX ICON AND LEX ICAL  SEMANTICSPaul S. Jacobs, ChairArt i f ic ia l  Inte l l igence LaboratoryGE Research  and  Deve lopment  CenterSchenectady ,  NY  12301MOTIVAT ION AND BACKGROUNDWhile other word-level marking tasks such as morphol-ogy and part-of-speech tagging have arrived recently ata well-developed methodology and a basis for compar-ing results across systems, the robust discrimination ofword senses in text is a less mature discipline.
Yet, wordsense discrimination is central to many natural anguageprocessing tasks, such as data extraction and machinetranslation.The three papers in this session all describe work at dis-tinguishing word senses in broad classes of naturally-occuring text, albeit using different approaches and fo-cusing on different aspects of the problem.
They all usestatistical methods to a degree, and attempt o producequantitative measures of accuracy for comparison.
Theydiffer substantially in the degree to which knowledge-based methods are considered as well as in the applica-tions for which the work is aimed.NOTEWORTHY PROGRESSThe paper, "One Sense per Discourse" by Gale, Church,and Yarowsky, reports that words used repeatedly in thesame section of text tend to he used in the same senseeach time.
The research reported uses the large, bilin-gual Hansard corpus, aligning each word in English, forexample, with its French translation as a source of in-formation about the sense of the English word.
TheEnglish word sentence for example, will align with theFrench word peine if it refers to a penalty, and with theword phrase if it describes a piece of text.Aside from the main result, reflected in the title, anda thorough analysis of a huge volume of textual data,the Gale et.
al.
paper seems to provide some hope thattesting on a relatively straightforward task with a readilyavailable source of data might carry over into other tasks.In other words, if one can train a system to distinguishword senses based on context using the Hansard corpus,perhaps this training will help to distinguish word sensesfor other translation tasks or even for data extractionor information retrieval.
This hypothesis remains to betested, but any carry-over would mean that these largequantities of training material would be useful withoutany special hand annotation.
"Lexical Disambiguation using Simulated Annealing" byCowie, Guthrie, and Guthrie, uses input text and datafrom the Longman Dictionary of Contemporary En-glish (LDOCE), guessing that word senses can be dis-tinguished using the possible subject fields of the wordsin the surrounding context.
For example, the word in-terest when surrounded by words that can have a finan-cial subject field is much more likely to have a finan-cial sense.
Annealing comes into play as an algorithmfor maximizing entropy in the combination of interpre-tations, in other words, finding the set of word senseswith the greatest degree of overlap in their possible sub-ject field encodings.One of the interesting aspects of the Cowie et.
al.
workis that it raises the possibility that dictionary definitionsthemselves could be made more useful by some kind ofautomatic sense disambiguation.
Also, training on thelexicographer's choice of examples in illustrating differ-ent senses might reduce the level of noise in these exam-ples, perhaps meaning that less data is required to pro-duce good contextual discriminators.
Unlike the Galework, this research assumes that sense discriminationtakes place with respect o a lexicon rather than withrespect o a corpus, perhaps a more realistic assumptionin the practice of current NLP.
"The Acquisition of Lexical Semantic Knowledge fromLarge Corpora", by James Pustejovsky, places corpusanalysis in a subservient role to lexical representation,using the statistical analysis of a corpus as a way of de-termining, for example, the degree to which a word sensecan be extended metonymically.
This sort of corpus-based evidence can be compelling.
A word like an-nounced, for example, normally demands an animatesubject, but in a particular corpus it might occur mostfrequently with an organization as the subject, offer-ing evidence that these occurrences are either extensionsof the word sense or examples of metonymy, i.e.
thatthe name of the organization represents an individual or231group cff individuals.Unlike the other two papers, the Pustejovsky work em-braces the role of statistics as part of knowledge-basedprocessing, not as a replacement for the developmentof lexicons and lexical theories.
Furthermore, using theT IPSTER data extraction task as an application, thisresearch is representative of work trying to use corpusanalysis as an aid to the knowledge acquisition problemsthat burden current text interpretation systems.CURRENT PROBLEMS AND ISSUESThere are some important points to consider in compar-ing and weighing these preliminary results.
Statisticalmethods have a special appeal: these systems robustlyprocess large volumes of text, and produce interesting,quantitative results.
Yet, are these results meaningful?Are they comparable?
How can one extrapolate from theresults to the effects of automated knowledge acquisitionon NLP tasks?Statistics is not a replacement for knowledge-based pro-cessing or knowledge representation theories, rather, itis one of many tools that can help to produce a robust,functional NLP system.
This observation isn't obviousfrom reading the Gale paper, but it does help to fit thepapers together.Another non-obvious question is how to view the dif-ferent examples that have been selected for analysis, aswell as the results that are produced using the sampleexamples in different corpora.
For example, the task ofdiscriminating the senses of slug (a worm-like creatureor a piece of metal) seems fundamentally different froma word like concern (a business or something to thinkabout).
Not only is concern harder, but we can see howthis would make a big difference in a task like T IPSTER,where a word like concern might refer to a key player ina transaction only if it takes the business ense.
Thus,the successful discrimination of the hard, relevant wordsseems ultimately to be the test of these methods.Even where the same words are used for testing (like in-terest),  the numbers appear to reflect drastically differentresults.
In Gale's work, interest  almost always seems tocome out right (96%), while it is 70% in Cowie's (andothers') reports.
This might mean that interest  is lessambiguous in Hansard than in the other corpora, or itmight be lucky that the different senses happen to trans-late into inter~t most of the time, anyway.
Thus, whileit's true that 70% is better on a given task than 50%,there's no way now to compare one set of numbers toanother or to know whether even 96% is any good.
Thisdoesn't mean we shouldn't report numbers, but meanswe have to find a meaningful way to compare.Finally, for all the reporting that's been done on statisti-cal analysis of corpora, there still hasn't been much useof automated training in text interpretation.
It seemsthat it is only a matter of time before statistical train-ing becomes part of all knowledge-based NLP, but untilthis happens, we don't have a measure of the degree towhich training actually helps, for example, in data ex-traction or machine translation.
We expect hat this willbe a topic for the lexical semantics ession at one of thefuture workshops.232
