Coling 2010: Poster Volume, pages 54?62,Beijing, August 2010Query Expansion based on Pseudo Relevance Feedbackfrom Definition ClustersDelphine BernhardLIMSI-CNRSDelphine.Bernhard@limsi.frAbstractQuery expansion consists in extendinguser queries with related terms in orderto solve the lexical gap problem in Infor-mation Retrieval and Question Answer-ing.
The main difficulty lies in identi-fying relevant expansion terms in orderto prevent query drift.
We propose touse definition clusters built from a com-bination of English lexical resources forquery expansion.
We apply the techniqueof pseudo relevance feedback to obtainexpansion terms from definition clusters.We show that this expansion method out-performs both local feedback, based onthe document collection, and expansionwith WordNet synonyms, for the task ofdocument retrieval in Question Answer-ing.1 IntroductionQuestion Answering (QA) systems aim at pro-viding precise answers to user questions.
MostQA systems integrate a document retrieval com-ponent, which is in charge of retrieving the mostrelevant documents or passages for a given userquestion.
Since document retrieval is performedin early stages of QA, it is of the uttermost im-portance that all relevant documents be retrieved,to limit the loss of relevant answers for furtherprocessing.
However, document retrieval systemshave to solve the lexical gap problem, which arisesfrom alternative ways of conveying the same pieceof information in questions and answers.
One ofthe solutions proposed to deal with this issue isquery expansion (QE), which consists in extend-ing user queries with related terms.This paper describes a new method for us-ing lexical-semantic resources in query expansionwith a focus on QA applications.
While someresearch has been devoted to using explicit se-mantic relationships for QE, such as synonymyor hypernymy, with rather disappointing results(Voorhees, 1994), we focus on the usefulness oftextual and unstructured dictionary definitions forquestion expansion.
Definitions extracted fromseven English lexical resources are first groupedto obtain definition clusters, which capture redun-dancies and sense mappings across resources.
Ex-pansion terms are extracted from these definitionclusters using pseudo relevance feedback: we firstretrieve the definition clusters which are most re-lated to the user query, and then extract the mostrelevant terms from these definition clusters to ex-pand the query.The contributions of this work are as fol-lows: (i) we build definition clusters across sevendifferent lexical resources for English, (ii) wethoroughly compare different question expansionmethods using local and global feedback, and (iii)we address both the lexical gap and question am-biguity problems by integrating expansion anddisambiguation in one and the same step.In the next section, we describe related work.In Section 3, we describe our method for acquir-ing definition clusters from seven English lexicalresources.
In Section 4, we detail query expan-sion methods.
We present experimental results inSection 5 and conclude in Section 6.2 Related WorkQuery expansion attempts to solve the vocabu-lary mismatch problem by adding new semanti-cally related terms to the query.
The goal is toincrease recall by retrieving more relevant doc-uments.
Two types of query expansion methodsare usually distinguished (Manning et al, 2008):global techniques, which do not take the resultsobtained for the original query into account, and54local techniques, which expand the query basedon an analysis of the documents returned.
Localmethods are also known as relevance feedback.A first type of global QE methods relies onexternal hand-crafted lexical-semantic resourcessuch as WordNet.
While expansion based on ex-ternal resources is deemed more efficient than ex-pansion relying on relevance feedback, it also hasto tackle problems of semantic ambiguity, whichexplains why local analysis has been shown tobe generally more effective than global analysis(Xu and Croft, 1996).
However, recent work byFang (2008) has demonstrated that global expan-sion based on WordNet and co-occurrence basedresources can lead to performance improvementin an axiomatic model of information retrieval.Corpus-derived co-occurrence relationships arealso exploited for query expansion.
Qiu and Frei(1993) build a corpus-based similarity thesaurususing the method described in Schu?tze (1998) andexpand a query with terms which are similar to thequery concept based on the similarity thesaurus.Song and Bruza (2003) construct vector represen-tations for terms from the target document collec-tion using the Hyperspace Analogue to Language(HAL) model (Lund and Burgess, 1996).
Therepresentations for all the terms in the query arethen combined by a restricted form of vector ad-dition.
Finally, expansion terms are derived fromthis combined vector by information flow.Quasi-parallel monolingual corpora have beenrecently employed for query expansion, using sta-tistical machine translation techniques.
Expan-sion terms are acquired by training a transla-tion model on question-answer pairs (Riezler etal., 2007) or query-snippets pairs (Riezler et al,2008) and by extracting paraphrases from bilin-gual phrase tables (Riezler et al, 2007).The main difficulty of QE methods lies in se-lecting the most relevant expansion terms, espe-cially when the query contains ambiguous words.Moreover, even if the original query is not am-biguous, it might become so after expansion.
Re-cent attempts at integrating word sense disam-biguation (WSD) in IR within the CLEF RobustWSD track1 have led to mixed results which show1http://ixa2.si.ehu.es/clirwsd/that in most cases WSD does not improve perfor-mance of monolingual and cross-lingual IR sys-tems (Agirre et al, 2009).
For query expansionbased on translation models, ambiguity problemsare solved by a language model trained on queries(Riezler et al, 2008), in order to select the mostlikely expansion terms in the context of a givenquery.In this article, we propose to integrate disam-biguation and expansion in one and the samestep by retrieving expansion terms from defini-tion clusters acquired by combining several En-glish lexical resources.3 Acquisition of Definition ClustersDictionary definitions constitute a formidable re-source for Natural Language Processing.
In con-trast to explicit structural and semantic relationsbetween word senses such as synonymy or hy-pernymy, definitions are readily available, evenfor less-resourced languages.
Moreover, they canbe used for a wide variety of tasks, ranging fromword sense disambiguation (Lesk, 1986), to pro-ducing multiple-choice questions for educationalapplications (Kulkarni et al, 2007) or synonymdiscovery (Wang and Hirst, 2009).
However, allresources differ in coverage and word sense gran-ularity, which may lead to several shortcomingswhen using a single resource.
For instance, thesense inventory in WordNet has been shown tobe too fine-grained for efficient word sense dis-ambiguation (Navigli, 2006; Snow et al, 2007).Moreover, gloss and definition-based measures ofsemantic relatedness which rely on the overlap be-tween the definition of a target word and its dis-tributional context (Lesk, 1986) or the definitionof another concept (Banerjee and Pedersen, 2003)yield low results when the definitions provided areshort and do not overlap sufficiently.As a consequence, we propose combining lex-ical resources to alleviate the coverage and gran-ularity problems.
To this aim, we automaticallybuild cross-resource sense clusters.
The goal ofour approach is to capture redundancy in severalresources, while improving coverage over the useof a single resource.553.1 ResourcesIn order to build definition clusters, we used thefollowing seven English resources:WordNet We used WordNet 3.0, which con-tains 117,659 synset definitions.2GCIDE The GCIDE is the GNU version of theCollaborative International Dictionary of English,derived from Webster?s 1913 Revised UnabridgedDictionary.
We used a recent XML version of thisresource,3 from which we extracted 196,266 defi-nitions.English Wiktionary and Simple English Wik-tionary Wiktionary is a collaborative onlinedictionary, which is also available in a simplerEnglish version targeted at children or non-nativespeakers.
We used the English Wiktionary dumpdated August 16, 2009 and the Simple EnglishWiktionary dump dated December 9, 2009.
TheEnglish Wiktionary comprises 245,078 defini-tions, while the Simple English Wiktionary totals11,535 definitions.English Wikipedia and Simple EnglishWikipedia Wikipedia is a collaborative onlineencyclopedia.
As Wiktionary, it provides aSimple English version.
We used the Medi-awiki API to extract 152,923 definitions fromthe English Wikipedia4 and 53,993 definitionsfrom the Simple English Wikipedia.
Since fullWikipedia articles can be very long in comparisonto the other resources, we only retrieved the firstsentence of each page to constitute the definitiondatabase, following (Kazama and Torisawa,2007).OmegaWiki OmegaWiki is a collaborativemultilingual dictionary based on a relationaldatabase.
We used the SQL database dated De-cember 17, 2009,5 comprising 29,179 definitions.2Statistics obtained from http://wordnet.princeton.edu/wordnet/man/wnstats.7WN.html3Retrieved from http://rali.iro.umontreal.ca/GCIDE/4As we mainly aimed at capturing the redundancy acrossresources, we only extracted definitions for the Wikipediaterms which were also found in the GCIDE, Omegawiki,Wiktionary or Simple English Wikipedia.5Retrieved from http://omegawiki.org/3.2 Definition ClusteringIn order to cluster definitions, we first build adefinition graph: each node in the graph corre-sponds to a definition in one of our input resourcesand two definition nodes are linked if they de-fine the same term and their definitions are similarenough.
Links are weighted by the cosine similar-ity of the definition nodes.
To compute the cosinesimilarity, we stem the definition words with thePorter Stemmer and remove stop words.
More-over, we weigh words with their tf.idf value in thedefinitions.
Document frequency (df ) counts arederived from the definitions contained in all ourresources.Definition clusters are identified with a com-munity detection algorithm applied to the defini-tion graph.
Communities correspond to groups ofnodes with dense interconnections: in our case,we aim at retrieving groups of related definitions.We used the algorithm proposed by Blondel et al(2008), based on modularity optimisation.6 Themodularity function measures the quality of a di-vision of a graph into communities (Newman andGirvan, 2004).In order to increase the precision of clustering,we remove edges from the graph whose cosinevalue is lower than a given threshold.3.3 Evaluation of Definition ClustersWe built a gold-standard by manually groupingthe definitions contained in our source resourcesfor 20 terms from the Basic English Word List,7totalling 726 definitions, grouped in 321 classes.We evaluated the definition clusters in terms ofclustering purity (Manning et al, 2008), which isa classical evaluation measure to measure cluster-ing quality.
Purity is defined as follows:purity(?, C) = 1N?kmaxj|?k ?
cj | (1)where N is the number of clustered definitions,?
= {?1, ?2, .
.
.
, ?K} is the set of definition6We used its Python implementation by ThomasAynaud, available at http://perso.crans.org/aynaud/communities/community.py [Visited onOctober 26, 2009].7http://en.wiktionary.org/wiki/Appendix:Basic_English_word_list56Resource DefinitionWordNet an arc of colored light in the sky caused by refraction of the sun?s rays byrainGcide A bow or arch exhibiting, in concentric bands, the several colors of thespectrum, and formed in the part of the hemisphere opposite to the sun bythe refraction and reflection of the sun?s rays in drops of falling rain.Simple Wikipedia A rainbow is an arc of color in the sky that you can see when the sun shinesthrough falling rain.Simple Wiktionary The arch of colours in the sky you can see in the rain when the sun is atyour back.Table 1: Excerpt from a definition cluster.clusters obtained, wk is the set of definitions incluster k, C = {c1, c2, .
.
.
, cJ} is the set of def-inition families expected and cj is the set of defi-nitions in family j.We also report the amount of clusters obtainedfor each cosine threshold value.
The evaluationresults are detailed in Table 2.Cosine threshold Purity # Clusters0.0 0.363 730.1 0.464 1350.2 0.644 2340.3 0.848 3840.4 0.923 4580.5 0.957 515Table 2: Evaluation results for definition cluster-ing.Overall, the results which account for the bestcompromise between purity and cluster count areobtained for a threshold of 0.3: for this threshold,we obtain 384 clusters, which is closest to the ex-pected value of 321 classes.
The purity obtainedfor this cosine threshold is very close to the val-ues obtained by Kulkarni et al (2007), who clus-tered definitions extracted from only two sourcedictionaries and report a purity of 0.88 for theirbest results.
In total we obtain 307,570 definitionclusters.
Table 1 displays an excerpt from one ofthe definition clusters obtained.4 Query Expansion MethodsIn this section, we describe the methods used forperforming query expansion.
We first describetwo simple baseline methods, one based on localfeedback, the other based on WordNet.
Then, wedetail our method relying on the definition clusterspreviously described.4.1 Query Expansion based on LocalFeedbackIn order to perform local feedback based on thedocument collection, we used the pseudo rel-evance feedback methods implemented in theTerrier information retrieval platform (Ounis etal., 2007): Bo1 (Bose-Einstein 1), Bo2 (Bose-Einstein 2) and KL (Kullback-Leibler).
Thesemethods extract informative terms from the top-ranked documents retrieved using the originalquery and use them for query expansion.4.2 Query Expansion based on WordNetSynonymsAs a second baseline for query expansion, weexpand the query terms with their synonyms ex-tracted from WordNet.
For each query term t,we retrieve its WordNet synsets and keep the cor-responding synset members as expansion terms.8We weigh the expansion terms in each synset bythe frequency score provided in WordNet, whichindicates how often the query term t occurs withthe corresponding sense.
In the rest of the paper,this method is referred to as WN-synonyms.The expansion terms obtained using WN-synonyms are further reweighted using Rocchio?sbeta formula which computes the weight qtw of8We use NLTK (http://www.nltk.org/) to accessWordNet.57query term t as follows (Rocchio, 1971; Macdon-ald et al, 2005):qtw = qtfqtfmax+ ?
w(t)wmax(t)(2)where qtf is the frequency of term t in the query,qtfmax is the maximum query term frequencyamong the query terms, w(t) is the expansionweight of t, detailed in Equation 3, and wmax(t)is the maximum w(t) of the expansion terms.
Inall our experiments, ?
is set to 0.4, which is thedefault value used in Terrier.Given this formula, if an original query term oc-curs among the expansion terms, its weight in theexpanded query increases.
For expansion termswhich do not occur in the original query, qtf = 0.This formula has been proposed in the settingof pseudo relevance feedback, where expansionterms are chosen based on the top documents re-trieved for the original query.
However, in ourWN-synonyms setting, one and the same expan-sion term might be obtained from different origi-nal query terms with different weights.
It is there-fore necessary to obtain a global similarity weightfor one expansion term with respect to the wholequery.
Following Qiu and Frei (1993), we definew(t) as:w(t) =?ti?q qtfi ?
s(t, ti)?ti?q qtfi(3)where q is the original query and s(t, ti) is thesimilarity between expansion term t and queryterm ti, i.e., the frequency score in WordNet.For final expansion, we keep the top T termswith the highest expansion weight.4.3 Query Expansion Based on DefinitionClustersIn order to use definition clusters (DC) for queryexpansion, we first use Terrier to index the clus-ters which obtained the best overall results in ourevaluation of definition clustering, correspondingto a cosine threshold of 0.3.9 For each cluster, weindex both the definitions and the list of terms theydefine, which makes it possible to include syn-onyms or Wikipedia redirects in the index.9We used the 2.2.1 version of Terrier, downloadable fromhttp://terrier.org/For a given question, we retrieve the top D def-inition clusters: the retrieval of definition clustersis based on all the question terms, and thus en-ables indirect contextual word sense disambigua-tion.
Then, we extract expansion terms from theseclusters using pseudo relevance feedback (PRF)as implemented in Terrier.
The top T most in-formative terms are retrieved from the top D def-inition clusters retrieved and used for expansion.The expansion terms are weighted using the KL(Kullback-Leibler) term weighting model in Ter-rier.
We chose this particular weighting model, asit yielded the best results for local feedback (seeTable 3).We name this method DC-PRF.5 ExperimentsIn this section, we describe the experimental re-sults obtained for the query expansion methodspresented in the previous section.
We used the Mi-crosoft Research Question-Answering Corpus10(MSRQA) as our evaluation dataset.5.1 Microsoft Research Question-AnsweringCorpus (MSRQA)MSRQA provides a fully annotated set of ques-tions and answers retrieved from the Encarta 98encyclopedia.
The Encarta corpus contains32,715 articles, ranging from very short (3 tokens)to very long (59,798 tokens).
QA systems usu-ally split documents into smaller passages.
Wehave therefore segmented the Encarta articles intosmaller parts representing subsections of the orig-inal article, using a regular expression for iden-tifying section headers in the text.
As a result,the dataset comprises 61,604 documents, with amaximum of 2,730 tokens.
The relevance judge-ments provided comprise the document id as wellas the sentences (usually one) containing the an-swer.
We processed these sentence level relevancejudgements to obtain judgements for documents:a document is considered as relevant if it containsan exact answer sentence.
Overall, we obtainedrelevance judgements for 1,098 questions.10Downloadable from http://research.microsoft.com/en-us/downloads/88c0021c-328a-4148-a158-a42d7331c6cf/58All questions Easy questions Medium questions Hard questionsExpansion MAP MRR MAP MRR MAP MRR MAP MRRnone 0.2257 0.2681 0.2561 0.3125 0.1720 0.1965 0.1306 0.1392Terrier-Bo1 0.2268 0.2674 0.2625 0.3157 0.1642 0.1903 0.1222 0.1240Terrier-Bo2 0.2234 0.2602 0.2581 0.3077 0.1660 0.1872 0.1126 0.1146Terrier-KL 0.2274 0.2684 0.2635 0.3167 0.1644 0.1915 0.1220 0.1236WN-synonyms 0.2260 0.2687 0.2536 0.3098 0.1785 0.2055 0.1254 0.1260DC-PRF 0.2428 0.2929 0.2690 0.3361 0.2004 0.2294 0.1385 0.1472+7.6% +9.2% +5.0% +7.5% +16.5% +16.7% +6.0% +5.7%DC-PRF 0.2361 0.2796 0.2625 0.3184 0.1928 0.2213 0.1389 0.1484+ Terrier KLTable 3: Experimental results.
The performance gaps between the DC-PRF and the baseline retrievalmodels without expansion (none), Terrier-KL and WN-synonyms are statistically significant (two-tailedpaired t-test, p < 0.05), except for hard questions and for the MAP comparison with Terrier-KL foreasy questions.
We also report the improvement percentage.Based on the annotations available in theMSRQA dataset, we further distinguish threequestion types:?
easy questions, which have at least one an-swer with a strong match (two or more queryterms in the answer).?
medium questions, which have no strongmatch answer, but at least an answer with aweak match (one query term in the answer).?
hard questions, which have neither a strongnor a weak match answer, but only answerswhich contain no query terms, and at thebest synonyms and derivational morpholog-ical variants for query terms.Overall, the evaluation dataset comprises 651easy questions, 397 medium questions and 64hard questions (some of these questions have noexact answer).5.2 ResultsAs our baseline, we use the BB2 (Bose-Einsteinmodel for randomness) retrieval model in Terrier.We varied the values for the parameters T (num-ber of expansion terms) and D (number of ex-pansion documents) and used the settings yield-ing the best evaluation results.
For the PRF meth-ods implemented in Terrier, the default settings(T=10, D=3) worked best; for DC-PRF, we usedT=20 and D=40.
Finally, for WN-synonyms weused T=10.
We also combined both DC-PRFand Terrier-KL by first applying DC-PRF expan-sion and then using local Terrier-KL feedback onthe retrieved documents (DC-PRF + Terrier KL).Prior to retrieval, all questions are tokenised andpart-of-speech tagged using Xerox?s IncrementalParser XIP (A?
?t-Mokhtar et al, 2002).
Moreover,we retrieve 100 documents for each question andstem the Encarta document collection.
The resultsshown in Table 3 are evaluated in terms of Mean-Average Precision (MAP) and Mean ReciprocalRank (MRR).
Table 4 provides examples of thetop 5 expansion terms obtained for each expan-sion method.The DC-PRF expansion method performs bestoverall, as well as for easy and medium questiontypes.
For medium questions, DC-PRF leads toan increase of 16.5% in MAP and 16.7% in MRR,with respect to the ?none?
baseline.
Local feed-back methods, such as Terrier-KL, only bring mi-nor improvements for easy questions, but lead toslightly lower results for medium and hard ques-tions.
This might be due to the small size of thedocument collection, which therefore lacks redun-dancy.
The simple baseline expansion methodbased on WordNet leads to very slight improve-ments for medium questions over the setting with-out expansion.
The combination of DC-PRF andTerrier-KL leads to lower results than using only59Terrier-KL WN-synonyms DC-PRF12: Are there UFOs?sight ?
unidentifi ?
report ?object ?
flyflying ?
unidentified ?
object?
UFO ?
saucerunidentified ?
ufo ?
flying ?ufology ?
objects104: What is the most deadly insect in the world?speci ?
plant ?
feed ?
anim ?livcosmos ?
creation ?
existence?
macrocosm ?
universenightshade ?
belladonna ?mortal ?
death ?
lethal107: When was the little ice agedrift ?
glacial ?
ago ?
sheet ?millionsmall ?
slight ?
historic ?period ?
waterfloe ?
period ?
glacial ?
cold ?interglacial449: How does a TV screen get a picture from the air waves?light ?
beam ?
televi ?electron ?
signalmoving ?
ridge ?
image ?
icon?
ikontelevision ?
movie ?
image ?motion ?
door810: Do aliens really exist?sedition ?
act ?
govern ?deport ?
seelive ?
subsist ?
survive ?alienate ?
extraterrestrialalien ?
extraterrestrial ?monsters ?
dreamworks ?animationTable 4: Expansion examples.
The expansion terms produced by Terrier-KL are actually stemmed, asthey are retrieved from a stemmed index.DC-PRF, except for hard questions, for which thecombination brings a very slight improvement.The expansion samples provided in Table 4 ex-emplify the query drift problem of local feed-back methods (Terrier-KL): for question 810, ex-pansion terms focus on the ?foreigner?
sense ofalien rather than on the ?extraterrestrial?
sense.The WN-synonyms method suffers from the prob-lem of weighting synonyms, and mainly focuseson synonyms for the most frequent term of thequestion, e.g.
?world?
in question 104.
Inter-estingly, the DC-PRF method accounts for neol-ogisms, such as ?ufology?
which can be foundin new collaboratively constructed resources suchas Wikipedia or Wiktionary, but not in WordNet.This is made possible by the combination of di-versified resources.
It is also able to provide en-cyclopedic knowledge, such as ?dreamworks?
and?animation?
in question 810, referring to the fea-ture film ?Monsters vs. Aliens?.The DC-PRF method also has some limitations.Even though the expansion terms ?dreamworks?and ?animation?
correspond to the intended mean-ing of the word ?alien?
in question 810, they nev-ertheless might introduce some noise in the re-trieval.
Some other cases exemplify slight drifts inmeaning from the query: in question 104, the ex-pansion terms ?nightshade?
and ?belladonna?
re-fer to poisonous plants and not insects; ?deadlynightshade?
is actually the other name of the ?bel-ladonna?.
Similarly, in question 449, the ex-pansion term ?door?
is obtained, in relation tothe word ?screen?
in the question (as in ?screendoor?).
This might be due to the fact that the termsdefined by the definition clusters are indexed aswell, leading to a high likelihood of retrievingsyntagmatically related terms for multiword ex-pressions.
In future work, it might be relevantto experiment with different indexing schemes fordefinition clusters, e.g.
indexing only the defini-tions, or adding the defined terms to the index onlyif they are not present in the definitions.6 Conclusions and Future WorkIn this paper, we presented a novel method forquery expansion based on pseudo relevance feed-back from definition clusters.
The definition clus-ters are built across seven different English lexicalresources, in order to capture redundancy whileimproving coverage over the use of a single re-source.
The expansions provided by feedbackfrom definition clusters lead to a significant im-60provement of the retrieval results over a retrievalsetting without expansion.In the future, we would like to further amelio-rate definition clustering and incorporate other re-sources, e.g.
resources for specialised domains.Moreover, we have shown that query expansionbased on definition clusters is most useful whenapplied to medium difficulty questions.
We there-fore consider integrating automatic prediction ofquery difficulty to select the best retrieval method.Finally, we would like to evaluate the method pre-sented in this paper for larger datasets.AcknowledgmentsThis work has been partially financed by OSEOunder the Qu?ro program.ReferencesAgirre, Eneko, Giorgio M. Di Nunzio, Thomas Mandl,and Arantxa Otegi.
2009.
CLEF 2009 Ad HocTrack Overview: Robust - WSD Task.
In WorkingNotes for the CLEF 2009 Workshop, Corfu, Greece.A?
?t-Mokhtar, Salah, Jean-Pierre Chanod, and ClaudeRoux.
2002.
Robustness beyond shallowness: in-cremental deep parsing.
Natural Language Engi-neering, 8(2-3):121?144.Banerjee, Satanjeev and Ted Pedersen.
2003.
Ex-tended Gloss Overlaps as a Measure of SemanticRelatedness.
In Proceedings of the Eighteenth In-ternational Joint Conference on Artificial Intelli-gence, pages 805?810.Blondel, Vincent D., Jean-Loup Guillaume, RenaudLambiotte, and Etienne Lefebvre.
2008.
Fast un-folding of communities in large networks.
Journalof Statistical Mechanics: Theory and Experiment,2008(10):P10008+, October.Fang, Hui.
2008.
A Re-examination of Query Ex-pansion Using Lexical Resources.
In Proceedingsof ACL-08: HLT, pages 139?147, Columbus, Ohio,June.Kazama, Jun?ichi and Kentaro Torisawa.
2007.Exploiting Wikipedia as External Knowledge forNamed Entity Recognition.
In Proceedings ofthe 2007 Joint Conference on Empirical Meth-ods in Natural Language Processing and Com-putational Natural Language Learning (EMNLP-CoNLL), pages 698?707.Kulkarni, Anagha, Jamie Callan, and Maxine Eske-nazi.
2007.
Dictionary Definitions: The Likesand the Unlikes.
In Proceedings of Speech andLanguage Technology in Education (SLaTE2007),pages 73?76.Lesk, Michael.
1986.
Automatic sense disambigua-tion using machine readable dictionaries: how totell a pine cone from an ice cream cone.
In SIG-DOC ?86: Proceedings of the 5th annual interna-tional conference on Systems documentation, pages24?26.Lund, Kevin and Curt Burgess.
1996.
Producinghigh-dimensional semantic spaces from lexical co-occurrence.
Behavior Research Methods, Instru-ments & Computers, 28(2):203?208.Macdonald, Craig, Ben He, Vassilis Plachouras, andIadh Ounis.
2005.
University of Glasgow atTREC 2005: Experiments in Terabyte and Enter-prise Tracks with Terrier.
In Proceedings of the 14thText REtrieval Conference (TREC 2005), Gaithers-burg, MD, USA.Manning, Christopher D., Prabhakar Raghavan, andHinrich Schu?tze.
2008.
Introduction to InformationRetrieval.
Cambridge University Press.Navigli, Roberto.
2006.
Meaningful clustering ofsenses helps boost word sense disambiguation per-formance.
In ACL-44: Proceedings of the 21st In-ternational Conference on Computational Linguis-tics and the 44th annual meeting of the Associationfor Computational Linguistics, pages 105?112.Newman, M. E. J. and M. Girvan.
2004.
Finding andevaluating community structure in networks.
Phys-ical review E, 69.Ounis, Iadh, Christina Lioma, Craig Macdonald, andVassilis Plachouras.
2007.
Research Directions inTerrier: a Search Engine for Advanced Retrievalon the Web.
Novatica/UPGRADE Special Issue onWeb Information Access, Ricardo Baeza-Yates et al(Eds), Invited Paper.Qiu, Yonggang and Hans-Peter Frei.
1993.
Conceptbased query expansion.
In SIGIR ?93: Proceedingsof the 16th annual international ACM SIGIR confer-ence on Research and development in informationretrieval, pages 160?169.Riezler, Stefan, Alexander Vasserman, IoannisTsochantaridis, Vibhu Mittal, and Yi Liu.
2007.Statistical Machine Translation for Query Ex-pansion in Answer Retrieval.
In Proceedings ofthe 45th Annual Meeting of the Association ofComputational Linguistics, pages 464?471, Prague,Czech Republic, June.Riezler, Stefan, Yi Liu, and Alexander Vasserman.2008.
Translating Queries into Snippets for Im-proved Query Expansion.
In Proceedings of the6122nd International Conference on ComputationalLinguistics (Coling 2008), pages 737?744, Manch-ester, UK, August.Rocchio, J., 1971.
The SMART Retrieval System,chapter Relevance Feedback in Information Re-trieval, pages 313?323.Schu?tze, Hinrich.
1998.
Automatic Word Sense Dis-crimination.
Computational Linguistics, 24(1):97?123.Snow, Rion, Sushant Prakash, Daniel Jurafsky, andAndrew Y. Ng.
2007.
Learning to Merge WordSenses.
In Proceedings of the 2007 Joint Con-ference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL), pages 1005?1014, Prague, Czech Republic, June.Song, Dawei and Peter D. Bruza.
2003.
Towards con-text sensitive information inference.
Journal of theAmerican Society for Information Science and Tech-nology (JASIST), 54(4):321?334.Voorhees, Ellen M. 1994.
Query expansion usinglexical-semantic relations.
In SIGIR ?94: Proceed-ings of the 17th annual international ACM SIGIRconference on Research and development in infor-mation retrieval, pages 61?69.Wang, Tong and Graeme Hirst.
2009.
Extracting Syn-onyms from Dictionary Definitions.
In Proceedingsof RANLP 2009.Xu, Jinxi and W. Bruce Croft.
1996.
Query expansionusing local and global document analysis.
In SIGIR?96: Proceedings of the 19th annual internationalACM SIGIR conference on Research and develop-ment in information retrieval, pages 4?11.62
