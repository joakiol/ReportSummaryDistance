Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 546?556, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsA Comparison of Vector-based Representations for Semantic CompositionWilliam Blacoe and Mirella LapataInstitute for Language, Cognition and ComputationSchool of Informatics, University of Edinburgh10 Crichton Street, Edinburgh EH8 9ABw.b.blacoe@sms.ed.ac.uk, mlap@inf.ed.ac.ukAbstractIn this paper we address the problem ofmodeling compositional meaning for phrasesand sentences using distributional methods.We experiment with several possible com-binations of representation and composition,exhibiting varying degrees of sophistication.Some are shallow while others operate oversyntactic structure, rely on parameter learn-ing, or require access to very large corpora.We find that shallow approaches are as goodas more computationally intensive alternativeswith regards to two particular tests: (1) phrasesimilarity and (2) paraphrase detection.
Thesizes of the involved training corpora and thegenerated vectors are not as important as thefit between the meaning representation andcompositional method.1 IntroductionDistributional models of semantics have seen con-siderable success at simulating a wide range of be-havioral data in tasks involving semantic cognitionand also in practical applications.
For example, theyhave been used to model judgments of semantic sim-ilarity (McDonald, 2000) and association (Denhireand Lemaire, 2004; Griffiths et al2007) and havebeen shown to achieve human level performanceon synonymy tests (Landauer and Dumais, 1997;Griffiths et al2007) such as those included in theTest of English as a Foreign Language (TOEFL).This ability has been put to practical use in numer-ous natural language processing tasks such as au-tomatic thesaurus extraction (Grefenstette, 1994),word sense discrimination (Schu?tze, 1998), lan-guage modeling (Bellegarda, 2000), and the iden-tification of analogical relations (Turney, 2006).While much research has been directed at themost effective ways of constructing representationsfor individual words, there has been far less con-sensus regarding the representation of larger con-structions such as phrases and sentences.
The prob-lem has received some attention in the connection-ist literature, particularly in response to criticisms ofthe ability of connectionist representations to handlecomplex structures (Smolensky, 1990; Plate, 1995).More recently, several proposals have been put for-ward for computing the meaning of word combina-tions in vector spaces.
This renewed interest is partlydue to the popularity of distributional methods andtheir application potential to tasks that require an un-derstanding of larger phrases or complete sentences.For example, Mitchell and Lapata (2010) intro-duce a general framework for studying vector com-position, which they formulate as a function f oftwo vectors u and v. Different composition mod-els arise, depending on how f is chosen.
Assumingthat composition is a linear function of the Cartesianproduct of u and v allows to specify additive mod-els which are by far the most common method ofvector combination in the literature (Landauer andDumais, 1997; Foltz et al1998; Kintsch, 2001).Alternatively, assuming that composition is a linearfunction of the tensor product of u and v, gives riseto models based on multiplication.One of the most sophisticated proposals for se-mantic composition is that of Clark et al2008) andthe more recent implementation of Grefenstette and546Sadrzadeh (2011a).
Using techniques from logic,category theory, and quantum information they de-velop a compositional distributional semantics thatbrings type-logical and distributional vector spacemodels together.
In their framework, words belongto different type-based categories and different cate-gories exist in different dimensional spaces.
The cat-egory of a word is decided by the number and type ofadjoints (arguments) it can take and the compositionof a sentence results in a vector which exists in sen-tential space.
Verbs, adjectives and adverbs act as re-lational functions, are represented by matrices, andmodify the properties of nouns, that are representedby vectors (see also Baroni and Zamparelli (2010)for a proposal similar in spirit).
Clarke (2012) intro-duces context-theoretic semantics, a general frame-work for combining vector representations, based ona mathematical theory of meaning as context, andshows that it can be used to describe a variety ofmodels including that of Clark et al2008).Socher et al2011a) and Socher et al2011b)present a framework based on recursive neural net-works that learns vector space representations formulti-word phrases and sentences.
The network isgiven a list of word vectors as input and a binarytree representing their syntactic structure.
Then, itcomputes an n-dimensional representation p of twon-dimensional children and the process is repeatedat every parent node until a representation for a fulltree is constructed.
Parent representations are com-puted essentially by concatenating the representa-tions of their children.
During training, the modeltries to minimize the reconstruction errors betweenthe n-dimensional parent vectors and those repre-senting their children.
This model can also computecompositional representations when the tree struc-ture is not given, e.g., by greedily inferring a binarytree.Although the type of function used for vectorcomposition has attracted much attention, relativelyless emphasis has been placed on the basic distri-butional representations on which the compositionfunctions operate.
In this paper, we examine threetypes of distributional representation of increasingsophistication and their effect on semantic composi-tion.
These include a simple semantic space, wherea word?s vector represents its co-occurrence withneighboring words (Mitchell and Lapata, 2010),a syntax-aware space based on weighted distribu-tional tuples that encode typed co-occurrence rela-tions among words (Baroni and Lenci, 2010), andword embeddings computed with a neural languagemodel (Bengio, 2001; Collobert and Weston, 2008).Word embeddings are distributed representations,low-dimensional and real-valued.
Each dimensionof the embedding represents a latent feature of theword, hopefully capturing useful syntactic and se-mantic properties.Using these representations, we construct severalcompositional models, based on addition, multipli-cation, and recursive neural networks.
We assessthe effectiveness of these models using two evalua-tion protocols.
The first one involves modeling sim-ilarity judgments for short phrases gathered in hu-man experiments (Mitchell and Lapata, 2010).
Thesecond one is paraphrase detection, i.e., the task ofexamining two sentences and determining whetherthey have the same meaning (Socher et al2011a).We find that shallow approaches are as good asmore computationally intensive alternatives.
Theyachieve considerable semantic expressivity withoutany learning, sophisticated linguistic processing, oraccess to very large corpora.Our contributions in this work are three-fold: anempirical comparison of a broad range of composi-tional models, some of which are introduced here forthe first time; the use of an evaluation methodologythat takes into account the full spectrum of compo-sitionality from phrases to sentences; and the em-pirical finding that relatively simple compositionalmodels can be used to perform competitively on theparaphrase detection and phrase similarity tasks.2 ModelingThe elementary objects that we operate on are vec-tors associated with words.
We instantiate theseword representations following three distinct seman-tic space models which we describe in Section 2.1below.
Analogously, in Section 2.2 we considerthree methods of vector composition, i.e., how aphrase or a sentence can be represented as a vectorusing the vectors of its constituent words.
Combin-ing different vector representations and compositionmethods gives rise to several compositional modelswhose performance we evaluate in Sections 3 and 4.5472.1 Word RepresentationsFor all of our experiments we employ column vec-tors from a Cartesian, finitely-dimensional space.The dimensionality will depend on the source ofthe vectors involved.
Similarly, the component val-ues inside each source?s vectors are not to be inter-preted in the same manner.
Nonetheless, they havein common that they originate from distributive cor-pus statistics.Co-occurence-based Semantic Space Wordmeaning is commonly represented in a high-dimensional space, where each component corre-sponds to some contextual element in which theword is found.
The contextual elements can bewords themselves, or larger linguistic units such assentences or documents, or even more complex lin-guistic representations such as the argument slots ofpredicates.
A semantic space that is often employedin studying compositionality across a variety oftasks (Mitchell and Lapata, 2010; Grefenstette andSadrzadeh, 2011a) uses a context window of fivewords on either side of the target word, and 2,000vector dimensions.
These are the common contextwords in the British National Corpus (BNC), acorpus of about 100 million tokens.
Their valuesare set to the ratio of the probability of the contextword given the target word to the probability of thecontext word overall.More formally, let us consider the BNC as a set ofsentences:BNC = {Sen(BNC)1 , ...,Sen(BNC)nBNC } (1)where the i-th sentence is a sequence of wordsSeni = (w(i)1 , ...,w(i)ni ) from the BNC?s vocabularyVocBNC.
Then f reqw is the amount of timesthat each word w ?
VocBNC appears in the BNC.Mitchell and Lapata (2010) collect the M mostfrequent non-stoplist words in the set ctxttop ={w(top)1 , ...,w(top)M } and let them consitute the wordvectors?
dimensions.
Each dimension?s value is ob-tained from a co-occurrence count:coCountw[ j] =nBNC?i=1ni?t=1(2)|{k ?
[t?5; t +5] |w(i)t = w, w(i)k = w(top)j }|for w?VocBNC and j = 1, ...,M. Using these counts,they define word vectors component-wise.wdVec(rp)w [ j] =p(w(top)j |w)p(w(top)j )= (3)coCountw[ j]f reqw?totalCountf reqw(top)jfor j = 1, ...,M, where totalCount is the total num-ber of words in the BNC.This space is relatively simple, it has few param-eters, requires no preprocessing other than tokeniza-tion and involves no syntactic information or param-eter learning.
Despite its simplicity, it is a good start-ing point for studying representations for composi-tional models as a baseline against which to evaluatemore elaborate models.Neural Language Model Another perhaps lesswell-known approach to meaning representation isto represent words as continuous vectors of param-eters.
Such word vectors can be obtained with anunsupervised neural language model (NLM, Bengio(2001); Collobert and Weston (2008)) which jointlylearns an embedding of words into a vector spaceand uses these vectors to predict how likely a wordis, given its context.We induced word embeddings with Collobertand Weston (2008)?s neural language model.
Themodel is discriminative and non-probabilistic.
Eachword i ?
D (the vocabulary) is embedded into ad-dimensional space using a lookup table LTW (?
):LTW (i) =Wi (4)where W ?
Rd?|D| is a matrix of parameters to belearned.
Wi ?
Rd is the i-th column of W and d isthe word vector size to be chosen by the user.
Theparameters W are automatically trained during thelearning process using backpropagation.Specifically, at each training update, the modelreads an n-gram x = (w1, .
.
.
,wn) from the cor-pus.
The n-gram is paired with a corrupted n-gramx?
= (w1, .
.
.
, w?n) where w?n 6= wn is chosen uniformlyfrom the vocabulary.
The model concatenates thelearned embeddings of the n words and predicts ascore for the n-gram sequence using the learned em-beddings as features.
The training criterion is that548n-grams that are present in the training corpus musthave a score at least some margin higher than thecorrupted n-grams.
The model learns via gradientdescent over the neural network parameters and theembedding lookup table.
Word vectors are stored ina word embedding matrix which captures syntacticand semantic information from co-occurrence statis-tics.
As these representations are learned, albeit inan unsupervised manner, one would hope that theycapture word meanings more succinctly, comparedto the simpler distributional representations that aremerely based on co-occurrence.We trained the neural language model on theBNC.
We optimized the model?s parameters on aword similarity task using 4% of the BNC as de-velopment data.
Specifically, we used WordSim353,a benchmark dataset (Finkelstein et al2001), con-sisting of relatedness judgments (on a scale of 0to 10) for 353 word pairs.
We experimented withvectors of varying dimensionality (ranging from 50to 200, with a step size of 50).
The size of the targetword?s context window was 2, 3 and 4 in turn.
Therate at which embeddings were learned ranged from3.4?
10?10 to 6.7?
10?10 to 10?9.
We ran eachtraining process for 1.1?108 to 2.7?108 iterations(ca.
2 days).
We obtained the best results with 50dimensions, a context window of size 4, and a em-bedding learning rate of 10?9.
The NLM with theseparameters was then trained for 1.51?109 iterations(ca.
2 weeks).Figure 1 illustrates a two-dimensional projectionof the embeddings for the 500 most common wordsin the BNC.
We only show two out of the actual50 dimensions involved, but one can already beginto see clusterings of a syntactic and semantic na-ture.
In one corner, for example, we encounter agrouping of possessive pronouns together with thepossessive clitic ?s.
The singular ones my, her andhis are closely positioned, as are the plural ones our,your and their.
Also, there is a clustering of socio-political terms, such as international, country, na-tional, government, and council.DistributionalMemory Tensor Baroni and Lenci(2010) present Distributional Memory, a general-ized framework for distributional semantics fromwhich several special-purpose models can be de-rived.
In their framework distributional informationFigure 1: A two-dimensional projection of the word em-beddings we trained on the BNC using Turian et al(2010) implementation of the NLM.
Two small sectionshave been blown up to a legible scale.
They show exam-ples of syntactic and semantic clustering, respectively.word w link l co-word v value c1950s-n of essence-n 2.48801950s-n during bring-v 16.4636Anyone-n nmod reaction-n 1.2161American-n coord-1 athlete-n 5.6485American-j nmod wasp-n 3.4945American-n such as-1 country-n 14.4269American-n sbj tr build-v 23.1014Table 1: Example entries in Baroni and Lenci (2010)?stensoris extracted from the corpus once, in the form of aset of weighted word-link-word tuples arranged intoa third-order tensor.
Different matrices are then gen-erated from the tensor, and their rows and columnsgive rise to different semantic spaces appropriate forcapturing different semantic problems.
In this way,the same distributional information can be sharedacross tasks such as word similarity or analogicallearning.More formally, Baroni and Lenci (2010) con-struct a 3-dimensional tensor T assigning a value cto instances of word pairs w,v and a connectinglink-word l. This representation operates over adependency-parsed corpus and the scores c are ob-tained via counting the occurrences of tuples, andweighting the raw counts by mutual information.Table 1 presents examples of tensor entries.
Thesewere taken from a distributional memory tensor11Available at http://clic.cimec.unitn.it/dm/.549frequency link l co-word v17059 obj include-v16713 obj use-v16573 obj call-v16475 obj see-v15962 obj make-v15707 nmod-1 other-j15554 nmod-1 new-j15224 obj find-v15221 nmod-1 more-j14715 nmod-1 first-j14348 obj give-vTable 2: The 11 most frequent contexts in Baroni andLenci (2010)?s tensor (v and j represent verbs and adjec-tives, respectively).that Baroni and Lenci obtained via preprocessingseveral corpora: the web-derived ukWac corpus ofabout 1.915 billion words, a mid-2009 dump ofthe English Wikipedia containing about 820 millionwords, and the BNC.Extracting a 3-dimensional tensor from the BNCalone would create very sparse representations.We therefore extract so-called word-fibres, essen-tially projections onto a lower-dimensional sub-space, from the same tensor Baroni and Lenci (2010)collectively derived from the 3 billion word corpusjust described (henceforth 3-BWC).
We view the3-dimensional tensorT = {(w(T )1 , l(T )1 ,v(T )1 ,c(T )1 ), ...} (5)as a mapping which assigns each target word w anon-zero value c, given the context (l,v).
All word-context combinations not listed in T are implicitlyassigned a zero value.Now we consider two possible approaches forobtaining vectors, depending on their application.First, we let the D most frequent contextsctxtD = {(l1,v1), ...,(lD,vD)} (6)constitute the D dimensions that each word vec-tor will have.
Table 2 shows the 11 contexts (l,v)that appear most frequently in T .
Thus, each targetword?s vector is defined component-wise as:wdVecw[ j] ={c, if (w, l j,v j,c) ?
T0, otherwise(7)for j = 1, ...,D. This approach is used when a fixedvector dimensionality is necessary.A more dynamic approach is possible when veryfew words w1, ...,wn are involved in a test.
Theirrepresentations can then have a denser format, thatis, with no zero-valued components.
For this weidentify the set of contexts common to the words in-volved,ctxtdyn = {(l(dyn)1 ,v(dyn)1 ),(l(dyn)2 ,v(dyn)2 ), ...} (8)= {(l,v) |(wi, l,v,c) ?
T,c ?
R, i = 1, ...,n}Each context (l,v) again constitutes a vector dimen-sion.
The dimensionality varies strongly depend-ing on the selection of words, but if n does not ex-ceed 4, the dimensionality |ctxtdyn| will typically besubstantial enough.
In this approach, each word?svector consists of the values c found along with thatword and its context in the tensor.wdVecwi [ j] = c, (9)where (wi, l(dyn)j ,v(dyn)j ,c)?
T , for j = 1, ..., |ctxtdyn|.2.2 Composition MethodsIn our experiments we compose word vectors to cre-ate representations for phrase vectors and sentencevectors.
The phrases we are interested in consist oftwo words each: an adjective and a noun like blackhair, a compound noun made up of two nouns suchas oil industry, or a verbal phrase with a transitiveverb and an object noun, e.g., pour tea.Conceiving of a phrase phr = (w1,w2) as a binarytuple of words, we obtain its vector from its words?vectors either by addition:phrVec(w1,w2) = wdVecw1 +wdVecw2 (10)or by point-wise multiplication:phrVec(w1,w2) = wdVecw1wdVecw2 (11)In the same way we acquire a vector senVeci rep-resenting a sentence Seni = (w(i)1 , ...,w(i)ni ) from thevectors for w1, ...,wni .
We simply sum the existingword vectors, that is, vectors obtained via the respec-tive corpus for words that are not on our stoplist:senVec(+)i [ j] = ?k=1,...,niwdVecwk existswdVecwk [ j] (12)550And do the same with point-wise multiplication:senVec()i [ j] = ?k=1,...,niwdVecwk existswdVecwk [ j] (13)The multiplication model in (13) can be seen as aninstantiation of the categorical compositional frame-work put forward by Clark et al2008).
In fact,a variety of multiplication-based models can be de-rived from this framework; and comparisons againstcomponent-wise multiplication on phrase similar-ity tasks yield comparable results (Grefenstetteand Sadrzadeh, 2011a; Grefenstette and Sadrzadeh,2011b).
We thus opt for the model (13) as an exam-ple of compositional models based on multiplicationdue to its good performance across a variety of tasks,including language modeling and prediction of read-ing difficulty (Mitchell, 2011).Our third method, for creating phrase and sen-tence vectors alike, is the application of Socher etal.
(2011a)?s model.
They use the Stanford parser(Klein and Manning, 2003) to create a binary parsetree for each input phrase or sentence.
This tree isthen used as the basis for a deep recursive autoen-coder (RAE).
The aim is to construct a vector rep-resentation for the tree?s root bottom-up where theleaves contain word vectors.
The latter can in the-ory be provided by any type of semantic space, how-ever Socher et alse word embeddings provided bythe neural language model (Collobert and Weston,2008).Given the binary tree input structure, the modelcomputes parent representations p from their chil-dren (c1,c2) using a standard neural network layer:p = f (W (1)[c1;c2]+b(1)), (14)where [c1;c2] is the concatenation of the two chil-dren, f is an element-wise activation function suchas tanh, b is a bias term, and W ?
Rn?2n is an en-coding matrix that we want to learn during training.One way of assessing how well p represents its di-rect children is to decode their vectors in a recon-struction layer:[c?1;c?2] = f (W(2)p+b(2)) (15)During training, the goal is to minimize the re-construction errors of all input pairs at nontermi-nal nodes p in a given parse tree by computing thesquare of the Euclidean distance between the origi-nal input and its reconstruction:Erec([c1;c2]) =12|[c1;c2]?
[c?1;c?2]|2 (16)Socher et al2011a) extend the standard re-cursive autoencoder sketched above in two ways.Firstly, they present an unfolding autoencoder thattries to reconstruct all leaf nodes underneath eachnode rather than only its direct children.
And sec-ondly, instead of transforming the two children di-rectly into a parent p, they introduce another hiddenlayer inbetween.We obtained three compositional models per rep-resentation resulting in nine compositional mod-els overall.
Plugging different representations intothe additive and multiplicative models is relativelystraightforward.
The RAE can also be used witharbitrary word vectors.
Socher et al2011a) ob-tain best results with 100-dimensional vectors whichwe also used in our experiments.
NLM vectorswere trained with this dimensionality on the BNCfor 7.9?
108 iterations (with window size 4 and anembedding learning rate of 10?9).
We constructeda simple distributional space with M = 100 dimen-sions, i.e., those connected to the 100 most frequentco-occurrence words.
In the case of vectors obtainedfrom Baroni and Lenci (2010)?s DM tensor, we dif-ferentiated between phrases and sentences, due tothe disparate amount of words contained in them(see Section 2.1).
To represent phrases, we usedvectors of dynamic dimensionality, since these forma richer and denser representation.
The sentencesconsidered in Section 4 are too large for this ap-proach and all word vectors must be members ofthe same vector space.
Hence, these sentence vec-tors have fixed dimensionality D = 100, consistingof the ?most significant?
100 dimensions, i.e., thosereflecting the 100 most frequent contexts.3 Experiment 1: Phrase SimilarityOur first experiment focused on modeling similarityjudgments for short phrases gathered in human ex-periments.
Distributional representations of individ-ual words are commonly evaluated on tasks basedon their ability to model semantic similarity rela-tions, e.g., synonymy or priming.
Thus, it seemsappropriate to evaluate phrase representations in a551dim.
c.m.
Adj-N N-N V-Obj2000 + 0.37 0.38 0.28SDS2000  0.48 0.50 0.35(BNC)100 RAE 0.31 0.30 0.28vary + 0.37 0.30 0.29DMvary  0.21 0.37 0.33(3-BWC)100 RAE 0.25 0.26 0.0950 + 0.28 0.26 0.24NLM50  0.26 0.22 0.18(BNC)100 RAE 0.19 0.24 0.28Table 3: Correlation coefficients of model predictionswith subject similarity ratings (Spearman?s ?
); columnsshow dimensionality: fixed or varying (see Section 2.1),composition method: + is additive vector composition,is component-wise multiplicative vector composition,RAE is Socher et al2011a)?s recursive auto-encoder.similar manner.
Specifically, we used the datasetfrom Mitchell and Lapata (2010) which containssimilarity judgments for adjective-noun, noun-nounand verb-object phrases, respectively.2 Each item isa phrase pair phr1, phr2 which has a human ratingfrom 1 (very low similarity) to 7 (very high similar-ity).Using the composition models described above,we compute the cosine similarity of phr1 and phr2:phrSimphr1,phr2 =phrVecphr1 ?
phrVecphr2|phrVecphr1 |?
|phrVecphr2 |(17)Model similarities were evaluated against the humansimilarity ratings using Spearman?s ?
correlation co-efficient.Table 3 summarizes the performance of the vari-ous models on the phrase similarity dataset.
Rowsin the table correspond to different vector repre-sentations: the simple distributional semantic space(SDS) from Mitchell and Lapata (2010), Baroni andLenci?s (2010) distributional memory tensor (DM)and the neural language model (NLM), for eachphrase combination: adjective noun (Adj-N), noun-noun (N-N) and verb object (V-Obj).
For eachphrase type we report results for each compositionalmodel, namely additive (+), multiplicative () andrecursive autoencoder (RAE).
The table also shows2The dataset is publicly available from http://homepages.inf.ed.ac.uk/s0453356/sharethe dimensionality of the input vectors next to thevector representation.As can be seen, for SDS the best performingmodel is multiplication, as it is mostly for DM.
Withregard to NLM, vector addition yields overall betterresults.
In general, neither DM or NLM in any com-positional configuration are able to outperform SDSwith multiplication.
All models in Table 3 are sig-nificantly correlated with the human similarity judg-ments (p < 0.01).
Spearman?s ?
differences of 0.3or more are significant at the 0.01 level, using a t-test (Cohen and Cohen, 1983).4 Experiment 2: Paraphrase DetectionAlthough the phrase similarity task gives a fairlydirect insight into semantic similarity and compo-sitional representations, it is somewhat limited inscope as it only considers two-word constructionsrather than naturally occurring sentences.
Ideally,we would like to augment our evaluation with a taskwhich is based on large quantities of natural data andfor which vector composition has practical conse-quences.
For these reasons, we used the MicrosoftResearch Paraphrase Corpus (MSRPC) introducedby Dolan et al2004).
The corpus consists of sen-tence pairs Seni1 ,Seni2 and labels indicating whetherthey are in a paraphrase relationship or not.
The vec-tor representations obtained from our various com-positional models were used as features for the para-phrase classification task.The MSRPC dataset contains 5,801 sentencepairs, we used the standard split of 4,076 trainingpairs (67.5% of which are paraphrases) and 1,725test pairs (66.5% of which are paraphrases).
In orderto judge whether two sentences have the same mean-ing we employ Fan et al2008)?s liblinear classifier.For each of our three vector sources and three differ-ent compositional methods, we create the followingfeatures: (a) a vector representing the pair of inputsentences either via concatenation (?con?)
or sub-traction (?sub?
); (b) a vector encoding which wordsappear therein (?enc?
); and (c) a vector made up ofthe following four other pieces of information: thecosine similarity of the sentence vectors, the lengthof Seni1 , the length of Seni2 , and the unigram overlapamong the two sentences.In order to encode which words appear in552NLM DM SDS(BNC) (3-BWC) (BNC)+ 69.04 73.51 72.93(con, other) (other) (other)67.83 67.54 73.04(sub, other) (other) (other)RAE 70.26 68.29 69.10(con, other) (sub, other) (con, other)Table 4: Paraphrase classification accuracy in %.
In-cluded features are in parentheses: ?con?
is sentence vec-tor concatenation, ?sub?
is sentence vector subtraction,?other?
stands for 4 other features (see Section 4)each sentence and how often, we define a vec-tor wdCounti for sentence Seni and enumerate allwords occuring in the MSRPC:VocMSRPC = {w(MSRPC)1 , ...,w(MSRPC)nMSRPC } (18)giving the word count vectors nMSRPC dimensions.Thus the k-th component of wdCounti is the fre-quency with which the word w(MSRPC)k appears inSeni = (w(i)1 , ...,w(i)ni ):wdCounti[k] = |{ j ?
[1;ni] |w(MSRPC)k = w(i)j }| (19)for k = 1, ...,nMSRPC.
Even though nMSRPC may belarge, the computer files storing our feature vectorsdo not explode in size because wdCount containsmany zeros and the classifier allows a sparse nota-tion of (non-zero) feature values.Regarding the last four features, we measured thesimilarity between sentences the same way as we didwith phrases in section 3.senSimi1,i2 =senVeci1 ?
senVeci2|senVeci1 |?
|senVeci2 |(20)Note that this is the cosine of the angle betweensenVeci1 and senVeci2 .
This enables us to observethe similarity or dissimilarity of two sentences inde-pendent of their sentence length.
Even though eachcontained word increases or decreases the norm ofthe resulting sentence vector, this does not distortthe overall similarity value, due to normalization.The lengths of Seni1 and Seni2 are simply thenumber of words they contain.
The unigram over-lap feature value may be viewed as the cardinal-NLM DM SDS(BNC) (3-BWC) (BNC)+ 81.00 82.16 80.76(con, other) (other) (other)80.41 80.18 82.33(sub, other) (other) (other)RAE 81.28 80.43 80.68(con, other) (sub, other) (con, other)Table 5: Paraphrase classification F1-score in %.
Theinvolved features are exactly the same as in Table 4.ity of the intersection of each sentence?s multiset-bag-of-words.
The latter is encoded in the already-introduced wdCount vectors.
Therefore,uniOverlapi1,i2 =nMSRPC?k=1mins=1,2{wdCountis [k]} (21)In order to establish which features work best foreach representation and composition method, we ex-haustively explored all combinations on a develop-ment set (20% of the original MSRPC training set).Tables 4 (accuracy) and 5 (F1) show our results onthe test set with the best feature combinations foreach model (shown in parentheses).
Each row cor-responds to a different type of composition and eachcolumn to a different word representation model.As can be seen, the distributional memory (DM)is the best performing representation for the addi-tive composition model.
The neural language model(NLM) gives best results for the recursive autoen-coder (RAE), although the other two representationscome close.
And finally the simple distributionalsemantic space (SDS) works best with multiplica-tion.
Also note that the best performing models,namely DM with addition and SDS with multipli-cation, use a basic feature space consisting only ofthe cosine similarity of the composed sentence vec-tors, the length of the two sentences involved, andtheir unigram word overlap.Although our intention was to use the paraphrasedetection task as a test-bed for evaluating composi-tional models rather than achieving state-of-the-artresults, Table 6 compares our approach against pre-vious work on the same task and dataset.
Initial re-search concentrated on individual words rather thansentential representations.
Several approaches used553Model Acc.
F1Baseline 66.5 79.9Mihalcea et al2006) 70.3 81.3Rus et al2008) 70.6 80.5Qiu et al2006) 72.0 81.6Islam and Inkpen (2007) 72.6 81.3Mitchell and Lapata (2010) () 73.0 82.3Baroni and Lenci (2010) (+) 73.5 82.2Fernando and Stevenson (2008) 74.1 82.4Wan et al2006) 75.6 83.0Das and Smith (2009) 76.1 82.7Socher et al2011a) 76.8 83.6Table 6: Overview of results on the MSRCP (test corpus).Accuracy differences of 3.3 or more are significant at the0.01 level (using the ?2 statistic).WordNet in conjunction with distributional similar-ity in an attempt to detect meaning conveyed by syn-onymous words (Islam and Inkpen, 2007; Mihalceaet al2006; Fernando and Stevenson, 2008).
Morerecently, the addition of syntactic features basedon dependency parse trees (Wan et al2006; Dasand Smith, 2009) has been shown to substantiallyboost performance.
The model of Das and Smith(2009), for example, uses quasi-synchronous depen-dency grammar to model the structure of the sen-tences involved in the comparison and their corre-spondences.
Socher et al2011a) obtain an accu-racy that is higher than previously published results.This model is more sophisticated than the one weused in our experiments (see Table 4 and 5).
Ratherthan using the output of the RAE as features for theclassifier, it applies dynamic pooling, a procedurethat takes a similarity matrix as input (e.g., createdby sentences with differing lengths) and maps it toa matrix of fixed size that represents more faithfullythe global similarity structure.3Overall, we observe that our own models do aswell as some of the models that employ WordNetand more sophisticated syntactic features.
With re-gard to F1, we are comparable with Das and Smith(2009) and Socher et al2011a) without using elab-orate features, or any additional manipulations overand above the output of the composition functions3Without dynamic pooling, their model yields an accuracyof 74.2.which if added could increase performance.5 DiscussionIn this paper we systematically compared three typesof distributional representation and their effect onsemantic composition.
Our comparisons involveda simple distributional semantic space (Mitchelland Lapata, 2010), word embeddings computedwith a neural language model (Collobert and We-ston, 2008) and a representation based on weightedword-link-word tuples arranged into a third-ordertensor (Baroni and Lenci, 2010).
These represen-tations vary in many respects: the amount of pre-processing and linguistic information involved (thethird-order tensor computes semantic representa-tions over parsed corpora), whether the semanticspace is the by-product of a learning process (in theneural language model the parameters of the lookuptable must be learned), and data requirements (thethird-order tensor involves processing billions ofwords).
These representations served as input tothree composition methods involving addition, mul-tiplication and a deep recursive autoencoder.
Againthese methods differ in terms of how they imple-ment compositionality: addition and multiplicationare commutative and associative operations and thusignore word order and, more generally, syntacticstructure.
In contrast, the recursive autoencoder issyntax-aware as it operates over a parse tree.
How-ever, the composed representations must be learnedwith a neural network.We evaluated nine models on the complementarytasks of phrase similarity and paraphrase detection.The former task simplifies the challenge of find-ing an adequate method of composition and placesmore emphasis on the representation, whereas thelatter poses, in a sense, the ultimate challenge forcomposition models.
It involves entire sentencesexhibiting varied syntactic constructions and in thelimit involves genuine natural language undertand-ing.
Across both tasks our results deliver a consis-tent message: simple is best.
Despite being in the-ory more expressive, the representations obtained bythe neural language model and the third-order ten-sor cannot match the simple semantic space on thephrase similarity task.
In this task syntax-obliviouscomposition models are superior to the more sophis-554ticated recursive autoencoder.
The latter performsbetter on the paraphrase detection task when its out-put is fed to a classifier.
The simple semantic spacemay not take word order or sentence structure intoaccount, but nevertheless achieves considerable se-mantic expressivity: it is on par with the third-ordertensor without having access to as much data (3 bil-lion words) or a syntactically parsed corpus.What do these findings tell us about the future ofcompositional models for distributional semantics?The problem of finding the right methods of vec-tor composition cannot be pursued independent ofthe choice of lexical representation.
Having testedmany model combinations, we argue that in a goodmodel of distributive semantics representation andcomposition must go hand in hand, i.e., they mustbe mutually learned.Acknowledgments We are grateful to JeffMitchell for his help with the re-implementationof his models.
Thanks to Frank Keller and MichaElsner for their input on earlier versions of this workand to Richard Socher for technical assistance.
Weacknowledge the support of EPSRC through projectgrant EP/I032916/1.ReferencesMarco Baroni and Alessandro Lenci.
2010.
Distribu-tional memory: A general framework for corpus-basedsemantics.
Computational Linguistics, 36(4):673?721.Marco Baroni and Roberto Zamparelli.
2010.
Nounsare vectors, adjectives are matrices: Representingadjective-noun constructions in semantic space.
InProceedings of the 2010 Conference on Empiri-cal Methods in Natural Language Processing, pages1183?1193, Cambridge, MA.Jerome R. Bellegarda.
2000.
Exploiting latent seman-tic information in statistical language modeling.
Pro-ceedings of the Institute of of Electrical and Electron-ics Engineers, 88(8):1279?1296.Yoshua Bengio.
2001.
Neural net language models.Scholarpedia, 3(1):3881.Stephen Clark, Bob Coecke, and Mehrnoosh Sadrzadeh.2008.
A compositional distributional model of mean-ing.
In Proceedings of the 2nd Quantum InteractionSymposium, pages 133?140, Oxford, UK.Daoud Clarke.
2012.
A context-theoretic framework forcompositionality in distributional semantics.
Compu-tational Linguistics, 38(1):41?71.J Cohen and P Cohen.
1983.
Applied Multiple Regres-sion/Correlation Analysis for the Behavioral Sciences.Hillsdale, NJ: Erlbaum.Ronan Collobert and Jason Weston.
2008.
A unified ar-chitecture for natural language processing: deep neuralnetworks with multitask learning.
In Proceedings ofthe 25th International Conference on Machine Learn-ing, pages 160?167, New York, NY.
ACM.Dipanjan Das and Noah A. Smith.
2009.
Paraphraseidentification as probabilistic quasi-synchronousrecognition.
In Proceedings of the Joint Conferenceof the 47th Annual Meeting of the ACL and the 4thInternational Joint Conference on Natural LanguageProcessing of the AFNLP, pages 468?476, Suntec,Singapore.G.
Denhire and B. Lemaire.
2004.
A computationalmodel of children?s semantic memory.
In Proceedingsof the 26th Annual Meeting of the Cognitive ScienceSociety, pages 297?302, Mahwah, NJ.
Lawrence Erl-baum Associates.Bill Dolan, Chris Quirk, and Chris Brockett.
2004.Unsupervised construction of large paraphrase cor-pora: Exploiting massively parallel news sources.
InProceedings of the 20th International Conference onComputational Linguistics, pages 350?356, Geneva,Switzerland.
COLING.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
Liblinear: Alibrary for large linear classification.
Journal of Ma-chine Learning Research, 9:1871?1874.Samuel Fernando and Mark Stevenson.
2008.
A seman-tic similarity approach to paraphrase detection.
Tech-nology.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and EytanRuppin.
2001.
Placing search in context: the conceptrevisited.
In WWW, pages 406?414.Peter Foltz, Walter Kintsch, and Thomas Landauer.1998.
The measurement of textual coherence with la-tent semantic analysis.
Discourse Process, 15:285?307.Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011a.Experimental support for a categorical compositionaldistributional model of meaning.
In Proceedings ofthe 2011 Conference on Empirical Methods in NaturalLanguage Processing, pages 1394?1404, Edinburgh,Scotland.Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011b.Experimenting with transitive verbs in a DisCoCat.
InProceedings of the GEMS 2011 Workshop on GEomet-rical Models of Natural Language Semantics, pages62?66, Edinburgh, UK.555Gregory Grefenstette.
1994.
Explorations in AutomaticThesaurus Discovery.
Kluwer Academic Publishers,Norwell, MA.Thomas L. Griffiths, Mark Steyvers, and Joshua B.Tenenbaum.
2007.
Topics in semantic representation.Psychological Review, 114(2):211?244.Aminul Islam and Diana Inkpen.
2007.
Semantic sim-ilarity of short texts.
In Proceedings of the Interna-tional Conference on Recent Advances in Natural Lan-guage Processing, Borovets, Bulgaria.Walter Kintsch.
2001.
Predication.
Cognitive Science,25(2):173?202.Dan Klein and Christopher D. Manning.
2003.
Accu-rate unlexicalized parsing.
In Proceedings of the 41stAnnual Meeting of the Association for ComputationalLinguistics, pages 423?430, Sapporo, Japan.T.
K. Landauer and S. T. Dumais.
1997.
A solution toPlato?s problem: the latent semantic analysis theoryof acquisition, induction and representation of knowl-edge.
Psychological Review, 104(2):211?240.Scott McDonald.
2000.
Environmental Determinants ofLexical Processing Effort.
Ph.D. thesis, University ofEdinburgh.Rada Mihalcea, Courtney Corley, and Carlo Strapparava,2006.
Corpus-based and knowledge-based measuresof text semantic similarity, pages 775?780.
AAAIPress.Jeff Mitchell and Mirella Lapata.
2010.
Composition indistributional models of semantics.
Cognitive Science,38(8):1388?1429.Jeff Mitchell.
2011.
Composition in distributional mod-els of semantics.
Ph.D. thesis, University of Edin-burgh.Tony A.
Plate.
1995.
Holographic reduced repre-sentations.
IEEE Transactions on Neural Networks,6(3):623?641.Long Qiu, Min-Yen Kan, and Tat-Seng Chua.
2006.Paraphrase recognition via dissimilarity significanceclassification.
In Proceedings of the 2006 Conferenceon Empirical Methods in Natural Language Process-ing, pages 18?26, Sydney, Australia.Vasile Rus, Philip M. McCarthy, Mihai C. Lintean,Danielle S. McNamara, and Arthur C. Graesser.
2008.Paraphrase identification with lexico-syntactic graphsubsumption.
In David Wilson and H. Chad Lane, ed-itors, Florida Artificial Intelligence Research SocietyConference, pages 201?206.
AAAI Press.Hinrich Schu?tze.
1998.
Automatic word sense discrimi-nation.
Computational Linguistics, 24(1):97?124.Paul Smolensky.
1990.
Tensor product variable bind-ing and the representation of symbolic structures inconnectionist systems.
Artificial Intelligence, 46:159?216.Richard Socher, Eric H. Huang, Jeffrey Pennin, An-drew Y. Ng, and Christopher D. Manning.
2011a.Dynamic Pooling and Unfolding Recursive Autoen-coders for Paraphrase Detection.
In Advances in Neu-ral Information Processing Systems 24, pages 801?809.
Granada, Spain.Richard Socher, Jeffrey Pennington, Eric H. Huang, An-drew Y. Ng, and Christopher D. Manning.
2011b.Semi-supervised recursive autoencoders for predictingsentiment distributions.
In Proceedings of the 2011Conference on Empirical Methods in Natural Lan-guage Processing, pages 151?161, Edinburgh, Scot-land.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: a simple and general method forsemi-supervised learning.
In Proceedings of the 48thAnnual Meeting of the Association for ComputationalLinguistics, pages 384?394, Uppsala, Sweden.Peter D. Turney.
2006.
Similarity of semantic relations.Computational Linguistics, 32(3):379?416.Stephen Wan, Mark Dras, Robert Dale, and Cecile Paris.2006.
Using dependency-based features to take the?para-farce?
out of paraphrase.
In Proceedings of theAustralasian Language Technology Workshop 2006,pages 131?138, Sydney, Australia.556
