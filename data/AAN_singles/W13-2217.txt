Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 148?153,Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational LinguisticsFeature-Rich Phrase-based Translation: Stanford University?s Submissionto the WMT 2013 Translation TaskSpence Green, Daniel Cer, Kevin Reschke, Rob Voigt*, John BauerSida Wang, Natalia Silveira?, Julia Neidert and Christopher D. ManningComputer Science Department, Stanford University*Center for East Asian Studies, Stanford University?Department of Linguistics, Stanford University{spenceg,cerd,kreschke,robvoigt,horatio,sidaw,natalias,jneid,manning}@stanford.eduAbstractWe describe the Stanford University NLPGroup submission to the 2013 Workshopon Statistical Machine Translation SharedTask.
We demonstrate the effectiveness of anew adaptive, online tuning algorithm thatscales to large feature and tuning sets.
Forboth English-French and English-German,the algorithm produces feature-rich mod-els that improve over a dense baseline andcompare favorably to models tuned withestablished methods.1 IntroductionGreen et al(2013b) describe an online, adaptivetuning algorithm for feature-rich translation mod-els.
They showed considerable translation qualityimprovements over MERT (Och, 2003) and PRO(Hopkins and May, 2011) for two languages in aresearch setting.
The purpose of our submission tothe 2013 Workshop on Statistical Machine Trans-lation (WMT) Shared Task is to compare the algo-rithm to more established methods in an evaluation.We submitted English-French (En-Fr) and English-German (En-De) systems, each with over 100k fea-tures tuned on 10k sentences.
This paper describesthe systems and also includes new feature sets andpractical extensions to the original algorithm.2 Translation ModelOur machine translation (MT) system is Phrasal(Cer et al 2010), a phrase-based system based onalignment templates (Och and Ney, 2004).
Likemany MT systems, Phrasal models the predictivetranslation distribution p(e|f ;w) directly asp(e|f ;w) = 1Z(f) exp[w>?
(e, f)](1)where e is the target sequence, f is the source se-quence, w is the vector of model parameters, ?(?
)is a feature map, and Z(f) is an appropriate nor-malizing constant.
For many years the dimensionof the feature map ?(?)
has been limited by MERT,which does not scale past tens of features.Our submission explores real-world translationquality for high-dimensional feature maps and as-sociated weight vectors.
That case requires a morescalable tuning algorithm.2.1 Online, Adaptive Tuning AlgorithmFollowingHopkins andMay (2011) we castMT tun-ing as pairwise ranking.
Consider a single sourcesentence f with associated references e1:k. Let dbe a derivation in an n-best list of f that has thetarget e = e(d) and the feature map ?(d).
Definethe linear model scoreM(d) = w ?
?(d).
For anyderivation d+ that is better than d?
under a goldmetric G, we desire pairwise agreement such thatG(e(d+), e1:k)> G(e(d?
), e1:k)??
M(d+) > M(d?
)Ensuring pairwise agreement is the same as ensur-ing w ?
[?(d+)?
?(d?)]
> 0.For learning, we need to select derivation pairs(d+, d?)
to compute difference vectors x+ =?
(d+) ?
?(d?).
Then we have a 1-class separa-tion problem trying to ensure w ?
x+ > 0.
Thederivation pairs are sampled with the algorithm ofHopkins and May (2011).
Suppose that we samples pairs for source sentence ft to compute a set ofdifference vectors Dt = {x1:s+ }.
Then we optimize`t(w) = `(Dt, w) = ?
?x+?Dtlog 11 + e?w?x+(2)which is the familiar logistic loss.
Hopkins andMay (2011) optimize (2) in a batch algorithmthat alternates between candidate generation (i.e.,n-best list or lattice decoding) and optimization(e.g., L-BFGS).
We instead use AdaGrad (Duchi148et al 2011), a variant of stochastic gradient de-scent (SGD) in which the learning rate is adaptedto the data.
Informally, AdaGrad scales the weightupdates according to the geometry of the data ob-served in earlier iterations.
Consider a particu-lar dimension j of w, and let scalars vt = wt,j ,gt = ?j`t(wt?1), and Gt = ?ti=1 g2i .
The Ada-Grad update rule isvt = vt?1 ?
?
G?1/2t gt (3)Gt = Gt?1 + g2t (4)In practice,Gt is a diagonal approximation.
IfGt =I , observe that (3) is vanilla SGD.In MT systems, the feature map may generateexponentially many irrelevant features, so we needto regularize (3).
The L1 norm of the weight vec-tor is known to be an effective regularizer in sucha setting (Ng, 2004).
An efficient way to applyL1 regularization is the Forward-Backward split-ting (FOBOS) framework (Duchi and Singer, 2009),which has the following two-step update:wt?
12 = wt?1 ?
?t?1?`t?1(wt?1) (5)wt = argminw12?w ?
wt?
12 ?22 + ?t?1r(w)(6)where (5) is just an unregularized gradient descentstep and (6) balances the regularization term r(w)with staying close to the gradient step.For L1 regularization we have r(w) = ?||w||1and the closed-form solution to (6) iswt = sign(wt?
12 )[|wt?
12 | ?
?t?1?
]+(7)where [x]+ = max(x, 0) is the clipping functionthat in this case sets a weight to 0 when it falls belowthe threshold ?t?1?.Online algorithms are inherently sequential; thisalgorithm is no exception.
If we want to scale thealgorithm to large tuning sets, then we need to par-allelize the weight updates.
Green et al(2013b)describe the parallelization technique that is imple-mented in Phrasal.2.2 Extensions to (Green et al 2013b)Sentence-Level Metric We previously used thegold metric BLEU+1 (Lin and Och, 2004), whichsmoothes bigram precisions and above.
This metricworked well with multiple references, but we foundthat it is less effective in a single-reference settinglike WMT.
To make the metric more robust, Nakovet al(2012) extended BLEU+1 by smoothing boththe unigram precision and the reference length.
Wefound that this extension yielded a consistent +0.2BLEU improvement at test time for both languages.Subsequent experiments on the data sets of Greenet al(2013b) showed that standard BLEU+1 worksbest for multiple references.Custom regularization parameters Green et al(2013b) showed that large feature-rich models over-fit the tuning sets.
We discovered that certain fea-tures caused greater overfitting than others.
Customregularization strengths for each feature set are onesolution to this problem.
We found that techniquelargely fixed the overfitting problem as shown bythe learning curves presented in section 5.1.Convergence criteria Standard MERT imple-mentations approximate tuning BLEU by re-ranking the previous n-best lists with the updatedweight vector.
This approximation becomes infeasi-ble for large tuning sets, and is less accurate for algo-rithms like ours that do not accumulate n-best lists.We approximate tuning BLEU by maintaining the1-best hypothesis for each tuning segment.
At theend of each epoch, we compute corpus-level BLEUfrom this hypothesis set.
We flush the set of storedhypotheses before the next epoch begins.
Althoughmemory-efficient, we find that this approximationis less dependable as a convergence criterion thanthe conventional method.
Whereas we previouslystopped the algorithm after four iterations, we nowselect the model according to held-out accuracy.3 Feature Sets3.1 Dense FeaturesThe baseline ?dense?
model has 19 features: thenine Moses (Koehn et al 2007) baseline features, ahierarchical lexicalized re-ordering model (Galleyand Manning, 2008), the (log) bitext count of eachtranslation rule, and an indicator for unique rules.The final dense feature sets for each languagediffer slightly.
The En-Fr system incorporates asecond language model.
The En-De system adds afuture cost component to the linear distortion model(Green et al 2010).The future cost estimate allowsthe distortion limit to be raised without a decreasein translation quality.1493.2 Sparse FeaturesSparse features do not necessarily fire on each hy-pothesis extension.
Unlike prior work on sparseMTfeatures, our feature extractors do not filter featuresbased on tuning set counts.
We instead rely on theregularizer to select informative features.Several of the feature extractors depend onsource-side part of speech (POS) sequences anddependency parses.
We created those annotationswith the Stanford CoreNLP pipeline.Discriminative Phrase Table A lexicalized in-dicator feature for each rule in a derivation.
Thefeature weights can be interpreted as adjustmentsto the associated dense phrase table features.Discriminative Alignments A lexicalized indi-cator feature for the phrase-internal alignments ineach rule in a derivation.
For one-to-many, many-to-one, and many-to-many alignments we extract theclique of aligned tokens, perform a lexical sort, andconcatenate the tokens to form the feature string.Discriminative Re-ordering A lexicalized indi-cator feature for each rule in a derivation that ap-pears in the following orientations: monotone-with-next, monotone-with-previous, non-monotone-with-next, non-monotone-with-previous.
Greenet al(2013b) included the richer non-monotoneclasses swap and discontinuous.
However, we foundthat these classes yielded no significant improve-ment over the simpler non-monotone classes.
Thefeature weights can be interpreted as adjustmentsto the generative lexicalized re-ordering model.Source Content-Word Deletion Count-basedfeatures for source content words that are ?deleted?in the target.
Content words are nouns, adjectives,verbs, and adverbs.
A deleted source word is ei-ther unaligned or aligned to one of the 100 mostfrequent target words in the target bitext.
For eachdeleted word we increment both the feature for theparticular source POS and an aggregate feature forall parts of speech.
We add similar but separatefeatures for head content words that are either un-aligned or aligned to frequent target words.Inverse Document Frequency Numeric fea-tures that compare source and target word frequen-cies.
Let idf(?)
return the inverse document fre-quency of a token in the training bitext.
Supposea derivation d = {r1, r2, .
.
.
, rn} is composed ofn translation rules, where e(r) is the target side ofthe rule and f(r) is the source side.
For each ruleBilingual MonolingualSentences Tokens TokensEn-Fr 5.0M 289M 1.51BEn-De 4.4M 223M 1.03BTable 1: Gross corpus statistics after data selectionand pre-processing.
The En-Fr monolingual countsinclude French Gigaword 3 (LDC2011T10).r that translates j source tokens to i target tokenswe computeq =?iidf(e(r)i)?
?jidf(f(r)j) (8)We add two numeric features, one for the source andanother for the target.
When q > 0 we incrementthe target feature by q; when q < 0 we incrementthe target feature by |q|.
Together these featurespenalize asymmetric rules that map rare words tofrequent words and vice versa.POS-based Re-ordering The lexicalized dis-criminative re-ordering model is very sparse, so weadded re-ordering features based on source parts ofspeech.
When a rule is applied in a derivation, weextract the associated source POS sequence alongwith the POS sequences from the previous and nextrules.
We add a ?with-previous?
indicator featurethat is the conjunction of the current and previousPOS sequences; the ?with-next?
indicator feature iscreated analogously.
This feature worked well forEn-Fr, but not for En-De.4 Data PreparationTable 1 describes the pre-processed corpora fromwhich our systems are built.4.1 Data SelectionWe used all of the monolingual and parallel En-De data allowed in the constrained condition.
Weincorporated all of the French monolingual data,but sampled a 5M-sentence bitext from the approx-imately 40M available En-Fr parallel sentences.To select the sentences we first created a ?target?corpus by concatenating the tuning and test sets(newstest2008?2013).
Then we ran the featuredecay algorithm (FDA) (Bi?ici and Yuret, 2011),which samples sentences that most closely resem-ble the target corpus.
FDA is a principled methodfor reducing the phrase table size by excluding lessrelevant training examples.1504.2 TokenizationWe tokenized the English (source) data accordingto the Penn Treebank standard (Marcus et al 1993)with Stanford CoreNLP.
The French data was to-kenized with packages from the Stanford FrenchParser (Green et al 2013a), which implements ascheme similar to that used in the French Treebank(Abeill?
et al 2003).German is more complicated due to pervasivecompounding.
We first tokenized the data with thesame English tokenizer.
Then we split compoundswith the lattice-based model (Dyer, 2009) in cdec(Dyer et al 2010).
To simplify post-processing weadded segmentation markers to split tokens, e.g.,?berschritt?
?ber #schritt.4.3 AlignmentWe aligned both bitexts with the Berkeley Aligner(Liang et al 2006) configured with standard set-tings.
We symmetrized the alignments accordingto the grow-diag heuristic.4.4 Language ModelingWe estimated unfiltered 5-gram language modelsusing lmplz (Heafield et al 2013) and loaded themwith KenLM (Heafield, 2011).
For memory effi-ciency and faster loading we also used KenLM toconvert the LMs to a trie-based, binary format.
TheGerman LM included all of the monolingual dataplus the target side of the En-De bitext.
We builtan analogous model for French.
In addition, weestimated a separate French LM from the Gigaworddata.14.5 French Agreement CorrectionIn French verbs must agree in number and personwith their subjects, and adjectives (and some pastparticiples) must agree in number and gender withthe nouns they modify.
On their own, phrasal align-ment and target side language modeling yield cor-rect agreement inflection most of the time.
Forverbs, we find that the inflections are often accurate:number is encoded in the English verb and subject,and 3rd person is generally correct in the absenceof a 1st or 2nd person pronoun.
However, since En-glish does not generally encode gender, adjectiveinflection must rely on language modeling, whichis often insufficient.1The MT system learns significantly different weights forthe two LMs: 0.086 for the primary LM and 0.044 for theGigaword LM.To address this problem we apply an automaticinflection correction post-processing step.
First, wegenerate dependency parses of our system?s out-put using BONSAI (Candito and Crabb?, 2009),a French-specific extension to the Berkeley Parser(Petrov et al 2006).
Based on these dependencies,we match adjectives with the nouns they modifyand past participles with their subjects.
Then weuse Lefff (Sagot, 2010), a machine-readable Frenchlexicon, to determine the gender and number of thenoun and to choose the correct inflection for theadjective or participle.Applied to our 3,000 sentence development set,this correction scheme produced 200 correctionswith perfect accuracy.
It produces a slight (?0.014)drop in BLEU score.
This arises from cases wherethe reference translation uses a synonymous butdifferently gendered noun, and consequently hasdifferent adjective inflection.4.6 German De-compoundingSplit German compounds must be merged aftertranslation.
This process often requires insertingaffixes (e.g., s, en) between adjacent tokens in thecompound.
Since the German compounding rulesare complex and exception-laden, we rely on a dic-tionary lookup procedure with backoffs.
The dic-tionary was constructed during pre-processing.
Tocompound the final translations, we first lookupthe compound sequence?which is indicated bysegmentation markers?in the dictionary.
If it ispresent, then we use the dictionary entry.
If the com-pound is novel, then for each pair of words to becompounded, we insert the suffix most commonlyappended in compounds to the first word of the pair.If the first word itself is unknown in our dictionary,we insert the suffix most commonly appended afterthe last three characters.
For example, words end-ing with ung most commonly have an s appendedwhen they are used in compounds.4.7 RecasingPhrasal includes an LM-based recaser (Lita et al2003), which we trained on the target side of thebitext for each language.
On the newstest2012 de-velopment data, the German recaser was 96.8% ac-curate and the French recaser was 97.9% accurate.5 Translation Quality ExperimentsDuring system development we tuned onnewstest2008?2011 (10,570 sentences) and tested151#iterations #features tune newstest2012 newstest2013?Dense 10 20 30.26 31.12 ?Feature-rich 11 207k 32.29 31.51 29.00Table 2: En-Fr BLEU-4 [% uncased] results.
The tuning set is newstest2008?2011.
(?)
newstest2013 isthe cased score computed by the WMT organizers.#iterations #features tune newstest2012 newstest2013?Dense 10 19 16.83 18.45 ?Feature-rich 13 167k 17.66 18.70 18.50Table 3: En-De BLEU-4 [% uncased] results.on newstest2012 (3,003 sentences).
We comparethe feature-rich model to the ?dense?
baseline.The En-De system parameters were: 200-bestlists, a maximum phrase length of 8, and a distortionlimit of 6 with future cost estimation.
The En-Frsystem parameters were: 200-best lists, a maximumphrase length of 8, and a distortion limit of 5.The online tuning algorithm used a default learn-ing rate ?
= 0.03 and a mini-batch size of 20.
Weset the regularization strength ?
to 10.0 for the dis-criminative re-ordering model, 0.0 for the densefeatures, and 0.1 otherwise.5.1 ResultsTables 2 and 3 show En-Fr and En-De results, re-spectively.
The ?Feature-rich?
model, which con-tains the full complement of dense and sparse fea-tures, offers ameager improvement over the ?Dense?baseline.
This result contrasts with the resultsof Green et al(2013b), who showed significanttranslation quality improvements over the samedense baseline for Arabic-English and Chinese-English.
However, they had multiple target refer-ences, whereas the WMT data sets have just one.We speculate that this difference is significant.
Forexample, consider a translation rule that rewritesto a 4-gram in the reference.
This event can in-crease the sentence-level score, thus encouragingthe model to upweight the rule indicator feature.More evidence of overfitting can be seen in Fig-ure 1, which shows learning curves on the devel-opment set for both language pairs.
Whereas thedense model converges after just a few iterations,the feature-rich model continues to creep higher.Separate experiments on a held-out set showed thatgeneralization did not improve after about eightiterations.6 ConclusionWe submitted a feature-rich MT system to WMT2013.
While sparse features did offer a measur-able improvement over a baseline dense feature set,the gains were not as significant as those shownby Green et al(2013b).
One important differencebetween the two sets of results is the number of ref-erences.
Their NIST tuning and test sets had fourreferences; the WMT data sets have just one.
Wespeculate that sparse features tend to overfit morein this setting.
Individual features can greatly in-fluence the sentence-level metric and thus becomelarge components of the gradient.
To combat thisphenomenon we experimented with custom reg-ularization strengths and a more robust sentence-level metric.
While these two improvements greatlyreduced the model size relative to (Green et al2013b), a generalization problem remained.
Nev-ertheless, we showed that feature-rich models arenow competitive with the state-of-the-art.Acknowledgments This work was supported by the DefenseAdvanced Research Projects Agency (DARPA) Broad Opera-tional Language Translation (BOLT) program through IBM.Any opinions, findings, and conclusions or recommendationsexpressed in this material are those of the author(s) and do notnecessarily reflect the view of DARPA or the US government.ReferencesA.
Abeill?, L. Cl?ment, and A. Kinyon, 2003.
Buildinga treebank for French, chapter 10.
Kluwer.E.
Bi?ici and D. Yuret.
2011.
Instance selection formachine translation using feature decay algorithms.In WMT.M.
Candito and B. Crabb?.
2009.
Improving generativestatistical parsing with semi-supervised word cluster-ing.
In IWPT.152ll l l l ll l l llll ll ll ll l293031321 2 3 4 5 6 7 8 9 10EpochBLEUnewtest2008?2011Modellldensefeature?rich(a) En-Fr tuninglll l l l l ll llll l ll ll l l7.510.012.515.017.51 2 3 4 5 6 7 8 9 10EpochBLEUnewtest2008?2011Modellldensefeature?rich(b) En-De tuningFigure 1: BLEU-4 [% uncased] Learning curves on newstest2008?2011 with loess trend lines.D.
Cer, M. Galley, D. Jurafsky, and C. D. Manning.2010.
Phrasal: A statistical machine translationtoolkit for exploring new model features.
In HLT-NAACL, Demonstration Session.J.
Duchi and Y.
Singer.
2009.
Efficient online and batchlearning using forward backward splitting.
JMLR,10:2899?2934.J.
Duchi, E. Hazan, and Y.
Singer.
2011.
Adaptive sub-gradient methods for online learning and stochasticoptimization.
JMLR, 12:2121?2159.C.
Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture,et al2010.
cdec: A decoder, alignment, and learn-ing framework for finite-state and context-free trans-lation models.
In ACL System Demonstrations.C.
Dyer.
2009.
Using a maximum entropy model tobuild segmentation lattices for MT.
In NAACL.M.
Galley and C. D. Manning.
2008.
A simple andeffective hierarchical phrase reordering model.
InEMNLP.S.
Green, M. Galley, and C. D. Manning.
2010.
Im-proved models of distortion cost for statistical ma-chine translation.
In HLT-NAACL.S.
Green, M-C. de Marneffe, and C. D. Manning.2013a.
Parsing models for identifying multiwordexpressions.
Computational Linguistics, 39(1):195?227.S.
Green, S. Wang, D. Cer, and C. D. Manning.
2013b.Fast and adaptive online training of feature-rich trans-lation models.
In ACL.K.
Heafield, I. Pouzyrevsky, J. H. Clark, and P. Koehn.2013.
Scalable modified Kneser-Ney languagemodel estimation.
In ACL, Short Papers.K.
Heafield.
2011.
KenLM: Faster and smaller lan-guage model queries.
In WMT.M.
Hopkins and J.
May.
2011.
Tuning as ranking.
InEMNLP.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, et al2007.
Moses: Opensource toolkit for statistical machine translation.
InACL, Demonstration Session.P.
Liang, B. Taskar, and D. Klein.
2006.
Alignment byagreement.
In NAACL.C.-Y.
Lin and F. J. Och.
2004.
ORANGE: a method forevaluating automatic evaluation metrics for machinetranslation.
In COLING.L.
V. Lita, A. Ittycheriah, S. Roukos, and N. Kambhatla.2003.
tRuEcasIng.
In ACL.M.
Marcus, M. A. Marcinkiewicz, and B. Santorini.1993.
Building a large annotated corpus of En-glish: The Penn Treebank.
Computational Linguis-tics, 19:313?330.P.
Nakov, F. Guzman, and S. Vogel.
2012.
Optimizingfor sentence-level BLEU+1 yields short translations.In COLING.A.
Y. Ng.
2004.
Feature selection, L1 vs. L2 regular-ization, and rotational invariance.
In ICML.F.
J. Och and H. Ney.
2004.
The alignment templateapproach to statistical machine translation.
Compu-tational Linguistics, 30(4):417?449.F.
J. Och.
2003.
Minimum error rate training for statis-tical machine translation.
In ACL.S.
Petrov, L. Barrett, R. Thibaux, and D. Klein.
2006.Learning accurate, compact, and interpretable treeannotation.
In ACL.B.
Sagot.
2010.
The Lefff, a freely available andlarge-coverage morphological and syntactic lexiconfor French.
In LREC.153
