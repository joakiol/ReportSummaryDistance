Semi-Automatic Recognition of Noun Modifier RelationshipsKen BARKER and Stan SZPAKOWICZSchool of Information Technology and EngineeringUniversity of OttawaOttawa, Canada K1N 6N5{kbarker, szpak)@site.uottawa.caAbstractSemantic relationships among words andphrases are often marked by explicit syntacticor lexical clues that help recognize such rela-tionships in texts.
Within complex nominals,however, few overt clues are available.
Sys-tems that analyze such nominals must com-pensate for the lack of surface clues withother information.
One way is to load thesystem with lexical semantics for nouns oradjectives.
This merely shifts the problemelsewhere: how do we define the lexical se-mantics and build large semantic lexicons?Another way is to find constructions similarto a given complex nominal, for which therelationships are already known.
This is theway we chose, but it too has drawbacks.Similarity is not easily assessed, similar ana-lyzed constructions may not exist, and if theydo exist, their analysis may not be appropriatefor the current nominal.We present a semi-automatic system thatidentifies semantic relationships in nounphrases without using precoded noun or ad-jective semantics.
Instead, partial matching onpreviously analyzed noun phrases leads to atentative interpretation of a new input.
Proc-essing can start without prior analyses, but theearly stage requires user interaction.
As morenoun phrases are analyzed, the system learnsto find better interpretations and reduces itsreliance on the user.
In experiments on Eng-lish technical texts the system correctly iden-tified 60-70% of relationships automatically.1 IntroductionAny system that extracts knowledge from textcannot ignore complex noun phrases.
In technicaldomains especially, noun phrases carry much ofthe information.
Part of that information is con-tained in words; cataloguing the semantics of sin-gle words for computational purposes is a difficulttask that has received much attention.
But part ofthe information in noun phrases is contained in therelationships between components.We have built a system for noun modifier e-lationship (NMR) analysis that assigns semanticrelationships in complex noun phrases.
Syntacticanalysis finds noun phrases in a sentence and pro-vides a flat list of premodifiers and postmodifyingprepositional phrases and appositives.
The NMRanalyzer first brackets the flat list of premodifiersinto modifier-head pairs.
Next, it assigns NMRs toeach pair.
NMRs are also assigned to the relation-ships between the noun phrase and each post-modifying phrase.2 Background2.1 Noun CompoundsA head noun along with a noun premodifier isoften called a noun compound.
Syntactically anoun compound acts as a noun: a modifier or ahead may again be a compound.
The NMR ana-lyzer deals with the semantics of a particular kindof compound, namely those that are transparentand endocentric.The meaning of a transparent compound canbe derived from the meaning of its elements.
Forexample, laser printer is transparent (a printerthat uses a laser).
Guinea pig is opaque: there isno obvious direct relationship to guinea or to pig.An endocentric compound is a hyponym of itshead.
Desktop computer is endocentric because itis a kind of computer.
Bird brain is exocentricbecause it does not refer to a kind of brain, butrather to a kind of person (whose brain resemblesthat of a bird).Since the NMR analyzer is intended for tech-nical texts, the restriction to transparent endocen-tric compounds should not limit the utility of thesystem.
Our experiments have found no opaque orexocentric compounds in the test texts.962.2 Semantic Relations in Noun PhrasesMost of the research on relationships betweennouns and modifiers deals with noun compounds,but these relationships also hold between ounsand adjective premodifiers or postmodifyingprepositional phrases.
Lists of semantic labelshave been proposed, based on the theory that acompound expresses one of a small number ofcovert semantic relations.Levi (1978) argues that semantics and wordformation make noun-noun compounds a hetero-geneous class.
She removes opaque compoundsand adds nominal non-predicating adjectives.
Forthis class Levi offers nine semantic labels.
Ac-cording to her theory, these labels represent un-derlying predicates deleted during compoundformation.
George (1987) disputes the claim thatLevi's non-predicating adjectives never appear inpredicative position.Warren (1978) describes a multi-level systemof semantic labels for noun-noun relationships.Warren (1984) extends the earlier work to coveradjective premodifiers as well as nouns.
Thesimilarity of the two lists suggests that many ad-jectives and premodifying nouns can be handledby the same set of semantic relations.2.3 Recognizing Semantic RelationsPrograms that uncover the relationships in modi-fier-noun compounds often base their analysis onthe semantics of the individual words (or a com-position thereof).
Such systems assume the exis-tence of some semantic lexicon.Leonard's ystem (1984) assigns emantic la-bels to noun-noun compounds based on a diction-ary that includes taxonomic and meronymic (part-whole) information, information about he syntac-tic behaviour of nouns and about he relationshipsbetween ouns and verbs.
Finin (1986) producesmultiple semantic interpretations of modifier-nouncompounds.
The interpretations are based on pre-coded semantic class information and domain-dependent frames describing the roles that can beassociated with certain nouns.
Ter Stal's system(1996) identifies concepts in text and unifies themwith structures extracted from a hand-coded lexi-con containing syntactic information, logical formtemplates and taxonomic information.In an attempt o avoid the hand-coding re-quired in other systems, Vanderwende (1993)automatically extracts emantic features of nounsfrom online dictionaries.
Combinations of featuresimply particular semantic interpretations of therelationship between two nouns in a compound.3 Noun Modifier Relationship LabelsTable 1 lists the NMRs used by our analyzer.
Thelist is based on similar lists found in literature onthe semantics of noun compounds.
It may evolveas experimental evidence suggests changes.Agent (agt)Beneficiary (benf)Cause (caus)Container (ctn)Content (cont)Destination (dest)Equative (equa)Instrument (inst)Located (led)Location (loc)Material (matr)Object (obj)Possessor (poss)Product (prod)Property (prop)Purpose (purp)Result (resu)Source (src)Time (time)Topic (top)Table 1: The noun modifier elationshipsFor each NMR, we give a paraphrase and examplemodifier-noun compounds.
Following the tradi-tion in the study of noun compound semantics, theparaphrases act as definitions and can be used tocheck the acceptability of different interpretationsof a compound.
The paraphrases serve as defini-tions in this section and to help with interpretationduring user interactions (as illustrated in section6).
In the analyzer, awkward paraphrases withadjectives could be improved by replacing adjec-tives with their WordNet pertainyms (Miller,1990), giving, for example, "charity benefits fromcharitable donation" instead of "charitable bene-fits from charitable donation".Agent: compound isperformed by modifierstudent protest, band concert, military assaultBeneficiary: modifier benefits from compoundstudent price, charitable donationCause: modifier causes compoundexam anxiety, overdue fineContainer: modifier contains compoundprinter tray, flood water, film music, story ideaContent: modifier is contained in compoundpaper tray, eviction otice, oil panDestination: modifier is destination of compoundgame bus, exit route, entrance stairs97Equative: modifier is also headcomposer arranger, player coachInstrument: modifier is used in compoundelectron microscope, diesel engine, laser printerLocated: modifier is located at compoundbuilding site, home town, solar systemLocation: modifier is the location of compoundlab printer, internal combustion, desert stormMaterial: compound ismade of modifiercarbon deposit, gingerbread man, water vapottrObject: modifier is acted on by compoundengine repair, horse doctorPossessor: modifier has compoundnational debt, student loan, company carProduct: modifier is a product of compoundautomobile factory, light bulb, colour printerProperty: compound is modifierblue car, big house, fast computerPurpose: compound is meant for modifierconcert hall soup pot, grinding abrasiveResult: modifier is a result of compoundstorm cloud, cold virus, death penaltySource: modifier is the source of compoundforeign capital, chest pain, north windTime: modifier is the time of compoundwinter semester, late supper, morning classTopic: compound is concerned with modifiercomputer expert, safety standard, horror novel4 Noun Modifier BracketingBefore assigning NMRs, the system must bracketthe head noun and the premodifier sequence intomodifier-head pairs.
Example (2) shows thebracketing for noun phrase (1).
(1) dynamic high impedance microphone(2) (dynamic ((high impedance) microphone))The bracketing problem for noun-noun-nouncompounds has been investigated by Liberrnan &Sproat (1992), Pustejovsky et al (1993), Resnik(1993) and Lauer (1995) among others.
Since theNMR analyzer must handle premodifier se-quences of any length with both nouns and adjec-tives, it requires more general techniques.
Oursemi-automatic bracketer (Barker, 1998) allowsfor any number of adjective or noun premodifiers.After bracketing, each non-atomic element ofa bracketed pair is considered a subphrase of theoriginal phrase.
The subphrases for the bracketingin (2) appear in (3), (4) and (5).
(3) high impedance(4) high_impedance microphone(5) dynamic high_impedance_microphoneEach subphrase consists of a modifier (possiblycompound, as in (4)) and a head (possibly com-pound, as in (5)).
The NMR analyzer assigns anNMR to the modifier-head pair that makes upeach subphrase.Once an NMR has been assigned, the systemmust store the assignment to help automate futureprocessing.
Instead of memorizing complete nounphrases (or even complete subphrases) and analy-ses, the system reduces compound modifiers andcompound heads to their own local heads andstores these reduced pairs with their assignedNMR.
This allows it to analyze different nounphrases that have only reduced pairs in commonwith previous phrases.
For example, (6) and (7)have the reduced pair (8) in common.
If (6) hasalready been analyzed, its analysis can be used toassist in the analysis of (7)--see section 5.1.
(6) (dynamic ((high impedance) microphone))(7) (dynamic (cardioid (vocal microphone)))(8) (dynamic microphone)5 Assigning NMRsThree kinds of construction require NMR assign-ments: the modifier-head pairs from the bracketedpremodifier sequence; postmodifying preposi-tional phrases; appositives.These three kinds of input can be generalizedto a single form--a triple consisting of modifier,head and marker (M, H, Mk).
For premodifiers,Mk is the symbol nil, since no lexical item linksthe premodifier to the head.
For postmodifyingprepositional phrases Mk is the preposition.
Forappositives, Mk is the symbol appos.
The(M, H, Mk) triples for examples (9), (10) and (11)appear in Table 2.
(9) monitor cable plug(10) large piece of chocolate cake(11) my brother, a friend to all young peopleTo assign an NMR to a triple (M, H, Mk), thesystem looks for previous triples whose distanceto the current riple is minimal.
The NMRs as-signed to previous imilar triples comprise lists ofcandidate NMRs.
The analyzer then finds what itconsiders the best NMR from these lists of candi-98Modifier Head Markermonitor cable nilmonitor_cable plug nilchocolate cake nillarge piece nilchocolate_cake large_piece ofyoung people nilyoung_people friend tofriend brother apposTable 2: (M, H, Mk) triples for (9), (I0) and (11)dates to present o the user for approval.
Apposi-tives are automatically assigned Equative.5.1 Distance Between TriplesThe distance between two triples is a measure ofthe degree to which their modifiers, heads andmarkers match.
Table 3 gives the eight differentvalues for distance used by NMR analysis.The analyzer looks for previous triples at thelower distances before attempting tofind triples athigher distances.
For example, it will try to findidentical triples before trying to find triples whosemarkers do not match.Several things about the distance measuresrequire explanation.
First, a preposition is moresimilar to a nil marker than to a different preposi-tion.
Unlike a different preposition, the nil markeris not known to be different from the marker in anovertly marked pair.Next, no evidence suggests that triples withmatching M are more similar or less similar thantriples with matching H (distances 3 and 6).Triples with matching prepositional marker(distance 4) are considered more similar than tri-ples with matching M or H only.
A preposition isan overt indicator of the relationship between Mand H (see Quirk, 1985: chapter 9) so a correla-tion is more likely between the preposition and theNMR than between agiven M or H and the NMR.If the current riple has a prepositional markernot seen in any previous triple (distance 5), thesystem finds candidate NMRs in its NMR markerdictionary.
This dictionary was constructed from alist of about 50 common atomic and phrasalprepositions.
The various meanings of eachpreposition were mapped to NMRs by hand.
Sincethe list of prepositions i small, dictionary con'struction was not a difficult knowledge ngineer-ing task (requiring just twenty hours of work of asecondary school student).5.2 The Best NMRsThe lists of candidate NMRs consist of all thoseNMRs previously assigned to (M, H, Mk) triplesat a minimum distance from the triple underanalysis.
If the minimum distance was 3 or 6,there may be two candidate lists: LM contains theNMRs previously assigned to triples with match-ing M, L,-with matching H. The analyzer at-tempts to choose a set R of candidates to suggestto the user as the best NMRs for the current riple,If there is one list L of candidate NMRs, Rcontains the NMR (or NMRs) that occur mostfrequently in L For two lists LM and L,, R couldbe found in several ways, We could take R tocontain the most frequent NMRs in LM u L,.
Thisabsolute frequency approach as a bias towardsNMRs in the larger of the two lists.Alternatively, the system could prefer NMRswith the highest relative frequency in their lists.
Ifthere is less variety in the NMRs in LM than in LH,M might be a more consistent indicator of NMRthan H. Consider example (12).
(12) front lineCompounds with the modifier front may alwayshave been assigned Location.
Compounds withdist current riple0 (M, H, Mk)1 (M, H, <prep>)2 (M, H, Mk)3 (M, H, Mk)4 (M, H, <prep>)5 (M, H, <prep>)6 (M, H, Mk)7 (M, H, Mk)previous triple example(M, H, Mk)(M, H, nil)(M, H,_)(M, _, Mk) or (_, H, Mk)( .
.
.
.
<prep>)(_ .
.
.
.
)(M .
.
.
.  )
or (_, H, _)( .
.
.
.
.
)wall beside a garden wall beside a gardenwall beside a garden garden wallwall beside a garden wall around a gardenpile of garbage pile of sweaterspile of garbage house of bricksice in the cup nmrm(in, \[ctn,inst, loc,src,time\])wall beside a garden garden fencewall beside a garden pile of garbageTable 3: Measures of distance between triples99the head line may have been assigned many dif-ferent NMRs.
If line has been seen as a head moreoften than front as a modifier, one of the NMRsassigned to line may have the highest absolutefrequency in LM u LH.
But if Location has thehighest relative frequency, this method correctlyassigns Location to (12).
There is a potential bias,however, for smaller lists (a single N-MR in a listalways has the highest relative frequency).To avoid these biases, we could combine ab-solute and relative frequencies.
Each NMR i isassigned a score si calculated as:freq(i ~ Lu) 2 freq(i e LH) 2s, = + IL.IR would contain NMR(s) with the highest score.This combined formula was used in the experi-ment described in section 7.5.3 Premodifiers as Classif iersSince NMR analysis deals with endocentric com-pounds we can recover a taxonomic relationshipfrom triples with a nil marker.
Consider example(13) and its reduced pairs in (14):(13) ((laser printer) stand)(14) (laser printer)(printer stand)These pairs produce the following output:laser..printer_stand isastandlaser_.printer isa printer6 User InteractionThe NMR analyzer is intended to start processingfrom scratch.
A session begins with no previoustriples to match against the triple at hand.
Tocompensate for the lack of previous analyses, thesystem relies on the help of a user, who suppliesthe correct NMR when the system cannot deter-mine it automatically.In order to supply the correct NMR, or evento determine if the suggested NMR is correct, theuser must be familiar with the NMR definitions.To minimize the burden of this requirement, allinteractions use the modifier and head of the cur-rent phrase in the paraphrases from section 3.Furthermore, if the appropriate NMR is notamong those suggested by the system, the usercan request he complete list of paraphrases withthe current modifier and head.6.1 An ExampleFigure 1 shows the interaction for phrases (15)-(18).
The system starts with no previously ana-lyzed phrases.
The NMR marker dictionary mapsthe preposition of to twelve NMRs: Agent, Cause,Content, Equative, Located, Material, Object,Possessor, Property, Result, Source, Topic.
(15) small gasoline ngine(16) the repair of diesel engines(17) diesel engine repair shop(18) an auto repair centerUser input is shown bold underlined.
At anyprompt he user may type 'list' to view the com-plete list of NMR paraphrases for the currentmodifier and head.7 EvaluationWe present he results of evaluating the NMRanalyzer in the context of a large knowledge ac-quisition experiment (see Barker et al, 1998).
TheNMR analyzer is one part of a larger interactivesemantic analysis system.
The experiment evalu-ated the semantic analysis of Atkinson (1990).
Werefer to it as the small engines experiment.
Otherexperiments have shown similar results.We consider three evaluation criteria.
First,we evaluate the analyzer's ability to learn to makebetter suggestions to the user as more nounphrases are analyzed.
Second, we evaluate itscoverage by comparing the number of relation-ships assigned with the total number of such rela-tionships in the text (i.e., the number it shouldhave assigned).
Third, we assess the burden thatsemi-automatic analysis places on the user.7.1 Improvement in System Per formanceSince the system starts with no previous nounphrase analyses, the user is responsible for sup-plying NMRs at the beginning of a session.
Tomeasure the rate of learning, we compare the cu-mulative number of assignments required from theuser to the cumulative number of correct assign-ments uggested by the system.In the small engines experiment, 886 modi-fier-noun pairs were assigned an NMR.
We con-sider the system's assignment correct when thecorrect label is among its suggestions.
Accordingto this definition, 608 of the 886 NMRs (69%)were assigned correctly by the system.
For most100of these assignments (97.5%) the system offered asingle suggestion.
It had multiple (on average 3.3)suggestions only 22 times.Phrase (15): small gasoline engineThere is a relationship betweengasoline and gasoline_engine.Please enter a valid NMR label: instDo you accept the NMR Instrument:gasoline is used in gasoline__engine Y_There is a relationship betweensmall and small_gasoline_engine.Please enter a valid NMR label: propDo you accept the NMR Property:small_gasoline__engine is small YPhrase (16): the repair of diesel enginesThere is a relationship betweendiesel and diesel_engine.NMR Analyzer's best suggestions for this input:(1) prop: diesel_engine is diesel(2) inst: diesel is used in diesel_enginePlease enter a number between 1 and 2: _2Do you accept the NMR Instrument:diesel is used in diesel_engine YThere is a relationship betweendiesel_engine and repair.NMR Analyzer's best suggestions for this input:(1) agt: repairis performed by dieselengine(2) caus: diesel_engine causes repair(7) obj: diesel_engine is acted on by repair(12) top: repairis concerned with diesel_enginePlease enter a number between 1 and 12: 7Do you accept the NMR Object:diesel_en~lin e is acted on by repair YPhrase (17): diesel engine repair shopDo you accept the NMR Instrument:diesel is used in diesel_engine Y__Do you accept the NMR Object:diesel_engine is acted on by diesel_engine_.repair YThere is a relationship betweendiesel_ engine_repair and diesel_enginerepair_shop.Please enter a valid NMR label: purpDo you accept the NMR Purpose:dieselengine_repair__shop is meant fordieseC engine_repair YPhrase (18): an auto repair centerDo you accept the NMR Object:auto is acted on by auto_repair YDo you accept the NMR Purpose:auto_repair_ centeris meant for auto_repair YFigure I: NMR analysis interaction for (15)-(18)Figure 2 shows the cumulative number ofNMR assignments upplied by the user versusthose determined correctly by the system.
Afterabout 100 assignments, the system was able tomake the majority of assignments automatically.The curves in the figure show that the systemlearns to make better suggestions as more phrasesare analyzed.700600Esoo'~ 400E 300~ 2o0i 1000-zTj" -E~E E EE~ ~numb er of m edifier-noun pairsFigure 2: Cumulative NMR assignments7.2 NMR CoverageThe NMR analyzer depends on a parser to findnoun phrases in a text.
If parsing is not 100% suc-cessful, the analyzer will not see all noun phrasesin the input text.
It is not feasible to find manuallythe total number of relationships in a text--evenin one of only a few hundred sentences.
To meas-ure coverage, we sampled 100 modifier-nounpairs at random from the small engines text andfound that 87 of them appeared in the analyzer'soutput.
At 95% confidence, we can say that thesystem extracted between 79.0% and 92.2% of themodifier-noun relationships in the text.7.3 User BurdenUser burden is a fairly subjective criterion.
Tomeasure burden, we assigned an "onus" rating toeach interaction during the small engines experi-ment.
The onus is a number from 0 to 3 .0  meansthat the correct NMR was obvious, whether sug-gested by the system or supplied by the user.
1means that selecting an NMR required a few mo-ments of reflection.
A rating of 2 means that theinteraction required serious thought, but we were101ultimately able to choose an NMR.
3 means thateven after much contemplation, we were unable toagree on an NMR.The average user onus rating was 0.1 forNMR interactions in the small engines experi-ment.
808 of the 886 NMR assignments receivedan onus rating of 0; 71 had a rating of 1; 7 re-ceived a rating of 2.
No interactions were ratedonus level 3.8 Future WorkAlthough the list of NMRs was inspired by therelationships found commonly in others' lists, ithas not undergone a more rigorous validation(such as one described in Barker et al, 1997).In section 5.2 we discussed different ap-proaches to choosing NMRs from two lists ofcandidates.
We have implemented and comparedfive different techniques for choosing the bestNMRs, but experimental results are inconclusiveas to which techniques are better.
We should seeka more theoretically sound approach followed byfurther experimentation.The NMR analyzer currently allows its storedtriples (and associated NMRs) to be saved in a fileat the end of a session.
Any number of such filescan be reloaded at the beginning of subsequentsessions, "seeding" the new sessions.
It is neces-sary to establish the extent o which the triples andassignments from one text or domain are useful inthe analysis of noun phrases from another domain.AcknowledgementsThis work is supported by the Natural Sciencesand Engineering Research Council of Canada.ReferencesAtkinson, Henry F. (1990).
Mechanics of Small En-gines.
New York: Gregg Division, McGraw-Hill.Barker, Ken (1998).
"A Trainable Bracketer for NounModifiers".
The Twelfth Canadian Conference onArtificial Intelligence.Barker, Ken, Terry Copeck, Sylvain Delisle & StanSzpakowicz (1997).
"Systematic Construction of aVersatile Case System."
Journal of Natural Lan-guage Engineering 3(4), December 1997.Barker, Ken, Sylvain Delisle & Stan Szpakowicz(1998).
"Test-Driving TANKA: Evaluating a Semi-Automatic System of Text Analysis for KnowledgeAcquisition."
The Twelfth Canadian Conference onArtificial Intelligence.Finin, Timothy W. (1986).
"Constraining the Interpre-tation of Nominal Compounds in a Limited Con-text."
In Analyzing Language in Restricted Domains:Sublanguage Description and Processing, R. Grish-man & R. Kittredge, eds., Lawrence Erlbaum,Hillsdale, pp.
163-173.George, Steffi (1987).
On "Nominal Non-Predicating"Adjectives in English.
Frankfurt am Main: PeterLang.Lauer, Mark (1995).
"Corpus Statistics Meet the NounCompound: Some Empirical Results."
Proceedingsof the 33rd Annual Meeting of the Association forComputational Linguistics.
Cambridge.
47-54.Leonard, Rosemary (1984).
The Interpretation of Eng-lish Noun Sequences on the Computer.
Amsterdam:North-Holland.Levi, Judith N. (1978).
The Syntax and Semantics ofComplex Nominals.
New York: Academic Press.Liberman, Mark & Richard Sproat (1992).
"Stress andStructure of Modified Noun Phrases."
Lexical Mat-ters (CSLI Lecture Notes, 24).
Stanford: Center forthe Study of Language and Information.Miller, George A., ed.
(1990).
"WordNet: An On-LineLexical Database."
International Journal of Lexicog-raphy 3(4).Pustejovsky, James, S. Bergler & P. Anick (1993).
"Lexical Semantic Techniques for Corpus Analysis.
"Computational Linguistics 19(2).
331-358.Quirk, Randolph, Sidney Greenbaum, Geoffrey Leech& Jan Svartvik (1985).
A Comprehensive Grammarof the English Language.
London: Longman.Resnik, Philip Stuart (1993).
"Selection and Informa-tion: A Class-Based Approach to Lexical Relation-ships."
Ph.D. thesis, IRCS Report 93-42, Universityof Pennsylvania.ter Stal, Wilco (1996).
"Automated Interpretation ofNominal Compounds in a Technical Domain."
Ph.D.thesis, University of Twente, The Netherlands.Vanderwende, Lucy (1993).
"SENS: The System forEvaluating Noun Sequences."
In Natural LanguageProcessing: The PLNLP Approach, K. Jensen, G.Heidorn & S. Richardson, eds., Kluwer AcademicPublishers, Boston, pp.
161-173.Warren, Beatrice (1978).
Semantic Patterns of Noun-Noun Compounds.
G/Steborg: Acta UniversitatisGothoburgensis.Warren, Beatrice (1984).
Classifying Adjectives.
GSte-borg: Acta Universitatis Gothoburgensis.102
