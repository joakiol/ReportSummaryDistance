Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2377?2382,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsLearning a Lexicon and Translation Model from Phoneme LatticesOliver Adams,??
Graham Neubig,??
Trevor Cohn,?Steven Bird,?
Quoc Truong Do,?
Satoshi Nakamura?
?The University of Melbourne, Australia?Carnegie Mellon University, Pittsburgh, PA, USA?Nara Institute of Science and Technology, JapanAbstractLanguage documentation begins by gather-ing speech.
Manual or automatic transcrip-tion at the word level is typically not possi-ble because of the absence of an orthographyor prior lexicon, and though manual phone-mic transcription is possible, it is prohibitivelyslow.
On the other hand, translations of theminority language into a major language aremore easily acquired.
We propose a methodto harness such translations to improve auto-matic phoneme recognition.
The method as-sumes no prior lexicon or translation model,instead learning them from phoneme latticesand translations of the speech being tran-scribed.
Experiments demonstrate phonemeerror rate improvements against two baselinesand the model?s ability to learn useful bilin-gual lexical entries.1 IntroductionMost of the world?s languages are dying out andhave little recorded data or linguistic documentation(Austin and Sallabank, 2011).
It is important to ad-equately document languages while they are aliveso that they may be investigated in the future.
Lan-guage documentation traditionally involves one-on-one elicitation of speech from native speakers in or-der to produce lexicons and grammars that describethe language.
However, this does not scale: lin-guists must first transcribe the speech phonemicallyas most of these languages have no standardizedorthography.
This is a critical bottleneck since ittakes a trained linguist about 1 hour to transcribe thephonemes of 1 minute of speech (Do et al, 2014).Smartphone apps for rapid collection of bilin-gual data have been increasingly investigated (DeVries et al, 2011; De Vries et al, 2014; Reiman,2010; Bird et al, 2014; Blachon et al, 2016).
It iscommon for these apps to collect speech segmentspaired with spoken translations in another language,making spoken translations quicker to obtain thanphonemic transcriptions.We present a method to improve automaticphoneme transcription by harnessing such bilingualdata to learn a lexicon and translation model directlyfrom source phoneme lattices and their written tar-get translations, assuming that the target side is amajor language that can be efficiently transcribed.1A Bayesian non-parametric model expressed with aweighted finite-state transducer (WFST) frameworkrepresents the joint distribution of source acousticfeatures, phonemes and latent source words giventhe target words.
Sampling of alignments is usedto learn source words and their target translations,which are then used to improve transcription of thesource audio they were learnt from.
Importantly,the model assumes no prior lexicon or translationmodel.This method builds on work on phoneme transla-tion modeling (Besacier et al, 2006; Stu?ker et al,2009; Stahlberg et al, 2012; Stahlberg et al, 2014;Adams et al, 2015; Duong et al, 2016), speechtranslation (Casacuberta et al, 2004; Matusov etal., 2005), computer-aided translation, (Brown et al,1994; Vidal et al, 2006; Khadivi and Ney, 2008;Reddy and Rose, 2010; Pelemans et al, 2015),translation modeling from automatically transcribed1Code is available at https://github.com/oadams/latticetm.2377speech (Paulik and Waibel, 2013), word segmenta-tion and translation modeling (Chang et al, 2008;Dyer, 2009; Nguyen et al, 2010; Chen and Xu,2015), Bayesian word alignment (Mermer et al,2013; Zezhong et al, 2013) and language modellearning from lattices (Neubig et al, 2012).
Whilewe previously explored learning a translation modelfrom word lattices (Adams et al, 2016), in this paperwe extend the model to perform unsupervised wordsegmentation over phoneme lattices in order to im-prove phoneme recognition.Experiments demonstrate that our method signifi-cantly reduces the phoneme error rate (PER) of tran-scriptions compared with a baseline recogniser anda similar model that harnesses only monolingual in-formation, by up to 17% and 5% respectively.
Wealso find that the model learns meaningful bilinguallexical items.2 Model descriptionOur model extends the standard automatic speechrecognition (ASR) problem by seeking the bestphoneme transcription ??
of an utterance in a jointprobability distribution that incorporates acousticfeatures x, phonemes ?, latent source words f andobserved target transcriptions e:??
= argmax?,fP (x|?
)P (?|f)P (f |e) , (1)assuming a Markov chain of conditional indepen-dence relationships (bold symbols denote utter-ances as opposed to tokens).
Deviating from stan-dard ASR, we replace language model probabilitieswith those of a translation model, and search forphonemes instead of words.
Also, no lexicon ortranslation model are given in training.2.1 Expression of the distribution usingfinite-state transducersWe use a WFST framework to express the factors of(1) since it offers computational tractability and sim-ple inference in a clear, modular framework.
Fig-ure 1 uses a toy German?English error resolutionexample to illustrate the components of the frame-work: a phoneme lattice representing phoneme un-certainty according to P (x|?
); a lexicon that trans-duces phoneme substrings ?s of ?
to source tokensf according to P (?s|f); and a lexical translationmodel representing P (f |e) for each e in the writtentranslation.
The composition of these componentsis also shown at the bottom of Figure 1, illustratinghow would-be transcription errors can be resolved.This framework is reminiscent of the WFST frame-work used by Neubig et al (2012) for lexicon andlanguage model learning from monolingual data.2.2 Learning the lexicon and translation modelBecause we do not have knowledge of the sourcelanguage, we must learn the lexicon and translationmodel from the phoneme lattices and their writtentranslation.
We model lexical translation probabil-ities using a Dirichlet process.
Let A be both thetranscription of each source utterance f and its wordalignments to the translation e that generated them.The conditional posterior can be expressed as:P (f |e;A) = cA(f, e) + ?P0(f)cA(e) + ?, (2)where cA(f, e) is a count of how many times f hasaligned to e in A and cA(e) is a count of how manytimes e has been aligned to; P0 is a base distributionthat influences how phonemes are clustered; and ?determines the emphasis on the base distribution.In order to express the Dirichlet process using theWFST components, we take the union of the lexi-con with a spelling model base distribution that con-sumes phonemes ?i .
.
.
?j and produces a special?unk?
token with probability P0(?i .
.
.
?j).
This?unk?
token is consumed by a designated arc in thetranslation model WFST with probability ?cA(e)+?
,yielding a composed probability of ?P0(f)cA(e)+?
.
Otherarcs in the translation model express the probabilitycA(f,e)cA(e)+?
of entries already in the lexicon.
The sumof these two probabilities equates to (2).As for the spelling model P0, we consider threedistributions and implement WFSTs to representthem: a geometric distribution, Geometric(?
), aPoisson distribution, Poisson(?
),2 and a ?shifted?
ge-ometric distribution, Shifted(?, ?).
The shifted ge-ometric distribution mitigates a shortcoming of thegeometric distribution whereby words of length 1have the highest probability.
It does so by having2While the geometric distribution can be expressed recur-sively, we cap the number of states in the Poisson WFST to100.2378start [h]/1.0[aU]/0.6[o:]/0.4[s]/0.6[f]/0.4start[h]:/1[aU]:/1 [s]:/1:[haUs]/1[h]:/1[o:]:/1 [f]:/1:[ho:f]/1start[haUs]:yard/0.1[ho:f]:yard/0.9start[h]:/1.0 [aU]:/0.6 [s]:/0.6 :yard/0.1[h]:/1.0[o:]:/0.4 [f]:/0.4 :yard/0.9Figure 1: Top left to right: the phoneme lattice, the lexicon, and the translation model.
Bottom: the resultingcomposed WFST.
Given an English translation ?yard?, the most likely transcription is corrected to [ho:f](?Hof?)
in the composed WFST, while in the original phoneme lattice it is [haUs] (?Haus?).
Solid edgesrepresent most likely paths.another parameter ?
that specifies the probability ofa word of length 1, with the remaining probabilitymass distributed geometrically.
All phonemes typesare treated the same in these distributions, with uni-form probability.2.3 InferenceIn order to determine the translation model param-eters as described above, we require the alignmentsA.
We sample these proportionally to their probabil-ity given the data and our prior, in effect integratingover all parameter configurations T :P (A|X ;?, P0) =?TP (A|X , T )P (T ;?, P0)dT ,(3)where X is our dataset of source phoneme latticespaired with target sentences.This is achieved using blocked Gibbs sam-pling, with each utterance constituting oneblock.
To sample from WFSTs, we use forward-filtering/backward-sampling (Scott, 2002; Neubiget al, 2012), creating forward probabilities usingthe forward algorithm for hidden Markov modelsbefore backward-sampling edges proportionally tothe product of the forward probability and the edgeweight.33No Metropolis-Hastings rejection step was used.3 Experimental evaluationWe evaluate the lexicon and translation model bytheir ability to improve phoneme recognition, mea-suring phoneme error rate (PER).3.1 Experimental setupWe used less than 10 hours of English?Japanese datafrom the BTEC corpus (Takezawa et al, 2002), com-prised of spoken utterances paired with textual trans-lations.
This allows us to assess the approach as-suming quality acoustic models.
We used acous-tic models similar to Heck et al (2015) to obtainsource phoneme lattices.
Gold phoneme transcrip-tions were obtained by transforming the text withpronunciation lexicons and, in the Japanese case,first segmenting the text into tokens using KyTea(Neubig et al, 2011).We run experiments in both directions: English?Japanese and Japanese?English (en?ja and ja?en),while comparing against three settings: the ASR 1-best path uninformed by the model (ASR); a mono-lingual version of our model that is identical exceptwithout conditioning on the target side (Mono); andthe model applied using the source language sen-tence as the target (Oracle).We tuned on the first 1,000 utterences (about 1hour) of speech and trained on up to 9 hours of the2379English (en) Japanese (ja)Mono ?ja Oracle Mono ?en OracleASR 22.1 24.3Vague 17.7 18.5 17.2 21.5 20.8 21.6Shifted 17.4 16.9 16.6 21.2 20.1 20.2Poisson 17.3 17.2 16.8 21.3 20.1 20.8Table 1: Phoneme error rates (percent) when train-ing on 9 hours of speech, averaged over 4 runs.2 4 6 8 10202122232425Training hoursPhonemeerrorrate(%)ASR; Oracle; ?en; MonoFigure 2: Japanese phoneme error rates using theshifted geometric prior when training data is scaledup from 1?9 hours, averaged over 3 runs.remaining data.4 Only the oracle setup was usedfor tuning, with Geometric(0.01) (taking the form ofa vague prior), Shifted(10?5, 0.25) and Poisson(7)performing best.3.2 Results and DiscussionTable 1 shows en?ja and ja?en results for all meth-ods with the full training data.
Figure 2 shows im-provements of ja?en over both the ASR baseline andthe Mono method as the training data increases, withtranslation modeling gaining an increasing advan-tage with more training data.Notably, English recognition gains less from us-ing Japanese as the target side (en?ja) than theother way around, while the ?oracle?
approach forJapanese recognition, which also uses Japanese asthe target, underperforms ja?en.
These observationssuggest that using the Japanese target is less help-ful, likely explained by the fine-grained morpholog-ical segmentation we used, making it harder for themodel to relate source phonemes to target tokens.The vague geometric prior significantly underper-forms the other priors.
In the en?ja/vague case, the4A 1 hour subset was used for PER evaluation.model actually underperforms its monolingual coun-terpart.
The vague prior biases slightly towards fine-grained English source segmentation, with words oflength 1 most common.
In this case, fine-grainedJapanese is also used as the target which resultsin most lexical entries arising from uninformativealignments between single English phonemes andJapanese syllables, such as [t]??.
For similar rea-sons, the shifted geometric prior gains an advantageover Poisson, likely because of its ability to even fur-ther penalize single-phoneme lexical items, whichregularly end up in all lexicons anyway due to theircombinatorical advantage when sampling.While many bilingual lexical entries are correct,such as [w2n]??
(?one?
), most are not.
Somehave segmentation errors [li:z]????
(?please?
);some are correctly segmented but misaligned tocommonly co-occurring words [w2t]??
(?what?aligned to ?time?
); others do not constitute indi-vidual words, but morphemes aligned to commonJapanese syllables [i:N]??
(?-ing?
); others stillalign multi-word units correctly [haUm2tS]????
(?how much?).
Note though that entries such asthose listed above capture information that may nev-ertheless help to reduce phoneme transcription er-rors.4 Conclusion and Future WorkWe have demonstrated that a translation model andlexicon can be learnt directly from phoneme latticesin order to improve phoneme transcription of thosevery lattices.One of the appealing aspects of this modularframework is that there is much room for exten-sion and improvement.
For example, by using adap-tor grammars to encourage syllable segmentation(Johnson, 2008), or incorporating language modelprobabilities in addition to our translation modelprobabilities (Neubig et al, 2012).We assume a good acoustic model with phonemeerror rates between 20 and 25%.
In a language doc-umentation scenario, acoustic models for the low-resource source language won?t exist.
Future workshould use a universal phoneme recognizer or acous-tic model of a similar language, thus making a steptowards true generalizability.2380AcknowledgmentsWe gratefully acknowledge support from theDARPA LORELEI program.ReferencesOliver Adams, Graham Neubig, Trevor Cohn, and StevenBird.
2015.
Inducing bilingual lexicons from smallquantities of sentence-aligned phonemic transcrip-tions.
In Proceedings of the International Workshopon Spoken Language Translation (IWSLT 2015), DaNang, Vietnam.Oliver Adams, Graham Neubig, Trevor Cohn, and StevenBird.
2016.
Learning a translation model from wordlattices.
In 17th Annual Conference of the Interna-tional Speech Communication Association (INTER-SPEECH 2016), San Francisco, California, USA.Peter Austin and Julia Sallabank.
2011.
The Cam-bridge Handbook of Endangered Languages.
Cam-bridge Handbooks in Language and Linguistics.
Cam-bridge University Press.Laurent Besacier, Bowen Zhou, and Yuqing Gao.
2006.Towards speech translation of non written languages.In 2006 IEEE Spoken Language Technology Workshop(SLT 2006), pages 222?225, Palm Beach, Aruba.Steven Bird, Florian R Hanke, Oliver Adams, and Hae-joong Lee.
2014.
Aikuma: A mobile app for collabo-rative language documentation.
In Proceedings of the2014 Workshop on the Use of Computational Meth-ods in the Study of Endangered Languages, pages 1?5,Baltimore, Maryland, USA.David Blachon, Elodie Gauthier, Laurent Besacier, Guy-Noe?l Kouarata, Martine Adda-Decker, and Annie Ri-alland.
2016.
Parallel speech collection for under-resourced language studies using the lig-aikuma mo-bile device app.
Procedia Computer Science, 81:61?66.Peter F Brown, Stanley F Chen, Stephen A Della Pietra,Vincent J Della Pietra, Andrew S Kehler, and Robert LMercer.
1994.
Automatic speech recognition inmachine-aided translation.
Computer Speech & Lan-guage, 8(3):177?187.Francisco Casacuberta, Hermann Ney, Franz Josef Och,Enrique Vidal, Juan Miguel Vilar, Sergio Barrachina,Ismael Garc?
?a-Varea, David Llorens, Ce?sar Mart?
?nez,Sirko Molau, and Others.
2004.
Some approaches tostatistical and finite-state speech-to-speech translation.Computer Speech & Language, 18(1):25?47.Pi-Chuan Chang, Michel Galley, and Christopher D Man-ning.
2008.
Optimizing Chinese word segmentationfor machine translation performance.
In Proceedingsof the Third Workshop on Statistical Machine Trans-lation (WMT 2008), pages 224?232, Columbus, Ohio,USA.Wei Chen and Bo Xu.
2015.
Semi-supervised Chineseword segmentation based on bilingual information.In Proceedings of the 2015 Conference on EmpiricalMethods in Natural Language Processing (EMNLP2015), pages 1207?1216, Lisbon, Portugal.Nic J De Vries, Jaco Badenhorst, Marelie H Davel, Eti-enne Barnard, and Alta De Waal.
2011.
Woefzela- an open-source platform for ASR data collection inthe developing world.
In 12th Annual Conference ofthe International Speech Communication Association(INTERSPEECH 2011), pages 3177?3180, Florence,Italy.Nic J De Vries, Marelie H Davel, Jaco Badenhorst,Willem D Basson, Febe De Wet, Etienne Barnard, andAlta De Waal.
2014.
A smartphone-based ASR datacollection tool for under-resourced languages.
SpeechCommunication, 56:119?131.Thi-Ngoc-Diep Do, Alexis Michaud, and Eric Castelli.2014.
Towards the automatic processing of Yongn-ing Na (Sino-Tibetan): developing a ?light?
acousticmodel of the target language and testing ?heavyweight?models from five national languages.
In 4th Inter-national Workshop on Spoken Language Technologiesfor Under-resourced Languages (SLTU 2014), pages153?160, St Petersburg, Russia.Long Duong, Antonios Anastasopoulos, David Chiang,Steven Bird, and Trevor Cohn.
2016.
An atten-tional model for speech translation without transcrip-tion.
In Proceedings of the 2016 Conference of theNorth American Chapter of the Association for Com-putational Linguistics: Human Language Technolo-gies (NAACL HLT 2016), pages 949?959, San Diego,California, USA.Chris Dyer.
2009.
Using a maximum entropy model tobuild segmentation lattices for MT.
In Proceedingsof Human Language Technologies: The 2009 AnnualConference of the North American Chapter of the As-sociation for Computational Linguistics (NAACL HLT2009), pages 406?414, Boulder, Colorado, USA.M Heck, Q T Do, S Sakti, G Neubig, T Toda, and S Naka-mura.
2015.
The NAIST ASR system for IWSLT2015.
In Proceedings of the International Workshopon Spoken Language Translation (IWSLT 2015), DaNang, Vietnam.Mark Johnson.
2008.
Unsupervised word segmenta-tion for Sesotho using adaptor grammars.
In Pro-ceedings of the Tenth Meeting of ACL Special Inter-est Group on Computational Morphology and Phonol-ogy (SIGMORPHON 2008), pages 20?27, Columbus,Ohio, USA.2381Shahram Khadivi and Hermann Ney.
2008.
Inte-gration of speech recognition and machine transla-tion in computer-assisted translation.
Audio, Speech,and Language Processing, IEEE Transactions on,16(8):1551?1564.Evgeny Matusov, Stephan Kanthak, and Hermann Ney.2005.
On the integration of speech recognition andstatistical machine translation.
In 6th Interspeech2005 and 9th European Conference on Speech Com-munication and Technology (INTERSPEECH 2005),pages 3177?3180, Lisbon, Portugal.Coskun Mermer, Murat Sarac?lar, and Ruhi Sarikaya.2013.
Improving statistical machine translation usingBayesian word alignment and Gibbs sampling.
Audio,Speech, and Language Processing, IEEE Transactionson, 21(5):1090?1101.Graham Neubig, Yosuke Nakata, and Shinsuke Mori.2011.
Pointwise prediction for robust, adaptableJapanese morphological analysis.
In Proceedings ofthe 49th Annual Meeting of the Association for Com-putational Linguistics: Human Language Technolo-gies: short papers-Volume 2 (ACL HLT 2011), pages529?533, Portland, Oregon, USA.Graham Neubig, Masato Mimura, and Tatsuya Kawa-hara.
2012.
Bayesian learning of a language modelfrom continuous speech.
IEICE TRANSACTIONS onInformation and Systems, 95(2):614?625.ThuyLinh Nguyen, Stephan Vogel, and Noah A Smith.2010.
Nonparametric word segmentation for machinetranslation.
In Proceedings of the 23rd InternationalConference on Computational Linguistics (COLING2010), pages 815?823, Beijing, China.Matthias Paulik and Alex Waibel.
2013.
Training speechtranslation from audio recordings of interpreter-mediated communication.
Computer Speech & Lan-guage, 27(2):455?474.Joris Pelemans, Tom Vanallemeersch, Kris Demuynck,Patrick Wambacq, and Others.
2015.
Efficient lan-guage model adaptation for automatic speech recog-nition of spoken translations.
In 16th Annual Confer-ence of the International Speech Communication As-sociation (INTERSPEECH 2015), pages 2262?2266,Dresden, Germany.Aarthi Reddy and Richard C Rose.
2010.
Integrationof statistical models for dictation of document transla-tions in a machine-aided human translation task.
Au-dio, Speech, and Language Processing, IEEE Trans-actions on, 18(8):2015?2027.D Will Reiman.
2010.
Basic oral language documen-tation.
In Language Documentation & Conservation,pages 254?268.Steven L Scott.
2002.
Bayesian methods for hiddenMarkov models.
Journal of the American StatisticalAssociation, pages 337?351.Felix Stahlberg, Tim Schlippe, Sue Vogel, and TanjaSchultz.
2012.
Word segmentation through cross-lingual word-to-phoneme alignment.
In 2012 IEEEWorkshop on Spoken Language Technology (SLT2012), pages 85?90, Miami, Florida, USA.Felix Stahlberg, Tim Schlippe, Stephan Vogel, and TanjaSchultz.
2014.
Word segmentation and pronun-ciation extraction from phoneme sequences throughcross-lingual word-to-phoneme alignment.
ComputerSpeech & Language, pages 234?261.Sebastian Stu?ker, Laurent Besacier, and Alex Waibel.2009.
Human translations guided language discov-ery for ASR systems.
In 10th Annual Conference ofthe International Speech Communication Association(INTERSPEECH 2009), pages 3023?3026, Brighton,United Kingdom.Toshiyuki Takezawa, Eiichiro Sumita, Fumiaki Sugaya,Hirofumi Yamamoto, and Seiichi Yamamoto.
2002.Toward a broad-coverage bilingual corpus for speechtranslation of travel conversations in the real world.In Third International Conference on Language Re-sources and Evaluation (LREC 2002), pages 147?152,Las Palmas, Canary Islands.Enrique Vidal, Francisco Casacuberta, Luis Rodriguez,Jorge Civera, and Carlos D Mart?
?nez Hinarejos.
2006.Computer-assisted translation using speech recogni-tion.
Audio, Speech, and Language Processing, IEEETransactions on, 14(3):941?951.L I Zezhong, Hideto Ikeda, and Junichi Fukumoto.
2013.Bayesian word alignment and phrase table trainingfor statistical machine translation.
IEICE TRANSAC-TIONS on Information and Systems, 96(7):1536?1543.2382
