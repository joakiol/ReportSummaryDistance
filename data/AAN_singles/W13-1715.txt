Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 119?123,Atlanta, Georgia, June 13 2013. c?2013 Association for Computational LinguisticsRecognizing English Learners?
Native Language from Their WritingsBaoli LIDepartment of Computer ScienceHenan University of Technology1 Lotus Street, High & New Technology Industrial Development ZoneZhengzhou, China, 450001csblli@gmail.comAbstractNative Language Identification (NLI), whichtries to identify the native language (L1) of asecond language learner based on their writ-ings, is helpful for advancing second languagelearning and authorship profiling in forensiclinguistics.
With the availability of relevantdata resources, much work has been done toexplore the native language of a foreign lan-guage learner.
In this report, we present oursystem for the first shared task in Native Lan-guage Identification (NLI).
We use a linearSVM classifier and explore features of words,word and character n-grams, style, andmetadata.
Our official system achieves accu-racy of 0.773, which ranks it 18th among the29 teams in the closed track.1 IntroductionNative Language Identification (NLI) (Ahn, 2011;Kochmar, 2011), which tries to identify the nativelanguage (L1) of a second language learner basedon their writings, is expected to be helpful for ad-vancing second language learning and authorshipprofiling in forensic linguistics.
With the availabil-ity of relevant data resources, much work has beendone to explore the effective way to identify thenative language of a foreign language learner(Koppel et al 2005; Wong et al 2011; Brookeand Hirst, 2012a, 2012b; Bykh and Meurers, 2012;Crossley and McNamara, 2012; Jarvis et al 2012;Jarvis and Paquot, 2012; Tofighi et al 2012; Tor-ney et al2012).To evaluate different techniques and approachesto Native Language Identification with the samesetting, the first shared task in Native LanguageIdentification (NLI) was organized by researchersfrom Nuance Communications and EducationalTesting Service (Tetreault et al 2013).
A largerand more reliable data set, TOEFL11 (Blanchard etal., 2013), was used in this open evaluation.This paper reports our NLI2013 shared task sys-tem that we built at the Department of ComputerScience, Henan University of Technology, China.To be involved in this evaluation, we would like toobtain a more thorough knowledge of the researchon native language identification and its state-of-the-art, as we may focus on authorship attribution(Koppel et al 2008) problems in the near future.The NLI2013 shared task is framed as a super-vised text classification problem where the set ofnative languages (L1s), i.e.
categories, is known,which includes Arabic, Chinese, French, German,Hindi, Italian, Japanese, Korean, Spanish, Telugu,and Turkish.
A system is given a large part of theTOEFL11 dataset for training a detection model,and then makes predictions on the test writingsamples.Inspired by our experience of dealing with dif-ferent text classification problems, we decide toemploy a linear support vector machine (SVM) inour NLI2013 system.
We plan to take this systemas a starting point, and may explore other complexclassifiers in the future.
Although in-depth syntac-119tic features may be helpful for this kind of tasks(Bergsma et al 2012; Wong and Dras, 2011;Swanson and Charniak, 2012; Wong et al 2012),we decide to explore the effectiveness of the tradi-tional word and character features, as well as stylefeatures, in our system.
We would like to verify onthe first open available large dataset whether thesetraditional features work and how good they are.Figure 1.
System Architecture.We submitted four runs with different featuresets.
The run with all the features achieved the bestaccuracy of 0.773, which ranks our system 18thamong the 29 systems in the closed track.In the rest of this paper we describe the detail ofour system and analyze the results.
Section 2 givesthe overview of our system, while Section 3 dis-cusses the various features in-depth.
We presentour experiments and discussions in Section 4, andconclude in Section 5.2 System DescriptionFigure 1 gives the architecture of our NLI2013system, which takes machine learning framework.At the training stage, annotated data is first pro-cessed through preprocessing and feature extrac-tion, then fed to the classifier learning module, andwe can finally obtain a NLI model.
At the testingstage, each test sample goes through the same pre-processing and feature extraction modules, and isassigned a category with the learned NLI model.Data Preprocessing: this module aims at trans-forming the original data into a suitable format forthe system, e.g.
inserting the category informationinto the individual writing sample and attachingmetadata to essays.Feature Extraction: this module tries to obtainall the useful features from the original data.
Weconsidered features like: word, word n-gram, char-acter n-gram, style, and available metadata.Linear SVM training and testing: these twomodules are the key components.
The trainingmodule takes the transformed digitalized vectors asinput, and train an effective NLI model, where thetesting module just applies the learned model onthe testing data.
As linear support vector machines(SVM) achieves quite good performance on a lotof text classification problems, we use this generalmachine learning algorithm in our NLI2013 system.The excellent SVM implementation, Libsvm(Chang and Lin, 2011), was incorporated in oursystem and TFIDF is used to derive the featurevalues in vectors.
Then, we turn to focus on whatfeatures are effective for native language identifi-cation.
We explore words, word n-grams, charactern-grams, style, and metadata features in the system.3 FeaturesIn this section, we explain what kind of features weused in our NLI2013 system.3.1 Word and Word n-gramThe initial feature set is words or tokens in the da-taset.
As the dataset is tokenized and sen-tence/paragraph split, we simply use space todelimit the text and get individual tokens.
We re-move rare features that appear only once in thetraining dataset.
Words or tokens are transformedto lowercase.Word n-grams are combined by consecutivewords or tokens.
They are expecting to capturesome syntactic characteristics of writing samples.Two special tokens, ?BOS?
and ?EOS?, which in-dicate ?Beginning?
and ?Ending?, are attached atthe two ends of a sentence.
We considered word 2-grams and word 3-grams in our system.3.2 Character n-gram120We assume sub-word features like prefix andsuffix are useful for detecting the learners?
nativelanguages.
To simplify the process rather thanemploying a complex morphological analyzer, weconsider character n-grams as another importantfeature set.
The n-grams are extracted from eachsentence by regarding the whole sentence as alarge word / string and replacing the delimitedsymbol (i.e.
white space) with a special uppercasecharacter ?S?.
As what we did in getting word n-grams, we attached two special character ?B?
and?E?
at the two ends of a sentence.
Character 2-grams, 3-grams, 4-grams, and 5-grams are used inour system.3.3 StyleWe would like to explore whether the traditionalstyle features are helpful for this task as those fea-tures are widely used in authorship attribution.
Weinclude the following style features:?
__PARA__: a paragraph in an essay;?
__SENT__: a sentence in an essay;?
PARASENTLEN=NN: a paragraph of NNsentences long;?
SENTWDLEN=NN: a sentence of 4*NNwords long;?
WDCL=NN: a word of NN characters long;3.4 OtherAs the TOEFL11 dataset includes two metadata foreach essay, English language proficiency level(high, medium, or low) and Prompt ID, we includethem as additional features in our system.4 Experiments and Results4.1 DatasetThe dataset of the NLI2013 shared task contains12,100 English essays from the Test of English asa Foreign Language (TOEFL).
Educational TestingService (ETS) published the dataset through theLDC with the motivation to create a larger andmore reliable data set for researchers to conductNative Language Identification experiments on.This dataset, henceforth TOEFL11, comprises 11native languages (L1s) with 1,000 essays per lan-guage.
The 11 covered native languages are: Ara-bic, Chinese, French, German, Hindi, Italian,Japanese, Korean, Spanish, Telugu, and Turkish.In addition, each essay in the TOEFL11 is markedwith an English language proficiency level (high,medium, or low) based on the judgments of humanassessment specialists.
The essays are usually 300to 400 words long.
9,900 essays of this set are cho-sen as the training data, 1,100 are for developmentand the rest 1,100 as test data.Runs HAUTCS-1 HAUTCS-2 HAUTCS-3 HAUTCS-4Accuracy 0.773 0.758 0.76 0.756ARA 0.7311 0.703 0.703 0.71CHI 0.82 0.794 0.794 0.782FRE 0.806 0.788 0.786 0.783GER 0.897 0.899 0.899 0.867HIN 0.686 0.688 0.694 0.707ITA 0.83 0.84 0.844 0.844JPN 0.832 0.792 0.798 0.81KOR 0.763 0.764 0.768 0.727SPA 0.703 0.651 0.651 0.65TEL 0.702 0.702 0.702 0.751TUR 0.736 0.715 0.716 0.698Table 1.
Official results of our system.Figure 2.
Performance of our official runs.4.2 Official ResultsAccuracy, which measures the percentage of howmany essays are correctly detected, is used as themain evaluation metric in the NLI2013 shared task.Table 1 gives the official results of our systemon the evaluation data.
We submitted four runswith different feature sets:HAUTCS-1: all the features, which includewords, word 2-grams, word 3-grams, character 2-grams, character 3-grams, character 4-grams,1This number, as well as others in the cells from this row tothe bottom, is value of F-1 measure for each language.121character 5-grams, style, and other metadata fea-tures;HAUTCS-2:  uses words, word 2-grams, word3-grams, style, and other metadata features;HAUTCS-3: uses words, word 2-grams, word3-grams, and other metadata features;HAUTCS-4: uses words or tokens and othermetadata features.For the runs HAUTCS-2, HAUTCS-3, andHAUTCS-4, we combined the development andtraining data for learning the identification model,where for the HAUTCS-1, it?s a pity that we forgotto include the development data for training themodel.Our best run (HAUTCS-1) achieved the overallaccuracy (0.773).
The system performs best on theGerman category, but poorest on the Hindi catego-ry, as can be easily seen on figure 2.Analyzing the four runs?
performance showingon figure 2, we observe: word features are quiteeffective for Telugu and Hindi categories, but notpowerful enough for others; word n-grams arehelpful for languages Chinese, French, German,Korean, and Turkish, but useless for others; Stylefeatures only boost a little for French; Character n-grams work for Arabic, Chinese, French, Japanese,Spanish, and Turkish; Spanish category preferscharacter n-grams, where Telugu category likesword features.
As different features have differenteffects on different languages, a better NLI systemis expected to use different features for differentlanguages.After the evaluation, we experimented with thesame setting as the HAUTCS-1 run, but includedboth training and development data for learning theNLI model.
We got accuracy 0.781 on the newreleased test data, which has the same format withparagraph split as the training and developmentdata.As we include style features like how many par-agraphs in an essay, the old test data, which re-moved the paragraph delimiters (i.e.
single blanklines), may be not good for our trained model.Therefore, we did experiments with the new testdata.
Unfortunately, the accuracy 0.772 is a littlepoorer than that we obtained with the old test data.It seems that the simple style features are not effec-tive in this task.
As shown in table 1, HAUTCS-2performs poorer than HAUTCS-3, which helps usderive the same conclusion.4.3 Additional ExperimentsWe did 10-fold cross validation on the training anddevelopment data with the same setting as theHAUTCS-1 run.
The data splitting is given by theorganizers.
Accuracies of the 10 runs are show intable 2.
The overall accuracy 0.799 is better thanthat on the test data.Fold 1 2 3 4 5Accuracy 0.802 0.795 0.81 0.791 0.79Fold 6 7 8 9 10Accuracy 0.805 0.789 0.803 0.798 0.805Table 2.
Results of 10-fold cross validation on the train-ing and development data.To check how metadata features work, we didanother run HAUTCS-5, which uses only words asfeatures.
This run got the same overall accuracy0.756 on the old test data as HAUTCS-4 did,which demonstrates that those metadata featuresmay not provide much useful information for na-tive language identification.5 Conclusion and Future WorkIn this paper, we report our system for theNLI2013 shared task, which automatically detect-ing the native language of a foreign English learnerfrom her/his writing sample.
The system was builton a machine learning framework with traditionalfeatures including words, word n-grams, charactern-grams, and writing styles.
Character n-grams aresimple but quite effective.We plan to explore syntactic features in the fu-ture, and other machine learning algorithms, e.g.ECOC (Li and Vogel, 2010), also deserve furtherexperiments.
As we discussed in section 4, we arealso interested in designing a framework to usedifferent features for different categories.AcknowledgmentsThis work was supported by the Henan ProvincialResearch Program on Fundamental and Cutting-Edge Technologies (No.
112300410007), and theHigh-level Talent Foundation of Henan Universityof Technology (No.
2012BS027).
Experimentswere performed on the Amazon Elastic ComputeCloud.122ReferencesAhn, C. S. 2011.
Automatically Detecting Authors' Na-tive Language.
Master's thesis, Naval PostgraduateSchool, Monterey, CA.Bergsma, S., Post, M., and Yarowsky, D. 2012.
Stylo-metric analysis of scientific articles.
In Proceedingsof the 2012 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies, pages 327?337,Montr?al, Canada.
Association for ComputationalLinguistics.Blanchard, D., Tetreault, J., Higgins, D., Cahill, A., andChodorow, M. 2013.
TOEFL11: A Corpus of Non-Native English.
Technical report, Educational Test-ing Service.Brooke, J. and Hirst, G. 2012a.
Measuring interlanguage:Native language identification with l1-influence met-rics.
In Calzolari, N., Choukri, K., Declerck, T.,Dogan, M. U., Maegaard, B., Mariani, J., Odijk, J.,and Piperidis, S., editors, Proceedings of the EighthInternational Conference on Language Resources andEvaluation (LREC-2012), pages 779?784, Istanbul,TurkeyBrooke, J. and Hirst, G. 2012b.
Robust, LexicalizedNative Language Identification.
In Proceedings ofCOLING 2012, pages 391-408, Mumbai, India.Bykh, S. and Meurers, D. 2012.
Native Language Iden-tification using Recurring n-grams - InvestigatingAbstraction and Domain Dependence.
In Proceedingsof COLING 2012, pages 425-440, Mumbai, India.Chang, C.-C. and Lin C.-J.
2011.
LIBSVM : a libraryfor support vector machines.
ACM Transactions onIntelligent Systems and Technology, 2:3:27:1-27.Crossley, S. A. and McNamara, D. 2012.
Detecting theFirst Language of Second Language Writers UsingAutomated Indices of Cohesion, Lexical Sophistica-tion, Syntactic Complexity and ConceptualKnowledge.
In Jarvis, S. and Crossley, S. A., editors,Approaching Language Transfer through Text Classi-fication, pages 106-126.
Multilingual Matters.Jarvis, S., Casta?eda-Jim?nez, G., and Nielsen, R. 2012.Detecting L2 Writers' L1s on the Basis of Their Lex-ical Styles.
In Jarvis, S. and Crossley, S. A., editors,Approaching Language Transfer through Text Classi-fication, pages 34-70.
Multilingual Matters.Jarvis, S. and Paquot, M. 2012.
Exploring the Role of n-Grams in L1 Identification.
In Jarvis, S. and Crossley,S.
A., editors, Approaching Language Transferthrough Text Classification, pages 71-105.
Multilin-gual Matters.Kochmar, E. 2011.
Identification of a writer?s nativelanguage by error analysis.
Master?s thesis, Universi-ty of Cambridge.Koppel, M., Schler, J., and Zigdon, K. 2005.
Determin-ing an author?s native language by mining a text forerrors.
In Proceedings of the eleventh ACMSIGKDD international conference on Knowledgediscovery in data mining, pages 624?628, Chicago,IL.
ACM.Koppel, M., Schler, J., and Argamon, S. 2008.
Compu-tational methods in authorship attribution.
Journal ofthe American Society for information Science andTechnology, 60(1):9?26.Li, B., and Vogel, C. 2010.
Improving Multiclass TextClassification with Error-Correcting Output Codingand Sub-class Partitions.
In Proceedings of the 23rdCanadian Conference on Artificial Intelligence, pag-es 4-15, Ottawa, Canada.Swanson, B. and Charniak, E. 2012.
Native languagedetection with tree substitution grammars.
In Pro-ceedings of the 50th Annual Meeting of the Associa-tion for Computational Linguistics (Volume 2: ShortPapers), pages 193?197, Jeju Island, Korea.Tetreault, J., Blanchard, D., and Cahill, A.
2013.
Areport on the first native language identificationshared task.
In Proceedings of the Eighth Workshopon Innovative Use of NLP for Building EducationalApplications.
Atlanta, GA, USA.Tofighi, P.; K?se, C.; and Rouka, L.  2012.
Author?snative language identification from web-basedtexts.
International Journal of Computer and Com-munication Engineering.
1(1):47-50Torney, R.; Vamplew, P.; and Yearwood,J.
2012.
Using psycholinguistic features for profil-ing first language of authors.
Journal of the Ameri-can Society for Information Science andTechnology.
63(6):1256-1269.Wong, S.-M. J. and Dras, M. 2011.
Exploiting ParseStructures for Native Language Identification.
InProceedings of the 2011 Conference on EmpiricalMethods in Natural Language Processing, pages1600?1610, Edinburgh, Scotland, UK.Wong, S.-M. J., Dras, M., and Johnson, M. 2012.
Ex-ploring Adaptor Grammars for Native LanguageIdentification.
In Proceedings of the 2012 Joint Con-ference on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning, pages 699?709, Jeju Island, Korea.123
