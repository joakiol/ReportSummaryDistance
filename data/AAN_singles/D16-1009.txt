Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 85?95,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsModeling Human Reading with Neural AttentionMichael Hahn Frank KellerInstitute for Language, Cognition and ComputationSchool of Informatics, University of Edinburgh10 Crichton Street, Edinburgh EH8 9AB, UKs1582047@inf.ed.ac.uk keller@inf.ed.ac.ukAbstractWhen humans read text, they fixate somewords and skip others.
However, there havebeen few attempts to explain skipping behav-ior with computational models, as most ex-isting work has focused on predicting read-ing times (e.g., using surprisal).
In this pa-per, we propose a novel approach that modelsboth skipping and reading, using an unsuper-vised architecture that combines a neural at-tention with autoencoding, trained on raw textusing reinforcement learning.
Our model ex-plains human reading behavior as a tradeoffbetween precision of language understanding(encoding the input accurately) and economyof attention (fixating as few words as possi-ble).
We evaluate the model on the Dundeeeye-tracking corpus, showing that it accuratelypredicts skipping behavior and reading times,is competitive with surprisal, and capturesknown qualitative features of human reading.1 IntroductionHumans read text by making a sequence of fixationsand saccades.
During a fixation, the eyes land on aword and remain fairly static for 200?250 ms. Sac-cades are the rapid jumps that occur between fixa-tions, typically lasting 20?40 ms and spanning 7?9 characters (Rayner, 1998).
Readers, however, donot simply fixate one word after another; some sac-cades go in reverse direction, and some words arefixated more than once or skipped altogether.A range of computational models have been de-veloped to account for human eye-movements inreading (Rayner and Reichle, 2010), including mod-els of saccade generation in cognitive psychology,such as EZ-Reader (Reichle et al, 1998, 2003,2009), SWIFT (Engbert et al, 2002, 2005), orthe Bayesian Model of Bicknell and Levy (2010).More recent approaches use machine learning mod-els trained on eye-tracking data to predict humanreading patterns (Nilsson and Nivre, 2009, 2010;Hara et al, 2012; Matthies and S?gaard, 2013).Both types of models involve theoretical assump-tions about human eye-movements, or at least re-quire the selection of relevant eye-movement fea-tures.
Model parameters have to be estimated in asupervised way from eye-tracking corpora.Unsupervised approaches, that do not involvetraining the model on eye-tracking data, have alsobeen proposed.
A key example is surprisal, whichmeasures the predictability of a word in context, de-fined as the negative logarithm of the conditionalprobability of the current word given the preced-ing words (Hale, 2001; Levy, 2008).
Surprisal iscomputed by a language model, which can take theform of a probabilistic grammar, an n-gram model,or a recurrent neural network.
While surprisal hasbeen shown to correlate with word-by-word readingtimes (McDonald and Shillcock, 2003a,b; Dembergand Keller, 2008; Frank and Bod, 2011; Smith andLevy, 2013), it cannot explain other aspects of hu-man reading, such as reverse saccades, re-fixations,or skipping.
Skipping is a particularly intriguingphenomenon: about 40% of all words are skipped(in the Dundee corpus, see below), without apparentdetriment to text understanding.In this paper, we propose a novel model architec-ture that is able to explain which words are skippedand which ones are fixated, while also predicting85reading times for fixated words.
Our approach iscompletely unsupervised and requires only unla-beled text for training.Compared to language as a whole, reading is arecent innovation in evolutionary terms, and peo-ple learning to read do not have access to compe-tent readers?
eye-movement patterns as training data.This suggests that human eye-movement patternsemerge from general principles of language pro-cessing that are independent of reading.
Our start-ing point is the Tradeoff Hypothesis: Human read-ing optimizes a tradeoff between precision of lan-guage understanding (encoding the input accurately)and economy of attention (fixating as few words aspossible).
Based on the Tradeoff Hypothesis, we ex-pect that humans only fixate words to the extent nec-essary for language understanding, while skippingwords whose contribution to the overall meaning canbe inferred from context.In order to test these assumptions, this paper in-vestigates the following questions:1.
Can the Tradeoff Hypothesis be implementedin an unsupervised model that predicts skippingand reading times in quantitative terms?
In par-ticular, can we compute surprisal based only onthe words that are actually fixated?2.
Can the Tradeoff Hypothesis explain knownqualitative features of human fixation patterns?These include dependence on word frequency,word length, predictability in context, a con-trast between content and function words, andthe statistical dependence of the current fixa-tion on previous fixations.To investigate these questions, we develop a genericarchitecture that combines neural language model-ing with recent ideas on integrating recurrent neuralnetworks with mechanisms of attention, which haveshown promise both in NLP and in computer vision.We train our model end-to-end on a large text cor-pus to optimize a tradeoff between minimizing inputreconstruction error and minimizing the number ofwords fixated.
We evaluate the model?s reading be-havior against a corpus of human eye-tracking data.Apart from the unlabeled training corpus and thegeneric architecture, no further assumptions aboutlanguage structure are made ?
in particular, no lex-icon or grammar or otherwise labeled data is re-quired.Our unsupervised model is able to predict humanskips and fixations with an accuracy of 63.7%.
Thiscompares to a baseline of 52.6% and a supervisedaccuracy of 69.9%.
For fixated words, the modelsignificantly predicts human reading times in a lin-ear mixed effects analysis.
The performance of ourmodel is comparable to surprisal, even though it onlyfixates 60.4% of all input words.
Furthermore, weshow that known qualitative features of human fix-ation sequences emerge in our model without addi-tional assumptions.2 Related WorkA range of attention-based neural network architec-tures have recently been proposed in the literature,showing promise in both NLP and computer vision(e.g., Mnih et al, 2014; Bahdanau et al, 2015).
Sucharchitectures incorporate a mechanism that allowsthe network to dynamically focus on a restricted partof the input.
Attention is also a central concept incognitive science, where it denotes the focus of cog-nitive processing.
In both language processing andvisual processing, attention is known to be limited toa restricted area of the visual field, and shifts rapidlythrough eye-movements (Henderson, 2003).Attention-based neural architectures either em-ploy soft attention or hard attention.
Soft attentiondistributes real-valued attention values over the in-put, making end-to-end training with gradient de-scent possible.
Hard attention mechanisms makediscrete choices about which parts of the input tofocus on, and can be trained with reinforcementlearning (Mnih et al, 2014).
In NLP, soft atten-tion can mitigate the difficulty of compressing longsequences into fixed-dimensional vectors, with ap-plications in machine translation (Bahdanau et al,2015) and question answering (Hermann et al,2015).
In computer vision, both types of attentioncan be used for selecting regions in an image (Baet al, 2015; Xu et al, 2015).3 The NEAT Reading ModelThe point of departure for our model is the Trade-off Hypothesis (see Section 1): Reading optimizes atradeoff between precision of language understand-86ing and economy of attention.
We make this idea ex-plicit by proposing NEAT (NEural Attention Trade-off), a model that reads text and attempts to re-construct it afterwards.
While reading, the networkchooses which words to process and which wordsto skip.
The Tradeoff Hypothesis is formalized us-ing a training objective that combines accuracy ofreconstruction with economy of attention, encourag-ing the network to only look at words to the extentthat is necessary for reconstructing the sentence.3.1 ArchitectureWe use a neural sequence-to-sequence architecture(Sutskever et al, 2014) with a hard attention mech-anism.
We illustrate the model in Figure 1, oper-ating on a three-word sequence w. The most basiccomponents are the reader, labeled R, and the de-coder.
Both of them are recurrent neural networkswith Long Short-Term Memory (LSTM, Hochreiterand Schmidhuber, 1997) units.
The recurrent readernetwork is expanded into time steps R0, .
.
.
,R3 inthe figure.
It goes over the input sequence, readingone word wi at a time, and converts the word se-quence into a sequence of vectors h0, .
.
.
,h3.
Eachvector hi acts as a fixed-dimensionality encodingof the word sequence w1, .
.
.
,wi that has been readso far.
The last vector h3 (more generally hN forsequence length N), which encodes the entire in-put sequence, is then fed into the input layer ofthe decoder network, which attempts to reconstructthe input sequence w. It is also realized as a recur-rent neural network, collapsed into a single box inthe figure.
It models a probability distribution overword sequences, outputting a probability distribu-tion PDecoder(wi|w1,...,i?1,hN) over the vocabulary inthe i-th step, as is common in neural language mod-eling (Mikolov et al, 2010).
As the decoder hasaccess to the vector representation created by thereader network, it ideally is able to assign the high-est probability to the word sequence w that was ac-tually read.
Up to this point, the model is a stan-dard sequence-to-sequence architecture reconstruct-ing the input sequence, that is, performing autoen-coding.As a basic model of human processing, NEATcontains two further components.
First, experimen-tal evidence shows that during reading, humans con-stantly make predictions about the upcoming input(e.g., Van Gompel and Pickering, 2007).
As a modelof this behavior, the reader network at each time stepoutputs a probability distribution PR over the lex-icon.
This distribution describes which words arelikely to come next (i.e., the reader network per-forms language modeling).
Unlike the modeling per-formed by the decoder, PR, via its recurrent connec-tions, has access to the previous context only.Second, we model skipping by stipulating thatonly some of the input words wi are fed into thereader network R, while R receives a special vec-tor representation, containing no information aboutthe input word, in other cases.
These are the wordsthat are skipped.
In NEAT, at each time step dur-ing reading, the attention module A decides whetherthe next word is shown to the reader network or not.When humans skip a word, they are able to identifyit using parafoveal preview (Rayner, 2009).
Thus,we can assume that the choice of which words toskip takes into account not only the prior contextbut also a preview of the word itself.
We thereforeallow the attention module to take the input wordinto account when making its decision.
In addition,the attention module has access to the previous statehi?1 of the reader network, which summarizes whathas been read so far.
To allow for interaction be-tween skipping and prediction, we also give the at-tention module access to the probability of the in-put word according to the prediction PR made at thelast time step.
If we write the decision made by Aas ?i ?
{0,1}, where ?i = 1 means that word wi isshown to the reader and 0 means that it is not, wecan write the probability of showing word wi as:P(?i = 1|?1...i?1,w)= PA(wi,hi?1,PR(wi|w1...i?1,?1...i?1)) (1)We implement A as a feed-forward network, fol-lowed by taking a binary sample ?i.We obtain the surprisal of an input word by takingthe negative logarithm of the conditional probabilityof this word given the context words that precede it:Surp(wi|w1...i?1) =?
logPR(wi|w1...i?1,?1...i?1)(2)As a consequence of skipping, not all input wordsare accessible to the reader network.
Therefore, the87probability and surprisal estimates it computes cru-cially only take into account the words that have ac-tually been fixated.
We will refer to this quantity asthe restricted surprisal, as opposed to full surprisal,which is computed based on all prior context words.The key quantities for predicting human readingare the fixation probabilities in equation (1), whichmodel fixations and skips, and restricted surprisal inequation (2), which models the reading times of thewords that are fixated.3.2 Model ObjectiveGiven network parameters ?
and a sequence wof words, the network stochastically chooses asequence ?
according to (1) and incurs a lossL(?|w,?)
for language modeling and reconstruc-tion:L(?|w,?)
=??ilogPR(wi|w1,...,i?1,?1,...,i?1;?)?
?ilogPDecoder(wi|w1,...,i?1;hN ;?
)(3)where PR(wi, .
.
.)
denotes the output of the reader af-ter reading wi?1, and PDecoder(wi| .
.
.
;hN) is the out-put of the decoder at time i?
1, with hN being thevector representation created by the reader networkfor the entire input sequence.To implement the Tradeoff Hypothesis, we trainNEAT to solve language modeling and reconstruc-tion with minimal attention, i.e., the network mini-mizes the expected loss:Q(?)
:= Ew,?
[L(?|w,?)+?
?
??
?`1 ] (4)where word sequences w are drawn from a corpus,and ?
is distributed according to P(?|w,?)
as de-fined in (1).
In (4), ??
?`1 is the number of wordsshown to the reader, and ?> 0 is a hyperparameter.The term ?
?
??
?`1 encourages NEAT to attend to asfew words as possible.Note that we make no assumption about linguis-tic structure ?
the only ingredients of NEAT are theneural architecture, the objective (4), and the corpusfrom which the sequences w are drawn.3.3 TrainingWe follow previous approaches to hard attention inusing a combination of gradient descent and rein-forcement learning, and separate the training of therecurrent networks from the training of A.
To trainthe reader R and the decoder, we temporarily re-move the attention network A, set ?
?
Binom(n, p)(n sequence length, p a hyperparameter), and mini-mize E[L(w|?,?)]
using stochastic gradient descent,sampling a sequence ?
for each input sequence.
Ineffect, NEAT is trained to perform reconstructionand language modeling when there is noise in theinput.
After R and the decoder have been trained,we fix their parameters and train A using the RE-INFORCE rule (Williams, 1992), which performsstochastic gradient descent using the estimate1|B| ?w?B;?
(L(?|w,?)+?
?
???`1)?
?A (logP(?|w,?
))(5)for the gradient ??AQ.
Here, B is a minibatch, ?
issampled from P(?|w,?
), and ?A ?
?
is the set ofparameters of A.
For reducing the variance of thisestimator, we subtract in the i-th step an estimate ofthe expected loss:U(w,?1...i?1) := E?i...N [L(?1...i?1?i...N |w,?
)+ ?
?
??
?`1 ](6)We compute the expected loss using an LSTMthat we train simultaneously with A to predict L+???
?`1 based on w and ?1...i?1.
To make learningmore stable, we add an entropy term encouraging thedistribution to be smooth, following Xu et al (2015).The parameter updates to A are thus:?w,??i(L(?|w,?)+????`1?U(w,?1...i?1))?
?
?A (log P(?i|?1...i?1,w,?))??
??A(?w,??iH[P(?i|?1,...,i?1,w,?)])
(7)where ?
is a hyperparameter, and H the entropy.4 MethodsOur aim is to evaluate how well NEAT predicts hu-man fixation behavior and reading times.
Further-more, we want show that known qualitative prop-erties emerge from the Tradeoff Hypothesis, eventhough no prior knowledge about useful features ishard-wired in NEAT.88w1 w2 w3A A AR0 R1 R2 R3 Decoderh0 h1 h2PR1 PR2 PR3Figure 1: The architecture of the proposed model, reading a three-word input sequence w1,w2,w3.
R is the reader network and PRthe probability distribution it computes in each time step.
A is the attention network.
At each time step, the input, its probabilityaccording to PR, and the previous state hi?1 of R are fed into A, which then decides whether the word is read or skipped.4.1 Training SetupFor both the reader and the decoder networks, wechoose a one-layer LSTM network with 1,000 mem-ory cells.
The attention network is a one-layer feed-forward network.
For the loss estimator U , we usea bidirectional LSTM with 20 memory cells.
Inputdata is split into sequences of 50 tokens, which areused as the input sequences for NEAT, disregardingsentence boundaries.
Word embeddings have 100 di-mensions, are shared between the reader and theattention network, and are only trained during thetraining of the reader.
The vocabulary consists ofthe 10,000 most frequent words from the trainingcorpus.
We trained NEAT on the training set of theDaily Mail section of the corpus described by Her-mann et al (2015), which consists of 195,462 arti-cles from the Daily Mail newspaper, containing ap-proximately 200 million tokens.
The recurrent net-works and the attention network were each trainedfor one epoch.
For initialization, weights are drawnfrom the uniform distribution.
We set ?
= 5.0, ?
=5.0, and used a constant learning rate of 0.01 for A.4.2 CorpusFor evaluation, we use the English section of theDundee corpus (Kennedy and Pynte, 2005), whichconsists of 20 texts from The Independent, anno-tated with eye-movement data from ten English na-tive speakers.
Each native speakers read all 20 textsand answered a comprehension question after eachtext.
We split the Dundee corpus into a developmentand a test set, with texts 1?3 constituting the devel-opment set.
The development set consists of 78,300tokens, and the test set of 281,911 tokens.
For evalu-ation, we removed the datapoints removed by Dem-berg and Keller (2008), mainly consisting of wordsat the beginning or end of lines, outliers, and casesof track loss.
Furthermore, we removed datapointswhere the word was outside of the vocabulary of themodel, and those datapoints mapped to positions 1?3 or 48?50 of a sequence when splitting the data.After preprocessing, 62.9% of the development to-kens and 64.7% of the test tokens remained.
To ob-tain the number of fixations on a token and readingtimes, we used the eye-tracking measures computedby Demberg and Keller (2008).
The overall fixationrate was 62.1% on the development set, and 61.3%on the test set.The development set was used to run preliminaryversions of the human evaluation studies, and to de-termine the human skipping rate (see Section 5).
Allthe results reported in this paper were computed onthe test set, which remained unseen until the modelwas final.5 Results and DiscussionThroughout this section, we consider the followingbaselines for the attention network: random atten-tion is defined by ?
?
Binom(n, p), with p = 0.62,the human fixation rate in the development set.
Forfull attention, we take ?
= 1, i.e., all words arefixated.
We also derive fixation predictions fromfull surprisal, word frequency, and word length bychoosing a threshold such that the resulting fixationrate matches the human fixation rate on the develop-ment set.5.1 Quantitative PropertiesBy averaging over all possible fixation sequences,NEAT defines for each word in a sequence a prob-ability that it will be fixated.
This probability isnot efficiently computable, so we approximate it bysampling a sequence ?
and taking the probabilitiesP(?i = 1|?1...i?1,w) for i = 1, .
.
.
,50.
These sim-89ulated fixation probabilities can be interpreted asdefining a distribution of attention over the inputsequence.
Figure 2 shows heatmaps of the simu-lated and human fixation probabilities, respectively,for the beginning of a text from the Dundee cor-pus.
While some differences between simulated andhuman fixation probabilities can be noticed, thereare similarities in the general qualitative features ofthe two heatmaps.
In particular, function words andshort words are less likely to be fixated than contentwords and longer words in both the simulated andthe human data.Reconstruction and Language Modeling Wefirst evaluate NEAT intrinsically by measuring howsuccessful the network is at predicting the nextword and reconstructing the input while minimiz-ing the number of fixations.
We compare perplex-ity on reconstruction and language modeling for?
?
P(?|w,?).
In addition to the baselines, we runNEAT on the fixations generated by the human read-ers of the Dundee corpus, i.e., we use the human fix-ation sequence as ?
instead of the fixation sequencegenerated by A to compute perplexity.
This will tellus to what extent the human behavior minimizes theNEAT objective (4).The results are given in Table 1.
In all settings, thefixation rates are similar (60.4% to 62.1%) whichmakes the perplexity figures directly comparable.While NEAT has a higher perplexity on both taskscompared to full attention, it considerably outper-forms random attention.
It also outperforms theword length, word frequency, and full surprisal base-lines.
The perplexity on human fixation sequences issimilar to that achieved using word frequency.
Basedon these results, we conclude that REINFORCE suc-cessfully optimizes the objective (4).Likelihood of Fixation Data Human reading be-havior is stochastic in the sense that different runs ofeye-tracking experiments such as the ones recordedin the Dundee corpus yield different eye-movementsequences.
NEAT is also stochastic, in the sense that,given a word sequence w, it defines a probability dis-tribution over fixation sequences ?.
Ideally, this dis-tribution should be close to the actual distribution offixation sequences produced by humans reading thesequence, as measured by perplexity.We find that the perplexity of the fixation se-Acc F1fix F1skipNEAT 63.7 70.4 53.0Supervised ModelsNilsson and Nivre (2009) 69.5 75.2 62.6Matthies and S?gaard (2013) 69.9 72.3 66.1Human Performance and BaselinesRandom Baseline 52.6 62.1 37.9Full Surprisal 64.1 70.7 53.6Word Frequency 67.9 74.0 58.3Word Length 68.4 77.1 49.0Human 69.5 76.6 53.6Table 2: Evaluation of fixation sequence predictions against hu-man data.
For the human baseline, we predicted the n-th reader?sfixations by taking the fixations of the n+ 1-th reader (withmissing values replaced by reader average), averaging the re-sulting scores over the ten readers.quences produced by the ten readers in the Dundeecorpus under NEAT is 1.84.
A perplexity of 2.0corresponds to the random baseline Binom(n,0.5),and a perplexity of 1.96 to random attentionBinom(n,0.62).
As a lower bound on what canachieved with models disregarding the context, us-ing the human fixation rates for each word as proba-bilities, we obtain a perplexity of 1.68.Accuracy of Fixation Sequences Previous workon supervised models for modeling fixations (Nils-son and Nivre, 2009; Matthies and S?gaard, 2013)has been evaluated by measuring the overlap of thefixation sequences produced by the models withthose in the Dundee corpus.
For NEAT, this methodof evaluation is problematic as differences betweenmodel predictions and human data may be due todifferences in the rate of skipping, and due to the in-herently stochastic nature of fixations.
We thereforederive model predictions by rescaling the simulatedfixation probabilities so that their average equals thefixation rate in the development set, and then greed-ily take the maximum-likelihood sequence.
That is,we predict a fixation if the rescaled probability isgreater than 0.5, and a skip otherwise.
As in previ-ous work, we report the accuracy of fixations andskips, and also separate F1 scores for fixations andskips.
As lower and upper bounds, we use the ran-dom baseline ?
?
Binom(n,0.62) and the agree-ment of the ten human readers, respectively.
The re-90The decision of the Human Fertility and Embryology Authority (HFEA) to allow a couple to select genetically their next baby was boundto raise concerns that advances in biotechnology are racing ahead of our ability to control the consequences.
The couple at the centre ofthis case have a son who suffers from a potentially fatal disorder and whose best hope is a marrow transplant from a sibling, so thestakes of this decision are particularly high.
The HFEA?s critics believe that it sanctions ?designer babies?
and does not show respect for thesanctity of individual life.
Certainly, the authority?s backing for Shahana and Raj Hashmi?s plea for genetic screening raises fundamental questionsThe decision of the Human Fertility and Embryology Authority (HFEA) to allow a couple to select genetically their next baby was boundto raise concerns that advances in biotechnology are racing ahead of our ability to control the consequences.
The couple at the centre ofthis case have a son who suffers from a potentially fatal disorder and whose best hope is a marrow transplant from a sibling, so thestakes of this decision are particularly high.
The HFEA?s critics believe that it sanctions ?designer babies?
and does not show respect for thesanctity of individual life.
Certainly, the authority?s backing for Shahana and Raj Hashmi?s plea for genetic screening raises fundamental questionsFigure 2: Top: Heatmap showing human fixation probabilities, as estimated from the ten readers in the Dundee corpus.
In cases oftrack loss, we replaced the missing value with the corresponding reader?s overall fixation rate.
Bottom: Heatmap showing fixationprobabilities simulated by NEAT.
Color gradient ranges from blue (low probability) to red (high probability); words without colorare at the beginning or end of a sequence, or out of vocabulary.NEAT Rand.
Att.
Word Len.
Word Freq.
Full Surp.
Human Full Att.Language Modeling 180 333 230 219 211 218/170 107Reconstruction 4.5 56 40 39 34 39/31 1.6Fixation Rate 60.4% 62.1% 62.1% 62.1% 62.1% 61.3%/72.0% 100%Table 1: Performance on language modeling and reconstruction as measured by perplexity.
Random attention is an upper boundon perplexity, while full attention is a lower bound.
For the human baseline, we give two figures, which differ in the treatment ofmissing data.
The first figure is obtained when replacing missing values with a random variable ?
?
Binom(n,0.61); the secondresults from replacing missing values with 1.sults are shown in Table 2.
NEAT clearly outper-forms the random baseline and shows results closeto full surprisal (where we apply the same rescal-ing and thresholding as for NEAT).
This is remark-able given that NEAT has access to only 60.4% ofthe words in the corpus in order to predict skipping,while full surprisal has access to all the words.Word frequency and word length perform well, al-most reaching the performance of supervised mod-els.
This shows that the bulk of skipping behavioris already explained by word frequency and wordlength effects.
Note, however, that NEAT is com-pletely unsupervised, and does not know that it hasto pay attention to word frequency; this is somethingthe model is able to infer.Restricted Surprisal and Reading Times Toevaluate the predictions NEAT makes for readingtimes, we use linear mixed-effects models contain-ing restricted surprisal derived from NEAT for theDundee test set.
The mixed models also include aset of standard baseline predictors, viz., word length,log word frequency, log frequency of the previousword, launch distance, landing position, and the po-sition of the word in the sentence.
We treat partici-pants and items as random factors.
As the dependentvariable, we take first pass duration, which is thesum of the durations of all fixations from first enter-ing the word to first leaving it.
We compare againstfull surprisal as an upper bound and against ran-dom surprisal as a lower bound.
Random surprisalis surprisal computed by a model with random at-tention; this allows us to assess how much surprisaldegrades when only 60.4% of all words are fixated,but no information is available as to which wordsshould be fixated.
The results in Table 3 show thatrestricted surprisal as computed by NEAT, full sur-prisal, and random surprisal are all significant pre-dictors of reading time.In order to compare the three surprisal estimates,we therefore need a measure of effect size.
For this,we compare the model fit of the three mixed ef-fects models using deviance, which is defined asthe difference between the log likelihood of themodel under consideration minus the log likelihoodof the baseline model, multiplied by ?2.
Higher de-91?
SE t(Intercept) 247.43 7.14 34.68*Word Length 12.92 0.21 60.62*Previous Word Freq.
?5.28 0.28 ?18.34*Prev.
Word Fixated ?24.67 0.81 ?30.55*Launch Distance -0.01 0.01 ?0.37Obj.
Landing Pos.
?8.07 0.20 ?41.25*Word Pos.
in Sent.
?0.10 0.03 ?2.98*Log Word Freq.
?1.59 0.21 ?7.73*Resid.
Random Surprisal 2.69 0.10 29.27*Resid.
Restr.
Surprisal 2.75 0.12 23.66*Resid.
Full Surprisal 2.99 0.12 25.23*Table 3: Linear mixed effects models for first pass duration.The first part of the table shows the coefficients, standard er-rors, and t values for the predictors in the baseline model.
Thesecond part of the table gives the corresponding values for ran-dom surprisal, restricted surprisal computed by NEAT, and fullsurprisal, residualized against the baseline predictors, in threemodels obtained by adding these predictors.viance indicates greater improvement in model fitover the baseline model.
We find that the mixedmodel that includes restricted surprisal achieves adeviance of 867, compared to the model contain-ing only the baseline features.
With full surprisal,we obtain a deviance of 980.
On the other hand, themodel including random surprisal achieves a lowerdeviance of 832.This shows that restricted surprisal as computedby NEAT not only significantly predicts readingtimes, it also provides an improvement in model fitcompared to the baseline predictors.
Such an im-provement is also observed with random surprisal,but restricted surprisal achieves a greater improve-ment in model fit.
Full surprisal achieves an evengreater improvement, but this is not unexpected, asfull surprisal has access to all words, unlike NEAT orrandom surprisal, which only have access to 60.4%of the words.5.2 Qualitative PropertiesWe now examine the second key question we de-fined in Section 1, investigating the qualitative fea-tures of the simulated fixation sequences.
We willfocus on comparing the predictions of NEAT withthat of word frequency, which performs comparablyat the task of predicting fixation sequences (see Sec-Human NEAT Word Freq.ADJ 78.9 (2) 72.8 (1) 98.4 (3)ADP 46.1 (8) 53.8 (8) 21.6 (9)ADV 70.4 (3) 67.2 (4) 96.4 (4)CONJ 36.7 (11) 50.7 (9) 14.6 (10)DET 45.2 (9) 44.8 (11) 22.9 (8)NOUN 80.3 (1) 69.8 (2) 98.7 (2)NUM 63.3 (6) 71.5 (3) 99.5 (1)PRON 49.2 (7) 57.0 (7) 42.6 (7)PRT 37.4 (10) 46.7 (10) 13.9 (11)VERB 66.7 (5) 64.7 (5) 74.4 (5)X 68.6 (4) 67.8 (3) 69.0 (6)Spearman?s ?
0.85 0.84Pearson?s r 0.92 0.94MSE 57 450Table 4: Actual and simulated fixation probabilities (in %) byPoS tag, with the ranks given in brackets, and correlations andmean squared error relative to human data.tion 5.1).
We show NEAT nevertheless makes rele-vant predictions that go beyond frequency.Fixations of Successive Words While predictorsderived from word frequency treat the decisionwhether to fixate or skip words as independent, hu-mans are more likely to fixate a word when the pre-vious word was skipped (Rayner, 1998).
This effectis also seen in NEAT.
More precisely, both in thehuman data and in the simulated fixation data, theconditional fixation probability P(?i = 1|?i?1 = 1)is lower than the marginal probability P(?i = 1).The ratio of these probabilities is 0.85 in the humandata, and 0.81 in NEAT.
The threshold predictor de-rived from word frequency also shows this effect (asthe frequencies of successive words are not indepen-dent), but it is weaker (ratio 0.91).To further test the context dependence of NEAT?sfixation behavior, we ran a mixed model predict-ing the fixation probabilities simulated by NEAT,with items as random factor and the log frequencyof word i as predictor.
Adding ?i?1 as a predic-tor results in a significant improvement in modelfit (deviance = 4,798, t = 71.3).
This shows thatNEAT captures the context dependence of fixationsequences to an extend that goes beyond word fre-quency alone.92Parts of Speech Part of speech categories areknown to be a predictor of fixation probabilities,with content words being more likely to be fixatedthan function words (Carpenter and Just, 1983).
InTable 4, we give the simulated fixation probabilitiesand the human fixation probabilities estimated fromthe Dundee corpus for the tags of the Universal PoStagset (Petrov et al, 2012), using the PoS annotationof Barrett et al (2015).
We again compare with theprobabilities of a threshold predictor derived fromword frequency.1 NEAT captures the differences be-tween PoS categories well, as evidenced by the highcorrelation coefficients.
The content word categoriesADJ, ADV, NOUN, VERB and X consistently showhigher probabilities than the function word cate-gories.
While the correlation coefficients for wordfrequency are very similar, the numerical values ofthe simulated probabilities are closer to the humanones than those derived from word frequency, whichtend towards more extreme values.
This differencecan be seen clearly if we compare the mean squarederror, rather than the correlation, with the human fix-ation probabilities (last row of Table 4).Correlations with Known Predictors In the lit-erature, it has been observed that skipping correlateswith predictability (surprisal), word frequency, andword length (Rayner, 1998, p. 387).
These correla-tions are also observed in the human skipping dataderived from Dundee, as shown in Table 5.
(Hu-man fixation probabilities were obtained by averag-ing over the ten readers in Dundee.
)Comparing the known predictors of skipping withNEAT?s simulated fixation probabilities, similar cor-relations as in the human data are observed.
We ob-serve that the correlations with surprisal are strongerin NEAT, considering both restricted surprisal andfull surprisal as measures of predictability.6 ConclusionsWe investigated the hypothesis that human read-ing strategies optimize a tradeoff between precisionof language understanding and economy of atten-tion.
We made this idea explicit in NEAT, a neuralreading architecture with hard attention that can be1We omit the tag ?.?
for punctuation, as punctuation charac-ters are not treated as separate tokens in Dundee.Human NEATRestricted Surprisal 0.465 0.762Full Surprisal 0.512 0.720Log Word Freq.
?0.608 ?0.760Word Length 0.663 0.521Table 5: Correlations between human and NEAT fixation prob-abilities and known predictorstrained end-to-end to optimize this tradeoff.
Exper-iments on the Dundee corpus show that NEAT pro-vides accurate predictions for human skipping be-havior.
It also predicts reading times, even though itonly has access to 60.4% of the words in the cor-pus in order to estimate surprisal.
Finally, we foundthat known qualitative properties of skipping emergein our model, even though they were not explicitlyincluded in the architecture, such as context depen-dence of fixations, differential skipping rates acrossparts of speech, and correlations with other knownpredictors of human reading behavior.ReferencesBa, Jimmy, Ruslan R. Salakhutdinov, Roger B.Grosse, and Brendan J. Frey.
2015.
Learningwake-sleep recurrent attention models.
In Ad-vances in Neural Information Processing Sys-tems.
pages 2575?2583.Bahdanau, Dzmitry, Kyunghyun Cho, and YoshuaBengio.
2015.
Neural machine translation byjointly learning to align and translate.
In Proceed-ings of the International Conference on LearningRepresentations.Barrett, Maria, Z?eljko Agic?, and Anders S?gaard.2015.
The Dundee treebank.
In Proceedingsof the 14th International Workshop on Treebanksand Linguistic Theories.
pages 242?248.Bicknell, Klinton and Roger Levy.
2010.
A ratio-nal model of eye movement control in reading.In Proceedings of the 48th Annual Meeting of theAssociation for Computational Linguistics.
pages1168?1178.Carpenter, P. A. and M. A.
Just.
1983.
What youreyes do while your mind is reading.
In K. Rayner,editor, Eye Movements in Reading, AcademicPress, New York, pages 275?307.93Demberg, Vera and Frank Keller.
2008.
Datafrom eye-tracking corpora as evidence for theo-ries of syntactic processing complexity.
Cognition109(2):193?210.Engbert, Ralf, Andre?
Longtin, and Reinhold Kliegl.2002.
A dynamical model of saccade generationin reading based on spatially distributed lexicalprocessing.
Vision Research 42(5):621?636.Engbert, Ralf, Antje Nuthmann, Eike M. Richter,and Reinhold Kliegl.
2005.
SWIFT: A dynami-cal model of saccade generation during reading.Psychological Review 112(4):777?813.Frank, S.L.
and R. Bod.
2011.
Insensitivity of thehuman sentence-processing system to hierarchi-cal structure.
Psychological Science 22:829?834.Hale, John.
2001.
A probabilistic Earley parser as apsycholinguistic model.
In Proceedings of Con-ference of the North American Chapter of theAssociation for Computational Linguistics.
vol-ume 2, pages 159?166.Hara, Tadayoshi, Daichi Mochihashi YoshinobuKano, and Akiko Aizawa.
2012.
Predicting wordfixations in text with a CRF model for capturinggeneral reading strategies among readers.
In Pro-ceedings of the 1st Workshop on Eye-tracking andNatural Language Processing.
pages 55?70.Henderson, John.
2003.
Human gaze control in real-world scene perception.
Trends in Cognitive Sci-ences 7:498?504.Hermann, Karl Moritz, Toma?s?
Koc?isky`, Ed-ward Grefenstette, Lasse Espeholt, Will Kay,Mustafa Suleyman, and Phil Blunsom.
2015.Teaching machines to read and comprehend.ArXiv:1506.03340.Hochreiter, Sepp and Ju?rgen Schmidhuber.
1997.Long short-term memory.
Neural Computation9(8):1735?1780.Kennedy, Alan and Joe?l Pynte.
2005.
Parafoveal-on-foveal effects in normal reading.
Vision Research45(2):153?168.Levy, Roger.
2008.
Expectation-based syntacticcomprehension.
Cognition 106(3):1126?1177.Matthies, Franz and Anders S?gaard.
2013.
Withblinkers on: Robust prediction of eye movementsacross readers.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Pro-cessing.
pages 803?807.McDonald, Scott A. and Richard C. Shillcock.2003a.
Eye movements reveal the on-line compu-tation of lexical probabilities during reading.
Psy-chological Science 14(6):648?652.McDonald, Scott A. and Richard C. Shillcock.2003b.
Low-level predictive inference in reading:the influence of transitional probabilities on eyemovements.
Vision Research 43(16):1735?1751.Mikolov, Toma?s?, Martin Karafia?t, Luka?s?
Burget, JanC?ernocky?, and Sanjeev Khudanpur.
2010.
Recur-rent neural network based language model.
InProceedings of Interspeech.
pages 1045?1048.Mnih, Volodymyr, Nicolas Heess, Alex Graves, andothers.
2014.
Recurrent models of visual atten-tion.
In Advances in Neural Information Process-ing Systems.
pages 2204?2212.Nilsson, Mattias and Joakim Nivre.
2009.
Learn-ing where to look: Modeling eye movements inreading.
In Proceedings of the 13th Conferenceon Computational Natural Language Learning.pages 93?101.Nilsson, Mattias and Joakim Nivre.
2010.
Towardsa data-driven model of eye movement control inreading.
In Proceedings of the Workshop on Cog-nitive Modeling and Computational Linguistics.pages 63?71.Petrov, Slav, Dipanjan Das, and Ryan T. McDon-ald.
2012.
A universal part-of-speech tagset.
InProceedings of the 8th International Conferenceon Language Resources and Evaluation.
pages2089?2096.Rayner, K. 1998.
Eye movements in reading and in-formation processing: 20 years of research.
Psy-chological Bulletin 124(3):372?422.Rayner, Keith.
2009.
Eye movements in reading:Models and data.
Journal of Eye Movement Re-search 2(5):1?10.Rayner, Keith and Erik D. Reichle.
2010.
Models ofthe reading process.
Wiley Interdisciplinary Re-views: Cognitive Science 1(6):787?799.Reichle, E. D., A. Pollatsek, D. L. Fisher, andK.
Rayner.
1998.
Toward a model of eye move-94ment control in reading.
Psychological Review105(1):125?157.Reichle, E. D., T. Warren, and K. McConnell.
2009.Using EZ Reader to model the effects of higherlevel language processing on eye movements dur-ing reading.
Psychonomic Bulletin & Review16(1):1?21.Reichle, Erik D., Keith Rayner, and Alexander Pol-latsek.
2003.
The EZ Reader model of eye-movement control in reading: Comparisons toother models.
Behavioral and Brain Sciences26(04):445?476.Smith, Nathaniel J. and Roger Levy.
2013.
The ef-fect of word predictability on reading time is log-arithmic.
Cognition 128(3):302?319.Sutskever, Ilya, Oriol Vinyals, and Quoc VV Le.2014.
Sequence to sequence learning with neu-ral networks.
In Advances in Neural InformationProcessing Systems.
pages 3104?3112.Van Gompel, Roger PG and Martin J. Pickering.2007.
Syntactic parsing.
In The Oxford Handbookof Psycholinguistics, Oxford University Press,pages 289?307.Williams, Ronald J.
1992.
Simple statisticalgradient-following algorithms for connectionistreinforcement learning.
Machine Learning 8(3-4):229?256.Xu, Kelvin, Jimmy Ba, Ryan Kiros, AaronCourville, Ruslan Salakhutdinov, Richard Zemel,and Yoshua Bengio.
2015.
Show, attend and tell:Neural image caption generation with visual at-tention.
ArXiv:1502.03044.95
