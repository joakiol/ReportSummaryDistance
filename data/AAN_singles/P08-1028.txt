Proceedings of ACL-08: HLT, pages 236?244,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsVector-based Models of Semantic CompositionJeff Mitchell and Mirella LapataSchool of Informatics, University of Edinburgh2 Buccleuch Place, Edinburgh EH8 9LW, UKjeff.mitchell@ed.ac.uk, mlap@inf.ed.ac.ukAbstractThis paper proposes a framework for repre-senting the meaning of phrases and sentencesin vector space.
Central to our approach isvector composition which we operationalizein terms of additive and multiplicative func-tions.
Under this framework, we introduce awide range of composition models which weevaluate empirically on a sentence similaritytask.
Experimental results demonstrate thatthe multiplicative models are superior to theadditive alternatives when compared againsthuman judgments.1 IntroductionVector-based models of word meaning (Lund andBurgess, 1996; Landauer and Dumais, 1997) havebecome increasingly popular in natural languageprocessing (NLP) and cognitive science.
The ap-peal of these models lies in their ability to rep-resent meaning simply by using distributional in-formation under the assumption that words occur-ring within similar contexts are semantically similar(Harris, 1968).A variety of NLP tasks have made good useof vector-based models.
Examples include au-tomatic thesaurus extraction (Grefenstette, 1994),word sense discrimination (Schu?tze, 1998) and dis-ambiguation (McCarthy et al, 2004), collocation ex-traction (Schone and Jurafsky, 2001), text segmen-tation (Choi et al, 2001) , and notably informationretrieval (Salton et al, 1975).
In cognitive sciencevector-based models have been successful in simu-lating semantic priming (Lund and Burgess, 1996;Landauer and Dumais, 1997) and text comprehen-sion (Landauer and Dumais, 1997; Foltz et al,1998).
Moreover, the vector similarities within suchsemantic spaces have been shown to substantiallycorrelate with human similarity judgments (McDon-ald, 2000) and word association norms (Denhire andLemaire, 2004).Despite their widespread use, vector-based mod-els are typically directed at representing words inisolation and methods for constructing representa-tions for phrases or sentences have received littleattention in the literature.
In fact, the common-est method for combining the vectors is to averagethem.
Vector averaging is unfortunately insensitiveto word order, and more generally syntactic struc-ture, giving the same representation to any construc-tions that happen to share the same vocabulary.
Thisis illustrated in the example below taken from Lan-dauer et al (1997).
Sentences (1-a) and (1-b) con-tain exactly the same set of words but their meaningis entirely different.
(1) a.
It was not the sales manager who hit thebottle that day, but the office worker withthe serious drinking problem.b.
That day the office manager, who wasdrinking, hit the problem sales worker witha bottle, but it was not serious.While vector addition has been effective in someapplications such as essay grading (Landauer andDumais, 1997) and coherence assessment (Foltzet al, 1998), there is ample empirical evidencethat syntactic relations across and within sentencesare crucial for sentence and discourse processing(Neville et al, 1991; West and Stanovich, 1986)and modulate cognitive behavior in sentence prim-ing (Till et al, 1988) and inference tasks (Heit and236Rubinstein, 1994).Computational models of semantics which usesymbolic logic representations (Montague, 1974)can account naturally for the meaning of phrases orsentences.
Central in these models is the notion ofcompositionality ?
the meaning of complex expres-sions is determined by the meanings of their con-stituent expressions and the rules used to combinethem.
Here, semantic analysis is guided by syntacticstructure, and therefore sentences (1-a) and (1-b) re-ceive distinct representations.
The downside of thisapproach is that differences in meaning are qualita-tive rather than quantitative, and degrees of similar-ity cannot be expressed easily.In this paper we examine models of semanticcomposition that are empirically grounded and canrepresent similarity relations.
We present a gen-eral framework for vector-based composition whichallows us to consider different classes of models.Specifically, we present both additive and multi-plicative models of vector combination and assesstheir performance on a sentence similarity rating ex-periment.
Our results show that the multiplicativemodels are superior and correlate significantly withbehavioral data.2 Related WorkThe problem of vector composition has receivedsome attention in the connectionist literature, partic-ularly in response to criticisms of the ability of con-nectionist representations to handle complex struc-tures (Fodor and Pylyshyn, 1988).
While neural net-works can readily represent single distinct objects,in the case of multiple objects there are fundamen-tal difficulties in keeping track of which features arebound to which objects.
For the hierarchical struc-ture of natural language this binding problem be-comes particularly acute.
For example, simplisticapproaches to handling sentences such as John lovesMary and Mary loves John typically fail to makevalid representations in one of two ways.
Eitherthere is a failure to distinguish between these twostructures, because the network fails to keep trackof the fact that John is subject in one and objectin the other, or there is a failure to recognize thatboth structures involve the same participants, be-cause John as a subject has a distinct representationfrom John as an object.
In contrast, symbolic repre-sentations can naturally handle the binding of con-stituents to their roles, in a systematic manner thatavoids both these problems.Smolensky (1990) proposed the use of tensorproducts as a means of binding one vector to an-other.
The tensor product u?
v is a matrix whosecomponents are all the possible products uiv j of thecomponents of vectors u and v. A major difficultywith tensor products is their dimensionality which ishigher than the dimensionality of the original vec-tors (precisely, the tensor product has dimensional-ity m?
n).
To overcome this problem, other tech-niques have been proposed in which the binding oftwo vectors results in a vector which has the samedimensionality as its components.
Holographic re-duced representations (Plate, 1991) are one imple-mentation of this idea where the tensor product isprojected back onto the space of its components.The projection is defined in terms of circular con-volution a mathematical function that compressesthe tensor product of two vectors.
The compressionis achieved by summing along the transdiagonal el-ements of the tensor product.
Noisy versions of theoriginal vectors can be recovered by means of cir-cular correlation which is the approximate inverseof circular convolution.
The success of circular cor-relation crucially depends on the components of then-dimensional vectors u and v being randomly dis-tributed with mean 0 and variance 1n.
This posesproblems for modeling linguistic data which is typi-cally represented by vectors with non-random struc-ture.Vector addition is by far the most commonmethod for representing the meaning of linguisticsequences.
For example, assuming that individualwords are represented by vectors, we can computethe meaning of a sentence by taking their mean(Foltz et al, 1998; Landauer and Dumais, 1997).Vector addition does not increase the dimensional-ity of the resulting vector.
However, since it is orderindependent, it cannot capture meaning differencesthat are modulated by differences in syntactic struc-ture.
Kintsch (2001) proposes a variation on the vec-tor addition theme in an attempt to model how themeaning of a predicate (e.g., run) varies dependingon the arguments it operates upon (e.g, the horse ranvs.
the color ran).
The idea is to add not only thevectors representing the predicate and its argumentbut also the neighbors associated with both of them.The neighbors, Kintsch argues, can ?strengthen fea-tures of the predicate that are appropriate for the ar-gument of the predication?.237animal stable village gallop jokeyhorse 0 6 2 10 4run 1 8 4 4 0Figure 1: A hypothetical semantic space for horse andrunUnfortunately, comparisons across vector compo-sition models have been few and far between in theliterature.
The merits of different approaches are il-lustrated with a few hand picked examples and pa-rameter values and large scale evaluations are uni-formly absent (see Frank et al (2007) for a criticismof Kintsch?s (2001) evaluation standards).
Our workproposes a framework for vector composition whichallows the derivation of different types of modelsand licenses two fundamental composition opera-tions, multiplication and addition (and their combi-nation).
Under this framework, we introduce novelcomposition models which we compare empiricallyagainst previous work using a rigorous evaluationmethodology.3 Composition ModelsWe formulate semantic composition as a functionof two vectors, u and v. We assume that indi-vidual words are represented by vectors acquiredfrom a corpus following any of the parametrisa-tions that have been suggested in the literature.1 Webriefly note here that a word?s vector typically rep-resents its co-occurrence with neighboring words.The construction of the semantic space depends onthe definition of linguistic context (e.g., neighbour-ing words can be documents or collocations), thenumber of components used (e.g., the k most fre-quent words in a corpus), and their values (e.g., asraw co-occurrence frequencies or ratios of probabil-ities).
A hypothetical semantic space is illustrated inFigure 1.
Here, the space has only five dimensions,and the matrix cells denote the co-occurrence of thetarget words (horse and run) with the context wordsanimal, stable, and so on.Let p denote the composition of two vectors uand v, representing a pair of constituents whichstand in some syntactic relation R. Let K stand forany additional knowledge or information which isneeded to construct the semantics of their composi-1A detailed treatment of existing semantic space models isoutside the scope of the present paper.
We refer the interestedreader to Pado?
and Lapata (2007) for a comprehensive overview.tion.
We define a general class of models for thisprocess of composition as:p = f (u,v,R,K) (1)The expression above allows us to derive models forwhich p is constructed in a distinct space from uand v, as is the case for tensor products.
It alsoallows us to derive models in which compositionmakes use of background knowledge K and mod-els in which composition has a dependence, via theargument R, on syntax.To derive specific models from this general frame-work requires the identification of appropriate con-straints to narrow the space of functions being con-sidered.
One particularly useful constraint is tohold R fixed by focusing on a single well definedlinguistic structure, for example the verb-subject re-lation.
Another simplification concerns K which canbe ignored so as to explore what can be achieved inthe absence of additional knowledge.
This reducesthe class of models to:p = f (u,v) (2)However, this still leaves the particular form of thefunction f unspecified.
Now, if we assume that plies in the same space as u and v, avoiding the issuesof dimensionality associated with tensor products,and that f is a linear function, for simplicity, of thecartesian product of u and v, then we generate a classof additive models:p = Au+Bv (3)where A and B are matrices which determine thecontributions made by u and v to the product p. Incontrast, if we assume that f is a linear function ofthe tensor product of u and v, then we obtain multi-plicative models:p = Cuv (4)where C is a tensor of rank 3, which projects thetensor product of u and v onto the space of p.Further constraints can be introduced to reducethe free parameters in these models.
So, if we as-sume that only the ith components of u and v con-tribute to the ith component of p, that these com-ponents are not dependent on i, and that the func-tion is symmetric with regard to the interchange of u238and v, we obtain a simpler instantiation of an addi-tive model:pi = ui + vi (5)Analogously, under the same assumptions, we ob-tain the following simpler multiplicative model:pi = ui ?
vi (6)For example, according to (5), the addition of thetwo vectors representing horse and run in Fig-ure 1 would yield horse+ run = [1 14 6 14 4].Whereas their product, as given by (6), ishorse ?
run = [0 48 8 40 0].Although the composition model in (5) is com-monly used in the literature, from a linguistic per-spective, the model in (6) is more appealing.
Sim-ply adding the vectors u and v lumps their contentstogether rather than allowing the content of one vec-tor to pick out the relevant content of the other.
In-stead, it could be argued that the contribution of theith component of u should be scaled according to itsrelevance to v, and vice versa.
In effect, this is whatmodel (6) achieves.As a result of the assumption of symmetry, boththese models are ?bag of words?
models and wordorder insensitive.
Relaxing the assumption of sym-metry in the case of the simple additive model pro-duces a model which weighs the contribution of thetwo components differently:pi = ?ui +?vi (7)This allows additive models to become moresyntax aware, since semantically important con-stituents can participate more actively in the com-position.
As an example if we set ?
to 0.4and ?
to 0.6, then horse = [0 2.4 0.8 4 1.6]and run = [0.6 4.8 2.4 2.4 0], and their sumhorse+ run = [0.6 5.6 3.2 6.4 1.6].An extreme form of this differential in the contri-bution of constituents is where one of the vectors,say u, contributes nothing at all to the combination:pi = v j (8)Admittedly the model in (8) is impoverished andrather simplistic, however it can serve as a simplebaseline against which to compare more sophisti-cated models.The models considered so far assume that com-ponents do not ?interfere?
with each other, i.e., thatonly the ith components of u and v contribute to theith component of p. Another class of models can bederived by relaxing this constraint.
To give a con-crete example, circular convolution is an instance ofthe general multiplicative model which breaks thisconstraint by allowing u j to contribute to pi:pi = ?ju j ?
vi?
j (9)It is also possible to re-introduce the dependenceon K into the model of vector composition.
For ad-ditive models, a natural way to achieve this is to in-clude further vectors into the summation.
These vec-tors are not arbitrary and ideally they must exhibitsome relation to the words of the construction underconsideration.
When modeling predicate-argumentstructures, Kintsch (2001) proposes including one ormore distributional neighbors, n, of the predicate:p = u+v+?n (10)Note that considerable latitude is allowed in select-ing the appropriate neighbors.
Kintsch (2001) con-siders only the m most similar neighbors to the pred-icate, from which he subsequently selects k, thosemost similar to its argument.
So, if in the composi-tion of horse with run, the chosen neighbor is ride,ride = [2 15 7 9 1], then this produces the repre-sentation horse+ run+ ride = [3 29 13 23 5].
Incontrast to the simple additive model, this extendedmodel is sensitive to syntactic structure, since n ischosen from among the neighbors of the predicate,distinguishing it from the argument.Although we have presented multiplicative andadditive models separately, there is nothing inherentin our formulation that disallows their combination.The proposal is not merely notational.
One poten-tial drawback of multiplicative models is the effectof components with value zero.
Since the productof zero with any number is itself zero, the presenceof zeroes in either of the vectors leads to informa-tion being essentially thrown away.
Combining themultiplicative model with an additive model, whichdoes not suffer from this problem, could mitigatethis problem:pi = ?ui +?vi + ?uivi (11)where ?, ?, and ?
are weighting constants.2394 Evaluation Set-upWe evaluated the models presented in Section 3on a sentence similarity task initially proposed byKintsch (2001).
In his study, Kintsch builds a modelof how a verb?s meaning is modified in the context ofits subject.
He argues that the subjects of ran in Thecolor ran and The horse ran select different sensesof ran.
This change in the verb?s sense is equated toa shift in its position in semantic space.
To quantifythis shift, Kintsch proposes measuring similarity rel-ative to other verbs acting as landmarks, for examplegallop and dissolve.
The idea here is that an appro-priate composition model when applied to horse andran will yield a vector closer to the landmark gallopthan dissolve.
Conversely, when color is combinedwith ran, the resulting vector will be closer to dis-solve than gallop.Focusing on a single compositional structure,namely intransitive verbs and their subjects, is agood point of departure for studying vector combi-nation.
Any adequate model of composition must beable to represent argument-verb meaning.
Moreoverby using a minimal structure we factor out inessen-tial degrees of freedom and are able to assess themerits of different models on an equal footing.
Un-fortunately, Kintsch (2001) demonstrates how hisown composition algorithm works intuitively on afew hand selected examples but does not provide acomprehensive test set.
In order to establish an inde-pendent measure of sentence similarity, we assem-bled a set of experimental materials and elicited sim-ilarity ratings from human subjects.
In the followingwe describe our data collection procedure and givedetails on how our composition models were con-structed and evaluated.Materials and Design Our materials consistedof sentences with an an intransitive verb and its sub-ject.
We first compiled a list of intransitive verbsfrom CELEX2.
All occurrences of these verbs witha subject noun were next extracted from a RASPparsed (Briscoe and Carroll, 2002) version of theBritish National Corpus (BNC).
Verbs and nounsthat were attested less than fifty times in the BNCwere removed as they would result in unreliable vec-tors.
Each reference subject-verb tuple (e.g., horseran) was paired with two landmarks, each a syn-onym of the verb.
The landmarks were chosen soas to represent distinct verb senses, one compatible2http://www.ru.nl/celex/with the reference (e.g., horse galloped ) and one in-compatible (e.g., horse dissolved ).
Landmarks weretaken from WordNet (Fellbaum, 1998).
Specifically,they belonged to different synsets and were maxi-mally dissimilar as measured by the Jiang and Con-rath (1997) measure.3Our initial set of candidate materials consistedof 20 verbs, each paired with 10 nouns, and 2 land-marks (400 pairs of sentences in total).
These werefurther pretested to allow the selection of a subsetof items showing clear variations in sense as wewanted to have a balanced set of similar and dis-similar sentences.
In the pretest, subjects saw areference sentence containing a subject-verb tupleand its landmarks and were asked to choose whichlandmark was most similar to the reference or nei-ther.
Our items were converted into simple sentences(all in past tense) by adding articles where appropri-ate.
The stimuli were administered to four separategroups; each group saw one set of 100 sentences.The pretest was completed by 53 participants.For each reference verb, the subjects?
responseswere entered into a contingency table, whose rowscorresponded to nouns and columns to each possi-ble answer (i.e., one of the two landmarks).
Eachcell recorded the number of times our subjects se-lected the landmark as compatible with the noun ornot.
We used Fisher?s exact test to determine whichverbs and nouns showed the greatest variation inlandmark preference and items with p-values greaterthan 0.001 were discarded.
This yielded a reducedset of experimental items (120 in total) consisting of15 reference verbs, each with 4 nouns, and 2 land-marks.Procedure and Subjects Participants first sawa set of instructions that explained the sentence sim-ilarity task and provided several examples.
Thenthe experimental items were presented; each con-tained two sentences, one with the reference verband one with its landmark.
Examples of our itemsare given in Table 1.
Here, burn is a high similaritylandmark (High) for the reference The fire glowed,whereas beam is a low similarity landmark (Low).The opposite is the case for the reference The face3We assessed a wide range of semantic similarity measuresusing the WordNet similarity package (Pedersen et al, 2004).Most of them yielded similar results.
We selected Jiang andConrath?s measure since it has been shown to perform consis-tently well across several cognitive and NLP tasks (Budanitskyand Hirst, 2001).240Noun Reference High LowThe fire glowed burned beamedThe face glowed beamed burnedThe child strayed roamed digressedThe discussion strayed digressed roamedThe sales slumped declined slouchedThe shoulders slumped slouched declinedTable 1: Example Stimuli with High and Low similaritylandmarksglowed.
Sentence pairs were presented serially inrandom order.
Participants were asked to rate howsimilar the two sentences were on a scale of oneto seven.
The study was conducted remotely overthe Internet using Webexp4, a software package de-signed for conducting psycholinguistic studies overthe web.
49 unpaid volunteers completed the exper-iment, all native speakers of English.Analysis of Similarity Ratings The reliabilityof the collected judgments is important for our eval-uation experiments; we therefore performed severaltests to validate the quality of the ratings.
First, weexamined whether participants gave high ratings tohigh similarity sentence pairs and low ratings to lowsimilarity ones.
Figure 2 presents a box-and-whiskerplot of the distribution of the ratings.
As we can seesentences with high similarity landmarks are per-ceived as more similar to the reference sentence.
AWilcoxon rank sum test confirmed that the differ-ence is statistically significant (p < 0.01).
We alsomeasured how well humans agree in their ratings.We employed leave-one-out resampling (Weiss andKulikowski, 1991), by correlating the data obtainedfrom each participant with the ratings obtained fromall other participants.
We used Spearman?s ?, a nonparametric correlation coefficient, to avoid makingany assumptions about the distribution of the simi-larity ratings.
The average inter-subject agreement5was ?
= 0.40.
We believe that this level of agree-ment is satisfactory given that naive subjects areasked to provide judgments on fine-grained seman-tic distinctions (see Table 1).
More evidence thatthis is not an easy task comes from Figure 2 wherewe observe some overlap in the ratings for High andLow similarity items.4http://www.webexp.info/5Note that Spearman?s rho tends to yield lower coefficientscompared to parametric alternatives such as Pearson?s r.High Low01234567Figure 2: Distribution of elicited ratings for High andLow similarity itemsModel Parameters Irrespectively of their form,all composition models discussed here are based ona semantic space for representing the meanings ofindividual words.
The semantic space we used inour experiments was built on a lemmatised versionof the BNC.
Following previous work (Bullinariaand Levy, 2007), we optimized its parameters on aword-based semantic similarity task.
The task in-volves examining the degree of linear relationshipbetween the human judgments for two individualwords and vector-based similarity values.
We ex-perimented with a variety of dimensions (rangingfrom 50 to 500,000), vector component definitions(e.g., pointwise mutual information or log likelihoodratio) and similarity measures (e.g., cosine or confu-sion probability).
We used WordSim353, a bench-mark dataset (Finkelstein et al, 2002), consisting ofrelatedness judgments (on a scale of 0 to 10) for 353word pairs.We obtained best results with a model using acontext window of five words on either side of thetarget word, the cosine measure, and 2,000 vectorcomponents.
The latter were the most common con-text words (excluding a stop list of function words).These components were set to the ratio of the proba-bility of the context word given the target word tothe probability of the context word overall.
Thisconfiguration gave high correlations with the Word-Sim353 similarity judgments using the cosine mea-sure.
In addition, Bullinaria and Levy (2007) foundthat these parameters perform well on a number ofother tasks such as the synonymy task from the Testof English as a Foreign Language (TOEFL).Our composition models have no additional pa-241rameters beyond the semantic space just described,with three exceptions.
First, the additive modelin (7) weighs differentially the contribution of thetwo constituents.
In our case, these are the sub-ject noun and the intransitive verb.
To this end,we optimized the weights on a small held-out set.Specifically, we considered eleven models, varyingin their weightings, in steps of 10%, from 100%noun through 50% of both verb and noun to 100%verb.
For the best performing model the weightfor the verb was 80% and for the noun 20%.
Sec-ondly, we optimized the weightings in the combinedmodel (11) with a similar grid search over its threeparameters.
This yielded a weighted sum consistingof 95% verb, 0% noun and 5% of their multiplica-tive combination.
Finally, Kintsch?s (2001) additivemodel has two extra parameters.
The m neighborsmost similar to the predicate, and the k of m neigh-bors closest to its argument.
In our experiments weselected parameters that Kintsch reports as optimal.Specifically, m was set to 20 and m to 1.Evaluation Methodology We evaluated theproposed composition models in two ways.
First,we used the models to estimate the cosine simi-larity between the reference sentence and its land-marks.
We expect better models to yield a pattern ofsimilarity scores like those observed in the humanratings (see Figure 2).
A more scrupulous evalua-tion requires directly correlating all the individualparticipants?
similarity judgments with those of themodels.6 We used Spearman?s ?
for our correlationanalyses.
Again, better models should correlate bet-ter with the experimental data.
We assume that theinter-subject agreement can serve as an upper boundfor comparing the fit of our models against the hu-man judgments.5 ResultsOur experiments assessed the performance of sevencomposition models.
These included three additivemodels, i.e., simple addition (equation (5), Add),weighted addition (equation (7), WeightAdd), andKintsch?s (2001) model (equation (10), Kintsch), amultiplicative model (equation (6), Multiply), andalso a model which combines multiplication with6We avoided correlating the model predictions with aver-aged participant judgments as this is inappropriate given the or-dinal nature of the scale of these judgments and also leads to adependence between the number of participants and the magni-tude of the correlation coefficient.Model High Low ?NonComp 0.27 0.26 0.08**Add 0.59 0.59 0.04*WeightAdd 0.35 0.34 0.09**Kintsch 0.47 0.45 0.09**Multiply 0.42 0.28 0.17**Combined 0.38 0.28 0.19**UpperBound 4.94 3.25 0.40**Table 2: Model means for High and Low similarityitems and correlation coefficients with human judgments(*: p < 0.05, **: p < 0.01)addition (equation (11), Combined).
As a baselinewe simply estimated the similarity between the ref-erence verb and its landmarks without taking thesubject noun into account (equation (8), NonComp).Table 2 shows the average model ratings for Highand Low similarity items.
For comparison, we alsoshow the human ratings for these items (Upper-Bound).
Here, we are interested in relative dif-ferences, since the two types of ratings correspondto different scales.
Model similarities have beenestimated using cosine which ranges from 0 to 1,whereas our subjects rated the sentences on a scalefrom 1 to 7.The simple additive model fails to distinguish be-tween High and Low Similarity items.
We observea similar pattern for the non compositional base-line model, the weighted additive model and Kintsch(2001).
The multiplicative and combined modelsyield means closer to the human ratings.
The dif-ference between High and Low similarity values es-timated by these models are statistically significant(p < 0.01 using the Wilcoxon rank sum test).
Fig-ure 3 shows the distribution of estimated similaritiesunder the multiplicative model.The results of our correlation analysis are alsogiven in Table 2.
As can be seen, all models are sig-nificantly correlated with the human ratings.
In or-der to establish which ones fit our data better, we ex-amined whether the correlation coefficients achieveddiffer significantly using a t-test (Cohen and Cohen,1983).
The lowest correlation (?
= 0.04) is observedfor the simple additive model which is not signif-icantly different from the non-compositional base-line model.
The weighted additive model (?
= 0.09)is not significantly different from the baseline eitheror Kintsch (2001) (?
= 0.09).
Given that the basis242High Low00.20.40.60.81Figure 3: Distribution of predicted similarities for thevector multiplication model on High and Low similarityitemsof Kintsch?s model is the summation of the verb, aneighbor close to the verb and the noun, it is notsurprising that it produces results similar to a sum-mation which weights the verb more heavily thanthe noun.
The multiplicative model yields a betterfit with the experimental data, ?
= 0.17.
The com-bined model is best overall with ?
= 0.19.
However,the difference between the two models is not statis-tically significant.
Also note that in contrast to thecombined model, the multiplicative model does nothave any free parameters and hence does not requireoptimization for this particular task.6 DiscussionIn this paper we presented a general framework forvector-based semantic composition.
We formulatedcomposition as a function of two vectors and intro-duced several models based on addition and multi-plication.
Despite the popularity of additive mod-els, our experimental results showed the superior-ity of models utilizing multiplicative combinations,at least for the sentence similarity task attemptedhere.
We conjecture that the additive models arenot sensitive to the fine-grained meaning distinc-tions involved in our materials.
Previous applica-tions of vector addition to document indexing (Deer-wester et al, 1990) or essay grading (Landauer et al,1997) were more concerned with modeling the gistof a document rather than the meaning of its sen-tences.
Importantly, additive models capture com-position by considering all vector components rep-resenting the meaning of the verb and its subject,whereas multiplicative models consider a subset,namely non-zero components.
The resulting vectoris sparser but expresses more succinctly the meaningof the predicate-argument structure, and thus allowssemantic similarity to be modelled more accurately.Further research is needed to gain a deeper un-derstanding of vector composition, both in terms ofmodeling a wider range of structures (e.g., adjective-noun, noun-noun) and also in terms of exploring thespace of models more fully.
We anticipate that moresubstantial correlations can be achieved by imple-menting more sophisticated models from within theframework outlined here.
In particular, the generalclass of multiplicative models (see equation (4)) ap-pears to be a fruitful area to explore.
Future direc-tions include constraining the number of free param-eters in linguistically plausible ways and scaling tolarger datasets.The applications of the framework discussed hereare many and varied both for cognitive science andNLP.
We intend to assess the potential of our com-position models on context sensitive semantic prim-ing (Till et al, 1988) and inductive inference (Heitand Rubinstein, 1994).
NLP tasks that could benefitfrom composition models include paraphrase iden-tification and context-dependent language modeling(Coccaro and Jurafsky, 1998).ReferencesE.
Briscoe, J. Carroll.
2002.
Robust accurate statisticalannotation of general text.
In Proceedings of the 3rdInternational Conference on Language Resources andEvaluation, 1499?1504, Las Palmas, Canary Islands.A.
Budanitsky, G. Hirst.
2001.
Semantic distance inWordNet: An experimental, application-oriented eval-uation of five measures.
In Proceedings of ACL Work-shop on WordNet and Other Lexical Resources, Pitts-burgh, PA.J.
Bullinaria, J.
Levy.
2007.
Extracting semantic rep-resentations from word co-occurrence statistics: Acomputational study.
Behavior Research Methods,39:510?526.F.
Choi, P. Wiemer-Hastings, J. Moore.
2001.
Latent se-mantic analysis for text segmentation.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing, 109?117, Pittsburgh, PA.N.
Coccaro, D. Jurafsky.
1998.
Towards better integra-tion of semantic predictors in statistical language mod-eling.
In Proceedings of the 5th International Confer-ence on Spoken Language Processsing, Sydney, Aus-tralia.243J.
Cohen, P. Cohen.
1983.
Applied Multiple Regres-sion/Correlation Analysis for the Behavioral Sciences.Hillsdale, NJ: Erlbaum.S.
C. Deerwester, S. T. Dumais, T. K. Landauer, G. W.Furnas, R. A. Harshman.
1990.
Indexing by latentsemantic analysis.
Journal of the American Society ofInformation Science, 41(6):391?407.G.
Denhire, B. Lemaire.
2004.
A computational modelof children?s semantic memory.
In Proceedings of the26th Annual Meeting of the Cognitive Science Society,297?302, Chicago, IL.C.
Fellbaum, ed.
1998.
WordNet: An ElectronicDatabase.
MIT Press, Cambridge, MA.L.
Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,Z.
Solan, G. Wolfman, E. Ruppin.
2002.
Placingsearch in context: The concept revisited.
ACM Trans-actions on Information Systems, 20(1):116?131.J.
Fodor, Z. Pylyshyn.
1988.
Connectionism and cogni-tive architecture: A critical analysis.
Cognition, 28:3?71.P.
W. Foltz, W. Kintsch, T. K. Landauer.
1998.
Themeasurement of textual coherence with latent semanticanalysis.
Discourse Process, 15:285?307.S.
Frank, M. Koppen, L. Noordman, W. Vonk.
2007.World knowledge in computational models of dis-course comprehension.
Discourse Processes.
In press.G.
Grefenstette.
1994.
Explorations in Automatic The-saurus Discovery.
Kluwer Academic Publishers.Z.
Harris.
1968.
Mathematical Structures of Language.Wiley, New York.E.
Heit, J. Rubinstein.
1994.
Similarity and property ef-fects in inductive reasoning.
Journal of Experimen-tal Psychology: Learning, Memory, and Cognition,20:411?422.J.
J. Jiang, D. W. Conrath.
1997.
Semantic similaritybased on corpus statistics and lexical taxonomy.
InProceedings of International Conference on Researchin Computational Linguistics, Taiwan.W.
Kintsch.
2001.
Predication.
Cognitive Science,25(2):173?202.T.
K. Landauer, S. T. Dumais.
1997.
A solution to Plato?sproblem: the latent semantic analysis theory of ac-quisition, induction and representation of knowledge.Psychological Review, 104(2):211?240.T.
K. Landauer, D. Laham, B. Rehder, M. E. Schreiner.1997.
How well can passage meaning be derived with-out using word order: A comparison of latent semanticanalysis and humans.
In Proceedings of 19th AnnualConference of the Cognitive Science Society, 412?417,Stanford, CA.K.
Lund, C. Burgess.
1996.
Producing high-dimensionalsemantic spaces from lexical co-occurrence.
Be-havior Research Methods, Instruments & Computers,28:203?208.D.
McCarthy, R. Koeling, J. Weeds, J. Carroll.
2004.Finding predominant senses in untagged text.
InProceedings of the 42nd Annual Meeting of the As-sociation for Computational Linguistics, 280?287,Barcelona, Spain.S.
McDonald.
2000.
Environmental Determinants ofLexical Processing Effort.
Ph.D. thesis, University ofEdinburgh.R.
Montague.
1974.
English as a formal language.
InR.
Montague, ed., Formal Philosophy.
Yale UniversityPress, New Haven, CT.H.
Neville, J. L. Nichol, A. Barss, K. I. Forster, M. F. Gar-rett.
1991.
Syntactically based sentence prosessingclasses: evidence form event-related brain potentials.Journal of Congitive Neuroscience, 3:151?165.S.
Pado?, M. Lapata.
2007.
Dependency-based construc-tion of semantic space models.
Computational Lin-guistics, 33(2):161?199.T.
Pedersen, S. Patwardhan, J. Michelizzi.
2004.
Word-Net::similarity - measuring the relatedness of con-cepts.
In Proceedings of the 5th Annual Meeting of theNorth American Chapter of the Association for Com-putational Linguistics, 38?41, Boston, MA.T.
A.
Plate.
1991.
Holographic reduced representations:Convolution algebra for compositional distributed rep-resentations.
In Proceedings of the 12th Interna-tional Joint Conference on Artificial Intelligence, 30?35, Sydney, Australia.G.
Salton, A. Wong, C. S. Yang.
1975.
A vector spacemodel for automatic indexing.
Communications of theACM, 18(11):613?620.P.
Schone, D. Jurafsky.
2001.
Is knowledge-free induc-tion of multiword unit dictionary headwords a solvedproblem?
In Proceedings of the Conference on Empir-ical Methods in Natural Language Processing, 100?108, Pittsburgh, PA.H.
Schu?tze.
1998.
Automatic word sense discrimination.Computational Linguistics, 24(1):97?124.P.
Smolensky.
1990.
Tensor product variable binding andthe representation of symbolic structures in connec-tionist systems.
Artificial Intelligence, 46:159?216.R.
E. Till, E. F. Mross, W. Kintsch.
1988.
Time course ofpriming for associate and inference words in discoursecontext.
Memory and Cognition, 16:283?299.S.
M. Weiss, C. A. Kulikowski.
1991.
Computer Sys-tems that Learn: Classification and Prediction Meth-ods from Statistics, Neural Nets, Machine Learning,and Expert Systems.
Morgan Kaufmann, San Mateo,CA.R.
F. West, K. E. Stanovich.
1986.
Robust effects ofsyntactic structure on visual word processing.
Journalof Memory and Cognition, 14:104?112.244
