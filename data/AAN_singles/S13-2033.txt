Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 183?187, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsWSD2: Parameter optimisation for Memory-based Cross-LingualWord-Sense DisambiguationMaarten van Gompel and Antal van den BoschCentre for Language Studies, Radboud University Nijmegenproycon@anaproy.nl, a.vandenbosch@let.ru.nlAbstractWe present our system WSD2 which partic-ipated in the Cross-Lingual Word-Sense Dis-ambiguation task for SemEval 2013 (Lefeverand Hoste, 2013).
The system closely resem-bles our winning system for the same task inSemEval 2010.
It is based on k-nearest neigh-bour classifiers which map words with localand global context features onto their transla-tion, i.e.
their cross-lingual sense.
The sys-tem participated in the task for all five lan-guages and obtained winning scores for fourof them when asked to predict the best trans-lation(s).
We tested various configurations ofour system, focusing on various levels of hy-perparameter optimisation and feature selec-tion.
Our final results indicate that hyperpa-rameter optimisation did not lead to the bestresults, indicating overfitting by our optimisa-tion method in this aspect.
Feature selectiondoes have a modest positive impact.1 IntroductionWSD2 is a rewrite and extension of our previoussystem (van Gompel, 2010) that participated in theCross-Lingual Word Sense Disambiguation task inSemEval 2010 (Lefever and Hoste, 2010).
In WSD2we introduce and test a new level of hyperparame-ter optimisation.
Unlike the previous occasion, weparticipate in all five target languages (Dutch, Span-ish, Italian, French, and German).
The task presentstwenty polysemous nouns with fifty instances eachto be mapped onto normalised (lemmatised) transla-tions in all languages.
The task is described in detailby Lefever and Hoste (2013).Trial data is provided and has been used to op-timise system parameters.
Due to the unsupervisednature of the task, no training data is provided.
How-ever, given that the gold standard of the task is basedexclusively on the Europarl parallel corpus (Koehn,2005), we select that same corpus to minimise ourchances of delivering translations that the humanannotators preparing the test data could have neverpicked.Systems may output several senses per instance,rather than producing just one sense prediction.These are evaluated in two different ways.
The scor-ing type ?best?
expects that the system outputs thesense it considers the most likely, or a number ofsenses in the order of its confidence in these sensesbeing correct.
Multiple guesses are penalised, how-ever.
In contrast, the scoring type ?out of five?
ex-pects five guesses, in which each answer carries thesame weight.
These metrics are more extensivelydescribed in Mihalcea et al(2010) and Lefever andHoste (2013).2 System DescriptionThe WSD2 system, like its predecessor, distributesthe task over word experts.
Each word expertis a k-nearest neighbour classifier specialising inthe disambiguation of a single of the twenty pro-vided nouns.
This is implemented using the TilburgMemory Based Learner (TiMBL) (Daelemans et al2009).
The classifiers are trained as follows: Firstthe parallel corpus which acts as training data is to-kenised using Ucto (van Gompel et al 2012), forall five language pairs.
Then, a word-alignment be-tween sentence pairs in the Europarl training datais established, for which we use GIZA++ (Ochand Ney, 2000).
We use the intersection of bothtranslation directions, as we know the sense reposi-183tory from which the human annotators preparing thetask?s test data can select their translations is createdin the same fashion.Whilst the word alignment is computed on the ac-tual word forms, we also need lemmas for both thesource language (English) as well as for all of thefive target languages.
The English nouns in the testdata can be either singular or plural, and both formsmay occur in the input.
Second, the target transla-tions all have to be mapped to their lemma forms.Moreover, to be certain we are dealing with nounsin the source language, a Part-of-Speech tagger isalso required.
PoS tagging and lemmatisation is con-ducted using Freeling (Atserias et al 2006) for En-glish, Spanish and Italian; Frog (van den Bosch etal., 2007) for Dutch, and TreeTagger (Schmid, 1994)for German and French.With all of this data generated, we then iterateover all sentences in the parallel corpus and extractoccurrences of any of the twenty nouns, along withthe translation they are aligned to according to theword alignment.
We extract the words themselvesand compute the lemma and the part-of-speech tag,and do the same for a specified number of wordsto the left and to the right of the found occurrence.These constitute the local context features.In addition to this, global context features are ex-tracted; these are a set of keywords per lemma andper translation which are found occurring above cer-tain occurrence thresholds at arbitrary positions inthe same sentence, as this is the widest context sup-plied in the task data.
The global context featuresare represented as a binary bag-of-words model inwhich the presence of each of the keywords that maybe indicative for a given mapping of the focus wordto a sense is represented by a boolean value.
Such aset of keywords is constructed for each of the twentynouns, per language.The method used to extract these keywords (k)is proposed by Ng and Lee (1996) and used alsoby Hoste et al(2002).
Assume we have a focusword f , more precisely, a lemma of one of the tar-get nouns.
We also have one of its aligned transla-tions/senses s, also a lemma.
We can now estimateP (s|k), the probability of sense s, given a keywordk.
Let Ns,klocal.
be the number of occurrences of apossible local context word k with particular focusword lemma-PoS combination and with a particularsense s. Let Nklocal be the number of occurrencesof a possible local context keyword k with a partic-ular focus word-PoS combination regardless of itssense.
If we also take into account the frequency ofa possible keyword k in the complete training corpus(Nkcorpus), we get:P (s|k) =Ns,klocalNklocal(1Nkcorpus) (1)Hoste et al(2002) select a keyword k for inclu-sion in the bag-of-words representation if that key-word occurs more than T1 times in that sense s, andif P (s|k) ?
T2.
Both T1 and T2 are predefinedthresholds, which by default were set to 3 and 0.001respectively.
In addition, WSD2 and its predecessorWSD1 contain an extra parameter which can be en-abled to automatically adjust the T1 threshold whenit yields too many or too few keywords.
The selec-tion of bag-of-word features is computed prior to theextraction of the training instances, as this informa-tion is a prerequisite for the successful generation ofboth training and test instances.3 Feature and HyperparameterOptimisationThe size of the local context, the inclusion of globalcontext features, and the inclusion of syntactic fea-tures are all features that can be selected, changed,or disabled, allowing for a variety of combinationsto be tested.
In addition, each word expert is a k-nearest neighbour classifier that can take on manyhyperparameters beyond k. In the present study weperformed both optimisations for all word experts,but the optimisations were performed independentlyto reduce complexity: we optimised classifier hyper-parameters on the basis of the training examples ex-tracted from our parallel corpus, producing optimalaccuracy on each word-expert.
We optimised fea-ture selection on the basis of the trial data providedfor the task.
As has been argued before (Hoste et al2002), the joint search space of feature selection andhyperparameters is prohibitively large.
Our currentsetup runs the risk of finding hyperparameters thatare not optimal for the feature selection in the sec-ond optimisation step.
Our final results indeed showthat only feature selection produced improved re-sults.
We choose the feature selection with the high-184Figure 1: Average accuracy for different local contextsizesest score on the trial set, for each of the nouns andseparately for both evaluation metrics in the task.To optimise the choice of hyperparameters perword expert, a heuristic parameter search algo-rithm (van den Bosch, 2004)1 was used that imple-ments wrapped progressive sampling using cross-validation: it performs a large number of experi-ments with many hyperparameter setting combina-tions on small samples of training data, and thenprogressively zooms in on combinations estimatedto perform well with larger samples of the trainingdata.
As a control run we also trained word expertswith default hyperparameters, i.e.
with k = 1 andwith all other hyperparameters at their default val-ues as specified in the TiMBL implementation.4 Experiments & ResultsTo assess the accuracy of a certain configuration ofour system as a whole, we take the average over allword experts.
An initial experiment on the trial dataexplores the impact of different context sizes, withhyperparameter optimisation on the classifiers.
Theresults, shown in Figure 1, clearly indicate that onaverage the classifiers perform best with a local con-text of just one word to the left and one to the right ofthe word to be disambiguated.
Larger context sizeshave a negative impact on average accuracy.
Thesetests include hyperparameter optimisation, but thesame trend shows without.1http://ilk.uvt.nl/paramsearch/BEST ES FR IT NL DEbaseline 19.65 21.23 15.17 15.75 13.16plain 21.76 23.89 20.10 18.47 16.25+lem (c1l) 21.88 23.93 19.90 18.61 16.43+pos 22.09 23.91 19.95 18.02 15.37lem+pos 22.12 23.61 19.82 18.18 15.48glob.context 20.57 23.34 17.76 17.06 16.05OUT-OF-5 ES FR IT NL DEbaseline 48.34 45.99 34.51 38.59 32.90plain 49.81 50.91 42.30 41.74 36.86+lem (c1l) 49.91 50.65 42.41 41.83 36.45+pos 47.86 49.72 41.91 41.31 35.93lem+pos 47.90 49.75 41.49 41.31 35.80glob.ccontext 48.09 49.68 40.87 37.70 34.47Table 1: Feature exploration on the trial dataBEST ES FR IT NL DEc1lN 22.60 24.09 19.87 18.70 16.43c1l 21.88 23.93 19.90 18.61 16.43var 23.79 25.66 21.65 20.19 19.06varN 23.90 25.65 21.52 19.92 18.96OUT-OF-5 ES FR IT NL DEc1lN 50.14 50.98 42.92 42.08 36.45c1l 49.91 50.65 42.41 41.83 36.45var 51.95 53.66 45.59 44.66 39.81varN 52.91 53.61 45.92 44.32 39.40Table 2: Results on the trial dataWe submitted three configurations of our systemto the shared task, the maximum number of runs.Adding lemma features to the local context win-dow of three words proves beneficial in general, asshown in Table 1.
This is therefore the first configu-ration we submitted (c1l).
As second configuration(c1lN) we submitted the same configuration with-out parameter optimisation on the classifiers.
Notethat neither of these include global context features.The third configuration (var) we submitted in-cludes feature selection, and selects per word ex-pert the configuration that has the highest score onthe trial data, and thus tests all kinds of configura-tions.
Note that hyperparameter optimisation is alsoenabled for this configuration.
Due to the featureselection on the trial data, we by definition obtainthe highest scores on this trial data, but this carriesthe risk of overfitting.
Results on the trial data areshown in Table 2.The hyperparameter optimisation on classifier ac-curacy has a slightly negative impact, suggestingoverfitting on the training data.
Therefore a fourthconfiguration (varN) was tried later to indepen-185dently assess the idea of feature selection, withouthyperparameter optimisation on the classifiers.
Thisproves to be a good idea.
However, the fourth con-figuration was not yet available for the actual com-petition.
This incidentally would have had no impacton the final ranking between competitors.
When werun these systems on the actual test data of the sharedtask, we obtain the results in Table 3.
The best scoreamongst the other competitors is mentioned in thelast row for reference, this is the HLTDI team (Rud-nick et al 2013) for all but Best-Spanish, whichgoes to the NRC contribution (Carpuat, 2013).BEST ES FR IT NL DEbaseline 23.23 25.74 20.21 20.66 17.42c1l 28.40 29.88 25.43 23.14 20.70c1lN 28.65 30.11 25.66 23.61 20.82var 23.3 25.89 20.38 17.17 16.2varN 29.05 30.15 24.90 23.57 21.98best.comp 32.16 28.23 24.62 22.36 19.92OUT-OF-5 ES FR IT NL DEbaseline 53.07 51.36 42.63 43.59 38.86c1l 58.23 59.07 52.22 47.83 43.17c1lN 57.62 59.80 52.73 47.62 43.24var 55.70 59.19 51.18 46.85 41.46varN 58.61 59.26 50.89 50.42 43.34best.comp 61.69 58.20 53.57 46.55 43.66Table 3: Results on the test setA major factor in this task is the accuracy of lem-matisation, and to lesser extent of PoS tagging.
Weconducted additional experiments on German andFrench without lemmatisation, tested on the trialdata.
Results immediately fell below baseline.Another main factor is the quality of the wordalignments, and the degree to which the found wordalignments correspond with the translations the hu-man annotators could choose from in preparing thegold standard.
An idea we tested is, instead of rely-ing on the mere intersection of word alignments, touse a phrase-translation table generated by and forthe Statistical Machine Translation system Moses(Koehn et al 2007), which uses the grow-diag-finalheuristic to extract phrase pairs.
This results in morephrases, and whilst this is a good idea for MT, inthe current task it has a detrimental effect, as it cre-ates too many translation options and we do not havean MT decoder to discard ineffective options in thistask.
The grow-diag-final heuristic incorporates un-aligned words to the end of a translation in the trans-lation option, a bad idea for CLWSD.5 ConclusionIn this study we have taken parameter optimisationone step further compared to our previous research(van Gompel, 2010), namely by selecting system pa-rameters per word expert from the best configura-tions on the trial data.
Optimising the hyperparam-eter of the classifiers on the training data proves tohave a slightly negative effect, especially when com-bined with the selection of features.
This is likelydue to the fact that feature selection was performedafter hyperparameter optimisation, causing certainoptimisations to be rendered ineffective.We can furthermore uphold the conclusion fromprevious research that including lemma features isgenerally a good idea.
As to the number of localcontext features, we observed that a context size ofone feature to the left, and one to the right, has thebest overall average accuracy.
Eventually, due toour feature selection without hyperparameter opti-misation on the classifier not being available yet atthe time of submission, our simplest system c1lNemerged as best in the contest.When asked to predict the best translation(s), oursystem comes out on top for four out of five lan-guages; only for Spanish we are surpassed by twocompetitors.
Our out-of-five predictions win for twoout of five languages, and are fairly close the the bestcompetitor for the others, except again for Spanish.We assumed independence between hyperparam-eter optimisation and feature selection, where theformer was conducted using cross-validation on thetraining data rather than on the development set.
Asthis independence assumption is a mere simplifi-cation to reduce algorithmic complexity, future re-search could focus on a more integrated approachand test hyperparameter optimisation of the classi-fiers on the trial set which may produce better scores.The WSD2 system is available as open-source un-der the GNU Public License v3.
It is implementedin Python (van Rossum, 2006) and can be obtainedfrom http://github.com/proycon/wsd22.
The experi-mental data and results are included in the git repos-itory as well.2git commit f10e796141003d8a2fbaf8c463588a6d7380c05erepresents a fair state of the system at the time of submission186ReferencesJ.
Atserias, B. Casas, E. Comelles, M. Gonzlez, L. Padro?,and M. Padro?.
2006.
FreeLing 1.3: Syntactic and se-mantic services in an open-source NLP library .
InProceedings of the Fifth International Conference onLanguage Resources and Evaluation (LREC 2006),Genoa, Italy.
ELRA.M.
Carpuat.
2013.
NRC: A Machine Translation Ap-proach to Cross-Lingual Word Sense Disambiguation(semeval-2013 task 10).
In Proceedings of the 7thInternational Workshop on Semantic Evaluation (Se-mEval 2013), in conjunction with the Second JointConference on Lexical and Computational Semantics.W.
Daelemans, J. Zavrel, K. Van der Sloot, and A. Vanden Bosch.
2009.
TiMBL: Tilburg memory basedlearner, version 6.2, reference guide.
Technical ReportILK 09-01, ILK Research Group, Tilburg University.V.
Hoste, I. Hendrickx, W. Daelemans, and A.
Van denBosch.
2002.
Parameter optimization for machinelearning of word sense disambiguation.
Natural Lan-guage Engineering, 8(4):311?325.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,and E. Herbst.
2007.
Moses: Open source toolkitfor statistical machine translation.
In Proceedings ofthe 45th Annual Meeting of the Association for Com-putational Linguistics Companion Volume Proceed-ings of the Demo and Poster Sessions, pages 177?180,Prague, Czech Republic, June.
Association for Com-putational Linguistics.P.
Koehn.
2005.
Europarl: A parallel corpus for sta-tistical machine translation.
In In Proceedings of theMachine Translation Summit X ([MT]?05)., pages 79?86.E.
Lefever and V. Hoste.
2010.
Semeval-2010 task 3:Cross-lingual word sense disambiguation.
In Proceed-ings of the 5th International Workshop on SemanticEvaluation, SemEval ?10, pages 15?20, Stroudsburg,PA, USA.
Association for Computational Linguistics.E.
Lefever and V. Hoste.
2013.
SemEval-2013 Task 10:Cross-Lingual Word Sense Disambiguation.
In Pro-ceedings of the 7th International Workshop on Seman-tic Evaluation (SemEval 2013), in conjunction withthe Second Joint Conference on Lexical and Compu-tational Semantics.R.
Mihalcea, R. Sinha, and D. McCarthy.
2010.
Semeval2010 task 2: Cross-lingual lexical substitution.
In Pro-ceedings of the 5th International Workshop on Seman-tic Evaluations (SemEval-2010), Uppsala, Sweden.H.
Tou Ng and H. Beng Lee.
1996.
Integrating multipleknowledge sources to disambiguate word sense: Anexemplar-based approach.
In ACL, pages 40?47.F.J.
Och and H. Ney.
2000.
Giza++: Training of sta-tistical translation models.
Technical report, RWTHAachen, University of Technology.A.
Rudnick, C. Liu, and M. Gasser.
2013.
HLTDI: CL-WSD using Markov Random Fields for SemEval-2013Task 10.
In Proceedings of the 7th International Work-shop on Semantic Evaluation (SemEval 2013), in con-junction with the Second Joint Conference on Lexicaland Computational Semantics.H Schmid.
1994.
Probabilistic part-of-speech taggingusing decision trees.A.
van den Bosch, G.J.
Busser, S. Canisius, andW.
Daelemans.
2007.
An efficient memory-basedmorpho-syntactic tagger and parser for Dutch.
InP.
Dirix, I. Schuurman, V. Vandeghinste, , and F. VanEynde, editors, Computational Linguistics in theNetherlands: Selected Papers from the SeventeenthCLIN Meeting, pages 99?114, Leuven, Belgium.A.
van den Bosch.
2004.
Wrapped progressive sam-pling search for optimizing learning algorithm param-eters.
In R. Verbrugge, N. Taatgen, and L. Schomaker,editors, Proceedings of the Sixteenth Belgian-DutchConference on Artificial Intelligence, pages 219?226,Groningen, The Netherlands.M.
van Gompel, K. van der Sloot, and A. van den Bosch.2012.
Ucto: Unicode tokeniser.
version 0.5.3.
Refer-ence Guide.
Technical Report ILK 12-05, ILK Re-search Group, Tilburg University.M.
van Gompel.
2010.
UvT-WSD1: A cross-lingualword sense disambiguation system.
In SemEval ?10:Proceedings of the 5th International Workshop on Se-mantic Evaluation, pages 238?241, Morristown, NJ,USA.
Association for Computational Linguistics.G.
van Rossum.
2006.
Python reference manual, release2.5.
Technical report, Amsterdam, The Netherlands,The Netherlands.187
