Functional CenteringGrounding Referential Coherencein Information StructureMichae l  Strube*University of PennsylvaniaUdo  HahntFreiburg UniversityConsidering empirical evidence from a free-word-order language (German) we propose a revi-sion of the principles guiding the ordering of discourse ntities in the forward-looking center listwithin the centering model.
We claim that grammatical role criteria should be replaced by criteriathat reflect he functional information structure of the utterances.
These new criteria are basedon the distinction between hearer-old and hearer-new discourse ntities.
We demonstrate hatsuch a functional model of centering can be successfully applied to the analysis of several formsof referential text phenomena, viz.
pronominal, nominal, and functional anaphora.
Our method-ological and empirical claims are substantiated by two evaluation studies.
In the first one, wecompare success rates for the resolution of pronominal naphora that result from a grammatical-role-driven centering algorithm and from a functional centering algorithm.
The second studydeals with a new cost-based evaluation methodology for the assessment ofcentering data, onewhich can be directly derived from and justified by the cognitive load premises of the centeringmodel.1.
IntroductionThe problem of establishing referential coherence in discourse can be rephrased as theproblem of determining the proper antecedent of a given anaphoric expression i  thecurrent or the preceding utterance(s) and the rendering of both as referentially iden-tical (coreferential).
This task can be approached in a very principled way by statinggeneral constraints on the grammatical compatibility of the expressions involved (e.g.,Haddock 1987; Alshawi 1992).
Linguists have devoted a lot of effort to identifyingconclusive syntactic and semantic riteria to reach this goal, e.g., for intrasententialanaphora within the binding theory part of the theory of Government and Binding(Chomsky 1981), or for intersentential anaphora within the context of the DiscourseRepresentation Theory (Kamp and Reyle 1993).Unfortunately, these frameworks fail to uniquely determine anaphoric antecedentsin a variety of cases.
As a consequence, referentially ambiguous interpretations haveto be dealt with in those cases in which several alternatives fulfill all the requiredsyntactic and semantic onstraints.
It seems that syntactic and semantic riteria con-stitute only necessary but by no means sufficient conditions for identifying the validantecedent among several possible candidates.
Hence, one is left with a preferentialchoice problem that falls outside of the scope of those strict grammaticality constraintsrelating to the level of syntax or semantics only.
Its solution requires considering pat-?
Institute for Research in Cognitive Science, 3401 Walnut Street, Suite 400A, Philadelphia, PA 19104, USAt Computational Linguistics Group, Text Understanding Lab, Werthmannplatz 1, 79085 Freiburg,Germany(~) 1999 Association for Computational LinguisticsComputational Linguistics Volume 25, Number 3terns of language use and, thus, introduces the level of discourse context and furtherpragmatic factors as a complementary description level.Computational linguists have recognized the need to account for referential mbi-guities in discourse and have developed various theories centered around the notion ofdiscourse focus (Grosz 1977; Sidner 1983).
In a seminal paper, Grosz and Sidner (1986)wrapped up the results of their research and formulated a model in which three levelsof discourse coherence are distinguished--attention, intention, and discourse segmentstructure.
While this paper gives a comprehensive picture of a complex, yet not ex-plicitly spelled-out theory of discourse coherence, the centering model (Grosz, Joshi,and Weinstein, 1983, 1995) marked a major step in clarifying the relationship betweenattentional states and (local) discourse segment structure.
More precisely, the centeringmodel accounts for the interactions between local coherence and preferential choices ofreferring expressions.
It relates differences in coherence (in part) to varying demandson inferences as required by different types of referring expressions, given a particularattentional state of the hearer in a discourse setting (Grosz, Joshi, and Weinstein 1995,204-205).
The claim is made then that the lower the inference load put on the hearer,the more coherent the underlying discourse appears.The centering model as formulated by Grosz, Joshi, and Weinstein (1995) refinesthe structure of "centers" of discourse, which are conceived as the representationaldevice for the attentional state at the local level of discourse.
They distinguish twobasic types of centers, which can be assigned to each utterance Ui--a single backward-looking center, Cb(Ui), and a partially ordered set of discourse ntities, the forward-looking centers, Cf(Ui).
The ordering on Cf is relevant for determining the Cb.
Itcan be viewed as a salience ranking that reflects the assumption that the higher theranking of a discourse ntity in Cf, the more likely it will be mentioned again in theimmediately following utterance.
Thus, given an adequate ordering of the discourseentities in Cf, the costs of computations necessary to establish local coherence areminimized.Given that the ordering on the Cf list is crucial for determining the Cb, it is nosurprise that there has been much discussion among researchers about the rankingcriteria appropriate for different languages.
In fact, Walker, Iida, and Cote (1994) hy-pothesize that the Cf ranking criteria are the only language-dependent fac ors withinthe centering model.
Though evidence for many additional criteria for the Cf rankinghave been brought forward in the literature, to some extent consensus has emergedthat grammatical roles play a major role in making ranking decisions (e.g., whetherthe referential expression appears as the grammatical subject, direct object, or indirectobject of an utterance).
Our own work on the centering model 1(Strube and Hahn 1996;Hahn and Strube 1996) brings in evidence from German, a free-word-order languagein which grammatical role information is far less predictive of the organization ofcenters than for fixed-word-order languages such as English.
In establishing properreferential relations, we found the functional information structure of the utterancesto be much more relevant.
By this we mean indicators of whether or not a discourseentity in the current utterance refers to another discourse ntity already introducedby previous utterances in the discourse.
Borrowing terminology from Prince (1981,1992), an entity that does refer to another discourse ntity already introduced is calleddiscourse-old or hearer-old, while an entity that does not refer to another discourseentity is called discourse-new or hearer-new.1 This article is an extended and revised version of our contribution to the 1996 Annual Meeting of theAssociation for Computational Linguistics (Strube and Hahn 1996).
It contains additional material fromthe doctoral thesis of the first author (Strube 1996a).310Strube and Hahn Functional CenteringBased on evidence from empirical studies in which we considered German as wellas English texts from different domains and genres, we make three contributions tothe centering approach.
The first, the introduction of functional notions of informationstructure into the centering model, is purely methodological in nature and concerns thecentering approach as a theory of local coherence.
The second deals with an empiricalissue, in that we demonstrate how a functional model of centering can be success-fully applied to the analysis of different forms of anaphoric text phenomena, namelypronominal, nominal, and functional anaphora.
Finally, we propose a new evaluationmethodology for centering data in terms of a cost-based evaluation approach that canbe directly derived from and justified by the cognitive load premises of the centeringmodel.At the methodological level, we develop arguments that (at least for some free-word-order languages) grammatical role criteria should be replaced by functionalrole criteria, since they seem to more adequately account for the ordering of discourseentities in the Cf list.
In Section 4, we elaborate on particular information structurecriteria underlying such a functional center ordering.
We also make a second, moregeneral methodological claim for which we have gathered some preliminary, thoughstill not conclusive vidence.
Based on a reevaluation of centering analyses of somechallenging language data that can be found in the literature on centering, we willargue that exchanging rammatical for functional criteria might also be a reason-able strategy for fixed-word-order languages.
What makes this proposal so attractiveis the obvious gain in the generality of the model--given a functional framework,fixed- and free-word-order languages might be accounted for by the same orderingprinciples.The second major contribution of this paper is related to the unified treatment ofdifferent ext coherence phenomena.
It consists of an equally balanced treatment ofintersentential (pro)nominal anaphora nd inferables (also called functional, bridging,or partial anaphora).
The latter phenomenon (cf.
the examples in the next section andthe in-depth treatment in Hahn, Markert, and Strube \[1996\]) is usually only sketchilydealt with in the centering literature, e.g., by asserting that the entity in question "is re-alized but not directly realized" (Grosz, Joshi, and Weinstein 1995, 217).
Furthermore,the distinction between these two kinds of realization is not part of the centeringmechanisms but delegated to the underlying semantic theory.
We will develop argu-ments for how to discern inferable discourse ntities and relate them properly to theirantecedent at the center level.
The ordering constraints we supply account for all ofthe types of anaphora mentioned above, including (pro)nominal anaphora (Strube andHahn 1995; Hahn and Strube 1996).
This claim will be validated by a substantial bodyof empirical data in Section 5.Our third contribution relates to the way the results of centering-based anaphoraresolution are usually evaluated.
Basically, we argue that rather than counting reso-lution rates for anaphora or comparing isolated transition types holding among headpositions in the center lists--preferred transition types stand for a high degree of localcoherence, while less preferred ones signal that the underlying discourse might lackcoherence--one should consider adjacent transition pairs and annotate such pairs withthe processing costs they incur.
This way, we define a dual theory-internal metric ofinference load by distinguishing between "cheap" and "expensive" transition types.Based on this distinction, some transition types receiving bad marks in isolation areranked "cheap" when they occur in the appropriate context, and vice versa.The article is organized as follows: In Section 2, we introduce the different ypes ofanaphora we consider subsequently, viz.
pronominal, nominal, and functional anaphora.We then turn to the proposed modification of the centering model.
After a brief in-311Computational Linguistics Volume 25, Number 3troduction into what we call the "grammatical" centering model (actually, a recap ofGrosz, Joshi, and Weinstein \[1995\]) in Section 3, we turn in Section 4 to our approach,the functional model of centering.
In Section 5, we present the methodological frame-work and the empirical data from two evaluation studies we carried out.
In Section 6,we relate our work to alternative approaches dealing with local text coherence.
InSection 7, we discuss ome remaining unsolved problems.2.
Types of Anaphoric ExpressionsIn this paper, we consider anaphora s a textual phenomenon only, and deal withanaphoric relations that hold between adjacent utterances (intersentential anaphora).
2Text phenomena are a challenging issue for the design of a text parser for any text-understanding system, since recognition facilities that are imperfect or altogether lack-ing result in referentially incomplete, invalid, or incohesive text knowledge represen-tation structures (Hahn, Romacker, and Schulz 1999).
Incomplete knowledge structuresemerge when references to already established discourse ntities are simply not rec-ognized, as in the case of conceptually neutral pronominal anaphora (e.g., er, 'it,' inexample (ld) co-specifying with 316LT, a particular notebook introduced in exam-ple (la)).
Invalid knowledge structures emerge when each entity that has a differentdenotation at the text surface is also treated as a formally distinct item at the levelof text knowledge representation, although they all refer literally to the same entity.These false referential descriptions result from unresolved nominal anaphora (e.g.,Rechner, 'computer' in example (lc) co-specifies with 316LT in (la)).
Finally, incohesiveor artificially fragmented knowledge structures emerge when entities that are linkedby various conceptual relations at the knowledge level occur in a text such that animplicit reference to these relations can be made without he need for explicit signal-ing at the text surface level.
Corresponding referential relations cannot be establishedat the text representation level, since these inferables remain unsolved (such as therelation between Akkus, 'rechargeable battery cell', and 316LT in examples (lb) and(la), respectively.
3 The linking conceptual relation between these two discourse le-ments has to be inferred in order to make it explicit at the level of text knowledgerepresentation structures (for an early statement of this idea in terms of "bridging"inferences, ee Clark \[1975\]).Note an interesting asymmetric relationship between these three types of anaphora.Pronominal anaphora re constrained by morphosyntactic and grammatical greementcriteria between the pronoun and the antecedent, 4 and no conceptual constraints ap-ply.
Nominal anaphora re only constrained by number compatibility between theanaphoric expression and the antecedent, while at the conceptual level the anaphoricexpression is related to its antecedent in terms of a conceptual generalization relation.Finally, no grammatical constraints apply to inferables, while conceptual constraintstypically require a nongeneralization relation (e.g., part-whole) to hold between theinferable and its antecedent.
Of course, contextual conceptual constraints are intro-duced for both nominal and pronominal anaphora by sortal requirements set up, e.g.,by the case roles of the main verb.2 We have also considered the role of anaphora within sentences.
The d-binding criterion we havedeveloped for resolving intrasentential anaphora is based on dependency grammar notions describedin more detail n Strube and Hahn (1995).3 Note that Reserve-Batteriepack in Example (la) and Akkus in (lb) denote conceptually different discourseentities that cannot be coindexed.4 See Jaeggli (1986) for special cases where this criterion isoverruled.312Strube and Hahn Functional CenteringLet us illustrate these different ypes of phenomena by considering the followingtext fragment:Example 1a.
Ein Reserve-Batteriepack versorgt den 316LT ca.
2 Minuten mit Strom.\[A reserve battery pack\]nom - supplies - the \[316LT\]acc - forapproximately 2 minutes - with power.The 316LT is supplied with power by a reserve battery pack forapproximately 2 minutes.b.
Der Status des Akkus wird dem Anwender angezeigt.\[The status - \[of the rechargeable battery cell\]gen\]nom -- is -- \[to the user\]aat -signalled.The status of the rechargeable battery cell is signalled to the user.c.
Ca.
30 Minuten vor der Entleerung beginnt der Rechner 5 Sekunden zupiepen.Approximately 30 minutes - before discharge - starts - \[thecomputer\]nmam c - for 5 seconds - to beep.Approximately 30 minutes before discharge the computer beeps for 5seconds.d.
5 Minuten bevor er sich ausschaltet, f/ingt die Low-Battery-LED an zublinken.5 minutes - before - \[it\]n m~sc - itself - turns off - begins - \[thelow-battery-LED\],om - to flash.5 minutes before it turns off, the low-battery-LED begins to flash.Common to all the varieties of anaphora we discuss is the search for the properantecedent in previous utterances, the correct determination of which is considered tobe the task of the centering mechanism.
The kinds of anaphora we treat can be distin-guished, however, in terms of the criteria being evaluated for referentiality.
In the caseof inferables, the missing conceptual link must be inferred in order to establish localcoherence between the utterances involved.
In the surface form of utterance (lb) the in-formation that Akkus,  'rechargeable battery cell', links up with 316LT is missing, while,due to obvious conceptual constraints, it cannot link up with Reserve-Batteriepack, forexample.
The underlying relation can only be made explicit if conceptual knowledgeabout the domain, viz.
the relation PART-OF between the concepts RECHARGEBAT-TERYCELL and 316LT, is available (see Hahn, Markert, and Strube \[1996\] for a detailedtreatment of the resolution of inferables).
In the case of nominal anaphors, a conceptualspecialization relation has to be determined between the specific antecedent and themore general anaphoric expression, for example, between 316LT and Rechner, 'com-puter', in (la) and (lc), respectively.
Finally, the resolution of pronominal anaphorsneed not take conceptual constraints into account at all, but is restricted to gram-matical constraints, as illustrated by the masculine gender of Rechner, 'computermasc',(co-specifying with 316LTmasc) and er 'it'masc, in (lc) and (ld), respectively.Certainly, the types of phenomena we discuss cover only a limited range ofanaphora.
In particular, we leave out the whole range of quantificational studies onanaphora (in particular, the "hard" issues related to generalized quantifiers), deicticphenomena, etc., which significantly complicate matters.
We return to these unresolvedissues in Section 7.313Computational Linguistics Volume 25, Number 3Table 1Cf ranking by grammatical roles.subject > object(s) > other(s)Table 2Transition types.Cb(U~) = Cb(U~_ 0 Cb(Ui) # Cb(U~_ 0Cb(Ui) = Cp(Ui) CONTINUECb(U~) :/: Cp(Ui) RETAINSHIFT3.
The Centering ModelThe centering model (Grosz, Joshi, and Weinstein 1983, 1995) is intended to describethe relationship between local coherence and the use of referring expressions.
Themodel requires two constructs, a single backward-looking center and a list of forward-looking centers, as well as a few rules and constraints that govern the interpretationof centers.
It is assumed that discourses are composed of constituent segments (Groszand Sidner 1986), each of which consists of a sequence of utterances.
Each utterance Uiin a given discourse segment DS is assigned a list of forward-looking centers, Cf(DS,Ui), and a unique backward-looking center, Cb(DS, Ui).
The forward-looking centersof Ui depend only on the discourse ntities that constitute the ith utterance; previousutterances provide no constraints on Cf(DS, Ui).
A ranking imposed on the elementsof the Cf reflects the assumption that the most highly ranked element of Cf(DS, Ui),the preferred center Cp(DS, Ui), will most likely be the Cb(DS, Ui+l).
The most highlyranked element of Cf(DS, Ui) that is finally realized in Ui+l (i.e., is associated with anexpression that has a valid interpretation i  the underlying semantic representation)is the actual Cb(DS, Ui+I).
Since in this paper we will not discuss the topics of globalcoherence and discourse macro segmentation (for recent reatments of these issues,see Hahn and Strube \[1997\] and Walker \[1998\]), we assume a priori that any centeringdata structure is assigned an utterance in a given discourse segment and simplify thenotation of centers to Cb(Ui) and Cf(Ui).Grosz, Joshi, and Weinstein (1995) state that the items in the Cf list have to beranked according to a number of factors including rammatical role, text position, andlexical semantics.
As far as their discussion of concrete English discourse phenomenais concerned, they nevertheless restrict heir ranking criteria to those solely based ongrammatical roles, which we repeat in Table 1.The centering model, in addition, defines transition relations across pairs of adja-cent utterances (Table 2).
These transitions differ from each other according to whetherbackward-looking centers of successive utterances are identical or not, and, if they areidentical, whether they match the most highly ranked element of the current forward-looking center list, the Cp(Ui), or not.Grosz, Joshi, and Weinstein (1995) also define two rules on center movement andrealization:314Strube and Hahn Functional Centeringclausestensedembedded same-levelinaccessible accessible, less salientI Idirect speech non-report complementsreported speech relative clausesFigure 1Kameyama's intrasentential centering categorization.untensedRule 1If any element of CJ:(Ui) is realized by a pronoun in Ui+l, then the Cb(Ui+l) must  berealized by a pronoun also.Rule 2Sequences of continuation are to be preferred over sequences of retaining; and se-quences of retaining are to be preferred over sequences of shifting.Rule 1 states that no element in an utterance can be realized by a pronoun unlessthe backward-looking center is realized by a pronoun, too.
This rule is intended tocapture one function of the use of pronominal anaphors--a pronoun in the Cb signalsto the hearer that the speaker is continuing to refer to the same discourse.
Rule 2should reflect he intuition that a pair of utterances that have the same theme is morecoherent than another pair of utterances with more than one theme.
The theory claims,above all, that to the extent hat a discourse adheres to these rules and constraints,its local coherence will increase and the inference load placed upon the hearer willdecrease.The basic unit for which the centering data structures are generated is the utter-ance U.
Since Grosz, Joshi, and Weinstein (1995) and Brennan, Friedman, and Pollard(1987) do not give a reasonable definition of utterance, we follow Kameyama's (1998)method for dividing a sentence into several center-updating units (Figure 1).
Her in-trasentential centering mechanisms operate at the clause level.
While tensed clausesare defined as utterances on their own, untensed clauses are processed with the mainclause so that the Cf list of the main clause contains the elements of the untensedembedded clause.
Kameyama further distinguishes, for tensed clauses, between se-quential and hierarchical centering.
Except for direct and reported speech (embed-ded and inaccessible to the superordinate level), nonreport complements, and relativeclauses (both embedded but accessible to the superordinate level; less salient han thehigher levels), all other types of tensed clauses build a chain of utterances atthe samelevel.3.1 A Centering Algorithm for Anaphora ResolutionThough the centering model was not originally intended to be used as a blueprintfor anaphora resolution, 5 several applications tackling this problem have made use of5 Aravind Joshi, personal communication.315Computational Linguistics Volume 25, Number 3Table 3Basic centering algorithm.1.
If a pronoun in Ui is encountered, test the elements of the Cf(Ui-1) in the given orderuntil an element under scrutiny satisfies all the required morphosyntactic, binding,and sortal criteria.
This element is chosen as the antecedent of the pronoun.2.
When utterance Ui is completely read, compute Cb(Ui) and generate Cf(Ui); rank theelements according to agreed-upon preference criteria (such as the ones from Table 1).the model, nevertheless.
One interpretation is due to Brennan, Friedman, and Pollard(1987) who utilize Rule 2 for computing preferences for antecedents of pronouns (seeSection 3.2).
In this section, we will specify a simple algorithm that uses the Cf listdirectly for providing preferences for the antecedents of pronouns.The algorithm (which we will refer to as the basic algorithm; Table 3) consists oftwo steps, which are triggered independently.We may illustrate this algorithm by referring to the text fragment in example (2): 6Example 2a.
The sentry was not dead.b.
He was, in fact, showing signs of reviving ...c. He was partially uniformed in a cavalry tunic.d.
Mike stripped this from him and donned it.e.
He tied and gagged the man .
.
.
.Table 4 gives the centering analysis for this text fragment using the algorithmfrom Table 3.
7 Since (2a) is the first sentence in this fragment, it has no Cb.
In (2b) ~ andin (2c) the discourse entity SENTRY is referred to by the personal pronoun he.
Sincewe assume a Cf ranking by grammatical roles in this example, SENTRY is rankedhighest in these sentences (the pronoun always appears in subject position).
In (2d),the discourse entity MIKE is introduced by a proper name in subject position.
Thepronoun him is resolved to the most highly ranked element of Cf(2c), namely SENTRY.Since Mike occupies the subject position, it is ranked higher in the Cf(2d) than SENTRY.Therefore the pronoun he in (2e) can be resolved correctly to MIKE.This example not only illustrates anaphora resolution using the basic algorithmfrom Table 3 but also incorporates the application of Rule 1 of the centering model.
(2d) contains the pronoun him, which is the Cb of this utterance.
In (2e), the Cb is alsorealized as a pronoun while SENTRY is realized by the definite noun phrase the man,which is allowed by Rule 1.3.2 The BFP AlgorithmThe centering algorithm described by Brennan, Friedman, and Pollard (1987, hence-forth BFP algorithm) interprets the centering model in a certain way and applies itto the resolution of pronouns.
The most obvious difference between Grosz, Joshi, and6 With slight simplifications taken from the Brown Corpus cn03.7 In the subsequent tables illustrating centering data, discourse ntities, anotion at the representationallevel, are denoted by SMALLCAPS and appear on the left side of the colon, while the correspondingsurface xpressions, at the level of linguistic data, appear on the right side of the colon.316Strube and Hahn Functional CenteringTable 4Analysis for the text fragment in Example 2 according tothe basic centering algorithm.
(2a) The sentry was not dead.Cb: -Cf: \[SENTRY: sentry\](2b) He was, in fact, showing signs of reviving ...Cb: SENTRY: heCf: \[SENTRY: he, SIGNS: signs\](2c) He was partially uniformed in a cavalry tunic.Cb: SENTRY: heCf: \[SENTRY: he, TUNIC: tunic\](2d) Mike stripped this from him and donned it.Cb: SENTRY: himCf: \[MIKE: Mike, TUNIC: this, it, SENTRY: him\](2e) He tied and gagged the man .
.
.
.Cb: MIKE: heCf: \[MIKE: he, SENTRY: the man\]Table 5Transition types according to BFP.Cb(Ui) = Cb(Ui_l) Cb(Ui) # Cb(Ui-1)OR Cb(Ui-1) undef.Cb(Ui) = Cp(Ui) CONTINUE SMOOTH-SHIFTCb(Ui) ~ Cp(Ui) RETAIN  ROUGH-SHIFTWeinstein (1983, 1995) and Brennan, Friedman, and Pollard (1987) is that the latteruse two SHIFT transitions instead of only one: SMOOTH-SHIFT 8 requires the Cb(Ui) toequal Cp(Ui), while ROUGH-SHIFT requires inequality (Table 5).
Brennan, Friedman,and Pollard (1987) also allow the Cb(Ui_I) to remain undefined.Brennan, Friedman, and Pollard (1987) extend the ordering constraints in Cf inthe following way: "We rank the items in Cf by obliqueness of grammatical relationsof the subcategorized functions of the main verb: that is, first the subject, object, andobject2, followed by other subcategorized functions, and finally, adjuncts."
(p. 156).
Inorder to apply the centering model to pronoun resolution, they use Rule 2 in makingpredictions for pronominal reference and redefine the rules as follows (quoting Walker,Iida, and Cote \[1994\]):Rule 1'If some element of Cf(Ui-1) is realized as a pronoun in Ui, then so is Cb(Ui).8 Brennan, Friedman, and Pollard (1987) call these transitions SHIFTING and SHIFTING-1.
The morefigurative names were introduced by Walker, Iida, and Cote (1994).317Computational Linguistics Volume 25, Number 3Table 6BFP-algorithm.1.
Generate possible Cb-Cf combinations.
In this step, all (plausible and implausible)assignments ofpronouns to elements of the previous Cf are computed.2.
Filter by constraints, e.g., contra-indexing, sortal predicates, centering rules andconstraints.
This way, possible antecedents are filtered out because of morphosyntactic,binding, and semantic riteria.
Also the realization of noun phrases in the currentutterance (e.g., realization as a pronoun vs. realization as a definite noun phrase orproper name) comes into play.3.
Rank by transition orderings.
This is the step, where the pragmatic onstraints ofcentering apply.
Basically, CONTINUE transitions are preferred, i.e., the antecedent of apronoun is more likely to turn up as the Cb of the previous utterance than any otherelement of the Cf.
In certain configurations, the algorithm includes a preference forparallelism in linguistic onstructions.Rule 2 ~Transition states are ordered.
CONTINUE is preferred to RETAIN is preferred to SMOOTH-SHIFT is preferred to ROUGH-SHIFT.Their algorithm (Table 6) consists of three basic steps (as described by Walker, Iida,and Cote \[1994\]).
9In order to illustrate this algorithm, we use example (2) from above and supply thecorresponding Cb/Cf data in Table 7.
Let us focus on the interpretation of utterance(2e) where the centering data diverges when one compares the basic and the BFPalgorithms.
After step 2 (filtering), the algorithm has produced two readings, whichare rated by the corresponding transitions in step 3.
Since SMOOTH-SHIFT is preferredover ROUGH-SHIFT, the pronoun he is resolved to MIKE, the highest-ranked element ofCf(2d).
Also, Rule 1 would be violated in the rejected reading.4.
Principles of Functional CenteringThe crucial point underlying functional centering is to relate the ranking of the forward-looking centers and the information structure of the corresponding utterances.
Hence,a proper correspondence relation between the basic centering data structures and therelevant functional notions has to be established and formally rephrased in termsof the centering model.
In this section, we first discuss two studies in which theinformation structure of utterances is already integrated into the centering model(Rambow 1993; Hoffman 1996, 1998).
Using these proposals as a point of depar-ture, we shall develop our own proposal--functional centering (Strube and Hahn1996).4.1 Integrating Information Structure and CenteringAs far as the centering model is concerned, the first account involving informationstructure criteria was given by Kameyama (1986) and further refined by Walker,Iida, and Cote (1994) in their study on the use of zero pronouns and topic mark-9 Walker, Iida, and Cote (1994) note that it is possible to improve the computational efficiency of thealgorithm by interleaving generating, filtering, and ranking steps; cf.
the version of the algorithmdescribed by Walker (1998).318Strube and Hahn Functional CenteringTable 7Centering analysis for the text fragment in example (2) according to theBFP algorithm.
(2a) The sentry was not dead.Cb: -Cf: \[SENTRY: sentry\](2b) He was, in fact, showing signs of reviving ...Cb: SENTRY: he CONTINUECf: \[SENTRY: he, SIGNS: signs\](2c) He was partially uniformed in a cavalry tunic.Cb: SENTRY: he CONTINUECf: \[SENTRY: he, TUNIC: tunic\](2d) Mike stripped this from him and donned it.Cb: SENTRY: him RETAINCf: \[MIKE: Mike, TUNIC: this, it, SENTRY: him\](2e) He tied and gagged the man .
.
.
.Cb: MIKE: he SMOOTH-SHIFTCf: \[MIKE: he, SENTRY: the man\]Cb: .
.
.
.
.
.
?vJ.Jr,.r~.
th6 iii~iz KGUGH-SHIFTLOm,~N J - l~t, l .
It?~ l &Vl lh f J .
~ll.G l l t l ,?
l l , \ ]ers in Japanese.
This led them to augment the grammatical ranking conditions for theforward-looking centers by additional functional notions.A deeper consideration of information structure principles and their relation tothe centering model has been proposed in two studies concerned with the analysis ofGerman and Turkish discourse.
Rambow (1993) was the first to apply the centeringmethodology to German, aiming at the description of information structure aspectsunderlying scrambling and topicalization.
As a side effect, he used centering to definethe utterance's theme and rheme in the sense of the functional sentence perspective(FSP) (Firbas 1974).
Viewed from this perspective, the theme/rheme-hierarchy of utter-ance Ui is determined by the Cf(Ui_l).
Elements of Ui that are contained in Cf(Ui-1) areless rhematic than those not contained in Cf(Ui-1).
He then concludes that the Cb(Ui)must be the theme of the current utterance.
Rambow does not exploit he informationstructure of utterances to determine the Cf ranking but formulates it on the basis oflinear textual precedence among the relevant discourse ntities.In order to analyze Turkish texts, Hoffman (1996, 1998) distinguishes betweenthe information structure of utterances and centering, since both constructs are as-signed different functions for text understanding.
A hearer exploits the informationstructure of an utterance to update his discourse model, and he applies the center-ing constraints in order to connect he current utterance to the previous discourse.Hoffman describes the information structure of an utterance in terms of topic (theme)and comment (rheme).
The comment is split again into focus and (back)ground (seealso Vallduvi \[1990\] and Vallduvf and Engdahl \[1996\]).
Based on previous work aboutTurkish, Hoffman argues that, in this language, the sentence-initial position corre-sponds to the topic, the position that immediately precedes the verb yields the focus,and the remainder of the sentence is to be considered the (back)ground.
Further-more, Hoffman relates this notion of information structure of utterances to center-ing, claiming that the topic corresponds to the Cb in most cases--with the excep-tion of segment-initial utterances, which do not have a Cb.
Hoffman does not sayanything about the relation between information structure and the ranking of the319Computational Linguistics Volume 25, Number 3Cf list.
In her approach, this ranking is achieved by thematic roles (see also Turan\[1998\]).Both Rambow (1983) as well as Hoffman (1996, 1998) argue for a correlation be-tween the information structure of utterances and centering.
Both of them find a cor-respondence between the Cb and the theme or the topic of an utterance.
They refrain,however, from establishing a strong link between the information structure and center-ing as we suggest in our model, one that mirrors the influence of information structurein the way the forward-looking centers are actually ranked.4.2 Functional CenteringGrosz, Joshi, and Weinstein (1995) admit that several factors may have an influenceon the ranking of the Cf but limit their exposition to the exploitation of grammaticalroles only.
We diverge from this proposal and claim that, at least for languages withrelatively free word order (such as German), the functional information structure ofthe utterance is crucial for the ranking of discourse ntities in the Cf list.
Originally,in Strube and Hahn (1996), we defined the Cf ranking criteria in terms of context-boundedness.
In this paper, we redefine the functional Cf ranking criteria by makingreference to Prince's work on the assumed familiarity of discourse entities (Prince1981) and information status (Prince 1992).
The term context-bound in Strube andHahn (1996) corresponds to the term evoked used by Prince.
I?We briefly list the major claims of our approach to centering.
In the followingsections, we elaborate on these claims, in particular the ranking of the forward-lookingcenters.?
The elements of the Cf list are ordered according to their informationstatus.
Hearer-old iscourse ntities are ranked higher than hearer-newdiscourse ntities.
The order of the elements of the Cf list for Ui providesthe preference for the interpretation of anaphoric expressions in Ui+l.?
The first element of the Cf(Ui), the preferred center, Cp(Ui), is thediscourse ntity the utterance Ui is "about."
In other words, the Cp is thecenter of attention.In contrast to the BFP algorithm, the model of functional centering requires neithera backward-looking center, nor transitions, nor transition ranking criteria for anaphoraresolution.
For text interpretation, at least, functional centering also makes no com-mitments to further constraints and rules.4.3 Cf Ranking Criteria in Functional CenteringIn this section, we introduce the functional Cf ranking criteria.
We first describe a basicversion, which is valid for a wide range of text genres in which pronominal referenceis the predominant text phenomenon.
This is the type of discourse to which centeringwas mainly applied in previous approaches (see, for example, Walker's \[1989\] or DiEugenio's \[1998\] test sets).
We then describe the extended version of the functionalCf ranking constraints.
The two versions differ with respect o the incorporation of (asubset of) inferables in the second version and, hence, with respect o the requirements10 In Strube and Hahn (1996), we assumed that the information status of a discourse ntity has the mainimpact on its salience.
In particular, evoked iscourse ntities were ranked higher in the Cf list thanbrand-new discourse ntities (using Prince's terminology).
We also restricted the category of the mostsalient discourse ntities to evoked (i.e., context-bound) discourse ntities.
In this article, we extend thiscategory to hearer-old discourse ntities, which includes, besides evoked iscourse ntities, unusedones (again, referring to Prince's terminology).320Strube and Hahn Functional Centering--<NEWFigure 2Information status and familiarity (basic version).relating to the availability of world knowledge, which is needed to properly accountfor inferables.
The extended version assumes a detailed treatment of a particular sub-set of inferables, so-called functional anaphora (in Hahn, Markert, and Strube \[1996\],functional anaphora are referred to as textual ellipses).
We claim that the extendedversion of ranking constraints is necessary to analyze texts from certain genres, e.g.,texts from technical or medical domains.
In these areas, pronouns are used rather in-frequently, while functional anaphors are the major text phenomena to achieve localcoherence.4.3.1 Basic Cf Ranking.
Usually, the Cf ranking is represented by an ordering relationon a single set of elements, e.g., grammatical relations (as in Table 1).
We use a layeredrepresentation for our criteria.
For the basic Cf ranking criteria, we distinguish betweentwo different sets of expressions, hearer-old iscourse ntities in Ui (OLD) and hearer-new discourse ntities in Ui (NEW).
These sets can be further split into the elementsof Prince's (1981, 245) familiarity scale.
The set of hearer-old iscourse ntities (OLD)consists of evoked (E) and unused (U) discourse ntities, while the set of hearer-newdiscourse ntities (NEW) consists of brand-new (BN) discourse ntities.
For the basicCf ranking criteria, it is sufficient o assign inferable (I), containing inferable (IC),and anchored brand-new (BN A) discourse ntities to the set of hearer-new discourseentities (NEW).
n See Figure 2 for an illustration of Prince's familiarity scale and itsrelation to the two sets.
Note that the elements of each set are indistinguishable withrespect o their information status.
Evoked and unused discourse ntities, for example,have the same information status because they belong to the set of hearer-old iscourseentities.
So the basic Cf ranking in Figure 2 boils down to the preference of OLDdiscourse ntities over NEW ones.For an operationalization f Prince's terms, we state that evoked discourse en-tities are simply cospecifying (resolved anaphoric) expressions, i.e., pronominal andnominal anaphora, relative pronouns, previously mentioned proper names, etc.
Un-used discourse ntities are proper names and titles.
In texts, brand-new proper namesare usually accompanied by a relative clause or an appositive that relates them tothe hearer's knowledge.
The corresponding discourse ntity is evoked only after thiselaboration.
Whenever these linguistic devices are missing, we treat proper names asunused.
12 In the following, we give some examples of evoked, unused, and brand-new11 Quoting Prince (1992, 305): "Inferrables are like Hearer-new (and, therefore, Discourse-new) entities inthat the hearer is not expected to already have in his/her head the entity in question.
"12 For examples of brand-new proper names and how they are introduced, see, for example, thebeginning of articles in the "obituaries" ection of the New York Times.321Computational Linguistics Volume 25, Number 3discourse ntities, though in naturally occurring texts these phenomena rarely showup unadulterated.
13 The remaining categories will be explained subsequently.Example 3a.
He lived his final nine years in one of \[two rent-subsidized buildings\]BNconstructed especially for elderly survivors.b.
When the \[buildings\]E opened - one in 1964, one in 1970 - there werewaiting lists.c.
Once, \[they\]E held 333 survivors.In example (3a), buildings is introduced as a discourse-new discourse ntity, whichis brand-new (BN).
In (3b), the definite NP the buildings cospecifies the discourse ntityfrom (3a).
Hence, buildings in (3b) is evoked (E), just as is they in (3c).Certain proper names are assumed to be known by any hearer.
Therefore, theseproper names need no further explanation.
Winnie Madikizela Mandela in example (4)is unused (U), i.e., it is discourse-new but hearer-old.
Other proper names have to beintroduced because they are discourse-new and hearer-new.
In example (5), MarianneKador is introduced by means of a lengthy appositive that relates the brand-new propername to the knowledge of the hearer.
In particular, the noun phrase the apartmentbuildings is discourse-old (see example (3)).Example 4\[A defiant Winnie Madikizela Mandela\]u testified for more than 10 hourstoday, dismissing all evidence that .. .Example 5"He was an undervalued person all his life," said Marianne Kador, asocial worker for Selfhelp Community Services, which operates theapartment buildings in Queens.In Table 8, we define various sets, which are used for the specification of the Cfranking criteria in Table 9.
We distinguish between two different sets of discourseentities, hearer-old iscourse ntities (OLD) and hearer-new discourse ntities (NEW).For any two discourse entities (x, posx) and (y, posy), with x and y denoting thelinguistic surface expression of those entities as they occur in the discourse, and pOSxand posy indicating their respective text position, pOSx ~ posy, in Table 9 we define thebasic ordering constraints on elements in the forward-looking centers Cf(Ui).
For anyutterance Ui, the ordering of discourse ntities in the Cf(Ui) that can be derived fromthe above definitions and the ordering constraints (1) to (3) are denoted by the relationII ..~ H.Ordering constraint (1) characterizes the basic relation for the overall ranking of theelements in the Cf.
Accordingly, any hearer-old expression in utterance Ui is given thehighest preference as a potential antecedent for an anaphoric expression in Ui+l.
Any13 Examples (3) and (5)-(8) are from the New York Times, Dec. 11, 1997.
("Remembering one whoremembered.
Eugen Zuckermann, survivor, kept the ghosts of the holocaust alive," by Barry Bearak.
)Example (4) is from the New York Times, Dec. 1, 1997.
("Winnie Mandela is defiant, calling accusations'lunacy'," by Suzanne Daley.)
We split complex sentences into the units specified by Kameyama (1998)following the categorization in Figure 1.322Strube and Hahn Functional CenteringTable 8Sets of discourse ntities for the basic Cf ranking.DE : the set of discourse ntities in UiE : the set of evoked discourse ntities in UiU : the set of unused discourse ntities in UiOLD := E U UNEW := DE - OLDTable 9Basic functional ranking constraints on the Cf list.1.
If x E OLD and y E NEW, then x-~ y.2.
If x, y E OLD or x, y E NEW, then x -~ y, if posx < posy3.
If (1) or (2) do not apply, then x and y are unordered with respect o the Cf-ranking.hearer-new expression is ranked below hearer-old expressions.
Ordering constraint (2)captures the ordering for the sets OLD or NEW when they contain elements of thesame type.
In this case, the elements of each set are ranked according to their textposition.4.3.2 Extended Cf Ranking.
While the basic Cf ranking criteria are sufficient for textswith a high proport ion of pronouns and nominal anaphora (e.g., l iterary texts, news-paper articles about persons), it is necessary to refine the ranking criteria in order todeal with expository texts, e.g., test reports, discharge summaries.
These texts usuallycontain few pronouns and are characterized by a large number  of inferrables, whichare often the major glue in achieving local coherence.
In order to accommodate thecentering model to texts from these genres, we distinguish a third set of expressions;mediated discourse entities in Ui (MED).
On Prince's (1981) familiarity scale, the setof hearer-old discourse entities (OLD) remains the same as before, i.e., it consists ofevoked (E) and unused (U) discourse entities, while the set of hearer-new discourseentities (NEW) now consists only of brand-new (BN) discourse entities.
Inferable (I),containing inferable (IC), and anchored brand-new (BN A) discourse entities, whichmake up the set of mediated iscourse ntities, have a status between hearer-old andhearer-new discourse ntities.
14 See Figure 3 for Prince's familiarity scale and its rela-tion to the three sets.
Again, the elements of this set are indistinguishable with respectto their information status-- for  instance, inferable and anchored brand-new discourseentities have the same information status because they belong to the set of mediateddiscourse entities.
Hence, the extended Cf ranking, depicted in Figure 3, will preferOLD discourse entities over MEDiated ones, and MEDiated ones will be preferredover NEW ones.We assume that the difference between containing inferables and anchored brand-new discourse entities is negligible.
(It was not well defined in Prince \[1981\] and in14 Again, quoting Prince (1992, 305-306): "Inferrables are thus like Hearer-old entities in that they rely oncertain assumptions about what the hearer does know, e.g.
that buildings typically have doors \[...\],and they are like Discourse-old entities in that they rely on there being already in the discourse-modelsome entity to trigger the inference \[...\].
"323Computational LinguisticsFigure 3Information status and familiarity (refined version).Volume 25, Number 3Prince \[1992\] she abandoned the second term.)
Therefore, we conflate them into thecategory of anchored brand-new discourse ntities.
These discourse ntities requirethat the anchor modifies a brand-new head and that the anchor is either an evokedor an unused discourse ntity.
In the following, we give examples of inferrables andanchored brand-new discourse ntities.Example 6a.
By his teen-age years, the distorted mentality of anti-Semitism was in fullwarp.b.
\[Thefamily\]i was expelled to Hungary in 1939 ...In example 6 the relation between the definite NP the family and the context hasto be inferred, therefore the family belongs to the category inferable (I).
It is markedby definiteness but it is not anaphoric since there is no anaphoric antecedent.
Thoughinferables are often marked by definiteness, it is possible that they are indefinite, likean uncle in example (7b).Example 7a.
He shared this bounty with his fatherb.
but \[a sickly uncle\]1 was left to remain hungry.Anchored brand-new (BN A) discourse ntities as in example (8) are heads ofphrases whose modifiers relate (anchor) them to the context.Example 8a.
He had already lost too many companions.b.
\[\[HiS\]E fianc~e\]BNA had died in a car wreck.With respect o inferables, there exist only a few computational treatments, allof which are limited in scope.
We here restrict inferables to the particular subset de-fined by Hahn, Markert, and Strube (1996), which we call functional anaphora (FA).In the following, we will limit our discussion of inferables to those which figure asfunctional anaphors.
In Table 10, we define the sets needed for the specification ofthe extended Cf ranking criteria in Table 11.
We distinguish between three differentsets of discourse ntities; hearer-old iscourse ntities (OLD), mediated iscourse n-tities (MED), and hearer-new discourse ntities (NEW).
Note that the antecedent of afunctional anaphor (the inferred discourse ntity) is included in the set of hearer-olddiscourse ntities.324Strube and Hahn Functional CenteringTable 10Sets of discourse ntities for the extended Cf ranking.DE : the set of discourse ntities in UiEUFA anteFABN A :the set of evoked discourse ntities in Uithe set of unused discourse ntities in Uithe set of antecedents of functional anaphors in Uithe set of functional anaphors in Uithe set of anchored brand-new discourse ntities in UiOLD := E U U U FA anteMED := FA U BN ANEW := DE - (MED U OLD)Table 11Extended functional ranking constraints on the Cf list.1.
If x E OLD and y E MED, then x-< y.If x C OLD and y C NEW, then x -< y.If x E MED and y E NEW, then x -4 y.2.
If x, y C OLD, or x, y C MED, or x, y C NEW, then x -< y, if pOSx < posy3.
If (1) or (2) do not apply, then x and y are unordered with respect to the Cf-ranking.For any two discourse entities (x, pOSx) and (y, po@), with x and y denoting thelinguistic surface expression of those entities as they occur in the discourse, and pOSxand posy indicating their respective text position, pOSx =fi posy, in Table 11 we define theextended functional ordering constraints on elements in the forward-looking centersCf(Ui).
In the following, for any utterance Ui, the ordering of discourse entities in theCf(Ui) that can be derived from the above definitions and the ordering constraints (1)to (3) are denoted by the relation "-<".Ordering constraint (1) characterizes the basic relation for the overall rankingof the elements in the Cf.
Accordingly, any hearer-old expression in utterance Ui isgiven the highest preference as a potential antecedent for an anaphoric or functionalanaphoric expression in Ui+l.
Any mediated expression is ranked just below hearer-old expressions.
Any hearer-new expression is ranked lowest.
Ordering constraint (2)fixes the ordering when the sets OLD, MED, or NEW contain elements of the sametype.
In these cases, the elements of each set are ranked according to their text position.In Table 12 we show the analysis of text fragment (2) using the basic algorithm seeTable 3) with the basic functional Cf ranking constraints (see Table 9).
The fragmentstarts with the evoked discourse entity SENTRY in (2a) (the definiteness of the NPindicates that it was already mentioned earlier in the text).
The pronouns he in (2b)and (2c) are evoked, while signs and tunic are brand-new.
We assume Mike in (2d) tobe evoked, too (MIKE is the main character of that story).
MIKE is the leftmost evokeddiscourse entity in (2d), hence ranked highest in the Cf(2d) and the most preferredantecedent for the pronoun he in (2e).5.
Eva luat ionIn this section, we discuss two evaluation experiments on naturally occurring data.We first compare the success rate of the functional centering algorithm with that ofthe BFP algorithm.
This evaluation uses the basic Cf ranking constraints from Table 9.325Computational Linguistics Volume 25, Number 3Table 12Analysis for text fragment in example (2) according to themodel of functional centering.
(2a)(2b)(2c)(2d)(2e)The sentry was not dead.Cb: -Cf: \[SENTtWE: sentry\]He was, in fact, showing signs of reviving ...Cb: SENTRYE: heCf: \[SENTRYE: he, SIGNS:BN signs\]He was partially uniformed in a cavalry tunic.Cb: SENTRYE: heCf: \[SENTRYE: he, TUNICBN: tunic\]Mike stripped this from him and donned it.Cb: SENTRYE: himCf: \[MIKEE: Mike, TUNICE: this, it, SENTRYE: him\]He tied and gagged the man,.
.
.Cb: MIKEE: heCf: \[MIKEE: he, SENTRY:E the man\]We then introduce a new cost-based evaluat ion method,  which we use for compar ingthe extended Cf rank ing constraints f rom Table 11 with several other approaches.5.1 Success  Rate Eva luat ion5.1.1 Data.
In order to compare  the functional centering a lgor i thm (i.e., the basic al-gor i thm from Table 3 operat ing with the basic functional Cf rank ing constraints f romTable 9) with the BFP algorithm, we analyzed a sample of English and German texts.The test set (Table 13) consisted of the beginnings of three short stories by  ErnestHemingway,  15 three articles f rom the New York Times (NYT), 16 the first three chapters ofa novel  by  Uwe JohnsonS the first two chapters of a short story by Heiner Mfiller, TMand seven articles f rom the Frankfurter Allgemeine Zeitung (FAZ).
1915 Hemingway, Ernest.
1987.
The Complete Short Stories of Ernest Hemingway.
Scribner, New York.
("AnAfrican story," pages 545-554; "Soldier's home," pages 111-116; "Up in Michigan," pages 59~62.
)16 (i) New York Times, Dec. 7, 1997.
("Shot in head, suspect goes free, then to college," by Jane Fritsch,pages A45-48.)
(ii) New York Times, Dec. 1, 1997.
("Winnie Mandela is defiant, calling accusations'lunacy'," by Suzanne Daley, pages A1-12.)
(iii) New York Times, Dec. 11, 1997.
("Remembering one whoremembered.
Eugen Zuckermann, survivor, kept the ghosts of the holocaust alive," by Barry Bearak,pages B1-8.
)17 Johnson, Uwe.
1965.
Zwei Ansichten.
Suhrkamp Verlag, Frankfurt am Main.18 Miiller, Heiner.
1974.
Geschichten aus der Produktion 2.
Rotbuch Verlag, Berlin.
("Liebesgeschichte,"pages 57-62.
)19 FAZ, Aug. 28, 1997.
("Die gute Nachricht ist: Wir k6nnen gewirmen.
New Yorks frthhererPolizeiprasident in Berlin," by Konrad Schuller.)
(ii) FAZ, Nov. 3, 1997.
("Biirgermeister Giuliani stehtvor einer fast sicheren Wiederwahl," by Verena Leucken.)
(iii) FAZ, Sept. 9, 1997.
("Wir haben vielvoneinander lernen kiSnnen," by Claus Peter Mfiller.)
(iv) FAZ, Sept. 10, 1997.
("Die Mutter derMeinungsforschung im Streit.
Ist Elisabeth Noelle-Neumann eine unverbesserliche D utsche?"
by KurtReumann.)
(v) FAZ, Aug. 4, 1997.
("Der zarte Riese, Geisterhaftes Klanglicht und ein Zug ins Weite:Zum Tode von Swjatoslaw Richter," by Gerhard R.
Koch.)
(vi) FAZ, Sept. 2, 1997.
("Glaubwtirdiger alsder K6ixigssohn.
Der Oppositionspolitiker Sam Rainsy k/trnpft ffir das bessere Kambodscha," by ErhardHaubold.)
(vii) FAZ, Sept. 3, 1997.
("Bald das Ende des Vorsitzenden Wagner?
Wechsel an der Spitzeder CDU-Fraktion," by Peter Jochen Winters.
)326Strube and Hahn Functional CenteringTable 13Test set for success rate evaluation.Hemingway NYT English Writers FAZ German3rd pers.
& poss.
pron.
274 302 576sentences 153 233 386words 2785 4546 7331299 320 619186 394 5803195 8005 112005.1.2 Method.
The evaluation was carried out manually by the authors, supportedby a small-scale discourse annotation tool.
We used the following guidelines for ourevaluation: We did not assume any world knowledge as part of the anaphora resolu-tion process.
Only agreement criteria and sortal constraints were applied.
We did notaccount for false positives and error chains, but marked the latter (see Walker 1989).We use Kameyama's (1998) specifications for dealing with complex sentences (fora description, see Section 3).
Following Walker (1989), a discourse segment is definedas a paragraph unless its first sentence has a pronoun in subject position or a pronounwhose syntactic features do not match the syntactic features of any of the precedingsentence-internal oun phrases.
Also, at the beginning of a segment, anaphora resolu-tion is preferentially performed within the same utterance.
According to the preferencefor intersentential c ndidates in the original centering model, we defined the followinganaphora resolution strategy (which is not the best solution for the anaphora resolutionproblem either, but sufficient for the purposes of the evaluation):1..3.Test elements of Cf(Ui_l)--according to the BFP algorithm, or thefunctional centering (henceforth abbreviated as FunC) algorithm.Test elements of Ui, which precede the pronoun, left-to-right.Test elements of Cf(Ui_2) , Cf(Ui_3) .
.
.
.
in  the given order.Since clauses are short in general, step 2 of the algorithm only rarely applies.5.1.3 Results.
The results of our evaluation are given in Table 14.
The first row givesthe number of third person pronouns and possessive pronouns in the data.
The up-per part of the table shows the results for the BFP algorithm, the lower part thosefor the FunC algorithm.
Overall, the data are consistently in favor of the FuncC al-gorithm, though no significance judgments can be made (the data were not drawnas a random sample).
The overall error rate of each approach is given in the rowslabeled as "wrong".
We also tried to determine the major sources of errors (see thenonbold sections in Table 14), and were able to distinguish three different types.
Oneclass of errors relates to the algorithm's strategy.
In the case of the BFP algorithm, thecorresponding row also contains the number of ambiguous cases generated by thisalgorithm (we counted ambiguities as errors, since FunC produced only one read-ing in these cases).
A second class of errors results from error chains, mainly causedby the strategy of each approach or by ambiguities in the BFP algorithm.
A thirderror class is caused by the intersentential specifications, e.g., the correct antecedentis not accessible because it is realized in an embedded clause (reported speech).
Fi-nally, other errors were mainly caused by split antecedents (plural pronouns referringto a couple of antecedents in singular), reference to events (or propositions), andcataphora.327Computational Linguistics Volume 25, Number 3Table 14Evaluation results for success rates.Hemingway NYT English Writers FAZ German3rd pers.
& poss.
pron.
274 302 576 299 320 619:orrect 193 245 438 (76%) 236 227 463 74,8%wrong 81 57 138 (24%) 63 93 156 (25,2%)wrong (strategy) 20 8 28 10 27 37wrong (error chains) 29 15 44 22 28 50wrong (intersentential) 17 27 44 18 24 42wrong (others) 15 7 22 13 14 27:orrect 214 252 466 (80,9%) 248 270 518 (83,7%)wrong 60 50 110 (19,1%) 51 50 101 (16,3%)wrong (strategy)wrong (error chains)wrong (intersentential)wrong (others)8 318 1318 2716 7113145233 318 617 2713 1462444275.1.4 Interpretat ion.
While the rate of errors caused by the specifications for complexsentences and by other reasons is almost identical (the small difference can be ex-plained by false positives), there is a remarkable difference between the algorithmswith respect o strategic errors and error chains.
Strategic errors occur whenever thepreference given by the algorithm under consideration leads to an error.
Most of thestrategic errors implied by the FunC algorithm also show up as errors for the BFPalgorithm.
We interpret his finding as an indication that these errors are caused bya lack of semantic or world knowledge.
The remaining errors of the BFP algorithmare caused by the strictly local definition of its criteria and because the BFP algorithmcannot deal with some particular configurations leading to ambiguities.
The FunC al-gorithm has fewer error chains not only because it yields fewer strategic errors, butalso because it is more robust with respect o real texts.
An utterance Ui, for instance,which intervenes between Ui-1 and Ui+l without any relation to Ui-1 does not affectthe preference decisions in Ui+2 for FunC, although it does affect them for the BFPalgorithm, since the latter cannot assign the Cb(Ui+l).
Also, error chains are sometimesshorter in the FunC analyses.Example (9) illustrates how the local restrictions as defined by the original cen-tering model and the BFP algorithm result in errors and lead to rather lengthy errorchains (see Table 15 for the corresponding centering analysis).
The discourse entitySENTENCE, which is cospecified by the pronoun er, 'it'masc, in (9b), is the Cb(9b).
There-fore, it is the most preferred antecedent for the pronoun ihn in (9c), which causes astrategic error.
This error, in turn, is the reason for a consequent error in (9d), becausethere are no semantic ues that enforce the correct interpretation, i.e., the coreferen-tiality between ihn and Giuliani.
The possible interruption of the error chain, indicatedby the alternative interpretation i (9c), is ruled out, however, by the preference forRETAIN over ROUGH-SHIFT transitions (cf.
Rule 2').Example  9a.
Der Satz, mit dem Ruth Messinger eine der Fernsehdebatten imBfirgermeisterwahlkampf in New York er6ffnete, wird der einzige sein,der von ihr in Erinnerung bleibt.328Strube and Hahn Functional CenteringTable 15BFP results for example (9).
(9a) Cb: -Cf: \[SENTENCE: Satz, dem, der, der, RUTH: Ruth Messinger, ihr,DEBATES: Fernsehdebatten, RACE: Biirgermeisterwahlkampf,NEW YORK: New York, RECOLLECTION: Erinnerung\](9b) Cb: SENTENCE: er CONTINUECf: \[SENTENCE: er, VICTORY: Wahlsieg, GIULIANI: Rudolph Giuliani\](9c) Cb: SENTENCE: ihn RETAINCf: \[NEWSPAPERS: Zeitungen, SENTENCE: ihn, NEW YORK: Stadt\]Cb .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
: ~ u t ~ .
~,~,~ ~OUGH-SH/FT  Cf r~T  .
.
.
.
.
.
.
.
.
.
.
-7_ :~ .
.
.
.
.
.
.
.
,-~ .
.
.
.
.
.
.
.
.
:1.._ ~,T .
.
.
.
~r  .
.
.
.
O , -  JL1(9d) Cb: RETAINCf:SENTENCE:  ihm\[UNIONS: Gewerkschaften, SENTENCE:  ihm\]\[The sentence\]smuabSjCect, with which Ruth Messinger - one of the TV debates- opened, - will - the only one - be, - which - of her - in memory  -remains.The sentence, with which Ruth Messinger opened one of the TV debates,will be the only one, which will be recollected of her.b.
Am nahezu sicheren Wahlsieg des Amtsinhabers Rudolph Giuliani amDienstag wird er nichts ~indern.\[Of the almost certain - victory in the election - of \[the officeholderRudolph Giuliani\]masc\]a'd~SCct - on Tuesday - will - \[it\]smu~Cct - nothing -alter.Of the officeholder Rudolph Giuliani's almost certain victory in theelection on Tuesday, it will alter nothing.c.
Alle Zeitungen der Stadt unterstiitzen ihn.\[All - newspapers of the city\]subject - support - \[him\]dmiraeScCt_object .He is supported by all newspapers of the city.d.
Die Gewerkschaften stehen hinter ihm.\[The un ions \ ]sub jec t  - stand behind - \[him\]imnadSiCrect_object .He is backed up by the unions.The nonlocal definition of hearer-old discourse entities enables the FunC algo-r i thm to compute the correct antecedent for the pronoun ihn in (9c) preventing it fromrunning into an error chain (see Table 16 for the functional centering data).
GIULIANI,who was mentioned earlier in the text, is the leftmost evoked discourse entity in (9b)and therefore the most preferred antecedent for the pronoun in (9c), though there is apronoun of the same gender in (9b).We encountered problems with Kameyama's  (1998) specifications for complex sen-tences.
The differences between clauses that are accessible from a higher syntactic leveland clauses that are not could not be verified by our analyses.
Also, her approach issometimes too coarse-grained (i.e., there are still antecedents within one utterance),and sometimes too fine-grained.
2?20 An  a l te rnat ive  to  Kameyama's  in t rasentent ia l  center ing ,  wh ich  overcomes  these  prob lems and  leads  to329Computational Linguistics Volume 25, Number 3Tabl,Fun((9a)(9b)(9c)(9d)16results for example (9).Cf:Cf:Cf:Cf:\[SENTENCEE: Satz, dem, der, der, RUTHE: Ruth Messinger,RACEE: B~irgermeisterwahlkampf, NEW YORKE: New York,DEBATESBNA : Fernsehdebatten, RECOLLECTIONBN: Erinnerung\]\[GIULIANIE: Rudolph Giuliani, SENTENCEE: er, VICTORYBNA: Wahlsieg\]\[NEW YORKE: Stadt, GIULIANIE: ihn, NEWSPAPERSBNA: Zeitungen\]\[GIuLIANIE: ihm, UNIONSBN: Gewerkschaften\]Table 17Test set for cost-based evaluation.IT Spiegel  Mtiller E(pro)nominal naphors 308 102 153 563functional anaphors 294 25 20 339sentences 451 82 87 620words 5542 1468 867 78775.2 Cost-based Evaluation5.2.1 Data.
The test set for our second evaluation experiment consisted of three dif-ferent ext genres: 15 product reviews from the information technology (IT) domain,one article from the German news magazine Der Spiegel, and the first two chaptersof a short story by the German writer Heiner Mtillerf I Table 17 summarizes the totalnumber of (pro)nominal naphors, functional anaphors, utterances and words in thetest set.5.2.2 Method (Distr ibution of Transition Types).
Given these sample texts, we com-pared three approaches to the ranking of the Cf: a model whose ordering principlesare based on grammatical role indicators only (see Table 1); an "intermediate" model,which can be considered a "naive" approach to free-word-order languages; and thefunctional model based on the information structure constraints stated in Table 11.
Forreasons discussed below, slightly modified versions of the naive and the grammaticalapproaches will also be considered.
They are characterized bythe additional constraintthat antecedents of functional anaphors are ranked higher than the functional anaphorsthemselves.
As in Section 5.1, the evaluation was carried out manually by the authors.Since most of the anaphors in these texts are nominal anaphors, the resolution ofwhich is much more restricted than that of pronominal anaphors, the success rate forthe whole anaphora resolution process is not distinctive nough for a proper evalu-ation of the functional constraints.
The reason for this lies in the fact that nominalanaphors are far more constrained by conceptual criteria than pronominal ones.
Thus,the chance of properly resolving a nominal anaphor, even when ranked at a lowerposition in the center lists, is greater than for pronominal anaphors.
By shifting ourevaluation criteria way from resolution success data to structural conditions reflectingthe proper ordering of center lists (in particular, we focus on the most highly rankeditem of the forward-looking centers), these criteria are intended to compensate for thea significant improvement i  the results, is proposed in Strube (1998).21 Mtiller, Heiner.
1974.
Geschichten aus der Produktion 2.
Rotbuch Verlag, Berlin.
("Liebesgeschichte,"pages 57~2.
)330Strube and Hahn Functional CenteringTable 18Quantitative distribution of centering transitions.Naive & Grammatical & Transition Types Naive FA ante > FA Grammatical FA ante > FA FunCIT CONTINUE 49 167 102 197 309RETAIN 269 158 226 131 25SMOOTH-SHIFT 32 41 24 35 51ROUGH-SHIFT 39 23 37 26 4Spiegel CONTINUE 17 28 37 43 50RETAIN 42 32 28 23 12SMOOTH-SHIFT 9 9 7 8 13ROUGH-SHIFT 7 6 3 1 0Mfiller CONTINUE 31 31 32 32 36RETAIN 19 19 18 18 15SMOOTH-SHIFT 15 17 15 16 18ROUGH-SHIFT 14 12 14 13 10E CONTINUE 97 226 171 272 395RETAIN 330 209 272 172 52SMOOTH-SHIFT 56 67 46 59 82ROUGH-SHIFT 60 41 54 40 14high proport ion of nominal anaphora in our sample.
Table 5 enumerates the types ofcentering transitions we consider.5.2.3 Results (Distribution of Transition Types).
In Table 18, we give the numbersof centering transitions between the utterances in the three test sets.
The first columncontains those generated by the naive approach (such a proposal was made by Gordon,Grosz, and Gill iom \[1993\] as well as by Rambow \[1993\], who, nevertheless, restricts it tothe German middlefield).
We simply ranked the elements of C/according to their textposition.
While it is usually assumed that the functional anaphor (FA) is ranked aboveits antecedent (FA ante) (Grosz, Joshi, and Weinstein 1995, 217), we assume the opposite.The second column contains the results of this modification with respect o the naiveapproach.
In the third column of Table 18, we give the numbers of transitions generatedby the grammatical  constraints (Table 1) stated by Grosz, Joshi, and Weinstein (1995,214, 217).
The fourth column supplies the results of the same modification as was usedfor the naive approach, namely, antecedents of functional anaphors are ranked higherthan the corresponding anaphoric expressions.
The fifth column shows the resultsgenerated by the functional constraints from Table 11.5.2.4 Interpretation (Distribution of Transition Types).
The centering model  assumesa preference order among transition types--CONTINUE ranks above RETAIN and RETAINranks above SHIFT.
This preference order reflects the presumed inference load put onthe hearer to coherently decode a discourse.
Since the functional approach generatesmore CONTINUE transitions (see Table 18), we interpret his as prel iminary evidencethat this approach provides for a more efficient processing than its competitors.
Inparticular, the observation of a predominance of CONTINUEs holds irrespective of thevarious text genres we considered for functional centering and, to a lesser degree, forthe modif ied grammatical  ranking constraints.331Computational Linguistics Volume 25, Number 35.2.5 Method (Costs of Transition Types).
The arguments we have given so far donot seem to be entirely convincing.
Counting single occurrences of transition types,in general, does not reveal the entire validity of the center lists.
Considering adja-cent transition pairs as an indicator of validity should give a more reliable picture,since depending on the text genre considered (e.g., technical vs. news magazine vs.literary texts), certain sequences of transition types may be entirely plausible thoughthey include transitions which, when viewed in isolation, seem to imply consider-able inferencing load (Table 18).
For instance, a CONTINUE transition that follows aCONTINUE transition is a sequence that requires the lowest processing costs.
But aCONTINUE transition that follows a RETAIN transition implies higher processing coststhan a SMOOTH-SHIFT transition following a RETAIN transition.
This is due to the factthat a RETAIN transition ideally predicts a SMOOTH-SHIFT in the following utterance.Hence, we claim that no one particular centering transition should be preferred overanother.
Instead, we advocate the idea that certain centering transition pairs are tobe preferred over others.
Following this line of argumentation, we propose here toclassify all occurrences of centering transition pairs with respect o the "costs" theyimply.
The cost-based evaluation of different Cf orderings refers to evaluation criteriathat form an intrinsic part of the centering model.Transition pairs hold for three immediately successive utterances.
We distinguishbetween two types of transition pairs, cheap ones and expensive ones.?
A transition pair is cheap if the backward-looking center of the currentutterance is correctly predicted by the preferred center of theimmediately preceding utterance, i.e., Cb(Ui) = Cp(Ui_l).?
A transition pair is expensive if the backward-looking center of thecurrent utterance is not correctly predicted by the preferred center of theimmediately preceding utterance, i.e., Cb(Ui) # G(Ui_I).In particular, chains of the RETAIN transition in passages where the Cb does notchange (passages with constant theme) show that the grammatical ordering constraintsfor the forward-looking centers are not appropriate.5.2.6 Results (Costs of Transition Types).
The numbers of centering transition pairsgenerated by the different approaches are shown in Table 19.
In general, the func-tional approach reveals the best results, while the naive and the grammatical p-proaches work reasonably well for the literary text, but exhibit a remarkably poorerperformance for the texts from the IT domain and, to a lesser degree, from the newsmagazine.
The results for the latter approaches improve only slightly with the modifi-cation of ranking the antecedent of an functional anaphor (FA ante) above the functionalanaphor itself (FA).
In any case, they do not compare to the results of the functionalapproach.5.3 Extension of the Centering TransitionsOur use of the centering transitions led us to the conclusion that CONTINUE andSMOOTH-SHIFT are not completely specified by Grosz, Joshi, and Weinstein (1995) andBrennan, Friedman, and Pollard (1987).
According to Brennan, Friedman, and Pol-lard's definition, it is possible that a transition is labeled SMOOTH-SHIFT even if Cp(Ui)Cp(Ui-1).
Such a SHIFT is less smooth, because it contradicts the intuition that aSMOOTH-SHIFT fulfills what a RETAIN predicted.
The same applies to a CONTINUE withthis characteristic.
Hence, we propose to extend the set of transitions as shown in Ta-332Strube and Hahn Functional CenteringTable 19Cost values for centering transition pair types.Grammatical & FunC Naive & Grammatical FA a'te > FA Cost Type Naive FA ante > FAIT cheap 72 180 129 236 321expensive 317 209 260 153 68Spiegel cheap 25 36 45 51 62expensive 50 39 30 24 13Mfiller cheap 45 48 46 48 55expensive 34 31 33 31 24E cheap 142 264 220 335 438expensive 401 279 323 208 105Table 20Revised transition types.Cb(Ui) = Cb(Ui-1) Cb(Ui) :/: Cb(Ui-1)OR Cb(Ui-1) undef.Cb(Ui) = Cp(Ui) AND Cp( Ui) = Cp( Ui-1) CONTINUE SMOOTH-SHIFTCb(Ui) = Cp(Ui) AND Cp(Ui)  Cp(U i_ l )  EXP-CONTINUE EKP-SMOOTH-SHIFTCb(Ui) -7 ?
Cp(Ui) RETAIN ROUGH-SHIFTTable 21Costs for transition pairs.CONT.
EXP-CONT.
RET.
SMOOTH-S.  EXP-SMOOTH-S .
ROUGH-S.- cheap - exp.
- - -CONT.
cheap - cheap exp.
- exp.EXP-CONT.
exp.
- exp.
exp.
- exp.RET.
exp.
exp.
exp.
cheap exp.
exp.SMOOTH-S. cheap exp.
exp.
exp.
exp.
exp.EXP-SMOOTH-S. exp.
exp.
exp.
exp.
exp.
exp.ROUGH-S. exp.
exp.
exp.
cheap exp.
exp.ble 20.
The def in i t ions of CONTINUE and  SMOOTH-SHIFT are extended by  the cond i t ionthat Cp(Ui) = Cp(Ui-1), whi le  EXP-CONTINUE and EXP-SMOOTH-SHIFT (expensive CON-TINUE and  expens ive  SMOOTH-SHIFT) require the oppos i te .
RETAIN and  ROUGH-SHIFTfulfil l Cp(Ui) =fi Cp(Ui-1) without  further  extensions.Table 21 contains a complete  overv iew of the t rans i t ion pairs.
On ly  those whosesecond t rans i t ion fulfi l ls the cr i ter ion Cp(Ui) = Cp(Ui-1) are labe led  as "cheap.
"5.4 Redef in i t ion  o f  Ru le  2Grosz,  Joshi, and  Weinste in  (1995) def ine Rule 2 of the center ing mode l  on the ba-sis of sequences of transit ions.
Sequences of CONTINUE trans i t ions are pre fer red  over333Computational Linguistics Volume 25, Number 3sequences of RETAIN transitions, which are preferred over sequences of SHIFT transi-tions.
Brennan, Friedman, and Pollard (1987) utilize this rule for anaphora resolutionbut restrict it to single transitions.
Based on the preceding discussion of cheap andexpensive transition pairs, we propose to redefine Rule 2 in terms of the costs oftransition types.
22 Rule 2 then reads as follows:Rule 2" Cheap transition pairs are preferred over expensive ones.We believe that this definition of Rule 2 allows for a far better assessment ofreferential coherence in discourse than a definition in terms of sequences oftransitions.For anaphora resolution, we interpret Rule 2" such that the preference for an-tecedents of anaphors in Ui can be derived directly from the Cf(Ui-1).
The higher adiscourse ntity is ranked in the Cf, the more likely it is the antecedent of a pronoun.We see the redefinition of Rule 2 as the theoretical basis for a centering algorithm forpronoun resolution that simply uses the Cf as a preference ranking device like thebasic centering algorithm shown in Table 3.
In this algorithm, the metaphor of coststranslates into the number of elements of the Cf that have to be tested until the correctantecedent is found.
If the Cp of the previous utterance is the correct one, then thecosts are indeed very low.5.5 Does Functional Centering Provide a More Satisfactory Explanation of the Data?We were also interested in finding out whether the functional criteria we proposemight explain the linguistic data in a more satisfactory way than the grammatical-role-based criteria discussed so far.
So, we screened sample data from the literature,which were already annotated by centering analyses (for English, we considered allexamples discussed in Grosz, Joshi, and Weinstein \[1995\] and Brennan, Friedman, andPollard \[1987\]).
We achieved consistent results for the grammatical nd the functionalapproach for all the examples contained in Grosz, Joshi, and Weinstein (1995) but founddiverging analyses for some examples discussed by Brennan, Friedman, and Pollard(1987).
While the RETAIN-SHIFT combination in examples (10c) and (10d') (slightlymodified from Brennan, Friedman, and Pollard \[1987, 157\]) did not indicate adifferencebetween the approaches, for the RETAIN-CONTINUE combination i examples (10c) and(10d), the two approaches led to different results (see Table 22 for the BFP algorithmand Table 23 for the FunC algorithm).Example 10a.
Brennan drives an Alfa Romeo.b.
She drives too fast.c.
Friedman races her on weekends.d.
She often wins.d'.
She often beats her.Within the functional approach, the proper name Friedman is unused and, there-fore, the leftmost hearer-old iscourse ntity of (10c).
Hence, FRIEDMAN is the mostpreferred antecedent for the pronoun she in (10d) and (10d').22 See Di Eugenio (1998) for a discussion regarding certain pairs of transitions and their elation to zerovs.
strong pronouns.334Strube and Hahn Functional CenteringTable 22BFP interpretation for example (10)--The "Friedman" scenario.
(10a) Cb: -Cf: \[BRENNAN: Brennan, ALFA ROMEO: Alfa Romeo\](10b) Cb: \[BRENNAN: she\] CONTINUECf: \[BRENNAN: she\](10c) Cb: \[BRENNAN: her\] RETAINCf: \[FRIEDMAN: Friedman, BRENNAN: her\](10d) Cb: \[BRENNAN: she\] CONTINUECf: \[BRENNAN: she\]Cb: \[FRIEDMAN: slie\] SMOOTH-SHIFTCf: \[FaiE~l"~ia~.
she\](10d') Cb: \[FRIEDMAN: she\] SMOOTH-SHIFTCf: \[FRIEDMAN: she, BRENNAN: her\]Cb: \[FaIE~ZvlaN: her\]Cf: \[ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
her\] .
.
.
.
.
- .
.
.
.
.
UI~I2~ININ/a-IN, DII~; 12 I~ I~ I J IV I /~ IN .Table 23FunC interpretation for example (10)--The "Friedman" scenario.
(10a)(10b)(10c)(10d)(10d')Cf:Cf:Cf:Cf:Cf:\[BRENNANu: Brennan, ALFA ROMEOBN: Alfa Romeo\]\[BRENNANE: she\]\[FRIEDMANu: Friedman, BRENNANE: her\]\[FRIEDMANE : she\]\[FRIEDMANE: she, BRENNANE: her\]But is subjecthood really the decisive factor?
When we replace Friedman with ahearer-new discourse ntity, e.g., a professional driver, as in (10c#), 23 then the proceduresgenerate inconsistent results, again.
In the BFP algorithm, the ranking of the Cf listdepends only on grammatical roles.
Hence, DRIVER is ranked higher than BRENNANin the Cf(lOc').
In (10d), the pronoun she is resolved to BRENNAN because of the pref-erence for CONTINUE over SMOOTH-SHIFT.
In (10d~), she is resolved to DRIVER becauseSMOOTH-SHIFT is preferred over ROUGH-SHIFT (see Table 24).10cq A professional driver races her on weekends.Within the functional approach, the evoked phrase her in (10c ~) is ranked higherthan the brand-new phrase a professional driver.
Therefore, the preference changes be-tween example (10c) and (10c').
In (10d) and (10d') the pronoun she is resolved toBRENNAN, the discourse ntity denoted by her (see Table 25).We find the analyses of functional centering to match our intuitions about theunderlying referential relations more closely than those that are computed by gram-matically based centering approaches.
Hence, in the light of this still preliminary ev-idence, we answer the question we posed at the beginning of this subsection in theaffirmative--functional centering indeed explains the data in a more satisfying mannerthan other well-known centering principles.23 We owe this variant o Andrew Kehler.
This example may misdirect readers because the phrase aprofessional driver might be assigned the "default" gender masculine.
Anyway, this example--like theoriginal example--seems not to be felicitous English and has only illustrative character.335Computational Linguistics Volume 25, Number 3Table 24BFP interpretation for Example (10)--The "driver" scenario.
(10a) Cb: -Cf: \[BRENNAN: Brennan, ALFA ROMEO: Alfa Romeo\](10b) Cb: \[BRENNAN: she\]Cf: \[BRENNAN: she\] CONTINUE(10c') CD: \[BRENNAN: her\]Cf: \[DRIVER: driver, BRENNAN: her\] RETAIN(10d) Cb: \[BRENNAN: she\]Cf: \[BRENNAN: she\] CONTINUECb: \[D F~7?F,R.
d~e\]Cf: \[D~ivF~a.
she\] .
.
.
.
.
.
.
.
- .
.
.
.
.
(lOd') Cb: \[DRIVER: she\] SMOOTH-SHIFTCf: \[DRIVER: she, BRENNAN: her\]Cb: \[DKIVEF~.
\]igr\] R,OUGH-SHIFTr~ .
.
.
.
.
.
.
.
.
Cf: \[ .
.
.
.
.
.
.
.
.
.
.
~,,~,- .
.
.
.
.
.
.
.
.
.
,-,,~,v ~.
he;\]Table 25FunC interpretation for Example (10)--The "driver" scenario.
(10a)(10b)00c')(10d)(10d')Cf:Cf:Cf:Cf:Cf:\[BRENNANu: Brennan, ALFA ROMEOBN: Alfa Romeo\]\[BRENNANE: she\]\[BRENNANE: her, DRIVERBN: driver\]\[BRENNANE: she\]\[BRENNANE: she, DRIVERE: her\]5.6 Summary of EvaluationTo summarize the results of our empirical evaluation, we claim, first, that our proposalbased on functional criteria leads to substantially improved and--with respect o theinference load placed on the text understander, whether human or machine--moreplausible results for languages with free word order than the structural constraintsgiven by Grosz, Joshi, and Weinstein (1995) and those underlying the naive approach.We base these observations on an evaluation study that considers transition pairs interms of the inference load specific pairs imply.
Second, we have gathered prelimi-nary evidence, still far from conclusive, that the functional constraints on centeringseem to explain linguistic data more satisfactorily than the common grammar-orientedconstraints.
Hence, we hypothesize that these functional constraints might constitutea general framework for treating free- and fixed-word-order languages by the samemethodology.
This claim, without doubt, has to be further substantiated by additionalcross-linguistic empirical studies.The cost-based evaluation we focused on in this section refers to evaluation cri-teria that form an intrinsic part of the centering model.
As a consequence, we haveredefined Rule 2 of the Centering Constraints (Grosz, Joshi, and Weinstein 1995, 215)appropriately.
We replaced the characterization f a preference for sequences of CON-TINUE over sequences of RETAIN and, similarly, sequences of RETAIN over sequencesof SHIFT by one in which cheap transitions are to be preferred over expensive ones.6.
Comparison with Related Approaches6.1 Focus-based ApproachesApproaches to anaphora resolution based on focus devices partly use the informa-tion status of discourse ntities to determine the current discourse focus.
However, a336Strube and Hahn Functional Centeringcommon area of criticism of these approaches i the diversity of data structures theyrequire.
These data structures are likely to hide the underlying linguistic regularities,because they promote the mix of preference and data structure considerations in the fo-cusing algorithms.
As an example, Sidner (1983, 292ff.)
distinguishes between an ActorFocus and a Discourse Focus, as well as corresponding lists, viz.
Potential Actor Focus Listand Potential Discourse Focus List.
Suri and McCoy (1994) in their RAFT/RAPR approachuse grammatical roles for ordering the focus lists and make a distinction between Sub-ject Focus, Current Focus, and corresponding lists.
Both focusing algorithms prefer anelement that represents he Focus to the elements in the list when the anaphoric ex-pression under consideration is not the agent (for Sidner) or the subject (for Suri andMcCoy).
Relating these approaches toour proposal, they already exhibit a weak prefer-ence for a single hearer-old (more precisely, evoked) discourse lement.
Dahl and Ball(1990), describing the anaphora resolution module of the PUNDIT system, improve thefocusing mechanism by simplifying its underlying data structures.
Thus, their proposalis more closely related to the centering model than any other focusing mechanism.
Fur-thermore, if there is a pronoun in the sentence for which the Focus List is built, thecorresponding evoked discourse ntity is shifted to the front of the list.
The followingelements of the Focus List are ordered by grammatical roles again.
Hence, their ap-proach still relies upon grammatical information for the ordering of the centering list,while we use only the functional information structure as the guiding principle.6.2 HeuristicsGiven its embedding in a cognitive theory of inference loads imposed on the hearerand, even more importantly, its fundamental role in a more comprehensive theoryof discourse understanding based on linguistic, attentional, and intentional layers,the centering model can be considered the first principled attempt to deal with pref-erence orders for plausible antecedent selection for anaphors.
Its predecessors wereentirely heuristic approaches to anaphora resolution.
These were concerned with var-ious criteria--beyond strictly grammatical constraints such as agreement--for the op-timization of the referent selection process based on preferential choices.
An elaboratedescription of several of these preference criteria is supplied by Carbonell and Brown(1988) who discuss, among others, heuristics involving case role filling, semantic andpragmatic alignment, syntactic parallelism, syntactic topicalization, and intersententialrecency.
Given such a wealth of criteria one may either try to order them a priori interms of importance or--as was proposed by the majority of researchers in this field--define several scoring functions that compute flexible orderings on the fly.
These com-bine the variety of available vidence, each one usually annotated by a specific weightfactor, and, finally, map the weights to a single salience score (Rich and LuperFoy1988; Haji~ovG KuboG and Kubo~ 1992; Lappin and Leass 1994)These heuristics helped to improve the performance of discourse-understandingsystems through significant reductions of the available search-space for antecedents.Their major drawback is that they require a great deal of skilled hand-crafting that,unfortunately, usually does not scale in broader application domains.
Hence, proposalswere made to replace these high-level "symbolic" categories by statistically interpretedoccurrence patterns derived from large text corpora (Dagan and Itai 1990).
Preferencesthen reflect patterns of statistically significant lexical usage rather than introspectiveabstractions of linguistic patterns uch as syntactic parallelism or pragmatic alignment.Among the heuristic approaches toanaphora resolution, those which consider theidentification of heuristics a machine learning (ML) problem are particularly inter-esting, since their heuristics dynamically adapt to the textual data.
Furthermore, MLprocedures operate on incomplete parses (hence, they accept noisy data), which dis-337Computational Linguistics Volume 25, Number 3tinguishes them from the requirements of perfect information and high data fidelityimposed by almost any other anaphora resolution scheme.
Connolly, Burger, and Day(1994) treat anaphora resolution as an ML classification problem and compare sevenclassifier approaches with the solution quality of a naive hand-crafted algorithm whoseheuristics incorporate the well-known agreement and recency indicators.
Aone andBennett (1996) outline an approach where they consider more than 60 features auto-matically obtained from the machinery of the host natural anguage processing systemthe learner is embedded in.
The features under consideration i clude lexical ones likecategories, yntactic ones like grammatical roles, semantic ones like semantic lasses,and text positional ones, e.g., the distance between anaphor and antecedent.
Thesefeatures are packed in feature vectors--for each pair of an anaphor and its possibleantecedent--and used to train a decision tree, employing Quinlan's C4.5 algorithm(Aone and Bennett 1996), or a whole battery of alternative classifiers in which hybridvariants yield the highest scores (Connolly, Burger, and Day 1994).
Though still notfully worked out, it is interesting to note that in both studies ML-derived heuristicstend to outperform those that were carefully developed by human experts (similarresults are reported by Cardie \[1992\] with respect o learning resolution heuristics forrelative pronouns pertaining to a case-based learning procedure).
This indicates, atleast, that heuristically based methods using simple combinations of features benefitfrom being exposed to and having to adapt to training data.
ML-based mechanismsmight constitute an interesting perspective for the further tuning of ordering criteriafor the forward-looking centers.These mixed heuristic approaches, using multidimensional metrics for ranking an-tecedent candidates, diverge from the assumption that underlies the centering modelthat a single type of criterion--the attentional state and its representation i  termsof the backward- and forward-looking centers--is crucial for referent selection.
Byincorporating functional considerations in terms of the information structure of utter-ances into the centering model we actually enrich the types of knowledge that go intocentered anaphora resolution decisions, i.e., we extend the "dimensionality" of thecentering model, too.
But unlike the numerical scoring approaches, our combinationremains at the symbolic computation level, preserves the modularity of criteria, and,in particular, is linguistically justified.
Although functional centering is not a com-plete theory of preferential naphora resolution, one should clearly stress the differentgoals behind heuristics-based systems, such as the ones just discussed, and the modelof centering.
Heuristic approaches combine introspectively acquired descriptive vi-dence and attempt o optimize reference resolution performance by proper evidence"engineering".
This is often done in an admittedly ad hoc way, requiring tricky retun-ing when new evidence is added (Rich and LuperFoy 1988).
On the other hand, manyof these systems work in a real-world environment (Rich and LuperFoy 1988; Lappinand Leass 1994; Kennedy and Boguraev 1996) in which noisy data and incomplete,sometimes even faulty, analysis results have to be accounted for.
The centering modeldiffers from these considerations in that it aims at unfolding a unified theory of dis-course coherence at the linguistic, attentional, and intentional level (Grosz and Sidner1986); hence, the search for a more principled, theory-based solution, but also the needfor (almost) perfect linguistic analyses in terms of parsing and semantic interpretation.7.
ConclusionIn this paper, we provided a novel account for ordering the forward-looking centerlist, a major construct of the centering model.
The new formulation is entirely based onfunctional notions, grounded in the information structure of utterances in a discourse.338Strube and Hahn Functional CenteringWe motivated our proposal by the constraints that hold for a free-word-order languagesuch as German and derived our results from empirical studies of real-world texts.We also augmented the ordering criteria of the forward-looking center list such thatit accounts not only for (pro)nominal anaphora but also for inferables (restricted tothe subset of functional anaphora), an issue that, up to now, has only been sketchilydealt with in the centering framework.
The extensions we proposed were validated bythe empirical analysis of various texts of considerable l ngth selected from differentdomains and genres.
The "evaluation metric" we used refers to a new cost-based modelof interpreting the validity of centering data.
The distinction between cognitively cheapand expensive transition pairs led us to replace Rule 2 from the original model by aformulation that explicitly incorporates this cost-oriented distinction.A resolution module for (pro)nominal anaphora (Strube and Hahn 1995) and onefor functional anaphora (Hahn, Markert, and Strube 1996) based on this functionalcentering model has been implemented as part of PARSETALK, a comprehensive t xtparser for German (Hahn, Schacht, and Br6ker 1994; Hahn, Neuhaus, and Br6ker1997) in our group.
All these modules are fully operational and integrated withinthe text-understanding backbone of SYNDIKATE, a large-scale text knowledge acqui-sition system for the two real-world domains of information technology (Hahn andSchnattinger 1998) and medicine (Hahn, Romacker, and Schulz 1999).Despite the progress made so far, many research problems remain open for furtherconsideration i the centering framework.
The following list mentions only the mostpertinent issues that have come to our attention and complements he list given byGrosz, Joshi, and Weinstein (1995):..The centering model is rather agnostic about the intricacies of complexsentences such as relative clauses, subordinate clauses, coordinations,and complex noun phrases.
The problem caused by these structuresfor the centering model is how to decompose a complex sentence intocenter-updating units and how to process complex utterances consistingof multiple clauses.
A first proposal is due to Kameyama (1998)who breaks a complex sentence into a hierarchy of center-updatingunits.
Furthermore, she distinguishes several types of constructions inorder to decide which part of the sentence is relevant for the resolutionof an intersentential naphor in the following sentence.
Strube (1996b)(with respect o centering) and Suri and McCoy (1994) (with respect othe focus model) describe similar approaches and provide algorithms forthe interaction of the resolution of inter- and intrasentential naphora,but the topic has certainly not been dealt with exhaustively.
The problemof complex NPs was pointed out by Walker and Prince (1996).
Since thegrammatical functions in a sentence may be realized by a complex NP, it isnot clear how to rank these phrases in the Cf list.
Walker and Prince (1996)propose a "working hypothesis" based on the surface order.
Strube (1998)provides a complete specification for dealing with complex sentences,but this approach departs ignificantly from the centering model.It seems that there exist only a few fully operational implementations ofcentering-based algorithms, since the interaction of the algorithm withglobal and local ambiguities generated by a sentence parser has notreceived much attention until now.
A first proposal for how to deal withcenter ambiguity in an incremental text parser has been made by Hahnand Strube (1996).339Computational Linguistics Volume 25, Number 3...The centering model covers the standard cases of anaphora, i.e.,pronominal and nominal anaphora nd even functional anaphora basedon the proposal we have developed in this article.
It does not, however,take into account several "hard" issues such as plural anaphora, genericdefinite noun phrases, propositional anaphora, and deictic forms (but seeEckert and Strube \[1999\] for a treatment of discourse-deictic anaphora indialogues within a centering-type framework).
These shortcomingsmight be traced back to the fact that the centering model, up to now, didnot consider the role of the (main) verb of the utterance under scrutiny.Other cases, such as VP anaphora (Hardt 1992), temporal anaphora(Kameyama, Passonneau, and Poesio 1993; Hitzeman, Moens, andGrover 1995) have already been examined within the centering model.The particular phenomenon of paycheck anaphora is described by Hardt(1996), though he uses only a rather simplified centering model for thiswork.
Other cases are only dealt with in the focusing framework such aspropositional anaphora (Dahl and Ball 1990).Evaluations of the centering model have so far only been carried outmanually.
This is clearly no longer rewarding, so appropriatecomputational support environments have to be provided.
What wehave in mind is a kind of discourse structure bank and associatedworkbenches comparable to grammar workbenches and parse treebanks.Aone and Bennett (1994), for example, report on a GUI-based DiscourseTagging Tool (DTT) that allows a user to link an anaphor with itsantecedent and specify the type of the anaphor (e.g., pronoun, definiteNP, etc.).
The tagged result can be written out to an SGML-marked file.Arguing for the need for discourse taggers, this also implies thedevelopment of a discourse structure interlingua (some sort of DiscourseStructure Mark-up Language) for describing discourse structures in acommon format in order to ease nonproblematic exchange andworld-wide distribution of discourse structure data sets.
Such anenvironment would provide excellent conditions for further testing, forexample, of our assumption that the information structure constraints wesuggest might apply in a universal manner.Centering theory, so far, is a model of local coherence in the minimalsense, i.e., it allows only the consideration of immediately adjacentcentering structures for establishing proper referential links.
In order toextend that theory to the level of global coherence, various steps have tobe taken.At the referential level, mechanisms have to be introduced toaccount for reference relationships that extend beyond theimmediately preceding utterance.
Empirical evidence for suchphenomena exists in the literature and we also found the need tohave such a mechanism available for longer texts.
The extensionof functional centering to these phenomena is presented in Hahnand Strube (1997), while Walker (1998) builds upon the centeringalgorithm described in Brennan, Friedman, and Pollard (1987).At the level of discourse pragmatics, a richer notion than merereference between terms is needed to account for coherencerelations uch as those aimed at by Rhetorical Structure Theory340Strube and Hahn Functional Centering(Mann and Thompson 1988).
In addition, an explicit relation tobasic notions from speech act theory is also missing, though itshould be considered vital for the global coherence of discourse(Grosz and Sidner 1986).
In general, it might becomeincreasingly necessary to integrate very deep forms of reasoning,perhaps even nonmonotonic (Dunin-Keplicz and Lukaszewicz1986) or abductive inference mechanisms (Nagao 1989), into theanaphora resolution process.
This might become a sheernecessity when incrementality of processing receives a higherlevel of attention in the centering community.AcknowledgmentsWe would like to thank our colleagues fromthe Computational Linguistics Group inFreiburg and at the University ofPennsylvania for fruitful discussions, inparticular Norbert Br6ker, Miriam Eckert,Aravind Joshi, Manfred Klenner, NoboKomagata, Katja Markert, Peter Neuhaus,Ellen Prince, Rashmi Prasad, OwenRambow, Susanne Schacht, and BonnieWebber.
We also owe special thanks to thefour reviewers whose challenges andsuggestions have considerably improvedthe presentation f our ideas aboutfunctional centering in this article.
The firstauthor was partially funded by LGFGBaden-WUrttemberg, a post-doctoral grantfrom DFG (Str 545/1-1) and a post-doctoralfellowship award from the Institute forResearch in Cognitive Science at theUniversity of Pennsylvania (NSF SBR8920230).ReferencesAlshawi, Hiyan.
1992.
Resolving quasilogical forms.
In H. Alshawi, editor, TheCore Language Engine.
MIT Press,Cambridge, MA, pages 187-216.Aone, Chinatsu and Scott W. Bennett.
1994.Discourse tagging tool anddiscourse-tagged multilingual corpora.
InProceedings ofthe International Workshop onSharable Natural Language Resources,pages 71-77, Ikoma, Nara, Japan, August.Aone, Chinatsu and Scott W. Bennett.
1996.Applying machine learning to anaphoraresolution.
In S. Wermter, E. Riloff, and G.Scheler, editors, Connectionist, S atisticaland Symbolic Approaches toLearning forNatural Language Processing.
Springer,Berlin, pages 302-314.Brennan, Susan E., Marilyn W. Friedman,and Carl J. Pollard.
1987.
A centeringapproach to pronouns.
In Proceedings ofthe25th Annual Meeting, pages 155-162,Association for ComputationalLinguistics, Stanford, CA, July.Carbonell, Jaime G. and Ralph D. Brown.1988.
Anaphora resolution: Amulti-strategy approach.
In Proceedings ofthe 12th International Conference onComputational Linguistics, volume 1,pages 96-101, Budapest, Hungary,August.Cardie, Claire.
1992.
Learning todisambiguate r lative pronouns.
InProceedings ofthe l Oth National Conference onArtificial Intelligence, pages 38-43, SanJosG CA, July.Chomsky, Noam.
1981.
Lectures onGovernment and Binding.
Foris, Dordrecht.Clark, Herbert H. 1975.
Bridging.
InProceedings ofthe Conference on TheoreticalIssues in Natural Language Processing,pages 169-174, Cambridge, MA, June.Connolly, Dennis, John D. Burger, andDavid S. Day.
1994.
A machine learningapproach to anaphoric reference.
InProceedings ofthe International Conference onNew Methods in Language Processing,pages 255-261, Manchester, England,September.Dagan, Ido and Alon Itai.
1990.
Automaticprocessing of large corpora for theresolution of anaphora references.
InProceedings ofthe 13th In ternationalConference on Computational Linguistics,volume 3, pages 330-332, Helsinki,Finland, August.Dahl, Deborah A. and Catherine N. Ball.1990.
Reference resolution in PUNDIT.
InP.
Saint-Dizier and S. Szpakowicz, editors,Logic and Logic Grammars for LanguageProcessing.
Ellis Horwood, Chichester,England, pages 168-184.Di Eugenio, Barbara.
1998.
Centering inItalian.
In M. A. Walker, A. K. Joshi, andE.
F. Prince, editors, Centering Theory inDiscourse, Oxford University Press,Oxford, England, pages 115-137.Dunin-Keplicz, Barbara nd Witold341Computational Linguistics Volume 25, Number 3Lukaszewicz.
1986.
Towardsdiscourse-oriented nonmonotonic system.In Proceedings ofthe llth InternationalConference on Computational Linguistics,pages 504-506, Bonn, Germany, August.Eckert, Miriam and Michael Strube.
1999.Resolving discourse deictic anaphora indialogues.
In Proceedings ofthe 9thConference ofthe European Chapter of theAssociation for Computational Linguistics,pages 37-44, Bergen, Norway, June.Firbas, Jan. 1974.
Some aspects of theCzechoslovak approach to problems offunctional sentence prespective.
In F.Daneg, editor, Papers on Functional SentencePerspective.
Academia, Prague,pages 11-37.Gordon, Peter C., Barbara J. Grosz, andLaura A. Gilliom.
1993.
Pronouns, names,and the centering of attention indiscourse.
Cognitive Science, 17:311-347.Grosz, Barbara J.
1977.
The representationand use of focus in a system forunderstanding dialogs.
In Proceedings ofthe 5th International Joint Conference onArtificial Intelligence, volume 1,pages 67-76, Cambridge, MA, August.Grosz, Barbara J., Aravind K. Joshi, andScott Weinstein.
1983.
Providing a unifiedaccount of definite noun phrases indiscourse.
In Proceedings ofthe 21st AnnualMeeting, pages 44-50, Cambridge, MA,June.
Association for ComputationalLinguistics.Grosz, Barbara J., Aravind K. Joshi, andScott Weinstein.
1995.
Centering: Aframework for modeling the localcoherence of discourse.
ComputationalLinguistics, 21(2):203-225.Grosz, Barbara J. and Candace L. Sidner.1986.
Attention, intentions, and thestructure of discourse.
ComputationalLinguistics, 12(3):175-204.Haddock, Nicholas J.
1987.
Incrementalinterpretation a d combinatory categorialgrammar.
In Proceedings ofthe lOthInternational Joint Conference on ArtificialIntelligence, volume 2, pages 661-663,Milan, Italy, August.Hahn, Udo, Katja Markert, and MichaelStrube.
1996.
A conceptual reasoningapproach to textual ellipsis.
In Proceedingsof the 12th European Conference on ArtificialIntelligence, pages 572-576, Budapest,Hungary, August.Hahn, Udo, Peter Neuhaus, and NorbertBrOker.
1997.
Message-passing protocolsfor real-world parsing: An object-orientedmodel and its preliminary evaluation.
InProceedings ofthe 5th International Workshopon Parsing Technologies, pages 101-112,Massachusetts Institute of Technology,Cambridge, MA, September.Hahn, Udo, Martin Romacker, and StefanSchulz.
1999.
Discourse structures inmedical reports--watch out!
Thegeneration of referentially coherent andvalid text knowledge bases inMEDSYND1KATE system.
InternationalJournal of Medical Informatics, 53(1):1-28.Hahn, Udo, Susanne Schacht, and NorbertBrOker.
1994.
Concurrent, object-orientednatural language parsing: ThePARSETALK model.
International Journal ofHuman-Computer Studies, 41(1/2):179-222.Hahn, Udo and Klemens Schnattinger.
1998.Towards text knowledge ngineering.
InProceedings ofthe 15th National Conference onArtificial Intelligence & the l Oth Conferenceon Innovative Applications of ArtificialIntelligence, pages 524-531, Madison, WI,July.Hahn, Udo and Michael Strube.
1996.Incremental centering and centerambiguity.
In Proceedings ofthe 18th AnnualConference ofthe Cognitive Science Society,pages 568-573, LaJolla, CA, July.Hahn, Udo and Michael Strube.
1997.Centering in-the-large: Computingreferential discourse segments.
InProceedings ofthe 35th Annual Meeting of theAssociation for Computational Linguistics andthe 8th Conference ofthe European Chapter ofthe Association for Computational Linguistics,pages 104-111, Madrid, Spain, July.HajigovG Eva, Vladislav Kubofi, and PetrKubofi.
1992.
Stock of shared knowledge:A tool for solving pronominal anaphora.In Proceedings ofthe 15th InternationalConference on Computational Linguistics,volume 1, pages 127-133, Nantes, France,August.Hardt, Daniel.
1992.
An algorithm for VPellipsis.
In Proceedings ofthe 30th AnnualMeeting, pages 9-14, Newark, DE,June-July.
Association for ComputationalLinguistics.Hardt, Daniel.
1996.
Centering in dynamicsemantics.
In Proceedings ofthe 16thInternational Conference on ComputationalLinguistics, volume 1, pages 519-524,Copenhagen, Denmark, August.Hitzeman, Janet.
Marc Moens, and ClaireGroven 1995.
Algorithms for analysingthe temporal structure of discourse.
InProceedings ofthe 7th Conference oftheEuropean Chapter of the Association forComputational Linguistics, pages 253-260,Dublin, Ireland, March.Hoffman, Beryl.
1996.
Translating into freeword order languages.
In Proceedings ofthe16th International Conference on342Strube and Hahn Functional CenteringComputational Linguistics, volume 1,pages 556-561, Copenhagen, Denmark,August.Hoffman, Beryl.
1998.
Word order,information structure, and centering inTurkish.
In M. A. Walker, A. K. Joshi, andE.
E Prince, editors, Centering Theory inDiscourse, pages 251-271, OxfordUniversity Press, Oxford, England.Jaeggli, Osvaldo.
1986.
Arbitrary pluralpronominals.
Natural Language andLinguistic Theory, 4:43-76.Kameyama, Megumi.
1986.
Aproperty-sharing constraint in centering.In Proceedings ofthe 24th Annual Meeting,pages 200-206, New York, NY, June.Association for ComputationalLinguistics.Kameyama, Megumi.
1998.
Intrasententialcentering: A case study.
In M. A. Walker,A.
K. Joshi, and E. F. Prince, editors,Centering Theory in Discourse.
OxfordUniversity Press, Oxford, England,pages 89-112.Kameyama, Megumi, Rebecca Passonneau,and Massimo Poesio.
1993.
Temporalcentering.
In Proceedings ofthe 31st AnnualMeeting, pages 70-77, Columbus, OH,June.
Association for ComputationalLinguistics.Kamp, Hans and Uwe Reyle.
1993.
FromDiscourse to Logic.
Introduction toModeltheoretic Semantics of NaturalLanguage, Formal Logic and DiscourseRepresentation Theory.
Kluwer, Dordrecht.Kennedy, Christopher and BranimirBoguraev.
1996.
Anaphora for everyone:Pronominal anaphora resolution withouta parser.
In Proceedings ofthe 16thInternational Conference on ComputationalLinguistics, volume 1, pages 113-118,Copenhagen, Denmark, August.Lappin, Shalom and Herbert J. Leass.
1994.An algorithm for pronominal anaphoraresolution.
Computational Linguistics,20(4):535-561.Mann, William C. and Sandra A.Thompson.
1988.
Rhetorical StructureTheory: Toward a functional theory oftext organization.
Text, 8(3):243-281.Nagao, Katashi.
1989.
Semanticinterpretation based on the multi-worldmodel.
In Proceedings ofthe 11thInternational Joint Conference on Arti~cialIntelligence, pages 1,467-1,473, Detroit, MI,August.Prince, Ellen E 1981.
Towards a taxonomyof given-new information.
In P. Cole,editor, Radical Pragmatics.
Academic Press,New York, NY, pages 223-255.Prince, Ellen E 1992.
The ZPG letter:Subjects, definiteness, andinformation-status.
In W. C. Mann andS.
A. Thompson, editors, DiscourseDescription: Diverse Linguistic Analyses of aFund-Raising Text.
John Benjamins,Amsterdam, pages 295-325.Rambow, Owen.
1993.
Pragmatic aspects ofscrambling and topicalization i German.In Workshop on Centering Theory inNaturally-Occurring Discourse.
Institute forResearch in Cognitive Science (IRCS),University of Pennsylvania, Philadelphia,PA, May.Rich, Elaine and Susann LuperFoy.
1988.
Anarchitecture for anaphora resolution.
InProceedings ofthe 2nd Conference on AppliedNatural Language Processing, pages 18-24,Austin, TX, February.Sidner, Candace L. 1983.
Focusing in thecomprehension f definite anaphora.
InM.
Brady and R. C. Berwick, editors,Computational Models of Discourse.
MITPress, Cambridge, MA, pages 267-330.Strube, Michael.
1996a.
FunktionalesCentering.
Ph.D. thesis,Albert-Ludwigs-Universit~it Freiburg,Freiburg.Strube, Michael.
1996b.
Processing complexsentences in the centering framework.
InProceedings ofthe 34th Annual Meeting,pages 378-380, Santa Cruz, CA, June.Association for ComputationalLinguistics.Strube, Michael.
1998.
Never look back: Analternative to centering.
In COLING-ACL"98: 36th Annual Meeting of the Associationfor Computational Linguistics and the 17thInternational Conference on ComputationalLinguistics, Montreal, Quebec, Canada,volume 2, pages 1,251-1,257.Strube, Michael and Udo Hahn.
1995.PARSETALK about sentence- andtext-level anaphora.
In Proceedings ofthe7th Conference ofthe European Chapter of theAssociation for Computational Linguistics,pages 237-244, Dublin, Ireland, March.Strube, Michael and Udo Hahn.
1996.Functional centering.
In Proceedings ofthe34th Annual Meeting, pages 270-277, SantaCruz, CA, June.
Association forComputational Linguistics.Suri, Linda Z. and Kathleen E McCoy.
1994.RAFT/RAPR and centering: Acomparison and discussion of problemsrelated to processing complex sentences.Computational Linguistics, 20(2):301-317.Turan, Omit Deniz.
1998.
Rankingforward-looking centers in Turkish:Universal and language specificproperties.
In M. A. Walker, A. K. Joshi,and E. E Prince, editors, Centering in343Computational Linguistics Volume 25, Number 3Discourse.
Oxford University Press,Oxford, England, pages 138-160.VallduvL Enric.
1990.
The InformationalComponent.
Ph.D. thesis, Department ofLinguistics, University of Pennsylvania,Philadelphia, PA.VallduvL Enric and Elisabet Engdahl.
1996.The linguistic realization of informationpackaging.
Linguistics, 34:459-519.Walker, Marilyn A.
1989.
Evaluatingdiscourse processing algorithms.
InProceedings of the 27th Annual Meeting,pages 251-261, Vancouver, B.C., Canada,June.
Association for ComputationalLinguistics.Walker, Marilyn A.
1998.
Centeringanaphora resolution, and discoursestructure.
In M. A. Walker, A. K. Joshi,and E. F. Prince, editors, Centering Theoryin Discourse.
Oxford University Press,Oxford, England, pages 401-435.Walker, Marilyn A., Masayo Iida, andSharon Cote.
1994.
Japanese discourseand the process of centering.Computational Linguistics, 20(2):193-233.Walker, Marilyn A. and Ellen F. Prince.
Abilateral approach to givenness: Ahearer-status algorithm and a centeringalgorithm.
In T. Fretheim and J. K.Gundel, editors, Reference and ReferentAccessibility.
John Benjarnins, Amsterdam,pages 291-306.344
