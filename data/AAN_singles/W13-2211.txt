Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 109?113,Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational LinguisticsT ?UB?ITAK-B?ILGEM German-English Machine Translation Systems forWMT?13?Ilknur Durgar El-Kahlout and Cos?kun MermerT ?UB?ITAK-B?ILGEMGebze 41470, Kocaeli, TURKEY{ilknur.durgar,coskun.mermer}@tubitak.gov.trAbstractThis paper describes T ?UB?ITAK-B?ILGEMstatistical machine translation (SMT) sys-tems submitted to the Eighth Work-shop on Statistical Machine Transla-tion (WMT) shared translation task forGerman-English language pair in both di-rections.
We implement phrase-basedSMT systems with standard parameters.We present the results of using a big tun-ing data and the effect of averaging tun-ing weights of different seeds.
Addition-ally, we performed a linguistically moti-vated compound splitting in the German-to-English SMT system.1 IntroductionT ?UB?ITAK-B?ILGEM participated for the first timein the WMT?13 shared translation task for theGerman-English language pairs in both directions.We implemented a phrase-based SMT system byusing the entire available training data.
In theGerman-to-English SMT system, we performed alinguistically motivated compound splitting.
Wetested different language model (LM) combina-tions by using the parallel data, monolingual data,and Gigaword v4.
In each step, we tuned systemswith five different tune seeds and used the averageof tuning weights in the final system.
We tunedour systems on a big tuning set which is generatedfrom the last years?
(2008, 2009, 2010, and 2012)development sets.
The rest of the paper describesthe details of our systems.2 German-English2.1 BaselineAll available data was tokenized, truecased, andthe maximum number of tokens were fixed to70 for the translation model.
The Moses openSMT toolkit (Koehn et al 2007) was used withMGIZA++ (Gao and Vogel, 2008) with the stan-dard alignment heuristic grow-diag-final (Och andNey, 2003) for word alignments.
Good-Turingsmoothing was used for phrase extraction.
Sys-tems were tuned on newstest2012 with MERT(Och, 2003) and tested on newstest2011.
4-gram language models (LMs) were trained onthe target side of the parallel text and the mono-lingual data by using SRILM (Stolcke, 2002)toolkit with Kneser-Ney smoothing (Kneser andNey, 1995) and then binarized by using KenLMtoolkit (Heafield, 2011).
At each step, systemswere tuned with five different seeds with lattice-samples.
Minimum Bayes risk decoding (Kumarand Byrne, 2004) and -drop-unknown parameterswere used during the decoding.This configuration is common for all of the ex-periments decribed in this paper unless stated oth-erwise.
Table 1 shows the number of sentencesused in system training after the clean-corpus pro-cess.Data Number of sentencesEuroparl 1908574News-Commentary 177712Commoncrawl 726458Table 1: Parallel Corpus.We trained two baseline systems in order to as-sess the effects of this year?s new parallel data,commoncrawl.
We first trained an SMT systemby using only the training data from the previ-ous WMT shared translation tasks that is europarland news-commentary (Baseline1).
As the secondbaseline, we also included the new parallel datacommoncrawl only in the translation model (Base-line2).
Then, we included commoncrawl corpusboth to the translation model and the languagemodel (Baseline3).Table 2 compares the baseline results.
For all109experiments throughout the paper, we present theminimum and the maximum BLEU scores ob-tained after five different tunes.
As seen in thetable, the addition of the commoncrawl corpus re-sultedin a 1.1 BLEU (Papineni et al 2002) pointsimprovement (on average) on the test set.
Al-though Baseline2 is slightly better than Baseline3,we used Baseline3 and kept commoncrawl corpusin LMs for further experiments.System newstest12 newstest11Baseline1 20.58|20.74 19.14|19.29Baseline2 21.37|21.58 20.16|20.46Baseline3 21.28|21.58 20.22|20.49Table 2: Baseline Results.2.2 Bayesian AlignmentIn the original IBM models (Brown et al 1993),word translation probabilities are treated as modelparameters and the expectation-maximization(EM) algorithm is used to obtain the maximum-likelihood estimates of the parameters and theresulting distributions on alignments.
However,EM provides a point-estimate, not a distribu-tion, for the parameters.
The Bayesian align-ment on the other hand takes into account allvalues of the model parameters by treating themas multinomial-distributed random variables withDirichlet priors and integrating over all possiblevalues.
A Bayesian approach to word alignmentinference in IBM Models is shown to result in sig-nificantly less ?garbage collection?
and a muchmore compact alignment dictionary.
As a result,the Bayesian word alignment has better transla-tion performances and obtains significant BLEUimprovements over EM on various language pairs,data sizes, and experimental settings (Mermer etal., 2013).We compared the translation performance ofword alignments obtained via Bayesian inferenceto those obtained via EM algorithm.
We used aa Gibbs sampler for fully Bayesian inference inHMM alignment model, integrating over all pos-sible parameter values in finding the alignmentdistribution by using Baseline3 word alignmentsfor initialization.
Table 3 compares the Bayesianalignment to the EM alignment.
The results showa slight increase in the development set newstest12but a decrease of 0.1 BLEU points on average inthe test set newstest11.System newstest12 newstest11Baseline3 21.28|21.58 20.22|20.49Gibbs Sampling 21.36|21.59 19.98|20.40Table 3: Bayesian Alignment Results.2.3 Development Data in TrainingDevelopment data from the previous years (i.e.newstest08, newstest09, newstest10), though beinga small set of corpus (7K sentences), is in-domaindata and can positively affect the translation sys-tem.
In order to make use of this data, we exper-imented two methods: i) adding the developmentdata in the translation model as described in thissection and ii) using it as a big tuning set for tun-ing the parameters more efficiently as explained inthe next section.Similar to including the commoncrawl corpus,we first add the development data both to the train-ing and language models by concatenating it to thebiggest corpus europarl (DD(tm+lm)) and thenwe removed this corpus from the language models(DD(tm)).
Results in Table 4 show that includingthe development data both the tranining and lan-guage model increases the performance in devel-opment set but decreases the performance in thetest set.
Including the data only in the translationmodel shows a very slight improvement in the testset.System newstest12 newstest11Baseline3 21.28|21.58 20.22|20.49DD(tm+lm) 21.28|21.65 20.00|20.49DD(tm) 21.23|21.52 20.26|20.49Table 4: Development Sets Results.2.4 Tuning with a Big Development DataThe second method of making use of the develop-ment data is to concatenate it to the tuning set.
Asa baseline, we tuned the system with newstest12as mentioned in Section 2.1.
Then, we concate-nated the development data of the previous yearswith the newstest12 and built a big tuning set.
Fi-nally, we obtained a tuning set of 10K sentences.We excluded the newstest11 as an internal test setto see the relative improvements of different sys-tems.
Table 5 shows the results of using a big tun-ing set.
Tuning the system with a big tuning setresulted in a 0.13 BLEU points improvement.110System newstest12 newstest11newstest12 21.28|21.58 20.22|20.49Big Tune 20.93|21.19 20.32|20.58Table 5: Tuning Results.2.5 Effects of Different Language ModelsIn this set of experiments, we tested the effectsof different combinations of parallel and monolin-gual data as language models.
As the baseline, wetrained three LMs, one from each parallel corpusas europarl, news-commentary, and commoncrawland one LM from the monolingual data news-shuffled (Baseline3).
We then trained two LMs,one from the whole parallel data and one from themonolingual data (2LMs).
Table 6 shows that us-ing whole parallel corpora as one LM performsbetter than individual corpus LMs and results in0.1 BLEU points improvement on the baseline.
Fi-nally, we trained Gigaword v4 (LDC2009T13) as athird LM (3LMs) which gives a 0.16 BLEU pointsimprovement over the 2LMs.System newstest12 newstest11Baseline3 21.28|21.58 20.22|20.492LMs 21.46|21.70 20.28|20.573LMs 21.78|21.93 20.54|20.68Table 6: Language Model Results.2.6 German PreprocessingIn German, compounding is very common.
Fromthe machine translation point of view, compoundsincrease the vocabulary size with high number ofthe singletons in the training data and hence de-crease the word alignment quality.
Moreover, highnumber of out-of-vocabulary (OOV) words in tun-ing and test sets results in several German wordsleft as untranslated.
A well-known solution to thisproblem is compound splitting.Similarly, having different word forms for asource side lemma for the same target lemmacauses the lexical redundancy in translation.
Thisredundancy results in unnecessary large phrasetranslation tables that overload the decoder, as aseparate phrase translation entry has to be kept foreach word form.
For example, German definite de-terminer could be marked in sixteen different waysaccording to the possible combinations of genders,case and number, which are fused in six differenttokens (e.g., der, das, die, den, dem, des).
Exceptfor the plural and genitive cases, all these formsare translated to the same English word ?the?.In the German preprocessing, we aimed bothnormalizing lexical redundancy and splitting Ger-man compounds with corpus driven splitting al-gorithm based on Koehn and Knight (2003).
Weused the same compound splitting and lexical re-dundancy normalization methods described in Al-lauzen et al(2010) and Durgar El-Kahlout andYvon (2010) with minor in-house changes.
Weused only ?addition?
(e.g., -s, -n, -en, -e, -es) and?truncation?
(e.g., -e, -en, -n) affixes for com-pound splitting.
We selected minimum candidatelength to 8 and minimum split length to 4.
By us-ing the Treetagger (Schmid, 1994) output, we in-cluded linguistic information in compound split-ting such as not splitting named entities and for-eign words (CS1).
We also experimented adding# as a delimiter for the splitted words except thelast word (e.g., Finanzkrisen is splitted as finanz#krisen) (CS2).On top of the compound splitting, weapplied the lexical redundancy normalization(CS+Norm1).
We lemmatized German articles,adjectives (only positive form), for some pronounsand for nouns in order to remove the lexical re-dundancy (e.g., Bildes as Bild) by using the fine-grained part-of-speech tags generated by RFTag-ger (Schmid and Laws, 2008).
Similar to CS2, Wetested the delimited version of normalized words(CS+Norm2).Table 7 shows the results of compound split-ting and normalization methods.
As a result, nor-malization on top of compounding did not per-form well.
Besides, experiments showed that com-pound word decomposition is crucial and helpsvastly to improve translation results 0.43 BLEUpoints on average over the best system describedin Section 2.5.System newstest12 newstest113LMs 21.78|21.93 20.54|20.68CS1 22.01|22.21 20.63|20.89CS2 22.06|22.22 20.74|20.99CS+Norm2 21.96|22.16 20.70|20.88CS+Norm1 20.63|20.76 22.01|22.16Table 7: Compound Splitting Results.1112.7 Average of WeightsAs mentioned in Section 2.1, we performed tun-ing with five different seeds.
We averaged the fivetuning weights and directly applied these weightsduring the decoding.
Table 8 shows that using theaverage of several tuning weights performs betterthan each individual tuning (0.2 BLEU points).System newstest12 newstest11CS2 22.06|22.22 20.74|20.99Avg.
of Weights 22.27 21.07Table 8: Average of Weights Results.2.8 Other parametersIn addition to the experiments described in theearlier sections, we removed the -drop-unknownparameter which gave us a 0.5 BLEU points im-provement.
We also included the monotone-at-punctuation, -mp in decoding.
We handled out-of-vocabulary (OOV) words by lemmatizing theOOV words.
Moreover, we added all developmentdata in training after fixing the parameter weightsas described in Section 2.7.
Although each ofthese changes increases the translation scores eachgave less than 0.1 BLEU point improvement.
Ta-ble 9 shows the results of the final system afterincluding all of the approaches except the ones de-scribed in Section 2.2 and 2.3.System newstest12 newstest11Final System 22.59|22.77 21.86|21.93Avg.
of Weights 22.66 22.00+ tune data in train ??
22.09Table 9: German-to-English Final System Results.3 English-GermanFor English-to-German translation system, thebaseline setting is the same as described in Sec-tion 2.1.
We also added the items that showedpositive improvement in the German to EnglishSMT system such as using 2 LMs, tuning with fiveseeds and averaging tuning parameters, using -mp,and not using -drop-unknown.
Table 10 shows theexperimental results for English-to-German SMTsystems.
Similar to the German-to-English direc-tion, tuning with a big development data outper-forms the baseline 0.26 BLEU points (on average).Additionally, averaging the tuning weights of dif-ferent seeds results in 0.2 BLEU points improve-ment.System newstest12 newstest11Baseline 16.95|17.03 15.93|16.13+ Big Tune 16.82|17.01 16.22|16.37Avg.
of Weights 16.99 16.47Table 10: English to German Final System Re-sults.4 Final System and ResultsTable 11 shows our official submission scores forGerman-English SMT systems submitted to theWMT?13.System newstest13De-En 25.60En-De 19.28Table 11: German-English Official Test Submis-sion.5 ConclusionIn this paper, we described our submissions toWMT?13 Shared Translation Task for German-English language pairs.
We used phrase-basedsystems with a big tuning set which is a com-bination of the development sets from last fouryears.
We tuned the systems on this big tuningset with five different tunes.
We averaged thesefive tuning weights in the final system.
We trained4-gram language models one from parallel dataand one from monolingual data.
Moreover, wetrained a 4-gram language model with Gigawordv4 for German-to-English direction.
For German-to-English, we performed a different compoundsplitting method instead of the Moses splitter.
Weobtained a 1.7 BLEU point increase for German-to-English SMT system and a 0.5 BLEU point in-crease for English-to-German SMT system for theinternal test set newstest2011.
Finally, we sub-mitted our German-to-English SMT system witha BLEU score 25.6 and English-to-German SMTsystem with a BLEU score 19.3 for the official testset newstest2013.112ReferencesAlexandre Allauzen, Josep M. Crego, ?Ilknur Durgar El-Kahlout, and Francois Yvon.
2010.
Limsi?s statisti-cal translation systems for wmt?10.
In Proceedingsof the Fifth Workshop on Statistical Machine Trans-lation, pages 54?59.Peter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra, and Robert L. Mercer.
1993.
Themathematics of statistical machine translation: Pa-rameter estimation.
Computational Linguistics,19:263?311.
?Ilknur Durgar El-Kahlout and Francois Yvon.
2010.The pay-offs of preprocessing German-English sta-tistical machine translation.
In Proceedings of theSeventh International Workshop on Spoken Lan-guage Translation (IWSLT), pages 251?258.Qin Gao and Stephan Vogel.
2008.
Parallel implemen-tations of word alignment tool.
In Proceedings ofACL WSETQANLP.Kenneth Heafield.
2011.
Kenlm: Faster and smallerlanguage model queries.
In Proceedings of the SixthWorkshop on Statistical Machine Translation, pages187?197.Reinhard Kneser and Hermann Ney.
1995.
Improvedbacking-off for m-gram language modeling.
In Pro-ceedings of the International Conference on Acous-tics, Speech and Signal Processing, pages 181?184.Philipp Koehn and Kevin Knight.
2003.
Empricalmethods for compound splitting.
In Proceedings ofEuropean Chapter of the ACL (EACL), pages 187?194.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-dra Constantin, and Evan Herbst.
2007.
Moses:Open source toolkit for statistical machine transla-tion.
In Proceedings of ACL Demo and Poster Ses-sion, pages 177?180.Shankar Kumar and William Byrne.
2004.
Minimumbayes-risk decoding for statistical machine transla-tion.
In Proceedings of HLT-NAACL, pages 169?176.Cos?kun Mermer, Murat Sarac?lar, and Ruhi Sarkaya.2013.
Improving statistical machine translation us-ing bayesian word alignment and gibbs sampling.IEEE Transactions on Audio, Speech and LanguageProcessing, 21:1090?1101.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 1:19?51.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofthe 41st Annual Meeting of the Association for Com-putational Linguistics (ACL), pages 160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A method for automaticevaluation of machine translation.
In Proceedingsof the 41st Annual Meeting of the Association forComputational Linguistics (ACL), pages 311?318.Helmut Schmid and Florian Laws.
2008.
Estimationof conditional probabilities with decision trees andan application to fine-granined pos tagging.
In Pro-ceedings of COLING.Helmut Schmid.
1994.
Probabilistic part-of-speechtagging using decision trees.
In Proceedings of theInternational Conference on New Methods in Lan-guage Processing.Andreas Stolcke.
2002.
SRILM - an extensible lan-guage modeling toolkit.
In Proceedings of the Inter-national Conference on Spoken Language Process-ing (ICSLP), pages 257?286.113
