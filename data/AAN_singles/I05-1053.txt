R. Dale et al (Eds.
): IJCNLP 2005, LNAI 3651, pp.
600 ?
611, 2005.?
Springer-Verlag Berlin Heidelberg 2005A Phrase-Based Context-Dependent Joint ProbabilityModel for Named Entity TranslationMin Zhang1, Haizhou Li1, Jian Su1, and Hendra Setiawan1,21Institute for Infocomm Research,21 Heng Mui Keng Terrace, Singapore 119613{mzhang, hli, sujian, stuhs}@i2r.a-star.edu.sg2Department of Computer Science,National University of Singapore, Singapore, 117543hendrase@comp.nus.edu.sgAbstract.
We propose a phrase-based context-dependent joint probabilitymodel for Named Entity (NE) translation.
Our proposed model consists of alexical mapping model and a permutation model.
Target phrases are generatedby the context-dependent lexical mapping model, and word reordering is per-formed by the permutation model at the phrase level.
We also present a two-step search to decode the best result from the models.
Our proposed model isevaluated on the LDC Chinese-English NE translation corpus.
The experimentresults show that our proposed model is high effective for NE translation.1   IntroductionA Named Entity (NE) is essentially a proper noun phrase.
Automatic NE translation isan indispensable component of cross-lingual applications such as machine translationand cross-lingual information retrieval and extraction.NE is translated by a combination of meaning translation and/or phoneme trans-literation [1].
NE transliteration has been given much attention in the literature.Many attempts, including phoneme and grapheme-based methods, various machinelearning and rule-based algorithms [2,3] and Joint Source-Channel Model (JSCM)[4], have been made recently to tackle the issue of NE transliteration.
However,only a few works have been reported in NE translation.
Chen et al [1] proposed afrequency-based approach to learn formulation and transformation rules for multi-lingual Named Entities (NEs).
Al-Onaizan and Knight [5] investigated the transla-tion of Arabic NEs to English using monolingual and bilingual resources.
Huang etal.
[6] described an approach to translate rarely occurring NEs by combining pho-netic and semantic similarities.
In this paper, we pay special attention to the issue ofNE translation.Although NE translation is less sophisticated than machine translation (MT) in gen-eral, to some extent, the issues in NE translation are similar to those in MT.
Its chal-lenges lie in not only the ambiguity in lexical mapping such as <?
(Fu),Deputy> and<?
(Fu),Vice> in Fig.1 in the next page, but also the position permutation and fertilityof words.
Fig.1 illustrates two excerpts of NE translation from the LDC corpus [7]:A Phrase-Based Context-Dependent Joint Probability Model 601(a) Regional office of science and technology for Africa??
(FeiZhou) ??
(DiQu) ??
(KeJi) ???
(BanShiChu)(b) Deputy chief of staff to office of the vice president?
(Fu) ??
(ZongTong) ???(BanGongShi)?(Fu)??(ZhuRen)Fig.
1.
Example bitexts with alignmentwhere the italic word is the Chinese pinyin transcription.Inspired by the JSCM model for NE transliteration [4] and the success of statisticalphrase-based MT research [8-12], in this paper we propose a phrase-based context-dependent joint probability model for NE translation.
It decomposes the NE transla-tion problem into two cascaded steps:1)  Lexical mapping step, using the phrase-based context-dependent joint prob-ability model, where the appropriate lexical item in the target language ischosen for each lexical item in the source language;2)  Reordering step, using the phrase-based n-gram permutation model, wherethe chosen lexical items are re-arranged in a meaningful and grammaticalorder of target language.A two-step decoding algorithm is also presented to allow for effective search of thebest result in each of the steps.The layout of the paper is as follows.
Section 2 introduces the proposed model.
InSection 3 and 4, the training and decoding algorithms are discussed.
Section 5 reportsthe experimental results.
In Section 6, we compare our model with the other relevantexisting models.
Finally, we conclude the study in Section 7.2   The Proposed ModelWe present our method by starting with a definition of translation unit in Section 2.1,followed by the formulation of the lexical mapping model and the permutation modelin Section 2.2.2.1   Defining Translation UnitPhrase level translation models in statistical MT have demonstrated significant im-provement in translation quality by addressing the problem of local re-ordering acrosslanguage boundaries [8-12].
Thus we also adopt the same concept of phrase used instatistical phrase-based MT [9,11,12] as the basic NE translation unit to address theproblems of word fertility and local re-ordering within phrase.Suppose that we have Chinese as the source language 1 1... ...=Jj Jc c c c and Eng-lish as the target language 1 1... ...Ii Ie e e e=  in an NE translation 1 1( , )J Ic e , where602 M. Zhang et al1Jjc c?
and 1Iie e?
are Chinese and English words respectively.
Given a directedword alignment A :{ 1 1?J Ic e , 1 1?I Je c }, the set of the bilingual phrase pairs ?
isdefined as follows:2 21 11 11 2 1 2( , , )={ ( , ) :{ ... }, { ... }:}j iJ Ij ic e c ej j j i i i j ivice versa??
?
?
?
?
?
?AA(1)The above definition means that two phrases are considered to be translations ofeach other, if the words are aligned exclusively within the phrase pair, and not to thewords outside [9,11,12].
The phrases have to be contiguous and a null phrase is notallowed.Suppose that the NE pair 1 1( , )J Ic e  is segmented into X phrase pairs ( 1Xc% , 1Xe% ) ac-cording to the phrase pair set ?
, where 1Xe% is reordered so that the phrase alignmentis in monotone order, i.e., xc% is aligned ?% %x xc e For simplicity, we denote by,?
=< >% %x x xc e  the xth phrase pair in ( 1Xc% , 1Xe% ) = 1... ...x X?
?
?
, ?
?
?x .2.2   Lexical Mapping Model and Permutation ModelGiven the phrase pair set ?
, an NE pair ( 1Jc , 1Ie ) can be rewritten as ( 1Xc% , 1Xe% ) =1... ...x X?
?
?
= 1X?
.
Let us describe a Chinese to English (C2E) bilingual trainingcorpus as the output of a generative stochastic process:(1) Initialize queue Qc and  Qe as empty sequences;(2) Select a phrase pair ,x x xc e?
=< >% %  according to the probability distribu-tion 11( | )xxp ??
?
, remove x?
from ?
;(3) Append the phrase xc%  to Qc and append the phrase xe%  to Qe;(4) Repeat steps 2) and 3) until ?
is empty;(5) Reorder all phrases in Qe according to the probability distribution of thepermutation model;(6) Output Qe and Qc .As 11( | )xxp ??
?
is typically obtained from a source-ordered aligned bilingualcorpus, reordering is needed only for the target language.
According to this generativestory, the joint probability of the NE pair ( 1Jc , 1Ie ) can then be obtained by summingthe probabilities over all possible ways of generating various sets of ?
and all possi-ble permutations that can arrive at ( 1Jc , 1Ie ).
This joint probability can be formulatedA Phrase-Based Context-Dependent Joint Probability Model 603in Eq.(2).
Here we assume that the generation of the set ?
and the reordering processare modeled by n-order Markov models, and the reordering process is independent ofthe source word position.11 1 1 1 1111( , )= { ( ) * ( | )}{( ( | ))* ( | )}????=??
????
?%% %XJ I X I XXkx Xx x n kxp c e p p e ep p e e(2)11 11( | ) ( | )xXx x nXkk Xk k kxp e e p e e ??=?
?% % % %                                                      (3)where1% Xkke  stands for one of the permutational sequences of 1% Xe  that can yield 1Ieby linearly joining all phrases, i.e.,11= % XkI ke e ().
The generative process, as formu-lated above, does not try to capture how the source NE is mapped into the target NE,but rather how the source and target translation units can be generated simultaneouslyin the source order and how the target NE can be constructed by reordering the targetphrases, 1% Xe .In essence, our proposed model consists of two sub-models: a lexical mappingmodel (LMM), characterized by 1( | )xx x np ???
?
, that models the monotonic genera-tive process of phrase pairs; and a permutation model (PM), characterized by1( | )xx x nkk kp e e ?
?% % , that models the permutation process for reordering of the targetlanguage.
The LMM in this paper is among the first attempts to introduce context-dependent lexical mapping into statistical MT (Och et al, 2003).
The PM here is alsodifferent from the widely used position-based distortion model in that it modelsphrase connectivity instead of position distortion.
Although PM functions as an n-gram language model, it only models the ordering connectivity between target lan-guage phrases, i.e., it is not in charge of target word selection.Since the proposed model is phrase-based and we use conditional joint probabilityin LMM and use context-dependent n-gram in PM, we call the proposed model aphrase-based context-dependent joint probability model.3   TrainingFollowing the modeling strategy discussed above, the training process consists ofthree steps: phrase alignment, reordering of corpus, and learning statistical parametersfor lexical mapping and permutation models.3.1   Acquiring Phrase PairsTo reduce vocabulary size and avoid sparseness, we constrain the phrase length to upto three words and the lower-frequency phrase pairs are pruned out for accurate604 M. Zhang et alphrase-alignment1.
Given a word alignment corpus which can be obtained by meansof the publicly available GIZA++ toolkit [15], it is very straightforward to constructthe phrase-alignment corpus by incrementally traversing the word-aligned NE fromleft to right2.
The set of resulting phrase pairs forms a lexical mapping table.3.2   Reordering CorpusThe context-dependent lexical mapping model assumes monotonic alignment in thebilingual training corpus.
Thus, the phrase aligned corpus needs to be reordered sothat it is in either source-ordered or target-ordered alignment.
We choose to reorderthe target phrases to follow the source order.
Only in this way can we use the lexicalmapping model to describe the monotonic generative process and leave the reorderingof target translation units to the permutation model.3.3   Training LMM and PMAccording to Eq.
(2), the lexical mapping model (LMM) and the permutationmodel (PM) can be interpreted as a kind of n-gram Markov model.
The phrase pair isthe basic token of LMM and the target phrase is the basic token of PM.
A bilingualcorpus aligned in the source language order is used to train LMM, and a target lan-guage corpus with phrase segmentation in their original word order is used to trainPM.
Given the two corpora, we use the SRILM Toolkit [13] to train the two n-grammodels.4   DecodingThe proposed modeling framework allows LMM and PM decoding to cascade as inFig.2.Fig.
2.
A cascaded decoding strategyThe two-step operation is formulated by Eq.
(4) and Eq.(5).
Here, the probabilitysummation as in Eq.
(2) is replaced with maximization to reduce the computationalcomplexity:111?
arg max{ ( | )}XX xx x nxe p ??
?== ?
?
?%                                                 (4)1Koehn et.
al.
[12] found that that in MT learning phrases longer than three words and learningphrases from high-accuracy word-alignment does not have strong impact on performance.2For the details of the algorithm to acquire phrase alignment from word alignment, please referto the section 2.2 & 3.2 in [9] and the section 3.1 in [12].%1?XeLMMDecoderPMDecoder1Jc 1IeA Phrase-Based Context-Dependent Joint Probability Model 605111?
arg max{ ( | )}xx x nXkIk kxe p e e ??
?== ?
% %                                                 (5)LMM decoding: Given the input 1Jc , the LMM decoder searches for the most prob-able phrase pair set ?
in the source order using Eq.(4).
Since this is a monotonesearch problem, we use a stack decoder [14,18] to arrive at the n-best results.PM decoding: Given the translation phrase sequence 1?Xe% from the LMM decoder,the PM decoder searches for the best phrase order that gives the highest n-gram scoreby using Eq.
(5) in the search space ?
, which is all the !X  permutations of the allphrases in 1?Xe% .
This is a non-monotone search problem.The PM decoder conducts a time-synchronized search from left to right, where timeclocking is synchronized over the number of phrases covered by the current partialpath.
To reduce the search space, we prune the partial paths along the way.
Two par-tial paths are considered identical if they satisfy the following both conditions:1) They cover the same set of phrases regardless of the phrase order;2) The last n-1 phrases and their ordering are identical, where n is the orderof the n-gram permutation model.For any two identical partial paths, only the path with higher n-gram score is retained.According to Eq.
(5), the above pruning strategy is risk-free because the two partialpaths cover the exact same portion of input phrases and the n-gram histories for thenext input phrases in the two partial paths are also identical.It is also noteworthy that the decoder only needs to perform / 2X  expansions asafter / 2X  expansions, all combinations of / 2X  phrases would have been exploredalready.
Therefore, after / 2X  expansions, we only need to combine the correspond-ing two partial paths to make up the entire input phrases, then select the path withhighest n-gram score as the best translation output.Let us examine the number of paths that the PM decoder has to traverse.
The prun-ing reduces the search space by a factor of !Z , from !
( )!ZXPXX Z=?to!!
( )!ZXCXZ X Z=?
?, where Z is the number of phrases in a partial path.Since X ZXZXC C?= , the maximum number of paths that we have to traverse is / 2XXC .For instance, when 10X = , the permutation decoder traverses 510 252C =  pathsinstead of the 510 30, 240P = in an exhausted search.By cascading the translation and permutation steps, we greatly reduce the searchspace.
In LMM decoding, the traditional stack decoder for monotone search is veryfast.
In PM decoding, since most of NE is less than 10 phrases, the permutation de-coder only needs to explore at most 510 252C =  living paths due to our risk-free prun-ing strategy.606 M. Zhang et al5   Experiments5.1   Experimental Setting and ModelingAll the experiments are conducted on the LDC Chinese-English NE translation corpus[7].
The LDC corpus consists of a large number of Chinese-Latin language NE en-tries.
Table 1 reports the statistics of the entire corpus.
Because person and placenames in this corpus are translated via transliteration, we only extract the categoriesof organization, industry, press, international organization, and others to form a cor-pus subset for our NE translation experiment, as indicated in bold in Table 1.
As thecorpus is in its beta release, there are still many undesired entries in it.
We performeda quick proofreading to correct some errors and remove the following types of entries:1) The duplicate entry;2) The entry of single Chinese or English word;3) The entries whose English translation contains two or more non-English words.We also segment the Chinese translation into a word sequence.
Finally, we obtain acorpus of 74,606 unique bilingual entries, which are randomly partitioned into 10equal parts for 10-fold cross validation.Table 1.
Statistics of the LDC Corpus# of EntriesCategory C2E E2CPerson 486,212 572,213Place 276,382 298,993Who-is-Who 30,028 36,881Organization 30,800 37,145Industry 54,747 58,468Press 29,757 32,922Int?l Org 7,040 7,040Others 13,007 14,066As indicated in Section 1, although MT is more difficult than NE translation, theyboth have many properties in common, such as lexical mapping ambiguity and permu-tation/distortion.
Therefore, to establish a comparison, we use the publicly availablestatistical MT training and decoding tools, which can represent the state-of-the-art ofstatistical phrase-based MT research, to carry out the same NE translation experimentsas reference cases.
All the experiments conducted in this paper are listed as follow:1) IBM method C: word-based IBM Model 4 trained by GIZA++3 [15] and ISIDecoder4 [14,16];3http://www.fjoch.com/4http://www.isi.edu/natural-language/software/decoder/manual.htmlA Phrase-Based Context-Dependent Joint Probability Model 6072) IBM method D:   phrase-based IBM Model 4 trained by GIZA++ on phrase-aligned corpus and ISI Decoder working on phrase-segmented testing corpus.3) Koehn method: Koehn et al?s phrase-based model [12] and PHARAOH5 de-coder6;4) Our method: phrase-based bi-gram LMM and bi-gram PM, and our two-stepdecoder.To make an accurate comparison, all the above three phrase-based models aretrained on the same phrase-segmented and aligned corpus, and tested on the samephrase-segmented corpus.
ISI Decoder carries out a greedy search, and PHARAOH isa beam-search stack decoder.
To optimize their performances, the two decoders areallowed to do unlimited reordering without penalty.
We train trigram language mod-els in the first three experiments and bi-gram models in the forth experiment.5.2   NE TranslationTable 2 and Table 3 report the performance of the four methods on the LDC NEtranslation corpus.
The results are interpreted in different scoring measures, whichallow us to compare the performances from different viewpoints.?
ACC reports the accuracy of the exact;?
WER reports the word error rate;?
PER is the position-independent, or ?bag-of-words?
word error rate;?
BLEU score measures n-gram precision [19]?
NIST score [20] is a weighted n-gram precision.Please note that WER and PER are error rates, the lower numbers represent betterresults.
For others, the higher numbers represents the better results.Table 2.
E2C NE translation performance (%)IBMmethod CIBMmethod DKoehnmethodOurmethodACC 24.5 36.3 47.1 51.5WER 51.0 38.5 32.5 26.6PER 48.5 36.2 26.8 16.3BLEU 29.9 41.8 51.2 56.1 Op entes tNIST 7.2 8.6 9.3 10.2ACC 51.1 78.9 88.2 90.9WER 34.1 12.8 6.3 4.3PER 31.5 9.5 4.1 2.7BLEU 54.7 80.9 89.1 91.9E2CClo se dtes tNIST 11.1 14.2 14.7 14.85http://www.isi.edu/licensed-sw/pharaoh/6http://www.isi.edu/licensed-sw/pharaoh/manual-v1.2.ps608 M. Zhang et alTable 3.
C2E NE translation performance (%)IBMmethod CIBMmethod DKoehnmethodOurmethodACC 13.4 21.8 31.2 36.1WER 60.8 45.8 41.3 38.9PER 49.6 38.2 32.6 26.6BLEU 25.1 49.8 52.9 54.1 op entes tNIST 5.94 8.21 8.91 9.25ACC 34.3 69.5 79.2 81.3WER 48.2 23.6 11.3 9.2PER 35.7 14.7 8.7 6.2BLEU 42.5 76.2 85.7 88.0C2Ec lo se dtes tNIST 8.7 12.7 13.8 14.4Table 2 & 3 show that our method outperforms the other three methods consis-tently in all cases and by all scores.
IBM method D gives better performance thanIBM method C, simply because it uses phrase as the translation unit instead of singleword.
Koehn et al?s phrase-based model [12] and IBM phrase-based Model 4 used inIBM method D are very similar in modeling.
They both use context-independentlexical mapping model, distortion model and trigram target language model.
Thereason why Koehn method outperforms IBM method D may be due to the differentdecoding strategy.
However, we still need further investigation to understand whyKoehn method outperforms IBM method D significantly.
It may also be due to thedifferent LM training toolkits used in the two experiments.Our method tops the performance among the four experiments.
The significant po-sition-independent word error rate (PER) reduction shows that our context-dependentjoint probability lexical mapping model is quite effective in target word selectioncompared with the other context-free conditional probability lexical model togetherwith target word n-gram language model.Table 4.
Step by step top-1 performance (%)LMM decoderLMM+PM decoderE2C59.951.5C2E 40.5 36.1Table 4 studies the performance of the decoder by steps.
The LMM decoder col-umn reports the top-1 ?bag-of-words?
accuracy of the LMM decoder regardless ofword order.
This is the upper bound of accuracy that the PM decoder can achieve.
TheLMM+PM decoder column shows the combined performance of two steps, where weA Phrase-Based Context-Dependent Joint Probability Model 609measure the top-1 LMM+PM accuracy by taking top-1 LMM decoding results asinput.
It is found that the PM decoder is surprisingly effective in that it perfectly reor-ders 85.9% (51.5/59.9) and 89.1% (36.1 /40.5) target languages in E2C and C2Etranslation respectively.All the experiments above recommend that our method is an effective solution forNE translation.6   Related WorkSince our method has benefited from the JSCM of Li et al [4] and statistical MTresearch [8-12], let us compare our study with the previous related work.The n-gram JSCM was proposed for machine transliteration by Li et al [4].
It cou-ples the source and channel constraints into a generative model to directly estimatethe joint probability of source and target algnment using n-gram statistics.
It wasshown that JSCM captures rich contextual information that is present in a bilingualcorpus to model the monotonic generative process of sequential data.
In this point, ourLMM model is the same as JSCM.
The only difference is that in machine translitera-tion Li et al [4] use phoneme unit as the basic modeling unit and our LMM is phrase-based.In our study, we enhance the LMM with the PM to account for the word reorder-ing issue in NE translation, so our model is capable of modeling the non-monotoneproblem.
In contrast, JSCM only models the monotone problem.Both rule-based [1] and statistical model-based [5,6] methods have been proposedto address the NE translation problem.
The model-based methods mostly are based onconditional probability under the noisy-channel framework [8].
Now let?s review thedifferent modeling methods:1) As far as lexical choice issue is concerned, the noisy-channel model, repre-sented by IBM Model 1-5 [8], models lexical dependency using a context-freeconditional probability.
Marcu and Wong [10] proposed a phrase-based con-text-free joint probability model for lexical mapping.
In contrast, our LMMmodels lexical dependency using n-order bilingual contextual information.2) Another characteristic of our method lies in its modeling and search strat-egy.
NE translation and MT are usually viewed as a non-monotone searchproblem and it is well-known that a non-monotone search is exponentiallymore complex than a monotone search.
Thus, we propose the two separatedmodels and the two-step search, so that the lexical mapping issue can be re-solved by monotone search.
This results in a large improvement on transla-tion selection.3) In addition, instead of the position-based distortion model [8-12], we use then-gram permutation model to account for word reordering.
A risk-free de-coder is also proposed for the permutation model.One may argue that our proposed model bears a strong resemblance to IBM Model1: a position-independent translation model and a language model on target sentencewithout explicit distortion modeling.
Let us discuss the major differences betweenthem:610 M. Zhang et al1) Our LMM models the lexical mapping and target word selection using a con-text-dependent joint probability while IBM Model 1 using a context-independent conditional probability and a target n-gram language model.2) Our LMM carries out the target word selection and our PM only models thetarget word connectivity while the language model in IBM Model 1 performsthe function of target word selection.Alternatively, finite-state automata (FSA) for statistical MT were previous sug-gested for decoding using contextual information [21,22].
Bangalore and Riccardi[21] proposed a phrase-based variable length n-gram model followed by a reorderingscheme for spoken language translation.
However, their re-ordering scheme was notevaluated by empirical experiments.7   ConclusionsIn this paper, we propose a new model for NE translation.
We present the training anddecoding methods for the proposed model.
We also compare the proposed methodwith related work.
Empirical experiments show that our method outperforms the pre-vious methods significantly in all test cases.
We conclude that our method worksmore effectively and efficiently in NE translation than previous work does.Our method does well in NE translation, which is relatively less sophisticated interms of word distortion.
We expect to improve its permutation model by integratinga distortion model to account for larger sentence structure and apply to machine trans-lation study.AcknowledgmentsWe would like to thank the anonymous reviews for their invaluable suggestions onour original manuscript.References1.
Hsin-Hsi Chen, Changhua Yang and Ying Lin.
2003.
Learning Formulation and Trans-formation Rules for Multilingual NEs.
Proceedings of the ACL 2003 Workshop onMMLNER2.
K. Knight and J. Graehl.
1998.
Machine Transliteration.
Computational Linguistics, 24(4)3.
Jong-Hoon Oh and Key-Sun Choi, 2002.
An English-Korean Transliteration Model UsingPronunciation and Contextual Rules.
Proceedings of COLING 20024.
Haizhou Li, Ming Zhang and Jian Su.
2004.
A Joint Source-Channel Model for MachineTransliteration.
Proceedings of the 42th ACL, Barcelona,  160-1675.
Y. Al-Onaizan and K. Knight, 2002.
Translating named entities using monolingual and bi-lingual resources.
Proceedings of the 40th ACL, Philadelphia,  400-4086.
Fei Huang, S. Vogel and A. Waibel, 2004.
Improving NE Translation Combining Phoneticand Semantic Similarities.
Proceedings of HLT-NAACL-20047.
LDC2003E01, 2003. http://www.ldc.upenn.edu/A Phrase-Based Context-Dependent Joint Probability Model 6118.
P.F.
Brown, S.A.D.
Pietra, V.J.D.
Pietra and R.L.
Mercer.1993.
The mathematics of statis-tical machine translation.
Computational Linguistics,19(2):263-3139.
Richard Zens and Hermann Ney.
2004.
Improvements in Phrase-Based Statistical MachineTranslation.
Proceedings of HLT-NAACL-200410.
D. Marcu and W. Wong.
2002.
A Phrase-based, Joint Probability Model for StatisticalMachine Translation.
Proceedings of EMNLP-200211.
Franz Joseh Och, C. Tillmann and H. Ney.
1999.
Improved Alignment Models for Statisti-cal Machine Translation.
Proceedings of Joint Workshop on EMNLP and Very Large Cor-pus: 20-2812.
P. Koehn, F. J. Och and D. Marcu.
2003.
Statistical Phrase-based Translation.
Proceedingsof HLT-200313.
A. Stolcke.
2002.
SRILM -- An Extensible Language Modeling Toolkit.
Proceedings ofICSLP-2002, vol.
2, 901-904, Denver.14.
U. Germann, M. Jahr, K. Knight, D. Marcu and K. Yamada.
2001.
Fast Decoding and Op-timal Decoding for Machine Translation.
Proceedings of ACL-200115.
Franz Joseh Och and Hermann Ney.
2003.
A Systematic Comparison of Various StatisticalAlignment Models.
Computational Linguistics, 29(1):19-5116.
U. Germann.
2003.
Greedy Decoding for Statistical Machine Translation in Almost LinearTime.
Proceedings of HLT-NAACL-200317.
Christoph Tillmann and Hermann Ney.
2003.
Word Reordering and a Dynamic Program-ming Beam Search Algorithm for Statistical Machine Translation.
Computational Linguis-tics, 29(1):97-13318.
R. Schwartz and Y. L. Chow.
1990.
The N-best algorithm: An efficient and Exact procedurefor finding the N most likely sentence hypothesis, Proceedings of ICASSP 1990, 81-8419.
K. Papineni, S. Roukos, T. Ward and W. J. Zhu.
2001.
BLEU: a method for automaticevaluation of machine translation.
Technical Report RC22176 (W0109-022), IBM Re-search Report.20.
G. Doddington.
2002.
Automatic evaluation of machine translation quality using n-gramco-occurrence statistics.
Proceedings of ARPA Workshop on HLT21.
S. Bangalore and G. Riccardi, 2000, Stochastic Finite State Models for Spoken LanguageMachine Translation, Workshop on Embedded MT System22.
Stephan Kanthak and Hermann Hey, 2004.
FSA: An Efficient and Flexiable C++ Tookkitfor Finite State Automata Using On-Demand Computation, Proceedings of ACL-2004
