Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 460?470,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsNLP on Spoken Documents without ASRMark Dredze, Aren Jansen, Glen Coppersmith, Ken ChurchHuman Language Technology Center of ExcellenceCenter for Language and Speech ProcessingJohns Hopkins Universitymdredze,aren,coppersmith,Kenneth.Church@jhu.eduAbstractThere is considerable interest in interdis-ciplinary combinations of automatic speechrecognition (ASR), machine learning, natu-ral language processing, text classification andinformation retrieval.
Many of these boxes,especially ASR, are often based on consid-erable linguistic resources.
We would liketo be able to process spoken documents withfew (if any) resources.
Moreover, connect-ing black boxes in series tends to multiply er-rors, especially when the key terms are out-of-vocabulary (OOV).
The proposed alternativeapplies text processing directly to the speechwithout a dependency on ASR.
The methodfinds long (?
1 sec) repetitions in speech,and clusters them into pseudo-terms (roughlyphrases).
Document clustering and classi-fication work surprisingly well on pseudo-terms; performance on a Switchboard task ap-proaches a baseline using gold standard man-ual transcriptions.1 IntroductionCan we do IR-like tasks without ASR?
Informationretrieval (IR) typically makes use of simple featuresthat count terms within/across documents such asterm frequency (tf) and inverse document frequency(IDF).
Crucially, to compute these features, it is suf-ficient to count repetitions of a term.
In particular,for many IR-like tasks, there is no need for an au-tomatic speech recognition (ASR) system to labelterms with phonemes and/or words.This paper builds on Jansen et al (2010), amethod for discovering terms with zero resources.This approach identifies long, faithfully repeatedpatterns in the acoustic signal.
These acoustic repe-titions often correspond to terms useful for informa-tion retrieval tasks.
Critically, this method does notrequire a phonetically interpretable acoustic modelor knowledge of the target language.By analyzing a large untranscribed corpus ofspeech, this discovery procedure identifies a vastnumber of repeated regions that are subsequentlygrouped using a simple graph-based clusteringmethod.
We call the resulting groups pseudo-termssince they typically represent a single word or phrasespoken at multiple points throughout the corpus.Each pseudo-term takes the place of a word orphrase in bag of terms vector space model of a textdocument, allowing us to apply standard NLP algo-rithms.
We show that despite the fully automatedand noisy method by which the pseudo-terms arecreated, we can still successfully apply NLP algo-rithms with performance approaching that achievedwith the gold standard manual transcription.Natural language processing tools can play a keyrole in understanding text document collections.Given a large collection of text, NLP tools can clas-sify documents by category (classification) and or-ganize documents into similar groups for a highlevel view of the collection (clustering).
For exam-ple, given a collection of news articles, these toolscan be applied so that the user can quickly see thetopics covered in the news articles, and organize thecollection to find all articles on a given topic.
Thesetools require little or no human input (annotation)and work across languages.Given a large collection of speech, we would like460tools that perform many of the same tasks, allow-ing the user to understand the contents of the col-lection while listening to only small portions of theaudio.
Previous work has applied these NLP toolsto speech corpora with similar results (see Hazenet al (2007) and the references therein.)
However,unlike text, which requires little or no preprocess-ing, audio files are typically first transcribed intotext before applying standard NLP tools.
Automaticspeech recognition (ASR) solutions, such as largevocabulary continuous speech recognition (LVCSR)systems, can produce an automatic transcript fromspeech, but they require significant development ef-forts and training resources, typically hundreds ofhours of manually transcribed speech.
Moreover,the terms that may be most distinctive in particularspoken documents often lie outside the predefinedvocabulary of an off-the-shelf LVCSR system.
Thismeans that unlike with text, where many tools can beapplied to new languages and domains with minimaleffort, the equivalent tools for speech corpora oftenrequire a significant investment.
This greatly raisesthe entry threshold for constructing even a minimaltool set for speech corpora analysis.The paper proceeds as follows.
After a reviewof related work, we describe Jansen et al (2010),a method for finding repetitions in speech.
Wethen explain how these repetitions are grouped intopseudo-terms.
Document clustering and classifica-tion work surprisingly well on pseudo-terms; perfor-mance on a Switchboard task approaches a baselinebased on gold standard manual transcriptions.2 Related WorkIn the low resource speech recognition regime,most approaches have focused on coupling smallamounts of orthographically transcribed speech (10sof hours) with much larger collections of untran-scribed speech (100s or 1000s of hours) to train ac-curate acoustic models with semi-supervised meth-ods (Novotney and Schwartz, 2009).
In these ef-forts, the goal is to reduce the annotation require-ments for the construction of competent LVCSR sys-tems.
This semi-supervised paradigm was relaxedeven further with the pursuit of self organizing units(SOUs), phone-like units for which acoustic mod-els are trained with completely unsupervised meth-ods (Garcia and Gish, 2006).
Even though the moveaway from phonetic acoustic models improves theuniversality of the architecture, small amounts of or-thographic transcription are still required to connectthe SOUs with the lexicon.The segmental dynamic time warping (S-DTW)algorithm (Park and Glass, 2008) was the first trulyzero resource effort, designed to discover portions ofthe lexicon directly by searching for repeated acous-tic patterns in the speech signal.
This work im-plicitly defined a new direction for speech process-ing research: unsupervised spoken term discovery,the entry point of our speech corpora analysis sys-tem.
Subsequent extensions of S-DTW (Jansen etal., 2010) permit applications to much larger speechcollections, a flexibility that is vital to our efforts.As mentioned above, the application of NLPmethods to speech corpora have traditionally reliedon high resource ASR systems to provide automaticword or phonetic transcripts.
Spoken documenttopic classification has been an application of partic-ular interest (Hazen et al, 2007), for which the rec-ognized words or phone n-grams are used to charac-terize the documents.
These efforts have producedadmirable results, with ASR transcript-based per-formance approached that obtained using the goldstandard manual transcripts.
Early efforts to per-form automatic topic segmentation of speech inputwithout the aid of ASR systems have been promis-ing (Malioutov et al, 2007), but have yet to exploitthe full the range of NLP tools.3 Identifying Matched RegionsOur goal is to identify pairs of intervals within andacross utterances of several speakers that containthe same linguistic content, preferably meaningfulwords or terms.The spoken term discovery algorithm of Jansen etal.
(2010) efficiently searches the space of(n2)in-tervals, where n is the number of speech frames.1Jansen et al (2010) is based on dotplots (Church andHelfman, 1993), a method borrowed from bioinfor-matics for finding repetitions in DNA sequences.1Typically, each frame represents a 25 or 30 ms window ofspeech sampled every 10 ms461t e x t  p r o c e s s i n g  v s .
s p e e c h  p r o c e s s i n gtextprocessingvs.speechprocessingFigure 1: An example of a dotplot for the string ?textprocessing vs. speech processing?
plotted against itself.The box calls out the repeated substring: ?processing.
?3.1 Acoustic DotplotsWhen applied to text, the dotplot construct is re-markably simple: given character strings s1 and s2,the dotplot is a Boolean similarity matrix K(s1, s2)defined asKij(s1, s2) = ?
(s1[i], s2[j]).Substrings common to s1 and s2 manifest them-selves as diagonal line segments in the visualizationof K. Figure 1 shows an example text dotplot whereboth s1 and s2 are taken to be the string ?text pro-cessing vs. speech processing.?
The boxed diago-nal line segment arises from the repeat of the word?processing,?
while the main diagonal line triviallyarises from self-similarity.
Thus, the search for linesegments inK off the main diagonal provides a sim-ple algorithmic means to identify repeated terms ofpossible interest, albeit sometimes partial, in a col-lection of text documents.
The challenge is to gen-eralize these dotplot techniques for application tospeech, an inherently noisy, real-valued data stream.The strategy is to replace character strings withframe-based speech representations of the formx = x1, x2, .
.
.
xN , where each xi ?
Rd is a d-dimensional vector space representation of the ithoverlapping window of the signal.
Given vector timeseries x = x1, x2, .
.
.
xN and y = y1, y2, .
.
.
yM fortwo spoken documents, the acoustic dotplot is thereal-valuedN?M cosine similarity matrixK(x,y)defined asTime (s)Time (s)K(qi,qj)1 2 3 4 5 6 7 8123456780.550.60.650.70.750.80.850.90.951Figure 2: An example of an acoustic dotplot for 8 secondsof speech (posteriorgrams) plotted against itself.
The boxcalls out a repetition of interest.Kij(x,y) =12[1 +?xi, yj??xi??yj?].
(1)Even though the application to speech is a distinctlynoisier endeavor, sequences of frames repeated be-tween the two audio clips will still produce approx-imate diagonal lines in the visualization of the ma-trix.
The search for matched regions thus reducesto the robust search for diagonal line segments inK, which can be efficiently performed with standardimage processing techniques.Included in this procedure is the application of adiagonal median filter of duration ?
seconds.
Thechoice of ?
determines an approximate thresholdon the duration of the matched regions discovered.Large ?
values (?
1 sec) will produce a relativelysparse list of matches corresponding to long wordsor short phrases; smaller ?
values (< 0.5 sec)will admit shorter words and syllables that may beless informative from a document analysis perspec-tive.
Given the approximate nature of the procedure,shorter ?
values also admit less reliable matches.3.2 Posteriorgram RepresentationThe acoustic dotplot technique can operate on anyvector time series representation of the speech sig-nal, including a standard spectrogram.
However, atthe individual frame level, the cosine similarities be-tween frequency spectra of distinct speakers produc-ing the same phoneme are not guaranteed to be high.462Time (s)Phone0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8silaaaeahaoawaxaxraybchddhehelemenereyfghhihiyjhklmnngowoyprsshtthuhuwvwyzzh0.10.20.30.40.50.60.70.80.9Figure 3: An example of a posteriorgram.Thus, to perform term discovery across a multi-speaker corpus, we require a speaker-independentrepresentation.
Phonetic posteriorgrams are a suit-able choice, as each frame is represented as the pos-terior probability distribution over a set of speechsounds given the speech observed at the particularpoint in time, which is largely speaker-independentby construction.
Figure 3 shows an example poste-riorgram for the utterance ?I had to do that,?
com-puted with a multi-layer perceptron (MLP)-basedEnglish phonetic acoustic model (see Section 5 fordetails).
Each row of the figure represents the pos-terior probability of the given phone as a function oftime through the utterance and each column repre-sents the posterior distribution over the phone set atthat particular point in time.The construction of speaker independent acous-tic models typically requires a significant amount oftranscribed speech.
Our proposed strategy is to em-ploy a speaker independent acoustic model trainedin a high resource language or domain to interpretmulti-speaker data in the zero resource target set-ting.2 Indeed, we do not need to know a languageto detect when a word of sufficient length has beenrepeated in it.3 By computing cosine similarities2A similarly-minded approach was taken in Hazen et al(2007) and extended in Hazen and Margolis (2008), where theauthors use Hungarian phonetic trigrams features to character-ize English spoken documents for a topic classification task.3While in this paper our acoustic model is based on our eval-uation corpus, this is not a requirement of our approach.
Futurework will investigate performance of other acoustic models.of phonetic posterior distribution vectors (as op-posed to reducing the speech to a one-best phonetictoken sequence), the phone set used need not bematched to the target language.
With this approach,a speaker-independent model trained on the phoneset of a reference language may be used to performspeaker independent term discovery in any other.In addition to speaker independence, the use ofphonetic posteriorgrams introduces representationalsparsity that permits efficient dotplot computationand storage.
Notice that the posteriorgram dis-played in Figure 3 consists of mostly near-zero val-ues.
Since cosine similarity (Equation 1) betweentwo frames can only be high if they have significantmass on the same phone, most comparisons need notbe made.
Instead, we can apply a threshold and storeeach posteriorgram as an inverted file, performinginner product multiplies and adds only when theycontribute.
Using a grid of approximately 100 cores,we were able to perform theO(n2) dotplot computa-tion and line segment search for 60+ hours of speech(corresponding to a 500 terapixel dotplot) in approx-imately 5 hours.Figure 2 displays the posteriorgram dotplot for8 seconds of speech against itself (i.e., x = y).The prominent main diagonal line results from self-similarity, and thus is ignored in the search.
Theboxed diagonal line segment results from two dis-tinct occurrences of the term one million dollars.The large black boxes in the image result fromlong stretches of silence of filled pauses; fortunately,these are easily filtered with speech activity detec-tion or simple measures of posteriorgram stability.4 Creating Pseudo-TermsSpoken documents will be represented as bags ofpseudo-terms, where pseudo-terms are computedfrom acoustic repetitions described in the previoussection.
Let M be a set of matched regions (m),each consisting of a pair of speech intervals con-tained in the corpus (m = [t(i)1 , t(i)2 ], [t(j)1 , t(j)2 ] indi-cates the speech from t(i)1 to t(i)2 is an acoustic matchto the speech from t(j)1 to t(j)2 ).
If a particular termoccurs k times, the setM can include as many as(k2)distinct elements corresponding to that term, so werequire a procedure to group them into clusters.
Wecall the resulting clusters pseudo-terms since each463cluster is a placeholder for a term (word or phrase)spoken in the collection.
Given the match list Mand the pseudo-term clusters, it is relatively straight-forward to represent spoken documents as bags ofpseudo-terms.To perform this pseudo-term clustering we repre-sented matched regions as vertices in a graph withedges representing similarities between these re-gions.
We employ a graph-clustering algorithm thatextracts connected components.
Let G = (V,E) bean unweighted, undirected graph with vertex set Vand edge set E. Each vi ?
V corresponds to a sin-gle speech interval ([t(i)1 , t(i)2 ]) present in M (eachm ?M has a pair of such intervals, so |V | = 2|M|)and each eij ?
E is an edge between vertex vi andvj .The set E consists of two types of edges.
Thefirst represents repeated speech at distinct pointsin the corpus as determined by the match list M.The second represents near-identical intervals in thesame utterance (i.e.
the same speech) since a sin-gle interval can show up in several matches in Mand the algorithm in Section 3 explicitly ignoresself-similarity.
Given the intervals [t(i)1 , t(i)2 ] and[t(j)1 , t(j)2 ] contained in the same utterance and withcorresponding vertices vi, vj ?
V , we introducean edge eij if fractional overlap fij exceeds somethreshold ?
, where fij = max(0, rij) andrij =(t(i)2 ?
t(i)1 ) + (t(j)2 ?
t(j)1 )max(t(i)2 , t(j)2 )?min(t(i)1 , t(j)1 )?
1.
(2)From the graph G, we produce one pseudo-termfor each connected component.
More sophisticatededge weighting schemes would likely provide ben-efit.
In particular, we expect improved clusteringby introducing weights that reflect acoustic sim-ilarity between match intervals, rather than rely-ing solely upon the term discovery algorithm tomake a hard decision.
Such confidence weightswould allow even shorter pseudo-terms to be con-sidered (by reducing ?)
without greatly increasingfalse alarms.
With such a shift, more sophisticatedgraph-clustering mechanisms would be warranted(e.g.
Clauset et al (2004)).
We plan to pursue thisin future work.Counts Terms5 keep track of5 once a month2 life insurance2 capital punishment9 paper; newspaper3 talking to youTable 1: Pseudo-terms resulting from a graph clusteringof matched regions (?
= 0.75, ?
= 0.95).
Counts indi-cate the number of times the times the pseudo-terms ap-pear across 360 conversation sides in development data.Table 1 contains several examples of pseudo-terms and the matched regions included in eachgroup.
The orthographic forms are taken from thetranscripts in the data (see Section 5).
Note that forsome pseudo-terms, the words match exactly, whilefor others, the phrases are distinct but phoneticallysimilar.
However, even in this case, there is oftensubstantial overlap in the spoken terms.5 DataFor our experiments we used the SwitchboardTelephone Speech Corpus (Godfrey et al, 1992).Switchboard is a collection of roughly 2,400 two-sided telephone conversations with a single partici-pant per side.
Over 500 participants were randomlypaired and prompted with a topic for discussion.Each conversation belongs to one of 70 pre-selectedtopics with the two sides restricted to separate chan-nels of the audio.To develop and evaluate our methods, we cre-ated three data sets from the Switchboard corpus: adevelopment data set, a held out tuning dataset and an evaluation data set.
The develop-ment data set was created by selecting the six mostcommonly prompted topics (recycling, capital pun-ishment, drug testing, family finance, job benefits,car buying) and randomly selecting 60 sides of con-versations evenly across the topics (total 360 con-versation sides.)
This corresponds to 35.7 hours ofaudio.
Note that each participant contributed at mostone conversation side per topic, so these 360 conver-sation sides represent 360 distinct speakers.
All al-gorithm development and experimentation was con-ducted exclusively on the development data.For the tuning data set, we selected an additional60 sides of conversations evenly across the same sixtopics used for development, for a total of 360 con-464versations and 37.5 hours of audio.
This data wasused to validate our experiments on the develop-ment data by confirming the heuristic used to selectalgorithmic parameters, as described below.
Thisdata was not used for algorithm development.
Theevaluation data set was created once parameters hadbeen selected for a final evaluation of our methods.We selected this data by sampling 100 conversationsides from the next six most popular conversationtopics (family life, news media, public education,exercise/fitness, pets, taxes), yielding 600 conversa-tion sides containing 61.6 hours of audio.In our experiments below, we varied the matchduration ?
between 0.6 s and 1.0 s and the overlapthreshold ?
between 0.75 and 1.0.
We measured theresulting effects on the number of unique pseudo-terms generated by the process.
In general, de-creasing ?
results in more matched regions increas-ing the number of pseudo-terms.
Similarly, increas-ing ?
forces fewer regions to be merged, increasingthe total number of pseudo-terms.
Table 2 showshow these parameters change the number of pseudo-terms (features) per document and the average num-ber of occurrences of each pseudo-term.
The usercould tune these parameters to select pseudo-termsthat were long and occurred in many documents.
Inthe next sections, we consider how these parameterseffect performance of various learning settings.To provide the requisite speaker independentacoustic model, we compute English phone pos-teriorgrams using the multi-stream multi-layerperceptron-based architecture of Thomas et al(2009), trained on 300 hours of conversational tele-phone speech.
While this is admittedly a largeamount of supervision, it is important to emphasizeour zero resource term discovery algorithm does notrely on the phonetic interpretability of this refer-ence acoustic model.
The only requirement is thatthe same target language phoneme spoken by dis-tinct speakers map to similar posterior distributionsover the reference language phoneme set.
Thus,even though we evaluate the system on matched-language Switchboard data, it can be just as easilyapplied to any target language with no language-specific knowledge or training resources required.44The generalization of the speaker independence of acous-tic models across languages is not well understood.
Indeed, theperformance of our proposed system would depend to some ex-?
?
Features Feat.
Frequency Feat./Doc.0.6 0.75 5,809 2.15 34.70.6 0.85 23,267 2.22 143.40.6 0.95 117,788 2.38 779.80.6 1.0 333,816 2.32 2153.40.75 0.75 8,236 2.31 52.80.75 0.85 18,593 2.36 121.70.75 0.95 48,547 2.36 318.20.75 1.0 90,224 2.18 546.90.85 0.75 5,645 2.52 39.50.85 0.85 8,832 2.44 59.80.85 0.95 15,805 2.24 98.30.85 1.0 24,480 2.10 142.41.0 0.75 1,844 2.39 12.31.0 0.85 2,303 2.24 14.41.0 0.95 3,239 2.06 18.61.0 1.0 4,205 1.93 22.7Table 2: Statistics on the number of features (pseudo-terms) generated for different settings of the match dura-tion ?
and the overlap threshold ?
.6 Document ClusteringWe begin by considering document clustering, apopular approach to discovering latent structure indocument collections.
Unsupervised clustering al-gorithms sort examples into groups, where eachgroup contains documents that are similar.
A userexploring a corpus can look at a few documents ineach cluster to gain an overview of the content dis-cussed in the corpus.
For example, clustering meth-ods can be used on search results to provide quickinsight into the coverage of the returned documents(Zeng et al, 2004).Typically, documents are clustered based on a bagof words representation.
In the case of clusteringconversations in our collection, we would normallyobtain a transcript of the conversation and then ex-tract a bag of words representation for clustering.The resulting clusters may represent topics, such asthe six topics used in our switchboard data.
Suchgroupings, available with no topic labeled trainingdata, can be a valuable tool for understanding thecontents of a speech data collection.
We would liketo know if similar clustering results can be obtainedwithout the use of a manual or automatic transcript.In our case, we substitute the pseudo-terms discov-ered in a conversation for the transcript, representingtent on the phonetic similarity of the target and reference lan-guage.
Unsupervised learning of speaker independent acousticmodels remains an important area of future research.465the document as a bag of pseudo-terms instead of ac-tual words.
Can a clustering algorithm achieve sim-ilar results along topical groups with our transcript-free representation as it can with a full transcript?In our experiments, we use the six topic labelsprovided by Switchboard as the clustering labels.The goal is to cluster the data into six balancedgroups according to these topics.
While Switch-board topics are relatively straightforward to iden-tify since the conversations were prompted with spe-cific topics, we believe this task can still demon-strate the effectiveness of our representation relativeto the baseline methods.
After all, topic classifica-tion without ASR is still a difficult task.6.1 Evaluation MetricsThere are numerous approaches to evaluating clus-tering algorithms.
We consider several methods: Pu-rity, Entropy and B-Cubed.
For a full treatment ofthese metrics, see Amigo?
et al (2009).Purity measures the precision of each cluster, i.e.,how many examples in each cluster belong to thesame true topic.
Purity ranges between zero and one,with one being optimal.
While optimal purity can beobtained by putting each document in its own clus-ter, we fix the number of clusters in all experimentsso purity numbers are comparable.
The purity of acluster is defined as the largest percentage of exam-ples in a cluster that have the same topic label.
Purityof the entire clustering is the average purity of eachcluster:purity(C,L) =1N?ci?Cmaxlj?L|ci ?
lj | (3)where C is the clustering, L is the reference label-ing, and N are the number of examples.
Followingthis notation, ci is a specific cluster and lj is a spe-cific true label.Entropy measures how the members of a clusterare distributed amongst the true labels.
The globalmetric is computed by taking the weighted aver-age of the entropy of the members of each cluster.Specifically, entropy(C,L) is given by:?
?ci?CNiN?lj?LP (ci, lj) log2 P (ci, lj) (4)where Ni is the number of instances in cluster i,P (ci, lj) is the probability of seeing label lj in clus-ter ci and the other variables are defined as above.B-Cubed measures clustering effectiveness fromthe perspective of a user?s inspecting the clusteringresults (Bagga and Baldwin, 1998).
B-Cubed preci-sion can be defined as an algorithm as follows: sup-pose a user randomly selects a single example.
Shethen proceeds to inspect every other example thatoccurs in the same cluster.
How many of these itemswill have the same true label as the selected exam-ple (precision)?
B-Cubed recall operates in a sim-ilar fashion, but it measures what percentage of allexamples that share the same label as the selectedexample will appear in the selected cluster.
Since B-Cubed averages its evaluation over each documentand not each cluster, it is less sensitive to small er-rors in large clusters as opposed to many small errorsin small clusters.
We include results for B-CubedF1, the harmonic mean of precision and recall.6.2 Clustering AlgorithmsWe considered several clustering algorithms: re-peated bisection, globally optimal repeated bisec-tion, and agglomerative clustering (see Karypis(2003) for implementation details).
Each bisectionalgorithm is run 10 times and the optimal clusteringis selected according to a provided criteria function(no true labels needed).
For each clustering method,we evaluated several criteria functions.
Addition-ally, we considered different scalings of the featurevalues (the number of times the pseudo-terms ap-pear in each document).
We found that scaling eachfeature by the inverse document frequency, effec-tively TFIDF, produced the best results, so we usethat scaling in all of our experiments.
We also ex-plored various similarity metrics and found cosinesimilarity to be the most effective.We used the Cluto clustering library for all clus-tering experiments (Karypis, 2003).
In the followingsection, we report results for the optimal clusteringconfiguration based on experiments on the develop-ment data.6.3 BaselinesWe compared our pseudo-term feature set perfor-mance to two baselines: (1) Phone Trigrams and466(2) Word Transcripts.
The Phone Trigram base-line is derived automatically using an approach sim-ilar to Hazen et al (2007).
This baseline is basedon a vanilla phone recognizer on top of the sameMLP-based acoustic model (see Section 5 and thereferences therein for details) used to discover thepseudo-terms.
In particular, the phone posterior-grams were transformed to frame-level monophonestate likelihoods (through division by the frame-level priors).
These state likelihoods were then usedalong with frame-level phone transition probabilitiesto Viterbi decode each conversation side.
It is impor-tant to emphasize that the reliability of phone recog-nizers depends on the phone set matching the appli-cation language.
Using the English acoustic modelin this manner on another language will significantlydegrade the performance numbers reported below.The Word Transcript baseline starts with Switch-board transcripts.
This baseline serves as an upperbound of what large vocabulary recognition can pro-vide for this task.
n-gram features are computedfrom the transcript.
Performance is reported sepa-rately for unigrams, bigrams and trigrams.6.4 ResultsTo optimize parameter settings, match duration (?
)and overlap threshold (? )
were swept over a widerange (0.6 < ?
< 1.0 and 0.75 < ?
< 1.0) using avariety of clustering algorithms and training criteria.Initial results on development data showed promis-ing performance for the default I2 criteria in Cluto(repeated bisection set to maximize the square rootof within cluster similarity).
Representative resultson development data with various parameter settingsfor this clustering configuration appear in Table 3.A few observations about results on developmentdata.
First, the three evaluation metrics are stronglycorrelated.
Second, for each ?
the same narrowrange of ?
values achieve good results.
In general,settings of ?
> 0.9 were all comparable.
Essen-tially, setting a high threshold for merging matchedregions was sufficient without further tuning.
Third,we observed that decreasing ?
meant more features,but that these additional features did not necessarilylead to more useful features for clustering.
For ex-ample, ?
= 0.70 gave a small number of reasonablygood features, while ?
= 0.60 can give an order ofmagnitude more features without much of a changePseudo-term Results?
?
Features Purity Entropy B3 F10.60 0.95 117,788 0.9639 0.2348 0.93060.60 0.96 143,299 0.9750 0.1664 0.95180.60 0.97 178,559 0.9667 0.2116 0.93660.60 0.98 223,511 0.9528 0.2717 0.91330.60 0.99 333,630 0.9583 0.2641 0.92100.60 1.0 333,816 0.9583 0.2641 0.92100.70 0.93 58,303 0.9528 0.3114 0.91050.70 0.94 66,054 0.9667 0.2255 0.93580.70 0.95 74,863 0.9583 0.2669 0.92100.70 0.96 86,070 0.9611 0.2529 0.92600.70 0.97 100,623 0.9639 0.2326 0.93120.70 0.98 117,535 0.9556 0.2821 0.91580.70 0.99 161,219 0.9056 0.4628 0.83720.70 1.0 161,412 0.9333 0.4011 0.8760Phone Recognizer BaselineType Features Purity Entropy B3 F1Phone Trigram 28,110 0.6194 1.3657 0.5256Manual Word Transcript BaselinesType Features Purity Entropy B3 F1Word Unigram 7,330 0.9917 0.0559 0.9839Word Bigram 74,216 0.9833 0.1111 0.9678Word Trigram 224,934 0.9889 0.0708 0.9787Table 3: Clustering results on development data usingglobally optimal repeated bisection and I2 criteria.
Thebest results over the manual word transcript baselinesand for each match duration (?)
are highlighted in bold.Pseudo-term results are better than the phonetic baselineand almost as good as the transcript baseline.in clustering performance.
Finally, while pseudo-term results are not as good as with the manualtranscripts (unigrams), they achieve similar results.Compared with the phone trigram features deter-mined by the phone recognizer output, the pseudo-terms perform significantly better.
Note that thesetwo automatic approaches were built using the iden-tical MLP-based phonetic acoustic model.We sought to select the optimal parameter settingsfor running on the evaluation data using the devel-opment data and the held out tuning data.
We de-fined the following heuristic to select the optimal pa-rameters.
We choose settings for ?, ?
and the clus-tering parameters that independently maximize theperformance averaged over all runs on developmentdata.
We then selected the single run correspond-ing to these parameter settings and checked the re-sult on the held out tuning data.
This setting wasalso the best performer on the held out set, so weused these parameters for evaluation.
The best per-forming parameters were globally optimal repeated467?
?
Features Purity Entropy B3 F10.70 0.98 123,901 0.9778 0.1574 0.9568Phone Trigram 28,374 0.6389 1.2345 0.5513Word Unigram 7,640 0.9972 0.0204 0.9945Word Bigram 77,201 0.9972 0.0204 0.9945Word Trigram 233,744 0.9972 0.0204 0.9945Table 4: Results on held out tuning data.
The parameters(globally optimal repeated bisection clustering with I2criteria, ?
= 0.70 seconds and ?
= 0.98) were selectedusing the development data and validated on tuning data.Note that the clusters produced by each manual transcripttest were identical in this case.?
?
Features Purity Entropy B3 F10.70 0.98 279,239 0.9517 0.3366 0.9073Phone Trigram 31,502 0.7000 1.0496 0.6355Word Unigram 9939 0.9883 0.0831 0.9772Word Bigram 110,859 0.9883 0.0910 0.9771Word Trigram 357,440 0.9900 0.0775 0.9803Table 5: Results on evaluation data.
The parameters(globally optimal repeated bisection clustering with I2criteria, ?
= 0.7 seconds and ?
= 0.98) were selectedusing the development data and validated on tuning data.bisection clustering with I2 criteria, ?
= 0.7 s and?
= 0.98.
Note that examining Table 3 alone maysuggest other parameters, but we found our selectionmethod to yield optimal results on the tuning data.Results on held out tuning and evaluation data forthis setting compared to the manual word transcriptsand phone recognizer output are shown in Tables4 and 5.
On both the tuning data and evaluationdata, we obtain similar results as on the developmentdata.
While the manual transcript baseline is bet-ter than our pseudo-term representations, the resultsare quite competitive.
This demonstrates that use-ful clustering results can be obtained without a full-blown word recognizer.
Notice also that the pseudo-term performance remains significantly higher thanthe phone recognizer baseline on both sets.7 Supervised Document ClassificationUnsupervised clustering methods are attractive sincethey require no human annotations.
However, ob-taining a few labeled examples for a simple label-ing task can be done quickly, especially with crowdsourcing systems such as CrowdFlower and Ama-zon?s Mechanical Turk (Snow et al, 2008; Callison-Burch and Dredze, 2010).
In this setting, a usermay listen to a few conversations and label them bytopic.
A supervised classification algorithm can thenbe trained on these labeled examples and used to au-tomatically categorize the rest of the data.
In thissection, we evaluate if supervised algorithms can betrained using the pseudo-term representation of thespeech.We set up a multi-class supervised classificationtask, where each document is labeled using one ofthe six Switchboard topics.
A supervised learningalgorithm is trained on a sample of labeled docu-ments and is then asked to label some test data.
Re-sults are measured in terms of accuracy.
Since thedocuments are a balanced sample of the six topics,random guessing would yield an accuracy of 0.1667.We proceed as with the clustering experiments.We evaluate different representations for various set-tings of ?
and ?
and different classifier parameterson the development data.
We then select the opti-mal parameter settings and validate this selection onthe held out tuning data, before generating the finalrepresentations for the evaluation once the optimalparameters have been selected.For learning we require a multi-class classifiertraining algorithm.
We evaluated four popularlearning algorithms: a) MIRA?a large margin on-line learning algorithm (Crammer et al, 2006); b)Confidence Weighted (CW) learning?a probabilis-tic large margin online learning algorithm (Dredzeet al, 2008; Crammer et al, 2009); c) Maxi-mum Entropy?a log-linear discriminative classi-fier (Berger et al, 1996); and d) Support Vec-tor Machines (SVM)?a large margin discriminator(Joachims, 1998).5 For each experiment, we useddefault settings of the parameters (tuning did not sig-nificantly change the results) and 10 online iterationsfor the online methods (MIRA, CW).
Each reportedresult is based on 10-fold cross validation.Table 6 shows results for various parameter set-tings and the four learning algorithms on develop-ment data.
As before, we observe that values for?
> 0.9 tend to do well.
The CW learning algo-rithm performs the best on this data, followed byMaximum Entropy, MIRA and SVM.
The optimal?
for classification is 0.75, close to the 0.7 valueselected in clustering.
As before, pseudo-terms do5We used the ?variance?
formulation with k = 1 for CWlearning, Gaussian regularization for the Maximum Entropyclassifier, and a linear kernel for the SVM.468?
?
MaxEnt SVM CW MIRA0.60 0.99 0.8972 0.6944 0.8667 0.89720.60 1.0 0.8972 0.6944 0.8639 0.89440.70 0.97 0.9000 0.7722 0.8500 0.80560.70 0.98 0.8806 0.7417 0.8917 0.81670.70 0.99 0.9000 0.6556 0.9194 0.90560.70 1.0 0.8917 0.6556 0.9194 0.90830.75 0.94 0.8778 0.7806 0.8639 0.80560.75 0.95 0.8778 0.7694 0.8889 0.81110.75 0.96 0.9028 0.7778 0.9000 0.87780.75 0.97 0.9111 0.7722 0.9250 0.92780.75 0.98 0.9056 0.7417 0.9194 0.91670.85 0.85 0.8639 0.7833 0.8500 0.81670.85 0.90 0.8611 0.7528 0.8611 0.85830.85 0.91 0.8389 0.7500 0.8722 0.85560.85 0.92 0.8528 0.7222 0.8944 0.8556Phone Trigram 0.6111 0.7139 0.9138 0.5000Word Unigram 0.9472 0.8861 0.9861 0.9306Word Bigram 0.9250 0.8833 0.9917 0.9278Word Trigram 0.9278 0.8611 0.9889 0.9222Table 6: The top 15 results (measured as average accu-racy across the 4 algorithms) for pseudo-terms on de-velopment data.
The best pseudo-term and manual tran-script results for each algorithm are bolded.
All resultsare based on 10-fold cross validation.
Pseudo-term re-sults are better than the phonetic baseline and almost asgood as the transcript baseline.well, though not as well as the upper bound basedon manual transcripts.
The performance for pseudo-terms and phone trigrams are roughly comparable,though we expect pseudo-terms to be more robustacross languages.Using the same selection heuristic as in cluster-ing, we select the optimal parameter settings, vali-date them on the held out tuning data, and computeresults on evaluation data.
The best performing con-figuration was for ?
= 0.75 seconds and ?
= 0.97.Notice these parameters are very similar to the bestparameters selected for clustering.
Results on heldout tuning and evaluation data for this setting com-pared to the manual transcripts are shown in Tables7 and 8.
As with clustering, we see good overallperformance as compared with manual transcripts.While the performance drops, results suggest thatuseful output can be obtained without a transcript.8 ConclusionsWe have presented a new strategy for applying stan-dard NLP tools to speech corpora without the aidof a large vocabulary word recognizer.
Built in-stead on top of the unsupervised discovery of term-?
?
MaxEnt SVM CW MIRA0.75 0.97 0.8722 0.7389 0.8972 0.8750Phone Trigram 0.7167 0.6972 0.9056 0.5083Word Unigram 0.9500 0.9056 0.9806 0.9250Word Bigram 0.9444 0.9111 0.9833 0.9250Word Trigram 0.9417 0.8972 0.9778 0.9250Table 7: Results on held out tuning data.
The parameters(?
= 0.75 seconds and ?
= 0.97) were selected using thedevelopment data and validated on tuning data.
All re-sults are based on 10-fold cross validation.
Pseudo-termresults are very close to the transcript baseline and oftenbetter than the phonetic baseline.?
?
MaxEnt SVM CW MIRA0.75 0.97 0.8683 0.7167 0.7850 0.7150Phone Trigram 0.8600 0.7750 0.9183 0.6233Word Unigram 0.9533 0.9317 0.9850 0.9267Word Bigram 0.9467 0.9200 0.9900 0.9367Word Trigram 0.9383 0.9233 0.9817 0.9367Table 8: Results on evaluation data.
The parameters(?
= 0.75 seconds and ?
= 0.97) were selected using thedevelopment data and validated on tuning data.
All re-sults are based on 10-fold cross validation.
Pseudo-termresults are very close to the transcript baseline and oftenbetter than the phonetic baseline.like units in the speech, we perform unsupervisedtopic clustering as well as supervised classificationof spoken documents with performance approachingthat achieved with the manual word transcripts, andgenerally matching or exceeding that achieved witha phonetic recognizer.
Our study identified severalopportunities and challenges in the development ofNLP tools for spoken documents that rely on littleor no linguistic resources such as dictionaries andtraining corpora.ReferencesEnrique Amigo?, Julio Gonzalo, Javier Artiles, and FelisaVerdejo.
2009.
A comparison of extrinsic clusteringevaluation metrics based on formal constraints.
Infor-mation Retrieval, 12(4).A.
Bagga and B. Baldwin.
1998.
Entity-based cross-document coreferencing using the vector space model.In Proceedings of the 17th international conference onComputational linguistics-Volume 1, pages 79?85.
As-sociation for Computational Linguistics.A.L.
Berger, V.J.D.
Pietra, and S.A.D.
Pietra.
1996.
Amaximum entropy approach to natural language pro-cessing.
Computational Linguistics, 22(1):39?71.Chris Callison-Burch and Mark Dredze.
2010.
Creatingspeech and language data with Amazon?s Mechanical469Turk.
In Workshop on Creating Speech and LanguageData With Mechanical Turk at NAACL-HLT.K.
W. Church and J. I. Helfman.
1993.
Dotplot: Aprogram for exploring self-similarity in millions oflines of text and code.
Journal of Computational andGraphical Statistics.Aaron Clauset, Mark E J Newman, and CristopherMoore.
2004.
Finding community structure in verylarge networks.
Physical Review E, 70.Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer.
2006.
Online passive-aggressive algorithms.
Journal of Machine LearningResearch (JMLR).Koby Crammer, Mark Dredze, and Alex Kulesza.
2009.Multi-class confidence weighted algorithms.
In Em-pirical Methods in Natural Language Processing(EMNLP).Mark Dredze, Koby Crammer, and Fernando Pereira.2008.
Confidence-weighted linear classification.In International Conference on Machine Learning(ICML).Alvin Garcia and Herbert Gish.
2006.
Keyword spottingof arbitrary words using minimal speech resources.
InICASSP.J.J.
Godfrey, E.C.
Holliman, and J. McDaniel.
1992.SWITCHBOARD: Telephone speech corpus for re-search and development.
In ICASSP.Timothy J. Hazen and Anna Margolis.
2008.
Discrimi-native feature weighting using MCE training for topicidentification of spoken audio recordings.
In ICASSP.Timothy J. Hazen, Fred Richardson, and Anna Margo-lis.
2007.
Topic identification from audio recordingsusing word and phone recognition lattices.
In IEEEWorkshop on Automatic Speech Recognition and Un-derstanding.Aren Jansen, Kenneth Church, and Hynek Hermansky.2010.
Towards spoken term discovery at scale withzero resources.
In Interspeech.T.
Joachims.
1998.
Text categorization with supportvector machines: Learning with many relevant fea-tures.
In European Conference on Machine Learning(ECML).George Karypis.
2003.
CLUTO: A software package forclustering high-dimensional data sets.
Technical Re-port 02-017, University of Minnesota, Dept.
of Com-puter Science.Igor Malioutov, Alex Park, Regina Barzilay, and JamesGlass.
2007.
Making Sense of Sound: UnsupervisedTopic Segmentation Over Acoustic Input.
In ACL.Scott Novotney and Richard Schwartz.
2009.
Analysisof low-resource acoustic model self-training.
In Inter-speech.Alex Park and James R. Glass.
2008.
Unsupervised pat-tern discovery in speech.
IEEE Transactions of Audio,Speech, and Language Processing.R.
Snow, B. O?Connor, D. Jurafsky, and A.Y.
Ng.
2008.Cheap and fast?but is it good?
: Evaluating non-expert annotations for natural language tasks.
In Pro-ceedings of the Conference on Empirical Methods inNatural Language Processing, pages 254?263.
Asso-ciation for Computational Linguistics.S.
Thomas, S. Ganapathy, and H. Hermansky.
2009.Phoneme recognition using spectral envelope andmodulation frequency features.
In Proc.
of ICASSP.H.J.
Zeng, Q.C.
He, Z. Chen, W.Y.
Ma, and J. Ma.
2004.Learning to cluster web search results.
In Conferenceon Research and development in information retrieval(SIGIR).470
