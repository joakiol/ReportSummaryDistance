LINGUISTIC PROCESSING USING A DEPENDENCY STRUCTURE GRAMMARFOR SPEECH RECOGNITION AND UNDERSTANDINGSho-lchl MATSUNAGANTT Human Interface LaboratoriesMusashino, To;?yo, 180, JapanandMasaki KOHDAUniversity of YamagataYonezawa, Yamagata, 922, JapanAbstractThis paper proposes an efficient l inguisticprocessing strategy for speech recognition andunderstanding using a dependency structure grammar.The strategy includes parsing and phrase predictionalgorithms.
After speech processing and phraserecognition based on phoneme recognition, the parserextracts the sentence with the best likelihood takingaccount of the phonetic l ikelihood of phrasecandidates and the linguistic likelihood of thesemantic inter-phrase dependency relationships.
Afast parsing algorithm using breadth-first search isalso proposed.
The predictor pre-selects the p}~.asecandidates using transition rules combined with adependency structure to reduce the amount of phoneticprocessing.
The proposed linguistic processor hasbeen tested through speech recognition experiments.The experimental results show that it greatlyincreases the accuracy of speech recognitions, andthe breadth-first parsing algorithm and predictorincrease processing speed.1.
IntroductionIn conventional continuous speech recognition andunderstanding systems\[1~4\], linguistic rules forsentences composed of phrase sequences are usuallyexpressed by a phrase structure grammar such as atransition network or context free grammar.
In suchmethods, however, phoneme recognition errors andrejections result in incorrect transition statesbecause of the strong syntactical constraints.These erroneous transitions cause the followingcandidates to be incorrectly chosen or the processingsystem to halt.
Therefore, these errors andrejections can be fatal to speech understanding.Furthermore, a complete set of these grammaticalrules for speech understanding is very difficult toprovide.To address these problems, this paper proposes anew linguistic processor based on a dependencystructure grammar, which integrates a bottom~upsentence parser and a top-down phrase predictor.
~lisgrammar is more semantic and less syntactic thanphrase structure grammar, and, therefore, syntacticpositional constraint in a sentence rarely occurswith this parser.
This effectively prevents extremedegradation in speech recognition from errors andrejections in phoneme recognit ion and greatlyincreases the accuracy of speech processing.
Thisgrammar only has two syntactic rules, so this parseris free of many cumbersome grammatical rules that areindispensable to other grammars.
This grammarparticularly suits phrase-order-free languages suchas Japanese.4a~rule Irule 2For the parser of this grammar, a depth-firstparsing algorithm with backtracking which guaranteesthe optimal solution was devised\[5\].
However, parsinglong sentences composed of many phrases with thisalgorithm can be t ime-consuming because ofcombinatorial  explosion, since the amount ofcomputation is exponential order with respect to thenumber of phrases.
Therefore, a fast parsingalgorithm using breadth-first search and beam searchwas developed.
This algorithm is based on flmdamentalalgorithms\[6,7\] which only take account of thedependency relationships of the modif ier andmodificant phrases, and it handles higher linguisticor semantic processing such as case structure.
Theprocessing ability of this breadth-first algorithm isequivalent to that of the depth-first algorithm.To effectively recognize speech, the amount ofphonetic processing must be reduced through top-downprediction of hypotheses.
However, top-down centre\]using the principal dependency structure isimpossible.
To solve this problem, this novel phrasepredictor was devised.
This predictor pre-selectshypotheses for phoneme recognition using predictionrules, and then it reduces the amount of phoneticprocessing.
Prediction rules are created byintegrating connection rules and phrase dependencystructures.The effeetivenessof this linguistic processingwas ascertained through speech recognitionexperiments.2.
L ingu is t i c  p rocessor2.
1 Dependency ntJcueturegrammarThis grammar is based on semantic dependencyrelationships between phrases.
The syntactic rulessatisfy the following two constraints.
First, anyphrase, except the last phrase of a sentence, canmodify only one later phrase.
Each modification,called a dependency relationship or dependenc 2structure, can be represented by one arc.
Second,modification arcs between phrases must not cross.These rules are i l lustrated in Fig.
I.
In twounacceptable sentences, one sentence is unacceptablebecause one phrase modifies the former phrase, andAcceptable sentence ~ ~ a ~ l e  sentenceA B C DA B C D?A B C D?A B C DFig.
1.
Examples using a dependency structure grammarA,B,C and D are sentence phrases.the ether sentence is unacceptable because arcs crossin its dependency structures.2.
2ParserAfter phonetic phrase recognition, recognitionresults are represented in a phonetic score matrixform as shown in Fig.
2.
When analyzing dependencyrelationships, the parser extracts the most likelysentence in this matrix by taking into account thephonetic \]_ikelihood of phrase candidates and thel inguistic l ikelihood of semantic inter-phrasedependency relationships.
The parser also obtains thedependenqy structure that corresponds to the semanticstructure of the extracted sentence.2.
2.
1 Objective functionThis parsing is equivalent to solving thefollowing objective function using the constraints ofdependency structure grammar.
For simplicity, thefollowing linguistic formulation is described forspeech uttered phrase by phrase.
The process forsentence speech is described in section 4.NT -- max\[ 2;'c(xi ,p) + maxZdep(w I j_1,Xj,piY1,j,p)\] (I)p j::l " Y j=l 'where I~j.<N, 1Kp~M, N is  the number of input phrases,M is the maximum number of phonetic recognitioncandidates for each phrase, Xj,p is a candidate ofthe j~th input phrase with the p-th best phoneticlikelihood, and c(xj,p) is its phonetic likelihood(positive value).
Also, Xi,j, p is a phrase sequencewith one phrase candidate for each i-th to j-th inputphrase and whose last phrase is Xj,p.
Yi,j,p is oneof the dependency structures of Xi,j,p, wi,j_ I is theset of phrases that modify Xj,p in the sequenceXi,j, p. Here, dep(w,xlY) is the linguistic likelihood(negatiw!
value) of dependency relationships betweenw and x taking Y into account.
Namely, the first itemof the teem on the right in Eq.
(I) is the summationof phonetic likelihoods of the hypothesized sentencecomposed of its phrase sequence, and the second itemis the summation of linguistic likelihood.
MaximizingEq.
(1) gives the sentence and the dependencystructure of it as speech recognition andt~dersta~ling results.Because dependency structure grammar is compatiblewith case grammar\[8\], the l inguistic semanticlikelihood(dep) of the dependency structure is easilyprovided using case structure.
The following areexample:~ of items for evaluating dependencyrelationships: the disagreement between the semanticprimitives of the modifier and that requested by themodificant, the lack of the ob\].igatory case phraserequested by the modificant, and the existence ofdifferenb phrases with the same case and modifyingthe same phrase.
The likelihood values for theseitems are given heuristically.To so\]re equation (I), a fast parsing algorithmusing breadth-first search and beam search wasdeveloped.
This algorithm deals with higherlinguistic or semantic processing such as the casestructure..
Although this algorithm offers sub-optimalsolutions, it is practical because it requires lessprocessing than depth-first search.2.
2.
2 l~ce~t~z-firstpe~csingalgorithmThe breadth-first algorithm is formulated asorder of candidates1 2 ...... Mx \ [ , l  Xl, 2 Xl,~4 I 12 x2,1 x2, 2/x3, I xs, 2 x3,MJ BFig.
2.
A matrix of phrase candidatesinpututterance :numberN-1follows.First, dep(w,xlY) can obviously be divided intotwo terms.dep(wl , j -1  ,Xj,plYl, j ,p) =E dep1(x,Xj,p) + dep2(Y1,j,p,Xj,p) (2)x Cw I ,j-1where depl is the l ikelihood associated withdependency relationships of only the modifier andmodificant phrases, and dep2 is the likelihoodassociated with Y1,j,p.
An example of dependencyrelationships is shown in Fig.
3.Eqs.
(I) and (2) give the objective function'swtlue S(1,Xj,p) of a phrase sequence including thetop phrase to Xj,p in the sentence as:S(1,Xj,p) =J J JZ C(Xh,p) + ~ ~ dep1(x,xh,p) +Edep2(Y 1,h,p,xh,p) (3)h::1 h=1 xCw I ,h-1 h=1On the other hand, the value of a phrase sequence notincluding the top phrase (i?I) is defined as:D(i,Xj,p) =j j j-1_Z C(Xh,p) + Z Z depl (X,Xh, p) + Z dep2(Y i ,h ,p ,xh,p)h: : i  h=i xCwi,h_ I h=i (~)Tim main difference between Eqs.
(3) and (4) is thatphrasesequencewh,FIiIIE (A, E, F)depl (F, G)depl (E, Gdepl (A, G', '.-t dep2( YA,Gdep(wA,p.
G i YA,G =dep2( YA.G ' G) +dspl (F, G)+depl (E. G) +depl (A, G)Fig.
3.
Illustration of dependency relationships403dep2(Yi,j,p,Xj,p) is not evaluated in Eq.
(4).Using notation S and D, the recurrence relationamong the objective functions are derived.
This isshown in Fig.
4.
The recurrence relation aretransforms into the following equations using beamsearch.S(1,Xj,p,r) = rth-max\[S(1,Xk,q,rl) + D(k+1,Xj,p,r2)k,q,rl,r2+ dep1(Xk,q,Xj,p) + dep2(Y1,j,p,Xj,p) , if i=I (5')D(i,xj,p,r) = rth-max\[S(i,Xk,q,rl) + D(k+1,Xj,p,r2)k,q,rl,r2+ dep1(xk,q,Xj,p) 4 dep2(Yi,k,q,Xk,q), if i~I (6')where i~k~j-1, 1~q~M, and 1~r, rl, r2<_L.
Here, r, rland r2 indicate the rank of beam, L is the maximumnumber of beams, S(1,Xj,p,r) and D(i,xj,p,r) are ther-th value of the element whose phrase sequence isXi,j, p and the dependency structure is Yi,j,p.ilere, rth-max\[ \] is a function for deriving the r-thbest value.
When Eq.
(5') or (6') is calculated,Yi,j,p is stored for use in the later stage ofevaluating dep2.Initial values are given as follows.S(1,Xl,p,1) = C(Xl, p) + dep2~1,1,p,Xl,p),if i=1(top phrase) (7)D(i,Xi,p,1 ) = e(Xi,p) , if i~I (not top phrase) (8)After calculating the recurrence relation, the valueof the objective functions is obtained,T = max\[S(1,XN,p,1)\] , (9)Pwhere 1~p~M.
The best sentenc~ and its dependencystructure are given through YI,N,p where p maximizesEq.
(9).
The parsing table is shown in Fig.
5 and theparsing algorithm is shown in Table I.
In Fig.
5,the first row corresponds to S, and others correspondto D. The phrase sequence for first to N-th phrasecorresponds to the right-most top cell.
Each cell iscomposed of ML sub-cells.
Arrows show the sequence ofcalculating the recurrence relation.
The processingamount order for this algorithm is O(N3M2L2).Comparing the theoretical amount of processing forthese two parsing algorithms, the breadth-firstparsing algorithm clearly requires much lessprocessingthanthe depth-first parsing algorithm.The amount of processing for each parsing algorithmis shown in Fig.
6.2.
3 PredictorTo pre-select the phrase hypotheses for the speechrecognition, the predictor is devised\[9\], usingprediction rules created by integrating connectionrules and dependency structures of phrases.
Theserules are described with rewriting rules:(Xi,j)->(Xi,k)(Xk+1,j)where Xi, j is the phrase sequence for the i-th to j-th phrase.
(Xi, j) is the sequence with a Closed-dependency-structure where the tail phrase xj has thedependency relationships with phrases out of Xi,j,and other phrases in Xi, j have dependencyrelationships with phrases within Xi, j.
(Xi, j) isdivided into two phrase sequences with the closed-dependency-structure by modifying x k by xj, andfollowing Xi, k by Xk+1, j.
A single phrase x i is also4O4(J) if i= l1F"Phrase no.k ... jdepl( Xk, q, Xj ,p) ~dep2(Y.. , x.S(I, Xk, q)  D(k+l, Xj,pS(1.
X .p .)
=S( l ,  x k q) +\]J(k+l,  x .
)S(I, Xj,p +depl( Xk,q', Xj ,p) +dep2t'P~l,j,p , Xj ,p) (5)?
if i ?
1 Phrase no.i ... k '" j\[ .
.
.
.
.
xUU___~kdepl ( , x j ) Xk.q ,pdep2( Yl,k,q ' Xk,q )D(t, Xk, q) D(k+l, X i ,p)I.-- " >1 F"D(I, x .p )) =D(t,  x k q)  +~(k+l, x,j _ )D(I, Xj,p +depl( Xk,q', Xj,p ) +dep2('~/i,k, q , xk, q ) (6)Fig.
4.
I l lustrat ion of  der iv ing recurrence relat iun amongob ject ive  funct ionsN1 2 ' j o Norder ofcandidates bea~(1, x N ?
.
)Fig.
5.
Conf igurat ion of  a pars ing tableregarded as a phrase sequence with a closed-dependency-structure.
These rules are described forthe sequence, the i-th phrase to j-th phrase modifiedby the i-th phrase, as(Xi,j)->(xi)(Xi+1,j)The hypotheses are predicted as follows.<I> x i is detected as a reliable phrase recognitionresult.
If there are no reliable phrase candidates inthe i-th phrase recognition results, the followingprocedure is not carried out.<2> The rules whose left term is scanned are such as(Xi+l,j)->(Xi+1,k)(Xk+1,j)After the left-most derivation is repeated to detecthypotheses for i+1~thphrase speech recognition, xi+ Iis detected in the following form.
(Xi+1,j)->(xi+1)(Xi+2,h)----(Xk+l,j)Generally, there is more than one (Xi+1,j) , so xi+ Iis a setof phrases.<3> The phrase recognition is carried out for thei+l-th ~\]case utterance whose hypotheses are elementsof the set xi+ I.<4> If the reliable phrase recognition result wasdetected in operation <3>, the rules which derivedelements of xi+ I are scanned again and hypotheses forthe next utterance are derived using same procedureas  <2>.<5> Thes~ operations, namely hypotheses derivationand its phonetic verification, are carried out untilxj is detected.<6> The detected phrase sequence Xi~ j and itsdependency structure Yi,j is passed to the parser.During bhese operations, if the phrase recognitionresults are unreliable in operation <3>, thedetectioa process of Xi, j is halted and phraserecognitSon for all hypotheses is carried out.Althe~,gh Japanese is a phrase-order-free language,there are some relatively fixed please-order parts ina sentence.
These rules are applied to these parts.The number of hypotheses and the amount of acousticprocessing can thus be reduced, maintaining the abovecharacteristics of the dependency structure grammar.By linking the predictor to the parser, parsing canbe accomplished using the dependency structuresdetected in operation <6> of the predictionprocedure.
This linkage method greatly increasesparsing speed.3* Speech recogn i t ion  exporimen%~3- 1St~echrecognibion sys temThe speech recognition and understanding systemis shown in Fig.
7.
The system is composed ofacoustic and linguistic processing stages.
Theacoustic processing stage consists of a featureextraction part and a phoneme recogn i t ionpart\[10,11\].
The linguistic processing stage consistsof a phrase recognition patti11\], a parsing part (adependency relationship analysis part), and a phraseprediction part.
The linguistic processing stage usesa word dictionary, word connection rules for intra-phrase syntax, dependency relationships rules andphrase prediction rules.
The word dictionary iscomposed of pronunciation expressions, parts ofspeech and case structures.
Dependency relationshiprules produce negative eva%~ation values that are setto the dependency relationships contrary to casestructure discipline.3.
2 Speschreeognit ionprocessFor separately uttered phrases, acoustic featureparameters are extracted and bottom-up phonemerecognition is carried out.
The phrase hypotheses fortop-down phoneme recognition are pre-selected by theTable 1.
Parsing algorithm{ 1 } Loop for the end phrase of the partial sequenceDO {2} to {5} for j = 1,2,---,N{2} Loop for the candidateDO {3} to {5} for p = 1,2,---,M\]{3} Setting the initial valueSET S(1,Xl,p,1) or D(j,Xj,p,1) (Eqs.
(7),(8))If J = 1, go back to {2}.
{4} Loop for the beginning phrase of the part ial  sequenceDO {5} for i = 1 -1 j -2 , - - ,1{5} Calculation of reccurence relation< Loop for tlle end phrase of the former sequence >{5-1} DO {5-2} to {5-4} for k = i,i?1,-~-,1-1{5-2} DO {5-3} to {5-4} for q = 1,2,--~,M k< Loop for the beam width >{5-3} DO {5-4} for rl = 1,2,---,L{5-4} DO for r2 = 1,2,---,L* Evaluation of S(1,x 1 n,r) or D(j,x~ n,r) takingaccount of YtZ,p or (Eq,.
* Store of Ytj,p{6} Acquisition of the parsing results* Detect ion of value p maximizing Eq.
(9)* Acquisition of the phrase sequence and its dependencystructure using Y1,N,pI@ ?F--5Io  2omr   ,oloNUMBER OF PHRASES0 enumerative methodA depth-f i rst  methodV breadth- f i rs t  method(L=oo)ill breadth- f i rs t  method(L=8 )# breadth- f l rs t  method(L=l)Fig.
6.
Comparison of  processing amount10 20 30predictor.
The pre-selection is also carried outusing bottom-up phoneme recognition results\[12\].Next, top-down phoneme verification is carried outand phrase recognition results are generated.
Phraserecognition results are represented in the form ofscore matrix with phonetic recognition scoresaveraged for each hypothesized phrase.
When the endof a sentence is detected, the parser extracts thephrases with t~e best sentence likelihood by scanningthis matrix, and determines the dependency structureof the extracted phrases.3- 3 PerformanceThe effectiveness of the proposed linguisticprocessor was tested in speech recognitionexperiments.
The experiments were carried out using20 sentences containing 320 phrases uttered by 2 malespeakers.
These results are shown in Table 2.4O5recognition and understanding resultsI wg\[dd~c~l._o+ary__~--~\] dependency relationships analysls~--~idependency, I case J ~ /\ I relatlonshlpsLstructure ~ hypotheses prediction ~----7 \[ \[~2~ .
.
.
.
.
.
.
.
.
J i ~  .
.
.
.
.
.
.
.
.
.
.
.
1 i ll inguistic I \[pronounciatlon\[ + M /I I processing , expression ~ hypotheses derivation | L_~predic( lon .
.
.
.
.
?..............
i+J- t, \[--  L~uJS~ .
.
.
.
.
.
.
.
.
J ~tntra-phrase \[pre-selec toL~Y~ ........ J ~ phrase@ (word) recognition.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
................................................................................................ bottom-sp honeme ~honeme\ [acoustic recognition recognition __\]processl ng ~\] feature parameter extraction \]input speechFig.
7.
Speech recognition and understanding system,~ 10~102W=8 10 013..000OO OOlb 1~i' "< 2b' '25NUMBER OF PHRASESIN EACH SENTENCElO 4F-  102tl,I~10 o000 00 0 00 00 ~ ~Y~i0 1~i '2'0NUMBER OF PHRASESIN EACH SENTENCE25(a) speaker KIO depth-first method~7 breadth-first method(L=8)(b) speaker KSO depth-first methodbreadth-first method(L=8)Fig.
8.
Comparison of processing time for dependency relationships analysisTable 2.
Speech recognition resultsphonetic recognition Snumber of i phonetichypotheses i processingi tlmeI without predictor 542  i 1.
0' :~ i~hp~i~i~ i : __  ........ i~~ ....... i 6 :~h ........without parserphrase recognitionrate \[~\]0 :  withintop 8 candidates57  (87)60  (89)with parserdepth-flrst parsing breadth-firstparsingphrase i parsingrecognition i timerate \[~\]* Recognition of predicted phrases ( 33~ to the total input phrasesThe proposed parser using the depth-first parsingalgorithm increased phrase recognition rate byapproximately 2OZ (from 57Z without the parser to 77~with the parser).
This result shows the effectivenessof a parser using a dependency structure grammar.The processing time with the breadth-f irstalgorithm was reduced to approximately IZ of thatwith the depth-flrst algorithm for sentence parsing,while keeping the same level of speech recognition77 i l .phrase i parslngrecognition i tlmerate \[~3 i77  ~1.. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.  }
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.78 i O.
89rate as with the depth-first algorithm.
This resultshows the great effectiveness of the breadth-firstparsing algorithm.
This result is shown in Fig.
8 foreach speaker when M is 3 and L is 8.Next, using 26 rules, the prediction was carriedout for 33~ of the total input phrases.
It reducedacoustic processing time to 60Z at these parts in asentence, and it increased speech recognition speed.Finally, linking the predictor to the parser reduced406parsing time to less than 10% of the time for thedepth-first parser, and to approximately 90% of thetime for the breadth-first parser.
This shows theusefulness of the linkage.4- Breadth-firs% parsing a lgor i thm for  sentencespeeehreco~t ionThe breadth-f irst parsing algorithm for thesentence speech or connected phrase speech isdevised\[13\] by the same procedure as in section 2.
2.Based on basic expansion algorithms\[14,15\] fromphrase-wise to sentence speech, the speechrecognition and understanding accuracy using theproposed algorithm is greatly increased compared tothe accuracies using the basic algorithms.
In thesentence speech, phrase recognition results afterphonetic processing are represented in a scorelattice form wi thphonet ic  recognition scoresaveraged.
The parser extracts the best sentencecomposed of a phrase sequence by scanning thislattice.
The processing order is O(NSM2L2), which ispractical amount of computation, where N is thenumber of detected phrase boundaries in the utteredsentence, M is the maximum number of phoneticrecognitio~ candidates for each phrase segment f~emone boundsry to the next boundary, and L is themaximum number of beams.The effectiveness of this parser was testedthrough sentence speech recognition with one speakeruttering i0 sentences containing a total of 67phrases.
This parser increased phrase recognitionperformance in the sentences by approximately 49Z(from 27~ without the parser to 76~ with the parser).5- Conc lus ionThis paper proposed an efficient linguisticprocessing strategy for speech recognition andunderstanding using a dependency structure grammar.This grammar suits processing of phrase-order-freelanguages such as Japanese and processing the resultof front-end speech recognition, which is usuallyerroneous~ This linguistic processing strategyincludes bottom-up parsing and a top-down phrasehypotheses predictor.
In particular, the bottom-upparser, taking account of the phonetic and linguisticlikelihood, greatly increases the accuracy and speedof speech recognition.
The predictor reduces theamount of phonetic processing by pre-selecting thephrase hypotheses.
The effectiveness of this parserand predictor was shown in speech recognitionexperiments.Future development is include the statisticallikelihood of dependency relationships, integrationwith the statistical phonetic method like HiddenMarkov Models, and higher linguistic processing usingthe semantics and context knowledge.AaknowlmlgmentThe authors would like to express theirappreciation to Dr. Kiyoshi Sugiyama and Dr. SadaokiFurui, for their invaluable guidance.
The authorswould also like to thai( to Dr. Kiyohiro Shikano andShigeki Sagayama for their useful suggestions.References\[I\] Leser V.R., et al1975)WOrganization of theHearsay II speech understanding system.
',IEEE Trans.ASSP., 23, I, pp.
11-24.\[2\] Woods W.A.
(1976)'Speech understanding system -Final Report',Tech.
Rep.,3438.\[3\] Levinson S.E.
(1985)'Structural Methods in auto-matic speech recognition',Proceeding of the IEEE, 11,pp.
1625-I 650.\[4\] Ney H.(1987),'Dynamic programming speech recogni-tion using a context-free grammar.'Proc.
1987 ICASSP,pp.
69-72.\[5\] Matsunaga S. & Kohda M. (1986)'Post-processingusing dependency structure of inter-phrases forspeech recognition.'Proc.
Acoust.
Soc.
Jpn.
SpringMeeting, pp.
45-46.\[6\] Hidaka T. & Yoshida S.(1983)'Syntax analysis forJapanese using case grammar.
'Natural Language Pro-cessing Technique Symposium,pp41-46.\[7\] Ozeki K.(1986)'A multi stage decision algorithmfor optimum hunsetsu sequence selection.'
Paper Tee.Group, IECE Japan,SP86-32,pp.41-48.\[8\] Filmore C.(1968)'The case for case.'
in Bach andHarms (eds.
), 1-88\[9\] Matsunaga S. & Sagayama S.(1987)'Candidatesprediction using dependency relationships for minimalphrase speech recognition.
'Paper Tec.
Group, IEICEJapan, SP87-29, pp.
59-62.\[10\] Aikawa K., Sugiyama M. & Shikano K.(1985)'Spekenword recognition based on top-down phoneme segmenta-tion.
'Proe.1985 ICASSP,pp33-36.\[11\] Matsunaga S. & Shikano K.(1985)'Speech recogni-tion based on top-down and bottom-up phoneme recogni-tion.'
Trans.
IECE Japan,J68-D,9,1641 -I 648.\[12\] Matsunaga S. & Kohda M.(1987)'Reduction of wordand minimal phrase candidates for speech recognitionbased on phoneme recognition.'Trans.
IEICE Japan,J70-D, 3, pp.
592-600.\[13\] Matsunaga S.(1988)'Dependency relationships ana-lysis for connected phrase recognition.'
Nat.
Conv.IEICE Japan, 84-I-3,pp.345-346.\[14\] Ozeki K.(1986)'A multi-stage decision algorithmfor optimum bunsetsu sequence selection from bunsetsulattice.
'Paper Tec.
Group, IECE Japan, COMP86-47,pp.
47-57.\[15\] Kohda M.(1986)'An algorithm for optimum selec-tion of phrase sequence from phrase lattice.'
PaperTech.
Group, IECE Japan,SP86-72,pp.9-16.407
