Coling 2010: Poster Volume, pages 1524?1532,Beijing, August 2010Dual-Space Re-ranking Model for Document RetrievalDong Zhou1, Seamus Lawless1, Jinming Min2, Vincent Wade11.
Center for Next Generation Localisation, University of Dublin, Trinity College2.
Center for Next Generation Localisation, Dublin City Universitydongzhou1979@hotmail.com, seamus.lawless@scss.tcd.ie,jinming.min@googlemail.com, Vincent.Wade@sccs.tcd.ieAbstractThe field of information retrieval stillstrives to develop models which allowsemantic information to be integrated inthe ranking process to improve perform-ance in comparison to standard bag-of-words based models.
A conceptualmodel has been adopted in general-purpose retrieval which can comprise arange of concepts, including linguisticterms, latent concepts and explicitknowledge concepts.
One of the draw-backs of this model is that the computa-tional cost is significant and often in-tractable in modern test collections.Therefore, approaches utilising concept-based models for re-ranking initial re-trieval results have attracted a consider-able amount of study.
This method en-joys the benefits of reduced documentcorpora for semantic space constructionand improved ranking results.
However,fitting such a model to a smaller collec-tion is less meaningful than fitting it intothe whole corpus.
This paper proposes adual-space model which incorporatesexternal knowledge to enhance the spaceproduced by the latent concept method.This model is intended to produceglobal consistency across the semanticspace: similar entries are likely to havethe same re-ranking scores with respectto the latent and manifest concepts.
Toillustrate the effectiveness of the pro-posed method, experiments were con-ducted using test collections across dif-ferent languages.
The results demon-strate that the method can comfortablyachieve improvements in retrieval per-formance.1 IntroductionInformation retrieval often suffers from the socalled ?vocabulary mismatch?
problem.
Adocument may be semantically relevant to aquery despite the fact that the specific queryterms used and the terms found in the documentcompletely or partially differ (Furnas et al,1987).
Consequently, overlap with respect tolinguistic terms should not be a necessary con-dition in query-document similarity and meth-ods relying on the bag-of-words model can dis-play poor performance as a result.
In order toovercome the vocabulary mismatch problem,several solutions have been suggested whichexploit semantic relations between text units.Among these methods, the latent model, theexplicit model and the mixed model are com-monly employed.The latent model (Landauer et al, 1998; Bleiet al, 2003) tries to directly model the internalstructure of ?topics?
or ?concepts?
in the textdata, thus building meaningful groups beyondsingle words.
Typically some form of dimen-sion reduction (Fodor, 2002) is applied to thedata matrix to find such latent dimensionswhich correspond to concepts.
In contrast, theexplicit model (Gabrilovich and Markovitch,2007) indexes texts according to an externalknowledge base.
Typically the meaning of apiece of text is represented as a weighted vectorof knowledge-based concepts derived from ex-1524ternal resources such as ODP 1  or Wikipedia 2articles.
The mixed model (Serban et al, 2005)extends the bag-of-words vector by adding ex-ternal categories derived from WordNet or simi-lar thesaurus.
Based upon these definitions, theexplicit model and the mixed model are similarin nature but differ in their use of externalknowledge sources.Models such as those described above, how-ever, have well documented drawbacks.
Firstly,these methods are very computationally com-plex.
In the latent model, complexity growslinearly with the number of dimensions and thenumber of documents.
For example, the compu-tational cost of singular value decomposition(SVD) is significant; no successful experimenthas been reported with over one million docu-ments (Manning et al, 2008).
This has been thebiggest obstacle to the widespread adoption ofthis kind of method.
For the explicit and mixedmodel, the dimensions of projecting documentsinto the external knowledge space are often lim-ited to ten thousand (Potthast et al, 2008) inorder to facilitate the large size of the test col-lections used.
Another problem with the explicitmodel is that the documents are often distrib-uted over thousands of dimensions in which thesemantic relatedness will degrade dramatically.For example, in (Sorg and Cimiano, 2008) whenthe whole Wikipedia collection is adopted tobuild the space, one document is mapped to tenthousand dimensions, in which it may only havevery few truly semantically related dimensions.The means of identifying these dimensions isnot reported and this may significantly influencethe retrieval performance.Therefore, researchers started to consider in-tegrating the aforementioned models intosmaller, controlled document collections toovercome these shortcomings and assist the re-trieval process.
(Zhou and Wade, 2009b) pro-posed a Latent Dirichlet Allocation (LDA)-based method to model the latent structure of?topics?
deduced from the initial retrieval re-sults.
The scores obtained from this process arethen combined with initial ranking scores toproduce a re-ranked list of results that are supe-rior to original ordering.
The method also en-joys the benefits of fast and tractable latent se-1 http://www.dmoz.org/2 http://www.wikipedia.org/mantic computation and successfully avoids theincremental build problem (Landauer et al,1998) which commonly exists in latent semanticanalysis (LSA) techniques.There is an important factor, however, thatneeds to be taken into account when applyingthis method.
Due to the smaller corpus size, fit-ting a latent model into this corpus is less mean-ingful than fitting the same model into a large,web-scale corpus.
This means that some form ofjustification has to be applied to achieve betterperformance.
A simple approach to address thisproblem is to directly apply the explicit ormixed model into a controlled corpus to im-prove ranking performance.
A similar problemwill arise in the latent model in this single se-mantic space, resulting in limited improvements.To address the challenges described above,this paper proposes a dual-space model whichincorporates external knowledge to enhance thesemantic space produced by the latent conceptmethod.
This model is intended to produceglobal consistency across the semantic space:similar entries are likely to have the same re-ranking scores with respect to the latent andmanifest concepts.
In other words: in this model,if a group of documents deal with the sametopic induced from a dual semantic space whichshares a strong similarity with a query, thedocuments will get alocated similar ranking asthey are more likely to be relevant to the query.In the experiments carried out in this paper,the dual-space model is applied to ad-hocdocument retrieval and compared with the ini-tial language model-based ranker and single-space model exploiting latent and explicit fea-tures.
The results show that the explicit modelcould only bring minor improvements over theinitial ranker.
The latent model delivered moresignificant improvements than the explicitmodel.
Both, however, are outperformed by thedual-space model.The main contribution of this paper is to pro-pose a dual-space semantic model for the re-ranking problem, which aims to improve preci-sion, especially of the most highly ranked re-sults.
Other contributions of the paper includeproposing a novel way of applying the explicitmodel to the re-ranking problem, and perform-ing a systematic comparison between differentmodels.1525The rest of this paper is organised as follows.Related work on re-ranking and concept-basedmethods is briefly summarised in Section 2.Section 3 describes the latent space model andexplicit space model used in the framework de-veloped by this research, Section 4 presents de-tails of how to build the dual-space model.
InSection 5 a report is provided on a series of ex-periments performed over three different testcollections written in English, French and Ger-man.
This report includes details of the resultsobtained.
Finally, Section 6 concludes the paperand speculates on future work.2 Related WorkThere exist several strands of related work inthe areas of re-ranking and concept-baseddocument retrieval.A family of work on the structural re-rankingparadigm over different sized document corporawas proposed to refine initial ranking scores.Kurland and Lee performed re-ranking based onmeasures of centrality in the graph formed bythe generation of links induced by languagemodel scores, through a weighted version of thePageRank algorithm (Kurland and Lee, 2005)and a HITS-style cluster-based approach(Kurland and Lee, 2006).
Zhang et al (Zhang etal., 2005) proposed a similar method to improveweb search based on a linear combination ofresults from text search and authority ranking.The graph, which they named an ?affinitygraph?, shares strong similarities with Kurlandand Lee?s work where the links are induced by amodified version of cosine similarity using thevector space model.
Diaz (Diaz, 2005) usedscore regularisation to adjust document retrievalrankings from an initial retrieval by a semi-supervised learning method.
Deng et al (Denget al, 2009) further developed this method bybuilding a latent space graph based on contentand explicit link information.
Unlike their ap-proach this research attempts to model the ex-plicit information directly.The latent concept retrieval model has a longhistory in information retrieval.
(Dumais, 1993;Dumais, 1995) conducted experiments with la-tent semantic indexing (LSI) on TREC3 docu-ments and tasks.
These experiments achieved3 http:// trec.nist.govprecision at, or above, that of the median TRECparticipant.
On about 20% of TREC topics thissystem was the top scorer, and reportedlyslightly better than average results in compari-son to standard vector spaces for LSI at about350 dimensions.
(Hofmann, 1999) provides aninitial probabilistic extension of the basic latentsemantic indexing technique.
A more satisfac-tory formal basis for a probabilistic latent vari-able model for dimensionality reduction is theLDA model (Blei et al, 2003), which is genera-tive and assigns probabilities to documents out-side of the training set.
Wei and Croft (Wei andCroft, 2006) presented the first large-scaleevaluation of LDA, finding it to significantlyoutperform the query likelihood model.
(Zhouand Wade, 2009b; Zhou and Wade, 2009a) suc-cessfully applied this method to document re-ranking and achieved significant improvementover language model-based ranking and variousgraph-based re-ranking methods.The explicit concept model has recently at-tracted much attention in the information re-trieval community.
Notably, explicit semanticanalysis (ESA) has been proposed as an ap-proach to computing semantic relatedness be-tween words and thus, has a natural applicationin this field (Gabrilovich and Markovitch, 2007).In essence, ESA indexes documents with re-spect to the Wikipedia article space, indicatinghow strongly a given word in the document isassociated to a specific Wikipedia article.
In thismodel, each article is regarded as a concept, ananalogical unit used in the latent model.
As inthe latent model, two words or texts can be se-mantically related in spite of not having anywords in common.
Specifically, this method hasbeen widely adopted in cross-language informa-tion retrieval (CLIR) as an approach to resolv-ing  an extreme case of the vocabulary mis-match problem, where queries and documentsare written in different languages (Potthast et al,2008).
(Anderka et al, 2009) showed that thisapproach has comparable performance to lin-guistic matching methods.
(Cimiano et al, 2009)compared this method with a latent conceptmodel based on LSI/LDA and concluded that itwill outperform the latent model if trained onWikipedia articles.15263 Latent and Explicit ModelsIn this section, an overview of the problem ad-dressed by this paper is presented and the latentand explicit document re-ranking models aredescribed in more detail.
This section also dem-onstrates how these models can be used in a re-ranking setting.3.1 Problem DefinitionLet ?
= {?1 ,?2 ,?
,??}
denote the set ofdocuments to be retrieved.
Given a query ?, aset of initial results ?????
?
?
of top documentsare returned by a standard information retrievalmodel (initial ranker).
However, typically theperformance of the initial ranker can be im-proved upon.
The purpose of the re-rankingmethod developed by this research is to re-ordera set of documents  ??????
so as to improve re-trieval accuracy at the most highly ranked re-sults.3.2 Latent Concept ModelThe specific method used here is borrowed from(Zhou and Wade, 2009b), which is based on theLDA model.
The topic mixture is drawn from aconjugate Dirichlet prior that remains the samefor all documents.
The process of generating adocument corpus is as follows:1) Pick a multinomial distribution ?
?
for eachtopic ?
from a Dirichlet distribution withhyperparameter ?
.2) For each document ?
, pick a multinomialdistribution ?
?
, from a Dirichlet distributionwith hyperparameter ?
.3) For each word token ?
in document ?, picka topic ?
?
{1??}
from the multinomialdistribution ?
?
.4) Pick word ?
from the multinomial distribu-tion ?
?
.LDA possesses fully consistent generativesemantics by treating the topic mixture distribu-tion as a ?-parameter hidden random variable.LDA offers a new and interesting framework tomodel a set of documents.
The documents andnew text sequences (for example, queries) caneasily be connected by ?mapping?
them to thetopics in the corpus.In a re-ranking setting, the probability that adocument ?
generates ?
is estimated using amixture model LDA.
It uses a convex combina-tion of a set of component distributions tomodel observations.
In this model, a word  ?
isgenerated from a convex combination of somehidden topics ?:????
?
= ?
?
?
?(?|?)?
?=1where each mixture model ?(?|?)
is a multi-nomial distribution over terms that correspondto one of the latent topics ?.
This could be gen-erated to give a distribution on a sequence oftext:????(?1?2???)
?
????(??
)?
?=1Then the distance between a query and adocument based on this model can be obtained.The method used here adopts the KL divergence(Baeza-Yates and Ribeiro-Neto, 1999) betweenthe query terms and document terms to computea Re-Rank score ???????
:???????
= ??(????(?)||????
?
)The final score is then obtained through alinear combination of the re-ranking scoresbased on the initial ranker and the latent docu-ment re-ranker, shown as follows:???????????
= ?
?
??
+ (1?
?)
?
??????
?where ??
denotes original scores returned bythe initial ranker and ?
is a parameter that canbe tuned with ?
= 1 meaning no re-ranking isperformed.Another well-known approach to the latentmodel is the LSI method.
It is based on SVD, atechnique from linear algebra.
This method hasnot been reported anywhere previously for re-ranking purposes.
It has been included here tocompare the effectiveness of different latentapproaches.
As a full SVD is a loss-free decom-position of a matrix ?, which is decomposedinto two orthogonal matrices ?
and ?
and a di-agonal matrix ?.
Estimating less singular valuesand their corresponding singular vectors leads toreduced dimensions resembling latent conceptsso that documents are no longer represented byterms but by concepts.
New documents (que-ries) are represented in terms of concepts byfolding them into the LSI model.
Next, cosinesimilarities may be used to compute the similar-ity between a query and a document to obtain????????
and combine it with the original score toproduce the final re-ranking score:1527???????????
= ??
?
??
+ (1?
??)
?
???????
?3.3 Explicit Concept ModelAs an example of explicit concept model(Gabrilovich and Markovitch, 2007), explicitsemantic analysis attempts to index or classify agiven text ?
with respect to a set of explicitlygiven external categories.
The basic idea is totake as input a document ?
and map it to a high-dimensional, real-valued vector space.
Thisspace is spanned by a Wikipedia database??
= {?1 ,?
,??
}.This mapping is given by thefollowing function:??
:?
?
?|??|??
?
?
?1 ,?
, ?|?
?|Where |??
|  is the number of articles inWikipedia ??
corresponding to language ?.
Thevalue ??
in the vector ?
expresses the strength ofassociation between ?
and the Wikipedia article??
and is defined as the cosine similarity:????????
=?, ???
?
??
??
?As pointed out in section 1, documents areoften distributed over thousands of dimensionsin which the semantic relatedness will degradedramatically.
The main purpose is to find themost relevant dimensions with respect to que-ries.
To apply this method to re-ranking, ??
islimited to the number of highly relevant docu-ments for a given query.
In other words, the en-tire set of Wikipedia articles in language ?
isretrieved, and only return a specific number ofdocuments as in ??
.
This modification will alsolead to fast computation of scores compared toscanning through the whole Wikipedia collec-tion.Similar to the latent model described above,the final ranking score is defined as:?????????????
= ?
?
??
+ (1?
?)
?
???????
?4 Dual space modelArmed with the latent and explicit models, thedual-space model proposed by this paper is nowdescribed.
In order to make a direct connectionbetween the two models, the key point is tomake the dimensions comparable across differ-ent models.
The detail presented on the latentand explicit concept models in the previous sec-tion did not describe how to define a specificnumber of dimensions.
A simple assumption istaken here in the dual-space model: the numberof dimensions produced by the explicit modelhas to correspond to the number of dimensionsinduced by the latent model.
As the same groupof documents are being mapped into two differ-ent semantic spaces, it is assumed that the con-cepts induced by the latent model reflect thehidden structures in this document collection.Therefore, the same phenomenon should be ob-served when applying the explicit model andvice-versa.
Based on this assumption, the dual-space model could be conducted so as to make aconstraint:??
= ?and the final ranking score for this dualspace is:?????????
= ?
?
?
?+  1?
??
?
?
??????
?+ ?
?
????????or?????????
= ?
?
?
?+  1?
??
?
?
???????
?+ ?
?
???????
?4 Experiments and ResultsIn this section, an empirical study of the effec-tiveness of the dual-space model over three datacollections written in English, French and Ger-man is presented.Collection Contents Language Num of docs Size QueriesBL(CLEF2009)British LibraryDataEnglish(Main)1,000,100 1.2 GB 50BNF(CLEF2009)Biblioth?que Na-tionale de FranceFrench(Main)1,000,100 1.3 GB 50ONB(CLEF2009)Austrian NationalLibraryGerman(Main)869,353 1.3 GB 50Table 1.
Statistics of test collections15284.1 Experimental SetupThe text corpus used in the experiment de-scribed below consisted of elements of theCLEF-20084 and CLEF-2009 European Library(TEL) collections 5  written in English, Frenchand German.
These collections are described ingreater detail in Table 1.
All of the documentsin the experiment were indexed using the Ter-rier toolkit6.
Prior to indexing, Porter's stemmerand a stopword list7 were used for the Englishdocuments.
A French and German analyser8 isused to analyse French and German documents.It is worth noting that the CLEF TEL data isactually multilingual: all collections to a greateror lesser extent contain records pointing todocuments in other languages.
However this isnot a major problem because the majority ofdocuments in the test collection are written inthe primary language of those test collections(BL-English, BNF-French, ONB-German).Please refer to (Ferro and Peters, 2009) for amore detailed discussion about this data.
Thesecollections were chosen to test the scalability ofthe proposed method in different settings andover different languages.The CLEF-2008 and CLEF-2009 query setswere also used.
Both query sets consist of 50topics in each language being tested.
TheCLEF-2008 queries written in English wereused in training the parameters and all of theCLEF-2009 queries were used in the experimentfor testing purposes.
Each topic is composed ofseveral parts, including: Title, Description andNarrative.
Title+Description combinationswere chosen as queries.
The queries are proc-essed similarly to the treatment of the test col-lections.
The relevance judgments are takenfrom the judged pool of top retrieved documentsby various participating retrieval systems fromprevious CLEF workshops.
The initial rankerused in this study is the classic vector spacemodel.
This was selected to facilitate the LSIand ESA models used and the main purpose ofthe experiments is to compare different models4 The test collections used in CLEF-2008 and CLEF-2009 are in fact identical.5 http://www.clef-campaign.org6 http://terrier.org7 ftp://ftp.cs.cornell.edu/pub/smart/8 http://lucene.apache.org/in addition to demonstrating the effectiveness ofthe dual-space model.A Wikipedia database in English, French andGerman was used as an explicit concept space.Only those articles that are connected via cross-language links between all three Wikipedia da-tabases were selected.
A snapshot was obtainedon the 29/11/2009, which contained an alignedcollection of 220,086 articles in all three lan-guages.The following evaluation metrics were cho-sen to measure the effectiveness of the variousapproaches: mean average precision (MAP), theprecision of the top 5 documents (Prec@5), theprecision of the top 10 documents (Prec@10),normalised discounted cumulative gain (NDCG)and Bpref.
Statistically-significant differencesin performance were determined using a pairedt-test at a confidence level of 95%.4.2 Parameter TuningThree primary categories of parameter combina-tions need to be determined in the experiments.For the latent re-ranking experiments, the pa-rameters ?, ??
must be defined.
For the explicitmodel the parameter ?
must be chosen.
For bothmodels, the weights ?, ?
have to be determined.In addition, the number of dimensions |??
| and?
must be specified.
Settings for these parame-ters were optimised with respect to MAP overthe BL collection using CLEF-2008 Englishqueries and were applied to all three collections.This optimisation was not conducted for theother metrics used.The search ranges for these two parameterswere:?, ??
,?, ?, ?
:     0.1, 0.2, ?, 0.9??
,?
:     5, 10, 15, ?, 40Note that parameters ?
and ?
are the weightsassigned to the latent model and the explicitmodel in the dual-space model.
The choice ofone will have direct influence over another.
Asit turned out, for many instances, the optimalvalue of ?, ??
with respect to MAP was either0.3 or 0.4, suggesting the initial retrieval scoresstill contain valuable information.
In contrast,parameter ?
shows no obvious difference in per-formance when the value is above 0.1.
With thisobservation, when setting the parameters ?
and?
more weight is assigned to the latent modelrather than the explicit model.
The optimal1529Dual space build upon LDA and ESA Dual space build upon LSI and ESABL BLinitialrankerlatentspaceexplicitspacedualspaceinitialrankerlatentspaceexplicitspacedualspacePrecision@5 0.508 0.528 0.514 0.54* 0.508 0.54* 0.508 0.556*Precision@10 0.468 0.498* 0.47 0.508* 0.468 0.51* 0.48 0.512*Precision@20 0.408 0.424 0.41 0.435* 0.408 0.408 0.407 0.409NDCG 0.4053 0.4137* 0.4053 0.416* 0.4053 0.4145* 0.4055 0.4213*MAP 0.2355 0.2433* 0.2358 0.2499* 0.2355 0.2478* 0.236 0.2499*R-Precision 0.316 0.3243 0.3165 0.3248 0.316 0.3173 0.3202* 0.3232bpref 0.271 0.2746 0.2725 0.2812 0.271 0.2836* 0.2714 0.2879*BNF BNFinitialrankerlatentspaceexplicitspacedualspaceinitialrankerlatentspaceexplicitspacedualspacePrecision@5 0.376 0.368 0.372 0.376 0.376 0.376 0.376 0.384*Precision@10 0.346 0.352* 0.35 0.352 0.346 0.348 0.35 0.354*Precision@20 0.297 0.297 0.297 0.3* 0.297 0.303 0.299 0.3*NDCG 0.3162 0.3158 0.3156 0.3163 0.3162 0.317 0.3164 0.3178MAP 0.1621 0.1622 0.162 0.1634 0.1621 0.1629 0.1622 0.1624R-Precision 0.2274 0.2279 0.2211 0.2285 0.2274 0.2278 0.2264 0.2277bpref 0.1897 0.1899 0.1887 0.19 0.1897 0.1914 0.1892 0.1918ONB ONBinitialrankerlatentspaceexplicitspacedualspaceinitialrankerlatentspaceexplicitspacedualspacePrecision@5 0.38 0.388 0.36 0.404* 0.38 0.4 0.364 0.412*Precision@10 0.308 0.322 0.302 0.332* 0.308 0.324 0.302 0.324Precision@20 0.246 0.252 0.252 0.259* 0.246 0.247 0.251 0.252NDCG 0.3042 0.304 0.3059 0.3101 0.3042 0.3152* 0.3062 0.3154*MAP 0.1482 0.1524 0.1509 0.1567* 0.1482 0.1567* 0.1494 0.1578*R-Precision 0.2115 0.2152 0.2137 0.2175 0.2115 0.212 0.2106 0.2128bpref 0.1778 0.1871 0.1799 0.1896 0.1778 0.1833 0.1788 0.1832Table 2.
Experimental Results.
For each evaluation setting, statistically significant differencesbetween different methods and the initial ranker are indicated by star.
Bold highlights the bestresults over all algorithms.value of ?
was between 25 and 35 for the LDAbased model and between 5 and 15 for the LSIbased model.
Although this demonstrates a rela-tively large variance, the differences in terms ofMAP have remained small and statistically in-significant.
?????
is set to 50 in all results re-ported.4.3 ResultsPrimary Evaluation The main experimentalresults, which describe the performance of thedifferent re-ranking algorithms on the CLEFdocument collection, are shown in Table 2.
Thefirst four rows in each test collection specify themost important measurements because this re-search is particularly interested in performanceover the most highly ranked results.
As illus-1530trated by the data, the initial ranker was alwaysthe lowest performer in terms of nearly allmeasurements.
This indicates the need for re-ranking.
Using the method computed by theexplicit space always led to an improvement inretrieval effectiveness.
But this improvement isonly minor in comparison to the other two mod-els and the results are often statistically insig-nificant.
When the re-ranking score was calcu-lated using the latent model, retrieval effective-ness always exceeded initial ranker and the ex-plicit model.
There was a noticeable improve-ment in retrieval effectiveness in the Englishcollection (BL, statistically significant resultswere often observed), but a modest increase forthe other two collections (BNF and ONB).The empirical results obtained using the dualspace model are very promising.
Pleasingly,both the LDA+ESA and LSI+ESA models out-performed the basic latent and explicit spacemodel in the majority of retrieval runs, with thebest scores relating to the LSI-based models.
Animportant phenomenon is that statistically sig-nificant improvements are always recorded inthe metrics which measure the most highlyranked results.
An even more exciting observa-tion is that in many cases, the dual-space model,even though tuned for MAP, can outperformvarious baselines and other models for all theevaluation metrics, with statistically significantimprovements in many runs.Another observation that can be drawn fromTable 2 is that the relative performance tends tobe stable across test collections written in dif-ferent languages.
This indicates a promisingfuture for studying document structure with re-spect to latent and explicit semantic space forre-ranking purposes.The Comparison of Latent Methods  Table 2also shows a side-by-side comparison of thevarious performance measurements between thelatent model used in this research on the CLEF-2009 BL test collection.
The LSI-based methodappeared to outscore the LDA-based method inthe latent model in the vast majority of cases,while the difference between the various scor-ings was fairly marginal as both methods de-liver statistically significant results.
For thedual-space model, similar results were ob-served.
A possible reason is that the initialranker used was based on the vector spacemodel and LSI is also vector based.
It showsthat more research with respect to the latentmodel selection will be necessary in the future.Effectiveness of Explicit Methods As part ofexperimental objectives of this research, it wasalso necessary to test the newly developed ex-plicit model for re-ranking.
In the parametertuning section, the explicit model displayed noobvious difference in terms of combination ef-fectiveness.
However, some variations could beobserved when applying different dimensionswhere statistically significant results often ap-pear in lower dimensions.
This confirms theneed to find more relevant dimensions, both forperformance and efficiency purposes.5 Conclusion and Future WorkThis paper proposed and evaluated a dual-spacedocument re-ranking method for re-ordering theinitial retrieval results.
The key to refining theresults is the global consistency over the seman-tic space, which leverages latent and explicitsemantic information and results in state-of-artperformance.
This paper also proposed a novelway to apply the explicit model to the re-ranking problem, and performed a systematiccomparison between different models.Further investigation is planned in many re-search directions.
It has been shown that thelatent model-based retrieval is a promisingmethod for ranking the whole corpus.
There is adesire to call for a direct comparison betweenranking and re-ranking using the proposed algo-rithmic variations.
Future work will also includeidentifying improvements upon linear combina-tion for engineering different models.
At thesame time, there exist a sufficient number oflatent and explicit semantic techniques whichwill be explored to compare their performance.AcknowledgmentsThe authors would like to thank the threeanonymous reviewers for their many construc-tive comments.
This research is supported bythe Science Foundation Ireland (Grant07/CE/I1142) as part of the Centre for NextGeneration Localisation (www.cngl.ie) at Uni-versity of Dublin, Trinity College and DublinCity University.1531ReferencesAnderka, Maik, Nedim Lipka and Benno Stein.
2009.Evaluating Cross-Language Explicit SemanticAnalysis and Cross Querying at TEL@CLEF 2009.In CLEF 2009 Workshop, Corfu, Greece.Baeza-Yates, Ricardo A. and Berthier Ribeiro-Neto.1999.
Modern Information Retrieval, Addison-Wesley Longman Publishing Co., Inc.Blei, David M., Andrew Y. Ng and Michael I. Jordan.2003.
Latent dirichlet alocation.
J. Mach.
Learn.Res.
3: 993-1022.Cimiano, Philipp, Antje Schultz, Sergej Sizov,Philipp Sorg and Steffen Staab.
2009.
Explicitversus latent concept models for cross-languageinformation retrieval.
In Proceedings of the 21stinternational jont conference on Artificalintelligence, Pasadena, California, USA, MorganKaufmann Publishers Inc. p. 1513-1518.Deng, Hongbo, Michael R. Lyu and Irwin King.2009.
Effective latent space graph-based re-rankingmodel with global consistency.
In Proceedings ofthe Second ACM WSDM conference, Barcelona,Spain, ACM.
p. 212-221.Diaz, Fernando.
2005.
Regularizing ad hoc retrievalscores.
In Proceedings of the 14th ACM CIKMconference, Bremen, Germany, ACM.
p. 672-679.Dumais, Susan T. 1993.
Latent semantic indexing(LSI) and TREC-2.
In Proceedings of TREC.
p.105-115.Dumais, Susan T. 1995.
Latent semantic indexing(LSI): TREC-3 report.
In Proceedings of TREC.
p.219-230.Ferro, Nicola and Carol Peters.
2009.
CLEF 2009 AdHoc Track Overview: TEL & Persian Tasks.
InWorking notes of CLEF2008, Corfu, Greece.Fodor, Imola K. 2002.
A Survey of DimensionReduction Techniques.http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.8.5098.
Accessed: 18th April 2010.Furnas, G. W. , T. K.  Landauer, L. M.  Gomez and S.T.
Dumais.
1987.
The vocabulary problem inhuman-system communication.
Commun.
ACM30(11): 964-971.Gabrilovich, Evgeniy and Shaul Markovitch.
2007.Computing semantic relatedness using Wikipedia-based explicit semantic analysis.
In Proceedings ofthe 20th international joint conference on Artificalintelligence, Hyderabad, India, Morgan KaufmannPublishers Inc. p. 1606-1611.Hofmann, Thomas.
1999.
Probabilistic latentsemantic indexing.
In Proceedings of the 22ndannual international ACM SIGIR conference,Berkeley, California, United States, ACM.
p. 50-57.Kurland, Oren and Lillian Lee.
2005.
PageRankwithout hyperlinks: structural re-ranking usinglinks induced by language models.
In Proceedingsof the 28th annual international ACM SIGIRconference, Salvador, Brazil, ACM.
p. 306-313.Kurland, Oren and Lillian Lee.
2006.
Respect myauthority!
: HITS without hyperlinks, utilizingcluster-based language models.
In Proceedings ofthe 29th annual international ACM SIGIRconference, Seattle, Washington, USA, ACM.
p.83-90.Landauer, Thomas  K., Peter  W. Foltz and DarrellLaham.
1998.
An Introduction to Latent SemanticAnalysis.
Discourse Processes 25: 259-284.Manning, Christopher D., Prabhakar Raghavan andHinrich Schtze.
2008.
Introduction to InformationRetrieval, Cambridge University Press.Potthast, Martin, Benno Stein and Maik Anderka.2008.
A Wikipedia-Based Multilingual RetrievalModel.
In Proceedings of 30th EuropeanConference on Information Retrieval, Glasgow,Scotland, Springer.
p. 522-530.Serban, Radu, Annette ten Teije, Frank vanHarmelen, Mar Marcos and Cristina Polo.
2005.Ontology-driven extraction of linguistic patterns formodelling clinical guidelines.
Proceedings of the10th European Conference on ArtificialIntelligence in Medicine (AIME-05).Wei, Xing and W. Bruce Croft.
2006.
LDA-baseddocument models for ad-hoc retrieval.
InProceedings of the 29th annual international ACMSIGIR conference, Seattle, Washington, USA,ACM.
p. 178-185.Zhang, Benyu, Hua Li, Yi Liu, Lei Ji, Wensi Xi,Weiguo Fan, Zheng Chen and Wei-Ying Ma.
2005.Improving web search results using affinity graph.In Proceedings of the 28th annual internationalACM SIGIR conference, Salvador, Brazil, ACM.
p.504-511.Zhou, Dong and Vincent Wade.
2009a.
LanguageModeling and Document Re-Ranking: TrinityExperiments at TEL@CLEF-2009.
In CLEF 2009Workshop, Corfu, Greece.Zhou, Dong and Vincent Wade.
2009b.
LatentDocument Re-Ranking.
In Proceedings of the 2009Conference on Empirical Methods in NaturalLanguage Processing, Singapore, ACL.
p. 1571-1580.1532
