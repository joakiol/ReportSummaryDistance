Taxonomy learning ?
factoring the structure of a taxonomy into asemantic classification decisionViktor PEKARBashkir State UniversityUfa, Russia, 450000vpekar@ufanet.ruSteffen STAABInstitute AIFB, University of Karlsruhehttp://www.aifb.uni-karlsruhe.de/WBS& Learning Lab Lower Saxonyhttp://www.learninglab.deAbstractThe paper examines different possibilitiesto take advantage of the taxonomic or-ganization of a thesaurus to improve theaccuracy of classifying new words into itsclasses.
The results of the study demon-strate that taxonomic similarity betweennearest neighbors, in addition to their dis-tributional similarity to the new word,may be useful evidence on which classifi-cation decision can be based.1.
IntroductionMachine-readable thesauri are now an indispen-sable part for a wide range of NLP applicationssuch as information extraction or semantics-sensitive information retrieval.
Since their man-ual construction is very expensive, a lot of recentNLP research has been aiming to develop waysto automatically acquire lexical knowledge fromcorpus data.In this paper we address the problem of large-scale augmenting a thesaurus with new lexicalitems.
The specifics of the task are a big numberof classes into which new words need to be clas-sified and hence a lot of poorly predictable se-mantic distinctions that have to be taken intoaccount.
For this reason, knowledge-poor ap-proaches such as the distributional approach areparticularly suited for this task.
Its previous ap-plications (e.g., Grefenstette 1993, Hearst andSchuetze 1993, Takunaga et al1997, Lin 1998,Caraballo 1999) demonstrated that cooccurrencestatistics on a target word is often sufficient forits automatical classification into one of numer-ous classes such as synsets of WordNet.Distributional techniques, however, are poorlyapplicable to rare words, i.e., those words forwhich a corpus does not contain enough cooc-currence data to judge about their meaning.
Suchwords are the primary concern of many practicalNLP applications: as a rule, they are semanti-cally focused words and carry a lot of importantinformation.
If one has to do with a specificdomain of lexicon, sparse data is a problem par-ticularly difficult to overcome.The major challenge for the application of thedistributional approach in this area is, therefore,the development of ways to minimize theamount of corpus data required to successfullycarry out a task.
In this study we focus on opti-mization possibilities of an important phase inthe process of automatically augmenting a the-saurus ?
the classification algorithm.
The mainhypothesis we test here is that the accuracy ofsemantic classification may be improved bytaking advantage of information about taxo-nomic relations between word classes containedin a thesaurus.On the example of a domain-specific thesauruswe compare the performance of three state-of-the-art classifiers which presume flat organiza-tion of thesaurus classes and two classificationalgorithms, which make use of taxonomic or-ganization of the thesaurus: the "tree descend-ing" and the "tree ascending" algorithms.
Wefind that a version of the tree ascending algo-rithm, though not improving on other methodsoverall, is much better at choosing a supercon-cept for the correct class of the new word.
Wethen propose to use this algorithm to first narrowdown the search space and then apply the kNNmethod to determine the correct class amongfewer candidates.The paper is organized as follows.
Sections 2and 3 describe the classification algorithms un-der study.
Section 4 describes the settings anddata of the experiments.
Section 5 details theevaluation method.
Section 6 presents the resultsof the experiments.
Section 7 concludes.2.
Classification methodsClassification techniques previously applied todistributional data can be summarized accordingto the following methods: the k nearest neighbor(kNN) method, the category-based method andthe centroid-based method.
They all operate onvector-based semantic representations, whichdescribe the meaning of a word of interest (tar-get word) in terms of counts1 of its coocurrencewith context words, i.e., words appearing withinsome delineation around the target word.
Thekey differences between the methods stem fromdifferent underlying ideas about how a semanticclass of words is represented, i.e.
how it is de-rived from the original cooccurrence counts, and,correspondingly, what defines membership in aclass.The kNN method is based on the assumptionthat membership in a class is defined by the newinstance?s similarity to one or more individualmembers of the class.
Thereby, similarity isdefined by a similarity score as, for instance, bythe cosine between cooccurrence vectors.
Toclassify a new instance, one determines the setof k training instances that are most similar tothe new instance.
The new instance is assignedto the class that has the biggest number of itsmembers in the set of nearest neighbors.
In addi-tion, the classification decision can be based onthe similarity measure between the new instanceand its neighbors: each neighbor may vote for itsclass with a weight proportional to its closenessto the new instance.
When the method is appliedto augment a thesaurus, a class of training in-stances is typically taken to be constituted bywords belonging to the same synonym set, i.e.lexicalizing the same concept (e.g., Hearst andSchuetze 1993).
A new word is assigned to thatsynonym set that has the biggest number of itsmembers among nearest neighbors.1 Or, probabilities determined via Maximum Likeli-hood Estimation.The major disadvantage of the kNN method thatis often pointed out is that it involves significantcomputational expenses to calculate similaritybetween the new instance and every instance ofthe training set.
A less expensive alternative isthe category-based method (e.g., Resnik 1992).Here the assumption is that membership in aclass is defined by the closeness of the new itemto a generalized representation of the class.
Thegeneralized representation is built by adding upall the vectors constituting a class and normalis-ing the resulting vector to unit length, thus com-puting a probabilistic vector representing theclass.
To determine the class of a new word, itsunit vector is compared to each class vector.Thus the number of calculations is reduced tothe number of classes.
Thereby, a class represen-tation may be derived from a set of vectors cor-responding to one synonym set (as is done byTakunaga et al 1997) or a set of vectors corre-sponding to a synonym set and some or all sub-ordinate synonym sets (Resnik 1992).Another way to prepare a representation of aword class is what may be called the centroid-based approach (e.g., Pereira et al 1993).
It isalmost exactly like the category-based method,the only difference being that a class vector iscomputed slightly differently.
All n vectors cor-responding to class members are added up andthe resulting vector is divided by n to computethe centroid between the n vectors.3.
Making use of the structure of the the-saurusThe classification methods described above pre-suppose that semantic classes being augmentedexist independently of each other.
For most ex-isting thesauri this is not the case: they typicallyencode taxonomic relations between wordclasses.
It seems worthwhile to employ this in-formation to enhance the performance of theclassifiers.3.1 Tree descending algorithmOne way to factor the taxonomic informationinto the classification decision is to employ the?tree-descending?
classification algorithm,which is a familiar technique in text categoriza-tion.
The principle behind this approach is thatthe semantics of every concept in the thesaurustree retains some of the semantics of all its hy-ponyms in such a way that the upper the concept,the more relevant semantic characteristics of itshyponyms it reflects.
It is thus feasible to deter-mine the class of a new word by descending thetree from the root down to a leaf.
The semanticsof concepts in the thesaurus tree can be repre-sented by means of one of the three methods torepresent a class described in Section 2.
At everytree node, the decision which path to follow ismade by choosing the child concept that has thebiggest distributional similarity to the new word.After the search has reached a leaf, the newword is assigned to that synonym set, whichlexicalizes the concept that is most similar to thenew word.
This manner of search offers twoadvantages.
First, it allows to gradually narrowdown the search space and thus save on compu-tational expenses.
Second, it ensures that, in aclassification decision, more relevant semanticdistinctions of potential classes are given morepreference than less relevant ones.
As in the casewith the category-based and the centroid-basedrepresentations, the performance of the methodmay be greatly dependent on the number of sub-ordinate synonyms sets included to represent aconcept.3.2 Tree ascending algorithmAnother way to use information about inter-classrelations contained in a thesaurus is to base theclassification decision on the combined meas-ures of distributional similarity and taxonomicsimilarity (i.e., semantic similarity induced fromthe relative position of the words in the thesau-rus) between nearest neighbors.
Suppose wordsin the nearest neighbors set for a given newword, e.g., trailer, all belong to different classesas in the following classification scenario: box(similarity score  to trailer: 0.8), house (0.7),barn (0.6), villa (0.5) (Figure 1).
In this case,kNN will classify trailer into the classCONTAINER, since it appears to have biggestsimilarity to box.
However, it is obvious that themost likely class of trailer is in a different partof the thesaurus: in the nearest neighbors setthere are three words which, though not belong-ing to one class, are semantically close to eachother.
It would thus be safer to assign the newword to a concept that subsumes one or all of thethree semantically similar neighbors.
For exam-ple, the concepts DWELLING or BUILDING couldbe feasible candidates in this situation.Figure 1.
A semantic classification scenario.The crucial question here is how to calculate thetotal of votes for these two concepts to be able todecide which of them to choose or whether toprefer CONTAINER.
Clearly, one cannot sum oraverage the distributional similarity measures ofneighbors below a candidate concept.
In the firstcase the root will always be the best-scoringconcept.
In the second case the score of the can-didate concept will always be smaller than thescore of its biggest-scoring hyponym.We propose to estimate the total of votes forsuch candidate concepts based on taxonomicsimilarity between relevant nodes.
The taxo-nomic similarity between two concepts is meas-ured according to the procedure elaborated in(Maedche & Staab, 2000).
Assuming that a tax-onomy is given as a tree with a set of nodes N, aset of edges E ?
N?N, a unique root ROOT ?
N,one first determines the least common supercon-cept of a pair of concepts a,b being compared.
Itis defined by)),(),(),((minarg),( crootcbcabalcsNc???
++=?
(1)where ?
(a,b) describes the number of edges onthe shortest path between a and b.
The taxonomicsimilarity between a and b is then given by( ) ),(),(),(),(, crootcbcacrootba ????++=?
(2)where c = lcs(a,b).
T is such that 0?
T ?
1, with 1standing for the maximum taxonomic similarity.T is directly proportional to the number of edgesfrom the least common superconcept to the root,which agrees with the intuition that a given num-ber of edges between two concrete concepts sig-nifies greater similarity than the same number ofedges between two abstract concepts.We calculate the total of votes for a candidateconcept by summing the distributional similaritymeasures of its hyponyms to the target word teach weighted by the taxonomic similaritymeasure between the hyponym and the candi-date node:??
?=nIhhnThtsimnW ),(),()(     (3)where In is the set of hyponyms below the can-didate concept n, sim(t,h) is the distributionalsimilarity between a hyponym h and the word tobe classified t, and T(n,h) is the taxonomic simi-larity between the candidate concept and thehyponym h.4.
Data and settings of the experimentsThe machine-readable thesaurus we used in thisstudy was derived from GETESS2, an ontologyfor the tourism domain.
Each concept in theontology is associated with one lexical item,which expresses this concept.
From this ontol-ogy, word classes were derived in the followingmanner.
A class was formed by words lexicaliz-ing all child concepts of a given concept.
Forexample, the concept CULTURAL_EVENT in theontology has successor concepts PERFORMANCE,OPERA, FESTIVAL, associated with words per-formance, opera, festival correspondingly.Though these words are not synonyms in thetraditional sense, they are taken to constitute onesemantic class, since out of all words of the on-tology?s lexicon their meanings are closest.
Thethesaurus thus derived contained 1052 wordsand phrases (the corpus used in the study haddata on 756 of them).
Out of the 756 concepts,182 were non-final; correspondingly, 182 wordclasses were formed.
The average depth level ofthe thesaurus is 5.615, the maximum number oflevels is 9.
The corpus from which distributionaldata was obtained was extracted from a web siteadvertising hotels around the world 3 .
It con-tained around 1 million words.Collection of distributional data was carried outin the following settings.
The preprocessing ofcorpus included a very simple stemming (most2 http://www.daml.org/ontologies/1713 http://www.placestostay.comcommon inflections were chopped off; irregularforms of verbs, adjectives and nouns werechanged to their first forms).
The context ofusage was delineated by a window of 3 words oneither side of the target word, withouttransgressing sentence boundaries.
In case a stopword other than a proper noun appeared insidethe window, the window was accordingly ex-panded.
The stoplist included 50 most frequentwords of the British National Corpus, wordslisted as function words in the BNC, and propernouns not appearing in the sentence-initial posi-tion.
The obtained frequencies of cooccurrencewere weighted by the 1+log weight function.The distributional similarity was measured bymeans of three different similarity measures: theJaccard?s coefficient, L1 distance, and the skewdivergence.
This choice of similarity measureswas motivated by results of studies by (Levy etal 1998) and (Lee 1999) which compared severalwell known measures on similar tasks and foundthese three to be superior to many others.
An-other reason for this choice is that there are dif-ferent ideas underlying these measures: whilethe Jaccard?s coefficient is a binary measure, L1and the skew divergence are probabilistic, theformer being geometrically motivated and thelatter being a version of the information theo-retic Kullback Leibler divergence (cf., Lee1999).5.
Evaluation methodThe performance of the algorithms was assessedin the following manner.
For each algorithm, weheld out a single word of the thesaurus as thetest case, and trained the system on the remain-ing 755 words.
We then tested the algorithm onthe held-out vector, observing if the assignedclass for that word coincided with its originalclass in the thesaurus, and counting the numberof correct classifications (?direct hits?).
This wasrepeated for each of the words of the thesaurus.However, given the intuition that a semanticclassification may not be simply either right orwrong, but rather of varying degrees of appro-priateness, we believe that a clearer idea aboutthe quality of the classifiers would be given byan evaluation method that takes into account?near misses?
as well.
We therefore evaluatedthe performance of the algorithms also in termsof Learning Accuracy (Hahn & Schnattinger1998), i.e., in terms of how close on average theproposed class for a test word was to the correctclass.
For this purpose the taxonomic similaritybetween the assigned and the correct classes ismeasured so that the appropriateness of a par-ticular classification is estimated on a scale be-tween 0 and 1, with 1 signifying assignment tothe correct class.
Thus Learning Accuracy iscompatible with the counting of direct hits,which, as will be shown later, may be useful forevaluating the methods.In the following, the evaluation of the classifica-tion algorithms is reported both in terms of theaverage of direct hits and Learning Accuracy (?di-rect+near hits?)
over all words in the thesaurus.To have a benchmark for evaluation of the algo-rithms, a baseline was calculated, which was theaverage hit value a given word gets, when itsclass label is chosen at random.
The baseline fordirect hits was estimated at 0.012; for di-rect+near hits, it was 0.15741.6.
ResultsWe first conducted experiments evaluating per-formance of the three standard classifiers.
Todetermine the best version for each particularclassifier, only those parameters were varied that,as described above, we deemed to be critical inthe setting of thesaurus augmentation.In order to get a view on how the accuracy of thealgorithms was related to the amount of avail-able distributional data on the target word, allwords of the thesaurus were divided into threegroups depending on the amount corpus dataavailable on them (see Table 1).
The amount ofdistributional data for a word (the ?frequency?
inthe left column) is the total of frequencies of itscontext words.Table 1.
Distribution of words of the thesaurusinto frequency rangesFrequency range # words in the range0-40 27440-500 190>500 292The results of the evaluation of the methods aresummarized in the tables below.
Rows specifythe measures used to determine distributionalsimilarity (JC for Jaccard?s coefficient, L1 forthe L1 distance and SD for the skew divergence)and columns specify frequency ranges.
Each celldescribes the average of direct+near hits / theaverage of direct hits over words of a particularfrequency range and over all words of the the-saurus.
The statistical significance of the resultswas measured in terms of the one-tailed chi-square test.kNN.
Evaluation of the method was conductedwith k=1, 3, 5, 7, 10, 15, 20, 25, and 30.
Theaccuracy of classifications increased with theincrease of k. However, starting with k=15 theincrease of k yielded only insignificant im-provement.
Table 2 describes results of evalua-tion of kNN using 30 nearest neighbors, whichwas found to be the best version of kNN.Table 2. kNN, k=30.0-40 40-500 >500 OverallJC .33773/.17142.33924/.15384.40181/.12457.37044/.15211L1  .33503/.16428.38424/.21025.38987/.14471.37636/.17195SD .31505/.14285.36316/.18461.45234/.17845.38806/.17063Category-based method.
To determine the bestversion of this method, we experimented withthe number of levels of hyponyms below a con-cept that were used to build a class vector).
Thebest results were achieved when a class wasrepresented by data from its hyponyms at mostthree levels below it (Table 3).Table 3.
Category-based method, 3 levels0-40 40-500 >500 OverallJC .26918/.12142.34743/.17948.47404/.28282.37554/.2023L1 .27533/.125.41736/.25128.56711/.38383.43242/.26190SD .28589/.12857.34932/.18461.51306/.31649.39755/.21957Centroid-based method.
As in the case withthe category-based method, we varied the num-ber of levels of hyponyms below the candidateconcept.
Table 4 details results of evaluation ofthe best version of this method (a class is repre-sented by 3 levels of its hyponyms).Table 4.
Centroid-based method, 3 levels.0-40 40-500 >500 OverallJC .17362/.07831.18063/.08119.30246/.14434.22973/.10714L1 .21711/.09793.30955/.13938.37411/.1687.30723/.12698SD .22108/.09972.23814/.11374.36486/.16147.28665/.10714Comparing the three algorithms we see thatoverall, kNN and the category-based methodexhibit comparable performance (with the ex-ception of measuring similarity by L1 distance,when the category-based method outperformskNN by a margin of about 5 points; statisticalsignificance p<0.001).
However, their perform-ance is different in different frequency ranges:for lower frequencies kNN is more accurate (e.g.,for L1 distance, p<0.001).
For higher frequen-cies, the category-based method improves onkNN (L1, p<0.001).
The centroid-based methodexhibited performance, inferior to both those ofkNN and the category-based method.Tree descending algorithm.
In experimentswith the algorithm, candidate classes were repre-sented in terms of the category-based method, 3levels of hyponyms, which proved to be the bestgeneralized representation of a class in previousexperiments.
Table 5 specifies the results of itsevaluation.Table 5.
Tree descending algorithm.0-40 40-500 >500 OverallJC .00726/0.01213/.00512.02312/.0101.014904/.005291L1 .08221/.03214.05697/.02051.21305/.11111.128844/.060846SD .08712/.03214.07739/.03589.16731/.06734.011796/.047619Its performance turns out to be much worse thanthat of the standard methods.
Both direct+nearand direct hits scores are surprisingly low, for 0-40 and 40-500 much lower than chance.
Thiscan be explained by the fact that some of topconcepts in the tree are represented by much lessdistributional data than other ones.
For example,there are less than 10 words that lexicalize thetop concepts MASS_CONCEPT andMATHEMATICAL_CONCEPT and all of theirhyponyms (compare to more than 150 wordslexicalizing THING and its hyponyms up to 3levels below it).
As a result, at the very begin-ning of the search down the tree, a very largeportion of test words was found to be similar tosuch concepts.Tree ascending algorithm.
The experimentswere conducted with the same number of nearestneighbors as with kNN.
Table 6 describes theresults of evaluation of the best version (formula3, k=15).Table 6.
Tree ascending algorithm, total of votesaccording to (3), k=15.0-40 40-500 >500 OverallJC .32112/.075.33553/.0923.40968/.08754.36643/.08597L1 .33369/.07142.34504/.0923.42627/.09764.38005/.08862SD .31809/.06785.32489/.05128.45529/.11111.38048/.08201There is no statistically significant improvementon kNN overall, or in any of the frequencyranges.
The algorithm favored more upper con-cepts and thus produced about twice as few di-rect hits than kNN.
At the same time, its di-rect+near hits score was on par with that of kNN!This algorithm thus produced much more nearhits than kNN, what can be interpreted as itsbetter ability to choose a superconcept of thecorrect class.
Based on this observation, wecombined the best version of the tree ascendingalgorithm with kNN in one algorithm in thefollowing manner.
First the former was used todetermine a superconcept of the class for thenew word and thus to narrow down the searchspace.
Then the kNN method was applied topick a likely class from the hyponyms of theconcept determined by the tree ascendingmethod.
Table 7 specifies the results of evalua-tion of the proposed algorithm.Table 7.
Tree ascending algorithm combined withkNN, k=30.0-40 40-500 >500 OverallJC .34444/.16428.35858/.14358.41260/.10774.38215/.14021L1 .35147/.16428.36545/.15384.41086/.11784.38584/.14682SD .32613/.13571.36485/.1641.45732/.16498.39456/.1574The combined algorithm demonstrated impro-vement both on kNN and the tree ascendingmethod of 1 to 3 points in every frequency rangeand overall for direct+near hits (except for the40-500 range, L1).
The improvement was statis-tically significant only for L1, ?>500?
(p=0.05)and for L1, overall (p=0.011).
For other similari-ty measures and frequency ranges it was insigni-ficant (e.g., for JC, overall, p=0.374; for SD,overall, p=0.441).
The algorithm did not im-prove on kNN in terms of direct hits.
The hitsscores set in bold in Table 7 are those which arehigher than those for kNN in correspondingfrequency ranges and similarity measures.7.
DiscussionIn this paper we have examined different possi-bilities to take advantage of the taxonomic or-ganization of a thesaurus to improve the accu-racy of classifying new words into its classes.The study demonstrated that taxonomic similar-ity between nearest neighbors, in addition totheir distributional similarity to the new word,may be a useful evidence on which classificationdecision can be based.
We have proposed a ?treeascending?
classification algorithm which ex-tends the kNN method by making use of thetaxonomic similarity between nearest neighbors.This algorithm was found to have a very goodability to choose a superconcept of the correctclass for a new word.
On the basis of this finding,another algorithm was developed that combinesthe tree ascending algorithm and kNN in orderto optimize the search for the correct class.
Al-though only limited statistical significance of itsimprovement on kNN was found, the results ofthe study indicate that this algorithm is a promis-ing possibility to incorporate the structure of athesaurus into the decision as to the class of thenew word.
We conjecture that the tree ascendingalgorithm leaves a lot of room for improvementsand combinations with other algorithms likekNN.The tree descending algorithm, a techniquewidely used for text categorization, proved to bemuch less efficient than standard classifierswhen applied to the task of augmenting a do-main-specific thesaurus.
Its poor performance isdue to the fact that in such a thesaurus there aregreat differences between top concepts in theamount of distributional data used to representthem, which very often misleads the top-downsearch.We believe that a study of the two algorithms onthe material of a larger thesaurus, where richertaxonomic information is available, can yield afurther understanding of its role in the perform-ance of the algorithms.ReferencesCaraballo S. A.
(1999) Automatic construction of ahypernym-labeled noun hierarchy from text.
In Pro-ceedings of the 37th Annual Meeting of the Associa-tion for Computational Linguistics, pp.
120-126.Hahn U. and Schnattinger K. (1998) Towards textknowledge engineering.
In Proc.
of AAAI/IAAI, pp.524-531.Hearst M. and Schuetze H. (1993) Customizing alexicon to better suit a computational task.
In Proc.of the SIGLEX Workshop on Acquisition of LexicalKnowledge from Text, Columbus Ohio, pp.
55--69.Grefenstette G. (1993) Evaluation techniques forautomatic semantic extraction: comparing syntac-tic and window based approaches.
In Proc.
of theSIGLEX Workshop on Acquisition of LexicalKnowledge from Text, Columbus Ohio.Lin D. (1998) Automatic retrieval and clustering ofsimilar words.
In Proc.
of the COLING-ACL?98,pp.
768-773.Lee L. (1999) Measures of distributional similarity.In Proc.
of the 37th Annual Meeting of the Asso-ciation for Computational Linguistics, pp.
25-32.Levy J., Bullinaria J., and Patel M. (1998) Explora-tions in the derivation of word co-occurrence sta-tistics.
South Pacific Journal of Psychology, 10/1,pp.
99-111.Maedche A. and Staab S. (2000) Discovering concep-tual relations from text.
In Proc.
of ECAI-2000,IOS Press, pp.
321-324.Pereira F., Tishby N., and Lee L. (1993) Distribu-tional clustering of English words.
In Proc.
of the31st Annual Meeting of the ACL, pp.
183-190.Resnik P. (1992) Wordnet and distributional analysis:A class-based approach to lexical discovery.
AAAIWorkshop on Statistically-based Natural LanguageProcessing Techniques.Tokunaga T., Fujii A., Iwayama M., Sakurai N., andTanaka H. (1997) Extending a thesaurus by classi-fying words.
In Proc.
of the ACL-EACL Workshopon Automatic Information Extraction and Buildingof Lexical Semantic Resources, pp.
16-21.
