Bridging the Gap: Academic and Industrial Research in Dialog Technologies Workshop Proceedings, pages 17?24,NAACL-HLT, Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsThe Multimodal Presentation DashboardMichael JohnstonAT&T Labs Research180 Park AveFlorham Park, NJjohnston@research.att.comPatrick EhlenCSLIStanford UniversityPalo Alto, CAehlen@csli.stanford.eduDavid GibbonAT&T Labs Research180 Park AveFlorham Park, NJdcg@research.att.comZhu LiuAT&T Labs Research180 Park AveFlorham Park, NJzliu@research.att.comAbstractThe multimodal presentation dashboard al-lows users to control and browse presenta-tion content such as slides and diagramsthrough a multimodal interface that sup-ports speech and pen input.
In addition tocontrol commands (e.g.
?take me to slide10?
), the system allows multimodal searchover content collections.
For example, ifthe user says ?get me a slide about internettelephony,?
the system will present aranked series of candidate slides that theycan then select among using voice, pen, ora wireless remote.
As presentations areloaded, their content is analyzed and lan-guage and understanding models are builtdynamically.
This approach frees the userfrom the constraints of linear order allow-ing for a more dynamic and responsivepresentation style.1 IntroductionAnthropologists have long informed us that theway we work?whether reading, writing, or givinga presentation?is tightly bound to the tools weuse.
Web browsers and word processors changedthe way we read and write from linear to nonlinearactivities, though the linear approach to giving apresentation to a roomful of people has evolvedlittle since the days of Mylar sheets and notecards,thanks to presentation software that reinforces?oreven further entrenches?a linear bias in our no-tion of what ?giving a presentation?
means to us.While today?s presentations may be prettier andflashier, the spontaneity once afforded by holding astack of easily re-arrangeable sheets has been lost.Figure 1 Presentation dashboard in actionInstead, a question from the audience or a changein plan at the podium results in a whizzing-by ofall the wrong slides as the presenter sweats throughan awkward silence while hammering an arrowkey to track down the right one.
In theory there are?search?
functions that presenters could use to findanother slide in the same presentation, or even inanother presentation on the same machine, thoughnone of the authors of this paper has ever seen apresenter do this.
A likely reason is that thesesearch functions are designed for desktop ergo-17nomics rather than for standing at a podium orwalking around the room, making them even moredisruptive to the flow of a presentation than franticarrow key hammering.In some utopian future, we envision presenterswho are unhindered by limitations imposed bytheir presentation tools, and who again possess, asAristotle counseled, ?all available means of per-suasion?
at the tips of their fingers?or theirtongues.
They enjoy freeform interactions withtheir audiences, and benefit from random access totheir own content with no arrow hammering and nodisruption in flow.
Their tools help to expand theirpossible actions rather than limiting them.
We arehardly alone in this vision.In that spirit, many tools have been developed oflate?both within and outside of research labs?with the aim of helping people work more effec-tively when they are involved in those assembliesof minds of mutual interest we often call ?meet-ings.?
Tools that capture the content of meetings,perform semantic understanding, and provide abrowsable summary promise to free meeting par-ticipants from the cognitive constraints of worryingabout trying to record and recall what happenedwhen a meeting takes place (e.g., Ehlen, Purver &Niekrasz, 2007; Tucker & Whittaker, 2005).Presentations are a kind of meeting, and severalpresentation tools have also sought to free present-ers from similar constraints.
For example, manyoff-the-shelf products provide speech interfaces topresentation software.
These often replace the lin-ear arrow key with the voice, offering command-based navigation along a one-dimensional vectorof slides by allowing a presenter to say ?next slideplease?
or ?go to the last slide.
?A notable exception is the Jabberwocky inter-face to PowerPoint (Franklin, Bradshaw &Hammond, 1999; 2000), which aims to followalong with a presenter?s talk?like a human assis-tant might do?and switch to the appropriate slidewhen the presenter seems to be talking about it.Using a method similar to topic modeling, wordsspoken by the presenter are compared to a prob-ability distribution of words across slides.
Jabber-wocky changes to a different slide when asufficient probability mass has been reached tojustify the assumption that the speaker is now talk-ing about a different slide from the one that?s al-ready showing.A similar effort (Rogina & Schaaf, 2002) useswords extracted from a presentation to augment aclass-based language model and attempt automatictracking of a presentation as it takes place.
Thisintelligent meeting room system then aligns thepresenter?s spoken words with parts of a presenta-tion, hoping to determine when a presenter hasmoved on to a new slide.A major drawback of this ?machine-initiative?approach to presentation assistance is that a pre-senter must speak enough words associated with anew slide for a sufficient probability mass to bereached before the slide is changed.
The resultingdelay is likely to make an audience feel like thepresentation assistant is rather dim-witted.
And anyerrors that change slides before the presenter isready can be embarrassing and disruptive in frontof potentially important audiences.So, in fashioning our own presentation controlinterface, we chose to allow the presenter to retainfull initiative in changing slides, while offering asmarter and more flexible way to navigate througha presentation than the single degree of freedomafforded by arrow keys that simply traverse a pre-determined order.
The result is the MultimodalPresentation Dashboard, a presentation interfacethat integrates command-based control with prob-abilistic, content-based search.
Our method startswith a context-free grammar of speech commands,but embeds a stochastic language model generatedfrom the presenter?s slide deck content so a pre-senter can request any slide from the deck?oreven a large set of decks?just by asking for itscontents.
Potentially ambiguous results are re-solved multimodally, as we will explain.2 Multimodal interface for interactivepresentationsThe presentation dashboard provides presenterswith the ability to control and adapt their presenta-tions on the fly in the meeting room.
In addition tothe traditional next/previous approach to navigat-ing a deck of slides, they can access slides by posi-tion in the active deck (e.g., ?show slide 10?
or?last slide please?)
or they can multimodally com-bine voice commands with pen or remote controlto browse for slides by content, saying, for in-stance, ?show the slide on internet telephony,?
andthen using the pen to select among a ranked list ofalternatives.182.1 Setup configurationThough the dashboard offers many setup configu-rations, the preferred arrangement uses a single PCwith two displays (Figure 1).
Here, the dashboardis running on a tablet PC with a large monitor as asecond external display.
On the tablet, thedashboard UI is visible only to the presenter.
Onthe external display, the audience sees the currentslide, as they would with a normal presentation.The presenter can interact with the dashboardusing either the microphone onboard the tablet PC,or, preferably, a wireless microphone.
A wirelessremote functions as a presentation control, whichcan be used to manually change slides in the tradi-tional manner, and also provides a ?push to talk?button to tell the dashboard when to listen.
A wire-less microphone combined with the wireless pres-entation control and voice selection mode (seeSection 2.3) allows a presenter to stroll around theroom or stage completely untethered.2.2 Presenter UIThe presenter?s primary control of the system isthrough the presenter UI, a graphical user interfaceaugmented with speech and pen input.
The inter-face has three main screens: a presentation panelfor controlling an ongoing presentation (Figure 2),a loader panel for selecting a set of presentations toload (Figure 4), and a control panel for adjustingsystem settings and bundling shareable index andgrammar models.
The user can select among thepanels using the tabs at the top left.Figure 2 The presentation panelThe presentation panel has three distinct functionalareas from top to bottom.
The first row shows thecurrent slide, along with thumbnails of the previ-ous and next slides to provide context.
The usercan navigate to the next or previous slide by click-ing on these thumbnails.
The next row shows ascrolling list of search results from content-basedqueries.
The last row contains interaction informa-tion.
There is a click & speak button for activatingthe speech recognizer and a feedback window thatdisplays recognized speech.Some user commands are independent of thecontent of slide decks, as with basic commands forslide navigation:- ?next slide please?- ?go back?- ?last slide?In practice, however, navigation to next and previ-ous slides is much easier using buttons on the wire-less control.
The presenter can also ask for slidesby position number, allowing random access:- ?take me to slide 10?- ?slide 4 please?But not many presenters can remember the posi-tion numbers of some 40 or 50 slides, we?d guess,so we added content-based search, a better methodof random access slide retrieval by simply sayingkey words or phrases from the desired slide, e.g.
:- ?slides about internet telephony?- ?get me the slide with thesystem architecture?- ?2006 highlights?- ?budget plan, please?When the presenter gives this kind of request, thesystem identifies any slides that match the queryand displays them in a rank ordered list in the mid-dle row of the presenter?s panel.
The presenter canthen scroll through the list of thumbnails and clickone to display it to the audience.This method of ambiguity resolution offers thepresenter some discretion in selecting the correctslide to display from multiple search results, sincesearch results appear first on the presenter?s privateinterface rather than being displayed to the audi-ence.
However, it requires the presenter to return tothe podium (or wherever the tablet is located) toselect the correct slide.192.3 Voice selection modeAlternatively, the presenter may sacrifice discre-tion for mobility and use a ?voice selection mode,?which lets the presenter roam freely throughout theauditorium while making and resolving content-based queries in plain view of the audience.
In thismode, if a presenter issues a content-based query(e.g., ?shows slides about multimodal access?
),thumbnails of the slides returned by the query ap-pear as a dynamically-generated interactive?chooser?
slide (Figure 3) in the main presentationviewed by the audience.
The presenter can thenselect the desired slide by voice (e.g., ?slide three?
)or by using the previous, next, and select controlson the wireless remote.
If more than six slides arereturned by the query, multiple chooser slides aregenerated with six thumbnails to each slide, whichcan be navigated with the remote.While voice selection mode allows the presentergreater mobility, it has the drawback of allowingthe audience to see thumbnails of every slide re-turned by a content-based query, regardless ofwhether the presenter intended for them to be seen.Hence this mode is more risky, but also more im-pressive!Figure 3 Chooser slide for voice selection mode2.4 Compiling deck setsSometimes a presenter wishes to have access tomore than one presentation deck at a time, in orderto respond to unexpected questions or comments,or to indulge in a whimsical tangent.
We respondto this wish by allowing the presenter to compile adeck set, which is, quite simply, a user-definedbundle of multiple presentations that can all besearched at once, with their slides available fordisplay when the user issues a query.
In fact, thisoption makes it easy for a presenter to follow spon-taneous tangents by switching from one presenta-tion to another, navigating through the alternatedeck for a while, and then returning to the originalpresentation, all without ever walking to the po-dium or disrupting the flow of a presentation bystopping and searching through files.Deck sets are compiled in the loader panel (Fig-ure 4), which provides a graphical browser for se-lecting a set of active decks from the file system.When a deck set is chosen, the system builds ASRand language understanding models and a retrievalindex for all the slides in the deck set.
A compileddeck set is also portable, with all of the grammarand understanding model files stored in a singlearchive that can be transferred via e-mail or thumbdrive and speedily loaded on another machine.A common use of deck sets is to combine amain presentation with a series of other slide decksthat provide background information and detail foranswering questions and expanding points, so thepresenter can adapt to the interests of the audience.Figure 4 The loader panel3 Multimodal architectureThe Multimodal Presentation Dashboard uses anunderlying multimodal architecture that inheritscore components from the MATCH architecture(Johnston et al2002).
The components communi-cate through a central messaging facilitator andinclude a speech recognition client, speech recog-nition server (Goffin et al2005), a natural lan-guage understanding component (Johnston &Bangalore 2005), an information retrieval engine,20and a graphical user interface client.
The graphicalUI runs in a web browser and controls PowerPointvia its COM interface.We first describe the compilation architecture,which builds models and performs indexing whenthe user selects a series of decks to activate.
Wethen describe the runtime architecture that operateswhen the user gives a presentation using the sys-tem.
In Section 3.3, we provide more detail on theslide indexing mechanism and in Section 3.4 wedescribe a mechanism used to determine key-phrases from the slide deck that are used on a dropdown menu and for determining relevancy.3.1 Compilation architectureIn a sense, the presentation dashboard uses neitherstatic nor dynamic grammars; the grammars com-piled with each deck set lie somewhere in-betweenthose two concepts.
Command-based speech inter-faces often fare best when they rely on the predict-ability of a fixed, context-free grammar, whileinterfaces that require broader vocabulary coverageand a wider range of syntax are better off leverag-ing the flexibility of stochastic language models.To get the best of both worlds for our ASR model,we use a context-free command ?wrapper?
to astochastic language model (c.f.
Wang & Acero2003).
This is coupled to the understandingmechanism using a transducer with a loop over thecontent words extracted from the slides.This combined grammar is best thought of as afixed, context-free template which contains an em-bedded SLM of dynamic slide contents.
Ourmethod allows a static background grammar andunderstanding model to happily co-exist with adynamic grammar component which is compiledon the fly when presentations are loaded, enablingcustom, content-based queries.When a user designates a presentation deck setand compiles it, the slides in the set are processedto create the combined grammar by composing anSLM training corpus based on the slide content.First, a slide preprocessor extracts sentences, ti-tles, and captions from each slide of each deck, andnormalizes the text by converting numerals andsymbols to strings, Unicode to ASCII, etc.
Thesecontent phrases are then used to compose (1) acombined corpus to use for training an SLM forspeech recognition, and (2) a finite-state transducerto use for multimodal natural language understand-ing (Johnston & Bangalore 2005).Combined CorpusPresentationsSlideindexKeyphrasesSlide PreprocessorSlide PreprocessorSentencesIndexServerIndexServerSLMfor ASRSLMfor ASR NLUMODELNLUMODELGUIMenuGUIMenuGrammarTemplateClass-taggedCorpusGrammarWordsFigure 5 Compilation architectureTo create a combined corpus for the SLM, the con-tent phrases extracted from slides are iterated overand folded into a static template of corpus classes.For instance, the template entry,<POLITE> <SHOWCON> <CONTENT_PHRASE>could generate the phrase ?please show the slideabout <CONTENT_PHRASE>?
for each contentphrase?as well as many others.
These templatesare currently manually written but could poten-tially be induced from data as it becomes available.The content corpus is appended to a commandcorpus of static command classes that generatephrases like ?next slide please?
or ?go back to thelast one.?
Since the number of these commandphrases remains constant for every grammar whilethe number of content phrases depends on howmany phrases are extracted from the deck set, aweighting factor is needed to ensure the number ofexamples of both content and command phrases isbalanced in the SLM training data.
The resultingcombined corpus is used to build a stochastic lan-guage model that can handle variations on com-mands and slide content.In parallel to the combined corpus, a stack ofslide content words is compiled for the finite stateunderstanding machine.
Phrases extracted for thecombined corpus are represented as a terminal_CWORD class.
(Terminals for tapes in each gram-mar class are separated by colons, in the formatspeech:meaning, with empty transitions repre-21sented as ?)
For example, the phrase ?internettelephony?
on a slide would appear in the under-standing grammar like so:_CWORD internet:internet_CWORD telephony:telephonyThese content word classes are then ?looped?
inthe FSM (Figure 6) into a flexible understandingmodel of potential slide content results using onlya few grammar rules, like:_CONTENT _CWORD _CONTENT_CONTENT _CWORDThe SLM and the finite-state understanding ma-chine now work together to extract plausible mean-ings from dynamic and inexact speech queries.Figure 6 Understanding FSMTo provide an example of how this combined ap-proach to understanding comes together in the run-ning system, let?s say a presenter?s slide containsthe title ?Report for Third Quarter?
and she asksfor it by saying, ?put up the third quarter reportslide.?
Though she asks for the slide with languagethat doesn?t match the phrase on the slide, our for-giving stochastic model might return a speech re-sult like, ?put up third quarter report mine.?
Thespeech result is then mapped to the finite-stategrammar, which catches ?third quarter reportmine?
as a possible content phrase, and returns,?third,quarter,report,mine?
as a con-tent-based meaning result.
That result is then usedfor information retrieval and ranking to determinewhich slides best match the query (Section 3.3).3.2 Runtime architectureA primary goal of the presentation dashboard wasthat it should run standalone on a single laptop.
Atablet PC works best for selecting slides with apen, though a mouse or touch screen can also beused for input.
We also developed a networkedversion of the dashboard system where indexing,compilation, speech recognition, and understand-ing are all network services accessed over HTTPand SIP, so any web browser-based client can login, upload a presentation, and present without in-stalling software aside from PowerPoint and a SIPplug-in.
However, our focus in this paper is on thetablet PC standalone version.ASR SERVERASR SERVERMultimodal DashboardUI (Browser)Multimodal DashboardUI (Browser)NLUNLUPowerpointApplicationPowerpointApplicationIndex Server (http)Index Server (http)LanguageModelSlide indexHTTPCommandsImagesFACILITATORFACILITATORSPEECHCLIENTSPEECHCLIENTUnderstandingModelFigure 7 Multimodal architectureThe multimodal user interface client is browser-based, using dynamic HTML and Javascript.
Inter-net Explorer provides COM access to the Power-Point object model, which reveals slide content andcontrols the presentation.
Speech recognition, un-derstanding, and compilation components are ac-cessed through a java-based facilitator via a socketconnection provided by an ActiveX control on theclient page (Figure 7).
When the user presses ortaps the click & speak button, a message is sent tothe Speech client, which sends audio to the ASRServer.
The recognizer?s speech result is processedby the NLU component using a finite-state trans-ducer to translate from the input string to an XMLmeaning representation.
When the multimodal UIreceives XML for simple commands like ?firstslide?
or ?take me to slide ten,?
it calls the appro-priate function through the PowerPoint API.
Forcontent-based search commands, an SQL query isconstructed and issued to the index server as anHTTP query.
When the results are returned, mul-timodal thumbnail images of each slide appear inthe middle row of the UI presenter panel.
The usercan then review the choices and switch to the ap-propriate slide by clicking on it?or, in voice se-lection mode, by announcing or selecting a slideshown in the dynamically-generated chooser slide.The system uses a three stage strategy in search-ing for slides.
First it attempts an exact match bylooking for slides which have the words of thequery in the same order on the same slide in a sin-gle phrase.
If no exact matches are found, the sys-tem backs off to an AND query and shows slideswhich contain all of the words, in any order.
If that22fails, the system resorts to an OR query and showsslides which have any of the query terms.3.3 Information retrievalWhen the slide preprocessor extracts text from apresentation, it retains the document structure asmuch as possible and stores this in a set of hier-archal XML documents.
The structure includesglobal document metadata such as creation dateand title, as well as more detailed data such as slidetitles.
It also includes information about whetherthe text was part of a bullet list or text box.
Withthis structure, queries can be executed against theentire text or against specified textual attributes(e.g.
?show me the chart titled ?project budget??
).For small document collections, XPath queriescan search the entire collection with good responsetime, providing a stateless search method.
But asthe collection of presentation decks to be searchedgrows, a traditional inverted index information re-trieval system achieves better response times.
Weuse a full text retrieval system that employs stem-ming, proximity search, and term weighting, andsupports either a simplified query syntax or SQL.Global metadata can also constrain queries.
Incre-mental indexing ensures that new presentationdecks cause the index to update automaticallywithout being rebuilt from scratch.3.4 Key phrase extractionKey phrases and keywords are widely used for in-dexing and retrieving documents in large data-bases.
For presentation slides, they can also helprank a slide?s relevance to a query.
We extract alist of key phrases with importance scores for eachslide deck, and phrases from a set of decks aremerged and ranked based on their scores.A popular approach to selecting keywords froma document within a corpus is to find keywordsthat frequently occur in one document but seldomoccur in others, based on term frequency-inversedocument frequency (TF-IDF).
Our task is slightlydifferent, since we wish to choose key phrases fora single document (the slide deck), independent ofother documents.
So our approach uses term fre-quency-inverse term probability (TF-ITP), whichexpresses the probability of a term calculated overa general language rather than a set of documents.Assuming a term Tk occurs tfk times in a docu-ment, and its term probability is tpk, the TF-ITP ofTk is defined as, wTk = tfk / tpk.
This method can beextended to assign an importance score to eachphrase.
For a phrase Fk = {T1 T2 T3 ?
TN}, whichcontains a sequence of N terms, assuming it ap-pears ffk times in a document, its importance score,ISk, is defined as,?==Ni ikk TffIS1.To extract a set of key phrases, we first segmentthe document into sentences based on punctuationand some heuristics.
A Porter stemming algorithm(Porter 1980) eliminates word variations, andphrases up to N=4 terms long are extracted, remov-ing any that start or end with noise words.
An im-portance score ranks each phrase, where termprobabilities are estimated from transcripts of 600hours of broadcast news data.
A term that is out ofthe vocabulary with a term frequency of more than2 is given a default term probability value, definedas the minimum term probability in the vocabulary.Phrases with high scores are chosen as keyphrases, eliminating any phrases that are containedin other phrases with higher scores.
For an overalllist of key phrases in a set of documents, we mergeindividual key phrase lists and sum the importancescores for key phrases that recur in different lists,keeping the top 10 phrases.4 Performance and future workThe dashboard is fully implemented, and has beenused by staff and management in our lab for inter-nal presentations and talks.
It can handle largedecks and collections (100s to 1000s of slides).
Atablet PC with a Pentium M 1.6Ghz processor and1GB of RAM will compile a presentation of 50slides?with ASR, understanding models, andslide index?in under 30 seconds.In ongoing work, we are conducting a usabilitytest of the system with users in the lab.
Effectiveevaluation of a tool of this kind is difficult withoutfielding the system to a large number of users.
Anideal evaluation would measure how users farewhen giving their own presentations, responding tonatural changes in narrative flow and audiencequestions.
Such interaction is difficult to simulatein a lab, and remains an active area of research.23We also hope to extend current retrieval meth-ods to operate at the level of concepts, rather thanwords and phrases, so a request to show ?slidesabout mortgages?
might return a slide titled ?homeloans.?
Thesauri, gazetteers, and lexicons likeWordNet will help achieve this.
Analyzing non-textual elements like tables and charts could alsoallow a user to say, ?get the slide with the networkarchitecture diagram.?
And, while we now use afixed lexicon of common abbreviations, an auto-mated analysis based on web search and othertechniques could identify likely expansions.5 ConclusionOur goal with the multimodal presentationdashboard was to create a meeting/presentationassistance tool that would change how people be-have, inspiring presenters to expand the methodsthey use to interact with audiences and with theirown material.
To this end, our dashboard runs on asingle laptop, leaves the initiative in the hands ofthe presenter, and allows slides from multiple pres-entations to be dynamically retrieved from any-where in the room.
Our assistant requires no?intelligent room?
; only an intelligent presenter,who may now offer the audience a presentationthat is as dynamic or as dull as imagination allows.As Tufte (2006) reminds us in his analysis ofhow PowerPoint presentations may have precipi-tated the Columbia shuttle tragedy, the way infor-mation is presented can have a profound?evenlife-threatening?impact on the decisions wemake.
With the multimodal presentationdashboard, we hope to free future presenters fromthat single, arrow-key dimension, offering accessto presentation slides and diagrams in any order,using a diverse combination of modes.
Presenterscan now pay more attention to the needs of theiraudiences than to the rigid determinism of a fixedpresentation.
Whether they will break free of thelinear presentation style imposed by current tech-nology if given a chance remains to be seen.ReferencesPatrick Ehlen, Matthew Purver, and John Niekrasz.2007.
A meeting browser that learns.
In Proceedingsof the AAAI Spring Symposium on Interaction Chal-lenges for Intelligent Assistants.David Franklin, Shannon Bradshaw, and KristianHammond.
1999.
Beyond ?Next slide, please?
: Theuse of content and speech in multi-modal control.
InWorking Notes of the AAAI-99 Workshop on Intelli-gent Information Systems.David Franklin, Shannon Bradshaw, and KristianHammond.
2000.
Jabberwocky: You don't have to bea rocket scientist to change slides for a hydrogencombustion lecture.
In Proceedings of IntelligentUser Interfaces 2000 (IUI-2000).Vincent Goffin, Cyril Allauzen, Enrico Bocchieri, DilekHakkani-T?r, Andrej Ljolje, Sarangarajan Partha-sarathy, Mazin Rahim, Giuseppe Riccardi, and MuratSaraclar.
2005.
The AT&T WATSON speech recog-nizer.
In Proceedings of ICASSP.Michael Johnston, Srinivas Bangalore, Guna Vasireddy,Amanda Stent, Patrick Ehlen, Marilyn Walker, SteveWhittaker, Preetam Maloor.
2002.
MATCH: An Ar-chitecture for Multimodal Dialogue Systems.
In Pro-ceedings of the 40th ACL.
376-383.Michael Johnston  and Srinivas Bangalore.
2005.
Finite-state multimodal integration and understanding.Journal of Natural Language Engineering.
11.2, pp.159-187, Cambridge University Press.Martin F. Porter.
1980.
An algorithm for suffix strip-ping, Program, 14, 130-137.Ivica Rogina and Thomas Schaaf.
2002.
Lecture andpresentation tracking in an intelligent meeting room.In Proceedings of the 4th IEEE International Confer-ence on Multimodal Interfaces.
47-52.Simon Tucker and Steve Whittaker.
2005.
Accessingmultimodal meeting data: Systems, problems andpossibilities.
In Samy Bengio and Herv?
Bourlard(Eds.)
Lecture Notes in Computer Science, 3361, 1-11Edward Tufte.
2006.
The Cognitive Style of PowerPoint.Graphics Press, Cheshire, CT.Ye-Yi Wang and Alex Acero.
2003.
Combination ofCFG and N-gram Modeling in Semantic GrammarLearning.
Proceedings of Eurospeech conference,Geneva, Switzerland.Acknowledgements We would like to thank Srinivas Banga-lore, Rich Cox, Mazin Gilbert, Vincent Goffin, and BehzadShahraray for their help and support.24
