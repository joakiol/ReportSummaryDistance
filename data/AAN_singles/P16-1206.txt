Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 2185?2194,Berlin, Germany, August 7-12, 2016. c?2016 Association for Computational LinguisticsA New Psychometric-inspired Evaluation Metricfor Chinese Word SegmentationPeng Qian Xipeng Qiu?
Xuanjing HuangShanghai Key Laboratory of Intelligent Information Processing, Fudan UniversitySchool of Computer Science, Fudan University825 Zhangheng Road, Shanghai, China{pqian11, xpqiu, xjhuang}@fudan.edu.cnAbstractWord segmentation is a fundamental taskfor Chinese language processing.
Howev-er, with the successive improvements, thestandard metric is becoming hard to distin-guish state-of-the-art word segmentationsystems.
In this paper, we propose a newpsychometric-inspired evaluation metricfor Chinese word segmentation, whichaddresses to balance the very skewedword distribution at different levels ofdifficulty1.
The performance on a realevaluation shows that the proposed metricgives more reasonable and distinguishablescores and correlates well with humanjudgement.
In addition, the proposedmetric can be easily extended to evaluateother sequence labelling based NLP tasks.1 IntroductionWord segmentation is a fundamental task forChinese language processing.
In recent years,Chinese word segmentation (CWS) has undergonegreat development, which is, to some degree,driven by evaluation conferences of CWS, suchas SIGHAN Bakeoffs (Emerson, 2005; Levow,2006; Jin and Chen, 2008; Zhao and Liu, 2010).The current state-of-the-art methods regard wordsegmentation as a sequence labeling problem(Xue, 2003; Peng et al, 2004).
The goalof sequence labeling is to assign labels to allelements in a sequence, which can be handled withsupervised learning algorithms, such as maximumentropy (ME) (Berger et al, 1996), conditionalrandom fields (CRF) (Lafferty et al, 2001) andPerceptron (Collins, 2002).
?Corresponding author.1We release the word difficulty of the popular wordsegmentation datasets at http://nlp.fudan.edu.cn/data/ .Benefiting from the public datasets and featureengineering, Chinese word segmentation achievesquite high precision after years of intensiveresearch.
To evaluate a word segmenter, thestandard metric consists of precision p, recall r,and an evenly-weighted F-score f1.However, with the successive improvement ofperformance, state-of-the-art segmenters are hardto be distinguished under the standard metric.Therefore, researchers also report results withsome other measures, such as out-of-vocabulary(OOV) recall, to show their strengths besides p, rand f1.Furthermore, although state-of-the-art methodshave achieved high performances on p, r and f1,there exists inconsistence between the evaluationranking and the intuitive feelings towards thesegmentation results of these methods.
Theinconsistence is caused by two reasons:(1) The high performance is due to the factthat the distribution of difficulties of words isunbalanced.
The proportion of trivial cases isvery high, such as ??
(?s)?????
(we)?, whichresults in that the non-trivial cases are relativelydespised.
Therefore, a good measure should havea capability to balance the skewed distribution byweighting the test cases.
(2) Human judgement depends on difficulties ofsegmentations.
A segmenter can earn extra creditswhen correctly segmenting a difficult word than aneasy word.
Conversely, a segmenter can take extrapenalties when wrongly segmenting an easy wordthan a difficult word.Taking a sentence and two predicted segmenta-tions as an example:S :????
?
?
?
??
??
(Trans: Resveratrol is a kind of phenols material.
)P1: ?
??
?
?
??
??
?
?P2: ??
??
?
?
?
???
?We can see that the two segmentations have the2185same scores in p, r and f1.
But intuitively, P1should be better than P2, since P2 is worse evenon the trivial cases, such as ???
(phenols)?
and???
(material)?.Therefore, we think that an appropriate evalua-tion metric should not only provide an all-aroundquantitative analysis of system performances, butalso explicitly reveal the strengths and potentialweaknesses of a model.Inspired by psychometrics, we propose a newevaluation metric for Chinese word segmentationin this paper.
Given a labeled dataset, notall words have the same contribution to judgethe performance of a segmenter.
Based onpsychometric research (Lord et al, 1968), weassign a difficulty value to each word.
Thedifficulty of a word is automatically rated by acommittee of segmenters, which are diversifiedby training on different datasets and features.
Wedesign a balanced precision, recall to pay differentattentions to words according to their difficulties.We also give detailed analysis on a real eval-uation of Chinese word segmentation with ourproposedmetric.
The analysis result shows that thenewmetric gives amore balanced evaluation resulttowards the human intuition of the segmentationquality.
We will release the weighted datasetsfocused this paper to the academic community.Although our proposed metric is applied toChinese word segmentation for a case study, itcan be easily extended to other sequence labellingbased NLP tasks.2 Standard Evaluation MetricThe standard evaluation usually uses threemeasures: precision, recall and balanced F-score.Precision p is defined as the number of correctlysegmented words divided by the total number ofwords in the automatically segmented corpus.Recall r is defined as the number of correctlysegmented words divided by the total number ofwords in the gold standard, which is the manuallyannotated corpus.F-score f1the harmonic mean of precision andrecall.Given a sentence, the gold-standard segmen-tation of a sentence is w1, ?
?
?
,WN, N is thenumber of words.
The predicted segmentation isw?1, ?
?
?
, w?N?, N ?
is the number of words.
Amongthat, the number of words correctly identified bythe predicted segmentation is c, and the number ofincorrectly predicted words is e.p, r and f1are defined as follows:p =cN?, (1)r =cN, (2)f1=2 ?
p?
rp + r. (3)As a complement to these metrics, researchersalso use the recall of out-of-vocabulary (OOV)words to measure the segmenter?s performance indetecting unknown words.3 A New Psychometric-inspiredEvaluation MetricWe involve the basic idea from psychometricsand improve the evaluation metric by assigningweights to test cases.3.1 Background TheoryThis work is inspired by the test theory inpsychometrics (Lord et al, 1968).
Psychologists,as well as educators, have studied the way ofanalyzing items in a psychological test, such asIQ test.
The general idea is that test casesshould be given different weights, which reflectsthe effectiveness of a certain item to a certainmeasuring object.Similarly, we consider an evaluation task as akind of special psychological test.
The psycho-logical traits, or the ability of the model, is notan explicit characteristics.
We propose that thetest cases for NLP task should also be assigned areal value to account for the credits that the taggerearned from answering the test case.In analogy to the way of computing difficulty inpsychometrics, the difficulty of a target word wiisdefined as the error rate of a committee in the caseof word segmentation.Given a committee of K base segmenter-s, we can get K segmentations for sentencew1, ?
?
?
,WN.
We use a mark mki?
{0, 1} toindicate whether word wiis correctly segmentedby the k-th segmenter.The number of words ck correctly identified bythe k-th segmenter isck=N?i=1mki.
(4)2186Thus, we can calculate the degree of difficultyof each word wi.di=1KK?k=1(1 ?mki).
(5)This methodology of measuring test item diffi-culty is also widely applied in assessing standard-ized exams such as TOEFL (Service, 2000).3.2 Psychometric-Inspired Evaluation MetricSince the distribution of the difficulties of wordsis very skew, we design a new metric to balancethe weights of different words according to theirdifficulties.
In addition, we also should keepstrictly a fair rule for rewards and punishments.Intuitively, if the difficulty of a word is high,a correct segmentation should be given an extrarewards; otherwise, if the difficulty of a word islow, it is reasonable to give an extra punishmentto a wrong segmentation.Our newmetric of precision, recall and balancedF-score is designed as follows.Balanced Recall Given a new predicted seg-mentation, themarkmi?
{0, 1} indicateswhetherwordwiis correctly segmented.
diis the degree ofdifficulty of word wi.According to the difficulties of each word, wecan calculated the reward recall rrewardwhichbiased for the difficult cases.rreward=?Ni=1di?mi?Ni=1di, (6)where r?reward?
[0, 1] is biased recall, whichplaces more attention on the difficult cases and lessattention on the easy cases.Conversely, we can calculated another punish-ment recall rpunishmentwhich biased for the easycases.rpunishment=?Ni=1(1 ?
di) ?mi?Ni=1(1 ?
di), (7)where rpunishment?
[0, 1] is biased recall, whichplaces more attention on the easy cases and lessattention on the difficult cases.rpunishmentcan be interpreted as a punishmentas bellows.rpunishment=?Ni=1(1 ?
di) ?mi?Ni=1(1 ?
di), (8)= 1 ?
?Ni=1(1 ?
di) ?
(1 ?mi)?Ni=1(1 ?
di).
(9)From Eq (9), we can see that an extra pun-ishment is given to wrong segmentation for lowdifficult word.
In detailed, for a word withat iseasy to segment, its weights (1 ?
di) is relativehigher.
When its segmentation is wrong, mi= 0.Therefore, (1?
di)?
(1?mi) = (1?
di) will belarger, which results to a smaller final score.To balance the reward and punishment, abalanced recall rbis used, which is the harmonicmean of rrewardand rpunishment.rb=2 ?
rpunishment?
rrewardrpunishment+ rreward(10)Balanced Precision Given a new predicted seg-mentation, the mark m?i?
{0, 1} to indicatewhether segment s?iis correctly segmented.
d?iis the degree of difficulty of segment s?i, whichis an average difficulty of the corresponding goldwords.Similar to balanced recall, we use the same wayto calculate balance precision pb.
Here N ?
is thenumber of words in the predicted segmentation.d?iis the weight for the predicted segmentationunit w?i.
It equals to the word difficulty ofthe corresponding word w that cover the rightboundary of w?iin the gold segmentation.preward=?Ni=1(1 ?
di) ?mi?N?i=1(1 ?
d?i), (11)ppunishment=?Ni=1(1 ?
di) ?mi?N?i=1(1 ?
d?i), (12)pb=2 ?
preward?
ppunishmentpreward+ ppunishment.
(13)(14)Balanced F-score The final balanced F-score isfb=2 ?
pbalanced?
rbalancedpbalanced+ rbalanced.
(15)4 Committee of SegmentersIt is infeasible to manually judge the difficultyof each word in a dataset.
Therefore, an empiricalmethod is needed to rate each word.
Since thedifficulty is also not derivable from the observationof the surface forms of the text, we use a committeeof automatic segmenters instead.
To keep fairness2187F1CiT0, (i = ?1, 0, 1)Ci:i+1T0, (i = ?1, 0)T?1,0F2CiT0, (i = ?2,?1, 0, 1, 2)Ci:i+1T0, (i = ?2,?1, 0, 1)T?1,0F3CiT0, (i = ?2,?1, 0, 1, 2)Ci:i+1T0, (i = ?2,?1, 0, 1)Ci:i+2T0, (i = ?2,?1, 0)T?1,0Table 1: Feature templates.
C represents a Chinesecharacter, and T represents the character-based tagin set {B, M, E, S}.
The subscript indicates itsposition relative to the current character, whosesubscript is 0.
Ci:jrepresents the subsequence ofcharacters form relative position i to j.and justice of the committee, we need a largenumber of diversified committee members.Thus, the grading result of committee is fair andaccurate, avoiding the laborious human annotationand the deviation caused by the subjective factor ofthe artificial judgement.4.1 Building the CommitteeBase Segmenters The committee is composedof a series of base segmenters, which are based ondiscriminative character-based sequence labelingmethod.
Each character is labeled as one of {B, M,E, S} to indicate the segmentation.
?B?
indicatesthe beginning character of a word.
?M?
indicatesthe middle character of a word.
?E?
indicates theend character of a word.
?S?
indicates that the wordconsists of only a single character.Diversity of Committee To objectively assessthe difficulty of a word, we need tomaintain a largeenough committee with diversity.To encourage diversity among committee mem-bers, we train them with different datasets andfeatures.
Specifically, each base segmenter adoptsone of three types of feature templates (shown inTable 1), and are trained on randomly samplingtraining sets.
To keep a large diversity, we setsampling ratio to be 10%, 20% and 30%.
In short,each base segmenter is constructed with a randomcombination of the candidate feature template andthe sampling ratio for training dataset.Size of Committee To obtain a valid and reliableassessment for a word, we need to choose the10 30 50 70 900.00.20.40.60.81.0The Size of the CommitteeDifficultyFigure 1: Judgement of difficulty against thecommittee size.
Each line represents a sampledword.appropriate size of committee.
For a given testcase, the judgement of its difficulty should berelatively stable.
We analyze how the judgementof its difficulty changes as the size of committeeincreases.Figure 2 show PKU data from SIGHAN 2005(Emerson, 2005) the difficulty is stable when thesample size is large enough.4.2 Interpreting Difficulty with LinguisticFeaturesSince we get the difficulty for each wordempirically, we naturally want to know whetherthe difficulty is explainable, as what TOEFLresearchers have done (Freedle and Kostin, 1993;Kostin, 2004).
We would like to know whetherthe variation of word difficulty can be partiallyexpalined by a series of traceable linguisticfeatures.Based on the knowledge about the charac-teristics of Chinese grammar and the practicalexperiences of corpus annotation, we considerthe following surface linguistic features.
Inorder to explicitly display the relationship betweenthe linguistic predictors and the distribution ofthe word difficulty at a micro level, we dividethe difficulty scale into ten discrete intervalsand calculate the distributions of these linguisticfeatures on different ranges of difficulty.Here, we interpret the difficulties of the wordsfrom the perspective of three important linguisticfeatures:Idiom In Chinese, the 4-character idioms havespecial linguistic structures.
These structureusually form a different pattern that is hard forthe machine algorithm to understand.
Therefore,218811.43%58.1%(a) Idiom90.53%(b) Disyllabic words47.55%20.06%0.0-0.1 0.1-0.20.2-0.3 0.3-0.40.4-0.5 0.5-0.60.6-0.7 0.7-0.80.8-0.9 0.9-1.0(c) OOVFigure 2: Difficulty distribution of (a) idioms, (b) dysyllabic words and (c) Out-of-vocabulary wordsfrom PKU dataset.
Similar pattern has also been found in other datasets.it is reasonable to hypothesize that the an idiomphrase is more likely to be a difficult word for wordsegmentation task.
We can see from Figure 2athat 58.1% of idioms have a difficulty at (0.9,1].The proportion does increase with the degreeof difficulty, which corresponds with the humanintuition.Dissyllabic Word Disyllabic word is a wordformed by two consecutive Chinese characters.We can see from Figure 2b that the frequency ofdisyllabic words has a negative correlations withthe degree of difficulty.
This is an interestingresult.
It means that a two-syllable word patternis easy for a machine algorithm to recognize.
Thisis consistent with the lexical statistics (Yip, 2000),which shows that dissyllabic words account for64% of the common words in Chinese.Out-of-vocabulary Word Processing out-of-vocabulary (OOV) word is regarded as one ofthe key factors to the improvement of modelperformance.
Since these words never occur inthe training dataset, it is for sure that the wordsegmentation system will find it hard to correctlyrecognize these words from the contexts.
We cansee from Figure 2c that OOV generally has highdifficulty.
However, a lot of OOV is relativelyeasy for segmenters.All the linguistic predictors above prove that thedegree of difficulty, namely the weight for eachword, is not only rooted in the foundation of testtheory, but also correlated with linguistic intuition.5 Evaluation with New MetricHere we demonstrate the effectiveness of theproposed method in a real evaluation by re-analyzing the submission results from NLPCCP1 P2 P3 P4 P5 P6 P70.80.9Participants IDScore0.20.40.60.8f1fbH(a) Closed TrackP2 P1 P8 P5 P70.80.9Participants IDScore0.20.40.60.8f1fbH(b) Open TrackFigure 3: Comparisons of standard metric and ournew metric for the closed track and the open trackof NLPCC 2015 Weibo Text Word SegmentationShared Task.
The black lines for f1and fbareplotted against the left y-axis.
The red lines forhuman judgement scores are plotted against theright y-axis.2015 Shared Task2 of Chinese word segmentation.The dataset of this shared task is collected frommicro-blog text.
For convenience, we use WB torepresent this dataset in the following discussions.We select the submissions of all 7 participantsfrom the closed track and the submissions of all2Conference on Natural Language Processing and Chi-nese Computing.
http://tcci.ccf.org.cn/conference/2015/21890 - 0.1 0.1 - 0.2 0.2 - 0.3 0.3 - 0.4 0.4 - 0.5 0.5 - 0.6 0.6 - 0.7 0.7 - 0.8 0.8 - 0.9 0.9 - 1.00.70.80.91Degree of DifficultyAccuracyP1 P2 P3 P4 P5 P6 P7Figure 4: Accuracies of different participants in Closed Track by different difficulties on WB dataset.5 participants from the open track.
In the closedtrack, participants could only use informationfound in the provided training data.
In the opentrack, participants could use the information whichshould be public and be easily obtained.We compare the standard precision, recall andF-score with our new metric.
The result isdisplayed in Figure 3.
Considering the relatedprivacy issues, we will refer to the participantsas P1, P2, etc.
The order of these participants inthe sub-figures is sorted according to the originalranking given by the standard metric in eachtrack.
The same ID number refers to the sameparticipants.It is interesting to see that the proposed metricgives out significantly different rankings for theparticipants, compared to the original rankings.Based on the standard metric, Participant 1 (P1)ranks the top in closed track while P7 is rankedas the worst in both tracks.
However, P2 ranksfirst under the evaluation of the new metric in theClosed track.
P7 also get higher ranking than itsoriginal one.5.1 Correlation with Human JudgementTo tell whether the standard metric or the pro-posed metric is more reasonable, we asked threeexperts to evaluate the quality of the submissionsfrom the participants.
We randomly selected 50test sentences from the WB dataset.
For each testsentence, we present all the submitted candidatesegmentation results to the human judges inrandom order.
Then, the judges are asked tochoose the best candidate(s) with the highestsegmentation quality as well as the second-bestcandidate(s) among all the submissions.
Humanjudges had no access to the source of the sentences.Once we collect the human judgement of thesegmentation quality, we can compute the scorefor each participants.
If a candidate segmentationresult from a certain participant is ranked first forn times, then this participants earned n point.
Ifsecond for m times, then this participants earnedm2points.
Then we can get the probability ofa participants being ranked the best or sub-bestby computing n+m250.
Finally, we get the human-intuition-based gold ranking of the participantsthrough the means of scores from all the humanjudges.It is worth noticing that the ranking result ofour proposed metric correlates with the humanjudgements better than that of the standard metric,as is shown in Figure 3.
The Pearson corre-lation between our proposed metric and humanjudgements are 0.9056 (p = 0.004) for closedsession and 0.8799 (p = 0.04) for open sessionwhile the Pearson correlation between standardmetric and human judgements are only 0.096(p = 0.836) for closed session and 0.670 (p =0.216).
This evidence strongly supports that theproposed method is a good approximate of humanjudgements.5.2 Detailed AnalysisSince we have empirically got the degree ofdifficulty for each word in the test dataset, we cancompute the distribution of the difficulty for wordsthat have been correctly segmented.
We dividedthe whole range of difficulty into 10 intervals.Then, we count the ratio of the correct segmentedunits for each difficulty interval.
In this way,we can quantitatively measure to what extent thesegmentation system performs on difficult testcases and easy test cases.As is shown in Figure 4, P7 works better ondifficult cases than other systems, but the worston easy cases.
This explains why P7 gets goodrank based on the new evaluation metric.
Besides,2190P1 P2 P3 P4 P5 P6 P70.60.70.80.91Participants IDScorep ppunishmentprewardpb(a) Standard and Weighted PrecisionP1 P2 P3 P4 P5 P6 P70.60.81Participants IDScorer rpunishmentrrewardrb(b) Standard and Weighted RecallFigure 5: Comparisons of standard and weighted precision and recall on NLPCC Closed Track.if we compare P1 and P2, we will notice thatP2 performs just slightly worse than P1 on easycases, but much better than P1 on difficult cases.Therefore, conventional evaluation metric rank P1as the top system because the P1 gets a lot ofcredits from a large portion of easy cases.
Unlikeconventional metric, our new metric achievesbalance between hard cases and easy cases andranks P2 as the top system.The experiment result indicates that the newmetric can reveal the implicit difference andimprovement of the model, while standard metriccannot provide us with such a fine-grained result.0.3 0.4 0.5 0.6 0.70.40.6fbon parallel test 1fbonparalleltest2Figure 6: Correlation between the evaluationresults fbof two parallel testsets with the proposedmetrics on a collection of models.
The Pearsoncorrelation is 0.9961, p = 0.000.5.3 Validity and ReliabilityJones (1994) concluded some important criteriafor the evaluationmetrics of NLP system.
It is veryimportant to check the validity and reliability of anew metric.Previous section has displayed the validity ofthe proposed evaluation metric by comparing theevaluation results with human judgements.
Theevaluation results with our new metric correlatedwith human intuition well.Regarding reliability, we perform the parallel-test experiment.
We randomly split the test datasetinto two halves.
These two halves have similardifficulty distribution and, therefore, can be con-sidered as a parallel test.
Then different models,including those used in the first experiment, areevaluated on the first half and the second half.
Theresults in Figure 6 shows that the performancesof different models with our proposed evaluationmetric are significantly correlated in two paralleltests.5.4 Visualization of the WeightAs is known, there might be some annotationinconsistency in the dataset.
We find that mostof the cases with high weight are really valuabledifficult test cases, such as the visualized sentencesfromWB dataset in Figure 7.
In the first sentence,the word ?BMW ??
(NOUN.People who takebus, metro and then walk to the destination) isan OOV word and contains English characters.The weight of this word, as expected, is veryhigh.
In the second sentence, the word ??????
(VERB.not familiar with each other) is a 4-character Chinese idiom.
the conjunction word ????
(CONJ.even if) has structural ambiguity.
Itcan also be decomposed into a two-word phrase???
(ADV.just) and ???
(VERB.count).
From thevisualization of the weight, we can see that thesedifficult words are all given high weights.2191Data Corpus Size p r f1 pbrbfbPKU20% 90.04 89.90 89.97 45.22 43.37 44.2850% 92.87 91.58 92.22 54.24 49.12 51.5580% 94.07 92.21 93.13 61.80 54.74 58.05100% 94.03 92.91 93.47 64.22 59.16 61.59MSR20% 92.93 92.58 92.76 45.76 44.13 44.9350% 95.22 95.18 95.20 63.00 62.22 62.6080% 95.68 95.74 95.71 67.26 66.96 67.11100% 96.19 96.02 96.11 70.80 69.45 70.12NCC20% 87.32 86.37 86.84 42.16 40.23 41.1750% 89.34 89.03 89.19 50.31 49.26 49.7880% 91.42 91.10 91.26 60.48 59.25 59.86100% 92.00 91.77 91.89 63.72 62.70 63.20SXU20% 89.70 89.31 89.50 43.53 42.35 42.9350% 93.04 92.42 92.73 56.21 54.27 55.2380% 94.45 93.94 94.19 64.55 62.50 63.51100% 94.89 94.61 94.75 68.10 66.63 67.36Table 2: Model evaluation with standard metric and our new metric.
Models vary in the amount oftraining data and feature types.6 Comparisons on SIGHAN datasetsIn this section, we give comparisons onSIGHAN datasets.
We use four simplifiedChinese datasets: PKU and MSR (SIGHAN2005) as well as NCC and SXU (SIGHAN 2008).For each dataset, we train four segmenters withvarying abilities, based on 20%, 50%, 80% and100% of training data respectively.
The usedfeature template is F2 in Table 1.Table 2 shows the different evaluation resultswith standard metric and our balanced metric.We can see that the proposed evaluation metricgenerally gives lower and more distinguishablescore, compared with the standard metric.7 Related workEvaluation metrics has been a focused topicfor a long time.
Researchers have been tryingto evaluate various NLP tasks towards humanintuition (Papineni et al, 2002; Graham, 2015a;Graham, 2015b).
Previous work (Fournier andInkpen, 2012; Fournier, 2013; Pevzner and Hearst,2002) mainly deal with the near-miss error case onthe context of text segmentation.
Much attentionhas been given to different penalization for theerror.
These work criticize that traditional metricssuch as precision, recall and F-score, consider allthe error similar.
In this sense, some studies aimedat assigning different penalization to the word.
Wethink that these explorations can be regarded asthe foreshadowing of our evaluation metric thatbalances reward and punishment.Our paper differs from previous research inthat we take the difficulty of the test case intoconsideration, while previous works only focuson the variation of error types and penalisation.We involve the basic idea from psychometrics andimprove the evaluation with a balance betweendifficult cases and easy cases, reward and punish-ment.We would like to emphasize that our weightedevaluation metric is not a replacement of thetraditional precision, recall, and F-score.
Instead,our new weighted metrics can reveal more detailsthat traditional evaluation may not be able topresent.8 ConclusionIn this paper, we put forward a newpsychometric-inspired method for Chineseword segmentation evaluation by weighting allthe words in test dataset based on the methodologyapplied to psychological tests and standardizedexams.
We empirically analyze the validityand reliability of the new metric on a realevaluation dataset.
Experiment results revealthat our weighted evaluation metrics givesmore reasonable and distinguishable scores and2192B M W0.000.150.300.450.600.750.90by this way travel of people thus be called BMW man(a) Sentence 1043 in WB dataset0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.000.150.300.450.600.750.90even if  not familiar still will  give a hand    help(b) Sentence 3852 in WB datasetFigure 7: Visualising the word weight of WB dataset.correlates well with human judgement.
We willrelease the weighted datasets to the academiccommunity.Additionally, the proposed evaluation metriccan be easily extended to word segmentationtask for other languages (e.g.
Japanese) andother sequence labelling-based NLP tasks, withjust tiny changes.
Our metric also points out apromising direction for the researchers to take intothe account of the biased distribution of test casedifficulty and focus on tackling the hard bones ofnatural language processing.AcknowledgmentsWe would like to thank the anonymous re-viewers for their valuable comments.
This workwas partially funded by National Natural ScienceFoundation of China (No.
61532011, 61473092,and 61472088), the National High TechnologyResearch andDevelopment Program of China (No.2015AA015408).ReferencesA.L.
Berger, V.J.
Della Pietra, and S.A. Della Pietra.1996.
A maximum entropy approach to naturallanguage processing.
Computational Linguistics,22(1):39?71.Michael Collins.
2002.
Discriminative trainingmethods for hidden markov models: Theory andexperiments with perceptron algorithms.
In Pro-ceedings of the 2002 Conference on EmpiricalMethods in Natural Language Processing.T.
Emerson.
2005.
The second international Chineseword segmentation bakeoff.
In Proceedings of theFourth SIGHAN Workshop on Chinese LanguageProcessing, pages 123?133.
Jeju Island, Korea.Chris Fournier and Diana Inkpen.
2012.
Segmentationsimilarity and agreement.
In Proceedings of the2012 Conference of the North American Chapterof the Association for Computational Linguistics:Human Language Technologies, pages 152?161.Association for Computational Linguistics.Chris Fournier.
2013.
Evaluating text segmentationusing boundary edit distance.
In Proceedingsof the Annual Meeting of the Association forComputational Linguistics, pages 1702?1712.Roy Freedle and Irene Kostin.
1993.
The prediction oftoefl reading item difficulty: Implications for con-struct validity.
Language Testing, 10(2):133?170.Yvette Graham.
2015a.
Improving evaluation ofmachine translation quality estimation.
In Proceed-ings of the 53th Annual Meeting on Association forComputational Linguistics.Yvette Graham.
2015b.
Re-evaluating automaticsummarization with bleu and 192 shades of rouge.In Proceedings of EMNLP.C.
Jin and X. Chen.
2008.
The fourth internationalChinese language processing bakeoff: Chinese wordsegmentation, named entity recognition and Chinesepos tagging.
In Sixth SIGHANWorkshop on ChineseLanguage Processing, page 69.Karen Sparck Jones.
1994.
Towards better nlpsystem evaluation.
In Proceedings of the workshopon Human Language Technology, pages 102?107.Association for Computational Linguistics.Irene Kostin.
2004.
Exploring item characteristics thatare related to the difficulty of toefl dialogue items.ETS Research Report Series, 2004(1):i?59.John D. Lafferty, Andrew McCallum, and FernandoC.
N. Pereira.
2001.
Conditional random fields:Probabilistic models for segmenting and labelingsequence data.
In Proceedings of the EighteenthInternational Conference on Machine Learning.Gina-Anne Levow.
2006.
The third international chi-nese language processing bakeoff: Word segmenta-tion and named entity recognition.
In Proceedings ofthe Fifth SIGHAN Workshop on Chinese LanguageProcessing, pages 108?117, Sydney, Australia, July.2193Frederic M Lord, Melvin R Novick, and AllanBirnbaum.
1968.
Statistical Theories of Mental TestScores.
Addison-Wesley.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: A method for automaticevaluation of machine translation.
In Proceedings ofthe 40th annual meeting on association for compu-tational linguistics, pages 311?318.
Association forComputational Linguistics.F.
Peng, F. Feng, and A. McCallum.
2004.
Chi-nese segmentation and new word detection usingconditional random fields.
Proceedings of the20th international conference on ComputationalLinguistics.Lev Pevzner and Marti A Hearst.
2002.
Acritique and improvement of an evaluation metricfor text segmentation.
Computational Linguistics,28(1):19?36.Educational Testing Service.
2000.
Computer-BasedTOEFL Score User Guide.
Princeton, NJ.N.
Xue.
2003.
Chinese word segmentation as charactertagging.
Computational Linguistics and ChineseLanguage Processing, 8(1):29?48.Po-Ching Yip.
2000.
The Chinese Lexicon: AComprehensive Survey.
Psychology Press.H.
Zhao and Q. Liu.
2010.
The cips-sighan clp 2010chinese word segmentation bakeoff.
In Proceedingsof the First CPS-SIGHAN Joint Conference onChinese Language Processing.2194
