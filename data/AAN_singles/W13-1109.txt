Proceedings of the Workshop on Language in Social Media (LASM 2013), pages 80?89,Atlanta, Georgia, June 13 2013. c?2013 Association for Computational LinguisticsTranslating Government Agencies?
Tweet Feeds:Specificities, Problems and (a few) SolutionsFabrizio Gotti, Philippe Langlais{gottif,felipe}@iro.umontreal.caRALI-DIROUniversite?
de Montre?alC.P.
6128, Succ Centre-VilleMontre?al (Que?bec) CanadaH3C 3J7Atefeh Farzindarfarzindar@nlptechnologies.caNLP Technologies Inc.52 Le RoyerMontre?al(Que?bec) CanadaH2Y 1W7AbstractWhile the automatic translation of tweets hasalready been investigated in different scenar-ios, we are not aware of any attempt to trans-late tweets created by government agencies.In this study, we report the experimental re-sults we obtained when translating 12 Twitterfeeds published by agencies and organizationsof the government of Canada, using a state-of-the art Statistical Machine Translation (SMT)engine as a black box translation device.
Wemine parallel web pages linked from the URLscontained in English-French pairs of tweets inorder to create tuning and training material.For a Twitter feed that would have been other-wise difficult to translate, we report significantgains in translation quality using this strategy.Furthermore, we give a detailed account of theproblems we still face, such as hashtag trans-lation as well as the generation of tweets oflegal length.1 IntroductionTwitter is currently one of the most popular onlinesocial networking service after Facebook, and is thefastest-growing, with the half-a-billion user markreached in June 2012.1 According to Twitter?s blog,no less than 65 millions of tweets are published eachday, mostly in a single language (40% in English).This hinders the spread of information, a situationwitnessed for instance during the Arab Spring.1http://semiocast.com/publications/2012_07_30_Twitter_reaches_half_a_billion_accounts_140m_in_the_USSolutions for disseminating tweets in differentlanguages have been designed.
One solution con-sists in manually translating tweets, which of courseis only viable for a very specific subset of the ma-terial appearing on Twitter.
For instance, the non-profit organization Meedan2 has been founded in or-der to organize volunteers willing to translate tweetswritten in Arabic on Middle East issues.
Anothersolution consists in using machine translation.
Sev-eral portals are facilitating this,3 mainly by usingGoogle?s machine translation API.Curiously enough, few studies have focused onthe automatic translation of text produced within so-cial networks, even though a growing number ofthese studies concentrate on the automated process-ing of messages exchanged on social networks.
See(Gimpel et al 2011) for a recent review of some ofthem.Some effort has been invested in translating shorttext messages (SMSs).
Notably, Munro (2010) de-scribes the service deployed by a consortium of vol-unteer organizations named ?Mission 4636?
duringthe earthquake that struck Haiti in January 2010.This service routed SMSs alerts reporting trappedpeople and other emergencies to a set of volunteerswho translated Haitian Creole SMSs into English,so that primary emergency responders could under-stand them.
In Lewis (2010), the authors describehow the Microsoft translation team developed a sta-tistical translation engine (Haitian Creole into En-glish) in as little as 5 days, during the same tragedy.2http://news.meedan.net/3http://www.aboutonlinetips.com/twitter-translation-tools/80Jehl (2010) addresses the task of translating En-glish tweets into German.
She concludes that theproper treatment of unknown words is of the utmostimportance and highlights the problem of producingtranslations of up to 140 characters, the upper limiton tweet lengths.
In (Jehl et al 2012), the authorsdescribe their efforts to collect bilingual tweets froma stream of tweets acquired programmatically, andshow the impact of such a collection on developingan Arabic-to-English translation system.The present study participates in the effort for thedissemination of messages exchanged over Twitterin different languages, but with a very narrow focus,which we believe has not been addressed specificallyyet: Translating tweets written by government in-stitutions.
What sets these messages apart is that,generally speaking, they are written in a proper lan-guage (without which their credibility would pre-sumably be hurt), while still having to be extremelybrief to abide by the ever-present limit of 140 char-acters.
This contrasts with typical social media textsin which a large variability in quality is observed(Agichtein et al 2008).Tweets from government institutions can also dif-fer somewhat from some other, more informal so-cial media texts in their intended audience and ob-jectives.
Specifically, such tweet feeds often attemptto serve as a credible source of timely informationpresented in a way that engages members of thelay public.
As such, translations should present asimilar degree of credibility, ease of understanding,and ability to engage the audience as in the sourcetweet?all while conforming to the 140 characterlimits.This study attempts to take these matters into ac-count for the task of translating Twitter feeds emittedby Canadian governmental institutions.
This couldprove very useful, since more than 150 Canadianagencies have official feeds.
Moreover, while onlycounting 34 million inhabitants, Canada ranks fifthin the number of Twitter users (3% of all users) afterthe US, the UK, Australia, and Brazil.4 This cer-tainly explains why Canadian governments, politi-cians and institutions are making an increasing useof this social network service.
Given the need of4http://www.techvibes.com/blog/how-canada-stacks-up-against-the-world-on-twitter-2012-10-17Canadian governmental institutions to disseminateinformation in both official languages (French andEnglish), we see a great potential value in targetedcomputer-aided translation tools, which could offera significant reduction over the current time and ef-fort required to manually translate tweets.We show that a state-of-the-art SMT toolkit, usedoff-the-shelf, and trained on out-domain data is un-surprisingly not up to the task.
We report in Sec-tion 2 our efforts in mining bilingual material fromthe Internet, which proves eventually useful in sig-nificantly improving the performance of the engine.We test the impact of simple adaptation scenariosin Section 3 and show the significant improvementsin BLEU scores obtained thanks to the corpora wemined.
In Section 4, we provide a detailed accountof the problems that remain to be solved, includingthe translation of hashtags (#-words) omnipresentin tweets and the generation of translations of legallengths.
We conclude this work-in-progress and dis-cuss further research avenues in Section 5.2 Corpora2.1 Bilingual Twitter FeedsAn exhaustive list of Twitter feeds published byCanadian government agencies and organizationscan be found on the GOV.PoliTWiTTER.ca website.5 As of this writing, 152 tweet feeds are listed,most of which are available in both French and En-glish, in keeping with the Official Languages Actof Canada.
We manually selected 20 of these feedpairs, using various exploratory criteria, such astheir respective government agency, the topics beingaddressed and, importantly, the perceived degree ofparallelism between the corresponding French andEnglish feeds.All the tweets of these 20 feed pairs were gatheredusing Twitter?s Streaming API on 26 March 2013.We filtered out the tweets that were marked by theAPI as retweets and replies, because they rarely havean official translation.
Each pair of filtered feedswas then aligned at the tweet level in order to cre-ate bilingual tweet pairs.
This step was facilitatedby the fact that timestamps are assigned to eachtweet.
Since a tweet and its translation are typi-5http://gov.politwitter.ca/directory/network/twitter81Tweets URLs mis.
probs sents.
HealthCanada1489 995 1 252 78,847.
DFAIT MAECI ?
Foreign Affairs and Int?l Trade1433 65 0 1081 10,428. canadabusiness1265 623 1 363 138,887. pmharper ?
Prime Minister Harper752 114 2 364 12,883.
TCS SDC ?
Canadian Trade Commissioner Service694 358 1 127 36,785.
Canada Trade601 238 1 92 22,594.
PHAC GC ?
Public Health Canada555 140 0 216 14,617. cida ca ?
Canadian Int?l Development Agency546 209 2 121 18,343.
LibraryArchives490 92 1 171 6,946.
CanBorder ?
Canadian Border matters333 88 0 40 9,329.
Get Prepared ?
Emergency preparedness314 62 0 11 10,092.
Safety Canada286 60 1 17 3,182Table 1: Main characteristics of the Twitter and URL cor-pora for the 12 feed pairs we considered.
The (English)feed name is underlined, and stands for the pair of feedsthat are a translation of one another.
When not obvious,a short description is provided.
Each feed name can befound as is on Twitter.
See Sections 2.1 and 2.3 for more.cally issued at about the same time, we were able toalign the tweets using a dynamic programming al-gorithm miminizing the total time drift between theEnglish and the French feeds.
Finally, we tokenizedthe tweets using an adapted version of Twokenize(O?Connor et al 2010), accounting for the hashtags,usernames and urls contained in tweets.We eventually had to narrow down further thenumber of feed pairs of interest to the 12 most pro-lific ones.
For instance, the feed pair PassportCan6that we initially considered contained only 54 pairsof English-French tweets after filtering and align-ment, and was discarded because too scarce.6https://twitter.com/PassportCanDid you know it?s best to test for #radon inthe fall/winter?
http://t.co/CDubjbpS#health #safetyL?automne/l?hiver est le meilleur moment pourtester le taux de radon.http://t.co/4NJWJmuN #sante?
#se?curiteFigure 1: Example of a pair of tweets extracted from thefeed pair HealthCanada .The main characteristics of the 12 feed pairs weultimately retained are reported in Table 1, for a to-tal of 8758 tweet pairs.
The largest feed, in termsof the number of tweet pairs used, is that of HealthCanada7 with over 1 489 pairs of retained tweetspairs at the time of acquisition.
For reference, thatis 62% of the 2 395 ?raw?
tweets available on theEnglish feed, before filtering and alignment.
An ex-ample of a retained pair of tweets is shown in Fig-ure 1.
In this example, both tweets contain a short-ened url alias that (when expanded) leads to web-pages that are parallel.
Both tweets also contain so-called hashtags (#-words): 2 of those are correctlytranslated when going from English to French, butthe hashtag #radon is not translated into a hashtag inFrench, instead appearing as the plain word radon,for unknown reasons.2.2 Out-of-domain Corpora: ParliamentDebatesWe made use of two different large corpora in or-der to train our baseline SMT engines.
We used the2M sentence pairs of the Europarl version 7 corpus.8This is a priori an out-of-domain corpus, and we didnot expect much of the SMT system trained on thisdataset.
Still, it is one of the most popular parallelcorpus available to the community and serves as areference.We also made use of 2M pairs of sentences weextracted from an in-house version of the CanadianHansard corpus.
This material is not completely out-of-domain, since the matters addressed within theCanadian Parliament debates likely coincide to somedegree with those tweeted by Canadian institutions.The main characteristics of these two corpora are re-ported in Table 2.
It is noteworthy that while both7https://twitter.com/HealthCanada8http://www.statmt.org/europarl/82Corpus sents tokens types s lengthhansard en 2M 27.1M 62.2K 13.6hansard fr 2M 30.7M 82.2K 15.4europarl en 2M 55.9M 94.5K 28.0europarl fr 2M 61.6M 129.6K 30.8Table 2: Number of sentence pairs, token and token typesin the out-of-domain training corpora we used.
s lengthstands for the average sentence length, counted in tokens.corpora contain an equal number of sentence pairs,the average sentence length in the Europarl corpus ismuch higher, leading to a much larger set of tokens.2.3 In-domain Corpus: URL CorpusAs illustrated in Figure 1, many tweets act as?teasers?, and link to web pages containing (much)more information on the topic the tweet feed typi-cally addresses.
Therefore, a natural way of adapt-ing a corpus-driven translation engine consists inmining the parallel text available at those urls.In our case, we set aside the last 200 tweet pairs ofeach feed as a test corpus.
The rest serves as the url-mining corpus.
This is necessary to avoid testing oursystem on test tweets whose URLs have contributedto the training corpus.Although simple in principle, this data collectionoperation consists in numerous steps, outlined be-low:1.
Split each feed pair in two: The last 200 tweetpairs are set aside for testing purposes, the restserves as the url-mining corpus used in the fol-lowing steps.2.
Isolate urls in a given tweet pair using our to-kenizer, adapted to handle Twitter text (includ-ing urls).3.
Expand shortened urls.
For instance, the urlin the English example of Figure 1 wouldbe expanded into http://www.hc-sc.gc.ca/ewh-semt/radiation/radon/testing-analyse-eng.php, using theexpansion service located at the domain t.co.There are 330 such services on the Web.4.
Download the linked documents.5.
Extract all text from the web pages, without tar-geting any content in particular (the site menus,breadcrumb, and other elements are thereforeretained).6.
Segment the text into sentences, and tokenizethem into words.7.
Align sentences with our in-house aligner.We implemented a number of restrictions duringthis process.
We did not try to match urls in caseswhere the number of urls in each tweet differed (seecolumn mis.
?mismatches?in Table 1).
The col-umn probs.
(problems) in Table 1 shows the count ofurl pairs whose content could not be extracted.
Thishappened when we encountered urls that we couldnot expand, as well as those returning a 404 HTTPerror code.
We also rejected urls that were identi-cal in both tweets, because they obviously could notbe translations.
We also filtered out documents thatwere not in html format, and we removed documentpairs where at least one document was difficult toconvert into text (e.g.
because of empty content, orproblematic character encoding).
After inspection,we also decided to discard sentences that countedless than 10 words, because shorter sentences aretoo often irrelevant website elements (menu items,breadcrumbs, copyright notices, etc.
).This 4-hour long operation (including download)yielded a number of useful web documents and ex-tracted sentence pairs reported in Table 1 (columnsURLs and sents respectively).
We observed that thedensity of url pairs present in pairs of tweets variesamong feeds.
Still, for all feeds, we were able togather a set of (presumably) parallel sentence pairs.The validity of our extraction process rests on thehypothesis that the documents mentioned in eachpair of urls are parallel.
In order to verify this, wemanually evaluated (a posteriori) the parallelness ofa random sample of 50 sentence pairs extracted foreach feed.
Quite fortunately, the extracted materialwas of excellent quality, with most samples contain-ing all perfectly aligned sentences.
Only canadabusi-ness, LibraryArchives and CanBorder counted a sin-gle mistranslated pair.
Clearly, the websites of theCanadian institutions we mined are translated withgreat care and the tweets referring to them are metic-ulously translated in terms of content links.833 Experiments3.1 MethodologyAll our translation experiments were conducted withMoses?
EMS toolkit (Koehn et al 2007), which inturn uses gizapp (Och and Ney, 2003) and SRILM(Stolcke, 2002).As a test bed, we used the 200 bilingual tweetswe acquired that were not used to follow urls, as de-scribed in Sections 2.1 and 2.3.
We kept each feedseparate in order to measure the performance of oursystem on each of them.
Therefore we have 12 testsets.We tested two configurations: one in which anout-of-domain translation system is applied (with-out adaptation) to the translation of the tweets ofour test material, another one where we allowed thesystem to look at in-domain data, either at trainingor at tuning time.
The in-domain material we usedfor adapting our systems is the URL corpus we de-scribed in section 2.3.
More precisely, we prepared12 tuning corpora, one for each feed, each contain-ing 800 heldout sentence pairs.
The same number ofsentence pairs was considered for out-domain tuningsets, in order not to bias the results in favor of largersets.
For adaptation experiments conducted at train-ing time, all the URL material extracted from a spe-cific feed (except for the sentences of the tuning sets)was used.
The language model used in our experi-ments was a 5-gram language model with Kneser-Ney smoothing.It must be emphasized that there is no tweet mate-rial in our training or tuning sets.
One reason for thisis that we did not have enough tweets to populate ourtraining corpus.
Also, this corresponds to a realisticscenario where we want to translate a Twitter feedwithout first collecting tweets from this feed.We use the BLEU metric (Papineni et al 2002)as well as word-error rate (WER) to measure trans-lation quality.
A good translation system maximizesBLEU and minimizes WER.
Due to initially poorresults, we had to refine the tokenizer mentionedin Section 2.1 in order to replace urls with serial-ized placeholders, since those numerous entities typ-ically require rule-based translations.
The BLEUand WER scores we report henceforth were com-puted on such lowercased, tokenized and serializedtexts, and did not incur penalties that would havetrain tune canadabusiness DFAIT MAECIfr?en wer bleu wer bleuhans hans 59.58 21.16 61.79 19.55hans in 58.70 21.35 60.73 20.14euro euro 64.24 15.88 62.90 17.80euro in 63.23 17.48 60.58 21.23en?fr wer bleu wer bleuhans hans 62.42 21.71 64.61 21.43hans in 61.97 22.92 62.69 22.00euro euro 64.66 19.52 63.91 21.65euro in 64.61 18.84 63.56 22.31Table 3: Performance of generic systems versus systemsadapted at tuning time for two particular feeds.
The tunecorpus ?in?
stands for the URL corpus specific to the feedbeing translated.
The tune corpora ?hans?
and ?euro?
areconsidered out-of-domain for the purpose of this experi-ment.otherwise been caused by the non-translation of urls(unknown tokens), for instance.3.2 Translation ResultsTable 3 reports the results observed for the two mainconfigurations we tested, in both translation direc-tions.
We show results only for two feeds here:canadabusiness, for which we collected the largestnumber of sentence pairs in the URL corpus, andDFAIT MAECI for which we collected very littlematerial.
For canadabusiness, the performance of thesystem trained on Hansard data is higher than thatof the system trained on Europarl (?
ranging from2.19 to 5.28 points of BLEU depending on the con-figuration considered).
For DFAIT MAECI , supris-ingly, Europarl gives a better result, but by a morenarrow margin (?
ranging from 0.19 to 1.75 pointsof BLEU).
Both tweet feeds are translated withcomparable performance by SMT, both in termsof BLEU and WER.
When comparing BLEU per-formances based solely on the tuning corpus used,the in-domain tuning corpus created by mining urlsyields better results than the out-domain tuning cor-pus seven times out of eight for the results shown inTable 3.The complete results are shown in Figure 2, show-ing BLEU scores obtained for the 12 feeds we con-sidered, when translating from English to French.Here, the impact of using in-domain data to tune840.000.050.100.150.200.250.300.35in out in out in out in out in out in out in out in out in out in out in out in outCanada_Trade canadabusiness CanBorder cida_ca DFAIT_MAECI Get_Prepared HealthCanada LibraryArchives PHAC_GC pmharper Safety_Canada TCS_SDCBLEUscoreeuro hanstraincorpus tuneMoyenne de bleudirectionFigure 2: BLEU scores measured on the 12 feed pairs we considered for the English-to-French translation direction.For each tweet test corpus, there are 4 results: a dark histogram bar refers to the Hansard training corpus, while alighter grey bar refers to an experiment where the training corpus was Europarl.
The ?in?
category on the x-axisdesignates an experiment where the tuning corpus was in-domain (URL corpus), while the ?out?
category refers to anout-of-domain tuning set.
The out-of-domain tuning corpus is Europarl or Hansard, and always matches the nature oftraining corpora.the system is hardly discernible, which in a senseis good news, since tuning a system for each feedis not practical.
The Hansard corpus almost alwaysgives better results, in keeping with its status as acorpus that is not so out-of-domain as Europarl, asmentioned above.
The results for the reverse trans-lation direction show the same trends.In order to try a different strategy than using onlytuning corpora to adapt the system, we also investi-gated the impact of training the system on a mix ofout-of-domain and in-domain data.
We ran one ofthe simplest adaptation scenarios where we concate-nated the in-domain material (train part of the URLcorpus) to the out-domain one (Hansard corpus) forthe two feeds we considered in Table 3.
The resultsare reported in Table 4.We measured significant gains both in WER andBLEU scores in conducting training time versus tun-ing time adaptation, for the canadabusiness feed (thelargest URL corpus).
For this corpus, we observean interesting gain of more than 6 absolute points inBLEU scores.
However, for the DFAIT MAECI (thesmallest URL corpus) we note a very modest loss intranslation quality when translating from French anda significant gain in the other translation direction.These figures could show that mining parallel sen-tences present in URLs is a fruitful strategy for adapt-ing the translation engine for feeds like canadabusi-ness that display poor performance otherwise, with-out harming the translation quality for feeds that per-Train corpus WER BLEUfr?enhans+canbusiness 53.46 (-5.24) 27.60 (+6.25)hans+DFAIT 60.81 (+0.23) 20.83 (-0.40)en?frhans+canbusiness 57.07 (-4.90) 26.26 (+3.34)hans+DFAIT 61.80 (-0.89) 24.93 (+2.62)Table 4: Performance of systems trained on a concatena-tion of out-of-domain and in-domain data.
All systemswere tuned on in-domain data.
Absolute gains are shownin parentheses, over the best performance achieved so far(see Table 3).form reasonably well without additional resources.Unfortunately, it suggests that retraining a system isrequired for better performance, which might hinderthe deployment of a standalone translation engine.Further research needs to be carried out to determinehow many tweet pairs must be used in a parallel URLcorpus in order to get a sufficiently good in-domaincorpus.4 Analysis4.1 Translation outputExamples of translations produced by the best sys-tem we trained are reported in Figure 3.
The firsttranslation shows a case of an unknown French word(soumissionnez).
The second example illustrates85a typical example where the hashtags should havebeen translated but were left unchanged.
The thirdexample shows a correct translation, except that thelength of the translation (once the text is detok-enized) is over the size limit allowed for a tweet.Those problems are further analyzed in the remain-ing subsections.4.2 Unknown wordsUnknown words negatively impact the quality ofMT output in several ways.
First, they typically ap-pear untranslated in the system?s output (we deemedmost appropriate this last resort strategy).
Sec-ondly, they perturb the language model, which oftencauses other problems (such as dubious word order-ing).
Table 5 reports the main characteristics of thewords from all the tweets we collected that were notpresent in the Hansard train corpus.The out-of-vocabulary rate with respect to tokentypes hovers around 33% for both languages.
Noless than 42% (resp.
37%) of the unknown English(resp.
French) token types are actually hashtags.
Wedefer their analysis to the next section.
Also, 15%(resp.
10%) of unknown English token types areuser names (@user), which do not require transla-tion.English Frenchtweet tokens 153 234 173 921tweet types 13 921 15 714OOV types 4 875 (35.0%) 5 116 (32.6%).
hashtag types 2 049 (42.0%) 1 909 (37.3%).
@user types 756 (15.5%) 521 (10.2%)Table 5: Statistics on out-of-vocabulary token types.We manually analyzed 100 unknown token typesthat were not hashtags or usernames and that did notcontain any digit.
We classified them into a num-ber of broad classes whose distributions are reportedin Table 6 for the French unknown types.
A simi-lar distribution was observed for English unknowntypes.
While we could not decide of the nature of21 types without their context of use (line ?type),we frequently observed English types, as well asacronyms and proper names.
A few unknown typesresult from typos, while many are indeed true Frenchtypes unseen at training time (row labeled french ),some of which being very specific (term).
Amus-ingly, the French verbal neologism twitter (to tweet)is unknown to the Hansard corpus we used.french 26 sautez, perforateurs , twitterenglish 22 successful , beauty?types 21 bumbo , traname 11 absorbica , konzonguiziacronym 7 hna , rnctypo 6 gazouilli , pendanterm 3 apostasie , sibutramineforeign 2 aanischaaukamikw, aliskirenothers 2 francophonesURLTable 6: Distribution of 100 unknown French token types(excluding hashtags and usernames).4.3 Dealing with HashtagsWe have already seen that translating the text inhashtags is often suitable, but not always.
Typically,hashtags in the middle of a sentence are to be trans-lated, while those at the end typically should not be.A model should be designed for learning when totranslate an hashtag or not.
Also, some hashtags arepart of the sentence, while others are just (semantic)tags.
While a simple strategy for translating hash-tags consists in removing the # sign at translationtime, then restoring it afterwards, this strategy wouldfail in a number of cases that require segmenting thetext of the hashtag first.
Table 7 reports the per-centage of hashtags that should be segmented beforebeing translated, according to a manual analysis weconducted over 1000 hashtags in both languages weconsidered.
While many hashtags are single words,roughly 20% of them are not and require segmenta-tion.4.4 Translating under size constraintsThe 140 character limit Twitter imposes on tweets iswell known and demands a certain degree of conci-sion even human users find sometimes bothersome.For machine output, this limit becomes a challeng-ing problem.
While there exists plain?but inelegant?workarounds9, there may be a way to produce tweettranslations that are themselves Twitter-ready.
(Jehl,9The service eztweets.com splits long tweets into smallerones; twitlonger.com tweets the beginning of a long message,86SRC: vous soumissionnez pour obtenir de gros contrats ?
voici 5 pratiques exemplaires a` suivre .
URLTRA: you soumissionnez big contracts for best practices ?
here is 5 URL to follow .REF: bidding on big contracts ?
here are 5 best practices to follow .
URLSRC: avis de #sante?publique : maladies associe?es aux #salmonelles et a` la nourriture pour animaux de com-pagnie URL #rappelTRA: notice of #sante?publique : disease associated with the #salmonelles and pet food #rappel URLREF: #publichealth notice : illnesses related to #salmonella and #petfood URL #recallSRC: des ha?
?tiens de tous les a?ges , milieux et me?tiers te?moignent de l?
aide qu?
ils ont rec?ue depuis le se?isme.
URL #ha?
?tiTRA: the haitian people of all ages and backgrounds and trades testify to the assistance that they have receivedfrom the earthquake #ha?
?ti URL .REF: #canada in #haiti : haitians of all ages , backgrounds , and occupations tell of the help they received .URLFigure 3: Examples of translations produced by an engine trained on a mix of in- and out-of-domain data.w.
en fr example1 76.5 79.9 intelligence2 18.3 11.9 gender equality3 4.0 6.0 africa trade mission4 1.0 1.4 closer than you think5 0.2 0.6 i am making a difference6 ?
0.2 fonds aide victime se?cheresseafrique estTable 7: Percentage of hashtags that require segmentationprior to translation.
w. stands for the number of wordsinto which the hashtag text should be segmented.2010) pointed out this problem and reported that3.4% of tweets produced were overlong, when trans-lating from German to English.
The reverse direc-tions produced 17.2% of overlong German tweets.To remedy this, she tried modifying the way BLEUis computed to penalize long translation during thetuning process, with BLEU scores worse than sim-ply truncating the illegal tweets.
The second strategythe author tried consisted in generating n-best listsand mining them to find legal tweets, with encour-aging results (for n = 30 000), since the numberof overlong tweets was significantly reduced whileleaving BLEU scores unharmed.In order to assess the importance of the problemfor our system, we measured the lengths of tweetsthat a system trained like hans+canbusiness in Ta-ble 4 (a mix of in- and out-of-domain data) couldproduce.
This time however, we used a larger test setand provides a link to read the remainder.
One could also simplytruncate an illegal tweet and hope for the best...counting 498 tweets.
To measure the lengths of theirtranslations, we first had to detokenize the transla-tions produced, since the limitation applies to ?nat-ural?
text only.
For each URL serialized token, wecounted 18 characters, the average length of a (short-ened) url in a tweet.
When translating from Frenchto English, the 498 translations had lengths rangingfrom 45 to 138 characters; hence, they were all legaltweets.
From English to French, however, the trans-lations are longer, and range from 32 characters to223 characters, with 22.5% of them overlong.One must recall that in our experiments, no tweetswere seen at training or tuning time, which explainswhy the rate of translations that do not meet thelimit is high.
This problem deserves a specific treat-ment for a system to be deployed.
One interest-ing solution already described by (Jehl, 2010) is tomine the n-best list produced by the decoder in or-der to find the first candidate that constitutes a legaltweet.
This candidate is then picked as the trans-lation.
We performed this analysis on the canad-abusiness output described earlier, from English toFrench.
We used n =1, 5, 10, 20, 50, 100, 200, 500,1000, 5000, 10000, 30000 and computed the result-ing BLEU scores and remaining percentage of over-long tweets.
The results are shown in Figure 4.
Theresults clearly show that the n-best list does containalternate candidates when the best one is too long.Indeed, not only do we observe that the percentageof remaining illegal tweets can fall steadily (from22.4% to 6.6% for n = 30 000) as we dig deeper intothe list, but also the BLEU score stays unharmed,showing even a slight improvement, from an ini-87tial 26.16 to 26.31 for n = 30 000.
This counter-intuitive result in terms of BLEU is also reportedin (Jehl, 2010) and is probably due to a less harshbrevity penalty by BLEU on shorter candidates.2013-03-01nbest listn wer ser BLEU1 /u/gottif/proj/nlptech2012/data/tweets/correction/nbest/trans.159.35 97.79 0.26 65 /u/gottif/proj/nlptech2012/data/tweets/correction/nbest/trans.559.3 97.79 0.262210 /u/gottif/proj/nlptech2012/data/tweets/correction/nbest/trans.1059.31 97.79 0.262320 /u/gottif/proj/nlptech2012/data/tweets/correction/nbest/trans.2059.31 97.79 0.26 250 /u/gottif/proj/nlptech2012/data/tweets/correction/nbest/trans.5059.34 97.79 0.2622100 /u/gottif/proj/nlptech2012/data/tweets/correction/nbest/trans.10059.27 97.79 0.2628200 /u/gottif/proj/nlptech2012/data/tweets/correction/nbest/trans.20059.23 97.79 0.2633500 /u/gottif/proj/nlptech2012/data/tweets/correction/nbest/trans.50059.24 97.79 0 2631000 /u/gottif/proj/nlptech2012/data/tweets/correction/nbest/trans.100059.21 97.79 0.26345000 /u/gottif/proj/nlptech2012/data/tweets/correction/nbest/trans.500059.26 97.79 0.263510000 /u/gottif/proj/nlptech2012/data/tweets/correction/nbest/trans.1000059.26 97.79 0.263830000 /u/gottif/p j/nlptech2012/data/tweets/correction/nbest/trans.3000059.32 97.79 0.26 10%5%10%15%20%25%0.2610.2620.2620.2630.2630.2640.2641 10 100 1000 10000 100000%of overlongtweetsBLEUscoren-best list length (n)BLEU % overlongFigure 4: BLEU scores and percentage of overlongtweets when mining the n-best list for legal tweets, whenthe first candidate is overlong.
The BLEU scores (dia-mond series) should be read off the left-hand vertical axis,while the remaining percentage of illegal tweets (circleseries) should be read off the right-hand axis.5 DiscussionWe presented a number of experiments where wetranslated tweets produced by Canadian govern-ments institutions and organizations.
Those tweetshave the distinguishing characteristic (in the Twitter-sphere) of being written in proper English or French.We show that mining the urls mentioned in thosetweets for parallel sentences can be a fruitful strat-egy for adapting an out-of-domain translation engineto this task, although further research could showother ways of using this resource, whose qualityseems to be high according to our manual evalua-tion.
We also analyzed the main problems that re-main to be addressed before deploying a useful sys-tem.While we focused here on acquiring useful cor-pora for adapting a translation engine, we admit thatthe adaptation scenario we considered is very sim-plistic, although efficient.
We are currently inves-tigating the merit of different methods to adaptation(Zhao et al 2004; Foster et al 2010; Daume III andJagarlamudi, 2011; Razmara et al 2012; Sankaranet al 2012).Unknown words are of concern, and should bedealt with appropriately.
The serialization of urlswas natural, but it could be extended to usernames.The latter do not need to be translated, but reduc-ing the vocabulary is always desirable when work-ing with a statistical machine translation engine.One interesting subcategories of out-of-vocabularytokens are hashtags.
According to our analysis,they require segmentation into words before beingtranslated in 20% of the cases.
Even if they aretransformed into regular words (#radon?radon or#genderequality?gender equality), however, it isnot clear at this point how to detect if they are usedlike normally-occurring words in a sentence, as in(#radon is harmful) or if they are simply tags addedto the tweet to categorize it.We also showed that translating under size con-straints can be handled easily by mining the n-bestlist produced by the decoder, but only up to a point.A remaining 6% of the tweets we analyzed in detailcould not find a shorter version.
Numerous ideasare possible to alleviate the problem.
One could forinstance modify the logic of the decoder to penal-ize hypotheses that promise to yield overlong trans-lations.
Another idea would be to manually in-spect the strategies used by governmental agencieson Twitter when attempting to shorten their mes-sages, and to select those that seem acceptable andimplementable, like the suppression of articles or theuse of authorized abbreviations.Adapting a translation pipeline to the very specificworld of governmental tweets therefore poses mul-tiple challenges, each of which can be addressed innumerous ways.
We have reported here the results ofa modest but fertile subset of these adaptation strate-gies.AcknowledgmentsThis work was funded by a grant from the Natu-ral Sciences and Engineering Research Council ofCanada.
We also wish to thank Houssem EddineDridi for his help with the Twitter API.88ReferencesEugene Agichtein, Carlos Castillo, Debora Donato, Aris-tides Gionis, and Gilad Mishne.
2008.
Finding high-quality content in social media.
In Proceedings ofthe 2008 International Conference on Web Search andData Mining, WSDM ?08, pages 183?194.Hal Daume III and Jagadeesh Jagarlamudi.
2011.
Do-main adaptation for machine translation by mining un-seen words.
In 49th ACL, pages 407?412, Portland,Oregon, USA, June.George Foster, Cyril Goutte, and Roland Kuhn.
2010.Discriminative instance weighting for domain adap-tation in statistical machine translation.
In EMNLP,pages 451?459, Cambridge, MA, October.Kevin Gimpel, Nathan Schneider, Brendan O?Connor,Dipanjan Das, Daniel Mills, Jacob Eisenstein, MichaelHeilman, Dani Yogatama, Jeffrey Flanigan, andNoah A. Smith.
2011.
Part-of-speech tagging for twit-ter: Annotation, features, and experiments.
In ACL(Short Papers), pages 42?47.Laura Jehl, Felix Hieber, and Stefan Riezler.
2012.
Twit-ter translation using translation-based cross-lingual re-trieval.
In 7th Workshop on Statistical Machine Trans-lation, pages 410?421, Montre?al, June.Laura Jehl.
2010.
Machine translation for twitter.
Mas-ter?s thesis, School of Philosophie, Psychology andLanguage Studies, University of Edinburgh.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-burch, Richard Zens, Rwth Aachen, Alexan-dra Constantin, Marcello Federico, Nicola Bertoldi,Chris Dyer, Brooke Cowan, Wade Shen, ChristineMoran, and Ondr?ej Bojar.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
pages 177?180.William D. Lewis.
2010.
Haitian creole: How to buildand ship an mt engine from scratch in 4 days, 17 hours,& 30 minutes.
In EAMT, Saint-Raphael.Robert Munro.
2010.
Crowdsourced translation foremergency response in Haiti: the global collaborationof local knowledge.
In AMTA Workshop on Collabo-rative Crowdsourcing for Translation, Denver.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Comput.
Linguist., 29(1):19?51, March.Brendan O?Connor, Michel Krieger, and David Ahn.2010.
TweetMotif: Exploratory Search and TopicSummarization for Twitter.
In William W. Cohen,Samuel Gosling, William W. Cohen, and SamuelGosling, editors, ICWSM.
The AAAI Press.Kishore Papineni, Salim Roukos, Todd Ward, and Wei J.Zhu.
2002.
BLEU: a method for automatic evalua-tion of machine translation.
In Proceedings of 40thAnnual Meeting of the Association for ComputationalLinguistics, pages 311?318, Philadelphia, Pennsylva-nia, USA, July.
Association for Computational Lin-guistics.Majid Razmara, George Foster, Baskaran Sankaran, andAnoop Sarkar.
2012.
Mixing multiple translationmodels in statistical machine translation.
In Proceed-ings of the 50th ACL, Jeju, Republic of Korea, jul.Baskaran Sankaran, Majid Razmara, Atefeh Farzindar,Wael Khreich, Fred Popowich, and Anoop Sarkar.2012.
Domain adaptation techniques for machinetranslation and their evaluation in a real-world setting.In Proceedings of 25th Canadian Conference on Arti-ficial Intelligence, Toronto, Canada, may.Andreas Stolcke.
2002.
SRILM ?
an extensible languagemodeling toolkit.
In Proceedings of ICSLP, volume 2,pages 901?904, Denver, USA.Bing Zhao, Matthias Eck, and Stephan Vogel.
2004.Language model adaptation for statistical machinetranslation with structured query models.
In 20thCOLING.89
