Proceedings of the Third ACL Workshop on Innovative Use of NLP for Building Educational Applications, pages 107?115,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsDiagnosing meaning errors inshort answers to reading comprehension questionsStacey BaileyDepartment of LinguisticsThe Ohio State University1712 Neil AvenueColumbus, Ohio 43210, USAs.bailey@ling.osu.eduDetmar MeurersSeminar fu?r SprachwissenschaftUniversita?t Tu?bingenWilhelmstrasse 1972074 Tu?bingen, Germanydm@sfs.uni-tuebingen.deAbstractA common focus of systems in Intelli-gent Computer-Assisted Language Learning(ICALL) is to provide immediate feedback tolanguage learners working on exercises.
Mostof this research has focused on providing feed-back on the form of the learner input.
Foreignlanguage practice and second language acqui-sition research, on the other hand, emphasizesthe importance of exercises that require thelearner to manipulate meaning.The ability of an ICALL system to diag-nose and provide feedback on the mean-ing conveyed by a learner response dependson how well it can deal with the responsevariation allowed by an activity.
We focuson short-answer reading comprehension ques-tions which have a clearly defined target re-sponse but the learner may convey the mean-ing of the target in multiple ways.
As empiri-cal basis of our work, we collected an Englishas a Second Language (ESL) learner corpusof short-answer reading comprehension ques-tions, for which two graders provided targetanswers and correctness judgments.
On thisbasis, we developed a Content-AssessmentModule (CAM), which performs shallow se-mantic analysis to diagnose meaning errors.
Itreaches an accuracy of 88% for semantic errordetection and 87% on semantic error diagno-sis on a held-out test data set.1 IntroductionLanguage practice that includes meaningful interac-tion is a critical component of many current lan-guage teaching theories.
At the same time, exist-ing research on intelligent computer-aided languagelearning (ICALL) systems has focused primarily onproviding practice with grammatical forms.
Formost ICALL systems, although form assessment of-ten involves the use of natural language processing(NLP) techniques, the need for sophisticated con-tent assessment of a learner response is limited byrestricting the kinds of activities offered in order totightly control the variation allowed in learner re-sponses, i.e., only one or very few forms can be usedby the learner to express the correct content.
Yetmany of the activities that language instructors typ-ically use in real language-learning settings supporta significant degree of variation in correct answersand in turn require both form and content assess-ment for answer evaluation.
Thus, there is a realneed for ICALL systems that provide accurate con-tent assessment.While some meaningful activities are too unre-stricted for ICALL systems to provide effective con-tent assessment, where the line should be drawn ona spectrum of language exercises is an open ques-tion.
Different language-learning exercises carrydifferent expectations with respect to the level andtype of linguistic variation possible across learnerresponses.
In turn, these expectations may be linkedto the learning goals underlying the activity design,the cognitive skills required to respond to the ac-tivity, or other properties of the activity.
To de-velop adequate processing strategies for content as-sessment, it is important to understand the connec-tion between exercises and expected variation, asconceptualized by the exercise spectrum shown inFigure 1, because the level of variation imposes re-107Tightly Restricted Responses Loosely Restricted ResponsesDecontextualizedgrammar fill-in-the-blanksShort-answer readingcomprehensionquestionsEssays onindividualizedtopicsThe Middle GroundViable Processing GroundFigure 1: Language Learning Exercise Spectrumquirements and limitations on different processingstrategies.
At one extreme of the spectrum, there aretightly restricted exercises requiring minimal analy-sis in order to assess content.
At the other extremeare unrestricted exercises requiring extensive formand content analysis to assess content.
In this work,we focus on determining whether shallow content-analysis techniques can be used to perform contentassessment for activities in the space between theextremes.
A good test case in this middle groundare loosely restricted reading comprehension (RC)questions.
From a teaching perspective, they are atask that is common in real-life learning situations,they combine elements of comprehension and pro-duction, and they are a meaningful activity suitedto an ICALL setting.
From a processing perspec-tive, responses exhibit linguistic variation on lexical,morphological, syntactic and semantic levels ?
yetthe intended contents of the answer is predictable sothat an instructor can define target responses.Since variation is possible across learner re-sponses in activities in the middle ground of thespectrum, we propose a shallow content assessmentapproach which supports the comparison of targetand learner responses on several levels including to-ken, chunk and relation.
We present an architec-ture for a content assessment module (CAM) whichprovides this flexibility using multiple surface-basedmatching strategies and existing language process-ing tools.
For an empirical evaluation, we collecteda corpus of language learner data consisting exclu-sively of responses to short-answer reading compre-hension questions by intermediate English languagelearners.2 The DataThe learner corpus consists of 566 responses toshort-answer comprehension questions.
The re-sponses, written by intermediate ESL students aspart of their regular homework assignments, weretypically 1-3 sentences in length.
Students had ac-cess to their textbooks for all activities.
For devel-opment and testing, the corpus was divided into twosets.
The development set contains 311 responsesfrom 11 students answering 47 different questions;the test set contains 255 responses from 15 studentsto 28 questions.
The development and test sets werecollected in two different classes of the same inter-mediate reading/writing course.Two graders annotated the learner answers witha binary code for semantic correctness and one ofseveral diagnosis codes to be discussed below.
Tar-get responses (i.e., correct answers) and keywordsfrom the target responses were also identified bythe graders.1 Because we focus on content assess-ment, learner responses containing grammatical er-rors were only marked as incorrect if the grammat-ical errors impacted the understanding of the mean-ing.The graders did not agree on correctness judg-ments for 31 responses (12%) in the test set.
Thesewere eliminated from the test set in order to obtain agold standard for evaluation.The remaining responses in the development andtest sets showed a range of variation for many of theprompts.
As the following example from the corpusillustrates, even straightforward questions based on1Keywords refer to terms in the target response essential toa correct answer.108an explicit short reading passage yield both linguis-tic and content variation:CUE: What are the methods of propaganda men-tioned in the article?TARGET: The methods include use of labels, visualimages, and beautiful or famous people promotingthe idea or product.
Also used is linking the productto concepts that are admired or desired and to createthe impression that everyone supports the product oridea.LEARNER RESPONSES:?
A number of methods of propaganda are usedin the media.?
Bositive or negative labels.?
Giving positive or negative labels.
Using vi-sual images.
Having a beautiful or famous per-son to promote.
Creating the impression thateveryone supports the product or idea.While the third answer was judged to be correct,the syntactic structures, word order, forms, and lexi-cal items used (e.g., famous person vs. famous peo-ple) vary from the string provided as target.
Of thelearner responses in the corpus, only one was stringidentical with the teacher-provided target and ninewere identical when treated as bags-of-words.
In thetest set, none of the learner responses was string orbag-of-word identical with the corresponding targetsentence.To classify the variation exhibited in learner re-sponses, we developed an annotation scheme basedon target modification, with the meaning error la-bels being adapted from those identified by James(1998) for grammatical mistakes.
Target modifica-tion encodes how the learner response varies fromthe target, but makes the sometimes incorrect as-sumption that the learner is actually trying to ?hit?the meaning of the target.
The annotation schemedistinguishes correct answers, omissions (of rele-vant concepts), overinclusions (of incorrect con-cepts), blends (both omissions and overinclusions),and non-answers.
These error types are exempli-fied below with examples from the corpus.
In ad-dition, the graders used the label alternate answerfor responses that were correct given the questionand reading passage, but that differed significantlyin meaning from what was conveyed by the targetanswer.21.
Necessary concepts left out of learner response.CUE: Name the features that are used in thedesign of advertisements.TARGET: The features are eye contact, color,famous people, language and cultural refer-ences.RESPONSE: Eye contact, color2.
Response with extraneous, incorrect concepts.CUE: Which form of programming on TVshows that highest level of violence?TARGET: Cartoons show the most violent acts.RESPONSE: Television drama, children?s pro-grams and cartoons.3.
An incorrect blend/substitution (correct con-cept missing, incorrect one present).CUE: What is alliteration?TARGET: Alliteration is where sequentialwords begin with the same letter or sound.RESPONSE: The worlds are often chosen tomake some pattern or play on works.
Sequen-tial works begins with the same letter or sound.4.
Multiple incorrect concepts.CUE: What was the major moral questionraised by the Clinton incident?3TARGET: The moral question raised by theClinton incident was whether a politician?spersonal life is relevant to their job perfor-mance.RESPONSE: The scandal was about the rela-tionship between Clinton and Lewinsky.3 MethodThe CAM design integrates multiple matchingstrategies at different levels of representation andvarious abstractions from the surface form to com-pare meanings across a range of response varia-tions.
The approach is related to the methods used in2We use the term concept to refer to an entity or a relationbetween entities in a representation of the meaning of a sen-tence.
Thus, a response generally contains multiple concepts.3Note the incorrect presupposition in the cue provided bythe instructor.109machine translation evaluation (e.g., Banerjee andLavie, 2005; Lin and Och, 2004), paraphrase recog-nition (e.g., Brockett and Dolan, 2005; Hatzivas-siloglou et al, 1999), and automatic grading (e.g.,Leacock, 2004; Mar?
?n, 2004).To illustrate the general idea, consider the exam-ple from our corpus in Figure 2.Figure 2: Basic matching exampleWe find one string identical match between the tokenwas occurring in the target and the learner response.At the noun chunk level we can match home withhis house.
And finally, after pronoun resolution it ispossible to match Bob Hope with he.The overall architecture of CAM is shown in Fig-ure 3.
Generally speaking, CAM compares thelearner response to a stored target response and de-cides whether the two responses are possibly differ-ent realizations of the same semantic content.
Thedesign relies on a series of increasingly complexcomparison modules to ?align?
or match compatibleconcepts.
Aligned and unaligned concepts are usedto diagnose content errors.
The CAM design sup-ports the comparison of target and learner responseson token, chunk and relation levels.
At the tokenlevel, the nature of the comparison includes abstrac-tions of the string to its lemma (i.e., uninflected rootform of a word), semantic type (e.g., date, location),synonyms, and a more general notion of similaritysupporting comparison across part-of-speech.The system takes as input the learner response andone or more target responses, along with the ques-tion and the source reading passage.
The compari-son of the target and learner input pair proceeds firstwith an analysis filter, which determines whetherlinguistic analysis is required for diagnosis.
Essen-tially, this filter identifies learner responses that werecopied directly from the source text.Then, for any learner-target response pair thatrequires linguistic analysis, CAM assessment pro-ceeds in three phases ?
Annotation, Alignment andDiagnosis.
The Annotation phase uses NLP tools toenrich the learner and target responses, as well asthe question text, with linguistic information, suchas lemmas and part-of-speech tags.
The questiontext is used for pronoun resolution and to eliminateconcepts that are ?given?
(cf.
Halliday, 1967, p. 204and many others since).
Here ?given?
informationrefers to concepts from the question text that are re-used in the learner response.
They may be neces-sary for forming complete sentences, but contributeno new information.
For example, if the question isWhat is alliteration?
and the response is Allitera-tion is the repetition of initial letters or sounds, thenthe concept represented by the word alliteration isgiven and the rest is new.
For CAM, responses areneither penalized nor rewarded for containing giveninformation.Table 1 contains an overview of the annotationsand the resources, tools or algorithms used.
Thechoice of the particular algorithm or implementationwas primarily based on availability and performanceon our development corpus ?
other implementationscould generally be substituted without changing theoverall approach.Annotation Task Language Processing ToolSentence Detection, MontyLingua (Liu, 2004)Tokenization,LemmatizationLemmatization PC-KIMMO (Antworth, 1993)Spell Checking Edit distance (Levenshtein, 1966),SCOWL word list (Atkinson, 2004)Part-of-speech Tagging TreeTagger (Schmid, 1994)Noun Phrase Chunking CASS (Abney, 1997)Lexical Relations WordNet (Miller, 1995)Similarity Scores PMI-IR (Turney, 2001;Mihalcea et al, 2006)Dependency Relations Stanford Parser(Klein and Manning, 2003)Table 1: NLP Tools used in CAMAfter the Annotation phase, Alignment maps new(i.e., not given) concepts in the learner response toconcepts in the target response using the annotatedinformation.
The final Diagnosis phase analyzesthe alignment to determine whether the learner re-110Annotation Alignment DiagnosisPunctuationInputLearner ResponseTarget Response(s)QuestionOutputSource TextActivity ModelSettingsSentence DetectionTokenizationLemmatizationPOS TaggingChunkingDependency ParsingSpelling CorrectionSimilarity ScoringPronoun ResolutionType RecognitionAnalysis FilterGivennessPre-Alignment FiltersToken-levelAlignmentChunk-levelAlignmentRelation-levelAlignmentErrorReportingDetectionClassificationDiagnosisClassificationFigure 3: Architecture of the Content Assessment Module (CAM)sponse contains content errors.
If multiple target re-sponses are supplied, then each is compared to thelearner response and the target response with themost matches is selected as the model used in di-agnosis.
The output is a diagnosis of the input pair,which might be used in a number of ways to providefeedback to the learner.3.1 Combining the evidenceTo combine the evidence from these different lev-els of analysis for content evaluation and diagno-sis, we tried two methods.
In the first, we hand-wrote rules and set thresholds to maximize perfor-mance on the development set.
On the developmentset, the hand-tuned method resulted in an accuracyof 81% for the semantic error detection task, a bi-nary judgment task.
However, performance on thetest set (which was collected in a later quarter witha different instructor and different students) madeclear that the rules and thresholds thus obtained wereoverly specific to the development set, as accuracydropped down to 63% on the test set.
The hand-written rules apparently were not general enough totransfer well from the development set to the test set,i.e., they relied on properties of the development setthat where not shared across data sets.
Given the va-riety of features and the many different options forcombining and weighing them that might have beenexplored, we decided that rather than hand-tuningthe rules to additional data, we would try to machinelearn the best way of combining the evidence col-lected.
We thus decided to explore machine learn-ing, even though the set of development data fortraining clearly is very small.Machine learning has been used for equivalencerecognition in related fields.
For instance, Hatzivas-siloglou et al (1999) trained a classifier for para-phrase detection, though their performance onlyreached roughly 37% recall and 61% precision.
Ina different approach, Finch et al (2005) found thatMT evaluation techniques combined with machinelearning improves equivalence recognition.
Theyused the output of several MT evaluation approachesbased on matching concepts (e.g., BLEU) as fea-tures/values for training a support vector machine(SVM) classifier.
Matched concepts and unmatched111concepts alike were used as features for training theclassifier.
Tested against the Microsoft ResearchParaphrase (MSRP) Corpus, the SVM classifier ob-tained 75% accuracy on identifying paraphrases.But it does not appear that machine learning tech-niques have so far been applied to or even discussedin the context of language learner corpora, where theavailable data sets typically are very small.To begin to address the application of machinelearning to meaning error diagnosis, the alignmentdata computed by CAM was converted into featuressuitable for machine learning.
For example, the firstfeature calculated is the relative overlap of alignedkeywords from the target response.
The full list offeatures are listed in Table 2.Features Description1.
Keyword Overlap Percent of keywords aligned(relative to target)2.
Target Overlap Percent of aligned target tokens3.
Learner Overlap Percent of aligned learner tokens4.
T-Chunk Percent of aligned target chunks5.
L-Chunk Percent of aligned learner chunks6.
T-Triple Percent of aligned target triples7.
L-Triple Percent of aligned learner triples8.
Token Match Percent of token alignmentsthat were token-identical9.
Similarity Match Percent of token alignmentsthat were similarity-resolved10.
Type Match Percent of token alignmentsthat were type-resolved11.
Lemma Match Percent of token alignmentsthat were lemma-resolved12.
Synonym Match Percent of token alignmentsthat were synonym-resolved13.
Variety of Match Number of kinds of token-level(0-5) alignmentsTable 2: Features used for Machine LearningFeatures 1-7 reflect relative numbers of matches (rel-ative to length of either the target or learner re-sponse).
Features 2, 4, and 6 are related to the targetresponse overlap.
Features 3, 5, and 7 are related tooverlap in the learner response.
Features 8?13 re-flect the nature of the matches.The values for the 13 features in Table 2 were usedto train the detection classifier.
For diagnosis, a four-teenth feature ?
a detection feature (1 or 0 dependingon whether the detection classifier detected an error)?
was added to the development data to train the di-agnosis classifier.
Given that token-level alignmentsare used in identifying chunk- and triple-level align-ments, that kinds of alignments are related to varietyof matches, etc., there is clear redundancy and inter-dependence among features.
But each feature addssome new information to the overall diagnosis pic-ture.The machine learning suite used in all the devel-opment and testing runs is TiMBL (Daelemans et al,2007).
As with the NLP tools used, TiMBLwas cho-sen mainly to illustrate the approach.
It was not eval-uated against several learning algorithms to deter-mine the best performing algorithm for the task, al-though this is certainly an avenue for future research.In fact, TiMBL itself offers several algorithms andoptions for training and testing.
Experiments withthese options on the development set included vary-ing how similarity between instances was measured,how importance (i.e., weight) was assigned to fea-tures and how many neighbors (i.e., instances) wereexamined in classifying new instances.
Given thevery small development set available, making em-pirical tuning on the development set difficult, wedecided to use the default learning algorithm (k-nearest neighbor) and majority voting based on thetop-performing training runs for each available dis-tance measure.4 ResultsTurning to the results obtained by the machine-learning based CAM, for the binary semantic errordetection task, the system obtains an overall 87% ac-curacy on the development set (using the leave-one-out option of TiMBL to avoid training on the testitem).
Interestingly, even for this small developmentset, machine learning thus outperforms the accuracyobtained for the manual method of combining theevidence reported above.
On the test set, the finalTiMBL-based CAM performance for detection im-proved slightly to 88% accuracy.
These results sug-gest that detection using the CAM design is viable,though more extensive testing with a larger corpusis needed.Balanced sets Both the development and test setscontained a high proportion of correct answers ?71% of the development set and 84% of the test setwere marked as correct by the human graders.
Thus,112we also sampled a balanced set consisting of 50%correct and 50% incorrect answers by randomly in-cluding correct answers plus all the incorrect an-swers to obtain a set with 152 cases (developmentsubset) and 72 (test subset) sentences.
The accuracyobtained for this balanced set was 78% (leave-one-out-testing with development set) and 67% (test set).The fact that the results for the balanced develop-ment set using leave-one-out-testing are comparableto the general results shows that the machine learnerwas not biased towards the ratio of correct and in-correct responses, even though there is a clear dropfrom development to test set, possibly related to thesmall size of the data sets available for training andtesting.Alternate answers Another interesting aspect todiscuss is the treatment of alternate answers.
Recallthat alternate answers are those learner responsesthat are correct but significantly dissimilar from thegiven target.
Of the development set response pairs,15 were labeled as alternate answers.
One wouldexpect that given that these responses violate the as-sumption that the learner is trying to hit the giventarget, using these items in training would negativelyeffect the results.
This turns out to be the case; per-formance on the training set drops slightly when thealternate answer pairs are included.
We thus did notinclude them in the development set used for train-ing the classifier.
In other words, the diagnosis clas-sifier was trained to label the data with one of fivecodes ?
correct, omissions (of relevant concepts),overinclusions (of incorrect concepts), blends (bothomissions and overinclusions), and non-answers.Because it cannot be determined beforehand whichitems in unseen data are alternate answer pairs, thesepairs were not removed from the test set in the finalevaluation.
Were these items eliminated, the detec-tion performance would improve slightly to 89%.Form errors Interestingly, the form errors fre-quently occurring in the student utterances did notnegatively impact the CAM results.
On average, alearner response in the test set contained 2.7 formerrors.
Yet, 68% of correctly diagnosed sentencesincluded at least one form error, but only 53% ofincorrectly diagnosed ones did so.
In other words,correct responses had more form errors than incor-rect responses.
Looking at numbers and combina-tions of form errors, no clear pattern emerges thatwould suggest that form errors are linked to mean-ing errors in a clear way.
One conclusion to drawbased on these data is that form and content assess-ment can be treated as distinct in the evaluation oflearner responses.
Even in the presence of a rangeof form-based errors, human graders can clearly ex-tract the intended meaning to be able to evaluate se-mantic correctness.
The CAM approach is similarlyable to provide meaning evaluation in the presenceof grammatical errors.Diagnosis For diagnosis with five codes, CAMobtained overall 87% accuracy both on the devel-opment and on the test set.
Given that the number oflabels increases from 2 to 5, the slight drop in overallperformance in diagnosis as compared to the detec-tion of semantic errors (from 88% to 87%) is bothunsurprising in the decline and encouraging in thesmallness of the decline.
However, given the samplesize and few numbers of instances of any given errorin the test (and development) set, additional quanti-tative analysis of the diagnosis results would not beparticularly meaningful.5 Related WorkThe need for semantic error diagnosis in previousCALL work has been limited by the narrow rangeof acceptable response variation in the supportedlanguage activity types.
The few ICALL systemsthat have been successfully integrated into real-lifelanguage teaching, such as German Tutor (Heift,2001) and BANZAI (Nagata, 2002), also tightlycontrol expected response variation through delib-erate exercise type choices that limit acceptable re-sponses.
Content assessment in the German Tutoris performed by string matching against the storedtargets.
Because of the tightly controlled exercisetypes and lack of variation in the expected input,the assumption that any variation in a learner re-sponse is due to form error, rather than legitimatevariation, is a reasonable one.
The recently de-veloped TAGARELA system for learners of Por-tuguese (Amaral and Meurers, 2006; Amaral, 2007)lifts some of the restrictions on exercise types, whilerelying on shallow semantic processing.
Usingstrategies inspired by our work, TAGARELA in-corporates simple content assessment for evaluating113learner responses in short-answer questions.ICALL system designs that do incorporate moresophisticated content assessment include FreeText(L?Haire and Faltin, 2003), the Military LanguageTutor (MILT) Program (Kaplan et al, 1998), andHerr Kommissar (DeSmedt, 1995).
These systemsrestrict both the exercise types and domains to makecontent assessment feasible using deeper semanticprocessing strategies.Beyond the ICALL domain, work in automaticgrading of short answers and essays has addressedwhether the students answers convey the correctmeaning, but these systems focus on largely scor-ing rather than diagnosis (e.g., E-rater, Bursteinand Chodorow, 1999), do not specifically addresslanguage learning contexts and/or are designed towork specifically with longer texts (e.g., AutoTu-tor, Wiemer-Hastings et al, 1999).
Thus, the extentto which ICALL systems can diagnose meaning er-rors in language learner responses has been far fromclear.As far as we are aware, no directly comparablesystems performing content-assessment on relatedlanguage learner data exist.
The closest related sys-tem that does a similar kind of detection is the C-rater system (Leacock, 2004).
That system obtains85% accuracy.
However, the test set and scoring sys-tem were different, and the system was applied toresponses from native English speakers.
In addition,their work focused on detection of errors rather thandiagnosis.
So, the results are not directly compara-ble.
Nevertheless, the CAM detection results clearlyare competitive.6 SummaryAfter motivating the need for content assessment inICALL, in this paper we have discussed an approachfor content assessment of English language learnerresponses to short answer reading comprehensionquestions, which is worked out in detail in Bailey(2008).
We discussed an architecture which relies onshallow processing strategies and achieves an accu-racy approaching 90% for content error detection ona learner corpus we collected from learners complet-ing the exercises assigned in a real-life ESL class.Even for the small data sets available in the area oflanguage learning, it turns out that machine learn-ing can be effective for combining the evidence fromvarious shallow matching features.
The good perfor-mance confirms the viability of using shallow NLPtechniques for meaning error detection.
By devel-oping and testing this model, we hope to contributeto bridging the gap between what is practical andfeasible from a processing perspective and what isdesirable from the perspective of current theories oflanguage instruction.ReferencesSteven Abney, 1997.
Partial Parsing via Finite-State Cas-cades.
Natural Language Engineering, 2(4):337?344.http://vinartus.net/spa/97a.pdf.Luiz Amaral, 2007.
Designing Intelligent Language Tu-toring Systems: Integrating Natural Language Pro-cessing Technology into Foreign Language Teaching.Ph.D.
thesis, The Ohio State University.Luiz Amaral and Detmar Meurers, 2006.
Wheredoes ICALL Fit into Foreign Language Teaching?Presentation at the 23rd Annual Conference of theComputer Assisted Language Instruction Consortium(CALICO), May 19, 2006.
University of Hawaii.http://purl.org/net/icall/handouts/calico06-amaral-meurers.pdf.Evan L. Antworth, 1993.
Glossing Text with the PC-KIMMO Morphological Parser.
Computers and theHumanities, 26:475?484.Kevin Atkinson, 2004.
Spell Checking OrientedWord Lists (SCOWL).
http://wordlist.sourceforge.net/.Stacey Bailey, 2008.
Content Assessment in IntelligentComputer-Aided Language Learning: Meaning ErrorDiagnosis for English as a Second Language.
Ph.D.thesis, The Ohio State University.Satanjeev Banerjee and Alon Lavie, 2005.
METEOR:An Automatic Metric for MT Evaluation with Im-proved Correlation with Human Judgments.
In Pro-ceedings of Workshop on Intrinsic and Extrinsic Eval-uation Measures for Machine Translation and/or Sum-marization at the 43th Annual Meeting of the Associ-ation of Computational Linguistics (ACL-2005).
AnnArbor, Michigan, pp.
65?72.
http://aclweb.org/anthology/W05-0909.Chris Brockett andWilliam B. Dolan, 2005.
Support Vec-tor Machines for Paraphrase Identification and Cor-pus Construction.
In Proceedings of the Third In-ternational Workshop on Paraphrasing (IWP2005).pp.
1?8.
http://aclweb.org/anthology/I05-5001.Jill Burstein and Martin Chodorow, 1999.
AutomatedEssay Scoring for Nonnative English Speakers.
InProceedings of a Workshop on Computer-MediatedLanguage Assessment and Evaluation of Natural Lan-guage Processing, Joint Symposium of the Asso-ciation of Computational Linguistics (ACL-99) andthe International Association of Language LearningTechnologies.
pp.
68?75.
http://aclweb.org/anthology/W99-0411.114Walter Daelemans, Jakub Zavrel, Kovan der Sloot andAntal van den Bosch, 2007.
TiMBL: Tilburg Memory-Based Learner Reference Guide, ILK Technical Re-port ILK 07-03.
Induction of Linguistic KnowledgeResearch Group Department of Communication andInformation Sciences, Tilburg University, P.O.
Box90153, NL-5000 LE, Tilburg, The Netherlands, ver-sion 6.0 edition.William DeSmedt, 1995.
Herr Kommissar: An ICALLConversation Simulator for Intermediate German.
InV.
Melissa Holland, Jonathan Kaplan and MichelleSams (eds.
), Intelligent Language Tutors: TheoryShaping Technology, Lawrence Erlbaum Associates,pp.
153?174.Andrew Finch, Young-Sook Hwang and Eiichiro Sumita,2005.
Using Machine Translation Evaluation Tech-niques to Determine Sentence-level Semantic Equiva-lence.
In Proceedings of the Third International Work-shop on Paraphrasing (IWP2005).
pp.
17?24.
http://aclweb.org/anthology/I05-5003.Michael Halliday, 1967.
Notes on Transitivity and Themein English.
Part 1 and 2.
Journal of Linguistics, 3:37?81, 199?244.Vasileios Hatzivassiloglou, Judith Klavans and EleazarEskin, 1999.
Detecting Text Similarity over Short Pas-sages: Exploring Linguistic Feature Combinations viaMachine Learning.
In Proceedings of Empirical Meth-ods in Natural Language Processing and Very LargeCorpora (EMNLP?99).
College Park, Maryland, pp.203?212.
http://aclweb.org/anthology/W99-0625.Trude Heift, 2001.
Intelligent Language TutoringSystems for Grammar Practice.
Zeitschrift fu?r In-terkulturellen Fremdsprachenunterricht, 6(2).
http://www.spz.tu-darmstadt.de/projekt_ejournal/jg-06-2/beitrag/heift2.htm.Carl James, 1998.
Errors in Language Learning and Use:Exploring Error Analysis.
Longman Publishers.Jonathan Kaplan, Mark Sobol, Robert Wisher and RobertSeidel, 1998.
The Military Language Tutor (MILT)Program: An Advanced Authoring System.
ComputerAssisted Language Learning, 11(3):265?287.Dan Klein and Christopher D. Manning, 2003.
Accu-rate Unlexicalized Parsing.
In Proceedings of the 41stMeeting of the Association for Computational Linguis-tics (ACL 2003).
Sapporo, Japan, pp.
423?430.
http://aclweb.org/anthology/P03-1054.Claudia Leacock, 2004.
Scoring Free-Responses Auto-matically: A Case Study of a Large-Scale Assessment.Examens, 1(3).Vladimir I. Levenshtein, 1966.
Binary Codes Capable ofCorrecting Deletions, Insertions, and Reversals.
SovietPhysics Doklady, 10(8):707?710.Se?bastien L?Haire and Anne Vandeventer Faltin, 2003.Error Diagnosis in the FreeText Project.
CALICOJournal, 20(3):481?495.Chin-Yew Lin and Franz Josef Och, 2004.
Auto-matic Evaluation of Machine Translation Quality Us-ing Longest Common Subsequence and Skip-BigramStatistics.
In Proceedings of the 42nd Annual Meet-ing of the Association for Computational Linguistics(ACL-04).
pp.
605?612.
http://aclweb.org/anthology/P04-1077.Hugo Liu, 2004.
MontyLingua: An End-to-End Natural Language Processor with CommonSense.
http://web.media.mit.edu/?hugo/montylingua, accessed October 30, 2006.Diana Rosario Pe?rez Mar?
?n, 2004.
Automatic Evaluationof Users?
Short Essays by Using Statistical and Shal-low Natural Language Processing Techniques.
Mas-ter?s thesis, Universidad Auto?noma deMadrid.
http://www.ii.uam.es/?dperez/tea.pdf.Rada Mihalcea, Courtney Corley and Carlo Strapparava,2006.
Corpus-based and Knowledge-based Measuresof Text Semantic Similarity.
In Proceedings of the Na-tional Conference on Artificial Intelligence.
AmericanAssociation for Artificial Intelligence (AAAI) Press,Menlo Park, CA, volume 21(1), pp.
775?780.George Miller, 1995.
WordNet: A Lexical Database forEnglish.
Communications of the ACM, 38(11):39?41.Noriko Nagata, 2002.
BANZAI: An Application of Nat-ural Language Processing to Web-Based LanguageLearning.
CALICO Journal, 19(3):583?599.Helmut Schmid, 1994.
Probabilistic Part-of-Speech Tag-ging Using Decision Trees.
In International Con-ference on New Methods in Language Processing.Manchester, United Kingdom, pp.
44?49.Peter Turney, 2001.
Mining theWeb for Synonyms: PMI-IR Versus LSA on TOEFL.
In Proceedings of theTwelfth European Conference on Machine Learning(ECML-2001).
Freiburg, Germany, pp.
491?502.Peter Wiemer-Hastings, Katja Wiemer-Hastings andArthur Graesser, 1999.
Improving an Intelligent Tu-tor?s Comprehension of Students with Latent Seman-tic Analysis.
In Susanne Lajoie and Martial Vivet(eds.
), Artificial Intelligence in Education, IOS Press,pp.
535?542.115
