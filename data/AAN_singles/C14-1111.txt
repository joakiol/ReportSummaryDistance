Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 1177?1185, Dublin, Ireland, August 23-29 2014.Morfessor FlatCat: An HMM-Based Method for Unsupervised andSemi-Supervised Learning of MorphologyStig-Arne Gr?onroos1stig-arne.gronroos@aalto.fiSami Virpioja2sami.virpioja@aalto.fiPeter Smit1peter.smit@aalto.fiMikko Kurimo1mikko.kurimo@aalto.fi1Department of Signal Processing and Acoustics, Aalto University2Department of Information and Computer Science, Aalto UniversityAbstractMorfessor is a family of methods for learning morphological segmentations of words basedon unannotated data.
We introduce a new variant of Morfessor, FlatCat, that applies a hid-den Markov model structure.
It builds on previous work on Morfessor, sharing model compo-nents with the popular Morfessor Baseline and Categories-MAP variants.
Our experiments showthat while unsupervised FlatCat does not reach the accuracy of Categories-MAP, with semi-supervised learning it provides state-of-the-art results in the Morpho Challenge 2010 tasks forEnglish, Finnish, and Turkish.1 IntroductionMorphological analysis is essential for automatic processing of compounding and highly-inflecting lan-guages, for which the number of unique word forms may be very large.
Apart from rule-based analyzers,the task has been approached by machine learning methodology.
Especially unsupervised methods thatrequire no linguistic resources have been studied widely (Hammarstr?om and Borin, 2011).
Typicallythese methods focus on morphological segmentation, i.e., finding morphs, the surface forms of the mor-phemes.For language processing applications, unsupervised learning of morphology can provide decent-quality analyses without resources produced by human experts.
However, while morphological ana-lyzers and large annotated corpora may be expensive to obtain, a small amount of linguistic expertise ismore easily available.
A well-informed native speaker of a language can often identify the different pre-fixes, stems, and suffixes of words.
Then the question is how many annotated words makes a difference.One answer was provided by Kohonen et al.
(2010), who showed that already one hundred manuallysegmented words provide significant improvements to the quality of the output when comparing to alinguistic gold standard.The semi-supervised approach by Kohonen et al.
(2010) was based on Morfessor Baseline, the sim-plest of the Morfessor methods by Creutz and Lagus (2002; 2007).
The statistical model of MorfessorBaseline is simply a categorical distribution of morphs?a unigram model in the terms of statistical lan-guage modeling.
As the semi-supervised Morfessor Baseline outperformed all unsupervised and semi-supervised methods evaluated in the Morpho Challenge competitions (Kurimo et al., 2010a) so far, thenext question is how the approach works for more complex models.Another popular variant of Morfessor, Categories-MAP (CatMAP) (Creutz and Lagus, 2005), modelsword formation using a hidden Markov model (HMM).
The context-sensitivity of the model improvesthe precision of the segmentation.
For example, it can prevent splitting a single s, a common Englishsuffix, from the beginning of a word.
Moreover, it can disambiguate between identical morphs that areactually surface forms of different morphemes.
Finally, separation of stems and affixes in the outputmakes it simple to use the method as a stemmer.In contrast to Morfessor Baseline, the lexicon of CatMAP is hierarchical: a morph that is already inthe lexicon may be used to encode the forms of other morphs.
This has both advantages and drawbacks.This work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/1177One downside is that it mixes the prior and likelihood components of the cost function, so that the semi-supervised approach presented by Kohonen et al.
(2010) is not usable.1.1 Hierarchical versus flat lexiconsFrom the viewpoint of data compression and following the two-part Minimum Description Length prin-ciple (Rissanen, 1978), Morfessor tries to minimize the number of bits needed to encode both the modelparameters and the training data.
Equivalently, the cost function L can be derived from the Maximum aPosteriori (MAP) estimate:??
= argmax?P(?
|D) = argmin?(?
log P(?)?
log P(D | ?
))= argmin?L(?,D), (1)where ?
are the model parameters, D is the training corpus, P(?)
is the prior of the parameters andP(D | ?)
is the data likelihood.In context-independent models such as Morfessor Baseline, the parameters include only the forms andprobabilities of the morphs in the lexicon of the model.
Morfessor Baseline and Categories-ML (CatML)(Creutz and Lagus, 2004) use a flat lexicon, in which the forms of the morphs are encoded directly asstrings: each letter requires a certain number of bits to encode.
Thus longer morphs are more expensive.Encoding a long morph is worthwhile only if the morph is referred to frequently enough from the wordsin the training data.
If a certain string, let us say segmentation, is common enough in the training data, itis cost-effective to have it as a whole in the lexicon.
Splitting it into two items, segment and ation, woulddouble the number of pointers from the data, even if those morphs were already in the lexicon.
Theundersegmentation of frequent words becomes evident especially if the training data is a corpus insteadof a list of unique word forms.In contrast, Morfessor CatMAP applies a hierarchical lexicon, which makes use of the morphs thatare already in the lexicon.
Instead of encoding the form of segmentation by its 12 letters, we could justencode the form with two references to the forms of the morphs segment and ation.
This may also causeerrors, for example encoding station with st and ation.The lexicon of Morfessor CatMAP allows but does not force hierarchical encoding for the forms:each morph has an extra parameter that indicates whether it has a hierarchical representation or not.
Theproblem of oversegmentation, as in st + ation, is solved using the morph categories.
The categories,which are states of the HMM, include stem, prefix, suffix, and a special non-morpheme category.
Thenon-morpheme category is intended to catch segments that do not fit well into the three proper morphcategories because they are fragments of a larger morph.
In our example, the morph st cannot be a suffixas it starts the word, it is unlikely to be a prefix as it directly precedes a common suffix ation, and it isunlikely to be a stem as it is very short.
Thus the algorithm is likely to use the non-morpheme state.
Thehierarchy is expanded only up to the level in which there are no non-morphemes, so the final analysis isstill station.
Without the hierarchy, the non-morphemes have to be removed heuristically, as in CatML(Creutz and Lagus, 2004).A hierarchical lexicon presents some challenges to model training.
For a standard unigram or HMMmodel, if you know the state and emission sequence of the training data, you can directly derive themaximum likelihood (ML) parameters of the model: a probability of a morph is proportional to thenumber of times it is referred to, conditional on the state in the HMM.
But if the lexicon is partlyhierarchical, also the references within the lexicon add to the reference counts, and there is no direct wayto find the ML parameters even if the encoding of the training data is known.
Similarly, semi-supervisedlearning cannot be accomplished simply by adding the counts from an annotated data set, as it is notclear when to use hierarchy instead of segmenting a word directly in the data.Moreover, for a flat lexicon, the cost function divides into two parts that have opposing optima: thecost of the data (likelihood) is optimal when there is minimal splitting and the lexicon consists of thewords in the training data, whereas the cost of the model (prior) is optimal when the lexicon is minimaland consists only of the letters.
In consequence, the balance of precision and recall of the segmentationboundaries can be directly controlled by setting a weight for the data likelihood.
Tuning this hyper-parameter is a very simple form of supervision, but it has drastic effects on the segmentation results1178(Kohonen et al., 2010).
A direct control of the balance may also be useful for some applications: Virpiojaet al.
(2011) found that the performance of the segmentation algorithms in machine translation correlatesmore with the precision than the recall.
The weighting approach does not work for hierarchical lexicons,for which changing the weight does not directly affect the decision whether to encode the morph withhierarchy or not.1.2 Morfessor FlatCatIn this paper, we introduce a new member to the Morfessor family, Morfessor FlatCat.
As indicated by itsname, FlatCat uses a flat lexicon.
Our hypothesis is that enabling semi-supervised learning is effective incompensating for the undersegmentation caused by the lack of hierarchy.
In particular, semi-supervisedlearning can improve modeling of suffixation.
In the examined languages, suffixes tend to serve syntacticpurposes, such as marking case, tense, person or number.
Examples are the suffix s marking tense andperson in she writes and number in stations.
Thus the suffix class is closed and has only a small numberof morphemes compared to the prefix and stem categories.
As a consequence, a large coverage of suffixescan be achieved already with a relatively small annotated data set.The basic model of morphotactics in FlatCat is the same as in the CatML and CatMAP variants: ahidden Markov model with states that correspond to a word boundary and four morph categories: stem,prefix, suffix, and non-morpheme.
As in CatML, we apply heuristics for removal of non-morphemesfrom the final segmentation.
However, because FlatCat uses MAP estimation of the parameters, theseheuristics are not necessary during the training for controlling the model complexity, but merely used asa post-processing step to get meaningful categories.Modeling of morphotactics improves the segmentation of compound words, by allowing the overalllevel of segmentation to be increased without increasing the number of correct morphs used in incorrectpositions.
As the benefits of semi-supervised learning and improved morphotactics are likely to com-plement each other, we can expect improved performance over the semi-supervised Morfessor Baselinemethod.
By experimental comparison to the previous Morfessor variants, we are able to shed more lighton the effects of using an HMM versus unigram model for morphotactics, using a hierarchical versus flatlexicon, and exploiting small amounts of annotated training data.2 FlatCat model and algorithmsMorfessor FlatCat uses components from the older Morfessor variants.
Instead of going through all thedetails, we refer to the previous work and highlight only the differences.
Common components betweenMorfessor methods are summarized in Table 1.As a generative model, Morfessor FlatCat describes the joint distribution P(A,W | ?)
of words andtheir analyses.
The wordsW are observed, but their analyses, A, is a latent variable in the model.
Ananalysis of a word contains its morphs and morph categories: prefix, stem, suffix, and non-morpheme.As marginalizing over all possible analyses is generally infeasible, point estimates are used during thetraining.
The likelihood conditioned on the current analyses isP(D |A, ?)
=|D|?j=1P(Aj| ?).
(2)If miare the morphs in Aj, ciare the hidden states of the HMM corresponding to the categories of themorphs, and # is the word boundary, P(Aj| ?)
isP(c1|#)|Aj|?i=1[P(mi| ci) P(ci+1| ci)]P(# | c|Aj|).
(3)Morfessor FlatCat applies an MDL-derived prior designed to control the number of non-zero param-eters.
The prior is otherwise the same as in Morfessor Baseline, but it includes the usage propertiesfrom Morfessor CatMAP: the length of the morph and its right and left perplexity.
The perplexity mea-sures describe the predictability of the contexts in which the morph occurs.
The emission probability of1179Morfessor methodComponent Baseline CatMAP CatML FlatCatLexicon type Flat Hierarchy Flat FlatMorphotactics Unigram HMM HMM HMMEstimation MAP MAP ML MAPSemi-supervised Implemented Not implemented Not implemented ImplementedTable 1: Overview of similarities and differences between Morfessor methods.a morph conditioned on the morph category, P(m | c), is calculated from the properties of the morphssimilarly as in CatMAP.2.1 Training algorithmsThe parameters are optimized using a local search.
Only a part of the parameters are optimized in eachstep: the parameters that are used in calculating the likelihood of a certain part, unit, of the corpus.
Unitsvary in complexity, from all occurrences of a certain morph to the occurrences of a morph bigram whosecontext fits to certain criteria.The algorithm tries to simultaneously find the optimal segmentation for the unit and the optimal pa-rameters consistent with that segmentation:(A, ?)
= argminOP(A,?){L(?,A,D)}.
(4)The training operators OP define the units changed by the local search and the alternative segmentationstried for each unit.
There are three training operators: split, join and resegment, analogous to the similarlynamed stages in CatMAP.The split operator is applied first.
It targets all occurrences of a specific morph in the corpus simultane-ously, attempting to split it into two parts.
The whole corpus is processed by sorting the current morphsby length from shortest to longest.The second operator attempts to join morph bigrams, grouped by the position of the bigram in theword.
The position grouped bigram counts are sorted by frequency, from most to least common.Finally, resegmenting uses the generalized Viterbi algorithm to find the currently optimal segmentationfor one whole word at a time.
This operator targets each corpus word in increasing order of frequency.The heuristics used in FlatCat to remove non-morphemes from the final segmentation are the fol-lowing: All consequent non-morphemes are joined together.
If the resulting morph is longer than 4characters, it is accepted as a stem.
All non-morphemes preceded by a suffix and followed by only suf-fixes or other non-morphemes are recategorized as suffixes without joining with their neighbors.
If anyshort non-morphemes remain, they are joined either to the preceding or following morphs (the latter onlyfor those in the initial position).2.2 Semi-supervised learningKohonen et al.
(2010) found that semi-supervised learning of Morfessor models was not effective byonly fixing the values of the analysis A for the annotated samplesDA.
Their solution was to introducecorpus likelihood weights ?
and ?, one for the unannotated data set and one for the annotated data set.Thus, instead of optimizing the MAP estimate, Kohonen et al.
(2010) minimize the costL(?,A,D,DA) = ?
log P(?)?
?
log P(D |A, ?)?
?
log P(DA|A, ?).
(5)The weights can be tuned on a development set.
We use the same scheme for FlatCat.The likelihood of the annotated data is calculated using the same HMM that is used for the unannotateddata.
The morph properties are estimated only from the unannotated data.
To ensure that the morphsrequired for the annotated data can be emitted, a copy of each word in the annotations is added to the1180(a) English.Method ?
?
Pre Rec FU Baseline 1.0 ?
.88 .59 .71U CatMAP ?
?
.89 .51 .65U FlatCat 1.0 ?
.90 .57 .69W Baseline 0.7 ?
.83 .62 .71W FlatCat 0.5 ?
.84 .60 .70SS Baseline 1.0 3000 .83 .77 .80SS FlatCat 0.9 2000 .86 .76 .81SS CRF+FlatCat 0.9 2000 .87 .77 .82S CRF ?
?
.92 .73 .81(b) Finnish.Method ?
?
Pre Rec FU Baseline 1.0 ?
.84 .38 .53U CatMAP ?
?
.76 .51 .61U FlatCat 1.0 ?
.84 .38 .52W Baseline .02 ?
.62 .54 .58W FlatCat .015 ?
.66 .52 .58SS Baseline .1 15000 .75 .72 .73SS FlatCat .2 1500 .79 .71 .75SS CRF+FlatCat .2 2500 .82 .76 .79S CRF ?
?
.88 .74 .80Table 2: Boundary Precision and Recall results in comparison to gold standard segmentation.
Abbrevi-ations have been used for Unsupervised (U), likelihood weighted (W), semi-supervised (SS) and fullysupervised (S) methods.
Best results for each measure have been hilighted using boldface.unannotated data.
This unannotated copy is loosely linked to the annotated word: operations that wouldresult in the removal of a morph required for the annotations from the lexicon cannot be selected, as suchan operation would have infinite cost.3 ExperimentsWe compare Morfessor FlatCat1to two previous Morfessor methods and a fully supervised discrimi-native segmentation method.
The Morfessor methods used as references are the CatMAP2and Base-line3implementations by Creutz and Lagus (2005) and Virpioja et al.
(2013), respectively.
Virpioja etal.
(2013) implements the semi-supervised method described by Kohonen et al.
(2010).
For a super-vised discriminative model, we use a character-level conditional random field (CRF) implementation byRuokolainen et al.
(2013)4.We use the English, Finnish and Turkish data sets from Morpho Challenge 2010 (Kurimo et al.,2010b).
They include large unannotated word lists, one thousand annotated words for training, 700?800 annotated words for parameter tuning, and 10?
1000 annotated words for testing.For evalution, we use the BPR score by Virpioja et al.
(2011).
The score calculates the precision (Pre),recall (Rec), and F1-score (F) of the predicted morph boundaries compared to a linguistic gold standard.In the presence of alternative gold standard analyses, we weight each alternative equally.We also report the mean average precision from the English and Finnish information retrieval (IR)tasks of the Morpho Challenge.
The Lemur Toolkit (Ogilvie and Callan, 2001) with Okapi BM25 rank-ing was used.
The Finnish data consists of 55K documents, 50 test queries and 23K binary relevanceassessments.
The English data consists of 170K documents, 50 test queries and 20K binary relevance as-sessments.
The domain of both data sets is short newspaper articles.
All word forms in both the corporaand the queries were replaced by the morphological segmentation to be evaluated.Morfessor FlatCat is a pipeline method that refines an initial segmentation given as input.
We try twodifferent initializations for the semi-supervised setting: initializing with the segmentation produced bysemi-supervised Morfessor Baseline, and initializing with the CRF segmentation.
All unsupervised andlikelihood-weighted results are initialized with the corresponding Baseline output.All methods were trained using word types.
The weight and perplexity threshold parameters wereoptimized separately for each method, using a grid search with the held-out data set.
The supervisedCRF method was trained using the one thousand word annotated training data set.1Available at https://github.com/aalto-speech/flatcat2Available at http://www.cis.hut.fi/projects/morpho/morfessorcatmap.shtml3Available at https://github.com/aalto-speech/morfessor4Available at http://users.ics.aalto.fi/tpruokol/1181Method ?
?
Pre Rec FU Baseline 1.0 ?
.85 .36 .51U CatMAP ?
?
.83 .50 .62U FlatCat 1.0 ?
.87 .36 .51W Baseline 0.1 ?
.71 .41 .52W FlatCat 0.3 ?
.88 .38 .53SS Baseline 0.4 2000 .86 .60 .71SS FlatCat 0.8 2666 .87 .59 .70SS CRF+FlatCat 1.0 3000 .87 .61 .72S CRF ?
?
.89 .58 .70Table 3: Boundary Precision and Recall results in comparison to gold standard segmentation for Turkish.Abbreviations have been used for Unsupervised (U), likelihood weighted (W), semi-supervised (SS) andfully supervised (S) methods.
Best results for each measure have been hilighted using boldface.3.1 Comparison to linguistic gold standardsThe results of the BPR evaluations are shown in Tables 2 (English, Finnish) and 3 (Turkish).
Semi-supervised FlatCat initialized using CRF achieves the highest F-score for both the English and Turkishdata sets.
The difference between the highest and second-highest scoring methods is statistically signifi-cant for Finnish and Turkish, but not for English (Wilcoxon signed-rank test, p < 0.01).Table 4 shows BPR for subsets of words consisting of different morph category patterns.
Each subsetconsists of 500 words from the English or Finnish gold standard, with one of five selected morph patternsas the only valid analysis.
The subsets consist of words with the following morph patterns: words thatshould not be segmented (STM), compound words consisting of exactly two stems (STM + STM), aprefix followed by a stem (PRE + STM), a stem followed by a single suffix (STM + SUF) and a stemand exactly two suffixes (STM + SUF + SUF).
For the STM pattern only precision is reported, as recallis not defined for an empty set of true boundaries.The fact that semi-supervised FlatCat compares well against CatMAP in recall, for all morph patternsand for the test set as a whole, indicates that supervision indeed is effective in compensating for theundersegmentation caused by the lack of hierarchy in the lexicon.
The benefit of modeling morphotacticscan be seen in improved precision for the STM + STM (for English and Finnish) and PRE + STM (forFinnish) patterns when comparing against semi-supervised Baseline.
The more aggressive segmentationof Baseline gives better results for the English PRE + STM subset than for Finnish due to the shortnessof the English prefixes (on average 3.6 letters for the English and 5.3 for the Finnish subset).
Whilenot directly observable in Table 4, a large part of the improvement over semi-supervised Baseline isexplained by that FlatCat does not use suffix-like morphs in incorrect positions.Initializing the FlatCat model with CRF segmentation improves the F-scores in all subsets comparedto the initialization with Morfessor Baseline.
While FlatCat cannot keep the accuracy of the suffixboundaries at as high level as CRF, it clearly improves the stem splitting.3.2 Information retrievalStemming has been shown to improve IR results (Kurimo et al., 2009), by removing inflection that isoften not relevant to the query.
The morph categories make it possible to simulate stemming by removingmorphs categorized as prefixes or suffixes.
As longer affixes are more likely to be meaningful, we limitedthe affix removal to morphs of at most 3 letters.
For methods that use morph categories, we report twoIR results: the first using all the data and the second with short affix removal (SAR) applied.In the IR results, we include the topline methods from Morpho Challenge: Snowball Porter stemmer(Porter, 1980) for English and ?TWOL first?
for Finnish.
The latter selects the lemma from the firstof the possible analyses given by the morphological analyzer FINTWOL (Lingsoft, Inc.) based on the1182(a) English.STM STM + STM PRE + STM STM + SUF STM + SUF + SUFMethod Pre Pre Rec F Pre Rec F Pre Rec F Pre Rec FU CatMAP .90 .94 .63 .75 .91 .64 .75 .87 .45 .59 .90 .51 .65SS Baseline .64 .93 .77 .84 .82 .74 .77 .83 .86 .84 .91 .79 .85SS FlatCat .68 .94 .65 .77 .78 .62 .69 .86 .88 .87 .94 .79 .86SS CRF+FlatCat .68 .95 .78 .86 .78 .66 .72 .87 .89 .88 .94 .80 .87S CRF .78 .94 .72 .81 .85 .59 .69 .92 .91 .91 .95 .82 .88(b) Finnish.STM STM + STM PRE + STM STM + SUF STM + SUF + SUFMethod Pre Pre Rec F Pre Rec F Pre Rec F Pre Rec FU CatMAP .77 .90 .97 .94 .88 .96 .92 .67 .46 .54 .68 .38 .49SS Baseline .50 .82 .88 .85 .73 .83 .78 .64 .85 .73 .76 .78 .77SS FlatCat .49 .91 .95 .93 .80 .89 .85 .67 .84 .75 .77 .75 .76SS CRF+FlatCat .53 .91 .96 .94 .84 .94 .88 .71 .88 .79 .80 .79 .79S CRF .68 .88 .91 .89 .90 .91 .91 .83 .91 .87 .91 .85 .88Table 4: Results of BPR experiments with different morph category patterns.
Best results for eachmeasure have been hilighted using boldface.two-level model by Koskenniemi (1983).
As baseline results we also include unsegmented word formsand truncating each word after the first five letters (First 5).The results of the IR experiment are shown in Table 5.
FlatCat provides the highest score for Finnish.The English scores are similar to those of the semi-supervised Baseline.
FlatCat performs better thanCRF for both languages.
This is explained by the higher level of consistency in the segmentationsproduced by FlatCat, which makes the resulting morphs more useful as query terms.
The number ofmorphs in the lexicons of FlatCat initialized using CRF are 108 391 (English), 46 123 (Finnish) and74 193 (Turkish), which is much smaller than the respective morph lexicon sizes counted from the CRFsegmentation: 339 682 (English), 396 869 (Finnish) and 182 356 (Turkish).
This decrease in lexiconsize indicates a more structured segmentation.The IR performance of semi-supervised FlatCat benefits from the removal of short affixes for Englishwhen initialized by CRF, and Finnish for both initializations.
It also improves the results of unsupervisedFlatCat and CatMAP for Finnish, but lowers the precision for English.
A possible explanation is that theunsupervised methods do not analyze the suffixes with a high enough accuracy.4 ConclusionsWe have introduced a new variant of the Morfessor method, Morfessor FlatCat.
It predicts both morphsand their categories based on unannotated data, but also annotated training data can be provided.
It wasshown to outperform earlier Morfessor methods in the semi-supervised learning task for English, Finnishand Turkish.The purely supervised CRF-based segmentation method proposed by Ruokolainen et al.
(2013) outper-forms FlatCat for Finnish and reaches the same level for English.
However, we show that a discriminativemodel such as CRF gives inconsistent segmentations that do not work as well in a practical application:In English and Finnish information retrieval tasks, FlatCat clearly outperformed the CRF-based segmen-tation.We see two major directions for future work.
Currently Morfessor FlatCat, like most Morfessor meth-ods, assumes that words in a sentence occur independently.
Making use of the sentence context in whichwords occur would, however, allow making Part-Of-Speech -like distinctions.
These distinctions could1183(a) English.Rank Method SAR MAP1 ?
Snowball Porter ?
0.40922 SS Baseline ?
0.38553 SS FlatCat No 0.38374 SS FlatCat Yes 0.38215 SS CRF+FlatCat Yes 0.38106 SS CRF+FlatCat No 0.37887 S CRF ?
0.37718 W Baseline ?
0.37619 U Baseline ?
0.369510 U CatMAP No 0.368211 U CatMAP Yes 0.365312 W FlatCat No 0.365113 ?
(First 5) ?
0.364814 W FlatCat Yes 0.360615 U FlatCat No 0.348616 U FlatCat Yes 0.345117 ?
(Words) ?
0.3303(b) Finnish.Rank Method SAR MAP1 W FlatCat No 0.50572 W FlatCat Yes 0.50293 SS FlatCat Yes 0.49874 ?
TWOL first ?
0.49735 SS CRF+FlatCat Yes 0.49126 U CatMAP Yes 0.48847 U CatMAP No 0.48658 SS CRF+FlatCat No 0.48269 SS FlatCat No 0.482110 ?
(First 5) ?
0.475711 SS Baseline ?
0.472212 S CRF ?
0.466013 W Baseline ?
0.458214 U Baseline ?
0.437815 U FlatCat Yes 0.434916 U FlatCat No 0.433417 ?
(Words) ?
0.3483Table 5: Information Retrieval results.
Results of the method presented in this paper are hilighted usingboldface.
Mean Average Precision is abbreviated as MAP.
Short affix removal is abbreviated as SAR.help disambiguate inflections of different lexemes that have the same surface form but should be analyzeddifferently (Can and Manandhar, 2013).The second direction is removal of the assumption that a morphology consists only of concatenativeprocesses.
Introducing transformations to model allomorphy in a similar manner as Kohonen et al.
(2009) would allow finding the shared abstract morphemes underlying different allomorphs.
This couldbe especially beneficial in information retrieval and machine translation applications.AcknowledgmentsThis research has been supported by European Community?s Seventh Framework Programme(FP7/2007?2013) under grant agreement n?287678 and the Academy of Finland under the Finnish Cen-tre of Excellence Program 2012?2017 (grant n?251170) and the LASTU Programme (grants n?256887and 259934).
The experiments were performed using computer resources within the Aalto UniversitySchool of Science ?Science-IT?
project.
We thank Teemu Ruokolainen for his help with the experiments.ReferencesBurcu Can and Suresh Manandhar.
2013.
Dirichlet processes for joint learning of morphology and PoS tags.
InProceedings of the International Joint Conference on Natural Language Processing, pages 1087?1091, Nagoya,Japan, October.Mathias Creutz and Krista Lagus.
2002.
Unsupervised discovery of morphemes.
In Mike Maxwell, editor,Proceedings of the ACL-02Workshop onMorphological and Phonological Learning, pages 21?30, Philadelphia,PA, USA, July.
Association for Computational Linguistics.Mathias Creutz and Krista Lagus.
2004.
Induction of a simple morphology for highly-inflecting languages.
InProceedings of the Seventh Meeting of the ACL Special Interest Group in Computational Phonology, pages43?51, Barcelona, Spain, July.
Association for Computational Linguistics.Mathias Creutz and Krista Lagus.
2005.
Inducing the morphological lexicon of a natural language from unanno-tated text.
In Timo Honkela, Ville K?on?onen, Matti P?oll?a, and Olli Simula, editors, Proceedings of AKRR?05,1184International and Interdisciplinary Conference on Adaptive Knowledge Representation and Reasoning, pages106?113, Espoo, Finland, June.
Helsinki University of Technology, Laboratory of Computer and InformationScience.Mathias Creutz and Krista Lagus.
2007.
Unsupervised models for morpheme segmentation and morphologylearning.
ACM Transactions on Speech and Language Processing, 4(1):3:1?3:34, January.Harald Hammarstr?om and Lars Borin.
2011.
Unsupervised learning of morphology.
Computational Linguistics,37(2):309?350, June.Oskar Kohonen, Sami Virpioja, and Mikaela Klami.
2009.
Allomorfessor: Towards unsupervised morphemeanalysis.
In Evaluating Systems for Multilingual and Multimodal Information Access: 9th Workshop of theCross-Language Evaluation Forum, CLEF 2008, Aarhus, Denmark, September 17?19, 2008, Revised SelectedPapers, volume 5706 of Lecture Notes in Computer Science, pages 975?982.
Springer Berlin / Heidelberg,September.Oskar Kohonen, Sami Virpioja, and Krista Lagus.
2010.
Semi-supervised learning of concatenative morphology.In Proceedings of the 11th Meeting of the ACL Special Interest Group on Computational Morphology andPhonology, pages 78?86, Uppsala, Sweden, July.
Association for Computational Linguistics.Kimmo Koskenniemi.
1983.
Two-level morphology: A general computational model for word-form recognitionand production.
Ph.D. thesis, University of Helsinki.Mikko Kurimo, Sami Virpioja, Ville T. Turunen, Graeme W. Blackwood, and William Byrne.
2009.
Overview andresults of Morpho Challenge 2009.
In Working Notes for the CLEF 2009 Workshop, Corfu, Greece, September.Mikko Kurimo, Sami Virpioja, Ville Turunen, and Krista Lagus.
2010a.
Morpho Challenge 2005-2010: Eval-uations and results.
In Jeffrey Heinz, Lynne Cahill, and Richard Wicentowski, editors, Proceedings of the11th Meeting of the ACL Special Interest Group on Computational Morphology and Phonology, pages 87?95,Uppsala, Sweden, July.
Association for Computational Linguistics.Mikko Kurimo, Sami Virpioja, and Ville T. Turunen.
2010b.
Overview and results of Morpho Challenge 2010.
InProceedings of the Morpho Challenge 2010 Workshop, pages 7?24, Espoo, Finland, September.
Aalto Univer-sity School of Science and Technology, Department of Information and Computer Science.
Technical ReportTKK-ICS-R37.Paul Ogilvie and James P Callan.
2001.
Experiments using the Lemur toolkit.
In TREC, volume 10, pages103?108.Martin F. Porter.
1980.
An algorithm for suffix stripping.
Program, 14(3):130?137.Jorma Rissanen.
1978.
Modeling by shortest data description.
Automatica, 14:465?471.Teemu Ruokolainen, Oskar Kohonen, Sami Virpioja, and Mikko Kurimo.
2013.
Supervised morphological seg-mentation in a low-resource learning setting using conditional random fields.
In Proceedings of the SeventeenthConference on Computational Natural Language Learning, pages 29?37, Sofia, Bulgaria, August.
Associationfor Computational Linguistics.Sami Virpioja, Ville T Turunen, Sebastian Spiegler, Oskar Kohonen, and Mikko Kurimo.
2011.
Empirical com-parison of evaluation methods for unsupervised learning of morphology.
Traitement Automatique des Langues,52(2):45?90.Sami Virpioja, Peter Smit, Stig-Arne Gr?onroos, and Mikko Kurimo.
2013.
Morfessor 2.0: Python implementationand extensions for Morfessor Baseline.
Report 25/2013 in Aalto University publication series SCIENCE +TECHNOLOGY, Department of Signal Processing and Acoustics, Aalto University.1185
