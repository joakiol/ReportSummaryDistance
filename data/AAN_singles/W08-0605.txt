BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 30?37,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsAccelerating the Annotation of Sparse Named Entitiesby Dynamic Sentence SelectionYoshimasa Tsuruoka1, Jun?ichi Tsujii1,2,3 and Sophia Ananiadou1,31 School of Computer Science, The University of Manchester, UK2 Department of Computer Science, The University of Tokyo, Japan3 National Centre for Text Mining (NaCTeM), Manchester, UKyoshimasa.tsuruoka@manchester.ac.uktsujii@is.s.u-tokyo.ac.jpsophia.ananiadou@manchester.ac.ukAbstractThis paper presents an active learning-likeframework for reducing the human effort formaking named entity annotations in a corpus.In this framework, the annotation work is per-formed as an iterative and interactive processbetween the human annotator and a proba-bilistic named entity tagger.
At each itera-tion, sentences that are most likely to con-tain named entities of the target category areselected by the probabilistic tagger and pre-sented to the annotator.
This iterative anno-tation process is repeated until the estimatedcoverage reaches the desired level.
Unlike ac-tive learning approaches, our framework pro-duces a named entity corpus that is free fromthe sampling bias introduced by the activestrategy.
We evaluated our framework bysimulating the annotation process using twonamed entity corpora and show that our ap-proach could drastically reduce the numberof sentences to be annotated when applied tosparse named entities.1 IntroductionNamed entities play a central role in conveying im-portant domain specific information in text, andgood named entity recognizers are often requiredin building practical information extraction systems.Previous studies have shown that automatic namedentity recognition can be performed with a reason-able level of accuracy by using various machinelearning models such as support vector machines(SVMs) or conditional random fields (CRFs) (TjongKim Sang and De Meulder, 2003; Settles, 2004;Okanohara et al, 2006).However, the lack of annotated corpora, which areindispensable for training machine learning models,makes it difficult to broaden the scope of text miningapplications.
In the biomedical domain, for exam-ple, several annotated corpora such as GENIA (Kimet al, 2003), PennBioIE (Kulick et al, 2004), andGENETAG (Tanabe et al, 2005) have been createdand made publicly available, but the named entitycategories annotated in these corpora are tailored totheir specific needs and not always sufficient or suit-able for text mining tasks that other researchers needto address.Active learning is a framework which can be usedfor reducing the amount of human effort required tocreate a training corpus (Dagan and Engelson, 1995;Engelson and Dagan, 1996; Thompson et al, 1999;Shen et al, 2004).
In active learning, samples thatneed to be annotated by the human annotator arepicked up by a machine learning model in an iter-ative and interactive manner, considering the infor-mativeness of the samples.
Active learning has beenshown to be effective in several natural languageprocessing tasks including named entity recognition.The problem with active learning is, however, thatthe resulting annotated data is highly dependent onthe machine learning algorithm and the samplingstrategy employed, because active learning anno-tates only a subset of the given corpus.
This sam-pling bias is not a serious problem if one is to use theannotated corpus only for their own machine learn-ing purpose and with the same machine learning al-gorithm.
However, the existence of bias is not desir-able if one also wants the corpus to be used by otherapplications or researchers.
For the same reason, ac-30tive learning approaches cannot be used to enrich anexisting linguistic corpus with a new named entitycategory.In this paper, we present a framework that enablesone to make named entity annotations for a givencorpus with a reduced cost.
Unlike active learn-ing approaches, our framework aims to annotate allnamed entities of the target category contained inthe corpus.
Obviously, if we were to ensure 100%coverage of annotation, there is no way of reducingthe annotation cost, i.e.
the human annotator has togo through every sentence in the corpus.
However,we show in this paper that it is possible to reducethe cost by slightly relaxing the requirement for thecoverage, and the reduction can be drastic when thetarget named entities are sparse.We should note here that the purpose of this pa-per is not to claim that our approach is superior toexisting active learning approaches.
The goals aredifferent?while active learning aims at optimizingthe performance of the resulting machine learning-based tagger, our framework aims to help developan unbiased named entity-annotated corpus.This paper is organized as follows.
Section 2 de-scribes the overall annotation flow in our framework.Section 3 presents how to select sentences using theoutput of a probabilistic tagger.
Section 4 describeshow to estimate the coverage during the course ofannotation.
Experimental results using two namedentity corpora are presented in section 5.
Section 6describes related work and discussions.
Concludingremarks are given in section 7.2 Annotating Named Entities by DynamicSentence SelectionFigure 1 shows the overall flow of our annotationframework.
The framework is an iterative processbetween the human annotator and a named entitytagger based on CRFs.
In each iteration, the CRFtagger is trained using all annotated sentences avail-able and is applied to the unannotated sentences toselect sentences that are likely to contain namedentities of the target category.
The selected sen-tences are then annotated by the human annotatorand moved to the pool of annotated sentences.This overall flow of annotation framework is verysimilar to that of active learning.
In fact, the only1.
Select the first n sentences from the corpus andannotate the named entities of the target cate-gory.2.
Train a CRF tagger using all annotated sen-tences.3.
Apply the CRF tagger to the unannotated sen-tences in the corpus and select the top n sen-tences that are most likely to contain targetnamed entities.4.
Annotate the selected sentences.5.
Go back to 2 (repeat until the estimated cover-age reaches a satisfactory level).Figure 1: Annotating named entities by dynamic sentenceselection.differences are the criterion of sentence selectionand the fact that our framework uses the estimatedcoverage as the stopping condition.
In active learn-ing, sentences are selected according to their infor-mativeness to the machine learning algorithm.
Ourapproach, in contrast, selects sentences that are mostlikely to contain named entities of the target cate-gory.
Section 3 elaborates on how to select sentencesusing the output of the CRF-based tagger.The other key in this annotation framework iswhen to stop the annotation work.
If we repeat theprocess until all sentences are annotated, then obvi-ously there is not merit of using this approach.
Weshow in section 4 that we can quite accurately esti-mate how much of the entities in the corpus are al-ready annotated and use this estimated coverage asthe stopping condition.3 Selecting Sentences using the CRFtaggerOur annotation framework takes advantage of theability of CRFs to output multiple probabilistic hy-potheses.
This section describes how we obtainnamed entity candidates and their probabilities fromCRFs in order to compute the expected number ofnamed entities contained in a sentence 1.1We could use other machine learning algorithms for thispurpose as long as they can produce probabilistic output.
For313.1 The CRF taggerCRFs (Lafferty et al, 2001) can be used for namedentity recognition by representing the spans ofnamed entities using the ?BIO?
tagging scheme, inwhich ?B?
represents the beginning of a named en-tity, ?I?
the inside, and ?O?
the outside (See Table 2for example).
This representation converts the taskof named entity recognition into a sequence taggingtask.A linear chain CRF defines a single log-linearprobabilistic distribution over the possible tag se-quences y for a sentence x:p(y|x) = 1Z(x) expT?t=1K?k=1?kfk(t, yt, yt?1,xt),where fk(t, yt, yt?1,xt) is typically a binary func-tion indicating the presence of feature k, ?k is theweight of the feature, and Z(X) is a normalizationfunction:Z(x) =?yexpT?t=1K?k=1?kfk(t, yt, yt?1,xt).This modeling allows us to define features on states(?BIO?
tags) and edges (pairs of adjacent ?BIO?tags) combined with observations (e.g.
words andpart-of-speech (POS) tags).The weights of the features are determinedin such a way that they maximize the condi-tional log-likelihood of the training data2 L(?)
=?Ni=1 log p?(y(i)|x(i)).
We use the L-BFGS algo-rithm (Nocedal, 1980) to compute those parameters.Table 1 lists the feature templates used in the CRFtagger.
We used unigrams of words/POS tags, andprefixes and suffixes of the current word.
The cur-rent word is also normalized by lowering capital let-ters and converting all numerals into ?#?, and usedas a feature.
We created a word shape feature fromthe current word by converting consecutive capitalletters into ?A?, small letters ?a?, and numerals ?#?.example, maximum entropy Markov models are a possible al-ternative.
We chose the CRF model because it has been provedto deliver state-of-the-art performance for named entity recog-nition tasks by previous studies.2In the actual implementation, we used L2 norm penalty forregularization.Word Unigram wi, wi?1, wi+1 & yiPOS Unigram pi, pi?1, pi+1 & yiPrefix, Suffix prefixes of wi & yisuffixes of wi & yi(up to length 3)Normalized Word N(wi) & yiWord Shape S(wi) & yiTag Bi-gram true & yi?1yiTable 1: Feature templates used in the CRF tagger.3.2 Computing the expected number of namedentitiesTo select sentences that are most likely to containnamed entities of the target category, we need toobtain the expected number of named entities con-tained in each sentence.
CRFs are well-suited forthis task as the output is fully probabilistic.Suppose, for example, that the sentence is ?Tran-scription factor GATA-1 and the estrogen receptor?.Table 2 shows an example of the 5-best sequencesoutput by the CRF tagger.
The sequences are rep-resented by the aforementioned ?BIO?
representa-tion.
For example, the first sequence indicates thatthere is one named entity ?Transcription factor?
inthe sequence.
By summing up these probabilistic se-quences, we can compute the probabilities for pos-sible named entities in a sentence.
From the five se-quences in Table 2, we obtain the following threenamed entities and their corresponding probabilities.
?Transcription factor?
(0.677 + 0.242 = 0.916)?estrogen receptor?
(0.242 + 0.009 = 0.251)?Transcription factor GATA-1?
(0.012 + 0.009 =0.021)The expected number of named entities in thissentence can then be calculated as 0.916 + 0.251 +0.021 = 1.188.In this example, we used 5-best sequences as anapproximation of all possible sequences output bythe tagger, which are needed to compute the exactexpected number of entities.
One possible way toachieve a good approximation is to use a large N forN -best sequences, but there is a simpler and moreefficient way 3, which directly produces the exact3We thank an anonymous reviewer for pointing this out.32Probability Transcription factor GATA-1 and the estrogen receptor0.677 B I O O O O O0.242 B I O O O B I0.035 O O O O O O O0.012 B I I O O O O0.009 B I I O O B I: : : : : : : :Table 2: N-best sequences output by the CRF tagger.expected number of entities.
Recall that named enti-ties are represented with the ?BIO?
tags.
Since oneentity always contains one ?B?
tag, we can computethe number of expected entities by simply summingup the marginal probabilities for the ?B?
tag on eachtoken in the sentence4.Once we compute the expected number of enti-ties for every unannotated sentence in the corpus,we sort the sentences in descending order of the ex-pected number of entities and choose the top n sen-tences to be presented to the human annotator.4 Coverage EstimationTo ensure the quality of the resulting annotated cor-pus, it is crucial to be able to know the current cov-erage of annotation at each iteration in the annota-tion process.
To compute the coverage, however,one needs to know the total number of target namedentities in the corpus.
The problem is that it is notknown until all sentences are annotated.In this paper, we solve this dilemma by usingan estimated value for the total number of entities.Then, the estimated coverage can be computed asfollows:(estimated coverage) = mm + ?i?U Ei(1)where m is the number of entities actually annotatedso far and Ei is the expected number of entities insentence i, and U is the set of unannotated sentencesin the corpus.
At any iteration, m is always knownand Ei is obtained from the output of the CRF taggeras explained in the previous section.4The marginal probabilities on each token can be computedby the forward-backward algorithm, which is much more effi-cient than computing N -best sequences for a large N .# Entities Sentences (%)CoNLL: LOC 7,140 5,127 (36.5%)CoNLL: MISC 3,438 2,698 (19.2%)CoNLL: ORG 6,321 4,587 (32.7%)CoNLL: PER 6,600 4,373 (31.1%)GENIA: DNA 2,017 5,251 (28.3%)GENIA: RNA 225 810 ( 4.4%)GENIA: cell line 835 2,880 (15.5%)GENIA: cell type 1,104 5,212 (28.1%)GENIA: protein 5,272 13,040 (70.3%)Table 3: Statistics of named entities.5 ExperimentsWe carried out experiments to see how our methodcan improve the efficiency of annotation processfor sparse named entities.
We evaluate our methodby simulating the annotation process using existingnamed entity corpora.
In other words, we use thegold-standard annotations in the corpus as the anno-tations that would be made by the human annotatorduring the annotation process.5.1 CorpusWe used two named entity corpora for the exper-iments.
One is the training data provided for theCoNLL-2003 shared task (Tjong Kim Sang andDe Meulder, 2003), which consists of 14,041 sen-tences and includes four named entity categories(LOC, MISC, ORG, and PER) for the general do-main.
The other is the training data provided forthe NLPBA shared task (Kim et al, 2004), whichconsists of 18,546 sentences and five named entitycategories (DNA, RNA, cell line, cell type, and pro-tein) for the biomedical domain.
This corpus is cre-ated from the GENIA corpus (Kim et al, 2003) bymerging the original fine-grained named entity cate-gories.3300.20.40.60.810  2000  4000  6000  8000  10000Number of SentencesCoverageEstimated CoverageBaselineFigure 2: Annotation of LOC in the CoNLL corpus.00.20.40.60.810  2000  4000  6000  8000  10000Number of SentencesCoverageEstimated CoverageBaselineFigure 3: Annotation of MISC in the CoNLL corpus.Table 3 shows statistics of the named entities in-cluded in the corpora.
The first column shows thenumber of named entities for each category.
Thesecond column shows the number of the sentencesthat contain the named entities of each category.
Wecan see that some of the named entity categories arevery sparse.
For example, named entities of ?RNA?appear only in 4.4% of the sentences in the corpus.In contrast, named entities of ?protein?
appear inmore than 70% of the sentences in the corpus.In the experiments reported in the following sec-tions, we do not use the ?protein?
category becausethere is no merit of using our framework when mostsentences are relevant to the target category.5.2 ResultsWe carried out eight sets of experiments, each ofwhich corresponds to one of those named entity cat-egories shown in Table 3 (excluding the ?protein?category).
The number of sentences selected in eachiteration (the value of n in Figure 1) was set to 10000.20.40.60.810  2000  4000  6000  8000  10000Number of SentencesCoverageEstimated CoverageBaselineFigure 4: Annotation of ORG in the CoNLL corpus.00.20.40.60.810  2000  4000  6000  8000  10000Number of SentencesCoverageEstimated CoverageBaselineFigure 5: Annotation of PER in the CoNLL corpus.throughout all experiments.Figures 2 to 5 show the results obtained on theCoNLL data.
The figures show how the coverageincreases as the annotation process proceeds.
Thex-axis shows the number of annotated sentences.Each figure contains three lines.
The normal linerepresents the coverage actually achieved, which iscomputed as follows:(coverage) = entities annotatedtotal number of entities .
(2)The dashed line represents the coverage estimatedby using equation 1.
For the purpose of comparison,the dotted line shows the coverage achieved by thebaseline annotation strategy in which sentences areselected sequentially from the beginning to the endin the corpus.The figures clearly show that our method candrastically accelerate the annotation process in com-parison to the baseline annotation strategy.
The im-provement is most evident in Figure 3, in which3400.20.40.60.810  2000  4000  6000  8000  10000Number of SentencesCoverageEstimated CoverageBaselineFigure 6: Annotation of DNA in the GENIA corpus.00.20.40.60.810  2000  4000  6000  8000  10000Number of SentencesCoverageEstimated CoverageBaselineFigure 7: Annotation of RNA in the GENIA corpus.named entities of the category ?MISC?
are anno-tated.We should also note that coverage estimation wassurprisingly accurate.
In all experiments, the differ-ence between the estimated coverage and the realcoverage was very small.
This means that we cansafely use the estimated coverage as the stoppingcondition for the annotation work.Figures 6 to 9 show the experimental results onthe GENIA data.
The figures show the same char-acteristics observed in the CoNLL data.
The accel-eration by our framework was most evident for the?RNA?
category.Table 4 shows how much we can save the annota-tion cost if we stop the annotation process when theestimated coverage reaches 99%.
The first columnshows the coverage actually achieved and the secondcolumn shows the number and ratio of the sentencesannotated in the corpus.
This table shows that, onaverage, we can achieve a coverage of 99.0% by an-notating 52.4% of the sentences in the corpus.
In00.20.40.60.810  2000  4000  6000  8000  10000Number of SentencesCoverageEstimated CoverageBaselineFigure 8: Annotation of cell line in the GENIA corpus.00.20.40.60.810  2000  4000  6000  8000  10000Number of SentencesCoverageEstimated CoverageBaselineFigure 9: Annotation of cell type in the GENIA corpus.other words, we could roughly halve the annotationcost by accepting the missing rate of 1.0%.As expected, the cost reduction was most drasticwhen ?RNA?, which is the most sparse named entitycategory (see Table 3), was targeted.
The cost reduc-tion was more than seven-fold.
These experimentalresults confirm that our annotation framework is par-ticularly useful when applied to sparse named enti-ties.Table 4 also shows the timing information on theexperiments 5.
One of the potential problems withthis kind of active learning-like framework is thecomputation time required to retrain the tagger ateach iteration.
Since the human annotator has towait while the tagger is being retrained, the compu-tation time required for retraining the tagger shouldnot be very long.
In our experiments, the worstcase (i.e.
DNA) required 443 seconds for retrain-ing the tagger at the last iteration, but in most cases5We used AMD Opteron 2.2GHz servers for the experimentsand our CRF tagger is implemented in C++.35Coverage Sentences Annotated (%) Cumulative Time (second) Last Interval (second)CoNLL: LOC 99.1% 7,600 (54.1%) 3,362 92CoNLL: MISC 96.9% 5,400 (38.5%) 1,818 61CoNLL: ORG 99.7% 8,900 (63.4%) 5,201 104CoNLL: PER 98.0% 6,200 (44.2%) 2,300 75GENIA: DNA 99.8% 11,900 (64.2%) 33,464 443GENIA: RNA 99.2% 2,500 (13.5%) 822 56GENIA: cell line 99.6% 9,400 (50.7%) 15,870 284GENIA: cell type 99.3% 8,600 (46.4%) 13,487 295Average 99.0% - (52.4%) - -Table 4: Coverage achieved when the estimated coverage reached 99%.the training time for each iteration was kept underseveral minutes.In this work, we used the BFGS algorithm fortraining the CRF model, but it is probably possible tofurther reduce the training time by using more recentparameter estimation algorithms such as exponenti-ated gradient algorithms (Globerson et al, 2007).6 Discussion and Related WorkOur annotation framework is, by definition, notsomething that can ensure a coverage of 100%.
Theseriousness of a missing rate of, for example, 1% isnot entirely clear?it depends on the application andthe purpose of annotation.
In general, however, itis hard to achieve a coverage of 100% in real an-notation work even if the human annotator scansthrough all sentences, because there is often ambi-guity in deciding whether a particular named entityshould be annotated or not.
Previous studies reportthat inter-annotator agreement rates with regards togene/protein name annotation are f-scores around90% (Morgan et al, 2004; Vlachos and Gasperin,2006).
We believe that the missing rate of 1% can bean acceptable level of sacrifice, given the cost reduc-tion achieved and the unavoidable discrepancy madeby the human annotator.At the same time, we should also note that ourframework could be used in conjunction with ex-isting methods for semi-supervised learning to im-prove the performance of the CRF tagger, whichin turn will improve the coverage.
It is also pos-sible to improve the performance of the tagger byusing external dictionaries or using more sophis-ticated probabilistic models such as semi-MarkovCRFs (Sarawagi and Cohen, 2004).
These enhance-ments should further improve the coverage, keepingthe same degree of cost reduction.The idea of improving the efficiency of annota-tion work by using automatic taggers is certainly notnew.
Tanabe et al (2005) applied a gene/proteinname tagger to the target sentences and modifiedthe results manually.
Culotta and McCallum (2005)proposed to have the human annotator select thecorrect annotation from multiple choices producedby a CRF tagger for each sentence.
Tomanek etal.
(2007) discuss the reusability of named entity-annotated corpora created by an active learning ap-proach and show that it is possible to build a cor-pus that is useful to different machine learning algo-rithms to a certain degree.The limitation of our framework is that it is use-ful only when the target named entities are sparsebecause the upper bound of cost saving is limitedby the proportion of the relevant sentences in thecorpus.
Our framework may therefore not be suit-able for a situation where one wants to make an-notations for named entities of many categories si-multaneously (e.g.
creating a corpus like GENIAfrom scratch).
In contrast, our framework should beuseful in a situation where one needs to modify orenrich named entity annotations in an existing cor-pus, because the target named entities are almost al-ways sparse in such cases.
We should also note thatnamed entities in full papers, which recently startedto attract much attention, tend to be more sparse thanthose in abstracts.7 ConclusionWe have presented a simple but powerful frameworkfor reducing the human effort for making name en-tity annotations in a corpus.
The proposed frame-work allows us to annotate almost all named entities36of the target category in the given corpus withouthaving to scan through all the sentences.
The frame-work also allows us to know when to stop the anno-tation process by consulting the estimated coverageof annotation.Experimental results demonstrated that the frame-work can reduce the number of sentences to be anno-tated almost by half, achieving a coverage of 99.0%.Our framework was particularly effective when thetarget named entities were very sparse.Unlike active learning, this work enables us tocreate a named entity corpus that is free from thesampling bias introduced by the active learning strat-egy.
This work will therefore be especially usefulwhen one needs to enrich an existing linguistic cor-pus (e.g.
WSJ, GENIA, or PennBioIE) with namedentity annotations for a new semantic category.AcknowledgmentThis work is partially supported by BBSRC grantBB/E004431/1.
The UK National Centre for TextMining is sponsored by the JISC/BBSRC/EPSRC.ReferencesAron Culotta and Andrew McCallum.
2005.
Reducinglabeling effort for structured prediction tasks.
In Pro-ceedings of AAAI-05, pages 746?751.Ido Dagan and Sean P. Engelson.
1995.
Committee-based sampling for training probabilistic classifiers.
InProceedings of ICML, pages 150?157.Sean Engelson and Ido Dagan.
1996.
Minimizing man-ual annotation cost in supervised training from cor-pora.
In Proceedings of ACL, pages 319?326.A.
Globerson, T. Koo, X. Carreras, and M. Collins.
2007.Exponentiated gradient algorithms for log-linear struc-tured prediction.
In Proceedings of ICML, pages 305?312.J.-D. Kim, T. Ohta, Y. Tateisi, and J. Tsujii.
2003.
GE-NIA corpus?a semantically annotated corpus for bio-textmining.
Bioinformatics, 19 (Suppl.
1):180?182.J.-D. Kim, T. Ohta, Y. Tsuruoka, Y. Tateisi, and N. Col-lier.
2004.
Introduction to the bio-entity recogni-tion task at JNLPBA.
In Proceedings of the Interna-tional Joint Workshop on Natural Language Process-ing in Biomedicine and its Applications (JNLPBA),pages 70?75.Seth Kulick, Ann Bies, Mark Libeman, Mark Mandel,Ryan McDonald, Martha Palmer, Andrew Schein, andLyle Ungar.
2004.
Integrated annotation for biomed-ical information extraction.
In Proceedings of HLT-NAACL 2004 Workshop: Biolink 2004, pages 61?68.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional random fields: Probabilistic mod-els for segmenting and labeling sequence data.
In Pro-ceedings of ICML, pages 282?289.Alexander A. Morgan, Lynette Hirschman, MarcColosimo, Alexander S. Yeh, and Jeff B. Colombe.2004.
Gene name identification and normalization us-ing a model organism database.
Journal of BiomedicalInformatics, 37:396?410.Jorge Nocedal.
1980.
Updating quasi-newton matriceswith limited storage.
Mathematics of Computation,35(151):773?782.Daisuke Okanohara, Yusuke Miyao, Yoshimasa Tsu-ruoka, and Jun?ichi Tsujii.
2006.
Improving the scal-ability of semi-markov conditional random fields fornamed entity recognition.
In Proceedings of COL-ING/ACL, pages 465?472.Sunita Sarawagi and William W. Cohen.
2004.
Semi-markov conditional random fields for information ex-traction.
In Proceedings of NIPS.Burr Settles.
2004.
Biomedical named entity recogni-tion using conditional random fields and rich featuresets.
In COLING 2004 International Joint workshopon Natural Language Processing in Biomedicine andits Applications (NLPBA/BioNLP) 2004, pages 107?110.Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, and Chew-Lim Tan.
2004.
Multi-criteria-based active learningfor named entity recognition.
In Proceedings of ACL,pages 589?596, Barcelona, Spain.Lorraine Tanabe, Natalie Xie, Lynne H. Thom, WayneMatten, and W. John Wilbur.
2005.
GENETAG: atagged corpus for gene/protein named entity recogni-tion.
BMC Bioinformatics, 6(Suppl 1):S3.Cynthia A. Thompson, Mary Elaine Califf, and Ray-mond J. Mooney.
1999.
Active learning for naturallanguage parsing and information extraction.
In Pro-ceedings of ICML, pages 406?414.Erik F. Tjong Kim Sang and Fien De Meulder.
2003.Introduction to the conll-2003 shared task: Language-independent named entity recognition.
In Proceedingsof CoNLL-2003, pages 142?147.Katrin Tomanek, Joachim Wermter, and Udo Hahn.2007.
An approach to text corpus construction whichcuts annotation costs and maintains reusability of an-notated data.
In Proceedings of EMNLP-CoNLL,pages 486?495.Andreas Vlachos and Caroline Gasperin.
2006.
Boot-strapping and evaluating named entity recognition inthe biomedical domain.
In Proceedings of the HLT-NAACL BioNLP Workshop on Linking Natural Lan-guage and Biology, pages 138?145.37
