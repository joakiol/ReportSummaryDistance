Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 275?279,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsEfficient Tree-Based Topic ModelingYuening HuDepartment of Computer ScienceUniversity of Maryland, College Parkynhu@cs.umd.eduJordan Boyd-GraberiSchool and UMIACSUniversity of Maryland, College Parkjbg@umiacs.umd.eduAbstractTopic modeling with a tree-based prior hasbeen used for a variety of applications be-cause it can encode correlations between wordsthat traditional topic modeling cannot.
How-ever, its expressive power comes at the costof more complicated inference.
We extendthe SPARSELDA (Yao et al, 2009) inferencescheme for latent Dirichlet alocation (LDA)to tree-based topic models.
This samplingscheme computes the exact conditional distri-bution for Gibbs sampling much more quicklythan enumerating all possible latent variableassignments.
We further improve performanceby iteratively refining the sampling distributiononly when needed.
Experiments show that theproposed techniques dramatically improve thecomputation time.1 IntroductionTopic models, exemplified by latent Dirichlet aloca-tion (LDA) (Blei et al, 2003), discover latent themespresent in text collections.
?Topics?
discovered bytopic models are multinomial probability distribu-tions over words that evince thematic coherence.Topic models are used in computational biology, com-puter vision, music, and, of course, text analysis.One of LDA?s virtues is that it is a simple modelthat assumes a symmetric Dirichlet prior over itsword distributions.
Recent work argues for structureddistributions that constrain clusters (Andrzejewski etal., 2009), span languages (Jagarlamudi and Daume?III, 2010), or incorporate human feedback (Hu et al,2011) to improve the quality and flexibility of topicmodeling.
These models all use different tree-basedprior distributions (Section 2).These approaches are appealing because theypreserve conjugacy, making inference using Gibbssampling (Heinrich, 2004) straightforward.
Whilestraightforward, inference isn?t cheap.
Particularlyfor interactive settings (Hu et al, 2011), efficientinference would improve perceived latency.SPARSELDA (Yao et al, 2009) is an efficientGibbs sampling algorithm for LDA based on a refac-torization of the conditional topic distribution (re-viewed in Section 3).
However, it is not directlyapplicable to tree-based priors.
In Section 4, we pro-vide a factorization for tree-based models within abroadly applicable inference framework that empiri-cally improves the efficiency of inference (Section 5).2 Topic Modeling with Tree-Based PriorsTrees are intuitive methods for encoding humanknowledge.
Abney and Light (1999) used tree-structured multinomials to model selectional restric-tions, which was later put into a Bayesian contextfor topic modeling (Boyd-Graber et al, 2007).
Inboth cases, the tree came from WordNet (Miller,1990), but the tree could also come from domainexperts (Andrzejewski et al, 2009).Organizing words in this way induces correlationsthat are mathematically impossible to represent witha symmetric Dirichlet prior.
To see how correlationscan occur, consider the generative process.
Start witha rooted tree structure that contains internal nodesand leaf nodes.
This skeleton is a prior that generatesK topics.
Like vanilla LDA, these topics are distribu-tions over words.
Unlike vanilla LDA, their structurecorrelates words.
Internal nodes have a distributionpik,i over children, where pik,i comes from per-nodeDirichlet parameterized by ?i.1 Each leaf node isassociated with a word, and each word must appearin at least (possibly more than) one leaf node.To generate a word from topic k, start at the root.Select a child x0 ?
Mult(pik,ROOT), and traversethe tree until reaching a leaf node.
Then emit theleaf?s associated word.
This walk replaces the drawfrom a topic?s multinomial distribution over words.1Choosing these Dirichlet priors specifies the direction (i.e.,positive or negative) and strength of correlations that appear.275The rest of the generative process for LDA remainsthe same, with ?, the per-document topic multinomial,and z, the topic assignment.This tree structure encodes correlations.
The closertypes are in the tree, the more correlated they are.Because types can appear in multiple leaf nodes, thisencodes polysemy.
The path that generates a token isan additional latent variable we must sample.Gibbs sampling is straightforward because the tree-based prior maintains conjugacy (Andrzejewski etal., 2009).
We integrate the per-document topic dis-tributions ?
and the transition distributions pi.
Theremaining latent variables are the topic assignment zand path l, which we sample jointly:2p(z = k, l = ?|Z?, L?, w) (1)?
(?k + nk|d)?(i?j)??
?i?j + ni?j|k?j?
(?i?j?
+ ni?j?|k)where nk|d is topic k?s count in the document d;?k is topic k?s prior; Z?
and L?
are topic and pathassignments excluding wd,n; ?i?j is the prior foredge i ?
j, ni?j|t is the count of edge i ?
j intopic k; and j?
denotes other children of node i.The complexity of computing the sampling distri-bution is O(KLS) for models with K topics, pathsat most L nodes long, and at most S paths per wordtype.
In contrast, for vanilla LDA the analogousconditional sampling distribution requires O(K).3 Efficient LDAThe SPARSELDA (Yao et al, 2009) scheme forspeeding inference begins by rearranging LDA?s sam-pling equation into three terms:3p(z = k|Z?, w) ?
(?k + nk|d)?
+ nw|k?V + n?|k(2)??k?
?V + n?|k?
??
?sLDA+nk|d?
?V + n?|k?
??
?rLDA+(?k + nk|d)nw|k?V + n?|k?
??
?qLDAFollowing their lead, we call these three terms?buckets?.
A bucket is the total probability massmarginalizing over latent variable assignments (i.e.,sLDA ??k?k?
?V+n?|k, similarly for the other buck-ets).
The three buckets are a smoothing only bucket2For clarity, we omit indicators that ensure ?
ends at wd,n.3To ease notation we drop the d,n subscript for z and w inthis and future equations.sLDA, document topic bucket rLDA, and topic wordbucket qLDA (we use the ?LDA?
subscript to contrastwith our method, for which we use the same bucketnames without subscripts).Caching the buckets?
total mass speeds the compu-tation of the sampling distribution.
Bucket sLDA isshared by all tokens, and bucket rLDA is shared by adocument?s tokens.
Both have simple constant timeupdates.
Bucket qLDA has to be computed specifi-cally for each token, but only for the (typically) fewtypes with non-zero counts in a topic.To sample from the conditional distribution, firstsample which bucket you need and then (and onlythen) select a topic within that bucket.
Because thetopic-term bucket qLDA often has the largest massand has few non-zero terms, this speeds inference.4 Efficient Inference in Tree-Based ModelsIn this section, we extend the sampling techniquesfor SPARSELDA to tree-based topic modeling.
Wefirst factor Equation 1:p(z = k, l = ?|Z?, L?, w) (3)?
(?k + nk|d)N?1k,?[S?
+Ok,?
].Henceforth we call Nk,?
the normalizer for path ?in topic k, S?
the smoothing factor for path ?, andOk,?
the observation for path ?
in topic k, which areNk,?
=?(i?j)???j?(?i?j?
+ ni?j?|k)S?
=?(i?j)??
?i?j (4)Ok,?
=?(i?j)??
(?i?j + ni?j|k)??(i?j)??
?i?j .Equation 3 can be rearranged in the same wayas Equation 5, yielding buckets analogous toSPARSELDA?s,p(z = k,l = ?|Z?, L?, w) (5)??kS?Nk,??
??
?s+nk|dS?Nk,??
??
?r+(?k + nk|d)Ok,?Nk,??
??
?q.Buckets sum both topics and paths.
The samplingprocess is much the same as for SPARSELDA: selectwhich bucket and then select a topic / path combina-tion within the bucket (for a slightly more complexexample, see Algorithm 1).276Recall that one of the benefits of SPARSELDA wasthat s was shared across tokens.
This is no longerpossible, as Nk,?
is distinct for each path in tree-based LDA.
Moreover, Nk,?
is coupled; changingni?j|k in one path changes the normalizers of allcousin paths (paths that share some node i).This negates the benefit of caching s, but we re-cover some of the benefits by splitting the normalizerto two parts: the ?root?
normalizer from the root node(shared by all paths) and the ?downstream?
normal-izer.
We precompute which paths share downstreamnormalizers; all paths are partitioned into cousin sets,defined as sets for which changing the count of onemember of the set changes the downstream normal-izer of other paths in the set.
Thus, when updatingthe counts for path l, we only recompute Nk,l?
for alll?
in the cousin set.SPARSELDA?s computation of q, the topic-wordbucket, benefits from topics with unobserved (i.e.,zero count) types.
In our case, any non-zero path, apath with any non-zero edge, contributes.4 To quicklydetermine whether a path contributes, we introducean edge-masked count (EMC) for each path.
Higherorder bits encode whether edges have been observedand lower order bits encode the number of times thepath has been observed.
For example, if a path oflength three only has its first two edges observed, itsEMC is 11000000.
If the same path were observedseven times, its EMC is 11100111.
With this formu-lation we can ignore any paths with a zero EMC.Efficient sampling with refined bucket Whilecaching the sampling equation as described in theprevious section improved the efficiency, the smooth-ing only bucket s is small, but computing the asso-ciated mass is costly because it requires us to con-sider all topics and paths.
This is not a problemfor SparseLDA because s is shared across all tokens.However, we can achieve computational gains withan upper bound on s,s =?k,??k?(i?j)??
?i?j?(i?j)???j?
(?i?j?
+ ni?j?|k)??k,??k?(i?j)??
?i?j?(i?j)???j?
?i?j?= s?.
(6)A sampling algorithm can take advantage of thisby not explicitly calculating s. Instead, we use s?4C.f.
observed paths, where all edges are non-zero.as proxy, and only compute the exact s if we hit thebucket s?
(Algorithm 1).
Removing s?
and alwayscomputing s yields the first algorithm in Section 4.Algorithm 1 SAMPLING WITH REFINED BUCKET1: for word w in this document do2: sample = rand() ?(s?
+ r + q)3: if sample < s?
then4: compute s5: sample = sample ?
(s+ r + q)/(s?
+ r + q)6: if sample < s then7: return topic k and path ?
sampled from s8: sample ?
= s9: else10: sample ?
= s?11: if sample < r then12: return topic k and path ?
sampled from r13: sample ?
= r14: return topic k and path ?
sampled from qSorting Thus far, we described techniques for ef-ficiently computing buckets, but quickly samplingassignments within a bucket is also important.
Herewe propose two techniques to consider latent vari-able assignments in decreasing order of probabilitymass.
By considering fewer possible assignments,we can speed sampling at the cost of the overheadof maintaining sorted data structures.
We sort top-ics?
prominence within a document (SD) and sort thetopics and paths of a word (SW).Sorting topics?
prominence within a document(SD) can improve sampling from r and q; when weneed to sample within a bucket, we consider paths indecreasing order of nk|d.Sorting path prominence for a word (SW) can im-prove our ability to sample from q.
The edge-maskedcount (EMC), as described above, serves as a proxyfor the probability of a path and topic.
If, when sam-pling a topic and path from q, we sample based onthe decreasing EMC, which roughly correlates withpath probability.5 ExperimentsIn this section, we compare the running time5 of oursampling algorithm (FAST) and our algorithm withthe refined bucket (RB) against the unfactored Gibbssampler (NAI?VE) and examine the effect of sorting.Our corpus has editorials from New York Times5Mean of five chains on a 6-Core 2.8-GHz CPU, 16GB RAM277Number of TopicsT50 T100 T200 T500NAIVE 5.700 12.655 29.200 71.223FAST 4.935 9.222 17.559 40.691FAST-RB 2.937 4.037 5.880 8.551FAST-RB-SD 2.675 3.795 5.400 8.363FAST-RB-SW 2.449 3.363 4.894 7.404FAST-RB-SDW 2.225 3.241 4.672 7.424Vocabulary SizeV5000 V10000 V20000 V30000NAI?VE 4.815 12.351 28.783 51.088FAST 2.897 9.063 20.460 38.119FAST-RB 1.012 3.900 9.777 20.040FAST-RB-SD 0.972 3.684 9.287 18.685FAST-RB-SW 0.889 3.376 8.406 16.640FAST-RB-SDW 0.828 3.113 7.777 15.397Number of CorrelationsC50 C100 C200 C500NAI?VE 11.166 12.586 13.000 15.377FAST 8.889 9.165 9.177 8.079FAST-RB 3.995 4.078 3.858 3.156FAST-RB-SD 3.660 3.795 3.593 3.065FAST-RB-SW 3.272 3.363 3.308 2.787FAST-RB-SDW 3.026 3.241 3.091 2.627Table 1: The average running time per iteration (S) over100 iterations, averaged over 5 seeds.
Experiments beginwith 100 topics, 100 correlations, vocab size 10000 andthen vary one dimension: number of topics (top), vocabu-lary size (middle), and number of correlations (bottom).from 1987 to 1996.6 Since we are interested in vary-ing vocabulary size, we rank types by average tf-idfand choose the top V .
WordNet 3.0 generates the cor-relations between types.
For each synset in WordNet,we generate a subtree with all types in the synset?that are also in our vocabulary?as leaves connectedto a common parent.
This subtree?s common parentis then attached to the root node.We compared the FAST and FAST-RB againstNAI?VE (Table 1) on different numbers of topics, var-ious vocabulary sizes and different numbers of cor-relations.
FAST is consistently faster than NAI?VEand FAST-RB is consistently faster than FAST.
Theirbenefits are clearer as distributions become sparse(e.g., the first iteration for FAST is slower than lateriterations).
Gains accumulate as the topic numberincreases, but decrease a little with the vocabularysize.
While both sorting strategies reduce time, sort-ing topics and paths for a word (SW) helps more thansorting topics in a document (SD), and combining the613284 documents, 41554 types, and 2714634 tokens.1 1.05 1.1 1.15 1.2 1.25 1.3 1.35 1.4246810121416Average number of senses per constraint wordAveragerunning time per iteration (S)NaiveFastFast?RBFast?RB?sDFast?RB?sWFast?RB?sDWFigure 1: The average running time per iteration againstthe average number of senses per correlated words.two is (with one exception) better than either alone.As more correlations are added, NAI?VE?s time in-creases while that of FAST-RB decreases.
This is be-cause the number of non-zero paths for uncorrelatedwords decreases as more correlations are added to themodel.
Since our techniques save computation forevery zero path, the overall computation decreasesas correlations push uncorrelated words to a limitednumber of topics (Figure 1).
Qualitatively, when thesynset with ?king?
and ?baron?
is added to a model,it is associated with ?drug, inmate, colombia, water-front, baron?
in a topic; when ?king?
is correlatedwith ?queen?, the associated topic has ?king, parade,museum, queen, jackson?
as its most probable words.These represent reasonable disambiguations.
In con-trast to previous approaches, inference speeds up astopics become more semantically coherent (Boyd-Graber et al, 2007).6 ConclusionWe demonstrated efficient inference techniques fortopic models with tree-based priors.
These methodsscale well, allowing for faster exploration of modelsthat use semantics to encode correlations without sac-rificing accuracy.
Improved scalability for such algo-rithms, especially in distributed environments (Smolaand Narayanamurthy, 2010), could improve applica-tions such as cross-language information retrieval,unsupervised word sense disambiguation, and knowl-edge discovery via interactive topic modeling.278AcknowledgmentsWe would like to thank David Mimno and the anony-mous reviewers for their helpful comments.
Thiswork was supported by the Army Research Labora-tory through ARL Cooperative Agreement W911NF-09-2-0072.
Any opinions or conclusions expressedare the authors?
and do not necessarily reflect thoseof the sponsors.ReferencesSteven Abney and Marc Light.
1999.
Hiding a seman-tic hierarchy in a Markov model.
In Proceedings ofthe Workshop on Unsupervised Learning in NaturalLanguage Processing.David Andrzejewski, Xiaojin Zhu, and Mark Craven.2009.
Incorporating domain knowledge into topic mod-eling via Dirichlet forest priors.
In Proceedings ofInternational Conference of Machine Learning.David M. Blei, Andrew Ng, and Michael Jordan.
2003.Latent Dirichlet alocation.
Journal of Machine Learn-ing Research, 3:993?1022.Jordan Boyd-Graber, David M. Blei, and Xiaojin Zhu.2007.
A topic model for word sense disambiguation.In Proceedings of Emperical Methods in Natural Lan-guage Processing.Gregor Heinrich.
2004.
Parameter estima-tion for text analysis.
Technical report.http://www.arbylon.net/publications/text-est.pdf.Yuening Hu, Jordan Boyd-Graber, and Brianna Satinoff.2011.
Interactive topic modeling.
In Association forComputational Linguistics.Jagadeesh Jagarlamudi and Hal Daume?
III.
2010.
Ex-tracting multilingual topics from unaligned corpora.
InProceedings of the European Conference on Informa-tion Retrieval (ECIR).George A. Miller.
1990.
Nouns in WordNet: A lexicalinheritance system.
International Journal of Lexicog-raphy, 3(4):245?264.Alexander J. Smola and Shravan Narayanamurthy.
2010.An architecture for parallel topic models.
InternationalConference on Very Large Databases, 3.Limin Yao, David Mimno, and Andrew McCallum.
2009.Efficient methods for topic model inference on stream-ing document collections.
In Knowledge Discovery andData Mining.279
