Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1914?1925,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsLearning Distributions over Logical Formsfor Referring Expression GenerationNicholas FitzGerald Yoav Artzi Luke ZettlemoyerComputer Science & EngineeringUniversity of WashingtonSeattle, WA 98195{nfitz,yoav,lsz}@cs.washington.eduAbstractWe present a new approach to referring ex-pression generation, casting it as a density es-timation problem where the goal is to learndistributions over logical expressions identi-fying sets of objects in the world.
Despitean extremely large space of possible expres-sions, we demonstrate effective learning ofa globally normalized log-linear distribution.This learning is enabled by a new, multi-stageapproximate inference technique that uses apruning model to construct only the mostlikely logical forms.
We train and evaluatethe approach on a new corpus of referencesto sets of visual objects.
Experiments showthe approach is able to learn accurate models,which generate over 87% of the expressionspeople used.
Additionally, on the previouslystudied special case of single object reference,we show a 35% relative error reduction overprevious state of the art.1 IntroductionUnderstanding and generating natural language re-quires reasoning over a large space of possiblemeanings; while many statements might achieve thesame goal in a certain situation, some are morelikely to be used than others.
In this paper, we modelthese preferences by learning distributions over sit-uated meaning use.We focus on the task of referring expression gen-eration (REG), where the goal is to produce an ex-pression which uniquely identifies a pre-defined ob-ject or set of objects in an environment.
In prac-tice, many such expressions can be produced.
Fig-ure 1 shows referring expressions provided by hu-man subjects for a set of objects (Figure 1a), demon-strating variation in utterances (Figure 1b) and theircorresponding meaning representations (Figure 1c).Although nearly a third of the people simply listedthe colors of the desired objects, many other strate-gies were also used and no single option dominated.Learning to model such variation would enable sys-tems to better anticipate what people are likely tosay and avoid repetition during generation, by pro-ducing appropriately varied utterances themselves.With these goals in mind, we cast REG as a den-sity estimation problem, where the goal is to learn adistribution over logical forms.Learning such distributions is challenging.
For atarget set of objects, the number of logical formsthat can be used to describe it grows combinatori-ally with the number of observable properties, suchas color and shape.
However, only a tiny fractionof these possibilities are ever actually used by peo-ple.
We must learn to efficiently find these few, andaccurately estimate their associated likelihoods.We demonstrate effective learning of a globallynormalized log-linear distribution with features toaccount for context dependence and communicativegoals.
We use a stochastic gradient descent algo-rithm, where the key challenge is the need to com-pute feature expectations over all possible logicalforms.
For that purpose, we present a multi-stageinference algorithm, which progressively constructsmeaning representations with increasing complex-ity, and learns a pruning model to retain only thosethat are likely to lead to high probability expres-sions.
This approach allows us to consider a large1914(a)The green, red, orange and yellow toys.
(1)The green, red, yellow, and orange objects.
(1)The red, green, yellow and orange toys.
(1)The red, yellow, orange and green objects.
(1)All the green, red, yellow and orange toys.
(1)All the yellow, orange, red and green objects.
(1)All the pieces that are not blue or brown.
(2)All items that are not brown or blue.
(2)All items that are not brown or blue.
(2)Everything that is not brown or blue.
(3)Everything that is not purple or blue.
(3)All but the black and blue ones.
(4)Any toy but the blue and brown toys.
(4)Everything that is green, red, orange or yellow.
(5)All objects that are not triangular or blue.
(6)Everything that is not blue or a wedge.
(7)Everything that is not a brown or blue toy.
(8)All but the blue piece and brown wedge.
(9)Everything except the brown wedge and the blue object.
(10)All pieces but the blue piece and brown triangle shape.
(11)(b)P?
(z|S,G) z0.30 ?(?x.
(yellow(x) ?
orange(x) ?
red(x) ?
green(x)) ?
object(x) ?
plu(x)) (1)0.15 ?(?x.?
(brown(x) ?
blue(x)) ?
object(x) ?
plu(x)) (2)0.10 Every(?x.?
(brown(x) ?
blue(x)) ?
object(x) ?
sg(x)) (3)0.10 Every(?x.object(x) ?
sg(x)) \ [?(?x.
(blue(x) ?
brown(x)) ?
object(x) ?
plu(x)] (4)0.05 Every(?x.
(yellow(x) ?
orange(x) ?
red(x) ?
green(x)) ?
object(x) ?
sg(x)) (5)0.05 ?(?x.
(triangle(x) ?
blue(x)) ?
object(x) ?
plu(x)) (6)0.05 Every(?x.object(x) ?
sg(x)?
(blue(x) ?
equal(x,A(?y.triangle(y) ?
sg(y))))) (7)0.05 Every(?x.object(x) ?
sg(x) ?
?equal(x,A(?y.
(brown(y) ?
blue(y)) ?
object(y) ?
sg(y)))) (8)0.05 Every(?x.object(x) ?
sg(x)) \ [?(?x.
(blue(x) ?
object(x) ?
sg(x)) ?
(brown(x) ?
triangle(x) ?
sg(x))] (9)0.05 Every(?x.object(x) ?
sg(x)) \ [?
(?x.brown(x) ?
triangle(x) ?
sg(x)) ?
?
(?y.blue(y) ?
object(y) ?
sg(x))] (10)0.05 ?
(?x.object(x) ?
plu(x)) \ [?(?x.
(blue(x) ?
object(x) ?
sg(x)) ?
(brown(x) ?
triangle(x) ?
object(x) ?
sg(x))] (11)(c)Figure 1: An example scene from our object selection dataset.
Figure 1a shows the image shown to subjectson Amazon Mechanical Turk.
The target set G is the circled objects.
Figure 1b shows the 20 sentencesprovided as responses.
Figure 1c shows the empirical distribution P?
(z|G,S) for this scene, estimated bylabeling the sentences in Figure 1b.
The correspondence between a sentence in 1b and its labeled logicalexpression in 1c is indicated by the number in parentheses.
Section 5.1 presents a discussion of the space ofpossible logical forms.set of possible meanings, while maintaining compu-tational tractability.To represent meaning we build on previous ap-proaches that use lambda calculus (Carpenter, 1997;Zettlemoyer and Collins, 2005; Artzi and Zettle-moyer, 2013b).
We extend these techniques by mod-eling the types of plurality and coordination that areprominent in expressions which refer to sets.We also present a new corpus for the task of re-ferring expression generation.1 While most previ-ous REG data focused on naming single objects,1The corpus was collected using Amazon Mechanical Turkand is available on the authors?
websites.to the best of our knowledge, this is the first cor-pus with sufficient coverage for learning to namesets of objects.
Experiments demonstrate highly ac-curate learned models, able to generate over 87%of the expressions people used.
On the previouslystudied special case of single object reference, weachieve state-of-the-art performance, with over 35%relative error reduction over previous state of theart (Mitchell et al 2013).2 Related WorkReferring expression generation has been exten-sively studied in the natural language generation1915community, dating as far back as SHRDLU (Wino-grad, 1972).
Most work has built on variations ofthe Incremental Algorithm (Dale and Reiter, 1995),a deterministic algorithm for naming single ob-jects that constructs conjunctive logical expressions.REG systems are used in generation pipelines (Daleand Reiter, 2000) and are also commonly designedto be cognitively plausible, for example by followingGricean maxims (Grice, 1975).
Krahmer and vanDeemter (2012) and van Deemter et al(2012a) sur-vey recent literature on REG.Different approaches have been proposed for gen-erating referring expressions for sets of objects.Van Deemter (2002) extended the Incremental Al-gorithm to allow disjunction and negation, enablingreference to sets.
Further work attempted to re-solve the unnaturally long expressions which couldbe generated by this approach (Gardent, 2002; Ho-racek, 2004; Gatt and van Deemter, 2007).
Later, de-scription logic was used to name sets (Areces et al2008; Ren et al 2010).
All of these algorithms aremanually engineered and deterministic.In practice, human utterances are surprisinglyvaried, loosely following the Gricean ideals (vanDeemter et al 2012b).
Much recent work in REGhas identified the importance of modeling the vari-ation observed in human-generated referring ex-pressions (Viethen and Dale, 2010; Viethen et al2013; van Deemter et al 2012b; Mitchell et al2013), and some approaches have applied machine-learning techniques to single-object references (Vi-ethen and Dale, 2010; Mitchell et al 2011a,b).
Re-cently, Mitchell et al(2013) introduced a proba-bilistic approach for conjunctive descriptions of sin-gle objects, which will provide a comparison base-line for experiments in Section 8.
To the best ofour knowledge, this paper presents the first learnedprobabilistic model for referring expressions defin-ing sets, and is the first effort to treat REG as a den-sity estimation problem.REG is related to content selection, whichhas been studied for generating text fromdatabases (Konstas and Lapata, 2012), eventstreams (Chen et al 2010), images (Berg et al2012; Zitnick and Parikh, 2013), and text (Barzilayand Lapata, 2005; Carenini et al 2006).
However,most approaches to this problem output bags of con-cepts, while we construct full logical expressions,allowing our approach to capture complex relationsbetween attributes.Finally, our approach to modeling meaning us-ing lambda calculus is related to a number of ap-proaches that used similar logical representationin various domains, including database query in-terfaces (Zelle and Mooney, 1996; Zettlemoyerand Collins, 2005, 2007), natural language instruc-tions (Chen and Mooney, 2011; Matuszek et al2012b; Kim and Mooney, 2012; Artzi and Zettle-moyer, 2013b), event streams (Liang et al 2009;Chen et al 2010), and visual descriptions (Ma-tuszek et al 2012a; Krishnamurthy and Kollar,2013).
Our use of logical forms follows this line ofwork, while extending it to handle plurality and co-ordination, as described in Section 4.1.
In addition,lambda calculus was shown to enable effective nat-ural language generation from logical forms (Whiteand Rajkumar, 2009; Lu and Ng, 2011).
If com-bined with these approaches, our approach wouldallow the creation of a complete REG pipeline.3 Technical OverviewTask Let Z be a set of logical expressions that se-lect a target set of objects G in a world state S, asformally defined in Section 5.1.
We aim to learn aprobability distribution P (z | S,G), with z ?
Z .For example, in the referring expressions domainwe work with, the state S = {o1, .
.
.
, on} is a setof n objects oi.
Each oi has three properties: color,shape and type.
The target setG ?
S is the subset ofobjects to be described.
Figure 1a shows an examplescene.
The world state S includes the 11 objects inthe image, where each object is assigned color (yel-low, green .
.
.
), shape (cube, cylinder .
.
. )
and type(broccoli, apple .
.
.
).
The target set G contains thecircled objects.
Our task is to predict a distributionwhich closely matches the empirical distribution inFigure 1c.Model and Inference We model P (z|S,G) as aglobally normalized log-linear model, using featuresof the logical form z, and its execution with respectto S and G. Since enumerating all z ?
Z is in-tractable, we develop an approximate inference al-gorithm which constructs a high quality candidateset, using a learned pruning model.
Section 5.2 de-scribes the globally scored log-linear model.
Sec-1916tion 5.3 presents a detailed description of the infer-ence procedure.Learning We use stochastic gradient descent tolearn both the global scoring model and the explicitpruning model, as described in section 6.
Our dataconsists of human-generated referring expressions,gathered from Amazon Mechanical Turk.
Thesesentences are automatically labelled with logicalforms with a learned semantic parser, providing astand-in for manually labeled data (see Section 7).Evaluation Our goal is to output a distributionthat closely matches the distribution that would beproduced by humans.
We therefore evaluate ourmodel with gold standard labeling of crowd-sourcedreferring expressions, which are treated as samplesfrom the implicit distribution we are trying to model.The data and evaluation procedure are described inSection 7.
The results are presented in Section 8.4 Modeling Referring Expressions4.1 Semantic ModelingOur semantic modeling approach uses simply-typedlambda-calculus following previous work (Carpen-ter, 1997; Zettlemoyer and Collins, 2005; Artzi andZettlemoyer, 2013b), extending it in one importantway: we treat sets of objects as a primitive type,rather than individuals.
This allows us to model plu-rality, cardinality, and coordination for the languageobserved in our data, and is further motivated by re-cent cognitive science evidence that sets and theirproperties are represented as single units in humancognition (Scontras et al 2012).Plurals Traditionally, noun phrases are identifiedwith the entity-type e and pick out individual ob-jects (Carpenter, 1997).
This makes it difficult tointerpret plural noun-phrases which pick out a set ofobjects, like ?The red cubes?.
Previous approacheswould map this sentence to the same logical expres-sion as the singular ?The red cube?, ignoring the se-mantic distinction encoded by the plural.Instead, we define the primitive entity e to rangeover sets of objects.
?e, t?-type expressions aretherefore functions from sets to a truth-value.
Theseare used in two ways, modeling both distributive andcollective predicates (cf.
Stone, 2000):1.
Distributive predicates are ?e, t?-type expres-sions which will return true if every individualin the set has a given property.
For example, theexpression ?x.red(x) will be true for all setswhich contain only objects for which the valuered is true.2.
Collective predicates are ?e, t?-type expres-sions which indicate a property of the set it-self.
For example, in the phrase ?the twocubes?, ?two?
corresponds to the expression?x.cardinality2(x) which will return trueonly for sets which have exactly two members.We define semantic plurality in terms of two spe-cial collective predicates: sg for singular and plufor plural.
For examples, ?cube?
is interpreted as?x.cube(x) ?
sg(x), whereas ?cubes?
is interpretedas ?x.cube(x) ?
plu(x).
The sg predicate returnstrue only for singleton sets.
The plu predicate re-turns true for sets that contain two or more objects.We also model three kinds of determiners,functional-type ?
?e, t?, e?-type expressions whichselect a single set from the power-set representedby their ?e, t?-type argument.
The definite deter-miner ?the?
is modeled with the predicate ?, whichresolves to the maximal set amongst those licensedby its argument.
The determinerEvery only accepts?e, t?-type arguments that define singleton sets (i.e.the argument includes the sg predicate) and returnsa set containing the union of these singletons.
Forexample, although ?red cube?
is a singular expres-sion, ?Every red cube?
refers to a set.
Finally, theindefinite determiner ?a?
is modeled with the logicalconstant A, which picks a singleton set by implic-itly introducing an existential quantifier (Artzi andZettlemoyer, 2013b).2Coordination Two types of coordination areprominent in set descriptions.
The first is attributecoordination, which is typically modeled with theboolean operators: ?
for conjunction and ?
for dis-junction.
For example, the phrase ?the red cubesand green rectangle?
involves a disjunction that joinstwo conjunctive expressions, both within the scopeof the definite determiner: ?(?x.
(red(x)?cube(x)?plu(x)) ?
(green(x) ?
rectangle(x) ?
sg(x))).2This treatment of the indefinite determiner is related to gen-eralized skolem terms as described by Steedman (2011).1917The second kind of coordination, a new additionof this work, occurs when two sets are coordinated.This can either be set union (?)
as in the phrase ?Thecubes and the rectangle?
(?(?x.cube(x)?
plu(x))??
(?x.rectangle(x) ?
sg(x)))), or set difference(\) as in the phrase ?All blocks except the greencube?
: (?(?x.object(x)?plu(x))\?
(?x.green(x)?cube(x) ?
sg(x))).4.2 Visual DomainObjects in our scenes are labeled with attribute val-ues for four attribute types: color (7 values, suchas red, green), shape (9 values, such as cube,sphere), type (16 values, such as broccoli, apple)and a special object property, which is true forall objects.
The special object property capturesthe role of descriptions that are true for all objects,such as ?toy?
or ?item?.
Each of these 33 attributevalues corresponds to an ?e, t?-type predicate.5 Model and InferenceIn this section, we describe our approach to mod-eling the probability P (z | S,G) of a logical formz ?
Z that names a set of objects G in a world S, asdefined in Section 3.
We first define Z (Section 5.1),and then present the distribution (Section 5.2) and anapproximate inference approach that makes use of alearned pruning model (Section 5.3).5.1 Space of Possible MeaningsThe set Z defines logical expressions that we willconsider for picking the target set G in state S. Ingeneral, we can construct infinitely many such ex-pressions.
For example, every z ?
Z can be triv-ially extended to form a new candidate z?
for Zby adding a true clause to any conjunct it contains.However, the vast majority of such expressions areoverly complex and redundant, and would never beused in practice as a referring expression.To avoid this explosion, we limit the type andcomplexity of the logical expressions that are in-cluded in Z .
We consider only e-type expressions,since they name sets, and furthermore only includeexpressions that name the desired target set G.3 We3We do not attempt to model underspecified or otherwiseincorrect expressions, although our model could handle this byconsidering all e-type expressions.?
p : ?
?e, t?, e?, e1 : ?e, t?
?
p(e1) : ee.g.p = ?
: ?
?e, t?, e?e1 = ?x.cube(x) ?
sg(x) : ?e, t??
(?x.cube(x) ?
sg(x)) : e?
p : ?t, t?, e1 : ?e, t?
?
?x.p(e1(x)) : ?e, t?e.g.p = ?
: ?t, t?e1 = ?x.red(x) : ?e, t??x.?
(red(x)) : ?e, t??
p : ?e, ?e, t?
?, e1 : e?
?x.
(p(x))(e1)e.g.p = equal : ?e, ?e, t?
?e1 = A(?y.cube(y) ?
sg(y)) : e?x.equal(x,A(?y.cube(y) ?
sg(y)))?
p : ?e, ?e, e?
?, e1 : e, e2 : e?
(p(e1))(e2) : ee.g.p = \ : ?e, ?e, e?
?e1 = ?
(?x.cube(x) ?
plu(x)) : ee2 = Every(?x.object(x) ?
sg(x)) : eEvery(?x.object(x) ?
sg(x)) \?
(?x.cube(x) ?
plu(x)) : e?
p : ?t, ?t, t?
?, e1 : ?e, t?, e2?e, t?
??x.
(p(e1(x)))(e2(x)) : ?e, t?e.g.p = ?
: ?t, ?t, t?
?e1 = ?x.red(x) : ?e, t?e2 = ?x.cube(x) : ?e, t?
?x.red(x) ?
cube(x) : eFigure 2: The five rules used during generation.Each rule is a template which takes a predicate p : tof type t and one or two arguments ei : ti, with typeti.
The output is the logical expression after the ar-row?, constructed using the inputs as shown.also limit the overall complexity of each z ?
Z , tocontain not more than M logical constants.To achieve these constraints, we define an induc-tive procedure for enumerating Z , in order of com-plexity.
We first define Aj to be the set of all e- and?e, t?-type expressions that contain exactly j logi-cal constants.
Figure 2 presents five rules that can beused to constructAj by induction, for j = 1, .
.
.
,?,by repeatedly adding new constants to expressionsin Aj?
for j?
< j.
Intuitively, Aj is the set of allcomplexity j expressions that can be used as sub-expressions for higher complexity entires in our finalset Z .
Next, we define Zj to be the e-type expres-sions inAj that name the correct setG.
And, finally,Z = ?j=1...MZj of all correct expressions up to amaximum complexity of M .This construction allows for a finite Z with good1918empirical coverage, as we will see in the experi-ments in Section 8.
However, Z is still prohibitivelylarge for the maximum complexities used in practise(for example M = 20).
Section 5.3 presents an ap-proach for learning models to prune Z , while stillachieving good empirical coverage.5.2 Global ModelGiven a finite Z , we can now define our desiredglobally normalized log-linear model, conditionedon the state S and set of target objects G:PG(z | S,G; ?)
=1Ce???
(z,S,G) (1)where ?
?
Rn is a parameter vector, ?
(z, S,G) ?Rn is a feature function and C is the normalizationconstant.
Section 5.4 defines the features we use.5.3 Pruning ZAs motivated in Section 5.1, the key challenge forour global model in Equation 1 is that the set Z istoo large to be explicitly enumerated.
Instead, wedesigned an approach for learning to approximate Zwith a subset of the highly likely entries, and use thissubset as a proxy for Z during inference.More specifically, we define a binary distributionthat is used to classify whether each a ?
Aj is likelyto be used as a sub-expression in Z , and prune eachAj to keep only the top k most likely entries.
Thisdistribution is a logistic regression model:Pj(a | S,G;pij) =epij ??
(a,S,G)1 + epij ??
(a,S,G)(2)with features ?
(a, S,G) ?
Rn and parameters pij ?Rn.
This distribution uses the same features as theglobal model presented in Equation 1, which we de-scribe in Section 5.4.Together, the pruning model and global model de-fine the distribution P?
(z | G,S; ?,?)
over z ?
Z ,conditioned on the world state S and target set G,and parameterized by both the parameters ?
of theglobal model and the parameters ?
= {pi1, .
.
.
, piM}of the pruning models.5.4 FeaturesWe use three kinds of features: logical expressionstructure features, situated features and a complexityfeature.
All features but the complexity feature areshared between the global model in Equation 1 andthe pruning model in Equation 2.
In order to avoidoverly specific features, the attribute value predi-cates in the logical expressions are replaced withtheir attribute type (ie.
red ?
color).
In addition,the special constants sg and plu are ignored whencomputing features.In the following description of our features, allexamples are computed for the logical expression?
(?x.red(x) ?
object(x) ?
plu(x)), with respect tothe scene and target set in Figure 1a.Structure Features We use binary features thataccount for the presence of certain structures in thelogical form, allowing the model to learn commonusage patterns.?
Head Predicate - indicator for use of a logi-cal constant as a head predicate in every sub-expression of the expression.
A head predicateis the top-level operator of an expression.
Forexample, the head predicate of the expression?
?x.red(x) ?
object(x)?
is ???
and the headof ?x.red(x) is red.
For our running example,the head features are ?, ?, color, object.?
Head-Predicate Bigrams and Trigrams -head-predicate bigrams are defined to be thehead predicate of a logical form, and the headpredicate of one of its children.
Trigramsare similarly defined.
E.g.
bigrams: [?,?
],[?, color], [?, object], and trigrams: [?,?, red],[?,?, object].?
Conjunction Duplicate - this feature fires if aconjunctive expression contains duplicate sub-expressions amongst its children.?
Coordination Children - this feature set indi-cates the presence of a coordination subexpres-sion (?, ?, ?
or \) and the head expressionsof all pairs and triples of its child expressions.E.g.
[?
; red, object].Situated Features These features take into ac-count the evaluation of the logical form z with re-spect to the state S and target set G. They capturecommon patterns between the target set G and theobject groups named by subexpressions of z.1919?
Head Predicate and Coverage - this fea-ture set indicates the head predicate of ev-ery sub-expression of the logical form, com-bined with a comparison between the execu-tion of the sub-expression and the target setG.
The possible values for this comparison(which we call the ?coverage?
of the expres-sion with respect to G) are: EQUAL, SUBSET(SUB), SUPERSET (SPR), DISJOINT, ALL,EMPTY and OTHER.
E.g.
[?,SUB], [?,SUB],[color,SUB], [object,ALL]?
Coordination Child Coverage - this featureset indicates the head-predicate of a coordina-tion subexpression, combined with the cover-age of all pairs and triples of its child expres-sions.
E.g.
[?;SUB,ALL].?
Coordination Child Relative Coverage - thisfeature set indicates, for every pair of child sub-expressions of coordination expressions in thelogical form, the coverage of the child sub-expressions relative to each other.
The pos-sible relative coverage values are: SUB-SPR,DISJOINT, OTHER.
E.g.
[?
;SUB-SPR].Complexity Features We use a single real-numbered feature to account for the complexity ofthe logical form.
We define the complexity of a log-ical form to be the number of logical constants used.Our running example has a complexity of 4.
Thisfeature is only used in the global model, since thepruning model always considers logical expressionsof fixed complexity.6 LearningFigure 3 presents the complete learning algorithm.The algorithm is online, using stochastic gradi-ent descent updates for both the globally scoreddensity estimation model and the learned pruningmodel.
The algorithm assumes a dataset of the form{(Zi, Si, Gi) : i = 1 .
.
.
n} where each examplescene includes a list of logical expressions Zi, aworld state Si, and a target set of objects, Gi, whichwill be identified by the resulting logical expres-sions.
The output is learned parameters for both theglobally scored density estimation model ?, and forthe learned pruning models ?.Inputs: Training set {(Zi, Si, Gi) : i = 1 .
.
.
n}, where Zi isa list of logical forms, Si is a world state, and Gi is a targetset of objects.
Number of iterations T .
Learning rate ?0.Decay parameter c. Complexity threshold M , as describedin Section 5.3.Definitions: Let P?
(z | Gi, Si; ?,?)
be the predicted globalprobability from Equation 1.
Let P?j(z | Gi, Si;pij) be thepredicted pruning probability from Equation 2.
Let A?j bethe set of all complexity-M logical expressions, after prun-ing (see Section 5.1).
Let SUB(j, z) be all complexity-jsub-expressions of logical expression z.
Let Qi(z | Si, Gi)be the empirical probability over z ?
Z , estimated fromZi.
Finally, let ?i(z) be a shorthand for the feature function?
(z, Si, Gi) as defined in Section 5.4.Algorithm:Initialize ?
?
~0, pij ?
~0 for j = 1 .
.
.MFor t = 1 .
.
.
T, i = 1 .
.
.
n:Step 1: (Update Global Model)a. Compute the stochastic gradient:??
?
EQi(z|Si,Gi)[?i(z)]?
EP?
(z|Gi,Si;?,?)[?i(z)]b.
Update the parameters:?
?
?01+c??
where ?
= i+ t?
n?
?
?
+ ??
?Step 2: (Update Pruning Model)For j = 1 .
.
.Ma.
Construct a set of positive and negative examples:D+ ?
?z?ZiSUB(j, z).D?
?
A?j \ D+b.
Compute mini-batch stochastic gradient, normalizingfor data skew:?pij ?1|D+|?z?D+(1?
Pj(z | Si, Gi;pij))?i(z)?
1|D?|?z?D?
Pj(z | Si, G;pij)?i(z)c. Update complexity-j pruning parameters:pij ?
pij + ?
?pijOutput: ?
and ?
= [pi1, .
.
.
, piM ]Figure 3: The learning algorithm.6.1 Global Model UpdatesThe parameters ?
of the globally scored density es-timation model are trained to maximize the log-likelihood of the data:Oi = log?z?ZiPG(z | Si, Gi) (3)Taking the derivative of this objective with re-spect to ?
yields the gradient in Step 1a of Fig-ure 3.
The marginals, EP?
(z|Gi,Si;?,?
)(?i(z)), arecomputed over the approximate finite subset of Zconstructed with the inference procedure describedin Section 5.3.19206.2 Pruning Model UpdatesTo update each of the M pruning models, we firstconstruct a set of positive and negative examples(Step 2a).
The positive examples, D+, include thosesub-expressions which should be in the beam - theseare all complexity j sub-expressions of logical ex-pressions in Zi.
The negative examples, D?, in-clude all complexity-j expressions constructed dur-ing beam search, minus those which are in D+.
Thegradient (Set 2b) is a binary mini-batch gradient,normalized to correct for data skew.7 Experimental SetupData Collection Our dataset consists of 118 im-ages, taken with a Microsoft Kinect camera.
Theseare the same images used by Matuszek et al(2012a), but we create multiple prompts for each im-age by circling different objects, giving 269 scenesin total.
These scenes were shown to workers onAmazon Mechanical Turk4 who were asked to imag-ine giving instructions to a robot and complete thesentence ?Please pick up ?
in reference to thecircled objects.
Twenty referring expressions werecollected for each scene, a total of 5380 expressions.From this data, 43 scenes (860 expressions) wereheld-out for use in a test set.
Of the remainingscenes, the sentences of 30 were labeled with log-ical forms.
10 of these scenes (200 expressions) areused as a labeled initialization set, and 20 are usedas a development test set (400 expressions).
A smallnumber of expressions (?5%) from the labeled ini-tial set were discarded, either because they did notcorrectly name the target set, or because they usedvery rare attributes (such as texture, or location) toname the target objects.Surrogate Labeling To avoid hand labeling thelarge majority of the scenes, we label the datawith a learned semantic parser (Zettlemoyer andCollins, 2005).
We created a hand-made lexiconfor the entire training set, which greatly simplifiesthe learning problem, and learned the parametersof the parsing moder on the 10-scene initializationset.
The weights were then further tuned usingsemi-supervised techniques (Artzi and Zettlemoyer,2011, 2013b) on the data to be labeled.
Testing on4http://www.mturk.comthe development set shows that this parser achievesroughly 95% precision and 70% recall.Using this parser, we label the sentences in ourtraining set.
We only use scenes where at least 15sentences were successfully parsed.
This gives atraining set of 141 scenes (2587 expressions).
Com-bining the automatically labeled training set with thehand-labelled initialization, development and held-out data, our labelled corpus totals 3938 labeled ex-pressions.
By contrast, the popular TUNA furnituresub corpus (Gatt et al 2007) contains 856 descrip-tions of 20 scenes, and although some of these referto sets, these sets contain two objects at most.Framework Our experiments were implementedusing the University of Washington Semantic Pars-ing Framework (Artzi and Zettlemoyer, 2013a).Hyperparameters Our inference procedure re-quires two hyperparameters: M , the maximum com-plexity threshold, and k, the beam size.
In practice,we set these to the highest possible values whichstill allow for training to complete in a reasonableamount of time (under 12 hours).
M is set to 20,which is sufficient to cover 99.5% of the observedexpressions.
The beam-size k is 100 for the firstthree complexity levels, and 50 thereafter.For learning, we use the following hyperparam-eters, which were tuned on the development set:learning rate ?0 = .25, decay rate c = .02, num-ber of epochs T = 10.Evaluation Metrics Evaluation metrics used inREG research have assumed a system that producesa single output.
Our goal is to achieve a distributionover logical forms that closely matches the distribu-tion observed from human subjects.
Therefore, wecompare our learned model to the labeled test datawith mean absolute error:MAE =12nn?i=1?z?Z|P (z | Si, Gi)?Q(z | Si, Gi)|where Q is the empirical distribution observed in thetraining data.
MAE measures the total probabilitymass which is assigned differently in the predicteddistribution than in the empirical distribution.
Weuse MAE as opposed to KL divergence or data like-lihood as both of these measures are uninformativewhen the support of the two distributions differ.1921?
MAE %dup %uniq Top1VOA 39.7 98.2 92.5 72.7GenX25.8 100 100 72.7(5.0) (0) (0) (0)Table 1: Single object referring expression gener-ation results.
Our approach (GenX) is comparedto the approach from Mitchell et al(2013) (VOA).Standard deviation over five shuffles of training setis reported in parentheses.This metric is quite strict; small differences in theestimated probabilities over a large number of logi-cal expressions can result in a large error, even if therelative ordering is quite similar.
Therefore, we re-port the percentage of observed logical expressionswhich the model produces, either giving credit mul-tiple times for duplicates (%dup) or counting eachunique logical expression in a scene once (%uniq).Put another way, %dup counts logical expression to-kens, whereas %uniq counts types.
We also reportthe proportion of scenes where the most likely log-ical expression according to the model matched themost common one in the data (Top1).Single Object Baseline In order to compare ourmethod against the state of the art for generatingreferring expressions for single objects, we use thesubset of our corpus where the target set is a sin-gle object.
This sub-corpus consists of 44 scenes fortraining and 11 held out for testing.For comparison we re-implemented the proba-bilistic Visual Objects Algorithm (VOA) of Mitchellet al(2013).
We refer the readers to the originalpaper for details of the approach.
The parametersof the model were tuned on the training data: theprior likelihood estimates for each of the four at-tribute types (?att) were estimated as the relativefrequency of each attribute in the data.
We pick theordering of attributes and the length penalty, ?, fromthe cross-product of all possible 4!
orderings and allintegers on the range of [1,10], choosing the settingwhich results in the lowest average absolute error(AAE) on the training set.
This process resultedin the following parameter settings: ?color = .916,?shape = .586, ?type = .094, ?object = .506, APordering = [type, shape, object, color], ?
= 4.
In-ference was done using 10,000 samples per scene.?
MAE %dup %uniq Top1Full GenX54.3 87.4 72.9 52.6(4.5) (0.6) (1.1) (8.3)NoPrune71.8 42.2 16.1 40.0(2.5) (2.7) (1.7) (5.0)NoCOV87.0 26.0 11.2 14.9(6.7) (3.7) (2.1) (9.7)NoSTRUC60.2 79.6 61.9 44.6(1.7) (0.3) (0.5) (4.5)HeadExpOnly88.8 21.9 9.3 14.0(6.4) (8.6) (3.5) (7.9)Table 2: Results on the complete corpus for thecomplete system (Full GenX), ablating the pruningmodel (NoPrune) and the different features: withoutcoverage features (NoCOV), without structure fea-tures (NoSTRUC) and using only the logical expres-sion HeadExp features (HeadExpOnly).
Standarddeviation over five runs is shown in parentheses.8 ResultsWe report results on both the single-object subset ofour data and the full dataset.
Since our approach isonline, and therefore sensitive to data ordering, weaverage results over 5 different runs with randomlyordered data, and report variance.Single Objects Table 1 shows the different metricsfor generating referring expression for single objectsonly.
Our approach outperforms VOA (Mitchellet al 2013) on all metrics, including an average ofapproximately 35% relative reduction in MAE.
Inaddition, unlike VOA, our system (GenX) producesevery logical expression used to refer to single ob-jects in our dataset, including a small number whichuse negation and equality.Object Sets Table 2 lists results on the full dataset.Our learned pruning approach produces an average72.9% of the unique logical expressions used presentin our dataset ?
over 87% when these counts areweighted by their frequency.
The globally scoredmodel achieves a mean absolute error of 54.3, andcorrectly assigns the highest probability to the mostlikely expression over 52% of the time.Also shown in Table 2 are results obtained whenelements of our approach are ablated.
Using theglobal model for pruning instead of an explicitlytrained model causes a large drop in performance,demonstrating that our global model is inappropri-1922Q P?
z.750 .320 ?
(?x.object(x) ?
(yellow(x) ?
red(x))).114 ?
(?x.lego(x)) ?
?
(?x.red(x) ?
apple(x)).114 ?
(?x.yellow(x) ?
lego(x))) ?
?
(?x.apple(x)).044 ?
(?x.lego(x) ?
(red(x) ?
apple(x))).044 ?(?x.
(yellow(x) ?
lego(x)) ?
apple(x)).036 ?
(?x.lego(x)) ?
?
(?x.red(x) ?
sphere(x)).026 ?
(?x.red(x) ?
lego(x)) ?
?
(?x.red(x) ?
sphere(x)).050 .021 ?(?x.
(lego(x) ?
yellow(x)) ?
(red(x) ?
apple(x))).017 ?(?x.
(lego(x) ?
yellow(x)) ?
(red(x) ?
sphere(x))).014 ?
(?x.yellow(x) ?
lego(x)) ?
?
(?x.red(x) ?
sphere(x)).100 .010 ?
(?x.yellow(x) ?
object(x)) ?
?
(?x.apple(x)).050 .007 ?
(?x.yellow(x) ?
object(x)) ?
?
(?x.red(x) ?
sphere(x)).050 .005 ?
(?x.yellow(x) ?
object(x)) ?
?
(?x.red(x) ?
object(x))(a) (b)Figure 4: Example output of our system for the scene on the right.
We show the top 10 expressions (z) fromthe predicted distribution (P? )
compared to the empirical distribution estimated from our labeled data (Q).The bottom section shows the predicted probability of the three expressions which were not in the top 10 ofthe predicted distribution.
Although the mean absolute error (MAE) of P?
and Q is 63.8, P?
covers all of theentries in Q in the correct relative order and also fills in many other plausible candidates.ate for pruning.
We also ablate subsets of our fea-tures, demonstrating that the coverage and structuralfeatures are both crucial for performance.Qualitatively, we found the learned distributionswere often higher quality than the seemingly highmean absolute error would imply.
Figure 4 showsan example output where the absolute error of thepredicted distribution was 63.8.
Much of the errorcan be attributed to probability mass assigned to log-ical expressions which, although not observed in ourtest data, are reasonable referring expressions.
Thismight be due to the fact that our estimate of the em-pirical distribution comes from a fairly small sample(20), or other factors which we do not model thatmake these expressions less likely.9 ConclusionIn this paper, we modeled REG as a density-estimation problem.
We demonstrated that we canlearn to produce distributions over logical referringexpressions using a globally normalized model.
Keyto the approach was the use of a learned pruningmodel to define the space of logical expression thatare explicitly enumerated during inference.
Exper-iments demonstrate state-of-the-art performance onsingle object reference and the first results for learn-ing to name sets of objects, correctly recovering over87% of the observed logical forms.This approach suggests several directions for fu-ture work.
Lambda-calculus meaning represen-tations can be designed for many semantic phe-nomena, such as spatial relations, superlatives, andgraded properties, that are not common in our data.Collecting new datasets would allow us to study theextent to which the approach would scale to domainswith such phenomena.Although the focus of this paper is on REG, theapproach is also applicable to learning distributionsover logical meaning representations for many othertasks.
Such learned models could provide a rangeof possible inputs for systems that map logical ex-pressions to sentences (White and Rajkumar, 2009;Lu and Ng, 2011), and could also provide a valuableprior on the logical forms constructed by semanticparsers in grounded settings (Artzi and Zettlemoyer,2013b; Matuszek et al 2012a).AcknowledgementsThis research was supported in part by the In-tel Science and Technology Center for PervasiveComputing, by DARPA under the DEFT programthrough the AFRL (FA8750-13-2-0019) and theCSSG (N11AP20020), the ARO (W911NF-12-1-0197), and the NSF (IIS-1115966).
The authorswish to thank Margaret Mitchell, Mark Yatskar,Anthony Fader, Kenton Lee, Eunsol Choi, GabrielSchubiner, Leila Zilles, Adrienne Wang, and theanonymous reviewers for their helpful comments.1923ReferencesAreces, C., Koller, A., and Striegnitz, K. (2008).Referring expressions as formulas of descriptionlogic.
In Proceedings of the International Natu-ral Language Generation Conference.Artzi, Y. and Zettlemoyer, L. (2011).
Bootstrappingsemantic parsers from conversations.
In Proceed-ings of the Conference on Empirical Methods inNatural Language Processing.Artzi, Y. and Zettlemoyer, L. (2013a).
UW SPF:The University of Washington Semantic ParsingFramework.Artzi, Y. and Zettlemoyer, L. (2013b).
Weakly su-pervised learning of semantic parsers for mappinginstructions to actions.
Transactions of the As-sociation for Computational Linguistics, 1(1):49?62.Barzilay, R. and Lapata, M. (2005).
Collectivecontent selection for concept-to-text generation.In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing.Berg, A. C., Berg, T. L., Daume, H., Dodge, J.,Goyal, A., Han, X., Mensch, A., Mitchell, M.,Sood, A., Stratos, K., et al(2012).
Under-standing and predicting importance in images.
InIEEE Conference on Computer Vision and PatternRecognition.Carenini, G., Ng, R. T., and Pauls, A.
(2006).
Multi-document summarization of evaluative text.
InProceedings of the Conference of the EuropeanChapter of the Association for ComputationalLinguistics.Carpenter, B.
(1997).
Type-Logical Semantics.
TheMIT Press.Chen, D., Kim, J., and Mooney, R. (2010).
Train-ing a multilingual sportscaster: using perceptualcontext to learn language.
Journal of Artificial In-telligence Research, 37(1):397?436.Chen, D. and Mooney, R. (2011).
Learning to inter-pret natural language navigation instructions fromobservations.
In Proceedings of the National Con-ference on Artificial Intelligence.Dale, R. and Reiter, E. (1995).
Computational in-terpretations of the gricean maxims in the gener-ation of referring expressions.
Cognitive Science,19:233?264.Dale, R. and Reiter, E. (2000).
Building natural lan-guage generation systems.
Cambridge UniversityPress.Gardent, C. (2002).
Generating minimal definite de-scriptions.
In Proceedings of the Annual Meetingof the Association for Computational Linguistics.Gatt, A. and van Deemter, K. (2007).
Incrementalgeneration of plural descriptions: Similarity andpartitioning.
In Proceedings of the Conference onEmpirical Methods in Natural Language Process-ing.Gatt, A., Van Der Sluis, I., and Van Deemter, K.(2007).
Evaluating algorithms for the generationof referring expressions using a balanced corpus.In Proceedings of the European Workshop on Nat-ural Language Generation.Grice, H. P. (1975).
Logic and conversation.
1975,pages 41?58.Horacek, H. (2004).
On referring to sets of objectsnaturally.
In Natural Language Generation, pages70?79.
Springer.Kim, J. and Mooney, R. J.
(2012).
UnsupervisedPCFG induction for grounded language learningwith highly ambiguous supervision.
In Proceed-ings of the Conference on Empirical Methods inNatural Language Processing.Konstas, I. and Lapata, M. (2012).
Unsupervisedconcept-to-text generation with hypergraphs.
InProceedings of the Conference of the North Amer-ican Chapter of the Association for Computa-tional Linguistics.Krahmer, E. and van Deemter, K. (2012).
Computa-tional generation of referring expressions: A sur-vey.
Computational Linguistics, 38(1):173?218.Krishnamurthy, J. and Kollar, T. (2013).
Jointlylearning to parse and perceive: Connecting nat-ural language to the physical world.
Transactionsof the Association for Computational Linguistics,1(2):193?206.Liang, P., Jordan, M., and Klein, D. (2009).
Learn-ing semantic correspondences with less supervi-sion.
In Proceedings of the Annual Meeting of theAssociation for Computational Linguistics.1924Lu, W. and Ng, H. T. (2011).
A probabilis-tic forest-to-string model for language generationfrom typed lambda calculus expressions.
In Pro-ceedings of the Conference on Empirical Methodsin Natural Language Processing.Matuszek, C., FitzGerald, N., Zettlemoyer, L., Bo,L., and Fox, D. (2012a).
A joint model of lan-guage and perception for grounded attribute learn-ing.
Proceedings of the International Conferenceon Machine Learning.Matuszek, C., Herbst, E., Zettlemoyer, L. S., andFox, D. (2012b).
Learning to parse natural lan-guage commands to a robot control system.
InProceedings of the International Symposium onExperimental Robotics.Mitchell, M., van Deemter, K., and Reiter, E.(2011a).
Applying machine learning to the choiceof size modifiers.
In Proceedings of the PRE-CogSci Workshop.Mitchell, M., Van Deemter, K., and Reiter, E.(2011b).
Two approaches for generating sizemodifiers.
In Proceedings of the European Work-shop on Natural Language Generation.Mitchell, M., van Deemter, K., and Reiter, E. (2013).Generating expressions that refer to visible ob-jects.
In Proceedings of Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics.Ren, Y., Van Deemter, K., and Pan, J.
Z.
(2010).Charting the potential of description logic for thegeneration of referring expressions.
In Proceed-ings of the International Natural Language Gen-eration Conference.Scontras, G., Graff, P., and Goodman, N. D. (2012).Comparing pluralities.
Cognition, 123(1):190?197.Steedman, M. (2011).
Taking Scope.
The MIT Press.Stone, M. (2000).
On identifying sets.
In Proceed-ings of the International Conference on NaturalLanguage Generation.van Deemter, K. (2002).
Generating referring ex-pressions: Boolean extensions of the incrementalalgorithm.
Computational Linguistics, 28:37?52.van Deemter, K., Gatt, A., Sluis, I. v. d., and Power,R.
(2012a).
Generation of referring expressions:Assessing the incremental algorithm.
CognitiveScience, 36(5):799?836.van Deemter, K., Gatt, A., van Gompel, R. P., andKrahmer, E. (2012b).
Toward a computationalpsycholinguistics of reference production.
Topicsin Cognitive Science, 4(2):166?183.Viethen, J. and Dale, R. (2010).
Speaker-dependentvariation in content selection for referring ex-pression generation.
In Proceedings of the Aus-tralasian Language Technology Workshop.Viethen, J., Mitchell, M., and Krahmer, E. (2013).Graphs and spatial relations in the generation ofreferring expressions.
In Proceedings of the Eu-ropean Workshop on Natural Language Genera-tion.White, M. and Rajkumar, R. (2009).
Perceptronreranking for ccg realization.
In Proceedings ofthe Conference on Empirical Methods in NaturalLanguage Processing.Winograd, T. (1972).
Understanding natural lan-guage.
Cognitive Psychology, 3(1):1?191.Zelle, J. and Mooney, R. (1996).
Learning to parsedatabase queries using inductive logic program-ming.
In Proceedings of the National Conferenceon Artificial Intelligence.Zettlemoyer, L. and Collins, M. (2005).
Learningto map sentences to logical form: Structured clas-sification with probabilistic categorial grammars.In Proceedings of the Conference on Uncertaintyin Artificial Intelligence.Zettlemoyer, L. and Collins, M. (2007).
Onlinelearning of relaxed CCG grammars for parsing tological form.
In Proceedings of the Joint Confer-ence on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning.Zitnick, C. L. and Parikh, D. (2013).
Bringing se-mantics into focus using visual abstraction.
InIEEE Conference on Computer Vision and PatternRecognition.1925
