Proceedings of EACL '9995% Replicability for Manual Word Sense TaggingAdam KilgarriffITRI, University of Brighton, Lewes Road, Brighton UKemail: adam@itri.bton.ac.ukPeople have been writing programs for auto-matic Word Sense Disambiguation (WSD) forforty years now, yet the validity of the task hasremained in doubt.
At a first pass, the task issimply defined: a word like bank can mean 'riverbank' or 'money bank' and the task-is to deter-mine which of these applies in a context in whichthe word bank appears.
The problems arise be-cause most sense distinctions are not as clear asthe distinction between 'river bank' and 'moneyb.~nk', so it is not always straightforward for aperson to say what the correct answer is.
Thuswe do not always know what it would mean tosay that a computer program got the right an-swer.
The issue is discussed in detail by (Galeet al, 1992) who identify the problem as oneof identifying the 'upper bound' for the perfor-mance of a WSD program.
If people can onlyagree on the correct answer x% of the time, aclaim that a program achieves more than x% ac-curacy is hard to interpret, and x% is the upperbound for what the program can (meaningfully)achieve.There have been some discussions as to whatthis upper bound might be.
Gale et al re-view a psycholinguistic study (Jorgensen, 1990)in which the level of agreement averaged 68%.But an upper  bound of 68% is disastrous forthe enterprise, since it implies that the best aprogram could possibly do is still not remotelygood enough for any practical purpose.Even worse news comes from (Ng and Lee,1996), who re-tagged parts of the manuallytagged SEMCOR corpus (Fellbaum, 1998).
Thetaggings matched only 57% of the time.If these represent as high a level of inter-tagger agreement as one could ever expect,WSD is a doomed enterprise.
However, neitherstudy set out to identify an upper bound forWSD and it is far from ideal to use their resultsin this way.
In this paper we report on a studywhich did aim specifically at achieving as higha level of replicability as possible.The study took place within the context ofSENSEVAL, an evaluation exercise for WSDprograms.
1 It was, clearly, critical to the va-lidity of SENSEVAL as a whole to establish theintegrity of the 'gold standard' corpus againstwhich WSD programs would be judged.Measures taken to maximise the agreementlevel were:humans: whereas other tagging exerciseshad mostly used students, SENSEVALused professional lexicographersdictionary: the dictionary that providedthe sense inventory had lengthy entries,with substantial numbers of examplestask definition: in cases where none, ormore than one, of the senses applied, thelexicographer was encouraged to tag the in-stance as "unassignable" or with multipletags 2The exercise is chronicled athttp://vn~.itri.bton.ac.uk/events/senseval and in(Kilgarriff and Palmer, Forthcoming), where a fuller ac-count of all matters covered in the poster can be found.2The scoring algorithm simply treated "unassignable"as another tag.
(Less than 1% of instnaces were tagged"unassignable".)
Where there were multiple tags anda partial match between taggings, partial credit wasassigned.277Proceedings of EACL '99?
arbitration: first, two or three lexicogra-phers provided taggings.
Then, any in-stances where these taggings were not iden-tical were forwarded to a third lexicogra-pher for arbitration.The data for SENSEVAL comprised around200 corpus instances for each of 35 words, mak-ing a total of 8455 instances.
A scoring schemewas developed which assigned partial creditwhere more than one sense had been assignedto an instance.
This was developed primarilyfor scoring the WSD systems, but was also usedfor scoring the lexicographers' taggings.At the time of the SENSEVAL workshop,the tagging procedure (including arbitration)had been undertaken once for each corpus in-stance.
We scored lexicographers' initial pre-arbitration results against he post-arbitrationresults.
The scores ranged between 88% to100%, with just five out of 122 results for<lexicographer, word> pairs falling below 95%.To determine the replicability of the wholeprocess in a thoroughgoing way, we repeated itfor a sample of four of the words.
The wordswere selected to reflect the spread of difficulty:we took the word which had given rise to thelowest inter-tagger agreement in the previousround, (generous, 6 senses), the word that hadgiven rise to the highest, (sack, 12 senses), andtwo words from the middle of the range (onion,5, and shake, 36).
The 1057 corpus instancesfor the four words were tagged by two lexicog-raphers who had not seen the data before; thenon-identical taggings were forwarded to a thirdfor arbitration.
These taggings were then com-pared with the ones produced previously.The table shows, for each word, the number ofcorpus instances (Inst), the number of multiply-tagged instances in each of the two sets of tag-gings (A and B), and the level of agreement be-tween the two sets (Agr).There were 240 partial mismatches, with par-tial credit assigned, in contrast o just 7 com-plete mismatches.A instance on which the taggings disagreedwas:Give plants generous root space.Word Inst A B Agr%generousonionsackshake227 76 68 88.7214 10 11 98.9260 0 3 99.4356 35 49 95.1ALL 1057 121 131 95.5Sense 4 of generous is defined as simply "abun-dant; copious", and sense 5 as "(of a room orbuilding) large in size; spacious".
One tag-ging selected each.
In general, taggings failedto match where the definitions were vague andoverlapping, and where, as in sense 5, some partof a defintion matches a corpus instance well("spacious") but another part does not ("of aroom or building").ConclusionThe upper bound for WSD is around 95%, andGale et al's worries about the integrity of thetask can be laid to rest.
In order for manuallytagged test corpora to achieve 95% replicability,it is critical to take care over the task definition,to employ suitably qualified individuals, and todouble-tag and include an arbitration phase.ReferencesChristiane Fellbaum, editor.
1998.
WordNet: AnElectronic Lexical Database.
MIT Press, Cam-bridge, Mass.William Gale, Kenneth Church, and DavidYarowsky.
1992.
Estimating upper and lowerbounds on the performance of word-sense disam-biguation programs.
In Proceedings, 30th A CL,pages 249-156.Julia C. Jorgensen.
1990.
The psychological realityof word senses.
Journal of Psycholinguistic Re-search, 19(3):167-190.Adam Kilgarriff and Martha Palmer.
Forthcom-ing.
Guest editors, Special Issue on SENSE-VAL: Evaluating Word Sense Disambiguation Pro-grams.
Computers and the Humanities.Hwee Tou Ng and Hian Beng Lee.
1996.
Integrat-ing multiple knowledge sources to disambiguateword sense: An exemplar-based approach.
InA CL Proceedings, pages 40-47, Technical Univer-sity, Berlin, Santa Cruz, California.278
