Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 345?354,Sydney, July 2006. c?2006 Association for Computational LinguisticsSentiment Retrieval using Generative ModelsKoji EguchiNational Institute of InformaticsTokyo 101-8430, Japaneguchi@nii.ac.jpVictor LavrenkoDepartment of Computer ScienceUniversity of MassachusettsAmherst, MA 01003, USAlavrenko@cs.umass.eduAbstractRanking documents or sentences accord-ing to both topic and sentiment relevanceshould serve a critical function in helpingusers when topics and sentiment polari-ties of the targeted text are not explicitlygiven, as is often the case on the web.
Inthis paper, we propose several sentimentinformation retrieval models in the frame-work of probabilistic language models, as-suming that a user both inputs query termsexpressing a certain topic and also speci-fies a sentiment polarity of interest in somemanner.
We combine sentiment relevancemodels and topic relevance models withmodel parameters estimated from trainingdata, considering the topic dependence ofthe sentiment.
Our experiments prove thatour models are effective.1 IntroductionThe recent rapid expansion of access to informa-tion has significantly increased the demands on re-trieval or classification of sentiment informationfrom a large amount of textual data.
The field ofsentiment classification has recently received con-siderable attention, where the polarities of senti-ment, such as positive or negative, were identifiedfrom unstructured text (Shanahan et al, 2005).A number of studies have investigated sentimentclassification at document level, e.g., (Pang et al,2002; Dave et al, 2003), and at sentence level,e.g., (Hu and Liu, 2004; Kim and Hovy, 2004;Nigam and Hurst, 2005); however, the accuracyis still less than desirable.
Therefore, ranking ac-cording to the likelihood of containing sentimentinformation is expected to serve a crucial func-tion in helping users.
We believe that our workis the first attempt at sentiment retrieval that aimsat finding sentences containing information with aspecific sentiment polarity on a certain topic.Intuitively, the expression of sentiment in textis dependent on the topic.
For example, a nega-tive view for some voting event may be expressedusing ?flaw?, while a negative view for some politi-cian may be expressed using ?reckless?.
Moreover,sentiment polarities are also dependent on topicsor domains.
For example, the adjective ?unpre-dictable?
may have a negative orientation in an au-tomotive review, in a phrase such as ?unpredictablesteering?, but it could have a positive orientation ina movie review, in a phrase such as ?unpredictableplot?, as mentioned in (Turney, 2002) in the con-text of his sentiment word detection.We propose sentiment retrieval models in theframework of generative language modeling, notonly assuming query terms expressing a certaintopic, but also assuming that the polarity of sen-timent interest is specified by the user in somemanner, where the topic dependence of the sen-timent is considered.
To the best of our knowl-edge, there have been no other studies on a re-trieval model unifying both topic and sentiment,and further, there have been no other studies onsentiment retrieval.
The sentiment information of-ten appears as local in a document, and thereforefocusing on finer levels, i.e., sentence or passagelevels rather than document level, is crucial.
Wethus experiment on sentiment retrieval at the sen-tence level in this paper.The rest of this paper is structured as follows.Section 2 introduces the work related to this study.Section 3 describes a generative model of sen-timent, which is proposed here as a theoreticalframework for our work.
Section 4 describes thetask definition and our sentiment retrieval model.345Section 5 explains the data we used for our experi-ments, and gives our experimental results.
Section6 concludes the paper.2 Related WorkSome efforts for the TREC Novelty Track wererelated to our work.
Although some of the topicsused in the Novelty Track in 2003 and 2004 (Sobo-roff and Harman, 2003; Soboroff, 2004) were re-lated to opinions, most of the efforts were fo-cused on topic, such as studies using term dis-tribution within each sentence, e.g., (Allan et al,2003; Losada, 2005; Murdock and Croft, 2005).Amongst the participants in the TREC NoveltyTrack, only (Kim et al, 2004) proposed a methodspecialized to opinion-bearing sentence retrieval,by making use of lists of words with positive ornegative polarities.
They aimed to find opinionson a given topic but did not distinguish or did notcare about sentiment polarities that should be rep-resented in some sentences (hereafter, opinion re-trieval).
We focus on finding positive views ornegative views according to a given topic and sen-timent of interest (hereafter, sentiment retrieval).Our work is the first work on sentiment retrieval,to the best of our knowledge.In the context of sentiment classification, someresearchers have conducted studies on the topicdependence of sentiment polarities.
(Nasukawaand Yi, 2003) and (Yi et al, 2003) extracted pos-itive or negative expressions on a given productname using handmade lexicons.
(Engstro?m, 2004)studied how the topic dependence influences theaccuracy of sentiment classification and attemptedto reduce the influence to improve the accuracy.
(Wilson et al, 2005) investigated how context in-fluences sentiment polarity at the phrase level in acorpus, beginning with a predefined list of wordswith polarities.
Their focus on the phenomena oftopic dependence of sentiment can be shared withour work; however, their work is not directly re-lated to ours, because we focus on a different task,sentiment retrieval, where different approaches arerequired.3 A Generative Model of SentimentIn this section we will provide a formal underpin-ning for our approach to sentiment retrieval.
Theapproach is based on the generative paradigm: wedescribe a statistical process that could be viewed,hypothetically, as a source of every statement ofinterest to our system.
We stress that this genera-tive process is to be treated as purely hypothetical;the process is only intended to reflect those aspectsof human discourse that are pertinent to the prob-lem of retrieving affectively appropriate and topic-relevant texts in response to a query posed by ouruser.Before giving a formal specification of ourmodel, we will provide a high-level overview ofthe main ideas.
We are trying to model a col-lection of natural-language statements, some ofwhich are relevant to a user?s query.
In our ex-periments, these statements are individual sen-tences, but the model can be applied to textualchunks of any length.
We assume that the con-tent of an individual statement can be modeledindependently of all other statements in the col-lection.
Each statement consists of some topic-bearing and some sentiment-bearing words.
Weassume that the topic-bearing words represent ex-changeable samples from some underlying topiclanguage model.
Exchangeability means that therelative order of the words is irrelevant, but thewords are not independent of each other?the ideaoften stated as a bag-of-words assumption.
Sim-ilarly, sentiment-bearing words are viewed as anorder-invariant ?bag?, sampled from the underly-ing sentiment language model.
We will explicitlymodel dependency between the topic and senti-ment language models, and will demonstrate thattreating them independently leads to sub-optimalretrieval performance.
When a sentiment polarityvalue is observed for a given statement, we willtreat it as a ternary variable influencing the topicand sentiment language models.We represent a user?s query as just another state-ment, consisting of topic and sentiment parts, sub-ject to all the independence assumptions statedabove.
We will use the query to estimate the topicand sentiment language models that are represen-tative of the user?s interests.
Following (Lavrenkoand Croft, 2001), we will use the term relevancemodels to describe these models, and will use themto rank statements in order of their relevance to thequery.3.1 DefinitionsWe start by providing a set of definitions that willbe used in the remainder of this section.
The taskof our model is to generate a collection of state-ments w1: : :wn.
A statement wiis a string of346wordswi1: : :wini, drawn from a common vocabu-lary V .
We introduce a binary variable bij2fS; Tgas an indicator of whether the word in the jth po-sition of the ith statement will be a topic word ora sentiment word.
For our purposes, bijis eitherprovided by a human annotator (manual annota-tion), or determined heuristically (automatic an-notation).The sentiment polarity xifor a given statementis a discrete random variable with three outcomes:f 1; 0;+1g, representing negative, neutral andpositive polarity values, respectively.
As a matterof convenience we will often denote a statement asa triple fwsi;wti; xig, where wsicontains the sen-timent words and wticontains the topic words.
Aswe mentioned above, the user?s query is treatedas just another statement.
It will be denoted asa triple fqs;qt;qxg, corresponding to sentimentwords, topic keywords, and the desired polarityvalue.
We will use p to denote a unigram lan-guage model, i.e., a function that assigns a numberp(v)2[0; 1?
to every word v in our vocabulary V ,such that vp(v)=1.
The set of all possible un-igram language models is the probability simplexIP .
Similarly, pxwill denote a distribution overthe three possible polarity values, and IPxis thecorresponding ternary probability simplex.
We de-fine  : IPIPIPx!
[0; 1?
to be a measure func-tion that assigns a probability (p1;p2;px) to apair of language models p1and p2together with apolarity model px.3.2 Generative modelUsing the definitions presented above, and assum-ing that () is given, we hypothesize that a newstatement wicontaining words wi1: : :wimwithsentiment polarity xican be generated accordingto the following mechanism.1.
Draw pt;psand pxfrom (; ; ).2.
Sample xifrom a polarity distribution px().3.
For each position j = 1: : :m:(a) if bij=T : draw wijfrom pt() ;(b) if bij=S: draw wijfrom ps() .The probability of observing the new statementwi1: : :wimunder this mechanism is given by:Xpt;ps;px(pt;ps;px)px(xi)mYj=1(pt(wij) if bij=Tps(wij) otherwise(1)The summation in equation (1) goes over all pos-sible pairs of language models pt;ps, but we canavoid integration by specifying a mass function() that assigns nonzero probabilities to a finitesubset of points in IPIPIPx.
We accomplishthis by using a nonparametric estimate for (), thedetails of which are provided below.3.2.1 A nonparametric generative massfunctionWe use a nonparametric estimate for (; ; ),which makes our generative model similar tokernel-based density estimators or Parzen-windowclassifiers (Silverman, 1986).
The primary dif-ference is that our model operates over discreteevents (strings of words), and accordingly themass function is defined over the space of distribu-tions, rather than directly over the data points.
Ourestimate relies on a collection of paired observa-tions C = fwti;wsi; xi: i=1::ng, which representstatements for which we know which words aretopic words (wti), and which are sentiment words(wsi).
Each of these observations corresponds toa unique point pti;psi;pxiin the space of paireddistributions IPIPIPx, defined by the follow-ing coordinates:pti(v) = t#(v;wti)=#(wti) + (1 t)tvpsi(v) = s#(v;wsi)=#(wsi) + (1 s)svpxi(x) = x1x=xi+ (1 x): (2)Here, #(v;wti) represents the number of times theword v was observed in the topic part of statementi, the length of which is denoted by #(wti).tvstands for the relative frequency of v in the topicpart of the collection.
The same definitions ap-ply to the sentiment parameters #(v;wsi), #(wsi)andsv.
The Boolean indicator function 1yreturnsone when the predicate y is true and zero other-wise.
Metaparameters t, sand xspecify theamount of Dirichlet smoothing (Zhai and Lafferty,2001) applied to the topic, sentiment and polarityestimates respectively; values for these parametersare determined empirically.We define (pt;ps;px) to have mass 1nwhenits argument pt;ps;pxcorresponds to some ob-servation pti;psi;pxi, and zero otherwise:(pt;ps;px) =1nnXi=11pt=pti1ps=psi1px=pxi:(3)Equation (3) maintains empirical dependenciesbetween the topic language model ptand the sen-timent model ps, because we assign nonzero prob-347ability mass only to pairs of models that actuallyco-occur in our observations.3.2.2 Limitations of the modelOur model represents each statement wias abag of words, or more formally an order-invariantsequence.
This representation is often confusedwith word independence, which is a much strongerassumption.
The generative model defined byequation (1) ignores the relative ordering of thewords, but it does allow arbitrarily strong un-ordered dependencies among them.
To illustrate,consider the probability of observing the words?unpredictable?
and ?plot?
in the same statement.Suppose we set t; s=1 in equation (2), reduc-ing the effects of smoothing.
It should be evi-dent that P (unpredictable,plot) will be non-zeroonly when the two words actually co-occur in thetraining data.
By carefully selecting the smoothingparameters, the model can preserve dependenciesbetween topic and sentiment words, and is quitecapable of distinguishing the positive sentiment of?unpredictable plot?
from the negative sentimentof ?unpredictable steering?.
On the other hand, themodel does ignore the ordering of the words, so itwill not be able to differentiate the negative phrase?gone from good to bad?
from its exact opposite.Furthermore, our model is not well suited for mod-eling adjacency effects: the phrase ?unpredictableplot?
is treated in the same way as two separatewords, ?unpredictable?
and ?plot?, co-occurring inthe same sentence.3.3 Using the model for retrievalThe generative model presented above can be ap-plied to sentiment retrieval in the following fash-ion.
We start with a collection of statements C anda query fqs;qt;qxg supplied by the user.
We usethe machinery outlined in Section 3.2 to estimatethe topic and sentiment relevance models corre-sponding to the user?s information need, and thendetermine which statements in our collection mostclosely correspond to these models of relevance.The topic relevance model Rtand sentiment rele-vance model Rsare estimated as follows.
We as-sume that our query qs;qt;qx is a random samplefrom a distribution defined by equation (1), andthen for each word v we estimate the likelihoodthat v would be observed if we sampled one moretopic or sentiment word:Rt(v)=P (qs;qt?v;qx)P (qs;qt;qx); Rs(v)=P (qs?v;qt;qx)P (qs;qt;qx):(4)Both the numerator and denominator are com-puted according to equation (1), with the massfunction () given by equations (3) and (2).
Weuse the notation q?v to denote appending word vto the string q. Estimation is done over the train-ing corpus, which may or may not include numericvalues of sentiment polarity.1 Once we have esti-mates for the topic and sentiment relevance mod-els, we can rank testing statements w by their sim-ilarity to Rtand Rs.
We rank statements usinga variation of cross-entropy, which was proposedby (Zhai, 2002):XvRt(v) logpt(v)+(1 )XvRs(v) logps(v):(5)Here the summations extend over all words v inthe vocabulary, Rtand Rsare given by equa-tion (4), while ptand psare computed accordingto equation (2).
A weighting parameter  allowsus to change the balance of topic and sentimentin the final ranking formula; its value is selectedempirically.4 Sentiment Retrieval Task4.1 Task definitionWe define two variations of the sentiment retrievaltask.
In one, the user supplies us with a numericvalue for the desired polarity qx.
In the other,the user supplies a set of seed words qs, reflect-ing the desired sentiment.
The first task requiresus to have polarity observations xiin our trainingdata, while the second does not.Task with training data:Input: (1) a set of topic keywords qt and (2)a sentiment specification qx 2 f 1; 1g.
Inthis case we assume qs to be the emptystring.Output: a ranked list of topic-relevant andsentiment-relevant sentences from the testdata.Task with seed words:Input: (1) a set of topic keywords qt and (2)a set of sentiment seed words qs .
In thiscase our model ignores qx and xi.1When the training corpus does not contain numeric po-larity values xi, we assume (pt;ps;px)=(pt;ps) andforce px(xi) to be a constant.348Output: a ranked list of topic-relevant andsentiment-relevant sentences from the testdata.In the first task, we split our corpus into threeparts: (i) the training set, which was used for es-timating the relevance models Rsand Rt; (ii) thedevelopment set, which was used for tuning themodel parameters t, sand ; and (iii) the testingset, from which we retrieved sentences in responseto the query.
In the second task, we split the corpusinto two parts: (i) the training set, which was usedfor tuning the model parameters; and (ii) the test-ing set, which was used for constructing RsandRtand from which we retrieved sentences in re-sponse to queries.2 The testing set was identicalin both tasks.
Note that the sentiment relevancemodel Rscan be constructed in a topic-dependentfashion for both tasks.4.2 Variations of the retrieval modelslm: the retrieval model as described in Sec-tion 3.3.lmt: the standard language modeling ap-proach (Ponte and Croft, 1998; Song andCroft, 1999) on the topic keywords qt for thetopic part of the text wt.lms: the standard language modeling approachon the sentiment keywords qs for the senti-ment part of the text ws.base: the weighted linear combination of lmtand lms.rmt: only the topic relevance model was usedfor ranking using qt and for wt .3rms: only the sentiment relevance model wasused for ranking using qs and for ws.rmt-base: the slm model with  = 1, ignoringthe sentiment relevance model.rms-base: the slm model with  = 0, ignoringthe topic relevance model.2Because the training set was used for tuning the modelparameters, no development set was required for this task.3When we use the automatic annotation that is describedin Section 5.2.2, we use the whole text instead of the topicpart of the text, for the reasons given in that section.
Thistreatment is applied to the base, rmt-base, rms-base, rmt-rms,rmt-slm and slm models that are described in this section forusing the automatic annotation.
However, we distinguish thelmt and rmt models using the topic part of the text and thelmtf and rmtf models, as baselines, using the whole text, re-spectively, even in the experiments using the automatic anno-tation.rmt-rms: the rmt and rms models are treatedindependently.rmt-slm: the rmt and rms-base models arecombined.lmtf: the standard language modeling ap-proach using qt for the nonsplit text, as base-line.rmtf: the conventional relevance model wasused for ranking using qt for the nonsplit text,as baseline.lmtsf: the standard language modeling ap-proach using both qt and qs for the nonsplittext, for reference.rmtsf: the conventional relevance model wasused for ranking using both qt and qs for thenonsplit text, for reference.Note that the relevance models are constructedusing training data for the training-based task, butare constructed using test data for the seed-basedtask, as mentioned in Section 4.1.
Therefore, thebase model is only used for the training data, notfor the test data, in the training-based task, whileit can be performed for the test data in the case ofthe seed-based task.
Moreover, the lms, lmtsf andrmtsf models are based on the premise of usingseed words to specify sentiments, and so they areonly applicable to the seed-based task.In the models described in this subsec-tion, tand sin equation (2) were set toDirichlet estimates (Zhai and Lafferty, 2001),#(wti)=(#(wti) + t) and #(wsi)=(#(wsi) + s)for the relevance models Rtand Rs, respectively,in equation (4), and were fixed at 0.9 for rankingas in equation (5) for our experiments in Section 5.Here, tand swere selected empirically accord-ing to the tasks described in Section 4.1.
Themodel parameter  in equation (5) was also se-lected empirically in the same manner.
The num-ber of ranked documents used in the relevancemodels Rtand Rs, in equation (4), was selectedempirically in the same manner as above; how-ever, we fixed the number of terms used in the rel-evance models as 1000.5 Experiments5.1 Data set and evaluation measureWe used the MPQA Opinion Corpus version1.2 (Wilson et al, 2005; Wiebe et al, 2005) tomeasure the effectiveness of our sentiment re-349trieval models.
We summarize this data set as fol-lows. This corpus contains news articles collectedfrom 187 different foreign and U.S. newssources from June 2001 to May 2002.
The cor-pus contains 535 documents, a total of 11,114sentences. The majority of the articles are on 10 differ-ent topics, which are labeled at document level,but, in addition to these, a number of additionalarticles were randomly selected from a largercorpus of 270,000 documents. Each article was manually annotated using anannotation scheme for opinions and other pri-vate states at phrase level.
We only used theannotations for sentiments that included someattributes such as polarity and strength.In this data set, the topic relevance for the 10topics is known at the document level, but un-known at the sentence level.
We assumed that allthe sentences in a relevant document could be con-sidered relevant to the topic.4This data set was annotated with sentiment po-larities at the phrase level, but not explicitly an-notated at the sentence level.
Therefore, we pro-vided sentiment polarities at the sentence level toprepare training data and data for evaluation.
Weset the sentence-level sentiment polarity equal tothe polarity with the highest strength in each sen-tence.5Queries were expressed using the title of one ofthe 10 topics and specified as positive or negative.Thus, we had 20 types of queries for our experi-ments.
Because the supposed relevance judgmentsin this setting are imperfect at sentence level, weused bpref (Buckley and Voorhees, 2004), in boththe training and testing phases, as it is known tobe tolerant of imperfect judgments.
Bpref uses bi-nary relevance judgments to define the preferencerelation (i.e., any relevant document is preferredover any nonrelevant document for a given topic),while other measures, such as mean average pre-cision, depend only on the ranks of the relevantdocuments.4This is a strong assumption to make and may not be truein all cases.
A larger, more complete data set is required toperform a more detailed analysis, which is left as future work.5We disregarded ?neutral?
and ?both?
if other polarities ap-peared.
We can also set the sentence-level sentiment polarityaccording to the presence of polarity in each sentence, but wedid not consider this setting here.5.2 Extracting sentiment expressions5.2.1 Using manual annotationBecause the MPQA corpus was annotated withphrase-level sentiments, we can use these anno-tations to split a sentence into a topic part wtand a sentiment part ws.
The Krovetz stem-mer (Krovetz, 1993) was applied to the topic part,the sentiment part and to the query terms6 and, forthe retrieval experiments in Sections 5.3 and 5.4,a total of 418 stopwords from a standard stopwordlist were removed when they appeared.5.2.2 Using automatic annotationIn automatic extraction of sentiment expres-sions in this study, we detected sentiment-bearingwords using lists of words with established polar-ities.
At this stage, topic dependence was not con-sidered; however, at the stage of sentiment model-ing, the topic dependence can be reflected, as de-scribed in Sections 3 and 4.We first prepared a list of words indicating sen-timents.
We used Hatzivassiloglou and McKe-own?s sentiment word list (Hatzivassiloglou andMcKeown, 1997), which consists of 657 positiveand 679 negative adjectives, and The General In-quirer (Stone et al, 1966), which contains 1621positive and 1989 negative words.7 By mergingthese lists, we obtained 1947 positive and 2348negative words.
After stemming these words in thesame manner as in Section 5.2.1, we were left with1667 positive and 2129 negative words, which wewill use hereafter in this paper.The sentiment polarities are sometimes sensi-tive to the structural information, for instance,a negation expression reverses the followingsentiment polarity.
To handle negation, ev-ery sentiment-bearing word was rewritten with a?NEG?
suffix, such as ?good NEG?, if an odd num-ber of negation expressions was found within thefive preceding words in the sentence.
To detectnegation expressions, we used a predefined nega-tion expression list.
This negation handling is sim-ilar to that used in (Das and Chen, 2001; Pang etal., 2002).
We extracted sentiment-bearing expres-sions using the list of words with established po-6We used the topic labels attached to the MPQA corpus asthe topic query terms qt in all the experiments in Sections 5.3and 5.4.7We extracted positive and negative words from the Gen-eral Inquirer basically in the same manner as in (Turney andLittman, 2003); however, we did not exclude any words, un-like (Turney and Littman, 2003), where some seed wordswere excluded for the evaluation of their work.350Table 1: Sample probabilities from the sentiment relevance modelsReaction to President Bush?s 2002 presidential election Israeli settlements inTopic-independent Topic-independent 2002 State of the Union Address in Zimbabwe Gaza and West Bankw/ manual annot.
w/ automatic annot.
w/ manual annot.
w/ automatic annot.
w/ manual annot.
w/ automatic annot.
w/ manual annot.
w/ automatic annot.P (wjQ) w P (wjQ) w P (wjQ) w P (wjQ) w P (wjQ) w P (wjQ) w P (wjQ) w P (wjQ) w0.047 demand 0.029 state 0.030 support 0.067 state 0.042 support 0.039 support 0.041 ask 0.097 settle0.031 expect 0.026 support 0.016 promise 0.034 support 0.033 legitimate 0.033 legitimate 0.036 agreed 0.032 peace0.031 defend 0.014 lead 0.014 call 0.024 call 0.031 free 0.033 lead 0.036 call 0.025 state0.031 invite 0.013 call 0.014 excellent 0.019 meet 0.029 congratulate 0.025 free 0.033 aim 0.022 secure0.031 humane 0.013 minister 0.013 goal 0.017 minister 0.028 fair 0.025 fair 0.028 immediate 0.015 call0.031 safeguard 0.011 right 0.013 express 0.015 promise 0.023 please 0.018 state 0.025 aware 0.014 conflict0.031 nutritious 0.010 foreign 0.013 best 0.014 white 0.017 confident 0.017 congratulate 0.024 key 0.013 support0.031 helpful 0.009 hope 0.012 count 0.013 foreign 0.017 call 0.015 call 0.022 expect 0.012 right0.016 time 0.009 meet 0.012 cooperate 0.012 success 0.012 hopeful 0.015 meet 0.018 justify 0.011 attack0.016 say 0.008 interest 0.011 proposal 0.011 defense 0.012 express 0.013 unity 0.018 honoure 0.011 minister0.091 evil 0.037 state 0.065 evil 0.098 state 0.029 flaw 0.028 flaw 0.018 palestinian 0.100 settle0.080 axis 0.022 evil 0.049 axis 0.051 evil 0.018 condemn 0.026 critic 0.013 protest 0.031 state0.045 threat 0.015 right 0.022 critic 0.028 critic 0.015 true 0.023 state 0.012 decide 0.019 peace0.033 qualify 0.015 prison 0.011 prepare 0.017 call 0.014 critic 0.022 opposition 0.011 peace 0.014 secure NEG0.030 wrote 0.013 critic 0.010 recognize 0.012 interest 0.012 expect 0.019 reject 0.011 fatten 0.013 critic0.020 particular 0.010 human 0.010 reckless 0.011 move 0.011 reject 0.017 condemn 0.011 believe 0.012 force0.020 word 0.008 support 0.010 country 0.011 reject 0.011 s 0.016 legal 0.009 plan 0.012 attack0.018 harsh 0.008 protest 0.009 upset 0.010 slam 0.011 fair 0.015 move 0.009 fear 0.012 war0.015 reject 0.008 war 0.009 pick 0.010 right 0.011 free 0.015 democratic 0.009 mistake 0.011 believe0.015 dangerous 0.008 force 0.009 eyesore 0.010 attack 0.010 angry 0.014 support 0.009 continue 0.011 ministerThe upper and lower tables correspond to positive and negative sentiments, respectively.
The topic-independentsentiment relevance models (in the left two columns) correspond to rms, and the topic-dependent models (in therest of the columns) correspond to rms-base, which is used for slm.larities, considering negation, as described above.Note that we used the list of words with sentimentsto extract sentiment expressions, but we did notuse the predefined sentiments to model sentimentrelevance.Some expressions are sometimes used to ex-press a certain topic, such as settlements in ?Is-raeli settlements in Gaza and West Bank?
; but atother times are used to express a certain sentiment,such as the same word in ?All parties signed court-mediated compromise settlements?.
Therefore, wewill use whole sentences to model topic relevance,while we will use the automatically extracted sen-timent expressions to model sentiment relevance,in Sections 5.3 and 5.4.5.3 Experiments on training-based taskWe conducted experiments on the training-basedtask described in Section 4.1, using either man-ual annotation as described in Section 5.2.1 or au-tomatic annotation as described in Section 5.2.2.Table 1 contrasts sample probabilities from topic-independent sentiment relevance models and thosefrom topic-dependent sentiment relevance models.In the left two columns of this table, two sets ofsample probabilities using the topic-independentmodel are presented.
One was computed from themanual annotation and the other was computedfrom the automatic annotation.
In the remain-ing columns, samples using the topic-dependentmodel are shown according to the three topics:(1) ?reaction to President Bush?s 2002 State ofthe Union Address?, (2) ?2002 presidential elec-tion in Zimbabwe?, and (3) ?Israeli settlementsin Gaza and West Bank?.
A number of posi-tive expressions appeared topic dependent, suchas ?promise?
(stemmed from ?promising?
or not)and ?support?
for Topic (1), ?legitimate?
and ?con-gratulate?
for Topic (2) and ?justify?
and ?se-cure?
for Topic (3); while negative expressions ap-peared topic-dependent, such as ?critic?
(stemmedfrom ?criticism?)
and ?eyesore?
for Topic (1),?flaw?
and ?condemn?
for Topic (2) and ?mistake?and ?secure NEG?
(i.e., ?secure?
was negated) forTopic (3).Some expressions were unexpectedly generatedregardless of the types of annotation, e.g., ?pales-tinian?
for Topic (3); however, we found somecharacteristics in the results using automatic anno-tation.
Some expressions on opinions that did notconvey sentiments, such as ?state?, frequently ap-peared regardless of topic.
This sort of expressionmay effectively function as degrading sentencesonly conveying facts, but may function harmfullyby catching sentences conveying opinions withoutsentiments in the task of sentiment retrieval.
Sometopic expressions, such as ?settle?
(stemmed from?settlement?
or not) for Topic (3), were generated,because such words convey positive sentiments insome other contexts and thus they were containedin the list of sentiment-bearing words that we usedfor automatic annotation.
This will not cause atopic relevance model to drift, because we mod-eled the topic relevance using whole sentences, asdescribed in Section 5.2.2; however, it may harmthe sentiment relevance model to some extent.351Table 2: Experimental results of training-basedtask using manually annotated data10% 25% 40%Models Bpref (AvgP) Bpref (AvgP) Bpref (AvgP)lmtf 0.1389 (0.1135) 0.1389 (0.1135) 0.1386 (0.1145)lmt 0.1499 (0.1164) 0.1499 (0.1164) 0.1444 (0.1148)rmtf 0.1811 (0.1706) 0.1887 (0.1770) 0.1841 (0.1691)rmt 0.1712 (0.1619) 0.1712 (0.1619) 0.1922 (0.1705)rmt-base 0.1922 (0.1723) 0.2005 (0.1812) 0.2100* (0.1951)rms 0.0464 (0.0384) 0.0452 (0.0394) 0.0375 (0.0320)rms-base 0.0772 (0.0640) 0.0869 (0.0704) 0.0865 (0.0724)rmt-rms 0.2025 (0.1413) 0.2210 (0.1925) 0.2117 (0.2003)rmt-slm 0.2278* (0.1715) 0.2249 (0.1676) 0.1999 (0.1819)slm 0.2006 (0.1914) 0.2247 (0.1824) 0.2441* (0.2427)?*?
indicates statistically significant improve-ment over rmtf where p < 0:05 with the two-sided Wilcoxon signed-rank test.We performed retrieval experiments in the stepsdescribed in Section 4.1.
For this purpose, we splitthe data into three parts: (i) x% as the trainingdata, (ii) (50   x)% as the evaluation data, and(iii) 50% as the test data.The test results of training-based task usingmanually annotated data and automatically anno-tated data are shown in Tables 2 and 3, respec-tively.
The scores were computed according to thebpref evaluation measure (Buckley and Voorhees,2004), as mentioned in Section 5.1.
In additionto the bpref, mean average precision values arepresented as ?AvgP?
in the tables, for reference.8In these tables, the top row indicates the percent-ages of the training data x.
It turned out thatin all our experiments the appropriate fraction oftraining data was 40%.
In this setting, our slmmodel worked 76.1% better than the query like-lihood model and 32.6% better than the conven-tional relevance model, when using manual anno-tation, and both improvements were statisticallysignificant according to the Wilcoxon signed-ranktest.9 When using automatic annotation, the slmmodel worked 67.2% better than the query like-lihood model and 25.9% better than the conven-tional relevance model, where both improvementswere statistically significant.
The rmt-base modelalso worked well with automatic annotation.5.4 Experiments on seed-based taskFor experiments on the seed-based task that wasdescribed in Section 4.1, we used three groups of8As mentioned in Section 5.1, the bpref is more appro-priate for the evaluation of our experiments than the meanaverage precision.9Significance tests involved only 20 queries, which makesit difficult to achieve statistical significance.Table 3: Experimental results of training-basedtask using automatically annotated data10% 25% 40%Models Bpref (AvgP) Bpref (AvgP) Bpref (AvgP)lmtf 0.1389 (0.1135) 0.1389 (0.1135) 0.1386 (0.1145)lmt 0.1325 (0.0972) 0.1315 (0.0976) 0.1325 (0.0972)rmtf 0.1811 (0.1706) 0.1887 (0.1770) 0.1841 (0.1691)rmt 0.1490 (0.1418) 0.1762 (0.1584) 0.1695 (0.1485)rmt-base 0.2076* (0.1936) 0.2252* (0.2139) 0.2302* (0.2196)rms 0.0347 (0.0287) 0.0501 (0.0408) 0.0501 (0.0408)rms-base 0.0943 (0.0733) 0.1196 (0.0896) 0.1241 (0.0979)rmt-rms 0.1690 (0.1182) 0.2063 (0.1938) 0.1603 (0.1591)rmt-slm 0.1980 (0.1426) 0.2013 (0.1835) 0.2148 (0.1882)slm 0.2011 (0.1537) 0.2261* (0.1716) 0.2318* (0.1802)?*?
indicates statistically significant improve-ment over rmtf where p < 0:05 with the two-sided Wilcoxon signed-rank test.seed words: KAM , TUR and ORG.
Each groupconsists of a positive word set qs(+)and a negativeword set qs( ), as follows:KAM : qs(+)= fgoodg, and qs( )= fbadg.TUR: qs(+)= fgood, nice, excellent, positive,fortunate, correct, superiorg, and qs( )= fbad,nasty, poor, negative, unfortunate, wrong, infe-riorg.ORG: qs(+)= fsupport, demand, promise,want, hopeg, and qs( )= frefuse, accuse, crit-icism, fear, rejectg.KAM and TUR were used in (Kamps andMarx, 2002) and (Turney and Littman, 2003),respectively.
We constructed ORG consideringsentiment-bearing words that may frequently ap-pear in newspaper articles.We experimented with the seed-based task,making use of each of these seed word groups, inthe steps described in Section 4.1.
For this pur-pose, we split the data into two parts: (i) 50% asthe estimation data and (ii) 50% as the test data.The test results using manually annotated dataand automatically annotated data are shown in Ta-bles 4 and 5, respectively, where the scores werecomputed according to the bpref evaluation mea-sure.
Mean average precision values are also pre-sented as ?AvgP?
in the tables, for reference.When using the manually annotated approach,our slm model worked well, especially with theseed word group ORG, as shown in Table 4.
Us-ing ORG, the slm model worked 61.2% betterthan the query likelihood model and 15.2% bet-ter than the conventional relevance model, whereboth improvements were statistically significantaccording to the Wilcoxon signed-rank test.
Even352Table 4: Experimental results of seed-based taskusing manually annotated dataORG TUR KAMModels Bpref (AvgP) Bpref (AvgP) Bpref (AvgP)lmtf 0.1385 (0.1119) 0.1385 (0.1119) 0.1385 (0.1119)lmtsf 0.1182 (0.1035) 0.1061 (0.0884) 0.1330 (0.1062)lmt 0.1501 (0.1171) 0.1501 (0.1171) 0.1501 (0.1171)base 0.1615 (0.1319) 0.1531 (0.1217) 0.1514 (0.1180)rmtf 0.1938 (0.1776) 0.1938 (0.1776) 0.1938 (0.1776)rmtsf 0.1884 (0.1775) 0.1661 (0.1412) 0.1927 (0.1754)rmt 0.1974 (0.1826) 0.1974 (0.1826) 0.1974 (0.1826)rmt-base 0.1960 (0.1918) 0.1931 (0.1703) 0.1837 (0.1721)rms 0.0434 (0.0262) 0.0295 (0.0205) 0.0280 (0.0170)rms-base 0.1142 (0.1022) 0.1144 (0.0841) 0.1226 (0.0973)rmt-rms 0.1705 (0.1117) 0.1403 (0.1424) 0.1405 (0.0842)rmt-slm 0.2266* (0.2034) 0.2272* (0.2012) 0.2264* (0.2016)slm 0.2233* (0.2048) 0.2160 (0.1945) 0.2072 (0.1929)?*?
indicates statistically significant improve-ment over rmtf where p < 0:05 with the two-sided Wilcoxon signed-rank test.using the other seed word groups, the slm modelworked 49?56% better than the query likelihoodmodel and 6?12% better than the conventionalrelevance model; however, the latter improve-ment was not statistically significant.
The rmt-slmmodel also worked well with manual annotation.When using automatic annotation, the slmmodel worked 46?48% better than the query like-lihood model and 4?6% better than the conven-tional relevance model, as shown in Table 5.
Theimprovements over the conventional relevancemodel were statistically significant only when us-ing TUR or KAM ; however, the score when us-ing ORG is almost comparable with the others.6 ConclusionWe propose sentiment retrieval models in theframework of probabilistic generative models, notonly assuming that a user inputs query terms ex-pressing a certain topic, but also assuming that theuser specifies a sentiment polarity of interest ei-ther as a sentiment specification qx 2 f 1; 1g oras a set of sentiment seed words qs.
For this pur-pose, we combine sentiment relevance models andtopic relevance models, considering the topic de-pendence of the sentiment.
In our experiments,our model worked significantly better than stan-dard language modeling approaches, both whenusing qx and qs, and with both manual and auto-matic annotation of the fragments expressing sen-timents in text.
With qs and automatic annota-tion, our model still worked significantly betterthan the standard approaches; however, the per-Table 5: Experimental results of seed-based taskusing automatically annotated dataORG TUR KAMModels Bpref (AvgP) Bpref (AvgP) Bpref (AvgP)lmtf 0.1385 (0.1119) 0.1385 (0.1119) 0.1385 (0.1119)lmtsf 0.1182 (0.1035) 0.1061 (0.0884) 0.1330 (0.1062)lmt 0.1325 (0.0972) 0.1325 (0.0972) 0.1325 (0.0972)basef 0.1550 (0.1369) 0.1451 (0.1188) 0.1416 (0.1142)rmtf 0.1938 (0.1776) 0.1938 (0.1776) 0.1938 (0.1776)rmtsf 0.1884 (0.1775) 0.1661 (0.1412) 0.1927 (0.1754)rmt 0.1757 (0.1578) 0.1757 (0.1578) 0.1757 (0.1578)rmt-base 0.1957 (0.1862) 0.1976 (0.1882) 0.1825 (0.1704)rms 0.0421 (0.0236) 0.0364 (0.0205) 0.0217 (0.0147)rms-base 0.1268 (0.1096) 0.1301 (0.1148) 0.1326 (0.1158)rmt-rms 0.1465 (0.1514) 0.1390 (0.1393) 0.1252 (0.0757)rmt-slm 0.1977 (0.1811) 0.2008 (0.1649) 0.1959 (0.1677)slm 0.2031 (0.1714) 0.2055* (0.1668) 0.2044* (0.1698)?*?
indicates statistically significant improve-ment over rmtf where p < 0:05 with the two-sided Wilcoxon signed-rank test.formance did not reach that achieved with othersettings.
We believe the performance can be im-proved with larger-scale data.We experimented to find sentences that wererelevant to a given topic and were appropriate toa given sentiment; however, our models can alsobe applied to textual chunks of any length, such asat document level or passage level.
Our model canbe easily extended to opinion retrieval, if the opin-ion retrieval is defined as retrieving sentences ordocuments that contain either positive or negativesentiments.
This issue is worth pursuing in futurework.
Approaches considering polarity strengthor continuous values for the polarity specification,rather than using f 1; 1g, can also be consideredin future work.AcknowledgmentsWe thank James Allan, W. Bruce Croft and the anony-mous reviewers for valuable discussions and comments.
Thiswork was supported in part by the Overseas Research Schol-ars Program and the Grant-in-Aid for Scientific Research(#17680011) from the Ministry of Education, Culture, Sports,Science and Technology, Japan, in part by the Telecommu-nications Advancement Foundation, Japan, in part by theCenter for Intelligent Information Retrieval, and in part bythe Defense Advanced Research Projects Agency (DARPA),USA under contract number HR0011-06-C-0023.
Any opin-ions, findings and conclusions or recommendations expressedin this material are those of the author(s) and do not necessar-ily reflect those of the sponsor.ReferencesJames Allan, Courtney Wade, and Alvaro Bolivar.
2003.
Re-trieval and novelty detection at the sentence level.
In Proc.of the 26th Annual International ACM SIGIR Conference,pages 314?321, Toronto, Canada.353Chris Buckley and Ellen M. Voorhees.
2004.
Retrieval eval-uation with incomplete information.
In Proc.
of the 27thAnnual International ACM SIGIR Conference, pages 25?32, Sheffield, United Kingdom.Sanjiv R. Das and Mike Y. Chen.
2001.
Yahoo!
for Ama-zon: Sentiment parsing from small talk on the Web.
InProc.
of the 2001 European Finance Association AnnualConference, Barcelona, Spain.Kushal Dave, Steve Lawrence, and David M. Pennock.
2003.Mining the peanut gallery: Opinion extraction and seman-tic classification of product reviews.
In Proc.
of the 12thInternational Conference on the World Wide Web, pages519?528, Budapest, Hungary.Charlotta Engstro?m.
2004.
Topic dependence in sentimentclassification.
Master?s thesis, University of Cambridge.Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997.Predicting the semantic orientation of adjectives.
In Proc.of the 35th Annual Meeting of the Association for Compu-tational Linguistics, pages 174?181, Madrid, Spain.Minqing Hu and Bing Liu.
2004.
Mining and summariz-ing customer reviews.
In Proc.
of the 10th ACM SIGKDDInternational Conference on Knowledge Discovery andData Mining, pages 168?177, Seattle, USA.Jaap Kamps and Maarten Marx.
2002.
Words with attitude.In Proc.
of the 1st International Conference on GlobalWordNet, pages 332?341, Mysore, India.Soo-Min Kim and Eduard Hovy.
2004.
Determining the sen-timent of opinions.
In Proc.
of the 20th International Con-ference on Computational Linguistics, Geneva, Czech Re-public.Soo-Min Kim, Deepak Ravichandran, and Eduard Hovy.2004.
ISI Novelty Track system for TREC 2004.
In Proc.of the 13th Text Retrieval Conference.
NIST Special Pub-lication 500-261.Robert Krovetz.
1993.
Viewing morphology as an inferenceprocess.
In Proc.
of the 16th Annual International ACMSIGIR Conference, pages 191?202, Pittsburgh, Pennsylva-nia, USA.Victor Lavrenko and W. Bruce Croft.
2001.
Relevance-basedlanguage models.
In Proc.
of the 24th Annual Interna-tional ACM-SIGIR Conference, pages 120?127, New Or-leans, Louisiana, USA.David E. Losada.
2005.
Language modeling for sentenceretrieval: A comparison between multiple-Bernoulli andmultinomial models.
In Information Retrieval and TheoryWorkshop, Glasgow, United Kingdom.Vanessa Murdock and W. Bruce Croft.
2005.
A translationmodel for sentence retrieval.
In Proc.
of HLT/EMNLP2005, pages 684?691, Vancouver, Canada.Tetsuya Nasukawa and Jeonghee Yi.
2003.
Sentiment anal-ysis: Capturing favorability using natural language pro-cessing.
In Proc.
of the 2nd International Conference onKnowledge Capture, pages 70?77, Sanibel Island, Florida,USA.Kamal Nigam and Matthew Hurst, 2005.
Computing Atti-tude and Affect in Text: Theory and Applications, chapterTowards a Robust Metric of Opinion.
Springer.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002.Thumbs up?
Sentiment classification using machinelearning techniques.
In Proc.
of the 2002 Conferenceon Empirical Methods in Natural Language Processing,pages 79?86, Philadelphia, Pennsylvania, USA.Jay M. Ponte and W. Bruce Croft.
1998.
A language mod-eling approach to information retrieval.
In Proc.
of the21st Annual International ACM-SIGIR Conference, pages275?281, Melbourne, Australia.James Shanahan, Yan Qu, and Janyce Wiebe, editors.
2005.Computing attitude and affect in text.
Springer.B.
W. Silverman, 1986.
Density Estimation for Statistics andData Analysis, pages 75?94.
CRC Press.Ian Soboroff and Donna Harman.
2003.
Overview of theTREC 2003 Novelty Track.
In Proc.
of the 12th Text Re-trieval Conference, pages 38?53.
NIST Special Publica-tion 500-255.Ian Soboroff.
2004.
Overview of the TREC 2004 NoveltyTrack.
In Proc.
of the 13th Text Retrieval Conference.NIST Special Publication 500-261.Fei Song and W. Bruce Croft.
1999.
A general languagemodel for information retrieval.
In Proc.
of the 8th Inter-national Conference on Information and Knowledge Man-agement, pages 316?321, Kansas City, Missouri, USA.Philip J.
Stone, Dexter C. Dunphy, Marshall S. Smith, andDaniel M. Ogilvie.
1966.
The General Inquirer: A Com-puter Approach to Content Analysis.
MIT Press.Peter D. Turney and Michael L. Littman.
2003.
Measur-ing praise and criticism: Inference of semantic orientationfrom association.
ACM Transactions on Information Sys-tems, 21(4):315?346.Peter D. Turney.
2002.
Thumbs up or thumbs down?
Se-mantic orientation applied to unsupervised classificationof reviews.
In Proc.
of the 40th Annual Meeting of the As-sociation for Computational Linguistics, pages 417?424,Philadelphia, Pennsylvania, USA.Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005.Annotating expressions of opinions and emotions in lan-guage.
Language Resources and Evaluation, 1(2):0?0.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005.Recognizing contextual polarity in phrase-level sentimentanalysis.
In Proc.
of HLT/EMNLP 2005, Vancouver,Canada.Jeonghee Yi, Tetsuya Nasukawa, Razvan Bunescu, andWayne Niblack.
2003.
Sentiment analyzer: Extractingsentiments about a given topic using natural language pro-cessing techniques.
In Proc.
of the 3rd IEEE InternationalConference on Data Mining, pages 427?
434, Melbourne,Florida, USA.Chengxiang Zhai and John Lafferty.
2001.
A study ofsmoothing methods for language models applied to ad hocinformation retrieval.
In Proc.
of the 24th Annual Interna-tional ACM-SIGIR Conference, pages 334?342, New Or-leans, Louisiana, USA.Chengxiang Zhai.
2002.
Risk Minimization and LanguageModeling in Text Retrieval.
PhD dissertation, CarnegieMellon University.354
