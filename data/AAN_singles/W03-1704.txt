Two-Character Chinese Word Extraction Based onHybrid of Internal and Contextual MeasuresShengfen Luo,    Maosong SunNational Lab.
of Intelligent Tech.
& SystemsTsinghua University, Beijing 100084, Chinalkc-dcs@mail.tsinghua.edu.cnAbstractWord extraction is one of the importanttasks in text information processing.There are mainly two kinds of statistic-based measures for word extraction: theinternal measure and the contextualmeasure.
This paper discusses these twokinds of measures for Chinese wordextraction.
First, nine widely adoptedinternal measures are tested andcompared on individual basis.
Thenvarious schemes of combining thesemeasures are tried so as to improve theperformance.
Finally, the left/rightentropy is integrated to see the effect ofcontextual measures.
Genetic algorithm isexplored to automatically adjust theweights of combination and thresholds.Experiments focusing on two-characterChinese word extraction show apromising result: the F-measure ofmutual information, the most powerfulinternal measure, is 57.82%, whereas thebest combination scheme of internalmeasures achieves the F-measure of59.87%.
With the integration of thecontextual measure, the word extractionachieves the F-measure of 68.48% at last.1 IntroductionNew words are generated quite often with therapid development of Chinese society, resultingthat the lexicon of Chinese cannot well meet therequirement of natural language processing.
Howto extract word automatically from immense textcollection has thus become an important problem.The task of extracting Chinese words withmulti-characters from texts is quite similar to thatof extracting phrases (e.g., compound nouns) inEnglish, if we regard Chinese characters as Englishwords.
Research in word/phrase extraction hasbeen carried out extensively.
Currently themainstream approach is statistic-based.
In general,there are two kinds of statistic-based measures forestimating the soundness of an extracted itembeing a word/phrase: One is the internal measure,which estimates the soundness by the internalassociative strength between constituents of theitem.
Nine widely adopted internal measures arelisted in (Schone et al 2001), including Frequency,Mutual Information, Selectional Association,Symmetric Conditional Probability, Dice Formula,Log-likelihood, Chi-squared, Z-score, Student?s t-score.
The other kind is the contextual measure,which estimates the soundness by the dependencyof the item on its context, such as the left/rightentropy (Sornlertlamvanich et al 2000), and theleft/right context dependency (Chien 1999).This paper firstly analyzes nine internalmeasures mentioned above, tests and comparestheir word extraction performance on individualbasis, then tries to improve the performance byproperly combining these measures.
Furthermore,the contextual measure is integrated with internalmeasures to acquire more improvement.Throughout the experiments, genetic algorithm isexplored to adjust weights of combination andthresholds automatically.
We only concern two-character word extraction in this paper, becausetwo-character words reflect the most popularword-formation of Chinese and possess the largestproportion in Chinese lexicon.22.1Internal MeasuresNine internal measures are discussed andcompared in this section.
These measures tend toestimate the internal associative strength fromdifferent perspectives, so it is possible to improvethe word extraction performance by properlycombining them.
This paper will try twocombination schemes, i.e., direct combination andinterval-based combination.As stated earlier, the evaluation is based on two-character Chinese word extraction.
PDR9596, araw corpus of People Daily of 1995 and 1996 withabout 50.0M characters, is used to train the matrixof Chinese character bigrams throughout theexperiments.
PDA98J, a manually word-segmented corpus composed of People Daily ofJanuary 1998 with about 1.3M characters(developed by Institute of ComputationalLinguistics of Peking University), is further usedto exhaustively generate a list of Chinese characterbigrams.
The list contains 218,863 distinct bigrams.We randomly divide the list into two parts, 9/10 ofit as TS1, the rest 1/10 as TS2.Nine Widely Adopted Internal MeasuresTable 1 lists nine widely adopted internalmeasures, as mentioned in (Schone 2000).
In thetable: xy  represents any two-character item, xstands for all characters except x, is the size oftraining corpus,  and are frequency andprobability of x respectively, and  arefrequency and probability ofNxyxf xpf xypxy  respectively, andxy?
is the frequency expectation of xy  suppose  xand y are independent.Obviously:NffNppNp yxyxxyxy /===?Table 1.
Nine Widely Adopted Internal MeasuresMeasure Marked As FormulaFrequency Freq xyfMutualInformation MIyxxyppp2logSelectional Association SA ?zzyMIyzpxyMIyxp)()|()()|(Symmetric Conditional Probability SCPyxxyppp 2Dice Formula Diceyxxyfff+2Log-likelihood LogLyxxyyfyxyxfxyxyfyxyxpppppppp)()()(log2?Chi-squared Chi))()()(()( 2yxxyyxxyyxxyyxxyyxyxxyxyffffffffffffN++++?Z-Score ZS )1( Nfxyxyxyxy????
?Student?s t-Score TS )1( Nfffxyxyxyxy??
?Table 2.
Comparison of Word Extraction Performance of Internal Measures (Open Test  on TS1)Top 17,333 Freq MI SA SCP Dice LogL Chi ZS TSF-measure (%) 26.28 54.77 42.98 51.77 49.37 43.13 52.97 53.20 39.12Comparison of F-measure:  MI > ZS > Chi > SCP >Dice > LogL > SA > TS > FreqTable 3.
Weights for Direct Combination Scheme (Fitness Function is Based on TS1)Freq MI SA SCP Dice LogL Chi ZS TS0.000598 0.351393 0.000263 0.146348 0.214541 0.002804 0.035930 0.072293 0.17583Comparison of weights:  MI > Dice > TS > SCP > ZS > Chi > LogL > Freq > SAWhen using these nine measures for wordextraction, the hypothesis is same: the larger valueof measure means the stronger associativestrength between x and y, and thus the morepossibility of xy  being a word.
The criterion ofjudgment is very simple: xy  would be acceptedas a word if its internal associative strength islarger than a given threshold.2.22.3Word Extraction Performance of EachInternal MeasureThe performance of each internal measure istested on TS1.
TS1 contains 196,977 distinctbigrams, among which 17,333 are two-characterwords according to PDA98J.
The procedure ofword extraction is to sort the 196,977 candidatebigrams in descending order in terms of the valueof  the measure to be tested, and then to select thetop 17,333 bigrams as words.
In this case, theprecision rate, recall rate and F-measure areexactly the same.
Table 2 shows the comparisonof performances of these nine internal measures.Mutual information achieves the best performancewith the F-measure of 54.77%.Direct Combination of Internal MeasuresThe first combination scheme is to directlycombine the nine measures with appropriateweights.
The internal associative strength of anitem xy  is estimated by:?=?=91))(()(iii xyscorewtxyscorewhere  is the value of )(xyscorei xy91?=wtgiven by thei-th measure, and is the weight for the i-thmeasure accordingly (satisfying ).iwt1=iiThe determination of weights is notstraightforward due to the presence ofcombinatorial explosion (notice that  is realnumber).
Genetic algorithm (Pan 1998) isexplored to adjust the weights automatically,trying to find the optimal one.
Let (wtiwt1, wt2,?,wt9)be a possible solution, we set the F-measure ofword extraction on TS1 to be the fitness function,and set the size of population to be 25.
We simplyuse the GenocopIII software (Michalewicz) to dothe job.In a PIII650 PC, GenocopIII runs 12 hours,iterates 1,161 generation, and converges to agroup of weights (as shown in Table 3).
With thisgroup of weights, the F-measure of wordextraction on TS1 is 55.44%, improving only0.67% over the most powerful single measure MI(54.77%).
Note this is not a pure open test,because the fitness function of genetic algorithmis based on TS1.2.4 Interval-based Combination of InternalMeasuresThe experimental result in section 2.3 showsthat it is not so effective to combine the ninemeasures directly.
We try another combinationscheme now, i.e., interval-based combination.2.4.1 The IdeaThe idea is as follows: for every measurementioned above, we first discretize its valuerange into a number of intervals.
Every interval ofevery measure is then assigned a correspondingprobability that indicates the tendency of any itembeing regarded as a word if its value with respectto this measure falls into this interval.
We namethis kind of probability ?the interval probability?.The soundness of an item being a word would bethe weighted sum of all of its interval probabilitiesover nine measures.We describe the idea in a more formal way.Suppose is the internal associativestrength of any item)(xyscoreixy  with respect to the i-thmeasure,  is its corresponding intervaldetermined by the value of score ,is the interval probability of v , then thesoundness of)(xyvi)(xyi)(xyi)(xypvixy  being word, , will begiven by:)(xypv?=?=91))(()(iii xypvwtxypvwhere is the weight for the i-th measure.
iwtIf  is larger than a threshold, )(xypv xywould be extracted out as a word.2.4.2 The Related IssuesThree related issues need to be clarified.
(1) How to discretize the range of a measurewith continuous values?We use D-2, an entropy-based top-downalgorithm (Catlett 1991) to discretize the valuerange by supervised learning.
It adopts theinformation gain as the criterion to decide whethera given training set should be further partitionedor not.
Given a set of examples S, the informationgain caused by a cut point t  will be:)(||||)(||||)(),( 2211 SEntSSSEntSSSEntStIG ?
?=1 Swhere Ent(S) is the entropy of S, and S  andare two subsets of S partitioned by the cut point t .2It has been proved that the information gainobtains optimal discretization only on boundarypoints (Fayyad et al 1992, Elomaa et al 2000).
Soonly boundary points need to be examined aspotential cut points.
Suppose T is the set ofboundary points, the D-2 algorithm fordiscretizing set S  is:ALGORITHM DISCRETE (S,T)BEGINStep1.
For each  in T, calculate  t ),( StIGStep2.
Select ,  is  )),((maxarg0 StIGtt= Spartitioned into two subsets: ,  1S 2SStep3.
If stopping criteria are satisfied,Step4.
then DO NOT partition , Return S ?
.//?
is an empty setStep5.
else     P1 = DISCRETE( , T1S 1)P2 = DISCRETE( ,T2S 2)P = P1 + { } + P2, Return P.   0t//P is the set of cut points fordiscretizing SENDThis algorithm only considers two stoppingcriteria.
One is the minimal number of samples inan interval, the other is the minimum informationgain.
With this algorithm, we finally get a set ofcut points each measure, which discretize thevalue range to variable-length intervals.
(2) How to assign the interval probability toeach interval?After discretization, training examples (i.e., alist of Chinese character bigrams) will bedistributed to a certain interval according to itsvalue of a given measure.
Let  represent the j-thinterval of the i-th measure, then pv , theinterval probability of v , is defined as:ijv)( ijvijinwordsbeingbigramsijijij vinbigramsofvofvpv##)( =)(xyi(3) How to set the weights for combining allin the process of word extraction?
pvGenetic algorithm is again invoked to adjust theweight .
The configuration of GenecopIII isthe same as that in session 2.3.iwt2.4.3 Effect of the Stopping Criteria andDiscretization Strategy on CombinationThe stopping criteria and discretization strategytake effect on the word extraction performance ofinterval-based combination.First, have a look at the effect of stoppingcriteria.
In DISCRETE, two stopping criteria areneeded to set.
We fix the minimal number ofsamples in one interval on 50 arbitrarily.
And, wechange the setting of the minimum informationgain.
In Table 4, performances under five differentminimum information gains are compared,marked as D1, D2 ,?, D5 respectively.
It can beseen that, the smaller the minimal informationgain, the finer the granularity of discretization,and the better the performance of word extraction.But if the discretization is too grainy, it may causeover-fitting problem.
Compared with D4 and D5,D3 achieves nearly the same performance but hasa much rough discretization.
So, we set theminimum information gain to be 0.0001 (D3) inthe following experiments.Second, observe the effect of the discretizationstrategy.
The equal-length discretization iscompared to the variable-length discretization.
Wedivide the value range of each measure into equal-length intervals, and let the number of intervals beidentical to that in D3 accordingly.
As shown inTable 4, the equal-length discretization onlyachieves the F-measure of 55.56%, which is muchless than D3 (57.45%).
This means the entropy-based discretization is more reasonable thanequal-length discretization, and the discretizationstrategy has significant impact on the performanceof interval-based combination.2.4.4 Reduction of Measures for CombinationTo improve the performance of word extractionthrough combination, the premise is that theremust be enough mutual supplements among thosemeasures.
However, if the combination involvestoo many measures, interference may becomeobvious.
We try to reduce the number of measuresfor combination.The reduction procedure is recursive: It firstcompares the performance after removing any ofthe n measures, then reduces the one that canbring the most improvement of performance if itis removed.
Repeat this reduction procedure in theleft n-1 measures, until the performance cannotimprove anymore.Table 5 shows the reduction procedure of thenine internal measures.
It indicates that, excludingSA and SCP, the interval-based combination ofother seven measures could achieve the best F-measure of 57.77%, with the weights in Table 6.That result is 3.00% higher than that of the mostpowerful internal measure MI (54.77%).Note again that all tests in section 2.4 are notpure open, because all the related parameters suchas granularity of discretization, reduction ofmeasures and adjustment of combination weights,are based on TS1.Table 4.
Effect of the Stopping Criteria and Discretization Strategy (Based on TS1)Entropy-basedDiscretization Number of PartitionsMin GainF-measure(%)Freq MI SA SCP Dice LogL Chi ZS TSD1 0.001 55.92 80 117 88 63 122 126 237 189 66D2 0.0005 56.75 104 341 230 96 255 314 380 343 625D3 0.0001 57.45 340 1449 641 109 471 1390 949 1411 1234D4 0.00005 57.67 385 1660 693 113 543 1777 1316 1555 1808D5 0.00001 57.69 423 2204 754 120 581 2522 2375 2233 2304Equal-lengthDiscretization 55.56 340 1449 641 109 471 1390 949 1411 1234Table 5.
The Reduction Procedure of the Nine Measures (Based on TS1)F-measure (%) after Removing NFreq MI SA SCP Dice LogL Chi Zs TsAction9 57.62 55.09 57.63 57.69 57.48 57.65 57.64 57.50 57.37 Reduce SCP8 57.71 55.19 57.77  57.63 57.60 57.40 57.36 57.49 Reduce SA7 57.67 55.25   57.66 57.71 57.74 57.64 57.48 No ReductionTable 6.
Weights for Interval-based Combination (Based on TS1)Freq MI SA SCP Dice LogL Chi ZS TS0.00034 0.47238 0 0 0.00238 0.00125 0.09339 0.25636 0.17390Comparison of weights:  MI > ZS > TS > Chi > Dice > FreqTable 7.
Open Test for Effect of Internal Measures, the Contextual Measures and the Hybrid (on TS2)Precision(%) Recall(%) F-measure(%) Setting t1 and t2 for Left/Right EntropyMI 56.72 58.97 57.82 N.A.Comb 60.41 59.35 59.87 N.A.MI+Le/Re 83.53 54.88 66.24 MI-tuned thresholdComb+Le/Re* 85.69 55.76 67.56 MI-tuned thresholdComb+Le/Re 85.71 57.02 68.48 Comb-tuned threshold3 The Contextual MeasureThis section turns to discuss how to make useof contextual measures.
The most commonly usedcontextual measure is the left/right entropy:????
?=AaxyaxypxyaxypxyLe )|(log)|()( 2????
?=Abxyxybpxyxybpxy )|(log)|()Re( 2where: xy  is the candidate item, a, b are Chinesecharacters belonging to A, the set of Chinesecharacters.In the sight of entropy, the larger the value ofLe(xy) and Re(xy), the more various the characterscoming after/before xy, and thus the more possiblexy to be a word.4 The Hybrid  of Internal and ContextualMeasuresCombining the contextual measure with internalmeasures, the word extraction process wouldbecome like this: First, any candidate item xy  notsatisfying the contextual condition is rejected.
Thecontextual condition is, Le(xy)>t1 and Re(xy)>t2.Second, those residual candidates will beextracted out as words if their internal measure orcombination of internal measures is high than agiven threshold t3.
In this paper, we try twoalternatives of hybrid for comparison: one is thecontextual measure with mutual information, thebest single internal measure; another one is thecontextual measure with Comb, the best result ofinterval-based combination of seven internalmeasures (Freq, MI, Dice, LogL, Chi, ZS, TS).We need to determine three thresholds in aboveprocess, Threshold t3 are set as the value to selectthe top 17,333 candidates from TS1: according toexperiments in section 2, MI will choose t3=4.0,while Comb will choose t3=0.26.
To setappropriate thresholds t1 and t2, we still employgenetic algorithm.
We let a group of threshold (t1,t2) be a possible solution, and let the F-measure ofword extraction on TS1 be fitness.
Two groups ofthresholds can be thus obtained:(1) MI-tuned thresholds: t1=2.2, t2=1.4.
(2) Comb-tuned thresholds: t1=1.8, t2=1.2.To further investigate the effect of internalmeasures, the contextual measure and the hybrid,we conduct a series of open tests on TS2, asdemonstrated in Table 7.
Since the left/rightentropy would become less reliable in cases thatthe occurrences of contexts are not sufficient, wedrop out those candidates whose frequencies areno more than 5 in TS2.
After dropping, TS2contains 14,867 candidates, out of which 1,589are words according to PDA98J.In the first two rows of Table 7, the best singleinternal measure, MI, and our best combination ofinternal measures Comb are open tested.
Thesuccessive three rows show the effect ofcontextual measures.
The row of ?MI+Le/Re?selects MI as the internal measure, and use theMI-tuned thresholds as t1 and t2.
The rows of?Comb+Le/Re*?
and ?Comb+Le/Re?
both selectComb as the internal measure, but use different t1and t2: The former uses MI-tuned thresholds,while the latter uses Comb-tuned thresholds.From Table 7, we can draw several conclusions:(1) With open test, the F-measure of MI, the bestsingle internal measure, is 57.82%, whereas the F-measure of our interval-based combination is59.87%; (2) The integration of the commonlyused contextual measure, the left/right entropywith internal measures, can bring a largeimprovement of about 8%~9%; (3) There is only amodest difference between the performances of?Comb+Le/Re*?
and ?Comb+Le/Re?, and twogroup of thresholds adjusted by different internalmeasures  have small difference as well.5 ConclusionThis paper focuses on the research of purestatistic-based measures for automatic extractionof two-character Chinese words.
Two kinds ofstatistic-based measures are discussed: internalmeasures and contextual measures.
Nine internalmeasures are tested and compared.
Two schemesare tried to improve the performance by properlycombining these nine measures.
Experimentalresults in open tests show that, the bestcombination scheme, interval-based combination,achieves the F-measure of 59.87%, improving2.05% over the best single internal measuremutual information.
On the other hand, theleft/right entropy, a kind of contextual measure, isintegrated to acquire further improvement in wordextraction.
With the left/right entropy andinterval-based combination of internal measures,the F-measure ultimately achieves 68.48%.Another point of this paper is that, weights forcombination and thresholds for left/right entropyare adjusted automatically by genetic algorithm,rather than manually.Future work will extend the proposed methodto automatic extraction of multi-character Chinesewords.
Other useful information, such as lexiconand semantic resource, are expected to beincluded for consideration so as to further improvethe performance.AcknowledgementsThis research is supported by the National?973?
Plan of China under grant no G1998030507and NSFC under grant no 60083005.ReferencesCatlett.
(1991) On changing continuous attributes intoodered discrete attributes.
In Proceedings of theEuropean Working Session on Learning, Berlin,Germany.
pp.
164-178Chien, L.F. (1999) Pat-tree-based adaptive keyphraseextraction for intelligent Chinese informationretrieval.
Information Processing and Managementvol.35 pp.501-521Elomaa T., Rousu J., (2000) Generalizing boundarypoints.
In Proceedings of the 17th NationalConference on Artificial Intelligence, Menlo Park,CA.Fayyad, U., Irani, K., (1992) On the handling ofcontinuous-valued attributes in decision treegeneration.
Machine Learning.
Vol.
(8) pp.87-102Pan, Z.J., (1998) Evolution Computing.
TsinghuaUniversity Press, Beijing.Sornlertlamvanich V., Potipiti T., Charoenporn T.(2000) Automatic corpus-based Thai wordextraction with the C4.5 learning algorithm.
InProceedings of COLING 2000.Schone, P., Jurafsky D. (2001) Is knowledge-freeinduction of multiword unit dictionary headwords asolved problem?
In proceedings of EMNLP 2001.Michalewicz, Z., Genocop III, available at:http://www.coe.uncc.edu/~gnazhiya/gchome.html
