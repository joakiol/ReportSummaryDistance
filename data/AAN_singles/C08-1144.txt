Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1145?1152Manchester, August 2008A Systematic Comparison of Phrase-Based, Hierarchical andSyntax-Augmented Statistical MTAndreas Zollmann?and Ashish Venugopal?and Franz Och and Jay PonteGoogle Inc.1600 Amphitheatre ParkwayMountain View, CA 94303, USA{zollmann,ashishv}@cs.cmu.edu {och,ponte}@google.comAbstractProbabilistic synchronous context-freegrammar (PSCFG) translation modelsdefine weighted transduction rules thatrepresent translation and reordering oper-ations via nonterminal symbols.
In thiswork, we investigate the source of the im-provements in translation quality reportedwhen using two PSCFG translation mod-els (hierarchical and syntax-augmented),when extending a state-of-the-art phrase-based baseline that serves as the lexicalsupport for both PSCFG models.
Weisolate the impact on translation qualityfor several important design decisions ineach model.
We perform this comparisonon three NIST language translation tasks;Chinese-to-English, Arabic-to-Englishand Urdu-to-English, each representingunique challenges.1 IntroductionProbabilistic synchronous context-free grammar(PSCFG) models define weighted transductionrules that are automatically learned from paralleltraining data.
As in monolingual parsing, suchrules make use of nonterminal categories to gener-alize beyond the lexical level.
In the example be-low, the French (source language) words ?ne?
and?pas?
are translated into the English (target lan-guage) word ?not?, performing reordering in thecontext of a nonterminal of type ?VB?
(verb).VP ?
ne VB pas, do not VB : w1?Work done during internships at Google Inc.?c?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.VB ?
veux,want : w2.As with probabilistic context-free grammars, eachrule has a left-hand-side nonterminal (VP and VBin the two rules above), which constrains the rule?susage in further composition, and is assigned aweight w, estimating the quality of the rule basedon some underlying statistical model.
Transla-tion with a PSCFG is thus a process of compos-ing such rules to parse the source language whilesynchronously generating target language output.PSCFG approaches such as Chiang (2005) andZollmann and Venugopal (2006) typically beginwith a phrase-based model as the foundation forthe PSCFG rules described above.
Starting withbilingual phrase pairs extracted from automaticallyaligned parallel text (Och and Ney, 2004; Koehn etal., 2003), these PSCFG approaches augment eachcontiguous (in source and target words) phrasepair with a left-hand-side symbol (like the VP inthe example above), and perform a generalizationprocedure to form rules that include nonterminalsymbols.
We can thus view PSCFG methods asan attempt to generalize beyond the purely lexi-cal knowledge represented in phrase based mod-els, allowing reordering decisions to be explicitlyencoded in each rule.
It is important to note thatwhile phrase-based models cannot explicitly repre-sent context sensitive reordering effects like thosein the example above, in practice, phrase basedmodels often have the potential to generate thesame target translation output by translating sourcephrases out of order, and allowing empty trans-lations for some source words.
Apart from oneor more language models scoring these reorder-ing alternatives, state-of-the-art phrase-based sys-tems are also equipped with a lexicalized distortionmodel accounting for reordering behavior more di-rectly.
While previous work demonstrates impres-1145sive improvements of PSCFG over phrase-basedapproaches for large Chinese-to-English data sce-narios (Chiang, 2005; Chiang, 2007; Marcu et al,2006; DeNeefe et al, 2007), these phrase-basedbaseline systems were constrained to distortionlimits of four (Chiang, 2005) and seven (Chiang,2007; Marcu et al, 2006; DeNeefe et al, 2007),respectively, while the PSCFG systems were ableto operate within an implicit reordering window of10 and higher.In this work, we evaluate the impact of the ex-tensions suggested by the PSCFG methods above,looking to answer the following questions.
Do therelative improvements of PSCFG methods persistwhen the phrase- based approach is allowed com-parable long-distance reordering, and when the n-gram language model is strong enough to effec-tively select from these reordered alternatives?
Dothese improvements persist across language pairsthat exhibit significantly different reodering effectsand how does resource availablility effect relativeperformance?
In order to answer these questionswe extend our PSCFG decoder to efficiently han-dle the high order LMs typically applied in state-of-the-art phrase based translation systems.
Weevaluate the phrase-based system for a range of re-ordering limits, up to those matching the PSCFGapproaches, isolating the impact of the nontermi-nal based approach to reordering.
Results are pre-sented in multiple language pairs and data sizescenarios, highlighting the limited impact of thePSCFG model in certain conditions.2 Summary of approachesGiven a source language sentence f , statistical ma-chine translation defines the translation task as se-lecting the most likely target translation e under amodel P (e|f), i.e.
:?e(f) = argmaxeP (e|f) = argmaxem?i=1hi(e, f)?iwhere the argmax operation denotes a searchthrough a structured space of translation ouputsin the target language, hi(e, f) are bilingual fea-tures of e and f and monolingual features of e,and weights ?iare trained discriminitively to max-imize translation quality (based on automatic met-rics) on held out data (Och, 2003).Both phrase-based and PSCFG approachesmake independence assumptions to structure thissearch space and thus most features hi(e, f) aredesigned to be local to each phrase pair or rule.A notable exception is the n-gram language model(LM), which evaluates the likelihood of the se-quential target words output.
Phrase-based sys-tems also typically allow source segments to betranslated out of order, and include distortion mod-els to evaluate such operations.
These featuressuggest the efficient dynamic programming al-gorithms for phrase-based systems described inKoehn et al (2004).We now discuss the translation models com-pared in this work.2.1 Phrase Based MTPhrase-based methods identify contiguous bilin-gual phrase pairs based on automatically gener-ated word alignments (Och et al, 1999).
Phrasepairs are extracted up to a fixed maximum length,since very long phrases rarely have a tangible im-pact during translation (Koehn et al, 2003).
Dur-ing decoding, extracted phrase pairs are reorderedto generate fluent target output.
Reordered trans-lation output is evaluated under a distortion modeland corroborated by one or more n-gram languagemodels.
These models do not have an explicit rep-resentation of how to reorder phrases.
To avoidsearch space explosion, most systems place a limiton the distance that source segments can be movedwithin the source sentence.
This limit, along withthe phrase length limit (where local reorderingsare implicit in the phrase), determine the scope ofreordering represented in a phrase-based system.All experiments in this work limit phrase pairs tohave source and target length of at most 12, andeither source length or target length of at most 6(higher limits did not result in additional improve-ments).
In our experiments phrases are extractedby the method described in Och and Ney (2004)and reordering during decoding with the lexical-ized distortion model from Zens and Ney (2006).The reordering limit for the phrase based system(for each language pair) is increased until no addi-tional improvements result.2.2 Hierarchical MTBuilding upon the success of phrase-based meth-ods, Chiang (2005) presents a PSCFG model oftranslation that uses the bilingual phrase pairs ofphrase-based MT as starting point to learn hierar-chical rules.
For each training sentence pair?s set ofextracted phrase pairs, the set of induced PSCFGrules can be generated as follows: First, each1146phrase pair is assigned a generic X-nonterminal asleft-hand-side, making it an initial rule.
We cannow recursively generalize each already obtainedrule (initial or including nonterminals)N ?
f1.
.
.
fm/e1.
.
.
enfor which there is an initial ruleM ?
fi.
.
.
fu/ej.
.
.
evwhere 1 ?
i < u ?
m and 1 ?
j < v ?
n, toobtain a new ruleN ?
fi?11Xkfmu+1/ej?11Xkenv+1where e.g.
fi?11is short-hand for f1.
.
.
fi?1, andwhere k is an index for the nonterminal X thatindicates the one-to-one correspondence betweenthe new X tokens on the two sides (it is not inthe space of word indices like i, j, u, v,m, n).
Therecursive form of this generalization operation al-lows the generation of rules with multiple nonter-minal symbols.Performing translation with PSCFG grammarsamounts to straight-forward generalizations ofchart parsing algorithms for PCFG grammars.Adaptations to the algorithms in the presence of n-gram LMs are discussed in (Chiang, 2007; Venu-gopal et al, 2007; Huang and Chiang, 2007).Extracting hierarchical rules in this fashion cangenerate a large number of rules and could in-troduce significant challenges for search.
Chiang(2005) places restrictions on the extracted ruleswhich we adhere to as well.
We disallow ruleswith more than two nonterminal pairs, rules withadjacent source-side nonterminals, and limit eachrule?s source side length (i.e., number of sourceterminals and nonterminals) to 6.
We extract rulesfrom initial phrases of maximal length 12 (exactlymatching the phrase based system).1Higher lengthlimits or allowing more than two nonterminals perrule do not yield further improvements for systemspresented here.During decoding, we allow application of allrules of the grammar for chart items spanning upto 15 source words (for sentences up to length 20),or 12 source words (for longer sentences), respec-tively.
When that limit is reached, only a specialglue rule allowing monotonic concatenation of hy-potheses is allowed.
(The same holds for the Syn-tax Augmented system.
)1Chiang (2005) uses source length limit 5 and initialphrase length limit 10.2.3 Syntax Augmented MTSyntax Augmented MT (SAMT) (Zollmann andVenugopal, 2006) extends Chiang (2005) to in-clude nonterminal symbols from target languagephrase structure parse trees.
Each target sentencein the training corpus is parsed with a stochas-tic parser?we use Charniak (2000))?to produceconstituent labels for target spans.
Phrases (ex-tracted from a particular sentence pair) are as-signed left-hand-side nonterminal symbols basedon the target side parse tree constituent spans.Phrases whose target side corresponds to a con-stituent span are assigned that constituent?s label astheir left-hand-side nonterminal.
If the target spanof the phrase does not match a constituent in theparse tree, heuristics are used to assign categoriesthat correspond to partial rewriting of the tree.These heuristics first consider concatenation oper-ations, forming categories such as ?NP+V?, andthen resort to CCG (Steedman, 1999) style ?slash?categories such as ?NP/NN.?
or ?DT\NP?.
In thespirit of isolating the additional benefit of syntacticcategories, the SAMT system used here also gen-erates a purely hierarchical (single generic nonter-minal symbol) variant for each syntax-augmentedrule.
This allows the decoder to choose betweentranslation derivations that use syntactic labels andthose that do not.
Additional features introducedin SAMT rules are: a relative frequency estimatedprobability of the rule given its left-hand-side non-terminal, and a binary feature for the the purelyhierachial variants.3 Large N-Gram LMs for PSCFGdecodingBrants et al (2007) demonstrate the value of largehigh-order LMs within a phrase-based system.
Re-cent results with PSCFG based methods have typ-ically relied on significantly smaller LMs, as aresult of runtime complexity within the decoder.In this work, we started with the publicly avail-able PSCFG decoder described in Venugopal et al(2007) and extended it to efficiently use distributedhigher-order LMs under the Cube-Pruning decod-ing method from Chiang (2007).
These extensionsallow us to verify that the benefits of PSCFG mod-els persist in the presence of large, powerful n-gram LMs.3.1 Asynchronous N-Gram LMsAs described in Brants et al (2007), using largedistributed LMs requires the decoder to perform1147asynchronous LM requests.
Scoring n-grams un-der this distributed LM involves queuing a setof n-gram probability requests, then distributingthese requests in batches to dedicated LM servers,and waiting for the resulting probabilities, beforeaccessing them to score chart items.
In orderto reduce the number of such roundtrip requestsin the chart parsing decoding algorithm used forPSCFGs, we batch all n-gram requests for eachcell.This single batched request per cell paradigmrequires some adaptation of the Cube-Pruning al-gorithm.
Cube-Pruning is an early pruning tech-nique used to limit the generation of low qualitychart items during decoding.
The algorithm callsfor the generation of N-Best chart items at eachcell (across all rules spanning that cell).
The n-gram LM is used to score each generated item,driving the N-Best search algorithm of Huang andChiang (2005) toward items that score well froma translation model and language model perspec-tive.
In order to accomodate batched asynchronousLM requests, we queue n-gram requests for the topN*K chart items without the n-gram LM whereK=100.
We then generate the top N chart itemswith the n-gram LM once these probabilties areavailable.
Chart items attempted to be generatedduring Cube-Pruning that would require LM prob-abilities of n-grams not in the queued set are dis-carded.
While discarding these items could leadto search errors, in practice they tend to be poorlyperforming items that do not affect final translationquality.3.2 PSCFG Minimal-State RecombinationTo effectively compare PSCFG approaches tostate-of-the-art phrase-based systems, we must beable to use high order n-gram LMs during PSCFGdecoding, but as shown in Chiang (2007), thenumber of chart items generated during decodinggrows exponentially in the the order of the n-gramLM.
Maintaining full n?1 word left and right his-tories for each chart item (required to correctly se-lect the argmax derivation when considering a n-gram LM features) is prohibitive for n > 3.We note however, that the full n ?
1 left andright word histories are unneccesary to safely com-pare two competing chart items.
Rather, giventhe sparsity of high order n-gram LMs, we onlyneed to consider those histories that can actuallybe found in the n-gram LM.
This allows signifi-cantly more chart items to be recombined duringdecoding, without additional search error.
The n-gram LM implementation described in Brants etal.
(2007) indicates when a particular n-gram isnot found in the model, and returns a shortenedn-gram or (?state?)
that represents this shortenedcondition.
We use this state to identify the left andright chart item histories, thus reducing the numberof equivalence classes per cell.Following Venugopal et al (2007), we also cal-culate an estimate for the quality of each chartitem?s left state based on the words representedwithin the state (since we cannot know the tar-get words that might precede this item in the fi-nal translation).
This estimate is only used duringCube-Pruning to limit the number of chart itemsgenerated.The extensions above allows us to experimentwith the same order of n-gram LMs used in state-of-the-art phrase based systems.
While experi-ments in this work include up to 5-gram mod-els, we have succesfully run these PSCFG systemswith higher order n-gram LM models as well.4 Experiments4.1 Chinese-English and Arabic-EnglishWe report experiments on three data configura-tions.
The first configuration (Full) uses all thedata (both bilingual and monolingual) data avail-able for the NIST 2008 large track translationtask.
The parallel training data comprises of 9.1Msentence pairs (223M Arabic words, 236M En-glish words) for Arabic-English and 15.4M sen-tence pairs (295M Chinese Words, 336M Englishwords) for Chinese-English.
This configuration(for both Chinese-English and Arabic-English) in-cludes three 5-gram LMs trained on the target sideof the parallel data (549M tokens, 448M 1..5-grams), the LDC Gigaword corpus (3.7B tokens,2.9B 1..5-grams) and the Web 1T 5-Gram Cor-pus (1T tokens, 3.8B 1..5-grams).
The secondconfiguration (TargetLM) uses a single languagemodel trained only on the target side of the paral-lel training text to compare approaches with a rela-tively weaker n-gram LM.
The third configurationis a simulation of a low data scenario (10%TM),where only 10% of the bilingual training data isused, with the language model from the TargetLMconfiguration.
Translation quality is automaticallyevaluated by the IBM-BLEU metric (Papineni etal., 2002) (case-sensitive, using length of the clos-est reference translation) on the following publicly1148Ch.-En.
System \%BLEU Dev (MT04) MT02 MT03 MT05 MT06 MT08 TstAvgFULLPhraseb.
reo=4 37.5 38.0 38.9 36.5 32.2 26.2 34.4Phraseb.
reo=7 40.2 40.3 41.1 38.5 34.6 27.7 36.5Phraseb.
reo=12 41.3* 41.0 41.8 39.4 35.2 27.9 37.0Hier.
41.6* 40.9 42.5 40.3 36.5 28.7 37.8SAMT 41.9* 41.0 43.0 40.6 36.5 29.2 38.1TARGET-LMPhraseb.
reo=4 35.9* 36.0 36.0 33.5 30.2 24.6 32.1Phraseb.
reo=7 38.3* 38.3 38.6 35.8 31.8 25.8 34.1Phraseb.
reo=12 39.0* 38.7 38.9 36.4 33.1 25.9 34.6Hier.
38.1* 37.8 38.3 36.0 33.5 26.5 34.4SAMT 39.9* 39.8 40.1 36.6 34.0 26.9 35.5TARGET-LM, 10%TMPhraseb.
reo=12 36.4* 35.8 35.3 33.5 29.9 22.9 31.5Hier.
36.4* 36.5 36.3 33.8 31.5 23.9 32.4SAMT 36.5* 36.1 35.8 33.7 31.2 23.8 32.1Ar.-En.
System \%BLEU Dev (MT04) MT02 MT03 MT05 MT06 MT08 TstAvgFULLPhraseb.
reo=4 51.7 64.3 54.5 57.8 45.9 44.2 53.3Phraseb.
reo=7 51.7* 64.5 54.3 58.2 45.9 44.0 53.4Phraseb.
reo=9 51.7 64.3 54.4 58.3 45.9 44.0 53.4Hier.
52.0* 64.4 53.5 57.5 45.5 44.1 53.0SAMT 52.5* 63.9 54.2 57.5 45.5 44.9 53.2TARGET-LMPhraseb.
reo=4 49.3 61.3 51.4 53.0 42.6 40.2 49.7Phraseb.
reo=7 49.6* 61.5 51.9 53.2 42.8 40.1 49.9Phraseb.
reo=9 49.6 61.5 52.0 53.4 42.8 40.1 50.0Hier.
49.1* 60.5 51.0 53.5 42.0 40.0 49.4SAMT 48.3* 59.5 50.0 51.9 41.0 39.1 48.3TARGET-LM, 10%TMPhraseb.
reo=7 47.7* 59.4 50.1 51.5 40.5 37.6 47.8Hier.
46.7* 58.2 48.8 50.6 39.5 37.4 46.9SAMT 45.9* 57.6 48.7 50.7 40.0 37.3 46.9Table 1: Results (% case-sensitive IBM-BLEU) for Ch-En and Ar-En NIST-large.
Dev.
scores with * indicate that the param-eters of the decoder were MER-tuned for this configuration and also used in the corresponding non-marked configurations.available NIST test corpora: MT02, MT03, MT05,MT06, MT08.
We used the NIST MT04 corpusas development set to train the model parameters?.
All of the systems were evaluated based on theargmax decision rule.
For the purposes of stablecomparison across multiple test sets, we addition-ally report a TstAvg score which is the average ofall test set scores.2Table 1 shows results comparing phrase-based,hierarchical and SAMT systems on the Chinese-English and Arabic-English large-track NIST 2008tasks.
Our primary goal in Table 1 is to evaluatethe relative impact of the PSCFG methods abovethe phrase-based approach, and to verify that theseimprovements persist with the use of of large n-gram LMs.
We also show the impact of largerreordering capability under the phrase-based ap-proach, providing a fair comparison to the PSCFGapproaches.2We prefer this over taking the average over the aggregatetest data to avoid artificially generous BLEU scores due tolength penalty effects resulting from e.g.
being too brief in ahard test set but compensating this by over-generating in aneasy test set.Chinese-to-English configurations: We seeconsistent improvements moving from phrase-based models to PSCFG models.
This trendholds in both LM configurations (Full and Tar-getLM) as well as the 10%TM case, with the ex-ception of the hierarchical system for TargetLM,which performs slightly worse than the maximum-reordering phrase-based system.We vary the reordering limit ?reo?
for thephrase-based Full and TargetLM configurationsand see that Chinese-to-English translation re-quires significant reordering to generate fluenttranslations, as shown by the TstAvg difference be-tween phrase-based reordering limited to 4 words(34.4) and 12 words (37.0).
Increasing the reorder-ing limit beyond 12 did not yield further improve-ment.
Relative improvements over the most capa-ble phrase-based model demonstrate that PSCFGmodels are able to model reordering effects moreeffectively than our phrase-based approach, evenin the presence of strong n-gram LMs (to aid thedistortion models) and comparable reordering con-straints.1149Our results with hierarchical rules are consis-tent with those reported in Chiang (2007), wherethe hierarchical system uses a reordering limit of10 (implicit in the maximum length of the initialphrase pairs used for the construction of the rules,and the decoder?s maximum source span length,above which only the glue rule is applied) and iscompared to a phrase-based system with a reorder-ing limit of 7.Arabic-to-English configurations: Neither thehierarchical nor the SAMT system show consis-tent improvements over the phrase-based baseline,outperforming the baseline on some test sets, butunderperforming on others.
We believe this is dueto the lack of sufficient reordering phenomena be-tween the two languages, as evident by the mini-mal TstAvg improvement the phrase-based systemcan achieve when increasing the reordering limitfrom 4 words (53.3) to 9 words (53.4).N-Gram LMs: The impact of using addi-tional language models in configuration Full in-stead of only a target-side LM (configuration Tar-getLM) is clear; the phrase-based system improvesthe TstAvg score from 34.6 to 37.0 for Chinese-English and from 50.0 to 53.4 for Arabic-English.Interestingly, the hierarchical system and SAMTbenefit from the additional LMs to the same extent,and retain their relative improvement compared tothe phrase-based system for Chinese-English.Expressiveness: In order to evaluate how muchof the improvement is due to the relatively weakerexpressiveness of the phrase-based model, we triedto regenerate translations produced by the hierar-chical system with the phrase-based decoder bylimiting the phrases applied during decoding tothose matching the desired translation (?forcedtranslation?).
By forcing the phrase-based systemto follow decoding hypotheses consistent with aspecific target output, we can determine whetherthe phrase-based system could possibly generatethis output.
We used the Chinese-to-English NISTMT06 test (1664 sentences) set for this experi-ment.
Out of the hierarchical system?s translations,1466 (88%) were generable by the phrase-basedsystem.
The relevant part of a sentence for whichthe hierarchical translation was not phrase-basedgenerable is shown in Figure 1.
The reason for thefailure to generate the translation is rather unspec-tacular: While the hierarchical system is able todelete the Chinese word meaning ?already?
usingthe rule spanning [27-28], which it learned by gen-eralizing a training phrase pair in which ?already?was not explicitly represented in the target side, thephrase-based system has to account for this Chi-nese word either directly or in a phrase combiningthe previous word (Chinese for ?epidemic?)
or fol-lowing word (Chinese for ?outbreak?
).Out of the generable forced translations, 1221(83%) had a higher cost than the phrase-based sys-tem?s preferred output; in other words, the factthat the phrase-based system does not prefer theseforced translations is mainly inherent in the modelrather than due to search errors.These results indicate that a phrase-based sys-tem with sufficiently powerful reordering featuresand LM might be able to narrow the gap to a hier-archical system.System \ %BLEU Dev MT08Phr.b.
reo=4 12.8 18.1Phr.b.
reo=7 14.2 19.9Phr.b.
reo=10 14.8* 20.2Phr.b.
reo=12 15.0 20.1Hier.
16.0* 22.1SAMT 16.1* 22.6Table 2: Translation quality (% case-sensitive IBM-BLEU)for Urdu-English NIST-large.
We mark dev.
scores with *to indicate that the parameters of the corresponding decoderwere MER-tuned for this configuration.4.2 Urdu-EnglishTable 2 shows results comparing phrase-based,hierarchical and SAMT system on the Urdu-English large-track NIST 2008 task.
Systems weretrained on the bilingual data provided by the NISTcompetition (207K sentence pairs; 2.2M Urduwords / 2.1M English words) and used a n-gramLM estimated from the English side of the paralleldata (4M 1..5-grams).
We see clear improvementsmoving from phrase-based to hierarchy, and addi-tional improvements from hierarchy to syntax.
Aswith Chinese-to-English, longer-distance reorder-ing plays an important role when translating fromUrdu to English (the phrase-based system is ableto improve the test score from 18.1 to 20.2), andPSCFGs seem to be able to take this reorderingbetter into account than the phrasal distance-basedand lexical-reordering models.4.3 Are all rules important?One might assume that only a few hierarchicalrules, expressing reordering phenomena based oncommon words such as prepositions, are sufficientto obtain the desired gain in translation quality1150Figure 1: Example from NIST MT06 for which the hierarchical system?s first best hypothesis was not generable by the phrase-based system.
The hierarchical system?s decoding parse tree contains the translation in its leaves in infix order (shaded).
Eachnon-leaf node denotes an applied PSCFG rule of the form: [Spanned-source-positions:Left-hand-side->source/target]Ch.-En.
System \%BLEU Dev (MT04) MT02 MT03 MT05 MT06 MT08 TstAvgPhraseb.
41.3* 41.0 41.8 39.4 35.2 27.9 37.0Hier.
default (mincount=3) 41.6* 40.9 42.5 40.3 36.5 28.7 37.8Hier.
mincount=4 41.4 41.0 42.5 40.4 36.1 28.4 37.7Hier.
mincount=8 41.0 41.0 42.0 40.5 35.7 27.8 37.4Hier.
mincount=16 40.7 40.3 41.5 40.0 35.2 27.8 37.0Hier.
mincount=32 40.4 40.0 41.5 39.5 34.8 27.5 36.6Hier.
mincount=64 39.8 40.0 40.9 39.1 34.6 27.3 36.4Hier.
mincount=128 39.4 39.8 40.3 38.7 34.0 26.6 35.9Hier.
1NT 40.1* 39.8 41.1 39.1 35.1 28.1 36.6Urdu-En.
System \%BLEU Dev MT08Phraseb.
15.0* 20.1Hier.
default (mincount=2) 16.0* 22.1Hier.
mincount=4 15.7 22.0Hier.
mincount=8 15.4 21.5Hier.
mincount=16 15.1 21.3Hier.
mincount=32 14.9 20.7Hier.
mincount=64 14.6 20.1Hier.
mincount=128 14.4 19.6Hier.
1NT 15.3* 20.8Table 3: Translation quality (% case-sensitive IBM-BLEU) for Chinese-English and Urdu-English NIST-large when restrictingthe hierarchical rules.
We mark dev.
scores with * to indicate that the parameters of the corresponding decoder were MER-tunedfor this configuration.over a phrase-based system.
Limiting the numberof rules used could reduce search errors caused byspurious ambiguity during decoding.
Potentially,hierarchical rules based on rare phrases may notbe needed, as these phrase pairs can be substitutedinto the nonterminal spots of more general andmore frequently encountered hierarchical rules.As Table 3 shows, this is not the case.
Inthese experiments for Hier., we retained all non-hierarchical rules (i.e., phrase pairs) but removedhierarchical rules below a threshold ?mincount?.Increasing mincount to 16 (Chinese-English) or 64(Urdu-English), respectively, already deterioratesperformance to the level of the phrase-based sys-tem, demonstrating that the highly parameterizedreordering model implicit in using more rules doesresult in benefits.
This immediate reduction intranslation quality when removing rare rules canbe explained by the following effect.
Unlike ina phrase-based system, where any phrase can po-tentially be reordered, rules in the PSCFG mustcompose to generate sub-translations that can bereordered.
Removing rare rules, even those thatare highly lexicalized and do not perform any re-ordering (but still include nonterminal symbols),increases the likelihood that the glue rule is appliedsimply concatenating span translations without re-ordering.Removing hierarchical rules occurring at mosttwice (Chinese-English) or once (Urdu-English),respectively, did not impact performance, and ledto a significant decrease in rule table size and de-coding speed.We also investigate the relative impact of the1151rules with two nonterminals, over using rules witha single nonterminal.
Using two nonterminals al-lows more lexically specific reordering patterns atthe cost of decoding runtime.
Configuration ?Hier.1NT?
represents a hierarchical system in whichonly rules with at most one nonterminal pair areextracted instead of two as in Configuration ?Hier.default?.
The resulting test set score drop is morethan one BLEU point for both Chinese-to-Englishand Urdu-to-English.5 ConclusionIn this work we investigated the value of PSCFGapproaches built upon state-of-the-art phrase-based systems.
Our experiments show that PSCFGapproaches can yield substantial benefits for lan-guage pairs that are sufficiently non-monotonic.Suprisingly, the gap (or non-gap) between phrase-based and PSCFG performance for a given lan-guage pair seems to be consistent across small andlarge data scenarios, and for weak and strong lan-guage models alike.
In sufficiently non-monotoniclanguages, the relative improvements of phrase-based systems persist when compared against astate-of-the art phrase-based system that is capableof equally long reordering operations modeled bya lexicalized distortion model and a strong n-gramlanguage model.
We hope that this work addressesseveral of the important questions that the researchcommunity has regarding the impact and value ofthese PSCFG approaches.AcknowledgmentsWe thank Richard Zens and the anonymous re-viewers for their useful comments and sugges-tions.ReferencesBrants, Thorsten, Ashok C. Popat, Peng Xu, Franz J.Och, and Jeffrey Dean.
2007.
Large language mod-els in machine translation.
In Proc.
of EMNLP-CoNLL.Charniak, Eugene.
2000.
A maximum entropy-inspired parser.
In Proc.
of HLT/NAACL.Chiang, David.
2005.
A hierarchical phrase-basedmodel for statistical machine translation.
In Proc.of ACL.Chiang, David.
2007.
Hierarchical phrase based trans-lation.
Computational Linguistics, 33(2):201?228.DeNeefe, Steve, Kevin Knight, Wei Wang, and DanielMarcu.
2007.
What can syntax-based MT learnfrom phrase-based MT?
In Proc.
of EMNLP-CoNLL.Huang, Liang and David Chiang.
2005.
Better k-bestparsing.
In Proc.
of IWPT.Huang, Liang and David Chiang.
2007.
Forest rescor-ing: Faster decoding with integrated language mod-els.
In Proc.
of ACL.Koehn, Philipp, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proc.of HLT/NAACL.Koehn, Philipp, Franz Josef Och, and Daniel Marcu.2004.
Pharaoh: A beam search decoder for phrase-base statistical machine translation models.
In Proc.of AMTA.Marcu, Daniel, Wei Wang, Abdessamad Echihabi,and Kevin Knight.
2006.
SPMT: Statistical Ma-chine Translation with Syntactified Target LanguagePhrases.
In Proc.
of EMNLP.Och, Franz and Hermann Ney.
2004.
The alignmenttemplate approach to statistical machine translation.Computational Linguistics, 30(4):417?449.Och, Franz Josef, Christoph Tillmann, and HermannNey.
1999.
Improved alignment models for statis-tical machine translation.
In Proc.
of EMNLP.Och, Franz Josef.
2003.
Minimum error rate trainingin statistical machine translation.
In Proc.
of ACL.Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proc.
of ACL.Steedman, Mark.
1999.
Alternative quantifier scope inCCG.
In Proc.
of ACL.Venugopal, Ashish, Andreas Zollmann, and VogelStephan.
2007.
An efficient two-pass approach tosynchronous-CFG driven statistical MT.
In Proc.
ofHLT/NAACL.Zens, Richard and Hermann Ney.
2006.
Discrimina-tive reordering models for statistical machine trans-lation.
In Proc.
of the Workshop on Statistical Ma-chine Translation, HLT/NAACL.Zollmann, Andreas and Ashish Venugopal.
2006.
Syn-tax augmented machine translation via chart pars-ing.
In Proc.
of the Workshop on Statistical MachineTranslation, HLT/NAACL.1152
