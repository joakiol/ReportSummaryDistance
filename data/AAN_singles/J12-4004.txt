Language Models for Machine Translation:Original vs.
Translated TextsGennadi Lembersky?University of HaifaNoam Ordan?University of HaifaShuly Wintner?University of HaifaWe investigate the differences between language models compiled from original target-languagetexts and those compiled from texts manually translated to the target language.
Corroboratingestablished observations of Translation Studies, we demonstrate that the latter are significantlybetter predictors of translated sentences than the former, and hence fit the reference set better.Furthermore, translated texts yield better language models for statistical machine translationthan original texts.1.
IntroductionStatistical machine translation (MT) uses large target languagemodels (LMs) to improvethe fluency of generated texts, and it is commonly assumed that for constructing lan-guage models, ?more data is better data?
(Brants and Xu 2009).
Not all data, however,are created the same.
In this work we explore the differences between language modelscompiled from texts originally written in the target language (O) and language modelscompiled from translated texts (T).This work is motivated by much research in Translation Studies that suggeststhat original texts are significantly different from translated ones in various aspects(Gellerstam 1986).
Recently, corpus-based computational analysis corroborated thisobservation, and Kurokawa, Goutte, and Isabelle (2009) apply it to statistical machinetranslation, showing that for an English-to-French MT system, a translation modeltrained on an English-translated-to-French parallel corpus is better than one trainedon French-translated-to-English texts.
The main research question we investigate hereis whether a language model compiled from translated texts may similarly improve theresults of machine translation.We test this hypothesis on several translation tasks, including translation fromseveral languages to English, and two additional tasks where the target language is?
Department of Computer Science, University of Haifa, 31905 Haifa, Israel.E-mails: glembers@campus.haifa.ac.il, noam.ordan@gmail.com, shuly@cs.haifa.ac.il.Submission received: 22 August 2011; revised submission received: 25 December 2011; accepted forpublication: 31 January 2012?
2012 Association for Computational LinguisticsComputational Linguistics Volume 38, Number 4not English.
For each language pair we build two language models from two types ofcorpora: texts originally written in the target language, and human translations fromthe source language into the target language.
We show that for each language pair, thelatter language model better fits a set of reference translations in terms of perplexity.
Wealso demonstrate that the differences between the two LMs are not biased by content,but rather reflect differences on abstract linguistic features.Research in Translation Studies holds a dual view on translationese, the sub-language of translated texts.
On the one hand, there is a claim for so-called transla-tion universals, traits of translationese which occur in any translated text irrespectiveof the source language.
Others hold, on the other hand, that each source language?spills over?
to the target text, and therefore creates a sub-translationese, the resultof a pair-specific encounter between two specific languages.
If both these claims aretrue then language models based on translations from the source language shouldbest fit target language reference sentences, and language models based on transla-tions from other source languages should fit reference sentences to a lesser extent yetoutperform originally written texts.
To test this hypothesis, we compile additionalEnglish LMs, this time using texts translated to English from languages other thanthe source.
Again, we use perplexity to assess the fit of these LMs to reference setsof translated-to-English sentences.
We show that these LMs depend on the sourcelanguage and differ from each other.
Whereas they outperform O-based LMs, LMscompiled from texts that were translated from the source language still fit the referenceset best.Finally, we train phrase-based MT systems (Koehn, Och, and Marcu 2003) for eachlanguage pair.
We use four types of LMs: original; translated from the source language;translated from other languages; and a mixture of translations from several languages.We show that the translated-from-source-language LMs provide a significant improve-ment in the quality of the translation output over all other LMs, and that the mixtureLMs always outperform the original LMs.
This improvement persists even when theoriginal LMs are up to ten times larger than the translated ones.
In other words, onehas to collect ten times more original material in order to reach the same quality as isprovided with translated material.It is important to emphasize that translated texts abound: in fact, Pym and Chrupa?a(2005) show (quantitatively!)
that the rate of translations into a language is inverselyproportional to the number of books published in that language: So whereas in Englishonly around 2% of texts published are translations, in languages such as Albanian,Arabic, Danish, Finnish, or Hebrew translated texts constitute between 20% and25% of the total publications.
Furthermore, such data can be automatically identified(see Section 2).
The practical impact of our work on MT is therefore potentiallydramatic.The main contributions of this work are thus a computational corroboration of thefollowing hypotheses:1.
Original and translated texts exhibit significant, measurable differences.2.
LMs compiled from translated texts better fit translated references thanLMs compiled from original texts of the same (and much larger) size (and,to a lesser extent, LMs compiled from texts translated from languagesother than the source language).3.
MT systems that use LMs based on manually translated texts significantlyoutperform LMs based on originally written texts.800Lembersky, Ordan, and Wintner Language Models for Machine TranslationThis article1 is organized as follows: Section 2 provides background and describesrelated work.
We explain our experimental set-up, research methodology and resourcesin Section 3 and detail our experiments and results in Section 4.
Section 5 discusses theresults and their implications, and suggests directions for future research.2.
Background and Related WorkNumerous studies suggest that translated texts are different from original ones.Gellerstam (1986) compares texts written originally in Swedish and texts translatedfrom English into Swedish.
He notes that the differences between them do not indicatepoor translation but rather a statistical phenomenon, which he terms translationese.
Hefocuses mainly on lexical differences, for example, less colloquialism in the translations,or foreign words used in the translations ?with new shades of meaning taken fromthe English lexeme?
(page 91).
Only later studies consider grammatical differences(see, e.g., Santos 1995).
The features of translationese were theoretically organizedunder the terms laws of translation and translation universals.Toury (1980, 1995) distinguishes between two laws: the law of interference and thelaw of growing standardization.
The law of interference pertains to the fingerprintsof the source text that are left in the translation product.
The law of standardizationpertains to the effort to standardize the translation product according to existing normsin the target language (and culture).
Interestingly, these two laws are in fact reflected inthe architecture of statistical machine translation: Interference in the translation modeland standardization in the language model.The combined effect of these laws creates a hybrid text that partly corresponds tothe source text and partly to texts written originally in the target language, but in factbelongs to neither (Frawley 1984).
Baker (1993, 1995, 1996) suggests several candidatesfor translation universals, which are claimed to appear in any translated text, regardlessof the source language.
These include simplification, the tendency of translated texts tosimplify the language, the message or both; and explicitation, their tendency to spellout implicit utterances that occur in the source text.During the 1990s, corpora were used extensively to study translationese.
For exam-ple, Al-Shabab (1996) shows that translated texts exhibit lower lexical variety (type-to-token ratio) and Laviosa (1998) shows that their mean sentence length is lower, as istheir lexical density (ratio of content to non-content words).
These studies, although notconclusive, provide some evidence for the simplification hypothesis.Baroni and Bernardini (2006) use machine learning techniques to distinguish be-tween original and translated Italian texts, reporting 86.7% accuracy.
They manage toabstract from content and perform the task using onlymorpho-syntactic cues.
Ilisei et al(2010) perform the same task for Spanish but enhance it theoretically in order to checkthe simplification hypothesis.
They first use a set of features which seem to capture?general?
characteristics of the text (ratio of grammatical words to content words); theythen add another set of features, each of which relates to the simplification hypothesis.Finally, they remove each ?simplification feature?
in turn and evaluate its contributionto the classification task.
The most informative features are lexical variety, sentencelength, and lexical density.1 Preliminary results were published in Lembersky, Ordan, and Wintner (2011).
This is an extended,revised version of that paper, providing fuller data and reporting on more language pairs.
Someexperiments (in particular, Section 4.2.3) are completely new, as is the bulk of the discussion inSection 5, including the human evaluation.801Computational Linguistics Volume 38, Number 4van Halteren (2008) focuses on six languages from Europarl (Koehn 2005): Dutch,English, French, German, Italian, and Spanish.
For each of these languages, a parallelsix-lingual subcorpus is extracted, including an original text and its translations into theother five languages.
The task is to identify the source language of translated texts, andthe reported results are excellent.
This finding is crucial: as Baker (1996) states, transla-tions do resemble each other; in accordance with the law of interference, however, thestudy of van Halteren (2008) suggests that translation from different source languagesconstitute different sublanguages.
As we show in Section 4.2, LMs based on translationsfrom the source language outperform LMs compiled from non-source translations, interms of both fitness to the reference set and improving MT.Kurokawa, Goutte, and Isabelle (2009) show that the direction of translation affectsthe performance of statistical MT.
They train systems to translate between French andEnglish (and vice versa) using a French-translated-to-English parallel corpus, and thenan English-translated-to-French one.
They find that in translating into French it isbetter to use the latter parallel corpus, and when translating into English it is betterto use the former.
Whereas they address the translationmodel, we focus on the languagemodel in this work.
We show that using a language model trained on a text translatedfrom the source language of the MT system does indeed improve the results of thetranslation.3.
Methodology and Resources3.1 HypothesesWe investigate the following three hypotheses:1.
Translated texts differ from original texts.2.
Texts translated from one language differ from texts translated from otherlanguages.3.
LMs compiled from manually translated texts are better for MT than LMscompiled from original texts.We test our hypotheses by considering translations from several languages toEnglish, and from English to German and French.
For each language pair we create areference set comprising several thousands of sentences written originally in the sourcelanguage and manually translated to the target language.
Section 3.4 provides detailson the reference sets.To investigate the first hypothesis, we train two LMs for each language pair, onecreated from texts originally written in the language (O-based) and the other from textstranslated into the target language (T-based).
Then, we check which LM better fits thereference set.Fitness of a language model to a set of sentences is measured in terms of perplexity(Jelinek et al 1977; Bahl, Jelinek, and Mercer 1983).
Given a language model and a test(reference) set, perplexity measures the predictive power of the language model overthe test set, by looking at the average probability the model assigns to the test data.Intuitively, a better model assigns higher probability to the test data, and consequentlyhas a lower perplexity; it is less surprised by the test data.
Formally, the perplexity PP of802Lembersky, Ordan, and Wintner Language Models for Machine Translationa language model L on a test setW = w1 w2 .
.
.wN is the probability ofW normalized bythe number of words N (Jurafsky and Martin 2008, page 96):PP(L,W) = N???
?N?i=11PL(wi|w1 .
.
.wi?1)(1)For the second hypothesis, we extend the experiment to LMs created from textstranslated from other languages.
For example, we test how well an LM trained onFrench-translated-to-English texts fits the German-translated-to-English reference set;and how well an LM trained on German-translated-to-English texts fits the French-translated-to-English reference set.Finally, for the third hypothesis, we use these LMs for statistical MT (SMT).
Foreach language pair we build several SMT systems.
All systems use a translation modelextracted from a parallel corpus which is oblivious to the direction of the translation;and one of the above-mentioned LMs.
Then, we compare the translation quality of thesesystems in terms of the Bleu metric (Papineni et al 2002) (as we show in Section 5.1,other automatic evaluation metrics reveal the same pattern).3.2 Language ModelsIn all the experiments, we use SRILM (Stolcke 2002) with interpolated modified Kneser-Ney discounting (Chen 1998) and no cut-off on all n-grams, to train n-gram languagemodels from various corpora.
Unless mentioned otherwise, n = 4.
We limit languagemodels to a fixed vocabulary and map out-of-vocabulary (OOV) tokens to a uniquesymbol to better control the OOV rates among various corpora.
We experimented withtwo techniques for setting the vocabulary: Use all words that occur more than oncein the evaluation set (see Section 3.4); and use the intersection of all words occurringin all corpora used to train the language model.
Both techniques produce very similarresults, and for brevity we only report the results achieved with the former technique.In addition, we tried various discounting schemes (e.g., Good-Turing smoothing [Chen1998]), and also ran experiments with an open vocabulary.
The results of all theseexperiments are consistent with our findings, and therefore we do not elaborate onthem here.Our main corpus is Europarl (Koehn 2005), specifically, portions collected overthe years 1996?1999 and 2001?2009.
This is a large multilingual corpus, containingsentences translated from several European languages.
It is organized as a collectionof bilingual corpora rather than as a single multilingual one, however, and it is hard toidentify sentences that are translated into several languages.We therefore treat each bilingual subcorpus in isolation; each such subcorpus con-tains sentences translated to English from various languages.
We rely on the languageattribute of the speaker tag to identify the source language of sentences in the Englishpart of the corpus.
Because this tag is rarely used with English-language speakers,we also exploit the ID attribute of the speaker tag, which we match against the listof British members of the European parliament.22 We wrote a small script that determines the original language of Europarl utterances in this way.
Thescript is publicly available.803Computational Linguistics Volume 38, Number 4Table 1Europarl English-target corpus statistics, translation from Lang.
to English.German?EnglishLang.
Sentences Tokens LengthMIX 82,700 2,325,261 28.1O-EN 91,100 2,324,745 25.5T-DE 87,900 2,322,973 26.4T-FR 77,550 2,325,183 30.0T-IT 65,199 2,325,996 35.7T-NL 94,000 2,323,646 24.7French?EnglishLang.
Sentences Tokens LengthMIX 90,700 2,546,274 28.1O-EN 99,300 2,545,891 25.6T-DE 94,900 2,546,124 26.8T-FR 85,750 2,546,085 29.7T-IT 72,008 2,546,984 35.4T-NL 103,350 2,545,645 24.6Italian?EnglishLang.
Sentences Tokens LengthMIX 87,040 2,534,793 29.1O-EN 93,520 2,534,892 27.1T-DE 90,550 2,534,867 28.0T-FR 82,930 2,534,930 30.6T-IT 69,270 2,535,225 36.6T-NL 96,850 2,535,053 26.2Dutch?EnglishLang.
Sentences Tokens LengthMIX 90,500 2,508,265 27.7O-EN 97,000 2,475,652 25.5T-DE 94,200 2,503,354 26.6T-FR 86,600 2,523,055 29.1T-IT 73,541 2,518,196 34.2T-NL 101,950 2,513,769 24.7We focus on the following languages: German (DE), French (FR), Italian (IT), andDutch (NL).
For each of these languages, L, we consider the L-English Europarl subcor-pus.
In each subcorpus, we extract chunks of approximately 2.5 million English tokenstranslated from each of these source languages (T-DE, T-FR, T-IT, and T-NL), as wellas sentences written originally in English (O-EN).
The mixture corpus (MIX), whichis designed to represent ?general?
translated language, is constructed by randomlyselecting sentences translated from any language (excluding original sentences).
ForEnglish-to-German and English-to-French, we use the German?English and French?English Europarl sub-corpora.
We extract German (and French) sentences translatedfrom English, French (or German), Italian, and Dutch, as well as sentences originallywritten in German (or French).Table 1 lists the number of sentences, number of tokens, and average sentencelength for each English subcorpus and each original language.
Table 2 lists the statisticsfor German and French corpora.Table 2Europarl corpus statistics, translation from Lang.
to German and French.English?GermanLang.
Sentences Tokens LengthMIX 81,447 2,215,044 27.2O-DE 89,739 2,215,036 24.7T-EN 88,081 2,215,040 25.2T-FR 77,555 2,215,021 28.6T-IT 64,374 2,215,030 34.4T-NL 94,289 2,215,033 23.5English?FrenchLang.
Sentences Tokens LengthMIX 89,660 2,845,071 31.7O-FR 89,875 2,844,265 31.6T-EN 96,057 2,847,238 29.6T-DE 93,468 2,843,730 30.4T-IT 73,257 2,848,931 38.9T-NL 102,498 2,835,006 27.7804Lembersky, Ordan, and Wintner Language Models for Machine TranslationTable 3Hansard corpus statistics.Original FrenchSize Sentences Tokens Length1M 54,851 1,000,076 18.25M 276,187 5,009,157 18.110M 551,867 10,001,716 18.1Original EnglishSize Sentences Tokens Length1M 54,216 1,006,275 18.65M 268,806 5,006,482 18.610M 537,574 10,004,191 18.625M 1,344,580 25,001,555 18.650M 2,689,332 50,009,861 18.6100M 5,376,886 100,016,704 18.6In another set of experiments we address the size of language models, to assesshow much more original material is needed compared with translated material (Sec-tion 4.2.2).
Because Europarl does not have enough trainingmaterial for this task, we usethe Hansard corpus, containing transcripts of the Canadian parliament from 1996?2007.This is a bilingual French?English corpus comprising about 80% original English texts(EO) and about 20% texts translated from French (FO).
We first separate original Englishtexts from texts translated from French and then, for each subcorpus, we randomlyextract portions of texts of different sizes: 1M, 5M, and 10M tokens from the FO corpusand 1M, 5M, 10M, 25M, 50M, and 100M tokens from the EO corpus; see Table 3.
For evenlarger amounts of data, we use the English Gigaword corpus (Graff and Cieri 2007),fromwhich we randomly extract portions of up to 1G tokens; see Table 4.
Unfortunately,we do not know how much of this corpus is original; because it includes data from theXinhua news agency, we suspect that parts of it are indeed translated.To experiment with a non-European language (and a different genre) we chooseHebrew (HE).
We use two English corpora: The original (O-EN) corpus comprisesarticles from the International Herald Tribune, downloaded over a period of sevenmonths(from January to July 2009).
The articles cover four topics: news (53.4%), business(20.9%), opinion (17.6%), and arts (8.1%).
The translated (T-HE) corpus consists ofarticles collected from the Israeli newspaper HaAretz over the same period of time.HaAretz is published in Hebrew, but portions of it are translated to English.
TheO-corpus was downsized in order for both subcorpora to have approximately the samenumber of tokens in each topic.
Table 5 lists basic statistics for this corpus.3.3 SMT Training DataTo focus on the effect of the language model on translation quality, we design SMTtraining corpora to be oblivious to the direction of translation.
Again, we use EuroparlTable 4Gigaword corpus statistics.English, various sourcesSize Sentences Tokens Length100M 4,448,260 107,483,194 24.2500M 20,797,060 502,380,054 24.21,000M 41,517,095 1,002,919,581 24.2805Computational Linguistics Volume 38, Number 4Table 5Hebrew-to-English corpus statistics.Hebrew?EnglishOrig.
Lang.
Sentences Tokens LengthO-EN 135,228 3,561,559 26.3T-HE 147,227 3,561,556 24.2(January 2000 to September 2000) as the main source of our parallel corpora.
We also usethe Hansard corpus: We randomly extract 50,000 sentences from the French-translated-to-English subcorpus and another 50,000 sentences from the original English sub-corpus.
For Hebrew we use the Hebrew?English parallel corpus (Tsvetkov andWintner2010) that contains sentences translated fromHebrew to English (54%) and from Englishto Hebrew (46%).
The English-to-Hebrew part comprises many short sentences (ap-proximately six tokens per sentence) taken from amovie subtitle database.
This explainsthe low average sentence length of this particular corpus.
Table 6 lists some details onthose corpora.3.4 Reference SetsThe reference sets have two uses.
First, they are used as the test sets in the experimentsthat measure the perplexity of the language models.
Second, in the MT experiments weuse them to randomly extract 1,000 sentences for tuning and 1,000 (different) sentencesfor evaluation.
All references are of course disjoint from the LM and training materials.For each language L we use the L-English subcorpus of Europarl (over the periodof October 2000 to December 2000).
For L-to-English translation tasks we only usesentences originally produced in L, and for English-to-L tasks we use sentences orig-inally written in English.
The Hansard reference set comprises only French-translated-to-English sentences.
The Hebrew-to-English reference set is an independent (disjoint)part of the Hebrew-to-English parallel corpus.
This set mostly comprises literary data(88.6%) and a small portion of news (11.4%).
All sentences are originally written inHebrew and are manually translated to English.
See Table 7 for the figures.Table 6Parallel corpora used for SMT training.Language pair Side Sentences Tokens LengthDE-ENDE 92,901 2,439,370 26.3EN 92,901 2,602,376 28.0FR-ENFR 93,162 2,610,551 28.0EN 93,162 2,869,328 30.8IT-ENIT 85,485 2,531,925 29.6EN 85,485 2,517,128 29.5NL-ENNL 84,811 2,327,601 27.4EN 84,811 2,303,846 27.2HansardFR 100,000 2,167,546 21.7EN 100,000 1,844,415 18.4HE-ENHE 95,912 726,512 7.6EN 95,912 856,830 8.9806Lembersky, Ordan, and Wintner Language Models for Machine Translation4.
Experiments and ResultsWe detail in this section the experiments performed to test the three hypotheses: thattranslated texts can be distinguished from original ones, and provide better languagemodels for other translated texts; that texts translated from other languages than thesource are still better predictors of translations than original texts (Section 4.1); and thatthese differences are important for SMT (Section 4.2).4.1 Translated vs.
Original texts4.1.1 Adequacy of O-based and T-based LMs.We begin with English as the target language.We train 1-, 2-, 3-, and 4-gram language models for each Europarl subcorpus, basedon the corpora described in Section 3.2.
For each language L, we compile a LM fromtexts translated (into English) from L; from texts translated from languages other thanL (including a mixture of such languages, MIX); and from texts originally written inEnglish.
The LMs are applied to the reference set of texts translated from L, and wecompute the perplexity: the fitness of the LM to the reference set.
Table 8 details theresults.
The lowest perplexity (reflecting the best fit) in each subcorpus is typeset inboldface, and the highest (worst fit) is italicized.These results overwhelmingly support our hypothesis.
For each language L, theperplexity of the language model that was created from L translations is lowest, fol-lowed immediately by the MIX LM.
Furthermore, the perplexity of the LM createdfrom originally-English texts is highest in all experiments (except the Dutch-to-Englishtranslation task, where the perplexity of the 2-gram LM created from texts translatedfrom Italian is slightly higher).
The perplexity of LMs constructed from texts translatedfrom languages other than L always lies between these two extremes: It is a better fitof the reference set than original texts, but not as good as texts translated from L (ormixture translations).
This gives rise to yet another hypothesis, namely, that translationsfrom typologically related languages form a similar ?translationese dialect,?
whereastranslations from more distant source languages form two different ?dialects?
in thetarget language (see Koppel and Ordan 2011).Table 7Reference sets.Language pair Side Sentences Tokens LengthDE-ENDE 6,675 161,889 24.3EN 6,675 178,984 26.8FR-ENFR 8,494 260,198 30.6EN 8,494 271,536 32.0IT-ENIT 2,269 82,261 36.3EN 2,269 78,258 34.5NL-ENNL 4,593 114,272 24.9EN 4,593 105,083 22.9EN-DEEN 8,358 215,325 25.8DE 8,358 214,306 25.6EN-FREN 4,284 108,428 25.3FR 4,284 125,590 29.3HansardFR 8,926 193,840 21.7EN 8,926 163,448 18.3HE-ENHE 7,546 102,085 13.5EN 7,546 126,183 16.7807Computational Linguistics Volume 38, Number 4Table 8Fitness of various LMs to the reference set.German to English translationsOrig.
Lang.
1-gram PPL 2-gram PPL 3-gram PPL 4-gram PPLMix 451.50 93.00 69.36 66.47O-EN 468.09 103.74 79.57 76.79T-DE 443.14 88.48 64.99 62.07T-FR 460.98 99.90 76.23 73.38T-IT 465.89 102.31 78.50 75.67T-NL 457.02 97.34 73.54 70.56French to English translationsOrig.
Lang.
1-gram PPL 2-gram PPL 3-gram PPL 4-gram PPLMix 472.05 99.04 75.60 72.68O-EN 500.56 115.48 91.14 88.31T-DE 486.78 108.50 84.39 81.41T-FR 463.58 94.59 71.24 68.37T-IT 476.05 102.69 79.23 76.36T-NL 490.09 110.67 86.61 83.55Italian to English translationsOrig.
Lang.
1-gram PPL 2-gram PPL 3-gram PPL 4-gram PPLMix 395.99 88.46 67.35 64.40O-EN 415.47 99.92 79.27 76.34T-DE 404.64 95.22 73.73 70.85T-FR 395.99 89.44 68.38 65.54T-IT 384.55 81.90 60.85 57.91T-NL 411.58 98.78 76.98 73.94Dutch to English translationsOrig.
Lang.
1-gram PPL 2-gram PPL 3-gram PPL 4-gram PPLMix 434.89 90.73 69.05 66.08O-EN 448.11 100.17 78.23 75.46T-DE 437.68 93.67 71.54 68.57T-FR 445.00 97.32 75.59 72.55T-IT 448.11 100.19 78.06 75.19T-NL 423.13 83.99 62.17 59.09Boldface = best fit; italics = worst fit.4.1.2 Linguistic Abstraction.
A possible explanation for the different perplexity resultsamong the LMs could be the specific content of the corpora used to compile the LMs.For example, onewould expect texts translated fromDutch to exhibit higher frequenciesof words such asAmsterdam or even canal.
This, indeed, is reflected by the lower (usuallylowest) number of OOV items in language models compiled from texts translated fromthe source language.As a specific example, the top five words that occur in the T-FR corpus and theevaluation set, but are absent from the O-EN corpus, are: biarritz, meat-and-bone,808Lembersky, Ordan, and Wintner Language Models for Machine Translationarmenian, ievoli, and ivorian.
The top five words that occur in the O-EN corpus, butare absent from the T-FR corpus, are: duhamel, paciotti, ivoirian, coke, and spds.
Ofthose, biarritz seems to be French-specific, but the other items seem more arbitrary.To rule out the possibility that the perplexity results are due to specific contentphenomena, and to further emphasize that the corpora are indeed structurally different,we conduct more experiments, in which we gradually abstract away from the domain-and content-specific features of the texts and emphasize their syntactic structure.
Wefocus on French-to-English, but the results are robust and consistent (we repeated thesame experiments for all language pairs, with very similar outcomes).First, we remove all punctuation to eliminate possible bias due to differences inpunctuation conventions.3 Then, we use the Stanford Named Entity Recognizer (Finkel,Grenager, and Manning 2005) to identify named entities, which we replace with aunique token (?NE?).
Next, we replace all nouns with their part-of-speech (POS) tag;we use the Stanford POS Tagger (Toutanova and Manning 2000).
Finally, for full lexicalabstraction, we replace all words with their POS tags, retaining only abstract syntacticstructures devoid of lexical content.At each step, we train six language models on O- and T-texts and apply themto the reference set (which is adapted to the same level of abstraction, of course).As the abstraction of the text increases, we also increase the order of the LMs: From4-grams for text without punctuation and NE abstraction, to 5-grams for noun abstrac-tion, to 8-grams for full POS abstraction.
In all cases we fix the LM vocabulary to onlycontain tokens that appear more than once in the ?abstracted?
reference set.
The results,depicted in Table 9, consistently show that the T-based LM is a better fit to the referenceset, albeit to a lesser extent.
The rightmost column specifies the improvement, in termsof perplexity, of each language model, compared with the worst-performing model.Although we do not show the details here, the same pattern is persistent in all the otherEuroparl languages we experiment with.4.1.3 More Language Pairs.
To further test the robustness of these phenomena, we repeatthese experiments with the Hebrew-to-English corpus and reference set, reflecting adifferent language family, a smaller corpus, and a different domain.
We train two4-gram language models on the O-EN and T-HE corpora.
We then apply the two LMsto the reference set and compute the perplexity.
The results are presented in Table 10.Again, the T-based LM is a better fit to the translated text than the O-based LM: Itsperplexity is lower by 12.8%.
We also repeat the abstraction experiments on the Hebrewscenario.
The results, depicted in Table 11, consistently show that the T-based LM is abetter fit to the reference set.Clearly, then, translated LMs better fit the references than original ones, and thedifferences can be traced back not just to (trivial) specific lexical choice, but also tosyntactic structure, as evidenced by the POS abstraction experiments.We further test our findings on other target languages, specifically English?Germanand English?French.
We train several 4-gram language models on the corpora specifiedin Table 2.
We then compute the perplexity of the German-translated-from-English andFrench-translated-from-English reference sets (see Section 3.4) with respect to theselanguage models.
Table 12 depicts the results; they are in complete agreement with ourhypothesis.3 In fact, there is reason to assume that punctuation constitutes part of the translationese effect.
Removingpunctuation therefore harms our cause of identifying this effect.809Computational Linguistics Volume 38, Number 4Table 9Fitness of O- vs. T-based LMs to the reference set (FR-EN), reflecting different abstraction levels.No PunctuationOrig.
Lang.
Perplexity Improvement (%)MIX 105.91 19.73O-EN 131.94T-DE 122.50 7.16T-FR 99.52 24.58T-IT 112.71 14.58T-NL 126.44 4.17NE AbstractionOrig.
Lang.
Perplexity Improvement (%)MIX 93.88 18.51O-EN 115.20T-DE 107.48 6.70T-FR 88.96 22.77T-IT 99.17 13.91T-NL 110.72 3.89Noun AbstractionOrig.
Lang.
Perplexity Improvement (%)MIX 36.02 11.34O-EN 40.62T-DE 38.67 4.81T-FR 34.75 14.46T-IT 36.85 9.30T-NL 39.44 2.91POS AbstractionOrig.
Lang.
Perplexity Improvement (%)MIX 7.99 2.66O-EN 8.20T-DE 8.08 1.47T-FR 7.89 3.77T-IT 8.00 2.47T-NL 8.11 1.11Boldface = best fit; italics = worst fit.4.1.4 Larger Language Models.
Can these phenomena be attributed to the relatively smallsize of the corpora we use?Will the perplexity of O texts converge to that of T texts whenmore data become available, or will the differences persist?
To address these questions,we use the (much larger) Hansard corpus and the (even larger) Gigaword corpus.
Wetrain 4-gram language models for each Hansard and Gigaword subcorpus described inSection 3.2.
We apply the LMs to the Hansard reference set, but also to the Europarlreference set, to examine the effect on out-of-domain (but similar genre) texts.
In bothcases we report perplexity (Table 13).810Lembersky, Ordan, and Wintner Language Models for Machine TranslationTable 10Fitness of O- vs. T-based LMs to the reference set (HE-EN).Hebrew to English translationsOrig.
Lang.
Perplexity Improvement (%)O-EN 187.26T-HE 163.23 12.83The results are fully consistent with our previous findings: In the case of theHansard reference set, a language model based on original texts must be up to tentimes larger to retain the low perplexity level of translated texts.
For example, whereas alanguagemodel compiled from 10million English-translated-from-French tokens yieldsa perplexity of 42.70 on the Hansard reference set, a LM compiled from original Englishtexts requires 100 million words to yield a similar perplexity of 43.70 on the samereference set.
The Gigaword LMs, which are trained on texts representing completelydifferent domains and genres, produce much higher (i.e., worse) perplexity in thisscenario.
In the case of the Europarl reference set, a language model based on originaltexts must be approximately five times larger (and a Gigaword language model approxi-mately twenty times larger) than a language model based on original texts to yield similarperplexity.Table 11Fitness of O- vs. T-based LMs to the reference set (HE-EN), reflecting different abstraction levels.No PunctuationOrig.
Lang.
Perplexity Improvement (%)O-EN 401.44T-HE 335.30 16.48NE AbstractionOrig.
Lang.
Perplexity Improvement (%)O-EN 298.16T-HE 251.39 15.69Noun AbstractionOrig.
Lang.
Perplexity Improvement (%)O-EN 81.92T-HE 72.34 11.70POS AbstractionOrig.
Lang.
Perplexity Improvement (%)O-EN 11.47T-HE 10.76 6.20811Computational Linguistics Volume 38, Number 44.2 Original vs.
Translated LMs for Machine Translation4.2.1 SMT Experiments.
The last hypothesis we test is whether a better fitting lan-guage model yields a better machine translation system.
In other words, we expectthe T-based LMs to outperform the O-based LMs when used as part of machinetranslation systems.
We construct German-to-English, English-to-German, French-to-English, French-to-German, Italian-to-English, and Dutch-to-English MT systems usingthe Moses phrase-based SMT toolkit (Koehn et al 2007).
The systems are trained onthe parallel corpora described in Section 3.3.
We use the reference sets (Section 3.4) asfollows: 1,000 sentences are randomly extracted for minimum error-rate training (Och2003), and another, disjoint set of 1,000 randomly selected sentences is used for evalu-ation.
Each system is built and tuned with six different LMs: MIX, O-based, and fourT-based models (Section 3.2).
We use Bleu (Papineni et al 2002) to evaluate translationquality.
The results are listed in Tables 14 and 15.The results are consistent and fully confirm our hypothesis.
Across all languagepairs, MT systems using LMs compiled from translated-from-source texts consistentlyoutperform all other systems.
Systems that use LMs compiled from texts originallywritten in the target language always perform worst or second worst.
We test the statis-tical significance of the differences between the results using the bootstrap resamplingmethod (Koehn 2004).
In all experiments, the best system (translated-from-source LM)is significantly better than the system that uses the O-based LM (p < 0.01).We now repeat the experiment with Hebrew to English translation.
We construct aHebrew-to-English MT system with Moses, using a factored translation model (Koehnand Hoang 2007).
Every token in the training corpus is represented as two factors:surface form and lemma.
The Hebrew input is fully segmented (Itai and Wintner 2008).The system is built and tuned with O- and T-based LMs.
The O-based LM yields aBleu score of 11.94, whereas using the T-based LM results in somewhat higher BleuTable 12Fitness of O- vs. T-based LMs to the reference set (EN-DE and EN-FR).English to German translationsOrig.
Lang.
Perplexity Improvement (%)Mix 106.37 20.24O-DE 133.37T-EN 99.39 25.47T-FR 119.21 10.61T-IT 123.35 7.51T-NL 119.99 10.03English to French translationsOrig.
Lang.
Perplexity Improvement (%)Mix 58.71 3.20O-FR 60.65T-EN 49.44 18.47T-DE 55.41 8.63T-IT 57.75 4.77T-NL 54.23 10.57812Lembersky, Ordan, and Wintner Language Models for Machine TranslationTable 13The effect of LM training corpus size on the fitness of LMs to the reference sets.Hansard Reference SetHansard T-FRSize Perplexity1M 64.685M 47.6310M 42.70Hansard O-ENSize Perplexity1M 91.405M 66.9510M 59.1925M 51.5950M 47.02100M 43.70GigawordSize Perplexity100M 165.03500M 151.001000M 145.88Europarl Reference SetHansard T-FRSize Perplexity1M 169.665M 137.7210M 128.65Hansard O-ENSize Perplexity1M 198.935M 162.0810M 150.0525M 137.3150M 129.43100M 123.10GigawordSize Perplexity100M 136.72500M 121.881000M 116.55score, 12.07, but the difference is not statistically significant (p = 0.18).
Presumably, thelow quality of both systems prevents the better LM frommaking a significant difference.4.2.2 Larger Language Models.
Again, the LMs used in the MT experiments reported hereare relatively small.
To assess whether the benefits of using translated LMs carry overto scenarios where larger original corpora exist, we build yet another set of French-to-English MT systems.
We use the Hansard SMT translation model and Hansard LMsto train nine MT systems, three with varying sizes of translated texts and six withvarying sizes of original texts.
We train additional MT systems with several subsetsof the Gigaword LM.
We tune and evaluate on the Hansard reference set.
In anotherset of experiments we use the Europarl French-to-English scenario (using Europarl813Computational Linguistics Volume 38, Number 4Table 14Machine translation with various LMs; English target language.DE to ENLM BleuMIX 21.43O-EN 21.10T-DE 21.90T-FR 21.16T-IT 21.29T-NL 21.20FR to ENLM BleuMIX 28.67O-EN 27.98T-DE 28.01T-FR 29.14T-IT 28.75T-NL 28.11IT to ENLM BleuMIX 25.41O-EN 24.69T-DE 24.62T-FR 25.37T-IT 25.96T-NL 24.77NL to ENLM BleuMIX 24.20O-EN 23.40T-DE 24.26T-FR 23.56T-IT 23.87T-NL 24.52Table 15Machine translation with various LMs; non-English target language.EN to DELM BleuMIX 13.00O-DE 12.47T-EN 13.10T-FR 12.46T-IT 12.65T-NL 12.86EN to FRLM BleuMIX 24.83O-FR 24.70T-EN 25.31T-DE 24.58T-IT 24.89T-NL 25.20corpora for the translation model as well as for tuning and evaluation), but we usethe Hansard and Gigaword LMs to see whether our findings are consistent also whenLMs are trained on out-of-domain material.Table 16 again demonstrates that language models compiled from original textsmust be up to ten times larger in order to yield translation quality similar to that ofLMs compiled from translated texts.4 In other words, much smaller translated LMsperform better than much larger original ones, and this holds for various LM sizes,both in-domain and out-of-domain.
For example, on the Hansard corpus, a 10-million-token T-FR language model yields a Bleu score of 34.67, whereas an O-EN languagemodel of 100 million tokens is required in order to yield a similar Bleu score of 34.44.The systems that use the Gigaword LMs perform much worse in-domain, even with alanguage model compiled from 1000M tokens.
Out-of-domain, the Gigaword systemsare better than O-EN, but they require approximately five times more data to match theperformance of T-FR systems.4.2.3 Enjoying Both Worlds.
The previous section established the fact that language mod-els compiled from translated texts are better for MT than ones compiled from originaltexts, even when the original LMs are much larger.
In many real-world scenarios,however, one has access to texts of both types.
Our results do not imply that original4 The table only specifies three subsets of the Gigaword corpus, but the graphs show more data points.Note that the x-axis is logarithmic.
Incidentally, the graphs show that increases in (Gigaword) corpus sizedo not monotonically translate to better MT quality.814Lembersky, Ordan, and Wintner Language Models for Machine TranslationTable 16The effect of LM size on MT performance.Hansard TM and TestHansard T-FRSize Bleu1M 33.035M 34.2510M 34.67Hansard O-ENSize Bleu1M 31.915M 33.2710M 33.4325M 33.4950M 34.29100M 34.44GigawordSize Bleu100M 31.77500M 32.311000M 32.51Europarl TM and TestHansard T-FRSize Bleu1M 26.365M 27.0610M 27.22Hansard O-ENSize Bleu1M 26.065M 26.0310M 26.7225M 26.7250M 27.01100M 27.04GigawordSize Bleu100M 27.47500M 27.711000M 27.69texts are useless, and that only translated ones should be used.
In this section we explorevarious ways to combine original and translated texts, thereby yielding even betterlanguage models.For these experiments we use 10 million English-translated-from-French tokensfrom the Hansard corpus (T-FR) and another 100 million original-English tokens fromthe same source (O-EN).
We combine them in five different ways: straightforwardconcatenation of the corpora; a concatenation of the original-English corpus with thetranslated corpus, upweighted by a factor of 10 and then of 20; log-linear modeling;and an interpolated language model.
In each experiment we report both the fitness ofthe LM to the reference set, in terms of perplexity, and the quality of machine translation815Computational Linguistics Volume 38, Number 4Table 17Various combinations of original and translated texts and their effect on perplexity (PPL) andtranslation quality (Bleu).Hansard TM, LM and TestCombination PPL BleuO-EN 43.70 34.44T-FR 42.70 34.67Concatenation 38.43 34.62Concatenation x10 41.15 35.09Concatenation x20 45.07 34.67Log-Linear LM ?
35.26Interpolated LM 36.69 35.35Europarl TM and Test; Hansard LMCombination PPL BleuO-EN 123.10 27.04T-FR 128.65 27.22Concatenation 116.71 27.14Concatenation x10 135.09 27.29Concatenation x20 152.02 27.09Log-Linear LM ?
27.30Interpolated LM 107.82 27.48that uses this LM, in terms of Bleu.5 We execute each experiment twice, once (in-domain)with the Hansard reference set and once (out-of-domain) where the translation model,tuning corpus, and reference set al come from the Europarl FR-EN subcorpus, as above.The results are listed in Table 17; we now provide a detailed explanation of theseexperiments.Concatenation of O and T texts.
We train three language models by concatenating theT-FR and O-EN corpora.
First, we simply concatenate the corpora obtaining 110 milliontokens.
Second, we upweight the T-FR corpus by a factor of 10 before the concatenation;and finally, we upweight the T-FR corpus by a factor of 20 before the concatenation.In the ?in-domain?
scenario, the LM trained on a simple concatenation of the corporareduces the perplexity by more than 10%.
The best translation quality is obtainedwhen the T-FR corpus is upweighted by a factor of 10.
It improves by 0.42 Bleu pointscompared to the MT system that uses T-FR (p = 0.074), and, more significantly, by 0.65Bleu points compared to O-EN (p < 0.05).
In the ?out-of-domain?
scenario, there is asmall reduction in perplexity (about 5%) with a language model that is trained on asimple concatenation of the corpora.
There is also a very small improvement in thetranslation quality (0.07 Bleu points compared to the T-FR system and 0.25 Bleu pointscompared to O-EN).Log-Linear combination of language models.
The MOSES decoder uses log-linear model-ing (Och and Ney 2001) to discriminate between better and worse hypotheses dur-ing decoding.
A log-linear model is defined as a combination of N feature functionshi(t, s), 1 ?
i ?
N, that map input (s), output (t), or a pair of input and output stringsto a numeric value.
Each feature function is associated with a model parameter ?i, itsfeature weight, which determines the contribution of the feature to the overall value ofP(t|s).
Formally, decoding based on a log-linear model is defined by:t?
= argmaxtP(t|s) = argmaxt{N?i=1?ihi(t, s)}(2)5 Except log-linear models, for which we only report the quality of machine translation, because there aretwo language models in this case and perplexity is harder to compute.816Lembersky, Ordan, and Wintner Language Models for Machine TranslationWe train two language models, based on T-FR and O-EN.
Then, we combine thesemodels by including them as different feature functions.
The feature weight of each LMis set by minimum error-rate tuning, optimizing the translation quality; this is the sametechnique that Koehn and Schroeder (2007) employ for domain adaptation.
In-domain,this combination is better by 0.82 Bleu points compared with an MT system that usesO-EN (p < 0.001), 0.59 Bleu points compared with the one that uses T-FR (p < 0.05).Out of domain, this combination is again not significantly better than using T-FR only(improvement of 0.08 Bleu points, p = 0.255).Interpolated language models.
In the interpolated scenario, two language models aremixed on a fixed proportion ?, according to the following equation (Weintraub et al1996):p(w|h) = (1?
?)
?
p(w|h;LM1)+ ?
?
p(w|h;LM2) (3)where w is a word, h is its ?history,?
and ?
is the fixed interpolation weight.
We useSRILM to train an interpolated language model from LM1 = O-EN and LM2 = T-FR.The interpolation weight is tuned to minimize the perplexity of the combined modelwith respect to the tuning set; we use the EM algorithm provided as part of the SRILMtoolkit to establish the optimized weights.
In the in-domain scenario ?
= 0.46 and in theout-of-domain scenario ?
= 0.49.
The interpolated language model yields additionalimprovement in perplexity and translation quality compared to all other models.
It issignificantly better (p < 0.05) than the T-FR system on the in-domain scenarios, but theimprovement is less significant (p = 0.075) out of domain.In summary, LMs compiled from source-translated-to-target texts are almost asgood asmuch larger LMs that also include large corpora of texts originally written in thetarget language.
Clearly, ignoring the status (original or translated) of monolingual textsand creating a single languagemodel from all of them (the concatenation scenario) is notmuch better than using only translated texts.
In order to benefit from (often much larger)original texts, one must consider more creative ways of combining the two subcorpora.Of the methods we explored here, interpolated LMs provide the greatest advantage.More research is needed in order to find an optimal combination.5.
DiscussionWe use language models computed from different types of corpora to investigatewhether their fitness to a reference set of translated sentences can differentiate betweenthem (and, hence, between the corpora on which they are based).
Our main findingsare that LMs compiled from manually translated corpora are much better predictors oftranslated texts than LMs compiled from original-language corpora of the same size.The results are robust, and are sustainable even when the corpora and the referencesentences are abstracted in ways that retain their syntactic structure but ignore spe-cific word meanings.
Furthermore, we show that translated LMs are better predictorsof translated sentences even when the LMs are compiled from texts translated fromlanguages other than the source language.
LMs based on texts translated from the sourcelanguage still outperform LMs translated from other languages, however.We also show that MT systems based on translated-from-source-language LMs out-perform MT systems based on originals LMs or LMs translated from other languages.Again, these results are robust and the improvements are statistically significant.
Thiseffect seems to be amplified as translation quality improves.
Furthermore, our results817Computational Linguistics Volume 38, Number 4Table 18MT system performance as measured by METEOR and TER.DE to ENOrig.
Lang.
METEOR TERO-EN 28.26 64.56T-DE 28.64 63.57FR to ENOrig.
Lang.
METEOR TERO-EN 33.05 54.45T-FR 33.30 53.65IT to ENOrig.
Lang.
METEOR TERO-EN 31.03 58.30T-IT 31.16 57.63NL to ENOrig.
Lang.
METEOR TERO-EN 29.97 60.29T-NL 30.40 59.63show that original LMs require five to ten times more data to exhibit the same fitnessto the reference set and the same translation quality as translated LMs.More generally, this study confirms that insights drawn from the field of theoreticalTranslation Studies, namely, the dual claim according to which translations as suchdiffer from originals, and translations from different source languages differ from eachother, can be verified experimentally and contribute to the performance of machinetranslation.One question, however, requires further investigation: Do MT systems based ontranslated-from-source-language LMs produce better translations, or do they merelygenerate sentences that are directly adapted to the reference set, thereby only improvinga specific evaluation metric, such as Bleu?
We address this issue in three ways, showingthat the former is indeed the case.
First, we use two automated evaluation metrics otherthan Bleu, and show that the T-based LMs yield better MT systems even with differentmetrics.
Second, we perform a manual evaluation of a portion of the evaluation set.
Theresults show that human evaluators prefer translations produced by an MT system thatuses a T-based LM over translations produced by a system built with an O-based LM.Finally, we provide a detailed analysis of the differences between O- and T-based LMs,explaining these differences in terms of insights from Translation Studies.5.1 Automatic EvaluationFirst, we use two alternative automatic evaluation metrics, METEOR6 (Denkowski andLavie 2011) and TER (Snover et al 2006), to assess the quality of the MT systemsdescribed in Section 4.2.
We focus on four translation tasks: From German, French,Italian, and Dutch to English.7 For each task we report the performance of two MTsystems: One that uses a language model compiled from original-English texts, and onethat uses a language model trained on texts translated from the source language.
Theresults, which are reported in Table 18, fully support our previous findings (recall thatlower TER is better): MT systems that use T-based LMs significantly outperform systemsthat use O-based LMs.6 More precisely, we use METEOR-RANK, the configuration used for WMT-2011.7 All MT systems were tuned using Bleu.818Lembersky, Ordan, and Wintner Language Models for Machine Translation5.2 Human EvaluationTo further establish the qualitative difference between translations produced with anEnglish-original language model and translations produced with a LM created fromFrench-translated-to-English texts, we conducted a human evaluation campaign, usingAmazon?sMechanical Turk as an inexpensive, reliable, and accessible pool of annotators(Callison-Burch and Dredze 2010).
We created a small evaluation corpus of 100 sen-tences, selected randomly among all (Europarl) reference sentences whose length isbetween 15 and 25 words.
Each instance of the evaluation task includes two Englishsentences, obtained from the two MT systems that use the O-EN and the T-FR languagemodels, respectively.
Annotators are presented with these two translations, and are re-quested to determine which one is better.
The definition given to annotators is: ?A bettertranslation is more fluent, reflecting better use of English.?
Observe that because theonly variable that distinguishes between the two MT systems is the different languagemodel, we only have to evaluate the fluency of the target sentence, not its faithfulnessto the source.
Consequently, we do not present the source or the reference translationto the annotators.
All annotators were located in the United States (and, therefore, arepresumably English speakers).As a control set, we added a set of 10 sentences produced with the O-based LM,which were paired with their (manually created) reference translations, and 10 sen-tences produced with the T-based LM, again paired with their references.
Each of the120 evaluation instances was assigned to 10 different Mechanical Turk annotators.
Wereport two evaluation metrics: score and majority.
The score of a given sentence pair?e1, e2?
is i/j, where i is the number of annotators who preferred e1 over e2, and j = 10?
iis the number of annotators preferring e2.
For such a sentence pair, the majority is e1 ifi > j, e2 if i < j, and undefined otherwise.The average score of the 10 sentences in the O-vs.-reference control set is 22/78,and the majority is the reference translation in all but one of the instances.
As for theT-vs.-reference control set, the average score is 18/82, and the majority is the referencein all of the instances.
This indicates that the annotators are reliable, and also that itis unrealistic to expect a clear-cut distinction even between human translations andmachine-generated output.As for the actual evaluation set, the average score of O-EN vs. T-FR is 38/62, andthe majority is T-FR in 75% of the cases, O-EN in only 25% of the sentence pairs.
Wetake these results as a very strong indication that English sentences generated by anMT system whose language model is compiled from translated texts are perceivedby humans as more fluent than ones generated by a system built with an O-basedlanguage model.
Not only is the improvement reflected in significantly higher Bleu(and METEOR, TER) scores, but it is undoubtedly also perceived as such by humanannotators.5.3 AnalysisIn order to look into the differences between T and O qualitatively, rather than quantita-tively, we turn now to study several concrete examples.
To do so, we extracted approx-imately 200 sentences from the French?English Europarl evaluation set; we chose allsentences of length between 15 and 25.
In addition, we extracted the 100 most frequentn-grams, for 1 ?
n ?
5, from both English-original and English-translated-from-FrenchEuroparl corpora.
As both corpora include approximately the same number of tokens,we report counts in the following rather than frequencies.819Computational Linguistics Volume 38, Number 4The differences between O and T texts are consistent with well-established observa-tions of translation scholars.
Consider the explicitation hypothesis (Blum-Kulka 1986),which Se?guinot (1998, page 108) spells out thus:1.
?something which was implied or understood through presupposition inthe source text is overtly expressed in the translation?2.
?something is expressed in the translation which was not in the original?3.
?an element in the source text is given greater importance in thetranslation through focus, emphasis, or lexical choice?Blum-Kulka (1986) uses the term cohesive markers to refer to items that are utilized bythe translator which cannot be found overtly in the source text.
One would expect suchmarkers to be much more prevalent in translationese.An immediate example of (1) is the case of acronyms: these tend to be spelled outin translated texts.
Indeed, the acronym EU is ranked 77 among the O-EN bigrams,whereas in T-FR it does not appear in the top 100.
On the other hand, the explicit trigramThe European Union occurs more frequently in T than in O.Similarly, an instance of (2) is the cohesivemarker because in the following example,which appears in T but neither appears in O nor can it be traced back to the originalsource sentence:Source Enfin, ce qui est grave dans le rapport de M. Olivier Tautologie, c?est qu?ilpropose une constitution tripotage.O Finally, which is serious in the report of Mr Olivier Tautologie, is that it proposes aconstitution tripotage.T Finally, and this is serious in the report by Mr olivier Tautologie, it is because itproposes a constitution tripotage.Another cohesive marker, nevertheless, is correctly generated only in the T-based trans-lation in the following example:Source C?est quand me?me quelque chose de pre?cieux qui a e?te?
souligne?
par tous lesmembres du conseil europe?en.O Even when it is something of valuable which has been pointed out by all the mem-bers of the European Council.T It is nevertheless something of a valuable which has been pointed out by all themembers of the European Council.Other cohesive markers discussed by Blum-Kulka (1986) are over-represented inT compared with O.
These include: therefore (3,187 occurrences in T, 1,983 in O); forexample (863 occurrences in T, 701 in O); in particular (1336 vs. 1068); first of all (601 vs.266); in fact (1014 vs. 441); in other words (553 vs. 87); with regard to (1137 vs. 310); inorder to (2,016 vs. 603); in this respect (363 vs. 94); on the one hand (288 vs. 72); on theother hand (428 vs. 76); and with a view to (213 vs. 51).
A similar list of markers havebeen shown to be excellent discriminating features between original and translated texts(from several European languages, including French) in an independent study (Koppeland Ordan 2011).820Lembersky, Ordan, and Wintner Language Models for Machine TranslationAnother phenomenon we notice is that the T-based language model does a muchbetter job translating verbs than the O-based language model.
In two very large corporaof French and English (Ferraresi et al 2008), verbs are much more frequent in Frenchthan in English (0.124 vs. 0.091).
Human translations from French to English, therefore,provide many more examples of verbs from which to model.
Indeed, we encounterseveral examples in which the O-based translation system fails to use a verb at all, or touse one correctly, compared with the T-based system:Source Une telle Europe serait un gage de paix et marquerait le refus de tout national-isme ethnique.O Such a Europe would be a show of peace and would the rejection of any ethnicnationalism.T Such a Europe would be a show of peace and would mark the refusal of all ethnicnationalism.Source Votre rapport, madame Sudre, met l?accent, a` juste titre, sur la ne?cessite?
d?agirdans la dure?e.O Your report, Mrs Sudre, its emphasis, quite rightly, on the need to act in the longterm.T Your report, Mrs Sudre, places the emphasis, quite rightly, on the need to act in thelong term.Source Cette proposition, si elle constitue un pas dans la bonne direction n?en comportepas moins de nombreuses lacunes auxquelles le rapport evans reme?die.O This proposal, if it is a step in the right direction do not least in contains manyshortcomings which the evans report resolve.T This proposal, if it is a step in the right direction it contains no less many shortcom-ings which the evans report resolve.Last, there are several cases of interference, which Toury (1995, page 275) defines asfollows: ?Phenomena pertaining to the make-up of the source text tend to be transferredto the target text.?
In the following example, do not say nothing more is a literaltranslation of the French construction On ne dit rien non plus.
The T-based translationis much more fluent:Source On ne dit rien non plus sur la responsabilite?
des fabricants, notamment engrande-bretagne, qui ont e?te?
les premiers responsables.O We do not say nothing more on the responsibility of the manufacturers, particularlyin Britain, which were the first responsible.T We do not say anything either on the responsibility of the manufacturers, particu-larly in great Britain, who were the first responsible.Incidentally, there are also some cultural differences between O and T that wedeem less important, because they are not part of the ?translationese dialect?
but ratherindicate differences pertaining to the culture from which the speaker arrives.
Mostnotable is the form ladies and gentlemen, which is the tenth most frequent trigram in T,but does not even rank among the top 100 in O.
This is already noted by van Halteren821Computational Linguistics Volume 38, Number 4(2008), according to whom this form is significantly more frequent in translations fromfive European languages as opposed to original English.In terms of (shallow) syntactic structure, we observe that part-of-speech n-gramsare distributed somewhat differently in O and in T (we use the POS-tagged Europarlcorpus of Section 4.1.2 for the following analysis).
For example, proper nouns are morefrequent in O (ranking 7 among all POS 1-grams) than in T (rank 9).
This has influenceon longer n-grams: For example, the 3-gram PRP MD VB is 20% more frequent in Othan in T. The sequence <S> PRP VBP is almost twice as frequent in O.
The 4-gram INDT NN </S> is 25% more frequent in O.
In contrast, the 4-gram IN DT NNS IN is 15%more frequent in T than in O.
A full analysis of such patterns is beyond the scope ofthis article.Summing up, T-based language models are more fluent and therefore yield bettertranslation results for the following reasons: They are more cohesive, less influenced bystructural differences between the languages, such as the under-representation of verbsin original English texts, and less prone to interference (i.e., they can break away fromthe original towards a more coherent model of the target language).5.4 Future ResearchThis work is among the first to use insights from Translation Studies in order to improvemachine translation, and to use computational linguistic methodologies to corroborateTranslation Studies hypotheses.
We believe that there are still vast opportunities forfertile cross-disciplinary research in these directions.
First, we only address the languagemodel in the present work.
Kurokawa, Goutte, and Isabelle (2009) investigate the rela-tions between the direction of translation and the quality of the translation model usedby SMT systems.
There are various ways in which the two approaches can be extendedand combined, and we are actively pursuing such research directions now (Lembersky,Ordan, and Wintner 2012).This work also bears on language typology: We conjecture that LMs compiledfrom texts translated not from the original language, but from a closely related one,can be better than LMs compiled from texts translated from a more distant language.Some of our results support this hypothesis, but more research is needed in order toestablish it.The fact that translations seem to make do with fewer words (cf.
also Laviosa 2008)call into question certain norms in comparing corpora in the field of machine transla-tion.
Translated and original texts can be expected to either have the same number ofsentences or the same number of tokens, but not both.
Similarly, theymay have the samenumber of tokens or the same number of types, but not both.Another interesting question that arises from this study is whether the perplexityof a language model on a reference set is a good predictor of a translation qualitymeasure, such as Bleu.
Although our results show a certain correlation between theperplexity and Bleu, we acknowledge the fact that these results need further corrob-oration.
Chen, Beeferman, and Rosenfeld (1998) examine the ability of perplexity toestimate the performance of speech recognition.
They find that perplexity often doesnot correlate well with word-error rates.
As it is extremely important to have a reliablemeasure capable of estimating the effect of language model improvements on transla-tion quality without requiring expensive decoding resources, we believe that findingcorrespondences between perplexity and the quality of MT is a valuable topic for futureresearch.822Lembersky, Ordan, and Wintner Language Models for Machine TranslationAcknowledgmentsWe are grateful to Cyril Goutte, GeorgeFoster, and Pierre Isabelle for providing uswith an annotated version of the Hansardcorpus.
Alon Lavie has been instrumentalin stimulating some of the ideas reportedin this article, as well as in his long-termsupport and advice.
We benefitted greatlyfrom several constructive suggestions by thethree anonymous Computational Linguisticsreferees.
This research was supported by theIsrael Science Foundation (grant no.
137/06)and by a grant from the Israeli Ministry ofScience and Technology.ReferencesAl-Shabab, Omar S. 1996.
Interpretation andthe Language of Translation: Creativity andConventions in Translation.
Janus,Edinburgh.Bahl, Lalit R., Frederick Jelinek, andRobert L. Mercer.
1983.
A maximumlikelihood approach to continuous speechrecognition.
IEEE Transactions on PatternAnalysis and Machine Intelligence,5(2):179?190.Baker, Mona.
1993.
Corpus linguistics andtranslation studies: Implications andapplications.
In Gill Francis Mona Bakerand Elena Tognini-Bonelli, editors, Text andTechnology: In Honour of John Sinclair.
JohnBenjamins, Amsterdam, pages 233?252.Baker, Mona.
1995.
Corpora in translationstudies: An overview and somesuggestions for future research.Target, 7(2):223?243.Baker, Mona.
1996.
Corpus-based translationstudies: The challenges that lie ahead.In Gill Francis Mona Baker and ElenaTognini-Bonelli, editors, Terminology,LSP and Translation.
Studies in LanguageEngineering in Honour of Juan C. Sager.
JohnBenjamins, Amsterdam, pages 175?186.Baroni, Marco and Silvia Bernardini.2006.
A new approach to the study ofTranslationese: Machine-learning thedifference between original and translatedtext.
Literary and Linguistic Computing,21(3):259?274.Blum-Kulka, Shoshana.
1986.
Shifts ofcohesion and coherence in translation.In Juliane House and Shoshana EditorsBlum-Kulka, editors, Interlingual andIntercultural Communication Discourse andCognition in Translation and Second LanguageAcquisition Studies, volume 35.
GunterNarr Verlag, Berlin, pages 17?35.Brants, Thorsten and Peng Xu.
2009.Distributed language models.
InProceedings of Human Language Technologies:The 2009 Annual Conference of the NorthAmerican Chapter of the Association forComputational Linguistics, CompanionVolume: Tutorial Abstracts, pages 3?4,Boulder, CO.Callison-Burch, Chris and Mark Dredze.2010.
Creating speech and languagedata with Amazon?s Mechanical Turk.In Proceedings of the NAACL HLT 2010Workshop on Creating Speech and LanguageData with Amazon?s Mechanical Turk,pages 1?12, Los Angeles, CA.Chen, Stanley, Douglas Beeferman, andRonald Rosenfeld.
1998.
Evaluationmetrics for language models.
InProceedings of the DARPA BroadcastNews Transcription and UnderstandingWorkshop (BNTUW), Landsdowne, PA.Chen, Stanley F. 1998.
An empirical studyof smoothing techniques for languagemodeling.
Technical report 10-98,Computer Science Group, HarvardUniversity, Cambridge, MA.Denkowski, Michael and Alon Lavie.
2011.Meteor 1.3: Automatic metric for reliableoptimization and evaluation of machinetranslation systems.
In Proceedings of theSixth Workshop on Statistical MachineTranslation, pages 85?91, Edinburgh.Ferraresi, Adriano, Silvia Bernardini, PicciGiovanni, and Marco Baroni.
2008.
Webcorpora for bilingual lexicography.
a pilotstudy of English/French collocationextraction and translation.
In Proceedingsof The International Symposium on UsingCorpora in Contrastive and TranslationStudies, Hangzhou.Finkel, Jenny Rose, Trond Grenager, andChristopher Manning.
2005.
Incorporatingnon-local information into informationextraction systems by Gibbs sampling.In ACL ?05: Proceedings of the 43rd AnnualMeeting of the Association for ComputationalLinguistics, pages 363?370, Morristown, NJ.Frawley, William.
1984.
Prolegomenon to atheory of translation.
In William Frawley,editor, Translation.
Literary, Linguistic andPhilosophical Perspectives.
University ofDelaware Press, Newark, pages 159?175.Gellerstam, Martin.
1986.
Translationese inSwedish novels translated from English.In Lars Wollin and Hans Lindquist,editors, Translation Studies in Scandinavia.CWK Gleerup, Lund, pages 88?95.Graff, David and Christopher Cieri.
2007.English Gigaword.
Linguistic Data823Computational Linguistics Volume 38, Number 4Consortium, Philadelphia, PA, thirdedition.
LDC Catalog No.
LDC2007T07.Ilisei, Iustina, Diana Inkpen, GloriaCorpas Pastor, and Ruslan Mitkov.
2010.Identification of translationese:A machine learning approach.In Alexander F. Gelbukh, editor,Proceedings of CICLing-2010: 11thInternational Conference on ComputationalLinguistics and Intelligent Text Processing,volume 6008 of Lecture Notes in ComputerScience, pages 503?511.
Springer, Berlin.Itai, Alon and Shuly Wintner.
2008.Language resources for Hebrew.
LanguageResources and Evaluation, 42(1):75?98.Jelinek, Frederick, Robert L. Mercer, Lalit R.Bahl, and J. K. Baker.
1977.
Perplexity?A measure of the difficulty of speechrecognition tasks.
Journal of the AcousticalSociety of America, 62:S63.Jurafsky, Daniel and James H. Martin.2008.
Speech and Language Processing:An Introduction to Natural LanguageProcessing, Computational Linguisticsand Speech Recognition.
Prentice Hall,Upper Saddle River, NJ.Koehn, Philipp.
2004.
Statistical significancetests for machine translation evaluation.In Proceedings of EMNLP 2004,pages 388?395, Barcelona.Koehn, Philipp.
2005.
Europarl: A parallelcorpus for statistical machine translation.In Proceedings of the MT Summit X,pages 79?86, Phuket.Koehn, Philipp and Hieu Hoang.
2007.Factored translation models.
In Proceedingsof the 2007 Joint Conference on EmpiricalMethods in Natural Language Processing andComputational Natural Language Learning(EMNLP-CoNLL), pages 868?876, Prague.Koehn, Philipp, Hieu Hoang, AlexandraBirch, Chris Callison-Burch, MarcelloFederico, Nicola Bertoldi, Brooke Cowan,Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses:Open source toolkit for statistical machinetranslation.
In Proceedings of the 45thAnnual Meeting of the Association forComputational Linguistics CompanionVolume Proceedings of the Demo andPoster Sessions, pages 177?180, Prague.Koehn, Philipp, Franz Josef Och, andDaniel Marcu.
2003.
Statisticalphrase-based translation.
In NAACL ?03:Proceedings of the 2003 Conference of theNorth American Chapter of the Association forComputational Linguistics on Human LanguageTechnology, pages 48?54, Edmonton.Koehn, Philipp and Josh Schroeder.
2007.Experiments in domain adaptationfor statistical machine translation.
InProceedings of the Second Workshop onStatistical Machine Translation, StatMT ?07,pages 224?227, Stroudsburg, PA.Koppel, Moshe and Noam Ordan.2011.
Translationese and its dialects.In Proceedings of the 49th Annual Meetingof the Association for ComputationalLinguistics: Human Language Technologies,pages 1318?1326, Portland, OR.Kurokawa, David, Cyril Goutte, and PierreIsabelle.
2009.
Automatic detection oftranslated text and its impact on machinetranslation.
In Proceedings of MT-SummitXII, Kurokawa.Laviosa, Sara.
1998.
Core patterns of lexicaluse in a comparable corpus of Englishlexical prose.Meta, 43(4):557?570.Laviosa, Sara.
2008.
Universals.
In MonaBaker and Gabriela Saldanha, editors,Routledge Encyclopedia of TranslationStudies, 2nd Edition.
Routledge (Taylorand Francis), New York, pages 288?292.Lembersky, Gennadi, Noam Ordan, andShuly Wintner.
2011.
Language modelsfor machine translation: Original vs.translated texts.
In Proceedings of EMNLP,pages 363?374, Edinburgh.Lembersky, Gennadi, Noam Ordan, andShuly Wintner.
2012.
Adapting translationmodels to translationese improves SMT.In Proceedings of the 13th Conference of theEuropean Chapter of the Association forComputational Linguistics (EACL-2012),Avignon.Och, Franz Josef.
2003.
Minimum error ratetraining in statistical machine translation.In ACL ?03: Proceedings of the 41st AnnualMeeting of the Association for ComputationalLinguistics, pages 160?167, Morristown, NJ.Och, Franz Josef and Hermann Ney.
2001.Discriminative training and maximumentropy models for statistical machinetranslation.
In ACL ?02: Proceedings of the40th Annual Meeting of the Association forComputational Linguistics, pages 295?302,Morristown, NJ.Papineni, Kishore, Salim Roukos, ToddWard, and Wei-Jing Zhu.
2002.
BLEU:A method for automatic evaluationof machine translation.
In ACL ?02:Proceedings of the 40th Annual Meeting of theAssociation for Computational Linguistics,pages 311?318, Morristown, NJ.Pym, Anthony and Grzegorz Chrupa?a.
2005.The quantitative analysis of translationflows in the age of an international824Lembersky, Ordan, and Wintner Language Models for Machine Translationlanguage.
In Albert Branchadell andLovell M. West, editors, Less TranslatedLanguages.
John Benjamins, Amsterdam,pages 27?38.Santos, Diana.
1995.
On grammaticaltranslationese.
In Koskenniemi, Kimmo(comp.
), Short papers presented at the TenthScandinavian Conference on ComputationalLinguistics (Helsinki), University ofHelsinki, pages 29?30.Se?guinot, Candice.
1998.
Pragmaticsand the explicitation hypothesis.
TTR:Traduction, Terminologie, Re?daction,11(2):106?114.Snover, Matthew, Bonnie Dorr, RichardSchwartz, Linnea Micciulla, andJohn Makhoul.
2006.
A study oftranslation edit rate with targetedhuman annotation.
In Proceedings of theAssociation for Machine Translation in theAmericas (AMTA-2006), pages 223?231,Cambridge, MA.Stolcke, Andreas.
2002.
SRILM?Anextensible language modeling toolkit.In Procedings of International Conferenceon Spoken Language Processing,pages 901?904, Denver, CO.Toury, Gideon.
1980.
In Search of a Theory ofTranslation.
The Porter Institute for Poeticsand Semiotics, Tel Aviv University,Tel Aviv.Toury, Gideon.
1995.
Descriptive TranslationStudies and Beyond.
John Benjamins,Amsterdam / Philadelphia.Toutanova, Kristina and Christopher D.Manning.
2000.
Enriching the knowledgesources used in a maximum entropypart-of-speech tagger.
In Proceedings of the2000 Joint SIGDAT Conference on EmpiricalMethods in Natural Language Processingand Very Large Corpora, pages 63?70,Morristown, NJ.Tsvetkov, Yulia and Shuly Wintner.
2010.Automatic acquisition of parallel corporafromWebsites with dynamic content.In Proceedings of the Seventh Conferenceon International Language Resources andEvaluation (LREC?10), pages 3389?3392,Valleta.van Halteren, Hans.
2008.
Source languagemarkers in EUROPARL translations.In COLING ?08: Proceedings of the 22ndInternational Conference on ComputationalLinguistics, pages 937?944, Morristown, NJ.Weintraub, Mitch, Yaman Aksu, SatyaDharanipragada, Sanjeev Khudanpur,Herman Ney, John Prange, AndreasStolcke, Fred Jelinek, and Liz Shriberg.1996.
Fast training and portability.
LM95project report, Center for Language andSpeech Processing, Johns HopkinsUniversity, Baltimore, MD.825
