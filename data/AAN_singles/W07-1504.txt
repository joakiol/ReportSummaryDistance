Proceedings of the Linguistic Annotation Workshop, pages 25?32,Prague, June 2007. c?2007 Association for Computational LinguisticsAssociating Facial Displays with Syntactic Constituents for GenerationMary Ellen FosterInformatik VI: Robotics and Embedded SystemsTechnical University of MunichBoltzmannstra?e 3, 85748 Garching, Germanyfoster@in.tum.deAbstractWe present an annotated corpus of conversa-tional facial displays designed to be used forgeneration.
The corpus is based on a record-ing of a single speaker reading scripted out-put in the domain of the target generationsystem.
The data in the corpus consists ofthe syntactic derivation tree of each sentenceannotated with the full syntactic and prag-matic context, as well as the eye and eye-brow displays and rigid head motion usedby the the speaker.
The behaviours of thespeaker show several contextual patterns,many of which agree with previous findingson conversational facial displays.
The cor-pus data has been used in several studies ex-ploring different strategies for selecting fa-cial displays for a synthetic talking head.1 IntroductionAn increasing number of systems designed to au-tomatically generate linguistic and multimodal out-put now make use of corpora to help in decision-making (cf.
Belz and Varges, 2005).
Some imple-mentations use corpora to help select output that isgrammatical or fluent; for example, Langkilde andKnight (1998) and White (2006) both used n-gramlanguage models to guide stochastic surface realis-ers.
In other systems, corpora are used to makedecisions based on pragmatic factors such as thereading level of the target user (Williams and Re-iter, 2005) or the visual features of an object be-ing described (Cassell et al, 2007).
The latter typeof domain-specific contextual information is not of-ten included in generally-available corpora.
For thisreason, developers of generation systems that needthis type of information often create and make useof application-specific corpora.The easiest method of including the necessarypragmatic information in a corpus is to base the cor-pus on output generated in situations where the con-textual factors are known; this eliminates the need toannotate these factors explicitly.
Stone et al (2004),for example, created a multimodal corpus based onthe voice and body language of an actor performingscripted output in the domain of the target genera-tion system: an animated instructor character for asnowboarding video game.
The contextual informa-tion in the corpus scripts included the move that theplayer attempted in the game and the result of thatattempt.
Similarly, van Deemter et al (2006) cre-ated a corpus of multimodal referring expressionsproduced in specific pragmatic contexts and used itto compare several referring-expression generationalgorithms to human performance.In this work, the task is to select facial displaysfor an animated talking head to use while present-ing output in the COMIC multimodal dialogue sys-tem (Foster et al, 2005), which generates spokendescriptions and comparisons of bathroom-tile op-tions.
The output of the COMIC text planner in-cludes a range of information in addition to the text:the syntactic derivation tree, the user?s evaluationof the object being described, the information sta-tus (new or old, contrastive) of each fact described,and the predicted speech-synthesiser prosody.
All ofthis contextual information can be used to help select25appropriate facial displays to accompany the spo-ken presentation; however?as in the other systemsmentioned above?this requires a corpus where thefull context for every facial display is known.
To cre-ate such a corpus, we recorded a speaker performingscripted output in the domain of COMIC.This paper is arranged as follows.
In Section 2,we first describe how the scripts for the corpus werecreated and how the recording was made.
Section 3then presents the annotation scheme and the tool thatwas used to perform the annotation, while Section 4describes the measures that were taken to ensure thatthe annotation was reliable.
Section 5 then sum-marises the high-level patterns that were found in thedisplays annotated in the corpus and compares themto other findings on conversational facial displays.At the end of the section, we use the corpus data totest two assumptions that were made in the annota-tion scheme.
After that, in Section 6, we describeseveral experiments in which different methods ofusing the data in this corpus to select facial displaysfor a synthetic head have been compared.
Finally,in Section 7, we summarise the contributions of thispaper and draw some conclusions about the useful-ness of this corpus for its intended task.2 RecordingFor this corpus, we recorded a single speaker read-ing a set of 444 scripted sentences in the domain ofthe COMIC multimodal dialogue system.
The sen-tences were generated by the full COMIC output-generation process, which uses the OpenCCG sur-face realiser (White, 2006) to create texts includ-ing prosodic specifications for the speech synthe-siser and incorporates information from the dialoguehistory and a model of the user?s likes and dislikes.Every node in the OpenCCG derivation tree foreach sentence in the script was initially annotatedwith all of the available syntactic and pragmatic in-formation from the output planner, including the fol-lowing features:?
The user-model evaluation of the object beingdescribed (positive or negative);?
Whether the fact being presented was previ-ously mentioned in the discourse (as I said be-fore, .
.
. )
or is new information;?Although it's in the family style, the tiles are by Alessi Tiles.??
although it's in the family style?
although?
it's in the family style?
it?
's in the family style?
's?
in the family style?
in?
the family style?
the?
family style?
family?
style?
the tiles are by Alessi Tiles?
the tiles?
the?
tiles?
are by Alessi Tiles?
are?
by Alessi Tiles?
Alessi TilesUser model: badClause: firstUser model: goodClause: secondAccent: L+H*Accent: H*Figure 1: Annotated OpenCCG derivation tree?
Whether the fact is explicitly compared or con-trasted with a feature of the previous tile design(once again .
.
.
but here .
.
.
);?
Whether the node is in the first clause of a two-clause sentence, in the second clause, or is anonly clause;1?
The surface string associated with the node;?
The surface string, with words replaced by se-mantic classes or stems drawn from the gram-mar (e.g., this design is classic becomes this[mental-obj] be [style]); and?
Any pitch accents specified by the text planner.Figure 1 illustrates the annotated OpenCCGderivation tree for a sample sentence drawn fromthe recording script.
The annotations indicate thatevery node in the first half of this sentence is associ-ated with a negative user-model evaluation and is inthe first clause of a two-clause sentence, while everynode in the second half is linked to a positive eval-uation and is in the second clause of the sentence.The figure also shows the pitch accents selected bythe output planner according to Steedman?s (2000)theory of information structure and intonation.For the recording, the sentences in the script werepresented one at a time to the speaker; the presen-1No sentence in the script had more than two clauses.26tation included both the linguistic content (with ac-cented words highlighted) as well as the intendedpragmatic context.
Each sentence was displayed ina large font on a laptop computer directly in frontof the speaker, with the camera positioned directlyabove the laptop to ensure that the speaker was look-ing towards the camera at all times.
The speaker wasinstructed to read each sentence out loud as expres-sively as possible into the camera.3 AnnotationOnce all of the sentences in the script had beenrecorded as described in the preceding section, thenext step was to annotate the facial displays that oc-curred.
We first used Anvil (Kipp, 2004) to splitthe video into individual clips corresponding to eachsentence.
This section describes how the facial dis-plays in each of the clips were then annotated.3.1 Annotation schemeWe annotated the speaker?s facial displays by linkingeach to the span of nodes in the OpenCCG derivationtree with which it was temporally related.
Makingcross-modal links at this level made it possible touse the annotated information directly in the output-generation process for the experiments described inSection 6.A display was associated with the full span ofwords that it coincided with temporally, as follows.If a single node in the derivation tree covered ex-actly all of the relevant words, then the annotationwas placed on that node; if the words spanned by adisplay did not coincide with a single node, it wasattached to the set of nodes that did span the neces-sary words.
For example, in the derivation shown inFigure 1, the sequence the family style is associatedwith a single node, so a motion temporally associ-ated with that sequence would be attached to thatnode.
On the other hand, if there were a motion as-sociated with the tiles are, it would be attached toboth the the tiles node and the are node.The following were the features that were consid-ered; for each feature, we note the correspondingAction Unit (AU) from the well-known Facial Ac-tion Coding System (Ekman et al, 2002).?
Eyebrows: up (AU 1+2) or down (AU 4)?
Eye squinting (AU 43)Figure 2: Annotation tool?
Head nodding: up (AU 53) or down (AU 54)?
Head leaning: left (AU 55) or right (AU 56)?
Head turning: left (AU 57) or right (AU 58)This set of displays was chosen based on a combi-nation of three factors: the emphatic facial displaysdocumented in the literature, the capabilities of thetarget talking head, and the actual displays of thespeaker during the recording session.3.2 Annotation toolThe tool for the annotation was a custom-writtenprogram that allowed the coder to play back arecorded sentence at full speed or slowed down, andto associate any combination of displays with anynode or set of nodes in the OpenCCG derivation treeof the sentence.
The tool also allowed the coder toplay back a proposed annotation sequence on a syn-thetic talking head to verify that it was as close aspossible to the actual motions.
Figure 2 shows ascreenshot of the annotation tool in use on the sen-tence from Figure 1.
In the screenshot, a left turn isattached to the entire sentence (i.e., the root node),while a series of nods is associated with single leafnodes in the first half of the sentence.
The annotatorhas already attached a brow raise to the word are inthe second half and is in the process of adding a nodto the same word.The output of the annotation tool is an XML doc-ument including the original contextually-annotated27<node surf=" although it ?s in the family style the tiles are by Alessi_Tiles" LEAN="left"sc=" although [pro3n] be in the [style] [abstraction] the [phys -obj] be by [manufacturer ]"><node surf=" although it ?s in the family style" um="b" first ="y"sc=" although [pro3n] be in the [style] [abstraction ]"><node surf=" although" um="b" first ="y" NOD="down" /><node surf="it ?s in the family style" um="b" first ="y"sc="[ pro3n] be in the [style] [abstraction ]"><node surf="it" stem=" pro3n" um="b" first ="y" NOD="down" /><node surf="?s in the family style" um="b" first ="y" sc="be in the [style] [abstraction ]"><node surf="?s" stem="be" um="b" first ="y" NOD="down" /><node surf="in the family style" um="b" first ="y" sc="in the [style] [abstraction ]"><node surf="in" um="b" first ="y" NOD="down" /><node surf="the family style" um="b" first ="y" sc="the [style] [abstraction ]"><node surf="the" um="b" first ="y" /><node surf=" family style" um="b" first ="y" sc="[ style] [abstraction ]"><node surf=" family" sc="[ style]" accent ="L+H*" um="b" first ="y" NOD="down" /><node surf="style" sc="[ abstraction ]" um="b" first ="y" /></node ></node ></node ></node ></node ></node ><node surf="the tiles are by Alessi_Tiles" um="g" first ="n"sc="the [phys -obj] be by [manufacturer ]"><node surf="the tiles" um="g" first ="n" sc="the [phys -obj]"><node surf="the" um="g" first ="n" /><node surf="tiles" sc="[phys -obj]" stem="tile" um="g" first ="n" /></node ><node surf="are by Alessi_Tiles" um="g" first ="n" sc="be by [manufacturer ]"><node surf="are" stem="be" accent ="H*" um="g" first ="n" BROW="up" NOD="down" /><node surf="by Alessi_Tiles" um="g" first ="n" sc="by [manufacturer ]"><node surf="by" um="g" first ="n" /><node surf=" Alessi_Tiles" sc="[ manufacturer ]" accent ="H*" um="g" first ="n" /></node ></node ></node ></node >Figure 3: Annotated sentence from the corpusOpenCCG derivation tree of each sentence, witheach node additionally labelled with a (possiblyempty) set of facial displays.
Figure 3 shows thefully-annotated version of the sentence from Fig-ure 1.
This document includes the contextual fea-tures from the original tree, indicated by italics: ev-ery node in the first subtree has um="b" and first="y",while every node in the second subtree has um="g"and first="n", while the accented items also have anaccent feature.
Every node also specifies the stringgenerated by the subtree that it spans, both in its sur-face form (surf) and with semantic-class and stemreplacement (sc).
This tree also includes the facialdisplays added by the coder in Figure 2, indicatedby underlining: (LEAN="left") attached to the rootnode), a number of downward nods (NOD="down") onindividual words in the first half of the sentence, anda nod accompanied by a brow raise (BROW="up") onare near the end.4 Reliability of the annotationSeveral measures were taken to ensure that the an-notation process was reliable.
As the first step, twoindependent coders each separately processed thesame set of 20 sentences, using an initial annotationscheme.
The outputs of these two coders were com-pared, and the coders discussed the differences andagreed on a revised scheme.
One of these codersthen used the final scheme to process the entire setof 444 sentences.
As a further test of reliability, an28additional coder was instructed on the use of the an-notation tool and scheme and used them to process286 sentences (approximately 65% of the corpus).To assess the degree of agreement between thesetwo coders, we used a version of the ?
agreementcoefficient proposed by Artstein and Poesio (2005).?
is designed as a coefficient that is weighted, thatapplies to multiple coders, and that uses a separateprobability distribution for each coder.
Weightedcoefficients like ?
permit degrees of agreement tobe measured, so that partial agreement is penalisedless severely than total disagreement.
Like otherweighted coefficients, ?
is based on the ratio be-tween the observed and expected disagreement onthe corpus.To use this coefficient, it is necessary to definea measure that computes the distance between twoproposed annotations.
In this case, to compute theobserved disagreement Do(S) on a sentence S, weuse a measure similar to that proposed by Passon-neau (2004) for measuring agreement on set-valuedannotations.
For each display proposed by eachcoder on the sentence, we search for a correspond-ing display proposed by the other coder?one withthe same value (e.g., a brow raise) and covering asimilar span of nodes.
If both proposed exactly thesame display, that indicates no disagreement (0); ifone display covers a strict subset of the nodes cov-ered by the other, that indicates minor disagreement(13 ); if the nodes covered by the two proposals over-lap, that is a more major disagreement (23 ); and if nocorresponding display can be found from the secondcoder, then that indicates the maximum level of dis-agreement (1).
The total observed disagreement ona sentence is the sum of the disagreement level foreach display proposed by each coder.The expected disagreement De(S) for a sentenceS depends on the length of that sentence, as fol-lows.
We first use the corpus counts to computethe probability of each coder assigning each pos-sible facial display to word spans of all possiblelengths.
We then use these probabilities to estimatethe likelihood of the two coders assigning identical,super/subset, overlapping, or disjoint annotations tothe sentence, for each possible display.
The totalexpected disagreement for the sentence is the sumof these probabilities across all displays, using thesame weights as the observed disagreement above.The overall observed disagreement in the corpusDo is the arithmetic mean of the disagreement oneach sentence; similarly, the overall expected dis-agreement De is the mean of the expected disagree-ment across all of the sentences.
To compute thevalue of ?
for the output of the two coders, we sub-tract the ratio of these two values from 1:?
= 1?DoDeAs Artstein and Poesio (2005) point out, forweighted measures such as ?, there is no signif-icance test for agreement, and the actual value isstrongly affected by the distance metric that is se-lected.
However, ?
values can be compared withone another to assess degrees of agreement.
Theoverall ?
value between the two coders on the fullset of 286 sentences processed by both was 0.561,with ?
values on individual facial displays rangingfrom a high of 0.661 on nodding to a low of 0.285on squinting (a very rare motion).
To put these val-ues into context, we computed ?
on the set of 20sentences processed by the final coder as part of thetraining process (which are not included in the setof 286).
The overall ?
value for these sentences is0.231, with negative values for some of the individ-ual displays.
This demonstrates that the training pro-cess had a positive effect on agreement.5 Patterns in the corpusWe investigated the contextual features to see whichhad the most significant effect on the facial displaysoccurring on a node.
To determine this, we usedmultinomial logit regression to select the factors andfactor interactions that had the most significant ef-fects on the distribution of each display; this form ofregression is appropriate when, as in this case, theresponse variable is categorical.
In this section, welist the most significant factors and give a qualitativedescription of the impact of each.The single most influential contextual factor wasthe user-model evaluation, which had an effect on allof the facial displays.
In positive user-model con-texts, eyebrow raising and turning to the right wererelatively more frequent (Figure 4(a)); in negativecontexts, on the other hand, the rates of eyebrowlowering, squinting, and leaning to the left were allhigher (Figure 4(b)).
Other factors also affected the29(a) Positive (b) NegativeFigure 4: Characteristic facial displays for different user-model evaluationsdistribution of facial displays.
In the first half oftwo-clause sentences, brow lowering was also morefrequent, as was upward nodding, while downwardnodding and right turns showed up more often in thesecond clause of two-clause sentences.
Nodding andbrow raising were both more frequent on nodes withany sort of predicted pitch accent.Several of these factors agree with previous find-ings on conversational body language.
The in-creased frequency of nodding and brow raising onaccented words agrees with many previous stud-ies: Ekman (1979), Cav?
et al (1996), Graf et al(2002), Keating et al (2003), Krahmer and Swerts(2004), and Flecha-Garc?a (2006) all noted similardisplays on prosodically accented parts of the sen-tence.
The speaker?s tendency to move right on pos-itive descriptions and left on negative descriptionsis also consistent with other findings.
Accordingto the work of Davidson and colleagues (Davidsonand Irwin, 1999), emotion and affect processing areasymmetrically organised in the human brain.
Theright hemisphere is associated with negative affect(and withdrawal behaviours), and the left with posi-tive affect (and approach behaviours).
Because bothperceptual and motor systems are contra-laterally or-ganised, this means that higher levels of right hemi-sphere activity are associated with attention beingoriented towards the left, while higher levels of lefthemisphere activity are associated with attention be-ing oriented to the right; this fits with our speaker?spattern of movements.The annotation scheme described here allowed adisplay to be associated with any contiguous span ofwords in the sentence.
Annotators were encouragedto use syntactic constituents wherever possible, butalso had the option to select multiple nodes where adisplay did not correspond with a single constituentin the derivation tree.
Earlier versions of the annota-tion scheme did not support this degree of flexibility,so we used the patterns in the corpus to test whetherthe modifications to the scheme were useful.In a previous study using the same video record-ings but a different, simpler scheme (Foster andOberlander, 2006), facial displays could only be as-sociated with single leaf nodes (i.e., words); that is,in the terminology of Ekman (1979), all motionswere considered to be batons rather than underlin-ers.
Based on the data in the current corpus, thatrestriction was clearly unrealistic: the mean numberof nodes spanned by a display in the full corpus was1.95, with a maximum of 15 and a standard devia-tion of 2.
The results were similar in the sub-corpusproduced by the final coder, in which the mean num-ber of nodes spanned by a display was 2.25.The annotation rules for this study did not ini-tially permit displays to be associated with morethan nodes in the derivation tree.
This capabilitywas added following inter-coder discussions afterthe initial test annotation to deal with cases wherethe speaker?s displays did not correspond to syntac-tic constituents?for example, if the speaker raisedhis eyebrows on the tiles are or some other suchnon-standard constituent.
The data in the annotatedcorpus supports this modification.
Approximately6% of the annotations in the main corpus?165 of2826?were attached to more than one node in thederivation tree; for the final coder, 4.5% of annota-tions were on multiple nodes.306 Generation experimentsThe primary reason for creating this corpus of fa-cial displays was to use the resulting data to selectfacial displays for the artificial talking head in theCOMIC multimodal dialogue system.
Several dif-ferent strategies have been implemented to use thecorpus data for this task, and a number of automatedand human evaluations have been carried out com-paring the different implementations.As described in the preceding section, the fac-tor with the largest influence on the displays ofthe recorded speaker was the user-model evaluation.Two studies (Foster, 2007b) were carried out to testthe generality of the characteristic positive and neg-ative displays (Figure 4).
In the first study, userswere asked to identify the intended user-model po-larity of a description presented by the talking headbased only on the facial displays.
The participantswere generally able to recognise the characteristicpositive and negative facial displays; they also iden-tified the displays intended to be neutral (noddingalone) as positive, and tended to judge videos withno facial displays to be negative.
In the second study,users?
subjective preferences were gathered betweenvideos in which the user-model evaluation expressedin speech was either consistent or inconsistent withthe facial displays.
In this study, the participantsgenerally preferred the videos that showed consis-tent content on the two output channels.In another study (Foster and Oberlander, 2007),two different data-driven strategies were imple-mented that used the corpus data to select facial dis-plays to accompany speech.
One strategy always se-lected the highest-probability option in all contexts,while the other made a stochastic choice among allof the options weighted by the corpus probabili-ties.
These two strategies were compared againsteach other using both automated and human eval-uation methods: the majority strategy scored morehighly on the automated cross-validation, while theweighted strategy was strongly preferred by humanjudges.
The judges also preferred resynthesised ver-sions of the original facial displays from the corpusto the output of either of the generation strategies.Two further human evaluation studies comparedthe weighted data-driven generation strategy fromthe preceding study to a rule-based strategy thatselected the most characteristic displays basedonly on the user-model evaluation (Foster, 2007a).When users?
subjective judgements were gatheredas above, they had a mild preference for the out-put of the weighted strategy over that of the rule-based strategy.
In a second study, videos generatedby the weighted strategy significantly decreased par-ticipants?
ability to select descriptions that were cor-rectly tailored to a given set of user preferences,while videos generated by the rule-based strategyhad no such impact.7 ConclusionsWe have described the collection and annotation ofan application-specific corpus of conversational fa-cial displays.
The designs of both the corpus andthe annotation scheme were driven by the needs ofa specific generation system, which makes use of arange of pragmatic information while creating out-put.
To use this information to make corpus-baseddecisions, it is necessary that the full context of ev-ery utterance and facial display in the corpus beavailable.
Rather than adding this information to anexisting corpus, we chose?like Stone et al (2004)and van Deemter et al (2006), for example?to cre-ate a corpus based on known contexts so that thefull information for every sentence was known be-fore the fact.The final annotation scheme required each facialdisplay to be linked to the set of nodes in the syntac-tic derivation tree of the sentence that exactly cov-ered the words temporally associated with the dis-play.
Two coders separately processed the sentencesin the corpus; on the sentences processed by bothcoders (about 65% of the corpus), the agreement asmeasured by ?
was 0.561.A number of contextual factors had an influ-ence on the displays used by the recorded speaker.The single most influential factor was the user-model evaluation of the object being described.The speaker?s characteristic side-to-side motions onthese sentences agree with findings on the relation-ship between brain hemispheres and affect.
In ad-dition, in user studies, human judges were reliablyable to identify the intended affect based on resyn-thesised versions of these characteristic displays.Other patterns in the data also agree with exist-31ing findings on facial displays: for example, thespeaker tended to nod and raise his eyebrows morefrequently on words with prosodic accents.Several experiments have been performed inwhich the annotated data from this corpus was usedto select the facial displays to accompany the out-put of an animated talking head.
These studies havefound interesting results on both the relationship be-tween automated and human judgements of outputquality and the relative utility of rule-based and data-driven approaches for selecting conversational facialdisplays.AcknowledgementsThis research was supported by the EU projectsCOMIC (IST-2001-32311) and JAST (FP6-003747-IP).
Thanks to Amy Isard, Ron Petrick, and TomSegler for annotation assistance, and to Jon Ober-lander and the LAW reviewers for useful comments.ReferencesR.
Artstein and M. Poesio.
2005.
Kappa3 = alpha (or beta).Technical Report CSM-437, University of Essex Departmentof Computer Science.A.
Belz and S. Varges, editors.
2005.
Corpus Linguistics 2005Workshop on Using Corpora for Natural Language Genera-tion.
http://www.itri.brighton.ac.uk/ucnlg/ucnlg05/.J.
Cassell, S. Kopp, P. Tepper, K. Ferriman, and K. Striegnitz.2007.
Trading spaces: How humans and humanoids usespeech and gesture to give directions.
In T. Nishida, edi-tor, Engineering Approaches to Conversational Informatics.Wiley.
In press.C.
Cav?, I. Gua?tella, R. Bertrand, S. Santi, F. Harlay, and R. Es-pesser.
1996.
About the relationship between eyebrowmove-ments and F0 variations.
In Proceedings of the 4th Interna-tional Conference on Spoken Language Processing (ICSLP1996).R.
J. Davidson and W. Irwin.
1999.
The functional neu-roanatomy of emotion and affective style.
Trends in Cog-nitive Sciences, 3(1):11?21.
doi:10.1016/S1364-6613(98)01265-0.K.
van Deemter, I. van der Sluis, and A. Gatt.
2006.
Buildinga semantically transparent corpus for the generation of refer-ring expressions.
In Proceedings of the Fourth InternationalNatural Language Generation Conference, pages 130?132.Sydney, Australia.
ACL Anthology W06-1420.P.
Ekman.
1979.
About brows: Emotional and conversationalsignals.
In M. von Cranach, K. Foppa, W. Lepenies, andD.
Ploog, editors, Human Ethology: Claims and limits of anew discipline.
Cambridge University Press.P.
Ekman, W. V. Friesen, and J. C. Hager.
2002.
Facial ActionCoding System.
A Human Face, Salt Lake City.M.
L. Flecha-Garc?a.
2006.
Eyebrow raising in dialogue:Discourse structure, utterance function, and pitch accents.Ph.D.
thesis, Department of Theoretical and Applied Lin-guistics, University of Edinburgh.M.
E. Foster.
2007a.
Comparing rule-based and data-driven se-lection of facial displays.
In Proceedings of the ACL 2007Workshop on Embodied Language Processing.M.
E. Foster.
2007b.
Generating embodied descriptions tailoredto user preferences.
In submission.M.
E. Foster and J. Oberlander.
2006.
Data-driven generationof emphatic facial displays.
In Proceedings of the 11th Con-ference of the European Chapter of the Association for Com-putational Linguistics (EACL 2006), pages 353?360.
Trento,Italy.
ACL Anthology E06-1045.M.
E. Foster and J. Oberlander.
2007.
Corpus-based generationof conversational facial displays.
In submission.M.
E. Foster, M. White, A. Setzer, and R. Catizone.
2005.
Mul-timodal generation in the COMIC dialogue system.
In Pro-ceedings of the ACL 2005 Demo Session.
ACL AnthologyW06-1403.H.
Graf, E. Cosatto, V. Strom, and F. Huang.
2002.
Visualprosody: Facial movements accompanying speech.
In Pro-ceedings of the 5th IEEE International Conference on Auto-matic Face and Gesture Recognition (FG 2002), pages 397?401.
doi:10.1109/AFGR.2002.1004186.P.
Keating, M. Baroni, S. Mattys, R. Scarborough, and A. Al-wan.
2003.
Optical phonetics and visual perception of lexi-cal and phrasal stress in English.
In Proceedings of the 15thInternational Congress of Phonetic Sciences (ICPhS), pages2071?2074.M.
Kipp.
2004.
Gesture Generation by Imitation - From Hu-man Behavior to Computer Character Animation.
Disserta-tion.com.E.
Krahmer and M. Swerts.
2004.
More about brows: A cross-linguistic study via analysis-by-synthesis.
In C. Pelachaudand Z. Ruttkay, editors, From Brows to Trust: EvaluatingEmbodied Conversational Agents, pages 191?216.
Kluwer.doi:10.1007/1-4020-2730-3_7.I.
Langkilde and K. Knight.
1998.
The practical value of n-grams in generation.
In Proceedings of the 9th InternationalNatural Language Generation Workshop (INLG 1998).
ACLAnthology W98-1426.R.
J. Passonneau.
2004.
Computing reliability for coreferenceannotation.
In Proceedings, Fourth International Conferenceon Language Resources and Evaluation (LREC 2004), vol-ume 4, pages 1503?1506.
Lisbon.M.
Steedman.
2000.
Information structure and the syntax-phonology interface.
Linguistic Inquiry, 31(4):649?689.doi:10.1162/002438900554505.M.
Stone, D. DeCarlo, I. Oh, C. Rodriguez, A. Lees, A. Stere,and C. Bregler.
2004.
Speaking with hands: Creatinganimated conversational characters from recordings of hu-man performance.
ACM Transactions on Graphics (TOG),23(3):506?513.
doi:10.1145/1015706.1015753.M.
White.
2006.
Efficient realization of coordinate struc-tures in Combinatory Categorial Grammar.
Research onLanguage and Computation, 4(1):39?75.
doi:10.1007/s11168-006-9010-2.S.
Williams and E. Reiter.
2005.
Deriving content selectionrules from a corpus of non-naturally occurring documentsfor a novel NLG application.
In Belz and Varges (2005).32
