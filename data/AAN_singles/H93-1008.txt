Gemini: A Natural Language System forSpoken-Language Understanding*John Dowding, Jean Mark Gawron, Doug Appelt,John Bear, Lynn Cherny, Robert Moore, and Doug MoranSRI  In ternat iona l333 Ravenswood AvenueMen lo  Park ,  CA 940251.
INTRODUCTIONGemini is a natural anguage understanding system de-veloped for spoken language applications.
This paperdescribes the details of the system, and includes rele-vant measurements of size, efficiency, and performanceof each of its sub-components in detail.The demands on a natural anguage understanding sys-tem used for spoken language differ somewhat from thedemands of text processing.
For processing spoken lan-guage, there is a tension between the system being asrobust as necessary, and as constrained as possible.
Therobust system will attempt o find as sensible an inter-pretation as possible, even in the presence of perfor-mance errors by the speaker, or recognition errors bythe speech recognizer.
In contrast, in order to providelanguage constraints to a speech recognizer, a systemshould be able to detect that a recognized string is nota sentence of English, and disprefer that recognition hy-pothesis from the speech recognizer.
If the coupling is tobe tight, with parsing and recognition interleaved, thenthe parser should be able to enforce as many constraintsas possible for partial utterances.
The approach takenin Gemini is to tightly constrain language recognition tolimit overgeneration, but to extend the language anal-ysis to recognize certain characteristic patterns of spo-ken utterances (but not generally thought of as part ofgrammar) and to recognize specific types of performanceerrors by the speaker.Processing starts in Gemini when syntactic, semantic,and lexical rules are applied by a bottom-up all-pathsconstituent parser to populate a chart with edges con-taining syntactic, semantic, and logical form informa-tion.
Then, a second utterance parser is used to applya second set of syntactic and semantic rules that are re-quired to span the entire utterance.
If no semantically-acceptable utterance-spanning edges are found duringthis phase, a component to recognize and correct certaingrammatical disfluencies is applied.
When an accept-able interpretation is found, a set of parse preferencesare used to choose a single best-interpretation from thechart to be used for subsequent processing.
Quantifierscoping rules are applied to this best-interpretation toproduce the final logical form, which is then used as in-put to a query answering system.
The following sectionswill describe ach of these components in detail, with theexception of the query answering subsystem, which willnot be described in this paper.Since this paper describes a component by componentview of Gemini, we will provide detailed statistics onthe size, speed, coverage, and accuracy of the variouscomponents.
These numbers detail our performance onthe subdomain of air-travel planning that is currently be-ing used by the DARPA spoken language understandingcommunity\[13\].
Gemini was trained on a 5875 utterancedataset from this domain, with another 688 utterancesused as a blind test (not explicitly trained on, but runmultiple times) to monitor our performance on a datasetthat we didn't train on.
We will also report here our re-sults on another 756 utterance fair test set, that we ranonly once.
Table 1 contains a summary of the cover-age of the various components on the both the trainingand fair test sets.
More detailed explanations of thesenumbers are given in the relevant sections.Training TestLexicon 99.1% 95.9%Syntax 94.2% 90.9%Semantics 87.4% 83.7%Syntax (Repair Correction) 96.0% 93.1%Semantics (Repair Correction) 89.1% 86.0%*This research was supported by the Advanced ResearchProjects Agency under Contract ONR N00014-90~C-0085 with theOffice of Naval Research.
The views and conclusions contained inthis document are those of the authors and should not he inter-preted as necessarily representing the official policies, either ex-pressed or implied, of the Advanced Research Projects Agency ofthe U.S. Government.Table 1: Domain Coverage by Component2.
SYSTEM DESCRIPT IONGemini maintains a firm separation between thelanguage- and domain-specific portions of the system,43and the underlying infrastructure and execution strate-gies.
The Gemini kernel consists of a set of compilers tointerpret he high-level anguages in which the lexiconand syntactic and semantic grammar ules are written,as wellI as the parser, semantic interpretation, quanti-fier scoping, and repair correction mechanisms, as wellas all other aspects of Gemini that are not specific toa language or domain.
Although this paper describesthe lexicon, grammar, and semantics of English, Gem-ini has also been used in a Japanese spoken languageunderstanding system \[10\].2 .1 .
Grammar  Formal i smGemini includes a midsized constituent grammar of En-glish (described in section 2.3), a small utterance gram-mar for assembling constituents into utterances (de-scribed in section 2.7), and a lexicon.
All three are writ-ten in a variant of the unification formalism used in theCore Language Engine \[1\].The basic building block of the grammar formalism is acategory with feature-constraints.
Here is an example:up: \[wh=ynq, case=(nomVacc),pers_num=(3rdAsg)\]This category can be instantiated by any noun phrasewith the value ynq for its wh feature (which means itmust be a wh-bearing noun phrase like which book, who,or whose mother), either acc (accusative) or nora (nom-inative) for its case feature, and the conjunctive value3rdAsg (third and singular) for its person-number fea-ture.
This formalism is related directly to the Core Lan-guage Engine, but more conceptually it is closely relatedto that of other unification-based grammar formalismswith a context-free skeleton, such as PATR-II \[21\], Cat-egorial Unification Grammar \[23\], Generalized Phrase-Structure Grammar \[6\] and Lexical Functional Grammar\[3\].We list some ways in which Gemini differs from otherunification formalisms.
Since many of the most inter-esting issues regarding the formalism concern typing, wedefer discussing motivation until section 2.5.. Gemini uses typed-unification.
Each category hasa set of features declared for it.
Each feature has adeclared value-space of possible values (value spacesmay be shared by different features).
Feature struc-tures in Gemini can be recursive, but only by havingcategories in their value-space, so typing is also re-cursive.
Typed feature-structures are also used inHPSG \[19\].
One important difference with the usein Gemini is that Gemini has no type-inheritance.2.
Some approaches do not assume a syntactic skeletonof category-introducing rules (for example, Func-tional Unification Grammar \[11\]).
Some make suchrules implicit (for example, the various categorialunification approaches, uch as Unification Catego-rial Grammar \[24\])..
Even when a syntactic skeleton is assumed, someapproaches do not distinguish the category of a con-stituent (np, vp, etc.)
from its other features (forexample, pers_num, gapsin, gapsout).
Thus for ex-ample, in one version of GPSG, categories were sim-ply feature bundles (attribute-value structures) andthere was a feature MAJ taking values like N,V,A,Pwhich determined the major category of constituent.4.
Gemini does not allow rules schematizing over syn-tactic categories.2 .2 .
Lex iconThe Gemini lexicon uses the same category notation asthe Gemini syntactic rules.
Lexical categories are typesas well, with sets of features defined for them.
The lexicalcomponent of Gemini includes the lexicon of base forms,lexical templates, morphological rules, and the lexicaltype and feature default specifications.The Gemini lexicon used for the air-travel planning do-main contains 1,315 base entries.
These expand by mor-phological rules to 2,019.
In the 5875 utterance train-ing set, 52 sentences contained unknown words (0.9%),compared to 31 sentences in the 756 utterance fair test(4.1%).2 .3 .
Const i tuent  GrammarA simplified example of a syntactic rule is:syn (whq_ynq_s 1ash.up,\[s: \[sentence_type=whq, form=tnsd,gapsin=G, gapsout=G\],up: \[wh=ynq, persmum=N\],s : \[sentence_type=ynq, form=tnsd,gaps in=up: \[pers mum=N\], gapsout =null\] \] ).This syntax rule (named whq_ynq_slash_up) says thata sentence (category s) can be built by finding a nounphrase (category up) followed by a sentence.
It requiresthat the daughter np have the value ynq for its wh fea-ture and that it have the value N (a variable) for itsperson-number feature.
It requires that the daughtersentence have a category value for its gapsin feature,namely an np with a person number value N, which isthe same as the person number value on the wh;bearingnoun phrase.
The interpretation of the entire rule isthat a gapless entence with sentence_type whq can be44built by finding a wh-phrase followed by a sentence witha noun-phrase gap in it that has the same person numberas the wh-phrase.Semantic rules are written in much the same rule format,except that in a semantic rule, each of the constituentsmentioned in the phrase-structure skeleton is associatedwith a logical form.
Thus, the semantics for the ruleabove is:sem (whq_ynq_s lash_np,\[(\[whq,S\], s: \[\]),(Np, np:\[\]),(S, s: \[gapsin=np: \[gapsem=Np\]\] )\]).Here the semantics of the mother s is just the seman-tics of the daughter s with the illocutionary force markerwhq wrapped around it.
Also the semantics of the s gap'snp's gapsem has been unified with the semantics of thewh-phrase.
Through a succession of unifications this willend up assigning the wh-phrases semantics to the gap po-sition in the argument structure of the s. Although eachsemantic rule must be keyed to a pre-existing syntacticrule, there is no assumption of rule-to-rule uniqueness.Any number of semantic rules maybe written for a sin-gle syntactic rule.
We discuss some further details of thesemantics in section .The constituent grammar used in Gemini contains 243syntactic rules, and 315 semantic rules.
Syntactic ov-erage on the 5875 utterance training set was 94.2%, andon the 756 utterance test set was 90.9%.2.4.
ParserSince Gemini was designed with spoken language inter-pretation in mind, key aspects of the Gemini parser aremotivated by the increased needs for robustness and ef-ficiency that characterize spoken language.
Gemini usesessentially a pure bottom-up chart parser, with some lim-ited left-context constraints applied to control creationof categories containing syntactic gaps.Some key properties of the parser are:.
The parser is all-paths bottom-up, so that all pos-sible edges admissible by the grammar are found.?
The parser uses subsumption checking to reduce thesize of the chart.
Essentially, an edge is not addedto the chart if it is less general than a pre-existingedge, and pre-existing edges are removed from thechart if the new edge is more general.?
The parser is on-line \[7\], essentially meaning thatall edges that end at position i are constructedbefore any that end at position i + 1.
This fea-ture is particularly desirable if the final architectureof the speech-understanding system couples Geminitightly with the speech recognizer, since it guaran-tees for any partial recognition input that all possi-ble constituents will be built.An important feature of the parser is the mechanismused to constrain the construction of categories contain-ing syntactic gaps.
In earlier work \[17\], we showed thatapproximately 80% of the edges built in an all-pathsbottom-up arser contained gaps, and that it is possibleto use prediction in a bottom-up arser only to constrainthe gap categories, without requiring prediction for non-gapped categories.
This limited form of left context con-straint greatly reduces the total number of edges builtfor a very low overhead.
In the 5875 utterance train-ing set, the chart for the average sentence contained 313edges, but only 23 predictions.2.5.
TypingThe main advantage of typed-unification is for grammardevelopment.
The type information on features allowsthe lexicon, grammar, and semantics compilers to pro-vide detailed error analysis regarding the flow of valuesthrough the grammar, and warn if features are assignedimproper values, or variables of incompatible types areunified.
Since the type-analysis is performed statically atcompile-time, there is no run-time overhead associatedwith adding types to the grammar.Syntactic categories play a special role in the typing-scheme of Gemini.
For each syntactic ategory, Geminimakes a set of declarations stipulating its allowable fea-tures and the relevant value spaces.
Thus, the distinctionbetween the syntactic ategory of a constituent and itsother features can be cashed out as follows: the syntac-tic category can be thought of as the feature-structuretype.
The only other types needed by Gemini are thevalue-spaces used by features.
Thus for example, thetype v (verb) admits a feature v:form, whose value-spacevform-types can be instantiated with values like presentparticiple, finite, and past participle.
Since all recursivefeatures are category-valued, these two kinds of typesSUf~Ce.2.6.
Interleaving Syntactic and SemanticInformationSorta l  Const ra in ts  Selectional restrictions are im-posed in Gemini through the sorts mechanism.
Selec-tional restrictions include both highly domain specificinformation about predicate-argument a d very generalpredicate restrictions.
For example, in our application45Edges TimeSyntax Only 197 3.4 sec.Syntax + Semantics 234 4.47 sec.Syntax + Semantics + Sorts 313 13.5 sec.Table 2: Average number of edges built by interleavedprocessingthe object of the transitive verb depart (as in flights de-parting Boston) is restricted to be an airport or a city,obviously a domain-specific requirement.
But the samemachinery also restricts a determiner like all to take twopropositions, and an adjective like further to take dis-tances as its measure-specifier (as in thirty miles fur-ther).
In fact, sortal constraints are assigned to everyatomic predicate and operator appearing in the logicalforms constructed by the semantic rules.Sorts are located in a conceptual hierarchy and are im-plemented as Prolog terms such that more general sortssubsume more specific sorts \[16\].
This allows the sub-sumption checking and packing in the parser to sharestructure whenever possible.
Semantic coverage whenapplying sortal constraints was 87.4% on the trainingset, and on the test set was 83.7%.In ter leav ing  Semant ics  w i th  Pars ing  In Geminisyntactic and semantic processing is fully interleaved.Building an edge requires that syntactic onstraints beapplied, which results in a tree structure, to which se-mantic rules can be applied, which results in a logicalform to which sortal contraints can be applied.Table 2 contains average dge counts and parse timingstatistics I statistics for the 5875 utterance training set.2 .7 .
Ut terance  Grammar  and  Ut teranceParserThe constituent parser uses the constituent grammar tobuild all possible categories bottom-up, independent oflocation within the string.
Thus, the constituent parserdoes not force any constituent to occur either at the be-ginning of the utterance, or at the end.
The utteranceparser is a top-down back-tracking parser that uses a dif-ferent grammar called the utterance grammar to glue theconstituents found during constituent parsing togetherto span the entire utterance.Many systems \[4\], \[9\], \[20\], \[22\] have added robustness1 Gemini is implemented primarily in Quintus Prolog version3.1.1.
All timing numbers given in this paper were run on a lightlyloaded Sun Spaxcstation 2 with at least 48MB of memory.
Undernormal conditions, Gemini runs in under 12MB of memory.with a similar post-processing phase.
The approachtaken in Gemini differs in that the utterance grammaruses the same syntactic and semantic rule formalismused by the constituent grammar.
Thus the same kindsof logical forms built during constituent-parsing are theoutput of utterance-parsing, with the same sortal con-straints enforced.
For example, an utterance consistingof a sequence of modifier fragments (like on Tuesday at3'o'clock on United) is interpreted as a conjoined prop-erty of a flight, because the only sort of thing in the ATISdomain which can be on Tuesday at 3'o'clock on Unitedis a flight.The utterance grammar is significantly smaller than theconstituent grammar, only 37 syntactic rules and 43 se-mantic rules.2 .8 .
Repa i rsGrammatical disfluencies occur frequently in sponta-neous spoken language.
We have implemented a com-ponent to detect and correct a large sub-class of thesedisfluencies (called repairs, or self-corrections) wherethe speaker intends that the meaning of the utterancebe gotten by deleting one or more words.
Often, thespeaker gives clues of their intention by repeating wordsor adding cue words that signal the repair:(1) a.
How many American airline flights leave Denveron June June tenth.b.
Can you give me information on all the flightsfrom San Francisco no from Pittsburgh to SanFrancisco n Monday.The mechanism used in Gemini to detect and correct re-pairs is currently applied as a fall-back mechanism if nosemantically acceptable interpretation is found for thecomplete utterance.
The mechanism finds sequences ofidentical or related words, possibly separated by a cueword indicating a repair, and attempts to interpret hestring with the first of the sequences deleted.
This ap-proach is presented in detail in \[2\].The repair correction mechanism helps increase the syn-tactic and semantic overage of Gemini (as reported inTable 1), at the cost miscorrecting some sentences thatdo not contain repairs.
In the 5875 utterance train-ing set, there were 178 sentences containing nontriv-ial repairs 2, of which Gemini found 89 (50%).
Of thesentences Gemini corrected, 81 were analyzed correctly(91%), 8 contained repairs, but were corrected wrongly.2For these results, we ignored repairs consisting of only an iso-late fragment word, or sentence-initial fil er words like "yes" and"okay".46In the entire training set, Gemini only misidentified 15sentences (0.25%) as containing repairs when they didnot.
Similarly, the 756 utterance test set contained 26repairs, of which Gemini found 11 (42%).
Of those 11, 8were analyzed correctly (77%), and 3 were analysed in-correctly.
In the training set, 2 sentences were misiden-tiffed as containing repairs (0.26%).2.9.
Parse  P re ference  Mechan ismThe parse preference mechanism used in Gemini beginswith a simple strategy to disprefer parse trees contain-ing specific "marked" syntax rules.
As an example ofa dispreferred rule, consider: Book those three flights toBoston.
This sentence has a parse on which those threeis a noun phrase with a missing head (consider a contin-uation of the discourse Three of our clients have suffi-cient credit).
After penalizing such dispreferred parses,the preference mechanism applies attachment heuristicsbased on the work by Pereira \[18\].Pereira's paper shows how the heuristics of Minimal At-tachment and Right Association \[12\] can both be imple-mented using a bottom-up shift-reduce parser.
(2) (a) John sang a song for Mary.
(b) John canceled the room Mary reserved yester-day.Minimal Attachment selects for the tree with the fewestnodes, so in (2a), the parse which makes for Mary acomplement of sings is preferred.
Right Association se-lects for the tree which incorporates a constituent A intothe rightmost possible constituent (where rightmost heremeans beginning the furthest o the right).
Thus, in(2b) the parse in which yesterday modifies reserved ispreferred.The problem with these heuristics i that when they areformulated loosely, as in the previous paragraph, theyappear to conflict.
In particular, in (2a), Right Associ-ation seems to call for the parse which makes for Marya modifier of song.Pereira's goal is to show how a shift-reduce parser canenforce both heuristics without conflict and enforce thedesired preferences for examples like (2a) and (2b).
Heargues that Minimal Attachment and Right Associationcan be enforced in the desired way by adopting the fol-lowing heuristics for the oracle to resolve conflicts with:1.
Right Association: In a shift-reduce conflict, prefershifts to reduces.2.
Minimal Attachment: In a reduce-reduce onflict,prefer longer educes to shorter educes.Since these two principles never apply to the same choice,they never conflict.In Gemini, Pereira's heuristics are enforced when extract-ing syntactically and semantically well-formed parse-trees from the chart.
In this respect, our approachdiffers from many other approaches to the problem ofparse preferences, which make their preference decisionsas parsing progresses, pruning subsequent parsing paths\[5\], \[8\], \[14\].
Applying parse preferences requires com-paring two subtrees panning the same portion of theutterance.
For purposes of invoking Pereira's heuristics,the derivation of a parse can be represented as the se-quence of S's (Shift) and R's (Reduce) needed to con-struct the parse's unlabeled bracketing.
Consider, forexample, tim choice between two unlabeled bracketingsof ( 2a):(a) \[John \[sang \[a song \] \[for Mary \] \] \]S S S S R S S RRR(b) \[John \[sang \[ \[a song \] \[for Mary \]\] \]\]S S S S R S S RRRRThere is a shift for each word and a reduce for each rightbracket.
Comparison of the two parses consists implyof pairing the moves in the shift-reduce derivation fromleft to right.
Any parse making a shift move that cor-responds to a reduce move loses by Right Association.Any parse making a reduce move that corresponds to alonger reduce loses by Minimal Attachment.
In deriva-tion (b) above the third reduce move builds the con-stituent a song for Mary from two constituents, whilethe corresponding reduce in (a) builds sang a song forMary from three constituents.
Parse (b) thus loses byMinimal Attachment.Questions about the exact nature of parse preferences(and thus about he empirical adequacy of Pereira's pro-posal) still remain open, but the mechanism sketcheddoes provide plausible results for a number of examples.2.10.
Scop ingThe final logical form produced by Gemini is the re-sult of applying a set of quantifier scoping rules to thebest-interpretation chosen by the parse preference mech-anism.
The semantic rules build quasi-logical forms,which contain complete semantic predicate-argumentstructure, but do not specify quantifier scoping.
Thescoping algorithm that we use combines yntactic andsemantic information with a set of quantifier scopingpreference rules to rank the possible scoped logical formsconsistent with the quasi-logical form selected by parsepreferences.
This algorithm is described in detail in \[15\].473.
CONCLUSIONThis paper describes the approach we have taken to ree-solving the tension between overgeneration a d robust-ness in a spoken language understanding system.
Someaspects of Gemini are specifically oriented towards lim-iting overgeneration, such as the on-line property for theparser, and fully interleaved syntactic and semantic pro-cessing.
Other components, uch as the fragment andrun-on processing provided by the utterance grammar,and the correction of recognizable grammatical repairs,increase the robustness of Gemini.
We believe a robustsystem can still recognize and disprefer utterances con-taining recognition errors.We have described the current state of the research in theconstruction of the Gemini system.
Research is ongoingto improve the speed and coverage of Gemini, as wellas examining deeper integration strategies with speechrecognition, and integration of prosodic information intospoken language disambiguation.Re ferences1.
Alshawi, H. (ed) (1992).
The Core Language Engine,MIT Press, Cambridge.2.
Bear, J., Dowding, J., and Shriberg, E. (1992).
"Inte-grating Multiple Knowledge Sources for the Detectionand Correction of Repairs in Human-Computer Dialog",30th Annual Meeting of the Association for Computa-tional Linguists, Newark, DE, pp.
56-63.3.
Bresnan, J.
(ed) (1982) The Mental Representation fGrammatical Relations.
MIT Press, Cambridge.4.
Carbonell, J. and P. Hayes, P., (1983).
"Recovery Strate-gies for Parsing Extragrammatical Language," Ameri-can Journal of Computational Linguistics, Vol.
9, Num-bers 3-4, pp.
123-146.5.
Frazier, L. and Fodor, J.D.
(1978).
"The Sausage Ma-chine: A New Two-Stage Parsing Model", Cognition,Vol.
6, pp.
291-325.6.
Gazdar, G., Klein, E., Pullum, G., Sag, I.
(1982).
Gen-eralized Phrase Structure Grammar.
Harvard UniversityPress, Cambridge.7.
Graham, S., Harrison, M., Ruzzo, W. (1980).
"An Im-proved Context-Free Recognizer", in A CM Transactionson Programming Languages and Systems, Vol.
2, No.
3,pp.
415-462.8.
Hobbs,J., Bear, J.
(1990).
"Two Principles of Parse Pref-erence", in Proceedings of the 13th International Confer-ence on Computational Linguistics, Helsinki, Vol.
3, pp.162-167.9.
Hobbs, J., Appelt, D., Bear, J., Tyson, M., Magerman,D.
(1992).
"Robust Processing of Real-World Natural-Language Texts", in Text Based Intelligent Systems, ed.P.
Jacobs, Lawrence Erlbaum Associates, Hillsdale, N J,pp.
13-33.10.
Kameyama, M., (1992).
"The syntax and semanticsof the Japanese Language Engine."
forthcoming.
InMazuka, R. and N. Nagai Eds.
Japanese Syntactic Pro-cessing Hillsdale, N J: Lawrence Erlbaum Associates.11.
Kay, M. (1979).
"Functional Grammar".
In Proceedingsof the 5th Annual Meeting of the Berkeley LinguisticsSociety.
pp.
142-158.12.
Kimball, J.
(1973) "Seven Principles of Surface Struc-ture Parsing in Natural Language," Cognition, Vol.
2,No.
1, pp.
15-47.13.
MADCOW (1992).
"Multi-site Data Collection for aSpoken Language Corpus," Proceedings of the DARPASpeech and Natural Language Workshop, February 23-26, 1992.14.
Marcus, M. (1980).
A Theory of Syntactic Recognitionfor Natural Language, MIT Press, Cambridge, Mas-sachusetts.15.
Moran, D. (1988).
"Quantifier Scoping in the SRI CoreLanguage Engine", Proceedings of the 26th Annual Meet-ing of the Association for Computational Linguistics,State University of New York at Buffalo, Buffalo, NY,pp.
33-40.16.
Mellish, C. (1988).
"Implementing Systemic Classifica-tion by Unification".
Computational Linguistics Vol.
14,pp.
40-51.17.
Moore, R. and J. Dowding (1991).
"Efficient Bottom-upParsing," Proceedings of the DARPA Speech and NaturalLanguage Workshop, February 19-22, 1991, pp.
200-203.18.
Pereira, F. (1985).
"A New Characterization f Attach-ment Preferences.
", in Natural Language Parsing, Ed.
byDowty, D., Karttunen, L., and Zwicky, A., CambridgeUniversity Press, Cambridge, pp.
307-319.19.
Pollard, C. and Sag, I.
(in press) Information-BasedSyn-tax and Semantics, Vol.
2, CSLI Lecture Notes.20.
Seneff, S. (1992) "A Relaxation Method for Understand-ing Spontaneous Speech Utterances", in Proceedings ofthe Speech and Natural Language Workshop, Harriman,NY, pp.
299-304.21.
Shieber, S., Uszkoreit, H., Pereira, F., Robinson, J., andTyson, M. (1983).
"The Formalism and Implementationof PATR-II", In Grosz,B.
and Stickel,M.
(eds) Researchon Interactive Acquisition and Use of Knowledge, SRIInternational.
pp.
39-79.22.
Stallard, D. and Bobrow, R. (1992) "Fragment Process-ing in the DELPHI System", in Proceedings of the Speechand Natural Language Workshop, Harriman, NY, pp.305-310.23.
Uszkoreit, H. (1986) "Categorial Unification Gram-mars".
In Proceedings of the 11th International Con-ference on Computational Linguistics and the the 2~thAnnual Meeting of the Association for ComputationalLinguistics, Institut fur Kummunikkationsforschung undPhonetik, Bonn University.24.
Zeevat, H., Klein, E., and Calder, J.
(1987) "An Intro-duction to Unification Categorial Grammar".
In Had-dock, N.,Klein,E., Merrill, G.
(eds.)
Edinburgh Work-ing Papers in Cognitive Science, Volume 1: CategorialGrammar, Unification Grammar, and Parsing.48
