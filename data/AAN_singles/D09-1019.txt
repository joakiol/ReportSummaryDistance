Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 180?189,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPSentiment Analysis of Conditional SentencesRamanathan NarayananDept.
of EECSNorthwestern Universityramanathan.an@gmail.comBing Liu *Dept.
of Computer ScienceUniv.
of Illinois at Chicagoliub@cs.uic.eduAlok ChoudharyDept.
of EECSNorthwestern Universityalokchoudhary01@gmail.comAbstractThis paper studies sentiment analysis of condi-tional sentences.
The aim is to determinewhether opinions expressed on different topicsin a conditional sentence are positive, negativeor neutral.
Conditional sentences are one of thecommonly used language constructs in text.
Ina typical document, there are around 8% ofsuch sentences.
Due to the condition clause,sentiments expressed in a conditional sentencecan be hard to determine.
For example, in thesentence, if your Nokia phone is not good, buythis great Samsung phone, the author is posi-tive about ?Samsung phone?
but does not ex-press an opinion on ?Nokia phone?
(althoughthe owner of the ?Nokia phone?
may be nega-tive about it).
However, if the sentence doesnot have ?if?, the first clause is clearly nega-tive.
Although ?if?
commonly signifies a con-ditional sentence, there are many other wordsand constructs that can express conditions.This paper first presents a linguistic analysis ofsuch sentences, and then builds some super-vised learning models to determine if senti-ments expressed on different topics in a condi-tional sentence are positive, negative or neu-tral.
Experimental results on conditional sen-tences from 5 diverse domains are given todemonstrate the effectiveness of the proposedapproach.1 IntroductionSentiment analysis (also called opinion mining)has been an active research area in recent years.There are many research directions, e.g., senti-ment classification (classifying an opinion doc-ument as positive or negative) (e.g., Pang, Leeand Vaithyanathan, 2002; Turney, 2002), subjec-tivity classification (determining whether a sen-tence is subjective or objective, and its associatedopinion) (Wiebe and Wilson, 2002; Yu and Hat-zivassiloglou, 2003; Wilson et al 2004; Kim andHovy, 2004; Riloff and Wiebe, 2005), fea-ture/topic-based sentiment analysis (assigningpositive or negative sentiments to topics or prod-uct features) (Hu and Liu 2004; Popescu and Et-zioni, 2005; Carenini et al, 2005; Ku et al,2006; Kobayashi, Inui and Matsumoto, 2007;Titov and McDonald.
2008).
Formal definitionsof different aspects of the sentiment analysisproblem and discussions of major research direc-tions and algorithms can be found in (Liu, 2006;Liu, 2009).
A comprehensive survey of the fieldcan be found in (Pang and Lee, 2008).Our work is in the area of topic/feature-basedsentiment analysis or opinion mining (Hu andLiu, 2004).
The existing research focuses onsolving the general problem.
However, we arguethat it is unlikely to have a one-technique-fit-allsolution because different types of sentences ex-press sentiments/opinions in different ways.
Adivide-and-conquer approach is needed, e.g., fo-cused studies on different types of sentences.This paper focuses on one type of sentences, i.e.,conditional sentences, which have some uniquecharacteristics that make it hard to determine theorientation of sentiments on topics/features insuch sentences.
By sentiment orientation, wemean positive, negative or neutral opinions.
Bytopic, we mean the target on which an opinionhas been expressed.
In the product domain, a top-ic is usually a product feature (i.e., a componentor attribute).
For example, in the sentence, I donot like the sound quality, but love the design ofthis MP3 player, the product features (topics) are?sound quality?
and ?design?
of the MP3 playeras opinions have been expressed on them.
Thesentiment is positive on ?design?
but negative on?sound quality?.Conditional sentences are sentences that de-scribe implications or hypothetical situations andtheir consequences.
In the English language, avariety of conditional connectives can be used toform these sentences.
A conditional sentencecontains two clauses: the condition clause and*  This work was done when Bing Liu was on sabbaticalleave at Northwestern University.180the consequent clause, that are dependent oneach other.
Their relationship has significant im-plications on whether the sentence describes anopinion.
One simple observation is that senti-ment words (also known as opinion words) (e.g.,great, beautiful, bad) alone cannot distinguish anopinion sentence from a non-opinion one.
Aconditional sentence may contain many senti-ment words or phrases, but express no opinion.Example 1: If someone makes a beautiful andreliable car, I will buy it expresses no sentimenttowards any particular car, although ?beautiful?and ?reliable?
are positive sentiment words.This, however, does not mean that a condition-al sentence cannot express opinions/sentiments.Example 2: If your Nokia phone is not good,buy this great Samsung phone is positive aboutthe ?Samsung phone?
but does not express anopinion on the ?Nokia phone?
(although theowner of the ?Nokia phone?
may be negativeabout it).
Clearly, if the sentence does not have?if?, the first clause is negative.
Hence, a methodfor determining sentiments in normal sentenceswill not work for conditional sentences.
The ex-amples below further illustrate the point.In many cases, both the condition and conse-quent together determine the opinion.Example 3: If you are looking for a phonewith good voice quality, don?t buy this Nokiaphone is negative about the ?voice quality?
of the?Nokia phone?, although there is a positive sen-timent word ?good?
in the conditional clausemodifying ?voice quality?.
However, in the fol-lowing example, the opinion is just the opposite.Example 4: If you want a phone with goodvoice quality, buy this Nokia phone is positiveabout the ?voice quality?
of the ?Nokia phone?.As we can see, sentiment analysis of condi-tional sentences is a challenging problem.One may ask whether there is a large percen-tage of conditional sentences to warrant a fo-cused study.
Indeed, there is a fairly large pro-portion of such sentences in evaluative text.
Theycan have a major impact on the sentiment analy-sis accuracy.
Table 1 shows the percentage ofconditional sentences (sentences containing thewords if, unless, assuming, etc) and also the totalnumber of sentences from which we computedthe percentage in several user-forums.
The fig-ures definitely suggest that there is considerablebenefit to be gained by developing techniquesthat can analyze conditional sentences.To the best of our knowledge, there is no fo-cused study on conditional sentences.
This papermakes such an attempt.
Specifically, we deter-mine whether a conditional sentence (which isalso called a conditional in the linguistic litera-ture) expresses positive, negative or neutral opi-nions on some topics/features.
Since our focus ison studying how conditions and consequents af-fect sentiments, we assume that topics are given,which are product attributes since our data setsare user comments on different products.Our study is conducted from two perspectives.We start with the linguistic angle to gain a goodunderstanding of existing work on different typesof conditionals.
As conditionals can be expressedwith other words or phrases than if, we will studyhow they behave compared to if.
We will alsoshow that the distribution of these conditionalsbased on our data sets.With the linguistic knowledge, we perform acomputational study using machine learning.
Aset of features for learning is designed to capturethe essential determining information.
Note thatthe features here are data attributes used in learn-ing rather than product attributes or features.Three classification strategies are designed tostudy how to best perform the classification taskdue to the complex situation of two clauses andtheir interactions in conditional sentences.
Thesethree classification strategies are clause-based,consequent-based and whole-sentence-based.Clause-based classification classifies each clauseseparately and then combines their results.
Con-sequent-based classification only uses conse-quents for classification as it is observed that inconditional sentences, it is often the consequentsthat decide the opinion.
Whole-sentence-basedclassification treats the entire sentence as a wholein classification.
Experimental results on condi-tional sentences from diverse domains demon-strate the effectiveness of these classificationmodels.
The results indicate that the whole-sentence-based classifier performs the best.Since this paper only studies conditional sen-tences, a natural question is whether the pro-posed technique can be easily integrated into anoverall sentiment analysis or opinion mining sys-tem.
The answer is yes because a large propor-tion of conditional sentences can be detected us-ing conditional connectives.
Keyword search isTable 1: Percent of conditional sentencesSource % of cond.
(total #.
of sent.
)Cellphone 8.6 (47711)Automobile 5.0 (8113)LCD TV 9.92 (258078)Audio Systems 8.1 (5702)Medicine 8.29 (160259)181thus sufficient to identify such sentences for spe-cial handling using the proposed approach.
Thereare, however, some subtle conditionals which donot use normal conditional connectives and willneed an additional module to identify them, butsuch sentences are very rare as Table 2 indicates.2 The Problem StatementThe paper follows the feature-based sentimentanalysis model in (Hu and Liu 2004; Popescuand Etzioni, 2005).
We are particularly interestedin sentiments on products and services, which arecalled objects or entities.
Each object is de-scribed by its parts and attributes, which are col-lectively called features in (Hu and Liu, 2004;Liu, 2006).
For example, in the sentence, If thiscamera has great picture quality, I will buy it,?picture quality?
is a feature of the camera.
Forformal definitions of objects and features, pleaserefer to (Liu, 2006; Liu, 2009).
In this paper, weuse the term topic to mean feature as the featurehere can confuse with the feature used in ma-chine learning.
The term topic has also been usedby some researchers (e.g., Kim and Hovy, 2004;Stoyanov and Cardie, 2008).Our objective is to predict the sentimentorientation (positive, negative or neutral) on eachtopic that has been commented on in a sentence.The problem of automatically identifying fea-tures or topics being spoken about in a sentencehas been studied in (Hu and Liu, 2004; Popescuand Etzioni, 2005; Stoyanov and Cardie, 2008).In this work, we do not attempt to identify suchtopics automatically.
Instead, we assume thatthey are given because our objective is to studyhow the interaction of the condition and conse-quent clauses affects sentiments.
For this pur-pose, we manually identify all the topics.3 Conditional SentencesThis section presents the linguistic perspective ofconditional sentences.3.1 Conditional ConnectivesA large majority of conditional sentences areintroduced by the subordinating conjunction If.However, there are also many other conditionalconnectives, e.g., even if, unless, in case, assum-ing/supposing, as long as, etc.
Table 2 shows thedistribution of conditional sentences with variousconnectives in our data.
Detailed linguistic dis-cussions of them are beyond the scope of thispaper.
Interested readers, please refer to (Dec-lerck and Reed, 2001).
Below, we briefly discusssome important ones and their interpretations.If: This is the most commonly used conditionalconnective.
In addition to its own usage, it canalso be used to replace other conditional connec-tives, except some semantically richer connec-tives (Declerck and Reed, 2001).
Most (but notall) conditional sentences can be logically ex-pressed in the form ?If P then Q?, where P is thecondition clause and Q is the consequent clause.For practical purposes, we can automaticallysegment the condition and consequent clausesusing simple rules generated by observinggrammatical and linguistic patterns.Unless: Most conditional sentences containingunless can be replaced with equivalent sentenceswith an if and a not.
For example, the sentenceUnless you need clarity, buy the cheaper modelcan be expressed with If you don?t need clarity,buy the cheaper model.Even if: Linguistic theories claim that even if isa special case of a conditional which may notalways imply an if-then relationship (Gauker2005).
However, in our datasets, we have ob-served that the usage of even if almost alwaystranslates into a conditional.
Replacing even if byif will yield a sentence that is semantically simi-lar enough for the purpose of sentiment analysis.Only if, provided/providing that, on conditionthat: Conditionals involving these phrases typi-cally express a necessary condition, e.g., I willbuy this camera only if they can reduce the price.In such sentences, only usually does not affectwhether the sentence is opinionated or not.In case: Conditional sentences containing incase usually describe a precaution (I will closethe window in case it rains), prevention (I woresunglasses in case I was recognized), or a relev-ance conditional (In case you need a car, you canrent one).
Identifying the conditional and conse-quent clauses is not straightforward in many cas-es.
Further, in these instances, replacing in casewith if may not convey the intended meaning ofthe conditional.
We have ignored these cases inTable 2: Percentage of sentences with some mainconditional connectivesConditional Connective % of sentencesIf 6.42Unless 0.32Even if 0.17Until 0.10As (so) long as 0.09Assuming/supposing 0.04In case 0.04Only if 0.03182our analysis as we believe that they need a sepa-rate study, and also such sentences are rare.As (so) long as: Sentences with these connec-tives behave similarly to if and can usually bereplaced with if.Assuming/Supposing: These are a category ofconditionals that behave quite differently.
Theparticiples supposing and assuming create condi-tional sentences where the conditional clause andthe consequent clause can be syntactically inde-pendent.
It is quite difficult to distinguish thoseconditional sentences which contain an explicitconsequent clause and fit within our analysisframework.
In our data, most of such sentenceshave no consequent, thus representing assump-tions rather than opinions.
We omit these sen-tences in our study (they are also rare).3.2 Types of ConditionalsThere are extensive studies of conditional sen-tences (also known as conditionals) in linguis-tics.
Various theories have led to a number ofclassification systems.
Popular types of condi-tionals include actualization conditionals, infe-rential conditionals, implicative conditionals, etc(Declerck and Reed, 2001).
However, these clas-sifications are mainly based on semantic mean-ings which are difficult to recognize by a com-puter program.
To build classification models,we instead exploit canonical tense patterns ofconditionals, which are often used in pedagogicgrammar books.
They are defined based on tenseand are associated with general meanings.
How-ever, as described in (Declerck and Reed, 2001),their meanings are much more complex and nu-merous than their associated general meanings.However, the advantage of this classification isthat different types can be detected easily be-cause they depend on tense which can be pro-duced by a part-of-speech tagger.
As we will seein Section 5, canonical tense patterns help senti-ment classification significantly.
Below, we in-troduce the four canonical tense patterns.Zero Conditional:  This conditional form isused to describe universal statements like facts,rules and certainties.
In a zero conditional, boththe condition and consequent clauses are in thesimple present tense.
An example of such sen-tences is: If you heat water, it boils.First Conditional: Conditional sentences ofthis type are also called potential or indicativeconditionals.
They are used to express a hypo-thetical situation that is probably true, but thetruth of which is unverified.
In the first condi-tional, the condition is in the simple presenttense, and the consequent can be either in pasttense or present tense, usually with a modal aux-iliary verb preceding the main verb, e.g., If theacceleration is good, I will buy it.Second Conditional: This is usually used todescribe less probable situations, for stating pre-ferences and imaginary events.
The conditionclause of a second conditional sentence is in thepast subjunctive (past tense), and the consequentclause contains a conditional verb modifier (likewould, should, might), in addition to the mainverb, e.g., If the cell phone was robust, I wouldconsider buying it.Third conditional: This is usually used to de-scribe contrary-to-fact (impossible) past events.The past perfect tense is used in the conditionclause, and the consequent clause is in thepresent perfect tense, e.g., If I had bought thea767, I would have hated it.Based on the above definitions, we have devel-oped approximate part-of-speech (POS) tags 1 forthe condition and the consequent of each pattern(Table 3), which do not cover all sentences, butoverall they cover a majority of the sentences.For those not covered cases, the problem ismainly due to incomplete sentences and wronggrammars, which are typical for informal writ-ings in forum postings and blogs.
For example,the sentence, Great car if you need powerful ac-celeration, does not fall into any category, but itactually means It is a great car if you need po-werful acceleration, which is a zero conditional.To handle such sentences, we designed a set ofrules to assign them some default types:If condition contains VB/VBP/VBZ ?
0 conditionalIf consequent contains VB/VBP/VBS ?
0 conditionalIf condition contains VBG ?
1st conditionalIf condition contains VBD ?
2nd conditionalIf conditional contains VBN ?
3rd conditional.1 The list of Part-Of-Speech (POS) tags can be found at:http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.htmlTable 3: Tenses for identifying conditional typesType Linguistic Rule ConditionPOS tagsConsequentPOS tags0 If + simple present?
simple presentVB/VBP/VBZ VB/VBP/VBZ1 If + simple present?
will + bare infinitiveVB/VBP/VBZ/VBGMD + VB2 If + past tense?
would + infinitiveVBD MD + VB3 If + past perfect?
present perfectVBD+VBN MD + VBD183By using these rules, we can increase the sen-tence coverage from 73% to 95%.4 Sentiment Analysis of ConditionalsWe now describe our computational study.
Wetake a machine learning approach to predict sen-timent orientations.
Below, we first describe fea-tures used and then classification strategies.4.1 Feature constructionI.
Sentiment words/phrases and their locations:Sentiment words are words used to expresspositive or negative opinions, which are in-strumental for sentiment classification for ob-vious reasons.
We obtained a list of over 6500sentiment words gathered from varioussources.
The bulk of it is fromhttp://www.cs.pitt.edu/mpqa.
We also addedsome of our own.
Our list is mainly from thework in (Hu and Liu, 2004; Ding, Liu and Yu,2008).
In addition to words, there are phrasesthat describe opinions.
We have identified aset of such phrases.
Although obtaining thesephrases was time-consuming, it was only aone-time effort.
We will make this list availa-ble as a community resource.
It is possiblethat there is a better automated method forfinding such phrases, such as the methods in(Kanayama and Nasukawa, 2006; Breck, Choiand Cardie, 2007).
However, automaticallygenerating sentiment phrases has not been thefocus of this work as our objective is to studyhow the two clauses interact to determineopinions given the sentiment words andphrases are known.
Our list of phrases is byno means complete and we will continue toexpand it in the future.For each sentence, we also identify wheth-er it contains sentiment words/phrases in itscondition or consequent clause.
It was ob-served that the presence of a sentimentword/phrase in the consequent clause hasmore effect on the sentiment of a sentence.II.
POS tags of sentiment words: Sentimentwords may be used in several contexts, not allof which may correspond to an opinion.
Forexample, I trust Motorola and He has a trustfund both contain the word trust.
But only theformer contains an opinion.
In such cases, thePOS tags can provide useful information.III.
Words indicating no opinion: Similar to howsentiment words are related to opinions, thereare also a number of words which imply theopposite.
Words like wondering, thinking, de-bating are used when the user is posing aquestion or expressing doubts.
Thus suchphrases usually do not contribute an opinion,especially if they are in the vicinity of the ifconnective.
We search a window of 3 wordson either side of if to determine if there is anysuch word.
We have compiled a list of thesewords as well and use it in our experiments.IV.
Tense patterns: These are the canonical tensepatterns in Section 3.2.
They are used to gen-erate a set of features.
We identify the firstverb in both the condition and consequentclauses by searching for the relevant POS tagsin Table 3.
We also search for the words pre-ceding the main verb to find modal auxiliaryverbs, which are also used as features.V.
Special characters: The presence or absenceof ???
and ?!?.VI.
Conditional connectives: The conditionalconnective used in the sentence (if, even if,unless, only if, etc) is also taken as a feature.VII.
Length of condition and consequent clauses:Using simple linguistic and punctuation rules,we automatically segment a sentence intocondition and consequent clauses.
The num-bers of words in the condition and consequentclauses are then used as features.
We ob-served that when the condition clause is short,it usually has no impact on whether the sen-tence expresses an opinion.VIII.
Negation words: The use of negation wordslike not, don?t, never, etc, often alter the sen-timent orientation of a sentence.
For example,the addition of not before a sentiment wordcan change the orientation of a sentence frompositive to negative.
We consider a window of3-6 words before an opinion word, and searchfor these kinds of words.The following two features are singled out foreasy reference later.
They are only used in oneclassification strategy.
The first feature is an in-dicator, and the second feature has a parameter(which will be evaluated separately).(1).
Topic location: This feature indicates wheth-er the topic is in the conditional clause or theconsequent clause.(2).
Opinion weight: This feature considers onlysentiment words in the vicinity of the topic,since they are more likely to influence theopinion on the topic.
A window size is usedto control what we mean by vicinity.
The fol-lowing formula is used to assign a weight toeach sentiment word, which is inversely pro-portional to the distance (Dop) of the senti-ment word to the topic mention.
Sentiment184value is +1 for a positive word and -1 for anegative word.
Sentwords are the set ofknown sentiment words and phrases.
}{,1sentwordsopDweightop op??
?=?4.2 Classification StrategiesSince we are interested in topic-based sentimentanalysis, how to perform classification becomesan interesting issue.
Due to the two clauses, itmay not be sufficient to classify the whole sen-tence as positive or negative as in the same sen-tence, some topics may be positive and somemay be negative.
We propose three strategies.Clause-based classification: Since there are twoclauses in a conditional sentence, in this casewe build two classifiers, one for the conditionand one for the consequent.Condition classifier: This method classifies thecondition clause as expressing positive, nega-tive or neutral opinion.Training data: Each training sentence isrepresented as a feature vector.
Its class is posi-tive, negative or neutral depending on whetherthe conditional clause is positive, negative orneutral while considering both clauses.Testing: For each test sentence, the resultingclassifier predicts the opinion of the conditionclause.Topic class prediction: To predict the opi-nion on a topic, if the topic is in the conditionclause, it takes the predicted class of theclause.Consequent classifier: This classifier classi-fies the consequent clause as expressing posi-tive, negative or neutral opinion.Training data: Each training sentence isrepresented as a feature vector.
Its class is posi-tive, negative or neutral depending on whetherthe consequent clause is positive, negative orneutral while considering both clauses.Testing: For each test sentence, the resultingclassifier predicts the opinion of the conse-quent clause.Topic class prediction: To predict the opi-nion on a topic, if the topic is in the consequentclause, it takes the predicted class of theclause.The combination of these two classifiers iscalled the clause-based classifier.
It works asfollows: If a topic is in the conditional clause,the condition classifier is used, and if a topic isin the consequent clause, the consequent clas-sifier is used.Consequent-based classification: It is observedthat in most cases, the condition clause con-tains no opinion whereas the consequent clausereflects the sentiment of the entire sentence.Thus, this method uses (in a different way) on-ly the above consequent classifier.
If it classi-fies the consequent of a testing conditionalsentence as positive, all the topics in the wholesentence are assigned the positive orientation,and likewise for negative and neutral.Whole-sentence-based classification: In thiscase, a single classifier is built to predict theopinion on each topic in a sentence.Training data: In addition to the normal fea-tures, the two features (1) and (2) in Section4.1 are used for this classifier.
If a sentencecontains multiple topics, multiple training in-stances of the same sentence are created in thetraining data.
Each instance represents onespecific topic.
The class of the instance de-pends on whether the opinion on the topic ispositive, negative or neutral.Testing: For each topic in each test sentence,the resulting classifier predicts its opinion.Topic class prediction: This is not needed asthe prediction has been done in testing.5 Results and Discussions5.1 Data setsOur data consists of conditional sentences from 5different user forums: Cellphone, Automobile,LCD TV, Audio systems and Medicine.
We ob-tained user postings from these forums and ex-tracted the conditional sentences.
We then ma-nually annotated 1378 sentences from this cor-pus.
We also annotated the conditional and con-sequent clauses and identified the topics (orproduct features) being commented upon, andtheir sentiment orientations.
In our annotation,we observed that sentences with no sentimentwords or phrases almost never express opinions,i.e., only around 3% of them express opinions.There are around 26% sentences containing nosentiment words or phrases in our data.
To makethe problem challenging, we restrict our attentionto only those sentences that contain at least onesentiment word or phrase.
We have annotatedtopics from around 900 such sentences.
Table 4shows the class distributions of this data.
At theclause level (topics are not considered), we ob-serve that conditional clauses contain few opi-nions.
At the topic-level, 43.5% of the topicshave positive opinions, 26.4% of the topics havenegative opinions, and the rest have no opinions.185Table 4: Distribution of classesFor the annotation of data, we assume thattopics are known.
One student annotated the top-ics first.
Then two students annotated the senti-ments on the topics.
If a student found that a top-ic annotation is wrong, he will let us know.
Somemistakes and missing topics were found but therewere mainly due to oversights rather than disa-greements.
The agreement on sentiment annota-tions were computed using the Kappa score.
Weachieved the Kappa score of 0.63, which indi-cates strong agreements.
The conflicting caseswere then solved through discussion to reachconsensus.
We did not find anything that the an-notators absolutely disagree with each other.5.2 Experimental resultsWe now present the results for different combi-nations of features and classification strategies.For model building, we used Support VectorMachines (SVM), and the LIBSVM implementa-tion (Chang and Lin, 2001) with a Gaussian ker-nel, which produces the best results.
All the re-sults are obtained via 10-fold cross validation.Two-class classification: We first discuss theresults for a simpler version of the problem thatinvolves only sentences with positive or negativeorientations on some topics (at least one of theclauses must have a positive/negative opinion ona topic).
Neutral sentences are not used (~28% ofthe total).
The results of all three classifiers aregiven in Table 5.
The feature sets have been de-scribed in Section 4.1.
For all the experimentsbelow, features (1) and (2) are only used by thewhole-sentence-based classifier, but not used bythe other two classifiers for obvious reasons.
{I+II}: This setting uses sentiment words andphrases, their positions and POS tags as features(we used Brill?s POS tagger).
This can be seen asthe baseline.
We observe that both the conse-quent-based and whole-sentence-based classifiersperform dramatically better than the clause-basedclassifier.
The consequent-based classifier andthe whole-sentence-based classifier perform si-milarly (with the latter being slightly better).
Theprecision, recall, and F-score are computed as theaverage of the two classes.
{I+II+III}: In this setting, the list of specialnon-sentiment related words is added to the fea-ture set.
All three classifiers improve slightly.
{I+II+III+IV}: This setting includes all the ca-nonical tense based features.
We see marked im-provements for the consequent-based and whole-sentence-based classifiers both in term of accura-cy and F-score, which are statistically significantcompared to those of {I+II+III} at the 95% con-fidence level based on paired t-test.All: When all the features are used, the resultsof all the classifiers improve further.Two main observations worth mentioning:1.
Both the consequent-based and whole-sentence-based classifiers outperform theclause-based classifier dramatically.
This con-firms our observation that the consequentusually plays the key role in determining thesentiment of the sentence.
This is further rein-forced by the fact that the consequent-basedclassifier actually performs similarly to thewhole-sentence-based classifier.
The condi-tion clause seems to give no help.2.
The second observation is that the linguisticknowledge of canonical tense patterns helpssignificantly.
This shows that the linguisticknowledge is very useful.We also noticed that many misclassifications arecaused by grammatical errors, use of slangphrases and improper punctuations, which aretypical of postings on the Web.
Due to languageirregularities (e.g., wrong grammar, missingpunctuations, sarcasm, exclamations), the POStagger makes many mistakes as well causingsome errors in the tense based features.Three-class classification: We now move to themore difficult and realistic case of three classes:positive, negative and neutral (no-opinion).
Ta-ble 6 shows the results.
The trend is similar ex-cept that the whole-sentence-based classifier nowperforms markedly better than the consequent-based classifier.
We believe that this is becausethe neutral class needs information from both thecondition and consequent clauses.
This is evidentfrom the fact that there is little or no improve-ment after {I+II} for the consequent-based clas-sifier.
We also observe that the accuracies and F-scores for the three-class classification are lowerthan those for the two-class classification.
This isunderstandable due to the difficulty of determin-ing whether a sentence has opinion or not.
Again,statistical test shows that the canonical tense-based features help significantly.As mentioned in Section 4.1, the whole-sentence-based classifier only considers thosesentiment words in the vicinity of the topic underPositive Negative NeutralCondition 6.9% 6.7% 86.4%Consequent 49.3% 16.5% 34%Topic-level 43.5% 26.4% 29.9%186investigation.
For this, we search a window of nwords on either side of the topic mention.
Tostudy the effect of varying n, we performed anexperiment with various values of the windowsize and measured the overall accuracy for eachcase.
Table 7 shows how the accuracy changes aswe increase the window size.
We found that awindow size of 6-10 yielded good accuracies.This is because lower values of n lead to loss ofinformation regarding sentiment words as somesentiment words could be far from the topic.
Wefinally used 8, which gave the best results.We also investigated ways of using the nega-tion word in the sentence to correctly predict thesentiment.
One method is to use the negationword as a feature, as described in Section 4.1.Another technique is to reverse the orientation ofthe prediction for those sentences which containnegation words.
We found that the former tech-nique yielded better results.
The results reportedso far are based on the former approach.6 Related WorkThere are several research directions in sentimentanalysis (or opinion mining).
One of the maindirections is sentiment classification, which clas-sifies the whole opinion document (e.g., a prod-uct review) as positive or negative (e.g., Pang etal, 2002; Turney, 2002; Dave et al 2003; Ng etal.
2006; McDonald et al 2007).
It is clearly dif-ferent from our work as we are interested in con-ditional sentences.Another important direction is classifyingsentences as subjective or objective, and classify-ing subjective sentences or clauses as positive ornegative (Wiebe et al 1999; Wiebe and Wilson,2002, Yu and Hatzivassiloglou, 2003; Wilson etal, 2004; Kim and Hovy, 2004; Riloff andWiebe, 2005; Gamon et al2005; McDonald et al2007).
Although these works deal with sen-tences, they aim to solve the general problem.This paper argues that there is unlikely a one-technique-fit-all solution, and advocates dealingwith specific types of sentences differently byexploiting their unique characteristics.
Condi-tional sentences are the focus of this paper.
Tothe best of our knowledge, there is no focusedstudy on them.Several researchers also studied feature/topic-based sentiment analysis (e.g., Hu and Liu, 2004;Popescu and Etzioni, 2005; Ku et al 2006; Care-nini et al 2006; Mei et al 2007; Ding, Liu andYu, 2008; Titov and R. McDonald, 2008; Stoya-nov and Cardie, 2008; Lu and Zhai, 2008).
Theirobjective is to extract topics or product featuresin sentences and determine whether the senti-ments expressed on them are positive or nega-tive.
Again, no focused study has been made tohandle conditional sentences.
Effectively han-dling of conditional sentences can help their ef-fort significantly.Table 5: Two-class classification ?
positive and negativeClause-basedclassifierConsequent-basedclassifierWhole-sentence-basedclassifierAcc.
Prec.
Rec.
F Acc.
Prec.
Rec.
F Acc.
Prec.
Rec.
FI+II (senti.
words+POS) 39.9 42.8 34.0 37.9 69.1 72.9 67.1 69.8 68.9 73.7 68.13 70.8I+II+III (+ non-senti.
words)  41.5 44.9 37.1 40.6 69.3 73.9 66.3   69.9 69.2 73.7 63.5 71.0I+II+III+IV (+ tenses) 42.7 45.2 38.5 41.6 72.7 76.4 72.0 74.1   71.1 77.9 72.2 74.9All 43.2 46.1 38.9 42.2 73.3 77.0 72.7 74.8 72.3 77.8 73.6 75.6Table 6: Three-class classification ?
positive, negative and neutral (no opinion)Clause-basedclassifierConsequent-basedclassifierWhole-sentence-basedclassifierAcc.
Prec.
Rec.
F Acc.
Prec.
Rec.
F Acc.
Prec.
Rec.
FI+II (senti.
words+POS) 45.2 41.3 35.1 37.9 54.6 57.7 52.9 55.2 59.1 58.1 56.4 57.2I+II+III (+ non-senti.
words)  46.9 42.8 37.8 40.1 55.3 60.0 51.3 55.3 61.4 60.1 60.8 60.4I+II+III+IV (+ tenses) 50.3 48.7 40.9 44.5 57.3 64.0 50.0 56.1 64.6 63.3 63.9 63.6All 53.3 49.8 44.1 46.8 58.7 64.5 50.1 56.4 67.8 66.9 65.1 66.0Table 7: Accuracy of the whole-sentence-based classifier with varying window sizes (n)Window size 1 2 3 4 5 6 7 8 9 10Accuracy 66.1 62.6 64.1 64.8 65.3 65.7 66.3 67.3 66.9 66.8187In this work, we used many sentiment wordsand phrases.
These words and phrases are usuallycompiled using different approaches (Hatzivassi-loglou and McKeown, 1997; Kaji and Kitsure-gawa, 2006; Kanayama and Nasukawa, 2006;Esuli and Sebastiani, 2006; Breck et al 2007;Ding, Liu and Yu.
2008; Qiu et al 2009).
Thereare several existing lists produced by researchers.We used the one from the MPQA corpus(http://www.cs.pitt.edu/mpqa) with added phras-es of our own from (Ding, Liu and Yu.
2008).
Inour work, we also assume that the topics areknown.
(Hu and Liu, 2004; Popescu and Etzioni,2005; Kobayashi, Inui and Matsumoto, 2007;Stoyanov and Cardie, 2008) have studied top-ic/feature extraction.One existing focused study is on comparativeand superlative sentences (Jindal and Liu, 2006;Bos and Nissim, 2006; Fiszman et al 2007; Ga-napathibhotla and Liu, 2008).
Their work identi-fies comparative sentences, extracts comparativerelations in the sentences and analyzes compara-tive opinions (Ganapathibhotla and Liu, 2008).An example comparative sentence is ?Hondalooks better than Toyota?.
As we can see, com-parative sentences are entirely different fromconditional sentences.
Thus, their methods can-not be directly applied to conditional sentences.7 ConclusionTo perform sentiment analysis accurately, weargue that a divide-and-conquer approach isneeded, i.e., focused study on each type of sen-tences.
It is unlikely that there is a one-size-fit-allsolution.
This paper studied one type, i.e., condi-tional sentences, which have some unique cha-racteristics that need special handling.
Our studywas carried out from both the linguistic andcomputational perspectives.
In the linguisticstudy, we focused on canonical tense patterns,which have been showed useful in classification.In the computational study, we built SVM mod-els to automatically predict whether opinions ontopics are positive, negative or neutral.
Experi-mental results have shown the effectiveness ofthe models.In our future work, we will further improvethe classification accuracy and study relatedproblems, e.g., identifying topics/features.
Al-though there are some special conditional sen-tences that do not use easily recognizable condi-tional connectives and identifying them are use-ful, such sentences are very rare and spendingtime and effort on them may not be cost-effectiveat the moment.AcknowledgementsThis work was supported in part by DOE SCI-DAC-2: Scientific Data Management Center forEnabling Technologies (CET) grant DE-FC02-07ER25808, DOE FASTOS award number DE-FG02-08ER25848, NSF HECURA CCF-0621443, NSF SDCI OCI-0724599, and NSFST-HEC CCF-0444405.ReferencesJ.
Bos, and M. Nissim.
2006.
An Empirical Ap-proach to the Interpretation of Superlatives.EMNLP-2006.E.
Breck, Y. Choi, and C. Cardie.
2007.
Identify-ing expressions of opinion in context, IJCAI-2007.C.-C. Chang and C.-J.
Lin.
2001.
LIBSVM: alibrary for support vector machines.http://www.csie.ntu.edu.tw /~cjlin/libsvmG.
Carenini, R. Ng, and A. Pauls.
2006.
Interac-tive Multimedia Summaries of EvaluativeText.
IUI-2006.C.
Gauker.
2005.
Conditionals in Context.
MITPress.D.
Dave, A. Lawrence, and D. Pennock.
2003.Mining the Peanut Gallery: Opinion Extrac-tion and Semantic Classification of ProductReviews.
WWW-2003.R.
Declerck, and S. Reed.
2001.
Conditionals: AComprehensive Empirical Analysis.
Berlin:Mouton de Gruyter.X.
Ding, B. Liu, and P. S. Yu.
2008.
A holisticlexicon-based approach to opinion mining.WSDM-2008.A.
Esuli, and F. 2006.
Sebastiani.
Determiningterm subjectivity and term orientation for opi-nion mining, EACL-2006.M.
Fiszman, D. Demner-Fushman, F. Lang, P.Goetz, and T. Rindflesch.
2007.
InterpretingComparative Constructions in BiomedicalText.
BioNLP-2007.M.
Gamon, A. Aue, S. Corston-Oliver, S. and E.Ringger.
2005.
Pulse: Mining customer opi-nions from free text.
IDA-2005.G.
Ganapathibhotla and B. Liu.
2008.
IdentifyingPreferred Entities in Comparative Sentences.COLING-2008.V.
Hatzivassiloglou, and K. McKeown, K. 1997.188Predicting the Semantic Orientation of Adjec-tives.
ACL-EACL-1997.M.
Hu and B. Liu.
2004.
Mining and summariz-ing customer reviews.
KDD-2004.N.
Jindal, and B. Liu.
2006.
Mining ComparativeSentences and Relations.
AAAI-2006.N.
Kaji, and M. Kitsuregawa.
2006.
Automaticconstruction of polarity-tagged corpus fromHTML documents.
ACL-2006.H.
Kanayama, and T. Nasukawa.
2006.
FullyAutomatic Lexicon Expansion for Domain-Oriented Sentiment Analysis.
EMNLP-2006.S.
Kim and E. Hovy.
2004.
Determining the Sen-timent of Opinions.
COLING-2004.N.
Kobayashi, K. Inui and Y. Matsumoto.
2007.Extracting Aspect-Evaluation and Aspect-ofRelations in Opinion Mining.
EMNLP-2007.L.-W. Ku, Y.-T. Liang, and H.-H. Chen.
2006,Opinion Extraction, Summarization andTracking in News and Blog Corpora.
AAAI-CAAW.B.
Liu.
2006.
Web Data Mining: ExploringHyperlinks, Content and Usage Data.
Sprin-ger.B.
Liu.
2009.
Sentiment Analysis and Subjectivi-ty.
To appear in Handbook of Natural Lan-guage Processing, Second Edition, (editors:N. Indurkhya and F. J. Damerau), 2009 or2010.Y.
Lu, and C. X. Zhai.
2008.
Opinion integrationthrough semi-supervised topic modeling.WWW-2008.R.
McDonald, K. Hannan, T. Neylon, M. Wells,and J. Reynar.
2007.
Structured models forfine-to-coarse sentiment analysis.
ACL-2007Q.
Mei, X. Ling, M. Wondra, H. Su, and C. X.Zhai.
2007.
Topic Sentiment Mixture: Model-ing Facets and Opinions in Weblogs.
WWW-2007.V.
Ng, S. Dasgupta, and S. M. Niaz Arifin.
2006.Examining the role of linguistic knowledgesources in the automatic identification andclassification of reviews.
ACL-2006.B.
Pang and L. Lee.
2008.
Opinion Mining andSentiment Analysis.
Foundations and Trendsin Information Retrieval 2(1-2), pp.
1?135,2008.B.
Pang, L. Lee.
and S. Vaithyanathan.
2002.Thumbs up?
Sentiment Classification UsingMachine Learning Techniques.
EMNLP-2002.A-M. Popescu, and O. Etzioni.
2005.
ExtractingProduct Features and Opinions from Reviews.EMNLP-2005.G.
Qiu, B. Liu, J. Bu and C. Chen.
2009.
Ex-panding Domain Sentiment Lexicon throughDouble Propagation.
IJCAI-2009.E.
Riloff, and J. Wiebe.
2003.
Learning extrac-tion patterns for subjective expressions.EMNLP-2003.V.
Stoyanov, and C. Cardie.
2008.
Topic Identi-fication for fine-grained opinion analysis.COLING-2008.I.
Titov and R. McDonald.
2008.
A Joint Modelof Text and Aspect Ratings for SentimentSummarization.
ACL-2008.P.
Turney.
2002.
Thumbs Up or Thumbs Down?Semantic Orientation Applied to Unsuper-vised Classification of Reviews.
ACL-2002.J.
Wiebe, R. Bruce, and T. O?Hara.
1999.
Devel-opment and use of a gold standard data set forsubjectivity classifications.
ACL-1999.J.
Wiebe, and T. Wilson.
2002.
Learning to Dis-ambiguate Potentially Subjective Expressions.CoNLL-2002.T.
Wilson, J. Wiebe.
and R. Hwa.
2004.
Just howmad are you?
Finding strong and weakopinion clauses.
AAAI-2004.H.
Yu, and Y. Hatzivassiloglou.
2003.
Towardsanswering opinion questions: Separating factsfrom opinions and identifying the polarity ofopinion sentences.
EMNLP-2003.189
