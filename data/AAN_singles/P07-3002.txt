Proceedings of the ACL 2007 Student Research Workshop, pages 7?12,Prague, June 2007. c?2007 Association for Computational LinguisticsInducing Combinatory Categorial Grammars with Genetic AlgorithmsElias PonvertDepartment of LinguisticsUniversity of Texas at Austin1 University Station B5100Austin, TX 78712-0198 USAponvert@mail.utexas.eduAbstractThis paper proposes a novel approach to theinduction of Combinatory Categorial Gram-mars (CCGs) by their potential affinity withthe Genetic Algorithms (GAs).
Specifically,CCGs utilize a rich yet compact notation forlexical categories, which combine with rela-tively few grammatical rules, presumed uni-versal.
Thus, the search for a CCG consistsin large part in a search for the appropri-ate categories for the data-set?s lexical items.We present and evaluates a system utilizinga simple GA to successively search and im-prove on such assignments.
The fitness ofcategorial-assignments is approximated bythe coverage of the resulting grammar on thedata-set itself, and candidate solutions areupdated via the standard GA techniques ofreproduction, crossover and mutation.1 IntroductionThe discovery of grammars from unannotated ma-terial is an important problem which has receivedmuch recent research.
We propose a novel approachto this effort by leveraging the theoretical insights ofCombinatory Categorial Grammars (CCG) (Steed-man, 2000), and their potential affinity with Ge-netic Algorithms (GA) (Goldberg, 1989).
Specifi-cally, CCGs utilize an extremely small set of gram-matical rules, presumed near-universal, which op-erate over a rich set of grammatical categories,which are themselves simple and straightforwarddata structures.
A search for a CCG grammar fora language can be construed as a search for ac-curate category assignments to the words of thatlanguage, albeit over a large landscape of poten-tial solutions.
GAs are biologically-inspired generalpurpose search/optimization methods that have suc-ceeded in these kinds of environments: wherein so-lutions are straightforwardly coded, yet neverthelessthe solution space is complex and difficult.We evaluate a system that uses a GA to suc-cessively refine a population of categorial lexiconsgiven a collection of unannotated training material.This is an important problem for several reasons.First of all, the development of annotated trainingmaterial is expensive and difficult, and so schemesto discover linguistic patterns from unannotated textmay help cut down the cost of corpora development.Also, this project is closely related to the problem ofresolving lexical gaps in parsing, which is a doggedproblem for statistical parsing systems in CCG, eventrained in a supervised manner.
Carrying over tech-niques from this project to that could help solve amajor problem in CCG parsing technology.Statistical parsing with CCGs is an active areaof research.
The development of CCGbank (Hock-enmaier and Steedman, 2005) based on the PennTreebank has allowed for the development of wide-coverage statistical parsers.
In particular, Hock-enmaier and Steedman (2001) report a generativemodel for CCG parsing roughly akin to the Collinsparser (Collins, 1997) specific to CCG.
WhereasHockenmaier?s parser is trained on (normal-form)CCG derivations, Clark and Curran (2003) presenta CCG parser trained on the dependency structureswithin parsed sentences, as well as the possiblederivations for them, using a log-linear (Maximum-Entropy) model.
This is one of the most accurateparsers for producing deep dependencies currentlyavailable.
Both systems, however, suffer from gaps7in lexical coverage.The system proposed here was evaluated againsta small corpus of unannotated English with the goalof inducing a categorial lexicon for the fragment.The system is not ultimately successful and fails toachieve the baseline category assignment accuracy,however it does suggest directions for improvement.2 Background2.1 Genetic AlgorithmsThe basic insight of a GA is that, given a problemdomain for which solutions can be straightforwardlyencoded as chromosomes, and for which candidatesolutions can be evaluated using a faithful fitnessfunction, then the biologically inspired operations ofreproduction, crossover and mutation can in certaincases be applied to multisets or populations of can-didate solutions toward the discovery of true or ap-proximate solutions.Among the applications of GA to computationallinguistics, (Smith and Witten, 1995) and (Korkmazand U?c?oluk, 2001) each present GAs for the induc-tion of phrase structure grammars, applied success-fully over small data-sets.
Similarly, (Losee, 2000)presents a system that uses a GA to learn part-of-speech tagging and syntax rules from a collection ofdocuments.
Other proposals related specifically tothe acquisition of categorial grammars are cited in?2.3.2.2 Combinatory Categorial GrammarCCG is a mildly context sensitive grammatical for-malism.
The principal design features of CCG is thatit posits a small set of grammatical rules that oper-ate over rich grammatical categories.
The categoriesare, in the simplest case, formed by the atomic cate-gories s (for sentence), np (noun phrase), n (com-mon noun), etc., closed under the slash operators/, \.
There is not a substantive distinction betweenlexical and phrasal categories.
The intuitive inter-pretation of non-atomic categories is as follows: aword for phrase of type A/B is looking for an itemof type B on the right, to form an item of type A.Likewise, an item of type A\B is looking for an itemof type B on the left.
type A.
For example, in thederivation in Figure 1, ?scores?
combines with thenp ?another goal?
to form the verb phrase ?scoresRonaldinhonpscores(s\np)/npanothernp/ngoaln>np>s\np<sFigure 1: Example CCG derivationApplicationA/B B ?> A B A\B ?< ACompositionA/B B/C ?>B A/C B\C A\B ?<B A\CCrossed-CompositionA/B B\C ?>B?
A\C B/C A\B ?<B?
A/CFigure 2: CCG Rulesanother goal?.
This, in turn, combines with the np?Ronaldinho?
to form a sentence.The example illustrates the rule of Application,denoted with < and > in derivations.
The schematafor this rule, along with the Composition rule (B)and the Crossed-Composition rule (B?
), are given inFigure 2.
The rules of CCG are taken as universals,thus the acquisition of a CCG grammar can be seenas the acquisition of a categorial lexicon.2.3 Related WorkIn addition to the supervised grammar systems out-lined in ?1, the following proposals have been putforward toward the induction of categorial gram-mars.Watkinson and Mandahar (2000) report a Catego-rial Grammar induction system related to that pro-posed here.
They generate a Categorial Grammarusing a fixed and limited set of categories and, uti-lizing an unannotated corpus, successively refine thelexicon by testing it against the corpus sentences oneat a time.
Using a constructed corpus, their strategyworked extremely well: 100% accuracy on lexicalcategory selection as well as 100% parsing accuracywith the resulting statistical CG parser.
With natu-rally occurring text, however, their system does notperform as well: approximately 77% lexical accu-racy and 37% parsing accuracy.One fundamental difference between the strategyproposed here and that of Watkinson and Manda-8har is that we propose to successively generate andevaluate populations of candidate solutions, ratherthan refining a single solution.
Also, while Watkin-son and Mandahar use logical methods to constructa probabilistic parser, the present system uses ap-proximate methods and yet derives symbolic parsingsystems.
Finally, Watkinson and Mandahar utilizean extremely small set of known categories, smallerthan the set used here.Clark (1996) outlines a strategy for the acquisi-tion of Tree-Adjoining Grammars (Joshi, 1985) sim-ilar to the one proposed here: specifically, he out-lines a learning model based on the co-evolution of aparser, which builds parse trees given an input stringand a set of category-assignments, and a shred-der, which chooses/discovers category-assignmentsfrom parse-trees.
The proposed strategy is not im-plemented and tested, however.Briscoe (2000) models the acquisition of catego-rial grammars using evolutionary techniques from adifferent perspective.
In his experiments, languageagents induced parameters for languages from otherlanguage agents generating training material.
Theacquisition of languages is not induced using GA perse, but the evolutionary development of languages ismodeled using GA techniques.Also closely related to the present proposal is thework of Villavicencio (2002).
Villavicencio presentsa system that learns a unification-based categorialgrammar from a semantically-annotated corpus ofchild-directed speech.
The learning algorithm isbased on a Principles-and-Parameters language ac-quisition scheme, making use of logical forms andword order to induce possible categories within atyped feature-structure hierarchy.
Her system hasthe advantage of not having to pre-compile a list ofknown categories, as did Watkinson and Mandaharas well as the present proposal.
However, Villav-icencio does make extensive use of the semanticsof the corpus examples, which the current proposaldoes not.
This is related to the divergent motivationsof two proposals: Villavicencio aims to present apsychologically realistic language learner and takesit as psychologically plausible that logical forms areaccessible to the language learner; the current pro-posal is preoccupied with grammar induction fromunannotated text, and assumes (sentence-level) log-ical forms to be inaccessible.n is the size of the populationA are candidate category assignmentsF are fitness scoresE are example sentencesm is the likelihood of mutationInitialize:for i?
1 to n :A[i]?
RANDOMASSIGNMENT()Loop:for i?
1 to length[A] :F [i]?
0P?
NEWPARSER(A[i])for j?
1 to length[E] :F [i]?
F [i]+SCORE(P.PARSE(E[i]))A?
REPRODUCE(A,F).
Crossover:for i?
1 to n?1 :CROSSOVER(A[i],A[i+1]).
Mutate:for i?
1 to n :if RANDOM() < m :MUTATE(A[i])Until: End conditions are metFigure 3: Pseudo-code for CCG induction GA.3 SystemAs stated, the task is to choose the correct CCG cat-egories for a set of lexical items given a collection ofunannotated or minimally annotated strings.
A can-didate solution genotype is an assignment of CCGcategories to the lexical items (types rather than to-kens) contained in the textual material.
A candi-date phenotype is a CCG parser initialized with thesecategory assignments.
The fitness of each candi-date solution is evaluated by how well its phenotype(parser) parses the strings of the training material.Pseudo-code for the algorithm is given in Fig.
3.For the most part, very simple GA techniques wereused; specifically:?
REPRODUCE The reproduction scheme utilizesroulette wheel technique: initialize a weightedroulette wheel, where the sections of the wheelcorrespond to the candidates and the weightsof the sections correspond to the fitness of thecandidate.
The likelihood that a candidate isselected in a roulette wheel spin is directly pro-portionate to the fitness of the candidate.?
CROSSOVER The crossover strategy is a simplepartition scheme.
Given two candidates C and9D, choose a center point 0 ?
i ?
n where n thenumber of genes (category-assignments), swapC[0, i]?
D[0, i] and D[i, n]?
C[i, n].?
MUTATE The mutation strategy simply swapsa certain number of individual assignments ina candidate solution with others.
For the ex-periments reported here, if a given candidateis chosen to be mutated, 25% of its genes aremodified.
The probability a candidate was se-lected is 10%.In the implementation of this strategy, the follow-ing simplifying assumptions were made:?
A given candidate solution only posits a singleCCG category for each lexical item.?
The CCG categories to assign to the lexicalitems are known a priori.?
The parser only used a subset of CCG ?
pureCCG (Eisner, 1996) ?
consisting of the Appli-cation and Composition rules.3.1 Chromosome EncodingsA candidate solution is a simplified assignment ofcategories to lexical items, in the following manner.The system creates a candidate solution by assigninglexical items a random category selection, as in:Ronaldinho (s\np)/npBarcelona ppkicks (s\np)/(s\np)...Given the fixed vocabulary, and the fixed categorylist, the representation can be simplified to lists ofindices to categories, indexed to the full vocabularylist:0 Ronaldinho1 Barcelona2 kicks......15 (s\np)/np...37 (s\np)/(s\np)...Then the category assignment can be construed asa finite function from word-indices to category-indices {0 7?
15,1 7?
42,2 7?
37, ...} or simply thevector ?15,42,37, ...?.
The chromosome encodingsfor the GA scheme described here are just this: vec-tors of integer category indices.3.2 FitnessThe parser used is straightforward implementationof the normal-form CCG parser presented by Eis-ner (1996).
The fitness of the parser is evaluated onits parsing coverage on the individual strings, whichis a score based on the chart output.
Several chartfitness scores were evaluated, including:?
SPANS The number of spans parsed?
RELATIVE The number of spans the stringparsed divided by the string length?
WEIGHTED The sum of the lengths of the spansparsedSee ?5.1 for a comparison of these fitness metrics.Additionally, the following also factored intoscoring parses:?
S-BONUS Add an additional bonus to candi-dates for each sentence they parse completely.?
PSEUDO-SMOOTHING Assign all parses atleast a small score, to help avoid prematureconvergence.
The metrics that count singletonspans do this informally.4 EvaluationThe system was evaluated on a small data-set of ex-amples taken from the World Cup test-bed includedwith the OpenCCG grammar development system1and simplified considerably.
This included 19 ex-ample sentences with a total of 105 word-types and613 tokens from (Baldridge, 2002).In spite of the simplifying assumption that an in-dividual candidate only assigns a single category toa lexical item, one can derive a multi-assignment ofcategories to lexemes from the population by choos-ing the top category elected by the candidates.
Itis on the basis of these derived assignments that thesystem was evaluated.
The examples chosen requireonly 1-to-1 category assignment, hence the relevantcategory from the test-bed constitutes the gold stan-dard (minus Baldridge (2002)?s modalities).
Thebaseline for this dataset, assigning np to all lexicalitems, was 28.6%.
The hypothesis is that optimizing1http://openccg.sf.net10Fitness Metric AccuracyCOUNT 18.5RELATIVE 22.0WEIGHTED 20.4Table 1: Final accuracy of the metricsparsing coverage with a GA scheme would correlatewith improved category-accuracy.The end-conditions apply if the parsing coveragefor the derived grammar exceeds 90%.
Such end-conditions generally were not met; otherwise, ex-periments ran for 100 generations, with a popula-tion of 50 candidates.
Because of the heavy relianceof GAs on pseudo-random number generation, indi-vidual experiments can show idiosyncratic successor failure.
To control for this, the experiments werereplicated 100 times each.
The results presentedhere are averages over the runs.5 Results5.1 Fitness MetricsThe various fitness metrics were each evaluated, andtheir final accuracies are reported in Table 1.
The re-sults were negative, as category accuracy did not ap-proach the baseline.
Examining the average systemaccuracy over time helps illustrate some of the issuesinvolved.
Figure 4 shows the growth of category ac-curacy for each of the metrics.
Pathologically, therandom assignments at the start of each experimenthave better accuracy than after the application of GAtechniques.Figure 5 compares the accuracy of the categoryassignments to the GA?s internal measure of its fit-ness, using the Count Spans metric as a point of ref-erence.
(The fitness metric is scaled for compari-son with the accuracy.)
While fitness, in the averagecase, steadily increases, accuracy does not increasewith such steadiness and degrades significantly inthe early generations.The intuitive reason for this is that, initially,the random assignment of categories succeeds bychance in many cases, however the likelihood of ac-curate or even compatible assignments to words thatoccur adjacent in the examples is fairly low.
TheGA promotes these assignments over others, appar-10152025300 10 20 30 40 50 60 70 80 90 100GenerationsCountRelativeWeightedBaselineFigure 4: Comparison of fitness metrics10152025300 10 20 30 40 50 60 70 80 90 100GenerationsAccuracyFitnessBaselineFigure 5: Fitness and accuracy: COUNTently committing the candidates to incorrect assign-ments early on and not recovering from these com-mitments.
The WEIGHTED and RELATIVE metricsare designed to try to overcome these effects by pro-moting grammars that parse longer spans, but theydo not succeed.
Perhaps exponential rather than lin-ear bonus for parsing spans of length greater thantwo would be effective.6 ConclusionsThis project attempts to induce a grammar fromunannotated material, which is an extremely diffi-cult problem for computational linguistics.
Withoutaccess to training material, logical forms, or otherrelevant features to aid in the induction, the systemattempts to learn from string patterns alone.
UsingGAs may aid in this process, but, in general, in-duction from string patterns alone takes much largerdata-sets than the one discussed here.The GA presented here takes a global perspectiveon the progress of the candidates, in that the indi-vidual categories assigned to the individual wordsare not evaluated directly, but rather as members ofcandidates that are scored.
For a system such as11this to take advantage of the patterns that arise outof the text itself, a much more fine-grained perspec-tive is necessary, since the performance of individ-ual category-assignments to words being the focusof the task.7 AcknowledgementsI would like to thank Jason Baldridge, Greg Kobele,Mark Steedman, and the anonymous reviewers forthe ACL Student Research Workshop for valuablefeedback and discussion.ReferencesJason Baldridge.
2002.
Lexically Specified DerivationalControl in Combinatory Categorial Grammar.
Ph.D.thesis, University of Edinburgh.Ted Briscoe.
2000.
Grammatical acquisition: Inductivebias and coevolution of language and the language ac-quisition device.
Language, 76:245?296.Stephen Clark and James R Curran.
2003.
Log-linearmodels for wide-coverage CCG parsing.
In Proceed-ings of EMNLP-03, pages 97?105, Sapporo, Japan.Robin Clark.
1996.
Complexity and the induction ofTree Adjoining Grammars.
Unpublished manuscript,University of Pennsylvania.Michael Collins.
1997.
Three generative, lexicalisedmodels for statistical parsing.
In Proceedings of ACL-97, pages 16?23, Madrid, Spain.Jason Eisner.
1996.
Efficient normal-form parsing forCombinatory Categorial Grammar.
In Proceedings ofACL-96, pages 79?86, Santa Cruz, USA.David E. Goldberg.
1989.
Genetic Algorithms in Search,Optimization and Machine Learning.
Addison-Wesley.Julia Hockenmaier and Mark Steedman.
2001.
Gener-ative models for statistical parsing with CombinatoryCategorial Grammar.
In Proceedings of ACL, pages335?342, Philadelphia, USA.Julia Hockenmaier and Mark Steedman.
2005.
CCG-bank: User?s manual.
Technical Report MC-SIC-05-09, Department of Computer and Information Science,University of Pennsylvania.Aravind Joshi.
1985.
An introduction to Tree AdjoiningGrammars.
In A. Manaster-Ramer, editor, Mathemat-ics of Language.
John Benjamins.Emin Erkan Korkmaz and Go?ktu?rk U?c?oluk.
2001.
Ge-netic programming for grammar induction.
In 2001Genetic and Evolutionary Computation Conference:Late Breaking Papers, pages 245?251, San Francisco,USA.Rober M. Losee.
2000.
Learning syntactic rules and tagswith genetic algorithms for information retrieval andfiltering: An empirical basis for grammatical rules.
In-formation Processing and Management, 32:185?197.Tony C. Smith and Ian H. Witten.
1995.
A genetic algo-rithm for the induction of natural language grammars.In Proc.
of IJCAI-95 Workshop on New Approaches toLearning for Natural Language Processing, pages 17?24, Montreal, Canada.Mark Steedman.
2000.
The Syntactic Process.
MIT,Cambridge, Mass.Aline Villavicencio.
2002.
The Acquisition of aUnification-Based Generalised Categorial Grammar.Ph.D.
thesis, University of Cambridge.Stephen Watkinson and Suresh Manandhar.
2000.
Un-supervised lexical learning with categorial grammarsusing the LLL corpus.
In James Cussens and Sas?oDz?eroski, editors, Language Learning in Logic, pages16?27, Berlin.
Springer.12
