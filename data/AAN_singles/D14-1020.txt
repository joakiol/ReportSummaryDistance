Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 172?176,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsTesting for Significance of Increased Correlation with Human JudgmentYvette Graham Timothy BaldwinDepartment of Computing and Information SystemsThe University of Melbournegraham.yvette@gmail.com, tb@ldwin.netAbstractAutomatic metrics are widely used in ma-chine translation as a substitute for hu-man assessment.
With the introductionof any new metric comes the question ofjust how well that metric mimics humanassessment of translation quality.
This isoften measured by correlation with hu-man judgment.
Significance tests are gen-erally not used to establish whether im-provements over existing methods such asBLEU are statistically significant or haveoccurred simply by chance, however.
Inthis paper, we introduce a significance testfor comparing correlations of two metrics,along with an open-source implementationof the test.
When applied to a range ofmetrics across seven language pairs, testsshow that for a high proportion of metrics,there is insufficient evidence to concludesignificant improvement over BLEU.1 IntroductionWithin machine translation (MT), efforts are on-going to improve evaluation metrics and find bet-ter ways to automatically assess translation qual-ity.
The process of validating a new metric in-volves demonstration that it correlates better withhuman judgment than a standard metric such asBLEU (Papineni et al., 2001).
However, althoughit is standard practice in MT evaluation to mea-sure increases in automatic metric scores with sig-nificance tests (Germann, 2003; Och, 2003; Ku-mar and Byrne, 2004; Koehn, 2004; Riezler andMaxwell, 2005; Graham et al., 2014), this hasnot been the case in papers proposing new met-rics.
Thus it is possible that some reported im-provements in correlation with human judgmentare attributable to chance rather than a systematicimprovement.In this paper, we motivate and introduce a novelsignificance test to assess the statistical signifi-cance of differences in correlation with humanjudgment for pairs of automatic metrics.
We ap-ply tests to the WMT-12 shared metrics task tocompare each of the participating methods, andfind that for a high proportion of metrics, there isnot enough evidence to conclude that they signifi-cantly outperform BLEU.2 Correlation with Human JudgmentA common means of assessing automatic MTevaluation metrics is Spearman?s rank correlationwith human judgments (Melamed et al., 2003),which measures the relative degree of monotonic-ity between the metric and human scores in therange [?1, 1].
The standard justification for cal-culating correlations over ranks rather than rawscores is to: (a) reduce anomalies due to absolutescore differences; and (b) focus evaluation on whatis generally the primary area of interest, namelythe ranking of systems/translations.An alternative means of evaluation is Pearson?scorrelation, which measures the linear correlationbetween a metric and human scores (Leusch et al.,2003).
Debate on the relative merits of Spear-man?s and Pearson?s correlation for the evaluationof automatic metrics is ongoing, but there is an in-creasing trend towards Pearson?s correlation, e.g.in the recent WMT-14 shared metrics task.Figure 1 presents the system-level results fortwo evaluation metrics ?
AMBER (Chen et al.,2012) and TERRORCAT (Fishel et al., 2012)?
over the WMT-12 Spanish-to-English metricstask.
These two metrics achieved the joint-highestrank correlation (?
= 0.965) for the task, but dif-fer greatly in terms of Pearson?s correlation (r =0.881 vs. 0.971, resp.).
The largest contributor tothis artifact is the system with the lowest humanscore, represented by the leftmost point in bothplots.172lllll lllllll?3 ?2 ?1 0 1 2 3?3?2?10123HumanAMBERSpearman: 0.965Pearson: 0.881(a) AMBERlllll lllllll?3 ?2 ?1 0 1 2 3?3?2?10123HumanTerrorCatSpearman: 0.965Pearson: 0.971(b) TERRORCATFigure 1: Scatter plot of human and automatic scores of WMT-12 Spanish-to-English systems for twoMT evaluation metrics (AMBER and TERRORCAT)Consistent with the WMT-14 metrics sharedtask, we argue that Pearson?s correlation is moresensitive than Spearman?s correlation.
There isstill the question, however, of whether an observeddifference in Pearson?s r is statistically significant,which we address in the next section.3 Significance TestingEvaluation of a new automatic metric, Mnew,commonly takes the form of quantifying the cor-relation between the new metric and human judg-ment, r(Mnew, H), and contrasting it with the cor-relation for some baseline metric, r(Mbase, H).
Itis very rare in the MT literature for significancetesting to be performed in such cases, however.We introduce a statistical test which can be usedfor this purpose, and apply the test to the evalua-tion of metrics participating in the WMT-12 metricevaluation task.At first gloss, it might seem reasonable to per-form significance testing in the following man-ner when an increase in correlation with humanassessment is observed: apply a significance testseparately to the correlation of each metric withhuman judgment, with the hope that the newlyproposed metric will achieve a significant correla-tion where the baseline metric does not.
However,besides the fact that the correlation between al-most any document-level metric and human judg-ment will generally be significantly greater thanzero, the logic here is flawed: the fact thatone correlation is significantly higher than zero(r(Mnew, H)) and that of another is not, does notnecessarily mean that the difference between thetwo correlations is significant.
Instead, a specifictest should be applied to the difference in corre-lations on the data.
For this same reason, con-fidence intervals for individual correlations withhuman judgment are also not particularly mean-ingful.In psychological studies, it is often the case thatsamples that data are drawn from are independent,and differences in correlations are computed on in-dependent data sets.
In such cases, the Fisher rto z transformation is applied to test for signifi-cant differences in correlations.
In the case of au-tomatic metric evaluation, however, the data setsused are almost never independent.
This meansthat if r(Mbase, H) and r(Mnew, H) are both> 0,the correlation between the metric scores them-selves, r(Mbase,Mnew), must also be > 0.
Thestrength of this correlation, directly between pairsof metrics, should be taken into account using asignificance test of the difference in correlation be-tween r(Mbase, H) and r(Mnew, H).3.1 Correlated CorrelationsCorrelations computed for two separate automaticmetrics on the same data set are not independent,and for this reason in order to test the difference incorrelation between them, the degree to which thepair of metrics correlate with each other should betaken into account.
The Williams test (Williams,173TerrorCatMETEOR SaganSempos PosFXEnErrCatsWBErrCatsAmberBErrCatsSimpBLEUBLEU.4cc TERTERBLEU?4ccSimpBLEUBErrCatsAmberWBErrCatsXEnErrCatsPosFSemposSaganMETEORTerrorCat(a) Pearson?s correlationTerrorCatMETEOR SaganSempos PosFXEnErrCatsWBErrCatsAmberBErrCatsSimpBLEUBLEU.4cc TERTERBLEU?4ccSimpBLEUBErrCatsAmberWBErrCatsXEnErrCatsPosFSemposSaganMETEORTerrorCat(b) Statistical significanceFigure 2: (a) Pearson?s correlation between pairs of automatic metrics; and (b) p-value of Williamssignificance tests, where a colored cell in row i (named on y-axis), col j indicates that metric i (namedon x-axis) correlates significantly higher with human judgment than metric j; all results are based on theWMT-12 Spanish-to-English data set.1959)1evaluates significance in a difference in de-pendent correlations (Steiger, 1980).
It is formu-lated as follows, as a test of whether the populationcorrelation betweenX1andX3equals the popula-tion correlation between X2and X3:t(n?
3) =(r13?
r23)?(n?
1)(1 + r12)?2K(n?1)(n?3)+(r23+r13)24(1?
r12)3,where rijis the Pearson correlation between Xiand Xj, n is the size of the population, and:K = 1?
r122?
r132?
r232+ 2r12r13r23The Williams test is more powerful than theequivalent for independent samples (Fisher r toz), as it takes the correlations between X1andX2(metric scores) into account.
All else beingequal, the higher the correlation between the met-ric scores, the greater the statistical power of thetest.4 Evaluation and DiscussionFigure 2a is a heatmap of the degree to which au-tomatic metrics correlate with one another whencomputed on the same data set, in the form of thePearson?s correlation between each pair of met-rics that participated in the WMT-12 metrics taskfor Spanish-to-English evaluation.
Metrics are or-dered in all tables from highest to lowest correla-tion with human assessment.
In addition, for the1Also sometimes referred to as the Hotelling?Williamstest.purposes of significance testing, we take the abso-lute value of all correlations, in order to compareerror-based metrics with non-error based ones.In general, the correlation is high amongst allpairs of metrics, with a high proportion of pairedmetrics achieving a correlation in excess of r =0.9.
Two exceptions to this are TERRORCAT(Fishel et al., 2012) and SAGAN (Castillo and Es-trella, 2012), as seen in the regions of yellow andwhite.Figure 2b shows the results of Williams sig-nificance tests for all pairs of metrics.
Since weare interested in not only identifying significantdifferences in correlations, but ultimately rankingcompeting metrics, we use a one-sided test.
Hereagain, the metrics are ordered from highest to low-est (absolute) correlation with human judgment.For the Spanish-to-English systems, approxi-mately 60% of WMT-12 metric pairs show a sig-nificant difference in correlation with human judg-ment at p < 0.05 (for one of the two metric di-rections).2As expected, the higher the correlationwith human judgment, the more metrics a givenmethod is superior to at a level of statistical signifi-cance.
Although TERRORCAT (Fishel et al., 2012)achieves the highest absolute correlation with hu-man judgment, it is not significantly better (p ?0.05) than the four next-best metrics (METEOR(Denkowski and Lavie, 2011), SAGAN (Castilloand Estrella, 2012), SEMPOS (Mach?a?cek and Bo-2Correlation matrices (red) are maximally filled, in con-trast to one-sided significance test matrices (green), where, ata maximum, fewer than half of the cells can be filled.174BLEU.4ccSimpBLEU Sempos Amber TER SaganMETEORTerrorCatBErrCatsXEnErrCats PosFWBErrCats WBErrCatsPosFXEnErrCatsBErrCatsTerrorCatMETEORSaganTERAmberSemposSimpBLEUBLEU?4cc(a) Czech-to-EnglishTerrorCat SemposMETEORSimpBLEU BLEU.4cc Amber PosFXEnErrCatsBErrCatsWBErrCats TERTERWBErrCatsBErrCatsXEnErrCatsPosFAmberBLEU?4ccSimpBLEUMETEORSemposTerrorCat(b) French-to-EnglishSemposMETEORTerrorCat AmberBErrCats PosFWBErrCatsXEnErrCatsSimpBLEU TER BLEU.4ccBLEU?4ccTERSimpBLEUXEnErrCatsWBErrCatsPosFBErrCatsAmberTerrorCatMETEORSempos(c) German-to-EnglishTerrorCatEnXErrCatsAmberBErrCatsWBErrCatsBLEU.4cc PosFSimpBLEU TER METEORMETEORTERSimpBLEUPosFBLEU?4ccWBErrCatsBErrCatsAmberEnXErrCatsTerrorCat(d) English-to-SpanishEnXErrCatsBErrCatsSimpBLEU METEORWBErrCatsAmberBLEU.4ccTerrorCat PosF TERTERPosFTerrorCatBLEU?4ccAmberWBErrCatsMETEORSimpBLEUBErrCatsEnXErrCats(e) English-to-FrenchTerrorCatSimpBLEU PosF BErrCatsEnXErrCatsAmber TERWBErrCatsBLEU.4ccMETEORMETEORBLEU?4ccWBErrCatsTERAmberEnXErrCatsBErrCatsPosFSimpBLEUTerrorCat(f) English-to-GermanFigure 3: Significance results for pairs of automatic metrics for each WMT-12 language pair.jar, 2011) and POSF (Popovic, 2012)).
There isnot enough evidence to conclude, therefore, thatthis metric is any better at evaluating Spanish-to-English MT system quality than the next four met-rics.Figure 3 shows the results of significance testsfor the six other language pairs used in the WMT-12 metrics shared task.3For no language pairis there an outright winner amongst the met-rics, with proportions of significant differences be-tween metrics for a given language pair rangingfrom 3% for Czech-to-English to 82% for English-to-French (p < 0.05).
The number of metrics thatsignificantly outperform BLEU for a given lan-guage pair is only 34% (p < 0.05), and no methodsignificantly outperforms BLEU over all languagepairs ?
indeed, even the best methods achieve sta-tistical significance over BLEU for only a smallminority of language pairs.
This underlines thedangers of assessing metrics based solely on cor-relation numbers, and emphasizes the importanceof statistical testing.It is important to note that the number of com-3We omit English-to-Czech due to some metric scores be-ing omitted from the WMT-12 data set.peting metrics a metric significantly outperformsshould not be used as the criterion for rankingcompeting metrics.
This is due to the fact thatthe power of the Williams test to identify signifi-cant differences between correlations changes de-pending on the degree to which the pair of met-rics correlate with each other.
Therefore, a metricthat happens to correlate strongly with many othermetrics would be at an unfair advantage, werenumbers of significant wins to be used to rank met-rics.
For this reason, it is best to interpret pairwisemetric tests in isolation.As part of this research, we have made avail-able an open-source implementation of statis-tical tests tailored to the assessment of MTmetrics available at https://github.com/ygraham/significance-williams.5 ConclusionsWe have provided an analysis of current method-ologies for evaluating automatic metrics in ma-chine translation, and identified an issue with re-spect to the lack of significance testing.
We in-troduced the Williams test as a means of cal-culating the statistical significance of differences175in correlations for dependent samples.
Analysisof statistical significance in the WMT-12 metricsshared task showed there is currently insufficientevidence for a high proportion of metrics to con-clude that they outperform BLEU.AcknowledgmentsWe wish to thank the anonymous reviewers fortheir valuable comments.
This research was sup-ported by funding from the Australian ResearchCouncil.ReferencesJulio Castillo and Paula Estrella.
2012.
Semantic tex-tual similarity for MT evaluation.
In Proceedings ofthe Seventh Workshop on Statistical Machine Trans-lation, pages 52?58, Montr?eal, Canada.Boxing Chen, Roland Kuhn, and George Foster.
2012.Improving AMBER, an MT evaluation metric.
InProceedings of the Seventh Workshop on Statisti-cal Machine Translation, pages 59?63, Montr?eal,Canada.Michael Denkowski and Alon Lavie.
2011.
Meteor1.3: Automatic metric for reliable optimization andevaluation of machine translation systems.
In Pro-ceedings of the Sixth Workshop on Statistical Ma-chine Translation, pages 85?91, Edinburgh, UK.Mark Fishel, Rico Sennrich, Maja Popovi?c, and Ond?rejBojar.
2012.
TerrorCat: a translation errorcategorization-based MT quality metric.
In Pro-ceedings of the Seventh Workshop on Statistical Ma-chine Translation, pages 64?70, Montr?eal, Canada.Ulrich Germann.
2003.
Greedy decoding for statis-tical machine translation in almost linear time.
InProceedings of the 2003 Conference of the NorthAmerican Chapter of the Assoc.
Computational Lin-guistics on Human Language Technology-Volume 1,pages 1?8, Edmonton, Canada.Yvette Graham, Nitika Mathur, and Timothy Baldwin.2014.
Randomized significance tests in machinetranslation.
In Proceedings of the ACL 2014 NinthWorkshop on Statistical Machine Translation, pages266?274, Baltimore, USA.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proceedings ofEmpirical Methods in Natural Language Processing2004 (EMNLP 2004), pages 388?395, Barcelona,Spain.Shankar Kumar and William Byrne.
2004.
MinimumBayes-risk decoding for statistical machine transla-tion.
In Proceedings of the 4th International Con-ference on Human Language Technology Researchand 5th Annual Meeting of the NAACL (HLT-NAACL2004), pages 169?176, Boston, USA.Gregor Leusch, Nicola Ueffing, and Hermann Ney.2003.
A novel string-to-string distance measurewith applications to machine translation evaluation.In Proceedings 9th Machine Translation Summit(MT Summit IX), pages 240?247, New Orleans,USA.Matou?s Mach?a?cek and Ond?rej Bojar.
2011.
Approx-imating a deep-syntactic metric for MT evaluationand tuning.
In Proceedings of the Sixth Workshop onStatistical Machine Translation, pages 92?98, Edin-burgh, UK.Dan Melamed, Ryan Green, and Joseph Turian.
2003.Precision and recall of machine translation.
In Pro-ceedings of the 2003 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics on Human Language Technology (HLT-NAACL 2003) ?
Short Papers, pages 61?63, Ed-monton, Canada.Franz Josef Och.
2003.
Minimum error rate train-ing in statistical machine translation.
In Proceed-ings of the 41st Annual Meeting of the Associationfor Computational Linguistics, pages 160?167, Sap-poro, Japan.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
BLEU: A method for automaticevaluation of machine translation.
Technical ReportRC22176 (W0109-022), IBM Research, Thomas J.Watson Research Center.Maja Popovic.
2012.
Class error rates for evaluationof machine translation output.
In Proceedings of theSeventh Workshop on Statistical Machine Transla-tion, pages 71?75, Montr?eal, Canada.Stefan Riezler and John T. Maxwell.
2005.
On somepitfalls in automatic evaluation and significance test-ing for mt.
In Proceedings of the ACL Workshopon Intrinsic and Extrinsic Evaluation Measures forMachine Translation and/or Summarization, pages57?64, Ann Arbor, USA.James H. Steiger.
1980.
Tests for comparing ele-ments of a correlation matrix.
Psychological Bul-letin, 87(2):245.Evan J. Williams.
1959.
Regression Analysis, vol-ume 14.
Wiley, New York, USA.176
