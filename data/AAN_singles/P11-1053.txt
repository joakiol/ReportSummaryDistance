Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 521?529,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsSemi-supervised Relation Extraction with Large-scale Word ClusteringAng Sun             Ralph Grishman             Satoshi SekineComputer Science DepartmentNew York University{asun,grishman,sekine}@cs.nyu.eduAbstractWe present a simple semi-supervisedrelation extraction system with large-scaleword clustering.
We focus onsystematically exploring the effectivenessof different cluster-based features.
We alsopropose several statistical methods forselecting clusters at an appropriate level ofgranularity.
When training on differentsizes of data, our semi-supervised approachconsistently outperformed a state-of-the-artsupervised baseline system.1 IntroductionRelation extraction is an important informationextraction task in natural language processing(NLP), with many practical applications.
The goalof relation extraction is to detect and characterizesemantic relations between pairs of entities in text.For example, a relation extraction system needs tobe able to extract an Employment relation betweenthe entities US soldier and US in the phrase USsoldier.Current supervised approaches for tackling thisproblem, in general, fall into two categories:feature based and kernel based.
Given an entitypair and a sentence containing the pair, bothapproaches usually start with multiple levelanalyses of the sentence such as tokenization,partial or full syntactic parsing, and dependencyparsing.
Then the feature based method explicitlyextracts a variety of lexical, syntactic and semanticfeatures for statistical learning, either generative ordiscriminative (Miller et al, 2000; Kambhatla,2004; Boschee et al, 2005; Grishman et al, 2005;Zhou et al, 2005; Jiang and Zhai, 2007).
Incontrast, the kernel based method does notexplicitly extract features; it designs kernelfunctions over the structured sentencerepresentations (sequence, dependency or parsetree) to capture the similarities between differentrelation instances (Zelenko et al, 2003; Bunescuand Mooney, 2005a; Bunescu and Mooney, 2005b;Zhao and Grishman, 2005; Zhang et al, 2006;Zhou et al, 2007; Qian et al, 2008).
Both lines ofwork depend on effective features, either explicitlyor implicitly.The performance of a supervised relationextraction system is usually degraded by thesparsity of lexical features.
For example, unless theexample US soldier has previously been seen in thetraining data, it would be difficult for both thefeature based and the kernel based systems todetect whether there is an Employment relation ornot.
Because the syntactic feature of the phrase USsoldier is simply a noun-noun compound which isquite general, the words in it are crucial forextracting the relation.This motivates our work to use word clusters asadditional features for relation extraction.
Theassumption is that even if the word soldier maynever have been seen in the annotated Employmentrelation instances, other words which share thesame cluster membership with soldier such aspresident and ambassador may have beenobserved in the Employment instances.
Theabsence of lexical features can be compensated by521the cluster features.
Moreover, word clusters mayimplicitly correspond to different relation classes.For example, the cluster of president may berelated to the Employment relation as in USpresident while the cluster of businessman may berelated to the Affiliation relation as in USbusinessman.The main contributions of this paper are: weexplore the cluster-based features in a systematicway and propose several statistical methods forselecting effective clusters.
We study the impactof the size of training data on cluster features andanalyze the performance improvements through anextensive experimental study.The rest of this paper is organized as follows:Section 2 presents related work and Section 3provides the background of the relation extractiontask and the word clustering algorithm.
Section 4describes in detail a state-of-the-art supervisedbaseline system.
Section 5 describes the cluster-based features and the cluster selection methods.We present experimental results in Section 6 andconclude in Section 7.2 Related WorkThe idea of using word clusters as features indiscriminative learning was pioneered by Miller etal.
(2004), who augmented name tagging trainingdata with hierarchical word clusters generated bythe Brown clustering algorithm (Brown et al, 1992)from a large unlabeled corpus.
They used differentthresholds to cut the word hierarchy to obtainclusters of various granularities for featuredecoding.
Ratinov and Roth (2009) and Turian etal.
(2010) also explored this approach for nametagging.
Though all of them used the samehierarchical word clustering algorithm for the taskof name tagging and reported improvements, wenoticed that the clusters used by Miller et al (2004)were quite different from that of Ratinov and Roth(2009) and Turian et al (2010).
To our knowledge,there has not been work on selecting clusters in aprincipled way.
We move a step further to exploreseveral methods in choosing effective clusters.
Asecond difference between this work and the aboveones is that we utilize word clusters in the task ofrelation extraction which is very different fromsequence labeling tasks such as name tagging andchunking.Though Boschee et al (2005) and Chan andRoth (2010) used word clusters in relationextraction, they shared the same limitation as theabove approaches in choosing clusters.
Forexample, Boschee et al (2005) chose clusters ofdifferent granularities and Chan and Roth (2010)simply used a single threshold for cutting the wordhierarchy.
Moreover, Boschee et al (2005) onlyaugmented the predicate (typically a verb or anoun of the most importance in a relation in theirdefinition) with word clusters while Chan and Roth(2010) performed this for any lexical featureconsisting of a single word.
In this paper, wesystematically explore the effectiveness of addingword clusters to different lexical features.3 Background3.1 Relation ExtractionOne of the well defined relation extraction tasks isthe Automatic Content Extraction1 (ACE) programsponsored by the U.S. government.
ACE 2004defined 7 major entity types: PER (Person), ORG(Organization), FAC (Facility), GPE (Geo-PoliticalEntity: countries, cities, etc.
), LOC (Location),WEA (Weapon) and VEH (Vehicle).
An entity hasthree types of mention: NAM (proper name), NOM(nominal) or PRO (pronoun).
A relation wasdefined over a pair of entity mentions within asingle sentence.
The 7 major relation types withexamples are shown in Table 1.
ACE 2004 alsodefined 23 relation subtypes.
Following most ofthe previous work, this paper only focuses onrelation extraction of major types.Given a relation instance ( , , )i jx s m m?, whereim  and jmare a pair of mentions and s  is thesentence containing the pair, the goal is to learn afunction which maps the instance x to a type c,where c is one of the 7 defined relation types or thetype Nil (no relation exists).
There are twocommonly used learning paradigms for relationextraction:Flat: This strategy performs relation detectionand classification at the same time.
One multi-classclassifier is trained to discriminate among the 7relation types plus the Nil type.Hierarchical: This one separates relationdetection from relation classification.
One binary1 Task definition: http://www.itl.nist.gov/iad/894.01/tests/ace/ACE guidelines: http://projects.ldc.upenn.edu/ace/522classifier is trained first to distinguish betweenrelation instances and non-relation instances.
Thiscan be done by grouping all the instances of the 7relation types into a positive class and the instancesof Nil into a negative class.
Then the thresholdedoutput of this binary classifier is used as trainingdata for learning a multi-class classifier for the 7relation types (Bunescu and Mooney, 2005b).Type ExampleEMP-ORG US presidentPHYS a military base in GermanyGPE-AFF U.S. businessmanPER-SOC a spokesman for the senatorDISC each of whomART US helicoptersOTHER-AFF Cuban-American peopleTable 1:  ACE relation types and examples from theannotation guideline 2 .
The heads of the two entitymentions are marked.
Types are listed in decreasingorder of frequency of occurrence in the ACE corpus.3.2 Brown Word ClusteringThe Brown algorithm is a hierarchical clusteringalgorithm which initially assigns each word to itsown cluster and then repeatedly merges the twoclusters which cause the least loss in averagemutual information between adjacent clustersbased on bigram statistics.
By tracing the pairwisemerging steps, one can obtain a word hierarchywhich can be represented as a binary tree.
A wordcan be compactly represented as a bit string byfollowing the path from the root to itself in the tree,assigning a 0 for each left branch, and a 1 for eachright branch.
A cluster is just a branch of that tree.A high branch may correspond to more generalconcepts while the lower branches it includesmight correspond to more specific ones.Brown et al (1992) described an efficientimplementation based on a greedy algorithm whichinitially assigned only the most frequent words intodistinct clusters.
It is worth pointing out that in thisimplementation each word occupies a leaf in thehierarchy, but each leaf might contain more thanone word as can be seen from Table 2.
The lengthsof the bit strings also vary among different words.2 http://projects.ldc.upenn.edu/ace/docs/EnglishRDCV4-3-2.PDFBit string Examples111011011100 US ?1110110111011 U.S. ?1110110110000 American ?1110110111110110 Cuban, Pakistani, Russian ?11111110010111 Germany, Poland, Greece ?110111110100 businessman, journalist, reporter1101111101111 president, governor, premier?1101111101100    senator, soldier, ambassador ?11011101110 spokesman, spokeswoman, ?11001100 people, persons, miners, Haitians110110111011111 base, compound, camps, camp ?110010111 helicopters, tanks, Marines ?Table 2: An example of words and their bit stringrepresentations obtained in this paper.
Words in bold arehead words that appeared in Table 1.4 Feature Based Relation ExtractionGiven a pair of entity mentions ,i jm m?
?and thesentence containing the pair, a feature basedsystem extracts a feature vector v  which containsdiverse lexical, syntactic and semantic features.The goal is to learn a function which can estimatethe conditional probability ( | )p c v , the probabilityof a relation type c given the feature vector v .
Thetype with the highest probability will be output asthe class label for the mention pair.We now describe a supervised baseline systemwith a very large set of features and its learningstrategy.4.1 Baseline Feature SetWe first adopted the full feature set from Zhou etal.
(2005), a state-of-the-art feature based relationextraction system.
For space reasons, we onlyshow the lexical features as in Table 3 and refer thereader to the paper for the rest of the features.At the lexical level, a relation instance can beseen as a sequence of tokens which form a fivetuple <Before, M1, Between, M2, After>.
Tokensof the five members and the interaction betweenthe heads of the two mentions can be extracted asfeatures as shown in Table 3.In addition, we cherry-picked the followingfeatures which were not included in Zhou et al(2005) but were shown to be quite effective forrelation extraction.Bigram of the words between the two mentions:This was extracted by both Zhao and Grishman(2005) and Jiang and Zhai (2007), aiming to523provide more order information of the tokensbetween the two mentions.Patterns:  There are three types of patterns: 1)the sequence of the tokens between the twomentions as used in Boschee et al (2005); 2) thesequence of the heads of the constituents betweenthe two mentions as used by Grishman et al (2005);3) the shortest dependency path between the twomentions in a dependency tree as adopted byBunescu and Mooney (2005a).
These patterns canprovide more structured information of how thetwo mentions are connected.Title list: This is tailored for the EMP-ORG typeof relations as the head of one of the mentions isusually a title.
The features are decoded in a waysimilar to that of Sun (2009).Position Feature DescriptionBefore BM1F first word before M1BM1L second word before M1M1 WM1 bag-of-words in M1HM1 head3 word of M1Between WBNULL when no word in betweenWBFL the only word in between whenonly one word in betweenWBF first word in between when atleast two words in betweenWBL last word in between when atleast two words in betweenWBO other words in between exceptfirst and last words when atleast three words in betweenM2 WM2 bag-of-words in M2HM2 head word of M2M12 HM12 combination of HM1 and HM2After AM2F  first word after M2AM2L  second word after M2Table 3: Lexical features for relation extraction.4.2 Baseline Learning StrategyWe employ a simple learning framework that issimilar to the hierarchical learning strategy asdescribed in Section 3.1.
Specifically, we first traina binary classifier to distinguish between relationinstances and non-relation instances.
Then ratherthan using the thresholded output of this binaryclassifier as training data, we use only theannotated relation instances to train a multi-classclassifier for the 7 relation types.
In the test phase,3 The head word of a mention is normally set as the last wordof the mention as in Zhou et al (2005).given a test instance x , we first apply the binaryclassifier to it for relation detection; if it is detectedas a relation instance we then apply the multi-classrelation classifier to classify it4.5 Cluster Feature SelectionThe selection of cluster features aims to answer thefollowing two questions: which lexical featuresshould be augmented with word clusters toimprove generalization accuracy?
How to selectclusters at an appropriate level of granularity?
Wewill describe our solutions in Section 5.1 and 5.2.5.1 Cluster Feature DecodingWhile each one of the lexical features in Table 3used by the baseline can potentially be augmentedwith word clusters, we believe the effectiveness ofa lexical feature with augmentation of wordclusters should be tested either individually orincrementally according to a rank of its importanceas shown in Table 4.
We will show theeffectiveness of each cluster feature in theexperiment section.Impor-tanceLexicalFeatureDescription oflexical featureCluster Feature1 HM HM1, HM2 andHM12HM1_WC,HM2_WC,HM12_WC2 BagWM WM1 and WM2 BagWM_WC3 HC a head5 of a chunkin contextHC_WC4 BagWC word of context BagWC_WCTable 4: Cluster features ordered by importance.The importance is based on linguistic intuitionsand observations of the contributions of differentlexical features from various feature based systems.Table 4 simplifies a relation instance as a threetuple <Context, M1, M2> where the Contextincludes the Before, Between and After from the4 Both the binary and multi-class classifiers output normalizedprobabilities in the range [0,1].
When the binary classifier?sprediction probability is greater than 0.5, we take theprediction with the highest probability of the multi-classclassifier as the final class label.
When it is in the range[0.3,0.5], we only consider as the final class label theprediction of the multi-class classifier with a probability whichis greater than 0.9.
All other cases are taken as non-relationinstances.5 The head of a chunk is defined as the last word in the chunk.524five tuple representation.
As a relation in ACE isusually short, the words of the two entity mentionscan provide more critical indications for relationclassification than the words from the context.Within the two entity mentions, the head word ofeach mention is usually more important than otherwords of the mention; the conjunction of the twoheads can provide an additional clue.
And ingeneral words other than the chunk head in thecontext do not contribute to establishing arelationship between the two entity mentions.The cluster based semi-supervised system worksby adding an additional layer of lexical featuresthat incorporate word clusters as shown in column4 of Table 4.
Take the US soldier as an example, ifwe decide to use a length of 10 as a threshold tocut the Brown word hierarchy to generate wordclusters, we will extract a cluster featureHM1_WC10=1101111101 in addition to thelexical feature HM1=soldier given that the full bitstring of soldier is  1101111101100 in Table 2.
(Note that the cluster feature is a nominal feature,not to be confused with an integer feature.
)5.2 Selection of ClustersGiven the bit string representations of all the wordsin a vocabulary, researchers usually use prefixes ofdifferent lengths of the bit strings to produce wordclusters of various granularities.
However, how tochoose the set of prefix lengths in a principled way?This has not been answered by prior work.Our main idea is to learn the best set of prefixlengths, perhaps through the validation of theireffectiveness on a development set of data.
To ourknowledge, previous research simply uses ad-hocprefix lengths and lacks this training procedure.The training procedure can be extremely slow forreasons to be explained below.Formally, let l  be the set of available prefixlengths ranging from 1 bit to the length of thelongest bit string in the Brown word hierarchy andlet m  be the set of prefix lengths we want to use indecoding cluster features, then the problem ofselecting effective clusters transforms to finding a| |m -combination of the set l which maximizessystem performance.
The training procedure can beextremely time consuming if we enumerate everypossible | |m -combination of l , given that | |mcan range from 1 to the size of l and the size ofl equals the length of the longest bit string which isusually 20 when inducing 1,000 clusters using theBrown algorithm.One way to achieve better efficiency is toconsider only a subset of l instead of the full set.
Inaddition, we limit ourselves to use sizes 3 and 4 form  for matching prior work.
This keeps the clusterfeatures to a manageable size considering thatevery word in your vocabulary could contribute toa lexical feature.
For picking a subset of l , wepropose below two statistical measures forcomputing the importance of a certain prefixlength.Information Gain (IG): IG measures thequality or importance of a feature f by computingthe difference between the prior entropy of classesC and the posterior entropy, given values V of thefeature f (Hunt et al, 1966; Quinlan, 1986).
Forour purpose, C is the set of relation types, f is acluster-based feature with a certain prefix lengthsuch as HM1_WC* where * means the prefixlength and a value v is the prefix of the bit stringrepresentation of HM1.
More formally, the IG of fis computed as follows:( ) ( ) log ( )( ( ) ( | ) log ( | ))c Cv V c CIG f p c p cp v p c v p c v??
??
?
??
???
?
(1)where the first and second terms refer to the priorand posterior entropies respectively.For each prefix length in the set l , we cancompute its IG for a type of cluster feature andthen rank the prefix lengths based on their IGs forthat cluster feature.
For simplicity, we rank theprefix lengths for a group of cluster features (agroup is a row from column 4 in Table 4) bycollapsing the individual cluster features into asingle cluster feature.
For example, we collapse the3 types: HM1_WC, HM2_WC and HM12_WC intoa single type HM_WC for computing the IG.Prefix Coverage (PC): If we use a short prefixthen the clusters produced correspond to the highbranches in the word hierarchy and would be verygeneral.
The cluster features may not provide moreinformative information than the words themselves.Similarly, if we use a long prefix such as the lengthof the longest bit string, then maybe only a few ofthe lexical features can be covered by clusters.
Tocapture this intuition, we define the PC of a prefixlength i as below:525( )( ) ( )iclcount fPC i count f?
(2)wherelf  stands for a lexical feature such as HM1andicfa cluster feature with prefix length i such asHM1_WCi, (*)count  is the number ofoccurrences of that feature in training data.Similar to IG, we compute PC for a group ofcluster features, not for each individual feature.In our experiments, the top 10 ranked prefixlengths based on IG and prefix lengths with PCvalues in the range [0.4, 0.9] were used.In addition to the above two statistical measures,for comparison, we introduce another two simplebut extreme measures for the selection of clusters.Use All Prefixes (UA): UA produces a clusterfeature at every available bit length with the hopethat the underlying supervised system can learnproper weights of different cluster features duringtraining.
For example, if the full bit representationof ?Apple?
is ?000?, UA would produce threecluster features: prefix1=0, prefix2=00 andprefix3=000.
Because this method does not needvalidation on the development set, it is the laziestbut the fastest method for selecting clusters.Exhaustive Search (ES): ES works by tryingevery possible combination of the set l and pickingthe one that works the best for the development set.This is the most cautious and the slowest methodfor selecting clusters.6 ExperimentsIn this section, we first present details of ourunsupervised word clusters, the relation extractiondata set and its preprocessing.
We then present aseries of experiments coupled with result analyses.We used the English portion of the TDT5corpora (LDC2006T18) as our unlabeled data forinducing word clusters.
It contains roughly 83million words in 3.4 million sentences with avocabulary size of 450K.
We left case intact in thecorpora.
Following previous work, we usedLiang?s implementation of the Brown clusteringalgorithm (Liang, 2005).
We induced 1,000 wordclusters for words that appeared at least twice inthe corpora.
The reduced vocabulary contains255K unique words.
The clusters are available athttp://www.cs.nyu.edu/~asun/data/TDT5_BrownWC.tar.gz.For relation extraction, we used the benchmarkACE 2004 training data.
Following most of theprevious research, we used in experiments thenwire (newswire) and bnews (broadcast news)genres of the data containing 348 documents and4374 relation instances.
We extracted an instancefor every pair of mentions in the same sentencewhich were separated by no more than two othermentions.
The non-relation instances generatedwere about 8 times more than the relation instances.Preprocessing of the ACE documents: We usedthe Stanford parser6 for syntactic and dependencyparsing.
We used chunklink7  to derive chunkinginformation from the Stanford parsing.
Becausesome bnews documents are in lower case, werecover the case for the head of a mention if itstype is NAM by making the first character into itsupper case.
This is for better matching between thewords in ACE and the words in the unsupervisedword clusters.We used the OpenNLP 8  maximum entropy(maxent) package as our machine learning tool.We choose to work with maxent because thetraining is fast and it has a good support for multi-class classification.6.1 Baseline PerformanceFollowing previous work, we did 5-fold cross-validation on the 348 documents with hand-annotated entity mentions.
Our results are shown inTable 5 which also lists the results of another threestate-of-the-art feature based systems.
For this andthe following experiments, all the results werecomputed at the relation mention level.System P(%) R(%) F(%)Zhou et al (2007)9 78.2 63.4 70.1Zhao and Grishman (2005)10 69.2 71.5 70.4Our Baseline 73.4 67.7 70.4Jiang and Zhai (2007) 11 72.4 70.2 71.3Table 5: Performance comparison on the ACE 2004data over the 7 relation types.6 http://nlp.stanford.edu/software/lex-parser.shtml7 http://ilk.uvt.nl/team/sabine/chunklink/README.html8 http://opennlp.sourceforge.net/9 Zhou et al (2005) tested their system on the ACE 2003 data;Zhou et al (2007) tested their system on the ACE 2004 data.10  The paper gives a recall value of 70.5, which is notconsistent with the given values of P and F. An examination ofthe correspondence in preparing this paper indicates that thecorrect recall value is 71.5.11 The result is from using the All features in Jiang and Zhai(2007).
It is not quite clear from the paper that whether theyused the 348 documents or the whole 2004 training data.526Note that although all the 4 systems did 5-foldcross-validation on the ACE 2004 data, thedetailed data partition might be different.
Also, wewere doing cross-validation at the document levelwhich we believe was more natural than theinstance level.
Nonetheless, we believe ourbaseline system has achieved very competitiveperformance.6.2 The Effectiveness of Cluster SelectionMethodsWe investigated the tradeoff between performanceand training time of each proposed method inselecting clusters.
In this experiment, we randomlyselected 70 documents from the 348 documents astest data which roughly equaled the size of 1 foldin the baseline in Section 6.1.
For the baseline inthis section, all the rest of the documents were usedas training data.
For the semi-supervised system,70 percent of the rest of the documents wererandomly selected as training data and 30 percentas development data.
The set of prefix lengths thatworked the best for the development set waschosen to select clusters.
We only used the clusterfeature HM_WC in this experiment.System F ?
Training  Time (in minute)Baseline 70.70  1UA 71.19 +0.49 1.5PC3 71.65 +0.95 30PC4 71.72 +1.02 46IG3 71.65 +0.95 45IG4 71.68 +0.98 78ES3 71.66 +0.96 465ES4 71.60 +0.90 1678Table 6: The tradeoff between performance and trainingtime of each method in selecting clusters.
PC3 meansusing 3 prefixes with the PC method.
?
in this papermeans the difference between a system and the baseline.Table 6 shows that all the 4 proposed methodsimproved baseline performance, with UA as thefastest and ES as the slowest.
It was interesting thatES did not always outperform the two statisticalmethods which might be because of its overfittingto the development set.
In general, both PC and IGhad good balances between performance andtraining time.
There was no dramatic difference inperformance between using 3 and 4 prefix lengths.For the rest of this paper, we will only use PC4as our method in selecting clusters.6.3 The Effectiveness of Cluster FeaturesThe baseline here is the same one used in Section6.1.
For the semi-supervised system, each test foldwas the same one used in the baseline and the other4 folds were further split into a training set and adevelopment set in a ratio of 7:3 for selectingclusters.
We first added the cluster featuresindividually into the baseline and then added themincrementally according to the order specified inTable 4.System F ?1 Baseline 70.42 1 + HM_WC 71.5 + 1.13 1 + BagWM_WC 71.0 + 0.64 1 + HC_WC 69.6 - 0.85 1 + BagWC_WC 46.1 - 24.36 2 + BagWM_WC 71.0 + 0.67 6 + HC_WC 70.6 + 0.28 7+ BagWC_WC 50.3 - 20.1Table 7: Performance 12  of the baseline and usingdifferent cluster features with PC4 over the 7 types.We found that adding clusters to the heads of thetwo mentions was the most effective way ofintroducing cluster features.
Adding clusters to thewords of the mentions can also help, though not asgood as the heads.
We were surprised that theheads of chunks in context did not help.
This mightbe because ACE relations are usually short and thelimited number of long relations is not sufficient ingeneralizing cluster features.
Adding clusters toevery word in context hurt the performance a lot.Because of the behavior of each individual feature,it was not surprising that adding themincrementally did not give more performance gain.For the rest of this paper, we will only useHM_WC as cluster features.6.4 The Impact of Training SizeWe studied the impact of training data size oncluster features as shown in Table 8.
The test datawas always the same as the 5-fold used in thebaseline in Section 6.1. no matter the size of thetraining data.
The training documents for the12  All the improvements of F in Table 7, 8 and 9 weresignificant at confidence levels >= 95%.527# docs F of Relation Classification F of Relation DetectionBaseline PC4 (?)
Prefix10(?)
Baseline PC4(?)
Prefix10(?
)50 62.9 63.8(+ 0.9) 63.7(+0.8) 71.4 71.9(+ 0.5) 71.6(+0.2)75 62.8 64.6(+ 1.8) 63.9(+1.1) 71.5 72.3(+ 0.8) 72.5(+1.0)125 66.1 68.1(+ 2.0) 67.5(+1.4) 74.5 74.8(+ 0.3) 74.3(-0.2)175 67.8 69.7(+ 1.9) 69.5(+1.7) 75.2 75.5(+ 0.3) 75.2(0.0)225 68.9 70.1(+ 1.2) 69.6(+0.7) 75.6 75.9(+ 0.3) 75.3(-0.3)?280 70.4 71.5(+ 1.1) 70.7(+0.3) 76.4 76.9(+ 0.5) 76.3(-0.1)Table 8: Performance over the 7 relation types with different sizes of training data.
Prefix10 uses the single prefixlength 10 to generate word clusters as used by Chan and Roth (2010).Type P R FBaseline PC4 (?)
Baseline PC4 (?)
Baseline PC4 (?
)EMP-ORG 75.4 77.2(+1.8) 79.8 81.5(+1.7) 77.6 79.3(+1.7)PHYS 73.2 71.2(-2.0) 61.6 60.2(-1.4) 66.9 65.3(-1.7)GPE-AFF 67.1 69.0(+1.9) 60.0 63.2(+3.2) 63.3 65.9(+2.6)PER-SOC 88.2 83.9(-4.3) 58.4 61.0(+2.6) 70.3 70.7(+0.4)DISC 79.4 80.6(+1.2) 42.9 46.0(+3.2) 55.7 58.6(+2.9)ART 87.9 96.9(+9.0) 63.0 67.4(+4.4) 73.4 79.3(+5.9)OTHER-AFF 70.6 80.0(+9.4) 41.4 41.4(0.0) 52.2 54.6(+2.4)Table 9: Performance of each individual relation type based on 5-fold cross-validation.current size setup were randomly selected andadded to the previous size setup (if applicable).
Forexample, we randomly selected another 25documents and added them to the previous 50documents to get 75 documents.
We made surethat every document participated in this experiment.The training documents for each size setup weresplit into a real training set and a development setin a ratio of 7:3 for selecting clusters.There are some clear trends in Table 8.
Undereach training size, PC4 consistently outperformedthe baseline and the system Prefix10 for relationclassification.
For PC4, the gain for classificationwas more pronounced than detection.
The mixeddetection results of Prefix10 indicated that onlyusing a single prefix may not be stable.We did not observe the same trend in thereduction of annotation need with cluster-basedfeatures as in Koo et al (2008) for dependencyparsing.
PC4 with sizes 50, 125, 175 outperformedthe baseline with sizes 75, 175, 225 respectively.But this was not the case when PC4 was testedwith sizes 75 and 225.
This might due to thecomplexity of the relation extraction task.6.5 AnalysisThere were on average 69 cross-type errors in thebaseline in Section 6.1 which were reduced to 56by using PC4.
Table 9 showed that most of theimprovements involved EMP-ORG, GPE-AFF,DISC, ART and OTHER-AFF.
The performancegain for PER-SOC was not as pronounced as theother five types.
The five types of relations areambiguous as they share the same entity type GPEwhile the PER-SOC relation only holds betweenPER and PER.
This reflects that word clusters canhelp to distinguish between ambiguous relationtypes.As mentioned earlier the gain of relationdetection was not as pronounced as classificationas shown in Table 8.
The unbalanced distributionof relation instances and non-relation instancesremains as an obstacle for pushing the performanceof relation extraction to the next level.7 Conclusion and Future WorkWe have described a semi-supervised relationextraction system with large-scale word clustering.We have systematically explored the effectivenessof different cluster-based features.
We have alsodemonstrated that the two proposed statisticalmethods are both effective and efficient inselecting clusters at an appropriate level ofgranularity through an extensive experimentalstudy.528Based on the experimental results, we plan toinvestigate additional ways to improve theperformance of relation detection.
Moreover,extending word clustering to phrase clustering (Linand Wu, 2009) and pattern clustering (Sun andGrishman, 2010) is worth future investigation forrelation extraction.ReferencesRie K. Ando and Tong Zhang.
2005 A Framework forLearning Predictive Structures from Multiple Tasksand Unlabeled Data.
Journal of Machine LearningResearch, Vol 6:1817-1853.Elizabeth Boschee, Ralph Weischedel, and AlexZamanian.
2005.
Automatic information extraction.In Proceedings of the International Conference onIntelligence Analysis.Peter F. Brown, Vincent J. Della Pietra, Peter V.deSouza,  Jenifer C. Lai, and Robert L. Mercer.
1992.Class-based n-gram models of natural language.Computational Linguistics, 18(4):467?479.Razvan C. Bunescu and Raymond J. Mooney.
2005a.
Ashortest path dependency kenrel for relationextraction.
In Proceedings of HLT/EMNLP.Razvan C. Bunescu and Raymond J. Mooney.
2005b.Subsequence kernels for relation extraction.
InProceedings of NIPS.Yee Seng Chan and Dan Roth.
2010.
Exploitingbackground knowledge for relation extraction.
InProc.
of COLING.Ralph Grishman, David Westbrook and Adam Meyers.2005.
NYU?s English ACE 2005 System Description.ACE 2005 Evaluation Workshop.Earl B.
Hunt, Philip J.
Stone and Janet Marin.
1966.Experiments in Induction.
New York: AcademicPress, 1966.Jing Jiang and ChengXiang Zhai.
2007.
A systematicexploration of the feature space for relationextraction.
In Proceedings of HLT-NAACL-07.Nanda Kambhatla.
2004.
Combining lexical, syntactic,and semantic features with maximum entropy modelsfor information extraction.
In Proceedings of ACL-04.Terry Koo, Xavier Carreras, and Michael Collins.
2008.Simple Semi-supervised Dependency Parsing.
InProceedings of ACL-08: HLT.Percy Liang.
2005.
Semi-Supervised Learning forNatural Language.
Master?s thesis, MassachusettsInstitute of Technology.Dekang Lin and Xiaoyun Wu.
2009.
Phrase Clusteringfor Discriminative Learning.
In Proc.
of ACL-09.Scott Miller, Heidi Fox, Lance Ramshaw, and RalphWeischedel.
2000.
A novel use of statistical parsingto extract information from text.
In Proc.
of NAACL.Scott Miller, Jethran Guinness and Alex Zamanian.2004.
Name Tagging with Word Clusters andDiscriminative Training.
In Proc.
of HLT-NAACL.Longhua Qian, Guodong Zhou, Qiaoming Zhu andPeide Qian.
2008.
Exploiting constituentdependencies for tree kernel-based semantic relationextraction .
In Proc.
of COLING.John Ross Quinlan.
1986.
Induction of decision trees.Machine Learning, 1(1), 81-106.Lev Ratinov and Dan Roth.
2009.
Design challengesand misconceptions in named entity recognition.
InProceedings of CoNLL-09.Ang Sun.
2009.
A Two-stage Bootstrapping Algorithmfor Relation Extraction.
In RANLP-09.Ang Sun and Ralph Grishman.
2010.
Semi-supervisedSemantic Pattern Discovery with Guidance fromUnsupervised Pattern Clusters.
In Proc.
of COLING.Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.2010.
Word representations: A simple and generalmethod for semi-supervised learning.
In Proceedingsof ACL.Dmitry Zelenko, Chinatsu Aone, and AnthonyRichardella.
2003.
Kernel methods for relationextraction.
Journal of Machine Learning Research,3:1083?1106.Zhu Zhang.
2004.
Weakly supervised relationclassification for information extraction.
In Proc.
ofCIKM?2004.Min Zhang, Jie Zhang, Jian Su, and GuoDong Zhou.2006.
A composite kernel to extract relationsbetween entities with both flat and structured features.In Proceedings of COLING-ACL-06.Shubin Zhao and Ralph Grishman.
2005.
Extractingrelations with integrated information using kernelmethods.
In Proceedings of ACL.Guodong Zhou, Jian Su, Jie Zhang, and Min Zhang.2005.
Exploring various knowledge in relationextraction.
In Proceedings of ACL-05.Guodong Zhou, Min Zhang, DongHong Ji, andQiaoMing Zhu.
2007.
Tree kernel-based relationextraction with context-sensitive structured parse treeinformation.
In Proceedings of EMNLPCoNLL-07.529
