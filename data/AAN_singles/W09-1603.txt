Proceedings of CLIAWS3, Third International Cross Lingual Information Access Workshop, pages 12?20,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsTransliteration based Search Engine for Multilingual Information AccessAnand Arokia RajSpeech and Language Technology LabBhrigus Software (I) Pvt LtdHyderabad, Indiarayar.anand@bhrigus.comHarikrishna MagantiSpeech and Language Technology LabBhrigus Software (I) Pvt LtdHyderabad, Indiahmaganti@bhrigus.comAbstractMost of the Internet data for Indian languagesexist in various encodings, causing difficul-ties in searching for the information throughsearch engines.
In the Indian scenario, ma-jority web pages are not searchable or the in-tended information is not efficiently retrievedby the search engines due to the following:(1) Multiple text-encodings are used whileauthoring websites.
(2) Inspite of Indianlanguages sharing common phonetic nature,common words like loan words (borrowedfrom other languages like Sanskrit, Urdu orEnglish), transliterated terms, pronouns etc.,can not be searched across languages.
(3) Fi-nally the query input mechanism is anothermajor problem.
Most of the users hardly knowhow to type in their native language and pre-fer to access the information through Englishbased transliteration.
This paper addresses allthese problems and presents a transliterationbased search engine (inSearch) which is ca-pable of searching 10 multi-script and multi-encoded Indian languages content on the web.1 IntroductionIndia is a multi-language and multi-script coun-try with 23 official languages and 11 written scriptforms.
About a billion people in India use these lan-guages as their first language.
About 5% of the pop-ulation (usually the educated class) can understandEnglish as their second language.
Hindi is spokenby about 30% (G. E. Burkhart, S. E. Goodman, A.Mehta and L. Press, 1998) of the population, but it isconcentrated in urban areas and north-central India,and is still not only foreign, but often unpopular inmany other regions.Though considerable amount of Indic content isavailable on the World Wide Web (WWW), we canobserve that search development is very less whencompared to the official languages of the United Na-tions (UN).
The primary reason for this can be at-tributed for much delayed standards and lack of sup-port from operating systems and browsers in ren-dering Indic scripts.
This caused web publishers todevelop their own proprietory encodings/fonts, whoare now hesitant to use available standards such asUnicode/ISCII.
This creates a major hinderance inaccessing Indian content through existing search en-gines.Most of the search engines support Indic searchin Unicode data only.
But, considerable amountof content is available in ASCII based font encod-ings which is much larger (more dynamic also) thanUnicode (Unicode Consortium - Universal CodeStandard, 1991) or ISCII (ISCII - Indian Stan-dard Code for Information Interchange, 1983) for-mats.
Apart from this, language independent infor-mation like loan words, transliterated words, pro-nouns etc., are also not accessible across Indian lan-guages.
Most users are familiar with English key-board typing than any Indian language, and wouldbe interested to query through English translitera-tion.
So, a meta standard transliteration scheme(IT3 sec3.1) has to be commonly defined across allthe Indian languages, and the web content has tobe appropriately converted.
Also, the web pagesneed to be indexed using phonetic features like(diphone/triphones/syllables), which will be conve-12nient to retrieve and rank the pages.
In this paper,we incorporate all these aspects to make search en-gine as the meaningful searching tool for Indian lan-guages.The paper is organized into six sections.
Thefirst section explains the nature of Indic scripts.The second section details the various major en-coding formats and transliteration scheme used tostore and render Indic data.
In section three, novelapproaches for preprocessing Indic data like font-encoding identification and font-data conversion areexplained.
In section four, the experiments regard-ing stemming and grapheme-to-phoneme (G2P) forIndian-English using Classification and RegressionTree (CART) are described and stop words identifi-cation is also explained.
The fifth section discussesthe issues in developing a multi-lingual search en-gine for Indian languages.
The sixth section explainsthe three possible ways to devlope a cross-lingualsearch engine.
Finally the report and summary areincluded with conclusion.2 Nature of Indic ScriptsThe scripts in Indian languages have originated fromthe ancient Brahmi script.
The basic units of thewriting system are referred to as Aksharas.
Theproperties of Aksharas are as follows: (1) An Ak-shara is an orthographic representation of a speechsound (2) Aksharas are syllabic in nature (3) Thetypical forms of Akshara are V, CV, CCV andCCCV, thus have a generalized form of C*V. Theshape of an Akshara depends on its composition ofconsonants and the vowel, and sequence of the con-sonants.
In defining the shape of an Akshara, oneof the consonant symbols acts as pivotal symbol (re-ferred to as semi-full form).
Depending on the con-text, an Akshara can have a complex shape withother consonant and vowel symbols being placed ontop, below, before, after or sometimes surroundingthe pivotal symbol (referred to as half-form).Thus to render an Akshara electronically, a set ofsemi-full or half-forms have to be rendered, whichare in turn rendered using a set of basic shapesreferred to as glyphs.
Often a semi-full form orhalf-form is rendered using two or more glyphs,thus there is no one-to-one correspondence betweenglyphs of a font and semi-full or half-forms.2.1 Convergence and DivergenceAll Indian languages except English and Urdu sharea common phonetic base, i.e., they share a com-mon set of speech sounds.
While all of these lan-guages share a common phonetic base, some of thelanguages such as Hindi, Marathi and Nepali alsoshare a common script known as Devanagari.
Butlanguages such as Telugu, Kannada and Tamil havetheir own scripts.
The property which distinguishesthese languages can be attributed to the phonotacticsin each of these languages rather than the scripts andspeech sounds.
Phonotactics is the permissible com-bination of phones that can co-occur in a language.This knowledge helps us in designing a commontransliteration scheme, and also in identifying andconverting different text encodings.3 Indic Data FormatsAnother aspect involved in the diversity of electroniccontent of Indian languages is their format of digi-tal storage.
Storage formats like ASCII (AmericanStandard Code for Information Interchange) basedfonts, ISCII (Indian Standard code for InformationInterchange), Unicode and phonetic based translit-eration schemes are often used to store the digitaltext data in Indian languages.
Most of the text isrendered using some fonts of these formats.3.1 Phonetic Transliteration SchemesTransliteration is a mapping from one system ofwriting into another, word by word, or ideally let-ter by letter.
It is the practice of transcribing a wordor text written in one writing system into anotherwriting system.
Transliterations in the narrow senseare used in situations where the original script isnot available to write down a word in that script,while still high precision is required.
One instanceof transliteration is the use of an English computerkeyboard to type in a language that uses a differentalphabet, such as Russian, Hindi etc.
Transliteratedtexts are often used in emails, blogs, and electroniccorrespondence where non-Latin keyboards are un-available, is sometimes referred to by special com-posite terms that demonstrate the combination ofEnglish characters and the original non-Latin wordpronunciation: Ruglish, Hebrish, Greeklish, Ara-bish or Hinlish.13To handle diversified storage formats of scripts ofIndian languages such as ASCII based fonts, ISCIIand Unicode etc., it is useful and becomes essen-tial to use a meta-storage format.
ISO 15919 stan-dards (Transliteration of Indic Scripts: How to useISO 15919, ) describes development of translitera-tion for Indic scripts.
A transliteration scheme mapsthe Aksharas of Indian languages onto English al-phabets and it could serve as meta-storage formatfor text-data.
Since Aksharas in Indian languagesare orthographic representation of speech sound, andthey have a common phonetic base, it is suggested tohave a phonetic transliteration scheme such as IT3(Ganapathiraju M., Balakrishnan M., BalakrishnanN.
and Reddy R., 2005) (Prahallad Lavanya, Pra-hallad Kishore and GanapathiRaju Madhavi, 2005).Thus, when the font-data is converted into IT3, it es-sentially turns the whole effort into font-to-Aksharaconversion.
Thus IT3 transliteration is used as com-mon representation scheme for all Indic data for-mats.
The same is used to get the input from theuser also.4 Indic Data PreprocessingIn search engine development, it is an absolute re-quirement that the content should be in an uniqueformat to build a efficient index table.
So, prepro-cessing the web content is unavoidable here.
Mostof the Indian language electronic data is either Uni-code encoded or glyph based font encoded.
Process-ing Unicode data is quite straight forward because itfollows distinguished code ranges for each languageand there is a one-to-one correspondence betweenglyphs (shapes) and characters.
But this is not true inthe case of glyph based font encoded data.
Hence, itbecomes necessary to identify the font encoding andconvert the font-data into a phonetic transliterationsheme like IT3.
The following subsections explainthe stages in detail.4.1 Font-Encoding IdentificationThe problem of font-identification could be definedas, given a set of words or sentences to identify thefont-encoding by finding the minimum distance be-tween the input glyph codes and the models repre-senting font-encodings.
Existing works (Anil Ku-mar Singh and Jagadeesh Gorla, 2007) addressed theTable 1: Font-Type Identification for Words.Font Name Uniglyph Biglyph TriglyphAmarujala (Hindi) 100% 100% 100%Jagran (Hindi) 100% 100% 100%Webdunia (Hindi) 0.1% 100% 100%Shree-Tel (Telugu) 7.3% 100% 100%Eenadu (Telugu) 0.2% 100% 100%Vaarttha (Telugu) 29.1% 100% 100%E-Panchali (Tamil) 93% 100% 100%Amudham (Tamil) 100% 100% 100%Shree-Tam (Tamil) 3.7% 100% 100%English-Text 0% 96.3% 100%same problem but with limited success.In this context, the proposed approach (A.
A. Rajand K. Prahallad, 2007) use vector space modeland Term Frequency - Inverse Document Frequency(TF-IDF) for font-encoding identification.
This ap-proach is used to weigh each term in the font-dataaccording to its uniqueness.
Thus it captures therelevancy among term and document.
Here, Term:refers to a unit of glyph.
In this work, experimentsare performed with different units such as singleglyph gi (uniglyph), two consecutive glyphs gi?1gi(biglyph) and three consecutive glyphs gi?1gigi+1(triglyph).
Document: It refers to the font-data(words and sentences) in a specific font-encoding.To build a model for each font-encoding scheme,we need sufficient data.
So we have collected man-ually an average of 0.12 million unique words pertype for nearly 37 different glyph based fonts.
Tocreate a vector space model for a font-encoding, pri-marily the term (uniglyph or biglyph or triglyph) isextracted out of the font-data.
Then TF-IDF weightsare calculated for all terms in the documents.Identification Results: The steps involved are asfollows.
Firstly, terms from the input word or sen-tence are extracted.
Then a query vector using thoseterms is created.
The distance between query vec-tor and all the models of font-encoding is computedusing TF-IDF weights.
The input word is said tobe originated from the model which gives a max-imum TF-IDF value.
It is typically observed thatTF-IDF weights are more sensitive to the length ofquery.
The accuracy increases with the increase inthe length of test data.
Thus, two different types14Table 2: Font-Type Identification for Sentences.Font Name Uniglyph Biglyph TriglyphAmarujala (Hindi) 100% 100% 100%Jagran (Hindi) 100% 100% 100%Webdunia (Hindi) 100% 100% 100%Shree-Tel (Telugu) 100% 100% 100%Eenadu (Telugu) 0% 100% 100%Vaarttha (Telugu) 100% 100% 100%E-Panchali (Tamil) 100% 100% 100%Amudham (Tamil) 100% 100% 100%Shree-Tam (Tamil) 100% 100% 100%English-Text 0% 100% 100%of test data were prepared for testing.
One is a setof unique words and the other is a set of sentences.It should also be noted that the accuracy dependson various factors: a) The number of font-encodingsfrom which the identifier has to select one b) The in-herent confusion of one font-encoding with anotherand c) The type of unit used in modeling.
For 1000different number of inputs (words and sentences) wehave identified the closest models and calculated theaccuracy.
It is repeatedly done for various (uniglyph,biglyph and triglyph) categories.
From Tables 1 and2, it is clear that triglyph seems to be an appropriateunit for a term in the identification of font-encoding.It can also be seen that the performance at word andsentence level is 100% with triglyph.4.2 Font-Data ConversionThe problem of font-data conversion could be de-fined as a module whose input is sequence of glyphcodes and whose output is a sequence of Aksharas(characters) of Indian languages.Existing methods and solutions proposed by (Hi-manshu Garg, 2005) (Khudanpur S. and Schafer C.,Devanagari Converters, 2003) lack in, a) Framinga generic methodology or algorithm for conversionof font-data of all Indian languages b) Since glyphcodes are manually constructed, 100% accurate con-version is achievable c) Existing methods requireslarge amount of effort for each font-encoding d)Large number of rules have to be written for rulebased system e) Large parallel corpora has to be pre-pared for training f) They don?t exploit shape andpositional information of the glyphs, thus reducingaccuracy in conversion process.Exploiting Position and Shape Information: (A.A. Raj and K. Prahallad, 2007) Characters in Indianlanguages change their shape where they appear(top, bottom, left, right) in the script.
In this work, anunambiguous glyph code mapping is done by intro-ducing a simple alphanumeric scheme where the al-phabets denote the corresponding phoneme and thenumber denotes the glyph position.
We followedIT3 phonetic notations and the position numbers asdescribed below.
Glyphs which could be in a) piv-otal (center) position are referred by code 0/1.
b)left position of pivotal symbol are referred by code2.
c) right position of pivotal symbol are referred bycode 3. d) top position of pivotal symbol are referredby code 4. e) bottom position of pivotal symbol arereferred by code 5.Training: First in the training, a font-encoding fora language is selected and a glyph-map table is pre-pared by hand-coding the relation between glyphs(suffixed with their position and shape information)and IT3 notations.
In the second stage, a simpleset of glyph assimilation rules are defined (Multi-Lingual Screen Reader and Processing of Font-datain Indian Languages, ).
We iterated through the fol-lowing steps until there are minimal errors on held-out test set of words.
Results are checked for errorsusing human evaluation.
If errors are found then therules are updated or redefined.
The above process isrepeated for 3 different font-encodings of differentfont-families of the chosen language.Evaluation: While testing, a new font from thesame language is selected and a glyph-mapping ta-ble is prepared.
It has to be noted that for new font,we don?t update or add any glyph assimilation rules,and thus we use the existing rules obtained duringtraining phase.
A random set of 500 words fromthat font-data is picked-up.
The conversion accuracyis evaluated using human evaluation.
We have builtconverters for 10 Indian languages and 37 differentfont-encodings.
The evaluations results in Table 3indicate that the font-data conversion performs con-sistently above 99% for a new font-encoding acrosslanguages except for Telugu.
Thus in our approachthe effort of building rules is limited to three differ-ent fonts of a language to build the converter.
To adda new font, only glyph-map table is required and nomore repetition of rule building process.15In Table 3, we can observe inferior performancefor Telugu.
It is due to the number of glyphs andtheir possible combinations are higher than otherlanguages.
Also it is common for all Indian lan-guages that the pivotal character glyph comes firstand other supporting glyphs come next in the script.But in Telugu the supporting glyphs may come be-fore the pivotal glyph which creates ambiguity informing assimilation rules.5 Experiments and DiscussionIn this section, the experiments performed to buildthe tools/modules are explained.
Most of them usedthe CART tool to train and test.
These modules/toolsare integrated and used for development of the pro-posed search engine.5.1 CART (Classification and Regression Tree)CART is a decision tree procedure introduced byBreiman et al, in 1984.
CART uses an exhaus-tive, recursive partitioning routine to generate binarysplits that divide each parent node into two childnodes by posing a series of yes-no questions.
CARTsearches for questions that split nodes into relativelyhomogenous child nodes.
As the tree evolves, thenodes become increasingly more homogenous, iden-tifying segments.
The basic CART building algo-rithm is a greedy algorithm which chooses the lo-cally best discriminatory feature at each stage in theprocess.Stop Parameter: The stop parameter specifies theminimum number of samples necessary in the train-ing set before a question is hypothesized to distin-guish the group.
Normally with smaller stop valuethe model may become over-trained.
The optionalstop value may differ for different datasets of differ-ent languages.Predictee: In a given feature set, the feature thatis to be predicted as the output is termed as the pre-dictee.
By default, the first feature in the feature-setis taken as the predictee, but always the predicteecan be specified while giving the description of thedata.
Some times CART is over-fit with trainingdata, which may reduce the performance.Feature Selection: Many experiments were con-ducted for different problems like grapheme tophoneme conversion (G2P) for English (Indian-Table 3: Font-Data Conversion Results (Precision Val-ues).Language Font Name Training / ResultTestingHindi Amarujala Training 99.2%Jagran Training 99.4%Naidunia Training 99.8%Webdunia Training 99.4%Chanakya Testing 99.8%Marathi ShreePudhari Training 100%ShreeDev Training 99.8%TTYogesh Training 99.6%Shusha Testing 99.6%Telugu Eenadu Training 93%Vaarttha Training 92%Hemalatha Training 93%TeluguFont Testing 94%Tamil ElangoValluvan Training 100%ShreeTam Training 99.6%ElangoPanchali Training 99.8%Tboomis Testing 100%Kannada Shree Kan Training 99.8%TTNandi Training 99.4%BRH Kannada Training 99.6%BRH Vijay Testing 99.6%Malayalam Revathi Training 100%Karthika Training 99.4%Thoolika Training 99.8%Shree Mal Testing 99.6%Gujarati Krishna Training 99.6%Krishnaweb Training 99.4%Gopika Training 99.2%Divaya Testing 99.4%Punjabi DrChatrikWeb Training 99.8%Satluj Training 100%Bengali ShreeBan Training 97.5%hPrPfPO1 Training 98%Aajkaal Training 96.5%Oriya Dharitri Training 95%Sambad Training 97%AkrutiOri2 Training 96%16English) and stemming.
These experiments wereconducted with different possible features and stopvalues.
Features for English G2P conversion weremanually prepared for each letter and for stemming,the roots were manually identified for each word.The features vary from experiment to experimentand consequently the dimension of the features alsovary.Evaluation: For each experiment, we have con-sidered ?N?
number of words per language and wehave generated ?M?
number of features out of them.From the available features we have segregated ?X?number of features for training and ?Y?
number offeatures for testing in 80:20 ratio.
Using these sets,we have evaluated the training and testing perfor-mance for various stop values.5.2 StemmingStemming is the use of linguistic analysis to getto the root form of a word.
Search engines thatuse stemming compare the root forms of the searchterms to the documents in its database.
For example,if the user enters ?viewer?
as the query, the searchengine reduces the word to its root (?view?)
and re-turns all documents containing the root - like doc-uments containing view, viewer, viewing, preview,review etc.
Since our training data is very small itfails for out-of-vocabulary words.
And also, it failsfor homographs (a homograph is one of a group ofwords that share the same spelling but have differentmeanings).For stemming in Indian languages, inflections ofthe words are formed mostly by suffixes than pre-fixes.
So considering the first 5 phones of a wordwould help to predict the root of the word.
But, forEnglish prefixes as well as suffixes are equally usedto form inflections.
So prefixes are separated andconsidered as a single unit like a phone here.
So wehave selected the features like?
First 6 phones for English and?
First 5 phones for Indian languagesStemming results for various languages are shownin Table 4.
It shows that sto-value 1 would be op-timal, when we used training and testing features inthe ratio 907:227 for English, 3606:902 for Tamiland 987:247 for Telugu.Table 4: Stemming Performance.Language Stop Value Training TestingEnglish 1 100% 99.55%2 98.78% 96.01%3 94.59% 90.26%4 86.64% 82.3%Tamil 1 93.25% 77.69%2 84.74% 75.24%3 80.63% 74.49%4 77.08% 73.47%Telugu 1 100% 93%2 100% 92%3 100% 93%4 100%g 94%5.3 English G2P ConversionOur search uses phonetic features like syllables.
Incross-linguagl search support for English input isneccessary.
So we need a mechanism to convert thequery from its grapheme form to phoneme form.
Itis very challenging since English words doesn?t fol-low one-to-one correspondence between its lettersand its phonemes.For G2P conversion of English words, the lettersof the word are used as features.
We hypothesizethat the first and last letters of the word and previ-ous and next letters of the current letter help muchto predict its phoneme.
So we have selected the fea-tures like?
First and Last letters of the word and Previousand Next letters of the Current letterThe G2P conversion results for Indian-English isshown in Table 5.
It shows that stop-value 1 wouldbe optimal for a training feature set of 106896 andtesting feature set of 26724.5.4 Stop Words IdentificationStop words, is the name given to the words whichare filtered out prior to, or after processing of nat-ural language data (text).
There is no definite listof stop words which all natural language process-ing tools incorporate.
Some search engines don?tindex/record extremely common words in order tosave space or to speed up searches.
The list of stop17Table 5: English G2P Conversion Performance.Stop Value Training Testing1 95.89% 85.56%2 92.15% 85.37%3 90.79% 85.56%4 89.73% 85.53%words for Indian languages have not been identifiedyet.
So, we tried to generate the list by the basic ideathat the most common words of a language mighthave occurred more frequently than other words inthe corpus.
We generated a list of top 500 frequentlyoccurred words in a language.
Then stop words listwas produced with the help of a linguist who manu-ally cleaned it.6 inSearch - Search Engine for IndianLanguagesMost information seekers use a search engine tobegin their web activity (Prasad Pingali, JagadeeshJalagarlamudi and Vasudeva Varma, 2006).
In thiscase, users submit a query (typically a list of key-words) and receive a list of web pages that may berelevant.
In conventional information retrieval sys-tems (Google, Live, Yahoo, Guruji etc.)
the usermust enter a search query in the language/encodingof the documents in order to retrieve it.
This re-striction clearly limits the amount of information towhich an user will have access.Developing a search engine for Indian languagesfaces many challenges.
Some of them are, identify-ing the text-encoding of the web content, convertingthem into a transliteration scheme, developing stem-mers and identifying the stop words etc.
Also oneneed to design a good mechanism/tool (A. Joshi, A.Ganu, A. Chand, V. Parmar and G. Mathur, 2004)to accept user?s query in transliteration scheme orstandard encoding like UTF-8 and even in Englishalso.
inSearch is a search engine for Indian lan-guages developed by considering all the above dis-cussed issues and solutions.
Fig 1 shows the basicarchitecture and the following sub-sections explainthem further.6.0.1 Web CrawlingOur web crawling is language focused.
It takes alist of identified URLs per language for which wehave converters.
Then it crawls those pages andstores the documents locally for further processing.It maintains the same directory structure as on theweb and ordered by date.6.0.2 IndexingThe effectiveness of any search engine mainly de-pends on its index structure.
The structure should becapable of catering sufficient and relevant informa-tion to the users in terms of their needs.
To serveusers with the contexual information in their ownlanguage, the system needs to index on meaning rep-resentation and not on plain text.
Also, the size ofthe index should not be too large.Conventional search engines use stemming tech-nology to get the root of the word and index the doc-ument about it.
Thus, it will search not only for thesearch terms, but also for its inflexions and similar tosome or all of those terms.
But in case of Indian lan-guages, there is no effective algorithm or tool to dostemming.
So we used phonetic features like sylla-bles to index the pages.
We extract the first two syl-lables (since they are almost equal to the root of theword most of the times) of the word and index aboutit.
Since, we have identified a method for stemming,we used them also for indexing.
The detailed exper-iments are provided in the above section 5.2.
Ourindex structure includes syllables, stem, word, term-frequency, language, date and doc-id.
This struc-ture enables efficient multi-lingual and cross-lingualsearch.6.0.3 RetrievalAt first, begining two syllables of the words ofthe query are extracted.
Then the words beginingwith those syllables are retrieved from the database.Hence the common words across languages are cap-tured here.
These words are ranked according totheir phonetic relativeness to the query calculatedby DTW method.
The words fall under thresholdare discarded, so that the documents containing themost related words pop-up.
Then the documents arere-ranked about their term frequency (TF) values (G.Salton and C. Buckley, 1988) and contextual infor-mation.186.0.4 User InterfacePresently, there is no standard/convenient nota-tion or keyboard layout to input the query in Indianlanguages.
Even with UTF-8 encoding most of theusers don?t know how to type the query.
So, forcross-lingual search we provide a phonetic mappingtable to be refered by the user to type the query inIT3 notation.
But for language specific search, weprovide a query typing tool.
This tool has buttonsfor characters of the selected language.
By click-ing the buttons, user can type the query in his nativescript since most of the queries won?t be more thana word/phrase.
After forming the query, user cansearch the web and the ranked results are displayedmuch like the standard search engine?s results.
Herethe cached pages for even font encoded pages aredisplayed in UTF-8 encoding.Figure 1: Search Engine Architecture.7 Cross-Lingual SearchThe development of digital and online informationrepositories has created many opportunities and newproblems in information retrieval.
Online docu-ments are available Internationally in many differentlanguages.
This makes it possible for users to di-rectly access previously unimagined sources of in-formation.
However in conventional information re-trieval systems, the user must enter a search query inthe language of the documents in order to retrieve it.This restriction clearly limits the amount and typeof information which an individual user really hasaccess to.
Cross Language Information Retrieval(CLIR) (F. Gey, N. Kando and C. Peters, 2002) (L.S.
Larkey, M. S. Connell and N. Abduljaleel, 2003)enables users to enter queries in languages they arefluent in, and uses language translation methods toretrieve documents originally written in other lan-guages.The aim of this attempt is to extend the search ca-pability to search across all Indian languages.
Theusers are ordinary Indians who master one of the In-dian languages, but have only passive knowledge inthe other neighbour languages.
This means that theycan read a text but not search for it since they do nothave active knowledge of how the different conceptsin the other languages are written or spelled.
Thiswill also strengthen the use of the Indian languageson the Internet and further avoid unnecessary use ofthe English language.
We are trying to achieve itstep-by-step by using the below mentioned methods.1.Phonetic Relativeness Measure: In this ap-proach the phonetic distance (how many inser-tions/substitutions/deletions occured) between thequery words and the available words is calculated.Then the closest words are considered as query re-lated words and the results are produced for thosewords.
There are many methods to calculate thephonetic distance and we used DTW (DynamicTime Warping) method to calculate the phonetic dis-tance for our experiments.
We used equal weightage(i.e 1) for insertion, substitution and deletion here.2.Dictionary Lookup: Here bilingual/multilingualdictionaries are used to get the translation of the key-words.
Creating such dictionaries for all the wordsof a language is time consuming process.
Instead,creating dictionaries for the stems of the words alonewill reduce the effort.
Unfortunately there are nosuch dictionaries available or methods to create thestems for all Indian languages.
So we developedCART based decision trees to produce the stems.
Wehave created such stem based bilingual dictionariesfor 5 Indian languages.
Also, we have created a mul-tilingual dictionary (Table 6) for 8 Indian languagesby keeping English words as keys.3.Machine Translation: This is considered as anappropriate solution for cross-language search (Dr.Pushpak Bhattacharyya, 2006).
The query in sourcelanguage gets translated into the destination lan-guage and the results will be produced for it.
In thiscontext, there is a close synergy between the fields of19Table 6: Multi-lingual Dictionary.Language WordsBengali 2028Gujarati 6141Hindi 22212Kannada 22695Malayalam 23338Oriya 7287Tamil 5521Telugu 8148English 43185Cross Language Information Retrieval (CLIR) andMachine Translation (MT).
But such systems for In-dian languages are under development.
We are alsofocussing our effort in the same direction to use itwith our engine in the future.8 ConclusionIn this paper we discussed the importance of be-ing able to search the Indian language web contentand presented a multi-lingual web search engine in-Search capable of searching 10 Indian languages.The nature of Indic scripts, Indic data storage for-mats and how to preprocess them efficiently are de-tailed.
It explained about how language identifica-tion, grapheme to phoneme conversion for Englishand stemming can be achieved using CART.
Thisshows that transcoding of proprietary encodings intoa meta standard transliteration scheme makes Indianlanguage web content accessible through search en-gines.9 AcknowledgmentsWe like to thank Speech and Language Technolo-gies Lab, Bhrigus (India) Pvt Ltd, Hyderabad, In-dia and our collegues Ms.Bhuvaneshwari, Mr.Prasadand others for all their support and encouragement.ReferencesA.
Joshi, A. Ganu, A. Chand, V. Parmar and G. Mathur.2004.
Keylekh: a keyboard for text entry in indicscripts.
CHI ?04 Extended Abstract on Human Fac-tors in Computing Systems, ACM Press.A.
A. Raj and K. Prahallad.
2007.
Identification andconversion of font-data in indian languages.
In In In-ternational Conference on Universal Digital Library(ICUDL), Pittsburgh, USA.A.
K. Singh and J. Gorla.
2007.
Identification of lan-guages and encodings in a multilingual document.
InProceedings of the 3rd ACL SIGWAC Workshop onWeb As Corpus, Louvain-la-Neuve, Belgium.Dr.
P. Bhattacharyya.
2006.
White paper on cross lingualsearch and machine translation.
Proposal to Govern-ment of India.F.
Gey, N. Kando and C. Peters.
2002.
Cross languageinformation retrieval: A research roadmap.
SIGIR Fo-rum, 36(2):72?80.G.
E. Burkhart, S. E. Goodman, A. Mehta and L. Press.1998.
The internet in india: Better times ahead?
Com-mun.
ACM, 41(11):21?26.G.
Salton and C. Buckley.
1988.
Term-weighting ap-proaches in automatic text retrieval.
Information Pro-cess.
Management, 24(5):513?523.M.
Ganapathiraju , M. Balakrishnan , N. Balakrishnanand R. Reddy 2005.
Om: One tool for many (in-dian) languages.
Journal of Zhejiang University Sci-ence, 6A(11):1348?1353.H.
Garg.
2005.
Overcoming the font and script barriersamong indian languages.
MS Thesis at InternationalInstitute of Information Technology Hyderabad, India.ISCII - Indian Standard Code for Information Inter-change.
1983. http://tdil.mit.gov.in/standards.htm.S.Khudanpur and C.Schafer , Devanagari Converters.2003.
http://www.cs.jhu.edu/cschafer/jhu devanagaricvt ver2.tar.gz.L.
S. Larkey, M. S. Connell and N. Abduljaleel.
2003.Hindi clir in thirty days.
ACM Trans.
on Asian Lan-guage Information Processing (TALIP), 2(2):130?142.P.
Lavanya, P. Kishore and G. R. Madhavi.
2005.
Asimple approach for building transliteration editors forindian languages.
Journal of Zhejiang University Sci-ence, 6A(11):1354?1361.P.
Prasad , J. Jagadeesh and V. Varma.
2006.
Webkhoj:Indian language ir from multiple character encodings.International World Wide Web Conference.Transliteration of Indic Scripts: How to use ISO 15919.http://homepage.ntlworld.com/stone-catend/trind.htm.Unicode Consortium - Universal Code Standard.
1991.http://www.unicode.org.Multi-Lingual Screen Reader and Processing of Font-data in Indian Languages.http://speech.iiit.net/speech/publications/Anand-Thesis-Final.pdf.20
