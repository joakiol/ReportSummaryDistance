Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 999?1005,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsA Hierarchical Model of Reviews for Aspect-based Sentiment AnalysisSebastian Ruder1,2, Parsa Ghaffari2, and John G. Breslin11Insight Centre for Data AnalyticsNational University of Ireland, Galway{sebastian.ruder,john.breslin}@insight-centre.org2Aylien Ltd.Dublin, Ireland{sebastian,parsa}@aylien.comAbstractOpinion mining from customer reviews hasbecome pervasive in recent years.
Sentencesin reviews, however, are usually classified in-dependently, even though they form part of areview?s argumentative structure.
Intuitively,sentences in a review build and elaborate uponeach other; knowledge of the review struc-ture and sentential context should thus in-form the classification of each sentence.
Wedemonstrate this hypothesis for the task ofaspect-based sentiment analysis by modelingthe interdependencies of sentences in a reviewwith a hierarchical bidirectional LSTM.
Weshow that the hierarchical model outperformstwo non-hierarchical baselines, obtains resultscompetitive with the state-of-the-art, and out-performs the state-of-the-art on five multilin-gual, multi-domain datasets without any hand-engineered features or external resources.1 IntroductionSentiment analysis (Pang and Lee, 2008) is used togauge public opinion towards products, to analyzecustomer satisfaction, and to detect trends.
With theproliferation of customer reviews, more fine-grainedaspect-based sentiment analysis (ABSA) has gainedin popularity, as it allows aspects of a product or ser-vice to be examined in more detail.Reviews ?
just with any coherent text ?
have anunderlying structure.
A visualization of the dis-course structure according to Rhetorical StructureTheory (RST) (Mann and Thompson, 1988) for theexample review in Figure 1 reveals that sentencesElaborationBackgroundthat they cookwith only sim-ple ingredients.I am amazed atthe quality ofthe foodI love thisrestaurant.Figure 1: RST structure of an example review.and clauses are connected via different rhetorical re-lations, such as Elaboration and Background.Intuitively, knowledge about the relations and thesentiment of surrounding sentences should informthe sentiment of the current sentence.
If a reviewerof a restaurant has shown a positive sentiment to-wards the quality of the food, it is likely that hisopinion will not change drastically over the courseof the review.
Additionally, overwhelmingly posi-tive or negative sentences in the review help to dis-ambiguate sentences whose sentiment is equivocal.Neural network-based architectures that have re-cently become popular for sentiment analysis andABSA, such as convolutional neural networks (Sev-eryn and Moschitti, 2015), LSTMs (Vo and Zhang,2015), and recursive neural networks (Nguyen andShirai, 2015), however, are only able to considerintra-sentence relations such as Background in Fig-ure 1 and fail to capture inter-sentence relations, e.g.Elaboration that rely on discourse structure and pro-vide valuable clues for sentiment prediction.We introduce a hierarchical bidirectional longshort-term memory (H-LSTM) that is able to lever-age both intra- and inter-sentence relations.
Thesole dependence on sentences and their structure999within a review renders our model fully language-independent.
We show that the hierarchical modeloutperforms strong sentence-level baselines foraspect-based sentiment analysis, while achieving re-sults competitive with the state-of-the-art and out-performing it on several datasets without relying onany hand-engineered features or sentiment lexica.2 Related WorkAspect-based sentiment analysis.
Past approachesuse classifiers with expensive hand-crafted featuresbased on n-grams, parts-of-speech, negation words,and sentiment lexica (Pontiki et al, 2014; Pontikiet al, 2015).
The model by Zhang and Lan (2015)is the only approach we are aware of that considersmore than one sentence.
However, it is less expres-sive than ours, as it only extracts features from thepreceding and subsequent sentence without any no-tion of structure.
Neural network-based approachesinclude an LSTM that determines sentiment towardsa target word based on its position (Tang et al, 2015)as well as a recursive neural network that requiresparse trees (Nguyen and Shirai, 2015).
In contrast,our model requires no feature engineering, no posi-tional information, and no parser outputs, which areoften unavailable for low-resource languages.
Weare also the first ?
to our knowledge ?
to frame sen-timent analysis as a sequence tagging task.Hierarchical models.
Hierarchical models havebeen used predominantly for representation learn-ing and generation of paragraphs and documents:Li et al (2015) use a hierarchical LSTM-based au-toencoder to reconstruct reviews and paragraphs ofWikipedia articles.
Serban et al (2016) use a hier-archical recurrent encoder-decoder with latent vari-ables for dialogue generation.
Denil et al (2014) usea hierarchical ConvNet to extract salient sentencesfrom reviews, while Kotzias et al (2015) use thesame architecture to learn sentence-level labels fromreview-level labels using a novel cost function.
Themodel of Lee and Dernoncourt (2016) is perhaps themost similar to ours.
While they also use a sentence-level LSTM, their class-level feed-forward neuralnetwork is only able to consider a limited number ofpreceding texts, while our review-level bidirectionalLSTM is (theoretically) able to consider an unlim-ited number of preceding and successive sentences.3 ModelIn the following, we will introduce the differentcomponents of our hierarchical bidirectional LSTMarchitecture displayed in Figure 2.3.1 Sentence and Aspect RepresentationEach review consists of sentences, which are paddedto length l by inserting padding tokens.
Each reviewin turn is padded to length h by inserting sentencescontaining only padding tokens.
We represent eachsentence as a concatentation of its word embeddingsx1:l where xt ?
Rk is the k-dimensional vector ofthe t-th word in the sentence.Every sentence is associated with an aspect.
As-pects consist of an entity and an attribute, e.g.FOOD#QUALITY.
Similarly to the entity represen-tation of Socher et al (2013), we represent everyaspect a as the average of its entity and attribute em-beddings 12(xe + xa) where xe, xa ?
Rm are them-dimensional entity and attribute embeddings re-spectively1.3.2 LSTMWe use a Long Short-Term Memory (LSTM)(Hochreiter and Schmidhuber, 1997), which addsinput, output, and forget gates to a recurrent cell,which allow it to model long-range dependenciesthat are essential for capturing sentiment.For the t-th word in a sentence, the LSTM takesas input the word embedding xt, the previous outputht?1 and cell state ct?1 and computes the next out-put ht and cell state ct.
Both h and c are initializedwith zeros.3.3 Bidirectional LSTMBoth on the review and on the sentence level, senti-ment is dependent not only on preceding but alsosuccessive words and sentences.
A BidirectionalLSTM (Bi-LSTM) (Graves et al, 2013) allows us tolook ahead by employing a forward LSTM, whichprocesses the sequence in chronological order, anda backward LSTM, which processes the sequence inreverse order.
The output ht at a given time step isthen the concatenation of the corresponding states ofthe forward and backward LSTM.1Averaging embeddings produced slightly better results thanusing a separate embedding for every aspect.1000Food is great.
Service is top notch.FOOD#QUALITY SERVICE#GENERALLSTM LSTM LSTMLSTM LSTM LSTM 00 LSTM LSTM LSTMLSTM LSTM LSTM LSTMLSTMLSTMLSTM LSTMLSTMOUT OUT0 0OutputOutputlayerReview-levelbackward LSTMReview-levelforward LSTMSentence-levelbackward LSTMSentence-levelforward LSTMAspect/wordembeddingsFigure 2: The hierarchical bidirectional LSTM (H-LSTM) for aspect-based sentiment analysis.
Word embeddings are fed intoa sentence-level bidirectional LSTM.
Final states of forward and backward LSTM are concatenated together with the aspect em-bedding and fed into a bidirectional review-level LSTM.
At every time step, the output of the forward and backward LSTM isconcatenated and fed into a final layer, which outputs a probability distribution over sentiments.3.4 Hierarchical Bidirectional LSTMStacking a Bi-LSTM on the review level on topof sentence-level Bi-LSTMs yields the hierarchicalbidirectional LSTM (H-LSTM) in Figure 2.The sentence-level forward and backward LSTMsreceive the sentence starting with the first and lastword embedding x1 and xl respectively.
The finaloutput hl of both LSTMs is then concatenated withthe aspect vector a2 and fed as input into the review-level forward and backward LSTMs.
The outputs ofboth LSTMs are concatenated and fed into a finalsoftmax layer, which outputs a probability distribu-tion over sentiments3 for each sentence.4 Experiments4.1 DatasetsFor our experiments, we consider datasets in fivedomains (restaurants, hotels, laptops, phones, cam-2We experimented with other interactions, e.g.
rescaling theword embeddings by their aspect similarity, an attention-likemechanism, as well as summing and multiplication, but foundthat simple concatenation produced the best results.3The sentiment classes are positive, negative, and neutral.eras) and eight languages (English, Spanish, French,Russian, Dutch, Turkish, Arabic, Chinese) fromthe recent SemEval-2016 Aspect-based SentimentAnalysis task (Pontiki et al, 2016), using the pro-vided train/test splits.
In total, there are 11 domain-language datasets containing 300-400 reviews with1250-6000 sentences4.
Each sentence is annotatedwith none, one, or multiple domain-specific aspectsand a sentiment value for each aspect.4.2 Training DetailsOur LSTMs have one layer and an output size of 200dimensions.
We use 300-dimensional word embed-dings.
We use pre-trained GloVe (Pennington et al,2014) embeddings for English, while we train em-beddings on frWaC5 for French and on the LeipzigCorpora Collection6 for all other languages.7 Entity4Exact dataset statistics can be seen in (Pontiki et al, 2016).5http://wacky.sslmit.unibo.it/doku.php?id=corpora6http://corpora2.informatik.uni-leipzig.de/download.html7Using 64-dimensional Polyglot embeddings (Al-Rfou etal., 2013) yielded generally worse performance.1001Language Domain Best XRCE IIT-TUDA CNN LSTM H-LSTM HP-LSTMEnglish Restaurants 88.1 88.1 86.7 82.1 81.4 83.0 85.3Spanish Restaurants 83.6 - 83.6 79.6 75.7 79.5 81.8French Restaurants 78.8 78.8 72.2 73.2 69.8 73.6 75.4Russian Restaurants 77.9 - 73.6 75.1 73.9 78.1 77.4Dutch Restaurants 77.8 - 77.0 75.0 73.6 82.2 84.8Turkish Restaurants 84.3 - 84.3 74.2 73.6 76.7 79.2Arabic Hotels 82.7 - 81.7 82.7 80.5 82.8 82.9English Laptops 82.8 - 82.8 78.4 76.0 77.4 80.1Dutch Phones 83.3 - 82.6 83.3 81.8 81.3 83.6Chinese Cameras 80.5 - - 78.2 77.6 78.6 78.8Chinese Phones 73.3 - - 72.4 70.3 74.1 73.3Table 1: Results of our system with randomly initialized word embeddings (H-LSTM) and with pre-trained embeddings(HP-LSTM) for ABSA for each language and domain in comparison to the best system for each pair (Best), the best two sin-gle systems (XRCE, IIT-TUDA), a sentence-level CNN (CNN), and our sentence-level LSTM (LSTM).and attribute embeddings of aspects have 15 dimen-sions and are initialized randomly.
We use dropoutof 0.5 after the embedding layer and after LSTMcells, a gradient clipping norm of 5, and no l2 regu-larization.We unroll the aspects of every sentence in the re-view, e.g.
a sentence with two aspects occurs twicein succession, once with each aspect.
We removesentences with no aspect8 and ignore predictions forall sentences that have been added as padding to a re-view so as not to force our model to learn meaning-less predictions, as is commonly done in sequence-to-sequence learning (Sutskever et al, 2014).
Wesegment Chinese data before tokenization.We train our model to minimize the cross-entropyloss, using stochastic gradient descent, the Adamupdate rule (Kingma and Ba, 2015), mini-batches ofsize 10, and early stopping with a patience of 10.4.3 Comparison modelsWe compare our model using random (H-LSTM)and pre-trained word embeddings (HP-LSTM)against the best model of the SemEval-2016 Aspect-based Sentiment Analysis task (Pontiki et al, 2016)for each domain-language pair (Best) as well asagainst the two best single models of the competi-tion: IIT-TUDA (Kumar et al, 2016), which useslarge sentiment lexicons for every language, andXRCE (Brun et al, 2016), which uses a parser aug-8Labeling them with a NONE aspect and predicting neutralslightly decreased performance.mented with hand-crafted, domain-specific rules.
Inorder to ascertain that the hierarchical nature of ourmodel is the deciding factor, we additionally com-pare against the sentence-level convolutional neuralnetwork of Ruder et al (2016) (CNN) and against asentence-level Bi-LSTM (LSTM), which is identicalto the first layer of our model.95 Results and DiscussionWe present our results in Table 1.
Our hierarchi-cal model achieves results superior to the sentence-level CNN and the sentence-level Bi-LSTM base-lines for almost all domain-language pairs by takingthe structure of the review into account.
We high-light examples where this improves predictions inTable 2.In addition, our model shows results competi-tive with the best single models of the competi-tion, while requiring no expensive hand-crafted fea-tures or external resources, thereby demonstratingits language and domain independence.
Overall,our model compares favorably to the state-of-the-art,particularly for low-resource languages, where fewhand-engineered features are available.
It outper-forms the state-of-the-art on four and five datasetsusing randomly initialized and pre-trained embed-dings respectively.9To ensure that the additional parameters do not account forthe difference, we increase the number of layers and dimensionsof LSTM, which does not impact the results.1002Id Sentence LSTM H-LSTM1.1 No Comparison negative positive1.2 It has great sushi and positive positiveeven better service.2.1 Green Tea creme positive positivebrulee is a must!2.2 Don?t leave the negative positiverestaurant without it.Table 2: Example sentences where knowledge of other sen-tences in the review (not necessarily neighbors) helps to dis-ambiguate the sentiment of the sentence in question.
For theaspect in 1.1, the sentence-level LSTM predicts negative, whilethe context of the service and food quality in 1.2 allows theH-LSTM to predict positive.
Similarly, for the aspect in 2.2,knowledge of the quality of the green tea cr?me brul?e helpsthe H-LSTM to predict the correct sentiment.5.1 Pre-trained embeddingsIn line with past research (Collobert et al, 2011), weobserve significant gains when initializing our wordvectors with pre-trained embeddings across almostall languages.
Pre-trained embeddings improve ourmodel?s performance for all languages except Rus-sian, Arabic, and Chinese and help it achieve state-of-the-art in the Dutch phones domain.
We releaseour pre-trained multilingual embeddings so that theymay facilitate future research in multilingual senti-ment analysis and text classification10.5.2 Leveraging additional informationAs annotation is expensive in many real-world appli-cations, learning from only few examples is impor-tant.
Our model was designed with this goal in mindand is able to extract additional information inherentin the training data.
By leveraging the structure ofthe review, our model is able to inform and improveits sentiment predictions as evidenced in Table 2.The large performance differential to the state-of-the-art for the Turkish dataset where only 1104 sen-tences are available for training and the performancegaps for high-resource languages such as English,Spanish, and French, however, indicate the limits ofan approach such as ours that only uses data avail-able at training time.While using pre-trained word embeddings is an10https://s3.amazonaws.com/aylien-main/data/multilingual-embeddings/index.htmleffective way to mitigate this deficit, for high-resource languages, solely leveraging unsupervisedlanguage information is not enough to perform on-par with approaches that make use of large exter-nal resources (Kumar et al, 2016) and meticulouslyhand-crafted features (Brun et al, 2016).Sentiment lexicons are a popular way to inject ad-ditional information into models for sentiment anal-ysis.
We experimented with using sentiment lexi-cons by Kumar et al (2016) but were not able to sig-nificantly improve upon our results with pre-trainedembeddings11.
In light of the diversity of domains inthe context of aspect-based sentiment analysis andmany other applications, domain-specific lexicons(Hamilton et al, 2016) are often preferred.
Find-ing better ways to incorporate such domain-specificresources into models as well as methods to injectother forms of domain information, e.g.
by con-straining them with rules (Hu et al, 2016) is thusan important research avenue, which we leave forfuture work.6 ConclusionIn this paper, we have presented a hierarchical modelof reviews for aspect-based sentiment analysis.
Wedemonstrate that by allowing the model to take intoaccount the structure of the review and the senten-tial context for its predictions, it is able to outper-form models that only rely on sentence informationand achieves performance competitive with mod-els that leverage large external resources and hand-engineered features.
Our model achieves state-of-the-art results on 5 out of 11 datasets for aspect-based sentiment analysis.AcknowledgmentsWe thank the anonymous reviewers, NicolasP?cheux, and Hugo Larochelle for their constructivefeedback.
This publication has emanated from re-search conducted with the financial support of theIrish Research Council (IRC) under Grant NumberEBPPG/2014/30 and with Aylien Ltd. as EnterprisePartner as well as from research supported by a re-search grant from Science Foundation Ireland (SFI)under Grant Number SFI/12/RC/2289.11We tried bucketing and embedding of sentiment scores aswell as filtering and pooling as in (Vo and Zhang, 2015)1003ReferencesRami Al-Rfou, Bryan Perozzi, and Steven Skiena.
2013.Polyglot: Distributed Word Representations for Multi-lingual NLP.
Proceedings of the Seventeenth Confer-ence on Computational Natural Language Learning,pages 183?192.Caroline Brun, Julien Perez, and Claude Roux.
2016.XRCE at SemEval-2016 Task 5: Feedbacked Ensem-ble Modelling on Syntactico-Semantic Knowledge forAspect Based Sentiment Analysis.
Proceedings of the10th International Workshop on Semantic Evaluation(SemEval 2016), pages 282?286.Ronan Collobert, Jason Weston, Leon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural Language Processing (almost) fromScratch.
Journal of Machine Learning Research,12(Aug):2493?2537.Misha Denil, Alban Demiraj, and Nando de Freitas.2014.
Extraction of Salient Sentences from LabelledDocuments.
arXiv preprint arXiv:1412.6815, pages1?9.Alex Graves, Abdel-rahman Mohamed, and GeoffreyHinton.
2013.
Speech Recognition with Deep Recur-rent Neural Networks.
IEEE International Conferenceon Acoustics, Speech and Signal Processing (ICASSP),(3):6645?6649.William L. Hamilton, Kevin Clark, Jure Leskovec, andDan Jurafsky.
2016.
Inducing Domain-Specific Sen-timent Lexicons from Unlabeled Corpora.
Proceed-ings of the 54th Annual Meeting of the Association forComputational Linguistics.Sepp Hochreiter and J?rgen Schmidhuber.
1997.Long Short-Term Memory.
Neural Computation,9(8):1735?1780.Zhiting Hu, Xuezhe Ma, Zhengzhong Liu, Eduard Hovy,and Eric Xing.
2016.
Harnessing Deep Neural Net-works with Logic Rules.
In Proceedings of the 54thAnnual Meeting of the Association for ComputationalLinguistics, pages 1?18.Diederik P. Kingma and Jimmy Lei Ba.
2015.
Adam:a Method for Stochastic Optimization.
InternationalConference on Learning Representations, pages 1?13.Dimitrios Kotzias, Misha Denil, Nando de Freitas, andPadhraic Smyth.
2015.
From Group to Individual La-bels using Deep Features.
Proceedings of the 21thACM SIGKDD International Conference on Knowl-edge Discovery and Data Mining, pages 597?-606.Ayush Kumar, Sarah Kohail, Amit Kumar, Asif Ekbal,and Chris Biemann.
2016.
IIT-TUDA at SemEval-2016 Task 5: Beyond Sentiment Lexicon: Combin-ing Domain Dependency and Distributional SemanticsFeatures for Aspect Based Sentiment Analysis.
Pro-ceedings of the 10th International Workshop on Se-mantic Evaluation (SemEval 2016).Ji Young Lee and Franck Dernoncourt.
2016.
SequentialShort-Text Classification with Recurrent and Convolu-tional Neural Networks.
Proceedings of NAACL-HLT2016.Jiwei Li, Minh-Thang Luong, and Daniel Jurafsky.
2015.A Hierarchical Neural Autoencoder for Paragraphsand Documents.
Proceedings of the 53rd AnnualMeeting of the Association for Computational Linguis-tics, pages 1106?1115.William C. Mann and Sandra A. Thompson.
1988.Rhetorical Structure Theory: Toward a functional the-ory of text organization.Thien Hai Nguyen and Kiyoaki Shirai.
2015.PhraseRNN: Phrase Recursive Neural Network forAspect-based Sentiment Analysis.
Proceedings of the2015 Conference on Empirical Methods in NaturalLanguage Processing, (September):2509?2514.Bo Pang and Lillian Lee.
2008.
Opinion Mining andSentiment Analysis.
Foundations and trends in infor-mation retrieval, 2(1-2):1?135.Jeffrey Pennington, Richard Socher, and Christopher D.Manning.
2014.
Glove: Global Vectors for Word Rep-resentation.
Proceedings of the 2014 Conference onEmpirical Methods in Natural Language Processing,pages 1532?1543.Maria Pontiki, Dimitrios Galanis, John Pavlopoulos,Haris Papageorgiou, Ion Androutsopoulos, and SureshManandhar.
2014.
SemEval-2014 Task 4: AspectBased Sentiment Analysis.
Proceedings of the 8thInternational Workshop on Semantic Evaluation (Se-mEval 2014), pages 27?35.Maria Pontiki, Dimitris Galanis, Haris Papageorgiou,Suresh Manandhar, and Ion Androutsopoulos.
2015.SemEval-2015 Task 12: Aspect Based SentimentAnalysis.
Proceedings of the 9th International Work-shop on Semantic Evaluation (SemEval 2015), pages486?495.Maria Pontiki, Dimitrios Galanis, Haris Papageorgiou,Ion Androutsopoulos, Suresh Manandhar, MohammadAL-Smadi, Mahmoud Al-Ayyoub, Yanyan Zhao, BingQin, Orph?e De Clercq, V?ronique Hoste, MariannaApidianaki, Xavier Tannier, Natalia Loukachevitch,Evgeny Kotelnikov, Nuria Bel, Salud Mar?a Jim?nez-Zafra, and G?ls?en Eryig?it.
2016.
SemEval-2016 Task5: Aspect-Based Sentiment Analysis.
In Proceedingsof the 10th International Workshop on Semantic Eval-uation, San Diego, California.
Association for Com-putational Linguistics.Sebastian Ruder, Parsa Ghaffari, and John G. Bres-lin.
2016.
INSIGHT-1 at SemEval-2016 Task 5:Deep Learning for Multilingual Aspect-based Senti-ment Analysis.
Proceedings of the 10th InternationalWorkshop on Semantic Evaluation (SemEval 2016).1004Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe,Laurent Charlin, Joelle Pineau, Aaron Courville, andYoshua Bengio.
2016.
A Hierarchical Latent VariableEncoder-Decoder Model for Generating Dialogues.Proceedings of the Advances in Neural InformationProcessing Systems 29 (NIPS 2016), pages 1?14.Aliaksei Severyn and Alessandro Moschitti.
2015.UNITN: Training Deep Convolutional Neural Net-work for Twitter Sentiment Classification.
Proceed-ings of the 9th International Workshop on SemanticEvaluation (SemEval 2015), pages 464?469.Richard Socher, Danqi Chen, Christopher D. Manning,and Andrew Y. Ng.
2013.
Reasoning With Neu-ral Tensor Networks for Knowledge Base Completion.Proceedings of the Advances in Neural InformationProcessing Systems 26 (NIPS 2013), pages 1?10.Ilya Sutskever, Oriol Vinyals, and Quoc V. Le.
2014.Sequence to sequence learning with neural networks.Advances in Neural Information Processing Systems,page 9.Duyu Tang, Bing Qin, Xiaocheng Feng, and TingLiu.
2015.
Target-Dependent Sentiment Classifica-tion with Long Short Term Memory.
arXiv preprintarXiv:1512.01100.Duy-tin Vo and Yue Zhang.
2015.
Target-DependentTwitter Sentiment Classification with Rich AutomaticFeatures.
IJCAI International Joint Conference on Ar-tificial Intelligence, pages 1347?1353.Zhihua Zhang and Man Lan.
2015.
ECNU: ExtractingEffective Features from Multiple Sequential Sentencesfor Target-dependent Sentiment Analysis in Reviews.Proceedings of the 9th International Workshop on Se-mantic Evaluation (SemEval 2015), pages 736?741.1005
