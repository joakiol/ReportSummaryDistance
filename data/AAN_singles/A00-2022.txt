Ambiguity Packing in Constraint-based ParsingPractical ResultsStephan OepenComputat iona l  LinguisticsSaar land University66041 Saarbriicken, Germanyoe@coli, uni-sb, deJohn CarrollCognitive and Computing SciencesUniversity of SussexBrighton BN1 9QH, UKj olmca?cogs, susx .
ac.
ukAbstractWe describe a novel approach to 'packing' of local am-biguity in parsing with a wide-coverage HPSG gram-mar, and provide an empirical assessment of the in-teraction between various packing and parsing strate-gies.
We present a linear-time, bidirectional subsump-tion test for typed feature structures and demonstratethat (a) subsumption- and equivalence-based packing isapplicable to large HPSG grammars and (b) average parsecomplexity can be greatly reduced in bottom-up chartparsing with comprehensive HPSG implementations.1 BackgroundThe ambiguity inherent in natural anguage meansthat during parsing, some segments of the inputstring may end up being analysed as the same typeof linguistic object in several different ways.
Eachof these different ways must be recorded, but subse-quent parsing steps must treat the set of analyses asa single entity, otherwise the computation becomestheoretically intractable.
Earley's algorithm (Ear-ley, 1970), for example, avoids duplication of parseitems by maintaining pointers to alternative deriva-tions in association with the item.
This processhas been termed 'local ambiguity packing' (Tomita,1985), and the structure built up by the parser, a'parse forest' (Billot &: Lang, 1989).
Context free(CF) grammars represent linguistic objects in termsof atomic category symbols.
The test for duplicateparse items--and thus being able to pack the sub-analyses associated with them--is equality of cate-gory symbols.
In the final parse forest every differ-ent combination ofpacked nodes induces a distinct,valid parse tree.Most existing unification-based parsing systemseither implicitly or explicitly contain a context-freecore.
For example, in the CLE (Alshawi, 1992)the (manually-assigned) functors of the Prolog termsforming the categories constitute a CF 'backbone'.In the Alvey Tools system (Carroll, 1993) each dis-tinct set of features i  automatically given a uniqueidentifier and this is associated with every categorycontaining those features.
The packing techniquehas been shown to work well in practice in theseand similar unification-augmented CF systems: theparser first tests for CF category equality, and theneither (a) checks that the existing feature structuresubsumes the newly derived one (Moore & Alshawi,1992), or (b) forms an efficiently processable disjunc-tion of the feature structures (Maxwell and Kaplan,1995).
Extracting parses from the parse forest issimilar to the CF case, except hat a global check forconsistency of feature values between packed nodesor between feature structure disjuncts is required(this global validation is not required if the sub-sumption test is strengthened to feature structureequivalence).In contrast, there is essentially no CF compo-nent in systems which directly interpret HPSG gram-mars.
Although HPSG feature structures are typed,an initial CF category equality test cannot be im-plemented straightforwardly in terms of the top-level types of feature structures since two compat-ible types need not be equal, but could stand ina subtype-supertype relationship.
In addition, thefeature structure subsumption test is potentially ex-pensive since feature structures are large, typicallycontaining hundreds of nodes.
It is therefore an openquestion whether parsing systems using grammars ofthis type can gain any advantage from local ambi-guity packing.The question is becoming increasingly impor-tant, though, as wide-coverage HPSG grammars arestarting to be deployed in practical applications--for example for 'deep' analysis in the VerbMo-bil speech-to-speech translation system (Wahlster,1997; Kiefer, Krieger, Carroll, & Malouf, 1999).
1 Inthis paper we answer the question by demonstratingthat (a) subsumption- and equivalence-based f aturestructure packing is applicable to large HPSG gram-mars, and (b) average complexity and time takenfor the parsing task can be greatly reduced.
InSection 2 we present a new, linear-time, bidirec-1A significant body of work on efficient processing withsuch grammars has been building up recently, with investi-gations into efficient feature structure operations, abstract-machine-based compilation, CF backbone computation, andfinite-state approximation f HPSG derivations, amongst o h-ers (Flickinger, Oepen, Uszkoreit, & Tsujii, 2000).162tional subsumption test for typed feature structures,which we use in a bottom-up, chart-based parsingalgorithm incorporating novel, efficient accountingmechanisms to guarantee minimal chart size (Sec-tion 3).
We present a full-scale evaluation of thetechniques on a large corpus (Section 4), and com-plete the picture with an empirically-based discus-sion of grammar estrictors and parsing strategies(Section 5).2 E f f i c ient  Subsumpt ion  andEqu iva lence  A lgor i thmsOur feature structure subsumption algorithm 2 as-sumes totally well-typed structures (Carpenter,1992) and employs similar machinery to thequasi-destructive unification algorithm described byTomabechi (1991).
In particular, it uses temporarypointers in dag nodes, each pointer tagged with ageneration counter, to keep track of intermediateresults in processing; incrementing the generationcounter invalidates all temporary pointers in a sin-gle operation.
But whereas quasi-destructive unifi-cation makes two passes (determining whether theunification will be successful and then copying outthe intermediate representation) the subsumptionalgorithm makes only one pass, checking reentran-cies and type-supertype r lationships at the sametime.
3 The algorithm, shown in Figure 1, also si-multaneously tests if both feature structures ub-sume each other (i.e.
they are equivalent), if eithersubsumes the other, or if there is no subsumptionrelation between them in either direction.The top-level entry point dag-subsumes-pO andsubsidiary function dag-subsumes-pO 0 each returntwo values, held in variables \]orwardp and back-wardp, both initially true, recording whether it ispossible that the first dag subsumes the secondand/or vice-versa, respectively.
When one of thesepossibilities has been ruled out the appropriate vari-able is set to false; in the statement of the algorithmthe two returned values are notated as a pair, i.e.
(/orwardp, backwardp).
If at any stage both vari-ables have become set to false the possibility of sub-sumption in both directions has been ruled out sothe algorithm exits.The (recursive) subsidiary function dag-subsumes-pO 0 does most of the work, traversing the two input2Although independently-developed implementations ofessentially the same algorithm can be found in the source codeof The Attribute Logic Engine (ALE) version 3.2 (Carpenter& Penn, 1999) and the SICStus Prolog term utilities library(Penn, personal communication), we believe that there is noprevious published escription of the algorithm.3Feature structure F subsumes feature structure G iff:(1) if path p is defined in F then p is also defined in G andthe type of the value of p in F is a supertype or equal to thevalue in G, and (2) all paths that are reentrant in F are alsoreentrant in G.dags in step.
First, it checks whether the currentnode in either dag is involved in a reentrancy thatis not present in the other: for each node visitedin one dag it adds a temporary pointer (held in the'copy' slot) to the corresponding node in the otherdag.
If a node is reached that already has a pointerthen this is a point of reentrancy in the dag, andif the pointer is not identical to the other dag nodethen this reentrancy is not present in the other dag.In this case the possibility that the former dag sub-sumes the latter is ruled out.
After the reentrancycheck the type-supertype r lationship between thetypes at the current nodes in the two dags is deter-mined, and if one type is not equal to or a supertypeof the other then subsumption cannot hold in thatdirection.
Finally, after successfully checking thetype-supertype r lationships, the function recursesinto the arcs outgoing from each node that have thesame label.
Since we are assuming totally well-typedfeature structures, it must be the case that either thesets of arc labels in the two dags are the same, orone is a strict superset of the other.
Only arcs withthe same labels need be processed; extra arcs neednot since the type-supertype check at the two nodeswill already have determined that the feature struc-ture containing the extra arcs must be subsumed bythe other, and they merely serve to further specifyit and cannot affect the final result.Our implementation f the algorithm contains ex-tra redundant but cheap optimizations which for rea-sons of clarity are not shown in figure 1; these in-clude tests that forwardp is true immediately beforethe first supertype check and that backwardp is truebefore the second.
4The use of temporary pointers means that thespace complexity of the algorithm is linear in thesum of the sizes of the feature structures.
However,in our implementation the 'copy' slot that the point-ers occupy is already present in each dag node (it isrequired for the final phase of unification to storenew nodes representing equivalence classes), so inpractice the subsumption test does not allocate anynew storage.
All pointer references take constanttime since there are no chains of 'forwarded' point-ers (forwarding takes place only during the course ofunification and no forwarded pointers are left after-wards).
Assuming the supertype tests can be carried4There is scope for further optimisation of the algorithm inthe case where dagl and dag2 are identical: full processing in-side the structure is not required (since all nodes inside it willbe identical between the two dags and any strictly internalreentrancies will necessarily be the same), but we would stillneed to assign temporary pointers inside it so that any exter-nal reentrancies into the structure would be treated correctly.In our tests we have found that as far as constituents hat arecandidates for local ambiguity packing are concerned there isin fact little equality of structures between them, so specialequality processing does not justify the extra complication.1631 procedure dag-subsumes-p(dagl,dag2) --_2 (forwardp, backwardp) <-- { establish context for non-local exit}3 catch with tag 'fail' dag-subsumes-pO(dagl, dag2, true, true);4 invalidate-temporary-pointers(); {reset emporary 'copy' pointers}5 return (forwardp, backwardp);6 end7 procedure dag-subsumes-pO(dagl,dag2,forwardp, backwardp) -8 if (dagl.copy isempty) then dagl.copy <--- dag2; {check reentraneies}9 else if~dagl.copy ~ dag2) then forwardp <-- false; f i10 if  (dag2.copy is empty) then dag2.copy ~- dagl;11 else i f  (dag2.copy p dagl) then backwardp ~- false; fi12 if  (forwardp = false and backwardp = false) then13 throw (false, false) with tag 'fail'; {reentrancy check failed}14 fi15 if (not supertype-or-equal-p(dagl.type, dag2.type)) then forwardp +- false; fi {check types}16 if (not supertype-or-equal-p(dag2.type, dagl.type)) then backwardp <-- false; fl17 if (forwardp = false and backwardp = false) then18 throw (false, false) with tag 'fail'; {no subtype relations}19 fi20 for each arc in intersect(dagl.arcs, dag2.arcs) do {check shared arcs recursively}21 (forwardp, backwardp) <-22 dag-subsumes-pO(destination of arc for dagl, destination of arc for dag2, forwardp, backwardp);23 od24 return (forwardp, backwardp); {signal result to caller}25 endFigure 1: Bidirectional, linear-time feature structure subsumption (and equivalence) algorithm.out in constant time (e.g.
by table lookup), and thatthe grammar allows us to put a small constant upperbound on the intersection ofoutgoing arcs from eachnode, the processing in the body of dag-subsumes-pO 0 takes unit time.
The body may be executed upto N times where N is the number of nodes in thesmaller of the two feature structures.
So overall thealgorithm has linear time complexity.
In practice,our implementation (i the environment described inSection 4) performs of the order of 34,000 top-levelfeature structure subsumption tests per second.3 Ambiguity Packing in the ParserMoore and Alshawi (1992) and Carroll (1993) haveinvestigated local ambiguity packing for unificationgrammars with CF backbones, using CF categoryequality and feature structure subsumption to testif a newly derived constituent can be packed.
If anew constituent is equivalent to or subsumed by anexisting constituent, then it can be packed into theexisting one and will take no further part in pro-cessing.
However, if the new constituent subsumesan existing one, the situation is not so straightfor-ward: either (a) no packing takes place and the newconstituent forms a separate dge (Carroll, 1993), or(b) previous processing involving the old constituentis undone or invalidated, and it is packed into thenew one (Moore & Alshawi, 1992; however, it is un-clear whether they achieve maximal compactness inpractice: see Table 1).
In the former case the parseforest produced will not be optimally compact; inthe latter it will be, but maintaining chart consis-tency and parser correctness becomes a non-trivialproblem.
Packing of a new edge into an existing onewe call proactive (or forward) packing; for the morecomplex situation involving a new edge subsumingan existing one we introduce the term retroactive (orbackward) packing.Several issues arise when packing an old edge (old)into one that was newly derived (new) retroactively:(i) everything derived from old (called derivatives ofold in the following) must be invalidated and ex-cluded from further processing (as new is knownto generate more general derivatives); and (ii) allpending computation i volving old and its deriva-tives has to be blocked efficiently.
Derivatives ofold that are invalidated because of retroactive pack-ing may already contain packed analyses, however,which still represent valid ambiguity.
These need tobe repacked into corresponding derivatives of newwhen those become available.
In turn, derivatives ofold may have been packed already, such that theyneed not be available in the chart for subsequent sub-sumption tests.
Therefore, the parser cannot simplydelete verything derived from old when it is packed;instead, derivatives must be preserved (but blocked)164123456789101112131415161718192021procedure  block(edge, mark) -i f  (edge.frozen = false or mark = freeze) then edge.frozen +- mark; fifor each parent in edge.parents do block(parent, freeze); odendprocedure packed-edge-p(new) -for each old in chart\[new.start\]\[new.end\] do(forwardp, backwardp) ~- dag-subsumes-p(old.dag, new.dag);i f  (forwardp = true and old.frozen = fa/se) thenold.packed ~-- (new I old.packed);re turn  true;fii f  (backwardp) thennew.packed ~-- (new.packed @old.packed);old.packed +-- 0;{mark current edge}{ recursively freeze derivatives}{passive dges with same span}{ test category subsumption}{ equivalent or proactive packing}{pack 'new' into 'old'}{return to caller; signal success}{retroactive packing}{raise all packings into new host}if (old.frozen = false) then  new.packed e- (old I new.packed); fi {pack 'old' into 'new'}block(old, frost); {frost 'old' and freeze derivatives}delete(old, chart); {remove 'old' from the chart}flodre turn  false; {signal failure to pack 'new' to caller}endFigure 2: Algorithm called on each newly derived edge to achieve maximal packing.until the derivations have been recomputed on thebasis of new.
5 As new is equivalent to or more gen-eral than old it is guaranteed to derive at least thesame set of edges; furthermore, the derivatives ofnew will again be equivalent to or more general thanthe corresponding edges derived from old.The procedure packed-edge-p(), sketched in Fig-ure 2, achieves pro- and retroactive packing with-out significant overhead in the parser; the algorithmcan be integrated with arbitrary bottom-up (chart-based) parsing strategies.
The interface assumesthat the parser calls packed-edge-pO on each newedge new as it is derived; a return value of true indi-cates that new was packed proactively and requiresno further processing.
Conversely, a false returnvalue from packed-edge-p 0 signals that new shouldsubsequently undergo regular processing.
The sec-ond part of the interface builds on notions we callfrosting and freezing, meaning temporary and per-mament invalidation of edges, respectively.
As aside-effect of calls to packed-edge-p(), a new edgecan cause retroactive packing, resulting in the dele-5The situation is simpler in the CLE parser (Moore & Al-shawl, 1992) because constituents and dominance relationsare separated in the chart.
The CLE encoding, in fact, does notrecord the actual daughters used in building a phrase (e.g.
asunique references or pointers, as we do), but instead preservesthe category information (i.e.
a description) of those daugh-ters.
Hence, in extracting complete parses from the chart,the CLE has to perform (a limited) search with re-unificationof categories; in this respect, the CLE parse forest still is anunderspecified representation f the set of analyses, whereasour encoding (see below) facilitates unpacking without extrasearch.tion of one or more existing edges from the chartand blocking of derivatives.
Whenever the parseraccesses the chart (i.e.
in trying to combine edges)or retrieves a task from the agenda, it is expectedto ignore all edges and parser tasks involving suchedges that have a non-null 'frozen' value.
When anexisting edge old is packed retroactively, it is frostedand ignored by the parser; as old now represents lo-cal ambiguity, it still has to be taken into accountwhen the parse forest is unpacked.
Derivatives ofold, on the other hand, need to be invalidated inboth further parsing and later unpacking, since theywould otherwise give rise to spurious analyses; ac-cordingly, such derivatives are frozen permanently.Frosting and freezing is done in the subsidiary pro-cedure block () that walks up the parent link recur-sively, storing a mark into the 'frozen' slot of edgesthat distinguishes between temporary frosting (inthe top-level call) and permanent freezing (in recur-sire calls).For a newly derived edge new, packed-edge-pOtests mutual subsumption against all passive edgesthat span the same portion of the input string.When forward subsumption (or equivalence) is de-tected and the existing edge old is not blocked, reg-ular proactive packing is performed (adding new tothe packing list for old) and the procedure returnsimmediately.
6 In the case of backward subsump-6packing an edge el into another edge e2 logically meansthat e2 will henceforth serve as a representative for el andthe derivation(s) that it encodes.
In practice, el is removedfrom the chart and ignored in subsequent parser action andsubsumption tests.
Only in unpacking the parse forest will1652000017500150001250010000750050002500 -0"No Chart Packing Ie passive edges \]?
* .o .
.
.
.
|  "'.
?, , i , , , = , i i = , ,3 5 7 9 11 13 15 17 19 21 23 25String Length (in words)Figure 3: Effects of maximal ambiguity packingtion, analyses packed into old are raised into new(using the append operator '~' because new can at-tract multiple xisting edges in the loop); old itself isonly packed into new when it is not blocked already.Finally, old is frosted, its derivatives are recursivelyfrozen, and old is deleted from the chart.
In contrastto proactive packing, the top-level loop in the pro-cedure continues so that new can pick up additionaledges retroactively.
However, once a backward sub-sumption is detected, it follows that no proactivepacking can be achieved for new, as the chart can-not contain an edge that is more general than old.4 Empi r i ca l  Resu l t sWe have carried out an evaluation of the algo-rithms presented above using the LinGO grammar(Flickinger & Sag, 1998), a publicly-available, multi-purpose, broad-coverage HPSG of English developedat CSLI Stanford.
With roughly 8,000 types, an av-erage feature structure size of around 300 nodes, and64 lexical and grammar rules (fleshing out the inter-action of HPSG ID schemata, wellformedness prin-ciples, and LP constraints), LinGO is among thelargest HPSG grammars available.
We used the LKBsystem (Copestake, 1992, 1999) as an experimen-tation platform since it provides a parameterisablebottom-up chart parser and precise, fine-grainedprofiling facilities (Oepen & Flickinger, 1998).
7 Allof our results were obtained in this environment,running on a 300 Mhz UltraSparc, and using a bal-anced test set of 2,100 sentences extracted fromVerbMobil corpora of transcribed speech: inputlengths from 1 to 20 words are represented with 100test items each; although sentences in the corpusrange up to 36 words in length there are relativelyfew longer than 20 words.the category of el and its decomposition(s) in daughter edges(and corresponding subtrees) be used again, to multiply outand project local ambiguity.
;'The LinGO grammar and LKB software are publicly avail-able at 'h t tp : / / l i ngo .
stanford,  edu/'.2000017500150001250010000750050002500O~Pro- and Retroactive Packing I\] o passive edges \]A ,, .kit atAatmt'lLttvtd~l~lL, m ~  w w w w w w w w l v l ; w w ~"} A 1'3 1'5 1'7 2'1 2'3 25String Length (in words)on the total chart size (truncated above 25 words).Figure 3 compares total chart size (in all-pathsmode) for the regular LKB parser and our variantwith pro- and retroactive packing enabled.
Factor-ing ambiguity reduces the number of passive dgesby a factor of more than three on average, while fora number of cases the reduction is by a factor of 30and more.
Compared to regular parsing, the rate ofincrease of passive chart items with respect o sen-tence length is greatly diminished.To quantify the degree of packing we achievein practice, we re-ran the experiment reported byMoore and Alshawi (1992): counting the number ofnodes required to represent all readings for a simpledeclarative sentence containing zero to six preposi-tional phrase (PP) modifiers.
The results reportedby Moore and Alshawi (1992) (using the CLE gram-mar of English) and those obtained using pro- andretroactive packing with the LinGO grammar arepresented in Table 1.
8 Although the comparisoninvolves different grammars we believe it to be in-structive, since (i) both grammars have comprehen-sive coverage, (ii) derive the same numbers of read-ings for all test sentences in this experiment, (iii)require (almost) the same number of nodes for thebasic cases (zero and one PP), (iv) exhibit a similarsize in nodes for one core PP (measured by the in-crement from n = 0 to n = 1), and (v) the syntacticsimplicity of the test material hardly allows crosstalkSMoore and Alshawi (1992) use the terms 'node' and'record' interchangeably in their discussion of packing, wherethe CLE chart is comprised of separate con(stituent) andana(lysis) entries for category and dominance information,respectively.
It is unclear whether the counting of 'packednodes' in Moore and Alshawi (1992) includes con records ornot, since only maa records are required in parse tree recovery.In any case, both types of chart record need to be checked bysubsumption as new entries are added to the chart.
Con-versely, in our setup each edge represents not only the nodecategory, but also pointers to the daughter(s) that gave riseto this edge, and moreover, where applicable, a list of packededges that are subsumed by the category (but not necessarilyby the daughters).
For the LKB, the column 'result edges' inTable 1 refers to the total number of edges in the chart thatcontribute to at least one complete analysis.166Kim saw a cat (in the hotel) nCPU Timen readings parse unpack I plainmsec msec msec0 11 22 53 144 425 1326 429Moore & Alshawipacked nodes10 1.021 2.138 3.862 6.294 9.4135 13.5186 18.6Our Methodresult edges11 1.023 2.138 3.556 5.177 7.0101 9.2128 11.62103404606008701,1501,4601040802005901,8605,6901802905301,1802,9908,79028,160Table h Comparison of retroactive packing vs. the method used by Moore and Alshawi (1992); columnslabeled '+' show the relative increase of packed nodes (result edges) normalised to the n -- 0 baseline.with other grammatical phenomena.
Comparing rel-ative packing efficiency with increasing ambiguity(the columns labeled ' - '  in Table 1), our method ap-pears to produce a more compact representation fambiguity than the CLE, and at the same time buildsa more specific representation f the parse forest hatcan be unpacked without search.
To give an impres-sion of parser throughput, Table 1 includes timingsfor our parsing and unpacking (validation) phases,contrasted with the plain, non-packing LKB parser:as would be expected, parse time increases linearlyin the number of edges, while unpacking costs re-flect the exponential increase in total numbers ofanalyses; the figures show that our packing schemeachieves a very significant speedup, even when un-packing time is included in the comparison.5 Choos ing  the  Grammar  Rest r i c to rand Pars ing  S t ra tegyIn order for the subsumption relation to apply mean-ingfully to HPSG signs, two conditions must be met.Firstly, parse tree construction must not be dupli-cated in the feature structures (by means of theHPSG DTRS feature) but be left to the parser (i.e.recorded in the chart); this is achieved in a stan-dard way by feature structure restriction (Shieber,1985) applied to all passive dges.
Secondly, the pro-cessing of constraints hat do not restrict he searchspace but build up new (often semantic) structureshould be postponed, since they are likely to inter-fere with subsumption.
For example, analyses thatdiffer only with respect o PP attachment wouldhave the same syntax, but differences in semanticsmay prevent hem being packed.
This problem canbe overcome by using restriction to (temporarily) re-move such (semantic) attributes from lexical entriesand also from the rule set, before they are inputto the parser in the initial parse forest constructionphase.
The second, unpacking phase of the parser e-verts to the unrestricted constraint set, so we can al-low overgeneration in the first phase and filter glob-ally inconsistent analyses during unpacking.
Thus,the right choice of grammar restrictor can be viewedas an empirical rather than analytical problem.Table 2 summarizes packing efficiency and parserperformance for three different restrictors (labeledno, partial, and full semantics, respectively); togauge effects of input complexity, the table is fur-ther subdivided by sentence l ngth into two groups(of around 1,000 sentences each).
Compared to reg-ular parsing, packing with the full semantics in placeis not effective: the chart size is reduced slightly, butthe extra cost for testing subsumption i creases totalparse times by a factor of more than four.
Eliminat-ing all semantics (i.e.
the entire HPSG C0NT value), onthe other hand, results in overgeneralisation: withless information i the feature structures we achievethe highest number of packings, but at the sametime rules apply much more freely, resulting in alarger chart compared to parsing with a partial se-mantics; moreover, unpacking takes longer becausethe parse forest now contains inconsistent analyses.Restricting compositional semantics but preservingattributes that participate in selection and agree-ment results in minimal chart size and parsing time(shown in the partial semantics figures) for both di-visions of the test corpus.The majority of packings involve equivalent fea-ture structures which suggests that unpacking couldbe greatly simplified if the grammar restrictor wasguaranteed to preserve the generative capacity ofthe grammar (in the first parsing phase); then, onlypackings involving actual subsumption would haveto be validated in the unpacking phase.
9 Finally,9There is room for further investigation here: partly fortheory-internal reasons, current development of the LinGOgrammar isworking towards astricter separation of restrictive(selectional) and constructive (compositional) constraints in1671-10wordsI Passive PackedParser Edges Treesno semanticspartial semanticsfull semanticsno packing1161111491600.90.82.85.6no semantics 622 1.2> 10 partial semantics 575 1.0words full semantics 1693 33-9no packing 2075 99-9I Packings CPU Time (sec)= I -D?
p se I unpack15"5 4"1 2"6 1"8 0"37 \] 0"0512"0 3"6 2"4 1"4 0"33 1 0"052"1 0"4 0"2 0"1 0"60 0"04. .
.
.
0"44179"0 42"1 23"8 26"0 2"37 0"70134"9 35"0  20"6 18"9 1"97 0"6338"3 3"4 2"9 3"2 29"40 0"56. .
.
.
6"46Table 2: Contrasting various grammar estrictors on short (top) and medium-length (bottom) inputs; allnumbers are averaged over 1,000 items per class; packings are, from left to right: equivalence ( ' - ' ) ,  pro-('-~') and retroactive ( 'r ' )  packings, and the number of edges that were frozen ('?
').we note that the number of retroactive packings isrelatively small, and on average ach such packingleads to only one previously derived edge being in-validated.
This, of course, is a function of the orderin which edges are derived, i.e.
the parsing strategy.All the results in Table 2 were obtained with a'right corner' strategy which aims to exhaust compu-tation for any suffix of the input string before mov-ing the input pointer to the left; this is achieved bystart (where start means of a scoring function end - -W-and end are the vertices of the derivation that wouldresult from the computation, and n is the total inputlength) that orders parser tasks in the agenda.
How-ever, we have observed (Oepen & Callmeier, 2000)that HPSG-type, highly lexicalized grammars bene-fit greatly from a bidirectional, 'key'-driven, activeparsing regime, since they often employ rules withunderspecified arguments that are only instantiatedby coreference with other daughters (where the 'key'daughter is the linguistic head in many but not allconstructions).
This requirement and the generalnon-predictability of categories derived for any to-ken substring (in particular with respect o unaryrule applications), means that a particular parsingstrategy may reduce retroactive packing but cannotavoid it in general.
With pro- and retroactive pack-ing and the minimal accounting overhead, we findoverall parser throughput to be very robust againstvariation in the parsing strategy.
Lavie and Rosd(2000) present heuristics for ordering parser actionsto achieve maximally compact parse forests--thoughonly with respect to a CF category backbone---in theabsence of retroactive packing; however, the tech-niques we have presented here allow local ambigu-ity packing and parser tuning--possibly includingpriority-driven best-first search--to be carried outmostly independently of each other.the grammar and underlying semantic theory.
We expect hatour approach to packing will benefit from these developments.6 Conc lus ionsWe have presented novel algorithms for efficient sub-sumption checking and pro- and retroactive localambiguity packing with large feature structures, andhave provided strong empirical evidence that ourapproach can be applied beneficially to chart pars-ing with a large, broad-coverage HPSG of English.By comparison to previous work in unification-basedparsing we have demonstrated that pro- and retroac-tive packing are well-suited to achieve optimal pack-ing; furthermore, xperimental results obtained witha publicly-available HPSG processing platform con-firm that ambiguity packing can greatly reduce av-erage parse complexity for this type of grammars.In related work, Miyao (1999) describes an ap-proach to packing in which alternative feature struc-tures are represented as packed, distributed isjunc-tions of feature structure fragments.
Although theapproach may have potential, the shifting of com-plex accounting into the unification algorithm is atvariance with the findings of Kiefer et al (1999),who report large speed-ups from the elimination ofdisjunction processing during unification.
Unfortu-nately, the reported evaluation measures and lack ofdiscussion of parser control issues are insufficient toallow a precise comparison.We intend to develop the approach presented inthis paper in several directions.
Firstly, we will en-hance the unpacking phase to take advantage of thelarge number of equivalence packings we observe.This will significantly reduce the amount of work itneeds to do.
Secondly, many application contextsand subsequent layers of semantic processing willnot require unfolding the entire parse forest; here,we need to define a selective, incremental unpack-ing procedure.
Finally, applications like VerbMo-bil favour prioritized best-first rather than all-pathsparsing.
Using slightly more sophisticated account-ing in the agenda, we plan to investigate priority168propagation i a best-first variant of our parser.AcknowledgementsWe are grateful to Ulrich Callmeier, Ann Copestake,Dan Flickinger, and three anonymous reviewers forcomments on a draft of the paper, to Bob Moore fora detailed explanation of the workings of the CLEparser, and to Gerald Penn for information aboutrelated implementations of the subsumption algo-rithm.
The research was supported by the DeutscheForschungsgemeinschaft as part of the CollaborativeResearch Division Resource-Adaptive Cognitive Pro-cesses, project B4 (PERFORM); and by a UK EPSRCAdvanced Fellowship to the second author.Re ferencesAlshawi, H.
(Ed.).
(1992).
The Core Language En-gine.
Cambridge, MA: MIT Press.Billot, S., & Lang, B.
(1989).
The structure ofshared forests in ambiguous parsing.
In Proceed-ings of the 27th Meeting of the Association forComputational Linguistics (pp.
143-151).
Van-couver, BC.Carpenter, B.
(1992).
The logic of typed featurestructures.
Cambridge, UK: Cambridge Univer-sity Press.Carpenter, B., & Penn, G. (1999)?
ALE.
The At-tribute Logic Engine.
User's guide version 3.2.
(Universit~it Tfibingen: http://wwww, sfs .
nphi l?
un i - tueb ingen,  de/~gpenn/ale, html)Carroll, J.
(1993).
Practical unification-basedparsing of natural language (Technical Re-port # 314).
Cambridge, UK: ComputerLaboratory, Cambridge University.
(Onlineat: f tp  : / / f tp .
el .
cam.
ac.
uk /papers / repor ts /TR314-j ac-pract ical-unif-pars ing.
ps.
gz)Copestake, A.
(1992).
The ACQUILEX LKB.
Rep-resentation issues in semi-automatic a quisition oflarge lexicons.
In Proceedings of the 3rd A CL Con-ference on Applied Natural Language Processing(pp.
88-96).
Trento, Italy.Copestake, A.
(1999).
The (new) LKB sys-tem.
User's guide.
(CSLI, Stanford Uni-versity: http ://www-csli.
stanford, edu/-~aac/ikb.
html)Earley, J.
(1970).
An efficient context-free parsingalgorithm.
Communications of the ACM, 13 (2),94 - 102.Flickinger, D., Oepen, S., Uszkoreit, H., & Tsu-jii, J.
(Eds.).
(2000).
Journal of Natural Lan-guage Engineering.
Special Issue on Efficient pro-cessing with HPSG: Methods, systems, evaluation.Cambridge, UK: Cambridge University Press.
(inpreparation)Flickinger, D. P., & Sag, I.
A.
(1998).
Linguis-tic Grammars Online.
A multi-purpose broad-coverage computational grammar of English.
InCSLI Bulletin 1999 (pp.
64-68).
Stanford, CA:CSLI Publications.Kiefer, B., Krieger, H.-U., Carroll, J., & Malouf, R.(1999).
A bag of useful techniques for efficient androbust parsing.
In Proceedings of the 37th Meetingof the Association for Computational Linguistics(pp.
473-480).
College Park, MD.Lavie, A., & Ros~, C. (2000).
Optimal ambiguitypacking in context-free parsers with interleavedunification.
In Proceedings of the 6th Interna-tional Workshop on Parsing Technologies (pp.147-158).
Trento, Italy.Maxwell III, J. T., & Kaplan, R. M. (1995).
Amethod for disjunctive constraint satisfaction.
InM.
Dalrymple, R. M. Kaplan, J. T. Maxwell III,& A. Zaenen (Eds.
), Formal issues in Lexical-Functional Grammar (pp.
381-401).
Stanford,CA: CSLI Publications.Miyao, Y.
(1999).
Packing of feature structures forefficient unification of disjunctive feature struc-tures.
In Proceedings of the 37th Meeting of theAssociation for Computational Linguistics (pp.579-84).
College Park, MD.Moore, R. C., & Alshawi, H. (1992).
Syntacticand semantic processing.
In H. Alshawi (Ed.
),The Core Language Engine (pp.
129-148).
Cam-bridge, MA: MIT Press.Oepen, S., & Callmeier, U.
(2000).
Measure formeasure: Parser cross-fertilization.
Towards in-creased component comparability and exchange.In Proceedings of the 6th International Workshopon Parsing Technologies (pp.
183-194).
Trento,Italy.Oepen, S., & Flickinger, D. P. (1998).
Towards ys-tematic grammar profiling.
Test suite technologyten years after.
Journal of Computer Speech andLanguage, 12 (4) (Special Issue on Evaluation),411-436.Shieber, S. M. (1985).
Using restriction to extendparsing algorithms for complex feature-based for-malisms.
In Proceedings of the 23rd Meeting of theAssociation for Computational Linguistics (pp.145-152).
Chicago, IL.Tomabechi, H. (1991).
Quasi-destructive graph uni-fication.
In Proceedings of the 29th Meeting of theAssociation for Computational Linguistics (pp.315- 322).
Berkeley, CA.Tomita, M. (1985).
An efficient context-free parsingalgorithm for natural anguages.
In Proceedings ofthe 9th International Joint Conference on Artifi-cial Intelligence (pp.
756- 764).
Los Angeles, CA.Wahlster, W. (1997).
VerbMobil -- Erken-hung, Analyse, Transfer, Generierung und Syn-these yon Spontansprache (VerbMobil Report# 198).
Saarbriicken, Germany: DeutschesForschungszentrum fiir Kiinstliche IntelligenzGmbH.169
