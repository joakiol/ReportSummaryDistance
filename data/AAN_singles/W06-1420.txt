Proceedings of the Fourth International Natural Language Generation Conference, pages 130?132,Sydney, July 2006. c?2006 Association for Computational LinguisticsBuilding a semantically transparent corpusfor the generation of referring expressionsKees van Deemter and Ielka van der Sluis and Albert GattDepartment of Computing ScienceUniversity of Aberdeen{kvdeemte,ivdsluis,agatt}@csd.abdn.ac.ukAbstractThis paper discusses the construction ofa corpus for the evaluation of algorithmsthat generate referring expressions.
It isargued that such an evaluation task re-quires a semantically transparent corpus,and controlled experiments are the bestway to create such a resource.
We addressa number of issues that have arisen in anongoing evaluation study, among which isthe problem of judging the output of GREalgorithms against a human gold standard.1 Creating and using a corpus for GREA decade ago, Dale and Reiter (1995) publisheda seminal paper in which they compared a num-ber of GRE algorithms.
These algorithms includeda Full Brevity (FB) algorithm which generates de-scriptions of minimal length, a greedy algorithm(GA), and an Incremental Algorithm (IA).
Theauthors argued that the latter was the best modelof human referential behaviour, and versions ofthe IA have since come to represent the stateof the art in GRE.
Dale and Reiter?s hypothe-sis was motivated by psycholinguistic findings,notably that speakers tend to initiate referencesbefore they have completely scanned a domain.However, this finding affords different algorithmicinterpretations.
Similarly, the finding that basic-level terms in referring expressions allow hearersto form a psychological gestalt could be incorpo-rated into practically any GRE algorithm.1We decided to put Dale and Reiter?s hypothesisto the test by an evaluation of the output of dif-1A separate argument for IA involves tractability, but al-though some alternatives (such as FB) are intractable, others(such as GA) are only polynomial, and can therefore not eas-ily be dismissed on purely computational grounds.ferent GRE algorithms against human production.However, it is notoriously difficult to obtain suit-able corpora for a task that is as semantically in-tensive as Content Determination (for GRE).
Al-though existing corpora are valuable resources,NLG often requires information that is not avail-able in text.
Suppose, for example, that a corpuscontained articles about politics, how would theoutput of a GRE algorithm be evaluated against thecorpus?
It would be difficult to infer from an ar-ticle exactly which representatives in the BritishHouse of Commons are Liberal Democrats, orScottish.
Combining multiple texts is hazardous,since facts could alter across sources and time.Moreover, the conditions under which such textswere produced (e.g.
fault-critical or not, as ex-plained below) are hard to determine.A recent GRE evaluation by Gupta and Stent(2005) focused on dialogue corpora, using MAP-TASK and COCONUT, both of which have an as-sociated domain.
Their results show that referentidentification in MAPTASK often requires no morethan a TYPE attribute, so that none of the algo-rithms performed better than a baseline.
In con-trast to MAPTASK, COCONUT has a more elabo-rate domain, but it is characterised by a collabora-tive task, and references frequently go beyond theidentification criterion that is typically invoked inGRE2.
Mindful of the limitations of existing cor-pora, and of the extent to which evaluation de-pends on the corpus under study, we are usingcontrolled experiments to create a corpus whoseconstruction will ensure that existing algorithmscan be adequately differentiated on an identifica-tion task.2Jordan and Walker (2000) have demonstrated a signifi-cantly better match to the human data when task-related con-straints are taken into account.1302 Setup of the experimentLike Dale and Reiter (1995), we focused on first-mention descriptions.
However, we decided to in-clude simple ?disjunctive?
references to sets (asin ?the red chair and the black table?
), in addi-tion to conjunctions of atomic properties, sincethese can be handled by essentially the same al-gorithms (van Deemter, 2002).
For generality, welooked at two very different domains.
One of theseinvolved artificially constructed pictures of furni-ture, where the available attributes and values arerelatively easy to determine.
The other involvedreal photographs of individuals, which provide aricher range of options to subjects.
To date, datahas been collected from 19 participants, and anal-ysis is in progress.Our first challenge was to make the experimentnaturalistic.
Subjects were shown 38 randomisedtrials, each depicting a set of objects, one or twoof which were the targets, surrounded by 6 dis-tractors (Figure 1).
In each case, a minimal distin-guishing description of the targets was available.Subjects were led to believe that they would bedescribing the targets for an interlocutor.
Once adescription was typed, the system removed fromthe screen what it took to be the referents.Figure 1: A stimulus example from the furniture domain.Three groups performed the task in differentconditions, namely: ?
?FaultCritical?, wherehalf the subjects in the ?+FaultCritical?
casecould use location (?in the top left corner?).
The?+FaultCritical?
group was told: ?Our programwill eventually be used in situations where it iscrucial that it understands descriptions accurately.In these situations, there will often be no option tocorrect mistakes.
Therefore, (...) you will not getthe chance to revise (your description)?.
By con-trast, the ??FaultCritical?
subjects were giventhe opportunity to revise their description shouldthe system have got it wrong.
Subjects in the??Location?
condition were told that their inter-locutor could see exactly the same pictures as theycould, but these had been jumbled up; by con-trast, ?+Location?
subjects were led to believethat their addressee could see the pictures in ex-actly the same position.The second main challenge was to create tri-als that would distinguish between all the algo-rithms.
For instance, if trials involved only one at-tribute, say an object?s TYPE (e.g., chair or table),they would not allow us to distinguish IA fromFB, as both would always generate the shortest de-scription.
Subtler issues arise with local brevity(Reiter, 1990), an optimisation strategy which re-quires sufficiently complex trials to make a differ-ence.3 How to analyse the data?Our semantically transparent corpus can beused for testing various hypotheses, for in-stance about when an algorithm shouldoverspecify descriptions (e.g.
more in?+FaultCritical,+Location?
(Arts, 2004),and/or when the target is a set).
Here, we focus onthe issue raised in Section 1, namely, which of thealgorithms discussed in Dale and Reiter (1995)matches human behaviour best.The first problem is determining the relevant al-gorithms.
The IA comes in different flavours, be-cause its output depends on the order in whichthe different properties are attempted (commonlycalled the preference order).
It is possible toconsider all different IAs (trying every conceiv-able preference order), but this would increase thenumber of statistical hypotheses to be tested, im-pacting the validity of the results and requiring aBonferroni correction.
Instead, we are using a pre-test to find the optimal version of IA, comparingonly that version to the other algorithms.The second question is how to assess algorithmperformance.
Since our production experimentdoes not yield a single gold standard (GS), an al-gorithm might match subjects better in one con-dition (e.g.
?+FaultCritical), or perform bet-ter in one domain (e.g.
furniture).
Moreover, itmight match subjects poorly overall due to sam-ple variation, while evincing a perfect match witha single individual.
Using both a by-subjects and aby-items analysis will partially control for sample131dispersion.How should we calculate the match between analgorithm and a GS?
Once again, there are twofacets to this problem.
Since we are focusing onContent Determination, each human descriptioncould be viewed as associating, with the relevanttrial, a set of properties.
Our approach will be toannotate each human description with the set of at-tributes it contains.
However, the real data is oftenmessy.
For example, when one subject called anobject ?the non-coloured table?, and another calledit ?the grey desk?, both may be expressing the sameattributes (i.e.
TYPE and COLOUR).
Also, while itis often assumed that the output of GRE is a def-inite noun phrase, this is not always the case inour corpus, which contains indefinite distinguish-ing descriptions such as ?a red chair, facing tothe right?, and telegraphic messages such as ?red,right-facing?.The second aspect to the problem concerns theactual human-algorithm comparison.
Suppose theGS equals the output of one subject, and we arecomparing two algorithms, x and y.
Suppose oursubject produced ?the two huge red sofas?, whichthe GS associates with the set {sofa, red, large}.Suppose our algorithms describe the target as:Output from x : {sofa, red, top}Output from y : {sofa, red, large, top}Which of these algorithms matches the GS best?Algorithm y adds a property (perhaps overspecify-ing even more than the GS).
Algorithm x has thesame length as the GS, but replaces one propertyby another.
Several reasonable ways of assess-ing the differences can be devised, one of which isLevenshtein distance (which suggests preferring yover x, since the latter involves a deletion and anaddition) (Levenshtein, 1966).
We also intend toexamine how often the GS over- or underspecifieswhere the algorithm does not.4 ConclusionCorpora can be an invaluable resource for NLGas long as the necessary contextual informationand the conditions under which the texts in a cor-pus were produced are known.
We believe thatcontrolled and balanced experiments are neededfor building semantically transparent resources,whose construction we have discussed.
As shownin this paper, evaluation of algorithms against thenumber of gold standards obtained with such acorpus needs careful consideration.Evaluation of GRE ?
and NLG systems moregenerally ?
would benefit from more investiga-tion of the differences between readers and pro-ducers.
In future work, we intend to follow upwith a reader-oriented experiment in which we testthe speed and/or accuracy with which the outputof different GRE algorithms is understood by sub-jects.
The dependent variables here will be non-linguistic (perhaps involving subjects clicking onpictures of presumed target referents).
This illus-trates a more general issue in this area, namelythat corpora should, in our view, only be a start-ing point, with which data of different kinds canbe associated.5 AcknowledgmentsThanks to Ehud Reiter, Richard Powerand Emiel Krahmer for useful comments.This work is part of the TUNA project(http://www.csd.abdn.ac.uk/research/tuna/), funded by the EPSRCin the UK (GR/S13330/01).References[Arts2004] A.
Arts.
2004.
Overspecification in Instruc-tive Texts.
Ph.D. thesis, Tilburg University.
[Dale and Reiter1995] R. Dale and E. Reiter.
1995.Computational interpretations of the Gricean max-ims in the generation of referring expressions.
Cog-nitive Science, 18:233?263.
[van Deemter2002] K. van Deemter.
2002.
Generat-ing referring expressions: Boolean extensions of theincremental algorithm.
Computational Linguistics,28(1):37?52.
[Gupta and Stent2005] S. Gupta and A. J. Stent.
2005.Automatic evaluation of referring expression gener-ation using corpora.
In Proceedings of the 1st Work-shop on Using Corpora in NLG, Birmingham, UK.
[Jordan and Walker2000] P. Jordan and M. Walker.2000.
Learning attribute selections for non-pronominal expressions.
In Proceedings of the 38thAnnual Meeting of the Association for Computa-tional Linguistics.
[Levenshtein1966] V. Levenshtein.
1966.
Binary codescapable of correcting deletions, insertions and rever-sals.
Soviet Physics Doklady, 10(8):707?710.
[Reiter1990] E. Reiter.
1990.
The computational com-plexity of avoiding conversational implicatures.
InProceedings of the 28th ACL Meeting, pages 97?104.
MIT Press.132
