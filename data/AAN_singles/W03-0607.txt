EBLA: A Perceptually Grounded Model of Language AcquisitionBrian E. PangburnThe Pangburn Company, Inc.103 Gisele StreetP.O.
Box 900New Roads, LA 70760-0900bpangburn@nqadmin.comS.
Sitharama IyengarDepartment of Computer Science298 Coates HallLouisiana State UniversityBaton Rouge, LA 70803iyengar@bit.csc.lsu.eduRobert C. MathewsDepartment of Psychology236 Audubon HallLouisiana State UniversityBaton Rouge, LA 70803psmath@lsu.eduJonathan P. AyoDepartment of Information Systemsand Decision SciencesLouisiana State University8001 Jefferson Hwy., Apt.
99Baton Rouge, LA 70809jonayo2@hotmail.comAbstractThis paper introduces an open computationalframework for visual perception and groundedlanguage acquisition called Experience-BasedLanguage Acquisition (EBLA).
EBLA can?watch?
a series of short videos and acquire asimple language of nouns and verbs corre-sponding to the objects and object-object rela-tions in those videos.
Upon acquiring thisprotolanguage, EBLA can perform basicscene analysis to generate descriptions ofnovel videos.The performance of EBLA has been evaluatedbased on accuracy and speed of protolanguageacquisition as well as on accuracy of gener-ated scene descriptions.
For a test set of sim-ple animations, EBLA had average acquisitionsuccess rates as high as 100% and average de-scription success rates as high as 96.7%.
Fora larger set of real videos, EBLA had averageacquisition success rates as high as 95.8% andaverage description success rates as high as65.3%.
The lower description success rate forthe videos is attributed to the wide variance inthe appearance of objects across the test set.While there have been several systems capa-ble of learning object or event labels for vid-eos, EBLA is the first known system toacquire both nouns and verbs using agrounded computer vision system.1 IntroductionWhile traditional, top-down research fields such as natu-ral language processing (NLP), computational linguis-tics, and speech recognition and synthesis have madegreat progress in allowing computers to process naturallanguage, they typically do not address perceptual un-derstanding.
In these fields, meaning and context for agiven word are based solely on other words and thelogical relationships among them.To make this clearer, consider the following Web-ster?s definition of apple: ?The fleshy usually roundedand red or yellow edible pome fruit of a tree of the rosefamily.?
(Webster?s 1989)  Using traditional ap-proaches, a computer might be able to determine fromsuch a definition that an apple is ?edible,?
that it is a?fruit,?
and that it is usually ?rounded and red or yel-low.?
But what does is mean to be ?rounded and red?
?People understand these words because their conceptualrepresentations are grounded in their perceptual experi-ences.
As for more abstract words, many have percep-tual analogs or can be defined in terms of groundedwords.
Although it is unlikely that any two peopleshare identical representations of a given word, there aregenerally enough similarities for that word to conveymeaning.
If computers can be enabled to ground lan-guage in perception, ultimately communication betweenman and machine may be facilitated.This paper details a new software framework, Ex-perience-Based Language Acquisition (EBLA), thatacquires a childlike language known as protolanguagein a bottom-up fashion based on visually perceived ex-periences.
EBLA uses an integrated computer visionsystem to watch short videos and to generate internalrepresentations of both the objects and the object-objectrelations in those videos.
It then performs languageacquisition by resolving these internal representations tothe individual words in protolanguage descriptions ofeach video.
Upon acquiring this grounded protolan-guage, EBLA can perform basic scene analysis to gen-erate simplistic descriptions of what it ?sees.
?EBLA operates in three primary stages:  visionprocessing, entity extraction, and lexical resolution.
Inthe vision processing stage, EBLA is presented withexperiences in the form of short videos, each containinga simple event such as a hand picking up a ball.
EBLAprocesses the individual frames in the videos to identifyand store information about significant objects.
In theentity extraction stage, EBLA aggregates the informa-tion from the video processing stage into internal repre-sentations called entities.
Entities are defined for boththe significant objects in each experience and for therelationships among those objects.
Finally, in the lexi-cal acquisition stage, EBLA attempts to acquire lan-guage for the entities extracted in the second stage usingprotolanguage descriptions of each event.
It extracts theindividual lexemes (words) from each description andthen attempts to generate entity-lexeme mappings usingan inference technique called cross-situational learning.EBLA is not primed with a base lexicon, so it faces thetask of bootstrapping its lexicon from scratch.While, to date, EBLA has only been evaluated usingshort descriptions comprised of nouns and verbs, one ofthe primary goals of this research has been to developan open system that can potentially learn any perceptu-ally grounded lexeme using a unified approach.
Theentities recognized EBLA are generic in nature and arecomprised of clusters of perceptual attributes linked in adatabase system.
Although only twelve basic attributeshave been programmed into the current system, both theEBLA software and database support the addition ofother attributes.
There are even mechanisms in the da-tabase to support dynamic loading/unloading of customattribute calculators.2 Related WorkEBLA is based on research into language acquisition inchildren as well as existing computational models.
Thissection highlights some of this related research.
For amore detailed discussion of existing works on early lan-guage acquisition in children including works by Calvinand Bickerton (2001), Lakoff (1990), Locke (1993),Norris and Hoffman (2002), Pinker (2000), and Smith(1999), see chapter 2 of Pangburn (2002).
For a moredetailed discussion of existing computational modelsincluding Steels and Kaplan (2000) and Roy (1999;2000), see chapter 3 of Pangburn (2002).2.1  Experiential Model of Child Development andLanguage AcquisitionKatherine Nelson (1998) has worked to bring togethermany of the domains involved in the cognitive devel-opment of children with special emphasis on the roleplayed by language.
She views language and cognitionas heavily intertwined?language cannot develop with-out early, nonlinguistic cognitive function, and full cog-nitive development cannot occur without language.Nelson takes an experiential approach to her work, fo-cusing on how children adapt to meet their currentneeds and how that adaptation then affects their futureexperiences.Nelson?s Experiential Model is centered on eventsin the child?s environment rather than objects.
Nelsonbroadly defines an event as ?an organized sequence ofactions through time and space that has a perceived goalor end point.?
(Nelson 1998, 93-94)  Events place ob-jects and actions on those objects in the context of theirultimate goal or purpose, adding temporal ordering witha beginning and an ending.
A child?s perception, proc-essing, classification, and storage of events form his/hermental event representations (MERs).
The MER be-comes the cognitive building block for increasinglycomplex knowledge representation and, ultimately,natural language.2.2 Cross-Situational Techniques for Lexical Ac-quisitionThroughout the 1990?s, Siskind (1992; 1997) has estab-lished algorithms to map words to symbolic representa-tions of their meanings.
For example, given theutterance, ?John walked to school.?
and a symbolic rep-resentation of the event, ?GO(John, TO(school)),?
hissystem would learn the mappings, ?John?John,walked?GO(x, y), t ?TO(x), and school?school.
?To perform the word-to-meaning mappings, Siskindutilizes cross-situational learning.
Basically, this meansthat the system resolves mappings only after being pre-sented with multiple utterance/symbolic concept setsrepresenting multiple situations.
By drawing inferencesabout word mappings from multiple uses, the system isable to determine the correct symbolic mappings.2.3 Force Dynamics and Event Logic forGrounded Event RecognitionIn distinct but related research, Siskind (1992; 2000;Siskind and Morris 1996) has developed several soft-ware systems to classify and describe dynamic events.In 1992, he described ABIGAIL, a system that con-structs semantic descriptions of events occurring incomputer-generated stick-figure animations.
ABIGAILperceives events by detecting support, contact, and at-tachment using counterfactual simulation.Using a subsequent system named HOWARD,Siskind and Morris built event representations based onreal video.
HOWARD produces hidden Markov models(HMMs) of the motion profiles of the objects involvedin an event.Siskind?s most recent approach has been to useevent-logic to describe changes in support, contact, andattachment, which he now terms force-dynamics.
Hislatest system, LEONARD, uses a camera to capture asequence of images and then processes that sequenceusing three subroutines:1.
Segmentation-and-Tracking ?
places a polygonaround the objects in each frame2.
Model-Reconstruction ?
builds a force dynamicmodel of each polygon scene, determining ground-ing, attachment, and depth/layering3.
Event-Classification ?
determines over which in-tervals various primitive event types are true andfrom that data, over which intervals various com-pound event types are true2.4 X-Schemas, F-Structs, and Model-Merging forVerb LearningBailey (1997) has developed a computational model ofthe role of motor control in verb acquisition.
He arguesthat proprioception, which is knowledge of the body?sown state, is linked to the acquisition of action verbs.
Infact, he maintains that grounding action verbs in themotor-control system constrains the variety of lexicalaction categories and makes verb acquisition tractable.Bailey introduces the executing schema (x-schema) as amechanism that can represent and carry out verbalcommands, and feature structures (f-structs) as amechanism for linking x-schema activities to relatedlinguistic features.X-schemas are formal representations of sequencesof motor control actions.
In Bailey?s model, x-schemasare modeled as Petri nets with extensions to handle thepassing of parameters.In order to connect x-schemas to verbs, the linkingfeature structure (f-struct) is introduced.
The f-struct isan intermediate set of features that allows a layer ofabstraction between the individual motions of an actionand the action verb that describes them.
An f-struct is alist of feature-value pairs represented in a table with tworows.
Each pair maps to a column with the feature lo-cated in the top row and the value in the bottom row.Bailey experientially determined a list of twelve fea-tures for his system comprised of eight motor controlfeatures and four perceived world state features.Bailey?s system performs verb acquisition using analgorithm that develops a lexicon of word senses basedon a training set of verbs and linking f-structs summa-rizing that verb.
Verb learning becomes an optimizationproblem to find the best possible lexicon given the train-ing examples.
Bailey terms this approach for mergingword senses, model-merging, and implements a solutionusing a hill-climbing algorithm.3 EBLA ModelThe EBLA Model (Pangburn 2002) operates by observ-ing a series of ?experiences?
in the form of short mov-ies.
Each movie contains a single event such as anarm/hand picking up a ball, and takes the form of eitheran animation or an actual video.
The model detects anysignificant objects in each movie and determines what,if any, relationships exist among those objects.
Thisinformation is then stored so that repeatedly occurringobjects and relations can be identified across multipleexperiences.Figure 1.
Method Used by EBLA to Process Ex-periencesAs part of each experience, EBLA receives a textualdescription of the event taking place.
These descrip-tions are comprised of protolanguage such as ?handpickup ball.?
To acquire this protolanguage, EBLAmust correlate the lexical items in the descriptions to theobjects and relations in each movie.
Figure 1 provides agraphical representation of the method used by EBLA toprocess experiences.3.1 Model Abstractions and ConstraintsThe EBLA Model has been constrained in several ways.First, the model?s perceptual capabilities are limited to atwo-dimensional vision system that reduces objects tosingle color polygons.Second, the model has not been provided with anyaudio processing capabilities.
Because of this, all ex-perience descriptions presented to or generated byEBLA are textual.Third, the model only attempts to acquire a proto-language of nouns and verbs.
Thus, syntax, word order,punctuation, etc.
do not apply.
This conforms withearly human language acquisition since children do notbegin to use phrases and clauses until somewhere be-tween eighteen and thirty-six months of age (Calvin andBickerton 2001).The final constraint on EBLA is that it only operatesin an unsupervised mode.
This means that the modeldoes not receive any sort of feedback regarding its accu-racy.
This is definitely a worst-case scenario since chil-dren receive frequent social mediation in all aspects ofdevelopment.3.2 Experiences Processed by the EBLA ModelThe experiences processed by the EBLA Model arebased on simple spatial-motion events, and take theform of either animations or real videos.
Each experi-ence contains an arm/hand performing some simpleaction on a variety of objects.
For the animations, theactions include pickup, putdown, touch, and slide, andthe objects include a green ball and a red cube (see fig-ure 2).
For the real videos, the actions include push,pull, slide, touch, tipover, roll, pickup, putdown, drop,and tilt, and the objects include several colored bowls,rings, and cups, a green ball, a dark blue box, a blueglass vase, a red book, and an orange stuffed Garfieldcat (see figure 3).hand pickup ballhand touch ballhand putdown cubeFigure 2.
Frames from Various Animations Proc-essed by EBLAAll of the videos were shot two to three times fromboth the left and right side of a makeshift stage.
Angleof approach, grasp, and speed were varied at random.Multiple actions were performed on each object, but theactual object-event combinations varied somewhatbased on what was feasible for each object.
Droppingthe glass vase, for example, seemed a bit risky.hand push vasehand roll ringhand touch garfieldhand tipover cuphand pickup ballhand pull bookFigure 3.
Frames from Various Videos Processedby EBLA3.3 Entity RecognitionThe EBLA Model has a basic perceptual system, whichallows it to ?see?
the significant objects in each of itsexperiences.
It identifies and places polygons aroundthe objects in each video frame, using a variation of themean shift analysis image segmentation algorithm (Co-maniciu 2002).
EBLA then calculates a set of staticattribute values for each object and a set of dynamicattribute values for each object-object relation.
The setsof attribute-value pairings are very similar to the linkingfeature structures (f-structs) used by Bailey (1997).Each unique set of average attribute values definesan entity, and is compared to the entities from prior ex-periences.
In order to match existing entities with thosein the current experience, the existing entity must haveaverage values for all attributes within a single standarddeviation (?)
of the averages for the current entity.When this occurs, the current entity is merged with theexisting entity, creating a more prototypical entity defi-nition.
Otherwise, a new entity definition is established.To prevent entity definitions from becoming too nar-rowly defined, a minimum standard deviation (?min) isestablished as a percentage of each average attributevalue.
In essence, ?min defines how much two entitiesmust differ to be considered distinct, and thus can havea significant impact on the number of unique entitiesrecognized by EBLA.Both the object and relation attributes for EBLAwere determined experimentally based on data availablefrom the computer vision system.
To aid in the debug-ging and evaluation of EBLA as well as to restrict anyassumptions about early perception in children, an effortwas made to keep the attributes as simple as possible.The five object attributes and seven relation attributescalculated by EBLA are briefly described in table 1.Entity Type Descriptionarea object area (in pixels) of a given objectgrayscalevalueobject grayscale color of  object (0-255)number ofedgesobject number of edges on polygon tracingobjectrelativecentroid (x)object horizontal coordinate of object?s cen-ter of gravity relative to the width of abounding rectangle around the objectrelativecentroid (y)object vertical coordinate of object?s centerof gravity relative to the height of abounding rectangle around the objectcontact relation Boolean value indicating if two objectsare in contact with one anotherx-relation relation indicates whether one object is to theleft of, on top of, or to the right ofanother objecty-relation relation indicates whether one object is above,on top of, or below another objectdelta-x relation indicates whether the horizontal dis-tance between two objects is increas-ing, decreasing, or unchangeddelta-y relation indicates whether the vertical distancebetween two objects is increasing,decreasing, or unchangedx-travel relation indicates direction of horizontal travelfor both objectsy-travel relation indicates direction of vertical travel forboth objectsTable 1.
Entity Attributes Calculated by EBLABecause average attribute values are used to defineentities, temporal ordering is not explicitly stored inEBLA.
Rather, the selected relation attributes implicitlyindicate how objects interact over time.
For example,EBLA is able to distinguish between pickup and put-down entities using the average ?delta-y?
attributevalue?for pickup, the vertical distance between the twoobjects involved is decreasing over the experience andfor putdown, the vertical distance is increasing.Currently, object entities are defined using all of theobject attributes, and relation entities are defined usingall of the relation attributes.
There is no mechanism todrop attributes that may not be relevant to a particularentity.
For example, grayscale color value may nothave anything to do with whether or not an object is aball, but EBLA would likely create separate entities fora light-colored ball and a dark-colored ball.A variation of the model-merging algorithm em-ployed by Bailey (1997) could be applied to drop attrib-utes unrelated to the essence of a particular entity.Because EBLA currently uses a limited number of at-tributes, dropping any would likely lead to overgener-alization of entities, but with more attributes, it could bea very useful mechanism.
Such a mechanism wouldalso improve EBLA?s viewpoint invariance.
For exam-ple, when detecting a putdown object-object relation,EBLA is not affected by small to moderate changes inangle, distance, or objects involved, but is affected bythe horizontal orientation.
Dropping the ?x-relation?and ?x-travel?
attributes from the putdown entity wouldremedy this.Work is underway to determine how to incorporate a3D graphics engine into EBLA in order to build a morerobust perceptual system.
While this would obviouslylimit the realism, it would allow for the quick additionof attributes for size, volume, distance, texture, speed,acceleration, etc.
Another option is to develop new at-tribute calculators for the current vision system such asthose employed by Siskind (2000) to determine forcedynamic properties.3.4 Lexical AcquisitionOnce EBLA has generated entities for the objects andobject-object relations in each experience, its final taskis to map those entities to the lexemes (words) in proto-language descriptions of each experience.
Protolan-guage was chosen because it is the first type of languageacquired by children.
The particular variety of proto-language used for the EBLA?s experience descriptionshas the following characteristics:1.
Word order is not important, although the descrip-tions provided to EBLA are generally in the format:subject-manipulation-object (e.g.
?hand touchball?).2.
Verbs paired with particles are combined into asingle word (e.g.
?pick up?
becomes ?pickup?).3.
Words are not case-sensitive (although there is anoption in EBLA to change this).4.
Articles (e.g.
?a,?
?an,?
?the?)
can be added to de-scriptions, but are generally uninterpretable byEBLA.It should be noted that EBLA is not explicitly coded toignore articles, but since they are referentially ambigu-ous when considered as individual, unordered lexemes,EBLA is unable to map them to entities.
Adding arti-cles to the protolanguage descriptions generally slowsdown EBLA?s average acquisition speed.In order to map the individual lexemes in the proto-language descriptions to the entities in each experience,EBLA must overcome referential ambiguity.
This isbecause EBLA operates in a bottom-up fashion and isnot primed with any information about specific entitiesor lexemes.
If the first experience encountered byEBLA is a hand sliding a box with the description ?handslide box,?
it has no idea whether the lexeme ?hand?refers to the hand object entity, the box object entity, orthe slide relation entity.
This same referential ambigu-ity exists for the ?slide?
and ?box?
lexemes.
EBLA canonly overcome this ambiguity by comparing and con-trasting the current experience with future experiences.This process of resolving entity-lexeme mappings is avariation of the cross-situational learning employed bySiskind  (1992; 1997).For each experience, two lists are created to hold allof the unresolved entities and lexemes.
EBLA attemptsto establish the correct mappings for these lists in threestages:1.
Lookup any known resolutions from prior experi-ences.2.
Resolve any single remaining entity-lexeme pair-ings.3.
Apply cross-situational learning, comparing unre-solved entities and lexemes across all prior experi-ences, repeating stage two after each newresolution.To perform the first stage of lexical resolution,EBLA reviews known entity-lexeme mappings fromprior experiences.
If any match both an entity and lex-eme in the current experience, those pairings are re-moved from the unresolved entity and lexeme lists.The second stage operates on a simple process ofelimination principal.
If at any point during the resolu-tion process both the unresolved entity and lexeme listscontain only a single entry, it is assumed that those en-tries map to one another.
In addition, prior experiencesare searched for the same entity-lexeme pairing andresolved if found.
Since resolving mappings in priorexperiences can generate additional instances of singleunmapped pairings, the entire second stage is repeateduntil no new resolutions are made.The third and final stage of resolution is by far themost complex and involves a type of cross-situationalinference.
Basically, by comparing the unresolved enti-ties and lexemes across all experiences in a pair wisefashion, EBLA can infer new mappings.
If the cardinal-ity of the intersection or difference between the un-mapped entities and lexemes for a pair of experiences isone, then that intersection or difference defines a map-ping.
In more formal terms:1.
Let i and j be any two experiences, i ?
j.2.
Let Ei and Ej ?
unmapped entities for i and jrespectively.3.
Let Li and Lj ?
unmapped lexemes for i and j re-spectively.4.
If |{Ei ?
Ej}| = 1 and |{Li ?
Lj}| = 1 then {Ei ?
Ej}maps to {Li ?
Lj}.5.
If |{Ei \ Ej}| = 1 and |{Li \ Lj}| = 1 then {Ei \ Ej}maps to {Li \ Lj}.6.
If |{Ej \ Ei}| = 1 and |{Lj \ Li}| = 1 then {Ej \ Ei}maps to {Lj \ Li}.To demonstrate how all three stages work together,consider the following example.
If the model was ex-posed to an experience of a hand picking up a ball withthe description ?hand pickup ball?
followed by an ex-perience of a hand picking up a box with the description?hand pickup box,?
it could take the set differences dis-cussed in stage three for the two experiences to resolvethe ?ball?
lexeme to the ball entity and the ?box?
lex-eme to the box entity.
Assuming that these were theonly two experiences presented to the model, it wouldnot be able to resolve ?hand?
or ?pickup?
to the corre-sponding entities because of referential ambiguity.
Ifthe model was then exposed to a third experience of ahand putting down a ball with the description ?handputdown ball,?
it could resolve all of the remainingmappings for all three experiences.
Using the techniquediscussed in stage one, it could resolve ?ball?
based onknown mappings from the prior experiences.
It couldthen take the set intersection with the unmapped itemsin either of the first two experiences to resolve ?hand.
?This would leave a single unmapped pairing in each ofthe three experiences, which could be resolved using theprocess of elimination discussed in stage two.
Note thattaking the set difference rather than the intersection be-tween the third and first or second experiences wouldhave worked equally well to resolve ?hand pickup?
and?hand putdown.
?4 EvaluationEBLA was evaluated using three criteria.
First, overallsuccess was measured by comparing the number of cor-rect entity-lexeme mappings to the total number of enti-ties detected.
Second, acquisition speed was measuredby comparing the average number of experiencesneeded to resolve a word in comparison to the totalnumber of experiences processed.
Third, descriptiveaccuracy was measured by presenting EBLA with new,unlabeled experiences, and determining its ability togenerate protolanguage descriptions based on prior ex-periences.The test sets for EBLA were comprised of eightsimple animations created using Macromedia Flash, and319 short digital videos.
While the results for the an-imations were somewhat better than those for the vid-eos, only the results for the larger and more complexvideo test set will be presented here.frame 9frame 26frame 35Figure 4.
Polygon Traces from a Single VideoDemonstrating Normal Segmentation, Underseg-mentation, and OversegmentationOf the 319 videos, 226 were delivered to EBLA forevaluating lexical acquisition accuracy and speed and167 were delivered to EBLA for evaluating descriptiveaccuracy.
Videos were removed from the full set of 319because of problems with over and undersegmentationin the vision processing system.
Figure 4 demonstratesthe types of problems encountered by EBLA?s visionsystem.
It shows the polygon tracings for three framesfrom a single video shot with the Garfield toy.
Theframe on the left was correctly segmented, the frame inthe middle was undersegmented where the hand hasbeen merged into the background and essentially disap-peared, and the frame on the right was oversegmentedwhere the Garfield toy has been split into two objects.05101520250 50 100 150 200 250Number of Experiences ProcessedAvg.
ResolutionTimeFigure 6.
Average Lexical Acquisition Time forVideosTo measure acquisition speed and accuracy, the 226videos were delivered to EBLA at random, ten times foreach of nineteen different minimum standard deviation(?min) values.
The value of ?min used to match the at-tribute values to existing entities was varied from 5% to95% in increments of 5%.Figure 5 shows the success rates for lexeme map-pings for each of the nineteen ?min values.
For ?min val-ues of 5% and 10%, the acquisition success was only76% and 85% respectively.
This can be attributed to theamount of variation in the entities for the videos.
Astricter matching criteria results in more unmatchedentities.
For all of the other ?min values the acquisitionsuccess rate was better than 90% and as high as 95.8%for a ?min value of 45%.
?min % Correct % Incorrect % Unknown5 50.33 9.00 40.6710 57.22 14.11 28.6715 65.33 16.00 18.6720 56.07 25.27 18.6725 57.44 27.89 14.6730 62.94 27.73 9.3335 59.30 35.03 5.6740 63.14 30.52 6.3345 60.95 34.05 5.0050 50.83 41.17 8.0055 55.04 40.62 4.3360 48.39 45.94 5.6765 46.21 49.46 4.3370 49.96 45.38 4.6775 43.63 53.03 3.3380 44.42 50.91 4.6785 46.45 50.55 3.0090 45.04 52.62 2.3395 39.51 54.49 6.007580859095100% o fLexemesM apped5 15 25 35 45 55 65 75 85 95M atching C riteria  (% deviat io nfro m avg.
at tribute value)Figure 5.
Lexeme Mapping Success Rates for Dif-ferent Minimum Standard DeviationsTable 2.
Accuracy of Video DescriptionsFor the lower values of ?min, there were very fewincorrect descriptions, but many entities did not map toa known lexeme.
As ?min was increased, the situationreversed with almost every entity mapping to some lex-eme, but many to the wrong lexeme.
The most accuratedescriptions were produced for a ?min value of 15%where just over 65% of the entities were described cor-rectly.
These are reasonably good results consideringthe amount that any given entity varied from video tovideo, especially the object-object relation entities.
Fora full discussion of both the animation and video resultsfor EBLA see chapter 6 of Pangburn (2002).Figure 6 displays the average acquisition speed forthe videos.
It indicates that for the first few videos, ittook an average of over twenty experiences to resolveall of the entity-lexeme mappings.
After about seventy-five experiences had been processed, this averagedropped to about five experiences, and after about 150experiences, the average fell below one.To evaluate the descriptive accuracy of EBLA, 157of the 167 best videos were randomly processed in ac-quisition mode and the remaining ten were processed indescription mode.
This scenario was run ten times foreach of the same nineteen ?min values used to evaluateacquisition success.
The results are shown in table 2.
Itis important to note that for a given ?min value, EBLAoften returned multiple ?matching?
lexemes.
When thishappened, both the correct and incorrect lexemes werescored pro-rata.5 ConclusionWhile there have been several systems capable of learn-ing object or event labels for videos, EBLA is the firstknown system to acquire both nouns and verbs using agrounded computer vision system.
In addition, becauseEBLA operates in an online fashion, it does not requirean explicit training phase.EBLA performed very well on the entity-lexememapping task for both the animations and the videos,achieving success rates as high as 100% and 95.8% re-spectively.
EBLA was also able to generate descrip-tions for the animations and videos with averageaccuracies as high as 96.7% and 65.3%.
The 65.3% isstill quite good when compared to the approximately15% average success rate obtained by generating threeword descriptions at random from the pool of nineteenlexemes processed by EBLA.While the initial results from the EBLA system areencouraging, much development and evaluation remainsto be done.
Adding new attribute calculators along witha mechanism for dropping extraneous attributes wouldlikely make EBLA?s entity definitions more robust andfacilitate the acquisition of additional nouns and verbsas well as other parts of speech.
Since there is nothingin the design of EBLA that prevents it from processingvideos with more than three entities/lexemes, it shouldbe thoroughly tested using more complex experiencesand/or descriptionsAs mentioned in the introduction, one of the primarygoals of EBLA has been to develop an open system thatwould be relatively easy for others to use and extend.To that end, EBLA was written entirely in Java with aPostgreSQL relational database for storage of all ex-perience parameters, intermediate results, attribute defi-nitions and values, lexemes, entity definitions, andentity-lexeme mappings.
EBLA has been released onSourceForge at http://sourceforge.net/projects/ebla/.For more information on EBLA, visithttp://www.greatmindsworking.comReferencesBrian E. Pangburn.
2002.
Experience-Based LanguageAcquisition: A Computational Model of HumanLanguage Acquisition, Ph.D. thesis, Louisiana StateUniversity, LA.David R. Bailey.
1997.
When Push Comes to Shove: AComputational Model of the Role of Motor Controlin the Acquisition of Action Verbs, Ph.D. thesis,University of California, Berkeley, CA.Deb Kumar Roy.
1999.
Learning Words from Sightsand Sounds: A Computational Model.
Ph.D. thesis,Massachusetts Institute of Technology.Deb Kumar Roy.
2000.
Learning Visually GroundedWords and Syntax of Natural Spoken Language.
InEvolution of Communication Journal 4, no.
1 (April):33-57.Dorin Comaniciu.
2002.
Mean Shift: A Robust Ap-proach Toward Feature Space Analysis.
IEEETransactions on Pattern Analysis and Machine Intel-ligence 24, no.
5 (May):  603-619.Emily Smith.
1999.
The Performance of Prekindergar-ten Children on Representational Tasks Across Lev-els of Displacement.
Ph.D. thesis, Louisiana StateUniversity.George Lakoff.
1990.
Women, Fire, and DangerousThings:  What Categories Reveal about the Mind.Chicago, IL:  The University of Chicago Press.Janet A. Norris and Paul R. Hoffman.
2002.
LanguageDevelopment and Late Talkers:  A ConnectionistPerspective.
In Connectionist Approaches to ClinicalProblems in Speech and Language:  Therapeutic andScientific Applications, ed.
Raymond G. Daniloff, 1-109.
Mahwah, NJ:  Lawrence Erlbaum Associates,Inc.Jeffrey M. Siskind.
1992.
Na?ve Physics, Event Percep-tion, Lexical Semantics, and Language Acquisition,Ph.D.
thesis, Massachusetts Institute of Technology,Cambridge, MA.Jeffrey M. Siskind.
1997.
A Computational Study ofCross-Situational Techniques for Learning Word-to-Meaning Mappings.
In Computational Approachesto Language Acquisition, ed.
Michael Brent, 39-91.Amsterdam, Netherlands:  Elsevier Science Publish-ers.Jeffrey M. Siskind.
2000.
Visual Event Classificationvia Force Dynamics.
Proceedings of the SeventeenthNational Conference on Artificial Intelligence, AAAIPress, Menlo Park, CA.Jeffrey Mark Siskind and Quaid Morris.
1996.
AMaximum-Likelihood Approach to Visual EventClassification.
In Proceedings of the Fourth Euro-pean Conference on Computer Vision (ECCV ?96)Vol.
2, 347-360.
New York, NY:  Springer-Verlag.John L. Locke.
1993.
The Child?s Path to Spoken Lan-guage.
Cambridge, MA:  Harvard University Press.Katherine Nelson.
1998.
Language in Cognitive De-velopment:  The Emergence of the Mediated Mind.Cambridge, UK:  Cambridge University Press.Luc Steels and Frederic Kaplan.
2000.
AIBO?s FirstWords:  The Social Learning of Language and Mean-ing.
In Evolution of Communication Journal 4, no.
1(April): 3-32.Steven Pinker.
2000.
The Language Instinct:  How theMind Creates Language.
New York, NY:  WilliamMorrow and Company.Webster?s Ninth New Collegiate Dictionary.
1989.
s.v.?apple.
?William H. Calvin and Derek Bickerton.
2001.
Linguaex Machina:  Reconciling Darwin and Chomsky withthe Human Brain.
Cambridge, MA: MIT Press.
