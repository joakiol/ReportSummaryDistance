Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 269?278,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsIterative Constrained Clustering for Subjectivity Word SenseDisambiguationCem Akkaya, Janyce WiebeUniversity of PittsburghPittsburgh PA, 15260, USA{cem,wiebe}@cs.pitt.eduRada MihalceaUniversity of North TexasDenton TX, 76207, USArada@cs.unt.eduAbstractSubjectivity word sense disambiguation(SWSD) is a supervised and application-specific word sense disambiguation taskdisambiguating between subjective andobjective senses of a word.
Not sur-prisingly, SWSD suffers from the knowl-edge acquisition bottleneck.
In this work,we use a ?cluster and label?
strategy togenerate labeled data for SWSD semi-automatically.
We define a new algo-rithm called Iterative Constrained Cluster-ing (ICC) to improve the clustering purityand, as a result, the quality of the gener-ated data.
Our experiments show that theSWSD classifiers trained on the ICC gen-erated data by requiring only 59% of thelabels can achieve the same performanceas the classifiers trained on the full dataset.1 IntroductionSubjectivity lexicons (e.g., (Turney, 2002;Whitelaw et al., 2005; Riloff and Wiebe, 2003; Yuand Hatzivassiloglou, 2003; Kim and Hovy, 2004;Bloom et al., 2007; Andreevskaia and Bergler,2008; Agarwal et al., 2009)) play an importantrole in opinion, sentiment, and subjectivityanalysis.
These systems typically look for thepresence of clues in text.
Recently, in (Akkayaet al., 2009), we showed that subjectivity cluesare fairly ambiguous as to whether they expresssubjectivity or not ?
words in such lexicons mayhave both subjective and objective usages.
Wecall this problem subjectivity sense ambiguity.Consider the following sentence containing theclue ?attack?
:(1) He was attacked by Milosevic for at-tempting to carve out a new party from theSocialists.Knowing that ?attack?
is a subjectivity clue withnegative polarity will help a system recognize thenegative sentiment in the sentence.
But for (2), thesame information is simply misleading, becausethe clue is used with an objective meaning.
(2) A new treatment based on training T-cellsto attack cancerous cells ...Any opinion analysis system which relies on asubjectivity lexicon will be misled by subjectiv-ity clues used with objective senses (false hits).In (Akkaya et al., 2009), we introduced the task,Subjectivity Word Sense Disambiguation, which isto automatically determine which word instancesin a corpus are being used with subjective senses,and which are being used with objective senses.SWSD can be considered as a coarse-grainedand application-specific word sense disambigua-tion task.
We showed that sense subjectivity in-formation about clues can be fed to subjectiv-ity and sentiment analysis resulting in substantialimprovement for both subjectivity and sentimentanalysis by avoiding false hits.Although SWSD is a promising tool, it suf-fers from the knowledge acquisition bottleneck.SWSD is defined as a supervised task, and fol-lows a targeted approach common in the WSD lit-erature for performance reasons.
This means, foreach target clue, a different classifier is trained re-quiring separate training data for each target clue.It is expensive and time-consuming to obtain an-notated datasets to train SWSD classifiers limit-ing scalability.
As a countermeasure, in (Akkayaet al., 2011), we showed that non-expert annota-tions collected through Amazon Mechanical Turk(MTurk) can replace expert annotations success-fully and might be used to apply SWSD on a largescale.Although non-expert annotations are cheap andfast, they still incur some cost.
In this work, weaim to reduce the human annotation effort needed269to generate the same amount of subjectivity sensetagged data by using a ?cluster and label?
strategy.We hypothesize that we can obtain large sets oflabeled data by labelling clusters of instances of atarget word instead of single instances.The main contribution of this work is a novelconstrained clustering algorithm called IterativeConstrained Clustering (ICC) utilizing an activeconstraint selection strategy.
A secondary con-tribution is a mixed word representation that is acombination of previously proposed context rep-resentations.
We show that a ?cluster and label?strategy relying on these two proposed compo-nents generates training data of good purity.
Theresulting data has sufficient purity to train reli-able SWSD classifiers.
SWSD classifiers trainedon only 59% of the data achieve the same perfor-mance as classifiers trained on 100% of the data,resulting in a significant reduction in the annota-tion effort.
Our results take SWSD another stepcloser to large scale application.2 Cluster and LabelOur approach is inspired by a method lexicogra-phers commonly employ to create sense invento-ries, where they create inventories based on ev-idence found in corpora.
They use concordanceinformation to mine frequent usage patterns.
(Kil-garriff, 1997) describes this process in detail.
Alexicographer collects usages of a word in cor-pora and groups them into coherent sets.
The in-stances in a set should have more in common witheach other than with the instances in other sets,according to the criteria the lexicographer consid-ers.
After generating the sets, the lexicographercodes each set as a dictionary definition based onthe common attributes of the instances.
Our goalis similar.
Instead of generating dictionary defini-tions, we are only interested in generating coher-ent sets of usages of a word, so that we can labeleach induced set ?
with its instances ?
to obtainlabeled data for SWSD.
Our high-level groupingcriterion is that the instances in a cluster should besimilar subjective (objective) usages of the word.Training data for an SWSD classifier consistsof instances of the target word tagged as havinga subjective sense (S) or an objective sense (O)(subjectivity sense tagged data).
We train a dif-ferent SWSD classifier for each target word as in(Akkaya et al., 2009).
Thus, we need a differenttraining dataset for each target word.
Our ultimategoal is to reduce the human annotation effort re-quired to create training data for SWSD classifiers.For this purpose, we utilize a ?cluster and label?strategy relying on context clustering.
Each in-stance of a word is represented as a feature vector(i.e., a context vector).
The annotation process hasthe following steps: (1) cluster the context vectorsof word instances, (2) label the induced clustersas S or O, (3) propagate the given label to all in-stances in a cluster.The induced clusters represent different usagepatterns of a word.
Thus, we build more than twoclusters, even though SWSD is a binary task.
Thisimplies that two different instances of a word canboth be subjective, but end up in different clusters,if they are different usages of the word.Since we are labelling clusters as a whole, wewill introduce noise in the labeled data.
Thus, indeveloping the clustering process, we need to min-imize that noise and find as pure clusters as possi-ble.The first step is to define the context representa-tion of the instances.
This is addressed in Section3.
Then, we turn in Section 4.2 to the clusteringprocess itself.To evaluate our ?cluster and label?
strategy, weuse two gold standard subjectivity sense taggeddatasets.1.
The first one is called senSWSD gen-erated in (Akkaya et al., 2009) and the secondone is called mturkSWSD generated in (Akkayaet al., 2011).
They consist of subjectivity sensetagged data for disjoint sets of 39 and 90 words,respectively.
In this paper, we opt to use thesmaller dataset senSWSD as our development set,on which we evaluate various context representa-tions (in Section 3) and our proposed constrainedclustering algorithm (in Section 4.2).
Then, onmturkSWSD, we evaluate the quality of semi-automatically generated data for SWSD classifi-cation (in Section 4.3.2).3 Context RepresentationsThere has been much work on context representa-tions of words for various NLP tasks.
Clusteringword instances in order to discriminate senses ofa word is called Word Sense Discrimination.
Con-text representations for this task rely on two maintypes of models: distributional semantic models(DSM) and feature-based models.1Available at http://mpqa.cs.pitt.edu/corpora270(Schutze, 1998), which is still a competi-tive model for word-sense discrimination by con-text clustering, relies on a distributional semanticmodel (DSM) (Turney and Pantel, 2010; Sahlgren,2006; Bullinaria and Levy, 2007).
A DSM is usu-ally a word-to-word co-occurrence matrix ?
alsocalled semantic space ?
such that each row repre-sents the distribution of a target word in a largetext corpus.
Each row gives the semantic sig-nature of a word, which is basically a high di-mensional numeric vector.
Note that this high di-mensional vector represents word types, not wordtokens.
Thus, it cannot model a word instancein context.
For token-based treatment, (Schutze,1998) utilizes a second-order representation by av-eraging co-occurrence vectors of the words (cor-responding to rows of the co-occurrence matrix)that occur in that particular context.
It is impor-tant to note that (Schutze, 1998) uses an addi-tive model for compositional representation.
Re-cently, in (Akkaya et al., 2012), we found that aDSM built using multiplicative composition ?
pro-posed by (Mitchell and Lapata, 2010) for a differ-ent task ?
gives better performance than the modeldescribed by (Schutze, 1998).We test both methods in this paper, using thesame semantic space.
The space is built from acorpus consisting of 120 million tokens.
The rowsof the space correspond to word forms and thecolumns correspond to word lemmas present in thecorpus.
We adopt the parameters for our semanticspace from (Mitchell and Lapata, 2010): windowsize of 10 and dimension size of 2000 (i.e., the2000 most frequent lemmas).
We do not filter outstop words, since they have been shown to be use-ful for various semantic similarity tasks in (Bulli-naria and Levy, 2007).
We use positive point-wisemutual information to compute values of the vec-tor components, which has also been shown to befavourable in (Bullinaria and Levy, 2007).Purandere and Pedersen is the prominent repre-sentative of feature-based models.
(Purandare andPedersen, 2004) creates context vectors from localfeature representations similar to the feature vec-tors found in supervised WSD.
In this work, weuse the following features from (Mihalcea, 2002)to build the local feature representation: (1) thetarget word itself and its part of speech, (2) sur-rounding context of 3 words and their part ofspeech, (3) the head of the noun phrase, (4) thefirst noun and verb before the target word, (5) thefirst noun and verb after the target word.skew local dsm add dsm mul mix repaverage 79.90 80.50 80.50 83.53 85.23appear-v 53.83 54.85 54.85 57.40 69.39fine-a 70.07 72.26 70.07 74.45 75.18interest-n 54.41 54.78 55.88 81.62 81.62restraint-n 70.45 71.97 75.00 71.21 81.82Table 1: Evaluation of Various Context Representations3.1 Evaluation of Context RepresentationsIn this section, we evaluate context representationsfor the context clustering task on the subjectivitysense tagged data, senSWSD.
The evaluation isdone separately for each word.We use the same clustering algorithm for allcontext representations: agglomerative hierarchi-cal clustering with average linkage criteria.
In allour experiments throughout the paper, we fix thecluster size to 7 as it is done in (Purandare andPedersen, 2004).
We think that is reasonable num-ber since SENSEVAL III reports that the averagenumber of senses per word is 6.47.
We choosecluster purity as our evaluation metric.
To com-pute cluster purity, we assign each cluster to asense label, which is the most frequent one in thecluster.
The number of the correctly assigned in-stances divided by the number of all the clusteredinstances gives us cluster purity.Row 1 of Table 1 holds the cumulative resultsover all the words in senSWSD (micro averages).The table also reports detailed results for 4 sampleselected words from senSWSD.
skew stands forthe percentage of the most frequent label.
dsm addis the representation based on (Schutze, 1998),dsm mul stands for the representation as describedin (Akkaya et al., 2012) and local features is thelocal feature representation based on (Purandareand Pedersen, 2004).
The results show that amongdsm mul, dsm add, and local features; dsm mulperforms the best.When we look at the context clustering re-sults for single words separately, we observethat the performance of different representationsvary.
There is not a single winner among allwords.
Thus, perhaps choosing one single repre-sentation for all the words is not optimal.
Hav-ing that in mind, we try merging the dsm muland local features representations.
We leave outdsm add representation, since both dsm mul anddsm add rely on the same type of semantic infor-mation (i.e., a DSM).
We hypothesize that the two271representations, one relying on a semantic spaceand the other relying on local WSD features, maycomplement each other.To merge the representations, we concatenatethe two feature vectors into one.
First, however,we normalize each vector to unit length, since theindividual vectors have different scales and wouldhave unequal contribution, otherwise.
We call thismixed representation mix rep.In Table 1, we see that, overall, mix rep per-forms better than all the other representations.
Theimprovement is statistically significant at the p <.05 level on a paired t-test.
We observe that, evenwhen mix rep does not perform the best, it is neverbad.
mix rep is the winner or ties for the winnerfor 25 out of 39 words.
This number is 13, 13, and15 for dsm add, dsm mul and local features, re-spectively.
For the words for which mix rep is notthe winner, it is, on average, 1.47 points lower thanthe winner.
This number is 4.22, 6.83, and 7.07for the others.
The results provide evidence thatmix rep is consistently good and reliable.
Thus, inour experiments, mix rep will be our choice as thecontext representation.4 Clustering ProcessWe now turn to the clustering process.
In a ?clus-ter and label?
strategy, in order to be able to labelclusters, we need to annotate some of the instancesin each cluster.
Then, we can accept the majoritylabel found in a cluster as its label.
Thus, somemanual labelling is required, preferably a smallamount.We propose to provide this small amount of an-notated data prior to clustering, and then performsemi-supervised clustering.
This way the providedlabels will guide the clustering algorithm to gener-ate the clusters that are more suitable for our endtask, namely clusters where subjective and objec-tive instances are grouped together.4.1 Constrained ClusteringConstrained clustering (Grira et al., 2004) alsoknown as semi-supervised clustering is a recentdevelopment in the clustering literature.
In addi-tion to the similarity information required by un-supervised clustering, constrained clustering re-quires pairwise constraints.
There are two typesof constraints: (1) must-link and (2) cannot-linkconstraints.
A must-link constraint dictates thattwo instances should be in the same cluster and acannot-link dictates that two instances should notbe in the same cluster.
In this work, we only con-sider cannot-links, because of the definition of ourSWSD task.
Two instances sharing the same labeldo not need to be in the same cluster, since the in-duced clusters represent different usage patterns ofa word.
For example, two instances labeled S neednot be similar to each other.
They can be differentusages, both having a subjective meaning.
On theother hand, if two instances are labeled having op-posing labels, we do not want them to be in thesame cluster.
Thus, we utilize cannot-links but notmust-links.Constraints can be obtained from domainknowledge or from available instance labels.
Inour work, constraints are generated from instancelabels.
Each instance pair with opposing labels isconsidered to be cannot-linked.There are two general strategies to incorporateconstraints into clustering.
The first is to adaptthe similarity between instances (Xing et al., 2002;Klein et al., 2002) by adjusting the underlying dis-tance metric.
The main idea is to make the dis-tance between must-linked instances ?
their neigh-bourhoods ?
smaller and the distance betweencannot-linked instances ?
their neighbourhoods ?larger.
The second strategy is modifying the clus-tering algorithm itself so that search is biased to-wards a partitioning for which the constraints hold(Wagstaff and Cardie, 2000; Basu et al., 2002;Demiriz et al., 1999).Our proposed constrained clustering method re-lies on some ideas from (Klein et al., 2002).
Thus,we explain it in more detail.
(Klein et al., 2002)utilizes agglomerative hierarchical clustering withcomplete-linkage.
The algorithm imposes con-straints by changing the distance matrix accord-ing to the given constraints.
The distances be-tween must-linked instances are set to 0.
That isnot enough by itself, since if two instances aremust-linked, other instances close to them shouldalso get closer to each other.
This means there isa need to propagate the constraints.
This is doneby calculating the shortest path between all the in-stances and updating the distance matrix accord-ingly.
To impose cannot-links, the distance be-tween two cannot-linked instances is set to somelarge number.
The complete-linkage propertyindirectly propagates the cannot-link constraints,since it will not allow two clusters to be merged ifthey contain instances that are cannot-linked.Although previous work report on average sub-272stantial improvement in the clustering purity,(Davidson et al., 2006) shows that even if theconstraints are generated from gold-standard data,some constraint sets can decrease clustering pu-rity.
The results vary significantly depending onthe specific set of constraints used.
To our knowl-edge, there have been two approaches for select-ing informative constraint sets (Basu et al., 2004;Klein et al., 2002).
The method described in(Basu et al., 2004) uses the farthest-first traversalscheme.
That strategy is not suitable in our setting,since we have only two labels.
After selectingjust one instance from both labels, this method be-comes the same as random selection.
The strategydescribed in (Klein et al., 2002) is more general.At first, the hierarchical clustering algorithm fol-lows in a unconstrained fashion until some moder-ate number of clusters are remaining.
Then, the al-gorithm starts to request constraints between rootswhenever two clusters are merged.4.2 Iterative Constrained ClusteringOur proposed algorithm is closely related to (Kleinet al., 2002).
We share the same backbone:(1) the agglomerative hierarchical clustering withcomplete-linkage and (2) the mechanism to im-pose cannot-link constraints described in Section4.1.
For our algorithm, we implement a secondmechanism for imposing constraints proposed by(Xing et al., 2002) (Section 4.2.1) and use bothmechanisms in combination.
We also propose anovel constraint selection method (Section 4.2.2).4.2.1 Imposing Constraints(Klein et al., 2002) imposes cannot-link con-straints by adjusting the distance between cannot-linked pairs heuristically and by relying on com-plete linkage for propagation.
Although this ap-proach was shown to be effective, we believe itdoes not make full use of the provided constraints.We believe that learning a new distance metric willresult in more reliable distance estimates betweenall instances.
For this purpose, we learn a Maha-lanobis distance function following the method de-scribed in (Davis et al., 2007).
(Davis et al., 2007)formulate the problem of distance metric learn-ing as minimizing the differential relative entropybetween two multivariate Gaussians under con-straints.
Note that using distance metric learningfor imposing constraints was previously proposedby (Xing et al., 2002).
(Xing et al., 2002) posemetric learning as a convex optimization problem.The reason we choose the metric learning method(Davis et al., 2007) over (Xing et al., 2002) is thatit is computationally more efficient.
(Klein et al., 2002) has a favourable property wewant to keep.
The constraints are imposed strictly,meaning that no cannot-linked instances can ap-pear in the same cluster.
I.e., they are hard con-straints.
In the case of metric learning, the con-straints are not imposed strictly.
In a new learneddistance metric, two cannot-linked instances willbe relatively distant, but there is no guarantee theywill not end up in the same cluster.
Although wethink that metric learning makes a better use ofprovided constraints, we do not want to lose thebenefit of hard constraints.
Thus, we use bothmechanisms in combination to impose constraints.We first learn a Mahalanobis distance based on theprovided constraints.
Then, we compute distancematrix and employ the mechanism proposed by(Klein et al., 2002) on the learned distance matrix.4.2.2 Active Constraint GenerationAs mentioned before, the choice of the set of con-straints affects the quality of the end clustering.
Inthis work, we define a novel method to choose in-formative instances, which we believe will havemaximum impact on the end cluster quality, whenthey are labeled and used to generate constraintsfor our task.
We use an iterative approach.
Eachiteration consists of three steps: (1) generatingclusters by the process described in Section 4.2.1imposing available constraints, (2) choosing themost informative instance, considering the clusterboundaries, and acquiring its label, (3) extendingthe available constraints with the ones we generatefrom the newly labeled instance.We consider an instance to be informative ifthere is a high probability that the knowledge ofits label may change the cluster boundaries.
Themore probable that change is, the more informa-tive is the instance.
The basic idea is that if aninstance is in a cluster holding instances of typea and it is close to another cluster holding in-stances of type b, that instance is most likely mis-clustered.
Thus, it should be queried.
Our hypoth-esis is that, in each iteration, the algorithm willchoose the most problematic ?
informative ?
in-stance that will end up changing cluster bound-aries.
This will result in each iteration in a morereliable distance metric, which in return will pro-vide more reliable estimates of problematic in-stances in future iterations.
The imposed con-273Algorithm 1 Iterative Constrained Clustering1: C = cluster(I)2: I{L}= labelprototypes(C)3: while??I{L}?
?< stop do4: Con = createconstraints(I{L})5: Matrixdist= learnmetric(I,Con)6: C = constraintedcluster(Matrixdist,Con)7: L = labelmostinformative(C)8: I{L}= I{L}?
L9: end while10: propagatelabels(I{L}, C) {C...Clusters; Con...Constraints;I...Instances; I{L}...Labeled Instances; Matrixdist...Distance Matrix}straints will move the clustering in each iterationtowards better separation of S and O instances.To define informativeness, we define a scoringfunction, which is used to score each data point onits goodness.
The lower the score, the more likelyit is that the instance is mis-clustered.
Choosingthe data point with the lowest score will likelychange clustering borders in the next iteration.Our scoring function is based on the silhouette co-efficient, a popular unsupervised cluster validationmetric to measure goodness (Tan et al., 2005) ofa cluster member.
Basically, the silhouette scoreassigns a cluster member that is close to anothercluster a lower score, and a cluster member thatis closer to the cluster center a higher score.
Thatis partly what we want.
In addition, we do notwant to penalize a cluster member that is close toanother cluster having members with the same la-bel.
For this purpose, we calculate the silhouettescore only over clusters with an opposing label(i.e., holding members with an opposing label).
Inaddition, we consider only instances labeled so farwhen computing the score.
We call this new coef-ficient silhconst.
It is computed as follows: (1) foran instance i, compute its average distance fromthe other instances in its cluster xiwhich are al-ready labeled, (2) for an instance i, compute itsaverage distance from the labeled instances of theclusters from an opposing label and take the mini-mum of these averages yi, (3) compute the silhou-ette coefficient as (yi-xi) / max(yi,xi).The silhconstcoefficient has favourable proper-ties.
First, it scores members that are close toa cluster with an opposing label lower than themembers that are close to a cluster with the samelabel.
According to our definition, these mem-bers are more informative.
Figure 1 holds a sam-ple cluster setting.
The shape of a member de-notes its label and its fill denotes whether or not ithas been queried.
In this example, silhconstscores3 12Figure 1: Behaviour of selection functionmembers 2 and 3 lower than 1.
Thus, member 1will not be selected, which is the right decision inthis example.
Both members 2 and 3 are close toclusters with an opposing label.
In this examplesilhconstscores member 3 lower, which is fartheraway from already labeled members in the clus-ter.
Thus, member 3 will be selected to be labeled.This type of behaviour results in an explorativestrategy.The active selection strategy proposed by (Kleinet al., 2002) is single pass.
Thus, it does not havethe opportunity to observe the complete clusterstructure before choosing constraints.
We hypoth-esize that our strategy will provide more informa-tive constraints, since it has the advantage of be-ing able to base the decision of which constraintsto generate on fully observed cluster structure ineach iteration.We call our proposed algorithm Iterative Con-strained Clustering (ICC).
In our final implemen-tation, ICC starts by simply clustering the in-stances without any constraints.
The algorithmqueries the label of the prototypical member ?the member closest to the cluster center ?
of eachcluster.
Then, the described iterations begin.
Al-gorithm 1 contains the complete ICC algorithm.Note that line 6 is equivalent to the algorithm of(Klein et al., 2002).4.3 ExperimentsThis section gives details on experiments to evalu-ate the purity of the semi-automatically generatedsubjectivity sense tagged data by our ?cluster andlabel?
strategy.
We carry out detailed analysis toquantify the effect of the proposed active selec-tion strategy and of metric learning on the purityof the generated data.
We compare our active se-lection strategy to random selection and also to(Klein et al., 2002).
The comparison is done onthe senSWSD dataset.
SenSWSD consists of threesubsets, SENSEVAL I,II and III.
Since we devel-274Figure 2: Label Purity ?
ICC vs. random selectionoped our active selection algorithm on the SEN-SEVAL I subset, we use only SENSEVAL II andIII subsets for comparison.
We apply ICC to eachword in the comparison set separately, and reportcumulative results for the purity of the generateddata.
We report results for different percentages ofthe queried data amount (e.g.
10% means that thealgorithm queried 10% of the data to create con-straints).
This way, we obtain a learning curve.We fix the cluster number to 7 as in the contextrepresentation experiments.4.3.1 Effect of Active Selection StrategyFigure 2 holds the comparison of ICC withsilhconstselection to a random selection baseline.?majority?
stands for majority label frequency inthe dateset.
We see that silhconstperforms betterthan the random selection.
By providing labels toonly 25% of the data, we can achieve 87.67% purefully labeled data.For comparison, we also evaluate the perfor-mance of (Klein et al., 2002) with their active con-straint selection strategy as described in Section4.1.
Note that originally (Klein et al., 2002) re-quests the constraint between two roots.
In oursetting, it requests labels of the roots and then gen-erates constraints from the obtained labels.
Sincewe have a binary task, querying labels makes moresense.
This has the advantage that more con-straints from each request are obtained.
More-over, it allows a direct comparison to our algo-rithm.
(Klein et al., 2002) does not use any metriclearning.
Thus, we run our algorithm also withoutmetric learning, in order to compare the effective-ness of both active selection strategies fairly.
InFigure 3, we see that silhconstperforms better thanthe active selection strategy described in (Klein etal., 2002).
We also see that metric learning resultsFigure 3: Label Purity ?
ICC vs. KleinFigure 4: SWSD accuracy on ICC generated datain a big improvement.
In addition, metric learn-ing results in a smoother learning curve, which isa favourable property for a real-world application.4.3.2 SWSD on semi-automatically generatedannotationsNow that we have a tool to generate training datafor SWSD, we want to evaluate it on the actualSWSD task.
We want to see if the obtained purityis enough to create reliable SWSD classifiers.
Forthis purpose, we test ICC on mturkSWSD dataset.For each word in our dataset, we conduct 10-fold cross-validation experiments.
ICC is ap-plied to training folds to label instances semi-automatically.
We train SWSD classifiers on thegenerated training fold labels and test the classi-fiers on the corresponding test fold.
We distin-guish between queried instances and propagatedlabels.
The queried instances are weighted as1 and the instances with propagated labels areweighted by their silhconstscore, since that mea-sure gives the goodness of an instance.
The scoreis defined between -1 and 1.
This score is normal-ized between 0 and 1, before it is used as a weight.SVM classifiers from the Weka package (Wittenand Frank., 2005) with its default settings are used275as in (Akkaya et al., 2011).We implement two baselines.
The first is sim-ple random sampling and the second is uncer-tainty sampling, which is an active learning (AL)method.
We use ?simple margin?
selection as de-scribed in (Tong and Koller, 2001).
It selects, ineach iteration, the instance closest to the decisionboundary of the trained SVM.
Each method is rununtil it reaches the accuracy of training fully onthe gold-standard data.
ICC reaches that bound-ary when provided only 59% of the labels in thedataset.
For uncertainty sampling and randomsampling, these values are 92% and 100%, respec-tively.
In Figure 4, we see the SWSD accuracy fordifferent queried data percentages.
?full?
standsfor training fully on gold-standard data.
We seethat training SWSD on semi-automatically labeleddata by ICC does consistently better than uncer-tainty sampling and random sampling.It is surprising to see that uncertainty samplingoverall does not do better than random sampling.We believe that it might be because of samplingbias.
During AL, as more and more labels areobtained, the training set quickly diverges fromthe underlying data distribution.
(Sch?utze et al.,2006) states that AL can explore the feature spacein such a biased way that it can end up ignoring en-tire clusters of unlabeled instances.
We think thatSWSD is highly prone for the mentioned missedcluster problem because of its unique nature.
Asmentioned, SWSD is a binary task where we dis-tinguish between subjective and objective usagesof a subjectivity word.
Although the classifica-tion is binary, the underlying usages are groupedinto multiple clusters corresponding to senses ofthe word.
It is possible that two groups of usageswhich are represented quite differently in the fea-ture space are both subjective or objective.
More-over, one usage group might be closer to a usagegroup from the opposing label than to a group withthe same label.We see that our method reduces the annotationamount by 36% in comparison to uncertainty sam-pling and by 41% in comparison to random sam-pling to reach the performance of the SWSD sys-tem trained on fully annotated data.5 Related WorkOne related line of research is constrained clus-tering also known as semi-supervised clustering(Xing et al., 2002; Wagstaff and Cardie, 2000;Grira et al., 2004; Demiriz et al., 1999).
It hasbeen applied to various datasets and tasks suchas image and document categorization.
To ourknowledge, we are the first to utilize constrainedclustering for a difficult NLP task.There have been only two previous works se-lecting constraints for constrained clustering ac-tively (Basu et al., 2004; Klein et al., 2002).
Thebiggest difference of our approach is that it is iter-ative as opposed to single pass.Active Learning (AL) (Settles, 2009; Settlesand Craven, 2008; Hwa, 2004; Tong and Koller,2001) builds another important set of related work.Our method is inspired by uncertainty sampling.We accomplish active selection in the clusteringsetting.6 ConclusionsIn this paper, we explore a ?cluster and la-bel?
strategy to reduce the human annotation ef-fort needed to generate subjectivity sense-taggeddata.
In order to keep the noise in the semi-automatically labeled data minimal, we investigatedifferent feature space types and evaluate their ex-pressiveness.
More importantly, we define a newalgorithm called iterative constrained clustering(ICC) with an active constraint selection strategy.We show that we can obtain a fairly reliable la-beled data when we utilize ICC.We show that the active selection strategywe propose outperforms a previous approach by(Klein et al., 2002) for generating subjectivitysense-tagged data.
Training SWSD classifiers onICC generated data improves over random sam-pling and uncertainty sampling (Tong and Koller,2001).
We can achieve on mturkSWSD 36% an-notation reduction over uncertainty sampling and41% annotation reduction over random samplingin order to reach the performance of SWSD clas-sifiers trained on fully annotated data.To our knowledge, this work is the first applica-tion of constrained clustering to a hard NLP prob-lem.
We showcase the power of constrained clus-tering.
We hope that the same ?cluster and label?strategy will be applicable to Word Sense Disam-biguation.
This will be part of our future work.7 AcknowledgmentsThis material is based in part upon work supportedby National Science Foundation awards #0917170and #0916046.276ReferencesApoorv Agarwal, Fadi Biadsy, and Kathleen Mckeown.2009.
Contextual phrase-level polarity analysis us-ing lexical affect scoring and syntactic N-grams.
InProceedings of the 12th Conference of the Euro-pean Chapter of the ACL (EACL 2009), pages 24?32, Athens, Greece, March.
Association for Compu-tational Linguistics.Cem Akkaya, Janyce Wiebe, and Rada Mihalcea.2009.
Subjectivity word sense disambiguation.
InProceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing, pages190?199, Singapore, August.
Association for Com-putational Linguistics.Cem Akkaya, Janyce Wiebe, Alexander Conrad, andRada Mihalcea.
2011.
Improving the impact ofsubjectivity word sense disambiguation on contex-tual opinion analysis.
In Proceedings of the Fif-teenth Conference on Computational Natural Lan-guage Learning, pages 87?96, Portland, Oregon,USA, June.
Association for Computational Linguis-tics.Cem Akkaya, Janyce Wiebe, and Rada Mihalcea.2012.
Utilizing semantic composition in distribu-tional semantic models for word sense discrimina-tion and word sense disambiguation.
In ICSC, pages45?51.Alina Andreevskaia and Sabine Bergler.
2008.
Whenspecialists and generalists work together: Over-coming domain dependence in sentiment tagging.In Proceedings of ACL-08: HLT, pages 290?298,Columbus, Ohio, June.
Association for Computa-tional Linguistics.Sugato Basu, Arindam Banerjee, and R. Mooney.2002.
Semi-supervised clustering by seeding.
InIn Proceedings of 19th International Conference onMachine Learning (ICML-2002).Sugato Basu, Arindam Banerjee, and Raymond J.Mooney.
2004.
Active semi-supervision for pair-wise constrained clustering.
In SDM.Kenneth Bloom, Navendu Garg, and Shlomo Argamon.2007.
Extracting appraisal expressions.
In HLT-NAACL 2007, pages 308?315, Rochester, NY.John Bullinaria and Joseph Levy.
2007.
Ex-tracting semantic representations from wordco-occurrence statistics: A computationalstudy.
Behavior Research Methods, 39:510?526.10.3758/BF03193020.Ian Davidson, Kiri Wagstaff, and Sugato Basu.
2006.Measuring constraint-set utility for partitional clus-tering algorithms.
In PKDD, pages 115?126.Jason V. Davis, Brian Kulis, Prateek Jain, Suvrit Sra,and Inderjit S. Dhillon.
2007.
Information-theoreticmetric learning.
In Proceedings of the 24th interna-tional conference on Machine learning, ICML ?07,pages 209?216, New York, NY, USA.
ACM.Ayhan Demiriz, Kristin Bennett, and Mark J. Em-brechts.
1999.
Semi-supervised clustering usinggenetic algorithms.
In In Artificial Neural Networksin Engineering (ANNIE-99, pages 809?814.
ASMEPress.Nizar Grira, Michel Crucianu, and Nozha Boujemaa.2004.
Unsupervised and semi-supervised cluster-ing: a brief survey.
In in A Review of Ma-chine Learning Techniques for Processing Multime-dia Content, Report of the MUSCLE European Net-work of Excellence.Rebecca Hwa.
2004.
Sample selection for statis-tical parsing.
Comput.
Linguist., 30(3):253?276,September.Adam Kilgarriff.
1997.
I dont believe in word senses.Computers and the Humanities, 31(2):91?113.Soo-Min Kim and Eduard Hovy.
2004.
Determin-ing the sentiment of opinions.
In Proceedings ofthe Twentieth International Conference on Compu-tational Linguistics (COLING 2004), pages 1267?1373, Geneva, Switzerland.D.
Klein, K. Toutanova, I.T.
Ilhan, S.D.
Kamvar, andC.
Manning.
2002.
Combining heterogeneous clas-sifiers for word-sense disambiguation.
In Proceed-ings of the ACL Workshop on ?Word Sense Dis-ambiguatuion: Recent Successes and Future Direc-tions, pages 74?80, July.R.
Mihalcea.
2002.
Instance based learning withautomatic feature selection applied to Word SenseDisambiguation.
In Proceedings of the 19th Inter-national Conference on Computational Linguistics(COLING 2002), Taipei, Taiwan, August.Jeff Mitchell and Mirella Lapata.
2010.
Compositionin distributional models of semantics.
Cognitive Sci-ence, 34(8):1388?1429.A.
Purandare and T. Pedersen.
2004.
Word sense dis-crimination by clustering contexts in vector and sim-ilarity spaces.
In Proceedings of the Conference onComputational Natural Language Learning (CoNLL2004), Boston.Ellen Riloff and Janyce Wiebe.
2003.
Learning extrac-tion patterns for subjective expressions.
In Proceed-ings of the Conference on Empirical Methods in Nat-ural Language Processing (EMNLP-2003), pages105?112, Sapporo, Japan.Magnus Sahlgren.
2006.
The Word-Space Model: us-ing distributional analysis to represent syntagmaticand paradigmatic relations between words in high-dimensional vector spaces.
Ph.D. thesis, StockholmUniversity.Hinrich Sch?utze, Emre Velipasaoglu, and Jan O. Ped-ersen.
2006.
Performance thresholding in practicaltext classification.
In Proceedings of the 15th ACMinternational conference on Information and knowl-edge management, CIKM ?06, pages 662?671, NewYork, NY, USA.
ACM.277H.
Schutze.
1998.
Automatic word sense discrimina-tion.
Computational Linguistics, 24(1):97?124.Burr Settles and Mark Craven.
2008.
An analysisof active learning strategies for sequence labelingtasks.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing,EMNLP ?08, pages 1070?1079, Stroudsburg, PA,USA.
Association for Computational Linguistics.Burr Settles.
2009.
Active Learning Literature Survey.Technical Report 1648, University of Wisconsin?Madison.Pang-Ning Tan, Michael Steinbach, and Vipin Kumar.2005.
Introduction to Data Mining, (First Edi-tion).
Addison-Wesley Longman Publishing Co.,Inc., Boston, MA, USA.Simon Tong and Daphne Koller.
2001.
Support vec-tor machine active learning with applications to textclassification.
Journal of Machine Learning Re-search, 2:45?66.Peter D. Turney and Patrick Pantel.
2010.
From fre-quency to meaning: Vector space models of seman-tics.
March.Peter Turney.
2002.
Thumbs up or thumbs down?
Se-mantic orientation applied to unsupervised classifi-cation of reviews.
In Proceedings of the 40th An-nual Meeting of the Association for ComputationalLinguistics (ACL-02), pages 417?424, Philadelphia,Pennsylvania.Kiri Wagstaff and Claire Cardie.
2000.
Clusteringwith instance-level constraints.
In Proceedings ofthe Seventeenth International Conference on Ma-chine Learning (ICML-2000), pages 1103?1110.Casey Whitelaw, Navendu Garg, and Shlomo Arga-mon.
2005.
Using appraisal taxonomies for sen-timent analysis.
In Proceedings of CIKM-05, theACM SIGIR Conference on Information and Knowl-edge Management, Bremen, DE.I.
Witten and E. Frank.
2005.
Data Mining: Practi-cal Machine Learning Tools and Techniques, SecondEdition.
Morgan Kaufmann, June.Eric P. Xing, Andrew Y. Ng, Michael I. Jordan, andStuart J. Russell.
2002.
Distance metric learningwith application to clustering with side-information.In NIPS, pages 505?512.Hong Yu and Vasileios Hatzivassiloglou.
2003.
To-wards answering opinion questions: Separating factsfrom opinions and identifying the polarity of opin-ion sentences.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing(EMNLP-2003), pages 129?136, Sapporo, Japan.278
