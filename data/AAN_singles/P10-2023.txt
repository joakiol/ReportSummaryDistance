Proceedings of the ACL 2010 Conference Short Papers, pages 126?131,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsAutomatically Generating Term-frequency-induced TaxonomiesKarin Murthy Tanveer A Faruquie L Venkata SubramaniamK Hima Prasad Mukesh MohaniaIBM Research - India{karinmur|ftanveer|lvsubram|hkaranam|mkmukesh}@in.ibm.comAbstractWe propose a novel method to automati-cally acquire a term-frequency-based tax-onomy from a corpus using an unsuper-vised method.
A term-frequency-basedtaxonomy is useful for application do-mains where the frequency with whichterms occur on their own and in combi-nation with other terms imposes a naturalterm hierarchy.
We highlight an applica-tion for our approach and demonstrate itseffectiveness and robustness in extractingknowledge from real-world data.1 IntroductionTaxonomy deduction is an important task to under-stand and manage information.
However, buildingtaxonomies manually for specific domains or datasources is time consuming and expensive.
Tech-niques to automatically deduce a taxonomy in anunsupervised manner are thus indispensable.
Au-tomatic deduction of taxonomies consist of twotasks: extracting relevant terms to represent con-cepts of the taxonomy and discovering relation-ships between concepts.
For unstructured text, theextraction of relevant terms relies on informationextraction methods (Etzioni et al, 2005).The relationship extraction task can be classi-fied into two categories.
Approaches in the firstcategory use lexical-syntactic formulation to de-fine patterns, either manually (Kozareva et al,2008) or automatically (Girju et al, 2006), andapply those patterns to mine instances of the pat-terns.
Though producing accurate results, theseapproaches usually have low coverage for manydomains and suffer from the problem of incon-sistency between terms when connecting the in-stances as chains to form a taxonomy.
The secondcategory of approaches uses clustering to discoverterms and the relationships between them (Royand Subramaniam, 2006), even if those relation-ships do not explicitly appear in the text.
Thoughthese methods tackle inconsistency by addressingtaxonomy deduction globally, the relationships ex-tracted are often difficult to interpret by humans.We show that for certain domains, the frequencywith which terms appear in a corpus on their ownand in conjunction with other terms induces a nat-ural taxonomy.
We formally define the conceptof a term-frequency-based taxonomy and showits applicability for an example application.
Wepresent an unsupervised method to generate sucha taxonomy from scratch and outline how domain-specific constraints can easily be integrated intothe generation process.
An advantage of the newmethod is that it can also be used to extend an ex-isting taxonomy.We evaluated our method on a large corpus ofreal-life addresses.
For addresses from emerginggeographies no standard postal address schemeexists and our objective was to produce a postaltaxonomy that is useful in standardizing addresses(Kothari et al, 2010).
Specifically, the experi-ments were designed to investigate the effective-ness of our approach on noisy terms with lots ofvariations.
The results show that our method isable to induce a taxonomy without using any kindof lexical-semantic patterns.2 Related WorkOne approach for taxonomy deduction is to useexplicit expressions (Iwaska et al, 2000) or lexi-cal and semantic patterns such as is a (Snow et al,2004), similar usage (Kozareva et al, 2008), syn-onyms and antonyms (Lin et al, 2003), purpose(Cimiano and Wenderoth, 2007), and employed by(Bunescu and Mooney, 2007) to extract and orga-nize terms.
The quality of extraction is often con-trolled using statistical measures (Pantel and Pen-nacchiotti, 2006) and external resources such aswordnet (Girju et al, 2006).
However, there are126domains (such as the one introduced in Section3.2) where the text does not allow the derivationof linguistic relations.Supervised methods for taxonomy inductionprovide training instances with global seman-tic information about concepts (Fleischman andHovy, 2002) and use bootstrapping to induce newseeds to extract further patterns (Cimiano et al,2005).
Semi-supervised approaches start withknown terms belonging to a category, constructcontext vectors of classified terms, and associatecategories to previously unclassified terms de-pending on the similarity of their context (Tanevand Magnini, 2006).
However, providing train-ing data and hand-crafted patterns can be tedious.Moreover in some domains (such as the one pre-sented in Section 3.2) it is not possible to constructa context vector or determine the replacement fit.Unsupervised methods use clustering of word-context vectors (Lin, 1998), co-occurrence (Yangand Callan, 2008), and conjunction features (Cara-ballo, 1999) to discover implicit relationships.However, these approaches do not perform wellfor small corpora.
Also, it is difficult to label theobtained clusters which poses challenges for eval-uation.
To avoid these problems, incremental clus-tering approaches have been proposed (Yang andCallan, 2009).
Recently, lexical entailment hasbeen used where the term is assigned to a cate-gory if its occurrence in the corpus can be replacedby the lexicalization of the category (Giuliano andGliozzo, 2008).
In our method, terms are incre-mentally added to the taxonomy based on theirsupport and context.Association rule mining (Agrawal and Srikant,1994) discovers interesting relations betweenterms, based on the frequency with which termsappear together.
However, the amount of patternsgenerated is often huge and constructing a tax-onomy from all the patterns can be challenging.In our approach, we employ similar concepts butmake taxonomy construction part of the relation-ship discovery process.3 Term-frequency-induced TaxonomiesFor some application domains, a taxonomy is in-duced by the frequency in which terms appear in acorpus on their own and in combination with otherterms.
We first introduce the problem formally andthen motivate it with an example application.Figure 1: Part of an address taxonomy3.1 DefinitionLet C be a corpus of records r. Each record isrepresented as a set of terms t. Let T = {t | t ?r ?
r ?
C} be the set of all terms of C. Let f(t)denote the frequency of term t, that is the numberof records in C that contain t. Let F (t, T+, T?
)denote the frequency of term t given a set of must-also-appear terms T+ and a set of cannot-also-appear terms T?.
F (t, T+, T?)
= | {r ?
C |t ?
r?
?
t?
?
T+ : t?
?
r ?
?
t?
?
T?
: t?
/?
r} |.A term-frequency-induced taxonomy (TFIT), isan ordered tree over terms in T .
For a node n inthe tree, n.t is the term at n, A(n) the ancestors ofn, and P (n) the predecessors of n.A TFIT has a root node with the special term ?and the conditional frequency ?.
The followingcondition is true for any other node n:?t ?
T, F (n.t, A(n), P (n)) ?
F (t, A(n), P (n)).That is, each node?s term has the highest condi-tional frequency in the context of the node?s an-cestors and predecessors.
Only terms with a con-ditional frequency above zero are added to a TFIT.We show in Section 4 how a TFIT taxonomycan be automatically induced from a given corpus.But before that, we show that TFITs are useful inpractice and reflect a natural ordering of terms forapplication domains where the concept hierarchyis expressed through the frequency in which termsappear.3.2 Example Domain: Address DataAn address taxonomy is a key enabler for addressstandardization.
Figure 1 shows part of such an ad-dress taxonomy where the root contains the mostgeneric term and leaf-level nodes contain the mostspecific terms.
For emerging economies buildinga standardized address taxonomy is a huge chal-127Row Term Part of address Category1 D-15 house number alphanumerical2 Rawal building name proper noun3 Complex building name proper noun4 Behind landmark marker5 Hotel landmark marker6 Ruchira landmark proper noun7 Katre street proper noun8 Road street marker9 Jeevan area proper noun10 Nagar area marker11 Andheri city (taluk) proper noun12 East city (taluk) direction13 Mumbai district proper noun14 Maharashtra state proper noun15 400069 ZIP code 6 digit stringTable 1: Example of a tokenized addresslenge.
First, new areas and with it new addressesconstantly emerge.
Second, there are very limitedconventions for specifying an address (Faruquie etal., 2010).
However, while many developing coun-tries do not have a postal taxonomy, there is oftenno lack of address data to learn a taxonomy from.Column 2 of Table 1 shows an example of anIndian address.
Although Indian addresses tend tofollow the general principal that more specific in-formation is mentioned earlier, there is no fixed or-der for different elements of an address.
For exam-ple, the ZIP code of an address may be mentionedbefore or after the state information and, althoughZIP code information is more specific than city in-formation, it is generally mentioned later in theaddress.
Also, while ZIP codes often exist, theiruse by people is very limited.
Instead, people tendto mention copious amounts of landmark informa-tion (see for example rows 4-6 in Table 1).Taking all this into account, there is often notenough structure available to automatically infer ataxonomy purely based on the structural or seman-tic aspects of an address.
However, for addressdata, the general-to-specific concept hierarchy isreflected in the frequency with which terms appearon their own and together with other terms.It mostly holds that f(s) > f(d) > f(c) >f(z) where s is a state name, d is a district name,c is a city name, and z is a ZIP code.
How-ever, sometimes the name of a large city may bemore frequent than the name of a small state.
Forexample, in a given corpus, the term ?Houston?
(a populous US city) may appear more frequentthan the term ?Vermont?
(a small US state).
Toavoid that ?Houston?
is picked as a node at the firstlevel of the taxonomy (which should only containstates), the conditional-frequency constraint intro-duced in Section 3.1 is enforced for each node in aTFIT.
?Houston?s state ?Texas?
(which is more fre-quent) is picked before ?Houston?.
After ?Texas?
ispicked it appears in the ?cannot-also-appear??
listfor all further siblings on the first level, thus giving?Houston?
has a conditional frequency of zero.We show in Section 5 that an address taxonomycan be inferred by generating a TFIT taxonomy.4 Automatically Generating TFITsWe describe a basic algorithm to generate a TFITand then show extensions to adapt to different ap-plication domains.4.1 Base AlgorithmAlgorithm 1 Algorithm for generating a TFIT.// For initialization T+, T?
are empty// For initialization l,w are zerogenTFIT(T+, T?, C, l, w)// select most frequent termtnext = tj with F (tj , T+, T?)
is maximal amongst alltj ?
C;fnext = F (tnext, T+, T?
);if fnext ?
support then//Output node (tj , l, w)...// Generate child nodegenTFIT(T+ ?
{tnext}, T?, C, l + 1, w)// Generate sibling nodegenTFIT(T+, T?
?
{tnext}, C, l, w + 1)end ifTo generate a TFIT taxonomy as defined in Sec-tion 3.1 we recursively pick the most frequent termgiven previously chosen terms.
The basic algo-rithm genTFIT is sketched out in Algorithm 1.When genTFIT is called the first time, T+ andT?
are empty and both level l and width w arezero.
With each call of genTFIT a new noden in the taxonomy is created with (t, l, w) wheret is the most frequent term given T+ and T?and l and w capture the position in the taxonomy.genTFIT is recursively called to generate a childof n and a sibling for n.The only input parameter required by our al-gorithm is support.
Instead of adding all termswith a conditional frequency above zero, we onlyadd terms with a conditional frequency equal to orhigher than support.
The support parameter con-trols the precision of the resulting TFIT and alsothe runtime of the algorithm.
Increasing supportincreases the precision but also lowers the recall.1284.2 Integrating ConstraintsStructural as well as semantic constraints can eas-ily be integrated into the TFIT generation.We distinguish between taxonomy-level andnode-level structural constraints.
For example,limiting the depth of the taxonomy by introduc-ing a maxLevel constraint and checking beforeeach recursive call if maxLevel is reached, isa taxonomy-level constraint.
A node-level con-straint applies to each node and affects the waythe frequency of terms is determined.For our example application, we introduce thefollowing node-level constraint: at each node weonly count terms that appear at specific positionsin records with respect to the current level of thenode.
Specifically, we slide (or incrementally in-crease) a window over the address records start-ing from the end.
For example, when picking theterm ?Washington?
as a state name, occurrences of?Washington?
as city or street name are ignored.Using a window instead of an exact position ac-counts for positional variability.
Also, to accom-modate varying amounts of landmark informationwe length-normalize the position of terms.
That is,we divide all positions in an address by the averagelength of an address (which is 10 for our 40 Mil-lion addresses).
Accordingly, we adjust the size ofthe window and use increments of 0.1 for sliding(or increasing) the window.In addition to syntactical constraints, semanticconstraints can be integrated by classifying termsfor use when picking the next frequent term.
In ourexample application, markers tend to appear muchmore often than any proper noun.
For example,the term ?Road?
appears in almost all addresses,and might be picked up as the most frequent termvery early in the process.
Thus, it is beneficial toignore marker terms during taxonomy generationand adding them as a post-processing step.4.3 Handling NoiseThe approach we propose naturally handles noiseby ignoring it, unless the noise level exceeds thesupport threshold.
Misspelled terms are generallyinfrequent and will as such not become part ofthe taxonomy.
The same applies to incorrect ad-dresses.
Incomplete addresses partially contributeto the taxonomy and only cause a problem if thesame information is missing too often.
For ex-ample, if more than support addresses with thecity ?Houston?
are missing the state ?Texas?, then?Houston?
may become a node at the first level andappear to be a state.
Generally, such cases only ap-pear at the far right of the taxonomy.5 EvaluationWe present an evaluation of our approach for ad-dress data from an emerging economy.
We imple-mented our algorithm in Java and store the recordsin a DB2 database.
We rely on the DB2 optimizerto efficiently retrieve the next frequent term.5.1 DatasetThe results are based on 40 Million Indian ad-dresses.
Each address record was given to us asa single string and was first tokenized into a se-quence of terms as shown in Table 1.
In a secondstep, we addressed spelling variations.
There is nofixed way of transliterating Indian alphabets to En-glish and most Indian proper nouns have variousspellings in English.
We used tools to detect syn-onyms with the same context to generate a list ofrules to map terms to a standard form (Lin, 1998).For example, in Table 1 ?Maharashtra?
can also bespelled ?Maharastra?.
We also used a list of key-words to classify some terms as markers such as?Road?
and ?Nagar?
shown in Table 1.Our evaluation consists of two parts.
First, weshow results for constructing a TFIT from scratch.To evaluate the precision and recall we also re-trieved post office addresses from India Post1,cleaned them, and organized them in a tree.Second, we use our approach to enrich the ex-isting hierarchy created from post office addresseswith additional area terms.
To validate the result,we also retrieved data about which area names ap-pear within a ZIP code.2 We also verified whetherGoogle Maps shows an area on its map.35.2 Taxonomy GenerationWe generated a taxonomy O using all 40 millionaddresses.
We compare the terms assigned tocategory levels district and taluk4 in O with thetree P constructed from post office addresses.Each district and taluk has at least one post office.Thus P covers all districts and taluks and allowsus to test coverage and precision.
We compute theprecision and recall for each category level CL as1http://www.indiapost.gov.in/Pin/pinsearch.aspx2http://www.whereincity.com/india/pincode/search3maps.google.com4Administrative division in some South-Asian countries.129Support Recall % Precision %100 District 93.9 57.4Taluk 50.9 60.5200 District 87.9 64.4Taluk 49.6 66.1Table 2: Precision and recall for categorizingterms belonging to the state MaharashtraRecallCL = # correct paths from root to CL in O# paths from root to CL in PPrecisionCL = # correct paths from root to CL in O# paths from root to CL in OTable 2 shows precision and recall for districtand taluk for the large state Maharashtra.
Recallis good for district.
For taluk it is lower because amajor part of the data belongs to urban areas wheretaluk information is missing.
The precision seemsto be low but it has to be noted that in almost 75%of the addresses either district or taluk informa-tion is missing or noisy.
Given that, we were ableto recover a significant portion of the knowledgestructure.We also examined a branch for a smaller state(Kerala).
Again, both districts and taluks appearat the next level of the taxonomy.
For a supportof 200 there are 19 entries in O of which all buttwo appear in P as district or taluk.
One entry is ataluk that actually belongs to Maharashtra and oneentry is a name variation of a taluk in P .
Therewere not enough addresses to get a good coverageof all districts and taluks.5.3 Taxonomy AugmentationWe used P and ran our algorithm for each branchin P to include area information.
We focus ourevaluation on the city Mumbai.
The recall is lowbecause many addresses do not mention a ZIPcode or use an incorrect ZIP code.
However,the precision is good implying that our approachworks even in the presence of large amounts ofnoise.Table 3 shows the results for ZIP code 400002and 400004 for a support of 100.
We get simi-lar results for other ZIP codes.
For each detectedarea we compared whether the area is also listedon whereincity.com, part of a post office name(PO), or shown on google maps.
All but fourareas found are confirmed by at least one of thethree external sources.
Out of the unconfirmedterms Fanaswadi and MarineDrive seem tobe genuine area names but we could not confirmDhakurdwarRoad.
The term th is due to ourArea Whereincity PO GoogleBhuleshwar yes no yesChira Bazar yes no yesDhobi Talao no no yesFanaswadi no no noKalbadevi Road yes yes yesMarine Drive no no noMarine Lines yes yes yesPrincess Street no no yesth no no noThakurdwar Road no no noZaveri Bazar yes no yesCharni Road no yes noGirgaon yes yes yesKhadilkar Road yes no yesKhetwadi Road yes no noKumbharwada no no yesOpera House no yes noPrathna Samaj yes no noTable 3: Areas found for ZIP code 400002 (top)and 400004 (bottom)tokenization process.
16 correct terms out of 18terms results in a precision of 89%.We also ran experiments to measure the cov-erage of area detection for Mumbai without us-ing ZIP codes.
Initializing our algorithm withMaharshtra and Mumbai yielded over 100 ar-eas with a support of 300 and more.
However,again the precision is low because quite a few ofthose areas are actually taluk names.Using a large number of addresses is necessaryto achieve good recall and precision.6 ConclusionIn this paper, we presented a novel approach togenerate a taxonomy for data where terms ex-hibit an inherent frequency-based hierarchy.
Weshowed that term frequency can be used to gener-ate a meaningful taxonomy from address records.The presented approach can also be used to extendan existing taxonomy which is a big advantagefor emerging countries where geographical areasevolve continuously.While we have evaluated our approach on ad-dress data, it is applicable to all data sources wherethe inherent hierarchical structure is encoded inthe frequency with which terms appear on theirown and together with other terms.
Preliminaryexperiments on real-time analyst?s stock markettips 5 produced a taxonomy of (TV station, An-alyst, Affiliation) with decent precision and recall.5See Live Market voices at:http://money.rediff.com/money/jsp/markets home.jsp130ReferencesRakesh Agrawal and Ramakrishnan Srikant.
1994.Fast algorithms for mining association rules in largedatabases.
In Proceedings of the 20th InternationalConference on Very Large Data Bases, pages 487?499.Razvan C. Bunescu and Raymond J. Mooney.
2007.Learning to extract relations from the web usingminimal supervision.
In Proceedings of the 45th An-nual Meeting of the Association of ComputationalLinguistics, pages 576?583.Sharon A. Caraballo.
1999.
Automatic constructionof a hypernym-labeled noun hierarchy from text.
InProceedings of the 37th Annual Meeting of the As-sociation for Computational Linguistics on Compu-tational Linguistics, pages 120?126.Philipp Cimiano and Johanna Wenderoth.
2007.
Au-tomatic acquisition of ranked qualia structures fromthe web.
In Proceedings of the 45th Annual Meet-ing of the Association for Computational Linguis-tics, pages 888?895.Philipp Cimiano, Gu?nter Ladwig, and Steffen Staab.2005.
Gimme?
the context: context-driven auto-matic semantic annotation with c-pankow.
In Pro-ceedings of the 14th International Conference onWorld Wide Web, pages 332?341.Oren Etzioni, Michael Cafarella, Doug Downey, Ana-Maria Popescu, Tal Shaked, Stephen Soderland,Daniel S. Weld, and Alexander Yates.
2005.
Un-supervised named-entity extraction from the web:an experimental study.
Artificial Intelligence,165(1):91?134.Tanveer A. Faruquie, K. Hima Prasad, L. VenkataSubramaniam, Mukesh K. Mohania, Girish Venkat-achaliah, Shrinivas Kulkarni, and Pramit Basu.2010.
Data cleansing as a transient service.
InProceedings of the 26th International Conference onData Engineering, pages 1025?1036.Michael Fleischman and Eduard Hovy.
2002.
Finegrained classification of named entities.
In Proceed-ings of the 19th International Conference on Com-putational Linguistics, pages 1?7.Roxana Girju, Adriana Badulescu, and Dan Moldovan.2006.
Automatic discovery of part-whole relations.Computational Linguistics, 32(1):83?135.Claudio Giuliano and Alfio Gliozzo.
2008.
Instance-based ontology population exploiting named-entitysubstitution.
In Proceedings of the 22nd Inter-national Conference on Computational Linguistics,pages 265?272.Lucja M. Iwaska, Naveen Mata, and Kellyn Kruger.2000.
Fully automatic acquisition of taxonomicknowledge from large corpora of texts.
In Lucja M.Iwaska and Stuart C. Shapiro, editors, Natural Lan-guage Processing and Knowledge Representation:Language for Knowledge and Knowledge for Lan-guage, pages 335?345.Govind Kothari, Tanveer A Faruquie, L V Subrama-niam, K H Prasad, and Mukesh Mohania.
2010.Transfer of supervision for improved address stan-dardization.
In Proceedings of the 20th Interna-tional Conference on Pattern Recognition.Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy.2008.
Semantic class learning from the web withhyponym pattern linkage graphs.
In Proceedings ofthe 46th Annual Meeting of the Association for Com-putational Linguistics: Human Language Technolo-gies, pages 1048?1056.Dekang Lin, Shaojun Zhao, Lijuan Qin, and MingZhou.
2003.
Identifying synonyms among distri-butionally similar words.
In Proceedings of the 18thInternational Joint Conference on Artificial Intelli-gence, pages 1492?1493.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proceedings of the 17th Inter-national Conference on Computational Linguistics,pages 768?774.Patrick Pantel and Marco Pennacchiotti.
2006.Espresso: leveraging generic patterns for automat-ically harvesting semantic relations.
In Proceed-ings of the 21st International Conference on Com-putational Linguistics and the 44th Annual Meet-ing of the Association for Computational Linguis-tics, pages 113?120.Shourya Roy and L Venkata Subramaniam.
2006.
Au-tomatic generation of domain models for call cen-ters from noisy transcriptions.
In Proceedings ofthe 21st International Conference on ComputationalLinguistics and the 44th Annual Meeting of the As-sociation for Computational Linguistics, pages 737?744.Rion Snow, Daniel Jurafsky, and Andrew Y. Ng.
2004.Learning syntactic patterns for automatic hypernymdiscovery.
In Advances in Neural Information Pro-cessing Systems, pages 1297?1304.Hristo Tanev and Bernardo Magnini.
2006.
Weaklysupervised approaches for ontology population.
InProceedings of the 11th Conference of the EuropeanChapter of the Association for Computational Lin-guistics, pages 3?7.Hui Yang and Jamie Callan.
2008.
Learning the dis-tance metric in a personal ontology.
In Proceed-ing of the 2nd International Workshop on Ontolo-gies and Information Systems for the Semantic Web,pages 17?24.Hui Yang and Jamie Callan.
2009.
A metric-basedframework for automatic taxonomy induction.
InProceedings of the Joint Conference of the 47th An-nual Meeting of the ACL and the 4th InternationalJoint Conference on Natural Language Processingof the AFNLP, pages 271?279.131
