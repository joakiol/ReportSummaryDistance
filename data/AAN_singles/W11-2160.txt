Proceedings of the 6th Workshop on Statistical Machine Translation, pages 478?484,Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational LinguisticsJoshua 3.0: Syntax-based Machine Translationwith the Thrax Grammar ExtractorJonathan Weese1, Juri Ganitkevitch1, Chris Callison-Burch1, Matt Post2 and Adam Lopez1,21Center for Language and Speech Processing2Human Language Technology Center of ExcellenceJohns Hopkins UniversityAbstractWe present progress on Joshua, an open-source decoder for hierarchical and syntax-based machine translation.
The main fo-cus is describing Thrax, a flexible, opensource synchronous context-free grammar ex-tractor.
Thrax extracts both hierarchical (Chi-ang, 2007) and syntax-augmented machinetranslation (Zollmann and Venugopal, 2006)grammars.
It is built on Apache Hadoop forefficient distributed performance, and can eas-ily be extended with support for new gram-mars, feature functions, and output formats.1 IntroductionJoshua is an open-source1 toolkit for hierarchicalmachine translation of human languages.
The origi-nal version of Joshua (Li et al, 2009) was a reim-plementation of the Python-based Hiero machine-translation system (Chiang, 2007); it was later ex-tended (Li et al, 2010) to support richer formalisms,such as SAMT (Zollmann and Venugopal, 2006).The main focus of this paper is to describe thispast year?s work in developing Thrax (Weese, 2011),an open-source grammar extractor for Hiero andSAMT grammars.
Grammar extraction has shownitself to be something of a black art, with decod-ing performance depending crucially on a varietyof features and options that are not always clearlydescribed in papers.
This hindered direct com-parison both between and within grammatical for-malisms.
Thrax standardizes Joshua?s grammar ex-1http://github.com/joshua-decoder/joshuatraction procedures by providing a flexible and con-figurable means of specifying these settings.
Sec-tion 3 presents a systematic comparison of the twogrammars using identical feature sets.In addition, Joshua now includes a single pa-rameterized script that implements the entire MTpipeline, from data preparation to evaluation.
Thisscript is built on top of a module called CachePipe.CachePipe is a simple wrapper around shell com-mands that uses SHA-1 hashes and explicitly-provided lists of dependencies to determine whethera command needs to be run, saving time both in run-ning and debugging machine translation pipelines.2 Thrax: grammar extractionIn modern machine translation systems such asJoshua (Li et al, 2009) and cdec (Dyer et al, 2010),a translation model is represented as a synchronouscontext-free grammar (SCFG).
Formally, an SCFGmay be considered as a tuple(N,S, T?, T?
, G)where N is a set of nonterminal symbols of thegrammar, S ?
N is the goal symbol, T?
and T?are the source- and target-side terminal symbol vo-cabularies, respectively, and G is a set of productionrules of the grammar.Each rule in G is of the formX ?
?
?, ?,?
?where X ?
N is a nonterminal symbol, ?
is a se-quence of symbols from N ?
T?, ?
is a sequence of478symbols from N ?
T?
, and ?
is a one-to-one cor-respondence between the nonterminal symbols of ?and ?.The language of an SCFG is a set of ordered pairsof strings.
During decoding, the set of candidatetranslations of an input sentence f is the set of alle such that the pair (f, e) is licensed by the transla-tion model SCFG.
Each candidate e is generated byapplying a sequence of production rules (r1 .
.
.
rn).The cost of applying each rule is:w(X ?
?
?, ??)
=?i?i(X ?
?
?, ??
)?i (1)where each ?i is a feature function and ?i is theweight for ?i.
The total translation model score ofa candidate e is the product of the rules used in itsderivation.
This translation model score is then com-bined with other features (such as a language modelscore) to produce an overall score for each candidatetranslation.2.1 Hiero and SAMTThroughout this work, we will reference two par-ticular SCFG types known as Hiero and Syntax-Augmented Machine Translation (SAMT).A Hiero grammar (Chiang, 2007) is an SCFGwith only one type of nonterminal symbol, tradi-tionally labeled X .
A Hiero grammar can be ex-tracted from a parallel corpus of word-aligned sen-tence pairs as follows: If (f ji , elk) is a sub-phraseof the sentence pair, we say it is consistent withthe pair?s alignment if none of the words in f ji arealigned to words outside of elk, and vice-versa.
Theconsistent sub-phrase may be extracted as an SCFGrule.
Furthermore, if a consistent phrase is containedwithin another one, a hierarchical rule may be ex-tracted by replacing the smaller piece with a nonter-minal.An SAMT grammar (Zollmann and Venugopal,2006) is similar to a Hiero grammar, except that thenonterminal symbol set is much larger, and its la-bels are derived from a parse tree over either thesource or target side in the following manner.
Foreach rule, if the target side is spanned by one con-stituent of the parse tree, we assign that constituent?slabel as the nonterminal symbol for the rule.
Other-wise, we assign an extended category of the formC1 + C2, C1/C2, or C2 \C1 ?
indicating that thedas begr?
?e ich sehr .iverymuchwelcome this.PRPNPSRB RBADVPVPVBP DTNP.Figure 1: An aligned sentence pair.target side spans two adjacent constituents, is a C1missing a C2 to the right, or is a C1 missing a C2on the left, respectively.
Table 1 contains a list ofHiero and SAMT rules extracted from the trainingsentence pair in Figure 1.2.2 System overviewThe following were goals in the design of Thrax:?
the ability to extract different SCFGs (such asHiero and SAMT), and to adjust various extrac-tion parameters for the grammars;?
the ability to easily change and extend the fea-ture sets for each rule?
scalability to arbitrarily large training corpora.Thrax treats the grammar extraction and scoringas a series of dependent Hadoop jobs.
Hadoop(Venugopal and Zollmann, 2009) is an implementa-tion of Google?s MapReduce (Dean and Ghemawat,2004), a framework for distributed processing oflarge data sets.
Hadoop jobs have two parts.
In themap step, a set of key/value pairs is mapped to a setof intermediate key/value pairs.
In the reduce step,all intermediate values associated with an interme-diate key are merged.The first step in the Thrax pipeline is to extract allthe grammar rules.
The map step in this job takes asinput word-aligned sentence pairs and produces a setof ordered pairs (r, c) where r is a rule and c is thenumber of times it was extracted.
During the reducestep, these rule counts are summed, so the result isa set of rules, along with the total number of timeseach rule was extracted from the entire corpus.479Span Hiero SAMT[1, 3] X ?
?sehr, very much?
ADV P ?
?sehr, very much?
[0, 3] X ?
?X sehr, X very much?
PRP +ADV P ?
?PRP sehr, PRP very much?
[3, 4] X ?
?begru??e,welcome?
V BP ?
?begru??e,welcome?
[0, 6] X ?
?X ich sehr ., i very much X .?
S ?
?V P ich sehr ., i very much V P .?
[0, 6] X ?
?X ., X .?
S ?
?S/.
., S/.
.
?Table 1: A subset of the Hiero and SAMT rules extracted from the sentence pair of Figure 1.Given the rules and their counts, a separateHadoop job is run for each feature.
These jobs canall be submitted at once and run in parallel, avoid-ing the linear sort-and-score workflow.
The outputfrom each feature job is the same set of pairs (r, c)as the input, except each rule r has been annotatedwith some feature score f .After the feature jobs have been completed, wehave several copies of the grammar, each of whichhas been scored with one feature.
A final Hadoopjob combines all these scores to produce the finalgrammar.Some users may not have access to a Hadoopcluster.
Thrax can be run in standalone or pseudo-distributed mode on a single machine.
It can alsobe used with Amazon Elastic MapReduce,2 a webservice that provides computation time on a Hadoopcluster on-demand.2.3 ExtractionThe first step in the Thrax workflow is the extractionof grammar rules from an input corpus.
As men-tioned above, Hiero and SAMT grammars both re-quire a parallel corpus with word-level alignments.SAMT additionally requires that the target side ofthe corpus be parsed.There are several parameters that can make a sig-nificant difference in a grammar?s overall translationperformance.
Each of these parameters is easily ad-justable in Thrax by changing its value in a configu-ration file.?
maximum rule span?
maximum span of consistent phrase pairs?
maximum number of nonterminals?
minimum number of aligned terminals in rule2http://aws.amazon.com/elasticmapreduce/?
whether to allow adjacent nonterminals onsource side?
whether to allow unaligned words at the edgesof consistent phrase pairsChiang (2007) gives reasonable heuristic choicesfor these parameters when extracting a Hiero gram-mar, and Lopez (2008) confirms some of them (max-imum rule span of 10, maximum number of source-side symbols at 5, and maximum number of non-terminals at 2 per rule).
?)
provided comparisonsamong phrase-based, hierarchical, and syntax-basedmodels, but did not report extensive experimentationwith the model parameterizations.When extracting Hiero- or SAMT-style gram-mars, the first Hadoop job in the Thrax workflowtakes in a parallel corpus and produces a set of rules.But in fact Thrax?s extraction mechanism is moregeneral than that; all it requires is a function thatmaps a string to a set of rules.
This makes it easyto implement new grammars and extract them usingThrax.2.4 Feature functionsThrax considers feature functions of two types: first,there are features that can be calculated by lookingat each rule in isolation.
Such features do not re-quire a Hadoop job to calculate their scores, sincewe may inspect the rules in any order.
(In practice,we calculate the scores at the very last moment be-fore outputting the final grammar.)
We call thesefeatures simple features.
Thrax implements the fol-lowing simple features:?
a binary indicator functions denoting:?
whether the rule is purely abstract (i.e.,has no terminal symbols)480?
the rule is purely lexical (i.e., has no non-terminals)?
the rule is monotonic or has reordering?
the rule has adjacent nonterminals on thesource side?
counters for?
the number of unaligned words in the rule?
the number of terminals on the target sideof the rule?
a constant phrase penaltyIn addition to simple features, Thrax also imple-ments map-reduce features.
These are features thatrequire comparing rules in a certain order.
Thraxuses Hadoop to sort the rules efficiently and calcu-late these feature functions.
Thrax implements thefollowing map-reduce features:?
Phrasal translation probabilities p(?|?)
andp(?|?
), calculated with relative frequency:p(?|?)
=C(?, ?)C(?
)(2)(and vice versa), where C(?)
is the number oftimes a given event was extracted.?
Lexical weighting plex(?|?,A) andplex(?|?,A).
We calculate these weightsas given in (Koehn et al, 2003): let A be thealignment between ?
and ?, so (i, j) ?
A ifand only if the ith word of ?
is aligned to thejth word of ?.
Then we can define plex(?|?)
asn?i=11|{j : (i, j) ?
A}|?
(i,j)?Aw(?j |?i) (3)where ?i is the ith word of ?, ?j is the jth wordof ?, and w(y|x) is the relative frequency ofseeing word y given x.?
Rarity penalty, given byexp(1?
C(X ?
?
?, ??))
(4)where again C(?)
is a count of the number oftimes the rule was extracted.The above features are all implemented and canbe turned on or off with a keyword in the Thrax con-figuration file.It is easy to extend Thrax with new feature func-tions.
For simple features, all that is needed is to im-plement Thrax?s SIMPLEFEATURE interface defin-ing a method that takes in a rule and calculates afeature score.
Map-reduce features are slightly morecomplex: to subclass MAPREDUCEFEATURE, onemust define a mapper and reducer, but also a sortcomparator to determine in what order the rules arecompared during the reduce step.2.5 Related workJoshua includes a simple Hiero extractor (Schwartzand Callison-Burch, 2010).
The extractor runs as asingle Java process, which makes it difficult to ex-tract larger grammars, since the host machine musthave enough memory to hold all of the rules at once.Joshua?s extractor scores each rule with three featurefunctions ?
lexical probabilities in two directions,and one phrasal probability score p(?|?
).The SAMT implementation of Zollmann andVenugopal (2006) includes a several-thousand-linePerl script to extract their rules.
In addition tophrasal and lexical probabilities, this extractor im-plements several other features that are also de-scribed in section 2.4.Finally, the cdec decoder (Dyer et al, 2010) in-cludes a grammar extractor that performs well onlywhen all rules can be held in memory.Memory usage is a limitation of both the Joshuaand cdec extractors.
Translation models can be verylarge, and many feature scores require accumulationof statistical data from the entire set of extractedrules.
Since it is impractical to keep the entire gram-mar in memory, rules are usually sorted on disk andthen read sequentially.
Different feature calcula-tions may require different sort orders, leading to alinear workflow that alternates between sorting thegrammar and calculating a feature score.
To cal-culate more feature scores, more sorts have to beperformed.
This discourages the implementation ofnew features.
For example, Joshua?s built-in rule ex-tractor calculates the phrasal probability p(?|?)
foreach rule but, to save time, does not calculate its ob-vious counterpart p(?|?
), which would require an-other sort.481Language pair sentences (K) words (M)cs?en 332 4.7de?en 279 5.5en?cs 487 6.9en?de 359 7.2en?fr 682 12.5fr?en 792 14.4Table 2: Training data size after subsampling.The SAMT extractor does not have a problemwith large data sets; SAMT can run on Hadoop, asThrax does.The Joshua and cdec extractors only extract Hierogrammars, and Zollmann and Venugopal?s extractorcan only extract SAMT-style grammars.
They arenot designed to score arbitrary feature sets, either.Since variation in translation models and feature setscan have a significant effect on translation perfor-mance, we have developed Thrax in order to make iteasy to build and test new models.3 ExperimentsWe built systems for six language pairs for the WMT2011 shared task: cz-en, en-cz, de-en, en-de, fr-en,and en-fr.3 For each language pair, we built bothSAMT and hiero grammars.4 Table 3 contains theresults on the complete WMT 2011 test set.To train the translation models, we used the pro-vided Europarl and news commentary data.
For cz-en and en-cz, we also used sections of the CzEngparallel corpus (Bojar and Z?abokrtsky?, 2009).
Theparallel data was subsampled using Joshua?s built-in subsampler to select sentences with n-grams rel-evant to the tuning and test set.
We used SRILMto train a 5-gram language model with Kneser-Neysmoothing using the appropriate side of the paral-lel data.
For the English LM, we also used EnglishGigaword Fourth Edition.5Before extracting an SCFG with Thrax, we usedthe provided Perl scripts to tokenize and normalize3fr=French, cz=Czech, de=German, en=English.4Except for fr-en and en-fr.
We were unable to decode withSAMT grammars for these language pairs due to their large size.We have since resolved this issue and will have scores for thefinal version of the paper.5LDC2009T13pair hiero SAMT improvementcz-en 21.1 21.7 +0.6en-cz 16.8 16.9 +0.1de-en 18.9 19.5 +0.6en-de 14.3 14.9 +0.6fr-en 28.0 - -en-fr 30.4 - -Table 3: Single-reference BLEU-4 scores.the data.
We also removed any sentences longer than50 tokens (after tokenization).
For SAMT grammarextraction, we parsed the English training data us-ing the Berkeley Parser (Petrov et al, 2006) with theprovided Treebank-trained grammar.We tuned the model weights against theWMT08 test set (news-test2008) using Z-MERT (Zaidan, 2009), an implementation of mini-mum error-rate training included with Joshua.
Wedecoded the test set to produce a 300-best list ofunique translations, then chose the best candidate foreach sentence using Minimum Bayes Risk reranking(Kumar and Byrne, 2004).
Figure 2 shows an exam-ple derivation with an SAMT grammar.
To re-casethe 1-best test set output, we trained a true-case 5-gram language model using the same LM trainingdata as before, and used an SCFG translation modelto translate from the lowercased to true-case output.The translation model used rules limited to five to-kens in length, and contained no hierarchical rules.4 CachePipe: Cached pipeline runsMachine translation pipelines involve the specifica-tion and execution of many different datasets, train-ing procedures, and pre- and post-processing tech-niques that can have large effects on translation out-come, and which make direct comparisons betweensystems difficult.
The complexity of managing thesepipelines and experimental environments has led to anumber of different experimental management sys-tems, such as Experiment.perl,6 Joshua 2.0?s Make-file system (Li et al, 2010), and LoonyBin (Clarkand Lavie, 2010).
In addition to managing thepipeline, these scripts employ different techniquesto avoid expensive recomputation by caching steps.6http://www.statmt.org/moses/?n=FactoredTraining.EMS482thereactor type will be operated with uraniumVBNDT+NPGLUEVPPPder reaktortyp , das nichtangereichertwird zwar mit uran betrieben, which isnotenrichedist .NPGLUENNCOMMA+SBAR+.ADJPJJ.SVBNDT+NPGLUEVPPPNPGLUENNCOMMA+SBAR+.ADJPJJSFigure 2: An SAMT derivation.
The shaded terminal symbols are the lexicalized part of a rule with terminalsand non-terminals.
The unshaded terminals are directly dominated by a nonterminal symbol.However, these approaches are based on simple butunreliable heuristics (such as timestamps or file ex-istence) to make the caching determination.Our solution to the caching dependency problemis CachePipe.
CachePipe is designed with the fol-lowing goals: (1) robust content-based dependencychecking and (2) ease of use, including minimalediting of existing scripts.
CachePipe is essentiallya wrapper around command invocations.
Presentedwith a command to run and a list of file dependen-cies, it computes SHA-1 hashes of the dependenciesand of the command invocation and stores them; thecommand is executed only if any of those hashes aredifferent from previous runs.
A basic invocation in-volves specifying (1) a name or identifier associatedwith the command or step, (2) the command to run,and (3) a list of file dependencies.
For example, tocopy file a to b from a shell prompt, the followingcommand could be used:cachecmd copy "cp a b" a bThe first time the command is run, the file would becopied; afterwards, the command would be skippedafter CachePipe verified that the contents of the de-pendencies a and b had not changed.CachePipe is open-source software, distributedwith Joshua or available separately.7 It currentlyprovides both a shell script interface and a program-matic API for Perl.
It accepts a number of otherarguments and dependency types.
It also serves asthe foundation of a new script in Joshua 3.0 that im-plements the complete Joshua pipeline, from datapreparation to evaluation.5 Future workThrax is currently limited to SCFG-based translationmodels.
A natural development would be to extractGHKM grammars (Galley et al, 2004) or more re-cent tree-to-tree models (Zhang et al, 2008; Liu etal., 2009; Chiang, 2010).
We also hope that Thraxwill continue to be extended with more feature func-tions as researchers develop and contribute them.AcknowledgementsThis research was supported by in part by the Eu-roMatrixPlus project funded by the European Com-mission (7th Framework Programme), and by theNSF under grant IIS-0713448.
Opinions, interpre-tations, and conclusions are the authors?
alone.7https://github.com/joshua-decoder/cachepipe483ReferencesOndr?ej Bojar and Zdene?k Z?abokrtsky?.
2009.
CzEng0.9:Large Parallel Treebank with Rich Annotation.Prague Bulletin of Mathematical Linguistics, 92. inprint.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?228.David Chiang.
2010.
Learning to translate with sourceand target syntax.
In Proc.
ACL, Uppsala, Sweden,July.Jonathan H. Clark and Alon Lavie.
2010.
Loony-bin: Keeping language technologists sane through au-tomated management of experimental (hyper) work-flows.
In Proc.
LREC.Jeffrey Dean and Sanjay Ghemawat.
2004.
Mapreduce:Simplified data processing on large clusters.
In OSDI.Chris Dyer, Adam Lopez, Juri Ganitkevitch, JonathanWeese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,Vladimir Eidelman, and Philip Resnik.
2010. cdec: Adecoder, alignment, and learning framework for finite-state and context-free translation models.
In Proc.ACL 2010 System Demonstrations, pages 7?12.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
In Proc.NAACL, Boston, Massachusetts, USA, May.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proc.NAACL, Morristown, NJ, USA.Shankar Kumar and William Byrne.
2004.
Minimumbayes-risk decoding for statistical machine translation.In Proc.
NAACL, Boston, Massachusetts, USA, May.Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-itkevitch, Sanjeev Khudanpur, Lane Schwartz, WrenThornton, Jonathan Weese, and Omar Zaidan.
2009.Joshua: An open source toolkit for parsing-based ma-chine translation.
In Proc.
WMT, Athens, Greece,March.Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-itkevitch, Ann Irvine, Sanjeev Khudanpur, LaneSchwartz, Wren N.G.
Thornton, Ziyuan Wang,Jonathan Weese, and Omar F. Zaidan.
2010.
Joshua2.0: a toolkit for parsing-based machine translationwith syntax, semirings, discriminative training andother goodies.
In Proc.
WMT.Yang Liu, Yajuan Lu?, and Qun Liu.
2009.
Improvingtree-to-tree translation with packed forests.
In Proc.ACL, Suntec, Singapore, August.Adam Lopez.
2008.
Tera-scale translation models viapattern matching.
In Proc.
COLING, Manchester, UK,August.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In Proc.
ACL, Sydney, Aus-tralia, July.Lane Schwartz and Chris Callison-Burch.
2010.
Hier-archical phrase-based grammar extraction in joshua:Suffix arrays and prefix trees.
The Prague Bulletin ofMathematical Linguistics, 93:157?166, January.Mark Steedman.
1999.
Alternating quantifier scope inccg.
In Proc.
ACL, Stroudsburg, PA, USA.Ashish Venugopal and Andreas Zollmann.
2009.
Gram-mar based statistical MT on Hadoop: An end-to-endtoolkit for large scale PSCFG based MT.
The PragueBulletin of Mathematical Linguistics, 91:67?78.Jonathan Weese.
2011.
A systematic comparison of syn-chronous context-free grammars for machine transla-tion.
Master?s thesis, Johns Hopkins University, May.Omar F. Zaidan.
2009.
Z-MERT: A fully configurableopen source tool for minimum error rate training ofmachine translation systems.
The Prague Bulletin ofMathematical Linguistics, 91:79?88.Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,Chew Lim Tan, and Sheng Li.
2008.
A tree se-quence alignment-based tree-to-tree translation model.In Proc.
ACL, Columbus, Ohio, June.Andreas Zollmann and Ashish Venugopal.
2006.
Syntaxaugmented machine translation via chart parsing.
InProc.
NAACL Workshop on Statistcal Machine Trans-lation, New York, New York.484
