Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 38?49,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsOptimal Search for Minimum Error Rate TrainingMichel GalleyMicrosoft ResearchRedmond, WA 98052, USAmgalley@microsoft.comChris QuirkMicrosoft ResearchRedmond, WA 98052, USAchrisq@microsoft.comAbstractMinimum error rate training is a crucial compo-nent to many state-of-the-art NLP applications,such as machine translation and speech recog-nition.
However, common evaluation functionssuch as BLEU or word error rate are generallyhighly non-convex and thus prone to searcherrors.
In this paper, we present LP-MERT, anexact search algorithm for minimum error ratetraining that reaches the global optimum usinga series of reductions to linear programming.Given a set of N -best lists produced from Sinput sentences, this algorithm finds a linearmodel that is globally optimal with respect tothis set.
We find that this algorithm is poly-nomial in N and in the size of the model, butexponential in S. We present extensions of thiswork that let us scale to reasonably large tuningsets (e.g., one thousand sentences), by eithersearching only promising regions of the param-eter space, or by using a variant of LP-MERTthat relies on a beam-search approximation.Experimental results show improvements overthe standard Och algorithm.1 IntroductionMinimum error rate training (MERT)?also knownas direct loss minimization in machine learning?is acrucial component in many complex natural languageapplications such as speech recognition (Chou et al,1993; Stolcke et al, 1997; Juang et al, 1997), statisti-cal machine translation (Och, 2003; Smith and Eisner,2006; Duh and Kirchhoff, 2008; Chiang et al, 2008),dependency parsing (McDonald et al, 2005), summa-rization (McDonald, 2006), and phonetic alignment(McAllester et al, 2010).
MERT directly optimizesthe evaluation metric under which systems are beingevaluated, yielding superior performance (Och, 2003)when compared to a likelihood-based discriminativemethod (Och and Ney, 2002).
In complex text gener-ation tasks like SMT, the ability to optimize BLEU(Papineni et al, 2001), TER (Snover et al, 2006), andother evaluation metrics is critical, since these met-rics measure qualities (such as fluency and adequacy)that often do not correlate well with task-agnosticloss functions such as log-loss.While competitive in practice, MERT faces severalchallenges, the most significant of which is search.The unsmoothed error count is a highly non-convexobjective function and therefore difficult to optimizedirectly; prior work offers no algorithm with a goodapproximation guarantee.
While much of the ear-lier work in MERT (Chou et al, 1993; Juang et al,1997) relies on standard convex optimization tech-niques applied to non-convex problems, the Och al-gorithm (Och, 2003) represents a significant advancefor MERT since it applies a series of special line min-imizations that happen to be exhaustive and efficient.Since this algorithm remains inexact in the multidi-mensional case, much of the recent work on MERThas focused on extending Och?s algorithm to findbetter search directions and starting points (Cer et al,2008; Moore and Quirk, 2008), and on experiment-ing with other derivative-free methods such as theNelder-Mead simplex algorithm (Nelder and Mead,1965; Zens et al, 2007; Zhao and Chen, 2009).In this paper, we present LP-MERT, an exactsearch algorithm for N -best optimization that ex-ploits general assumptions commonly made withMERT, e.g., that the error metric is decomposableby sentence.1 While there is no known optimal algo-1Note that MERT makes two types of approximations.
First,the set of all possible outputs is represented only approximately,by N -best lists, lattices, or hypergraphs.
Second, error func-tions on such representations are non-convex and previous workonly offers approximate techniques to optimize them.
Our workavoids the second approximation, while the first one is unavoid-able when optimization and decoding occur in distinct steps.38rithm to optimize general non-convex functions, theunsmoothed error surface has a special property thatenables exact search: the set of translations producedby an SMT system for a given input is finite, so thepiecewise-constant error surface contains only a fi-nite number of constant regions.
As in Och (2003),one could imagine exhaustively enumerating all con-stant regions and finally return the best scoring one?Och does this efficiently with each one-dimensionalsearch?but the idea doesn?t quite scale when search-ing all dimensions at once.
Instead, LP-MERT ex-ploits algorithmic devices such as lazy enumeration,divide-and-conquer, and linear programming to effi-ciently discard partial solutions that cannot be max-imized by any linear model.
Our experiments withthousands of searches show that LP-MERT is neverworse than the Och algorithm, which provides strongevidence that our algorithm is indeed exact.
In theappendix, we formally prove that this search algo-rithm is optimal.
We show that this algorithm ispolynomial in N and in the size of the model, butexponential in the number of tuning sentences.
Tohandle reasonably large tuning sets, we present twomodifications of LP-MERT that either search onlypromising regions of the parameter space, or that relyon a beam-search approximation.
The latter modifica-tion copes with tuning sets of one thousand sentencesor more, and outperforms the Och algorithm on aWMT 2010 evaluation task.This paper makes the following contributions.
Toour knowledge, it is the first known exact searchalgorithm for optimizing task loss on N -best lists ingeneral dimensions.
We also present an approximateversion of LP-MERT that offers a natural means oftrading speed for accuracy, as we are guaranteed toeventually find the global optimum as we graduallyincrease beam size.
This trade-off may be beneficialin commercial settings and in large-scale evaluationslike the NIST evaluation, i.e., when one has a stablesystem and is willing to let MERT run for days orweeks to get the best possible accuracy.
We think thiswork would also be useful as we turn to more humaninvolvement in training (Zaidan and Callison-Burch,2009), as MERT in this case is intrinsically slow.2 Unidimensional MERTLet fS1 = f1 .
.
.
fS denote the S input sentencesof our tuning set.
For each sentence fs, let Cs =es,1 .
.
.
es,N denote a set of N candidate translations.For simplicity and without loss of generality, weassume that N is constant for each index s. Eachinput and output sentence pair (fs, es,n) is weightedby a linear model that combines model parametersw = w1 .
.
.
wD ?
RD with D feature functionsh1(f , e,?)
.
.
.
hD(f , e,?
), where ?
is the hiddenstate associated with the derivation from f to e, suchas phrase segmentation and alignment.
Furthermore,let hs,n ?
RD denote the feature vector representingthe translation pair (fs, es,n).In MERT, the goal is to minimize an error countE(r, e) by scoring translation hypotheses against aset of reference translations rS1 = r1 .
.
.
rS .
As-suming as in Och (2003) that error count is addi-tively decomposable by sentence?i.e., E(rS1 , eS1 ) =?sE(rs, es)?this results in the following optimiza-tion problem:2w?
= argminw{ S?s=1E(rs, e?
(fs;w))}= argminw{ S?s=1N?n=1E(rs, es,n)?
(es,n, e?(fs;w))}(1)wheree?
(fs;w) = argmaxn?
{1...N}{w?hs,n}The quality of this approximation is dependent onhow accurately the N -best lists represent the searchspace of the system.
Therefore, the hypothesis list isiteratively grown: decoding with an initial parametervector seeds the N -best lists; next, parameter esti-mation and N -best list gathering alternate until thesearch space is deemed representative.The crucial observation of Och (2003) is that theerror count along any line is a piecewise constantfunction.
Furthermore, this function for a single sen-tence may be computed efficiently by first finding thehypotheses that form the upper envelope of the modelscore function, then gathering the error count for eachhypothesis along the range for which it is optimal.
Er-ror counts for the whole corpus are simply the sumsof these piecewise constant functions, leading to an2A metric such as TER is decomposable by sentence.
BLEUis not, but its sufficient statistics are, and the literature offersseveral sentence-level approximations of BLEU (Lin and Och,2004; Liang et al, 2006).39efficient algorithm for finding the global optimum ofthe error count along any single direction.Such a hill-climbing algorithm in a non-convexspace has no optimality guarantee: without a perfectdirection finder, even a globally-exact line search maynever encounter the global optimum.
Coordinate as-cent is often effective, though conjugate direction setfinding algorithms, such as Powell?s method (Powell,1964; Press et al, 2007), or even random directionsmay produce better results (Cer et al, 2008).
Ran-dom restarts, based on either uniform sampling or arandom walk (Moore and Quirk, 2008), increase thelikelihood of finding a good solution.
Since randomrestarts and random walks lead to better solutionsand faster convergence, we incorporate them into ourbaseline system, which we refer to as 1D-MERT.3 Multidimensional MERTFinding the global optimum of Eq.
1 is a difficulttask, so we proceed in steps and first analyze thecase where the tuning set contains only one sentence.This gives insight on how to solve the general case.With only one sentence, one of the two summationsin Eq.
1 vanishes and one can exhaustively enumer-ate the N translations e1,n (or en for short) to findthe one that yields the minimal task loss.
The onlydifficulty with S = 1 is to know for each translationen whether its feature vector h1,n (or hn for short)can be maximized using any linear model.
As wecan see in Fig.
1(a), some hypotheses can be maxi-mized (e.g., h1, h2, and h4), while others (e.g., h3and h5) cannot.
In geometric terminology, the formerpoints are commonly called extreme points, and thelatter are interior points.3 The problem of exactlyoptimizing a single N -best list is closely related tothe convex hull problem in computational geometry,for which generic solvers such as the QuickHull al-gorithm exist (Eddy, 1977; Bykat, 1978; Barber etal., 1996).
A first approach would be to construct theconvex hull conv(h1 .
.
.hN ) of the N -best list, thenidentify the point on the hull with lowest loss (h1 inFig.
1) and finally compute an optimal weight vectorusing hull points that share common facets with the3Specifically, a point h is extreme with respect to a convexset C (e.g., the convex hull shown in Fig.
1(a)) if it does not liein an open line segment joining any two points of C. In a minorabuse of terminology, we sometimes simply state that a givenpoint h is extreme when the nature of C is clear from context.w h1 h3: 0.41h1: 0.43 h4: 0.48h5: 0.46 h2: 0.51LMCM(a) (b)Figure 1: N -best list (h1 .
.
.hN ) with associated losses(here, TER scores) for a single input sentence, whoseconvex hull is displayed with dotted lines in (a).
For effec-tive visualization, our plots use only two features (D = 2).While we can find a weight vector that maximizes h1 (e.g.,the w in (b)), no linear model can possibly maximize anyof the points strictly inside the convex hull.optimal feature vector (h2 and h4).
Unfortunately,this doesn?t quite scale even with a single N -best list,since the best known convex hull algorithm runs inO(N bD/2c+1) time (Barber et al, 1996).4Algorithms presented in this paper assume that Dis unrestricted, therefore we cannot afford to buildany convex hull explicitly.
Thus, we turn to linearprogramming (LP), for which we know algorithms(Karmarkar, 1984) that are polynomial in the numberof dimensions and linear in the number of points, i.e.,O(NT ), where T = D3.5.
To check if point hi isextreme, we really only need to know whether we candefine a half-space containing all points h1 .
.
.hN ,with hi lying on the hyperplane delimiting that half-space, as shown in Fig.
1(b) for h1.
Formally, avertex hi is optimal with respect to argmaxi{w?hi}if and only if the following constraints hold:5w?hi = y (2)w?hj ?
y, for each j 6= i (3)w is orthogonal to the hyperplane defining the half-space, and the intercept y defines its position.
The4A convex hull algorithm polynomial in D is very unlikely.Indeed, the expected number of facets of high-dimensional con-vex hulls grows dramatically, and?assuming a uniform distribu-tion of points, D = 10, and a sufficiently large N?the expectednumber of facets is approximately 106N (Buchta et al, 1985).In the worst case, the maximum number of facets of a convexhull is O(NbD/2c/bD/2c!)
(Klee, 1966).5A similar approach for checking whether a given point isextreme is presented in http://www.ifor.math.ethz.ch/?fukuda/polyfaq/node22.html, but our methodgenerates slightly smaller LPs.40above equations represent a linear program (LP),which can be turned into canonical formmaximize c?
wsubject to Aw ?
bby substituting y with w?hi in Eq.
3, by definingA = {an,d}1?n?N ;1?d?D with an,d = hj,d ?
hi,d(where hj,d is the d-th element of hj), and by settingb = (0, .
.
.
, 0)?
= 0.
The vertex hi is extreme ifand only if the LP solver finds a non-zero vector wsatisfying the canonical system.
To ensure that w iszero only when hi is interior, we set c = hi ?
h?,where h?
is a point known to be inside the hull (e.g.,the centroid of the N -best list).6 In the remainingof this section, we use this LP formulation in func-tion LINOPTIMIZER(hi;h1 .
.
.hN ), which returnsthe weight vector w?
maximizing hi, or which returns0 if hi is interior to conv(h1 .
.
.hN ).
We also useconv(hi;h1 .
.
.hN ) to denote whether hi is extremewith respect to this hull.Algorithm 1: LP-MERT (for S = 1).input : sent.-level feature vectors H = {h1 .
.
.hN}input : sent.-level task losses E1 .
.
.
EN , whereEn := E(r1, e1,n)output :optimal weight vector w?1 begin.
sort N -best list by increasing losses:2 (i1 .
.
.
iN )?
INDEXSORT(E1 .
.
.
EN )3 for n?
1 to N do.
find w?
maximizing in-th element:4 w??
LINOPTIMIZER(hin ;H)5 if w?
6= 0 then6 return w?7 return 0An exact search algorithm for optimizing a singleN -best list is shown above.
It lazily enumerates fea-ture vectors in increasing order of task loss, keepingonly the extreme ones.
Such a vertex hj is known tobe on the convex hull, and the returned vector w?
max-imizes it.
In Fig.
1, it would first run LINOPTIMIZERon h3, discard it since it is interior, and finally acceptthe extreme point h1.
Each execution of LINOPTI-MIZER requires O(NT ) time with the interior point6We assume that h1 .
.
.hN are not degenerate, i.e., that theycollectively span RD .
Otherwise, all points are necessarily onthe hull, yet some of them may not be uniquely maximized.0.0010.010.11101001000100001000002 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20QuickHullLPDimensionsSecondsFigure 2: Running times to exactly optimize N -best listswith an increasing number of dimensions.
To determinewhich feature vectors were on the hull, we use either linearprogramming (Karmarkar, 1984) or one of the most effi-cient convex hull computation tools (Barber et al, 1996).method of (Karmarkar, 1984), and since the mainloop may run O(N) times in the worst case, timecomplexity is O(N2T ).
Finally, Fig.
2 empiricallydemonstrates the effectiveness of a linear program-ming approach, which in practice is seldom affectedby D.3.1 Exact search: general caseWe now extend LP-MERT to the general case, inwhich we are optimizing multiple sentences at once.This creates an intricate optimization problem, sincethe inner summations over n = 1 .
.
.
N in Eq.
1can?t be optimized independently.
For instance,the optimal weight vector for sentence s = 1 maybe suboptimal with respect to sentence s = 2.So we need some means to determine whether aselection m = m(1) .
.
.m(S) ?
M = [1, N ]S offeature vectors h1,m(1) .
.
.hS,m(S) is extreme, that is,whether we can find a weight vector that maximizeseach hs,m(s).
Here is a reformulation of Eq.
1 thatmakes this condition on extremity more explicit:m?
= argminconv(h[m];H)m?M{ S?s=1E(rs, es,m(n))}(4)whereh[m] =S?s=1hs,m(s)H =?m??Mh[m?
]41One na?
?ve approach to address this optimizationproblem is to enumerate all possible combinationsamong the S distinct N -best lists, determine for eachcombination m whether h[m] is extreme, and returnthe extreme combination with lowest total loss.
It isevident that this approach is optimal (since it followsdirectly from Eq.
4), but it is prohibitively slow sinceit processes O(NS) vertices to determine whetherthey are extreme, which thus requires O(NST ) timeper LP optimization and O(N2ST ) time in total.
Wenow present several improvements to make this ap-proach more practical.3.1.1 Sparse hypothesis combinationIn the na?
?ve approach presented above, each LPcomputation to evaluate conv(h[m];H) requiresO(NST ) time since H contains NS vertices, butwe show here how to reduce it to O(NST ) time.This improvement exploits the fact that we can elimi-nate the majority of the NS points of H , since onlyS(N ?1)+1 are really needed to determine whetherh[m] is extreme.
This is best illustrated using an ex-ample, as shown in Fig.
3.
Both h1,1 and h2,1 in (a)and (b) are extreme with respect to their own N -bestlist, and we ask whether we can find a weight vectorthat maximizes both h1,1 and h2,1.
The algorith-mic trick is to geometrically translate one of the twoN -best lists so that h1,1 = h?2,1, where h?2,1 is thetranslation of h?2,1.
Then we use linear programmingwith the new set of 2N ?
1 points, as shown in (c), todetermine whether h1,1 is on the hull, in which casethe answer to the original question is yes.
In the caseof the combination of h1,1 and h2,2, we see in (d) thatthe combined set of points prevents the maximizationh1,1, since this point is clearly no longer on the hull.Hence, the combination (h1,1,h2,2) cannot be maxi-mized using any linear model.
This trick generalizesto S ?
2.
In both (c) and (d), we used S(N ?
1) + 1points instead of NS to determine whether a givenpoint is extreme.
We show in the appendix that thissimplification does not sacrifice optimality.3.1.2 Lazy enumeration, divide-and-conquerNow that we can determine whether a given combi-nation is extreme, we must next enumerate candidatecombinations to find the combination that has low-est task loss among all of those that are extreme.Since the number of feature vector combinations isO(NS), exhaustive enumeration is not a reasonableh1,1  h2,2  h2,1(a) (b)h1,1  h?2,2(c)  (d)h1,1 h?2,1Figure 3: Given two N -best lists, (a) and (b), we uselinear programming to determine which hypothesis com-binations are extreme.
For instance, the combination h1,1and h2,1 is extreme (c), while h1,1 and h2,2 is not (d).option.
Instead, we use lazy enumeration to pro-cess combinations in increasing order of task loss,which ensures that the first extreme combination fors = 1 .
.
.
S that we encounter is the optimal one.
AnS-ary lazy enumeration would not be particularly ef-ficient, since the runtime is still O(NS) in the worstcase.
LP-MERT instead uses divide-and-conquerand binary lazy enumeration, which enables us todiscard early on combinations that are not extreme.For instance, if we find that (h1,1,h2,2) is interior forsentences s = 1, 2, the divide-and-conquer branchfor s = 1 .
.
.
4 never actually receives this bad com-bination from its left child, thus avoiding the costof enumerating combinations that are known to beinterior, e.g., (h1,1,h2,2,h3,1,h4,1).The LP-MERT algorithm for the general case isshown as Algorithm 2.
It basically only calls a re-cursive divide-and-conquer function (GETNEXTBEST)for sentence range 1 .
.
.
S. The latter function uses bi-nary lazy enumeration in a manner similar to (Huangand Chiang, 2005), and relies on two global variables:I and L. The first of these, I , is used to memoize theresults of calls to GETNEXTBEST; given a range ofsentences and a rank n, it stores the nth best combina-tion for that range of sentences.
The global variableL stores hypotheses combination matrices, one ma-trix for each range of sentences (s, t) as shown in42h11 h12h21 h22 h2369.1 69.2 69.3 69.2 69.4 h31 h32 h33h41 h4256.8 57.1 57.3 57.6h2357.9{h11, h23}{h31, h41}126.0 126.5126.1{h32, h41}{h12, h21}Combinations checked:  {h11, h23, h31, h41} {h12, h21, h31, h41}Combinations discarded:  {h11, h21, h31, h41} {h12, h22, h31, h41} {h12, h12, h31, h42} (and 7 others) h13h2469.9 70.0L[3,4] L[1,2]Figure 4: LP-MERT minimizes loss (TER) on four sen-tences.
O(N4) translation combinations are possible,but the LP-MERT algorithm only tests two full combi-nations.
Without divide-and-conquer?i.e., using 4-arylazy enumeration?ten full combinations would have beenchecked unnecessarily.Algorithm 2: LP-MERTinput : feature vectors H = {hs,n}1?s?S;1?n?Ninput : task losses E = {Es,n}1?s?S;1?n?N ,where sent.-level costs Es,n := E(rs, es,n)output :optimal weight vector w?
and its loss L1 begin.
sort N -best lists by increasing losses:2 for s?
1 to S do3 (is,1..is,N )?
INDEXSORT(Es,1..Es,N ).
find best hypothesis combination for 1 .
.
.
S:4 (h?, H?, L)?
GETNEXTBEST(H,E, 1, S)5 w??
LINOPTIMIZER(h?;H?
)6 return (w?, L)Fig.
4, to determine which combination to try next.The function EXPANDFRONTIER returns the indicesof unvisited cells that are adjacent (right or down) tovisited cells and that might correspond to the nextbest hypothesis.
Once no more cells need to be addedto the frontier, LP-MERT identifies the lowest losscombination on the frontier (BESTINFRONTIER), anduses LP to determine whether it is extreme.
To do so,it first generates an LP using COMBINE, a functionthat implements the method described in Fig.
3.
Ifthe LP offers no solution, this combination is ignored.LP-MERT iterates until it finds a cell entry whosecombination is extreme.
Regarding ranges of lengthone (s = t), lines 3-10 are similar to Algorithm 1 forS = 1, but with one difference: GETNEXTBEST maybe called multiple times with the same argument s,since the first output of GETNEXTBEST might not beextreme when combined with other feature vectors.Lines 3-10 of GETNEXTBEST handle this case effi-ciently, since the algorithm resumes at the (n+1)-thFunction GetNextBest(H,E,s,t)input : sentence range (s, t)output :h?
: current best extreme vertexoutput :H?
: constraint verticesoutput :L: task loss of h?.
Losses of partial hypotheses:1 L?
L[s, t]2 if s = t then.
n is the index where we left off last time:3 n?
NBROWS(L)4 Hs ?
{hs,1 .
.
.hs,N}5 repeat6 n?
n+ 17 w??
LINOPTIMIZER(hs,in ;Hs)8 L[n, 1]?
Es,in9 until w?
6= 010 return (hs,in , Hs,L[n, 1])11 else12 u?
b(s+ t)/2c, v ?
u+ 113 repeat14 while HASINCOMPLETEFRONTIER(L) do15 (m,n)?
EXPANDFRONTIER(L)16 x?
NBROWS(L)17 y ?
NBCOLUMNS(L)18 form?
?
x+ 1 tom do19 I[s, u,m?]?
GETNEXTBEST(H,E, s, u)20 for n?
?
y + 1 to n do21 I[v, t, n?]?
GETNEXTBEST(H,E, v, t)22 L[m,n]?
LOSS(I[s, u,m])+LOSS(I[v, t, n])23 (m,n)?
BESTINFRONTIER(L)24 (hm, Hm, Lm)?
I[s, u,m]25 (hn, Hn, Ln)?
I[v, t, n]26 (h?, H?)?
COMBINE(hm, Hm,hn, Hn)27 w??
LINOPTIMIZER(h?;H?
)28 until w?
6= 029 return (h?, H?,L[m,n])element of the N -best list (where n is the positionwhere the previous execution left off).7 We can seethat a strength of this algorithm is that inconsistentcombinations are deleted as soon as possible, whichallows us to discard fruitless candidates en masse.3.2 Approximate SearchWe will see in Section 5 that our exact algorithmis often too computationally expensive in practiceto be used with either a large number of sentencesor a large number of features.
We now present two7Each N -best list is augmented with a placeholder hypothesiswith loss +?.
This ensures n never runs out of bounds at line 7.43Function Combine(h, H,h?, H ?
)input :H,H ?
: constraint verticesinput :h,h?
: extreme vertices, wrt.
H and H ?output :h?, H?
: combination as in Sec.
3.1.11 for i?
1 to size(H) do2 Hi ?
Hi + h?3 for i?
1 to size(H ?)
do4 H ?i ?
H ?i + h5 return (h+ h?, H ?H ?
)approaches to make LP-MERT more scalable, withthe downside that we may allow search errors.In the first case, we make the assumption that wehave an initial weight vector w0 that is a reasonableapproximation of w?, where w0 may be obtained ei-ther by using a fast MERT algorithm like 1D-MERT,or by reusing the weight vector that is optimal withrespect to the previous iteration of MERT.
The ideathen is to search only the set of weight vectors thatsatisfy cos(w?,w0) ?
t, where t is a threshold oncosine similarity provided by the user.
The larger thet, the faster the search, but at the expense of moresearch errors.
This is implemented with two simplechanges in our algorithm.
First, LINOPTIMIZER setsthe objective vector c = w0.
Second, if the outputw?
originally returned by LINOPTIMIZER does notsatisfy cos(w?,w0) ?
t, then it returns 0.
While thismodification of our algorithm may lead to searcherrors, it nevertheless provides some theoretical guar-antee: our algorithm finds the global optimum if itlies within the region defined by cos(w?,w0) ?
t.The second method is a beam approximation of LP-MERT, which normally deals with linear programsthat are increasingly large in the upper branches ofGETNEXTBEST?s recursive calls.
The main idea isto prune the output of COMBINE (line 26) by modelscore with respect to wbest, where wbest is our cur-rent best model on the entire tuning set.
Note thatbeam pruning can discard h?
(the current best ex-treme vertex), in which case LINOPTIMIZER returns0.
wbest is updated as follows: each time we pro-duce a new non-zero w?, run wbest ?
w?
if w?
has alower loss than wbest on the entire tuning set.
Theidea of using a beam here is similar to using cosinesimilarity (since wbest constrains the search towardsa promising region), but beam pruning also helpsreduce LP optimization time and thus enables us toexplore a wider space.
Since wbest often improvesduring search, it is useful to run multiple iterations ofLP-MERT until wbest doesn?t change.
Two or threeiterations suffice in our experience.
In our experi-ments, we use a beam size of 1000.4 Experimental SetupOur experiments in this paper focus on only the ap-plication of machine translation, though we believethat the current approach is agnostic to the particularsystem used to generate hypotheses.
Both phrase-based systems (e.g., Koehn et al (2007)) and syntax-based systems (e.g., Li et al (2009), Quirk et al(2005)) commonly use MERT to train free param-eters.
Our experiments use a syntax-directed trans-lation approach (Quirk et al, 2005): it first appliesa dependency parser to the source language data atboth training and test time.
Multi-word translationmappings constrained to be connected subgraphs ofthe source tree are extracted from the training data;these provide most lexical translations.
Partially lexi-calized templates capturing reordering and functionword insertion and deletion are also extracted.
Atruntime, these mappings and templates are used toconstruct transduction rules to convert the source treeinto a target string.
The best transduction is soughtusing approximate search techniques (Chiang, 2007).Each hypothesis is scored by a relatively standardset of features.
The mappings contain five features:maximum-likelihood estimates of source given targetand vice versa, lexical weighting estimates of sourcegiven target and vice versa, and a constant value that,when summed across a whole hypothesis, indicatesthe number of mappings used.
For each template,we include a maximum-likelihood estimate of thetarget reordering given the source structure.
Thesystem may fall back to templates that mimic thesource word order; the count of such templates is afeature.
Likewise we include a feature to count thenumber of source words deleted by templates, and afeature to count the number of target words insertedby templates.
The log probability of the target stringaccording to a language models is also a feature; weadd one such feature for each language model.
Weinclude the number of target words as features tobalance hypothesis length.For the present system, we use the training data ofWMT 2010 to construct and evaluate an English-to-44-1012345670 100 200 300 400 500 600 700 800 900 1000S=8S=4S=2?BLEU[%]Figure 5: Line graph of sorted differences inBLEUn4r1[%] scores between LP-MERT and 1D-MERTon 1000 tuning sets of size S = 2, 4, 8.
The highest differ-ences for S = 2, 4, 8 are respectively 23.3, 19.7, 13.1.German translation system.
This consists of approx-imately 1.6 million parallel sentences, along with amuch larger monolingual set of monolingual data.We train two language models, one on the target sideof the training data (primarily parliamentary data),and the other on the provided monolingual data (pri-marily news).
The 2009 test set is used as develop-ment data for MERT, and the 2010 one is used as testdata.
The resulting system has 13 distinct features.5 ResultsThe section evaluates both the exact and beam ver-sion of LP-MERT.
Unless mentioned otherwise, thenumber of features isD = 13 and theN -best list sizeis 100.
Translation performance is measured witha sentence-level version of BLEU-4 (Lin and Och,2004), using one reference translation.
To enablelegitimate comparisons, LP-MERT and 1D-MERTare evaluated on the same combined N -best lists,even though running multiple iterations of MERTwith either LP-MERT or 1D-MERT would normallyproduce different combined N -best lists.
We useWMT09 as tuning set, and WMT10 as test set.
Be-fore turning to large tuning sets, we first evaluateexact LP-MERT on data sizes that it can easily han-dle.
Fig.
5 offers a comparison with 1D-MERT, forwhich we split the tuning set into 1,000 overlappingsubsets for S = 2, 4, 8 on a combined N -best afterfive iterations of MERT with an average of 374 trans-lation per sentence.
The figure shows that LP-MERTnever underperforms 1D-MERT in any of the 3,000experiments, and this almost certainly confirms thatlength tested comb.
total comb.
order8 639,960 1.33?
1020 O(N8)4 134,454 2.31?
1010 O(2N4)2 49,969 430,336 O(4N2)1 1,059 2,624 O(8N)Table 1: Number of tested combinations for the experi-ments of Fig.
5.
LP-MERT with S = 8 checks only 600Kfull combinations on average, much less than the totalnumber of combinations (which is more than 1020).1101001,00010,0002 3 4 5 6 7 8 9seconds10242561286432168421dimension (D)Figure 6: Effect of the number of features (runtime on1 CPU of a modern computer).
Each curve represents adifferent number of tuning sentences.LP-MERT systematically finds the global optimum.In the case S = 1, Powell rarely makes search er-rors (about 15%), but the situation gets worse as Sincreases.
For S = 4, it makes search errors in 90%of the cases, despite using 20 random starting points.Some combination statistics for S up to 8 areshown in Tab.
1.
The table shows the speedup pro-vided by LP-MERT is very substantial when com-pared to exhaustive enumeration.
Note that this isusing D = 13, and that pruning is much more ef-fective with less features, a fact that is confirmed inFig.
6.
D = 13 makes it hard to use a large tuningset, but the situation improves with D = 2 .
.
.
5.Fig.
7 displays execution times when LP-MERTconstrains the output w?
to satisfy cos(w0, w?)
?
t,where t is on the x-axis of the figure.
The figureshows that we can scale to 1000 sentences when(exactly) searching within the region defined bycos(w0, w?)
?
.84.
All these running times wouldimprove using parallel computing, since divide-and-conquer algorithms are generally easy to parallelize.We also evaluate the beam version of LP-MERT,which allows us to exploit tuning sets of reasonable451101001,00010,0000.99 0.98 0.96 0.92 0.84 0.68 0.36 -0.28 -1seconds10245122561286432168421cosineFigure 7: Effect of a constraint on w (runtime on 1 CPU).32 64 128 256 512 10241D-MERT 22.93 20.70 18.57 16.07 15.00 15.44our work 25.25 22.28 19.86 17.05 15.56 15.67+2.32 +1.59 +1.29 +0.98 +0.56 +0.23Table 2: BLEUn4r1[%] scores for English-German onWMT09 for tuning sets ranging from 32 to 1024 sentences.size.
Results are displayed in Table 2.
The gainsare fairly substantial, with gains of 0.5 BLEU pointor more in all cases where S ?
512.8 Finally, weperform an end-to-end MERT comparison, whereboth our algorithm and 1D-MERT are iteratively usedto generate weights that in turn yield newN -best lists.Tuning on 1024 sentences of WMT10, LP-MERTconverges after seven iterations, with a BLEU scoreof 16.21%; 1D-MERT converges after nine iterations,with a BLEU score of 15.97%.
Test set performanceon the full WMT10 test set for LP-MERT and 1D-MERT are respectively 17.08% and 16.91%.6 Related WorkOne-dimensional MERT has been very influential.
Itis now used in a broad range of systems, and has beenimproved in a number of ways.
For instance, latticesor hypergraphs may be used in place of N -best liststo form a more comprehensive view of the searchspace with fewer decoding runs (Macherey et al,2008; Kumar et al, 2009; Chatterjee and Cancedda,2010).
This particular refinement is orthogonal to ourapproach, though.
We expect to extend LP-MERT8One interesting observation is that the performance of 1D-MERT degrades as S grows from 2 to 8 (Fig.
5), which contrastswith the results shown in Tab.
2.
This may have to do with thefact that N -best lists with S = 2 have much fewer local maximathan with S = 4, 8, in which case 20 restarts is generally enough.to hypergraphs in future work.
Exact search may bechallenging due to the computational complexity ofthe search space (Leusch et al, 2008), but approxi-mate search should be feasible.Other research has explored alternate methodsof gradient-free optimization, such as the downhill-simplex algorithm (Nelder and Mead, 1965; Zenset al, 2007; Zhao and Chen, 2009).
Although thesearch space is different than that of Och?s algorithm,it still relies on one-dimensional line searches to re-flect, expand, or contract the simplex.
Therefore, itsuffers the same problems of one-dimensional MERT:feature sets with complex non-linear interactions aredifficult to optimize.
LP-MERT improves on thesemethods by searching over a larger subspace of pa-rameter combinations, not just those on a single line.We can also change the objective function in anumber of ways to make it more amenable to op-timization, leveraging knowledge from elsewherein the machine learning community.
Instance re-weighting as in boosting may lead to better param-eter inference (Duh and Kirchhoff, 2008).
Smooth-ing the objective function may allow differentiationand standard ML learning techniques (Och and Ney,2002).
Smith and Eisner (2006) use a smoothed ob-jective along with deterministic annealing in hopesof finding good directions and climbing past locallyoptimal points.
Other papers use margin methodssuch as MIRA (Watanabe et al, 2007; Chiang et al,2008), updated somewhat to match the MT domain,to perform incremental training of potentially largenumbers of features.
However, in each of these casesthe objective function used for training no longermatches the final evaluation metric.7 ConclusionsOur primary contribution is the first known exactsearch algorithm for direct loss minimization on N -best lists in multiple dimensions.
Additionally, wepresent approximations that consistently outperformstandard one-dimensional MERT on a competitivemachine translation system.
While Och?s method ofMERT is generally quite successful, there are caseswhere it does quite poorly.
A more global searchsuch as LP-MERT lowers the expected risk of suchpoor solutions.
This is especially important for cur-rent machine translation systems that rely heavily onMERT, but may also be valuable for other textual ap-46plications.
Recent speech recognition systems havealso explored combinations of more acoustic and lan-guage models, with discriminative training of 5-10features rather than one million (Lo?o?f et al, 2010);LP-MERT could be valuable here as well.The one-dimensional algorithm of Och (2003)has been subject to study and refinement for nearlya decade, while this is the first study of multi-dimensional approaches.
We demonstrate the poten-tial of multi-dimensional approaches, but we believethere is much room for improvement in both scalabil-ity and speed.
Furthermore, a natural line of researchwould be to extend LP-MERT to compact representa-tions of the search space, such as hypergraphs.There are a number of broader implications fromthis research.
For instance, LP-MERT can aid in theevaluation of research on MERT.
This approach sup-plies a truly optimal vector as ground truth, albeitunder limited conditions such as a constrained direc-tion set, a reduced number of features, or a smallerset of sentences.
Methods can be evaluated based onnot only improvements over prior approaches, butalso based on progress toward a global optimum.AcknowledgementsWe thank Xiaodong He, Kristina Toutanova, andthree anonymous reviewers for their valuable sug-gestions.Appendix A: Proof of optimalityIn this appendix, we prove that LP-MERT (Algorithm 2)is exact.
As noted before, the na?
?ve approach of solvingEq.
4 is to enumerate allO(NS) hypotheses combinationsinM, discard the ones that are not extreme, and returnthe best scoring one.
LP-MERT relies on algorithmicimprovements to speed up this approach, and we now showthat none of them affect the optimality of the solution.Divide-and-conquer.
Divide-and-conquer in Algo-rithm 2 discards any partial hypothesis combinationh[m(j) .
.
.m(k)] if it is not extreme, even before consid-ering any extension h[m(i) .
.
.m(j) .
.
.m(k) .
.
.m(l)].This does not sacrifice optimality, since if conv(h;H)is false, then conv(h;H ?G) is false for any set G.Proof: Assume conv(h;H) is false, so h is interior toH .
By definition, any interior point h can be written asa linear combination of other points: h =?i ?ihi, with?i(hi ?
H , hi 6= h, ?i ?
0) and ?i ?i = 1.
This samecombination of points also demonstrates that h is interiorto H ?G, thus conv(h;H ?G) is false as well.Sparse hypothesis combination.
We show herethat the simplification of linear programs in Section 3.1.1from size O(NS) to size O(NS) does not change thevalue of conv(h;H).
More specifically, this means thatlinear optimization of the output of the COMBINE methodat lines 26-27 of function GETNEXTBEST does notintroduce any error.
Let (g1 .
.
.gU ) and (h1 .
.
.hV ) betwo N -best lists to be combined, then:conv(gu + hv;U?i=1(gi + hv) ?V?j=1(gu + hj))= conv(gu + hv;U?i=1V?j=1(gi + hj))Proof: To prove this equality, it suffices to show that: (1)if gu+hv is interior wrt.
the first conv binary predicatein the above equation, then it is interior wrt.
the secondconv, and (2) if gu+hv is interior wrt.
the second conv,then it is interior wrt.
the first conv.
Claim (1) is evident,since the set of points in the first conv is a subset of theother set of points.
Thus, we only need to prove (2).
Wefirst geometrically translate all points by ?gu?hv .
Sincegu+hv is interior wrt.
the second conv, we can write:0 =U?i=1V?j=1?i,j(gi + hj ?
gu ?
hv)=U?i=1V?j=1?i,j(gi ?
gu) +U?i=1V?j=1?i,j(hj ?
hv)=U?i=1(gi ?
gu)V?j=1?i,j +V?j=1(hj ?
hv)U?i=1?i,j=U?i=1?
?i(gi ?
gu) +V?j=1?
?U+j(hj ?
hv)where {?
?i}1?i?U+V values are computed from{?i,j}1?i?U,1?j?V as follows: ?
?i =?j ?i,j , i ?
[1, U ]and ?
?U+j =?i ?i,j , j ?
[1, V ].
Since the interiorpoint is 0, ?
?i values can be scaled so that they sum to 1(necessary condition in the definition of interior points),which proves that the following predicate is false:conv(0;U?i=1(gi ?
gu) ?V?j=1(hj ?
hv))which is equivalent to stating that the following is false:conv(gu + hv;U?i=1(gi + hv) ?V?j=1(gu + hj))47ReferencesC.
Bradford Barber, David P. Dobkin, and Hannu Huhdan-paa.
1996.
The QuickHull algorithm for convex hulls.ACM Trans.
Math.
Softw., 22:469?483.C.
Buchta, J. Muller, and R. F. Tichy.
1985.
Stochasticalapproximation of convex bodies.
Math.
Ann., 271:225?235.A.
Bykat.
1978.
Convex hull of a finite set of points intwo dimensions.
Inf.
Process.
Lett., 7(6):296?298.Daniel Cer, Dan Jurafsky, and Christopher D. Manning.2008.
Regularization and search for minimum errorrate training.
In Proceedings of the Third Workshop onStatistical Machine Translation, pages 26?34.Samidh Chatterjee and Nicola Cancedda.
2010.
Min-imum error rate training by sampling the translationlattice.
In Proceedings of the 2010 Conference on Em-pirical Methods in Natural Language Processing, pages606?615.
Association for Computational Linguistics.David Chiang, Yuval Marton, and Philip Resnik.
2008.Online large-margin training of syntactic and structuraltranslation features.
In EMNLP.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?228.W.
Chou, C. H. Lee, and B. H. Juang.
1993.
Minimumerror rate training based on N-best string models.
InProc.
IEEE Int?l Conf.
Acoustics, Speech, and SignalProcessing (ICASSP ?93), pages 652?655, Vol.
2.Kevin Duh and Katrin Kirchhoff.
2008.
Beyond log-linear models: boosted minimum error rate training forprogramming N-best re-ranking.
In Proceedings of the46th Annual Meeting of the Association for Computa-tional Linguistics on Human Language Technologies:Short Papers, pages 37?40, Stroudsburg, PA, USA.William F. Eddy.
1977.
A new convex hull algorithm forplanar sets.
ACM Trans.
Math.
Softw., 3:398?403.Liang Huang and David Chiang.
2005.
Better k-best pars-ing.
In Proceedings of the Ninth International Work-shop on Parsing Technology, pages 53?64, Stroudsburg,PA, USA.Biing-Hwang Juang, Wu Hou, and Chin-Hui Lee.
1997.Minimum classification error rate methods for speechrecognition.
Speech and Audio Processing, IEEE Trans-actions on, 5(3):257?265.N.
Karmarkar.
1984.
A new polynomial-time algorithmfor linear programming.
Combinatorica, 4:373?395.Victor Klee.
1966.
Convex polytopes and linear program-ming.
In Proceedings of the IBM Scientific ComputingSymposium on Combinatorial Problems.Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,Christopher Callison-Burch, Marcello Federico, NicolaBertoldi, Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InProc.
of ACL, Demonstration Session.Shankar Kumar, Wolfgang Macherey, Chris Dyer, andFranz Och.
2009.
Efficient minimum error rate train-ing and minimum Bayes-risk decoding for translationhypergraphs and lattices.
In Proceedings of the JointConference of the 47th Annual Meeting of the ACLand the 4th International Joint Conference on NaturalLanguage Processing of the AFNLP: Volume 1, pages163?171.Gregor Leusch, Evgeny Matusov, and Hermann Ney.2008.
Complexity of finding the BLEU-optimal hy-pothesis in a confusion network.
In Proceedings of theConference on Empirical Methods in Natural LanguageProcessing, pages 839?847, Stroudsburg, PA, USA.Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Ganitke-vitch, Sanjeev Khudanpur, Lane Schwartz, Wren N. G.Thornton, Jonathan Weese, and Omar F. Zaidan.
2009.Joshua: an open source toolkit for parsing-based MT.In Proc.
of WMT.P.
Liang, A.
Bouchard-Co?te?, D. Klein, and B. Taskar.2006.
An end-to-end discriminative approach to ma-chine translation.
In International Conference on Com-putational Linguistics and Association for Computa-tional Linguistics (COLING/ACL).Chin-Yew Lin and Franz Josef Och.
2004.
ORANGE:a method for evaluating automatic evaluation metricsfor machine translation.
In Proceedings of the 20thinternational conference on Computational Linguistics,Stroudsburg, PA, USA.Jonas Lo?o?f, Ralf Schlu?ter, and Hermann Ney.
2010.
Dis-criminative adaptation for log-linear acoustic models.In INTERSPEECH, pages 1648?1651.Wolfgang Macherey, Franz Och, Ignacio Thayer, andJakob Uszkoreit.
2008.
Lattice-based minimum errorrate training for statistical machine translation.
In Pro-ceedings of the 2008 Conference on Empirical Methodsin Natural Language Processing, pages 725?734.David McAllester, Tamir Hazan, and Joseph Keshet.
2010.Direct loss minimization for structured prediction.
InAdvances in Neural Information Processing Systems23, pages 1594?1602.Ryan McDonald, Koby Crammer, and Fernando Pereira.2005.
Online large-margin training of dependencyparsers.
In Proceedings of the 43rd Annual Meetingon Association for Computational Linguistics, pages91?98.Ryan McDonald.
2006.
Discriminative sentence compres-sion with soft syntactic constraints.
In Proceedings ofEACL, pages 297?304.Robert C. Moore and Chris Quirk.
2008.
Random restartsin minimum error rate training for statistical machinetranslation.
In Proceedings of the 22nd International48Conference on Computational Linguistics - Volume 1,pages 585?592.J.
A. Nelder and R. Mead.
1965.
A simplex method forfunction minimization.
Computer Journal, 7:308?313.Franz Josef Och and Hermann Ney.
2002.
Discriminativetraining and maximum entropy models for statisticalmachine translation.
In Proc.
of the 40th Annual Meet-ing of the Association for Computational Linguistics,pages 295?302.Franz Josef Och.
2003.
Minimum error rate training forstatistical machine translation.
In Proc.
of ACL.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
BLEU: a method for automatic evalu-ation of machine translation.
In Proc.
of ACL.M.J.D.
Powell.
1964.
An efficient method for findingthe minimum of a function of several variables withoutcalculating derivatives.
Comput.
J., 7(2):155?162.William H. Press, Saul A. Teukolsky, William T. Vetter-ling, and Brian P. Flannery.
2007.
Numerical Recipes:The Art of Scientific Computing.
Cambridge UniversityPress, 3rd edition.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.Dependency treelet translation: syntactically informedphrasal SMT.
In Proc.
of ACL, pages 271?279.David A. Smith and Jason Eisner.
2006.
Minimum riskannealing for training log-linear models.
In Proceed-ings of the COLING/ACL on Main conference postersessions, pages 787?794, Stroudsburg, PA, USA.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.
InProc.
of AMTA, pages 223?231.Andreas Stolcke, Yochai Knig, and Mitchel Weintraub.1997.
Explicit word error minimization in N-best listrescoring.
In In Proc.
Eurospeech, pages 163?166.Taro Watanabe, Jun Suzuki, Hajime Tsukada, and HidekiIsozaki.
2007.
Online large-margin training for statisti-cal machine translation.
In EMNLP-CoNLL.Omar F. Zaidan and Chris Callison-Burch.
2009.
Feasibil-ity of human-in-the-loop minimum error rate training.In Proceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing: Volume 1 -Volume 1, pages 52?61.Richard Zens, Sasa Hasan, and Hermann Ney.
2007.A systematic comparison of training criteria for sta-tistical machine translation.
In Proceedings of the2007 Joint Conference on Empirical Methods in Natu-ral Language Processing and Computational NaturalLanguage Learning (EMNLP-CoNLL), pages 524?532,Prague, Czech Republic.Bing Zhao and Shengyuan Chen.
2009.
A simplex Armijodownhill algorithm for optimizing statistical machinetranslation decoding parameters.
In Proceedings ofHuman Language Technologies: The 2009 Annual Con-ference of the North American Chapter of the Associ-ation for Computational Linguistics, Companion Vol-ume: Short Papers, pages 21?24.49
