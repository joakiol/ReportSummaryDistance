Proceedings of the 2010 Workshop on NLP and Linguistics: Finding the Common Ground, ACL 2010, pages 22?24,Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational LinguisticsMatching needs and resources:How NLP can help theoretical linguisticsAlexis DimitriadisUtrecht institute of Linguistics OTSa.dimitriadis@uu.nlAbstractWhile some linguistic questions pose chal-lenges that could be met by developingand applying NLP techniques, other prob-lems can best be approached with a blendof old-fashioned linguistic investigationand the use of simple, well-establishedNLP tools.
Unfortunately, this means thatthe NLP component is too simple to beof interest to the computationally-minded,while existing tools are often difficult forthe programming novice to use.
For NLPto come to the aid of research in theoreti-cal linguistics, a continuing investment ofeffort is required to bridge the gap.
Thisinvestment can be made from both sides.1 IntroductionLinguistics is in its heart an empirical discipline,and the data management and data analysis tech-niques of computational linguistics could, in prin-ciple, be productively brought to bear on descrip-tive and theoretical questions.
That this does nothappen as much as it could is, as I understand it,the point of departure for this colloquium.
Insteadof focusing on exciting research questions that arecrying out for fruitful collaboration between the-oretical and computational linguists, I want to ex-amine the broader range of ways that NLP know-how could be put to productive use in the domainof theoretical linguistics, and some of the waysthat this could come to happen more.In brief, I believe that the lack of interaction isnot simply due to lack of interest, or lack of infor-mation, on both sides.
Rather, the goals and needsof computational interests are not always servedwell by catering to the community of theoreticaland descriptive linguists, the so-called ?OrdinaryWorking Linguists?
with a minimum of computa-tional skills and (equally important) no direct in-terest in computational questions.Such linguists could draw a lot of benefit fromboring, old-hat NLP tools that computational lin-guists take for granted: searchable parsed corpora,tools to search large collections of text or com-pute lexicostatistics, online questionnaire tools forcollecting and analyzing speaker judgements, etc.Computational linguists have ready access to anumber of wonderful tools of this sort.
In factthese are often the building blocks and resourceson which new applications at the forefront of NLPare built: Who would build a text summarizationsystem without access to a large corpus of text topractice on?But such uses of NLP are too simple to be of in-terest from the computational standpoint.
Search-ing a huge corpus for particular syntactic struc-tures could be invaluable to a syntactician, butmaking this possible is not interesting to a compu-tational linguist: it?s not research anymore.
Thisshould not be taken to suggest, however, that com-putational linguists ought to become more ?altru-istic.?
Creating tools targeted to non-technical lin-guists, even successful tools, can still have draw-backs in the long run.2 The Linguist?s Search EngineThe Linguist?s Search Engine (Resnik et al 2004)is an example of an application created for the ben-efit of ordinary, non-technical linguists.
It allowedusers to search the web for a specified syntacticstructure.
Out of view of the user, the engine firstexecuted an ordinary word-match web search andthen parsed the hits and matched against the searchstructure.
The user interface (a java application)allowed the query term to be graphically con-structed and refined (?query by example?).
Theauthors?
goal was to create a true web application:Easy to launch from a web browser, and easy touse without lengthy user manuals or a complicatedcommand language.
While the user interface wasinnovative, its linguistic function was not: The ap-22plication provided a web interface to a collectionof tools that had been assembled to support struc-tured searches.
The application stagnated after theend of the project, and ceased working altogetheras of April, 2010.While it was operating, the LSE was used asintended: Resnik et al report on a number ofcase studies of users who independently used thesearch engine to carry out linguistic research.
Un-fortunately, however, the burden of maintenanceturned out to be too great for an application that isof no real continuing interest for a computationallinguist.2.1 The cost of new toolsComplex resources are difficult to create and canbe difficult to use.
In the world of Language Re-sources, large corpora are created by the millionsof words in various standardized formats, often inconjunction with integrated mega-tools for access-ing and managing them.
But language resourcesare geared for institutional clients, can cost a lotof money, and are not acquired or used effectivelyby individuals without access to dedicated IT sup-port.At the frontier of NLP, on the other hand, toolsdon?t usually come shrink-wrapped with graphicalinstallers.
They often don?t come with a graphicalinterface at all.
A new research project may in-volve a new workflow to be created.
Needed cor-pora will be bought, shared or created as needed.A typical project will involve a jumble of file for-mats, filters, and workflows that manage text in adhoc ways until the sought-for result is perfected.Making such a tool available to someone out-side the project, even another computational lin-guist, is a time-consuming enterprise.
Like anycomplicated body of software, it needs to be doc-umented, encapsulated, and then configured andunderstood by its new users.
This requires a con-siderable time investment which an NLP lab iswilling to undertake, but which is of dubious util-ity to a theoretical linguist?
even one who hasthe computer skills necessary to undertake it.
Inbrief, the expected amount of use must justify theinvestment in setting up and learning the system.Tagging, parsing and tree-searching programs arecommonplace, but setting up a system for one?sown use is a non-trivial exercise.
A syntacticianlooking for a few examples of a rare constructionmay prefer trial and error on google instead of try-ing to get a complex system to compile.
A syntac-tician looking for similar data from multiple lan-guages is even less likely to take the plunge, sincethe benefit derived from a single language is pro-portionally reduced.3 Services and interoperabilityWith the goal of reducing the burden of installingcomplex resources and getting them to talk toeach other, the CLARIN program (Common Lan-guage Resources and Technology Infrastructure)is working to establish a cutting edge infrastruc-ture of standards and protocols, which will allowlanguage resources and applications to be utilizedremotely, and workflows to be constructed interac-tively in (hopefully) intuitive ways.
The vision isto be able to gain remote access to a language cor-pus, couple it to a processing application (perhapsan experimental parser using a new syntactic anal-ysis), send the results to yet another application foranalysis, etc.It would be great to have ready access to thetools and resources envisioned for the network.But will it be a platform for development of ex-perimental applications by tomorrow?s computa-tional linguists, or will the command line con-tinue to compete with web services as an inter-face?
The answer probably depends on the ben-efits that CLARIN (and any such framework) willoffer to researcher-developers.
If adopted, it offershopes of opening up the computational linguist?stoolbox to a wider range of users.4 Helping ourselvesWouldn?t it be great to have a simple tool for exe-cuting simple web searches, converting hits intoflat text and compiling the results into a simplecorpus?
Throw in a tagger, a parser and a searchapplication, and we have the functionality of theLinguist?s Search Engine but in several pieces.Tools for most of these tasks are already widelyavailable, but only as part of a complex infrastruc-ture that requires skill and non-trivial time invest-ment to deploy.
Other tasks are solved over andover on an ad hoc basis, according to the needs ofeach NLP project.
Until the vision of CLARINbecomes reality, ordinary linguists without accessto a team of developers are out of luck.Still, we need not agree with the perspective(held by Resnik et al 2005, inter alia) that toolsfor linguists should be point-and-click and really23easy for an untrained user to figure out.
Setting thebar that high greatly shrinks the pool of compu-tational linguists willing to write software for thenon-technical masses.
The life cycle of the Lin-guist?s Search Engine is a case in point.Instead, linguists should meet the new technol-ogy halfway: As Bird (2006) has argued, no inte-grated tools can be expected to provide the flexi-bility needed for the creativity of original research.The NLTK (Natural Language Toolkit) is a moreflexible alternative: It is a python library provid-ing a high-level scripting environment for interac-tive linguistic exploration, with a reasonably smallamount of technical skill required.
Crucially, theNLTK comes with a very accessible book (Birdet al 2009) that allows an ?ordinary working lin-guist?
to learn how to use the system.The NLTK will still be beyond the reach of lin-guists unable, or unwilling, to make the neces-sary time investment.
Is this a big problem?
Ibelieve that it should be addressed by persuad-ing linguists (especially junior and future ones) ofthe benefits of achieving a minimal level of com-putational competence.
The availability of moretools that are usable and installable with a mod-erate investment in training, time and equipmentwould encourage linguists to make this kind of in-vestment, and would in the long run decrease thesupport burden for those technology folks who tryto make life easier for non-programming linguists.Conversely, computational linguists would hope-fully be encouraged to package their programs ina reasonably accessible format if a growing num-ber of potential users is clamoring for them?
and if?packaging?
need not mean a complete point-and-click interface.On the subject of command-line tools, I believethat the obstacle is not with the command lineper se (anyone can learn to open a terminal win-dow and type a few symbols), but with the power-ful and flexible workflows that the command linemakes possible.
This is the bread and butter ofthe computational linguist (and of any program-mer), and its benefits could belong to descriptiveand theoretical linguists as well.Theoretical linguistics, of course, also has NLPneeds that are anything but trivial.
At UiL-OTSthere are projects underway to model the acquisi-tion of phonotactic constraints; to improve textualentailments (in a linguistically informative way)by taking into account the contribution of lexicalmeaning; and others.
These and other projectscan provide challenges that a computational lin-guist can be happy to tackle.
But for theoreticallinguistics to fully benefit from NLP, we theoreti-cal linguists need to pick up more of the tools ofthe computational linguist.ReferencesBird, Steven.
2006.
?Linguistic Data Managementwith the Natural Language Toolkit.?
Plenary talk atthe Annual Meeting of the DGfS, Universita?t Biele-feld.Bird, Steven, Ewan Klein, and Edward Loper.
2006.Natural Language Processing with Python: An-alyzing Text with the Natural Language Toolkit.O?Reilly Media.CLARIN.
Common Language Resources and Technol-ogy Infrastructure.
http://www.clarin.eu/.Resnik, Philip, Aaron Elkiss, Ellen Lau, and HeatherTaylor.
2005.
?The Web in Theoretical LinguisticsResearch: Two Case Studies Using the Linguist?sSearch Engine.?
31st Meeting of the Berkeley Lin-guistics Society, pp.
265-276.24
