The Need for Accurate Alignment inNatural Language System EvaluationAndrew Kehler*UC San DiegoDouglas Appelt*SRI InternationalJohn Bear*SRI InternationalAs evaluations of computational linguistics technology progress toward higher-level interpre-tation tasks, the problem o/determining alignments between system responses and answer keyentries may become l ss straightforward.
We present an extensive analysis o/the alignment pro-cedure used in the MUC-6 evaluation o/information extraction technology, which reveals effectsthat interfere with the stated goals of the evaluation.
These ffects are shown to be pervasive noughthat they have the potential to adversely impact he technology development process.
These resultsargue strongly/or the use o/accurate alignment criteria in natural anguage valuations, and/ormaintaining the independence o/alignment criteria and mechanisms used to calculate scores.1.
IntroductionIt would be hard to overestimate he influence of evaluation on current natural an-guage processing (NLP) research and development.
In contrast o the primarily qual-itative methodologies that characterized research in the 1980's, purely quantitativeevaluation methods now pervade all aspects of the research and development processin many areas of NLP.
These roles are well summarized by Chinchor and Dungca(1995), who refer specifically to the methods used for the U.S.-government-sponsoredMessage Understanding Conference (MUC) evaluations of information extraction (IE)technology:The resulting scores \[assigned by the MUC evaluation process\] areused for decision-making over the entire evaluation cycle, includingrefinement of the task definition based on interannotator comparisons,technology development using training data, validating answer keys,and benchrnarking both system and human capabilities on the testdata.
(p. 33)This passage highlights three major roles an evaluation method can serve.
First,the method may be used during the process of task definition, to assess interannotatoragreement on proposed task specifications and revise them accordingly, and to sub-sequently validate the final answer keys.
Second, the method may be used to drive* Department of Linguistics #0108, University of California, San Diego, 9500 Gilman Drive, La Jolla, CA92093-0108.
E-mail: kehler@ling.ucsd.eduj" Artificial Intelligence C nter, 333 Ravenswood Avenue, Menlo Park, CA 94025.
E-mail: bear@ai.sri.com.~: Artificial Intelligence C nter, 333 Ravenswood Avenue, Menlo Park, CA 94025.
E-maihappelt@ai.sri.com.?
2001 Association for Computational LinguisticsComputational Linguistics Volume 27, Number 2the system development process, as system developers and machine learning algo-rithms rely heavily on the feedback it provides to determine whether proposed sys-tem changes hould be adopted.
Third, the results may be used for final cross-systemcomparison, on which judgments concerning the adequacy of competing technologiesare based.
Given their pervasiveness in the entire technology development process,the need for adequate valuation methods is of the essence, and has thus becomea prominent opic for research in itself (Sparck-Jones and Galliers 1996; CSL SpecialIssue 1998, inter alia).The need to serve all these roles has necessitated the development of automatedevaluation methods that are capable of being run repeatedly with little time and costoverhead.
Automated methods are commonly used for application tasks, includingspeech recognition, information retrieval, and IE, as well as for natural anguage com-ponent echnologies, including part-of-speech tagging, syntactic annotation, and coref-erence resolution.
Such methods generally consist of a two-step rocess--first, system-generated responses are aligned with corresponding human-generated responses en-coded in an answer key, and then a predetermined scoring procedure is applied to thealigned response pairs.The second of these tasks (scoring procedure) has been the focus of most previousresearch on evaluation.
In this paper, we focus instead on the less well studied problemof alignment.
The relative inattention to problems in alignment is no doubt a result ofthe fact that alignment is relatively unproblematic n many natural anguage valua-tion scenarios.
In evaluations of component technologies such as part-of-speech tag-ging and treebank-style syntactic annotation systems, for instance, a system-generatedannotation is simply scored against the human-generated annotation for the sameword or sentence, regardless of whether matching it to the annotation for a differentword or sentence would improve the overall score assigned.
Alignment is similarlytrivial in evaluations of applications uch as information retrieval, since the notion ofdocument identity is well defined.
Alignment in speech recognition evaluations canbe a bit more complex, but constraints inherent in the methods nonetheless prohibitclear misalignments, uch that a system cannot receive credit for recognizing a wordfrom an acoustic signal that occurred several utterances later, for instance.As the field progresses to address higher-level interpretation tasks, however, theproblem of determining alignments may become less straightforward.
Such tasks mayrequire that the output contain information that is synthesized (and perhaps eveninferred) from disparate parts of the input signal, making the correspondence b tweeninformation in the system output and information in the answer key more difficult torecover.
A case in point is the evaluation IE technology, as most prominently carriedout by the series of MUCs.
For a typical MUC-style IE task, a text may contain severalextractable events, and thus a method is required for aligning the (often only partiallycorrect) event descriptions extracted by a system to the appropriate ones in the answerkey.
It is therefore important o investigate the issues involved in the definition ofalignment criteria in such tasks, and we can use the MUC experience as a basis forsuch an investigation.In this paper, we focus specifically on the criterion used for alignment in MUC-6.
Inlight of difficulties in identifying a perfect alignment criterion for the MUC-6 task, theMUC-6 community agreed upon on a rather weak and forgiving criterion, leaving theresolution of alignment ambiguities to a mechanism that sought o maximize the scoreassigned.
While this decision may have been thought o be relatively benign, we reporton the results of an extensive, post hoc analysis of a 13 1/2-month effort focused on theMUC-6 task which reveals several unforeseen and negative consequences a sociatedwith this decision, particularly with respect o its influence on the incremental system232Kehler, Bear, and Appelt Accurate Alignment in Evaluationdevelopment process.
While we do not argue that these consequences are so severethat they call the integrity of the MUC-6 evaluation into question, they are substantialenough that they demonstrate the potential of such an alignment strategy to have asignificantly adverse impact on the goals of an evaluation.
It is therefore importantthat these lessons be brought to bear in the design of future evaluations for IE andother high-level language processing tasks.We begin with an overview of IE tasks, systems, and evaluation, including thealignment procedure used for MUC-6.
We then provide an example from the MUC-6development corpus that illustrates properties of the alignment process that interferewith the stated goals of the evaluation.
We assess the pervasiveness of these problemsbased on data compiled from an extended evelopment effort centered on the MUC-6task, and conclude that the effect of the alignment criterion is robust enough that itcould potentially undermine the technology development process.
We conclude thatthese results argue strongly for the use of strict and accurate alignment criteria in futurenatural anguage valuations---evaluations in which alignment problems will becomeexacerbated as the natural anguage applications addressed become more complex--and for maintaining the independence of alignment criteria and the mechanisms usedto calculate scores.2.
Information Extraction, MUC, and the F-score MetricIE systems process streams of natural language input and produce representationsof the information relevant o a particular task, typically in the form of databasetemplates.
In accordance with the aforementioned array of roles served by evaluationmethods, the MUCs have been very influential, being the primary driving force behindIE research in the past decade:The MUCs have helped to define a program of research and develop-ment .. .
.
The MUCs are notable ... in that they have substantiallyshaped the research program in information extraction and brought itto its current state.
(Grishman and Sundheim 1995, 1-2)There have been seven MUCs, starting with MUC-1 in 1987, and ending withMUC-7 in 1997.
The metrics used--precision, recall, and F-score--are probably themost exhaustively used metrics for any natural anguage understanding application;precision and recall have been in use since MUC-2 in 1989 (Grishman and Sundheim1995), and F-score since MUC-4 in 1992 (Hirschman 1998).
IE evaluation has thus beenextensively thought out, revised, and experimented with:For the natural anguage processing community in the United States,the pre-eminent evaluation activity has been the series of Message Un-derstanding Conferences (MUCs) .. .
.
the MUC conferences provide uswith over a decade of experience in evaluating language understand-ing.
(Hirschman 1998, 282)The MUCs therefore provide a rich and established basis for the study of theeffects of evaluation with respect o its roles noted above.
As previously indicated,we will focus in this paper on MUC-6, held in 1995, and exclusively on the procedureused for aligning system-generated r sponses with those in an answer key.233Computational Linguistics Volume 27, Number 2(TEMPLATE)CONTENT (succession_event)(SUCCESSION_EVENT}SUCCESSION_ORE (organization)POST stringIN_AND_OUT (in_and_out}VACANCY_REASON { DEPART_WORKFORCE,REASSIGNMENT,NEW _POST _CREATED IOTH_UNK }(ORGANIZATION)ORE_NAME stringORE_ALIAS stringORG_DESCRIPTOR stringORG_TYPE { COMPANY~ GOVERNMENT,OTHER }ORE_LOCALE stringORE_COUNTRY string(IN_AND_OUT)IO_PERSON (person}NEW_STATUS { IN, m_ACT~NG,OUT~ OUT-ACTING }ON_THE_ JOB { YES, NO, UNCLEAR }OTHER_ORE (organization)REL_OTHER_ORG { SAME_ORE,RELATED-ORE tOUTSIDE_ORE }{PERSON}PER_NAME stringPER_ALIAS stringPER_TITLE stringFigure 1Output template for MUC-6.Marketing & Media: Star TV Chief Steps Down After News Corp. TakeoverIn the wake of a takeover by News Corp., the chief executive officer of Star TV resigned after lessthan six months in that post, industry executives said.Last week, News Corp. bought 63.6% of the satellite broadcaster, which serves the entire Asianregion, for $525 million in cash and stock from Hutchison Whampoa Ltd. and the family of Li Ka-shing.At the time of the purchase, News Corp. executives said they would like the executive, JulianMounter, to stay.
However, Star's chief balked at the prospect of not reporting directly to RupertMurdoch, News Corp.'s chairman and chief executive officer, people close to Mr. Mounter said.
BothMr.
Mounter and a Star spokesman declined to comment.It is likely that Star's new chief executive will report to either Sam Chisholm, chief executive ofNews Corp.'s U.K.-based satellite broadcaster, British Sky Broadcasting, or Chase Carey, chief operatingofficer of News Corp.'s Fox Inc. film and television unit.Mr.
Mounter's departure is expected to be formally announced this week.Although there are no obvious successors, it is expected that Mr. Murdoch will choose someonefrom either British Sky Broadcasting or Fox to run Star, said a person close to News Corp.Figure 2Example text from MUC-6 development set (9308040024).2.1 Task DefinitionThe MUC-6 task was, roughly speaking, to identify information in business newsthat describes executives moving in and out of high-level positions within companies(Grishman and Sundheim 1995).
The template structure that MUC-6 systems popu-lated is shown in Figure 1.
There are three types of values used to fill template slots.String fills, shown italicized, are simply strings taken from the text, such as CEO forthe POST slot in a SUCCESSION_EVENT.
Set fills are chosen from a fixed set of values,such as DEPART_WORKFORCE for the VACANCY_REASON slot in a SUCCESSION_EVENT.Finally, Pointer fills, shown in angle brackets, hold the identifier of another templatestructure, e.g., the SUCCESSION_ORE slot in a SUCCESSION_EVENT will contain a pointerto an ORGANIZATION template.Figure 2 displays a text from the MUC-6 development corpus.
When a partici-pating system encounters this passage, it should extract he information that Julian234Kehler, Bear, and Appelt Accurate Alignment in EvaluationcorcorcorcorinccormismiscorspuspucorincinccormismisFigure 3TEMPLATE-024-1)CONTENT (SUCC_EVENT-024-  i}SUCC_EVENT-024-1}SUCCESS ION_ORG (ORG-024-1}POST "CEO"IN_AND_OUT ( IN_AND_OUT-024-  i}VACANCY_REASON REASSIGNMENTORG-024-1)ORG_NAME "STAR Tv"ORG_ALIAS "STAR"ORG_DESCRIPTOR "THE SAT BDSTR"ORG_TYPE COMPANYIN_AND_OUT-024-1}IO_PERSON (PERSON-024-1}NEW_STATUS OUTON_THE_JOB UNCLEARPERSON-024-1}PER_NAME "JULIAN MOUNTER"PER_ALIAS "MouNTER"PER_TITLE "MR."Target and hypothetical outputs for the example text.
(TEMPLATE-024-1)CONTENT (SUCC_EVENT-024- I}(SUCC_EVENT-024-1)SUCCESS ION_ORG (ORG-024-11POST "CEO"IN_AND_OUT ( IN_AND_OUT-024-1}VACANCY_REASON OTH_UNK(ORG-024-1)ORG_NAME "STAR TV"ORG_TYPE COMPANYORG_LOCALE WHAMPOAORG_COUNTRY UNITED KINGDOM(IN_AND_OUT-024-1}IO_PERSON (PERSON-024-1)NEW_STATUS INON_THE_JOB No(PERSON-024-1)PER_NAME "JULIAN MOUNTER"Mounter is "out" of the position of CEO of company Star TV, along with other infor-mation associated with the event.
The correct results for this passage, as encoded in ahuman-annotated answer key, are shown in the middle column of Figure 3.2.2 Evaluation: AlignmentThe rightmost column of Figure 3 shows hypothetical output of an IE system.
Thefirst step of the evaluation algorithm, alignment, determines which templates in thesystem's output correspond to which ones in the key.
Generally speaking, there canbe any number of templates in the key and system response for a given document;all, some, or no pairs of which may be descriptions of the same event.
The alignmentalgorithm must thus determine the correct emplate pairing.This process is not necessarily straightforward, since there may be no slot in atemplate that uniquely identifies the event or object which it describes.
In responseto this problem, the MUC community decided to adopt a relatively lax alignmentcriterion, leaving it to the alignment algorithm to find the alignment hat optimizesthe resulting score.
The procedure has two major steps.
First, it determines whichpairs of templates are possible candidates for alignment; the criterion for candidacywas only that the templates hare a common value for at least one slot.
(Pointer fillsshare a common value if they point to objects that are aligned by the algorithm.)
Thiscriterion often results in alignment ambiguities--a key template will often share acommon slot value with several templates in the system's output, and vice versa--and thus a method for selecting among the alternative mappings is necessary.
Thecandidate pairs are rank ordered by a mapping score, which simply counts the numberof slot values the templates have in common.
1 The scoring algorithm then considers1 The scoring software provided for MUC-6 allows for alternative scoring configurations based on slotcontent, including the ability to assign different weights to slots, but this was the configuration usedfor development and evaluation i MUC-6.235Computational Linguistics Volume 27, Number 2key and response template pairs according to this order, aligning them when neithermember has already been mapped to another template.
Ties between pairs with thesame number of common slot values are broken arbitrarily.
Because this algorithm isheuristic--with many combinations of alignments never being considered--the r sultmay not be the globally optimal alignment in terms of score assigned.In our example, there is only one template of each type in each response, andthus there are no mapping ambiguities.
The only requirement is that each pair sharea common slot value, which is the case, and so the algorithm aligns each as shown inFigure 3.2.3 Evaluation: ScoringOnce the templates are aligned, the scoring algorithm performs lot-by-slot compar-isons to determine rrors.
The leftmost column in Figure 3 shows examples of thethree types of errors that the algorithm will mark.
First, while our hypothetical sys-tem recognized that the correct PERSON and ORGANIZATION are Julian Mounter andStar TV, respectively, it missed the ORG_ALIAS, ORG_DESCRIPTOR, PER_ALIAS, andPER_TITLE values that appear later in the passage, resulting in four missing slot fills(denoted by mis in the left hand column).
Next, it also erroneously assigned a valueto the ORG_LOCALE and ORG_COUNTRY slots in the ORGANIZATION, resulting in twospurious slot fills (denoted by spu).
Finally, the system got three of the set fill slotswrong--the VACANCY_REASON slot in the SUCCESSION_EVENT, and the NEW_STATUSand ON_THE_JOB slots of the IN_AND_OuT template--resulting in three incorrect slotfills (denoted by inc).
The remainder of the slot fills are correct (denoted by cor).
2Again, pointer slots are scored as correct when they point to templates that have beenaligned.The possible fills are those in the key which contribute to the final score, and theactual fills are those in the system's response which contribute to the final score:POS = COR + INC + MISACT = COR + INC + SPUThree metrics are computed from these results: precision, recall, and F-score.
Pre-cision is the number of correct fills divided by the total number generated by thesystem, and recall is the number of correct fills divided by the the total number in thekey:CORPRE -ACTCORREC -POSIn the example, there are 8 correct fills, 13 generated fills, and 15 possible fillsin the key, resulting in a precision of 0.615 and a recall of 0.533.
Note that the fourmissing slot fills do not figure into the precision computation, and the two spuriousfills do not figure into the recall computation.
F-score is determined by a harmonicmean of precision and recall (van Rijsbergen 1979):1 (f12 + 1) x PRE x REC(1 -- 1 1 1 1 (f12 X PRE) + REC2 There is also the possibility of getting partial credit, which we will not discuss further nor include inthe equations below.236Kehler, Bear, and Appelt Accurate Alignment in EvaluationIn the standard computation of F-score, fl is set to one (indicating equal weightfor precision and recall), resulting in:2 x PRE x RECF=PRE + RECFor our example, we get an F-score of 0.571.2 x 0.615 x 0.533 = 0.5710.615 + 0.5332.4 Focus of the PaperBefore proceeding to the analysis, we take care to note that our aim is neither to providea thorough analysis of all problematic aspects of NLP evaluation, nor to provide acriticism of the MUCs in particular.
Problematic aspects of the scoring procedures usedin a variety of NLP evaluations are well attested, including the existence of side effectsof scoring procedures which reward behavior that may not be perfectly consistent withthe goals of the evaluations that these scoring procedures erve.
For instance, worderror metrics used in evaluations of speech recognition technology have been criticizedfor the fact that they assign the same degree of credit for the recognition of all words,a policy that rewards a focus on recognizing frequent but less important words (e.g.,urn) over more important but less frequent content words.
Similarly, evaluations ofsyntactic annotation systems that use locally oriented crossing brackets and labeledprecision/recall metrics can assign high degrees of credit to cases in which the assignedstructures are fairly inaccurate when viewed more globally.
Likewise, there are aspectsof the scoring procedure used for MUC-6 that one could question, such as the choiceof slots included in each template and their corresponding definitions, the decisionto weight all slots equally without regard to perceived importance, and the choice todefine the templates to have hierarchical structure and give credit for slots that merelycontain pointers to lower-level templates.
We believe that any mechanical evaluationis likely to have such issues, and while they are very worthy of study and debate, amore detailed discussion of them would take us too far afield from the main purposeof this paper.The focus of this paper is purposefully more narrow, being concerned only withthe effects of alignment criteria on the goals of evaluation.
While it is unclear asof this writing whether there will be future MUCs, evaluation-driven efforts in IEcontinue to be sponsored, and future evaluations of interpretation tasks of equal orgreater complexity are not only likely but crucial if the field is to progress whilemaintaining its current focus on quantitative evaluation.
Because alignment questionswill almost certainly become exacerbated as the interpretation problems addressedbecome more complex, and because of the aforementioned pervasiveness of evaluationin the entire technology development process, the payoff in avoiding potential pitfallsin such evaluations i high.
Thus, our aim is to bring lessons learned from the MUCexperience to the fore so that they can inform future evaluations that, like the MUCs,are likely to be principal driving forces for research over extended periods of time.3.
The Impact of Inaccurate AlignmentIn the introduction, we described several roles that the MUC-6 evaluation has playedin bringing IE technology to its current state: task definition, system development, andcross-system evaluation.
We focus here on its role in the system development process,where its influence has been substantial, as it has provided system developers the abil-ity to obtain rapid feedback with which to iteratively gauge their progress on a training237Computational Linguistics Volume 27, Number 2cor(TEMPLATE-024-1}CONTENT (SUCC_EVENT-024-1}(SUCC_EVENT-024-1)cor SUCCESSION_ORG (ORG-024-1}cor POST "CEO"cor IN_AND_OUT (IN_AND_OUT-024-1)inc VACANCY_REASON REASSIGNMENT(TEMPLATE-024-1}CONTENT (SUCC_EVENT-024-1)(SUCC_EVENT-024-1}SUCCESSION_ORG {ORG-024-1}POST "CEO"IN_AND_OUT (IN_AND_OUT-024-1}VACANCY_REASON OTH_UNK(ORG-024-1)  (ORG-024-1}inc ORG_NAME "STAR TV"  ORG_NAME "NEWS CORP."mis ORG_ALIAS "STAR"mis ORG_DESCRIPTOR "THE SAT BDSTR"cor ORG_TYPE COMPANY ORG_TYPE COMPANY(IN_AND_OUT-024-1} (IN_AND_OUT-024-1}cor IO_PERSON (PERSON-024-1} IO_PERSON (PERSON-024-1}inc NEW_STATUS OUT NEW_STATUS INcor ON_THE_JOB UNCLEAR ON_THE_JOB UNCLEAR(PERSON-024-1) (PERSON-024-1)inc PERd~AME "JULIAN MOUNTER"  PER_NAME "RUPERT MURDOCH"inc PER_ALIAS "MOUNTER" PER_ALIAS "MURDOCH"cor PER_TITLE "MR." PER_TITLE "MR."Figure 4Target and actual outputs for the example text.corpus.
In essence, a deve loper  can incorporate  a new process ing s t rategy or data  mod-if ication, eva luate  the sys tem wi th  the change,  keep it if end- to -end  per fo rmance  (i.e.,F-score) improves ,  and  w i thdraw it if it does  not.
Often changes  have  unant ic ipatedresults,  and  thus a formal  eva luat ion  method is requ i red  to aff i rm or deny  deve lopers '(often mis lead ing)  intuit ions.
L ikewise,  the method has an ana logous  role for suppor t -ing systems that learn rules automat ica l ly .
In a typica l  learn ing scenario,  an automatedprocedure  i terat ive ly  proposes  rule modi f icat ions  and  adopts  them on ly  in those casesin wh ich  an object ive funct ion - - that  is, the eva luat ion  method- - ind icates  that perfor-mance  has improved.
Thus, it is abso lute ly  crucial  that both  pos i t ive  and  negat ive  sys-tem changes  are ref lected as such in the feedback  prov ided  by  the scor ing mechan ism.As we i l lustrate w i th  the passage  shown in F igure 2, the weak  a l ignment  cr i ter ionused  in MUC-6  causes the scor ing mechan ism to not  respect  this requ i rement .
3 Theanswer  key  in F igure 4 is as it was  in F igure  3, represent ing  the event  of Ju l ian Mounterleav ing the pos i t ion  of CEO at Star TV.
The results  of an IE sys tem (in this case, as l ight ly  mod i f ied  vers ion  of what  SRI's FASTUS \ [Appe l t  et al 1995\] extracted for thisexample)  are shown in the r ighthand co lumn.
The IE sys tem made two s igni f icantmis takes  on this text: It fa i led to extract any  in format ion  re lat ing to the correct event,and  it overgenerated  an incorrect  "event ,"  speci f ical ly  Ruper t  Murdoch 's  becomingCEO of News  Corp.  4 Thus, the sys tem's  prec is ion shou ld  be o = 0, s ince it generated3 In this section we will be arguing our point primarily on the basis of a single illustrative xample,where in fact the MUC development process utilized a 100-text corpus.
The arguments extend to thisbroader setting, of course, a topic to which we return in Section 4.4 An anonymous reviewer points out that because no slot in the template structure can be guaranteed toidentify an object, intuitions alone are not enough (despite how strong they might be in a case such asthis) to establish that the system output in Figure 4 actually represents a different event, rather than theevent described in the output in Figure 3 corrupted by the selection of wrong names for certain slots.The FASTUS system produces byte offsets to tie information i  templates to the places in the text fromwhich they were created, and from this we can confirm that the event was indeed created solely fromtextual material unrelated to the event described in the key.
See also footnote 7.238Kehler, Bear, and Appelt Accurate Alignment in EvaluationTable 1Approximate distribution of values for low-entropy slots.Template Type Slot Total Templates Slot Value DistributionSUCCESSION_EVENT VACANCY_REASON 214 REASSIGNMENT: 101OTH_UNK: 77NEW_POST_CREATED: 21DEPART_WORKFORCE: 18IN_AND_OuT NEW_STATUS 287 IN: 129OUT: 148IN_ACTING: 7OUT_ACTING: 4IN_AND_OuT ON_THE_JOB 287 YES: 82NO: 137UNCLEAR: 96IN_AND_OuT REL_OTHER_ORG 287 SAME_ORG: 93OUTSIDE_ORG: 66RELATED_ORG: 43(none): 91ORGANIZATION ORG_TYPE 117 COMPANY: 115GOVERNMENT: 3OTHER:  0PERSON PER_TITLE 158 "Mr.": 68"Dr.": 3"Ms.": 3(none): 8413 spur ious  slot fills for an i r re levant  event,  and  its recal l  shou ld  be o = 0, since itgenerated  none  of the 15 slots assoc iated wi th  the correct event.
5In actuality, however ,  the scor ing a lgor i thm a l igned these templates  as shown inF igure 4, since each template  pa i r  shares at least one slot value.
With  this a l ignment ,the scor ing procedure  ident i f ies 8 correct slot values,  5 incorrect  values,  and  2 miss ingvalues,  resul t ing in a recal l  of 8 = 0.533, a prec is ion of 8 = 0.615, and  an F-score of0.571.
These are the same scores received for the output  in F igure 3, in wh ich  a part ia lbut  most ly  accurate descr ip t ion  of the correct event  was  extracted.The fact that the who l ly  incorrect output  in F igure 4 and  the largely  correct outputin F igure 3 receive equa l ly  good  scores demonst ra tes  a ser ious f law wi th  the eva luat ionmethod.
As shown in Table 1, the d is t r ibut ions  of va lues  for several  slots in the MUC-6t ra in ing data  have re lat ive ly  low entropy,  and  thus a l ignments  based  on a fortu i tousover lap  in these va lues  are not  rare.
6 For  instance, the fact that  the ORG_TYPE slot inORGANIZATION templates  has the va lue COMPANY in 115 out  of 117 instances a lmostensures that any  ORGANIZATION template  produced by  a sys tem can get matched to anarb i t rary  one in the key  for a g iven document .
F rom this, in turn, the inaccurate  al ign-ments  may then cascade: Two unre la ted  SUCCESSION_EVENT templates  can be a l igned5 Its F-score should be undefined, since the denominator in the F-score quation will be zero.
For allintents and purposes, however, the F-score can be considered to be zero, in the sense that its overallcontribution to the F-score assigned to the results over a larger corpus will be zero.
With this in mind,for simplicity we may speak of such cases as having an F-score of zero.6 In some cases, the sum of the counts in the rightmost column is greater than the total template count.This is because in some cases a key entry allowed for alternative slot values; matching any of thealternatives was sufficient for both alignment and scoring.
Likewise, instances of optional slot fills werealso included.239Computational Linguistics Volume 27, Number 2on the basis of sharing pointers to two unrelated (but nonetheless aligned) ORGANIZA-TION templates.
Likewise, two unrelated IN_AND_OUT templates can be aligned on thebasis of sharing pointers to two unrelated PERSON templates that share the value Mr.for the TITLE slot.
Between this prospect and the three set fills for IN_AND_OUT tem-plates in Table 1, it is highly likely that an arbitrary IN_AND_OUT template produced bya system will overlap in at least one slot value with an arbitrary one in the key, whichmay in turn allow the SUCCESSION_EVENTS that point to them to be incorrectly aligned.Although the fact that the scoring algorithm is capable of assigning undeservedcredit due to overly optimistic alignments i  not unknown to the MUC community (see,for instance, Aberdeen et al \[1995, 153, Table 4\] for a reference to "enthusiastic s oringmapping" for the Template Element ask of MUC-6), we are unaware of any previousacknowledgement of, or similar example which demonstrates, the potential severityof the problem to the extent hat Figure 4 does.
This notwithstanding, one might stillbe tempted to view this behavior as relatively benign--perhaps there is no real harmdone by giving systems the benefit of the doubt, along with a little undeserved creditthat goes with it.
While this will perhaps result in somewhat artificially inflated scores,there may be no reason to think that it would benefit one system or approach morethan another, and this concession might seem reasonable considering the fact that thereis likely to be no completely foolproof way to perform alignments.However, the potential harm that this behavior manifests in terms of the tech-nology development process--which, to our knowledge, has never been brought olight--is that it creates a situation in which uncontroversially positive changes in sys-tem output may result in a dramatically worse score, and likewise negative changesmay result in a dramatically better score.
Consider a (common) development scenarioin which, starting from a state in which the system produces the output in Figure 4for the text in Figure 2, it is technically too difficult o modify the system to extract hecorrect (i.e., Julian Mounter) event, but in which a change can nonetheless be made toblock the overgenerated (i.e., Rupert Murdoch) event.
After such a modification, onewould expect no change in recall, since no correct output is created or removed, andan improvement in precision, since an overgenerated vent is removed.What actually happens in this example is that recall drops from 0.533 (8) to zero(o), and precision goes from 0.615 (8) to undefined (0).
To circumvent comparisonswith undefined values, we can suppose that there was another, independent eventextracted from a different part of the text that was aligned correctly against its cor-responding event in the answer key.
For simplicity, we will assume that this eventreceives the same score as the overgenerated vent: a precision of 8 and a recall of 8 ig"With the overgenerated vent left in, the same scores as before are obtained: 16 ~___ 0.61516 ~--  0.533 recall, resulting in an F-score of 0.571.
With the overgenerated precision and ~event removed, we obtain 8 = 0.615 precision and 8 = .267 recall, resulting in anF-score of 0.372.
Instead of no change in recall and an improvement in precision, thereward for eliminating the overgenerated vent is the same precision, a 50% reductionin recall, and a 20-point reduction in F-score.
Our clear "improvement" thus has theeffect of reducing performance dramatically, implicitly instructing the developer toreintroduce the rules responsible for producing the overgenerated vent.The converse scenario yields an analogous problem.
Consider a situation in whicha system developer can add a rule to extract at least some of the information inthe correct event--producing the output shown in Figure 5, for instance--but forwhatever reason cannot make a change to block the overgenerated vent.
We wouldexpect his change to result in a marked increase in both recall and precision, sinceunlike before, information for a relevant event is now being produced.
Indeed, the240Kehler, Bear, and Appelt Accurate Alignment in Evaluation<TEMPLATE-024-1>CONTENT <SUCC_EVENT-024-1>(SUCC_EVENT-024-1>SUCCESSION_ORG (ORG-024-1>POST "CEO"IN_AND_OUT <IN_AND_OUT-024-1}VACANCY_REASON OTH_UNK(ORG-024-1>ORG_NAME "STAR TV"ORG-TYPE COMPANYFigure 5Additional output for example text.
(IN_AND_OUT-024-1>IO_PERSON (PERSON-024-1)NEW_STATUS INON_THE_JOB NO<PERSON-024-1>PER_NAME "JULIAN MOUNTER"PER_ALIAS "MOUNTER"alignment algorithm will correctly align this event with the key and leave the RupertMurdoch event unaligned, resulting in a precision of 9 = .600, a recall of 9 = .360,and an F-score of 0.450.
This is the anticipated result, and would constitute a largeincrease over the zero F-score that the overgenerated vent should have received whenstanding alone.
However, this is a substantial reduction from the F-score of 0.571that the overgenerated vent actually receives, a change which implicitly instructs thedeveloper to remove the rules responsible for extracting the correct event.In these two scenarios, positive changes to system output resulted in a dramat-ically reduced score.
The opposite situation can also occur, in which a change thatreduces the quality of the system's response nonetheless receives an increased score.One can merely reverse the scenarios.
For instance, in a situation in which no outputis being created for the Julian Mounter event and a developer considers adding a rulethat produces the Rupert Murdoch event, the rise in F-score will indicate that thisrule should be kept.
Likewise, starting with the incorrect output in Figure 4 togetherwith the correct output in Figure 5, a developer might consider emoving the ruleresponsible for creating the correct output.
This would cause the F-score to rise from0.450 to 0.571, implicitly instructing the developer to keep it removed.Thus, in all of these scenarios, the feedback provided by the evaluation methodmay steer our system developer off of the path to the optimal system state.
Likewise,the same effect would occur when employing automatic learning methods that useF-score as an objective function.
Starting from the state of producing the output in Fig-ure 4, for example, suppose the learning procedure could in fact propose ach changenecessary to get to the desired output, that is, to (i) eliminate the rules producing theerroneous output, and (ii) add rules for producing the output shown in Figure 5.
Thesechanges would result in a precision of 9 = .60, a recall of 9 = .75, and an F-scoreof 0.667, which is an improvement over both the zero result that the current outputshould receive, and the (artificially inflated) score of 0.571 it actually does receive.However, the type of incremental search process that efficiency concerns generallynecessitate--one that can only perform one of steps (i) or (ii) in a single iteration andwill only adopt the proposed change if it improves on its objective function--will notfind this system state, since as we have seen, either move taken first would actuallyreduce the F-score.To sum, the fortuitous alignments allowed by the MUC-6 evaluation method createa situation in which both positive and negative system changes may not be reflectedas such in the evaluation results.
While there are other properties of the evaluationthat conspire to help produce these anomalous results--including the choice to scoreall slot fills equally without respect o importance or entropy of their distribution ofvalues, and to score slots which contain only pointers to other templates--these only241Computational Linguistics Volume 27, Number 2serve to make the effects more or less dramatic than they might otherwise be.
Theroot cause of this behavior is the alignment process: None of the foregoing behaviorswould occur if the alignment criterion was such that the templates in Figure 4 werenot alignable, thus producing an F-score of zero.4.
A Case StudyThe foregoing examples demonstrate he pitfalls of not employing strong alignmentconstraints in natural anguage system evaluations.
In the case of IE, the constraintsshould come as close as possible to establishing that two template representationsare meant o describe the same events or objects.
Just as it would be nonsensical forevaluations of part-of-speech tagging systems to give credit for a correct ag assignedto a different word, or evaluations of syntactic annotation systems to give credit forbracketings assigned to a different sentence, IE evaluations should not give credit fora template structure that represents a different event in the text.
While it may betempting to give systems the benefit of the doubt in light of the fact that alignmentin IE is inherently more difficult than in these other scenarios, we have seen that thenegative consequences of such a move can subvert he goals and purposes of theevaluation, and indeed the technology development process.Having said this, a question that naturally arises is how robust his effect actuallywas for the MUC-6 task.
Is the example shown in Figure 4 exceptional for MUC-6, andthus useful mainly for pedagogical purposes, or is it indicative of a more pervasiveproblem that could impact development using a larger set of training documents?
Itis difficult to answer this question, of course, since one cannot replay past phases oftechnology development.
However, we do have a case study with which to investigatethis question, as we have previously performed a 13 1/2-month effort in which wefocused on improving performance on the MUC-6 task.
Specifically, our goal was to seehow far the FASTUS paradigm could be pushed by way of making as many incrementalimprovements a possible.
We relied heavily on the MUC-6 scoring mechanism duringthis process, using the feedback it provided to drive our development in the mannerdescribed in Section 3.
Throughout this process, we remained ignorant of the problemsthat we are reporting on presently.As a result of our effort, we have a record of the output of our system as it ex-isted in 53 distinct states of development.
We can compare the feedback provided bythe scoring algorithm used during the development process (henceforth, the standardalgorithm), with the feedback that would have been received from a more accurateand restrictive alignment criterion (henceforth, the restrictive algorithm), which wedescribe in Section 4.1.
We report on two types of comparison, each at the level of in-dividual documents (Section 4.2) and the entire 100-text development set (Section 4.3).We first report on the overall effects that the restrictive algorithm has on scores forindividual system states.
We then report on the extent o which the two algorithmsdisagreed on the direction of the difference in performance between a pair of systemstates--that is,whether the changes implemented between these states had a positiveor negative ffect--as this is the central factor that developers and learning algorithmsuse to determine whether to adopt proposed system changes.
As the difference be-tween any pair of states constitutes a set of intermediate system changes that one canevaluate, our 53 distinct states provide us with 53?52 = 1,378 pairs to examine.
24.1 A Stricter Alignment CriterionAs we mentioned in footnote 1, the scoring system provided for MUC-6 allows one tocustomize alignment criteria based on slot content.
Because no slot in a template will242Kehler, Bear, and Appelt Accurate Alignment in Evaluationconsistently and uniquely identify the event or object that it describes, it is difficultor impossible to design a perfect slot-based criterion.
Nonetheless, there are morerestrictive parameterizations that come much closer to producing only the correctalignments.
We sought out the criterion that eliminates as many incorrect alignmentsas possible without being so restrictive that a system would be denied partial creditfor correctly extracted information.Table 1 readily suggests a principled manner in which to restrict the mappingcriterion: Avoid aligning templates olely on the basis of slots with only a small setof possible values (and in particular, those which have low entropy distributions),since shared values for these provide little evidence that the two templates representthe same entity or event.
Indeed, our experience confirms that the large majority ofalignment errors result from a fortuitous match on one of these slots.
Each of the slotsin Table 1 have at most four possible slot values, whereas the set of possible values forthe remainder of the slots is essentially unbounded (except possibly for the POST slotof the SUCCESSION_EVENT template; we will return to this momentarily).
Thus, whileit is unlikely that templates for two unrelated companies will have the same companyname, it is very likely that they both will have the value COMPANY in the ORG_TYPEslot.We therefore modified the MUC-6 alignment criterion so that any single sharedvalue for a slot not in Table I is sufficient for alignment.
(All other aspects of the align-ment criterion and procedure remained unchanged.)
This criterion is still generousin some cases, for instance, the system output shown in Figure 4 receives an F-scoreof 0.143: The system will get credit for the coincidentally identical POST slot valuesand for the pointers to the SUCCESSION_\]~VENTS that will be aligned on the basis ofthose values.
However, a criterion that bars such alignments may also disallow cer-tain cases in which a system should arguably deserve credit.
7While a small amountof undeserved credit may therefore remain, this amount is dramatically reduced fromthat which results from the standard algorithm.
All slot values were still counted forscoring purposes, as in MUC-6.4.2 Effects of Stricter A l ignment  on Individual  Document  ResultsWith our more restrictive alignment algorithm in hand, we begin by looking at itseffect on performance at the document level.
Only 53 of the 100 MUC-6 developmenttexts were relevant, and because the recall of an irrelevant document is undefinedregardless of what the system produces, only these 53 have defined F-scores.4.2.1 Effect on Scores.
The restrictive algorithm tended to assign lower scores thanthe standard algorithm, as one would expect, since the best-scoring alignment foundby the standard algorithm may be correctly disallowed by the restrictive algorithm.
In7 As indicated in footnote 4, it is theoretically possible that a template produced by a system and atemplate in the answer key originate from a description ofthe same ntity or event, but in which thesystem's template isso corrupted by inaccurate orincomplete processing that the only correct slots thatremain are included in Table 1.
In our extensive analyses of system results, we found the number ofsuch cases to be quite infrequent, and overwhelmed bythe number of cases in which alignments basedonly on these slots were demonstrably incorrect.
One could argue about whether any partial credit isactually deserved in the former set of cases; in any case, we believe that not assigning the smallamount of credit a system would receive isa small price to pay for rectifying the much greater negativeeffects of maintaining an overly lax alignment s rategy.
Even if such partial credit is deemed eserved,however, our analyses suggest that the missed credit is more than made up for by the undeservedcredit resulting from fortuitous matches on the POST slots of SUGCESSION_EVENT templates that ournew criterion still allows, as described above.
In fact, the overall effect of both appears to be rathernegligible alone, and even more so when their opposite ffects on scores are taken together.243Computational Linguistics Volume 27, Number 2Madison Group Says Board Has Dismissed Lucas as Its PresidentMadison Group Associates Inc. said its board dismissed Kenneth Lucas, president, naming DeanJ.
Trantalis as interim president.The company also said two new directors - Roland Breton and Steve Gibboney - had been appointedto its board.Mr.
Lucas became chief executive of the media concern less than two months ago, when WilliamT.
Craig resigned from his job as a director and chief executive officer.The company gave no reason for Mr. Lucas's dismissal.
Neither he nor Madison executives couldbe reached for comment.The management change is the latest in a series of events that have shaken the company in recentmonths.
As previously reported, the Securities and Exchange Commission contacted several individualsabout their dealings with the company.
One of those individuals aid the SEC had asked about how thecompany valued its assets.Those assets consist largely of video libraries.
According to a recent securities filing, an accountantformerly hired by Madison recommended that an independent specialist be hired to evaluate the videolibraries.Figure 6Example text from MUC-6 development set (9403100087).our system runs, the scores for an average of 21.2 of the 53 documents were reduced.Thus, in a typical run, a substantial percentage of the document results--40%--hadbenefited from incorrect alignments from the standard algorithm.
The magnitude ofthe reduction in document scores ranged from 0.14 to 36.84 points of F-score, averaging7.74.Interestingly, there were also cases in which the restrictive algorithm actually as-signed a higher score to the results for a document than the standard algorithm.
Onemight wonder how this could happen, since the set of possible alignments allowedby the restrictive algorithm is a strict subset of those allowed by the standard algo-rithm.
The reason lies in the fact that the alignment procedure does not perform anexhaustive search; instead, it uses the heuristic search method described in Section 2.It is therefore possible that an optimal but nonetheless correct solution exists whichthe standard algorithm does not find, but which the restrictive algorithm finds withinthe narrower search-space associated with its more restrictive criterion.Figure 6 shows an example from the MUC-6 development corpus, and Figures 7and 8 show a fragment of the alignments produced by the standard and restrictive al-gorithms, respectively.
All of the remaining system output not shown received thesame alignment by both algorithms.
William Craig was represented by templates(PERSON-087-5) in the key and (PERSON-087-8) in the system response, and KennethLucas was represented by templates (PERSON-087-1) in the key and {PERSON-087-20)in the system response; both were aligned correctly by both algorithms.The alignment in Figure 7, along with the remainder of the output not shown,results in an F-score of 69.44 for the text.
Template /IN_AND_OUT-087-1) is alignedwith (IN_AND_OuT-087-1), and (IN_AND_OuT-087-4) is aligned with (IN_AND_OUT-087-3), although in neither case do the IO_PERSON slots point to the (correctly) alignedPERSON templates.
Nonetheless, each pair yields two correct values, specifically forthe NEW_STATUS and ON_THE_JOB slots.This alignment is not possible with the restrictive algorithm, since it is performedsolely on the basis of slots listed in Table 1.
The restrictive algorithm finds the oppositealignment between the IN_AND_OuT templates, hown in Figure 8.
This mapping alsoyields two correct slot fills for each IN_AND_OUT template, in this case, the IO_PERSON244Kehler, Bear, and Appelt Accurate Alignment in EvaluationCOR (IN-AND_OUT-087-1) (IN-AND_OUT-087-1)inc IO~PERSON:  (PERSON-087-1) (PERSON-087-8)cor NEW_STATUS: OUT OUTcor ON_THE_JOB: UNCLEAR UNCLEARCOR (IN_AND_OUT-087-4) (IN-AND_OUT-087-3)inc IO_PERSON:  (PERSON-087-5) (PERSON-087-20)cor NEW-STATUS: OUT OUTcor ON_THE_JOB: No Nospu OTHER_ORG: (ORGANIZATION-087-3)spu REL_OTHER_ORG: OUTSIDE_ORGCOR (SUCCESSION_EVENT-087-1) (SUCCESSION_EVENT-087-2)cor SUCCESSION_ORG: (ORGANIZATION-087-1) (ORGANIZATION-087-9)cor POST: "PRESIDENT .
.
.
.
PRESIDENT"cor IN-AND_OUT: (IN-AND_OUT-087-2) (IN-AND_OUT-087-4)inc (IN-AND_OUT-087-1) (IN-AND_OUT-087-3)cor VACANCY_REASON: REASSIGNMENT REASSIGNMENTCOR (SUCCESS1ONfl~VENT-087-2) (SUCCESSION_EVENT-087-1)inc SUCCESSION_ORG: (ORGANIZATION-087-1) (ORGANIZATION-087-3)cor POST: "CHIEF EXECUTIVE .
.
.
.
CHIEF EXECUTIVE"cor IN-AND_OUT: (IN-AND_OUT-087-3) (1N-AND_OUT-087-2)inc (IN-AND_OUT-087-4) (IN-AND_OUT-087-1)inc VACANCY_REASON: REASSIGNMENT OTH-UNKFigure 7Alignment for text 9403100087 with the standard algorithm.COR (IN_AND_OUT-087-4) (IN_AND_OUT-087-1)cor IO_PERSON:  (PERSON-087-5) (PERSON-087-8)cor NEW_STATUS: OUT OUTinc ON_THE_JOB: No UNCLEARCOR (1NAND_OUT-087-1) (INAND_OUT-087-3)cor IOA)ERSON:  (PERSON-087-1) (PERSON-087-20)cor NEW_STATUS: OUT OUTinc ON_THE_JOB: UNCLEAR NOspu OTHER_ORG: (ORGANIZATION-087-3)spu REL_OTHER_ORG: OUTSIDE_ORGCOR (SUCCESSIONfl~VENT-087-1) (SUCCESSION_EVENT-087-2)cor SUCCESSION_ORG: (ORGANIZATION-087-1) (ORGANIZATION-087-9)cor POST: "PRESIDENT .
.
.
.
PRESIDENT"cor IN.AND_OUT: ( IN_AND_OUT-087-1) (IN-AND_OUT-087-3)cor (IN_AND_OUT-087-2) (IN_AND_OUT-087o4)cor VACANCY_REASON: REASSIGNMENT REASSIGNMENTCOR (SUCCESSION~VENT-087-2) (SUCCESSION_EVENT-087-1)inc SUCCESSION_ORG: (ORGANIZATION-087-1) (ORGANIZATION-087-3)cor POST: "CHIEF EXECUTIVE" "CHIEF EXECUTIVE"cor IN.AND_OUT: (IN-AND_OUT-087-4) (IN-AND_OUT-087-1)cor (IN-AND_OUT-087-3} (IN-AND_OUT-087-2)inc VACANCY_REASON: REASSIGNMENT OTH_UNKFigure 8Alignment for text 9403100087 with the restrictive algorithm.and  NEW_STATUS slots.
This a l ignment - -wh ich  is the correct one- - resu l t s  in an F-score of 75.00: Despi te  the fact that  the SUCCESSION_EVENT templates  are a l igned thesame way in both  cases, the restr ict ive a lgor i thm a l ignment  leads to two add i t iona lcorrect fills for the po in ters  to the proper ly  a l igned IN_AND_OUT templates.
Whi le  thisa l ignment  was  a poss ib le  so lut ion for the s tandard  a lgor i thm,  it arb i t rar i ly  chose thewrong pa i r ing  of IN_AND_OuT templates - -both  possib i l i t ies  resu l ted in two shared slotva lues - -and  thus the system was den ied  more  than five po ints  of F-score on the article.245Computational Linguistics Volume 27, Number 2This behavior was not specific to this example in our system runs; 18 of the 53relevant texts (34%) displayed this behavior for at least one of the 53 system states.
Anaverage of 2.7 documents rose in score per system run.
The magnitude of the increasein document score ranged from 0.25 to 7.27 points of F-score.To sum, the more accurate alignment criterion affected approximately 24 out of53 relevant documents (45%) for an average system run.
In most cases, the effect wasto reduce the score assigned by the standard algorithm, since the best-scoring (albeitincorrect) alignment found by the standard algorithm was often disallowed by therestrictive algorithm.
However, due to the fact that the heuristic search process usedfor alignment is less likely to find the optimal mapping with the standard criterion,there were also cases in which the effect was to increase the score.4.2.2 Diverging Indications Between System States.
We now ask to what extent hesedocuments exhibit the behaviors een in the four development scenarios describedin Section 3.
In the first two of these scenarios, a system change that should haveimproved F-score decreased it instead.
For these, we would expect he restrictive al-gorithm to correctly indicate an improvement.
In the second two scenarios, a systemchange that should have decreased F-score increased it instead.
For these, we would ex-pect the restrictive algorithm to correctly indicate a reduction.
Thus, we are interestedin identifying those documents for which one algorithm signaled an improvement andthe other signaled a reduction in score between a pair of system states.It turns out that the output for 42 of the 53 relevant documents (79%) displayedthis behavior for at least one pair of system states.
The magnitude of the differencebetween document scores varied greatly, from 0.10 to 25.00 points of F-score.
In thecase of the 25.00 point difference, the standard algorithm indicated a change from59.62 to 51.69, whereas the restrictive algorithm indicated achange from 34.62 to 51.69.Thus, while both algorithms assigned the same score to the second system state, thestandard algorithm had assigned undeserved credit to the first system state, and whatshould have resulted in a 17-point improvement was shown instead as an 8-pointreduction.To sum, the problems we noted in Section 3 are not peculiar to the example shownin Figure 2; examples exhibiting this behavior are readily found in practice.4.3 Effects of Stricter Alignment on Entire MUC-6 Development Set ResultsIn an actual development setting, of course, developers generally do not focus ondifferences in F-score for a single text, but rely instead on the scores assigned to theentire 100-text development corpus.
Although the previous discussion showed thatour development scenarios occur in practice, it is quite possible that these document-level differences were inconsequential n terms of the feedback obtained for the entiredevelopment set.
We thus look at the difference between the algorithms with respectto the scores they assign to this larger set.4.3.1 Effect on Scores.
Unsurprisingly, the restrictive algorithm assigned a lower over-all score than the standard algorithm for all 53 system states.
The magnitude of thereduction ranged from 2.09 to 4.97 points of F-score, averaging 3.29 points.4.3.2 Diverging Indications Between System States.
We now ask if the two algorithmsever disagree about whether the difference between two system states constitutes apositive or negative change.
This occurred for 90 of the 1,378 system state pairs (6.5%).The magnitude of the difference between the changes of performance measured byeach algorithm ranged from 0.14 to 2.55 points, averaging 1.38.
The magnitude was246Kehler, Bear, and Appelt Accurate Alignment in Evaluationless than I point in 33 cases, between i and 2 points in 42 cases, and was greater than2 points in 15 cases.These differences, and in particular those in the 2-point range, are large enoughthat they could affect a developer's decision about whether to adopt proposed systemchanges.
In the case in which the difference was 2.55 points, for instance, a posi-tive change was reported by the standard algorithm as a negative one: It indicated adecrement in performance of 1.26 points (from 56.74 to 55.48), whereas the restrictivealgorithm indicated an increment of 1.29 points (from 51.84 to 53.13).
In our experi-ence, by the time an IE system is in the 50-point performance range on the MUC-6task, the majority of further progress results from a series of incremental changes thathave a relatively small affect on the overall score.
Thus, a change that decreases per-formance by 1.26 points will almost certainly be removed from the system, whereas achange that increases performance by 1.29 will almost certainly be kept.There were also cases in which a negative change was reported by the standardalgorithm as a positive one.
For one pair of system states, for example, the standardalgorithm indicated an increase of 2.13 points (from 51.49 to 53.62), whereas the restric-tive algorithm indicated a reduction of 0.26 (from 48.91 to 48.65).
Again, this differencecould cause a developer to keep rules in the system that negatively impact he qualityof its output.4.4 SummaryA study of the results of a 13 1/2-month effort focused on the MUC-6 task suggests thatthe problems described in Section 3 are not merely pedagogical, but can and do occurin actual practice.
These problems are prevalent enough that they could realisticallyaffect he technology development process in an adverse manner.While it would be difficult to determine the extent o which these problems mayhave affected evelopment in MUC-6, we take care to note that we do not find thattheir severity was so strong that they, in and of themselves, compromised the integrityof the MUC-6 evaluation process.
Indeed, the existing evidence suggests that anyimpact he alignment criterion may have had on the manner in which MUC-6 systemswere developed, as well as how they were ranked with respect o each other in thefinal evaluation, was not likely to have been dramatic.The outcome of our study is therefore dually positive, in that the results demon-strate the potential of a lax alignment strategy to have a dramatically adverse ffecton the technology development process, without this potential having actually beenfully realized in MUC-6 itself.
It should be borne in mind that it would not be dif-ficult to construct a scenario in which the ramifications for a MUC-like task wouldhave been far more severe--with a different task specification, template structure, setof slot definitions, or scoring scheme, for instance--to the extent hat the integrity ofsuch an evaluation could be compromised.
Thus, these results erve to highlight a po-tential pitfall to be avoided in future evaluations of IE and other high-level languageprocessing tasks, which, like the MUCs, may be the principle driving forces behindtechnology development for extended periods of time.5.
ConclusionsMethods for evaluating NLP systems are essential for tracking progress during thetechnology development process.
To properly drive this process in both system build-ing and machine learning settings, it is crucial that positive and negative modificationsbe reflected as such in the feedback provided by the scoring mechanism.
We have pre-sented several scenarios which demonstrate hat the alignment strategy employed in247Computational Linguistics Volume 27, Number 2the MUC-6 evaluation creates a situation in which this requirement is not respected.Furthermore, we have shown that the problem is pervasive nough that it could re-alistically impact the development process using a larger set of development data.These results argue strongly for the use of strict and accurate alignment criteria infuture natural language evaluations and for maintaining the independence of align-ment criteria and the mechanisms used to calculate scores.
This lesson is importantbecause alignment problems will likely become exacerbated in future evaluations, asthe natural anguage applications addressed become yet more complex.AcknowledgmentsThis work was completed while the firstauthor was at SRI International.
We wouldlike to thank Nancy Chinchor, LynetteHirschman, Jerry Hobbs, David Israel,Andreas Stolcke, Beth Sundheim, MabryTyson, Ralph Weischedel, and twoanonymous reviewers for helpful commentson earlier versions of the paper.
Allopinions expressed herein remain our own.This work would not have been possiblewithout he contributions of the followingpeople at SRI: Jerry Hobbs, David IsraelMegumi Kameyama, David Martin, KarenMyers, and Mabry Tyson.ReferencesAberdeen, John, John Burger, David Day,Lynette Hirschman, Patricia Robinson,and Marc Vilain.
1995.
MITRE:Description of the Alembic system usedfor MUC-6.
In Proceedings ofthe SixthMessage Understanding Conference (MUC-6),pages 141-155, Columbia, MD,November.
Morgan Kaufmann.Appelt, Douglas E., Jerry R. Hobbs, JohnBear, David Israel, Megumi Kameyama,Andy Kehler, David Martin, Karen Myers,and Mabry Tyson.
1995.
SRI InternationalFASTUS system MUC-6 test results andanalysis.
In Proceedings ofthe Sixth MessageUnderstanding Conference (MUC-6),pages 237-248, Columbia, MD,November.
Morgan Kaufmann.Chinchor, Nancy and Gary Dungca.
1995.Four scorers and seven years ago: Thescoring method for MUC-6.
In Proceedingsof the Sixth Message UnderstandingConference (MUC-6), pages 33-38,Columbia, MD, November.
MorganKaufmann.CSL Special Issue.
1998.
Special Issue onEvaluation in Language and SpeechTechnology.
Computer Speech and Language,12(4).Grishman, Ralph and Beth Sundheim.
1995.Design of the MUC-6 evaluation.
InProceedings ofthe Sixth MessageUnderstanding Conference (MUC-6),pages 1-11, Columbia, MD, November.Morgan Kaufmann.Hirschrnan, L. 1998.
The evolution ofevaluation: Lessons from the messageunderstanding conferences.
ComputerSpeech and Language, 12(4):281-305.Sparck-Jones, Karen and Julia Rose Galliers.1996.
Evaluating Natural LanguageProcessing Systems: An Analysis and Review.Lecture notes in computer science, 1083.Springer.van Rijsbergen, C. J.
1979.
InformationRetrieval.
Butterworths, London.248
