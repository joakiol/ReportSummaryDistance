Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 176?179,Paris, October 2009. c?2009 Association for Computational LinguisticsThe effect of correcting grammatical errors on parse probabilitiesJoachim WagnerCNGLSchool of ComputingDublin City University, Irelandjwagner@computing.dcu.ieJennifer FosterNCLTSchool of ComputingDublin City University, Ireland.jfoster@computing.dcu.ieAbstractWe parse the sentences in three parallel er-ror corpora using a generative, probabilis-tic parser and compare the parse probabil-ities of the most likely analyses for eachgrammatical sentence and its closely re-lated ungrammatical counterpart.1 IntroductionThe syntactic analysis of a sentence provided bya parser is used to guide the interpretation processrequired, to varying extents, by applications suchas question-answering, sentiment analysis and ma-chine translation.
In theory, however, parsing alsoprovides a grammaticality judgement as shown inFigure 1.
Whether or not a sentence is grammati-cal is determined by its parsability with a grammarof the language in question.The use of parsing to determine whether a sen-tence is grammatical has faded into the back-ground as hand-written grammars aiming to de-scribe only the grammatical sequences in a lan-guage have been largely supplanted by treebank-derived grammars.
Grammars read from treebankstend to overgenerate.
This overgeneration is un-problematic if a probabilistic model is used to rankanalyses and if the parser is not being used to pro-vide a grammaticality judgement.
The combina-tion of grammar size, probabilistic parse selectionand smoothing techniques results in high robust-ness to errors and broad language coverage, de-sirable properties in applications requiring a syn-tactic analysis of any input, regardless of noise.However, for applications which rely on a parser?sability to distinguish grammatical sequences fromungrammatical ones, e.g.
grammar checkers, over-generating grammars are perhaps less useful asthey fail to reject ungrammatical strings.A naive solution might be to assume that theprobability assigned to a parse tree by its proba-bilistic model could be leveraged in some way toFigure 1: Grammaticality and formal languagesdetermine the sentence?s grammaticality.
In thispaper, we explore one aspect of this question byusing three parallel error corpora to determine theeffect of common English grammatical errors onthe parse probability of the most likely parse treereturned by a generative probabilistic parser.2 Related WorkThe probability of a parse tree has been used be-fore in error detection systems.
Sun et al (2007)report only a very modest improvement when theyinclude a parse probability feature in their systemwhose features mostly consist of linear sequentialpatterns.
Lee and Seneff (2006) detect ungram-matical sentences by comparing the parse proba-bility of a possibly ill-formed input sentence to theparse probabilities of candidate corrections whichare generated by arbitrarily deleting, inserting andsubstituting articles, prepositions and auxiliariesand changing the inflection of verbs and nouns.Foster et al (2008) compare the parse probabil-ity returned by a parser trained on a regular tree-bank to the probability returned by the same parsertrained on a ?noisy?
treebank and use the differ-ence to decide whether the sentence is ill-formed.Research in the field of psycholinguistics hasexplored the link between frequency and gram-maticality, often focusing on borderline acceptablesentences (see Crocker and Keller (2006) for a dis-cussion of the literature).
Koonst-Garboden andJaeger (2003) find a weak correlation between the176frequency ratios of competing surface realisationsand human acceptability judgements.
Hale (2003)calculates the information-theoretic load of wordsin sentences assuming that they were generated ac-cording to a probabilistic grammar and finds thatthese values are good predictors for observed read-ing time and other measures of cognitive load.3 Experimental SetupThe aim of this experiment is to find out towhat extent ungrammatical sentences behave dif-ferently from correct sentences as regards theirparse probabilities.
There are two types of corporawe study: two parallel error corpora that consistof authentic ungrammatical sentences and manualcorrections, and a parallel error corpus that con-sists of authentic grammatical sentences and auto-matically induced errors.
Using parallel corporaallows us to compare pairs of sentences that havethe same or very similar lexical content and dif-fer only with respect to their grammaticality.
Acorpus with automatically induced errors is in-cluded because such a corpus is much larger andcontrolled error insertion allows us to examine di-rectly the effect of a particular error type.The first parallel error corpus contains 1,132sentence pairs each comprising an ungrammaticalsentence and a correction (Foster, 2005).
The sen-tences are taken from written texts and contain ei-ther one or two grammatical errors.
The errors in-clude those made by native English speakers.
Wecall this the Foster corpus.
The second corpusis a learner corpus.
It contains transcribed spo-ken utterances produced by learners of English ofvarying L1s and levels of experience in a class-room setting.
Wagner et al (2009) manually cor-rected 500 sentences of the transcribed utterances,producing a parallel error corpus which we callGonzaga 500.
The third parallel corpus contains199,600 sentences taken from the British NationalCorpus and ungrammatical sentences produced byintroducing errors of the following five types intothe original BNC sentences: errors involving anextra word, errors involving a missing word, real-word spelling errors, agreement errors and errorsinvolving an incorrect verbal inflection.All sentence pairs in the three parallel cor-pora are parsed using the June 2006 versionof the first-stage parser of Charniak and John-son (2005), a lexicalised, generative, probabilisticparser achieving competitive performance on WallStreet Journal text.
We compare the probability ofthe highest ranked tree for the grammatical sen-tence in the pair to the probability of the highestranked tree for the ungrammatical sentence.4 ResultsFigure 2 shows the results for the Foster corpus.For ranges of 4 points on the logarithmic scale,the bars depict how many sentence pairs have aprobability ratio within the respective range.
Forexample, there are 48 pairs (5th bar from left) forwhich the correction has a parse probability whichis between 8 and 12 points lower than the parseprobability of its erroneous original, or, in otherwords, for which the probability ratio is betweene?12 and e?8.
853 pairs show a higher probabil-ity for the correction vs. 279 pairs which do not.Since the probability of a tree is the product ofits rule probabilities, sentence length is a factor.If we focus on corrections that do not change thesentence length, the ratio sharpens to 414 vs. 90pairs.
Ungrammatical sentences do often receivelower parse probabilities than their corrections.Figure 3 shows the results for the Gonzaga 500.Here we see a picture similar to the Foster cor-pus although the peak for the range from e0 = 1to e4 ?
54.6 is more pronounced.
This timethere are more cases where the parse probabilitydrops despite a sentence being shortened and viceversa.
Overall, 348 sentence pairs show an in-creased parse probability, 152 do not.
For sen-tences that stay the same length the ratio is 154to 34, or 4.53:1, for this corpus which is almostidentical to the Foster corpus (4.60:1).How do these observations translate to the artifi-cial parallel error corpus created from BNC data?Figure 4 shows the results for the BNC data.
Inorder to keep the orientation of the graph as be-fore, we change the sign by looking at decrementsinstead of increments.
Also, we swap the keysfor shortened and lengthened sentences.
Clearly,the distribution is wider and moved to the right.The peak is at the bar labelled 10.
Accordingly,the ratio of the number of sentence pairs aboveand below the zero line is much higher than be-fore (overall 32,111 to 167, 489 = 5.22, for samelength only 8,537 to 111,171 = 13.02), suggest-ing that our artificial errors might have a strongereffect on parse probability than authentic errors.Another possible explanation is that the BNC dataonly contains five error types, whereas the range of177Figure 2: Effect of correcting erroneous sentences (Foster corpus) on the probability of the best parse.Each bar is broken down by whether and how the correction changed the sentence length in tokens.
Abar labelled x covers ratios from ex?2 to ex+2 (exclusive).Figure 3: Effect of correcting erroneous sentences (Gonzaga 500 corpus) on the probability of the bestparse.Figure 4: Effect of inserting errors into BNC sentences on the probability of the best parse.178errors in the Foster and Gonzaga corpus is wider.Analysing the BNC data by error type and look-ing firstly at those error types that do not involve achange in sentence length, we see that:?
96% of real-word spelling errors cause a re-duction in parse probability.?
91% of agreement errors cause a reduction inparse probability.
Agreement errors involvingarticles most reliably decrease the probability.?
92% of verb form errors cause a reduction.Changing the form from present participle topast participle1 is least likely to cause a reduc-tion, whereas changing it from past participleto third singular is most likely.The effect of error types which change sentencelength is more difficult to interpret.
Almost all ofthe extra word errors cause a reduction in parseprobability and it is difficult to know whether thisis happening because the sentence length has in-creased or because an error has been introduced.The errors involving missing words do not system-atically result in an increase in parse probability?
41% of them cause a reduction in parse proba-bility, and this is much more likely to occur if themissing word is a function word (article, auxiliary,preposition).Since the Foster corpus is also error-annotated,we can also examine its results by error type.
Thisanalysis broadly agrees with that of the BNC data,although the percentage of ill-formed sentencesfor which there is a reduction in parse probabilityis generally lower (see Fig.
2 vs.
Fig.
4).5 ConclusionWe have parsed the sentences in three parallel er-ror corpora using a generative, probabilistic parserand examined the parse probability of the mostlikely analysis of each sentence.
We find thatgrammatical errors have some negative effect onthe probability assigned to the best parse, a find-ing which corroborates previous evidence linkingsentence grammaticality to frequency.
In our ex-periment, we approximate sentence probability bylooking only at the most likely analysis ?
it mightbe useful to see if the same effect holds if we sum1This raises the issue of covert errors, resulting in gram-matical sentence structures.
Lee and Seneff (2008) give theexample I am prepared for the exam which was produced bya learner of English instead of I am preparing for the exam.These occur in authentic error corpora and cannot be com-pletely avoided when automatically introducing errors.over parse trees.
To fully exploit parse or sentenceprobability in an error detection system, it is nec-essary to fully account for the effect on probabilityof 1) non-structural factors such as sentence lengthand 2) particular error types.
This study repre-sents a contribution towards the latter.AcknowledgementsWe are grateful to James Hunter from GonzagaUniversity for providing us with a learner corpus.We thank Josef van Genabith and the reviewers fortheir comments and acknowledge the Irish Cen-tre for High-End Computing for the provision ofcomputational facilities.
The BNC is distributedby Oxford University Computing Services.ReferencesEugene Charniak and Mark Johnson.
2005.
Course-to-fine n-best-parsing and maxent discriminativereranking.
In Proceedings of ACL.Matthew W. Crocker and Frank Keller.
2006.
Prob-abilistic grammars as models of gradience in lan-guage processing.
In Gisbert Fanselow, C. Fe?ry,R.
Vogel, and M. Schlesewsky, editors, Gradiencein Grammar: Generative Perspectives, pages 227?245.
Oxford University Press.Jennifer Foster, Joachim Wagner, and Josef van Gen-abith.
2008.
Adapting a WSJ-trained parser togrammatically noisy text.
In Proceedings of ACL.Jennifer Foster.
2005.
Good Reasons for Noting BadGrammar: Empirical Investigations into the Pars-ing of Ungrammatical Written English.
Ph.D. the-sis, University of Dublin, Trinity College.John Hale.
2003.
The information conveyed by wordsin sentences.
Journal of Psycholinguistic Research,32(2):101?123.Andrew Koontz-Garboden and T. Florian Jaeger.2003.
An empirical investigation of the frequency-grammaticality correlation hypothesis.
Student es-say received or downloaded on 2006-03-13.John Lee and Stephanie Seneff.
2006.
Automaticgrammar correction for second-language learners.In Interspeech 2006 - 9th ICSLP, pages 1978?1981.John Lee and Stephanie Seneff.
2008.
Correcting mis-use of verb forms.
In Proceedings of ACL.Guihua Sun, Xiaohua Liu, Gao Cong, Ming Zhou,Zhongyang Xiong, John Lee, and Chin-Yew Lin.2007.
Detecting erroneous sentences using automat-ically mined sequential patterns.
In Proc.
of ACL.Joachim Wagner, Jennifer Foster, and Josef van Gen-abith.
2009.
Judging grammaticality: Experimentsin sentence classification.
CALICO Journal, 26(3).179
