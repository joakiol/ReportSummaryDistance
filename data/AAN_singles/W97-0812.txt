Using Semantic Similarity to Acquire Cooccurrence Restrictions from CorporaAntonio SanfilippoSHARP Laboratories of EuropeOxford Science ParkOxford OX4 4GA, UKantonio@sharp, co. ukAbstractWe describe a method for acquiring semanticcooccurrence r strictions for tuples of syntacti-cally related words (e.g.
verb-object pairs) fromtext corpora utomatically.
This method uses thenotion of semantic similarity to assign a sensefrom a dictionary database (e.g.
WordNet) toambiguous words occurring in a syntactic de-pendency.
Semantic similarity is also used tomerge disambiguated word tuples into classes ofcooccurrence r strictions.
This encoding makesit possible to reduce subsequent disambiguationevents to simple table lookups.1 IntroductionAlthough the assessment of semantic similarity using adictionary database as knowledge source has been rec-ognized as providing significant cues for word clustering(Resnik 1995b) and the determination of lexical cohe-sion (Morris & Hirst, 1991), its relevance for word dis-ambiguation in running text remains relatively unex-plored.
The goal of this paper is to investigate ways inwhich semantic similarity can be used to address thedisambiguation of syntactic collocates with specificreference to the automatic acquisition of semantic ooc-currence restrictions from text corpora.A variety of methods have been proposed to ratewords for semantic similarity with reference to an ex-isting word sense bank.
In Rada et al (1989), semanticsimilarity is evaluated as the shortest path connecting theword senses being compared in a hierarchically struc-tured thesaurus.
Kozima & Furugori (1993) Measureconceptual distance by spreading activation on a seman-tic network derived from LDOCE.
Resnik (1995a) de-fines the semantic similarity between two words as theentropy value of the most informative concept subsum-ing the two words in a hierarchically structured thesau-rus.
A comparative assessment of these methods fallsoutside the scope of this paper as the approach to disam-biguation we propose is in principle compatible withvirtually any treatment of semantic similarity.
Rather,our objective is to show that given a reliable calculationof semantic similarity, good results can be obtained inthe disambiguation of words in context.
In the workdescribed here, Resnik's approach was used.Following Resnik, semantic similarity is assessed withreference to the WordNet lexical database (Miller, 1990)where word senses are hierarchically structured.
Forexample, (all senses of) the nouns clerk and salespersonin WordNet are connected to the first sense of the nounsemployee, worker, person so as to indicate that clerk andsalesperson are a kind of employee which is a kind ofworker which in turn is a kind of person.
In this case, thesemantic similarity between the words clerk and sales-person would correspond to the entropy value of em-ployee which is the most informative (i.e.
most specific)concept shared by the two words.
Illustrative xtracts ofWordNet with specific reference to the examples usedthroughout the paper are provided in table 1.The information content (or entropy) of a concept c ---which in WordNet corresponds to a set of such asfire_v_4, dismiss_v_4, terminate_v_4, sack v 2 --- is for-mally defined as -log p(c) (Abramson, 1963:6-13).
Theprobability of a concept c is obtained for each choice oftext corpus or corpora collection K by dividing the fre-quency of c in K by the total number of words W ob-served in K which have the same part of speech p as theword senses in c:(1) prob(cp) = fieq(cp)WpThe frequency of a concept is calculated by countingthe occurrences of all words which are potential in-stances of (i.e.
subsumed by) the concept.
These includewords which have the same orthography and part ofspeech as the synonyms defining the concept as well asthe concept's uperordinates.
Each time a word Wr~isencountered in K, the count of each concepts Cp ~ub-suming Wp (in any of its senses) is increased by one:(2) fieq(cp) = E count(Wp)c. e{x~,sub(x, Wp)}The semantic similarity between two words Wlp W2pis expressed as the entropy value of the most informativeconcept cp which subsumes both Wlp and W2p, asshown in (3).82max(3) ssm(Wlp, W2p) = \[- log p(cp)\] cp e {x \[ sub(x, Wlp) ^  sub(x, W2p)}The specific senses of Wlp W2p under which semanticsimilarity holds is determified with respect o the sub-sumption relation linking Cp with Wlp ;f2p.
Suppose forexample that in calculating the semantic similarity ofthe two verbs fire, dlsmtss using the WordNet lexicaldatabase we find that the most informative subsumingconcept is represented by the synonym set containing theword sense remove v 2.
We will then know that thesenses for fire, dismiss under which the similarity holdsare fire v 4 and dismiss v 4 as these are the only in-stances of the verbs fire and dismlss subsumed by re-move v 2 in the WordNet hierarchy.We propose to use semantic similarity to disambiguatesyntactic ollocates and to merge disambiguated collo-cates into classes of cooccurrence r strictions.
Disam-biguation of syntactic ollocates results from intersect-ing pairs consisting of (i) a cluster containing all sensesof a word collocate W1 having appropriate syntacticusage, and (it) a cluster of semantically similar wordsenses related to W1 by the same syntactic dependency,e.g.
:(4) IN: < {firej_213141617/8},{clerk_n_i/2, employee_n_l} >< {fire_v2~3~4~6/7/8},{gun n 1, rockeL n_l} >< {hire v_3, recruit_v 2},{clerk n_1/2} >< {dismiss v 4, fire_v_4},{clerk_n_1/2} ?OUT: < {fire v_4}, {clerk n_1/2} ?The results of distinct disambiguation events are mergedinto pairs of semantically compatible word clusters usingthe notion of semantic similarity.2 Ext ract ion  o f  Syntact i c  Word  Co l lo -cates  f rom CorporaFirst, all instances of the syntactic dependency pairsunder consideration (e.g.
verb-object, verb-subject, ad-jective-noun) are extracted from a collection of textcorpora using a parser.
In performing this task, only themost important words (e.g.
heads of immediate constitu-ents) are chosen.
The chosen words are also lemmatized.For example, the extraction of verb-object collocatesfrom a text fragment such as have certamly htred thebest financial analysts tn the area would yield the pair <hire, analyst >.The extracted pairs are sorted according to the syntac-tic dependency involved (e.g.
verb-object).
All pairswhich involve the same dependency and share one wordcollocate are then merged.
Each new pair consists of aunique associating word and a set of associated wordscontaining all "statistically relevant" words (see below)which are related to the associating word by the samesyntactic dependency, e.g.
(5) IN: < fire_v, gun_n >< firev, rocket n >< fire_v, employee_n >< fire_v, clerk n ?< firev, hymn n ?< fwev, rate_n >OUT: < fre_v,{gun_n,rocket n,employee_n,clerk_n} >IN: < firev, employee_n >< dtsmms_v, employee_n ?< hire_v, employee_n >< recruitv,  employee_n ?< attract_v, employee_n?< be_v, employeen?< makev,  employee_n>< affectv, employee_n?OUT: < {fire_v,dmmiss_v,hire v,recruit v},employee_n ?The statistical relevance of associated words is de-fined with reference to their conditional probability.
Forexample, consider the equations in (6) where the nu-meric values express the (conditional) probability ofoccurrence in some corpus for each verb in (5) given thenoun employee.
(6) fi'eq(fire v \[ employee_n)= .3freq(dlsmiss v \[ employee_n)= .28freq(hlre_v \[ employee_n)= .33freq(recrmt v \[ employee_n)= .22fi-eq(attract v \[ employee n) = .02freq(be v \[ employee_n) = .002freq(make v \] employee_n)= .005freq(affect_v \[employee_n?
)= .01These conditional probabilities are obtained by dividingthe number of occurrences of the verb with employee bythe total number of occurrences of the verb with refer-ence to the text corpus under consideration, as indicatedin (7).
(7) prob(W1 ~ W2) = count(W1, W2)count(Wl)Inclusion in the set of statistically relevant associated wordsis established with reference toa threshold TI which can beeither selected manually or determined automatically asthemost ubiquitous probability value for each choice of associ-ating word.
For example, the threshold T1 for the selectionof verbs taking the noun employee as direct object withreference to the conditional probabilities in (6) cart be cal-culated as follows.
First, all probabilities in (6) are distrib-uted over a ten-bin template, where each bin is to receiveprogressively larger values tarting from a fixed lowestpoint greater than 0, e.g.
:83y rom_~._ 02 - _L.
_L__3 .To 1 2 3 43Values -- -- 28 33I i 224\[  5 I 6 7 ~.98 -~F~"-T '  -7  " "Then one of the values from the bin containing mostelements (e.g.
the lowest) is chosen as the threshold.The exclusion of collocates which are not statisticallyrelevant in the sense specified above makes it possible toavoid interference from collocations which do not pro-vide sufficiently specific exemplifications of word us-age.3 Word C luster ing  and Sense Expans ionEach pair of syntactic ollocates at this stage consists ofeither?
an associating head word (AING) and a set of de-pendent associated words (AED), e.g.< AING: fire_v,AED: {gun n,rocket_n,employee_n,clerk n} >?
or an associating dependent word (AING) and a setof associated head words (AED), e.g.< AED: {fire_v, dismiss v,hire v,recruit_v},AING: employee_n>The next step consists in partitioning the set of associ-ated words into clusters of semantically congruent wordsenses.
This is done in three stages.1.
Form all possible unique word pairs with non-identicalmembers out of each associated word set, e.g.IN: {fire, dismiss, htre, recrmt}OUT: {ftre-dismms,fire-htre,fire-recrult,dmmms-hlre,dmmiss-recrmt,hire-recruit}IN: {gun,rocket,employee,clerk}OUT: {gun-rocket,gun-employee,gun-clerk,rocket-employee,rocket-clerk, employee-clerk}.
Find the semantic similarity (expressed as a numericvalue) for each such pair, specifying the senses withrespect to which the similarity holds (if any), e.g.IN: {fire-dasmms,fire-hwe,fire-recrult,dmrmss-hlre,dmmms-reeruit,hire-recrmt}OUT: {sim(fire_v4,dismiss_v_4) = 6.124,sim(fire,hirel = 0,ram(fire,recruit) = O,sirnldmmms,hire I = 0,stm(dismiss,recruit) = 0,sim(hlrev_3,recruit_v_2) = 3.307}IN: {gun-rocket,gun-employee,gun-clerk, rocket-employee,rocket-clerk,employee-clerk}OUT: {s~m(gun_n_l,rocket n_l) = 5.008,mm(gun_n_l-3,employee n_l) = 1.415,ram(gun_n_ 1-3,clerk n 1/2) = 1.415,mm(rocket_n_3,employee_n 1} = 2.255,stm(rocket n_3,clerk._n l /2\] = 2.255,stm(employee_n_l,clerk n 1/2\] = 4.144}The assessment of semantic similarity and the ensuingword sense specification are carried out using Resnik'sapproach (see section 1).3.
Fix the threshold for membership into clusters of se-mantically congruent word senses (either manually or bycalculation of the most ubiquitous emantic similarityvalue) and generate such clusters.
For example, assum-ing a threshold value of 3, we will have:IN: {stm(fire_v_4,dmmiss_v4) = 6.124,strrt(fire,htre) = 0,szm(fire,recrmt,} = 0,sim(dtsmms,htre} = O,szrn(dismtss,recrult} = 0,sire(hire v_3,recruikv_2} = 3.307}OUT: {fire v_4,dismiss v 4}{hlre_v 3, recru It_v_2}IN: {stm(gun_n_l,rocket_nl I = 5.008,s~m(gun_n_i/2/3,employee_nl} = 1.415,stm(gun_n_I / 2 / 3,clerk_n_i / 2} = 1.415,stm(rocket_n 3,employee n_1) = 2.255,stm(rocketn_3,clerkn_l/2} = 2.255,stm(employee_n i ,clerk n_l/2} = 4.144}OUT: {clerk n i/2,employee_n_1}{gun_n_ i ,rocket n_1}Once associated words have been partitioned into se-mantically congruent clusters, new sets of collocationsare generated as shown in (8) by?
pairing each cluster of semantically congruent asso-ciated words with its associating word, and?
expanding the associating word into all of its possi-ble senses.At this stage, all word senses which are syntacticallyincompatible with the original input words are removed.For example, the intransitive verb senses fire v 1 andfire_v_5 (see table 1) are eliminated since the occurrenceof fire in the input collocation which we are seeking todisambiguate relates to the transitive use of the verb.Note that the noun employee has only one sense inWordNet (see table 1); therefore, employee has a singleexpansion when used as an associating word.84(8) IN: < AED: { {h:re_v_3,recruit_v_2},{dmmiss v_4,fire_y_4} ,AING: employee_n >OUT: < {hire v 3,recrutLv_2}, {employee_n_1} >< {dismtss_v 4,fire v_4}, {employee n_l} >IN: < AING:fire v,AED:{ {clerk_n_l,clerk n_2,employee_n_l},{gun_n 1,rocket_n_1} } >OUT: < {fire_v_2~3~4~6~7~8},{clerk n i/2,employee_n_l} >< {fire v_2/3/4/6/7/8},{gun n_l,rocket_n I} >4 Disambiguating the "Associating"Word and Merging DisambiguatedCollocationsThe disambiguation f the associating word is performedby intersecting correspondent subsets across pairs of thenewly generated collocations.
In, the case of verb-objectpairs, for example, the subsets of these new sets con-taining verbs are intersected and likewise the subsetscontaining objects are intersected.
The output comprisesa new set which is non-empty if the two sets have one ormore common members in both the verb and object sub-sets.
For the specific example of newly expanded collo-cations given in (8), there is only one pairwise intersec-tion producing anon empty result, as shown in (9).
(9) IN: < {fwe_v_2/3/4/6/7/8},{clerk n i /2,employee_n_l} >< {dlsmiss v_4,flre_v_4} ,{employee_n_l} >OUT: < {fire v_4}, {employee n_l} >All other pairwise intersections are empty as there are noverbs and objects common to both sets of each pairwisecombination.The result of distinct disambiguation events can bemerged into pairs of semantically compatible wordclusters using the notion of semantic similarity.
Forexample, the verbs and nouns of all the input pairs in(10) are closely related in meaning and can therefore bemerged into a single pair.
(10) IN: < fire_v 4 ,  employee_n_l >< dmmiss v_4, clerk n_l >< give_the_axe_v_1 , salesclerk_n_1 >< sack v_2, shop_clerk n 1 >< terminate_v_4, clerk n_2 >OUT: < {fire_v_.4, dmmlss_v_4, sack v_2,give_the_axe_v_ 1, termmate_v 4},{clerkn_l, employee_n_1, clerk_n_2salesclerk n 1, shop_clerk_n 1} >5 Storing ResultsPairs of semantically congruent word sense clusters uchas the one shown in the output of (10) are stored ascooccurrence r strictions o that future disambiguationevents involving any head-dependent word sense pair inthem can be reduced to simple table lookups.The storage procedure is structured in three phases.First, each cluster of word senses in each pair is assigneda unique code consisting of an id number and the syn-tactic dependency involved:(I I) < {I 02_VO, fire_v_4, diatoms v_4, sack_v 2,g,ve_the axe v_ I, send_away_v_2,force_out v_2, terminate v 4},{I 02_OV, clerk_n_I/2, employee_n I,salesclerk_n_1, shop_clerk n I} >< {103_VO, lease v_4, rent v_3, hire.N_3,charter_v 3, engage_v 6, take_v_22,recruxt_v_2},{102 OV, clerk n_1/2, employee_n_l,salesclerkn_l,  shop_clerk_n_l} >< {104VO, shoot_v3, flre v 1 .... },{104_OV, gun_n_l, rocket n_l .... } >Then, the cluster codes in each pair are stored in a cooc-currence restriction table:\] 102_VO , 102_OV I103VO , 103_OV104VO , I04_OVFinally, each word sense is stored along with its associ-ated cluster code(s):fire v_4dismms v_4clerk_n_ 1 / 2employee_n 1h,re_v 3recruit v 2shoot_v 3f i rev  1gun_n_1rocket n_l102_VO102VO102VOi 02_VO103VO102VOi 04_VO104_VOI04VOi04VOThe disambiguation of a pair of syntactically relatedwords such as the pair <fire_v, employee_n> can be car-ried out byretrieving all the cluster codes for each word in thepair and create all possible pairwise combinations,e.g.IN: < fire v ,  employee_n >OUT: < 102_VO, 102_OV >< i04 VO, 102_OV >85?
eliminating code pairs which are not in the table ofcooccurrence r strictions for cluster codes, e.g.INPUT: < 102 VO, 102_OV ?< 104_VO, 102_OV ?OUTPUT: < 102_VO, 102_OV ?using the resolved cluster code pairs to retrieve theappropriate s nses of the input words from previ-ously stored pairs of word senses and cluster codessuch as those in the table above, e.g.INPUT: < \[fire v, 102 VO\] ,\[employee_n, 102_OV\] ?OUTPUT: < fire v_4, employee_n_l ?By repeating the acquisition process described in sec-tions 2-4 for collections of appropriately selected sourcecorpora, the acquired cooccurrence r strictions can beparameterized for sublanguage specific domains.
Thisaugmentation can be made by storing each word senseand associated cluster code with a sublanguage specifi-cation and a percentage descriptor indicating the relativefrequency of the word sense with reference to the clus-ter code in the specified sublanguage, .g.flre_v 4fire_v_4fire_v 1fire v_l102_VO Business 65%102VO Crime 25%104VO Business 5%104VO Crime 70%6 Statist ical ly Inconspicuous CollocatesBecause only statistically relevant collocations are cho-sen to drive the disambiguation process (see section 2),it follows that no cooccurrence r strictions will be ac-quired for a variety of word pairs.
This, for example,might be the case with verb-object pairs such as < firev,hand_n > where the noun is a somewhat atypical object.This problem can be addressed by using the cooccur-rence restrictions already acquired to classify statisti-cally inconspicuous collocates, as shown below withreference to the verb object pair < firev, hand n >.Find all verb-object ooccurrence r strictions con-taining the verbfire, which as shown in the previoussection are< 102_VO, 102_OV >< 104 VO, 104_OV ??
Retrieve all members of the direct object collocateclass, e.g.102OV -> clerk_n_ 1/2, employee_n_1104OV -> gun_n_l, rocket n_lCluster the statistically inconspicuous collocate with allmembers of the direct object collocate class.
This willprovide one or more sense classifications for the statisti-cally inconspicuous collocate.
In the .present case, theWordNet senses 2 and 9 (glossed as "farm labourer"and "crew member" respectively) are given whenhand_n clusters with clerk n 1/2 and employee_n_1,e.g.IN: {hand_n, clerk n_l/2, employee_n 1,gun_n_ 1, rocketL n_l}OUT: {hand_n_2/9, clerk_n 1/2, employee_n_1}{gun_n_1, rocketn_l}Associate the disambiguated statistically incon-spicuous collocate with the same code of the wordsenses with which it has been clustered, e.g.IIhand In 12 I 10"O \[I hand n g 102_VOThis will make it possible to choose senses 2 and 9for hand in contexts where hand occurs as the directobject of verbs such asfire, as explained in the pre-vious section.7 Pre l iminary Results and Future WorkA prototype of the system described was partially im-plemented to test the effectiveness of the disambiguationmethod.
The prototype comprises:a component performing semantic similarity judge-ments for word pairs using WordNet (this is an im-plementation fResnik's approach);a component which turns sets of word pairs rated forsemantic similarity into clusters of semantically con-gruent word senses, anda component which performs the disambiguation fsyntactic ollocates in the manner described in sec-tion 4.The current functionality provides the means to disam-biguate a pair of words <W1 W2> standing in a givensyntactic relation Dep given a list of words related to W1by Dep, a list of words related to W2 by Dep, and a se-mantic similarity threshold for word clustering, as shownin (12).In order to provide an indication of how well the sys-tem performs, a few examples are presented in (12).
Ascan be confirmed with reference to the WordNet entriesin table 1, these preliminary results are encouraging asthey show a reasonable resolution of ambiguities.
Amore thorough evaluation is currently being carried out.86(12) IN: < fire_v-\[employee n,clerk_n, gun_n,plstol_nl,\[fire,dasmlss, htre,recrmt\]-employee_n, 3 >OUT: < fire_v_4 employee n_l >IN" < fire v-\[employee_n,clerk._n, gun_n,plstol n\],\[fire v,shoot_v, pop v,&sharge v\]-gun , 3 >OUT: < fire_~..1 gun_n_l >IN: < wear_v-\[sult...n,garment_n, clothes_.n,umform n\],\[wear_v, have_onv, record v,iile v\]-smt_n, 3 >OUT.
< wear_v_l/9 smt n_l >IN.
< file_v-\[sult_.n,proceedmgs n, lawsult_n,htagataon n\],\[wear,have_on, record_v,file v\]-sult n, 3 >OUT.
< file_v 1/5 sult_n_2 >Note that disambiguation can yield multiple senses, asshown with reference to the resolution of the verbs f i leand wear in the third and fourth examples hown in (12).Multiple disambiguation results typically occur whensome of the senses given for a word in the source dic-tionary database are close in meaning.
For example, bothsense 1 and 9 of wear relate to an eventuality of"clothing oneself".
Multiple word sense resolutions canbe ranked with reference to the semantic similarityscores used in clustering word senses during disam-biguation.
The basic idea is that the word sense resolu-tion contained in the word cluster which has highestsemantic similarity scores provides the best disambigua-tion hypothesis.
For example, specific word senses forthe verb-object pair < wear suit > in the third example of(12) above are given by the disambiguated word tuplesin (13) which arise from intersecting pairs consisting ofall senses of an associating word and a semanticallycongruent cluster of its associated words, as describedin section 4.
(13) { < {have_on_v_l,wear v I},{clothes n 1 ,garment n_l, suit_n_1,uniform n 1} >< {file v_2,wear_v_9},{clothes n_l,garment_n_l,  sult_n_l,uniform_n l} > }Taking into account he scores shown in (14), the bestword sense candidate for the verb wear in the contextwear suit would be wear_v 1.
In this case, the semanticsimilarity scores for the second cluster (i.e.
the nouns)do not matter as there is only one such cluster.
(14) szm(have_on_v_l, wear_v 1) = 6.291sim(file_v_2, wear v_9} = 3.309Preliminary results suggest hat the present reatmentof disambiguation can achieve good results with smallquantities of input data.
For example, as few as fourinput collocations may suffice to provide acceptableresults, e.g.
(15) IN: < flre_v-\[employee_n,clerk n\],\[fire,dlsmiss\]-employee_n, 3 ?OUT: < fire_v_4 employee_n_1 ?IN: < wear v-\[star n,clothes_n\],\[wear_v,have_on_v\]-suit_n, 3 >OUT: < wea_ v_l su l tn  1 >This is because word clustering --- which is the decisivestep in disambiguation --- is carried out using a measureof semantic similarity which is essentially induced fromthe hyponymic links of a semantic word net.
As long asthe collocations chosen as input data generate someword clusters, there is a good chance for disambiguation.The reduction of input data requirements offers a sig-nificant advantage compared with methods uch as thosepresented in Brown et al (1991), Gale et al (1992),Yarowsky (1995), and Karol & Edelman (1996) wherestrong reliance on statistical techniques for the calcula-tion of word and context similarity commands largesource corpora.
Such advantage can be particularly ap-preciated with reference to the acquisition of cooccur-rence restrictions for those sublanguage domains wherelarge corpora are not available.Ironically, the major advantage of the approach pro-posed --- namely, a reliance on structured semantic wordnets as the main knowledge source for assessing seman-tic similarity --- is also its major drawback.
Semanticallystructured lexical databases, especially those which aretuned to specific sublanguage domains, are currently noteasily available and expensive to build manually.
How-ever, advances in the area of automatic thesaurus dis-covery (Grefenstette, 1994) as well as progress in thearea of automatic merger of machine readable diction-aries (Sanfilippo & Poznanski, 1992; Chang & Chen,1997) indicate that availability of the lexical resourcesneeded may gradually improve in the future.
In addition,ongoing research on rating conceptual distance fromunstructured synonym sets (Sanfilippo, 1997) may soonprovide an effective way of adapting any commerciallyavailable thesaurus to the task of word clustering, thusincreasing considerably the range of lexical databasesused as knowledge sources in the assessment of semanticsimilarity.AcknowledgementsThis research was carried out within the SPARKLEproject (LE-12111).
I am indebted to Geert Adriaens,Simon Berry, Ted Briscoe, Ian Johnson, Victor Poznan-ski, Karen Sparck Jones, R.alf Steinberger and YorickWilks for valuable feedback.ReferencesN.
Abramson.
1963.
Information Theory and Coding.McGraw-Hill, NY.87P.
Brown, S. Pietra, V. Pietra & R. Mercer.
1991.
Wordsense disambiguation using statistical methods.
In Pro-ceedmgs of ACL, pp.
264-270.J.
Chang and J. Chert.
1997.
Topical Clustering of MRDSenses based on Information Retrieval Techniques.
Ms.Dept.
of Computer Science, National Tsing Hua University,Taiwan.W.
Gale, K. Church & D. Yarowsky.
1992.
A method fordisambiguating word senses in a large corpus.
Computersand the Humanities, 26:415-439.G.
Grefenstette.
1994.
Explorations inAutomatw ThesaurusDiscovery.
Kluwer Academic Publishers, Boston.Y.
Karov & S. Edelman.
1993.
Learning similarity-basedword sense disambigu~ion &om sparse data.
AvailableFout!
Bladwi\]zer abet gedef~nieerd.
~ paperNo.9605009H.
Kozima & T. Furugori.
1993.
Similarity between WordsComputed by Spreading Activation on an English Diction-ary.
In Proceedings of EACL.G.
Miller.
1990 Five Papers on WordNet.
Special issue ofthe International Journal of Lexicography, 3 (4).J.
Morris & G. Hirst.
1991.
Lexical Cohesion Computedby Thesaural Relations as an Indicator of the Structure ofText.
ComputatzonalLmguistws, 17:21-48.R.
Rada, M. Hafedh, E. Bicknell and M. Blettner.
1989.Development and application of a metric on semantic nets.IEEE Transactions on System, Man, and Cybernetws,19(1):17-30.P.
Resnik.
1995a.
Using information content to evaluatesemantic similarity in a taxonomy.
In Proceedmgs of1JCALP.
Resnik.
1995b.
Disarnbiguating noun groupings withrespect to WordNet Senses.
In Proceedings of 3rd Work-shop on Very Large Corpora.
Association for Computa-tional Linguistics.A.
Sanfilippo.
1997.
Rating conceptual distance using ex-tended synonym sets.
Ms. SHARP Lab.
of Europe, Oxford.A.
Sanfilippo and V. Poznanski.
1992.
The Acquisition ofLexical Knowledge from Combined Machine-ReadableDictionary Sources.
In Proceedings of the 3rd Conferenceon Apphed Natural Language Processing, Trento.D.
Yarowsky.
1995.
Unsupervised Word Sense Disam-biguation Rivaling Supervised Methods.
In Proceedings ofthe 33rd Annual Meeting of the ACL, pp.
189-96.dismissfilefirehirerecruitwearemployeeclerkgunrocketsuitShe dismissed his advancesput out of judicial considerationstop assocmtlng withgive noticeend one's encounter with somebody bycausing or permitting the person to leaveregzster in a pubhc office or in a court of lawsmooth with a fileproceed in filefile a formal charge againstplace in a fileopen firefire a gun, fire a bulletof potterygive noticeThe gun firedCall forth, of emotions, feelings, and re-sponsesThey burned the house and his diariesprowde with, fuelengage or hire for workof goods and servicesengage tn a commercial transactmnregister formally, as a participant or memberThe lab director eermted an able crew ofassistantsconscript, levybe dressed inHe wore a red ribbonhave In one's aspectWear ones haw m a certain wayhold out, endurewear off, wear out, wear thingo to pReceswear output clothing on one's bodya worker who is hired to perform ajobkeeps records or accountsa salesperson i  a storea salesperson i  a storea weapon that discharges a missilelarge but transportable gunsa pedal or hand-operated lever that controlsthe throttleany weapon propelled by a rocket enginea device containing its own propellant anddriven by reaction propulsionerect European annual often grown as a saladcrop to be harvested when young and tenderpropels bright hght high in the sky, or usedto propel a lifesaving hne or harpoonsends a firework display high into the skya set of garments for outerwear all of the samefabric and color88a judicial proceeding brought by one pattyagainst anothersuingany of four sets of 13" cards in a packTable I Extract entries of the WordNet Lexical Database.Synonyms (in boldface) and examples (in italic) are omittedunless used in place of definitions.
Other thesaural relations ---e.g.
hyponymy, holonymy, etc.
--- are also omitted.89
