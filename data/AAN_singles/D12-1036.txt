Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 391?401, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsAnswering Opinion Questions on Products by Exploiting HierarchicalOrganization of Consumer ReviewsJianxing Yu, Zheng-Jun Zha, Tat-Seng ChuaSchool of ComputingNational University of Singapore{jianxing, zhazj, chuats}@comp.nus.edu.sgAbstractThis paper proposes to generate appropriateanswers for opinion questions about prod-ucts by exploiting the hierarchical organiza-tion of consumer reviews.
The hierarchy orga-nizes product aspects as nodes following theirparent-child relations.
For each aspect, the re-views and corresponding opinions on this as-pect are stored.
We develop a new frameworkfor opinion Questions Answering, which en-ables accurate question analysis and effectiveanswer generation by making use the hierar-chy.
In particular, we first identify the (ex-plicit/implicit) product aspects asked in thequestions and their sub-aspects by referringto the hierarchy.
We then retrieve the corre-sponding review fragments relevant to the as-pects from the hierarchy.
In order to gener-ate appropriate answers from the review frag-ments, we develop a multi-criteria optimiza-tion approach for answer generation by simul-taneously taking into account review salience,coherence, diversity, and parent-child rela-tions among the aspects.
We conduct eval-uations on 11 popular products in four do-mains.
The evaluated corpus contains 70,359consumer reviews and 220 questions on theseproducts.
Experimental results demonstratethe effectiveness of our approach.1 IntroductionWith the rapid development of E-commerce, mostretail websites encourage consumers to post reviewsto express their opinions on the products.
For exam-ple, the review ?The battery of Nokia N95 is amaz-ing.?
reveals positive opinion on the aspect ?bat-Figure 1: Overview of product opinion-QA frameworktery?
of product Nokia N95.
An aspect here refersto a component or an attribute of a certain prod-uct.
Numerous consumer reviews are now availableonline, and these reviews contain rich opinionatedinformation on various aspects of products.
Theyare naturally a valuable resource for answering opin-ion questions about products, such as ?How do peo-ple think about the battery of Nokia N95??
Opin-ion Question Answering (opinion-QA) on productsseeks to uncover consumers?
thinking and feelingabout the products or aspects of products.
It is dif-ferent from traditional factual QA, where the ques-tions ask for the fact, such as ?Where is the capitalof United States??
and the answer is ?Washington,D.C.
?For a product opinionated question, the answershould not be just a best answer.
It should reflect theopinions of various segments of users, and incorpo-391rate both positive and negative viewpoints.
Hencethe answer should be a summarization of publicopinions and comments on the product or specificaspect asked in the question (Jiang et al2010).In addition, it should also include public opinionsand comments on the sub-aspects.
Such answerswould help users to understand the inherent reasonsof the opinions on the asked aspect.
For exam-ple, the question ?What do people think the cam-era of Nokia 5800??
asks for public positive andnegative opinions on the aspect ?camera?
of prod-uct ?Nokia 5800.?
The summarization of opinionson the sub-aspects such as ?lens?
and ?resolution?would help users better understand that the publiccomplaints on the aspect ?camera?
are due to thepoor ?lens?
and/or low ?resolution.?
Moreover, theanswer should be presented following the general-to-specific logic, i.e., from general aspects to spe-cific sub-aspects.
This makes the answer easier tounderstand by the users (Ouyang et al2009).Current Opinion-QA methods mainly includethree components, including question analysis thatidentifies aspects and opinions asked in the ques-tions, answer fragment retrieval, and answer gen-eration which summarizes the retrieved fragments(Lloret et al2011).
Although existing methodsshow encouraging performance, they are usually notable to generate satisfactory answers due to the fol-lowing drawbacks.
First, current methods oftenidentify aspects as the noun phrases in the questions.However, noun phrases contain noises that are notaspects.
This gives rise to imprecise aspect identifi-cation.
For example, in the question ?What reasonscan I persuade my wife that people prefer the batteryof Nokia N95??
noun phrases ?wife?
and ?people?are not aspects.
Moreover, current methods reliedon noun phrases are not able to reveal the implicitaspects, which are not explicitly asked in the ques-tions.
For example, the question ?Is iPhone 4 expen-sive??
asks about the aspect ?price?, but the term?price?
does not appear in the question.
Second,current methods cannot discover sub-aspects of theasked aspect due to its ignorance of parent-child re-lations among aspects.
Third, the answers generatedby the existing methods do not follow the general-to-specific logic, leading to difficulty in understandingthe answers.To overcome these problems, we can resort tothe hierarchical organization of consumer reviewson products.
As illustrated in Figure 2, the hier-archy organizes product aspects as nodes, follow-ing their parent-child relations.
For each aspect, thereviews and corresponding opinions on this aspectare stored.
Such hierarchy can naturally facilitate toidentify aspects asked in questions.
While explicitaspects can be recognized by referring to the hier-archy, implicit aspects can be inferred based on theassociations between sentiment terms and aspects inthe hierarchy (Yu et al2011).
The sentiment termsare discovered from the reviews on correspondingaspects.
Moreover, by following the parent-child re-lations in the hierarchy, sub-aspects of the asked as-pect can be directly acquired, and the answers canpresent aspects from general to specific.Motivated by the above observations, we proposeto exploit the hierarchical organization of consumerreviews for product opinion-QA.
As illustrated inFigure 1, our framework first organizes consumerreviews of a certain product into a hierarchical or-ganization.
The resulting hierarchy is in turn usedto help question analysis and relevant review frag-ments retrieval.
In order to generate appropriateanswers from the retrieved fragments, we developa multi-criteria optimization approach by simulta-neously taking into account review salience, co-herence, and diversity.
The parent-child relationsamong aspects are also incorporated into the ap-proach to ensure the answers be general-to-specific.We conduct evaluations on 11 popular products infour domains.
The evaluated corpus contains 70,359consumer reviews and 220 questions on these prod-ucts.
More details of the dataset are discussed inSection 4.
Experimental results to demonstrate theeffectiveness of our approach.The main contributions of this paper include,?
We propose to exploit the hierarchical organi-zation of consumer reviews for answering opin-ion questions on products.?
With the help of the hierarchy, our pro-posed framework can accurately identify (ex-plicit/implicit) aspects asked in the questions,and the corresponding sub-aspects.?
We develop a multi-criteria optimization ap-proach to generate informative, coherent, di-verse and general-to-specific answers.392Figure 2: Hierarchical organization for Nokia N95The rest of this paper is organized as follows.
Sec-tion 2 introduces the components of hierarchical or-ganization of reviews, question analysis, and answerfragment retrieval.
Section 3 elaborates the multi-criteria optimization approach for answer generation.
Section 4 presents experimental details, while Sec-tion 5 reviews related works.
Finally, Section 6 con-cludes this paper with future works.2 Hierarchical Organization, QuestionAnalysis, and Answer FragmentRetrievalLet R = {r1, ?
?
?
, r|R|} denote a collection of con-sumer reviews of a certain product.
Each review re-flects consumer opinions on the product and/or prod-uct aspects.
Let q denote an opinion question, whichasks for public opinions on a product or some as-pects of the product.
The task is to retrieve the opin-ionated review fragments relevant to the asked prod-uct/product aspects, and summarize these fragmentsto form an appropriate answer to question q.Next, we introduce the components of hierarchi-cal organization that organizes consumer reviewsinto a hierarchy, question analysis which identifiesthe products/aspects and opinions asked in the ques-tions, and answer fragment retrieval that retrieves re-view fragments relevant to the questions.2.1 Hierarchical Organization of ReviewsWe employ the method proposed by Yu et al2011)to organize consumer reviews of a product into a hi-erarchical organization.
As shown in Figure 2, thehierarchy organizes product aspects as nodes, fol-lowing their parent-child relations.
In particular, thismethod first automatically acquires an initial aspecthierarchy from the domain knowledge and identifiesaspects commented in the reviews.
It then incremen-tally inserts the identified aspects into appropriatepositions in the initial hierarchy, and finally obtainsan aspect hierarchy that allocates all the newly iden-tified aspects.
The consumer reviews are then orga-nized to their corresponding aspect nodes in the hi-erarchy.
Sentiment classification is then performedto determine consumer opinions on the reviews.The reported performance of Yu et al2011)on aspect identification, aspect hierarchy generationand sentiment classification are 0.731, 0.705, 0.787in terms of average F1-measure, respectively.2.2 Question Analysis and Answer FragmentRetrievalQuestion analysis consists of five sub-tasks: recog-nizing product asked in the question; identifying as-pects in the question; classifying opinions that thequestion asks for (the asked opinion could be posi-tive, negative or both); identifying the question type(e.g.
asking for public opinions, or the reason ofthe opinions, etc.
); and identifying the question form(i.e.
comparative question or single form question).Recognizing the product: A name entity recog-nizer 1 is trained to recognize the product name.
Inparticular, we collect 420 auxiliary questions fromYahoo!Answer 2, and manually annotate the prod-uct names (submitted as supplementary material inAppendix A).
A name entity recognizer for productis learned on these data, with unigrams and POS tagsas features.
Given a testing question, the recognizerpredicts each word as B, I, E or O, where B, I, E de-note the begin, internal, and end of a product namerespectively, and O corresponds to other words.Identifying aspects: As aforementioned, simplyextracting the noun phrases as aspects would importnoises.
Also, some ?implicit?
aspects do not ex-1http://nlp.stanford.edu/software/CRF-NER.shtml2http://answers.yahoo.com393plicitly appear in the reviews.
One simple solutionfor these problems can resort to the review hierar-chy.
The hierarchy has organized product aspects,which can be used to filter the noise noun phrasesfor accurately identifying the explicit aspects.
Forthe implicit aspects, we observe they are usuallymodified by some peculiar sentiment terms (Su etal., 2008).
For example, the aspect ?size?
is oftenmodified by the sentiment terms such as ?large?,but seldom by the terms such as ?expensive.?
Thus,there are some associations between the aspects andsentiment terms.
Such associations can be learnedfrom the hierarchy and leveraged to infer the im-plicit aspects (Yu et al2011).
In order to simul-taneously identify the (explicit/implicit) aspects, weadopt a hierarchical classification technique.
Thetechnique simultaneously learns to identify explicitaspects, and discovers the associations between as-pects and sentiment terms by multiple classifiers.
Inparticular, given a testing question, we identify itsaspect by hierarchically classify (Silla et al2011) itinto the appropriate aspect node of a particular prod-uct hierarchy.
The classification greedily searches apath in the hierarchy from top to down.
The searchbegins at the root node, and stops at the leaf nodeor a specific node where the relevance score is lowerthan a pre-defined threshold.
The relevance score oneach node is determined by a SVM classifier.
Mul-tiple SVM classifiers are learned on the hierarchy,one distinct classifier for a node.
The reviews thatare stored in the node and its child-nodes are usedas training samples.
We employ the features of nounterms, and sentiment terms in the sentiment lexiconprovided by MPQA project (Wilson et al2005).Classifying the opinions: Given a set of testingquestions, we first distinguish the opinion questionsfrom the factual ones (Yu et al2003).
Since theopinion questions often contain one or more senti-ment terms, we classify them by employing the sen-timent terms in the sentiment lexicon provided fromMPQA project (Wilson et al2005).
Subsequently,we learn a SVM sentiment classifier to determine theopinion polarity of the opinion questions.
In partic-ular, the reviews and corresponding opinions storedin the hierarchy are used as training samples, whichare represented by the unigram features.Identifying the question type: Opinion questionsare often categorized into four types (Ku et al2007),?
Attitude question, asking for public opinion ona product or product aspect, such as ?What dopeople think iPhone 3gs???
Reason question, asking for the reason of pub-lic opinion on a product or product aspect, suchas ?Why do people like iPhone 3gs???
Target question, asking for the object in thepublic opinion, such as ?Which phone is betterthan Nokia N95???
Yes/No question, asking for whether a state-ment is correct, such as ?Is Nokia N95 bad?
?We formulate the question type identification as amulti-class classification problem.
A multi-classSVM classifier 3 is learned for the classification.
Wecollect 420 auxiliary questions from Yahoo!Answerand manually annotate their types (submitted as sup-plementary material in Appendix B).
These ques-tions are used for training, with POS tags and ques-tion words (i.e.
why, what, how, do, is) as features.Identifying the question form: Question form in-cludes single and comparative.
A question is viewedas comparative if it contains comparative adjectivesand adverbs (e.g.
cheaper, etc.
), otherwise as the sin-gle form (Moghaddam et al2011).
The POS tagsare exploited to detect comparative adjectives (i.e.tag ?JJR?)
and adverbs (i.e.
tag ?RBR?
).After analyzing the question, we retrieve all re-view sentences on the asked aspect and all its sub-aspects from a certain product hierarchy, and choosethe ones relevant to the opinion asked in the ques-tion.
For the single form question, we view theretrieved sentences as the answer fragments.
Forthe comparative questions, we select comparativesentences on the compared products from the re-trieved sentences, and treat them as the answer frag-ments.
Subsequently, question type is used to definethe template for the answers.
In particular, for thequestions asking for reason and attitude, we gener-ate the answers by summarizing corresponding an-swer fragments.
For questions seeking for a targetas the answer, we output the product names basedon the majority voting of the opinions in the re-trieved answer fragments.
For the yes/no questions,we first generate the ?yes/no?
answer based on the3http://svmlight.joachims.org/svm multiclass.html394consistency between the asked opinions and the ma-jor opinions in the answer fragments, and then sum-marize these fragments to form the answers.3 Answer GenerationAnswer generation aims to generate an appropriateanswer for a given opinion question based on theretrieved answer fragments, i.e., review sentences.An answer is essentially a sequence of sentences.Hence, the task of answer generation is to select sen-tences from the retrieved answer fragments and or-der them appropriately.
We formulate this task intoa multi-criteria optimization problem.
We incorpo-rate multiple criteria in the answer generation pro-cess, including answer salience, coherence, and di-versity.
The parent-child relations between aspectsis also incorporated to ensure the answer follow thegeneral-to-specific logic.
In the next subsections, wewill introduce details of the proposed multi-criteriaoptimization approach.3.1 FormulationWe first introduce the multiple criteria and thenpresent the optimization problem.Salience is used to measure the representative-ness of the answer.
A good answer should consistof salient review sentences.
Let S denote the setof retrieved sentences.
We define a binary variablesi ?
{0, 1} to indicate the selection of sentence ifor the answer, i.e.
si = 1 (or 0) indicates that si isselected (or not).
Let ?i denote the salience of sen-tence i.
The estimation of ?i will be described inSection 3.2.
The salience score of the answer (i.e.,a set of sentences) is computed by summing up thescores of all its constituent sentences, as?i?S ?isi.Coherence is used to quantify the readability ofan answer.
To make the answer readable, the con-stituent sentences in the answer should be orderedproperly.
That is, the adjacent sentences shouldbe coherent.
We define ei,j ?
{0, 1} to indicatewhether the sentences i and j are adjacent in the an-swer; where ei,j = 1 (or 0) means they are (or not)adjacent.
The coherence between two adjacent sen-tences is measured by cij .
The estimation of cij willbe described in Section 3.3.
As aforementioned, theanswer is expected to be presented in a general-to-specific manner, i.e.
from general aspects to specificsub-aspects.
We define hi,j in Eq.1 to measure thegeneral-to-specific coherence of sentences i and j.hi,j ={e?1leveli?levelj ; if leveli ?= levelj ;1; otherwise,(1)where leveli denotes level position of the aspectcommented in sentence i by referring to the hi-erarchy, with the root level being 0.
The coher-ence score of the answer is computed by sum-ming up the scores of all its adjacent sentences as,?j?S?i?S hi,jci,jei,j .Diversity.
A good answer should diversely coverall the important information.
We introduce a ma-trix M in Eq.2 to measure the pairwise diversitiesamong sentences.
Mij corresponds to the diversitybetween sentences i and j.
When sentences i andj comment on the same aspects, Mij will favor toselect the pair of sentences that discusses on diversecontent (i.e.
low similarity).
Otherwise, the pair ofsentences commented on different aspects is viewedto be diverse, and Mij is set as a constant biggerthan one.Mij ={1?
similar(i, j) if i, j commented on same aspect?
otherwise,(2)where ?
is a constant 4.Multi-Criteria Optimization We integrate theabove criteria into the multi-criteria optimizationformulation,max{?1 ?
?i?S ?isi + ?2 ?
?j?S?i?S hi,jci,jei,j+ ?3 ?
?j?S?i?S siMij ;{si, ei,j ?
{0, 1},?i, j;?1 + ?2 + ?3 = 1, 0 ?
?1, ?2, ?3 ?
1,(3)where ?1, ?2, ?3 are the trade-off parameters.We further incorporate the following constrainsinto the optimization framework, so as to derive ap-propriate answers.?
The length of the answer is up to K,?i?Slisi ?
K, (4)where li is the length of sentence i.?
When sentence i is not selected (i.e.
si = 0),the adjacency between any sentence to i is set4Empirically set to 10 in the experiment.395to zero (i.e.
?i?S ei,j =?i?S ej,i = 0).When sentence i is selected, there are two sen-tences adjacent to sentence i in the answer, onebefore i and another after i.
(i.e.
?i?S ei,j =?i?S ej,i = 1).
?i?Sei,j =?i?Sej,i = sj , ?j.
(5)?
In order to avoid falling into a cycle in sentenceselection, we employ the following constraints(Deshpande et al2009).
?i?S f0,i = n + 1;?i?S fi,n+1 ?
1;?i?S fi,j ?
?i?S fj,i = sj , ?j;0 ?
fi,j ?
(n + 1) ?
ei,j , ?i, j,(6)where the variable fi,j is an integer to numberthe selected adjacent sentences from 1 to n+1,and the first selected sentence is numbered f0,i= n + 1.
If the last selected sentence obtains anumber fi,n+1 which is bigger then 1, then theselection has no cycle.SolutionGiven the salience weights ?i|Si=1, and coherenceweights ci,j |Si,j=1, the above multi-criteria optimiza-tion problem can be solved by Integer Linear Pro-gramming (Schrijver et al1998).
The optimal so-lutions si|Si=1 and ei,j |Si,j=1 indicate the selected sen-tences and the order of them.
In the next subsec-tions, we will introduce the estimations of ?i|Si=1and ci,j |Si,j=1.3.2 Salience Weight EstimationThe salience weight of sentence i is formulated as?i =?Gg=1 ?g(i)/G, where ?
(i) denotes the mea-surement for the importance of sentence i.
We de-fine seven measurements (i.e.
G = 7) below.Helpfulness: Many forum websites provide ahelpfulness score, which is used to rate the qualityof a review.
The sentences that come from helpfulreviews are often representative (Mizil et al2009).We compute ?
(i) of sentence i by using helpfulnessscore from its host review.Timeliness: The new coming sentence often con-tains more updated and useful information (Liu etal., 2008).
?
(i) is the post time of sentence i. Wenormalize it to [0, 1].Grammaticality: The grammatical sentence isoften more readable.
We employ the method inAgichtein et al2008) to calculate the grammarscore.
In particular, ?
(i) is calculated by the KL-divergence between language models of sentence ito Wikipedia articles.Position: The first sentence in a review is usu-ally informative (He et al2011).
?
(i) is computedbased on the position of the sentence in the review,i.e.
?
(i) = 1/positioni.Aspect Frequency: The sentence that contains thefrequent aspects is often salient (Nishikawa et al2010).
Hence, ?
(i) is computed as the sum of thefrequency for aspects in sentence i.Centroid Distance: As aforementioned, reviewsentences are stored in the corresponding aspectnodes in the hierarchy.
The sentence that is close tothe centroid of the reviews stored in an aspect nodeis more likely to be salient (Erkan et al2004).
?
(i)is computed as the Cosine similarity between sen-tence i to the corresponding review cluster centroidbased on the unigram features.Local Density: The sentence would be informa-tive when it is in the dense part of the aspect nodein the feature space (Scott et al1992).
We em-ploy Multivariate Kernel Density Estimation to es-timate the density.
We first represent all the sen-tences stored in each node into feature vectors, withunigram as features.
The density of a sentence isthen calculated as ?
(x) =?ni=1 KH(x?
xi)/n,where x denotes the feature vector of sentence i,n is the size of sentences stored in the node, andKH(x) = (2?
)?1/2 exp(?1/2(xTx)) representsthe Gaussian kernel.3.3 Coherence Weight EstimationThe coherence ci,j between sentences i and j isformulated as ci,j = ?
?
?
(i, j), where ?
is aweight vector, and ?
(i, j) denotes the feature func-tion.
?
(i, j) takes two sentences i and j as input,and outputs a vector with each dimension indicatingthe present/absent of a feature.
In order to capturethe sequential relations among sentences, we utilizefeatures as the Cartesian product over the terms ofN-gram (N=1,2) and POS tags generated from sen-tences i and j (Lapata et al2003).To learn the weight vector ?, we employthe Passive-Aggressive algorithm (Crammer et al3962006).
It is an online learning algorithm, so that wecan update the weight when more consumer reviewsare available.
The algorithm takes up one trainingsample and outputs the solution that has the highestscore under the current weight.
If the output differsfrom training samples, the weight vector is updatedaccording to Eq.7.
Since the consumer reviews ofteninclude multiple sentences, we can directly use theadjacency of these sentences as training samples.
Inparticular, we treat the adjacent sentence pairs in thereviews as training samples (i.e.
ci,j = 1).min??
?i+1 ?
?i??
{?i+1 ??(p,q?)?
?i+1 ??
(p, q?)
?
?(q?,q?);?(q?,q?)
= 2?T (q?,q?
)m(m?1)/2 ,(7)where ?i is the current weight vector and ?i+1 isthe updated vector, q?
and q?
are the gold standardand predicted sequence of sentences, respectively, pdenotes a set of sentences, ?(?)
is the feature func-tion on the whole feature space (i.e.??(?
)), ?
(?, ?
)is a Kendall?s tau lost function (Lapata et al2006),T (?, ?)
represents the number of inversion operationsthat needs to bring q?
to q?, and m denotes the num-ber of sentences.4 EvaluationsIn this section, we evaluate the effectiveness of theproposed approach, in terms of question analysisand answer generation.4.1 Data Set and Experimental SettingsWe employed the product review dataset used in Yuet al2011) as corpus.
As illustrated in Table 1, thedataset contained 70,359 reviews about 11 popularproducts in four domains.
In addition, we created220 questions for these products by referring to realquestions in Yahoo!Anwser service.
We correctedthe typos and grammar errors for these real ques-tions.
Each product contains 15 opinion questionsand 5 factual questions, respectively.
All questionswere shown in Appendix C in supplementary mate-rial.
Three annotators were invited to generate thegold standard.
Each question was labeled by twoannotators.
The labels include product name, prod-uct aspect, opinion, question type and question form.The average inter-rater agreement in terms of Kappastatistics is 89%.
These annotators were then invitedto read the reviews, and create the ground truth an-swers by selecting and ordering some review sen-tences.
Such process is time consuming and labor-intensive.
We speed up the annotation process as fol-lows.
We first collected all the review sentences inthe answers generated by three evaluated methods tobe discussed in Section 4.3.1.
In addition, we sam-pled the top-N (N=20) sentences on each asked as-pect and its sub-aspects respectively, where the sen-tences were ranked based on their salient weights inSection 3.2.
We then provided such subset of reviewsentences to the three annotators, and let them indi-vidually create an answer of up to 100 words (i.e.K=100) for each question.Product Name Domain Review# Sentence#Canon EOS 450D (Canon EOS) camera 440 628Fujifilm Finepix AX245W (Fujifilm) camera 541 839Panasonic Lumix DMC-TZ7 (Panasonic) camera 650 1,546Apple MacBook Pro (MacBook) laptop 552 4,221Samsung NC10 (Samsung) laptop 2,712 4,946Apple iPod Touch 2nd (iPod Touch) MP3 4,567 10,846Sony NWZ-S639 16GB (Sony NWZ) MP3 341 773BlackBerry Bold 9700 (BlackBerry) phone 4,070 11,008iPhone 3GS 16GB (iPhone 3GS) phone 12,418 43,527Nokia 5800 XpressMusic (Nokia 5800) phone 28,129 75,001Nokia N95 phone 15,939 44,379Table 1: Statistics of the product review dataset, # denotesthe number of the reviews/sentences.We employed precision (P), recall (R) and F1-measure (F1) as the evaluation metric for questionanalysis, and utilized ROUGE (Lin et al2003) asthe metric to evaluate the quality of answer gener-ation.
ROUGE is a widely accepted standard forsummarization, which measures the quality of thesummarized answers by counting the overlapping N-grams between the answers generated by machineand human, respectively.
In the experiment, wereported the F1-measure of ROUGE-1, ROUGE-2and ROUGE-SU4, which count the overlapping un-igrams, bigrams and skip-4 bigrams respectively.ROUGE-1 can measure informativeness of the an-swers, while higher order ROUGE-N (N=2,4) cap-tures the matching of subsequences, which can mea-sure the fluency and readability of the answers.
Forthe trade-off parameters, we empirically set ?1 =0.4, ?2 = 0.3 and ?3 = 0.3.4.2 Evaluations on Question AnalysisWe first evaluated the performance of product recog-nition, opinion/factual question classification, opin-ion classification, question type and question formidentification.
The experimental results are shown397in Table 2.
The results show that traditional methodsachieve encouraging performance on the aforemen-tioned tasks.Evaluated Topics P R F1Product recognition 0.755 0.618 0.680Opinion/factual 0.897 0.895 0.893Opinion classification 0.755 0.745 0.748Question type 0.800 0.775 0.783Question form 0.910 0.903 0.905Table 2: Performance of question analysis.Methods P R F1Our method 0.851* 0.763* 0.805*Balahur?s method 0.825 0.400 0.538Table 3: Performance of aspect identification for questionanalysis.
* denotes the results (i.e.
P , R, F1) are testedfor statistical significance using T-Test, p-values<0.05.Methods P R F1Our method 0.726* 0.643* 0.682*Su?s method 0.689 0.571 0.625Table 4: Performance of implicit aspect identification forquestion analysis.
T-Test, p-values<0.05We next examined the performance of our ap-proach on aspect identification.
The method pro-posed by Balahur et al2008) was reimplementedas the baseline, which identifies aspects based onnoun phrase extraction.
This method achieved goodperformance on the opinion QA task in TAC 2008and was employed in subsequent works.
As demon-strated in Table 3, our approach significantly outper-forms Balahur?s method by over 49.4% in terms ofaverage F1-measure.
A probable reason is that Bal-ahur?s method relies on noun phrases, which maymis-identify some noise noun phrases as aspects,while our approach performs hierarchical classifica-tion based on the hierarchy, which can leverage theprior knowledge encoded in the hierarchy to filterout the noise and obtain accurate aspects.Moreover, we evaluated the effectiveness of ourapproach on implicit aspect identification.
The 70implicit aspect questions in our question corpuswere used here.
The method proposed by Su et al(2008) was reimplemented as the baseline.
It identi-fies implicit aspects by mutual clustering, and it wasFigure 3: Evaluations on multiple optimization criteriain terms of ROUGE-1, ROUGE-2, and ROUGE-SU4, re-spectively.evaluated in Yu et al2011).
As shown in Table 4,our approach significantly outperforms Su?s methodby over 9.1% in terms of average F1-measure.
Theresults show that the hierarchy can help to identifyimplicit aspects by exploiting the underlying associ-ations among sentiment terms and aspects.Methods ROUGE1 ROUGE2 ROUGE-SU4Our method 0.364* 0.137* 0.138*Li?s method 0.127 0.043 0.049Lloret?s method 0.149 0.058 0.065Table 5: Performance of answer generation.
T-Test, p-values<0.05.4.3 Evaluations on Answer Generation4.3.1 Comparisons to the State-of-the-ArtsWe compared our multi-criteria optimization ap-proach against two state-of-the-arts methods: a) the398method presented in Li et al2009), which selectssome retrieved sentences to generate the answersbased on a graph-based algorithm; b) the methodproposed by Lloret et al2011) that forms the an-swers by re-ranking the retrieved sentences.As shown in Table 5, our approach outperformsLi?s method and Lloret?s method by the significantabsolute gains of over 23.7%, and 21.5% respec-tively, in terms of average ROUGE-1.
It improvesthe performance over these two methods in termsof average ROUGE-2 by the absolute gains of over9.41% and 7.87%, respectively; and in terms ofROUGE-SU4 by the absolute gains of over 8.86%and 7.31%, respectively.
By analyzing the results,we find that the improvements come from the useof the hierarchical organization and the answer gen-eration algorithm which exploits multiple criteria,especially the parent-child relation among aspects.In addition, our approach can generate the answersby following the general-to-specific logic, while Li?sand Lloret?s methods fail to do so due to their igno-rance of parent-child relations among aspects.4.3.2 Evaluations on the Effectiveness ofMultiple CriteriaWe further evaluated the effectiveness of each op-timization criterion by tuning the trade-off parame-ters (i.e.
?1, ?2, and ?3).
We fixed ?1 as a con-stant in [0, 1] with 0.1 as an interval, and updated ?2from 0 to 1 ?
?1, ?3 = 1 ?
?1 ?
?2, correspond-ingly.
The performance change is shown in Figure3 in terms of ROUGE-1, ROUGE-2, and ROUGE-SU4, respectively.
The best performance is achievedat ?1 = 0.4, ?2 = 0.3, ?3 = 0.3.
We observe theperformance drops dramatically when any parame-ter (i.e.
?1, ?2, ?3) is close to 0 (i.e.
remove any ofthe corresponding criterion).
Thus, we can concludethat all the criteria are useful in answer generation.We also find that the performance change is sharpwhen ?1 changes.
This indicates that the saliencecriterion is crucial for answer generation.Table 6 shows the exemplar answers generated byour approach.
Each answer first gives the statis-tic of positive and negative reviews.
This helpsuser to quickly get an overview of public opin-ions.
The summary of relevant review sentencesis then presented in the answer.
The answer di-versely comments the asked aspect and all its avail-able sub-aspects following the general-to-specificlogic.
Moreover, we feel that the answers are in-formative and readable.5 Related WorksIn this section, we review existing works relatedto the four components of our approach, includingorganization of reviews, question analysis, answerfragment retrieval, and answer generation.For organization of reviews, Carenini et al2006)proposed to organize the reviews by a hand-craftedtaxonomy, which was not scalable.
Yu et al2011)exploited the domain knowledge and consumer re-views to automatically generate a hierarchy for or-ganizing consumer reviews.Question analysis often has to distinguish theopinion question from the factual one, and find thekey points asked in the questions, such as the prod-uct aspect and product name.
For example, Yu etal.
(2003) proposed to separate opinions from factsat both document and sentence level, and determinethe polarity on the opinionated sentences in the an-swer documents.
Similarly, Somasundaran et al(2007) utilized a SVM classifier to recognize opin-ionated sentences.
The paper argued that the sub-jective types (i.e.
sentiment and arguing) can im-prove the performance of opinion-QA.
Later, Ku etal.
(2007) proposed a two-layered classifier for ques-tion analysis, and retrieved the answer-fragments bykeyword matching.
In particular, they first identifiedthe opinion questions, and classified them into sixpredefined question types, including holder, target,attitude, reason, majority, and yes/no.
These ques-tion types and corresponding polarity on the ques-tions were used to filter non-relevant sentences inthe answer fragments.
F1-measure was employed asthe evaluation metric.For the topic of answer generation in opinion-QA,Li et al2009) formulated it as a sentence rankingtask.
They argued that the answers should be simul-taneously relevant to topics and opinions asked inthe questions.
They thus designed the graph-basedmethods (i.e.
PageRank and HITS) to select somehigh-ranked sentences to form answers.
They firstbuilt a graph on the retrieved sentences, with eachsentence as the node, and the similarity (i.e.
Co-sine similarity) between each sentences pair as the399Question 1: What reasons do people give for preferring iPhone 3gs?There are 9,928 opinionated reviews about product ?iphone 3gs?, with 5,717 positive and 4,221 negative reviews.This phone is amazing and I would recommend it to anyone.
It looks funky and cool.
It is worth the money.
It?s greatorganiser, simple easy to use software.
It is super fast, excellent connection via wifi or 3G.
It is able to instantly access email.It?s amazing and has so many free apps.
The design is so simple and global.
The hardware is good and reliable.
The camera isa good and colors are vibrant.
The touch screen is user friendly and the aesthetics are top notch.
Battery is charged quickly,and power save right after stop using.Question 2: Does anyone think it is expensive to get a iPhone 3GS?Yes.There are 2,645 opinionated reviews on aspect ?price?
about product ?iphone 3gs?, with 889 positive and 1,756 negativereviews.Throw the costly phone, apple only knows to sell stupid stuff expensively.
Don?t fool yourself with iPhone 3gs, believing that itcosts much by Apple luxurious advertising.
Apple is so greedy and it just wants to earn easy & fast money by selling itstechless product expensively.
The phone will charge once you insert any sim card.
iPhone 3gs is high-priced due to thecapacitive and Apple license.
You need to pay every application at the end it costs too much.
The network provider will makeup some of the cost of the phone on your call charges.Table 6: Sample answers of our approach.weight of the corresponding edge.
Given a question,its similarity to each sentence in the graph was com-puted.
Such similarity was viewed as the relevantscore to the corresponding sentence.
The sentencesthen were ranked based on three metric, i.e.
relevantscore to the query, similarity score obtained from thegraph algorithm over sentences, and degree of opin-ion matching to the query.
Respectively, Lloret etal.
(2011) proposed to form answers by re-rankingthe retrieved sentences based on the metric of wordfrequency, non-redundancy and the number of nounphrases.
Their method includes three components,including information retrieval, opinion mining andtext summarization.
Evaluations were conducted onthe TAC 2008 Opinion Summarization track.
After-wards, Moghaddam et al2011) developed a systemcalled AQA to generate answers for questions aboutproducts (i.e.
opinion QA on products).
It classi-fies the questions into five types, including target,attitude, reason, majority and yes/no.
As comparedto Ku et al2007), the question types of holderand majority are not included.
They argued thatproduct questions were seldom asked for the hold-ers, since the holders (i.e.
reviewers) were com-monly shown in the reviews.
Also, product ques-tions mainly asked for majority opinions, and ma-jority type was thus not considered.
The AQA sys-tem includes five components, including questionanalysis, question expansion, high quality review re-trieval, subjective sentence extraction, and answergrouping.
The answers are generated by aggregat-ing opinions in the retrieved fragments.6 Conclusions and Future WorksIn this paper, we have developed a new productopinion-QA framework, which exploits the hierar-chical organization of consumer reviews on prod-ucts.
With the help of the hierarchical organization,our framework can accurately identify the aspectsasked in the questions and also discover their sub-aspects.
We have further formulated the answer gen-eration from retrieved review sentences as a multi-criteria optimization problem.
The multiple criteriaused include answer salience, diversity, and coher-ence.
The parent-child relations between the aspectsare incorporated into the approach to ensure that theanswers follow the general-to-specific logic.
Theproposed framework has been evaluated on 11 pop-ular products in four domains using 220 questionson the products.
Significant performance improve-ments were obtained.
In the future, we will explorethe more sophisticated NLP features to improve theproposed framework.
This will be done by incorpo-rating more NLP features in salience and coherenceweights estimation.AcknowledgmentsThis work is supported in part by NUS-Tsinghua Ex-treme Search (NExT) project under the grant num-ber: R-252-300-001-490.
We give warm thanks tothe project and anonymous reviewers for their com-ments.400ReferencesE.
Agichtein, C. Castillo, and D. Donato.
Finding High-Quality Content in Social Media.
WSDM, 2008.A.
Balahur, E. Boldrini, O. Ferrandez, A. Montoyo,M.
Palomar, and R. Munoz.
The DLSIUAES Team?sParticipation in the TAC 2008 Tracks.
TAC, 2008.C.
Cardie, J. Wiebe, T. Wilson, and D. Litman.
Com-bining Low-level and Summary Representations ofOpinions for Multi-Perspective Question Answering.AAAI, 2003.G.
Carenini, R. Ng, and E. Zwart.
Multi-document Sum-marization of Evaluative Text.
ACL, 2006.P.
Cimiano.
Ontology Learning and Population fromText: Algorithms, Evaluation and Applications.Springer-Verlag New York, Inc. Secaucus, NJ, USA,2006.K.
Crammer, O. Dekel, J. Keshet, S.S. Shwartz, andY.
Singer.
Online Passive Aggressive Algorithms.Journal of Machine Learning Research, 2006.P.
Deshpande, R. Barzilay, and D.R.
Karger.
Random-ized Decoding for Selection-and-Ordering Problems.NAACL, 2007.G.
Erkan and D.R.
Radev.
LexRank: Graph-based lexi-cal centrality as salience in text summarization.
AAAI,2004.T.
Givon.
Syntax: A functional-typological Introduction.Benjamins Pub, 1990.J.
He and D. Dai.
Summarization of Yes/No QuestionsUsing a Feature Function Model.
JMLR, 2011.P.
Jiang, H. Fu, C. Zhang, and Z. Niu.
A Framework forOpinion Question Answering.
IMS, 2010.H.D.
Kim, D.H. Park, V.G.V.
Vydiswaran, andC.X.
Zhai.
Opinion Summarization using Entity Fea-tures and Probabilistic Sentence Coherence Optimiza-tion: UIUC at TAC 2008 Opinion Summarization Pi-lot.
TAC, 2008.D.
Koller and M. Sahami.
Hierarchically ClassifyingDocuments Using Very Few Words.
ICML, 1997.L.W.
Ku, Y.T.
Liang, and H.H.
Chen.
Question Analysisand Answer Passage Retrieval for Opinion QuestionAnswering Systems.
International Journal of Compu-tational Linguistics & Chinese Language Processing,2007.M.
Lapata.
Probabilistic Text Structuring: Experimentswith Sentence Ordering.
ACL, 2003.M.
Lapata.
Automatic Evaluation of Information Order-ing: Kendalls?
Tau.
Computational Linguistics, 2006.F.
Li, Y. Tang, M. Huang, and X. Zhu.
AnsweringOpinion Questions with Random Walks on Graphs.ACL/AFNLP, 2009.C.Y.
Lin and E.Hovy.
Automatic Evaluation of Sum-maries Using N-gram Co-Occurrence Statistics.
HLT-NAACL, 2003.Y.
Liu, X. Huang, A.
An, and X. Yu.
Modeling and Pre-dicting the Helpfulness of Online Reviews.
ICDM,2008.E.
Lloret, A. Balahur, M. Palomar, and A. Montoyo.
To-wards a Unified Approach for Opinion Question An-swering and Summarization.
ACL-HLT, 2011.H.
Nishikawa, T. Hasegawa, Y. Matsuo, and G. Kikui.Opinion Summarization with Integer Linear Program-ming Formulation for Sentence Extraction and Order-ing.
COLING, 2010.C.D.
Mizil and G. Kossinets and J. Kleinberg and L. Lee.How Opinions are Received by Online Communities:A Case Study on Amazon.com Helpfulness Votes.WWW, 2009.S.
Moghaddam and M. Ester.
AQA: Aspect-based Opin-ion Question Answering.
IEEE-ICDMW, 2011.Y.
Ouyang, W. Li, and Q. Lu.
An Integrated Multi-document Summarization Approach based on WordHierarchical Representation.
ACL-IJCNLP, 2009.A.
Schrijver.
Theory of Linear and Integer Programming.John Wiley & Sons, 1998.D.W.
Scott.
Multivariate Density Estimation: Theory,Practice, and Visualization.
John Wiley & Sons, Inc.,1992.C.
Silla and A. Freitas.
A Survey of Hierarchical Classi-fication Across Different Application Domains.
DataMining and Knowledge Discovery, 2011.S.
Somasundaran, T. Wilson, J. Wiebe and V. Stoyanov.QA with Attitude: Exploiting Opinion Type Analysisfor Improving Question Answering in Online Discus-sions and the News.
ICWSM, 2007.V.
Stoyanov, C. Cardie and J. Wiebe.
Multi-Perspective Question Answering Using the OpQACorpus.
EMNLP, 2005.Q.
Su, X. Xu, H. Guo, X. Wu, X. Zhang, B. Swen, andZ.
Su.
Hidden Sentiment Association in Chinese WebOpinion Mining.
WWW, 2008.T.
Wilson, J. Wiebe, and P. Hoffmann.
RecognizingContextual Polarity in Phrase-level Sentiment Analy-sis.
HLT/EMNLP, 2005.J.
Yu, Z.J.
Zha, M. Wang, K. Wang and T.S.
Chua.Domain-Assisted Product Aspect Hierarchy Genera-tion: Towards Hierarchical Organization of Unstruc-tured Consumer Reviews.
EMNLP, 2011.J.
Yu, Z.J.
Zha, M. Wang, and T.S.
Chua.
Hierarchi-cal Organization of Unstructured Consumer Reviews.WWW, 2011.J.
Yu, Z.J.
Zha, M. Wang and T.S.
Chua.
Aspect Rank-ing: Identifying Important Product Aspects from On-line Consumer Reviews.
ACL, 2011.H.
Yu and V. Hatzivassiloglou.
Towards AnsweringOpinion Questions: Separating Facts from Opinionsand Identifying the Polarity of Opinion Sentences.EMNLP, 2003.401
