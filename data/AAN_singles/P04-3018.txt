Resource Analysis for Question AnsweringLucian Vlad LitaCarnegie Mellon Universityllita@cs.cmu.eduWarren A. HuntCarnegie Mellon Universitywhunt@andrew.cmu.eduEric NybergCarnegie Mellon Universityehn@cs.cmu.eduAbstractThis paper attempts to analyze and bound the utilityof various structured and unstructured resources inQuestion Answering, independent of a specific sys-tem or component.
We quantify the degree to whichgazetteers, web resources, encyclopedia, web doc-uments and web-based query expansion can helpQuestion Answering in general and specific ques-tion types in particular.
Depending on which re-sources are used, the QA task may shift from com-plex answer-finding mechanisms to simpler data ex-traction methods followed by answer re-mapping inlocal documents.1 IntroductionDuring recent years the Question Answering (QA)field has undergone considerable changes: questiontypes have diversified, question complexity has in-creased, and evaluations have become more stan-dardized - as reflected by the TREC QA track(Voorhees, 2003).
Some recent approaches havetapped into external data sources such as the Web,encyclopedias, databases in order to find answercandidates, which may then be located in the spe-cific corpus being searched (Dumais et al, 2002; Xuet al, 2003).
As systems improve, the availabilityof rich resources will be increasingly critical to QAperformance.
While on-line resources such as theWeb, WordNet, gazetteers, and encyclopedias arebecoming more prevalent, no system-independentstudy has quantified their impact on the QA task.This paper focuses on several resources and theirinherent potential to provide answers, without con-centrating on a particular QA system or component.The goal is to quantify and bound the potential im-pact of these resources on the QA process.2 Related WorkMore and more QA systems are using the Web asa resource.
Since the Web is orders of magni-tude larger than local corpora, redundancy of an-swers and supporting passages allows systems toproduce more correct, confident answers (Clarke etal., 2001; Dumais et al, 2002).
(Lin, 2002) presentstwo different approaches to using the Web: access-ing the structured data and mining the unstructureddata.
Due to their complementary nature of theseapproaches, hybrid systems are likely to performbetter (Lin and Katz, 2003).Definitional questions (?What is X?
?, ?Whois X??)
are especially compatible with structuredresources such as gazetteers and encyclopedias.The top performing definitional systems (Xu etal., 2003) at TREC extract kernel facts similar toa question profile built using structured and semi-structured resources: WordNet (Miller et al, 1990),Merriam-Webster dictionary www.m-w.com), theColumbia Encyclopedia (www.bartleby.com),Wikipedia (www.wikipedia.com), a biog-raphy dictionary (www.s9.com) and Google(www.google.com).3 ApproachFor the purpose of this paper, resources consist ofstructured and semi-structured knowledge, such asthe Web, web search engines, gazetteers, and ency-clopedias.
Although many QA systems incorporateor access such resources, few systems quantify in-dividual resource impact on their performance andlittle work has been done to estimate bounds on re-source impact to Question Answering.
Independentof a specific QA system, we quantify the degree towhich these resources are able to directly provideanswers to questions.Experiments are performed on the 2,393 ques-tions and the corresponding answer keys providedthrough NIST (Voorhees, 2003) as part of the TREC8 through TREC 12 evaluations.4 GazetteersAlthough the Web consists of mostly unstructuredand loosely structured information, the availablestructured data is a valuable resource for questionanswering.
Gazetteers in particular cover severalfrequently-asked factoid question types, such as?What is the population of X??
or ?What is the cap-ital of Y??.
The CIA World Factbook is a databasecontaining geographical, political, and economi-cal profiles of all the countries in the world.
Wealso analyzed two additional data sources contain-ing astronomy information (www.astronomy.com)and detailed information about the fifty US states(www.50states.com).Since gazetteers provide up-to-date information,some answers will differ from answers in localcorpora or the Web.
Moreover, questions requir-ing interval-type answers (e.g.
?How close is thesun??)
may not match answers from differentsources which are also correct.
Gazetteers offerhigh precision answers, but have limited recall sincethey only cover a limited number of questions (SeeTable 1).CIA AllQ-Set #qtions R P R PTREC8 200 4 100% 6 100%TREC9 693 8 100% 22 79%TREC10 500 14 100% 23 96%TREC11 500 8 100% 20 100%TREC12 500 2 100% 11 92%Overall 2393 36 100% 82 91%Table 1: Recall (R): TREC questions can be directlyanswered directly by gazetteers - shown are resultsfor CIA Factbook and All gazetteers combined.
Ourextractor precision is Precision (P).5 WordNetWordnets and ontologies are very common re-sources and are employed in a wide variety of di-rect and indirect QA tasks, such as reasoning basedon axioms extracted from WordNet (Moldovan etal., 2003), probabilistic inference using lexical rela-tions for passage scoring (Paranjpe et al, 2003), andanswer filtering via WordNet constraints (Leidner etal., 2003).Q-Set #qtions All Gloss Syns HyperTREC 8 200 32 22 7 13TREC 9 693 197 140 73 75TREC 10 500 206 148 82 88TREC 11 500 112 80 29 46TREC 12 500 93 56 10 52Overall 2393 641 446 201 268Table 2: Number of questions answerable usingWordNet glosses (Gloss), synonyms (Syns), hyper-nyms and hyponyms (Hyper), and all of them com-bined All.Table 2 shows an upper bound on how manyTREC questions could be answered directly usingWordNet as an answer source.
Question terms andphrases were extracted and looked up in WordNetglosses, synonyms, hypernyms, and hyponyms.
Ifthe answer key matched the relevant WordNet data,then an answer was considered to be found.
Sincesome answers might occur coincidentally, we theseresults to represent upper bounds on possible utility.6 Structured Data SourcesEncyclopedias, dictionaries, and other webdatabases are structured data sources that are oftenemployed in answering definitional questions (e.g.,?What is X?
?, ?Who is X??).
The top-performingdefinitional systems at TREC (Xu et al, 2003)extract kernel facts similar question profiles builtusing structured and semi-structured resources:WordNet (Miller et al, 1990), the Merriam-Webster dictionary www.m-w.com), the ColumbiaEncyclopedia (www.bartleby.com), Wikipedia(www.wikipedia.com), a biography dictionary(www.s9.com) and Google (www.google.com).Table 3 shows a number of data sources andtheir impact on answering TREC questions.
N-grams were extracted from each question and runthrough Wikipedia and Google?s define operator(which searches specialized dictionaries, definitionlists, glossaries, abbreviation lists etc).
Table 3show that TREC 10 and 11 questions benefit themost from the use of an encyclopedia, since theyinclude many definitional questions.
On the otherhand, since TREC 12 has fewer definitional ques-tions and more procedural questions, it does notbenefit as much from Wikipedia or Google?s defineoperator.Q-Set #qtions WikiAll Wiki1st DefOpTREC 8 200 56 5 30TREC 9 693 297 49 71TREC 10 500 225 45 34TREC 11 500 155 19 23TREC 12 500 124 12 27Overall 2393 857 130 185Table 3: The answer is found in a definition ex-tracted from Wikipedia WikiAll, in the first defi-nition extracted from Wikipedia Wiki1st, throughGoogle?s define operator DefOp.7 Answer Type CoverageTo test coverage of different answer types, we em-ployed the top level of the answer type hierarchyused by the JAVELIN system (Nyberg et al, 2003).The most frequent types are: definition (e.g.
?Whatis viscosity??
), person-bio (e.g.
?Who was La-can??
), object(e.g.
?Name the highest mountain.?
),process (e.g.
?How did Cleopatra die??
), lexicon(?What does CBS stand for??)temporal(e.g.
?Whenis the first day of summer??
), numeric (e.g.
?Howtall is Mount Everest??
), location (e.g.
?Where isTokyo??
), and proper-name (e.g.
?Who owns theRaiders??
).AType #qtions WikiAll DefOp Gaz WNobject 1003 426 92 58 309lexicon 50 25 3 0 26defn 178 105 9 11 112pers-bio 39 15 11 0 17process 138 23 6 9 16temporal 194 63 14 0 50numeric 121 27 13 10 18location 151 69 21 2 47proper 231 76 10 0 32Table 4: Coverage of TREC questions divided bymost common answer types.Table 4 shows TREC question coverage brokendown by answer type.
Due to temporal consistency,numeric questions are not covered very well.
Al-though the process and object types are broad an-swer types, the coverage is still reasonably good.As expected, the definition and person-bio answertypes are covered well by these resources.8 The Web as a ResourceAn increasing number of QA systems are using theweb as a resource.
Since the Web is orders of mag-nitude larger than local corpora, answers occur fre-quently in simple contexts, which is more conduciveto retrieval and extraction of correct, confident an-swers (Clarke et al, 2001; Dumais et al, 2002;Lin and Katz, 2003).
The web has been employedfor pattern acquisition (Ravichandran et al, 2003),document retrieval (Dumais et al, 2002), query ex-pansion (Yang et al, 2003), structured informationextraction, and answer validation (Magnini et al,2002) .
Some of these approaches enhance exist-ing QA systems, while others simplify the questionanswering task, allowing a less complex approachto find correct answers.8.1 Web DocumentsInstead of searching a local corpus, some QA sys-tems retrieve relevant documents from the web (Xuet al, 2003).
Since the density of relevant web doc-uments can be higher than the density of relevantlocal documents, answer extraction may be moresuccessful from the web.
For a TREC evaluation,answers found on the web must also be mapped torelevant documents in the local corpus.0 10 20 30 40 50 60 70 80 90 10001002003004005006007008009001000Web Retrieval Performance For QAdocument rank#questionsCorrect Doc DensityFirst Correct DocFigure 1: Web retrieval: relevant document densityand rank of first relevant document.In order to evaluate the impact of web docu-ments on TREC questions, we performed an ex-periment where simple queries were submitted toa web search engine.
The questions were to-kenized and filtered using a standard stop wordlist.
The resulting keyword queries were used toretrieve 100 documents through the Google API(www.google.com/api).
Documents containing thefull question, question number, references to TREC,NIST, AQUAINT, Question Answering and similarcontent were filtered out.Figure 1 shows the density of documents contain-ing a correct answer, as well as the rank of the firstdocument containing a correct answer.
The sim-ple word query retrieves a relevant document foralmost half of the questions.
Note that for mostsystems, the retrieval performance should be supe-rior since queries are usually more refined and addi-tional query expansion is performed.
However, thisexperiment provides an intuition and a very goodlower bound on the precision and density of currentweb documents for the TREC QA task.8.2 Web-Based Query ExpansionSeveral QA systems participating at TREC haveused search engines for query expansion (Yang etal., 2003).
The basic query expansion methodutilizes pseudo-relevance feedback (PRF) (Xu andCroft, 1996).
Content words are selected from ques-tions and submitted as queries to a search engine.The top n retrieved documents are selected, and kterms or phrases are extracted according to an op-timization criterion (e.g.
term frequency, n-gramfrequency, average mutual information using cor-pus statistics, etc).
These k items are used in theexpanded query.We experimented by using the top 5, 10, 15, 20,0 5 10 15 20 25 30 35 40 45 5010020030040050060070080090010001100Answer frequency using PRF# PRF terms#questionsTop 5 documentsTop 10 documentsTop 15 documentsTop 20 documentsTop 50 documentsTop 100 documentsFigure 2: Finding a correct answer in PRF expan-sion terms - applied to 2183 questions for witch an-swer keys exist.50, and 100 documents retrieved via the Google APIfor each question, and extracted the most frequentfifty n-grams (up to trigrams).
The goal was to de-termine the quality of query expansion as measuredby the density of correct answers already presentin the expansion terms.
Even without filtering n-grams matching the expected answer type, simplePRF produces the correct answer in the top n-gramsfor more than half the questions.
The best correctanswer density is achieved using PRF with only 20web documents.8.3 ConclusionsThis paper quantifies the utility of well-known andwidely-used resources such as WordNet, encyclope-dias, gazetteers and the Web on question answering.The experiments presented in this paper representloose bounds on the direct use of these resources inanswering TREC questions.
We reported the perfor-mance of these resources on different TREC collec-tions and on different question types.
We also quan-tified web retrieval performance, and confirmed thatthe web contains a consistently high density of rel-evant documents containing correct answers evenwhen simple queries are used.
The paper alsoshows that pseudo-relevance feedback alone usingweb documents for query expansions can producea correct answer for fifty percent of the questionsexamined.9 AcknowledgementsThis work was supported in part by the AdvancedResearch and Development Activity (ARDA)?sAdvanced Question Answering for Intelligence(AQUAINT) Program.ReferencesC.L.A.
Clarke, G.V.
Cormack, and T.R.
Lynam.2001.
Exploiting redundancy in question answer-ing.
SIGIR.S.
Dumais, M. Banko, E. Brill, J. Lin, and A. Ng.2002.
Web question answering: Is more alwaysbetter?
SIGIR.J.
Leidner, J. Bos, T. Dalmas, J. Curran, S. Clark,C.
Bannard, B. Webber, and M. Steedman.
2003.Qed: The edinburgh trec-2003 question answer-ing system.
TREC.J.
Lin and B. Katz.
2003.
Question answering fromthe web using knowledge annotation and knowl-edge mining techniques.
CIKM.J.
Lin.
2002.
The web as a resource for questionanswering: Perspectives and challenges.
LREC.B.
Magnini, M. Negri, R. Pervete, and H. Tanev.2002.
Is it the right answer?
exploiting web re-dundancy for answer validation.
ACL.G.A.
Miller, R. Beckwith, C. Fellbaum, D. Gross,and K. Miller.
1990.
Five papers on wordnet.
In-ternational Journal of Lexicography.D.
Moldovan, D. Clark, S. Harabagiu, and S. Maio-rano.
2003.
Cogex: A logic prover for questionanswering.
ACL.E.
Nyberg, T. Mitamura, J. Callan, J. Carbonell,R.
Frederking, K. Collins-Thompson, L. Hiyaku-moto, Y. Huang, C. Huttenhower, S. Judy, J. Ko,A.
Kupsc, L.V.
Lita, V. Pedro, D. Svoboda, andB.
Vand Durme.
2003.
A multi strategy approachwith dynamic planning.
TREC.D.
Paranjpe, G. Ramakrishnan, and S. Srinivasan.2003.
Passage scoring for question answering viabayesian inference on lexical relations.
TREC.D.
Ravichandran, A. Ittycheriah, and S. Roukos.2003.
Automatic derivation of surface text pat-terns for a maximum entropy based question an-swering system.
HLT-NAACL.E.M.
Voorhees.
2003.
Overview of the trec 2003question answering track.
TREC.J.
Xu and W.B.
Croft.
1996.
Query expansion usinglocal and global analysis.
SIGIR.J.
Xu, A. Licuanan, and R. Weischedel.
2003.
Trec2003 qa at bbn: Answering definitional ques-tions.
TREC.H.
Yang, T.S.
Chua, S. Wang, and C.K.
Koh.
2003.Structured use of external knowledge for event-based open domain question answering.
SIGIR.
