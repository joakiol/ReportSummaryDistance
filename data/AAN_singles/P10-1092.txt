Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 897?906,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsString Extension LearningJeffrey HeinzUniversity of DelawareNewark, Delaware, USAheinz@udel.eduAbstractThis paper provides a unified, learning-theoretic analysis of several learnableclasses of languages discussed previouslyin the literature.
The analysis shows thatfor these classes an incremental, globallyconsistent, locally conservative, set-drivenlearner always exists.
Additionally, theanalysis provides a recipe for constructingnew learnable classes.
Potential applica-tions include learnable models for aspectsof natural language and cognition.1 IntroductionThe problem of generalizing from examples topatterns is an important one in linguistics andcomputer science.
This paper shows that manydisparate language classes, many previously dis-cussed in the literature, have a simple, naturaland interesting (because non-enumerative) learnerwhich exactly identifies the class in the limit fromdistribution-free, positive evidence in the sense ofGold (Gold, 1967).1 These learners are calledString Extension Learners because each string inthe language can be mapped (extended) to an ele-ment of the grammar, which in every case, is con-ceived as a finite set of elements.
These learnershave desirable properties: they are incremental,globally consistent, and locally conservative.Classes previously discussed in the litera-ture which are string extension learnable in-clude the Locally Testable (LT) languages, theLocally Testable Languages in the Strict Sense1The allowance of negative evidence (Gold, 1967) or re-stricting the kinds of texts the learner is required to succeedon (i.e.
non-distribution-free evidence) (Gold, 1967; Horn-ing, 1969; Angluin, 1988) admits the learnability of the classof recursively enumerable languages.
Classes of languageslearnable in the harder, distribution-free, positive-evidence-only settings are due to structural properties of the languageclasses that permit generalization (Angluin, 1980b; Blumeret al, 1989).
That is the central interest here.
(Strictly Local, SL) (McNaughton and Papert,1971; Rogers and Pullum, to appear), the Piece-wise Testable (PT) languages (Simon, 1975), thePiecewise Testable languages in the Strict Sense(Strictly Piecewise, SP) (Rogers et al, 2009), theStrongly Testable languages (Beauquier and Pin,1991), the Definite languages (Brzozowski, 1962),and the Finite languages, among others.
To ourknowledge, this is the first analysis which identi-fies the common structural elements of these lan-guage classes which allows them to be identifiablein the limit from positive data: each language classinduces a natural partition over all logically possi-ble strings and each language in the class is theunion of finitely many blocks of this partition.One consequence of this analysis is a recipefor constructing new learnable classes.
One no-table case is the Strictly Piecewise (SP) languages,which was originally motivated for two reasons:the learnability properties discussed here and itsability to describe long-distance dependencies innatural language phonology (Heinz, 2007; Heinz,to appear).
Later this class was discovered to haveseveral independent characterizations and formthe basis of another subregular hierarchy (Rogerset al, 2009).It is expected string extension learning will haveapplications in linguistic and cognitive models.
Asmentioned, the SP languages already provide anovel hypothesis of how long-distance dependen-cies in sound patterns are learned.
Another exam-ple is the Strictly Local (SL) languages which arethe categorical, symbolic version of n-gram mod-els, which are widely used in natural language pro-cessing (Jurafsky and Martin, 2008).
Since the SPlanguages also admit a probabilistic variant whichdescribe an efficiently estimable class of distribu-tions (Heinz and Rogers, 2010), it is plausible toexpect the other classes will as well, though this isleft for future research.String extension learners are also simple, mak-897ing them accessible to linguists without a rigorousmathematical background.This paper is organized as follow.
?2 goesover basic notation and definitions.
?3 definesstring extension grammars, languages, and lan-guage classes and proves some of their fundamen-tal properties.
?4 defines string extension learn-ers and proves their behavior.
?5 shows how im-portant subregular classes are string extension lan-guage classes.
?6 gives examples of nonregularand infinite language classes which are string ex-tension learnable.
?7 summarizes the results, anddiscusses lines of inquiry for future research.2 PreliminariesThis section establishes notation and recalls basicdefinitions for formal languages, the paradigm ofidentification in the limit from positive data (Gold,1967).
Familiarity with the basic concepts of sets,functions, and sequences is assumed.For some set A, P(A) denotes the set of allsubsets of A and Pfin(A) denotes the set of allfinite subsets of A.
If f is a function such thatf : A ?
B then let f?
(a) = {f(a)}.
Thus,f?
: A ?
P(B) (note f?
is not surjective).
Aset ?
of nonempty subsets of S is a partition of Siff the elements of ?
(called blocks) are pairwisedisjoint and their union equals S.?
denotes a fixed finite set of symbols, the al-phabet.
Let ?n, ?
?n, ?
?, ?+ denote all stringsformed over this alphabet of length n, of lengthless than or equal to n, of any finite length, andof any finite length strictly greater than zero, re-spectively.
The term word is used interchangeablywith string.
The range of a string w is the setof symbols which are in w. The empty string isthe unique string of length zero denoted ?.
Thusrange(?)
= ?.
The length of a string u is de-noted by |u|, e.g.
|?| = 0.
A language L issome subset of ??.
The reverse of a languageLr = {wr : w ?
L}.Gold (1967) establishes a learning paradigmknown as identification in the limit from positivedata.
A text is an infinite sequence whose ele-ments are drawn from ??
?
{#} where # rep-resents a non-expression.
The ith element of t isdenoted t(i), and t[i] denotes the finite sequencet(0), t(1), .
.
.
t(i).
Following Jain et al (1999),let SEQ denote the set of all possible finite se-quences:SEQ = {t[i] : t is a text and i ?
N}The content of a text is defined below.content(t) ={w ?
??
: ?n ?
N such that t(n) = w}A text t is a positive text for a language L iffcontent(t) = L. Thus there is only one text t forthe empty language: for all i, t(i) = #.A learner is a function ?
which maps ini-tial finite sequences of texts to grammars,i.e.
?
: SEQ ?
G. The elements of G (the gram-mars) generate languages in some well-definedway.
A learner converges on a text t iff there existsi ?
N and a grammar G such that for all j > i,?
(t[j]) = G.For any grammar G, the language it generates isdenoted L(G).
A learner ?
identifies a languageL in the limit iff for any positive text t for L, ?converges on t to grammar G and L(G) = L. Fi-nally, a learner ?
identifies a class of languages Lin the limit iff for any L ?
L, ?
identifies L inthe limit.
Angluin (1980b) provides necessary andsufficient properties of language classes which areidentifiable in the limit from positive data.A learner ?
of language class L is globally con-sistent iff for each i and for all texts t for someL ?
L, content(t[i]) ?
L(?(t[i])).
A learner ?
islocally conservative iff for each i and for all textst for some L ?
L, whenever ?
(t[i]) 6= ?(t[i?
1]),it is the case that t(i) 6?
L(?([i?1])).
These termsare from Jain et al (2007).
Also, learners whichdo not depend on the order of the text are calledset-driven (Jain et al, 1999, p. 99).3 Grammars and LanguagesConsider some set A.
A string extension functionis a total function f : ??
?
Pfin(A).
It is notrequired that f be onto.
Denote the class of func-tions which have this general form SEF .Each string extension function is naturally as-sociated with some formal class of grammars andlanguages.
These functions, grammars, and lan-guages are called string extension functions, gram-mars, and languages, respectively.Definition 1 Let f ?
SEF .1.
A grammar is a finite subset of A.2.
The language of grammar G isLf (G) = {w ?
??
: f(w) ?
G}8983.
The class of languages obtained by all possi-ble grammars isLf = {Lf (G) : G ?
Pfin(A)}The subscript f is omitted when it is understoodfrom context.A function f ?
SEF naturally induces a par-tition ?f over ??.
Strings u and v are equivalent(u ?f v) iff f(u) = f(v).Theorem 1 Every language L ?
Lf is a finiteunion of blocks of ?f .Proof: Follows directly from the definition of ?fand the finiteness of string extension grammars.
2We return to this result in ?6.Theorem 2 Lf is closed under intersection.Proof: We show L1?L2 = L(G1?G2).
Considerany word w belonging to L1 and L2.
Then f(w)is a subset of G1 and of G2.
Thus f(w) ?
G1 ?G2, and therefore w ?
L(G1 ?
G2).
The otherinclusion follows similarly.
2String extension language classes are not in gen-eral closed under union or reversal (counterexam-ples to union closure are given in ?5.1 and to re-versal closure in ?6.
)It is useful to extend the domain of the functionf from strings to languages.f(L) =?w?Lf(w) (1)An element g of grammar G for language L =Lf (G) is useful iff g ?
f(L).
An element is use-less if it is not useful.
A grammar with no uselesselements is called canonical.Remark 1 Fix a function f ?
SEF .
For everyL ?
Lf , there is a canonical grammar, namelyf(L).
In other words, L = L(f(L)).Lemma 1 Let L,L?
?
Lf .
L ?
L?
iff f(L) ?f(L?
)Proof: (?)
Suppose L ?
L?
and consider anyg ?
f(L).
Since g is useful, there is a w ?
L suchthat g ?
f(w).
But f(w) ?
f(L?)
since w ?
L?.(?)
Suppose f(L) ?
f(L?)
and consider anyw ?
L. Then f(w) ?
f(L) so by transitivity,f(w) ?
f(L?).
Therefore w ?
L?.
2The significance of this result is that as the gram-mar G monotonically increases, the languageL(G) monotonically increases too.
The followingresult can now be proved, used in the next sectionon learning.2Theorem 3 For any finite L0 ?
?
?, L =L(f(L0)) is the smallest language in Lf contain-ing L0.Proof: Clearly L0 ?
L. Suppose L?
?
Lf andL0 ?
L?.
It follows directly from Lemma 1 thatL ?
L?
(since f(L) = f(L0) ?
f(L?)).
24 String Extension LearningLearning string extension classes is simple.
Theinitial hypothesis of the learner is the empty gram-mar.
The learner?s next hypothesis is obtained byapplying function f to the current observation andtaking the union of that set with the previous one.Definition 2 For all f ?
SEF and for all t ?SEQ, define ?f as follows:?f (t[i]) =????
if i = ?1?f (t[i?
1]) if t(i) = #?f (t[i?
1]) ?
f(t(i)) otherwiseBy convention, the initial state of the grammaris given by ?
(t[?1]) = ?.
The learner ?f exem-plifies string extension learning.
Each individualstring in the text reveals, by extension with f , as-pects of the canonical grammar for L ?
Lf .Theorem 4 ?f is globally consistent, locally con-servative, and set-driven.Proof: Global consistness and local conservative-ness follow immediately from Definition 2.
Forset-drivenness, witness (by Definition 2) it is thecase that for any text t and any i ?
N, ?
(t[i]) =f(content(t[i])).
2The key to the proof that ?f identifies Lf in thelimit from positive data is the finiteness of G forall L(G) ?
L. The idea is that there is a pointin the text in which every element of the grammarhas been seen because (1) there are only finitelymany useful elements of G, and (2) the learner isguaranteed to see a word in L which yields (via f )each element of G at some point (since the learnerreceives a positive text for L).
Thus at this point2The requirement in Theorem 3 that L0 be finite can bedropped if the qualifier ?in Lf ?
be dropped as well.
Thiscan be seen when one considers the identity function and theclass of finite languages.
(The identity function is a stringextension function, see ?6.)
In this case, id(??)
= ?
?, but??
is not a member of Lfin.
However since the interest hereis learners which generalize on the basis of finite experience,Theorem 3 is sufficient as is.899the learner ?
is guaranteed to have converged tothe target G as no additional words will add anymore elements to the learner?s grammar.Lemma 2 For all L ?
Lf , there is a finite sampleS such that L is the smallest language in Lf con-taining S. S is called a characteristic sample of Lin Lf (S is also called a tell-tale).Proof: For L ?
Lf , construct the sample S asfollows.
For each g ?
f(L), choose some wordw ?
L such that g ?
f(w).
Since f(L) is finite(Remark 1), S is finite.
Clearly f(S) = f(L) andthus L = L(f(S)).
Therefore, by Theorem 3, L isthe smallest language in Lf containing S. 2Theorem 5 Fix f ?
SEF .
Then ?f identifies Lfin the limit.Proof: For any L ?
Lf , there is a characteristic fi-nite sample S for L (Lemma 2).
Thus for any text tfor L, there is i such that S ?
content(t[i]).
Thusfor any j > i, ?
(t(j)) is the smallest languagein Lf containing S by Theorem 3 and Lemma 2.Thus, ?
(t(j)) = f(S) = f(L).
2An immediate corollary is the efficiency of ?fin the length of the sample, provided f is efficientin the length of the string (de la Higuera, 1997).Corollary 1 ?f is efficient in the length of thesample iff f is efficiently computable in the lengthof a string.To summarize: string extension grammars arefinite subsets of some set A.
The class of lan-guages they generate are determined by a func-tion f which maps strings to finite subsets of A(chunks of grammars).
Since the size of the canon-ical grammars is finite, a learner which develops agrammar on the basis of the observed words andthe function f identifies this class exactly in thelimit from positive data.
It also follows that if fis efficient in the length of the string then ?f is ef-ficient in the length of the sample and that ?f isglobally consistent, locally conservative, and set-driven.
It is striking that such a natural and gen-eral framework for generalization exists and that,as will be shown, a variety of language classes canbe expressed given the choice of f .5 Subregular examplesThis section shows how classes which make upthe subregular hierarchies (McNaughton and Pa-pert, 1971) are string extension language classes.Readers are referred to Rogers and Pullum (2007)and Rogers et al (2009) for an introduction to thesubregular hierarchies, as well as their relevanceto linguistics and cognition.5.1 K-factor languagesThe k-factors of a word are the contiguous subse-quences of length k in w. Consider the followingstring extension function.Definition 3 For some k ?
N, letfack(w) ={x ?
?k : ?u, v ?
?
?such that w = uxv} when k ?
|w| and{w} otherwiseFollowing the earlier definitions, for some k, agrammar G is a subset of ?
?k and a word w be-longs to the language of G iff fack(w) ?
G.Example 1 Let ?
= {a, b} and consider gram-mars G = {?, a, aa, ab, ba}.
Then L(G) ={?, a} ?
{w : |w| ?
2 and w 6?
??bb??}.
The 2-factor bb is a prohibited 2-factor for L(G).
Clearly,L(G) ?
Lfac2 .Languages in Lfack make distinctions based onwhich k-factors are permitted or prohibited.
Sincefack ?
SEF , it follows immediately from theresults in ?
?3-4 that the k-factor languages areclosed under intersection, and each has a char-acteristic sample.
For example, a characteristicsample for the 2-factor language in Example 1 is{?, a, ab, ba, aa}; i.e.
the canonical grammar it-self.
It follows from Theorem 5 that the class ofk-factor languages is identifiable in the limit by?fack .
The learner ?fac2 with a text from the lan-guage in Example 1 is illustrated in Table 1.The class Lfack is not closed underunion.
For example for k = 2, con-sider L1 = L({?, a, b, aa, bb, ba}) andL2 = L({?, a, b, aa, ab, bb}).
Then L1 ?
L2excludes string aba, but includes ab and ba, whichis not possible for any L ?
Lfack .K-factors are used to define other languageclasses, such as the Strictly Local and Lo-cally Testable languages (McNaughton and Pa-pert, 1971), discussed in ?5.4 and ?5.5.5.2 Strictly k-Piecewise languagesThe Strictly k-Piecewise (SPk) languages (Rogerset al, 2009) can be defined with a function whoseco-domain is P(??k).
However unlike the func-tion fack, the function SPk, does not require thatthe k-length subsequences be contiguous.900i t(i) fac2(t(i)) Grammar G L(G)-1 ?
?0 aaaa {aa} {aa} aaa?1 aab {aa, ab} {aa, ab} aaa?
?
aaa?b2 a {a} {a, aa, ab} aa?
?
aa?b.
.
.Table 1: The learner ?fac2 with a text from the language in Example 1.
Boldtype indicates newly addedelements to the grammar.A string u = a1 .
.
.
ak is a subsequence ofstring w iff ?
v0, v1, .
.
.
vk ?
??
such that w =v0a1v1 .
.
.
akvk.
The empty string ?
is a subse-quence of every string.
When u is a subsequenceof w we write u ?
w.Definition 4 For some k ?
N,SPk(w) = {u ?
?
?k : u ?
w}In other words, SPk(w) returns all subse-quences, contiguous or not, in w up to length k.Thus, for some k, a grammar G is a subset of ?
?k.Following Definition 1, a word w belongs to thelanguage of G only if SP2(w) ?
G.3Example 2 Let ?
= {a, b} and consider thegrammar G = {?, a, b, aa, ab, ba}.
Then L(G) =??\(??b??b??
).As seen from Example 2, SP languages encodelong-distance dependencies.
In Example 2, L pro-hibits a b from following another b in a word, nomatter how distant.
Table 2 illustrates ?SP2 learn-ing the language in Example 2.Heinz (2007,2009a) shows that consonantalharmony patterns in natural language are describ-able by such SP2 languages and hypothesizesthat humans learn them in the way suggested by?SP2 .
Strictly 2-Piecewise languages have alsobeen used in models of reading comprehension(Whitney, 2001; Grainger and Whitney, 2004;Whitney and Cornelissen, 2008) as well as textclassification(Lodhi et al, 2002; Cancedda et al,2003) (see also (Shawe-Taylor and Christianini,2005, chap.
11)).5.3 K-Piecewise Testable languagesA language L is k-Piecewise Testable iff when-ever strings u and v have the same subsequences3In earlier work, the function SP2 has been describedas returning the set of precedence relations in w, and thelanguage class LSP2 was called the precedence languages(Heinz, 2007; Heinz, to appear).of length at most k and u is in L, then v is in L aswell (Simon, 1975; Simon, 1993; Lothaire, 2005).A language L is said to be Piecewise-Testable(PT) if it is k-Piecewise Testable for some k ?
N.If k is fixed, the k-Piecewise Testable languagesare identifiable in the limit from positive data(Garc?
?a and Ruiz, 1996; Garc?
?a and Ruiz, 2004).More recently, the Piecewise Testable languageshas been shown to be linearly separable with asubsequence kernel (Kontorovich et al, 2008).The k-Piecewise Testable languages can alsobe described with the function SP ?k .
Recall thatf?
(a) = {f(a)}.
Thus functions SP ?k definegrammars as a finite list of sets of subsequencesup to length k that may occur in words in the lan-guage.
This reflects the fact that the k-PiecewiseTestable languages are the boolean closure of theStrictly k-Piecewise languages.45.4 Strictly k-Local languagesTo define the Strictly k-Local languages, it is nec-essary to make a pointwise extension to the defini-tions in ?3.Definition 5 For sets A1, .
.
.
, An, suppose foreach i, fi : ??
?
Pfin(Ai), and let f =(f1, .
.
.
, fn).1.
A grammar G is a tuple (G1, .
.
.
, Gn) whereG1 ?
Pfin(A1), .
.
.
, Gn ?
Pfin(An).2.
If for any w ?
?
?, each fi(w) ?
Gi for all1 ?
i ?
n, then f(w) is a pointwise subsetof G, written f(w) ??
G.3.
The language of grammar G isLf (G) = {w : f(w) ??
G}4.
The class of languages obtained by all suchpossible grammars G is Lf .4More generally, it is not hard to show that Lf?
is theboolean closure of Lf .901i t(i) SP2(t(i)) Grammar G Language of G-1 ?
?0 aaaa {?, a, aa} {?, a, aa} a?1 aab {?, a, b, aa, ab} {?, a, aa, b, ab} a?
?
a?b2 baa {?, a, b, aa, ba} {?, a, b, aa, ab, ba} ??\(??b??b??
)3 aba {?, a, b, ab, ba} {?, a, b, aa, ab, ba} ??\(??b??b??).
.
.Table 2: The learner ?SP2 with a text from the language in Example 2.
Boldtype indicates newly addedelements to the grammar.These definitions preserve the learning resultsof ?4.
Note that the characteristic sample of L ?Lf will be the union of the characteristic samplesof each fi and the language Lf (G) is the intersec-tion of Lfi(Gi).Locally k-Testable Languages in the StrictSense (Strictly k-Local) have been studied by sev-eral researchers (McNaughton and Papert, 1971;Garcia et al, 1990; Caron, 2000; Rogers and Pul-lum, to appear), among others.
We follow thedefinitions from (McNaughton and Papert, 1971,p.
14), effectively encoded in the following func-tions.Definition 6 Fix k ?
N. Then the (left-edge) pre-fix of length k, the (right-edge) suffix of length k,and the interior k-factors of a word w areLk(w) = {u ?
?k : ?v ?
??
such that w = uv}Rk(w) = {u ?
?k : ?v ?
??
such that w = vu}Ik(w) = fack(w)\(Lk(w) ?Rk(w))Example 3 Suppose w = abcba.
Then L2(w) ={ab}, R2(w) = {ba} and I2(w) = {bc, cb}.Example 4 Suppose |w| = k. Then Lk(w) =Rk(w) = {w} and Ik(w) = ?.Example 5 Suppose |w| is less than k. ThenLk(w) = Rk(w) = ?
and Ik(w) = {w}.A language L is k-Strictly Local (k-SL) iff forall w ?
L, there exist sets L,R, and I suchthat w ?
L iff Lk(w) ?
L, Rk(w) ?
R, andIk(w) ?
I .
McNaughton and Papert note that ifw is of length less than k than L may be perfectlyarbitrary about w.This can now be expressed as the string exten-sion function:LRIk(w) = (Lk(w), Rk(w), Ik(w))Thus for some k, a grammar G is triple formedby taking subsets of ?k, ?k, and ?
?k, respec-tively.
A word w belongs to the language of Gonly if LRIk(w) ??
G. Clearly, LLRIk = k-SL, and henceforth we refer to this class as k-SL.Since, for fixed k, LRIk ?
SEF , all of the learn-ing results in ?4 apply.5.5 Locally k-Testable languagesThe Locally k-testable languages (k-LT) are orig-inally defined in McNaughton and Papert (1971)and are the subject of several studies (Brzozowskiand Simon, 1973; McNaughton, 1974; Kim etal., 1991; Caron, 2000; Garc?
?a and Ruiz, 2004;Rogers and Pullum, to appear).A language L is k-testable iff for all w1, w2 ???
such that |w1| ?
k and |w2| ?
k, andLRIk(w1) = LRIk(w2) then either both w1, w2belong to L or neither do.
Clearly, every languagein k-SL belongs to k-LT.
However k-LT prop-erly include k-SL because a k-testable languageonly distinguishes words whenever LRIk(w1) 6=LRIk(w2).
It is known that the k-LT languagesare the boolean closure of the k-SL (McNaughtonand Papert, 1971).The function LRI?k exactly expresses k-testablelanguages.
Informally, each word w is mappedto a set containing a single element, this elementis the triple LRIk(w).
Thus a grammar G is asubset of the triples used to define k-SL.
Clearly,LLRI?k = k-LT since it is the boolean closure ofLLRIk .
Henceforth we refer to LLRI?k as the k-Locally Testable (k-LT) languages.5.6 Generalized subsequence languagesHere we introduce generalized subsequence func-tions, a general class of functions to which theSPk and fack functions belong.
Like thosefunctions, generalized subsequence functions mapwords to a set of subsequences found within thewords.
These functions are instantiated by a vec-tor whose number of coordinates determine howmany times a subsequence may be discontiguous902and whose coordinate values determine the lengthof each contiguous part of the subsequence.Definition 7 For some n ?
N, let ~v =?v0, v1, .
.
.
, vn?, where each vi ?
N. Let k bethe length of the subsequences; i.e.
k =?n0 vi.f~v(w) ={u ?
?k : ?x0, .
.
.
, xn, u0, .
.
.
, un+1 ?
?
?such that w = u0x0u1x1, .
.
.
, unxnun+1and |xi| = vi for all 0 ?
i ?
n}when k ?
|w|, and{w} otherwiseThe following examples help make the general-ized subsequence functions clear.Example 6 Let ~v = ?2?.
Then f?2?
= fac2.
Gen-erally, f?k?
= fack.Example 7 Let ~v = ?1, 1?.
Then f?1,1?
= SP2.Generally, if ~v = ?1, .
.
.
1?
with |~v| = k. Thenf~v = SPk.Example 8 Let ~v = ?3, 2, 1?
and a, b, c, d, e, f??.
Then Lf?3,2,1?
includes languages whichprohibit strings w which contain subsequencesabcdef where abc and de must be contiguous inw and abcdef is a subsequence of w.Generalized subsequence languages make dif-ferent kinds of distinctions to be made than PT andLT languages.
For example, the language in Ex-ample 8 is neither k-LT nor k?-PT for any valuesk, k?.
Generalized subsequence languages prop-erly include the k-SP and k-SL classes (Exam-ples 6 and 7), and the boolean closure of the sub-sequence languages (f?~v ) properly includes the LTand PT classes.Since for any ~v, f~v and f?~v are string extensionfunctions the learning results in ?4 apply.
Notethat f~v(w) is computable in time O(|w|k) where kis the length of the maximal subsequences deter-mined by ~v.6 Other examplesThis section provides examples of infinite andnonregular language classes that are string exten-sion learnable.
Recall from Theorem 1 that stringextension languages are finite unions of blocks ofthe partition of ??
induced by f .
Assuming theblocks of this partition can be enumerated, therange of f can be construed as Pfin(N).grammar G Language of G?
?
{0} anbn{1} ?
?\anbn{0, 1} ?
?Table 3: The language class Lf from Example 9In the examples considered so far, the enumera-tion of the blocks is essentially encoded in partic-ular substrings (or tuples of substrings).
However,much less clever enumerations are available.Example 9 Let A = {0,1} and consider the fol-lowing function:f(w) ={0 iff w ?
anbn1 otherwiseThe function f belongs to SEF because it is mapsstrings to a finite co-domain.
Lf has four lan-guages shown in Table 3.The language class in Example 9 is not regular be-cause it includes the well-known context-free lan-guage anbn.
This collection of languages is alsonot closed under reversal.There are also infinite language classes that arestring extension language classes.
Arguably thesimplest example is the class of finite languages,denoted Lfin.Example 10 Consider the function id whichmaps words in ??
to their singleton sets, i.e.id(w) = {w}.5 A grammar G is then a finitesubset of ?
?, and so L(G) is just a finite set ofwords in ??
; in fact, L(G) = G. It follows thatLid = Lfin.It can be easily seen that the function id inducesthe trivial partition over ?
?, and languages arejust finite unions of these blocks.
The learner ?idmakes no generalizations at all, and only remem-bers what it has observed.There are other more interesting infinite stringextension classes.
Here is one relating to theParikh map (Parikh, 1966).
For all a ?
?, letfa(w) be the set containing n where n is the num-ber of times the letter a occurs in the string w. For5Strictly speaking, this is not the identity function perse, but it is as close to the identity function as one can getsince string extension functions are defined as mappings fromstrings to sets.
However, once the domain of the function isextended (Equation 1), then it follows that id is the identityfunction when its argument is a set of strings.903example fa(babab) = {2}.
Thus fa is a total func-tion mapping strings to singleton sets of naturalnumbers, so it is a string extension function.
Thisfunction induces an infinite partition of ?
?, wherethe words in any particular block have the samenumber of letters a.
It is convenient to enumeratethe blocks according to how many occurrences ofthe letter a may occur in words within the block.Hence, B0 is the block whose words have no oc-currences of a, B1 is the block whose words haveone occurrence of a, and so on.In this case, a grammar G is a finite subset ofN,e.g.
{2, 3, 4}.
L(G) is simply those words whichhave either 2, 3, or 4, occurrences of the letter a.Thus Lfa is an infinite class, which contains lan-guages of infinite size, which is easily identified inthe limit from positive data by ?fa .This section gave examples of nonregular andnonfinite string extension classes by pursuing theimplications of Theorem 1, which established thatf ?
SEF partition ??
into blocks of which lan-guages are finite unions thereof.
The string exten-sion function f provides an effective way of en-coding all languages L in Lf because f(L) en-codes a finite set, the grammar.7 Conclusion and open questionsOne contribution of this paper is a unified way ofthinking about many formal language classes, allof which have been shown to be identifiable inthe limit from positive data by a string extensionlearner.
Another contribution is a recipe for defin-ing classes of languages identifiable in the limitfrom positive data by this kind of learner.As shown, these learners have many desirableproperties.
In particular, they are globally consis-tent, locally conservative, and set-driven.
Addi-tionally, the learner is guaranteed to be efficientin the size of the sample, provided the function fitself is efficient in the length of the string.Several additional questions of interest remainopen for theoretical linguistics, theoretical com-puter science, and computational linguistics.For theoretical linguistics, it appears that thestring extension function f = (LRI3, P2), whichdefines a class of languages which obey restric-tions on both contiguous subsequences of length3 and on discontiguous subsequences of length 2,provides a good first approximation to the seg-mental phonotactic patterns in natural languages(Heinz, 2007).
The string extension learner forthis class is essentially two learners: ?LRI3 and?P2 , operating simultaneously.6 The learnersmake predictions about generalizations, which canbe tested in artificial language learning experi-ments on adults and infants (Rogers and Pullum, toappear; Chambers et al, 2002; Onishi et al, 2003;Cristia?
and Seidl, 2008).7For theoretical computer science, it remains anopen question what property holds of functionsf in SEF to ensure that Lf is regular, context-free, or context-sensitive.
For known subregularclasses, there are constructions that provide deter-ministic automata that suggest the relevant prop-erties.
(See, for example, Garcia et al (1990) andGarica and Ruiz (1996).
)Also, Timo Ko?tzing and Samuel Moelius (p.c.
)suggest that the results here may be generalizedalong the following lines.
Instead of defining thefunction f as a map from strings to finite subsets,let f be a function from strings to elements of alattice.
A grammar G is an element of the latticeand the language of the G are all strings w suchthat f maps w to a grammar less than G. Learners?f are defined as the least upper bound of its cur-rent hypothesis and the grammar to which f mapsthe current word.8 Kasprzik and Ko?tzing (2010)develop this idea and demonstrate additional prop-erties of string extension classes and learning, andshow that the pattern languages (Angluin, 1980a)form a string extension class.9Also, hyperplane learning (Clark et al, 2006a;Clark et al, 2006b) and function-distinguishablelearning (Fernau, 2003) similarly associate lan-guage classes with functions.
How those analysesrelate to the current one remains open.Finally, since the stochastic counterpart of k-SL class is the n-gram model, it is plausible thatprobabilistic string extension language classes canform the basis of new natural language process-ing techniques.
(Heinz and Rogers, 2010) show6This learner resembles what learning theorists call par-allel learning (Case and Moelius, 2007) and what cognitivescientists call modular learning (Gallistel and King, 2009).7I conjecture that morphological and syntactic patternsare generally not amenable to a string extension learninganalysis because these patterns appear to require a paradigm,i.e.
a set of data points, before any conclusion can be confi-dently drawn about the generating grammar.
Stress patternsalso do not appear to be amenable to a string extension learn-ing (Heinz, 2007; Edlefsen et al, 2008; Heinz, 2009).8See also Lange et al (2008, Theorem 15) and Case et al(1999, pp.101-103).9The basic idea is to consider the lattice L = ?Lfin,?
?.Each element of L is a finite set of strings representing theintersection of all pattern languages consistent with this set.904how to efficiently estimate k-SP distributions, andit is conjectured that the other string extension lan-guage classes can be recast as classes of distri-butions, which can also be successfully estimatedfrom positive evidence.AcknowledgmentsThis work was supported by a University ofDelaware Research Fund grant during the 2008-2009 academic year.
I would like to thank JohnCase, Alexander Clark, Timo Ko?tzing, SamuelMoelius, James Rogers, and Edward Stabler forvaluable discussion.
I would also like to thankTimo Ko?tzing for careful reading of an earlierdraft and for catching some errors.
Remaining er-rors are my responsibility.ReferencesDana Angluin.
1980a.
Finding patterns common toa set of strings.
Journal of Computer and SystemSciences, 21:46?62.Dana Angluin.
1980b.
Inductive inference of formallanguages from positive data.
Information Control,45:117?135.Dana Angluin.
1988.
Identifying languages fromstochastic examples.
Technical Report 614, YaleUniversity, New Haven, CT.D.
Beauquier and J.E.
Pin.
1991.
Languages and scan-ners.
Theoretical Computer Science, 84:3?21.Anselm Blumer, Andrzej Ehrenfeucht, David Haus-sler, and Manfred K. Warmuth.
1989.
Learnabilityand the Vapnik-Chervonenkis dimension.
J. ACM,36(4):929?965.J.A.
Brzozowski and I. Simon.
1973.
Characterizationof locally testable events.
Discrete Math, 4:243?271.J.A.
Brzozowski.
1962.
Canonical regular expres-sions and minimal state graphs for definite events.
InMathematical Theory of Automata, pages 529?561.New York.Nicola Cancedda, Eric Gaussier, Cyril Goutte, andJean-Michel Renders.
2003.
Word-sequence ker-nels.
Journal of Machine Learning Research,3:1059?1082.Pascal Caron.
2000.
Families of locally testable lan-guages.
Theoretical Computer Science, 242:361?376.John Case and Sam Moelius.
2007.
Parallelismincreases iterative learning power.
In 18th An-nual Conference on Algorithmic Learning Theory(ALT07), volume 4754 of Lecture Notes in ArtificialIntelligence, pages 49?63.
Springer-Verlag, Berlin.John Case, Sanjay Jain, Steffen Lange, and ThomasZeugmann.
1999.
Incremental concept learning forbounded data mining.
Information and Computa-tion, 152:74?110.Kyle E. Chambers, Kristine H. Onishi, and CynthiaFisher.
2002.
Learning phonotactic constraints frombrief auditory experience.
Cognition, 83:B13?B23.Alexander Clark, Christophe Costa Flore?ncio, andChris Watkins.
2006a.
Languages as hyperplanes:grammatical inference with string kernels.
In Pro-ceedings of the European Conference on MachineLearning (ECML), pages 90?101.Alexander Clark, Christophe Costa Flore?ncio, ChrisWatkins, and Mariette Serayet.
2006b.
Planarlanguages and learnability.
In Proceedings of the8th International Colloquium on Grammatical Infer-ence (ICGI), pages 148?160.Alejandrina Cristia?
and Amanda Seidl.
2008.
Phono-logical features in infants phonotactic learning: Ev-idence from artificial grammar learning.
Language,Learning, and Development, 4(3):203?227.Colin de la Higuera.
1997.
Characteristic sets for poly-nomial grammatical inference.
Machine Learning,27:125?138.Matt Edlefsen, Dylan Leeman, Nathan Myers,Nathaniel Smith, Molly Visscher, and David Well-come.
2008.
Deciding strictly local (SL) lan-guages.
In Jon Breitenbucher, editor, Proceedingsof the Midstates Conference for Undergraduate Re-search in Computer Science and Mathematics, pages66?73.Henning Fernau.
2003.
Identification of function dis-tinguishable languages.
Theoretical Computer Sci-ence, 290:1679?1711.C.R.
Gallistel and Adam Philip King.
2009.
Memoryand the Computational Brain.
Wiley-Blackwell.Pedro Garc?
?a and Jose?
Ruiz.
1996.
Learning k-piecewise testable languages from positive data.
InLaurent Miclet and Colin de la Higuera, editors,Grammatical Interference: Learning Syntax fromSentences, volume 1147 of Lecture Notes in Com-puter Science, pages 203?210.
Springer.Pedro Garc?
?a and Jose?
Ruiz.
2004.
Learning k-testableand k-piecewise testable languages from positivedata.
Grammars, 7:125?140.Pedro Garcia, Enrique Vidal, and Jose?
Oncina.
1990.Learning locally testable languages in the strictsense.
In Proceedings of the Workshop on Algorith-mic Learning Theory, pages 325?338.E.M.
Gold.
1967.
Language identification in the limit.Information and Control, 10:447?474.J.
Grainger and C. Whitney.
2004.
Does the huamnmnid raed wrods as a wlohe?
Trends in CognitiveScience, 8:58?59.905Jeffrey Heinz and James Rogers.
2010.
Estimatingstrictly piecewise distributions.
In Proceedings ofthe ACL.Jeffrey Heinz.
2007.
The Inductive Learning ofPhonotactic Patterns.
Ph.D. thesis, University ofCalifornia, Los Angeles.Jeffrey Heinz.
2009.
On the role of locality in learningstress patterns.
Phonology, 26(2):303?351.Jeffrey Heinz.
to appear.
Learning long distancephonotactics.
Linguistic Inquiry.J.
J. Horning.
1969.
A Study of Grammatical Infer-ence.
Ph.D. thesis, Stanford University.Sanjay Jain, Daniel Osherson, James S. Royer, andArun Sharma.
1999.
Systems That Learn: An In-troduction to Learning Theory (Learning, Develop-ment and Conceptual Change).
The MIT Press, 2ndedition.Sanjay Jain, Steffen Lange, and Sandra Zilles.
2007.Some natural conditions on incremental learning.Information and Computation, 205(11):1671?1684.Daniel Jurafsky and James Martin.
2008.
Speechand Language Processing: An Introduction to Nat-ural Language Processing, Speech Recognition, andComputational Linguistics.
Prentice-Hall, UpperSaddle River, NJ, 2nd edition.Anna Kasprzik and Timo Ko?tzing.
to appear.
Stringextension learning using lattices.
In Proceedings ofthe 4th International Conference on Language andAutomata Theory and Applications (LATA 2010),Trier, Germany.S.M.
Kim, R. McNaughton, and R. McCloskey.
1991.A polynomial time algorithm for the local testabil-ity problem of deterministic finite automata.
IEEETrans.
Comput., 40(10):1087?1093.Leonid (Aryeh) Kontorovich, Corinna Cortes, andMehryar Mohri.
2008.
Kernel methods for learn-ing languages.
Theoretical Computer Science,405(3):223 ?
236.
Algorithmic Learning Theory.Steffen Lange, Thomas Zeugmann, and Sandra Zilles.2008.
Learning indexed families of recursive lan-guages from positive data: A survey.
TheoreticalComputer Science, 397:194?232.H.
Lodhi, N. Cristianini, J. Shawe-Taylor, andC.
Watkins.
2002.
Text classification using stringkernels.
Journal of Machine Language Research,2:419?444.M.
Lothaire, editor.
2005.
Applied Combinatorics onWords.
Cmbridge University Press, 2nd edition.Robert McNaughton and Seymour Papert.
1971.Counter-Free Automata.
MIT Press.R.
McNaughton.
1974.
Algebraic decision proceduresfor local testability.
Math.
Systems Theory, 8:60?76.Kristine H. Onishi, Kyle E. Chambers, and CynthiaFisher.
2003.
Infants learn phonotactic regularitiesfrom brief auditory experience.
Cognition, 87:B69?B77.R.
J. Parikh.
1966.
On context-free languages.
Journalof the ACM, 13, 570581., 13:570?581.James Rogers and Geoffrey Pullum.
to appear.
Auralpattern recognition experiments and the subregularhierarchy.
Journal of Logic, Language and Infor-mation.James Rogers, Jeffrey Heinz, Gil Bailey, Matt Edlef-sen, Molly Visscher, David Wellcome, and SeanWibel.
2009.
On languages piecewise testable inthe strict sense.
In Proceedings of the 11th Meetingof the Assocation for Mathematics of Language.John Shawe-Taylor and Nello Christianini.
2005.
Ker-nel Methods for Pattern Analysis.
Cambridge Uni-versity Press.Imre Simon.
1975.
Piecewise testable events.
In Au-tomata Theory and Formal Languages, pages 214?222.Imre Simon.
1993.
The product of rational lan-guages.
In ICALP ?93: Proceedings of the 20thInternational Colloquium on Automata, Languagesand Programming, pages 430?444, London, UK.Springer-Verlag.Carol Whitney and Piers Cornelissen.
2008.
SE-RIOL reading.
Language and Cognitive Processes,23:143?164.Carol Whitney.
2001.
How the brain encodes the or-der of letters in a printed word: the SERIOL modeland selective literature review.
Psychonomic Bul-letin Review, 8:221?243.906
