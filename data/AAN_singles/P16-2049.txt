Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 299?305,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsSyntactically Guided Neural Machine TranslationFelix Stahlberg?and Eva Hasler?and Aurelien Waite?and Bill Byrne??
?Department of Engineering, University of Cambridge, UK?SDL Research, Cambridge, UKAbstractWe investigate the use of hierarchicalphrase-based SMT lattices in end-to-endneural machine translation (NMT).
Weightpushing transforms the Hiero scores forcomplete translation hypotheses, with thefull translation grammar score and full n-gram language model score, into posteri-ors compatible with NMT predictive prob-abilities.
With a slightly modified NMTbeam-search decoder we find gains overboth Hiero and NMT decoding alone, withpractical advantages in extending NMT tovery large input and output vocabularies.1 IntroductionWe report on investigations motivated by the ideathat the structured search spaces defined by syn-tactic machine translation approaches such as Hi-ero (Chiang, 2007) can be used to guide NeuralMachine Translation (NMT) (Kalchbrenner andBlunsom, 2013; Sutskever et al, 2014; Cho et al,2014; Bahdanau et al, 2015).
NMT and Hierohave complementary strengths and weaknessesand differ markedly in how they define probabil-ity distributions over translations and what searchprocedures they use.The NMT encoder-decoder formalism providesa probability distribution over translations y = yT1of a source sentence x as (Bahdanau et al, 2015)P (yT1|x) =T?t=1P (yt|yt?11,x) =T?t=1g(yt?1, st, ct)(1)where st= f(st?1, yt?1, ct) is a decoder statevariable and ctis a context vector depending onthe source sentence and the attention mechanism.This posterior distribution is potentially verypowerful, however it does not easily lend itselfto sophisticated search procedures.
Decoding isdone by ?beam search to find a translation that ap-proximately maximizes the conditional probabil-ity?
(Bahdanau et al, 2015).
Search looks onlyone word ahead and no deeper than the beam.Hiero defines a synchronous context-free gram-mar (SCFG) with rules: X ?
?
?, ?
?, where ?and ?
are strings of terminals and non-terminals inthe source and target languages.
A target languagesentence y can be a translation of a source lan-guage sentence x if there is a derivation D in thegrammar which yields both y and x: y = y(D),x = x(D).
This defines a regular language Yover strings in the target language via a projectionof the sentence to be translated: Y = {y(D) :x(D) = x} (Iglesias et al, 2011; Allauzen et al,2014).
Scores are defined over derivations via alog-linear model with features {?i} and weights?.
The decoder searches for the translation y(D)in Y with the highest derivation score S(D) (Chi-ang, 2007, Eq.
24) :?y = y???argmaxD:x(D)=xPG(D)PLM(y(D))?LM?
??
?S(D)???
(2)where PLMis an n-gram language model andPG(D) ??(X???,??
)?D?i?i(X ?
?
?, ??
)?i.Hiero decoders attempt to avoid search er-rors when combining the translation and lan-guage model for the translation hypotheses (Chi-ang, 2007; Iglesias et al, 2009).
These proceduressearch over a vast space of translations, muchlarger than is considered by the NMT beam search.However the Hiero context-free grammars thatmake efficient search possible are weak models oftranslation.
The basic Hiero formalism can be ex-tended through ?soft syntactic constraints?
(Venu-gopal et al, 2009; Marton and Resnik, 2008) or by299adding very high dimensional features (Chiang etal., 2009), however the translation score assignedby the grammar is still only the product of prob-abilities of individual rules.
From the modellingperspective, this is an overly strong conditional in-dependence assumption.
NMT clearly has the po-tential advantage in incorporating long-term con-text into translation scores.NMT and Hiero differ in how they ?consume?source words.
Hiero applies the translation rules tothe source sentence via the CYK algorithm, witheach derivation yielding a complete and unam-biguous translation of the source words.
The NMTbeam decoder does not have an explicit mecha-nism for tracking source coverage, and there is ev-idence that may lead to both ?over-translation?
and?under-translation?
(Tu et al, 2016).NMT and Hiero also differ in their internal rep-resentations.
The NMT continuous representa-tion captures morphological, syntactic and seman-tic similarity (Collobert and Weston, 2008) acrosswords and phrases.
However, extending these rep-resentations to the large vocabularies needed foropen-domain MT is an open area of research (Jeanet al, 2015a; Luong et al, 2015; Sennrich et al,2015; Chitnis and DeNero, 2015).
By contrast,Hiero (and other symbolic systems) can easily usetranslation grammars and language models withvery large vocabularies (Heafield et al, 2013; Linand Dyer, 2010).
Moreover, words and phrasescan be easily added to a fully-trained symbolicMT system.
This is an important considerationfor commercial MT, as customers often wish tocustomise and personalise SMT systems for theirown application domain.
Adding new words andphrases to an NMT system is not as straightfor-ward, and it is not clear that the advantages of thecontinuous representation can be extended to thenew additions to the vocabularies.NMT has the advantage of including long-rangecontext in modelling individual translation hy-potheses.
Hiero considers a much bigger searchspace, and can incorporate n-gram language mod-els, but a much weaker translation model.
In thispaper we try to exploit the strengths of each ap-proach.
We propose to guide NMT decoding usingHiero.
We show that restricting the search space ofthe NMT decoder to a subset of Y spanned by Hi-ero effectively counteracts NMT modelling errors.This can be implemented by generating translationlattices with Hiero, which are then rescored by theNMT decoder.
Our approach addresses the lim-ited vocabulary issue in NMT as we replace NMTOOVs with lattice words from the much larger Hi-ero vocabulary.
We also find good gains from neu-ral and Kneser-Ney n-gram language models.2 Syntactically Guided NMT (SGNMT)2.1 Hiero Predictive PosteriorsThe Hiero decoder generates translation hypothe-ses as weighted finite state acceptors (WFSAs), orlattices, with weights in the tropical semiring.
Fora translation hypothesis y(D) arising from the Hi-ero derivation D, the path weight in the WFSAis ?
logS(D), after Eq.
2.
While this representa-tion is correct with respect to the Hiero translationgrammar and language model scores, having Hi-ero scores at the path level is not convenient forworking with the NMT system.
What we need arepredictive probabilities in the form of Eq.
1.The Hiero WFSAs are determinised and min-imised with epsilon removal under the tropicalsemiring, and weights are pushed towards the ini-tial state under the log semiring (Mohri and Riley,2001).
The resulting transducer is stochastic in thelog semiring, i.e.
the log sum of the arc log prob-abilities leaving a state is 0 (= log 1).
In addi-tion, because the WFSA is deterministic, there isa unique path leading to every state, which corre-sponds to a unique Hiero translation prefix.
Sup-pose a path to a state accepts the translation prefixyt?11.
An outgoing arc from that state with symboly has a weight that corresponds to the (negativelog of the) conditional probabilityPHiero(yt= y|yt?11,x).
(3)This conditional probability is such that for a Hi-ero translation yT1= y(D) accepted by the WFSAPHiero(yT1) =T?t=1PHiero(yt|yt?11,x) ?
S(D).
(4)The Hiero WFSAs have been transformed so thattheir arc weights have the negative log of the con-ditional probabilities defined in Eq.
3.
All theprobability mass of this distribution is concen-trated on the Hiero translation hypotheses.
Thecomplete translation and language model scorescomputed over the entire Hiero translations arepushed as far forward in the WFSAs as possible.This is commonly done for left-to-right decodingin speech recognition (Mohri et al, 2002).3002.2 NMT?Hiero DecodingAs above, suppose a path to a state in the WFSAaccepts a Hiero translation prefix yt?11, and let ytbe a symbol on an outgoing arc from that state.
Wedefine the joint NMT+Hiero score aslogP (yt|yt?11,x) =?HierologPHiero(yt|yt?11,x) +?NMT{logPNMT(yt|yt?11,x) yt?
?NMTlogPNMT(unk|yt?11,x) yt6?
?NMT(5)Note that the NMT-HIERO decoder only con-siders hypotheses in the Hiero lattice.
As dis-cussed earlier, the Hiero vocabulary can be muchlarger than the NMT output vocabulary ?NMT.
Ifa Hiero translation contains a word not in the NMTvocabulary, the NMT model provides a score andupdates its decoder state as for an unknown word.Our decoding algorithm is a natural extension ofbeam search decoding for NMT.
Due to the formof Eq.
5 we can build up hypotheses from left-to-right on the target side.
Thus, we can representa partial hypothesis h = (yt1, hs) by a transla-tion prefix yt1and an accumulated score hs.
Ateach iteration we extend the current hypotheses byone target token, until the best scoring hypothesisreaches a final state of the Hiero lattice.
We re-fer to this step as node expansion, and in Sec.
3.1we report the number of node expansions per sen-tence, as an indication of computational cost.We can think of the decoding algorithm asbreath-first search through the translation latticeswith a limited number of active hypotheses (abeam).
Rescoring is done on-the-fly: as the de-coder traverses an edge in the WFSA, we updateits weight by Eq.
5.
The output-synchronous char-Train set Dev set Test seten de en de en de# sentences 4.2M 6k 2.7k# word tokens 106M 102M 138k 138k 62k 59k# unique words 647k 1.5M 13k 20k 9k 13kOOV (Hiero) 0.0% 0.0% 0.8% 1.6% 1.0% 2.0%OOV (NMT) 1.6% 5.5% 2.5% 7.5% 3.1% 8.8%en fr en fr en fr# sentences 12.1M 6k 3k# word tokens 305M 348M 138k 155k 71k 81k# unique words 1.6M 1.7M 14k 17k 10k 11kOOV (Hiero) 0.0% 0.0% 0.6% 0.6% 0.4% 0.4%OOV (NMT) 3.5% 3.8% 4.5% 5.3% 5.0% 5.3%Table 1: Parallel texts and vocabulary coverage onnews-test2014.acteristic of beam search enables us to computethe NMT posteriors only once for each historybased on previous calculations.Alternatively, we can think of the algorithm asNMT decoding with revised posterior probabil-ities: instead of selecting the most likely sym-bol ytaccording the NMT model, we adjust theNMT posterior with the Hiero posterior scores anddelete NMT entries that are not allowed by the lat-tice.
This may result in NMT choosing a differentsymbol, which is then fed back to the neural net-work for the next decoding step.3 Experimental EvaluationWe evaluate SGNMT on the WMT news-test2014test sets (the filtered version) for English-German(En-De) and English-French (En-Fr).
We also re-port results on WMT news-test2015 En-De.The En-De training set includes Europarl v7,Common Crawl, and News Commentary v10.
Sen-tence pairs with sentences longer than 80 wordsor length ratios exceeding 2.4:1 were deleted, aswere Common Crawl sentences from other lan-guages (Shuyo, 2010).
The En-Fr NMT systemwas trained on preprocessed data (Schwenk, 2014)used by previous work (Sutskever et al, 2014;Bahdanau et al, 2015; Jean et al, 2015a), butwith truecasing like our Hiero baseline.
Follow-ing (Jean et al, 2015a), we use news-test2012 andnews-test2013 as a development set.
The NMT vo-cabulary size is 50k for En-De and 30k for En-Fr,taken as the most frequent words in training (Jeanet al, 2015a).
Tab.
1 provides statistics and showsthe severity of the OOV problem for NMT.The BASIC NMT system is built using theBlocks framework (van Merri?enboer et al, 2015)based on the Theano library (Bastien et al, 2012)with standard hyper-parameters (Bahdanau et al,2015): the encoder and decoder networks consistof 1000 gated recurrent units (Cho et al, 2014).The decoder uses a single maxout (Goodfellow etal., 2013) output layer with the feed-forward at-tention model (Bahdanau et al, 2015).The En-De Hiero system uses rules which en-courage verb movement (de Gispert et al, 2010).The rules for En-Fr were extracted from the fulldata set available at the WMT?15 website using ashallow-1 grammar (de Gispert et al, 2010).
5-gram Kneser-Ney language models (KN-LM) forthe Hiero systems were trained on WMT?15 par-allel and monolingual data (Heafield et al, 2013).301(Jean et al, 2015a, Tab.
2) SGNMTSetup BLEU Setup BLEUBASIC NMT 16.46 BASIC NMT 16.31NMT-LV 16.95 HIERO 19.44+ UNK Replace 18.89 NMT-HIERO 20.69?
?
+ Tuning 21.43+ Reshuffle 19.40 + Reshuffle 21.87+ Ensemble 21.59(a) English-German(Jean et al, 2015a, Tab.
2) SGNMTSetup BLEU Setup BLEUBASIC NMT 29.97 BASIC NMT 30.42NMT-LV 33.36 HIERO 32.86+ UNK Replace 34.11 NMT-HIERO 35.37?
?
+ Tuning 36.29+ Reshuffle 34.60 + Reshuffle 36.61+ Ensemble 37.19(b) English-FrenchTable 2: BLEU scores on news-test2014 calculated with multi-bleu.perl.
NMT-LV refers to theRNNSEARCH-LV model from (Jean et al, 2015a) for large output vocabularies.Search Vocab.
NMT Grammar KN-LM NPLM # of node exp- BLEU BLEUspace scores scores scores scores ansions per sen. (single) (ensemble)1 Lattice Hiero X X ?
21.1 (Hiero)2 Lattice Hiero X X X ?
21.7 (Hiero)3 Unrestricted NMT X 254.8 19.5 21.84 100-best Hiero X2,233.6(DFS: 832.1)22.8 23.35 100-best Hiero X X X 22.9 23.46 100-best Hiero X X X X 22.9 23.37 1000-best Hiero X21,686.2(DFS: 6,221.8)23.3 23.88 1000-best Hiero X X X 23.4 23.99 1000-best Hiero X X X X 23.5 24.010 Lattice NMT X 243.3 20.3 21.411 Lattice Hiero X 243.3 23.0 24.212 Lattice Hiero X X 243.3 23.0 24.213 Lattice Hiero X X 240.5 23.4 24.514 Lattice Hiero X X X 243.9 23.4 24.415 Lattice Hiero X X X X 244.3 24.0 24.416 Neural MT ?
UMontreal-MILA (Jean et al, 2015b) 22.8 25.2Table 3: BLEU English-German news-test2015 scores calculated with mteval-v13a.pl.Our SGNMT system1is built with the Pyfst inter-face2to OpenFst (Allauzen et al, 2007).3.1 SGNMT PerformanceTab.
2 compares our combined NMT+Hiero de-coding with NMT results in the literature.
We usea beam size of 12.
In En-De and in En-Fr, we findthat our BASIC NMT system performs similarly(within 0.5 BLEU) to previously published results(16.31 vs. 16.46 and 30.42 vs. 29.97).In NMT-HIERO, decoding is as described inSec.
2.2, but with ?Hiero= 0.
The decodersearches through the Hiero lattice, ignoring theHiero scores, but using Hiero word hypotheses inplace of any UNKs that might have been producedby NMT.
The results show that NMT-HIERO ismuch more effective in fixing NMT OOVs thanthe ?UNK Replace?
technique (Luong et al, 2015);this holds in both En-De and En-Fr.For the NMT-HIERO+TUNING systems, latticeMERT (Macherey et al, 2008) is used to optimise?Hieroand ?NMTon the tuning sets.
This yieldsfurther gains in both En-Fr and En-De, suggesting1http://ucam-smt.github.io/sgnmt/html/2https://pyfst.github.io/that in addition to fixing UNKs, the Hiero predic-tive posteriors can be used to improve the NMTtranslation model scores.Tab.
3 reports results of our En-De system withreshuffling and tuning on news-test2015.
BLEUscores are directly comparable to WMT?15 re-sults3.
By comparing row 3 to row 10, we see thatconstraining NMT to the search space defined bythe Hiero lattices yields an improvement of +0.8BLEU for single NMT.
If we allow Hiero to fixNMT UNKs, we see a further +2.7 BLEU gain(row 11).
The majority of gains come from fix-ing UNKs, but there is still improvement from theconstrained search space for single NMT.We next investigate the contribution of the Hi-ero system scores.
We see that, once latticesare generated, the KN-LM contributes more torescoring than the Hiero grammar scores (rows 12-14).
Further gains can be achieved by adding afeed-forward neural language model with NPLM(Vaswani et al, 2013) (row 15).
We observe thatn-best list rescoring with NMT (Neubig et al,2015) also outperforms both the Hiero and NMT3http://matrix.statmt.org/matrix/systems list/1774302Figure 1: Performance with NPLM over beam sizeon English-German news-test2015.
A beam of 12corresponds to row 15 in Tab.
3.Determini- Minimi- Weight Sentencessation sation pushing per secondX 2.51X X 1.57X X X 1.47Table 4: Time for lattice preprocessing operationson English-German news-test2015.baselines, although lattice rescoring gives the bestresults (row 9 vs. row 15).
Lattice rescoring withSGNMT also uses far fewer node expansions persentence.
We report n-best rescoring speeds forrescoring each hypothesis separately, and a depth-first (DFS) scheme that efficiently traverses the n-best lists.
Both these techniques are very slowcompared to lattice rescoring.
Fig.
1 shows thatwe can reduce the beam size from 12 to 5 withonly a minor drop in BLEU.
This is nearly 100times faster than DFS over the 1000-best list.Cost of Lattice Preprocessing As described inSec.
2.1, we applied determinisation, minimisa-tion, and weight pushing to the Hiero lattices inorder to work with probabilities.
Tab.
4 shows thatthose operations are generally fast4.Lattice Size For previous experiments we setthe Hiero pruning parameters such that lattices had8,510 nodes on average.
Fig.
2 plots the BLEUscore over the lattice size.
We find that SGNMTworks well on lattices of moderate or large size,but pruning lattices too heavily has a negative ef-fect as they are then too similar to Hiero first besthypotheses.
We note that lattice rescoring involvesnearly as many node expansions as unconstrainedNMT decoding.
This confirms that the lattices at8,510 nodes are already large enough for SGNMT.4Testing environment: Ubuntu 14.04, Linux 3.13.0, singleIntelR?XeonR?X5650 CPU at 2.67 GHzFigure 2: SGNMT performance over lattice sizeon English-German news-test2015.
8,510 nodesper lattice corresponds to row 14 in Tab.
3.Local Softmax In SGNMT decoding we havethe option of normalising the NMT translationprobabilities over the words on outgoing wordsfrom each state rather than over the full 50,000words translation vocabulary.
There are ?4.5 arcsper state in our En-De?14 lattices, and so avoidingthe full softmax could cause significant computa-tional savings.
We find this leads to only a modest0.5 BLEU degradation: 21.45 BLEU in En-De?14,compared to 21.87 BLEU using NMT probabili-ties computed over the full vocabulary.Modelling Errors vs. Search Errors In our En-De?14 experiments with ?Hiero= 0 we findthat constraining the NMT decoder to the Hierolattices yields translation hypotheses with muchlower NMT probabilities than unconstrained BA-SIC NMT decoding: under the NMT model, NMThypotheses are 8,300 times more likely (median)than NMT-HIERO hypotheses.
We conclude (ten-tatively) that BASIC NMT is not suffering onlyfrom search errors, but rather that NMT-HIEROdiscards some hypotheses ranked highly by theNMT model but lower in the evaluation metric.4 ConclusionWe have demonstrated a viable approach to Syn-tactically Guided Neural Machine Translation for-mulated to exploit the rich, structured search spacegenerated by Hiero and the long-context transla-tion scores of NMT.
SGNMT does not suffer fromthe severe limitation in vocabulary size of basicNMT and avoids any difficulty of extending dis-tributed word representations to new vocabularyitems not seen in training data.AcknowledgementsThis work was supported in part by the U.K. En-gineering and Physical Sciences Research Council(EPSRC grant EP/L027623/1).303ReferencesCyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-jciech Skut, and Mehryar Mohri.
2007.
OpenFst: Ageneral and efficient weighted finite-state transducerlibrary.
In Implementation and Application of Au-tomata, pages 11?23.
Springer.Cyril Allauzen, Bill Byrne, de Adri`a Gispert, GonzaloIglesias, and Michael Riley.
2014.
Pushdown au-tomata in statistical machine translation.
Volume 40,Issue 3 - September 2014, pages 687?723.Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio.
2015.
Neural machine translation by jointlylearning to align and translate.
In ICLR.Fr?ed?eric Bastien, Pascal Lamblin, Razvan Pascanu,James Bergstra, Ian Goodfellow, Arnaud Bergeron,Nicolas Bouchard, David Warde-Farley, and YoshuaBengio.
2012.
Theano: new features and speed im-provements.
In Deep Learning and UnsupervisedFeature Learning NIPS 2012 Workshop.David Chiang, Kevin Knight, and Wei Wang.
2009.11,001 new features for statistical machine transla-tion.
In ACL, pages 218?226.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2):201?228.Rohan Chitnis and John DeNero.
2015.
Variable-length word encodings for neural translation models.In EMNLP, pages 2088?2093.Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-cehre, Fethi Bougares, Holger Schwenk, and YoshuaBengio.
2014.
Learning phrase representationsusing RNN encoder-decoder for statistical machinetranslation.
In EMNLP.Ronan Collobert and Jason Weston.
2008.
A unifiedarchitecture for natural language processing: Deepneural networks with multitask learning.
In Pro-ceedings of the 25th International Conference onMachine Learning, pages 160?167.
ACM.Adri`a de Gispert, Gonzalo Iglesias, Graeme Black-wood, Eduardo R Banga, and William Byrne.
2010.Hierarchical phrase-based translation with weightedfinite-state transducers and shallow-n grammars.Computational Linguistics, 36(3):505?533.Ian Goodfellow, David Warde-farley, Mehdi Mirza,Aaron Courville, and Yoshua Bengio.
2013.
Max-out networks.
In ICML, pages 1319?1327.Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.Clark, and Philipp Koehn.
2013.
Scalable modifiedKneser-Ney language model estimation.
In ACL,pages 690?696.Gonzalo Iglesias, Adri`a de Gispert, Eduardo R Banga,and William Byrne.
2009.
Hierarchical phrase-based translation with weighted finite state transduc-ers.
In NAACL-HLT, pages 433?441.Gonzalo Iglesias, Cyril Allauzen, William Byrne,Adri`a de Gispert, and Michael Riley.
2011.
Hier-archical phrase-based translation representations.
InEMNLP, pages 1373?1383.S?ebastien Jean, Kyunghyun Cho, Roland Memisevic,and Yoshua Bengio.
2015a.
On using very largetarget vocabulary for neural machine translation.
InACL, pages 1?10.S?ebastien Jean, Orhan Firat, Kyunghyun Cho, RolandMemisevic, and Yoshua Bengio.
2015b.
Montrealneural machine translation systems for WMT15.
InProceedings of the Tenth Workshop on StatisticalMachine Translation, pages 134?140.Nal Kalchbrenner and Phil Blunsom.
2013.
Recurrentcontinuous translation models.
In EMNLP, page413.Jimmy Lin and Chris Dyer.
2010.
Data-intensive textprocessing with MapReduce.
Morgan &Claypool.Minh-Thang Luong, Ilya Sutskever, Quoc V Le, OriolVinyals, and Wojciech Zaremba.
2015.
Addressingthe rare word problem in neural machine translation.In ACL.Wolfgang Macherey, Franz Josef Och, Ignacio Thayer,and Jakob Uszkoreit.
2008.
Lattice-based minimumerror rate training for statistical machine translation.In EMNLP, pages 725?734.Yuval Marton and Philip Resnik.
2008.
Soft syntac-tic constraints for hierarchical phrased-based trans-lation.
In ACL, pages 1003?1011.Mehryar Mohri and Michael Riley.
2001.
A weightpushing algorithm for large vocabulary speechrecognition.
In Interspeech, pages 1603?1606.Mehryar Mohri, Fernando Pereira, and Michael Ri-ley.
2002.
Weighted finite-state transducers inspeech recognition.
Computer Speech and Lan-guage, 16(1).Graham Neubig, Makoto Morishita, and Satoshi Naka-mura.
2015.
Neural reranking improves subjectivequality of machine translation: NAIST at WAT2015.In Workshop on Asian Translation, pages 35?41.Holger Schwenk.
2014.
Universit du Maine.http://www-lium.univ-lemans.fr/?schwenk/nnmt-shared-task/.
[Online;accessed 1-March-2016].Rico Sennrich, Barry Haddow, and Alexandra Birch.2015.
Neural machine translation of rare words withsubword units.
arXiv preprint arXiv:1508.07909.Nakatani Shuyo.
2010.
Language detection li-brary for Java.
http://code.google.com/p/language-detection/.
[Online; accessed1-March-2016].304Ilya Sutskever, Oriol Vinyals, and Quoc V Le.
2014.Sequence to sequence learning with neural net-works.
In Advances in Neural Information Process-ing Systems, pages 3104?3112.Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu,and Hang Li.
2016.
Coverage-based neural machinetranslation.
arXiv preprint arXiv:1601.04811.Bart van Merri?enboer, Dzmitry Bahdanau, Vincent Du-moulin, Dmitriy Serdyuk, David Warde-Farley, JanChorowski, and Yoshua Bengio.
2015.
Blocks andfuel: Frameworks for deep learning.
arXiv preprintarXiv:1506.00619.Ashish Vaswani, Yinggong Zhao, Victoria Fossum, andDavid Chiang.
2013.
Decoding with large-scaleneural language models improves translation.
InEMNLP, pages 1387?1392.Ashish Venugopal, Andreas Zollmann, Noah A. Smith,and Stephan Vogel.
2009.
Preference grammars:Softening syntactic constraints to improve statisticalmachine translation.
In NAACL-HLT, pages 236?244.305
