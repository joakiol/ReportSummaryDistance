Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 427?434, Vancouver, October 2005. c?2005 Association for Computational LinguisticsChinese Named Entity Recognition Based on Multiple FeaturesYouzheng Wu, Jun Zhao, Bo Xu Hao YuNational Laboratory of Pattern Recognition Fujitsu R&D Center Co., LtdInstitute of Automation, CAS Beijing 100016, ChinaBeijing, 100080, China yu@frdc.fujitsu.com(yzwu,jzhao,bxu)@nlpr.ia.ac.cnAbstractThis paper proposes a hybrid Chinesenamed entity recognition model based onmultiple features.
It differentiates frommost of the previous approaches mainlyas follows.
Firstly, the proposed HybridModel integrates coarse particle feature(POS Model) with fine particle feature(Word Model), so that it can overcomethe disadvantages of each other.
Secondly,in order to reduce the searching space andimprove the efficiency, we introduce heu-ristic human knowledge into statisticalmodel, which could increase the perform-ance of NER significantly.
Thirdly, weuse three sub-models to respectively de-scribe three kinds of transliterated personname, that is, Japanese, Russian andEuramerican person name, which can im-prove the performance of PN recognition.From the experimental results on People'sDaily testing data, we can conclude thatour Hybrid Model is better than the mod-els which only use one kind of features.And the experiments on MET-2 testingdata also confirm the above conclusion,which show that our algorithm has consis-tence on different testing data.1 IntroductionNamed Entity Recognition (NER) is one of the keytechniques in the fields of Information Extraction,Question Answering, Parsing, Metadata Tagging inSemantic Web, etc.
In MET-2 held in conjunctionwith the Seventh Message Understanding Confer-ence (MUC-7), the task of NER is defined as rec-ognizing seven sub-categories entities: person (PN),location (LN), organization (ON), time, date, cur-rency and percentage.
As for Chinese NEs, we fur-ther divide PN into five sub-classes, that is,Chinese PN (CPN), Japanese PN (JPN), RussianPN (RPN), Euramerican PN (EPN) and abbrevi-ated PN (APN) like "???/Mr.
Wu".
Similarly,LN is split into common LN (LN) like "??
?/Zhongguancun" and abbreviated LN (ALN) suchas "?/Beijing", "?/Shanghai".
The recognition oftime (TM) and numbers (NM) is comparativelysimpler and can be implemented via finite stateautomata.
Therefore, our research focuses on therecognition of CPN, JPN, RPN, EPN, APN, LN,ALN and ON.Compared to English NER, Chinese NER ismore difficult.
We think that the main differencesbetween Chinese NER and English NER lie in: (1)Unlike English, Chinese lacks the capitalizationinformation which can play very important roles inidentifying named entities.
(2) There is no spacebetween words in Chinese, so we have to segmentthe text before NER.
Consequently, the errors inword segmentation will affect the result of NER.In this paper, we proposes a hybrid ChineseNER model based on multiple features which em-phasizes on (1) combining fine particle features(Word Model) with coarse particle features (POSModel); (2) integrating human knowledge into sta-tistical model; (3) and using diverse sub-modelsfor different kinds of entities.
Especially, we dividetransliterated person name into three sub-classesaccording to their characters set, that is, JPN, RPNand EPN.
In order to deduce the complexity of themodel and the searching space, we divide the rec-427ognition process into two steps: (1) word segmen-tation and POS tagging; (2) named entity recogni-tion based on the first step.Trained on the NEs labeled corpus of five-month People's Daily corpus and tested on one-month People's Daily corpus, the Hybrid Modelachieves the following performance.
The precisionand the recall of PN (including CPN, JPN, RPN,EPN, AP N), LN (including ALN) and ON are re-spectively (94.06%, 95.21%), (93.98%, 93.48%),and (84.69%, 86.86%).
From the experimental re-sults on People's Daily testing data, we can con-clude that our Hybrid Model is better than othermodels which only use one kind of features.
Andthe experiments on MET-2 testing data also con-firm the above conclusion, which show that ouralgorithm has consistence on different testing data.2 Related WorkOn the impelling of international evaluations likeMUC, CoNLL, IEER and ACE, the researches onEnglish NER have achieved impressive results.
Forexample, the best English NER system[Chinchor.1998] in MUC7 achieved 95% precision and 92%recall.
However, Chinese NER is far from mature.For example, the performance (precision, recall) ofthe best Chinese NER system in MET-2 is (66%,92%), (89%, 91%), (89%, 88%) for PN, LN andON respectively.Recently, approaches for NER are a shift awayfrom handcrafted rules[Grishman, et al 1995][Krupka, et al 1998][Black et al 1998] towardsmachine learning algorithms, i.e.
unsupervisedmodel like DL-CoTrain, CoBoost[Collins, 1999,2002], supervised learning like Error-driven [Ab-erdeen, et al 1995], Decision Tree [Sekine, et al1998], HMM[Bikel, et al 1997] and MaximumEntropy[Borthwick, et al 1999][Mikheev, etal.1998].Similarly, the models for Chinese NER can alsobe divided into two categories: Individual Modeland Integrated Model.Individual Model[Chen, et al 1998][Sun, et al1994][Zheng, et al 2000] consists of several sub-models, each of them deals with a kind of entities.For example, the recognition of PN may be statis-tical-based model, while LN and ON may be rule-based model like [Chen, et al 1998].
IntegratedModel[Sun, et al 2002] [Zhang, et al 2003][Yu, etal.
1998][Chua, et al 2002] deals with all kinds ofentities in a unified statistical framework.
Most ofthese integrated models can be viewed as a HMMmodel.
The differences among them are the defini-tion of state and the features used in entity modeland context model.In fact, a NER model recognizes named entitiesthrough mining the intrinsic features in the entitiesand the contextual features around the entities.Most of existing approaches employ either coarseparticle features, like POS and ROLE[Zhang, et al2003], or fine particle features like word.
The datasparseness problem is serious if only using fineparticle features, and coarse particle features willlose much important information though withoutserious data sparseness problem.
Our idea is thatcoarse particle features should be integrated intofine particle features to overcome the disadvan-tages of them.
However, most systems do not com-bine them and especially ignore the impact of POS.Inspired by the algorithms of identifyingBaseNP and Chunk[Xun, et al 2000], we proposea hybrid NER model which emphasizes on com-bining coarse particle features (POS Model) withfine particle features (Word Model).
Though theHybrid Model can overcome the disadvantages ofthe Word Model and the POS Model, there are stillsome problems in such a framework.
Data sparse-ness still exists and very large searching space indecoding will influence efficiency.
Our idea is thatheuristic human knowledge can not only improvethe time efficiency, but also solve the data sparse-ness problem to some extent by restricting the gen-eration of entity candidates.
So we intend toincorporate human knowledge into the statisticalmodel to improve efficiency and effectivity of theHybrid Model.Similarly, for capturing intrinsic features in dif-ferent types of entities, we design several sub-models for each kind of entities.
For example, wedivide transliterated person name into three sub-classes according to their characters sets, that is,JPN, RPN and EPN.3 Chinese NER with Multiple FeaturesChinese NEs have very distinct word features intheir composition and contextual information.
Forexample, about 365 highest frequently used sur-names cover 99% Chinese surnames[Sun, et al1994].
Similarly the characters used for transliter-ated names are also limited.
LNs and ONs often428end with the specific words like "?/province" and"??/company".
However, data sparseness is veryserious when using word features.
So we try tointroduce coarse particle feature to overcome thedata sparseness problem.
POS features are simplestand easy to obtain.
Therefore, our hybrid modelcombines word feature with POS feature to recog-nize Chinese NEs.Given a word/pos sequence as equation (1):nnii twtwtwTW //// 11 LL=                    (1)where n is the number of words and ti is the POSof word wi.
The task of Chinese NE identificationis to find the optimal sequence WC*/ TC* by split-ting, combining and classifying the sequence of (1).mmii21 tc/wctc/wctc/wc*TC/*WC LL=     (2)where [ ]ljji wwwc += L , [ ]ljji tttc += L , nm ?
.Note that the definition of words in {wi} set isthat each kind of NEs (including PN, APN, LN,ALN, ON, TM, NM) is defined as a word and allthe other words in the vocabulary are also definedas individual words.
Consequently, {wi} set has|V|+7 words, where |V| is the size of vocabulary.The size of {ti} set is 48 which include PKU POStagging set1 and each kind of NEs.Obviously, we could obtain the optimal se-quence WC*/TC* through the following threemodels: the Word Model, the POS Model and theHybrid Model.The Word Model employs word features forNER, which is introduced by [Sun, et al 2002].The POS Model employs POS features for NER.This paper proposes a Hybrid Model which com-bines word features with POS features.We will describe these models in detail in fol-lowing section.3.1 The Hybrid ModelFor the convenience of description, we take apartequation (1) into two components: word sequenceas equation (3) and POS sequence as (4).ni21 wwwwW LL=                                     (3)ni21 ttttT LL=                                          (4)The Word Model estimates the probability ofgenerating a NE from the viewpoint of word se-quence, which can be expressed in equation (5).1 http://icl.pku.edu.cn/nlp-tools/catetkset.html( ) ( )WC|WPWCPargmax*WC wc=                  (5)The POS Model estimates the probability ofgenerating a NE from the viewpoint of POS se-quence, which can be expressed in equation (6).
( ) ( )TC|TPTCPargmax*TC TC=                      (6)Our proposed Hybrid Model combines the WordModel with the POS Model, which can be ex-pressed in the equation (7).
( )( ) ( )( ) ( ) ( )( ) ( )( ) ( ) ( ) ( ) ( ) ?
]TCPTC|T[PWCPWC|WPargmaxW,TWC,TC,PargmaxT,WPW,TWC,TC,PargmaxW,T|WC,TCPargmax*TC*,WCTCWC,TCWC,TCWC,TCWC,?===(7)where factor ?
> 0 is to balance the Word Modeland the POS Model.Therefore, the Hybrid Model consists of foursub-models: word context model P(WC), POS con-text model P(TC), word entity model P(W|WC)and POS entity model P(T|TC).3.2 Context ModelThe word context model and the POS contextmodel estimate the probability of generating aword or a POS given previous context.
P(WC) andP(TC) can be estimated according to (8) and (9)respectively.
( ) ( )?==m1i1i2ii wcwc|wcPWCP                       (8)( ) ( )?==m1i1i2ii tctc|tcPTCP                             (9)3.3 Word Entity ModelDifferent types of NEs have different structuresand intrinsic characteristics.
Therefore, a singlemodel can't capture all types of entities.
Typical,character-based model is more appropriate for PNs,whereas, word-based model is more competent forLNs and ONs.
Especially, we divided transliteratedPN into three categories such as JPN, RPN andEPN.For the sake of estimating the probability ofgenerating a NE, we define 19 sub-classes shownas Table 1 according to their position in NEs.429Tag DescriptionSur Surname of CPNDgb First character of Given Name of CPNDge Last character of Give Name of CPNBfn First character of EPNMfn Middle character of EPNEfn Last character of EPNRBfn First character of RPNRMfn Middle character of RPNREfn Last character of RPNJBfn surname of JPNJMfn Middle character of JPNJEfn Last character of JPNBol First word of LNMol Middle word of LNEol Last word of LNAloc Single character LNBoo First word of ONMoo Middle word of ONEoo Last word of ONTable 1 Sub-classes in Entity Model3.3.1 Word Entity Model for PNFor the class of PN (including CPN, APN, JPN,RPN and EPN), the word entity model is a charac-ter-based trigram model which can be expressed inequation (10).
( )( ) ( )( )( )( )1kiik1liil1iik1iik1iwcwc1k2lwcwcwc2kwcwciwcwcw,ENe|wPw,MNe|wPBNe|wPENeMNeMNeBNe|wwPwc|wwP?????????????=??=?
448476LLL(10)where, BNe, MNe and ENe denotes the first, mid-dle and last characters respectively.The word entity models for PN are estimatedwith Chinese, Japanese, Russian and Euramericannames lists which contain 15.6 million, 0.15 mil-lion, 0.44 million, 0.4 million entities respectively.3.3.2 Word Entity Model for LN and ONFor the class of LN and ON, the word entity modelis a word-based trigram model.
The model can beexpressed by (11).
( )( ) ( )( )( ) ( )( )( ) ( )ikwcwcwcwcwcwcwc1k2l1liilwcendwcstartwcwc2kwcwcwciendwcstartwcwc|wwPwc,ENe|wcPwc|wwPwc,MNe|wcPwc|w..wPBNe|wcPENeMNeMNeBNe|wcwcwcPwc|wwPdenikstartik1kiikilendilstartil1i1i1i1iikil1iii?LL448476LLLL??=???????
?==(11)The word entity models and the POS entitymodel for LN and ON are estimated with LN andON names lists which respectively contain 0.44mil-lion and 3.2 million entities.3.3.3 Word Entity Model for ALNFor the class of ALN, we use word-based bi-grammodel.
The entity model for ALN can be expressedby equation (12).
( ) ( ))LocA(CocAL,wCocAL|wP ii =                           (12)where wi is the ALN which includes single andmultiple characters ALN.3.4 POS Entity ModelBut for the class of PN, it's very difficult to obtainthe corpus to train POS Entity Model.
For the sakeof simplification, we use word entity model shownin equation (10) to replace the POS entity model.For the class of LN and ON, POS entity modelcan be expressed by equation (13).
( )( ) ( )( )( ) ( )( )( ) ( )iktctctctctctctc1k2l1liiltcendwcstarttctc2ktctctciendtcstarttctc|ttPtc,ENe|tcPtc|ttPtc,MNe|tcPtc|t..tPBNe|tcPENeMNeMNeBNe|tctctcPtc|ttPdenikstartik1kiikilendilstartil1i1i1i1iikil1iii?LL448476LLLL??=???????
?==(13)While for the class of ALN, POS entity model isshown as equation (14).
( ) ( ))ocAL(CocAL,tiCocAL|tP i =                               (14)4 Heuristic Human KnowledgeIn this section, we will introduce heuristic humanknowledge that is used for Chinese NER and the430method of how to incorporate them into statisticalmodel which are shown as follows.1.
CPN surname list (including 476 items) andJPN surnames list (including 9189 items): Onlythose characters in the surname list can trigger per-son name recognition.2.
RPN and EPN characters lists: Only thoseconsecutive characters in the transliterated charac-ter list form a candidate transliterated name.3.
Entity Length Restriction: Person name can-not span any punctuation and the length of CNcannot exceed 8 characters while the length of TNis unrestrained.4.
Location keyword list (including 607 items):If the word belongs to the list, 2~6 words beforethe salient word are accepted as candidate LNs.5.
General word list (such as verbs and preposi-tions): Words in the list usually is followed by alocation name, such as "?/at", "?/go".
If the cur-rent word is in the list, 2~6 words following it areaccepted as candidate LNs.6.
ALN name list (including 407 items): If thecurrent word belongs to the list, we accept it as acandidate ALN.7.
Organization keyword list (including 3129items): If the current word is in organization key-word list, 2~6 words before keywords are acceptedas the candidate ONs.8.
An organization name template list: Wemainly use organization name templates to recog-nize the missed nested ONs in the statistical model.Some of these templates are as follows:ON-->LN D* OrgKeyWordON-->PN D* OrgKeyWordON-->ON OrgKeyWordD and OrgKeyWord denote words in the middleof ONs and ONs keywords.
D* means repeatingzero or more times.5 Back-off Model to SmoothData sparseness problem still exists.
As some pa-rameters were never observed in training corpus,the model will back off to a less powerful model.The escape probability[Black, et al 1998] was ad-opted to smooth the statistical model shown as (15).00N11N2N1N1N1NN1N1N^p)W(p)WWW(p)WWW(p)WWW(p???
?++++=LLLL  (15)where NN e1?
= , Ni0,e)e1(?N1ikkii <<=+=?
, and eiis the escape probability which can be estimated byequation (16).
)WWW(f)WWW(qe1N211N21N LL=                           (16)q(w1w2?wN-1) in (16) denotes the number of dif-ferent symbol wN that have directly followed theword sequence w1w2?wN-1.6 ExperimentsIn this chapter, we will conduct experiments toanswer the following questions.Will the Hybrid Model be more effective thanthe Word Model and the POS Model?
To answerthis question, we will compare the performances ofmodels with different parameter ?
and find the bestvalue of ?
in equation (7).Will the conclusion from different testing sets beconsistent?
To answer this question, we evaluatemodels on the MET-2 test data and compare theperformances of the Word Model, the POS Modeland the Hybrid Model.Will the performance be improved significantlyafter combining human knowledge?
To answer thisquestion, we compare two models with and with-out human knowledge.In our evaluation, only NEs with correctboundaries and correct categories are considered asthe correct recognition.
We conduct evaluations interms of precision, recall and F-Measure.
Note thatPNs in experiments includes all kinds of PNs andLNs include ALNs.6.1 Will the Hybrid Model be More EffectiveThan the Word Model and POS Model?The parameter ?
in equation (7) denotes the balanc-ing factor of the Word Model and the POS Model.The larger ?, the larger contribution of the POSModel.
The smaller ?, the larger contribution of theWord Model.
So the task of this experiment is tofind the best value of ?.
In this experiment, thetraining corpus is from five-month's People's Dailytagged with NER tags and the testing set is fromone-month's People's Daily.With the change of ?, the performances of rec-ognizing PNs are shown in Fig.1.Note that the left, middle and right point in ab-scissa respectively denote the performance of the431Word Model, the Hybrid Model and the POSModel.0 1.6 3.2 4.8 6.4 8 9.60.880.890.90.910.920.930.940.950.96Lamda%PrecisionRecallF?MeasureFig.1 Performance of Recognizing LNs Impactedby ?From Fig.1, we can find that the performancesof recognizing PNs are improved with the increas-ing of ?
in the beginning stage but decline in theending.
This experiment shows that the WordModel and the POS Model can overcome their dis-advantages, and it is a feasible approach to inte-grate the Word Model and the POS Model in orderto improve the performance PNs recognition.With the change of ?, the performances of rec-ognizing LNs are shown in Fig.2.0 1.6 3.2 4.8 6.4 8 9.60.90.910.920.930.94Lamda%PrecisionRecallF?MeasureFig.2 Performance of Recognizing LNs Impactedby ?As the Fig.2 shows, the precision and recall ofLNs are improved with the increasing of ?
and de-creased in the later stage.
This phenomenon alsoproves that the Hybrid Model is better for recog-nizing LN than either the Word Model or the POSModel.Similarly, with the change of ?, the perform-ances of recognizing ONs are shown in Fig.3.0 1.6 3.2 4.8 6.4 8 9.60.70.750.80.85Lamda%PrecisionRecallF?MeasureFig.3 Performance of Recognizing LNs Impactedby ?Comparing Fig.3 with Fig.1 and Fig.2, we findthat the POS Model has different impact on recog-nizing ONs from that on recognizing PNs and LNs.Especially, the POS Model has obvious side-effecton the recall.
We speculate that the reasons may bethat the probability of generating POS sequence byPOS entity model is lower than that by POS con-text model.According to Fig.1~Fig.3, we choose the bestvalue ?
= 2.8.
And the performances of differentmodels are shown in Table 2 in detail.P(%) R(%) F(%)PN 94.06 95.21 94.63LN 93.98 93.48 93.73HybridModel(?= 2.8)ON 84.69 86.86 85.76PN 88.24 90.11 89.16LN 91.50 93.17 92.32WordModelON 78.85 88.77 83.52PN 93.44 95.11 94.27LN 89.97 92.20 91.07POSModelON 80.90 69.29  74.65Table 2 Performance of the Hybrid Model, theWord Model and the POS ModelFrom Table 2, we find that the F-Measures ofthe Hybrid Model for PN, LN, ON are improvedby 5.4%, 1.4%, 2.2% respectively in comparisonwith the Word Model, and these F-Measures areimproved by 0.4%, 2.7%, 11.1% respectively incomparison with the POS Model.432Conclusion 1: The experimental results validateour idea that the Hybrid Model can improve theperformance of both the Word Model and the POSModel.
However, the improvements for PN, LNand ON are different.
That is, the POS Model hasobvious side-effect on the recall of ON recognitionat all times, while the recalls for PN and ON rec-ognition are improved in the beginning but de-creased in the ending with the increasing of ?.6.2 Will the Conclusion from Different Test-ing Sets be Consistent?We also conduct experiments on the MET-2 test-ing corpus to validate our conclusion from Exp.1,that is, the Hybrid Model could achieve better per-formance than either the Word Model or the POSModel alone.
The experimental results (F-Measure)on MET-2 are shown in Table 3.Model Word ModelHybridModelPOSModelPN 75.21% 80.77% 76.61%LN 89.78% 90.95% 89.81%ON 76.30% 80.21% 76.83%Table 3 F-Measure on MET-2 test corpusComparing Table 3 with Table 2, we find thatthe performances of models on MET-2 are not asgood as that on People Daily's testing data.
Themain reason lies in that the NE definitions in Peo-ple Daily's corpus are different from that in MET-2.However, Table 3 can still validate our conclude 1,that is, the Hybrid Model is better than both theWord Model and the POS Model.
For example, theF-Measures of the Hybrid Model for PN, LN andON are improved by 5.6%, 1.2% and 3.9% respec-tively in comparison with the Word Model, andthese F-Measures are improved by 4.2%, 3.1% and3.4% respectively in comparison with the POSModel.Conclusion 2: Though the performances of theHybrid Model on MET-2 are not as good as thaton People's Daily corpus, the experimental resultsalso support conclusion 1, i.e.
the Hybrid Modelwhich combining the Word Model with the POSModel can achieve better performance than eitherthe Word Model or the POS Model.6.3 Will the Performance be Improved Sig-nificantly after Incorporating HumanKnowledge?One of our ideas in this paper is that humanknowledge can not only reduce the search space,but also improve the performance through avoidinggenerating the noise NEs.
This experiment will beconducted to validate this idea.
Table 4 shows theperformances of models with and without humanknowledge.P(%) R(%) F(%)PN 91.81 70.65 79.85LN 79.47 88.83 83.89 Model ION 64.95 80.63 71.95PN 94.06 95.21 94.63LN 93.98 93.48 93.73 Model IION 84.69 86.86 85.76Table 4 Performances Impacted by Human Know-ledgeFrom Table 4, we find that F-Measure of modelwith human knowledge (Model II) is improved by14.8%?9.8%?13.8% for PN, LN and ON respec-tively compared with that of the model withouthuman knowledge (Model I).Conclusion 3: From this experiment, we learnthat human knowledge can not only reduce thesearch space, but also significantly improve theperformance of pure statistical model.7 ConclusionIn this paper, we propose a hybrid Chinese NERmodel which combines multiple features.
The maincontributions are as follows: ?
The proposed Hy-brid Model emphasizes on integrating coarse parti-cle feature (POS Model) with fine particle feature(Word Model), so that it can overcome the disad-vantages of each other; ?
In order to reduce thesearch space and improve the efficiency of model,we incorporate heuristic human knowledge intostatistical model, which could increase the per-formance of NER significantly; ?
For capturingintrinsic features in different types of entities, wedesign several sub-models for different entities.Especially, we divide transliterated person nameinto three sub-classes according to their charactersset, that is, CPN JPN, RPN and EPN.There is a lack of effective recognition strategyfor abbreviated ONs such as ????
(KunmingMachine Tool Co.,Ltd), ?
?
?
?
(PhoenixPhotonics Ltd) in this paper.
And most of mis-433recognized ONs in current system belong to them.So in the future work, we will be focusing more onrecognizing abbreviated ONs.8 AcknowledgementsThis research is carried out as part of the coopera-tive project with Fujitsu R&D Center Co., Ltd. Wewould like to thank Yingju Xia, Fumihito Nisinofor helpful feedback in the process of developingand implementing.
This work was supported by theNatural Sciences Foundation of China under grantNo.
60372016 and 60272041, the Natural ScienceFoundation of Beijing under grant No.
4052027.ReferencesN.A.
Chinchor: Overview of MUC-7/MET-2.
In: Pro-ceedings of the Seventh Message UnderstandingConference (MUC-7), April.
(1998).Youzheng Wu, Jun Zhao, Bo Xu: Chinese Named En-tity Recognition Combining Statistical Model withHuman Knowledge.
In: The Workshop attached with41st ACL for Multilingual and Mix-language NamedEntity Recognition, Sappora, Japan.
(2003) 65-72.Endong Xun, Changning Huang, Ming Zhou: A UnifiedStatistical Model for the Identification of EnglishBaseNP.
In: Proceedings of ACL-2000, Hong Kong.
(2000).Jian Sun, Jianfeng Gao, Lei Zhang, Ming Zhou,Changning Huang: Chinese Named Entity Identifica-tion Using Class-based Language Model.
In:COLING 2002.
Taipei, August 24-25.
(2002).Huaping Zhang, Qun Liu, Hongkui Yu, Xueqi Cheng,Shuo Bai: Chinese Named Entity Recognition UsingRole Model.
In: the International Journal of Compu-tational Linguistics and Chinese Language Process-ing, vol.8, No.2.
(2003) 29-60.D.M.
Bikel, Scott Miller, Richard Schwartz, RalphWeischedel: Nymble: a High-Performance LearningName-finder.
In: Fifth Conference on Applied Natu-ral Language Processing, (published by ACL).
(1997)194-201.Borthwick .A: A Maximum Entropy Approach toNamed Entity Recognition.
PhD Dissertation.
(1999).Mikheev A., Grover C. and Moens M: Description ofthe LTG System Used for MUC-7.
In: Proceedings of7th Message Understanding Conference (MUC-7),1998.Sekine S., Grishman R. and Shinou H: A decision treemethod for finding and classifying names in Japanesetexts.
In: Proceedings of the Sixth Workshop on VeryLarge Corpora, Canada, 1998.Aberdeen, John, et al MITRE: Description of theALEMBIC System Used for MUC-6.
In: Proceedingsof the Sixth Message Understanding Conference(MUC-6), November.
(1995) 141-155.Ralph Grishman and Beth Sundheim: Design of theMUC-6 evaluation.
In: 6th Message UnderstandingConference, Columbia, MD.
(1995)Krupka, G. R. and Hausman, K. IsoQuest: Inc.: Descrip-tion of the NetOwl TM Extractor System as Used forMUC-7.
In Proceedings of the MUC-7, 1998.Black, W.J.
; Rinaldi, F, Mowart, D: FACILE: Descrip-tion of the NE System Used for MUC-7.
In Proceed-ings of the MUC-7, 1998.Michael Collins, Yoram Singer: Unsupervised modelsfor named entity classification.
In Proceedings ofEMNLP.
(1999)Michael Collins: Ranking Algorithms for Named EntityExtraction: Boosting and the Voted Perceptron.
In:Proceeding of ACL-2002.
(2002) 489-496.S.Y.Yu, et al Description of the Kent Ridge DigitalLabs System Used for MUC-7.
In: Proceedings of theSeventh Message Understanding Conference, 1998.H.H.
Chen, et al Description of the NTU System Usedfor MET2.
In: Proceedings of the Seventh MessageUnderstanding Conference.Tat-Seng Chua, et al Learning Pattern Rules for Chi-nese Named Entity Extraction.
In: Proceedings ofAAAI'02.
(2002)Maosong Sun, et al Identifying Chinese Names in Un-restricted Texts.
Journal of Chinese InformationProcessing.
(1994).Jiahen Zheng, Xin Li, Hongye Tan: The Research ofChinese Names Recognition Methods Based on Cor-pus.
In: Journal of Chinese Information Processing.Vol.14 No.1.
(2000).CoNLL.
http://cnts.uia.ac.be/conll2004/IEER.
http://www.nist.gov/speech/tests/ie-er/er99/er99.htmACE.
http://www.itl.nist.gov/iad/894.01/tests/ace/434
