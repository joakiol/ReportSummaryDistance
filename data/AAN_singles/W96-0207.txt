iCombining Hand-crafted Rules and Unsupervised LearninginConstraint-based Morphological DisambiguationKemal Oflazer and GSkhan TfirDepartment of Computer Engineering and Information ScienceBilkent University, Bilkent, Ankara, TR-06533, TURKEY{ko, tur}~cs, bilkent, edu.
trAbstract  1 Introduct ionThis paper presents a constraint-basedmorphological disambiguation approachthat is applicable languages with complexmorphology-specifically agglutinative lan-guages with productive inflectional andderivational morphological phenomena.
Incertain respects, our approach has beenmotivated by Brill's recent work (Brill,1995b), but with the observation that histransformational pproach is not directlyapplicable to languages like Turkish.
Oursystem combines corpus independent hand-crafted constraint rules, constraint rulesthat are learned via unsupervised learn-ing from a training corpus, and additionalstatistical information from the corpus tobe morphologically disambiguated.
Thehand-crafted rules are linguistically moti-vated and tuned to improve precision with-out sacrificing recall.
The unsupervisedlearning process produces two sets of rules:(i) choose rules which choose morpholog-ical parses of a lexical item satisfying con-straint effectively discarding other parses,and (ii) delete rules, which delete parsessatisfying a constraint.
Our approach alsouses a novel approach to unknown wordprocessing by employing a secondary mor-phological processor which recovers any rel-evant inflectional and derivational informa-tion from a lexieal item whose root is un-known.
With this approach, well below1% of the tokens remains as unknown inthe texts we have experimented with.
Ourresults indicate that by combining thesehand-crafted, statistical and learned infor-mation sources, we can attain a recall of 96to 97% with a corresponding precision of93 to 94%, and ambiguity of 1.02 to 1.03parses per token.Automatic morphological disambiguation is a verycrucial component in higher level analysis of naturallanguage text corpora.
Morphological disambigua-tion facilitates parsing, essentially by performing acertain amount of ambiguity resolution using rela-tively cheaper methods (e.g., Gfing6rdii and Oflazer(1995)).
There has been a large number of studiesin tagging and morphological disambiguation usingvarious techniques.
Part-of-speech tagging systemshave used either a statistical approach where a largecorpora has been used to train a probabilistic modelwhich then has been used to tag new text, assign-ing the most likely tag for a given word in a givencontext (e.g., Church (1988), Cutting et al (1992),DeRose (1988)).
Another approach is the rule-basedor constraint-based approach, recently most promi-nently exemplified by the Constraint Grammar work(Karlsson et al, 1995; Voutilainen, 1995b; Vouti-lainen et al, 1992; Voutilainen and Tapanainen,1993), where a large number of hand-crafted linguis-tic constraints are used to eliminate impossible tagsor morphological parses for a given word in a givencontext.
Brill (1992; 1994; 1995a) has presented atransformation-based l arning approach, which in-duces rules from tagged corpora.
Recently he hasextended this work so that learning can proceedin an unsupervised manner using an untagged cor-pus (Brill, 1995b).
Levinger et al (1995) have re-cently reported on an approach that learns morpho-lexical probabilities from untagged corpus and havethe used the resulting information in morphologicaldisambiguation i  Hebrew.In contrast o languages like English, for whichthere is a very small number of possible word formswith a given root word, and a small number of tagsassociated with a given lexical form, languages likeTurkish or Finnish with very productive agglutina-tive morphology where it is possible to produce thou-sands of forms (or even millions (Hankamer, 1989))for a given root word, pose a challenging problemfor morphological disambiguation.
In English, forexample, a word such as make or set can be verb69or a noun.
In Turkish, even though there are ambi-guities of such sort, the agglutinative nature of thelanguage usually helps resolution of such ambiguitiesdue to restrictions on lnorphotactics.
On the otherhand, this very nature introduces another kind ofambiguity, where a lexical form can be morpholog-ically interpreted in many ways, some with totallyunrelated roots and morphological features, as willbe exemplified in the next section.Our previous approach to tagging and morpho-logical disambiguation for Turkish text had em-ployed a constraint-based approach (Oflazer andKuru6z, 1994) along the general ines of similar pre-vious work for English (Karlsson et al, 1995; Vouti-lainen et al, 1992; Voutilainen and Tapanainen,1993).
Although the results obtained there were rea-sonable, the fact that all constraint rules were handcrafted, posed a rather serious impediment to thegenerality and improvement of the system.In this paper we present a constraint-based mor-phological disambiguation approach that uses unsu-pervised learning component to discover some of theconstraints it uses in conjunction with hand-craftedrules.
It is specifically applicable to languages withproductive inflectional and derivational morpholog-ical processes, such as Turkish, where morpholog-ical ambiguity has a rather different nature thanthat found in languages like English.
Our approachstarts with a set of corpus-independent hand-craftedrules that reduce morphological ambiguity (henceimprove precision) without sacrificing recall.
It thenuses an untagged training corpus in which all lexicalitems have been annotated with all possible morpho-logical analyses, incrementally proposing and eval-uating additional (possibly corpus dependent) con-straints for disambiguation of morphological parsesusing the constraints imposed by unambiguous con-texts.
These rules choose or delete parses with spec-ified features.
In certain respects, our approach asbeen motivated by Brill's recent work (Brill, 1995b),but with the observation that his transformationalapproach is not directly applicable to languages likeTurkish, where tags associated with forms are notpredictable in advance.In the following sections, we present an overviewof the morphological disambiguation problem, high-lighted with examples from Turkish.
We thenpresent he details of our approach and results.
Wefinally conclude after a discussion and evaluation ofour results.2 Tagg ing  and  Morpho log ica lD isambiguat ionIn almost all languages, words are usually ambigu-ous in their parts-of-speech or other lexical features,and may represent lexical items of different syntac-tic categories, or morphological structures depend-ing on the syntactic and semantic ontext.
Part-of-speech (POS) tagging involves assigning every wordits proper part-of-speech based upon the context heword appears in.
In English, for example a wordsuch as set can be a verb in certain contexts (e.g.,He set the table for dinner) and a noun in some oth-ers (e.g., We are now facing a whole set of problems).In Turkish, there are ambiguities of the sortabove.
However, the agglutinative nature of thelanguage usually helps resolution of such ambigui-ties due to the restrictions on morphotactics.
Onthe other hand, this very nature introduces anotherkind of ambiguity, where a whole lexical form can bemorphologically interpreted in many ways not pre-dictable in advance.
For instance, our full-scale mor-phological analyzer for Turkish returns the followingset of parses for the word oysa: 1,21.
\[\[CAT CONN\] \[ROOT oysa\]\](on the other hand)2.
\[\[CAT NOUN\] \[ROOT oy\] \[AGR 3SG\]\[POSS NONE\] \[CASE NOM\]\[c0Nv VERB NONE\]\[TAM1COND\] \[AGR 3SG\]\](if it is a vote)3.
\[\[CAT PRONOUN\] \[ROOT o\] \[TYPE DEMONS\]\[AGR 3SG\] \[POSS NONE\]\[CASE NOM\] \[CONV VERB NONE\]\[TAM1COND\]\[AGR 3SG\]\](if it is)4.
\[\[CAT PRONOUN\] \[ROOT o\] \[TYPE PERSONAL\]\[AGR 3SG\] \[POSS NONE\] \[CASE NOM\]\[CONV VERB NONE\] \[TAM1COND\]\[AGR 3SG\]\](if s/he is)s. \[\[CAT VERB\] \[ROOT oy\] \[SENSE POS\]\[TAM1 DES\] \[AGR 3SG\]\](wish s/he would carve)On the other hand, the form oya gives rise to thefollowing parses:1.
\[\[CAT NOUN\] \[ROOT oya\] \[AGR 3SG\]\[POSS NONE\] \[CASE NOM\]\] (lace)2.
\[\[CAT NOUN\] \[ROOT oy\] \[AGR 3SG\]\[POSS NONE\] \[CASE DAT\]\] (to the vote)3.
\[\[CAT VERB\] \[ROOT oy\] \[SENSE POS\]\[TAM1 OPT\] \[AGR 3SG\]\] (let him carve)and the form oyun gives rise to the following parses:iOutput of the morphological anaJyzer is edited forclarity, and English glosses have been given.2Glosses are given as linear feature value sequencescorresponding to the morphemes (which are not shown).The feature names are as follows: CAT-major category,TYPE-minor category, R00T-main root form, AGR -numberand person agreement, POSS- possessive agreement, CASEsurface case, CONV- conversion to the category follow-ing with a certain suffix indicated by the argument afterthat, TAMl-tense, aspect, mood marker 1, SENSE-verbalpolarity, DES- desire mood, IMP-imperative mood, 0PT-optative mood, COND-Conditional70II.
\[\[CAT NOUN\] \[ROOT oyun\] \[AGR 3SG\]\[POSS NONE\] \[CASE NOM\]\] (game)2.
\[\[CAT NOUN\] \[ROOT oy\] \[AGR 3SG\]\[POSS NONE\] \[CASE GEN\]\] (of the vote)3.
\[\[CAT NOUN\] \[ROOT oy\] \[AGR 3SG\]\[POSS 2SG\] \[CASE NOM\]\] (your vote)4.
\[\[CAT VERB\] \[ROOT oy\] \[SENSE POS\]\[TAM1 IMP\] \[AGR 2PL\]\] (carve it!
)On  the other hand, the local syntactic contextmay help reduce some of the ambiguity above, asin: 3sen-in oy-un ..PRON(you)+GEN NOUN(vote)+POSS-2SGyour voteoy-un reng-i ..NOUN(vote)+GEN NOUN(color)+POSS-3SG(NOUN-GEN NOUN-POSS form)color of the voteoyun reng-i ..NOUN(game) NOUN(color)+POSS-3SGgame color(NOUN NOUN-POSS form)using some very basic noun phrase agreement con-straints in Turkish.
Obviously in other similar cases,it may be possible to resolve the ambiguity com-pletely.There are also numerous other examples of wordforms where productive derivational processes comeinto play: 4geldiGimdeki (at the time I came)\[\[CAT VERB\] \[ROOT gel\] \[SENSE POS\](basic form)\[CONV NOUN DIE\] \[AGR 3SG\]\[POSS iSG\] \[CASE LOC\](participle form)\[CONV ADJ REL\]\](final adjectivalization by therelative (ki) suffix)Here, the original root is verbal but the final part-of-speech is adjectival.
In general, the ambiguities ofthe forms that come before such a form in text can beresolved with respect o its original (or intermediate)parts-of-speech (and inflectional features), while theambiguities of the forms that follow can be resolvedbased on its final part-of-speech.The main intent of our system is to achieve a mor-phological ambiguity reduction in the text by choos-ing for a given ambiguous token, a subset of itsZWith a slightly different but nevertheless commonglossing convention.4Upper cases in morphological output indicates one ofthe non-ASCII special Turkish characters: e.g., G denotes~, U denotes /i, etc.parses which are not disallowed by the syntactic on-text it appears in.
It is certainly possible that a giventoken may have multiple correct parses, usually withthe same inflectional features or with inflectional fea-tures not ruled out by the syntactic context.
Thesecan only be disambiguated usually on semantic ordiscourse constraint grounds.
5We consider a token fully disambiguated if it hasonly one morphological parse remaining after auto-matic disambiguation.
We consider as token as cor-rectly disambiguated, if one of the parses remain-ing for that token is the correct intended parse.
6We evaluate the resulting disambiguated text by anumber of metrics defined as follows (Voutilainen,1995a):#ParsesAmbiguity -#TokensRecall = #Tokens  Correctly Disambiguatcd#Tokens~Tokcns  Correctly DisambiguatedPrecision =~ParsesIn the ideal case where each token is uniquely andcorrectly disambiguated with the correct parse, bothrecall and precision will be 1.0.
On the other hand, atext where each token is annotated with all possibleparses, 7 the recall will be 1.0 but the precision willbe low.
The goal is to have both recall and precisionas high as possible.3 Const ra in t -based  Morpho log ica lD isambiguat ionThis section outlines our approach to constraint-based morphological disambiguation i corporatingunsupervised learning component.
Our system withthe structure presented in Figure 1 has three maincomponents:1. the preprocessor,2.
the learning module, and3.
the morphological disambiguation module.Preprocessing is common to both the learning andthe morphological disambiguation modules.
Themodule takes as input to the system raw Turkishtext and preprocesses it in a manner to be describedshortly.If the text is to be used for training, the learningmodule then1.
applies an initial set of linguistically motivatedhand-crafted constraint rules to choose and/ordelete certain parses, and5For instance the third and fourth parses for oysaabove.6It is certainly possible that, a parse that is deletedmay also be a valid parse in that context.rAssuming no unknown words.712.
uses an unsupervised learning procedure to in-duce some additional (an possibly corpus de-pendent) rules to choose and delete some parses.Morphological disambiguation of previously un-seen text proceeds as follows:1.
The hand-crafted rules are applied first.2.
Certain parses are deleted using context statis-tics on the corpus to be tagged.3.
Rules learned to choose and delete parses arethen applied.3.1 The  PreprocessorThe preprocessing module takes as input a Turk-ish text, segments it into sentences using variousheuristics about punctuation, tokenizes and runs itthrough a wide-coverage high-performance morpho-logical analyzer developed using two-level morphol-ogy tools by Xerox (Karttunen, 1993).
This modulealso performs a number of additional functions:?
it groups lexicalized collocations such as id-iomatic forms, semantically coalesced formssuch as proper noun groups, certain numericforms, etc.?
it groups any compound verb formations whichare formed by a lexically adjacent, direct oroblique object, and a verb, which for the pur-poses of syntactic analysis, may be consideredas single lexical item: e.g., saygz durmak (to payrespect), kafay~ yemek (literally t0 eat the head- to get mentally deranged), etc.?
it groups non-lexicalized collocations: Turkishabounds with various non-lexicalized colloca-tions where the sentential role of the colloca-tion has (almost) nothing to do with the parts-of-speech of the individual forms involved.
Al-most all of these collocations involve duplica-tions, and have forms like w + x w + y wherew is the duplicated string comprising the rootand certain sequence of suffixes and x and y arepossibly different (or empty) sequences of othersuffixes.The following is a list of multi-word constructsfor Turkish that we handle in our preproces-sor.
This list is not meant o be comprehensive,and new construct specifications can easily beadded.
It is conceivable that such a function-ality can be used in almost any language.
(SeeOflazer and Kuru6z (1994) and KuruSz (1994)for details of all other forms for Turkish.)1.
duplicated optative and 3SG verbal formsfunctioning as manner adverb.
An exampleis ko~a ko~a, where each lexical item has themorphological parse\[\[CAT VERB\] \[ROOT koS\] \[SENSE POS\]\[TAM1 0PT\] \[AGR3SG\]\]The preprocessor recognizes this and gen-erates the feature sequence:2.\[\[CAT VERB\] \[ROOT koS\] \[SENSE POS\]\[TAM1 OPT\] \[AGR 3SG\]\[CONV ADVERB DUPi\] \[TYPE MANNER\]\]aorist verbal forms with root duplicationsand sense negation, functioning as tem-poral adverbs.
For instance for the non-lexicalized collocation yapar yapmaz, whereitems have the parses\[\[CAT VERB\] \[ROOT yap\] \[SENSE P0S\]\[TAM1 AORIST \] \[AGR 3SG\]\]\[\[CAT VERB\] \[ROOT yap\] \[SENSE BEG\]\[TAM1 AORIST \] \[AGR 3SG\]\]respectively, the preprocessor generates thefeature sequence\[\[CAT VERB\] \[ROOT koS\] \[SENSE POS\]\[TAM1 AORIST\] \[AGR 3SG\]\[CONV ADVERB DUP-AOR\] \[TYPE TEMP\]\]3. duplicated verbal and derived adverbialforms with the same verbal root acting astemporal adverbs, e.g., gitti gideli,4.
emphatic adjectival forms involving dupli-cation and question clitic, e.g., g71zel mig~zel (beautiful question-clitic beautiful-very beautiful)5. adjective or noun duplications that act asmanner adverbs, e.g., hzzh hzzh, evev,This module recognizes all such forms and coa-lesces them into new feature structures reflect-ing the final structure along with any inflec-tional information.?
The preprocessor then converts each parse intoa hierarchical feature structure so that the in-flectional feature of the form with the last cat-egory conversion (if any) are at the top level.Thus in the example above for geldi~imdeki, thefollowing feature structure is generated:\[\[CAT VERB\] \[ROOT gel \ ]  \[SENSE POS\]\[CONV NOUN DIK\] \[AGR 3SG\]\[P0SS ISG\] \[CASE LOC\]\[CONV ADJ REL\]\]"CAT ADJ"CATAGRPOSSCASENOUN3SGiSGLOCSTEM CAT VERB'STEM \ [ROOT gel\ [ .SENSE POSSUFF IX  D IKSUFF IX  ~EL?
Finally, each such feature structure is then pro-jected on a subset of its features.
The featuresselected are- inflectional and certain derivational mark-ers, and stems for open class of words,72TOKEN IZATION MORPHOLOGY NON-LEXICAL UNKNOWN FORMATCOLLOCATION WORD CONVERSIONRECOGNIZER PROCESSOR ( / PRO/ECTION )PREPROCESSORMORPHOLOGICALDISAM BIGUATIONMODULELEARNING LEARNED RULESMOI)ULEFigure h The structure of the constraint-based morphological disambiguation system.-- roots and certain relevant features uch assubcategorization requirements for closedclasses of words such as connectives, post-positions, etc.The set of features selected for each part-of-speech category is determined by a templateand hence is controllable, permitting experi-mentation with differing levels of information.The information selected for stems are deter-mined by the category of the stem itself recur-sively.Under certain circumstances where a token hastwo or more parses that agree in the selectedfeatures, those parses will be represented bya single projected parse, hence the number ofparses in the (projected) training corpus may besmaller than the number of parses in the origi-nal corpus.
For example, the feature structureabove is projected into a feature structure suchas :-CAT  ADJ\[OAT NOUN \]\]|AGR 3SG|POSS 1SGSTEM /CASE LOC/STEM \[CAT VERB\[SUFFIX DIKSUFFIX REL3.2 Unknown WordsAlthough the coverage of our morphological nalyzerfor Turkish (Oflazer, 1993), with about 30,000 rootwords and about 35,000 proper names, is very sat-isfactory, it is inevitable that there will be formsin the corpora being processed that are not recog-nized by the morphological nalyzer.
These are al-most always foreign proper names, words adaptedinto the language and not in the lexicon, or veryobscure technical words.
These are nevertheless in-flected (using Turkish word formation paradigms)with inflectional features demanded by the syntacticcontext and sometimes even go through derivationalprocesses.
For improved disambiguation, one has toat least recover any morphological features even ifthe root word is unknown.
To deal with this, wehave made the assumption that all unknown wordshave nominal roots, and built a second morphologi-cal analyzer whose (nominal) root lexicon recognizesS + where S is the Turkish surface alphabet (in thetwo-level morphology sense), but then tries to in-terpret an arbitrary postfix of the unknown wordas a sequence of Turkish suffixes subject to all mor-phographemic constraints.
For instance when a formsuch as ta lkshowumun is entered, this second ana-lyzer hypothesizes the following analyses:I.
\[\[CAT NOUN\] \[ROOT talkshowumun\]\[AGR 3SG\] \[POSS NONE\] \[CASE NOM\]\]2.
\[\[CAT NOUN\] \[ROOT talkshowumu\]\[AGR 3SG\] \[POSS 2SG\] \[CASE NOM\]\]3.
\[\[CAT NOUN\] \[ROOT talksho~um\]\[AGR 3SG\] \[POSS NONE\] \[CASE GEN\]\]4.
\[\[CAT NOUN\] \[ROOT talkshowum\]\[AGR 3SG\] \[POSS 2SG\] \[CASE NOM\]\]5.
\[\[CAT NOUN\] \[ROOT talksho~u\]\[AGR 3SG\] \[POSS 1SG\] \[CASE GENII6.
\[\[CAT NOUN\] \[ROOT talkshow\]\[AGR 3SG\] \[POSS ISG\] \[CASE GEN\]\]which are then processed just like any other duringdisambiguation.SThis however is not a sufficient solution for somevery obscure situations where for the foreign wordis written using its, say, English orthography, whilesuffixation goes on according to its English pronun-ciation, which may make some constraints like vowel8Incidentally, the correct analysis is the 6 th, meaningo.\[ my talk show.
The 5 th one has the same morphologicalfeatures except for the root.73harmony inapplicable on the graphemic representa-tion, though harmony is in effect in the pronuncia-tion.
For instance one sees the form Carter'a wherethe last vowel in Carter is pronounced so that itharmonizes with a in Turkish, while the e in thesurface form does not harmonize with a.
We arenevertheless rather satisfied with our solution as inour experiments we have noted that well below 1%of the forms remain as unknown and these are usu-ally item markers in formatted or itemized lists, orobscure foreign acronyms.3.3 Const ra in t  RulesThe system uses rules of the sortif LC and RC then choose PARSE orif LC and RC then de le te  PARSEwhere LC and RC are feature constraints on unam-biguous left and right contexts of a given token, andPARSE is a feature constraint on the parse(s) that is(are) chosen (or deleted) in that context if they aresubsumed by that constraint.
Currently the left andright contexts can be at most 2 tokens, hence welook at a window of at most 5 tokens of which oneis ambiguous.
We refer to the unambiguous tokensin the context as l l c  (left-left context) lc  (left con-text), rc  (right context) and r rc  (right-right con-text).
Depending on the amount of unambiguoustokens in a context, our rules can have one of thefollowing context structures, listed in order of de-creasing specificity:i. l l c ,  Ic  .
.
.
.
rc ,  r rc2.
llc, ic ....rc,  r rc3.
ic rc4.
lcrcTo illustrate the flavor of our rules we can givethe following examples.
The first example choosesparses with case feature ablative, preceding an un-ambiguous postposition which subcategorizes for anablative nominal form.\[llc: \[\] ,Ic: \[\] ,choose : \[case : abl\] ,rc: \[\[cat :postp,subcat :abl\]\] ,rrc: \[\]\]A second example rule is\[llc : \[ \ [ ca t  : adj  , type : determiner\]  \] ,ic: \[\[cat :adj ,stem: \[cat :noun\]\]\] ,choose: \[cat :adj\] ,r c : \ [ \ [cat :noun,poss : 'NONE' \ ] \ ] ,  rrc:\[ \] \] .which selects and adjective parse following a deter-miner, adjective sequence, and before a noun with-out a possessive marker.Another sample rule is:\[llc: \[\] ,Ic: \[ \[agr: '2SG' ,case:gen\]\] ,choose: \[cat :noun,poss: '2SG'\] ,rc: \[3 ,rrc: \[\]3which chooses a nominal form with a possessivemarker 2SG following a pronoun with 2SG agree-ment and genitive case, enforcing the simplest formof noun-noun form noun phrase constraints.Our system uses two hand-crafted sets of rules,in combination with the rules that are learned byunsupervised learning:1.
We use an initial set of hand-crafted chooserules to speed-up the learning process by cre-ating disambiguated contexts over which statis-tics can be collected.
These rules (examplesof which are given above) are independent ofthe corpus that is to be tagged, and are lin-guistically motivated.
They enforce some verycommon feature patterns especially where wordorder is rather strict as in NP's or PP's.
9The motivation behind these rules is that theyshould improve precision without sacrificing re-call.
These are rules which impose very tightconstraints o as not to make any recall errors.Our experience is that after processing withthese rules, the recall is above 99% while pre-cision improves by about 20 percentage points.Another important feature of these rules is thatthey are applied even if the contexts are alsoambiguous, as the constraints are tight.
Thatis, if each token in a sequence of, say, three am-biguous tokens have a parse matching one of thecontext constraints (in the proper order), thenall of them are simultaneously disambiguated.In hand crafting these rules, we have used ourexperience from an earlier tagger (Oflazer andKuruhz, 1994).
Currently we use 288 hand-crafted choose rules.2.
We also use a set of hand-crafted heuristic deleterules to get rid of any very low probabilityparses.
For instance, in Turkish, postpositionshave rather strict contextual constraints and ifthere are tokens remaining with multiple parsesone of which is a postposition reading, we deletethat reading.
Our experience is that these rulesimprove precision by about 10 to 12 additionalpercentage points with negligible impact on re-call.
Currently we use 43 hand-crafted eleterules.3.4 Learning Choose RulesGiven a training corpus, with tokens annotated withpossible parses (projected over selected features), wefirst apply the hand-crafted rules.
Learning thengoes on as a number of iterations over the trainingcorpus.
We proceed with the following schema whichis an adaptation of Brill's formulation (Brill, 1995b):9Turkish is a free const i tuent  order language whoseunmarked order is SOV.741.
We generate a table, called incontext, of allpossible unambiguous contexts which contain atoken with an unambiguous (projected) parse,along with a count of how many times thisparse occurs unambiguously in exactly the samecontext in the corpus.
We refer to an en-try in table with a context C and parse P asincontext(C, P).2.
We also generate a table, called count, of allunambiguous parses in the corpus along with acount of how many times this parse occurs inthe corpus.
We refer to an entry in this tablewith a given parse P, as count(P).3.
We then start going over the corpus token bytoken generating contexts as we go.4.
For each unambiguous context encountered,C = (LC, RC) 1?
around an ambiguous token wwith parses P1,.
?
?
Pk, and for each parse Pi, wegenerate a candidate rule of the sortif LC and RC then choose Pi5.
Every such candidate rule is then scored in thefollowing fashion:(a) We computePma~ = argmaxpj (j#i) count(Pj)"incontext( C, Pj ).
(b) The score of the candidate rule is then com-puted as:Scorei = incontext(C, P i )  - count(P~) count(Pmax)"incontext( C, Pmaz)6.
We order all candidate rules generated uringone pass over the corpus, along two dimensions:(a) we group candidate rules by context speci-ficity (given by the order in Section 3.3),(b) in each group, we order rules by descendingscore.We maintain score thresholds associated witheach context specificity group: the threshold ofa less specific group being higher than that ofa more specific group.
We then choose the topscoring rule from any group whose score equalsor exceeds the threshold associated with thatgroup.
The reasoning is that we prefer morespecific and/or high scoring rules: high scor-ing rules are applicable, in general, in moreplaces; while more specific rules have stricterconstraints and more accurate morphologicalparse selections, We have noted that choosingthe highest scoring rule at every step may some-times make premature commitments which cannot be undone later.1?Either of LC or RC may be empty.7.
The selected rules are then applied in thematching contexts and ambiguity in those con-texts is reduced.
During this application thefollowing are also performed:(a) if the application results in an unambigu-ous parse in the context of the applied rule,we increment the count associated with thisparse in table count.
We also update theincontext able for the same context, andother contexts which contains the disam-biguated parse.
(b) we also generate any new unambiguouscontexts that this newly disambiguated to-ken may give rise to, and add it to theincontext able along with count 1.Note that for efficiency reasons, rule candidatesare not generated repeatedly during each passover the corpus, but rather once at the begin-ning, and then when selected rules are appliedto very specific portions of the corpus.8.
If there are no rules in any group that exceedits threshold, group thresholds are reduced bymultiplying by a damping constant d (0 < d <1) and iterations are continued.9.
If the threshold for the most specific contextfalls below a given lower limit, the learning pro-cess is terminated.Some of the rules that have been generated by thislearning process are given below:1.
Disambiguate around a coordinating conjunc-tion:\ [ l l c :  \[\] , i c :  \[\] ,choose : \[cat :noun,agr :  3SG ,case :nom\],rc : \[ \[cat :conn, root  : re\] \] ,r rc  : \[ \[cat : noun, agr : 3SG, poss : NONE\] \] \]2.
Choose participle form adjectival over a nomi-nal reading:\ [ l l c :  \ [ \]  , I c :  \ [ \ ] ,choose  : \[cat : adj,  su f f i x  : yah\ ] ,rc : \[ \[cat : noun,  agr  : 3SG, poss  : NONE\]  \] ,rrc:  \ [ \ [cat :noun ,agr :3SG,poss :  3SG\] \ ] \ ]  .3.
Choose a nominal reading (over an adjectival)if a three token compound noun agreement canbe established with the next two tokens:\ [ l lc :  \[\] , lc:  \[\] ,choose : \[cat :noun,agr: 3SG ,case :nom\],rc : \[ \[cat :noun, agr : 3SG,poss : 3SG\] ] ,rrc : \[ \[cat : noun, agr : 3SG,poss : 3SG\] \]3.4.1 Contexts  induced  by morpho log ica lder ivat ionThe procedure outlined in the previous ection hasto be modified slightly in the case when the unam-biguous token in the rc position is a morphologi-cally derived form.
For such cases one has to takeinto consideration additional pieces of information.75We will motivate this using a simple example fromTurkish.
Consider the example fragment:... bir masa+dlr.... a tab le+is... is a tablewhere the first token has the morphological parses:I.
\[\[CAT ADJ\] \[ROOT bir\] \[TYPE CARDINAL\]\](one)2.
\[\[CAT ADJ\] \[ROOT bir\] \[TYPE DETERMINER\]\](a)3.
\[\[CAT ADVERB\] \[ROOT bir\]\](only/merely)and the second form has the unambiguous morpho-logical parse:1.
\[\[CAT NOUN\] \[ROOT masa\] \[AGR 3SG\] \[POSS NONE\]\[CASE NOM\] \[CONV VERB NONE\]\[TAM1PRES\] \[AGR 3SG\]\] (is table)which in hierarchical formcorresponds to the ~atureVERBPRES3SGROOT masaSTEM 3SGI POSS NONELCASE NOMSUFFIX NONEIn the syntactic ontext his fragment is interpretedasstructure:"C ATTAM1%GRVPNP +dlrDET NOUNI Ibir masawhere the determiner is attached to the noun andthe whole phrase is then taken as a VP although theverbal marker is on the second lexical item.
If, inthis case, the token bi t  is considered to neighbor atoken whose top level inflectional features indicateit is a verb, it is likely that bi t  will be chosen asan adverb as it precedes a verb, whereas the correctparse is the determiner reading.In such a case where the right context of an am-biguous token is a derived form, one has to con-sider as the right context, both the top level featuresof final form, and the s tem from which it was de-rived.
During the set-up of the i ncontext  table, sucha context is entered twice: once with the top levelfeature constraints of the immediate unambiguousright-context, and once with the feature constraintsof the stem.
The unambiguous token in the rightcontext is also entered to the count  table once withits top level feature structure and once with the fea-ture structure of the stem.When generating candidate choose or delete rules,for contexts where rc is a derived form and r rc  isempty, we actually generate two candidates rules foreach ambiguous token in that context:1. if llc, ic and rc then choose/delete Pi .2. if llc, Ic and s tem(re)  then choose/deleteP~.These candidate rules are then evaluated as de-scribed above.
In general all derivations in a lexicalform have to be considered though we have notedthat considering one level gives satisfactory results.3.4.2 Ignoring FeaturesSome morphological features are only meaningfulor relevant for disambiguation only when they ap-pear to the left or to the right of the token to bedisambiguated.
For instance, in the case of Turkish,the CASE feature of a nominal form is only useful inthe immediate left context, while the POSS (the pos-sessive agreement marker) is useful only in the rightcontext.
If these features along with their possiblevalues are included in context positions where theyare not relevant, they "split" scores and hence causethe selection of some other irrelevant rule.
Using themaxim that union gives strength, we create contextsso that features not relevant o a context position arenot included, thereby treating context hat differ inthese features as same.
113.5 Learning Delete RulesFor choosing delete rules we have experimented withtwo approaches.
One obvious approach is to usethe formulation described above for learning chooserules, but instead of generating choose rules, pickthe parses that score (significantly) worse than andgenerate delete rules for such parses.
We have imple-mented this approach and found that it is not verydesirable due to two reasons:1. it generates far too many delete rules, and2.
it impacts recall seriously without a correspond-ing increase in precision.The second approach that we have used is consid-erably simpler.
We first reprocess the training cor-pus but this time use a second set of projection tem-plates, and apply initial rules, learned choose rulesand heuristic delete rules.
Then for every unambigu-ous context C = (LC, RC), with either an immediateleft, or an immediate right components or both (son Obviously these features are specific to a language.76the contexts used here are the last 3 in Section 3.3),a scoreincontext( C, Pi )count (Pi )for each parse Pi of the (still) ambiguous token, iscomputed.
Then, delete rules of the sortif LC and RC then de le te  Piare generated for all parses with a score below a cer-tain fraction (0.2 in our experiments) of the highestscoring parse.
In this process, our main goal is toremove any seriously improbable parses which maysomehow survive all the previous choose and deleteconstraints applied so far.
Using a second set of tem-plates which are more specific than the templatesused during the learning of the choose rules, we in-troduce features we were originally projected out.Our experience has been that less strict contexts(e.g., just a lc  or rc)  generate very useful deleterules, which basically weed out what can (almost)never happen as it is certainly not very feasible toformulate hand-crafted rules that specify what se-quences of features are not possible.Some of the interesting delete rules learned hereare:1.
Delete the first of two consecutive verb parses:\ [ l l c  : \[\] , lc :  \[\] ,de le te  : \ [ cat  : verb \ ]  ,re :  \ [ \ [ ca t  :verb \ ] \ ] , r rc :  \[\]\]2.
Delete accusative case marked noun parse be-fore a postposition that subcategorizes for anominative noun:\ [ l l c :  \[\] ,Ic: \[\] ,de le te  : \ [ cat  :noun,  agr  : 3SG,poss  : NONE,  case  : acc \ ]  ,r c  : \[ \ [ ca t  : pos tp ,  subcat  : nora\] \] , r rc  : \[\] \] .3.
Delete the accusative case marked parse with-out any possessive marking, if the previous formhas genitive case marking (signaling a genitive-possessive NP construction):\ [ l l c :  \[\] ,lc  : \[ \[cat : noun, agr : 3SG,poss : NONE, case :gen\] \ ] ,de lete  : \[cat :noun, agr: 3SG ,poss :NONE, case :ace\],re:  \[\] , r rc :  \ [ \ ] \ ] .3.6 Us ing  context  s ta t i s t i cs  to  de le te  parsesAfter applying hand-crafted rules to a text to be dis-ambiguated we arrive at a state where ambiguity isabout 1.10 to 1.15 parses per token (down from 1.70to 1.80 parses per token) without any serious losson recall.
This state allows statistics to be collectedover unambiguous contexts.
To remove additionalparses which never appear in any unambiguous con-text we use the scoring described above for choosingdelete rules, to discard parses on the current textbased on context statistics} 2 We make three passes12Please note that delete rules learned may be appliedto future texts to be disambiguated, while this step isover the current text, scoring parses in unambigu-ous contexts of the form used in generating deleterules, and discarding parses whose score is below acertain fraction of the maximum scoring parse, onthe fly.
The only difference with the scoring used fordelete rules, is that the score of a parse Pi here is aweighted sum of the quantityincontext(C, Pi)count( Pi )evaluated for three contexts in the case both the l cand rc  are unambiguous/3.7 S teps  in D isamblguat ing  a TextGiven a new text annotated with all morphologicalparses (this time the parses are not projected), weproceed with the following steps for disambiguation:1.
The initial hand-crafted choose rules are appliedfirst.
These rules always constrain top level in-flectional features, and hence, any stems fromnderivational processes are not considered unlessexplicitly indicated in the constraint itself.2.
The hand-crafted elete clean-up rules are ap-plied.3.
Context statistics described in the precedingsection are used to discard further parses.4.
The choose rules that have been learned ear-lier, are then repeatedly applied to unambigu-ous contexts, until no more ambiguity reduc-tion is possible.
During the application of theserules, if the immediate right context of a tokenis a derived form, then the stem of the rightcontext is also checked against the constraintimposed by the rule.
So if the rule right contextconstraint subsumes the top level feature struc-ture or the stem feature structure, then the rulesucceeds and is applied if all other constraintsare also satisfied.5.
Finally, the delete rules that have been learnedare applied repeatedly to unambiguous con-texts, until no more ambiguity reduction is pos-sible.4 Experimental Resu l tsWe have applied our learning system to two Turk-ish texts.
Some statistics on these texts are givenin Table 1.
The first text labeled ARK is a shorttext on near eastern archaeology.
The second textfrom which fragments whose labels start with C arederived, is a book on early 20 ~h history of TurkishRepublic.In Table 1, the tokens considered are that are gen-erated after morphological analysis, unknown wordprocessing and any lexical coalescing is done.
Theapplied to the current text on which disambiguation isperformed.77TextARKC2400C270Sentences4922,407270Tokens0 17,928 0.15% 49.34%39,800 0.03% 50.56%5212 0.02% 50.63%Distr ibut ionofMorphologica l  Parses2 3 4 >430.93% 9.19% 8.46% 1.93%28.66% 10.12% 8.16% 2.47%30.68% 8 .62% 8 .36& 1 .69%Table 1: Statistics on Textswords that are unknown are those that could noteven be processed by the unknown noun proces-sor.
Whenever an unknown word had more than oneparse it was counted under the appropriate group.We learned rules from ARK itself, and on thefirst 500, 1000, and 2000 sentence portions of C2400.C270 which was from the remaining 400 sentences ofC2400 was set aside for testing.
Gold standard is-ambiguated versions for ARK, C270 were preparedmanually to evaluate the automatically tagged ver-sions.Our results are summarized in the following setof tables.
Tables 2 and 3 give the ambiguity, re-call and precision initially, after hand-crafted rulesare applied, and after the contextual statistics areused to remove parses - all applications being cu-mulative.
The rows labeled BASE give the initialstate of the text to be tagged.
The rows labeledINITIAL CHOOSE give the state after hand-craftedchoose rules are applied, while the rows labeled INI-TIAL DELETE give the state after the hand-craftedchoose and delete rules are applied.
The rows la-beled CONTEXT STATISTICS give the state afterthe rules are applied and context statistics are used(as described earlier) to remove additional parses.Disambiguation Ambiguity Recall Pre .Stage (~)  (~)BASE 1.828 100.OO 54.69IN IT IAL  CHOOSE 1.339 99.28 74.13IN IT IAL  DELETE 1.110 99.08 88.91CONTEXT STAT IST ICS  1.032 97.38 94.35Table 2: Average parses, recall and precision for textARKDisambiguationStageAmbiguityBASE 1.719IN IT IAL  CHOOSE 1.353IN IT IAL  DELETE 1.130CONTEXT STAT IST ICS  1.038Reca l l  P re .
(%) (%)100.00 58.1899.16 73.2798.73 87.2496.70 93.15Table 3: Average parses, recall and precision for textC270Tables 5 and 6 present he results of further dis-ambiguation of ARK, and C270 using rules learnedfrom training texts C500, C1000, C2000 and ARK.These rules are applied after the last stage in the ta-bles above.
13 The number of rules learned are givenin Table 4.14Tra in ing  Choose  DeleteText Rules RulesARK 23 89C500 ii 113C1000 29 195C2000 61 245Table 4: Number of choose and delete rules learnedfrom training texts.Disambiguation Ambiguity Reca l l  P re .Stage (%)  (?Z~)Tra in ing Set ARKI I LEARNED DELETE 1.027 97.20 94.63Tra in ing Set C5O0LEARNED DELETE 1.028 97.30 94.61Tra in ing Set C1000I I LEARNED DELETE 1.026 97.18 94.68Tra in ing Set C2000LEARNED CHOOSE 1.028 I 97.24 I 94.60LEARNED DELETE 1.025 97 .1394.71Table 5: Average parses, recall and precision for textARK after applying learned rules.Table 7 gives some additional statistical results atthe sentence level, for each of the test texts.
Thecolumns labeled UA/C and A/C give the numberand percentage of the sentences that are correctlydisambiguated with one parse per token, and withmore than one parse for at least one token, respec-tively.
The columns labeled 1, 2, 3, and >3 denotethe number and percentage of sentences that have1, 2, 3, and >3 tokens, with all remaining parsesincorrect.
It can be seen that well 60% of the sen-tences are correctly morphologically disambiguatedwith very small number of ambiguous parses remain-ing.13Please note for ARK, in the first two rows, the train-ing and the test texts are the same.nLearning iterations have been stopped when themaximum rule score fell below 7.78TextARKC270SentencesTotal UA/C  \[ A /C  C (UA/C+A/C)  I 2 3 >3494 220 (44.53%) 97 (19.6,I%) 317 (64.17%) 133 (26.92%) 41 (8.30%) 3 (0.61%) 0 (0.00%)270 116 (42.96%) 50 (18.52%) 166 (61.48%) 55 (20.37%,) 27 (10.00%) 17 (6.30%) 5 (1.85%)Table 7: Disambiguation results at the sentence level using rules learned from C2000.Disambiguat ion  Stage Ambigu i ty  \[ Recall (%) Pre.
\] (~)Training Set ARKLEARNED CHOOSE 1.035LEARNED DELETE 1.029~ain ingSetLEARNED CHOOSELEARNED DELETE96.64 93.3696.40 93.71C500 I 1.035 96.66 93.321.029 96.40 93.66Training Set C1000LEARNED CHOOSE 1.035LEARNED DELETE 1.029Training Set C20O0LEARNED CHOOSE 1.034LEARNED DELETE 1.03096.66  93 .3496.42 93.6496 .64  \] 93.4196.52 93.70Table 6: Average parses, recall and precision for text270 after applying learned rules.4.1 D iscuss ion  of  Resu l tsWe can make a number of observations from ourexperience: Hand-crafted rules go a long way in im-proving precision substantially, but in a languagelike Turkish, one has to code rules that allow no, oronly carefully controlled derivations, otherwise lotsof things go massively wrong.
Thus we have usedvery tight and conservative rules in hand-crafting.Although the additional impact of choose and rulesthat are induced by the unsupervised learning is notsubstantiM, this is to be expected as the stage atwhich they are used is when all the "easy" workhas been done and the more notorious cases re-main.
An important class of rules we explicitly haveavoided hand crafting are rules for disambiguatingaround coordinating conjunctions.
We have notedthat while learning choose rules, the system zeroesin rather quickly on these contexts and comes upwith rather successful rules for conjunctions.
Simi-larly, the delete rules find some interesting situationswhich would be virtually impossible to enumerate.Although it is easy to formulate what things can gotogether in a context, it is rather impossible to for-mulate what things can not go together.We have also attempted to learn rules directlywithout applying any hand-crafted rules, but thishas resulted in a failure with the learning processgetting stuck fairly early.
This is mainly due to thelack of sufficient unambiguous contexts to bootstrapthe whole disambiguation process.From analysis of our results we have noted thattrying to choose one correct parse for every token israther ambitious (at least for Turkish).
There are anumber of reasons for this:There are genuine ambiguities.
The word o iseither a personal or a demonstrative pronoun(in addition to being a determiner).
One simplycan not choose among the first two using anyamount of contextual information.A given word may be interpreted in more thanone way but with the same inflectional features,or with features not inconsistent with the syn-tactic context.
This usually happens when theroot of one of the forms is a proper prefix ofthe root of the other one.
One would need se-rious amounts of semantic, or statistical rootword and word form preference information forresolving these.
For instance, inkoyun sfirfisfikoyun sfirfi+sfisheep herd+POSS-3SG(sheep herd)koy+un siirfi+sfibay+GEN herd+POSS-3SG(??
bay's herd)both noun phrases are syntactically possible,though the second one is obviously nonsense.It is not clear how one would disambiguate hisusing just contextual or syntactic information.Another similar example is:kurmaya yardlm ettikur+ma+ya yardlm et+ticonstruct+INF+DAT help make+PASThelped construct (something)kurmay+a yard~m et+timilit ary-officer+DAT help make+PASThelped the military-officerwhere again with have a similar problem.
Itmay be possible to resolve this one using sub-categorization constraints on the object of theverb kur assuming it is in the very near preced-ing context, but this may be very unlikely asTurkish allows arbitrary adjuncts between theobject and the verb.Turkish allows sentences to consist of a numberof sentences separated by commas.
Hence locat-ing a verb in the middle of a sentence is ratherdifficult, as certain verbal forms also have anadjectival reading, and punctuation is not veryhelpful as commas have many other uses.The distance between two constituents (of, say,a noun phrase) that have to agree in vari-ous morphosyntactic features may be arbitrar-79ily long and this causes occasional mislnatches,especially if the right nominal constituent hasa surface plural marker which causes a 4-wayambiguity, as in masalam.masalarII.
\[\[CAT NOUN\] \[ROOT masa\] \[AGR 3PL\]\[POSS NONE\] \[CASE ACC\]\](tables accusative)2.
\[\[CAT NOUN\] \[ROOT masa\] \[AGR 3PL\]\[POSS 3SG\] \[CASE NOM\]\](his tables)3.
\[\[CAT NOUN\] \[ROOT masa\]\[POSS 3PL\] \[CASE NON\]\](their tables)\[AGR 3PL\]4.
\[\[CAT NOUN\] \[ROOT masa\] \[AGR 3SG\]\[POSS 3PL\] \[CASE NOM\]\](their table)Choosing among the last three is rather prob-lematic if the corresponding enitive form toforce agreement with is outside the context.Among these problems, the most crucial is thesecond one which we believe can be solved to a greatextent by using root word preference statistics andword form preference statistics.
We are currentlyworking on obtaining such statistics.5 Conc lus ionsThis paper has presented a rule-based morphologi-cal disambiguation approach which combines a set ofhand-crafted constraint rules and learns additionalrules to choose and delete parses, from untagged textin an unsupervised manner.
We have extended therule learning and application schemes so that theimpact of various morphological phenomena and fea-tures are selectively taken into account.
We have ap-plied our approach to the morphological disambigua-tion of Turkish, a free-constituent order language,with agglutinative morphology, exhibiting produc-tive inflectional and derivational processes.
We havealso incorporated a rather sophisticated unknownform processor which extracts any relevant inflec-tional or derivational markers even if the root wordis unknown.Our results indicate that by combining thesehand-crafted, statistical and learned informationsources, we can attain a recall of 96 to 97% witha corresponding precision of 93 to 94% and ambigu-ity of 1.02 to 1.03 parses per token, on test texts,however the impact of the rules that are learned isnot significant as hand-crafted rules do most of theeasy work at the initial stages.6 AcknowledgmentsWe would like to thank Xerox Advanced DocumentSystems, and Lauri Karttunen of Xerox Pare andof Rank Xerox Research Centre (Grenoble) for pro-viding us with the two-level transducer developmentsoftware on which the morphological and unknownword recognizer were implemented.
This researchhas been supported in part by a NATO Science forStability Grant TU-LANGUAGE.ReferencesEric Brill.
1992.
A simple-rule based part-of-speechtagger.
In Proceedings of the Third Conferenceon Applied Natural Language Processing, Trento,Italy.Eric Brill.
1994.
Some advances in rule-based part ofspeech tagging.
In Proceedings of the Twelfth Na-tional Conference on Articial Intelligence (AAAI-9~), Seattle, Washinton.Eric Brill.
1995a.
Transformation-based rror-driven learning and natural language processing:A case study in part-of-speech tagging.
Computa-tional Linguistics, 21(4):543-566, December.Eric Brill.
1995b.
Unsupervised learning of dis-ambiguation rules for part of speech tagging.
InProceedings of the Third Workshop on Very LargeCorpora, Cambridge, MA, June.Kenneth W. Church.
1988.
A stochastic parts pro-gram and a noun phrase parser for unrestrictedtext.
In Proceedings of the Second Conferenceon Applied Natural Language Processing, Austin,Texas.Doug Cutting, Julian Kupiec, Jan Pedersen, andPenelope Sibun.
1992.
A practical part-of-speechtagger.
In Proceedings of the Third Conferenceon Applied Natural Language Processing, Trento,Italy.Steven J. DeRose.
1988.
Grammatical  category dis-ambiguation by statistical optimization.
Compu-tational Linguistics, 14(1):31-39.Zelal GiingSrdii and Kemal Oflazer.
1995.
Pars-ing Turkish using the Lexical-Functional Gram-mar formalism.
Machine Translation, 11(4):293-319.Jorge Hankamer.
1989.
Morphological parsing andthe lexicon.
In W. Marslen-Wilson, editor, LexicalRepresentation and Process.
MIT Press.Fred Karlsson, Atro Voutilainen, Juha HeikkilS,and Arto Anttila.
1995.
Constraint Grammar-ALanguage-Independent System for Parsing Unre-stricted Text.
Mouton de Gruyter.Lauri Karttunen.
1993.
Finite-state lexicon com-piler.
XEROX, Palo Alto Research Center- Tech-nical Report, April.80ilker Kuru6z.
1994.
Tagging and morphologicaldisambiguation of Turkish text.
Master's thesis,Bilkent University, Department of Computer En-gineering and Information Science, July.Moshe Levinger, Uzzi Ornan, and Alon Itai.
1995.Learning morpho-lexical probabilities fi'om anuntagged corpus with an application to He-brew.
Computational Linguistics, 21(3):383-404,September.Kemal Oflazer and ilker KuruSz.
1994.
Tagging andmorphological disambiguation f Turkish text.
InProceedings of the 4 th Applied Natural LanguageProcessing Conference, pages 144-149.
ACL, Oc-tober.Kemal Oflazer.
1993.
Two-level description of Turk-ish morphology.
In Proceedings of the Sixth Con-ference of the European Chapter of the Associa-tion for Computational Linguistics, April.
A fullversion appears in Literary and Linguistic Com-puting, Vol.9 No.2, 1994.Atro Voutilainen and Pasi Tapanainen.
1993.
Am-biguity resolution in a reductionistic parser.
InProceedings of EACL'93, Utrecht, Holland.Atro Voutilainen, auha Heikkilg, and Arto Anttila.1992.
Constraint Grammar of English.
Universityof ttelsinki.Atro Voutilainen.
1995a.
Morphological disam-biguation.
In Fred Karlsson, Atro Voutilainen,Juha Heikkilg, and Arto Anttila, editors, Con-straint Grammar-A Language-Independent Sys-tem for Parsing Unrestricted Tea:t, chapter 5.Mouton de Gruyter.Atro Voutilainen.
1995b.
A syntax-based part-of-speech analyzer, In Proceedings of the SeventhConference of the European Chapter of the Asso-ciation of Computational Linguistics, Dublin, Ire-land.81
