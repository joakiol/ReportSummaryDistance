Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 227?231,Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational LinguisticsDialog System Using Real-Time Crowdsourcingand Twitter Large-Scale CorpusFumihiro Bessho, Tatsuya Harada, Yasuo KuniyoshiThe University of TokyoDepartment of Mechano-Informatics7-3-1 Hongo, Bunkyo-ku, Tokyo 113-8656, Japan{bessho, harada, kuniyosh}@isi.imi.i.u-tokyo.ac.jpAbstractWe propose a dialog system that creates re-sponses based on a large-scale dialog corpusretrieved from Twitter and real-time crowd-sourcing.
Instead of using complex dialogmanagement, our system replies with the ut-terance from the database that is most simi-lar to the user input.
We also propose a real-time crowdsourcing framework for handlingthe case in which there is no adequate re-sponse in the database.1 IntroductionThere is a lot of language data on the Internet.
Twit-ter offers many APIs to retrieve or search post sta-tus data, and this data is frequently used in research,such as in stock market prediction (Bollen et al,2011), the spread of information through social me-dia (Bakshy and Hofman, 2011), and representationsof textual content(Ramage et al, 2010).
Severalmodels for conversation using Twitter data (Ritter etal., 2010; Higashinaka et al, 2011) have been pro-posed because of the data?s vast size and conversa-tional nature.Kelly (2009) previously showed that 37% of En-glish tweets are Conversational, of which 69% aretwo-length (one status post and a reply).
In our anal-ysis of over 2.5 million tweets, 37.5% of all Japanesetweets are Conversational, which matches Kelly?sdata.
However, less than 58.3% of these are two-length tweets.Many chat bots are rule-based, which requires alot of human effort to create or add new rules.
Forexample, A.L.I.C.E (Wallace, 2009), which won theA BUUtterance PairStatus Post ReplyUser UtteranceCalculation SimilarityIt was cold yesterday.
Yeah, it was freezing.Figure 1: Utterance pair.Loebner Prize three times, creates responses basedon a dialog strategy database written in a markuplanguage named AIML.
Recently, some other chatbots based on a large-scale dialog corpus have beenproposed1,2.In this paper, we propose a novel dialog sys-tem (chat bot) that uses real-time crowdsourcing andTwitter large-scale corpus.
We evaluate response se-lection methods based on positive/negative exampleto judge if each feature could be exploited to judgesimilarity between uterrances.2 Method2.1 OverviewWe create an ?Utterance Pair?
database as shown inFigure 1.
Each pair is composed of an utterance(Figure 1, A) and a reply to the utterance (Figure1, B).
Our approach for creating responses is simpleand is illustrated in Figure 2.
For each user input, thesystem searches the utterance-pair database for thepair of which the tweet (Figure 1, A) is most similarto that input.
The reply contained in this pair (Figure1, B) forms the system?s response to the user?s input.1Jabberwacky: http://www.jabberwacky.com2Cleverbot: http://www.cleverbot.com227UserUser InputSystemResponseWebCrowdUtterance PairCollectingModuleUtterance PairCorpusResponseSelectionModuleFigure 2: System overview.If the system cannot find a post that is sufficientlysimilar to the user?s input, then it ?outsources?
theresponse to another user.To build the conversation database, we collected1.2 million utterance-pairs from the microbloggingservice, Twitter.
We fetched public timeline datausing the Streaming API 3 , and then looked fortweets which were written in Japanese 4 and had anin-reply-to field.
We followed the replies using theREST API 5.Raw posts from Twitter included mentions (thesymbol @ followed by a user name), quotes (writ-ten with letters ?RT?
), hashtags (a word preceded bythe symbol #), and URLs.
We filtered away this in-formation using regular expressions.
Unlike Englishtweets, in Japanese users placed hashtags at the endof their tweet, separated from the bodytext, makingthe deletion of hashtags feasible.2.2 Method for the retrieval of SimilarUtterance-pairsIn this section, we define a similarity measure be-tween user input and each utterance data in thedatabase.
Each utterance in the database is analyzedby a morphological analyzer after Twitter-specificrepresentations are eliminated as mentioned in Sec-tion 2.1.
Analyzed data are filtered based on part-of-speech (POS) tags.
In this paper we only ex-tract noun, verb and interjection, and because manyJapanese tweets include emoticons which cannot3https://dev.twitter.com/docs/streaming-api4We assume tweets as Japanese-written which are written byusers who set Language as Japanese.5https://dev.twitter.com/docs/apibe tagged correctly by the morpological analyzerwe used.
We filtered out emoticons using a key-character-filter.These documents (tweets) were then convertedinto document vectors.
For a document di, the vec-tor element corresponding to word wj is representedasxi,j =tf i,jnj , (1)where tf i,j represents the number of times wj ap-pears in di (term frequency), and nj represents thelength of di.The similarity between two documents is calcu-lated by taking the inner product of the two docu-ment vectors, that isSimilarity(da, db) = xTaxb.
(2)2.3 Real-Time CrowdsourcingWe propose to integrate the dialog system with?real-time crowdsourcing?.
When the system failsto find an adequate response to a user input, in otherwords, when the similarity score of the most similartweet in the database is below a certain threshold,the system relegates the user?s input to other users(crowd).
The original user input is a tweet to the chatbot and therefore includes the system?s name as thetarget user.
In our experiment, the system exchangesits own name to the address of the crowd and uttersthe tweet to the crowd.
If a crowd member respondsto the system before a preset timeout period, the sys-tem uses the crowd member?s reply as a responseto the original user.
One of the advantages of thismethod is that people in the crowd do not know thatthey are part of a crowd; instead, they think they arebeing addressed by the system.
The original useralso thinks (s)he is talking with the system.
We im-plemented this real-time crowdosourcing frameworkusing a Twitter clone, StatusNet6 as an interface (seeFigure 5).3 EvaluationWe prepared 90 user input examples and extracted20 utterance-pairs (utterance and responses in thedatabase retrieved from Twitter) per user input, sothat a total of 1,800 of triples (a user input and an6http://status.net/228utterance pair) were included in our sample.
Thirtysubjects evaluated the naturalness and versatility ofthe responses.
Each subject evaluated 600 triples.We note that subjects saw only the user input andresponse in an utterance pair (B in Figure 1), andwere not shown A in Figure 1 in the survey sheets.In this paper, versatility of a response correspondsto the number of utterances to which it was rated assensible response (e.g., ?What do you mean??
canserve as a response to almost any input, and is there-fore highly versatile).
Michael (1994) points out thatchat bots have many tricks to fool people, and pro-viding a versatile answer is one of them.
We believeour system can avoids versatile answers by using alarge-scale database.In the following, we describe how we evalu-ate each scoring function (which takes a triplet asan input and the score as the output) using posi-tive/negative learning data.
We treat scoring func-tions as classifiers, that is, when the function re-ceives a triplet as input, we assume that the functionjudges the triplet as positive data if the output scoreis above a certain threshold and negative data if it isbelow it.Triplets collected in the survey were divided intopositive and negative triplets.
We consider an utter-ance pair to be positive if it is judged as natural bymore than 7 out of 10 subjects and versatile by lessthan 7 out of 10 subjects.
All else were considerednegative triplets.The ROC curve is used to illustrate the perfor-mance of the classifiers.
It is a two-dimensionalgraph in which true positive rate is plotted on the Yaxis and false positive rate is plotted on the X axis.Here, the true positive rate (rTP ) and false positiverate (rFP ) are given byrTP = Positives correctly classifiedTotal positives , (3)rFP = Negatives incorrectly classifiedTotal negatives .
(4)The area under the curve (AUC), or the area underthe ROC curve, was used to measure classifier per-formance.
A random classifier has an AUC of 0.5,and ideal classifier has an AUC of 1.0.
We applied anumber of scoring functions to the triples, and thencalculated the AUC for each function (classifier) forvalidation.
We chose scoring functions whichCalculate similarity with A or A+B.
A+BUse tf?
YESUse idf?
NOEliminate Twitter-specific representations?
YESFilter POS?
YESTable 1: Scoring function we chose.?
calculate similarity only with A in Figure 1, orA and B in Figure 1,?
use term frequency (tf) when the documentvector is calculated, or not,?
use inverse document frequency (idf) when thedocument vector is calculated, or not,?
eliminate Twitter-specific representations (seeSection 2.1) or not,?
normalize by character count or not,?
filter POS or not.We compared a total of 64 (=26) scoring functions.Figure 3 illustrates some or our results.
As it shows,when only Twitter-specific expressions are filtered,classifier performance is similar to a random classi-fier.
The addition of word count normalization andPOS filter improved to classification performance.This is because longer utterances normally includemore specific information, so that the topic is moreprone to be missed during the response selectionprocess.
Adverbs (e.g.
?very?)
or particles (corre-sponds preposition in English, e.g.
?as?)
had littleeffect on the context of an utterance, so POS filter-ing acts as noise elimination.
With respect to tf andidf, the effect of tf varied widely, and idf hinderedclassification performance (c, d, g, h).We chose the scoring function with the best per-formance (see Table 1 for details), of which the AUCis 0.803.4 Conclusions and Future WorkIn this paper, we proposed a new dialog systembased on real-time crowdsourcing and a large-scaledatabase which is collected from the web automati-cally.
We also evaluated scoring functions based onpositive/negative utterance pairs.In future work, we will keep on enlarging our ut-terance pair corpus, and conduct the same experi-229(a) normal (A) (b) eliminated (A) (c) eliminated +normalized (A)(d) eliminated +normalized + POS filter(A)(e) normal (A+B) (f) eliminated (A+B) (g) eliminated +normalized (A+B)(h) eliminated +normalized + POS filter(A+B)Figure 3: ROC curve for each scoring function.
In each graph there are 4 lines, and each line represents whether tfand idf are used to calculate the document vector.
Only A in Figure 1 is treated in the first line (a, b, c, d), whereasA and B is considered in the bottom (e, f, g, h).
Normal, eliminated, normalized, POS filter mean doing nothing,twitter-specific description is eliminated, normalized by character count, considering only specified POS, respectively.ments as in this paper on the larger database.
Wewill also use more than two-length tweets for thedatabase.
We believe that this approach could leadto a context-aware dialog system (e.g., suppose theuser gives ?Why??
as the response to the system?sutterance ?I like dogs.?.
Our current system cannothandle the topic, ?dog?, however a system can do soby considering 3 utterances).ReferencesEytan Bakshy and Jake M. Hofman.
2011.
Every-one?s an Influencer: Quantifying Influence on Twitter.In Proceedings of International Conference on WebSearch and Data Mining.Johan Bollen, Huina Mao, and Xiaojun Zeng.
2011.Twitter Mood Predicts the Stock Market.
Journal ofComputational Science.Ryuichiro Higashinaka, Noriaki Kawamae, KugatsuSadamitsu, and Yasuhiro Minami.
2011.
Building aConversational Model from Two-Tweets.
IEEE Work-shop on Automatic Speech Recognition and Under-standing.Ryan Kelly.
2009.
Pear Analytics Twitter Study(Whitepaper).Michael L. Mauldin.
1994.
Chatterbots, TinyMuds, andthe Turing Test Entering the Loebner Prize Competi-tion.
In Proceedings of the National Conference onArtificial Intelligence.Daniel Ramage, Susan Dumais, and Dan Liebling.
2010.Characterizing Microblogs with Topic Models.
In Pro-ceedings of the International AAAI Conference onWeblogs and Social Media.Alan Ritter, Colin Cherry, and Bill Dolan.
2010.
Unsu-pervised Modeling of Twitter Conversations.
In Pro-ceedings of North American Chapter of the Associa-tion for Computational Linguistics - Human LanguageTechnologies.Richard S. Wallace.
2009.
The Anatomy of A.L.I.C.E..Parsing the Turing Test.230USER ?????
(konnitiha: Hello)SYSTEM ?????????????????????
(konnitiha!!
konnnitiha!!
konnnitiha!!
: Hello!!
Hello!!Hello!!
)USER ??????????
(kyou-mo-samui-desu-yone: It?s cold today, isn?t it?
)SYSTEM ????????????????????????????
(da, daijobu-zero?
uwagi-wo-haotte,danbou-wo-tuketemitari-sita?
: Are you ok?
How about putting on a jacket, and turning on the heater?
))USER ????????(danbou-tuketeru-yo.
: I?ve already turned on the heater.
)SYSTEM ???????????
(?????)
(danbou-nai-kedo-attakai <emoticon>: There is no heater(inthis room), but it?s warm <emoticon>)USER ?????????
(nandaka-netuppoi: I feel feverish.
)SYSTEM ????
(daijoubu?
: Are you ok?
)USER ????????????????????
(Infuruenza-ni-kannsenn-shitenai-to-iikedo.
: I hope I?mnot ill with flu.
)SYSTEM ????????????????????
48????????????????
(infuruenza-you-no-kouuiruzuzai-ha-kannsenn-kara-48jikanninai-ni-shika-amari-kouka-ga-nai-wana: Antiviral drug forflu only works to those who got infected in 48 hours.
)USER ????(majika.
: really?
)SYSTEM ????
(neroyo!
: You should sleep.
)USER ?????????(otonashiku-nemasu.
: I?ll take it easy and sleep.
)SYSTEM ??????????
(otonashiku-suru-beki-da: You should do so.
)Table 2: Dialog examplesORMClientConnectionMechanism(TCP)ScoringModuleCreate Inverted IndexDocumentVectorsDocument Dictionary Keyed by WordLoad to MemoryPreprocessDocumentVectorsUtterancePairUtterance Pair(Snapshot)Utterance PairCollectingModuleDocumentVectorPOS FilterDocument AnalyzerData CollectorResponse SelectorMorphologialAnalyzerWordDictionaryWordDictionary(Cache)DocumentUser InputSystem Response&ScoreDatabase (Indexed)Database (NOT Indexed)Main MemoryFigure 4: System Implementation.
Figure 5: System implementation on StatusNet.Figure 6: System response time distribution.
(datasize =1,154,621)231
