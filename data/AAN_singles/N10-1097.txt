Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 673?676,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsMaking Conversational Structure Explicit:Identification of Initiation-response Pairs within Online DiscussionsYi-Chia Wang Carolyn P. Ros?Language Technologies Institute  Language Technologies InstituteCarnegie Mellon University Carnegie Mellon UniversityPittsburgh, PA 15213, USA Pittsburgh, PA 15213, USAyichiaw@cs.cmu.edu cprose@cs.cmu.eduAbstractIn this paper we investigate how to identifyinitiation-response pairs in asynchronous,multi-threaded, multi-party conversations.We formulate the task of identifying initia-tion-response pairs as a pairwise rankingproblem.
A novel variant of Latent SemanticAnalysis (LSA) is proposed to overcome a li-mitation of standard LSA models, namely thatuncommon words, which are critical for sig-naling initiation-response links, tend to bedeemphasized as it is the more frequent termsthat end up closer to the latent factors selectedthrough singular value decomposition.
Wepresent experimental results demonstratingsignificantly better performance of the novelvariant of LSA over standard LSA.1 IntroductionIn recent years, research in the analysis of socialmedia (e.g., weblogs, discussion boards, and mes-sengers) has grown in popularity.
Unlike exposito-ry text, the data produced through use of socialmedia is often conversational, multi-threaded, andmore complex because of the involvement of nu-merous participants who are distributed both acrosstime and across space.
Recovering the multi-threaded structure is an active area of research.In this paper, we form the foundation for abroader study of this type of data by investigatingthe basic unit of interaction, referred to as an initi-ation-response pair (Schegloff, 2007).
Initiation-response pairs are pairs of utterances that are typi-cally contributed by different participants, andwhere the first pair part sets up an expectation forthe second pair part.
Types of common initiation-response pairs include question-answer, assess-ment-agreement, blame-denial, etc.
Note that al-though sometimes discussion forum interfacesmake the thread structure of the interaction expli-cit, these affordances are not always present.
Andeven in forums that have these affordances, theapparent structure of the discourse as representedthrough the interface may not capture all of thecontingencies between contributions in the unfold-ing conversation.
Thus, the goal of this investiga-tion is to investigate approaches for automaticallyidentifying initiation-response pairs in conversa-tions.One of the challenges in identifying initiation-response pairs is that the related messages are notnecessarily adjacent to each other in the stream ofcontributed messages, especially within the asyn-chronous environment of social media.
Further-more, individual differences related to writing styleor creative expression of self may also complicatethe identification of the intended connections be-tween contributions.
Identification of initiation-response pairs is an important step towards auto-matic processing of conversational data.
One po-tential application of this work is conversationsummarization.
A summary should include boththe initiation and response as a coherent unit or itmay fail to capture the intended meaning.We formulate the task of identifying initiation-response pairs as a pairwise ranking problem.
Thegoal is to distinguish message pairs that constitutean initiation-response pair from those that do not.We believe a ranking approach, where the degreeof relatedness between a message pair can be con-sidered in light of the relatedness between each ofthem and the surrounding messages within thesame thread, is a more suitable paradigm for thistask than a discrete classification-based paradigm.Previous work on recovering conversationalstructure has relied on simple lexical cohesion673measures (i.e., cosine similarity), temporal infor-mation (Lewis and Knowles, 1997; Wang et al,2008), and meta-data (Minkov et al, 2006).
How-ever, relatively little work has investigated the im-portance of specifically in-focus connectionsbetween initiation-response pairs and utilized themas clues for the task.
Consider, for example, thefollowing excerpt discussing whether congressshould pass a bill requiring the use of smaller carsto save the environment:a) Regressing to smaller vehicles would discouragebusiness from producing more pollution.b) If CO2 emissions are lowered, wouldn't tax revenuesbe lowered as well?
Are the democrats going to wil-lingly give up Medicaid and social security?Although segment (b) is a reply to segment (a), theamount of word overlap is minimal.
Nonetheless,we can determine that (b) is a response to (a) byrecognizing the in-focus connections, such as "ve-hicles-CO2" and "pollution-CO2."
To properlyaccount for connections between initiations andresponses, we introduce a novel variant of LatentSemantic Analysis (LSA) into our ranking model.In section 2, we describe the Usenet data andhow we extract a large corpus of initiation-response pairs from it.
Section 3 explains our rank-ing model as well as the proposed novel LSA vari-ation.
The experimental results and discussion aredetailed in Section 4 and Section 5, respectively.2 Usenet and Generation of DataThe experiment for this paper was conducted usingdata crawled from the alt.politics.usa Usenet (UserNetwork) discussion forum, including all postsfrom the period between June 2003 and June 2008.The resulting set contains 784,708 posts.
The postsin this dataset alo contain meta-data that makesparent-child relationships explicit (i.e., through theReferences field).
Thus, we know 625,116 of theposts are explicit responses to others posts.
Themessages are organized into a total of 77,985 dis-cussion threads, each of which has 2 or more posts.In order to evaluate the quality of using the ex-plicit reply structure as our gold standard for initia-tion-response links, we asked human judges toannotate the response structure of a random-selected medium-length discussion (19 posts)where we had removed the meta-data that indi-cated the initiation-reply structure.
The resultshows the accuracy of our gold standard is 0.89.To set up the data as a pairwise ranking prob-lem, we arranged the posts in the corpus into in-stances containing three messages each, one ofwhich is a response message, one of which is theactual initiating message, and the other of which isa foil selected from the same thread.
The idea isthat the ranking model will be trained to prefer theactual initiating message in contrast to the foil.The grain size of our examples is finer thanwhole messages.
More specifically, positive exam-ples are pairs of spans of text that have an initia-tion-reply relationship.
We began the process withpairs of messages where the meta-data indicatesthat an initiation-reply relationship exits, but wedidn?t stop there.
For our task it is important tonarrow down to the specific spans of text that havethe initiation-response relation.
For this, we usedthe indication of quoted material within a message.We observed that when users explicitly quote aportion of a previously posted message, the portionof text immediately following the quoted materialtends to have an explicit discourse connection withit.
Consider the following example:>> Why is the quality of life of the child, mother,>> and society at large, more important than the>> sanctity of life?> Because in the case of anencephaly at least,> the life is ended before it begins.We disagree on this point.
Why do you refuse toprovide your very own positive definition of life?Do you believe life begins before birth?
At birth?After birth?
Never?In this thread, the reply expresses an opinionagainst the first level quote, but not the second lev-el quote.
Thus, we used segments of text with sin-gle quotes as an initiation and the immediatelyfollowing non-quoted text as the response.
We ex-tracted positive examples by scanning each post tolocate the first level quote that is immediately fol-lowed by unquoted content.
If such quoted materialwas found, the quoted material and the unquotedresponse were both extracted to form a positiveexample.
Otherwise, the message was discarded.For each post P where we extracted a positiveexample, we also extracted a negative example bypicking a random post R from the same thread asP.
We selected the negative example in such a wayto make the task difficult in a realistic way.
Choos-ing R from other threads would make the task tooeasy because the topics of P and R would mostlikely be different.
We also stipulated that R cannotbe the parent, grandparent, sibling, or child of P.674Together the non-quoted text of P and R forms anegative instance.
Thus, the final dataset consistsof pairs of message pairs ((pi, pj), (pi, pk)), wherethey have the same reply message pi, and pj is thecorrect quote message of pi, but pk is not.
In otherwords, (pi, pj) is considered as a positive example;(pi, pk) is a negative example.
We constructed atotal of 100,028 instances for our dataset, 10,000(~10%) of which were used for testing, and 90,028(~90%) of which were the learning set used to con-struct the LSA space described in the next section.3 Ranking Models for Identification ofInitiation-Response PairsOur pairwise ranking model1 takes as input an or-dered pair of message pairs ((pi, pj), (pi, pk)) andcomputes their relatedness using a similarity func-tion sim.
Specifically,( xij, xik ) = ( sim (pi, pj), sim (pi, pk) )where xij is the similarity value between post pi andpj; xik is the similarity value between post pi and pk.To determine which of the two message pairs rankshigher regarding initiation-response relatedness,we use the following scoring function to comparetheir corresponding similarity values:score (xij, xik) = xij ?
xikIf the score is positive, the model ranks (pi, pj)higher than (pi, pk) and vice versa.
A message pairranked higher means it has more evidence of beingan initiation-reply link, compared to the other pair.3.1 Alternative Similarity FunctionsWe introduce and motivate 3 alternative similarityfunctions, where the first two are considered asbaseline approaches and the third one is a novelvariation of LSA.
We argue that the proposed LSAvariation is an appropriate semantic similaritymeasurement for identifying topic continuation andinitiation-reply pairs in online discussions.Cosine Similarity (cossim).
We choose an ap-proach that uses only lexical cohesion as our base-line.
Previous work (Lewis and Knowles, 1997;Wang et al, 2008) has verified its usefulness forthe thread identification task.
In this case,1 We cast the problem as a pairwise ranking problem in orderto focus specifically on the issue of characterizing how initia-tion-response links are encoded in language through lexicalchoice.
Note that once trained, pairwise ranking models canbe used to rank multiple instances.sim(pi,pj) = cossim(pi,pj)where cossim(pi,pj) computes the cosine of the an-gle between two posts pi and pj while they arerepresented as term vectors.LSA Average Similarity (lsaavg).
LSA is a well-known method for grouping semantically relatedwords (Landauer et al, 1998).
It represents wordmeanings in a concept space with dimensionality k.Before we describe how to compute average simi-larity given an LSA space, we explain how theLSA space was constructed in our work.
First, weconstruct a term-by-document matrix, where weuse the 90,028 message learning set mentioned atthe end of Section 2.
Next, LSA applies singularvalue decomposition to the matrix, and reduces thedimensionality of the feature space to a k dimen-sional concept space.
This generated LSA space isused by both lsaavg and lsacart later.For lsaavg, we follow Foltz et al (1998):The meaning of each post is represented as a vec-tor in the LSA space by averaging across the LSArepresentations for each of its words.
The similari-ty between the two posts is then determined bycomputing the cosine value of their LSA vectors.This is the typical method for using LSA in textsimilarity comparisons.
However, note that not allwords carry equal weight within the vector thatresults from this averaging process.
Words that arecloser to the "semantic prototypes" represented byeach of the k dimensions of the reduced vectorspace will have vectors with longer lengths thanwords that are less prototypical.
Thus, those wordsthat are closer to those prototypes will have a larg-er effect on the direction of the resulting vector andtherefore on the comparison with other texts.
Animportant consideration is whether this is a desira-ble effect.
It would lead to deemphasizing thoseunusual types of information that might be beingdiscussed as part of a post.
However, one mightexpect that those things that are unusual types ofinformation might actually be more likely to be thein-focus information within an initiation that res-ponses may be likely to refer to.
In that case, forour purposes, we would not expect this typical me-thod for applying LSA to work well.LSA Cartesian Similarity (lsacart).
To properlyaccount for connections between initiations and( ) ( )????????????==???
?jptbiptajiji ptptpplsaavgppsim jbia ,cos,,675responses that include unusual words, we introducethe following similarity function:where we take the mean of the cosine values for allthe word pairs in the Cartesian product of posts piand pj.
Note that in this formulation, all words havean equal chance to affect the overall similarity be-tween vectors since it is the angle represented byeach word in a pair that comes to play when cosinedistance is applied to a word pair.
Length is nolonger a factor.
Moreover, the averaging is acrosscosine similarity scores rather than LSA vectors.4 Experimental ResultsThe results are found in Table 1.
For comparison,we also report the random baseline (0.50).RandomBaselineCos-SimilarityLSA-AverageLSA-CartAccuracy 0.50 0.66 0.60 0.71Table 1.
Overview of resultsBesides the random baseline, LSA-Average per-forms the worst (0.60), with simple Cosine similar-ity (0.66) in the middle, and LSA-Cart (0.71) thebest, with each of the pairwise contrasts being sta-tistically significant.
We believe the reason whyLSA-Average performs so poorly on this task isprecisely because, as discussed in last section, itdeemphasizes those words that contribute the mostunusual content.
LSA-Cart addresses this issue.To further understand this effect, we conductedan error analysis.
We divided the instances into 4sets based on the lexical cohesion between the re-sponse and the true initiation and between the re-sponse and the foil, by taking the median split onthe distributions of these two cohesion scores.
Ourfinding is that model performances vary by subset.In particular, we find that it is only in cases wherethe positive example has low lexical cohesion (e.g.our "vehicles-CO2" and "pollution-CO2" examplefrom the earlier section), that we see the benefit ofthe LSA-Cart approach.
In other cases, where thecohesion between the reply and the true initiationis high, Cos-Similarity performs best.5 Discussion and ConclusionWe have argued why the task of detecting initia-tion-response pairs in multi-party discussions isimportant and challenging.
We proposed a methodfor acquiring a large corpus for use to identify init-iation-response pairs.
In our experiments, we haveshown that the ranking model using a variant ofLSA performs best, which affirms our hypothesisthat unusual information and uncommon wordstends to be the focus of ongoing discussions andtherefore to be the key in identifying initiation-response links.In future work, we plan to further investigate theconnection between an initiation-response pairsfrom multiple dimensions, such as topical cohe-rence, semantic relatedness, conversation acts, etc.One important current direction is to develop aricher operationalization of the interaction that ac-counts for the way posts sometimes respond to auser, a collection of users, or a user?s posting histo-ry, rather than specific posts per se.AcknowledgmentsWe thank Mary McGlohon for sharing her datawith us.
This research was funded through NSFgrant DRL-0835426.ReferencesDavid D. Lewis and Kimberly A. Knowles.
1997.Threading electronic mail: A preliminary study.
In-formation Processing and Management, 33(2), 209?217.Einat Minkov, William W. Cohen, Andrew Y. Ng.2006.
Contextual Search and Name Disambiguationin Email using Graphs.
In Proceedings of the Inter-national ACM Conference on Research and Devel-opment in Information Retrieval (SIGIR), pages 35?42.
ACM Press, 2006.Peter W. Foltz, Walter Kintsch, Thomas K. Landauer.1998.
Textual coherence using latent semantic analy-sis.
Discourse Processes, 25, 285?307.Thomas K. Landauer, Peter W. Foltz, and Darrell La-ham.
1998.
Introduction to latent semantic analysis.Discourse Processes, 25, 259-284.Schegloff, E. 2007.
Sequence Organization in Interac-tion: A Primer in Conversation Analysis, CambridgeUniversity Press.Yi-Chia Wang, Mahesh Joshi, William W. Cohen, Ca-rolyn P. Ros?.
2008.
Recovering Implicit ThreadStructure in Newsgroup Style Conversations.
In Pro-ceedings of the 2nd International Conference onWeblogs and Social Media (ICWSM II), Seattle,USA.
( ) ( )( )jippttbajiji ppttpplsacartppsim jiba,cos,,),(??
?==676
