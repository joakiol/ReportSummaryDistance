Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1522?1531,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsKnowledge-rich Word Sense Disambiguation Rivaling Supervised SystemsSimone Paolo PonzettoDepartment of Computational LinguisticsHeidelberg Universityponzetto@cl.uni-heidelberg.deRoberto NavigliDipartimento di InformaticaSapienza Universita` di Romanavigli@di.uniroma1.itAbstractOne of the main obstacles to high-performance Word Sense Disambigua-tion (WSD) is the knowledge acquisi-tion bottleneck.
In this paper, we presenta methodology to automatically extendWordNet with large amounts of seman-tic relations from an encyclopedic re-source, namely Wikipedia.
We showthat, when provided with a vast amountof high-quality semantic relations, sim-ple knowledge-lean disambiguation algo-rithms compete with state-of-the-art su-pervisedWSD systems in a coarse-grainedall-words setting and outperform them ongold-standard domain-specific datasets.1 IntroductionKnowledge lies at the core of Word Sense Dis-ambiguation (WSD), the task of computation-ally identifying the meanings of words in context(Navigli, 2009b).
In the recent years, two mainapproaches have been studied that rely on a fixedsense inventory, i.e., supervised and knowledge-based methods.
In order to achieve high perfor-mance, supervised approaches require large train-ing sets where instances (target words in con-text) are hand-annotated with the most appropri-ate word senses.
Producing this kind of knowl-edge is extremely costly: at a throughput of onesense annotation per minute (Edmonds, 2000)and tagging one thousand examples per word,dozens of person-years would be required for en-abling a supervised classifier to disambiguate allthe words in the English lexicon with high accu-racy.
In contrast, knowledge-based approaches ex-ploit the information contained in wide-coveragelexical resources, such as WordNet (Fellbaum,1998).
However, it has been demonstrated thatthe amount of lexical and semantic informationcontained in such resources is typically insuffi-cient for high-performance WSD (Cuadros andRigau, 2006).
Several methods have been pro-posed to automatically extend existing resources(cf.
Section 2) and it has been shown that highly-interconnected semantic networks have a great im-pact on WSD (Navigli and Lapata, 2010).
How-ever, to date, the real potential of knowledge-richWSD systems has been shown only in the presenceof either a large manually-developed extension ofWordNet (Navigli and Velardi, 2005) or sophisti-cated WSD algorithms (Agirre et al, 2009).The contributions of this paper are two-fold.First, we relieve the knowledge acquisition bot-tleneck by developing a methodology to extendWordNet with millions of semantic relations.
Therelations are harvested from an encyclopedic re-source, namely Wikipedia.
Wikipedia pages areautomatically associated with WordNet senses,and topical, semantic associative relations fromWikipedia are transferred to WordNet, thus pro-ducing a much richer lexical resource.
Sec-ond, two simple knowledge-based algorithms thatexploit our extended WordNet are applied tostandard WSD datasets.
The results show thatthe integration of vast amounts of semantic re-lations in knowledge-based systems yields per-formance competitive with state-of-the-art super-vised approaches on open-text WSD.
In addition,we support previous findings from Agirre et al(2009) that in a domain-specific WSD scenarioknowledge-based systems perform better than su-pervised ones, and we show that, given enoughknowledge, simple algorithms perform better thanmore sophisticated ones.2 Related WorkIn the last three decades, a large body of workhas been presented that concerns the develop-ment of automatic methods for the enrichment ofexisting resources such as WordNet.
These in-1522clude proposals to extract semantic informationfrom dictionaries (e.g.
Chodorow et al (1985)and Rigau et al (1998)), approaches using lexico-syntactic patterns (Hearst, 1992; Cimiano et al,2004; Girju et al, 2006), heuristic methods basedon lexical and semantic regularities (Harabagiu etal., 1999), taxonomy-based ontologization (Pen-nacchiotti and Pantel, 2006; Snow et al, 2006).Other approaches include the extraction of seman-tic preferences from sense-annotated (Agirre andMartinez, 2001) and raw corpora (McCarthy andCarroll, 2003), as well as the disambiguation ofdictionary glosses based on cyclic graph patterns(Navigli, 2009a).
Other works rely on the dis-ambiguation of collocations, either obtained fromspecialized learner?s dictionaries (Navigli and Ve-lardi, 2005) or extracted by means of statisticaltechniques (Cuadros and Rigau, 2008), e.g.
basedon the method proposed by Agirre and de Lacalle(2004).
But while most of these methods representstate-of-the-art proposals for enriching lexical andtaxonomic resources, none concentrates on aug-menting WordNet with associative semantic rela-tions for many domains on a very large scale.
Toovercome this limitation, we exploit Wikipedia, acollaboratively generated Web encyclopedia.The use of collaborative contributions from vol-unteers has been previously shown to be beneficialin the Open Mind Word Expert project (Chklovskiand Mihalcea, 2002).
However, its current statusindicates that the project remains a mainly aca-demic attempt.
In contrast, due to its low en-trance barrier and vast user base, Wikipedia pro-vides large amounts of information at practicallyno cost.
Previous work aimed at transformingits content into a knowledge base includes open-domain relation extraction (Wu and Weld, 2007),the acquisition of taxonomic (Ponzetto and Strube,2007a; Suchanek et al, 2008; Wu andWeld, 2008)and other semantic relations (Nastase and Strube,2008), as well as lexical reference rules (Shnarchet al, 2009).
Applications using the knowledgecontained in Wikipedia include, among others,text categorization (Gabrilovich and Markovitch,2006), computing semantic similarity of texts(Gabrilovich and Markovitch, 2007; Ponzetto andStrube, 2007b; Milne and Witten, 2008a), coref-erence resolution (Ponzetto and Strube, 2007b),multi-document summarization (Nastase, 2008),and text generation (Sauper and Barzilay, 2009).In our work we follow this line of research andshow that knowledge harvested from Wikipediacan be used effectively to improve the perfor-mance of a WSD system.
Our proposal builds onprevious insights from Bunescu and Pas?ca (2006)and Mihalcea (2007) that pages in Wikipedia canbe taken as word senses.
Mihalcea (2007) manu-ally maps Wikipedia pages to WordNet senses toperform lexical-sample WSD.
We extend her pro-posal in three important ways: (1) we fully autom-atize the mapping between Wikipedia pages andWordNet senses; (2) we use the mappings to en-rich an existing resource, i.e.
WordNet, rather thanannotating text with sense labels; (3) we deploythe knowledge encoded by this mapping to per-form unrestricted WSD, rather than apply it to alexical sample setting.Knowledge from Wikipedia is injected into aWSD system by means of a mapping to Word-Net.
Previous efforts aimed at automatically link-ing Wikipedia to WordNet include full use of thefirst WordNet sense heuristic (Suchanek et al,2008), a graph-based mapping of Wikipedia cat-egories to WordNet synsets (Ponzetto and Nav-igli, 2009), a model based on vector spaces (Ruiz-Casado et al, 2005) and a supervised approachusing keyword extraction (Reiter et al, 2008).These latter methods rely only on text overlaptechniques and neither they take advantage of theinput from Wikipedia being semi-structured, e.g.hyperlinked, nor they propose a high-performingprobabilistic formulation of the mapping problem,a task to which we turn in the next section.3 Extending WordNetOur approach consists of two main phases: first,a mapping is automatically established betweenWikipedia pages and WordNet senses; second, therelations connecting Wikipedia pages are trans-ferred to WordNet.
As a result, an extended ver-sion of WordNet is produced, that we call Word-Net++.
We present the two resources used in ourmethodology in Section 3.1.
Sections 3.2 and 3.3illustrate the two phases of our approach.3.1 Knowledge ResourcesWordNet.
Being the most widely used compu-tational lexicon of English in Natural LanguageProcessing, WordNet is an essential resource forWSD.
A concept in WordNet is represented as asynonym set, or synset, i.e.
the set of words whichshare a common meaning.
For instance, the con-1523cept of soda drink is expressed as:{ pop2n, soda2n, soda pop1n, soda water2n, tonic2n }where each word?s subscripts and superscripts in-dicate their parts of speech (e.g.
n stands for noun)and sense number1, respectively.
For each synset,WordNet provides a textual definition, or gloss.For example, the gloss of the above synset is: ?asweet drink containing carbonated water and fla-voring?.Wikipedia.
Our second resource, Wikipedia, isa collaborative Web encyclopedia composed ofpages2.
A Wikipedia page (henceforth, Wikipage)presents the knowledge about a specific concept(e.g.
SODA (SOFT DRINK)) or named entity (e.g.FOOD STANDARDS AGENCY).
The page typi-cally contains hypertext linked to other relevantWikipages.
For instance, SODA (SOFT DRINK)is linked to COLA, FLAVORED WATER, LEMON-ADE, and many others.
The title of a Wikipage(e.g.
SODA (SOFT DRINK)) is composed of thelemma of the concept defined (e.g.
soda) plusan optional label in parentheses which specifiesits meaning in case the lemma is ambiguous(e.g.
SOFT DRINK vs.
SODIUM CARBONATE).
Fi-nally, some Wikipages are redirections to otherpages, e.g.
SODA (SODIUM CARBONATE) redirectsto SODIUM CARBONATE.3.2 Mapping Wikipedia to WordNetDuring the first phase of our methodology we aimto establish links between Wikipages and Word-Net senses.
Formally, given the entire set of pagesSensesWiki and WordNet senses SensesWN, we aimto acquire a mapping:?
: SensesWiki ?
SensesWN,such that, for each Wikipage w ?
SensesWiki:?
(w) =????
?s ?
SensesWN(w) if a link can beestablished, otherwise,where SensesWN(w) is the set of senses of thelemma of w in WordNet.
For example, if our1We use WordNet version 3.0.
We use word senses to un-ambiguously denote the corresponding synsets (e.g.
plane1nfor { airplane1n, aeroplane1n, plane1n }).2http://download.wikipedia.org.
We use theEnglish Wikipedia database dump from November 3, 2009,which includes 3,083,466 articles.
Throughout this paper, weuse Sans Serif for words, SMALL CAPS for Wikipedia pagesand CAPITALS for Wikipedia categories.mapping methodology linked SODA (SOFT DRINK)to the corresponding WordNet sense soda2n, wewould have ?
(SODA (SOFT DRINK)) = soda2n.In order to establish a mapping between thetwo resources, we first identify different kinds ofdisambiguation contexts for Wikipages (Section3.2.1) and WordNet senses (Section 3.2.2).
Next,we intersect these contexts to perform the mapping(see Section 3.2.3).3.2.1 Disambiguation Context of a WikipageGiven a target Wikipage w which we aim to mapto a WordNet sense of w, we use the followinginformation as a disambiguation context:?
Sense labels: e.g.
given the page SODA (SOFTDRINK), the words soft and drink are added tothe disambiguation context.?
Links: the titles?
lemmas of the pages linkedfrom the Wikipage w (outgoing links).
For in-stance, the links in the Wikipage SODA (SOFTDRINK) include soda, lemonade, sugar, etc.?
Categories: Wikipages are classified accord-ing to one or more categories, which repre-sent meta-information used to categorize them.For instance, the Wikipage SODA (SOFT DRINK)is categorized as SOFT DRINKS.
Since manycategories are very specific and do not appear inWordNet (e.g., SWEDISH WRITERS or SCI-ENTISTS WHO COMMITTED SUICIDE),we use the lemmas of their syntactic heads asdisambiguation context (i.e.
writer and scien-tist).
To this end, we use the category headsprovided by Ponzetto and Navigli (2009).Given a Wikipage w, we define its disambiguationcontext Ctx(w) as the set of words obtained fromsome or all of the three sources above.3.2.2 Disambiguation Context of a WordNetSenseGiven a WordNet sense s and its synset S, we usethe following information as disambiguation con-text to provide evidence for a potential link in ourmapping ?:?
Synonymy: all synonyms of s in synset S. Forinstance, given the synset of soda2n, all its syn-onyms are included in the context (that is, tonic,soda pop, pop, etc.).1524?
Hypernymy/Hyponymy: all synonyms in thesynsets H such that H is either a hypernym(i.e., a generalization) or a hyponym (i.e., a spe-cialization) of S. For example, given soda2n,we include the words from its hypernym { softdrink1n }.?
Sisterhood: words from the sisters of S. A sistersynset S?
is such that S and S?
have a commondirect hypernym.
For example, given soda2n, itcan be found that bitter lemon1n and soda2n aresisters.
Thus the words bitter and lemon are in-cluded in the disambiguation context of s.?
Gloss: the set of lemmas of the content wordsoccurring within the gloss of s. For instance,given s = soda2n, defined as ?a sweet drinkcontaining carbonated water and flavoring?, weadd to the disambiguation context of s the fol-lowing lemmas: sweet, drink, contain, carbon-ated, water, flavoring.Given a WordNet sense s, we define its disam-biguation context Ctx(s) as the set of words ob-tained from some or all of the four sources above.3.2.3 Mapping AlgorithmIn order to link each Wikipedia page to a Word-Net sense, we developed a novel algorithm, whosepseudocode is shown in Algorithm 1.
The follow-ing steps are performed:?
Initially (lines 1-2), our mapping ?
is empty, i.e.it links each Wikipage w to .?
For each Wikipage w whose lemma is monose-mous both in Wikipedia and WordNet (i.e.|SensesWiki(w)| = |SensesWN(w)| = 1) we mapw to its only WordNet sense w1n (lines 3-5).?
Finally, for each remaining Wikipage w forwhich no mapping was previously found (i.e.,?
(w) = , line 7), we do the following:?
lines 8-10: for each Wikipage d which is aredirection to w, for which a mapping waspreviously found (i.e.
?
(d) 6= , that is, d ismonosemous in both Wikipedia and Word-Net) and such that it maps to a sense ?
(d) ina synset S that also contains a sense of w, wemap w to the corresponding sense in S.?
lines 11-14: if a Wikipage w has not beenlinked yet, we assign the most likely senseto w based on the maximization of the con-ditional probabilities p(s|w) over the sensesAlgorithm 1 The mapping algorithmInput: SensesWiki, SensesWNOutput: a mapping ?
: SensesWiki ?
SensesWN1: for each w ?
SensesWiki2: ?
(w) := 3: for each w ?
SensesWiki4: if |SensesWiki(w)| = |SensesWN(w)| = 1 then5: ?
(w) := w1n6: for each w ?
SensesWiki7: if ?
(w) =  then8: for each d ?
SensesWiki s.t.
d redirects to w9: if ?
(d) 6=  and ?
(d) is in a synset of w then10: ?
(w) := sense ofw in synset of ?
(d); break11: for each w ?
SensesWiki12: if ?
(w) =  then13: if no tie occurs then14: ?
(w) := argmaxs?SensesWN(w)p(s|w)15: return ?s ?
SensesWN(w) (no mapping is establishedif a tie occurs, line 13).As a result of the execution of the algorithm, themapping ?
is returned (line 15).
At the heart of themapping algorithm lies the calculation of the con-ditional probability p(s|w) of selecting the Word-Net sense s given the Wikipage w. The sense swhich maximizes this probability can be obtainedas follows:?
(w) = argmaxs?SensesWN(w)p(s|w) = argmaxsp(s, w)p(w)= argmaxsp(s, w)The latter formula is obtained by observing thatp(w) does not influence our maximization, as it isa constant independent of s. As a result, the mostappropriate sense s is determined by maximizingthe joint probability p(s, w) of sense s and page w.We estimate p(s, w) as:p(s, w) =score(s, w)?s??SensesWN(w),w?
?SensesWiki(w)score(s?, w?
),where score(s, w) = |Ctx(s)?Ctx(w)|+1 (we add1 as a smoothing factor).
Thus, in our algorithmwe determine the best sense s by computing the in-tersection of the disambiguation contexts of s andw, and normalizing by the scores summed over allsenses of w in Wikipedia and WordNet.3.2.4 ExampleWe illustrate the execution of our mapping algo-rithm by way of an example.
Let us focus on the1525Wikipage SODA (SOFT DRINK).
The word sodais polysemous both in Wikipedia and WordNet,thus lines 3?5 of the algorithm do not concernthis Wikipage.
Lines 6?14 aim to find a mapping?
(SODA (SOFT DRINK)) to an appropriateWordNetsense of the word.
First, we check whether a redi-rection exists to SODA (SOFT DRINK) that was pre-viously disambiguated (lines 8?10).
Next, we con-struct the disambiguation context for the Wikipageby including words from its label, links and cate-gories (cf.
Section 3.2.1).
The context includes,among others, the following words: soft, drink,cola, sugar.
We now construct the disambiguationcontext for the two WordNet senses of soda (cf.Section 3.2.2), namely the sodium carbonate (#1)and the drink (#2) senses.
To do so, we includewords from their synsets, hypernyms, hyponyms,sisters, and glosses.
The context for soda1n in-cludes: salt, acetate, chlorate, benzoate.
Thecontext for soda2n contains instead: soft, drink,cola, bitter, etc.
The sense with the largest inter-section is #2, so the following mapping is estab-lished: ?
(SODA (SOFT DRINK)) = soda2n.3.3 Transferring Semantic RelationsThe output of the algorithm presented in the previ-ous section is a mapping between Wikipages andWordNet senses (that is, implicitly, synsets).
Ourinsight is to use this alignment to enable the trans-fer of semantic relations from Wikipedia to Word-Net.
In fact, given a Wikipage w we can collectall Wikipedia links occurring in that page.
Forany such link from w to w?, if the two Wikipagesare mapped to WordNet senses (i.e., ?
(w) 6= and ?(w?)
6= ), we can transfer the correspond-ing edge (?
(w), ?(w?))
to WordNet.
Note that ?
(w)and ?(w?)
are noun senses, as Wikipages describenominal concepts or named entities.
We refer tothis extended resource as WordNet++.For instance, consider the Wikipage SODA(SOFT DRINK).
This page contains, among oth-ers, a link to the Wikipage SYRUP.
Assuming?
(SODA (SODA DRINK)) = soda2n and ?
(SYRUP) =syrup1n, we can add the corresponding semanticrelation (soda2n, syrup1n) to WordNet3.Thus, WordNet++ represents an extension ofWordNet which includes semantic associative re-lations between synsets.
These are originally3Note that such relations are unlabeled.
However, for ourpurposes this has no impact, since our algorithms do not dis-tinguish between is-a and other kinds of relations in the lexi-cal knowledge base (cf.
Section 4.2).found in Wikipedia and then integrated into Word-Net by means of our mapping.
In turn, Word-Net++ represents the English-only subset of alarger multilingual resource, BabelNet (Navigliand Ponzetto, 2010), where lexicalizations of thesynsets are harvested for many languages usingthe so-called Wikipedia inter-language links andapplying a machine translation system.4 ExperimentsWe perform two sets of experiments: we first eval-uate the intrinsic quality of our mapping (Section4.1) and then quantify the impact of WordNet++for coarse-grained (Section 4.2) and domain-specific WSD (Section 4.3).4.1 Evaluation of the MappingExperimental setting.
We first conducted anevaluation of the mapping quality.
To createa gold standard for evaluation, we started fromthe set of all lemmas contained both in Word-Net and Wikipedia: the intersection between thetwo resources includes 80,295 lemmas which cor-respond to 105,797 WordNet senses and 199,735Wikipedia pages.
The average polysemy is 1.3 and2.5 for WordNet senses and Wikipages, respec-tively (2.8 and 4.7 when excluding monosemouswords).
We selected a random sample of 1,000Wikipages and asked an annotator with previousexperience in lexicographic annotation to providethe correct WordNet sense for each page title (anempty sense label was given if no correct mappingwas possible).
505 non-empty mappings werefound, i.e.
Wikipedia pages with a correspondingWordNet sense.
In order to quantify the qualityof the annotations and the difficulty of the task,a second annotator sense tagged a subset of 200pages from the original sample.
We computed theinter-annotator agreement using the kappa coeffi-cient (Carletta, 1996) and found out that our anno-tators achieved an agreement coefficient ?
of 0.9,indicating almost perfect agreement.Table 1 summarizes the performance of our dis-ambiguation algorithm against the manually anno-tated dataset.
Evaluation is performed in terms ofstandard measures of precision (the ratio of cor-rect sense labels to the non-empty labels outputby the mapping algorithm), recall (the ratio ofcorrect sense labels to the total of non-empty la-bels in the gold standard) and F1-measure ( 2PRP+R ).We also calculate accuracy, which accounts for1526P R F1 AStructure 82.2 68.1 74.5 81.1Gloss 81.1 64.2 71.7 78.8Structure + Gloss 81.9 77.5 79.6 84.4MFS BL 24.3 47.8 32.2 24.3Random BL 23.8 46.8 31.6 23.9Table 1: Performance of the mapping algorithm.empty sense labels (that is, calculated on all 1,000test instances).
As baseline we use the most fre-quent WordNet sense (MFS), as well as a ran-dom sense assignment.
We evaluate the map-ping methodology described in Section 3.2 againstdifferent disambiguation contexts for the Word-Net senses (cf.
Section 3.2.2), i.e.
structure-based(including synonymy, hypernymy/hyponymy andsisterhood), gloss-derived evidence, and a combi-nation of the two.
As disambiguation context ofa Wikipage (Section 3.2.1) we use all informationavailable, i.e.
sense labels, links and categories4.Results and discussion.
The results show thatour method improves on the baseline by a largemargin and that higher performance can beachieved by using more disambiguation informa-tion.
That is, using a richer disambiguation con-text helps to better choose the most appropriateWordNet sense for a Wikipedia page.
The combi-nation of structural and gloss information attains aslight variation in terms of precision (?0.3% and+0.8% compared to Structure and Gloss respec-tively), but a significantly high increase in recall(+9.4% and +13.3%).
This implies that the differ-ent disambiguation contexts only partially overlapand, when used separately, each produces differ-ent mappings with a similar level of precision.
Inthe joint approach, the harmonic mean of preci-sion and recall, i.e.
F1, is in fact 5 and 8 pointshigher than when separately using structural andgloss information, respectively.As for the baselines, the most frequent sense isjust 0.6% and 0.4% above the random baseline interms of F1 and accuracy, respectively.
A ?2 testreveals in fact no statistically significant differenceat p < 0.05.
This is related to the random distri-bution of senses in our dataset and the Wikipediaunbiased coverage of WordNet senses.
So select-4We leave out the evaluation of different contexts for aWikipage for the sake of brevity.
During prototyping wefound that the best results were given by using the largestcontext available, as reported in Table 1.ing the most frequent sense rather than any othersense for each target page represents a choice asarbitrary as picking a sense at random.The final mapping contains 81,533 pairs ofWikipages and word senses they map to, covering55.7% of the noun senses in WordNet.Using our best performing mapping we areable to extend WordNet with 1,902,859 semanticedges: of these, 97.93% are deemed novel, i.e.
nodirect edge could previously be found between thesynsets.
In addition, we performed a stricter eval-uation of the novelty of our relations by check-ing whether these can still be found indirectly bysearching for a connecting path between the twosynsets of interest.
Here we found that 91.3%,87.2% and 78.9% of the relations are novel toWordNet when performing a graph search of max-imum depth of 2, 3 and 4, respectively.4.2 Coarse-grained WSDExperimental setting.
We extrinsically evalu-ate the impact of WordNet++ on the Semeval-2007 coarse-grained all-words WSD task (Nav-igli et al, 2007).
Performing experiments in acoarse-grained setting is a natural choice for sev-eral reasons: first, it has been argued that the finegranularity of WordNet is one of the main obsta-cles to accurate WSD (cf.
the discussion in Nav-igli (2009b)); second, the meanings of Wikipediapages are intuitively coarser than those in Word-Net5.
For instance, mapping TRAVEL to the firstor the second sense in WordNet is an arbitrarychoice, as the Wikipage refers to both senses.
Fi-nally, given their different nature, WordNet andWikipedia do not fully overlap.
Accordingly,we expect the transfer of semantic relations fromWikipedia to WordNet to have sometimes the sideeffect to penalize some fine-grained senses of aword.We experiment with two simple knowledge-based algorithms that are set to perform coarse-grained WSD on a sentence-by-sentence basis:?
Simplified Extended Lesk (ExtLesk): The firstalgorithm is a simplified version of the Lesk5Note that our polysemy rates from Section 4.1 also in-clude Wikipages whose lemma is contained in WordNet, butwhich have out-of-domain meanings, i.e.
encyclopedic en-tries referring to specialized named entities such as e.g., DIS-COVERY (SPACE SHUTTLE) or FIELD ARTILLERY (MAGA-ZINE).
We computed the polysemy rate for a random sampleof 20 polysemous words by manually removing these NEsand found that Wikipedia?s polysemy rate is indeed lowerthan that of WordNet ?
i.e.
average polysemy of 2.1 vs. 2.8.1527algorithm (Lesk, 1986), that performs WSDbased on the overlap between the context sur-rounding the target word to be disambiguatedand the definitions of its candidate senses (Kil-garriff and Rosenzweig, 2000).
Given a tar-get word w, this method assigns to w thesense whose gloss has the highest overlap (i.e.most words in common) with the context of w,namely the set of content words co-occurringwith it in a pre-defined window (a sentence inour case).
Due to the limited context providedby the WordNet glosses, we follow Banerjeeand Pedersen (2003) and expand the gloss ofeach sense s to include words from the glossesof those synsets in a semantic relation with s.These include all WordNet synsets which aredirectly connected to s, either by means of thesemantic pointers found in WordNet or throughthe unlabeled links found in WordNet++.?
Degree Centrality (Degree): The second algo-rithm is a graph-based approach that relies onthe notion of vertex degree (Navigli and Lap-ata, 2010).
Starting from each sense s of the tar-get word, it performs a depth-first search (DFS)of the WordNet(++) graph and collects all thepaths connecting s to senses of other words incontext.
As a result, a sentence graph is pro-duced.
A maximum search depth is establishedto limit the size of this graph.
The sense of thetarget word with the highest vertex degree is se-lected.
We follow Navigli and Lapata (2010)and run Degree in a weakly supervised settingwhere the system attempts no sense assignmentif the highest degree score is below a certain(empirically estimated) threshold.
The optimalthreshold and maximum search depth are es-timated by maximizing Degree?s F1 on a de-velopment set of 1,000 randomly chosen nouninstances from the SemCor corpus (Miller etal., 1993).
Experiments on the developmentdataset using Degree on WordNet++ revealeda performance far lower than expected.
Erroranalysis showed that many instances were in-correctly disambiguated, due to the noise fromweak semantic links, e.g.
the links from SODA(SOFT DRINK) to EUROPE or AUSTRALIA.
Ac-cordingly, in order to improve the disambigua-tion performance, we developed a filter to ruleout weak semantic relations from WordNet++.Given a WordNet++ edge (?
(w), ?(w?))
wherew and w?
are both Wikipages and w links to w?,Resource AlgorithmNouns onlyP R F1WordNetExtLesk 83.6 57.7 68.3Degree 86.3 65.5 74.5WikipediaExtLesk 82.3 64.1 72.0Degree 96.2 40.1 57.4WordNet++ExtLesk 82.7 69.2 75.4Degree 87.3 72.7 79.4MFS BL 77.4 77.4 77.4Random BL 63.5 63.5 63.5Table 2: Performance on Semeval-2007 coarse-grained all-words WSD (nouns only subset).we first collect all words from the category la-bels of w and w?
into two bags of words.
We re-move stopwords and lemmatize the remainingwords.
We then compute the degree of overlapbetween the two sets of categories as the num-ber of words in common between the two bagsof words, normalized in the [0, 1] interval.
We fi-nally retain the link for the DFS if such score isabove an empirically determined threshold.
Theoptimal value for this category overlap thresh-old was again estimated by maximizing De-gree?s F1 on the development set.
The finalgraph used by Degree consists of WordNet, to-gether with 152,944 relations from our semanticrelation enrichment method (cf.
Section 3.3).Results and discussion.
We report our results interms of precision, recall and F1-measure on theSemeval-2007 coarse-grained all-words dataset(Navigli et al, 2007).
We first evaluated ExtLeskand Degree using three different resources: (1)WordNet only; (2) Wikipedia only, i.e.
only thoserelations harvested from the links found withinWikipedia pages; (3) their union, i.e.
WordNet++.In Table 2 we report the results on nouns only.
Ascommon practice, we compare with random senseassignment and the most frequent sense (MFS)from SemCor as baselines.
Enriching WordNetwith encyclopedic relations from Wikipedia yieldsa consistent improvement against using WordNet(+7.1% and +4.9% F1 for ExtLesk and Degree)or Wikipedia (+3.4% and +22.0%) alone.
Thebest results are obtained by using Degree withWordNet++.
The better performance of Wikipediaagainst WordNet when using ExtLesk (+3.7%)highlights the quality of the relations extracted.However, no such improvement is found with De-1528AlgorithmNouns only All wordsP/R/F1 P/R/F1ExtLesk 81.0 79.1Degree 85.5 81.7SUSSX-FR 81.1 77.0TreeMatch N/A 73.6NUS-PT 82.3 82.5SSI 84.1 83.2MFS BL 77.4 78.9Random BL 63.5 62.7Table 3: Performance on Semeval-2007 coarse-grained all-words WSD with MFS as a back-offstrategy when no sense assignment is attempted.gree, due to its lower recall.
Interestingly, Degreeon WordNet++ beats the MFS baseline, which isnotably a difficult competitor for unsupervised andknowledge-lean systems.We finally compare our two algorithms usingWordNet++ with state-of-the-art WSD systems,namely the best unsupervised (Koeling and Mc-Carthy, 2007, SUSSX-FR) and supervised (Chanet al, 2007, NUS-PT) systems participating inthe Semeval-2007 coarse-grained all-words task.We also compare with SSI (Navigli and Velardi,2005) ?
a knowledge-based system that partici-pated out of competition ?
and the unsupervisedproposal from Chen et al (2009, TreeMatch).
Ta-ble 3 shows the results for nouns (1,108) andall words (2,269 words): we use the MFS as aback-off strategy when no sense assignment is at-tempted.
Degree with WordNet++ achieves thebest performance in the literature6.
On the noun-only subset of the data, its performance is com-parable with SSI and significantly better than thebest supervised and unsupervised systems (+3.2%and +4.4% F1 against NUS-PT and SUSSX-FR).On the entire dataset, it outperforms SUSSX-FRand TreeMatch (+4.7% and +8.1%) and its re-call is not statistically different from that of SSIand NUS-PT.
This result is particularly interest-ing, given that WordNet++ is extended only withrelations between nominals, and, in contrast toSSI, it does not rely on a costly annotation effortto engineer the set of semantic relations.
Last butnot least, we achieve state-of-the-art performancewith a much simpler algorithm that is based on thenotion of vertex degree in a graph.6The differences between the results in bold in each col-umn of the table are not statistically significant at p < 0.05.AlgorithmSports FinanceP/R/F1 P/R/F1k-NN ?
30.3 43.4Static PR ?
20.1 39.6Personalized PR ?
35.6 46.9ExtLesk 40.1 45.6Degree 42.0 47.8MFS BL 19.6 37.1Random BL 19.5 19.6Table 4: Performance on the Sports and Financesections of the dataset from Koeling et al (2005):?
indicates results from Agirre et al (2009).4.3 Domain WSDThe main strength of Wikipedia is to provide widecoverage for many specific domains.
Accord-ingly, on the Semeval dataset our system achievesthe best performance on a domain-specific text,namely d004, a document on computer sciencewhere we achieve 82.9% F1 (+6.8% when com-pared with the best supervised system, namelyNUS-PT).
To test whether our performance on theSemeval dataset is an artifact of the data, i.e.
d004coming from Wikipedia itself, we evaluated oursystem on the Sports and Finance sections of thedomain corpora from Koeling et al (2005).
In Ta-ble 4 we report our results on these datasets andcompare them with Personalized PageRank, thestate-of-the-art system from Agirre et al (2009)7,as well as Static PageRank and a k-NN supervisedWSD system trained on SemCor.The results we obtain on the two domains withour best configuration (Degree using WordNet++)outperform by a large margin k-NN, thus sup-porting the findings from Agirre et al (2009)that knowledge-based systems exhibit a more ro-bust performance than their supervised alterna-tives when evaluated across different domains.
Inaddition, our system achieves better results thanStatic and Personalized PageRank, indicating thatcompetitive disambiguation performance can stillbe achieved by a less sophisticated knowledge-based WSD algorithm when provided with a richamount of high-quality knowledge.
Finally, theresults show that WordNet++ enables competitiveperformance also in a fine-grained domain setting.7We compare only with those system configurations per-forming token-based WSD, i.e.
disambiguating each instanceof a target word separately, since our aim is not to performtype-based disambiguation.15295 ConclusionsIn this paper, we have presented a large-scalemethod for the automatic enrichment of a com-putational lexicon with encyclopedic relationalknowledge8.
Our experiments show that the largeamount of knowledge injected into WordNet is ofhigh quality and, more importantly, it enables sim-ple knowledge-based WSD systems to perform aswell as the highest-performing supervised ones ina coarse-grained setting and to outperform themon domain-specific text.
Thus, our results goone step beyond previous findings (Cuadros andRigau, 2006; Agirre et al, 2009; Navigli and La-pata, 2010) and prove that knowledge-rich dis-ambiguation is a competitive alternative to super-vised systems, even when relying on a simple al-gorithm.
We note, however, that the present con-tribution does not show which knowledge-rich al-gorithm performs best with WordNet++.
In fact,more sophisticated approaches, such as Personal-ized PageRank (Agirre and Soroa, 2009), could bestill applied to yield even higher performance.
Weleave such exploration to future work.
Moreover,while the mapping has been used to enrich Word-Net with a large amount of semantic edges, themethod can be reversed and applied to the ency-clopedic resource itself, that is Wikipedia, to per-form disambiguation with the corresponding senseinventory (cf.
the task of wikification proposedby Mihalcea and Csomai (2007) and Milne andWitten (2008b)).
In this paper, we focused onEnglish Word Sense Disambiguation.
However,since WordNet++ is part of a multilingual seman-tic network (Navigli and Ponzetto, 2010), we planto explore the impact of this knowledge in a mul-tilingual setting.ReferencesEneko Agirre and Oier Lopez de Lacalle.
2004.
Pub-licly available topic signatures for all WordNet nom-inal senses.
In Proc.
of LREC ?04.Eneko Agirre and David Martinez.
2001.
Learningclass-to-class selectional preferences.
In Proceed-ings of CoNLL-01, pages 15?22.Eneko Agirre and Aitor Soroa.
2009.
PersonalizingPageRank forWord Sense Disambiguation.
In Proc.of EACL-09, pages 33?41.Eneko Agirre, Oier Lopez de Lacalle, and Aitor Soroa.2009.
Knowledge-based WSD on specific domains:8The resulting resource, WordNet++, is freely available athttp://lcl.uniroma1.it/wordnetplusplus forresearch purposes.performing better than generic supervised WSD.
InProc.
of IJCAI-09, pages 1501?1506.Satanjeev Banerjee and Ted Pedersen.
2003.
Extendedgloss overlap as a measure of semantic relatedness.In Proc.
of IJCAI-03, pages 805?810.Razvan Bunescu and Marius Pas?ca.
2006.
Using en-cyclopedic knowledge for named entity disambigua-tion.
In Proc.
of EACL-06, pages 9?16.Jean Carletta.
1996.
Assessing agreement on classi-fication tasks: The kappa statistic.
ComputationalLinguistics, 22(2):249?254.Yee Seng Chan, Hwee Tou Ng, and Zhi Zhong.
2007.NUS-ML: Exploiting parallel texts for Word SenseDisambiguation in the English all-words tasks.
InProc.
of SemEval-2007, pages 253?256.Ping Chen, Wei Ding, Chris Bowes, and David Brown.2009.
A fully unsupervised Word Sense Disam-biguation method using dependency knowledge.
InProc.
of NAACL-HLT-09, pages 28?36.Tim Chklovski and Rada Mihalcea.
2002.
Building asense tagged corpus with Open Mind Word Expert.In Proceedings of the ACL-02 Workshop on WSD:Recent Successes and Future Directions at ACL-02.Martin Chodorow, Roy Byrd, and George E. Heidorn.1985.
Extracting semantic hierarchies from a largeon-line dictionary.
In Proc.
of ACL-85, pages 299?304.Philipp Cimiano, Siegfried Handschuh, and SteffenStaab.
2004.
Towards the self-annotating Web.
InProc.
of WWW-04, pages 462?471.Montse Cuadros and German Rigau.
2006.
Qualityassessment of large scale knowledge resources.
InProc.
of EMNLP-06, pages 534?541.Montse Cuadros and German Rigau.
2008.
KnowNet:building a large net of knowledge from the Web.
InProc.
of COLING-08, pages 161?168.Philip Edmonds.
2000.
Designing a task forSENSEVAL-2.
Technical report, University ofBrighton, U.K.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Database.
MIT Press, Cambridge, MA.Evgeniy Gabrilovich and Shaul Markovitch.
2006.Overcoming the brittleness bottleneck usingWikipedia: Enhancing text categorization withencyclopedic knowledge.
In Proc.
of AAAI-06,pages 1301?1306.Evgeniy Gabrilovich and Shaul Markovitch.
2007.Computing semantic relatedness using Wikipedia-based explicit semantic analysis.
In Proc.
of IJCAI-07, pages 1606?1611.Roxana Girju, Adriana Badulescu, and Dan Moldovan.2006.
Automatic discovery of part-whole relations.Computational Linguistics, 32(1):83?135.Sanda M. Harabagiu, George A. Miller, and Dan I.Moldovan.
1999.
WordNet 2 ?
a morphologicallyand semantically enhanced resource.
In Proceed-ings of the SIGLEX99 Workshop on StandardizingLexical Resources, pages 1?8.1530Marti A. Hearst.
1992.
Automatic acquisition ofhyponyms from large text corpora.
In Proc.
ofCOLING-92, pages 539?545.Adam Kilgarriff and Joseph Rosenzweig.
2000.Framework and results for English SENSEVAL.Computers and the Humanities, 34(1-2).Rob Koeling and Diana McCarthy.
2007.
Sussx: WSDusing automatically acquired predominant senses.In Proc.
of SemEval-2007, pages 314?317.Rob Koeling, Diana McCarthy, and John Carroll.2005.
Domain-specific sense distributions and pre-dominant sense acquisition.
In Proc.
of HLT-EMNLP-05, pages 419?426.Michael Lesk.
1986.
Automatic sense disambiguationusing machine readable dictionaries: How to tell apine cone from an ice cream cone.
In Proceedingsof the 5th Annual Conference on Systems Documen-tation, Toronto, Ontario, Canada, pages 24?26.Diana McCarthy and John Carroll.
2003.
Disam-biguating nouns, verbs and adjectives using auto-matically acquired selectional preferences.
Compu-tational Linguistics, 29(4):639?654.Rada Mihalcea and Andras Csomai.
2007.
Wikify!Linking documents to encyclopedic knowledge.
InProc.
of CIKM-07, pages 233?242.Rada Mihalcea.
2007.
Using Wikipedia for automaticWord Sense Disambiguation.
In Proc.
of NAACL-HLT-07, pages 196?203.George A. Miller, Claudia Leacock, Randee Tengi, andRoss Bunker.
1993.
A semantic concordance.
InProceedings of the 3rd DARPA Workshop on HumanLanguage Technology, pages 303?308, Plainsboro,N.J.David Milne and Ian H. Witten.
2008a.
An effective,low-cost measure of semantic relatedness obtainedfrom Wikipedia links.
In Proceedings of the Work-shop on Wikipedia and Artificial Intelligence: AnEvolving Synergy at AAAI-08, pages 25?30.David Milne and Ian H. Witten.
2008b.
Learning tolink with Wikipedia.
In Proc.
of CIKM-08, pages509?518.Vivi Nastase and Michael Strube.
2008.
DecodingWikipedia category names for knowledge acquisi-tion.
In Proc.
of AAAI-08, pages 1219?1224.Vivi Nastase.
2008.
Topic-driven multi-documentsummarization with encyclopedic knowledge andactivation spreading.
In Proc.
of EMNLP-08, pages763?772.Roberto Navigli and Mirella Lapata.
2010.
An ex-perimental study on graph connectivity for unsuper-vised Word Sense Disambiguation.
IEEE Transac-tions on Pattern Anaylsis and Machine Intelligence,32(4):678?692.Roberto Navigli and Simone Paolo Ponzetto.
2010.BabelNet: Building a very large multilingual seman-tic network.
In Proc.
of ACL-10.Roberto Navigli and Paola Velardi.
2005.
Struc-tural Semantic Interconnections: a knowledge-basedapproach to Word Sense Disambiguation.
IEEETransactions on Pattern Analysis and Machine In-telligence, 27(7):1075?1088.Roberto Navigli, Kenneth C. Litkowski, and Orin Har-graves.
2007.
Semeval-2007 task 07: Coarse-grained English all-words task.
In Proc.
of SemEval-2007, pages 30?35.Roberto Navigli.
2009a.
Using cycles and quasi-cycles to disambiguate dictionary glosses.
In Proc.of EACL-09, pages 594?602.Roberto Navigli.
2009b.
Word Sense Disambiguation:A survey.
ACM Computing Surveys, 41(2):1?69.Marco Pennacchiotti and Patrick Pantel.
2006.
On-tologizing semantic relations.
In Proc.
of COLING-ACL-06, pages 793?800.Simone Paolo Ponzetto and Roberto Navigli.
2009.Large-scale taxonomy mapping for restructuringand integrating Wikipedia.
In Proc.
of IJCAI-09,pages 2083?2088.Simone Paolo Ponzetto and Michael Strube.
2007a.Deriving a large scale taxonomy from Wikipedia.
InProc.
of AAAI-07, pages 1440?1445.Simone Paolo Ponzetto and Michael Strube.
2007b.Knowledge derived from Wikipedia for computingsemantic relatedness.
Journal of Artificial Intelli-gence Research, 30:181?212.Nils Reiter, Matthias Hartung, and Anette Frank.2008.
A resource-poor approach for linking ontol-ogy classes to Wikipedia articles.
In Johan Bos andRodolfo Delmonte, editors, Semantics in Text Pro-cessing, volume 1 of Research in Computational Se-mantics, pages 381?387.
College Publications, Lon-don, England.German Rigau, Horacio Rodr?
?guez, and Eneko Agirre.1998.
Building accurate semantic taxonomies frommonolingual MRDs.
In Proc.
of COLING-ACL-98,pages 1103?1109.Maria Ruiz-Casado, Enrique Alfonseca, and PabloCastells.
2005.
Automatic assignment of Wikipediaencyclopedic entries to WordNet synsets.
In Ad-vances in Web Intelligence, volume 3528 of LectureNotes in Computer Science.
Springer Verlag.Christina Sauper and Regina Barzilay.
2009.
Automat-ically generating Wikipedia articles: A structure-aware approach.
In Proc.
of ACL-IJCNLP-09, pages208?216.Eyal Shnarch, Libby Barak, and Ido Dagan.
2009.
Ex-tracting lexical reference rules from Wikipedia.
InProc.
of ACL-IJCNLP-09, pages 450?458.Rion Snow, Dan Jurafsky, and Andrew Ng.
2006.
Se-mantic taxonomy induction from heterogeneous ev-idence.
In Proc.
of COLING-ACL-06, pages 801?808.Fabian M. Suchanek, Gjergji Kasneci, and GerhardWeikum.
2008.
Yago: A large ontology fromWikipedia and WordNet.
Journal of Web Semantics,6(3):203?217.Fei Wu and Daniel Weld.
2007.
Automatically se-mantifying Wikipedia.
In Proc.
of CIKM-07, pages41?50.FeiWu and Daniel Weld.
2008.
Automatically refiningthe Wikipedia infobox ontology.
In Proc.
of WWW-08, pages 635?644.1531
