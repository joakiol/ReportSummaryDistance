Multiple Interpreters in aPrinciple-Based Model of Sentence ProcessingMatthew W. Crockere-mail: mwc@aipna.ed.ac.ukDepartment of Artificial Intelligence Human Communication Research CentreUniversity of Edinburgh and University of Edinburgh80 South Bridge 2 Buccleuch PlaceEdinburgh, Scotland, EH1 1HN Edinburgh, Scotland, EH8 9LWAbstractThis paper describes a computational model of humansentence processing based on the principles and pa-rameters paradigm of current linguistic theory.
Thesyntactic processing model posits four modules, re-covering phrase structure, long-distance dependencies,coreference, and thematic structure.
These four mod-ules are implemented as recta-interpreters over theirrelevant components of the grammar, permitting vari-ation in the deductive strategies employed by eachmodule.
These four interpreters are also 'coroutined'via the freeze directive of constraint logic program-ruing to achieve incremental interpretation across themodules.1 IntroductionA central aim of computational psycholinguistics is thedevelopment of models of human sentence processingwhich account not only for empirical performance phe-nomena, but which also provide some insight into thenature of between parser and grammar elationship.In concurrent research, we are developing a model ofsentence processing which has its roots in the princi-ples and parameters paradigm of syntactic theory \[1\],\[2\], which holds that a number of representations areinvolved in determining a well-formed analysis of anutterance.
This, in conjunction with Fodor's Modu-larity Hypothesis \[6\], has led us to postulate a modelwhich consists of four informationally encapsulatedmodules for recovering (1) phrase structure, (2) chains(3) coreference, and (4) thematic structure.In this paper we will briefly review a model of sen-tence processing which has been previously proposedin \[5\] and \[3\].
We will illustrate how this model canbe naturally implemented within the logic program-ming paradigm.
In particular, we sketch a subset ofGB theory which defines principles in tern~ of theirrepresentational units, or schemas.
We then discusshow the individual processors may be implemented asspecialised, informationally encapsulated interpreters,and discuss how the 'freeze' directive of constraintlogic programming can be used to effectively coroutinethe interpreters, to achieve incremental interpretationand concurrency.2 The Processing ModelIn the proposed model, we assume that the sentenceprocessor strives to optimise local comprehension a dintegration of incoming material, into the current con-text.
That is, decisions about the current syntacticanalysis are made incrementally (for each input item)on the basis of principles which are intended max-imise the overall interpretation.
We have dubbed thisthe Principle of Incremental Comprehension (PIC),stated roughly as follows:(1) Pr inc ip le of  Incrementa l  Comprehension:The sentence processor operates in such away as to maximise comprehension of thesentence at each stage of processing.The PIC demands that the language comprehen-sion system (LCS), and any sub-system containedwithin it (such as the syntactic and semantic pro-cessors), apply maximally to any input, thereby con-structing a maximal, partial interpretation for a givenpartial input signal.
This entails that each modulewithin the LCS apply concurrently.The performance model is taken to be directlycompatible with the modular, language universal,principle-based theory of current transformationalgrammar \[3\].
We further suggest a highly modular or-ganisation of the sentence processor wherein modulesare determined by the syntactic representations theyrecover.
This is motivated more generally by Fodor'sModularity Hypothesis \[6\] which argues that the var-ious perceptual/input systems consist of fast, dumbinformationally encapsulated modules.
Specifically,we posit four modules within the syntactic processor,each affiliated with a "representational" or "informa-tional" aspect of the grammar.
These are outlined be-low in conjunction with the grammatical subsystemsto which they are related1:1 This grouping of grammatical principles with representationsis both partial and provisional, mad is intended only to givethe reader a feel for the "natural  classes" exploited by themodel.185 -(2)Modules & Principles:Phrase structure (PS):Chains (ChS):Theta structure (TS):Coreference (CiS):F-theory, Move-~Bounding, ECP0-theoryBinding, ControlIn Figure 1, we illustrate one possible instance ofthe organisation within the Syntactic Processor.
Weassume that the phrase structure module drives pro-cessing based on lexical input, and that the thematicstructure co-ordinates the relevant aspects of each pro-cessor for output to the semantic processor.t.~/c~ tnt,.tChain I !
\[ I CoindexationProcessor ~ ProcessorThematic \] '"L___  -e.- Processor ~ .
.
.
.
2Thematic OutputFigure 1: Syntactic Processor OrganisationJust as the PIC applies to the main modules of theLCS as discussed above, it also entails that all moduleswithin the syntactic processor act concurrently so asto apply maximally for a par(ial input, as illustratedby the operation shown in Figure 2.
For the partialinput "What did John put .
.
.
~, we can recover thepartial phrase structure, including the trace in Infl 2.In addition, we can recover the chain linking the did toits deep structure position in Infl (e-l), and also the0-grid for the relation 'put' including the saturatedagent role 'John'.
We might also go one step furtherand postulate a trace as the direct object of put, whichcould be the 0-position of What, but this action mightbe incorrect if the sentence turned out to be What didJohn pui the book oaf, for example.3 Principles and RepresentationsBefore proceeding to a discussion of the model's im-plementation, we will first examine more closely therepresentational paradigm which is employed, and dis-cuss some aspects of the principles and parameterstheory we have adopted, restricting our attention pri-marily to phrase structure and Chains.
In general, a2 We assume here a head movement analysis, where the head ofInfl moves to the head of Comp, to account for Subject-Auxinversion.I-I , \ [What , .
.
\ ]lI \[dld,e-I \]I"What did ~ohn put ..."' Spec' 'Whati !i , d/d , ?
NP  ~ \ ] ~?
?
: John " ?
I!
?
?
e- II ~ V .. .I ?
I I" ,, putIrel: put 11 ragent :  John:l grk'" i th.m,.i_L__ 12 __,Figure 2: Syntactic Processor Operationparticular epresentation can be broken down into twofundamental components: 1) units of information, i.e.the 'nodes' or feature-bundles which are fundamentalto the representation, and 2) units of structure, theminimal structural 'schema' for relating 'nodes' witheach other.
With these two notions defined, the rep-resentation can be viewed as some recursive instanceof its particular schema over a collection of nodes.The representation f phrase structure (PS), as de-termined principally by ~'-theory, encodes the local,sister-hood relations and defines constituency.
Thebar-level notation is used to distinguish the status ofsatellites:k (3) (a) x s cific,, x(b) ~ --.
Modifier,'U~(c) ~- .
Complements, X(d) X ~ LexemeThe linear precedence of satellites with respect otheir sister X-nodes is taken to be parametrised foreach category and language.
The final rule (d) above,is simply the rule for lexical insertion.
In addition tothe canonical structures defined by ~--theory, we re-quire additional rules to permit Chomsky-adjunctionof phrases to maximal projections, via Move-~, andthe rules for inserting traces (or more generally, emptycategories) - -  a consequence of the Projection Princi-ple - -  for moved heads and maximal projections.Given the rules above, we can see that possiblephrase structures are limited to some combination ofbinary (non-terminal) and unary (terminal) branches.As discussed above, we can characterise the represen-tational framework in terms of nodes and schemas:186 -Phrase Structure SchemaNode: N-Node: {Cat,Level, ID,Ftrs}T-Node: {Cat,Phon,ID,Ftrs}Schema: Branch: N-N ode/IN-Node,N-Node\]Branch: N-Node/T-NodeStructure: Tree: N-Node/\[Treer.,TreeR\]Tree: N-Node/T-NodeWe allow two types of nodes: 1) non-terminals (N-Nodes), which are the nodes projected by X-theory,consisting of category, bar level, a unique ID, and thefeatures projected from the head, and 2) terminals (T-Nodes), which are either lexical items or empty cat-egories, which lack bar level, but posses phonologicalfeatures (although these may be 'nil' for some emptycategories).
The schema, defines the unit of structure,using the '/' to represent immediate dominance, andsquare brackets '\[... \]' to encode sister-hood and linearprecedence.
Using this notation we define the two pos-sible types of branches, binary and unary, where thelatter is applicable just in case the daughter is a termi-nal node.
The full PS representation (or Tree) is de-fined by allowing non-terminal daughters to dominatea recursive instance of the schema.
It is interesting tonote that, for phrase structure at least, the relevantprinciples of grammar can be stated purely as condi-tions on branches, rather that trees.
More generally,We will assume the schema of a particular epresenta-tion provides a formal characterisation f locality.Just as phrase structure is defined in terms ofbranches, we can define Chains as a sequence of links.More specifically, each position contained by the chain'is a node, which represents its category and level (aphrase or a head), the status of that position (eitherA or A--), its ID (or location), and relevant features(such as L-marking, Case, and 0).
If we adhere to therepresentational paradigm used above, we can defineChains in the following manner:Chain S chevaaNode: C-Node: {Cat,Level,Pos,ID,Ftrs}Schema: Link: <C-Nodei oo C-Nodej>Structure: Chain: \[ C-Node I Chain \] (where,<C-Node oo head(Chain)> )Chain: \[ \]If we let 'co' denote the linking of two C-Nodes,then we can define a Chain to be an ordered fist of C-Nodes, such that successive C-Nodes satisfy the linkrelation.
In the above definition we have used the'1' operator and list notation in the standard Prologsense.
The 'head' function returns the first C-Node ina (sub) Chain (possibly \[ \]), for purposes of satisfyingthe link relation.
Furthermore, <C-Node co \[ \]> isa well-formed link denoting the tail, Deep-Structureposition, of a Chain.
Indeed, if this is the only link inthe Chain we refer to it as a 'unit' Chain, representingan unmoved element.We noted above that each representation's schemaprovides a natural locality constraint.
That is, weshould be able to state relevant principles and con-straints locally, in terms of the schematic unit.
Thisclearly holds for Subjacency, a well-formedness condi-tion which holds between two nodes of a link:(4) <C-Nodei co C-Nodej> ---,subjacent(C-Nodei,C-Nodej)Other Chain conditions include the Case filter and0-Criterion.
The former stipulates that each NPChain receive Case at the 'highest' A-position, whilethe latter entails that each argument Chain receive x-actly one 0-role, assigned to the uniquely identifiable< C-Node# co \[ \] > link in a Chain.
It is thereforepossible to enforce both of these conditions on locallyidentifiable links of a Chain:(5) In an argument (NP) Chain,i) <C-Node- A  co C-NodeA>case-mark(C-Nodea) or,ii) C-NodeA - head(Chain) --*ease-mark(C-Nodea)In an argument Chain,<C-Nodes co \[\]> --, theta-mark(C-Node0)In describing the representation f a Chain, we havedrawn upon Prolog's list notation.
To carry this fur-ther, we can consider the link operator 'co' to beequivalent to the ',' separator in a list, such that for all\[... C-Nodei,C-Nodej ... \], C-Nodei co C-NOdej holds.In this way, we ensure that each node is well-formedwith respect to adjacent nodes (i.e.
in accordance withprinciples uch as those identified in (4) & (5)).4 The  Computat iona l  Mode lIn the same manner that linguistic theory makes thedistinction between competence (the grammar) andperformance (the parser), logic programming distin-guishes the declarative specification of a program fromits execution.
A program specification consists of a setof axioms from which solution(s) can be proved as de-rived theorems.
Within this paradigm, the nature ofcomputation is determined by the inferencing strat-egy employed by the theorem prover.
This aspect oflogic programming has often been exploited for pars-ing; the so called Parsing as Deduction hypothesis.In particular it has been shown that meta.interpretersor program transformations can be used to affect themanner in which a logic grammar is parsed \[10\] \[1i\],Recently, there has been an attempt o extend .thePAD hypothesis beyond its application to simple logicgrammars \[14\], \[13\] and \[8\].
In particular, Johnsonhas developed a prototype parser for a fragment ofa GB grammar \[9\].
The system consists of a declara-tive specification of the GB model, which incorporates- 187-the various principles of grammar and multiple levelsof representation.
Johnson then illustrates how thefold/unfold transformation and goal freezing, whenapplied to various components ofthe grammar, can beused to render more or less efficient implementations.Unsurprisingly, this deductive approach to parsing in-herits a number of problems with automated educ-tion in general.
Real (implemented) theorem proversare, at least in the general case, incomplete.
Indeed,we can imagine that a true, deductive implementationof GB would present a problem.
Unlike traditional,homogeneous phrase structure grammars, GB makesuse of abstract, modular principles, each of which maybe relevant o only a particular type or level of repre-sentation.
This modular, heterogeneous organisationtherefore makes the task of deriving some single, spe-cialised interpreter with adequate coverage and effi-ciency, a very difficult one.4.1 Deduct ion in a Modular  SystemIn contrast with the single processor model employedby Johnson, the system we propose consists of a num-ber of processors over subsets of the grammar.
Cen-tral to the model is a declarative specification of theprinciples of grammar, defined in terms of the rep-resentations listed in (2), as described in ?3.
If wetake this specification of the grammar to be the "com-petence component", then the "performance compo-nent" can be stated as a parse relation which mapsthe input string to a well-formed "State", where State= { PS,TS,ChS,CiS }, the 4-tuple constituting all as-pects of syntactic analysis.
The highest level of theparser specifies how each module may communicatewith the others.
Specifically, the PS processor actsas input to the other processors which construct theirrepresentations based on the PS representations andtheir own "representation specific" knowledge.
In aweaker model, it may be possible for processors to in-spect the current State (i.e.
the other representations)but crucially, no processor ever actually "constructs"another processor's representation.
The communica-tion between modules is made explicit by the Prologspecification shown below:(6) parse(Lexlnput,State) : -State = {PS,TS,ChS,CiS},ts.module(PS,TS),chs.xnodule(PS,ChS),cis_module(PS,CiS),ps_module(Lexlnput,PS).The parse relation defines the organisation of theprocessors as shown in Figure 1.
The Prolog speci-fication above appears to suffer from the traditionaldepth-first, left-to-right computation strategy used byProlog.
That is, parse seems to imply the sequen-tial execution of each processor.
As Stabler has illus-trated, however, it is possible to alter the computationrule used \[12\], so as to permit incremental interpreta-tion by each module: effectively coroutining the vari-ous modules.
Specifically, Prolog's freeze directive al-lows processing of a predicate to be delayed temporar-ily until a particular argument variable is (partially)instantiated.
In accord with the input dependenciesshown in (7) ,  each module is frozen (or 'waits') on itsinput:(7) \[- Input dependenciests=module PSchs~odule PScis_module PSps.module LexIn'putUsing this feature we may effectively "coroutine"the four modules, by freezing the PS processor onInput, and freezing the remaining processors on PS.Theresult is that each representation is constructedincrementally, at each stage of processing.
To illus-trate this, consider once again the partial input string"What did John put .
.
. '
.
The result of the callpars e ( \[what, did, john, put I 1, St at e) would yieldthe following instantiation of State (with the repre-sentations simplified for readability):(8)State -- { PS = cp/\[np/what,cl/\[c/did,ip/\[np/Johnil/\[i/trace-1,vp/\[v/put, _\]\]\]\],TS = \[rel:put,grid:\[agent:john \].\]\],ChS - \[\[what, _\], \[did,trace-l\] 1,CiS = _ }The PS representation has been constructed asmu6h as possible, including the trace of the movedhead of Intl.
The ChS represents a partial chain forwhat and the entire chain for did, which moved fromits deep structure position to the head of CP, and "IScontains a partial 0-grid for the relation 'put', in whichthe Agent role as been saturated.This is reminiscent of Johnson's approach \[9\], butdiffers crucially in a number of respects.
Firstly, weposit several processors which logically exploit thegrammar, and it is these processors which are corou-tined, not the principles of grammar themselves.
Eachinterpreter is responsible for recovering only one, ho-mogeneous representation, with respect o one inputrepresentation.
This makes reasoning about the com-putational behaviour of individual processors mucheasier.
At each stage of processing, the individual pro-censors increment their representations if and only if,for :the current input, there is a "theorem" provablefrom the grammar, which permits the new structure tobe added.
This me\[a-level "parsing as deduction" ap-proach permits more finely tuned control of the parseras a whole, and allows us to specify distinct inferenc-ing strategies for each interpreter, tailoring it to theparticular epresentation.- 188-4.2 The  PS-Modu le  Spec i f i ca t ionLexical ~ \[ Interpreter for ~ PS-TreeInput Phrase Structure Output!PS-Vlew: Mother/\[Left, Right\] \[Mother/Terminal IX-Bar TheoryXP"~ Specifwr, X'X' ~ Modifuer, X?X' "~ Complemenzs, XX "~LexemeMove-alphaXP ~ Adjunct, XPXP ~ traceX "~traceFigure 3: The Phrase Structure ModuleWe have illustrated in ?3 that the various represen-tations and grammatical principles may be defined interms of their respective schematic units.
Given this,the task of recovering representations (roughly pars-ing) is simply a matter of proving well-formed rep-resentations, as recursive instances of 'schematic ax-ioms', i.e.
those instantiations of a schema which areconsidered well-formed by the grammar.
The formof the PS-Module can be depicted as in Figure 3.The PS interpreter incorporates lexical input into thephrase structure tree based on possible structures al-lowed by the grammar.
Possible structures are deter-mined by the ps_view relation, which returns thosepossible instantiations of the PS schema (as describedin ?3) which are well-formed with respect o the rele-vant principles of grammar.
In general, ps_view willreturn any possible branch structure licensed by thegrammar, but is usually constrained by partial instan-tiation of the query.
In cases where multiple analysesare possible, the ps_view predicate may use some se-lection rule to choose between them 3.
The followingis a specification of the PS interpreter:(9) ps_module(X-X0,Node/\[L/LD,R/RD\]) : -non_lexical(Node),ps.view(Node/\[L,R\]),ps_module(X-X1,L/LD),ps_module(Xl-X0,R/RD).ps.module(X-X0,Node/Daughters) : -ps_ec.eval(X-X0,Node/Daughters).ps.module(X-X0,Node/Daughters) : -psAex_e val( X- X O,N ode/ Daughters).As we have discussed above, the ps..module isfrozenon lexical input represented here as as difference-list.This is one way in which we might implement attachmentprinciples to account for human preferences, see Crocker \[4\]for discussion.The top-level of the PS interpreter is broken downinto three possible cases.
The first handles non-lexicainodes, i.e.
those of category C or I, since phrase struc-ture for these categories is not lexically determined,and can be derived 'top-down', strictly from the gram-mar.
We can, for example, automatically hypothesizea Subject specifier and VP complement for Intl.
Thesecond clause deals with the postulation of empty cat-egories, while the third can be considered the 'base'case which simply attaches lexical material.
Roughly,ps.ec_eval attempts to derive a leaf which is a trace.This action is then verified by the concurrent Chainprocessor, which determines whether or not the traceis licensed (see the following section).
This imple-ments an approximation of the filler-driven strategyfor identifying traces, a strategy widely accepted inthe psycholinguistic literature 4.4 .3  The  Cha in -Modu le  Spec i f i ca t ionJust as the phrase structure processor is delayed onlexical input, the chain processor is frozen with re-spect to phrase structure.
The organisation of theChain Module is shown in Figure 4, and is virtuallyidentical to that of the PS Module (in Figure 3).
How-ever rather than recovering branches of phrase struc-ture, it recovers links of chains, determining their well-formedness with respect o the relevant grammaticalaxioms.PS-Tree ~ Interpreter for ChainsOutput Chain Structure Outputt i t\[ C-Nodal, C-Node2 \] lChain-View: \[ C-Node, \[l IL / '  \ x ,SubJacency:\[ C-Nodal, C.Node 2 \] -~  subjacent(C-Nodal,C-Node2)Theta-Crlterion:\[ C-Node, \[\] \] "~ theta-marked(C-Node)A-to-A-bar Constraint:\[ C-Node1, C-Node 2 \] ~ not(a-pos(C-Nodel) anda-bar-pos( C-Node2 ) )Figure 4: The Chain ModuleFor this module, incremental processing is imple-mented by 'freezing' with respect o the input treerepresentation.
The following code illustrates how thetop-level of the Chain interpreter can traverse the PStree, such that it is coroutined with the recovery ofthe PS representation:4 The is roughly the Active Filler Strateoy \[7\].
For discussionon implementing such strategies within the present model see\[4\].- 189  -(10) chs_module(X/\[L/LD,R/RD\],CS) : -chain_int(X/\[L,R\],CS),chs_module(L/LD,CS),chs.module(R/RD,CS).chs_module(X/Leaf, CS) : -chain_int(X/Leaf, CS).I will assume that che Jodu le  is frozen such thatit will only execute if the daughter(s) of the currentsub-tree is instantiated.
Given this, che.~odnle willperform a top-down traversal of the PS tree, delayingwhen the daughters are uninstantiated, thus corou-tined with the PS-module.
The chain_inl~ predicatethen determines if any action is to be taken, for thecurrent branch, by the chain interpreter:(11) chain.int(X/\[Satellite,Right\],CS) : -visible(X/\[Satellite,Right\],C-Node),chain_member(C-Nodes,CS).chain_int (X/\[Left,Satellite\],CS) : -visible(X/\[Left,Satellite\],C-Node),chain.member(C-Nodes,CS).The chain ~ut predicate decides whether or notthe satellite of the current branch is relevant, or'visible' to the Chain interpreter, and if so returnsan appropriate C-Node for that element.
The twovisible entities are antecedents, i.e.
arguments (ifwe assume that all arguments form chains, possiblyof unit length) or elements in an ~ positions (suchas \[Spec,CP\] or a Chomsky-adjoined position) andtraces.
If a trace or an antecedent is identified, thenit must be a member of a well-formed chain.
Thechain.~ember predicate postulates new chains for lex-ical antecedents, and attempts to append traces to ex-isting chains.
This operation must in turn satisfy thechain_view relation, to ensure the added link obeysthe relevant grammatical constraints.5 Summary and DiscussionIn constructing a computational model of the pro-posed theory of processing, we have employed the logicprogramming paradigm which permits the transpar-ent distinction between competence and performance.At the highest level, we have a simple specificationof the models organisation, in addition we have em-ployed a 'freeze' control strategy which permits us tocoroutine the individual processors, permitting max-imal incremental interpretation.
The individual pro-cessors consist of specialised interpreters which are inturn designed to perform incrementally.
The inter-preters construct their representations, by incremen-tally adding units of structure which are locally well-formed with respect o the principles of the module.The implementation is intended to allow some flex-ibility in specifying the grammatical principles, andvarying the control strategies of various modules.
Itis possible that some instantiations of the syntactictheory will prove more or less compatible with theprocessing model.
It is hoped that such results maypoint the way to a more unified theory of grammarand processing or will at least shed greater light onthe nature of their relationship.AcknowledgementsI would like to thank Elisabet Engdahl and RobinCooper for their comments on various aspects of thiswork.
This research was conducted under the sup-port of an ORS award, an Edinburgh University Stu-dentship and the Human Communication ResearchCentre.Re ferences\[1\] N. Chomsky.
Barriers.
Linguistic Inquiry Mono-graph Thirteen, The MIT Press, Cambridge, Mas-sachusetts, 1986.\[2\] N. Chomsky.
Knowledge of Language: Its Nature,Origin and Use.
Convergence S ries, Praeger,New York, 1986.\[3\] M. Crocker.
Inerementality and Modularity ina Principle-Based Model of Sentence Process-ing.
Presented at The Workshop on GB-Parsing,Geneva, Switzerland, 1990.\[4\] M. Crocker.
Multiple Meta-Interpreters in a Log-ical Model of Sentence Processing.
In Brown andKoch, editors, Natural Language Understandingand Logic Programming, III, Elsevier SciencePublishers (North-Holland), (to appear).\[5\] M. Crocker.
Principle-Based Sentence Process.ing: A Cross-Linguistic Account.
HCRC/RP 1,Human Communication Research Centre, Uni-versity of Edinburgh, U.K., March 1990.\[6\] J. Fodor.
Modularity of Mind.
MIT Press, Cam-bridge, Massachusetts, 1983.\[7\] L. Frazier and C. Clifton.
Successive Cyclicity inthe Grammar and Parser.
Language and Cogni,tire Processes, 4(2):93-126, 1989.\[8\] M. Johnson.
Program Transformation Tech-niques for Deductive Parsing.
In Brown andKoch, editors, Natural Language Understandingand Logic Programming, III, Elsevier SciencePublishers (North-Holland), (to appear).\[9\] M. Johnson.
Use of Knowledge of Lan-guage.
Journal of Psycholinguistic Research,18(1), 1989.\[10\] F. Pereira and D. Warren.
Parsing as Deduction.In Proceedings ofTwenty-First Conference oftheACL, Cambridge, Massachusetts, 1983.\[11\] F. Pereira and S. Shieber.
Prolog and Natural-Language Analysis.
CSLI Lecture Notes, Centerfor the Study of Language and Information, Stan-ford, California, 1987.\[12\] E. Stabler.
Avoid the pedestrian's paradox, un-published ms., Dept.
of Linguistics, UCLA, 1989.\[13\] E. Stabler.
Relaxation Techniques for Principle-Based Parsing.
Presented at The Workshop onGB Parsing, Geneva, Switzerland, 1990.\[14\] E. Stabler.
The Logical Approach to Syntaz.
TheMIT Press, Cambridge, Massachusetts, (forth-coming).190 -
