An Indexing Method Based on Sentences*Li Li 1, Chunfa Yuan1 , K.F.
Wong2, and Wenjie Li31State Key Laboratory of Intelligent Technology and System1Dept.
of Computer Science & Technology, Tsinghua University, Beijing 100084Email: lili97@mails.tsinghua.edu.cn; cfyuan@tsinghua.edu.cn2D e p t. o f  S y s te m  E n g in e e r in g  &  E n g in e e r in g  M a n a g e m e n t, T h e  C h in e se  U n iv e rs ity  o f H o n g  K o n g , H o n g  K o n g .Email: kfwong@se.cuhk.edu.hk3Department of Computing, The Hong Kong Polytechnic University, Hung Hom, Hong Kong.Email: cswjli@comp.polyu.edu.hk*Supported by Natural Science Foundation of China( and 973 project (G1998030507)AbstractTraditional indexing methods often recordphysical positions for the specified words, thus failto recognize context information.
We suggest thatChinese text index should work on the layer ofsentences.
This paper presents an indexing methodbased on sentences and demonstrates how to usethis method to help compute the mutualinformation of word pairs in a running text.
Itbrings many conveniences to work of naturallanguage processing.Keywords: natural language processing, index file,mutual information1.
IntroductionNatural Language Processing often needs toanalyze the relationships between words within thesame sentences or the syntax of the sentences byconsidering the specific words.
To obtain suchinformation, sentences are usually considered asthe basic processing units [4].
The fixed windowapproach is often used in previous studies toobserve the contexts of the specific words andextract them from corpora to form a sub corpus forsome purposes [5,6].
To observe the other words,corpora have to be scanned again and again.Therefore, creating an index file in advance willhelp locate the specified words fast and couldextend the ability to cope with the large-scaleproblems.Although the traditional indexing methods canlocate the specific words fast, it needs extra workto provide the context information.
Traditionalcomputer indexing methods record the physicalposition of the words in the corpus.
The positioninformation is stored in the index file.
To find outwhere the specified word is, the index file canprovide physical position directly.
Then the wordin the corpus can be quickly located [3].
However,if we want to extract the sentences containing thewords, the traditional processing methods have tosearch forward and backward to find the boundaryof these sentences.The indexing method presented in this papercreates the index file based on sentences.
Unliketraditional indexing methods that record thephysical position of the word in the corpus, thisnew method records the logical positions of thewords.
Not only can the index file give thenumbers of the sentences in which the specifiedword occurs, but also locate these sentences in thecorpus instantly.
Since the indexing method basedon sentences records the information of thecontexts of the words, we are able to convenientlystudy some problems with the words in thesentences concerned, which could be called thelogical layer.
That makes it feasible to solve somenatural language processing problems in a large-scale corpus.The rest of this paper is organized as follows.The second section describes the principle of themethod proposed in this paper.
Then the thirdsection summarizes its advantages.
And anexample applying the method is given in the fourthsection.
The fifth section closes this paper withconclusion.2.
Description of the methodAs mentioned above, the difference betweenthe indexing method presented in this paper andthe traditional ones is: the method presented hererecords the logical positions (sentence number),which can be mapped to physical positions (filepointer), while traditional ones only record thephysical positions.
By using the method presentedhere, when we want to get where the concernedword is, what we need to know first is not thephysical positions, but the logic ones.
Then weextract the sentences including the word from thecorpus with the logic positions mapping tophysical positions.The indexing method presented in this paperdeals with the following five kinds of files:(1) Corpus File: a large-scale text file.
(2) Separation File: a binary file, recording thepositions of the delimiter of each sentence inthe corpus.
(3) Word List File: a text file, which consists of asorted list of words.
(4) Frequency File: a binary file, which records thefrequencies and the starting positions of thecorresponding blocks in the Index File.
(5) Index File: a binary file, which consists of aseries of blocks, the logical positions of thewords in the corpus.Corpus File and Word List File are provided byusers.
The other three kinds of files, SeparationFile, Frequency File and Index File, are created inindexing process.
With the method presented here,we deal with one large-scale text file as our corpus.Thus we avoid the problem of coding the multipledocuments and subdirectories.
It is generallybelieved that Chinese information retrieval shouldbe based on words, not characters [1,2].
So weprocess the corpus with segmentation.Before creating an index file, we must have aWord List File that we want to create an index for.Generally, the word list is a sorted list.
Thus wecan fast locate any specified word in the list.The Separation File is created according to theCorpus File.
So the Separation File needs to beupdated if the Corpus File is changed.
TheFrequency File and the Index File correspond tothe Word List File.
These three files are boundtogether.
We may have many groups of these threekinds of files built on the same Corpus File and thecorresponding Separation File.
If the Word ListFile changes, the corresponding Frequency Fileand Index File will be updated as well.The procedure of creating index files is dividedinto two steps, which are described respectively inthe following two parts.2.1 Create the Separation FileFive delimiters are defined as the separationpunctuations of Chinese sentences: Comma (?
),Period (?
), Interrogation (?
), Semicolon (?
),and Interjection (?).
We scan the corpus for thesefive delimiters and record the physical positionsinto the separation file.
The Separation File iscomposed of a series of records; each recordconsists of two parts:(1) The code of the delimiters, which distinguishesthe different kinds of the delimiters;(2) The physical position of each delimiter foundin the corpus.The following table shows the structure of theSeparation File (one row represents one record):Code of the 1st delimiter Physical position 1Code of the 2nd delimiter Physical position 2Code of the 3rd delimiter Physical position 3?
?Table-1 The structure of the Separation FileFrom the Separation File, the sentence with thespecified number from the corpus can be extractedquickly.
For instance, the i_th sentence in thecorpus is obviously between the physical positionstored in record i-1 and the one in record i.2.2 Create the Frequency File andIndex FileFrom the Separation File, we can retrieve eachsentence from the Corpus File in ascending orderof the sentence number.
Then we record thelogical position of every word in the sentence,that is, the sentence number, into the index file.The index file is composed of a series of theblocks.
Each word in the Word File corresponds tosome consecutive blocks stored in the Index File.The number of the blocks each word associatesequals the frequency of the word in the CorpusFile.
So we need to create the Frequency File torecord the frequency of the word and to store theposition of the starting block in the Index File.Each record of the Frequency File consists of twoparts:(1) The frequency of the word occurring in theCorpus, which is equal to the number of theblocks the word associates in the Index File.
(2) The starting position in the Index File, which isthe starting position of a series ofcorresponding blocks in the index file.The following table shows the structure of theFrequency File (one row represents one record):Frequency of the 1st word Starting position 1Frequency of the 2nd word Starting position 2Frequency of the 3rd word Starting position 3?
?Table-2 The structure of the Frequency FileA word may appear several times in onesentence.
We record the sentence number for eachoccurrence of the word, in the Index File.
That is,the Index File will have some sequential blocksrecording the identical sentence number for theword.2.3 SearchWhen a user input the word, the program willsearch that word in the word file first and get theword number, such as No.i.
Then the No.i recordin the Frequency File will be obtained.
The No.irecord includes the information of word frequencyand the starting position of its blocks in the IndexFile.
From these blocks the logical positions(sentence numbers) are obtained and will betransformed into physical positions by SeparationFile.
Then, we can extract all the sentencescontaining the word if necessary.The following is the data-flow map, whichillustrates the procedures described above.CorpusSeparationWords FrequencyIndex WordBAClauseFig-1 The data flowing map: Create separation file: Create index file and frequency file: Find a word in corpus through indexA : Corpus File and Separation File boundedB : Word File, Frequency File and Index File bounded3.
The advantages of the methodText files include many control characters, suchas carriage-return and new-line characters.
So thenatural language content is separated by thesecontrol characters.
The meaningful separationsshould be some punctuations in natural languages.Our indexing method screens the effects of thecontrol characters and brings more conveniencefor natural language processing than traditionalones.The method can be applied on both raw corporaand processed corpora, quickly supplying thesentences containing keywords.
Traditionalindexing method can only give the physicalpositions of the keywords, lacking contextinformation.
It has to search forward andbackward to find out sentence boundaries ifneeded.
Actually, our method has done the portionof sentence locating work, recorded theinformation already in the procedure of creatingSeparation File and saved the time of searching.When we study the relationship of some wordsin a large corpus, the method allows preprocessingon the sentences, which make viable some kinds ofreal-time computing in large-scale corpora.Traditional methods often use fixed-size windowto observe the contexts of specified words and thuslimit the ability to solve large-scale problems.
Thesentences, however, are the natural observingwindows.
The indexing method based onsentences reduces much time consumed formatching words in the corpus and concentrates onthe concerned ranges directly.
The next sectiondemonstrates an example that applies the methodto compute the mutual information of an adjective-noun word pair in a large-scale corpus.4.
An Example applying the methodIn some natural language processing tasks, wemay need to compute the mutual information ofword pairs.
In this example, it is assumed that theobjective is to compute the mutual information ofan adjective-noun pair.
The adjective is ?b?(?beautiful?
), and the noun is ?
s?
(?grassland?
).Firstly, we create the Separation File for thecorpus, the Frequency File and Index File for theWord List File.
Secondly, we get the sentencescontaining the adjective and the noun.
Finally, weselect the proper sentences and compute themutual information.4.1 Source FilesThe initial sources are a corpus file and a wordlist file.
The program runs in a personal computerwith Pentium II 466 processor and 128 MB RAM.It costs one hour and two minutes to create theSeparation File, three hours and fifteen minutes tocreate the Frequency File and Index File.
Table-3shows the size and content of these files.FILE NAME SIZE CONTENTCorpus 240,000KB 120 million tokensWord List 385KB 62,467 word itemsSeparation 27,7000KB 5,816,952 sentences in CorpusFrequency 488KB 62,467 recordsIndex 100,000KB 26,351,631 word occurrences in CorpusTable-3 The source files4 .2  S ea rch  th e  a d jectiv e  a n d  n o u n  p a irsWhen we search the adjective and the noun inthe corpus, we can obtain the adjective?s sentencenumbers and the noun?s sentence numbers fromthe Frequency File and Index File.
By comparingthe two series of sentence numbers in order andfinding the common ones.
We get the sentences inwhich the adjective and the noun both appear.
Infact, we do not see these sentences now, but onlyget the sentence numbers in the corpus.
However,we can extract these sentences from the corpusaccording to the separation file if necessary.
If weare only concerned about the frequency that theadjective-noun pair co-occurrences and don?t careabout the contexts, there?s no need to use theseparation file and the corpus file.We describe the algorithm of obtaining thesentences including the adjective-noun pair in thefollowing procedure:(1)Get the sentence numbers of the adjective: a10a20a30?0am according to the frequency fileand the index file;(2)Similarly, get the sentence numbers of thenoun:b10b20b30?0bn;(3)Initialize i=1, j=1, count=0;(4)If ai=bj, then memorize the integer i, and i++,j++, count++else if ai<bj then i++else j++;(5)Repeat (4) until i=m or j=n;(6)If observing another adjective-noun pair, repeat(1)-(5)Actually, we?ve got the intersection of theadjective?s sentence number set and the noun?s.The sentence numbers are naturally in ascendingorder, since we scan the corpus sentences one byone to create the index file.
This reduces thecomplexity of the algorithm to be O(m+n), as isshown in Step (3) ?
Step (5).
If they are not inorder, the complexity of obtaining the intersectionhas to be O(m*n); if they are ordered in runningprograms, the complexity of algorithm has to beO(m*log(m)) or O(n*log(n)).4.3 Compute mutual informationMutual information is widely used to measurethe association strength of the two events [1,6].The following equation is used to compute themutual information of the adjective-noun pair:)()(),(log  ),( 2sbsbsbpppMI =cpN)(N)( bb ?,cpN)(N)( ss ?cpN) ,(N),( sbsb ?cN  is the total number of sentences in thecorpus, so5,816,952=cN .
It is observed that1984  )N(  1884,  )N( == sband 18 ) ,(N' =sbwhich is the number that two words appear in thesame sentences.
If the observing window size isassumed to be one sentence and the goal is tocompute the distributional joint probability of thetwo words, 18  ) ,('N  ) ,N(  == sbsb ,then 4.807976 ) ,( =sbMI .If only selecting the sentences in which theadjective ?b?
modifies the noun ?
s?, weneed to extract the 18 sentences and parse them orperform semantic analysis, then18  ) ,('N  ) ,N(  =?
sbsb.
Consequently,the result is 10  ) ,N(  =sb , that means in theother 8 sentences the adjective doesn?t modify thenoun, but some other word.
So3.959981  ) ,( =sbMI.5.
ConclusionThis paper demonstrates how the methodcreates the index file and gives the sentencesincluding keywords.
It then shows an example thatemploys the method to discover the sentencescontaining the adjective-noun pairs and computetheir mutual information.
As it is shown, themethod can effectively extract the sentencesincluding specific words and make the real-timeprobabilistic computation possible.
It is also easyto extend the algorithm to search for three or morespecific words appearing in the same sentences orto obtain the intersection, union and difference oftheir sentence number sets.The method can be widely applied for manyapplications in Chinese information processing,such as information extraction, segmentation,tagging, parsing, semantic analysis, dictionarycompilation and information retrieval.
It isparticularly fit for the situation of dealing withspecific words and sentences in large-scalecorpora and is a supporting tool for the researchesof natural language processing.References[1] Aitao Chen, Jianzhang He, Liangjie Xu,Fredric C. Gey and Jason Meggs, Chinese TextRetrieval Without Using a Dictionary, InSIGIR, pages 42-49, 1997.
[2] Jian-yun Nie, Martin Brisebois and XiaoboRen, On Chinese Text Retrieval, In SIGIR,pages 225-233, 1996.
[3] Gerard Salton and Michael J. McGill,Introduction to Modern Information Retrieval,McGraw-Hill, Inc., 1983.
[4] R.Rosenfeld, A Whole Sentence MaximumEntropy Language Model, In Proceedings ofthe IEEE Workshop on Automatic SpeechRecognition and Understanding, 1997.
[5] -ck, ??!?[ X?
?MU, [?C??????
[?, 1998.
[6] -?R, ?
?U, ?K, ?!n?
?d?v, ??
[, 1997H  1?, 29-38I.
