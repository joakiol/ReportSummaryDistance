Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 875?885,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsConvolution Kernel over Packed Parse ForestMin Zhang     Hui Zhang    Haizhou LiInstitute for Infocomm ResearchA-STAR, Singapore{mzhang,vishz,hli}@i2r.a-star.edu.sgAbstractThis paper proposes a convolution forest ker-nel to effectively explore rich structured fea-tures embedded in a packed parse forest.
Asopposed to the convolution tree kernel, theproposed forest kernel does not have to com-mit to a single best parse tree, is thus able toexplore very large object spaces and muchmore structured features embedded in a forest.This makes the proposed kernel more robustagainst parsing errors and data sparseness is-sues than the convolution tree kernel.
The pa-per presents the formal definition of convolu-tion forest kernel and also illustrates the com-puting algorithm to fast compute the proposedconvolution forest kernel.
Experimental resultson two NLP applications, relation extractionand semantic role labeling, show that the pro-posed forest kernel significantly outperformsthe baseline of the convolution tree kernel.1 IntroductionParse tree and packed forest of parse trees aretwo widely used data structures to represent thesyntactic structure information of sentences innatural language processing (NLP).
The struc-tured features embedded in a parse tree havebeen well explored together with different ma-chine learning algorithms and proven very usefulin many NLP applications (Collins and Duffy,2002; Moschitti, 2004; Zhang et al, 2007).
Aforest (Tomita, 1987) compactly encodes an ex-ponential number of parse trees.
In this paper, westudy how to effectively explore structured fea-tures embedded in a forest using convolutionkernel (Haussler, 1999).As we know, feature-based machine learningmethods are less effective in modeling highlystructured objects (Vapnik, 1998), such as parsetree or semantic graph in NLP.
This is due to thefact that it is usually very hard to represent struc-tured objects using vectors of reasonable dimen-sions without losing too much information.
Forexample, it is computationally infeasible to enu-merate all subtree features (using subtree a fea-ture) for a parse tree into a linear feature vector.Kernel-based machine learning method is a goodway to overcome this problem.
Kernel methodsemploy a kernel function, that must satisfy theproperties of being symmetric and positive, tomeasure the similarity between two objects bycomputing implicitly the dot product of certainfeatures of the input objects in high (or even in-finite) dimensional feature spaces without enu-merating all the features (Vapnik, 1998).Many learning algorithms, such as SVM(Vapnik, 1998), the Perceptron learning algo-rithm (Rosenblatt, 1962) and Voted Perceptron(Freund and Schapire, 1999), can work directlywith kernels by replacing the dot product with aparticular kernel function.
This nice property ofkernel methods, that implicitly calculates the dotproduct in a high-dimensional space over theoriginal representations of objects, has madekernel methods an effective solution to modelingstructured objects in NLP.In the context of parse tree, convolution treekernel (Collins and Duffy, 2002) defines a fea-ture space consisting of all subtree types of parsetrees and counts the number of common subtreesas the syntactic similarity between two parsetrees.
The tree kernel has shown much success inmany NLP applications like parsing (Collins andDuffy, 2002), semantic role labeling (Moschitti,2004; Zhang et al, 2007), relation extraction(Zhang et al, 2006), pronoun resolution (Yang etal., 2006), question classification (Zhang andLee, 2003) and machine translation (Zhang andLi, 2009), where the tree kernel is used to com-pute the similarity between two NLP applicationinstances that are usually represented by parsetrees.
However, in those studies, the tree kernelonly covers the features derived from single 1-875best parse tree.
This may largely compromise theperformance of tree kernel due to parsing errorsand data sparseness.To address the above issues, this paper con-structs a forest-based convolution kernel to minestructured features directly from packed forest.
Apacket forest compactly encodes exponentialnumber of n-best parse trees, and thus containingmuch more rich structured features than a singleparse tree.
This advantage enables the forest ker-nel not only to be more robust against parsingerrors, but also to be able to learn more reliablefeature values and help to solve the data sparse-ness issue that exists in the traditional tree kernel.We evaluate the proposed kernel in two real NLPapplications, relation extraction and semanticrole labeling.
Experimental results on thebenchmark data show that the forest kernel sig-nificantly outperforms the tree kernel.The rest of the paper is organized as follows.Section 2 reviews the convolution tree kernelwhile section 3 discusses the proposed forestkernel in details.
Experimental results are re-ported in section 4.
Finally, we conclude the pa-per in section 5.2 Convolution Kernel over Parse TreeConvolution kernel was proposed as a concept ofkernels for discrete structures by Haussler (1999)and related but independently conceived ideas onstring kernels first presented in (Watkins, 1999).The framework defines the kernel function be-tween input objects as the convolution of ?sub-kernels?, i.e.
the kernels for the decompositions(parts) of the input objects.The parse tree kernel (Collins and Duffy, 2002)is an instantiation of convolution kernel oversyntactic parse trees.
Given a parse tree, its fea-tures defined by a tree kernel are all of its subtreetypes and the value of a given feature is thenumber of the occurrences of the subtree in theparse tree.
Fig.
1 illustrates a parse tree with allof its 11 subtree features covered by the convolu-tion tree kernel.
In the tree kernel, a parse tree Tis represented by a vector of integer counts ofeach subtree type (i.e., subtree regardless of itsancestors, descendants and span covered):( )T?
?
(# subtreetype1(T), ?, # subtreetypen(T))where # subtreetypei(T) is the occurrence numberof the ith subtree type in T. The tree kernel countsthe number of common subtrees as the syntacticsimilarity between two parse trees.
Since thenumber of subtrees is exponential with the treesize, it is computationally infeasible to directlyuse the feature vector ( )T?
.
To solve this com-putational issue, Collins and Duffy (2002) pro-posed the following tree kernel to calculate thedot product between the above high dimensionalvectors implicitly.1 1 2 21 1 2 21 2 1 21 21 21 2( , ) ( ), ( )# ( ) # ( )( ) ( )( , )i ii iisubtree subtreei n N n Nn N n NK T T T Tsubtreetype T subtreetype TI n I nn n?
??
??
???
???
?
?
??
?
?
?
??
?
?
??
?????
?
??
?where N1 and N2 are the sets of nodes in trees T1and T2, respectively, and ( )isubtreeI nis a functionthat is 1 iff the subtreetypei occurs with root atnode n and zero otherwise, and1 2( , )n n?
is thenumber of the common subtrees rooted at n1 andn2, i.e.,1 2 1 2( , ) ( ) ( )i isubtree subtreein n I n I n?
?
?
?1 2( , )n n?
can be computed by the following recur-sive rules:INin the bankDT NNPPINthe bankDT NNPPINin bankDT NNPPINin theDT NNPPINinDT NNPPINtheDTPPNN INbankDT NNPPIN DT NNPPINinthe bankDT NNINin the bankDT NNPPFigure 1.
A parse tree and its 11 subtree features covered by convolution tree kernel876Rule 1: if the productions (CFG rules) at1n  and2n  are different, 1 2( , ) 0n n?
?
;Rule 2: else if both1n  and 2n  are pre-terminals(POS tags),1 2( , ) 1n n ??
?
?
;Rule 3: else,1( )1 2 1 21( , ) (1 ( ( , ), ( , )))nc njn n ch n j ch n j???
?
?
?
?
?,where1( )nc n is the child number of 1n , ch(n,j) isthe jth child of node n  and?
(0<?
?1) is the de-cay factor in order to make the kernel value lessvariable with respect to the subtree sizes (Collinsand Duffy, 2002).
The recursive Rule 3 holdsbecause given two nodes with the same children,one can construct common subtrees using thesechildren and common subtrees of furtheroffspring.
The time complexity for computingthis kernel is1 2(| | | |)O N N?
.As discussed in previous section, when convo-lution tree kernel is applied to NLP applications,its performance is vulnerable to the errors fromthe single parse tree and data sparseness.
In thispaper, we present a convolution kernel overpacked forest to address the above issues by ex-ploring structured features embedded in a forest.3 Convolution Kernel over ForestIn this section, we first illustrate the concept ofpacked forest and then give a detailed discussionon the covered feature space, fractional count,feature value and the forest kernel function itself.3.1 Packed forest of parse treesInformally, a packed parse forest, or (packed)forest in short, is a compact representation of allthe derivations (i.e.
parse trees) for a given sen-tence under context-free grammar (Tomita, 1987;Billot and Lang, 1989; Klein and Manning,2001).
It is the core data structure used in naturallanguage parsing and other downstream NLPapplications, such as syntax-based machinetranslation (Zhang et al, 2008; Zhang et al,2009a).
In parsing, a sentence corresponds toexponential number of parse trees with differenttree probabilities, where a forest can compact allthe parse trees by sharing their common subtreesin a bottom-up manner.
Formally, a packed for-est ?
can be described as a triple:?
= < ?,?, ?
>where  ?is the set of non-terminal nodes, ?
is theset of hyper-edges and ?
is a sentenceNNP[1,1] VV[2,2] NN[4,4] IN[5,5]John saw a manNP[3,4]in the bankDT[3,3] DT[6,6]NN[7,7]PP[5,7]VP[2,4] NP[3,7]VP[2,7]IP[1,7]NNP VV NN     IN     DT   NNJohn saw a man in the bankDTVPNPVPIPPPNNP VV NN     IN     DT   NNJohn saw a man in the bankDTNPNPVPIPPPIP[1,7]VP[2,7]NNP[1,1]a) A Forest fb) A Hyper-edge ec) A Parse Tree T1d) A Parse Tree T2Figure 2.
An example of a packed forest, a hyper-edge and two parse trees covered by the packed forest877represented as an ordered word sequence.
A hy-per-edge ?
is a group of edges in a parse treewhich connects a father node and its all childnodes, representing a CFG rule.
A non-terminalnode in a forest is represented as a ?label [start,end]?, where the ?label?
is its syntax categoryand ?
[start, end]?
is the span of words it covers.As shown in Fig.
2, these two parse trees (?1and ?2) can be represented as a single forest bysharing their common subtrees (such as NP[3,4]and PP[5,7]) and merging common non-terminalnodes covering the same span (such as VP[2,7],where there are two hyper-edges attach to it).Given the definition of forest, we introducethe concepts of inside probability ?
.
and out-side probability ?(. )
that are widely-used inparsing (Baker, 1979; Lari and Young, 1990) andare also to be used in our kernel calculation.?
?
?,?
= ?(?
?
?[?])?
?
?, ?
=?
??
??
?
?????
??????????
???
??
??
?(??[??
, ??])??
??
,??
??
?
????????
??
??
????(?)
= 1?
?
?, ?
=?
????
?
?
?
??
??
?
?????
?????
???
???
???
???????
?????
?(??[??
, ??]))??
??
,??
??
?????????
??????
?
??????
?where ?
is a forest node, ?[?]
is the ???
word ofinput sentence ?, ?(?
?
?[?])
is the probabilityof the CFG rule ?
?
?[?]
, ????(. )
returns theroot node of input structure, [??
, ??]
is a sub-spanof  ?, ?
, being covered by ??
, and ?
?
is thePCFG probability of ?
.
From these definitions,we can see that the inside probability is totalprobability of generating words ?
?, ?
fromnon-terminal node ?
?, ?
while the outsideprobability is the total probability of generatingnode ?
?, ?
and words outside ?
[?, ?]
from theroot of forest.
The inside probability can be cal-culated using dynamic programming in a bottom-up fashion while the outside probability can becalculated using dynamic programming in a top-to-down way.3.2 Convolution forest kernelIn this subsection, we first define the featurespace covered by forest kernel, and then definethe forest kernel function.3.2.1 Feature space, object space and fea-ture valueThe forest kernel counts the number of commonsubtrees as the syntactic similarity between twoforests.
Therefore, in the same way as tree kernel,its feature space is also defined as all the possiblesubtree types that a CFG grammar allows.
In aforest kernel, forest ?
is represented by a vectorof fractional counts of each subtree type (subtreeregardless of its ancestors, descendants and spancovered):( )F?
?
(# subtreetype1(F), ?,# subtreetypen(F))= (#subtreetype1(n-best parse trees), ?,   (1)# subtreetypen(n-best parse trees))where # subtreetypei(F) is the occurrence numberof the ith subtree type (subtreetypei) in forest F,i.e., a n-best parse tree lists with a huge n.Although the feature spaces of the two kernelsare the same, their object spaces (tree vs. forest)and feature values (integer counts vs. fractionalcounts) differ very much.
A forest encodes expo-nential number of parse trees, and thus contain-ing exponential times more subtrees than a singleparse tree.
This ensures forest kernel to learnmore reliable feature values and is also able tohelp to address the data sparseness issues in abetter way than tree kernel does.
Forest kernel isalso expected to yield more non-zero feature val-ues than tree kernel.
Furthermore, different parsetree in a forest represents different derivation andinterpretation for a given sentence.
Therefore,forest kernel should be more robust to parsingerrors than tree kernel.In tree kernel, one occurrence of a subtreecontributes 1 to the value of its correspondingfeature (subtree type), so the feature value is aninteger count.
However, the case turns out verycomplicated in forest kernel.
In a forest, each ofits parse trees, when enumerated, has its own878probability.
So one subtree extracted from differ-ent parse trees should have different fractionalcount with regard to the probabilities of differentparse trees.
Following the previous work (Char-niak and Johnson, 2005; Huang, 2008), we de-fine the fractional count of the occurrence of asubtree in a parse tree ??
as?
??????
?, ??
=0                      ??
???????
?
???
??????
?, ?
?|?, ?
????????
?=0                         ??
???????
?
???
?
?|?, ?
????????
?where we have ?
??????
?, ?
?|?, ?
= ?
?
?|?, ?
if???????
?
??
.
Then we define the fractional countof the occurrence of a subtree in a forest f as?
???????,?
= ?
??????
?|?, ?=   ?
??????
?, ??
|?, ?
??
(2)=   ????????
??
?
?
?
?|?, ?
?
?where ????????
??
is a binary function that is 1iif the ???????
?
??
and zero otherwise.
Ob-viously, it needs exponential time to compute theabove fractional counts.
However, due to theproperty of forest that compactly represents allthe parse trees, the posterior probability of asubtree in a forest, ?
??????
?|?, ?
, can be easi-ly computed in an Inside-Outside fashion as theproduct of three parts: the outside probability ofits root node, the probabilities of parse hyper-edges involved in the subtree, and the insideprobabilities of its leaf nodes (Lari and Young,1990; Mi and Huang, 2008).?
???????,?
= ?
??????
?|?, ?
(3)=??(???????)??(????
?
)where??
???????
= ?
????
???????
(4)?
?
???????????
?
???????
???????and??
????
?
= ?
????
?
?
?
????
?= ?
????
?where ?
.
and ?(. )
denote the outside and in-side probabilities.
They can be easily obtainedusing the equations introduced at section 3.1.Given a subtree, we can easily compute itsfractional count (i.e.
its feature value) directlyusing eq.
(3) and (4) without the need of enume-rating each parse trees as shown at eq.
(2) 1 .Nonetheless, it is still computationally infeasibleto directly use the feature vector ?(?)
(see eq.
(1)) by explicitly enumerating all subtrees  al-though its fractional count is easily calculated.
Inthe next subsection, we present the forest kernelthat implicitly calculates the dot-product betweentwo ?(?
)s in a polynomial time.3.2.2 Convolution forest kernelThe forest kernel counts the fractional numbersof common subtrees as the syntactic similaritybetween two forests.
We define the forest kernelfunction ??
?1 ,?2  in the following way.??
?1 ,?2 =< ?
?1 ,?
?2 >                       (5)= #????????????(?1).
#????????????
(?2)?=      ???
??????
?1, ???????2???????
1??1???????
2??2?
?
???????1,?1?
?
??????
?2,?2=   ??
?1 ,?2  ?2??2?1??1where?
???
?,?
is a binary function that is 1 iifthe input two subtrees are identical (i.e.they have the same typology and nodelabels) and zero otherwise;?
?
?,?
is the fractional count defined ateq.
(3);?
?1  and ?2  are the sets of nodes in fo-rests ?1 and ?2;?
??
?1,?2  returns the accumulated valueof products between each two fractionalcounts of the common subtrees rooted at?1 and ?2, i.e.,??
?1,?2=      ???
??????
?1, ???????2????
???????
1 =?1????
???????
2 =?2?
?
???????1,?1?
?
??????
?2,?21  It has been proven in parsing literatures (Baker,1979; Lari and Young, 1990) that eq.
(3) defined byInside-Outside probabilities is exactly to compute thesum of those parse tree probabilities that cover thesubtree of being considered as defined at eq.
(2).879We next show that ??
?1 ,?2  can be computedrecursively in a polynomial time as illustrated atAlgorithm 1.
To facilitate discussion, we tempo-rarily ignore all fractional counts in Algorithm 1.Indeed, Algorithm 1 can be viewed as a naturalextension of convolution kernel from over tree toover forest.
In forest2, a node can root multiplehyper-edges and each hyper-edge is independentto each other.
Therefore, Algorithm 1 iterateseach hyper-edge pairs with roots at ?1  and ?2(line 3-4), and sums over (eq.
(7) at line 9) eachrecursively-accumulated sub-kernel scores ofsubtree pairs extended from the hyper-edge pair?1 , ?2  (eq.
(6) at line 8).
Eq.
(7) holds becausethe hyper-edges attached to the same node areindependent to each other.
Eq.
(6) is very similarto the Rule 3 of tree kernel (see section 2) exceptits inputs are hyper-edges and its further expan-sion is based on forest nodes.
Similar to tree ker-nel (Collins and Duffy, 2002), eq.
(6) holds be-cause a common subtree by extending from(?1 , ?2) can be formed by taking the hyper-edge(?1 , ?2), together with a choice at each of theirleaf nodes of simply taking the non-terminal atthe leaf node, or any one of the common subtreeswith root at the leaf node.
Thus there are1 + ??
????
?1 , ?
, ????
?2 , ?
possiblechoices at the jth leaf node.
In total, there are???
?1 , ?2  (eq.
(6)) common subtrees by extend-ing from (?1 , ?2)  and ??
?1,?2  (eq.
(7)) com-mon subtrees with root at  ?1 ,?2 .Obviously ??
?1 ,?2  calculated by Algorithm1 is a proper convolution kernel since it simplycounts the number of common subtrees under theroot  ?1 ,?2 .
Therefore, ??
?1,?2  defined at eq.
(5) and calculated through ??
?1,?2  is also aproper convolution kernel.
From eq.
(5) and Al-gorithm 1, we can see that each hyper-edge pair(?1 , ?2) is only visited at most one time in com-puting the forest kernel.
Thus the time complexi-ty for computing ??
?1,?2  is ?
(|?1| ?
|?2|) ,where ?1  and ?2 are the set of hyper-edges inforests ?1  and ?2 , respectively.
Given a forestand the best parse trees, the number of hyper-edges is only several times (normally <=3 afterpruning) than that of tree nodes in the parse tree3.2 Tree can be viewed as a special case of forest withonly one hyper-edge attached to each tree node.3 Suppose there are K forest nodes in a forest, eachnode has M associated hyper-edges fan out and eachhyper-edge has N children.
Then the forest is capableof encoding ???1?
?1  parse trees at most (Zhang et al,2009b).Same as tree kernel, forest kernel is runningmore efficiently in practice since only two nodeswith the same label needs to be further processed(line 2 of Algorithm 1).Now let us see how to integrate fractionalcounts into forest kernel.
According to Algo-rithm 1 (eq.
(7)), we have (?1/?2  are attached to?1/?2, respectively)??
?1, ?2 =  ???
?1, ?2 ?1=?2Recall eq.
(4), a fractional count consists ofoutside, inside and subtree probabilities.
It ismore straightforward to incorporate the outsideand subtree probabilities since all the subtreeswith roots at  ?1 , ?2  share the same outsideprobability and each hyper-edge pair is only vi-sited one time.
Thus we can integrate the twoprobabilities into ??
?1,?2  as follows.??
?1,?2 = ?
?
?
?1 ?
?
?2?
?
?1 ?
?
?2 ?
???
?1, ?2  ?1=?2    (8)where, following tree kernel, a decay factor?
(0 < ?
?
1) is also introduced in order to makethe kernel value less variable with respect to thesubtree sizes (Collins and Duffy, 2002).
It func-tions like multiplying each feature value by?????
?
, where ?????
is the number of hyper-edgesin ????????
.Algorithm 1.Input:?1 ,?2: two packed forests?1 ,?2: any two nodes of ?1 and ?2Notation:???
?,?
: defined at eq.
(5)??
?1 : number of leaf node of ?1    ????
?1 , ?
: the jth leaf node of ?1Output:  ??
?1,?21.
??
?1,?2 = 02. if  ?1 .
?????
?
?2 .
?????
exit3.
for each hyper-edge ?1 attached to ?1 do4.
for each hyper-edge ?2 attached to ?2 do5.
if ???
?1, ?2 == 0 do6.
goto line 37.           else do8.
???
?1 , ?2  =    1 +??
?1?=1??
????
?1 , ?
, ????
?2 , ?
(6)9.
??
?1,?2  +=  ???
?1, ?2            (7)10.            end if11.
end for12.
end for880The inside probability is only involved when anode does not need to be further expanded.
Theinteger 1 at eq.
(6) represents such case.
So theinside probability is integrated into eq.
(6) byreplacing the integer 1 as follows.???
?1, ?2 =   ?
????
?1, ?
?
?
????
?2, ???
?1?=1+??
????
?1 , ?
, ????
?2, ??
????
?1 , ?
?
?
????
?2, ?
(9)where in the last expression the two outsideprobabilities ?
????
?1 , ?
and ?
????
?2 , ?are removed.
This is because  ????
?1 , ?
and????
?2 , ?
are not roots of the subtrees of beingexplored (only outside probabilities of the root ofa subtree should be counted in its fractionalcount), and  ??
????
?1 , ?
, ????
?2 , ?
alreadycontains the two outside probabilities of????
?1 , ?
and ????
?2 , ?
.Referring to eq.
(3), each fractional countneeds to be normalized by ??(????
?
).
Since??(????
? )
is independent to each individualfractional count, we do the normalization outsidethe recursive function ???
?1 , ?2 .
Then we canre-formulize eq.
(5) as??
?1,?2 =< ?
?1 ,?
?2 >=??
?1,?2  ?2??2?1??1??
????
?1  ?
??
????
?2(10)Finally, since the size of input forests is notconstant, the forest kernel value is normalizedusing the following equation.?
?
?1,?2 =??
?1,?2??
?1,?1 ?
??
?2,?2(11)From the above discussion, we can see that theproposed forest kernel is defined together by eqs.
(11), (10), (9) and (8).
Thanks to the compactrepresentation of trees in forest and the recursivenature of the kernel function, the introduction offractional counts and normalization do notchange the convolution property and the timecomplexity of the forest kernel.
Therefore, theforest kernel ?
?
?1 ,?2  is still a proper convolu-tion kernel with quadratic time complexity.3.3 Comparison with previous workTo the best of our knowledge, this is the firstwork to address convolution kernel over packedparse forest.Convolution tree kernel is a special case of theproposed forest kernel.
From feature explorationviewpoint, although theoretically they explorethe same subtree feature spaces (defined recur-sively by CFG parsing rules), their feature valuesare different.
Forest encodes exponential numberof trees.
So the number of subtree instances ex-tracted from a forest is exponential number oftimes greater than that from its correspondingparse tree.
The significant difference of theamount of subtree instances makes the parame-ters learned from forests more reliable and alsocan help to address the data sparseness issue.
Tosome degree, forest kernel can be viewed as atree kernel with very powerful back-off mechan-ism.
In addition, forest kernel is much more ro-bust against parsing errors than tree kernel.Aiolli et al (2006; 2007) propose using DirectAcyclic Graphs (DAG) as a compact representa-tion of tree kernel-based models.
This can largelyreduce the computational burden and storage re-quirements by sharing the common structuresand feature vectors in the kernel-based model.There are a few other previous works done bygeneralizing convolution tree kernels (Kashimaand Koyanagi, 2003; Moschitti, 2006; Zhang etal., 2007).
However, all of these works limitthemselves to single tree structure from modelingviewpoint in nature.From a broad viewpoint, as suggested by onereviewer of the paper, we can consider the forestkernel as an alternative solution proposed for thegeneral problem of noisy inference pipelines (eg.speech translation by composition of FSTs, ma-chine translation by translating over 'lattices' ofsegmentations (Dyer  et al, 2008) or using parsetree info for downstream applications in our cas-es) .
Following this line, Bunescu (2008) andFinkel et al (2006) are two typical related worksdone in reducing cascading noisy.
However, ourworks are not overlapped with each other asthere are two totally different solutions for thesame general problem.
In addition, the main mo-tivation of this paper is also different from theirs.4 ExperimentsForest kernel has a broad application potential inNLP.
In this section, we verify the effectivenessof the forest kernel on two NLP applications,semantic role labeling (SRL) (Gildea, 2002) andrelation extraction (RE) (ACE, 2002-2006).In our experiments, SVM (Vapnik, 1998) isselected as our classifier and the one vs. othersstrategy is adopted to select the one with the881largest margin as the final answer.
In our imple-mentation, we use the binary SVMLight (Joa-chims, 1998) and borrow the framework of theTree Kernel Tools (Moschitti, 2004) to integrateour forest kernel into the SVMLight.
We modifyCharniak parser (Charniak, 2001) to output apacked forest.
Following previous forest-basedstudies (Charniak and Johnson, 2005), we use themarginal probabilities of hyper-edges (i.e., theViterbi-style inside-outside probabilities and setthe pruning threshold as 8) for forest pruning.4.1 Semantic role labelingGiven a sentence and each predicate (either atarget verb or a noun), SRL recognizes and mapsall the constituents in the sentence into their cor-responding semantic arguments (roles, e.g., A0for Agent, A1 for Patient ?)
of the predicate ornon-argument.
We use the CoNLL-2005 sharedtask on Semantic Role Labeling (Carreras andMa rquez, 2005) for the evaluation of our forestkernel method.
To speed up the evaluationprocess, the same as Che et al (2008), we use asubset of the entire training corpus (WSJ sections02-05 of the entire sections 02-21) for training,section 24 for development and section 23 fortest, where there are 35 roles including 7 Core(A0?A5, AA), 14 Adjunct (AM-) and 14 Refer-ence (R-) arguments.The state-of-the-art SRL methods (Carrerasand Ma rquez, 2005) use constituents as the labe-ling units to form the labeled arguments.
Due tothe errors from automatic parsing, it is impossi-ble for all arguments to find their matching con-stituents in the single 1-best parse trees.
Statisticson the training data shows that 9.78% of argu-ments have no matching constituents using theCharniak parser (Charniak, 2001), and the num-ber increases to 11.76% when using the Collinsparser (Collins, 1999).
In our method, we breakthe limitation of 1-best parse tree and regard eachspan rooted by a single forest node (i.e., a sub-forest with one or more roots) as a candidate ar-gument.
This largely reduces the unmatched ar-guments from 9.78% to 1.31% after forest prun-ing.
However, it also results in a very largeamount of argument candidates that is 5.6 timesas many as that from 1-best tree.
Fortunately,after the pre-processing stage of argument prun-ing (Xue and Palmer, 2004) 4 , although the4  We extend (Xue and Palmer, 2004)?s argumentpruning algorithm from tree-based to forest-based.The algorithm is very effective.
It can prune outaround 90% argument candidates in parse tree-basedamount of unmatched argument increases a littlebit to 3.1%, its generated total candidate amountdecreases substantially to only 1.31 times of thatfrom 1-best parse tree.
This clearly shows theadvantages of the forest-based method over tree-based in SRL.The best-reported tree kernel method for SRL???????
= ?
?
????
?
+ (1?
?)
?
???
(0 ?
?
?1), proposed by Che et al (2006)5, is adopted asour baseline kernel.
We implemented the ??????
?in tree case (?????????
, using tree kernel tocompute ????
?
and ??? )
and in forest case(?????????
, using tree kernel to compute ????
?and ???
).Precision Recall  F-Score?????????
(Tree) 76.02 67.38  71.44?????????
(Forest) 79.06 69.12 73.76Table 1: Performance comparison of SRL (%)Table 1 shows that the forest kernel significant-ly outperforms (?2 test with p=0.01) the tree ker-nel with an absolute improvement of 2.32 (73.76-71.42) percentage in F-Score, representing a rela-tive error rate reduction of 8.19% (2.32/(100-71.64)).
This convincingly demonstrates the ad-vantage of the forest kernel over the tree kernel.
Itsuggests that the structured features representedby subtree are very useful to SRL.
The perfor-mance improvement is mainly due to the fact thatforest encodes much more such structured featuresand the forest kernel is able to more effectivelycapture such structured features than the tree ker-nel.
Besides F-Score, both precision and recallalso show significantly improvement (?2 test withp=0.01).
The reason for recall improvement ismainly due to the lower rate of unmatched argu-ment (3.1% only) with only a little bit overhead(1.31 times) (see the previous discussion in thissection).
The precision improvement is mainlyattributed to fact that we use sub-forest torepresent argument instances, rather than sub-tree used in tree kernel, where the sub-tree is on-ly one tree encoded in the sub-forest.SRL and thus makes the amounts of positive and neg-ative training instances (arguments) more balanced.We apply the same pruning strategies to forest plusour heuristic rules to prune out some of the argumentswith span overlapped with each other and those ar-guments with very small inside probabilities, depend-ing on the numbers of candidates in the span.5 Kpath and Kcs are two standard convolution tree ker-nels to describe predicate-argument path substructuresand argument syntactic substructures, respectively.8824.2 Relation extractionAs a subtask of information extraction, relationextraction is to extract various semantic relationsbetween entity pairs from text.
For example, thesentence ?Bill Gates is chairman and chief soft-ware architect of Microsoft Corporation?
con-veys the semantic relation ?EMPLOY-MENT.executive?
between the entities ?BillGates?
(person) and ?Microsoft Corporation?(company).
We adopt the method reported inZhang et al (2006) as our baseline method as itreports the state-of-the-art performance usingtree kernel-based composite kernel method forRE.
We replace their tree kernels with our forestkernels and use the same experimental settings astheirs.
We carry out the same five-fold cross va-lidation experiment on the same subset of ACE2004 data (LDC2005T09, ACE 2002-2004) asthat in Zhang et al (2006).
The data contain 348documents and 4400 relation instances.In SRL, constituents are used as the labelingunits to form the labeled arguments.
However,previous work (Zhang et al, 2006) shows that ifwe use complete constituent (MCT) as done inSRL to represent relation instance, there is alarge performance drop compared with using thepath-enclosed tree (PT)6.
By simulating PT, weuse the minimal fragment of a forest covering thetwo entities and their internal words to representa relation instance by only parsing the span cov-ering the two entities and their internal words.Precision  Recall  F-ScoreZhang et al (2006):Tree 68.6 59.3 6  63.6Ours: Forest  70.3 60.0   64.7Table 2: Performance Comparison of RE (%)over 23 subtypes on the ACE 2004 dataTable 2 compares the performance of the for-est kernel and the tree kernel on relation extrac-tion.
We can see that the forest kernel significant-ly outperforms (?2 test with p=0.05) the tree ker-nel by 1.1 point of F-score.
This further verifiesthe effectiveness of the forest kernel method for6 MCT is the minimal constituent rooted by the near-est common ancestor of the two entities under consid-eration while PT is the minimal portion of the parsetree (may not be a complete subtree) containing thetwo entities and their internal lexical words.
Since inmany cases, the two entities and their internal wordscannot form a grammatical constituent, MCT mayintroduce too many noisy context features and thuslead to the performance drop.modeling NLP structured data.
In summary, wefurther observe the high precision improvementthat is consistent with the SRL experiments.
How-ever, the recall improvement is not as significantas observed in SRL.
This is because unlike SRL,RE has no un-matching issues in generating rela-tion instances.
Moreover, we find that the perfor-mance improvement in RE is not as good as thatin SRL.
Although we know that performance istask-dependent, one of the possible reasons isthat SRL tends to be long-distance grammaticalstructure-related while RE is local and semantic-related as observed from the two experimentalbenchmark data.5 Conclusions and Future WorkMany NLP applications have benefited from thesuccess of convolution kernel over parse tree.Since a packed parse forest contains much richerstructured features than a parse tree, we are mo-tivated to develop a technology to measure thesyntactic similarity between two forests.To achieve this goal, in this paper, we design aconvolution kernel over packed forest by genera-lizing the tree kernel.
We analyze the objectspace of the forest kernel, the fractional count forfeature value computing and design a dynamicprogramming algorithm to realize the forest ker-nel with quadratic time complexity.
Comparedwith the tree kernel, the forest kernel is more ro-bust against parsing errors and data sparsenessissues.
Among the broad potential NLP applica-tions, the problems in SRL and RE provide twopointed scenarios to verify our forest kernel.
Ex-perimental results demonstrate the effectivenessof the proposed kernel in structured NLP datamodeling and the advantages over tree kernel.In the future, we would like to verify the forestkernel in more NLP applications.
In addition, assuggested by one reviewer, we may consider res-caling the probabilities (exponentiating them bya constant value) that are used to compute thefractional counts.
We can sharpen or flatten thedistributions.
This basically says "how seriouslydo we want to take the very best derivation"compared to the rest.
However, the challenge isthat we compute the fractional counts togetherwith the forest kernel recursively by using theInside-Outside probabilities.
We cannot differen-tiate the individual parse tree?s contribution to afractional count on the fly.
One possible solutionis to do the probability rescaling off-line beforekernel calculation.
This would be a very interest-ing research topic of our future work.883ReferencesACE (2002-2006).
The Automatic Content ExtractionProjects.
http://www.ldc.upenn.edu/Projects/ACE/Fabio Aiolli, Giovanni Da San Martino, AlessandroSperduti and Alessandro Moschitti.
2006.
Fast On-line Kernel Learning for Trees.
ICDM-2006Fabio Aiolli, Giovanni Da San Martino, AlessandroSperduti and Alessandro Moschitti.
2007.
EfficientKernel-based Learning for Trees.
IEEE Sympo-sium on Computational Intelligence and Data Min-ing (CIDM-2007)J. Baker.
1979.
Trainable grammars for speech rec-ognition.
The 97th meeting of the Acoustical So-ciety of AmericaS.
Billot and S. Lang.
1989.
The structure of sharedforest in ambiguous parsing.
ACL-1989Razvan Bunescu.
2008.
Learning with ProbabilisticFeatures for Improved Pipeline Models.
EMNLP-2008X.
Carreras and Llu?s Ma rquez.
2005.
Introduction tothe CoNLL-2005 shared task: SRL.
CoNLL-2005E.
Charniak.
2001.
Immediate-head Parsing for Lan-guage Models.
ACL-2001E.
Charniak and Mark Johnson.
2005.
Corse-to-fine-grained n-best parsing and discriminative re-ranking.
ACL-2005Wanxiang Che, Min Zhang, Ting Liu and Sheng Li.2006.
A hybrid convolution tree kernel for seman-tic role labeling.
COLING-ACL-2006 (poster)WanXiang Che, Min Zhang, Aiti Aw, Chew Lim Tan,Ting Liu and Sheng Li.
2008.
Using a HybridConvolution Tree Kernel for Semantic Role Labe-ling.
ACM Transaction on Asian Language Infor-mation ProcessingM.
Collins.
1999.
Head-driven statistical models fornatural language parsing.
Ph.D. dissertation,Pennsylvania UniversityM.
Collins and N. Duffy.
2002.
Convolution Kernelsfor Natural Language.
NIPS-2002Christopher Dyer, Smaranda Muresan and Philip Res-nik.
2008.
Generalizing Word Lattice Translation.ACL-HLT-2008Jenny Rose Finkel, Christopher D. Manning and And-rew Y. Ng.
2006.
Solving the Problem of Cascad-ing Errors: Approximate Bayesian Inference forLinguistic Annotation Pipelines.
EMNLP-2006Y.
Freund and R. E. Schapire.
1999.
Large marginclassification using the perceptron algorithm.
Ma-chine Learning, 37(3):277-296D.
Guldea.
2002.
Probabilistic models of verb-argument structure.
COLING-2002D.
Haussler.
1999.
Convolution Kernels on DiscreteStructures.
Technical Report UCS-CRL-99-10,University of California, Santa CruzLiang Huang.
2008.
Forest reranking: Discriminativeparsing with non-local features.
ACL-2008Karim Lari and Steve J.
Young.
1990.
The estimationof stochastic context-free grammars using the in-side-outside algorithm.
Computer Speech and Lan-guage.
4(35?56)H. Kashima and T. Koyanagi.
2003.
Kernels for Semi-Structured Data.
ICML-2003Dan Klein and Christopher D. Manning.
2001.
Pars-ing and Hypergraphs.
IWPT-2001T.
Joachims.
1998.
Text Categorization with SupportVecor Machine: learning with many relevant fea-tures.
ECML-1998Haitao Mi and Liang Huang.
2008.
Forest-basedTranslation Rule Extraction.
EMNLP-2008Alessandro Moschitti.
2004.
A Study on ConvolutionKernels for Shallow Semantic Parsing.
ACL-2004Alessandro Moschitti.
2006.
Syntactic kernels fornatural language learning: the semantic role labe-ling case.
HLT-NAACL-2006 (short paper)Martha Palmer, Dan Gildea and Paul Kingsbury.2005.
The proposition bank: An annotated corpusof semantic roles.
Computational Linguistics.
31(1)F. Rosenblatt.
1962.
Principles of Neurodynamics:Perceptrons and the theory of brain mechanisms.Spartan Books, Washington D.C.Masaru Tomita.
1987.
An Efficient Augmented-Context-Free Parsing Algorithm.
ComputationalLinguistics 13(1-2): 31-46Vladimir N. Vapnik.
1998.
Statistical LearningTheory.
WileyC.
Watkins.
1999.
Dynamic alignment kernels.
In A. J.Smola, B. Sch o?lkopf, P. Bartlett, and D. Schuur-mans (Eds.
), Advances in kernel methods.
MITPressNianwen Xue and Martha Palmer.
2004.
Calibratingfeatures for semantic role labeling.
EMNLP-2004Xiaofeng Yang, Jian Su and Chew Lim Tan.
2006.Kernel-Based Pronoun Resolution with StructuredSyntactic Knowledge.
COLING-ACL-2006Dell Zhang and W. Lee.
2003.
Question classificationusing support vector machines.
SIGIR-2003Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw andChew Lim Tan.
2009a.
Forest-based Tree Se-quence to String Translation Model.
ACL-IJCNLP-2009Hui Zhang, Min Zhang, Haizhou Li and Chew LimTan.
2009b.
Fast Translation Rule Matching for884Syntax-based Statistical Machine Translation.EMNLP-2009Min Zhang, Jie Zhang, Jian Su and GuoDong Zhou.2006.
A Composite Kernel to Extract Relations be-tween Entities with Both Flat and Structured Fea-tures.
COLING-ACL-2006Min Zhang, W. Che, A. Aw, C. Tan, G. Zhou, T. Liuand S. Li.
2007.
A Grammar-driven ConvolutionTree Kernel for Semantic Role Classification.ACL-2007Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,Chew Lim Tan and Sheng Li.
2008.
A Tree Se-quence Alignment-based Tree-to-Tree TranslationModel.
ACL-2008Min Zhang and Haizhou Li.
2009.
Tree Kernel-basedSVM with Structured Syntactic Knowledge forBTG-based Phrase Reordering.
EMNLP-2009885
