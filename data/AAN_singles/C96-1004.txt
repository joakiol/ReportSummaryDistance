Learning Dependenc ies  between Case Frame SlotsHang L i  and  Naok i  AbeTheory  NEC Laboratory ,  RWCP*c /o  ('.&C.
Research  Labora.
tor ies ,  NEC.4-1-1 M iyazak i  Miyama.e-l~u, Kawasak i ,  2116 Japan{lih ang ,abe} (~.sbl.cl .nec.co.jpAbst rac tWe address the problem of automati-cally acquiring case frame patterns (se-lectional patterns) from large corpusdata.
In particular, we l)ropose a methodof learning dependencies between caseframe slots.
We view the problem oflearning case frame patterns as thatof learning a multi-dimensional discretejoint distribution, where random vari-ables represent case slots.
We then for-mMize the dependencies between caseslots as the probabilislic dependenciesbetween these random variables.
Sincethe number of parameters in a multi-dimensional joint distribution is expo-nential in general, it is infeasible to ac-curately estimate them in practice.
Toovercome this difficulty, we settle withapproximating the target joint distribu-tion by the product of low order com-ponent distributions, based on corpusdata.
In particular we propose to employan efficient learning algorithm based onthe MDL principle to realize this task.Our experimental results indicate thatfor certain classes of verbs, the accuracyachieved in a disambiguation experimentis improved by using the acquired knowl-edge of dependencies.1 In t roduct ionWe address the problem of automatically acquir-ing case frame patterns (selectional patterns) fromlarge corpus data.
The acquisition of case framepatterns normally involves the following threesubproblems: 1) Extracting case fl'ames from cor-pus data, 2) Generalizing case frame slots wMfinthese case frames, 3) Learning dependencies thatexist between these generalized case frame slots.In this paper, we propose a method of learn-ing dependencies between case frame slots.
By*Real World Computing Partnership'dependency' is meant the relation that exists be-tween case frame slots which constrains the pos-sible values assumed by each of those slots.
Asillustrative examples, consider tile following sen-tences.The girl will fly a jet.This aMine company flies many jets.The gM will fly Japan AMines.
*The airline conlpany will fly ,Japan Airlines.
(1)We see that an 'airline company' can be the sub-ject of verb 'fly' (the value of case slot 'argl ') ,when the direct object (the value of ease slot'arg2') is an 'airplane' but not  when it is an 'air-line company '1.
These, examples indicate that thepossible values of case slots depend in general onthose of the other case slots: that is, there exist'dependencies' between different case slots.
Theknowledge of such dependencies i  useflfl in var-ious tasks in natural language processing, espe-cially in analysis of sentences involving multipleprepositional phrases, such asThe girl will fly a jet fl'om Tokyo to Beijing.
(2)Note in the above example that the case slot of'from' and that of 'to' should be considered epen-dent and the attachment sit(."
of one of the prepo-sitional phrases (case slots) can be determined bythat of the other with high accuracy and confi-dence.There has been no method proposed to date,however, that learns dependencies between caseframe slots in the natural anguage processing lit-erature.
In the past research, the distributionalpattern of each case slot is learned independently,1 One may argue that 'fly' has different word sensesin these sentences and for each of these word sensesthere is no dependency between the case frames.
Wordsenses are in general difficult to define precisely, how-ever, and in language processing, they would haveto be disambiguated Dora the context ~nyway, whichis essentially equivalent to assuming that the depen-dencies between case slots exist.
Thus, our proposedmethod can in effect 'discover' implicit word sensesfi'om corpus data.20and methods of resolving ambiguity are also basedon the assuml:ition th.at case slots are independent(llindle and Rooth, 1991), or dependencies lm-tween at most two case slots are considered (Brilland Resnik, 1994).
Thus, provision of an efl'ec-tive method of learning de, pendencies between (;as(;slots, as well as investigation of the usefulness ofthe acquired dependencies in disambiguation andother natural language processing tasks would bean inll)ortant contributiota to the fie.ld.In this paper, wc view the problem of learning(;as(?
frame patterns as that of learning a lnulti-dimensional discrete joint distribution, where rawdoni variables represent case slots.
We then for-malize the dependencies between case slots as theprobabilistic dependencies betweeit these ralldoiilvariables.
Since the illllllber Of dependencies thatexist, in a nmlti-dimensiona.l joint disl.ribution isexponential if we allow n-ary dependencies in gen-eral, it is int>asible to accurately esi.itllate themwith high accuracy with a data size available inpractice.
It is also clear that relatiw;ly few of thesera.ndom variahles (case slots) are actually depeit-dent on each other with any signiticance.
Thus itis likely that the target joint distribution can beapproximated reasonably well by the product ofcomponent distributions of low order, drasticallyreducing the nuniber (:if paralneters /.hat need tobe considered.
'Fhis is indeed the apl>roach wetake in this lmper.Now the probleni is how to approxilnal,e a ,jointdistribution by the product of lower or<ler com-pOlletit distributions, llecently, (Suzuki, 1993)l)roposed a.ii algorithnl to approxhnal.cly learii alnulti-dimensional joint distribution exlwessible asa 'dendroid distribution', which is both efticientand tlworet, ica.ily so/lnd.
~,.Ve mploy Suzuki's al-gorithm 1,o learn case fralim patterns ;is dendroiddistributions.
We conducted sollle experinlelits toautomatically acquire case fi'alne patterns fromthe Penn 'Free Bank bra.cketed corpus.
Our ex-perimental results indicate that for seine class ofverbs the accuracy achiew?d ill a disa.nlbiguni.ionexperinlent can be inlproved by using the acquiredknowledge of dependencies between case slots.2 Probab i l i ty  Mode ls  fo r  CaseF rame Pat ternsSuppose that we haw?
data given by ills(antes ofthe case frame of a verb automatically extractedfrom a corpus, using conventional techniques.
Asexplained in Introduction, the l:irol~lelu of learningcase fraille l)atteriis ca.it be viewed as that of es-tilnating the unde~rlying mulli-dimemsioltal joillldistribulioT~ which giw~s rise to such data.
111this research, we assume that <'as(.'
t}ame instanceswith the same head are generated by a joint dis-tribution of type,I'~, (& ,  X~, .
.
.
,  X,,) ,  (:3)where index Y stands for the head, and each of therandonl variables Xi , /  = 1 ,2 , .
.
.
,  n, represents acase slot.
In this paper, we use 'case slots' to meanre,face case slots, and we uniformly treat obliga-tory cases and optional cases.
'rhus the muN)ern of the random variables is roughly equal to thenunfl)er of prepositions in English (and less than100).
These models can be further classified intothree types of probability models according to thetype of values each random variable.
Xi assumes 2.When Xi assumes a word or a special symbol '0'as its value, we refl:r to the corresponding modelPv (Xi , .
?., X , )  as a 'word-based model.'
Here '0'indicates the absence of the case slot in question.When Xi assumes a. word-class or '0' as its value,the corresponding model is called a 'class-basedmodel.'
When Xi takes on 1 or 0 as its value,we call the model a 'slot-based model.'
Here thevalue of ' l '  indicates the presence of the case slotin question, and '0' al>sence.
Suppose for sim-plicity that there are only 4 possible case slots(random variables) corresponding respectively tothe subject, direct object, 'front' phrase, and 'to'phrase.
Then,l'flv(X.,.,at = girl, X.,.g2 = jet, Xf,.
.... = 0, X~o = O)(4)is given a specific l)robability value by a word-based model.
In contrast,Ig,u(X<,,.ai = <person), S.,.
:,~ = (airplane),Xf,.o,, = O, Xto = O)(a)is given a specilic l)robability by a class-based,nodel, where (l,e,'son) alid (airplane) denote~ wordclasses.
Finally,l )tzy(X,, .
,a~ = 1,X~,.au = 1, X.r,.o,,, = O, X to  = O)(o)is assigned a specific probability by a slot-basedmodel.We then forlmllale the dependencies betweencase slots as the probabilislic dependencies be-tween the randonl variabh~s in each of these threetrtodcls.
In the absence of any constraints, how-ever, the number of parameters in each of theabove three lnodels is exponential (even the slot-based model has 0(2")  parameters ), and thus itis infeasible to accurately estimate them in prac-tice.
A simplifying assumption that is often madeto deal with this difficulty is that random variables(case slots) are mutually independent.Sul)pose for examl:ile that in the analysis of thesetltellCel saw a girl with a t.elescope, (7)two interpretatiolls are obtained.
We wish to se-lect.
the nlore appropriate of the two in(eft:itera-tions.
A heuristic word-based method for disam-biguation, in which the slots arc assumed to be2A representation of a probability distribution isusually called a probability model, or simply a model.22dependent, is to calculate tile following values ofword-based likelihood and to select tile interpreta-tion corresponding to the higher likelihood value.Psee(Xa,',.1t =" \[, Xar92  = girl, )l'~uit h ~- telescope)(s)P.~.~(Xa,.al = I, Xa,.oe = girl) (9)x l~li,.l( X~,,io,.
= telescope )If on the other hand we a.ssume that the ran-dom variables are independe'~l, we only need tocalculate and compare t~,:(X~,iH, = telescope)and Pgi,'t(.\'with = telescope) (c.f.
(Li and Abe.,1995)).
The independence assumption can alsobe made in the case of a class-based model or aslot-based model.
For slot-based models,  with tileindependence assumption, P.~(X,~,ith = 1) andPs, .
l (Xwi t f l  = 1) are to be compared (c.f.
(Hindleand Rool:.h, 1991)).Assuming that random variables (case slots)are mutually independent would drastically re-duce tile number of parameters.
(Note that.
un-der the independence assuml)tion tile nmnber ofparameters in a slot-based model becomes 0(~).
)As illustrated in Section 1, t.his assumption is notnecessarily valid in practice.
What seems to betrue in practice is that some case slots are ill factdependent but overwhelming majority of t.hem a.reindependent, due partly to the fa.cl that usuallyonly a few slots are obligatory and most othersare optional.
:~ Thus the target, joint distributionis likely to be a.pproximabie by the product ofseveral component distributions of low order, andthus have in fact a reasonably small number ofparameters.
We are thus lead to the approachof approximating tile tal:get joint distribution bysuch a simplified model, based on corpus data.3 Approx imat ion  by  Dendro idD is t r ibut ionWithout loss of generality, any n-dinlensiorlal jointdistribution can be writl.en asP(x i ,  x._, .
.
.
.
.
x,,) = H P(x , , ,  IX  ..... .
.
.
.x%,_ , )i=1(1o)for some pernnttation (mq, m._, .... nb~ ) of 1, 2 .... n,here we let P(X,~,I x ..... ) denote FIX,,,,).A pta.usib\[e assumption on I.he dependencies be-tween random variables is intuitively that eachvariable direetbj depends oil at most one othervariable.
(Note that this assumption is tile sim-plest among those that relax the independence a.s-sumption.)
For example, if a joint distributionP(X1,  X,,, X:3) over 3 random variables X1, X2, XaaOptiona.1 slots ~tre not necessarily independent,but if two optional slots are randomly selected, it islikely that they are indet)endent of one a.nother.can be written (approximated) as follows, it (al>proximately) satisfies such an assumption.P(.z?1,-"k2, X3 ) : (~,"~)P(-\'1 ) ' / ) (X2  IX1 ).
P(X:, IX\[  )(11)Such distributions are referred to as 'dendroid dis-tributions' in tile literature.
A dendroid distribu-tion can be represenled by a dependency forest(i.e.
a set of dependency trees), whose nodes rep-resent the random variaMes, and whose directedarcs represent the dependencies that exist betweenthese random w/riahles, each labeled with a num-ber of parameters specil}'ing the probabilistic de-pendency.
(A dendroid distribution can also beconsidered as a re.stricted form of the BayesianNetwork (Pearl, 1988).)
It is not difficult t.o seetha.t there are 7 and only 7 such representationsfor the joint distribution P(X1, X,2, X3) disregard-ing the actual nmnerical values of t.he probabilityparameters.Now we turn to the problem of how to select thebest dendroid distribution fi:om among all possi-ble ones to approximate a target joint distributionbased on input data generated by it.
This prob-lem has been inw?stiga.ted in the area of machinelearning and related fields.
A classical method isChow & Liu's algorMnn for estimating a nmlti-dimensional .joint distribution as a dependencytree, ill a way which is both el-~cient and theo-retically sound (C.how and I,iu, 1968).
More re-cent.ly (Suzuki, 1993) extended their algorithm sothat it estimates the target ,joint.
distribution asa dependency Forest.
or 'dendroid distrihution', al-lowing for the possibility of learning one groupof random variables to be completely independentof another.
Since nlany of the random variables(case slots) in case flame patterns are esseutiallyindependent, his feature is crucial in our context,and we thus employ Suzuki's algorithm for learn-ing our case frame patterns.
Figure 1 shows thedetail of this Mgorithm, where ki denotes the nun>her of possible values assumed by node (randomvariable) Xi, N the input data size, and qog' de-notes the logarithm to the base 2.
It is easy tosee that the nulnber of parameters in a dendroiddistribution is of the order O(k2ne), where k isthe maxinmni of all ki, and n is the.
number ofrandom variables, and the time complexity of thealgorithm is of the same order, as it is linear inthe number of parameters.Suzuki's algorithm is derived from the Mini-mum Description Length (MDL) principle (liis-sanen, 1989) which is a principle for statistical es-timation in information theory.
It is known thatas a. method of estimat.ion, MI)L is guaranteedto be near optinm.l 4.
\[n applying MDL, we usu-ally assume that the given data are generated by aprobability model t.hat belongs to a certain class ofmodels and selects a model within tile class which4We reDr the interested reorder 1o (Li and Abe,1995) for an introduction to MDL.22I,et 7' := (/); ('.alculat.e 1,he mut tm|  in\[ol:~nat.ionI( Xi, X5 ) for all uo(:t(~ pairs (,Y/, X j  ); Sort.
1\]wnode pairs in d(~scen(liug o\]'(h+r of l ,  and stor(~l.hent int.o qm'ue Q; l,(;t V 1)c /.he set  of  {Xi} ,i =: 1,2, ...,~\]:whih'+ The  llla.xittltlltl vahw of  l in Q sar is \ [ i ts\](&., :v~) > o(x~ &)  = (<: -  t)(a,~ 1)>~" ' ' 2 Ndo  t ) ( ;g inI~muov(" tlw nod(> l)air (,\7i.
,\+j) h;/vil~g th(,ni;/xi\]~mil+ v;t.ltw <)I' / \['ro~t Q;If" ,\7+ aml  A j  I>(,lot~g to diIl'('r(mt, sc l s  I t+,ll':+, in 1;Them I{el)lac(> IVI a.n(l II +., in l wilhH'I U I1":,, and add edge ( , \ i .
A'j ) 10 "\[':endOutput.
7' as 1.ho set.
of (xlgcs o1' the ('stitnal('(tmodel .l"ig.trc l : The hm.rtfing algori l .hulbest ( 'xpla i i> l.he dal.a.
I1.
i.
(m(ls Io I~(' l lw ('ascusua.Hy t;hal, a s iml lh'r  model  has a l)oor(,r Ill t.o1,he dal.a, a.H(/ a nlore complex mo(hq h+ts a l+(,i,l,(q:fil I lIO I'll() (la.t.a.
Thus  t,h('l:e is n t.rad('-ofI' I>ctw(>cnt,t> s impl ic i ty  of a mod(q gum l.h(' go(>dn('ss of lit.
todata.
M1)I, resolves I.his I.ra(h~-<)\[l' in a (lis('il>ti\[>dway: 11. s(eh,cl.s a Illod('l which is i '(msonably silu-I/l(> a.nd fits l.he data  sal.isl"acl.orily as w('\[l. In our('lil'I;('l/l prol) l (ml,  a :-;iltlI)\](?
IHod('l iil(:;tl/S ;t IIIC)(\[('Iwil.h less d(q)('l~(l(mcies, and thus Ni l ) l ,  l)rovi(l(,.-,a (h(?or(q.ic;dly sound way 1.o learn ()Ill N Ihosc &,pcq\]dcncies thai, arc sl.al.isticMly s igni l icant in Ill(:given (\[al;a.
Air esp(~c\[;dly iJll('t'(,s(iug \[}~alur(~ ofMI l l ,  is l\]lal it.
incorl)orat.es l:he il\]l)tll, da la  sizein it.s model  soh>ct.ion crit.crion. '
l 'his is rcfl~'('led,in our (u~,s(>, in t.hc <terival.i(>n ()l' 1,h(' thr('sh(,hl O.Nol.e l, haI, wh(m wc (lo not, \]l;iv(~ enough data  (i.e.\[or smf l l  N),  the thr(>shohls will b(' large andIbw nodes Icn(I 1.o 1)c Iinlccd, rcsul i i l lg ill a sil\]l-pie mod(' l  ill which most.
o\[ t,l> ('as(> tTr+m> slotsarc ,jtt(lgc'd in(h':l)(m,,hml..
This  is r(uts(>na.lA(, sincewith a smal l  data.
size most  cas,, slot> cam\]oi I)(,degermin(xl i.o I)c dep(m(h-\]tt with a.uy signif icance.4 Exper imenta l  Resu l t ; s\~"o COl\](\[/l(%.'.
"(I soltt(" l)r(,l indluu'y ('xp('ritn(qtts tolest.
the i;(,rl'otul|atlc(, o\[ t.hc l;l',.
)lt()s(+(l tt+('th()(/ as ;\]m(,I.ho(I o1' +requiring ('aso l'r+uu(' i);tt~cru~, lit i);n +-1.icular, wc t.cs(('(l t.o see hoxx cl\[(?
('tiv( ~ th(> p;tl t(q'usa.cquired by our nJ( ' lhod ar<' i\]~ s lruct ural d isam-b iguat ion.
\V(' will dcs(:rib(' the resull.s o17 this ex-por in \ ]cntat ion i this sccl;ion.4.1.
Exper intt ; i~tt ,  1: Slo l ; -basc,  d Mode llu otn' tirsl, cxp(erim(,nt, w(, Iri('d io  a('(luir(' slot});~s(~(I case f'ra.tt\]e patt.(u:us.
Fil'sl., W(' ('xl.r;t('l.
('(\[18 t ,250 case fra.ules from l,hc Wall S1 r('(>t .l()u rnal(WSJ )  I)rackcted COl'IreS o/' l,\]tc I 'enu ' lrve I~ankas t;t:a.iniug data.. Thor(> w('t'(~ 357 vcrl)s \[or which'\['al)le 1: Verbs  and l:hoir l )e rptex i tyVerb I ndel)(mdent l ) (mdroidndd 5.g2 5.36buy 5.0,11 4.98find 2.07 1.92ol)(m 20.5(3 16.53l)rot.c('t.
:L3!)
3.13l/rovid(> ,l.46 4.13r(?t)r(,s(m t 1.2G 1.26s(qld 3.20 3.29s u(:cc(+d '23)7 2.57tell 1.3(5 1.36more (,hmi 50 cas(~ frame examph~s appeared  in l iral ra in ing  data .lqrsl, wo acquit>d l,hc s lo i -bascd case f lame pal -iOI'|IS for ;Ill of  t.he 357 verbs.
\,'V(~ \[lll(~ ii (~()t~ (l ~ \[(: to(l ~I ,cwfohl  cross va\]idai, ion to cva\[uai,e t lw  %esI, datap(u:ph~xii,y' of t,/w acquired case frame pat, terns,that  is, w?~ used nine l,(ml, h o\[ the case f lames %reach verb as t ra in ing dat,a (saving what, rema.insas t, es(, data), t,o acquire case f lame pai, l, erns, andthen ca lcu la lcd  pCrl)lexil.
?
using the lesl, data.
VV(>rel>Catc'd this procoss t.cn lim(~s a.nd ca lcu lated tlm;tvcragc l)Crl)lexity.
'\[ 'able I shows the average per-plexit.y ()btmm'd for some randomly  s('h'ctcd verbs.\Ve also calculat.cd t im av(u:age perplexi l .y of theqndcpcndettt ,  slof n lodcls '  acquired bas(~d on 1.h('assumpt, ion t, hal.
(~ach slof is hMepcmhml, .
Our  exl )cr imenl ,a l  rcsull, s shown in ' l 'able 1 ind icate  (ha l1.he use o\[ t.he +'ndroid models can achieve up t.o2()~.
pcrpl(~xil:y reducl ion as COmlmt'ed ~o the imb-\[)Ol|d(Hll, slot ll\](,)(\[OIS.
It scorns sail" lo say lhere\['oretha i  the dendro id  utodcl is more stt i tablc I'or rcp-rcscnl:ing the Ira+ model  o\[' case f lames than l.\[whMq)emlcn l  s\]ol.
lttOdO\[.\Vc also used lhe acquir(>d depend(racy knowl-c+,{gc ill a pl>at, l achmenl,  d isambiguai .
ion exper i -i\]lol\]l., kV(' used the case h'an\]~s of' all 357 verbsas o\]tr t.raining dat.a.
Wc used Chc cttl:irc + brack-etc<l corpus as Iil'a.illillg dat.a it\] part: because wcwanl.ed t.o uti l ize as many t.raining data  as possi-ble.
We ext.ract.
(<l (c~ rb, ,ou?q ,  prep, ?,)tt?~2) or( v(,A,, t .
'cpt,  ~otml,  prr  p.2, ~ou~\]2) pat.terns \['rotltthe \VSJ tagged ('orplts ;ts i,est.
( lata, ItSillg pa.t-tc\]'n match ing  tccl!t\]iqucs.
\Vc t.ook care to ensureiha l  otlly t, hc part.
o\[' l\[w (agg('d (non-l)rackt,t.cll)cor lms which do(,s not ov('r lap xxit.h the I)rack('l,('(IcorptlS is tlSC(I a,'< test.
dai.a.
(The bracl,:(,ted cor lmslots over lap wii.h i)arl, o\[ the t,ttgg~x:\[ (orpus.
)\Vc acquired ('aso \[ratne pal t.crns using t, hc| .raining da, ta.
\V~ found l:hai there were 266v<wl>s, whose 'arg2'  slot is (tel'~(qtdc'l l l  Ol1 SOl\]tO.of i, hc ot,lwr prepos i t ion slots.
'l 'hm'v were 37(Se~' exmr@es  in 'l'al)lc 2) verbs whose depen-(h>l\]cy I)cl,w(>en ;u:g2 and ol, hcr slots is positAv(,atl(l (~x(:o,,d.
'-; a COl;t.ailt threshold,  i.e.
P lay92 -l ,p r+p = J) 2> 0.25.
'1'11(> depend(moles \[ound:1_3by our method seem to agree with human intu-ition in most cases.
There were 93 examples inTable 2: Verbs and their dependent slotsVerb Dependent slotsaddblamebuyclimbcompareconvertdefendexplainfilefocusarg2 toarg2 forarg2 forarg2 fromarg2 witha.rg2 toarg2 againstarg2 toarg2 againstarg2 onTable 3: Disambiguation results 1DendroidIndependentAccuracy(%)90/93(96.8)79/93(84.9)the test data ((verb, nounl ,prcp,  no'an2) pattern)in which tile two slots 'a.rg2' and prep of verbare determined to be positively dependent andtheir dependencies are stronger than tile thresh-old of 0.25.
We forcibly attached prep nou~t2 toverb for these 93 examples.
For comparison, wealso tested the disambiguation method based onthe independence assumption proposed by (Li andAbe, 1995) on these examples.
Table 3 showsthe results of these experiments, where 'Dendroid'stands for the former method and ' Independent'the latter.
We see that using tile information ondependency we can significantly improve the dis-ambiguation accuracy on this part of the dataSince we can use existing methods to per-form disambiguation for the rest of the data, wecan improve the disambiguation accuracy for theentire test data using this knowledge.
Further-more, we found that there were 140 verbs hav-ing inter-dependent preposition slots.
There were22 (See examples in Table 4 ) out of these 140verbs such that their ease slots hawe positive de-pendency that exceeds a certain threshold, i.e.P(prepl  = 1,prep2 = 1) > 0.25.
Again the de-pendencies found by our method seem to agreewith human intuition.
In the test data (whichare of verb,prep:t,nount,prep~, nou~ pattern),there were 21 examples that involw?
one of theabove 22 verbs whose preposition slots show de-pendency exceeding 0.25.
We forcibly attachedbot.h prep, no'unl and prep2 noun2 to verb onthese 21 examples, since the two slots prept andprep~ are judged to be dependent.
Table 5 showsthe results of this experimentation, where 'Den-droid' and ' Independent' respectively representTable 4: Verbs and their dependent slotsHead Dependent slotsacquireapplyboostclimbfallgrowimproveraisesellthinkfroII1 forfor tofrom tofrom tofrom tofi'om tofrom tofl'om toto forOf asthe method of using and not using the knowl-edge of dependencies.
Again, we found that forthe part of the test data in which dependency ispresent, the use of the dei)endency knowledge canbe used to improve the accuracy of a disambigua-tion method, Mthough our experimental resultsare inconclusive at this stage.Table 5: Disambiguation results 2Accuracy(%)Dendroid 21./21(100)Independent 20/21(95.2)4.2 Exper iment  2: C lass -based  Mode lWe also used the 357 verbs and their case framesused in Experiment 1 to acquire class-based caseframe patterns using the proposed method.
Werandomly selected 100 verbs among these 35rverbs and attempted to acquire their case framepatterns.
We generalized the case slots withineach of these case frames using the method pro-posed by (Li and Abe, 1995) to obtain class-basedcase slots, and then replaced the word-based caseslots in the data with the obtained class-basedcase slots.
What resulted are class-based caseframe examples.
We used these data as input tothe learning algorithm and acquired case framepatterns for each of' the 100 verbs.
We found iJmtno two case slots are determined as dependent inany of the case frame patterns.
This is becausethe number of parameters in a class based modelis very large compared to the size of the data wehad available.Our experimental result verifies the validity inpractice of the assumption widely made in statis-tical natural language processing that class-basedcase slots (and also word-based case slots) are mu-tually independent, at least when the data sizeavailable is that provided by the current versionof the Penn Tree Bank.
This is an empirical find-ing that is worth noting, since up to now the in-dependence assumption was based soMy on hu-24/ /// ...?
...............///~'"" j:/.. /2.5 "Figure 2: (a) Number of dependencies versus data size and (b) KL distance versus data sizeman intuit, ion, to the best of our knowledge.
Totest how large a data size is required to eslimatea class-based model, we conducted the followingexperiment.
We defined an artifMal class-basedmodel and genera.ted some data.
according to itsdistribution.
We then used the data to estimatea class-based model (dendroid distribution), andevaluated the estimated model by measuring themlmber of dependencies (dependency arcs) it hasand the KL distance between the estimated modeland the true model.
We repeatedly generated ataand obserwed the learning 'curve', nan,ely the re-lationship between the number of dependencies inthe estimated model and the data.
size used in esti-mation, and the relationship betweett the KI, dis-tance between the estimated and true modols andthe data size.
We defined two other models andconducted the same experiments.
Figure 2 showsthe results of these experiments for these three ar-tificial models averaged ower tO trials.
(The num-ber of parameters in Modell, Model2, and Model3are 18, 30, and 44 respectiv(_'ly, while the numberof dependencies are 1, 3, aud 5 respectively.)
Wesee that to accurately estimate a model the datasize required is as large as 100 times the nmnberof parameters.
Since a class-based mode\[ tends tohave more than 100 parameters usually, the cur-rent data size available in the Penn Tree Bank isnot enough for accurate stimation of the depen-dencies wilhin case fi'antes of most verbs.5 ConclusionsWe conclude this paper with the following re-marks.1.
The primary contribution of research re-ported in this paper is that we ha.ve proposeda method of learning dependencies betweencase fi'ame slots, which is theoretically somldand elficient, thus 1)roviding au effective toolfor acquiriug (;as(' depend(racy information.2.
For the sk)t-based too(M, sometimes caseslots are found to I)e del)endent.
Experimeu-t.al results demonstrate that using the depen-dency information, when dependency doesexist, structural disambignation results canbe improved.3.
For the word-based or class-based models,case slots are judged independent, with thedata size cm'renl,Iy available in the Penn TreeBank.
This empirical finding verifies the in-dependence assumption widely made in prac-tice in statistical natural anguage processing.We proposed to use dependency forests to repre-sent case frame pa~terns.
It is possible that morecomplicated probabilistic dependency graphs likeBayesian networks would be more appropriate forrepresenting case frame patterns.
This would re-quire even more data and thus the I)roblenl ofhow to collect sufficient data would be.a crucialissue, in addition to the methodology (ff learningcase frame patterns as probabilistic dependencygraphs.
Finally the problem of how to determineobligatory/optional cases based on dependencies(acquired fi'om data.)
should also be addressed.ReferencesEric Bril\] and Philip Resnik.
1994.
A rule-basedapproach to prepositional phrase attaclunentdisantbignation, lb'occediT~gs of the 15lh COl,-\[N(;, pages 1198 -1204.C,.K.
Chow and C',.N.
Liu.
1968.
Approximat-ing discrete probability distributions with de-pendence trees.
\[NEE Transaclions on \[nfor-marion Theory, t4(3):,t62 467.Donald Hindle and Mats Rooth.
1991.
Structuralambiguity and lexical relations.
Proceedings ofthe 29th ACL, pages 229- 236.Hang Li and Naoki Abe.
1995.
Generalizing caseframes using a thesaurus attd the MDL princi-ple.
Proceedings of Recent Advances in NaluralLanguage Processing, pages 239--248.Judea Pearl.
1988.
Probabilistic Reasoning in In-telligent Eyslems: Networks of Plausible Infer-euce.
Morgan Kauflnann Publishers Inc.Jorma Rissanen.
1989.
Slochastic Complexily in5'talistical Inquiry.
World Scientific Publis}tingCo.Joe Suzuki.
1993.
A construction of bayesian et-works fi'om databases based on an MDL princi-ple.
Proceedings of Uncerlainty in A\[ '92.11_5
