Audio Hot Spotting and Retrieval Using Multiple FeaturesQian HuMITRE Corporationqian@mitre.orgFred Goodman, Stanley Boykin,Randy Fish, Warren GreiffMITRE Corporationfgoodman@mitre.org,sboykin@mitre.org,fishr@mitre.orggreiff@mitre.orgAbstractThis paper reports our on-going efforts toexploit multiple features derived from anaudio stream using source material such asbroadcast news, teleconferences, andmeetings.
These features are derived fromalgorithms including automatic speechrecognition, automatic speech indexing,speaker identification, prosodic and audiofeature extraction.
We describe our researchprototype ?
the Audio Hot Spotting System ?that allows users to query and retrieve datafrom multimedia sources utilizing thesemultiple features.
The system aims toaccurately find segments of user interest, i.e.,audio hot spots within seconds of the actualevent.
In addition to spoken keywords, thesystem also retrieves audio hot spots byspeaker identity, word spoken by a specificspeaker, a change of speech rate, and othernon-lexical features, including applause andlaughter.
Finally, we discuss our approach tosemantic, morphological, phonetic queryexpansion to improve audio retrievalperformance and to access cross-lingual data.1.
IntroductionAudio contains more information than isconveyed by the text transcript produced byan automatic speech recognizer [Johnson etal., 2000; Hakkani-Tur et al, 1999].Information such as: a) who is speaking, b)the vocal effort used by each speaker, and c)prosodic features and certain non-speechbackground sounds, are lost in a simplespeech transcript.
In addition, due to thevariability of acoustic channels and noiseconditions, speaker variance, languagemodels the recognizer is based on, and thelimitations of automatic speech recognition(ASR), speech transcripts can be full oferrors.
Deletion errors can prevent the usersfrom finding what they are looking for fromaudio or video data, while insertion andsubstitution errors can be misleading andconfusing.
Our approach is to automaticallydetect, index, and retrieve multiple featuresfrom the audio stream to compensate for theweakness of using speech transcribed textalone.
The multiple time-stamped featuresfrom the audio include an automaticallygenerated index derived from ASR speechtranscripts, automatic speaker identification,and automatically identified prosodic andaudio cues.
In this paper, we describe ourindexing algorithm that automaticallyidentifies potential search keywords that areinformation rich and provide a quick clue tothe document content.
We also describe howour Audio Hot Spotting prototype systemuses multiple features to automatically locateregions of interest in an audio or video filethat meet a user?s specified query criteria.
Inthe query, users may search for keywords orphrases, speakers, keywords and speakerstogether, non-verbal speech characteristics,or non-speech signals of interest.
The systemalso uses multiple features to refine queryresults.
Finally, we discuss our queryexpansion mechanism by using naturallanguage processing techniques to improveretrieval performance and to access cross-lingual data.2.
DataWe use a variety of multimedia data for theexperiments in order to test Audio Hot Spottingalgorithms and performance under differentacoustic and noise environments.
These includebroadcast news (e.g.
HUB4), teleconferences,meetings, and MITRE corporate multimediaevents.
In some cases, synthetic noise was addedto clean source material to test algorithmrobustness.3.
Automatic Spoken KeywordIndexingAs automatic speech recognition is imperfect,automatic speech transcripts contain errors.
Ourindexing algorithm focuses on finding words thatare information rich (i.e.
content words) andmachine recognizable.
Our approach is based onthe principle that short duration and weaklystressed words are much more likely to be mis-recognized, and are less likely to be important.To eliminate words that are information poor andprone to mis-recognition, our algorithm examinesthe speech recognizer output and creates an indexlist of content words.
The index-generationalgorithm takes the following factors intoconsideration: a) absolute word length by itsutterance duration, b) the number of syllables, c)the recognizer?s own confidence score, and d) thepart of speech (i.e.
verb, noun) using a POStagger with some heuristic rules.
Experiments wehave conducted using broadcast news data, withGaussian white noise added to achieve a desiredSignal-to-Noise Ratio (SNR), indicate that theindex list produced typically covers about 10% ofthe total words in the ASR output, while morethan 90% of the indexed words are actuallyspoken and correctly recognized given a WordError Rate (WER [Fiscus, et al]) of 30%.
Thefollowing table illustrates the performance of theautomatic indexer as a function of Signal-to-Noise Ratio during a short pilot study.SNR(dB)ASRWER(%)IndexCoverage(%)IWER(%)Orig.
26.8 13.6 4.324 32.0 12.3 3.318 39.4 10.8 5.912 54.7 8.0 12.26 75.9 3.4 20.63 87.9 1.4 41.7Table 1 Indexer SNR Performancewhere Index Coverage is the fraction of the wordsin the transcript chosen as index words and IWERis the index word error rate.As expected, increases in WER result in fewerwords meeting the criteria for the index list.However, the indexer algorithm manages to findreliable words even in the presence of very noisydata.
At 12dB SNR, while the recognizer WERhas jumped up to 54.7%, the Index Word ErrorRate (IWER) has risen to 12.2%.
Note that anindex-word error indicates that an index wordchosen from the ASR output transcript did not infact occur in the original reference transcription.Whether this index list is valuable willdepend on the application.
If a user wants to get afeel for a 1-hour conversation in just a fewseconds, automatically generated topic terms suchas those described in [Kubala  et al, 2000] or anindex list such as this could be quite valuable.4.
Detecting and Using MultipleFeatures from the AudioAutomatic speech recognition has beenused extensively in spoken document retrieval[Garofolo et al, 2000; Rendals et al, 2000].However, high speech WER in the speechtranscript, especially in less-trained domains suchas spontaneous and non-broadcast quality data,greatly reduces the effectiveness of navigationand retrieval using the speech transcripts alone.Furthermore, the retrieval of a whole document ora story still requires the user to read the wholedocument or listen to the entire audio file in orderto locate the segments where relevant informationresides.
In our approach, we recognize that thereis more information in the audio file than just thewords and that other attributes such as speakeridentification, prosodic features, and the type ofbackground noise may also be helpful for theretrieval of information.
In addition, we aim toretrieve the exact segments of interest rather thanthe whole audio or document so that the user canzero in on these specific segments rapidly.
One ofthe challenges facing researchers is the need toidentify "which" non-lexical features haveinformation value.
Since these features have notbeen available to users in the past, they don'tknow enough to ask for them.
We have chosen toimplement a variety of non-lexical cues with theintent of stimulating feedback from our usercommunity.As an example of this, by extending aresearch speaker identification algorithm[Reynolds, 1995], we integrated speakeridentification into the Audio Hot Spottingprototype to allow a user to retrieve three kinds ofinformation.
First, if the user cannot find whathe/she is looking for using keyword search butknows who spoke, the user can retrieve contentdefined by the beginning and ending timestampsassociated with the specified speaker; assumingenough speech exists to build a model for thatspeaker.
Secondly, the system automaticallygenerates speaker participation statisticsindicating how many turns each speaker spokeand the total duration of each speaker?s audio.Finally, the system uses speaker identification torefine the query result by allowing the user toquery keywords and speaker together.
Forexample, using the Audio Hot Spotting prototype,the user can find the audio segment in whichPresident Bush spoke the word ?anthrax".In addition to speaker identification, wewanted to illustrate the information value of othernon-lexical sounds in the audio track.
As a proof-of-concept, we created detectors for crowdapplause and laughter.
The algorithms used bothspectral information as well as the estimatedprobability density function (pdf) of the raw audiosamples to determine when one of these situationswas present.
Laughter has a spectral envelopewhich is similar to a vowel, but since manypeople are voicing at the same time, the audio hasno coherence.
Applause, on the other hand, isspectrally speaking, much like noisy speechphones such as ?sh?
or ?th.?
However, wedetermined that the pdf of applause differed fromthose individual sounds in the number of highamplitude outlier samples present.
Applying thisalgorithm to the 2003 State of the Union address,we identified all instances of applause with only a2.6% false alarm rate (results were compared withhand-labeled data).
One can imagine a situationwhere a user would choose this non-lexical cue toidentify statements that generated a positiveresponse.Last year, we began to look at speechrate as a separate feature.
Speech rate estimationis important, both as an indicator of emotion andstress, as well as an aid to the speech recognizeritself (see for example [Mirghafori et al, 1996;Morgan, 1998; Zheng et al, 2000]).
Currently,recognizer word error rates are highly correlatedto speech rate.
For the user, marking that areturned passage is from an abnormal speech ratesegment and therefore more likely to containerrors allows him/her to save time by ignoringthese passages or reading them with discretion ifdesired.
However, if passages of high stress areof interest, these are just the passages to bereviewed.
For the recognizer, awareness ofspeech rate allows modification of HMM stateprobabilities, and even permits differentsequences of phones.One approach to determine the speech rateaccurately is to examine the phone-level output ofthe speech recognizer.
Even though the phone-level error rate is quite high, the timinginformation is still valuable for rate estimation.By comparing the phone lengths of the recognizeroutput to phone lengths tabulated over manyspeakers, we have found that a rough estimate ofspeech rate is possible [Mirgafori et al 1996].Initial experiments using MITRE Corporate eventdata have shown a rough correspondence betweenhuman perception of speed and the algorithmoutput.
One outstanding issue is how to treataudio that includes both fast rate speech andsignificant silences between utterances.
Is thistruly fast speech?We are currently conducting research todetect other prosodic features by estimating vocaleffort.
These features may indicate when aspeaker is shouting suggesting elevated emotionsor near a whisper.
Queries based on such featurescan lead to the identification of very interestingaudio hot spots for the end user.
Initialexperiments are examining the spectral propertiesof detected glottal pulses obtained during voicedspeech.5.
Query Expansion and RetrievalTradeoffs5.1  Effect of Passage LengthTREC SDR found both a linear correlationbetween speech word error rate and retrieval rate[Garofolo et al, 2000] and that retrieval wasfairly robust to WER.
However, the robustnesswas attributed to the fact that misrecognizedwords are likely to also be properly recognized inthe same document if the document is longenough.
Since we limit our returned passages toroughly 10 seconds, we do not benefit from thisfull-document phenomenon.
The relationshipbetween passage retrieval rate and passage lengthwas studied by searching 500 hours of broadcastnews from the TREC SDR corpus.
Using 679keywords, each with an error rate across thecorpus of at least 30%, we found that passageretrieval rate was 71.7% when the passage waslimited to only the query keyword.
It increased to76.2% when the passage length was increased to10sec and rose to 83.8% if the returned passagewas allowed to be as long as 120sec.In our Audio Hot Spotting prototype, weexperimented with semantic, morphological, andphonetic query expansion to achieve twopurposes, 1) to improve the retrieval rate ofrelated passages when exact word match fails, and2) to allow cross lingual query and retrieval.5.2  Keyword Query ExpansionThe Audio Hot Spotting prototypeintegrated the Oracle 9i Text engine to expand thequery semantically, morphologically andphonetically.
For morphological expansion, weactivated the stemming function.
For semanticexpansion, we utilized expansion to includehyponyms, hypernyms, synonyms, andsemantically related terms.
For example, whenthe user queried for "oppose", the exact matchyielded no returns, but when semantic andmorphological expansion options are selected, thequery was expanded to include anti, anti-government, against, opposed, opposition, andreturned several passages containing theseexpanded terms.To address the noisy nature of speechtranscripts, we used the phonetic expansion, i.e.
"sound alike" feature from the Oracle databasesystem.
This is helpful especially for propernames.
For example, if the proper name Nesbit isnot in the speech recognizer vocabulary, the wordwill not be correctly transcribed.
In fact, it wastranscribed as Nesbitt (with two 't's).
By phoneticexpansion, Nesbit is retrieved.
We are aware ofthe limitations of Oracle?s phonetic expansionalgorithms, which are simply based on spelling.This doesn?t work well when text is a mis-transcription of the actual speech.
Hypothetically,a phoneme-based recognition engine may be abetter candidate for phonetic query expansion.We are currently evaluating a phoneme-basedaudio retrieval system and comparing itsperformance with a word-based speechrecognition system.
The comparison will help usto determine the strengths and weaknesses of eachsystem so that we can leverage the strength ofeach system to improve audio retrievalperformance.Obviously more is not always better.Some of the expanded queries are not exactlywhat the users are looking for, and the number ofpassages returned increases.
In our Audio HotSpotting implementation we made queryexpansion an option allowing the user to chooseto expand semantically and/or, morphologically,or phonetically.5.3   Cross-lingual Query ExpansionIn some applications it is helpful for auser to be able to query in a single language andretrieve passages of interest from documents inseveral languages.
We treated translingual searchas another form of query expansion.
We created abilingual thesaurus by augmenting Oracle'sdefault English thesaurus with Spanish dictionaryterms.
With this type of query expansionenabled, the system retrieves passages thatcontain the keyword in either English or Spanish.A straightforward extension of this approach willallow other languages to be supported.6.
Future DirectionsAs our research and prototype evolve, weplan to develop algorithms to detect moremeaningful prosodic and audio features to allowthe users to search for and retrieve them.
We arealso developing algorithms that can generatespeaker identify in the absence of speaker trainingdata.
For example, given an audio script, weexpect the algorithms to automatically identifythe number of different speakers present and thetime speaker X changes to Y.
For semantic queryexpansion, we are considering using morecomprehensive thesauri and local context analysisto locate relevant segments to compensate forhigh ASR word error rate.
We are alsoconsidering combining a word-based speechrecognition system with a phoneme-based systemto improve the retrieval performance especiallyfor out of vocabulary words and multi-wordqueries.7.
ConclusionIn this paper, we have shown that byautomatically detecting multiple audio featuresand making use of these features in a relationaldatabase, our Audio Hot Spotting prototypeallows a user to begin to apply the range of cuesavailable in audio to the task of multi-mediainformation retrieval.
Areas of interest can bespecified using keywords, phrases, speakeridentity, prosodic features, and information-bearing background sounds, such as applause andlaughter.
When matches are found, the systemdisplays the recognized text and allows the user toplay the audio or video in the vicinity of theidentified "hot spot".
With the advance ofcomponent technologies such as automatic speechrecognition, speaker identification, and prosodicand audio feature extraction, there will be a widerarray of audio features for the multimediainformation systems to query and retrieve,allowing the user to access the exact informationdesired rapidly.References1.
John Garofolo, et al Nov., 2000.
The TRECSpoken Document Retrieval Track: A SuccessfulStory.
TREC 9.2.
Julia Hirschberg, Steve Whittaker, Don Hindle,Fernando Pereira and Amit Singhal.
April 1999.Finding Information In Audio: A New ParadigmFor Audio Browsing/Retrieval, ESCA ETRWworkshop Accessing information in spoken audio,Cambridge.3.
Sue Johnson, Pierre Jourlin, Karen SparckJones, and Philip Woodland.
Nov., 2000.
SpokenDocument Retireval for TREC-9 at CambridgeUniversity.
TREC-9.4.
John Fiscus, et al Speech Recognition ScoringToolkit   (http://www.nist.gov/speech/tools/)5.
D. Hakkani-Tur, G. Tur, A.Stolcke, E. Shriberg.Combining Words and Prosody for InformationExtraction from Speech.
Proc.
EUROSPEECH'99,6.. N. Mirghafori, E. Fosler, and N. H. Morgan.Towards Robustness to Fast Speech in ASR,Proc.
ICASSP, Atlanta, GA, May 1996.7.
N. Morgan and E. Fosler-Lussier.
CombiningMultiple Estimators of peaking Rate,  Proc.ICASSP-98, pp.
729-732, Seattle, 19988.
M. D. Plumpe, T. F. Quatieri, and D. A.Reynolds.
Modeling of the Glottal FlowDerivative Waveform with Application toSpeaker Identification, IEEE Trans.
On Speechand Audio Processing, September 1999.9.
S. Rendals, and D. Abberley.
The THISL SDRSystem at TREC-9.
TREC-9, Nov., 2000.10.
K. N. Stevens and H. .M.
Hanson.Classification of Glottal Vibration from AcousticMeasurements.
In Vocal Fold Physiology: VoiceQuality Control, Fujimura O., Hirano H.
(eds.
),Singular Publishing Group, San Diego, 1995.11.
J. Zheng, H. Franco, F. Weng, A. Sankar andH.
Bratt.. Word-level Rate-of-Speech ModelingUsing Rate-Specific Phones and Pronunciations,Proc.
ICASSP, vol 3, pp 1775-1778, 200012.
F. Kubala, S. Colbath, D. Liu, A. Srivastava, J.Makhoul.
Integrated Technologies For IndexingSpoken Language, Communications of the ACM,February 2000.13.
D. Reynolds.
Speaker Identification AndVerification Using Gaussian Mixture SpeakerModels, Speech Communications, vol.17, pp.91,1995
