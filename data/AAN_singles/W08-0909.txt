Proceedings of the Third ACL Workshop on Innovative Use of NLP for Building Educational Applications, pages 71?79,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsAn Analysis of Statistical Models and Features for Reading DifficultyPredictionMichael Heilman, Kevyn Collins-Thompson and Maxine EskenaziLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213, USA{mheilman,kct,max}@cs.cmu.eduAbstractA reading difficulty measure can be describedas a function or model that maps a text to anumerical value corresponding to a difficultyor grade level.
We describe a measure of read-ability that uses a combination of lexical fea-tures and grammatical features that are derivedfrom subtrees of syntactic parses.
We alsotested statistical models for nominal, ordinal,and interval scales of measurement.
The re-sults indicate that a model for ordinal regres-sion, such as the proportional odds model, us-ing a combination of grammatical and lexicalfeatures is most effective at predicting readingdifficulty.1 IntroductionA reading difficulty, or readability, measure can bedescribed as a function or model that maps a textto a numerical value corresponding to a difficulty orgrade level.
Inputs to this function are usually statis-tics for various lexical and grammatical features ofthe text.
The output is one of a set of ordered dif-ficulty levels, usually corresponding to grade levelsfor elementary school through high school.
As such,reading difficulty prediction can be viewed as a re-gression of grade level on a set of textual features.Early work on readability measures employedsimple proxies for grammatical and lexical complex-ity, including sentence length and the number of syl-lables in a word.
Fairly simple features were oftenemployed because of a lack of computational power.Such features exhibit high bias because they rely onstrong assumptions about what makes a text difficultto read.
For example, the use of sentence length as ameasure of grammatical complexity assumes that alonger sentence is more grammatically complex thana shorter one, which is often but not always the case.In one early model, the Dale-Chall model (Dale andChall, 1948; Chall and Dale, 1995), reading diffi-culty is a linear function of the mean sentence lengthand the percentage of rare words, as defined by a listof 3,000 words commonly known by 4th grade.
Inthis paper, sentence length is defined as the meannumber of words in the sentences of a text.Many early measures did not employ direct esti-mates of word frequency due to computational lim-itations (e.g., (Gunning, 1952; McLaughlin, 1969;Kincaid et al, 1975)).
Instead, these measures reliedon the strong relationship between the frequency ofand the number of syllables in a word.
More fre-quent words are more likely to have fewer syllables(e.g., ?the?)
than less frequent words (e.g., ?vocab-ulary?
), an association that is related to Zipf?s Law(Zipf, 1935).
The Flesch-Kincaid measure (Kincaidet al, 1975) is probably the most common readingdifficulty measure in use.
It is implemented in com-mon word processing programs.
This measure is alinear function of the mean number of syllables perword and the mean number of words per sentence.Klare (1974) provides a summary of other earlywork on readability.More recent approaches to reading difficulty em-ploy more sophisticated models that make use of thegrowth in computational power.
The Lexile Frame-work (e.g., (Stenner, 1996)) uses individual wordfrequency estimates as a measure of lexical diffi-culty.
The word frequency estimates are derived71from a large, varied corpus of text.
Lexile uses aRasch model (Rasch, 1980) with the mean log wordfrequency as a lexical feature and the log of the meansentence length as a grammatical feature.
The Raschmodel, related to logistic regression, is used to esti-mate the level of a student that would comprehend75% of a given text.
The converted log odds ratiocalled a ?Lexile?
that is used as part of this measurecan be easily mapped to grade school levels.A reading difficulty measure developed byCollins-Thompson and Callan (2005) usessmoothed unigram language modeling to capturethe predictive ability of individual words basedon their frequency at each reading difficulty level.Collins-Thompson and Callan found that certainwords were very predictive of certain levels.
Forexample, ?grownup?
was very predictive of grade1, and ?essay?
was very predictive of grade 12.
Fora given text, this measure estimates the likelihoodthat the text was generated by each level?s languagemodel.
The prediction is the level of the model withthe highest likelihood of generating the text.
Thereare no grammatical features.Natural language processing techniques enablemore sophisticated grammatical analysis for read-ing difficulty measures.
Rather than using sentencelength as a proxy, measures can employ tools for au-tomatic analysis of the syntactic structure of texts(e.g., (Charniak, 2000)).
A measure by Schwarmand Ostendorf (2005) incorporates syntactic analy-ses, among a variety of other types of features.
It in-cludes four grammatical features derived from syn-tactic parses of text: the mean parse tree height, themean number of noun phrases, mean number of verbphrases, and mean number of ?SBARs.?
?SBARs?are non-terminal nodes that are associated with sub-ordinate clauses.
Their system led to better pre-dictions than the Flesch-Kincaid and Lexile mea-sures, but the predictive value of the grammaticalfeatures is not entirely clear.
In initial experimentsusing such course-grain grammatical features alone,rather than in conjunction with language modelingand other features as in Schwarm and Ostendorf?ssystem, we found relatively poor prediction perfor-mance.
Our final approach using subtrees of syn-tactic parses allows for a finer level of discrimina-tion that may support the detection of differences ingrade levels between texts that exhibit the same highlevel features.A reading difficulty measure developed by Heil-man, Collins-Thompson, Callan, and Eskenazi(2007) uses the frequency of grammatical construc-tions as a measure of grammatical difficulty.
A setof approximately twenty constructions were selectedfrom English as a Second Language grammar text-books.
This set includes grammatical constructionssuch as the passive voice, relative clauses, and vari-ous verb tenses.
The frequencies are used as featuresfor a nearest neighbor classification algorithm.
Theunigram language modeling approach of Collins-Thompson and Callan (2005) is used to estimatelexical difficulty in this measure.
The final predic-tion is a linear function of the lexical and grammat-ical components.
That model assumes that gram-matical difficulty is adequately captured by a smallnumber of constructions chosen according to de-tailed knowledge of English grammar.
In that work,the constructions were selected from an English asa Second Language grammar textbook, a labor- andknowledge-intensive task that may be less practicalfor other languages.We aim to identify the appropriate scale of mea-surement for reading difficulty?nominal, ordinal, orinterval?by comparing the effectiveness of statisticalmodels for each type of data.
We also extend pre-vious work combining lexical and grammatical fea-tures (Heilman et al, 2007) by making it possibleto include a large number of grammatical featuresderived from syntactic structures without requiringsignificant linguistic or pedagogical content knowl-edge, such as a reference guide for the grammar ofthe language of interest.2 Types of Features2.1 Lexical FeaturesThis section and the following section describe thelexical and grammatical features used in our read-ing difficulty models.
The lexical features are therelative frequencies of word unigrams.
The use ofword unigrams is a standard approach in text clas-sification (Yang and Pedersen, 1997), and has alsobeen successfully used to predict reading difficulty(Collins-Thompson and Callan, 2005).
Higher ordern-grams such as bigrams and trigrams were not usedas features because they did not improve predictions72in preliminary tests.
The specific set of lexical fea-tures was chosen based on the frequencies of wordsin the training corpus.
The system performs mor-phological stemming and stopword removal.
Theremaining 5000 most common words comprised thelexical feature set.2.2 Grammatical FeaturesGrammatical features are extracted from automaticcontext-free grammar parses of sentences.
The sys-tem computes relative frequencies of partial syn-tactic derivations, which will be called ?subtrees?hereafter.
The approach extends (Heilman et al,2007), where frequencies of manually defined syn-tactic patterns were extracted from syntactic struc-tures.
In that approach, the features are defined man-ually using linguistic knowledge of the target lan-guage to implement tree search patterns, a labor- andknowledge-intensive process.
The approach advo-cated in this paper, however, extracts frequencies foran automatically defined set of subtree patterns.
Thesystem considers all subtrees up to a given depth thatoccur in the training corpus.
Examples of grammati-cal features at levels 0 through 2 are shown in Figure1.
The sentence for the parse tree shown was takenfrom a third grade text.For depth 0, the system includes all subtrees con-sisting of just nonterminal nodes.
This includes allparts of speech, as well as non-terminal nodes fornoun phrases, adjective phrases, clauses, etc.
Fordepth 1, the system includes subtrees correspondingto the application of a single context free grammarrule in the derivation of the tree.
An example of afeature at this level would be a sentence node thatdominates nodes for noun phrases and verb phrases.For deeper levels, the system includes subtrees cor-responding to the successive application of rules onnon-terminals symbols until either a terminal sym-bol is reached or the given depth is reached.
Anexample feature for level 2 is a subtree in whicha prepositional phrase node dominates a preposi-tion node and noun phrase node, and the prepositionnode in turn dominates a preposition, and the nounphrase dominates determiner, adjective, and nounnodes.We used a maximum depth of 3 in our exper-iments.
Features of deeper levels occur less fre-quently in general, and deeper levels were avoideddue to data sparseness.
A depth first search algo-rithm extracts candidate grammatical features fromthe training corpus.
First, a context-free grammarparser (Klein and Manning, 2003) derives parsetrees for all texts in the training corpus.
The algo-rithm traverses these parses, at each node countingall subtree features up to the given depth that arerooted at that node.
The subtree features are sortedby their overall counts in the corpus.
In our ex-periments, frequencies of the most common 1000subtrees were chosen as the final features.
Theseincluded 64 level 0 features corresponding to non-terminal symbols, 334 level 1 features, 461 level 2features, and 141 level 3 features.
Deeper levelshave more possible features, but sparsity at level 3resulted in fewer level 3 features being selected.In our experiments, the subtrees included terminalsymbols for stopwords.
However, the system effec-tively removed content word terminals from parsesbefore extracting features.
The system could bemodified to include terminal symbols for contentwords, or even to ignore all nodes for terminal sym-bols.
Subtree features including terminal symbolsfor content words would, of course, occur with lowfrequency and not likely be included in the final fea-ture set.
Terminal symbols for content words wereomitted so that lexical information was not includedin the set of grammatical features.
Similar to leavinghigher order n-grams out of the lexical feature set,omitting terminal symbols for content words avoidsconfounding grammatical and lexical information inthe grammatical feature set.
Subtree counts are nor-malized by the number of words in a text to computethe relative frequencies.
Normalization by the num-ber of sentences in a text is also possible, but did notperform as well in preliminary tests.
The StanfordParser (Klein andManning, 2003) version 1.5.1 wasused to derive tree structures for sentences.
We usedthe unlexicalized model included in the distributionwhich was trained on Wall Street Journal texts.3 Statistical Models3.1 Scales of Measurement for ReadingDifficultySeveral statistical models were tested for effective-ness at predicting reading difficulty.
The appropri-ateness of these models depends on the nature of73Figure 1: Parse Tree for Sentence from Third Grade Text with Example Subtree Features.reading difficulty data, particularly the scale of mea-surement.
The standard unit for reading difficulty isthe grade level.
First through twelfth grade levels inAmerican schools have been used in previous work(e.g., (Heilman et al, 2007; Collins-Thompson andCallan, 2005)).
English as a Second Language lev-els have also been used (Heilman et al, 2007),as well as grade levels for other languages suchas French (Collins-Thompson and Callan, 2005).While these grades are assigned evenly spaced inte-gers, the ranges of reading difficulty correspondingto these grades are not necessarily evenly spaced.
Itis possible, of course, that assuming even spacingbetween levels might produce more parsimoniousand accurate statistical models.
A more reasonableassumption is that the grade numbers assigned toeach difficulty level denote an ordering: for exam-ple, that grade 1 is in some sense less than grade 2,which is less than grade 3, etc.
Different statisticalmodels handle this assumption more or less well.Statistics generally distinguish four scales of mea-surement, which are, ordered by increasing assump-tions about the relationships between values: nomi-nal, ordinal, interval, and ratio (Stevens, 1946; Co-hen et al, 2003).
Nominal data involve no relation-ships between the labels or classes of the data.
Anexample would be types of fruits, where a modelmight be used to make decisions between apples andoranges.
This type of prediction is generally calledclassification in machine learning and related fields.Ordinal data have a natural ordering, but the val-ues are not necessarily evenly spaced.
For exam-ple, data about the severity of illnesses might havelabels such as mild, moderate, severe, deceased, inwhich the transitions between consecutive classesall have the same direction but not the same mag-nitude.
Making predictions about such data is gen-erally called ordinal regression (McCullagh, 1980).Interval data, however, are both ordered and evenlyspaced.
An example would be temperature as mea-sured in Fahrenheit degrees.
Such data have an ar-bitrary zero point, and negative values may occur.Ratio data, of which annual income is an example,do have a meaningful zero point.
We will not dis-cuss ratio data further since its distinction from in-terval data is not relevant to this paper.
It is not clearto which scale reading difficulty corresponds.
Theassumption of an interval scale allows for simplermodels with fewer parameters.
However, models forordinal or even nominal data might be more appro-priate if the strong assumption of an interval scaledoes not hold.We experimented with three linear and log-linearmodels corresponding to interval, ordinal, and nom-inal data.
Parameters were estimated using L2 reg-ularization, which corresponds to a Gaussian priordistribution with zero mean and a user-specifiedvariance over the parameters.
We chose these mod-els because they are commonly used in the statis-tics, machine learning, and behavioral science com-munities, and aimed to set up meaningful compar-isons among the scales of measurement.
Other ma-chine learning algorithms might also be employed.In fact, we briefly tested the maximummargin (Vap-nik, 1995) approach, which led to comparable re-sults and might be worth exploring in future work.743.2 Linear RegressionLinear Regression (LIN) produces a linear model inwhich the dependent, or outcome, variable is a lin-ear function of the values for predictor variables,or features.
A prediction for a given text is theinner product of a vector of feature values for thetext and a vector of regression coefficients estimatedfrom training data.
For the case of reading difficulty,the grade level is a linear combination of the lexi-cal and/or grammatical feature values.
LIN providescontinuous estimates of reading difficulty, such thata prediction might fall between grade levels.
Theestimates were not rounded to whole numbers in theexperiments.
For rare cases of an LIN predictionfalling outside the appropriate range of grade levels,the value was set to the maximum or minimum gradelevel.
LIN implicitly assumes that the data fall onan interval scale, meaning that the levels are evenlyspaced.
The LIN model has relatively few parame-ters but makes strong assumptions about the scale ofmeasurement.
For details, see (Hastie et al, 2001).3.3 Proportional Odds ModelThe Proportional Odds (PO) model, also called theparallel regression model and the cumulative logitmodel, is a form of log-linear, or exponential, modelfor ordinal data (McCullagh, 1980).
Given a newunlabeled instance as input, the model provides es-timates of the probability that the instance belongsto a class at or above a particular level.
In Equation(1), P (y ?
j) is this estimated probability, ?j is anintercept parameter for the given level j, ?
is vectorof regression coefficients, Xi is the vector of featurevalues for instance i, and yi is the predicted readingdifficulty level.P (yi ?
j) =exp(?j + ?TXi)1 + exp(?j + ?TXi)(1)lnP (yi ?
j)1?
P (yi ?
j)= ?j + ?TXi (2)The PO model has a parameter ?j for the thresh-old, or intercept, at each level j, but only a single set?
of parameters for the features.
These two types ofparameters correspond to an implicit assumption ofordinality.
Having a single set of parameters for fea-tures across the levels means that changes in featurevalues proportionally affect the odds of transitioningfrom any one class to another.The estimated probability of an instance belong-ing to a particular class is the difference between es-timates for that class and the next highest class.
Forexample, the estimated probability of a text beingat the eighth grade level would be the estimate forbeing at or above eighth grade minus the estimatefor being at or above ninth grade.
As in binary lo-gistic regression, the PO model estimates log oddsratios based on the values of features or predictorvariables.
The numerator of the odds ratio is theprobability of being at or above a level, and the de-nominator is the probability of being below a level.Equation (2) shows the form of the model that islinear in the parameters.3.4 Multi-class Logistic RegressionMulti-class Logistic Regression (LOG), or multino-mial logit regression, is a log-linear model for nom-inal data.
In contrast to the simpler PO model, themodel maintains parameters for all of the featuresfor every class except one category, which is usedfor comparison.
Thus, for reading difficulty, thereare about 11 times as many parameters to estimatecompared to LIN and PO.
The increased difficultyof parameter estimation for this model is offset fordomains in which assumptions of ordinality or lin-earity do not hold.
For more details, see (Hastie etal., 2001).4 Evaluation4.1 Web CorpusThe corpus of materials used for training and test-ing the models consists of the content text extractedfrom Web pages with reading difficulty level labels.Web pages were used because the system for pre-dicting reading difficulty is being used as part of theREAP tutoring system, which finds authentic andappropriate Web pages for English vocabulary prac-tice (Brown and Eskenazi, 2004; Heilman et al,2006).
Approximately half of these texts were au-thored by students at the particular grade level, andhalf were authored by teachers or writers and aimedat readers at a particular grade level.
Texts werefound for grade levels 1 through 12.
The twelfthgrade level also included some post-secondary level75texts.
Various genres and subjects were represented.In all cases, either the text itself or a link to it iden-tified it as having a certain level.
The content textwas manually extracted from these Web pages sothat noisy information such as navigation menus andadvertisements were not included.
Automatic con-tent extraction may, however, be able to remove suchnoisy information without human intervention (e.g.,(Gupta et al, 2003)).
This Web corpus is adaptedfrom the corpora used in prior work on reading dif-ficulty predication (Collins-Thompson and Callan,2005; Heilman et al, 2007).
We modified that cor-pus because it contained a number of documentspertaining to mathematics and vocabulary practice.The majority of tokens in these texts were not partof well-formed, grammatical sentences suitable forreading practice.
Since our goal is to measure thedifficulty of reading passages, we removed thesedocuments and added additional texts consisting ofmore suitable reading material.
The corpus con-sisted of approximately 150,000 words, distributedamong 289 texts.
The number of texts for each gradelevel was approximately the same, with at least 28texts at each level.
The mean length in words ofthe texts was approximately 500 words, which corre-sponds to about a page.
Texts for lower grades werenecessarily shorter.
We extracted excerpts for higherlevel texts so that texts were otherwise roughly equalin length across levels.
For these excerpts, the first500 or so words of text were extracted, while re-specting sentence and paragraph boundaries.4.2 Evaluation MetricsRoot mean square error (RMSE), Pearson?s correla-tion coefficient, and accuracy within 1 grade levelserved as metrics for evaluating the performanceof reading difficulty predictions.
Multiple statisticswere used because it is not entirely clear what thebest measure of prediction quality is for reading dif-ficulty.
RMSE is the square root of the empiricalmean of the squared error of predictions.
It morestrongly penalizes those errors that are further awayfrom the true value.
It can be interpreted as the aver-age number of grade levels that predictions measuredeviate from human-assigned labels.Pearson?s correlation coefficient measures thestrength of the linear relationship, or similarity oftrends, between two random variables.
A high corre-lation would indicate that difficult texts would morelikely receive high predicted difficulty values, andeasier texts would be more likely to receive low pre-dicted difficulty values.
Correlations do not, how-ever, measure the degree to which values match inabsolute terms.Adjacent accuracy is the proportion of predic-tions that were within one grade level of the human-assigned label for the given text.
Exact accuracy istoo stringent a measure because the human-assignedreading levels are not always perfect and consis-tent.
For example, one school might read ?Romeoand Juliet?
in 9th grade while another school mightread it in 10th grade.
The drawback of this accuracymetric is that predictions that are two levels off aretreated the same as predictions that are ten levels off.4.3 BaselinesThe performance of other algorithms for estimat-ing reading difficulty was estimated using the samedata.
These comparison include Collins-Thompsonand Callan?s implementation of their language mod-eling approach (2005), an implementation of theFlesch-Kincaid reading level measure (Kincaid etal., 1975), and a measure using word frequency andsentence length similar to Lexile (Stenner et al,1983).
We did not directly test the approach de-scribed by (Heilman et al, 2007).
We observethat its reported results for first language texts werenot significantly different in terms of correlation andonly slightly better in terms of mean squared er-ror than the language modeling approach.
Finally,a simple uniform baseline, which always chose themiddle value of 6.5, was tested.The Lexile-like measure (LX) used the same twofeatures as the Lexile measure: mean log frequencyor words and log mean sentence length.
Instead ofusing a Rasch model and converting scores to ?Lex-iles,?
however, the PO model was used to directlypredict grade levels.
The log frequency values forwords were estimated from the second release of theAmerican National Corpus (Reppen et al, 2005),a 20 million word corpus with texts in AmericanEnglish from different genres on a variety of sub-jects.
Using the proportional odds models is effec-tively equivalent to using Lexile?s Rasch model andmapping its output to grade levels.
The major differ-ence between the Lexile measure and the implemen-76tation used in these experiments is the training datasets used to estimated word frequencies and modelparameters.4.4 ProcedureThe Web Corpus was randomly split into trainingand test sets.
The test set consisted of 25% of theindividual texts at each level, a total of 84 texts.Ten-fold stratified cross-validation on the trainingset was employed to estimate the prediction per-formance according to the evaluation metrics.
Incross-validation, data are partitioned randomly intoa given number of folds, and each fold is used fortesting while all others are used for training.
Formore details and a discussion of validation meth-ods, see (Hastie et al, 2001).
The regularizationhyper-parameters were tuned on the training set dur-ing cross-validation by a simple grid search.
Aftercross-validation, models were trained on the entiretraining set, and then evaluated using the held-outtest data.We tested whether each feature-set, algorithm pairor baseline performed significantly differently thanour hypothesized best model, the PO model withthe combined feature set.
We employed the bias-corrected and accelerated (BCa) Bootstrap (Efronand Tibshirani, 1993) with 50,000 replications of theheld-out test data to generate confidence intervalsfor differences in evaluation results.
If the (1??
)%confidence intervals for the difference do not con-tain zero, which is the value corresponding to thenull hypothesis, then that difference is significant atthe ?
level.
For example, the 99% confidence inter-val for the difference in adjacent accuracy betweenthe language modeling baseline and the PO modelwith the combined feature set was (-1.86, -0.336),indicating that this difference is significant at the .01level since it does not contain zero.5 ResultsTable 1 presents correlation coefficients, RMSE val-ues, and accuracy values for cross-validation andheld-out test data.
Statistical significance was testedonly for the held-out test data since the hyper-parameters were tuned during cross-validation.
Ourdiscussion of the results pertains mostly to the eval-uation on the test-set.Of the various statistical models, the PO modelfor ordinal data appears to provide superior perfor-mance over the LIN and LOG models.
Comparedto the LOG model, the PO model performs sig-nificantly better in terms of correlation and RMSEand comparably well in terms of adjacent accuracy.Compared to the LINmodel, the POmodel performsalmost as well in terms of correlation, comparablywell in terms of RMSE, and far better in terms ofaccuracy.The performance of the methods when using dif-ferent feature sets does not clearly indicate a best setof features to use for predicting reading difficulty.For the PO model, none of the feature sets lead tosignificant gains over the others in terms of any ofthe metrics.
However, the combined feature set ledto the best performance in terms of correlation andadjacent accuracy during cross-validation as well asRMSE on the test set, suggesting at the very leastthat including the extra features does not degradeperformance.The PO model with the combined feature set out-performed most of the baseline measures.
LX hadthe same accuracy value on the test set.
The LXmethod appears to perform the best in general ofthe baselines models.
Interestingly, LX uses pro-portional odds logistic regression like PO, and thusassumes an ordinal but not interval scale of measure-ment.
RMSE values were significantly lower for thePO model than for LX and the language modelingapproach.No statistically significant advantages are seenfor PO model when compared to Flesch-Kincaid.We observe however, that for the sample of webpages which constitutes the evaluation corpus thePO model produced superior results across evalua-tion metrics.
That is, PO performed better in termsof adjacent accuracy, RMSE, and correlation coeffi-cients, both in cross-validation and testing with held-out data.6 DiscussionIn our tests, the PO model, which assumes ordinaldata, lead to the most effective predictions of read-ing difficulty in general.
This result indicates that thereading difficulty of texts, according to grade level,lies on an ordinal scale of measurement.
That is,77Method Features Cross-Validation Held-Out Test SetCorrel.
RMSE Adj.
Acc.
Correl.
RMSE Adj.
Acc.LIN Lexical .629 2.73 .242 .779 2.42 .167**Grammatical .767 2.26 .294 .753 2.33 .274*Combined .679 2.57 .284 .819** 2.21 .226**PO Lexical .713 2.57 .498 .780 2.29 .464Grammatical .762 2.22 .505 .734 2.42 .560Combined .773 2.24 .519 .767 2.23 .440LOG Lexical .517 3.24 .443 .619* 2.83* .548Grammatical .632 2.87 .443 .506** 3.38** .464Combined .582 2.94 .446 .652* 2.71* .556LX - .659 2.77 .467 .731 2.67* .464Lang.
Modeling - .590 2.74 .370 .630 2.70** .381Flesch-Kincaid - .697 2.66 .388 .718 2.54 .369Uniform - .000 3.39 .170 .000** 3.45** .167**Table 1: Results from Cross-Validation and Test Set Evaluations, as measured by Correlation Coefficients (Correl.
),Root Mean Square Error (RMSE), and Adjacent Accuracy.
The best result for each metric for each evaluation isgiven in bold.
Asterisks indicate significant differences compared to the PO model with a Combined Feature Set.
* =p < .05, ** = p < .01.reading difficulty appears to increase steadily but notlinearly with grade level.
As such, the LIN approachthat produces linear models was less effective, par-ticularly in terms of adjacent accuracy.
The LOGmodel, for nominal data, also led to inferior perfor-mance compared to the PO model, which can be at-tributed to the difficulty of accurately estimating amore complex model with many parameters for eachlevel.Our tests found that grammatical features alonecan be effective predictors of readability.
This find-ing disagrees with a previous result that found that amodel using a combination of lexical and manuallydefined grammatical features (Heilman et al, 2007)outperformed a model using grammatical featuresalone.
The superior predictive ability of the mod-els we describe that use grammatical features can beattributed to the automatic derivation of a grammat-ical feature set that is more than an order of magni-tude larger than in the previous approach.
Our ap-proach enables the use of much larger grammaticalfeature sets because it does not require the extensivelinguistic knowledge and effort to manually definethe grammatical features.
The automatic approachalso enables an easier transition to other languages,assuming a parser is available.
Using the combinedfeature set did not hurt performance, however, andsince regularized statistical models can avoid over-fitting large numbers of parameters, a combined fea-ture set still seems appropriate.AcknowledgmentsWe thank Jamie Callan for his comments and sug-gestions.
This research was supported in part bythe Institute of Education Sciences, U.S. Depart-ment of Education, through Grant R305B040063 toCarnegie Mellon University; Dept.
of Educationgrant R305G03123; the Pittsburgh Science of Learn-ing Center which is funded by the National Sci-ence Foundation, award number SBE-0354420; anda National Science Foundation Graduate ResearchFellowship awarded to the first author.
Any opin-ions, findings, conclusions, or recommendations ex-pressed in this material are the authors, and do notnecessarily reflect those of the sponsors.ReferencesJon Brown and Maxine Eskenazi.
2004.
Retrieval of au-thentic documents for reader-specific lexical practice.Proceedings of InSTIL/ICALL Symposium 2004.78J.
S. Chall and E. Dale.
1995.
Readability Revisited:The New Dale-Chall Readability Formula.
BrooklineBooks.
Cambridge, MA.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
Proceedings of the NAACL.J.
Cohen, P. Cohen, S. G. West, and L. S. Aiken.
2003.Applied Multiple Regression/Correlation Analysis forthe Behavioral Sciences, 3rd Edition.
Lawrence Erl-baum Associates, Inc.Michael Collins and Nigel Duffy.
2002.
ConvolutionKernels for Natural Language.
Advances in Neural In-formation Processing Systems..Kevyn Collins-Thompson and Jamie Callan.
2005.Predicting reading difficulty with statistical languagemodels.
Journal of the American Society for Informa-tion Science and Technology, 56(13).
pp.
1448-1462..E. Dale and J. S. Chall.
1948.
A Formula for PredictingReadability.
Educational Research Bulletin Vol.
27,No.
1.Bradley Efron and Robert J. Tibshirani.
1993.
An In-troduction to the Bootstrap.
Chapman and Hall, NewYork.R.
Gunning.
1952.
The technique of clear writing..McGraw-Hill, New York.S.
Gupta, G. Kaiser, D. Neistadt, and P. Grimm.
2003.DOM-based content extraction of HTML documents.ACM Press, New York.Trevor Hastie, Robert Tibshirani, Jerome Friedman.2003.
The Elements of Statistical Learning:Data Min-ing, Inference, and Prediction.
Springer.Michael Heilman, Kevyn Collins-Thompson, JamieCallan, and Maxine Eskenazi.
2007.
Combining Lex-ical and Grammatical Features to Improve ReadabilityMeasures for First and Second Language Texts.
Pro-ceedings of the Human Language Technology Confer-ence.
Rochester, NY.Michael Heilman, Kevyn Collins-Thompson, JamieCallan, and Maxine Eskenazi.
2006.
Classroom suc-cess of an Intelligent Tutoring System for lexical prac-tice and reading comprehension.
Proceedings of theNinth International Conference on Spoken LanguageProcessing.
Pittsburgh, PA.J.
Kincaid, R. Fishburne, R. Rodgers, and B. Chissom.1975.
Derivation of new readability formulas for navyenlisted personnel.
Branch Report 8-75.
Chief ofNaval Training, Millington, TN.G.
R. Klare.
1974.
Assessing Readability.
Reading Re-search Quarterly, Vol.
10, No.
1. pp.
62-102..Dan Klein and Christopher D. Manning.
2003.
AccurateUnlexicalized Parsing.
Proceedings of the 41st Meet-ing of the Association for Computational Linguistics,pp.
423-430.G.
H. McLaughlin.
1969.
SMOG grading: A new read-ability formula.
Journal of Reading.P.
McCullagh.
1980.
Regression Models for OrdinalData.
Journal of the Royal Statistical Society.
SeriesB (Methodological), Vol.
42, No.
2. pp.
109-142.G.
Rasch.
1980.
Probabilistic Models for Some Intelli-gence and Attainment Tests.
MESA Press, Chicago,IL.G.
Rasch.
2005.
American National Corpus (ANC) Sec-ond Release.. Linguistic Data Consortium.
Philadel-phia, PA.Sarah Schwarm and Mari Ostendorf.
2005.
Read-ing level assessment using support vector machinesand statistical language models.
Proceedings of the43rd Annual Meeting on Association for Computa-tional Linguistics.A.
J. Stenner, M. Smith, and D. S. Burdick.
1983.
To-ward a Theory of Construct Definition.
Journal of Ed-ucational Measurement, Vol.
20, No.
4. pp.
305-316.A.
J. Stenner.
1996.
Measuring reading comprehensionwith the Lexile framework.
Fourth North AmericanConference on Adolescent/Adult Literacy.S.
S. Stevens.
1946.
On the theory of scales of measure-ment.
Science, 103, pp.
677-680.V.
N. Vapnik.
1995.
The Nature of Statistical LearningTheory.
Springer.Y.
Yang and J. P. Pedersen.
1997.
A Comparative Studyon Feature Selection in Text Categorization.
Proceed-ings of the Fourteenth International Conference onMachine Learning (ICML?97), pp.
412-420.G.
K. Zipf.
1935.
The Psychobiology off Language.Houghton Mifflin, Boston, MA.79
