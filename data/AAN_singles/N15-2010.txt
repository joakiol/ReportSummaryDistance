Proceedings of NAACL-HLT 2015 Student Research Workshop (SRW), pages 71?78,Denver, Colorado, June 1, 2015.c?2015 Association for Computational LinguisticsBenchmarking Machine Translated Sentiment Analysis for Arabic TweetsEshrag RefaeeInteraction Lab, Heriot-Watt UniversityEH144AS Edinburgh, UKeaar1@hw.ac.ukVerena RieserInteraction Lab, Heriot-Watt UniversityEH144AS Edinburgh, UKv.t.rieser@hw.ac.ukAbstractTraditional approaches to Sentiment Anal-ysis (SA) rely on large annotated data sets orwide-coverage sentiment lexica, and as suchoften perform poorly on under-resourced lan-guages.
This paper presents empirical evi-dence of an efficient SA approach using freelyavailable machine translation (MT) systems totranslate Arabic tweets to English, which wethen label for sentiment using a state-of-the-art English SA system.
We show that this ap-proach significantly outperforms a number ofstandard approaches on a gold-standard held-out data set, and performs equally well com-pared to more cost-intense methods with 76%accuracy.
This confirms MT-based SA as acheap and effective alternative to building afully fledged SA system when dealing withunder-resourced languages.Keywords: Sentiment Analysis, Arabic, Twitter,Machine Translation1 IntroductionOver the past decade, there has been a growing in-terest in collecting, processing and analysing user-generated text from social media using SentimentAnalysis (SA).
SA determines the polarity of agiven text, i.e.
whether its overall sentiment is neg-ative or positive.
While previous work on SA forEnglish tweets reports an overall accuracy of 65-71% on average (Abbasi et al, 2014), recent stud-ies investigating Arabic tweets only report accuracyscores ranging between 49-65% (Mourad and Dar-wish, 2013; Abdul-Mageed et al, 2012; Refaee andRieser, 2014b).
Arabic SA faces a number of chal-lenges: first, Arabic used in social media is usuallya mixture of Modern Standard Arabic (MSA) andone or more of its dialects (DAs).
Standard toolk-its for Natural Language Processing (NLP) mainlycover the former and perform poorly on the latter1.These tools are vital for the performance of machinelearning (ML) approaches to Arabic SA: tradition-ally, ML approaches use a ?bag of words?
(BOW)model (e.g.
Wilson et al (2009)).
However, formorphologically rich languages, such as Arabic, amixture of stemmed tokens and morphological fea-tures have shown to outperform BOW approaches(Abdul-Mageed et al, 2011; Mourad and Darwish,2013), accounting for the fact that Arabic contains avery large number of inflected words.
In addition (ormaybe as a result), there is much less interest fromthe research community in tackling the challenge ofArabic SA for social media.
As such, there are muchfewer open resources available, such as annotateddata sets or sentiment lexica.
We therefore explorean alternative approach to Arabic SA on social me-dia, using off-the-shelf Machine Translation systemsto translate Arabic tweets into English and then usea state-of-the-art sentiment classifier (Socher et al,2013) to assign sentiment labels.
To the best of ourknowledge, this is the first study to measure the im-pact of automatically translated data on the accuracyof sentiment analysis of Arabic tweets.
In particular,we address the following research questions:1.
How does off-the-shelf MT on Arabic socialdata influence SA performance?1Please note the ongoing efforts on extending NLP tools toDAs (e.g.
(Pasha et al, 2014; Salloum and Habash, 2012)).712.
Can MT-based approaches be a viable alterna-tive to improve sentiment classification perfor-mance on Arabic tweets?3.
Given the linguistic resources currently avail-able for Arabic and its dialects, is it more ef-fective to adapt an MT-based approach insteadof building a new system from scratch?2 Related WorkThere are currently two main approaches to auto-matic sentiment analysis: using a sentiment lexi-con or building a classifier using machine learning.Lexicon-based approaches, on the one hand, utilisesentiment lexica to retrieve and annotate sentimentbearing word tokens for their sentiment orientationand then utilise a set of rules to assign the overallsentiment label (Taboada et al, 2011).
MachineLearning (ML) approaches, on the other hand, fre-quently make use of annotated data sets, to learn astatistical classifier (Mourad and Darwish, 2013;Abdul-Mageed et al, 2011; Wilson et al, 2009).These approaches gain high performance for En-glish tweets: a benchmark test on commercial andfreely-available SA tools report accuracy levels be-tween 65% - 71% on English tweets (Abbasi et al,2014).For Arabic tweets, one of the best results for SAto date is reported in Mourad and Darwish (2013)with 72.5% accuracy using 10-fold-cross validationand SVM on a manually annotated data set (2300tweets).
However, this performance drops dramat-ically to 49.65% - 65.32% accuracy when testingan independent held-out set (Abdul-Mageed et al,2012; Refaee and Rieser, 2014c).
One possibleexplanation is the time-changing nature of twitter(Eisenstein, 2013): models trained on data collectedat one point in time will not generalise to tweets col-lected at a later stage, due to changing topics and vo-cabulary.
As such, current work investigates DistantSupervision (DS) to collect and annotate large datasets in order to train generalisable models (e.g.
Goet al (2009)).
Recent work by Refaee and Rieser(2014b) has evaluated DS approaches on ArabicTweets.
They report accuracy scores of around 57%which significantly outperforms a majority baselineand a fully supervised ML approach, but it is stillconsiderably lower than scores achieved on Englishtweets.In the following, we compare these previous ap-proaches to an approach using automatic MachineTranslation (MT).
So far, there is only limited ev-idence that this approach works for languages lacklarge SA training data-set, such as Arabic.
Bautinet al (2008) investigate MT to aggregate sentimentfrom multiple news documents written in a numberof different languages.
The authors argue that de-spite the difficulties associated with MT, e.g.
infor-mation loss, the translated text still maintains a suffi-cient level of captured sentiments for their purposes.This work differs from our work in terms of domainand in measuring summary consistency rather thanSA accuracy.
Balahur and Turchi (2013) investigatethe use of an MT system (Google) to translate an an-notated corpus of English tweets into four Europeanlanguages in order to obtain an annotated trainingset for learning a classifier.
The authors report anaccuracy score of 64.75% on the English held-outtest set.
For the other languages, reported accuracyscores ranged between 60 - 62%.
Hence, they con-clude that it is possible to obtain high quality train-ing data using MT, which is an encouraging result tomotivate our approach.Wan (2009) proposes a co-training approach totackle the lack of Chinese sentiment corpora by em-ploying Google Translate as publicly available ma-chine translation (MT) service to translate a set ofannotated English reviews into Chinese.
Using aheld-out test set, the best reported accuracy scorewas at 81.3% with SVM on binary classificationtask: positive vs negative.Our approach differs from the ones described, inthat we use automatic MT to translate Arabic tweetsinto English and then perform SA using a state-of-the-art SA classifier for English (Socher et al,2013).
Most importantly, we empirically benchmarkits performance towards previous SA approaches,including lexicon-based, fully supervised and dis-tant supervision SA.3 Experimental Setup3.1 Data-setWe follow a similar approach to Refaee and Rieser(2014a) for collecting the held-out data set we usefor benchmarking.
First, we randomly retrieve72tweets from the Twitter public stream.
We restrictthe language of all retrieved tweets to Arabic by set-ting the language parameter to ar.
The data-set wasmanually labeled with gold-standard sentiment ori-entation by two native speakers of Arabic, obtain-ing a Kappa score of 0.81, which indicates highlyreliable annotations.
Table 1 summarises the dataset and its distribution of labels.
For SA, we per-form binary classification using positive and nega-tive tweets.
We apply a number of common pre-processing steps following Go et al (2009) and Pakand Paroubek (2010) to account for noise introducedby Twitter.
The data set will be released as part ofthis submission.Sentiment Pos.
Neg.
Totalno.
of tweets 470 467 937no.
of tokens 4,516 5,794 10,310no.
of tok.
types 2,664 3,200 5,864Table 1: Evaluation data-set.3.2 MT-based approachIn order to obtain the English translation of our Twit-ter data-set, we employ two common and freely-available MT systems: Google Translate and Mi-crosoft Translator Service.
We then use the StanfordSentiment Classifier (SSC) developed by Socher etal.
(2013) to automatically assign sentiment labels(positive, negative) to translated tweets.
The classi-fier is based on a deep learning (DL) approach, usingrecursive neural models to capture syntactic depen-dencies and compositionality of sentiments.
Socheret al (2013) show that this model significantly out-performs previous standard models, such as Na?
?veBayes (NB) and Support Vector Machines (SVM)with an accuracy score of 85.4% for binary classi-fication (positive vs. negative) at sentence level2.The authors observe that the recursive models workwell on shorter text while BOW features with NBand SVM perform well only on longer sentences.Using Socher et al (2013)?s approach for directlytraining a sentiment classifier will require a largertraining data-set, which is not available yet for Ara-2SSC distinguishes between 5 sentiments, including very-positive, positive, neutral, negative, and very-negative.
For ourpurposes, all very-positive and very-negative were mapped tothe standard positive and negative classes.bic3.3.3 Baseline SystemsWe benchmark the MT-approach against threebaseline systems representing current standard ap-proaches to SA: a lexicon-based approach, a fullysupervised machine learning approach and a dis-tant supervision approach (also see Section 2).
Thelexicon-based baseline combines three sentimentlexica.
We exploit two existing subjectivity lex-ica: a manually annotated Arabic subjectivity lexi-con (Abdul-Mageed and Diab, 2012) and a publiclyavailable English subjectivity lexicon, called MPQA(Wilson et al, 2009), which we automatically trans-late using Google Translate, following a similartechnique to Mourad and Darwish (2013).
Thetranslated lexicon is manually corrected by remov-ing translations with a no clear sentiment indicator4.
This results in 2,627 translated instances aftercorrection.
We then construct a third dialectal lex-icon of 484 words that we extract from an indepen-dent Twitter development set and manually annotatefor sentiment.
All lexica are merged into a com-bined lexicon of 4,422 annotated sentiment words(duplicates removed).
In order to obtain automaticlabels for positive and negative instances, we followa simplified version of the rule-based aggregationapproach of Taboada et al (2011).
First, all lexi-cons and tweets are lemmatised using MADAMIRA(Pasha et al, 2014).
For each tweet, matched senti-ment words are marked with either (+1) or (-1) to in-corporate the semantic orientation of individual con-stituents.
This achieves a coverage level of 76.62%(which is computed as a percentage of tweets withat least one lexicon word) using the combined lexi-con.
To account for negation, we reverse the polarity(switch negation) following Taboada et al (2011).The sentiment orientation of the entire tweet is thencomputed by summing up the sentiment scores ofall sentiment words in a given tweet into a singlescore that automatically determines the label as be-ing: positive or negative.
Instances where the scoreequals zero are excluded from the training set as they3SSC was trained using a set of 215,154 unique and manu-ally labeled phrases.4For instance, the day of judgement is assigned with a nega-tive label while its Arabic translation is neutral considering thecontext-independent polarity.73Metrics Google-Trans.+DL Microsoft-Trans.+DL Lexicon-based Distant Superv.
Fully-supervisedPos.
Neg.
Pos.
Neg.
Pos.
Neg.
Pos.
Neg.
Pos.
Neg.precision 44.64 92.52 56.60 91.60 75.87 77.72 52.1 73.3 48.2 59.7avg.
precision 68.58 74.10 76.79 63.5 54.3recall 21.27 55.67 25.53 53.74 36.81 32.12 86.6 31.7 89.4 14.1avg.
recall 38.47 39.63 34.46 57.1 49.7F-score 28.81 69.52 35.19 67.74 49.57 45.45 65.1 44.2 0.627 22.8avg.
F-score 49.16 51.46 47.51 53.9 41.6accuracy 71.28 76.34 76.72 57.06 49.65Table 2: Benchmarking Arabic sentiment classification: results for positive vs. negativerepresent mixed-sentiment instances with an evennumber of sentiment words.The fully-supervised ML baseline uses a freelyavailable corpus of gold-standard annotated Arabictweets (Refaee and Rieser, 2014c) to train a classifierusing word n-grams and SVMs (which we found toachieve the best performance amongst a number ofother machine learning schemes we explored).The Distant Supervision (DS) baseline useslexicon-based annotation to create a training set of134,069 automatically labeled tweets (using the ap-proach we described for the lexicon-based baseline),where the identified sentiment-bearing words are re-placed by place-holders to avoid bias.
We then usethese noisy sentiment labels to train a classifier us-ing SVMs.
Note that previous work has also experi-mented with emoticon-based DS, but has found thata lexicon-based DS approach leads to superior re-sults (Refaee and Rieser, 2014b).4 Experiment ResultsTable 2 summarises the results for comparing theabove baselines to our MT-based approaches (usingGoogle and Microsoft MT), reporting on per-classand average recall, precision and F-measure.
Wealso measure statistical significance by performinga planned comparison between the top-performingapproaches (namely, the lexicon-based baseline andthe two MT systems) using ?2with Bonferroni cor-rection on binary accuracy values (see Table 3).
Weobserve the following:?
In general, MT-based approaches reach a similarperformance to the more resource-intense baselinesystems.
There is no significant distance in ac-curacy between the MT-based approaches and theoverall best performing lexicon-based approach.?
Microsoft MT significantly outperforms GoogleMT for this task.?
Overall, the fully supervised baseline performsworst.
A possible explanation for that is the time-changing nature of Twitter resulting in issues liketopic-shift resulting in word token-based featuresbeing less effective in such a medium (Refaee andRieser, 2014c).?
MT-based SA approaches in general have a prob-lem of identifying positive tweets (low recall andprecision), often misclassifying them as negative.The reverse it true for the DS and fully super-vised baselines, which find it hard to identifynegative tweets.
This is in line with results re-ported by Refaee and Rieser (2014b) which evalu-ate DS approaches to Arabic SA.
Only the lexicon-approach is balanced between the positive andnegative class.
Note that our ML baseline systemsas well as the English SA classifier by Socher etal.
(2013) are trained on balanced data sets, i.e.
wecan assume no prior bias towards one class.Planned Contrasts ?2(p) EffectSize (p)Google MT vs. MicrosoftMT273.67(p=0.000)*0.540(p=0.000)*Microsoft MT vs. lexicon-based1.64(p=0.206)0.042(p=0.200)lexicon-based vs. GoogleMT3.32(p=0.077)0.060(p=0.068)Table 3: Comparison between top approaches with re-spect to accuracy; * indicates a sig.
difference at p<0.0014.1 Error AnalysisThe above results highlight the potential of an MT-based approach to SA for languages that lack a large74Example Tweet Human Translation Auto Translation Manual Auto Label1?
?A?
AJK A?QK.Y ?
?
????X????
@ ?Q?
@ ??
?j?
?Crown Prince of Britainlooks very elegant in theSaudi attireCrown Prince of Britainclimber Kchkh in Saudioutfitpositive negative2, Y?B@ ?@X??
?J.??
@ @Y??Q??K.????
?J?A?K?<?
@That cub is from that lion,God bless you with ahealthy and long lifeThat drops of Assad Godheal and go on your agepositive negative3?Y??AK.Y?m?
?mkQ?Muhammad?s happinesswith scoring a goalFarahhh Muhammad goal positive negative4AKP??
??
@ Q??
@ ?
<?
@ AK?PQ?@??
?BAK.Oh God, shower people ofSyria with safety and liveli-hoodOh God rained folks Syriasecurity and livelihoodpositive negative5AK @ AKA?
?
?
?K @?A?
??I.kIJ?J?
@ , ?AJkIJ?J?
@Because you are with me,I?m full of life and loveAnd Ashan you having IAmtlat Amtlat love lifepositive negative6?G.X ??
?
J??
?
m?
'@ ?
??
?
@?jJ ??
???
k@Q??.??
?P , QKY?J?
@Frankly, the GovernmentSummit in Dubai is asplended work that de-serves recognitionGovernment summit inDubai Frankly workdeserves recognition,splendorpositive negativeTable 4: Examples of misclassified tweetstraining data-set annotated for sentiment analysis,such as Arabic.
In the following, we conduct a de-tailed error analysis to fully understand the strengthand weaknesses of this approach.
First, we inves-tigate the superior performance of Microsoft overGoogle MT by manually examining examples whereMicrosoft translated data is assigned the correct SAlabel, but the reverse is true for Google translateddata, which is the case for 108 instances of our testset (11.5%).
This analysis reveals that the main dif-ference is the ability of Microsoft MT to maintain abetter sentence structure (see Table 5).For the following example-based error analysis ofthe MT approach, we therefore only consider exam-ples where both MT systems lead to the same SAlabel, taking a random sample of 100 misclassifiedtweets.
We observe the following cases of incor-rectly classified tweets (see examples in Table 4):1.
Example 1 fails to translate the sentiment-bearing dialectical word, ?elegant?, transcribingit as Kchkh but not translating it.2.
Incorrectly translated sentiment-bearingphrases/idioms, see e.g.
that cub is from thatlion in example 2.3.
Misspelled and hence incorrectly translatedsentiment-bearing words in the original text,see example 3 ?Farahhh?
(?happpiness?)
withrepeated letters.
This problem is also high-lighted by Abbasi et al (2014) as one of chal-lenges facing sentiment analysis for social net-works.4.
Example 4 shows a correctly translated tweet,but with an incorrect sentiment label.
Weassume that this is a case of cultural differ-ences: the phrase ?oh God?
can have a nega-tive connotation in English (Strapparava et al,2012).
Note that the Stanford Sentiment clas-sifier makes use of a manually labeled Englishsentiment phrase-based lexicon, which may in-troduce a cultural bias.5.
Example 5 represents a case of correctly trans-lated sentiment-bearing words (love, life), butfailed to translate surrounding text (?Ashan?and ?Amtlat?).
Bautin et al (2008) point outthat this type of contextual information loss isone of the main challenges of MT-based SA.6.
Example 6 represents a case of a correctlytranslated tweet, but with an incorrectly as-signed sentiment label.
We assume that this isdue to changes in sentence structure introducedby the MT system.
Balahur and Turchi (2013)state that word ordering is one of the mostprominent causes of SA misclassification.
Inorder to confirm this hypothesis, we manually75corrected sentence structure before feeding itinto the SA classifier.
This approach led to thecorrect SA label, and thus, confirmed that thecause of the problem is word-ordering.
Notethat the Stanford SA system pays particular at-tention to sentence structure due to its ?deep?architecture that adds to the model the featureof being sensitive to word ordering (Socher etal., 2013).
In future work, we will verify this bycomparing these results to other high perform-ing English SA tools (see for example Abbasiet al (2014)).ExampleTweet ?J?AJ????
@ PY?
@A?QK?KGoogleTrans.I really appreciate what Twitter De-scribe the HnaathMicrosoftTrans.Twitter what I describe his uglinessHumanTrans.I cannot describe how ugly is TwitterTable 5: Example tweet alng with its Google, Microsoftand human translationsIn sum, one of the major challenges of this ap-proach seems to be the use of Arabic dialects in so-cial media, such as Twitter.
In order to confirm thishypothesis, we automatically label Dialectal Ara-bic (DA) vs. Modern Standard Arabic (MSA) usingAIDA (Elfardy et al, 2014) and analyse the perfor-mance of MT-based SA.
The results in Fig.
1 showa significant correlation (Pearson, p<0.05) betweenlanguage class and SA accuracy, with MSA outper-forming DA.
This confirms DA as a major source oferror in the MT-based approach.
Issues like dialec-tal variation and the vowel-free writing system stillpresent a challenge to machine-translation (Zbib etal., 2012).
This is especially true for tweets as theytend to be less formal resulting in issues like mis-spelling and individual spelling variations.
How-ever, with more resources being released for infor-mal Arabic and Arabic dialects, e.g.
(Cotterell andCallison-Burch, 2014; Refaee and Rieser, 2014a),we assume that off-the-shelf MT systems will im-prove their performance in the near future.Figure 1: Performance of the sentiment classifier withrespect to language class (MSA or DA)5 ConclusionThis paper is the first to investigate and empiri-cally evaluate the performance of Machine Transla-tion (MT)-based Sentiment Analysis (SA) for Ara-bic Tweets.
In particular, we make use of off-the-shelf MT tools, such as Google and Microsoft MT,to translate Arabic Tweets into English.
We thenuse the Stanford Sentiment Classifier (Socher etal., 2013) to automatically assign sentiment labels(positive, negative) to translated tweets.
In con-trast to previous work, we benchmark this approachon a gold-standard test set of 937 manually anno-tated tweets and compare its performance to stan-dard SA approaches, including lexicon-based, su-pervised and distant supervision approaches.
Wefind that MT approaches reach a comparable per-formance or significantly outperform more resource-intense standard approaches.
As such, we con-clude that using off-the-shelf tools to perform SA forunder-resourced languages, such as Arabic, is an ef-fective and efficient alternative to building SA clas-sifiers from scratch.Future directions of this work include quantifyingthe impact of the used off-the-shelf tools, e.g.
by us-ing alternative high performing English SA tools.
Inaddition, we plan to investigate multi-classifier sys-tems, given the strength and weaknesses identifiedfor each of the approaches.ReferencesAhmed Abbasi, Ammar Hassan, and Milan Dhar.
2014.Benchmarking twitter sentiment analysis tools.
InProceedings of the Ninth International Conference76on Language Resources and Evaluation (LREC?14),Reykjavik, Iceland, may.
European Language Re-sources Association (ELRA).Muhammad Abdul-Mageed and Mona Diab.
2012.AWATIF: A multi-genre corpus for modern standardArabic subjectivity and sentiment analysis.
In Pro-ceedings of the Eight International Conference onLanguage Resources and Evaluation (LREC?12), Is-tanbul, Turkey.
European Language Resources Asso-ciation (ELRA).Muhammad Abdul-Mageed, Mona T. Diab, and Mo-hammed Korayem.
2011.
Subjectivity and sentimentanalysis of modern standard Arabic.
In Proceedings ofthe 49th Annual Meeting of the Association for Com-putational Linguistics: Human Language Technolo-gies: short papers - Volume 2, HLT ?11, pages 587?591, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Muhammad Abdul-Mageed, Sandra Kuebler, and MonaDiab.
2012.
SAMAR: A system for subjectivity andsentiment analysis of Arabic social media.
In Pro-ceedings of the 3rd Workshop in Computational Ap-proaches to Subjectivity and Sentiment Analysis, pages19?28.
Association for Computational Linguistics.Alexandra Balahur and Marco Turchi.
2013.
Improvingsentiment analysis in twitter using multilingual ma-chine translated data.
In Proceedings of the Interna-tional Conference Recent Advances in Natural Lan-guage Processing RANLP 2013, pages 49?55, Hissar,Bulgaria, September.
INCOMA Ltd. Shoumen, BUL-GARIA.Mikhail Bautin, Lohit Vijayarenu, and Steven Skiena.2008.
International sentiment analysis for news andblogs.
In ICWSM.Ryan Cotterell and Chris Callison-Burch.
2014.
A multi-dialect, multi-genre corpus of informal written Ara-bic.
In The 9th edition of the Language Resources andEvaluation Conference, Reykjavik, Iceland, May.
Eu-ropean Language Resources Association.Jacob Eisenstein.
2013.
What to do about bad languageon the internet.
In Proceedings of NAACL-HLT, pages359?369.Heba Elfardy, Mohamed Al-Badrashiny, and Mona Diab.2014.
AIDA: Identifying code switching in informalArabic text.
EMNLP 2014, page 94.Alec Go, Richa Bhayani, and Lei Huang.
2009.
Twit-ter sentiment classification using distant supervision.CS224N Project Report, Stanford, pages 1?12.Ahmed Mourad and Kareem Darwish.
2013.
Subjectiv-ity and sentiment analysis of modern standard Arabicand Arabic microblogs.
WASSA 2013, page 55.A.
Pak and P. Paroubek.
2010.
Twitter as a corpus forsentiment analysis and opinion mining.
In Proceed-ings of LREC.Arfath Pasha, Mohamed Al-Badrashiny, Mona Diab,Ahmed El Kholy, Ramy Eskander, Nizar Habash,Manoj Pooleery, Owen Rambow, and Ryan Roth.2014.
MADAMIRA: A fast, comprehensive tool formorphological analysis and disambiguation of Arabic.In Proceedings of the Ninth International Conferenceon Language Resources and Evaluation (LREC?14),Reykjavik, Iceland, may.
European Language Re-sources Association (ELRA).Eshrag Refaee and Verena Rieser.
2014a.
An Arabictwitter corpus for subjectivity and sentiment analy-sis.
In 9th International Conference on Language Re-sources and Evaluation (LREC?14).Eshrag Refaee and Verena Rieser.
2014b.
Evaluating dis-tant supervision for subjectivity and sentiment analysison Arabic twitter feeds.
In Proceedings of the EMNLP2014 Workshop on Arabic Natural Language Process-ing (ANLP).Eshrag Refaee and Verena Rieser.
2014c.
Subjectiv-ity and sentiment analysis of Arabic twitter feeds withlimited resources.
In Workshop on Free/Open-SourceArabic Corpora and Corpora Processing Tools (OS-ACT).Wael Salloum and Nizar Habash.
2012.
Elissa: A dialec-tal to standard Arabic machine translation system.
InCOLING (Demos), pages 385?392.Richard Socher, Alex Perelygin, Jean Y Wu, JasonChuang, Christopher D Manning, Andrew Y Ng, andChristopher Potts.
2013.
Recursive deep models forsemantic compositionality over a sentiment treebank.In Proceedings of the conference on empirical meth-ods in natural language processing (EMNLP), volume1631, page 1642.
Citeseer.Carlo Strapparava, Oliviero Stock, and Ilai Alon.
2012.Corpus-based explorations of affective load differ-ences in arabic-hebrew-english.
In COLING (Posters),pages 1201?1208.Maite Taboada, Julian Brooke, Milan Tofiloski, KimberlyVoll, and Manfred Stede.
2011.
Lexicon-based meth-ods for sentiment analysis.
Computational linguistics,37(2):267?307.Xiaojun Wan.
2009.
Co-training for cross-lingual senti-ment classification.
In Proceedings of the Joint Con-ference of the 47th Annual Meeting of the ACL andthe 4th International Joint Conference on Natural Lan-guage Processing of the AFNLP: Volume 1-Volume 1,pages 235?243.
Association for Computational Lin-guistics.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2009.
Recognizing contextual polarity: An explo-ration of features for phrase-level sentiment analysis.Computational Linguistics, 35(3):399?433.Rabih Zbib, Erika Malchiodi, Jacob Devlin, DavidStallard, Spyros Matsoukas, Richard Schwartz, John77Makhoul, Omar F Zaidan, and Chris Callison-Burch.2012.
Machine translation of Arabic dialects.
In Pro-ceedings of the 2012 Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics: Human Language Technologies, pages 49?59.
Association for Computational Linguistics.78
