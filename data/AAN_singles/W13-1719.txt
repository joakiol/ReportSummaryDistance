Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 146?151,Atlanta, Georgia, June 13 2013. c?2013 Association for Computational LinguisticsExploring Syntactic Representationsfor Native Language IdentificationBen SwansonBrown UniversityProvidence, RIchonger@cs.brown.eduAbstractTree Substitution Grammar rules form a largeand expressive class of features capable of rep-resenting syntactic and lexical patterns thatprovide evidence of an author?s native lan-guage.
However, this class of features canbe applied to any general constituent basedmodel of grammar and previous work hasdone little to explore these options, relyingprimarily on the common Penn Treebank an-notation standard.
In this work we contrastthe performance of syntactic features for Na-tive Language Indentification using five dif-ferent formalisms.
The use of different for-malisms captures complementary informationfrom second language data, and can be usedin combination to yield classification perfor-mance superior to any formalism taken on itsown.1 IntroductionNative Language Identification, the automatic deter-mination of an author?s native language (L1) fromtheir writing in a second language (L2), follows ageneral trend of supervised classification using fea-tures extracted from text.
These systems can be opti-mized by both classification algorithm selection andthe integration of diverse feature sets, and in thiswork we focus on the latter.Syntactic features have been shown to providea strong discriminative signal of an author?s na-tive language (Wong and Dras, 2011; Swanson andCharniak, 2012), but little work has been done to ex-plore the various options for representation of syn-tax of learner text.
Many such representations ex-ist, and are routinely employed to improve perfor-mance on the widely studied task of parsing the PennTreebank.
Furthermore, most techniques that provewidely successful at this task have publicly availableimplementations, making them very feasible optionsfor NLI systems.In this work we investigate the use of Tree Sub-stitution Grammars as features for NLI, focusing onthe implication of syntactic paradigm (constituent vsdependency grammar) and the addition of annota-tions that have proved useful in statistical parsing.A Tree Substitution Grammar (TSG) is an intuitiveextension of the Context Free Grammar (CFG) thatallows rewrite rules of arbitrary tree structure.
Alter-natively, a CFG can be seen as a TSG in which therewrite rules obey the constraint that each is a treestructure of unit depth.While a collection of parsed data can be poten-tially generated by a TSG that is exponential in thelength of the text, recent techniques allow for the ef-ficient induction of compact grammars (Cohn andBlunsom, 2010).
At a high level, this techniqueemploys the rich-get-richer dynamics of a Dirich-let Process to sample derivations for the trees in thetraining corpus: the more that a rule is used in otherderivations, the more likely it is that we will chooseit when sampling a derivation.We follow previous work in stylometry withTSGs for the NLI in that we parse the entirety ofthe training data and use it to induce a compact TSGusing the method described above.1 We then use the1An alternative method of note that we do not consider inthis work is to induce TSG rules on hand-annotated data suchas the Penn Treebank, as in Bergsma et al(2012).146S@SPPINWithoutNPNNdeviationNPprogressVP@VPVBZisRBnotNPJJpossibleS-5@S-2PP-3IN-10WithoutNP-12NN-13deviationNP-15progressVP-2@VP-1VBZ-3isRB-5notNP-6JJ-2possibleFigure 1: Sample parse trees produced by the Berkeley Parser.
An example of what the tree might look like with splitsymbol annotations is shown on the right.TSG rules as binary features for supervised classi-fication such that the feature for a TSG rule is trig-gered on a document if that rule appears in the parseof some derivation of any of its sentences.
This de-scription purposefully treats the parsing of text as ablack box whose input is plain text and whose out-put is any valid tree structure.
Our work considersfive alternatives for this black box, and evaluates theeffect of this choice on the NLI Shared Task at theBEA Workshop of NAACL 2013 (Tetreault et al2013).2 Syntactic RepresentationsWe investigate five variations on the output of theparsing process.
All five are easily produced byfreely available Java software; two with the BerkeleyParser, two with the Stanford Parser, and one with acombination of both software packages.2.1 Berkeley Constituent ParsesOur first representation reproduces previous work byusing the output of the Berkeley Parser (Petrov etal., 2006), one of highest performing systems on thebenchmark Penn Treebank task.
The basic motivat-ing principle involved is that the traditional nonter-minal symbols used in Penn Treebank parsing aretoo coarse to satisfy the context free assumption ofa CFG.
To combat this, hierarchical latent annota-tions are induced that split a symbol into severalsubtypes, and a larger CFG is estimated on this setof split nonterminals.
A sentence is parsed usingthis large CFG and each resulting symbol is mappedback to its original unsplit supertype to produce thefinal parse.One important subtlety of the Berkeley Parser isits default binarization, which we leave intact in ourdownstream use of its parses.
While binarization isnormally motivated by the desired cubic complexityof parsing algorithms, it also benefits syntactic sty-lometry.
Consider the nugget of wisdom from thegreat Frank Zappa shown on the left in Figure 1, inwhich artificially introduced binarization nodes aremarked with the @ symbol.The use of binarization allows us to capture pat-terns such as verb phrases that begin with ?is not?independent of the following child constituents.
Thecapabilities of TSG rules makes the use of binariza-tion even more apt, as we can easily choose to re-cover the unbinarized pattern with a slightly largerfragment.
This choice will be made in TSG induc-tion based on the frequency with which the combi-nation occurs, which intuitively aligns with our goalof choosing representative features.The second form that we investigate is identicalto the normal Berkeley Parser output, but with thesplit annotations used in parsing left intact, as shownthe right of Figure 1.
This parsed sentence showshow each nonterminal is annotated with a split cat-egory, and illustrates the potential advantages thatthis method affords.
For example, consider the @VPnode in the left-hand tree, whose subtree is gen-erated with a CFG by first choosing to produce aVBZ and RB, and then by lexicalizing each inde-pendently.
These two lexicalizations are not in factindependent, as can be seen by the combination of?is?
with the RB ?may?, which is impossible al-147though each are independently quite likely.
Splittingthe symbols as shown on the right allows us to cre-ate a special RB node that is most likely to produce?not?
and VBZ node likely to produce ?is?.
Theirlikely co-occurrence can then be modeled as shownby a rule with both specialized tags as children.It is worth noting that this particular ability ofsplit symbol grammars to coordinate lexical itemsis easily captured with the TSG rules that we induceon these parses, regardless of the presence of splitsymbols.
The more orthogonal quality of these splitgrammars is their ability to categorize symbols thatappear in similar syntactic situations.
Consider thatsome adjectives are more likely to appear in ?X is Y?sentences in the ?Y?
position, while some are morelikely to be used directly to the left of nouns.
A splitsymbol grammar handily captures this trait with asplit POS tag, while a TSG cannot associate patternscontaining different lexical items on its own.2.2 Stanford Dependency ParsesThe third and fourth syntactic models we employare derived from dependency parses produced by theStanford parser(Marneffe et al 2006).
In its stan-dard form, a dependency parse is a directed tree inwhich each word except the special ROOT node hasexactly one incoming edge and zero to many outgo-ing edges, where edges represent syntactic depen-dence.
Arcs are labeled with the type of syntacticdependence that they indicate.
Following conven-tion, we represent each word in combination with itspart of speech tag, as shown in the following exam-ple dependency parse.ROOT DT NN VBZ PRPThe poodle chews itrootdet nsubj dobjIn order to apply the techniques of TSG inductionto dependency parsed data, we implement a conver-sion from dependency tree to constituent form.
Themechanics of this conversion are simple and illus-trated in full by the following conversion of the de-pendency tree shown above, and are similar to trans-forms used in previous work in unsupervised depen-dency parsing(Carroll and Charniak, 1992).ROOTVBZ-LnsubjNN-LdetDTtheNNpoodleVBZchewsVBZ-RdobjPRPitNote that it is always the case that the arc labelsfrom the dependency parses are always producedby unary rules.
This allows the simple removal ofthe nodes corresponding to arc labels, yielding ourfourth syntactic model.ROOTVBZ-LNN-LDTtheNNpoodleVBZchewsVBZ-RPRPitThose familiar with the Stanford Parser may beconcerned that the dependency parses used here aredetermined by a deterministic transform of a con-stituent parse of Penn Treebank style, and then sim-ply transformed back into constituent form.
This isespecially concerning when considering the secondform in which arc labels have been removed; thisform can be constructed directly from the BerkeleyParse form used above, and contains no additionalinformation.
Our motivation in the investigation ofdependency parses is not that they offer new infor-mation, but that they are organized differently thanconstituent parses.
When inducing a TSG, our abil-ity to find a useful connections is impeded by phys-ical distance between structures.
In particular, in adependency parse, the head of the subject and theverb are always contained in some TSG fragment148made up of small number of CFG rules, five or fourdepending on the presence of arc labels.
In con-stituent parses, the presence of modifying phrasescan arbitrarily increase this distance.2.3 Stanford Heuristic AnnotationsOur final variation uses the annotations internal tothe Stanford Penn Treebank parser, as presented inKlein and Manning (2003).
These annotations aremotivated in the same way as Berkeley Parser splitstates, but are deterministically applied to parse treesusing linguistic motivations.
Besides handling ex-plicit tracking of binarization and parent annotation,several additional annotations are applied, such asthe splitting of certain POS tags into useful cate-gories and annotation of some nodes with their num-ber of children or siblings.For ease of implementation, we do not use theStanford Parser itself to produce our trees, insteadwe used our results from the Berkeley Parser.
TheStanford Parser annotations were then applied tothese trees after binarization symbols were first col-lapsed.
The following tree is an example of theactual annotations applied by this process, and in-cludes a fair subset of the many annotation typesthat are used.
The original symbol in each case isthe leftmost string of capital letters in the resultingsymbol strings shown.ROOTS-vNP-BNNP?NPAceVP-VBF-vVBZ?VB-BEisPPIN?PPinNP-BDT?NPtheNN?NPhouse3 ExperimentsWe contrast the syntactic formalisms on the NLIshared task experimental setup for the NAACL 2013BEA workshop.
This new data set (Blanchard et al2013) consists of TOEFL essays drawn from speak-ers of 11 different L1 backgrounds.
9900 Essayswere supplied as a training set, with an additional1100 development set essays and 1100 test essays.Previous work in NLI has relied heavily on theInternational Corpus of Learner English, but due tosignificant topic biases along L1 lines in this dataset the explicit use of word tokens was frequentlylimited to a predetermined set of stopwords.
Withthis in mind, the data set for the shared task was bal-anced across TOEFL essay prompts and proficiencylevels.
The result was that the participants in thistask were not forced to limit the word tokens explic-itly employed, with the hopes that mitigating factorshad been minimized.We prepared the data in the five forms describedabove and induced TSGs on each version of theparsed training set with the blocked sampling algo-rithm of Cohn and Blunsom (2010).
The resultingrules were used as binary feature functions over doc-uments indicating the presence of the rule in somederivation of sentence in that document.
We usedthe Mallet implementation of a log-linear (MaxEnt)classifier with a zero mean Gaussian prior with vari-ance .1 on the classifier?s weights.
Our results on thedevelopment set are shown in Figure 3.While a range of performance is achieved, whenwe construct a classifier that simply averages thepredictive distributions of all five methods we getbetter accuracy than any model on its own.
We ob-served further evidence of the orthogonality of thesemethods by looking at pairs of formalisms and ob-serving how many development set items were pre-dicted correctly by one formalism and incorrectly byanother.
This was routinely around 10 percent of thedevelopment set in each direction for a given pair,implying that gains of up to at least 20 percent classi-fication accuracy are possible with an expert systemthat approaches oracle selection of which formalismto use.As our submission to the shared task, we used theBerkeley Parser output in isolation, the average ofthe five classifiers, and the weighted average of theclassifiers using the optimal weights on the devel-opment set.
The former two models use the devel-opment set as additional training data, which is onepossible explanation of the slightly higher perfor-mance of the equally weighted average model.
An-149ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TUR P R FARA 76 2 4 1 2 2 2 1 4 3 3 76.8 76.0 76.4CHI 2 86 0 1 1 0 4 4 1 0 1 81.1 86.0 83.5FRE 2 1 77 3 2 6 2 1 5 1 0 82.8 77.0 79.8GER 0 1 1 91 1 1 0 0 2 0 3 86.7 91.0 88.8HIN 2 2 1 2 71 0 0 0 0 20 2 73.2 71.0 72.1ITA 2 0 2 1 1 84 0 1 7 0 2 79.2 84.0 81.6JPN 3 4 0 1 0 0 83 7 1 0 1 74.1 83.0 78.3KOR 1 6 1 1 1 0 20 65 2 1 2 69.1 65.0 67.0SPA 4 2 4 3 2 12 0 3 66 0 4 71.7 66.0 68.8TEL 1 2 0 0 16 0 0 0 0 81 0 76.4 81.0 78.6TUR 6 0 3 1 0 1 1 12 4 0 72 80.0 72.0 75.8Figure 2: Confusion Matrix and per class results on the final test set evaluation using the evenly averaged model.other explanation of note is that while the weightoptimization was carried out with EM over the like-lihood of the development set labels, this did notin correlate positively with classification accuracy;even as we optimized on the development set the ac-curacy in absolute classification of these items de-creased slightly.The confusion matrix for the evenly averagedmodel, our best performing system, is shown in Fig-ure 2.
The most frequently confused L1 pairs wereHindi and Telegu, Japanese and Korean, and Span-ish and Italian.
The similarity between Hindi andTelegu is particularly troubling, as they come fromtwo completely different language families and theirmost obvious similarity is that they are both spokenprimarily in India.
This suggests that even thoughthe TOEFL corpus has been balanced by topic thatthere is a strong geographical signal that is corre-lated with but not caused by native language.BP BPS DP DPA KM AVGAcc 74.5 69.3 72.4 73.5 73.5 77.3Figure 3: The resulting classification accuracies on thedevelopment set for the various syntactic forms that weconsidered.
The forms used are plain Berkeley Parses(BP), Berkeley Parses with split symbols (BPS), depen-dency parses (DP), dependency parses without arc la-bels (DPA), and the heuristic annotations from (Klein andManning, 2003) (KM).
When the predictive distributionsof the five models are averaged (AVG), a higher accuracyis achieved.BP AVG AVG-EMAcc 74.7 77.5 77.0Figure 4: The classification accuracies obtained on thetest data using the Berkeley parser output alone (BP), thearithmetic mean of all five predictive distributions (AVG)and the weighted mean using the optimal weights fromthe development set as determined with EM (AVG-EM)4 ConclusionIn this work we open investigation of a generally un-considered variable in syntactic stylometry: the ac-tual syntactic formalism.
We examine five poten-tial candidates of which only one has been previ-ously presented in the context of TSG features forNLI.
These five formalisms cover both constituentand dependency grammars, and explore the possi-bility of split state annotations for constituent gram-mars and the inclusion of arc labels for dependencygrammars.
We find that the use of different grammarformalisms captures orthogonal information aboutan author?s native language.
Furthermore, the com-bination of different formalisms can be used to in-crease classification accuracy.While our results are intriguing, they primarilyserve as a proof of concept that syntactic stylome-try can benefit from a range of representations andshould not be taken as an exhaustive search for thebest representations to use.
Other syntactic formsexist, and even in our methods there are additionalvariables that can be adjusted.One such variable is the number of splits used in150the Berkeley Parser when split states are included;the default number that we use in this work is 6,the optimal value for the parsing task, but this maybe suboptimal as a representation for feature extrac-tion.
Binarization is another easily adjusted variable,with several available options in the literature.
Forexample, binarization can be done that is aware ofhead attachment.
Another option is to binarize moreheavily, increasing the ability of TSG fragments toseparate sister nodes and find frequent patterns.Alternative syntactic forms not explored in thiswork are also available.
These include well stud-ied grammars such as Hierarchical Phrase StructureGrammars and Combinatory Categorial Grammars,and transforms that rearrange the tree such as theLeft Corner Transform used in Roark and Johnson(1999).
Furthermore, the use of the TSG as a fea-ture extractor itself has the potential for extensionto more powerful systems such as Tree AdjoiningGrammars or Tree Insertion Grammars.ReferencesShane Bergsma, Matt Post, and David Yarowsky.
2012.Stylometric Analysis of Scientific Articles.
In Pro-ceedings of the 2012 Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics: Human Language Technologies, pages 327?337, Montre?al, Canada, June.
Association for Compu-tational Linguistics.Daniel Blanchard, Joel Tetreault, Derrick Higgins, AoifeCahill, and Martin Chodorow.
2013.
Toefl11: A cor-pus of non-native english.
Technical report, Educa-tional Testing Service.Glenn Carroll and Eugene Charniak.
1992.
Two experi-ments on learning probabilistic dependency grammarsfrom corpora.
Technical Report CS-92-16, BrownUniversity, Providence, RI, USA.Trevor Cohn and Phil Blunsom.
2010.
Blocked inferencein bayesian tree substitution grammars.
In ACL (ShortPapers), pages 225?230.Dan Klein and Christopher D. Manning.
2003.
Accurateunlexicalized parsing.
In ACL, pages 423?430.Marie Catherine De Marneffe, Bill Maccartney, andChristopher D. Manning.
2006.
Generating typed de-pendency parses from phrase structure parses.
In InProc.
Intl Conf.
on Language Resources and Evalua-tion (LREC, pages 449?454.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In Proceedings of the 21st In-ternational Conference on Computational Linguisticsand 44th Annual Meeting of the Association for Com-putational Linguistics, pages 433?440, Sydney, Aus-tralia, July.
Association for Computational Linguistics.Brian Roark and Mark Johnson.
1999.
Efficient proba-bilistic top-down and left-corner parsing.
In ACL.Benjamin Swanson and Eugene Charniak.
2012.
Na-tive Language Detection with Tree Substitution Gram-mars.
In Proceedings of the 50th Annual Meeting ofthe Association for Computational Linguistics (Vol-ume 2: Short Papers), pages 193?197, Jeju Island, Ko-rea, July.
Association for Computational Linguistics.Joel Tetreault, Daniel Blanchard, and Aoife Cahill.
2013.A report on the first native language identificationshared task.
In Proceedings of the Eighth Workshopon Innovative Use of NLP for Building EducationalApplications, Atlanta, GA, USA, June.
Association forComputational Linguistics.Sze-Meng Jojo Wong and Mark Dras.
2011.
ExploitingParse Structures for Native Language Identification.In Proceedings of the 2011 Conference on Empiri-cal Methods in Natural Language Processing, pages1600?1610, Edinburgh, Scotland, UK., July.
Associa-tion for Computational Linguistics.151
