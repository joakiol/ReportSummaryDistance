Proceedings of the Workshop on Innovative Hybrid Approaches to the Processing of Textual Data (Hybrid2012), EACL 2012, pages 69?77,Avignon, France, April 23 2012. c?2012 Association for Computational LinguisticsCoupling Knowledge-Based and Data-Driven Systemsfor Named Entity RecognitionDamien Nouvel Jean-Yves Antoine Nathalie Friburger Arnaud SouletUniversite?
Franc?ois Rabelais Tours, Laboratoire d?Informatique3, place Jean Jaures, 41000 Blois, France{damien.nouvel, jean-yves.antoine, nathalie.friburger, arnaud.soulet}@univ-tours.frAbstractWithin Information Extraction tasks,Named Entity Recognition has receivedmuch attention over latest decades.
Fromsymbolic / knowledge-based to data-driven/ machine-learning systems, many ap-proaches have been experimented.
Ourwork may be viewed as an attempt tobridge the gap from the data-driven per-spective back to the knowledge-based one.We use a knowledge-based system, basedon manually implemented transducers,that reaches satisfactory performances.
Ithas the undisputable advantage of beingmodular.
However, such a hand-craftedsystem requires substantial efforts tocope with dedicated tasks.
In this con-text, we implemented a pattern extractorthat extracts symbolic knowledge, usinghierarchical sequential pattern miningover annotated corpora.
To assess theaccuracy of mined patterns, we designed amodule that recognizes Named Entities intexts by determining their most probableboundaries.
Instead of considering NamedEntity Recognition as a labeling task, itrelies on complex context-aware featuresprovided by lower-level systems andconsiders the tagging task as a markovianprocess.
Using thos systems, couplingknowledge-based system with extractedpatterns is straightforward and leads to acompetitive hybrid NE-tagger.
We reportexperiments using this system and compareit to other hybridization strategies alongwith a baseline CRF model.1 IntroductionNamed Entity Recognition (NER) is an informa-tion extraction (IE) task that aims at extractingand categorizing specific entities (proper namesor dedicated linguistic units as time expressions,amounts, etc.)
in texts.
These texts can be pro-duced in diverse conditions.
In particular, theymay correspond to either electronic written doc-uments (Marsh & Perzanowski, 1998) or morerecently speech transcripts provided by a humanexpert or an automatic speech recognition (ASR)system (Galliano et al, 2009).
The recognized en-tities may later be used by higher-level tasks fordifferent purposes such as Information Retrievalor Open-Domain Question-Answering (Voorhees& Harman, 2000).While NER is often considered as quite a sim-ple task, there is still room for improvement whenit is confronted to difficult contexts.
For instance,NER systems may have to cope with noisy datasuch as word sequences containing speech recog-nition errors in ASR.
In addition, NER is no morecircumscribed to proper names, but may also in-volve common nouns (e.g., ?the judge?)
or com-plex multi-word expressions (e.g.
?the Com-puter Science department of the New York Uni-versity?).
These complementary needs for robustand detailed processing explain that knowledge-based and data-driven approaches remain equallycompetitive on NER tasks as shown by numerousevaluation campaigns.
For instance, the French-speaking Ester2 evaluation campaign on radiobroadcasts (Galliano et al, 2009) has shown thatknowledge-based approaches outperformed data-driven ones on manual transcriptions while a sys-tem based on Conditional Random Fields (CRFs,participant LIA) is ranked first on noisy ASR tran-scripts.
This is why the development of hybridsystems has been investigated by the NER com-munity.69In this paper, we present a strategy of hy-bridization benefiting from features produced bya knowledge-based system (CasEN) and a data-driven pattern extractor (mineXtract).
CasENhas been manually implemented based on finite-state transducers.
Such a hand-crafted systemrequires substantial efforts to be adapted to ded-icated tasks.
We developed mineXtract, a text-mining system that automatically extracts infor-mative rules, based on hierarchical sequential pat-tern mining.
Both implement processings that arecontext-aware and use lexicons.
Finally, to rec-ognize NEs, we propose mStruct, a light multi-purpose automatic annotator, parameterized usinglogistic regression over available features.
It takesinto account features provided by lower-level sys-tems and annotation scheme constraints to outputa valid annotation maximizing likelihood.
Our ex-periments show that the resulting hybrid systemoutperforms standalone systems and reaches per-formances comparable to a baseline hybrid CRFsystem.
We consider this as a step forward to-wards a tighter integration of knowledge-basedand data-driven approaches for NER.The paper is organized as follows.
Section 2describes the context of this work and reviewsrelated work.
Section 3 describes CasEN, theknowledge-based NE-tagger.
Section 4 details theprocess of extracting patterns from annotated dataas informative rules.
We then introduce the au-tomatic annotator mStruct in Section 5.
Section 6describes how to gather features from systems andpresent diverse hybridization strategies.
Corpora,metrics used and evaluation results are reported inSection 7.
We conclude in Section 8.2 Context and Related Work2.1 Ester2 Evaluation CampaignThis paper focuses on NER in the context ofthe Ester2 evaluation campaign (Galliano et al,2009).
This campaign assesses system?s perfor-mance for IE tasks over ASR outputs and manualtranscriptions of radio broadcast news (see detailsin Section 7).
The annotation guidelines speci-fied 7 kinds of entities to be detected and cate-gorized: persons (?pers?
), organizations (?org?
),locations (?loc?
), amounts (?amount?
), time ex-pressions (?time?
), functions (?func?
), products(?prod?).
Technically, the annotation scheme isquite simple: only one annotation per entity, al-DSent.
Tokens and NEss1 <pers> Isaac Newton </pers> was admitted in<time> June 1661 </time> to <org> Cambridge</org>.s2 <time> In 1696 </time>, he moved to <loc> Lon-don </loc> as <func> warden of the Royal Mint</func>.s3 He was buried in <loc> Westminster Abbey </loc>.Table 1: Sentences from an annotated corpusmost no nesting (except for persons collocatedwith their function: both should be embedded inan encompassing ?pers?
NE).We illustrate the annotation scheme using arunning example.
Table 1 presents the expectedannotation in the context of Ester2 from ?IsaacNewton was admitted in June 1661 to Cam-bridge.
In 1696, he moved to London as wardenof the Royal Mint.
He was buried in Westmin-ster Abbey.?.
This example illustrates frequentproblems for NER task.
Determining the extentof a NE may be difficult.
For instance, NERshould consider here either ?Westminster?
(city)or ?Westminster Abbey?
(church, building).
Cat-egorizing NEs is confronted to words ambiguities,for instance ?Cambridge?
may be considered as acity (?loc?)
or a university (?org?).
In addition, oraltranscripts may contain disfluencies, repetitions,hesitations, speech recognition errors: overall dif-ficulty is significantly increased.
For these rea-sons, NER over such noisy data is a challengingtask.2.2 State of the ArtKnowledge-based approaches Most of thesymbolic systems rely on shallow parsing tech-niques, applying regular expressions or linguisticpatterns over Part-Of-Speech (POS), in additionto proper name lists checking.
Some of them han-dle a deep syntactic analysis which has provenits ability to reach outstanding levels of perfor-mances (Brun & Hage`ge, 2004; Brun & Hage`ge,2009; van Shooten et al, 2009).Data-driven approaches A large diversity ofdata-driven approaches have been proposed dur-ing the last decade for NER.
Generative modelssuch as Hidden Markov Models or stochastic fi-nite state transducers (Miller et al, 1998; Favre etal., 2005) benefit from their ability to take intoaccount the sequential nature of language.
Onthe other hand, discriminative classifiers such as70Support Vector Machines (SVMs) are very effec-tive when a large variety of features (Isozaki &Kazawa, 2002) is used, but lack the ability totake a global decision over an entire sentence.Context Random Fields (CRFs) (Lafferty et al,2001) have enabled NER to benefit from the ad-vantages of both generative and discriminative ap-proaches (McCallum & Li, 2003; Zidouni et al,2010; Be?chet & Charton, 2010).
Besides, therobustness of data-driven / machine-learning ap-proaches explains that the latter are more appro-priate on noisy data such as ASR transcripts.Hybrid systems Considering the complemen-tary behaviors of knowledge-based and data-driven systems for NER, projects have been con-ducted to investigate how to conciliate both ap-proaches.
Work has been done to automaticallyinduce symbolic knowledge (Hingston, 2002;Kushmerick et al, 1997) that may be used asNE taggers.
But in most cases, hybridization forNER relies a much simpler principle: outputs ofknowledge-based systems are considered as fea-tures by a machine learning algorithm.
For in-stance, maximum entropy may be used when ahigh diversity of knowledge sources are to betaken into account (Borthwick et al, 1998).
CRFsalso have demonstrated their ability to mergesymbolic and statistic processes in a machinelearning framework (Zidouni et al, 2010).We propose an approach to combineknowledge-based and data-driven approaches ina modular way.
Our first concern is to implementa module that automatically extracts knowledgethat should be interoperable with the existingsystem?s transducers.
This is done by focusing, inannotated corpora, more on ?markers?
(tags) thatare to be inserted between tokens (e.g.
<pers>,</pers>, <org>, </org>, etc.
), than on?labels?
assigned to each token, as transducerdo.
By doing so, we expect to establish a bettergrounding for hybriding manually implementedand automatically extracted patterns.
Afterwards,another module is responsible of annotatingNEs by using those context-aware patterns andstandard machine-learning techniques.3 CasEN: a knowledge-based systemThe knowledge-based system is based on CasSys(Friburger & Maurel, 2004), a finite-state cascadesystem that implements processings on texts at di-verse levels (morphology, lexicon, chunking).
Itmay be used for various IE tasks, or simply totransform or prepare a text for further processings.The principle of this finite-state processor is tofirst consider islands of certainty (Abney, 2011),so as to give priority to most confident rules.
Eachtransducer describes local patterns correspondingto NEs or interesting linguistic units available tosubsequent transducers within the cascade.Casen is the set of NE recognition transduc-ers.
It was initially designed to process writtentexts, taking into account diverse linguistic clues,proper noun lists (covering a broad range of firstnames, countries, cities, etc.)
and lexical evi-dences (expressions that may trigger recognitionof a named entity).Figure 1: A transducer recognizing person namesFigure 2: Transducer ?patternFirstName?As an illustration, Figure 1 presents a very sim-ple transducer tagging person names made of anoptional title, a first name and a surname.
Theboxes contain the transitions of the transducer asitems to be matched for recognizing a person?sname.
Grayed boxes contain inclusions of othertransducers (e.g.
box ?patternFirstName?
in Fig-ure 1 is to be replaced by the transducer depictedin Figure 2).
Other boxes can contain lists ofwords or diverse tags (e.g.
<N+firstname>for a word tagged as first name by lexicon).
Theoutputs of transducers are displayed below boxes(e.g.
?{?
and ?,.entity+pers+hum}?
in Figure 1).For instance, that transducer matches theword sequence ?Isaac Newton?
and outputs:?
{{Isaac ,.firstname} {Newton ,.surname} ,.en-tity+pers+hum}?.
By applying multiple transduc-71ers on a text sequence, CasEN can provide sev-eral (possibly nested) annotations on a NE andits components.
This has the advantage of pro-viding detailed information about CasEN internalprocessings for NER.Finally, the processing of examples in Table 1leads to annotations such as:?
{ { June ,.month} { 1661 ,.year} ,en-tity+time+date+rel}?
{ Westminster ,.entity+loc+city}{ Abbey ,buildingName} ,.en-tity+loc+buildingCityName }In standalone mode, post-processing steps con-vert outputs into Ester2 annotation scheme (e.g.<pers> Isaac Newton </pers>).Experiments conducted on newspaper docu-ments for recognizing persons, organizations andlocations on an extract of the Le Monde corpushave shown that CasEN reaches 93.2% of recalland 91.1% of f-score (Friburger, 2002).
Dur-ing the Ester2 evaluation campaign, CasEN (?LITours?
participant in (Galliano et al, 2009)) ob-tained 33.7% SER (Slot Error Rate, see sectionabout metrics description) and a f-score of 75%.This may be considered as satisfying when oneknows the lack of adaptation of Casen to speci-ficities of oral transcribed texts.4 mineXtract: Pattern Mining Method4.1 Enriching an Annotated CorpusWe investigate the use of data mining techniquesin order to supplement our knowledge-based sys-tem.
To this end, we use an annotated corpus tomine patterns related to NEs.
Sentences are con-sidered as sequences of items (this precludes ex-traction of patterns accross sentences).
An item iseither a word from natural language (e.g.
?admit-ted?, ?Newton?)
or a tag delimiting NE categories(e.g., <pers>, </pers> or <loc>).
The an-notated corpus D is a multiset of sequences.Preprocessing steps enrich the corpus by (1) us-ing lexical resources (lists of toponyms, anthro-ponyms and so on) and (2) lemmatizing and ap-plying a POS tagger.
This results in a multi-dimensional corpus where a token may graduallybe generalized to its lemma, POS or lexical cate-gory.
Figure 3 illustrates this process on the wordssequence ?moved to <loc> London </loc>?.moveVERmovedPRPto<loc> PNCITY</loc>Figure 3: Multi-dimensional representation of thephrase ?moved to <loc> London </loc>?The first preprocessing step consists in consid-ering lexical resources to assign tokens to lexi-cal categories (e.g., CITY for ?London?)
when-ever possible.
Note that those resources containmulti-word expressions.
Figure 4 provides a shortextract limited to tokens of Table 1) of lexicalressources (totalizing 201,057 entries).
This as-signment should be ambiguous.
For instance, pro-cessing ?Westminster Abbey?
would lead to cat-egorizing ?Westminster?
as CITY and the wholeas INST.Afterwards, a POS tagger based on TreeTag-ger (Schmid, 1994) distinguishes common nouns(NN) from proper names (PN).
Besides, token isdeleted (only PN category is kept) to avoid extrac-tion of patterns that would be specific to a givenproper name (on Figure 3, ?London?
is removed).Figure 5 shows how POS, tokens and lemmas areorganized as a hierarchy.Category TokensANTHRO Newton, Royal .
.
.CITY Cambridge, London, Westminster .
.
.INST Cambridge, Royal Mint, Westminster Abbey .
.
.METRIC Newton .
.
.. .
.
.
.
.Figure 4: Lexical Ressourcesin of toPRPadmitadmittedbewasburyburiedVERFigure 5: Items Hierarchy4.2 Discovering Informative RulesWe mine this large enriched annotated corpus tofind generalized patterns correlated to NE mark-ers.
It consists in exhaustively enumerating all thecontiguous patterns mixing words, POS and cat-72egories.
This provides a very broad spectrum ofpatterns, diversely accurate to recognize NEs.
Asan illustration, if you consider the words sequence?moved to <loc> London </loc>?
in Figure 3leads to examining patterns as:?
?
VER PRP <loc> PN </loc>??
?
VER to <loc> PN </loc>??
?
moved PRP <loc> CITY </loc>?The most relevant patterns will be filtered byconsidering two thresholds which are usual indata mining: support and confidence (Agrawal& Srikant, 1994).
The support of a pattern Pis its number of occurrences in D, denoted bysupp(P,D).
The greater the support of P , themore general the pattern P .
As we are only inter-ested in patterns sufficiently correlated to mark-ers, a transduction rule R is defined as a patterncontaining at least one marker.
To estimate em-pirically how much R is accurate to detect mark-ers, we calculate its confidence.
A dedicated func-tion suppNoMark(R,D) returns the support ofR when markers are omitted both in the rule andin the data.
The confidence of R is:conf(R,D) =supp(R,D)suppNoMark(R,D)For instance, consider the rule R = ?
VER PRP<loc>?
in Table 1.
Its support is 2 (sentencess2 and s3).
But its support without consideringmarkers is 3, since sentence s1 matches the rulewhen markers are not taken in consideration.
Theconfidence of R is 2/3.In practice, the whole collection of transduc-tion rules exceeding minimal support and con-fidence thresholds remains too large, especiallywhen searching for less frequent patterns.
Conse-quently, we filter-out ?redundant rules?
: those forwhich a more specific rule exists with same sup-port (both cover same examples in corpus).
Forinstance, the rules R1 = ?
VER VER in <loc>?and R2 = ?
VER in <loc>?
are more generaland have same support than R3 = ?
was VERin <loc>?
: we only retain the latter.The system mineXtract implements those pro-cessing using a level-wise algorithm (Mannila &Toivonen, 1997).5 mStruct: Stochastic Model for NERWe have established a common ground for thesystems to interact with a higher level model.Our assumption is that lower level systems ex-amine the input (sentences) and provide valu-able clues playing a key role in the recognitionof NEs.
In that context, the annotator is im-plemented as an abstracted view of sentences.Decisions will only have to be taken wheneverone of the lower-level systems provides infor-mation.
Formally, beginning or ending a NEat a given position i may be viewed as the af-fectation of a random variable P (Mi = mji)where the value of mji is one of the markers({?,<pers>,</pers>,<loc>,<org>, .
.
.
}).For a given sentence, we use binary featurestriggered by lower-level systems at a given posi-tion (see section 6.1) for predicting what markerwould be the most probable at that very position.This may be viewed as an instance of a classifi-cation problem (more precisely multilabel clas-sification since several markers may appear at asingle position, but we won?t enter into that levelof detail due to lack of space).
Empirical exper-iments with diverse machine learning algorithmsusing Scikit-learn (Pedregosa et al, 2011) lead usto consider logistic regression as the most effec-tive on the considered task.Considering those probabilities, it is now pos-sible to estimate the likelihood of a given annota-tion over a sentence.
Here, markers are assumedto be independent.
With this approximation, thelikehood of an annotation is computed by a sim-ple product:P (M1 = mj1 ,M2 = mj2 , .
.
.
,Mn = mjn)?
?i=1...nP (Mi = mji)As an illustration, Figure 6 details the compu-tation of an annotation given the probability of ev-ery markers, using the Ester2 annotation scheme.For clarity purposes, only sufficiently probablemarkers (including ?)
are displayed at each po-sition.
A possible <func> is discarded (crossedout), being less probable than a previous one.
Anannotation solution <org> .
.
.</org> is evalu-ated, but is less likely (0.3 ?
0.4 ?
0.9 ?
0.4 ?
0.4 ?0.1 = 0.0017) than warden of the Royal Mint as afunction (0.6?0.4?0.9?0.3?0.5?0.4 = 0.0129)73which will be retained (and is the expected anno-tation).asPRP?
0.3<func> 0.6wardenNNJOB?
0.4</func> 0.5ofPRP?
0.9theDET?
0.3<org> 0.2<pers> 0.4RoyalNPINST?
0.5</pers> 0.4MintNPINST?
0.1</func> 0.4<org> 0.4Figure 6: Stochastic Annotation of a SequenceEstimating markers probabilities allows themodel to combine evidences from separateknowledge sources when recognizing starting orending boundaries.
For instance, CasEN may re-congize intermediary structures but not the wholeentity (e.g.
when unexpected words appear insideit) while extracted rules may propose markers thatare not necessarily paired.
The separate detectionof markers enables the system to recognize namedentities without modeling all their tokens.
Thismay be useful when NER has to face noisy dataor speech disfluences.Finally, it is not necessary to compute likeli-hoods over all possible combination of markers,since the annotation scheme is much constrained.As the sentence is processed, some annotation so-lutions are to be discarded.
It is straightforwardto see that this problem may be resolved usingdynamic programming, as did Borthwick et al(1998).
Depending on the annotation scheme,constraints are provided to the annotator whichoutputs an annotation for a given sentence thatis valid and that maximizes likelihood.
Our sys-tem mStruct (micro-Structure) implements this(potentially multi-purpose) automatic annotationprocess as a separate module.6 Hybriding systems6.1 Gathering Clues from SystemsFigure 7 describes the diverse resources and algo-rithms that are plugged together.
The knowledge-based system uses lists that recognize lexical pat-terns useful for NER (e.g.
proper names, but alsoautomata to detect time expressions, functions,etc.).
Those resources are exported and availableto the data mining software as lexical resources(see section 4) and (as binary features) to the base-line CRF model.ListsMiningCorpus mineXtractTransducers CasENLearningCorpusHybridationGatherFeaturesmStructFigure 7: Systems Modules (Hybrid data flow)Each system processes input text and providesfeatures used by the Stochastic Model mStruct.
Itis quite simple to take in consideration mined in-formative rules: each time a rule i proposes itsjth marker, a Boolean feature Mij is activated.What is provided by CasEN is more sophisticated,since each transducer is able to indicate more de-tailed information (see section 3), as multiple fea-tures separated by ?+?
(e.g.
?entity+pers+hum?
).We want to benefit as much as possible from thisrichness: whenever a CasEN tag begins or ends,we activate a boolean feature for each mentionedfeature plus one for each prefixes of features (e.g.
?entity?, ?pers?, ?hum?
but also ?entity.pers?
and?entity.pers.hum?
).6.2 Coupling StrategiesWe report results for the following hybridizationsand CRF-based system using Wapiti (Lavergne etal., 2010).?
CasEN: knowledge-based system standalone?
mXS: mineXtract extracts, mStruct annotates?
Hybrid: gather features from CasEN and mineX-tract, mStruct annotates?
Hybrid-sel: as Hybrid, but features are selected?
CasEN-mXS-mine: as mXS, but text is pre-processed by CasEN (adding a higher general-ization level above lexical lists)?
mXS-CasEN-vote: as mXS, plus a post-processing step as a majority vote based on mXSand CasEN outputs?
CRF: baseline CRF, using BIO and common fea-tures (unigrams: lemma and lexical lists, bi-grams: previous, current and next POS)74Corpus Tokens Sentences NEsEster2-Train 1 269 138 44 211 80 227Ester2-Dev 73 375 2 491 5 326Ester2-Test-corr 39 704 1 300 2 798Ester2-Test-held 47 446 1 683 3 067Table 2: Characteristics of Corpora?
CasEN-CRF: same as CRF, but the output ofCasEN is added as a single feature (concatena-tion of CasEN features)7 Experimentations7.1 Corpora and MetricsFor experimentations, we use the corpus that hasbeen made available after the Ester2 evaluationcampaign.
Table 2 gives statistics on diverse sub-parts of this corpus.
Unfortunately, many incon-sistencies where noted for manual annotation, es-pecially for ?Ester2-Train?
part that won?t be usedfor training.There were fewer irregularities in other parts ofthe corpus.
Although, manual corrections weredone on half of the Test corpus (Nouvel et al,2010) (Ester2-Test-corr in Table 2), to obtain agold standard that we will use to evaluate our ap-proach.
The remaining part of the Test corpus(Ester2-Test-held in Table 2) merged with the Devpart constitute our training set (Ester2-Dev in Ta-ble 2), used as well to extract rules with mineX-tract, to estimate stochastic model probabilities ofmStruct and to learn CRF models.We evaluate systems using following metrics:?
detect: rate of detection of the presence ofany marker (binary decision) at any position?
desamb: f-score of markers when comparingN actual markers to N most probable mark-ers, computed over positions where k mark-ers are expected (N=k) or the most probablemarker is not ?
(N=1)?
precision, recall, f-score: evaluation of NERby categories by examining labels assignedto tokens (similarly to Ester2 results)?
SER (Slot Error Rate): weighted error rate ofNER (official Ester2 performance metric, tobe lowered), where errors are discounted perentity as Galliano et al (2009) (deletion andinsertion errors are weighted 1 whereas typeand boundary errors, 0.5)System support confidence detect disamb f-score SERCasEN ?
?
?
?
78 30.8mXS 5 0.1 97 73 76 28.45 0.5 96 71 74 31.215 0.1 96 72 73 30.1Hybrid 5 0.1 97 78 79 26.35 0.5 97 77 77 28.315 0.1 97 78 76 28.2inf inf 96 71 70 42.0Table 3: Performance of Systems7.2 Comparing Hybridation with SystemsFirst, we separately evaluate systems.
WhileCasEN is not to be parameterized, mineXtracthas to be given minimum frequency and supportthresholds.
Table 3 shows results for each sys-tem separately and for the combination of sys-tems.
Results obtained by mXS show that evenless confident rules are improving performances.Generally speaking, the detect score is very high,but this mainly due to the fact that the ?
case isvery frequent.
The disamb score is much corre-lated to the SER.
This reflects the fact that thechallenge is for mStruct to determine the correctmarkers to insert.Comparing systems shows that the hybridiza-tion strategy is competitive.
The knowledge-based system yields to satisfying results.
mXSobtains slightly better SER and the hybrid sys-tem outperforms both in most cases.
ConsideringSER, the only exception to this is the ?inf?
line(mStruct uses only CasEN features) where perfor-mances are degraded.
We note that mStruct ob-tains better results as more rules are extracted.7.3 Assessing Hybridation Strategiesamount func loc org pers time all1020304050CasENmXSHybridHybrid-selFigure 8: SER of Systems by NE types75System precision recall f-score SERHybrid-sel 83.1 74.8 79 25.2CasEN-mXS-mine 76.8 75.5 76 29.4mXS-CasEN-vote 78.7 79.0 79 26.9CRF 83.8 77.3 80 26.1CasEN-CRF 84.1 77.5 81 26.0Table 4: Comparing performances of systemsIn a second step, we look in detail what NEtypes are the most accurately recognized.
Thoseresults are reported in Figure 8, where is depictedthe error rates (to be lowered) for main types(?prod?, being rare, is not reported).
This revealedthat features provided by CasEN for ?loc?
type ap-peared to be unreliable for mStruct.
Therefore, wefiltered-out related features, so as to couple sys-tems in a more efficient fashion.
This leads to a1.1 SER gain (from 26.3 to 25.2) when runningthe so-called ?Hybrid-sel?
system, and demon-strates that the hybridation is very sensitive towhat is provided by CasEN.With this constrained hybridization, we com-pare previous results to other hybridization strate-gies and a baseline CRF system as described insection 6.
Those experiments are reported in Ta-ble 4.
We see that, when considering SER, the hy-bridization strategy using CasEN features withinmStruct stochastic model slightly outperforms?simpler?
hybridizations schemes (pre-processingor post-processing with CasEN) and the CRFmodel (even when it uses CasEN preprocessingas a single unigram feature).However the f-score metric gives advantageto CasEN-CRF, especially when considering re-call.
By looking indepth into errors and when re-minded that SER is a weighted metric based onslots (entities) while f-score is based on tokens(see section 7.1), we noted that on longest NEs(mainly ?func?
), Hybrid-sel does type errors (dis-counted as 0.5 in SER) while CasEN-CRF doesdeletion errors (1 in SER).
This is pointed out byTable 5.
The influence of error?s type is clearwhen considering the SER for ?func?
type forwhich Hybrid-sel is better while f-score doesn?tmeasure such a difference.7.4 Discussion and PerspectivesAssessment of performances using a baselineCRF pre-processed by CasEN and the hybridedstrategy system shows that our approach is com-petitive, but do not allow to draw definitive con-System NE type insert delet type SER f-scoreHybrid-sel func 8 21 7 40.3 65all 103 205 210 25.2 79CasEN-CRF func 9 37 0 53.5 64all 77 251 196 26.0 81Table 5: Impact of ?func?
over SER and f-scoreclusions.
We keep in mind that the evaluated CRFcould be further improved.
Other methods havebeen successfully experimented to couple moreefficiently that kind of data-driven approach witha knowledge-based one (for instance Zidouni etal.
(2010) reports 20.3% SER on Ester2 test cor-pus, but they leverage training corpus).Nevertheless, the CRFs models do not allowto directly extract symbolic knowledge from data.We aim at organizing our NER system in a mod-ular way, so as to be able to adapt it to dedicatedtasks, even if no training data is available.
Resultsshow that this proposed hybridization reaches asatisfactory level of performances.This kind of hybridization, focusing on ?mark-ers?, is especially relevant for annotation tasks.As a next step, experiments are to be conductedon other tasks, especially those involving nestedannotations that our current system is able to pro-cess.
We will also consider how to better organizeand integrate automatically extracted informativerules into our existing knowledge-based system.8 ConclusionIn this paper, we consider Named Entity Recog-nition task as the ability to detect boundaries ofNamed Entities.
We use CasEN, a knowledge-based system based on transducers, and mineX-tract, a text-mining approach, to extract informa-tive rules from annotated texts.
To test these rules,we propose mStruct, a light multi-purpose annota-tor that has the originality to focus on boundariesof Named Entities (?markers?
), without consider-ing the labels associated to tokens.
The extractionmodule and the stochastic model are plugged to-gether, resulting in mXS, a NE-tagger that givessatisfactory results.
Those systems altogethermay be hybridized in an efficient fashion.
We as-sess performances of our approach by reportingresults of our system compared to other baselinehybridization strategies and CRF systems.76ReferencesSteven P. Abney.
1991.
Parsing by Chunks.
Principle-Based Parsing, 257?278.Rakesh Agrawal and Ramakrishnan Srikant.
1994.Fast algorithms for mining association rules in largedatabases.
Very Large Data Bases, 487?499.Fre?de?ric Bechet and Eric Charton.
2010.
Unsuper-vised knowledge acquisition for Extracting NamedEntities from speech.
Acoustics, Speech, and SignalProcessing (ICASSP?10), Dallas, USA.Andrew Borthwick, John Sterling, Eugene Agichteinand Ralph Grishman.
1998.
Exploiting Di-verse Knowledge Sources via Maximum Entropyin Named Entity Recognition.
Very Large Corpora(VLC?98), Montreal, Canada.Caroline Brun and Caroline Hage`ge.
2004.
Intertwin-ing Deep Syntactic Processing and Named EntityDetection.
Advances in Natural Language Process-ing, 3230:195-206.Caroline Brun and Maud Ehrmann.
2009.
Adapta-tion of a named entity recognition system for the es-ter 2 evaluation campaign.
Natural Language Pro-cessing and Knowledge Engineering (NLPK?09),Dalian, China.Beno?
?t Favre, Fre?de?ric Be?chet, and Pascal Nocera.2005.
Robust Named Entity Extraction from LargeSpoken Archives.
Human Language Technologyand Empirical Methods in Natural Language Pro-cessing (HLT/EMNLP?05), Vancouver, Canada.Nathalie Friburger.
2002.
Reconnaissance automa-tique des noms propres: Application a` la classifica-tion automatique de textes journalistiques.
PhD.Nathalie Friburger and Denis Maurel.
2004.
Finite-state transducer cascades to extract named entities.Theoretical Computer Sciences (TCS), 313:93?104.Sylvain Galliano, Guillaume Gravier and LauraChaubard.
2009.
The ESTER 2 evaluation cam-paign for the rich transcription of French radiobroadcasts.
International Speech CommunicationAssociation (INTERSPEECH?09), Brighton, UK.Philip Hingston.
2002.
Using Finite State Automatafor Sequence Mining.
Australasian Computer Sci-ence Conference (ACSC?02), Melbourne, Australia.Hideki Isozaki and Hideto Kazawa.
2002.
Efficientsupport vector classifiers for named entity recog-nition.
Conference on Computational linguistics(COLING?02), Taipei, Taiwan.Nicholas Kushmerick and Daniel S. Weld and RobertDoorenbos.
1997.
Wrapper Induction for Informa-tion Extraction.
International Joint Conference onArtificial Intelligence (IJCAI?97), Nagoya, Japan.John D. Lafferty, Andrew McCallum and FernandoC.
N. Pereira.
2001.
Conditional Random Fields:Probabilistic Models for Segmenting and LabelingSequence Data.
International Conference on Ma-chine Learning (ICML?01), Massachusetts, USA.Thomas Lavergne and Olivier Cappe?
and Franc?oisYvon 2010.
Practical Very Large Scale CRFs.
As-sociation for Computational Linguistics (ACL?10),Uppsala, Sweden.Heikki Mannila and Hannu Toivonen.
1997.
Level-wise search and borders of theories in knowledgediscovery.
Data Mining and Knowledge Discovery,1(3):241?258.Elaine Marsh and Dennis Perzanowski.
1998.
MUC-7Evaluation of IE Technology: Overview of Results.Message Understanding Conference (MUC-7).Andrew McCallum and Wei Li.
2003.
Early re-sults for named entity recognition with conditionalrandom fields, feature induction and web-enhancedlexicons.
Computational Natural Language Learn-ing (CONLL?03), Edmonton, Canada.Scott Miller, Michael Crystal, Heidi Fox, LanceRamshaw, Richard Schwartz, Rebecca Stone andRalph Weischedel.
1998.
Algorithms That LearnTo Extract Information BBN: Description Of TheSift System As Used For MUC-7.
Message Under-standing Conference (MUC-7).Damien Nouvel, Jean-Yves Antoine, NathalieFriburger and Denis Maurel.
2010.
An Analysisof the Performances of the CasEN Named EntitiesRecognition System in the Ester2 EvaluationCampaign.
Language Resources and Evaluation(LREC?10), Valetta, Malta.Fabian Pedregosa, Gae?l Varoquaux, Alexandre Gram-fort, Vincent Michel, Bertrand Thirion, OlivierGrisel, Mathieu Blondel, Peter Prettenhofer, RonWeiss, Vincent Dubourg, Jake Vanderplas, Alexan-dre Passos, David Cournapeau, Matthieu Brucher,Matthieu Perrot and E?douard Duchesnay.
2011.Scikit-learn: Machine Learning in Python.
Journalof Machine Learning Research, 12:2825?2830.Helmut Schmid.
1994.
Probabilistic POS TaggingUsing Decision Trees.
New Methods in LanguageProcessing (NEMLP?94, Manchester, UK.Boris W. van Schooten, Sophie Rosset, Olivier Galib-ert, Aure?lien Max, Rieks op den Akker, and GabrielIllouz.
2009.
Handling speech in the ritel QAdialogue system.
International Speech Communi-cation Association (INTERSPEECH?09), Brighton,UK.Ellen M. Voorhees and Donna Harman.
2000.Overview of the Ninth Text REtrieval Conference(TREC-9).
International Speech CommunicationAssociation (INTERSPEECH?09), Brighton, UK.Azeddine Zidouni and Sophie Rosset and Herve?
Glotin2010.
Efficient combined approach for namedentity recognition in spoken language.
Interna-tional Speech Communication Association (INTER-SPEECH?10), Makuhari, Japan.77
