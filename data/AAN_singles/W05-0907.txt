Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translationand/or Summarization, pages 49?56, Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsEvaluating DUC 2004 Tasks with the QARLA FrameworkEnrique Amigo?, Julio Gonzalo, Anselmo Pen?as, Felisa VerdejoDepartamento de Lenguajes y Sistemas Informa?ticosUniversidad Nacional de Educacio?n a Distanciac/Juan del Rosal, 16 - 28040 Madrid - Spain{enrique,julio,anselmo,felisa}@lsi.uned.esAbstractThis papers reports the application ofthe QARLA evaluation framework to theDUC 2004 testbed (tasks 2 and 5).
Ourexperiment addresses two issues: howwell QARLA evaluation measures corre-late with human judgements, and what ad-ditional insights can be provided by theQARLA framework to the DUC evalua-tion exercises.1 IntroductionQARLA (Amigo?
et al, 2005) is a framework thatuses similarity to models as a building block forthe evaluation of automatic summarisation systems.The input of QARLA is a summarisation task, a setof test cases, a set of similarity metrics, and sets ofmodels and automatic summaries (peers) for eachtest case.
With such a testbed, QARLA provides:?
A measure, QUEEN, which combines assortedsimilarity metrics to estimate the quality of au-tomatic summarisers.?
A measure, KING, to select the best combina-tion of similarity metrics.?
An estimation, JACK, of the reliability of thetestbed for evaluation purposes.The QARLA framework does not rely on humanjudges.
It is interesting, however, to find out howwell an evaluation using QARLA correlates with hu-man judges, and whether QARLA can provide ad-ditional insights into an evaluation based on humanassessments.In this paper, we apply the QARLA framework(QUEEN, KING and JACK measures) to the out-put of two different evaluation exercises: DUC 2004tasks 2 and 5 (Over and Yen, 2004).
Task 2 re-quires short (one-hundred word) summaries for as-sorted document sets; Task 5 consists of generatinga short summary in response to a ?Who is?
question.In Section 2, we summarise the QARLA evalua-tion framework; in Section 3, we describe the sim-ilarity metrics used in the experiments.
Section 4discusses the results of the QARLA framework us-ing such metrics on the DUC testbeds.
Finally, Sec-tion 5 draws some conclusions.2 The QARLA evaluation frameworkQARLA uses similarity to models for the evalua-tion of automatic summarisation systems.
Here wesummarise its main features; the reader may refer to(Amigo?
et al, 2005) for details.The input of the framework is:?
A summarisation task (e.g.
topic oriented, in-formative multi-document summarisation on agiven domain/corpus).?
A set T of test cases (e.g.
topic/document setpairs for the example above)?
A set of summaries M produced by humans(models), and a set of automatic summaries A(peers), for every test case.?
A set X of similarity metrics to compare sum-maries.With this input, QARLA provides three mainmeasures that we describe below.492.1 QUEEN : Estimating the quality of anautomatic summaryQUEEN operates under the assumption that a sum-mary is better if it is closer to the model summariesaccording to all metrics; it is defined as the probabil-ity, measured onM ?M ?M , that for every metricin X the automatic summary a is closer to a modelthan two models to each other:QUEENX,M (a) ?
P (?x ?
X.x(a,m) ?
x(m?,m??
))where a is the automatic summary being eval-uated, ?m,m?,m???
are three models in M , andx(a,m) stands for the similarity ofm to a. QUEENis stated as a probability, and therefore its range ofvalues is [0, 1].We can think of the QUEEN measure as using aset of tests (every similarity metric in X) to falsifythe hypothesis that a given summary a is a model.Given ?a,m,m?,m??
?, we test x(a,m) ?
x(m?,m??
)for each metric x. a is accepted as a model only ifit passes the test for every metric.
QUEEN(a) is,then, the probability of acceptance for a in the sam-ple space M ?M ?M .This measure has some interesting properties: (i)it is able to combine different similarity metricsinto a single evaluation measure; (ii) it is not af-fected by the scale properties of individual metrics,i.e.
it does not require metric normalisation andit is not affected by metric weighting.
(iii) Peerswhich are very far from the set of models all receiveQUEEN=0.
In other words, QUEEN does not distin-guish between very poor summarisation strategies.
(iv) The value of QUEEN is maximised for peersthat ?merge?
with the models under all metrics inX .
(v) The universal quantifier on the metric parameterx implies that adding redundant metrics do not biasthe result of QUEEN.Now the question is: which similarity metricsare adequate to evaluate summaries?
Imagine thatwe use a similarity metric based on sentence co-selection; it might happen that humans do not agreeon which sentences to select, and therefore emulat-ing their sentence selection behaviour is both easy(nobody agrees with each other) and useless.
Weneed to take into account which are the features thathuman summaries do share, and evaluate accordingto them.
This is provided by the KING measure.2.2 KING: estimating the quality of similaritymetricsThe measure KINGM,A(X) estimates the quality ofa set of similarity metrics X using a set of modelsM and a set of peers A.
KING is defined as theprobability that a model has higher QUEEN valuethan any peer in a test sample.
Formally:KINGM,A(X) ?P (?a ?
A,QUEENM,X(m) > QUEENM,X(a))For example, an ideal metric -that puts all modelstogether-would give QUEEN(m) = 1 for all mod-els, and QUEEN(a) = 0 for all peers which are notput together with the models, obtaining KING = 1.KING satisfies several interesting properties: (i)KING does not depend on the scale properties of themetric; (ii) Adding repeated or very similar peersdo not alter the KING measure, which avoids oneway of biasing the measure.
(iii) the KING value ofrandom and constant metrics is zero or close to zero.2.3 JACK: reliability of the peer setOnce we detect a difference in quality between twosummarisation systems, the question is now whetherthis result is reliable.
Would we get the same resultsusing a different test set (different examples, differ-ent human summarisers (models) or different base-line systems)?The first step is obviously to apply statistical sig-nificance tests to the results.
But even if they give apositive result, it might be insufficient.
The problemis that the estimation of the probabilities in KINGassumes that the sample sets M,A are not biased.If M,A are biased, the results can be statisticallysignificant and yet unreliable.
The set of examplesand the behaviour of human summarisers (models)should be somehow controlled either for homogene-ity (if the intended profile of examples and/or usersis narrow) or representativity (if it is wide).
But howto know whether the set of automatic summaries isrepresentative and therefore is not penalising certainautomatic summarisation strategies?This is addressed by the JACK measure:50JACK(X,M,A) ?
P (?a, a?
?
A|?x ?
X.x(a, a?)
?
x(a,m) ?
x(a?, a) ?
x(a?,m)?QUEEN(a) > 0 ?
QUEEN(a?)
> 0)i.e.
the probability over all model summariesm offinding a couple of automatic summaries a, a?
whichare closer to m than to each other according to allmetrics.
This measure satisfies three desirable prop-erties: (i) it can be enlarged by increasing the sim-ilarity of the peers to the models (the x(m,a) fac-tor in the inequalities), i.e.
enhancing the quality ofthe peer set; (ii) it can also be enlarged by decreas-ing the similarity between automatic summaries (thex(a, a?)
factor in the inequality), i.e.
augmenting thediversity of (independent) automatic summarisationstrategies represented in the test bed; (iii) adding el-ements to A cannot diminish the JACK value, be-cause of the existential quantifier on a, a?.3 Selection of similarity metricsEach different similarity metric characterises differ-ent features of a summary.
Our first objective isto select the best set of metrics, that is, the metricswhich best characterise the human summaries (mod-els) as opposed to automatic summaries.
The secondobjective is to obtain as much information as possi-ble about the behaviour of automatic summaries.In this Section, we begin by describing a set of59 metrics used as a starting point.
Some of themprovide overlapping information; the second step isthen to select a subset of metrics that minimises re-dundancy and, at the same time, maximises quality(KING values).
Finally, we analyse the characteris-tics of the selected metrics.3.1 Similarity metricsFor this work, we have considered the followingsimilarity metrics:ROUGE based metrics (R): ROUGE (Lin andHovy, 2003) estimates the quality of an au-tomatic summary on the basis of the n-gramcoverage related to a set of human summaries(models).
Although ROUGE is an evaluationmetric, we can adapt it to behave as a sim-ilarity metric between pairs of summaries ifwe consider only one model in the computa-tion.
There are different kinds of ROUGE met-rics such as ROUGE-W, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-3, ROUGE-4, etc.
(Lin,2004b).
Each of these metrics has been ap-plied over summaries with three preprocessingoptions: with stemming and stopword removal(type c); only with stopwords removal (type b);or without any kind of preprocessing (type a).All these combinations give 24 similarity met-rics based on ROUGE.Inverted ROUGE based metrics (Rpre): ROUGEmetrics are recall oriented.
If we reverse the di-rection of the similarity computation, we obtainprecision oriented metrics (i.e.
Rpre(a, b) =R(b, a)).
In this way, we generate another 24metrics based on inverted ROUGE.TruncatedVectModel (TVMn): This family of met-rics compares the distribution of the n mostrelevant terms from original documents in thesummaries.
The process is the following: (1)obtaining the n most frequent lemmas ignoringstopwords; (2) generating a vector with the rel-ative frequency of each term in the summary;(3) calculating the similarity between two vec-tors as the inverse of the Euclidean distance.We have used 9 variants of this measure withn = 1, 4, 8, 16, 32, 64, 128, 256, 512.AveragedSentencelengthSim (AVLS): This is a verysimple metric that compares the average lengthof the sentences in two summaries.
It can beuseful to compare the degree of abstraction ofthe summaries.GRAMSIM: This similarity metric compares thedistribution of the part-of-speech tags in thetwo summaries.
The processing is the follow-ing: (1) part-of-speech tagging of summariesusing TreeTagger ; (2) generation of a vectorwith the tags frequency for each summary; (3)calculation of the similarity between two vec-tors as the inverse of the Euclidean distance.This similarity metric is not content oriented,but syntax-oriented.51Figure 1: Similarity Metric Clusters3.2 Clustering similarity metricsFrom the set of metrics described above we have 57(24+24+9) content oriented metrics, plus two met-rics based on stylistic features (AVLS and GRAM-SIM).
However, the 57 metrics characterising sum-mary contents are highly redundant.
Thus, cluster-ing similar metrics seems desirable.We perform an automatic clustering process us-ing the following notion of proximity between twometric sets:sim(X,X ?)
?
Prob[H(X) ?
H(X ?
)]where H(X) ?
?x ?
X.x(a,m) ?
x(m?,m??
)Two metrics sets are similar, according to the for-mula, if they behave similarly with respect to theQUEEN condition (H predicate in the formula),i.e.
the probability that the two sets of metrics dis-criminate the same automatic summaries when theyare compared to the same pair of models.Figure 1 shows the clustering of similarity met-rics for the DUC 2004 Task 2.
The number of clus-ters was fixed in 10.
After the clustering process, the48 ROUGE metrics are grouped in 7 sets, and the 9TVM metrics are grouped in 3 sets.
In each clus-ter, the metric with highest KING has been markedin boldface.
Note that the ROUGE-c metrics (withstemming) with highest KING are those based on re-call whereas the ROUGE-a/b metrics (without stem-ming) are those based on precision.
Regarding TVMclusters, the metrics with highest KING in each clus-ter are those based on a higher number of terms.Finally, we select the metric with highest KINGin each group, obtaining the 10 most representativemetrics.3.3 Best evaluation metric: KING valuesFigure 2 shows the KING values for the selectedsimilarity metrics, which represent how every metriccharacterises model summaries as opposed to auto-matic summaries.
These are the main results:?
The last column shows the best metric set,considering all possible metric combinations.In both DUC tasks, the best combination is{Rpre-W-1.2.b,TVM.512.
This metric set getsbetter KING values than any individual metricin isolation (17% better than the second best fortask 2, and 23% better for task 5).
This is an in-teresting result confirming that we can improveour ability to characterise human summariesjust by combining standard similarity metricsin the QARLA framework.
Note also that bothmetrics in the best set are content-oriented.?
Rpre-W.1.2.b (inverted ROUGE measure, us-ing non-contiguous word sequences, remov-ing stopwords, without stemming) obtains thehighest individual KING for task 2, and is oneof the best in task 5, confirming that ROUGE-based metrics are a robust way of evaluatingsummaries, and indicating that non-contiguousword sequences can be more useful for evalua-tion purposes than n-grams.52Figure 2: Similarity Metric quality?
TVM metrics get higher values when consid-ering more terms (TVM.512), confirming thatcomparing with just a few terms (e.g.
TVM.4)is not informative enough.?
Overall, KING values are higher for task5, suggesting that there is more agreementbetween human summaries in topic-orientedtasks.3.4 Reliability of the resultsThe JACK measure estimates the reliability ofQARLA results, and is correlated with the diversityof automatic summarisation strategies included inthe testbed.
In principle, the larger the number of au-tomatic summaries, the higher the JACK values weshould obtain.
The important point is to determinewhen JACK values tend to stabilise; at this point, itis not useful to add more automatic summaries with-out introducing new summarisation strategies.Figure 3 shows how JACKRpre-W,TVM.512 valuesgrow when adding automatic summaries.
For morethan 10 systems, JACK values grow slower in bothtasks.
Absolute JACK values are higher in Task 2than in task 5, indicating that systems tend to pro-duce more similar summaries in Task 5 (perhaps be-cause it is a topic-oriented task).
This result suggeststhat we should incorporate more diverse summarisa-tion strategies in Task 5 to enhance the reliability ofthe testbed for evaluation purposes with QARLA.4 Evaluation of automatic summarisers:QUEEN valuesThe QUEEN measure provides two kinds of infor-mation to compare automatic summarisation sys-tems: which are the best systems -according to thebest metric set-, and which are the individual fea-tures of every automatic summariser -according toindividual similarity metrics-.4.1 System rankingThe best metric combination for both tasks was{Rpre-W,TVM.512}; therefore, our global systemevaluation uses this combination of content-orientedmetrics.
Figure 4 shows the QUEEN{Rpre-W,TVM.512}values for each participating system in DUC 2004,also including the model summaries.
As expected,model summaries obtain the highest QUEEN valuesin both DUC tasks, with a significant distance withrespect to the automatic summaries.4.2 Correlation with human judgementsThe manual ranking generated in DUC is based on aset of human-produced evaluation criteria, whereasthe QARLA framework gives more weight to the as-pects that characterise model summaries as opposedto automatic summaries.
It is interesting, however,to find out whether both evaluation methodologiesare correlated.
Indeed, this is the case: the Pearsoncorrelation between manual and QUEEN rankings is0.92 for the Task 2 and 0.96 for the Task 5.Of course, QUEEN values depend on the chosenmetric set X; it is also interesting to check whether53Figure 3: JACK vs.
Number of Automatic SummariesFigure 4: QUEEN system ranking for the best metric set (A-H are models)Figure 5: Correlation Between DUC and QARLA results54Figure 6: QUEEN values over GRAMSIMmetrics with higher KING values lead to QUEENrankings more similar to human judgements.
Fig-ure 5 shows the Pearson correlation between man-ual and QUEEN rankings for 1024 metric combina-tions with different KING values.
The figure con-firms that higher KING values are associated withrankings closer to human judgements.4.3 Stylistic featuresThe best metric combination leaves out similaritymetrics based on stylistic features.
It is interesting,however, to see how automatic summaries behavewith respect to this kind of features.
Perhaps themost remarkable fact about stylistic similarities isthat, in the case of the GRAMSIM metric, task 2and task 5 exhibit a rather different behaviour (seeFigure 6).
In task 2, systems merge with the models,while in task 5 the QUEEN values of the systemsare inferior to the models.
This suggests that thereis some stylistic component in models that systemsare not capturing in the topic-oriented task.5 Related workThe methodology which is closest to our frame-work is ORANGE (Lin, 2004a), which evaluates asimilarity metric using the average ranks obtainedby reference items within a baseline set.
As inour framework, ORANGE performs an automaticmeta-evaluation, there is no need for human assess-ments, and it does not depend on the scale propertiesof the metric being evaluated (because changes ofscale preserve rankings).
The ORANGE approachis, indeed, intimately related to the original QARLAmeasure introduced in (Amigo et al, 2004).There are several approaches to the automaticevaluation of summarisation and Machine Transla-tion systems (Culy and Riehemann, 2003; Coughlin,2003).
Probably the most significant improvementover ORANGE is the ability to combine automati-cally the information of different metrics.
Our im-pression is that a comprehensive automatic evalua-tion of a summary must necessarily capture differentaspects of the problem with different metrics, andthat the results of every individual checking (metric)should not be combined in any prescribed algebraicway (such as a linear weighted combination).
Ourframework satisfies this condition.ORANGE, however, has also an advantage overthe QARLA framework, namely that it can be usedfor evaluation metrics which are not based on sim-ilarity between model/peer pairs.
For instance,ROUGE can be applied directly in the ORANGEframework without any reformulation.6 ConclusionsThe application of the QARLA evaluation frame-work to the DUC testbed provides some useful in-sights into the problem of evaluating text summari-sation systems:?
The results show that a combination of simi-larity metrics behaves better than any metric inisolation.
The best metric set is {Rpre-W, TVM.512},a combination of content-oriented metrics.
Un-55surprisingly, stylistic similarity is less usefulfor evaluation purposes.?
The evaluation provided by QARLA correlateswell with the rankings provided by DUC hu-man judges.
For both tasks, metric sets withhigher KING values slightly outperforms thebest ROUGE evaluation measure.?
QARLA measures show that DUC tasks 2 and5 are quite different in nature.
In Task 5, humansummaries are more similar, and the automaticsummarisation strategies evaluated are less di-verse.AcknowledgementsWe are indebted to Ed Hovy, Donna Harman, PaulOver, Hoa Dang and Chin-Yew Lin for their inspir-ing and generous feedback at different stages in thedevelopment of QARLA.
We are also indebted toNIST for hosting Enrique Amigo?
as a visitor andfor providing the DUC test beds.
This work hasbeen partially supported by the Spanish government,project R2D2 (TIC-2003-7180).ReferencesE.
Amigo?, J. Gonzalo, A.
Pen?as, and F. Verdejo.
2005.QARLA: a Framework for the Evaluation of TextSummarization Systems.
In Proceedings of the 43rdAnnual Meeting of the Association for ComputationalLinguistics (ACL 2005).E.
Amigo, V. Peinado, J. Gonzalo, A.
Pen?as, andF.
Verdejo.
2004.
An Empirical Study of InformationSynthesis Tasks.
In Proceedings of the 42th AnnualMeeting of the Association for Computational Linguis-tics (ACL), Barcelona, July.Deborah Coughlin.
2003.
Correlating Automated andHuman Assessments of Machine Translation Quality.In In Proceedings of MT Summit IX, New Orleans,LA.Christopher Culy and Susanne Riehemann.
2003.
TheLimits of N-Gram Translation Evaluation Metrics.
InProceedings of MT Summit IX, New Orleans,LA.C.
Lin and E. H. Hovy.
2003.
Automatic Evaluation ofSummaries Using N-gram Co-ocurrence Statistics.
InProceeding of 2003 Language Technology Conference(HLT-NAACL 2003).C.
Lin.
2004a.
Orange: a Method for Evaluating Au-tomatic Metrics for Machine Translation.
In Pro-ceedings of the 36th Annual Conference on Compu-tational Linguisticsion for Computational Linguistics(Coling?04), Geneva, August.Chin-Yew Lin.
2004b.
Rouge: A Package for Auto-matic Evaluation of Summaries.
In Marie-FrancineMoens and Stan Szpakowicz, editors, Text Summariza-tion Branches Out: Proceedings of the ACL-04 Work-shop, pages 74?81, Barcelona, Spain, July.
Associa-tion for Computational Linguistics.P.
Over and J.
Yen.
2004.
An introduction to DUC 2004Intrinsic Evaluation of Generic New Text Summariza-tion Systems.
In Proceedings of DUC 2004 DocumentUnderstanding Workshop, Boston.56
