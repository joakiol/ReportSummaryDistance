Proceedings of the SIGDIAL 2013 Conference, pages 157?159,Metz, France, 22-24 August 2013. c?2013 Association for Computational LinguisticsMulti-step Natural Language UnderstandingPierrick Milhorat, Stephan Schlo?gl, Ge?rard CholletInstitut Mines-Te?le?comTe?le?com ParisTech, CNRS LTCIParis, France{lastname}@enst.frJe?ro?me BoudyInstitut Mines-Te?le?comTe?le?com SudParisParis, Franceboudy@telecom-sudparis.euAbstractWhile natural language as an interactionmodality is increasingly being accepted byusers, remaining technological challengesstill hinder its widespread employment.Tools that better support the design, devel-opment and improvement of these typesof applications are required.
This demopresents a prototyping framework for Spo-ken Dialog System (SDS) design whichcombines existing language technologycomponents for Automatic Speech Recog-nition (ASR), Dialog Management (DM),and Text-to-Speech Synthesis (TTS) witha multi-step component for Natural Lan-guage Understanding (NLU).1 IntroductionRecently speech and other types of natural lan-guage are experiencing an increased acceptancewhen being used for interacting with ?intelli-gent?
computing systems.
This trend is particu-larly reflected by products such as Apple?s Siri1,Google?s Now2 and Nuance?s Dragon Solutions3.While these applications demonstrate the indus-try?s vision of how we should be interacting withour current and future devices, they also highlightsome of the great challenges that still exist.
Oneof these challenges may be seen in the fact thatAutomatic Speech Recognition (ASR) remains ahighly error-prone technology which influencessubsequent natural language processing compo-nents such as Natural Language Understanding(NLU) and Dialog Management (DM) and leadsto often unsatisfying user experiences.
Hence werequire appropriate tools that better support thetesting and studying of language as an interaction1http://www.apple.com/ios/siri/2http://www.google.com/landing/now/3http://www.nuance.com/dragon/modality and consequently allow us to build bet-ter, more user-centered applications.This demo presents our approach of develop-ing a prototyping tool for Spoken Dialog Systems(SDS).
Our solution is particularly focusing onthe natural language understanding aspect of SDSdesign.
The overall framework is composed ofa set of existing open-source technology compo-nents (i.e.
ASR, DM, TTS) which are expandedby several additional NLP modules responsible fornatural language understanding as well as genera-tion.
The following sections first provide a generaloverview of the entire framework and then focusparticularly on the NLU part of our solution andthe different sub-modules it integrates.2 Spoken Dialog System DesignA state-of-the-art SDS usually consists of a set oftechnology components that are integrated to forma consecutive processing chain.
Starting on theinput side the ASR module produces a hypothe-sis about the orthographic content of a spoken ut-terance.
The NLU takes this recognized utteranceand converts it into a machine readable commandor input Dialog Act (DA).
The DM processes thisinput DA and sends the relevant output DA to theNatural Language Generation (NLG) component.The NLG is then responsible for converting theoutput DA into appropriate natural language text.Finally, the Text-to-Speech (TTS) synthesis com-ponent takes the text transmitted by the NLG andspeaks it to a user.According to this general architecture differentopen-source language components have been in-tegrated to form a loosely coupled SDS frame-work.
The framework includes ASR performed bythe Julius Large Vocabulary Continuous SpeechRecognition engine4, dialog management basedon the Disco DM library (Rich, 2009; Rich4http://julius.sourceforge.jp/en index.php157and Sidner, 2012) and TTS achieved through theMARY Text-to-Speech Synthesis Platform5.
Ad-ditionally, we have integrated the WebWOZ Wiz-ard of Oz Prototyping Platform6 (Schlo?gl et al2010) in order to allow for the simulation of (flaw-less) natural language understanding.
Expandingthese existing components we have then developedas a set of modules responsible for actual system-based natural language processing.
The followingsection describes these modules in more detail andhighlights the types of challenges they try to over-come.3 Natural Language UnderstandingWithin the processing chain of a spoken/text-based dialog system, the NLU component is thelink between the wide and informal communica-tion space of a user?s input and the formal andrather restrictive semantic space that can be pro-cessed by the DM (Mori et al 2007).
Trying tobridge these two spaces we have connected sev-eral modules to form an NLU processing segmentwhose different modules are described below.3.1 Semantic ParsingFirst we use a Semantic Parsing (SP) module toconvert the transcribed speech provided by theASR into so-called Semantic Frames (SFs).
Toachieve this mapping Jurc??
?c?ek et al(2009) de-signed a Transformation-Based Learning Seman-tic Parser (Brill, 1995) which we adapted to inte-grate it with our framework.
The algorithm appliesan ordered set of rules to hypothetical [utterance,SF] pairs in order to find the closest matching SF.3.2 Semantic UnificationNext we use what we call the Semantic Unifierand Reference Resolver (SURR) module to con-vert input SFs into SFs that can be processed bythe DM input interface.
To do this we imple-mented a bottom-up search algorithm for rewrit-ing trees whose nodes contain lists of valued slots.The algorithm looks for a group of root nodes thatcan be reached in the forest (i.e.
the existing num-ber of trees) by transforming an input SF?s set ofslots according to the given rewriting rules.
It suc-ceeds when all slots can be rewritten into a rootlist of slots.
This module is supported by exter-nal knowledge sources such as for example the5http://mary.dfki.de/6https://github.com/stephanschloegl/WebWOZcontext in which an utterance has been produced(i.e.
it receives input from the Context Catchermodule described below).
Furthermore it couldcall operating system functions, sensor readings7 or other knowledge sources capable of provid-ing relevant data, in order to resolve and disam-biguate input.
For instance, special-valued slotslike ?date=today?
are dynamically resolved to thecorrect data type and value, making the NLU moresensitive to its surrounding environment.3.3 Context InclusionIn order to optimize information exchangeHuman-Human interactions usually build up acommon knowledge between dialog participants.This inherent grounding process can be comparedto the dialog history recorded in an SDS?s DM.Using these recordings we have introduced a so-called Context Catcher (CC) module.
The waythis module is currently working is as follows: TheDM requests information from the user to progressthrough the task-oriented dialog.
The user replieswithout specifying the type of data he/she is pro-viding, the overall intent of the utterance or the re-lation to any dialog slot.
The CC evaluates the re-quest expressed by the DM and consequently up-dates various parameters of the SURR component.Consequently the SURR is able to provide a better,more context-specific mapping between raw SFsprovided by the SP module and the expected slotsto be filled by the DM component.3.4 Dialog Act ConversionAn SDS?s DM expects formal meaning represen-tations to be converted to actual dialog moves orDialog Acts (DA); similar to parametrized dialogcommands.
A DA is the smallest unit of determin-istic action to support the dialogue flow.
The num-ber of DAs that are available at any given point isfinite, dynamic and depends on the current state ofthe dialog (Note: Here a state does not refer to a?real?
state, such as the ones used in Markov De-cision Processes or Partially Observable MarkovDecision Processes, but rather to a general statusof the dialog).
In other words, two input utter-ances carrying the same meaning may lead to dif-ferent consequences depending on a given dialogstate.
The right action, i.e.
the accurate DA, is tobe determined by the NLU component.
As there7Note: At the moment sensor readings are not imple-mented as they are currently not available in the developingenvironment158is usually a many-to-many matching between SFsand actual DAs we integrated an additional DialogAct Converter (DAC) module.
This module usesthe context to generate a list of expected slots forwhich a user may provide a value (i.e.
it convertspossible DAs to SFs).
Then a matching betweenthe actual inputs and the expectations is applied inorder to find the most probable DA.4 Supporting Mixed InitiativesSDS dialog designs usually run along an initia-tive scale that ranges from user-driven to strictlymachine-driven interaction.
In the case of amachine-driven dialog a user has to follow the re-quests of the system.
Interactions that lie out of thescope of this dialog design are not understood andmay either be discarded or, in the worst case, leadto a system failure.
Despite this potential for fail-ure, machine-driven designs make the dialog eas-ier to control and thus less prone to errors, yet,due to the lack of adaptability exposed by the sys-tem, also less human-like.
On the other hand, pureuser-driven dialog designs minimize the functionalrange of a system as they only react to commandswithout assuring their functional integrity.The above described modular approach to NLUaims to support a mixed initiative design where asystem?s integrity and its goals are sufficiently de-fined; the user, however, is not restricted by thetype and amount of spoken input he/she can useto interact.
To offer this type of interaction thesystem needs to handle three kinds of potentialmis-usages: (1) out-of-application cases, (2) out-of-dialog cases and (3) out-of-turn cases.
To ad-dress the first one our training corpus has beenaugmented so that it includes examples of garbageSFs.
As a result an out-of-application utterancetriggers a generic reply from the system, notifyingthe user that he/she is outside the scope of the ap-plication.
In the case where a user stays withinthe scope of the application but tries to initiatea new unrelated dialog (i.e.
out-of-dialog case),the DM?s stack of tasks is incremented with thenew dialog.
The system will lead the user backto the previous topic once the newly added oneis completed.
Finally, as for the out-of-turn casesi.e.
the cases where a user would answer a sys-tem request with a non-expected utterance such asan over-complete one, the NLU process, retriev-ing the DM?s expectations, discards unrelated orover-complete information.5 Demo DescriptionFocusing on the NLU aspect of the SDS pipelinethis demo will demonstrate how the different mod-ules described above (i.e.
SP, SURR, CC, andDAC) work together.
An application scenariofrom the ambient assisted living domain (i.e.
theoperation of a ?Pillbox?
application) will serve asan example use case.
It will be shown how thenatural language input potentially recognized byan ASR component is further interpreted by ourNLU processing segment.
All the steps discussedin Section 3 will be visible.6 ConclusionIn this paper we described a set of NLU compo-nents that were integrated as part of a loosely cou-pled SDS.
Separate modules for semantic parsing,semantic unification and reference resolution, con-text inclusion as well as dialog act conversion havebeen described.
Furthermore we have highlightedhow our system offers support for mixed-initiativedialog interactions.
A first test of this NLU pro-cessing chain showed that the use of our multi-component approach is feasible, and we believethat this solution can be seen as a valuable test anddevelopment framework for natural language pro-cessing research.ReferencesE.
Brill.
1995.
Transformation-based error-drivenlearning and natural language processing: A casestudy in part-of-speech tagging.
Computational lin-guistics.F.
Jurc??
?c?ek, F. Mairesse, M.
Gas?ic?, S. Keizer, B. Thom-son, K. Yu, and S. Young.
2009.
Transformation-based Learning for semantic parsing.
Proceedingsof INTERSPEECH, pages 2719?2722.R.
De Mori, F. Be?chet, D. Hakkani-Tur, M. McTear,G.
Riccardi, and G. Tur.
2007.
Spoken languageunderstanding: A survey.
Proceedings of ASRU.C.
Rich and C. L. Sidner.
2012.
Using collaborativediscourse theory to partially automate dialogue treeauthoring.
Intelligent Virtual Agents, pages 327?340.C.
Rich.
2009.
Building task-based user interfaceswith ANSI/CEA-2018.
Computer.S.
Schlo?gl, G. Doherty, S. Luz, and N. Karamanis.2010.
WebWOZ: A Wizard of Oz PrototypingFramework.
In Proceedings of ACM EICS, pages109?114.159
