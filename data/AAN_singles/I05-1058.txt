Heuristic Methods for Reducing Errors ofGeographic Named Entities Learnedby BootstrappingSeungwoo Lee and Gary Geunbae LeeDepartment of Computer Science and Engineering,Pohang University of Science and Technology,San 31, Hyoja-dong, Nam-gu, Pohang, 790-784, Republic of KoreaAbstract.
One of issues in the bootstrapping for named entity recogni-tion is how to control annotation errors introduced at every iteration.
Inthis paper, we present several heuristics for reducing such errors usingexternal resources such as WordNet, encyclopedia and Web documents.The bootstrapping is applied for identifying and classifying fine-grainedgeographic named entities, which are useful for applications such as in-formation extraction and question answering, as well as standard namedentities such as PERSON and ORGANIZATION.
The experiments showthe usefulness of the suggested heuristics and the learning curve evalu-ated at each bootstrapping loop.
When our approach was applied to anewspaper corpus, it could achieve 87 F1 value, which is quite promisingfor the fine-grained named entity recognition task.1 IntroductionA bootstrapping process for named entity recognition is usually as follows.
Inthe initial stage, it selects seeds and annotates a raw corpus using the seeds.From the annotation, internal and contextual patterns are learned and appliedto the corpus again to obtain new candidates of each type.
Several methodsare adopted to reduce over-generation and incorrect annotation and accept onlycorrect ones.
One sense per discourse heuristic may also be adopted to expandthe annotated instances.
It repeats until no more new patterns and entitiesare learned.There are several issues in bootstrapping approaches for named entity recog-nition task to achieve successful performance.
One of them is how to controlannotation errors introduced in the bootstrapping process, on which we are fo-cusing in this paper.
As iteration continues, the bootstrapping expands previousannotation to increase recall.
But this expansion may also introduce annotationerrors and, as a result, decrease the precision.
Ambiguous entities may be mis-classified since learning speed per class depends on seeds.
For example, ?NewYork?
may be misclassified to a city name before the patterns that correctlyclassify it to a state name are learned.
Especially such errors in the early stageof the bootstrapping are quite harmful because the errors are accumulated.
TheR.
Dale et al (Eds.
): IJCNLP 2005, LNAI 3651, pp.
658?669, 2005.c?
Springer-Verlag Berlin Heidelberg 2005Heuristic Methods for Reducing Errors of Geographic Named Entities 659annotation errors are classified into following four cases: inclusion, crossing, typeconflict and spurious.
The first three errors occur when a learned entity overlapsa true entity whereas the last one occurs when a learned entity does not overlapany true entity.
Most previous works depend only on the statistics (e.g., scoresof patterns) obtained from the previous annotation to control such errors.
How-ever, this strategy is not always the best because some trivial errors can alsobe corrected by simple heuristics.
We suggest several heuristics that control theannotation errors in Section 4.
The heuristics are embedded in a bootstrappingalgorithm, which is modified and improved from [4] and shortly described inSection 3.Unlike the traditional named entity task, we deal with sub-categorized ge-ographic named entities (i.e., locations) in addition to PERSON and ORGA-NIZATION.
Geographic named entities can be classified into many sub-typesthat are critical for applications such as information extraction and questionanswering.
As a first step, we define their ten sub-classes: COUNTRY, STATE,COUNTY, CITY, MOUNTAIN, RIVER, ISLAND, LAKE, CONTINENT andOCEAN.
We attempt to identify and classify all instances of the eleven classes aswell as PERSON and ORGANIZATION in plain text.
Annotation of geographicnamed entities is a formidable task.
Geographic named entities are frequentlyshared between their sub-classes as well as with person names.
For example,?Washington?
may indicate a person in one context but may also mean a city orstate in another context.
Even country names cannot be exceptions.
For someAmericans, ?China?
and ?Canada?
may be cities where they live.
Geographicnamed entities such as ?Turkey?
and ?Chile?
can also be shared with commonnouns.
Contextual similarity among geographic named entities is much higherthan the one between PLO (Person-Location-Organization) entities since theyare much closer semantically.
These make geographic named entity annotationtask more difficult than that of the traditional named entity task.The remainder of this paper is as follows.
Section 2 presents and comparesrelated works to our approach.
The bootstrapping algorithm is shortly describedin Section 3 and several heuristics for controlling the annotation errors are ex-plained in Section 4.
Section 5 gives some experimental results verifying ourapproach, which is followed by conclusions and future works in Section 6.2 Related WorksMost bootstrapping approaches start with incomplete annotations and patternsobtained from selected seeds and learn to obtain more complete annotations andpatterns.
However, the incompleteness is apt to cause annotation errors to beintroduced in each bootstrapping iteration.
Most previous works have designedtheir own statistical measures to control such errors.
Phillips and Riloff [9] de-veloped evidence and exclusivity measures to filter out ambiguous terms andYangarber et al [12] calculated accuracy, confidence and score of their pat-terns to select better patterns.
However, those statistical measures are calcu-lated only using data obtained from their training corpus which cannot often660 S. Lee and G.G.
Leegive enough information.
Instead, other resources like World Wide Web as wellas a gazetteer can be incorporated to compensate the lack of information from thetraining corpus.Research on analysis of geographic references recently started to appear andhas two directions.
One is to focus on building gazetteer databases [6,11] andthe other is to focus on classifying geographic entity instances in text [5].Manov et al [6] presented KIM (Knowledge and Information Management)that consists of an ontology and a knowledge base.
They used it for informationextraction but did not show notable results.
Uryupina [11] presented a boot-strapping method to obtain gazetteers from the internet.
By searching for seednames on the internet, she obtained lexical patterns and learned each classi-fier for six location sub-types, such as COUNTRY, CITY, ISLAND, RIVER,MOUNTAIN and REGION.
Then she obtained and classified candidate namesby searching the patterns in the internet.
Li et al [5] suggested a hybrid approachto classify geographic entities already identified as location by an existing namedentity tagger.
They first matched local context patterns and then used a max-imum spanning tree search for discourse analysis.
They also applied a defaultsense heuristic as well as one sense per discourse principle.
According to theirexperiments, the default sense heuristic showed the highest contribution.3 BootstrappingOur bootstrapping algorithm was modified and improved from [4] and the boot-strapping flow has one initial step and four iterative steps, as shown in Figure 1.In the initial step, we annotate a raw corpus with seeds automatically obtainedfrom various gazetteers.
Starting and ending boundary patterns are learned fromthe annotation and applied to the corpus again to obtain new candidates ofeach type.
Then we eliminate annotation errors in the candidates using severalInitial AnnotationCorpus(raw  tagged(partial  complete))LearnBoundary PatternsExtractEntity CandidatesControlAnnotation ErrorsExpand/AcceptAnnotationBoundaryPatternsEntityCandidatesCorrectEntitiesSeedsAcceptedEntitiesFig.
1.
The bootstrapping overviewHeuristic Methods for Reducing Errors of Geographic Named Entities 661linguistic heuristics, which is described in detail in Section 4.
Finally, the re-maining entity candidates propagate their annotations into other occurrenceswithin the same document by one sense per discourse principle [2].
This loopcontinues until there are no new patterns learned.
The algorithm is summarizedas follows:Step 0: Seed Preparation and Initial AnnotationWe prepare seeds from the gazetteer and obtain initial entity candidate set, C1,by marking occurrences of the seeds in the training raw corpus.C1 = {ei|ei is an entity candidate obtained from seeds but not acceptedyet};And we initialize the number of iteration (k), the set of accepted boundarypatterns (P0) and the set of accepted entities (E0) as follows:k = 1; P0 = ?
; E0 = ?
;Step 1: Controlling the Annotation ErrorsWe filter out annotation errors among the entity candidates (Ck) using severalheuristics with external resources and construct Ek, a set of entities checked ascorrect (see Section 4).Ek = {ei|ei ?
Ck and ei is checked as correct by heuristics};Step 2: Expanding and Accepting the AnnotationAfter removing erroneous candidates, we expand the correct entities by applyingone sense per document heuristic and then accept M1 top-ranked entities toconstruct a new Ek, the set of currently accepted entities.Ek = {ei|ei ?
Ek or is an instance expanded from ej ?
Ek, andRank(ei) ?
Rank(ei+1), 1 ?
i ?
M};Ek = Ek?1 ?
Ek;The rank of an entity candidate, Rank(ei), is computed as follows:Rank(ei) = 1 ?
{1 ?
Score(BPs(ei))} ?
{1 ?
Score(BPe(ei))} (1)BPs(e) and BPe(e) indicate starting and ending boundary patterns of an entitye, respectively.1 M was set to 300 in our experiment.662 S. Lee and G.G.
LeeStep 3: Learning Boundary PatternsFrom the currently accepted entity set, Ek, we learn a new boundary pattern can-didate set, P?k.
We generate starting and ending boundary patterns and computethe accuracy (Acc(pi)) of each pattern pi which is used to filter out inaccuratepatterns below ?a2 and construct P?k.
Then we compute the score (Score(pi))of each pattern pi and add new N3 top-scored patterns among P?k to the ac-cepted boundary pattern set, Pk, if there exist new patterns in P?k.
Otherwise,the bootstrapping process stops.P?k = {pi|pi = BP (e), e ?
Ek and pi /?
Pk?1 and Acc(pi) ?
?a};If P?k = ?
then stop;Otherwise, Pk = Pk?1 ?
{pi|pi ?
P?k and Score(pi) ?
Score(pi+1), 1 ?i ?
N};The accuracy, Acc(p) and the score, Score(p), of a boundary pattern, p, arecomputed as follows:Acc(p) =pos(p)pos(p) + neg(p)?1 ?
1pos(p)2+11 ?
1Np2+1, (2)Score(p) =pos(p)pos(p) + 2 ?
neg(p) + unk(p) ?1 ?
1ln(pos(p)+3)1 ?
1ln(Np+3), (3)where pos(p) is the number of instances that are matched to p and alreadyannotated with the same entity type; neg(p) is the number of instances that arematched to p but already annotated with a different type or previously filteredout; unk(p) is the number of instances that are matched to p but not annotatedyet; Np is the maximum value of pos(p).Step 4: Applying Boundary Patterns and Extracting CandidatesWe extract new entity candidates, Ck+1, for the next iteration by applying theaccepted boundary patterns, Pk, to the training corpus and then go to Step 1.Ck+1 = {ei|BPs(ei) ?
Pk and BPe(ei) ?
Pk and ei /?
Ek};k := k + 1;Go to Step 1.Since each pattern determines only one ?
i.e., starting or ending ?
boundary, acandidate is identified and classified by a pair of starting and ending boundarypatterns with the same type.2 ?a was set to 0.1 in our experiment.3 N was set to 700 in our experiment.Heuristic Methods for Reducing Errors of Geographic Named Entities 6634 Error ControlsThe annotation errors introduced in the bootstrapping process are classified intofollowing four cases, based on the inconsistency between an erroneous entity can-didate and a true entity: inclusion, crossing, type conflict and spurious.
Inclusionoccurs when a candidate is a sub-phrase of a true entity ?
e.g., ?U.S.?
in ?U.S.Army?.
Crossing occurs when a candidate partially overlaps with a true entity?
e.g., ?Columbia River?
in ?British Columbia River?, which means a river in?British Columbia?.
Type conflict occurs when a candidate has the same textspan but different type from a true entity ?
e.g., ?New York?
may be misclassi-fied into STATE but it is CITY.
Spurious indicates that a candidate is spuriousand does not interfere with any true entities.To resolve these inconsistencies, we basically use statistical measures such asthe score of a boundary pattern, Score(p), and the rank of an entity candidate,Rank(e), as in most previous works.
However, this strategy is not always thebest because some trivial errors can also be removed by simple heuristics andlinguistic knowledge.
Especially, the strategy cannot be applied to erroneousentities whose inconsistencies cannot be detected since their true entities are notidentified yet.
We call it potential inconsistency.
We examine potential inclusionand potential type conflict for each entity candidate using the gazetteer and Webresources.
To overcome this limitation of statistical measures obtained from thetraining corpus, we design several methods that incorporate linguistic knowledgeand external resources, which are described in the following subsections.4.1 Co-occurrence InformationCo-occurrence information (CI) has been widely used to resolve word sense am-biguity [3,8,10] and also can be employed to resolve crossing and type conflictinconsistencies, which can be regarded as word sense ambiguity problem.
Weassume that two instances of an ambiguous entity that occur in different textscan be classified into the same class if they share their CI.
CI can be collectedfrom definition statements of an entity of an encyclopedia.
For example, theunderlined phrases are collected as CI of an entity ?Clinton?
with class CITYfrom a statement ?Clinton is a city in Big Stone County, Minnesota, USA?.
Inthis way, we could construct initial CI for 18000 entities from the Probert En-cyclopedia (http://www.probertencyclopaedia.com/places.htm), most of whichare geographic entities.
We also augment CI from the accepted entity instancesduring the bootstrapping process.
We consider capitalized nouns or noun phrasesin the window of up to left/right 60 words, within sentence boundary, from anentity as its CI.
Then, the score of an entity e with class t, Coinfo(e, t), iscalculated as the similarity of CI:Coinfo(e, t) =?Ni=1 freq(cwi, e, t) ?
count(cwi, e)N, (4)where N is the number of co-occurrence information cwi, freq(cwi, e, t) meansthe frequency of cwi co-occurring with an entity e of class t in the learned664 S. Lee and G.G.
Leeco-occurrence information and count(cwi, e) means the frequency of cwi co-occurring with the entity in the current pending context.
When two candidatescause crossing or type conflict, the candidate having smaller Coinfo is consid-ered to be incorrect and removed.4.2 Gazetteer with LocatorMost entities are often mentioned with geographic entities where they are lo-cated, especially when they are not familiar to general readers.
For example,?Dayton?
in ?the Dayton Daily News, Dayton, Ohio?
is restricted to an entityin ?Ohio?.
This means that we can classify ?Dayton?
into CITY if we know afact that there is a city named ?Dayton?
and located at ?Ohio?.
We can say thatthe locator information is a special case of the co-occurrence information.
Thelocator information was also collected from the Probert Encyclopedia.
If one oftwo entity candidates causing crossing or type conflict has a verified locator, theother can be regarded as an error and removed.4.3 Prior ProbabilityAmbiguous entities often have different prior probability according to each class.For example, ?China?
appears frequently in general text as a country name butrarely as a city name.
?Canada?
is another example.
This means that whentwo entity candidates cause type conflict we can remove one having lower priorprobability.
It is hard to acquire such probabilities if we do not have a largeannotated corpus.
However, WordNet [7] can give us the information that isneeded to infer the relative prior probability since the sense order in WordNetreflects the frequency that the sense appears in text.
According to WordNet, forexample, ?New York?
is more frequently mentioned as a city name than as a statename and, therefore, is classified into CITY if its context does not give stronginformation that it is a state name.
We could construct relative prior probabilitiesfor 961 ambiguous gazetteer entries from WordNet.
The prior probability ofentity e with type t based on WordNet, PriorWN (e, t), is calculated as follows:PriorWN (e, t)={1N+1 + ?WN ?
(m+1)?Sense#W N (e,t)?
mi=1 iif there exist in WordNet1N+1 ?
?WN otherwise,(5)where N is the number of possible types of entity candidate e, m is the number oftypes of entity candidate e registered in WordNet, and Sense#WN (e, t) meansthe WordNet sense no.
of entity candidate e with type t. ?WN and ?WN arecalculated as follows:?WN = ?WN ?
(N ?
m) +1N + 1?WN =m(N + 1)2Heuristic Methods for Reducing Errors of Geographic Named Entities 665Based on these formulas, the prior probabilities of an entity ?New York ?
aregiven as follows according to its type: (CITY, 0.44), (STATE, 0.32), (TOWN,0.12), and (COUNTY, 0.12).4Although this prior probability is quite accurate, it does not have sufficientapplicability.
Therefore, we need to develop another method that can acquireprior probabilities of much more entities and Web can be one alternative.
Foreach ambiguous entity X, we query ?X is a/an?
to at least two Web searchengines5 and extract and collect a noun phrase Y matching to ?X is a/an Y?.Then, we determine a type, which Y belongs to, using WordNet and count itsfrequency.
This frequency for each possible type of the entity X is regardedas sense order information.
That is, we can assign to each possible type a sensenumber in the descending order of the frequency.
Now, the prior probability of anentity e with type t based on the Web, PriorWeb(e, t), can be similarly calculated.Then, the final prior probability, Prior(e, t), is computed by arithmetic meanof PriorWN (e, t) and PriorWeb(e, t).
Combined with the Web search, the priorprobabilities of the above example are changed as follows: (CITY, 0.36), (STATE,0.29), (TOWN, 0.18), and (COUNTY, 0.17).4.4 Default TypeWhen an ambiguous candidate causing type conflict is not registered in WordNetand cannot be detected by the Web search, we can apply default type heuristic.Unlike the prior probability, default type indicates a priority between any twotarget classes regardless of each individual entity.
In general, we can say that, foran ambiguous entity between COUNTRY and CITY, COUNTRY is more dom-inant than CITY since a country name is more familiar to common people.
Webuilt up default types between all pairs of target classes using human linguisticknowledge and prior probability described in the previous subsection.4.5 Part of Other EntityPotential inclusion is often not exposed at a bootstrapping iteration since bound-ary patterns for each class are generated at different speeds and, in addition, allrequired boundary patterns cannot be generated from seeds.
For this, we designtwo methods in addition to gazetteer consulting.First, we check if there exists an acronym for a super-phrase.
[1] says thatwe can consult a commonly-used acronym to determine extent of a named en-tity.
In other words, ?University of California, Los Angeles?, for example, must4 WordNet does not have COUNTY and TOWN senses of ?New York ?.5 We used eight well-known Web search engines such as Google(http://www.google.com/), Ask Jeeves (http://web.ask.com/), AltaVista(http://www.altavista.com/), LookSmart (http://search.looksmart.com/),Teoma (http://s.teoma.com/), AlltheWeb (http://www.alltheweb.com/), Ly-cos (http://search.lycos.com/), and Yahoo!
(http://search.yahoo.com/).
Wespecially thank to the service providers.666 S. Lee and G.G.
Leebe annotated as a unique organization name since the university is commonlyreferred to as ?UCLA?.
As an another example, ?U.S.?
in ?U.S.
Navy?
shouldnot be annotated as a country name but ?U.S.?
in ?U.S.
President?
should besince ?U.S.
Navy?
is represented as the acronym ?USN?
but ?U.S.
President?
isnot represented as ?USP?.
To check the existence of their acronyms, we can con-sult Web search engines by querying the suspected phrases with their possibleacronyms, such as ?U.S.
Navy (USN)?
and ?U.S.
President (USP)?, respectively,with exact match option.Another solution is to check if a super-phrase beginning with a candidatewhose class is one of geographic classes can be modified by a prepositional phrasewhich is derived by in or comma (,) plus the candidate (denoted as in-loc).
Forexample, we can decide that ?Beijing?
in ?Beijing University?
is a part of theuniversity name, since the phrase ?Beijing University in Beijing?
is found byWeb search engines.
If the ?Beijing?
denotes CITY, ?Beijing University?
meansa university in Beijing and is not modified by the prepositional phrase ?inBeijing?
duplicately.5 ExperimentsThe bootstrapping algorithm was developed and trained on part of New YorkTimes articles (the first half of June, 1998; 28MB; 5,330 articles) from theAQUAINT corpus.
We manually annotated 107 articles for test and the countsof annotated instances were listed in Table 1.
A gazetteer composed of 80,000entries was compiled from several Web sites6.
This includes non-target entitiesas well as various aliases of entity names.Table 1.
The counts of instances annotated in the test corpusCOUNTRYSTATECOUNTYCITYRIVERMOUNT.ISLANDCONTI.OCEANLAKEPERSONORGAN.Total596 422 61 868 26 15 29 74 19 9 2,660 1,436 6,215We first examined the usefulness of the heuristics, based on the instances(i.e., key instances) annotated in the test corpus.
Applicability (app.)
is definedas the number of key instances (denoted as #app), to which the heuristic can beapplied, divided by the number of ambiguous ones (denoted as #ambi).
Accuracy(acc.)
is defined as the number of instances correctly resolved (denoted as #corr)divided by #app.
There were 2250 ambiguous key instances in the test corpus.6 http://www.census.gov/, http://crl.nmsu.edu/Resources/resource.htm,http://www.timeanddate.com/, http://www.probertencyclopaedia.com/places.htm,http://www.world-of-islands.com/, and http://islands.unep.ch/isldir.htmHeuristic Methods for Reducing Errors of Geographic Named Entities 667Applicability and accuracy of the first four heuristics for resolving type conflictare summarized in Table 2.
As shown in the table, the first two heuristics ?
co-occurrence information and gazetteer with locator ?
have very low applicabilitybut very high accuracy.
On the contrary, the last two heuristics ?
prior probabilityand default type ?
show moderate accuracy with relatively high applicability.Based on this result, we combine the four heuristics in sequence such as highaccurate one first and high applicable one last.We also examined how well the heuristics such as acronym and in-loc candetect potential inclusion of an entity.
In case of acronym, there were 2,555 keyinstances (denoted as #app) composed of more than one word and we searchedthe Web to check the existence of any possible acronym of each instance.
Asa result, we found out the correct acronyms for 1,143 instances (denoted as#corr).
On the contrary, just 47 instances were incorrectly matched when wetried to search any acronyms of super-phrases of each key instance.
In otherwords, acronym can detect potential inclusion at 46.58 applicability and 96.05accuracy.
In case of in-loc, 1,282 key instances beginning with a geographic wordare tried to be checked if they appear with in-loc pattern in Web documents and313 instances of them were confirmed.
On the contrary, only 1 super-phrase ofa key instance was incorrectly detected.
Therefore, in-loc can detect potentialinclusion at 24.49 applicability and 99.68 accuracy.
These are summarized inTable 3.
It says that the heuristics can detect quite accurately the extent ofnamed entities although they do not have high applicability.Finally, we evaluated the bootstrapping with the heuristics by investigatingthe performance change at every iteration.
50,349 seeds were selected from thegazetteer after removing ambiguous ones and only 3,364 seeds among them,which could be applied to the training corpus, were used for training.
The recalland precision were measured using the standard MUC named entity scoringscheme and plotted in Figure 2.
Starting at low recall and high precision, itgradually increases recall but slightly degrades precision, and it arrived at 87 F1Table 2.
Applicability and accuracy of the heuristics for resolving the inconsistencyof 2,250 ambiguous instances (#ambi=2,250)#app #corr app.
acc.co.
info.
44 42 1.96 95.45gaz.
loc.
148 141 6.58 95.27prior prob.
2,072 1,741 92.09 84.03def.
type 2,225 1,367 98.89 61.44Table 3.
Applicability and accuracy of the heuristics for detecting potential inclusion#ambi #app #corr app.
acc.acronym 2,555 1,190 1,143 46.58 96.05in-loc 1,282 314 313 24.49 99.68668 S. Lee and G.G.
Lee8590951000 10 20 30 40 50 60 70 80 90RecallPre cis ionwithout heuristicswith heuristicsFig.
2.
The learning curve of the bootstrapping with the heuristics(81 recall and 93 precision) after 1,100 iterations.
We think that this performanceis quite notable considering our fine-grained target classes, and the suggestedheuristics work well to prevent incorrect entity candidates from being acceptedduring bootstrapping process.6 ConclusionsIn this paper, we observed four kinds of inconsistencies that degrade the per-formance of bootstrapping for named entity recognition with fine-grained geo-graphic classes.
To resolve such inconsistencies, we suggested several heuristicsincorporating human linguistic knowledge and external resources like encyclope-dia and Web documents.
By analyzing the capability of each heuristic, we com-bined them in sequence.
The bootstrapping with the heuristics was evaluated.Starting at low recall and high precision, the bootstrapping largely increasedrecall at a small cost of precision, and finally it achieved 87 F1.
This meansthat the suggested approach is quite promising for the fine-grained named entityrecognition task and the suggested heuristics can effectively reduce incorrect can-didates introduced at the intermediate bootstrapping steps.
In future, we planto design a uniform statistical method that can augment the suggested heuris-tics especially using Web resources and also incorporate our heuristic knowledgeused for filtering into the statistical model.AcknowledgementsThis work was supported by 21C Frontier Project on Human-Robot Interface(MOCIE) and by BK21 Project (Ministry of Education).Heuristic Methods for Reducing Errors of Geographic Named Entities 669References1.
Chinchor, N., Brown, E., Ferro, L., Robinson, P.: 1999 Named EntityRecognition Task Definition (version 1.4).
http://www.nist.gov/speech/tests/ie-er/er 99/doc/ne99 taskdef v1 4.pdf (1999)2.
Gale, W.A., Church, K.W., Yarowsky, D.: One Sense Per Discourse.
In: Proceedingsof the 4th DARPA Speech and Natural Language Workshop.
(1992) 233?2373.
Guthrie, J.A., Guthrie, L., Wilks, Y., Aidinejad, H.: Subject-dependent Co-occurrence and Word Sense Disambiguation.
In: Proceedings of the 29th AnnualMeeting of the Association for Computational Linguistics (ACL), Berkeley, CA(1991) 146?1524.
Lee, S., Lee, G.G.
: A Bootstrapping Approach for Geographic Named EntityAnnotation.
In: Proceedings of the 2004 Conference on Asia Information RetrievalSymposium (AIRS2004), Beijing, China (2004) 128?1335.
Li, H., Srihari, R.K., Niu, C., Li, W.: InfoXtract location normalization: a hybridapproach to geographic references in information extraction.
In: Proceedings ofthe HLT-NAACL 2003 Workshop on Analysis of Geographic References, Alberta,Canada (2003) 39?446.
Manov, D., Kirjakov, A., Popov, B., Bontcheva, K., Maynard, D., Cunningham, H.:Experiments with geographic knowledge for information extraction.
In: Proceed-ings of the HLT-NAACL 2003 Workshop on Analysis of Geographic References,Alberta, Canada (2003) 1?97.
Miller, G.A.
: WordNet: A lexical database for English.
Communications of theACM 38 (1995) 39?418.
Niwa, Y., Nitta, Y.: Co-occurrence Vectors from Corpora vs Distance Vectors fromDictionaries.
In: Proceedings of the 15th International Conference on Computa-tional Linguistics (COLING?94), Kyoto, Japan (1994) 304?3099.
Phillips, W., Riloff, E.: Exploiting Strong Syntactic Heuristics and Co-Trainingto Learn Semantic Lexicons.
In: Proceedings of the 2002 Conference on EmpiricalMethods in Natural Language Processing (EMNLP2002), Philadelphia, PA (2002)125?13210.
Shin, S., soek Choi, Y., Choi, K.S.
: Word Sense Disambiguation Using Vectorsof Co-occurrence Information.
In: Proceedings of the Sixth Natural LanguageProcessing Pacific Rim Symposium (NLPRS2001), Tokyo, Japan (2001) 49?5511.
Uryupina, O.: Semi-supervised learning of geographical gazetteers from the inter-net.
In: Proceedings of the HLT-NAACL 2003 Workshop on Analysis of GeographicReferences, Alberta, Canada (2003) 18?2512.
Yangarber, R., Lin, W., Grishman, R.: Unsupervised Learning of GeneralizedNames.
In: Proceedings of the 19th International Conference on ComputationalLinguistics (COLING 2002), Taipei, Taiwan (2002) 1135?1141
