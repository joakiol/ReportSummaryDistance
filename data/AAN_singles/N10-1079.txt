Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 546?554,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsOnline Learning for Interactive Statistical Machine TranslationDaniel Ortiz-Mart??nezDpto.
de Sist.
Inf.
y Comp.Univ.
Polite?c.
de Valencia46071 Valencia, Spaindortiz@dsic.upv.esIsmael Garc??a-VareaDpto.
de Informa?ticaUniv.
de Castilla-La Mancha02071 Albacete, Spainivarea@info-ab.uclm.esFrancisco CasacubertaDpto.
de Sist.
Inf.
y Comp.Univ.
Polite?c.
de Valencia46071 Valencia, Spainfcn@dsic.upv.esAbstractState-of-the-art Machine Translation (MT)systems are still far from being perfect.
An al-ternative is the so-called Interactive MachineTranslation (IMT) framework.
In this frame-work, the knowledge of a human translator iscombined with a MT system.
The vast ma-jority of the existing work on IMT makes useof the well-known batch learning paradigm.In the batch learning paradigm, the training ofthe IMT system and the interactive translationprocess are carried out in separate stages.
Thisparadigm is not able to take advantage of thenew knowledge produced by the user of theIMT system.
In this paper, we present an ap-plication of the online learning paradigm tothe IMT framework.
In the online learningparadigm, the training and prediction stagesare no longer separated.
This feature is par-ticularly useful in IMT since it allows the userfeedback to be taken into account.
The onlinelearning techniques proposed here incremen-tally update the statistical models involved inthe translation process.
Empirical results showthe great potential of online learning in theIMT framework.1 IntroductionInformation technology advances have led to theneed for more efficient translation methods.
CurrentMT systems are not able to produce ready-to-usetexts.
Indeed, MT systems usually require humanpost-editing to achieve high-quality translations.One way of taking advantage of MT systems is tocombine them with the knowledge of a human trans-lator in the IMT paradigm, which is a special type ofthe computer-assisted translation paradigm (Isabelleand Church, 1997).
An important contribution toIMT technology was pioneered by the TransTypeproject (Foster et al, 1997; Langlais et al, 2002)where data driven MT techniques were adapted fortheir use in an interactive translation environment.Following the TransType ideas, Barrachina etal.
(2009) proposed a new approach to IMT, in whichfully-fledged statistical MT (SMT) systems are usedto produce full target sentence hypotheses, or por-tions thereof, which can be partially or completelyaccepted and amended by a human translator.
Eachpartial, correct text segment is then used by theSMT system as additional information to achieveimproved suggestions.
Figure 1 illustrates a typicalIMT session.source(f ): Para ver la lista de recursosreference(e?
): To view a listing of resourcesinter.-0epes To view the resources listinter.-1ep To viewk aes list of resourcesinter.-2ep To view a listk list ies list i ng resourcesinter.-3ep To view a listingk oes o f resourcesaccept ep To view a listing of resourcesFigure 1: IMT session to translate a Spanish sen-tence into English.
In interaction-0, the system sug-gests a translation (es).
In interaction-1, the usermoves the mouse to accept the first eight characters?To view ?
and presses the a key (k), then the sys-tem suggests completing the sentence with ?list ofresources?
(a new es).
Interactions 2 and 3 are sim-ilar.
In the final interaction, the user accepts the cur-rent suggestion.In this paper, we also focus on the IMT frame-work.
Specifically, we present an IMT system that isable to learn from user feedback.
For this purpose,we apply the online learning paradigm to the IMTframework.
The online learning techniques that wepropose here allow the statistical models involved inthe translation process to be incrementally updated.546Figure 2 (inspired from (Vidal et al, 2007)) showsa schematic view of these ideas.
Here, f is the in-put sentence and e is the output derived by the IMTsystem from f .
By observing f and e, the user inter-acts with the IMT system until the desired output e?
isproduced.
The input sentence f and its desired trans-lation e?
can be used to refine the models used by thesystem.
In general, the model is initially obtainedthrough a classical batch training process from a pre-viously given training sequence of pairs (fi,ei) fromthe task being considered.
Now, the models can beextended with the use of valuable user feedback.efekfInteractiveSMT SystemBatchLearningOnlineLearning.
.
.f  , e2    2f  , e1    1feedback/interactionsef ^^IncrementalModelsFigure 2: An Online Interactive SMT system2 Interactive machine translationIMT can be seen as an evolution of the SMT frame-work.
Given a sentence f from a source lan-guage F to be translated into a target sentence eof a target language E , the fundamental equation ofSMT (Brown et al, 1993) is the following:e?
= argmaxe{Pr(e | f)} (1)= argmaxe{Pr(f | e)Pr(e)} (2)where Pr(f | e) is approximated by a translationmodel that represents the correlation between thesource and the target sentence and where Pr(e) isapproximated by a language model representing thewell-formedness of the candidate translation e.State-of-the-art statistical machine translationsystems follow a loglinear approach (Och and Ney,2002), where direct modelling of the posterior prob-ability Pr(e | f) of Equation (1) is used.
In this case,the decision rule is given by the expression:e?
= argmaxe{M?m=1?mhm(e, f)}(3)where each hm(e, f) is a feature function represent-ing a statistical model and ?m its weight.Current MT systems are based on the use ofphrase-based models (Koehn et al, 2003) as transla-tion models.
The basic idea of Phrase-based Trans-lation (PBT) is to segment the source sentence intophrases, then to translate each source phrase into atarget phrase, and finally to reorder the translatedtarget phrases in order to compose the target sen-tence.
If we summarize all the decisions made dur-ing the phrase-based translation process by means ofthe hidden variable a?K1 , we obtain the expression:Pr(f |e) =?K,a?K1Pr(f?K1 , a?K1 | e?K1 ) (4)where each a?k ?
{1 .
.
.K} denotes the index of thetarget phrase e?
that is aligned with the k-th sourcephrase f?k, assuming a segmentation of length K.According to Equation (4), and following a max-imum approximation, the problem stated in Equa-tion (2) can be reframed as:e?
?
arg maxe,a{p(e) ?
p(f ,a | e)}(5)In the IMT scenario, we have to find an extensiones for a given prefix ep.
To do this we reformulateEquation (5) as follows:e?s ?
arg maxes,a{p(es | ep) ?
p(f ,a | ep, es)}(6)where the term p(ep) has been dropped since it doesdepend neither on es nor on a.Thus, the search is restricted to those sentences ewhich contain ep as prefix.
It is also worth mention-ing that the similarities between Equation (6) andEquation (5) (note that epes ?
e) allow us to usethe same models whenever the search procedures areadequately modified (Barrachina et al, 2009).Following the loglinear approach stated in Equa-tion (3), Equation (6) can be rewriten as:e?s = argmaxes,a{M?m=1?mhm(e,a, f)}(7)547which is the approach that we follow in this work.A common problem in IMT arises when the usersets a prefix (ep) which cannot be found in thephrase-based statistical translation model.
Differ-ent solutions have been proposed to deal with thisproblem.
The use of word translation graphs, as acompact representation of all possible translationsof a source sentence, is proposed in (Barrachinaet al, 2009).
In (Ortiz-Mart?
?nez et al, 2009), atechnique based on the generation of partial phrase-based alignments is described.
This last proposal hasalso been adopted in this work.3 Related workIn this paper we present an application of the onlinelearning paradigm to the IMT framework.
In the on-line learning setting, models are trained sample bysample.
Our work is also related to model adapta-tion, although model adaptation and online learningare not exactly the same thing.The online learning paradigm has been previ-ously applied to train discriminative models inSMT (Liang et al, 2006; Arun and Koehn, 2007;Watanabe et al, 2007; Chiang et al, 2008).
Theseworks differ from the one presented here in that weapply online learning techniques to train generativemodels instead of discriminative models.In (Nepveu et al, 2004), dynamic adaptation ofan IMT system via cache-based model extensions tolanguage and translation models is proposed.
Thework by Nepveu et al (2004) constitutes a domainadaptation technique and not an online learningtechnique, since the proposed cache components re-quire pre-existent models estimated in batch mode.In addition to this, their IMT system does not usestate-of-the-art models.To our knowledge, the only previous work on on-line learning for IMT is (Cesa-Bianchi et al, 2008),where a very constrained version of online learn-ing is presented.
This constrained version of onlinelearning is not able to extend the translation modelsdue to technical problems with the efficiency of thelearning process.
In this paper, we present a purelystatistical IMT systemwhich is able to incrementallyupdate the parameters of all of the different modelsthat are used in the system, including the transla-tion model, breaking with the above mentioned con-straints.
What is more, our system is able to learnfrom scratch, that is, without any preexisting modelstored in the system.
This is demonstrated empiri-cally in section 5.4 Online IMTIn this section we propose an online IMT system.First, we describe the basic IMT system involvedin the interactive translation process.
Then we in-troduce the required techniques to incrementally up-date the statistical models used by the system.4.1 Basic IMT systemThe basic IMT system that we propose uses a log-linear model to generate its translations.
Accordingto Equation (7), we introduce a set of seven featurefunctions (from h1 to h7):?
n-gram language model (h1)h1(e) = log(?|e|+1i=1 p(ei|ei?1i?n+1)), 1 wherep(ei|ei?1i?n+1) is defined as follows:p(ei|ei?1i?n+1) =max{cX(eii?n+1)?Dn, 0}cX(ei?1i?n+1)+DncX(ei?1i?n+1)N1+(ei?1i?n+1?)
?
p(ei|ei?1i?n+2) (8)where Dn = cn,1cn,1+2cn,2 is a fixed discount (cn,1and cn,2 are the number of n-grams with oneand two counts respectively),N1+(ei?1i?n+1?)
is thenumber of unique words that follows the historyei?1i?n+1 and cX(eii?n+1) is the count of the n-grameii?n+1, where cX(?)
can represent true countscT (?)
or modified counts cM (?).
True counts areused for the higher order n-grams and modifiedcounts for the lower order n-grams.
Given a cer-tain n-gram, its modified count consists in thenumber of different words that precede this n-gram in the training corpus.Equation (8) corresponds to the probability givenby an n-gram language model with an interpolatedversion of the Kneser-Ney smoothing (Chen andGoodman, 1996).1|e| is the length of e, e0 denotes the begin-of-sentence sym-bol, e|e|+1 denotes the end-of-sentence symbol, eji ?
ei...ej548?
target sentence-length model (h2)h2(e, f) = log(p(|f | | |e|)) = log(?|e|(|f |+0.5)?
?|e|(|f | ?
0.5)), where ?|e|(?)
denotes the cumula-tive distribution function (cdf) for the normal dis-tribution (the cdf is used here to integrate the nor-mal density function over an interval of length 1).We use a specific normal distribution with mean?|e| and standard deviation ?|e| for each possibletarget sentence length |e|.?
inverse and direct phrase-based models (h3, h4)h3(e,a, f) = log(?Kk=1 p(f?k|e?a?k)), wherep(f?k|e?a?k) is defined as follows:p(f?k|e?a?k) = ?
?
pphr(f?k|e?a?k) +(1?
?
).phmm(f?k|e?a?k) (9)In Equation (9), pphr(f?k|e?a?k) denotes the proba-bility given by a statistical phrase-based dictionaryused in regular phrase-based models (see (Koehnet al, 2003) for more details).
phmm(f?k|e?a?k) isthe probability given by an HMM-based (intra-phrase) alignment model (see (Vogel et al, 1996)):phmm(f?
|e?)
= ??a|f?
|1|f?
|?j=1p(f?j |e?aj ) ?
p(aj |aj?1, |e?|)(10)The HMM-based alignment model probability isused here for smoothing purposes as describedin (Ortiz-Mart?
?nez et al, 2009).Analogously h4 is defined as:h4(e,a, f) = log(?Kk=1 p(e?a?k |f?k))?
target phrase-length model (h5)h5(e,a, f) = log(?Kk=1 p(|e?k|)), where p(|e?k|) =?(1?
?)|e?k|.
h5 implements a target phrase-lengthmodel by means of a geometric distribution withprobability of success on each trial ?.
The use of ageometric distribution penalizes the length of tar-get phrases.?
source phrase-length model (h6)h6(e,a, f) = log(?Kk=1 p(|f?k| | |e?a?k |)),where p(|f?k| | |e?a?k |) = ?(1?
?
)abs(|f?k|?|e?a?k |) andabs(?)
is the absolute value function.
A geometricdistribution is used to model this feature (it penal-izes the difference between the source and targetphrase lengths).?
distortion model (h7)h7(a) = log(?Kk=1 p(a?k|a?k?1)), wherep(a?k|a?k?1) = ?
(1 ?
?
)abs(ba?k?la?k?1 ), ba?kdenotes the beginning position of the sourcephrase covered by a?k and la?k?1 denotes the lastposition of the source phrase covered by a?k?1.A geometric distribution is used to model thisfeature (it penalizes the reorderings).The log-linear model, which includes the abovedescribed feature functions, is used to generate thesuffix es given the user-validated prefix ep.
Specif-ically, the IMT system generates a partial phrase-based alignment between the user prefix ep and aportion of the source sentence f , and returns the suf-fix es as the translation of the remaining portion off (see (Ortiz-Mart?
?nez et al, 2009)).4.2 Extending the IMT system from userfeedbackAfter translating a source sentence f , a new sen-tence pair (f , e) is available to feed the IMT system(see Figure 1).
In this section we describe how thelog-linear model described in section 4.1 is updatedgiven the new sentence pair.
To do this, a set of suf-ficient statistics that can be incrementally updated ismaintained for each feature function hi(?).
A suffi-cient statistic for a statistical model is a statistic thatcaptures all the information that is relevant to esti-mate this model.Regarding feature function h1 and according toequation (8), we need to maintain the following data:ck,1 and ck,2 given any order k, N1+(?
), and cX(?
)(see section 4.1 for the meaning of each symbol).Given a new sentence e, and for each k-gram eii?k+1of e where 1 ?
k ?
n and 1 ?
i ?
|e|+1, we mod-ify the set of sufficient statistics as it is shown in Al-gorithm 1.
The algorithm checks the changes in thecounts of the k-grams to update the set of sufficientstatistics.
Sufficient statistics forDk are updated fol-lowing the auxiliar procedure shown in Algorithm 2.Feature function h2 requires the incremental cal-culation of the mean ?|e| and the standard deviation?|e| of the normal distribution associated to a targetsentence length |e|.
For this purpose the proceduredescribed in (Knuth, 1981) can be used.
In this pro-cedure, two quantities are maintained for each nor-mal distribution: ?|e| and S|e|.
Given a new sentence549input : n (higher order), eii?k+1 (k-gram),S = {?j(cj,1, cj,2), N1+(?
), cX(?
)}(current set of sufficient statistics)output : S (updated set of sufficient statistics)beginif cT (eii?k+1) = 0 thenif k ?
1 ?
1 thenupdD(S,k-1,cM (ei?1i?k+2),cM (ei?1i?k+2)+1)if cM (ei?1i?k+2) = 0 thenN1+(ei?1i?k+2) = N1+(ei?1i?k+2) + 1cM (ei?1i?k+2) = cM (ei?1i?k+2) + 1cM (eii?k+2) = cM (eii?k+2) + 1if k = n thenN1+(ei?1i?k+1) = N1+(ei?1i?k+1) + 1if k = n thenupdD(S,k,cT (eii?k+1),cT (eii?k+1) + 1)cT (ei?1i?k+1)=cT (ei?1i?k+1) + 1cT (eii?k+1)=cT (eii?k+1) + 1endAlgorithm 1: Pseudocode for updating the suf-ficient statistics of a given k-graminput : S (current set of sufficient statistics),k(order), c (current count), c?
(new count)output : (ck,1, ck,2) (updated sufficient statistics)beginif c = 0 thenif c?
= 1 then ck,1 = ck,1 + 1if c?
= 2 then ck,2 = ck,2 + 1if c = 1 thenck,1 = ck,1 ?
1if c?
= 2 then ck,2 = ck,2 + 1if c = 2 then ck,2 = ck,2 ?
1endAlgorithm 2: Pseudocode for the updD proce-durepair (f , e), the two quantities are updated using a re-currence relation:?|e| = ?
?|e| + (|f | ?
?
?|e|)/c(|e|) (11)S|e| = S?|e| + (|f | ?
?
?|e|)(|f | ?
?|e|) (12)where c(|e|) is the count of the number of sentencesof length |e| that have been seen so far, and ?
?|e| andS?|e| are the quantities previously stored (?|e| is ini-tialized to the source sentence length of the first sam-ple and S|e| is initialized to zero).
Finally, the stan-dard deviation can be obtained from S as follows:?|e| =?S|e|/(c(|e|)?
1).Feature functions h3 and h4 implement inverseand direct smoothed phrase-based models respec-tively.
Since phrase-based models are symmetricmodels, only an inverse phrase-based model is main-tained (direct probabilities can be efficiently ob-tained using appropriate data structures, see (Ortiz-Mart?
?nez et al, 2008)).
The inverse phrase modelprobabilities are estimated from the phrase counts:p(f?
|e?)
= c(f?
, e?)?f?
?
c(f?
?, e?
)(13)According to Equation (13), the set of suffi-cient statistics to be stored for the inverse phrasemodel consists of a set of phrase counts (c(f?
, e?)
and?f?
?
c(f?
?, e?)
must be stored separately).
Given anew sentence pair (f , e), the standard phrase-basedmodel estimation method uses a word alignment ma-trix between f and e to extract the set of phrase pairsthat are consistent with the word alignment ma-trix (see (Koehn et al, 2003) for more details).
Oncethe consistent phrase pairs have been extracted, thephrase counts are updated.
The word alignment ma-trices required for the extraction of phrase pairs aregenerated by means of the HMM-based models usedin the feature functions h3 and h4.Inverse and direct HMM-based models are usedhere for two purposes: to smooth the phrase-basedmodels via linear interpolation and to generate wordalignment matrices.
The weights of the interpola-tion can be estimated from a development corpus.Equation (10) shows the expression of the probabil-ity given by an inverse HMM-based model.
Theprobability includes lexical probabilities p(fj |ei)and alignment probabilities p(aj |aj?1, l).
Since thealignment in the HMM-based model is determinedby a hidden variable, the EM algorithm is requiredto estimate the parameters of the model (see (Ochand Ney, 2003)).
However, the standard EM algo-rithm is not appropriate to incrementally extend ourHMM-based models because it is designed to workin batch training scenarios.
To solve this problem,we apply the incremental view of the EM algorithmdescribed in (Neal and Hinton, 1998).
Accordingto (Och and Ney, 2003), the lexical probability for a550pair of words is given by the expression:p(f |e) = c(f |e)?f ?
c(f ?|e)(14)where c(f |e) is the expected number of times thatthe word e is aligned to the word f .
The alignmentprobability is defined in a similar way:p(aj |aj?1, l) =c(aj |aj?1, l)?a?jc(a?j |aj?1, l)(15)where c(aj |aj?1, l) denotes the expected number oftimes that the alignment aj has been seen after theprevious alignment aj?1 given a source sentencecomposed of l words.Given the equations (14) and (15), the set of suf-ficient statistics for the inverse HMM-based modelconsists of a set of expected counts (numerator anddenominator values are stored separately).
Given anew sentence pair (f , e), we execute a new iterationof the incremental EM algorithm on the new sampleand collect the contributions to the expected counts.The parameters of the direct HMM-based modelare estimated analogously to those of the inverseHMM-based model.
Once the direct and the inverseHMM-based model parameters have been modifieddue to the presentation of a new sentence pair to theIMT system, both models are used to obtain wordalignments for the new sentence pair.
The resultingdirect and inverse word alignment matrices are com-bined by means of the symmetrization alignment op-eration (Och and Ney, 2003) before extracting theset of consistent phrase pairs.HMM-based alignment models are used herebecause, according to (Och and Ney, 2003)and (Toutanova et al, 2002), they outperform IBM 1to IBM 4 alignment models while still allowing theexact calculation of the likelihood for a given sen-tence pair.The ?
parameters of the geometric distributionsassociated to the feature functions h5, h6 and h7 areleft fixed.
Because of this, there are no sufficientstatistics to store for these feature functions.Finally, the weights of the log-linear combinationare not modified due to the presentation of a newsentence pair to the system.
These weights can beadjusted off-line by means of a development corpusand well-known optimization techniques.5 ExperimentsThis section describes the experiments that we car-ried out to test our online IMT system.5.1 Experimental setupThe experiments were performed using the XE-ROX XRCE corpus (SchlumbergerSema S.A.
etal., 2001), which consists of translations of Xe-rox printer manuals involving three different pairsof languages: French-English, Spanish-English, andGerman-English.
The main features of these cor-pora are shown in Table 1.
Partitions into training,development and test were performed.
This corpusis used here because it has been extensively used inthe literature on IMT to report results.IMT experiments were carried out from Englishto the other three languages.5.2 Assessment criteriaThe evaluation of the techniques presented in thispaper were carried out using the Key-stroke andmouse-action ratio (KSMR) measure (Barrachinaet al, 2009).
This is calculated as the number ofkeystrokes plus the number of mouse movementsplus one more count per sentence (aimed at simulat-ing the user action needed to accept the final transla-tion), the sum of which is divided by the total num-ber of reference characters.
In addition to this, wealso used the well-known BLEU score (Papineni etal., 2001) to measure the translation quality of thefirst translation hypothesis produced by the IMT sys-tem for each source sentence (which is automaticallygenerated without user intervention).5.3 Online IMT resultsTo test the techniques proposed in this work, wecarried out experiments in two different scenarios.In the first one, the first 10 000 sentences extractedfrom the training corpora were interactively trans-lated by means of an IMT system without any pre-existent model stored in memory.
Each time a newsentence pair was validated, it was used to incremen-tally train the system.
Figures 3a, 3b and 3c show theevolution of the KSMR with respect to the numberof sentence pairs processed by the IMT system; theresults correspond to the translation from English toSpanish, French and German, respectively.
In addi-551En Sp En Fr En GeTrainSent.
pairs 55761 52844 49376Running words 571960 657172 542762 573170 506877 440682Vocabulary 25627 29565 24958 27399 24899 37338Dev.Sent.
pairs 1012 994 964Running words 12111 13808 9480 9801 9162 8283Perplexity (3-grams) 46.2 34.0 96.2 74.1 68.4 124.3Sent.
pairs 1125 984 996TestRunning words 7634 9358 9572 9805 10792 9823Perplexity (3-grams) 107.0 59.6 192.6 135.4 92.8 169.2Table 1: XEROX corpus statistics for three different language pairs (from English (En) to Spanish (Sp),French (Fr) and German (Ge))tion, for each language pair we interactively trans-lated the original portion of the training corpus andthe same portion of the original corpus after beingrandomly shuffled.As these figures show, the results clearly demon-strate that the IMT system is able to learn fromscratch.
The results were similar for the three lan-guages.
It is also worthy of note that the obtainedresults were better in all cases for the original cor-pora than for the shuffled ones.
This is because,in the original corpora, similar sentences appearmore or less contiguosly (due to the organization ofthe contents of the printer manuals).
This circum-stance increases the accuracy of the online learning,since with the original corpora the number of lat-eral effects ocurred between the translation of sim-ilar sentences is decreased.
The online learning ofa new sentence pair produces a lateral effect whenthe changes in the probability given by the modelsnot only affect the newly trained sentence pair butalso other sentence pairs.
A lateral effect can causethat the system generates a wrong translation for agiven source sentence due to undesired changes inthe statistical models.The accuracy were worse for shuffled corpora,since shuffling increases the number of lateral ef-fects that may occur between the translation of sim-ilar sentences (because they no longer appear con-tiguously).
A good way to compare the quality ofdifferent online IMT systems is to determine theirrobustness in relation to sentence ordering.
How-ever, it can generally be expected that the sentencesto be translated in an interactive translation sessionwill be in a non-random order.Alternatively, we carried out experiments in a dif-ferent learning scenario.
Specifically, the XEROX304050607080901000  1000  2000  3000  4000  5000  6000  7000  8000  9000 10000KSMR#Sentencesoriginalshuffled(a) English-Spanish4050607080901000  1000  2000  3000  4000  5000  6000  7000  8000  9000 10000KSMR#Sentencesoriginalshuffled(b) English-French4050607080901000  1000  2000  3000  4000  5000  6000  7000  8000  9000 10000KSMR#Sentencesoriginalshuffled(c) English-GermanFigure 3: KSMR evolution translating a portion ofthe training corporatest corpora were interactively translated from theEnglish language to the other three languages, com-paring the performance of a batch IMT system with552that of an online IMT system.
The batch IMT sys-tem is a conventional IMT system which is not ableto take advantage of user feedback after each transla-tion while the online IMT system uses the new sen-tence pairs provided by the user to revise the sta-tistical models.
Both systems were initialized witha log-linear model trained in batch mode by meansof the XEROX training corpora.
The weights of thelog-linear combination were adjusted for the devel-opment corpora by means of the downhill-simplexalgorithm.
Table 2 shows the obtained results.
Thetable shows the BLEU score and the KSMR for thebatch and the online IMT systems (95% confidenceintervals are shown).
The BLEU score was calcu-lated from the first translation hypothesis producedby the IMT system for each source sentence.
The ta-ble also shows the average online learning time (LT)for each new sample presented to the system2.
Allthe improvements obtained with the online IMT sys-tem were statistically significant.
Also, the averagelearning times clearly allow the system to be used ina real-time scenario.IMT system BLEU KSMR LT (s)En-Spbatch 55.1?
2.3 18.2?
1.1 -online 60.6?
2.3 15.8?
1.0 0.04En-Frbatch 33.7?
2.0 33.9?
1.3 -online 42.2?
2.2 27.9?
1.3 0.09En-Gebatch 20.4?
1.8 40.3?
1.2 -online 28.0?
2.0 35.0?
1.3 0.07Table 2: BLEU and KSMR results for the XEROXtest corpora using the batch and the online IMT sys-tems.
The average online learning time (LT) in sec-onds is shown for the online systemFinally, in Table 3 a comparison of the KSMR re-sults obtained by the online IMT system with state-of-the-art IMT systems is reported (95% confidenceintervals are shown).
We compared our system withthose presented in (Barrachina et al, 2009): thealignment templates (AT), the stochastic finite-statetransducer (SFST), and the phrase-based (PB) ap-proaches to IMT.
The results were obtained usingthe same Xerox training and test sets (see Table 1)for the four different IMT systems.
Our system out-performed the results obtained by these systems.2All the experiments were executed on a PC with a 2.40 GhzIntel Xeon processor with 1GB of memory.AT PB SFST OnlineEn-Sp 23.2?1.3 16.7?1.2 21.8?1.4 15.8?
1.0En-Fr 40.4?1.4 35.8?1.3 43.8?1.6 27.9?
1.3En-Ge 44.7?1.2 40.1?1.2 45.7?1.4 35.0?
1.3Table 3: KSMR results comparison of our systemand three different state-of-the-art batch systems6 ConclusionsWe have presented an online IMT system.
The pro-posed system is able to incrementally extend the sta-tistical models involved in the translation process,breaking technical limitations encountered in otherworks.
Empirical results show that our techniquesallow the IMT system to learn from scratch or frompreviously estimated models.One key aspect of the proposed system is the useof HMM-based alignment models trained by meansof the incremental EM algorithm.The incremental adjustment of the weights of thelog-linear models and other parameters have notbeen tackled here.
For the future we plan to incor-porate this functionality into our IMT system.The incremental techniques proposed here canalso be exploited to extend SMT systems (in fact,our proposed IMT system is based on an incremen-tally updateable SMT system).
For the near futurewe plan to study possible aplications of our tech-niques in a fully automatic translation scenario.Finally, it is worthy of note that the main ideaspresented here can be used in other interactive ap-plications such as Computer Assisted Speech Tran-scription, Interactive Image Retrieval, etc (see (Vi-dal et al, 2007) for more information).
In conclu-sion, we think that the online learning techniquesproposed here can be the starting point for a newgeneration of interactive pattern recognition systemsthat are able to take advantage of user feedback.AcknowledgmentsWork supported by the EC (FEDER/FSE), theSpanish Government (MEC, MICINN, MITyC,MAEC, ?Plan E?, under grants MIPRCV ?Con-solider Ingenio 2010?
CSD2007-00018, iTrans2TIN2009-14511, erudito.com TSI-020110-2009-439), the Generalitat Valenciana (grant Prome-teo/2009/014), the Univ.
Polite?cnica de Valencia(grant 20091027) and the Spanish JCCM (grantPBI08-0210-7127).553ReferencesA.
Arun and P. Koehn.
2007.
Online learning methodsfor discriminative training of phrase based statisticalmachine translation.
In Proc.
of the MT Summit XI,pages 15?20, Copenhagen, Denmark, September.S.
Barrachina, O. Bender, F. Casacuberta, J. Civera,E.
Cubel, S. Khadivi, A. Lagarda, H. Ney, J. Toma?s,and E. Vidal.
2009.
Statistical approaches tocomputer-assisted translation.
Computational Lin-guistics, 35(1):3?28.Peter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and R. L. Mercer.
1993.
The mathematics ofstatistical machine translation: Parameter estimation.Computational Linguistics, 19(2):263?311.N.
Cesa-Bianchi, G. Reverberi, and S. Szedmak.
2008.Online learning algorithms for computer-assistedtranslation.
Deliverable D4.2, SMART: Stat.
Multi-lingual Analysis for Retrieval and Translation, Mar.S.F.
Chen and J. Goodman.
1996.
An empirical study ofsmoothing techniques for language modeling.
In Proc.of the ACL, pages 310?318, San Francisco.D.
Chiang, Y. Marton, and P. Resnik.
2008.
Online large-margin training of syntactic and structural translationfeatures.
In Proc.
of EMNLP.George Foster, Pierre Isabelle, and Pierre Plamondon.1997.
Target-text mediated interactive machine trans-lation.
Machine Translation, 12(1):175?194.P.
Isabelle and K. Church.
1997.
Special issue onnew tools for human translators.
Machine Translation,12(1?2).D.E.
Knuth.
1981.
Seminumerical Algorithms, volume 2of The Art of Computer Programming.
Addison-Wesley, Massachusetts, 2nd edition.P.
Koehn, F. J. Och, and D. Marcu.
2003.
Statisticalphrase-based translation.
In Proc.
of the HLT/NAACL,pages 48?54, Edmonton, Canada, May.P.
Langlais, G. Lapalme, and M. Loranger.
2002.Transtype: Development-evaluation cycles to boosttranslator?s productivity.
Machine Translation,15(4):77?98.P.
Liang, A.
Bouchard-Co?te?, D. Klein, and B. Taskar.2006.
An end-to-end discriminative approach to ma-chine translation.
In Proc.
of the 44th ACL, pages 761?768, Morristown, NJ, USA.R.M.
Neal and G.E.
Hinton.
1998.
A view of the EMalgorithm that justifies incremental, sparse, and othervariants.
In Proceedings of the NATO-ASI on Learningin graphical models, pages 355?368, Norwell, MA,USA.L.
Nepveu, G. Lapalme, P. Langlais, and G. Foster.
2004.Adaptive language and translation models for interac-tive machine translation.
In Proc.
of EMNLP, pages190?197, Barcelona, Spain, July.Franz Josef Och and Hermann Ney.
2002.
Discrimina-tive Training and Maximum Entropy Models for Sta-tistical Machine Translation.
In Proc.
of the 40th ACL,pages 295?302, Philadelphia, PA, July.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51, March.D.
Ortiz-Mart?
?nez, I.
Garc?
?a-Varea, and Casacuberta F.2008.
The scaling problem in the pattern recognitionapproach to machine translation.
Pattern RecognitionLetters, 29:1145?1153.Daniel Ortiz-Mart?
?nez, Ismael Garc?
?a-Varea, and Fran-cisco Casacuberta.
2009.
Interactive machine trans-lation based on partial statistical phrase-based align-ments.
In Proc.
of RANLP, Borovets, Bulgaria, sep.Kishore A. Papineni, Salim Roukos, Todd Ward, andWei-Jing Zhu.
2001.
Bleu: a method for auto-matic evaluation of machine translation.
TechnicalReport RC22176 (W0109-022), IBM Research Divi-sion, Thomas J. Watson Research Center, YorktownHeights, NY, September.SchlumbergerSema S.A., ITI Valencia, RWTH Aachen,RALI Montreal, Celer Soluciones, Socie?te?
Gamma,and XRCE.
2001.
TT2.
TransType2 - computer as-sisted translation.
Project Tech.
Rep.Kristina Toutanova, H. Tolga Ilhan, and ChristopherManning.
2002.
Extensions to hmm-based statisticalword alignment models.
In Proc.
of EMNLP.E.
Vidal, L.
Rodr?
?guez, F. Casacuberta, and I.
Garc??a-Varea.
2007.
Interactive pattern recognition.
In Proc.of the 4th MLMI, pages 60?71.
Brno, Czech Republic,28-30 June.Stephan Vogel, Hermann Ney, and Christoph Tillmann.1996.
HMM-based word alignment in statistical trans-lation.
In Proc.
of COLING, pages 836?841, Copen-hagen, Denmark, August.T.
Watanabe, J. Suzuki, H. Tsukada, and H. Isozaki.2007.
Online large-margin training for statistical ma-chine translation.
In Proc.
of EMNLP and CoNLL,pages 764?733, Prage, Czeck Republic.554
