Similarity-Based Methods For Word Sense DisambiguationI do  DaganDept.
of Mathematics andComputer  ScienceBar Ilan UniversityRamat  Gan 52900, Israeldagan?macs, biu.
ac.
ilLi l l ian Lee  Fernando Pere i raDiv.
of Engineering and AT&T Labs - ResearchApplied Sciences 600 Mountain Ave.Harvard University Murray Hill, NJ 07974, USACambridge, MA 01238, USA pere i ra?research ,  a t t .
cornllee?eecs, harvard, eduAbst ractWe compare four similarity-based sti-mation methods against back-off andmaximum-likelihood estimation meth-ods on a pseudo-word sense disam-biguation task in which we controlledfor both unigram and bigram fre-quency.
The similarity-based meth-ods perform up to 40% better on thisparticular task.
We also concludethat events that occur only once inthe training set have major impact onsimilarity-based estimates.1 In t roduct ionThe problem of data sparseness affects all sta-tistical methods for natural language process-ing.
Even large training sets tend to misrep-resent low-probability events, since rare eventsmay not appear in the training corpus at all.We concentrate here on the problem of es-timating the probability of unseen word pairs,that is, pairs that do not occur in the train-ing set.
Katz's back-off scheme (Katz, 1987),widely used in bigram language modeling, esti-mates the probability of an unseen bigram byutilizing unigram estimates.
This has the un-desirable result of assigning unseen bigrams thesame probability if they are made up of uni-grams of the same frequency.Class-based methods (Brown et al, 1992;Pereira, Tishby, and Lee, 1993; Resnik, 1992)cluster words into classes of similar words, sothat one can base the estimate of a word pair'sprobability on the averaged cooccurrence prob-ability of the classes to which the two words be-long.
However, a word is therefore modeled bythe average behavior of many words, which maycause the given word's idiosyncrasies to be ig-nored.
For instance, the word "red" might wellact like a generic olor word in most cases, butit has distinctive cooccurrence patterns with re-spect to words like "apple," "banana," and soon.We therefore consider similarity-based esti-mation schemes that do not require buildinggeneral word classes.
Instead, estimates forthe most similar words to a word w are com-bined; the evidence provided by word w' isweighted by a function of its similarity to w.Dagan, Markus, and Markovitch (1993) pro-pose such a scheme for predicting which un-seen cooccurrences are more likely than others.However, their scheme does not assign probabil-ities.
In what follows, we focus on probabilisticsimilarity-based estimation methods.We compared several such methods, in-cluding that of Dagan, Pereira, and Lee (1994)and the cooccurrence smoothing method ofEssen and Steinbiss (1992), against classical es-timation methods, including that of Katz, in adecision task involving unseen pairs of direct ob-jects and verbs, where unigram frequency waseliminated from being a factor.
We found thatall the similarity-based schemes performed al-most 40% better than back-off, which is ex-pected to yield about 50% accuracy in ourexperimental setting.
Furthermore, a schemebased on the total divergence of empirical dis-56tributions to their average 1 yielded statisticallysignificant improvement in error rate over cooc-currence smoothing.We also investigated the effect of removingextremely low-frequency events from the train-ing set.
We found that, in contrast o back-off smoothing, where such events are often dis-carded from training with little discernible f-fect, similarity-based smoothing methods suf-fer noticeable performance degradation whensingletons (events that occur exactly once) areomitted.2 D is t r ibut iona l  S imi la r i ty  Mode lsWe wish to model conditional probability distri-butions arising from the coocurrence of linguis-tic objects, typically words, in certain configura-tions.
We thus consider pairs (wl, w2) E Vi ?
V2for appropriate sets 1/1 and V2, not necessar-ily disjoint.
In what follows, we use subscripti for the i th element of a pair; thus P(w21wi)is the conditional probability (or rather, someempirical estimate, the true probability beingunknown) that a pair has second element w2given that its first element is wl; and P(wllw2)denotes the probability estimate, according tothe base language model, that wl is the firstword of a pair given that the second word is w2.P(w) denotes the base estimate for the unigramprobability of word w.A similarity-based language model consistsof three parts: a scheme for deciding whichword pairs require a similarity-based estimate,a method for combining information from simi-lar words, and, of course, a function measuringthe similarity between words.
We give the de-tails of each of these three parts in the followingthree sections.
We will only be concerned withsimilarity between words in V1.1To the best of our "knowledge, this is the first useof this particular distribution dissimilarity function instatistical language processing.
The function itself is im-plicit in earlier work on distributional c ustering (Pereira,Tishby, and Lee, 1993}, has been used by Tishby (p.e.
)in other distributional similarity work, and, as sug-gested by Yoav Freund (p.c.
), it is related to results ofHoeffding (1965) on the probability that a given samplewas drawn from a given joint distribution.2.1 Discounting and RedistributionData sparseness makes the maximum likelihoodestimate (MLE) for word pair probabilities un-reliable.
The MLE for the probability of a wordpair (Wl, w2), conditional on the appearance ofword wl, is simplyPML(W2\ [w l )  --  c(wl, w2) (1)c( i)where c(wl, w2) is the frequency of (wl, w2) inthe training corpus and c(wl) is the frequencyof wt.
However, PML is zero for any unseenword pair, which leads to extremely inaccurateestimates for word pair probabilities.Previous proposals for remedying the aboveproblem (Good, 1953; Jelinek, Mercer, andRoukos, 1992; Katz, 1987; Church and Gale,1991) adjust the MLE in so that the total prob-ability of seen word pairs is less than one, leav-ing some probability mass to be redistributedamong the unseen pairs.
In general, the ad-justment involves either interpolation, in whichthe MLE is used in linear combination with anestimator guaranteed to be nonzero for unseenword pairs, or discounting, in which a reducedMLE is used for seen word pairs, with the prob-ability mass left over from this reduction usedto model unseen pairs.The discounting approach is the one adoptedby Katz (1987):/Pd(w2\]wx) C(Wl, w2) > 0/5(w2lwl) = \[o~(wl)Pr(w2\[wl) o.w.
(2)where Pd represents the Good-Turing dis-counted estimate (Katz, 1987) for seen wordpairs, and Pr denotes the model for probabil-ity redistribution among the unseen word pairs.c~(wl) is a normalization factor.Following Dagan, Pereira, and Lee (1994),we modify Katz's formulation by writingPr(w2\]wl) instead P(w2), enabling us to usesimilarity-based stimates for unseen word pairsinstead of basing the estimate for the pair on un-igram frequency P(w2).
Observe that similarityestimates are used for unseen word pairs only.We next investigate stimates for Pr(w21wl)57derived by averaging information from wordsthat are distributionally similar to Wl.2.2 Combin ing  Ev idenceSimilarity-based models assume that if word w~is "similar" to word wl, then w~ can yield in-formation about the probability of unseen wordpairs involving wl.
We use a weighted aver-age of the evidence provided by similar words,where the weight given to a particular word w~depends on its similarity to wl.More precisely, let W(wl, W~l) denote an in-creasing function of the similarity between wland w\[, and let $(Wl) denote the set of wordsmost similar to Wl.
Then the general form ofsimilarity model we consider is a W-weightedlinear combination of predictions of similarwords:PSIM('W2IWl) = ~V(Wl, W~) E~ ~s(~1 )(3)where = is a nor-malization factor.
According to this formula,w2 is more likely to occur with wl if it tends tooccur with the words that are most similar toWI.Considerable latitude is allowed in definingthe set $(Wx), as is evidenced by previous workthat can be put in the above form.
Essenand Steinbiss (1992) and Karov and Edelman(1996) (implicitly) set 8(wl) = V1.
However,it may be desirable to restrict ,5(wl) in somefashion, especially if 1/1 is large.
For instance,Dagan.
Pereira, and Lee (1994) use the closestk or fewer words w~ such that the dissimilaritybetween wl and w~ is less than a threshold valuet; k and t are tuned experimentally.Now, we could directly replace P,.
(w2\[wl)in the back-off equation (2) with PSIM(W21Wl).However, other variations a re  possible, suchas interpolating with the unigram probabilityP(w2):P,.
(w2lwl) = 7P(w2) + (1 - 7)PsiM(W2lWl),where 7 is determined experimentally (Dagan,Pereira, and Lee, 1994).
This represents, in ef-fect, a linear combination of the similarity es-timate and the back-off estimate: if 7 -- 1,then we have exactly Katz's back-off scheme.As we focus in this paper on alternatives forPSlM, we will not consider this approach here;that is, for the rest of this paper, Pr(w2\]wl) =PslM(W21wl).2.3 Measures of  Simi lar i tyWe now consider several word similarity func-tions that can be derived automatically fromthe statistics of a training corpus, as opposedto functions derived from manually-constructedword classes (Resnik, 1992).
All the similarityfunctions we describe below depend just on thebase language model P('I'), not the discountedmodel /5(.\[.)
from Section 2.1 above.2.3.1 KL  d ivergenceKullback-Leibler (KL) divergence is a stan-dard information-theoretic measure of the dis-similarity between two probability mass func-tions (Cover and Thomas, 1991).
We can ap-ply it to the conditional distribution P(.\[wl) in-duced by Wl on words in V2:D(wx\[lW ) = P(w2lwl) log P(wu\[wx) P(w21wl)" (4)For D(wxHw~l) to be defined it must be thecase that P(w2\]w~l) > 0 whenever P(w21wl) >0.
Unfortunately, this will not in general bethe case for MLEs based on samples, so wewould need smoothed estimates of P(w2\]w~)that redistribute some probability mass to zero-frequency events.
However, using smoothed es-timates for P(w2\[wl) as well requires a sumover all w2 6 172, which is expensive \['or thelarge vocabularies under consideration.
Giventhe smoothed enominator distribution, we setl/V(wl, w~) = lO -~D(wlllw'l) ,where/3 is a free parameter.2.3.2 Total  d ivergence to the  averageA related measure is based on the total KLdivergence to the average of the two distribu-tions:+ wl A(wx, W11) = D (w, wl )+D (w~\[ + w~)258where (Wl ?
w~)/2 shorthand for the distribu-tion ?
(P(.IwJ + P(.Iw~))Since D('II-) > O, A(Wl,W~) >_ O. Furthermore,letting p(w2) = P(w2\[wJ, p'(w2) = P(w2lw~)and C : {w2 : p(w2) > O,p'(w2) > O}, it isstraightforward to show by grouping terms ap-propriately thatA(wi,wb=-H(p(w2)) - H(p'(w2)) }+ 2 log 2,where H(x) = -x logx.
Therefore, d(wl, w~)is bounded, ranging between 0 and 2log2, andsmoothed estimates are not required becauseprobability ratios are not involved.
In addi-tion, the calculation of A(wl, w~) requires um-ming only over those w2 for which P(w2iwJ andP(w2\]w~) are both non-zero, which, for sparsedata, makes the computation quite fast.As in the KL divergence case, we setW(Wl, W~l) to be 10 -~A(~'wl).2.3.3 LI normThe L1 norm is defined asn(wi, wl) : ~ IP(w2lwj - P(w21w'Jl .
(6)W2By grouping terms as before, we can expressL(wI, w~) in a form depending only on the"common" w2:n(wl, w~) = 2-  E p (w2) -  E p'(w2)w26C w2EC?
Ip(w2)-p'(w2)t.w2ECThis last form makes it clear that 0 <L(Wl, w\[) _< 2, with equality if and only if thereare no words w2 such that both P(w2lwJ andP(w2lw\[) are strictly positive.Since we require a weighting scheme that isdecreasing in L, we setW(wl, w~) = (2 - n(wl, W/l)) flwith fl again free.2.3.4 Confusion probabi l i tyEssen and Steinbiss (1992) introduced confu-sion probability 2,which estimates the probabil-ity that word w~ can be substituted for wordWl:Pc(w lWl) = w(wl,= ~,  P(wllw2)P(w~\[w2)P(w2)w2 P(Wl)Unlike the measures described above, wl maynot necessarily be the "closest" word to itself,that is, there may exist a word w~ such thatPc(W'l\[Wl ) > Pc(w,\[wl) .The confusion probability can be computedfrom empirical estimates provided all unigramestimates are nonzero (as we assume through-out).
In fact, the use of smoothed estimateslike those of Katz's back-off scheme is problem-atic, because those estimates typically do notpreserve consistency with respect to marginalestimates and Bayes's rule.
However, using con-sistent estimates (such as the MLE), we canrewrite Pc as follows:' w P(w2lwl) .
P(w21w'JP(w'J.
Pc(W1\[ 1)= ~ P(w2)W2This form reveals another important differencebetween the confusion probability and the func-tions D, A, and L described in the previous ec-tions.
Those functions rate w~ as similar to wlif, roughly, P(w21w~) is high when P(w21'wj is.Pc(w~\[wl), however, is greater for those w~ forwhich P(w~, wJ is large when P(w21wJ/P(w2)is.
When the ratio P(w21wl)/P(w2) is large, wemay think of w2 as being exceptional, since if w2is infrequent, we do not expect P(w21wJ to belarge.2.3.5 SummarySeveral features of the measures of similaritylisted above are summarized in table 1.
"BaseLM constraints" are conditions that must besatisfied by the probability estimates of the base2Actually, they present wo alternative definitions.We use their model 2-B, which they found yielded thebest experimental results.59language model.
The last column indicateswhether the weight W(wl, w~) associated witheach similarity function depends on a parameterthat needs to be tuned experimentally.3 Exper imenta l  Resu l tsWe evaluated the similarity measures listedabove on a word sense disambiguation task, inwhich each method is presented with a noun andtwo verbs, and decides which verb is more likelyto have the noun as a direct object.
Thus, we donot measure the absolute quality of the assign-ment of probabilities, as would be the case ina perplexity evaluation, but rather the relativequality.
We are therefore able to ignore constantfactors, and so we neither normalize the similar-ity measures nor calculate the denominator inequation (3).3.1 Task: Pseudo-word  SenseDisambiguationIn the usual word sense disambiguation prob-lem, the method to be tested is presented withan ambiguous word in some context, and isasked to identify the correct sense of the wordfrom the context.
For example, a test instancemight be the sentence fragment "robbed thebank"; the disambiguation method must decidewhether "bank" refers to a river bank, a savingsbank, or perhaps ome other alternative.While sense disambiguation is clearly an im-portant task, it presents numerous experimen-tal difficulties.
First, the very notion of "sense"is not clearly defined; for instance, dictionariesmay provide sense distinctions that are too fineor too coarse for the data at hand.
Also, oneneeds to have training data for which the cor-rect senses have been assigned, which can re-quire considerable human effort.To circumvent hese and other difficulties,we set up a pseudo-word isambiguation ex-periment (Schiitze, 1992; Gale, Church, andYarowsky, 1992) the general format of which isas follows.
We first construct a list of pseudo-words, each of which is the combination of twodifferent words in V2.
Each word in V2 con-tributes to exactly one pseudo-word.
Then, wereplace each w2 in the test set with its cor-responding pseudo-word.
For example, if wechoose to create a pseudo-word out of the words"make" and "take", we would change the testdata like this:make plans =~ {make, take} planstake action =~ {make, take} actionThe method being tested must choose betweenthe two words that make up the pseudo-word.3.2 DataWe used a statistical part-of-speech tagger(Church, 1988) and pattern matching and con-cordancing tools (due to David Yarowsky) toidentify transitive main verbs and head nounsof the corresponding direct objects in 44 mil-lion words of 1988 Associated Press newswire.We selected the noun-verb pairs for the 1000most frequent nouns in the corpus.
These pairsare undoubtedly somewhat noisy given the er-rors inherent in the part-of-speech tagging andpattern matching.We used 80%, or 587833, of the pairs so de-rived, for building base bigram language mod-els, reserving 20.o/0 for testing purposes.
Assome, but not all, of the similarity measures re-quire smoothed language models, we calculatedboth a Katz back-off language model (P = 15(equation (2)), with Pr(w2\[wl) = P(w2)), anda maximum-likelihood model (P = PML)- Fur-thermore, we wished to investigate Katz's claimthat one can delete singletons, word pairs thatoccur only once, from the training set with-out affecting model performance (Katz, 1987);our training set contained 82407 singletons.
Wetherefore built four base language models, sum-marized in Table 2.MLEKatzwith singletons no singletons(587833 pairs) (505426 pairs)MLE-1 MLE-olBO-1 BO-olTable 2: Base Language ModelsSince we wished to test the effectiveness of us-ing similarity for unseen word cooccurrences, weremoved from the test set any verb-object pairs60nameDALPcrange\[0, co\]\[0, 2 log 2\]\[0, 2\]\[0, ?
maxw, P(w2)\]base LM constraintsP(w21w~l) ?
0 if P(w2\[wx) ~: 0nonenoneBayes consistencyTable 1: Summary of similarity function propertiestune?yesyesyesnothat occurred in the training set; this resultedin 17152 unseen pairs (some occurred multipletimes).
The unseen pairs were further dividedinto five equal-sized parts, T1 through :/'5, whichformed the basis for fivefold cross-validation: ineach of five runs, one of the Ti was used as aperformance test set, with the other 4 sets com-bined into one set used for tuning parameters(if necessary) via a simple grid search.
Finally,test pseudo-words were created from pairs ofverbs with similar frequencies, so as to controlfor word frequency in the decision task.
We useerror rate as our performance metric, defined as(# incorrect choices + (# of ties)/2) ofwhere N was the size of the test corpus.
A tieoccurs when the two words making up a pseudo-word are deemed equally likely.3.3 Basel ine Exper imentsThe performances of the four base languagemodels are shown in table 3.
MLE-1 andMLE-ol both have error rates of exactly .5 be-cause the test sets consist of unseen bigrams,which are all assigned a probability of 0 bymaximum-likelihood estimates, and thus are allties for this method.
The back-off models BO-1and BO-ol also perform similarly.MLE-1MLE-olBO-1BO-ol7'1 T~ % T4 %.5 .5 .5 .5 .5i r0.517 0.520 0.512 0.513 0.5160.517 0.520 0.512 0.513 0.516Table 3: Base Language Model Error RatesSince the back-off models consistently per-formed worse than the MLE models, we choseto use only the MLE models in our subse-quent experiments.
Therefore, we only ran com-parisons between the measures that could uti-lize unsmoothed ata, namely, the Lt norm,L(wx, w~); the total divergence to the aver-age, A(wx, w~); and the confusion probability,Pc(w~lwx).
3 In the full paper, we give de-tailed examples howing the different neighbor-hoods induced by the different measures, whichwe omit here for reasons of space.3.4 Per formance of  S imi lar i ty-BasedMethodsFigure 1 shows the results on the five test sets,using MLE-1 as the base language model.
Theparameter/3 was always set to the optimal valuefor the corresponding training set.
RAND,which is shown for comparison purposes, sim-ply chooses the weights W(wl,w~) randomly.S(wl) was set equal to Vt in all cases.The similarity-based methods consistentlyoutperform the MLE method (which, recall, al-ways has an error rate of .5) and Katz's back-off method (which always had an error rate ofabout .51) by a huge margin; therefore, we con-clude that information from other word pairs isvery useful for unseen pairs where unigram fre-quency is not informative.
The similarity-basedmethods also do much better than RAND,which indicates that it is not enough to simplycombine information from other words arbitrar-ily: it is quite important o take word similarityinto account.
In all cases, A edged out the othermethods.
The average improvement in using Ainstead of Pc is .0082; this difference is signifi-cant to the .1 level (p < .085), according to thepaired t-test.3It should be noted, however, that on BO-1 data, KL-divergence performed slightly better than the L1 norm.61T1 T2Er r~ Rates on T~t  Sets, 8aN Language Moci J  MLE I"RANOMLEI "  - -"CONFMU~ I "  - .
.
.
."
I .MLE I "  ?
.
.
.
.?
AMLE I  ?
- -iiT3 T4  T5Figure 1: Error rates for each test set, where thebase language model was MLE-1.
The methods,going from left to right, are RAND, Pc, L, andA.
The performances shown are for settings offlthat were optimal for the corresponding trainingset.
I3 ranged from 4.0 to 4.5 for L and from 10to 13 for A.The results for the MLE-ol case are depictedin figure 2.
Again, we see the similarity-basedmethods achieving far lower error rates than theMLE, back-off, and RAND methods, and again,A always performed the best.
However, withsingletons omitted the difference between A andPc is even greater, the average difference being.024, which is significant o the .01 level (pairedt-test).An important observation is that all meth-ods, including RAND, were much more effectiveif singletons were included in the base languagemodel; thus, in the case of unseen word pairs,Katz's claim that singletons can be safely ig-nored in the back-off model does not hold forsimilarity-based models.4 Conc lus ionsSimilarity-based language models provide anappealing approach for dealing with datasparseness.
We have described and comparedthe performance offour such models against woclassical estimation methods, the MLE methodand Katz's back-off scheme, on a pseudo-worddisambiguation task.
We observed that thesimilarity-based methods perform much betteron unseen word pairs, with the measure basedE~or  ~tes  on  TeSt ~.
~ Umgua91 Model MLE.ot.... F - \ ]Tt;)-IT2 1"3 T4"RANDMLEo l* - -"CONFMLE01" -  .
.
.
.
"LMLEo l " -  .
.
.
.
"7"AMLEo l  .
.
.
.
.
."?
'!-...i i !
: ' FT5Figure 2: Error rates for each test set, wherethe base language model was MLE-ol.
/~ rangedfrom 6 to 11 for L and from 21 to 22 for A.on the KL divergence to the average, being thebest overall.We also investigated Katz's claim that onecan discard singletons in the training data, re-sulting in a more compact language model,without significant loss of performance.
Our re-sults indicate that for similarity-based languagemodeling, singletons are quite important; theiromission leads to significant degradation of per-formance.AcknowledgmentsWe thank Hiyan Alshawi, Joshua Goodman,Rebecca Hwa, Stuart Shieber, and YoramSinger for many helpful comments and discus-sions.
Part of this work was done while the firstand second authors were visiting AT&:T Labs.This material is based upon work supported inpart by the National Science Foundation underGrant No.
IRI-9350192.
The second authoralso gratefully acknowledges support from a Na-tional Science Foundation Graduate Fellowshipand an AT&T GRPW/ALFP grant.Re ferencesBrown, Peter F., Vincent J. DellaPietra, Peter V.deSouza, Jennifer C. Lai, and Robert L. Mercer.1992.
Class-based n-gram models of natural an-guage.
Computational Linguistics, 18(4):467-479,December.62Church, Kenneth.
1988.
A stochastic parts programand noun phrase parser for unrestricted text.
InProceedings of the Second Conference on AppliedNatural Language Processing, pages 136-143.Church, Kenneth W. and William A. Gale.
1991.A comparison of the enhanced Good-Turing anddeleted estimation methods for estimating proba-bilites of english bigrams.
Computer Speech andLanguage, 5:19-54.Cover, Thomas M. and Joy A. Thomas.
1991.
Ele-ments of Information Theory.
John Wiley.Dagan, Ido, Fernando Pereira, and Lillian Lee.
1994.Similarity-based stimation of word cooccurrenceprobabilities.
In Proceedings of the 32nd AnnualMeeting of the ACL, pages 272-278, Las Cruces,NM.Essen, Ute and Volker Steinbiss.
1992.
Co-occurrence smoothing for stochastic languagemodeling.
In Proceedings of ICASSP, volume 1,pages 161-164.Gale, William, Kenneth Church, and DavidYarowsky.
1992.
Work on statistcal methods forword sense disambiguation.
In Working Notes,AAAI Fall Symposium Series, Probabilistic Ap-proaches to Natural Language, pages 54-60.Good, I.J.
1953.
The population frequencies ofspecies and the estimation of population parame-ters.
Biometrika, 40(3 and 4):237-264.Hoeffding, Wassily.
1965.
Asymptotically optimaltests for nmttinomial distributions.
Annals ofMathematical Statistics, pages 369-401.Jelinek, Frederick, Robert L. Mercer, and SalimRoukos.
1992.
Principles of lexical languagemodeling for speech recognition.
In In SadaokiFurui and M. Mohan Sondhi, editors, Advancesin Speech Signal Processing.
Mercer Dekker, Inc.,pages 651-699.Karov, Yael and Shimon Edelman.
1996.
Learningsimilarity-based word sense disambiguation fromsparse data.
In 4rth Workshop on Very LargeCorpora.Katz, Slava M. 1987.
Estimation of probabilitiesfrom sparse data for the language model com-ponent of a speech recognizer.
IEEE Transac-tions on Acoustics, Speech and Signal Processing,ASSP-35(3) :400-401, March.Pereira, Fernando, Naftali Tishby, and Lillian Lee.1993.
Distributional c ustering of English words.In Proceedings of the 31st Annual Meeting of theACL, pages 183-190, Columbus, OH.Resnik, Philip.
1992.
Wordnet and distributionalanalysis: A class-based approach to lexical discov-ery.
AAAI Workshop on Statistically-based Natu-ral Language Processing Techniques, pages 56-64,July.Schiitze, Hinrich.
1992.
Context space.
In Work-ing Notes, AAAI Fall Symposium on ProbabilisticApproaches to Natural Language.63
