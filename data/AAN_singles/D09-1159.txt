Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1533?1541,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPPhrase Dependency Parsing for Opinion MiningYuanbin Wu, Qi Zhang, Xuanjing Huang, Lide WuFudan UniversitySchool of Computer Science{ybwu,qi zhang,xjhuang,ldwu}@fudan.edu.cnAbstractIn this paper, we present a novel approachfor mining opinions from product reviews,where it converts opinion mining task toidentify product features, expressions ofopinions and relations between them.
Bytaking advantage of the observation that alot of product features are phrases, a con-cept of phrase dependency parsing is in-troduced, which extends traditional depen-dency parsing to phrase level.
This con-cept is then implemented for extracting re-lations between product features and ex-pressions of opinions.
Experimental eval-uations show that the mining task can ben-efit from phrase dependency parsing.1 IntroductionAs millions of users contribute rich informationto the Internet everyday, an enormous number ofproduct reviews are freely written in blog pages,Web forums and other consumer-generated medi-ums (CGMs).
This vast richness of content be-comes increasingly important information sourcefor collecting and tracking customer opinions.
Re-trieving this information and analyzing this con-tent are impossible tasks if they were to be manu-ally done.
However, advances in machine learningand natural language processing present us witha unique opportunity to automate the decoding ofconsumers?
opinions from online reviews.Previous works on mining opinions can be di-vided into two directions: sentiment classificationand sentiment related information extraction.
Theformer is a task of identifying positive and neg-ative sentiments from a text which can be a pas-sage, a sentence, a phrase and even a word (So-masundaran et al, 2008; Pang et al, 2002; Daveet al, 2003; Kim and Hovy, 2004; Takamura etal., 2005).
The latter focuses on extracting the el-ements composing a sentiment text.
The elementsinclude source of opinions who expresses an opin-ion (Choi et al, 2005); target of opinions whichis a receptor of an opinion (Popescu and Etzioni,2005); opinion expression which delivers an opin-ion (Wilson et al, 2005b).
Some researchers referthis information extraction task as opinion extrac-tion or opinion mining.
Comparing with the for-mer one, opinion mining usually produces richerinformation.In this paper, we define an opinion unit as atriple consisting of a product feature, an expres-sion of opinion, and an emotional attitude(positiveor negative).
We use this definition as the basis forour opinion mining task.
Since a product reviewmay refer more than one product feature and ex-press different opinions on each of them, the rela-tion extraction is an important subtask of opinionmining.
Consider the following sentences:1.
I highly [recommend](1)the Canon SD500(1)toanybody looking for a compact camera that can take[good](2)pictures(2).2.
This camera takes [amazing](3)image qualities(3)and its size(4)[cannot be beat](4).The phrases underlined are the product features,marked with square brackets are opinion expres-sions.
Product features and opinion expressionswith identical superscript compose a relation.
Forthe first sentence, an opinion relation exists be-tween ?the Canon SD500?
and ?recommend?, butnot between ?picture?
and ?recommend?.
The ex-ample shows that more than one relation may ap-pear in a sentence, and the correct relations are notsimple Cartesian product of opinion expressionsand product features.Simple inspection of the data reveals that prod-uct features usually contain more than one word,such as ?LCD screen?, ?image color?, ?CanonPowerShot SD500?, and so on.
An incompleteproduct feature will confuse the successive anal-ysis.
For example, in passage ?Image color is dis-1533appointed?, the negative sentiment becomes ob-scure if only ?image?
or ?color?
is picked out.Since a product feature could not be representedby a single word, dependency parsing might not bethe best approach here unfortunately, which pro-vides dependency relations only between words.Previous works on relation extraction usually usethe head word to represent the whole phrase andextract features from the word level dependencytree.
This solution is problematic because the in-formation provided by the phrase itself can not beused by this kind of methods.
And, experimentalresults show that relation extraction task can ben-efit from dependencies within a phrase.To solve this issue, we introduce the conceptof phrase dependency parsing and propose an ap-proach to construct it.
Phrase dependency pars-ing segments an input sentence into ?phrases?
andlinks segments with directed arcs.
The parsingfocuses on the ?phrases?
and the relations be-tween them, rather than on the single words insideeach phrase.
Because phrase dependency parsingnaturally divides the dependencies into local andglobal, a novel tree kernel method has also beenproposed.The remaining parts of this paper are organizedas follows: In Section 2 we discuss our phrase de-pendency parsing and our approach.
In Section 3,experiments are given to show the improvements.In Section 4, we present related work and Section5 concludes the paper.2 The ApproachFig.
1 gives the architecture overview for our ap-proach, which performs the opinion mining taskin three main steps: (1) constructing phrase de-pendency tree from results of chunking and de-pendency parsing; (2) extracting candidate prod-uct features and candidate opinion expressions; (3)extracting relations between product features andopinion expressions.2.1 Phrase Dependency Parsing2.1.1 Overview of Dependency GrammarDependency grammar is a kind of syntactic the-ories presented by Lucien Tesni`ere(1959).
In de-pendency grammar, structure is determined by therelation between a head and its dependents.
Ingeneral, the dependent is a modifier or comple-ment; the head plays a more important role in de-termining the behaviors of the pair.
Therefore, cri-Phrase Dependency ParsingReview CrawlerReview DatabaseChunking DependencyParsingCandidateProduct FeaturesIdentificationCandidateOpinion ExpressionsExtractionRelation ExtractionOpinionDatabasePhrase Dependency TreeFigure 1: The architecture of our approach.teria of how to establish dependency relations andhow to distinguish the head and dependent in suchrelations is central problem for dependency gram-mar.
Fig.
2(a) shows the dependency represen-tation of an example sentence.
The root of thesentence is ?enjoyed?.
There are seven pairs ofdependency relationships, depicted by seven arcsfrom heads to dependents.2.1.2 Phrase Dependency ParsingCurrently, the mainstream of dependency parsingis conducted on lexical elements: relations arebuilt between single words.
A major informa-tion loss of this word level dependency tree com-pared with constituent tree is that it doesn?t ex-plicitly provide local structures and syntactic cat-egories (i.e.
NP, VP labels) of phrases (Xia andPalmer, 2001).
On the other hand, dependencytree provides connections between distant words,which are useful in extracting long distance rela-tions.
Therefore, compromising between the two,we extend the dependency tree node with phrases.That implies a noun phrase ?Cannon SD500 Pow-erShot?
can be a dependent that modifies a verbphrase head ?really enjoy using?
with relation type?dobj?.
The feasibility behind is that a phrase is asyntactic unit regardless of the length or syntac-tic category (Santorini and Kroch, 2007), and it isacceptable to substitute a single word by a phrasewith same syntactic category in a sentence.Formally, we define the dependency parsingwith phrase nodes as phrase dependency parsing.A dependency relationship which is an asymmet-ric binary relationship holds between two phrases.One is called head, which is the central phrase inthe relation.
The other phrase is called dependent,which modifies the head.
A label representing the1534enjoyedWe nsubj reallyadvmod usingpartmodSD500thedet Canon PowerShotnn nndobjenjoyednsubj really usingpartmodWeVPNP SD500thedetCanon PowerShotnn nnNPadvmoddobj(a)(c)(b)NP SEGMENT:      [We] VP SEGMENT:      [really]      [enjoyed ]      [using] NP SEGMENT:      [the]      [Canon]      [PowerShot]      [SD500]Figure 2: Example of Phrase Dependency Parsing.relation type is assigned to each dependency rela-tionship, such as subj (subject), obj (object), andso on.
Fig.2(c) shows an example of phrase de-pendency parsing result.By comparing the phrase dependency tree andthe word level dependency tree in Fig.2, the for-mer delivers a more succinct tree structure.
Localwords in same phrase are compacted into a sin-gle node.
These words provide local syntactic andsemantic effects which enrich the phrase they be-long to.
But they should have limited influences onthe global tree topology, especially in applicationswhich emphasis the whole tree structures, such astree kernels.
Pruning away local dependency re-lations by additional phrase structure information,phrase dependency parsing accelerates followingprocessing of opinion relation extraction .To construct phrase dependency tree, we pro-pose a method which combines results from anexisting shallow parser and a lexical dependencyparser.
A phrase dependency tree is defined asT = (V ,E ), where V is the set of phrases,E is the dependency relations among the phrasesin V representing by direct edges.
To reservethe word level dependencies inside a phrase, wedefine a nested structure for a phrase Tiin V :Ti= (Vi, Ei).
Vi= {v1, v2, ?
?
?
, vm} is the inter-nal words, Eiis the internal dependency relations.We conduct the phrase dependency parsing inthis way: traverses word level dependency treein preorder (visits root node first, then traversesthe children recursively).
When visits a node R,searches in its children and finds the node set Dwhich are in the same phrase with R accordingAlgorithm 1 Pseudo-Code for constructing thephrase dependency treeINPUT:T?= (V?, E?)
a word level dependency treeP = phrasesOUTPUT:phrase dependency tree T = (V , E ) whereV = {T1(V1, E1), T2(V2, E2), ?
?
?
, Tn(Vn, En)}Initialize:V ?
{({v?
}, {})|v??
V?
}E ?
{(Ti, Tj)|(v?i, v?j) ?
E?, v?i?
Vi, v?j?
Vj}R = (Vr, Er) root of TPhraseDPTree(R, P )1: Find pi?
P where word[R] ?
pi2: for each S = (Vs, Es), (R,S) ?
E do3: if word[S] ?
pithen4: Vr?
Vr?
vs; vs?
Vs5: Er?
Er?
(vr, root[S]); vr?
Vr6: V ?
V ?
S7: E ?
E + (R, l); ?
(S, l) ?
E8: E ?
E ?
(R,S)9: end if10: end for11: for each (R,S) ?
E do12: PhraseDPTree(S,P )13: end for14: return (V , E )to the shallow parsing result.
Compacts D and Rinto a single node.
Then traverses all the remain-ing children in the same way.
The algorithm isshown in Alg.
1.The output of the algorithm is still a tree, for weonly cut edges which are compacted into a phrase,the connectivity is keeped.
Note that there will beinevitable disagrees between shallow parser andlexical dependency parser, the algorithm impliesthat we simply follow the result of the latter one:the phrases from shallow parser will not appear inthe final result if they cannot be found in the pro-cedure.Consider the following example:?We really enjoyed using the Canon PowerShot SD500.
?Fig.2 shows the procedure of phrase depen-dency parsing.
Fig.2(a) is the result of the lex-ical dependency parser.
Shallow parsers resultis shown in Fig.2(b).
Chunk phrases ?NP(We)?,?VP(really enjoyed using)?
and ?NP(the CanonPowerShot SD500)?
are nodes in the output phrasedependency tree.
When visiting node ?enjoyed?
inFig.2(a), the shallow parser tells that ?really?
and?using?
which are children of ?enjoy?
are in thesame phrase with their parent, then the three nodesare packed.
The final phrase dependency parsingtree is shown in the Fig.
2(c).15352.2 Candidate Product Features and OpinionExpressions ExtractionIn this work, we define that product featuresare products, product parts, properties of prod-ucts, properties of parts, company names and re-lated objects.
For example,in consumer elec-tronic domain, ?Canon PowerShot?, ?image qual-ity?,?camera?, ?laptop?
are all product features.From analyzing the labeled corpus, we observethat more than 98% of product features are in asingle phrase, which is either noun phrase (NP) orverb phrase (VP).
Based on it, all NPs and VPsare selected as candidate product features.
Whileprepositional phrases (PPs) and adjectival phrases(ADJPs) are excluded.
Although it can covernearly all the true product features, the precisionis relatively low.
The large amount of noise can-didates may confuse the relation extraction clas-sifier.
To shrink the size of candidate set, we in-troduce language model by an intuition that themore likely a phrase to be a product feature, themore closely it related to the product review.
Inpractice, for a certain domain of product reviews,a language model is build on easily acquired unla-beled data.
Each candidate NP or VP chunk in theoutput of shallow parser is scored by the model,and cut off if its score is less than a threshold.Opinion expressions are spans of text that ex-press a comment or attitude of the opinion holder,which are usually evaluative or subjective phrases.We also analyze the labeled corpus for opinion ex-pressions and observe that many opinion expres-sions are used in multiple domains, which is iden-tical with the conclusion presented by Kobayashiet al (2007).
They collected 5,550 opinion ex-pressions from various sources .
The coverage ofthe dictionary is high in multiple domains.
Moti-vated by those observations, we use a dictionarywhich contains 8221 opinion expressions to selectcandidates (Wilson et al, 2005b).
An assump-tion we use to filter candidate opinion expressionsis that opinion expressions tend to appear closelywith product features, which is also used to extractproduct features by Hu and Liu (2004).
In our ex-periments, the tree distance between product fea-ture and opinion expression in a relation should beless than 5 in the phrase dependency parsing tree.2.3 Relation ExtractionThis section describes our method on extractingrelations between opinion expressions and productfeatures using phrase dependency tree.
Manuallybuilt patterns were used in previous works whichhave an obvious drawback that those patterns canhardly cover all possible situations.
By taking ad-vantage of the kernel methods which can search afeature space much larger than that could be repre-sented by a feature extraction-based approach, wedefine a new tree kernel over phrase dependencytrees and incorporate this kernel within an SVM toextract relations between opinion expressions andproduct features.The potential relation set consists of the allcombinations between candidate product featuresand candidate opinion expressions in a sentence.Given a phrase dependency parsing tree, wechoose the subtree rooted at the lowest commonparent(LCP) of opinion expression and productfeature to represent the relation.Dependency tree kernels has been proposed by(Culotta and Sorensen, 2004).
Their kernel is de-fined on lexical dependency tree by the convolu-tion of similarities between all possible subtrees.However, if the convolution containing too manyirrelevant subtrees, over-fitting may occur and de-creases the performance of the classifier.
In phrasedependency tree, local words in a same phrase arecompacted, therefore it provides a way to treat ?lo-cal dependencies?
and ?global dependencies?
dif-ferently (Fig.
3).
As a consequence, these twokinds of dependencies will not disturb each otherin measuring similarity.
Later experiments provethe validity of this statement.BA CDEBACD EPhrase Local dependenciesGlobal dependenciesFigure 3: Example of ?local dependencies?
and?global dependencies?.We generalize the definition by (Culotta andSorensen, 2004) to fit the phrase dependency tree.Use the symbols in Section 2.1.2, Tiand Tjaretwo trees with root Riand Rj, K(Ti,Tj) is thekernel function for them.
Firstly, each tree nodeTk?
Tiis augmented with a set of features F ,and an instance of F for Tkis Fk= {fk}.
Amatch function m(Ti, Tj) is defined on comparinga subset of nodes?
features M ?
F .
And in thesame way, a similarity function s(Ti, Tj) are de-1536fined on S ?
Fm(Ti, Tj) ={1 if fim= fjm?fm?
M0 otherwise(1)ands(Ti, Tj) =?fs?SC(fis, fjs) (2)whereC(fis, fjs) ={1 if fis= fjs0 otherwise(3)For the given phrase dependency parsing trees,the kernel function K(Ti,Tj) is defined as fol-low:K(Ti,Tj) =????
?0 if m(Ri, Rj) = 0s(Ri, Rj) +Kin(Ri, Rj)+Kc(Ri.C, Rj.C) otherwise(4)where Kin(Ri, Rj) is a kernel function overRi= (Vir, Eir) and Rj= (Vjr, Ejr)?s internalphrase structures,Kin(Ri, Rj) = K(Ri, Rj) (5)Kcis the kernel function over Riand Rj?s chil-dren.
Denote a is a continuous subsequence of in-dices a, a+1, ?
?
?
a+ l(a) for Ri?s children wherel(a) is its length, asis the s-th element in a. Andlikewise b for Rj.Kc(Ri.C, Rj.C) =?a,b,l(a)=l(b)?l(a)K(Ri.
[a], Rj.[b])??s=1..l(a)m(Ri.
[as], Rj.
[bs])(6)where the constant 0 < ?
< 1 normalizes the ef-fects of children subsequences?
length.Compared with the definitions in (Culotta andSorensen, 2004), we add term Kinto handle theinternal nodes of a pharse, and make this exten-sion still satisfy the kernel function requirements(composition of kernels is still a kernel (Joachimset al, 2001)).
The consideration is that the localwords should have limited effects on whole treestructures.
So the kernel is defined on externalchildren (Kc) and internal nodes (Kin) separately,Table 1: Statistics for the annotated corpusCategory # Products # SentencesCell Phone 2 1100Diaper 1 375Digital Camera 4 1470DVD Player 1 740MP3 Player 3 3258as the result, the local words are not involved insubsequences of external children for Kc.
Afterthe kernel computing through training instances,support vector machine (SVM) is used for classi-fication.3 Experiments and ResultsIn this section, we describe the annotated corpusand experiment configurations including baselinemethods and our results on in-domain and cross-domain.3.1 CorpusWe conducted experiments with labeled corpuswhich are selected from Hu and Liu (2004), Jin-dal and Liu (2008) have built.
Their documentsare collected from Amazon.com and CNet.com,where products have a large number of reviews.They also manually labeled product features andpolarity orientations.
Our corpus is selectedfrom them, which contains customer reviews of11 products belong to 5 categories(Diaper, CellPhone, Digital Camera, DVD Player, and MP3Player).
Table 1 gives the detail statistics.Since we need to evaluate not only the prod-uct features but also the opinion expressions andrelations between them, we asked two annotatorsto annotate them independently.
The annotatorsstarted from identifying product features.
Then foreach product feature, they annotated the opinionexpression which has relation with it.
Finally, oneannotator A1extracted 3595 relations, while theother annotator A2extracted 3745 relations, and3217 cases of them matched.
In order to measurethe annotation quality, we use the following metricto measure the inter-annotator agreement, which isalso used by Wiebe et al (2005).agr(a||b) =|A matches B||A|1537Table 2: Results for extracting product featuresand opinion expressionsP R FProduct Feature 42.8% 85.5% 57.0%Opinion Expression 52.5% 75.2% 61.8%Table 3: Features used in SVM-1: o denotes anopinion expression and t a product feature1) Positions of o/t in sentence(start, end, other);2) The distance between o and t (1, 2, 3, 4, other);3) Whether o and t have direct dependency relation;4) Whether o precedes t;5) POS-Tags of o/t.where agr(a||b) represents the inter-annotatoragreement between annotator a and b, A and Bare the sets of anchors annotated by annotators aand b. agr(A1||A2) was 85.9% and agr(A2||A1)was 89.5%.
It indicates that the reliability of ourannotated corpus is satisfactory.3.2 Preprocessing ResultsResults of extracting product features and opin-ion expressions are shown in Table 2.
We useprecision, recall and F-measure to evaluate perfor-mances.
The candidate product features are ex-tracted by the method described in Section 2.2,whose result is in the first row.
6760 of 24414candidate product features remained after the fil-tering, which means we cut 72% of irrelevant can-didates with a cost of 14.5%(1-85.5%) loss in trueanswers.
Similar to the product feature extraction,the precision of extracting opinion expression isrelatively low, while the recall is 75.2%.
Sinceboth product features and opinion expressions ex-tractions are preprocessing steps, recall is moreimportant.3.3 Relation Extraction Experiments3.3.1 Experiments SettingsIn order to compare with state-of-the-art results,we also evaluated the following methods.1.
Adjacent method extracts relations between aproduct feature and its nearest opinion expression,which is also used in (Hu and Liu, 2004).2.
SVM-1.
To compare with tree kernel basedTable 4: Features used in SVM-PTreeFeatures for match function1) The syntactic category of the tree node(e.g.
NP, VP, PP, ADJP).2) Whether it is an opinion expression node3) Whether it is a product future node.Features for similarity function1) The syntactic category of the tree node(e.g.
NP, VP, PP, ADJP).2) POS-Tag of the head word of node?s internalphrases.3) The type of phrase dependency edge linkingto node?s parent.4) Feature 2) for the node?s parent5) Feature 3) for the node?s parentapproaches, we evaluated an SVM1result with aset of manually selected features(Table 3), whichare also used in (Kobayashi et al, 2007).3.
SVM-2 is designed to compare the effective-ness of cross-domain performances.
The featuresused are simple bag of words and POS-Tags be-tween opinion expressions and product features.4.
SVM-WTree uses head words of opinion ex-pressions and product features in the word-leveldependency tree, as the previous works in infor-mation extraction.
Then conducts tree kernel pro-posed by Culotta and Sorensen (2004).5.
SVM-PTree denotes the results of our tree-kernel based SVM, which is described in the Sec-tion 2.3.
Stanford parser (Klein and Manning,2002) and Sundance (Riloff and Phillips, 2004)are used as lexical dependency parser and shallowparser.
The features in match function and simi-larity function are shown in Table 4.6.
OERight is the result of SVM-PTree withcorrect opinion expressions.7.
PFRight is the result of SVM-PTree withcorrect product features.Table 5 shows the performances of differentrelation extraction methods with in-domain data.For each domain, we conducted 5-fold cross val-idation.
Table 6 shows the performances of theextraction methods on cross-domain data.
We usethe digital camera and cell phone domain as train-ing set.
The other domains are used as testing set.1libsvm 2.88 is used in our experiments1538Table 5: Results of different methodsCell Phone MP3 Player Digital Camera DVD Player DiaperMethods P R F P R F P R F P R F P R FAdjacent 40.3% 60.5% 48.4% 26.5% 59.3% 36.7% 32.7% 59.1% 42.1% 31.8% 68.4% 43.4% 23.4% 78.8% 36.1%SVM-1 69.5% 42.3% 52.6% 60.7% 30.6% 40.7% 61.4% 32.4% 42.4% 56.0% 27.6% 37.0% 29.3% 14.1% 19.0%SVM-2 60.7% 19.7% 29.7% 63.6% 23.8% 34.6% 66.9% 23.3% 34.6% 66.7% 13.2% 22.0% 79.2% 22.4% 34.9%SVM-WTree 52.6% 52.7% 52.6% 46.4% 43.8% 45.1% 49.1% 46.0% 47.5% 35.9% 32.0% 33.8% 36.6% 31.7% 34.0%SVM-PTree 55.6% 57.2% 56.4% 51.7% 50.7% 51.2% 54.0% 49.9% 51.9% 37.1% 35.4% 36.2% 37.3% 30.5% 33.6%OERight 66.7% 69.5% 68.1% 65.6% 65.9% 65.7% 64.3% 61.0% 62.6% 59.9% 63.9% 61.8% 55.8% 58.5% 57.1%PFRight 62.8% 62.1% 62.4% 61.3% 56.8% 59.0% 59.7% 56.2% 57.9% 46.9% 46.6% 46.7% 58.5% 51.3% 53.4%Table 6: Results for total performance with cross domain training dataDiaper DVD Player MP3 PlayerMethods P R F P R F P R FAdjacent 23.4% 78.8% 36.1% 31.8% 68.4% 43.4% 26.5% 59.3% 36.7%SVM-1 22.4% 30.6% 25.9% 52.8% 30.9% 39.0% 55.9% 36.8% 44.4%SVM-2 71.9% 15.1% 25.0% 51.2% 13.2% 21.0% 63.1% 22.0% 32.6%SVM-WTree 38.7% 52.4% 44.5% 30.7% 59.2% 40.4% 38.1% 47.2% 42.2%SVM-PTree 37.3% 53.7% 44.0% 59.2% 48.3% 46.3% 43.0% 48.9% 45.8%3.3.2 Results DiscussionTable 5 presents different methods?
results in fivedomains.
We observe that the three learning basedmethods(SVM-1, SVM-WTree, SVM-PTree) per-form better than the Adjacent baseline in the firstthree domains.
However, in other domains, di-rectly adjacent method is better than the learningbased methods.
The main difference between thefirst three domains and the last two domains is thesize of data(Table 1).
It implies that the simple Ad-jacent method is also competent when the trainingset is small.A further inspection into the result of first 3domains, we can also conclude that: 1) Treekernels(SVM-WTree and SVM-PTree) are betterthan Adjacent, SVM-1 and SVM-2 in all domains.It proofs that the dependency tree is importantin the opinion relation extraction.
The reasonfor that is a connection between an opinion andits target can be discovered with various syntac-tic structures.
2) The kernel defined on phrasedependency tree (SVM-PTree) outperforms ker-nel defined on word level dependency tree(SVM-WTree) by 4.8% in average.
We believe the mainreason is that phrase dependency tree provides amore succinct tree structure, and the separativetreatment of local dependencies and global depen-dencies in kernel computation can indeed improvethe performance of relation extraction.To analysis the results of preprocessing steps?influences on the following relation extraction,we provide 2 additional experiments which theproduct features and opinion expressions are allcorrectly extracted respectively: OERight andPFRight.
These two results show that given anexactly extraction of opinion expression and prod-uct feature, the results of opinion relation extrac-tion will be much better.
Further, opinion expres-sions are more influential which naturally meansthe opinion expressions are crucial in opinion re-lation extraction.For evaluations on cross domain, the Adjacentmethod doesn?t need training data, its results arethe same as the in-domain experiments.
Notein Table 3 and Table 4, we don?t use domainrelated features in SVM-1, SVM-WTree, SVM-PTree, but SVM-2?s features are domain depen-dent.
Since the cross-domain training set is largerthan the original one in Diaper and DVD domain,the models are trained more sufficiently.
The fi-nal results on cross-domain are even better thanin-domain experiments on SVM-1, SVM-WTree,and SVM-PTree with percentage of 4.6%, 8.6%,10.3% in average.
And the cross-domain train-ing set is smaller than in-domain in MP3, butit also achieve competitive performance with the1539in-domain.
On the other hand, SVM-2?s resultdecreased compared with the in-domain experi-ments because the test domain changed.
At thesame time, SVM-PTree outperforms other meth-ods which is similar in in-domain experiments.4 Related WorkOpinion mining has recently received consider-able attention.
Amount of works have beendone on sentimental classification in different lev-els (Zhang et al, 2009; Somasundaran et al, 2008;Pang et al, 2002; Dave et al, 2003; Kim andHovy, 2004; Takamura et al, 2005).
While wefocus on extracting product features, opinion ex-pressions and mining relations in this paper.Kobayashi et al (2007) presented their work onextracting opinion units including: opinion holder,subject, aspect and evaluation.
Subject and aspectbelong to product features, while evaluation is theopinion expression in our work.
They convertedthe task to two kinds of relation extraction tasksand proposed a machine learning-based methodwhich combines contextual clues and statisticalclues.
Their experimental results showed that themodel using contextual clues improved the perfor-mance.
However since the contextual informationin a domain is specific, the model got by their ap-proach can not easily converted to other domains.Choi et al (2006) used an integer linear pro-gramming approach to jointly extract entities andrelations in the context of opinion oriented infor-mation extraction.
They identified expressions ofopinions, sources of opinions and the linking re-lation that exists between them.
The sources ofopinions denote to the person or entity that holdsthe opinion.Another area related to our work is opinionexpressions identification (Wilson et al, 2005a;Breck et al, 2007).
They worked on identify-ing the words and phrases that express opinionsin text.
According to Wiebe et al (2005), there aretwo types of opinion expressions, direct subjectiveexpressions and expressive subjective elements.5 ConclusionsIn this paper, we described our work on min-ing opinions from unstructured documents.
Wefocused on extracting relations between productfeatures and opinion expressions.
The noveltiesof our work included: 1) we defined the phrasedependency parsing and proposed an approachto construct the phrase dependency trees; 2) weproposed a new tree kernel function to modelthe phrase dependency trees.
Experimental re-sults show that our approach improved the perfor-mances of the mining task.6 AcknowledgementThis work was (partially) funded by ChineseNSF 60673038, Doctoral Fund of Ministry ofEducation of China 200802460066, and Shang-hai Science and Technology Development Funds08511500302.
The authors would like to thank thereviewers for their useful comments.ReferencesEric Breck, Yejin Choi, and Claire Cardie.
2007.
Iden-tifying expressions of opinion in context.
In Pro-ceedings of IJCAI-2007.Yejin Choi, Claire Cardie, Ellen Riloff, and SiddharthPatwardhan.
2005.
Identifying sources of opinionswith conditional random fields and extraction pat-terns.
In Proceedings of HLT/EMNLP.Yejin Choi, Eric Breck, and Claire Cardie.
2006.
Jointextraction of entities and relations for opinion recog-nition.
In Proceedings EMNLP.Aron Culotta and Jeffrey Sorensen.
2004.
Dependencytree kernels for relation extraction.
In In Proceed-ings of ACL 2004.Kushal Dave, Steve Lawrence, and David M. Pennock.2003.
Mining the peanut gallery: opinion extractionand semantic classification of product reviews.
InProceedings of WWW 2003.Minqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of the ACMSIGKDD 2004.Nitin Jindal and Bing Liu.
2008.
Opinion spam andanalysis.
In Proceedings of WSDM ?08.Thorsten Joachims, Nello Cristianini, and John Shawe-Taylor.
2001.
Composite kernels for hypertext cate-gorisation.
In Proceedings of ICML ?01.Soo-Min Kim and Eduard Hovy.
2004.
Determiningthe sentiment of opinions.
In Proceedings of Coling2004.
COLING.Dan Klein and Christopher D. Manning.
2002.
Fastexact inference with a factored model for naturallanguage parsing.
In In Advances in Neural Infor-mation Processing Systems.Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.2007.
Extracting aspect-evaluation and aspect-ofrelations in opinion mining.
In Proceedings ofEMNLP-CoNLL 2007.1540Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
Sentiment classification usingmachine learning techniques.
In Proc.
of EMNLP2002.Ana-Maria Popescu and Oren Etzioni.
2005.
Extract-ing product features and opinions from reviews.
InProceedings of HLT/EMNLP.E.
Riloff and W. Phillips.
2004.
An introduction tothe sundance and autoslog systems.
In University ofUtah School of Computing Technical Report UUCS-04-015.Beatrice Santorini and Anthony Kroch.
2007.The syntax of natural language: An on-line introduction using the Trees program.http://www.ling.upenn.edu/ beatrice/syntax-textbook.Swapna Somasundaran, Janyce Wiebe, and Josef Rup-penhofer.
2008.
Discourse level opinion interpreta-tion.
In Proceedings of COLING 2008.Hiroya Takamura, Takashi Inui, and Manabu Okumura.2005.
Extracting semantic orientations of words us-ing spin model.
In Proceedings of ACL?05.L.
Tesni`ere.
1959.
El?ements de syntaxe structurale.Editions Klincksieck.Janyce Wiebe, Theresa Wilson, and Claire Cardie.2005.
Annotating expressions of opinions and emo-tions in language.
Language Resources and Evalu-ation, 39(2/3).Theresa Wilson, Paul Hoffmann, Swapna Somasun-daran, Jason Kessler, Janyce Wiebe, Yejin Choi,Claire Cardie, Ellen Riloff, and Siddharth Patward-han.
2005a.
Opinionfinder: A system for subjectiv-ity analysis.
In Demonstration Description in Con-ference on Empirical Methods in Natural LanguageProcessing.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005b.
Recognizing contextual polarity in phrase-level sentiment analysis.
In Proceedings of HLT-EMNLP.Fei Xia and Martha Palmer.
2001.
Converting depen-dency structures to phrase structures.
In HLT ?01:Proceedings of the first international conference onHuman language technology research.Qi Zhang, Yuanbin Wu, Tao Li, Mitsunori Ogihara,Joseph Johnson, and Xuanjing Huang.
2009.
Min-ing product reviews based on shallow dependencyparsing.
In Proceedings of SIGIR 2009.1541
