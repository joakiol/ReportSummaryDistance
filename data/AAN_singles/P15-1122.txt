Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1263?1271,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsImplicit Role Linking on Chinese Discourse:Exploiting Explicit Roles and Frame-to-Frame RelationsRu Li1,2, Juan Wu1, Zhiqiang Wang1and Qinghua Chai31School of Computer and Information Technology2Key Laboratory of Ministry of Education for Computation Intelligence and Chinese Information Processing3School of Foreign Languages, Shanxi University, Taiyuan, China{liru,charles}@sxu.edu.cn, {wujuan_0922,zhiq.wang}@163.comAbstractThere is a growing interest in research-ing null instantiations, which are thoseimplicit semantic arguments.
Many ofthese implicit arguments can be linked toreferents in context, and their discoveriesare of great benefits to semantic process-ing.
We address the issue of automat-ically identifying and resolving implicitarguments in Chinese discourse.
For theirresolutions, we present an approach thatcombines the information about overtly la-beled arguments and frame-to-frame rela-tions defined by FrameNet.
Experimentalresults on our created corpus demonstratethe effectiveness of our approach.1 IntroductionIn natural discourse, only a small proportion ofthe theoretically possible semantic arguments ofpredicates tend to be locally instantiated.
Otherlocally unrealized semantic roles are called nullinstantiations (NIs).
Nevertheless, many of theseimplicit roles, while linguistically unexpressed,can often be bound to antecedent referents inthe discourse context.
What?s more, capturingsuch implicit semantic roles and linking themto their antecedents can dramatically help textunderstanding.Example (1) shows an analyzed result (Li,2012) by employing Chinese FrameNet (Liu,2011), which is a lexical semantic knowledge basebased on the frame semantics of Fillmore (1982)and takes Berkeley?s FrameNet Project (Baker etal., 1998) as the reference.
In Chinese FrameNet,the predicates, called lexical units (LU), evokeframes which roughly correspond to differentevents or scenarios.
Each frame defines a set ofarguments called Frame Elements (FE).
The setof FEs is further split into core FEs and non-coreFEs.
Particularly, the core FEs are the essentialcomponents of a frame and can be defined bythemselves.
However, not all core FEs of a framecan be realized simultaneously in a sentence.These non-instantiated FEs are considered as nullinstantiations of the frame elements.
Dependingon the interpretation type of the omission, ChineseFrameNet divides the NIs into two categories: 1)Indefinite Null Instantiations (INIs), the missingelement which can be understood given interpre-tational conventions and do not need resolution,and 2) Definite Null Instantiations (DNIs), themissing element which is something that can beunderstood in the linguistic or discourse context,and the fillers need to be inferred from the contextthrough resolutions.
(1) [U:?(]Entity??
?u,a [<E?
(]Category , ?	?(!?&?(!???
(??
?a"[The celestial burial satellite]EntityisBeing_in_category[artificial satellite]Category, and belongs to the samecategory with reconnaissance, communications andmeteorological satellite.???^?????;?^5????p???????u???
[?/?L?3000??p???;?
]Goal"[Theme DNI] [Agent INI]The purpose is different, specially used for storingthe urn; due to the different heights, generallylaunchedCause_motioninto [the orbit over 3000 kilo-meters away from the surface of earth]Goal.
[ThemeDNI] [Agent INI]Particularly, in example (1), lexical unit (ortarget) launched/u evokes the semantic frameCause_motion, which has nine core FEs,namely Agent, Theme, Source, Path, Goal, Area,Cause, Result, Initial_State, but only one of themis instantiated, i.e.
Goal, whose filler is [the orbitover 3000 kilometers away from the surface ofearth/?/?L?3000??p???;?
].For another core FE Theme, it is filled by [Thecelestial burial satellite/U:?
(] that occurs in1263the previous sentence.Clearly, human beings have no problem toinfer these uninstantiated roles and find the cor-responding fillers based on the relevant contextinformation, but this is beyond the capacity ofstate-of-the-art semantic role labeling systems.Next, we formalize the problem as follows:given a discourse D = {S1, S2, ..., Sn}, whereSk(k ?
[1, n]) is the k-th sentence in D. Thelexical unit set in Skis Tk= {Tk1, Tk2, ..., Tkp},and Fk= {Fk1, Fk2, ..., Fkp} is relevant frameset.
For a particular frame Fki(i ?
[1, p]), its coreFE set is Eki= {e1, e2, ..., em}, but it is possiblethat only part of core FEs Ckiappears in Sk,i.e.
Cki?
Eki.
Apparently the set Eki?
Ckiincludes the uninstantiated core FEs.
Thus, weneed to determine which elements in Eki?
Ckiare null instantiations.
If em(em?
Eki?Cki) hasbeen identified as a null instantiated FE, we shoulddetermine whether emis a DNI.
If so, we need tofind the corresponding antecedent dmin context.The major contributions of this paper can besummarized as follows:(i) We have created a null instantiation (NI)annotations corpus, consisting of 164 Chinesediscourses across different fields.
(ii) We use frame-to-frame relations to findantecedents from those explicit semantic roles.2 Related WorkAmong the researches of null instantiation onEnglish, the most representative work is thetask ?Linking Events and Their Participants inDiscourse?
shared by the SemEval-2010 (Ruppen-hofer et al, 2010).
The two systems participatedin the NI resolution task, VENSES++ and SE-MAFOR, took very different approaches.Tonelli and Delmonte (2010) develop aknowledge-based system called VENSES++,and describe two strategies depending on thepredicate class (either nominal or verbal).
Forverbal predicates, they try to map the predicateargument structure extracted by VENSES withthe valence patterns generated from FrameNetdata, to identify missing arguments.
And NIsare resolved by reasoning about the semanticsimilarity between an NI and a potential fillerusing WordNet.
For nominal predicates, theyresolve NIs by utilizing a common sense reasoningmodule that builds on ConceptNet (Liu and Singh,2004).
The final Precision and Recall are 4.62%and 0.86% respectively.Later on, Tonelli and Delmonte (2011) proposea simpler role linking strategy that based oncomputing a relevancy score for the nominal headof each potential antecedent.
The intuition is thatheads which often serve as role fillers and occurclose to the target NI are more likely to functionas antecedents for the NI.
Finally they reported anF-score of 8% for role linking.
However, beingstrongly lexicalized, their trained model seemsheavily dependent on the training data.The second system (Chen et al, 2010) isstatistical based and extends an existing semanticrole labeler (Das et al, 2010).
Resolving DNIsis modeled in the same way as labeling overtarguments, with the search space being extendedto nouns, pronouns, and noun phrases from theprevious three sentences.
When evaluating apotential filler, the syntactic features used inargument labeling of overt arguments are replacedby two semantic features: firstly the system checkswhether a potential filler fills the null instantiatedrole overtly in at least one of the FrameNet sen-tences and train data, if not, the system calculatesthe distributional similarity between filler and role.While this system achieved 5% in F-score, datasparseness is a potential limiting factor.Also closely related studies are as follows.Silberer and Frank (2012) cast NI resolution asa coreference resolution (CR) task, and employan entity-mention model.
They experiment withfeatures of SRL and CR, and automatically expandthe training set with examples generated fromcoreference corpus to avoid data sparseness, ulti-mately achieving F-score of 7.1%.Gorinski et al (2013) present a weakly su-pervised approach that investigates and combinesa number of linguistically motivated strategies,which consist of four basic NI resolvers thatexploit different types of linguistic knowledge,and achieve F-score of 12%.Wang et al (2013) conduct DNI resolutionon SemEval2010 task10 data.
They consideredthe task as a classified problem, by adding newfeatures such as the information of head wordand frame to traditional features, proposed arule to choose the best candidate words set andcombination of features, achieving F-score of14.65% finally.Laparra and Rigau (2013) present an attempt toapply a set of features that have been traditionally1264used to model anaphora and coreference resolu-tion tasks to implicit argument resolution, and gotthe best results: F-score of 18%.For nominal predicates, Gerber and Chai (2010)investigate the linking of implicit arguments usingthe PropBank role labeling scheme.
In contrast tothe SemEval task, which focuses on a verbs andnouns, their system is only applied to nouns andis restricted to 10 predicates with 120 annotatedinstances per predicate on average.
They proposea discriminative model that selects an antecedentfor an implicit role from an extended contextwindow.
The approach incorporates some aspectsrelating to CR that go beyond the SRL orientedSemEval systems: A candidate representationincludes information about all the candidates?coreferent mentions (determined by automaticCR), in particular their semantic roles (provid-ed by gold annotations) and WordNet synsets.Patterns of semantic associations between fillercandidates and implicit roles are learned for allmentions contained in the candidate?s entity chain.They achieve an F-score of 42.3%, which isnoticeably higher than those obtained on theSemEval data.And Gerber (2011) presents an extended modelthat incorporates strategies suggested in Burchardtet al (2005): using frame relations as well ascoreference patterns acquired from large corpora.This model achieves an F-score of 50.3%.Lei et al (2013) conduct DNI identification onSemEval2010 task10 data.
They adopt the methodof combining rules and machine learning.
Differ-ent from them, we conduct two-level identifyingfor NI detection and use more features on Chinesedata.
Wang et al (2013) take noun phrases andpronoun as candidate words for DNI filler.
We useseveral similar features with them.
The differencesare that 1) we take the fillers of overt instantiatedFE as candidate words and 2) we use Frame-to-Frame relations.
And Gerber (2011) also usedframe relations.
Different from them, we limitrelation paths to 2.3 Null Instantiation DetectionNow, we are ready to address the first subtask, i.e.null instantiation detection.3.1 Frame element relationsNot all core arguments of all frames can berealized simultaneously.
Some frames involvecore FEs that are mutually exclusive.
In example(2), in the Amalgamation frame, there arefour core FEs, namely Part_1, Part_2, Parts andWhole, in which the first two FEs are mutuallyexclusive with Parts, thus formed an Excludesrelation (relation 1).
At the same time, Part_1and Part_2 are in a Requires relation (relation2), which means that if one of these two coreFEs is present, then the other must occur as well.FE Whole, the result of the Amalgamation,is only existentially bound within the discourse,annotated as NI.CoreSet (relation 3) specifies that at least oneof the set must be instantiated overtly, thoughmore of them can also be instantiated.
As shownin example (3), in the Awareness frame, thetwo FEs Content and Topic are in one CoreSet.As Content is overtly realized, we consider Topicis not annotated as NI.
The frame owning thisrelation is complicated.
Sometimes, if one FE ofthis set is explicit, the absence of the other FEs inthe set is not annotated as NI, but sometimes it isnot true.
(2) [?N?]Part_1?[#N?]Part_2(?(?
3??
"[Whole INI][The old system]Part_1and [the new system]Part_2are combinedAmalgamationtogether.
[Whole INI](3) [\P?
]Cognizer [\??
]Content"[Your boss]Cognizeris awareAwareness[of your com-mitment ]Content.3.2 Modeling Null Instantiation detectionAs shown in example (1), given a frame Fki(e.g.
Cause_motion evoked by launched/u), NI detector needs to determine whether coreFEs in EFki?
subEFkiare missing, relying oninformation about the three types of the relationsamong core FEs: CoreSetFki, ExcludesFki,RequiresFki(as discussed in Section 3.1).
InCause_motion, the core FEs Initial_State,Goal, Path, Source and Result belong to thesame CoreSet, and Goal is instantiated, thusInitial_State, Path, Source and Result are notannotated as NIs.
Meanwhile core FEs Goal andArea are connected by the Excludes relation, sodo Cause and Agent.
Therefore, according to thecontext, Area and Cause are not annotated as NIs.Our approach for performing this detectionis described as follows.
For the first-level ofdetection, we make full use of the three types ofrelations, and adopt a rule-based strategy proposed1265by Lei et al (2013) to detect NIs.
As for CoreSetrelation, in particular, as long as one of the FEs inthis set is expressed overtly, NIs are not annotatedfor the absence of the other FEs in the set.
Ifnone of CoreSet is expressed, the contextuallymost relevant one should be annotated as a NI.However, this is difficult for automatic detector,which inevitably introduces some false detectedNIs.Thus, we conduct a second-level identifying.
Tobe specific, for the current lexical unit, i.e.
thetarget word, we collect its frame element patternsfrom the training dataset.
Frame element patternsare annotated semantic roles, which include theroles annotated as NIs.
Taking lexical unitlaunched/u as an example, Table 1 shows itsframe element patterns in our data.
Depending onthis kind of patterns, we are able to filter out somefalse NIs effectively.Patte1 Time AgentINITheme GoalINIPatte2 Agent Theme GoalINITable 1: Frame element patterns for the target u/launched in our data4 Definite Null InstantiationIdentificationIn this section, we focus on our second task ofdefinite null instantiation (DNI) identification.Before performing the implicit argument reso-lution in discourse, we have to decide which nullinstantiated frame elements should be selected, i.e.which null instantiations are definite.
As shownin example (1) above, assuming one detected nullinstantiated FE in the previous step is em(e.g.Theme), we should determine whether emneedsto be filled or not, that is, we should determine emas DNI or INI.Num Feature names Feature DescriptionsT1 Target Target predicateT2 PosThe part of speech of targetT3 FrameThe frame that target evokesT4 FENI NI of frame elementsT5 FE Overtly expressed FEsTable 2: Features description in DNI IdentificationWe treat this issue as a classification problem,and build a binary maximum entropy model topredict the null instantiation type of em.
Table2 lists all features used for training our models.In addition, we employ some similar features thatwere used in Lei et al (2013).
Meanwhile, wechoose to learn a SVM classifier for comparisonpurpose.5 Definite Null Instantiation ResolutionIn this section, we tackle the last subtask, namelydefinite null instantiation resolution.5.1 Frame-to-Frame RelationsThe relations of Frame-to-Frame and FE-to-FE inFrameNet, serve as important information sources,to be leveraged for DNI resolutions.FrameNet arranges frames into a net by definingframe-to-frame relations, including Inheritance,Inchoative Of, Subframe, Causative Of, Precedes,Using, See_also and Perspective On.
In the caseof Inheritance relation, it defines two frames,i.e.
one more general frame and the other morespecific frame.
The specific frame Commercebuy, for example, is inherited from the generalframe Getting.As Figure 1 shows, the inheritance relationallows a general frame (e.g., Getting) to bespecialized with a particular semantic interpreta-tion (e.g., Commerce buy).
Also the inheritancerelation exists between the frame elements of tworelated frames.
Each of the inheriting FEs containsall semantic properties of the inherited generalframe elements and also owns its additional pri-vate properties.GettingCore RecipientCore ThemeNcore ExplanationNcore MannerNcore MeansNcore PlaceNcore PurposeNcore SourceNcore TimeCommerce buyCore GoodsCore BuyerNcore ExplanationNcore TimeNcore MannerNcore MeansNcore PlaceNcore PurposeNcore SellerNcore Result Ncore MoneyRateNcoreFigure 1: FE-FE relations of frame Getting andCommerce buy1266Number Features Name Features DescriptionF1 DistCT The number of sentences between candidate FE content and targetT1 F2 CanFEcon Candidate frame element contentF3 CanFEpt The phrase type or POS of candidate frame element contentF4 Frame The frame that target predicate evokesT2 F5 FEDNI DNI frame elementF6 Target Target predicateF7 TargetPOS The part of speech of targetTable 3: Features description in Overt Frame Elements Based Resolver5.2 Modeling Definite Null InstantiationResolutionAfter accomplishing the previous processes, wecan perform DNI resolutions.
If the uninstantiatedFE em(e.g., Theme in example (1)) has beenidentified as DNI previously, we need to find thecorresponding antecedent mention dm(e.g., [Thecelestial burial satellite/U:?
(] in example(1)).
Due to having fine-grained frame semanticrole labeled for each sentence, we think the fillerof DNI maybe also instantiates the FE of otherannotated frames in the context.
Therefore, wecollect the overt FE content set ?
instantiated inthe discourse, and this set forms the overall set ofcandidates for DNI linking.
Then, for DNI em, asubset of candidates ?m(?m?
?)
is chosen ascandidate search space for resolving em.We implement two semantic resolvers basedon different methods.
For either of these tworesolvers, if two or more candidates score equallywell, the one closest to the target predicate ischosen.OvertFE is based on machine learning, and FFRis an inference method.
As the inherent difficultyof task, it?s difficult to find all fillers for DNIs onlyusing one of them.
Thus finally we simultaneouslyemploy OvertFE and FFR to find as many fillersfor DNIs as possible.Overt Frame Elements Based Resolver(OvertFE)This resolver is based on the assumption that thefiller of DNI can be found among the overt FEcontent set in context.
Given a DNI em, DNIlinking can be treated as a classification problemto judge whether a candidate overt FE contentd (d ?
?m) could be taken as filler of a DNI.Therefore, we employ a classification method tosolve the problem.
Clearly, the performance ofclassifiers largely depends on constructed features.Since corresponding antecedent of DNI is notovertly expressed, it is difficult to get someinformation from context to describe them.
Whatwe take as features is the information of candidateframe element contents and frame information.Table 3 lists all features used for training ourmodels.
Some similar features were employedby Wang et al (2013) where they also consideredDNI linking as a classification problem.Then maximum entropy models, widely usedin natural language processing (such as Chineseword segmentation and machine translation), areemployed to predict whether a candidate FEcontent is the filler of DNI.Frame-to-Frame Relations Based Resolver (F-FR)Another way of finding the correct filler is throughsearching Frame-to-Frame relations in a givencontext window.
This is because Frame-to-Framerelations and FE-to-FE relations can provide rel-evant information for finding DNI filler amongcandidate frame element contents.
Specifically,for one frame f1that contains a DNI, firstly weneed to find related frame f2with it from context.Then, if DNI frame element in f1has relation withthe frame element (marked with fe2) of f2, thefiller of fe2is the corresponding filler of this DNI.The detailed steps are reported in Algorithm 1.If frame names are the same, we think theyare related, and Figure 2 illustrates this case.As the frames evoked in two sentences are bothArriving, we link the antecedent of Goal inthe second sentence to [Tiananmen Square/US?2|], which is the content of Goal in the firstsentence.For other cases, we use the related frameswhich at most contain two relation paths (e.g.,the paths from Event to Process_start toActivity_start in Figure 3).
As shown inFigure 3, the target initiated/u?
in the firstsentence evokes the Activity_start frame,1267in which the two frame elements (Agent, Place) isexpressed in a single constituent [our country/?I], i.e.
the phenomenon of frame elementfusion arises.
Frame Event is evoked by thetarget happened/?y in the second sentence,where Time and Event FEs are expressed overt-ly, except the core FE Place.
In the net ofFrameNet, frame Activity_start inheritsfrom the frame Process_start which furtherinherits from the Event frame.
These inheritancerelationships also hold between the frame ele-ments of the related frames.
According to the FE-to-FE relations, the content of FE Place in the firstsentence, [our country/?I], is the correspondingfiller of implicit FE Place in the second sentence.Algorithm 1 : Frame-to-Frame Relations BasedResolverInput: The frame set in discourse is F ={f1, f2, ..., fn}; overt core frame element setfor frame fiis Ei= {e1, e2, ..., em}, its corre-sponding filler set is Ai= {a1, a2, ..., am}; oneframe that contains DNI e?is f?, target t evokesthe frame f?
; dis (ai, t) is the distance betweenDNI filler aiand target t; relationpath (fi, f?
)are the relation paths from fito f?
; Atempistemporary DNI filler setOutput: the filler a?of DNI e?Atemp= ?for each fi?
F doif fihas frame relation with f?ANDrelationpath (fi, f?)
?
2 thenfor each ei?
Ei, ai?
Aidoif eihas relation with e?thenai?
Atempend ifend forelse if fi= f?thenai?
Atempend ifend forif Atemp6= ?
thenfor ai?
Atempdoif dis (ai, t) is minimum thena?= aiend ifend forend ifreturn a?;????????????????????:LWKLQRQO\WKUHHWRILYHPLQXWHVDGR]HQSHRSOHDOODUULYHG????????????????????
?2Q2FWREHUUGZHFDPHWR7LDQDQPHQ6TXDUHDWWKHDSSRLQWHGWLPHTime Thm GoalArrivingTime ThmGoal DNIArrivingFigure 2: Two consecutive sentences owning thesame frame.
Bold fonts represent lexical units orframes.
Dashed boxes represent FEs.EventTime Agent Place?????
??
???
?????????????
?In the 50's, our country initiated the movement of killing sparrows.However, in the years after the vastly killing of sparrows, a plague of insects happened.??????????????????????????
??Time EventActivityActivity_startProcess_start,QKHULWDQFHTime Place EventPlace DNI,QKHULWDQFHFigure 3: Two consecutive sentences owing relatedframes.
Bold fonts represent lexical units or frames.Dashed boxes represent FEs.6 Experiments6.1 Experimental SettingsData: Experimental data set comes from SemanticComputing and Chinese FrameNet Research Cen-tor of Shanxi University1.
Because of the currentlow performance of CFN automatic semantic anal-ysis systems, all discourses are labeled semanticroles manually, and the process is similar with theFrameNet annotation.First, the ICTCLAS are used for part-of-speechtagging (omitted in examples), and we treat verbs,adjectives and nouns in each sentence as potentialtargets.
As not all potential targets can beannotated, it is necessary to identify those targetswhich can evoke frames.Then, we choose corresponding frames forthose targets.
For one verb target launched/u in example (1), we find its evoked frameCause_motion.Then annotate semantic roles for those con-stituents which share syntactical relations with thistarget, so the span [the orbit over 3000 kilometersaway from the surface of earth/?/?L?3000??p???;?]
is annotated as role Goal,which is, however, the only one instantiated, outof nine Cause_motion?s core frame elements.So according to the context and frame elementrelations, we need to determine whether each1http://sccfn.sxu.edu.cn/1268missing frame element should be annotated asDNI or INI.Next, we generate the XML format for ourannotated corpus, which is similar to the dataformat in SemEval-10 Task 10.Our 164 discourses had been annotated by oneperson (to make it consistent), and they consistof 57 discourses from People?s Daily and 107discourses from Chinese reading comprehension,which cover technology, health care, social, geog-raphy and other fields.
Each discourse contains10 sentences in average.
The data set containsabout 37526 words in 1618 sentences; it has175 frame types, including 2283 annotated frameinstances.
Table 4 shows the detailed statisticsof our data set.
we?ll share our data in thewebsite(http://sccfn.sxu.edu.cn/).discourses sentencesframeinst.frametypesINIs DNIs164 1618 2283 175 213 212Table 4: Corpus StatisticsDefinite Null Instantiation identification andresolution model: Our maximum entropy classi-fication model uses the toolkit from Zhang (2005)with the default parameter values.
The SVMclassifier for comparison was trained via SVMtoolkit LIBSVM with the default parameter valuestoo.6.2 Experimental ResultsBased on the experimental methods describedin the previous section, we have systematicallyevaluated our approach on the constructed Chinesenull instantiation corpus.
Note all the perfor-mances are achieved using 5-fold cross validation.Null Instantiation DetectionTable 5 gives the performance of NI detection,which achieves 72.71%, 86.12% and 78.84% inprecision, recall and F-score, respectively.
Here,the relatively lower precision is mainly due to theheuristic rules used to detect NIs.
However, it isworth to point out that lower precision and higherrecall is highly beneficial, as higher recall meansless filtering of true NIs.P% R% F%Ours 72.71 86.12 78.84Lei et al 56.18 90.57 69.34Table 5: Performance of NI DetectionTo illustrate the effectiveness of our method,we compare it with the Lei et al?s method onour data, as shown in the Table 5.
The F-scoreof our method is 78.84%, which is 9% higherthan that of Lei et al?s method.
Clearly, theseexperimental results further prove that our second-level identification is very effective.Definite Null Instantiation IdentificationTable 6 provides the performance of DNI iden-tification on our automatic NI detection results.It shows that DNI identification based on max-imum entropy model achieves the performanceof 67.86%, 69.93% and 68.88% in terms ofprecision, recall and F-score respectively, whichare better than the results using SVM classifier, aswell as the results employing Lei et al?s methodon our data.We observe, from Table 6, that the performanceof DNI identification is not high, possibly due tothe poorer results of NI detection in the previousstep.
Moreover, because of the diversity ofNI distribution, the difference of frames, andtarget words or missing core frame elements, theinterpretation of NI types may be quite different.Thus it is difficult to build a suitable and accurateuniform classification model.P% R% F%DNI IdenME 67.86 69.93 68.88DNI IdenSVM 67.25 62.02 64.53Lei et al 64.58 67.73 66.12Table 6: Performance of DNI IdentificationResolution on golden Definite Null InstantiationIn order to select the most effective featuresfor OvertFE resolver and choose the best searchspace, we assume perfect results for the firsttwo steps, that is, we perform DNI resolutionexperiment just with the correct DNIs in discourse.After extensive experiments employing differentsets of features in different window sizes, weconclude that combining all features can achievethe best performance.
Table 7 shows the resultson correct DNIs using the best feature set in thewindow of 2, 3 and 4 sentences containing andbefore the target predicate (Win2, Win3, Win4 forshort).For OvertFE resolver, it shows that the F-scorewith Win2 is higher than that in other windows,because the bigger the window size, the more thecandidate fillers for DNI, and the more difficult for1269Win2 Win3 Win4P% R% F% P% R% F% P% R% F%OvertFE 45.22 18.20 25.95 43.04 17.64 25.02 38.63 15.23 21.84FFR 65.56 16.29 26.11 63.50 16.81 26.58 58.53 17.32 26.72OvertFE+FFR 51.17 31.59 39.06 52.41 32.02 39.75 45.88 31.10 37.07Table 7: Results on golden DNIOvertFE classifier to find right fillers.For FFR resolver, it needs to find related frames,and we find that its resolved DNIs are less than thatby OvertFE resolver, thereby resulting in the lowerprecision of OvertFE than FFR.Though performances of OvertFE and FFR bothare relatively low, FFR can resolve several DNIsthat OvertFE can not.
Figures 2 and 3 bothare such cases.
So when combining the tworesolvers, the final result of OvertFE+FFR outper-forms that of each individual resolver.
Meanwhile,as shown in Table 7, for the combined resolverOvertFE+FFR, the F-score is the highest when thewindow size is 3 (i.e.
Win3).Overall: Null Instantiation ResolutionTable 8 gives the performance of overall nullinstantiations resolution with automatic NI detec-tion and automatic DNI determination.
It showsthat our resolver OvertFE+FFR achieves 40.53%,21.54% and 28.13% in terms of precision, recalland F-score.
In comparison with the results(52.41%, 32.02% and 39.75% in P, R and F) inWin3 of Table 7, it shows that the errors causedby automatic NI detection and automatic DNIdetermination decrease the performance of overallNI resolution by about 11% in terms of F-score.P% R% F%OvertFE 33.28 13.78 19.49FFR 52.95 9.71 16.41OvertFE+FFR 40.53 21.54 28.13Wang et al 31.93 12.76 18.23Table 8: Performance of NI resolution for our modelsand comparative systemsFor comparison, we also conduct DNI reso-lution on our constructed corpus employing themethod proposed by Wang et al (2013).
Since ourcorpus does not contain annotation of head words,the results are obtained by using their featureswithout head word information.
As the last line ofTable 8 shows, the performance behaves similarlywith our OvertFE resolver.
In addition, wenotice current state-of-the-art approach of Laparraand Rigau (2013) employs coreference models,although our corpus does not contain coreferenceannotation information.
As such, we are not ableto conduct experiments on our dataset using theirmethod for comparison purpose.Overall, the relatively low performance of res-olution reflects the inherent difficulty of this task,also reveals that further research is needed.7 Conclusion and Future WorkApparently, linking implicit participants of a pred-icate is a challenging problem.
We have presenteda study for identifying implicit arguments andfinding their antecedents in Chinese discourse.As shown in this paper, we split the difficulttask into three subtasks: null instantiation detec-tion, definite null instantiation identification anddefinite null instantiation resolution.
Among thethree subtasks, the third is our major focus.
For thethird subtask, we build two different resolvers: 1)OvertFE resolver, which represents that the fillerof a DNI can be found among those overt FEcontent set in context, by employing classificationmethods; 2) FFR resolver, which is the frame-related search, leverages rich network of frame-frame relations to find antecedents.
We haveproved that these two resolvers are very usefulfor the third subtask, and a combination of tworesolvers produced the best results.In the near future, we plan to create andrelease a larger null instantiation corpus.
As nullinstantiation detection and definite null instantia-tion identification are the foundation of resolvingdefinite null instantiation, it is critical to improvethe performance of both subtasks.
Moreover, asdifferent information sources have been used inour study, we cannot directly compare with someof the existing methods.
For our future work, weplan to manually annotate coreference informationso that we can compare with more methods.Finally, we hope to exploit some additional knowl-edge resources, such as HowNet, which could1270potentially further improve the performance of ourproposed method.AcknowledgmentsWe would like to thank anonymous reviewers andthe mentor Jacob Eisenstein for their valuablecomments and suggestions, and Xiaoli Li for help-ing us polish the paper.
This work was supportedby the National Natural Science Foundation ofChina (No.61373082, 61432011, U1435212), Na-tional 863 Project of China(No.2015AA015407),Shanxi Platform Project(2014091004-0103) andScholarship Council(2013-015), and Open ProjectFoundation of Information Security EvaluationCenter of Civil Aviation, Civil Aviation Universityof China(No.CAAC-ISECCA-201402).ReferencesCollin F. Baker, Charles J. Fillmore, and John B.Lowe.
1998.
The berkeley framenet project.
InProceedings of COLING/ACL.Aljoscha Burchardt, Anette Frank, and ManfredPinkal.
2005.
Building text meaning representa-tions from contextually related frames?a case study.Proceedings of the 6th International Workshop onComputational Semantics (IWCS-6), pages 66?77.Desai Chen, Nathan Schneider, Dipanjan Das, andNoah A Smith.
2010.
Semafor: Frame argumentresolution with log-linear models.
In Proceedingsof the 5th international workshop on semanticevaluation, pages 264?267.Dipanjan Das, Nathan Schneider, Desai Chen, andNoah A Smith.
2010.
Probabilistic frame-semanticparsing.
In Human language technologies: The2010 annual conference of the North Americanchapter of the association for computational linguis-tics, pages 948?956.Charles J. Fillmore.
1982.
Frame semantics.Linguistics in the morning calm, pages 111?137.Matthew Gerber and Joyce Y. Chai.
2010.
Beyondnombank: a study of implicit arguments for nominalpredicates.
In Proceedings of the 48th AnnualMeeting of the Association for ComputationalLinguistics, pages 1583?1592.Matthew Steven Gerber.
2011.
Semantic Role Label-ing of Implicit Arguments for Nominal Predicates.Ph.D.
thesis, Michigan State University.Philip Gorinski, Josef Ruppenhofer, and CarolineSporleder.
2013.
Towards weakly supervisedresolution of null instantiations.
In Proceedings ofthe 10th International Conference on ComputationalSemantics (IWCS 2013)?Long Papers, pages 119?130.Egoitz Laparra and German Rigau.
2013.
Sourcesof evidence for implicit argument resolution.
InProceedings of the 10th International Conferenceon Computational Semantics (IWCS 2013)?LongPapers, pages 155?166.Zhangzhang Lei, Ning Wang, Ru Li, and ZhiqiangWang.
2013.
Definite null instantiation recognizingin framenet.
Journal of Chinese Information,27(3):107?112.Ru Li.
2012.
Research on Frame Semantic StructureAnalysis Technology for Chinese Sentences.
Ph.D.thesis, ShanXi university.Hugo Liu and Push Singh.
2004.
Conceptnet:a practical commonsense reasoning tool-kit.
BTtechnology journal, 22(4):211?226.Kaiying Liu.
2011.
Research on chinese framenetconstruction and application technologies.
Journalof Chinese Information Processing, 6:006.Josef Ruppenhofer, Caroline Sporleder, RoserMorante, Collin Baker, and Martha Palmer.
2010.Semeval-2010 task 10: Linking events and theirparticipants in discourse.
In Proceedings of the 5thInternational Workshop on Semantic Evaluation,pages 45?50.Carina Silberer and Anette Frank.
2012.
Castingimplicit role linking as an anaphora resolution task.In Proceedings of the First Joint Conference onLexical and Computational Semantics-Volume 1:Proceedings of the main conference and the sharedtask, and Volume 2: Proceedings of the SixthInternational Workshop on Semantic Evaluation,pages 1?10.Sara Tonelli and Rodolfo Delmonte.
2010.
Venses++:Adapting a deep semantic processing system to theidentification of null instantiations.
In Proceedingsof the 5th international workshop on semanticevaluation, pages 296?299.Sara Tonelli and Rodolfo Delmonte.
2011.
Desperate-ly seeking implicit arguments in text.
In Proceed-ings of the ACL 2011 workshop on relational modelsof semantics, pages 54?62.Ning Wang, Ru Li, Zhangzhang Lei, Zhiqiang Wang,and Jingpan Jin.
2013.
Document oriented gapfilling of definite null instantiation in framenet.In Chinese Computational Linguistics and NaturalLanguage Processing Based on Naturally AnnotatedBig Data, pages 85?96.Le Zhang.
2005.
Maximum entropy modelingtoolkit for python and c++.
http://homepages.inf.ed.ac.uk/lzhang10/maxent_toolkit.html.1271
