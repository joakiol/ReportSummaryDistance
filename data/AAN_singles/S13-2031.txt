Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 171?177, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsHLTDI: CL-WSD Using Markov Random Fields for SemEval-2013 Task 10Alex Rudnick, Can Liu and Michael GasserIndiana University, School of Informatics and Computing{alexr,liucan,gasser}@indiana.eduAbstractWe present our entries for the SemEval-2013 cross-language word-sense disambigua-tion task (Lefever and Hoste, 2013).
Wesubmitted three systems based on classifierstrained on local context features, with someelaborations.
Our three systems, in increasingorder of complexity, were: maximum entropyclassifiers trained to predict the desired target-language phrase using only monolingual fea-tures (we called this system L1); similar clas-sifiers, but with the desired target-languagephrase for the other four languages as features(L2); and lastly, networks of five classifiers,over which we do loopy belief propagation tosolve the classification tasks jointly (MRF).1 IntroductionIn the cross-language word-sense disambiguation(CL-WSD) task, given an instance of an ambigu-ous word used in a context, we want to predict theappropriate translation into some target language.This setting for WSD has an immediate applicationin machine translation, since many words have mul-tiple possible translations.
Framing the resolution oflexical ambiguities as an explicit classification taskhas a long history, and was considered in early SMTwork at IBM (Brown et al 1991).
More recently,Carpuat and Wu have shown how to use CL-WSDtechniques to improve modern phrase-based SMTsystems (Carpuat and Wu, 2007), even though thelanguage model and phrase-tables of these systemsmitigate the problem of lexical ambiguities some-what.In the SemEval-2013 CL-WSD shared task(Lefever and Hoste, 2013), entrants are asked tobuild a system that can provide translations fortwenty ambiguous English nouns, given appropri-ate contexts ?
here the particular usage of the am-biguous noun is called the target word.
The five tar-get languages of the shared task are Spanish, Dutch,German, Italian and French.
In the evaluation, foreach of the twenty ambiguous nouns, systems are toprovide translations for the target word in each offifty sentences or short passages.
The translationsof each English word may be single words or shortphrases in the target language, but in either case,they should be lemmatized.Following the work of Lefever and Hoste (2011),we wanted to make use of multiple bitext corporafor the CL-WSD task.
ParaSense, the system ofLefever and Hoste, takes into account evidence fromall of the available parallel corpora.
Let S be the setof five target languages and t be the particular targetlanguage of interest at the moment; ParaSense cre-ates bag-of-words features from the translations ofthe target sentence into the languages S?{t}.
Givencorpora that are parallel over many languages, thisis straightforward at training time.
However, at test-ing time it requires a complete MT system for eachof the four other languages, which is computation-ally prohibitive.
Thus in our work, we learn fromseveral parallel corpora but require neither a locallyrunning MT system nor access to an online transla-tion API.We presented three systems in this shared task,all of which were variations on the theme of a max-imum entropy classifier for each ambiguous noun,trained on local context features similar to thoseused in previous work and familiar from the WSDliterature.
The first system, L1 (?layer one?
), usesmaximum entropy classifiers trained on local con-171text features.
The second system, L2 (?layer two?
),is the same as the L1 system, with the additionof the correct translations into the other target lan-guages as features, which at testing time are pre-dicted with L1 classifiers.
The third system, MRF(?Markov random field?)
uses a network of inter-acting classifiers to solve the classification problemfor all five target languages jointly.
Our three sys-tems are all trained from the same data, which weextracted from the Europarl Intersection corpus pro-vided by the shared task organizers.At the time of the evaluation, our simplest sys-tem had the top results in the shared task for theout-of-five evaluation for three languages (Spanish,German, and Italian).
However, after the evaluationdeadline, we fixed a simple bug in our MRF code,and the MRF system then achieved even better re-sults for the oof evaluation.
For the best evaluation,our two more sophisticated systems posted better re-sults than the L1 version.
All of our systems beat the?most-frequent sense?
baseline in every case.In the following sections, we will describe ourthree systems1, our training data extraction process,the results on the shared task, and conclusions andfuture work.2 L1The ?layer one?
classifier, L1, is a maximum en-tropy classifier that uses only monolingual featuresfrom English.
Although this shared task is describedas unsupervised, the L1 classifiers are trained withsupervised learning on instances that we extract pro-grammatically from the Europarl Intersection cor-pus; we describe the preprocessing and training dataextraction in Section 5.Having extracted the relevant training sentencesfrom the aligned bitext for each of the five lan-guage pairs, we created training instances with localcontext features commonly used in WSD systems.These are described in Figure 1.
Each instance isassigned the lemma of the translation that was ex-tracted from the training sentence as its label.We trained one L1 classifier for each target lan-guage and each word of interest, resulting in 20?5 =1Source is available athttp://github.iu.edu/alexr/semeval2013?
target word features?
literal word form?
POS tag?
lemma?
window unigram features (within 3 words)?
word form?
POS tag?
word with POS tag?
word lemma?
window bigram features (within 5 words)?
bigrams?
bigrams with POS tagsFigure 1: Features used in our classifiers100 classifiers.
Classifiers were trained with theMEGA Model optimization package 2 and its corre-sponding NLTK interface (Bird et al 2009).
Upontraining, we cache these classifiers with Pythonpickles, both to speed up L1 experiments and alsobecause they are used as components of the othermodels.We combined the word tokens with their tagsin some features so that the classifier would nottreat them independently, since maximum entropyclassifiers learn a single weight for each feature.Particularly, the ?POS tag?
feature is distinct fromthe ?word with tag?
feature; for the tagged word?house/NN?, the ?POS tag?
feature would be NN ,and the ?word with tag?
feature is house NN .3 L2The ?layer two?
classifier, L2, is an extension tothe L1 approach, with the addition of multilingualfeatures.
Particularly, L2 makes use of the trans-lations of the target word into the four target lan-guages other than the one we are currently trying topredict.
At training time, since we have the transla-tions of each of the English sentences into the othertarget languages, the appropriate features are ex-tracted from the corresponding sentences in thoselanguages.
This is the same as the process by whichlabels are given to training instances, described inSection 5.
At testing time, since translations of the2http://www.umiacs.umd.edu/?hal/megam/172es denlfr itFigure 2: The network structure used in the MRFsystem: a complete graph with five nodes whereeach node represents a variable for the translationinto a target languagetest sentences are not given, we estimate the transla-tions for the target word in the four other languagesusing the cached L1 classifiers.Lefever and Hoste (2011) used the Google Trans-late API to translate the source English sentencesinto the four other languages, and extracted bag-of-words features from these complete sentences.
TheL2 classifiers make use of a similar intuition, butthey do not rely on a complete MT system or anavailable online MT API; we only include the trans-lations of the specific target word as features.4 MRFOur MRF model builds a Markov network (oftencalled a ?Markov random field?)
of L1 classifiersin an effort to find the best translation into all fivetarget languages jointly.
This network has nodesthat correspond to the distributions produced by theL1 classifiers, given an input sentence, and edgeswith pairwise potentials that are derived from thejoint probabilities of target-language labels occur-ring together in the training data.
Thus the task offinding the optimal translations into five languagesjointly is framed as a MAP (Maximum A Posteriori)inference problem, where we try to maximize thejoint probability P (wfr, wes, wit, wde, wnl), giventhe evidence of the features extracted from thesource-language sentence.
The inference process isperformed using loopy belief propagation (Murphyet al 1999), which is an approximate but tractableinference algorithm that, while it gives no guaran-tees, often produces good solutions in practice.The intuition behind using a Markov network forthis task is that, since we must make five decisionsfor each source-language sentence, we should makeuse of the correlations between the target-languagewords.
Correlations might occur in practice due tocognates ?
the languages in the shared task are fairlyclosely related ?
or they may simply reflect ambigu-ities in the source language that are resolved in twotarget languages.So by building a Markov network in which all ofthe classifiers can communicate (see Figure 2), weallow nodes to influence the translation decisions oftheir neighbors, but only proportionally to the cor-relation between the translations that we observe inthe two languages.We frame the MAP inference task as a minimiza-tion problem; we want to find an assignment thatminimizes the sum of all of our penalty functions,which we will describe next.
First, we have a unaryfunction from each of the five L1 classifiers, whichcorrespond to nodes in the network.
These func-tions each assign a penalty to each possible label forthe target word in the corresponding language; thatpenalty is simply the negative log of the probabilityof the label, as estimated by the classifier.Formally, a unary potential ?i, for some fixed setof features f and a particular language i, is a func-tion from a label l to some positive penalty value.
?i(l) = ?logP (Li = l|F = f)Secondly, for each unordered pair of classifiers(i, j) (i.e., each edge in the graph) there is a pairwisepotential function ?
(i,j) that assigns a penalty to anyassignment of that pair of variables.?
(i,j)(li, lj) = ?logP (Li = li, Lj = lj)Here by P (Li = li, Lj = lj), we mean the prob-ability that, for a fixed ambiguous input word, lan-guage i takes the label li and language j takes thelabel lj .
These joint probabilities are estimated fromthe training data; we count the number of timeseach pair of labels li and lj co-occurs in the train-173ing sentences and divide, with smoothing to avoidzero probabilities and thus infinite penalties.When it comes time to choose translations, wewant to find a complete assignment to the five vari-ables that minimizes the sum of all of the penal-ties assigned by the ?
functions.
As mentioned ear-lier, we do this via loopy belief propagation, usingthe formulation for pairwise Markov networks thatpasses messages directly between the nodes ratherthan first constructing a cluster graph (Koller andFriedman, 2009, ?11.3.5.1).As we are trying to compute the minimum-penalty assignment to the five variables, we use themin-sum version of loopy belief propagation.
Themessages are mappings from the possible valuesthat the recipient node could take to penalty values.At each time step, every node passes to each ofits neighbors a message of the following form:?ti?j(Lj) = minli?Li[?i(li) + ?
(i,j)(li, lj)+?k?S?
{i,j}?t?1k?i(li)]By this expression, we mean that the messagefrom node i to node j at time t is a function frompossible labels for node j to scalar penalty values.Each penalty value is determined by minimizingover the possible labels for node i, such that we findthe label li that minimizes sum of the unary cost forthat label, the binary cost for li and lj taken jointly,and all of the penalties in the messages that node ireceived at the previous time step, except for the onefrom node j.Intuitively, these messages inform a given neigh-bor about the estimate, from the perspective of thesending node and what it has heard from its otherneighbors, of the minimum penalty that would beincurred if the recipient node were to take a givenlabel.
As a concrete example, when the nl nodesends a message to the fr node at time step 10, thismessage is a table mapping from all possible Frenchtranslations of the current target word to their as-sociated penalty values.
The message depends onthree things: the function ?nl (itself dependent onthe probability distribution output by the L1 classi-fier), the binary potential function ?
(nl,fr), and themessages from es, it and de from time step 9.
Notethat the binary potential functions are symmetric be-cause they are derived from joint probabilities.Loopy belief propagation is an approximate infer-ence algorithm, and it is neither guaranteed to finda globally optimal solution, nor even to convergeat all, but it does often find good solutions in prac-tice.
We run it for twenty iterations, which empir-ically works well.
After the message-passing iter-ations, each node chooses the value that minimizesthe sum of the penalties from messages and from itsown unary potential function.
To avoid accumulat-ing very large penalties, we normalize the outgoingmessages at each time step and give a larger weightto the unary potential functions.
These normaliza-tion and weighting parameters were set by hand, butseem to work well in practice.5 Training Data ExtractionFor simplicity and comparability with previouswork, we worked with the Europarl Intersectioncorpus provided by the task organizers.
Europarl(Koehn, 2005) is a parallel corpus of proceedings ofthe European Parliament, currently available in 21European languages, although not every sentence istranslated into every language.
The Europarl Inter-section is the intersection of the sentences from Eu-roparl that are available in English and all five of thetarget languages for the task.In order to produce the training data for the classi-fiers, we first tokenized the text for all six languageswith the default NLTK tokenizer and tagged the En-glish text with the Stanford Tagger (Toutanova etal., 2003).
We aligned the untagged English witheach of the target languages using the BerkeleyAligner (DeNero and Klein, 2007) to get one-to-many alignments from English to target-languagewords, since the target-language labels may bemulti-word phrases.
We used nearly the default set-tings for Berkeley Aligner, except that we ran 20iterations each of IBM Model 1 and HMM align-ment.We used TreeTagger (Schmid, 1995) to lemma-tize the text.
At first this caused some confusion inour pipeline, as TreeTagger by default re-tokenizesinput text and tries to recognize multi-word expres-174sions.
Both of these, while sensible behaviors, wereunexpected, and resulted in a surprising number oftokens in the TreeTagger output.
Once we turned offthese behaviors, TreeTagger provided useful lem-mas for all of the languages.Given the tokenized and aligned sentences, withtheir part-of-speech tags and lemmas, we useda number of heuristics to extract the appropriatetarget-language labels for each English-language in-put sentence.
For each target word, we extracted asense inventory Vi from the gold standard answersfrom the 2010 iteration of this task (Lefever andHoste, 2009).
Then, for each English sentence thatcontains one of the target words used as a noun,we examine the alignments to determine whetherthat word is aligned with a sense present in Vi , orwhether the words aligned to that noun are a sub-sequence of such a sense.
The same check is per-formed both on the lemmatized and unlemmatizedversions of the target-language sentence.
If we dofind a match, then that sense from the gold stan-dard Vi is taken to be the label for this sentence.While a gold standard sense inventory will clearlynot be present for general translation systems, therewill be some vocabulary of possible translations foreach word, taken from a bilingual dictionary or thephrase table in a phrase-based SMT system.If a label from Vi is not found with the align-ments, but some other word or phrase is alignedwith the ambiguous noun, then we trust the outputof the aligner, and the lemmatized version of thistarget-language phrase is assigned as the label forthis sentence.
In this case we used some heuristicfunctions to remove stray punctuation and attachedarticles (such as d?
from French or nell?
from Ital-ian) that were often left appended to the tokens bythe default NLTK English tokenizer.We dropped all of the training instances withlabels that only occurred once, considering themlikely alignment errors or other noise.6 ResultsThere were two settings for the evaluation, best andoof.
In either case, systems may present multiplepossible answers for a given translation, althoughin the best setting, the first answer is given moreweight in the evaluation, and the scoring encour-ages only returning the top answer.
In the oof set-ting, systems are asked to return the top-five mostlikely translations.
In both settings, the answers arecompared against translations provided by severalhuman annotators for each test sentence, who pro-vided a number of possible target-language transla-tions in lemmatized form, and more points are givenfor matching the more popular translations given bythe annotators.
In the ?mode?
variant of scoring,only the one most common answer for a given testsentence is considered valid.
For a complete ex-planation of the evaluation and its scoring, pleasesee the shared task description (Lefever and Hoste,2013).The scores for our systems3 are reported in Figure3.
In all of the settings, our systems posted some ofthe top results among entrants in the shared task,achieving the best scores for some evaluations andsome languages.
For every setting and language,our systems beat the most-frequent sense baseline,and our best results usually came from either the L2or MRF system, which suggests that there is somebenefit in using multilingual information from theparallel corpora, even without translating the wholesource sentence.For the best evaluation, considering only themode gold-standard answers, our L2 systemachieved the highest scores in the competition forSpanish and German.
For the oof evaluation, ourMRF system ?
with its post-competition bug fix ?posted the best results for Spanish, German and Ital-ian in both complete and mode variants.
Also, cu-riously, our L1 system posted the best results in thecompetition for Dutch in the oof variant.For the best evaluation, our results were lowerthan those posted by ParaSense, and in the stan-dard best setting, they were also lower than thosefrom the c1lN system (van Gompel and van denBosch, 2013) and adapt1 (Carpuat, 2013).
This,combined with the relatively small difference be-tween our simplest system and the more sophisti-cated ones, suggests that there are many improve-ments that could be made to our system; perhaps3The oof scores for the MRF system reflect a small bug fixafter the competition.175system es nl de it frMFS 23.23 20.66 17.43 20.21 25.74best 32.16 23.61 20.82 25.66 30.11PS 31.72 25.29 24.54 28.15 31.21L1 29.01 21.53 19.5 24.52 27.01L2 28.49 22.36 19.92 23.94 28.23MRF 29.36 21.61 19.76 24.62 27.46(a) best evaluation results: precisionsystem es nl de it frMFS 53.07 43.59 38.86 42.63 51.36best 62.21 47.83 44.02 53.98 59.80L1 61.69 46.55 43.66 53.57 57.76L2 59.51 46.36 42.32 53.05 58.20MRF 62.21 46.63 44.02 53.98 57.83(b) oof evaluation results: precisionsystem es nl de it frMFS 27.48 24.15 15.30 19.88 20.19best 37.11 27.96 24.74 31.61 26.62PS 40.26 30.29 25.48 30.11 26.33L1 36.32 25.39 24.16 26.52 21.24L2 37.11 25.34 24.74 26.65 21.07MRF 36.57 25.72 24.01 26.26 21.24(c) best evaluation results: mode precisionsystem es nl de it frMFS 57.35 41.97 44.35 41.69 47.42best 65.10 47.34 53.75 57.50 57.57L1 64.65 47.34 53.50 56.61 51.96L2 62.52 44.06 49.03 54.06 53.57MRF 65.10 47.29 53.75 57.50 52.14(d) oof evaluation results: mode precisionFigure 3: Task results for our systems.
Scores in bold are the best result for that language and evaluationout of our systems, and those in bold italics are the best posted in the competition.
For comparison, wealso give scores for the most-frequent-sense baseline (?MFS?
), ParaSense (?PS?
), the system developed byLefever and Hoste, and the best posted score for competing systems this year (?best?
).we could integrate ideas from the other entries inthe shared task this year.7 Conclusions and future workOur systems had a strong showing in the compe-tition, always beating the MFS baseline, achiev-ing the top score for three of the five languages inthe oof evaluation, and for two languages in thebest evaluation when considering the mode gold-standard answers.
The systems that took into ac-count evidence from multiple sources had betterperformance than the one using monolingual fea-tures: our top result in every language came fromeither the L2 or the MRF classifier for both eval-uations.
This suggests that it is possible to makeuse of the evidence in several parallel corpora in aCL-WSD task without translating every word in asource sentence into many target languages.We expect that the L2 classifier could be im-proved by adding features derived from more classi-fiers and making use of information from many dis-parate sources.
We would like to try adding classi-fiers trained on the other Europarl languages, as wellas completely different corpora.
The L2 classifierapproach only requires that the first-layer classifiersmake some prediction based on text in the sourcelanguage.
They need not be trained from the samesource text, depend on the same features, or evenoutput words as labels.
In future work we will ex-plore all of these variations.
One could, for exam-ple, train a monolingual WSD system on a sense-tagged corpus and use this as an additional informa-tion source for an L2 classifier.There remain a number of avenues that we wouldlike to explore for the MRF system; thus far, wehave used the joint probability of two labels to setthe binary potentials.
We would like to investigateother functions, especially ones that do not incurlarge penalties for rare labels, as the joint probabil-ity of two labels that often co-occur but are both rarewill be low.
Also, in the current system, the relativeweights of the binary potentials and the unary po-tentials were set by hand, with a very small amountof empirical tuning.
We could, in the future, tune the176weights with a more principled optimization strat-egy, using a development set.As with the L2 classifiers, it would be helpful inthe future for the MRF system to not require manymutually parallel corpora for training ?
however, thecurrent approach for estimating the edge potentialsrequires the use of bitext for each edge in the net-work.
Perhaps these correlations could be estimatedin a semi-supervised way, with high-confidence au-tomatic labels being used to estimate the joint dis-tribution over target-language phrases.
We wouldalso like to investigate approaches to jointly disam-biguate many words in the same sentence, since lex-ical ambiguity is not just a problem for a few nouns.Aside from improvements to the design of ourCL-WSD system itself, we want to use it in a practi-cal system for translating into under-resourced lan-guages.
We are now working on integrating thisproject with our rule-based MT system, L3 (Gasser,2012).
We had experimented with a similar, thoughless sophisticated, CL-WSD system for Quechua(Rudnick, 2011), but in the future, L3 with the inte-grated CL-WSD system should be capable of trans-lating Spanish to Guarani, either as a standalonesystem, or as part of a computer-assisted translationtool.ReferencesSteven Bird, Ewan Klein, and Edward Loper.
2009.
Nat-ural Language Processing with Python.
O?Reilly Me-dia.Peter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1991.
Word-Sense Dis-ambiguation Using Statistical Methods.
In Proceed-ings of the 29th Annual Meeting of the Association forComputational Linguistics, pages 264?270.Marine Carpuat and Dekai Wu.
2007.
How PhraseSense Disambiguation Outperforms Word Sense Dis-ambiguation for Statistical Machine Translation.
In11th Conference on Theoretical and MethodologicalIssues in Machine Translation.Marine Carpuat.
2013.
NRC: A Machine TranslationApproach to Cross-Lingual Word Sense Disambigua-tion (SemEval-2013 Task 10).
In Proceedings of the7th International Workshop on Semantic Evaluation(SemEval 2013), Atlanta, USA.John DeNero and Dan Klein.
2007.
Tailoring WordAlignments to Syntactic Machine Translation.
In Pro-ceedings of the 45th Annual Meeting of the Asso-ciation of Computational Linguistics, pages 17?24,Prague, Czech Republic, June.
Association for Com-putational Linguistics.Michael Gasser.
2012.
Toward a Rule-Based Sys-tem for English-Amharic Translation.
In LREC-2012:SALTMIL-AfLaT Workshop on Language technologyfor normalisation of less-resourced languages.Philipp Koehn.
2005.
Europarl: A Parallel Corpus forStatistical Machine Translation.
In Proceedings ofThe Tenth Machine Translation Summit, Phuket, Thai-land.D.
Koller and N. Friedman.
2009.
Probabilistic Graphi-cal Models: Principles and Techniques.
MIT Press.Els Lefever and Ve?ronique Hoste.
2009.
SemEval-2010Task 3: Cross-lingual Word Sense Disambiguation.In Proceedings of the Workshop on Semantic Evalu-ations: Recent Achievements and Future Directions(SEW-2009), pages 82?87, Boulder, Colorado, June.Association for Computational Linguistics.Els Lefever and Ve?ronique Hoste.
2013.
SemEval-2013Task 10: Cross-Lingual Word Sense Disambiguation.In Proceedings of the 7th International Workshop onSemantic Evaluation (SemEval 2013), Atlanta, USA.Els Lefever, Ve?ronique Hoste, and Martine De Cock.2011.
ParaSense or How to Use Parallel Corpora forWord Sense Disambiguation.
In Proceedings of the49th Annual Meeting of the Association for Compu-tational Linguistics: Human Language Technologies,pages 317?322, Portland, Oregon, USA, June.
Asso-ciation for Computational Linguistics.Kevin P. Murphy, Yair Weiss, and Michael I. Jordan.1999.
Loopy Belief Propagation for Approximate In-ference: An Empirical Study.
In UAI ?99: Proceed-ings of the Fifteenth Conference on Uncertainty in Ar-tificial Intelligence, Stockholm, Sweden.Alex Rudnick.
2011.
Towards Cross-Language WordSense Disambiguation for Quechua.
In Proceedingsof the Second Student Research Workshop associatedwith RANLP 2011, pages 133?138, Hissar, Bulgaria,September.
RANLP 2011 Organising Committee.Helmut Schmid.
1995.
Improvements In Part-of-SpeechTagging With an Application To German.
In Proceed-ings of the ACL SIGDAT-Workshop, pages 47?50.Kristina Toutanova, Dan Klein, Christopher D. Man-ning, and Yoram Singer.
2003.
Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network.In PROCEEDINGS OF HLT-NAACL, pages 252?259.Maarten van Gompel and Antal van den Bosch.
2013.WSD2: Parameter optimisation for Memory-basedCross-Lingual Word-Sense Disambiguation.
In Pro-ceedings of the 7th International Workshop on Seman-tic Evaluation (SemEval 2013), Atlanta, USA.177
