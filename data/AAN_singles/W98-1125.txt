Discourse Parsing: A Decision Tree ApproachTadash i  NomotoAdvanced Research  LaboratoryH i tach i  L td .2520 Hatoyama Sa i tama,350-0395 Japannomoto@harl, hitachi, co .
jpYu j i  MatsumotoNara  Ins t i tu te  of Sc ience and  Techno logy8916-5 Takayarna  Ikoma Nara ,630-0101 Japanmatsu@is, aist-nara, ac.
jpAbst rac tThe paper presents a new statistical method, forparsing discourse.
A parse of discourse is definedas a set of semantic dependencies among sentencesthat make up the discourse.
A collection of newsarticles from a Japanese economics daily are man-ually marked for dependency and used as a train-ing/testing corpus.
We use a C4.5 decision treemethod to develop a model of sentential dependen-cies.
However, rather than to use class decisionsmade by C4.5, we exploit information on class dis-tributions to rank possible dependencies among sen-tences according to their probabilistic strength andtake a parse to be a set of highest ranking dependen-cies.
We also study effects of features uch as cluewords, distance and similarity on the performance ofthe discourse parser.
Experiments have found thatthe method performs reasonably well on diverse texttypes, scoring an accuracy rate of over 60%.1 In t roduct ionAttempts to the automatic identification of a struc-ture in discourse have so far met with a limitedsuccess in the computational linguistics literature.Part of the reason is that, compared to sizable dataresources available to parsing research such as thePenn Treebank (Marcus et al, 1993), large cor-pora annotated for discourse information are hard tocome by.
Researchers in discourse usually work witha corpus of a few hundred sentences (Kurohashi andNagao, 1994; Litman and Passonneau, 1995; Hearst,1994).
The lack of a large-scale corpus has made itimpossible to talk about results of discourse studieswith the sufficient degree of reliability.In the work described here, we created a corpuswith discourse information, containing 645 articlesfrom a Japanese conomic paper, an order of magni-tude larger than any previous work on discourse pro-cessing.
It had a total of 12.770 sentences and 5,352paragraphs.
Each article in the corpus was manuallyannotated for a discourse dependency" relation.
Wethen built a statistical discourse parser based on theC4.5 decision tree method (Quinlan, 1993), whichwas trained and tested on the corpus we have cre-Figure 1: A discourse tree.
'S' denotes a sentence.ated.
The design of a parser was inspired by Haruno(1997)'s work on statistical sentence parsing.The paper is organized as follows.
Section 2presents general ideas about statistical parsing asapplied to the discourse, After a brief introductionto some of the points of a decision tree model, we dis-cuss incorporating a decision tree within a statisticalparsing model.
In Section 3, we explain how we havebuilt an annotated corpus.
There we also describea procedure of experiments we have conducted, andconclude the section with their results.2 S ta t i s t i ca l  D iscourse  Pars ingFirst, let us make ourselves clear about what wemean by parsing a discourse.
The job of parsing isto find whatever dependencies there are among ele-ments that make up a particular linguistic unit.
Indiscourse parsing, elements one is interested in find-ing dependencies among correspond to sentences,and a level of unit under investigation is a discourse.We take a naive approach to the notion of a depen-dency here.
We think of it as a re!ationship betweena pair of sentences such that the interpretation of onesentence in some way depends on that of the other.Thus a dependency relationship is not a structuralone, but rather a semantic or rhetorical one.The job of a discourse parser is to take as input216a discourse, or a set of sentences that make up adiscourse and to produce as output a parse, or aset of dependency relations (which may give rise toa tree-like structure as in Figure 1).
In statisticalparsing, this could be formulated as a problem offinding a best parse with a model P(T I D), whereT is a set of dependencies and D a discourse.Tbest = arg maXTP(T \[ D )Tbe,t is a set of dependencies that maximizes theprobability P(T I D).
Further, we assume that adiscourse D is a set of sentences marked for somepre-defined set of features F = { f l , .
.
- , fn} .
LetCF ($1) be a characterization f sentence $1 in termsof a feature set F.  Then for D = {S1,...,Sm},CF(D)  = {CF(S1) ,CF(S2) , .
.
.
,CF(Sm)} .
Let usassume that:P(T I D) = r I  P(A ~ B \[ CF(D)).A+..-BET'A ~- B'  reads like " sentence B is dependent onsentence A", where A,B E {$i, .
.
.
,Sin}.
The prob-ability of T being an actual parse of discourse D isestimated as the product of probabilities of its ele-ment dependencies when a discourse has a represen-tation CF(D).
We make a usual assumption thatelement dependencies are probabilistically iiadepen-dent.2.1 Dec is ion Tree  Mode lA general framework for discourse parsing describedabove is thus not much different from that for sta-tistical sentence parsing.
Differences, however, lie ina makeup of the feature set F. Rather than to useinformation on word forms, word counts, and part-of-speech tags as in much research on statistical sen-tence parsing, we exploit as much information as canbe gleaned from a discourse, such as lexical cohesion,distance, location, and clue words, to characterize asentence.
Therefore it is important hat you do notend up with a mountain of irrelevant features.A decision tree method represents one of ap-proaches to classification problems, where featuresare ranked according to how much they contribute toa classification, and models are then built with fea-tures most relevant o that classification.
Suppose,for example, that you work for a travel agency andwant to find out what features of a hotel are moreimportant for tourists, based on data from your cus-tomers like Table 1.
With decision tree techniques,you would be able to tell what features are moreclosely associated with customers' preferences.The aim of the decision tree approach is to in-duce rules from data that best characterize classes.A particular approach called C4.5 (Quinlan, 1993),which we adopt here, builds rules by recursively di-viding the training data into subsets until all divi-sions contain only single class cases.
In which subsetTab le  1: An illustration: hotel preferences.
'Bath/shower' means a room has a bath, a shower ornone.
'T ime' means the travel t ime in min.
from anairport.
'Class' indicates whether a particular hotelis a customer's choice.bath/shower time room rate class1 bath 15 expensive no2 shower 20 inexpensive no3 shower 10 inexpensive yes4 bath 15 moderate yes5 bath 25 moderate yes6 none 20 inexpensive no7 shower 50 inexpensive noFigure 2: A decision tree for the hotel example.bath/shower hroom raze ive moderateNO YESI showcr~onetime NOYES NOa particular case is placed is determined by the out-come of a 'test '  on that case.
Let us explain howthis works by way of the hotel example above.
Sup-pose that the first test is "bath/shower ' ,  which hasthree outcomes, bath,  shower, and none.
Then thedata set breaks up into three groups, {1,4,5} (bath),{2,3,7} (shower), and {6}(none).
Since the lastgroup {6} consists of only a single case, there is nofurther division of the group.
The bath  group, beinga multi-class et, is further divided by a test "roomrate", which produces two subdivisions, one with { 1 }(expensive), and the other with {4,5} (moderate).Either set now consists of only single class cases.For the shower group, applying the time test(<=15)would produce two subsets, one with {3}, and theother with {2,7}.
l Either one now contains casesfrom a single class.
A decision tree for divisions wemade is shown in Figure 2.Now compare a hand-created decision tree in Fig-i Here we choose a midpoint between I0 and 20 as in C4.5.217Figure 3: A tree for the hotel example by C4.5.Figures in parentheses indicate the number of casesthat reach relevant nodes.
A figure after a slash, eg.
(4/1), indicates the number of misclassified cases.room rale (7)NO (1) YES (2) NO (4/I)ure 2 with one in Figure 3, which is generated byC4.5 for the same data.
Surprisingly, the latter treeconsists of only one test node.
This happens becauseC4.5 ranks possible tests, which we did not, and ap-ply one that gives a most effective partitioning ofdata based on information-theoretic criteria knownas the gain criterion and the gain ratio criterion.
2The intuitive idea behind the criteria is to prefer atest with a least entropy, i.e., a test that partitionsdata in such a way that a particular class may be-come dominant for each subset it creates.
Thus afeature that best accounts for a class distribution indata is always chosen in preference to others.
Forthe data in Table 1, C4.5 determined that the testroom ra te  is a best class identifier and everything2 The  gain criterion measures the effectiveness of part i t ion-ing a data  set T with respect to a test X ,  and is definedfollows.gain(X) = info(T) - in /ox(T  )Define info(T) to be an entropy of T,  that  is, the averageamount  of information generated by T.  Then  we have:kin/o(T) = - ~-~ freq.
(Cj, T)  x log 2 freq(Cj' T)ITI IVlj= lfreq(C,T) is the number  of cases from a class C divided bythe sum of cases in T. Now infox(T ) is the average amountof information generated by part i t ioning T with respect to atest X.
That  is,in/ox(T) = ~ 1~ \[ info(T,)i=1Thus  a good classifier would give a small  value for info X (T)and a large value for in/o X (T).The gain ratio criterion is a modif ication to the gain crite-rion.
It has the effect of mak ing a spl i tt ing of a data  set lessintense.gain ratio(X) = gain(X)/split info~')where:split info(X) = - ~ IT, \[ x " \[ Ti II T \[ rag2 I T I*-=lThe  ratio decreases with an increase in the number  of splits.else is irrelevant o identifying the classes.
All thatone needs to account for the class distribution in Ta-ble 1 turn out to be just one feature.
So we mightjust as well conclude that the customers are just in-terested in the room charge when they pick up ahotel.A benefit of using the decision tree method is thatit enables us to identify relevant features for classi-fication and disregard those that are not relevant,which is particularly useful for a task such as ours,where a large number of features are potentially in-volved and their relevance to classification is not al-ways known.2.2 Pars ing  w i th  Dec is ion  TreeAs we mentioned in section 2, we define discourseparsing as a task of finding a best tree T, or a set ofdependencies among sentences that maximizes P(T ID).Tbest = arg maxTP(T  \[ D)F(T  \[ D) = H P(A ~ B \[ CF(D)).A+--BETWhat we do now is to equip the model with a fea-ture selection functionality.
This can be done byassuming:P(A ~ B \[ CF(D)) = P(A ~ B \[ CF(D) ,DTF)Z P(X  ~- B \[ CF(D),DTF)X<B(1)DTF is a decision tree constructed with a featureset F by C4.5.
'X  < B'  means that X is a sen-tence that precedes B:  P (X  ~ Y \[ CF(D) ,DTv)is the probability that sentence Y depends on sen-' tence X under the condition that both CF(D) andDTF are used.
We estimate P,  using class distribu-tions from the decision tree DTF .
For example, wehave numbers in parentheses after leaves in the de-cision tree in Figure 3.
They indicate the number ofcases that reach a particular leaf and also the num-ber of misclassified cases.
Thus a leaf with the labelinexpens ive  has the total of 4 cases, one of whichis misclassified.
This means that we have 3 casescorrectly classified as "NO" and one case wronglyclassified.
Thus a class distribution for "NO" is 3/4and that for "YES" is 1/4.
In practice, however,we slightly correct class frequencies, using Laplace'srule of succession, i.e., x/n ~ x + 1/n + 2.Now suppose that we have a discourse D ={.
.
.
,S i , .
.
.
,S j , .
.
.
,Sk , .
.
.}
and want to know whatSi depends on, assuming that Si depends on eitherSj or Sk.
To find that out involves constructing3 Note that  here we are in effect mak ing a claim aboutthe st ructure of a discourse, namely  that  a sentence modifiesone that  precedes it.
Changing it to someth ing like 'X  6D ,X  # B '  allows one to have forward as well as backwarddependencies.218Figure 4: A hypothetical decision tree.distYES (10/3) YES (14/8)CF(D)  and DTF .
Let us represent sentences Sj andSk in terms of how far they are separated from Si,measured in sentences.
Suppose that dist(Sj) = 2and dist(Sj) = 4; that is, sentence S.# appears 2 sen-tences behind Si and Sk 4 sentences behind.
Assumefurther that we have a decision tree constructed fromdata elsewhere that looks like Figure 4.With CF(D)  and DTF  at hand, we are now ina position to find P(A  ~ B I CF(D)) ,  for eachpossible dependency Sj ~ Si, and Sk +-- Si.P(S j  ~ Si l Ca i , t (D) ,DTai , t )= (10-  3 + 1)/(10 + 2).= .67P(Sk +- Si \[ Caist(D), DTaist)= (14 - s + 1) / (14  + 2)= .44Since Si links with either Sj or Sk, by Equation 1,we normalize the probability estimates o that theysum to 1.P(S  i ~ Si I Cd , t (D) )  = .67/(.67 + .44) = .60P(.-~ ~ Si \[ Caist(D)) = .44/(.67 + .44) = .40Recall that class frequencies are corrected byLaplace's rule.
Let T 1 = {Sj ~ Si} and Tk = {,-~Si} Then P(Tj I D) > P(Tk t D).
Thus Tb,,t = Tj.We conclude that Si is more likely to depend on Sjthan SI,.2.2.1 FeaturesThe following list a set of features we used to encodea discourse.
As a convention, we refer to a sentencefor which we like to find a dependency as 'B', and asentence preceding 'B' as 'A'.<DistSen> records information on how far aheadA appears from B, measured in sentences.#S(B)  - #S(A)3InT..Sen_Distance'#S(X) '  denotes an ordinal number indicatingthe position of a sentence X in a text, i.e.,#S(kth_sentence) = k. 'Max_Sen_Distance' de-notes a distance, measured in sentences, fromB to A, when B ocurrs farthest from A, i.e.,#S(last_sentence_in_text) - 1.
DistSen thus hascontinuous values between 0 and 1.
We discard textswhich contain no more than one sentence.<DistPax> is defined similarly to DistSen, exceptthat the distance is measured in paragraphs.#Par (B)  - #Par (A)Max_Par .D is tance'Par (X) '  is a paragraph that contains a sentenceX, and '#Par (X)?
denotes an ordinal number ofPar (X) .
'Max_Par_Distance' is a maximal distanceone could have between two paragraphs in a text,that is, #Par(last_sentence.in_text) - 1.<LocSen> defines the location of a sentence by:#s(x)# S ( Last_S en tence )Here 'Last_Sentence' is the last sentence of a text.LocSen takes values between 0 and 1.
A discourse-initial sentence takes 0, and a discourse-final sen-tence 1.<LocPax> is defined similarly to DistPa.r.
It givesinformation on the location of a paragraph in whicha sentence X occurs.#Par (X)#Last._Paragraph'#Last_Paragraph'  is the position of the last para-graph in a text, represented by its ordinal number.?
<LocW?thinPar> gives information on the locationof a sentence X within a paragraph in which it ap-pears.#S(X)  - #S(Par_ ln iL .Sen)Length(Par (X) )'ParAnit_Sen' refers to the initial sentence of a para-graph in which X occurs, 'Length(Par(X)) '  denotesthe number of sentences that occur in that para-graph.
LocWithinPar takes continuous values rang-ing from 0 to 1.
A paragraph initial sentence wouldhave 0 and a paragraph final sentence 1.<LenText> the length of a text, measured inJapanese characters.<LenSenA> the length of A in Japanese characters.<LenSenB> the length of B in Japanese characters.<Sire> gives information on the lexical similaritybetween A and B, based on an information-retrievalmeasure known as t f  ?
idf.
4 One important point4 For a word j E Si, its weight u'i2 is defined by:N wi.i = tf, j .
log d~j219here is  that we did not use words per se in mea-suring the similarity.
What we did was to breakup nominals from sentences into simple characters(grapheme) and use only them to measure the sim-ilarity.
We did this to deal with abbreviations andrewordings, which we found quite frequent in thecorpus we used.<Sire2> same as Sire feature, except that the sim-ilarity is measured between A and Par(B), a para-graph in which B occurs.
We define Siva2 as'SIM(A,Concat(Par(B)))' (see footnote 4 for thedefinition of SIM), where 'Concat(Par(B))' is aconcatenation of sentences in Par(B).<IsATit\].e> indicates whether A is a title.
We re-garded a title as a special sentence that initiates adiscourse.<Clues> differs from features above in that it doesnot refer to any single feature but is a collective termfor a set of clue-related features, each of which isused to indicate the presence or absence of a rele-vant clue in A and B.
We examined N most frequentwords found in a corpus and associated each with adifferent clue feature.
We experimented with caseswhere N is 0, 100, 500 and 1000.
A sentence canbe marked for a multiple number of clue expressionsat the same time.
For a clue c, an associated Cluesfeature d takes one of the four values, dependingon the way c appears in A and B. c' = 0 if c ap-pears in neither A or B; d = 1 if c appears in bothA and B; d = 2 if c appears in A and not in B;and d = 3 if c appears not in A but in B.
We con-sider clue expressions from the following grammat-ical classes: nominals, adjectives, demonstratives,adverbs, sentence connectives, verbs, sentence-finalparticles, topic-marking particles, and punctuationmarks.
5 While we did not consider a complex clueexpression, which can be made up of multiple ele-ments from various grammatical classes 6 , it is pos-d/j is the number  of sentences in the text which have an oc-currence of a word j .
N is the total number  of sentences inthe text.
The t f .
id f  metric has  the property of favoring highfrequency words with local distr ibut ion.
For a pair of sen-fences X = {zl .
.
.
.  }
and \]" = {YI, - - .
},  where z and y arewords, we define the lexical s imilar ity between X and Y by:tS IM(X ,  Y )  = i=1w(xi)2, w(yi)2i .~ l  i= lThey- are extracted from a corpus by a Japanese tokenizerprogram (Sakurai and Hisamitsu,  1997).6 English examples would be for  example, as a result,etc., which are thought  of as an indicator of a discourserelationship.Table 2: Top 20 lexical clues.
Suushi below is agrammar term of a class of numerals.
Since thereare infinitely many of them, we decided not to treatthem individually, but to represent them collectivelywith a single feature .~uushi.l emma exp lanat ionosuushi*wasuruJfmo()nadonaiarukarakotodewanenhinocommaperiodnumeralstopic marker'do'right angular parenthesisleft angular parenthesistopic markerleft parenthesisright parenthesis'so forth'dashnegative auxiliary'exist ' , 'be'' from'nominalizertopic marker'year''day'possessive particlesible to think of a complex clue in terms of its com-ponent clues for which a sentence is marked.Classes  For a sentence pair A and B, the class iseither yes or no, corresponding to the presence orabsence of a dependency link from B to A.The features above are more or less plucked fromthe air.
Some are motivated, and some are less so.Our strategy here, however, is to rely on the deci-sion tree mechanism to select 'good' features andfilter out features that are not relevant o the classidentification.2.2.2 Notes  on  D iscourse  Encod ingLet us make further notes on how to encode a dis-course with the set of features we have described.We characterize a sentence in relation to its po-tential "modifyee" sentence, a sentence in a dis-course which it is likely to depend on.
Thus en-coding is based on a pair of sentences, rather thanon a single sentence.
For example, a discourseD = {St, $2, $3 } would give a set of possible de-pendency pairs "P(D) = {< S2,Si >,< S3, Si >,<S,,& >,< S3,S~_ >,< S~,S3 >,< S2,Ss >}.
Weassume that CF(D)  = CF(P(D)).
Furthermore, wemay want to constrain P by restricting the attentionto pairs of a particular type.
If we are interestedonly in backward dependencies, then we will have220?
(D) = {< Sl,& >,< Sl,S3 >,< S2, S3 >}.In experiments, we assumed a discourse as con-sisting of backward dependency pairs and encodedeach pair with the set of features above.
Assump-tions we made about the structure of a discourse arethe following:1.
Every sentence in a discourse has exactly onepreceding "modifyee" to link to.2.
A discourse may have crossing dependencies.3 Evaluat ion3.1 DataTo evaluate our method, we have done a set of ex-periments, using data from a Japanese economicsdaily (Nihon-Keizai-Shimbun-Sha, 1995).
They con-sist of 645 articles of diverse text types (prose, nar-rative, news report, expository text, editorial, etc.
),which are randomly drawn from the entire set of arti-cles published during the year.
Sentences and para-graphs contained in the data set totalled 12,770 and5,352, respectively.
We had, on the average, 984.5characters, 19.2 sentences, and 8.2 paragraphs, forone article in the data.
Each sentence in an articlewas annotated with a link to its associated modi-fyee sentence.
Annotations were given manually bythe first author.
Each sentence was associated withexactly one sentence.In assigning a link tag to a sentence, we did notfollow any specific discourse theories uch as Rhetor-ical Structure Theory (Mann and Thompson, 1987).This was because they often do not provide informa-tion on discourse relations detailed enough to serveas tagging guidelines.
In the face of this, we fellback on our intuition to determine which sentencelinks with which.
Nonetheless, we followed an in-formal rule, motivated by a linguistic theory of co-hesion by Halliday and Hasan (1990): which saysthat we relate a sentence to one that is contextuallymost relevant o it, or one that has a cohesive linkwith it.
This included not only rhetorical relation-ships such as 'reason', 'cause-result', 'elaboration','justification' or 'background' (Mann and Thomp-son, 1987), but also communicative relationshipssuch as 'question-answer' and those of the 'initiative-response' sort (Fox, 1987; Levinson, 1994; Carlettaet al, 1997).Since the amount of data available at the time ofthe experiments was rather moderate (645 articles),we decided to resort to a test procedure known ascross-validation.
The following is a quote from Quin-lan (1993).
"In this procedure, the available data is di-vided into N blocks so as to make eachblock's number of cases and class distri-bution as uniform as possible.
N differ-ent classification models are then built, ineach of which one block is omitted from thetraining data, and the resulting model istested on the cases in that omitted block.
"The average performance over the N tests is sup-posed to be a good predictor of the performance ofa model built from all the data.
It is common to setN=IO.However, we are concerned here with the accuracyof dependency parses and not with that of class de-cisions by decision tree models.
This requires somemodification to the way the validation procedure isapplied to the data.
What we did was to apply theprocedure not on the set of cases as in C4.5, buton the set of articles.
We divided the set of articlesinto 10 blocks in such a way that each block containsas uniform a number of sentences as possible.
Theprocedure would make each block contain a uniformnumber of correct dependencies.
(Recall that everysentence in an article is manually annotated with ex-actly one link.
So the number of correct links equalsthat of sentences.)
The number of sentences in eachblock ranged from 1,256 to 1,306.The performance is rated for each article in thetest set by using a metric:number of correct dependencies retrievedprecision =total number of dependencies retrievedAt each validation step, we took an average perfor-mance score for articles in the test set as a precisionof that step's model.
Results from 10 parsing modelswere then averaged to give a summary figure.3.2 Resu l ts  and  Ana lysesWe list major results of the experiments in Table 3,The results show that clues are not of much helpto improve performance.
Indeed we get the bestresult of 0.642 when N = 0, i.e., the model doesnot use clues at all.
We even find that an overallperformance tends to decline as models use more Ofthe words in the corpus as clues.
It is somewhattempting to take the results as indicating that clueshave bad effects on the performance (more discus-sion on this later).
This, however, appears to runcounter to what we expect from results reported inprior work on discourse(Kurohashi and Nagao, 1994;Litman and Passonneau, 1995; Grosz and Sidner,1986; Marcu, 1997), where the notion of clues orcue phrases forms an important part of identifyinga structure of discourse7Table 4 shows how the confidence value (CF) af-fects the performance of discourse models.
The CF7 One problem with earlier work is that evaluations aredone on very small data; 9 sections from a scientific writing(approx.
300 sentences) (Kurohashi and Nagao, 1994): 15narrathes (I113 clauses) (Lhman and Passonneau.
1995): 3texts (Marcu, 1997).
It is not clear how reliable estimates ofperformance obtained there would be.221Table 3: Effects of lexical clues on the performance of models.
N is the number of clues used.
Figures inparentheses represent the ratio of improvements against a model with N = 0.\[ N=0 N= 100 N=500 N=IO00 t\[ 0.642 0.635 (-1.100%) 0.632 (-1.580%) 0.628 (-2.220%) ITable 4: Effects of pruning on performance.
CF refers to a confidence value.
Small CF values cause moreprunings than large values.Clues CF=5% CF=10% CF=25% CF=50% CF=75% CF=95%0 0.626 0.636 0.642 0.633 0.625 0.624100 0.629 0.627 0.635 0.626 0.614 0.609500 0.626 0.630 0.632 0.616 0.604 0.6011000 0.628 0.627 0.628 0.616 0.601 0.597represents the extent to which a decision tree ispruned; A small CF leads to a heavy pruning ofa tree.
The tree pruning is a technique by whichto prevent a decision tree from fitting training datatoo closely.
The problem of a model fitting data tooclosely or overfitting usually causes an increase oferrors on unseen data.
Thus a heavier pruning of atree would result in a more general tree.While Haruno (1997) reports that a less pruningproduces a better performance for Japanese sentenceparsing with a decision tree, results we got in Table 4show that this is not true with discourse parsing.
InHaruno (1997), the performance improves by 1.8%from 82.01% (CF = 25%) to 83.35% (CF = 95%).25% is the default value for CF in C4.5, which isgenerally known to be the best CF level in machinelearning.
Table 4 shows that this is indeed the case:we get a best performance at around CF = 25% forall the values of N.Let us turn to effects that each feature might haveon the model's performance.
For each feature, we re-moved it from the model and trained and tested themodel on the same set of data as before the removal.Results are summarized in Table 5.
It was foundthat, of the features considered, DistSen, which en-codes a distance between two sentences, contributesmost to the performance; at N = 0, its removalcaused as much as an 8.62% decline in performance.On the other hand, lexical features Sire and Sire2made little contribution to the overall performance;their removal even led to a small improvement insome cases, which seems consistent with the earlierobservation that lexical features are a poor class pre-dictor.To further study effects of lexical clues, we haverun another experiment where clues are limited tosentence connectives (as identified by a tokenizerprogram).
Clues included any connective that hasan occurrence in the corpus, which is listed in Ta-ble 6.
Since a sentence connective is relevant o es-tablishing inter-sententiaI relationships, it was ex-pected that restricting clues to connectives wouldimprove performance.
As with earlier experiments,we have run a 10-fold cross validation experiment onthe corpus, with 52 attributes for lexical clues.
Wefound that the accuracy was 0.642.
So it turned outthat using connectives i no better than when we donot use clues at all.Figure 5 gives a graphical summary of the signif-icance of features in terms of the ratio of improve-ment after their removal (given as parenthetical fig-ures in Table 5).
Curiously, while the absence ofthe DistSen feature caused a largest decline, thesignificance of a feature tends to diminish with thegrowth of N. The reason, we suspect, may have todo with the susceptibility of a decision tree modelto irrelevant features, particularly when their num-ber is large.
But some more work needs to be donebefore we can say anything about how irrelevancyaffects a parser's performance.One caveat before leaving the section; the experi-ments so far did not establish any correlation, eitherpositive or negative, between the use of lexical infor-mation and the performance on discourse parsing.To say anything definite would probably require ex-periments on a corpus much larger than is currentlyavailable.
However, it would be safe to say that dis-tance and length features are more prominent hanlexical features when a corpus is relatively small.4 Conc lus ionThe paper demonstrated how it is possible to build adiscourse parser which performs reasonably well ondiverse data.
It relies crucially on (a) feature selec-tion by a decision tree and (b) the way a discourseis encoded.
While we have found that distance and222Table 5: Measuring the significance of features.
Figures below indicate how much the performance is affectedby the removal of a feature.
'REF' refers to a model where no feature is removed.
'Clues' indicates the numberof clues used for a model.
A minus sign at a feature indicates the removal of that feature from a model.FEATURES/#CLUES 0 100 500 1000REF.
0.642 0.635 0.632 0.628DistSen- 0.591LenText- 0.626LocWithinPar- 0.631Sim- 0.643(-8.620%) 0.598 (-6.180%)(-2.550%) 0.626 (-1.430%)(--1.740%) 0.627 (-1.270%)(+0.160%) 0.640 (+0.790%)0.604 (-4.630%) 0.603 (-4.140%)0.620 (-1.930%) 0.623 (-0.800%)0.624 (-1.280%) 0.628 (?0.000%)0.632 (?0.000%) 0.630 (+0.320%)0.638 (+0.950%) 0.629 (+0.160%)0.632 (?0.000%) 0.632 (+0.640%)0.629 (-0A70%) 0.631 (+0.480%)0.631 (-0.150%) 0.627 (-0.150%)0.634 (+0.320%) 0.630 (+0.320%)0.628 (-0.630%) 0.628 (?0.000%)0.631 (-0.150%) 0.628 (?0.000%)Sim2-LenSenA-LenSenB-LocPar-LocSen-DistPa.r-I sAt i t le -0.644 (+0.320%) 0.647 (+1.860%)0.641 (-0.150%) 0.638 (+0.480%)0.642 (?0.000%) 0.639 (+0.630%)0.640 (-0.310%) 0.637 (+0.320%)0.639 (-0.460%) 0.631 (-0.630%)0.636 (-0.940%) 0.631 (-0.630%)0.638 (-0.620%) 0.635 (~0.000%)22'~ -2o< -6>o -S-100IFigure 5: The ratio of improvement after removal of feature.. ~,  .,.,~,1,,, .
.
.
.
.
.
.
.
.
.
.
,4. .
.
.
.
.
.
.
.
.
.
.
I I I: :2 ::2 &% .
",'7 -'.
r ~"r:'='~---:---~'-T:~'~':-:2.Z-'~:'=':'=-~-:-~-~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
-t~" --- ~ - ;-.--- : ---.-- - ~ ~'=~-.'..=.:'..=..:.~7~..~.
_ .--: 7" -" 7 ;~\]~(:.--'.3...~........z...=-~.
~ - ,  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.. .
.
.
.
.
.
.
~-JT~:.=~-_-==:.~---=~.~.7 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
o .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.Dis tSen  oLenText  -+- -LocWi th inP  -O- -b4m -.x- ....S i re2  -a , - .LenSenA --~--, ~  LenSenB -.o-- -LocPar  - + --LocSen -E3--D is tParl sAt i t le  -~- -I I I200 400 600 800 100The  number  o f  c luesTable 6: Connectives found in the corpus.
Underlined items (also marked with an asterisk) are those thatthe tokenizer program erroneously identified as a connective.shikashi but, ippou whereas, daga but, soreo (.
), shikamo moreover, tokoroga but, soshiteand, soreni moreover, sokode incidentally, soredemo still, sore (.
), tadashi provided that,soredakeni all the more because, tokini by the way, dakara so, demo but, sonoue moreover,sitagatte therefore, dewa now, nimokakawarazu despite, soredewa well, sorede and then,sorekara after that, towaie nevertheless, hitagatte therefore, tsuide while, katoitte but.dakarakoso consequently, matawa or, soretomo r else, soreto for another thing, nanishiroanyhow, omakeni in addition, sunawachi in other words, toiunowa because, naraba if.sonokawari instead, samunaktL.ba or else, sunawachi namely, naishiwa or.
sate by the way,toshite ('.).
toiunomo because, sorenimokakawarazu nonetheless, orenishitemo yet, oyobimoreover, tokorode incidentally, nazenara because, tosureba if, nanishiro anyhow, otto(*), nanoni but223length features are more prominent than lexical fea-tures, we were not able to establish the usefulnessof the latter features, which is expected from earlierworks on discourse as well as on sentence parsing(Magerman, 1995; Collins, 1996).The following are some of the future research is-sues :Building a larger corpus Our discourse parserdid not perform as well as a statistical sentenceparser, which normally performs with over 80% pre-cision.
We suspect hat the reason may have ~to dowith inconsistencies in tagging and the size of thecorpus we used.Parsing with the rhetorical s t ructure theoryTechnically it is straightforward to turn the presentparsing method into a full-fledged RST parser, whichinvolves modifying the way classes are defined andredefining constraints on a structure of discourse.
Aproblem, however, is that the task of assigning sen-tences to rhetorical relations with some consistencycould turn out to be quite difficult for human coders.Extending to other Languages The generalframework in which our parser is built does not pre-suppose lements pecific to a particular language.It should be possible to carry it over to other lan-guages with no significant modification to it:Re ferencesJean Carlotta, Amy Isard, Stephen Isard, Jacque-line C. Kowtko, Gwyneth Doherty-Sneddon, andAnne H. Anderson.
1997.
The Reliability of a Di-alogue Structure Coding Scheme.
ComputationalLinguistics, 23(1):13-31.Michael John Collins.
1996.
A New StatisticalParser Based on Bigram Lexical Dependencies.In Proceedings of the 34th Annual Meeting of theAssociation for Computational Linguistics, pages184-191, California, USA., June.
Association forComputational Linguistics.Barbara A.
Fox.
1987.
Discourse structure andanaphora.
Cambridge Studies in Linguistics 48.Cambridge University Press, Cambridge, UK.Barbara Grosz and Candance Sidner.
1986.
Atten-tion, Intentions and the Structure of Discourse.Computational Linguistics, 12(3):175-204.M.A.K.
Halliday and R. Hasan.
1990.
Cohesion inEnglish.
Longman, New York.Masahiko Haruno.
1997.
Kettei-gi o mochiita ni-hongo kakariuke kaiseki (A Japanese DependencyParser Using A Decision Tree).
Submitted forpublication.Marti A. Hearst.
1994.
Multi-Paragraph Segmen-tation of Expository Text.
In Proceedings of the32nd Annual Meeting of the Association for Com-putational Linguistics, pages 9-16, New Mexico,USA.Sadao Kurohashi and Makoto Nagao.
1994.
Auto-matic Detection od Discourse Structure by Check-ing Surface Information in Sentences.
In Proceed-ings of The 15th Intewrnational Conference onComputational Linguistics, pages 1123-1127, Au-gust.
Kyoto,Japan.Stephen C. Levinson.
1994.
Pragmatics.
CambridgeTextbooks in Linguistics.
Cambridge UniversityPress.Diane Litman and Rebecca J. Passonneau.
1995.Combining multiple knowledge sources for dis-course segmentation.
In Proceedings of the 33rdAnnual Meeting of the Association for Computa-tional Linguistics, pages 108-115.
The Associationfor Computational Linguistics, June.
Cambridge,MASS.
USA.David M. Magerman.
1995.
Statistical Decision-Tree Models for Parsing.
In Proceedings of the33rd Annual Meeting of the Association for Com-putational Linguistics, pages 276--283, Camridge,MASS.
USA., June.
Association for Computa-tional Linguistics.William C. Mann and Sandra A. Thompson.
1987.Rhetorical Structure Theory : A Theory of TextOrganization.
Technical Report ISI/RS 87-190,ISI.Daniel Marcu.
1997.
The Rhetorical Parsing of Nat-ural Language Texts.
In Proceedings of the 35thAnnual Meetings of the Association for Computa-tional Linguistics and the 8th European Chapterof the Association for Computational Linguistics,pages 96-102, Madrid, Spain, July.Mitcell Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a Large AnnotatedCorpus of English: The Penn Treebank.
Compu-tational Linguistics, 19(2):313-330, June.Nihon-Keizai-Shimbun-Sha.
1995.
Nihon KeizaiShimbun 95 sen CD-ROM ban.
CD-ROM.
NihonKeizai Shimbun, Inc., Tokyo.J.
Ross Quinlan.
1993.
C4.5: Programs for MachineLearning.
Morgan Kaufmann.Hirofumi Sa.kurai and Toru Hisamitsu.
1997.Keitaiso Puroguramu ANIMA no Sekkei to Jissoo.In Jyoohoo Shori Gakkai Zenki Zenkoku TaikaiKooen Ronbun Shuu, volume 2, pages 57-56.
In-formation Processing Society of Japan, March 12-14.224
