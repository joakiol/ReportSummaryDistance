Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 41?51,ACL HLT 2011, Portland, Oregon, USA, June 2011. c?2011 Association for Computational LinguisticsMulti-Word Unit Dependency Forest-based Translation RuleExtractionHwidong Na Jong-Hyeok LeeDepartment of Computer Science and EngineeringPohang University of Science and Technology (POSTECH)San 31 Hyoja Dong, Pohang, 790-784, Republic of Korea{leona,jhlee}@postech.ac.krAbstractTranslation requires non-isomorphictransformation from the source to thetarget.
However, non-isomorphism canbe reduced by learning multi-word units(MWUs).
We present a novel way ofrepresentating sentence structure basedon MWUs, which are not necessarilycontinuous word sequences.
Our pro-posed method builds a simpler structureof MWUs than words using wordsas vertices of a dependency structure.Unlike previous studies, we collectmany alternative structures in a packedforest.
As an application of our proposedmethod, we extract translation rules inform of a source MWU-forest to thetarget string, and verify the rule coverageempirically.
As a consequence, weimprove the rule coverage compare to aprevious work, while retaining the linearasymptotic complexity.1 IntroductionSyntax is the hierarchical structure of a natu-ral language sentence.
It is generally repre-sented with tree structures using phrase struc-ture grammar (PSG) or dependency grammarFigure 1: A pair of sentences that require longdistance reordering (dashed line) and discontinuoustranslation (thick line)(DG).
Although the state-of-the-art statisticalmachine translation (SMT) paradigm is phrase-based SMT (PBSMT), many researchers haveattempted to utilize syntax in SMT to over-come the weaknesses of PBSMT.
An emergingparadigm alternative to PBSMT is syntax-basedSMT, which embeds the source and/or targetsyntax in its translation model (TM).
Utilizingsyntax in TM has two advantages over PBSMT.The first advantage is that syntax eases globalreordering between the source and the targetlanguage.
Figure 1 shows that we need globalreordering in a complex real situation, wherea verbal phrase requires a long distance move-ment.
PBSMT often fails to handle global re-ordering, for example, from subject-verb-object(SVO) to SOV transformation where V shouldbe moved far away from the original position in41Table 1: Statistics of the corresponding target wordsfor the continuous word sequences in the source lan-guage, or vice versa.
C denotes consistent, O over-lapped, D discontinuous, and N null.Word Alignment C O D NManual 25 60 10 5Automatic 20 55 15 5the source language.
This is because of the twodistance-based constraints in PBSMT: the dis-tortion model cost and the distortion size limit.For the distortion model cost, PBSMT sets zerocost to the monotone translation and penalizesthe distorted translations as the distortion growslarger.
For the distortion size limit, a phrase canonly be moved from its original position withina limit.
Therefore, PBSMT fails to handle longdistance reordering.
Syntax-based SMT man-ages global reordering as structural transforma-tion.
Because reordering occurs at the sub-structure level such as constituents or treeletsin syntax-based SMT, the transformation of thesub-structure eventually yields the reordering ofthe whole sentence.The second advantage of using syntax in TMis that syntax guides us to discontinuous trans-lation patterns.
Because PBSMT regards onlya continuous sequence of words as a transla-tion pattern, it often fails to utilize many use-ful discontinuous translation patterns.
For ex-ample, two discontinuous source words corre-spond to a target word in Figure 1.
In our in-spection of the training corpus, a continuousword sequence often corresponds to a set ofdiscontinuous words in the target language, orvice versa (Table 1).
Discontinuous translationpatterns frequently appear in many languages(S?gaard and Kuhn, 2009).
Syntax-based SMTovercomes the limitations of PBSMT because itfinds discontinuous patterns along with the hier-Figure 2: The maximum branching factor (BF) anddepth factor (DF) in a dependency tree in our corpusarchical structure.
For example, the two discon-tinuous source words have a head-dependent re-lation (Figure 3).
Especially with the depen-dency tree, we can easily identify patterns thathave non-projectivity (Na et al, 2010).
How-ever, syntax-based patterns such as constituentsor treelets do not sufficiently cover various use-ful patterns, even if we have the correct syn-tactic analysis (Chiang, 2010).
For this reason,many researchers have proposed supplementarypatterns such as an intra/inter constituent or se-quence of treelets (Galley et al, 2006; Shen etal., 2008).Unlike PSG, DG does not include non-terminal symbols, which represent constituentinformation.
This makes DG simpler than PSG.For instance, it directly associates syntatic rolewith the structure, but introduces a difficulty insyntax-based SMT.
The branching factor of adependency tree becomes larger when a headword dominates many dependents.
We ob-serve that the maximum branching factor ofan automatically parsed dependency tree rangeswidely, while most trees have depth under a cer-tain degree (Figure 2).
This indicates that wehave a horizontally flat dependency tree struc-ture.
The translation patterns extracted from the42flat dependency tree are also likely to be flat.Unfortunately, the flat patterns are less appli-cable at the decoding stage.
When one of themodifiers does not match, for instance, we failto apply the translation pattern.
Therefore, weneed a more generally applicable representationfor syntax-based SMT using DG.We propose a novel representation of DG thatregards a set of words as a unit of the depen-dency relations, similar to (Ding, 2006; Wu etal., 2009; Na et al, 2010).
Unlike their work,we consider many alternatives without prede-fined units, and construct a packed forest of themulti-word units (MWUs) from a dependencytree.
For brevity, we denote the forest based onMWUs as an MWU-forest.
Because all pos-sible alternatives are exponentially many, wegive an efficient algorithm that enumerates thek-best alternatives in section 3.
As an appli-cation, we extract translation patterns in formof a source MWU-forest to the target string inorder to broaden the coverage of the extractedpatterns for syntax-based SMT in section 4.
Wealso report empirical results related to the use-fulness of the extracted pattern in section 5.
Theexperimental results show that the MWU-forestrepresentation gives more applicable translationpatterns than the original word-based tree.2 Related WorkPrevious studies have proposed merging alter-native analyses to deal with analysis errors fortwo reasons: 1) the strongest alternative is notnecessarily the correct analysis, and 2) mostalternatives contain similar elements such ascommon sub-trees.
For segmentation alterna-tives, Dyer et al (2008) proposed a word latticethat represents exponentially large numbers ofsegmentations of a source sentence, and inte-grates reordering information into the lattice aswell.
For parsing alternatives, Mi et al (2008)suggested a packed forest that encodes alterna-tive PSG derivations.
Futher, Mi et al (2010)combined the two approaches in order to bene-fit from both.The translation literature also shows thattranslation requires non-isomorphic transfor-mation from the source to the target.
This yieldstranslation divergences such as head-switching(Dorr, 1994).
Ding and Palmer (2005) reportedthat the percentage of the head-swapping casesis 4.7%, and that of broken dependencies is59.3% between Chinese and English.
The largeamount of non-isomorphism, however, will bereduced by learning MWUs such as elementarytrees (Eisner, 2003).There are few studies that consider a depen-dency structure based on MWUs.
Ding (2006)suggested a packed forest which consists of theelementary trees, and described how to findthe best decomposition of the dependency tree.However, Ding (2006) did not show how to de-termine the MWUs and restrict them to forma subgraph from a head.
For opinion mining,Wu et al (2009) also utilized a dependencystructure based on MWUs, although they re-stricted MWUs with predefined relations.
Naet al (2010) proposed an MWU-based depen-dency tree-to-string translation rule extraction,but considered only one decomposition for ef-ficiency.
Our proposed method includes addi-tional units over Ding?s method, such as a se-quence of subgraphs within a packed forest.
Itis also more general than Wu et al?s methodbecause it does not require any predefined re-lations.
We gain much better rule coverageagainst Na et al?s method, while retaining linearasymptotical computational time.43Figure 3: A dependency tree of the source sentencein Figure 13 MWU-based Dependency ForestThere are two advantages when we use theMWU-forest representaion with DG.
First, weexpress the discontinuous patterns in a vertex,so that we can extract more useful translationpatterns beyond continuous ones for syntax-based SMT.
Second, an MWU-forest containsmany alternative structures which may be sim-pler structures than the original tree in terms ofthe branching factor and the maximum depth.Wu et al (2009) utilized an MWU-tree to iden-tify the product features in a sentence easily.As in previous literature in syntax-basedSMT using DG, we only consider the well-formed MWUs where an MWU is either atreelet (a connected sub-graph), or a sequenceof treelets under a common head.
In otherwords, each vertex in an MWU-forest is either?fixed on head?
or ?floating with children?.
Theformal definitions can be found in (Shen et al,2008).We propose encoding multiple dependencystructures based on MWUs into a hypergraph.A hypergraph is a compact representation ofexponetially many variations in a polynomi-nal space.
Unlike PSG, DG does not haveFigure 4: An MWU-forest of Figure 3.
The dashedline indicates the alternative hyperedges.non-terminals that represent the linguisticallymotivated, intermediate structure such as nounphrases and verb phrases.
For this simplicity,Tu et al (2010) proposed a dependency forestas a hypergraph, regarding a word as a vertexwith a span that ranges for all its descendants.The dependency forest offers tolerence of pars-ing errors.Our representation is different from the de-pendency forest of Tu et al (2010) since a ver-tex corresponds to multiple words as well aswords.
Note that our representation is alsocapable of incorporating multiple parse trees.Therefore, MWU-forests will also be tolerantof the parsing error if we provide multiple parsetrees.
In this work, we concentrate on the ef-fectiveness of MWUs, and hence utilize thebest dependency parse tree.
Figure 4 shows anMWU-forest of the dependency tree in Figure3.More formally, a hypergraph H = ?V,E?consists of the vertices V and hyperedgesE.
We assume that a length-J sentence hasa dependency graph which is single-headed,44acyclic, and rooted, i.e.
hj is the index of thehead word of the j-th word, or 0 if the word isthe root.
Each vertex v = {j|j ?
[1, J ]} de-notes a set of the indices of the words that satis-fies the well-formed constraint.
Each hyperedgee = ?tails(e), head(e)?
denotes a set of the de-pendency relations between head(e) and ?v ?tails(e).
We include a special node v0 ?
Vthat denotes the dummy root of an MWU-forest.Note that v0 does not appear in tails(e) for allhyperedges.
We denote |e| is the arity of hyper-edge e, i.e.
the number of tail nodes, and thearity of a hypergraph is the maximum arity overall hyperedges.
Also, let ?
(v) be the indices ofthe words that the head lays out of the vertex,i.e.
?
(v) = {j|hj 6?
v ?
j ?
v}, and ?
(v) bethe indices of the direct dependent words of thevertex, i.e.
?
(v) = {j|hj ?
v ?
j 6?
v}.
LetOUT (v) and IN(v) be the outgoing and in-coming hyperedges of a vertex v, respectively.It is challenging to weight the hyperedgesbased on dependency grammar because a de-pendency relation is a binary relation from ahead to a dependent.
Tu et al (2010) assigneda probability for each hyperedge based on thescore of the binary relation.
We simply preferthe hyperedges that have lower arity by scoringas follows:c(e) =?v?tails(e) |v||e|p(e) = c(e)?e?
?IN(head(e)) c(e?
)We convert a dependency tree into a hyper-graph in two steps using the Inside-Outside al-gorithm.
Algorithm 1 shows the pseudo codeof our proposed method.
At the first step, wefind the k-best incoming hyperedges for eachvertex (line 3-8), and compute the inside proba-bility (line 9), in bottom-up order.
At the sec-ond step, we compute the outside probabilityAlgorithm 1 Build Forest1: Initialize V2: for v ?
V in bottom-up order do3: Create a chart C = |?
(v)|24: for chart span [p, q] do5: Initialize C[p, q] if ?v s.t.
[p, q] = v or?
(v)6: Combine C[p, i] and C[i + 1, q]7: end for8: Set IN(v) to the k-best in C[TOP ]9: Set ?
(v) as in Eq.
110: end for11: for v ?
V in top-down order do12: Set ?
(v) as in Eq.
213: end for14: Prune out e if p(e) ?
?15: return v0(line 12) for each vertex in a top-down manner.Finally we prune out less probable hyperedges(line 14) similar to (Mi et al, 2008).
The insideand outside probabilities are defined as follows:?
(v) =?e?IN(v)p(e)?d?tails(e)?
(d) (1)where ?
(v) = 1.0 if IN(v) = ?, and?
(v) =?h?OUT (v)e?IN(head(h))?
(head(e))p(e)|OUT (v)|??d?tails(e)\{v}?
(d) (2)where ?
(v) = 1.0 if OUT (v) = ?.In practice, we restrict the number of wordsin a vertex in the initialization (line 1).
We ap-proximate all possible alternative MWUs thatinclude each word as follows:45Figure 5: A sub-forest of Figure 4 with annotationof aspan and cspan for each vertex.
We omit thespan if it is not consistent.?
A horizontal vertex is a sequence of modi-fiers for a common head word, and?
A vertical vertex is a path from a word toone of the ancestors, and?
A combination of the horizontal verticesand the vertical vertices, and?
A combination of the vertical vertices andthe vertical vertices.The computational complexity of the initial-izaion directly affects the complexity of the en-tire procedure.
For each word, generating thehorizontal vertices takes O(b2), and the verticalvertices take O(bd?1), where b is the maximumbranching factor and d is the maximum depthof a dependency tree.
The two combinationstake O(bd+1) and O(b2(d?1)) time to initializethe vertices.
However, it takes O(mm+1) andO(m2(m?1)) if we restrict the maximum num-ber of the words in a vertex to a constant m.Ding and Palmer (2005) insisted that theViterbi decoding of an MWU-forest takes lin-ear time.
In our case, we enumerate the k-bestincoming hyperedeges instead of the best one.Because each enumeration takes O(k2|?
(v)|3),Table 2: The extracted rules in Figure 5.
N denotesthe non-lexicalized rules with variables xi for eachv ?
tails(e), and L denotes the lexicalized rule.head(e) tails(e) rhs(?
)N{3} {8} : x1 x1{8} {4} : x1, {5} : x2 when x1 x2{3, 8} {4, 5} : x1 when x1{3, 8} {4} : x1, {5} : x2 when x1 x2{4, 5} {6, 7} : x1 I?m in x1{5} {6, 7} : x1 in x1L{6, 7}N/Athe States{4} I?m{5} in{4, 5} I?m in{5, 6} in the State{3, 8} Whenthe total time complexity also becomes linearto the length of the sentence n similar to Dingand Palmer (2005), i.e.
O(|V |k2|?
(v)|3), where|V | = O(na2(a?1)) and a = min(m, b, d).4 MWU-Forest-to-String TranslationRule ExtractionAs an application of our proposed MWU-forest,we extract translation rules for syntax-basedSMT.
Forest-based translation rule extractionhas been suggested by Mi and Huang (2008)although their forest compacts the k-best PSGtrees.
The extraction procedure is essentiallythe same as Galley et al (2004), which iden-tifies the cutting points (frontiers) and extractsthe sub-structures from a root to frontiers.The situation changes in DG because DGdoes not have intermediate representation.
Atthe dependency structure, a node correspondsto two kinds of target spans.
We borrow thedefinitions of the aligned span (aspan), and thecovered span (cspan) from Na et al (2010), i.e.46?
aspan(v) = [min(av),max(av)], and?
cspan(v) =aspan(v)?d?tails(e)e?IN(v)cspan(d), where av = {i|j ?
v ?
(i, j) ?
A}.
Figure 5shows aspans and cspans of a sub-forest of ofthe MWU-forest in the previous example.Each span type yields a different rule type:aspan yields a lexicalized rule without anyvariables, and cspan yields a non-lexicalizedrule with variables for the dependents of thehead word.
For example, Table 2 shows the ex-tracted rule in Figure 5.In our MWU-forest, the rule extraction pro-cedure is almost identical to a dependencytree-to-string rule extraction except we regardMWUs as vertices.
Let fj and ei be the j-thsource and i-th target word, respectively.
As anMWU itself has a internal structure, a lexicalrule is a tree-to-string translation rule.
There-fore, a lexicalized rule is a pair of the sourcewords s and the target words t as follows:s(v) = {fj |j ?
v}t(v) = {ei|i ?
aspan(v)} (3)In addition, we extract the non-lexicalizedrules from a hyperedge e to cspan of thehead(e).
A non-lexicalized rule is a pair of thesource words in the vertices of a hyperedge andthe cspan of the target words with substitutionsof cspan(d) for each d ?
tails(e).
We abstractd on the source with ?
(d) for non-lexicalizedrules (row 2 in Table 2).
We define the sourcewords s and the target words t as follows:s(e) = {fj |j ?
head(e) ?
j ?
?
(d)}t(e) = {ei|i ?
cspan(v) ?
i 6?
cspan(d)}?
{xi|d?
xi} (4)Algorithm 2 Extract Rules( H = ?V,E?
)1: ?
= ?2: for v ?
V do3: if aspan(v) is consistent then4: ??
?
?
?
s(v) , t(v) ?
as in Eq.
35: end if6: if cspan(v) is consistent then7: for e ?
IN(v) do8: if cspan(d)?d ?
tails(e) then9: ??
??
?
s(e), t(e) ?
as in Eq.
410: end if11: end for12: end if13: end for14: return ?where d ?
tails(e).More formally, we extract a synchronous treesubstitution grammar (STSG) which regards theMWUs as non-terminals.Definition 1 A STSG using MWU (STSG-MWU) is a 6-tuple G = ?
?S ,?T ,?,?, S, ??,where:?
?S and ?T are finite sets of terminals(words, POSs, etc.)
of the source and tar-get languages, respectively.?
?
is a finite set of MWUs in the sourcelanguage, i.e.
?
= {?S}+?
?
is a finite set of production ruleswhere a production rule ?
: X ??
lhs(?)
, rhs(?
), ?
?, which is a relation-ship from ?
to {x ?
?T } ?, where ?
is thebijective function from the source verticesto the variables x in rhs(?).
The asteriskrepresents the Kleenstar operation, and?
S is the start symbol used to represent thewhole sentence, i.e.
?0 : S ?
?
X , X ?.47For each type of span, we only extract therules if the target span has consistent wordalignments, i.e.
span 6= ?
?
?i ?
span,{j|(i, j) ?
A ?
(i?, j) ?
A s.t.
i?
6?
span} = ?.Algorithm 2 shows the pseudo code of theextraction.
Because a vertex has aspan andcsapn, we extract a lexicalized rule (line 3-5)and/or non-lexicalized rules (line 6-12) for eachvertex.5 ExperimentWe used the training corpus provided for theDIALOG Task in IWSLT10 between Chineseand English .
The corpus is a collection of30,033 sentence pairs and consists of dialogs intravel situations (10,061) and parts of the BTECcorpus (19,972).
Details about the providedcorpus are described in (Paul, 2009).
We usedthe Stanford Parser 1 to obtain word-level de-pendency structures of Chinese sentences, andGIZA++ 2 to obtain word alignments of thebiligual corpus.We extracted the SCFG-MWU from thebiligual corpus with word alignment.
In or-der to investigate the coverage of the extractedrule, we counted the number of the recoveredsentences, i.e.
counted if the extracted rulefor each sentence pair generates the target sen-tence by combining the extracted rules.
As wecollected many alternatives in an MWU-forest,we wanted to determine the importance of eachsource fragment.
Mi and Huang (2008) penal-ized a rule ?
by the posterior probability of itstree fragment lhs(?).
This posterior probabilityis also computed in the Inside-Outside fashionthat we used in Algorithm 1.
Therefore, we re-garded the fractional count of a rule ?
as1http://nlp.stanford.edu/software/lex-parser.shtml,Version 1.6.42http://code.google.com/p/giza-pp/Figure 6: The rule coverage according to the numberof the words in a vertex.c(?)
= ??(lhs(?))??
(v0)We prioritized the rule according to the frac-tional count.
The priority is used when we com-bine the rules to restore the target sentence us-ing the extracted rule for each sentence.
We var-ied the maximum size of a vertex m, and thenumber of incoming hyperedges k. Figure 6shows the emprical result.6 DiscussionFigure 6 shows that we need MWU to broadenthe coverage of the extracted translation rules.The rule coverage increases as the number ofwords in an MWU increases, and almost con-verges at m = 6.
Our proposed method re-cover around 75% of the sentences in the cor-pus when we properly restrict m and k. This isa great improvement over Na et al (2010), whoreported around 60% of the rule coverage with-out the limitaion of the size of MWUs.
Theyonly considered the best decomposition of thedependency tree, while our proposed methodcollects many alternative MWUs into an MWU-forest.
When we considered the best decom-position (k = 1), the rule coverage dropped to48Figure 7: The frequency of the recovery accordingto the length of the sentences in 1,000 sentencesaround 65%.
This can be viewed as an indirectcomparison between Na et al (2010) and ourproposed method in this corpus.Figure 7 shows that the frequency of suc-cess and failure in the recovery depends on thelength of the sentences.
As the length of sen-tences increase, the successful recovery occursless frequently.
We investigated the reason offailure in the longer sentences.
As a result, thetwo main sources of the failure are the wordalignment error and the dependency parsing er-ror.Our proposed method does not include alltranslation rules in PBSMT because of the syn-tactic constraint.
Generally speaking, our pro-posed method cannot deal with MWUs that donot satisfy the well-formed constraint.
How-ever, ill-formed MWUs seems to be useful aswell.
For example, our proposed method dosenot allow ill-formed vertices in an MWU-forestas shown in Figure 8.
This would be problem-atic when we use an erroneuos parsing result.Because dealing with parsing error has beenstudied in literature, our proposed method hasthe potential to improve thought future work.Figure 8: An illustration of ill-formed MWUs7 ConclusionWe have presented a way of representing sen-tence structure using MWUs on DG.
Because ofthe absence of the intermdiate representation inDG, we built a simpler structure of MWUs thanwords using words as vertices of a dependencystructure.
Unlike previous studies, we collectedmany alternative structures using MWUs in apacked forest, which is novel.
We also ex-tracted MWU-forest-to-string translation rules,and verified the rule coverage empirically.
As aconsequence, we improvemed the rule coveragecompared with a previous work, while retainingthe linear asymptotic complexity.
We will ex-pand our propose method to develop a syntax-based SMT system in the future, and incoporatethe parsing error by considering multiple syn-tactic analyses.AcknowledgmentsWe appreciate the three anonymous reviewers.This work was supported in part by the KoreaScience and Engineering Foundation (KOSEF)grant funded by the Korean government (MESTNo.
2011-0003029), and in part by the BK 21Project in 2011.49ReferencesDavid Chiang.
2010.
Learning to translate withsource and target syntax.
In Proceedings of the48th Annual Meeting of the Association for Com-putational Linguistics, pages 1443?1452, Upp-sala, Sweden, July.
Association for Computa-tional Linguistics.Yuan Ding and Martha Palmer.
2005.
Machinetranslation using probabilistic synchronous de-pendency insertion grammars.
In ACL ?05: Pro-ceedings of the 43rd Annual Meeting on Associ-ation for Computational Linguistics, pages 541?548, Morristown, NJ, USA.
Association for Com-putational Linguistics.Yuan Ding.
2006.
Machine Translation UsingProbabilistic Synchronous Dependency InsertionGrammars.
Ph.D. thesis, August.Bonnie J. Dorr.
1994.
Machine translation diver-gences: a formal description and proposed solu-tion.
Comput.
Linguist., 20:597?633, December.Christopher Dyer, Smaranda Muresan, and PhilipResnik.
2008.
Generalizing word lattice trans-lation.
In Proceedings of ACL-08: HLT, pages1012?1020, Columbus, Ohio, June.
Associationfor Computational Linguistics.Jason Eisner.
2003.
Learning non-isomorphic treemappings for machine translation.
In ACL ?03:Proceedings of the 41st Annual Meeting on As-sociation for Computational Linguistics, pages205?208, Morristown, NJ, USA.
Association forComputational Linguistics.Michel Galley, Mark Hopkins, Kevin Knight, andDaniel Marcu.
2004.
What?s in a transla-tion rule?
In Daniel Marcu Susan Dumaisand Salim Roukos, editors, HLT-NAACL 2004:Main Proceedings, pages 273?280, Boston, Mas-sachusetts, USA, May 2 - May 7.
Association forComputational Linguistics.Michel Galley, Jonathan Graehl, Kevin Knight,Daniel Marcu, Steve DeNeefe, Wei Wang, andIgnacio Thayer.
2006.
Scalable inference andtraining of context-rich syntactic translation mod-els.
In Proceedings of the 21st International Con-ference on Computational Linguistics and 44thAnnual Meeting of the Association for Computa-tional Linguistics, pages 961?968, Sydney, Aus-tralia, July.
Association for Computational Lin-guistics.Haitao Mi and Liang Huang.
2008.
Forest-basedtranslation rule extraction.
In Proceedings of the2008 Conference on Empirical Methods in Nat-ural Language Processing, pages 206?214, Hon-olulu, Hawaii, October.
Association for Compu-tational Linguistics.Haitao Mi, Liang Huang, and Qun Liu.
2008.Forest-based translation.
In Proceedings of ACL-08: HLT, pages 192?199, Columbus, Ohio, June.Association for Computational Linguistics.Haitao Mi, Liang Huang, and Qun Liu.
2010.
Ma-chine translation with lattices and forests.
InColing 2010: Posters, pages 837?845, Beijing,China, August.
Coling 2010 Organizing Commit-tee.Hwidong Na, Jin-Ji Li, Yeha Lee, and Jong-HyeokLee.
2010.
A synchronous context free grammarusing dependency sequence for syntax-base sta-tistical machine translation.
In The Ninth Confer-ence of the Association for Machine Translationin the Americas (AMTA 2010), Denver, Colorado,October.Michael Paul.
2009.
Overview of the iwslt 2009evaluation campaign.Libin Shen, Jinxi Xu, and Ralph Weischedel.
2008.A new string-to-dependency machine translationalgorithm with a target dependency languagemodel.
In Proceedings of ACL-08: HLT, pages577?585, Columbus, Ohio, June.
Association forComputational Linguistics.Anders S?gaard and Jonas Kuhn.
2009.
Empiricallower bounds on aligment error rates in syntax-based machine translation.
In Proceedings of theThird Workshop on Syntax and Structure in Statis-tical Translation (SSST-3) at NAACL HLT 2009,pages 19?27, Boulder, Colorado, June.
Associa-tion for Computational Linguistics.Zhaopeng Tu, Yang Liu, Young-Sook Hwang, QunLiu, and Shouxun Lin.
2010.
Dependency for-est for statistical machine translation.
In Pro-ceedings of the 23rd International Conference onComputational Linguistics (Coling 2010), pages501092?1100, Beijing, China, August.
Coling 2010Organizing Committee.Yuanbin Wu, Qi Zhang, Xuangjing Huang, and LideWu.
2009.
Phrase dependency parsing for opin-ion mining.
In Proceedings of the 2009 Con-ference on Empirical Methods in Natural Lan-guage Processing, pages 1533?1541, Singapore,August.
Association for Computational Linguis-tics.51
