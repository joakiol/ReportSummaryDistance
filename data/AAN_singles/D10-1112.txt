Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1151?1161,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsWord-based dialect identification with georeferenced rulesYves ScherrerLATLUniversit?
de Gen?veGen?ve, Switzerlandyves.scherrer@unige.chOwen RambowCCLSColumbia UniversityNew York, USArambow@ccls.columbia.eduAbstractWe present a novel approach for (written) di-alect identification based on the discrimina-tive potential of entire words.
We generateSwiss German dialect words from a StandardGerman lexicon with the help of hand-craftedphonetic/graphemic rules that are associatedwith occurrence maps extracted from a linguis-tic atlas created through extensive empiricalfieldwork.
In comparison with a character-n-gram approach to dialect identification, ourmodel is more robust to individual spelling dif-ferences, which are frequently encountered innon-standardized dialect writing.
Moreover, itcovers the whole Swiss German dialect contin-uum, which trained models struggle to achievedue to sparsity of training data.1 IntroductionDialect identification (dialect ID) can be viewed asan instance of language identification (language ID)where the different languages are very closely re-lated.
Written language ID has been a popular re-search object in the last few decades, and relativelysimple algorithms have proved to be very successful.The central question of language ID is the following:given a segment of text, which one of a predefinedset of languages is this segment written in?
Languageidentification is thus a classification problem.Dialect identification comes in two flavors: spokendialect ID and written dialect ID.
These two tasks arerather different.
Spoken dialect ID relies on speechrecognition techniques which may not cope well withdialectal diversity.
However, the acoustic signal isalso available as input.
Written dialect ID has to dealwith non-standardized spellings that may occult realdialectal differences.
Moreover, some phonetic dis-tinctions cannot be expressed in orthographic writingsystems and limit the input cues in comparison withspoken dialect ID.This paper deals with written dialect ID, applied tothe Swiss German dialect area.
An important aspectof our model is its conception of the dialect area as acontinuum without clear-cut borders.
Our dialect IDmodel follows a bag-of-words approach based on theassumption that every dialectal word form is definedby a probability with which it may occur in eachgeographic area.
By combining the cues of all wordsof a sentence, it should be possible to obtain a fairlyreliable geographic localization of that sentence.The main challenge is to create a lexicon of dialectword forms and their associated probability maps.We start with a Standard German word list and usea set of phonetic, morphological and lexical rulesto obtain the Swiss German forms.
These rules aremanually extracted from a linguistic atlas.
This lin-guistic atlas of Swiss German dialects is the result ofdecades-long empirical fieldwork.This paper is organized as follows.
We start withan overview of relevant research (Section 2) andpresent the characteristics of the Swiss German di-alect area (Section 3).
Section 4 deals with the im-plementation of word transformation rules and thecorresponding extraction of probability maps fromthe linguistic atlas of German-speaking Switzerland.We present our dialect ID model in Section 5 anddiscuss its performance in Section 6 by relating it toa baseline n-gram model.11512 Related workVarious language identification methods have beenproposed in the last three decades.
Hughes et al(2006) and R?ehu?r?ek and Kolkus (2009) provide re-cent overviews of different approaches.
One ofthe simplest and most popular approaches is basedon character n-gram sequences (Cavnar and Tren-kle, 1994).
For each language, a character n-gramlanguage model is learned, and test segments arescored by all available language models and labeledwith the best scoring language model.
Related ap-proaches involve more sophisticated learning tech-niques (feature-based models, SVM and other kernel-based methods).A completely different approach relies on the iden-tification of entire high-frequency words in the testsegment (Ingle, 1980).
Other models have proposedto use morpho-syntactic information.Dialect ID has usually been studied from a speechprocessing point of view.
For instance, Biadsy etal.
(2009) classify speech material from four Arabicdialects plus Modern Standard Arabic.
They first runa phone recognizer on the speech input and use theresulting transcription to build a trigram languagemodel.
Classification is done by minimizing the per-plexity of the trigram models on the test segment.An original approach to the identification of SwissGerman dialects has been taken by the Chochich?stli-Orakel.1 By specifying the pronunciation of ten pre-defined words, the web site creates a probability mapthat shows the likelihood of these pronunciations inthe Swiss German dialect area.
Our model is heavilyinspired by this work, but extends the set of cues tothe entire lexicon.As mentioned, the ID model is based on a largeSwiss German lexicon.
Its derivation from a StandardGerman lexicon can be viewed as a case of lexiconinduction.
Lexicon induction methods for closelyrelated languages using phonetic similarity have beenproposed by Mann and Yarowsky (2001) and Schaferand Yarowsky (2002), and applied to Swiss Germandata by Scherrer (2007).The extraction of digital data from hand-drawn di-alectological maps is a time-consuming task.
There-fore, the data should be made available for differ-ent uses.
Our Swiss German raw data is accessible1http://dialects.from.chon an interactive web page (Scherrer, 2010), andwe have proposed ideas for reusing this data formachine translation and dialect parsing (Scherrerand Rambow, 2010).
An overview of digital dialec-tological maps for other languages is available onhttp://www.ericwheeler.ca/atlaslist.3 Swiss German dialectsThe German-speaking area of Switzerland encom-passes the Northeastern two thirds of the Swiss ter-ritory, and about two thirds of the Swiss populationdefine (any variety of) German as their first language.In German-speaking Switzerland, dialects are usedin speech, while Standard German is used nearly ex-clusively in written contexts (diglossia).
It followsthat all (adult) Swiss Germans are bidialectal: theymaster their local dialect and Standard German.
Inaddition, they usually have no difficulties understand-ing Swiss German dialects other than their own.Despite the preference for spoken dialect use, writ-ten dialect data has been produced in the form ofdialect literature and transcriptions of speech record-ings made for scientific purposes.
More recently,written dialect has been used in electronic media likeblogs, SMS, e-mail and chatrooms.
The AlemannicWikipedia contains about 6000 articles, among whichmany are written in a Swiss German dialect.2 How-ever, all this data is very heterogeneous in terms ofthe dialects used, spelling conventions and genre.4 Georeferenced word transformationrulesThe key component of the proposed dialect ID modelis an automatically generated list of Swiss Germanword forms, each of which is associated with amap that specifies its likelihood of occurrence overGerman-speaking Switzerland.
This word list is gen-erated with the help of a set of transformation rules,taking a list of Standard German words as a start-ing point.
In this section, we present the differenttypes of rules and how they can be extracted from adialectological atlas.2http://als.wikipedia.org; besides Swiss German, theAlemannic dialect group encompasses Alsatian, South-West Ger-man Alemannic and Vorarlberg dialects of Austria.11524.1 OrthographyOur system generates written dialect words accordingto the Dieth spelling conventions without diacritics(Dieth, 1986).3 These are characterized by a transpar-ent grapheme-phone correspondence and are widelyused by dialect writers.
However, they are by nomeans enforced or even taught.This lack of standardization is problematic for di-alect ID.
We have noted two major types of deviationsfrom the Dieth spelling conventions in our data.
First,Standard German orthography may unduly influencedialect spelling.
For example, spiele is modelled af-ter Standard German spielen ?to play?, although thevowel is a short monophthong in Swiss German andshould thus be written spile (ie represents a diph-thong in Dieth spelling).
Second, dialect writers donot always distinguish short and long vowels, whilethe Dieth conventions always use letter doubling toindicate vowel lengthening.
Future work will incor-porate these fluctuations directly into the dialect IDmodel.Because of our focus on written dialect, the follow-ing discussion will be based on written representa-tions, but IPA equivalents are added for convenience.4.2 Phonetic rulesOur work is based on the assumption that many wordsshow predictable phonetic differences between Stan-dard German and the different Swiss German dialects.Hence, in many cases, it is not necessary to explicitlymodel word-to-word correspondences, but a set ofphonetic rules suffices to correctly transform words.For example, the word-final sequence nd [nd?]
(asin Standard German Hund ?dog?4) is maintained inmost Swiss German dialects.
However, it has to betransformed to ng [N] in Berne dialect, to nn [n]in Fribourg dialect, and to nt [nt] in Valais and Uridialects.This phenomenon is captured in our system by fourtransformation rules nd?
nd, nd?
ng, nd?
nn andnd?
nt.
Each rule is georeferenced, i.e.
linked to3Of course, these spelling conventions make use of umlautslike in Standard German.
There is another variant of the Di-eth conventions that uses additional diacritics for finer-grainedphonetic distinctions.4Standard German nd is always pronounced [nt] following ageneral final devoicing rule; we neglect that artifact as we relyonly on graphemic representations.a probability map that specifies its validity in everygeographic point.
These four rules capture one singlelinguistic phenomenon: their left-hand side is thesame, and they are geographically complementary.Some rules apply uniformly to all Swiss Ger-man dialects (e.g.
the transformation st [st]?
scht[St]).
These rules do not immediately contribute tothe dialect identification task, but they help to ob-tain correct Swiss German forms that contain otherphonemes with better localization potential.More information about the creation of the proba-bility maps is given in Sections 4.5 and 4.6.4.3 Lexical rulesSome differences at the word level cannot be ac-counted for by pure phonetic alternations.
One reasonare idiosyncrasies in the phonetic evolution of highfrequency words (e.g.
Standard German und ?and?is reduced to u in Bern dialect, where the phoneticrules would rather suggest *ung).
Another reason isthe use of different lexemes altogether (e.g.
StandardGerman immer ?always?
corresponds to geng, immer,or all, depending on the dialect).
We currently uselexical rules mainly for function words and irregularverb stems.4.4 Morphological rulesThe transformation process from inflected StandardGerman word forms to inflected Swiss German wordforms is done in two steps.
First, the word stem isadapted with phonetic or lexical rules, and then, theaffixes are generated according to the morphologicalfeatures of the word.Inflection markers also provide dialect discrimina-tion potential.
For example, the verbal plural suffixesoffer a surprisingly rich (and diachronically stable)interdialectal variation pattern.4.5 The linguistic atlas SDSOne of the largest research projects in Swiss Germandialectology has been the elaboration of the Sprachat-las der deutschen Schweiz (SDS), a linguistic atlasthat covers phonetic, morphological and lexical dif-ferences of Swiss German dialects.
Data collectionand publication were carried out between 1939 and1997 (Hotzenk?cherle et al, 1962-1997).
Linguis-tic data were collected in about 600 villages (in-quiry points) of German-speaking Switzerland, and1153resulted in about 1500 published maps (see Figure 1for an example).Each map represents a linguistic phenomenon thatpotentially yields a set of transformation rules.
Forour experiments, we selected a subset of the maps ac-cording to the perceived importance of the describedphenomena.
There is no one-to-one correspondencebetween maps and implemented phenomena, for sev-eral reasons.
First, some SDS maps represent in-formation that is best analyzed as several distinctphenomena.
Second, a set of maps may illustrate thesame phenomenon with different words and slightlydifferent geographic distributions.
Third, some mapsdescribe (especially lexical) phenomena that are be-coming obsolete and that we chose to omit.As a result, our rule base contains about 300 pho-netic rules covering 130 phenomena, 540 lexicalrules covering 250 phenomena and 130 morpholog-ical rules covering 60 phenomena.
We believe thiscoverage to be sufficient for the dialect ID task.4.6 Map digitization and interpolationRecall the nd -example used to illustrate the phoneticrules above.
Figure 1 shows a reproduction of theoriginal, hand-drawn SDS map related to this phe-nomenon.
Different symbols represent different pho-netic variants of the phenomenon.5 We will use thisexample in this section to explain the preprocessingsteps involved in the creation of georeferenced rules.In a first preprocessing step, the hand-drawn mapis digitized manually with the help of a geographicalinformation system.
The result is shown in Figure 2.To speed up this process, variants that are used in lessthan ten inquiry points are omitted.
(Many of thesesmall-scale variants likely have disappeared since thedata collection in the 1940s.)
We also collapse minorphonetic variants which cannot be distinguished inthe Dieth spelling system.The SDS maps, hand-drawn or digitized, are pointmaps.
They only cover the inquiry points, but do notprovide information about the variants used in otherlocations.
Therefore, a further preprocessing step in-terpolates the digitized point maps to obtain surfacemaps.
We follow Rumpf et al (2009) to create kerneldensity estimators for each variant.
This method is5We define a variant simply as a string that may occur on theright-hand side of a transformation rule.Figure 1: Original SDS map for the transformation ofword-final -nd.
The map contains four major linguisticvariants, symbolized by horizontal lines (-nd ), verticallines (-nt), circles (-ng), and triangles (-nn) respectively.Minor linguistic variants are symbolized by different typesof circles and triangles.Figure 2: Digitized equivalent of the map in Figure 1.Figure 3: Interpolated surface maps for the variants -nn(upper left), -ng (upper right), -nt (lower left) and -nd(lower right).
Black areas represent a probability of 1,white areas a probability of 0.1154less sensitive to outliers than simpler linear interpola-tion methods.6 The resulting surface maps are thennormalized such that at each point of the surface, theweights of all variants sum up to 1.
These normalizedweights can be interpreted as conditional probabili-ties of the corresponding transfer rule: p(r | t), wherer is the rule and t is the geographic location (repre-sented as a pair of longitude and latitude coordinates)situated in German-speaking Switzerland.
(We callthe set of all points in German-speaking SwitzerlandGSS.)
Figure 3 shows the resulting surface maps foreach variant.
Surface maps are generated with a reso-lution of one point per square kilometer.As mentioned above, rules with a common left-hand side are grouped into phenomena, such that atany given point t ?
GSS, the probabilities of all rulesr describing a phenomenon Ph sum up to 1:?t?GSS?r?Php(r | t) = 15 The modelThe dialect ID system consists of a Swiss Germanlexicon that associates word forms with their geo-graphical extension (Section 5.1), and of a testingprocedure that splits a sentence into words, looksup their geographical extensions in the lexicon, andcondenses the word-level maps into a sentence-levelmap (Sections 5.2 to 5.4).5.1 Creating a Swiss German lexiconThe Swiss German word form lexicon is createdwith the help of the georeferenced transfer rules pre-sented above.
These rules require a lemmatized, POS-tagged and morphologically disambiguated StandardGerman word as an input and generate a set of di-alect word/map tuples: each resulting dialect wordis associated with a probability map that specifies itslikelihood in each geographic point.To obtain a Standard German word list, we ex-tracted all leaf nodes of the TIGER treebank (Brantset al, 2002), which are lemmatized and morphologi-cally annotated.
These data also allowed us to obtainword frequency counts.
We discarded words withone single occurrence in the TIGER treebank, as wellas forms that contained the genitive case or preterite6A comparison of different interpolation methods will be theobject of future work.tense attribute (the corresponding grammatical cate-gories do not exist in Swiss German dialects).The transfer rules are then applied sequentially oneach word of this list.
The notation w0??
wn repre-sents an iterative derivation leading from a StandardGerman word w0 to a dialectal word form wn by theapplication of n transfer rules of the type wi?
wi+1.The probability of a derivation corresponds to thejoint probability of the rules it consists of.
Hence,the probability map of a derivation is defined as thepointwise product of all rule maps it consists of:?t?GSSp(w0??
wn | t) =n?1?k=0p(wi?
wi+1 | t)Note that in dialectological transition zones, theremay be several valid outcomes for a given w0.The Standard German word list extracted fromTIGER contains about 36,000 entries.
The derivedSwiss German word list contains 560,000 wordforms, each of which is associated with a map thatspecifies its regional distribution.7 Note that propernouns and words tagged as ?foreign material?
werenot transformed.
Derivations that did not obtain aprobability higher than 0.1 anywhere (because ofgeographically incompatible transformations) werediscarded.5.2 Word lookup and dialect identificationAt test time, the goal is to compute a probability mapfor a text segment of unknown origin.8 As a prepro-cessing step, the segment is tokenized, punctuationmarkers are removed and all words are converted tolower case.The identification process can be broken down inthree levels:1.
The probability map of a text segment dependson the probability maps of the words containedin the segment.2.
The probability map of a word depends on theprobability maps of the derivations that yieldthe word.7Technically, we do not store the probability map, but thesequence of rule variants involved in the derivation.
The proba-bility map is restored from this rule sequence at test time.8The model does not require the material to be syntacticallywell-formed.
Although we use complete sentences to test thesystem, any sequence of words is accepted.11553.
The probability map of a derivation depends onthe probability maps of the rules it consists of.In practice, every word of a given text segment islooked up in the lexicon.
If this lookup does not suc-ceed (either because its Standard German equivalentdid not appear in the TIGER treebank, or because therule base lacked a relevant rule), the word is skipped.Otherwise, the lookup yields m derivations from mdifferent Standard German words.9 The lexicon al-ready contains the probability maps of the derivations(see 5.1), so that the third level does not need to bediscussed here.
Let us thus explain the first two levelsin more detail, in reverse order.5.3 Computing the probability map for a wordA dialectal word form may originate in different Stan-dard German words.
For example, the three deriva-tions sind [VAFIN]??
si (valid only in Western di-alects), sein [PPOSAT]??
si (in Western and Centraldialects), and sie [PPER]??
si (in the majority ofSwiss German dialects) all lead to the same dialectalform si.Our system does not take the syntactic contextinto account and therefore cannot determine whichderivation is the correct one.
We approximate bychoosing the most probable one in each geographiclocation.
The probability map of a Swiss Germanword w is thus defined as the pointwise maximum10of all derivations leading to w, starting with differentStandard German words w( j)0 :?t?GSSp(w | t) = maxjp(w( j)0??
w | t)This formula does not take into account the relativefrequency of the different derivations of a word.
Thismay lead to unintuitive results.
Consider the twoderivations der [ART]??
dr (valid only in Westerndialects) and Dr.
[NN]??
dr (valid in all dialects).The occurrence of the article dr in a dialect text is agood indicator for Western Swiss dialects, but it iscompletely masked by the potential presence of the9Theoretically, two derivations can originate at the sameStandard German word and yield the same Swiss German word,but nevertheless use different rules.
Our system handles suchcases as well, but we are not aware of such cases occurring withthe current rule base.10Note that these derivations are alternatives and not jointevents.
This is thus not a joint probability.abreviation Dr. in all dialects.
We can avoid this byweighting the derivations by the word frequency ofw0: the article der is much more frequent than theabreviation Dr. and is thus given more weight in theidentification task.
This weighting can be justifiedon dialectological grounds: frequently used wordstend to show higher interdialectal variation than rarewords.Another assumption in the above formula is thateach derivation has the same discriminative poten-tial.
Again, this is not true: a derivation that is validin only 10% of the Swiss German dialect area ismuch more informative than a derivation that is validin 95% of the dialect area.
Therefore, we proposeto weight each derivation by the proportional size ofits validity area.
The discriminative potential of aderivation d is defined as follows:11DP(d) = 1?
?t?GSS p(d | t)|GSS|The experiments in Section 6 will show the relativeimpact of these two weighting techniques and of thecombination of both with respect to the unweightedmap computation.5.4 Computing the probability map for asegmentThe probability of a text segment s can be defined asthe joint probability of all words w contained in thesegment.
Again, we compute the pointwise productof all word maps.
In contrast to 5.1, we performedsome smoothing in order to prevent erroneous wordderivations from completely zeroing out the proba-bilities.
We assumed a minimum word probability of?
= 0.1 for all words in all geographic points:?t?GSSp(s | t) =?w?smax(?
, p(w | t))Erroneous derivations were mainly due to non-implemented lexical exceptions.6 Experiments and results6.1 DataIn order to evaluate our model, we need texts an-notated with their gold dialect.
We have chosen touse the Alemannic Wikipedia as a main data source.11d is a notational abreviation for w0??
wn.1156Wikipedia name Abbr.
Pop.
SurfaceBaseldytsch BA 8% 1%B?rnd?tsch BE 17% 13%Seislert?tsch FR 2% 1%Ostschwizert?tsch OS 14% 8%Wallisertiitsch WS 2% 7%Z?rit?
?tsch ZH 22% 4%Table 1: The six dialect regions selected for our tests,with their annotation on Wikipedia and our abreviation.We also show the percentage of the German-speakingpopulation living in the regions, and the percentage of thesurface of the region relative to the entire country.Figure 4: The localization of the six dialect regions usedin our study.The Alemannic Wikipedia allows authors to writearticles in any dialect, and to annotate the articleswith their dialect.
Eight dialect categories containedmore than 10 articles; we selected six dialects for ourexperiments (see Table 1 and Figure 4).We compiled a test set consisting of 291 sentences,distributed across the six dialects according to theirpopulation size.
The sentences were taken from dif-ferent articles.
In addition, we created a developmentset consisting of 550 sentences (100 per dialect, ex-cept FR, where only 50 sentences were available).This development set was also used to train the base-line model discussed in section 6.2.In order to test the robustness of our model, wecollected a second set of texts from various web sitesother than Wikipedia.
The gold dialect of these textscould be identified through metadata.12 This informa-tion was checked for plausibility by the first author.The Web data set contains 144 sentences (again dis-12We mainly chose websites of local sports and music clubs,whose localization allowed to determine the dialect of their con-tent.Wikipedia WebDialect P R F P R FBA 34 61 44 27 61 37BE 78 51 61 51 47 49FR 28 71 40 10 33 15OS 63 64 64 50 38 43WS 58 100 74 14 33 20ZH 77 62 69 77 41 53W.
Avg.
62 46Table 2: Performances of the 5-gram model on Wikipediatest data (left) and Web test data (right).
The columnsrefer to precision, recall and F-measure respectively.
Theaverage is weighted by the relative population sizes of thedialect regions.tributed according to population size) and is thusroughly half the size of the Wikipedia test set.The Wikipedia data contains an average of 17.8words per sentence, while the Web data shows 14.9words per sentence on average.6.2 Baseline: N-gram modelTo compare our dialect ID model, we created a base-line system that uses a character-n-gram approach.This approach is fairly common for language ID andhas also been successfully applied to dialect ID (Bi-adsy et al, 2009).
However, it requires a certainamount of training data that may not be available forspecific dialects, and it is uncertain how it performswith very similar dialects.We trained 2-gram to 6-gram models for each di-alect with the SRILM toolkit (Stolcke, 2002), usingthe Wikipedia development corpus.
We scored eachsentence of the Wikipedia test set with each dialectmodel.
The predicted dialect was the one which ob-tained the lowest perplexity.13The 5-gram model obtained the best overall per-formance, and results on the Wikipedia test set weresurprisingly good (see Table 2, leftmost columns).14Note that in practice, 100% accuracy is not alwaysachievable; a sentence may not contain a sufficientlocalization potential to assign it unambiguously toone dialect.13We assume that all test sentences are written in one of thesix dialects.14All results represent percentage points.
We omit decimalplaces as all values are based on 100 or less data points.
We didnot perform statistical significance tests on our data.1157However, we suspect that these results are due tooverfitting.
It turns out that the number of SwissGerman Wikipedia authors is very low (typically,one or two active writers per dialect), and that ev-ery author uses distinctive spelling conventions andwrites about specific subjects.
For instance, mostZH articles are about Swiss politicians, while manyOS articles deal with religion and mysticism.
Ourhypothesis is thus that the n-gram model learns torecognize a specific author and/or topic rather thana dialect.
This hypothesis is confirmed on the Webdata set: the performances drop by 15 percentagepoints or more (same table, rightmost columns; theperformance drops are similar for n = [2..6]).In all our evaluations, the average F-measures forthe different dialects are weighted according to therelative population sizes of the dialect regions be-cause the size of the test corpus is proportional topopulation size (see Section 6.1).15We acknowledge that a training corpus of only 100sentences per dialect provides limited insight into theperformance of the n-gram approach.
We were ableto double the training corpus size with additionalWikipedia sentences.
With this extended corpus,the 4-gram model performed better than the 5-grammodel.
It yielded a weighted average F-measureof 79% on Wikipedia test data, but only 43% onWeb data.
The additional increase on Wikipedia data(+17% absolute with respect to the small trainingset), together with the decrease on Web data (?3%absolute) confirms our hypothesis of overfitting.
Anideal training corpus should thus contain data fromseveral sources per dialect.To sum up, n-gram models can yield good perfor-mance even with similar dialects, but require largeamounts of training data from different sources toachieve robust results.
For many small-scale dialects,such data may not be available.6.3 Our modelThe n-gram system presented above has no geo-graphic knowledge whatsoever; it just consists ofsix distinct language models that could be locatedanywhere.
In contrast, our model yields probability15Roughly, this weighting can be viewed as a prior (the proba-bility of the text being constant):p(dialect | text) = p(text | dialect)?
p(dialect)maps of German-speaking Switzerland.
In order toevaluate its performance, we thus had to determinethe geographic localization of the six dialect regionsdefined by the Wikipedia authors (see Table 1).
Wedefined the regions according to the respective can-ton boundaries and to the German-French languageborder in the case of bilingual cantons.
The result ofthis mapping is shown in Figure 4.The predicted dialect region of a sentence s is de-fined as the region in which the most probable pointhas a higher value than the most probable point inany other region:Region(s) = arg maxRegion(maxt?Regionp(s | t))Experiments were carried out for the four combi-nations of the two derivation-weighting techniquespresented in Section 5.3 and for the two test sets(Wikipedia and Web).
Results are displayed in Ta-bles 3 to 6.
The majority of FR sentences were mis-classified as BE, which reflects the geographic andlinguistic proximity of these regions.The tables show that frequency weighting helpson both corpora: the discriminative potential onlyslightly improves performance on the web corpus.Crucially, the two techniques are additive, so incombination, they yield the best overall results.
Incomparison with the baseline model, there is a per-formance drop of about 16 percent absolute onWikipedia data.
In contrast, our model is very ro-bust and outperforms the baseline model on the Webtest set by about 7 percent absolute.These results seem to confirm what we suggestedabove: that the n-gram model overfitted on the smallWikipedia training corpus.
Nevertheless, it is stillsurprising that our model has a lower performanceon Wikipedia than on Web data.
The reason for thisdiscrepancy probably lies in the spelling conventionsassumed in the transformation rules: it seems thatWeb writers are closer to these (implicit) spellingconventions than Wikipedia authors.
This may beexplained by the fact that many Wikipedia articlesare translations of existing Standard German articles,and that some words are not completely adapted totheir dialectal form.
Another reason could be thatWikipedia articles use a proportionally larger amountof proper nouns and low-frequency words which can-1158Wikipedia WebDialect P R F P R FBA 41 19 26 80 22 35BE 42 62 50 48 76 59FR 0 0 0 17 33 22OS 36 41 38 45 41 43WS 3 14 5 8 33 13ZH 65 33 44 62 37 46W.
Avg.
40 46Table 3: Performances of the word-based model usingunweighted derivation maps.Wikipedia WebDialect P R F P R FBA 50 33 40 57 22 32BE 47 60 53 60 79 68FR 0 0 0 0 0 0OS 29 31 30 46 50 48WS 11 29 15 17 33 22ZH 60 47 53 65 53 58W.
Avg.
44 53Table 4: Performances of the word-based model usingderivation maps weighted by word frequency.not be found in the lexicon and which therefore re-duce the localization potential of a sentence.However, one should note that the word-based di-alect ID model is not limited on the six dialect regionsused for evaluation here.
It can be used with any sizeand number of dialect regions of German-speakingSwitzerland.
This contrasts with the n-gram modelwhich has to be trained specifically on every dialectregion; in this case, the Swiss German Wikipediaonly contains two additional dialect regions with anequivalent amount of data.6.4 VariationsIn the previous section, we have defined the predicteddialect region as the one in which the most probablepoint (maximum) has a higher probability than themost probable point of any other region.
The resultssuggest that this metric penalizes small regions (BA,FR, ZH).
In these cases, it is likely that the mostprobable point is slightly outside the region, but thatthe largest part of the probability mass is still insidethe correct region.
Therefore, we tested another ap-proach: we defined the predicted dialect region as theone in which the average probability is higher thanWikipedia WebDialect P R F P R FBA 34 31 32 38 17 23BE 46 47 47 54 76 63FR 11 14 13 20 33 25OS 34 50 40 53 59 56WS 5 14 7 0 0 0ZH 47 27 34 75 43 55W.
Avg.
37 51Table 5: Performances of the word-based model usingderivation maps weighted by their discriminative potential.Wikipedia WebDialect P R F P R FBA 46 28 35 33 11 17BE 47 62 54 58 84 69FR 0 0 0 20 33 25OS 35 31 33 47 47 47WS 8 29 13 14 33 20ZH 63 53 58 66 51 58W.
Avg.
46 52Table 6: Performances using derivation maps weighted byword frequency and discriminative potential.the average probability in any other region:Region(s) = arg maxRegion(?t?Region p(s | t)|Region|)This metric effectively boosts the performance onthe smaller regions, but comes at a cost for largerregions (Table 7).
We also combined the two metricsby using the maximum metric for the three largerregions and the average metric for the three smallerones (the cutoff lies at 5% of the Swiss territory).This combined metric further improves the perfor-mance of our system while relying on an objectivemeasure of region surface.We believe that region surface as such is not socrucial for the metrics discussed above, but ratherserves as a proxy for linguistic heterogeneity.
Geo-graphically large regions like BE tend to have internaldialect variation, and averaging over all dialects inthe region leads to low figures.
In contrast, smallregions show a quite homogeneous dialect landscapethat may protrude over adjacent regions.
In this case,the probability peak is less relevant than the averageprobability in the entire region.
Future work will at-tempt to come up with more fine-grained measures of1159Wikipedia WebDialect Max Avg Cmb Max Avg CmbBA 35 32 32 17 43 43BE 54 39 54 69 54 69FR 0 7 7 25 11 11OS 33 23 33 47 49 47WS 13 13 13 20 31 20ZH 58 60 60 58 68 68W.
Avg.
46 40 47 52 55 58Table 7: Comparison of different evaluation metrics.
Allvalues refer to F-measures obtained with frequency anddiscriminative potential-weighted derivation maps.
Maxrefers to the Maximum metric as used in Table 6.
Avgrefers to the average metric, and Cmb is the combinationof both metrics depending on region surfaces.
The under-lined values in the Avg and Max columns represent thoseused for the Cmb metric.linguistic heterogeneity in order to test these claims.7 Future workIn our experiments, the word-based dialect identifi-cation model skipped about one third of all words(34% on the Wikipedia test set, 39% on the Webtest set) because they could not be found in the lex-icon.
While our model does not require completelexical coverage, this figure shows that the systemcan be improved.
We see two main possibilities ofimprovement.
First, the rule base can be extendedto better account for lexical exceptions, orthographicvariation and irregular morphology.
Second, a mixedapproach could combine the benefits of the word-based model with the n-gram model.
This wouldrequire a larger, more heterogeneous set of trainingmaterial for the latter in order to avoid overfitting.Additional training data could be extracted from theweb and automatically annotated with the currentmodel in a semi-supervised approach.In the evaluation presented above, the task con-sisted of identifying the dialect of single sentences.However, one often has access to longer text seg-ments, which makes our evaluation setup harderthan necessary.
This is especially important in situ-ations where a single sentence may not always con-tain enough discriminative material to assign it to aunique dialect.
Testing our dialect identification sys-tem on the paragraph or document level could thusprovide more realistic results.8 ConclusionIn this paper, we have compared two empirical meth-ods for the task of dialect identification.
The n-grammethod is based on the approach most commonlyused in NLP: it is a supervised machine learning ap-proach where training data of the type we need toprocess is annotated with the desired outcome of theprocessing.Our second approach ?
the main contribution ofthis paper ?
is quite different.
The empirical compo-nent consists in a collection of data (the SDS atlas)which is not of the type we want to process, but ratherembodies some features of the data we ultimatelywant to process.
We therefore analyze this data inorder to extract empirically grounded knowledge formore general use (the creation of the georeferencedrules), and then use this knowledge to perform thedialect ID task in conjunction with an unrelated datasource (the Standard German corpus).Our choice of method was of course related to thefact that few corpora, annotated or not, were avail-able for our task.
But beyond this constraint, wethink it may be well worthwhile for NLP tasks ingeneral to move away from a narrow machine learn-ing paradigm (supervised or not) and to considera broader set of empirical resources, sometimes re-quiring methods which are quite different from theprevalent ones.AcknowledgementsPart of this work was carried out during the first au-thor?s stay at Columbia University, New York, fundedby the Swiss National Science Foundation (grantPBGEP1-125929).ReferencesFadi Biadsy, Julia Hirschberg, and Nizar Habash.
2009.Spoken Arabic dialect identification using phonotacticmodeling.
In EACL 2009 Workshop on ComputationalApproaches to Semitic Languages, Athens.S.
Brants, S. Dipper, S. Hansen, W. Lezius, and G. Smith.2002.
The TIGER Treebank.
In Proceedings of theWorkshop on Treebanks and Linguistic Theories, So-zopol.W.
B. Cavnar and J. M. Trenkle.
1994.
N-gram basedtext categorization.
In Proceedings of SDAIR?94, LasVegas.1160Eugen Dieth.
1986.
Schwyzert?tschi Dial?ktschrift.Sauerl?nder, Aarau, 2nd edition.Rudolf Hotzenk?cherle, Robert Schl?pfer, Rudolf Tr?b,and Paul Zinsli, editors.
1962-1997.
Sprachatlas derdeutschen Schweiz.
Francke, Berne.Baden Hughes, Timothy Baldwin, Steven Bird, JeremyNicholson, and Andrew MacKinlay.
2006.
Recon-sidering language identification for written languageresources.
In Proceedings of LREC?06, Genoa.N.
Ingle.
1980.
A language identification table.
TechnicalTranslation International.Gideon S. Mann and David Yarowsky.
2001.
Multipathtranslation lexicon induction via bridge languages.
InProceedings of NAACL?01, Pittsburgh.Radim R?ehu?r?ek and Milan Kolkus.
2009.
Languageidentification on the web: Extending the dictionarymethod.
In Computational Linguistics and IntelligentText Processing ?
Proceedings of CICLing 2009, pages357?368, Mexico.
Springer.Jonas Rumpf, Simon Pickl, Stephan Elspa?, WernerK?nig, and Volker Schmidt.
2009.
Structural analysisof dialect maps using methods from spatial statistics.Zeitschrift f?r Dialektologie und Linguistik, 76(3).Charles Schafer and David Yarowsky.
2002.
Inducingtranslation lexicons via diverse similarity measures andbridge languages.
In Proceedings of CoNLL?02, pages146?152, Taipei.Yves Scherrer and Owen Rambow.
2010.
Natural lan-guage processing for the Swiss German dialect area.
InProceedings of KONVENS?10, Saarbr?cken.Yves Scherrer.
2007.
Adaptive string distance measuresfor bilingual dialect lexicon induction.
In Proceedingsof ACL?07, Student Research Workshop, pages 55?60,Prague.Yves Scherrer.
2010.
Des cartes dialectologiquesnum?ris?es pour le TALN.
In Proceedings of TALN?10,Montr?al.Andreas Stolcke.
2002.
SRILM ?
an extensible languagemodeling toolkit.
In Proceedings of ICSLP?02, pages901?904, Denver.1161
