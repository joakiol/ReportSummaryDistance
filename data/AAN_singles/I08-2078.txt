An Online Cascaded Approach to Biomedical Named Entity Recognition ?Shing-Kit Chan, Wai Lam, Xiaofeng YuDepartment of Systems Engineering and Engineering ManagementThe Chinese University of Hong KongShatin, Hong Kong{skchan, wlam, xfyu}@se.cuhk.edu.hkAbstractWe present an online cascaded approach tobiomedical named entity recognition.
Thisapproach uses an online training methodto substantially reduce the training time re-quired and a cascaded framework to relaxthe memory requirement.
We conduct de-tailed experiments on the BioNLP datasetfrom the JNLPBA shared task and com-pare the results with other systems and pub-lished works.
Our experimental results showthat our approach achieves comparable per-formance with great reductions in time andspace requirements.1 IntroductionIn the biomedical domain, the vast amount of dataand the great variety of induced features are two ma-jor bottlenecks for further natural language process-ing on the biomedical literature.
In this paper, weinvestigate the biomedical named entity recognition(NER) problem.
This problem is particularly impor-tant because it is a necessary pre-processing step inmany applications.This paper addresses two main issues that arisefrom biomedical NER.
?The work described in this paper is substantially supportedby grants from the Research Grant Council of the Hong KongSpecial Administrative Region, China (Project Nos: CUHK4179/03E and CUHK4193/04E) and the Direct Grant of theFaculty of Engineering, CUHK (Project Codes: 2050363 and2050391).
This work is also affiliated with the Microsoft-CUHK Joint Laboratory for Human-centric Computing and In-terface Technologies.Long Training Time: Traditional approachesthat depend on the maximum likelihood trainingmethod are slow even with large-scale optimiza-tion methods such as L-BFGS.
This problem wors-ens with the sheer volume and growth rate of thebiomedical literature.
In this paper, we propose theuse of an online training method that greatly reducestraining time.Large Memory Space: The total number offeatures used to extract named entities from docu-ments is very large.
To extract biomedical namedentities, we often need to use extra features in addi-tion to those used in general-purpose domains, suchas prefix, suffix, punctuation, and more orthographicfeatures.
We need a correspondingly large mem-ory space for processing, exacerbating the first issue.We propose to alleviate this problem by employinga cascaded approach that divides the NER task intoa segmentation task and a classification task.The overall approach is the online cascaded ap-proach, which is described in the remaining sectionsof this paper: Section 2 describes the general modelthat is used to address the above issues.
We addressthe issue of long training time in Section 3.
The is-sue of large memory space is addressed in Section 4.Experimental results and analysis are presented inSection 5.
We discuss related work in Section 6 andconclude with Section 7.2 Model DescriptionsOur proposed model is similar to a conditional ran-dom field in a sequence labeling task, but we avoiddirectly dealing with the probability distribution.
Weuse a joint feature representation F(x,y) for each595input sequence x and an arbitrary output sequencey, as follows.F(x,y) =|x|?i=1f(x,y, i) (1)where each f(x,y, i) is a local feature function atposition i.
For example, in a segmentation task usingthe IOB2 notation, the k-th local feature in f(x,y, i)can be defined asfk(x,y, i) =??
?1 if xi is the word ?boy?,and yi is the label ?B?0 otherwise(2)With parameter w, the best output sequence y?
foran input sequence x can be found by calculating thebest score:y?
= argmaxy?w ?
F(x,y?)
(3)3 Online TrainingWe propose to estimate the parameter w in an onlinemanner.
In particular, we use the online passive-aggressive algorithm (Crammer et al, 2006).
Pa-rameters are estimated by margin-based training,which chooses the set of parameters that attemptsto make the ?margin?
on each training instance(xt,yt) greater than a predefined value ?,w ?
F(xt,yt) ?
w ?
F(xt,y?)
?
?
?y?
6= yt(4)A hinge loss function ?
(w;xt) is defined as?
(w;xt) ={0 if ?t ?
??
?
?t otherwise(5)where ?t is the margin on input xt defined as?t = w ?
F(xt,yt) ?
maxy?
6=ytw ?
F(xt,y?)
(6)In online training, the parameter w is updated itera-tively.
Formally speaking, in the t-th iteration withthe parameter wt and the training instance xt, wetry to solve the following optimization problem.wt+1 = argminw12?w ?
wt?2 + C?
(7)such that ?
(w; (xt,yt)) ?
?where C > 0 is a user-defined aggressiveness pa-rameter and ?
?
0 is a slack term for the trainingdata when it is not linearly-separable.
C controlsthe penalty of the slack term and the aggressivenessof each update step.
A larger C implies a more ag-gressive update and hence a higher tendency to over-fit.
The solution to Problem (7) iswt+1 = wt ?
?t[F(xt,yt) ?
F(xt, y?t)](8)where ?t = min{C, ?
(wt; (xt,yt))?F(xt,yt) ?
F(xt, y?t)?2}(9)The passiveness of this algorithm comes from thefact that the parameter wt is not updated when thehinge loss for xt is zero.
It can be proved that the rel-ative loss bound on the training data (and which alsobounds the number of prediction mistakes on thetraining data) cannot be much worse than the bestfixed parameter chosen in hindsight.
See (Crammeret al, 2006) for a detailed proof.Following most of the work on margin-basedtraining, in this paper we choose ?
to be a functionof the correct output sequence y and the predictedoutput sequence y?.?
(y, y?)
={0 if y = y?
?|y|i=1[[yi 6= y?i]] otherwise(10)where [[z]] is 1 if z is true, and 0 otherwise.The major computation difficulty in this onlinetraining comes from Equation (3).
Finding the bestoutput y?
is in general an intractable task.
We fol-low the usual first-order independence assumptionmade in a linear-chained CRF (Lafferty et al, 2001)model and calculate the best score using the Viterbialgorithm.4 Cascaded FrameworkWe divide the NER task into a segmentation taskand a classification task.
In the segmentation task,a sentence x is segmented, and possible segmentsof biomedical named entities are identified.
In theclassification task, the identified segments are clas-sified into one of the possible named entity types orrejected.596In other words, in the segmentation task, the sen-tence x are segmented byy?s = argmaxy?ws ?
Fs(x,y?)
(11)where Fs(?)
is the set of segment features, and ws isthe parameter for segmentation.In the classification task, the segments (which canbe identified by ys) in a sentence x are classified byy?c = argmaxy?wc ?
Fc(x,ys,y?)
(12)where Fc(?)
is the set of classification features, andwc is the parameter for classification.In this cascaded framework, the number of possi-ble labels in the segmentation task is Ns.
For exam-ple, Ns = 3 in the IOB2 notation.
In the classifi-cation task, the number of possible labels is Nc + 1,which is the number of entity types and one label for?Other?.
Following the first-order independence as-sumption, the maximum total number of features inthe two tasks is O(max(N2s ,N2c )), which is muchsmaller than the single-phase approach in which thetotal number of features is O((NsNc)2).Another potential advantage of dividing the NERtask into two tasks is that it allows greater flexibilityin choosing an appropriate set of features for eachtask.
In fact, adding more features may not nec-essarily increase performance.
(Settles, 2004) re-ported that a system using a subset of features out-performed one using a full set of features.5 ExperimentsWe conducted our experiments on the GENIA cor-pus (Kim et al, 2003) provided in the JNLPBA (Kimet al, 2004) shared task1.
There are 2,000 MED-LINE abstracts in the GENIA corpus with namedentities tagged in the IOB2 format.
There are 18,546sentences and 492,551 words in the training set, and3,856 sentences and 101,039 words in the evalua-tion set.
The line indicating the MEDLINE abstractID boundary information is not used in our experi-ments.
Each word is tagged with ?B-X?, ?I-X?, or?O?
to indicate that the word is at the ?beginning?
(B) or ?inside?
(I) of a named entity of type X, or1http://research.nii.ac.jp/?collier/workshops/JNLPBA04st.htmSystem F1(Zhou and Su, 2004) 72.55Online Cascaded 72.16(Okanohara et al, 2006) 71.48(Kim et al, 2005) 71.19(Finkel et al, 2004) 70.06(Settles, 2004) 69.80Table 1: Comparisons with other systems on overallperformance (in percentage).?outside?
(O) of a named entity.
The named entitytypes are: DNA, RNA, cell line, cell type, and pro-tein.5.1 FeaturesThe features used in our experiments mainly fol-low the work of (Settles, 2004) and (Collins, 2001).For completeness, we briefly describe the featureshere.
They include word features, orthographic fea-tures, parts-of-speech (POS), and two lexicons.
Theword features include unigram, bigram, and trigram(e.g.
the previous word, the next word, and theprevious two words), whereas the orthographic fea-tures include capital letter, dash, punctuation, andword length.
Word class (WC) features are alsoadded, which replace a capital letter with ?A?, alower case letter with ?a?, a digit with ?0?, and allother characters with ?
?.
Similar brief word class(BWC) features are added by collapsing all of theconsecutive identical characters in the word classfeatures into one character.
For example, for theword NF-kappa, WC = AA aaaaa, and BWC= A a.
These are listed in Tables 2 and 3.
The POSfeatures are added by the GENIA tagger2.All of these features except for the prefix/suffixfeatures are applied to the neighborhood window[i ?
1, i + 1] for every word.
Two lexicons for celllines and genes are drawn from two online publicdatabases: the Cell Line Database3 and the BBID4.The prefix/suffix and lexicon features are applied toposition i only.
All of the above features are com-2http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/tagger/3http://www.biotech.ist.unige.it/cldb/cname-tz.html4http://bbid.grc.nia.nih.gov/bbidgene.html597Unigram (w?2), (w?1), (w0),(w1), (w2)Bigram (w?2 w?1), (w?1 w0),(w0 w1), (w1 w2)Trigram (w?2 w?1 w0),(w?1 w0 w1),(w0 w1 w2)Table 2: Word features used in the experiment: w0is the current word, w?1 is the previous word, etc.Word features as in Table 2Prefix/suffix Up to a length of 5Word Class WCBrief Word Class BWCCapital Letter ?
[A-Z][a-z][A-Z]{2,}[a-z]+[A-Z]+Digit [0-9]+?[?0-9]*[0-9][?0-9]*$?[?0-9]*[0-9][0-9][?0-9]*$?[0-9]+$[0-9]+[,.][0-9,.
]+[A-Za-z]+[0-9]+[0-9]+[A-Za-z]+Dash [-]+?
[-]+[-]+$Punctuation [,;:?!-+?
"\/]+Word length length of the current word xiTable 3: Features used in the JNLPBA experiment.The features for Capital Letter, Digit, Dash, andPunctuation are represented as regular expressions.bined with the previous label yi?1 and the currentlabel yi to form the final set of features.In the segmentation task, only three labels (i.e.
B,I, O) are needed to represent the segmentation re-sults.
In the classification task, the possible labelsare the five entity types and ?Other?.
We also addthe segmentation results as features in the classifica-tion task.5.2 ResultsWe tried different methods to extract the named en-tities from the JNLPBA dataset for comparisons.These programs were developed based on the samebasic framework.
All of the experiments were runon a Unix machine with a 2.8 GHz CPU and 16 GBRAM.
In particular, the CRF trained by maximum-likelihood uses the L-BFGS algorithm (Liu and No-cedal, 1989), which converges quickly and givesa good performance on maximum entropy mod-els (Malouf, 2002; Sha and Pereira, 2003).
We com-pare our experimental results in several dimensions.Training Time: Referring to Table 4, the train-ing time of the online cascaded approach is substan-tially shorter than that of all of the other approaches.In the single-phase approach, training a CRF bymaximum likelihood (ML) using the L-BFGS algo-rithm is the slowest and requires around 28 hours.The online method greatly reduces the training timeto around two hours, which is 14 times faster.
Byemploying a two-phase approach, the training timeis further reduced to half an hour.Memory Requirement: Table 4 shows the num-ber of features that are required by the differentmethods.
For methods that use the single-phase ap-proach, because the full set of features (See Sec-tion 4) is too big for practical experiments on ourmachine, we need to set a higher cutoff value to re-duce the number of features.
With a cutoff of 20(i.e.
only features that occur more than 20 times areused), the number of features can still go up to about8 million.
However, in the two-phase approach, evenwith a smaller cutoff of 5, the number of features canstill remain at about 8 million.F1-measure: Table 4 shows the F1-measure inour experiments, and Table 1 compares our resultswith different systems in the JNLPBA shared tasksand other published works5.
Our performance of thesingle-phase CRF with maximum likelihood train-ing is 69.44%, which agrees with (Settles, 2004)who also uses similar settings.
The single-phase on-line method increases the performance to 71.17%.By employing a cascaded framework, the perfor-mance is further increased to 72.16%, which can beregarded as comparable with the best system in theJNLPBA shared task.6 Related WorkThe online training approach used in this paperis based on the concept of ?margin?
(Cristianini,2001).
A pioneer work in online training is theperceptron-like algorithm used in training a hiddenMarkov model (HMM) (Collins, 2002).
(McDonald5We are aware of the high F1 in (Vishwanathan et al, 2006).We contacted the author and found that their published resultmay be incomplete.598Experiments no.
of features training time F1 rel.
err.red.
on F1single-phase CRF + ML 8,004,392 1699 mins 69.44 ?CRF + Online 8,004,392 116 mins 71.17 5.66%two-phase Online seg: 2,356,590 14 + 15 72.16 8.90%+ Cascaded class: 8,278,794 = 29 minsTable 4: The number of features, training time, and F1 that are used in our experiments.
The cutoff thresh-olds for the single-phase CRFs are set to 20, whereas that of the online cascaded approach is set to 5 in bothsegmentation and classification.
The last column shows the relative error reductions on F1 (compared toCRF+ML).Experiments R P F1Segmentation 80.13 73.68 76.77Classification 92.75 92.76 92.76Table 5: Results of the individual task in the onlinecascaded approach.
The F1 of the classification taskis 92.76% (which is based on the fully correct seg-mented testing data).et al, 2005) also proposed an online margin-basedtraining method for parsing.
This type of trainingmethod is fast and has the advantage that it doesnot need to form the dual problem as in SVMs.
Adetailed description of the online passive-aggressivealgorithm used in this paper and its variants canbe found in (Crammer et al, 2006).
The MarginInfused Relaxed Algorithm (MIRA), which is theancestor of the online passive-aggressive algorithmand mainly for the linearly-separable case, can befound in (Crammer and Singer, 2003).
(Kim et al, 2005) uses a similar two-phaseapproach but they need to use rule-based post-processing to correct the final results.
Their CRFsare trained on a different dataset that contains all ofthe other named entities such as lipid, multi cell, andother organic compound.
Table 1 shows the com-parisons of the final results.In the JNLPBA shared task, eight NER systemswere used to extract five types of biomedical namedentities.
The best system (Zhou and Su, 2004) uses?deep knowledge?, such as name alias resolution,cascaded entity name resolution, abbreviation res-olution, and in-domain POS.
Our approach is rela-tively simpler and uses a unified model to accom-plish the cascaded tasks.
It also allows other post-processing tasks to enhance performance.7 ConclusionWe have presented an online cascaded approach tobiomedical named entity recognition.
This approachsubstantially reduces the training time required andrelaxes the memory requirement.
The experimen-tal results show that our approach achieves perfor-mance comparable to the state-of-the-art system.ReferencesMichael Collins.
2001.
Ranking algorithms for named-entity extraction: boosting and the voted perceptron.In ACL ?02: Proceedings of the 40th Annual Meetingon Association for Computational Linguistics, pages489?496.Michael Collins.
2002.
Discriminative training methodsfor hidden markov models: theory and experimentswith perceptron algorithms.
In EMNLP ?02: Proceed-ings of the ACL-02 conference on Empirical methodsin natural language processing, pages 1?8.Koby Crammer and Yoram Singer.
2003.
Ultraconserva-tive online algorithms for multiclass problems.
Jour-nal of Machine Learning Research, 3:951?991.Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer.
2006.
Online passive-aggressive algorithms.
Journal of Machine LearningResearch, 7:551?585.Nello Cristianini.
2001.
Support vector and ker-nel machines.
ICML tutorial.
Available athttp://www.support-vector.net/icml-tutorial.pdf.J.
Finkel, S. Dingare, H. Nguyen, M. Nissim, C. Man-ning, and G. Sinclair.
2004.
Exploiting context forbiomedical entity recognition: from syntax to the web.In Proceedings of the International Joint Workshop on599Natural Language Processing in Biomedicine and itsApplications (NLPBA), pages 88?91.J.d.
Kim, T. Ohta, Y. Tateisi, and J. Tsujii.
2003.
Ge-nia corpus - a semantically annotated corpus for bio-textmining.
Bioinformatics (Supplement: EleventhInternational Conference on Intelligent Systems forMolecular Biology), 19:180?182.J.
Kim, T. Ohta, Y. Tsuruoka, Y. Tateisi, and N. Collier.2004.
Introduction to the bio-entity recognition task atJNLPBA.
In N. Collier, P. Ruch, and A. Nazarenko,editors, Proceedings of the International Joint Work-shop on Natural Language Processing in Biomedicineand its Applications (JNLPBA), Geneva, Switzerland,pages 70?75, August 28?29.
held in conjunction withCOLING?2004.Seonho Kim, Juntae Yoon, Kyung-Mi Park, and Hae-Chang Rim.
2005.
Two-phase biomedical named en-tity recognition using a hybrid method.
In Proceedingsof The Second International Joint Conference on Nat-ural Language Processing (IJCNLP-05), pages 646?657.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional random fields: Probabilistic modelsfor segmenting and labeling sequence data.
In Proc.18th International Conf.
on Machine Learning, pages282?289.D.
C. Liu and J. Nocedal.
1989.
On the limited mem-ory bfgs method for large scale optimization.
Math.Program., 45(3):503?528.Robert Malouf.
2002.
A comparison of algorithms formaximum entropy parameter estimation.
In Proceed-ings of CoNLL-2002, pages 49?55.Ryan McDonald, Koby Crammer, and Fernando Pereira.2005.
Online large-margin training of dependencyparsers.
In ACL ?05: Proceedings of the 43rd An-nual Meeting on Association for Computational Lin-guistics, pages 91?98.Daisuke Okanohara, Yusuke Miyao, Yoshimasa Tsu-ruoka, and Jun?ichi Tsujii.
2006.
Improving the scal-ability of semi-markov conditional random fields fornamed entity recognition.
In ACL ?06: Proceedings ofthe 21st International Conference on ComputationalLinguistics and the 44th annual meeting of the ACL,pages 465?472.B.
Settles.
2004.
Biomedical named entity recognitionusing conditional random fields and rich feature sets.In Proceedings of the International Joint Workshop onNatural Language Processing in Biomedicine and itsApplications (NLPBA), pages 104?107.Fei Sha and Fernando Pereira.
2003.
Shallow parsingwith conditional random fields.
In NAACL ?03: Pro-ceedings of the 2003 Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics on Human Language Technology, pages 134?141.S.
V. N. Vishwanathan, Nicol N. Schraudolph, Mark W.Schmidt, and Kevin P. Murphy.
2006.
Acceleratedtraining of conditional random fields with stochasticgradient methods.
In ICML ?06: Proceedings of the23rd international conference on Machine learning,pages 969?976.GuoDong Zhou and Jian Su.
2004.
Exploring deepknowledge resources in biomedical name recognition.In COLING 2004 International Joint workshop onNatural Language Processing in Biomedicine and itsApplications (NLPBA/BioNLP) 2004, pages 99?102.600
