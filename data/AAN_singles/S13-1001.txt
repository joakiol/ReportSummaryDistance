Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conferenceand the Shared Task, pages 1?10, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational LinguisticsTowards a Formal Distributional Semantics:Simulating Logical Calculi with TensorsEdward GrefenstetteUniversity of OxfordDepartment of Computer ScienceWolfson Building, Parks RoadOxford OX1 3QD, UKedward.grefenstette@cs.ox.ac.ukAbstractThe development of compositional distribu-tional models of semantics reconciling the em-pirical aspects of distributional semantics withthe compositional aspects of formal seman-tics is a popular topic in the contemporary lit-erature.
This paper seeks to bring this rec-onciliation one step further by showing howthe mathematical constructs commonly usedin compositional distributional models, suchas tensors and matrices, can be used to sim-ulate different aspects of predicate logic.This paper discusses how the canonical iso-morphism between tensors and multilinearmaps can be exploited to simulate a full-blownquantifier-free predicate calculus using ten-sors.
It provides tensor interpretations of theset of logical connectives required to modelpropositional calculi.
It suggests a variantof these tensor calculi capable of modellingquantifiers, using few non-linear operations.It finally discusses the relation between thesevariants, and how this relation should consti-tute the subject of future work.1 IntroductionThe topic of compositional distributional semanticshas been growing in popularity over the past fewyears.
This emerging sub-field of natural languagesemantic modelling seeks to combine two seeminglyorthogonal approaches to modelling the meaning ofwords and sentences, namely formal semantics anddistributional semantics.These approaches, summarised in Section 2, dif-fer in that formal semantics, on the one hand, pro-vides a neatly compositional picture of natural lan-guage meaning, reducing sentences to logical rep-resentations; one the other hand, distributional se-mantics accounts for the ever-present ambiguity andpolysemy of words of natural language, and pro-vides tractable ways of learning and comparingword meanings based on corpus data.Recent efforts, some of which are briefly re-ported below, have been made to unify both ofthese approaches to language modelling to pro-duce compositional distributional models of seman-tics, leveraging the learning mechanisms of distri-butional semantics, and providing syntax-sensitiveoperations for the production of representations ofsentence meaning obtained through combination ofcorpus-inferred word meanings.
These efforts havebeen met with some success in evaluations suchas phrase similarity tasks (Mitchell and Lapata,2008; Mitchell and Lapata, 2009; Grefenstette andSadrzadeh, 2011; Kartsaklis et al 2012), sentimentprediction (Socher et al 2012), and paraphrase de-tection (Blacoe and Lapata, 2012).While these developments are promising withregard to the goal of obtaining learnable-yet-structured sentence-level representations of lan-guage meaning, part of the motivation for unifyingformal and distributional models of semantics hasbeen lost.
The compositional aspects of formal se-mantics are combined with the corpus-based empir-ical aspects of distributional semantics in such mod-els, yet the logical aspects are not.
But it is theselogical aspects which are so appealing in formal se-mantic models, and therefore it would be desirableto replicate the inferential powers of logic within1compositional distributional models of semantics.In this paper, I make steps towards addressing thislost connection with logic in compositional distri-butional semantics.
In Section 2, I provide a briefoverview of formal and distributional semantic mod-els of meaning.
In Section 3, I give mathemati-cal foundations for the rest of the paper by intro-ducing tensors and tensor contraction as a way ofmodelling multilinear functions.
In Section 4, I dis-cuss how predicates, relations, and logical atomsof a quantifier-free predicate calculus can be mod-elled with tensors.
In Section 5, I present tenso-rial representations of logical operations for a com-plete propositional calculus.
In Section 6, I discussa variant of the predicate calculus from Section 4aimed at modelling quantifiers within such tensor-based logics, and the limits of compositional for-malisms based only on multilinear maps.
I con-clude, in Section 7, by suggesting directions for fur-ther work based on the contents of this paper.This paper does not seek to address the questionof how to determine how words should be trans-lated into predicates and relations in the first place,but rather shows how such predicates and relationscan be modelled using multilinear algebra.
As such,it can be seen as a general theoretical contributionwhich is independent from the approaches to com-positional distributional semantics it can be appliedto.
It is directly compatible with the efforts of Co-ecke et al(2010) and Grefenstette et al(2013), dis-cussed below, but is also relevant to any other ap-proach making use of tensors or matrices to encodesemantic relations.2 Related workFormal semantics, from the Montagovian school ofthought (Montague, 1974; Dowty et al 1981), treatsnatural languages as programming languages whichcompile down to some formal language such as apredicate calculus.
The syntax of natural languages,in the form of a grammar, is augmented by seman-tic interpretations, in the form of expressions froma higher order logic such as the lambda-beta calcu-lus.
The parse of a sentence then determines thecombinations of lambda-expressions, the reductionof which yields a well-formed formula of a predi-cate calculus, corresponding to the semantic repre-sentation of the sentence.
A simple formal semanticmodel is illustrated in Figure 1.Syntactic Analysis Semantic InterpretationS?
NP VP [[VP]]([[NP]])NP?
cats, milk, etc.
[[cats]], [[milk]], .
.
.VP?
Vt NP [[Vt]]([[NP]])Vt?
like, hug, etc.
?yx.
[[like]](x, y), .
.
.
[[like]]([[cats]], [[milk]])[[cats]] ?x.
[[like]](x, [[milk]])?yx.
[[like]](x, y) [[milk]]Figure 1: A simple formal semantic model.Formal semantic models are incredibly powerful,in that the resulting logical representations of sen-tences can be fed to automated theorem provers toperform textual inference, consistency verification,question answering, and a host of other tasks whichare well developed in the literature (e.g.
see (Love-land, 1978) and (Fitting, 1996)).
However, the so-phistication of such formal semantic models comesat a cost: the complex set of rules allowing forthe logical interpretation of text must either be pro-vided a priori, or learned.
Learning such represen-tations is a complex task, the difficulty of which iscompounded by issues of ambiguity and polysemywhich are pervasive in natural languages.In contrast, distributional semantic models, bestsummarised by the dictum of Firth (1957) that ?Youshall know a word by the company it keeps,?
pro-vide an elegant and tractable way of learning se-mantic representations of words from text.
Wordmeanings are modelled as high-dimensional vectorsin large semantic vector spaces, the basis elementsof which correspond to contextual features such asother words from a lexicon.
Semantic vectors forwords are built by counting how many time a targetword occurs within a context (e.g.
within k wordsof select words from the lexicon).
These contextcounts are then normalised by a term frequency-inverse document frequency-like measure (e.g.
TF-IDF, pointwise mutual information, ratio of proba-bilities), and are set as the basis weights of the vec-tor representation of the word?s meaning.
Word vec-tors can then be compared using geometric distance2furrystrokepetcatdogsnakeFigure 2: A simple distributional semantic model.metrics such as cosine similarity, allowing us to de-termine the similarity of words, cluster semanticallyrelated words, and so on.
Excellent overviews of dis-tributional semantic models are provided by Curran(2004) and Mitchell (2011).
A simple distributionalsemantic model showing the spacial representationof words ?dog?, ?cat?
and ?snake?
within the contextof feature words ?pet?, ?furry?, and ?stroke?
is shownin Figure 2.Distributional semantic models have been suc-cessfully applied to tasks such as word-sensediscrimination (Schu?tze, 1998), thesaurus extrac-tion (Grefenstette, 1994), and automated essaymarking (Landauer and Dumais, 1997).
However,while such models provide tractable ways of learn-ing and comparing word meanings, they do not natu-rally scale beyond word length.
As recently pointedout by Turney (2012), treating larger segments oftexts as lexical units and learning their representa-tions distributionally (the ?holistic approach?)
vio-lates the principle of linguistic creativity, accordingto which we can formulate and understand phraseswhich we?ve never observed before, provided weknow the meaning of their parts and how they arecombined.
As such, distributional semantics makesno effort to account for the compositional nature oflanguage like formal semantics does, and ignores is-sues relating to syntactic and relational aspects oflanguage.Several proposals have been put forth over thelast few years to provide vector composition func-tions for distributional models in order to introducecompositionality, thereby replicating some of the as-pects of formal semantics while preserving learn-ability.
Simple operations such as vector additionand multiplication, with or without scalar or matrixweights (to take word order or basic relational as-pects into account), have been suggested (Zanzottoet al 2010; Mitchell and Lapata, 2008; Mitchell andLapata, 2009).Smolensky (1990) suggests using the tensor prod-uct of word vectors to produce representations thatgrow with sentence complexity.
Clark and Pulman(2006) extend this approach by including basis vec-tors standing for dependency relations into tensorproduct-based representations.
Both of these ten-sor product-based approaches run into dimensional-ity problems as representations of sentence mean-ing for sentences of different lengths or grammati-cal structure do not live in the same space, and thuscannot directly be compared.
Coecke et al(2010)develop a framework using category theory, solvingthis dimensionality problem of tensor-based modelsby projecting tensored vectors for sentences into aunique vector space for sentences, using functionsdynamically generated by the syntactic structure ofthe sentences.
In presenting their framework, whichpartly inspired this paper, they describe how a verbcan be treated as a logical relation using tensors inorder to evaluate the truth value of a simple sentence,as well as how negation can be modelled using ma-trices.A related approach, by Baroni and Zamparelli(2010), represents unary relations such as adjectivesas matrices learned by linear regression from cor-pus data, and models adjective-noun compositionas matrix-vector multiplication.
Grefenstette et al(2013) generalise this approach to relations of anyarity and relate it to the framework of Coecke et al(2010) using a tensor-based approach to formal se-mantic modelling similar to that presented in this pa-per.Finally, Socher et al(2012) apply deep learningtechniques to model syntax-sensitive vector compo-sition using non-linear operations, effectively turn-ing parse trees into multi-stage neural networks.Socher shows that the non-linear activation func-tion used in such a neural network can be tailored toreplicate the behaviour of basic logical connectivessuch as conjunction and negation.33 Tensors and multilinear mapsTensors are the mathematical objects dealt with inmultilinear algebra just as vectors and matrices arethe objects dealt with in linear algebra.
In fact, ten-sors can be seen as generalisations of vectors andmatrices by introducing the notion of tensor rank.Let the rank of a tensor be the number of indices re-quired to describe a vector/matrix-like object in sumnotation.
A vector v in a space V with basis {bVi }i canbe written as the weighted sum of the basis vectors:v =?icvi bViwhere the cvi elements are the scalar basis weightsof the vector.
Being fully described with one index,vectors are rank 1 tensors.
Similarly, a matrix M isan element of a space V ?W with basis {(bVi ,bWj )}i j(such pairs of basis vectors of V and W are com-monly written as {bVi ?bWj }i j in multilinear algebra).Such matrices are rank 2 tensors, as they can be fullydescribed using two indices (one for rows, one forcolumns):M =?i jcMi j bVi ?
bWjwhere the scalar weights cMi j are just the i jth ele-ments of the matrix.A tensor T of rank k is just a geometric object witha higher rank.
Let T be a member of V1?.
.
.
?Vk; wecan express T as follows, using k indices ?1 .
.
.
?k:T =??1...?kcT?1...
?k bV1?1?
.
.
.
?
bVk?kIn this paper, we will be dealing with tensors of rank1 (vectors), rank 2 (matrices) and rank 3, which canbe pictured as cuboids (or a matrix of matrices).Tensor contraction is an operation which allowsus to take two tensors and produce a third.
It is ageneralisation of inner products and matrix multipli-cation to tensors of higher ranks.
Let T be a tensor inV1?.
.
.
?V j?Vk and U be a tensor in Vk?Vm?.
.
.
?Vn.The contraction of these tensors, written T?U, cor-responds to the following calculation:T ?
U =??1...?ncT?1...
?k cU?k ...?nbV1?1 ?
.
.
.
?
bV j?
j ?
bVm?m?
.
.
.
?
bVn?nTensor contraction takes a tensor of rank k and atensor of rank n ?
k + 1 and produces a tensor ofrank n ?
1, corresponding to the sum of the ranks ofthe input tensors minus 2.
The tensors must satisfythe following restriction: the left tensor must havea rightmost index spanning the same number of di-mensions as the leftmost index of the right tensor.This is similar to the restriction that a m by n matrixcan only be multiplied with a p by q matrix if n = p,i.e.
if the index spanning the columns of the first ma-trix covers the same number of columns as the indexspanning the rows of the second matrix covers rows.Similarly to how the columns of one matrix ?merge?with the rows of another to produce a third matrix,the part of the first tensor spanned by the index kmerges with the part of the second tensor spanned byk by ?summing through?
the shared basis elementsbVk?k of each tensor.
Each tensor therefore loses arank while being joined, explaining how the tensorproduced by T?U is of rank k+(n?k+1)?2 = n?1.There exists an isomorphism between tensors andmultilinear maps (Bourbaki, 1989; Lee, 1997), suchthat any curried multilinear mapf : V1 ?
.
.
.?
V j ?
Vkcan be represented as a tensor T f ?
Vk?V j?
.
.
.
?V1(note the reversed order of the vector spaces), withtensor contraction acting as function application.This isomorphism guarantees that there exists such atensor T f for every f , such that the following equal-ity holds for any v1 ?
V1, .
.
.
, v j ?
V j:f v1 .
.
.
v j = vk = T f ?
v1 ?
.
.
.
?
v j4 Tensor-based predicate calculiIn this section, I discuss how the isomorphism be-tween multilinear maps and tensors described abovecan be used to model predicates, relations, and log-ical atoms of a predicate calculus.
The four aspectsof a predicate calculus we must replicate here us-ing tensors are as follows: truth values, the logicaldomain and its elements (logical atoms), predicates,and relations.
I will discuss logical connectives inthe next section.Both truth values and domain objects are the ba-sic elements of a predicate calculus, and thereforeit makes sense to model them as vectors rather thanhigher rank tensors, which I will reserve for rela-tions.
We first must consider the vector space used4to model the boolean truth values of B. Coecke et al(2010) suggest, as boolean vector space, the space Bwith the basis {>,?
}, where > = [1 0]> is inter-preted as ?true?, and ?
= [0 1]> as ?false?.I assign to the domain D, the set of objects inour logic, a vector space D on R|D| with basis vec-tors {di}i which are in bijective correspondence withelements of D. An element of D is therefore rep-resented as a one-hot vector in D, the single non-null value of which is the weight for the basis vectormapped to that element of D. Similarly, a subset ofD is a vector of D where those elements ofD in thesubset have 1 as their corresponding basis weights inthe vector, and those not in the subset have 0.
There-fore there is a one-to-one correspondence betweenthe vectors in D and the elements of the power setP(D), provided the basis weights of the vectors arerestricted to one of 0 or 1.Each unary predicate P in the logic is representedin the logical model as a set MP ?
D containing theelements of the domain for which the predicate istrue.
Predicates can be viewed as a unary functionfP : D ?
B wherefP(x) ={> if x ?
MP?
otherwiseThese predicate functions can be modelled as rank 2tensors in B ?
D, i.e.
matrices.
Such a matrix MP isexpressed in sum notation as follows:MP =???????
?icMP1i > ?
di???????
+???????
?icMP2i ?
?
di??????
?The basis weights are defined in terms of the set MPas follows: cMP1i = 1 if the logical atom xi associ-ated with basis weight di is in MP, and 0 otherwise;conversely, cMP2i = 1 if the logical atom xi associatedwith basis weight di is not in MP, and 0 otherwise.To give a simple example, let?s consider a do-main with three individuals, represented as the fol-lowing one-hot vectors in D: john = [1 0 0]>,chris = [0 1 0]>, and tom = [0 0 1]>.
Let?simagine that Chris and John are mathematicians, butTom is not.
The predicate P for ?is a mathemati-cian?
therefore is represented model-theoretically asthe set MP = {chris, john}.
Translating this into amatrix gives the following tensor for P:MP =[1 1 00 0 1]To compute the truth value of ?John is a mathemati-cian?, we perform predicate-argument application astensor contraction (matrix-vector multiplication, inthis case):MP ?
john =[1 1 00 0 1]??????????010?????????
?=[10]= >Likewise for ?Tom is a mathematician?
:MP ?
tom =[1 1 00 0 1]??????????001?????????
?=[01]= ?Model theory for predicate calculus representsany n-ary relation R, such as a verb, as the set MRof n-tuples of elements from D for which R holds.Therefore such relations can be viewed as functionsfR : Dn ?
B where:fR(x1, .
.
.
, xn) ={> if (x1, .
.
.
, xn) ?
MR?
otherwiseWe can represent the boolean function for such a re-lation R as a tensor TR in B ?
D ?
.
.
.
?
D?
??
?n:TR =??????????1...?ncTR1?1...
?n> ?
d?1 ?
.
.
.
?
d?n????????+??????????1...?ncTR2?1...?n?
?
d?1 ?
.
.
.
?
d?n???????
?As was the case for predicates, the weights for re-lational tensors are defined in terms of the set mod-elling the relation: cTR1?1...?nis 1 if the tuple (x, .
.
.
, z)associated with the basis vectors d?n .
.
.
d?1 (again,note the reverse order) is in MR and 0 otherwise; andcTR2?1...?nis 1 if the tuple (x, .
.
.
, z) associated withthe basis vectors d?n .
.
.
d?1 is not in MR and 0 oth-erwise.To give an example involving relations, let ourdomain be the individuals John ( j) and Mary (m).Mary loves John and herself, but John only loveshimself.
The logical model for this scenario is asfollows:D = { j,m} Mloves = {( j, j), (m,m), (m, j)}Distributionally speaking, the elements of the do-main will be mapped to the following one-hot vec-tors in some two-dimensional space D as follows:5j = [1 0]> and m = [0 1]>.
The tensor for ?loves?can be written as follows, ignoring basis elementswith null-valued basis weights, and using the dis-tributivity of the tensor product over addition:Tloves = > ?
((d1 ?
d1) + (d2 ?
d2) + (d1 ?
d2))+ (?
?
d2 ?
d1)Computing ?Mary loves John?
would correspond tothe following calculation:(Tloves ?m) ?
j =((> ?
d2) + (> ?
d1)) ?
j = >whereas ?John loves Mary?
would correspond to thefollowing calculation:(Tloves ?
j) ?m =((> ?
d1) + (?
?
d2)) ?m = ?5 Logical connectives with tensorsIn this section, I discuss how the boolean connec-tives of a propositional calculus can be modelled us-ing tensors.
Combined with the predicate and rela-tion representations discussed above, these form acomplete quantifier-free predicate calculus based ontensors and tensor contraction.Negation has already been shown to be modelledin the boolean space described earlier by Coecke etal.
(2010) as the swap matrix:T?
=[0 11 0]This can easily be verified:T?
?
> =[0 11 0] [10]=[01]= ?T?
?
?
=[0 11 0] [01]=[10]= >All other logical operators are binary, and hencemodelled as rank 3 tensors.
To make talking aboutrank 3 tensors used to model binary operations eas-ier, I will use the following block matrix notation for2 ?
2 ?
2 rank 3 tensors T:T =[a1 b1 a2 b2c1 d1 c2 d2]which allows us to express tensor contractions asfollows:T ?
v =[a1 b1 a2 b2c1 d1 c2 d2] [??]=[?
?
a1 + ?
?
a2 ?
?
b1 + ?
?
b2?
?
c1 + ?
?
c2 ?
?
d1 + ?
?
d2]or more concretely:T ?
> =[a1 b1 a2 b2c1 d1 c2 d2] [10]=[a1 b1c1 d1]T ?
?
=[a1 b1 a2 b2c1 d1 c2 d2] [01]=[a2 b2c2 d2]Using this notation, we can define tensors for thefollowing operations:(?)
7?
T?
=[1 1 1 00 0 0 1](?)
7?
T?
=[1 0 0 00 1 1 1](?)
7?
T?
=[1 0 1 10 1 0 0]I leave the trivial proof by exhaustion that these fitthe bill to the reader.It is worth noting here that these tensors pre-serve normalised probabilities of truth.
Let us con-sider a model such at that described in Coecke etal.
(2010) which, in lieu of boolean truth values,represents truth value vectors of the form [?
?
]>where ?
+ ?
= 1.
Applying the above logical op-erations to such vectors produces vectors with thesame normalisation property.
This is due to the factthat the columns of the component matrices are allnormalised (i.e.
each column sums to 1).
To givean example with conjunction, let v = [?1 ?1]> andw = [?2 ?2]> with ?1 + ?1 = ?2 + ?2 = 1.
The con-junction of these vectors is calculated as follows:(T?
?
v) ?
w=[1 0 0 00 1 1 1] [?1?1] [?2?2]=[?1 0?1 ?1 + ?1] [?2?2]=[?1?2?1?2 + (?1 + ?1)?2]6To check that the probabilities are normalised wecalculate:?1?2 + ?1?2 + (?1 + ?1)?2= (?1 + ?1)?2 + (?1 + ?1)?2= (?1 + ?1)(?2 + ?2) = 1We can observe that the resulting probability distri-bution for truth is still normalised.
The same prop-erty can be verified for the other connectives, whichI leave as an exercise for the reader.6 Quantifiers and non-linearityThe predicate calculus described up until this pointhas repeatedly been qualified as ?quantifier-free?,for the simple reason that quantification cannot bemodelled if each application of a predicate or rela-tion immediately yields a truth value.
In perform-ing such reductions, we throw away the informa-tion required for quantification, namely the infor-mation which indicates which elements of a domainthe predicate holds true or false for.
In this sec-tion, I present a variant of the predicate calculusdeveloped earlier in this paper which allows us tomodel simple quantification (i.e.
excluding embed-ded quantifiers) alongside a tensor-based approachto predicates.
However, I will prove that this ap-proach to quantifier modelling relies on non-linearfunctions, rendering them non-suitable for compo-sitional distributional models relying solely on mul-tilinear maps for composition (or alternatively, ren-dering such models unsuitable for the modelling ofquantifiers by this method).We saw, in Section 4, that vectors in the seman-tic space D standing for the logical domain couldmodel logical atoms as well as sets of atoms.
Withthis in mind, instead of modelling a predicate P asa truth-function, let us now view it as standing forsome function fP : P(D)?
P(D), defined as:fP(X) = X ?
MPwhere X is a set of domain objects, and MP is the setmodelling the predicate.
The tensor form of such afunction will be some T fP in D ?
D. Let this squarematrix be a diagonal matrix such that basis weightscT fpii = 1 if the atom x corresponding to di is in MPand 0 otherwise.
Through tensor contraction, thistensor maps subsets ofD (elements of D) to subsetsof D containing only those objects of the originalsubset for which P holds (i.e.
yielding another vectorin D).To give an example: let us consider a domain withtwo dogs (a and b) and a cat (c).
One of the dogs (b)is brown, as is the cat.
Let S be the set of dogs, and Pthe predicate ?brown?.
I represent these statementsin the model as follows:D = {a, b, c} S = {a, b} MP = {b, c}The set of dogs is represented as a vector S =[1 1 0]> and the predicate ?brown?
as a tensor inD ?
D:TP =?????????
?0 0 00 1 00 0 1?????????
?The set of brown dogs is obtained by computingfB(S ), which distributionally corresponds to apply-ing the tensor TP to the vector representation of Svia tensor contraction, as follows:TP ?
S =?????????
?0 0 00 1 00 0 1????????????????????110??????????=??????????010?????????
?= bThe result of this computation shows that the set ofbrown dogs is the singleton set containing the onlybrown dog, b.
As for how logical connectives fitinto this picture, in both approaches discussed be-low, conjunction and disjunction are modelled usingset-theoretic intersection and union, which are sim-ply the component-wise min and max functions overvectors, respectively.Using this new way of modelling predicates astensors, I turn to the problem of modelling quantifi-cation.
We begin by putting all predicates in vectorform by replacing each instance of the bound vari-able with a vector 1 filled with ones, which extractsthe diagonal from the predicate matrix.An intuitive way of modelling universal quantifi-cation is as follows: expressions of the form ?All Xsare Ys?
are true if and only if MX = MX?MY , whereMX and MY are the set of Xs and the set of Ys, re-spectively.
Using this, we can define the map forallfor distributional universal quantification modellingexpressions of the form ?All Xs are Ys?
as follows:forall(X,Y) ={> if X = min(X,Y)?
otherwise7To give a short example, the sentence ?All Greeks arehuman?
is verified by computing X = (Mgreek ?
1),Y = (Mhuman ?
1), and verifying the equality X =min(X,Y).Existential statements of the form ?There existsX?
can be modelled using the function exists, whichtests whether or not MX is empty, and is defined asfollows:exists(X) ={> if |X| > 0?
otherwiseTo give a short example, the sentence ?there exists abrown dog?
is verified by computing X = (Mbrown ?1) ?
(Mdog ?
1) and verifying whether or not X is ofstrictly positive length.An important point to note here is that neither ofthese quantification functions are multi-linear maps,since a multilinear map must be linear in all argu-ments.
A counter example for forall is to considerthe case where MX and MY are empty, and multi-ply their vector representations by non-zero scalarweights ?
and ?.
?X = X?Y = Yforall(?X, ?Y) = forall(X,Y) = >forall(?X, ?Y) , ?
?>I observe that the equations above demonstrate thatforall is not a multilinear map.The proof that exists is not a multilinear map isequally trivial.
Assume MX is an empty set and ?
isa non-zero scalar weight:?X = Xexists(?X) = exists(X) = ?exists(?X) , ?
?It follows that exists is not a multi-linear function.7 Conclusions and future workIn this paper, I set out to demonstrate that it waspossible to replicate most aspects of predicate logicusing tensor-based models.
I showed that tensorscan be constructed from logical models to representpredicates and relations, with vectors encoding ele-ments or sets of elements from the logical domain.I discussed how tensor contraction allows for evalu-ation of logical expressions encoded as tensors, andthat logical connectives can be defined as tensors toform a full quantifier-free predicate calculus.
I ex-posed some of the limitations of this approach whendealing with variables under the scope of quantifiers,and proposed a variant for the tensor representationof predicates which allows us to deal with quantifi-cation.
Further work on tensor-based modelling ofquantifiers should ideally seek to reconcile this workwith that of Barwise and Cooper (1981).
In this sec-tion, I discuss how both of these approaches to pred-icate modelling can be put into relation, and suggestfurther work that might be done on this topic, and onthe topic of integrating this work into compositionaldistributional models of semantics.The first approach to predicate modelling treatspredicates as truth functions represented as tensors,while the second treats them as functions from sub-sets of the domain to subsets of the domain.
Yet bothrepresentations of predicates contain the same infor-mation.
Let MP and M?P be the tensor represen-tations of a predicate P under the first and secondapproach, respectively.
The relation between theserepresentations lies in the equality diag(pMP) =M?P, where p is the covector [1 0] (and hence pMPyields the first row of MP).
The second row of MPbeing defined in terms of the first, one can also re-cover MP from the diagonal of M?P.Furthermore, both approaches deal with separateaspects of predicate logic, namely applying predi-cates to logical atoms, and applying them to boundvariables.
With this in mind, it is possible to see howboth approaches can be used sequentially by notingthat tensor contraction allows for partial applicationof relations to logical atoms.
For example, apply-ing a binary relation to its first argument under thefirst tensor-based model yields a predicate.
Translat-ing this predicate into the second model?s form usingthe equality defined above then permits us to use itin quantified expressions.
Using this, we can eval-uate expressions of the form ?There exists someonewho John loves?.
Future work in this area shouldtherefore focus on developing a version of this ten-sor calculus which permits seamless transition be-tween both tensor formulations of logical predicates.Finally, this paper aims to provide a starting pointfor the integration of logical aspects into composi-8tional distributional semantic models.
The work pre-sented here serves to illustrate how tensors can sim-ulate logical elements and operations, but does notaddress (or seek to address) the fact that the vectorsand matrices in most compositional distributionalsemantic models do not cleanly represent elementsof a logical domain.
However, such distributionalrepresentations can arguably be seen as represent-ing the properties objects of a logical domain holdin a corpus: for example the similar distributions of?car?
and ?automobile?
could serve to indicate thatthese concepts are co-extensive.
This suggests twodirections research based on this paper could take.One could use the hypothesis that similar vectors in-dicate co-extensive concepts to infer a (probabilis-tic) logical domain and set of predicates, and use themethods described above without modification; al-ternatively one could use the form of the logical op-erations and predicate tensors described in this pa-per as a basis for a higher-dimensional predicate cal-culus, and investigate how such higher-dimensional?logical?
operations and elements could be definedor learned.
Either way, the problem of reconcilingthe fuzzy ?messiness?
of distributional models withthe sharp ?cleanliness?
of logic is a difficult problem,but I hope to have demonstrated in this paper that asmall step has been made in the right direction.AcknowledgmentsThanks to Ondr?ej Rypa?c?ek, Nal Kalchbrennerand Karl Moritz Hermann for their helpful com-ments during discussions surrounding this pa-per.
This work is supported by EPSRC ProjectEP/I03808X/1.ReferencesM.
Baroni and R. Zamparelli.
Nouns are vectors, adjec-tives are matrices: Representing adjective-noun con-structions in semantic space.
In Proceedings of the2010 Conference on Empirical Methods in NaturalLanguage Processing, pages 1183?1193.
Associationfor Computational Linguistics, 2010.J.
Barwise and R. Cooper Generalized quantifiers andnatural language.
Linguistics and philosophy, pages159?219.
Springer, 1981.W.
Blacoe and M. Lapata.
A comparison of vector-basedrepresentations for semantic composition.
Proceed-ings of the 2012 Conference on Empirical Methods inNatural Language Processing, 2012.N.
Bourbaki.
Commutative Algebra: Chapters 1-7.Springer-Verlag (Berlin and New York), 1989.S.
Clark and S. Pulman.
Combining symbolic and distri-butional models of meaning.
In AAAI Spring Sympo-sium on Quantum Interaction, 2006.B.
Coecke, M. Sadrzadeh, and S. Clark.
MathematicalFoundations for a Compositional Distributional Modelof Meaning.
Linguistic Analysis, volume 36, pages345?384.
March 2010.J.
R. Curran.
From distributional to semantic similarity.PhD thesis, 2004.D.
R. Dowty, R. E. Wall, and S. Peters.
Introduction toMontague Semantics.
Dordrecht, 1981.J.
R. Firth.
A synopsis of linguistic theory 1930-1955.Studies in linguistic analysis, 1957.M.
Fitting.
First-order logic and automated theoremproving.
Springer Verlag, 1996.E.
Grefenstette, G. Dinu, Y. Zhang, M. Sadrzadeh, andM.
Baroni.
Multi-step regression learning for com-positional distributional semantics.
In Proceedings ofthe Tenth International Conference on ComputationalSemantics.
Association for Computational Linguistics,2013.E.
Grefenstette and M. Sadrzadeh.
Experimental supportfor a categorical compositional distributional model ofmeaning.
In Proceedings of the 2011 Conference onEmpirical Methods in Natural Language Processing,2011.G.
Grefenstette.
Explorations in automatic thesaurus dis-covery.
1994.D.
Kartsaklis, and M. Sadrzadeh and S. Pulman.
AUnified Sentence Space for Categorical Distributional-Compositional Semantics: Theory and Experiments.In Proceedings of 24th International Conference onComputational Linguistics (COLING 2012): Posters,2012.T.
K. Landauer and S. T. Dumais.
A solution to Plato?sproblem: The latent semantic analysis theory of ac-quisition, induction, and representation of knowledge.Psychological review, 1997.J.
Lee.
Riemannian manifolds: An introduction to curva-ture, volume 176.
Springer Verlag, 1997.D.
W. Loveland.
Automated theorem proving: A logicalbasis.
Elsevier North-Holland, 1978.J.
Mitchell and M. Lapata.
Vector-based models of se-mantic composition.
In Proceedings of ACL, vol-ume 8, 2008.J.
Mitchell and M. Lapata.
Language models based on se-mantic composition.
In Proceedings of the 2009 Con-ference on Empirical Methods in Natural LanguageProcessing: Volume 1-Volume 1, pages 430?439.
As-sociation for Computational Linguistics, 2009.9J.
J. Mitchell.
Composition in distributional models ofsemantics.
PhD thesis, 2011.R.
Montague.
English as a Formal Language.
FormalSemantics: The Essential Readings, 1974.H.
Schu?tze.
Automatic word sense discrimination.
Com-putational linguistics, 24(1):97?123, 1998.P.
Smolensky.
Tensor product variable binding and therepresentation of symbolic structures in connection-ist systems.
Artificial intelligence, 46(1-2):159?216,1990.R.
Socher, B. Huval, C.D.
Manning, and A.Y Ng.Semantic compositionality through recursive matrix-vector spaces.
Proceedings of the 2012 Conference onEmpirical Methods in Natural Language Processing,pages 1201?1211, 2012.P.
D. Turney.
Domain and function: A dual-space modelof semantic relations and compositions.
Journal of Ar-tificial Intelligence Research, 44:533?585, 2012.F.
M. Zanzotto, I. Korkontzelos, F. Fallucchi, and S. Man-andhar.
Estimating linear models for compositionaldistributional semantics.
In Proceedings of the 23rdInternational Conference on Computational Linguis-tics, pages 1263?1271.
Association for ComputationalLinguistics, 2010.10
