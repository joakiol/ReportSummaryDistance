Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 11?21,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsIntegrating Translation Memory into Phrase-BasedMachine Translation during DecodingKun Wang?
Chengqing Zong?
Keh-Yih Su?
?National Laboratory of Pattern Recognition, Institute of Automation,Chinese Academy of Sciences, Beijing, China?Behavior Design Corporation, Taiwan?
{kunwang, cqzong}@nlpr.ia.ac.cn,?kysu@bdc.com.twAbstractSince statistical machine translation (SMT)and translation memory (TM) complementeach other in matched and unmatched regions,integrated models are proposed in this paper toincorporate TM information into phrase-basedSMT.
Unlike previous multi-stage pipelineapproaches, which directly merge TM resultinto the final output, the proposed models referto the corresponding TM information associat-ed with each phrase at SMT decoding.
On aChinese?English TM database, our experi-ments show that the proposed integrated Mod-el-III is significantly better than either theSMT or the TM systems when the fuzzy matchscore is above 0.4.
Furthermore, integratedModel-III achieves overall 3.48 BLEU pointsimprovement and 2.62 TER points reductionin comparison with the pure SMT system.
Be-sides, the proposed models also outperformprevious approaches significantly.1 IntroductionStatistical machine translation (SMT), especiallythe phrase-based model (Koehn et al, 2003), hasdeveloped very fast in the last decade.
For cer-tain language pairs and special applications,SMT output has reached an acceptable level, es-pecially in the domains where abundant parallelcorpora are available (He et al, 2010).
However,SMT is rarely applied to professional translationbecause its output quality is still far from satis-factory.
Especially, there is no guarantee that aSMT system can produce translations in a con-sistent manner (Ma et al, 2011).In contrast, translation memory (TM), whichuses the most similar translation sentence (usual-ly above a certain fuzzy match threshold) in thedatabase as the reference for post-editing, hasbeen widely adopted in professional translationfield for many years (Lagoudaki, 2006).
TM isvery useful for repetitive material such as updat-ed product manuals, and can give high qualityand consistent translations when the similarity offuzzy match is high.
Therefore, professionaltranslators trust TM much more than SMT.However, high-similarity fuzzy matches areavailable unless the material is very repetitive.In general, for those matched segments1, TMprovides more reliable results than SMT does.One reason is that the results of TM have beenrevised by human according to the global context,but SMT only utilizes local context.
However,for those unmatched segments, SMT is more re-liable.
Since TM and SMT complement eachother in those matched and unmatched segments,the output quality is expected to be raised signif-icantly if they can be combined to supplementeach other.In recent years, some previous works have in-corporated TM matched segments into SMT in apipelined manner (Koehn and Senellart, 2010;Zhechev and van Genabith, 2010; He et al, 2011;Ma et al, 2011).
All these pipeline approachestranslate the sentence in two stages.
They firstdetermine whether the extracted TM sentencepair should be adopted or not.
Most of them usefuzzy match score as the threshold, but He et al(2011) and Ma et al (2011) use a classifier tomake the judgment.
Afterwards, they merge therelevant translations of matched segments intothe source sentence, and then force the SMT sys-tem to only translate those unmatched segmentsat decoding.There are three obvious drawbacks for theabove pipeline approaches.
Firstly, all of themdetermine whether those matched segments1 We mean ?sub-sentential segments?
in this work.11should be adopted or not at sentence level.
Thatis, they are either all adopted or all abandonedregardless of their individual quality.
Secondly,as several TM target phrases might be availablefor one given TM source phrase due to insertions,the incorrect selection made in the merging stagecannot be remedied in the following translationstage.
For example, there are six possible corre-sponding TM target phrases for the given TMsource phrase ??
?4 ?5 ??6?
(as shown inFigure 1) such as ?object2 that3 is4 associated5?,and ?an1 object2 that3 is4 associated5  with6?, etc.And it is hard to tell which one should be adopt-ed in the merging stage.
Thirdly, the pipelineapproach does not utilize the SMT probabilisticinformation in deciding whether a matched TMphrase should be adopted or not, and which tar-get phrase should be selected when we have mul-tiple candidates.
Therefore, the possible im-provements resulted from those pipeline ap-proaches are quite limited.On the other hand, instead of directly mergingTM matched phrases into the source sentence,some approaches (Bi?ici and Dymetman, 2008;Simard and Isabelle, 2009) simply add the long-est matched pairs into SMT phrase table, andthen associate them with a fixed large probabilityvalue to favor the corresponding TM targetphrase at SMT decoding.
However, since onlyone aligned target phrase will be added for eachmatched source phrase, they share most draw-backs with the pipeline approaches mentionedabove and merely achieve similar performance.To avoid the drawbacks of the pipeline ap-proach (mainly due to making a hard decisionbefore decoding), we propose several integratedmodels to completely make use of TM infor-mation during decoding.
For each TM sourcephrase, we keep all its possible correspondingtarget phrases (instead of keeping only one ofthem).
The integrated models then consider allcorresponding TM target phrases and SMT pref-erence during decoding.
Therefore, the proposedintegrated models combine SMT and TM at adeep level (versus the surface level at which TMresult is directly plugged in under previous pipe-line approaches).On a Chinese?English computer technicaldocuments TM database, our experiments haveshown that the proposed Model-III improves thetranslation quality significantly over either thepure phrase-based SMT or the TM systems whenthe fuzzy match score is above 0.4.
Comparedwith the pure SMT system, the proposed inte-grated Model-III achieves 3.48 BLEU points im-provement and 2.62 TER points reduction over-all.
Furthermore, the proposed models signifi-cantly outperform previous pipeline approaches.2 Problem FormulationCompared with the standard phrase-based ma-chine translation model, the translation problemis reformulated as follows (only based on thebest TM, however, it is similar for multiple TMsentences):(1)Where  is the given source sentence to be trans-lated,  is the corresponding target sentence andis the final translation;are the associated information of the best TMsentence-pair;  and  denote the corre-sponding TM sentence pair;  denotes itsassociated fuzzy match score (from 0.0 to 1.0);is the editing operations between  and ;and  denotes the word alignment betweenand .Let  and  denote the k-th associatedsource phrase and target phrase, respectively.Also,  and  denote the associated sourcephrase sequence and the target phrase sequence,respectively (total  phrases without insertion).Then the above formula (1) can be decomposedas below:(2)Afterwards, for any given source phrase ,we can find its corresponding TM source phraseand all possible TM target phrases (eachof them is denoted by ) with the help ofcorresponding editing operations  and wordalignment .
As mentioned above, we canhave six different possible TM target phrases forthe TM source phrase ???
4 ?
5 ??
6?.
This?
?0                    ?1  ?
?2  ?
?3  ?
?4  ?5  ?
?6  ?7?
?0  ?1  ?
?2  ?3  ?
?4             ?
?5  ?6  ?
?7  ?8gets0  n1  obj ct2  that3  is4  associated5  with6  the7  annotation8  label9  .10SourceTM SourceTM TargetFigure 1: Phrase Mapping Example12is because there are insertions around the directlyaligned TM target phrase.In the above Equation (2), we first segment thegiven source sentence into various phrases, andthen translate the sentence based on those sourcephrases.
Also,  is replaced by , as theyare actually the same segmentation sequence.Assume that the segmentation probabilityis a uniform distribution, with the corre-sponding TM source and target phrases obtainedabove, this problem can be further simplified asfollows:(3)Where  is the corresponding TM phrasematching status for , which is a vector consist-ing of various indicators (e.g., Target PhraseContent Matching Status, etc., to be defined lat-er), and reflects the quality of the given candi-date;  is the linking status vector of  (thealigned source phrase of  within ), and indi-cates the matching and linking status in thesource side (which is closely related to the statusin the target side); also,  indicates the corre-sponding TM fuzzy match interval specified later.In the second line of Equation (3), we convertthe fuzzy match score  into its correspond-ing interval , and incorporate all possible com-binations of TM target phrases.
Afterwards, weselect the best one in the third line.
Last, in thefourth line, we introduce the source matchingstatus and the target linking status (detailed fea-tures would be defined later).
Since we mighthave several possible TM target phrases ,the one with the maximum score will be adoptedduring decoding.The first factor  in the above for-mula (3) is just the typical phrase-based SMTmodel, and the second factor  (to bespecified in the Section 3) is the information de-rived from the TM sentence pair.
Therefore, wecan still keep the original phrase-based SMTmodel and only pay attention to how to extractuseful information from the best TM sentencepair to guide SMT decoding.3 Proposed ModelsThree integrated models are proposed to incorpo-rate different features as follows:3.1 Model-IIn this simplest model, we only consider TargetPhrase Content Matching Status (TCM) for .For , we consider four different features at thesame time: Source Phrase Content MatchingStatus (SCM), Number of Linking Neighbors(NLN), Source Phrase Length (SPL), and Sen-tence End Punctuation Indicator (SEP).
Thosefeatures will be defined below.
isthen specified as:All features incorporated in this model are speci-fied as follows:TM Fuzzy Match Interval (z): The fuzzy matchscore (FMS) between source sentence  and TMsource sentence  indicates the reliability ofthe given TM sentence, and is defined as (Sikes,2007):Where  is the word-basedLevenshtein Distance (Levenshtein, 1966) be-tween  and .
We equally divide FMS intoten fuzzy match intervals such as: [0.9, 1.0), [0.8,0.9) etc., and the index  specifies the corre-sponding interval.
For example, since the fuzzymatch score between  and  in Figure 1 is0.667, then .Target Phrase Content Matching Status(TCM): It indicates the content matching statusbetween   and , and reflects the qualityof .
Because  is nearly perfect when FMSis high, if the similarity between    andis high, it implies that the given  is possibly agood candidate.
It is a member of {Same, High,Low, NA (Not-Applicable)}, and is specified as:(1) If  is not null:(a) if , ;(b) else if , ;(c) else, ;(2) If  is null, ;Here  is null means that either there is nocorresponding TM source phrase  orthere is no corresponding TM target phrase13aligned with .
In the example ofFigure 1, assume that the given  is ???
5?
6  ??
7?
and  is ?object that is associated?.If  is ?object2 that3 is4 associated5?,; if  is ?an1 object2 that3is4 associated5?, .Source Phrase Content Matching Status(SCM): Which indicates the content matchingstatus between  and , and it affectsthe matching status of  and  greatly.The more similar  is to , the moresimilar   is to .
It is a member of {Same,High, Low, NA} and is defined as:(1) If  is not null:(a) if , ;(b) else if ,;(c) else, ;(2) If  is null, ;Here  is null means that there is no corre-sponding TM source phrase  for the giv-en source phrase .
Take the source phrase???
5 ?
6 ??
7?
in Figure 1 for an ex-ample, since its corresponding  is ???
4?
5 ??
6?, then .Number of Linking Neighbors (NLN): Usually,the context of a source phrase would affect itstarget translation.
The more similar the contextare, the more likely that the translations are thesame.
Therefore, this NLN feature reflects thenumber of matched neighbors (words) and it is avector of <x, y>.
Where ?x?
denotes the numberof matched source neighbors; and ?y?
denoteshow many those neighbors are also linked to tar-get words (not null), which also affects the TMtarget phrase selection.
This feature is a memberof {<x, y>: <2, 2>, <2, 1>, <2, 0>, <1, 1>, <1, 0>,<0, 0>}.
For the source phrase ???
5 ?
6 ??7?
in Figure 1, the corresponding TM sourcephrase is ???
4 ?
5 ??
6?
.
As only theirright neighbors ??8?
and ??7?
are matched, and??7?
is aligned with ?.10?, NLN will be <1, 1>.Source Phrase Length (SPL): Usually the long-er the source phrase is, the more reliable the TMtarget phrase is.
For example, the correspondingfor the source phrase with 5 wordswould be more reliable than that with only oneword.
This feature denotes the number of wordsincluded in , and is a member of {1, 2, 3, 4,?5}.
For the case ???
5 ?
6 ??
7?, SPL willbe 3.Sentence End Punctuation Indicator (SEP):Which indicates whether the current phrase is apunctuation at the end of the sentence, and is amember of {Yes, No}.
For example, the SEP for???
5 ?
6 ??
7?
will be ?No?.
It is intro-duced because the SCM and TCM for a sen-tence-end-punctuation are always ?Same?
re-gardless of other features.
Therefore, it is used todistinguish this special case from other cases.3.2 Model-IIAs Model-I ignores the relationship among vari-ous possible TM target phrases, we add two fea-tures TM Candidate Set Status (CSS) and Long-est TM Candidate Indicator (LTC) to incorporatethis relationship among them.
Since CSS is re-dundant after LTC is known, we thus ignore itfor evaluating TCM probability in the followingderivation:The two new features CSS and LTC adopted inModel-II are defined as follows:TM Candidate Set Status (CSS): Which re-stricts the possible status of , and is amember of {Single, Left-Ext, Right-Ext, Both-Ext,NA}.
Where ?Single?
means that there is onlyone  candidate for the given sourcephrase ; ?Left-Ext?
means that there aremultiple  candidates, and all the candi-dates are generated by extending only the leftboundary; ?Right-Ext?
means that there are mul-tiple  candidates, and all the candidatesare generated by only extending to the right;?Both-Ext?
means that there are multiplecandidates, and the candidates are generated byextending to both sides; ?NA?
means thatis null.For ???
4 ?
5 ??
6?
in Figure 1, thelinked TM target phrase is ?object2 that3 is4 asso-ciated5?, and there are 5 other candidates by ex-tending to both sides.
Therefore,.Longest TM Candidate Indicator (LTC):Which indicates whether the given  is thelongest candidate or not, and is a member of{Original, Left-Longest, Right-Longest, Both-Longest, Medium, NA}.
Where ?Original?
meansthat the given  is the one without exten-sion; ?Left-Longest?
means that the given14is only extended to the left and is thelongest one; ?Right-Longest?
means that the giv-en  is only extended to the right and isthe longest one; ?Both-Longest?
means that thegiven  is extended to both sides and is thelongest one; ?Medium?
means that the givenhas been extended but not the longestone; ?NA?
means that  is null.For  ?object2 that3 is4 associated5?
inFigure 1, ; for  ?an1 ob-ject2 that3 is4 associated5?, ;for the longest  ?an1 object2 that3 is4 as-sociated5 with6 the7?, .3.3 Model-IIIThe abovementioned integrated models ignorethe reordering information implied by TM.Therefore, we add a new feature Target PhraseAdjacent Candidate    Relative   PositionMatching    Status (CPM) into Model-II andModel-III is given as:We assume that CPM is independent with SPLand SEP, because the length of source phrasewould not affect reordering too much and SEP isused to distinguish the sentence end punctuationwith other phrases.The new feature CPM adopted in Model-III isdefined as:Target Phrase Adjacent Candidate RelativePosition Matching Status (CPM): Which indi-cates the matching status between the relativeposition ofand the relative position of.
It checks if  arepositioned in the same order with, and reflects the quality ofordering the given target candidate .
It is amember of {Adjacent-Same, Adjacent-Substitute,Linked-Interleaved, Linked-Cross, Linked-Reversed, Skip-Forward, Skip-Cross, Skip-Reversed, NA}.
Recall thatis always right ad-jacent to , then various cases are defined asfollows:(1) If both  and  are not null:(a) If  is on the right ofand they are also adjacent to each other:i.
If the right boundary words of  andare the same, and the leftboundary words of  and  arethe same, ;ii.
Otherwise, ;(b) If  is on the right ofbut they are not adjacent to each other,;(c) If  is not on the right of:i.
If there are cross parts betweenand , ;ii.
Otherwise, ;(2) If   is null but  is not null,then find the first which isnot null (  starts from 2)2:(a) If  is on the right of ,;(b) If  is not on the right of:i.
If there are cross parts betweenand , ;ii.
Otherwise, .
(3) If  is null, .In Figure 1, assume that ,  andare ?gets an?, ?object that is associat-ed with?
and ?gets0 an1?, respectively.
For?object2 that3 is4 associated5?, becauseis on the right of  and they areadjacent pair, and both boundary words (?an?and ?an1?
; ?object?
and ?object2?)
are matched,; for  ?an1 object2that3 is4 associated5?, because there are crossparts ?an1?
between  and ,.
On the other hand, as-sume that ,  and  are ?gets?, ?ob-ject that is associated with?
and ?gets0?, respec-tively.
For  ?an1 object2 that3 is4 associ-ated5?, because  and  are adja-cent pair, but the left boundary words of  and(?object?
and ?an1?)
are not matched,; for  ?object2that3 is4 associated5?, because  is on theright of  but they are not adjacent pair,therefore, .
One moreexample, assume that ,  and  are?the annotation label?, ?object that is associatedwith?
and ?the7 annotation8 label9?, respectively.For  ?an1 object2 that3 is4 associated5?,because  is on the left of , andthere are no cross parts, .2 It can be identified by simply memorizing the index ofnearest non-null  during search.154 Experiments4.1 Experimental SetupOur TM database consists of computer domainChinese-English translation sentence-pairs,which contains about 267k sentence-pairs.
Theaverage length of Chinese sentences is 13.85words and that of English sentences is 13.86words.
We randomly selected a development setand a test set, and then the remaining sentencepairs are for training set.
The detailed corpus sta-tistics are shown in Table 1.
Furthermore, devel-opment set and test set are divided into variousintervals according to their best fuzzy matchscores.
Corpus statistics for each interval in thetest set are shown in Table 2.For the phrase-based SMT system, we adoptedthe Moses toolkit (Koehn et al, 2007).
The sys-tem configurations are as follows: GIZA++ (Ochand Ney, 2003) is used to obtain the bidirectionalword alignments.
Afterwards, ?intersection?
3refinement (Koehn et al, 2003) is adopted to ex-tract phrase-pairs.
We use the SRI LanguageModel toolkit (Stolcke, 2002) to train a 5-grammodel with modified Kneser-Ney smoothing(Kneser and Ney, 1995; Chen and Goodman,1998) on the target-side (English) training corpus.All the feature weights and the weight for eachprobability factor (3 factors for Model-III) aretuned on the development set with minimum-error-rate training (MERT) (Och, 2003).
Themaximum phrase length is set to 7 in our exper-iments.In this work, the translation performance ismeasured with case-insensitive BLEU-4 score(Papineni et al, 2002) and TER score (Snover etal., 2006).
Statistical significance test is conduct-ed with re-sampling (1,000 times) approach(Koehn, 2004) in 95% confidence level.4.2 Cross-Fold TranslationTo estimate the probabilities of proposed models,the corresponding phrase segmentations for bi-lingual sentences are required.
As we want tocheck what actually happened during decoding inthe real situation, cross-fold translation is used toobtain the corresponding phrase segmentations.We first extract 95% of the bilingual sentences asa new training corpus to train a SMT system.Afterwards, we generate the correspondingphrase segmentations for the remaining 5% bi-3 ?grow-diag-final?
and ?grow-diag-final-and?
are also test-ed.
However, ?intersection?
is the best option in our exper-iments, especially for those high fuzzy match intervals.lingual sentences with Forced Decoding (Li etal., 2000; Zollmann et al, 2008; Auli et al, 2009;Wisniewski et al, 2010), which searches the bestphrase segmentation for the specified output.Having repeated the above steps 20 times4, weobtain the corresponding phrase segmentationsfor the SMT training data (which will then beused to train the integrated models).Due to OOV words and insertion words, notall given source sentences can generate the de-sired results through forced decoding.
Fortunate-ly, in our work, 71.7% of the training bilingualsentences can generate the corresponding targetresults.
The remaining 28.3% of the sentencepairs are thus not adopted for generating trainingsamples.
Furthermore, more than 90% obtainedsource phrases are observed to be less than 5words, which explains why five different quanti-zation levels are adopted for Source PhraseLength (SPL) in section 3.1.4.3 Translation ResultsAfter obtaining all the training samples via cross-fold translation, we use Factored LanguageModel toolkit (Kirchhoff et al, 2007) to estimatethe probabilities of integrated models with Wit-ten-Bell smoothing (Bell et al, 1990; Witten etal., 1991) and Back-off method.
Afterwards, weincorporate the TM information  foreach  phrase  at  decoding.
All  experiments  are4  This training process only took about 10 hours on ourUbuntu server (Intel 4-core Xeon 3.47GHz, 132 GB ofRAM).Train Develop Test#Sentences 261,906 2,569 2,576#Chn.
Words 3,623,516 38,585 38,648#Chn.
VOC.
43,112 3,287 3,460#Eng.
Words 3,627,028 38,329 38,510#Eng.
VOC.
44,221 3,993 4,046Table 1: Corpus StatisticsIntervals #Sentences #Words W/S[0.9, 1.0) 269 4,468 16.6[0.8, 0.9) 362 5,004 13.8[0.7, 0.8) 290 4,046 14.0[0.6, 0.7) 379 4,998 13.2[0.5, 0.6) 472 6,073 12.9[0.4, 0.5) 401 5,921 14.8[0.3, 0.4) 305 5,499 18.0(0.0, 0.3) 98 2,639 26.9(0.0, 1.0) 2,576 38,648 15.0Table 2: Corpus Statistics for Test-Set16Intervals TM SMT Model-I Model-II Model-III Koehn-10 Ma-11 Ma-11-U[0.9, 1.0) 81.31 81.38 85.44  * 86.47  *# 89.41  *# 82.79 77.72 82.78[0.8, 0.9) 73.25 76.16 79.97  * 80.89  * 84.04  *# 79.74  * 73.00 77.66[0.7, 0.8) 63.62 67.71 71.65  * 72.39  * 74.73  *# 71.02  * 66.54 69.78[0.6, 0.7) 43.64 54.56 54.88    # 55.88  *# 57.53  *# 53.06 54.00 56.37[0.5, 0.6) 27.37 46.32 47.32  *# 47.45  *# 47.54  *# 39.31 46.06 47.73[0.4, 0.5) 15.43 37.18 37.25    # 37.60    # 38.18  *# 28.99 36.23 37.93[0.3, 0.4) 8.24 29.27 29.52    # 29.38    # 29.15    # 23.58 29.40 30.20(0.0, 0.3) 4.13 26.38 25.61    # 25.32    # 25.57    # 18.56 26.30 26.92(0.0, 1.0) 40.17 53.03 54.57  *# 55.10  *# 56.51  *# 50.31 51.98 54.32Table 3: Translation Results (BLEU%).
Scores marked by ?*?
are significantly better (p < 0.05) than both TMand SMT systems, and those marked by ?#?
are significantly better (p < 0.05) than Koehn-10.Intervals TM SMT Model-I Model-II Model-III Koehn-10 Ma-11 Ma-11-U[0.9, 1.0) 9.79 13.01 9.22      # 8.52    *# 6.77    *# 13.01 18.80 11.90[0.8, 0.9) 16.21 16.07 13.12  *# 12.74  *# 10.75  *# 15.27 20.60 14.74[0.7, 0.8) 27.79 22.80 19.10  *# 18.58  *# 17.11  *# 21.85 25.33 21.11[0.6, 0.7) 46.40 33.38 32.63    # 32.27  *# 29.96  *# 35.93 35.24 31.76[0.5, 0.6) 62.59 39.56 38.24  *# 38.77  *# 38.74  *# 47.37 40.24 38.01[0.4, 0.5) 73.93 47.19 47.03    # 46.34  *# 46.00  *# 56.84 48.74 46.10[0.3, 0.4) 79.86 55.71 55.38    # 55.44    # 55.87    # 64.55 55.93 54.15(0.0, 0.3) 85.31 61.76 62.38    # 63.66    # 63.51    # 73.30 63.00 60.67(0.0, 1.0) 50.51 35.88 34.34  *# 34.18  *# 33.26  *# 40.75 38.10 34.49Table 4: Translation Results (TER%).
Scores marked by ?*?
are significantly better (p < 0.05) than both TM andSMT systems, and those marked by ?#?
are significantly better (p < 0.05) than Koehn-10.conducted using the Moses phrase-based decoder(Koehn et al, 2007).Table 3 and 4 give the translation results ofTM, SMT, and three integrated models in the testset.
In the tables, the best translation results (ei-ther in BLEU or TER) at each interval have beenmarked in bold.
Scores marked by ?*?
are signif-icantly better (p < 0.05) than both the TM andthe SMT systems.It can be seen that TM significantly exceedsSMT at the interval [0.9, 1.0) in TER score,which illustrates why professional translatorsprefer TM rather than SMT as their assistant tool.Compared with TM and SMT, Model-I is signif-icantly better than the SMT system in eitherBLEU or TER when the fuzzy match score isabove 0.7; Model-II significantly outperformsboth the TM and the SMT systems in eitherBLEU or TER when the fuzzy match score isabove 0.5; Model-III significantly exceeds boththe TM and the SMT systems in either BLEU orTER when the fuzzy match score is above 0.4.All these improvements show that our integratedmodels have combined the strength of both TMand SMT.However, the improvements from integratedmodels get less when the fuzzy match score de-creases.
For example, Model-III outperformsSMT 8.03 BLEU points at interval [0.9, 1.0),while the advantage is only 2.97 BLEU points atinterval [0.6, 0.7).
This is because lower fuzzymatch score means that there are more un-matched parts between  and ; the output ofTM is thus less reliable.Across all intervals (the last row in the table),Model-III not only achieves the best BLEU score(56.51), but also gets the best TER score (33.26).If intervals are evaluated separately, when thefuzzy match score is above 0.4, Model-III out-performs both Model-II and Model-I in eitherBLEU or TER.
Model-II also exceeds Model-I ineither BLEU or TER.
The only exception is atinterval [0.5, 0.6), in which Model-I achieves thebest TER score.
This might be due to that theoptimization criterion for MERT is BLEU ratherthan TER in our work.4.4 Comparison with Previous WorkIn order to compare our proposed models withprevious work, we re-implement two XML-Markup approaches: (Koehn and Senellart, 2010)and (Ma et al 2011), which are denoted asKoehn-10 and Ma-11, respectively.
They areselected because they report superior perfor-mances in the literature.
A brief description ofthem is as follows:17Source??
0 ??
1 ?
2 ??
3 ??
4 ?5 internet6 explorer7 ?
8 ??
9 internet10 ??
11 ???
12?
13 ?
14 ??
15 ?16 ??
17 ?
18 ?
19 ??
20 ??
21 ??
22 ?23Referenceif0 you1 disable2 this3 policy4 setting5 ,6 internet7 explorer8 does9 not10 check11 the12 internet13for14 new15 versions16 of17 the18 browser19 ,20 so21 does22 not23 prompt24 users25 to26 install27them28 .29TMSource??
0 ?
1 ??
2 ?
3 ??
4 ??
5 ?6 internet7 explorer8 ?
9 ??
10 internet11 ??
12 ???
13 ?
14 ?
15 ??
16 ?17 ??
18 ?
19 ?
20 ??
21 ??
22 ??
23 ?24TMTargetif0 you1 do2 not3 configure4 this5 policy6 setting7 ,8 internet9 explorer10 does11 not12 check13 the14internet15 for16 new17 versions18 of19 the20 browser21 ,22 so23 does24 not25 prompt26 users27 to28install29 them30 .31TMAlignment0-0 1-3 2-4 3-5 4-6 5-7 6-8 7-9 8-10 9-11 11-15 13-21 14-19 15-17 16-18 17-22 18-23 19-2421-26 22-27 23-29 24-31SMTif you disable this policy setting , internet explorer does not prompt users to install internet fornew versions of the browser .
[Miss 7 target words: 9~12, 20~21, 28; Has one wrong permuta-tion]Koehn-10if you do you disable this policy setting , internet explorer does not check the internet for newversions of the browser , so does not prompt users to install them .
[Insert two spurious targetwords]Ma-11if you disable this policy setting , internet explorer does not prompt users to install internet fornew versions of the browser .
[Miss 7 target words: 9~12, 20~21, 28; Has one wrong permuta-tion]Model-Iif you disable this policy setting , internet explorer does not prompt users to install new ver-sions of the browser , so does not check the internet .
[Miss 2 target words: 14, 28; Has onewrong permutation]Model-IIif you disable this policy setting , internet explorer does not prompt users to install new ver-sions of the browser , so does not check the internet .
[Miss 2 target words: 14, 28; Has onewrong permutation]Model-IIIif you disable this policy setting , internet explorer does not check the internet for new versionsof the browser , so does not prompt users to install them .
[Exactly the same as the reference]Figure 2: A Translation Example at Interval [0.9, 1.0] (with FMS=0.920)Koehn et al (2010) first find out the un-matched parts between the given source sentenceand TM source sentence.
Afterwards, for eachunmatched phrase in the TM source sentence,they replace its corresponding translation in theTM target sentence by the corresponding sourcephrase in the input sentence, and then mark thesubstitution part.
After replacing the correspond-ing translations of all unmatched source phrasesin the TM target sentence, an XML input sen-tence (with mixed TM target phrases and markedinput source phrases) is thus obtained.
The SMTdecoder then only translates the un-matched/marked source phrases and gets the de-sired results.
Therefore, the inserted parts in theTM target sentence are automatically included.They use fuzzy match score to determine wheth-er the current sentence should be marked or not;and their experiments show that this method isonly effective when the fuzzy match score isabove 0.8.Ma et al (2011) think fuzzy match score is notreliable and use a discriminative learning methodto decide whether the current sentence should bemarked or not.
Another difference between Ma-11 and Koehn-10 is how the XML input is con-structed.
In constructing the XML input sentence,Ma-11 replaces each matched source phrase inthe given source sentence with the correspondingTM target phrase.
Therefore, the inserted parts inthe TM target sentence are not included.
In Ma?sanother paper (He et al, 2011), more linguisticfeatures for discriminative learning are also add-ed.
In our work, we only re-implement the XML-Markup method used in (He et al, 2011; Ma et al2011), but do not implement the discriminativelearning method.
This is because the featuresadopted in their discriminative learning are com-plicated and difficult to re-implement.
However,the proposed Model-III even outperforms theupper bound of their methods, which will be dis-cussed later.Table 3 and 4 give the translation results ofKoehn-10 and Ma-11 (without the discriminator).Scores marked by ?#?
are significantly better (p< 0.05) than Koehn-10.
Besides, the upper boundof (Ma et al 2011) is also given in the tables,which is denoted as Ma-11-U.
We calculate this18upper bound according to the method describedin (Ma et al, 2011).
Since He et al, (2011) onlyadd more linguistic features to the discriminativelearning method, the upper bound of (He et al,2011) is still the same with (Ma et al, 2011);therefore, Ma-11-U applies for both cases.It is observed that Model-III significantly ex-ceeds Koehn-10 at all intervals.
More important-ly, the proposed models achieve much betterTER score than the TM system does at interval[0.9, 1.0), but Koehn-10 does not even exceedthe TM system at this interval.
Furthermore,Model-III is much better than Ma-11-U at mostintervals.
Therefore, it can be concluded that theproposed models outperform the pipeline ap-proaches significantly.Figure 2 gives an example at interval [0.9, 1.0),which shows the difference among different sys-tem outputs.
It can be seen that ?you do?
is re-dundant for Koehn-10, because they are inser-tions and thus are kept in the XML input.
How-ever, SMT system still inserts another ?you?,regardless of ?you do?
has already existed.
Thisproblem does not occur at Ma-11, but it missessome words and adopts one wrong permutation.Besides, Model-I selects more right words thanSMT does but still puts them in wrong positionsdue to ignoring TM reordering information.
Inthis example, Model-II obtains the same resultswith Model-I because it also lacks reorderinginformation.
Last, since Model-III considers bothTM content and TM position information, itgives a perfect translation.5 Conclusion and Future WorkUnlike the previous pipeline approaches, whichdirectly merge TM phrases into the final transla-tion result, we integrate TM information of eachsource phrase into the phrase-based SMT at de-coding.
In addition, all possible TM targetphrases are kept and the proposed models selectthe best one during decoding via referring SMTinformation.
Besides, the integrated model con-siders the probability information of both SMTand TM factors.The experiments show that the proposedModel-III outperforms both the TM and the SMTsystems significantly (p < 0.05) in either BLEUor TER when fuzzy match score is above 0.4.Compared with the pure SMT system, Model-IIIachieves overall 3.48 BLEU points improvementand 2.62 TER points reduction on a Chinese?English TM database.
Furthermore, Model-IIIsignificantly exceeds all previous pipeline ap-proaches.
Similar improvements are also ob-served on the Hansards parts of LDC2004T08(not shown in this paper due to space limitation).Since no language-dependent feature is adopted,the proposed approaches can be easily adaptedfor other language pairs.Moreover, following the approaches ofKoehn-10 and Ma-11 (to give a fair comparison),training data for SMT and TM are the same inthe current experiments.
However, the TM isexpected to play an even more important rolewhen the SMT training-set differs from the TMdatabase, as additional phrase-pairs that are un-seen in the SMT phrase table can be extractedfrom TM (which can then be dynamically addedinto the SMT phrase table at decoding time).
Ouranother study has shown that the integrated mod-el would be even more effective when the TMdatabase and the SMT training data-set are fromdifferent corpora in the same domain (not shownin this paper).
In addition, more source phrasescan be matched if a set of high-FMS sentences,instead of only the sentence with the highestFMS, can be extracted and referred at the sametime.
And it could further raise the performance.Last, some related approaches (Smith andClark, 2009; Phillips, 2011) combine SMT andexample-based machine translation (EBMT)(Nagao, 1984).
It would be also interesting tocompare our integrated approach with that oftheirs.AcknowledgmentsThe research work has been funded by the Hi-Tech Research and Development Program(?863?
Program) of China under Grant No.2011AA01A207, 2012AA011101, and2012AA011102 and also supported by the KeyProject of Knowledge Innovation Program ofChinese Academy of Sciences under GrantNo.KGZD-EW-501.The authors would like to thank the anony-mous reviewers for their insightful commentsand suggestions.
Our sincere thanks are also ex-tended to Dr. Yanjun Ma and Dr. Yifan He fortheir valuable discussions during this study.ReferencesMichael Auli, Adam Lopez, Hieu Hoang and PhilippKoehn, 2009.
A systematic analysis of translationmodel search spaces.
In Proceedings of the FourthWorkshop on Statistical Machine Translation, pag-es 224?232.19Timothy C. Bell, J.G.
Cleary and Ian H. Witten, 1990.Text compression: Prentice Hall, Englewood Cliffs,NJ.Ergun Bi?ici and Marc Dymetman.
2008.
Dynamictranslation memory: using statistical machine trans-lation to improve translation memory fuzzy match-es.
In Proceedings of the 9th International Confer-ence on Intelligent Text Processing and Computa-tional Linguistics (CICLing 2008), pages 454?465.Stanley F. Chen and Joshua Goodman.
1998.
An em-pirical study of smoothing techniques for languagemodeling.
Technical Report TR-10-98, HarvardUniversity Center for Research in ComputingTechnology.Yifan He, Yanjun Ma, Josef van Genabith and AndyWay, 2010.
Bridging SMT and TM with transla-tion recommendation.
In Proceedings of the 48thAnnual Meeting of the Association for Computa-tional Linguistics (ACL), pages 622?630.Yifan He, Yanjun Ma, Andy Way and Josef vanGenabith.
2011.
Rich linguistic features for transla-tion memory-inspired consistent translation.
InProceedings of the Thirteenth Machine TranslationSummit, pages 456?463.Reinhard Kneser and Hermann Ney.
1995.
Improvedbacking-off for m-gram language modeling.
InProceedings of the IEEE International Conferenceon Acoustics, Speech and Signal Processing, pages181?184.Katrin Kirchhoff, Jeff A. Bilmes and Kevin Duh.2007.
Factored language models tutorial.
Technicalreport, Department of Electrical Engineering, Uni-versity of Washington, Seattle, Washington, USA.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proceedings ofthe 2004 Conference on Empirical Methods inNatural Language Processing (EMNLP), pages388?395, Barcelona, Spain.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer and Ond?ej Bojar.
2007.Moses: Open source toolkit for statistical machinetranslation.
In Proceedings of the ACL 2007 Demoand Poster Sessions, pages 177?180.Philipp Koehn, Franz Josef Och and Daniel Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology,pages 48?54.Philipp Koehn and Jean Senellart.
2010.
Convergenceof translation memory and statistical machinetranslation.
In AMTA Workshop on MT Researchand the Translation Industry, pages 21?31.Elina Lagoudaki.
2006.
Translation memories survey2006: Users?
perceptions around tm use.
In Pro-ceedings of the ASLIB International ConferenceTranslating and the Computer 28, pages 1?29.Qi Li, Biing-Hwang Juang, Qiru Zhou, and Chin-HuiLee.
2000.
Automatic verbal information verifica-tion for user authentication.
IEEE transactions onspeech and audio processing, Vol.
8, No.
5, pages1063?6676.Vladimir Iosifovich Levenshtein.
1966.
Binary codescapable of correcting deletions, insertions, and re-versals.
Soviet Physics Doklady, 10 (8).
pages 707?710.Yanjun Ma, Yifan He, Andy Way and Josef vanGenabith.
2011.
Consistent translation using dis-criminative learning: a translation memory-inspiredapproach.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguis-tics, pages 1239?1248, Portland, Oregon.Makoto Nagao, 1984.
A framework of a mechanicaltranslation between Japanese and English by anal-ogy principle.
In: Banerji, Alick Elithorn and  Ran-an (ed).
Artifiical and Human Intelligence: EditedReview Papers Presented at the InternationalNATO Symposium on Artificial and Human Intelli-gence.
North-Holland, Amsterdam, 173?180.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofthe 41st Annual Meeting of the Association forComputational Linguistics, pages 160?167.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignmentmodels.
Computational Linguistics, 29 (1).
pages19?51.Kishore Papineni, Salim Roukos, Todd Ward andWei-Jing Zhu.
2002.
BLEU: a method for automat-ic evaluation of machine translation.
In Proceed-ings of the 40th Annual Meeting of the Associationfor Computational Linguistics (ACL), pages 311?318.Aaron B. Phillips, 2011.
Cunei: open-source machinetranslation with relevance-based models of eachtranslation instance.
Machine Translation, 25 (2).pages 166-177.Richard Sikes.
2007, Fuzzy matching in theory andpractice.
Multilingual, 18(6):39?43.Michel Simard and Pierre Isabelle.
2009.
Phrase-based machine translation in a computer-assistedtranslation environment.
In Proceedings of theTwelfth Machine Translation Summit (MT SummitXII), pages 120?127.James Smith and Stephen Clark.
2009.
EBMT forSMT: a new EBMT-SMT hybrid.
In Proceedingsof the 3rd International Workshop on Example-20Based Machine Translation (EBMT'09), pages 3?10, Dublin, Ireland.Matthew Snover, Bonnie Dorr, Richard Schwartz,Linnea Micciulla and John Makhoul.
2006.
Astudy of translation edit rate with targeted humanannotation.
In Proceedings of Association for Ma-chine Translation in the Americas (AMTA-2006),pages 223?231.Andreas Stolcke.
2002.
SRILM-an extensible lan-guage modeling toolkit.
In Proceedings of the In-ternational Conference on Spoken Language Pro-cessing, pages 311?318.Guillaume Wisniewski, Alexandre Allauzen andFran?ois Yvon, 2010.
Assessing phrase-basedtranslation models with oracle decoding.
In Pro-ceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, pages933?943.Ian H. Witten and Timothy C. Bell.
1991.
The zero-frequency problem: estimating the probabilities ofnovel events in adaptive test compression.
IEEETransactions on Information Theory, 37(4): 1085?1094, July.Ventsislav Zhechev and Josef van Genabith.
2010.Seeding statistical machine translation with transla-tion memory output through tree-based structuralalignment.
In Proceedings of the 4th Workshop onSyntax and Structure in Statistical Translation,pages 43?51.Andreas Zollmann, Ashish Venugopal, Franz JosefOch and Jay Ponte, 2008.
A systematic comparisonof phrase-based, hierarchical and syntax-augmented statistical MT.
In Proceedings of the22nd International Conference on ComputationalLinguistics (Coling 2008), pages 1145?1152.21
