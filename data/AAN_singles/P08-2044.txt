Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 173?176,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsDistributed Listening: A Parallel Processing Approach to AutomaticSpeech RecognitionYolanda McMillian Juan E. Gilbert3101 Shelby Center 3101 Shelby CenterAuburn University Auburn UniversityAuburn, AL  36849-5347, USA Auburn, AL  36849-5347, USAmcmilym@auburn.edu gilbert@auburn.eduAbstractWhile speech recognition systems have comea long way in the last thirty years, there is stillroom for improvement.
Although readilyavailable, these systems are sometimes inac-curate and insufficient.
The research pre-sented here outlines a technique calledDistributed Listening which demonstrates no-ticeable improvements to existing speech rec-ognition methods.
The Distributed Listeningarchitecture introduces the idea of multiple,parallel, yet physically separate automaticspeech recognizers called listeners.
Distrib-uted Listening also uses a piece of middlewarecalled an interpreter.
The interpreter resolvesmultiple interpretations using the PhraseResolution Algorithm (PRA).
These effortswork together to increase the accuracy of thetranscription of spoken utterances.1 IntroductionResearch in the area of natural language processinghas been on-going for over thirty years (NaturalLanguage Software Registry, 2004; Jurafsky andMartin, 2000); however, there is still room for im-provement with mainstream speech recognitionsystems (Deng, 2004).
Distributed Listening willfurther research in this area.
The concept is basedaround the idea of multiple speech input sources.Previous research activities involved a single mi-crophone with multiple, separate recognizers thatall yielded improvements in accuracy.
DistributedListening uses multiple, parallel speech recogniz-ers, with each recognizer having its own inputsource (Gilbert, 2005).
Each recognizer is a lis-tener.
Once input is collected from the listeners,one machine, the interpreter, processes all of theinput (see figure 1).
To process the input, a phraseresolution algorithm is used.This approach is analogous to a crime scenewith multiple witnesses (the listeners) and a detec-tive (the interpreter) who pieces together the sto-ries of the witnesses using his/her knowledge ofcrime scenes to form a hypothesis of the actualevent.
Each witness will have a portion of thestory that is the same as the other witnesses.
It isup to the detective to fill in the blanks.
With Dis-tributed Listening, the process is very similar.Each listener will have common recognition resultsand the interpreter will use the phrase resolutionalgorithm to resolve conflicts.Figure 1.
Distributed Listening Architecture1732 BackgroundAutomatic speech recognition systems convert aspeech signal into a sequence of words, usuallybased on the Hidden Markov Model (HMM), inwhich words are constructed from a sequence ofstates (Baum, 1972; Young et al, 1989; Young1990; Furui, 2002).There are several systems that used the HMMalong with multiple speech recognizers in an effortto improve speech recognition, as discussed next.2.1 Enhanced Majority RulesBarry (et al, 1994) took three different AutomaticSpeech Recognition (ASR) systems, along with anEnhanced Majority Rules (EMR) softwarealgorithm.
Each of the three individual systemsreceived the same input, performed speechrecognition, and sent the result to the mastersystem.The EMR resolved inconsistencies by lookingfor agreement from the individual systems for therecognized word.
If there was no majority agree-ment, the EMR looked to the second word foragreement before relying on the distance scores.This architecture produced better recognition accu-racy than each of the individual systems.While an improvement was made, the architec-ture can suffer from distorted input.
Since eachsystem receives the same input, if the input signalis not good, then all of the individual systems willreceive bad input.2.2 Virtual Intelligent CodriverThe Virtual Intelligent Codriver (VICO) projectalso used multiple ASR systems in parallel (Bruttiet al, 2004; Cristoforetti et al, 2003).
Each ASRreceived the same input and had its own languagemodel.
The resulting interpretations from eachASR are compared to each other using confidencescores.
The interpretation with the highestrecognition accuracy is selected.
While theexperiments resulted in noticeable improvementsover the individual ASR systems, there are twoshortcomings.
First, if the input signal is distorted,then each recognizer will receive bad input.Secondly, if each recognizer contains a piece of theoptimal interpretation, then this architecture fallsshort.2.3 Recognized Output Voting Error Re-ductionThe Recognizer Output Voting Error Reduction(ROVER) system is a composite of multiple ASRsystems that uses a voting process to reconciledifferences in the individual ASR system outputs(Fiscus, 1997).
Multiple interpretations are passedfrom each recognition engine to the alignmentmodule.
Once aligned, the voting module iscalled.
The voting module scores each wordwithin the alignment vertically and the words withthe highest scores are chosen.
On average, thiscomposite ASR system produces a lower error ratethan any of the individual systems, but suffersfrom order of combination and ties.2.4 Modified ROVERTo solve the problem that results from the order ofcombination and ties of the original ROVERsystem, Schwenk proposed a modified ROVERsystem that used a dynamic programmingalgorithm built on language models (Schwenk andGauvain, 2000).
The modified ROVER systemresulted in a reduction in the word error rates overthe original ROVER system.3 Distributed ListeningDistributed Listening builds on the architecturesthat use multiple speech recognizers and enhancesit with the use of multiple input sources.Distributed Listening is made of three signifi-cant parts: Listeners, an Interpreter, and a PhraseResolution Algorithm.3.1 ListenersDistributed Listening uses multiple speech recog-nizers, working in parallel, to process the spokeninput.
Each recognizer is called a listener and isequipped with it?s own input source.
Each listeneris a separate, physical computing device with itsown memory, processor, and disk space.
Each lis-tener collects input from the user.
The result ofeach listener is passed to the interpreter.3.2 InterpreterOnce input is collected from the listeners, the inputis passed to the interpreter.
The interpreter will174process all of the input collected from each listeneras described next.3.3 Phrase Resolution AlgorithmTo resolve multiple interpretations from the listen-ers, the Phrase Resolution Algorithm (PRA) isused.The underlying grammar of the PRA is based onan N-gram language model.
An N-gram languagemodel is used by the recognizer to predict wordsequences.
Distributed Listening uses an N-gramof size 1, also known as a unigram.
The grammarconsists of known utterances that can be made bythe user.The unigram grammar is stored in a phrasedatabase.
The grammar is organized according toindividual words and phrases.
Each phrase isplaced in a table.
The phrases are broken downinto their individual words and placed in anothertable.
The table of words keeps a count of thenumber of times each word appears in each phrase,resembling the unigram language model.To determine the most likely spoken phrase,queries are made against the collection of individ-ual words, also known as the complete word set.The queries try to identify matching phrase(s)based on specified words.
The matching phrase(s)with the highest concentrations of words is re-turned by the query.The word concentration is determined by com-paring the length of the phrase with the number ofmatching words found in the complete word set.The concentration of the number of words foundwithin each phrase is calculated using all interpre-tations from the listeners.
The phrase(s) with thehighest concentration of words is the most likelyspoken phrase.4 System ArchitectureThere are multiple models for Distributed Listen-ing; Homogeneous, Heterogeneous, and Hybrid.The Homogeneous model uses the same grammarfor each listener.
Within the Heterogeneousmodel, each listener uses a different grammar.
TheHybrid model contains a combination of the Ho-mogenous and Heterogeneous models.4.1 HomogeneousIn a homogenous Distributed Listening architec-ture, each listener has the same grammar or lan-guage model.
Although all of the listeners areidentical in capturing the input, this architectureallows for the different perspectives of the utter-ances to also be captured.4.2 HeterogeneousHeterogeneous architectures use different gram-mars or language models on each listener.
Eachlistener has its own input source and recognizerand implies a distributed grammar/language model.This allows for flexibility as very large grammarsand vocabularies can be distributed across severallisteners.4.3 HybridThe hybrid architecture is a homogenous architec-ture of heterogeneous Distributed Listening nodes,as shown in figure 2.
This gives the embeddedenvironment the ability to recognize multiple lan-guages, as well as accommodate translations ofinter-mixed spoken language.Figure 2.
Hybrid Distributed Listening Architecture5 ConclusionThe goal of Distributed Listening research is totake a unique approach in order to enhance thesuccess of the traditional approaches to speechrecognition.
The approach of Distributed Listen-ing directly mimics people.
The psychology do-main has shown that people use a form ofDistributed Listening called Dichotic Listening,where people listen to two voices, one in each ear,175at the same time (Bruder, 2004).
Distributed Lis-tening is a natural extension of Dichotic Listening,where computers are listening in the same manneras people.
Distributed Listening is an attempt toenable computer systems to perform similar tohumans while decreasing error rates.Preliminary studies have shown a decrease inerror rates.
Early results indicate that DistributedListening is a viable alternative to current speechrecognition systems.
Additional studies are beingplanned that will effectively test the PhraseResolution Algorithm.ReferencesBarry, T., Solz, T., Reising, J.
& Williamson, D. Thesimultaneous use of three machine speech recogni-tion systems to increase recognition accuracy, InProceedings of the IEEE 1994 National Aerospaceand Electronics Conference, vol.2, pp.
667 - 671,1994.Baum, L.E.
An inequality and associated maximiza-tion technique in statistical estimation for prob-abilistic functions of Markov process.
Inequalities3, 1-8, 1972.Bruder, G.E., Stewart, J.W., McGrath, P.J., Deliyan-nides, D., Quitkin, F.M.
Dichotic listening tests offunctional brain asymmetry predict response tofluoxetine in depressed women and men.
Neuro-psychopharmacology, 29(9), pp.
1752-1761, 2004.Brutti, A., Coletti, P.,  Cristoforetti, L., Geutner, P.,Giacomini, A.,  Gretter, R., et al Use of MultipleSpeech Recognition Units in a In-car AssistanceSystems, chapter in "DSP for Vehicle and MobileSystems", Kluwer Publishers, 2004.Cristoforetti, L., Matassoni, M., Omologo, M. &Svaizer, P., Use of parallel recognizers for robustin-car speech interaction, In Proceedings of theIEEE International Conference on Acoustic, Speech,and Signal Processing [ICASSP 2003], Hong-Kong,2003.Deng, L. & Huang, X.,  Challenges in adopting speechrecognition, Communications of the ACM, vol.
47,no.
1, pp.
69-75, January 2004.Fiscus, J. G., A post-processing system to yield re-duced error word rates: Recognizer output votingerror reduction (ROVER).
In IEEE Workshop onAutomatic Speech Recognition and Understanding,pp.
347?354, 1997.Furui, S., Recent progress in spontaneous speech rec-ognition and understanding, In Proceedings of theIEEE Workshop on Multimedia Signal Processing,2002.Gilbert, J. E.  (2005).
Distributed Listening Research.In Proceedings of AVIOS Speech Technology Track,San Francisco, California, SpeechTEK West, pp.
1 ?10.Jurafsky, D. & Martin, J., Speech and Language Proc-essing, Prentice Hall, 2000.Natural Language Software Registry, [Online].
Availa-ble: http://registry.dfki.de/, 2004.Schwenk, H. & Gauvain, J., Improved ROVER usingLanguage Model Information, In ISCA ITRWWorkshop on Automatic Speech Recognition: Chal-lenges for the new Millenium, Paris, pp.
47?52,2000.Young, S.R., Use of dialog, pragmatics and semanticsto enhance speech recognition, Speech Communi-cation, vol.
9, pp.
551-564, 1990.Young, S.R.,  Hauptmann, A.G. , Ward, W.H.
, Smith,E.T.
& Werner, P., High level knowledge sources inusable speech recognition systems, Communica-tions of the ACM, vol.
31, no.
2, pp.
183-194, 1989.176
