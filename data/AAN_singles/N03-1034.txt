Evaluating the Evaluation:A Case Study Using the TREC 2002 Question Answering TrackEllen M. VoorheesNational Institute of Standards and TechnologyGaithersburg, MD 20899ellen.voorhees@nist.govAbstractEvaluating competing technologies on a com-mon problem set is a powerful way to improvethe state of the art and hasten technology trans-fer.
Yet poorly designed evaluations can wasteresearch effort or even mislead researchers withfaulty conclusions.
Thus it is important to ex-amine the quality of a new evaluation task to es-tablish its reliability.
This paper provides an ex-ample of one such assessment by analyzing thetask within the TREC 2002 question answer-ing track.
The analysis demonstrates that com-parative results from the new task are stable,and empirically estimates the size of the dif-ference required between scores to confidentlyconclude that two runs are different.Metric-based evaluations of human language technol-ogy such as MUC and TREC and DUC continue to pro-liferate (Sparck Jones, 2001).
This proliferation is notdifficult to understand: evaluations can forge communi-ties, accelerate technology transfer, and advance the stateof the art.
Yet evaluations are not without their costs.
Inaddition to the financial resources required to support theevaluation, there are also the costs of researcher time andfocus.
Since a poorly defined evaluation task wastes re-search effort, it is important to examine the validity of anevaluation task.
In this paper, we assess the quality ofthe new question answering task that was the focus of theTREC 2002 question answering track.TREC is a workshop series designed to encourage re-search on text retrieval for realistic applications by pro-viding large test collections, uniform scoring procedures,and a forum for organizations interested in comparingresults.
The conference has focused primarily on thetraditional information retrieval problem of retrieving aranked list of documents in response to a statement ofinformation need, but also includes other tasks, calledtracks, that focus on new areas or particularly difficultaspects of information retrieval.
A question answering(QA) track was started in TREC in 1999 (TREC-8) to ad-dress the problem of returning answers, rather than doc-ument lists, in response to a question.The task for each of the first three years of the QA trackwas essentially the same.
Participants received a largecorpus of newswire documents and a set of factoid ques-tions such as How many calories are in a Big Mac?
andWho invented the paper clip?.
Systems were required toreturn a ranked list of up to five [document-id, answer-string] pairs per question such that each answer stringwas believed to contain an answer to the question.
Hu-man assessors read each string and decided whether thestring actually did contain an answer to the question.
Anindividual question received a score equal to the recip-rocal of the rank at which the first correct response wasreturned, or zero if none of the five responses containeda correct answer.
The score for a submission was thenthe mean of the individual questions?
reciprocal ranks.Analysis of the TREC-8 track confirmed the reliability ofthis evaluation task (Voorhees and Tice, 2000): the asses-sors understood and could do their assessing job; relativescores between systems were stable despite differencesof opinion by assessors; and intuitively better systems re-ceived better scores.The task for the TREC 2002 QA track changed sig-nificantly from the previous years?
task, and thus a newassessment of the track is needed.
This paper providesthat assessment by examining both the ability of the hu-man assessors to make the required judgments and theeffect that differences in assessor opinions have on com-parative results, plus empirically establishing confidenceintervals for the reliability of a comparison as a functionof the difference in effectiveness scores.
The first sectiondefines the 2002 QA task and provides a brief summaryof the system results.
The following three sections lookat each of the evaluation issues in turn.
The final sec-Edmonton, May-June 2003Main Papers , pp.
181-188Proceedings of HLT-NAACL 2003tion summarizes the findings, and outlines shortcomingsof the evaluation that remain to be addressed.1 The TREC 2002 QA TrackThe goal of the question answering track is to foster re-search on systems that retrieve answers rather than docu-ments, with particular emphasis on systems that functionin unrestricted domains.
To date the track has consid-ered only a very restricted version of the general ques-tion answering problem, finding answers to closed-classquestions in a large corpus of newspaper articles.
Kupiecdefined a closed-class question as ?a question stated innatural language, which assumes some definite answertypified by a noun phrase rather than a procedural an-swer?
(Kupiec, 1993).
The TREC 2002 track continuedto use closed-class questions, but made two major de-partures from the task as defined in earlier years.
Thefirst difference was that systems were to return exact an-swers rather than the text snippets containing an answerthat were accepted previously.
The second difference wasthat systems were required to return exactly one responseper question and the questions were to be ranked by thesystem?s confidence in the answer it had found.The change to exact answers was motivated by the be-lief that a system?s ability to recognize the precise extentof the answer is crucial to improving question answeringtechnology.
The problems with using text snippets as re-sponses were illustrated in the TREC 2001 track.
Each ofthe answer strings shown in Figure 1 was judged correctfor the question What river in the US is known as the BigMuddy?, yet earlier responses are clearly better than laterones.
Accepting only exact answers as correct forces sys-tems to demonstrate that they know precisely where theanswer lies in the snippets.The second change, ranking questions by confidencein the answer, tested a system?s ability to recognize whenit has found a correct answer.
Systems must be able torecognize when they do not know the answer to avoidreturning incorrect responses.
In many applications re-turning a wrong answer is much worse than returning a?Don?t know?
response.1.1 Task DefinitionIncorporating these two changes into the previous QAtask resulted in the following task definition.
Participantswere given a large corpus of newswire articles and a setof 500 closed-class questions.
Some of the questions didnot have answers in the document collection.
A run con-sisted of exactly one response for each question.
A re-sponse was either a [document-id, answer-string] pair orthe string ?NIL?, which was used to indicate that the sys-tem believed there was no correct answer in the collec-tion.
Within a run, questions were ordered from mostconfident response to least confident response.
All runswere required to be produced completely automatically?no manual intervention of any kind was permitted.The document collection used as the source of answerswas the the AQUAINT Corpus of English News Text(LDC catalog number LDC2002T31).
The collectionis comprised of documents from three different sources:the AP newswire from 1998?2000, the New York Timesnewswire from 1998?2000, and the (English portion ofthe) Xinhua News Agency from 1996?2000.
There areapproximately 1,033,000 documents and 3 gigabytes oftext in the collection.The test set of questions were drawn from MSNSearchand AskJeeves logs.
NIST assessors searched the docu-ment collection for answers to candidate questions fromthe logs.
NIST staff selected the final test set from amongthe candidates that had answers, keeping some questionsfor which the assessors found no answer.
NIST correctedthe spelling, punctuation, and grammar of the questionsin the logs1, but left the content as it was.
NIST didnot include any definition questions (Who is Duke Elling-ton?
What are polymers?)
in the test set, but otherwisemade no attempt to control the relative number of differ-ent types of questions in the test set.A system response consisting of an [document-id,answer-string] pair was assigned exactly one judgmentby a human assessor as follows:wrong: the answer string does not contain a correct an-swer or the answer is not responsive;not supported: the answer string contains a correct an-swer but the document returned does not supportthat answer;not exact: the answer string contains a correct answerand the document supports that answer, but thestring contains more than just the answer (or is miss-ing bits of the answer);right: the answer string consists of exactly a correct an-swer and that answer is supported by the documentreturned.Only responses judged right were counted as correct inthe final scoring.
A NIL response was counted as correctif there is no known answer in the document collectionfor that question (i.e., the assessors did not find an an-swer during the candidate selection phase and no systemreturned a right response for it).
Forty-six questions haveno known answer in the collection.The scoring metric used, called the confidence-weighted score, was chosen to emphasize the system?sability to correctly rank its responses.
The metric is1Unfortunately, some errors remain in the test questions.Scores were nevertheless computed over all 500 questions asreleased by NIST.the MississippiKnown as Big Muddy, the Mississippi is the longestas Big Muddy , the Mississippi is the longestmessed with .
Known as Big Muddy , the MississipMississippi is the longest river in the USthe Mississippi is the longest river in the US,the Mississippi is the longest river(Mississippi)has brought the Mississippi to its lowestipes.In Life on the Mississippi, Mark Twain wrote tSoutheast;Mississippi;Mark Twain;officials beganKnown; Mississippi; US,; Minnesota;Gulf MexicoMud Island,;Mississippi;"The;-- history,;MemphisFigure 1: Correct text snippets for What river in the US is known as the Big Muddy?an analog of document retrieval?s uninterpolated averageprecision in that it rewards a system for a correct answerearly in the ranking more than it rewards for a correct an-swer later in the ranking.
More formally, if there are questions in the test set, the confidence-weighted score isdefined to be  number correct in first  ranks1.2 Track ResultsTable 1 gives evaluation results for a subset of the runssubmitted to the TREC 2002 QA track.
The table in-cludes one run each from the ten groups who submittedthe top-scoring runs.
The run shown in the table is therun with the best confidence-weighted score (?Score?
).Also given in the table are the percentage of questionsanswered correctly, and the precision and recall for rec-ognizing when there is no correct answer in the documentcollection (?NIL Accuracy?).
Precision of recognizingno answer is the ratio of the number of times NIL was re-turned and correct to the number of times it was returned;recall is the ratio of the number of times NIL was returnedand correct to the number of times it was correct (46).QA systems have become increasingly complex overthe four years of the TREC track such that there is now lit-tle in common across all systems.
Generally a system willclassify an incoming question according to an ontology ofquestion types (which varies from small sets of broad cat-egories to highly-detailed hierarchical schemes) and thenperform type-specific processing.
Many TREC 2002 sys-tems used specific data sources such as name lists andgazetteers, which were searched when the system deter-mined the question to be of an appropriate type.
The webwas used as a data source by most systems, though it wasused in different ways.
For some systems the web was theprimary source of an answer that the system then mappedto a document in the corpus to return as a response.
Other% NIL AccuracyRun Tag Score Correct Prec RecallLCCmain2002exactanswerpris2002IRST02D1 IBMPQSQACYCuwmtB3 BBN2002C isi02 fflimsiQalir2 	 ali2002b 	Table 1: Evaluation scores for a subset of the TREC 2002QA track runs.systems did the reverse: used the corpus as the primarysource of answers and then verified candidate answers onthe web.
Still other systems used the web as one of sev-eral sources whose combined evidence selected the finalresponse.The results in Table 1 illustrate that the confidence-weighted score does indeed emphasize a system?s abil-ity to rank correctly answered questions before incor-rectly answered questions.
For example, the exactan-swer run has a greater confidence-weighted score than thepris2002 run despite answering 19 fewer questions cor-rectly (54.2 % answered correctly vs. 58.0 % answeredcorrectly).
The systems used a variety of approaches tocreating their question rankings.
Almost all systems usedquestion type as a factor since some question types areeasier to answer than others.
Some systems use a score torank candidate answers for a question.
When that score iscomparable across questions, it can also be used to rankquestions.
A few groups used a training set of previousyears?
questions and answers to learn a good feature setand corresponding weights to predict confidence.
Manysystems used NIL as an indicator that the system couldn?tfind an answer (rather than the system was sure there wasno answer), so ranked NIL responses last.
With the ex-ception of the top-scoring LCCmain2002 run, though, theNIL accuracy scores are low, indicating that systems hadtrouble recognizing when there was no answer in the doc-ument collection.2 Judging ResponsesThe TREC QA track is a comparative evaluation.
In acomparative evaluation, each of two methods is used tosolve a common sample set of problems, and the meth-ods?
output is scored using some evaluation metric.
Themethod whose output produces a better evaluation scoreis assumed to be the more effective method.
An importantfeature of a comparative evaluation is that only relativescores are required.
In other words, the only requirementof the evaluation methodology for a comparative evalua-tion is that it reliably rank better methods ahead of worsemethods.The remainder of this paper examines the question ofwhether the QA task defined above reliably ranks sys-tems.
The first aspect of the investigation examineswhether human assessors can recognize exact answers.The evidence suggests that they can, though the differ-ences of opinion as to correctness observed in earlier QAtracks remain.
The second part of the investigation looksat the effect the differences of opinion have on rankings ofsystems given that there is only response per question andthe evaluation metric emphasizes the systems?
ranking ofquestions by confidence.
The final aspect of the investi-gation addresses the sensitivity of the evaluation.
Whileevaluation scores can be computed to an arbitrary numberof decimal places, not all differences are meaningful.
Thesensitivity analysis empirically determines the minimumdifference in scores required to have a small probabilityof error in concluding that one system is better than theother.While the idea of an exact answer is intuitively obvi-ous, it is very difficult to formally define.
As with correct-ness, exactness is essentially a personal opinion.
Thuswhether or not an answer is exact is ultimately up to theassessor.
NIST did provide guidelines to the assessorsregarding exactness.
The guidelines stated that exact an-swers need not be the most minimal response possible.For example, ?Mississippi river?
should be accepted asexact for the Big Muddy question despite the fact that?river?
is redundant since all correct responses must be ariver.
The guidelines also suggested that ungrammaticalresponses are generally not exact; a location question canhave ?in Mississippi?
as an exact answer, but not ?Mis-sissippi in?.
The guidelines also emphasized that even?quality?
responses?strings that contained both a cor-rect answer and justification for that answer?were to beCounts CountsJudged # % Judged # %WWR 174    WXX 86  WWU 151   RRU 141WWX 141 	  RRX 418  WRR 167    RUU 87  WRU 32 	 RUX 36  WRX 93    RXX 201WUU 81   UUX 23 WUX 34  UXX 21Table 2: Distribution of disagreements in assessor judg-ments.considered inexact for the purposes of this evaluation.To test whether assessors consistently recognize ex-act answers, each question was independently judged bythree different assessors.
Of the 15,948 [document-id,answer-string] response pairs across all 500 questions,1886 pairs (11.8 %) had some disagreement among thethree assessors as to which of the four judgments shouldbe assigned to the pair.
Note, however, that there wereonly 3725 pairs that had at least one judge assign a judg-ment that was something other than ?wrong?.
Thus, therewas some disagreement among the judges for half of allresponses that were not obviously wrong.Table 2 shows the distribution of the assessors?
dis-agreements.
Each response pair is associated with a tripleof judgments according to the three judgments assignedby the different assessors.
In the table the judgments aredenoted by W for wrong, R for right, U for unsupported,and X for inexact.
The table shows the number of pairsthat are associated with each triple, plus the percentageof the total number of disagreements that that triple rep-resents.The largest number of disagreements involves rightand inexact judgments: the RRX and RXX combinationsaccount for a third of the total disagreements.
Fortunatelyinspection of these disagreements reveals that they do notin general represent a new category of disagreement.
In-stead, many of the granularity differences observed inearlier QA judgment sets (Voorhees and Tice, 2000) arenow reflected in this distinction.
For example, a correctresponse for Who is Tom Cruise married to?
is NicoleKidman.
Some assessors accepted just ?Kidman?, butothers marked ?Kidman?
as inexact.
Some assessors alsoaccepted ?actress Nicole Kidman?, which some rejectedas inexact.
Similar issues arose with dates and placenames.
For dates and quantities, there was disagreementwhether slightly off responses are wrong or inexact.
Forexample, when the correct response is April 20, 1999, isApril 19, 1999 wrong or inexact?
This last distinctiondoesn?t matter very much in practice since in either casethe response is not right.Set 1 Set 2 Set 3Adjudicated           Set 1       Set 2   a) Correlations for confidence-weighted scoringSet 1 Set 2 Set 3Adjudicated           Set 1      Set 2    b) Correlations for raw count of number correctTable 3: Kendall  correlations for system rankings basedon different judgment sets and different measures.3 Stability of Comparative ResultsThe TREC-8 track demonstrated that QA evaluation re-sults based on text snippets and mean reciprocal rankscoring is stable despite differences in assessor opin-ions (Voorhees and Tice, 2000).
Given that the exact an-swer judgments reflect these same differences of opinion,are confidence-weighted scores computed over only oneresponse per question also stable?
We repeat the test forstability used in TREC-8 to answer this question.The three assessors who judged a question were arbi-trarily assigned as assessor 1, assessor 2, or assessor 3.The assessor 1 judgments for all questions were gatheredinto judgment set 1, the assessor 2 judgments into judg-ment set 2, and the assessor 3 judgments into judgmentset 3.
These three judgment sets were combined throughadjudication into a final judgment set, which is the judg-ment set used to produce the official TREC 2002 scores.Each run was scored using each of the four judgmentsets.
For each judgment set, the runs were ranked in or-der from most effective to least effective using either theconfidence-weighted score or the raw number of correctlyanswered questions.
The distance between two rankingsof runs was computed using a correlation measure basedon Kendall?s  (Stuart, 1983).
Kendall?s  computes thedistance between two rankings as the minimum numberof pairwise adjacent swaps to turn one ranking into theother.
The distance is normalized by the number of itemsbeing ranked such that two identical rankings produce acorrelation of , the correlation between a ranking andits perfect inverse is  , and the expected correlation oftwo rankings chosen at random is.
Table 3 gives thecorrelations between all pairs of rankings for both evalu-ation metrics.The average  correlation with the adjudicated rankingfor the TREC-8 results was 0.956; for TREC 2001, wheretwo assessors judged each question, the average correla-tion was 0.967.
The correlations for the exact answer caseare somewhat smaller: the average correlation is 0.930for the confidence-weighted score and 0.945 for the rawcount of number correct.
Correlations are slightly higherfor the adjudicated judgment set, probably because theadjudicated set has a very small incidence of errors.
Thehigher correlation for the raw count measure likely re-flects the fact that the confidence-weighted score is muchmore sensitive to differences in judgments for questionsat small (close to one) ranks.Smaller correlations between system rankings indicatethat comparative results are less stable.
It is not surprisingthat an evaluation based on one response per question isless stable than an evaluation based on five responses perquestion?there is inherently less information included inthe evaluation.
At issue is whether the rankings are sta-ble enough to have confidence in the evaluation results.
Itwould be nice to have a critical value for  such that cor-relations greater than the critical value guarantee a qualityevaluation.
Unfortunately, no such value can exist since values depend on the set of runs being compared.
In prac-tice, we have considered correlations greater than 0.9 tobe acceptable (Voorhees, 2001), so both evaluating usingthe confidence-weighted score and evaluating using theraw count of number correct are sufficiently stable.The vast majority of ?swaps?
(pairs of run such thatone member of the pair evaluates as better under one eval-uation condition while the other evaluates as better underthe alternate condition) that occur when using differenthuman assessors involve systems whose scores are verysimilar.
There is a total of 177 swaps that occur when thethree one-judge rankings are compared with the adjudi-cated ranking when using the confidence-weighted score.Only 4 of the 177 swaps involve pairs of runs whose dif-ference in scores,, is at least 0.05 as computed using theadjudicated judgment set, and there are no swaps when is at least 0.07.
As will be shown in the next section, runswith scores that are this similar should be assumed to beequally effective, so some swapping is to be expected.4 Sensitivity AnalysisHuman judgments are not the only source of variabilitywhen evaluating QA systems.
As is true with documentretrieval systems, QA system effectiveness depends onthe questions that are asked, so the particular set of ques-tions included in a test set will affect evaluation results.Since the test set of questions is assumed to be a ran-dom sample of the universe of possible questions, thereis always some chance that a comparison of two systemsusing any given test set will lead to the wrong conclusion.The probability of an error can be made arbitrarily smallby using arbitrarily many questions, but there are practi-cal limits to the number of questions that can be includedin an evaluation.Following our work for document retrieval evalua-tion (Voorhees and Buckley, 2002), we can use the runssubmitted to the QA track to empirically determine therelationship between the number of questions in a test set,the observed difference in scores (  ), and the likelihoodthat a single comparison of two QA runs leads to the cor-rect conclusion.
Once established, the relationship can beused to derive the minimum difference in scores requiredfor a certain level of confidence in the results given thereare 500 questions in the test set.The core of the procedure is comparing the effective-ness of a pair runs on two disjoint question sets of equalsize to see if the two sets disagree as to which of the runsis better.
We define the error rate as the percentage ofcomparisons that result in a swap.
Since the QA trackused 500 questions, we can directly compute the errorrate for question set sizes up to 250 questions.
By fittingcurves to the values observed for question set sizes up to250, we can extrapolate the error rates to question sets upto 500 questions.When calculating the error rate, the difference betweentwo runs?
confidence-weighted scores is categorized intoone of 21 bins based on the size of the difference.
Thefirst bin contains runs with a difference of less than 0.01(including no difference at all).
The next bin containsruns whose difference is at least 0.01 but less than 0.02.The limits for the remaining bins increase by incrementsof 0.01, with the last bin containing all runs with a differ-ence of at least 0.2.The requirement that the question sets be disjoint en-sures that the comparisons are made on independent sam-ples of the space of questions.
That is, we assume a uni-verse of all possible closed-class questions, and an (un-known) probability distribution of the scores for each ofthe two runs.
We also assume that the set of questionsused in the TREC 2002 QA track is a random sample ofthe universe of questions.
A random selection from theTREC question set gives a random, paired selection fromeach of the runs?
confidence-weighted score distributions.We take one random sample as a base case, and a differ-ent random sample (the disjoint sets) as the test case tosee if the results agree.Each question set size from 1 to 250 is treated as a sep-arate experiment.
Within an experiment, we randomly se-lect two disjoint sets of questions of the required size.
Wecompute the confidence-weighted score over both ques-tion sets for all runs, then count the number of times wesee a swap for all pairs of runs using the bins to segre-gate the counts by size of the difference in scores.
Theentire procedure is repeated 10 times (i.e., we perform 10trials), with the counts of the number of swaps kept asrunning totals over all trials2.
The ratio of the number of2While the two question sets used within any one trial aredisjoint, and thus independent samples, the question sets acrosstrials are drawn from the same initial set of 500 questions andthus overlap.
Because the question sets among the differentswaps to the total number of cases that land in a bin is theerror rate for that bin.The error rates computed from this procedure are thenused to fit curves of the form    where   and  are parameters to be estimated and ffis the size of the question set.
A different curve is fit foreach different bin.
The input to the curve-fitting proce-dure used only question set sizes greater than 20 sincesmaller question set sizes are both uninteresting and verynoisy.
Curves could not be fit for the first bin (differencesless than .01), for the same reason, or for bins where dif-ferences were greater than 0.16.
Curves could not be fitfor large differences because too much of the curve is inthe long flat tail.The resulting extrapolated error rate curves are plot-ted in Figure 2.
In the figure, the question set size isplotted on the x-axis and the error rate is plotted on they-axis.
The error rate for 500 questions when a differ-ence of 0.05 in confidence-weighted scores is observed isapproximately 8 %.
That is, if we know nothing abouttwo systems except their scores which differ by 0.05, andif we repeat the experiment on 100 different sets of 500questions, then on average we can expect 8 out of those100 sets to favor one system while the remaining 92 tofavor the other.The horizontal line in the graph in Figure 2 is drawn atan error rate of 5 %, a level of confidence commonly usedin experimental designs.
For question set sizes of 500questions, there needs to be an absolute difference of atleast 0.07 in confidence-weighted scores before the errorrate is less than 5 %.
Using the 5 % error rate standard,the pris2002, IRST02D1, and IBMPQSQACYC runs fromTable 1 should be considered equivalently effective, asshould the uwmtB3, BBN2002C, isi02, limsiQalir2, andali2002b runs.5 ConclusionEvaluating natural language processing technology iscritical to advancing the state of the art, but also con-sumes significant resources.
It is therefore important tovalidate new evaluation tasks and to establish the bound-aries of what can legitimately be concluded from the eval-uation.
This paper presented an assessment of the task inthe TREC 2002 QA track.While the task in earlier QA tracks had already beenvalidated, changes to the 2002 task were significantenough to warrant further examination.
In particular, the2002 task required systems to return exact answers, to re-turn one response per question, and to rank questions bytrials overlap, there may be correlations among the trials thatcould bias the estimates of the error rates as compared to whatwould be obtained with an equal number of samples drawn froma much larger initial set of questions.0 100 200 300 400 500Question set size0.00.20.4Error rates(%swaps)0.01 <= diff < 0.020.02 <= diff < 0.030.03 <= diff < 0.040.04 <= diff < 0.050.05 <= diff < 0.060.06 <= diff < 0.070.07 <= diff < 0.080.08 <= diff < 0.090.09 <= diff < 0.100.10 <= diff < 0.110.11 <= diff < 0.120.12 <= diff < 0.130.13 <= diff < 0.140.14 <= diff < 0.150.15 <= diff < 0.16Figure 2: Error rates extrapolated to test sets of 500 questions.confidence in the response; the evaluation metric empha-sized the ranking.
Each of these changes could increasethe variability in the evaluation as compared to the earliertask.
Examination of the track results did show some in-crease in variability, but also confirmed that system com-parisons are sufficiently stable for an effective evaluation.Human assessors do not always agree as to whether an an-swer is exact, but the differences reflect the well-knowndifferences in opinion as to correctness rather than inher-ent difficulty in recognizing whether an answer is exact.The confidence-weighted score is sensitive to changes injudgments for questions that are ranked highly, and there-fore is a less stable measure than a raw count of num-ber correct.
Nonetheless, all of the observed inversionsin confidence-weighted scores when systems were evalu-ated using different judgment sets were between systemswhose scores differed by less than 0.07, the smallest dif-ference for which the error rate of concluding two runsare different is less than 5 % for test sets of 500 ques-tions.A major part of the cost an evaluation is building thenecessary evaluation infrastructure such as training mate-rials, scoring procedures, and judgment sets.
The net costof an evaluation is greatly reduced if such infrastructureis reusable since the initial costs are amortized over manyadditional users.
Reusable infrastructure also acceleratesthe pace of technological advancement since it allows re-searchers to run their own experiments and receive rapidfeedback as to the quality of alternative methods.
Un-fortunately, neither the initial task within the TREC QAtrack nor the TREC 2002 task produces a reusable QAtest collection.
That is, it is not currently possible touse the judgment set produced during TREC to accu-rately evaluate a QA run that uses the same documentand question sets as the TREC runs but was not judged bythe human assessors.
Methods for approximating evalua-tion scores exist (Breck et al, 2000; Voorhees and Tice,2000), but they are not completely reliable.
A key areafor future work is to devise a truly reusable QA evalua-tion infrastructure.AcknowledgementsMy thanks to Chris Buckley who used the FUDGIT pack-age of gnuplot (Lacasse, 1993) to fit the error rate curves.ReferencesEric Breck, John Burger, Lisa Ferro, Lynette Hirschman,David House, Marc Light, and Inderjeet Mani.
2000.How to evaluate your question answering system ev-ery day .
.
.
and still get real work done.
In Proceed-ings of the Second International Conference on Lan-guage Resources and Evaluation (LREC-2000), vol-ume 3, pages 1495?1500.Julian Kupiec.
1993.
MURAX: A robust linguistic ap-proach for question answering using an on-line ency-clopedia.
In Proceedings of the Sixteenth Annual In-ternational ACM SIGIR Conference on Research andDevelopment in Information Retrieval, pages 181?190.Martin-Daniel Lacasse.
1993.
FUDGIT A multi-purposedata-processing and fitting program user?s manual ver-sion 2.31.
Technical report, Center for the Physicsof Materials and Department of Physics, Montreal,Canada, April.Karen Sparck Jones.
2001.
Automatic language and in-formation processing: Rethinking evaluation.
NaturalLanguage Engineering, 7(1):29?46.Alan Stuart.
1983.
Kendall?s tau.
In Samuel Kotz andNorman L. Johnson, editors, Encyclopedia of Statisti-cal Sciences, volume 4, pages 367?369.
John Wiley &Sons.Ellen M. Voorhees and Chris Buckley.
2002.
The effectof topic set size on retrieval experiment error.
In Pro-ceedings of the 25th Annual International ACM SIGIRConference on Research and Development in Informa-tion Retrieval, pages 316?323.Ellen M. Voorhees and Dawn M. Tice.
2000.
Buildinga question answering test collection.
In Proceedingsof the Twenty-Third Annual International ACM SIGIRConference on Research and Development in Informa-tion Retrieval, pages 200?207, July.Ellen M. Voorhees.
2001.
Evaluation by highly relevantdocuments.
In Proceedings of the 24th Annual Inter-national ACM SIGIR Conference on Research and De-velopment in Information Retrieval, pages 74?82.
