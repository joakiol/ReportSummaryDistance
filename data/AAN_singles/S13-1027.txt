Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conferenceand the Shared Task, pages 187?193, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational LinguisticsIBM_EG-CORE: Comparing multiple Lexical and NE matchingfeatures in measuring Semantic Textual similaritySara NoemanIBM Cairo Technology and Development CenterGiza, EgyptP.O.
Box 166 Al-Ahramnoemans@eg.ibm.comAbstractWe  present  in  this  paper  the  systems  weparticipated  with  in  the  Semantic  TextualSimilarity  task  at  SEM  2013.
The  SemanticTextual Similarity Core task  (STS)  computes thedegree  of  semantic  equivalence  between  twosentences  where  the  participant  systems  will  becompared to the manual scores, which range from5  (semantic  equivalence)  to  0  (no  relation).
Wecombined  multiple  text  similarity  measures  ofvarying complexity.
The experiments illustrate thedifferent  effect  of  four  feature  types  includingdirect  lexical  matching,  idf-weighted  lexicalmatching,  modified BLEU N-gram matching andnamed entities matching.
Our team submitted threeruns  during  the  task  evaluation  period  and  theyranked  number  11,  15  and  19  among  the  90participating  systems  according  to  the  officialMean Pearson correlation metric for the task.
Wealso  report  an  unofficial  run  with  mean  Pearsoncorrelation  of  0.59221  on  STS2013  test  dataset,ranking  as  the  3rd  best  system  among  the  90participating systems.1 IntroductionThe  Semantic  Textual  Similarity  (STS)  task  atSEM 2013 is  to measure  the degree of semanticequivalence between pairs of sentences as a gradednotion  of  similarity.
Text  Similarity  is  veryimportant  to  many  Natural  Language  Processingapplications, like extractive summarization (Saltonet al 1997), methods for automatic evaluation ofmachine translation (Papineni et al 2002), as wellas  text  summarization  (Lin  and  Hovy,  2003).
InText  Coherence  Detection  (Lapata  and  Barzilay,2005), sentences are linked together by similar orrelated  words.
For  Word  Sense  Disambiguation,researchers  (Banerjee  and  Pedersen,  2003;  Guoand  Diab,  2012a)  introduced  a  sense  similaritymeasure using the sentence similarity of the sensedefinitions.
In this paper we illustrate the differenteffect of four feature types including direct lexicalmatching, idf-weighted lexical matching, modifiedBLEU  N-gram  matching  and  named  entitiesmatching.
The rest  of  this  paper  will  proceed asfollows, Section 2 describes the four text similarityfeatures  used.
Section  3  illustrates  the  systemdescription,  data  resources  as  well  as  Featurecombination.
Experiments  and  Results  areillustrated  in  section  4.  then  we  report  ourconclusion and future work.2 Text Similarity FeaturesOur  system  measures  the  semantic  textualsimilarity between two sentences through a numberof matching features which should cover four maindimensions: i) Lexical Matching ii)  IDF-weightedLexical  Matching  iii)  Contextual  sequenceMatching (Modified BLEU Score), and iv) NamedEntities Matching.First we introduce the alignment technique used.For a sentence pair {s1, s2} matching is done ineach direction separately to detect the sub-sentenceof  s1  matched  to  s2  and  then  detect  the  sub-sentence of s2 matched to s1.
For each word wi ins1 we search for its match  wj in s2 according tomatching features.S1: w0 w1 w2 w3 w4 ?...
wi ?...
wnS2: w0 w1 w2 w3 w4 ?.......wj ?.........
wm1872.1 Lexical Matching:In this feature we handle the two sentences as bagsof  words  to  be  matched  using  three  types  ofmatching, given that all stop words are cleaned outbefore matching:I) Exact word matching.II) Stemmed word matching: I used PorterStemming algorithm (M.F.
Porter, 1980) inmatching, where  it is a process for removingthe commoner morphological and inflectionalendings from words in English.
Stemmingwill render inflections like ?requires, required,requirements, ...?
to ?requir?
so they can beeasily matchedIII) Synonyms matching: we used a corpus baseddictionary of 58,921 entries and theirequivalent synonyms.
The next sectiondescribes how we automatically generated thislanguage resource.2.2 IDF-weighted Lexical MatchingWe used the three matching criteria used inLexical Matching after weighting them withInverse-Document-Frequency.
we applied theaggregation strategy by Mihalcea et al(2006): Thesum of the idf-weighted similarity scores of eachword with the best-matching counterpart in theother text is computed in both directions.
For asentence pair s1, s2, if s1 consists of m words {w0,w1, ?., w(m-1)} and s2 consists of n words {w0,w1, ?., w(n-1)} ,after cleaning stop words fromboth, and the matched words are?@Matched_word_List?
of ?k?
words, then2.3 Contextual Sequence Matching (ModifiedBLEU score)We used a modified version of Bleu score tomeasure n-gram sequences matching, where forsentence pair s1, s2 we align the matched wordsbetween them (through exact, stem, synonymsmatch respectively).
Bleu score as presented by (K.Papineni et al 2002) is an automated method forevaluating Machine Translation.
It compares n-grams of the candidate translation with the n-gramsof the reference human translation and counts thenumber of matches.
These matches are positionindependent, where candidate translations withunmatched length to reference translations arepenalized with Sentence brevity penalty.This helps in measuring n-gram similarity insentences structure.
We define ?matchedsequence?
of a sentence S1 as the sequence ofwords {wi, wi+1, wi+2, ?..
wj}, where wi, and wjare the first and last words in sentence S1 that arematched with words in S2.For example in sentence pair S1, S2:S1: Today's great Pax Europa and today's pan-European prosperity depend on this.S2: Large Pax Europa of today, just like currentprosperity paneurop?enne, depends on it.After stemming:S1: todai's great pax europa and todai's pan-european prosper depend on thi.S2: larg pax europa of todai, just like currentprosper paneurop?enn, depend on it.
?Matched sequence of S1?
:[todai 's great pax europa todai 's pan - europeanprosper depend]?Matched sequence of S2?
:[pax europa todai just like current prosperpaneurop?enn depend]We measure the Bleu score such that:Bleu{S1, S2} = &BLEU(S1_stemmed,"Matchedsequence of S2");Bleu{S2, S1} = &BLEU(S2_stemmed,"Matchedsequence of S1");The objective of trimming the excess wordsoutside the ?Matched Sequence?
range, beforematching is to make use of the  Sentence brevitypenalty in case sentence pair S1, S2 may be notsimilar but having matched lengths.1882.4 Named Entities MatchingNamed entities carry an important portion ofsentence semantics.
For example:Sentence1: In Nigeria , Chevron has been accusedby the All - Ijaw indigenous people of instigatingviolence against them and actually payingNigerian soldiers to shoot protesters at the Warrinaval base .Sentence2: In Nigeria , the whole ijaw indigenousshowed Chevron to encourage the violenceagainst them and of up to pay Nigerian soldiers toshoot the demonstrators at the naval base fromWarri .The underlined words are Named entities ofdifferent types ?COUNTRY, ORG, PEOPLE,LOC, EVENT_VIOLENCE?
which capturethe most important information in eachsentence.
Thus named entities matching is ameasure of semantic matching between thesentence pair.3 System Description3.1 Data Resources and ProcessingAll  data  is  tokenized,  stemmed,  and  stopwords are cleaned.Corpus based resources:i. Inverse Document Frequency (IDF)language resource: The  document frequencydf(t) of a term t is defined as the number ofdocuments in a large collection of documentsthat contain a term ?t?.
Terms that are likelyto appear in most of the corpus documentsreflect less importance than words that appearin specific documents only.
That's why theInverse Document Frequency is used as ameasure of term importance in informationretrieval and text mining tasks.
We used theLDC English Gigaword Fifth Edition(LDC2011T07) to generate our idf dictionary.LDC Gigaword contains a huge collection ofnewswire from (afp, apw, cna, ltw, nyt, wpb,and xin).
The generated idf resource contains5,043,905 unique lower cased entries, andthen we generated a stemmed version of theidf dictionary contains 4,677,125 entries.
Theequation below represents the idf of term twhere N is the total number of documents inthe  corpus.ii.
English  Synonyms  Dictionary:  Using  thePhrase  table  of  an  Arabic-to-English  DirectTranslation Model,  we generated English-to-English phrase table using the double-link ofEnglish-to-Arabic  and  Arabic-to-Englishphrase translation probabilities over all pivotArabic  phrases.
Then  English-to-Englishtranslation  probabilities  are  normalized  overall  generated  English  synonyms.
(ChrisCallison-Burch  et  al,  2006) used  a  similartechnique to generate paraphrases to improvetheir SMT system.
Figure (1) shows the steps:Figure(1) English phrase-to-phrase synonymsgeneration from E2A phrase table.In our system we used the phrase table of theDirect Translation Model 2 (DTM2) (Ittycheriahand Roukos, 2007) SMT system, where eachsentence pair in the training corpus was word-aligned, e.g.
using a MaxEnt aligner (Ittycheriahand Roukos, 2005) or an HMM aligner (Ge, 2004).then Block Extraction step is done.
The generatedphrase table contains candidate phrase to phrasetranslation pairs with source-to-target and target-tosource translation probabilities.
However the opensource Moses SMT system (Koehn et al 2007)For each English Phrase ?e1?
{@ar_phrases = list of Arabic Phrases aligned to ?e?in the phrase table;For each a (@ar_phrases){@en_phrases = list of English phrases alignedto ?a?
in the phrase table;For each e2 (@en_phrases){$Prob(e2\e1) = Prob(a\e1)*Prob(e2\a);}}}189can be used in the same way to generate asynonyms dictionary from phrase table.By applying the steps in figure (1):a) English phrase-to-phrase synonyms table (orEnglish-to-English phrase table), by applying thesteps in a generic way.b) English word-to-word synonyms table, bylimiting the generation over English single wordphrases.For example, to get alpossible synonyms of theEnglish word ?bike?, we used all the Arabicphrases that are aligned to ?bike?
in the phrasetable { ???????
?, ??????
?, ????????
, ?????
},P: 1905645 14 0.0142582 0.170507 |  ?????
| bike |P: 1910841 25 0.0262152 0.221198 |  ????????
| bike |P: 2127826 4 0.0818182 0.0414747 |  ????????
| bike |P: 2396796 2 0.375 0.0138249 |  ???????
| bike |then we get althe English words in the phrasetable aligned to these Arabic translations { ?????,???????
?, ??????
?, ????????
}This results in an English word-to-word synonymslist for the word ?bike?
like this:bike:motorcycle      0.365253185010659bicycle 0.198195663512781cycling 0.143290354808692motorcycles     0.0871686646772204bicycles        0.0480779974950311cyclists        0.0317670845504069motorcyclists   0.0304152910853553cyclist 0.0278451740161998riding  0.0215366691148431motorbikes      0.0148697281155676Dictionary based resources:?
WordNet (Miller, 1995): is a large lexicaldatabase of English.
Nouns, verbs, adjectivesand adverbs are grouped into sets of cognitivesynonyms (synsets), each expressing a distinctconcept.
Synsets are interlinked by means ofconceptual-semantic and lexical relations.WordNet groups words together based ontheir meanings and interlinks not just wordforms?strings of letters?but specific sensesof words.
As a result, words that are found inclose proximity to one another in the networkare semantically disambiguated.
Second,WordNet labels the semantic relations amongwords.
Using WordNet, we can measure thesemantic similarity or relatedness between apair of concepts (or word senses), and byextension, between a pair of sentences.
Weuse the similarity measure described in (Wuand Palmer, 1994) which finds the path lengthto the root node from the least commonsubsumer (LCS) of the two word senses whichis the most specific word sense they share asan ancestor.3.2 Feature CombinationThe feature combination step uses the pre-computed  similarity  scores.
Each  of  thetext  similarity  features  can  be  given  aweight  that  sets  its  importance.Mathematically,  the  text  similarity  scorebetween two sentences can be formulatedusing  a  cost  function  weighting  thesimilarity  features  as  follows:  N.B.
:  Thesimilarity score according to the featuresabove is considered as a directional score.Similarity(s1, s2) = [w1*Lexical_Score(s1, s2) +w2*IDF_Lexical_Score(s1, s2) +w3*Modified_BLEU(s1, s2) +w4*NE_Score(s1, s2)] / (w1+w2+w3+w4)Similarity(s2, s1) = [w1*Lexical_Score(s2, s1) +w2*IDF_Lexical_Score(s2, s1) +w3*Modified_BLEU(s2, s1) +w4*NE_Score(s2, s1)] / (w1+w2+w3+w4)Overall_Score = 5/2*[Similarity(s1, s2)+Similarity(s2, s1)]where w1, w2, w3, w4 are the weights assigned tothe similarity features (lexical, idf-weighted,modified_BLEU, and NE_Match featuresrespectively).
The similarity score will benormalized over (w1+w2+w3+w4).In our experiments, the weights are tuned manuallywithout applying machine learning techniques.
Weused both *SEM 2012 training and testing data setsfor tuning these weights to get the best featureweighting combination to get highest PearsonCorrelation score.4 Experiments and ResultsSubmitted RunsOur experiments showed that some features aremore dominant in affecting the similarity scoringthan others.
We performed a separate experimentfor each of the four feature types to illustrate theireffect on textual semantic similarity measurement190using direct lexical matching, stemming matching,synonyms matching,  as well as (stem+synonyms)matching.
Table (1) reports the mean Pearsoncorrelation results of these experiments onSTS2012-test datasetDirect StemonlySynonymsonlySynonyms +StemNE  0.303 0.297 0.306 0.304BLEU 0.439 0.446 0.469 0.453Lexical 0.59 0.622 0.611 0.624IDF 0.488 0.632 0.504 0.634Table (1) reports the mean Pearson score for NE,BLEU, Lexical, and idf-weighted matching featuresrespectively on STS2012-test dataset.The submitted runs IBM_EG-run2, IBM_EG-run5,IBM_EG-run6 are the three runs with featureweighting and experiment set up that performedbest on STS 2012 training and testing data sets.Run 2: In this run the word matching was done onexact, and synonyms match only.
Stemmed wordmatching was not introduced in this experiment.we tried the following weighting  betweensimilarity feature scores, where we decreased theweight of BLEU scoring feature to  0.5, andincreased the idf_Lexical match weight of 3.5. thisis because our initial tuning experiments showedthat increasing the idf lexical weight compared toBLEU weight gives improved results.
The NEmatching feature weight was as follows:NE_weight = 1.5* percent of NE word to sentence word count= 1.5* (NE_words_count/Sentence_word_count)Run 5: In this experiment we introduced Porterstemming word matching, as well as stemmedsynonyms matching (after generating a stemmedversion of the synonyms dictionary).
BLEU scorefeature was removed from this experiment, whilekeeping the idf-weight= 3, lexical-weight = 1, andNE-matching feature weight = 1.Run 6: For this run we kept only IDF-weightedlexical matching feature which proved to be thedominant feature in the previous runs, in additionto Porter stemming word matching, and stemmedsynonyms matching.Data:  the training data of STS 2013 Core taskconsist of the STS 2012 train and test data.
Thisdata covers 5 datasets: paraphrase sentence pairs(MSRpar), sentence pairs from video descriptions(MSRvid), MT evaluation sentence pairs(SMTnews and SMTeuroparl) and gloss pairs(OnWN).Results on Training DataSystem outputs will be evaluated according to theofficial scorer  which computes weighted MeanPearson Correlation across the evaluation datasets,where the weight depends on the number of pairsin each dataset.Table (2), reports the results achieved on each ofthe STS 2012 training dataset.
While table (3),reports the results achieved on STS 2012 testdataset.IBM_run2 IBM_run5 IBM_run6Mean 0.59802 0.64170 0.68395MSRpar 0.61607 0.63870 0.62629MSRvid 0.70356 0.80879 0.83722SMTeuroparl 0.47173 0.47403 0.58627Table (2) Results on STS 2012 training datasets.IBM_run2 IBM_run5 IBM_run6Mean 0.59408 0.62614 0.63365MSRpar 0.56059 0.59108 0.61306MSRvid 0.73189 0.79960 0.87154SMTeuroparl 0.51480 0.50563 0.41298OnWN 0.62927 0.65760 0.67136SMTnews 0.42305 0.44551 0.40819Table (3) Results on STS 2012 test datasets.Results on Test Data:The  best  configuration  of  our  system  wasIBM_EG-run6 which  was  ranked  #11  for  theevaluation metric Mean  (r  =  0.5502)  whensubmitted during the task evaluation period .
Run6as illustrated before was planned to measure idf-weighted lexical matching feature only, over Porterstemmed,  and  stemmed  synonyms  words.However when  revising  this  experiment  set  up191during  preparing  the  paper,  after  the  evaluationperiod,  we  found  that  the  English-to-Englishsynonyms  table  was  not  correctly  loaded  duringmatching,  thus  skipping  synonyms  matchingfeature  from  this  run.
So  the  official  resultIBM_EG-run6 reports  only  idf-weightedmatching over Porter stemmed bag of words.
Byfixing  this  and  replicating  the  experimentIBM_EG-run6-UnOfficial  as  planned to  be,  themean  Pearson  correlation  jumps  4  points  (r  =0.59221)  which  ranks  this  system  as  the  3rdsystem  among  90  submitted  systems  veryslightly  below  the  2nd system  (only  0.0006difference on the mean correlation metric).
Intable (4), we report the official results achieved onSTS 2013 test data.
While  table (5),  reports theunofficial  results  achieved  after   activating  thesynonyms  matching  feature  in  IBM_EG-run6(unofficial) and comparing this run to the best tworeported systems.IBM_EG-run2IBM_EG-run5IBM_EG-run6headlines 0.7217 0.7410 0.7447OnWN 0.6110 0.5987 0.6257FNWN 0.3364 0.4133 0.4381SMT 0.3460 0.3426 0.3275Mean 0.5365 0.5452 0.5502Rank #19 #15 #11Table (4) Official Results on STS 2013 test datasets.UMBC_EBIQUITY-ParingWordsUMBC_EBIQUITY-galactusIBM_EG-run6(UnOfficial)headlines 0.7642 0.7428 0.77241OnWN 0.7529 0.7053 0.70103FNWN 0.5818 0.5444 0.44356SMT 0.3804 0.3705 0.36807Mean 0.6181 0.5927 0.59221Rank #1 #2 #3Table (5) UnOfficial Result after activating thesynonyms matching feature in IBM_EG-run6compared to the best two performing systems in theevaluation.Results of un-official run:One  unofficial  run  was  performed  after  theevaluation  submission  deadline  due  to  the  tightschedule  of  the  evaluation.
This  experimentintroduces the effect of WordNet  Wu and Palmersimilarity  measure  on  the  configuration  of  Run5(Porter stemming word matching,  with  synonymsmatching, zero weight for   BLEU score feature,while keeping the idf-weight= 3, lexical-weight =1, and NE-matching feature weight = 1)Table (6) reports the unofficial result achieved onSTS 2013 test data, compared to the Official runIBM_Eg-run5.Unofficial-Run IBM_EG-run5Mean 0.52682 0.5452headlines 0.70018 0.7410OnWN 0.60371 0.5987FNWN 0.35691 0.4133SMT 0.33875 0.3426Table (6) Un-Official Result on STS 2013 test datasets.From the results in Table (6) it is clear that Corpusbased synonyms matching outperforms dictionary-based WordNet matching over SEM2013 testset.5 ConclusionWe  proposed  an  unsupervised  approach  formeasuring  semantic  textual  similarity  based  onLexical  matching  features  (with porter  stemmingmatching  and  synonyms  matching),  idf-Lexicalmatching  features,  Ngram  Frquency  (ModifiedBLEU)  matching  feature,  as  well  as  NamedEntities matching feature combined together with aweighted cost  function.
Our experiments  provedthat idf-weighted Lexical matching in addition toporter stemming and synonyms-matching featuresperform best on most released evaluation datasets.Our  best  system  officially  ranked  number  11among 90 participating system reporting a PearsonMean  correlation  score  of  0.5502.
However  ourbest  experimental  set  up  ?idf-weighted  Lexicalmatching  in  addition  to  porter  stemming  andsynonyms-matching?
reported in an unofficial runa mean correlation score of  0.59221 which ranksthe system as number 3 among the 90 participatingsystems.
In our future work we intend to try somemachine  learning  algorithms  (like  AdaBoost  for192example)  for  weighting  our  similarity  matchingfeature scores.
Also we plan to extend the usage ofsynonyms matching from the word level to the n-gram  phrase  matching  level,  by  modifying  theBLEU Score N-gram matching function to handlesynonym phrases matching.AcknowledgmentsWe would  like  to  thank  the  reviewers  for  theirconstructive criticism and helpful comments.ReferencesAlfred.
V.  Aho  and  Jeffrey  D.  Ullman.
1972.
TheTheory  of  Parsing,  Translation  and  Compiling,volume 1.
Prentice-Hall, Englewood Cliffs, NJ.American  Psychological  Association.
1983.Publications  Manual.
American  PsychologicalAssociation, Washington, DC.Association  for  Computing  Machinery.
1983.Computing Reviews, 24(11):503-512.Ashok  K.  Chandra,  Dexter  C.  Kozen,  and  LarryJ.Stockmeyer.
1981.
Alternation.
Journal  of  theAssociation  for  Computing  Machinery,  28(1):114-133.C.
Y.  Lin  and  E.  H.  Hovy.
2003.
Automaticevaluation  of  summaries  using  n-gram  co-occurrence  statistics.
In  Proceedings  of  HumanLanguage Technology Conference (HLT-NAACL2003), Edmonton, Canada, May.Chris  Callison-Burch,  Philipp  Koehn,  and  MilesOsborne.
2006.
Improved  statistical  machinetranslation  using  paraphrases.
In  Proceedings  ofHLT-NAACL.Dan Gusfield.
1997.
Algorithms on Strings, Trees andSequences.
Cambridge University Press, Cambridge,UK.G.
Salton  and  C.  Buckley.
1997.
Term  weightingapproaches in automatic text retrieval.
In  Readingsin  Information  Retrieval.
Morgan  KaufmannPublishers, San Francisco, CA.Ittycheriah,  A.  and  Roukos,  S.  (2007).
Directtranslation  model  2.
In  Human  LanguageTechnologies  2007:  The  Conference  of  the  NorthAmerican  Chapter  of  the  Association  forComputational Linguistics; Proceedings of the MainConference, pp.57?64, Rochester, NY.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2002.Bleu: a method for automatic evaluation of machinetranslation.
In  Proceedings  of  the  40th  AnnualMeeting  of  the  Association  for  ComputationalLinguistics, Cambridge, UK.M.
Lapata  and  R.  Barzilay.
2005.
Automaticevaluation  of  text  coherence:  Models  andrepresentations.
In  Proceedings  of  the  19thInternational  Joint  Conference  on  ArtificialIntelligence, Edinburgh.P.
Koehn,  F.J.  Och,  and  D.  Marcu.
2003.
StatisticalPhrase-Based  Translation.
Proc.
Of  the  HumanLanguage  Technology  Conference,  HLTNAACL?2003, May.Philipp  Koehn,  Hieu  Hoang,  Alexandra  Birch,  ChrisCallison-Burch,  Marcello Federico,  Nicola Bertoldi,Brooke  Cowan,  Wade  Shen,  Christine  Moran,Richard Zens, Chris Dyer, Ond?rej Bojar, AlexandraConstantin,  and  Evan  Herbst.
2007.
Moses:  OpenSource Toolkit for Statistical Machine Translation.
InProceedings  of  the  ACL  2007  Demo  and  PosterSessions, pages 177?180.R.
Mihalcea  ,  C.  Corley,  and  C.  Strapparava 2006.Corpus-based and knowledge-based measures of textsemantic similarity.
In  Proceedings of the AmericanAssociation for Artificial Intelligence.
(Boston, MA).Satanjeev Banerjee and Ted Pedersen.
2003.
Extendedgloss overlaps as a measure of semantic relatedness.In  Proceedings  of  the  18th  International  JointConference on Artificial Intelligence, pages 805?810.Ted  Pedersen,  Siddharth  Patwardhan,  and  JasonMichelizzi,  2004,  WordNet::Similarity  -  Measuringthe  Relatedness  of  Concepts.
Proceedings  of  FifthAnnual Meeting of the North American Chapter  ofthe  Association  for  Computational  Linguistics(NAACL-2004).Wu,  Z.,  and  Palmer,  M.  1994.
Verb  semantics  andlexical  selection.
In  32nd  Annual  Meeting  of  theAssociation for Computational Linguistics, 133?138.Weiwei  Guo  and  Mona  Diab.
2012a.
Learning  thelatent semantics of a concept from its definition.
InProceedings  of  the  50th  Annual  Meeting  of  theAssociation for Computational Linguistics.193
