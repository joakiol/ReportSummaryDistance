Acquiring Predicate-Argument MappingInformation from Multilingual TextsChinatsu Aone, Douglas McKeeSystems Research and Applications (SRA)2000 15th Street NorthArlington, VA 22201aonec@sra, corn, mckeed@sra.comAbst ractThis paper discusses automatic acquisition of predicate-argument mapping in-formation from multilingual texts.
The lexicon of our NLP system abstracts thelanguage-dependent portion of predicate-argument mapping information from thecore meaning of verb senses (i.e.
semantic oncepts as defined in the knowledge base).We represent this mapping information in terms of cross-linguistically generalizedmapping types called situation types and word sense-specific idiosyncrasies.
Thisrepresentation has enabled us to automatically acquire predicate-argument map-ping information, specifically situation types and idiosyncrasies, for verbs in English,Spanish, and Japanese texts.1 IntroductionLexicons for a natural language processing (NLP) system that perform syntactic andsemantic analysis require more than purely syntactic (e.g.
part-of-speech information)and semantic information (e.g.
a concept hierarchy).
Language understanding requiresmapping from syntactic structures into conceptual representation (henceforth predicate-argument mapping), while language generation requires the inverse mapping.
That is,grammatical functions in the syntactic structures (e.g.
subject, object, etc.)
should bemapped to thematic roles in the semantic structures (e.g.
agent, theme, etc.
).In this paper, we discuss how we acquire such predicate-argument mapping informationfrom multilingual texts automatically (cf.
Zernik and Jaeobs work on collecting thematicroles \[20\]).
As discussed in Aone and Mckee \[1\], the lexicon of our NLP system abstractsthe language-dependent portion of predicate-argument mapping information from the coremeaning of verb senses (i.e.
semantic concepts as defined in the knowledge base).
Werepresent this mapping information in terms of cross-linguistically generalized mappingtypes called situation types and word sense-specific idiosyncrasies.
This representation hasenabled us to automatically acquire predicate-argument mapping information, specificallysituation types and idiosyncrasies, for verbs in English, Spanish, an.d Japanese texts.In the following sections, we first describe how we represent the predicate-mappinginformation.
Then, we discuss how we acquire situation type and idiosyncrasy informationautomatically from multilingual texts and show some results.2 Predicate-Argument Mapping RepresentationEach lexical sense of a verb in our lexicon encodes its default predicate-argument mappingtype (i.e.
situation type), any word-specific mapping exceptions (i.e.
idiosyncrasies), and107# of required NP or S argumentsCAUSED-PROCESS 2PROCESS-OR-STATE 1AGENTIVE-ACTION 1INVEKSE-STATE 2default thematic roles prohibited thematic rolesAsent ThemeTheme AgentAgentGoal Theme AgentTable 1: Definitions of Situation TypesEnglish Spanish JapaneseCAUSED-PROCESS kill matar,  mirar korosu, miruPROCESS-OH-STATE die morir shibousuruAGENTIVE-ACTION look bailar odoruINVERSE-STATE see vet  mieruTable 2: Situation Types and Verbs in Three Languagesits semantic meaning (i.e.
semantic oncept) in addition to its morphological nd syntacticinformation.
In the following, we discuss these three levels in detail.2.1 S i tuat ion  TypesEach of a verb's lexical senses is classified into one of the four default predicate-argumentmapping types called situation types.
As shown in Table 1, situation types of verbs aredefined by two kinds of information: 1) the number of subcategorized NP or S argumentsand 2) the types of thematic roles which these arguments should or should not map to.Since this kind of information is applicable to verbs of any language, situation types arelanguage-independent predicate-argument mapping types.
Thus, in any language, a verbof type CAUSED-PROCESS has two arguments which map to AGENT and THEME inthe default case (e.g.
"kill").
A verb of type PROCESS-OR-STATE has one argumentwhose thematic role is THEME, and it does not allow AGENT as one of its thematic roles(e.g.
"die").
An AGENTIVE-ACTION verb also has one argument but the argument mapsto AGENT (e.g.
"look").
Finally, an INVERSE-STATE verb has two arguments whichmap to THEME and GOAL; it does not allow AGENT for its thematic role (e.g.
"see").Examples from three languages are shown in Table 2.Although verbs in different languages are classified into the same four situation typesusing the same definition, mapping rules which map grammatical functions (i.e.
subject,object, etc.)
in the syntactic structures 1 to thematic roles in the semantic structures maydiffer from one language to another.
This is because languages do not necessarily expressthe same thematic roles with the same grammatical functions.
This mapping informationis language-specific ( f. Nirenburg and Levin \[16\]).The default mapping rules for the four situation types are shown in Table 3.
They arenearly identical for the three languages (English, Spanish, and Japanese) we have analyzedso far.
The only difference is that in Japanese the THEME of an INVERSE-STATE verbis expressed by marking the object NP with a particle "-ga", which is usually a subject1 We use s t ruc tures  imi lar  to LFG 's  f -structures.108CAUSED-PROCESS AGENTTHEMEPROCESS-OR-STATE THEMEAGENTIVE-ACTION AGENTINVERSE-STATE GOALTHEMEEnglish/Spanish Mapping Japanese Mapping(SURFACE SUBJECT) (SURFACE SUBJECT)(SURFACE OBJECT) (SURFACE OBJECT)~SURFACE SUBJECT) ~SURFACE SUBJECT)(SURFACE SUBJECT) ~SURFACE SUBJECT)(SURFACE SUBJECT) (SURFACE SUBJECT)(SURFACE OBJECT) .
(SURFACE OBJECT) (PARTICLE "GA")Table 3: Default Mapping Rules for Three Languagesmarker (cf.
Kuno \[12\]).
2 3 So we add such information to the INVERSE-STATE mappingrule for Japanese.
Generalization expressed in situation types has saved us from definingsemantic mapping rules for each verb sense in each language, and also made it possible toacquire them from large corpora automatically.This classification system has been partially derived from Vendler and Dowty's as-pectual classifications \[19, 9\] and Talmy's lexicalization patterns \[18\].
For example, allAGENTIVE-ACTION verbs are so-called activity verbs, and so-called stative verbs fallunder either INVERSE-STATE (if transitive) or PROCESS-OR-STATE (if intransitive).However, the situation types are not for specifying the semantics of aspect, which is ac-tually a property of the whole sentence rather than a verb itself (cf.
Krifka \[11\], Dorr \[8\],Mocns and Steedman \[15\]).
For instance, as shown below, the same verb can be classifiedinto two different aspectual classes (i.e.
activity and accomplishment) depending on thetypes of Object NP's or existence of certain PP's.
(1) a.
Sue drank wine for/*in an hour.b.
Sue drank a bottle of wine *for/in an hour.
(2) a. Harry climbed for/*in an hour.b.
Harry climbed to the top *for/in an hour.Situation types are intended to address the issue of cross-linguistic predicate-argumentmapping eneralization, rather than the semantics of aspect.2.2 Id iosyncras iesIdiosyncrasies slots in the lexicon specify word sense-specific diosyncratic phenomenawhich cannot be captured by semantic oncepts or situation types.
In particular, subcat-egorized pre/postpositions of verbs are specified here.
For example, the fact that "look"denotes its TItEME argument by the preposition "at" is captured by specifying idiosyn-crasies.
Examples of lexical entries with idiosyncrasies in English, Spanish and Japaneseare shown in Figure 1.
As discussed in the next section, we derive this kind of word-specificinformation automatically from corpora.2There is a debate over whether the NP with "ga" is a subject or object.
However, our approach canaccommodate either analysis.3The GOAL of some INVERSE-STATE verbs in Japanese can be expressed by a "ni" postpositionalphrase.
However, as Kuno \[12\] points out, since this is an idiosyncratic phenomenon, such informationdoes not go to the default mapping rule.109(LOOK (CATEGORY .
V)(SENSE-NAME.
LOOK-l)(SEMANTIC-CONCEPT #LOOK#)(IDIOSYNCRASIES (THEME (MAPPING (LITERAL "AT"))))(SITUATION-TYPE AGENTIVE-ACTION))(INFECTAR (CATEGORY .
V)(SENSE-NAME.
INFECTAFt- 1)(SEMANTIC-CONCEPT #INFECT#)(IDIOSYNCRASIES (THEME (MAPPING (LITERAL "CON" "DE")))(GOAL (MAPPING (SURFACE OBJECT))))(SITUATION-TYPE CAUSED-PROCESS))(NARU (CATEGORY .
V)(SENSE-NAME.
NARU- I)(SEMANTIC-CONCEPT #BECOME#)(IDIOSYNCRASIES (GOAL (MAPPING (LITERAL "TO" "NI"))))(SITUATION-TYPE PROCESS-OR-STATE))Figure 1: Lexical entries for "look", "infectar", and "naru"2 .3  Semant ic  ConceptsEach lexical meaning of a verb is represented by a semantic concept (or frame) in ourlanguage-independent knowledge base, which is similar to the one described in Onyshkevychand Nirenburg \[17\].
Each verb frame has thematic role slots, which have two facets,TYPE and MAPPING.
A TYPE facet value of a given slot provides a constraint on thetype of objects which can be the value of the slot.
In the MAPPING facets, we haveencoded some cross-linguistically general predicate-argument mapping information.
Forexample, we have defined that all the subclasses of #COMMUNICATION-EVENTS (e.g.#REPORT#,  #CONFIRM#, etc.)
map their sentential complements (SENT-COMP)to THEME, as shown below.
(#COMMUNICATION-EVENT#(AKO #DYNAMIC-SITUATION#)(AGENT (TYPE #PERSON# #ORGANIZATION#))(THEME (TYPE #SITUATION# #ENTITY#)(MAPPING (SENT-COMP T)))(GOAL (TYPE #PERSON# #ORGANIZATION#)(MAPPING (P-ARG GOAL))))2.4  Merg ing  Pred icate -Argument  Mapp ing  In format ionFor each verb, the information stored in the three levels discussed above is merged to forma complete set of mapping rules.
During this merging process, the idiosyncrasies takeprecedence over the situation types and the semantic concepts, and the situation typesover the semantic concepts.
For example, the two derived mapping rules for "break"(i.e.one for "break" as in "John broke the window" and the other for "break" as in "Thewindow broke") are shown in Figure 2.
Notice that the semantic TYPE restriction andINSTRUMENT role stored in the knowledge base are also inherited at this time.110(MAPPING (P-ARG I ~CAUSED-PROCESS ~I  ~ PROCESS-OR-STATE"break"(AGENT {TYPE (#CREATURE# #ORGANIZATION#}) \] i( THEME (TYPE #ENTITY#} {MAPPING (SURFACE SUBJECT)) | I (MAPPING (SURFA?
(THE~ (TYPE #ENTITY#) j (INSTRUI~NT (TYPE (#PH?
(MAPPING (SURFACE OBJECT)}) | (MAPPING (p(INSTRUMENT (TYPE #PHYSICAL-OBJECT#)) |(MAPPING (P-ARC INSTRUMENT})}DSit tumt Lon ~" l~sI,,,~,,.,4.
oonITY ) CE SUBJECT))) I | YSICAL-OBJECT#)) | P-ARG INSTRU~NT)I)JFigure 2: Information from the KB, the situation type, and the lexicon all combine toform two predicate-argument mappings for the verb "break.
"3 Automatic Acquisition from CorporaIn order to expand our lexicon to the size needed for broad coverage and to be able to tunethe system to specific domains quickly, we have implemented algorithms to automaticallybuild multilingual lexicons from corpora.
In this section, we discuss how the situationtypes and lexical idiosyncrasies are determined for verbs.Our overall approach is to use simple robust parsing techniques that depend on afew language-dependent syntactic heuristics (e.g.
in English and Spanish, a verb's objectusually directly follows the verb), and a dictionary for part of speech information.
Wehave used these techniques to acquire information from English, Spanish, and Japanesecorpora varying in length from about 25000 words to 2.7 million words.3.1 Acqu i r ing  S i tuat ion  Type  In fo rmat ionWe use two surface features to restrict the possible situation types of a verb: the verb'stransitivity rating and its subject animacy.The transitivity rating of a verb is defined to be the number of transitive occurrencesin the corpus divided by the total occurrences of the verb.
In English, a verb appears inthe transitive when either:?
The verb is directly followed by a noun, determiner, personal pronoun, adjective, orwh-pronoun (e.g.
"John owns a cow.")?
The verb is directly followed by a "THAT" as a subordinate conjunction (e.g.
"Johnsaid that he liked llamas.")?
The verb is directly followed by an infinitive (e.g.
"John promised to walk the dog.")?
The verb past participle is preceded by "BE," as would occur in a passive construc-tion (e.g.
"The apple was eaten by the pig.
")111verb occs TR SA Pred.
ST Correct ST Prepositional IdioSUFFICE 8 0.6250 0.0000 I$~ I~)TIME 15 0.8333 1.0000 C IS) CTRAIN 20 1.0000 1.0000 CP IS/ CP PS) atWRAP 22 0.7222 0.6667 CP IS CP) up over in withPSI I out SORT 25 0.4211 1.0000 CP IS AA CPAA UNITE 27 0.5833 1.0000 CP IS AA PS CPAA 0.
1 00.  cPISUSTAIN 32 0.9062 0.6842 CP  IS CPSUBST ITUTE 33 0.7500 0.5000 IS) 'CP PS) forTARGET 36 0.7778 0.8000 CP  IS 1 'CP 1 from?n STORE 36 0 .9091 1.0000 CP  IS :cCpPSTEAL  36 0.9167 0.6667 CP  ISSHUT 36 0.2400 0.5000 IS PS) 'CP PS) up forSTRETCH 53 0.5278 0.5000 IS PSI 'CP PS) over into out fromSTRIP 57 0.7609 0.8571 'CP IS) 'CP) from into ofTHREATEN 58 0.8793 0.4419 IS) ~CP IS) over01 I iil sS I TREAT 77 0.8052 0.8000 'CP IS as TERMINATE 79 0.9726 1.0000 'CP IS'I,~ PS) on with into WEIGH 81 0.2069 0.5294 'C.r IS)'CP ISTEACH 82 0.7794 0.6875 'CP) atSURROUND 85 0.8000 0.6667 /CP/TOTAL 97 0.0515 0.2759 PS) (CP PS) atVARY 112 0.1354 0.0294 IS PS) (CP PS) from overWAIT 130 0.1923 1.0000 CP IS AA PS) (AA) for upSPEAK 139 0.1667 0.7500 'CP IS AA PS) (AA CP) out at upSURVIVE  146 0.4754 0.3846 IS PS) (IS PS)UNDERSTAND 180 0.6946 0.8684 CP IS) IS)SURGE 188 0.0182 0.3125 PS) PS)SUPPLY  188 0.7176 0.8571 CP IS) CP) withSIT 199 0.0625 0.7027 AA PS) AA  PS) on with at out in upTEND 200 0.8594 0.4340 (IS) CP IS)BREAK 219 0.4771 0.5000 (IS PS) CP PS) up into outWRITE 243 0.4637 0.9123 (CP IS AA PS) CP AA) offWATCH 268 0.7069 0.8462 (CP IS) CP) out overSUCCEED 277 0.5379 0.8899 (CP IS AA PS) CP PS)STAY 300 0.2156 0.6604 ~CP IS AA PS) PS) out up on with atSTAND 310 0.2841 0.7237 (CP IS AA PS) PS CP AA) up at as out onTELL 368 0.8054 0.8101 ICP IS) CP)SPEND 445 0.3823 0.8125 (CP IS AA PS) CP) on over,?4 l, i c ,s I SUGGEST 570 0.7782 0.5918 IS CP  ISTURN 852 0.3418 0.5891 ~IS PS) CP  PS) out into up overSTART 890 0.3474 0.6221 (CP IS AA PS) CP  PS) with off outLOOK 1084 0.1718 0.6520 (CP IS AA PS) AA  PS) at into for upTHINK 1227 0.7602 0.9237 (CP IS) CP)TRY  1272 0.7904 0.8743 (CP IS) CP  )WANT 1659 0.8559 0.8787 (CP IS) IS)USE  2211 0.8416 0.7725 (CP IS) CP)TAKE 2525 0.7447 0.5933 (IS) CP  IS) over off out into upTable 4: Automatically Derived Situation Type and Idiosyncrasy Data112Transitivity:CP/ IS  6.0 ().1 6.2 6.3Ambig.
, , , , ,  : .
- .
,  ?0.0 0.1 0.2 0.3AA/PS  ,, , , , ' , ,0.0 O1 0.2 ().3*1 0.4 6.5 d.~ " ' ' - ' "  0.7 o8 0.9 i oi ~ 99  ~ qlQ e~ .T t  ?
0.4 0.5 0.6 6 0.8 0.9 1.0d.4 d.5 6.6 6.7 6.8 6.9 1'.0Subject Animacy:CP/AA ?
o e~ we6.0 6.1 d.2 d.3 d.4 6.5 6.6"' 8 ~ Y 0.7 0.8 0.9 1.06.1 d.2 d4w ?
Ambig.
00"."
".30 T- ,~" ,- ", r 0.5 0.6 0.7 6.8 1.0 0.9IS/PS t ?
-O0 IJ l  6.2 " 0.3 0.4 6.5 6.6 6.7 6.8 '0.9 i.OFigure 3: This graph shows the accuracy of the Transitivity and Subject Animacy metrics.For Spanish, we use a very similar algorithm, and for Japanese, we look for nounphrases with an object marker "-wo" near and to the left of the verb.
A high transitivityis correlated with CAUSED-PROCESS and INVERSE-STATE while a low transitivitycorrelates with AGENTIVE.-ACTION and PROCESS-OR-STATE.
Table 4 shows 50 verbsand their calculated transitivity rating.
Figure 3 shows that for all but one of the verbsthat are unambiguously transitive the transitivity rating is above 0.6.
The verb "spend"has a transitivity rating of 0.38 because most of its direct objects are numeric dollaramounts, Phrases which begin with a number are not recognized as direct objects, sincemost numeric amounts following verbs are adjuncts as in "John ran 3 miles.
"We define a verb's subject animacy to be the number of times the verb appears withan animate subject over the total occurrences of the verb where we identified the subject.Any noun or pronoun directly preceding a verb is considered to be its subject.
Thisheuristic fails in eases where the subject NP is modified by a PP or relative clause as in"The man under the car wore a red shirt."
We have only implemented this metric forEnglish.
The verb's subject is considered to be animate if it is any one of the following:?
A personal pronoun ("it" and "they" were excluded, since they may refer back toinanimate objects.)?
A proper name?
A word under "agent" or "people" in WordNet (cf.
\[14\])?
A word that appears in a MUC-4 template slot that can be filled only with humans(cf.
\[7\])Verbs that have a low subject animacy cannot be either CAUSED-PROCESS orAGENTIVE-ACTION,  since the syntactic subject must map to the AGENT thematic113role.
A high subject animacy does not correlate with any particular situation type, sinceseveral stative verbs take only animate subjects (e.g.
perception verbs).The predicted situation types shown in Figure 3 were calculated with the followingalgorithm:1.
Assume that the verb can occur with every situation type.2.
If the transitivity rating is greater than 0.6, then discard the AGENTIVE-ACTIONand PROCESS-OR-STATE possibilities.3.
If the transitivity rating is below 0.1, then discard the CAUSED-PROCESS andINVERSE-STATE possibilities.4.
If the subject animacy is below 0.6, then discard the CAUSED-PROCESS andAGENTIVE-ACTION possibilities.We are planning several improvements to our situation type determination algorithms.First, because some stative verbs can take animate subjects (e.g.
perception verbs like"see", "know", etc.
), we sometimes cannot distinguish between INVERSE-STATE orPROCESS-OR-STATE and CAUSED-PROCESS or AGENTIVE-ACTION verbs.
Thisproblem, however, can be solved by using algorithms by Brent \[3\] or Dorr \[8\] for identifyingstative verbs.Second, verbs ambiguous between CAUSED-PROCESS and PROCESS-OR-STATE(e.g.
"break", "vary") often get inconclusive results because they appear transitively about50% of the time.
When these verbs are transitive, the subjects are almost always animateand when they are intransitive, the subjects are nearly always inanimate.
We plan torecognize these situations by calculating animacy separately for transitive and intransitivecases .3.2  Acqu i r ing  Id iosyncrat i c  In fo rmat ionWe automatically identify likely pre/postpositional argument structures for a given verbby looking for pre/postpositions in places where they are likely to attach to the verb (i.e.within a few words to the right for Spanish and English, and to the left for Japanese).When a particular pre/postposition appears here much more often than chance (basedon either Mutual Information or a chi-squared test \[5, 4\]), we assume that it is a likelyargument.
A very similar strategy works well at identifying verbs that take sententiaicomplements by looking for complementizers (e.g.
"that", "to") in positions of likelyattachment.
Some English examples are shown in Tables 4 and 5, and Spanish examplesare shown in Tables 6 and 7.
The details of the exact algorithms used for English arecontained in McKee and Maloney \[13\].
Areas for improvement include distinguishingbetween cases where a verb takes a prepositional rguments, a prepositional particle, ora common adjunct.4 Conc lus ionWe have automatically built lexicons with predicate-argument mapping information fromEnglish, Spanish and Japanese corpora.
These lexicons have been used for several multi-lingual data extraction applications (cf.
Aone et ai.
\[2\]) and a prototype Japanese-English114word possible clausal complementsknow THATCOMPvow THATCOMP,  TOCOMPeatwant  TOCOMPresume INGCOMPTable 5: English Verbs which Take Complementizersverb MI with "que"indicar 9.3sefialar 8.7est imar 8.6calcular 7.7precisar 7.7anunclar i 7.7Table 6: Spanish Verbs which Take ComplementizersverblucharunitvacunarcifrarconsultarpaaaracordarcontarrelacionarnotificaroeurrirencontrarprepositioncontracontracontrasobresobresobreconconconenenenMI between verb and preposition12.48.98.99.69.68.610.810.39.78.78.07.8Table 7: Spanish Verbs that Take Prepositional Arguments11 5machine translation system.
The algorithms presented here have minimized our lexicalacquisition effort considerably.Currently we are investigating ways in which thematic role slots of verb frames andsemantic type restrictions on these slots can be derived automatically from corpora (cf.Dagan and Itai \[6\], Hindle and Rooth \[10\], Zernik and Jacobs \[20\]) so that knowledgeacquisition at all three levels of predicate-argument mapping can be automated.References\[1\] Chinatsu Aone and Doug McKee.
Three-Level Knowledge Representation ofPredicate-Argument Mapping for Muitilingual Lexicons.
In AAAI Spring Sympo-sium Working Notes on Building Lexicons for Machine Translation, 1993.\[2\] Chinatsu Aone, Doug McKee, Sandy Shinn, and Hatte Blejer.
SRA: Descriptionof the SOLOMON System as Used for MUC-4.
In Proceedings of Fourth MessageUnderstanding Conference (MUC-$), 1992.\[3\] Michael Brent.
Automatic Semantic Classification of Verbs from Their SyntacticContexts: An Implemented Classifier for Stativity.
In Proceedings ofthe 5th EuropeanACL Conference, 1991.\[4\] Kenneth Church and William Gale.
Concordances for Parallel Text.
In Proceedingsof the Seventh Annual Conference ofthe University of Waterloo Centre for the NewOED and Text Research: Using Corpora, 1991.\[5\] Kenneth Church and Patrick Hanks.
Word Association Norms, Mutual Information,and Lexicography.
Computational Linguistics, 16(1), 1990.\[6\] Ido Dagan and Alon Itai.
Automatic Acquisition of Constraints for the Resolutionof Anaphora References and Syntactic Ambiguities.
In Proceedings ofthe 13th Inter-national Conference on Computational Linguistics, 1990.\[7\] Defense Advanced Research Projects Agency.
Proceedings ofFourth Message Under-standing Conference (MUG-4).
Morgan Kaufmann Publishers, 1992.\[8\] Bonnie Dorr.
A Parameterized Approach to Integrating Aspect with Lexical-Semantics for Machine Translation.
In Proceedings of 30th Annual Meeting of theACL, 1992.\[9\] David Dowty.
Word Meaning and Montague Grammar.
D. Reidel, 1979.\[10\] Donald ttindle and Mats Rooth.
Structural Ambiguity and Lexical Relations.
InProceedings of 29th Annual Meeting of the ACL, 1991.\[11\] Manfred Krifka.
Nominal Reference, Temporal Construction, and Quantification iEvent Semantics.
In R. Bartsch et al, editors, Semantics and Contextual Expressions.Forts, Dordrecht, 1989.\[12\] Susumu Kuno.
The Structure of the Japanese Language.
M1T Press, 1973.116\[13\] Doug McKee and John Maloney.
Using Statistics Gained from Corpora ina Knowledge-Based NLP System.
In Proceedings of The AAAI Workshop onStatistically-Based NLP Techniques, 1992.\[14\] George Miller, Richard Beckwith, Christiane Fellbaum, Derek Gross, and KatherineMiller.
Five papers on WordNet.
Technical Report CSL Report 43, Cognitive ScienceLaboratory, Princeton University, 1990.\[15\] Marc Moens and Mark Steedman.
Temporal ontology and temporal reference.
Com-putational Linguistics, 14(2), 1988.\[16\] Sergei Nirenburg and Lori Levin.
Syntax-Driven and Ontology-Driven Lexical Se-mantics.
In Proceedings of ACL Lexical Semantics and Knowledge RepresentationWorkshop, 1991.\[17\] Boyan Onyshkevych and Sergei Nirenburg.
Lexicon, Ontology and Text Meaning.In Proceedings of A CL Lezical Semantics and Knowledge Representation Workshop,1991.\[18\] Leonard Talmy.
Lexicalization Patterns: Semantic Structure in Lexical Forms.
InTimothy Shopen, editor, Language Typology and Syntactic Descriptions.
CambridgeUniversity Press, 1985.\[19\] Zeno Vendler.
Linguistics in Philosophy.
Cornell University Press, 1967.\[20\] Uri Zernik and Paul Jacobs.
Tagging for Learning: Collecting Thematic Relationsfrom Corpus.
In Proceedings of the 13th International Conference on ComputationalLinguistics, 1990.116 a
