Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 196?206,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsFast and Robust Compressive Summarizationwith Dual Decomposition and Multi-Task LearningMiguel B.
Almeida??
Andre?
F. T.
Martins??
?Priberam Labs, Alameda D. Afonso Henriques, 41, 2o, 1000-123 Lisboa, Portugal?Instituto de Telecomunicac?o?es, Instituto Superior Te?cnico, 1049-001 Lisboa, Portugal{mba,atm}@priberam.ptAbstractWe present a dual decomposition frame-work for multi-document summarization,using a model that jointly extracts andcompresses sentences.
Compared withprevious work based on integer linear pro-gramming, our approach does not requireexternal solvers, is significantly faster, andis modular in the three qualities a sum-mary should have: conciseness, informa-tiveness, and grammaticality.
In addition,we propose a multi-task learning frame-work to take advantage of existing datafor extractive summarization and sentencecompression.
Experiments in the TAC-2008 dataset yield the highest publishedROUGE scores to date, with runtimes thatrival those of extractive summarizers.1 IntroductionAutomatic text summarization is a seminal prob-lem in information retrieval and natural languageprocessing (Luhn, 1958; Baxendale, 1958; Ed-mundson, 1969).
Today, with the overwhelmingamount of information available on the Web, thedemand for fast, robust, and scalable summariza-tion systems is stronger than ever.Up to now, extractive systems have been themost popular in multi-document summarization.These systems produce a summary by extractinga representative set of sentences from the origi-nal documents (Kupiec et al, 1995; Carbonell andGoldstein, 1998; Radev et al, 2000; Gillick et al,2008).
This approach has obvious advantages: itreduces the search space by letting decisions bemade for each sentence as a whole (avoiding fine-grained text generation), and it ensures a grammat-ical summary, assuming the original sentences arewell-formed.
The typical trade-offs in these mod-els (maximizing relevance, and penalizing redun-dancy) lead to submodular optimization problems(Lin and Bilmes, 2010), which are NP-hard but ap-proximable through greedy algorithms; learning ispossible with standard structured prediction algo-rithms (Sipos et al, 2012; Lin and Bilmes, 2012).Probabilistic models have also been proposed tocapture the problem structure, such as determinan-tal point processes (Gillenwater et al, 2012).However, extractive systems are rather limitedin the summaries they can produce.
Long, partlyrelevant sentences tend not to appear in the sum-mary, or to block the inclusion of other sen-tences.
This has motivated research in compres-sive summarization (Lin, 2003; Zajic et al, 2006;Daume?, 2006), where summaries are formed bycompressed sentences (Knight and Marcu, 2000),not necessarily extracts.
While promising resultshave been achieved by models that simultaneouslyextract and compress (Martins and Smith, 2009;Woodsend and Lapata, 2010; Berg-Kirkpatrick etal., 2011), there are still obstacles that need tobe surmounted for these systems to enjoy wideadoption.
All approaches above are based on in-teger linear programming (ILP), suffering fromslow runtimes, when compared to extractive sys-tems.
For example, Woodsend and Lapata (2012)report 55 seconds on average to produce a sum-mary; Berg-Kirkpatrick et al (2011) report sub-stantially faster runtimes, but fewer compressionsare allowed.
Having a compressive summarizerwhich is both fast and expressive remains an openproblem.
A second inconvenience of ILP-basedapproaches is that they do not exploit the modu-larity of the problem, since the declarative specifi-cation required by ILP solvers discards importantstructural information.
For example, such solversare unable to take advantage of efficient dynamicprogramming routines for sentence compression(McDonald, 2006).196This paper makes progress in two fronts:?
We derive a dual decomposition framework forextractive and compressive summarization (?2?3).
Not only is this framework orders of mag-nitude more efficient than the ILP-based ap-proaches, it also allows the three well-knownmetrics of summaries?conciseness, informa-tiveness, and grammaticality?to be treated sep-arately in a modular fashion (see Figure 1).
Wealso contribute with a novel knapsack factor,along with a linear-time algorithm for the corre-sponding dual decomposition subproblem.?
We propose multi-task learning (?4) as a prin-cipled way to train compressive summarizers,using auxiliary data for extractive summariza-tion and sentence compression.
To this end,we adapt the framework of Evgeniou and Pon-til (2004) and Daume?
(2007) to train structuredpredictors that share some of their parts.Experiments on TAC data (?5) yield state-of-the-art results, with runtimes similar to that of extrac-tive systems.
To our best knowledge, this hadnever been achieved by compressive summarizers.2 Extractive SummarizationIn extractive summarization, we are given a setof sentences D := {s1, .
.
.
, sN} belonging to oneor more documents, and the goal is to extract asubset S ?
D that conveys a good summary of Dand whose total number of words does not exceeda prespecified budget B.We use an indicator vector y := ?yn?Nn=1 to rep-resent an extractive summary, where yn = 1 ifsn ?
S, and yn = 0 otherwise.
Let Ln be thenumber of words of the nth sentence.
By design-ing a quality score function g : {0, 1}N ?
R, thiscan be cast as a global optimization problem witha knapsack constraint:maximize g(y)w.r.t.
y ?
{0, 1}Ns.t.
?Nn=1 Lnyn ?
B.
(1)Intuitively, a good summary is one which selectssentences that individually convey ?relevant?
in-formation, while collectively having small ?re-dundancy.?
This trade-off was explicitly mod-eled in early works through the notion of max-imal marginal relevance (Carbonell and Gold-stein, 1998; McDonald, 2007).
An alternativeare coverage-based models (?2.1; Filatova andHatzivassiloglou, 2004; Yih et al, 2007; Gillicket al, 2008), which seek a set of sentences thatcovers as many diverse ?concepts?
as possible; re-dundancy is automatically penalized since redun-dant sentences cover fewer concepts.
Both modelscan be framed under the framework of submodularoptimization (Lin and Bilmes, 2010), leading togreedy algorithms that have approximation guar-antees.
However, extending these models to allowfor sentence compression (as will be detailed in?3) breaks the diminishing returns property, mak-ing submodular optimization no longer applicable.2.1 Coverage-Based SummarizationCoverage-based extractive summarization can beformalized as follows.
Let C(D) := {c1, .
.
.
, cM}be a set of relevant concept types which arepresent in the original documents D.1 Let ?m be arelevance score assigned to the mth concept, andlet the set Im ?
{1, .
.
.
, N} contain the indices ofthe sentences in which this concept occurs.
Then,the following quality score function is defined:g(y) =?Mm=1 ?mum(y), (2)where um(y) := ?n?Im yn is a Boolean functionthat indicates whether the mth concept is presentin the summary.
Plugging this into Eq.
1, one ob-tains the following Boolean optimization problem:maximize?Mm=1 ?mumw.r.t.
y ?
{0, 1}N , u ?
{0, 1}Ms.t.
um =?n?Im yn, ?m ?
[M ]?Nn=1 Lnyn ?
B, (3)where we used the notation [M ] := {1, .
.
.
,M}.This can be converted into an ILP and addressedwith off-the-shelf solvers (Gillick et al, 2008).
Adrawback of this approach is that solving an ILPexactly is NP-hard.
Even though existing commer-cial solvers can solve most instances with a mod-erate speed, they still exhibit poor worst-case be-haviour; this is exacerbated when there is the needto combine an extractive component with othermodules, as in compressive summarization (?3).1Previous work has modeled concepts as events (Filatovaand Hatzivassiloglou, 2004), salient words (Lin and Bilmes,2010), and word bigrams (Gillick et al, 2008).
In the sequel,we assume concepts are word k-grams, but our model canhandle other representations, such as phrases or predicate-argument structures.1972.2 A Dual Decomposition FormulationWe next describe how the problem in Eq.
3 can beaddressed with dual decomposition, a class of op-timization techniques that tackle the dual of com-binatorial problems in a modular, extensible, andparallelizable manner (Komodakis et al, 2007;Rush et al, 2010).
In particular, we employ al-ternating directions dual decomposition (AD3;Martins et al, 2011a, 2012) for solving a linear re-laxation of Eq.
3.
AD3 resembles the subgradient-based algorithm of Rush et al (2010), but it enjoysa faster convergence rate.
Both algorithms splitthe original problem into several components,and then iterate between solving independent lo-cal subproblems at each component and adjustingmultipliers to promote an agreement.2 The differ-ence between the two methods is that the AD3 lo-cal subproblems, instead of requiring the compu-tation of a locally optimal configuration, requiresolving a local quadratic problem.
Martins et al(2011b) provided linear-time solutions for severallogic constraints, with applications to syntax andframe-semantic parsing (Das et al, 2012).
We willsee that AD3 can also handle budget and knapsackconstraints efficiently.To tackle Eq.
3 with dual decomposition, wesplit the coverage-based summarizer into the fol-lowing M + 1 components (one per constraint):1.
For each of the M concepts in C(D), onecomponent for imposing the logic constraintin Eq.
3.
This corresponds to the OR-WITH-OUTPUT factor described by Martins et al(2011b); the AD3 subproblem for themth factorcan be solved in time O(|Im|).2.
Another component for the knapsack con-straint.
This corresponds to a (novel) KNAP-SACK factor, whose AD3 subproblem is solv-able in time O(N).
The actual algorithm is de-scribed in the appendix (Algorithm 1).33 Compressive SummarizationWe now turn to compressive summarization,which does not limit the summary sentences to beverbatim extracts from the original documents; in-2For details about dual decomposition and Lagrangian re-laxation, see the recent tutorial by Rush and Collins (2012).3The AD3 subproblem in this case corresponds to com-puting an Euclidean projection onto the knapsack polytope(Eq.
11).
Others addressed the related, but much harder, inte-ger quadratic knapsack problem (McDonald, 2007).stead, it allows the extraction of compressed sen-tences where some words can be deleted.Formally, let us express each sentence of Das a sequence of word tokens, sn := ?tn,`?Ln`=0,where tn,0 ?
$ is a dummy symbol.
We rep-resent a compression of sn as an indicator vec-tor zn := ?zn,`?Ln`=0, where zn,` = 1 if the `thword is included in the compression.
By conven-tion, the dummy symbol is included if and only ifthe remaining compression is non-empty.
A com-pressive summary can then be represented by anindicator vector z which is the concatenation ofN such vectors, z = ?z1, .
.
.
,zN ?
; each positionin this indicator vector is indexed by a sentencen ?
[N ] and a word position ` ?
{0} ?
[Ln].Models for compressive summarization wereproposed by Martins and Smith (2009) and Berg-Kirkpatrick et al (2011) by combining extractionand compression scores.
Here, we follow the lat-ter work, by combining a coverage score functiong with sentence-level compression score functionsh1, .
.
.
, hN .
This yields the decoding problem:maximize g(z) +?Nn=1 hn(zn)w.r.t.
zn ?
{0, 1}Ln , ?n ?
[N ]s.t.
?Nn=1?Ln`=1 zn,` ?
B.
(4)3.1 Coverage ModelWe use a coverage function similar to Eq.
2, buttaking a compressive summary z as argument:g(z) =?Mm=1 ?mum(z), (5)where we redefine um as follows.
First, weparametrize each occurrence of the mth concept(assumed to be a k-gram) as a triple ?n, `s, `e?,where n indexes a sentence, `s indexes a start po-sition within the sentence, and `e indexes the endposition.
We denote by Tm the set of triples repre-senting all occurrences of the mth concept in theoriginal text, and we associate an indicator vari-able zn,`s:`e to each member of this set.
We thendefine um(z) via the following logic constraints:?
A concept type is selected if some of its k-gramtokens are selected:um(y) :=??n,`s,`e?
?Tm zn,`s:`e .
(6)?
A k-gram concept token is selected if all itswords are selected:zn,`s:`e :=?`e`=`s zn,`.
(7)198Sentences $     The      leader    of   moderate  Kashmiri  separatists warned   Thursday   that ...$     Talks    with   Kashmiri  separatists began    last       year ..."Kashmiri separatists"BudgetConcept tokensConcept typeFigure 1: Components of our compressive summarizer.
Factors depicted in blue belong to the compres-sion model, and aim to enforce grammaticality.
The logic factors in red form the coverage component.Finally, the budget factor, in green, is connected to the word nodes; it ensures that the summary fits theword limit.
Shaded circles represent active variables while white circles represent inactive variables.We set concept scores as ?m := w ?
?cov(D, cm),where ?cov(D, cm) is a vector of features (de-scribed in ?3.5) and w the corresponding weights.3.2 Compression ModelFor the compression score function, we followMartins and Smith (2009) and decompose it as asum of local score functions ?n,` defined on de-pendency arcs:hn(zn) :=?Ln`=1 ?n,`(zn,`, zn,pi(`)), (8)where pi(`) denotes the index of the word whichis the parent of the `th word in the dependencytree (by convention, the root of the tree is thedummy symbol).
To model the event that anarc is ?cut?
by disconnecting a child from itshead, we define arc-deletion scores ?n,`(0, 1) :=w ?
?comp(sn, `, pi(`)), where ?comp is a featuremap, which is described in detail in ?3.5.
We set?n,`(0, 0) = ?n,`(1, 1) = 0, and ?n,`(1, 0) = ?
?,to allow only the deletion of entire subtrees.A crucial fact is that one can maximize Eq.
8efficiently with dynamic programming (using theViterbi algorithm for trees); the total cost is linearin Ln.
We will exploit this fact in the dual decom-position framework described next.43.3 A Dual Decomposition FormulationIn previous work, the optimization problem inEq.
4 was converted into an ILP and fed to an off-the-shelf solver (Martins and Smith, 2009; Berg-Kirkpatrick et al, 2011; Woodsend and Lapata,2012).
Here, we employ the AD3 algorithm, in a4The same framework can be readily adapted to othercompression models that are efficiently decodable, such asthe semi-Markov model of McDonald (2006), which wouldallow incorporating a language model for the compression.similar manner as described in ?2, but with an ad-ditional component for the sentence compressor,and slight modifications in the other components.We have the following N +M +?Mm=1 |Tm|+ 1components in total, illustrated in Figure 1:1.
For each of the N sentences, one componentfor the compression model.
The AD3 quadraticsubproblem for this factor can be addressed bysolving a sequence of linear subproblems, as de-scribed by Martins et al (2012).
Each of thesesubproblems corresponds to maximizing an ob-jective function of the same form as Eq.
8; thiscan be done in O(Ln) time with dynamic pro-gramming, as discussed in ?3.2.2.
For each of the M concept types in C(D),one OR-WITH-OUTPUT factor for the logic con-straint in Eq.
6.
This is analogous to the onedescribed for the extractive case.3.
For each k-gram concept token in Tm, oneAND-WITH-OUTPUT factor that imposes theconstraint in Eq.
7.
This factor was describedby Martins et al (2011b) and its AD3 subprob-lem can be solved in time linear in k.4.
Another component linked to all the words im-posing that at most B words can be selected;this is done via a BUDGET factor, a particularcase of KNAPSACK.
The runtime of this AD3subproblem is linear in the number of words.In addition, we found it useful to add a secondBUDGET factor limiting the number of sentencesthat can be selected to a prescribed value K. Weset K = 6 in our experiments.1993.4 Rounding StrategyRecall that the problem in Eq.
4 is NP-hard, andthat AD3 is solving a linear relaxation.
Whilethere are ways of wrapping AD3 in an exact searchalgorithm (Das et al, 2012), such strategies workbest when the solution of the relaxation has fewfractional components, which is typical of pars-ing and translation problems (Rush et al, 2010;Chang and Collins, 2011), and attractive networks(Taskar et al, 2004).
Unfortunately, this is not thecase in summarization, where concepts ?compete?with each other for inclusion in the summary, lead-ing to frustrated cycles.
We chose instead to adopta fast and simple rounding procedure for obtaininga summary from a fractional solution.The procedure works as follows.
First, solvethe LP relaxation using AD3, as described above.This yields a solution z?, where each componentlies in the unit interval [0, 1].
If these componentsare all integer, then we have a certificate that thisis the optimal solution.
Otherwise, we collect theK sentences with the highest values of z?n,0 (?pos-teriors?
on sentences), and seek the feasible sum-mary which is the closest (in Euclidean distance)to z?, while only containing those sentences.
Thiscan be computed exactly in timeO(B?Kk=1 Lnk),through dynamic programming.53.5 Features and Hard ConstraintsAs Berg-Kirkpatrick et al (2011), we usedstemmed word bigrams as concepts, to which weassociate the following concept features (?cov):indicators for document counts, features indicat-ing if each of the words in the bigram is a stop-word, the earliest position in a document each con-cept occurs, as well as two and three-way conjunc-tions of these features.For the compression model, we include the fol-lowing arc-deletion features (?comp):?
the dependency label of the arc being deleted, aswell as its conjunction with the part-of-speechtag of the head, of the modifier, and of both;?
the dependency labels of the arc being deletedand of its parent arc;?
the modifier tag, if the modifier is a functionword modifying a verb ;5Briefly, if we link the roots of theK sentences to a super-root node, the problem above can be transformed into thatof finding the best configuration in the resulting binary treesubject to a budget constraint.
We omit details for space.?
a feature indicating whether the modifier or anyof its descendants is a negation word;?
indicators of whether the modifier is a temporalword (e.g., Friday) or a preposition pointing toa temporal word (e.g., on Friday).In addition, we included hard constraints to pre-vent the deletion of certain arcs, following pre-vious work in sentence compression (Clarke andLapata, 2008).
We never delete arcs whose de-pendency label is SUB, OBJ, PMOD, SBAR, VC, orPRD (this makes sure we preserve subjects and ob-jects of verbs, arcs departing from prepositions orcomplementizers, and that we do not break verbchains or predicative complements); arcs linkingto a conjunction word or siblings of such arcs (toprevent inconsistencies in handling coordinativeconjunctions); arcs linking verbs to other verbs,to adjectives (e.g., make available), to verb parti-cles (e.g., settle down), to the word that (e.g., saidthat), or to the word to if it is a leaf (e.g., allowedto come); arcs pointing to negation words, cardinalnumbers, or determiners; and arcs connecting twoproper nouns or words within quotation marks.4 Multi-Task LearningWe next turn to the problem of learning the modelfrom training data.
Prior work in compressivesummarization has followed one of two strategies:Martins and Smith (2009) and Woodsend and La-pata (2012) learn the extraction and compressionmodels separately, and then post-combine them,circumventing the lack of fully annotated data.Berg-Kirkpatrick et al (2011) gathered a smalldataset of manually compressed summaries, andtrained with full supervision.
While the latterapproach is statistically more principled, it hasthe disadvantage of requiring fully annotated data,which is difficult to obtain in large quantities.
Onthe other hand, there is plenty of data contain-ing manually written abstracts (from the DUC andTAC conferences) and user-generated text (fromWikipedia) that may provide useful weak supervi-sion.With this in mind, we put together a multi-tasklearning framework for compressive summariza-tion (which we name task #1).
The goal is totake advantage of existing data for related tasks,such as extractive summarization (task #2), andsentence compression (task #3).
The three tasksare instances of structured predictors (Bak?r et200Tasks Features DecoderComp.
summ.
(#1) ?cov, ?comp AD3 (solve Eq.
4)Extr.
summ.
(#2) ?cov AD3 (solve Eq.
3)Sent.
comp.
(#3) ?comp dyn.
prg.
(max.
Eq.
8)Table 1: Features and decoders used for each task.al., 2007), and for all of them we assume feature-based models that decompose over ?parts?:?
For the compressive summarization task, theparts correspond to concept features (?3.1) andto arc-deletion features (?3.2).?
For the extractive summarization task, there areparts for concept features only.?
For the sentence compression task, the partscorrespond to arc-deletion features only.This is summarized in Table 1.
Features forthe three tasks are populated into feature vectors?1(x, y), ?2(x, y), and ?3(x, y), respectively,where ?x, y?
denotes a task-specific input-outputpair.
We assume the feature vectors are all D di-mensional, where we place zeros in entries cor-responding to parts that are absent.
Note thatthis setting is very general and applies to arbi-trary structured prediction problems (not just sum-marization), the only assumption being that someparts are shared between different tasks.Next, we associate weight vectors v1,v2,v3 ?RD to each task, along with a ?shared?
vector w.Each task makes predictions according to the rule:y?
:= arg maxy(w + vk) ?
?k(x, y), (9)where k ?
{1, 2, 3}.
This setting is equiva-lent to the approach of Daume?
(2007) for domainadaptation, which consists in splitting each fea-ture into task-component features and a sharedfeature; but here we do not duplicate features ex-plicitly.
To learn the weights, we regularize theweight vectors separately, and assume that eachtask has its own loss function Lk, so that the to-tal loss L is a weighted sum L(w,v1,v2,v3) :=?3k=1 ?kLk(w + vk).
This yields the followingobjective function to be minimized:F (w,v1,v2,v3) =?2 ?w?2 +3?k=1?k2 ?vk?2+ 1N3?k=1?kLk(w + vk), (10)where ?
and the ?k?s are regularization constants,andN is the total number of training instances.6 Inour experiments (?5), we let the Lk?s be structuredhinge losses (Taskar et al, 2003; Tsochantaridis etal., 2004), where the corresponding cost functionsare concept recall (for task #2), precision of arcdeletions (for task #3), and a combination thereof(for task #1).7 These losses were normalized, andwe set ?k = N/Nk, where Nk is the number oftraining instances for the kth task.
This ensuresall tasks are weighted evenly.
We used the samerationale to set ?
= ?1 = ?2 = ?3, choosing thisvalue through cross-validation in the dev set.We optimize Eq.
10 with stochastic subgradientdescent.
This leads to update rules of the formw ?
(1?
?t?
)w ?
?t?k?
?Lk(w + vk)vj ?
(1?
?t?j)vj ?
?t?jk?k?
?Lk(w + vk),where ?
?Lk are stochastic subgradients for the kthtask, that take only a single instance into account,and ?jk = 1 if and only if j = k. Stochastic sub-gradients can be computed via cost-augmented de-coding (see footnote 7).Interestingly, Eq.
10 subsumes previous ap-proaches to train compressive summarizers.
Thelimit ???
(keeping the ?k?s fixed) forces w ?0, decoupling all the tasks.
In this limit, inferencefor task #1 (compressive summarization) is basedsolely on the model learned from that task?s data,recovering the approach of Berg-Kirkpatrick et al(2011).
In the other extreme, setting ?1 = 0 sim-ply ignores task #1?s training data.
As a result, theoptimal v1 will be a vector of zeros; since tasks#2 and #3 have no parts in common, the objectivewill decouple into a sum of two independent terms6Note that, by substituting uk := w+vk and solving forw, the problem in Eq.
10 becomes that of minimizing the sumof the losses with a penalty for the (weighted) variance of thevectors {0,u1,u2,u3}, regularizing the difference towardstheir average, as in Evgeniou and Pontil (2004).
This is sim-ilar to the hierarchical joint learning approach of Finkel andManning (2010), except that our goal is to learn a new task(compressive summarization) instead of combining tasks.7Let Yk denote the output set for the kth task.
Givena task-specific cost function ?k : Yk ?
Yk ?
R,and letting ?xt, yt?Tt=1 be the labeled dataset for thistask, the structured hinge loss takes the form Lk(uk) :=?tmaxy?
?Yk(uk ?
(?k(xt, y?)?
?k(xt, yt)) + ?k(y?, yt)).The inner maximization over y?
is called the cost-augmenteddecoding problem: it differs from Eq.
9 by the inclusionof the cost term ?k(y?, yt).
Our costs decompose over themodel?s factors, hence any decoder for Eq.
9 can be usedfor the maximization above: for tasks #1?#2, we solve arelaxation by running AD3 without rounding, and for task #3we use dynamic programming; see Table 1.201involving v2 and v3, which is equivalent to train-ing the two tasks separately and post-combiningthe models, as Martins and Smith (2009) did.5 Experiments5.1 Experimental setupWe evaluated our compressive summarizers ondata from the Text Analysis Conference (TAC)evaluations.
We use the same splits as previ-ous work (Berg-Kirkpatrick et al, 2011; Wood-send and Lapata, 2012): the non-update portionsof TAC-2009 for training and TAC-2008 for test-ing.
In addition, we reserved TAC-2010 as a dev-set.
The test partition contains 48 multi-documentsummarization problems; each provides 10 relatednews articles as input, and asks for a summarywith up to 100 words, which is evaluated againstfour manually written abstracts.
We ignored allthe query information present in the TAC datasets.Single-Task Learning.
In the single-task exper-iments, we trained a compressive summarizer onthe dataset disclosed by Berg-Kirkpatrick et al(2011), which contains manual compressive sum-maries for the TAC-2009 data.
We trained a struc-tured SVM with stochastic subgradient descent;the cost-augmented inference problems are re-laxed and solved with AD3, as described in ?3.3.8We followed the procedure described in Berg-Kirkpatrick et al (2011) to reduce the number ofcandidate sentences: scores were defined for eachsentence (the sum of the scores of the conceptsthey cover), and the best-scored sentences weregreedily selected up to a limit of 1,000 words.
Wethen tagged and parsed the selected sentences withTurboParser.9 Our choice of a dependency parserwas motivated by our will for a fast system; in par-ticular, TurboParser attains top accuracies at a rateof 1,200 words per second, keeping parsing timesbelow 1 second for each summarization problem.Multi-Task Learning.
For the multi-task ex-periments, we also used the dataset of Berg-Kirkpatrick et al (2011), but we augmented thetraining data with extractive summarization andsentence compression datasets, to help train the8We use the AD3 implementation in http://www.ark.cs.cmu.edu/AD3, setting the maximum number ofiterations to 200 at training time and 1000 at test time.
Weextended the code to handle the knapsack and budget factors;the modified code will be part of the next release (AD3 2.1).9http://www.ark.cs.cmu.edu/TurboParsercompressive summarizer.
For extractive sum-marization, we used the DUC 2003 and 2004datasets (a total of 80 multi-document summariza-tion problems).
We generated oracle extracts bymaximizing bigram recall with respect to the man-ual abstracts, as described in Berg-Kirkpatrick etal.
(2011).
For sentence compression, we adaptedthe Simple English Wikipedia dataset of Wood-send and Lapata (2011), containing aligned sen-tences for 15,000 articles from the English andSimple English Wikipedias.
We kept only the4,481 sentence pairs corresponding to deletion-based compressions.5.2 ResultsTable 2 shows the results.
The top rows referto three strong baselines: the ICSI-1 extractivecoverage-based system of Gillick et al (2008),which achieved the best ROUGE scores in theTAC-2008 evaluation; the compressive summa-rizer of Berg-Kirkpatrick et al (2011), denotedBGK?11; and the multi-aspect compressive sum-marizer of Woodsend and Lapata (2012), denotedWL?12.
All these systems require ILP solvers.The bottom rows show the results achieved byour implementation of a pure extractive system(similar to the learned extractive summarizer ofBerg-Kirkpatrick et al, 2011); a system that post-combines extraction and compression componentstrained separately, as in Martins and Smith (2009);and our compressive summarizer trained as a sin-gle task, and in the multi-task setting.The ROUGE and Pyramid scores show that thecompressive summarizers (when properly trained)yield considerable benefits in content coverageover extractive systems, confirming the results ofBerg-Kirkpatrick et al (2011).
Comparing thetwo bottom rows, we see a clear benefit by train-ing in the multi-task setting, with a consistentgain in both coverage and linguistic quality.
OurROUGE-2 score (12.30%) is, to our knowledge,the highest reported on the TAC-2008 dataset,with little harm in grammaticality with respect toan extractive system that preserves the originalsentences.
Figure 2 shows an example summary.5.3 RuntimesWe conducted another set of experiments to com-pare the runtime of our compressive summarizerbased on AD3 with the runtimes achieved byGLPK, the ILP solver used by Berg-Kirkpatrick etal.
(2011).
We varied the maximum number of it-202System R-2 R-SU4 Pyr LQICSI-1 11.03 13.96 34.5?
?BGK?11 11.71 14.47 41.3?
?WL?12 11.37 14.47 ?
?Extractive 11.16 14.07 36.0 4.6Post-comb.
11.07 13.85 38.4 4.1Single-task 11.88 14.86 41.0 3.8Multi-task 12.30 15.18 42.6 4.2Table 2: Results for compressive summarization.Shown are the ROUGE-2 and ROUGE SU-4 re-calls with the default options from the ROUGEtoolkit (Lin, 2004); Pyramid scores (Nenkova andPassonneau, 2004); and linguistic quality scores,scored between 1 (very bad) to 5 (very good).
ForPyramid, the evaluation was performed by twoannotators, each evaluating half of the problems;scores marked with ?
were computed by differentannotators and are not directly comparable.
Lin-guistic quality was evaluated by two linguists; weshow the average of the reported scores.Solver Runtime (sec.)
ROUGE-2ILP Exact 10.394 12.40LP-Relax.
2.265 12.38AD3-5000 0.952 12.38AD3-1000 0.406 12.30AD3-200 0.159 12.15Extractive (ILP) 0.265 11.16Table 3: Runtimes of several decoders on a IntelCore i7 processor @2.8 GHz, with 8GB RAM.
Foreach decoder, we show the average time taken tosolve a summarization problem in TAC-2008.
Thereported runtimes of AD3 and LP-Relax includethe time taken to round the solution (?3.4), whichis 0.029 seconds on average.erations of AD3 in {200, 1000, 5000}, and clockedthe time spent by GLPK to solve the exact ILPsand their relaxations.
Table 3 depicts the results.10We see that our proposed configuration (AD3-1000) is orders of magnitude faster than the ILPsolver, and 5 times faster than its relaxed variant,while keeping similar accuracy levels.11 The gainwhen the number of iterations in AD3 is increasedto 5000 is small, given that the runtime is more10Within dual decomposition algorithms, we verified ex-perimentally that AD3 is substantially faster than the subgra-dient algorithm, which is consistent with previous findings(Martins et al, 2011b).11The runtimes obtained with the exact ILP solver seemslower than those reported by Berg-Kirkpatrick et al (2011).
(around 1.5 sec.
on average, according to their Fig.
3).
Weconjecture that this difference is due to the restricted set ofsubtrees that can be deleted by Berg-Kirkpatrick et al (2011),which greatly reduces their search space.Japan dispatched four military ships to help Russia res-cue seven crew members aboard a small submarinetrapped on the seabed in the Far East.
The RussianPacific Fleet said the crew had 120 hours of oxygenreserves on board when the submarine submerged atmidday Thursday (2300 GMT Wednesday) off the Kam-chatka peninsula, the stretch of Far Eastern Russia fac-ing the Bering Sea.
The submarine, used in rescue,research and intelligence-gathering missions, becamestuck at the bottom of the Bay of Berezovaya off Rus-sia?s Far East coast when its propeller was caught in afishing net.
The Russian submarine had been tendingan underwater antenna mounted to the sea floor when itbecame snagged on a wire helping to stabilize a ventila-tion cable attached to the antenna.
Rescue crews low-ered a British remote-controlled underwater vehicle to aRussian mini-submarine trapped deep under the PacificOcean, hoping to free the vessel and its seven trappedcrewmen before their air supply ran out.Figure 2: Example summary from our compres-sive system.
Removed text is grayed out.than doubled; accuracy starts to suffer, however, ifthe number of iterations is reduced too much.
Inpractice, we observed that the final rounding pro-cedure was crucial, as only 2 out of the 48 testproblems had integral solutions (arguably becauseof the ?repulsive?
nature of the network, as hintedin ?3.4).
For comparison, we also report in the bot-tom row the average runtime of the learned extrac-tive baseline.
We can see that our system?s runtimeis competitive with this baseline.
To our knowl-edge, this is the first time a compressive sum-marizer achieves such a favorable accuracy/speedtradeoff.6 ConclusionsWe presented a multi-task learning framework forcompressive summarization, leveraging data forrelated tasks in a principled manner.
We decodewith AD3, a fast and modular dual decompositionalgorithm which is orders of magnitude faster thanILP-based approaches.
Results show that the stateof the art is improved in automatic and manualmetrics, with speeds close to extractive systems.Our approach is modular and easy to extend.For example, a different compression model couldincorporate rewriting rules to enable compres-sions that go beyond word deletion, as in Cohnand Lapata (2008).
Other aspects may be addedas additional components in our dual decom-position framework, such as query information(Schilder and Kondadadi, 2008), discourse con-203straints (Clarke and Lapata, 2007), or lexical pref-erences (Woodsend and Lapata, 2012).
Our multi-task approach may be used to jointly learn pa-rameters for these aspects; the dual decomposi-tion algorithm ensures that optimization remainstractable even with many components.A Projection Onto KnapsackThis section describes a linear-time algorithm (Al-gorithm 1) for solving the following problem:minimize ?z ?
a?2w.r.t.
zn ?
[0, 1], ?n ?
[N ],s.t.
?Nn=1 Lnzn ?
B, (11)where a ?
RN and Ln ?
0,?n ?
[N ].
This in-cludes as special cases the problems of projectingonto a budget constraint (Ln = 1,?n) and ontothe simplex (same, plus B = 1).Let clip(t) := max{0,min{1, t}}.
Algorithm 1starts by clipping a to the unit interval; if thatyields a z satisfying ?Nn=1 Lnzn ?
B, we aredone.
Otherwise, the solution of Eq.
11 must sat-isfy ?Nn=1 Lnzn = B.
It can be shown from theKKT conditions that the solution is of the formz?n := clip(an+ ?
?Ln) for a constant ??
lying in aparticular interval of split-points (line 11).
To seekthis constant, we use an algorithm due to Pardalosand Kovoor (1990) which iteratively shrinks thisinterval.
The algorithm requires computing medi-ans as a subroutine, which can be done in lineartime (Blum et al, 1973).
The overall complexityin O(N) (Pardalos and Kovoor, 1990).AcknowledgmentsWe thank all reviewers for their insightful com-ments; Trevor Cohn for helpful discussions aboutmulti-task learning; Taylor Berg-Kirkpatrick foranswering questions about their summarizer andfor providing code; and Helena Figueira and PedroMendes for helping with manual evaluation.
Thiswork was partially supported by the EU/FEDERprogramme, QREN/POR Lisboa (Portugal), underthe Discooperio project (contract 2011/18501),and by a FCT grant PTDC/EEI-SII/2312/2012.ReferencesG.
Bak?r, T. Hofmann, B. Scho?lkopf, A. Smola,B.
Taskar, and S. Vishwanathan.
2007.
PredictingStructured Data.
The MIT Press.Algorithm 1 Projection Onto Knapsack.1: input: a := ?an?Nn=1, costs ?Ln?Nn=1, maximum cost B2:3: {Try to clip into unit interval:}4: Set zn ?
clip(an) for n ?
[N ]5: if?Nn=1 Lnzn ?
B then6: Return z and stop.7: end if8:9: {Run Pardalos and Kovoor (1990)?s algorithm:}10: Initialize working set W?
{1, .
.
.
,K}11: Initialize set of split points:P?
{?an/Ln, (1?
an)/Ln}Nn=1 ?
{??
}12: Initialize ?L ?
?
?, ?R ?
?, stight ?
0, ?
?
0.13: while W 6= ?
do14: Compute ?
?
Median(P)15: Set s?
stight + ??
+?n?W Lnclip(an + ?Ln)16: If s ?
B, set ?L ?
?
; if s ?
B, set ?R ?
?17: Reduce set of split points: P?
P ?
[?L, ?R]18: Define the sets:WL := {n ?W | (1?
an)/Ln < ?L}WR := {n ?W | ?
an/Ln > ?R}WM :={n ?W????
?anLn?
?L ?
1?
anLn?
?R}19: Update working set: W?W \ (WL ?WR ?WM)20: Update tight-sum:stight ?
stight+?n?WL Ln(1?an)?
?n?WR Lnan21: Update slack-sum: ?
?
?
+?n?WM L2n22: end while23: Define ??
?
(B ?
?Ni=1 Liai ?
stight)/?24: Set zn ?
clip(an + ?
?Ln), ?n ?
[N ]25: output: z := ?zn?Nn=1.P.
B. Baxendale.
1958.
Machine-made index for tech-nical literature?an experiment.
IBM Journal of Re-search Development, 2(4):354?361.Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.2011.
Jointly learning to extract and compress.
InProc.
of Annual Meeting of the Association for Com-putational Linguistics.Manuel Blum, Robert W Floyd, Vaughan Pratt,Ronald L Rivest, and Robert E Tarjan.
1973.
Timebounds for selection.
Journal of Computer and Sys-tem Sciences, 7(4):448?461.J.
Carbonell and J. Goldstein.
1998.
The use of MMR,diversity-based reranking for reordering documentsand producing summaries.
In SIGIR.Y.-W. Chang and M. Collins.
2011.
Exact decoding ofphrase-based translation models through lagrangianrelaxation.
In Proc.
of Empirical Methods for Natu-ral Language Processing.James Clarke and Mirella Lapata.
2007.
Modellingcompression with discourse constraints.
In Proc.
ofEmpirical Methods in Natural Language Process-ing.J.
Clarke and M. Lapata.
2008.
Global Inference forSentence Compression An Integer Linear Program-204ming Approach.
Journal of Artificial IntelligenceResearch, 31:399?429.T.
Cohn and M. Lapata.
2008.
Sentence compressionbeyond word deletion.
In Proc.
COLING.D.
Das, A. F. T. Martins, and N. A. Smith.
2012.
AnExact Dual Decomposition Algorithm for ShallowSemantic Parsing with Constraints.
In Proc.
of FirstJoint Conference on Lexical and Computational Se-mantics (*SEM).H.
Daume?.
2006.
Practical Structured Learning Tech-niques for Natural Language Processing.
Ph.D. the-sis, University of Southern California.H.
Daume?.
2007.
Frustratingly easy domain adapta-tion.
In Proc.
of Annual Meeting of the Associationfor Computational Linguistics.H.
P. Edmundson.
1969.
New methods in automaticextracting.
Journal of the ACM, 16(2):264?285.T.
Evgeniou and M. Pontil.
2004.
Regularized multi?task learning.
In Proc.
of ACM SIGKDD Inter-national Conference on Knowledge Discovery andData Mining, pages 109?117.
ACM.Elena Filatova and Vasileios Hatzivassiloglou.
2004.A formal model for information selection in multi-sentence text extraction.
In Proc.
of InternationalConference on Computational Linguistics.J.R.
Finkel and C.D.
Manning.
2010.
Hierarchicaljoint learning: Improving joint parsing and namedentity recognition with non-jointly labeled data.
InProc.
of Annual Meeting of the Association for Com-putational Linguistics.J.
Gillenwater, A. Kulesza, and B. Taskar.
2012.
Dis-covering diverse and salient threads in documentcollections.
In Proc.
of Empirical Methods in Natu-ral Language Processing.Dan Gillick, Benoit Favre, and Dilek Hakkani-Tur.2008.
The icsi summarization system at tac 2008.In Proc.
of Text Understanding Conference.K.
Knight and D. Marcu.
2000.
Statistics-basedsummarization?step one: Sentence compression.In AAAI/IAAI.N.
Komodakis, N. Paragios, and G. Tziritas.
2007.MRF optimization via dual decomposition:Message-passing revisited.
In Proc.
of InternationalConference on Computer Vision.J.
Kupiec, J. Pedersen, and F. Chen.
1995.
A trainabledocument summarizer.
In SIGIR.H.
Lin and J. Bilmes.
2010.
Multi-document summa-rization via budgeted maximization of submodularfunctions.
In Proc.
of Annual Meeting of the NorthAmerican chapter of the Association for Computa-tional Linguistics.H.
Lin and J. Bilmes.
2012.
Learning mixtures of sub-modular shells with application to document sum-marization.
In Proc.
of Uncertainty in Artificial In-telligence.C.-Y.
Lin.
2003.
Improving summarization perfor-mance by sentence compression-a pilot study.
In theInt.
Workshop on Inf.
Ret.
with Asian Languages.Chin-Yew Lin.
2004.
Rouge: A package for auto-matic evaluation of summaries.
In Stan SzpakowiczMarie-Francine Moens, editor, Text SummarizationBranches Out: Proceedings of the ACL-04 Work-shop, pages 74?81, Barcelona, Spain, July.H.
P. Luhn.
1958.
The automatic creation of literatureabstracts.
IBM Journal of Research Development,2(2):159?165.A.
F. T. Martins and N. A. Smith.
2009.
Summariza-tion with a Joint Model for Sentence Extraction andCompression.
In North American Chapter of the As-sociation for Computational Linguistics: Workshopon Integer Linear Programming for NLP.A.
F. T. Martins, M. A. T. Figueiredo, P. M. Q. Aguiar,N.
A. Smith, and E. P. Xing.
2011a.
An Aug-mented Lagrangian Approach to Constrained MAPInference.
In Proc.
of International Conference onMachine Learning.A.
F. T. Martins, N. A. Smith, P. M. Q. Aguiar, andM.
A. T. Figueiredo.
2011b.
Dual Decompositionwith Many Overlapping Components.
In Proc.
ofEmpirical Methods for Natural Language Process-ing.Andre F. T. Martins, Mario A. T. Figueiredo, PedroM.
Q. Aguiar, Noah A. Smith, and Eric P. Xing.2012.
Alternating Directions Dual Decomposition.Arxiv preprint arXiv:1212.6550.R.
McDonald.
2006.
Discriminative sentence com-pression with soft syntactic constraints.
In Proc.
ofAnnual Meeting of the European Chapter of the As-sociation for Computational Linguistics.R.
McDonald.
2007.
A study of global inference algo-rithms in multi-document summarization.
In ECIR.A.
Nenkova and R. Passonneau.
2004.
Evaluatingcontent selection in summarization: The pyramidmethod.
In Proceedings of NAACL, pages 145?152.Panos M. Pardalos and Naina Kovoor.
1990.
An al-gorithm for a singly constrained class of quadraticprograms subject to upper and lower bounds.
Math-ematical Programming, 46(1):321?328.D.
R. Radev, H. Jing, and M. Budzikowska.
2000.Centroid-based summarization of multiple docu-ments: sentence extraction, utility-based evaluation,and user studies.
In the NAACL-ANLP Workshop onAutomatic Summarization.205A.M.
Rush and M. Collins.
2012.
A Tutorial on DualDecomposition and Lagrangian Relaxation for In-ference in Natural Language Processing.
Journal ofArtificial Intelligence Research, 45:305?362.A.
Rush, D. Sontag, M. Collins, and T. Jaakkola.
2010.On dual decomposition and linear programming re-laxations for natural language processing.
In Proc.of Empirical Methods for Natural Language Pro-cessing.Frank Schilder and Ravikumar Kondadadi.
2008.
Fast-sum: Fast and accurate query-based multi-documentsummarization.
In Proc.
of Annual Meeting of theAssociation for Computational Linguistics.R.
Sipos, P. Shivaswamy, and T. Joachims.
2012.Large-margin learning of submodular summariza-tion models.B.
Taskar, C. Guestrin, and D. Koller.
2003.
Max-margin Markov networks.
In Proc.
of Neural Infor-mation Processing Systems.B.
Taskar, V. Chatalbashev, and D. Koller.
2004.Learning associative Markov networks.
In Proc.
ofInternational Conference of Machine Learning.I.
Tsochantaridis, T. Hofmann, T. Joachims, and Y. Al-tun.
2004.
Support vector machine learning for in-terdependent and structured output spaces.
In Proc.of International Conference of Machine Learning.K.
Woodsend and M. Lapata.
2010.
Automatic gener-ation of story highlights.
In Proc.
of Annual Meet-ing of the Association for Computational Linguis-tics, pages 565?574.Kristian Woodsend and Mirella Lapata.
2011.
Learn-ing to simplify sentences with quasi-synchronousgrammar and integer programming.
In Proc.
of Em-pirical Methods in Natural Language Processing.Kristian Woodsend and Mirella Lapata.
2012.
Mul-tiple aspect summarization using integer linear pro-gramming.
In Proc.
of Empirical Methods in Natu-ral Language Processing.Wen-tau Yih, Joshua Goodman, Lucy Vanderwende,and Hisami Suzuki.
2007.
Multi-document summa-rization by maximizing informative content-words.In Proc.
of International Joint Conference on Artifi-cal Intelligence.D.
Zajic, B. Dorr, J. Lin, and R. Schwartz.
2006.Sentence compression as a component of a multi-document summarization system.
In the ACL DUCWorkshop.206
