Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1561?1570,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPChinese Novelty MiningYi ZhangNanyang Technological University50 Nanyang AvenueSingapore 639798yizhang@ntu.edu.sgFlora S. TsaiNanyang Technological University50 Nanyang AvenueSingapore 639798fst1@columbia.eduAbstractAutomated mining of novel documentsor sentences from chronologically ordereddocuments or sentences is an open chal-lenge in text mining.
In this paper, wedescribe the preprocessing techniques fordetecting novel Chinese text and discussthe influence of different Part of Speech(POS) filtering rules on the detection per-formance.
Experimental results on AP-WSJ and TREC 2004 Novelty Track datashow that the Chinese novelty mining per-formance is quite different when choosingtwo dissimilar POS filtering rules.
Thus,the selection of words to represent Chinesetext is of vital importance to the success ofthe Chinese novelty mining.
Moreover, wecompare the Chinese novelty mining per-formance with that of English and investi-gate the impact of preprocessing steps ondetecting novel Chinese text, which willbe very helpful for developing a Chinesenovelty mining system.1 IntroductionThe bloom of information nowadays brings us richuseful information as well as tons of redundant in-formation in news articles, social networks (Tsai etal., 2009), and blogs (Chen et al, 2008).
Noveltymining (NM), or novelty detection, aims at miningnovel information from a chronologically orderedlist of relevant documents/sentences.
It can facil-itate users to quickly get useful information with-out going through a lot of redundant information,which is usually a tedious and time-consumingtask.The process of detecting novel text containsthree main steps, (i) preprocessing, (ii) cate-gorization, and (iii) novelty mining.
The firststep preprocesses the text documents/sentencesby removing stop words, performing word stem-ming, implementing POS tagging etc.
Categoriza-tion classifies each incoming document/sentenceinto its relevant topic bin.
Then, within eachtopic bin containing a group of relevant docu-ments/sentences, novelty mining searches throughthe time sequence of documents/sentences and re-trieves only those with ?novel?
information.
Thispaper focuses on applying document/sentence-level novelty mining on Chinese.
In this task,we need to identify all novel Chinese text givengroups of relevant documents/sentences.Novelty mining has been performed at three dif-ferent levels: event level, sentence level and doc-ument level (Li and Croft, 2005).
Works on nov-elty mining at the event level originated from re-search on Topic Detection and Tracking (TDT),which is concerned with online new event detec-tion/first story detection (Allan et al, 1998; Yanget al, 2002; Stokes and Carthy, 2001; Franz etal., 2001; Brants et al, 2003).
Research on doc-ument and sentence-level novelty mining aims tofind relevant and novel documents/sentences givena stream of documents/sentences.
Previous stud-ies on document and sentence-level novelty min-ing tend to apply some promising content-orientedtechniques (Li and Croft, 2005; Allan et al, 1998;Yang et al, 1998; Zhang and Tsai, 2009).
Simi-larity metrics that can be used for detecting noveltext are word overlap, cosine similarity (Yang etal., 1998), new word count (Brants et al, 2003),etc.
Other works utilize ontological knowledge,especially taxonomy, such as WordNet (Zhang etal., 2002; Allan et al, 2003), synonym dictionary(Franz et al, 2001), HowNet (Eichmann and Srini-vasan, 2002), etc.Previous studies for novelty mining have beenconducted on the English and Malay languages(Kwee et al, 2009; Tang et al, 2009; Tang andTsai, 2009).
Novelty mining studies on the Chi-nese language have been performed on topic de-1561tection and tracking, which identifies and collectsrelevant stories on certain topics from informationstream (Zheng et al, 2008; Hong et al, 2008).Also many works have discussed the issues, suchas word segmentation, POS tagging etc, betweenEnglish and Chinese (Wang et al, 2006; Wu etal., 2003).
However, to the best of our knowledge,no studies have been reported on discussing pre-processing techniques on Chinese document andsentence-level novelty mining, which is the focusof our paper.The rest of this paper is organized as follows.Section 2 gives a brief overview of related work ondetecting novel documents and sentences on En-glish and Chinese.
Section 3 introduces the detailsof preprocessing steps for English and Chinese.A general novelty mining algorithm is describedin Section 4.
Section 5 reports experimental re-sults.
Section 6 summarizes the research findingsand discusses issues for further research.2 Related WorkIn the pioneering work for detecting novel doc-uments (Zhang et al, 2002), document noveltywas predicted based on the distance between thenew document and the previously delivered doc-uments in history.
The detected document whichis very similar to any of its history documents isregarded as a redundant document.
To serve usersbetter, it could be more helpful to further highlightnovel information at the sentence level.
Therefore,later studies focused on detecting novel sentences,such as those reported in TREC 2002-2004 Nov-elty Tracks (Harman, 2002; Soboroff and Harman,2003; Soboroff, 2004), which compared variousnovelty metrics (Allan et al, 2003), and integrateddifferent natural language techniques (Ng et al,2007; Li and Croft, 2008).Although novelty mining studies have mainlybeen conducted on the English language, stud-ies on the Chinese language have been performedon topic detection and tracking.
A prior study(Zheng et al, 2008) proposed an improved rel-evance model to detect the novelty informationin topic tracking feedback and modified the topicmodel based on this information.
Experimentalresults on Chinese datasets TDT4 and TDT2003proved the effectiveness in topic tracking.
Anotherstudy proposed a method of applying semantic do-main language model to link detection, based onthe structure relation among contents and the se-mantic distribution in a story (Hong et al, 2008).3 Preprocessing for English and Chinese3.1 EnglishSince the focus of this paper is on novelty min-ing, we begin from a list of relevant documents orsentences that have already undergone the catego-rization process.The first step for English preprocessing is to re-move all stop words from documents or sentences,such as conjunctions, prepositions, and articles.Stop words are words that are too common tobe informative.
These words should be removed,otherwise it will influence the novelty predictionof documents or sentences.
After stop words re-moval, the remaining words are then stemmed.The inflected (or sometimes derived) words arereduced to their root forms.
This paper usedPorter stemming algorithm (Porter, 1997) for En-glish word stemming.
This algorithm removes thecommoner morphological and inflexional endingsfrom the words in English.
The entire preprocess-ing steps in English novelty mining can be seen inFigure 1.3.2 ChineseIn Chinese, the word is the smallest independentmeaningful element.
There is no obvious bound-ary between words so that Chinese lexical anal-ysis, such as Chinese word segmentation, is theprerequisite for novelty mining.Unlike English, Chinese word segmentationis a very challenging problem because of thedifficulties in defining what constitutes a word(Gao et al, 2005).
While each criteria pro-vides valuable insights into ?word-hood?
in Chi-nese, they do not consistently lead us to thesame conclusions.
Moreover, there is no whitespace between Chinese words or expressionsand there are many ambiguities in the Chineselanguage, such as: ????????
(means?mainboard and server?
in English) might be ???/?/????
(means ?mainboard/and/server?
inEnglish) or ???/??/?/??
(means ?main-board/kimono/task/utensil?
in English).
This am-biguity is a great challenge for Chinese word seg-mentation.
In addition, there is no obvious in-flected or derived words in Chinese so that wordstemming is not applicable.Therefore, in order to reduce the noise broughtby Chinese word segmentation and get a better1562Remove StopWordsStem WordsNovelty MiningRelevant documents /sentencesNovel documents /sentencesPreprocessingstepsFigure 1: Preprocessing steps on English.word list for one document or sentence, we firstlyapply word segmentation on the Chinese text andthen utilize Part-of-Speech (POS) tagging to se-lect the meaningful candidate words.
Figure 2shows the preprocessing steps on the Chinese textfor novelty mining.
POS tagging is a process ofmarking up the word in a text as correspondingto a particular part of speech.
It is learnt that theidea of a text mainly relies on some meaningfulwords, such as nouns and verbs, so that we can getthe main content by extracting these meaningfulwords.
Moreover, it will decrease the impact ofthe errors in Chinese word segmentation on nov-elty mining because only meaningful words areconsidered and other words (including stop words)such as ????
(means ?although?
in English) willnot appear in the word list for the following sim-ilarity computation in novelty mining.
Losee alsomentioned that POS tagging shows a great poten-tial to avoid lexical ambiguity and it can help toimprove the performance of information retrieval(Losee, 2001).ICTCLAS is used when performing word seg-mentation and POS tagging in our experiments(ICTCLAS, 2008).
It is an open source projectand achieves a better precision in Chinese wordsegmentation and POS tagging than other Chi-nese POS tagging softwares (ICTCLAS, 2008).First, we apply word segmentation on the relevantChinese documents/sentences.
Chinese word seg-mentation includes atom segmentation, N-shortestpath based rough segmentation and unknownwords recognition (see Figure 3).
Atom segmen-tation is an initial step of the Chinese languagesegmentation process, where atom is defined tobe the minimal unit that cannot be split further.The atom can be a Chinese character, punctua-tion, symbol string, etc.
Then, rough segmentationtries to discover the correct segmentation with asfew candidates as possible.
The N-Shortest Path(NSP) method (Zhang and Liu, 2002) is appliedfor rough segmentation.
Next, we detect some un-known words such as person name, location nameso as to optimize the segmentation result.
Finally,we POS tag the words and keep some kinds ofwords in the word list according to the selectiverule, which are used in novelty mining.4 Novelty MiningFrom the output of preprocessing, we can obtain abag of words.
The corresponding term-documentmatrix (TDM)/term-sentence matrix (TSM) can beconstructed by counting the term frequency (TF)of each word.
The novelty mining system predictsany incoming document/sentence by comparing itwith its history documents/sentences in this vectorspace.
Therefore, given a Chinese TDM/TSM, thenovelty mining system designed for English canalso be applied to Chinese.In novelty mining, the novelty of a docu-ment/sentence can be quantitatively measured by anovelty metric and represented by a novelty score.The most popular novelty metric, i.e.
cosine sim-ilarity (see (Allan et al, 2003)), is adopted.
Thismetric first calculates the similarities between thecurrent document/sentence dtand each of its his-1563WordSegmentationPOS TaggingNovelty MiningRelevant documents /sentencesNovel documents /sentencesPreprocessingstepsFigure 2: Preprocessing steps on Chinese.tory documents/sentences di(1 ?
i ?
t ?
1).Then, the novelty score is simply one minus themaximum of these cosine similarities, as shown inEq.
(1).Novelty Score(dt) = 1?
max1?i?t?1cos(dt, di) (1)cos(dt, di) =?nk=1wk(dt) ?
wk(di)?dt?
?
?di?where Ncos(d) denotes the cosine similarity scoreof the document/sentence d and wk(d) is theweight of kthelement in the document/sentenceweighted vector d. The term weighting functionused in our work is TF(term frequency).The final decision on whether a docu-ment/sentence is novel or not depends on whetherthe novelty score falls above or below a thresh-old.
The document/sentence predicted as ?novel?will be placed into the list of history docu-ments/sentences.5 Experiments and Results5.1 DatasetsTwo public datasets APWSJ (Zhang et al, 2002)and TREC Novelty Track 2004 (Soboroff, 2004)are selected as our experimental datasets for thedocument-level and the sentence-level noveltymining respectively.
APWSJ data consists ofnews articles from Associated Press (AP) and WallStreet Journal (WSJ).
There are 50 topics fromQ101 to Q150 in APWSJ and 5 topics (Q131,Q142, Q145, Q147, Q150) are excluded from theTable 1: Statistics of experimental dataDataset Novel Non-novelAPWSJ 10839(91.10%) 1057(8.90%)TREC2004 3454(41.40%) 4889(58.60%)experiments because they lack non-novel docu-ments (Zhao et al, 2006).
The assessors providetwo degrees of judgements on non-novel docu-ments, absolute redundant and somewhat redun-dant.
In our experiments, we adopt the strict defi-nition used in (Zhang et al, 2002) where only ab-solute redundant documents are regarded as non-novel.
TREC 2004 Novelty Track data is devel-oped from AQUAINT collection.
Both relevantand novel sentences are selected by TREC?s asses-sors.
The statistics of these two datasets are sum-marized in Table 1.5.2 Evaluation MeasuresFrom many previous works, redundancy precision(RP ), redundancy recall (RR) and redundancy FScore (RF ) are used to evaluate the performanceof document-level novelty mining (Zhang et al,2002).
Precision (P ), recall (R) and F Score (F )are mainly used in evaluating the performance forsentence-level novelty mining (Allan et al, 2003).Therefore, we use RP , RR, RF and redundancyprecision-recall (R-PR) curve to evaluate our ex-perimental results on the document level.
P , R, Fand precision-recall (PR) curve are used to eval-uate the performance on the sentence-level nov-elty mining.
The larger the area under the R-PR1564Document/ Sentence StringAtomSegmentationNSP-basedRough SegmentationUnknownWord RecognitionPOS TaggingAtom SequenceTop N SequenceRevised N ResultPOS SequenceNovelty MiningFigure 3: Word segmentation on Chinese.curve/PR curve, the better the algorithm.
Alsowe drew the standard redundancy F Score/F Scorecontours (Soboroff, 2004), which indicate the FScore values when setting precision and recallfrom 0 to 1 with a step of 0.1.
These contours canfacilitate us to compare redundancy F Scores/FScores in R-PR curves/PR curves.
Redundancyprecision, redundancy recall, precision and recallon a certain topic are defined as:Redundancy Precision =R?R?+N?
(2)Redundancy Recall =R?R?+R+(3)Precision =N+N++R+(4)Recall =N+N++N?
(5)where R+,R?,N+,N?correspond to the numberof documents/sentences that fall into each cate-gory (see Table 2).Based on all the topics?
RP /P and RR/R, wecould get the average RP /P and average RR/R bycalculating the arithmetic mean of these scores onall topics.
Then, the average redundancy F Score(RF )/F Score (F ) is obtained by the harmonic av-erage of the average RP /P and average RR/R.Table 2: Categories for evaluationNon-novel NovelDelivered R+N+Not Delivered R?N?5.3 Experimental ResultsIn this experimental study, the focus was noveltymining rather than relevant documents/sentencescategorization.
Therefore, our experiments startedwith all given relevant Chinese text, from whichthe novel text should be identified.Since the datasets that we used for document-level novelty mining and sentence-level noveltymining both were written in English, we first trans-lated them into Chinese.
During this process,we investigated issues on machine translation vs.manually corrected translation.We compared the novelty mining performanceon 107 text in TREC 2004 Novelty Track betweenautomatically translated using Google TranslateAPI1and the manually corrected translation.
Forexample, here is an English sentence in Topic 51:According to a Chilean government report, atotal of 4,299 political opponents died or disap-peared during Pinochet?s term.After machine translation using Google Trans-lator, the above sentence is translated as:????????????4299??????????????????
?1http://code.google.com/p/google-api-translate-java1565Then we manually corrected the machine trans-lation and obtained the corrected translation:???????????????????????4299???????
?After novelty mining on the machine transla-tion sentences and the humanly corrected transla-tion sentences individually, we found that there isa slight difference (<2%) in precision and F Score.Thus, we used machine translation to translate theremaining documents/sentences to Chinese.
Thisindicates that the noise in machine translation forChinese had little impact on our actual results.Then on English text, we applied the prepro-cessing steps discussed in Section 3.1, includ-ing stop word removing and word stemming.For Chinese datasets, we segmented the docu-ments/sentences into words and then performedPOS filtering to acquire the candidate words forthe space vector.Based on the vectors of Chinese text, wecalculated the similarities between docu-ments/sentences and predicted the noveltyfor each document/sentence in the Chinese andEnglish datasets.
An incoming Chinese/Englishdocument will be compared with all the systemdelivered 10 novel documents.
If the novelty scoreis above the novelty score threshold, the documentis considered to be novel.
Thresholds used werebetween 0.05 and 0.65.
We also performedChinese/English sentence-level novelty mining.Whether an incoming Chinese/English sentenceis novel is predicted by comparing with the mostrecent system-delivered 1000 novel sentences.Thresholds adopted were between 0.05 and 0.95with an equal step of 0.10.
Then, we evaluated theChinese/English novel text detection performanceby setting a series of novelty score thresholds.5.3.1 POS Filtering RuleWe adopted two different rules to select the can-didate words to represent one document/sentenceand investigated the POS filtering influence on de-tecting the novel Chinese text.?
Rule1: only some non-meaningful words,including pronouns (?r?
in Peking Univer-sity/Chinese Academy of Sciences ChinesePOS tagging criterions (PKU and CAS,1999)), auxiliary words (?u?
), tone words(?y?
), conjunctions (?c?
), prepositions (?p?
)and punctuation words (?w?)
are removed.?
Rule2: fewer kinds of words are selected torepresent a document/sentence.
Only nouns(including ?n?
short for common nouns, ?nr?short for person name, ?ns?
short for locationname, ?nt?
short for organization name, ?nz?short for other proper nouns), verbs (?v?
), ad-jectives (?a?)
and adverbs (?d?)
are kept.For example, here is a simple Chinese sen-tence: ??????????
(There is a pictureon the wall).
After POS filtering using Rule1,the words we keep are: ??(?n?),?(?v?),?(?v?),?(?m?
), ?(?q?
), ?(?n?)?.
After POS filteringusing Rule2, the remaining words are: ??(?n?),?(?v?
), ?(?v?
), ?(?n?)?.
It is noticed that byusing Rule2, we can remove more non-importantwords.Figure 4 and Figure 5 show the performanceson the document and sentence-level novelty min-ing when choosing the stricter rule (Rule2) andthe less strict rule (Rule1) in POS filtering.
Thegrey dashed lines show contours at intervals of 0.1points of F Score.From Figure 4 and Figure 5, we learn that theChinese novelty mining performance varies whenchoosing the stricter rule (Rule2) and the less strictrule (Rule1) in POS filtering.
We can obtain abetter performance when choosing a stricter rule(Rule2).
Therefore, it is necessary to perform POSfiltering in the preprocessing steps on Chinese andjust removing some non-meaningful words (likestop words) may not be enough.
POS filteringcan help to remove the less meaningful words sothat each vector is represented better.
Compared tochoosing more kinds of words (Rule1), only keep-ing nouns, verbs, adjectives and adverbs (Rule2)will be a better choice for novelty mining.
Wealso noticed that the selection of words to repre-sent Chinese text is of vital importance to the suc-cess of Chinese novelty mining.5.3.2 Comparison with EnglishWe compared the novelty mining performanceon the English and Chinese documents/sentencesdatasets.
For Chinese, we chose Rule2 to selectthe candidate words.
Figure 6 and Figure 7 showthe R-PR and PR curves of document/sentence-level novelty mining in English and Chinese whengiven a series of novelty score thresholds.From Figure 6 and Figure 7, we observe thatthe performance on detecting novel Chinese docu-ments is slightly lower than that on English.
Thismay be due to the different linguistical characteris-15660 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 100.10.20.30.40.50.60.70.80.91Redundancy?RecallRedundancy?PrecisionDocument?Level Novelty Mining on APWSJ0.90.80.70.60.50.40.30.2RF score0.1Chinese D_NM_Rule2Chinese D_NM_Rule10.250.350.45 0.550.150.250.2 0.350.45 0.550.2Threshold=0.15Threshold=0.05Figure 4: R-PR curves for document-level novelty mining on Chinese when choosing different rules onAPWSJ.
The grey dashed lines show contours at intervals of 0.1 points of RF .0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.450.50.550.60.650.7RecallPrecisionSentence?Level Novelty Mining on TREC 2004F score0.80.70.60.5 0.4 0.3 0.2Chinese S_NM_Rule2Chinese S_NM_Rule10.650.55Threshold=0.95Threshold=0.950.850.850.750.75 0.650.55Figure 5: PR curves for sentence-level novelty mining on Chinese when choosing different rules onTREC 2004.
The grey dashed lines show contours at intervals of 0.1 points of F .15670 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 100.10.20.30.40.50.60.70.80.91Redundancy?RecallRedundancy?PrecisionRF score0.90.80.70.60.50.40.30.20.1Document?Level Novelty Mining on APWSJChinese D_NM_Rule2English D_NMThreshold=0.05 0.150.250.350.45 0.650.55Figure 6: R-PR curves for document-level novelty mining on Chinese and English on APWSJ.
The greydashed lines show contours at intervals of 0.1 points of RF .0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 100.10.20.30.40.50.60.70.80.91RecallPrecisionF score0.90.80.70.60.50.40.30.20.1Sentence?Level Novelty Mining on TREC 2004Chinese S_NM_Rule2E gli h S_NMThreshold=0.95 0.85 0.750.550.65Figure 7: PR curves for sentence-level novelty mining on Chinese and English on TREC 2004.
The greydashed lines show contours at intervals of 0.1 points of F .1568tics of each language so that the preprocessing in-fluence on each language?s novelty mining is dis-similar.
Furthermore, the Chinese preprocessingquality is not as good as that on English so that itis difficult to obtain a good ?bag of words?
froma document.
Moreover, the errors in word seg-mentation will influence the result of POS tagging.These issues make tokenizing and POS taggingextremely difficult for the Chinese text.However, the performance of Chinese sentence-level novelty mining is almost the same as that onEnglish.
The reason is that the novelty mining per-formance at the sentence level is not so sensitiveto the preprocessing steps as that at the documentlevel.
If the similarity computation is based on thesentence level, the word segmentation and POStagging errors actually will not have a big influ-ence on the result as that on documents.6 ConclusionThis paper studied the preprocessing issues onmining novel Chinese text, which, to the bestof our knowledge, have not been sufficientlyaddressed in previous studies.
We describedthe Chinese preprocessing steps and discussedthe influence when choosing different Part-of-Speech (POS) filtering rules.
Then we appliednovelty mining on Chinese and English docu-ments/sentences and compared their performance.The experimental results on APWSJ and TREC2004 Novelty Track showed that after adoptinga stricter POS filtering rule, the Chinese nov-elty mining performed better on both documentsand sentences.
This is because non-meaningfulwords have a negative influence on detecting noveltext.
However, compared to English, Chinese per-formed worse on the document level and similarlyon the sentence level.
The reason may be due tothe lower sensitivity of preprocessing at the sen-tence level.
The main contributions of this workare as follows:1) We investigated the preprocessing techniquesfor detecting novel Chinese text on both doc-ument and sentence level.2) The POS filtering rule, telling how to selectwords to represent one document/sentence,was discussed.3) Several experiments were conducted to com-pare the novelty mining performance be-tween Chinese and English.
The noveltymining performance on Chinese can be im-proved as good as that on English if we canincrease the preprocessing precision on Chi-nese text.Our findings will be very helpful for develop-ing a real-time Chinese novelty mining system atboth the document and sentence level.
In futurework, we will try other word combinations and in-vestigate better ways to represent the Chinese text.In addition, we will explore how to utilize the bet-ter Chinese sentence-level novelty mining result toimprove the detection performance on documents.1569ReferencesJames Allan, Ron Papka, and Victor Lavrenko.
1998.
On-line new event detection and tracking.
In SIGIR 1998,Melbourne, Australia, pages 37?45.James Allan, Courtney Wade, and Alvaro Bolivar.
2003.
Re-trieval and novelty detection at the sentence level.
In SI-GIR 2003, Toronto, Canada, pages 314?321.
ACM, Au-gust.Thorsten Brants, Francine Chen, and Ayman Farahat.
2003.A system for new event detection.
In SIGIR 2003,Toronto, Canada, pages 330?337.Yun Chen, Flora S. Tsai, and Kap Luk Chan.
2008.
Machinelearning techniques for business blog search and mining.Expert Syst.
Appl., 35(3):581?590.D.
Eichmann and P. Srinivasan.
2002.
Novel results andsome answers.
In TREC 2002 - the 11th Text REtrievalConference.Martin Franz, Abraham Ittycheriah, J.Scott McCarley, andTodd Ward.
2001.
First story detection: combining simi-larity and novelty based approach.
In Topic Detection andTracking Workshop.Jianfeng Gao, Mu Li, Andi Wu, and Chang-Ning Huang.2005.
Chinese word segmentation and named entityrecognition: A pragmatic approach.
Computational Lin-guistics, 31(4):531?574, December.D.
Harman.
2002.
Overview of the TREC 2002 NoveltyTrack.
In TREC 2002 - the 11th Text Retrieval Confer-ence, pages 46?55.Yu Hong, Yu Zhang, Jili Fan, Ting Liu, and Sheng Li.
2008.Chinese topic link detection based on semantic domainlanguage model.
Journal of Software, 19(9):2265?2275.ICTCLAS.
2008. http://ictclas.org/index.html.Agus Trisnajaya Kwee, Flora S Tsai, and Wenyin Tang.2009.
Sentence-level novelty detection in English andMalay.
In Lecture Notes in Computer Science (LNCS),volume 5476, pages 40?51.Xiaoyong Li and W. Bruce Croft.
2005.
Novelty detectionbased on sentence level patterns.
In CIKM 2005, pages744?751.Xiaoyong Li and W. Bruce Croft.
2008.
An information-pattern-based approach to novelty detection.
InformationProcessing and Management: an International Journal,44(3):1159?1188, May.Robert M. Losee.
2001.
Natural language processing in sup-port of decision making: Phrases and part-of-speech tag-ging.
Information Processing and Management: an Inter-national Journal, 37(6).Kok Wah Ng, Flora S. Tsai, Kiat Chong Goh, and Lihui Chen.2007.
Novelty detection for text documents using namedentity recognition.
In Information, Communications andSignal Processing, 2007 6th International Conference on,pages 1?5, December.PKU and CAS.
1999.
Chinese POS tagging criterion.http://icl.pku.edu.cn/icl groups/corpus/addition.htm.M.F.
Porter.
1997.
An algorithm for suffix stripping.
Read-ings in information retrieval, pages 313?316.Ian Soboroff and D. Harman.
2003.
Overview of the TREC2003 Novelty Track.
In TREC 2003 - the 12th Text Re-trieval Conference.Ian Soboroff.
2004.
Overview of the TREC 2004 NoveltyTrack.
In TREC 2004 - the 13th Text Retrieval Confer-ence.N.
Stokes and J. Carthy.
2001.
First story detection using acomposite document representation.
In HLT 2001, pages134?141.Wenyin Tang and Flora S Tsai.
2009.
Threshold setting andperformance monitoring for novel text mining.
In SIAMInternational Conference on Data Mining Workshop onText Mining.Wenyin Tang, Agus Trisnajaya Kwee, and Flora S Tsai.2009.
Accessing contextual information for interactivenovelty detection.
In European Conference on Informa-tion Retrieval (ECIR) Workshop on Contextual Informa-tion Access, Seeking and Retrieval Evaluation.Flora S. Tsai, Wenchou Han, Junwei Xu, and Hock ChuanChua.
2009.
Design and development of a mobile peer-to-peer social networking application.
Expert Syst.
Appl.,36(8):11077 ?
11087.Mengqiu Wang, Kenji Sagae, and Teruko Mitamura.
2006.A fast, accurate deterministic parser for Chinese.
In ACL2006, Sydney, Australia, pages 425 ?
432.Youzheng Wu, Jun Zhao, and Bo Xu.
2003.
Chinese namedentity recognition combining a statistical model with hu-man knowledge.
In ACL 2003 workshop on Multilingualand mixed-language named entity recognition, pages 65?72.Yiming Yang, Tom Pierce, and Jaime Carbonell.
1998.
Astudy on retrospective and on-line event detection.
pages28?36.
ACM Press.Yiming Yang, Jian Zhang, Jaime Carbonell, and Chun Jin.2002.
Topic-conditioned novelty detection.
In SIGKDD2002, pages 688 ?
693.Huaping Zhang and Qun Liu.
2002.
Model of Chinese wordsrough segmentation based on n-shortest paths method.Journal of Chinese Information Processing, 15:1?7.Yi Zhang and Flora S. Tsai.
2009.
Combining named enti-ties and tags for novel sentence detection.
In ESAIR ?09:Proceedings of the WSDM ?09 Workshop on ExploitingSemantic Annotations in Information Retrieval, pages 30?34.Yi Zhang, Jamie Callan, and Thomas Minka.
2002.
Noveltyand redundancy detection in adaptive filtering.
In ACMSIGIR 2002, Tampere, Finland, pages 81?88.Le Zhao, Min Zheng, and Shaoping Ma.
2006.
The nature ofnovelty detection.
Information Retrieval, 9:527?541.Wei Zheng, Yu Zhang, Bowei Zou, Yu Hong, and Ting Liu.2008.
Research of Chinese topic tracking based on rele-vance model.1570
