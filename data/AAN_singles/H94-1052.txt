Decision Tree Parsing using a Hidden Derivation ModelE Jelinek,* J. Lafferty, D. Magerman, R. Mercer*, A. Ratnaparkhi, S. RoukosIBM Research Div is ionThomas  J. Watson Research CenterYork town Heights,  NY  105981.
IntroductionParser development is generally viewed as a primarily linguis-tic enterprise.
A grammarian examines entences, killfullyextracts the linguistic generalizations evident in the data, andwrites grammar rules which cover the language.
The gram-marian then evaluates the performance of the grammar, andupon analysis of the errors made by the grammar-based parser,carefully refines the rules, repeating this process, typicallyover a period of several years.This grammar refinement process is extremely time-consuming and difficult, and has not yet resulted in a grammarwhich can be used by a parser to analyze accurately a largecorpus of unrestricted text.
As an alternative to writing gram-mars, one can develop corpora of hand-analyzed sentences(treebanks) with significantly less effort 1.
With the avail-ability of treebanks of annotated sentences, one can view NLparsing as simply treebank recognition where the methodsfrom statistical pattern recognition can be brought to bear.This approach divides the parsing problem into two separatetasks: treebanking, defining the annotation scheme which willencode the linguistic ontent of the sentences and applyingit to a corpus, and treebank recognition, generating theseannotations automatically for new sentences.The treebank can contain whatever information is deemedvaluable by the treebanker, as long as it is annotated accordingto some consistent scheme, probably one which representsthe intended meaning of the sentence.
The goal of treebankrecognition isto produce the exact same analysis of a sentencethat the treebanker would generate.As treebanks became available during the past five years,many "statistical models" for parsing a sentence w~ of nwords still relied on a grammar.
Statistics were used to sim-ply rank the parses that a grammar allowed for a sentence.Unfortunately, this requires the effort of grammar creation(whether by hand or from data) in addition to the Treebankand suffers from the coverage problem that the correct parse*E Jelinek and R. Mercer, formerly of IBM, are now will, John HopkinsUniversity and Renaissance T chnologies, Inc., respectively.1 In addition, these annotated corpora have a more permanent value forfuture research use than particular grammarsmay not be allowed by the grammar.
Parsing with these mod-els is to determine the most probable parse, T*, from amongall the parses, denoted by Ta(w~), allowed by the grammarG for the sentence w~:T* = argmax p(T \[w~).
(1)T6To(w~)The a posteriori probability of a tree T given the sentencew?
is usually derived by Bayes rule from a generative model,denoted by p(T, w~), based on the grammar.
For example,probabilistic CFGs (P-CFG) can be estimated from a treebankto construct such a model \[I, 2\].But there is no reason to require that a grammar be used toconstruct a probabilistic model p(T \[ w~) that can be used forparsing.
In this paper, we present amethod for contructing amodel for the conditional distribution of trees given a sentencewithout the need to define a grammar.
So with this newviewpoint parsing avoids the step of extracting a grammarand is merely the search of the most probable tree:T* = arg maxp(Tlw~) (2)T6T(w~)where the maximization is over all trees that span the n-word sentence.
While others have attempted to build parsersfrom treebanks using correctly tagged sentences as input, wepresent in this paper the first results we know of in buildinga parser automatically that produces the surface structure di-rectly from a word sequence and does not require a correctsequence of tags.The probabilistic models we explore are conditional on thederivational order of the parse tree.
In \[4\], this type of modelis referred to as a history-based grammar model where a (de-terministic) leftmost derivation order is used to factor theprobabilistic model.
In this work, we use a set of bottom-up derivations 2 of parse trees.
We explore the use of a self-organized hidden derivational model as well as a deterministicderivational model to assign the probability of a parse tree.In the remaining sections, we discuss the derivation historymodel, the parsing model, the probabilistic models for node2Traditional use of derivation order identifies the order of application ofgrammar ules; in this work, we extend the notion to identify the order inwhich edges in a tree are created.272Iwcl I""'?IA, I ?lwol ?..
,I I I Figure 1: The extensions corresponding to a constituent for a phrase such as "the Enter key".features, the training algorithms, the experimental results, andour colaclusions.Me Enter keyFigure2: Representation f constituent and labeling of exten-sions.2.
A der ivat ion  history modelCurrent reebanks are a collection of n-ary branching trees,with each node in a tree labeled by either a non-terminal labelor a part-of-speech label (called a tag).
Usually, grammarianselevate constituents to the status of elementary units in a parse,especially in the case of rewrite-rule grammars where eachrewrite rule defines a legal constituent.
However, if a parsetree is interpreted as a geometric pattern, a constituent is nomore than a set of edges which meet at the same tree node.
InFigure 1, the noun phrase,"N", which spans the tags "AT VVCNN 1", which correspond to an article, a command verb, anda singular noun, respectively, consists of an edge extendingto the right from "AT," an edge extending straight up from"VVC," and an edge extending to the left from "NNI" (seeFigure 1).We introduce a new definition for a derivation of a parse treeby using Figure 2 which gives a subtree used in our parser forrepresenting the noun phrase "the Enter key".
We associatewith every node in the parse tree two features, aname whichis either a tag or a non-terminal label, and an extension whichindicates whether the edge going to its parent is going right,left, up, or unary.
Unary corresponds toa renaming of a non-terminal.
By specifying the two features (name and extension)for each node we can reconstruct the parse tree.
The orderof the nodes in which we specify these two features definesthe derivation order.
We only consider bottom-up derivations.In a bottom-up derivation, a node is named first, it may beextended only after it's named, and it is not named until allof the nodes beneath it are extended.
Naming a node maybea tagging or labeling action depending on whether or not thenode is a leaf in the parse tree.Using Figure 2, one derivation is to tag the first word "the"as "AT", then to extend it "right", then to tag the third word"key" as "NNI", then to tag the second word "Enter" as"VVC" (command verb), then to extend the resulting node bya "unary", then to label the resulting node as "Nn" (computernoun), then to extend the resulting node "up", then to extendthe "NNi" node by a "left" to yield a node that spans thewhole phrase "the Enter key".
By our definition of bottom-upderivation, it's only at this point in the derivation that we canlabel the node that spans the whole phrase as "N", and thenextend it "left" as is implied in Figure 2.
Using the nodenumbering scheme in Figure 2, we have at the beginning ofthis derivation the words with the nodes {2, 4, 5} that haveunassigned names.
These are the active nodes at this point.Suppose that node 2 is picked and then tagged "AT".
Thatcorresponds to the derivation \[2\]; at this point, only nodes{2, 4, 5} are active.
If we pick node 2 again, then an extensionstep is required and the derivation is \[22\].
The derivationpresented at the beginning of this paragraph corresponds tothe sequence of nodes \ [2254433511\ ] .To derive the tree in Figure I when we are given the three-tagsequence, there are 6 possible derivations.
We could start byextending any of the 3 tags, then we have either of 2 choicesto extend, and we extend the one remaining choice, then wename the resulting node.
This leads to 3x2xl=6 derivationsfor that tree.If we use a window of 1, then only a single derivation is per-mitted and we call it the bottom-up leftmost derivation.
In ourexample, this leftmost derivation would be \ [224433551\ ] .2733.
The Parsing ModelWe represent a derivation of a parse tree by the sequence ofnodes as they are visited by the derivation, denoted by d.Denote by ~ the i-th node of the derivation d. Denote byld, the nanm feature for a node selected at the i-th step inthe derivation and by ed~ its extension.
A parse derivation isconstructed' by the following 2-step algorithm:?
select which node to extend among active nodes usingp( active = di \[context),?
then either- assign a name to the selected node whether it istagging or labelling anode (constituent) with a non-terminal label using p(la, \[ context), or- extend the selected node (which adds an edge tothe parse graph) using p(ed, \[ contezt).If the node selected has its name identified then an extensionstep is performed otherwise a naming step is performed.
Notethat only extension steps change which nodes are active.We have a different probabilistic model for each type of stepin a parse derivation.
The probabilistic models do not usethe whole derivation history as context; but rather a five nodewindow around the node in question.
We will discuss this inmore detail ater on.The probability of a derivation of a parse tree is the productof the probabilities of each of the feature value assignmentsin that derivation and the probability of each active nodeselection made in that derivation:p(T, dlw ) = IXX<j<Idlwh~e= p(active = dj I conte t(di-1))p(wj I ont  t(dl))where xj is either the name lj of node dj or its extension ejand d~ is the derivation up to thej-th step.
The probability ofa parse tree given the sentence is the sum over all derivationsof that parse tree:p(T I w~) = ~p(T ,  d l w~)dDue to computational complexity, we restrict he number ofbottom-up derivations we consider by using a window of nactive nodes.
For a window of 2, we can only choose itherof the two leftmost nodes in the above process.
So for theparse in Figure 1, we only get 4 derivations with a derivationwindow of 2.Eesh charscter used by the computer Is listedFigure 3: Treebank analysis encoded using feature values.Each internal node contains, from top to bottom, a label,word, tag, and extension value, and each leaf node contains aword, tag, and extension value.4.
Probabil ist ic Models for Node FeaturesNode Representation We do not use all the subtree infor-mation rooted at a node N to condition our probabilistic mod-els.
But rather we have an equivalence class defined by thenode name (if it's available), we also have for constituentnodes, a word, along with its corresponding part-of-speechtag, that is selected from each constituent to act as a lexicalrepresentative.
The lexical representative from a constituentcorresponds loosely to the linguistic notion of a head word.For example, the lexical representative of a noun phrase is therightmost noun, and the lexical representative of a verb phraseis the leftmost non-auxiliary verb.
However, the correlationto linguistic theory ends there.
The deterministic rules (oneper label) which select he representative word from each con-stituent were developed in the better part of an hour, in keep-ing with the philosophy of avoiding excessive dependenceon carefully crafted rule-based methods.
Figure 3 illustratesthe word and tag features propagated along the parse tree foran example sentence.
Each internal node is represented as a4-feature vector: label, head word, head tag, and extension.Notation In the remainder of this section, the following no-tational scheme will be used.
wi and ti refer to the wordcorresponding to the ith token in the sentence mad its part-of-274speech tag, respectively.
N ~ refers to the 4-tuple of featurevalues at the kth node in the current parse state, where thenodes are numbered from left to right.
N/~, N~, Nt k, andN~ refer, respectively, tothe label, word, tag, and extensionfeature values at the node k. N ?j refers to the jth child ofthe current node where the leftmost child is child 1.
N e-~refers to the jth child of the current node where the rightmostchild is child 1.
The symbol Q,te refers to miscellaneousquestions about the current state of the parser, such as thenumber of nodes in the sentence and the number of childrenof a particular node.The Tagging Model The tag feature value prediction iscon-ditioned on the two words to the left, the two words to theright, and all information at two nodes to the left and twonodes to the right.p(ti \[ contezt) ~ p(t~ \[ w~wi-twi-2wi+twi+2t~-tti-2t~+lti+2Nk-l N~-2N~+t N~+ 2)The Extension Model The extension feature value predic-tion is conditioned on the node information at the node beingextended, all information from two nodes to the left and twonodes to the right, and the two leftmost children and the tworightmost children of the current node (these will be redundantif there are less than 4 children at a node).v(N  I  o=te t)The Label Model The label feature value prediction is con-ditioned on questions about he presence of selected words inthe constituent, all information from two nodes to the left andtwo nodes to the right, and the two leftmost children and thetwo rightmost children of the current node.p(N~ I contezt) ~ p(N~ I Q ~Nk-INk-2Nk+INk+2N?INC~NC-~NC-~)questions about the history.
We have described in earlierpapers, \[6, 4\], how we use mutual information clustering ofwords to define a set of classes on words that form the basisof the binary questions about words in the history.
We alsohave defined by the same mutual information on the bigramtag distribution classes for binary questions on tags.
We haveidentified by hand a set of classes for the binary questions onthe labels.
The decision trees are grown using the standardmethods described in \[5\].
In the case of hidden derivations,the forward-backward algorithms can be used to get partialcounts for the different events used in building the decisiontrees.5.
Expectation Maximization TrainingThe proposed history-based model cannot be estimated bydirect frequency counts because the model contains ahiddencomponent: the derivation model.
The order in which thetreebank parse trees were constructed is not encoded in thetreebank, but the parser assigns probabilities to specific de-rivations of a parse tree.
A forward-backward (FB) algorithmcan be easily defined to compute a posteriori probabilitiesfor.
the states.
These probabilities can then be used to de-fine counts for the different events that are used to build thedecision trees.To train the parser, all legal derivations of a parse tree (ac-cording to the derivational window constraint) are computed.~p(N~\ [  N~NtkNpN~N~- iN  ~-2 Each derivation can be viewed as a path from a common ini-Nk+iNk+2NC~NC~NC-lNC-~}ial st te, the words in the sentence, to a common final state,the completed parse tree.
These derivations form a latticeof states, since different derivations of the same parse treeinevitably merge.
For instance, the state created by taggingthe first word in the sentence and then the second is the samestate created by tagging the second word and then the first.These two derivations of this state have different probabilityestimates, but the state can be viewed as one state for futureactions, since it represents a single history.The Derivation Model In initial experiments, the ac-tive node selection process was modelled by a uniform(p(active) = 1/n) model with n = 2.
Our intuition was thatby parametrizing the choice of which active node to process,we could improve the parser by delaying labeling and exten-sion steps when the partial parse indicates ambiguity.
Weused the current node information and the node informationavailable within the five node window.5.1.
Decision Trees and the Forward-BackwardAlgorithmEach leaf of decision tree represents he distribution of a classof histories.
The parameters of these distributions can beupdated using the F-B algorithm.Initially, the models in the parser are assumed to be uniform.Accordingly, each event in each derivation contributes equallyto tlm process which selects which questions to ask aboutthe history in order to predict each feature value.
However,k k ~ 1 k ~ ~+1 ~ 2theuni f?r lnm?del is  certainly not avery good model ofp(active I contezt) ,~ p(active \[ Q "N N "- N - N N "-~ )feature value assignments.
And, since some derivations ofa parse tree are better than others, the events generated byStatistical Decision Trees The above probability distribu- the better derivations should contribute more to the decisiontion are each modeled as a statistical decision tree with binary tree-growing process.
The decision trees grown using the275uniform as!
;umption collectively form a parsing model, MI.The F-B count for each event in the training corpus usingMI can be used to grow a new set of decision trees, M2.The decision trees in M2 are constructed in a way whichgives more weight to the events which contributed most to theprobability of the corpus.
However, there is no guarantee thatM2 is a betl.er model than MI.
It isn't even guaranteed that theprobability of the training corpus according to M2 is higherthan the probability according to MI.
However, based onexperimental results, the use of F-B counts in the constructionof new decision trees is effective in acquiring a better modelof the data.Thereis no >way of knowing, apriori, which combination of thepreviously mentioned applications of the forward-backwardalgorithm will produce the best model.
After initial exper-imentation, the following sequence of training steps provedeffective:Grow initial decision trees (MI) based on uniform mod-elsCreate M2 by pruning trees in MI to a maximum depthof 10.Grow decision trees (M3) from F-B counts from M2.Perform F-B reestimation for leaves of decision trees inM3.Smoothing Decision Trees Once the leaf distributions for a,set of decision trees are fixed, the model must be smoothed us-ing held-out data to avoid overtraining on the original trainingcorpus.Each node in a decision tree potentially assigns a differentdistribution to the set of future values predicted by that tree.The problem of smoothing is to decide which combination ofthe distributions along a path from a leaf to the root will resultin the most accurate model.
The decision trees are smoothedby assigning a parameter to each node.
This parameter repre-sents the extent to which the distribution at that node shouldbe trusted with respect to the distribution at the parent node.6.
Experimental ResultsTask Domain We have chosen computer manuals as a taskdomain.
We picked the most frequent 3000 words from 10manuals as our vocabulary.
We then extracted about 35,000sentences covered by this vocabulary3 from40,000,000 wordsof computer manuals.
This corpus was treebanked by theUniversity of Lancaster.
The Treebank uses 17 non-terminallabels and 240 tags4.actual vocabulary is around 7,000 words when we include the manysymbols, formulas, and numbers that occur in t l~e  manuals*we have projected the tag set to 193Table 1: Distribution of sentences, average wordslsentence,and average number of non-terminals per sentence for theblind test set.A parse produced by the parser is judged to be correct underthe "Exact Match" criterion if it agrees with the Treebankparse structurally and all NT labels and tags agree5LengthExperiment 1 The parser using a stack decoding searchwhich produced 1 parse for each sentence, and this parse wascompared to the treebank parse for that sentence.
On this testset, the parser produced the correct parse, i.e.
a parse whichmatched the treebank parse exactly, for 38% of the sentences.Ignoring part-of-speech tagging errors, it produced the correctparse tree for 47% of the sentences.
Further, the correct parsetree is present in the top 20 parses produced by the parser for64% of the sentences.Words/Sentence# ofSentencesNo other parsers have reported results on exactly matchingtreebank parses, so we also evaluated on the crossing brack-ets measure from [2], which represents the percentage of sen-tences for which none of the constituents in a parser's analysisviolate the constituent boundaries of the treebank parse.
Thecrossing-brackets measure is a very weak measure of parsingaccuracy, since it does not verify prepositional phrase attach-ment or any other decision which is indicated by omittingstructure.
However, based on analysis of parsing errors, inthe current state-of-the-art, increases in the crossing bracketsmeasure appear to correlated with improvements in overallparsing performance.
This may not remain true as parsersbecome more accurate.Constituent1SentenceThe 1100 sentence corpus that we used in this first experi-ment was one of the test corpora used in several experimentsreported in [2].
The grammar-based parser discussed in [2]uses a P-CFG based on a rule-based grammar developed bya grammarian by examining the same training set used aboveover a period of more than 3 years.
This P-CFG parser pro-duced parses which passed the crossing brackets test for 69%of the 1100 sentences.
Our decision tree hidden derivationparser improves upon this result, passing the crossing bracketstest for 78% of the sentences.
The details of this experimentare discussed in [9].% sample of 5000 sentences (a training set of 4000, a developmenttest of 500, and an evaluation test of 500) is available by request fromroukos Q watson.ibm.com.Length TreebankConsistency1-10 69.1%1-15 64.9%1-23 58.3%1-30 - - -1-oo 52.5%Exact top 20 CrossingMatch Bracket55.9% 80.8% 91.5%51.7% 78.7% 86.2%41.9% 68.9% 76.5%38.1% 64.0% 70.9%34.9% 59.1% 65.7%# Sentencesin Exact  Top 20Training D~a Mmch15000 34.2 61.120000 37.4 64.825000 37 67.730000 38.1 68.434000 38.9 73Table 2: Performance of leftmost bottom-up derivation forComputer Manuals.Experiment 2 By using a derivation window of 1, we findthat Exact Match accuracy decreases by two percentage pointswith a significant reduction in computational complexity.
Us-ing the simpler single derivation model, we built a new set ofmodels.
We also combined the naming and extension stepsinto one, improved some of our processing of the casing ofwords, and added a few additional questions.
Using thesemodels, we ran on all sentences in our blind test set.
Ta-ble 1 gives some statistics a function of sentence length onour test set of 1656 sentences.
Table 2 gives the parser'sperformance e. In Table 2, we show a measure of treebankconsistency.
During treebanking, a random sample of about1000 sentences was treebanked by two treebankers.
The per-centage of sentences for which they both produce the sameexact trees (tags included) is shown as Treebank Consistencyin Table 2.
We also show the percentage of sentences thatmatch the Treebank, the percentage where the Treebank parseis among the top 20 parses produced by the parser, and thepercentage ofsentences without a crossing bracket.
Currently,the parser parses every third sentence xactly as a treebankerand is about 15 percentage points below what the treebankersagree on when they are parsing in production mode.
A morecarefully treebanked test set may be necessary in the future aswe improve our parser.We also explored the effect of training set size on parsingperformance with an earlier version of the parsing model.Table 3 shows the Exact Match score for sentences of 23words or less.
From this data, we see that we have a smallimprovement in accuracy by doubling the training set sizefrom 15k to 30k sentences./7.
Conc lus ionWe presented a "linguistically" naive parsing model that hasa parsing accuracy rate that we believe is state-of-the-art.
Weanticipate that by refining the "linguistic" features that can beexamined by the decision trees, we can improve the parser'sperformance significantly.
Of particular interest are linguistic6 While we prefer to use Exact Match for automatic parsing, we computedthe PARSEVAL performance measures tobe: 80% Recall, 81% Precision,and 10% Crossing Brackets on the unseen test set of Experiment 2.
Note: Onthis test set, 65.7% of the sentences are parsed without any crossing brackets.Table 3: Performance as a function of Training Set Sizefeatures that may be helpful in conjunction and other longdistance dependency.
We are currently investigating somemehtods for building in some of these features.AcknowledgementWe wish to thank Robert T. Ward for his measurements ofTreebank consistency.
This work was supported in part byARPA under ONR contract No.
N00014-92-C-0189.References1.
Baker, J. K., 1975.
Stochastic Modeling for Automatic SpeechUnderstanding.
In Speech Recognition, edited by Raj Reddy,Academic Press, pp.
521-542.2.
Black, E., Garside, R., and Leech, G., 1993.
Statistically-driven Computer Grammars of English: The IBM/LancasterApproach.
Rodopi.
Atlanta, Georgia.3.
Black, E., Lafferty, J., and Roukos, S., 1992.
Developmentand Evaluation of a Broad-Coverage Probabilistic Grammarof English-Language Computer Manuals.
In Proceedings ofthe Association for Computational Linguistics, 1992.
Newark,Delaware.4.
Black, E., Jelinek, F., Lafferty, J., Magerman, D. M., Mercer,R., and Roukos, S., 1993.
Towards History-based Grammars:Using Richer Models for Probabilistic Parsing.
In Proceed-ings of the Association for Computational Linguistics, 1993.Columbus, Ohio.5.
Breiman, L., Friedman, J. H., Olshen, R. A., and Stone, C.J., 1984.
Classification and Regression Trees.
Wadsworth andBrooks.
Pacific Grove, California.6.
Brown, P. F., Della Pietra, V. J., deSouza, P. V., Lai, J. C.,and Mercer, R. L. Class-based n-gram Models of NaturalLanguage.
In Proceedings of the IBM Natural Language ITL,March, 1990.
Paris, France.7.
Magerman, D. M. and Marcus, M. P. 1991.
Pearl: A Probabilis-tic Chart Parser.
In Proceedings of the February 1991 DARPASpeech and Natural Language Workshop.
Asilomar, California.8.
Magerman, D. M. and Weir, C. 1992.
Efficiency, Robust-ness, and Accuracy in Picky Chart Parsing.
In Proceedings ofthe Association for Computational Linguistics, 1992.
Newark,Delaware.9.
Magerman, D., 1994.
Natural Language Parsing as StatisticalPattern Recognition.
Ph.
D. dissertation, Stanford University,California.10.
Sharman, R. A., Jelinek, F., and Mercer, R. 1990.
Generat-ing a Grammar for Statistical Training.
In Proceedings of theJune 1990 DARPA Speech and Natural Language Workshop.Hidden Valley, Pennsylvania.277
