Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 905?914,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsA Graph Approach to Spelling Correction in Domain-Centric SearchZhuowei BaoUniversity of PennsylvaniaPhiladelphia, PA 19104, USAzhuowei@cis.upenn.eduBenny KimelfeldIBM Research?AlmadenSan Jose, CA 95120, USAkimelfeld@us.ibm.comYunyao LiIBM Research?AlmadenSan Jose, CA 95120, USAyunyaoli@us.ibm.comAbstractSpelling correction for keyword-searchqueries is challenging in restricted domainssuch as personal email (or desktop) search,due to the scarcity of query logs, and due tothe specialized nature of the domain.
For thattask, this paper presents an algorithm that isbased on statistics from the corpus data (ratherthan the query log).
This algorithm, whichemploys a simple graph-based approach, canincorporate different types of data sourceswith different levels of reliability (e.g., emailsubject vs. email body), and can handlecomplex spelling errors like splitting andmerging of words.
An experimental studyshows the superiority of the algorithm overexisting alternatives in the email domain.1 IntroductionAn abundance of applications require spelling cor-rection, which (at the high level) is the followingtask.
The user intends to type a chunk q of text,but types instead the chunk s that contains spellingerrors (which we discuss in detail later), due to un-careful typing or lack of knowledge of the exactspelling of q.
The goal is to restore q, when givens.
Spelling correction has been extensively studiedin the literature, and we refer the reader to compre-hensive summaries of prior work (Peterson, 1980;Kukich, 1992; Jurafsky and Martin, 2000; Mitton,2010).
The focus of this paper is on the special casewhere q is a search query, and where s instead of qis submitted to a search engine (with the goal of re-trieving documents that match the search query q).Spelling correction for search queries is important,because a significant portion of posed queries maybe misspelled (Cucerzan and Brill, 2004).
Effectivespelling correction has a major effect on the expe-rience and effort of the user, who is otherwise re-quired to ensure the exact spellings of her queries.Furthermore, it is critical when the exact spelling isunknown (e.g., person names like Schwarzenegger).1.1 Spelling ErrorsThe more common and studied type of spelling erroris word-to-word error: a single word w is misspelledinto another single word w?.
The specific spelling er-rors involved include omission of a character (e.g.,atachment), inclusion of a redundant character(e.g., attachement), and replacement of charac-ters (e.g., attachemnt).
The fact that w?
is a mis-spelling of (and should be corrected to) w is denotedby w?
?
w (e.g., atachment ?
attachment).Additional common spelling errors are splitting ofa word, and merging two (or more) words:?
attach ment ?
attachment?
emailattachment?
email attachmentPart of our experiments, as well as most of ourexamples, are from the domain of (personal) emailsearch.
An email from the Enron email collec-tion (Klimt and Yang, 2004) is shown in Figure 1.Our running example is the following misspelling ofa search query, involving multiple types of errors.sadeep kohli excellatach ment ?sandeep kohli excel attachment (1)In this example, correction entails fixing sadeep,splitting excellatach, fixing excell, mergingatach ment, and fixing atachment.
Beyond thecomplexity of errors, this example also illustratesother challenges in spelling correction for search.We need to identify not only that sadeep is mis-spelled, but also that kohli is correctly spelled.Just having kohli in a dictionary is not enough.905Subject: Follow-Up on Captive GenerationFrom: sandeep.kohli@enron.comX-From: Sandeep KohliX-To: Stinson Gibner@ECT, Vince J Kaminski@ECTVince/Stinson,Please find below two attachemnts.
The Excell spreadsheetshows some calculations.
.
.
The seond attachement (Word) hasthe wordings that I think we can send in to the press.
.
.I am availabel on mobile if you have questions o clarifications.
.
.Regards,Sandeep.Figure 1: Enron email (misspelled words are underlined)For example, in kohli coupons the user may verywell mean kohls coupons if Sandeep Kohli hasnothing to do with coupons (in contrast to the storechain Kohl?s).
A similar example is the word nail,which is a legitimate English word, but in the con-text of email the query nail box is likely to bea misspelling of mail box (unless nail boxes areindeed relevant to the user?s email collection).
Fi-nally, while the word kohli is relevant to someemail users (e.g., Kohli?s colleagues), it may haveno meaning at all to other users.1.2 Domain KnowledgeThe common approach to spelling correction uti-lizes statistical information (Kernighan et al, 1990;Schierle et al, 2007; Mitton, 2010).
As a sim-ple example, if we want to avoid maintaining amanually-crafted dictionary to accommodate thewealth of new terms introduced every day (e.g.,ipod and ipad), we may decide that atachmentis a misspelling of attachment due to both the(relative) proximity between the words, and thefact that attachment is significantly more pop-ular than atachment.
As another example, thefact that the expression sandeep kohli is fre-quent in the domain increases our confidence insadeep kohli ?
sandeep kohli (rather than,e.g., sadeep kohli ?
sudeep kohli).
Onecan further note that, in email search, the fact thatSandeep Kohli sent multiple excel attachments in-creases our confidence in excell ?
excel.A source of statistics widely used in prior workis the query log (Cucerzan and Brill, 2004; Ahmadand Kondrak, 2005; Li et al, 2006a; Chen et al,2007; Sun et al, 2010).
However, while query logsare abundant in the context of Web search, in manyother search applications (e.g.
email search, desktopsearch, and even small-enterprise search) query logsare too scarce to provide statistical information thatis sufficient for effective spelling correction.
Evenan email provider of a massive scale (such as GMail)may need to rely on the (possibly tiny) query log ofthe single user at hand, due to privacy or securityconcerns; moreover, as noted earlier about kohli,the statistics of one user may be relevant to one user,while irrelevant to another.The focus of this paper is on spelling correctionfor search applications like the above, where query-log analysis is impossible or undesirable (with emailsearch being a prominent example).
Our approachrelies mainly on the corpus data (e.g., the collectionof emails of the user at hand) and external, genericdictionaries (e.g., English).
As shown in Figure 1,the corpus data may very well contain misspelledwords (like query logs do), and such noise is a part ofthe challenge.
Relying on the corpus has been shownto be successful in spelling correction for text clean-ing (Schierle et al, 2007).
Nevertheless, as we laterexplain, our approach can still incorporate query-logdata as features involved in the correction, as well asmeans to refine the parameters.1.3 Contribution and OutlineAs said above, our goal is to devise spelling cor-rection that relies on the corpus.
The corpus oftencontains various types of information, with differentlevels of reliability (e.g., n-grams from email sub-jects and sender information, vs. those from emailbodies).
The major question is how to effectivelyexploit that information while addressing the vari-ous types of spelling errors such as those discussedin Section 1.1.
The key contribution of this work isa novel graph-based algorithm, MaxPaths, that han-dles the different types of errors and incorporates thecorpus data in a uniform (and simple) fashion.
Wedescribe MaxPaths in Section 2.
We evaluate theeffectiveness of our algorithm via an experimentalstudy in Section 3.
Finally, we make concluding re-marks and discuss future directions in Section 4.2 Spelling-Correction AlgorithmIn this section, we describe our algorithm forspelling correction.
Recall that given a search query906s of a user who intends to phrase q, the goal is tofind q.
Our corpus is essentially a collection D ofunstructured or semistructured documents.
For ex-ample, in email search such a document is an emailwith a title, a body, one or more recipients, and soon.
As conventional in spelling correction, we de-vise a scoring function scoreD(r | s) that estimatesour confidence in r being the correction of s (i.e.,that r is equal to q).
Eventually, we suggest a se-quence r from a set CD(s) of candidates, such thatscoreD(r | s) is maximal among all the candidatesin CD(s).
In this section, we describe our graph-based approach to finding CD(s) and to determiningscoreD(r | s).We first give some basic notation.
We fix an al-phabet ?
of characters that does not include anyof the conventional whitespace characters.
By ?
?we denote the set of all the words, namely, fi-nite sequences over ?.
A search query s is asequence w1, .
.
.
, wn, where each wi is a word.For convenience, in our examples we use whites-pace instead of comma (e.g., sandeep kohli in-stead of sandeep, kohli).
We use the Damerau-Levenshtein edit distance (as implemented by theJazzy tool) as our primary edit distance between twowords r1, r2 ?
?
?, and we denote this distance byed(r1, r2).2.1 Word-Level CorrectionWe first handle a restriction of our problem, wherethe search query is a single word w (rather thana general sequence s of words).
Moreover, weconsider only candidate suggestions that are words(rather than sequences of words that account for thecase where w is obtained by merging keywords).Later, we will use the solution for this restrictedproblem as a basic component in our algorithm forthe general problem.Let UD ?
??
be a finite universal lexicon, which(conceptually) consists of all the words in the corpusD.
(In practice, one may want add to D words ofauxiliary sources, like English dictionary, and to fil-ter out noisy words; we did so in the site-search do-main that is discussed in Section 3.)
The set CD(w)of candidates is defined byCD(w) def= {w} ?
{w?
?
UD | ed(w,w?)
?
?}
.for some fixed number ?.
Note that CD(w) containsTable 1: Feature set WFD in email searchBasic Featuresed(w,w?
): weighted Damerau-Levenshtein edit distanceph(w,w?
): 1 if w and w?
are phonetically equal, 0 otherwiseenglish(w?
): 1 is w?
is in English, 0 otherwiseCorpus-Based Featureslogfreq(w?
)): logarithm of #occurrences of w?
in the corpusDomain-Specific Featuressubject(w?
): 1 if w?
is in some ?Subject?
field, 0 otherwisefrom(w?
): 1 if w?
is in some ?From?
field, 0 otherwisexfrom(w?
): 1 if w?
is in some ?X-From?
field, 0 otherwisew even if w is misspelled; furthermore, CD(w) maycontain other misspelled words (with a small editdistance to w) that appear in D.We now define scoreD(w?
| w).
Here, our cor-pus D is translated into a set WFD of word features,where each feature f ?
WFD gives a scoring func-tion scoref (w?
| w).
The function scoreD(w?
| w) issimply a linear combination of the scoref (w?
| w):scoreD(w?
| w) def=?f?WFDaf ?
scoref (w?
| w)As a concrete example, the features of WFD we usedin the email domain are listed in Table 1; the result-ing scoref (w?
|w) is in the spirit of the noisy channelmodel (Kernighan et al, 1990).
Note that additionalfeatures could be used, like ones involving the stemsof w and w?, and even query-log statistics (whenavailable).
Rather than manually tuning the param-eters af , we learned them using the well knownSupport Vector Machine, abbreviated SVM (Cortesand Vapnik, 1995), as also done by Schaback andLi (2007) for spelling correction.
We further discussthis learning step in Section 3.We fix a natural number k, and in the sequel wedenote by topD(w) a set of k words w?
?
CD(w)with the highest scoreD(w?
| w).
If |CD(w)| < k,then topD(w) is simply CD(w).2.2 Query-Level Correction: MaxPathsWe now describe our algorithm, MaxPaths, forspelling correction.
The input is a (possibly mis-spelled) search query s = s1, .
.
.
, sn.
As done inthe word-level correction, the algorithm produces aset CD(s) of suggestions and determines the values907Algorithm 1 MaxPathsInput: a search query sOutput: a set CD(s) of candidate suggestions r,ranked by scoreD(r | s)1: Find the strongly plausible tokens2: Construct the correction graph3: Find top-k full paths (with the largest weights)4: Re-rank the paths by word correlationscoreD(r | s), for all r ?
CD(s), in order to rankCD(s).
A high-level overview of MaxPaths is givenin the pseudo-code of Algorithm 1.
In the rest of thissection, we will detail each of the four steps in Al-gorithm 1.
The name MaxPaths will become cleartowards the end of this section.We use the following notation.
For a word w =c1 ?
?
?
cm of m characters ci and integers i < jin {1, .
.
.
,m + 1}, we denote by w[i,j) the wordci ?
?
?
cj?1.
For two words w1, w2 ?
?
?, the wordw1w2 ?
??
is obtained by concatenating w1 andw2.
Note that for the search query s = s1, .
.
.
, snit holds that s1 ?
?
?
sn is a single word (in ??).
Wedenote the word s1 ?
?
?
sn by bsc.
For example, ifs1 = sadeep and s2 = kohli, then s correspondsto the query sadeep kohli while bsc is the wordsadeepkohli; furthermore, bsc[1,7) = sadeep.2.2.1 Plausible TokensTo support merging and splitting, we first iden-tify the possible tokens of the given query s. Forexample, in excellatach ment we would like toidentify excell and atach ment as tokens, sincethose are indeed the tokens that the user has in mind.Formally, suppose that bsc = c1 ?
?
?
cm.
A token isa word bsc[i,j) where 1 ?
i < j ?
m + 1.
Tosimplify the presentation, we make the (often false)assumption that a token bsc[i,j) uniquely identifiesi and j (that is, bsc[i,j) 6= bsc[i?,j?)
if i 6= i?
orj 6= j?
); in reality, we should define a token as atriple (bsc[i,j), i, j).
In principle, every token bsc[i,j)could be viewed as a possible word that user meantto phrase.
However, such liberty would require ouralgorithm to process a search space that is too largeto manage in reasonable time.
Instead, we restrict tostrongly plausible tokens, which we define next.A token w = bsc[i,j) is plausible if w is a wordof s, or there is a word w?
?
CD(w) (as defined inSection 2.1) such that scoreD(w?
| w) > ?
for somefixed number ?.
Intuitively, w is plausible if it is anoriginal token of s, or we have a high confidence inour word-level suggestion to correct w (note that thesuggested correction for w can be w itself).
Recallthat bsc = c1 ?
?
?
cm.
A tokenization of s is a se-quence j1, .
.
.
, jl, such that j1 = 1, jl = m+1, andji < ji+1 for 1 ?
i < l. The tokenization j1, .
.
.
, jlinduces the tokens bsc[j1,j2),.
.
.
,bsc[jl?1,jl).
A tok-enization is plausible if each of its induced tokensis plausible.
Observe that a plausible token is notnecessarily induced by any plausible tokenization;in that case, the plausible token is useless to us.Thus, we define a strongly plausible token, abbre-viated sp-token, which is a token that is induced bysome plausible tokenization.
As a concrete example,for the query excellatach ment, the sp-tokens inour implementation include excellatach, ment,excell, and atachment.As the first step (line 1 in Algorithm 1), we findthe sp-tokens by employing an efficient (and fairlystraightforward) dynamic-programming algorithm.2.2.2 Correction GraphIn the next step (line 2 in Algorithm 1), we con-struct the correction graph, which we denote byGD(s).
The construction is as follows.We first find the set topD(w) (defined in Sec-tion 2.1) for each sp-token w. Table 2 shows the sp-tokens and suggestions thereon in our running exam-ple.
This example shows the actual execution of ourimplementation within email search, where s is thequery sadeep kohli excellatach ment; forclarity of presentation, we omitted a few sp-tokensand suggested corrections.
Observe that some ofthe corrections in the table are actually misspelledwords (as those naturally occur in the corpus).A node of the graph GD(s) is a pair ?w,w?
?, wherew is an sp-token and w?
?
topD(w).
Recall oursimplifying assumption that a token bsc[i,j) uniquelyidentifies the indices i and j.
The graph GD(s) con-tains a (directed) edge from a node ?w1, w?1?
to anode ?w2, w?2?
if w2 immediately follows w1 in bqc;in other words, GD(s) has an edge from ?w1, w?1?to ?w2, w?2?
whenever there exist indices i, j and k,such that w1 = bsc[i,j) and w2 = bsc[j,k).
Observethat GD(s) is a directed acyclic graph (DAG).908exceptexcellexcelexcellenceexcellentsandeepjaideepkohliattachementattachmentattachedsandeep kohlisentmeetmentFigure 2: The graph GD(s)For example, Figure 2 shows GD(s) for thequery sadeep kohli excellatach ment, withthe sp-tokens w and the sets topD(w) being those ofTable 2.
For now, the reader should ignore the nodein the grey box (containing sandeep kohli) andits incident edges.
For simplicity, in this figure wedepict each node ?w,w??
by just mentioning w?
; theword w is in the first row of Table 2, above w?.2.2.3 Top-k PathsLet P = ?w1, w?1?
?
?
?
?
?
?wk, w?k?
be a pathin GD(s).
We say that P is full if ?w1, w?1?
has noincoming edges in GD(s), and ?wk, w?k?
has no out-going edges in GD(s).
An easy observation is that,since we consider only strongly plausible tokens, ifP is full then w1 ?
?
?wk = bsc; in that case, the se-quence w?1, .
.
.
, w?k is a suggestion for spelling cor-rection, and we denote it by crc(P ).
As an example,Figure 3 shows two full paths P1 and P2 in the graphGD(s) of Figure 2.
The corrections crc(Pi), fori = 1, 2, are jaideep kohli excellent mentand sandeep kohli excel attachement, re-spectively.To obtain corrections crc(P ) with high quality,we produce a set of k full paths with the largestweights, for some fixed k; we denote this set bytopPathsD(s).
The weight of a path P , denotedweight(P ), is the sum of the weights of all the nodesand edges in P , and we define the weights of nodesand edges next.
To find these paths, we use a wellknown efficient algorithm (Eppstein, 1994).kohlikohliexcellent mentexcel attachmentjaideepsandeepP1P2Figure 3: Full paths in the graph GD(s) of Figure 2Consider a node u = ?w,w??
of GD(s).
In theconstruction of GD(s), zero or more merges of (partof) original tokens have been applied to obtain thetoken w; let #merges(w) be that number.
Consideran edge e of GD(s) from a node u1 = ?w1, w?1?
tou2 = ?w2, w?2?.
In s, either w1 and w2 belong todifferent words (i.e., there is a whitespace betweenthem) or not; in the former case define #splits(e) =0, and in the latter #splits(e) = 1.
We define:weight(u) def= scoreD(w?
| w) + am ?#merges(w)weight(e) def= as ?#splits(e)Note that am and as are negative, as they penalizefor merges and splits, respectively.
Again, in ourimplementations, we learned am and as by meansof SVM.Recall that topPathsD(s) is the set of k full paths(in the graph GD(s)) with the largest weights.
FromtopPathsD(s) we get the set CD(s) of candidatesuggestions:CD(s) def= {crc(P ) | P ?
topPathsD(s)} .2.2.4 Word CorrelationTo compute scoreD(r|s) for r ?
CD(s), we incor-porate correlation among the words of r. Intuitively,we would like to reward a candidate with pairs ofwords that are likely to co-exist in a query.
Forthat, we assume a (symmetric) numerical functioncrl(w?1, w?2) that estimates the extent to which thewords w?1 and w?2 are correlated.
As an example, inthe email domain we would like crl(kohli, excel)to be high if Kohli sent many emails with excel at-tachments.
Our implementation of crl(w?1, w?2) es-sentially employs pointwise mutual information thathas also been used in (Schierle et al, 2007), and that909Table 2: topD(w) for sp-tokens wsadeep kohli excellatach ment excell atachmentsandeep kohli excellent ment excel attachmentjaideep excellence sent excell attachedmeet except attachementcompares the number of documents (emails) con-taining w?1 and w?2 separately and jointly.Let P ?
topPathsD(s) be a path.
We de-note by crl(P ) a function that aggregates the num-bers crl(w?1, w?2) for nodes ?w1, w?1?
and ?w2, w?2?of P (where ?w1, w?1?
and ?w2, w?2?
are not nec-essarily neighbors in P ).
Over the email domain,our crl(P ) is the minimum of the crl(w?1, w?2).
Wedefine scoreD(P ) = weight(P ) + crl(P ).
Toimprove the performance, in our implementationwe learned again (re-trained) all the parameters in-volved in scoreD(P ).Finally, as the top suggestions we take crc(P )for full paths P with highest scoreD(P ).
Note thatcrc(P ) is not necessarily injective; that is, there canbe two full paths P1 6= P2 satisfying crc(P1) =crc(P2).
Thus, in effect, scoreD(r | s) is determinedby the best evidence of r; that is,scoreD(r | s) def= max{scoreD(P ) | crc(P ) = r?P ?
topPathsD(s)} .Note that our final scoring function essentially viewsP as a clique rather than a path.
In principle,we could define GD(s) in a way that we wouldextract the maximal cliques directly without find-ing topPathsD(s) first.
However, we chose ourmethod (finding top paths first, and then re-ranking)to avoid the inherent computational hardness in-volved in finding maximal cliques.2.3 Handling ExpressionsWe now briefly discuss our handling of frequent n-grams (expressions).
We handle n-grams by intro-ducing new nodes to the graph GD(s); such a newnode u is a pair ?t, t?
?, where t is a sequence ofn consecutive sp-tokens and t?
is a n-gram.
Theweight of such a node u is rewarded for consti-tuting a frequent or important n-gram.
An exam-ple of such a node is in the grey box of Figure 2,where sandeep kohli is a bigram.
Observe thatsandeep kohli may be deemed an important bi-gram because it occurs as a sender of an email, andnot necessarily because it is frequent.An advantage of our approach is avoidanceof over-scoring due to conflicting n-grams.
Forexample, consider the query textile importexpert, and assume that both textile importand import export (with an ?o?
rather than an?e?)
are frequent bigrams.
If the user referred to thebigram textile import, then expert is likely tobe correct.
But if she meant for import export,then expert is misspelled.
However, only one ofthese two options can hold true, and we would liketextile import export to be rewarded onlyonce?for the bigram import export.
This isachieved in our approach, since a full path in GD(s)may contain either a node for textile import ora node for import export, but it cannot containnodes for both of these bigrams.Finally, we note that our algorithm is in the spiritof that of Cucerzan and Brill (2004), with a few in-herent differences.
In essence, a node in the graphthey construct corresponds to what we denote hereas ?w,w??
in the special case where w is an actualword of the query; that is, no re-tokenization is ap-plied.
They can split a word by comparing it to a bi-gram.
However, it is not clear how they can split intonon-bigrams (without a huge index) and to handle si-multaneous merging and splitting as in our runningexample (1).
Furthermore, they translate bigram in-formation into edge weights, which implies that theabove problem of over-rewarding due to conflictingbigrams occurs.3 Experimental StudyOur experimental study aims to investigate the ef-fectiveness of our approach in various settings, aswe explain next.3.1 Experimental SetupWe first describe our experimental setup, and specif-ically the datasets and general methodology.Datasets.
The focus of our experimental study ison personal email search; later on (Section 3.6),we will consider (and give experimental results for)a totally different setting?site search over www.ibm.com, which is a massive and open domain.Our dataset (for the email domain) is obtained from910the Enron email collection (Bekkerman et al, 2004;Klimt and Yang, 2004).
Specifically, we chose thethree users with the largest number of emails.
We re-fer to the three email collections by the last names oftheir owners: Farmer, Kaminski and Kitchen.
Eachuser mailbox is a separate domain, with a separatecorpus D, that one can search upon.
Due to the ab-sence of real user queries, we constructed our datasetby conducting a user study, as described next.For each user, we randomly sampled 50 emailsand divided them into 5 disjoint sets of 10 emailseach.
We gave each 10-email set to a unique hu-man subject that was asked to phrase two searchqueries for each email: one for the entire email con-tent (general query), and the other for the From andX-From fields (sender query).
(Figure 1 shows ex-amples of the From and X-From fields.)
The latterrepresents queries posed against a specific field (e.g.,using ?advanced search?).
The participants were nottold about the goal of this study (i.e., spelling correc-tion), and the collected queries have no spelling er-rors.
For generating spelling errors, we implementeda typo generator.1 This generator extends an onlinetypo generator (Seobook, 2010) that produces a vari-ety of spelling errors, including skipped letter, dou-bled letter, reversed letter, skipped space (merge),missed key and inserted key; in addition, our gener-ator produces inserted space (split).
When appliedto a search query, our generator adds random typosto each word, independently, with a specified prob-ability p that is 50% by default.
For each collectedquery (and for each considered value of p) we gener-ated 5 misspelled queries, and thereby obtained 250instances of misspelled general queries and 250 in-stances of misspelled sender queries.Methodology.
We compared the accuracy ofMaxPaths (Section 2) with three alternatives.
Thefirst alternative is the open-source Jazzy, whichis a widely used spelling-correction tool based on(weighted) edit distance.
The second alternative isthe spelling correction provided by Google.
Weprovided Jazzy with our unigram index (as a dic-tionary).
However, we were not able to do sowith Google, as we used remote access via its JavaAPI (Google, 2010); hence, the Google tool is un-1The queries and our typo generator are publicly availableat https://dbappserv.cis.upenn.edu/spell/.aware of our domain, but is rather based on itsown statistics (from the World Wide Web).
Thethird alternative is what we call WordWise, whichapplies word-level correction (Section 2.1) to eachinput query term, independently.
More precisely,WordWise is a simplified version of MaxPaths,where we forbid splitting and merging of words (i.e.,only the original tokens are considered), and wherewe do not take correlation into account.Our emphasis is on correcting misspelled queries,rather than recognizing correctly spelled queries,due to the role of spelling in a search engine: wewish to provide the user with the correct query uponmisspelling, but there is no harm in making a sug-gestion for correctly spelled queries, except for vi-sual discomfort.
Hence, by default accuracy meansthe number of properly corrected queries (withinthe top-k suggestions) divided by the number of themisspelled queries.
An exception is in Section 3.5,where we study the accuracy on correct queries.Since MaxPaths and WordWise involve parame-ter learning (SVM), the results for them are consis-tently obtained by performing 5-folder cross valida-tion over each collection of misspelled queries.3.2 Fixed Error ProbabilityHere, we compare MaxPaths to the alternativeswhen the error probability p is fixed (0.5).
We con-sider only the Kaminski dataset; the results for theother two datasets are similar.
Figure 4(a) shows theaccuracy, for general queries, of top-k suggestionsfor k = 1, k = 3 and k = 10.
Note that we can getonly one (top-1) suggestion from Google.
As canbe seen, MaxPaths has the highest accuracy in allcases.
Moreover, the advantage of MaxPaths overthe alternatives increases as k increases, which indi-cates potential for further improving MaxPaths.Figure 4(b) shows the accuracy of top-k sugges-tions for sender queries.
Overall, the results are sim-ilar to those of Figure 4(a), except that top-1 of bothWordWise and MaxPaths has a higher accuracy insender queries than in general queries.
This is dueto the fact that the dictionaries of person names andemail addresses extracted from the X-From andFrom fields, respectively, provide strong featuresfor the scoring function, since a sender query refersto these two fields.
In addition, the accuracy ofMaxPaths is further enhanced by exploiting the cor-9110%20%40%60%80%100%Top 1 Top 3 Top 10Google Jazzy WordWise MaxPaths(a) General queries (Kaminski)0%20%40%60%80%100%Top 1 Top 3 Top 10Google Jazzy WordWise MaxPaths(b) Sender queries (Kaminski)0%25%50%75%100%0% 20% 40% 60% 80% 100%Google Jazzy WordWise MaxPathsSpelling Error Probability(c) Varying error probability (Kaminski)Figure 4: Accuracy for Kaminski (misspelled queries)relation between the first and last name of a person.3.3 Impact of Error ProbabilityWe now study the impact of the complexity ofspelling errors on our algorithm.
For that, we mea-sure the accuracy while the error probability p variesfrom 10% to 90% (with gaps of 20%).
The re-sults are in Figure 4(c).
Again, we show the resultsonly for Kaminski, since we get similar results forthe other two datasets.
As expected, in all exam-ined methods the accuracy decreases as p increases.Now, not only does MaxPaths outperform the alter-natives, its decrease (as well as that of WordWise) isthe mildest?13% as p increases from 10% to 90%(while Google and Jazzy decrease by 23% or more).We got similar results for the sender queries (and foreach of the three users).3.4 Adaptiveness of ParametersObtaining the labeled data needed for parameterlearning entails a nontrivial manual effort.
Ideally,we would like to learn the parameters of MaxPathsin one domain, and use them in similar domains.0%25%50%75%100%0% 20% 40% 60% 80% 100%Google Jazzy MaxPaths* MaxPathsSpelling Error Probability(a) General queries (Farmer)0%25%50%75%100%0% 20% 40% 60% 80% 100%Google Jazzy MaxPaths* MaxPathsSpelling Error Probability(b) Sender queries (Farmer)Figure 5: Accuracy for Farmer (misspelled queries)More specifically, our desire is to use the parame-ters learned over one corpus (e.g., the email collec-tion of one user) on a second corpus (e.g., the emailcollection of another user), rather than learning theparameters again over the second corpus.
In this setof experiments, we examine the feasibility of thatapproach.
Specifically, we consider the user Farmerand observe the accuracy of our algorithm with twosets of parameters: the first, denoted by MaxPaths inFigures 5(a) and 5(b), is learned within the Farmerdataset, and the second, denoted by MaxPaths?, islearned within the Kaminski dataset.
Figures 5(a)and 5(b) show the accuracy of the top-1 suggestionfor general queries and sender queries, respectively,with varying error probabilities.
As can be seen,these results mean good news?the accuracies ofMaxPaths?
and MaxPaths are extremely close (theircurves are barely distinguishable, as in most casesthe difference is smaller than 1%).
We repeated thisexperiment for Kitchen and Kaminski, and got sim-ilar results.3.5 Accuracy for Correct QueriesNext, we study the accuracy on correct queries,where the task is to recognize the given query as cor-rect by returning it as the top suggestion.
For eachof the three users, we considered the 50 + 50 (gen-eral + sender) collected queries (having no spellingerrors), and measured the accuracy, which is thepercentage of queries that are equal to the top sug-912Table 3: Accuracy for Correct QueriesDataset Google Jazzy MaxPathsKaminski (general) 90% 98% 94%Kaminski (sender) 94% 98% 94%Farmer (general) 96% 98% 96%Farmer (sender) 96% 96% 92%Kitchen (general) 86% 100% 92%Kitchen (sender) 94% 100% 98%gestion.
Table 3 shows the results.
Since Jazzy isbased on edit distance, it almost always gives the in-put query as the top suggestion; the misses of Jazzyare for queries that contain a word that is not the cor-pus.
MaxPaths is fairly close to the upper bound setby Jazzy.
Google (having no access to the domain)also performs well, partly because it returns the in-put query if no reasonable suggestion is found.3.6 Applicability to Large-Scale Site SearchUp to now, our focus has been on email search,which represents a restricted (closed) domain withspecialized knowledge (e.g., sender names).
In thispart, we examine the effectiveness of our algorithmin a totally different setting?large-scale site searchwithin www.ibm.com, a domain that is popular ona world scale.
There, the accuracy of Google is veryhigh, due to this domain?s popularity, scale, and fullaccessibility on the Web.
We crawled 10 milliondocuments in that domain to obtain the corpus.
Wemanually collected 1348 misspelled queries fromthe log of search issued against developerWorks(www.ibm.com/developerworks/) during aweek.
To facilitate the manual collection of thesequeries, we inspected each query with two or fewersearch results, after applying a random permutationto those queries.
Figure 6 shows the accuracy oftop-k suggestions.
Note that the performance ofMaxPaths is very close to that of Google?only 2%lower for top-1.
For k = 3 and k = 10, MaxPathsoutperforms Jazzy and the top-1 of Google (fromwhich we cannot obtain top-k for k > 1).3.7 SummaryTo conclude, our experiments demonstrate variousimportant qualities of MaxPaths.
First, it outper-forms its alternatives, in both accuracy (Section 3.2)and robustness to varying error complexities (Sec-tion 3.3).
Second, the parameters learned in onedomain (e.g., an email user) can be applied to sim-0%20%40%60%80%100%Top 1 Top 3 Top 10Google Jazzy WordWise MaxPathsFigure 6: Accuracy for site searchilar domains (e.g., other email users) with essen-tially no loss in performance (Section 3.4).
Third,it is highly accurate in recognition of correct queries(Section 3.5).
Fourth, even when applied to large(open) domains, it achieves a comparable perfor-mance to the state-of-the-art Google spelling correc-tion (Section 3.6).
Finally, the higher performanceof MaxPaths on top-3 and top-10 corrections sug-gests a potential for further improvement of top-1(which is important since search engines often re-strict their interfaces to only one suggestion).4 ConclusionsWe presented the algorithm MaxPaths for spellingcorrection in domain-centric search.
This algo-rithm relies primarily on corpus statistics and do-main knowledge (rather than on query logs).
It canhandle a variety of spelling errors, and can incor-porate different levels of spelling reliability amongdifferent parts of the corpus.
Our experimental studydemonstrates the superiority of MaxPaths over ex-isting alternatives in the domain of email search, andindicates its effectiveness beyond that domain.In future work, we plan to explore how to utilizeadditional domain knowledge to better estimate thecorrelation between words.
Particularly, from avail-able auxiliary data (Fagin et al, 2010) and tools likeinformation extraction (Chiticariu et al, 2010), wecan infer and utilize type information from the cor-pus (Li et al, 2006b; Zhu et al, 2007).
For instance,if kohli is of type person, and phone is highly cor-related with person instances, then phone is highlycorrelated with kohli even if the two words do notfrequently co-occur.
We also plan to explore as-pects of corpus maintenance in dynamic (constantlychanging) domains.913ReferencesF.
Ahmad and G. Kondrak.
2005.
Learning a spellingerror model from search query logs.
In HLT/EMNLP.R.
Bekkerman, A. Mccallum, and G. Huang.
2004.
Au-tomatic categorization of email into folders: Bench-mark experiments on Enron and Sri Corpora.
Techni-cal report, University of Massachusetts - Amherst.Q.
Chen, M. Li, and M. Zhou.
2007.
Improvingquery spelling correction using Web search results.
InEMNLP-CoNLL, pages 181?189.L.
Chiticariu, R. Krishnamurthy, Y. Li, S. Raghavan,F.
Reiss, and S. Vaithyanathan.
2010.
SystemT: Analgebraic approach to declarative information extrac-tion.
In ACL, pages 128?137.C.
Cortes and V. Vapnik.
1995.
Support-vector networks.Machine Learning, 20(3):273?297.S.
Cucerzan and E. Brill.
2004.
Spelling correction as aniterative process that exploits the collective knowledgeof Web users.
In EMNLP, pages 293?300.D.
Eppstein.
1994.
Finding the k shortest paths.
InFOCS, pages 154?165.R.
Fagin, B. Kimelfeld, Y. Li, S. Raghavan, andS.
Vaithyanathan.
2010.
Understanding queries in asearch database system.
In PODS, pages 273?284.Google.
2010.
A Java API for Google spelling check ser-vice.
http://code.google.com/p/google-api-spelling-java/.D.
Jurafsky and J. H. Martin.
2000.
Speech andLanguage Processing: An Introduction to NaturalLanguage Processing, Computational Linguistics, andSpeech Recognition.
Prentice Hall PTR.M.
D. Kernighan, K. W. Church, and W. A. Gale.
1990.A spelling correction program based on a noisy chan-nel model.
In COLING, pages 205?210.B.
Klimt and Y. Yang.
2004.
Introducing the Enron cor-pus.
In CEAS.K.
Kukich.
1992.
Techniques for automatically correct-ing words in text.
ACM Comput.
Surv., 24(4):377?439.M.
Li, M. Zhu, Y. Zhang, and M. Zhou.
2006a.
Explor-ing distributional similarity based models for queryspelling correction.
In ACL.Y.
Li, R. Krishnamurthy, S. Vaithyanathan, and H. V. Ja-gadish.
2006b.
Getting work done on the web: sup-porting transactional queries.
In SIGIR, pages 557?564.R.
Mitton.
2010.
Fifty years of spellchecking.
WringSystems Research, 2:1?7.J.
L. Peterson.
1980.
Computer Programs for SpellingCorrection: An Experiment in Program Design, vol-ume 96 of Lecture Notes in Computer Science.Springer.J.
Schaback and F. Li.
2007.
Multi-level feature extrac-tion for spelling correction.
In AND, pages 79?86.M.
Schierle, S. Schulz, and M. Ackermann.
2007.
Fromspelling correction to text cleaning - using context in-formation.
In GfKl, Studies in Classification, DataAnalysis, and Knowledge Organization, pages 397?404.Seobook.
2010.
Keyword typo generator.http://tools.seobook.com/spelling/keywords-typos.cgi.X.
Sun, J. Gao, D. Micol, and C. Quirk.
2010.
Learningphrase-based spelling error models from clickthroughdata.
In ACL, pages 266?274.H.
Zhu, S. Raghavan, S. Vaithyanathan, and A.
Lo?ser.2007.
Navigating the intranet with high precision.
InWWW, pages 491?500.914
