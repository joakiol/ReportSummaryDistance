Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 19?24,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsAn Iterative Approach for JointDependency Parsing and Semantic Role LabelingQifeng DaiDepartment of Computer Sci-ence, University of Science andTechnology of China, Hefei,Chinadaiqifeng001@126.comEnhong ChenDepartment of Computer Sci-ence, University of Science andTechnology of China, Hefei,Chinacheneh@ustc.edu.cnLiu ShiDepartment of Computer Sci-ence, University of Science andTechnology of China, Hefei,Chinashiliu@ustc.eduAbstractWe propose a system to carry out the joint pars-ing of syntactic and semantic dependencies inmultiple languages for our participation in theshared task of CoNLL-2009.
We present an it-erative approach for dependency parsing andsemantic role labeling.
We have participated inthe closed challenge, and our system achieves73.98% on labeled macro F1 for the completeproblem, 77.11% on labeled attachment scorefor syntactic dependencies, and 70.78% on la-beled F1 for semantic dependencies.
The cur-rent experimental results  show that our methodeffectively improves system performance.1 IntroductionIn this paper we describe the system submitted tothe closed challenge of the CoNLL-2009 sharedtask on joint parsing of syntactic and semantic de-pendencies in multiple languages.Give a sentence, the task of dependency parsingis to identify the syntactic head of each word in thesentence and classify the relation between the de-pendent and its head.
The task of semantic rolelabeling is to label the senses of predicates in thesentence and labeling the semantic role of eachword in the sentence relative to each predicate.The difficulty of this shared task is to performjoint task on dependency parsing and semantic rolelabeling.
We split the shared task into four sub-problems: syntactic dependency parsing, syntacticdependency label classification, word sense disam-biguation, and semantic role labeling.
And we pro-pose a novel iterative approach to perform the jointtask.
In the first step, the system performs depend-ency parsing and semantic role labeling in a pipe-lined manner and the four sub-problems extractfeatures based on the known information.
In theiterative step, the system performs the four tasks ina pipelined manner but uses features extractedfrom the previous parsing result.The remainder of the paper is structured as fol-lows.
Section 2 presents the technical details of oursystem.
Section 3 presents experimental results andthe performance analysis.
Section 4 looks into afew issues concerning our forthcoming work forthis shared task, and concludes the paper.2 System descriptionThis section briefly describes the main componentsof our system: a) system flow; b) syntactic parsing;c) semantic role labeling; d) an iterative approachto perform joint syntactic-semantic parsing.2.1 System flowAs many systems did in CoNLL Shared Task 2008,the most direct way for such task is pipeline ap-proach.
First, Split the system into four subtasks:syntactic dependency parsing, syntactic depend-ency relation labeling, predicate sense labeling andsemantic role labeling.
Then, execute them one byone.
In our system, we extend this pipeline systemto an iterative system so that it can do a joint label-ing to improve the performance.Our iterative system is based on the pipelinesystem.
For the first iteration (original step), weuse the pipeline system to parse and label the19whole sentence.
For the rest iterations (iterativestep), we use another pipeline system to parse andlabel it.
The structure of this pipeline is the same asthe original one, but each subtask can have muchmore features than the original subtask.
Becausethe whole sentence has been labeled in the originalstep, all information is available for every subtask.For example, when doing syntactic dependencyrelation labeling, we can add some features aboutsense and semantic role.
It seems like using syntac-tic results to do semantic labeling, then using se-mantic results to improve syntactic labeling.
Thisis the core idea of our joint system.
Figure 1 showsthe main flow of our system.Figure 1.
The main flow of iteration system2.2 Dependency ParsingIn the dependency parsing step, we split the taskinto two sub-problems: syntactic dependency pars-ing and syntactic dependency relation labeling.In the syntactic dependency parsing stage,MSTParser1, a dependency parser that searches formaximum spanning trees over directed graphs, isapplied.
Due to the differences between the sevenlanguages, we use different parameters to train aparsing model.
Specifically, as Czech and Germanlanguages are none-projective and the others areprojective, we train Czech and German languageswith parameter ?none-projective?
and the otherswith ?projective?.On the syntactic dependency label classificationstep, we used the max-entropy classification algo-rithm to train the model.
This step contains twoprocesses.
In the first process the sub-problemtrains the model with the following basic features:StartEndSyntactic dependencyparsingSyntactic dependencyrelation labelingSet count = iterate timesSet isIterStep = falsePredicate sense label-ingSemantic role labelingcount --isIterStep = truecount = 0YGet fea-tures:this stepreturn thefeature ofsystemjudge bythe type ofsub taskand theparameterisIterStep.N?
FORM1: FORM of the head.?
LEMMA1: LEMMA of the head.?
STEM1 (English only): STEM of the head.?
POS1: POS of the head.?
IS_PRED1: the value of FILLPRED of thehead.?
FEAT1: FEAT of the head.?
LM_STEM1 (English only): the left-mostmodifier?s STEM of head.?
LM_POS1: the left-most modifier?s POSof head.?
L_NUM1: number of the head?s left modi-fiers.?
RM_STEM1 (English only): the right-most modifier?s STEM of head.?
RM_POS1: the right-most modifier?s POSof head.?
M_NUM1: number of modifiers of thehead.?
SUFFIX1 (English only): suffix of thehead.?
FORM2: FORM of the dependent.?
LEMMA2: LEMMA of the dependent.?
STEM2 (English only): STEM of the de-pendent.?
POS2: POS of the dependent.?
IS_PRED2: the value of FILLPRED of thedependent.1 http://sourceforge.net/projects/mstparser20?
FEAT2: FEAT of the dependent.?
LM_STEM2 (English only): the left-mostmodifier?s STEM of dependent.?
LM_POS2: the left-most modifier?s POSof dependent.?
L_NUM2:  number of the dependent?s leftmodifiers.?
RM_STEM2 (English only): the right-most modifier?s STEM of dependent.?
RM_POS2: the right-most modifier?s POSof dependent.?
M_NUM2: number of modifiers of the de-pendent.?
SUFFIX2 (English only): suffix of the de-pendent.?
DEP_PATH_ROOT_POS2: POS list fromdependent to tree?s root through the syn-tactic dependency path.?
DEP_PATH_ROOT_LEN2: length fromdependent to tree?s root through the syn-tactic dependency path.?
POSITION: The position of the word withrespect to its predicate.
It has three values,?before?, ?is?
and ?after?, for the predicate.In the iterative step, in addition to the featuresmentioned above, the sub-task trains the modelwith the following features:?
DEP_PATH_ROOT_POS1: POS list fromhead to tree?s root through the syntacticdependency path.?
DEP_PATH_ROOT_REL1: length fromdependent to tree?s root through the syn-tactic dependency path.?
PRED_POS: POS list of all predicates inthe sentence.?
FORM2 + DEP_PATH_REL: componentof FORM2 and the POS list from head tothe dependent through the syntactic de-pendency path.?
POSITION + FORM2?
STEM1 + FORM2 (English only)?
STEM1 + STEM2 (English only)?
POSITION + POS2?
ROLE_LIST2: list of APRED when thedependent is a predicate.?
ROLE: list of APRED and PRED whenthe head is predicate.?
L_ROLE: the nearest semantic role in itsleft side when head is a predicate.?
R_ROLE: the nearest semantic role in itsright side when head is a predicate.?
IS_ROLE1: whether dependent is a se-mantic role of head when head is a predi-cate.2.3 Semantic role labelingUnlike CoNLL-2008 shared task, this shared taskdoes not need to identify predicates.
So the maintask of this step is to label the sense of each predi-cate and label the semantic role for each predicate.When labeling the sense of each predicate, webuild a classification model for each predicate.
Asthe senses of different predicates are usually unre-lated even if they have the same sense label, thismakes it difficult for us to use only one classifier tolabel them.
But this approach leads to another issue.The set of predicates in the training set cannotcover all predicates.
For new predicates in the testset, no classification model can be found for them,and we build a most common sense for them.
Thefeatures we used are as follow:?
DEPREL1: DEPREL of the predicate.?
STEM1?
POS1?
RM_STEM1 (English only)?
RM_POS1?
FORM2?
POS2?
SUFFIX2?
VOICE (English only): VOICE of predi-cate.?
POSITION + POS2?
L_POS1 + POS1 + R_POS1: componentof left word?s POS and predicate POS andright word?s POS.?
FORM2 + DEP_PATH_REL?
DEP_PATH_ROOT_POS1?
DEP_PATH_ROOT_REL1When labeling the semantic role, we use a simi-lar approach as we did in CoNLL Shared Task2008.
However, as the frames information is notsupplied for all languages, we do not use it in thistask.
The features we use are as follows:?
DEPREL1?
STEM1 (English only)?
POS1?
RM_STEM1 (English only)?
RM_POS121?
FORM2?
POS2?
SUFFIX2?
VOICE2 (English only)?
POSITION?
DEP_PATH_REL?
DEP_PATH_POS?
SENSE2?
SENSE2 + VOICE2?
POSITION +  VOICE2?
DEP_PATH_LEN?
DEP_PATH_ROOT_REL1Moreover, we build an iterative model in thisshared task.
When doing an iterative labeling, theprevious labeling results are known.
So we candesign some new features for checking the previ-ous results in a global view.
The features we addfor the iterative model are as follows:?
SENSE1: SENSE of the predicate.?
SENSE1 + VOICE1: component of theSENSE + VOICE of predicate.?
VOICE1 + FORM1: component of VOICEand FORM.?
ROLE_LIST1: list of APRED of predicate.2.4 Iterative ApproachAs described above, some subtasks have twogroups of features.
One is for the pipeline model,and the other is for the iterative model.
The usageof these two types of model is the same.
The onlydifference is that they use different features.
Theiterative model can get more information, so theycan use more features.
These additional featurescan contain some joint and global (like frame andglobal structure) information.
The performancemay be improved because the viewer is extended.Some structural error and semantic conflict can befixed.Although the usage of the two types of model isthe same, there are some differences when buildingthe models.In the iterative step, all information is availablefor doing parsing and labeling.
For example, whendoing syntactic dependency relation labeling in theiterative step, the fields ?HEAD?, ?DEPREL?,?PRED?
and ?APREDs?
are filled by the perviousiteration.
So all these information can be used inthe iterative step.
This will cause one issue: use?HEAD1?
to label ?HEAD2?.
When training themodel, ?HEAD1?
is golden.
The classifier willbuild a model directly and let ?HEAD2?
equal to?HEAD1?.
However, in the iterative step,?HEAD1?
is not golden, but such model makes itimpossible to change the results..
The iterative stepwill be useless.We design a simple method to avoid this issue.?
Firstly, split the training set into N (N>1)subsets.?
Secondly, for each subset, use the left N-1subsets to build an original sub-model (usefeatures in the pipeline step).?
Thirdly, use each sub-model to label thecorresponding subset.?
Lastly, use these labeled N subsets to ex-tract samples (use features in the iterativestep) for building the iterative model.In this way, the ?HEAD1?
is not golden anymore.
And for each sub-task, we can use the simi-lar method to build the original model and the it-erative model.Moreover, in our system, we only build the it-erative models for syntactic dependency relationlabeling and semantic role labeling.
For syntacticdependency parsing, we use an approach with veryhigh time and space complexity, so it is not addedto the iterative step.
Thus, its results will not bechanged in the iterative step.
For sense labeling,we build classification models for every predicate.There are too many models and each model con-tains only a few classes.
We think they are notsuitable for building the iterative model.
But, as itsprevious sub-task (syntactic dependency relationlabeling) is added to the iterative step, it is usefulto add it to the iterative step.
Though we do notbuild an iterative model for sense labeling, we candirectly use its pipeline model.
This is another ad-vantage of our iterative model: if one subtask is notsuitable for doing iterative labeling/parsing, we canuse its pipeline model instead.3 Experiments and ResultsWe have tested our system with the test set andobtained official results as shown in Table 1.
Wehave tried to find how the iterative step influencessyntactic dependency parsing and semantic rolelabeling.
For syntactic dependency parsing andsemantic role labeling, we do experiments on thetest set.22Macro F1 ScoreAverage 73.98Catalan 72.09Chinese 72.72Czech 67.14English 81.89German 75.00Japanese 80.89Spanish 68.14Table 1.
The Macro F1 Score of every languages andthe average value.3.1 Syntactic Dependency ParsingDependency Parsing can be split into two sub-problems: syntactic dependency parsing and syn-tactic dependency label classification.
We use theiterative method on syntactic dependency labelclassification.
We do experiments on the test set.On the test set, we do two group experiments.
Inthe first group, we build a subtest to test this sub-task only.
All other information is given, and wejust label the dependency relation.
The results areshown in Table 2.
The row of ?Initial step?
showsthe results of this sub task in the original step.
Theleft two rows show the results in the iterative stepwith iterating once and twice.
The table shows thatthe iterative approach improves the performance.Especially for Catalan, the performance increasesby 2.89%.Certainly, in the whole system, this subtask can-not get golden information about sense and seman-tic roles.
So we test it in the whole system (jointtest) on the test set in the second group of experi-ments.
As shown in Table 3, the iterative step isnot as good as previous test.
But it is still useful forsome languages.
The reason that some languageshave no improvements on the iterative step is thatthe result of the initial step is not so good.3.2 Semantic Role LabelingLike syntactic dependency parsing, we do two testson Semantic Role Labeling.
This result is not con-sistent with the official data because we have add-ed some features of the subtask.
The results ofsubtest can be found in Table 4.
And Table 5shows the results of the joint test.
These twogroups of results show that the advantage of theiterative step is not as good as that of syntactic de-pendency labeling in subtest.
But it improves theperformance for most languages.
The iterative stepimproves the performance in both two tests.3.3 Analysis of ResultsFrom the experimental results, we can see that theeffect of each part of the iterative step depends onthe overall labeling result of the previous step.
Andthe labeling effect varies with different languages.Iterative approach can improve the performance ofthe system but it strongly depends on the initiallabeling result.4 Conclusion and Future WorkThis paper has presented a simple discriminativesystem submitted to the CoNLL-2009 shared taskto address the learning task of syntactic and seman-tic dependencies.
The paper first describes how tocarry out syntactic dependency parsing and seman-tic role labeling, and then a new iterative approachis presented for joint parsing.
The experimentalresults show that the iterative process can improvethe labeling accuracy on syntactic and semanticanalysis.
However, this approach probably dependson the accuracy of the initial labeling results.
Theresults of the initial labeling results will affect theeffect of the iterative process.Because of time constraints and inadequate ex-perimental environment, our first results do notmeet our expectation, and the effect of the iterativestep is not so clear.
Next, we will strive to refineour approach to produce good results for the syn-tactic dependency parsing, since it has a great im-pact on the final parsing results.AcknowledgmentsThe authors would like to thank the reviewers fortheir helpful comments.
This work was supportedby National Natural Science Foundation of China(No.60573077, No.60775037) and the NationalHigh Technology Research and Development Pro-gram of China (863 Program) (grant no.2009AA01Z123).
We also thank the High-Performance Center of USTC for providing uswith the experimental platform.23Average Catalan Chinese Czech Czech-ood English English-ood German German-ood Japanese SpanishInitial step 93.64 95.66 95.01 88.10 88.10 96.79 92.98 96.41 89.71* 98.17 95.48Iteration 1 94.60 98.56* 96.08* 88.59 88.29 97.31* 94.57* 96.63* 89.31 98.34 98.30Iteration 2 94.65 98.55 96.08* 88.68* 88.45* 97.29 94.56 96.63* 89.53 98.35* 98.33*Table 2.
The subtest result of Labeled Syntactic Accuracy of each language and the average performance valueon test set.
(* denotes the best score for the system)Average Catalan Chinese Czech Czech-ood English English-ood German German-ood Japanese SpanishInitial step 74.02 77.75 73.81 58.69* 55.50* 84.75 78.85 82.45 66.27* 90.45* 71.64Iteration 1 73.90 77.82 73.86* 58.17 54.95 84.81 78.95 82.51* 65.78 90.43 71.68Iteration 2 73.94 77.85* 73.86* 58.31 55.13 84.82* 79.02* 82.46 65.85 90.45* 71.69*Table 3.
The joint test result of Labeled Syntactic Accuracy of each language and the average performance valueon test set.
(* denotes the best score for the system)Average Catalan Chinese Czech Czech-ood English English-ood German German-ood Japanese SpanishInitial step 83.83 88.56 85.86 88.08 86.20* 86.23 82.09 80.98 78.82 74.32* 87.45Iteration 1 84.34 89.02* 87.14* 87.88 86.09 86.66 82.07 83.66* 79.28* 74.06 87.59Iteration 2 84.36 89.02* 87.01 88.10* 86.17 86.78* 82.34* 83.15 79.18 74.06 87.81*Table 4.
The sub test result of Semantic Labeled F1 of each language and the average performance value on testset.
(* denotes the best score for the system)Average Catalan Chinese Czech Czech-ood English English-ood German German-ood Japanese SpanishInitial step 70.01 66.87 71.63 75.50 75.71 78.97 69.87 67.50 58.47 70.91* 64.64Iteration 1 70.15 67.12 71.98 75.54 75.68 79.40 70.17* 68.08* 58.55* 70.69 64.32Iteration 2 70.20 67.33* 71.99* 75.65* 75.90* 79.47* 69.98 67.98 58.33 70.70 64.65*Table 5.
The joint test result of Semantic Labeled F1 of each language and the average performance value on testset.
(* denotes the best score for the system)ReferencesJan Haji?, Massimiliano Ciaramita, Richard Johansson,Daisuke Kawahara, Maria Antonia Mart?, Llu?sM?rquez, Adam Meyers, Joakim Nivre, SebastianPad?, Jan ?t?p?nek, Pavel Stra?
?k, Mihai Surdeanu,Nianwen Xue and Yi Zhang.
2009.
The CoNLL-2009Shared Task: Syntactic and Semantic Dependenciesin Multiple Languages.
Proceedings of the 13th Con-ference on Computational Natural Language Learn-ing (CoNLL-2009).
Boulder, Colorado, USA.
June 4-5. pp.
3-22.Mariona Taul?, Maria Ant?nia Mart?
and Marta Re-casens.
2008.
AnCora: Multilevel Annotated Corporafor Catalan and Spanish.
Proceedings of the 6th In-ternational Conference on Language Resources andEvaluation (LREC-2008).
Marrakech, Morocco.Nianwen Xue and Martha Palmer.
2009.
Adding seman-tic roles to the Chinese Treebank.
Natural LanguageEngineering, 15(1):143-172.Jan Haji?, Jarmila Panevov?, Eva Haji?ov?, Petr Sgall,Petr Pajas, Jan ?t?p?nek, Ji??
Havelka, Marie Miku-lov?
and Zden?k ?abokrtsk?.
2006.
The Prague De-pendency Treebank 2.0.
CD-ROM.
Linguistic DataConsortium, Philadelphia, Pennsylvania, USA.
ISBN1-58563-370-4.
LDC Cat.
No.
LDC2006T01.
URL:http://ldc.upenn.edu.Surdeanu, Mihai, Richard Johansson, Adam Meyers,Llu?s M?rquez, and Joakim Nivre.
2008.
TheCoNLL-2008 Shared Task on Joint Parsing of Syn-tactic and Semantic Dependencies.
In Proceedings ofthe 12th Conference on Computational Natural Lan-guage Learning (CoNLL-2008).Aljoscha Burchardt, Katrin Erk, Anette Frank, AndreaKowalski, Sebastian Pad?
and Manfred Pinkal.
2006.The SALSA Corpus: a German Corpus Resource forLexical Semantics.
Proceedings of the 5th Interna-tional Conference on Language Resources andEvaluation (LREC-2006).
Genoa, Italy.Daisuke Kawahara, Sadao Kurohashi and Koiti Hasida.2002.
Construction of a Japanese Relevance-taggedCorpus.
Proceedings of the 3rd International Confer-ence on Language Resources and Evaluation (LREC-2002).
Las Palmas, Spain.
pp.
2008-2013.McDonald, Ryan.
2006.
Discriminative learning andSpanning Tree Algorithms for Dependency    parsing.Ph.D.
thesis, University of Pennyslvania.Stanley F. Chen and Ronald Rosenfeld.
1999.
A gaus-sian prior for smoothing maximum entropy models.Technical.
Report CMU-CS-99-108.24
