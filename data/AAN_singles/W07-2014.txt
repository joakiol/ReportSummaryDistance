Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 75?80,Prague, June 2007. c?2007 Association for Computational LinguisticsSemEval-2007 Task 15: TempEval Temporal Relation IdentificationMarc Verhagen?, Robert Gaizauskas?, Frank Schilder?, Mark Hepple?,Graham Katz?
and James Pustejovsky??
Brandeis University, {marc,jamesp}@cs.brandeis.edu?
University of Sheffield, {r.gaizauskas,m.hepple}@dcs.shef.ac.uk?
Thomson Legal & Regulatory, frank.schilder@thomson.com,?
Stanford University, egkatz@stanford.eduAbstractThe TempEval task proposes a simple wayto evaluate automatic extraction of temporalrelations.
It avoids the pitfalls of evaluat-ing a graph of inter-related labels by defin-ing three sub tasks that allow pairwise eval-uation of temporal relations.
The task notonly allows straightforward evaluation, italso avoids the complexities of full tempo-ral parsing.1 IntroductionNewspaper texts, narratives and other texts describeevents that occur in time and specify the temporallocation and order of these events.
Text comprehen-sion, amongst other capabilities, clearly requires thecapability to identify the events described in a textand locate these in time.
This capability is crucial toa wide range of NLP applications, from documentsummarization and question answering to machinetranslation.Recent work on the annotation of events and tem-poral relations has resulted in both a de-facto stan-dard for expressing these relations and a hand-builtgold standard of annotated texts.
TimeML (Puste-jovsky et al, 2003a) is an emerging ISO standardfor annotation of events, temporal expressions andthe anchoring and ordering relations between them.TimeBank (Pustejovsky et al, 2003b; Boguraev etal., forthcoming) was originally conceived of as aproof of concept that illustrates the TimeML lan-guage, but has since gone through several rounds ofrevisions and can now be considered a gold standardfor temporal information.
TimeML and TimeBankhave already been used as the basis for automatictime, event and temporal relation annotation tasks ina number of research projects in recent years (Maniet al, 2006; Boguraev et al, forthcoming).An open evaluation challenge in the area of tem-poral annotation should serve to drive research for-ward, as it has in other areas of NLP.
The auto-matic identification of all temporal referring expres-sions, events and temporal relations within a text isthe ultimate aim of research in this area.
However,addressing this aim in a first evaluation challengewas judged to be too difficult, both for organizersand participants, and a staged approach was deemedmore effective.
Thus we here present an initial eval-uation exercise based on three limited tasks that webelieve are realistic both from the perspective of as-sembling resources for development and testing andfrom the perspective of developing systems capableof addressing the tasks.
They are also tasks, whichshould they be performable automatically, have ap-plication potential.2 Task DescriptionThe tasks as originally proposed were modifiedslightly during the course of resource developmentfor the evaluation exercise due to constraints on dataand annotator availability.
In the following we de-scribe the tasks as they were ultimately realized inthe evaluation.There were three tasks ?
A, B and C. For allthree tasks the data provided for testing and train-ing includes annotations identifying: (1) sentenceboundaries; (2) all temporal referring expression as75specified by TIMEX3; (3) all events as specifiedin TimeML; (4) selected instances of temporal re-lations, as relevant to the given task.
For tasks A andB a restricted set of event terms were identified ?those whose stems occurred twenty times or more inTimeBank.
This set is referred to as the Event TargetList or ETL.TASK A This task addresses only the temporal re-lations holding between time and event expressionsthat occur within the same sentence.
Furthermoreonly event expressions that occur within the ETL areconsidered.
In the training and test data, TLINK an-notations for these temporal relations are provided,the difference being that in the test data the relationtype is withheld.
The task is to supply this label.TASK B This task addresses only the temporalrelations holding between the Document CreationTime (DCT) and event expressions.
Again onlyevent expressions that occur within the ETL are con-sidered.
As in Task A, TLINK annotations for thesetemporal relations are provided in both training andtest data, and again the relation type is withheld inthe test data and the task is to supply this label.TASK C Task C relies upon the idea of their beinga main event within a sentence, typically the syn-tactically dominant verb.
The aim is to assign thetemporal relation between the main events of adja-cent sentences.
In both training and test data themain events are identified (via an attribute in theevent annotation) and TLINKs between these mainevents are supplied.
As for Tasks A and B, the taskhere is to supply the correct relation label for theseTLINKs.3 Data Description and Data PreparationThe TempEval annotation language is a simplifiedversion of TimeML 1.
For TempEval, we use the fol-lowing five tags: TempEval, s, TIMEX3, EVENT,and TLINK.
TempEval is the document root and smarks sentence boundaries.
All sentence tags in theTempEval data are automatically created using theAlembic Natural Language processing tools.
Theother three tags are discussed here in more detail:1See http://www.timeml.org for language specifica-tions and annotation guidelines?
TIMEX3.
Tags the time expressions in the text.It is identical to the TIMEX3 tag in TimeML.See the TimeML specifications and guidelinesfor further details on this tag and its attributes.Each document has one special TIMEX3 tag,the Document Creation Time, which is inter-preted as an interval that spans a whole day.?
EVENT.
Tags the event expressions in the text.The interpretation of what an event is is takenfrom TimeML where an event is a cover termfor predicates describing situations that happenor occur as well as some, but not all, stativepredicates.
Events can be denoted by verbs,nouns or adjectives.
The TempEval event an-notation scheme is somewhat simpler than thatused in TimeML, whose complexity was de-signed to handle event expressions that intro-duced multiple event instances (consider, e.g.He taught on Wednesday and Friday).
Thiscomplication was not necessary for the Tem-pEval data.
The most salient attributes encodetense, aspect, modality and polarity informa-tion.
For TempEval task C, one extra attributeis added: mainevent, with possible valuesYES and NO.?
TLINK.
This is a simplified version of theTimeML TLINK tag.
The relation types for theTimeML version form a fine-grained set basedon James Allen?s interval logic (Allen, 1983).For TempEval, we use only six relation typesincluding the three core relations BEFORE, AF-TER, and OVERLAP, the two less specific re-lations BEFORE-OR-OVERLAP and OVERLAP-OR-AFTER for ambiguous cases, and finally therelation VAGUE for those cases where no partic-ular relation can be established.As stated above the TLINKs of concern for eachtask are explicitly included in the training and in thetest data.
However, in the latter the relType at-tribute of each TLINK is set to UNKNOWN.
For eachtask the system must replace the UNKNOWN valueswith one of the six allowed values listed above.The EVENT and TIMEX3 annotations were takenverbatim from TimeBank version 1.2.2 The annota-2TimeBank 1.2 is available for free through the LinguisticData Consortium, see http://www.timeml.org for more76tion procedure for TLINK tags involved dual anno-tation by seven annotators using a web-based anno-tation interface.
After this phase, three experiencedannotators looked at all occurrences where two an-notators differed as to what relation type to selectand decided on the best option.
For task C, therewas an extra annotation phase where the main eventswere marked up.
Main events are those events thatare syntactically dominant in the sentences.It should be noted that annotation of temporal re-lations is not an easy task for humans due to ram-pant temporal vagueness in natural language.
As aresult, inter-annotator agreement scores are well be-low the often kicked-around threshold of 90%, bothfor the TimeML relation set as well as the TempEvalrelation set.
For TimeML temporal links, an inter-annotator agreement of 0.77 was reported, whereagreement was measured by the average of preci-sion and recall.
The numbers for TempEval are evenlower, with an agreement of 0.72 for anchorings ofevents to times (tasks A and B) and an agreement of0.65 for event orderings (task C).
Obviously, num-bers like this temper the expectations for automatictemporal linking.The lower number for TempEval came a bit asa surprise because, after all, there were fewer rela-tions to choose form.
However, the TempEval an-notation task is different in the sense that it did notgive the annotator the option to ignore certain pairsof events and made it therefore impossible to skiphard-to-classify temporal relations.4 Evaluating Temporal RelationsIn full temporal annotation, evaluation of temporalannotation runs into the same issues as evaluation ofanaphora chains: simple pairwise comparisons maynot be the best way to evaluate.
In temporal annota-tion, for example, one may wonder how the responsein (1) should be evaluated given the key in (2).
(1) {A before B, A before C, B equals C}(2) {A after B, A after C, B equals C}Scoring (1) at 0.33 precision misses the interde-pendence between the temporal relations.
What weneed to compare is not individual judgements buttwo partial orders.details.For TempEval however, the tasks are defined ina such a way that a simple pairwise comparison ispossible since we do not aim to create a full temporalgraph and judgements are made in isolation.Recall that there are three basic temporal relations(BEFORE, OVERLAP, and AFTER) as well as threedisjunctions over this set (BEFORE-OR-OVERLAP,OVERLAP-OR-AFTER and VAGUE).
The additionof these disjunctions raises the question of how toscore a response of, for example, BEFORE given akey of BEFORE-OR-OVERLAP.
We use two scor-ing schemes: strict and relaxed.
The strict scoringscheme only counts exact matches as success.
Forexample, if the key is OVERLAP and the responseBEFORE-OR-OVERLAP than this is counted as fail-ure.
We can use standard definitions of precisionand recallPrecision = Rc/RRecall = Rc/Kwhere Rc is number of correct answers in the re-sponse, R the total number of answers in the re-sponse, and K the total number of answers in thekey.
For the relaxed scoring scheme, precision andrecall are defined asPrecision = Rcw/RRecall = Rcw/Kwhere Rcw reflects the weighted number of correctanswers.
A response is not simply counted as 1 (cor-rect) or 0 (incorrect), but is assigned one of the val-ues in table 1.B O A B-O O-A VB 1 0 0 0.5 0 0.33O 0 1 0 0.5 0.5 0.33A 0 0 1 0 0.5 0.33B-O 0.5 0.5 0 1 0.5 0.67O-A 0 0.5 0.5 0.5 1 0.67V 0.33 0.33 0.33 0.67 0.67 1Table 1: Evaluation weightsThis scheme gives partial credit for disjunctions,but not so much that non-commitment edges out pre-cise assignments.
For example, assigning VAGUE asthe relation type for every temporal relation resultsin a precision of 0.33.775 ParticipantsSix teams participated in the TempEval tasks.
Threeof the teams used statistics exclusively, one used arule-based system and the other two employed a hy-brid approach.
This section gives a short descriptionof the participating systems.CU-TMP trained three support vector machine(SVM) models, one for each task.
All models usedthe gold-standard TimeBank features for events andtimes as well as syntactic features derived from thetext.
Additionally, the relation types obtained byrunning the task B system on the training data forTask A and Task C, were added as a feature to thetwo latter systems.
A subset of features was selectedusing cross-validations on the training data, dis-carding features whose removal improved the cross-validation F-score.
When applied to the test data,the Task B system was run first in order to supplythe necessary features to the Task A and Task C sys-tems.LCC-TE automatically identifies temporal refer-ring expressions, events and temporal relations intext using a hybrid approach, leveraging variousNLP tools and linguistic resources at LCC.
For tem-poral expression labeling and normalization, theyused a syntactic pattern matching tool that deploys alarge set of hand-crafted finite state rules.
For eventdetection, they used a small set of heuristics as wellas a lexicon to determine whether or not a token isan event, based on the lemma, part of speech andWordNet senses.
For temporal relation discovery,LCC-TE used a large set of syntactic and semanticfeatures as input to a machine learning components.NAIST-japan defined the temporal relation iden-tification task as a sequence labeling model, inwhich the target pairs ?
a TIMEX3 and an EVENT?
are linearly ordered in the document.
For analyz-ing the relative positions, they used features fromdependency trees which are obtained from a depen-dency parser.
The relative position between the tar-get EVENT and a word in the target TIMEX3 is usedas a feature for a machine learning based relationidentifier.
The relative positions between a word inthe target entities and another word are also intro-duced.The USFD system uses an off-the-shelf MachineLearning suite(WEKA), treating the assignment oftemporal relations as a simple classification task.The features used were the ones provided in theTempEval data annotation together with a few fea-tures straightforwardly computed from the docu-ment without any deeper NLP analysis.WVALI?s approach for discovering intra-sentence temporal relations relies on sentence-levelsyntactic tree generation, bottom-up propaga-tion of the temporal relations between syntacticconstituents, a temporal reasoning mechanismthat relates the two targeted temporal entities totheir closest ancestor and then to each other, andon conflict resolution heuristics.
In establishingthe temporal relation between an event and theDocument Creation Time (DCT), the temporal ex-pressions directly or indirectly linked to that eventare first analyzed and, if no relation is detected,the temporal relation with the DCT is propagatedtop-down in the syntactic tree.
Inter-sentence tem-poral relations are discovered by applying severalheuristics and by using statistical data extractedfrom the training corpus.XRCE-T used a rule-based system that relies ona deep syntactic analyzer that was extended to treattemporal expressions.
Temporal processing is inte-grated into a more generic tool, a general purposelinguistic analyzer, and is thus a complement for abetter general purpose text understanding system.Temporal analysis is intertwined with syntactico-semantic text processing like deep syntactic analy-sis and determination of thematic roles.
TempEval-specific treatment is performed in a post-processingstage.6 ResultsThe results for the six teams are presented in tables2, 3, and 4.team strict relaxedP R F P R FCU-TMP 0.61 0.61 0.61 0.63 0.63 0.63LCC-TE 0.59 0.57 0.58 0.61 0.60 0.60NAIST 0.61 0.61 0.61 0.63 0.63 0.63USFD* 0.59 0.59 0.59 0.60 0.60 0.60WVALI 0.62 0.62 0.62 0.64 0.64 0.64XRCE-T 0.53 0.25 0.34 0.63 0.30 0.41average 0.59 0.54 0.56 0.62 0.57 0.59stddev 0.03 0.13 0.10 0.01 0.12 0.08Table 2: Results for Task A78team strict relaxedP R F P R FCU-TMP 0.75 0.75 0.75 0.76 0.76 0.76LCC-TE 0.75 0.71 0.73 0.76 0.72 0.74NAIST 0.75 0.75 0.75 0.76 0.76 0.76USFD* 0.73 0.73 0.73 0.74 0.74 0.74WVALI 0.80 0.80 0.80 0.81 0.81 0.81XRCE-T 0.78 0.57 0.66 0.84 0.62 0.71average 0.76 0.72 0.74 0.78 0.74 0.75stddev 0.03 0.08 0.05 0.03 0.06 0.03Table 3: Results for Task Bteam strict relaxedP R F P R FCU-TMP 0.54 0.54 0.54 0.58 0.58 0.58LCC-TE 0.55 0.55 0.55 0.58 0.58 0.58NAIST 0.49 0.49 0.49 0.53 0.53 0.53USFD* 0.54 0.54 0.54 0.57 0.57 0.57WVALI 0.54 0.54 0.54 0.64 0.64 0.64XRCE-T 0.42 0.42 0.42 0.58 0.58 0.58average 0.51 0.51 0.51 0.58 0.58 0.58stddev 0.05 0.05 0.05 0.04 0.04 0.04Table 4: Results for Task CAll tables give precision, recall and f-measure forboth the strict and the relaxed scoring scheme, aswell as averages and standard deviation on the pre-cision, recall and f-measure numbers.
The entry forUSFD is starred because the system developers areco-organizers of the TempEval task.3For task A, the f-measure scores range from 0.34to 0.62 for the strict scheme and from 0.41 to 0.63for the relaxed scheme.
For task B, the scores rangefrom 0.66 to 0.80 (strict) and 0.71 to 0.81 (relaxed).Finally, task C scores range from 0.42 to 0.55 (strict)and from 0.56 to 0.66 (relaxed).The differences between the systems is not spec-tacular.
WVALI?s hybrid approach outperforms theother systems in task B and, using relaxed scoring,in task C as well.
But for task A, the winners barelyedge out the rest of the field.
Similarly, for task Cusing strict scoring, there is no system that clearlyseparates itself from the field.It should be noted that for task A, and in lesser ex-tent for task B, the XRCE-T system has recall scoresthat are far below all other systems.
This seemsmostly due to a choice by the developers to not as-sign a temporal relation if the syntactic analyzer didnot find a clear syntactic relation between the two3There was a strict separation between people assisting inthe annotation of the evaluation corpus and people involved insystem development.elements that needed to be linked for the TempEvaltask.7 Conclusion: the Future of TemporalEvaluationThe evaluation approach of TempEval avoids the in-terdependencies that are inherent to a network oftemporal relations, where relations in one part of thenetwork may constrain relations in any other part ofthe network.
To accomplish that, TempEval delib-erately focused on subtasks of the larger problem ofautomatic temporal annotation.One thing we may want to change to the presentTempEval is the definition of task A.
Currently, itinstructs to temporally link all events in a sentenceto all time expressions in the same sentence.
In thefuture we may consider splitting this into two tasks,where one subtask focuses on those anchorings thatare very local, like ?...White House spokesman Mar-lin Fitzwater [said] [late yesterday] that...?.
We ex-pect both inter-annotator agreement and system per-formance to be higher on this subtask.There are two research avenues that loom beyondthe current TempEval: (1) definition of other sub-tasks with the ultimate goal of establishing a hierar-chy of subtasks ranked on performance of automatictaggers, and (2) an approach to evaluate entire time-lines.Some other temporal linking tasks that can beconsidered are ordering of consecutive events in asentence, ordering of events that occur in syntacticsubordination relations, ordering events in coordi-nations, and temporal linking of reporting events tothe document creation time.
Once enough temporallinks from all these subtasks are added to the en-tire temporal graph, it becomes possible to let confi-dence scores from the separate subtasks drive a con-straint propagation algorithm as proposed in (Allen,1983), in effect using high-precision relations toconstrain lower-precision relations elsewhere in thegraph.With this more complete temporal annotation itis no longer possible to simply evaluate the entiregraph by scoring pairwise comparisons.
Insteadthe entire timeline must be evaluated.
Initial ideasregarding this focus on transforming the temporalgraph of a document into a set of partial orders built79around precedence and inclusion relations and thenevaluating each of these partial orders using somekind of edit distance measure.4We hope to have taken the first baby steps withthe three TempEval tasks.8 AcknowledgementsWe would like to thank all the people who helpedprepare the data for TempEval, listed here in noparticular order: Amber Stubbs, Jessica Littman,Hongyuan Qiu, Emin Mimaroglu, Emma Barker,Catherine Havasi, Yonit Boussany, Roser Saur?
?, andAnna Rumshisky.Thanks also to all participants to this new task:Steven Bethard and James Martin (University ofColorado at Boulder), Congmin Min, Munirath-nam Srikanth and Abraham Fowler (Language Com-puter Corporation), Yuchang Cheng, Masayuki Asa-hara and Yuji Matsumoto (Nara Institute of Scienceand Technology), Mark Hepple, Andrea Setzer andRob Gaizauskas (University of Sheffield), CarolineHageg`e and Xavier Tannier (XEROX Research Cen-tre Europe), and Georgiana Pus?cas?u (University ofWolverhampton and University of Alicante).Part of the work in this paper was funded bythe DTO/AQUAINT program under grant num-ber N61339-06-C-0140 and part funded by the EUVIKEF project (IST- 2002-507173).ReferencesJames Allen.
1983.
Maintaining knowledge abouttemporal intervals.
Communications of the ACM,26(11):832?843.Bran Boguraev, James Pustejovsky, Rie Ando, and MarcVerhagen.
forthcoming.
Timebank evolution as acommunity resource for timeml parsing.
LanguageResources and Evaluation.Inderjeet Mani, BenWellner, Marc Verhagen, ChongMinLee, and James Pustejovsky.
2006.
Machine learn-ing of temporal relations.
In Proceedings of the 44thAnnual Meeting of the Association for ComputationalLinguistics, Sydney, Australia.
ACL.James Pustejovsky, Jose?
Castan?o, Robert Ingria, RoserSaur?
?, Robert Gaizauskas, Andrea Setzer, and Gra-ham Katz.
2003a.
TimeML: Robust specification of4Edit distance was proposed by Ben Wellner as a way toevaluate partial orders of precedence relations (personal com-munication).event and temporal expressions in text.
In Proceedingsof the Fifth International Workshop on ComputationalSemantics (IWCS-5), Tilburg, January.James Pustejovsky, Patrick Hanks, Roser Saur?
?, AndrewSee, Robert Gaizauskas, Andrea Setzer, DragomirRadev, Beth Sundheim, David Day, Lisa Ferro, andMarcia Lazo.
2003b.
The TIMEBANK corpus.
InProceedings of Corpus Linguistics 2003, pages 647?656, Lancaster, March.80
