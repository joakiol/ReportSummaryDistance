Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 1066?1074,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPLanguage Identification of Search Engine QueriesHakan CeylanDepartment of Computer ScienceUniversity of North TexasDenton, TX, 76203hakan@unt.eduYookyung KimYahoo!
Inc.2821 Mission College Blvd.Santa Clara, CA, 95054ykim@yahoo-inc.comAbstractWe consider the language identificationproblem for search engine queries.
First,we propose a method to automaticallygenerate a data set, which uses click-through logs of the Yahoo!
Search En-gine to derive the language of a query indi-rectly from the language of the documentsclicked by the users.
Next, we use thisdata set to train two decision tree classi-fiers; one that only uses linguistic featuresand is aimed for textual language identi-fication, and one that additionally uses anon-linguistic feature, and is geared to-wards the identification of the languageintended by the users of the search en-gine.
Our results show that our methodproduces a highly reliable data set very ef-ficiently, and our decision tree classifieroutperforms some of the best methods thathave been proposed for the task of writtenlanguage identification on the domain ofsearch engine queries.1 IntroductionThe language identification problem refers to thetask of deciding in which natural language a giventext is written.
Although the problem is heav-ily studied by the Natural Language Processingcommunity, most of the research carried out todate has been concerned with relatively long textssuch as articles or web pages which usually con-tain enough text for the systems built for this taskto reach almost perfect accuracy.
Figure 1 showsthe performance of 6 different language identifi-cation methods on written texts of 10 Europeanlanguages that use the Roman Alphabet.
It canbe seen that the methods reach a very high ac-curacy when the text has 100 or more characters.However, search engine queries are very short inlength; they have about 2 to 3 words on average,Figure 1: Performance of six Language Identifica-tion methods on varying text size.
Adapted from(Poutsma, 2001).which requires a reconsideration of the existingmethods built for this problem.Correct identification of the language of thequeries is of critical importance to search engines.Major search engines such as Yahoo!
Search(www.yahoo.com), or Google (www.google.com)crawl billions of web pages in more than 50 lan-guages, and about a quarter of their queries are inlanguages other than English.
Therefore a correctidentification of the language of a query is neededin order to aid the search engine towards more ac-curate results.
Moreover, it also helps further pro-cessing of the queries, such as stemming or spellchecking of the query terms.One of the challenges in this problem is the lackof any standard or publicly available data set.
Fur-thermore, creating such a data set is expensive asit requires an extensive amount of work by hu-man annotators.
In this paper, we introduce a newmethod to overcome this bottleneck by automat-ically generating a data set of queries with lan-guage annotations.
We show that the data gener-ated this way is highly reliable and can be used totrain a machine learning algorithm.We also distinguish the problem of identifyingthe textual language vs. the language intended bythe users for the search engine queries.
For searchengines, there are cases where a correct identifi-1066cation of the language does not necessarily im-ply that the user wants to see the results in thesame language.
For example, although the textualidentification of the language for the query ?homosapiens?
is Latin, a user entering this query fromSpain, would most probably want to see Spanishweb pages, rather than web pages in Latin.
We ad-dress this issue by adding a non-linguistic featureto our system.We organize the rest of the paper as follows:First, we provide an overview of the previous re-search in this area.
Second, we present our methodto automatically generate a data set, and evaluatethe effectiveness of this technique.
As a result ofthis evaluation, we obtain a human-annotated dataset which we use to evaluate the systems imple-mented in the following sections.
In Section 4, weimplement some of the existing models and com-pare their performance on our test set.
We thenuse the results from these models to build a deci-sion tree system.
Next, we consider identifying thelanguage intended by the user for the results of thequery, and describe a system geared towards thistask.
Finally, we conclude our study and discussthe future directions for the problem.2 Related WorkMost of the work carried out to date on the writ-ten language identification problem consists of su-pervised approaches that are trained on a list ofwords or n-gram models for each reference lan-guage.
The word based approaches use a list ofshort words, common words, or a complete vocab-ulary which are extracted from a corpus for eachlanguage.
The short words approach uses a list ofwords with at most four or five characters; such asdeterminers, prepositions, and conjunctions, andis used in (Ingle, 1976; Grefenstette, 1995).
Thecommon words method is a generalization overthe short words one which, in addition, includesother frequently occuring words without limitingthem to a specific length, and is used in (Souter etal., 1994; Cowie et al, 1999).
For classification,the word-based approaches sort the list of words indescending order of their frequency in the corpusfrom which they are extracted.
Then the likelihoodof each word in a given text can be calculated byusing rank-order statistics or by transforming thefrequencies into probabilities.The n-gram based approaches are based on thecounts of character or byte n-grams, which are se-quences of n characters or bytes, extracted froma corpus for each reference language.
Differentclassification models that use the n-gram featureshave been proposed.
(Cavnar and Trenkle, 1994)used an out-of-place rank order statistic to mea-sure the distance of a given text to the n-gramprofile of each language.
(Dunning, 1994) pro-posed a system that uses Markov Chains of byte n-grams with Bayesian Decision Rules to minimizethe probability error.
(Grefenstette, 1995) simplyused trigram counts that are transformed into prob-abilities, and found this superior to the short wordstechnique.
(Sibun and Reynar, 1996) used Rela-tive Entropy by first generating n-gram probabil-ity distributions for both training and test data, andthen measuring the distance between the two prob-ability distributions by using the Kullback-LieblerDistance.
(Poutsma, 2001) developed a systembased on Monte Carlo Sampling.Linguini, a system proposed by (Prager, 1999),combines the word-based and n-gram models us-ing a vector-space based model and examines theeffectiveness of the combined model and the in-dividual features on varying text size.
Similarly,(Lena Grothe and Nrnberger, 2008) combines bothmodels using the ad-hoc method of (Cavnar andTrenkle, 1994), and also presents a comparisonstudy.
The work most closely related to ours ispresented very recently in (Hammarstro?m, 2007),which proposes a model that uses a frequency dic-tionary together with affix information in order toidentify the language of texts as short as one word.Other systems that use methods aside fromthe ones discussed above have also been pro-posed.
(Takci and Sogukpinar, 2004) used letterfrequency features in a centroid based classifica-tion model.
(Kruengkrai et al, 2005) proposed afeature based on alignment of string kernels us-ing suffix trees, and used it in two different clas-sifiers.
Finally, (Biemann and Teresniak, 2005)presented an unsupervised system that clusters thewords based on sentence co-occurence.Recently, (Hughes et al, 2006) surveyed theprevious work in this area and suggested that theproblem of language identification for written re-sources, although well studied, has too many openchallenges which requires a more systematic andcollaborative study.3 Data GenerationWe start the construction of our data set by re-trieving the queries, together with the clicked urls,from the Yahoo!
Search Engine for a three monthstime period.
For each language desired in our dataset, we retrieve the queries from the corresponding1067Yahoo!
web site in which the default language isthe same as the one sought.1 Then we preprocessthe queries by getting rid of the ones that have anynumbers or special characters in them, removingextra spaces between query terms, and lowercas-ing all the letters of the queries2.
Next, we ag-gregate the queries that are exactly the same, bycalculating the frequencies of the urls clicked foreach query.As we pointed out in Section 1, and illustratedin Figure 1, the language identification methodsgive almost perfect accuracy when the text has 100or more characters.
Furthermore, it is suggested in(Levering and Cutler, 2006) that the average tex-tual content in a web page is 474 words.
Thus weassume that it is a fairly trivial task to identify thelanguage for an average web page using one of theexisting methods.3 In our case, this task gets al-ready accomplished by the crawler for all the webpages crawled by the search engine.Thus we can summarize our information in twoseparate tables; T1 and T2.
For Table T1, we havea set of queries Q, and each q ?
Q maps to aset of url-frequency pairs.
Each mapping is of theform (q, u, fu), where u is a url clicked for q, andfu is the frequency of u.
Table T2, on the otherhand, contains the urls of all the web pages knownto the search engine and has only two columns;(u, l), where u is a unique url, and l is the languageidentified for u.
Since we do not consider multi-lingual web pages, every url in T2 is unique andhas only one language associated with it.Next, we combine the tables T1 and T2 usingan inner join operation on the url columns.
Afterthe join, we group the results by the language andquery columns, during which we also count thenumber of distinct urls per query, and sum theirfrequencies.
We illustrate this operation with aSQL query in Algorithm 1.
As a result of theseoperations, we have, for each query q ?
Q, a set oftriplets (l, fl, cu,l) where l is a language, fl is thecount of clicks for l (which we obtained throughthe urls in language l), and cu,l is the count ofunique urls in language l.The resulting table T3 associates queries withlanguages, but also contains a lot of noise.
First,1We do not make a distinction between the different di-alects of the same languge.
For English, Spanish and Por-tuguese we gather queries from the web sites of United States,Mexico, and Brazil respectively.2In this study, we only considered languages that use theRoman alphabet.3Although not done in this study, the urls of web pagesthat have less than a defined number of words, such as 100,can be discarded to ensure a higher confidence.Input: Tables T1:[q, u, fu], T2:[u, l]Output: Table T3:[q, l, fl, cu,l]CREATE VIEW T3 ASSELECTT1.q, T2.l, COUNT(T1.u) AS cu,l, SUM(T1.fu) AS flFROM T1INNER JOIN T2ON T1.u = T2.uGROUP BY q, l;Algorithm 1: Join Tables T1 and T2, group byquery and language, aggregate distinct url and fre-quency counts.we have queries that map to more than one lan-guage, which suggests that the users clicked on theurls in different languages for the same query.
Toquantify the strength of each of these mappings,we calculate a weight wq,l for each mapping of aquery q to a language l as:wq,l = fl/Fqwhere Fq, the total frequency of a query q, is de-fined as:Fq =?l?Lqflwhere Lq is the set of languages for which q has amapping.
Having computed a weight wq,l for eachmapping, we introduce our first threshold param-eter, W .
We eliminate all the queries in our dataset, which have weights, wq,l, below the thresholdW .Second, even though some of the queries map toonly one language, this mapping cannot be trusteddue to the high frequency of the queries togetherwith too few distinct urls.
This case suggests thatthe query is most likely navigational.
The intentof navigational queries, such as ?ACL 2009?, is tofind a particular web site.
Therefore they usuallyconsist of proper names, or acronyms that wouldnot be of much use to our language identificationproblem.
Hence we would like to get rid of thenavigational queries in our data set by using someof the features proposed for the task of automatictaxonomy of search engine queries.
For a moredetailed discussion of this task, we refer the readerto (Broder, 2002; Rose and Levinson, 2004; Lee etal., 2005; Liu et al, 2006; Jansen et al, 2008).Two of the features used in (Liu et al, 2006)in identification of the navigational queries fromclick-through data, are the number of Clicks Satis-fied (nCS) and number of Results Satisfied (nRS).In our problem, we substitute nCS with Fq, the to-tal click frequency of the query q, and nRS with1068Uq, the number of distinct urls clicked for q. Thuswe eliminate the queries that have a total click fre-quency above a given frequency threshold F , and,that have less than a given distinct number of urls,U .
Thus, we have three parameters that help us ineliminating the noise from the inital data; W , F ,and U .
We show the usage of these parameters inSQL queries, in Algorithm 2.Input: Tables T1:[q, u, fu], T2:[u, l], T3:[q, l, fl, cu,l]Parameters W , F , and UOutput: Table D:[q, l]CREATE VIEW T4 ASSELECT T1.q, COUNT(T1.u) AS cu, SUM(T1.fu) AS FqFROM T1INNER JOIN T2 ON T1.u = T2.uGROUP BY q;CREATE VIEW D ASSELECT T3.q, T3.l, T3.fl / T4.Fq AS wq,lFROM T1INNER JOIN T4 ON T3.q = T4.q 10WHERET4.Fq < F ANDwq,l >= W ANDT4.cu,l >= U ;Algorithm 2: Construction of the final data setD, by eliminating queries from T3 based on theparameters W , F , and U .The parameters F , U , and W are actually de-pendent on the size of the data set under consid-eration, and the study in (Silverstein et al, 1999)suggests that we can get enough click-through datafor our analysis by retrieving a large sample ofqueries.
Since we retrieve the queries that are sub-mitted within a three months period, for each lan-guage, we have millions of unique queries in ourdata set.
Investigating a held-out development setof queries retrieved from the United States website (www.yahoo.com), we empirically decidedthe following values for the parameters, W = 1,F = 50, and U = 5.
In other words, we onlyaccepted the queries for which the contents of theurls agree on the same language, that are submit-ted less than 50 times, and at least have 5 uniqueurls clicked.The filtering process leaves us with 5-10% ofthe queries due to the conservative choice of theparameters.
From the resulting set, we randomlypicked 500 queries and asked a native speaker toannotate them.
For each query, the annotator wasto classify the query into one of three categories:?
Category-1: If the query does not containany foreign terms.Language Category-1 Category-1+2 Category-3English 90.6% 94.2% 5.8%French 84.6% 93.4% 6.6%Portuguese 85.2% 93.4% 6.6%Spanish 86.6% 97.4% 2.6%Italian 82.4% 96.6% 3.4%German 76.8% 87.2% 12.8%Dutch 81.0% 92.0% 8.0%Danish 82.4% 93.2% 6.8%Finnish 87.2% 94.0% 6.0%Swedish 86.6% 95.4% 4.6%Average 84.3% 93.7% 6.3%Table 1: Annotation of 500 sample queries drawnfrom the automatically generated data.?
Category-2: If there exists some foreignterms but the query would still be expectedto bring web pages in the same language.?
Category-3: If the query belongs to otherlanguages, or all the terms are foreign to theannotator.490.6% of the queries in our data set were anno-tated as Category-1, and 94.2% as Category-1 andCategory-2 combined.
Having successful resultsfor the United States data set, we applied the sameparameters to the data sets retrieved for other lan-guages as well, and had the native speakers of eachlanguage annotate the queries in the same way.
Welist these results in Table 1.The results for English have the highest accu-racy for Category-1, mostly due to the fact that wetuned our parameters using the United States data.The scores for German on the other hand, are thelowest.
We attribute this fact to the highly multi-linguality of the Yahoo!
Germany website, whichreceives a high number of non-German queries.In order to see how much of this multi-lingualityour parameter selection successfully eliminate, werandomly picked 500 queries from the aggregatedbut unfiltered queries of the Yahoo!
Germanywebsite, and had them annotated as before.As suspected, the second annotation resultsshowed that, only 47.6% of the queries were an-notated as Category-1 and 60.2% are annotatedas Category-1 and Category-2 combined.
Ourmethod was indeed successful and achieved 29.2%improvement over Category-1, and 27% improve-ment over Category-1 and Category-2 queriescombined.Another interesting fact to note is the absolutedifferences between Category-1 and Category-1+2scores.
While this number is very low, 3.8%,for English, it is much higher for the other lan-4We do not expect the annotators to know the etymologyof the words or have the knowledge of all the acronyms.1069Language MinC MaxC ?C MinW MaxW ?WEnglish 7 46 21.8 1 6 3.35French 6 74 22.6 1 10 3.38Portug.
3 87 22.5 1 14 3.55Spanish 5 57 23.5 1 9 3.51Italian 4 51 21.9 1 8 3.09German 3 53 18.1 1 6 2.05Dutch 5 43 16.3 1 6 2.11Danish 3 40 14.3 1 6 1.93Finnish 3 34 13.3 1 5 1.49Swedish 3 42 13.7 1 8 1.80Average 4.2 52.7 18.8 1 7.8 2.63Table 2: Properties of the test set formed by taking350 Category-1 queries from each language.guages.
Through an investigation of Category-2non-English queries, we find out that this is mostlydue to the usage of some common internet orcomputer terms such as ?download?, ?software?,?flash player?, among other native language queryterms.4 Language IdentificationWe start this section with the implementation ofthree models each of which use a different exist-ing feature.
We categorize these models as statis-tical, knowledge based, and morphological.
Wethen combine all three models in a machine learn-ing framework using a novel approach.
Finally, weextend this framework by adding a non-linguisticfeature in order to identify the language intendedby the search engine user.To train each model implemented, we used theEuroParl Corpora, (Koehn, 2005), and the same 10languages in Section 3.
EuroParl Corpora is wellbalanced, so we would not have any bias towardsa particular language resulting from our choice ofthe corpora.We tested all the systems in this section on atest set of 3500 human annotated queries, whichis formed by taking 350 Category-1 queries fromeach language.
All the queries in the test set areobtained from the evaluation results in Section3.
In Table 2, we give the properties of this testset.
We list the minimum, maximum, and averagenumber of characters and words (MinC, MaxC,?C , MinW, MaxW, and ?W respectively).As can be seen in Table 2, the queries in our testset have 18.8 characters on average, which is muchlower than the threshold suggested by the existingsystems to achieve a good accuracy.
Another in-teresting fact about the test set is that, languageswhich are in the bottom half of Table 2 (German,Dutch, Danish, Finnish, and Swedish) have lowernumber of characters and words on average com-pared to the languages in the upper half.
Thisis due to the characteristics of those languages,which allow the construction of composite wordsfrom multiple words, or have a richer morphology.Thus, the concepts can be expressed in less num-ber of words or characters.4.1 Models for Language IdentificationWe implement a statistical model using a charac-ter based n-gram feature.
For each language, wecollect the n-gram counts (for n = 1 to n = 7also using the word beginning and ending spaces)from the vocabulary of the training corpus, andthen generate a probability distribution from thesecounts.
We implemented this model using theSRILM Toolkit (Stolcke, 2002) with the mod-ified Kneser-Ney Discounting and interpolationoptions.
For comparison purposes, we also imple-mented the Rank-Order method using the parame-ters described in (Cavnar and Trenkle, 1994).For the knowledge based method, we used thevocabulary of each language obtained from thetraining corpora, together with the word counts.From these counts, we obtained a probability dis-tribution for all the words in our vocabulary.
Inother words, this time we used a word-based n-gram method, only with n = 1.
It should be notedthat increasing the size of n, which might help inlanguage identification of other types of writtentexts, will not be helpful in this task due to theunique nature of the search engine queries.For the morphological feature; we gathered theaffix information for each language from the cor-pora in an unsupervised fashion as described in(Hammarstro?m, 2006).
This method basicallyconsiders each possible morphological segmenta-tion of the words in the training corpora by as-suming a high frequency of occurence of salientaffixes, and also assuming that words are made upof random characters.
Each possible affix is as-signed a score based on its frequency, random ad-justment, and curve-drop probabilities, which re-spectively indicate the probability of the affix be-ing a random sequence, and the probability of be-ing a valid morphological segment based on the in-formation of the preceding or the succeding char-acter.
In Table 3, we present the top 10 results ofthe probability distributions obtained from the vo-cabulary of English, Finnish, and German corpora.We give the performance of each model onour test set in Table 4.
The character based n-gram model outperforms all the other models withthe exception of French, Spanish, and Italian onwhich the word-based unigram model is better.1070English Finnish German-nts 0.133 erityis- 0.216 -ungen 0.172-ity 0.119 ihmisoikeus- 0.050 -en 0.066-ised 0.079 -inen 0.038 gesamt- 0.066-ated 0.075 -iksi 0.037 gemeinschafts- 0.051-ing 0.069 -iseksi 0.030 verhandlugs- 0.040-tions 0.069 -ssaan 0.028 agrar- 0.024-ted 0.048 maatalous- 0.028 su?d- 0.018-ed 0.047 -aisesta 0.024 menschenrechts- 0.018-ically 0.041 -iseen 0.023 umwelt- 0.017-ly 0.040 -amme 0.023 -ches 0.017Table 3: Top 10 prefixes and suffixes together withtheir probabilities, obtained for English, Finnish,and German.The word-based unigram model performs poorlyon languages that may have highly inflected orcomposite words such as Finnish, Swedish, andGerman.
This result is expected as we cannotmake sure that the training corpus will includeall the possible inflections or compositions of thewords in the language.
The Rank-Order methodperforms poorly compared to the character basedn-gram model, which suggests that for shortertexts, a well-defined probability distribution with aproper discounting strategy is better than using anad-hoc ranking method.
The success of the mor-phological feature depends heavily on the prob-ability distribution of affixes in each language,which in turn depends on the corpus due to the un-supervised affix extraction algorithm.
As can beseen in Table 3, English affixes have a more uni-form distribution than both Finnish and German.Each model implemented in the previous sec-tion has both strengths and weaknesses.
The sta-tistical approach is more robust to noise, such asmisspellings, than the others, however it may failto identify short queries or single words becauseof the lack of enough evidence, and it may confusetwo languages that are very similar.
In such cases,the knowledge-based model could be more useful,as it can find those query terms in the vocabulary.On the other hand, the knowledge-based modelwould have a sparse vocabulary for languages thatcan have heavily inflected words such as Turkish,and Finnish.
In such cases, the morphological fea-ture could provide a strong clue for identificationfrom the affix information of the terms.4.2 Decision Tree ClassificationNoting the fact that each model can complementthe other(s) in certain cases, we combined them byusing a decision tree (DT) classifier.
We trainedthe classifier using the automatically annotateddata set, which we created in Section 3.
Sincethis set comes with a certain amount of noise, weLanguage Stat.
Knowl.
Morph.
Rank-OrderEnglish 90.3% 83.4% 60.6% 78.0%French 77.4% 82.0% 4.86% 56.0%Portuguese 79.7% 75.7% 11.7% 70.3%Spanish 73.1% 78.3% 2.86% 46.3%Italian 85.4% 87.1% 43.4% 77.7%German 78.0% 60.0% 26.6% 58.3%Dutch 85.7% 64.9% 23.1% 65.1%Danish 87.7% 67.4% 46.9% 61.7%Finnish 87.4% 49.4% 38.0% 82.3%Swedish 81.7% 55.1% 2.0% 56.6%Average 82.7% 70.3% 26.0% 65.2%Table 4: Evaluation of the models built from theindividual features, and the Rank-Order methodon the test set.pruned the DT during the training phase to avoidoverfitting.
This way, we built a robust machinelearning framework at a very low cost and withoutany human labour.As the features of our DT classifier, we use theresults of the models that are implemented in Sec-tion 4.1, together with the confidence scores cal-culated for each instance.
To calculate a confi-dence score for the models, we note that sinceeach model makes its selection based on the lan-guage that gives the highest probability, a confi-dence score should indicate the relative highnessof that probability compared to the probabilitiesof other languages.
To calculate this relative high-ness, we use the Kurtosis measure, which indicateshow peaked or flat the probabilities in a distribu-tion are compared to a normal distribution.
To cal-culate the Kurtosis value, ?, we use the equationbelow.?
=?l?L(pl ?
?
)4(N ?
1)?4where L is the set of languages, N is the numberof languages in the set, pl is the probability forlanguage l ?
L, and ?
and ?
are respectively themean and the the standard deviation values of P ={pl|l ?
L}.We calculate a ?
measure for the result of eachmodel, and then discretize it into one of three cat-egories:?
HIGH: If ?
?
(??
+ ??)?
MEDIUM: If [?
> (?????)??
< (??+??)]?
LOW: If ?
?
(??
?
??
)where ??
and ??
are the mean and the standarddeviation values respectively, for a set of confi-dence scores calculated for a model on a small de-velopment set of 25 annotated queries from eachlanguage.
For the statistical model, we found??
= 4.47, and ??
= 1.96, for the knowledge1071Language 500 1,000 5,000 10,000English 78.6% 81.1% 84.3% 85.4%French 83.4% 85.7% 85.4% 86.6%Portuguese 81.1% 79.1% 81.7% 81.1%Spanish 77.4% 79.4% 81.4% 82.3%Italian 90.6% 89.7% 90.6% 90.0%German 81.1% 82.3% 83.1% 83.1%Dutch 86.3% 87.1% 88.3% 87.4%Danish 86.3% 87.7% 88.0% 88.0%Finnish 88.3% 88.3% 89.4% 90.3%Swedish 81.4% 81.4% 81.1% 81.7%Average 83.5% 84.2% 85.3% 85.6%Table 5: Evaluation of the Decision Tree Classifierwith varying sizes of training data.based ??
= 4.69, and ??
= 3.31, and finally forthe morphological model we found ??
= 4.65, and??
= 2.25.Hence, for a given query, we calculate the iden-tification result of each model together with themodel?s confidence score, and then discretize theconfidence score into one of the three categoriesdescribed above.
Finally, in order to form an as-sociation between the output of the model andits confidence, we create a composite attribute byappending the discretized confidence to the iden-tified language.
As an example, our statisticalmodel identifies the query ?the sovereign individ-ual?
as English (en), and reports a ?
= 7.60,which is greater than or equal to ?
?+ ??
= 4.47+1.96 = 6.43.
Therefore the resulting compositeattribute assigned to this query by the statisticalmodel is ?en-HIGH?.We used the Weka Machine Learning Toolkit(Witten and Frank, 2005) to implement our DTclassifier.
We trained our system with 500, 1,000,5,000, and 10,000 instances of the automaticallyannotated data and evaluate it on the same test setof 3500 human-annotated queries.
We show theresults in Table 5.The results in Table 5 show that our DT clas-sifier, on average, outperforms all the models inTable 4 for each size of the training data.
Fur-thermore, the performance of the system increaseswith the increasing size of training data.
In par-ticular, the improvement that we get for Spanish,French, and German queries are strikingly good.This shows that our DT classifier can take ad-vantage of the complementary features to makea better classification.
The classifier that uses10,000 instances gets outperformed by the statis-tical model (by 4.9%) only in the identification ofEnglish queries.In order to evaluate the significance of our im-provement, we performed a paired t-test, with anull hypothesis and ?
= 0.01 on the outputs ofda de en es fi fr it nl sv ptda 308 4 9 0 2 3 1 7 14 2de 7 291 6 2 4 4 5 19 9 3en 6 8 299 3 3 9 4 5 8 5es 3 2 4 288 2 2 10 1 1 37fi 0 5 3 4 316 1 7 4 7 3fr 2 7 6 3 2 303 10 7 2 8it 0 1 2 7 4 4 315 2 1 14nl 5 8 8 4 6 4 4 306 4 1sv 24 8 6 5 6 2 2 6 286 5pt 0 1 3 41 1 4 13 2 1 284Figure 2: Confusion Matrix for the Decision TreeClassifier that uses 10,000 training instances.the statistical model, and the DT classifier thatuses 10,000 training instances.
The test resultedin P = 1.12?10  ?, which strongly indicatesthat the improvement of the DT classifier over thestatistical model is statistically significant.In order to illustrate the errors made by our DTclassifier, we show the confusion matrixM in Fig-ure 2.
The matrix entry Mli,lj simply gives thenumber of test instances that are in language li butmisclassified by the system as lj .
From the figure,we can infer that, Portuguese and Spanish are thelanguages that are confused mostly by the system.This is an expected result because of the high sim-ilarity between the two languages.4.3 Towards Identifying the Language IntentAs a final step in our study, we build another DTclassifier by introducing a non-linguistic featureto our system, which is the language informationof the country from which the user entered thequery.5 Our intuition behind introducing this extrafeature is to help the search engine in guessing thelanguage in which the user wants to see the result-ing web pages.
Since the real purpose of a searchengine is to bring the expected results to its users,we believe that a correct identification of the lan-guage that the user intended for the results whentyping the query is an important first part of thisprocess.To illustrate this with an example, we con-sider the query, ?how to tape for plantar fasci-itis?, which we selected among the 500 human-annotated queries retrieved from the United Statesweb site.
This query is labelled as Category-2 bythe human annotator.
Our DT classifier, togetherwith the statistical and knowledge-based models,classifies this query falsely as a Porteguese query,which is most likely caused due to the presence ofthe Latin phrase ?plantar fasciitis?.In order to test the effectiveness of our new fea-ture, we introduce all the Category-2 queries to our5For countries, where the number of official languages ismore than one, we simply pick the first one listed in our table.1072Language New Feat.
Classifier-1 Classifier-2English 74.9% 82.8% 89.5%French 77.0% 85.6% 93.7%Portuguese 79.1% 78.1% 93.3%Spanish 84.1% 80.7% 94.2%Italian 90.6% 86.7% 96.3%German 80.2% 80.7% 94.2%Dutch 91.6% 85.8% 95.3%Danish 88.6% 87.0% 94.9%Finnish 94.0% 87.7% 97.9%Swedish 87.9% 80.9% 95.3%Average 85.0% 83.6% 94.5%Table 6: Evaluation of the new feature and the twodecision tree classifiers on the new test set.test set and increase its size to 430 queries for eachlanguage.6 Then we run both classifiers, with andwithout the new feature, using a training data sizeof 10,000 instances, and display the results in Ta-ble 6.
We also show the contribution of the newfeature as a standalone classifier in the first col-umn of Table 6.
We labeled the DT classifier thatwe implemented in Section 4.2 as ?Classifier-1?and the new one as ?Classifier-2?.Interestingly, the results in Table 6 tell us that asearch engine can achieve a better accuracy thanClassifier-1 on average, should it decide to bringthe results based only on the geographical infor-mation of its users.
However one can argue thatthis would be a bad idea for the web sites that re-ceive a lot of visitors from all over the world, andalso are visited very often.
For example, if thesearch engine?s United States web site, which isconsidered as one of the most important marketsin the world, was to employ such an approach, it?donly receive 74.9% accuracy by misclassifying theEnglish queries entered from countries for whichthe default language is not English.
On the otherhand, when this geographical information is usedas a feature in our decision tree framework, we geta very high boost on the accuracy of the resultsfor all the languages.
As can be seen in Table 6,Classifier-2 gives the best results.5 Conclusions and Future WorkIn this paper, we considered the language identi-fication problem for search engine queries.
First,we presented a completely automated method togenerate a reliable data set with language anno-tations that can be used to train a decision treeclassifier.
Second, we implemented three featuresused in the existing language identification meth-6We don?t have equal number of Category-2 queries ineach language.
For example, English has only 18 of themwhereas Italian has 71.
Hence the resulting data set won?t bebalanced in terms of this category.ods, and compared their performance.
Next, webuilt a decision tree classifier that improves the re-sults on average by combining the outputs of thethree models together with their confidence scores.Finally, we considered the practical application ofthis problem for search engines, and built a secondclassifier that takes into account the geographicalinformation of the users.Human annotations on 5000 automatically an-notated queries showed that our data generationmethod is highly accurate, achieving 84.3% accu-racy on average for Category-1 queries, and 93.7%accuracy for Category-1 and Category-2 queriescombined.
Furthermore, the process is fast as wecan get a data set of size approximately 50,000queries in a few hours by using only 15 computersin a cluster.The decision tree classifier that we built for thetextual language identification in Section 4.2 out-performs all three models that we implemented inSection 4.1, for all the languages except English,for which the statistical model is better by 4.9%,and Swedish, for which we get a tie.
Introducingthe geographical information feature to our deci-sion tree framework boosts the accuracy greatlyeven in the case of a noisier test set.
This sug-gests that the search engines can do a better job inpresenting the results to their users by taking thenon-linguistic features into account in identifyingthe intended language of the queries.In future, we would like to improve the accu-racy of our data generation system by consideringadditional features proposed in the studies of au-tomated query taxonomy, and doing a more care-ful examination in the assignment of the parametervalues.
We are also planning to extend the num-ber of languages in our data set.
Furthermore, wewould like to improve the accuracy of Classifier-2 with additional non-linguistic features.
Finally,we will consider other alternatives to the decisiontree framework when combining the results of themodels with their confidence scores.6 AcknowledgmentsWe are grateful to Romain Vinot, and Rada Mi-halcea, for their comments on an earlier draft ofthis paper.
We also would like to thank SriramCherukiri for his contributions during the courseof this project.
Finally, many thanks to Murat Bir-inci, and Sec?kin Kara, for their help on the data an-notation process, and Cem So?zgen for his remarkson the SQL formulations.1073ReferencesC.
Biemann and S. Teresniak.
2005.
Disentanglingfrom babylonian confusion - unsupervised languageidentification.
In Proceedings of CICLing-2005,Computational Linguistics and Intelligent Text Pro-cessing, pages 762?773.
Springer.Andrei Broder.
2002.
A taxonomy of web search.
SI-GIR Forum, 36(2):3?10.William B. Cavnar and John M. Trenkle.
1994.
N-gram-based text categorization.
In Proceedings ofSDAIR-94, 3rd Annual Symposium on DocumentAnalysis and Information Retrieval, pages 161?175,Las Vegas, US.J.
Cowie, Y. Ludovic, and R. Zacharski.
1999.
Lan-guage recognition for mono- and multi-lingual docu-ments.
In Proceedings of Vextal Conference, Venice,Italy.Ted Dunning.
1994.
Statistical identification of lan-guage.
Technical Report MCCS-94-273, Comput-ing Research Lab (CRL), New Mexico State Uni-versity.Gregory Grefenstette.
1995.
Comparing two languageidentification schemes.
In Proceedings of JADT-95,3rd International Conference on the Statistical Anal-ysis of Textual Data, Rome, Italy.Harald Hammarstro?m.
2006.
A naive theory of affix-ation and an algorithm for extraction.
In Proceed-ings of the Eighth Meeting of the ACL Special Inter-est Group on Computational Phonology and Mor-phology at HLT-NAACL 2006, pages 79?88, NewYork City, USA, June.
Association for Computa-tional Linguistics.Harald Hammarstro?m.
2007.
A fine-grained model forlanguage identification.
In F. Lazarinis, J. Vilares,J.
Tait (eds) Improving Non-English Web Searching(iNEWS07) SIGIR07 Workshop, pages 14?20.B.
Hughes, T. Baldwin, S. G. Bird, J. Nicholson, andA.
Mackinlay.
2006.
Reconsidering language iden-tification for written language resources.
In 5th In-ternational Conference on Language Resources andEvaluation (LREC2006), Genoa, Italy.Norman C Ingle.
1976.
A language identification ta-ble.
The Incorporated Linguist, 15(4):98?101.Bernard J. Jansen, Danielle L. Booth, and AmandaSpink.
2008.
Determining the informational, navi-gational, and transactional intent of web queries.
Inf.Process.
Manage., 44(3):1251?1266.Philipp Koehn.
2005.
Europarl: A parallel corpusfor statistical machine translation.
In Proceedings ofthe 10th Machine Translation Summit, Phuket, Thai-land, pages 79?86.Canasai Kruengkrai, Prapass Srichaivattana, VirachSornlertlamvanich, and Hitoshi Isahara.
2005.
Lan-guage identification based on string kernels.
InIn Proceedings of the 5th International Symposiumon Communications and Information Technologies(ISCIT-2005, pages 896?899.Uichin Lee, Zhenyu Liu, and Junghoo Cho.
2005.
Au-tomatic identification of user goals in web search.In WWW ?05: Proceedings of the 14th internationalconference on World Wide Web, pages 391?400,New York, NY, USA.
ACM.Ernesto William De Luca Lena Grothe and AndreasNrnberger.
2008.
A comparative study on lan-guage identification methods.
In Proceedings of theSixth International Language Resources and Eval-uation (LREC?08), Marrakech, Morocco, May.
Eu-ropean Language Resources Association (ELRA).http://www.lrec-conf.org/proceedings/lrec2008/.Ryan Levering and Michal Cutler.
2006.
The portraitof a common html web page.
In DocEng ?06: Pro-ceedings of the 2006 ACM symposium on Documentengineering, pages 198?204, New York, NY, USA.ACM Press.Yiqun Liu, Min Zhang, Liyun Ru, and Shaoping Ma.2006.
Automatic query type identification based onclick through information.
In AIRS, pages 593?600.Arjen Poutsma.
2001.
Applying monte carlo tech-niques to language identification.
In In Proceed-ings of Computational Linguistics in the Nether-lands (CLIN).John M. Prager.
1999.
Linguini: Language identifi-cation for multilingual documents.
In HICSS ?99:Proceedings of the Thirty-Second Annual Hawaii In-ternational Conference on System Sciences-Volume2, page 2035, Washington, DC, USA.
IEEE Com-puter Society.Daniel E. Rose and Danny Levinson.
2004.
Under-standing user goals in web search.
In WWW ?04:Proceedings of the 13th international conference onWorld Wide Web, pages 13?19, New York, NY, USA.ACM.Penelope Sibun and Jeffrey C. Reynar.
1996.
Lan-guage identification: Examining the issues.
In5th Symposium on Document Analysis and Informa-tion Retrieval, pages 125?135, Las Vegas, Nevada,U.S.A.Craig Silverstein, Hannes Marais, Monika Henzinger,and Michael Moricz.
1999.
Analysis of a verylarge web search engine query log.
SIGIR Forum,33(1):6?12.C.
Souter, G. Churcher, J. Hayes, and J. Hughes.
1994.Natural language identification using corpus-basedmodels.
Hermes Journal of Linguistics, 13:183?203.Andreas Stolcke.
2002.
Srilm ?
an extensible languagemodeling toolkit.
In Proc.
Intl.
Conf.
on SpokenLanguage Processing, volume 2, pages 901?904,Denver, CO.Hidayet Takci and Ibrahim Sogukpinar.
2004.Centroid-based language identification using letterfeature set.
In CICLing, pages 640?648.Ian H. Witten and Eibe Frank.
2005.
Data Mining:Practical Machine Learning Tools and Techniques.Morgan Kaufmann, 2 edition.1074
