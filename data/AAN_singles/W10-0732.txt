Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 204?207,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsNon-Expert Correction of Automatically Generated Relation AnnotationsMatthew R.
Gormley??
and Adam Gerber??
and Mary Harper??
and Mark Dredze ??
?Human Language Technology Center of Excellence?Center for Language and Speech ProcessingJohns Hopkins University, Baltimore, MD 21211, USA?Laboratory for Computational Linguistics and Information ProcessingUniversity of Maryland, College Park, MD 20742 USAmrg@cs.jhu.edu,adam.gerber@jhu.edu,mharper@umd.edu,mdredze@cs.jhu.eduAbstractWe explore a new way to collect human an-notated relations in text using Amazon Me-chanical Turk.
Given a knowledge base ofrelations and a corpus, we identify sentenceswhich mention both an entity and an attributethat have some relation in the knowledge base.Each noisy sentence/relation pair is presentedto multiple turkers, who are asked whether thesentence expresses the relation.
We describea design which encourages user efficiency andaids discovery of cheating.
We also presentresults on inter-annotator agreement.1 IntroductionRelation extraction (RE) is the task of determiningthe existence and type of relation between two tex-tual entity mentions.
Slot filling, a general form ofrelation extraction, includes relations between non-entities, such as a person and an occupation, age, orcause of death (McNamee and Dang, 2009).RE annotated data, such as ACE (2008), is expen-sive to produce so systems take different approachesto minimizing data needs.
For example, tree kernelscan reduce feature sparsity and generalize acrossmany examples (GuoDong et al, 2007; Zhou etal., 2009).
Distant supervision automatically gen-erates noisy training examples from a knowledgebase (KB) without needing annotations (Bunescuand Mooney, 2007; Mintz et al, 2009).
Whilethis method can quickly generate training data, italso generates many false examples.
We reduce thenoise in such examples by using Amazon Mechani-cal Turk (MTurk), which has been shown to producehigh quality annotations for a variety of natural lan-guage processing tasks (Snow et al, 2008).We use MTurk for annotation of textual relationsto establish an inexpensive and rapid method of cre-ating data for slot filling.
We present a two step an-notation process: (1) automatic creation of noisy ex-amples, and (2) human validation of examples.2 Method2.1 Automatic generation of noisy examplesTo create noisy examples we use a similar approachto Mintz et al (2009).
We extract relations from aKB in the form of tuples, (e, r, v), where e is anentity, v is a value, and r is a relation that holdsbetween them; for example (J.R.R.
Tolkien, occu-pation, author).
Our KB is Freebase1, an onlinedatabase of structured information, and our corpusis from the TAC KBP task (McNamee and Dang,2009)2.
For each tuple, we find sentences in a cor-pus that contain both an exact mention of the entitye and of the value v. Of course, such sentences maynot attest to the relation r, so the process producesmany incorrect examples.2.2 Human Intelligence TasksA Human Intelligence Task (HIT) is a short paid taskon MTurk.
In our HITs, we present the turker withten relation examples as sentence/relation pairs.
Foreach example, the user is asked to select from threeannotation options: the sentence (1) expresses therelation, (2) does not express the relation, or (3) the1http://www.freebase.com2http://projects.ldc.upenn.edu/kbp/2041.
The sentence expresses the relation.Sentence: For the past eleven years, James haslived in Tucson.Relation: ?Tucson?
is the residence of ?James?2.
The sentence does not express the relation.Sentence: Samuel first met Divya in 1990, whileshe was still a student.Relation: ?Divya?
is a spouse of ?Samuel?3.
The relation does not make sense.Sentence: Soojin was born in January.Relation: ?January?
is the birth place of ?Soojin?Figure 1: The three annotation options with examples.relation does not make sense (figure 1.
)Of the ten examples that comprise each HIT,seven are automatically generated by the methodabove.
The correct answer is known for the three re-maining examples; these are included for quality as-surance (control examples.)
The three control exam-ples are a positive example (expresses the relation,) anegative example (contradicts the true relation,) anda nonsense example (relation is nonsensical.
)All control examples derive from a subset of theautomatically generated person examples.
Positiveexamples were randomly sampled and hand anno-tated.
Negative examples are familial relations inwhich we change the relation type so that it wouldnot be expressed in the sentence.
For example,the relation ?Barack Obama is the parent of MaliaObama?
would be changed to ?Barack Obama is asibling of Malia Obama.?
To generate nonsense ex-amples we employ the same method for a differentmapping of relations, which produces relations like?New Zealand is the gender of John Key.
?2.3 HIT DesignMTurk is a marketplace so users have total freedomin choosing which HITs to complete.
As such, HITdesign should maximize its appeal.
We assume thatusers find appealing those HITs through which theymay maximize their own monetary gain, while mini-mizing moment-to-moment frustrations.
We empha-sized clarity and ease of use.The layout consists of three sections (figure 2).The leftmost section is a progress list, which showsthe user?s answers and current position; the middlesection contains the current relation example and an-notation options; the rightmost section (not pictured)# HITs Cost Time (hours)Trial 50 $2.75 27Batch 1 500 $27.50 34Batch 2 765 $42.08 25Batch 3 500 $27.50 22Total 1815 $99.83 108Table 1: Size, cost and time to complete each HITs batch.contains instructions.
All sections and all UI ele-ments remain visible and in the same position for theduration of the HIT, with only the text of the sen-tence and relation changing according to questionnumber.
Because only a single question is displayedat a time, we are able to minimize user actions suchas scrolling, clicking small targets, or making largemouse movements.
Additionally, we can monitorhow much time a user spends on each question.At all times the user is able to consult the instruc-tions for the task, which include examples of eachannotation option.
The user is also reminded of thetechnical requirements for the HIT and expectationsfor honesty and accuracy.
A comment box providesusers with the opportunity to ask questions, makesuggestions, or clarify their responses.3 ResultsWe submitted a trial run and three full batches ofHITs.
Table 1 summarizes the costs and completiontimes for all HITs.
The HITs were labeled rapidlyand for a low cost ($0.05 per HIT, i.e., .5?
per anno-tation).
Each HIT was assigned to five unique work-ers.
We found that 50% of the 352 different workerscompleted 2 or more HITs (figure 3.)
Our resultsexclude a trial run of 50 hits.
Across the 17,650 ex-amples the mean time spent was 20.77 seconds, witha standard deviation of 99.96 seconds.
The mediantime per example was 10.0 seconds.3.1 AnalysisTo evaluate the annotations, two of the authors an-notated a random sample of 247 (10%) of the 2471noisy examples.
In addition, we analyzed the work-ers agreement with the control examples.We used two metrics to assess agreement.
Thefirst metric is pairwise percent agreement (Pair-wise): the average of the example agreement scores,where the example agreement score is the percent of205Figure 2: An example HIT with instructions excluded.0204060801001201400 50 100150200250300350# HITsWorkersFigure 3: The number of HITs per worker, with columnssorted left to right.pairs of annotators that agreed for a particular exam-ple.
The second metric is the exact kappa coefficient(Exact-?)
(Conger, 1980), which takes into accountthat agreement can occur by chance.
The number ofannotators (R) varies with the test scenario.Table 2 presents the inter-annotator agreementscores for various subsets of the examples and com-binations of annotators.
On a sample of examples,we evaluated agreement between the first and sec-ond expert annotators (E1/E2) and also the agree-ment between each expert and the majority vote ofthe workers (E1/M and E2/M).
The agreement be-tween the two experts is substantially higher thantheir individual agreements with the majority.
Yet,we achieve our goal of reducing noise.We also analyzed the agreement between theknown control answer and the majority vote of theworkers (C/M).
This high level of agreement sup-ports our belief that the automatically generated neg-ative and nonsense examples were easier to identify# Ex.
R Exact-?
PairwiseE1/E2 247 2 0.64 0.81E1/M 247 2 0.29 0.60E2/M 247 2 0.39 0.70C/M 1059 2 0.90 0.93T(sample) 247 5 0.31 0.69T(control) 1059 5 0.52 0.68T(all) 3530 5 0.45 0.68Table 2: Inter-annotator agreementthan noisy negative and nonsense examples.
Finally,we evaluated the agreement between the five work-ers for different subsets of the data: the sample ofnoisy examples (T(sample)), the control examplesonly (T(control)), and all examples (T(all)).
Table 3lists the number of examples collected and the agree-ment scores for all workers for each relation type.Table 4 shows the divergence of the workers?
an-notations from those of an expert.
The high levelof confusability for those examples which the expertannotated as Not Expressed suggests their inherentdifficulty.
The workers labeled more examples asExpressed than the expert, but both labeled few ex-amples as Nonsense.4 Quality ControlWe identify spurious responses and unreliable usersin two ways.
First, worker responses are comparedto control examples; greater agreement with controlsshould indicate greater confidence in the user.
Wefiltered any worker whose agreement with the con-trols was less than 0.85 (Control Filtered).
The sec-ond approach uses behavioral data.
Because only asingle example is visible at any time, we can mea-206Relation # Ex.
Exact-?
Pairwisesiblings 13 0.67 0.82children 12 0.57 0.83gender 80 0.46 0.70place of death 40 0.43 0.68parent 12 0.40 0.64spouse 54 0.37 0.65title 71 0.30 0.78residences 228 0.29 0.60ethnicity 38 0.28 0.54occupation 551 0.26 0.77activism 4 0.26 0.55religion 22 0.23 0.55place of birth 160 0.20 0.64nationality 1044 0.19 0.67schools attended 8 0.16 0.55employee of 132 0.16 0.70charges 2 0.14 0.70Total 2471 0.35 0.69Table 3: Inter-annotator agreement across relation type.# Ex.
is the number of noisy examples.
Exact-?
and Pair-wise agreement are among the five workers.WorkerE NE Nn TotalExpert-1 E 561 89 20 670NE 284 248 28 560Nn 1 1 3 5Total 846 338 51 1235Table 4: Confusion matrix of expert-1 and user?s anno-tations on the sample of noisy examples, for the choicesExpressed (E), Not Expressed (NE), and Nonsense (Nn)sure how much time a user spends on each exam-ple.
The UI is designed to allow for the extremelyrapid completion of examples and of the HIT in gen-eral.
Thus, a user could complete the HIT in only afew seconds without even reading any of the exam-ples.
Still other users spend only a moment on all-but-one question, and then several minutes on theremaining question.
Here, we filter a user answeringthree or more questions each in under three seconds(Time Filtered).
We combine these two approaches(Control and Time), which yields the highest expert-agreement levels (table 5.
)5 ConclusionUsing non-expert annotators from Amazon Mechan-ical Turk for the correction of noisy, automaticallyE1/M E2/MUnfiltered 0.28 0.38Time Filtered 0.32 0.43Control Filtered 0.34 0.47Control and Time 0.37 0.48Table 5: Exact-?
scores for three levels of quality controland a baseline, between each expert and the majority voteon 231 sampled examples.
For a fair comparison, we re-duced the sample size to include only examples for whicheach level of quality control had at least one worker an-notation remaining.generated examples is inexpensive and fast.
Weachieve good inter-annotator agreement using qual-ity assurance measures to detect cheating.
The resultis thousands of new annotated slot filling examplesentences for 17 person relations.AcknowledgmentsWe would like to thank the 352 turkers who madethis work possible.ReferencesACE.
2008.
Automatic content extraction.http://projects.ldc.upenn.edu/ace/.R.
Bunescu and R. Mooney.
2007.
Learning to extractrelations from the web using minimal supervision.
InAssociation for Computational Linguistics (ACL).A.J.
Conger.
1980.
Integration and generalization ofkappas for multiple raters.
Psychological Bulletin,88(2):322?328.Z.
GuoDong, M. Zhang, D. H Ji, and Z. H. U. QiaoM-ing.
2007.
Tree kernel-based relation extraction withcontext-sensitive structured parse tree information.
InEmpirical Methods in Natural Language Processing(EMNLP).Paul McNamee and Hoa Dang.
2009.
Overview of theTAC 2009 knowledge base population track.
In TextAnalysis Conference (TAC).M.
Mintz, S. Bills, R. Snow, and D. Jurafsky.
2009.
Dis-tant supervision for relation extraction without labeleddata.
In Association for Computational Linguistics(ACL).R.
Snow, B. O?Connor, D. Jurafsky, and A.Y.
Ng.
2008.Cheap and fast?but is it good?
: evaluating non-expertannotations for natural language tasks.
In EmpiricalMethods in Natural Language Processing (EMNLP).G.
Zhou, L. Qian, and J.
Fan.
2009.
Tree kernel-basedsemantic relation extraction with rich syntactic and se-mantic information.
Information Sciences.207
