Tu?SBL: A Similarity-Based Chunk Parserfor Robust Syntactic ProcessingSandra Ku?blerSeminar fu?r SprachwissenschaftUniversity of Tu?bingenWilhelmstr.
113D-72074 Tu?bingen, Germanykuebler@sfs.nphil.uni-tuebingen.deErhard W. HinrichsSeminar fu?r SprachwissenschaftUniversity of Tu?bingenWilhelmstr.
113D-72074 Tu?bingen, Germanyeh@sfs.nphil.uni-tuebingen.deABSTRACTChunk parsing has focused on the recognition of partial constituentstructures at the level of individual chunks.
Little attention has beenpaid to the question of how such partial analyses can be combinedinto larger structures for complete utterances.The Tu?SBL parser extends current chunk parsing techniques bya tree-construction component that extends partial chunk parses tocomplete tree structures including recursive phrase structure as wellas function-argument structure.
Tu?SBL?s tree construction algo-rithm relies on techniques from memory-based learning that allowsimilarity-based classification of a given input structure relative toa pre-stored set of tree instances from a fully annotated treebank.A quantitative evaluation of Tu?SBL has been conducted usinga semi-automatically constructed treebank of German that consistsof appr.
67,000 fully annotated sentences.
The basic PARSEVALmeasures were used although they were developed for parsers thathave as their main goal a complete analysis that spans the entire in-put.
This runs counter to the basic philosophy underlying Tu?SBL,which has as its main goal robustness of partially analyzed struc-tures.Keywordsrobust parsing, chunk parsing, similarity-based learning1.
INTRODUCTIONCurrent research on natural language parsing tends to gravitatetoward one of two extremes: robust, partial parsing with the goalof broad data coverage versus more traditional parsers that aim atcomplete analysis for a narrowly defined set of data.
Chunk pars-ing [1, 2] offers a particularly promising and by now widely usedexample of the former kind.
The main insight that underlies thechunk parsing strategy is to isolate the (finite-state) analysis of non-recursive, syntactic structure, i.e.
chunks, from larger, recursivestructures.
This results in a highly-efficient parsing architecturethat is realized as a cascade of finite-state transducers and that pur-.sues a longest-match, right-most pattern-matching strategy at eachlevel of analysis.Despite the popularity of the chunk parsing approach, there seemto be two apparent gaps in current research:1.
Chunk parsing research has focused on the recognition ofpartial constituent structures at the level of individual chunks.By comparison, little or no attention has been paid to thequestion of how such partial analyses can be combined intolarger structures for complete utterances.2.
Relatively little has been reported on quantitative evaluationsof chunk parsers that measure the correctness of the outputstructures obtained by a chunk parser.The main goal of the present paper is help close those two re-search gaps.2.
THE T ?USBL ARCHITECTUREIn order to ensure a robust and efficient architecture, Tu?SBL, asimilarity-based chunk parser, is organized in a three-level archi-tecture, with the output of each level serving as input for the nexthigher level.
The first level is part-of-speech (POS) tagging of theinput string with the help of the bigram tagger LIKELY [10].1 Theparts of speech serve as pre-terminal elements for the next step,i.e.
the chunk analysis.
Chunk parsing is carried out by an adaptedversion of Abney?s [2] scol parser, which is realized as a cascadeof finite-state transducers.
The chunks, which extend if possible tothe simplex clause level, are then remodeled into complete trees inthe tree construction level.The tree construction is similar to the DOP approach [3, 4] in thatit uses complete tree structures instead of rules.
Contrary to Bod,we do not make use of probabilities and do not allow tree cuts,instead we only use the complete trees and minimal tree modifica-tions.
Thus the number of possible combinations of partial treesis strictly controlled.
The resulting parser is highly efficient (3770English sentences took 106.5 seconds to parse on an Ultra Sparc10).3.
CHUNK PARSING AND TREE CONSTRUC-TIONThe division of labor between the chunking and tree constructionmodules can best be illustrated by an example.1The inventory of POS tags is based on the Stuttgart-Tu?bingenTagset (STTS) [11].0 1 2 3 4 5 6 7 8 9 10 11 12 13500501502503504505506507508509510511512513514515516517dannADVw"urdeVAFINichPPERvielleichtADVnochADVvorschlagenVVINFDonnerstagNNdenARTelftenNNundKONFreitagNNdenARTzw"olftenADJAAugustNNHDHDHDVXINFOVHDHDVXFINHD?
HDNXHD APPADVXMODHDNX ADVX ADVXON MOD MODHDADJX?
?
HDNXHD APPNXNX?
?
?NXOAVF LK MF VCNFSIMPX?
?
?
?
?Figure 2: Sample tree construction outputInput:dann w?urde ich vielleicht noch vorschlagen Donnerstag den elftenund Freitag den zw?olften August(then I would suggest maybe Thursday eleventh and Friday twelfthof August)Chunk parser output:[simpx [advx [adv dann]][vxfin [vafin w"urde]][nx2 [pper ich]][advx [adv vielleicht]][advx [advmd noch]][vvinf vorschlagen]][nx3 [day Donnerstag][art den][adja elften]][kon und][nx3 [day Freitag][art den][adja zw"olften][month August]]Figure 1: Chunk parser outputFor complex sentences such as the German input dann w?urdeich vielleicht noch vorschlagen Donnerstag den elften und Fre-itag den zw?olften August (then I would suggest maybe Thursdayeleventh and Friday twelfth of August), the chunker produces astructure in which some constituents remain unattached or partiallyannotated in keeping with the chunk-parsing strategy to factor outrecursion and to resolve only unambigous attachments, as shown inFig.
1.In the case at hand, the subconstituents of the extraposed co-ordinated noun phrase are not attached to the simplex clause thatends with the non-finite verb that is typically in clause-final posi-tion in declarative main clauses of German.
Moreover, each con-junct of the coordinated noun phrase forms a completely flat struc-ture.
Tu?SBL?s tree construction module enriches the chunk outputas shown in Fig.
22.
Here the internally recursive NP conjunctshave been coordinated and integrated correctly into the clause as awhole.
In addition, function labels such as mod (for: modifier), hd(for: head), on (for: subject), oa (for: direct object), and ov (for:verbal object) have been added that encode the function-argumentstructure of the sentence.4.
SIMILARITY-BASED TREE CONSTRUC-TIONThe tree construction algorithm is based on the machine learningparadigm of memory-based learning [12].3 Memory-based learn-ing assumes that the classification of a given input should be basedon the similarity to previously seen instances of the same type thathave been stored in memory.
This paradigm is an instance of lazylearning in the sense that these previously encountered instancesare stored ?as is?
and are crucially not abstracted over, as is typi-cally the case in rule-based systems or other learning approaches.Past applications of memory-based learning to NLP tasks consistof classification problems in which the set of classes to be learntis simple in the sense that the class items do not have any internalstructure and the number of distinct items is small.The use of a memory-based approach for parsing implies thatparsing needs to be redefined as a classification task.
There are twofundamentally different, possible approaches: the one is to splitparsing up into different subtasks, that is, one needs separate clas-sifiers for each functional category and for each level in a recur-sive structure.
Since the classifiers for the functional categories aswell as the individual decisions of the classifiers are independent,multiple or no candidates for a specific grammatical function orconstituents with several possible functions may be found so thatan additional classifier is needed for selecting the most appropriateassignment (cf.
[6]).The second approach, which we have chosen, is to regard thecomplete parse trees as classes so that the task is defined as theselection of the most similar tree from the instance base.
Since in2All trees in this contribution follow the data format for trees de-fined by the NEGRA project of the Sonderforschungsbereich 378at the University of the Saarland, Saarbru?cken.
They were printedby the NEGRA annotation tool [5].3Memory-based learning has recently been applied to a variety ofNLP classification tasks, including part-of-speech tagging, nounphrase chunking, grapheme-phoneme conversion, word sense dis-ambiguation, and pp attachment (see [9], [14], [15] for details).construct tree(chunk list, treebank):while (chunk list is not empty) doremove first chunk from chunk listprocess chunk(chunk, treebank)Figure 3: Pseudo-code for tree construction, main routine.process chunk(chunk, treebank):words := string yield(chunk)tree := complete match(words, treebank)if (tree is not empty) direct hit,then output(tree) i.e.
complete chunk found in treebankelsetree := partial match(words, treebank)if (tree is not empty)thenif (tree = postfix of chunk)thentree1 := attach next chunk(tree, treebank)if (tree is not empty)then tree := tree1if ((chunk - tree) is not empty) if attach next chunk succeededthen tree := extend tree(chunk - tree, tree, treebank) chunk might consist of both chunksoutput(tree)if ((chunk - tree) is not empty) chunk might consist of both chunks (s.a.)then process chunk(chunk - tree, treebank) i.e.
process remaining chunkelse back off to POS sequencepos := pos yield(chunk)tree := complete match(pos, treebank)if (tree is not empty)then output(tree)else back off to subchunkswhile (chunk is not empty) doremove first subchunk c1 from chunkprocess chunk(c1, treebank)Figure 4: Pseudo-code for tree construction, subroutine process chunk.this case, the internal structure of the item to be classified (i.e.
theinput sentence) and of the class item (i.e.
the most similar tree in theinstance base) need to be considered, the classification task is muchmore complex, and the standard memory-based approach needs tobe adapted to the requirements of the parsing task.The features Tu?SBL uses for classification are the sequence ofwords in the input sentence, their respective POS tags and (to alesser degree) the labels in the chunk parse.
Rather than choosing abag-of-words approach, since word order is important for choosingthe most similar tree, the algorithm needed to be modified in orderto rely more on sequential information.Another modification was necessitated by the need to generalizefrom the limited number of trees in the instance base.
The classifi-cation is simple only in those cases where a direct hit is found, i.e.where a complete match of the input with a stored instance exists.In all other cases, the most similar tree from the instance base needsto be modified to match the chunked input.If these strategies for matching complete trees fail, Tu?SBL at-tempts to match smaller subchunks in order to preserve the qual-ity of the annotations rather than attempt to pursue only completeparses.The algorithm used for tree construction is presented in a slightlysimplified form in Figs.
3-6.
For readability?s sake, we assumehere that chunks and complete trees share the same data structureso that subroutines like string yield can operate on both of themindiscriminately.The main routine construct tree in Fig.
3 separates the list of in-put chunks and passes each one to the subroutine process chunk inFig.
4 where the chunk is then turned into one or more (partial)trees.
process chunk first checks if a complete match with an in-stance from the instance base is possible.4 If this is not the case,a partial match on the lexical level is attempted.
If a partial treeis found, attach next chunk in Fig.
5 and extend tree in Fig.
6 areused to extend the tree by either attaching one more chunk or by re-sorting to a comparison of the missing parts of the chunk with treeextensions on the POS level.
attach next chunk is necessary to en-sure that the best possible tree is found even in the rare case that theoriginal segmentation into chunks contains mistakes.
If no partialtree is found, the tree construction backs off to finding a completematch in the POS level or to starting the subroutine for processinga chunk recursively with all the subchunks of the present chunk.The application of memory-based techniques is implemented inthe two subroutines complete match and partial match.
The pre-sentation of the two cases as two separate subroutines is for ex-pository purposes only.
In the actual implementation, the searchis carried out only once.
The two subroutines exist because of4string yield returns the sequence of words included in the inputstructure, pos yield the sequence of POS tags.attach next chunk(tree, treebank): attempts to attach the next chunk to the treetake first chunk chunk2 from chunk listwords2 := string yield(tree, chunk2)tree2 := complete match(words2, treebank)if (tree2 is not empty)thenremove chunk2 from chunk listreturn tree2else return emptyFigure 5: Pseudo-code for tree construction, subroutine attach next chunk.extend tree(rest chunk, tree, treebank): extends the tree on basis of POS comparisonwords := string yield(tree)rest pos := pos yield(rest chunk)tree2 := partial match(words + rest pos, treebank)if ((tree2 is not empty) and (subtree(tree, tree2)))then return tree2else return emptyFigure 6: Pseudo-code for tree construction, subroutine extend tree.the postprocessing of the chosen tree which is necessary for par-tial matches and which also deviates from standard memory-basedapplications.
Postprocessing mainly consists of shortening the treefrom the instance base so that it covers only those parts of the chunkthat could be matched.
However, if the match is done on the lexicallevel, a correction of tagging errors is possible if there is enough ev-idence in the instance base.
Tu?SBL currently uses an overlap met-ric, the most basic metric for instances with symbolic features, asits similarity metric.
This overlap metric is based on either lexicalor POS features.
Instead of applying a more sophisticated metriclike the weighted overlap metric, Tu?SBL uses a backing-off ap-proach that heavily favors similarity of the input with pre-storedinstances on the basis of substring identity.
Splitting up the classi-fication and adaptation process into different stages allows Tu?SBLto prefer analyses with a higher likelihood of being correct.
Thisstrategy enables corrections of tagging and segmentation errors thatmay occur in the chunked input.4.1 ExampleInput:dann w?urde ich sagen ist das vereinbart(then I would say this is arranged)Chunk parser output:[simpx [advx [adv dann]][vxfin [vafin w"urde]][nx2 [pper ich]][vvinf sagen]][simpx [vafin ist][nx2 [pds das]][vvpp vereinbart]]Figure 7: Chunk parser outputFor the input sentence dann w?urde ich sagen ist das vereinbart(then I would say this is arranged), the chunked output is shown inFig.
7.
The chunk parser correctly splits the input into two clausesTable 1: Quantitative evaluationminimum maximum averageprecision 76.82% 77.87% 77.23%recall 66.90% 67.65% 67.28%crossing accuracy 93.44% 93.95% 93.70%dann w?urde ich sagen and ist das vereinbart.
A look-up in theinstance base finds a direct hit for the first clause.
Therefore, thecorrect tree can be output directly.
For the second clause, only apartial match on the level of words can be found.
The system findsthe tree for the subsequence of words ist das, as shown in Fig.
8.By backing off to a comparison on the POS level, it finds a tree forthe sentence hatten die gesagt (they had said) with the same POSsequence and the same structure for the first two words.
Thus theoriginal tree that covers only two words is extended via the newlyfound tree.
Tu?SBL?s output for the complete sentence is shown inFig.
9.5.
QUANTITATIVE EVALUATIONA quantitative evaluation of Tu?SBL has been conducted usinga semi-automatically constructed treebank of German that consistsof appr.
67,000 fully annotated sentences or sentence fragments.5The evaluation consisted of a ten-fold cross-validation test, wherethe training data provide an instance base of already seen cases forTu?SBL?s tree construction module.The evaluation focused on three PARSEVAL measures: labeledprecision, labeled recall and crossing accuracy, with the resultsshown in Table 1.While these results do not reach the performance reported forother parsers (cf.
[7], [8]), it is important to note that the task carriedout here is more difficult in a number of respects:1.
The set of labels does not only include phrasal categories, butalso functional labels marking grammatical relations such assubject, direct object, indirect object and modifier.
Thus, theevaluation carried out here is not subject to the justified crit-icism levelled against the gold standards that are typically5See [13] for further details.0 1500 501502 503504istVAFINdasPDSHD HDVXFINHDNXONLK MFSIMPX?
?Figure 8: A partial tree found be the system0 1 2 3 4 5 6500 501 502 503 504 505506 507 508 509510 511513514515516dannADVw"urdeVAFINichPPERsagenVVINFistVAFINdasPDSvereinbartVVPPHD HD HD HD HD HDADVXMODVXFINHDNXONVXFINHDVXINFOVNXONVF LK MF VC?
?
?
?SIMPXHDVXINFOVLK MF VCSIMPX?
?
?Figure 9: Tu?SBL?s output for the complete sentencein conjunction with the PARSEVAL measures, namely thatthe gold standards used typically do not include annotationsof syntactic-semantic dependencies between bracketed con-stituents.2.
The German treebank consists of transliterated spontaneousspeech data.
The fragmentary and partially ill-formed na-ture of such spoken data makes them harder to analyze thanwritten data such as the Penn treebank typically used as goldstandard.It should also be kept in mind that the basic PARSEVAL mea-sures were developed for parsers that have as their main goal acomplete analysis that spans the entire input.
This runs counter tothe basic philosophy underlying an amended chunk parser such asTu?SBL, which has as its main goal robustness of partially analyzedstructures: Precision and recall measure the percentage of brackets,i.e.
constituents with the same yield or bracketing scope, which areidentical in the parse tree and the gold standard.
If Tu?SBL findsonly a partial grouping on one level, both measures consider thisgrouping wrong, as a consequence of the different bracket scopes.In most cases, the error ?percolates?
up to the highest level.
Fig.10 gives an example of a partially matched tree structure for thesentence ?bei mir ginge es im Februar ab Mittwoch den vierten?
(for me it would work in February after Wednesday the fourth).The only missing branch is the branch connecting the second nounphrase (NX) above ?Mittwoch?
to the NX ?den vierten?.
This re-sults in precision and recall values of 10 out of 15 because of thealtered bracketing scopes of the noun phrase, the two prepositionalphrases (PX), the field level (MF) and the sentence level (SIMPX).In order to capture this specific aspect of the parser, a secondevaluation was performed that focused on the quality of the struc-tures produced by the parser.
This evaluation consisted of manuallyjudging the Tu?SBL output and scoring the accuracy of the recog-nized constituents.
The scoring was performed by the human an-notator who constructed the treebank and was thus in a privilegedposition to judge constituent accuracy with respect to the treebankannotation standards.
This manual evaluation resulted in a scoreof 92.4% constituent accuracy; that is: of all constituents that wererecognized by the parser, 92.4% were judged correct by the hu-man annotator.
This seems to indicate that approximately 20% ofthe precision errors are due to partial constituents whose yield isshorter than in the corresponding gold standard.
Such discrepan-cies typically arise when Tu?SBL outputs only partial trees.
Thisoccurs when no complete tree structures can be constructed thatspan the entire input.6.
CONCLUSION AND FUTURE RESEARCHIn this paper we have described how the Tu?SBL parser extendscurrent chunk parsing techniques by a tree-construction compo-nent that completes partial chunk parses to tree structures includingfunction-argument structure.As noted in section 4, Tu?SBL currently uses an overlap metric, i.e.
the most basic metric for instances with symbolic features, as its0 1 2 3 4 5 6 7 8 9 10 11500 501 502 503 504 505506 507 508 509510 511512513514beiAPPRmirPPERgingeVVFINesPPERimAPPRARTFebruarNNabAPPRMittwochNN,$,denARTviertenNN.$.HD HD HD HD HD ?
HDNX?NXHDVXFINHD ?NXHDNXHDPXFOPP ?NXHDPXHDPX?NXONPXV?MODVF?LK?MF?SIMPXFigure 10: A partially grouped tree output of the T ?USBL systemsimilarity metric.
We anticipate that the results reported in Fig.
1can be further improved by experimenting with more sophisticatedsimilarity metrics.
However, we will have to leave this matter tofuture research.67.
ACKNOWLEDGMENTSThe research reported here was funded both by the German Fed-eral Ministry of Education, Science, Research, and Technology(BMBF) in the framework of the VERBMOBIL Project under Grant01 IV 101 N 0 and by the Deutsche Forschungsgemeinschaft (DFG)in the framework of the Sonderforschungsbereich 441.8.
REFERENCES[1] S. Abney.
Parsing by chunks.
In R. Berwick, S. Abney, andC.
Tenney, editors, Principle-Based Parsing.
KluwerAcademic Publishers, 1991.
[2] S. Abney.
Partial parsing via finite-state cascades.
InJ.
Carroll, editor, Workshop on Robust Parsing (ESSLLI ?96),1996.
[3] R. Bod.
Beyond Grammar: An Experience-Based Theory ofLanguage.
CSLI Publications, Stanford, California, 1998.
[4] R. Bod.
Parsing with the shortest derivation.
In Proceedingsof COLING 2000, 2000.
[5] T. Brants and W. Skut.
Automation of treebank annotation.In Proceedings of NeMLaP-3/CoNLL98, Sydney, Australia,1998.
[6] S. Buchholz, J. Veenstra, and W. Daelemans.
Cascadedgrammatical relation assignment.
In Proceedings ofEMNLP/VLC-99, University of Maryland, USA, June 21-22,1999, pages 239 ?
246, 1999.
[7] E. Charniak.
Statistical parsing with a context-free grammarand word statistics.
In Proceedings of the FourteenthNational Conference on Artifical Intelligence, Menlo Park,1997.6[9] reports that the gain ratio similarity metric has yielded excel-lent results for the NLP applications considered by these investiga-tors.
[8] M. Collins.
Head-Driven Statistical Models for NaturalLanguage Parsing.
PhD thesis, University of Pennsylvania,1999.
[9] W. Daelemans, J. Zavrel, and A. van den Bosch.
Forgettingexceptions is harmful in language learning.
MachineLearning: Special Issue on Natural Language Learning, 34,1999.
[10] H. Feldweg.
Stochastische Wortartendisambiguierung fu?r dasDeutsche: Untersuchungen mit dem robusten SystemLIKELY.
Technical report, Universita?t Tu?bingen, 1993.SfS-Report-08-93.
[11] A. Schiller, S. Teufel, and C. Thielen.
Guidelines fu?r dasTagging deutscher Textkorpora mit STTS.
Technical report,Universita?t Stuttgart and Universita?t Tu?bingen, 1995.
(URL:http://www.sfs.nphil.uni-tuebingen.de/Elwis/stts/stts.html).
[12] C. Stanfill and D. Waltz.
Towards memory-based reasoning.Communications of the ACM, 29(12), 1986.
[13] R. Stegmann, H. Schulz, and E. W. Hinrichs.
Stylebook forthe German Treebank in VERBMOBIL.
Technical Report239, Verbmobil, 2000.
[14] J. Veenstra, A. van den Bosch, S. Buchholz, W. Daelemans,and J. Zavrel.
Memory-based word sense disambiguation.Computers and the Humanities, Special Issue on Senseval,Word Sense Disambiguations, 34, 2000.
[15] J. Zavrel, W. Daelemans, and J. Veenstra.
Resolving PPattachment ambiguities with memory-based learning.
InM.
Ellison, editor, Proceedings of the Workshop onComputational Natural Language Learning (CoNLL?97),Madrid, 1997.
