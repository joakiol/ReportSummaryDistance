Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 558?567,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsA Sentence Interaction Network for Modeling Dependence betweenSentencesBiao Liu1, Minlie Huang1?, Song Liu2, Xuan Zhu2, Xiaoyan Zhu11State Key Lab.
of Intelligent Technology and Systems1National Lab.
for Information Science and Technology1Dept.
of Computer Science and Technology, Tsinghua University, Beijing, China2Samsung R&D Institute, Beijing, China1liubiao2638@gmail.com, {aihuang, zxy-dcs}@tsinghua.edu.cnAbstractModeling interactions between two sen-tences is crucial for a number of natu-ral language processing tasks includingAnswer Selection, Dialogue Act Analy-sis, etc.
While deep learning methodslike Recurrent Neural Network or Convo-lutional Neural Network have been provedto be powerful for sentence modeling,prior studies paid less attention on inter-actions between sentences.
In this work,we propose a Sentence Interaction Net-work (SIN) for modeling the complex in-teractions between two sentences.
By in-troducing ?interaction states?
for word andphrase pairs, SIN is powerful and flexi-ble in capturing sentence interactions fordifferent tasks.
We obtain significant im-provements on Answer Selection and Dia-logue Act Analysis without any feature en-gineering.1 IntroductionThere exist complex interactions between sen-tences in many natural language processing (NLP)tasks such as Answer Selection (Yu et al, 2014;Yin et al, 2015), Dialogue Act Analysis (Kalch-brenner and Blunsom, 2013), etc.
For instance,given a question and two candidate answers below,though they are all talking about cats, only the firstQ What do cats look like?A1 Cats have large eyes and furry bodies.A2 Cats like to play with boxes and bags.answer correctly answers the question about cats?appearance.
It is important to appropriately modelthe relation between two sentences in such cases.
?Correspondence authorFor sentence pair modeling, some methods firstproject the two sentences to fix-sized vectors sep-arately without considering the interactions be-tween them, and then fed the sentence vectorsto other classifiers as features for a specific task(Kalchbrenner and Blunsom, 2013; Tai et al,2015).
Such methods suffer from being unable toencode context information during sentence em-bedding.A more reasonable way to capture sentence in-teractions is to introduce some mechanisms to uti-lize information from both sentences at the sametime.
Some methods attempt to introduce an at-tention matrix which contains similarity scores be-tween words and phrases to approach sentence in-teractions (Socher et al, 2011; Yin et al, 2015).While the meaning of words and phrases may driftfrom contexts to contexts, simple similarity scoresmay be too weak to capture the complex interac-tions, and a more powerful interaction mechanismis needed.In this work, we propose a Sentence InteractionNetwork (SIN) focusing on modeling sentence in-teractions.
The main idea behind this model isthat each word in one sentence may potentially in-fluence every word in another sentence in somedegree (the word ?influence?
here may refer to?answer?
or ?match?
in different tasks).
So, weintroduce a mechanism that allows informationto flow from every word (or phrase) in one sen-tence to every word (or phrase) in another sen-tence.
These ?information flows?
are real-valuedvectors describing how words and phrases interactwith each other, for example, a word (or phrase)in one sentence can modify the meaning of a word(or phrase) in another sentence through such ?in-formation flows?.Specifically, given two sentences s1and s2, forevery word xtin s1, we introduce a ?candidateinteraction state?
for every word x?in s2.
This558state is regarded as the ?influence?
of x?to xt, andis actually the ?information flow?
from x?to xtmentioned above.
By summing over all the ?can-didate interaction states?, we generate an ?interac-tion state?
for xt, which represents the influence ofthe whole sentence s2to word xt.
When feedingthe ?interaction state?
and the word embedding to-gether into Recurrent Neural Network (with LongShort-Time Memory unit in our model), we ob-tain a sentence vector with context informationencoded.
We also add a convolution layer onthe word embeddings so that interactions betweenphrases can also be modeled.SIN is powerful and flexible for modeling sen-tence interactions in different tasks.
First, the ?in-teraction state?
is a vector, compared with a singlesimilarity score, it is able to encode more informa-tion for word or phrase interactions.
Second, theinteraction mechanism in SIN can be adapted todifferent functions for different tasks during train-ing, such as ?word meaning adjustment?
for Di-alogue Act Analysis or ?Answering?
for AnswerSelection.Our main contributions are as follows:?
We propose a Sentence Interaction Network(SIN) which utilizes a new mechanism tomodel sentence interactions.?
We add convolution layers to SIN, which im-proves the ability to model interactions be-tween phrases.?
We obtain significant improvements on An-swer Selection and Dialogue Act Analysiswithout any handcrafted features.The rest of the paper is structured as follows:We survey related work in Section 2, introduce ourmethod in Section 3, present the experiments inSection 4, and summarize our work in Section 5.2 Related WorkOur work is mainly related to deep learning forsentence modeling and sentence pair modeling.For sentence modeling, we have to first repre-sent each word as a real-valued vector (Mikolov etal., 2010; Pennington et al, 2014) , and then com-pose word vectors into a sentence vector.
Severalmethods have been proposed for sentence model-ing.
Recurrent Neural Network (RNN) (Elman,1990; Mikolov et al, 2010) introduces a hiddenstate to represent contexts, and repeatedly feed thehidden state and word embeddings to the networkto update the context representation.
RNN suf-fers from gradient vanishing and exploding prob-lems which limit the length of reachable context.RNN with Long Short-Time Memory Networkunit (LSTM) (Hochreiter and Schmidhuber, 1997;Gers, 2001) solves such problems by introducinga ?memory cell?
and ?gates?
into the network.
Re-cursive Neural Network (Socher et al, 2013; Qianet al, 2015) and LSTM over tree structures (Zhu etal., 2015; Tai et al, 2015) are able to utilize somesyntactic information for sentence modeling.
Kim(2014) proposed a Convolutional Neural Network(CNN) for sentence classification which models asentence in multiple granularities.For sentence pair modeling, a simple idea is tofirst project the sentences to two sentence vectorsseparately with sentence modeling methods, andthen feed these two vectors into other classifiersfor classification (Tai et al, 2015; Yu et al, 2014;Yang et al, 2015).
The drawback of such meth-ods is that separately modeling the two sentencesis unable to capture the complex sentence inter-actions.
Socher et al (2011) model the two sen-tences with Recursive Neural Networks (Unfold-ing Recursive Autoencoders), and then feed sim-ilarity scores between words and phrases (syntaxtree nodes) to a CNN with dynamic pooling to cap-ture sentence interactions.
Hu et al (2014) firstcreate an ?interaction space?
(matching score ma-trix) by feeding word and phrase pairs into a multi-layer perceptron (MLP), and then apply CNN tosuch a space for interaction modeling.
Yin et al(2015) proposed an Attention based ConvolutionalNeural Network (ABCNN) for sentence pair mod-eling.
ABCNN introduces an attention matrix be-tween the convolution layers of the two sentences,and feed the matrix back to CNN to model sen-tence interactions.
There are also some methodsthat make use of rich lexical semantic features forsentence pair modeling (Yih et al, 2013; Yanget al, 2015), but these methods can not be easilyadapted to different tasks.Our work is also related to context modeling.Hermann et al (2015) proposed a LSTM-basedmethod for reading comprehension.
Their modelis able to effectively utilize the context (given bya document) to answer questions.
Ghosh et al(2016) proposed a Contextual LSTM (CLSTM)which introduces a topic vector into LSTM forcontext modeling.
The topic vector in CLSTM is559Figure 1: RNN (a) and LSTM (b)1computed according to those already seen words,and therefore reflects the underlying topic of thecurrent word.3 Method3.1 Background: RNN and LSTMRecurrent Neural Network (RNN) (Elman, 1990;Mikolov et al, 2010), as depicted in Figure 1(a), isproposed for modeling long-distance dependencein a sequence.
Its hidden layer is connected to it-self so that previous information is considered inlater times.
RNN can be formalized asht= f(Wxxt+Whht?1+ bh)where xtis the input at time step t and htis thehidden state.
Though theoretically, RNN is ableto capture dependence of arbitrary length, it tendsto suffer from the gradient vanishing and explod-ing problems which limit the length of reachablecontext.
In addition, an additive function of theprevious hidden layer and the current input is toosimple to describe the complex interactions withina sequence.RNN with Long Short-Time Memory Networkunit (LSTM, Figure 1(b)) (Hochreiter and Schmid-huber, 1997; Gers, 2001) solves such problems byintroducing a ?memory cell?
and ?gates?
into thenetwork.
Each time step is associated with a sub-net known as a memory block in which a ?memorycell?
stores the context information and ?gates?control which information should be added or dis-carded or reserved.
LSTM can be formalized asft= ?(Wf?
[xt, ht?1] + bf)it= ?(Wi?
[xt, ht?1] + bi)?Ct= tanh(WC?
[xt, ht?1] + bC)1This figure referred to http://colah.github.io/posts/2015-08-Understanding-LSTMs/Ct= ft?
Ct?1+ it?
?Ctot= ?(Wo?
[xt, ht?1] + bo)ht= ot?
tanh(Ct)where ?
means element-wise multiplication,ft, it, otis the forget, input and output gate thatcontrol which information should be forgot, inputand output, respectively.
?Ctis the candidate infor-mation to be added to the memory cell state Ct. htis the hidden state which is regarded as a represen-tation of the current time step with contexts.In this work, we use LSTM with peephole con-nections, namely adding Ct?1to compute the for-get gate ftand the input gate it, and adding Cttocompute the output gate ot.3.2 Sentence Interaction Network (SIN)Sentence Interaction Network (SIN, Figure 2)models the interactions between two sentences intwo steps.First, we use a LSTM (referred to as LSTM1)to model the two sentences s1and s2separately,and the hidden states related to the t-th word in s1and the ?
-th word in s2are denoted as z(1)tandz(2)?respectively.
For simplicity, we will use theposition (t, ? )
to denote the corresponding wordshereafter.Second, we propose a new mechanism to modelthe interactions between s1and s2by allowinginformation to flow between them.
Specifically,word t in s1may be potentially influenced by allwords in s2in some degree.
Thus, for word t ins1, a candidate interaction state c?
(i)t?and an inputgate i(i)t?are introduced for each word ?
in s2asfollows:c?
(i)t?= tanh(W(i)c?
[z(1)t, z(2)?]
+ b(i)c)i(i)t?= ?(W(i)i?
[z(1)t, z(2)?]
+ b(i)i)here, the superscript ?i?
indicates ?interaction?.W(i)c,W(i)i, b(i)c, b(i)iare model parameters.
Theinteraction state c(i)tfor word t in s1can then beformalized asc(i)t=|s2|??=1c?(i)t??
i(i)t?where |s2| is the length of sentence s2, and c(i)tcan be viewed as the total interaction informationreceived by word t in s1from sentence s2.
Theinteraction states of words in s2can be similarly560Figure 2: SIN for modeling sentence s1at timestep t. First, we model s1and s2separately with LSTM1and obtain the hidden states z(1)tfor s1and z(2)?for s2.
Second, we compute interaction states based onthese hidden states, and incorporate c(i)tinto LSTM2.
Information flows (interaction states) from s1tos2are not depicted here for simplicity.computed by exchanging the position of z(1)tandz(2)?in c?
(i)t?and i(i)t?while sharing the model param-eters.We now introduce the interaction states intoanother LSTM (referred to as LSTM2) to com-pute the sentence vectors.
Therefore, informationcan flow between the two sentences through thesestates.
For sentence s1, at timestep t, we haveft= ?(Wf?
[xt, ht?1, c(i)t, Ct?1] + bf)it= ?(Wi?
[xt, ht?1, c(i)t, Ct?1] + bi)?Ct= tanh(WC?
[xt, ht?1, c(i)t] + bC)Ct= ft?
Ct?1+ it?
?Ctot= ?(Wo?
[xt, ht?1, c(i)t, Ct] + bo)ht= ot?
tanh(Ct)By averaging all hidden states of LSTM2, we ob-tain the sentence vector vs1of s1, and the sentencevector vs2of s2can be computed similarly.
vs1and vs2can then be used as features for differenttasks.In SIN, the candidate interaction state c?
(i)t?rep-resents the potential influence of word ?
in s2toword t in s1, and the related input gate i(i)t?con-trols the degree of the influence.
The element-wisemultiplication c?(i)t?
?i(i)t?is then the actual influence.By summing over all words in s2, the interactionstate c(i)tgives the influence of the whole sentences2to word t.3.3 SIN with Convolution (SIN-CONV)SIN is good at capturing the complex interactionsof words in two sentences, but not strong enoughfor phrase interactions.
Since convolutional neuralnetwork is widely and successfully used for mod-eling phrases, we add a convolution layer beforeSIN to model phrase interactions between two sen-tences.Let v1, v2, ..., v|s|be the word embeddings of asentence s, and let ci?
Rwd, 1 ?
i ?
|s| ?
w +1, be the concatenation of vi:i+w?1, where w isthe window size.
The representation pifor phrasevi:i+w?1is computed as:pi= tanh(F ?
ci+ b)where F ?
Rd?wdis the convolution filter, and dis the dimension of the word embeddings.In SIN-CONV, we first use a convolution layerto obtain phrase representations for the two sen-tences s1and s2, and the SIN interaction proce-dure is then applied to these phrase representationsas before to model phrase interactions.
The aver-age of all hidden states are treated as sentence vec-tors vcnns1and vcnns2.
Thus, SIN-CONV is SIN withword vectors substituted by phrase vectors.
The561two phrase-based sentence vectors are then fed to aclassifier along with the two word-based sentencevectors together for classification.The LSTM and interaction parameters are notshared between SIN and SIN-CONV.4 ExperimentsIn this section, we test our model on two tasks:Answer Selection and Dialogue Act Analysis.Both tasks require to model interactions betweensentences.
We also conduct auxiliary experimentsfor analyzing the interaction mechanism in ourSIN model.4.1 Answer SelectionSelecting correct answers from a set of candidatesfor a given question is quite crucial for a numberof NLP tasks including question-answering, natu-ral language generation, information retrieval, etc.The key challenge for answer selection is to appro-priately model the complex interactions betweenthe question and the answer, and hence our SINmodel is suitable for this task.We treat Answer Selection as a classificationtask, namely to classify each question-answer pairas ?correct?
or ?incorrect?.
Given a question-answer pair (q, a), after generating the questionand answer vectors vqand vausing SIN, we feedthem to a logistic regression layer to output a prob-ability.
And we maximize the following objectivefunction:p?
(q, a) = ?
(W ?
[vq, va]) + b)L =?
(q,a)y?q,alog p?
(q, a)+(1?
y?q,a) log(1?
p?
(q, a))where y?q,ais the true label for the question-answerpair (q, a) (1 for correct, 0 for incorrect).
For SIN-CONV, the sentence vector vcnnqand vcnnaare alsofed to the logistic regression layer.During evaluation, we rank the answers of aquestion q according to the probability p?
(q, a).The evaluation metrics are mean average precision(MAP) and mean reciprocal rank (MRR).4.1.1 DatasetThe WikiQA2(Yang et al, 2015) dataset is usedfor this task.
Following Yin et al (2015), wefiltered out those questions that do not have any2http://aka.ms/WikiQAQ QA pair A/Q correct A/QTrain 2,118 20,360 9.61 0.49Dev 126 1,130 8.97 1.11Test 243 2,351 9.67 1.21Table 1: Statistics of WikiQA (Q=Question,A=Answer)correct answers from the development and test set.Some statistics are shown in Table 1.4.1.2 SetupWe use the 100-dimensional GloVe vectors3(Pen-nington et al, 2014) to initialize our word embed-dings, and those words that do not appear in Glovevectors are treated as unknown.
The dimension ofall hidden states is set to 100 as well.
The windowsize of the convolution layer is 2.
To avoid overfit-ting, dropout is introduced to the sentence vectors,namely setting some dimensions of the sentencevectors to 0 with a probability p (0.5 in our experi-ment) randomly.
No handcrafted features are usedin our methods and the baselines.Mini-batch Gradient Descent (30 question-answer pairs for each mini batch), with AdaDeltatuning learning rate, is used for model training.We update model parameters after every minibatch, check validation MAP and save model af-ter every 10 batches.
We run 10 epochs in to-tal, and the model with highest validation MAPis treated as the optimal model, and we report thecorresponding test MAP and MRR metrics.4.1.3 BaselinesWe compare our SIN and SIN-CONV model with5 baselines listed below:?
LCLR: The model utilizes rich semantic andlexical features (Yih et al, 2013).?
PV: The cosine similarity score of paragraphvectors of the two sentences is used to rankanswers (Le and Mikolov, 2014).?
CNN: Bigram CNN (Yu et al, 2014).?
ABCNN: Attention based CNN, no hand-crafted features are used here (Yin et al,2015).?
LSTM: The question and answer are modeledby a simple LSTM.
Different from SIN, thereis no interaction between sentences.3http://nlp.stanford.edu/projects/glove/5624.1.4 ResultsResults are shown in Table 2.
SIN performs muchbetter than LSTM, PV and CNN, this justifies thatthe proposed interaction mechanism well capturesthe complex interactions between the question andthe answer.
But SIN performs slightly worse thanABCNN because it is not strong enough at model-ing phrases.
By introducing a simple convolutionlayer to improve its phrase-modeling ability, SIN-CONV outperforms all the other models.For SIN-CONV, we do not observe much im-provements by using larger convolution filters(window size ?
3) or stacking more convolutionlayers.
The reason may be the fact that interactionsbetween long phrases is relatively rare, and in ad-dition, the QA pairs in the WikiQA dataset maybe insufficient for training such a complex modelwith long convolution windows.4.2 Dialogue Act AnalysisDialogue acts (DA), such as Statement, Yes-No-Question, Agreement, indicate the sentence prag-matic role as well as the intention of the speakers(Williams, 2012).
They are widely used in natu-ral language generation (Wen et al, 2015), speechand meeting summarization (Murray et al, 2006;Murray et al, 2010), etc.
In a dialogue, the DAof a sentence is highly relevant to the content ofitself and the previous sentences.
As a result, tomodel the interactions and long-range dependencebetween sentences in a dialogue is crucial for dia-logue act analysis.Given a dialogue (n sentences) d =[s1, s2, ..., sn], we first use a LSTM (LSTM1)to model all the sentences independently.
Thehidden states of sentence siobtained at this stepare used to compute the interaction states ofsentence si+1, and SIN will generate a sentencevector vsiusing another LSTM (LSTM2) for eachsentence siin the dialogue (see Section 3.2) .These sentence vectors can be used as featuresfor dialogue act analysis.
We refer to this methodas SIN (or SIN-CONV for adding a convolutionlayer).For dialogue act analysis, we add a softmaxlayer on the sentence vector vsito predict the prob-ability distribution:p?
(yj|vsi) =exp(vTsi?
wj+ bj)?kexp(vTsi?
wk+ bk)4With extra handcrafted features, ABCNN?s performanceis: MAP(0.692), MRR(0.711).Model MAP MRRLCLR 0.599 0.609PV 0.511 0.516CNN 0.619 0.628ABCNN 0.660 0.677LSTM 0.634 0.648SIN 0.657 0.672SIN-CONV 0.674 0.693Table 2: Results on answer selection4.Figure 3: SIN-LD for dialogue act analysis.LSTM1is not shown here for simplicity.
x(sj)tmeans word t in sj, c(i,sj)tmeans the interactionstate for word t in sj.where yjis the j-th DA tag,wjand bjis the weightvector and bias corresponding to yj.
We maximizethe following objective function:L =?d?D|d|?i=1log p?
(y?si|vsi)where D is the training set, namely a set of dia-logues, |d| is the length of the dialogue, siis thei-th sentence in d, y?siis the true dialogue act labelof si.In order to capture long-range dependence inthe dialogue, we can further join up the sentencevector vsiwith another LSTM (LSTM3).
Thehidden state hsiof LSTM3are treated as the fi-nal sentence vector, and the probability distri-bution is given by substituting vsiwith hsiinp?(yj|vsi).
We refer to this method as SIN-LD (orSIN-CONV-LD for adding a convolution layer),where LD means long-range dependence.
Figure3 shows the whole structure (LSTM1is not shownhere for simplicity).563Dialogue Act Example Train(%) Test(%)Statement-non-Opinion Me, I?m in the legal department.
37.0 31.5Backchannel/Acknowledge Uh-huh.
18.8 18.3Statement-Opinion I think it?s great 12.8 17.2Abandoned/Uninterpretable So,- 7.6 8.6Agreement/Accept That?s exactly it.
5.5 5.0Appreciation I can imagine.
2.4 1.8Yes-No-Question Do you have to have any special training?
2.3 2.0Non-Verbal [Laughter], [Throat-clearing] 1.8 2.3Yes-Answers Yes.
1.5 1.7Conventional-closing Well, it?s been nice talking to you.
1.3 1.9Other Labels(32) 9.1 9.8Total number of sentences 196258 4186Total number of dialogues 1115 19Table 3: Dialogue act labels4.2.1 DatasetWe use the Switch-board Dialogue Act (SwDA)corpus (Calhoun et al, 2010) in our experiments5.SwDA contains the transcripts of several peoplediscussing a given topic on the telephone.
Thereare 42 dialogue act tags in SwDA,6and we list the10 most frequent tags in Table 3.The same data split as in Stolcke et al (2000)is used in our experiments.
There are 1,115 dia-logues in the training set and 19 dialogues in thetest set7.
We also randomly split the original train-ing set as a new training set (1,085 dialogues) anda validation set (30 dialogues).4.2.2 SetupThe setup is the same as that in Answer Selectionexcept: (1) Only the most common 10,000 wordsare used, other words are all treated as unknown.
(2) Each mini batch contains all sentences from3 dialogues for Mini-batch Gradient Descent.
(3)The evaluation metric is accuracy.
(4) We run 30epochs in total.
(5) We use the last hidden state ofLSTM2as sentence representation since the sen-tences here are much shorter compared with thosein Answer Selection.4.2.3 BaselinesWe compare with the following baselines:?
unigram, bigram, trigram LM-HMM: HMMvariants (Stolcke et al, 2000).5http://compprag.christopherpotts.net /swda.html.6SwDA actually contains 43 tags in which ?+?
should notbe treated as a valid tag since it means continuation of theprevious sentence.7http://web.stanford.edu/%7ejurafsky/ws97/Model Accuracy(%)unigram LM-HMM 68.2bigram LM-HMM 70.6trigram LM-HMM 71.0RCNN 73.9LSTM 72.8SIN 74.8SIN-CONV 75.1SIN-LD 76.0SIN-CONV-LD 76.5Table 4: Accuracy on dialogue act analysis.
Inter-annotator agreement is 84%.?
RCNN: Recurrent Convolutional Neural Net-works (Kalchbrenner and Blunsom, 2013).Sentences are first separately embedded withCNN, and then joined up with RNN.?
LSTM: All sentences are modeled separatelyby one LSTM.
Different from SIN, there isno sentence interactions in this method.4.2.4 ResultsResults are shown in Table 4.
HMM variants,RCNN and LSTM model the sentences separatelyduring sentence embedding, and are unable to cap-ture the sentence interactions.
With our inter-action mechanism, SIN outperforms LSTM, andproves that well modeling the interactions be-tween sentences in a dialogue is important for di-alogue act analysis.
After introducing a convo-lution layer, SIN-CONV performs slightly betterthan SIN.
SIN-LD and SIN-CONV-LD model the564Figure 4: L2-norm of the interaction states from question to answer (linearly mapped to [0, 1]).Q: what creates a cloudA: in meteorology , a cloud is a visible massof liquid droplets or frozen crystals madeof water or various chemicals suspendedin the atmosphere above the surface of aplanetary body.Table 5: A question-answer pair example.long-range dependence in the dialogue with an-other LSTM, and obtain further improvements.4.3 Interaction Mechanism AnalysisWe investigate into the interaction states of SINfor Answer Selection to see how our proposed in-teraction mechanism works.Given a question-answer pair in Table 5, forSIN, there is a candidate interaction state c?
(i)?tandan input gate i(i)?tfrom each word t in the ques-tion to each word ?
in the answer.
We investigateinto the L2-norm ||c?(i)?t?
i(i)?t||2to see how wordsin the two sentences interact with each other.
Notethat we have linearly mapped the originalL2-normvalue to [0, 1] as follows:f(x) =x?
xminxmax?
xminAs depicted in Figure 4, we can see that theword ?what?
in the question has little impact tothe answer through interactions.
This is reason-able since ?what?
appears frequently in questions,and does not carry much information for answerselection8.
On the contrary, the phrase ?createsa cloud?, especially the word ?cloud?, transmitsmuch information through interactions to the an-swer, this conforms with human knowledge since8Our statements focus on the interaction, in a sense of?answering?
or ?matching?.
Definitely, such words like?what?
and ?why?
are very important for answering ques-tions from the general QA perspective since they determinethe type of answers.we rely on these words to answer the question aswell.In the answer, interactions concentrate on thephrase ?a cloud is a visible mass of liquiddroplets?
which seems to be a good and com-plete answer to the question.
Although there arealso other highly related words in the answer, theyare almost ignored.
The reason may be failing tomodel such a complex phrase (three relatively sim-ple sentences joined by ?or?)
or the existence ofthe previous phrase which is already a good an-swer.This experiment clearly shows how the interac-tion mechanism works in SIN.
Through interac-tion states, SIN is able to figure out what the ques-tion is asking about, namely to detect those highlyinformative words in the question, and which partin the answer can answer the question.5 Conclusion and Future WorkIn this work, we propose Sentence Interaction Net-work (SIN) which utilizes a new mechanism formodeling interactions between two sentences.
Wealso introduce a convolution layer into SIN (SIN-CONV) to improve its phrase modeling ability sothat phrase interactions can be handled.
SIN ispowerful and flexible to model sentence interac-tions for different tasks.
Experiments show thatthe proposed interaction mechanism is effective,and we obtain significant improvements on An-swer Selection and Dialogue Act Analysis withoutany handcrafted features.Previous works have showed that it is importantto utilize the syntactic structures for modeling sen-tences.
We also find out that LSTM is sometimesunable to model complex phrases.
So, we are go-ing to extend SIN to tree-based SIN for sentencemodeling as future work.
Moreover, applying themodels to other tasks, such as semantic relatednessmeasurement and paraphrase identification, would565also be interesting attempts.6 AcknowledgmentsThis work was partly supported by the NationalBasic Research Program (973 Program) undergrant No.
2012CB316301/2013CB329403, theNational Science Foundation of China under grantNo.
61272227/61332007, and the Beijing HigherEducation Young Elite Teacher Project.
The workwas also supported by Tsinghua University ?
Bei-jing Samsung Telecom R&D Center Joint Labora-tory for Intelligent Media Computing.ReferencesSasha Calhoun, Jean Carletta, Jason M Brenier, NeilMayo, Dan Jurafsky, Mark Steedman, and DavidBeaver.
2010.
The nxt-format switchboard corpus:a rich resource for investigating the syntax, seman-tics, pragmatics and prosody of dialogue.
Languageresources and evaluation, 44(4):387?419.Jeffrey L Elman.
1990.
Finding structure in time.Cognitive science, 14(2):179?211.Felix Gers.
2001.
Long short-term memory in re-current neural networks.
Unpublished PhD disser-tation,?Ecole Polytechnique F?ed?erale de Lausanne,Lausanne, Switzerland.Shalini Ghosh, Oriol Vinyals, Brian Strope, Scott Roy,Tom Dean, and Larry Heck.
2016.
Contextuallstm (clstm) models for large scale nlp tasks.
arXivpreprint arXiv:1602.06291.Karl Moritz Hermann, Tomas Kocisky, EdwardGrefenstette, Lasse Espeholt, Will Kay, Mustafa Su-leyman, and Phil Blunsom.
2015.
Teaching ma-chines to read and comprehend.
In Advances in Neu-ral Information Processing Systems, pages 1684?1692.Sepp Hochreiter and J?urgen Schmidhuber.
1997.Long short-term memory.
Neural computation,9(8):1735?1780.Baotian Hu, Zhengdong Lu, Hang Li, and QingcaiChen.
2014.
Convolutional neural network archi-tectures for matching natural language sentences.In Advances in Neural Information Processing Sys-tems, pages 2042?2050.Nal Kalchbrenner and Phil Blunsom.
2013.
Recurrentconvolutional neural networks for discourse compo-sitionality.
arXiv preprint arXiv:1306.3584.Yoon Kim.
2014.
Convolutional neural net-works for sentence classification.
arXiv preprintarXiv:1408.5882.Quoc V Le and Tomas Mikolov.
2014.
Distributedrepresentations of sentences and documents.
arXivpreprint arXiv:1405.4053.Tomas Mikolov, Martin Karafi?at, Lukas Burget, JanCernock`y, and Sanjeev Khudanpur.
2010.
Recur-rent neural network based language model.
In IN-TERSPEECH 2010, 11th Annual Conference of theInternational Speech Communication Association,Makuhari, Chiba, Japan, September 26-30, 2010,pages 1045?1048.Gabriel Murray, Steve Renals, Jean Carletta, and Jo-hanna Moore.
2006.
Incorporating speaker anddiscourse features into speech summarization.
InProceedings of the main conference on Human Lan-guage Technology Conference of the North Amer-ican Chapter of the Association of ComputationalLinguistics, pages 367?374.
Association for Com-putational Linguistics.Gabriel Murray, Giuseppe Carenini, and Raymond Ng.2010.
Generating and validating abstracts of meet-ing conversations: a user study.
In Proceedings ofthe 6th International Natural Language GenerationConference, pages 105?113.
Association for Com-putational Linguistics.Jeffrey Pennington, Richard Socher, and Christo-pher D. Manning.
2014.
Glove: Global vectors forword representation.
In Empirical Methods in Nat-ural Language Processing (EMNLP), pages 1532?1543.Qiao Qian, Bo Tian, Minlie Huang, Yang Liu, XuanZhu, and Xiaoyan Zhu.
2015.
Learning tag em-beddings and tag-specific composition functions inrecursive neural network.
In Proceedings of the53rd Annual Meeting of the Association for Compu-tational Linguistics and the 7th International JointConference on Natural Language Processing, vol-ume 1, pages 1365?1374.Richard Socher, Eric H Huang, Jeffrey Pennin, Christo-pher D Manning, and Andrew Y Ng.
2011.
Dy-namic pooling and unfolding recursive autoencodersfor paraphrase detection.
In Advances in Neural In-formation Processing Systems, pages 801?809.Richard Socher, Alex Perelygin, Jean Y Wu, JasonChuang, Christopher D Manning, Andrew Y Ng,and Christopher Potts.
2013.
Recursive deep mod-els for semantic compositionality over a sentimenttreebank.
In Proceedings of the conference onempirical methods in natural language processing(EMNLP), volume 1631, page 1642.
Citeseer.Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-beth Shriberg, Rebecca Bates, Daniel Jurafsky, PaulTaylor, Rachel Martin, Carol Van Ess-Dykema, andMarie Meteer.
2000.
Dialogue act modeling forautomatic tagging and recognition of conversationalspeech.
Computational linguistics, 26(3):339?373.566Kai Sheng Tai, Richard Socher, and Christopher DManning.
2015.
Improved semantic representa-tions from tree-structured long short-term memorynetworks.
arXiv preprint arXiv:1503.00075.Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Pei-Hao Su, David Vandyke, and Steve Young.
2015.Semantically conditioned lstm-based natural lan-guage generation for spoken dialogue systems.arXiv preprint arXiv:1508.01745.Jason D Williams.
2012.
A belief tracking challengetask for spoken dialog systems.
In NAACL-HLTWorkshop on Future Directions and Needs in theSpoken Dialog Community: Tools and Data, pages23?24.
Association for Computational Linguistics.Yi Yang, Wen-tau Yih, and Christopher Meek.
2015.Wikiqa: A challenge dataset for open-domain ques-tion answering.
In Proceedings of the Conference onEmpirical Methods in Natural Language Process-ing.
Citeseer.Wen-tau Yih, Ming-Wei Chang, Christopher Meek, andAndrzej Pastusiak.
2013.
Question answering usingenhanced lexical semantic models.Wenpeng Yin, Hinrich Sch?utze, Bing Xiang, andBowen Zhou.
2015.
Abcnn: Attention-based con-volutional neural network for modeling sentencepairs.
arXiv preprint arXiv:1512.05193.Lei Yu, Karl Moritz Hermann, Phil Blunsom, andStephen Pulman.
2014.
Deep learning for answersentence selection.
arXiv preprint arXiv:1412.1632.Xiaodan Zhu, Parinaz Sobhani, and Hongyu Guo.2015.
Long short-term memory over tree structures.arXiv preprint arXiv:1503.04881.567
