Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1924?1929,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsSupervised Keyphrase Extraction as Positive Unlabeled LearningLucas Sterckx?, Cornelia Caragea?, Thomas Demeester?, Chris Develder?
?Ghent University ?
iMinds, Ghent, Belgium{lusterck,tdmeeste,cdvelder}@intec.ugent.be?University of North Texas, Texas, USAcornelia.caragea@unt.eduAbstractThe problem of noisy and unbalanced train-ing data for supervised keyphrase extractionresults from the subjectivity of keyphrase as-signment, which we quantify by crowdsourc-ing keyphrases for news and fashion magazinearticles with many annotators per document.We show that annotators exhibit substantialdisagreement, meaning that single annotatordata could lead to very different training setsfor supervised keyphrase extractors.
Thus, an-notations from single authors or readers leadto noisy training data and poor extraction per-formance of the resulting supervised extractor.We provide a simple but effective solution tostill work with such data by reweighting theimportance of unlabeled candidate phrases ina two stage Positive Unlabeled Learning set-ting.
We show that performance of trainedkeyphrase extractors approximates a classi-fier trained on articles labeled by multiple an-notators, leading to higher average F1scoresand better rankings of keyphrases.
We ap-ply this strategy to a variety of test collec-tions from different backgrounds and showimprovements over strong baseline models.1 IntroductionKeyphrase extraction is the task of extracting a se-lection of phrases from a text document to conciselysummarize its contents.
Applications of keyphrasesrange from summarization (D?Avanzo et al, 2004)to contextual advertisement (Yih et al, 2006) or sim-ply as aid for navigation through large text corpora.Existing work on automatic keyphrase extractioncan be divided in supervised and unsupervised ap-proaches.
While unsupervised approaches are do-main independent and do not require labeled train-ing data, supervised keyphrase extraction allows formore expressive feature design and is reported tooutperform unsupervised methods on many occa-sions (Kim et al, 2012; Caragea et al, 2014).
Arequirement for supervised keyphrase extractors isthe availability of labeled training data.
In literature,training collections for supervised keyphrase extrac-tion are generated in different settings.
In these col-lections, keyphrases for text documents are eithersupplied by the authors or their readers.
In the firstcase, authors of academic papers or news articles as-sign keyphrases to their content to enable fast index-ing or to allow for the discovery of their work inelectronic libraries (Frank et al, 1999; Hulth, 2003;Bulgarov and Caragea, 2015).
Other collections arecreated by crowdsourcing (Marujo et al, 2012) orbased on explicit deliberation by a small group ofreaders (Wan and Xiao, 2008).
A minority of testcollections provide multiple opinions per document,but even then the amount of opinions per documentis kept minimal (Nguyen and Kan, 2007).The traditional procedure for supervisedkeyphrase extraction is reformulating the taskas a binary classification of keyphrase candidates.Supervision for keyphrase extraction faces severalshortcomings.
Candidate phrases (generated in aseparate candidate generation procedure), whichare not annotated as keyphrases, are seen asnon-keyphrase and are used as negative trainingdata for the supervised classifiers.
First, on manyoccasions these negative phrases outnumber truekeyphrases many times, creating an unbalanced19240.0 0.2 0.4 0.6 0.8 1.0Fraction of Users Agree0.00.10.20.30.40.50.6FractionofKeyphrasesOnline NewsLifestyle MagazinesFigure 1: This plot shows the fraction of allkeyphrases from the training set agreed upon versusthe fraction of all annotators.training set (Frank et al, 1999; Kim et al, 2012).Second, as Frank et al (1999) noted: authors donot always choose keyphrases that best describe thecontent of their paper, but they may choose phrasesto slant their work a certain way, or to maximizeits chance of being noticed by searchers.
Anotherproblem is that keyphrases are inherently subjective,i.e., keyphrases assigned by one annotator are notthe only correct ones (Nguyen and Kan, 2007).These assumptions have consequences for training,developing and evaluating supervised models.Unfortunately, a large collection of annotateddocuments by reliable annotators with high overlapper document is missing, making it difficult to studydisagreement between annotators or the resultinginfluence on trained extractors, as well as to providea reliable evaluation setting.
In this paper, weaddress these problems by creating a large testcollection of articles with many different opinionsper article, evaluate the effect on extraction per-formance, and present a procedure for supervisedkeyphrase extraction with noisy labels.2 Noisy Training Data for SupervisedKeyphrase ExtractionA collection of online news articles and lifestylemagazine articles was presented to a panel of 357annotators of various ages and backgrounds, (se-Figure 2: Effect of overlap on extraction perfor-mance.lected and managed by iMinds - Living Labs1) whowere trained to select a limited number of shortphrases that concisely reflect the documents?
con-tents.
No prior limits or constraints were set on theamount, length, or form of the keyphrases.
Eachdocument was presented multiple times to differentusers.
Each user was assigned with 140 articles, butwas not required to finish the full assignment.
Theconstructed training collections have on average sixand up to ten different opinions per article.We visualize the agreement on single keyphrasesin Figure 1, which shows the fraction of annotatedkeyphrases versus agreement by the complete set ofreaders.
Agreement on keyphrases appears low, asa large fraction of all keyphrases assigned to doc-uments (>50%) are only assigned by single annota-tors.
We note that different sets of keyphrases by dif-ferent annotators are the result of the subjectivenessof the task, of different interpretations by the anno-tators of the document, but also because of seman-tically equivalent keyphrases being annotated in dif-ferent forms, e.g., ?Louis Michel?
vs. ?Prime Min-ister Louis Michel?
or ?Traffic Collision?
vs. ?CarAccident?.The observation in Figure 1 has important con-sequences for training models on keyphrases anno-tated by a single annotator, since other annotatorsmay have chosen some among the ones that the sin-1https://www.iminds.be/en/succeed-with-digital-research/proeftuinonderzoek/1925gle selected annotator did not indicate (and hencethese should not be used as negative training data).A single annotator assigning keyphrases to 100documents results on average in a training set with369 positive training instances and 4,981 negativetraining instances generated by the candidate ex-tractor.
When assigning these 100 documents to 9other annotators, the amount of positive instances in-creases to 1,258 keyphrases, which means that labelsfor 889 keyphrase candidates, or 17% of the originalnegative candidates when training on annotations bya single annotator, can be considered noise and rela-beled.
As a result, ratios of positive to negative dataalso change drastically.
We visualize the effect ofusing training data from multiple annotators per doc-ument in Figure 2.
Classifiers trained on the aggre-gated training collection with multiple opinions (us-ing all assigned keyphrases at least once as positivetraining data) perform better on held-out test collec-tions containing only keyphrases of high agreement(assigned by > 2 annotators).When using keyphrases from many different an-notators per document, the amount of positive can-didates increases and as a result, the Macro AverageF1 (MAF1) of the corresponding classifier.
We de-tail our experimental setup and supervised classifierin Section 4.3 Reweighting Keyphrase CandidatesObservations described in Section 2 indicate thatunlabeled keyphrase candidates are not reliable asnegative examples by default.
A more suitable as-sumption is to treat supervised keyphrase extractionas Positive Unlabeled Learning, i.e., an incompleteset of positive examples is available as well as a setof unlabeled examples, of which some are positiveand others negative.
This topic has received muchattention as it knows many applications (Ren et al,2014; du Plessis et al, 2014), but has not been linkedto supervised keyphrase extraction.
We base our ap-proach on work by Elkan and Noto (2008) and mod-ify the supervised extractor by assigning individualweights to training examples.
Instead of assumingthe noise to be random, we assign weights depend-ing on the document and the candidate.By reweighting importance of training samples,we seek to mimic the case of multiple annotators, toFeature DefinitionHead match headkeyphrase == headcandidateExtent match extentkeyphrase == extentcandidateSubstring headkeyphrase substring of headcandidateAlias acronym(headkeyphrase) == headcandidateTable 1: String relation features for coreference res-olutionmodel the uncertainty of negative keyphrase candi-dates, based only on annotations by a single annota-tor.
In a first stage, we train a classifier on the singleannotator data and use predictions on the negative orunlabeled candidates, to reweigh training instances.The reweighted training collection is then used totrain a second classifier to predict a final ranking orthe binary labels of the keyphrase candidates.Positive examples are given unit weight and unla-beled examples are duplicated; one copy of each un-labeled keyphrase candidate x is made positive withweight w(x) = P (keyphrase|x, s = 0) and theother copy is made negative with weight 1 ?
w(x)with s indicating whether x is labeled or not.Instead of assigning this weight as a constant fac-tor of the predictions by the initial classifier as inElkan and Noto (2008), we found that two modifica-tions allow improving the weight estimate, w(x) ?1.
We normalize probabilities P (keyphrase, x, s =0) to candidates not included in the initial setof keyphrases per document.
Next to this self-predicted probability, we include a simple mea-sure indicating pairwise coreference between unla-beled candidates and known keyphrases in a func-tion Coref(candidate, keyphrase) ?
{0, 1}, re-turning 1 if one of the binary indicator features, pre-sented in (Bengtson and Roth, 2008) and shown inTable 1, is present.
In this description, the termhead means the head noun phrase of a candidate orkeyphrase and the extent is the largest noun phraseheaded by the head noun phrase.
The self-predictedprobability is summed with the output of the coref-erence resolver and the final weight becomes:w(x) =min(1, P (keyphrase|x)max(x?,s=0)?d P (keyphrase|x?
).+ max?keyphrase?dCoref(x, keyphrase))(1)with d being a document from the training collec-tion.1926Test CollectionsName Online News Lifestyle Magazines WWW KDD InspecType Sports Articles Fashion, Lifestyle WWW Paper Abstracts KDD Paper Abstracts Paper Abstracts# Documents 1,259 2,202 1,895 1,011 500# Keyphrases 19,340 29,970 3,922 1,966 4,913Keyphrases/User 5.7 4.7 / / /Keyphrases/Document 15.4 13.7 2.0 1.8 9.8Tokens/Document 332 284 164 195 134Candidate Keyphrases/Doc.
52 49 47 54 341/2/3/3+ -gram distribution (%) 55/27/9/9 58/25/9/8 63/27/8/2 60/28/9/3 13/53/25/9Table 2: Description of test collections.Method Online News Lifestyle Magazines WWW KDD InspecMAF1 P@5 MAF1 P@5 MAF1 P@5 MAF1 P@5 MAF1 P@5Single Annotator .364 .416 .294 .315 .230 .189 .266 .200 .397 .432Multiple Annotators .381 .426 .303 .327 / / / / / /Self Training .366 .417 .301 .317 .236 .190 .269 .196 .401 .434Reweighting (Elkan and Noto, 2008) .364 .417 .297 .313 .238 .189 .275 .201 .401 .429Reweighting +Norm +Coref .374 .419 .305 .322 .245 .194 .275 .200 .402 .434Table 3: Mean average F1score per document and precision for five most confident keyphrases on differenttest collections.4 Experiments and ResultsHasan and Ng (2010) have shown that techniquesfor keyphrase extraction are inconsistent and needto be tested across different test collections.
Nextto our collections with multiple opinions (On-line News and Lifestyle Magazines), we apply thereweighting strategy on test collections with setsof author-assigned keyphrases: two sets from Cite-Seer abstracts from the World Wide Web Conference(WWW) and Knowledge Discovery and Data Min-ing (KDD), similar to the ones used in (Bulgarovand Caragea, 2015).
The Inspec dataset is a collec-tion of 2,000 abstracts commonly used in keyphraseextraction literature, where we use the ground truthphrases from controlled vocabulary (Hulth, 2003).Descriptive statistics of these test collections aregiven in Table 2.We use a rich feature set consisting of statis-tical, structural, and semantic properties for eachcandidate phrase, that have been reported as ef-fective in previous studies on supervised extrac-tors (Frank et al, 1999; Hulth, 2003; Kim andKan, 2009): (i) term frequency, (ii) number oftokens in the phrase, (iii) length of the longestterm in the phrase, (iv) number of capital lettersin the phrase, (v) the phrase?s POS-tags, (vi) rel-ative position of first occurrence, (vii) span (rela-tive last occurrence minus relative first occurrence),(viii) TF*IDF (IDF?s trained on large backgroundcollections from the same source) and (ix) TopicalWord Importance, a feature measuring the similar-ity between the word-topic topic-document distribu-tions presented in (Sterckx et al, 2015), with topicmodels trained on background collections from acorresponding source of content.As classifier we use gradient boosted decisiontrees implemented in the XGBoost package (Chenand Guestrin, 2016).
During development, this clas-sifier consistently outperformed Naive Bayes andlinear classifiers like logistic regression or supportvector machines.We compare the reweighting strategy with uni-form reweighting and strategies to counter the im-balance or noise of the training collections, such assubsampling, weighting unlabeled training data as in(Elkan and Noto, 2008), and self-training in whichonly confident initial predictions are used as positiveand negative data.
For every method, global thresh-olds are chosen to optimize the macro averaged F1per document (MAF1).
Next to the threshold sensi-tive F1, we report on ranking quality using the Pre-cision@5 metric.Results are shown in Table 3 with five-fold cross-validation.
To study the effect of reweighting, welimit training collections during folds to 100 docu-ments for each test collection.
Our approach consis-tently improves on single annotator trained classi-fiers, on one occasion even outperforming a trainingcollection with multiple opinions.
Compensating for1927imbalance and noise tends to have less effect whenthe ratio of keyphrases versus candidates is high (asfor Inspec) or training collection is very large.
Whenthe amount of training documents increases, the ra-tio of noisy versus true negative labels drops.5 ConclusionIt has been suggested that keyphrase annotationis highly subjective.
We present two data setswhere we purposely gathered multiple annotationsof the same document, as to quantify the lim-ited overlap between keyphrases selected by differ-ent annotators.
We suggest to treat non-selectedphrases as unlabeled rather than negative trainingdata.
We further show that using multiple anno-tations leads to more robust automatic keyphraseextractors, and propose reweighting of single an-notator labels based on probabilities from a first-stage classifier.
This reweighting approach outper-forms other single-annotator state-of-the-art auto-matic keyphrase extractors on different test collec-tions, when we normalize probabilities per docu-ment and include co-reference indicators.AcknowledgmentsThe authors like to thank the anonymous review-ers for their helpful comments.
The researchpresented in this article relates to STEAMER(http://www.iminds.be/en/projects/2014/07/12/steamer), a MiX-ICON projectfacilitated by iMinds Media and funded by IWT andInnoviris.References[Bengtson and Roth2008] Eric Bengtson and Dan Roth.2008.
Understanding the value of features for coref-erence resolution.
In 2008 Conference on Empiri-cal Methods in Natural Language Processing, EMNLP2008, Proceedings of the Conference, 25-27 October2008, Honolulu, Hawaii, USA, A meeting of SIGDAT,a Special Interest Group of the ACL, pages 294?303.
[Bulgarov and Caragea2015] Florin Adrian Bulgarov andCornelia Caragea.
2015.
A comparison of super-vised keyphrase extraction models.
In Proceedings ofthe 24th International Conference on World Wide WebCompanion, WWW 2015, Florence, Italy, May 18-22,2015 - Companion Volume, pages 13?14.
[Caragea et al2014] Cornelia Caragea, Florin AdrianBulgarov, Andreea Godea, and Sujatha Das Gollapalli.2014.
Citation-enhanced keyphrase extraction fromresearch papers: A supervised approach.
In Proceed-ings of the 2014 Conference on Empirical Methods inNatural Language Processing (EMNLP), pages 1435?1446, Doha, Qatar, October.
Association for Compu-tational Linguistics.
[Chen and Guestrin2016] Tianqi Chen and CarlosGuestrin.
2016.
Xgboost: A scalable tree boostingsystem.
CoRR, abs/1603.02754.
[D?Avanzo et al2004] Ernesto D?Avanzo, BernardoMagnini, and Alessandro Vallin.
2004.
Keyphraseextraction for summarization purposes: The LAKEsystem at DUC-2004.
In Proceedings of the 2004DUC.
[du Plessis et al2014] Marthinus Christoffel du Plessis,Gang Niu, and Masashi Sugiyama.
2014.
Analysisof learning from positive and unlabeled data.
In Ad-vances in Neural Information Processing Systems 27:Annual Conference on Neural Information ProcessingSystems 2014, December 8-13 2014, Montreal, Que-bec, Canada, pages 703?711.
[Elkan and Noto2008] Charles Elkan and Keith Noto.2008.
Learning classifiers from only positive and unla-beled data.
In Proceedings of the 14th ACM SIGKDDInternational Conference on Knowledge Discoveryand Data Mining, Las Vegas, Nevada, USA, August24-27, 2008, pages 213?220.
[Frank et al1999] Eibe Frank, Gordon W. Paynter, Ian H.Witten, Carl Gutwin, and Craig G. Nevill-manning.1999.
Domain specific keyphrase extraction.
In Pro-ceedings of the 16th International Joint Conference onAI, pages 668?673.
[Hasan and Ng2010] Kazi Saidul Hasan and Vincent Ng.2010.
Conundrums in unsupervised keyphrase extrac-tion: Making sense of the state-of-the-art.
In Proceed-ings of the 23rd COLING, COLING 2010, pages 365?373, Stroudsburg, PA, USA.
[Hulth2003] Anette Hulth.
2003.
Improved automatickeyword extraction given more linguistic knowledge.In Proceedings of the 2003 conference on Empiricalmethods in natural language processing, pages 216?223.
[Kim and Kan2009] Su Nam Kim and Min-Yen Kan.2009.
Re-examining automatic keyphrase extractionapproaches in scientific articles.
In Proceedings of theworkshop on multiword expressions: Identification, in-terpretation, disambiguation and applications, pages9?16.
Association for Computational Linguistics.
[Kim et al2012] Su Nam Kim, Olena Medelyan, Min-Yen Kan, and Timothy Baldwin.
2012.
Automatickeyphrase extraction from scientific articles.
Lan-guage Resources and Evaluation, 47(3):723?742, De-cember.1928[Marujo et al2012] Luis Marujo, Anatole Gershman,Jaime Carbonell, Robert Frederking, and Jo ao P. Neto.2012.
Supervised topical key phrase extraction ofnews stories using crowdsourcing, light filtering andco-reference normalization.
In Proceedings of LREC2012.
ELRA.
[Nguyen and Kan2007] Thuy Dung Nguyen and Min-YenKan.
2007.
Key phrase extraction in scientific publi-cations.
In Proceeding of International Conference onAsian Digital Libraries, pages 317?326.
[Ren et al2014] Yafeng Ren, Donghong Ji, and Hong-bin Zhang.
2014.
Positive unlabeled learning fordeceptive reviews detection.
In Proceedings of the2014 Conference on Empirical Methods in NaturalLanguage Processing, EMNLP 2014, October 25-29,2014, Doha, Qatar, A meeting of SIGDAT, a SpecialInterest Group of the ACL, pages 488?498.
[Sterckx et al2015] Lucas Sterckx, Thomas Demeester,Johannes Deleu, and Chris Develder.
2015.
Topi-cal word importance for fast keyphrase extraction.
InProceedings of the 24th International Conference onWorld Wide Web Companion, pages 121?122.
Interna-tional World Wide Web Conferences Steering Com-mittee.
[Wan and Xiao2008] Xiaojun Wan and Jianguo Xiao.2008.
Single document keyphrase extraction usingneighborhood knowledge.
In Proceedings of the 23rdNational Conference on Artificial Intelligence - Vol-ume 2, AAAI 2008, pages 855?860.
[Yih et al2006] Wen-tau Yih, Joshua Goodman, and Vi-tor R. Carvalho.
2006.
Finding advertising keywordson web pages.
In Proceedings of the 15th Interna-tional Conference on World Wide Web, WWW ?06,pages 213?222, New York, NY, USA.
ACM.1929
