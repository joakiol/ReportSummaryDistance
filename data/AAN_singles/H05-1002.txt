Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 9?16, Vancouver, October 2005. c?2005 Association for Computational LinguisticsData-driven Approaches for Information Structure IdentificationOana Postolache,Ivana Kruijff-Korbayova?University of Saarland,Saarbru?cken, Germany{oana,korbay}@coli.uni-saarland.deGeert-Jan M. KruijffGerman Research Center forArtificial Intelligence (DFKI GmbH)Saarbru?cken, Germanygj@dfki.deAbstractThis paper investigates automatic identi-fication of Information Structure (IS) intexts.
The experiments use the PragueDependency Treebank which is annotatedwith IS following the Praguian approachof Topic Focus Articulation.
We auto-matically detect t(opic) and f(ocus), us-ing node attributes from the treebank asbasic features and derived features in-spired by the annotation guidelines.
Wepresent the performance of decision trees(C4.5), maximum entropy, and rule in-duction (RIPPER) classifiers on all tec-togrammatical nodes.
We compare the re-sults against a baseline system that alwaysassigns f(ocus) and against a rule-basedsystem.
The best system achieves an ac-curacy of 90.69%, which is a 44.73% im-provement over the baseline (62.66%).1 IntroductionInformation Structure (IS) is a partitioning of thecontent of a sentence according to its relation tothe discourse context.
There are numerous theo-retical approaches describing IS and its semantics(Halliday, 1967; Sgall, 1967; Vallduv?
?, 1990; Steed-man, 2000) and the terminology used is diverse ?see (Kruijff-Korbayova?
and Steedman, 2003) for anoverview.
However, all theories consider at least oneof the following two distinctions: (i) a Topic/Focus1distinction that divides the linguistic meaning of thesentence into parts that link the sentence content?
We use the Praguian terminology for this distinction.to the discourse context, and other parts that ad-vance the discourse, i.e., add or modify informa-tion; and (ii) a background/kontrast2 distinction be-tween parts of the utterance which contribute to dis-tinguishing its actual content from alternatives thecontext makes available.Information Structure is an important factor in de-termining the felicity of a sentence in a given con-text.
Applications in which IS is crucial are text-to-speech systems, where IS helps to improve thequality of the speech output (Prevost and Steedman,1994; Kruijff-Korbayova?
et al, 2003; Moore et al,2004), and machine translation, where IS improvestarget word order, especially that of free word orderlanguages (Stys and Zemke, 1995).Existing theories, however, state their principlesusing carefully selected illustrative examples.
Be-cause of this, they fail to adequately explain howdifferent linguistic dimensions cooperate to realizeInformation Structure.In this paper we describe data-driven, machinelearning approaches for automatic identification ofInformation Structure; we describe what aspects ofIS we deal with and report results of the performanceof our systems and make an error analysis.
For ourexperiments, we use the Prague Dependency Tree-bank (PDT) (Hajic?, 1998).
PDT follows the theoryof Topic-Focus Articulation (Hajic?ova?
et al, 1998)and to date is the only corpus annotated with IS.Each node of the underlying structure of sentencesin PDT is annotated with a TFA value: t(opic), dif-ferentiated in contrastive and non-contrastive, andf(ocus).
Our system identifies these two TFA val-ues automatically.
We trained three different clas-?
The notion ?kontrast?
with a ?k?
has been introduced in (Vall-duv??
and Vilkuna, 1998) to replace what Steedman calls ?fo-cus?, and to avoid confusion with other definitions of focus.9sifiers, C4.5, RIPPER and MaxEnt using basic fea-tures from the treebank and derived features inspiredby the annotation guidelines.
We evaluated the per-formance of the classifiers against a baseline sys-tem that simulates the preprocessing procedure thatpreceded the manual annotation of PDT, by alwaysassigning f(ocus), and against a rule-based systemwhich we implemented following the annotation in-structions.
Our best system achieves a 90.69% accu-racy, which is a 44.73% improvement over the base-line (62.66%).The organization of the paper is as follows.Section 2 describes the Prague Dependency Tree-bank and the Praguian approach of Topic-Focus Ar-ticulation, from two perspectives: of the theoreti-cal definition and of the annotation guidelines thathave been followed to annotate the PDT.
Section 3presents our experiments, the data settings, resultsand error analysis.
The paper closes with conclu-sions and issues for future research (Section 4).2 Prague Dependency TreebankThe Prague Dependency Treebank (PDT) consists ofnewspaper articles from the Czech National Corpus( ?Cerma?k, 1997) and includes three layers of annota-tion:1.
The morphological layer gives a full mor-phemic analysis in which 13 categories aremarked for all sentence tokens (including punc-tuation marks).2.
The analytical layer, on which the ?surface?syntax (Hajic?, 1998) is annotated, contains an-alytical tree structures, in which every tokenfrom the surface shape of the sentence has acorresponding node labeled with main syntac-tic functions like SUBJ, PRED, OBJ, ADV.3.
The tectogrammatical layer renders the deep(underlying) structure of the sentence (Sgall etal., 1986; Hajic?ova?
et al, 1998).
Tectogram-matical tree structures (TGTSs) contain nodescorresponding only to the autosemantic wordsof the sentence (e.g., no preposition nodes) andto deletions on the surface level; the condi-tion of projectivity is obeyed, i.e., no cross-ing edges are allowed; each node of the tree isassigned a functor such as ACTOR, PATIENT,ADDRESSEE, ORIGIN, EFFECT, the repertoireof which is very rich; elementary coreferencelinks are annotated for pronouns.2.1 Topic-Focus Articulation (TFA)The tectogrammatical level of the PDT was moti-vated by the ever increasing need for large corpora toinclude not only morphological and syntactic infor-mation but also semantic and discourse-related phe-nomena.
Thus, the tectogrammatical trees have beenenriched with features indicating the informationstructure of sentences which is a means of showingtheir contextual potential.In the Praguian approach to IS, the content of thesentence is divided into two parts: the Topic is ?whatthe sentence is about?
and the Focus represents theinformation asserted about the Topic.
A prototypicaldeclarative sentence asserts that its Focus holds (ordoes not hold) about its Topic: Focus(Topic) or not-Focus(Topic).The TFA definition uses the distinction betweenContext-Bound (CB) and Non-Bound (NB) parts ofthe sentence.
To distinguish which items are CB andwhich are NB, the question test is applied, (i.e., thequestion for which a given sentence is the appropri-ate answer is considered).
In this framework, weakand zero pronouns and those items in the answerwhich reproduce expressions present in the question(or associated to those present) are CB.
Other itemsare NB.In example (1), (b) is the sentence under investi-gation, in which CB and NB items are marked.
Sen-tence (a) is the context in which the sentence (b) isuttered, and sentence (c) is the question for whichthe sentence (b) is an appropriate answer:(1) (a) Tom and Mary both came to John?s party.
(b) JohnCBinvitedCBonlyNBherNB.
(c) Whom did John invite?It should be noted that the CB/NB distinction isnot equivalent to the given/new distinction, as thepronoun ?her?
is NB although the cognitive entity,Mary, has already been mentioned in the discourse(therefore is given).The following rules determine which lexical items(CB or NB) belong to the Topic or to the Focus of thesentence (Hajic?ova?
et al, 1998; Hajic?ova?
and Sgall,2001):101.
The main verb and any of its direct dependentsbelong to the Focus if they are NB;2.
Every item that does not depend directly on themain verb and is subordinated to a Focus el-ement belongs to the Focus (where ?subordi-nated to?
is defined as the irreflexive transitiveclosure of ?depend on?);3.
If the main verb and all its dependents are CB,then those dependents di of the verb whichhave subordinated items sm that are NB arecalled ?proxi foci?
; the items sm together withall items subordinated to them belong to the Fo-cus (i,m > 1);4.
Every item not belonging to the Focus accord-ing to 1 ?
3 belongs to the Topic.Applying these rules for the sentence (b) in exam-ple (1) we find the Topic and the Focus of the sen-tence: [John invited]Topic [only her]Focus.It is worth mentioning that although most of thetime, CB items belong to the Topic and NB itemsbelong to the Focus (as it happens in our exam-ple too), there may be cases when the Focus con-tains some NB items and/or the Topic contains someCB items.
Figure 1 shows such configurations: inthe top-left corner the tectogrammatical representa-tion of sentence (1) (b) is presented together withits Topic-Focus partitioning.
The other three con-figurations are other possible tectogrammatical treeswith their Topic-Focus partitionings; the top-rightone corresponds to the example (2), the bottom-leftto (3), and bottom-right to (4).
(2) Q: Which teacher did Tom meet?A: TomCBmetCBthe teacherCBof chemistryNB.
(3) Q: What did he think about the teachers?A: HeCBlikedNBthe teacherCBof chemistryNB.
(4) Q: What did the teachers do?A: The teacherCBof chemistryNBmetNBhisCBpupilsNB.2.2 TFA annotationWithin PDT, the TFA attribute has been annotatedfor all nodes (including the restored ones) from thetectogrammatical level.
Instructions for the assign-ment of the TFA attribute have been specified inFigure 1: Topic-Focus partitionings of tectogram-matical trees.(Bura?n?ova?
et al, 2000) and are summarized in Ta-ble 1.
These instructions are based on the surfaceword order, the position of the sentence stress (into-nation center ?
IC)3 and the canonical order of thedependents.The TFA attribute has three values:1. t ?
for non-contrastive CB items;2. f ?
for NB items;3. c ?
for contrastive CB items.In this paper, we do not distinguish between con-trastive and non-contrastive items, considering bothof them as being just t. In the PDT annotation, thenotation t (from topic) and f (from focus) was chosento be used because, as we mentioned earlier, in themost common cases and in prototypical sentences,t-items belong to the Topic and f-items to the Focus.Prior the manual annotation, the PDT corpus waspreprocessed to mark all nodes with the TFA at-tribute of f, as it is the most common value.
Thenthe annotators corrected the value according to theguidelines in Table 1.Figure 2 illustrates the tectogramatical tree struc-ture of the following sentence:(5) Sebeve?dom?
?mself-confidencevotroku?bastardstoitalebutneotr?a?slo.not shake?But it did not shake the self-confidence of those bas-tards?.?
In the PDT the intonation center is not annotated.
However,the annotators were instructed to use their judgement wherethe IC would be if they uttered the sentence.111.
The bearer of the IC (typically, the rightmost child of the verb) f2.
If IC is not on the rightmost child, everything after IC t3.
A left-side child of the verb (unless it carries IC) t4.
The verb and the right children of the verb before the f-node (cf.
1) that are canon-ically orderedf5.
Embedded attributes (unless repeated or restored) f6.
Restored nodes t7.
Indexical expressions (ja?
I, ty you, te?d now, tady here), weak pronouns, pronominalexpressions with a general meaning (ne?kdo somebody, jednou once) (unless theycarry IC)t8.
Strong forms of pronouns not preceded by a preposition (unless they carry IC) tTable 1: Annotation guidelines; IC = Intonation Center.Each node is labeled with the corresponding word?slemma, the TFA attribute, and the functor attribute.For example, votroku?
has lemma votrok, the TFA at-tribute f, and the functor APP (appurtenance).Figure 2: Tectogramatical tree annotated with t/f.In order to measure the consistency of the annota-tion, Interannotator Agreement has been measured(Vesela?
et al, 2004).4 During the annotation pro-cess, there were four phases in which parallel anno-tations have been performed; a sample of data waschosen and annotated in parallel by three annotators.AGREEMENT 1 2 3 4 AVGt/c/f 81.32 81.89 76.21 89.57 82.24t/f 85.42 83.94 84.18 92.15 86.42Table 2: Interannotator Agreement for TFA assign-ment in PDT 2.0.The agreement for each of the four phases, as wellas an average agreement, is shown in Table 2.
Thesecond row of the table displays the percentage ofnodes for which all three annotators assigned the?
In their paper the authors don?t give Kappa values, nor thecomplete information needed to compute a Kappa statisticsourselves.same TFA value (be it t, c or f).
Because in ourexperiments we do not differentiate between t and c,considering both as t, we computed, in the last rowof the table, the agreement between the three anno-tators after replacing the TFA value c with t.53 Identification of topic and focusIn this section we present data-driven, machinelearning approaches for automatic identification ofInformation Structure.
For each tectogrammaticalnode we detect the TFA value t(opic) or f(ocus) (thatis CB or NB).
With these values one can apply therules presented in Subsection 2.1 in order to find theTopic-Focus partitioning of each sentence.3.1 Experimental settingsOur experiments use the tectogrammatical treesfrom The Prague Dependency Treebank 2.0.6 Statis-tics of the experimental data are shown in Table 3.Our goal is to automatically label the tectogram-matical nodes with topic or focus.
We built ma-chine learning models based on three different wellknown techniques, decision trees (C4.5), rule induc-tion (RIPPER) and maximum entropy (MaxEnt), inorder to find out which approach is the most suitablefor our task.
For C4.5 and RIPPER we use the Wekaimplementations (Witten and Frank, 2000) and forMaxEnt we use the openNLP package.7?
In (Vesela?
et al, 2004), the number of cases when the anno-tators disagreed when labeling t or c is reported; this allowedus to compute the t/f agreement, by disregarding this number.?
We are grateful to the researchers at the Charles University inPrague for providing us the data before the PDT 2.0 officialrelease.?
http://maxent.sourceforge.net/12PDT DATA TRAIN DEV EVAL TOTAL#files 2,53680%31610%31610%3,168100%#sentences 38,73778.3%5,22810.6%5,47711.1%49,442100%#tokens 652,70078.3%87,98810.6%92,66911.1%833,356100%#tecto-nodes 494,75978.3%66,71110.5%70,32311.2%631,793100%Table 3: PDT data: Statistics for the training, devel-opment and evaluation sets.All our models use the same set of 35 features (pre-sented in detail in Appendix A), divided in twotypes:1.
Basic features, consisting of attributes of thetectogrammatical nodes whose values weretaken directly from the treebank annotation.We used a total of 25 basic features, that mayhave between 2 and 61 values.2.
Derived features, inspired by the annotationguidelines.
The derived features are computedusing the dependency information from the tec-togrammatical level of the treebank and thesurface order of the words corresponding tothe nodes.8 We also used lists of forms ofCzech pronouns that are used as weak pro-nouns, indexical expressions, pronouns withgeneral meaning, or strong pronouns.
All thederived features have boolean values.3.2 ResultsThe classifiers were trained on 494,759 instances(78.3%) (cf.
Table 3) (tectogrammatical nodes) fromthe training set.
The performance of the classifierswas evaluated on 70,323 instances (11.2%) from theevaluation set.
We compared our models against abaseline system that assigns focus to all nodes (as itis the most common value) and against a determinis-tic, rule-based system, that implements the instruc-tions from the annotation guidelines.Table 4 shows the percentages of correctly classi-fied instances for our models.
We also performed a?
In the tectogramatical level in the PDT, the order of the nodeshas been changed during the annotation process of the TFAattribute, so that all t items precede all f items.
Our fea-tures use the surface order of the words corresponding to thenodes.10-fold cross validation, which for C4.5 gives accu-racy of 90.62%.BASELINE RULE-BASED C4.5 RIPPER MAXENT62.66 58.92 90.69 88.46?
88.97Table 4: Correctly classified instances (the numbersare given as percentages).
?The RIPPER classifierwas trained with only 40% of the training data.The baseline value is considerably high due to thetopic/focus distribution in the test set (a similar dis-tribution characterizes the training set as well).
Therule-based system performs very poorly, although itfollows the guidelines according to which the datawas annotated.
This anomaly is due to the fact thatthe intonation center of the sentence, which plays avery important role in the annotation, is not markedin the corpus, thus the rule-based system doesn?thave access to this information.The results show that all three models performmuch better than the baseline and the rule-based sys-tem.
We used the ??
test to examine if the dif-ference between the three classifiers is statisticallysignificant.
The C4.5 model significantly outper-forms the MaxEnt model (??
= 113.9, p < 0.001)and the MaxEnt model significantly outperforms theRIPPER model although with a lower level of confi-dence (??
= 9.1, p < 0.01).The top of the decision tree generated by C4.5 inthe training phase looks like this:coref = true| is_member = true| | POS = ...| is_member = false| | is_rightmost = ...coref = false| is_generated = true| | nodetype = ...| is_generated = false| | iterativeness = ...It is worth mentioning that the RIPPER classifierwas built with only 40% of the training set (withmore data, the system crashes due to insufficientmemory).
Interestingly and quite surprisingly, thevalues of all three classifiers are actually greater thanthe interannotator agreement which has an averageof 86.42%.What is the cause of the classifiers?
success?
Howcome that they perform better than the annotatorsthemselves?
Is it because they take advantage of a13large amount of training data?
To answer this ques-tion we have computed the learning curves.
Theyare shown in the figure 3, which shows that, actu-ally, after using only 1% of the training data (4,947instances), the classifiers already perform very well,and adding more training data improves the resultsonly slightly.
On the other hand, for RIPPER,adding more data causes a decrease in performance,and as we mentioned earlier, even an impossibilityof building a classifier.0.820.830.840.850.860.870.880.890.90.910  10  20  30  40  50  60  70  80  90CorrectlyClassifiedInstances% of Training DataFigure 3: Learning curves for C4.5 (+),RIPPER(?
), MaxEnt(?)
and a na?
?ve predictor(2) (introduced in Section 3.3).3.3 Error AnalysisIf errors don?t come from the lack of training data,then where do they come from?
To answer this ques-tion we performed an error analysis.
For each in-stance (tectogrammatical node), we considered itscontext as being the set of values for the features pre-sented in Appendix A.
Table 5 displays in the secondcolumn the number of all contexts.
The last threecolumns divide the contexts in three groups:1.
Only t ?
all instances having these contexts areassigned t;2.
Only f ?
all instances having these contextsare assigned f;3.
Ambiguous ?
some instances that have thesecontexts are assigned t and some other are as-signed f.The last row of the table shows the number of in-stances for each type of context, in the training data.All Only t Only f Ambiguous#contexts 27,901 9,901 13,009 4,991#instances 494,759100%94,05619.01%42,0488.49%358,65572.49%Table 5: Contexts & Instances in the training set.Table 5 shows that the source of ambiguity (andtherefore of errors) stays in 4,991 contexts that cor-respond to nodes that have been assigned both t andf.
Moreover these contexts yield the largest amountof instances (72.49%).
We investigated further theseambiguous contexts and we counted how many ofthem correspond to a set of nodes that are mostly as-signed t (#t > #f), respectively f (#t < #f), and howmany are highly ambiguous (half of the correspond-ing instances are assigned t and the other half f (#t =#f)).
The numbers, shown in Table 6, suggest that inthe training data there are 41,851 instances (8.45%)(the sum of highlighted numbers in the third row ofthe Table 6) that are exceptions, meaning they havecontexts that usually correspond to instances that areassigned the other TFA value.
There are two ex-planations for these exceptions: either they are partof the annotators disagreement, or they have somecharacteristics that our set of features fail to capture.#t > #f #t = #f #t < #f#ambiguouscontexts 998 833 3,155#instancest=50,722f=4,854all=55,57611.23%t=602f=602all=1,2040.24%t=35,793f=266,082all=301,87561.01%Table 6: Ambiguous contexts in the training data.The error analysis led us to the idea of implementinga na?
?ve predictor.
This predictor trains on the train-ing set, and divides the contexts into five groups.
Ta-ble 7 describes these five types of contexts and dis-plays the TFA value assigned by the na?
?ve predictorfor each type.If an instance has a context of type #t = #f, wedecide to assign f because this is the most commonvalue.
Also, for the same reason, new contexts inthe test set that don?t appear in the training set areassigned f.The performance of the na?
?ve predictor on theevaluation set is 89.88% (correctly classified in-stances), a value which is significantly higher than14Context Type In the training set, instances witha context of this type are:PredictedTFA valueOnly t all t tOnly f all f f#t > #f more t than f t#t = #f half t, half f f#t < #f more f than t funseen not seen fTable 7: Na?
?ve Predictor: its TFA prediction foreach type of context.the one obtained by the MaxEnt and RIPPER clas-sifiers (??
= 30.7, p < 0.001 and respectively ?
?= 73.3, p < 0.001), and comparable with the C4.5value, although the C4.5 classifier still performs sig-nificantly better (??
= 26.3, p < 0.001).To find out whether the na?
?ve predictor would im-prove if we added more data, we computed the learn-ing curve, shown in Figure 3.
Although the curveis slightly more abrupt than the ones of the otherclassifiers, we do not have enough evidence to be-lieve that more data in the training set would bringa significant improvement.
We calculated the num-ber of new contexts in the development set, and al-though the number is high (2,043 contexts), theycorrespond to only 2,125 instances.
This suggeststhat the new contexts that may appear are very rare,therefore they cannot yield a big improvement.4 ConclusionsIn this paper we investigated the problem of learn-ing Information Structure from annotated data.
Thecontribution of this research is to show for the firsttime that IS can be successfuly recovered usingmostly syntactic features.
We used the Prague De-pendency Treebank which is annotated with Infor-mation Structure following the Praguian theory ofTopic Focus Articulation.
The results show that wecan reliably identify t(opic) and f(ocus) with over90% accuracy while the baseline is at 62%.Issues for further research include, on the onehand, a deeper investigation of the Topic-Focus Ar-ticulation in the Prague Dependency Treebank ofCzech, by improving the feature set, consideringalso the distinction between contrastive and non-contrastive t items and, most importantly, by inves-tigating how we can use the t/f annotation in PDT(and respectively our results) in order to detect theTopic/Focus partitioning of the whole sentence.We also want to benefit from our experience withthe Czech data in order to create an English corpusannotated with Information Structure.
We have al-ready started to exploit a parallel English-Czech cor-pus, in order to transfer to the English version thetopic/focus labels identified by our systems.ReferencesEva Bura?n?ova?, Eva Hajic?ova?, and Petr Sgall.
2000.
Tagging of very large corpora:Topic-Focus Articulation.
In Proceedings of the 18th International Confer-ence on Computational Linguistics (COLING 2000), pages 139?144.Jan Hajic?.
1998.
Building a syntactically annotated corpus: The Prague Depen-dency Treebank.
In Eva Hajic?ova?, editor, Issues of valency and Meaning.Studies in Honor of Jarmila Panevova?.
Karolinum, Prague.Eva Hajic?ova?
and Petr Sgall.
2001.
Topic-focus and salience.
In Proceedingsof the 39th Annual Meeting of the Association for Computational Linguistics(ACL 2001), pages 268?273, Toulose, France.Eva Hajic?ova?, Barbara Partee, and Petr Sgall.
1998.
Topic-focus articulation,tripartite structures, and semantic content.
In Studies in Linguistics and Phi-losophy, number 71.
Dordrecht: Kluwer.M.
Halliday.
1967.
Notes on transitivity and theme in english, part ii.
Journal ofLinguistic, (3):199?244.Ivana Kruijff-Korbayova?
and Mark Steedman.
2003.
Discourse and InformationStructure.
Journal of Logic, Language and Information, (12):249?259.Ivana Kruijff-Korbayova?, Stina Erricson, Kepa J.
Rodr?
?gues, and ElenaKaragjosova.
2003.
Producing Contextually Appropriate Intonation in anInformation-State Based Dialog System.
In Proceeding of European Chapterof the Association for Computational Linguistics, Budapest, Hungary.Johanna Moore, Mary Ellen Foster, Oliver Lemon, and Michael White.
2004.Generating Tailored, Comparative Description in Spoken Dialogue.
In Pro-ceedings of the Seventeenth International Florida Artificial Intelligence Re-search Sociey Conference.Scott Prevost and Mark Steedman.
1994.
Information Based Intonation Synthe-sis.
In Proceedings of the ARPA Workshop on Human Language Technology,Princeton, USA.Petr Sgall, Eva Hajic?ova?, and Jarmila Panevova?.
1986.
The Meaning of the Sen-tence in Its Semantic and Pragmatic Aspects.
Reidel, Dordrecht.Petr Sgall.
1967.
Functional sentence perspective in a generative description.Prague Studies in Mathematical Linguistics, (2):203?225.Mark Steedman.
2000.
Information Structure and the syntax-phonology inter-face.
Linguistic Inquiry, (34):649?689.Malgorzata Stys and Stefan Zemke.
1995.
Incorporating Discourse Aspects inEnglish-Polish MT: Towards Robust Implementation.
In Recent Advances inNLP, Velingrad, Bulgaria.Enrich Vallduv??
and Maria Vilkuna.
1998.
On rheme and kontrast.
In P. Culicoverand L. McNally, editors, Syntax and Semantics Vol 29: The Limits of Syntax.Academic Press, San Diego.Enrich Vallduv??.
1990.
The information component.
Ph.D. thesis, University ofPennsylvania.Frantis?ek ?Cerma?k.
1997.
Czech National Corpus: A Case in Many Contexts.International Journal of Corpus Linguistics, (2):181?197.Kater?ina Vesela?, Jir???
Havelka, and Eva Hajic?ova.
2004.
Annotators?
Agreement:The Case of Topic-Focus Articulation.
In Proceedings of the Language Re-sources and Evaluation Conference (LREC 2004).Ian H. Witten and Eibe Frank.
2000.
Practical Machine Learning Tools andTechniques with Java Implementations.
Morgan Kaufmann, San Francisco.15Appendix AIn this appendix we provide a full list of the feature names and the values they take (a feature for MaxEnt being acombination of the name, value and the prediction).BASIC FEATURE POSSIBLE VALUESnodetype complex, atom, dphr, list, qcomplexis generated true, falsefunctor ACT, LOC, DENOM, APP, PAT, DIR1, MAT, RSTR, THL, TWHEN, REG,CPHR, COMPL, MEANS, ADDR, CRIT, TFHL, BEN, ORIG, DIR3, TTILL,TSIN, MANN, EFF, ID, CAUS, CPR, DPHR, AIM, EXT, ACMP, THO, DIR2,RESTR, TPAR, PAR, COND, CNCS, DIFF, SUBS, AUTH, INTT, VOCAT,TOWH, ATT, RHEM, TFRWH, INTF, RESL, PREC, PRED, PARTL, HER,MOD, CONTRDcoref true, falseafun Pred, Pnom, AuxV, Sb, Obj, Atr, Adv, AtrAdv, AdvAtr, Coord, AtrObj, ObjAtr,AtrAtr, AuxT, AuxR, AuxP, Apos, ExD, AuxC, Atv, AtvV, AuxO, AuxZ, AuxY,AuxG, AuxK, NAPOS N, A, R, V, D, C, P, J, T, Z, I, NASUBPOS NN, AA, NA, RR, VB, Db, Vp, C=, Dg, PD, Vf, J, J?, P7, P4, PS, Cl, TT, RV, PP,P8, Vs, Cr, AG, Cn, PL, PZ, Vc, AU, PH, Z:, PW, AC, NX, Ca, PQ, P5, PJ, Cv,PK, PE, P1, Vi, P9, A2, CC, P6, Cy, C?, RF, Co, Ve, II, Cd, Ch, J*, AM, Cw,AO, Vt, Vmis member true, falseis parenthesis true, falsesempos n.denot, n.denot.neg, n.pron.def.demon, n.pron.def.pers, n.pron.indef,n.quant.def, adj.denot, adj.pron.def.demon, adj.pron.indef, adj.quant.def,adj.quant.indef, adj.quant.grad, adv.denot.grad.nneg, adv.denot.ngrad.nneg,adv.denot.grad.neg, adv.denot.ngrad.neg, adv.pron.def, adv.pron.indef, v, NAnumber sg, pl, inher, nr, NAgender anim, inan, fem, neut, inher, nr, NAperson 1, 2, 3, inher, NAdegcmp pos, comp, acomp, sup, nr, NAverbmod ind, imp, cdn, nr, NAaspect proc, cpl, nr, NAtense sim, ant, post, nil, NAnumertype basic, set, kind, ord, frac, NAindeftype relat, indef1, indef2, indef3, indef4, indef5, indef6, inter, negat, total1, total2,NAnegation neg0, neg1, NApoliteness polite, basic, inher, NAdeontmod deb, hrt, vol, poss, perm, fac, decl, NAdispmod disp1, disp0, nil, NAresultative res1, res0, NAiterativeness it1, it0, NADERIVED FEATURE POSSIBLE VALUESis rightmost true, falseis rightside from verb true, falseis leftside dependent true, falseis embedded attribute true, falsehas repeated lemma true, falseis in canonical order true, falseis weak pronoun true, falseis indexical expression true, falseis pronoun with general meaning true, falseis strong pronoun with no prep true, false16
