Automatic Acquisition of Phrase Grammars for StochasticLanguage ModelingGiuseppe R iccard i  and Sr in ivas  Banga loreAT&T Labs - Research180 Park  AvenueF lo rham Park ,  NJ  09732{dsp3, srini}@research, at t .
tomAbst ractPhrase-based language models have been recognizedto have an advantage over word-based languagemodels since they allow us to capture long span-ning dependencies.
Class based language modelshave been used to improve model generalization andovercome problems with data sparseness.
In this pa-per, we present a novel approach for combining thephrase acquisition with class construction processto automatically acquire phrase-grammar fragmentsfrom a given corpus.
The phrase-grammar learningis decomposed into two sub-problems, namely thephrase acquisition and feature selection.
The phraseacquisition is based on entropy minimization and thefeature selection is driven by the entropy reductionprinciple.
We further demonstrate hat the phrase-grammar based n-gram language model significantlyoutperforms a phrase-based n-gram language modelin an end-to-end evaluation of a spoken languageapplication.1 In t roduct ionTraditionally, n-gram language models implicitly as-sume words as the basic lexical unit.
However, cer-tain word sequences (phrases) are recurrent in con-strained domain languages and can be thought as asingle lexical entry (e.g.
by and large,  I wouldl i ke  to, United States  of America, etc..).
Atraditional word n-gram based language model canbenefit greatly by using variable length units to cap-ture long spanning dependencies, for any given or-der n of the model.
Furthermore, language mod-eling based on longer length units is applicable tolanguages which do not have a predefined notion ofa word.
However, the problem of data sparseness imore acute in phrase-based language models than inword-based language models.
Clustering words intoclasses has been used to overcome data sparsenessin word-based language models (et.al., 1992; Kneserand Ney, 1993; Pereira et al, 1993; McCandlessand Glass, 1993; Bellegarda et al, 1996; Saul andPereira, 1997).
Although the automatically acquiredphrases can be later clustered into classes to over-come data sparseness, we present a novel approach188of combining the construction of classes during theacquisition of phrases.
This integration of phraseacquisition and class construction results in the ac-quisition of phrase-grammar fragments.
In (Gorin,1996; Arai et al, 1997), grammar fragment acqui-sition is performed through Kullback-Liebler diver-gence techniques with application to topic classifica-tion from text.Although phrase-grammar fragments reduce theproblem of data sparseness, they can result in over-generalization.
For example, one of the classesinduced in our experiments was C1 = {and, but ,because} which one might call the class of conjunc-tions.
However, this class was part of a phrase-grammar fragment such as A T C1 T which resultsin phrases A T and T, A T but T, A T becauseT - a clear case of over-generalization given our cor-pus.
Hence we need to further stochastically sepa-rate phrases generated by a phrase-grammar frag-ment.In this paper, we present our approach to integrat-ing phrase acquisition and clustering and our tech-nique to specialize the acquired phrase fragments.We extensively evaluate the effectiveness of phrase-grammar based n-gram language model and demon-strate that it outperforms a phrase-based n-gramlanguage model in an end-to-end evaluation of a spo-ken language application.
The outline of the paper isas follows.
In Section 2, we review the phrase acqui-sition algorithm presented in (Riccardi et al, 1997).In Section 3, we discuss our approach to phraseacquisition and clustering respectively.
The algo-rithm integrating the phrase acquisition and clus-tering processes is presented in Section 4.
The spo-ken language application for automatic all routing(How May I Help You?
(HMIHY)) that is used forevaluating our approach and the results of our ex-periments are described in Section 5.2 Learn ing  PhrasesIn previous work, we have shown the effectivenessof incorporating manually selected phrases for re-ducing the test set perplexity 1 and the word errorrate of a large vocabulary recognizer (Riccardi etal., 1995; Riccardi et al, 1996).
However, a criticalissue for the design of a language model based onphrases is the algorithm that automatically choosesthe units by optimizing a suitable cost function.
Forimproving the prediction of word probabilities, thecriterion we used is the minimization of the languageperplexity PP(T)  on a training corpus 7".
This al-gorithm for extracting phrases from a training cor-pus is similar in spirit to (Giachin, 1995), but differsin the language model components and optimiza-tion parameters (Riccardi et al, 1997).
In addition,we extensively evaluate the effectiveness of phrasen-gram (n > 2) language models by means of anend-to-end evaluation of a spoken language system(see Section 5).
The phrase acquisition method isa greedy algorithm that performs local optimizationbased on an iterative process which converges to alocal minimum of PP(T) .
As depicted in Figure 1,the algorithm consists of three main parts:?
Generation and ranking of a set of candidatephrases.
This step is repeated at each iterationto constrain the search for all possible symbolsequences observed in the training corpus.?
Each candidate phrase is evaluated in terms ofthe training set perplexity.?
At the end of the iteration, the set of selectedphrases is used to filter the training corpus andreplace each occurrence of the phrase with anew lexical unit.
The filtered training corpuswill be referenced as TII.In the first step of the procedure, a set of candi-date phrases (unit pairs) o is drawn out of a train-ing corpus T and ranked according to a correlationcoefficient.
The most used measure for the interde-pendence of two events is the mutual informationMI(z ,y )  = log P?~4,1 However, in this experi- P(z)P(y)  "ment, we use a correlation coefficient that has pro-vided the best convergence speed for the optimiza-tion procedure:P(~' Y) (1)P~'~ - P(z) + P(y)where P(z) is the probability of symbol z.
The coef-ficient p~,y (0 _< p~,~ _< 0.5) is easily extended to de-fine Pz~,z2 ..... z. for the n-tuple (xl, x2 .
.
.
.
.
zn) (0 _<p~, ......... _< l/n).
Phrases (x, y) with high p~,y or~.. P(x) = P(y).
In MI(z ,y)  are such that P(z ,y)  _lThe perplexity PP(T)  of a corpus 7" is PP(T)  =exp( -~ log P(T) ) ,  where n is the number of words in T.:aWe ranked symbol pairs and increased the phrase lengthby successive iteration.
An additional speed up to the algo-rithm could be gained by ran.king symbol k-tuple$ (k > 2) ateach iteration.189the case of P(z,  y) = P(z) = P(y), px,y = 0.5 whileMI  = - logP(z).
Namely, the ranking by MI  isbiased towards events with low probability eventswhich are not likely to be selected by our MaximumLikelihood algorithm.
In fact, the phrase (z,y)F\[ Training Set FilteringGenerateandrankcandidate phrasess elect.atpdhrases Vk perp'e  7 ,o,m,za,,o?
|\$$III ||||||JIIFigure 1: Algorithm for phrase selection1!17!17la ~0 J i ,0o ,so ~o 2;0Figure 2: Training set perplexity vs number of se-lected phrases using p (solid line) and MI (dashedline).will be selected only if P ( r ,  y) = P(x) ~_ P(y) andthe training set perplexity is decreased when (z, y) istreated as a single unit.
In Figure 2 we show the be-havior of the training set perplexity (learning curve)by incorporating an increasing number of selectedphrases using p~,y and MI(x,  y) as ranking coeffi-cients.
In particular, after evaluating 1000 phrasesand selecting 300 of those, the perplexity decrease is20% and 4% using P~4, and MI(z ,  y) respectively.Each of the candidate phrases (z, y) is treated as asingle unit in order to build a stocha.stic model A ofk-th (k > 2) order based on the filtered training cor-pus, T!
a.
Then, (z, y) is selected by the algorithmif PP~,(T) < P'P(7.).
At the end of each iterationthe set {(~, ~)} is selected and employed to filter thetraining corpus.
The algorithm iterates until theperplexity" decrease saturates or a specified numberof phrases have been selected.
4The second issue in building a phrase-based lan-guage model is the training of large vocabularystochastic finite state machines.
In (Riccardi etal., 1996) we present a unified framework for learn-ing stochastic finite state machines (Variable NgramStochastic Automata, VNSA) from a given corpus7- for large vocabulary tasks.
The stochastic finitestate machine learning algorithm in (Riccardi et al,1995) is designed in such a way that it can recognizeany possible sequence of basic unit while?
minimizing the number of parameters (statesand transitions).?
computing state transition probabilities basedon word, phrase and class n-grams by imple-menting different back-off strategies.For the word sequence II e = uq,w2, .
.
.
,wg ,  astandard word n-gram model provides the followingprobability" decomposition:P(IV) = H P(wilwi_n+l .... .
wi-x) (2)iThe phrase n-gram model maps from W into abracketed sequence suchas \[w,\]l , \[w~, u,a\].t~ .
.
.
.
.
\[wiv-.~, w,v-t, u,'6,\]f^,.
Then,the probability P(IV) can be computed as:P(W)  = I IP(f i l f i_ ,~+t .....  f i _ l )  (3)iBy comparing equations 2 and 3 it is evident how thephrase n-gram model allows for an increased rightand left context in computing P(W).In order to evaluate the test perplexity perfor-mance of our phrase-based VNSA, we have split theHow May I Help You?
data collection into an 8Kand 1K training and test set, respectively.
In Fig-ure 3, the test set perplexity is measured versus theVNSA orders for word and phrase language mod-els.
It is worth noticing that the largest perplex-ity decrease comes from using phrase bigram whencompared against word bigram.
Furthermore, theperplexity of the phrase models is always lower thanthe corresponding word models.3At  the  f i s t  i te ra t ion  T ~7" 1.4The language model, estimation component of  the  algo-rithm guards against he problem of overfitting (Riccardi etal.. 1996).190212819 - - -  ~,~!17~17 - -- ~'~ ~,~14.Z-pin"15..6 ISA3 &..p~2:word bigram2-phr:phrase bigram3:word trigram3-Fhr:phrase trigramFigure 3: Test set perplexity vs VNSA LanguageModel Order3 Cluster ing  PhrasesIn the context of language modeling, clustering hastypically been used on words to induce classes thatare then used to predict smoothed probabilities ofoccurrence for rare or unseen events in the train-ing corpus.
Most clustering schemes (et.al., 1992;Kneser and Ney, 1993; Pereira et al, 1993; McCan-dless and Glass, 1993; Bellegarda et al, 1996; Sauland Pereira, 1997) use the average ntropy reductionto decide when two words fall into the same cluster.In contrast, our approach to clustering words is sim-ilar to Schutze(1992).
The words to be clusteredare each represented as a feature vector and similar-ity between two words is measured in terms of thedistance between their feature vectors.
Using thesedistances, words are clustered to produce a hierar-chy.
The hierarchy is then cut at a certain depth topi-oduce clusters which are then ranked by a good-ness metric.
This method assigns each word to aunique class, thus producing hard clusters.3.1 Vector  Representat ionA set of 50 high frequency words from the givencorpus are designated as the "context words".
Theidea is that the high frequency words will mostly befunction words which serve as good discriminatorsfor content words (certain content words appear onlywith certain function words).Each word is associated with a feature vectorwhose components are as follows:1.
Left context: The coocurrence frequency ofeach of the context word appearing in a windowof 3 words to the left of the current word is com-puted.
This determines the distribution of the con-text words to the left of the current word within awindow of 3 words.2.
Right context: Similarly, the distribution ofthe context words appearing within a window of 3words to the right of the current word is computed.This leaves us with adjacent wordssharing a lot oftile surrounding context and hence might end upClassIndexC363CI18C357C260C300C301C277C202C204C77C275C256C197C68C41C199C27C327C48C69C143C89C23C90Compactness Class MembersValue0.1310.1800.1900.2160.2330.2360.2410.2520.2630.2680.2720.2740.2780.2780.2900.2910.2960.2960.2990.3080.3120.3140.3230.332make placeeight eighty five four nine oh one seven six three two zerobill chargean and because but so whenKOokfrom pleaseagain hereis it'sdifferent hirdnumber numbersneed needed want wantedassistance directory informationall before happenedninety sixtyhis our the theircalled dialed got haveas by in no not now of or something that that's there whatever workingI I 'm I'vecanada england france germany israel italy japan london mexico parisback direct out throughconnected going itarizona california carolina florida georgia illinois island jersey maryland michigan missouriohio pennsylvania virginia west yorkbe either go see somebody themabout me off some up youTable 1: The results of clustering words from tile How May I Help You ?
corpusin the same class, s To prevent this situation fromhappening, we include two additional sets of featuresfor the immediate l ft and immediate right contexts.Adjacent words then will have different immediatecontext profiles.3.
Immediate Left context: The distribution ofthe context words appearing to the immediate leftof the current word.4: Immediate Right context: The distribution ofthe context words appearing to the immediate rightof the current word.Thus each word of the vocabulary is representedby a 200 component vector.
The frequencies of thecomponents of the vector are normalized by the fre-quency of the word itself.The Left and Right features are intended to cap-ture the effects of wider range contexts thus col-lapsing contexts that differ only due to modifiers,while the Immediate Left and Right features are in-tended to capture the effects of local contexts.
Byincluding both sets of features, the effects of the lo-cal contexts are weighted more than the effects ofthe wider range contexts, a desirable property.
Thesame result might be obtained by weighting the con-tributions of individual context positions differently,~It is unlikely that with fine grained classes, two wordsbelonging to the same class will follow each other.with the closest position weighted most heavily.3.2 D is tance  Computat ion  andH ierarch ica l  c lus ter ingHaving set up a feature vector for each word, thesimilarity between two words is measured using theManhattan distance metric between their featurevectors.
Manhattan distance is based on the sum ofthe absolute value of the differences among the coor-dinates.
This metric is much less sensitive to outliersthan the Euclidean metric.
We experimented withother distance metrics such as Euclidean and maxi-mum, but Manhattan gave us the best results.Having computed the distance matrix, the wordsare hierarchically clustered with a compact linkage 6,in which the distance between two clusters is thelargest distance between a point in one cluster and apoint in the other cluster(Jain and Dubes, 1988).
Ahierarchical clustering method was chosen since weexpected to use the hierarchy as a back-off model.Also, since we don't know a priori the number ofclusters we want, we did not use clustering schemessuch as k-means clustering method where we wouldhave to specify the number of clusters from the start.6~Ve tried other l inkage strategies such as average linkageand connected linkage, but compact  l inkage gave the bestresults.191ClassIndexD365 0.226D325 0.232D380 0.239D386 0.243D382 0.276D288 0.281D186 0.288D148 0.315D87 0.315D183 0.321D143 0.326D387 0.327D4 0.336DT0 0.338D383 0.341D381 0.347D159 0.347Compactness Class MembersValuewrong:C77 secondC256:C256 C256area:code:C11&C118:Cl18:C118:C118 C68a:C77 this:C77C260:C357:C143:to:another C260:C357:C143:to:my:homeC327:C275:to:C363 I'd:like:to:C363 to:C363 yes:I'd:like:to:C363good:morning yes:ma'am yes:operator hello hi ma'am may wellproblems troubleA:T:C260:T C260:C327 C27:C27 C41:C77 Cl18 C143 C260C197 C199 C202 C23 C260 C27 C277C301 C69 C77 C90 operator toC118:C118:hundred C204 telephonenew:C89 C48 C89 colorado massachusetts ennessee t xasmy:home my:home:phonemy:calling my:calling:card my:cardC199:a:wrong:C77 misdialedlike:to:C363 trying:to:C363 would:like:to:C363like:to:C363:a:collect:call:to like:to:C363:collect:callwould:like:to:C363:a:collect:callwould:like:to:C363:a:collect:call:toCl18:Cl18 Cl18:Cl18:Cl18C118:C118:C118:C118:CI 18:C118C118:C118:Cl18:C118:C118:C118:C118C118:C118:C118:Cl18:C118:C118:C118:C118C118:Cl18:Cl18:Cl18:C118:Cl18:C118:Cl18:Cl18:C118Cl18:C118:Cl18:Cl18:Cl18:Cl18:Cl18:Cl18:C118:CI 18:C118area:code:C118:C118:C118 C300Table 2: The results of the first iteration of combining phrase acquistion and clustering from tile HMIHYcorpus.
(Words in a phrase are separated by a ":".
Tile members of Ci's are shown in Table 1)3.2.1 Choosing the number  of clustersOne of the most tricky issues in clustering is thechoice of the number of clusters after the clusteringis complete.
Instead of predetermining the numberof clusters to be fixed, we use the median of thedistances between clusters merged at the successivestages as the cutoff and prune the hierarchy at thepoint where the cluster distance xceeds the cutoffvalue.
Clusters are defined by the structure of thetree above the cutoff point.
(Note that the clusterdistance increases as we climb up the hierarchy).3.2.2 Ranking the clustersOnce the clusters are formed, the goodness of thecluster is measured by its compactness value.
Thecompactness value of a cluster is simply the averagedistance of the members of the cluster from the cen-troid of the cluster.
The components of the centroidvector is computed as the component-wise averageof the vector epresentations of each of the membersof the cluster.The method described above is general in that itcan  be used to either cluster words and phrases.
"Fa-ble 1 illustrates the result of clustering words andTable 2 illustrates tile result of clustering phrasesfor the training data from our application domain.For example, the first iteration of the algorithmclusters words and the result is shown in Table 1.Each word in the corpus is replaced by its class la-bel.
If the word is not a member of any class then itis left unchanged.
This transformed corpus is inputto the phrase acquisition process.
Figure 4 showsinteresting and long phrases that are formed afterthe phrase acquisition process.
Table 2 shows the re-sult of subsequent clustering of the phrase-annotatedcorpus.1.
like:to:C363:a:collect:call:to2.
like:to:C363:collect:call3.
would:like:to:C363:a:collect:call4.
would:like:to:C363:a:collect:call:toFigure 4: Sample phrases that include classlabel C363={make place}.The components o:\[ aphrase are separated by a :.1924 Learn ing  Phrase  GrammarIn the previous ections we have shown algorithmsfor acquiring (see section 2) and clustering (seesection 3) phrases.
While it is straightforward topipeline the phrase acquisition and clustering algo-rithms, in the context of learning phrase-grammarsthey are not separable.
Thus, we cannot first learnphrases and then cluster them or vice versa.
For ex-ample, in order to cluster together the phrase cuto f f  and disconnected, we first have to learn thephrase cut of:f. On the other hand, in order tolearn the phrase area code :for <city> we firsthave to learn the cluster <city>, containing citynames (e.g.
Boston, New York, etc..).Learning phrase grammars can be thought as an it-erative process that is composed of two languageacquisition strategies.
The goal is to search thosefeatures f, sequence of terminal and non-terminalsymbols, that provide the highest learning rate (theentropy reduction within a learning interval, firststrategy) and minimize the language entropy (sec-ond strategy, same as in section 2).Phrase Clustm'ingPhrase Grammar Learningby .. ...... .~Perplexity MinimizationFigure 5: Algorithm for Phrase-grammar acquisitionInitially, the set of features f drawn from a corpus7" contains terminal symbols V0 only.
New featurescan be generated by either1.
grouping (conjunction operator) an existing setof symbols, Vi, into phrasesor2.
map an existing set ofsymbols ~ into a new setof symbols V/+l (disjunction operator) throughthe categorization provided by the clustering al-gorithm.The whole symbol space is then given by V = U i  v/as shown in Figure 6 and the problem of learning193the best feature set is then decomposed into two sub-problems: to find the oplimalsubset ofV (first learn-ing strategy) that gives us the best features (secondlearning strategy) generated by a given set V/.v0Figure 6: The sequence of symbol sets V/ generatedby successive clustering steps.In order to combine the two optimization prob-lems, we have integrated them into a greedy al-gorithm as shown in Figure 5.
In each algorithmiteration we might first cluster the current set ofphrases and extract a set of non-terminal symbolsand then acquire the phrases (containing terminaland non-terminal symbols) in order to minimize thelanguage ntropy.
XVe use the clustering step of oura!gorithm to control the steepness of the learningcurve within a subset V/of the whole feature space.In fact, by varying the clustering rate (number oftimes clustering is performed for an entire acquisi-tion experiment) we optimize the reduction of thelanguage ntropy for each feature selection (entropyreduction principle).
Thus, the search for the op-timal subset of V is designed so as to maximizethe entropy reduction AH(f )  over a set of featuresfl = {fl ,f2 .
.
.
.
,fro} in V/:maz~AH(fl) = mazr~\['I~.
(T) - fl~,,(T) (4)1 to (7) = mart, i (5)where/~,1 (7") is the entropy of the corpus 7" basedon the phrase n-gram model Aft and Ao is the ini-tial model and equation 5 follows from equation 4 inthe sense of the law of large numbers.
The searchspace over all possible features f in equation 4 isbuilt upon the notion of phrase ranking accordingto the p measure (see Section 2) and phrase cluster-ing rate.
By varying these two parameters we cansearch for the best learning strategies following thegreedy algorithm given in Section 2.
In Figure 7,we give an example of slow and quick learning, de-fined by the rate of entropy reduction within an in-terval.
The discontinuities in the learning curvescorrespond to the clustering algorithm step.
Themaximization in equation 5 is carried out for eachinterval between the entropy discontinuities.
There-fore.
the quick learning strategy provides the ~estlearning curve in the sense of equation 5.i, .
.
.
.
.
.
.so ~oo ~o ~ ~ 5oo i~o too ~ 1~o ~g,~oO,,,,a~ t .~ l12 , ,  .
.
.
.
.
\[,I.
1 :t .
.
.
.
.
.
.
.
.Figure 7: Examples of slow and quick phrase-grammar learning.4.1 Training Language Models for LargeVocabulary SystemsPhrase-grammars allow for an increased generaliza-tion.
since they can generate phrases that may neverhave been observed in the training corpus, but yetsimilar to the ones that have been observed.
Thisgeneralization property is also used for smoothingthe word probabilities in the context of stochasticlanguage modeling for speech recognition and under-standing.
Standard class-based models smooth tileword n-gram probability P(wi\[wi_n+l, .
.
.
, Wi-l) inthe following way:P(wilWi-n+l .
.
.
.
, Wi-l) = (6)P(CiICi-,~+, .
.
.
.
.
Ci-,)P(wilC~)where P(CilCi-,~+l,-..,Ci-l) is the class n-gramprobability and P(wilCi) is the class membershipprobability.
However, phrases recognized by thesame phrase-grammar can actually occur within dif-ferent syntactic ontexts but their similarity is basedon their most likely lexical context.
In (Riccardi etal., 1996) we have developed a context dependenttraining algorithm of the phrase class probabilities.In particular,P(u, ilwi-,~+l .
.
.
.
.
wi-1) =P(C, ICi-,~+l .
.
.
.
,C , -1 ;S)e(wi lC , ;S)  (7)where S is the state of the language model assignedby the VNSA model (Riccardi et al, 1996).
Inparticular, S = S(wi-n+l .
.
.
.
,Wi_l;,,~/) is deter-mined by the word history wi_n+l , .
.
.
,Wi_ l  and194the phrase-grammar model A I.
For example, our al-gorithm has acquired the conjunction cluster {but,and, because} that leads to generate phrases likeA T and T or A T because T, the latter clearly anerroneous generalization given our corpus.
However,training context dependent probabilities as shown inEquation 7 delivers a stochastic separation betweenthe correct and incorrect phrases:P(A T and T)logp(A T but T)=5"7 (8)Given a set of phrases containing terminal andnon terminal symbols, the goal of large vocabularystochastic language modeling for speech recognitionand understanding is to assign a probability to allterminal symbol sequences.
One of the main motiva-tion for learning phrase-grammars is to decrease thelocal uncertainty in decoding spontaneous speechby embedding tightly constrained structure in thelarge vocabulary automaton.
The language mod-els trained on the acquired phrase-grammars give aslight improvement in perplexity (average measureof uncertainty).
Another figure of merit in evalu-ating a stochastic language model is its local en-tropy ( -~ i  P(s, ls)togP(s, ls)) which is related tothe notion of the branching factor of a languagemodel state s. In Figure 8 we plot the local entropyhistograms for word, phrase and phrase-grammarbigram stochastic models.
The word bigram dis-tribution reflects the sparseness of the word pairconstraints.
The phrase-grammar based languagemodel delivers a local entropy distribution skewedin tile range \ [0 -  1\] because of the tight constraintsenforced by the phrase-grammars...tO20?
s , , s  ,.,,.Lo.,~,..a.~,.,., ~'sI-I_ I71_-Figure 8: Local entropy histograms for word, phraseand phrase-grammar bigram VNSAs.5 Spoken Language App l i ca t ionWe have applied the algorithms for phrase-grammaracquisition to the How May I Kelp You?
(Gorin etal., 1997) speech understanding task.
We briefly re-view the problem and the spoken language system.The goal is to understand caller's responses to theopen-ended prompt How May I Help ~bu?
and routesuch a call based on the meaning of the response.Thus we aim at extracting a relatively small num-ber of semantic actions from the utterances of a verylarge set of users who are not trained to the system'scapabilities and limitations.The first utterance of each transaction has beentranscribed and marked with a call-type by label-ers.
There are 14 call-types plus a class other forthe complement class.
In particular, we focused ourstudy on the classification of the caller's first utter-ance in these dialogs.
The spoken sentences varywidely in duration, with a distribution distinctivelyskewed around a mean value of 5.3 seconds corre-sponding to 19 words per utterance.
Some examplesof the first utterances are given below:?
Yes ma'am ~here  is a rea  code two  zeroone??
I'm tryn'a call and I can't get it I;ogo through I wondered if you could tryit for me please??
HelloIn the the training set there are 3,6K words whichdefine the lexicon.
Tile out-of-vocabulary (OOV)rate at the token level is 1.6%, yielding a sentence-level OOV rate of 30%.
Significantly, only 50 out ofthe I00 lowest rank singletons were cities and nameswhile the other were regular words like authorized,realized, etc.For call type classification from speech we designeda large vocabulary one-step speech recognizer utiliz-ing the phrase-grammar stochastic (section 4) modelthat achieved 60% word accuracy.
Then, we catego-rized the decoded speech input into call-types, usingthe salient fragment classifier developed in (Gorin,1996; Gorin et al, 1997).
The salient phrases havethe property of modeling local constraints of thelanguage while carrying most of the semantic inter-pretation of the whole utterance.
A block diagramof the speech understanding system is given in Fig-ure 10.
In an automated call router there are twoimportant performance measures.
The first is theprobability of false rejection, where a call is falselyrejected or classified as other.
Since such calls wouldbe transferred to a human agent, this corresponds toa missed opportunity for automation.
The secondmeasure is the probability of correct classification.Errors in this dimension lead to misinterpretationsthat must be resolved by a dialog manager (Abellaand Gorin, 1997).
In Figure 9, we plot the proba-bility of correct classification versus the probabil-ity of false rejection, for different speech recogni-tion language models and the same classifier (Gorinet al, 1997).
The curves are generated by vary-ing a salience threshold (Gorin, 1996).
In a dialog195system, it would be useful even if the correct call-type was one of the top 2 choices of the decisionrule (Abella and Gorin, 1997).
Thus, in Figure 9 theclassification scores are shown for the first and sec-ond ranked call-types identified by the understand-ing algorithm.
Phrase-grammar trigram model iscompared to the baseline system which is based onthe phrase-based stochastic finite state machines de-scribed in (Gorin et al, 1997).
The phrase-grammarmodel outperforms the baseline phrase-based model.and it achieves a 22% classification error rate re-duction.
The second set of curves (Text) in Figure 9give an upper bound on the performance from speechexperiments.
It is worth noting, the rank 2 perfor-mance of the phrase-grammar model is aligned withrank 1 classification performance on the true tran-scriptions (dashed lines).100 m95-~ 85ROC CUl%,es lot 1K test set.
.
.
.
.
.
.
~ ~ Rank.
2" .
- ~ " ~  ~ :" * ~ ~0 Rank 1.d  ~ R~kZJ~ ABtlk IopI ~ Speed~: Auto-Pt'~ra.se C~mar0 20 30 40 50 60False relec~on ~te  (%)Figure 9: Rank 1 and 2 classification rate plot fromtext and speech with phrase-grammar trigramI Large roe .~ulary twClassifierCSFSM~ech~ ~n.eSt~e Mach~eW~cx: led  v.*xt mi iq lFigure 10: Block diagram of the speech understand-ing system6 Conc lus ionsIn this paper, we have presented a novel approach toautomatically combine the acquisition of grammarfragments for language modeling.
The phrase gram-mar learning is decomposed into two sub-problems,namely the phrase acquisition and feature selection.The phrase acquisition is based on entropy mini-mization and the feature selection is driven by theentropy reduction principle.
This integration resultsin the learning of stochastic phrase-grammar frag-ments, which are then context dependent trained onthe corpus at hand.
We also demonstrated that aphrase-grammar b sed language model significantlyoutperformed a phrase-based language model in anend-to-end evaluation of a spoken language applica-tion.Re ferencesA.
Abella and A. L. Gorin.
1997.
Generating se-mantically consistent inputs to a dialog manager.In Proc.
EUROSPEECH, European Conferenceon Speech Communication and Technology, pages1879-1882, September.K.
Arai, J. H. Wright, G. Riccardi, and A. L. Gorin.1997.
Grammar fragment acquisition using ,;yn-tactic and semantic clustering.
In Proc.
WorkshopSpoken Language Understanding and Communi-cation, December.J.
Bellegarda, J. Butzberger, Y. Chow, N. Coccaro,and D. Naik.
1996.
A novel word clustering algo-rithm based on latent semantic analysis.
In Pro-ceedings of ICASSP-96, pages 172-175.Brown et.al.
1992.
Class-based n-gram models ofnatural anguage.
In Computational Linguistics,volume 18(4), pages 467-479.E.
Giachin.
1995.
Phrase bigram for continousspeech recognition.
In Proc.
\[EEE Int.
Conf.Acoust., Speech, Signal Proc., pages 225-228,May.A.
L. Gorin, G. Riccardi, and J. H. Wright.
1997.How May I Help You?
In Speech Communication,volume 23, pages 113-127.A.
L. Gorin.
1996.
Processing of semantic informa-tion in fluently spoken language.
In Proc.
of Intl.Conf.
on Spoken Language Processing, October.A.K.
Jain and R.C.
Dubes.
1988.
Algorithms forClustering Data.
Englewood Cliffs, NJ: Prentice-Hall.Reinhard Kneser and Hermann Ney.
1993.
Im-proved clustering techniques for class--based sta-tistical anguage modelling.
In Eurospeech.Michael K. McCandless and James R. Glass.
1993.Empirical acquisition of word and phrase classesin the atis domain.
In In Third European Confer-ence on Speech Communication and Technology,Berlin.
Germany, September.196Fernando C. N. Pereira, Naftali Z. Tishby, and Lil-lian Lee.
1993.
Distributional clustering of en-glish words.
In 30th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 183-190, Columbus, Ohio.G.
Riccardi, E. Bocchieri, and R. Pieraccini.
1995.Non deterministic stochastic language models forspeech recognition.
In Proc.
IEEE Int.
Conf.Acoust., Speech, Signal Proc., May.G.
Riccardi, R. Pieraccini, and E. Bocchieri.
1996.Stochastic automata for language modeling.
InComputer Speech and Language, pages 265-293,December.G.
Riccardi, A. L. Gorin, A. Ljolje, and M. Riley.1997.
A spoken language understanding for au-tomated call routing.
In Proc.
IEEE Int.
Conf.Acoust., Speech, Signal Proc., pages 1143-1146,April.L.
Saul and F. Pereira.
1997.
Aggregate and mixed-order markov models for statistical language pro-cessing.
In Proceedings of the 2nd Conference onEmpirical Methods in Natural Language Process-ing.Hinrich Schutze.
1992.
Dimensions of meaning.
InProceedings of Supercomputing, Minneapolis MN.,pages 787-796.
