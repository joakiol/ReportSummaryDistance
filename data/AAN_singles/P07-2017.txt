Proceedings of the ACL 2007 Demo and Poster Sessions, pages 65?68,Prague, June 2007. c?2007 Association for Computational LinguisticsAn Approximate Approach for Training Polynomial Kernel SVMs inLinear TimeYu-Chieh Wu Jie-Chi Yang Yue-Shi LeeDept.
of Computer Science andInformation EngineeringGraduate Institute of Net-work Learning TechnologyDept.
of Computer Science andInformation EngineeringNational Central University National Central University Ming Chuan UniversityTaoyuan, Taiwan Taoyuan, Taiwan Taoyuan, Taiwanbcbb@db.csie.ncu.edu.tw yang@cl.ncu.edu.tw lees@mcu.edu.twAbstractKernel methods such as support vector ma-chines (SVMs) have attracted a great dealof popularity in the machine learning andnatural language processing (NLP) com-munities.
Polynomial kernel SVMs showedvery competitive accuracy in many NLPproblems, like part-of-speech tagging andchunking.
However, these methods areusually too inefficient to be applied to largedataset and real time purpose.
In this paper,we propose an approximate method toanalogy polynomial kernel with efficientdata mining approaches.
To prevent expo-nential-scaled testing time complexity, wealso present a new method for speeding upSVM classifying which does independentto the polynomial degree d. The experi-mental results showed that our method is16.94 and 450 times faster than traditionalpolynomial kernel in terms of training andtesting respectively.1 IntroductionKernel methods, for example support vectormachines (SVM) (Vapnik, 1995) are successfullyapplied to many natural language processing (NLP)problems.
They yielded very competitive andsatisfactory performance in many classificationtasks, such as part-of-speech (POS) tagging(Gimenez and Marquez, 2003), shallow parsing(Kudo and Matsumoto, 2001, 2004; Lee and Wu,2007), named entity recognition (Isozaki andKazawa, 2002), and parsing (Nivre et al, 2006).In particular, the use of polynomial kernel SVMimplicitly takes the feature combinations into ac-count instead of explicitly combines features.
Bysetting with polynomial kernel degree (i.e., d), dif-ferent number of feature conjunctions can be im-plicitly computed.
In this way, polynomial kernelSVM is often better than linear kernel which didnot use feature conjunctions.
However, the trainingand testing time costs for polynomial kernel SVMis far slow than the linear kernel.
For example, ittook one day to train the CoNLL-2000 task withpolynomial kernel SVM, while the testing speed ismerely 20-30 words per second (Kudo and Ma-tsumoto, 2001).
Although the author provided thesolution for fast classifying with polynomial kernel(Kudo and Matsumoto, 2004), the training time isstill inefficient.
Nevertheless, the testing time oftheir method exponentially scales with polynomialkernel degree d, i.e., O(|X|d) where |X| denotes asthe length of example X.On the contrary, even the linear kernel SVMsimply disregards the effect of feature combina-tions during training and testing, it performs notonly more efficient than polynomial kernel, butalso can be improved through directly appendingfeatures derived from the set of feature combina-tions.
Examples include bigram, trigram, etc.
Nev-ertheless, selecting the feature conjunctions wasmanually and heuristically encoded and shouldperform amount of validation trials to discoverwhich is useful or not.
In recent years, severalstudies had reported that the training time of linearkernel SVM can be reduced to linear time(Joachims, 2006; Keerthi and DeCoste, 2005).
Butthey did not and difficult to be extent to polyno-mial kernels.In this paper, we propose an approximate ap-proach to extend the linear kernel SVM towardpolynomial.
By introducing the well-known se-quential pattern mining approach (Pei et al, 2004),65frequent feature conjunctions, namely patternscould be discovered and also kept as expand fea-ture space.
We then adopt the mined patterns to re-represent the training/testing examples.
Subse-quently, we use the off-the-shelf linear kernelSVM algorithm to perform training and testing.Besides, to exponential-scaled testing time com-plexity, we propose a new classification methodfor speeding up the SVM testing.
Rather thanenumerating all patterns for each example, ourmethod requires O(Favg*Navg) which is independentto the polynomial kernel degree.
Favg is the averagenumber of frequent features per example, while theNavg is the average number of patterns per feature.2 SVM and Kernel MethodsSuppose we have the training instance set for bi-nary classification problem:}1 ,1{ ,  ),,(),...,,(),,( 2211 ?+???
iDinn yxyxyxyxwhere xi is a feature vector in D-dimensionspace of the i-th example, and yi is the label of xieither positive or negative.
The training of SVMsinvolves in minimize the following object (primalform, soft-margin) (Vapnik, 1995):?=+?=niii yxWLossCWWW1),(21)(  :minimize ?
(1)The loss function indicates the loss of trainingerror.
Usually, the hinge-loss is used (Keerthi andDeCoste, 2005).
The factor C in (1) is a parameterthat allows one to trade off training error and mar-gin.
A small value for C will increase the numberof training errors.To determine the class (+1 or -1) of an examplex can be judged by computing the following equa-tion.
))),(((sign)( ?
?+=SVsxiiiibxxKyxy ?
(2)?i is the weight of training example xi (?i>0),and b denotes as a threshold.
Here the xi should bethe support vectors (SVs), and are representative oftraining examples.
The kernel function K is thekernel mapping function, which might map fromD?
to 'D?
(usually D<<D?).
The natural linear ker-nel simply uses the dot-product as (3).
),(),( ii xxdotxxK =                                             (3)A polynomial kernel of degree d is given by (4).dii xxdotxxK )),(1(),( +=                                      (4)One can design or employ off-the-shelf kerneltypes for particular applications.
In particular to theuse of polynomial kernel-based SVM, it wasshown to be the most successful kernels for manynatural language processing (NLP) problems(Kudo and Matsumoto, 2001; Isozaki and Kazawa,2002; Nivre et al, 2006).It is known that the dot-product (linear form)represents the most efficient kernel computingwhich can produce the output value by linearlycombining all support vectors such as?
?=+=SVsxiiiixywbwxdotxy ?
ere        wh)),((sign)((5)By combining (2) and (4), the determination ofan example of x using the polynomial kernel canbe shown as follows.
)))1),((((sign)( bxxdotyxy diSVsxiii++= ???
(6)Usually, degree d is set more than 1.
When d isset as 1, the polynomial kernel backs-off to linearkernel.
Although the effectiveness of polynomialkernel, it can not be shown to linearly combine allsupport vectors into one weight vector whereas itrequires computing the kernel function (4) for eachsupport vector xi.
The situation is even worse whenthe number of support vectors become huge (Kudoand Matsumoto, 2004).
Therefore, whether intraining or testing phrase, the cost of kernel com-putations is far more expensive than linear kernel.3 Approximate Polynomial KernelIn 2004, Kudo and Matsumoto (2004) derived bothimplicitly (6) and explicitly form of polynomialkernel.
They indicated that the use of explicitlyenumerate the feature combinations is equivalentto the polynomial kernel (see Lemma 1 and Exam-ple 1, Kudo and Matsumoto, 2004) which sharedthe same view of (Cumby and Roth, 2003).We follow the similar idea of the above studiesthat requires explicitly enumerated all feature com-binations.
To meet with our problem, we employthe well-known sequential pattern mining algo-rithm, namely PrefixSpan (Pei et al, 2004) to effi-cient mine the frequent patterns.
However, directlyadopt the algorithm is not a good idea.
To fit withSVM, we modify the original PrefixSpan algo-rithm according to the following constraints.Given a set features, the PrefixSpan mines thefrequent patterns which occurs more than prede-fined minimum support in the training set and lim-ited in the length of predefined d, which is equiva-lent to the polynomial kernel degree d. For exam-66ple, if the minimum support is 5, and d=2, then afeature combination (fi, fj) must appear more than 5times in set of x.Definition 1 (Frequent single-item sequence):Given a set of feature vectors x, minimum support,and d, mining the frequent patterns (feature combi-nations) is to mine the patterns in the single-itemsequence database.Lemma 2 (Ordered feature vector):For each example, the feature vector could betransformed into an ordered item (feature) list, i.e.,f1<f2<?<fmax where fmax is the highest dimension ofthe example.Proof.
It is very easy to sort an unordered featurevector into the ordered list with conventional sort-ing algorithm.Definition 3 (Uniqueness of the features per ex-ample):Given the set of mined patterns, for any feature fi,it is impossible to appear more than once in thesame pattern.Different from conventional sequential patternmining method, in feature combination mining forSVM only contains a set of feature vectors each ofwhich is independently treated.
In other words, nocompound features in the vector.
If it exists, onecan simply expand the compound features as an-other new feature.By means of the above constraints, mining thefrequent patterns can be reduced to mining the lim-ited length of frequent patterns in the single-itemdatabase (set of ordered vectors).
Furthermore,during each phase, we need only focus on findingthe ?frequent single features?
to expand previousphase.
More detail implementation issues can refer(Pei et al, 2004).3.1 Speed-up TestingTo efficiently expand new features for the originalfeature vectors, we propose a new method to fastdiscovery patterns.
Essentially, the PrefixSpan al-gorithm gradually expands one item from previousresult which can be viewed as a tree growing.
Anexample can be found in Figure 1.Each node in Figure 1 is the associate feature ofroot.
The whole patterns expanded by fj can be rep-resented as the path from root to each node.
Forexample, pattern (fj, fk, fm, fr) can be found via trav-ersing the tree starting from fj.
In this way, we canre-expand the original feature vector via visitingcorresponding trees for each feature.Figure 1: The tree representation of feature fjTable 1: Encoding frequent patterns with DFS arrayrepresentationLevel 0 1 2 3 2 1 2 1 2 2Label Root k m r p m p o p qItem fj fk fm fr fp fm fp fo fp fqHowever, traversing arrays is much more effi-cient than visiting trees.
Therefore, we adopt the l2-sequences encoding method based on the DFS(depth-first-search) sequence as (Wang et al, 2004)to represent the trees.
An l2-sequence does not onlystore the label information but also take the nodelevel into account.
Examples can be found in Table1.Theorem 4 (Uniqueness of l2-sequence): Giventrees T1, and T2, their l2-sequences are identical ifand only if T1 and T2 are isomorphic, i.e., thereexists a one-to-one mapping for set of nodes, nodelabels, edges, and root nodes.Proof.
see theorem 1 in (Wang et al, 2004).Definition 5 (Ascend-descend relation):Given a node k of feature fk in l2-sequence, all ofthe descendant of k that rooted by k have thegreater feature numbers than fk.Definition 6 (Limited visiting space):Given the highest feature fmax of vector X, and fkrooted l2-sequence, if fmax<fk, then we can not findany pattern that prefix by fk.Both definitions 5 and 6 strictly follow lemma 2that kept the ordered relations among features.
Forexample, once node k could be found in X, it isunnecessary to visit its children.
More specifically,to determine whether a frequent pattern is in X, weneed to compare feature vector of X and l2-sequence database.
It is clearly that the time com-plexity of our method is O(Favg*Navg) where Favg isthe average number of frequent features per exam-ple, while the Navg is the average length of l2-sequence.
In other words, our method does not de-pendent on the polynomial kernel degree.674 ExperimentsTo evaluate our method, we examine the well-known shallow parsing task which is the task ofCoNLL-20001.
We also adopted the released perl-evaluator to measure the recall/precision/f1 rates.The used feature consists of word, POS, ortho-graphic, affix(2-4 prefix/suffix letters), and previ-ous chunk tags in the two words context windowsize (the same as (Lee and Wu, 2007)).
We limitedthe features should at least appear more than twicein the training set.For the learning algorithm, we replicate themodified finite Newton SVM as learner which canbe trained in linear time (Keerthi and DeCoste,2005).
We also compare our method with the stan-dard linear and polynomial kernels with SVMlight 2.4.1 ResultsTable 2 lists the experimental results on theCoNLL-2000 shallow parsing task.
Table 3 com-pares the testing speed of different feature expan-sion techniques, namely, array visiting (our method)and enumeration.Table 2: Experimental results for CoNLL-2000 shal-low parsing taskCoNLL-2000 F1 Mining TimeTrainingTimeTestingTimeLinear Kernel 93.15 N/A 0.53hr 2.57sPolynomial(d=2) 94.19 N/A 11.52hr 3189.62sPolynomial(d=3) 93.95 N/A 19.43hr 6539.75sOur Method(d=2,sup=0.01)93.71 <10s 0.68hr 6.54sOur Method(d=3,sup=0.01)93.46 <15s 0.79hr 9.95sTable 3: Classification time performance of enu-meration and array visiting techniquesArray visiting Enumeration CoNLL-2000 d=2 d=3 d=2 d=3Testing time 6.54s 9.95s 4.79s 11.73sChunking speed(words/sec) 7244.19 4761.50 9890.81 4038.95It is not surprising that the best performance wasobtained by the classical polynomial kernel.
Butthe limitation is that the slow in training and test-ing time costs.
The most efficient method is linearkernel SVM but it does not as accurate as polyno-mial kernel.
However, our method stands for bothefficiency and accuracy in this experiment.
Interms of training time, it slightly slower than thelinear kernel, while it is 16.94 and ~450 timesfaster than polynomial kernel in training and test-1 http://www.cnts.ua.ac.be/conll2000/chunking/ 2 http://svmlight.joachims.org/ing.
Besides, the pattern mining time is far smallerthan SVM training.As listed in Table 3, we can see that our methodprovide a more efficient solution to feature expan-sion when d is set more than two.
Also it demon-strates that when d is small, the enumerate-basedmethod is a better choice (see PKE in (Kudo andMatsumoto, 2004)).5 ConclusionThis paper presents an approximate method forextending linear kernel SVM to analogy polyno-mial-like computing.
The advantage of this methodis that it does not require maintaining the cost ofsupport vectors in training, while achieves satisfac-tory result.
On the other hand, we also propose anew method for speeding up classification which isindependent to the polynomial kernel degree.
Theexperimental results showed that our method closeto the performance of polynomial kernel SVM andbetter than the linear kernel.
In terms of efficiency,our method did not only improve 16.94 timesfaster in training and 450 times in testing, but alsofaster than previous similar studies.ReferencesChad Cumby and Dan Roth.
2003.
Kernel methods for rela-tional learning.
International Conference on MachineLearning, pages 104-114.Hideki Isozaki and Hideto Kazawa.
2002.
Efficient supportvector classifiers for named entity recognition.
Interna-tional Conference on Computational Linguistics, pages 1-7.Jian Pei, Jiawei Han, Behzad Mortazavi-Asl, Jianyong Wang,Helen Pinto, Qiming Chen, Umeshwar Dayal and Mei-Chun Hsu.
2004.
Mining Sequential Patterns by Pattern-Growth: The Prefix Span Approach.
IEEE Trans.
onKnowledge and Data Engineering, 16(11): 1424-1440.Sathiya Keerthi and Dennis DeCoste.
2005.
A modified finiteNewton method for fast solution of large scale linear SVMs.Journal of Machine Learning Research.
6: 341-361.Taku Kudo and Yuji Matsumoto.
2001.
Fast methods forkernel-based text analysis.
Annual Meeting of the Associa-tion for Computational Linguistics, pages 24-31.Taku Kudo and Yuji Matsumoto.
2001.
Chunking with sup-port vector machines.
Annual Meetings of the NorthAmerican Chapter and the Association for the Computa-tional Linguistics.Yue-Shi Lee and Yu-Chieh Wu.
2007.
A Robust MultilingualPortable Phrase Chunking System.
Expert Systems withApplications, 33(3): 1-26.Vladimir N. Vapnik.
1995.
The Nature of Statistical Learn-ing Theory.
Springer.Chen Wang, Mingsheng Hong, Jian Pei, Haofeng Zhou, WeiWang and Baile Shi.
2004.
Efficient Pattern-GrowthMethods for Frequent Tree Pattern Mining.
Pacific knowl-edge discovery in database (PAKDD).68
