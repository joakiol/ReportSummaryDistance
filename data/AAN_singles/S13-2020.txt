Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 114?117, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsUNAL: Discriminating between Literal and FigurativePhrasal Usage Using Distributional Statistics and POS tagsSergio Jimenez, Claudia BecerraUniversidad Nacional de ColombiaCiudad Universitaria,edificio 453, oficina 114Bogot?, Colombiasgjimenezv@unal.edu.cocjbecerrac@unal.edu.coAlexander GelbukhCIC-IPNAv.
Juan Dios B?tiz, Av.
Mendiz?bal,Col.
Nueva Industrial VallejoCP 07738, DF, M?xicogelbukh@gelbukh.comAbstractIn this paper we describe the system used toparticipate in the sub task 5b in the Phrasal Se-mantics challenge (task 5) in SemEval 2013.This sub task consists in discriminating lit-eral and figurative usage of phrases withcompositional and non-compositional mean-ings in context.
The proposed approach isbased on part-of-speech tags, stylistic featuresand distributional statistics gathered from thesame development-training-test text collec-tion.
The system obtained a relative improve-ment in accuracy against the most-frequent-class baseline of 49.8% in the ?unseen con-texts?
(LexSample) setting and 8.5% in ?un-seen phrases?
(AllWords).1 IntroductionThe Phrasal Semantics task-5b in SemEval 2013consisted in the discrimination of literal of figura-tive usage of phrases in context (Korkontzelos et al2013).
For instance, the occurrence in a text of thephrase ?a piece of cake?
can be used whether to re-fer to something that is pretty easy or to an actualpiece of cake.
The motivation for this task is thatsuch discrimination could improve the quality andperformance of other tasks like machine translationand information retrieval.This problem has been studied in the past.
Lin(1999) observed that the distributional characteris-tics of the literal and figurative usage are different.Katz and Giesbrecht (2006) showed that the similar-ities among contexts are correlated with their literalor figurative usage.
Birke and Sarkar (2006) clus-tered literal and figurative contexts using a word-sense-disambiguation approach.
Fazly et al(2009)showed that literal and figurative usages are relatedto particular syntactical forms.
Sporleder and Li(2009) showed that for a particular phrase the con-texts of its literal usages are more cohesive thanthose of its figurative usages.
Inspired by theseworks and in a new observation, we proposed a set orfeatures based on cohesiveness, syntax and stylom-etry (Section 2), which are used to train a machinelearning classifier.The cohesiveness between a phrase an its contextcan be measured aggregating the relatedness of thecontext words against the target phrase.
This cohe-siveness should be high for phrases used literally.Conversely, figurative usages can occur in a largevariety of contexts implying low cohesiveness.
Forinstance, the cohesiveness of the phrase ?a piece ofcake?
against context words such as ?coffee?, ?birth-day?
and ?bakery?
should be high.
The distribu-tional measures used to obtain the needed related-ness scores and the proposed measures of cohesive-ness are presented in subsection 2.1.Moreover, we observed a stylistic trend in thetraining data set.
That is, figurative usage tends tooccur later in the document in comparison with theliteral usage.
Consequently, a small set of featuresthat exploits this particular observation is proposedin subsection 2.2.Fazly et al(2009) showed that idiomatic phrasescomposed of a verb and a noun (e.g.
?break a leg?
)differ from their literal usages in the use of somesyntactic structures.
For instance, idiomatic phrasesare less flexible in the use of determiners, pluraliza-114tion and passivization.
In order to capture that no-tion in a simple way, a set of features form a part-of-speech tagger was included in the feature set (seesubsection 2.3).In Section, additional details of the proposed sys-tem are provided jointly with the obtained officialresults.
Finally, in sections 4 and 5 a brief discus-sion of the results and some concluding remarks arepresented.2 FeaturesEach instance of the training and test sets consist of ashort document d where one or more occurrences ofits target phase pd are annotated.
For each particularphrase p, several instances are provided correspond-ing to literal or figurative usages.
In this section, theset of features that was extracted from each instanceto provide a vectorial representation is presented.2.1 Cohesiveness FeaturesLet?s start with some definitions borrowed from theinformation retrieval field: D is a collection of doc-uments, df(w) is the number of documents in Dwhere the word w occurs (document frequency),df(w ?
pd) is the number of documents where wand a target phrase pd co-occur, tf(w, d) is the num-ber of occurrences of w in a document d ?
D (termfrequency), and idf(w) = log2df(w)|D| is the inversedocument frequency of w (Jones, 2004).A simple distributional measure of relatedness be-tween w and p can be obtained with the followingratio:R(w, p) =df(w ?
pd)df(w)(1)Pointwise mutual information (PMI) (Church andHanks, 1990) is another distributional measure thatcan be used for measuring the relatedness of w andp.
The probabilities needed for its calculation can beobtained by maximum likelihood estimation (MLE):P (w) ?
df(w)|D| , P (pd) ?df(pd)|D| and P (w ?
pd) ?df(w?pd)|D| .Thus, PMI is given by this expression:PMI(w, pd) = log2(P (w ?
pd)P (w) ?
P (pd))(2)F1:?w?d?
R(w, pd)F2:?w?d?
tf(w, d)F3:?w?d?
idf(w)F4:?w?d?
PMI(w, pd)F5:?w?d?
NPMI(w, pd)F6:?w?d?
(tf(w,d) ?
R(w, pd))F7:?w?d?
(idf(w) ?
R(w, pd))F8?w?d?
(R(w, pd) ?
PMI(w, pd))F9:?w?d?
(R(w, pd) ?NPMI(w, pd))F10:?w?d?
(tf(w, d) ?
idf(w))F11:?w?d?
(tf(w, pd) ?
PMI(w, pd))F12:?w?d?
(tf(w, pd) ?NPMI(w, pd))F13:?w?d?
(idf(w) ?
PMI(w, pd))F14:?w?d?
(idf(w) ?NPMI(w, pd))F15:?w?d?
(PMI(w, pd) ?NPMI(w, pd))F16:?w?d?
(tf(w, d) ?
idf(w) ?
R(w,pd))F17:?w?d?
(tf(w, d) ?
R(w, pd) ?
PMI(w, pd))F18:?w?d?
(tf(w, d) ?
R(w, pd) ?NPMI(w, pd))F19:?w?d?
(tf(w, d) ?
idf(w) ?
PMI(w,pd))F20:?w?d?
(tf(w, d) ?
idf(w) ?NPMI(w,pd))Table 1: Cohesiveness featuresFurthermore, the scores obtained through eq.
2can be normalized in the interval [+2,0] with the fol-lowing expression:NPMI(w, pd) =PMI(w, pd)?
log2(P (w ?
pd))+ 1 (3)A measure of the cohesiveness between a docu-ment d against its target phrase pd, can be obtainedby aggregating the pairwise relatedness scores be-tween all the words in d and pd.
For instance, us-ing eq.
1 that measure is?w?d?
R(w, pd), where d?is the set of different words in d. The equations 1,2 and 3 can be used as weights associated to eachword, which can also be combined among them andwith tf and idf weights.
Such weight combinationsproduce measures that can be used as cohesivenessfeatures for a document.
The set of 20 features ob-tained using this approach is shown in Table 1.2.2 Stylistic FeaturesThe set of stylistic features related to the documentlength, vocabulary size and relative position of theoccurrence of the target phrase in a document isshown in Table 2.115F21: Relative position of pd in dF22: Document length in charactersF23: Document length in tokensF24: Number of different wordsTable 2: Stylistic features2.3 Syntactic FeaturesThe features F25 to F67 correspond to the set of 43part-of-speech tags of the NLTK English POS tag-ger (Loper and Bird, 2002).
Each feature containsthe frequency of occurrence of each POS-tag in adocument d.3 Experimental Setup and ResultsThe data provided for this task consists of two datasets LexSample and AllWords, which are dividedinto development, training and test sets.
Neverthe-less, we considered a single training set aggregat-ing the development and training parts from bothdata sets for a total of 3,230 instances.
Each train-ing instance has a class label whether ?literally?
or?figuratively?
depending on the usage or the tar-get phrase.
Similarly, the aggregated test set con-tains 1,112 instances, but with unknown values inthe class attribute.Firstly, the syntactic features for each text wereobtained using the POS tagger included in the NLTKv.2.0.4 (Loper and Bird, 2002).
Secondly, all textswere preprocessed by tokenizing, lowecasing, stop-word removing, punctuation removing and stem-ming using the Porter?s algorithm (1980).
This pre-processed version of the texts was used to obtain theremaining cohesiveness and stylistic features.
Theresulting vectorial data set was used to produce thepredictions labeled ?UNAL.RUN1?
through a Lo-gistic classifier (Cessie and Houwelingen, 1992).The implementation used for this classifier was theincluded in WEKA v.3.6.9 (Hall et al 2009).
Theaccuracies obtained by the different feature groupsin the training set using 10-fold cross validation areshown in Table 3.
The last column shows the per-centage of relative improvement of different featuresets combinations from the most frequent class base-line to our best system using all features.The predictions labeled ?UNAL.RUN2?
were ob-tained with the same vectorial data set but addingFeatures Accuracy % improv.All features 0.7272 100.0%Cohesiveness+Syntactic 0.7034 87.1%Cohesiveness 0.6833 76.2%Syntactic 0.6229 43.5%Stylistic 0.5492 3.5%Baseline MFC 0.5427 0.0%Table 3: Results by group of features in the training setusing 10-fold cross validationSystem LexSample AllWords BothUNAL.RUN1 0.7222 0.6680 0.6970UNAL.RUN2 0.7542 0.6448 0.7032Baseline MFC 0.5034 0.6158 0.5558Best SemEval?13 0.7795 0.6680 0.7276# test instances 594 518 1,112Table 4: Official results in the test set (accuracy)as a nominal feature the target phrase of each in-stance.
The official results obtained by both sub-mitted runs are shown in Table 4.
Note that officialresults in the test set are reported separately for thedata sets LexSample and AllWords.
The LexSampletest set contains instances whose target phrases wereseen in the training set (i.e.
unseen contexts).
Un-like LexSample, AllWords contains instances whosetarget phrases were unseen in the training set (i.e.unseen phrases).4 DiscussionAs it was expected, the results obtained in the ?un-seen context?
setting were consistently better thanin ?unseen phrases?.
This result suggests that thediscrimination of literal and figurative usage heavilydepends on particular idiomatic phrases.
This canalso be confirmed by the best accuracy obtained byRUN2 compared with RUN1 in LexSample.
Clearly,the classifier used in RUN2 exploited the identifica-tion of the phrase to leverage a priori informationabout the phrase such as the most frequent usage.Another factor that could undermine the results inthe ?unseen phrases?
setting is the low number of in-stances per phrase in the AllWords test set, roughly athird in comparison with LexSample.
Given that theeffectiveness of the cohesiveness features depends116on the number of documents where the idiomaticphrase occurs, the predictions for this test set reliedmainly on the less effective features, namely syn-tactic and stylistic features (see Table 3).
However,this problem could be alleviated obtaining the distri-butional statistics from a large corpus with enoughoccurrences of the unseen phrases.Besides it is important to note, that in spite of thelow individual contribution of the stylistic featuresto the overall accuracy (3.5%), when these are com-bined with the remaining features they provide animprovement of 12.9% (see Table 3).5 ConclusionsWe participated in the Phrasal Semantics sub task 5bin SemEval 2013.
Our system proved the effective-ness of the use of cohesiveness, stylistic and syn-tactic features for discriminating literal from figura-tive usage of idiomatic phrases.
The most-frequent-class baseline was overcame by 49.8% in the ?un-seen contexts?
setting (LexSample) and 8.5% in ?un-seen phrases?
(AllWords).AcknowledgmentsThis research was funded in part by the Systemsand Industrial Engineering Department, the Officeof Student Welfare of the National University ofColombia, Bogot?, and through a grant from theColombian Department for Science, Technologyand Innovation, Colciencias, proj.
1101-521-28465with funding from ?El Patrimonio Aut?nomo FondoNacional de Financiamiento para la Ciencia, la Tec-nolog?a y la Innovaci?n, Francisco Jos?
de Caldas.
?The third author recognizes the support from Mexi-can Government (SNI, COFAA-IPN, SIP 20131702,CONACYT 50206-H) and CONACYT?DST India(proj.
122030 ?Answer Validation through TextualEntailment?
).ReferencesJulia Birke and Anoop Sarkar.
2006.
A clustering ap-proach for nearly unsupervised recognition of nonlit-eral language.
In Proceedings of the 11th Conferenceof the European Chapter of the Association for Com-putational Linguistics, Trento, Italy.S.
Le Cessie and J. C. Van Houwelingen.
1992.
Ridgeestimators in logistic regression.
Applied Statistics,41(1):191.Kenneth Ward Church and Patrick Hanks.
1990.
Wordassociation norms, mutual information, and lexicogra-phy.
Comput.
Linguist., 16(1):22?29, March.Afsaneh Fazly, Paul Cook, and Suzanne Stevenson.2009.
Unsupervised type and token identifica-tion of idiomatic expressions.
Comput.
Linguist.,35(1):61?103, March.Mark Hall, Frank Eibe, Geoffrey Holmes, and BernhardPfahringer.
2009.
The WEKA data mining software:An update.
SIGKDD Explorations, 11(1):10?18.Karen Sp?rck Jones.
2004.
A statistical interpretation ofterm specificity and its application in retrieval.
Jour-nal of Documentation, 60(5):493?502, October.Graham Katz and Eugenie Giesbrecht.
2006.
Automaticidentification of non-compositional multi-word ex-pressions using latent semantic analysis.
In Proceed-ings of the Workshop on Multiword Expressions: Iden-tifying and Exploiting Underlying Properties, MWE?06, pages 12?19, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Ioannis Korkontzelos, Torsten Zesch, Fabio MassimoZanzotto, and Chris Biemann.
2013.
SemEval-2013task 5: Evaluating phrasal semantics.
In Proceedingsof the 7th International Workshop on Semantic Evalu-ation (SemEval 2013).Dekang Lin.
1999.
Automatic identification of non-compositional phrases.
In Proceedings of the 37thannual meeting of the Association for ComputationalLinguistics on Computational Linguistics, ACL ?99,page 317?324, Stroudsburg, PA, USA.
Association forComputational Linguistics.Edward Loper and Steven Bird.
2002.
NLTK: the natu-ral language toolkit.
In Proceedings of the ACL Work-shop on Effective Tools and Methodologies for Teach-ing Natural Language Processing and ComputationalLinguistics.
Philadelphia.
Association for Computa-tional Linguistics.Martin Porter.
1980.
An algorithm for suffix stripping.Program, 3(14):130?137, October.Caroline Sporleder and Linlin Li.
2009.
Unsupervisedrecognition of literal and non-literal use of idiomaticexpressions.
In Proceedings of the 12th Conference ofthe European Chapter of the Association for Computa-tional Linguistics, EACL ?09, page 754?762, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.117
