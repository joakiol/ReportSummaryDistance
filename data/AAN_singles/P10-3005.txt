Proceedings of the ACL 2010 Student Research Workshop, pages 25?30,Uppsala, Sweden, 13 July 2010. c?2010 Association for Computational LinguisticsSentiment Translation through Lexicon InductionChristian ScheibleInstitute for Natural Language ProcessingUniversity of Stuttgartscheibcn@ims.uni-stuttgart.deAbstractThe translation of sentiment informationis a task from which sentiment analy-sis systems can benefit.
We present anovel, graph-based approach using Sim-Rank, a well-established vertex similar-ity algorithm to transfer sentiment infor-mation between a source language and atarget language graph.
We evaluate thismethod in comparison with SO-PMI.1 IntroductionSentiment analysis is an important topic in compu-tational linguistics that is of theoretical interest butalso implies many real-world applications.
Usu-ally, two aspects are of importance in sentimentanalysis.
The first is the detection of subjectivity,i.e.
whether a text or an expression is meant to ex-press sentiment at all; the second is the determina-tion of sentiment orientation, i.e.
what sentimentis to be expressed in a structure that is consideredsubjective.Work on sentiment analysis most often cov-ers resources or analysis methods in a single lan-guage, usually English.
However, the transferof sentiment analysis between languages can beadvantageous by making use of resources for asource language to improve the analysis of the tar-get language.This paper presents an approach to the transferof sentiment information between languages.
It isbuilt around an algorithm that has been success-fully applied for the acquisition of bilingual lexi-cons.
One of the main benefits of the method is itsability of handling sparse data well.Our experiments are carried out using Englishas a source language and German as a target lan-guage.2 Related WorkThe translation of sentiment information has beenthe topic of multiple publications.Mihalcea et al (2007) propose two methods fortranslating sentiment lexicons.
The first methodsimply uses bilingual dictionaries to translate anEnglish sentiment lexicon.
A sentence-based clas-sifier built with this list achieved high precisionbut low recall on a small Romanian test set.
Thesecond method is based on parallel corpora.
Thesource language in the corpus is annotated withsentiment information, and the information is thenprojected to the target language.
Problems arisedue to mistranslations, e.g., because irony is notrecognized.Banea et al (2008) use machine translation formultilingual sentiment analysis.
Given a corpusannotated with sentiment information in one lan-guage, machine translation is used to produce anannotated corpus in the target language, by pre-serving the annotations.
The original annotationscan be produced either manually or automatically.Wan (2009) constructs a multilingual classifierusing co-training.
In co-training, one classifierproduces additional training data for a second clas-sifier.
In this case, an English classifier assists intraining a Chinese classifier.The induction of a sentiment lexicon is the sub-ject of early work by (Hatzivassiloglou and McK-eown, 1997).
They construct graphs from coor-dination data from large corpora based on the in-tuition that adjectives with the same sentiment ori-entation are likely to be coordinated.
For example,fresh and delicious is more likely than rotten anddelicious.
They then apply a graph clustering al-gorithm to find groups of adjectives with the sameorientation.
Finally, they assign the same label toall adjectives that belong to the same cluster.
Theauthors note that some words cannot be assigned aunique label since their sentiment depends on con-25text.Turney (2002) suggests a corpus-based extrac-tion method based on his pointwise mutual infor-mation (PMI) synonymy measure He assumes thatthe sentiment orientation of a phrase can be deter-mined by comparing its pointwise mutual infor-mation with a positive (excellent) and a negativephrase (poor).
An introduction to SO-PMI is givenin Section 5.13 Bilingual Lexicon InductionTypical approaches to the induction of bilinguallexicons involve gathering new information froma small set of known identities between the lan-guages which is called a seed lexicon and incor-porating intralingual sources of information (e.g.cooccurrence counts).
Two examples of suchmethods are a graph-based approach by Dorow etal.
(2009) and a vector-space based approach byRapp (1999).
In this paper, we will employ thegraph-based method.SimRank was first introduced by Jeh andWidom (2002).
It is an iterative algorithm thatmeasures the similarity between all vertices in agraph.
In SimRank, two nodes are similar if theirneighbors are similar.
This defines a recursive pro-cess that ends when the two nodes compared areidentical.
As proposed by Dorow et al (2009), wewill apply it to a graph G in which vertices repre-sent words and edges represent relations betweenwords.
SimRank will then yield similarity valuesbetween vertices that indicate the degree of relat-edness between them with regard to the propertyencoded through the edges.
For two nodes i andj in G, similarity according to SimRank is definedassim(i, j) = c|N(i)||N(j)?k?N(i),l?N(j)sim(k, l),where N(x) is the neighborhood of x and c isa weight factor that determines the influence ofneighbors that are farther away.
The initial con-dition for the recursion is sim(i, i) = 1.Dorow et al (2009) further propose the applica-tion of the SimRank algorithm for the calculationof similarities between a source graph S and a tar-get graph T .
Initially, some relations between thetwo graphs need to be known.
When operating onword graphs, these can be taken from a bilinguallexicon.
This provides us with a framework forthe induction of a bilingual lexicon which can beconstructed based on the obtained similarity val-ues between the vertices of the two graphs.One problem of SimRank observed in experi-ments by Laws et al (2010) was that while wordswith high similarity were semantically related,they often were not exact translations of eachother but instead often fell into the categories ofhyponymy, hypernomy, holonymy, or meronymy.However, this makes the similarity values appli-cable for the translation of sentiment since it is aproperty that does not depend on exact synonymy.4 Sentiment TransferAlthough unsupervised methods for the design ofsentiment analysis systems exist, any approachcan benefit from using resources that have beenestablished in other languages.
The main problemthat we aim to deal with in this paper is the trans-fer of such information between languages.
TheSimRank lexicon induction method is suitable forthis purpose since it can produce useful similarityvalues even with a small seed lexicon.First, we build a graph for each language.
Thevertices of these graphs will represent adjectiveswhile the edges are coordination relations betweenthese adjectives.
An example for such a graph isgiven in Figure 1.Figure 1: Sample graph showing English coordi-nation relations.The use of coordination information has beenshown to be beneficial for example in early workby Hatzivassiloglou and McKeown (1997).Seed links between those graphs will be takenfrom a universal dictionary.
Figure 2 shows an ex-ample graph.
Here, intralingual coordination rela-tions are represented as black lines, seed relationsas solid grey lines, and relations that are inducedthrough SimRank as dashed grey lines.After computing similarities in this graph, we26Figure 2: Sample graph showing English and German coordination relations.
Solid black lines representcoordinations, solid grey lines represent seed relations, and dashed grey lines show induced relations.need to obtain sentiment values.
We will definethe sentiment score (sent) assent(nt) =?ns?Ssimnorm(ns, nt) sent(ns),where ntis a node in the target graph T , and Sthe source graph.
This way, the sentiment scoreof each node is an average over all nodes in Sweighted by their normalized similarity, simnorm.We define the normalized similarity assimnorm(ns, nt) =sim(ns, nt)?ns?Ssim(ns, nt).Normalization guarantees that all sentimentscores lie within a specified range.
Scores are nota direct indicator for orientation since the similar-ities still include a lot of noise.
Therefore, weinterpret the scores by assigning each word to acategory by finding score thresholds between thecategories.5 Experiments5.1 Baseline Method (SO-PMI)We will compare our method to the well-established SO-PMI algorithm by Turney (2002)to show an improvement over an unsupervisedmethod.
The algorithm works with cooccurrencecounts on large corpora.
To determine the seman-tic orientation of a word w, the hits near positive(Pwords) and negative (Nwords) seed words isused.
The SO-PMI equation is given asSO-PMI(word) =log2(?pword?Pwordshits(word NEAR pword)?nword?Nwordshits(word NEAR nword)?
?nword?Nwordshits(nword)?pword?Pwordshits(pword))5.2 Data AcquisitionWe used the English and German Wikipediabranches as our corpora.
We extracted coor-dinations from the corpus using a simple CQPpattern search (Christ et al, 1999).
For our ex-periments, we looked only at coordinations withand.
For the English corpus, we used the pattern[pos = "JJ"] ([pos = ","] [pos ="JJ"])*([pos = ","]?
"and" [pos= "JJ"])+, and for the German corpus, thepattern [pos = "ADJ.
*"] ([pos = ","][pos = "ADJ.
*"])* ("und" [pos ="ADJ"])+ was used.
This yielded 477,291 pairsof coordinated English adjectives and 44,245German pairs.
We used the dict.cc dictionary1 asa seed dictionary.
It contained a total of 30,551adjectives.After building a graph out of this data as de-scribed in Section 4, we apply the SimRank algo-rithm using 7 iterations.Data for the SO-PMI method had to be col-lected from queries to search engines since the in-formation available in the Wikipedia corpus wastoo sparse.
Since Google does not provide a sta-ble NEAR operator, we used coordinations instead.For each of the test words w and the SO-PMI seedwords s we made two queries +"w und s" and+"s und w" to Google.
The quotes and + wereadded to ensure that no spelling correction or syn-onym replacements took place.
Since the originalexperiments were designed for an English corpus,a set of German seed words had to be constructed.We chose gut, nett, richtig, scho?n, ordentlich, an-genehm, aufrichtig, gewissenhaft, and hervorra-gend as positive seeds, and schlecht, teuer, falsch,bo?se, feindlich, verhasst, widerlich, fehlerhaft, and1http://www.dict.cc/27word valuestrongpos 1.0weakpos 0.5neutral 0.0weakneg ?0.5strongneg ?1.0Table 1: Assigned values for positivity labelsmangelhaft as negative seeds.We constructed a test set by randomly selecting200 German adjectives that occurred in a coordi-nation in Wikipedia.
We then eliminated adjec-tives that we deemed uncommon or too difficult tounderstand or that were mislabeled as adjectives.This resulted in a 150 word test set.
To deter-mine the sentiment of these adjectives, we asked9 human judges, all native German speakers, toannotate them given the classes neutral, slightlynegative, very negative, slightly positive, and verypositive, reflecting the categories from the train-ing data.
In the annotation process, another 7 ad-jectives had to be discarded because one or moreannotators marked them as unknown.Since human judges tend to interpret scalesdifferently, we examine their agreement usingKendall?s coefficient of concordance (W ) includ-ing correction for ties (Legendre, 2005) whichtakes ranks into account.
The agreement was cal-culated as W = 0.674 with a significant confi-dence (p < .001), which is usually interpreted assubstantial agreement.
Manual examination of thedata showed that most disagreement between theannotators occurred with adjectives that are tiedto political implications, for example nuklear (nu-clear).5.3 Sentiment Lexicon InductionFor our experiments, we used the polarity lexi-con of Wilson et al (2005).
It includes annota-tions of positivity in the form of the categoriesneutral, weakly positive (weakpos), strongly posi-tive (strongpos), weakly negative (weakneg), andstrongly positive (strongneg).
In order to con-duct arithmetic operations on these annotations,mapped them to values from the interval [?1, 1]by using the assignments given in Table 1.5.4 ResultsTo compare the two methods to the human raters,we first reproduce the evaluation by Turney (2002)and examine the correlation coefficients.
Bothmethods will be compared to an average over thehuman rater values.
These values are calculatedon values asserted based on Table 1.
The corre-lation coefficients between the automatic systemsand the human ratings, SO-PMI yields r = 0.551,and SimRank yields r = 0.587 which are not sig-nificantly different.
This shows that SO and SRhave about the same performance on this broadmeasure.Since many adjectives do not express sentimentat all, the correct categorization of neutral adjec-tives is as important as the scalar rating.
Thus,we divide the adjectives into three categories ?positive, neutral, and negative.
Due to disagree-ments between the human judges there exists noclear threshold between these categories.
In orderto try different thresholds, we assume that senti-ment is symmetrically distributed with mean 0 onthe human scores.
For x ?
{ i20|0 ?
i ?
19}, wethen assign word w with human rating score(w)to negative if score(w) ?
?x, to neutral if ?x <score(w) < x and to positive otherwise.
Thisgives us a three-category gold standard for eachx that is then the basis for computing evaluationmeasures.
Each category contains a certain per-centile of the list of adjectives.
By mapping thesepercentiles to the rank-ordered scores for SO-PMIand SimRank, we can create three-category par-titions for them.
For example if for x = 0.3521% of the adjectives are negative, then the 21%of adjectives with the lowest SO-PMI scores aredeemed to have been rated negative by SO-PMI.00.20.40.60.810.950.90.850.80.750.70.650.60.550.50.450.40.350.30.250.20.150.10.050AccuracyxSO-PMI (macro)SimRank (macro)SO-PMI (micro)SimRank (micro)Figure 3: Macro- and micro-averaged AccuracyFirst, we will look at the macro- and micro-averaged accuracies for both methods (cf.
Fig-ure 3).
Overall, SimRank performs better for x28between 0.05 and 0.4 which is a plausible inter-val for the neutral threshold on the human ratings.The results diverge for very low and high valuesof x, however these values can be considered un-realistic since they implicate neutral areas that aretoo small or too large.
When comparing the ac-curacies for each of the classes (cf.
Figure 4), weobserve that in the aforementioned interval, Sim-Rank has higher accuracy values than SO-PMI forall of them.00.20.40.60.810.950.90.850.80.750.70.650.60.550.50.450.40.350.30.250.20.150.10.050Accuracyxpositive (SO-PMI)positive (SimRank)neutral (SO-PMI)neutral (SimRank)negative (SO-PMI)negative (SimRank)Figure 4: Accuracy for individual classesTable 2 lists some interesting example words in-cluding their human ratings and SO-PMI and Sim-Rank scores which illustrate advantages and pos-sible shortcomings of the two methods.
The medi-ans of SO-PMI and SimRank scores are ?15.58and ?0.05, respectively.
The mean values are?9.57 for SO-PMI and 0.08 for SimRank, thestandard deviations are 13.75 and 0.22.
SimRankvalues range between ?0.67 and 0.41, SO-PMIranges between ?46.21 and 46.59.
We will as-sume that the medians mark the center of the setof neutral adjectives.Ausdrucksvoll receives a positive score fromSO-PMI which matches the human rating, how-ever not from SimRank, which assigns a scoreclose to 0 and would likely be considered neutral.This error can be explained by examining the sim-ilarity distribution for ausdrucksvoll which revealsthat there are no nodes that are similar to this node,which was most likely caused by its low degree.Auferstanden (resurrected) is perceived as a posi-tive adjective by the human judges, however it ismisclassified by SimRank as negative due to itsoccurrence with words like gestorben (deceased)and gekreuzigt (crucified) which have negative as-word (translation) SR SO judgesausdrucksvoll (expressive) 0.069 22.93 0.39grafisch (graphic) -0.050 -4.75 0.00kriminell (criminal) -0.389 -15.98 -0.94auferstanden (resurrected) -0.338 -10.97 0.34Table 2: Example adjectives including translation,and their scoressociations.
This suggests that coordinations aresometimes misleading and should not be used asthe only data source.
Grafisch (graphics-related)is an example for a neutral word misclassified bySO-PMI due to its occurrence in positive contextson the web.
Since SimRank is not restricted to re-lations between an adjective and a seed word, alladjective-adjective coordinations are used for theestimation of a sentiment score.
Kriminell is alsomisclassified by SO-PMI for the same reason.6 Conclusion and OutlookWe presented a novel approach to the translationof sentiment information that outperforms SO-PMI, an established method.
In particular, wecould show that SimRank outperforms SO-PMIfor values of the threshold x in an interval thatmost likely leads to the correct separation of pos-itive, neutral, and negative adjectives.
We intendto compare our system to other available work inthe future.
In addition to our findings, we createdan initial gold standard set of sentiment-annotatedGerman adjectives that will be publicly available.The two methods are very different in nature;while SO-PMI is suitable for languages in whichvery large corpora exist, this might not be thecase for knowledge-sparse languages.
For someGerman words (e.g.
schwerstkrank (seriouslyill)), SO-PMI lacked sufficient results on the webwhereas SimRank correctly assigned negative sen-timent.
SimRank can leverage knowledge fromneighbor words to circumvent this problem.
Inturn, this information can turn out to be mislead-ing (cf.
auferstanden).
An advantage of ourmethod is that it uses existing resources from an-other language and can thus be applied withoutmuch knowledge about the target language.
Ourfuture work will include a further examination ofthe merits of its application for knowledge-sparselanguages.The underlying graph structure provides a foun-dation for many conceivable extensions.
In thispaper, we presented a fairly simple experiment re-stricted to adjectives only.
However, the method29is suitable to include arbitrary parts of speech aswell as phrases, as used by Turney (2002).
An-other conceivable application would be the directcombination of the SimRank-based model with astatistical model.Currently, our input sentiment list exists only ofprior sentiment values, however work by Wilsonet al (2009) has advanced the notion of contextualpolarity lists.
The automatic translation of this in-formation could be beneficial for sentiment analy-sis in other languages.Another important problem in sentiment anal-ysis is the treatment of ambiguity.
The senti-ment expressed by a word or phrase is context-dependent and is for example related to word sense(Akkaya et al, 2009).
Based on regularities ingraph structure and similarity, ambiguity resolu-tion might become possible.ReferencesC.
Akkaya, J. Wiebe, and R. Mihalcea.
2009.
Sub-jectivity Word Sense Disambiguation.
In Proceed-ings of the 2009 Conference on Empirical Methodsin Natural Language Processing, pages 190?199.Carmen Banea, Rada Mihalcea, Janyce Wiebe, andSamer Hassan.
2008.
Multilingual subjectivityanalysis using machine translation.
In Proceedingsof the 2008 Conference on Empirical Methods inNatural Language Processing, pages 127?135, Hon-olulu, Hawaii, October.
Association for Computa-tional Linguistics.O.
Christ, B.M.
Schulze, A. Hofmann, and E. Koenig.1999.
The IMS Corpus Workbench: Corpus QueryProcessor (CQP): User?s Manual.
University ofStuttgart, March, 8:1999.Beate Dorow, Florian Laws, Lukas Michelbacher,Christian Scheible, and Jason Utt.
2009.
A graph-theoretic algorithm for automatic extension of trans-lation lexicons.
In Proceedings of the Workshop onGeometrical Models of Natural Language Seman-tics, pages 91?95, Athens, Greece, March.
Associ-ation for Computational Linguistics.Vasileios Hatzivassiloglou and Kathleen R. McKeown.1997.
Predicting the semantic orientation of adjec-tives.
In Proceedings of the 35th Annual Meetingof the Association for Computational Linguistics,pages 174?181, Madrid, Spain, July.
Association forComputational Linguistics.Glen Jeh and Jennifer Widom.
2002.
Simrank: a mea-sure of structural-context similarity.
In KDD ?02:Proceedings of the eighth ACM SIGKDD interna-tional conference on Knowledge discovery and datamining, pages 538?543, New York, NY, USA.
ACM.F.
Laws, L. Michelbacher, B. Dorow, U. Heid, andH.
Schu?tze.
2010.
Building a Cross-lingual Re-latedness Thesaurus Using a Graph Similarity Mea-sure.
Submitted on Nov 7, 2009, to the InternationalConference on Language Resources and Evaluation(LREC).P.
Legendre.
2005.
Species associations: the Kendallcoefficient of concordance revisited.
Journal ofAgricultural Biological and Environment Statistics,10(2):226?245.Rada Mihalcea, Carmen Banea, and Janyce Wiebe.2007.
Learning multilingual subjective language viacross-lingual projections.
In Proceedings of the 45thAnnual Meeting of the Association of ComputationalLinguistics, pages 976?983, Prague, Czech Repub-lic, June.
Association for Computational Linguis-tics.Reinhard Rapp.
1999.
Automatic identification ofword translations from unrelated english and germancorpora.
In Proceedings of the 37th Annual Meet-ing of the Association for Computational Linguis-tics, pages 519?526, College Park, Maryland, USA,June.
Association for Computational Linguistics.Peter Turney.
2002.
Thumbs up or thumbs down?
se-mantic orientation applied to unsupervised classifi-cation of reviews.
In Proceedings of 40th AnnualMeeting of the Association for Computational Lin-guistics, pages 417?424, Philadelphia, Pennsylva-nia, USA, July.
Association for Computational Lin-guistics.Xiaojun Wan.
2009.
Co-training for cross-lingual sen-timent classification.
In Proceedings of the JointConference of the 47th Annual Meeting of the ACLand the 4th International Joint Conference on Natu-ral Language Processing of the AFNLP, pages 235?243, Suntec, Singapore, August.
Association forComputational Linguistics.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-level sentiment analysis.
In Proceedings of Hu-man Language Technology Conference and Confer-ence on Empirical Methods in Natural LanguageProcessing, pages 347?354, Vancouver, BritishColumbia, Canada, October.
Association for Com-putational Linguistics.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2009.
Recognizing Contextual Polarity: an Explo-ration of Features for Phrase-level Sentiment Analy-sis.
Computational Linguistics, 35(3):399?433.30
