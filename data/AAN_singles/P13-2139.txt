Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 797?803,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsRecognizing Identical Events with Graph KernelsGoran Glavas?
and Jan S?najderUniversity of ZagrebFaculty of Electrical Engineering and ComputingText Analysis and Knowledge Engineering LabUnska 3, 10000 Zagreb, Croatia{goran.glavas,jan.snajder}@fer.hrAbstractIdentifying news stories that discuss thesame real-world events is important fornews tracking and retrieval.
Most exist-ing approaches rely on the traditional vec-tor space model.
We propose an approachfor recognizing identical real-world eventsbased on a structured, event-oriented doc-ument representation.
We structure docu-ments as graphs of event mentions and usegraph kernels to measure the similarity be-tween document pairs.
Our experimentsindicate that the proposed graph-based ap-proach can outperform the traditional vec-tor space model, and is especially suitablefor distinguishing between topically simi-lar, yet non-identical events.1 IntroductionNews stories typically describe real-world events.Topic detection and tracking (TDT) aims to de-tect stories that discuss identical or directly relatedevents, and track these stories as they evolve overtime (Allan, 2002).
Being able to identify the sto-ries that describe the same real-world event is es-sential for TDT, and event-based information re-trieval in general.In TDT, an event is defined as something hap-pening in a certain place at a certain time (Yanget al 1999), while a topic is defined as a set ofnews stories related by some seminal real-worldevent (Allan, 2002).
To identify news stories onthe same topic, most TDT approaches rely on tra-ditional vector space models (Salton et al 1975),as more sophisticated natural language processingtechniques have not yet proven to be useful forthis task.
On the other hand, significant advancesin sentence-level event extraction have been madeover the last decade, in particular as the result ofstandardization efforts such as TimeML (Puste-jovsky et al 2003a) and TimeBank (Pustejovskyet al 2003b), as well as dedicated evaluation tasks(ACE, 2005; Verhagen et al 2007; Verhagen etal., 2010).
However, these two lines of researchhave largely remained isolated from one another.In this paper we bridge this gap and addressthe task of recognizing stories discussing identicalevents by considering structured representationsfrom sentence-level events.
More concretely, westructure news stories into event graphs built fromindividual event mentions extracted from text.
Tomeasure event-based similarity of news stories, wecompare their event graphs using graph kernels(Borgwardt, 2007).
We conduct preliminary ex-periments on two event-oriented tasks and showthat the proposed approach can outperform tradi-tional vector space model in recognizing identicalreal-world events.
Moreover, we demonstrate thatour approach is especially suitable for distinguish-ing between topically similar, yet non-identicalreal-world events.2 Related WorkThe traditional vector space model (VSM) (Saltonet al 1975) computes the cosine between bag-of-words representations of documents.
The VSM isat the core of most approaches that identify same-topic news stories (Hatzivassiloglou et al 2000;Brants et al 2003; Kumaran and Allan, 2005;Atkinson and Van der Goot, 2009).
However, ithas been observed that some word classes (e.g.,named entities, noun phrases, collocations) havemore significance than the others.
Among them,named entities have been considered as particu-larly important, as they often identify the partici-pants of an event.
In view of this, Hatzivassiloglouet al(2000) restrict the set of words to be usedfor document representation to words constitutingnoun phrases and named entities.
Makkonen et797al.
(2004) divide document terms into four seman-tic categories (locations, temporal expressions,proper names, and general terms) and constructseparate vector for each of them.
Kumaran andAllan (2004) represent news stories with three dif-ferent vectors, modeling all words, named-entitywords, and all non-named-entity words occurringin documents.
When available, recognition ofidentical events can rely on meta-information as-sociated with news stories, such as document cre-ation time (DCT).
Atkinson and Van der Goot(2009) combine DCT with VSM, assuming thattemporally distant news stories are unlikely to de-scribe the same event.In research on event extraction, the task of rec-ognizing identical events is known as event coref-erence resolution (Bejan and Harabagiu, 2010;Lee et al 2012).
There, however, the aim is toidentify sentence-level event mentions referring tothe same real-world events, and not stories thatdiscuss identical events.3 Kernels on Event GraphsTo identify the news describing the same real-world event, we (1) structure event-oriented in-formation from text into event graphs and (2) usegraph kernels to measure the similarity between apair of event graphs.3.1 Event graphsAn event graph is a vertex- and edge-labeledmixed graph in which vertices represent individ-ual event mentions and edges represent temporalrelations between event mentions.
We adopt ageneric representation of event mentions, as pro-posed by Glavas?
and S?najder (2013): each men-tion consists of an anchor (a word that conveysthe core meaning) and four types of arguments(agent, target, time, location).
Furthermore, weconsider four types of temporal relations betweenevent mentions: before, after, overlap, and equal(Allen, 1983).
As relations overlap and equal aresymmetric, whereas before and after are not, anevent graph may contain both directed and undi-rected edges.Formally, an event graph G is represented as atuple G = (V,E,A,m, r), where V is the set ofvertices, E is the set of undirected edges, A is theset of directed edges (arcs), m : V ?
M is abijection mapping the vertices to event mentions,and r : E ?
R is the edge-labeling function, as-signing temporal relations to edges (cf.
Fig.
1).The construction of an event graph from a newsstory involves the extraction of event mentions(anchors and arguments) and the extraction oftemporal relations between mentions.
We use asupervised model (with 80% F1 extraction perfor-mance) based on a rich set of features similar tothose proposed by Bethard (2008) to extract eventanchors.
We then employ a robust, rule-based ap-proach proposed by Glavas?
and S?najder (2013) toextract generic event arguments.
Finally, we em-ploy a supervised model (60% micro-averaged F1classification performance) with a rich set of fea-tures, similar to those proposed by Bethard (2008),to extract temporal relations between event men-tions.
A detailed description of the graph con-struction steps is outside the scope of this paper.To compute event graph kernels (cf.
Section3.2), we need to determine whether two eventmentions co-refer.
To resolve cross-documentevent coreference, we use the model proposedby Glavas?
and S?najder (2013).
The model de-termines coreference by comparing factual eventanchors and arguments of four coarse-grained se-mantic types (agent, target, location, and time),and achieves an F-score of 67% (79% precisionand 57% recall) on the cross-document mentionpairs from the EventCorefBank dataset (Bejan andHarabagiu, 2008).
In what follows, cf (m1,m2)denotes whether event mentions m1 and m2 co-refer (equals 1 if mentions co-refer, 0 otherwise).3.2 Graph kernelsGraph kernels are fast polynomial alternativesto traditional graph comparison techniques (e.g.,subgraph isomorphism), which provide an expres-sive measure of similarity between graphs (Borg-wardt, 2007).
We employ two different graph ker-nels: product graph kernel and weighted decom-position kernel.
We chose these kernels becausetheir general forms have intuitive interpretationsfor event matching.
These particular kernels haveshown to perform well on a number of tasks fromchemoinformatics (Mahe?
et al 2005; Menchettiet al 2005).Product graph kernel.
A product graph kernel(PGK) counts the common walks between two in-put graphs (Ga?rtner et al 2003).
The graph prod-uct of two labeled graphs, G and G?
, denotedGP = G?G?, is a graph with the vertex setVP ={(v, v?)
| v ?
VG, v?
?
VG?
, ?
(v, v?
)}798where ?
(v, v?)
is a predicate that holds whenvertices v and v?
are identically labeled (Ham-mack et al 2011).
Given event graphs G =(V,E,A,m, r) and G?
= (V ?, E?, A?,m?, r?
), weconsider the vertices to be identically labeled ifthe corresponding event mentions co-refer, i.e.,?
(v, v?)
.= cf (m(v),m?(v?)).
The edge set of thegraph product depends on the type of the product.We experiment with two different products: ten-sor product and conormal product.
In the tensorproduct, an edge is introduced iff the correspond-ing edges exist in both input graphs and the labelsof those edges match (i.e., both edges represent thesame temporal relation).
In the conormal product,an edge is introduced iff the corresponding edgeexists in at least one input graph.
Thus, a conor-mal product may compensate for omitted temporalrelations in the input graphs.Let AP be the adjacency matrix of the graphproductGP built from input graphsG andG?.
Theproduct graph kernel that counts common walks inG and G?
can be computed efficiently as:KPG(G,G?)
=|VP |?i,j=1[(I ?
?AP )?1]ij (1)when ?
< 1/t , where t is the maximum degree ofa vertex in the graph product GP .
In our experi-ments, we set ?
to 1/(t+ 1) .Weighted decomposition kernel.
A weighteddecomposition kernel (WDK) compares smallgraph parts, called selectors, being matched ac-cording to an equality predicate.
The importanceof the match is weighted by the similarity of thecontexts in which the matched selectors occur.For a description of a general form of WDK, seeMenchetti et al(2005).Let S(G) be the set of all pairs (s, z), where s isthe selector (subgraph of interest) and z is the con-text of s. We decompose event graphs into individ-ual vertices, i.e., we define selectors to be the indi-vidual vertices.
In this case, similarly as above, theequality predicate ?
(v, v?)
for two vertices v ?
Gand v?
?
G?
holds if and only if the correspond-ing event mentions m(v) and m?(v?)
co-refer.
Us-ing selectors that consist of more than one vertexwould require a more complex and perhaps a lessintuitive definition of the equality predicate ?.
Theselector context Zv of vertex v is a subgraph of Gthat contains v and all its immediate neighbors.
Inother words, we consider as context all event men-tions that are in a direct temporal relation with theselected mention.
WDK between event graphs Gand G?
is computed as:KWD(G,G?)
=?v?VG,v?
?VG?cf (m(v),m?(v?))
?
(Zv, Z ?v?
)(2)where ?
(Zv, Z ?v?)
is the context kernel measuringthe similarity between the context Zv of selectorv ?
G and the context Z ?v?
of selector v?
?
G?.We compute the context kernel ?
as the number ofcoreferent mention pairs found between the con-texts, normalized by the context size:?
(Zv, Z ?v?)
=?w?VZv ,w??VZ?v?cf(m(w),m?(w?
))max(|VZv |, |VZ?v?
|)The intuition behind this is that a pair of corefer-ent mentions m(v) and m?(v?)
should contributeto the overall event similarity according to thenumber of pairs of coreferent mentions,m(w) andm?(w?
), that are in temporal relation with v and v?,respectively.Graph kernels example.
As an example, con-sider the following two story snippets describingthe same sets of real-world events:Story 1: A Cezanne masterpiece worth at least $131million that was the yanked from the wall of a Zurichart gallery in 2008 has been recovered, Serbian po-lice said today.
Four arrests were made overnightin connection with the theft, which was one of thebiggest art heists in recent history.Story 2: Serbian police have recovered a paintingby French impressionist Paul Cezanne worth an esti-mated 100 million euros (131.7 million U.S. dollars),media reported on Thursday.
The painting ?A boy ina red vest?
was stolen in 2008 from a Zurich museumby masked perpetrators.
Four members of an interna-tional crime ring were arrested Wednesday.The corresponding event graphs G and G?
areshown in Fig.
1a and 1b, respectively, while theirproduct is shown in Fig.
1c.
There are three pairsof coreferent event mentions between G and G?
:(yanked, stolen), (recovered, recovered), and (ar-rests, arrested).
Accordingly, the product graphP has three nodes.
The dashed edge between ver-tices (yanked, stolen) and (arrests, arrested) existsonly in the conormal product graph.
By substi-tuting into (1) the adjacency matrix and maximumvertex degree of tensor product graph P , we obtain799(a) Event graph G (Story 1) (b) Event graph G?
(Story 2) (c) Product graph PFigure 1: Example event graphs and their productthe tensor PGK score as:KPG =3?i,j=1??
(I ?
13(0 0 10 0 11 1 0))?1??i,j?
5.6Similarly, for the conormal product graph P weobtain the conormal PGK score of KPG = 9.
BysubstitutingG andG?
into (2), we obtain the WDKscore as:KWD =?(v,v?)?VP?
(Zv, Z ?v?)
=23 +34 +24 ?
1.9where VP contains pairs of coreferent event men-tions: (yanked, stolen), (recovered, recovered),and (arrests, arrested).4 ExperimentsWe conducted two preliminary experiments to in-vestigate whether kernels on event graphs can beused to recognize identical events.4.1 Task 1: Recognizing identical eventsDataset.
In the first experiment, we classifypairs of news stories as either describing identicalreal-world events or not.
For this we need a collec-tion of stories in which pairs of stories on identi-cal events have been annotated as such.
TDT cor-pora (Wayne, 2000) is not directly usable becauseit has no such annotations.
We therefore decidedto build a small annotated dataset.1 To this end,we use the news clusters of the EMM NewsBriefservice (Steinberger et al 2009).
EMM clustersnews stories from different sources using a docu-ment similarity score.
We acquired 10 randomlychosen news clusters, manually inspected each ofthem, and retained in each cluster only the doc-uments that describe the same real-world events.Additionally, we ensured that no documents from1Datasets for both experiments are available at:http://takelab.fer.hr/evkernelsModel P R FTensor PGK 89.7 82.3 85.8Conormal PGK 89.3 77.8 83.2WDK 88.6 73.7 80.5SVM Graph 91.1 87.6 89.3SVM Graph + VSM 93.8 96.2 95.0VSM baseline 90.9 82.9 86.7Table 1: Results for recognition of identical eventsdifferent clusters discuss the same event.
To ob-tain the gold standard dataset, we build all pairsof documents.
The final dataset consists of 64documents in 10 clusters, with 195 news pairsfrom the same clusters (positive pairs) and 1821news pairs from different clusters (negative pairs).We divide the dataset into a train and a test set(7:3 split ratio).
Note that, although our datasethas ground-truth annotations, it is incomplete inthe sense that some pairs of documents describ-ing the same events, which were not recognizedas such by the EMM, are not included.
Further-more, because EMM similarity score uses VSMcosine similarity as one of the features, VSM co-sine similarity constitutes a competitive baselineon this dataset.Results.
For each graph kernel and the VSMbaseline, we determine the optimal threshold onthe train set and evaluate the classification per-formance on the test set.
The results are givenin Table 1.
The precision is consistently higherthan recall for all kernels and the baseline.
Highprecision is expected, as clusters represent topi-cally dissimilar events.
PGK models (both ten-sor and conormal) outperform the WDK model,indicating that common walks correlate better toevent-based document similarity than commonsubgraphs.
Individually, none of the graph kernelsoutperforms the baseline.
To investigate whetherthe two kernels complement each other, we fed the800Original?Taliban militants have attacked a prison in north-westPakistan, freeing at least 380 prisoners.
.
.
?Event-preserving paraphrase?Taliban militants in northwest Pakistan attacked theprison, liberated at least 380 prisoners .
.
.
?Event-shifting paraphrase?Taliban militants have been arrested in north-west Pak-istan.
At least 380 militants have been arrested.
.
.
?Table 2: Event paraphrasing exampleindividual kernel scores to an SVM model (withRBF kernel), along with additional graph-basedfeatures such as the number of nodes and the num-ber of edges (SVM graph model).
Finally, we com-bined the graph-based features with the VSM co-sine similarity (SVM graph + VSM model).
SVMgraph model significantly (at p < 0.05, student?s2-tailed t-test) outperforms the individual kernelmodels and the baseline.
The combined model(SVM graph + VSM) significantly (at p < 0.01)outperforms the baseline and all kernel models.4.2 Task 2: Event-based similarity rankingDataset.
In the second experiment we focuson the task of distinguishing between news sto-ries that describe topically very similar, yet dis-tinct events.
For this purpose, we use a smallset of event paraphrases, constructed as fol-lows.
We manually selected 10 news stories fromEMM NewsBrief and altered each of them toobtain two meaning-preserving (event-preserving)and two meaning-changing (event-shifting) para-phrases.
To obtain the meaning-preserving para-phrases, we use Google translate and round-triptranslation via two pairs of arbitrarily chosen lan-guages (Danish/Finnish and Croatian/Hungarian).Annotators manually corrected lexical and syn-tactic errors introduced by the round-trip transla-tion.
To obtain meaning-changing paraphrases, weasked human annotators to alter each story so thatit topically resembles the original, but describes adifferent real-world event.
The extent of the al-teration was left to the annotators, i.e., no specifictransformations were proposed.
Paraphrase exam-ples are given in Table 2.
The final dataset consistsof 60 news pairs: 30 positive and 30 negative.Results.
For each method we ranked the pairsbased on the assigned similarity scores.
An idealmethod would rank all positive pairs above all neg-ative pairs.
We evaluated the performance usingModel R-prec.
Avg.
prec.Tensor PGK 86.7 96.8Conormal PGK 93.3 97.5WDK 86.7 95.7VSM baseline 80.0 77.1Table 3: Results for event-based similarity rankingtwo different rank evaluation metrics: R-precision(precision at rank 30, as there are 30 positive pairs)and average precision.
The performance of graphkernel models and the VSM baseline is given inTable 3.
We tested the significance of differencesusing stratified shuffling (Yeh, 2000).
When con-sidering average precision, all kernel models sig-nificantly (at p < 0.01) outperform the baseline.However, when considering R-precision, only theconormal PGK model significantly (at p < 0.05)outperforms the baseline.
There is no statisticalsignificance in performance differences betweenthe considered kernel methods.
Inspection of therankings reveals that graph kernels assign very lowscores to negative pairs, i.e., they distinguish wellbetween textual representations of topically simi-lar, but different real-world events.5 ConclusionWe proposed a novel approach for recognizingidentical events that relies on structured, graph-based representations of events described in adocument.
We use graph kernels as an expres-sive framework for modeling the similarity be-tween structured events.
Preliminary results ontwo event-similarity tasks are encouraging, indi-cating that our approach can outperform tradi-tional vector-space model, and is suitable for dis-tinguishing between topically very similar events.Further improvements could be obtained by in-creasing the accuracy of event coreference resolu-tion, which has a direct influence on graph kernels.The research opens up many interesting direc-tions for further research.
Besides a systematicevaluation on larger datasets, we intend to inves-tigate the applications in event tracking and event-oriented information retrieval.AcknowledgmentsThis work has been supported by the Ministry ofScience, Education and Sports, Republic of Croa-tia under the Grant 036-1300646-1986.
We thankthe reviewers for their constructive comments.801ReferencesACE.
2005.
Evaluation of the detection and recogni-tion of ACE: Entities, values, temporal expressions,relations, and events.James Allan.
2002.
Topic Detection and Tracking:Event-based Information Organization, volume 12.Kluwer Academic Pub.James Allen.
1983.
Maintaining knowledge abouttemporal intervals.
Communications of the ACM,26(11):832?843.Martin Atkinson and Erik Van der Goot.
2009.
Nearreal time information mining in multilingual news.In Proceedings of the 18th International Conferenceon World Wide Web, pages 1153?1154.
ACM.Cosmin Adrian Bejan and Sanda Harabagiu.
2008.
Alinguistic resource for discovering event structuresand resolving event coreference.
In Proceedings ofthe 6th International Conference on Language Re-sources and Evaluation (LREC 2008).Cosmin Adrian Bejan and Sanda Harabagiu.
2010.Unsupervised event coreference resolution with richlinguistic features.
In Proceedings of the 48th An-nual Meeting of the Association for ComputationalLinguistics, pages 1412?1422.
Association for Com-putational Linguistics.Steven Bethard.
2008.
Finding Event, Temporal andCausal Structure in Text: A Machine Learning Ap-proach.
Ph.D. thesis, University of Colorado atBoulder.Karsten Michael Borgwardt.
2007.
Graph Ker-nels.
Ph.D. thesis, Ludwig-Maximilians-Universita?tMu?nchen.Thorsten Brants, Francine Chen, and Ayman Farahat.2003.
A system for new event detection.
In Pro-ceedings of the 26th Annual International ACM SI-GIR Conference on Research and Development inInformation Retrieval, pages 330?337.
ACM.Thomas Ga?rtner, Peter Flach, and Stefan Wrobel.2003.
On graph kernels: Hardness results and ef-ficient alternatives.
In Learning Theory and KernelMachines, pages 129?143.
Springer.Goran Glavas?
and Jan S?najder.
2013.
Exploring coref-erence uncertainty of generically extracted eventmentions.
In Proceedings of 14th InternationalConference on Intelligent Text Processing and Com-putational Linguistics, pages 408?422.
Springer.Richard Hammack, Wilfried Imrich, and SandiKlavz?ar.
2011.
Handbook of Product Graphs.
Dis-crete Mathematics and Its Applications.
CRC Press.Vasileios Hatzivassiloglou, Luis Gravano, and Anki-needu Maganti.
2000.
An investigation of linguisticfeatures and clustering algorithms for topical doc-ument clustering.
In Proceedings of the 23rd An-nual International ACM SIGIR Conference on Re-search and Development in Information Retrieval,pages 224?231.
ACM.Giridhar Kumaran and James Allan.
2004.
Text clas-sification and named entities for new event detec-tion.
In Proceedings of the 27th Annual Interna-tional ACM SIGIR Conference on Research and De-velopment in Information Retrieval, pages 297?304.ACM.Giridhar Kumaran and James Allan.
2005.
Usingnames and topics for new event detection.
In Pro-ceedings of the Conference on Human LanguageTechnology and Empirical Methods in Natural Lan-guage Processing, pages 121?128.
Association forComputational Linguistics.Heeyoung Lee, Marta Recasens, Angel Chang, MihaiSurdeanu, and Dan Jurafsky.
2012.
Joint entity andevent coreference resolution across documents.
InProceedings of the 2012 Joint Conference on Empir-ical Methods in Natural Language Processing andComputational Natural Language Learning, pages489?500.
Association for Computational Linguis-tics.Pierre Mahe?, Nobuhisa Ueda, Tatsuya Akutsu, Jean-Luc Perret, and Jean-Philippe Vert.
2005.
Graphkernels for molecular structure-activity relationshipanalysis with support vector machines.
Journalof Chemical Information and Modeling, 45(4):939?951.Juha Makkonen, Helena Ahonen-Myka, and MarkoSalmenkivi.
2004.
Simple semantics in topic detec-tion and tracking.
Information Retrieval, 7(3):347?368.Sauro Menchetti, Fabrizio Costa, and Paolo Frasconi.2005.
Weighted decomposition kernels.
In Pro-ceedings of the 22nd International Conference onMachine Learning, pages 585?592.
ACM.James Pustejovsky, Jose?
Castano, Robert Ingria, RoserSauri, Robert Gaizauskas, Andrea Setzer, GrahamKatz, and Dragomir Radev.
2003a.
Timeml: Robustspecification of event and temporal expressions intext.
New Directions in Question Answering, 3:28?34.James Pustejovsky, Patrick Hanks, Roser Sauri, An-drew See, Robert Gaizauskas, Andrea Setzer,Dragomir Radev, Beth Sundheim, David Day, LisaFerro, et al2003b.
The TimeBank corpus.
In Cor-pus Linguistics, volume 2003, page 40.Gerard Salton, Anita Wong, and Chung-Shu Yang.1975.
A vector space model for automatic indexing.Communications of the ACM, 18(11):613?620.802Ralf Steinberger, Bruno Pouliquen, and Erik VanDer Goot.
2009.
An introduction to the euro-pean media monitor family of applications.
In Pro-ceedings of the Information Access in a Multilin-gual World-Proceedings of the SIGIR 2009 Work-shop, pages 1?8.Marc Verhagen, Robert Gaizauskas, Frank Schilder,Mark Hepple, Graham Katz, and James Pustejovsky.2007.
Semeval-2007 Task 15: TempEval tempo-ral relation identification.
In Proceedings of the 4thInternational Workshop on Semantic Evaluations,pages 75?80.
Association for Computational Lin-guistics.Marc Verhagen, Roser Sauri, Tommaso Caselli, andJames Pustejovsky.
2010.
Semeval-2010 Task 13:TempEval-2.
In Proceedings of the 5th Interna-tional Workshop on Semantic Evaluation, pages 57?62.
Association for Computational Linguistics.Charles Wayne.
2000.
Multilingual topic detectionand tracking: Successful research enabled by cor-pora and evaluation.
In Proceedings of the SecondInternational Conference on Language Resourcesand Evaluation Conference (LREC 2000), volume2000, pages 1487?1494.Yiming Yang, Jaime G Carbonell, Ralf D Brown,Thomas Pierce, Brian T Archibald, and Xin Liu.1999.
Learning approaches for detecting and track-ing news events.
Intelligent Systems and their Ap-plications, IEEE, 14(4):32?43.Alexander Yeh.
2000.
More accurate tests for the sta-tistical significance of result differences.
In Pro-ceedings of the 18th Conference on Computationallinguistics, pages 947?953.
Association for Compu-tational Linguistics.803
