Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),pages 136?143, Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsBeyond the Pipeline: Discrete Optimization in NLPTomasz Marciniak and Michael StrubeEML Research gGmbHSchloss-Wolfsbrunnenweg 3369118 Heidelberg, Germanyhttp://www.eml-research.de/nlpAbstractWe present a discrete optimization model based ona linear programming formulation as an alternativeto the cascade of classiers implemented in manylanguage processing systems.
Since NLP tasks arecorrelated with one another, sequential processingdoes not guarantee optimal solutions.
We apply ourmodel in an NLG application and show that it per-forms better than a pipeline-based system.1 IntroductionNLP applications involve mappings between com-plex representations.
In generation a representa-tion of the semantic content is mapped onto thegrammatical form of an expression, and in analy-sis the semantic representation is derived from thelinear structure of a text or utterance.
Each suchmapping is typically split into a number of differ-ent tasks handled by separate modules.
As notedby Daelemans & van den Bosch (1998), individ-ual decisions that these tasks involve can be formu-lated as classification problems falling in either oftwo groups: disambiguation or segmentation.
Theuse of machine-learning to solve such tasks facil-itates building complex applications out of manylight components.
The architecture of choice forsuch systems has become a pipeline, with strict or-dering of the processing stages.
An example ofa generic pipeline architecture is GATE (Cunning-ham et al, 1997) which provides an infrastructurefor building NLP applications.
Sequential process-ing has also been used in several NLG systems (e.g.Reiter (1994), Reiter & Dale (2000)), and has beensuccessfully used to combine standard preprocess-ing tasks such as part-of-speech tagging, chunkingand named entity recognition (e.g.
Buchholz et al(1999), Soon et al (2001)).In this paper we address the problem of aggregat-ing the outputs of classifiers solving different NLPtasks.
We compare pipeline-based processing withdiscrete optimization modeling used in the field ofcomputer vision and image recognition (Kleinberg& Tardos, 2000; Chekuri et al, 2001) and recentlyapplied in NLP by Roth & Yih (2004), Punyakanoket al (2004) and Althaus et al (2004).
WhereasRoth and Yih used optimization to solve two tasksonly, and Punyakanok et al and Althaus et al fo-cused on a single task, we propose a general for-mulation capable of combining a large number ofdifferent NLP tasks.
We apply the proposed modelto solving numerous tasks in the generation processand compare it with two pipeline-based systems.The paper is structured as follows: in Section 2 wediscuss the use of classifiers for handling NLP tasksand point to the limitations of pipeline processing.In Section 3 we present a general discrete optimiza-tion model whose application in NLG is describedin Section 4.
Finally, in Section 5 we report on theexperiments and evaluation of our approach.2 Solving NLP Tasks with ClassifiersClassification can be defined as the task Ti of as-signing one of a discrete set of mi possible labelsLi = {li1, ..., limi}1 to an unknown instance.
Sincegeneric machine-learning algorithms can be appliedto solving single-valued predictions only, complex1Since we consider different NLP tasks with varying num-bers of labels we denote the cardinality of Li, i.e.
the set ofpossible labels for task Ti, as mi.136Startl n1 l n2l 22l 21l 11 l 12lnnml 22ml1m1p(l    )11 p(l     )1m1p(l)121TT2Tn2m2p(l     )p(l    )22p(l   )21...........................Figure 1: Sequential processing as a graph.structures, such as parse trees, coreference chains orsentence plans, can only be assembled from the out-puts of many different classifiers.In an application implemented as a cascade ofclassifiers the output representation is built incre-mentally, with subsequent classifiers having accessto the outputs of previous modules.
An importantcharacteristic of this model is its extensibility: itis generally easy to change the ordering or insertnew modules at any place in the pipeline2 .
A ma-jor problem with sequential processing of linguis-tic data stems from the fact that elements of linguis-tic structure, at the semantic or syntactic levels, arestrongly correlated with one another.
Hence clas-sifiers that have access to additional contextual in-formation perform better than if this information iswithheld.
In most cases, though, if task Tk can usethe output of Ti to increase its accuracy, the reverseis also true.
In practice this type of processing maylead to error propagation.
If due to the scarcity ofcontextual information the accuracy of initial clas-sifiers is low, erroneous values passed as input tosubsequent tasks can cause further misclassificationswhich can distort the final outcome (also discussedby Roth and Yih and van den Bosch et al (1998)).As can be seen in Figure 1, solving classifica-tion tasks sequentially corresponds to the best-firsttraversal of a weighted multi-layered lattice.
Nodesat separate layers (T1, ..., Tn) represent labels of dif-ferent classification tasks and transitions betweenthe nodes are augmented with probabilities of se-2Both operations only require retraining classifiers with anew selection of the input features.lecting respective labels at the next layer.
In the se-quential model only transitions between nodes be-longing to subsequent layers are allowed.
At eachstep, the transition with the highest local probabilityis selected.
Selected nodes correspond to outcomesof individual classifiers.
This graphical representa-tion shows that sequential processing does not guar-antee an optimal context-dependent assignment ofclass labels and favors tasks that occur later, by pro-viding them with contextual information, over thosethat are solved first.3 Discrete Optimization ModelAs an alternative to sequential ordering of NLPtasks we consider the metric labeling problem for-mulated by Kleinberg & Tardos (2000), and orig-inally applied in an image restoration application,where classifiers determine the ?true?
intensity val-ues of individual pixels.
This task is formulated as alabeling function f : P ?
L, that maps a set P of nobjects onto a set L of m possible labels.
The goalis to find an assignment that minimizes the overallcost function Q(f), that has two components: as-signment costs, i.e.
the costs of selecting a particularlabel for individual objects, and separation costs, i.e.the costs of selecting a pair of labels for two relatedobjects3 .
Chekuri et al (2001) proposed an integerlinear programming (ILP) formulation of the met-ric labeling problem, with both assignment cost andseparation costs being modeled as binary variablesof the linear cost function.Recently, Roth & Yih (2004) applied an ILPmodel to the task of the simultaneous assignmentof semantic roles to the entities mentioned in a sen-tence and recognition of the relations holding be-tween them.
The assignment costs were calculatedon the basis of predictions of basic classifiers, i.e.trained for both tasks individually with no access tothe outcomes of the other task.
The separation costswere formulated in terms of binary constraints, thatspecified whether a specific semantic role could oc-cur in a given relation, or not.In the remainder of this paper, we present a moregeneral model, that is arguably better suited to hand-ling different NLP problems.
More specifically, we3These costs were calculated as the function of the metricdistance between a pair of pixels and the difference in intensity.137put no limits on the number of tasks being solved,and express the separation costs as stochastic con-straints, which for almost any NLP task can be cal-culated off-line from the available linguistic data.3.1 ILP FormulationWe consider a general context in which a specificNLP problem consists of individual linguistic de-cisions modeled as a set of n classification tasksT = {T1, ..., Tn}, that potentially form mutuallyrelated pairs.
Each task Ti consists in assigning alabel from Li = {li1, ..., limi} to an instance thatrepresents the particular decision.
Assignments aremodeled as variables of a linear cost function.
Wedifferentiate between simple variables that model in-dividual assignments of labels and compound vari-ables that represent respective assignments for eachpair of related tasks.To represent individual assignments the followingprocedure is applied: for each task Ti, every labelfrom Li is associated with a binary variable x(lij).Each such variable represents a binary choice, i.e.
arespective label lij is selected if x(lij) = 1 or re-jected otherwise.
The coefficient of variable x(lij),that models the assignment cost c(lij), is given by:c(lij) = ?log2(p(lij))where p(lij) is the probability of lij being selected asthe outcome of task Ti.
The probability distributionfor each task is provided by the basic classifiers thatdo not consider the outcomes of other tasks4.The role of compound variables is to provide pair-wise constraints on the outcomes of individual tasks.Since we are interested in constraining only thosetasks that are truly dependent on one another we firstapply the contingency coefficient C to measure thedegree of correlation for each pair of tasks5.In the case of tasks Ti and Tk which are sig-nificantly correlated, for each pair of labels from4In this case the ordering of tasks is not necessary, and theclassifiers can run independently from each other.5C is a test for measuring the association of two nominalvariables, and hence adequate for the type of tasks that we con-sider here.
The coefficient takes values from 0 (no correlation)to 1 (complete correlation) and is calculated by the formula:C = (?2/(N + ?2))1/2, where ?2 is the chi-squared statisticand N the total number of instances.
The significance of C isthen determined from the value of ?2 for the given data.
Seee.g.
Goodman & Kruskal (1972).Li ?
Lk we build a single variable x(lij , lkp).
Eachsuch variable is associated with a coefficient repre-senting the constraint on the respective pair of labelslij , lkp calculated in the following way:c(lij , lkp) = ?log2(p(lij,lkp))with p(lij,lkp) denoting the prior joint probability oflabels lij, and lkp in the data, which is independentfrom the general classification context and hence canbe calculated off-line6 .The ILP model consists of the target function anda set of constraints which block illegal assignments(e.g.
only one label of the given task can be se-lected)7 .
In our case the target function is the costfunction Q(f), which we want to minimize:min Q(f) =?Ti?T?lij?Lic(lij) ?
x(lij)+?Ti,Tk?T,i<k?lij ,lkp?Li?Lkc(lij , lkp) ?
x(lij , lkp)Constraints need to be formulated for both thesimple and compound variables.
First we want toensure that exactly one label lij belonging to task Tiis selected, i.e.
only one simple variable x(lij) rep-resenting labels of a given task can be set to 1:?lij?Lix(lij) = 1, ?i ?
{1, ..., n}We also require that if two simple variables x(lij)and x(lkp), modeling respectively labels lij and lkpare set to 1, then the compound variable x(lij , lkp),which models co-occurrence of these labels, is alsoset to 1.
This is done in two steps: we first en-sure that if x(lij) = 1, then exactly one variablex(lij , lkp) must also be set to 1:x(lij) ?
?lkp?Lkx(lij , lkp) = 0,?i, k ?
{1, ..., n}, i < k ?
j ?
{1, ..., mi}and do the same for variable x(lkp):6In Section 5 we discuss an alternative approach which con-siders the actual input.7For a detailed overview of linear programming and differ-ent types of LP problems see e.g.
Nemhauser & Wolsey (1999).138l 111TTnl 21T2l n1l 1m1l 2m2l nmn.........c(l    )11c(l    ,l     )11   2m11   21c(l    ,l    )1m21c(l    ,l    )c(l    ,l)21   nmc(l     ,l     )c(l,l)21n1c(l,l)2mn12mnmc(l,l)c(l     )2m21c(l    )c(l    )n1c(l     )nm1mc(l     )1m   2mFigure 2: Graph representation of the ILP model.x(lkp) ?
?lij?Lix(lij , lkp) = 0,?i, k ?
{1, ..., n}, i < k ?
p ?
{1, ..., mk}Finally, we constrain the values of both simpleand compound variables to be binary:x(lij) ?
{0, 1} ?
x(lij , lkp) ?
{0, 1},?i, k ?
{1, ..., n} ?
j ?
{1, ..., mi} ?
p ?
{1, ..., mk}3.2 Graphical RepresentationWe can represent the decision process that our ILPmodel involves as a graph, with the nodes corre-sponding to individual labels and the edges markingthe association between labels belonging to corre-lated tasks.
In Figure 2, task T1 is correlated withtask T2 and task T2 with task Tn.
No correlationexists for pair T1, Tn.
Both nodes and edges areaugmented with costs.
The goal is to select a sub-set of connected nodes, minimizing the overall cost,given that for each group of nodes T1, T2, ..., Tn ex-actly one node must be selected, and the selectednodes, representing correlated tasks, must be con-nected.
We can see that in contrast to the pipelineapproach (cf.
Figure 1), no local decisions determinethe overall assignment as the global distribution ofcosts is considered.4 Application for NL Generation TasksWe applied the ILP model described in the previoussection to integrate different tasks in an NLG ap-plication that we describe in detail in Marciniak &Strube (2004).
Our classification-based approach tolanguage generation assumes that different types oflinguistic decisions involved in the generation pro-cess can be represented in a uniform way as clas-sification problems.
The linguistic knowledge re-quired to solve the respective classifications is thenlearned from a corpus annotated with both seman-tic and grammatical information.
We have appliedthis framework to generating natural language routedirections, e.g.
:(a) Standing in front of the hotel (b) fol-low Meridian street south for about 100meters, (c) passing the First Union Bankentrance on your right, (d) until you seethe river side in front of you.We analyze the content of such texts in terms oftemporally related situations, i.e.
actions (b), states(a) and events (c,d), denoted by individual discourseunits8.
The semantics of each discourse unit is fur-ther given by a set of attributes specifying the se-mantic frame and aspectual category of the pro-filed situation.
Our corpus of semantically anno-tated route directions comprises 75 texts with a to-tal number of 904 discourse units (see Marciniak &Strube (2005)).
The grammatical form of the textsis modeled in terms of LTAG trees also representedas feature vectors with individual features denotingsyntactic and lexical elements at both the discourseand clause levels.
The generation of each discourseunit consists in assigning values to the respectivefeatures, of which the LTAG trees are then assem-bled.
In Marciniak & Strube (2004) we implementedthe generation process sequentially as a cascade ofclassifiers that realized incrementally the vector rep-resentation of the generated text?s form, given themeaning vector as input.
The classifiers handled thefollowing eight tasks, all derived from the LTAG-based representation of the grammatical form:T1: Discourse Units Rank is concerned with or-dering discourse units at the local level, i.e.
onlyclauses temporally related to the same parent clauseare considered.
This task is further split into a seriesof binary precedence classifications that determinethe relative position of two discourse units at a time8The temporal structure was represented as a tree, with dis-course units as nodes.139Discourse Unit T3 T4 T5Pass the First Union Bank ... null vp bare inf.It is necessary that you pass ... null np+vp bare inf.Passing the First Union Bank ... null vp gerundAfter passing ... after vp gerundAfter your passing .
.
.
after np+vp gerundAs you pass ... as np+vp fin.
pres.Until you pass ... until np+vp fin.
pres.Until passing .
.
.
until vp gerundTable 1: Different realizations of tasks: Connective, VerbForm and S Exp.
Rare but correct constructions are in italics.T :  Verb Lex6T :  Phrase Type7T :  Phrase Rank84T :  S Exp.T :  Disc.
Units Dir.2T :  Verb Form5T :  Disc.
Units Rank1T :  Connective3Figure 3: Correlation network for the generation tasks.
Cor-related tasks, are connected with lines.(e.g.
(a) before (c), (c) before (d), etc.).
These partialresults are later combined to determine the ordering.T2: Discourse Unit Position specifies the positionof the child discourse unit relative to the parent one(e.g.
(a) left of (b), (c) right of (b), etc.
).T3: Discourse Connective determines the lexicalform of the discourse connective (e.g.
null in (a), un-til in (d)).T4: S Expansion specifies whether a given dis-course unit would be realized as a clause with theexplicit subject (i.e.
np+vp expansion of the root Snode in a clause) (e.g.
(d)) or not (e.g.
(a), (b)).T5: Verb Form determines the form of the mainverb in a clause (e.g.
gerund in (a), (c), bare infini-tive in (b), finite present in (d)).T6: Verb Lexicalization provides the lexical formof the main verb (e.g.
stand, follow, pass, etc.
).T7: Phrase Type determines for each verb argu-ment in a clause its syntactic realization as a nounphrase, prepositional phrase or a particle.T8: Phrase Rank determines the ordering of verbarguments within a clause.
As in T1 this task is splitinto a number binary classifications.To apply the LP model to the generation problemdiscussed above, we first determined which pairs oftasks are correlated.
The obtained network (Fig-ure 3) is consistent with traditional analyses of thelinguistic structure in terms of adjacent but sepa-rate levels: discourse, clause, phrase.
Only a fewcorrelations extend over level boundaries and taskswithin those levels are correlated.
As an exampleconsider three interrelated tasks: Connective, S Exp.and Verb Form and their different realizations pre-sented in Table 1.
Apparently different realizationof any of these tasks can affect the overall meaningof a discourse unit or its stylistics.
It can also be seenthat only certain combinations of different forms areallowed in the given semantic context.
We can con-clude that for such groups of tasks sequential pro-cessing may fail to deliver an optimal assignment.5 Experiments and ResultsIn order to evaluate our approach we conductedexperiments with two implementations of the ILPmodel and two different pipelines (presented below).Each system takes as input a tree structure, repre-senting the temporal structure of the text.
Individ-ual nodes correspond to single discourse units andtheir semantic content is given by respective featurevectors.
Generation occurs in a number of stages,during which individual discourse units are realized.5.1 Implemented SystemsWe used the ILP model described in Section 3 tobuild two generation systems.
To obtain assignmentcosts, both systems get a probability distribution foreach task from basic classifiers trained on the train-ing data.
To calculate the separation costs, modelingthe stochastic constraints on the co-occurrence of la-bels, we considered correlated tasks only (cf.
Figure3) and applied two calculation methods, which re-sulted in two different system implementations.In ILP1, for each pair of tasks we computed thejoint distribution of the respective labels consider-ing all discourse units in the training data before theactual input was known.
Such obtained joint distri-butions were used for generating all discourse unitsfrom the test data.
An example matrix with joint dis-tribution for selected labels of tasks Connective andVerb Form is given in Table 2.
An advantage of this140null and as after until T3 ConnectiveT5 Verb Form0.40 0.18 0 0 0 bare inf0 0 0 0.04 0.01 gerund0.05 0.01 0.06 0.03 0.06 n pres0.06 0.05 0 0 0 will infTable 2: Joint distribution matrix for selected labels of tasksConnective (horizontal) and Verb Form (vertical), computed forall discourse units in a corpus.null and as after until T3 ConnectiveT5 Verb Form0.13 0.02 0 0 0 bare inf0 0 0 0 0 gerund0 0 0.05 0.02 0.27 n pres0.36 0.13 0 0 0 will infTable 3: Joint distribution matrix for tasks Connective andVerb Form, considering only discourse units similar to (c): untilyou see the river side in front of you, at Phi-threshold ?
0.8.approach is that the computation can be done in anoffline mode and has no impact on the run-time.In ILP2, the joint distribution for a pair of taskswas calculated at run-time, i.e.
only after the actualinput had been known.
This time we did not con-sider all discourse units in the training data, but onlythose whose meaning, represented as a feature vec-tor was similar to the meaning vector of the inputdiscourse unit.
As a similarity metric we used thePhi coefficient9 , and set the similarity threshold at0.8.
As can be seen from Table 3, the probabilitydistribution computed in this way is better suited tothe specific semantic context.
This is especially im-portant if the available corpus is small and the fre-quency of certain pairs of labels might be too low tohave a significant impact on the final assignment.As a baseline we implemented two pipeline sys-tems.
In the first one we used the ordering oftasks most closely resembling the conventional NLGpipeline (see Figure 4).
Individual classifiers had ac-cess to both the semantic features, and those outputby the previous modules.
To train the classifiers,the correct feature values were extracted from thetraining data and during testing the generated, andhence possibly erroneous, values were taken.
In the9Phi is a measure of the extent of correlation between twosets of binary variables, see e.g.
Edwards (1976).
To representmulti-class features on a binary scale we applied dummy cod-ing which transforms multi class-nominal variables to a set ofdummy variables with binary values.other pipeline system we wanted to minimize theerror-propagation effect and placed the tasks in theorder of decreasing accuracy.
To determine the or-dering of tasks we applied the following procedure:the classifier with the highest baseline accuracy wasselected as the first one.
The remaining classifierswere trained and tested again, but this time they hadaccess to the additional feature.
Again, the classi-fier with the highest accuracy was selected and theprocedure was repeated until all classifiers were or-dered.5.2 EvaluationWe evaluated our system using leave-one-out cross-validation, i.e.
for all texts in the corpus, eachtext was used once for testing, and the remainingtexts provided the training data.
To solve individ-ual classification tasks we used the decision treelearner C4.5 in the pipeline systems and the NaiveBayes algorithm10 in the ILP systems.
Both learn-ing schemes yielded highest results in the respec-tive configurations11 .
For each task we applieda feature selection procedure (cf.
Kohavi & John(1997)) to determine which semantic features shouldbe taken as the input by the respective basic classi-fiers12.
We started with an empty feature set, andthen performed experiments checking classificationaccuracy with only one new feature at a time.
Thefeature that scored highest was then added to the fea-ture set and the whole procedure was repeated itera-tively until no performance improvement took place,or no more features were left.To evaluate individual tasks we applied two met-rics: accuracy, calculated as the proportion of cor-rect classifications to the total number of instances,and the ?
statistic, which corrects for the propor-tion of classifications that might occur by chance1310Both implemented in the Weka machine learning software(Witten & Frank, 2000).11We have found that in direct comparison C4.5 reacheshigher accuracies than Naive Bayes but the probability distri-bution that it outputs is strongly biased towards the winning la-bel.
In this case it is practically impossible for the ILP systemto change the classifier?s decision, as the costs of other labelsget extremely high.
Hence the more balanced probability dis-tribution given by Naive Bayes can be easier corrected in theoptimization process.12I.e.
trained using the semantic features only, with no accessto the outputs of other tasks.13Hence the ?
values obtained for tasks of different difcul-141Pipeline 1 Pipeline 2 ILP 1 ILP 2Tasks Pos.
Accuracy ?
Pos.
Accuracy ?
Accuracy ?
Accuracy ?Dis.Un.
Rank 1 96.81% 90.90% 2 96.81% 90.90% 97.43% 92.66% 97.43% 92.66%Dis.Un.
Pos.
2 98.04% 89.64% 1 98.04% 89.64% 96.10% 77.19% 97.95% 89.05%Connective 3 78.64% 60.33% 7 79.10% 61.14% 79.15% 61.22% 79.36% 61.31%S Exp.
4 95.90% 89.45% 3 96.20% 90.17% 99.48% 98.65% 99.49% 98.65%Verb Form 5 86.76% 77.01% 4 87.83% 78.90% 92.81% 87.60% 93.22% 88.30%Verb Lex 6 64.58% 60.87% 8 67.40% 64.19% 75.87% 73.69% 76.08% 74.00%Phr.
Type 7 86.93% 75.07% 5 87.08% 75.36% 87.33% 76.75% 88.03% 77.17%Phr.
Rank 8 84.73% 75.24% 6 86.95% 78.65% 90.22% 84.02% 91.27% 85.72%Phi 0.85 0.87 0.89 0.90Table 4: Results reached by the implemented ILP systems and two baselines.
For both pipeline systems, Pos.
stands for theposition of the tasks in the pipeline.
(Siegel & Castellan, 1988).
For end-to-end evalua-tion, we applied the Phi coefficient to measure thedegree of similarity between the vector representa-tions of the generated form and the reference formobtained from the test data.
The Phi statistic is sim-ilar to ?
as it compensates for the fact that a matchbetween two multi-label features is more difficult toobtain than in the case of binary features.
This mea-sure tells us how well all the tasks have been solvedtogether, which in our case amounts to generatingthe whole text.The results presented in Table 4 show that the ILPsystems achieved highest accuracy and ?
for mosttasks and reached the highest overall Phi score.
No-tice that for the three correlated tasks that we consid-ered before, i.e.
Connective, S Exp.
and Verb Form,ILP2 scored noticeably higher than the pipeline sys-tems.
It is interesting to see the effect of sequentialprocessing on the results for another group of cor-related tasks, i.e.
Verb Lex, Phrase Type and PhraseRank (cf.
Figure 3).
Verb Lex got higher scoresin Pipeline2, with outputs from both Phrase Typeand Phrase Rank (see the respective pipeline posi-tions), but the reverse effect did not occur: scoresfor both phrase tasks were lower in Pipeline1 whenthey had access to the output from Verb Lex, con-trary to what we might expect.
Apparently, this wasdue to the low accuracy for Verb Lex which causedthe already mentioned error propagation14 .
This ex-ample shows well the advantage that optimizationprocessing brings: both ILP systems reached muchties can be directly compared, which gives a clear notion howwell individual tasks have been solved.14Apparantly, tasks which involve lexical choice get lowscores with retrieval measures as the semantic content allowstypically more than one correct formhigher scores for all three tasks.5.3 Technical NotesThe size of an LP model is typically expressed in thenumber of variables and constraints.
In the modelpresented here it depends on the number of tasks inT , the number of possible labels for each task, andthe number of correlated tasks.
For n different taskswith the average of m labels, and assuming everytwo tasks are correlated with each other, the num-ber of variables in the LP target functions is givenby: num(var) = n ?
m + 1/2 ?
n(n ?
1) ?
m2and the number of constraints by: num(cons) =n + n ?
(n ?
1) ?
m. To solve the ILP models in oursystem we use lp solve, an efficient GNU-licenceMixed Integer Programming (MIP) solver15, whichimplements the Branch-and-Bound algorithm.
Inour application, the models varied in size from: 557variables and 178 constraints to 709 variables and240 constraints, depending on the number of ar-guments in a sentence.
Generation of a text with23 discourse units took under 7 seconds on a two-processor 2000 MHz AMD machine.6 ConclusionsIn this paper we argued that pipeline architectures inNLP can be successfully replaced by optimizationmodels which are better suited to handling corre-lated tasks.
The ILP formulation that we proposedextends the classification paradigm already estab-lished in NLP and is general enough to accommo-date various kinds of tasks, given the right kind ofdata.
We applied our model in an NLG applica-tion.
The results we obtained show that discrete15http://www.geocities.com/lpsolve/142optimization eliminates some limitations of sequen-tial processing, and we believe that it can be suc-cessfully applied in other areas of NLP.
We viewour work as an extension to Roth & Yih (2004) intwo important aspects.
We experiment with a largernumber of tasks having a varying number of labels.To lower the complexity of the models, we applycorrelation tests, which rule out pairs of unrelatedtasks.
We also use stochastic constraints, which areapplication-independent, and for any pair of taskscan be obtained from the data.A similar argument against sequential modular-ization in NLP applications was raised by van denBosch et al (1998) in the context of word pronun-ciation learning.
This mapping between words andtheir phonemic transcriptions traditionally assumesa number of intermediate stages such as morpho-logical segmentation, graphemic parsing, grapheme-phoneme conversion, syllabification and stress as-signment.
The authors report an increase in gener-alization accuracy when the the modular decompo-sition is abandoned (i.e.
the tasks of conversion tophonemes and stress assignment get conflated andthe other intermediate tasks are skipped).
It is inter-esting to note that a similar dependence on the inter-mediate abstraction levels is present in such applica-tions as parsing and semantic role labelling, whichboth assume POS tagging and chunking as their pre-ceding stages.Currently we are working on a uniform data for-mat that would allow to represent different NLP ap-plications as multi-task optimization problems.
Weare planning to release a task-independent Java APIthat would solve such problems.
We want to use thisgeneric model for building NLP modules that tradi-tionally are implemented sequentially.Acknowledgements: The work presented herehas been funded by the Klaus Tschira Foundation,Heidelberg, Germany.
The first author receives ascholarship from KTF (09.001.2004).ReferencesAlthaus, E., N. Karamanis & A. Koller (2004).
Computing lo-cally coherent discourses.
In Proceedings of the 42 AnnualMeeting of the Association for Computational Linguistics,Barcelona, Spain, July 21-26, 2004, pp.
399?406.Buchholz, S., J. Veenstra & W. Daelemans (1999).
Cascadedgrammatical relation assignment.
In Joint SIGDAT Confer-ence on Empirical Methods in Natural Language Processingand Very Large Corpora, College Park, Md., June 21-22,1999, pp.
239?246.Chekuri, C., S. Khanna, J. Naor & L. Zosin (2001).
Approx-imation algorithms for the metric labeling problem via anew linear programming formulation.
In Proceedings of the12th Annual ACM SIAM Symposium on Discrete Algorithms,Washington, DC, pp.
109?118.Cunningham, H., K. Humphreys, Y. Wilks & R. Gaizauskas(1997).
Software infrastructure for natural language process-ing.
In Proceedings of the Fifth Conference on Applied Natu-ral Language Processing Washington, DC, March 31 - April3, 1997, pp.
237?244.Daelemans, W. & A. van den Bosch (1998).
Rapid develop-ment of NLP modules with memory-based learning.
In Pro-ceedings of ELSNET in Wonderland.
Utrecht: ELSNET, pp.105?113.Edwards, Allen, L. (1976).
An Introduction to Linear Regres-sion and Correlation.
San Francisco, Cal.
: W. H. Freeman.Goodman, L. A.
& W. H. Kruskal (1972).
Measures of asso-ciation for cross-classification, iv.
Journal of the AmericanStatistical Association, 67:415?421.Kleinberg, J. M. & E. Tardos (2000).
Approximation algorithmsfor classification problems with pairwise relationships: Met-ric labeling and Markov random fields.
Journal of the ACM,49(5):616?639.Kohavi, R. & G. H. John (1997).
Wrappers for feature subsetselection.
Articial Intelligence Journal, 97:273?324.Marciniak, T. & M. Strube (2004).
Classification-based gen-eration using TAG.
In Proceedings of the 3rd InternationalConference on Natural Language Generation, Brockenhurst,UK, 14-16 July, 2004, pp.
100?109.Marciniak, T. & M. Strube (2005).
Modeling and annotating thesemantics of route directions.
In Proceedings of the 6th In-ternational Workshop on Computational Semantics, Tilburg,The Netherlands, January 12-14, 2005, pp.
151?162.Nemhauser, G. L. & L. A. Wolsey (1999).
Integer and combi-natorial optimization.
New York, NY: Wiley.Punyakanok, V., D. Roth, W. Yih & Z. Dav (2004).
Semanticrole labeling via integer linear programming inference.
InProceedings of the 20th International Conference on Com-putational Linguistics, Geneva, Switzerland, August 23-27,2004, pp.
1346?1352.Reiter, E. (1994).
Has a consensus NL generation architectureappeared, and is it psycholinguistically plausible?
In Pro-ceedings of the 7th International Workshop on Natural Lan-guage Generation, Kennebunkport, Maine, pp.
160?173.Reiter, E. & R. Dale (2000).
Building Natural Language Gener-ation Systems.
Cambridge, UK: Cambridge University Press.Roth, D. & W. Yih (2004).
A linear programming formulationfor global inference in natural language tasks.
In Proceed-ings of the 8th Conference on Computational Natural Lan-guage Learning, Boston, Mass., May 2-7, 2004, pp.
1?8.Siegel, S. & N. J. Castellan (1988).
Nonparametric Statisticsfor the Behavioral Sciences.
New York, NY: McGraw-Hill.Soon, W. M., H. T. Ng & D. C. L. Lim (2001).
A machinelearning approach to coreference resolution of noun phrases.Computational Linguistics, 27(4):521?544.van den Bosch, A., T. Weijters & W. Daelemans (1998).
Modu-larity in inductively-learned word pronunciation systems.
InD.
Powers (Ed.
), Proceedings of NeMLaP3/CoNLL98, pp.185?194.Witten, I. H. & E. Frank (2000).
Data Mining - Practical Ma-chine Learning Tools and Techniques with Java Implementa-tions.
San Francisco, Cal.
: Morgan Kaufmann.143
