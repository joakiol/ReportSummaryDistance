IMPROVEMENTS IN STOCHASTIC LANGUAGE MODELINGRonald Rosenfeld and Xuedong HuangSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213ABSTRACTWe describe two attempt to improve our stochastic language models.In the first, we identify a systematic overestimation in the traditionalbackoff model, and use statisticalreasoning to correct i .
Our modifi-cation results in up to 6% reduction i  the perplexity of various tasks.Although the improvement is modest, it is achieved with hardly anyincreasein the complexity of the model.
Both analysis and empiricaldata suggestthat the moditieation is most suitable when training datais sparse.In the second attempt, we propose anew type of adaptive languagemodel.
Existing adaptive models use a dynamic eacbe, based onthe history of the document seen up to that point.
But anothersource of information i the history, within-document word sequencecorrelations, has not yet been tapped.
We describe a model thatattempts to capture this information, using a framework where oneword sequence laJggers another, eansing its estimated probability tobe raised.
We discuss various issues in the design of such a model,and describe our first attempt at building one.
Our preliminary resultsinclude a perplexity reduction of between 10% and 32%, dependingon the test set.1.
INTRODUCTIONLinguistic constraints are an important factor in human com-prehension of speech.
Their effect on automatic speech recog-nition is similar, in that they provide both a pruning methodand a means of  ordering likely candidates.
As vocabulariesfor speech recognition systems increase in size, more accuratemodeling of  linguistic constraints becomes essential.Two fundamental issues in language modeling are smooth-ing and adaptation.
Smoothing allows a model to assignreasonable probabilities to events that have never been ob-served before.
Adaptation takes advantage of recently gainedknowledge - -  the text seen so far - -  to adjust the model'sexpectations.In what follows, we discuss two attempts at improving ourcurrent stochastic language modeling techniques.
In the first,we try to improve smoothing by correcting a deficiency ina successful and well known smoothing method, the backoffmodel.
In the second, we propose a novel kind of adapta-tion, one that is based on correlation among word sequencesoccurring in the same document.2.
CORRECTING OVERESTIMATIONIN THE BACKOFF MODEL2.1.
The Prob lemThe backoff n-gram language model\[l\] estimates the prob-n--1 ability of w,, given the immediate past history w~ =(wl .
.
.
.
w~-0.
It is defined recursively as:Pn(w"lw~-t) =/ (1 - d)C(w~) / C(~1-1) if C(w~) > 0o~(C(w~-l)) ?
en_l(wnlw~ -1) if C(w~) = 0k(1)where d, the discount ratio, is a function of C(w~), and thea's  are the backoff weights, calculated to satisfy the sum-to-1probability constraints.The backoff language model is a compact yet powerful way ofmodeling the dependence ofthe current word on its immediatehistory.
An important factor in the backoff model is its be-havior on the backed-off cases, namely when a given n-gramw~ is found not to have occurred in the training data.
In thesecases, the model assumes that the probability is proportionalto the estimate provided by the n-1-gram, Pn-l(Wn \[W~- 1).This last assumption is reasonable most of the time, since noother sources of  information are available.
But for frequentn-l-grams, there may exist sufficient statistical evidence tosuggest hat the backed-off probabilities hould in fact bemuch lower.
This phenomenon occurs at any value of n, butis easiest o demonstrate for the simple case of  n = 2, i.e.
abigram.
Consider the following fictitious but typical example:N = 1,000,000C("ON") = 10,000CCAT') = 10,000C("CALL") = I00C("ON","AT") = 0C("ON" , "CALL" )  = 0N is the total number of  words in the training set, and C(wz, w i)is the number of  (wi, wj) bigrams occurring in that set.
Thebackoff model computes:107P("Kr ' )  =P ( "CALL" )  = i 10,000P("Nr'r'ON") = ~("ON")  ?
P ("AT")  = a("ON") .
\]-~P("CALL"I"ON") = ~("ON") P("CALL") = c~("ON")- 1 10000Thus, according to this model, P("AT"I"ON") >>P("CALL"\["ON").
But this is clearly incorrect.
In the case of"CAIJ?
', the expected number of ("ON","CALL") bigrams,assuming independence b tween "ON" and "CALL", is 1, soan actual count of 0 does not give much information, andmay be ignored.
However, in the case of "AT", the expectedchance count of ("ON","AT") is 100, so an actual count of0 means that the real probability of P("AT"I"ON") is in factmuch lower than chance.
The backoff model does not cap-ture this information, and thus grossly overestimates P("AT"I"ON").This deficiency of the backoff model has been pointed outbefore\[2, p.457\], but, to the best of our knowledge, has neverbeen corrected.
We suspect he reasons are twofold.
First,it only occurs during backed-off cases.
For a well trainedbigram or trigram, this happens in only a small fraction ofthe time.
Second, overestimation degrades perplexity onlymildly and indirectly, by affecting a slight underestimation ofall the other probabilities.We therefore did not expect this phenomenon to have a strongimpact on perplexity.
Nevertheless, we wanted to correct heproblem and to measure its effect.2.2.
The Solution:Confidence Interval CappingLet C(~1) = 0.
Given a global confidence level Q, to bedetermined empirically, we calculate a confidence intervalin which the true value of P(w~lw~ -1) should lie, using theconstraint:\[1 -- P(wnmw~-l)\]c(~ -') > Q (2)The confidence interval is therefore \[0 .. .
(1 - Q1/C(~-')) \].We then provide another parameter, P (0 < P < 1), and es-tablish a ceiling, or a cap, at a point P within the confidenceinterval:CAPe,e(C(w~- I)) = P. (1 - Q1/C(~ -~)) (3)We now require that the estimated P(wnlw~ -1) satisfy:P(wn Iw~- 1) _< CAPQ,p (C(w?
- 1 )) (4)The backoff case of the standard model is therefore modifiedto:e(w.lw~ -1) =min \[ o~(w~-l) .
P,~_l(w,,Iw~-l), CAPQ,p(C(w~-X)) I5)This capping off of the estimates requires renormalization.But renormalization would increase the a's, which would inturn cause some backed-off probabilities to exceed the cap.An iterative reestimation f the cz's is therefore r quired.
Theprocess was found to converge rapidly in all cases.Note that, although some computation is required to determinethe new weights, once the model has been computed, it is nomore complicated neither significantly more time consumingthan the original one.2.3.
Resu l tsThe bigrarn perplexity reduction for various tasks is shownin table 1.
BC-48K is the brown corpus with the unabridgedtest set backoff rate PP reductionBC-48K 30% 6.3%BC-5K 15% 2.5%ATIS 5% 1.7%WSJ-5K 2% 0.8%Table 1: Perplexity reduction by Confidence Interval Cappingvocabulary of 48,455 words.
BC-5K is the same corpus,restricted to the most frequent 5,000 words.
ATIS is the class-based bigram developed at CMU for the ATIS task.
WSJ isthe official CSR 5c.vp task.Although the reduction is modest, as expected, it should beremembered that it is achieved with hardly any increase inthe complexity of the model.
As can be predicted from thestatistical nalysis, when the vocabulary is larger, the backoffrate is greater, and the improvement in perplexity can beexpected to be greater too.3.
TR IGGER-BASED ADAPTAT ION3.1.
Motivation and Ana lys i sSeveral adaptive language models have been proposed re-cently \[3, 4, 5, 6\], which use caching of the partially dictateddocument, and interpolate a dynamic omponent based on thecache with the static component.
These models have beensuccessful in reducing the perplexity of the text considerably,and \[5\] also reports apositive ffect on the word recognitionrate.All of these models make direct use of the words in the his-tory of the document.
They take advantage of the fact thatygords, and combinations of words, once occurred in a givenedocument, have a higher likelihood of occurring in it again.But there is another source of information i  the history thathas not yet been tapped: within-document correlation between108words or word sequences.
Consider the sentence:"The district attorney's office launched a compre-hensive investigation i to loans made by severalwell connected banks.
"Based on this sentence alone, a cache-based model willnot be able to anticipate any of the constituent words.But a human reader might use "DISTRICT ATTORNEY"and/or "LAUNCHED" to anticipate "INVESTIGATION",and "LOANS" to anticipate "BANKS".In what follows, we describe amodel that attempts o capturethis type of information i a systematic way, using correlationbetween word sequences derived from a large corpus of text.In this model, if a word sequence A is positively and signifi-cantly correlated with another word sequence B, then (A ---~ B)is considered a "trigger pair", with A being the trigger and Bthe triggered sequence.
When A occurs in the document, ittriggers B, causing its probability estimate to be increased.In order for such a model to be effective, the following issueshave to be addressed:1.
How to filter all possible trigger pairs.
Even if we restrictour attention to pairs where A and B are both singlewords, the number of such pairs is too large.
Let V bethe size of the vocabulary.
Note that, unlike in a bigrammodel, where the number of different consecutive wordpairs is much less than V 2, the number of word pairswhere both words occurred in the same document is asignificant fraction of V 2.2.
How to combine vidence from multiple triggers.
This isa special case of the general problem of combining evi-dence from several sources.
We discuss several heuris-tics, and a plan for a more disciplined approach.3.
How to combine the triggering model with the staticmodel.We will discuss all 3 problems and our proposed solutions tothem.
This is ongoing research, and not all of our ideas havebeen tested yet.
A solution to (1) will be discussed in somedetail.
When combined with simple minded solutions to (2)and (3), it resulted in a perplexity reduction of between 10%and 32%, depending on the test set.
We are currently workingon implementing and testing some of the other solutions.3.2.
Filtering the Trigger-PairsLet "history" denote the part of the text already seen by thesystem.
Let A, B be any two word sequences.
Then the eventsB and Bo are defined as follows:B : B occurred in the history.Bo : B occurs next in the document.Let P(Bo) be the (unconditional) probability of Bo, and letP(Bo IA) be the conditional probability assigned to Bo by thetrigger pair (A ---~ B).
A natural measure of the informationprovided by A on Bo is the mutual information between thetwo:I(A :Bo)  = log n,n~lA) (6)(o)Note that, although mutual information is symmetric withregard to its arguments, it is generally not true that I(A : Bo) =l(g :At).Should mutual information be our figure of merit in selectingthe most promising trigger pairs?
I(A : Bo) measures theaverage number of bits we can save by considering A in pre-dictingBo.
But this savings will materialize onlyifBo is true,namely if we indeed encounter the word sequence B next inthe document.
Our best estimate of this, at the time filteringis carried out, is P(Bo IA).
We therefore define the expectedutility of the trigger pair (A ~ B):U(A ---~ B) d~f I(A : Bo)P(Bo IA) (7)and suggest i as a criterion for selecting trigger pairs.3.3.
Multiply-Triggered SequencesThe problem of combining evidence from multiple sources isa general, largely unsolved problem in modeling.
The idealsolution is to model explicitly each combination of valuesof the predictor variables, but this leads to an exponentialgrowth in the number of parameters, which renders the modeluntrainable.
At the other extreme, we can assume linearityand simply sum the contribution from the different sources.This may be a reasonable approximation in some models, butit is clearly inadequate in our case: "LOAN" is not 3 timesmore likely after 3 occurrences of "BANK" than it is afteronly 1 occurrence.Multiple triggers have several important functions:Increase the reliability of the prediction in the face ofunreliable history.
Since we usually rely on the speechrecognizer to provide us with the history, each word hasa nonnegligible chance of being erroneous.Disambiguate multiple-sense words.Compare:P("LOAN"o r'BANK")P("LOAN"o I"B ANK","FINANCIAL")P("LOAN"o I"BANK","RIVER")Intersect several broad semantic domains, and assign ahigher weight o the intersected region.i09Compare:P("PETE-ROSE"o I"BASEBALL")P("PETE-ROSE"o r'GAMBLING")P("PETE-ROSE".
r'BASEBAI~I:',"GAMBLING")We plan to model multiply triggered sequences in a way thatwill capture at least some of the above phenomena.
Thisrequires tatistical nalysis of the interaction among the trig-gers, especially as it relates to the triggered sequence.
Wehave just begun this analysis.
One possibility, suggested byKai-Fu Lee, is to consider the mutual information betweenthe triggers.
Triggers with high mutual information providelittle additional evidence, and thus should not be added up.For the system reported below, we considered several simpleheuristics: averaging the effect of the different triggers, usingthe most informative trigger only, and a quickly saturatingsum.
In the limited context of our current model we found nosignificant difference between the three.3.4.
Integration with the Static ModelA straightforward way to integrate the trigger model with astatic model is to interpolate hem linearly, using independentdata to determine the weights.
A somewhat fancier variantcould use weights that depend on the length of the history.
Weexpect the weight of the adaptive component toincrease as thehistory grows.
Using linear interpolation, the trigger modelcan be viewed as an adaptive unigram.
This is the solutionwe used in the system reported below.However, linear interpolation is not without its faults.
Ex-isting static models, such as N-grams, are excellent at usingshort-range information.
For our adaptive component to beuseful, it should complement the prediction power of the staticcomponent.
But linear interpolation means that the adaptivecomponent is blind to short-term constraints, yet the latterstrongly affect he behavior of the static model.
For example,in processing the sentence"The district attorney's office launched an investi-gation into loans made by several well connectedbanks.
""DISTRICT-ATtORNEY" may trigger "INVESTIGA-TION", causing its unigram probability to be raised toits level in documents containing the words "DISTRICT-ATrORNEY".
But when "INVESTIGATION" actually oc-curs, it is preceded by "LAUNCHED AN", which causes atrigram model to predict it with an even higher probability,rendering the adaptive contribution useless.Thus a better method of combining the two components is toconsider the information already provided by the static model.This can be done in two different ways:?
By using a POS-based trigger model, in the spirit of \[4\].?
By dynamically considering the probabilities producedby the static component, and modifying only those forwhich the adaptive component provided useful informa-tion.
We are now experimenting with this method.
Sinceit requires dynamic renormalization, it is only suitablefor recognizers which compute the entire array of prob-abilities for every word.3.5.
The ExperimentWe used most of the WSJ LM training corpus, 42M words inall, to train a conventional backoff trigram model\[l\] for theDARPA 20,000 closed-vocabulary task.
We used the samedata to derive the triggering list, as described below.The conditional probability provided by the trigger pair (AB) was estimated as:P(B, IA) =Count of B in documents containing Anumber of words in documents containing A(8)For the unconditional probability P(Bo) we used the staticunigram probability of B.
We have since switched to usingthe average probability with which occurrences of B in thetraining data re predicted by the trigram model, but the resultsreported here do not reflect his change.We first created an index of all but the 100 most frequentwords, keeping for each word a detailed escription of itsoccurrences.
We included paragraph, sentence, and wordlocation information, to allow consideration f different dis-tance measures and different context levels.
Excluding thetop 100 words reduced the storage requirements bymore than50%.
We assumed that frequently used words provide littlecontextual information.
Using the index, we systematicallysearched for ordered word pairs whose expected utility, asgiven by Eq.
7, exceeded a given threshold.
Of the 400 mil-lion possible pairs, we selected some 620,000.For combining multiple triggering of the same word, we usedMAX or AVERAGE or SUM saturating at 2*MAX, as de-scribed in section 3.3.
We found no significant differencebetween these methods.We combined the trigger model with the static trigram usinglinear interpolation.
The automatically derived weights variedfrom task to task, but were usually in the range of 0.02 to 0.06for the trigger component.
We also tried to use weights thatdepend on the length of the history, but were surprised to findno improvement.ii03.6.
Results and DiscussionWe tested our combined model on a large collection of testsets, using perplexity reduction as our measure.
A selectionis given in table 2.
Set WSJ-dev is the CSR developmenttest set (70K words).
Set BC-3 is the entire Brown Corpus,where the history was flushed arbitrarily every 3 sentences.Set BC-20 is the same as BC-3, but with history-flushingevery 20 sentences.
Set RM is the 39K words used in trainingthe Resource Management system, with no history flushing.The last result in table 2 was derived by training the trigramon only 1.2M words of WSJ data, and testing on the WSJdevelopment set.
This was done to facilitate amore equitablecomparison with the results reported in \[5\].test set static PP dynamic PP improvementWSJ-dev 170 153 10%BC-3 430 311 28%BC-20 430 293 32%RM 987 : 116 88%WSJ/1.2M-dev 350 : 295 16%Table 2: Perplexity reduction by the trigger-based adaptivemodel for several test setsOur biggest surprise was that "self triggering" (trigger pairs ofthe form (A ~ A)) was found to play a larger ole than wouldbe indicated by our utility measure.
Correlations of this typeare an important special case, and are already captured bythe conventional cache based models.
We decided to adaptour model in the face of reality, and maintained a separateself-triggering model that was added as a third interpolationcomponent ( he results in table 2 already reflect his change).This independent component, although consisting of far fewertrigger pairs, was responsible for as much as half of the overallperplexity reduction.
On tasks with a vastly different unigrambehavior, such as the Resource Management data set, the self-triggering component accounted for most of the improvement.Why do self-triggering pairs have a higher impact han an-ticipated?
One reason could be an inadequacy in our utilitymeasure.
Another could spring from the difference betweentraining and testing.
If the test set were statistically identicalto the training set, the utility of every trigger pair would beexactly as predicted by our expected utility measure.
Since inreality the training and testing sets differ, the actual utility islower than predicted.
All trigger pairs suffer a degradation,except for the self-triggering ones.
The latter hold their ownbecause self correlations are robust and are better maintainedacross different corpora.
This explains why the self-triggeringcomponent is most dominant when the statistical differencebetween the training and testing data is greatest.4.
SUMMARY AND CONCLUSIONSWe presented two attempts to improve our stochastic lan-guage modeling.
In the first, we identified a deficiency inthe conventional backoff language model, and used statisticalreasoning to correct i .
Our modified model is about as simpleas the original one, but gives a slightly lower perplexity onvarious tasks.
Our analysis uggests that the modification ismost suitable when training data is sparse.In our second attempt, we extended the notion of adaptationto incorporate within-document word sequence correlation,using the framework of a trigger pair.
We discussed the issuesinvolvedin constructing such a model, and reported promisingimprovements in perplexity.
We have only begun to explorethe potential of trigger-based adaptive models.
The resultsreported here are preliminary.
We believe we can improve ourperformance by implementing many of the ideas suggested insections 3.2, 3.3 and 3.4 above.
Work is already under way.5.
ACKNOWLEDGEMENTSWe are grateful to Doug Paul for providing us with the prepro-cessed CSR language training data in a timely manner; to DanJulin for much help in systems i sues; to Kai-Fu Lee for help-ful discussions; to Fil Alleva for many helpful interactions;and to Raj Reddy for support and encouragement.References1.
Katz, S. M., "Estimation of Probabilities from Sparse Datafor the Language Model Component ofa Speech Recognizer,"IEEE Trans.Acoust., Speech, SignaI Processing, voL ASSP-35,pp.
400-401, March 1987.2.
Jelinek, F., "Self-Organized Language Modeling for SpeechRecognition," inReadings in Speech Recognition, Alex Waibeland Kai-Fu Lee (Eds.
), Morgan Kaufmann, 1989.3.
Kupiec, J., "Probabilistic Models of Short and Long DistanceWord Dependencies in Running Text," Proceedings of theSpeech and Natural LanguageDARPA Workshop, p.290--295,Feb.
1989.4.
Kuhn, R., andDe Mori, R., "A Cache-BasedNatural L nguageModel for Speech Recognition," IEEE Trans.
PatternAnalysisand Machine Intelligence, vol.
PAMI-12, pp.
570-583, June1990.5.
Jelinek, E, Medaldo, B., Roukos, S., and Strauss, M., "ADynamic Language Model for Speech Recognition," Proceed-ings of the Speech and Natural Language DARPA Workshop,pp.293-295, Feb. 1991.6.
Essen, U., andNey, H., "Statistical LanguageModeUing Usinga Cache Memory," Proceedings of the First Quantitative Lin-guistics Conference, University of Trier, Germany.
September1991.i i i
