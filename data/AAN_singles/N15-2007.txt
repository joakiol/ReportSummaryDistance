Proceedings of NAACL-HLT 2015 Student Research Workshop (SRW), pages 48?56,Denver, Colorado, June 1, 2015.c?2015 Association for Computational LinguisticsEntity/Event-Level Sentiment Detection and InferenceLingjia DengIntelligent Systems ProgramUniversity of Pittsburghlid29@pitt.eduAbstractMost of the work in sentiment analysis andopinion mining focuses on extracting explicitsentiments.
Opinions may be expressed im-plicitly via inference rules over explicit senti-ments.
In this thesis, we incorporate the in-ference rules as constraints in joint predictionmodels, to develop an entity/event-level sen-timent analysis system which aims at detect-ing both explicit and implicit sentiments ex-pressed among entities and events in the text,especially focusing on but not limited to sen-timents toward events that positively or nega-tively affect entities (+/-effect events).1 IntroductionNowadays there is an increasing number of opin-ions expressed online in various genres, includingreviews, newswire, editorial, blogs, etc.
To fullyunderstand and utilize the opinions, much workin sentiment analysis and opinion mining focuseson more-fined grained levels rather than document-level (Pang et al, 2002; Turney, 2002), includingsentence-level (Yu and Hatzivassiloglou, 2003; Mc-Donald et al, 2007), phrase-level (Choi and Cardie,2008), aspect-level (Hu and Liu, 2004; Titov andMcDonald, 2008), etc.
Different from them, thisworks contributes to the sentiment analysis at theentity/event-level.
A system that could recognizesentiments toward entities and events would be valu-able in an application such as Automatic QuestionAnswering, to support answering questions such as?Who is negative/positive toward X??
(Stoyanov etal., 2005).
It could also be used to facilitate the en-tity and event resolution (e.g.
wikification system(Ratinov et al, 2011)).
A recent NIST evaluation ?The Knowledge Base Population (KBP) Sentimenttrack1?
aims at using corpora to collect informa-tion regarding sentiments expressed toward or bynamed entities.
We will compare the entity/event-level sentiment analysis task to other fine-grainedlevel sentiment analysis tasks in Section 2, and pro-pose to annotate a new entity/event-level sentimentcorpus in Section 3.The ultimate goal of this proposal is to develop anentity/event-level sentiment analysis system whichaims at detecting both explicit and implicit senti-ments expressed among entities and events in thetext.
Previous work in sentiment analysis mainly fo-cuses on detecting explicit opinions (Wiebe et al,2005; Johansson and Moschitti, 2013; Yang andCardie, 2013).
But not all the opinions are expressedin a straight forward way (i.e.
explicitly).
Considerthe example below.EX(1) It is great that the bill was defeated.There is a positive sentiment, great, explicitly ex-pressed.
It is toward the clause the bill was defeated.In other words, the writer is explicitly positive to-ward the event defeating bill.
Previous work maystop here.
However, it is indicated in the sentencethat the writer is negative toward the bill because(s)he is happy to see that the bill was defeated.
Thenegative sentiment is implicit.
Compared to detect-ing the explicit sentiment, it requires inference torecognize the implicit sentiment.1http://www.nist.gov/tac/2014/KBP/Sentiment/index.html48Now consider example Ex(2).EX(2) It is great that the bill was passed.In Ex(2), the writer?s sentiment toward the bill ispositive, because (s)he is happy to see that the billwas passed.
The writer is positive toward the eventsin both Ex(1) and Ex(2).
But different events leadto different sentiments toward the bill.
The defeatevent is harmful to the bill, while the pass event isbeneficial to the bill.
We call such events are named+/-effect events (Deng et al, 2013)2.
Many implicitsentiments are expressed via the +/-effect events, aswe have seen in Ex(1) and Ex(2).
Previously wehave developed rules to infer the sentiments toward+/-effect events (Deng and Wiebe, 2014).
An intro-duction of the rules will be given in Section 4.This proposal aims at embedding the inferencerules and incorporating +/-effect event informationinto a computational framework, in order to detectand infer both explicit and implicit entity/event-levelsentiments.
An overview of this proposed work willbe presented in Section 5.
Later, we will discuss themethods we propose to extract explicit entity/event-level sentiment in Section 6, and talk about how toincorporate the rules to jointly infer implicit senti-ments and disambiguate the ambiguities in each stepin Section 7.
The contributions of this thesis pro-posal are summarized in Section 8.2 Related WorkSentiment Corpus.
Annotated corpora of reviews(e.g., (Hu and Liu, 2004; Titov and McDonald,2008)), widely used in NLP, often include target an-notations.
Such targets are often aspects or featuresof products or services, which can be seen as entitiesor events that are related to the product.
However,the set of aspect terms is usually a pre-defined andclosed set.
(As stated in SemEval-2014: ?we anno-tate only aspect terms naming particular aspects?.
)For an event in newsire (e.g.
a terrorist attack), itis difficult to define a closed set of aspects.
Re-cently, to create the Sentiment Treebank (Socher etal., 2013), researchers crowdsourced annotations ofmovie review data and then overlaid the annotations2It was initially named as goodFor/badFor event (Deng etal., 2013; Deng and Wiebe, 2014).
Later we renamed it as +/-effect event (Deng et al, 2014; Choi and Wiebe, 2014).onto syntax trees.
Thus, the targets are not limited toaspects of products/services.
However, turkers wereasked to annotate small and then increasingly largersegments of the sentence.
Thus, all the informationof the sentence is not shown to turkers when theyannotate the span.
Moreover, in both corpora of re-views and Sentiment Treebank, the sources are lim-ited to the writer.+/-Effect Event.
Some work have mined varioussyntactic patterns (Choi and Cardie, 2008), proposedlinguistic templates (Zhang and Liu, 2011; Anandand Reschke, 2010; Reschke and Anand, 2011) tofind events similar to +/-effect events.
There hasbeen work generating a lexicon of patient polarityverbs (Goyal et al, 2012).
We define that a +effectevent has positive effect on the theme (e.g.
pass,save, help), while a -effect event has negative effecton the theme (e.g.
defeat, kill, prevent) (Deng etal., 2013).
A +/-effect event has four components:the agent, the +/-effect event, the polarity, and thetheme.
Later, Choi and Wiebe (2014) have devel-oped sense-level +/-effect event lexicons.Sentiment Analysis.
Most work in sentimentanalysis focuses on classifying explicit sentimentsand extracting explicit opinion expressions, sourcesand targets (Wiebe et al, 2005; Wiegand andKlakow, 2012; Johansson and Moschitti, 2013; Yangand Cardie, 2013).
There is some work investigat-ing features that directly indicate implicit sentiments(Zhang and Liu, 2011; Feng et al, 2013).
In con-trast, to bridge between explicit and implicit senti-ments via inference, we have defined a generalizedset of inference rules and proposed a graph-basedmodel to achieve sentiment propagation between thesentiments toward the agents and themes of +/-effectevents (Deng and Wiebe, 2014).
But it requires eachcomponent of an +/-effect event from manual anno-tations as input.
Later we use an Integer Linear Pro-gramming framework to reduce the need of manualannotations in the same task (Deng et al, 2014).3 Corpus of Entity/Event-Level Sentiment:MPQA 3.0The MPQA 2.0 (Wiebe et al, 2005; Wilson, 2007)is a widely-used, rich opinion resource.
It includeseditorials, reviews, news reports, and scripts of in-terviews from different news agencies, and covers49a wide range of topics3.
The MPQA annotationsconsist of private states, states of a source hold-ing an attitude, optionally toward a target.
Sincewe focus on sentiments, we only consider the atti-tudes which types are sentiments4.
MPQA 2.0 alsocontains expressive subjective element (ESE) anno-tations, which pinpoint specific expressions used toexpress subjectivity (Wiebe et al, 2005).
We onlyconsider ESEs whose polarity is positive or negative(excluding those marked neutral).To create MPQA 3.0, we propose to add entity-target and event-target (eTarget) annotations to theMPQA 2.0 annotations.
An eTarget is an entityor event that is the target of an opinion (identi-fied in MPQA 2.0 by a sentiment attitude or pos-itive/negative ESE span).
The eTarget annotationis anchored to the head word of the NP or VP thatrefers to the entity or event.Let?s consider some examples.
The annotationsin MPQA 2.0 are in the brackets, with the subscriptindicating the annotation type.
The eTargets we addin MPQA 3.0 are boldfaced.Ex(3) When the Imam [issued the fatwaagainst]sentiment[Salman Rushdie for in-sulting the Prophet]target...In Ex(3), Imam has a negative sentiment (issuedthe fatwa against) toward the target span, SalmanRushdie for insulting the Prophet, as annotated inMPQA 2.0.
We find two eTargets in the target span:Rushdie himself and his act of insulting.
Though theProphet is another entity in the target span, we don?tmark it because it is not negative.
This shows thatwithin a target span, the sentiments toward differ-ent entities may be different.
Thus it is necessary tomanually annotate the eTargets of a particular senti-ment or ESE.In the following example, the target span is short.Ex(4) [He]targetis therefore [planning totrigger wars]sentiment...He is George W. Bush; this article appeared in theearly 2000s.
The writer is negative toward Bush be-cause (the writer claims) he is planning to trigger3Available at http://mpqa.cs.pitt.edu4The other types of attitudes include belief, arguing, etc.wars.
As shown in the example, the MPQA 2.0 tar-get span is only He, for which we do create an eTar-get.
But there are three additional eTargets, whichare not included in the target span.
The writer isnegative toward Bush planning to trigger wars; weinfer that the writer is negative toward the idea oftriggering wars and thus toward war itself.We carried out an agreement study to show thefeasibility of this annotation task (Deng and Wiebe,2015).
Two annotators together annotated four doc-uments, including 292 eTargets in total.
To evalu-ate the results, the same agreement measure is usedfor both attitude and ESE eTargets.
Given an atti-tude or ESE, let set A be the set of eTargets an-notated by annotator X , and set B be the set ofeTargets annotated by annotator Y .
Following (Wil-son and Wiebe, 2003; Johansson and Moschitti,2013), which treat each set A and B in turn as thegold-standard, we calculate the average F-measureagr(A,B) = (|A ?B|/|B|+ |A ?B|/|A|)/2.
Theagr(A,B) is 0.82 on average over the four docu-ments, showing that this annotation task is feasible.In the future we will continue annotating the MPQAcorpus.We believe that the corpus will be a valuable newresource for developing entity/event-level sentimentanalysis systems and facilitating other NLP applica-tions in the future.4 Inference RulesPreviously we have proposed rules to infer senti-ments toward +/-effect events and the components(Deng and Wiebe, 2014).
The rule used to infer sen-timents in Ex(1) in Section 1 is listed below.writer positive (E2-effect E3)?writer positive E2& writer negative E3The rule above can be explained as: the writeris positive toward the defeating event (-effect) withthe agent (E2) being implicit and the bill (E3) be-ing the theme, so that the writer is negative towardthe bill.
However, these rules are limited to senti-ments toward the particular type of event, +/-effectevents.
Later we develop more rules to infer senti-ments toward all types of entities and events (Wiebeand Deng, 2014).
One of the rules and an examplesentence is:50Figure 1: Overview of Subtasks.E1positive (E2positive E3)?E1positive E2& E1positive E3Ex(5) Great!
Mike praised my project!The rule above can be explained as: if Mike (E2)is positive toward project (E3), and the speaker (E1)is positive about that positive sentiment, then wecould infer: (1) the speaker is positive toward Mike,because the speaker is glad that Mike holds the senti-ment, implying that the two entities agree with eachother.
(2) Because the speaker agrees with Mike, thespeaker is positive toward project.5 OverviewThe ultimate goal of this proposed work is to utilizethe +/-effect events information and inference rulesto improve detecting entity/event-level sentiments inthe documents.
There are ambiguities in each step ofthe whole task.
We decompose this task into severalsubtasks, as shown in Figure 1.
In this section, weillustrate what are the ambiguities in each subtask.
(1) The region in the blue circle in Figure 1 repre-sents the +/-effect events and the components to beidentified.
The ambiguities come from: (1.1) Whichspans are +/-effect events?
(1.2) Which NPs are theagents, which are the themes?
(1.3) What is the po-larity of the +/-effect event?
(1.4) Is the polarity re-versed (e.g.
negated)?
(2) The region in the red circle represents senti-ments we need to extract from the document.
Theambiguities are: (2.1) Is there any explicit senti-ment?
(2.2) What are the sources, targets and polari-ties of the explicit sentiments?
(2.3) Is there any im-plicit sentiment inferred?
(2.4) What are the sources,targets and polarities of the implicit sentiments?
(3) The region in the green circle represents alltypes of subjectivities of the writer, including sen-timents, beliefs and arguing .
The ambiguities aresimilar to those in the red circle: (3.1) Is there anysubjectivity of the writer?
(3.2) What are the targetsand polarities of the subjectivity?Though there are many ambiguities, they areinterdependent.
Inference rules in Section 4 de-fine dependencies among these ambiguities.
Our pi-lot study identifies and infers the writer?s sentimentstoward +/-effect events and the components (Denget al, 2014).
We first develop local classifiers us-ing traditional methods to generate the candidates ofeach ambiguity.
Each candidate is defined as a vari-able in an Integer Linear Programming (ILP) frame-work and four inference rules are incorporated asconstraints in the framework.
The pilot study cor-responds to the intersection of the three regions inFigure 1.
The success of it encourages us to extendfrom the intersection to all the regions with solidlines pointed to: the sources of sentiments are notlimited to only the writer but all entities , and thetargets of sentiments are not only the +/-effect eventsand the components, but all the entities and events.The pilot study used a simplified version of the set ofrules in (Wiebe and Deng, 2014).
In this proposal,we will use the full set.In summary, this proposal focuses on (a) extract-ing +/-effect events and the components, and (b) ex-tracting explicit and implicit sentiments.
For subtask(a), we propose to utilize the +/-effect event lexicon(Choi and Wiebe, 2014) and semantic role labelingtools to generate candidates of each ambiguity.
Forsubtask (b), we will discuss how to extract explicitsentiments in the next section.
Finally, we will dis-cuss how to simultaneously infer implicit sentimentsand disambiguate the ambiguities listed above in ajoint model in Section 7.Gold Standard.
The MPQA 3.0 proposed in Sec-tion 3 and the KBP sentiment dataset will be used asgold standard in this thesis.Note that, although the two regions with dashedlines pointed to are out of scope in this proposal, wecan adopt the framework in this proposal to jointlyanalyze sentiments and beliefs in the future.516 Explicit Entity/Event-Level SentimentTo fully utilize the off-the-shelf resources and toolsin the span-level and phrase-level sentiment analysis(Wiegand and Klakow, 2012; Johansson and Mos-chitti, 2013; Yang and Cardie, 2013; Socher et al,2013; Yang and Cardie, 2014), we will use the opin-ion spans and source spans extracted by previouswork.
To extract eTargets, which are newly anno-tated in the MPQA 3.0 corpus, we propose to modelthis subtask as a classification problem: Given anextracted opinion span returned by the resources, adiscriminative classifier judges whether a head ofNP/VP in the same sentence is the correct eTargetof the extracted opinion.
Two sets of features willbe considered.Opinion Span Features.
Several common fea-tures used to extract targets will be used, includingPart-Of-Speech, path in the dependency parse graph,distance of the constituents on the parse tree, etc(Yang and Cardie, 2013; Yang and Cardie, 2014).Target Span Features.
Among the off-the-shelfsystems and resources, some work extracts the tar-get spans in addition to the opinions.
We will in-vestigate features depicting the relations between aNP/VP head and the extracted target spans, suchas whether the head overlaps with the target span.However, some off-the-shelf systems only extractthe opinion spans, but do not extract any target span.For a NP/VP head, if the target span feature is false,there may be two reasons: (1) There is a target spanextracted, but the target span feature is false (e.g.
thehead doesn?t overlap with the target span).
(2) Thereis no target span extracted by any tool at all.Due to this fact, we propose three ways to de-fine target span features.
The simplest method (M1)is to assign zero to a false target span feature, re-gardless of the reason.
A similar method (M2) is toassign different values (e.g.
0 or -1) to a false tar-get span feature, according to the reason that causesthe feature being false.
For the third method (M3),we propose the Max-margin SVM (Chechik et al,2008).
Unlike the case where a feature exists butits value is not observed or false, here this modelfocus on the case where a feature may not evenexist (structurally absent) for some of the samples(Chechik et al, 2008).
In other words, the Max-margin SVM deals with features that are known tobe non-existing, rather than have an unknown value.This allows us to fully utilize the different structuresof outputs from different state-of-the-art resources.7 Implicit Entity/Event-Level SentimentThe explicit sentiments extracted from Section 6above are treated as input for inferring the implicitsentiment.
We are pursing such a joint predictionmodel that combines the probabilistic calculation ofmany ambiguities under the constraints of the de-pendencies of the data, defined by inference rules inthe first order logic.
Every candidate of every ambi-guity is represented as a variable in the joint model.The goal is to find an optimal configuration of allthe variables, thus the ambiguities are solved.
Mod-els differ in the way constraints are expressed.
Weplan to mainly investigate undirected lifted graphi-cal models, including Markov Logic Network, andProbabilistic Soft Logics.Though our pilot study (Deng et al, 2014) andmany previous work in various applications of NLP(Roth and Yih, 2004; Punyakanok et al, 2008; Choiet al, 2006; Martins and Smith, 2009; Somasun-daran and Wiebe, 2009) have used Integer LinearProgramming (ILP) as a joint model, by settingthe dependencies as constraints in the ILP frame-work, there is one limitation of ILP: we have tomanually translate the first order logic rules intothe linear equations and inequations as constraints.Now we have more complicated rules.
In order tochoose a framework that computes the first orderlogic directly, we propose the Markov Logic Net-work (MLN) (Richardson and Domingos, 2006).The MLN is a framework for probabilistic logicthat employ weighted formulas in first order logic tocompactly encode complex undirected probabilisticgraphical models (i.e., Markov networks) (Beltagyet al, 2014).
It has been applied to various NLPtasks to achieves good results (Poon and Domingos,2008; Fahrni and Strube, 2012; Dai et al, 2011;Kennington and Schlangen, 2012; Yoshikawa et al,2009; Song et al, 2012; Meza-Ruiz and Riedel,2009).
It consists of a set of first order logic formula,each associated with a weight.
The goal of the MLNis to find an optimal grounding which maximizes thevalues of all the satisfied first order logic formulain the knowledge base (Richardson and Domingos,522006).
We use the inference rules in Section 4 asthe set of first order logic formula in MLN, and de-fine atoms in the logic corresponding to our variouskinds of ambiguities.
Thus, solving the MLN is toassign true or false value to each atom, that is solv-ing the ambiguities at the same time.
For example,THEME(x,y) represents that the +/-effect event x hasa theme y, TARGET(x,y) represents that the senti-ment x has a target y, POS(s,x) represents that s ispositive toward x.
The inferences used in Ex(1) andEx(5) are shown in Table 1.It is great that the bill was defeated.
( THEME(x, y) ?
POLARITY(x, -effect) )?
( POS(s, x)?
NEG(s, y) )( THEME(defeat, bill) ?
POLARITY(defeat, -effect) )?
( POS(writer, defeat)?
NEG(writer, bill) )Great!
Mike praised my project!
( TARGET(x, y) ?
POLARITY(x, positive) )?
( POS(s, x)?
POS(s, y) )( TARGET(praised, project) ?POLARITY(praised, positive) )?
( POS(speaker, praised)?
POS(speaker, project) )Table 1: Examples and Inference Rules.
In each box, line1: sentence.
Line 2: inference rule.
Line 3: presentingthe sentence in the rule.Though MLN is a good choice of our task, it hasa limitation.
Each atom in the first order formulain MLN is boolean value.
However, as we statedabove, each atom represents an candidate of ambi-guity returned by local classifiers, which may be nu-merical value.
We can manually set thresholds forthe numerical values to be boolean values, or train aregression over different atoms to select thresholds,but both methods need more parameters and maylead to over-fitting.
Therefore, we propose anothermethod, Probabilistic Soft Logic (PSL) (Broecheleret al, 2010).
PSL is a new model of statistical rela-tion learning and has been quickly applied to solvemany NLP and other machine learning tasks in re-cent years (Beltagy et al, 2014; London et al, 2013;Pujara et al, 2013; Bach et al, 2013; Huang et al,2013; Memory et al, 2012; Beltagy et al, 2013).
In-stead of only being boolean value, the atom in PSLcould have numerical values.
Given the atoms be-ing numerical, PSL uses the Lukasiewicz t-norm andits corresponding co-norm to quantify the degree towhich a grounding of the logic formula is satisfied(Kimmig et al, 2014).Not limited to the lifted graphical models pro-posed above, other graphical models are attractiveto explore.
The Latent Dirichelet Allocation (LDA)(Blei et al, 2003), is widely used in sentiment anal-ysis (Titov and McDonald, 2008; Si et al, 2013; Linand He, 2009; Li et al, 2010).
Li et al (2010) pro-posed a LDA model assuming that sentiments de-pend on each other, which is similar to our assump-tion that the implicit sentiments depend on explicitsentiment by the inference rules.
There is work com-bining LDA and PSL together (Ramesh et al, 2014),which may be another exploration for us.8 ContributionsThe proposed thesis mainly contributes to sentimentanalysis and opinion mining in various genres suchas newswire, blogs, editorials, etc.?
Develop MPQA 3.0, an entity/event-level sen-timent corpus.
It will be a valuable new re-source for developing entity/event-level senti-ment analysis systems, which are useful forvarious NLP applications including opinion-oriented Question Answering systems, wikifi-cation systems, etc.?
Propose a classification model to extract ex-plicit entity/event-level sentiments.
Differentfrom previous classifications in sentiment anal-ysis, we propose to distinguish opinion spanfeatures, which are applicable to all the datasamples, and target span features, which maybe structure absent for some samples (i.e.
fea-tures do not exist at all).?
Propose a joint prediction framework aimsat utilizing the +/-effect events informationand inference rules to improve detectingentity/event-level sentiments in the documentsand disambiguate the followed ambiguities ineach step simultaneously.Acknowledgement.
Thank my advisor Dr. JanyceWiebe for her very helpful suggestions in this thesisproposal.
Thank the anonymous reviewers for theiruseful comments.53ReferencesPranav Anand and Kevin Reschke.
2010.
Verb classes asevaluativity functor classes.
In Interdisciplinary Work-shop on Verbs.
The Identification and Representationof Verb Features.Stephen H Bach, Bert Huang, and Lise Getoor.
2013.Learning latent groups with hinge-loss markov randomfields.
In Inferning: ICML Workshop on Interactionsbetween Inference and Learning.Islam Beltagy, Cuong Chau, Gemma Boleda, Dan Gar-rette, Katrin Erk, and Raymond Mooney.
2013.
Mon-tague meets markov: Deep semantics with probabilis-tic logical form.
In 2nd Joint Conference on Lexi-cal and Computational Semantics: Proceeding of theMain Conference and the Shared Task, Atlanta, pages11?21.
Citeseer.Islam Beltagy, Katrin Erk, and Raymond Mooney.
2014.Probabilistic soft logic for semantic textual similar-ity.
In Proceedings of the 52nd Annual Meeting ofthe Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 1210?1219, Baltimore,Maryland, June.
Association for Computational Lin-guistics.David M Blei, Andrew Y Ng, and Michael I Jordan.2003.
Latent dirichlet alocation.
the Journal of ma-chine Learning research, 3:993?1022.Matthias Broecheler, Lilyana Mihalkova, and LiseGetoor.
2010.
Probabilistic similarity logic.
In Un-certainty in Artificial Intelligence (UAI).Gal Chechik, Geremy Heitz, Gal Elidan, Pieter Abbeel,and Daphne Koller.
2008.
Max-margin classificationof data with absent features.
The Journal of MachineLearning Research, 9:1?21.Yejin Choi and Claire Cardie.
2008.
Learning with com-positional semantics as structural inference for subsen-tential sentiment analysis.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing, pages 793?801.
Association for Compu-tational Linguistics.Yoonjung Choi and Janyce Wiebe.
2014.
+/-effectwordnet: Sense-level lexicon acquisition foropinion inference.
In Proceedings of the 2014 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP), pages 1181?1191, Doha, Qatar,October.
Association for Computational Linguistics.Yejin Choi, Eric Breck, and Claire Cardie.
2006.
Jointextraction of entities and relations for opinion recog-nition.
In Proceedings of the 2006 Conference onEmpirical Methods in Natural Language Processing,EMNLP ?06, pages 431?439, Stroudsburg, PA, USA.Association for Computational Linguistics.Hong-Jie Dai, Richard Tzong-Han Tsai, Wen-Lian Hsu,et al 2011.
Entity disambiguation using a markov-logic network.
In IJCNLP, pages 846?855.
Citeseer.Lingjia Deng and Janyce Wiebe.
2014.
Sentiment prop-agation via implicature constraints.
In Proceedingsof the 14th Conference of the European Chapter ofthe Association for Computational Linguistics, pages377?385, Gothenburg, Sweden, April.
Association forComputational Linguistics.Lingjia Deng and Janyce Wiebe.
2015.
Mpqa 3.0: Anentity/event-level sentiment corpus.
In Conferenceof the North American Chapter of the Association ofComputational Linguistics: Human Language Tech-nologies.Lingjia Deng, Yoonjung Choi, and Janyce Wiebe.
2013.Benefactive/malefactive event and writer attitude an-notation.
In ACL 2013 (short paper).
Association forComputational Linguistics.Lingjia Deng, Janyce Wiebe, and Yoonjung Choi.
2014.Joint inference and disambiguation of implicit senti-ments via implicature constraints.
In Proceedings ofCOLING 2014, the 25th International Conference onComputational Linguistics: Technical Papers, pages79?88, Dublin, Ireland, August.
Dublin City Univer-sity and Association for Computational Linguistics.Angela Fahrni and Michael Strube.
2012.
Jointly dis-ambiguating and clustering concepts and entities withMarkov logic.
In Proceedings of COLING 2012,pages 815?832, Mumbai, India, December.
The COL-ING 2012 Organizing Committee.Song Feng, Jun Sak Kang, Polina Kuznetsova, and YejinChoi.
2013.
Connotation lexicon: A dash of senti-ment beneath the surface meaning.
In Proceedings ofthe 51th Annual Meeting of the Association for Com-putational Linguistics (Volume 2: Short Papers), Sofia,Bulgaria, Angust.
Association for Computational Lin-guistics.Amit Goyal, Ellen Riloff, and Hal Daum III.
2012.
Acomputational model for plot units.
Computational In-telligence, pages 466?488.Minqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of the tenthACM SIGKDD international conference on Knowl-edge discovery and data mining, pages 168?177.ACM.Bert Huang, Angelika Kimmig, Lise Getoor, and Jen-nifer Golbeck.
2013.
A flexible framework for prob-abilistic models of social trust.
In Social Computing,Behavioral-Cultural Modeling and Prediction, pages265?273.
Springer.Richard Johansson and Alessandro Moschitti.
2013.Relational features in fine-grained opinion analysis.Computational Linguistics, 39(3):473?509.Casey Kennington and David Schlangen.
2012.
Markovlogic networks for situated incremental natural lan-guage understanding.
In Proceedings of the 13th An-54nual Meeting of the Special Interest Group on Dis-course and Dialogue, pages 314?323.
Association forComputational Linguistics.Angelika Kimmig, Lilyana Mihalkova, and Lise Getoor.2014.
Lifted graphical models: a survey.
MachineLearning, pages 1?45.Fangtao Li, Minlie Huang, and Xiaoyan Zhu.
2010.
Sen-timent analysis with global topics and local depen-dency.
In AAAI.Chenghua Lin and Yulan He.
2009.
Joint sentiment/topicmodel for sentiment analysis.
In Proceedings of the18th ACM conference on Information and knowledgemanagement, pages 375?384.
ACM.Ben London, Sameh Khamis, Stephen H. Bach, BertHuang, Lise Getoor, and Larry Davis.
2013.
Col-lective activity detection using hinge-loss Markov ran-dom fields.
In CVPR Workshop on Structured Predic-tion: Tractability, Learning and Inference.Andr?e F. T. Martins and Noah a. Smith.
2009.
Summa-rization with a joint model for sentence extraction andcompression.
In Proceedings of the Workshop on In-teger Linear Programming for Natural Langauge Pro-cessing - ILP ?09, pages 1?9, Morristown, NJ, USA.Association for Computational Linguistics.Ryan McDonald, Kerry Hannan, Tyler Neylon, MikeWells, and Jeff Reynar.
2007.
Structured mod-els for fine-to-coarse sentiment analysis.
In AnnualMeeting-Association For Computational Linguistics,volume 45, page 432.
Citeseer.Alex Memory, Angelika Kimmig, Stephen Bach, LouiqaRaschid, and Lise Getoor.
2012.
Graph summariza-tion in annotated data using probabilistic soft logic.In Proceedings of the 8th International Workshop onUncertainty Reasoning for the Semantic Web (URSW2012), volume 900, pages 75?86.Ivan Meza-Ruiz and Sebastian Riedel.
2009.
Multilin-gual semantic role labelling with markov logic.
InProceedings of the Thirteenth Conference on Com-putational Natural Language Learning: Shared Task,pages 85?90.
Association for Computational Linguis-tics.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
: sentiment classification using ma-chine learning techniques.
In Proceedings of the ACL-02 conference on Empirical methods in natural lan-guage processing-Volume 10, pages 79?86.
Associa-tion for Computational Linguistics.Hoifung Poon and Pedro Domingos.
2008.
Joint un-supervised coreference resolution with Markov Logic.In Proceedings of the 2008 Conference on EmpiricalMethods in Natural Language Processing, pages 650?659, Honolulu, Hawaii, October.
Association for Com-putational Linguistics.Jay Pujara, Hui Miao, Lise Getoor, and William Cohen.2013.
Knowledge graph identification.
In The Seman-tic Web?ISWC 2013, pages 542?557.
Springer.Vasin Punyakanok, Dan Roth, and Wen-tau Yih.
2008.The importance of syntactic parsing and inference insemantic role labeling.
Computational Linguistics,34(2):257?287.Arti Ramesh, Dan Goldwasser, Bert Huang, HalDaume III, and Lise Getoor.
2014.
Understandingmooc discussion forums using seeded lda.
In 9th ACLWorkshop on Innovative Use of NLP for Building Ed-ucational Applications.
ACL.Lev Ratinov, Dan Roth, Doug Downey, and Mike An-derson.
2011.
Local and global algorithms for dis-ambiguation to wikipedia.
In Proceedings of the 49thAnnual Meeting of the Association for ComputationalLinguistics: Human Language Technologies-Volume1, pages 1375?1384.
Association for ComputationalLinguistics.Kevin Reschke and Pranav Anand.
2011.
Extractingcontextual evaluativity.
In Proceedings of the Ninth In-ternational Conference on Computational Semantics,IWCS ?11, pages 370?374, Stroudsburg, PA, USA.Association for Computational Linguistics.Matthew Richardson and Pedro Domingos.
2006.Markov logic networks.
Machine learning, 62(1-2):107?136.Dan Roth and Wen-tau Yih.
2004.
A linear programmingformulation for global inference in natural languagetasks.
In CONLL.Jianfeng Si, Arjun Mukherjee, Bing Liu, Qing Li, HuayiLi, and Xiaotie Deng.
2013.
Exploiting topic basedtwitter sentiment for stock prediction.
In ACL (2),pages 24?29.Richard Socher, Alex Perelygin, Jean Y Wu, JasonChuang, Christopher D Manning, Andrew Y Ng, andChristopher Potts.
2013.
Recursive deep models forsemantic compositionality over a sentiment treebank.In Proceedings of the Conference on Empirical Meth-ods in Natural Language Processing (EMNLP), pages1631?1642.
Citeseer.Swapna Somasundaran and Janyce Wiebe.
2009.
Rec-ognizing stances in online debates.
In Proceedingsof the Joint Conference of the 47th Annual Meetingof the ACL and the 4th International Joint Conferenceon Natural Language Processing of the AFNLP, pages226?234, Suntec, Singapore, August.
Association forComputational Linguistics.Yang Song, Jing Jiang, Wayne Xin Zhao, Sujian Li, andHoufeng Wang.
2012.
Joint learning for coreferenceresolution with markov logic.
In Proceedings of the2012 Joint Conference on Empirical Methods in Nat-ural Language Processing and Computational Natu-55ral Language Learning, pages 1245?1254.
Associa-tion for Computational Linguistics.Veselin Stoyanov, Claire Cardie, and Janyce Wiebe.2005.
Multi-Perspective Question Answering us-ing the OpQA corpus.
In Proceedings of the Hu-man Language Technologies Conference/Conferenceon Empirical Methods in Natural Language Process-ing (HLT/EMNLP-2005), pages 923?930, Vancouver,Canada.Ivan Titov and Ryan T McDonald.
2008.
A joint modelof text and aspect ratings for sentiment summarization.In ACL, volume 8, pages 308?316.
Citeseer.Peter D Turney.
2002.
Thumbs up or thumbs down?
:semantic orientation applied to unsupervised classifi-cation of reviews.
In Proceedings of the 40th annualmeeting on association for computational linguistics,pages 417?424.
Association for Computational Lin-guistics.Janyce Wiebe and Lingjia Deng.
2014.
An account ofopinion implicatures.
arXiv, 1404.6491[cs.CL].Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005.Annotating expressions of opinions and emotions inlanguage ann.
Language Resources and Evaluation,39(2/3):164?210.Michael Wiegand and Dietrich Klakow.
2012.
General-ization methods for in-domain and cross-domain opin-ion holder extraction.
In Proceedings of the 13th Con-ference of the European Chapter of the Association forComputational Linguistics, pages 325?335.
Associa-tion for Computational Linguistics.Theresa Wilson and Janyce Wiebe.
2003.
Annotatingopinions in the world press.
In Proceedings of the 4thACL SIGdial Workshop on Discourse and Dialogue(SIGdial-03), pages 13?22.Theresa Wilson.
2007.
Fine-grained Subjectivity andSentiment Analysis: Recognizing the Intensity, Polar-ity, and Attitudes of private states.
Ph.D. thesis, Intel-ligent Systems Program, University of Pittsburgh.Bishan Yang and Claire Cardie.
2013.
Joint inferencefor fine-grained opinion extraction.
In ACL (1), pages1640?1649.Bishan Yang and Claire Cardie.
2014.
Context-awarelearning for sentence-level sentiment analysis withposterior regularization.
In Proceedings of ACL.Katsumasa Yoshikawa, Sebastian Riedel, Masayuki Asa-hara, and Yuji Matsumoto.
2009.
Jointly identifyingtemporal relations with markov logic.
In Proceedingsof the Joint Conference of the 47th Annual Meeting ofthe ACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP: Volume1-Volume 1, pages 405?413.
Association for Compu-tational Linguistics.Hong Yu and Vasileios Hatzivassiloglou.
2003.
Towardsanswering opinion questions: Separating facts fromopinions and identifying the polarity of opinion sen-tences.
In Proceedings of the 2003 conference on Em-pirical methods in natural language processing, pages129?136.
Association for Computational Linguistics.Lei Zhang and Bing Liu.
2011.
Identifying noun prod-uct features that imply opinions.
In Proceedings of the49th Annual Meeting of the Association for Compu-tational Linguistics: Human Language Technologies,pages 575?580, Portland, Oregon, USA, June.
Associ-ation for Computational Linguistics.56
