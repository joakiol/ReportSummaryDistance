Transductive Pattern Learning for Information ExtractionBrian McLernon Nicholas KushmerickSchool of Computer Science and InformaticsUniversity College Dublin, Ireland{brian.mclernon,nick}@ucd.ieAbstractThe requirement for large labelled trainingcorpora is widely recognized as a key bot-tleneck in the use of learning algorithms forinformation extraction.
We present TPLEX,a semi-supervised learning algorithm for in-formation extraction that can acquire extrac-tion patterns from a small amount of labelledtext in conjunction with a large amount of un-labelled text.
Compared to previous work,TPLEX has two novel features.
First, the al-gorithm does not require redundancy in thefragments to be extracted, but only redundancyof the extraction patterns themselves.
Second,most bootstrapping methods identify the high-est quality fragments in the unlabelled data andthen assume that they are as reliable as man-ually labelled data in subsequent iterations.In contrast, TPLEX?s scoring mechanism pre-vents errors from snowballing by recordingthe reliability of fragments extracted from un-labelled data.
Our experiments with severalbenchmarks demonstrate that TPLEX is usu-ally competitive with various fully-supervisedalgorithms when very little labelled trainingdata is available.1 IntroductionInformation extraction is a form of shallow text anal-ysis that involves identifying domain-specific frag-ments within natural language text.
Most recent re-search has focused on learning algorithms that auto-matically acquire extraction patterns from manuallylabelled training data (e.g., (Riloff, 1993; Califf andMooney, 1999; Soderland, 1999; Freitag and Kushm-erick, 2000; Ciravegna, 2001; Finn and Kushmerick,2004)).
This training data takes the form of the originaltext, annotated with the fragments to be extracted.Due to the expense and tedious nature of this la-belling process, it is widely recognized that a key bot-tleneck in deploying such algorithms is the need to cre-ate a sufficiently large training corpus for each new do-main.
In response to this challenge, many researchershave investigated semi-supervised learning algorithmsthat learn from a (relatively small) set of labelled textsin conjunction with a (relatively large) set of unlabelledtexts (e.g., (Riloff, 1996; Brin, 1998; Yangarber et al,2002)).In this paper, we present TPLEX, a semi-supervisedalgorithm for learning information extraction patterns.The key idea is to exploit the following recursive defini-tions: good patterns extract good fragments, and goodfragments are extracted by good patterns.
To opera-tionalize this recursive definition, we initialize the pat-tern and fragment scores with labelled data, and theniterate until the scores have converged.Most prior semi-supervised approaches to informa-tion extraction assume that fragments are essentiallynamed entities, so that there will be many occurrencesof any given fragment.
For example, for the task ofdiscovering diseases (?influenza?, ?Ebola?, etc), prioralgorithms assume that each disease will be mentionedmany times, and that every occurrence of such a dis-ease in unlabelled text should be extracted.
However,it may not be the case that fragments to be extractedoccur more than once in the corpus, or that every oc-currence of a labelled fragment should be extracted.For example, in the well-known CMU Seminars cor-pus, any given person usually gives just one seminar,and a fragment such as ?3pm?
sometimes indicates thestart time, other occurrences indicate the end time, andsome occurrence should not be extracted at all.
Ratherthan relying on redundancy of the fragments, TPLEXexploits redundancy of the learned extraction patterns.TPLEX is a transductive algorithm (Vapnik, 1998), inthat the goal is to perform extraction from a given un-labelled corpus, given a labelled corpus.
This is in con-trast to the typical machine learning framework, wherethe goal is a set of extraction patterns (which can ofcourse then be applied to new unlabelled text).
As aside-effect, TPLEX does generate a set of extractionpatterns which may be a useful in their own right, de-pending on the application.We have compared TPLEX with various competitorson a variety of real-world extraction tasks.
We have ob-served that TPLEX?s performance matches or exceedsthese in several benchmark tasks.
The remainder of thispaper is organized as follows.
After describing relatedwork in more detail (Sec.
2), we describe the TPLEXalgorithm (Sec.
3).
We then discuss a series of exper-iments to compare TPLEX with various supervised al-25gorithms (Sec.
4).
We conclude with a summary of ob-servations made during the evaluation, and a discussionof future work (Sec.
5).2 Related workA review of machine learning for information extrac-tion is beyond the scope of this paper; see e.g.
(Cardie,1997; Kushmerick and Thomas, 2003).A number of researchers have previously developedbootstrapping or semi-supervised approaches to infor-mation extraction, named entity recognition, and re-lated tasks (Riloff, 1996; Brin, 1998; Riloff and Jones,1999; Agichtein et al, 2001; Yangarber et al, 2002;Stevenson and Greenwood, 2005; Etzioni et al, 2005).Several approaches for learning from both labeledand unlabeled data have been proposed (Yarowsky,1995; Blum and Mitchell, 1998; Collins and Singer,1999) where the unlabeled data is utilised to boost theperformance of the algorithm.
In (Collins and Singer,1999) Collins and Singer show that unlabeled data canbe used to reduce the level of supervision required fornamed entity classification.
However, their approachis reliant on the presence of redundancy in the namedentities to be identified.TPLEX is most closely related to the NOMEN algo-rithm (Yangarber et al, 2002).
NOMEN has a very sim-ple iterative structure: at each step, a very small num-ber of high-quality new fragments are extracted, whichare treated in the next step as equivalent to seeds fromthe labeled documents.
NOMEN has a number of pa-rameters which must be carefully tuned to ensure thatit does not over-generalise.
Erroneous additions to theset of trusted fragments can lead to a snowballing oferrors.Also, NOMEN uses a binary scoring mechanism,which works well in dense corpora with substantial re-dundancy.
However, many information extraction tasksfeature sparse corpora with little or no redundancy.
Wehave extended NOMEN by allowing it to make finer-grained (as opposed to binary) scoring decisions at eachiteration.
Instead of definitively assigning a position toa given field, we calculate the likelihood that it belongsto the field over multiple iterations.3 The TPLEX algorithmThe goal of the algorithm is to identify the membersof the target fields within unlabelled texts by generaliz-ing from seed examples in labelled training texts.
Weachieve this by generalizing boundary detecting pat-terns and scoring them with a recursive scoring func-tion.As shown in Fig.
1, TPLEX bootstraps the learningprocess from a seed set of labelled examples.
The ex-amples are used to populate initial pattern sets for eachtarget field, with patterns that match the start and endpositions of the seed fragments.
Each pattern is thengeneralised to produce more patterns, which are in turn      Figure 1: An overview of TPLEX.
A key idea of the al-gorithm is the following recursive scoring method: pat-tern scores are a function of the scores of the positionsthey extract, and position scores are a function of thescores of the patterns that extract them.applied to the corpus in order to identify more base pat-terns.
This process iterates until no more patterns canbe learned.TPLEX employs a recursive scoring metric in whichgood patterns reinforce good positions, and good posi-tions reinforce good patterns.
Specifically, we calculateconfidence scores for positions and patterns.
Our scor-ing mechanism calculates the score of a pattern as afunction of the scores of the positions that it matches,and the score of a position as a function of the scoresof the patterns that extract it.TPLEX is a multi-field extraction algorithm in that itextracts multiple fields simultaneously.
By doing this,information learned for one field can be used to con-strain patterns learned for others.
Specifically, our scor-ing mechanism ensures that if a pattern scores highlyfor one field, its score for all other fields is reduced.In the remainder of this section, we describe the al-gorithm by formalizing the space of learned patterns,and then describing TPLEX?s scoring mechanism.3.1 Boundary detection patternsTPLEX extracts fragments of text by identifying prob-able fragment start and end positions, which are thenassembled into complete fragments.
TPLEX?s patternsare therefore boundary detectors which identify oneend of a fragment or the other.
TPLEX learns patternsto identify the start and end of target occurrences inde-pendently of each other.
This strategy has previouslybeen employed successfully (Freitag and Kushmerick,2000; Ciravegna, 2001; Yangarber et al, 2002; Finnand Kushmerick, 2004).TPLEX?s boundary detectors are similar to thoselearned by BWI (Freitag and Kushmerick, 2000).
Aboundary detector has two parts, a left pattern and aright pattern.
Each of these patterns is a sequence oftokens, where each is either a literal or a generalizedtoken.
For example, the boundary detector26[will be <punc>][<caps> <fname>]would correctly find the start of a name in an utterancesuch as ?will be: Dr Robert Boyle?
and ?will be, San-dra Frederick?, but it will fail to identify the start of thename in ?will be Dr. Robert Boyle?.
The boundary de-tectors that find the beginnings of fragments are calledthe pre-patterns, and the detectors that find the ends offragments are called the post-patterns.3.2 Pattern generationAs input, TPLEX requires a set of tagged seed docu-ments for training, and an untagged corpus for learn-ing.
The seed documents are used to initialize thepre-pattern and post-pattern sets for each of the targetfields.
Within the seed documents each occurrence ofa fragment belonging to any of the target categories issurrounded by a set of special tags that denote the fieldto which it belongs.The algorithm parses the seed documents and iden-tifies the tagged fragments in each document.
It thengenerates patterns for the start and end positions ofeach fragment based on the surrounding tokens.Each pattern varies in length from 2 to n tokens.
Fora given pattern length `, the patterns can then over-lap the position by zero to ` tokens.
For example,a pre-pattern of length four with an overlap of onewill match the three tokens immediately preceeding thestart position of a fragment, and the one token immedi-ately following that position.
In this way, we generate?ni=2(i + 1) patterns from each seed position.
In ourexperiments, we set the maximum pattern length to ben = 4.TPLEX then grows these initial sets for each field bygeneralizing the initial patterns generated for each seedposition.
We employ eight different generalization to-kens when generalizing the literal tokens of the initialpatterns.
The wildcard token <*> matches every lit-eral.
The second type of generalization is <punc>,which matches punctuation such as commas and peri-ods.
Similarly, the token <caps>matches literals withan initial capital letter, <num> matches a sequence ofdigits, <alpha num> matches a literal consisting ofletters followed by digits, and <num alpha>matchesa literal consisting of digits followed by letters.
The fi-nal two generalisations are <fname> and <lname>,which match literals that appear in a list of first and lastnames (respectively) taken from US Census data.All patterns are then applied to the entire corpus, in-cluding the seed documents.
When a pattern matches anew position, the tokens at that position are convertedinto a maximally-specialized pattern, which is added tothe pattern set.
Patterns are repeatedly generalized un-til only one literal token remains.
This whole processiterates until no new patterns are discovered.
We donot generalize the new maximally-specialized patternsdiscovered in the unlabelled data.
This ensures that allpatterns are closely related to the seed data.
(We exper-imented with generalizing patterns from the unlabelleddata, but this rapidly leads to overgeneralization.
)The locations in the corpus where the patterns matchare regarded as potential target positions.
Pre-patternsindicate potential start positions for target fragmentswhile post-patterns indicate end positions.
When all ofthe patterns have been matched against the corpus, eachfield will have a corresponding set of potential start andend positions.3.3 Notation & problem statementPositions are denoted by r, and patterns are denotedby p. Formally, a pattern is equivalent to the set ofpositions that it extracts.The notation p ?
r indicatesthat pattern p matches position r. Fields are denoted byf , and F is the set of all fields.The labelled training data consists of a set of po-sitions R = {.
.
.
, r, .
.
.
}, and a labelling functionT : R ?
F ?
{X} for each such position.
T (r) = findicates that position r is labelled with field f in thetraining data.
T (r) = X means that r is not labelledin the training data (i.e.
r is a negative example for allfields).The unlabelled test data consists of an additional setof positions U .Given this notation, the learning task can be statedconcisely as follows: extend the domain of T to U ,i.e.
generalize from T (r) for r ?
R, to T (r) for r ?
U .3.4 Pattern and position scoringWhen the patterns and positions for the fields have beenidentified we must score them.
Below we will de-scribe in detail the recursive manner in which we definescoref (r) in terms of scoref (p), and vice versa.
Giventhat definition, we want to find fixed-point values forscoref (p) and scoref (r).
To achieve this, we initializethe scores, and then iterate through the scoring process(i.e.
calculate scores at step t+1 from scores at step t).This process repeats until convergence.Initialization.
As the scores of the patterns and posi-tions of a field are recursively dependant, we must as-sign initial scores to one or the other.
Initially the onlyelements that we can classify with certainty are the seedfragments.
We initialise the scoring function by assign-ing scores to the positions for each of the fields.
In thisway it is then possible to score the patterns based onthese initial scores.From the labelled training data, we derive the priorprobability pi(f) that a randomly selected position be-longs to field f ?
F :pi(f) = |{r ?
R |T (r) = f}|/|R|.Note that 1 ?
?f pi(f) is simply the prior probabil-ity that a randomly selected position should not be ex-tracted at all; typically this value is close to 1.Given the priors pi(f), we score each potential posi-27tion r in field f :score0f (r) =??
?pi(f) if r ?
U,1 if r ?
R ?
T (r) = f, and0 if r ?
R ?
T (r) 6= f.The first case handles positions in the unlabelled docu-ments; at this point we don?t know anything about themand so fall back to the prior probabilities.
The secondand third cases handle positions in the seed documents,for which we have complete information.Iteration.
After initializing the scores of the posi-tions, we begin the iterative process of scoring thepatterns and the positions.
To compute the score ofa pattern p for field f we compute a positive score,posf (p); a negative score, negf (p); and an unknownscore, unk(p).
posf (p) can be thought of as a measureof the benefit of p to f , while negf (p) measures theharm of p to f , and unk(p) measures the uncertaintyabout the field with which p is associated.These quantities are defined as follows: posf (p) isthe average score for field f of positions extracted byp.
We first compute:posf (p) =1Zp?p?rscoretf (r),where Zp =?f?p?r scoretf (r) is a normalizingconstant to ensure that?f posf (p) = 1.For each field f and pattern p, negf (p) is the extentto which p extracts positions whose field is not f :negf (p) = 1 ?
posf (p).Finally, unk(p) measures the degree to which p ex-tract positions whose field is unknown:unk(p) = 1|{p ?
r}|?p?runk(r),where unk(r) measures the degree to which position ris unknown.
To be completely ignorant of a position?sfield is to fall back on the prior field probabilities pi(f).Therefore, we calculate unk(r) by computing the sumof squared differences between scoretf (r) and pi(f):unk(r) = 1 ?
1Z SSD(r),SSD(r) =?f(scoretf (r) ?
pi(f))2 ,Z = maxrSSD(r).The normalization constant Z ensures that unk(r) = 0for the position r whose scores are the most differentfrom the priors?ie, r is the ?least unknown?
position.For each field f and pattern p, scoretf (p) is definedin terms of posf (p), negf (p) and unk(p) as follows:scoret+1f (p) =posf (p)posf (p)+negf (p)+unk(p)?
posf (p)= posf (p)21+unk(p)This definition penalizes patterns that are either in-accurate or have low coverage.Finally, we complete the iterative step by calculatinga revised score for each position:scoret+1f (r) ={ scoretf (r) if r ?
R?p?rscoretf (p)?minmax?min if r ?
U,where min = minf,p?r?p?r scoretf (p) and max =maxf,p?r?p?r scoretf (p), are used to normalize thescores to ensure that the scores of unlabelled positionsnever exceed the scores of labelled positions.
The firstcase in the function for scoret+1f (r) handles positiveand negative seeds (i.e.
positions in labelled texts), thesecond case is for unlabelled positions.We iterate this procedure until the scores of the pat-terns and positions converge.
Specifically, we stopwhen?f(?p|scoretf (p)?scoret?1f (p)|2 +?r|scoretf (r)?scoret?1f (r)|2)< ?.In our experiments, we fixed ?
= 1.3.5 Position filtering & fragment identificationDue to the nature of the pattern generation strategy,many more candidate positions will be identified thanthere are targets in the corpus.
Before we can proceedwith matching start and end positions to form frag-ments, we filter the positions to remove the weaker can-didates.We rank all of positions for each field according totheir score.
We then select positions with a score abovea threshold ?
as potential positions.
In this way wereduce the number of candidate positions from tens ofthousands to a few hundred.The next step in the process is to identify completefragments within the corpus by matching pre-positionswith post-positions.
To do this we compute the lengthprobabilities for the fragments of field f based on thelengths of the seed fragments of f .
Suppose that posi-tion r1 has been identified as a possible start for field f ,and position r2 has been identified as a possible field fend, and let Pf (`) be the fraction of field f seed frag-ments with length `.
Then the fragment e = (r1, r2) isassigned a scorescoref (e) = scoref (r1) ?
scoref (r2) ?Pf (r2 ?
r1 +1).Despite these measures, overlapping fragments stilloccur.
Since the correct fragments can not overlap, weknow that if two extracted fragments overlap, at leastone must be wrong.
We resolve overlapping fragmentsby calculating the set of non-overlapping fragmentsthat maximises the total score while also accountingfor the expected rate of occurrence of fragments fromeach field in a given document.In more detail, let E be the set of all fragments ex-tracted from some particular document D. We are in-terested in the score of some subset G ?
E of D?s28fragments.
Let score(G) be the chance that G is thecorrect set of fragments for D. Assuming that the cor-rectness of the fragments can be determined indepen-dently given that the correct number of fragments havebeen identified for each field, then score(G) can be de-fined zero if ?
(r1, r1), (s2, r2) ?
G such that (s1, r1)overlaps (s2, r2), and score(G) =?f score(Gf ) oth-erwise, where Gf ?
G is the fragments in G forfield f .
The score of Gf = {e1, e2, .
.
.}
is defined asscore(Gf ) = Pr(|Gf |)?
?j score(ej), where Pr(|Gf |)is the fraction of training documents that have |Gf | in-stances of field f .It is infeasible to enumerate all subsets G ?
E, sowe perform a heuristic search.
The states in the searchspace are pairs of the form (G,P ), where G is a listof good fragments (i.e.
fragments that have been ac-cepted), and P is a list of pending fragments (i.e.
frag-ments that haven?t yet been accepted or rejected).The search starts in the state ({}, E), and states ofthe form (G, {}) are terminal states.
The children ofstate (G,P ) are all ways to move a single fragmentfrom P to G. When forming a child?s pending set, themoved fragment along with all fragments that it over-laps are removed (meaning that the moved fragment isselected and all fragments with which it overlaps arerejected).
More precisely, the children of state (G,P )are:{(G?, P ?)???
?e?P ?
G?=G?
{e} ?P ?={e?
?P | e?
doesn?t overlap e}}.The search proceeds as follows.
We maintain a setS of the best K non-terminal states that are to be ex-panded, and the best terminal state B encountered sofar.
Initially, S contains just the initial state.
Then, thechildren of each state in S are generated.
If there areno such children, the search halts and B is returned asthe best set of fragments.
Otherwise, B is updated ifappropriate, and S is set to the best K of the new chil-dren.
Note that K = ?
corresponds to an exhaustivesearch.
In our experiments, we used K = 5.4 ExperimentsTo evaluate the performance of our algorithm we con-ducted experiments with four widely used corpora: theCMU seminar set, the Austin jobs set, the Reutersacquisition set [www.isi.edu/info-agents/RISE], and theMUC-7 named entity corpus [www.ldc.upenn.edu].We randomly partitioned each of the datasets intotwo evenly sized subsets.
We then used these as la-beled and unlabeled sets.
In each experiment the al-gorithm was presented with a document set comprisedof the test set and a randomly selected percentage ofthe documents from the training set.
For example, ifan experiment involved providing the algorithm with a5% seed set, then 5% of the documents in the trainingset (2.5% of the documents in the entire dataset) wouldbe selected at random and used in conjunction with thedocuments in the test set.For each training set size, we ran five iterations witha randomly selected subset of the documents used fortraining.
Since we are mainly motivated by scenarioswith very little training data, we varied the size of thetraining set from 1?10% (1?16% for MUC-7NE) of theavailable documents.
Precision, recall and F1 were cal-culated using the BWI (Freitag and Kushmerick, 2000)scorer.
We used all occurrences mode, which recordsa match in the case where we extract all of the validfragments in a given document, but we get no credit forpartially correct extractions.We compared TPLEX to BWI (Freitag andKushmerick, 2000), LP2 (Ciravegna, 2001), ELIE(Finn and Kushmerick, 2004), and an approachbased on conditional random fields (Lafferty et al,2001).
The data for BWI was obtained usingthe TIES implementation [tcc.itc.it/research/textec/tools-resources/ties.html].
The data for the LP2 learning curvewas obtained from (Ciravegna, 2003).
The results forELIE were generated by the current implementation[http://smi.ucd.ie/aidan/Software.html].
For the CRF re-sults, we used MALLET?s SimpleTagger (McCallum,2002), with each token encoded with a set of binaryfeatures (one for each observed literal, as well as theeight token generalizations).Our results in Fig.
2 indicate that in Acquisitionsdataset, our algorithm substantially outperforms thecompetitors at all points on the learning curve.
For theother datasets, the results are mixed.
For SA and Jobs,TPLEX is the second best algorithm at the low end ofthe learning curve, and steadily loses ground as morelabelled data is available.
TPLEX is the least accuratealgorithm for the MUC data.
In Sec.
5, we discuss avariety of modifications to the TPLEX algorithm thatwe anticipate may improve its performance.Finally, the graph in Fig.
3 compares TPLEX for theSA dataset, in two configurations: with a combinationof labelled and unlabelled documents as usual, and withonly labelled documents.
In both instances the algo-rithm was given the same seed and testing documents.In the first case the algorithm learned patterns usingboth the labeled and unlabeled documents.
However,in the second case, only the labeled documents wereused to generate the patterns.
These data confirm thatTPLEX is indeed able to improve performance from un-labelled data.5 DiscussionWe have described TPLEX, a semi-supervised algo-rithm for learning information extraction patterns.
Thekey idea is to exploit the following recursive definition:good patterns are those that extract good fragments,and good fragments are those that are extracted by goodpatterns.
This definition allows TPLEX to perform wellwith very little training data in domains where other ap-proaches that assume fragment redundancy would fail.290.10.20.30.40.50.60.70.80 2 4 6 8 10F1Percentage of data used for trainingTplex3333 333Elie+++++++BWI222 2222CRF????
?LP24444 44400.050.10.150.20.250.30.350.40 2 4 6 8 10F1Percentage of data used for trainingTplex333 3333Elie+++ ++++BWI2 2 2 2222CRF????
?00.10.20.30.40.50.60.70.80 2 4 6 8 10F1Percentage of data used for trainingTplex3333 333Elie+++ + +++BWI222 2222CRF????
?00.10.20.30.40.50.60.70.80 2 4 6 8 10 12 14 16F1Percentage of data used for trainingTplex3 33 3 333Elie++++++BWI2222 222CRF???
?
?Figure 2: F1 averaged across all fields, for the Seminar(top), Acquisitions (second), Jobs (third) andMUC-NE(bottom) corpora.Conclusions.
From our experiments we have ob-served that our algorithm is particularly competitivein scenarios where very little labelled training data isavailable.
We contend that this is a result of our algo-rithm?s ability to use the unlabelled test data to validatethe patterns learned from the training data.We have also observed that the number of fields thatare being extracted in the given domain affects the per-formance of our algorithm.
TPLEX extracts all fieldssimultaneously and uses the scores from each of the00.10.20.30.40.50.60.70.80 2 4 6 8 10F1Percentage of data used for trainingTrained with unlabeled data333 33 33Trained with only labeled data++++ +++Figure 3: F1 averaged across all fields, for the Semi-nar dataset trained on only labeled data and trained onlabeled and unlabeled datapatterns that extract a given position to determine themost likely field for that position.
With more fieldsin the problem domain there is potentially more infor-mation on each of the candidate positions to constrainthese decisions.Future work.
We are currently extending TPLEX inseveral directions.
First, position filtering is currentlyperformed as a distinct post-processing step.
It wouldbe more elegant (and perhaps more effective) to incor-porate the filtering heuristics directly into the positionscoring mechanism.
Second, so far we have focusedon a BWI-like pattern language, but we speculate thatricher patterns permitting (for example) optional or re-ordered tokens may well deliver substantial increasesin accuracy.We are also exploring ideas for semi-supervisedlearning from the machine learning community.Specifically, probabilistic finite-state methods suchhidden Markov models and conditional random fieldshave been shown to be competitive with more tradi-tional pattern-based approaches to information extrac-tion (Fuchun and McCallum, 2004), and these methodscan exploit the Expectation Maximization algorithm tolearn from a mixture of labelled and unlabelled data(Lafferty et al, 2004).
It remains to be seen whetherthis approach would be effective for information ex-traction.Another possibility is to explore semi-supervised ex-tensions to boosting (d?Alche?
Buc et al, 2002).
Boost-ing is a highly effective ensemble learning technique,and BWI uses boosting to tune the weights of thelearned patterns, so if we generalize boosting to han-dle unlabelled data, then the learned weights may wellbe more effective than those calculated by TPLEX.Acknowledgements.
This research was supported bygrants SFI/01/F.1/C015 from Science Foundation Ire-land, and N00014-03-1-0274 from the US Office ofNaval Research.30ReferencesE.
Agichtein, L. Gravano, J. Pavel, V. Sokolova, andA.
Voskoboynik.
2001.
Snowball: A prototype sys-tem for extracting relations from large text collec-tions.
In Proc.
Int.
Conf.
Management of Data.A.
Blum and T. Mitchell.
1998.
Combining la-beled and unlabeled data with co-training.
In Proc.11th Annual Conference on Computational LearningTheory, pages 92?100.S.
Brin.
1998.
Extracting patterns and relations fromthe World Wide Web.
In WebDB Workshop at theInt.
Conf.
Extending Database Technology.M.
E. Califf and R. Mooney.
1999.
Relational learningof pattern-match rules for information extraction.
InProc.
American Nat.
Conf.
Artificial Intelligence.C.
Cardie.
1997.
Empirical methods in informationextraction.
AI Magazine, 18(4).F.
Ciravegna.
2001.
Adaptive information extractionfrom text by rule induction and generalisation.
InProc.
Int.
J. Conf.
Artificial Intelligence.F.
Ciravegna.
2003.
LP2: Rule induction for infor-mation extraction using linguistic constraints.
Tech-nical report, Department of Computer Science, Uni-versity of Sheffield.M.
Collins and Y.
Singer.
1999.
Unsupervised mod-els for named entity classification.
In Proc.
jointSIGDAT Conference on Empirical Methods in Nat-ural Language Processing and Very Large Corpora,pages 100?110.F.
d?Alche?
Buc, Y. Grandvalet, and C. Ambroise.
2002.Semi-supervised MarginBoost.
In Proc.
Neural In-formation Processing Systems.O.
Etzioni, M. Cafarella, D. Downey, A. Popescu,T.
Shaked, S. Soderland, D. Weld, and A. Yates.2005.
Unsupervised named-entity extraction fromthe Web: An experimental study.
Artificial Intelli-gence.
In press.A.
Finn and N. Kushmerick.
2004.
Multi-level bound-ary classification for information extraction.
InProc.
European Conf.
Machine Learning.D.
Freitag and N .
Kushmerick.
2000.
Boosted wrap-per induction.
In Proc.
American Nat.
Conf.
Artifi-cial Intelligence.P.
Fuchun and A. McCallum.
2004.
Accurate infor-mation extraction from research papers using con-ditional random fields.
In Proc.
Human LanguageTechnology Conf.N.
Kushmerick and B. Thomas.
2003.
Adaptive in-formation extraction: Core technologies for infor-mation agents.
Lecture Notes in Computer Science,2586.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In Proc.
Int.Conf.
Machine Learning.J.
Lafferty, X. Zhu, and Y. Liu.
2004.
Kernel con-ditional random fields: Representation, clique se-lection, and semi-supervised learning.
In Proc.
Int.Conf.
Machine Learning.A.
McCallum.
2002.
Mallet: A machine learning forlanguage toolkit.
http://mallet.cs.umass.edu.E.
Riloff and R. Jones.
1999.
Learning dictionaries forinformation extraction by multi-level bootstrapping.In Proc.
American Nat.
Conf.
Artificial Intelligence.E.
Riloff.
1993.
Automatically constructing a dictio-nary for information extraction tasks.
In Proc.
Amer-ican Nat.
Conf.
Artificial Intelligence.E.
Riloff.
1996.
Automatically generating extractionpatterns from untagged text.
In Proc.
American Nat.Conf.
Artificial Intelligence.S.
Soderland.
1999.
Learning information extractionrules for semi-structured and free text.
MachineLearning, 34(1-3):233?272.M.
Stevenson and M. Greenwood.
2005.
Automaticlearning of information extraction patterns.
In Proc.19th Int.
J. Conf.
on Artificial Intelligence.V.
Vapnik.
1998.
Statistical learning theory.
Wiley.R.
Yangarber, W. Lin, and R. Grishman.
2002.
Unsu-pervised learning of generalised names.
In Proc.
Int.Conf.
Computational Linguistics.D.
Yarowsky.
1995.
Unsupervised word sense dis-ambiguation rivaling supervised methods.
In Meet-ing of the Association for Computational Linguistics,pages 189?196.31
