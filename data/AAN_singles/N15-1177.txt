Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1537?1547,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsA Corpus and Model Integrating Multiword Expressions and SupersensesNathan SchneiderSchool of InformaticsUniversity of EdinburghEdinburgh, Scotland, UKnschneid@inf.ed.ac.ukNoah A. SmithSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, Pennsylvania, USAnasmith@cs.cmu.eduAbstractThis paper introduces a task of identifying andsemantically classifying lexical expressions inrunning text.
We investigate the online reviewsgenre, adding semantic supersense annotationsto a 55,000 word English corpus that was pre-viously annotated for multiword expressions.The noun and verb supersenses apply to fulllexical expressions, whether single- or mul-tiword.
We then present a sequence taggingmodel that jointly infers lexical expressionsand their supersenses.
Results show that evenwith our relatively small training corpus in anoisy domain, the joint task can be performedto attain 70% class labeling F1.1 IntroductionThe central challenge in computational lexical se-mantics for text corpora is to develop and apply ab-stractions that characterize word meanings beyondwhat can be derived superficially from the orthog-raphy.
Such abstractions can be found in type-levelhuman-curated lexical resources such as WordNet(Fellbaum, 1998), but such intricate resources areexpensive to build and difficult to annotate with atthe token level, hindering their applicability beyonda narrow selection of languages and domains.
Amore portable and scalable?yet still linguistically-grounded?way to represent lexical meanings is withcoarse-grained semantic classes.
Here we build onprior work with an inventory of semantic classes (fornouns and verbs) known as supersenses.
The 41 su-persenses resemble the types used for named entities(PERSON, LOCATION, etc.
), but are more general,with semantic categories relevant to common nounsand verbs as well.
As a result, their application tosentences is dense (describing a large proportion oftokens), in contrast to annotations that only describenamed entities.Because most supersense tagging studies haveworked with data originally annotated for fine-grained WordNet senses, then automatically mappedto supersenses, the resulting systems have been tiedto the lexical coverage of WordNet.
Schneider et al(2012) and Johannsen et al (2014) overcame thislimitation in part by annotating supersenses directlyin text; thus, nouns and verbs not in WordNet werenot neglected.
However, the issue of which unitsought to receive supersenses has not been addressedsatisfactorily.
We argue that the semantically holisticnature of multiword expressions (MWEs) includingidioms, light verb constructions, verb-particle con-structions, and many compounds (Baldwin and Kim,2010) means that they should be considered as unitsfor manual and automatic supersense tagging.Below, we motivate the need for an integrated rep-resentation for broad-coverage lexical semantic an-alysis that identifies MWEs and labels single- andmultiword noun and verb expressions with super-senses (?2).
By annotating supersenses directly onsentences with existing comprehensive MWE anno-tations, we circumvent WordNet?s spotty coverageof many kinds of MWEs (?3).
Then we demonstratethat the two kinds of information are readily com-bined in a discriminative sequence tagging model(?4).
Notably, our analyzer handles gappy expres-sions that are ignored by existing supersense taggers,and it marks miscellaneous MWEs even though theydo not receive a noun or verb supersense.Our annotations of the REVIEWS section of theEnglish Web Treebank (Bies et al, 2012), which1537Noun VerbGROUP 1469 place STATIVE 2922 isPERSON 1202 people COGNITION 1093 knowARTIFACT 971 car COMMUNIC.
?974 recommendCOGNITION 771 way SOCIAL 944 useFOOD 766 food MOTION 602 goACT 700 service POSSESSION 309 payLOCATION 638 area CHANGE 274 fixTIME 530 day EMOTION 249 loveEVENT 431 experience PERCEPTION 143 seeCOMMUNIC.
?417 review CONSUMPTION 93 havePOSSESSION 339 price BODY 82 get.
.
.
doneATTRIBUTE 205 quality CREATION 64 cookQUANTITY 102 amount CONTACT 46 putANIMAL 88 dog COMPETITION 11 winBODY 87 hair WEATHER 0 ?STATE 56 pain all 15 VSSTs 7806NATURAL OBJ.
54 flowerRELATION 35 portion N/A (see ?3.2)SUBSTANCE 34 oil `a 1191 haveFEELING 34 discomfort ` 821 anyonePROCESS 28 process `j 54 friedMOTIVE 25 reasonPHENOMENON 23 result?COMMUNIC.is short forCOMMUNICATIONSHAPE 6 squarePLANT 5 treeOTHER 2 stuffall 26 NSSTs 9018Table 1: Summary of noun and verb supersense cate-gories.
Each entry shows the label along with the countand most frequent lexical item in the STREUSLE corpus.enrich the MWE annotations of the CMWE corpus1(Schneider et al, 2014b), are publicly released underthe name STREUSLE.2This includes new guidelinesfor verb supersense annotation.
Our open-sourcetagger, implemented in Python, is available from thatpage as well.2 Background: Supersense TagsWordNet?s supersense categories are the top-levelhypernyms in the taxonomy (sometimes known assemantic fields) which are designed to be broadenough to encompass all nouns and verbs (Miller,1990; Fellbaum, 1990).31http://www.ark.cs.cmu.edu/LexSem/2Supersense-Tagged Repository of English with a UnifiedSemantics for Lexical Expressions3WordNet synset entries were originally partitioned intolexicographer files for these coarse categories, which becameknown as ?supersenses.?
The lexname function in WordNet/The 26 noun and 15 verb supersense categories arelisted with examples in table 1.
Some of the namesoverlap between the noun and verb inventories, butthey are to be considered separate categories; here-after, we will distinguish the noun and verb categorieswith prefixes, e.g.
N:COGNITION vs. V:COGNITION.Though WordNet synsets are associated with lex-ical entries, the supersense categories are unlexical-ized.
The N:PERSON category, for instance, containssynsets for both principal and student.
A differentsense of principal falls under N:POSSESSION.As far as we are aware, the supersenses wereoriginally intended only as a method of organizingthe WordNet structure.
But Ciaramita and Johnson(2003) pioneered the coarse word sense disambigua-tion task of supersense tagging, noting that the su-persense categories provided a natural broadeningof the traditional named entity categories to encom-pass all nouns.
Ciaramita and Altun (2006) laterexpanded the task to include all verbs, and applieda supervised sequence modeling framework adaptedfrom NER.
Evaluation was against manually sense-tagged data that had been automatically converted tothe coarser supersenses.
Similar taggers have sincebeen built for Italian (Picca et al, 2008) and Chi-nese (Qiu et al, 2011), both of which have their ownWordNets mapped to English WordNet.Although many of the annotated expressions in ex-isting supersense datasets contain multiple words, therelationship between MWEs and supersenses has notreceived much attention.
(Piao et al (2003, 2005) didinvestigate MWEs in the context of a lexical taggerwith a finer-grained taxonomy of semantic classes.
)Consider these examples from online reviews:(1) IT IS NOT A HIGH END STEAK HOUSE(2) The white pages allowed me to get in touch withparents of my high school friends so that I couldtrack people down one by oneHIGH END functions as a unit to mean ?sophis-ticated, expensive?.
(It is not in WordNet, thoughNLTK (Bird et al, 2009) returns a synset?s lexicographer file.A subtle difference is that a special file called noun.Topscontains each noun supersense?s root synset (e.g., group.n.01for N:GROUP) as well as a few miscellaneous synsets, such asliving_thing.n.01, that are too abstract to fall under any singlesupersense.
Following Ciaramita and Altun (2006), we treat thelatter cases under an N:OTHER supersense category and mergethe former under their respective supersense.1538it could be added in principle.)
Assigning a seman-tic class such as N:LOCATION to END would, inour judgment, be overly literal.
To paint a coherentpicture of the meaning of this sentence, it is betterto treat HIGH END as a single unit, and becauseit serves as an adjective rather than a noun or verb,leave it semantically unclassified.4STEAK HOUSE is arguably an entrenched enoughcompound that it should receive a single supersense?in fact, WordNet spells it without a space.
Thephrases white pages, high school, (get) in touch(with), track.
.
.
down, and one by one all are listedas MWEs in WordNet.
As detailed in ?4.1 below,the conventional BIO scheme used in existing super-sense taggers is capable of representing most of these.However, it does not allow for gappy (discontinuous)uses of an expression, such as track people down.The corpus and analyzer presented in this workaddress these shortcomings by integrating a richer,more comprehensive representation of MWEs in thesupersense tagging task.3 Supersense Annotation for EnglishAs suggested above, supersense tags offer a practicalsemantic label space for an integrated analysis of lex-ical semantics in context.
For English, we have cre-ated the STREUSLE dataset, which fully annotatesthe REVIEWS corpus (55k words) for noun and verbsupersenses in a manner consistent with Schneideret al?s (2014b) multiword expression annotations.Schneider et al (2012) offered a methodology fornoun supersense annotation in Arabic Wikipedia, andpredicted that it would port well to other languagesand domains.
Our experience with English web re-views has borne this out.
We generally adhered tothe same supersense annotation process (for nouns);the most important difference was that the data hadalready been annotated for MWEs, and supersenselabels apply to any strong5MWEs as a whole.4Future supersense annotation schemes for additional partsof speech could be assimilated into our framework.
Tsvetkovet al (2014) take a step in this direction for adjectives.5The CMWE corpus distinguishes strong and weak MWEs?essentially, the former are strongly entrenched and likely non-compositional, whereas weak MWEs are merely statisticallycollocated.
See Schneider et al (2014b) for details.
Becausethey are deemed semantically compositional, weak MWEs donot receive a supersense as a whole.The same annotators had already done the MWEannotation; whenever they encountered an apparentmistake from an earlier stage (usually an oversight),they were encouraged to correct it.
Our annotationinterface supports modification of MWEs as well assupersense labels in one view.To lessen the cognitive burden when reasoningabout tagsets, supersense annotation was broken intoseparate phases: first we annotated nearly the en-tire REVIEWS corpus for noun supersenses; then wemade another pass to annotate for verbs.
Roughlya tenth of the sentences were saved for a combinednoun+verb phase at the end; annotators reported thatconstantly switching their attention between the twotagsets made this mode of annotation more difficult.3.1 NounsTargets.
Per the annotation standard, all noun sin-gletons and noun-like MWEs should receive a nounsupersense label.
Annotation targets were determinedheuristically from the gold (PTB-style) POS tags inthe corpus: all lexical expressions containing a noun6were selected.
This heuristic overpredicts noun-likeMWEs occasionally because it does not check thesyntactic status of the MWE as a whole.
During thisphase, the backtick symbol (`) was therefore reservedfor MWEs (such as light verb constructions) that con-tain a noun but should not receive a noun supersense.7The annotation interface prevented submission ofblank annotation targets to avoid oversights.Tagset conventions.
Several brief annotationrounds were devoted to practice with Schneideret al?s (2012) noun annotation guidelines,8since theannotators were new to the scheme.
Metonymy posedthe chief difficulty in this domain: institutions witha premises (such as restaurants, hotels, and schools)are frequently ambiguous between N:GROUP (insti-tution as a whole), N:ARTIFACT (the building), andN:LOCATION (site as a whole).
Our convention wasto use the reading that seemed most salient in context:for example, restaurant in a comment about the qual-6Specifically, any POS tag starting with N or ADD (webaddresses); pronouns were excluded.7Pronouns like anything also fall into this category becausethey are POS-tagged as nouns.8http://www.ark.cs.cmu.edu/ArabicSST/corpus/guidelines.html1539Figure 1: Annotation interface, with drop-down menu for verb supersenses.
The largetext box at the bottom can be used to editthe MWE annotation by typing underscoresand tildes to connect tokens.ity of the service would be labeled N:GROUP.9Somesubjectivity is involved, suggesting that the schemeis not ideal for such multifaceted concepts.3.2 VerbsTargets.
The set of lexical expressions that shouldreceive a verb supersense consists of (a) all verb sin-gletons that are not auxiliaries, and (b) all verb-likeMWEs.
Again, simple but overly liberal heuristicswere used to detect annotation targets, so whereverthe heuristics overpredicted, annotators entered:?
`a for auxiliary verbs?
`j for adjectives (some -ing and -ed adjectivesare POS-tagged as VBG and VBD, respectively)?
` for all other casesTagset conventions.
We wrote new guidelines tocharacterize the verb supersenses for annotation.They briefly define and exemplify each category, andalso relate them via precedence rules: e.g., the rule{V:PERCEPTION, V:CONSUMPTION} >V:BODY > V:CHANGEstipulates that verbs of perception or consumption(hear, eat, etc.)
be labeled as such rather than the lessspecific class V:BODY.
The precedence rules helpto resolve many of the cases of meaning overlap be-tween the categories.
The guidelines were developedover several weeks and informed by annotation dif-ficulties and disagreements.
We release them alongwith the STREUSLE corpus.3.3 InterfaceWe extended the online MWE annotation tool ofSchneider et al (2014b) to also support supersenselabeling, as well as grouping tokens into multiwordlexical expressions.
This is visualized in figure 1.Specifically, singletons and strong MWEs may re-ceive labels (subject to a POS filter).
This allows9This rule is sometimes at odds with WordNet, which onlylists N:ARTIFACT for hotel and restaurant.the two types of annotation to be worked on in tan-dem, especially when a supersense annotator wishesto change a multiword grouping.
The tool offersan autocomplete dropdown menu when typing a tagname, and validates that the submitted annotationis complete and internally consistent.
Additionally,the tool provides a complete version history of thesentence and a ?reconciliation?
mode that mergestwo users?
annotations of a sentence, flagging anydifferences for manual resolution; these features areextremely useful when breaking the annotation downinto multiple rounds among several annotators.3.4 Quality ControlThere were 2 primary annotators and 3 others whoparticipated in annotation to a lesser degree, includ-ing the first author of this paper, whose role wasmainly supervisory.
All 5 hold bachelor?s degreesin linguistics.
The annotators were trained in thenoun supersense annotation scheme of Schneideret al (2012) and cooperatively developed and docu-mented interpretations for the verb supersenses.
Ourmain quality control mechanism for the annotationprocess was to obtain two independent annotationsfor every sentence?differences between them werereconciled by negotiation (between the two anno-tators in most cases, and between the two primaryannotators in a small number of cases).To get a sense of the difficulty of the task, we exam-ine the annotation history for a sample of sentencesto measure inter-annotator agreement.
Estimated be-tween the 2 primary annotators on the batch of sen-tences annotated last during each phase (350, 302,and 379 sentences, respectively), inter-annotator F1scores (excluding auxiliaries and other miscellaneouscategories) are: 76% for noun expression supersensesafter the noun phase, 93% for verb expression super-senses after the verb phase, and 88% for all super-senses after the combined annotation phase.10These10Cohen?s ?
, limited to tokens for which both annotators1540are over different sentences, so they are not directlycomparable, but they point to the robustness of theannotation scheme.
Thanks to the double annotationplus reconciliation procedure, these numbers shouldunderestimate the reliability of the final annotations.3.5 Corpus StatisticsA total of 9,000 noun mentions (1,300 of themMWEs) and 7,800 verb mentions (1,200 MWEs) in-corporating 20,000 word tokens are annotated.11Ta-ble 1 shows supersense mention counts and the mostfrequent example of each category in the corpus.3.6 Copenhagen Supersense DataAn independent English noun+verb supersense an-notation effort targeting the Twitter domain was un-dertaken by the COASTAL lab at the University ofCopenhagen (Johannsen et al, 2014).
The overarch-ing goal of annotating supersenses directly in runningtext was the same as in the present work, but there arethree important differences.
First, general-purposeMWE annotations were not considered in that work;second, sentences were pre-annotated by a heuristicsystem and then manually corrected, whereas herethe annotations are supplied from scratch; and third,Johannsen et al (2014) provided minimal instruc-tions and training to their annotators, whereas herewe have worked hard to encourage consistent inter-pretations of the supersense categories.
Johannsenet al have released their annotations on two samplesof tweets (over 18,000 tokens in total).Johannsen et al?s dataset illustrates why super-sense annotation by itself is not the same as thefull scheme for lexical semantic analysis proposedhere.
Many of the expressions that they havesupersense-annotated as single-word nouns/verbsprobably would have been part of larger units inMWE annotation: examining Johannsen et al?s in-house sample, multiword chunks arguably shouldhave been used for verb phrases like gain entry, makesure, and make it (?succeed?
), and for verb-particleconstructions like take over, find out, and check out(?ogle?).
Moreover, in the traditional supersense an-notation scheme, there are no chunks not labeledassigned a supersense, is very similar: .76, .93, and .90, respec-tively, reflecting strong agreement.11This excludes 1,200 auxiliary verb mentions, 100 of whichare MWEs: have to, is going to, etc.with a supersense; thus, e.g., PPs such as on tap, ofALL-Time, and up to [value limit] are not chunked.Many of the nominal expressions in Johannsenet al?s (2014) data appear to have overly liberalboundaries, grouping perfectly compositional mod-ifiers along with their heads as a multiword chunk:e.g., Panhandling Ban, Panda Cub, farm road crash,and Tomic?s dad.
Presumably, some of these wereboundary errors made by the heuristic pre-annotationsystem that human annotators failed to notice.4 Automatic TaggingWe now turn to automating the combined multiwordexpression and supersense prediction task in a singlestatistical model.4.1 Background: Supersense Tagging with aDiscriminative Sequence ModelCiaramita and Altun?s (2006) model represents thestate of the art for full12English supersense taggingon the standard SemCor test set, achieving an F1score of 77%.
It is a feature-based discriminativesequence model learned in a supervised fashion withthe structured perceptron (Collins, 2002).For Ciaramita and Altun (2006) and hereafter, se-quences correspond to sentences, with each sentencepre-segmented into words according to some tok-enization.
Figure 2 shows how token-level tags com-bine Ramshaw and Marcus (1995)?style BIO flagswith supersense class labels to represent the segmen-tation and supersense labeling of a sentence.
Thesetags are observed during training, predicted at testtime, and compared against the gold standard tags.Ciaramita and Altun?s (2006) model uses a sim-ple feature set capturing the lemmas, word shapes,and parts of speech of tokens in a small context win-dow, as well as the supersense category of the firstWordNet sense of the current word.
(WordNet sensesare ordered roughly by frequency.)
On SemCor data,the model achieves a 10% absolute improvement inF1over the first sense baseline.12Paa?
and Reichartz (2009) train a similar sequence modelfor classifying noun and verb supersenses, but treat multiwordphrases as single words.
Their model is trained as a CRF ratherthan a structured perceptron, and adds LDA word cluster features,but the effects of these two changes are not separated in theexperiments.
They also find benefit from constraining the labelspace according to WordNet for in-vocabulary words (with whatthey call ?lumped labels?
).1541United States financier and philanthropist ( 1855 - 1937 )BN:LOCATIONIN:LOCATIONBN:PERSON OBN:PERSON OBN:TIME OBN:TIME OFigure 2: A supersense tagging shown with per-token BIO tags in the style of Ciaramita and Altun (2006).The white pages allowed me to get in touch withBN:COMMUNICATION I?
OV:COGNITION O O BV:SOCIAL I?
I?
I?parents of my high schoolON:PERSON O O BN:GROUP I?friends so that I could track people down one by oneON:PERSON O O O O BV:SOCIAL oN:PERSON I?
B I?
I?Figure 3: Tagging for part of the lexical semantic analysis depicted in figure 1.
Note that for nominal and verbalMWEs, the supersense label is only attached to the first tag of the expression.Though our focus in this paper is on English, auto-matic supersense tagging has also been explored inItalian (Picca et al, 2008, 2009; Attardi et al, 2010,2013; Rossi et al, 2013), Chinese (Qiu et al, 2011),and Arabic (Schneider et al, 2013).4.2 ModelLike Ciaramita and Altun (2006) and Schneider et al(2014a), we train a first-order structured perceptron(Collins, 2002) with averaging.
This is a standarddiscriminative modeling setup, involving: a linearscoring function over features of input?output pairs;a Viterbi search to choose the highest-scoring validoutput tag sequence given the input; and an onlinelearning algorithm that makes M passes through thetraining data, searching for the best tagging giventhe current model and updating the parameters (lin-ear feature weights) where the best tagging doesn?tmatch the gold tagging.
With a first-order Markovassumption and tagset Y , the Viterbi search for asentence x requires O(?Y ?2 ?
?x?)
runtime.
The datasetused to train and evaluate the model, the taggingscheme, and the features are described below.4.3 DataThe STREUSLE dataset, as described in ?3, is anno-tated for multiword expressions as well as noun andverb supersenses and auxiliary verbs.
We use thisdataset for training and testing an integrated lexicalsemantic analyzer.
Schneider et al (2014a) used theCMWE dataset?i.e., the same REVIEWS sentences,but annotated only for MWEs.
A handful of apparenterrors in the MWE analyses were fixed in the courseof our supersense annotation.4.4 TagsetIn the STREUSLE dataset, supersense labels applyto strong noun and verb expressions?i.e., singletonnouns/verbs as well as strong nominal/verbal MWEs.Weak MWEs are not holistically labeled with a su-persense (see fn.
5).The 8-way scheme.
To encode the lexical seg-mentation via token-level tags, we use the 8-wayscheme from Schneider et al (2014a) for positionalflags.
The 8-way scheme extends Ramshaw and Mar-cus?s (1995) BIO chunking tags to also encode (a) astrong/weak distinction for MWEs, and (b) gappyMWEs (there is no formal limit on the number ofgaps per MWE or the number of other lexical ex-pressions occurring within each gap, though thereis a limit of one level of nesting).
The 4 lowercasepositional flags indicate that an expression is withina gap, and otherwise have the same interpretation astheir uppercase counterparts, which are:?
O for single-word expressions?
B for the first word of an MWE?
I?
for a word continuing a strong MWE?
I?
for a word weakly linked to its predecessor,forming a weak MWE13As with the original BIO scheme, a globally well-formed sequence of tags in the 8-tag scheme can beconstructed by respecting bigram constraints.14Adding class labels.
The tagset used to annotatethe data for our tagger combines 8-way positionalflags with supersense class labels.
We decorateclass labels only on beginners of strong lexicalexpressions?so this includes O or o on a single-wordnoun or verb, but always excludes I?
and ?
?.15 Figure 313Weak MWE links may join together strong MWEs.14Among these constraints are: B must always be immediatelyfollowed by I?
or I?
(because B marks the beginning of an MWE);and within-gap (lowercase-tagged) tokens must immediatelyfollow a tag other than O and precede a tag other than O or B.15Unlike prior work with the plain BIO scheme, we do notinclude the class in tags continuing a (strong) MWE, though the1542gives an example.
In this formulation, bigram con-straints are sufficient to ensure a globally consistenttagging of the sentence.There are ?N ?
= 26 noun supersense classes and?V ?
= 16 verb classes (including the auxiliary verbclass, abbreviated `a).
In principle, then, there are?
{O o B b I?
??}??????????????????????????????????????????????????????????????????6?
(1+ ?N ?+ ?V ?)?????????????????????????????????????????????????????????????????
?43+ ?{I?
??}????????????
?2= 260possible tags encoding position and class information,allowing for chunks with no class because they areneither nominal nor verbal expressions.
In practice,though, many of these combinations are nonexistentin our data; for experiments we only consider tagsoccurring in train, yielding ?Y ?
= 146.We also run a condition where the supersense re-finements are collapsed, i.e.
Y consists of the 8 MWEtags.
This allows us to measure the impact of the su-persenses on MWE identification performance.4.5 FeaturesWe constrast three feature sets for full supersense tag-ging: (a) Schneider et al?s (2014a) basic MWE fea-tures, which include lemmas, POS tags, word shapes,and whether the token potentially matches entriesin any of several multiword lexicons; (b) the basicMWE features plus the Brown clusters (Brown et al,1992) used by Schneider et al (2014a); and (c) thebasic MWE features and Brown clusters, plus sev-eral new features shown in figure 4.
Chiefly, thesenew features consult the supersenses of WordNetsynsets associated with words in the sentence: thefirst WordNet supersense feature is inspired by Cia-ramita and Altun (2006) and subsequent work on su-persense tagging, while the has-supersense feature isnovel.
There is also a feature aimed at distinguishingauxiliary verbs from main verbs, and new capital-ization features take into account the capitalizationof the first word in the sentence and the majority ofwords in the sentence.
To keep the system as modularas possible, we refrain from including any featuresthat depend on a syntactic parser.class label should be interpreted as extending across the entireexpression.
This is for a technical reason: as our scheme allowsfor gaps, the classes of the tags flanking a gap in a strong MWEwould be required to match for the analysis to be consistent.To enforce this in a bigram tagger, the within-gap tags wouldhave to encode the gappy expression?s class as well as their own,leading to an undesirable blowup in the size of the state space.New Capitalization Features1.
capitalized ?
?i = 0?
?
?majority of tokens in thesentence are capitalized?2.
capitalized ?
i > 0 ?
w0is lowercaseAuxiliary Verb vs. Main Verb Feature3.
posiis a verb ?
?posi+1 is a verb?
(posi+1 is an adverb?posi+2 is a verb)?WordNet Supersense Features (unlexicalized)Let cposidenote the coarse part-of-speech of token i:common noun, proper noun, pronoun, verb, adjective, ad-verb, etc.
This feature aims primarily to inform the su-persense label on the first token of nominal compoundsand light verb constructions, where the ?semantic head?
isusually a common noun subsequent to the beginning of theexpression:4. subsequent noun?s 1st supersense: where cposiis acommon noun, verb, or adjective, cposi?
for thesmallest k > i such that poskis a common noun, thesupersense of the first WordNet synset for lemma?k?provided there is no intervening verb ( j suchthat cposjis a verb and i < j < k)The following two feature templates depend on the tagyi.
Let flag(yi) denote the positional flag part of the tag (O,B, etc.)
and sst(yi) denote the supersense class label:5.
1st supersense:?
if flag(yi) ?
{O,o}: the supersense of the firstWordNet synset for lemma ?i?
else if cposiis a verb and there is a subsequentverb particle at position k > i with no interven-ing verb: the supersense of the first synset forthe compound lemma ??i,?k?
(provided thatthe particle verb is found in WordNet)?
otherwise: the supersense of the first WordNetsynset for the longest contiguous lemma start-ing at position i that is present in WordNet:?
?i,?i+1, .
.
.
,?
j?
( j ?
i)6. has supersense: same cases as the above, but insteadof encoding the highest-ranking synset?s supersense,encodes whether sst(yi) is represented in any of thematched synsets for the given lemma.
Note that for agiven token, this feature can take on different valuesfor different tags.Figure 4: New features for MWE and supersense tagging.They augment the basic MWE feature set of Schneideret al (2014a), and are conjoined with the current tag, yi.The model?s percepts (binary or real-valued func-tions of the input16) can be conjoined with any tagy ?
Y to form a feature that receives its own weight16We use the term percept rather than ?feature?
here to em-phasize that we are talking about functions of the input only,rather than input?output combinations that each receive a param-eter during learning.1543(parameter).
To avoid having to learn a model withtens of millions of parameters, we impose a perceptcutoff during learning: only zero-order percepts thatare active at least 5 times in the training data (with anytag) are retained in the model (with features for alltags).
There is no minimum threshold for first-orderpercepts.17The resulting models are of a manageablesize: about 4 million parameters with the full tagset.4.6 Experimental SetupOur setup mostly echoes that of Schneider et al(2014a).
We adopt their train (3312 sentences/48k words) vs. test (500 sentences/7k words) split,and tune hyperparameters by 8-fold cross-validationon train.
By this procedure we chose a perceptcutoff of 5 to use throughout, and tuned the num-ber of training iterations for each experimental con-dition (early stopping within each cross-validationfold so as to greedily maximize tagging accuracy onthe held-out portion, and averaging the best num-ber of iterations across folds).
For simplicity, weuse oracle POS tags in our experiments and do notuse Schneider et al?s (2014a) recall-oriented costfunction.
Experiments were managed with JonathanClark?s ducttape tool.184.7 ResultsTable 2 shows full supersense tagging results, separat-ing the MWE identification performance (measuredby link-based precision, recall, and F1; see Schnei-der et al, 2014a) from the precision, recall, and F1of class labels on the first token of each expression(segments with no class label are ignored).19Exacttagging accuracy (last column) is higher because it re-wards true negatives, i.e.
single-word segments withno nominal or verbal class label (the O and o tags).Tag space.
The sequence tagging frameworkmakes it simple to model MWE identification jointlywith supersense tagging: this is accomplished bypacking information about both kinds of output into17Zero-order percepts are percepts which are to be conjoinedwith only the present tag to form zero-order features.
First-orderpercepts are to be conjoined with the present and previous tags.18https://github.com/jhclark/ducttape/19We count the class label only once for MWEs?otherwisethis measure would be strongly dependent on segmentation per-formance.
However, the MWE predictions do have an effectwhen the prediction and gold standard disagree on which tokenbegins a strong nominal or verbal expression.the tags.
But there is always a risk that a larger tagspace will impair the model?s ability to generalize.By comparing the first two rows of the results, we cansee that jointly modeling supersenses along with mul-tiword expressions results in only a minor decrease(<2 F1points) in MWE identification performanceunder the most basic feature set.
Further, we seethat most of that decrease is recovered with richerfeatures.
Thus, we conclude that it is empiricallyreasonable to model these phenomena together.Runtime.
Our final system (146 tags; last row oftable 2) tags ?140 words (10 sentences) per second.Features.
Comparing the bottom three rows in thetable indicates that features that generalize beyondlexical items lead to better supersense labeling.
Thebest model has access to supersense information inthe WordNet lexicon; it is 4 F1points better at choos-ing the correct class label than its nearest competitor,which relies on word clusters to abstract away fromindividual lexical items.
Nouns, verbs, and auxil-iaries all see improvements.We also inspect the learned parameters.
Thehighest-weighted parameters suggest that the bestmodel relies heavily on the supersense lookupfeatures, whereas the second-best model?lackingthose?in large part relies on Brown clusters (cf.Grave et al, 2013).
The auxiliary verb vs. main verbfeature in the best model is highly weighted as well,helping to distinguish between `a and V:STATIVE.Polysemy.
We have motivated the task of super-sense tagging in part as a coarse form of word sensedisambiguation.
Therefore, it is worth investigatinghow well the learned model manages to choose thecorrect supersense for nouns and verbs that are am-biguous in the data.
A handful of lemmas in testhave at least two different supersenses predicted sev-eral times; an examination of four such lemmas intable 3 shows that for three of them the tagging ac-curacy exceeds the majority baseline.
In the case oflook, the model is usually able to distinguish betweenV:COGNITION (as in looking for a company with de-cent rates) and V:PERCEPTION (as in sometimes thebroccoli looks browned around the edges).Out-of-domain baseline.
To assess the impor-tance of in-domain data for learning, we used aSemCor-trained supersense tagger?a reimplemen-1544MWE ID Class labeling TagFeature Set ?Y ?
Model Size M P R F1P R F1NSST R VSST R Aux R AccMWE 8 194k 4 72.97 55.55 63.01 ?
?
?
?
?
?
?MWE 146 3,555k 5 67.77 55.76 61.14 64.68 66.78 65.71 59.14 71.64 93.71 80.73MWE+clusters 146 4,371k 5 68.55 56.73 62.04 65.69 67.76 66.71 61.49 71.34 92.45 81.20MWE+clusters+SST 146 4,388k 4 71.05 56.24 62.74 69.47 71.90 70.67 66.95 74.17 94.97 82.49Table 2: Results on test for lexical semantic analysis of noun and verb supersenses and MWEs with increasinglycomplex models.
Class labeling performance is given in aggregate, and class labeling recall is further broken down intonoun supersense tagging (NSST), verb supersense tagging (VSST), and auxiliary verb tagging.
All of these resultsuse a percept cutoff of 5.
The first result row uses a collapsed tagset (just the MWE status) rather than predicting fullsupersense labels, as described in ?4.4.
The number of training iterations M was tuned by cross-validation on train.The best result in each column and section is bolded.lemma unique SSTs majority baseline accuracyget 7 gold, 8 pred.
12/28 6/28look 2 gold, 3 pred.
8/13 12/13take 5 gold, 5 pred.
8/21 11/21time(s) 3 gold, 2 pred.
8/14 9/14Table 3: Four polysemous lemmas and counts of theirgold vs. predicted supersenses in test (limited to caseswhere both the gold standard tag and the predicted tagincluded a supersense).
The distribution of gold super-senses for take, for example, is V:SOCIAL: 8, V:MOTION: 7,V:POSSESSION: 1, V:STATIVE: 4, V:EMOTION: 1.tation of Ciaramita and Altun (2006)20?to tag ourtest data in the reviews domain.
By our class labelingevaluation, the result is 51.05% precision, 48.93% re-call, and 49.97% F1.21Even without word clusters orthe supersense-tailored features of figure 4, our sim-plest in-domain model reaches 65.71% F1.
Thoughthere are minor differences in features between thetwo models, both are first-order structured perceptrontaggers.
We believe that this wide gulf is primarily anartifact of the training data.
The annotation methodol-ogy was very different (direct MWE and supersenseannotation in our case, vs. relying on mappings fromWordNet synsets in the case of SemCor), and thevocabulary and style are vastly different between ca-sual online writing and edited prose.
Building lexicalsemantic models that are robust to many domains atonce will require further experimentation, and in our20By Michael Heilman (Heilman, 2011, pp.
47?48);downloaded from: http://www.ark.cs.cmu.edu/mheilman/questions/SupersenseTagger-10-01-12.tar.gz21Excluding auxiliaries (which are not part of the originalsupersense representation and thus not predicted by Heilman?stagger) from the evaluation, recall rises to 52.50% and F1to51.76%.estimation, additional annotated resources that covera fuller spectrum of written language.5 ConclusionWe have integrated the multiword expression identi-fication task formulated in Schneider et al (2014a)with the supersense tagging task of Ciaramita andAltun (2006).
Supersenses offer coarse-grained andbroadly applicable semantic labels for lexical expres-sions and naturally complement multiword expres-sions in lexical semantic analysis.
We have annotatedEnglish online reviews for supersenses, includingdeveloping detailed annotation criteria for verbs.
Ex-periments with discriminative joint tagging of MWEsand supersenses establish a strong baseline for futurework, which may incorporate new features, richermodels, and indirect forms of supervision (cf.
Graveet al, 2013; Johannsen et al, 2014) for this task.
Wealso expect future investigations will apply our tag-ger to a downstream task such as semantic parsing ormachine translation (for further discussion of poten-tial applications, see Schneider, 2014, pp.
179?189).Our data and open-source software is available athttp://www.ark.cs.cmu.edu/LexSem/.AcknowledgmentsWe thank our energetic annotators, Nora Kazour,Spencer Onuffer, Emily Danchik, and Michael T.Mordowanec, as well as Chris Dyer, Lori Levin, EdHovy, Tim Baldwin, Mark Steedman, and anony-mous reviewers for useful feedback on the techni-cal content.
This research was supported in part byNSF CAREER grant IIS-1054319 and DARPA grantFA8750-12-2-0342 funded under the DEFT program.1545ReferencesGiuseppe Attardi, Luca Baronti, Stefano Dei Rossi, andMaria Simi.
2013.
SuperSense Tagging with a Maxi-mum Entropy Markov Model.
In Bernardo Magnini,Francesco Cutugno, Mauro Falcone, and EmanuelePianta, editors, Evaluation of Natural Language andSpeech Tools for Italian, number 7689 in Lecture Notesin Computer Science, pages 186?194.
Springer, Berlin.Giuseppe Attardi, Stefano Dei Rossi, Giulia Di Pietro,Alessandro Lenci, Simonetta Montemagni, and MariaSimi.
2010.
A resource and tool for super-sense tag-ging of Italian texts.
In Nicoletta Calzolari, KhalidChoukri, Bente Maegaard, Joseph Mariani, Jan Odijk,Stelios Piperidis, Mike Rosner, and Daniel Tapias, edi-tors, Proc.
of LREC, pages 2242?2248.
Valletta, Malta.Timothy Baldwin and Su Nam Kim.
2010.
Multiwordexpressions.
In Nitin Indurkhya and Fred J. Damerau,editors, Handbook of Natural Language Processing,Second Edition, pages 267?292.
CRC Press, Taylor andFrancis Group, Boca Raton, FL.Ann Bies, Justin Mott, Colin Warner, and SethKulick.
2012.
English Web Treebank.
Tech-nical Report LDC2012T13, Linguistic DataConsortium, Philadelphia, PA. URL http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?catalogId=LDC2012T13.Steven Bird, Ewan Klein, and Edward Loper.
2009.
Natu-ral Language Processing with Python: Analyzing Textwith the Natural Language Toolkit.
O?Reilly Media,Inc., Sebastopol, CA.Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-cent J. Della Pietra, and Jenifer C. Lai.
1992.
Class-based n-gram models of natural language.
Computa-tional Linguistics, 18(4):467?479.Massimiliano Ciaramita and Yasemin Altun.
2006.
Broad-coverage sense disambiguation and information extrac-tion with a supersense sequence tagger.
In Proc.
ofEMNLP, pages 594?602.
Sydney, Australia.Massimiliano Ciaramita and Mark Johnson.
2003.
Su-persense tagging of unknown nouns in WordNet.
InMichael Collins and Mark Steedman, editors, Proc.
ofEMNLP, pages 168?175.
Sapporo, Japan.Michael Collins.
2002.
Discriminative training methodsfor Hidden Markov Models: theory and experimentswith perceptron algorithms.
In Proc.
of EMNLP, pages1?8.
Philadelphia, PA, USA.Christiane Fellbaum.
1990.
English verbs as a semanticnet.
International Journal of Lexicography, 3(4):278?301.Christiane Fellbaum, editor.
1998.
WordNet: an electroniclexical database.
MIT Press, Cambridge, MA.Edouard Grave, Guillaume Obozinski, and Francis Bach.2013.
Hidden Markov tree models for semantic classinduction.
In Proc.
of CoNLL, pages 94?103.
Sofia,Bulgaria.Michael Heilman.
2011.
Automatic factual question gen-eration from text.
Ph.D. dissertation, Carnegie MellonUniversity, Pittsburgh, Pennsylvania.
URL http://www.ark.cs.cmu.edu/mheilman/questions/papers/heilman-question-generation-dissertation.pdf.Anders Johannsen, Dirk Hovy, H?ctor Mart?nez Alonso,Barbara Plank, and Anders S?gaard.
2014.
More orless supervised supersense tagging of Twitter.
In Proc.of *SEM, pages 1?11.
Dublin, Ireland.George A. Miller.
1990.
Nouns in WordNet: a lexicalinheritance system.
International Journal of Lexicogra-phy, 3(4):245?264.Gerhard Paa?
and Frank Reichartz.
2009.
Exploitingsemantic constraints for estimating supersenses withCRFs.
In Proc.
of the Ninth SIAM International Confer-ence on Data Mining, pages 485?496.
Sparks, Nevada.Scott S. L. Piao, Paul Rayson, Dawn Archer, Andrew Wil-son, and Tony McEnery.
2003.
Extracting multiwordexpressions with a semantic tagger.
In Proc.
of theACL 2003 Workshop on Multiword Expressions: Analy-sis, Acquisition and Treatment, pages 49?56.
Sapporo,Japan.Scott Songlin Piao, Paul Rayson, Dawn Archer, and TonyMcEnery.
2005.
Comparing and combining a seman-tic tagger and a statistical tool for MWE extraction.Computer Speech & Language, 19(4):378?397.Davide Picca, Alfio Massimiliano Gliozzo, and SimoneCampora.
2009.
Bridging languages by SuperSense en-tity tagging.
In Proc.
of NEWS, pages 136?142.
Suntec,Singapore.Davide Picca, Alfio Massimiliano Gliozzo, and Massimil-iano Ciaramita.
2008.
Supersense Tagger for Italian.
InNicoletta Calzolari, Khalid Choukri, Bente Maegaard,Joseph Mariani, Jan Odjik, Stelios Piperidis, and DanielTapias, editors, Proc.
of LREC, pages 2386?2390.
Mar-rakech, Morocco.Likun Qiu, Yunfang Wu, Yanqiu Shao, and AlexanderGelbukh.
2011.
Combining contextual and structural in-formation for supersense tagging of Chinese unknownwords.
In Computational Linguistics and IntelligentText Processing: Proceedings of the 12th InternationalConference (CICLing?11), volume 6608 of LectureNotes in Computer Science, pages 15?28.
Springer,Berlin.Lance A. Ramshaw and Mitchell P. Marcus.
1995.
Textchunking using transformation-based learning.
In Proc.of the Third ACL Workshop on Very Large Corpora,pages 82?94.
Cambridge, MA.1546Stefano Dei Rossi, Giulia Di Pietro, and Maria Simi.
2013.Description and results of the SuperSense tagging task.In Bernardo Magnini, Francesco Cutugno, Mauro Fal-cone, and Emanuele Pianta, editors, Evaluation of Nat-ural Language and Speech Tools for Italian, number7689 in Lecture Notes in Computer Science, pages166?175.
Springer, Berlin.Nathan Schneider.
2014.
Lexical Semantic Analy-sis in Natural Language Text.
Ph.D. dissertation,Carnegie Mellon University, Pittsburgh, Pennsylva-nia, USA.
URL http://www.cs.cmu.edu/~nschneid/thesis/thesis-print.pdf.Nathan Schneider, Emily Danchik, Chris Dyer, andNoah A. Smith.
2014a.
Discriminative lexical seman-tic segmentation with gaps: running the MWE gamut.Transactions of the Association for Computational Lin-guistics, 2:193?206.Nathan Schneider, Behrang Mohit, Chris Dyer, KemalOflazer, and Noah A. Smith.
2013.
Supersense taggingfor Arabic: the MT-in-the-middle attack.
In Proc.
ofNAACL-HLT, pages 661?667.
Atlanta, Georgia, USA.Nathan Schneider, Behrang Mohit, Kemal Oflazer, andNoah A. Smith.
2012.
Coarse lexical semantic annota-tion with supersenses: an Arabic case study.
In Proc.of ACL, pages 253?258.
Jeju Island, Korea.Nathan Schneider, Spencer Onuffer, Nora Kazour, EmilyDanchik, Michael T. Mordowanec, Henrietta Conrad,and Noah A. Smith.
2014b.
Comprehensive annota-tion of multiword expressions in a social web corpus.In Nicoletta Calzolari, Khalid Choukri, Thierry De-clerck, Hrafn Loftsson, Bente Maegaard, Joseph Mari-ani, Asuncion Moreno, Jan Odijk, and Stelios Piperidis,editors, Proc.
of LREC, pages 455?461.
Reykjav?k, Ice-land.Yulia Tsvetkov, Nathan Schneider, Dirk Hovy, ArchnaBhatia, Manaal Faruqui, and Chris Dyer.
2014.
Aug-menting English adjective senses with supersenses.In Nicoletta Calzolari, Khalid Choukri, Thierry De-clerck, Hrafn Loftsson, Bente Maegaard, Joseph Mari-ani, Asuncion Moreno, Jan Odijk, and Stelios Piperidis,editors, Proc.
of LREC, pages 4359?4365.
Reykjav?k,Iceland.1547
