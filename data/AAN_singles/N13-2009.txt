Proceedings of the NAACL HLT 2013 Student Research Workshop, pages 62?68,Atlanta, Georgia, 13 June 2013. c?2013 Association for Computational LinguisticsLarge-Scale Paraphrasing for Natural Language UnderstandingJuri GanitkevitchCenter for Language and Speech ProcessingJohns Hopkins Universityjuri@cs.jhu.eduAbstractWe examine the application of data-drivenparaphrasing to natural language understand-ing.
We leverage bilingual parallel corporato extract a large collection of syntactic para-phrase pairs, and introduce an adaptationscheme that allows us to tackle a variety oftext transformation tasks via paraphrasing.
Weevaluate our system on the sentence compres-sion task.
Further, we use distributional sim-ilarity measures based on context vectors de-rived from large monolingual corpora to anno-tate our paraphrases with an orthogonal sourceof information.
This yields significant im-provements in our compression system?s out-put quality, achieving state-of-the-art perfor-mance.
Finally, we propose a refinement ofour paraphrases by classifying them into nat-ural logic entailment relations.
By extend-ing the synchronous parsing paradigm towardsthese entailment relations, we will enable oursystem to perform recognition of textual en-tailment.1 IntroductionIn this work, we propose an extension of currentparaphrasing methods to tackle natural language un-derstanding problems.
We create a large set of para-phrase pairs in a data-driven fashion, rank thembased on a variety of similarity metrics, and attachan entailment relation to each pair, facilitating nat-ural logic inference.
The resulting resource has po-tential applications to a variety of NLP applications,including summarization, query expansion, questionanswering, and recognizing textual entailment.Specifically, we build on Callison-Burch (2007)?spivot-based paraphrase extraction method, whichuses bilingual parallel data to learn English phrasepairs that share the same meaning.
Our approach ex-tends the pivot method to learn meaning-preservingsyntactic transformations in English.
We repre-sent these using synchronous context-free grammars(SCFGs).
This representation allows us to re-usea lot of machine translation machinery to performmonolingual text-to-text generation.
We demon-strate the method on a sentence compression task(Ganitkevitch et al 2011).To improve the system, we then incorporate fea-tures based on monolingual distributional similar-ity.
This orthogonal source of signal allows us tore-scores the bilingually-extracted paraphrases us-ing information drawn from large monolingual cor-pora.
We show that the monolingual distributionalscores yield significant improvements over a base-line that scores paraphrases only with bilingually-extracted features (Ganitkevitch et al 2012).Further, we propose a semantics for paraphras-ing by classifying each paraphrase pair with oneof the entailment relation types defined by naturallogic (MacCartney, 2009).
Natural logic is usedto perform inference over pairs of natural languagephrases, like our paraphrase pairs.
It defines a set ofrelations including, equivalence (?
), forward- andbackward-entailments (@, A), antonyms (?
), andothers.
We will build a classifier for our paraphrasesthat uses features extracted from annotated resourceslike WordNet and distributional information gath-ered over large text corpora to assign one or moreentailment relations to each paraphrase pair.
We willevaluate the entailment assignments by applying thisenhanced paraphrasing system to the task of recog-nizing textual entailment (RTE).2 Extraction of Syntactic Paraphrasesfrom BitextsA variety of different types of corpora have beenused to automatically induce paraphrase collectionsfor English (see Madnani and Dorr (2010) for a sur-62... f?nf Landwirte , weil... 5 farmers were in Ireland ......oder wurden , gefoltertor have been , torturedfestgenommenthrown into jailfestgenommenimprisoned......
......Figure 1: An example of pivot-based phrasal paraphraseextraction ?
we assume English phrases that translate toa common German phrase to be paraphrases.
Thus weextract ?imprisoned?
as a paraphrase of ?thrown into jail.
?vey of these methods).
Bannard and Callison-Burch(2005) extracted phrasal paraphrases from bitext byusing foreign language phrases as a pivot: if twoEnglish phrases e1 and e2 both translate to a for-eign phrase f , they assume that e1 and e2 are para-phrases of one another.
Figure 1 gives an exampleof a phrasal paraphrase extracted by Bannard andCallison-Burch (2005).Since ?thrown into jail?
is aligned to multipleGerman phrases, and since each of those Germanphrases align back to a variety of English phrases,the method extracts a wide range of possible para-phrases including good paraphrase like: imprisonedand thrown into prison.
It also produces less goodparaphrases like: in jail and put in prison for, andbad paraphrases, such as maltreated and protec-tion, because of noisy/inaccurate word alignmentsand other problems.
To rank these, Bannard andCallison-Burch (2005) derive a paraphrase probabil-ity p(e1|e2):p(e2|e1) ?
?fp(e2|f)p(f |e1), (1)where the p(ei|f) and p(f |ei) are translation proba-bilities estimated from the bitext (Brown et al 1990;Koehn et al 2003).We extend this method to extract syntactic para-phrases (Ganitkevitch et al 2011).
Table 1shows example paraphrases produced by our sys-tem.
While phrasal systems memorize phrase pairswithout any further generalization, a syntactic para-phrasing system can learn more generic patterns.These can be better applied to unseen data.
Theparaphrases implementing the possessive rule andPossessive ruleNP ?
the NN of the NNP the NNP ?s NNNP ?
the NP made by NN the NN ?s NPDative shiftVP ?
give NN to NP give NP the NNVP ?
provide NP1 to NP2 give NP2 NP1Partitive constructionsNP ?
CD of the NN CD NNNP ?
all NN all of the NNReduced relative clauseSBAR/S ?
although PRP VBP that although PRP VBPADJP ?
very JJ that S JJ STable 1: A selection of example paraphrase patterns ex-tracted by our system.
These rules demonstrate that, us-ing the pivot approach from Figure 1, our system is capa-ble of learning meaning-preserving syntactic transforma-tions in English.the dative shift shown in Table 1 are good examplesof this: the two noun-phrase arguments to the ex-pressions are abstracted to nonterminals while eachrule?s lexicalization provides an appropriate frameof evidence for the transform.2.1 Formal RepresentationIn this proposal we focus on a paraphrasemodel based on synchronous context-free gram-mar (SCFG).
The SCFG formalism (Aho and Ull-man, 1972) was repopularized for statistical ma-chine translation by (Chiang, 2005).
An probabilis-tic SCFG G contains rules r of the form r = C ??
?, ?,?, w?.
A rule r?s left-hand side C is a nonter-minal, while its right-hands sides ?
and ?
can bemixed strings of words and nonterminal symbols.There is a one-to-one correspondency between thenonterminals in ?
and ?.
Each rule is assigned acost wr ?
0, reflecting its likelihood.To compute the cost wr of the application of arule r, we define a set of feature functions ~?
={?1...?N} that are combined in a log-linear model.The model weights are set to maximize a task-dependent objective function.2.2 Syntactic Paraphrase Rules via BilingualPivotingOur paraphrase acquisition method is based on theextraction of syntactic translation rules in statisticalmachine translation (SMT).
In SMT, SCFG rules areextracted from English-foreign sentence pairs thatare automatically parsed and word-aligned.
For a63CR Meaning GrammarReference 0.80 4.80 4.54ILP 0.74 3.44 3.41PP 0.78 3.53 2.98PP + n-gram 0.80 3.65 3.16PP + syntax 0.79 3.70 3.26Random Deletions 0.78 2.91 2.53Table 2: Results of the human evaluation on longer com-pressions: pairwise compression ratios (CR), meaningand grammaticality scores.
Bold indicates a statisticallysignificant best result at p < 0.05.
The scores range from1 to 5, 5 being perfect.foreign phrase the corresponding English phrase isfound via the word alignments.
This phrase pairis turned into an SCFG rule by assigning a left-hand side nonterminal symbol, corresponding tothe syntactic constituent that dominates the Englishphrase.
To introduce nonterminals into the right-hand sides of the rule, we can replace correspond-ing sub-phrases in the English and foreign phraseswith nonterminal symbols.
Doing this for all sen-tence pairs in a bilingual parallel corpus results in atranslation grammar that serves as the basis for syn-tactic machine translation.To create a paraphrase grammar from a transla-tion grammar, we extend the syntactically informedpivot approach of (Callison-Burch, 2008) to theSCFG model: for each pair of translation rules r1and r2 with matching left-hand side nonterminal Cand foreign language right-hand side ?
: r1 = C ??
?, ?1,?1, ~?1?
and r2 = C ?
?
?, ?2,?2, ~?2?,we pivot over ?
and create a paraphrase rule rp:rp = C ?
?
?1, ?2,?, ~??.
We estimate the costfor rp following Equation 1.2.3 Task-Based EvaluationSharing its SCFG formalism permits us to re-usemuch of SMT?s machinery for paraphrasing appli-cations, including decoding and minimum error ratetraining.
This allows us to easily tackle a variety ofmonolingual text-to-text generation tasks, which canbe cast as sentential paraphrasing with task-specificconstraints or goals.For our evaluation, we apply our paraphrase sys-tem to sentence compression.
However, to success-fully use paraphrases for sentence compression, weneed to adapt the system to suit the task.
We intro-duce a four-point adaptation scheme for text-to-texttwelvecartoons insulting the prophet mohammadCD NNS JJDTNNPNPNPVPNPDT+NNP12the prophet mohammadCD NNS JJ DT NNPNPNPVPNPDT+NNPcartoons offensiveof the that are toFigure 2: An example of a synchronous paraphrasticderivation in sentence compression.generation via paraphrases, suggesting:?
The use task-targeted features that capture in-formation pertinent to the text transformation.For sentence compression the features includeword count and length-difference features.?
An objective function that takes into accountthe contraints imposed by the task.
We usePRE?CIS, an augmentation of the BLEU metric,which introduces a verbosity penalty.?
Development data that represents the precisetransformations we seek to model.
We use a setof human-made example compressions minedfrom translation references.?
Optionally, grammar augmentations that allowfor the incorporation of effects that the learnedparaphrase grammar cannot capture.
We exper-imented with automatically generated deletionrules.Applying the above adaptations to our generic para-phraser (PP), quickly yields a sentence compressionsystem that performs on par with a state-of-the-artinteger linear programming-based (ILP) compres-sion system (Clarke and Lapata, 2008).
As Table 2shows, human evaluation results suggest that oursystem outperforms the contrast system in meaningretention.
However, it suffers losses in grammatical-ity.
Figure 2 shows an example derivation producedas a result of applying our paraphrase rules in thedecoding process.3 Integrating Monolingual DistributionalSimilarity into Bilingually ExtractedParaphrasesDistributional similarity-based methods (Lin andPantel, 2001; Bhagat and Ravichandran, 2008) rely64on the assumption that similar expressions appearin similar contexts ?
a signal that is orthogonal tobilingual pivot information we have considered thusfar.
However, the monolingual distributional signalis noisy: it suffers from problems such as mistakingcousin expressions or antonyms (such as ?rise, fall?or ?boy , girl?)
for paraphrases.
We circumvent thisissue by starting with a paraphrase grammar ex-tracted from bilingual data and reranking it with in-formation based on distributional similarity (Gan-itkevitch et al 2012).3.1 Distributional SimilarityIn order to compute the similarity of two expressionse1 and e2, their respective occurrences across a cor-pus are aggregated in context vectors ~c1 and ~c2.
The~ci are typically vectors in a high-dimensional fea-ture space with features like counts for words seenwithin a window of an ei.
For parsed data more so-phisticated features based on syntax and dependencystructure around an occurrence are possible.
Thecomparison of e1 and e2 is then made by comput-ing the cosine similarity between ~c1 and ~c2.Over large corpora the context vectors for evenmoderately frequent ei can grow unmanageablylarge.
Locality sensitive hashing provides a way ofdealing with this problem: instead of retaining theexplicit sparse high-dimensional ~ci, we use a ran-dom projection h(?)
to convert them into compact bitsignatures in a dense b-dimensional boolean spacein which approximate similarity calculation is pos-sible.3.2 Integrating Similarity with SyntacticParaphrasesIn order to incorporate distributional similarity in-formation into the paraphrasing system, we needto calculate similarity scores for the paraphrasticSCFG rules in our grammar.
For rules with purelylexical right-hand sides e1 and e2 this is a simpletask, and the similarity score sim(e1, e2) can be di-rectly included in the rule?s feature vector ~?.
How-ever, if e1 and e2 are long, their occurrences be-come sparse and their similarity can no longer bereliably estimated.
In our case, the right-hand sidesof our rules also contain non-terminal symbols andre-ordered phrases, so computing a similarity scoreis not straightforward.the long-termachieve25goals 23plans 97investment 10confirmed64revise43 the long-termthe long-termthe long-termthe long-termthe long-term....L-achieve = 25L-confirmed= 64L-revise = 43?R-goals= 23R-plans  = 97R-investment= 10?the long-term?=~sig?Figure 3: An example of the n-gram feature extractionon an n-gram corpus.
Here, ?the long-term?
is seen pre-ceded by ?revise?
(43 times) and followed by ?plans?
(97times).Our solution is to decompose the discontinuouspatterns that make up the right-hand sides of a rule rinto pairs of contiguous phrases, for which we thenlook up distributional signatures and compute sim-ilarity scores.
To avoid comparing unrelated pairs,we require the phrase pairs to be consistent with a to-ken alignment a, defined and computed analogouslyto word alignments in machine translation.3.3 Data Sets and Types of DistributionalSignaturesWe investigate the impact of the data and feature setused to construct distributional signatures.
In partic-ular we contrast two approaches: a large collectionof distributional signatures with a relatively simplefeature set, and a much smaller set of signatures witha rich, syntactically informed feature set.The larger n-gram model is drawn from a web-scale n-gram corpus (Brants and Franz, 2006; Lin etal., 2010).
Figure 3 illustrates this feature extractionapproach.
The resulting collection comprises distri-butional signatures for the 200 million most frequent1-to-4-grams in the n-gram corpus.For the syntactically informed model, we usethe constituency and dependency parses providedin the Annotated Gigaword corpus (Napoles et al2012).
Figure 4 illustrates this model?s feature ex-traction for an example phrase occurrence.
Usingthis method we extract distributional signatures forover 12 million 1-to-4-gram phrases.3.4 EvaluationFor evaluation, we follow the task-based approachtaken in Section 2 and apply the similarity-scored65long-term investment holding on todetamodtheJJ NN VBG IN TO DTNPPPVP?
?the long-term?=~sig?dep-det-R-investmentpos-L-TOpos-R-NNlex-R-investmentlex-L-todep-amod-R-investmentsyn-gov-NP syn-miss-L-NNlex-L-on-topos-L-IN-TOdep-det-R-NN dep-amod-R-NNFigure 4: An example of the syntactic feature-set.
Thephrase ?the long-term?
is annotated with position-awarelexical and part-of-speech n-gram features, labeled de-pendency links, and features derived from the phrase?sCCG label (NP/NN ).paraphrases to sentence compression.
The distri-butional similarity scores are incorporated into theparaphrasing system as additional rule features intothe log-linear model.
The task-targeted parametertuning thus results in a reranking of the rules thattakes into consideration, the distributional informa-tion, bilingual alignment-based paraphrase probabil-ities, and compression-centric features.Table 2 shows comparison of the bilingual base-line paraphrase grammar (PP), the reranked gram-mars based on signatures extracted from the Googlen-grams (n-gram), the richer signatures drawn fromAnnotated Gigaword (Syntax), and Clarke and La-pata (2008)?s compression system (ILP).
In bothcases, the inclusion of distributional similarity in-formation results in significantly better output gram-maticality and meaning retention.
Despite its lowercoverage (12 versus 200 million phrases), the syn-tactic distributional similarity outperforms the sim-pler Google n-gram signatures.3.5 PPDBTo facilitate a more widespread use of paraphrases,we release a collection of ranked paraphrases ob-tained by the methods outlined in Sections 2 and 3to the public (Ganitkevitch et al 2013).4 Paraphrasing with Natural LogicIn the previously derived paraphrase grammar it isassumed that all rules imply the semantic equiva-lence of two textual expressions.
The varying de-grees of confidence our system has in this relation-ship are evidenced by the paraphrase probabilitiesand similarity scores.
However, the grammar canalso contain rules that in fact represent a range of se-mantic relationships, including hypernym- hyponymrelationships, such as India ?
this country.To better model such cases we propose an anno-tation of each paraphrase rule with explicit relationlabels based on natural logic.
Natural logic (Mac-Cartney, 2009) defines a set of pairwise relations be-tween textual expressions, such as equivalence (?
),forward (@) and backward (A) entailment, negation?)
and others.
These relations can be used to notonly detect semantic equivalence, but also infer en-tailment.
Our resulting system will be able to tackletasks like RTE, where the more a fine-grained reso-lution of semantic relationships is crucial to perfor-mance.We favor a classification-based approach to thisproblem: for each pair of paraphrases in the gram-mar, we extract a feature vector that aims to captureinformation about the semantic relationship in therule.
Using a manually annotated development setof paraphrases with relation labels, we train a clas-sifier to discriminate between the different naturallogic relations.We propose to leverage both labeled and unla-beled data resources to extract useful features forthe classification.
Annotated resources like Word-Net can be used to derive a catalog of word andphrase pairs with known entailment relationships,for instance ?India, country ,@?.
Using word align-ments between our paraphrase pairs, we can estab-lish what portions of a pair have labels in WordNetand retain corresponding features.To leverage unlabeled data, we propose extendingour notion of distributional similarity.
Previously,we used cosine similarity to compare the signaturesof two phrases.
However, cosine similarity is a sym-metric measure, and it is unlikely to prove helpfulfor determining the (asymmetric) entailment direc-tionality of a paraphrase pair (i.e.
whether it is ahypo- or hypernym relation).
We therefore proposeto extract a variety of asymmetric similarity fea-tures from distributional contexts.
Specifically, weseek a measure that compares both the similarity andthe ?breadth?
of two vectors.
Assuming that widerbreadth implies a hypernym, i.e.
a @-entailment, thescores produced by such a measure can be highly66twelve illustrations insultingmuhammadCD NNS JJNPNPVPNPthe prophetNNS JJNPNPVPNPcartoons offensiveeditorial that were to12CDVBNPScaused unrestVBNPSsparked  riots bywereNPin DenmarkPPNP PPJJJJ""NPNPof theParaphrase rules Entailment classificationCD ?
twelve | 12 twelve ?
12JJ ?
 | editorial  A editorialNNS ?
illustrations | cartoons illustrations A cartoonsJJ ?
insulting | offensive insulting ?
/ @ offensiveNP ?
the prophet | muhammad the prophet ?
muhammadVB ?
caused | sparked caused A sparkedNP ?
unrest | riots unrest A riotsPP ?
 | in Denmark  A in DenmarkNP ?
CD(?)
NNS(A) | CD(?)
of the NNS(A) twelve illustrations A 12 of the cartoonsFigure 5: Our system will use synchronous parsing and paraphrase grammars to perform natural language inference.Each paraphrase transformation will be classified with a natural logic entailment relation.
These will be joined bottom-up, as illustrated by the last rule, where the join of the smaller constituents ?
./ A results in A for the larger phrasepairs.
This process will be propagated up the trees to determine if the hypothesis can be inferred from the premise.informative for our classification problem.
Asym-metric measures like Tversky indices (Tolias et al2001) appear well-suited to the problem.
We willinvestigate application of Tversky indices to our dis-tributional signatures and their usefulness for entail-ment relation classification.4.1 Task-Based EvaluationWe propose evaluating the resulting system on tex-tual entailment recognition.
To do this, we cast theRTE task as a synchronous parsing problem, as illus-trated in Figure 5.
We will extend the notion of syn-chronous parsing towards resolving entailments, anddefine and implement a compositional join operator./ to compute entailment relations over synchronousderivations from the individual rule entailments.While the assumption of a synchronous parsestructure is likely to be valid for translations andparaphrases, we do not expect it to straightforwardlyhold for entailment recognition.
We will thus in-vestigate the limits of the synchronous assumptionover RTE data.
Furthermore, to expand the sys-tem?s coverage in a first step, we propose a simplerelaxation of the synchronousness requirement viaentailment-less ?glue rules.?
These rules, similar toout-of-vocabulary rules in translation, will allow usto include potentially unrelated or unrecognized por-tions of the input into the synchronous parse.5 ConclusionWe have described an extension of the state of the artin paraphrasing in a number of important ways: weleverage large bilingual data sets to extract linguis-tically expressive high-coverage paraphrases basedon an SCFG formalism.
On an example text-to-text generation task, sentence compression, we showthat an easily adapted paraphrase system achievesstate of the art meaning retention.
Further, we in-clude a complementary data source, monolingualcorpora, to augment the quality of the previouslyobtained paraphrase grammar.
The resulting sys-tem is shown to perform significantly better thanthe purely bilingual paraphrases, in both meaningretention and grammaticality, achieving results onpar with the state of the art.
Finally, we proposean extension of SCFG-based paraphrasing towardsa more fine grained semantic representation using aclassification-based approach.
In extending the syn-chronous parsing methodology, we outline the ex-pansion of the paraphraser towards a system capableof tackling entailment recognition tasks.67AcknowledgementsThe ideas described in this paper were developed incollaboration with Benjamin Van Durme and ChrisCallison-Burch.
This material is based on researchsponsored by the NSF under grant IIS-1249516and DARPA under agreement number FA8750-13-2-0017 (the DEFT program).
The U.S. Governmentis authorized to reproduce and distribute reprints forGovernmental purposes.
The views and conclusionscontained in this publication are those of the authorsand should not be interpreted as representing officialpolicies or endorsements of DARPA, the NSF, or theU.S.
Government.ReferencesAlfred V. Aho and Jeffrey D. Ullman.
1972.
The Theoryof Parsing, Translation, and Compiling.
Prentice Hall.Colin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with bilingual parallel corpora.
In Proceed-ings of ACL.Rahul Bhagat and Deepak Ravichandran.
2008.
Largescale acquisition of paraphrases for learning surfacepatterns.
In Proceedings of ACL/HLT.Thorsten Brants and Alex Franz.
2006.
Web 1T 5-gramversion 1.Peter Brown, John Cocke, Stephen Della Pietra, VincentDella Pietra, Frederick Jelinek, Robert Mercer, andPaul Poossin.
1990.
A statistical approach to languagetranslation.
Computational Linguistics, 16(2), June.Chris Callison-Burch.
2007.
Paraphrasing and Trans-lation.
Ph.D. thesis, University of Edinburgh, Edin-burgh, Scotland.Chris Callison-Burch.
2008.
Syntactic constraints onparaphrases extracted from parallel corpora.
In Pro-ceedings of EMNLP.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofACL.James Clarke and Mirella Lapata.
2008.
Global infer-ence for sentence compression: An integer linear pro-gramming approach.
Journal of Artificial IntelligenceResearch, 31:273?381.Juri Ganitkevitch, Chris Callison-Burch, CourtneyNapoles, and Benjamin Van Durme.
2011.
Learningsentential paraphrases from bilingual parallel corporafor text-to-text generation.
In Proceedings of EMNLP.Juri Ganitkevitch, Benjamin Van Durme, and ChrisCallison-Burch.
2012.
Monolingual distributionalsimilarity for text-to-text generation.
In Proceedingsof *SEM.
Association for Computational Linguistics.Juri Ganitkevitch, Chris Callison-Burch, and BenjaminVan Durme.
2013.
Ppdb: The paraphrase database.
InProceedings of HLT/NAACL.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proceed-ings of HLT/NAACL.Dekang Lin and Patrick Pantel.
2001.
Discovery of infer-ence rules from text.
Natural Language Engineering.Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,David Yarowsky, Shane Bergsma, Kailash Patil, EmilyPitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani,and Sushant Narsale.
2010.
New tools for web-scalen-grams.
In Proceedings of LREC.Bill MacCartney.
2009.
Natural language inference.Ph.D.
thesis, Stanford University.Nitin Madnani and Bonnie Dorr.
2010.
Generat-ing phrasal and sentential paraphrases: A surveyof data-driven methods.
Computational Linguistics,36(3):341?388.Courtney Napoles, Matt Gormley, and Benjamin VanDurme.
2012.
Annotated gigaword.
In Proceedingsof AKBC-WEKEX 2012.Yannis A. Tolias, Stavros M. Panas, and Lefteri H.Tsoukalas.
2001.
Generalized fuzzy indices for simi-larity matching.
Fuzzy Sets and Systems, 120(2):255?270.68
