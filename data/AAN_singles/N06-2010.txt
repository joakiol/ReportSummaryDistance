Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 37?40,New York, June 2006. c?2006 Association for Computational LinguisticsGesture Improves Coreference ResolutionJacob Eisenstein and Randall DavisComputer Science and Artificial Intelligence LaboratoryMassachusetts Institute of TechnologyCambridge, MA 02139 USA{jacobe+davis}@csail.mit.eduAbstractCoreference resolution, like many problemsin natural language processing, has most of-ten been explored using datasets of writtentext.
While spontaneous spoken languageposes well-known challenges, it also offers ad-ditional modalities that may help disambiguatesome of the inherent disfluency.
We explorefeatures of hand gesture that are correlatedwith coreference.
Combining these featureswith a traditional textual model yields a statis-tically significant improvement in overall per-formance.1 IntroductionAlthough the natural language processing community hastraditionally focused largely on text, face-to-face spokenlanguage is ubiquitous, and offers the potential for break-through applications in domains such as meetings, lec-tures, and presentations.
We believe that in face-to-facediscourse, it is important to consider the possibility thatnon-verbal communication may offer features that arecritical to language understanding.
However, due to thelong-standing emphasis on text datasets, there has beenrelatively little work on non-textual features in uncon-strained natural language (prosody being the most no-table exception).Multimodal research in NLP has typically focusedon dialogue systems for human-computer interaction(e.g., (Oviatt, 1999)); in contrast, we are interested inthe applicability of multimodal features to unconstrainedhuman-human dialogues.
We believe that such featureswill play an essential role in bringing NLP applicationssuch as automatic summarization and segmentation tomultimedia documents, such as lectures and meetings.More specifically, in this paper we explore the possi-bility of applying hand gesture features to the problemof coreference resolution, which is thought to be fun-damental to these more ambitious applications (Baldwinand Morton, 1998).
To motivate the need for multimodalfeatures in coreference resolution, consider the followingtranscript:?
[This circle (1)] is rotating clockwise and [thispiece of wood (2)] is attached at [this point (3)]and [this point (4)] but [it (5)] can rotate.
So as[the circle (6)] rotates, [this (7)] moves in andout.
So [this whole thing (8)] is just going backand forth.
?Even given a high degree of domain knowledge (e.g.,that ?circles?
often ?rotate?
but ?points?
rarely do), de-termining the coreference in this excerpt seems difficult.The word ?this?
accompanied by a gesture is frequentlyused to introduce a new entity, so it is difficult to deter-mine from the text alone whether ?
[this (7)]?
refers to?
[this piece of wood (2)],?
or to an entirely different partof the diagram.
In addition, ?
[this whole thing (8)]?
couldbe anaphoric, or it might refer to a new entity, perhapssome superset of predefined parts.The example text was drawn from a small corpus of di-alogues, which has been annotated for coreference.
Par-ticipants in the study had little difficulty understandingwhat was communicated.
While this does not prove thathuman listeners are using gesture or other multimodalfeatures, it suggests that these features merit further in-vestigation.
We extracted hand positions from the videosin the corpus, using computer vision.
From the raw handpositions, we derived gesture features that were used tosupplement traditional textual features for coreferenceresolution.
For a description of the study?s protocol, auto-matic hand tracking, and a fuller examination of the ges-ture features, see (Eisenstein and Davis, 2006).
In this pa-per, we present results showing that these features yield asignificant improvement in performance.372 ImplementationA set of commonly-used linguistic features were selectedfor this problem (Table 1).
The first five features applyto pairs of NPs; the next set of features are applied indi-vidually to both of the NPs that are candidates for coref-erence.
Thus, we include two features each, e.g., J isPRONOUN and I is PRONOUN, indicating respectivelywhether the candidate anaphor and candidate antecedentare pronouns.
We include separate features for each ofthe four most common pronouns: ?this?, ?it?, ?that?, and?they,?
yielding features such as J=?this?.2.1 Gesture FeaturesThe gesture features shown in Table 1 are derived fromthe raw hand positions using a simple, deterministic sys-tem.
Temporally, all features are computed at the mid-point of each candidate NP; for a further examinationof the sensitivity to temporal offset, see (Eisenstein andDavis, 2006).At most one hand is determined to be the ?focus hand,?according to the following heuristic: select the hand far-thest from the body in the x-dimension, as long as thehand is not occluded and its y-position is not below thespeaker?s waist.
If neither hand meets these criteria, thanno hand is said to be in focus.
Occluded hands are alsonot permitted to be in focus; the listener?s perspective wasvery similar to that of the camera, so it seemed unlikelythat the speaker would occlude a meaningful gesture.
Inaddition, our system?s estimates of the position of an oc-cluded hand are unlikely to be accurate.If focus hands can be identified during both mentions,the Euclidean distance between focus points is computed.The distance is binned, using the supervised method de-scribed in (Fayyad and Irani, 1993).
An advantage ofbinning the continuous features is that we can create aspecial bin for missing data, which occurs whenever a fo-cus hand cannot be identified.If the same hand is in focus during both NPs, then thevalue of WHICH HAND is set to ?same?
; if a differenthand is in focus then the value is set to ?different?
; if afocus hand cannot be identified in one or both NPs, thenthe value is set to ?missing.?
This multi-valued feature isautomatically converted into a set of boolean features, sothat all features can be represented as binary variables.2.2 Coreference Resolution Algorithm(McCallum and Wellner, 2004) formulates coreferenceresolution as a Conditional Random Field, where men-tions are nodes, and their similarities are represented asweighted edges.
Edge weights range from ??
to ?,with larger values indicating greater similarity.
The op-timal solution is obtained by partitioning the graph intocliques such that the sum of the weights on edges withincliques is maximized, and the sum of the weights onedges between cliques is minimized:y?
= argmaxy?i,j,i6=jyi,js(xi, xj) (1)In equation 1, x is a set of mentions and y is a corefer-ence partitioning, such that yi,j = 1 if mentions xi and xjcorefer, and yi,j = ?1 otherwise.
s(xi, xj) is a similarityscore computed on mentions xi and xj .Computing the optimal partitioning y?
is equivalent tothe problem of correlation clustering, which is known tobe NP-hard (Demaine and Immorlica, to appear).
De-maine and Immorlica (to appear) propose an approxima-tion using integer programming, which we are currentlyinvestigating.
However, in this research we use average-link clustering, which hierarchically groups the mentionsx, and then forms clusters using a cutoff chosen to maxi-mize the f-measure on the training set.We experiment with both pipeline and joint models forcomputing s(xi, xj).
In the pipeline model, s(xi, xj) isthe posterior of a classifier trained on pairs of mentions.The advantage of this approach is that any arbitrary clas-sifier can be used; the downside is that minimizing the er-ror on all pairs of mentions may not be equivalent to min-imizing the overall error of the induced clustering.
Forexperiments with the pipeline model, we found best re-sults by boosting shallow decision trees, using the Wekaimplementation (Witten and Frank, 1999).Our joint model is based on McCallum and Well-ner?s (2004) adaptation of the voted perceptron to corefer-ence resolution.
Here, s is given by the product of a vec-tor of weights ?
with a set of boolean features ?
(xi, xj)induced from the pair of noun phrases: s(xi, xj) =??
(xi, xj).
The maximum likelihood weights can be ap-proximated by a voted perceptron, where, in the iterationt of the perceptron training:?t = ?t?1 +?i,j,i6=j?
(xi, xj)(y?i,j ?
y?i,j) (2)In equation 2, y?
is the ground truth partitioning fromthe labeled data.
y?
is the partitioning that maximizesequation 1 given the set of weights ?t?1.
As before,average-link clustering with an adaptive cutoff is used topartition the graph.
The weights are then averaged acrossall iterations of the perceptron, as in (Collins, 2002).3 EvaluationThe results of our experiments are computed usingmention-based CEAF scoring (Luo, 2005), and are re-ported in Table 2.
Leave-one-out evaluation was used toform 16 cross-validation folds, one for each document inthe corpus.
Using a planned, one-tailed pairwise t-test,the gesture features improved performance significantly38MARKABLE DIST The number of markables between the candidate NPsEXACT MATCH True if the candidate NPs have identical surface formsSTR MATCH True if the candidate NPs match after removing articlesNONPRO MATCH True if the candidate NPs are not pronouns and have identical surface formsNUMBER MATCH True if the candidate NPs agree in numberPRONOUN True if the NP is a pronounDEF NP True if the NP begins with a definite article, e.g.
?the box?DEM NP True if the NP is not a pronoun and begins with the word ?this?INDEF NP True if the NP begins an indefinite article, e.g.
?a box?pronouns Individual features for each of the four most common pronouns: ?this?, ?it?, ?that?, and?they?FOCUS DIST Distance between the position of the in-focus hand during j and i (see text)WHICH HAND Whether the hand in focus during j is the same as in i (see text)Table 1: The feature setSystem Feature set F1AdaBoost Gesture + Speech 54.9AdaBoost Speech only 52.8Voted Perceptron Gesture + Speech 53.7Voted Perceptron Speech only 52.9Baseline EXACT MATCH only 50.2Baseline None corefer 41.5Baseline All corefer 18.8Table 2: Resultsfor the boosted decision trees (t(15) = 2.48, p < .02),though not for the voted perceptron (t(15) = 1.07, p =.15).In the ?all corefer?
baseline, all NPs are grouped intoa single cluster; in the ?none corefer?, each NP gets itsown cluster.
In the ?EXACT MATCH?
baseline, two NPscorefer when their surface forms are identical.
All ex-perimental systems outperform all baselines by a statis-tically significant amount.
There are few other reportedresults for coreference resolution on spontaneous, uncon-strained speech; (Strube and Mu?ller, 2003) similarly findslow overall scores for pronoun resolution on the Switch-board Corpus, albeit by a different scoring metric.
Unfor-tunately, they do not compare performance to equivalentbaselines.For the AdaBoost method, 50 iterations of boosting areperformed on shallow decision trees, with a maximumtree depth of three.
For the voted perceptron, 50 trainingiterations were performed.
The performance of the votedperceptron on this task was somewhat unstable, varyingdepending on the order in which the documents werepresented.
This may be because a small change in theweights can lead to a very different partitioning, whichin turn affects the setting of the weights in the next per-ceptron iteration.
For these results, the order of presenta-tion of the documents was randomized, and the scores forthe voted perceptron are the average of 10 different runs(?
= 0.32% with gestures, 0.40% without).Although the AdaBoost method minimizes pairwiseerror rather than the overall error of the partitioning, itsperformance was superior to the voted perceptron.
Onepossible explanation is that by boosting small decisiontrees, AdaBoost was able to take advantage of non-linearcombinations of features.
We tested the voted perceptronusing all pairwise combinations of features, but this didnot improve performance.4 DiscussionIf gesture features play a role in coreference resolu-tion, then one might expect the probability of corefer-ence to vary significantly when conditioned on featuresdescribing the gesture.
As shown in Table 3, the pre-diction holds: the binned FOCUS DIST gesture featurehas the fifth highest ?2 value, and the relationship be-tween coreference and all gesture features was significant(?2 = 727.8, dof = 4, p < .01).
Note also that althoughFOCUS DIST ranks fifth, three of the features above itare variants of a string-match feature, and so are highlyredundant.The WHICH HAND feature is less strongly corre-lated with coreference, but the conditional probabilitiesdo correspond with intuition.
If the NPs corefer, thenthe probability of using the same hand to gesture duringboth NPs is 59.9%; if not, then the likelihood is 52.8%.The probability of not observing a focus hand is 20.3%when the NPs corefer, 25.1% when they do not; in otherwords, gesture is more likely for both NPs of a corefer-ent pair than for the NPs of a non-coreferent pair.
Therelation between the WHICH HAND feature and coref-erence is also significantly different from the null hypoth-esis (?2 = 57.2, dof = 2, p < .01).39Rank Feature ?21.
EXACT MATCH 1777.92.
NONPRO MATCH 1357.53.
STR MATCH 1201.84.
J = ?it?
732.85.
FOCUS DIST 727.86.
MARKABLE DIST 619.67.
J is PRONOUN 457.58.
NUMBER 367.99.
I = ?it?
238.610.
I is PRONOUN 132.611.
J is INDEF NP 79.312.
SAME FOCUS HAND 57.2Table 3: Top 12 Features By Chi-Squared5 Related WorkResearch on multimodality in the NLP communityhas usually focused on multimodal dialogue systems(e.g., (Oviatt, 1999)).
These systems differ fundamen-tally from ours in that they address human-computer in-teraction, whereas we address human-human interaction.Multimodal dialogue systems tackle interesting and dif-ficult challenges, but the grammar, vocabulary, and rec-ognized gestures are often pre-specified, and dialogue iscontrolled at least in part by the computer.
In our data, allof these things are unconstrained.Prosody has been shown to improve performance onseveral NLP problems, such as topic and sentence seg-mentation (e.g., (Shriberg et al, 2000)).
We are aware ofno equivalent work showing statistically significant im-provement on unconstrained speech using hand gesturefeatures.
(Nakano et al, 2003) shows that body posturepredicts turn boundaries, but does not show that thesefeatures improve performance beyond a text-only system.
(Chen et al, 2004) shows that gesture may improve sen-tence segmentation; however, in this study, the improve-ment afforded by gesture is not statistically significant,and evaluation was performed on a subset of their originalcorpus that was chosen to include only the three speakerswho gestured most frequently.
Still, this work provides avaluable starting point for the integration of gesture fea-ture into NLP systems.6 ConclusionWe have described how gesture features can be used toimprove coreference resolution on a corpus of uncon-strained speech.
Hand position and hand choice corre-late significantly with coreference, explaining this gain inperformance.
We believe this is the first example of handgesture features improving performance by a statisticallysignificant margin on unconstrained speech.ReferencesBreck Baldwin and Thomas Morton.
1998.
Dy-namic coreference-based summarization.
In Proc.
ofEMNLP.Lei Chen, Yang Liu, Mary P. Harper, and Eliza-beth Shriberg.
2004.
Multimodal model integra-tion for sentence unit detection.
In Proceedings ofInternational Conference on Multimodal Interfaces(ICMI?04).
ACM Press.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and experi-ments with perceptron algorithms.
In Proceedings ofEMNLP.Erik D. Demaine and Nicole Immorlica.
to appear.
Cor-relation clustering in general weighted graphs.
Theo-retical Computer Science.Jacob Eisenstein and Randall Davis.
2006.
Gesture fea-tures for coreference resolution.
In Workshop on Mul-timodal Interaction and Related Machine Learning Al-gorithms.Usama M. Fayyad and Keki B. Irani.
1993.
Multi-interval discretization of continuousvalued attributesfor classification learning.
In Proceedings of IJCAI-93.Xiaoqiang Luo.
2005.
On coreference resolution perfor-mance metrics.
In Proc.
of HLT-EMNLP, pages 25?32.Andrew McCallum and Ben Wellner.
2004.
Conditionalmodels of identity uncertainty with application to nouncoreference.
In Neural Information Processing Sys-tems.Yukiko Nakano, Gabe Reinstein, Tom Stocky, and Jus-tine Cassell.
2003.
Towards a model of face-to-facegrounding.
In Proceedings of ACL?03.Sharon L. Oviatt.
1999.
Mutual disambiguation ofrecognition errors in a multimodel architecture.
In Hu-man Factors in Computing Systems (CHI?99), pages576?583.Elizabeth Shriberg, Andreas Stolcke, Dilek Hakkani-Tur,and Gokhan Tur.
2000.
Prosody-based automatic seg-mentation of speech into sentences and topics.
SpeechCommunication, 32.Michael Strube and Christoph Mu?ller.
2003.
A machinelearning approach to pronoun resolution in spoken di-alogue.
In Proceedings of ACL ?03, pages 168?175.Ian H. Witten and Eibe Frank.
1999.
Data Mining: Prac-tical Machine Learning Tools and Techniques withJava Implementations.
Morgan Kaufmann.40
