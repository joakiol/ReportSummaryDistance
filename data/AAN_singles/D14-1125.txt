Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1181?1191,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational Linguistics+/-EffectWordNet: Sense-level Lexicon Acquisition for Opinion InferenceYoonjung Choi and Janyce WiebeDepartment of Computer ScienceUniversity of Pittsburghyjchoi, wiebe@cs.pitt.eduAbstractRecently, work in NLP was initiated on atype of opinion inference that arises whenopinions are expressed toward eventswhich have positive or negative effectson entities (+/-effect events).
This paperaddresses methods for creating a lexiconof such events, to support such work onopinion inference.
Due to significantsense ambiguity, our goal is to develop asense-level rather than word-level lexicon.To maximize the effectiveness of differenttypes of information, we combine agraph-based method using WordNet1relations and a standard classifier usinggloss information.
A hybrid between thetwo gives the best results.
Further, weprovide evidence that the model is aneffective way to guide manual annotationto find +/-effect senses that are not in theseed set.1 IntroductionOpinion mining (or sentiment analysis) identifiespositive or negative opinions in many kinds oftexts such as reviews, blogs, and news articles.
Ithas been exploited in many application areas suchas review mining, election analysis, and infor-mation extraction.
While most previous researchfocusses on explicit opinion expressions, recentwork addresses a type of opinion inference thatarises when opinions are expressed toward eventswhich have positive or negative effects on enti-ties (Deng et al., 2013; Deng and Wiebe, 2014).We call such events +/-effect events.2Deng andWiebe (2014) show how sentiments toward one1WordNet 3.0, http://wordnet.princeton.edu/2While the term goodFor/badFor is used in previous pa-pers (Deng et al., 2013; Deng and Wiebe, 2014; Deng et al.,2014), we have since decided that +/-effect is a better term.entity may be propagated to other entities viaopinion inference rules.
They give the followingexample:(1) The bill would curb skyrocketinghealth care costs.The writer expresses an explicit negative senti-ment (by skyrocketing) toward the object (healthcare costs).
The event, curb, has a negative effecton costs, since they are reduced.
We can reasonthat the writer is positive toward the event becauseit has a negative effect on costs, toward which thewriter is negative.
From there, we can reason thatthe writer is positive toward the bill, since it isthe agent of the positive event.
Deng and Wiebe(2014) show that such inferences may be exploitedto significantly improve explicit sentiment analy-sis systems.However, to achieve its results, the system de-veloped by Deng and Wiebe (2014) requires thatall instances of +/-effect events in the corpus bemanually provided as input.
For the system tobe fully automatic, it needs to be able to recog-nize +/-effect events automatically.
This paperaddresses methods for creating lexicons of suchevents, to support such work on opinion inference.We have discovered that there is significant senseambiguity, meaning that words often have mix-tures of senses among the classes +effect, -effect,and Null.
Thus, we develop a sense-level ratherthan word-level lexicon.One of our goals is to investigate whetherthe +/-effect property tends to be shared amongsemantically-related senses, and another is touse a method that applies to all word senses, notjust to the senses of words in a given word-levellexicon.
Thus, we build a graph-based model inwhich each node is a WordNet sense, and edgesrepresent semantic WordNet relations betweensenses.
In addition, we hypothesized that glossesalso contain useful information.
Thus, we develop1181a supervised gloss classifier and define a hybridmodel which gives the best overall performance.Finally, because all WordNet verb senses areincorporated into the model, we investigate theability of the method to identify unlabeled sensesthat are likely to be +/-effect senses.
We find thatby iteratively labeling the top-weighted unlabeledsenses and rerunning the model, it may be used asan effective method for guiding annotation efforts.2 BackgroundThere are many varieties of +/-effect events, in-cluding creation/destruction (changes in states in-volving existence), gain/loss (changes in statesinvolving possession), and benefit/injury (Anandand Reschke, 2010; Deng et al., 2013).
The cre-ation, gain, and benefit classes are +effect events.For example, baking a cake has a positive effect onthe cake because it is created;3increasing the taxrate has a positive effect on the tax rate; and com-forting the child has a positive effect on the child.The antonymous classes of each are -effect events:destroying the building has a negative effect on thebuilding; demand decreasing has a negative effecton demand; and killing Bill has a negative effecton Bill.4While sentiment (Esuli and Sebastiani, 2006;Wilson et al., 2005; Su and Markert, 2009) andconnotation lexicons (Feng et al., 2011; Kang etal., 2014) are related, sentiment, connotation, and+/-effects are not the same; a single event mayhave different sentiment and +/-effect polarities,for example.
Consider the following example:perpetrate:S: (v) perpetrate, commit, pull (performan act, usually with a negative connota-tion) ?perpetrate a crime?
; ?pull a bankrobbery?This sense of perpetuate has a negativeconnotation, and is an objective term inSentiWordNet.
However, it has a positiveeffect on the object, a crime, since performing acrime brings it into existence.3Deng et al.
(2013) point out that +/-effect objects are notequivalent to benefactive/malefactive semantic roles.
An ex-ample they give is She baked a cake for me: a cake is the ob-ject of the +effect event baked as just noted, while me is thefiller of its benefactive semantic role (Ziga and Kittil, 2010).4Their annotation manual, which gives additional cases, isavailable with the annotated data at http://mpqa.cs.pitt.edu/.As we mentioned, the +/-effect ambiguity can-not be avoided in a word-level lexicon.
In the+/-effect corpus of Deng et al.
(2013),5+/-effectevents and their agents and objects are annotatedat the word level.
In that corpus, 1,411 +/-effect in-stances are annotated; 196 different +effect wordsand 286 different -effect words appear in theseinstances.
Among them, 10 words appear inboth +effect and -effect instances, accounting for9.07% of all annotated instances.
They show that+/-effect events (and the inferences that motivatethis work) appear frequently in sentences with ex-plicit sentiment.
Further, all instances of +/-effectwords that are not identified as +/-effect events arefalse hits from the perspective of a recognition sys-tem.The following is an example of a word withsenses of different classes:purge:S: (v) purge (oust politically) ?DengXiao Ping was purged several timesthroughout his lifetime?
-effectS: (v) purge (clear of a charge) +effectS: (v) purify, purge, sanctify (make pureor free from sin or guilt) ?he left themonastery purified?
+effectS: (v) purge (rid of impurities) ?purgethe water?
; ?purge your mind?
+effectThis is part of the WordNet output for the wordpurge.
In the first sense, the polarity is -effectsince it has a negative effect on the object, DengXizo Ping.
However, the other cases have positiveeffect on the object.
Moreover, although a wordmay not have both +effect and -effect senses, itmay have mixtures of ((+effect or -effect) andNull).
A purely word-based approach is blind tothese cases.3 Related WorkLexicons are widely used in sentiment analysisand opinion mining.
Several works such as Hatzi-vassiloglou and McKeown (1997), Turney andLittman (2003), Kim and Hovy (2004), Strappar-ava and Valitutti (2004), and Peng and Park (2011)have tackled automatic lexicon expansion or ac-quistion.
However, in most such work, the lexi-cons are word-level rather than sense-level.5Called the goodFor/badFor corpus in that paper.1182For the related (but different) tasks of de-veloping subjectivity, sentiment and connota-tion lexicons, some do take a sense-level ap-proach.
Esuli and Sebastiani (2006) constructSentiWordNet.
They assume that terms withthe same polarity tend to have similar glosses.
So,they first expand a manually selected seed set ofsenses using WordNet lexical relations such asalso-see and direct antonymy and train two clas-sifiers, one for positive and another for negative.As features, a vectorial representation of glossesis adopted.
These classifiers were applied to allWordNet senses to measure positive, negative, andobjective scores.
In extending their work (Esuliand Sebastiani, 2007), the PageRank algorithm isapplied to rank senses in terms of how stronglythey are positive or negative.
In the graph, eachsense is one node, and two nodes are connectedwhen they contain the same words in their Word-Net glosses.
Moreover, a random-walk step isadopted to refine the scores in their recent work(Baccianella et al., 2010).
In contrast, our ap-proach uses WordNet relations and graph propa-gation in addition to gloss classification.Gyamfi et al.
(2009) construct a classifier to la-bel the subjectivity of word senses.
The hierarchi-cal structure and domain information in WordNetare exploited to define features in terms of sim-ilarity (using the LCS metric in Resnik (1995))of target senses and a seed set of senses.
Also,the similarity of glosses in WordNet is consid-ered.
Even though they investigated the hierarchi-cal structure by LCS values, WordNet relations arenot exploited directly.Su and Markert (2009) adopt a semi-supervisedmincut method to recognize the subjectivity ofword senses.
To construct a graph, each node cor-responds to one WordNet sense and is connectedto two classification nodes (one for subjectivityand another for objectivity) via a weighted edgethat is assigned by a classifier.
For this classifier,WordNet glosses, relations, and monosemousfeatures are considered.
Also, several WordNetrelations (e.g., antonymy, similiar-to, directhypernym, etc.)
are used to connect two nodes.Although they make use of both WordNet glossesand relations, and gloss information is utilizedfor a classifier, this classifier is generated onlyfor weighting edges between sense nodes andclassification nodes, not for classifying all senses.Kang et al.
(2014) present a unified model thatassigns connotation polarities to both words andsenses.
They formulate the induction process ascollective inference over pairwise-Markov Ran-dom Fields, and apply loopy belief propagationfor inference.
Their approach relies on selectionalpreferences of connotative predicates; the polarityof a connotation predicate suggests the polarity ofits arguments.
We have not discovered an analo-gous type of predicate for the problem we address.Goyal et al.
(2010) generate a lexicon of patientpolarity verbs (PPVs) that impart positive or neg-ative states on their patients.
They harvest PPVsfrom a Web corpus by co-occurance with Kind andEvil agents and by bootstrapping over conjunc-tions of verbs.
Riloff et al.
(2013) learn positivesentiment phrases and negative situation phrasesfrom a corpus of tweets with hashtag ?sarcasm?.However, both of these methods are word-levelrather than sense-level.Ours is the first NLP research into developinga sense-level lexicon for events that have negativeor positive effects on entities.4 +/-Effect Word-Level Seed Lexiconand Sense AnnotationsTo create the corpus used in this work, we devel-oped a word-level seed lexicon, and then manuallyannotated all the senses of the words in that lexi-con.FrameNet6is based on a theory of meaningcalled Frame Semantics.
In FrameNet, a LexicalUnit (LU) is a pairing of a word with a meaning,i.e., it corresponds to a sense in WordNet.
EachLU of a polysemous word belongs to a differentsemantic frame, which is a description of a typeof event, relation, or entity and, where appropri-ate, its participants.
For instance, in the Creatingframe, the definition is that a Cause leads to theformation of a Created entity.
It has a positiveeffect on the object, Created entity.
This framecontains about 10 LUs such as assemble, create,yield, and so on.
FrameNet consists of about 1,000semantic frames and about 10,000 LUs.FrameNet is a useful resource to select +/-effectwords since each semantic frame covers multi-ple LUs.
We believe that using FrameNet tofind +/-effect words is easier than finding +/-effectwords without any information since words may6FrameNet, https://framenet.icsi.berkeley.edu/fndrupal/1183be filtered by semantic frames.
To select +/-effectwords, an annotator (who is not a co-author) firstidentified promising frames as +/-effect and ex-tracted all LUs from them.
Then, he went throughthem and picked out the LUs which he judged tobe +effect or -effect.
In total, 736 +effect LUs and601 -effect LUs were selected from 463 semanticframes.While Deng et al.
(2013) and Deng and Wiebe(2014) specifically focus on events affecting ob-jects (i.e., themes), we do not want to limit thelexicon to only that case.
Sometimes, events havepositive or negative effects on agents or other en-tities as well.
Thus, in this paper, we considera sense to be +effect (-effect) if it has +effect(-effect) on an entity, which may be the agent, thetheme, or some other entity.In a previous paper (Choi et al., 2014), we con-ducted a study of the sense-level +/-effect prop-erty.
For the evaluation, two annotators (whoare co-authors of that paper) independently anno-tated senses of selected words, where some arefrom pure +effect (-effect) words (i.e., all sensesof the words are classified into the same class)and some are from mixed words (i.e., the wordshave both +effect and -effect senses).
In the agree-ment study, we calculated percent agreement and?
(Artstein and Poesio, 2008), and achieved 0.84percent agreement and 0.75 ?
value.For a seed set and an evaluation set in this pa-per, we need annotated sense-level +/-effect data.Mappings between FrameNet and WordNet arenot perfect.
Thus, we opted to manually anno-tate the senses of the words in the word-level lexi-con.
We first extracted all words from 736 +effectLUs and 601 -effect LUs; this extracts 606 +effectwords and 537 -effect words (the number of wordsis smaller than the number of LUs because oneword can have more than one LU).
Among them,14 words (e.g., crush, order, etc.)
are in both the+effect word set and the -effect word set.
That is,these words have both +effect and -effect mean-ings.
Recall that this annotator was focusing onframes, not on words - he did not look at all thesenses of all the words.
As we will see just below,when all the senses of all the words are annotated,a much higher percentage of the words have both+effect and -effect senses.
We will also see thatmany of the senses are revealed to be Null, show-ing that +effect vs. Null and -effect vs. Null ambi-guities are quite prevalent.A different annotator (a co-author) then wentthrough all senses of all the words from the pre-vious step and manually annotated each sense asto whether it is +effect, -effect, or Null.
Note thatthis annotator participated in an agreement studywith positive results in Choi et al.
(2014).For the experiments in this paper, we dividedthis annotated data into two equal-sized sets.
Oneis a fixed test set that is used to evaluate both thegraph model and the gloss classifier.
The other setis used as a seed set by the graph model, and as atraining set by the gloss classifer.
Table 1 showsthe distribution of the data.
In total, there are 258+effect senses, 487 -effect senses, and 880 Nullsenses.
To avoid too large a bias toward the Nullclass,7we randomly chose half (i.e., the Null setcontains 440 senses).
Half of each set is used asseed and training data, and the other half is usedfor evaluation.+effect -effect Null# annotated data 258 487 880# Seed/TrainSet 129 243 220# TestSet 129 244 220Table 1: Distribution of annotated data.5 Graph-based Semi-SupervisedLearning for WordNet RelationsWordNet (Miller et al., 1990) is organized by se-mantic relations such as hypernymy, troponymy,grouping, and so on.
These semantic relations canbe used to build a network.
Since the most fre-quently encoded relation is the super-subordinaterelation, most verb senses are arranged into hi-erarchies; verb senses towards the bottom of thegraph express increasingly specific manner.
Thus,by following this hierarchical information, we hy-pothesized that +/-effect polarity tends to propa-gate.
We use a graph-based semi-supervised learn-ing (GSSL) method to carry out the label propaga-tion.5.1 Graph FormulationWe formulate a graph for semi-supervised learningas follows.
Let G = {X,E,W} be the undirectedgraph in which X is the set of nodes, E is the set7As mentioned in the introduction, we want our methodto be able to identify unlabeled senses that are likely to be+/-effect senses (see Section 8); we resize the Null class tosupport this goal.1184of edges, and W represents the edge weights (i.e.,the weight of edge Eijis Wij).
The weight matrixis a non-negative matrix.Each data point in X = {x1, ... ,xn} is onesense.
The labeled data of X is represented asXL= {x1, ... ,xl} and the unlabeled data is rep-resented as XU= {xl+1, ... ,xn}).
The labeleddata XLis associated with labels YL= {y1, ...,yl}, where yi?
{1, ..., c} (c is the number ofclasses).
As is typical in such settings, l  n:n is 13,767, i.e., the number of verb senses inWordNet.
Seed/TrainSet in Table 1 is the labeleddata.To connect two nodes, WordNet relations areutilized.
We first connect nodes by the hierar-chical relations.
Since hypernym relations repre-sent more general senses and troponym relationsrepresent more specific verb senses, we hypothe-sized that hypernyms or troponyms of a verb sensetends to have its same polarity.
Verb groups rela-tions that represent verb senses having a similarmeaning are also promising.
Even though verb-group coverage is not large, its relations are reli-able since they are manually grouped.
The entail-ment relation is defined as the verb Y is entailedby X if you must be doing Y by doing X .
Sincepairs connected by this relation are co-extensive,we can assume that both are the same type ofevent.
The synonym relation is not used becauseit is already defined in senses (i.e., each node inthe graph is a synset), and the antonym relation isalso not applied since the weight matrix should benon-negative.
The weight value of all edges is 1.0.5.2 Label PropagationGiven a constructed graph, the label inference (orprediction) task is to propagate the seed labels tothe unlabeled nodes.
One of the classic GSSL la-bel propagation methods is the local and globalconsistency (LGC) method suggested by Zhou etal.
(2004).
The LGC method is a graph transduc-tion algorithm which is sufficiently smooth withrespect to the intrinsic structure revealed by knownlabeled and unlabeled data.
The cost function typ-ically involves a tradeoff between the smoothnessof the predicted labels over the entire graph andthe accuracy of the predicted labels in fitting thegiven labeled nodes XL.
LGC fits in a univariateregularization framework, where the output ma-trix is treated as the only variable in optimization,and the optimal solutions can be easily obtained bysolving a linear system.
Thus, we adopt the LGCmethod in this paper.
Although there are some ro-bust GSSL methods for handling noisy labels, wedo not need to handle noisy labels because our in-put is the annotated data.Let F be a n ?
c matrix to save the outputvalues of label propagation.
So, we can labeleach instance xias a label yi= argmaxj?cFijafter the label propagation.
The initial discrete la-bel matrix Y , which is also n ?
c, is defined asYij= 1 if xiis labeled as yi= j in YL, andYij= 0 otherwise.
The vertex degree matrixD = diag([D11, ..., Dnn]) is defined by Dii=?nj=1Wij.LGC defines the cost function Q which inte-grates two penalty components, global smooth-ness and local fitting (?
is the regularization pa-rameter):Q =12n?i=1n?j=1Wij?Fi?Dii?Fj?Djj?2+?n?i=1?Fi?
Yi?2The first part of the cost function is thesmoothness constraint: a good classifying func-tion should not change too much between nearbypoints.
That is, if xiand xjare connected withan edge, the difference between them should besmall.
The second is the fitting constraint: a goodclassifying function should not change too muchfrom the initial label assignment.
The final labelprediction matrix F can be obtained by minimiz-ing the cost function Q.5.3 Experimental ResultsNote that, in the rest of this paper, all tables exceptthe last one give results on the same fixed test set(TestSet in Table 1).We can apply the graph model in two ways.?
UniGraph: All three classes (+effect, -effect,and Null) are represented in one graph.?
BiGraph: Two separate graphs are first con-structed and then combined.
One graph is forclassifying +effect and Other (i.e., -effect orNull).
This graph is called +eGraph.
Theother graph, called -eGraph, is for classify-ing -effect and Other (i.e., +effect or Null).1185UniGraph BiGraph BiGraph*baseline-0.411accuracyaccuracy 0.630 0.623 0.658+effect P 0.621 0.610 0.642R 0.655 0.647 0.680F 0.637 0.628 0.660-effect P 0.644 0.662 0.779R 0.720 0.677 0.612F 0.680 0.670 0.686Null P 0.615 0.583 0.583R 0.516 0.550 0.695F 0.561 0.561 0.634Table 2: Results of UniGraph, BiGraph, and Bi-Graph*.These are combined into one model as fol-lows.
Nodes that are labeled as +effect by+eGraph and Other by -eGraph are regardedas +effect, and nodes that are labeled as-effect by -eGraph and Other by +eGraph areregarded as -effect.
If nodes are labeled as+effect by +eGraph and -effect by -eGraph,they are deemed to be Null.
Nodes that arelabeled Other by both graphs are also consid-ered as Null.We had two motivations for experimentingwith the BiGraph model: (1) SVM, the super-vised learning method used for gloss classifica-tion, tends to have better performance on binaryclassification tasks, and (2) the two graphs of thecombined model can ?negotiate?
with each othervia constraints.In Table 2, we calculate precision (P), recall (R),and f-measure (F) for all three classes.
The base-line shown in the top row is the accuracy of a ma-jority class classifier.
The first two columns of Ta-ble 2 show the results of UniGraph and BiGraphwhen they are built using the hypernym, troponym,and verb group relations.
UniGraph outperformsBiGraph in this experiment.To improve the results by performing some-thing possible with BiGraph (but not UniGraph),constraints are added when determining the class.As we explained, the label of instance xiisdetermined by Fiin the graph.
When the labelof xiis decided to be j, we can say that its con-fidence value is Fij.
There are two constraints asfollows.H+T +V +E+effect P 0.653 0.642 0.651R 0.660 0.680 0.683F 0.656 0.660 0.667-effect P 0.784 0.779 0.786R 0.547 0.612 0.604F 0.644 0.686 0.683Null P 0.557 0.583 0.564R 0.735 0.695 0.691F 0.634 0.634 0.621Table 3: Effect of each relation?
If a sense is labeled as +effect (-effect), butthe confidence value is less than a threshold,we count it as Null.?
If a sense is labeled as both +effect and -effectby BiGraph, we choose the label with thehigher confidence value only if the higher oneis larger than a threshold and the lower one isless than a threshold.The thresholds are determined on Seed/TrainSetby running BiGraph several times with differentthresholds, and choosing the one that gives thebest performance on Seed/TrainSet.
(The chosenvalue is 0.025 for +effect and 0.03 for -effect).As can be seen in Table 2, BiGraph with con-straints (called BiGraph*) outperforms not onlyBiGraph without any constraints but also Uni-Graph.
Especially, for BiGraph*, the recall of theNull class is considerably increased, showing thatconstraints not only help overall, but are particu-larly important for detecting Null cases.Table 3 gives ablation results, showing the con-tribution of each WordNet relation in BiGraph*.With only hierarchical information (i.e., hyper-nym (H) and troponym (T) relations), it alreadyshows good performance for all classes.
How-ever, they cannot cover some senses.
Among the13,767 verb senses in WordNet, 1,707 (12.4%)cannot be labeled because there are not sufficienthierarchical links to propagate polarity informa-tion.
When adding the verb group (+V) rela-tion, it shows improvement in both +effect and-effect.
Especially, the recall for +effect and-effect is significantly increased.
In addition, thecoverage of the 13,767 verb senses increases to95.1%.
For entailment (+E), whereas adding itshows a slight improvement in +effect (and in-creases coverage by 1.1 percentage points), the1186performance is decreased a little bit in the -effectand Null classes.
Since the average f-measure forall classes is the highest with hypernym (H), tro-ponym (T), and verb group (V) relations (not en-tailment), we only consider these three relationswhen constructing the graph.6 Supervised Learning applied toWordNet GlossesIn WordNet, each sense contains a gloss consist-ing of a definition and optional example sentences.Since a gloss consists of several words and thereare no direct links between glosses, we believe thata word vector representation is appropriate to uti-lize gloss information as in Esuli and Sebastiani(2006).
For that, we adopt an SVM classifier.6.1 FeaturesTwo different feature types are used.Word Features (WF): The bag-of-wordsmodel is applied.
We do not ignore stop wordsfor several reasons.
Since most definitions and ex-amples are not long, each gloss contains a smallnumber of words.
Also, among them, the total vo-cabulary of WordNet glosses is not large.
More-over, some prepositions such as against are some-times useful to determine the polarity (+effect or-effect).Sentiment Features (SF): Some glosses of+effect (-effect) senses contain positive (negative)words.
For instance, the definition of {hurt#4,injure#4} is ?cause damage or affect negatively.
?It contains a negative word, negatively.
Since agiven event may positively (negatively) affect enti-ties, some definitions or examples already containpositive (negative) words to express this.
Thus, asfeatures, we check how many positive (negative)words a given gloss contains.
To detect sentimentwords, the subjectivity lexicon provided by Wil-son et al.
(2005)8is utilized.6.2 Gloss ClassifierWe have three classes, +effect, -effect, and Null.Since SVM shows better performance on binaryclassification tasks, we generate two binary clas-sifiers, one (+eClassifier) to determine whethera given sense is +effect or Other, and another(-eClassifier) to classify whether a given sense is-effect or Other.
Then, they are combined as inBiGraph.8Available at http://mpqa.cs.pitt.edu/6.3 Experimental ResultsSeed/TrainSet in Table 1 is used to train the twoclassifiers, and TestSet is utilized for the evalua-tion.
So, the training set for +eClassifier consistsof 129 +effect instances and 463 Other instances,and the training set for -eClassifier contains 243-effect instances and 349 Other instances.
As abaseline, we adopt a majority class classifier.Table 4 shows the results on TestSet.
Perfor-mance is better for the -effect than for the +effectclass, perhaps because the -effect class has moreinstances.When sentiment features (SF) are added,all metric values increase, providing evidencethat sentiment features are helpful to determine+/-effect classes.WF WF+SFbaseline accuracy 0.411accuracy 0.509 0.539+effect P 0.541 0.588R 0.354 0.393F 0.428 0.472-effect P 0.616 0.672R 0.500 0.511F 0.552 0.580Null P 0.432 0.451R 0.612 0.657F 0.507 0.535Table 4: Results of the gloss classifier.7 Hybrid MethodTo use more combined knowledge, the gloss clas-sifier and BiGraph* can be combined.
That is, forWordNet gloss information, the gloss classifier isutilized, and for WordNet relations, BiGraph* isused.
With the Hybrid method, we can see notonly the effect of propagation by WordNet rela-tions but also the usefulness of gloss informationand sentiment features.
Also, while BiGraph*cannot cover all senses in WordNet, the Hybridmethod can.The outputs of the gloss classifier and Bi-Graph* are combined as follows.
The label ofthe gloss classifier is one of +effect, -effect, Null,or Both (when a given sense is classified as both+effect by +eClassifier and -effect by -eClassifier).Possible labels of BiGraph* are +effect, -effect,Null, Both, or None (when a given sense is not1187labeled by BiGraph*).
There are five rules:?
If both labels are +effect (-effect), it is +effect(-effect).?
If one of them is Both and the other is +effect(-effect), it is +effect (-effect).?
If the label of BiGraph* is None, believe thelabel of the gloss classifier?
If both labels are Both, it is Null?
Otherwise, it is NullThe results for Hybrid are given in the firstrow of the lower half of Table 5; the results forBiGraph* are in the first row of the upper half,for comparison.
Generally, the Hybrid methodshows better performance than the gloss classifierand BiGraph*.
In the Hybrid method, since more+/-effect senses are detected than by BiGraph*,while precision is decreased, recall is increasedby more.
However, by the same token, the over-all performance for the Null class is decreased.Actually, that is expected since the Null class isdetermined by the Other class in the gloss clas-sifier and BiGraph*.
Through this experiment, wesee that the Hybrid method is better for classifying+/-effect senses.7.1 Model ComparisonTo provide evidence for our assumption that dif-ferent models are needed for different informationto maximize effectiveness, we compare the hy-brid method with the supervised learning and thegraph-based learning (GSSL) methods, each uti-lizing both WordNet relations and gloss informa-tion.Supervised Learning (onlySL): The gloss clas-sifier is trained with word features and sentimentfeatures for WordNet Gloss.
To exploit Word-Net relations in supervised learning, especiallythe hierarchical information, we use least com-mon subsumer (LCS) values as in Gyamfi et al.
(2009), which, recall, performs supervised learn-ing of subjective/objective senses.
The values arecalculated as follows.
For a target sense t and aseed set S, the maximum LCS value between atarget sense and a member of the seed set is foundas:Score(t, S) = maxs?SLCS(t, s)With this LCS feature and the features describedin Section 6, we run SVM on the same training andtest data.
For LCS values, the similarity using theinformation content proposed by Resnik (1995) ismeasured.
WordNet Similarity9package providespre-computed pairwise similarity values for that.Table 6 shows results of onlySL.
Compared toTable 4, while +effect and Null classes show aslight improvement, the performance is degradedfor -effect.
This means that the added feature israther harmful to -effect.
Even though the hierar-chical feature is very helpful to expand +/-effect,it is not helpful for onlySL since SVM cannot cap-ture propagation according to the hierarchy.Graph-based Learning (onlyGraph): In Sec-tion 5, the graph is constructed by using Word-Net relations.
To apply WordNet gloss informa-tion in onlyGraph, we calculate a cosine similaritybetween glosses.
If the similarity value is higherthan a threshold, two nodes are connected with thissimilarity value.
The threshold is determined bytraining and testing on Seed/TrainSet (the chosenvalue is 0.3).Comparing Tables 2 and 6, BiGraph* generallyoutperforms onlyGraph (the exception is precisionof +effect).
By gloss similarity, many nodes areconnected to each other.
However, since uncertainconnections can cause incorrect propagation in thegraph, this negatively affects the performance.Through this experiment, we see that since eachtype of information has a different character, weneed different models to maximize the effective-ness of each type.
Thus, the hybrid method withdifferent models can have better performance.Hybrid onlySL onlyGraph+effect P 0.610 0.584 0.701R 0.735 0.400 0.364F 0.667 0.475 0.480-effect P 0.717 0.778 0.651R 0.669 0.316 0.562F 0.692 0.449 0.603Null P 0.556 0.440 0.473R 0.520 0.813 0.679F 0.538 0.571 0.557Table 6: Comparison to onlySL and onlyGraph.9WordNet Similarity,http://wn-similarity.sourceforge.net/1188+effect -effect NullP R F P R F P R FBiGraph* Initial 0.642 0.680 0.660 0.779 0.612 0.686 0.583 0.695 0.6341st 0.636 0.684 0.663 0.770 0.632 0.694 0.591 0.672 0.6292nd 0.642 0.701 0.670 0.748 0.656 0.699 0.605 0.655 0.6293rd 0.636 0.708 0.670 0.779 0.652 0.710 0.599 0.669 0.6324th 0.681 0.674 0.678 0.756 0.674 0.712 0.589 0.669 0.626Hybrid Initial 0.610 0.735 0.667 0.717 0.669 0.692 0.556 0.520 0.5381st 0.614 0.713 0.672 0.728 0.681 0.704 0.562 0.523 0.5422nd 0.613 0.743 0.672 0.716 0.697 0.706 0.559 0.497 0.5263rd 0.616 0.739 0.672 0.717 0.706 0.712 0.559 0.494 0.5254th 0.688 0.681 0.684 0.712 0.764 0.732 0.565 0.527 0.545Table 5: Results of an iterative approach.8 Guided AnnotationRecall that Seed/TrainSet and TestSet, the dataused so far, are all the senses of the words in aword-level +/-effect lexicon.
This section presentsevidence that our method can guide annotation ef-forts to find other words that have +/-effect senses.A bonus is that the method pinpoints particular+/-effect senses of those words.All unlabeled data are senses of words that arenot included in the original lexicon.
Since pre-sumably the majority of verbs do not have any+/-effect senses, a sense randomly selected fromWordNet is very likely to be Null.
We explore aniterative approach to guided annotation, using Bi-Graph* and Hybrid as the method for assigninglabels.The system is initially created as describedabove using Seed/TrainSet as the initial seed set.Each iteration has four steps: 1) rank all unlabeleddata (i.e., the data other than TestSet and the cur-rent seed set) based on the Fijconfidence values(see Section 5.3); 2) choose the top 5% and manu-ally annotate them (the same annotator as abovedid this); 3) add them to the seed set; 4) rerunthe system using the expanded seed set.
We per-formed four iterations in this paper.The upper and lower parts of Table 5 show theintial results and the results after each iteration forBiGraph* and Hybrid.
Recall that these are resultson the fixed set, TestSet.
Overall for both mod-els, f-measure increases for both the +effect and-effect classes as more seeds are added, mainlydue to improvements in recall.
The evaluation onthe fixed set is also useful in the annotation processbecause it trades off +/-effect vs. Null annotations.If the new manual annotations were biased, in thatthey incorrectly label Null senses as +/-effect, thenthe f-measure results would instead degrade on thefixed TestSet, since the system is created each timeusing the increased seed set.We now consider the accuracy of the systemon the newly labeled annotated data in Step 2.Note that our method is similar to Active Learn-ing (Tong and Koller, 2001), in that both auto-matically identify which unlabeled instances thehuman should annotate next.
However, in activelearning, the goal is to find instances that are diffi-cult for a supervised learning system.
In our case,the goal is to find needles in the haystack of Word-Net senses.
In Step 3, we add the newly labeledsenses to the seed set, enabling the model to findunlabeled senses close to the new seeds when thesystem is rerun for the next iteration.We assess the system?s accuracy on the newlylabeled data by comparing the system?s labels withthe human?s new labels.
Accuracy for +effect and-effect is calculated such as:Accuracy+effect=# annotated +effect# top 5% +effect dataAccuracy?effect=# annotated -effect# top 5% -effect dataThat is, the accuracy means that out of the top 5%of the +effect (-effect) data as scored by the sys-tem, what percentage are correct as judged by ahuman annotator.
Table 7 shows the accuracy foreach iteration in the top part and the number ofsenses labeled in the bottom part.
As can be seen,the accuracies range between 60% and 78%; these1189values are much higher than what would be ex-pected if labeling senses of words randomly cho-sen from WordNet.10The annotator spent, on av-erage, approximately an hour to label 100 senses.For finding new words with +/-effect usages, itwould be much more cost-effective if a significantpercentage of the data chosen for annotation aresenses of words that in fact have +/-effect senses.1st 2nd 3rd 4th+effect 65.63% 62.50% 63.79% 59.83%-effect 73.55% 73.97% 77.78% 70.30%+effect 128 122 116 117-effect 155 146 153 145total 283 268 269 262Table 7: Accuracy and frequency of the top 5% foreach iteration9 Conclusion and Future WorkIn this paper, we investigated methods for creat-ing a sense-level +/-effect lexicon.
To maximizethe effectiveness of each type of information, wecombined a graph-based method using WordNetrelations and a standard classifier using gloss in-formation.
A hybrid between the two gives thebest results.
Further, we provide evidence that themodel is an effective way to guide manual anno-tation to find +/-effect words that are not in theseed word-level lexicon.
This is important, as thelikelihood that a random WordNet sense (and thusword) is +effect or -effect is not large.So as not to limit the inferences that may bedrawn, our annotations include events that are+effect or -effect either the agent or object.
In fu-ture work, we plan to exploit corpus-based meth-ods using patterns as in Goyal et al.
(2010) com-bined with semantic role labeling to refine the lex-icon to distinguish which is the affected entity.Further, to actually exploit the acquired lexicon toprocess corpus data, an appropriate coarse-grainedsense disambiguation process must be added, asAkkaya et al.
(2009) and Akkaya et al.
(2011) didfor subjective/objective classification.We hope the general methodology will be ef-fective for other semantic properties.
In opin-ion mining and sentiment analysis this is partic-10For reference, in 5th iteration, the +effect accuracy is60.18% and the -effect accuracy is 69.93%, and in 6th itera-tion, the +effect accuracy is 59.81% and the -effect accuracyis 69.12%.ularly needed, because different meanings of pos-itive and negative are appropriate for different ap-plications.
This is a way to create lexicons that arecustomized with respect to one?s own definitions.It would be promising to combine our methodwith other methods to enable it to find +effectand -effect senses that are outside the coverageof WordNet.
However, a WordNet-based lexicongives a substantial base to build from.AcknowledgmentsThis work was supported in part by DARPA-BAA-12-47 DEFT grant #12475008 and National Sci-ence Foundation grant #IIS-0916046.
We wouldlike to thank the reviewers for their helpful sug-gestions and comments.ReferencesCem Akkaya, Janyce Wiebe, and Rada Mihalcea.2009.
Subjectivity word sense disambiguation.
InProceedings of EMNLP 2009, pages 190?199.Cem Akkaya, Janyce Wiebe, Alexander Conrad, andRada Mihalcea.
2011.
Improving the impact of sub-jectivity word sense disambiguation on contextualopinion analysis.
In Proceedings of CoNLL 2011,pages 87?96.Pranna Anand and Kevin Reschke.
2010.
Verb classesas evaluativity functor classes.
In InterdisciplinaryWorkshop on Verbs.
The Identification and Repre-sentation of Verb Features.Ron Artstein and Massimo Poesio.
2008.
Inter-coderagreement for computational linguistics.
Comput.Linguist., 34(4):555?596.Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-tiani.
2010.
Sentiwordnet 3.0: An enhanced lexicalresource for sentiment analysis and opinion mining.In Proceedings of LREC, pages 2200?2204.Yoonjung Choi, Lingjia Deng, and Janyce Wiebe.2014.
Lexical acquisition for opinion inference:A sense-level lexicon of benefactive and malefac-tive events.
In Proceedings of the 5th Workshopon Computational Approaches to Subjectivity, Sen-timent and Social Media Analysis (WASSA), pages107?112.
Association for Computational Linguis-tics.Lingjia Deng and Janyce Wiebe.
2014.
Sentimentpropagation via implicature constraints.
In Proceed-ings of EACL.Lingjia Deng, Yoonjung Choi, and Janyce Wiebe.2013.
Benefactive/malefactive event and writer atti-tude annotation.
In Proceedings of 51st ACL, pages120?125.1190Lingjia Deng, Janyce Wiebe, and Yoonjung Choi.2014.
Joint inference and disambiguation of implicitsentiments via implicature constraints.
In Proceed-ings of COLING, page 7988.Andrea Esuli and Fabrizio Sebastiani.
2006.
Senti-wordnet: A publicly available lexical resource foropinion mining.
In Proceedings of 5th LREC, pages417?422.Andrea Esuli and Fabrizio Sebastiani.
2007.
Pager-anking wordnet synsets: An application to opinionmining.
In Proceedings of ACL, pages 424?431.Song Feng, Ritwik Bose, and Yejin Choi.
2011.
Learn-ing general connotation of words using graph-basedalgorithms.
In Proceedings of EMNLP, pages 1092?1103.Amit Goyal, Ellen Riloff, and Hal DaumeIII.
2010.Automatically producing plot unit representationsfor narrative text.
In Proceedings of EMNLP, pages77?86.Yaw Gyamfi, Janyce Wiebe, Rada Mihalcea, and CemAkkaya.
2009.
Integrating knowledge for subjectiv-ity sense labeling.
In Proceedings of NAACL HLT2009, pages 10?18.Vasileios Hatzivassiloglou and Kathleen R. McKeown.1997.
Predicting the semantic orientation of adjec-tives.
In Proceedings of ACL, pages 174?181.Jun Seok Kang, Song Feng, Leman Akoglu, and YejinChoi.
2014.
Connotationwordnet: Learning conno-tation over the word+sense network.
In Proceedingsof the 52nd ACL, page 15441554.Soo-Min Kim and Eduard Hovy.
2004.
Determiningthe sentiment of opinions.
In Proceedings of 20thCOLING, pages 1367?1373.George A. Miller, Richard Beckwith, Christiane Fell-baum, Derek Gross, and Katherine Miller.
1990.Wordnet: An on-line lexical database.
InternationalJournal of Lexicography, 13(4):235?312.Wei Peng and Dae Hoon Park.
2011.
Generate adjec-tive sentiment dictionary for social media sentimentanalysis using constrained nonnegative matrix fac-torization.
In Proceedings of ICWSM.Philip Resnik.
1995.
Using information content toevaluate semantic similarity.
In Proceedings of 14thIJCAI, pages 448?453.Ellen Riloff, Ashequl Qadir, Prafulla Surve, Lalin-dra De Silva, Nathan Gilbert, and Ruihong Huang.2013.
Sarcasm as contrast between a positive sen-timent and negative situation.
In Proceedings ofEMNLP, pages 704?714.Carlo Strapparava and Alessandro Valitutti.
2004.Wordnet-affect: An affective extension of wordnet.In Proceedings of 4th LREC, pages 1083?1086.Fangzhong Su and Katja Markert.
2009.
Subjectiv-ity recognition on word senses via semi-supervisedmincuts.
In Proceedings of NAACL HLT 2009,pages 1?9.Simon Tong and Daphne Koller.
2001.
Support vectormachin active learning with applications to text clas-sification.
Journal of Machine Learning Research,2:45?66.Peter Turney and Michael Littman.
2003.
Measuringpraise and criticism: Inference of semantic orienta-tion from association.
ACM Transactions on Infor-mation Systems, 21(4):315?346.Theresa Wilson, Janyce Wiebe, , and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-level sentiment analysis.
In Proceedings of HLT-EMNLP, pages 347?354.Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal,Jason Weston, and Bernhard Scholkopf.
2004.Learning with local and global consistency.
Ad-vances in Neural Information Processing Systems,16:321?329.Fernando Ziga and Seppo Kittil.
2010.
Benefactivesand malefactives, Typological perspectives and casestudies.
John Benjamins Publishing.1191
