Parsing the WSJ using CCG and Log-Linear ModelsStephen ClarkSchool of InformaticsUniversity of Edinburgh2 Buccleuch Place, Edinburgh, UKstephen.clark@ed.ac.ukJames R. CurranSchool of Information TechnologiesUniversity of SydneyNSW 2006, Australiajames@it.usyd.edu.auAbstractThis paper describes and evaluates log-linearparsing models for Combinatory CategorialGrammar (CCG).
A parallel implementation ofthe L-BFGS optimisation algorithm is described,which runs on a Beowulf cluster allowing thecomplete Penn Treebank to be used for estima-tion.
We also develop a new efficient parsingalgorithm for CCG which maximises expectedrecall of dependencies.
We compare modelswhich use all CCG derivations, including non-standard derivations, with normal-form models.The performances of the two models are com-parable and the results are competitive with ex-isting wide-coverage CCG parsers.1 IntroductionA number of statistical parsing models have recentlybeen developed for Combinatory Categorial Gram-mar (CCG; Steedman, 2000) and used in parsers ap-plied to the WSJ Penn Treebank (Clark et al, 2002;Hockenmaier and Steedman, 2002; Hockenmaier,2003b).
In Clark and Curran (2003) we arguedfor the use of log-linear parsing models for CCG.However, estimating a log-linear model for a wide-coverage CCG grammar is very computationally ex-pensive.
Following Miyao and Tsujii (2002), weshowed how the estimation can be performed effi-ciently by applying the inside-outside algorithm toa packed chart.
We also showed how the completeWSJ Penn Treebank can be used for training by de-veloping a parallel version of Generalised IterativeScaling (GIS) to perform the estimation.This paper significantly extends our earlier workin a number of ways.
First, we evaluate a numberof log-linear models, obtaining results which arecompetitive with the state-of-the-art for CCG pars-ing.
We also compare log-linear models which useall CCG derivations, including non-standard deriva-tions, with normal-form models.
Second, we findthat GIS is unsuitable for estimating a model of thesize being considered, and develop a parallel ver-sion of the L-BFGS algorithm (Nocedal and Wright,1999).
And finally, we show that the parsing algo-rithm described in Clark and Curran (2003) is ex-tremely slow in some cases, and suggest an efficientalternative based on Goodman (1996).The development of parsing and estimation algo-rithms for models which use all derivations extendsexisting CCG parsing techniques, and allows us totest whether there is useful information in the addi-tional derivations.
However, we find that the perfor-mance of the normal-form model is at least as goodas the all-derivations model, in our experiments to-date.
The normal-form approach allows the use ofadditional constraints on rule applications, leadingto a smaller model, reducing the computational re-sources required for estimation, and resulting in anextremely efficient parser.This paper assumes a basic understanding ofCCG; see Steedman (2000) for an introduction, andClark et al (2002) and Hockenmaier (2003a) for anintroduction to statistical parsing with CCG.2 Parsing Models for CCGCCG is unusual among grammar formalisms in that,for each derived structure for a sentence, there canbe many derivations leading to that structure.
Thepresence of such ambiguity, sometimes referred toas spurious ambiguity, enables CCG to produce el-egant analyses of coordination and extraction phe-nomena (Steedman, 2000).
However, the introduc-tion of extra derivations increases the complexity ofthe modelling and parsing problem.Clark et al (2002) handle the additional deriva-tions by modelling the derived structure, in theircase dependency structures.
They use a conditionalmodel, based on Collins (1996), which, as the au-thors acknowledge, has a number of theoretical de-ficiencies; thus the results of Clark et al provide auseful baseline for the new models presented here.Hockenmaier (2003a) uses a model whichfavours only one of the derivations leading to aderived structure, namely the normal-form deriva-tion (Eisner, 1996).
In this paper we compare thenormal-form approach with a dependency model.For the dependency model, we define the probabil-ity of a dependency structure as follows:P(pi|S ) =?d??
(pi)P(d, pi|S ) (1)where pi is a dependency structure, S is a sentenceand ?
(pi) is the set of derivations which lead to pi.This extends the approach of Clark et al (2002)who modelled the dependency structures directly,not using any information from the derivations.
Incontrast to the dependency model, the normal-formmodel simply defines a distribution over normal-form derivations.The dependency structures considered in this pa-per are described in detail in Clark et al (2002)and Clark and Curran (2003).
Each argument slotin a CCG lexical category represents a dependencyrelation, and a dependency is defined as a 5-tuple?h f , f , s, ha, l?, where h f is the head word of the lex-ical category, f is the lexical category, s is the argu-ment slot, ha is the head word of the argument, andl indicates whether the dependency is long-range.For example, the long-range dependency encodingcompany as the extracted object of bought (as in thecompany that IBM bought) is represented as the fol-lowing 5-tuple:?bought, (S[dcl]\NP1)/NP2, 2, company, ?
?where ?
is the category (NP\NP)/(S[dcl]/NP) as-signed to the relative pronoun.
For local dependen-cies l is assigned a null value.
A dependency struc-ture is a multiset of these dependencies.3 Log-Linear Parsing ModelsLog-linear models (also known as Maximum En-tropy models) are popular in NLP because of theease with which discriminating features can be in-cluded in the model.
Log-linear models have beenapplied to the parsing problem across a range ofgrammar formalisms, e.g.
Riezler et al (2002) andToutanova et al (2002).
One motivation for usinga log-linear model is that long-range dependencieswhich CCG was designed to handle can easily be en-coded as features.A conditional log-linear model of a parse ?
?
?,given a sentence S , is defined as follows:P(?|S ) = 1ZSe?.
f (?)
(2)where ?.
f (?)
= ?i ?i fi(?).
The function fi is afeature of the parse which can be any real-valuedfunction over the space of parses ?.
Each featurefi has an associated weight ?i which is a parameterof the model to be estimated.
ZS is a normalisingconstant which ensures that P(?|S ) is a probabilitydistribution:ZS =?????
(S )e?.
f (??)
(3)where ?
(S ) is the set of possible parses for S .For the dependency model a parse, ?, is a ?d, pi?pair (as given in (1)).
A feature is a count of thenumber of times some configuration occurs in d orthe number of times some dependency occurs in pi.Section 6 gives examples of features.3.1 The Dependency ModelWe follow Riezler et al (2002) in using a discrimi-native estimation method by maximising the condi-tional likelihood of the model given the data.
For thedependency model, the data consists of sentencesS 1, .
.
.
, S m, together with gold standard dependencystructures, pi1, .
.
.
, pim.
The gold standard structuresare multisets of dependencies, as described earlier.Section 6 explains how the gold standard structuresare obtained.The objective function of a model ?
is the condi-tional log-likelihood, L(?
), minus a Gaussian priorterm, G(?
), used to reduce overfitting (Chen andRosenfeld, 1999).
Hence, given the definition of theprobability of a dependency structure (1), the objec-tive function is as follows:L?(?)
= L(?)
?G(?)
(4)= logm?j=1P?
(pi j|S j) ?n?i=1?2i2?2i=m?j=1log?d??
(pi j) e?.
f (d,pi j)????
(S j) e?.
f (?)
?n?i=1?2i2?2i=m?j=1log?d??
(pi j)e?.
f (d,pi j)?m?j=1log????
(S j)e?.
f (?)
?n?i=1?2i2?2iwhere n is the number of features.
Rather than havea different smoothing parameter ?i for each feature,we use a single parameter ?.We use a technique from the numerical optimisa-tion literature, the L-BFGS algorithm (Nocedal andWright, 1999), to optimise the objective function.L-BFGS is an iterative algorithm which requires thegradient of the objective function to be computed ateach iteration.
The components of the gradient vec-tor are as follows:?L?(?)??i=m?j=1?d??
(pi j)e?.
f (d,pi j) fi(d, pi j)?d??
(pi j) e?.
f (d,pi j) (5)?m?j=1????
(S j)e?.
f (?)
fi(?)????
(S j) e?.
f (?)
?
?i?2iThe first two terms in (5) are expectations of fea-ture fi: the first expectation is over all derivationsleading to each gold standard dependency struc-ture; the second is over all derivations for each sen-tence in the training data.
Setting the gradient tozero yields the usual maximum entropy constraints(Berger et al, 1996), except that in this case theempirical values are themselves expectations (overall derivations leading to each gold standard depen-dency structure).
The estimation process attemptsto make the expectations equal, by putting as muchmass as possible on the derivations leading to thegold standard structures.1 The Gaussian prior termpenalises any model whose weights get too large inabsolute value.Calculation of the feature expectations requiressumming over all derivations for a sentence, andsumming over all derivations leading to a gold stan-dard dependency structure.
In both cases there canbe exponentially many derivations, and so enumer-ating all derivations is not possible (at least forwide-coverage automatically extracted grammars).Clark and Curran (2003) show how the sum overthe complete derivation space can be performed ef-ficiently using a packed chart and a variant of theinside-outside algorithm.
Section 5 shows how thesame technique can also be applied to all derivationsleading to a gold standard dependency structure.3.2 The Normal-Form ModelThe objective function and gradient vector for thenormal-form model are as follows:L?(?)
= L(?)
?G(?)
(6)= logm?j=1P?
(d j|S j) ?n?i=1?2i2?2i?L?(?)?
?i=m?j=1fi(d j) (7)?m?j=1?d??
(S j)e?.
f (d) fi(d)?d??
(S j) e?.
f (d) ?
?i?2i1See Riezler et al (2002) for a similar description in thecontext of LFG parsing.where d j is the the gold standard derivation for sen-tence S j and ?
(S j) is the set of possible derivationsfor S j.
Note that the empirical expectation in (7) issimply a count of the number of times the featureappears in the gold-standard derivations.4 Packed ChartsThe packed charts perform a number of roles: theyare a compact representation of a very large num-ber of CCG derivations; they allow recovery of thehighest scoring parse or dependency structure with-out enumerating all derivations; and they representan instance of what Miyao and Tsujii (2002) call afeature forest, which is used to efficiently estimate alog-linear model.
The idea behind a packed chart issimple: equivalent chart entries of the same type, inthe same cell, are grouped together, and back point-ers to the daughters indicate how an individual entrywas created.
Equivalent entries form the same struc-tures in any subsequent parsing.Since the packed charts are used for model es-timation and recovery of the highest scoring parseor dependency structure, the features in the modelpartly determine which entries can be grouped to-gether.
In this paper we use features from the de-pendency structure, and features defined on the lo-cal rule instantiations.2 Hence, any two entries withidentical category type, identical head, and identicalunfilled dependencies are equivalent.
Note that notall features are local to a rule instantiation; for ex-ample, features encoding long-range dependenciesmay involve words which are a long way apart inthe sentence.For the purposes of estimation and finding thehighest scoring parse or dependency structure, onlyentries which are part of a derivation spanning thewhole sentence are relevant.
These entries can beeasily found by traversing the chart top-down, start-ing with the entries which span the sentence.
Theentries within spanning derivations form a featureforest (Miyao and Tsujii, 2002).
A feature forest ?is a tuple ?C,D,R, ?, ??
where:  C is a set of conjunctive nodes;  D is a set of disjunctive nodes;  R ?
D is a set of root disjunctive nodes;  ?
: D?
2C is a conjunctive daughter function;  ?
: C ?
2D is a disjunctive daughter function.The individual entries in a cell are conjunctivenodes, and the equivalence classes of entries are dis-2By rule instantiation we mean the local tree arising fromthe application of a CCG combinatory rule.
?C,D,R, ?, ??
is a packed chart / feature forestG is a set of gold standard dependenciesLet c be a conjunctive nodeLet d be a disjunctive nodedeps(c) is the set of dependencies on node ccdeps(c) ={?1 if, for some ?
?
deps(c), ?
< G|deps(c)| otherwisedmax(c) =?????????
?1 if cdeps(c) = ?1?1 if dmax(d) = ?1 for some d ?
?(c)?d??
(c) dmax(d) + cdeps(c) otherwisedmax(d) = max{dmax(c) | c ?
?
(d)}mark(d):mark d as a correct nodeforeach c ?
?
(d)if dmax(c) = dmax(d)mark c as a correct nodeforeach d?
?
?(c)mark(d?
)foreach dr ?
R such that dmax.
(dr) = |G|mark(dr)Figure 1: Finding nodes in correct derivationsjunctive nodes.
The roots of the CCG derivationsrepresent the root disjunctive nodes.35 Efficient EstimationThe L-BFGS algorithm requires the following val-ues at each iteration: the expected value, and theempirical expected value, of each feature (to calcu-late the gradient); and the value of the likelihoodfunction.
For the normal-form model, the empiri-cal expected values and the likelihood can easily beobtained, since these only involve the single gold-standard derivation for each sentence.
The expectedvalues can be calculated using the method in Clarkand Curran (2003).For the dependency model, the computations ofthe empirical expected values (5) and the likelihoodfunction (4) are more complex, since these requiresums over just those derivations leading to the goldstandard dependency structure.
We will refer tosuch derivations as correct derivations.Figure 1 gives an algorithm for finding nodes ina packed chart which appear in correct derivations.cdeps(c) is the number of correct dependencies onconjunctive node c, and takes the value ?1 if thereare any incorrect dependencies on c. dmax(c) is3A more complete description of CCG feature forests isgiven in Clark and Curran (2003).the maximum number of correct dependencies pro-duced by any sub-derivation headed by c, and takesthe value ?1 if there are no sub-derivations produc-ing only correct dependencies.
dmax(d) is the samevalue but for disjunctive node d. Recursive defini-tions for calculating these values are given in Fig-ure 1; the base case occurs when conjunctive nodeshave no disjunctive daughters.The algorithm identifies all those root nodesheading derivations which produce just the cor-rect dependencies, and traverses the chart top-downmarking the nodes in those derivations.
The in-sight behind the algorithm is that, for two conjunc-tive nodes in the same equivalence class, if onenode heads a sub-derivation producing more cor-rect dependencies than the other node (and eachsub-derivation only produces correct dependencies),then the node with less correct dependencies cannotbe part of a correct derivation.The conjunctive and disjunctive nodes appearingin correct derivations form a new correct feature for-est.
The correct forest, and the complete forest con-taining all derivations spanning the sentence, can beused to estimate the required likelihood value andfeature expectations.
Let E??
fi be the expected valueof fi over the forest ?
for model ?
; then the valuesin (5) can be obtained by calculating E?
j?
fi for thecomplete forest ?
j for each sentence S j in the train-ing data (the second sum in (5)), and also E?
j?
fi foreach forest ?
j of correct derivations (the first sumin (5)):?L(?)??i=m?j=1(E?
j?
fi ?
E?
j?
fi) (8)The likelihood in (4) can be calculated as follows:L(?)
=m?j=1(log Z?
j ?
log Z?
j) (9)where log Z?
is the normalisation constant for ?.6 Estimation in PracticeThe gold standard dependency structures are pro-duced by running our CCG parser over thenormal-form derivations in CCGbank (Hocken-maier, 2003a).
Not all rule instantiations in CCG-bank are instances of combinatory rules, and not allcan be produced by the parser, and so gold standardstructures were created for 85.5% of the sentencesin sections 2-21 (33,777 sentences).The same parser is used to produce the packedcharts.
The parser uses a maximum entropy su-pertagger (Clark and Curran, 2004) to assign lexicalcategories to the words in a sentence, and applies theCKY chart parsing algorithm described in Steedman(2000).
For parsing the training data, we ensure thatthe correct category is a member of the set assignedto each word.
The average number of categories as-signed to each word is determined by a parameterin the supertagger.
For the first set of experiments,we used a setting which assigns 1.7 categories onaverage per word.The feature set for the dependency model con-sists of the following types of features: dependencyfeatures (with and without distance measures), ruleinstantiation features (with and without a lexicalhead), lexical category features, and root categoryfeatures.
Dependency features are the 5-tuples de-fined in Section 1.
There are also three additionaldependency feature types which have an extra dis-tance field (and only include the head of the lex-ical category, and not the head of the argument);these count the number of words (0, 1, 2 or more),punctuation marks (0, 1, 2 or more), and verbs (0,1 or more) between head and dependent.
Lexi-cal category features are word?category pairs at theleaf nodes, and root features are headword?categorypairs at the root nodes.
Rule instantiation featuressimply encode the combining categories togetherwith the result category.
There is an additional rulefeature type which also encodes the lexical head ofthe resulting category.
Additional generalised fea-tures for each feature type are formed by replacingwords with their POS tags.The feature set for the normal-form model isthe same except that, following Hockenmaier andSteedman (2002), the dependency features are de-fined in terms of the local rule instantiations, byadding the heads of the combining categories to therule instantiation features.
Again there are 3 addi-tional distance feature types, as above, which onlyinclude the head of the resulting category.
We hadhoped that by modelling the predicate-argument de-pendencies produced by the parser, rather than localrule dependencies, we would improve performance.However, using the predicate-argument dependen-cies in the normal-form model instead of, or in ad-dition to, the local rule dependencies, has not led toan improvement in parsing accuracy.Only features which occurred more than once inthe training data were included, except that, for thedependency model, the cutoff for the rule featureswas 9 and the counting was performed across allderivations, not just the gold-standard derivation.The normal-form model has 482,007 features andthe dependency model has 984,522 features.We used 45 machines of a 64-node Beowulf clus-ter to estimate the dependency model, with an av-erage memory usage of approximately 550 MB foreach machine.
For the normal-form model we wereable to reduce the size of the charts considerably byapplying two types of restriction to the parser: first,categories can only combine if they appear togetherin a rule instantiation in sections 2?21 of CCGbank;and second, we apply the normal-form restrictionsdescribed in Eisner (1996).
(See Clark and Curran(2004) for a description of the Eisner constraints.
)The normal-form model requires only 5 machinesfor estimation, with an average memory usage of730 MB for each machine.Initially we tried the parallel version of GIS de-scribed in Clark and Curran (2003) to performthe estimation, running over the Beowulf cluster.However, we found that GIS converged extremelyslowly; this is in line with other recent results in theliterature applying GIS to globally optimised mod-els such as conditional random fields, e.g.
Sha andPereira (2003).
As an alternative to GIS, we haveimplemented a parallel version of our L-BFGS codeusing the Message Passing Interface (MPI) standard.L-BFGS over forests can be parallelised, using themethod described in Clark and Curran (2003) to cal-culate the feature expectations.
The L-BFGS algo-rithm, run to convergence on the cluster, takes 479iterations and 2 hours for the normal-form model,and 1,550 iterations and roughly 17 hours for thedependency model.7 Parsing AlgorithmFor the normal-form model, the Viterbi algorithm isused to find the most probable derivation.
For thedependency model, the highest scoring dependencystructure is required.
Clark and Curran (2003) out-lines an algorithm for finding the most probable de-pendency structure, which keeps track of the high-est scoring set of dependencies for each node inthe chart.
For a set of equivalent entries in thechart (a disjunctive node), this involves summingover all conjunctive node daughters which head sub-derivations leading to the same set of high scoringdependencies.
In practice large numbers of suchconjunctive nodes lead to very long parse times.As an alternative to finding the most probabledependency structure, we have developed an algo-rithm which maximises the expected labelled re-call over dependencies.
Our algorithm is based onGoodman?s (1996) labelled recall algorithm for thephrase-structure PARSEVAL measures.Let Lpi be the number of correct dependencies inpi with respect to a gold standard dependency struc-ture G; then the dependency structure, pimax, whichmaximises the expected recall rate is:pimax = arg maxpi E(Lpi/|G|) (10)= arg maxpi?piiP(pii|S )|pi ?
pii|where S is the sentence for gold standard depen-dency structure G and pii ranges over the depen-dency structures for S .
This expression can be ex-panded further:pimax = arg maxpi?piiP(pii|S )??
?pi1 if ?
?
pii= arg maxpi???pi?pi?
|?
?pi?P(pi?|S )= arg maxpi???pi?d??(pi?)|?
?pi?P(d|S ) (11)The final score for a dependency structure pi is asum of the scores for each dependency ?
in pi; andthe score for a dependency ?
is the sum of the proba-bilities of those derivations producing ?.
This lattersum can be calculated efficiently using inside andoutside scores:pimax = arg maxpi??
?pi1ZS?c?C?c?c if ?
?
deps(c)(12)where ?c is the inside score and ?c is the outsidescore for node c (see Clark and Curran (2003)); Cis the set of conjunctive nodes in the packed chartfor sentence S and deps(c) is the set of dependen-cies on conjunctive node c. The intuition behindthe expected recall score is that a dependency struc-ture scores highly if it has dependencies producedby high scoring derivations.4The algorithm which finds pimax is a simple vari-ant on the Viterbi algorithm, efficiently finding aderivation which produces the highest scoring set ofdependencies.8 ExperimentsGold standard dependency structures were derivedfrom section 00 (for development) and section 23(for testing) by running the parser over the deriva-tions in CCGbank, some of which the parser couldnot process.
In order to increase the number of testsentences, and to allow a fair comparison with otherCCG parsers, extra rules were encoded in the parser(but we emphasise these were only used to obtain4Coordinate constructions can create multiple dependenciesfor a single argument slot; in this case the score for the multipledependencies is the average of the individual scores.LP LR UP UR catDep model 86.7 85.6 92.6 91.5 93.5N-form model 86.4 86.2 92.4 92.2 93.6Table 1: Results on development set; labelled and unla-belled precision and recall, and lexical category accuracyFeatures LP LR UP UR catRULES 82.6 82.0 89.7 89.1 92.4+HEADS 83.6 83.3 90.2 90.0 92.8+DEPS 85.5 85.3 91.6 91.3 93.5+DISTANCE 86.4 86.2 92.4 92.2 93.6FINAL 87.0 86.8 92.7 92.5 93.9Table 2: Results on development set for the normal-form modelsthe section 23 test data; they were not used to parseunseen data as part of the testing).
This resulted in2,365 dependency structures for section 23 (98.5%of the full section), and 1,825 (95.5%) dependencystructures for section 00.The first stage in parsing the test data is to applythe supertagger.
We use the novel strategy devel-oped in Clark and Curran (2004): first assign a smallnumber of categories (approximately 1.4) on aver-age to each word, and increase the number of cate-gories if the parser fails to find an analysis.
We wereable to parse 98.9% of section 23 using this strategy.Clark and Curran (2004) shows that this supertag-ging method results in a highly efficient parser.For the normal-form model we returned the de-pendency structure for the most probable derivation,applying the two types of normal-form constraintsdescribed in Section 6.
For the dependency modelwe returned the dependency structure with the high-est expected labelled recall score.Following Clark et al (2002), evaluation is byprecision and recall over dependencies.
For a la-belled dependency to be correct, the first 4 elementsof the dependency tuple must match exactly.
Foran unlabelled dependency to be correct, the headsof the functor and argument must appear togetherin some relation in the gold standard (in any order).The results on section 00, using the feature sets de-scribed earlier, are given in Table 1, with similarresults overall for the normal-form model and thedependency model.
Since experimentation is easierwith the normal-form model than the dependencymodel, we present additional results for the normal-form model.Table 2 gives the results for the normal-formmodel for various feature sets.
The results showthat each additional feature type increases perfor-LP LR UP UR catClark et al 2002 81.9 81.8 90.1 89.9 90.3Hockenmaier 2003 84.3 84.6 91.8 92.2 92.2Log-linear 86.6 86.3 92.5 92.1 93.6Hockenmaier(POS) 83.1 83.5 91.1 91.5 91.5Log-linear (POS) 84.8 84.5 91.4 91.0 92.5Table 3: Results on the test setmance.
Hockenmaier also found the dependenciesto be very beneficial ?
in contrast to recent resultsfrom the lexicalised PCFG parsing literature (Gildea,2001) ?
but did not gain from the use of distancemeasures.
One of the advantages of a log-linearmodel is that it is easy to include additional infor-mation, such as distance, as features.The FINAL result in Table 2 is obtained by us-ing a larger derivation space for training, createdusing more categories per word from the supertag-ger, 2.9, and hence using charts containing morederivations.
(15 machines were used to estimate thismodel.)
More investigation is needed to find the op-timal chart size for estimation, but the results showa gain in accuracy.Table 3 gives the results of the best performingnormal-form model on the test set.
The resultsof Clark et al (2002) and Hockenmaier (2003a)are shown for comparison.
The dependency setused by Hockenmaier contains some minor differ-ences to the set used here, but ?evaluating?
our testset against Hockenmaier?s gives an F-score of over97%, showing the test sets to be very similar.
Theresults show that our parser is performing signifi-cantly better than that of Clark et al, demonstratingthe benefit of derivation features and the use of asound statistical model.The results given so far have all used gold stan-dard POS tags from CCGbank.
Table 3 also gives theresults if automatically assigned POS tags are usedin the training and testing phases, using the C&CPOS tagger (Curran and Clark, 2003).
The perfor-mance reduction is expected given that the supertag-ger relies heavily on POS tags as features.More investigation is needed to properly com-pare our parser and Hockenmaier?s, since there area number of differences in addition to the modelsused: Hockenmaier effectively reads a lexicalisedPCFG off CCGbank, and is able to use all of theavailable training data; Hockenmaier does not usea supertagger, but does use a beam search.Parsing the 2,401 sentences in section 23 takes1.6 minutes using the normal-form model, and 10.5minutes using the dependency model.
The differ-ence is due largely to the normal-form constraintsused by the normal-form parser.
Clark and Curran(2004) shows that the normal-form constraints sig-nificantly increase parsing speed and, in combina-tion with adaptive supertagging, result in a highlyefficient wide-coverage parser.As a final oracle experiment we parsed the sen-tences in section 00 using the correct lexical cate-gories from CCGbank.
Since the parser uses only asubset of the lexical categories in CCGbank, 7% ofthe sentences could not be parsed; however, the la-belled F-score for the parsed sentences was almost98%.
This very high score demonstrates the largeamount of information in lexical categories.9 ConclusionA major contribution of this paper has been the de-velopment of a parsing model for CCG which usesall derivations, including non-standard derivations.Non-standard derivations are an integral part of theCCG formalism, and it is an interesting questionwhether efficient estimation and parsing algorithmscan be defined for models which use all derivations.We have answered this question, and in doing sodeveloped a new parsing algorithm for CCG whichmaximises expected recall of dependencies.We would like to extend the dependency model,by including the local-rule dependencies which areused by the normal-form model, for example.
How-ever, one of the disadvantages of the dependencymodel is that the estimation process is already usinga large proportion of our existing resources, and ex-tending the feature set will further increase the exe-cution time and memory requirement of the estima-tion algorithm.We have also shown that a normal-form modelperforms as well as the dependency model.
Thereare a number of advantages to the normal-formmodel: it requires less space and time resourcesfor estimation and it produces a faster parser.
Ournormal-form parser significantly outperforms theparser of Clark et al (2002) and produces resultsat least as good as the current state-of-the-art forCCG parsing.
The use of adaptive supertagging andthe normal-form constraints result in a very efficientwide-coverage parser.
Our system demonstratesthat accurate and efficient wide-coverage CCG pars-ing is feasible.Future work will investigate extending the featuresets used by the log-linear models with the aim offurther increasing parsing accuracy.
Finally, the ora-cle results suggest that further experimentation withthe supertagger will significantly improve parsingaccuracy, efficiency and robustness.AcknowledgementsWe would like to thank Julia Hockenmaier forthe use of CCGbank and helpful comments, andMark Steedman for guidance and advice.
JasonBaldridge, Frank Keller, Yuval Krymolowski andMiles Osborne provided useful feedback.
This workwas supported by EPSRC grant GR/M96889, and aCommonwealth scholarship and a Sydney Univer-sity Travelling scholarship to the second author.ReferencesAdam Berger, Stephen Della Pietra, and Vincent DellaPietra.
1996.
A maximum entropy approach to nat-ural language processing.
Computational Linguistics,22(1):39?71.Stanley Chen and Ronald Rosenfeld.
1999.
A Gaussianprior for smoothing maximum entropy models.
Tech-nical report, Carnegie Mellon University, Pittsburgh,PA.Stephen Clark and James R. Curran.
2003.
Log-linearmodels for wide-coverage CCG parsing.
In Proceed-ings of the EMNLP Conference, pages 97?104, Sap-poro, Japan.Stephen Clark and James R. Curran.
2004.
The impor-tance of supertagging for wide-coverage CCG pars-ing.
In Proceedings of COLING-04, Geneva, Switzer-land.Stephen Clark, Julia Hockenmaier, and Mark Steedman.2002.
Building deep dependency structures with awide-coverage CCG parser.
In Proceedings of the40th Meeting of the ACL, pages 327?334, Philadel-phia, PA.Michael Collins.
1996.
A new statistical parser based onbigram lexical dependencies.
In Proceedings of the34th Meeting of the ACL, pages 184?191, Santa Cruz,CA.James R. Curran and Stephen Clark.
2003.
InvestigatingGIS and smoothing for maximum entropy taggers.
InProceedings of the 10th Meeting of the EACL, pages91?98, Budapest, Hungary.Jason Eisner.
1996.
Efficient normal-form parsing forCombinatory Categorial Grammar.
In Proceedings ofthe 34th Meeting of the ACL, pages 79?86, SantaCruz, CA.Daniel Gildea.
2001.
Corpus variation and parser per-formance.
In Proceedings of the EMNLP Conference,pages 167?202, Pittsburgh, PA.Joshua Goodman.
1996.
Parsing algorithms and metrics.In Proceedings of the 34th Meeting of the ACL, pages177?183, Santa Cruz, CA.Julia Hockenmaier and Mark Steedman.
2002.
Gen-erative models for statistical parsing with Combina-tory Categorial Grammar.
In Proceedings of the 40thMeeting of the ACL, pages 335?342, Philadelphia, PA.Julia Hockenmaier.
2003a.
Data and Models for Statis-tical Parsing with Combinatory Categorial Grammar.Ph.D.
thesis, University of Edinburgh.Julia Hockenmaier.
2003b.
Parsing with generativemodels of predicate-argument structure.
In Proceed-ings of the 41st Meeting of the ACL, pages 359?366,Sapporo, Japan.Yusuke Miyao and Jun?ichi Tsujii.
2002.
Maximum en-tropy estimation for feature forests.
In Proceedingsof the Human Language Technology Conference, SanDiego, CA.Jorge Nocedal and Stephen J. Wright.
1999.
NumericalOptimization.
Springer, New York, USA.Stefan Riezler, Tracy H. King, Ronald M. Kaplan,Richard Crouch, John T. Maxwell III, and Mark John-son.
2002.
Parsing the Wall Street Journal using aLexical-Functional Grammar and discriminative esti-mation techniques.
In Proceedings of the 40th Meet-ing of the ACL, pages 271?278, Philadelphia, PA.Fei Sha and Fernando Pereira.
2003.
Shallow parsingwith conditional random fields.
In Proceedings of theHLT/NAACL Conference, pages 213?220, Edmonton,Canada.Mark Steedman.
2000.
The Syntactic Process.
The MITPress, Cambridge, MA.Kristina Toutanova, Christopher Manning, StuartShieber, Dan Flickinger, and Stephan Oepen.
2002.Parse disambiguation for a rich HPSG grammar.
InProceedings of the First Workshop on Treebanksand Linguistic Theories, pages 253?263, Sozopol,Bulgaria.
