Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 46?55,Portland, Oregon, 24 June 2011. c?2011 Association for Computational LinguisticsPerformance of Automated Scoring for Children?s Oral ReadingRyan Downey,  David Rubin,  Jian Cheng,  Jared BernsteinPearson Knowledge Technologies299 S. California Ave.Palo Alto, California 94306Ryan.Downey@Pearson.comAbstractFor adult readers, an automated system canproduce oral reading fluency (ORF) scores(e.g., words read correctly per minute) that areconsistent with scores provided by humanevaluators (Balogh et al, 2005, and in press).Balogh?s work on NAAL materials usedpassage-specific data to optimize statisticallanguage models and scoring performance.
Thecurrent study investigates whether or not anautomated system can produce scores for youngchildren?s reading that are consistent withhuman scores.
A novel aspect of the presentstudy is that text-independent rule-basedlanguage models were employed (Cheng andTownshend, 2009) to score reading passagesthat the system had never seen before.
Oralreading performances were collected over cellphones from 1st, 2nd, and 3rd grade children (n =95) in a classroom environment.
Readings werescored 1) in situ by teachers in the classroom,2) later by expert scorers, and 3) by anautomated system.
Statistical analyses provideevidence that machine Words Correct scorescorrelate well with scores provided by teachersand expert scorers, with all (Pearson?scorrelation coefficient) r?s > 0.98 at theindividual response level, and all r?s > 0.99 atthe ?test?
level (i.e., median scores out of 3).1 IntroductionOral reading fluency (ORF), defined as ?the abilityto read a text quickly, accurately, and with properexpression?
(National Reading Panel, 2000; p.3.5), is a reflection of readers?
decoding ability.Skilled readers can recognize words effortlessly(Rasinski and Hoffman, 2003), due to?automaticity?
of processing (LaBerge andSamuels, 1974) whereby a reader?s attention is nolonger focused on ?lower level?
processing (e.g.,letter to phoneme correspondence, wordidentification, etc.).
Instead, attention can bedevoted to ?higher level?
functions such ascomprehension and expression (LaBerge andSamuels, 1974).
As a means of assessing generalreading ability, oral reading fluency performance isalso a predictor of student success in academicareas such as reading and math (e.g., Crawford,Tindal, and Stieber, 2001).
Oral reading fluency isone of the key basic skills identified in the ReadingFirst initiative used to satisfy the standards of theNo Child Left Behind Act (NCLB, 2001).Although oral reading fluency is comprised ofseveral abilities, due to practical constraints themost commonly reported reflection of oral readingfluency is reading rate, specifically, the words readcorrectly per minute (WCPM).
Typically, ORFperformance is measured by a classroom teacherwho sits alongside a student, marking andannotating ?
in real time ?
the student?s reading ona sheet of paper containing the passage to be read.Classroom testing is time-consuming and requiresa teacher?s full attention.
In practice, teaching timeis often sacrificed to ?testing time?
to satisfy localand federal reporting standards (e.g., NCLB).ORF scoring guidelines are specific to particularpublishers; teachers must undergo training tobecome familiar with these guidelines, and cost,availability, and quality of training varies.
Finally,despite good-faith attempts to score accurately,teachers may impose errors and inconsistencies in46scoring ORF performances due to unavoidablefactors such as classroom distractions, varyingexperience with different accents/dialects, varyingexperience with scoring conventions, anddifferences in training, among others.To address the need for a rapid and reliable wayto assess oral reading fluency, a growing body ofresearch has supported the use of automatedapproaches.
Beginning with work by Bernstein etal.
(1990) and Mostow et al (1994), prototypesystems for automatic measurement of basiccomponents of reading have appeared.
Recentprojects have addressed finer event classification inreading aloud (Black, Tepperman, Lee, Price, andNarayanan, 2007), and word level reading(Tepperman et al, 2007), among others.
Researchhas increasingly focused on systems to scorepassage-level reading performances (e.g., Baloghet al, 2005; Zechner, Sabatini, and Chen, 2009;Cheng and Townshend, 2009).
Eskenazi (2009)presents a general historical perspective on speechprocessing applications in language learning,including reading.The present automated ORF assessment wasdeveloped to deliver and score tests of oral readingfluency, allowing teachers to spend less timetesting and more time teaching, while at the sametime improving score consistency across time andlocation.
Automated ORF tests are initiated by aclick in a web-based class roster.
Once a test isinitiated, a call is placed to a local phone numberand the test begins when the phone is answered.Instructions presented through the handset directthe student to read passages out loud into the cellphone, and these readings are sent to the automatedORF system for processing and scoring.2 Present StudyThe scoring models used by the automated ORFtest (see Method below) were originally developedbased on adult readings, and then optimized onlarge sets of data collected from students readingpassages produced by AIMSweb, a publisher ofReading Curriculum-Based Measurement (R-CBM) oral reading fluency passages(www.aimsweb.com).
AIMSweb passages areleveled and normed across large samples ofstudents.
Previous validation studies found thatwhen the system was optimized using data fromstudents reading AIMSweb passages, machinescores correlated with trained human expert scorewith r = 0.95 to 0.98, depending on the grade levelof the student readers.The primary question that the present studiesattempt to answer is whether the automated scoringsystem can score newly inserted content ?
in thiscase, ORF passages offered by Sopris called?Dynamic Indicators of Basic Early LiteracySkills?, or DIBELS (www.dibels.com) ?
accuratelyand at a high level of reliability.
This is anevaluation of text-independent Rule BasedLanguage Models (RBLMs) that were developedwith training data from other readers performingon other passages and then applied to the newpassages.A secondary question of interest involves howdifferent types of scorers may assign WordsCorrect scores differently.
Two groups of humanscorers were recruited:  1) teachers who wererecently trained in DIBELS scoring methods whowould perform scoring in the classroom, and 2)expert scorers with the ability to score readingrecordings carefully and at their convenience,without classroom distractions.
Answering thefirst part of the question involves comparingmachine Words Correct scores to human scoreswhen teachers make ratings in the classroomenvironment as the student reads into the phone.This analysis reveals if the machine and teachersproduce systematically different scores whentesting is performed in a ?live?
classroom with thetypical attentional demands placed on a teacherscoring an ORF passage.
Answering the secondpart of the question involves comparing machineWords Correct scores to a ?consensus?, or medianWords Correct value, from expert scorers.
Thesethree experts, with over 14 years of combinedexperience scoring DIBELS passages, listened torecordings of the same readings made in theclassroom.
Because the recordings were digitallypreserved in a database, the expert scorers wereable to replay any part(s) of the recordings todetermine whether each word was read correctly.The benefit of being able to replay recordings isthat such scores obtained are, in theory, closer tocapturing the ?truth?
of the student?s performance,unaffected by biases or distractions encountered byscorers performing a ?live?
rating.472.1 Method2.1.1 Rule Based Language ModelsThe scoring models used by the automated ORFsystem are RBLMs such as those described byCheng and Townshend (2009).
Such models out-perform traditional n-gram language models(Cheng and Shen, 2010), in part by addingintuitively simple rules such as allowing a longsilence as an alternative to a short pause after everyword, leading to improvements in accuracy.
Also,rules like those described by Cheng andTownshend (2009) consider much longersequential dependencies.
The basic idea for thiskind of language model is that each passage gets asimple directed graph with a path from the firstword to the last word.
Different arcs are added torepresent different common errors made by thereaders, such as skipping, repeating, inserting, andsubstituting words.
For each arc, a probability isassigned to represent the chance that the arc will bechosen.
Knowledge of performance on otherreadings produces linguistic rules, such as she cansubstitute for he, a single noun can replace a pluralnoun, the reader may skip from any place to theend, etc.
All the rules used in RBLMs can beclassified into five broad groups:1. skip/repeat rules2.
rules using part-of-speech (POS) tagginginformation3.
rules accommodating for insertion ofpartial words4.
general word level rules5.
hesitation and mouth noise rulesA detailed analysis of the role of rules in RBLMswas described in Cheng and Shen (2010).The language rules are extrapolated fromtranscriptions of oral reading responses to passagesusing four base rules: any word substitutes for anyword with a low probability; any word is insertedafter any word with a low probability; any word isskipped with a low probability; any word isrepeated immediately with a low probability.Following Cheng and Townshend (2009), the firsttwo are the only rules that allow out-of-vocabularywords and their probabilities are fixed to the lowestlevel, so their arcs will never be traversed unlessthere is no other choice.General language model rules for reading canbe inferred from clustering traversals of the basicmodels and proposing further rules that can beapplied to new reading passages and used to inferunderlying knowledge about the reading.
Arcs areadded to represent commonly observed non-canonic readings.
Further analysis of rule-firingdetails may provide diagnostic linguisticinformation about children?s reading habits thatcan be reported and analyzed.In the present automated scoring system, newpassages are automatically tagged for part-of-speech (POS) using the Penn Tree Tagger (Marcus,Santorini, and Marcinkiewicz, 1993).
POS tagsallow specification of certain general rules basedon linguistic properties, such as:?
NN (noun, singular or mass) can become NNS(noun, plural);?
VBZ (verb, 3rd person singular present) canbecome VBP (verb, non-3rd person singularpresent); and so on.These patterns occur quite frequently in realresponses and can therefore be accounted for byrules.
Sentence, clause, and end-of-line boundariesare tagged manually.
Marked up passages are theninserted into the ORF scoring system, providingdata regarding places in the reading that may resultin pauses, hesitations, corrections, etc.
If theexpected response to a reading passage is highlyconstrained, the system can verify the occurrenceof the correct lexical content in the correctsequence.
It is expected that the system, usingpreviously trained data coupled with the RBLMsfrom the newly inserted passages, will be able toproduce Words Correct scores with high accuracy(i.e., consistent with human Words Correct scores).Here, we make a final note on the use of WordsCorrect instead of words correct per minute(WCPM), when WCPM is the most commonmeasure for quantifying oral reading performance.The automated system presents students with a 60-second recording window to read each passage, butit calculates a truer WCPM by trimming leadingand trailing silence.
Human scorers simplyreported the number of words correct, on theassumption that the reading time is the recordingwindow duration.
Thus, Words Correct scores arethe appropriate comparison values, with a fixed 60-second nominal reading time.2.1.2 ParticipantsA total of 95 students were recruited from the SanJose Unified School District in San Jose,48California.
The students were 20 first graders, 20second graders, and 55 third graders, all enrolled ina summer school program.
Students with knownspeech disorders were included in the study, as wasone student with a hearing impairment.
Roughlyhalf of the participants were male and half werefemale.
A number of English Language Learnersare known to have been included in the sample,though language status was not recorded as avariable for this study.
It is not known whetherany of the students had been diagnosed withreading disabilities.Four Teachers were trained to administer andscore DIBELS ORF passages by an officialDIBELS trainer, over the course of a two daytraining session.
All Teachers were readingexperts or teachers with experience in readingeducation.
They were trained to navigate a webapplication that triggers delivery of tests over cellphones under classroom testing conditions.Evaluator qualifications are summarized in Table1.Evaluator Highest degree, or relevant certificationYearsassessingreadingTeacher 1 MA Education 8Teacher 2 MA Education 7Teacher 3 Reading Credential 15Teacher 4 BA Education 12Expert 1 MS, Statistics 5Expert 2 EdS, Education 2Expert 3 MA Education 20Table 1.
Evaluator qualifications2.1.3 ProcedureFirst, nine passages ?
three for each of the threegrades, presented together in a single test ?
weredrawn from the DIBELS Benchmark test materials.Each DIBELS passage was tagged for parts ofspeech and formatting (e.g., line breaks) andinserted into the automated scoring system.
Rule-based language models were produced for eachpassage.During data collection, each student read thegrade-appropriate DIBELS Benchmark test (3passages) into a cellular telephone in theclassroom.
With three passages per student, thisprocess yielded 285 individual readingperformances.Once a test was initiated, Teachers allowed thetest to run independently and scored manuallyalongside the student reading into the phone.According to standard DIBELS scoringconventions, the students were allowed to readeach passage for one minute.
Teachers calculatedand recorded the Words Correct score on aworksheet for each passage.
Teachers returned theannotated score sheets for analysis.Later, three Expert scorers logged in to a web-based interface via the Internet, where they listenedto the digitized recordings of the readings.
Allthree Expert scorers had extensive experience withDIBELS rating.
One Expert was the DIBELStrainer who provided the DIBELS training to theTeachers for this study.
Experts scored students?performance manually using score sheets with theinstruction to use standard DIBELS scoringconventions.
Each Expert entered a Words Correctscore for each passage using the web interface, andthe score sheets were returned for analysis.2.1.4 Automated scoringIncoming spoken responses were digitallyrecorded and sent to a speech processing systemthat is optimized for both native and non-nativespeech.
Recognition was performed by an HMM-based recognizer built using the HTK toolkit(Young, et al, 2000).
Acoustic models,pronunciation dictionaries, and expected-responsenetworks were developed in-house using data fromprevious training studies involving manythousands of responses.
The words, pauses,syllables, phones, and even some subphonemicevents can be located in the recorded signal, and?words recognized?
are compared with ?wordsexpected?
to produce a recognized response andword count.The acoustic models for the speech recognizerwere developed using data from a diverse sampleof non-native speakers of English.
In addition,recordings from 57 first-grade children were usedto optimize the automated scoring system toaccommodate for characteristics specific to youngchildren?s voices and speech patterns.
Theseparticipants produced 136 usable, individualreading samples.
These samples were each ratedby two expert human raters.
Using this finaltraining set, the scoring models were refined to thepoint that the correlation between human andmachine scoring was 0.97 for WCPM.492.1.5 Human scoringDuring data preparation, it was noted that manyof the teacher scores were several words longerthan would be expected based on the machinescores.
Further investigation revealed that teacherswould occasionally continue scoring after the oneminute point at which the system stoppedrecording a passage, perhaps because they hadn?theard the notification that the reading wascomplete.
A total of 31 out of 285 instances(~10.8%) were found where teachers continuedscoring for more than 3 words beyond the 1 minuterecording window, leading to artificially inflatedTeacher scores.
This artifact of the testingapparatus/environment warranted making a carefulcorrection, whereby all Teacher scores wereadjusted to account for what the machine ?heard?.That is, words and errors which Teachers scoredafter the automated system stopped recording (i.e.,to which the automated system did not haveaccess) were subtracted from the original TeacherWords Correct scores.
All Teacher Words Correctscores reported hereafter are thus ?corrected?.For purposes of finding a ?consensus?
Expertscore, the median of the 3 expert human scores foreach passage was obtained and is referred to asExpertM in the following analyses.Nine readings from eight separate studentsreceived no scores from teachers.
Information wasnot provided by the teachers regarding why theyfailed to complete the scoring process for thesereadings.
However, we made the followingobservations based on the teachers?
marked-upscoring sheets.
For three readings, the teacher?sfinal score was blank when the student appeared tohave skipped lines in the passage.
It is possiblethat, despite recent scoring training, the teacherwas uncertain how to score skipped lines in thereadings and left the final score blank pendingconfirmation.
For one reading, the teacher made anote that the system stopped recording well beforeone minute had expired because the child?s readingwas too quiet to be picked up, and the teacher didnot record the final score on the score sheet.
Forone reading, the student did not hear the prompt tobegin reading (confirmed by listening to theresponse recording) and therefore did not read theentire passage; the teacher did not enter a finalscore.
For the four remaining readings, the teacherannotated the performance but did not write downthe final score for unclear reasons.We might have elected to fill in the teachers?final scores for these 9 readings prior to subjectingthe data to analysis, especially in the cases where ateacher annotated the reading correctly on thescore sheet but simply failed to record the finalWords Correct score, perhaps due to oversight ornot knowing how to handle unusual events (e.g.,entire line of reading skipped).
Excluding suchreadings from the analysis ensured that theteachers?
scores reflected ?their own?
scoring ?including any errors they might make ?
rather thanour interpretation of what the Teachers probablywould have written.
In addition, to maintain themost conservative approach, whenever a singlereading passage from a student lacked a teacher?sscore, all 3 of that student?s readings wereexcluded.
The decision to exclude all readingsfrom students with only a single passage missingwas made because relevant analyses reportedbelow involve reporting median scores, and amedian score for students lacking one or twopassage scores would not be possible.1  The finalset of graded responses thus consisted of 261responses from 87 students.22.2 Results2.2.1 Score Group ComparisonsWords Correct scores from Teachers, ExpertM, andmachine are displayed in Table 2.
Repeatedmeasures ANOVA with Scorer Type (machine,Teacher, ExpertM) as the repeated measure andScore Group as the between-subjects factorrevealed a main effect of group for the 2611 The excluded 8 students produced 15 readings with all three(Machine, Teacher, Expert) scores.
Machine scores vs.Teacher scores and Machine scores vs. ExpertM scores for these 15 individual responses yielded correlations of(Pearson?s) r = 0.9949 and 0.9956, respectively.
Thus,excluding these responses from the larger dataset is unlikely tohave significantly affected the overall results.2 In production, such a system would not commit these errorsof omission.
Readings that are unscorable for technicalreasons can trigger a ?Median score not be calculated?message and request a teacher to manually score a recordingor re-administer the assessment.
Also, anomalousperformances where Words Correct on one passage is verydifferent from Words Correct on the two other passages couldreturn a message.50readings3, F(2, 520) = 9.912, p < .01, ?2 < .001.Post-hoc pairwise comparisons 4  showed thatWords Correct scores from Teachers were higheron average than both the machine and ExpertMscores (higher by 1.559 and 0.923 words correct,respectively; both p?s < .05).
On the other hand,Machine and ExpertM scores did not differsignificantly from each other (diff = 0.636).Although the ANOVA showed that the meansIn the above analysis were significantly different,the effect size was negligible: ?2 was = .0002,indicating that Score Group by itself accounted forless than 1% of the overall variance in scores.These results indicate that, for all 261 passages, theExpertM and machine scores were statisticallycomparable (e.g., within 1 word correct of eachother), while Teachers tended to assign slightly ?but not meaningfully ?
higher scores, on average.Next, comparisons were made using the medianvalue of each student?s three readings.
MedianWords Correct scores for the 87 individualstudents were subjected to repeated measuresANOVA with the same factor (Scorer Group).Teachers?
Words Correct scores were again higherthan ExpertM scores (diff = 1.115) and Machinescores (diff = 0.851), but this was not statisticallysignificant in the main analysis, F(2, 172) = 3.11, p> .05, ?2 < .001.
Machine Words Correct scoreswere, on average, 0.264 words higher than ExpertMscores, but this, too, was not statisticallysignificant.
These results support the previouscomparisons, in that machine scores fall wellwithin ~1 word correct of scores from carefulexperts, while teachers tended to give scores ofabout 1 word correct higher than both experts andmachine.2.2.2 Scorer performanceTo compare reliability, the Pearson?s ProductMoment coefficient (r) was used to estimate thecorrelation between paired human and machinescores, and between pairs of human raters.
Twotypes of analyses are reported.
First, analyses ofWords Correct scores were conducted acrossscorers.
Next, analyses were conducted on thebasis of the median Words Correct score for each3  For both ANOVAs, uncorrected degrees of freedom arereported but reported F values are corrected using Huynh-Feldt estimates of sphericity.4 Using Bonferroni adjustment for multiple comparisons.student?s readings (i.e., the median score across allthree passages).
This score reflects the ?real-life?score of DIBELS ORF tests because the medianscore is the one that is ultimately reportedaccording to DIBELS scoring/reportingconventions.2.2.2.1.
Intra-rater reliabilityEach Teacher scored each reading once during thelive grading; intra-rater reliability could thus not beWords CorrectScoreType261 readingsMean (SD)87 studentsMean (SD)Teacher 84.3 (42.5) 84.0 (42.1)ExpertM 83.4 (42.3) 82.9 (41.8)Machine 82.8 (39.6) 83.2 (39.3)Table 2.
Mean Words Correct for all readings andall students.estimated for the Teacher group.
During Expertrating, a randomly selected 5% of the passageswere presented again for rating to each scorer.Overall Expert intra-rater reliability was 0.9998,with intra-rater reliability scores for Expert 1,Expert 2, and Expert 3 at 0.9996, 1.0, and 1.0,respectively.
These results indicate that Experthuman scorers are extremely consistent whenasked to provide Words Correct scores for readingpassages when given the opportunity to listen tothe passages at a careful, uninterrupted pace.
Theautomated scoring system would produce the exactsame score (reliability = 1.0) every time it scoredthe same recordings, making its reliabilitycomparable.2.2.2.2.
Inter-rater reliabilityPearson?s r was used to estimate the inter-raterreliability.
All three Experts scored all passages,whereas any particular Teacher scored only asubset of the passages; thus, the Teacher?s scorewas used without consideration of which teacherprovided the score.
Inter-rater reliability resultsare summarized in Table 3.51Reliability  (N = 261)Teacher Expert 1 Expert 2Expert 1 0.998Expert 2 0.999 0.999Expert 3 0.998 0.999 0.999Table 3.
Inter-rater reliability estimates for Expertscorers.To provide a measure of a ?consensus?
expertscore, the median score from all 3 Experts wasderived for each passage, and then compared withthe Teacher score.
This comparison (Teacher vs.ExpertM) yielded a reliability of 0.999, p < .01.
Asshown in Table 3, all inter-rater reliabilityestimates are extremely high, indicating, in part,that teachers in the classroom produce scores thatdo not differ systematically from those given bycareful experts.2.2.3 Human-machine performancePearson?s r was computed to estimate thecorrelations.
The different scorer groups (i.e.,ExpertM, Teacher, and Machine) provided similarlyconsistent scoring, as evidenced by highcorrelations between scores from the three groups.These correlations were maintained even whendata were broken down into individual grades.Table 4 reveals correlations between WordsCorrect scores provided by all 3 scoring groups,for each grade individually, for all three gradescombined, and finally for the median scores for all87 students.Grade level (N) Machine ~ TeacherMachine ~ExpertMTeacher ~ExpertM1st grade (54) 0.990 0.990 0.9962nd grade (60) 0.990 0.991 0.9993rd grade (147) 0.964 0.962 0.997Grades 1-3 (261) 0.989 0.988 0.999Only medians 87 0.994 0.994 0.999Table 4.
Correlations between Words Correct scoresby Experts, Teachers, and machine.All correlations are 0.96 or higher.
Correlationsare highest between Teacher and ExpertM, butcorrelations between machine and both humangroups are consistently 0.96 or above.
Therelatively lower correlations between human andmachine scores seen in the third grade data may beattributed in large part to two outliers noted in theFigures below.
If these outliers are excluded fromthe analysis, both correlations between human andmachine scores in the third grade rise to 0.985.
(See below for discussion of these outliers.)2.2.4.1.
Teacher vs. Machine performancePearson?s r was used to estimate the correlationbetween Teacher and Machine scores.
First, theTeacher-generated Words Correct score andMachine-generated Words Correct scores wereobtained for each of the 261 individual recordings,where the correlation was found to be r = 0.989, p< .01.Words Correct scoresTeacher vs. Machiner = 0.989n = 2610204060801001201401601800 20 40 60 80 100 120 140 160 180Teacher WCMachineWCFigure 1.
Words Correct (WC) scores from Teachersand Machine; response level (n = 261 responses)Figure 1 shows a small number of outliers in thescatterplot (circled in red).
One outlier (human =3, machine = 21) came from a student whose lowlevel of reading skill required him to sound out theletters as he read; machine scores were high for all3 recordings from this reader.
One outlier (human= 21, machine = 10) occurred because the readerhad an unusually high pitched voice quality whichposed a particular challenge to the recognizer.Two outliers (human = 141, machine = 76; human= 139, machine = 104) suffered from a similarrecording quality-based issue whereby only someof the words were picked up by the system becausethe student read rapidly but quietly, making itdifficult for the system to consistently pick up theirvoices.
That is, for these calls the Teacher wasclose enough to hear the students?
entire reading52but the machine picked up only some of the wordsdue to distance from the telephone handset.5Next, median Words Correct scores for eachstudent were computed.
Median scores derivedfrom machine and Teachers correlated at 0.994, p< .01 for the 87 students.
These scores arepresented in Figure 2.Words Correct scoresTeacher vs. Machiner = 0.994n = 8703060901201501800 30 60 90 120 150 180Teacher WCMachineWCFigure 2.
Words Correct (WC) scores from Teachersand Machine scoring at the reader level (n = 87).Figure 2 shows that some of the outliers visible inthe individual recording data disappear when themedian score is computed for each student?sreading performance, as would be expected.2.2.4.2.
Expert vs. Machine performanceThe median of the 3 expert human scores for eachpassage (ExpertM) was compared to the Machinescore.
The correlation between machine-generatedWords Correct scores and ExpertM-generatedWords Correct scores was 0.988, p < .01, for the261 individual readings, and 0.999, p < .01, for themedian (student-level) scores.
These results aredisplayed in Figure 3.Figure 3 shows that two notable outliers presentin the Teacher analysis were also present in theExpertM analysis.
This may be due to the fact thatwhile the recordings were of a low enough volumeto present a challenge to the automated scoringsystem, they were of a sufficient quality for experthuman scorers to ?fill in the blanks?
by listening5 In a production version, these recordings would return aninstruction to re-administer the readings with better recordingconditions or to score the recordings.repeatedly (e.g., with the ability to turn up thevolume), and in some cases giving the studentcredit for a word spoken correctly even thoughthey, the scorers, were not completely confident ofhaving heard every portion of the word correctly.Though conjectural, it is reasonable to expect thatthe human listeners were able to interpolate thewords in a ?top down?
fashion in a way that themachine was not.Words Correct scoresExpert (consensus) vs. Machiner = 0.988n = 2610204060801001201401601800 20 40 60 80 100 120 140 160 180Expert WCMachineWCWords Correct scoresExpert (consensus) vs. Machiner = 0.994n = 8703060901201501800 30 60 90 120 150 180Expert WCMachineWCFigure 3.
Words Correct (WC) Machine scores vs.Expert scores for all 261 individual responses(top) and for 87 students at test level (bottom).2.2.5.
Scoring PrecisionIt is reasonable to assume that careful expertscorers provide the closest possible representationof how a reading should be scored, particularly ifthe Expert score represents a ?consensus?
of expertopinions.
Given the impracticality of having ateam of experts score every passage read by a child53in the classroom, automated machine scoringmight provide the preferred alternative if its scorescan be shown to be consistent with expert scores.To explore the consistency between scores fromTeachers and scores from the machine with scoresprovided by Experts, Teacher and Machine scoreswere compared against the median Expert score foreach call using linear regression.The standard error of the estimate (SEE) for thetwo human groups was computed.
The SEE maybe considered a measure of the accuracy of thepredictions made for Teacher and Machine scoresbased on the (median, ?consensus?)
Expert scores.Figure 4 below shows a scatterplot of the data,along with the R2 and SEE measures for bothTeacher and machine scores based on ExpertMscores.Scores from Teachers and Machine produce verysimilar regression lines and coefficients ofdetermination (R2 = 0.998 and 0.988 for Teachersand Machine, respectively).
The figure also showsthat, compared with the Machine scores, Teachers?scores approximate the predicted ExpertMed scoresmore closely (SEE for Teachers = 1.80 vs. 4.25 formachine).
This disparity appears to be driven bydiverging scores at the upper and lower end of thedistribution, as might be expected due to relativelysmaller numbers of scores at the ends of thedistribution.Median Words Correct ScoresTeacher vs Expert ; Machine vs ExpertR2 = 0.988, S.E.E.
= 4.25R2 = 0.998, S.E.E.
= 1.80n = 870204060801001201401601800 20 40 60 80 100 120 140 160 180Expert WCTeacher/Machine WCFigure 4.
Median Words Correct scores fromMachine (red squares) and Teachers (bluetriangles) plotted against median Expert scoresfor 87 students.
S.E.E.
= Standard error ofestimate.3 Summary/DiscussionCorrelations between human- and machine-basedWords Correct scores were found to be above 0.95for both individual reading passages and formedian scores per student.
The machine scoringwas consistent with human scoring performed byteachers following along with the readings in realtime (r = 0.989), and was also consistent withhuman scoring when performed by careful expertscorers who had the ability to listen to recordedrenditions repeatedly (r = 0.988).
Correlationswere consistent with those between expert scorers(all r?s between 0.998 and 0.999) and betweenTeachers and Experts (r = 0.999 and 0.988,respectively).These results demonstrate that text-independentmachine scoring of Words Correct for children?sclassroom reading predicts human scoresextremely well (almost always within a word ortwo).AcknowledgmentsThe authors acknowledge useful feedback from theanonymous reviewers that improved this paper.54ReferencesJennifer Balogh, Jared Bernstein, Jian Cheng & Brent Townshend.2005.
Ordinate Scoring of FAN in NAAL Phase III: AccuracyAnalysis.
Ordinate Corporation: Menlo Park, California.Jennifer Balogh, Jared Bernstein, Jian Cheng, Alistair Van Moere,Brent Townshend, Masanori Suzuki.
In press.
Validation ofautomated scoring of oral reading.
Educational and PsychologicalMeasurement.Jared Bernstein, Michael Cohen, Hy Murveit, Dmitry Rtischev,Dmitry, and Mitch Weintraub.
1990.
Automatic evaluation andtraining in English pronunciation.
In: Proc.
ICSLP-90: 1990Internat.
Conf.
on Spoken Language Processing, Kobe, Japan, pp.1185?1188.Matthew Black, Joseph Tepperman, Sungbok Lee, Patti Price, andShrikanth Narayanan.
2006.
Automatic detection andclassification of disfluent reading miscues in young children?sspeech for the purpose of assessment.
Proc.
InINTERSPEECH/ICSLP, Antwerp, Belgium.Jian Cheng & Jianqiang Shen.
2010.
Towards Accurate Recognitionfor Children's Oral Reading Fluency.
IEEE-SLT 2010, 91-96.Jian Cheng & Brent Townshend.
2009.
A rule-based language modelfor reading recognition.
SLaTE 2009.Lindy Crawford, Gerald Tindal, & Steve Stieber.
2001.
Using OralReading Rate to Predict Student Performance on StatewideAchievement Tests.
Educational Assessment, 7(4), 303-323.Maxine Eskanazi.
2009.
An overview of spoken language technologyfor education.
Speech Communication, 51, 832-844.David LaBerge & S. Jay Samuels.
1974.
Toward a theory ofautomatic information processing in reading.
CognitivePsychology, 6(2), 293-323.Mitchell P. Marcus, Beatrice Santorini, & Mary Ann Marcinkiewicz.1993.
Building a large annotated corpus of English: The PennTreebank.
Computational Linguistics 19(2):313-330.Jack Mostow, Steven F. Roth, Alexander G. Hauptmann, & MatthewKane.
1994.
A prototype reading coach that listens.
In Proc.
ofAAAI-94, 785?792.National Institute of Child Health and Human Development, ?Reportof the national reading panel.
Teaching children to read: Anevidence-based assessment of the scientific research literature onreading and its implications for reading instruction,?
Tech.
Rep.NIH Publication No.
00-4769, U.S. Government Printing Office,2000.Timothy V. Rasinski & .
James V. Hoffman.
2003.
Theory andresearch into practice: Oral reading in the school literacycurriculum.
Reading Research Quarterly, 38, 510-522.Joseph Tepperman, Matthew Black, Patti Price, Sungbok Lee, AbeKazemzadeh, Matteo Gerosa, Margaret Heritage, Abeer Always,and Shrikanth Narayanan.
2007.
A Bayesian network classifierfor word-level reading assessment.
Proceedings of ICSLP,Antwerp, Belgium.Steve Young, D. Ollason, V. Valtchev, & Phil Woodland.
2002.
TheHTK Book (for HTK Version 3.2).
Cambridge UniversityEngineering Department.Klaus Zechner, John, Sabatini, & Lei Chen.
2009.
Automatic scoringof children?s read-aloud text passages and word lists.
Proceedingsof the NAACL-HLT Workshop on Innovative Use of NLP forBuilding Educational Applications.
Boulder, Colorado.55
