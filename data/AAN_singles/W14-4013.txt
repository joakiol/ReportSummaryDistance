Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 112?121,October 25, 2014, Doha, Qatar.
c?2014 Association for Computational LinguisticsTransduction Recursive Auto-Associative Memory:Learning Bilingual Compositional Distributed Vector Representations ofInversion Transduction GrammarsKarteek Addanki Dekai WuHKUSTHuman Language Technology CenterDepartment of Computer Science and EngineeringHong Kong University of Science and Technology{vskaddanki|dekai}@cs.ust.hkAbstractWe introduce TRAAM, or TransductionRAAM, a fully bilingual generalizationof Pollack?s (1990) monolingual Recur-sive Auto-AssociativeMemory neural net-workmodel, in which each distributed vec-tor represents a bilingual constituent?i.e.,an instance of a transduction rule, whichspecifies a relation between two monolin-gual constituents and how their subcon-stituents should be permuted.
Bilingualterminals are special cases of bilingualconstituents, where a vector represents ei-ther (1) a bilingual token?a token-to-token or ?word-to-word?
translation rule?or (2) a bilingual segment?a segment-to-segment or ?phrase-to-phrase?
transla-tion rule.
TRAAMs have properties thatappear attractive for bilingual grammar in-duction and statistical machine translationapplications.
Training of TRAAM drivesboth the autoencoder weights and the vec-tor representations to evolve, such thatsimilar bilingual constituents tend to havemore similar vectors.1 IntroductionWe introduce Transduction RAAM?or TRAAMfor short?a recurrent neural network model thatgeneralizes the monolingual RAAMmodel of Pol-lack (1990) to a distributed vector representationof compositionally structured transduction gram-mars (Aho andUllman, 1972) that is fully bilingualfrom top to bottom.
In RAAM, which stands forRecursive Auto-Associative Memory, using fea-ture vectors to characterize constituents at everylevel of a parse tree has the advantages that (1)the entire context of all subtrees inside the con-stituent can be efficiently captured in the featurevectors, (2) the learned representations generalizewell because similar feature vectors represent sim-ilar constituents or segments, and (3) representa-tions can be automatically learned so as to max-imize prediction accuracy for various tasks usingsemi-supervised learning.
We argue that different,but analogous, properties are desirable for bilin-gual structured translation models.Unlike RAAM, where each distributed vectorrepresents a monolingual token or constituent,each distributed vector in TRAAM represents abilingual constituent or biconstituent?that is, aninstance of a transduction rule, which asserts a re-lation between two monolingual constituents, aswell as specifying how to permute their subcon-stituents in translation.
Bilingual terminals, orbiterminals, are special cases of biconstituentswhere a vector represents either (1) a bitoken?atoken-to-token or ?word-to-word?
translation rule?or (2) a bisegment?a segment-to-segment or?phrase-to-phrase?
translation rule.The properties of TRAAMs are attractive formachine translation applications.
As with RAAM,TRAAMs can be trained via backpropagationtraining, which simultaneously evolves both theautoencoder weights and the biconstituent vectorrepresentations.
As with RAAM, the evolutionof the vector representations within the hiddenlayer performs automatic feature induction, and formany applications can obviate the need for man-ual feature engineering.
However, the result isthat similar vectors tend to represent similar bicon-stituents, rather than monolingual constituents.The learned vector representations thus tend toform clusters of similar translation relations in-stead of merely similar strings.
That is, TRAAMclusters represent soft nonterminal categories ofcross-lingual relations and translation patterns, asopposed to soft nonterminal categories of mono-lingual strings as in RAAM.Also, TRAAMs inherently make full simulta-neous use of both input and output language fea-112tures, recursively, in an elegant integrated fash-ion.
TRAAM does not make restrictive a pri-ori assumptions of conditional independence be-tween input and output language features.
Whenevolving the biconstituent vector representations,generalization occurs over similar input and out-put structural characteristics simultaneously.
Inmost recurrent neural network applications to ma-chine translation to date, only input side featuresor only output language features are used.
Even inthe few previous cases where recurrent neural net-works have employed both input and output lan-guage features for machine translation, the modelshave typically been factored so that their recursiveportion is applied only to either the input or outputlanguage, but not both.As with RAAM, the objective criteria for train-ing can be adjusted to reflect accuracy on nu-merous different kinds of tasks, biasing the di-rection that vector representations evolve toward.But again, TRAAM?s learned vector representa-tions support making predictions that simultane-ously make use of both input and output struc-tural characteristics.
For example, TRAAM hasthe ability to take into account the structure ofboth input and output subtree characteristics whilemaking predictions on reordering them.
Similarly,for specific cross-lingual tasks such as word align-ment, sense disambiguation, or machine transla-tion, classifiers can simultaneously be trained inconjunction with evolving the vector representa-tions to optimize task-specific accuracy (Chris-man, 1991).In this paper we use as examples binary bi-parse trees consistent with transduction grammarsin a 2-normal form, which by definition are in-version transduction grammars (Wu, 1997) sincethey are binary rank.
This is not a requirementfor TRAAM, which in general can be formed fortransduction grammars of any rank.
Moreover,with distributed vector representations, the notionof nonterminal categories in TRAAM is that of softmembership, unlike in symbolically representedtransduction grammars.
We start with bracketedtraining data that contains no bilingual categorylabels (like training data for Bracketing ITGs orBITGs).
Training results in self-organizing clus-ters that have been automatically induced, repre-senting soft nonterminal categories (unlike BITGs,which do not have differentiated nonterminal cat-egories).2 Related workTRAAM builds on different aspects of a spec-trum of previous work.
A large body of work ex-ists on various different types of self-organizingrecurrent neural network approaches to model-ing recursive structure, but mostly in monolin-gual modeling.
Even in applications to ma-chine translation or cross-lingual modeling, thetypical practice has been to insert neural net-work scoring components while still maintain-ing older SMT modeling assumptions like bags-of-words/phrases, ?shake?n?bake?
translation thatrelies heavily on strong monolingual languagemodels, and log-linear models?in contrast toTRAAM?s fully integrated bilingual approach.Here we survey representative work across thespectrum.2.1 Monolingual related workDistributed vector representations have longbeen used for n-gram language modeling; thesecontinuous-valued models exploit the general-ization capabilities of neural networks, althoughthere is no hidden contextual or hierarchicalstructure as in RAAM.
Schwenk (2010) appliesone such language model within an SMT system.In the simple recurrent neural networks (RNNsor SRNs) of Elman (1990), hidden layer represen-tations are fed back to the input to dynamically rep-resent an aggregate of the immediate contextualhistory.
More recently, the probabilistic NNLMsof Bengio et al.
(2003) and Bengio et al.
(2009)follow in this vein.To represent hierarchical tree structure usingvector representations, one simple family of ap-proaches employs convolutional networks, as inLee et al.
(2009) for example.
Collobert and We-ston (2008) use a convolution neural network layerquite effectively to learn vector representations forwords which are then used in a host of NLP taskssuch as POS tagging, chunking, and semantic rolelabeling.RAAM approaches, and related recursive au-toencoder approaches, can be more flexible thanconvolutional networks.
Like SRNs, they can beextended in numerous ways.
The URAAM (Uni-fication RAAM) model of Stolcke and Wu (1992)extended RAAM to demonstrate the possibility ofusing neural networks to perform more sophisti-cated operations like unification directly upon thedistributed vector representations of hierarchical113feature structures.
Socher et al.
(2011) used mono-lingual recursive autoencoders for sentiment pre-diction, with or without parse tree information; thiswas perhaps the first use of a RAAM style ap-proach on a large scale NLP task, albeit mono-lingual.
Scheible and Sch?tze (2013) automat-ically simplified the monolingual tree structuresgenerated by recursive autoencoders, validated thesimplified structures via manual evaluation, andshowed that sentiment classification accuracy isnot affected.2.2 Bilingual related workThe majority of work on learning bilingual dis-tributed vector representations has not made use ofrecursive approaches or hidden contextual or com-positional structure, as in the bilingual word em-bedding learning of Klementiev et al.
(2012) or thebilingual phrase embedding learning of Gao et al.(2014).
Schwenk (2012) uses a non-recursive neu-ral network to predict phrase translation probabil-ities in conventional phrase-based SMT.Attempts have been made to generalize the dis-tributed vector representations of monolingual n-gram language models, avoiding any hidden con-textual or hierarchical structure.
Working withinthe framework of n-gram translation models, Sonet al.
(2012) generalize left-to-right monolingualn-gram models to bilingual n-grams, and studybilingual variants of class-based n-grams.
How-ever, their model does not allow tackling the chal-lenge of modeling cross-lingual constituent order,as TRAAM does; instead it relies on the assump-tion that some other preprocessor has already man-aged to accurately re-order the words of the inputsentence into exactly the order of words in the out-put sentence.Similarly, generalizations of monolingual SRNsto the bilingual case have been studied.
Zouet al.
(2013) generalize the monolingual recur-rent NNLM model of Bengio et al.
(2009) tolearn bilingual word embeddings using conven-tional SMTword alignments, and demonstrate thatthe resulting embeddings outperform the baselinesin word semantic similarity.
They also add a sin-gle semantic similarity feature induced with bilin-gual embeddings to a phrase-based SMT log-linearmodel, and report improvements in BLEU.
Com-pared to TRAAM, however, they only learn non-compositional features, with distributed vectorsonly representing biterminals (as opposed to bi-constituents or bilingual subtrees), and so othermechanisms for combining biterminal scores stillneed to be used to handle hierarchical structure,as opposed to seamlessly being integrated intothe distributed vector representation model.
De-vlin et al.
(2014) obtain translation accuracy im-provements by extending the probabilistic NNLMsof Bengio et al.
(2003), which are used for theoutput language, by adding input language con-text features.
Unlike TRAAM, neither of theseapproaches symmetrically models the recursivestructure of both the input and output languagesides.For convolutional network approaches, Kalch-brenner and Blunsom (2013) use a recurrent prob-abilistic model to generate a representation of thesource sentence and then generate the target sen-tence from this representation.
This use of in-put language context to bias translation choicesis in some sense a neural network analogy tothe PSD (phrase sense disambiguation) approachfor context-dependent translation probabilities ofCarpuat and Wu (2007).
Unlike TRAAM, themodel does not contain structural constraints, andpermutation of phrases must still be done in con-ventional PBSMT ?shake?n?bake?
style by rely-ing mostly on a language model (in their case, aNNLM).A few applications ofmonolingual RAAM-stylerecursive autoencoders to bilingual tasks have alsoappeared.
For cross-lingual document classifica-tion, Hermann and Blunsom (2014) use two sep-arate monolingual fixed vector composition net-works, one for each language.
One provides thetraining signal for the other, and training is onlyon the embeddings.Li et al.
(2013) described a use of monolingualrecursive autoencoders within maximum entropyITGs.
They replace their earlier model for pre-dicting reordering based on the first and the lasttokens in a constituent, by instead using the con-text vector generated using the recursive autoen-coder.
Only input language context is used, unlikeTRAAM which can use the input and output lan-guage contexts equally.Autoencoders have also been applied to SMT ina very different way by Zhao et al.
(2014) but with-out recursion and not for learning distributed vec-tor representations of words; rather, they used non-recursive autoencoders to compress very high-dimensional bilingual sparse features down to low-dimensional feature vectors, so that MIRA or PRO114could be used to optimize the log-linear modelweights.3 Representing transduction grammarswith TRAAMAs a recurrent neural network representation of atransduction grammar, TRAAM learns bilingualdistributed representations that parallel the struc-tural composition of a transduction grammar.
Aswith transduction grammars, the learned represen-tations are symmetric and model structured rela-tional correlations between the input and outputlanguages.
The induced feature vectors in effectrepresent soft categories of cross-lingual relationsand translations.
The TRAAM model integrateselegantly with the transduction grammar formal-ism and aims to model the compositional struc-ture of the transduction grammar as opposed toincorporating external alignment information.
Itis straightforward to formulate TRAAMs for arbi-trary syntax directed transduction grammars; herewe shall describe an example of a TRAAM modelfor an inversion transduction grammar (ITG).Formally, an ITG is a tuple ?N,?,?, R, S?,where N is a finite nonempty set of nonterminalsymbols,?
is a finite set of terminal symbols inL0,?
is a finite set of terminal symbols in  L1, R is afinite nonempty set of inversion transduction rulesandS ?
N is a designated start symbol.
A normal-form ITG consists of rules in one of the followingfour forms:S ?
A, A ?
[BC] , A ?
?BC?, A ?
e/fwhere S ?
N is the start symbol, A,B,C ?N  are nonterminal symbols and e/f  is a biter-minal.
A biterminal is a pair of symbol strings:????
?, where at least one of the strings have tobe nonempty.
The square and angled brackets sig-nal straight and inverted order respectively.
Withstraight order, both the L0and the L1productionsare generated left-to-right, but with inverted order,the L1production is generated right-to-left.In the distributed TRAAM representation of theITG, we represent each bispan, using a feature vec-tor v of dimension d that represents a fuzzy encod-ing of all the nonterminals that could generate it.This is in contrast to the ITG model where eachnonterminal that generates a bispan has to be enu-merated separately.
Feature vectors correspond-ing to larger bispans are compositionally generatedfrom smaller bispans using a compressor networkwhich takes two feature vectors of dimension d,corresponding to the smaller bispans and gener-ates the feature vector of dimension d correspond-ing to the larger bispan.
A single bit correspond-ing to straight or inverted order is also fed as aninput to the compressor network.
The compres-sor network in TRAAM serves a similar role asthe syntactic rules in the symbolic ITG, but keepsthe encoding fuzzy.
Figure 2 shows the straightand inverted syntactic rules and the correspond-ing inputs to the compressor network.
Modelingof unary rules (with start symbol on the left handside) although similar, is beyond the scope of thispaper.It is easy to demonstrate that TRAAM mod-els are capable of representing any symbolic ITGmodel.
All the nonterminals representing a bispancan be encoded as a bit vector in the feature vectorof the bispan.
Using the universal approximationtheorem of neural networks (Hornik et al., 1989),an encoder with a single hidden layer can representany set of syntactic rules.
Similarly, all TRAAMmodels can be represented using a symbolic ITGby assuming a unique nonterminal label for everyfeature vector.
Therefore, TRAAM and ITGs rep-resent two equivalent classes of models for repre-senting compositional bilingual relations.It is important to note that although bothTRAAM and ITG models might be equivalent, thefuzzy encoding of nonterminals in TRAAM is suit-able for modeling the generalizations in bilingualrelationswithout exploding the search space unlikethe symbolic models.
This property of TRAAMmakes it attractive for bilingual category learningand machine translation applications as long as ap-propriate language bias and objective functions aredetermined.Given our objective of inducing categories ofbilingual relations in an unsupervised manner, webias our TRAAM model by using a simple non-linear activation function to be our compressor,similar to the monolingual recursive autoencodermodel proposed by Socher et al.
(2011).
Having asingle layer in our compressor provides the neces-sary language bias by forcing the network to cap-ture the generalizations while reducing the dimen-sions of the input vectors.
We use tanh as the non-linear activation function and the compressor ac-cepts two vectors c1and c2of dimension d corre-sponding to the nonterminals of the smaller bis-pans and a single bit o corresponding to the in-115Figure 1: Example of English-Telugu biparse trees where inversion depends on output language sense.CompressorCompressorReconstructorReconstructoro1#=#1# c1# c2# c3#o2#=#(1#p1#o1'# c1'# c2'#p2#o2'# p1'# c3'#Figure 2: Architecture of TRAAM.version order and generates a vector p of dimen-sion d corresponding to the larger bispan generatedby combining the two smaller bispans as shown inFigure 2.
The vector p then serves as the input forthe successive combinations of the larger bispanwith other bispans.p = tanh(W1[o; c1; c2] + b1) (1)whereW1and b1are the weight matrix and the biasvector of the encoder network.To ensure that the computed vector p capturesthe fuzzy encodings of its children and the inver-sion order, we use a reconstructor network whichattempts to reconstruct the inversion order and thefeature vectors corresponding of its children.
Weuse the error in reconstruction as our objectivefunction and train our model to minimize the re-construction error over all the nodes in the biparsetree.
The reconstructor network in our TRAAMmodel can be replaced by any other network thatenables the computed feature vector representa-tions to be optimized for the given task.
In ourcurrent implementation, we reconstruct the inver-sion order o?
and the child vectors c?1and c?2usinganother nonlinear activation function as follows:[o?
; c?1; c?2] = tanh(W2p+ b2) (2)whereW2and b2are the weight matrix and the biasvector of the reconstructor network.4 Bilingual training4.1 InitializationThe weights and the biases of the compressor andthe reconstructor networks of the TRAAM modelare randomly initialized.
Bisegment embeddings116corresponding to the leaf nodes (biterminals in thesymbolic ITG notation) in the biparse trees are alsoinitialized randomly.
These constitute the modelparameters and are optimized to minimize our ob-jective function of reconstruction error.
The parsetrees for providing the structural constraints aregenerated by a bracketing inversion transductiongrammar (BITG) induced in a purely unsupervisedfashion, according to the algorithm in Saers et al.(2009).
Due to constraints on the training time, weconsider only the Viterbi biparse trees accordingto the BITG instead of all the biparse trees in theforest.4.2 Computing feature vectorsWe compute the feature vectors at each internalnode in the biparse tree, similar to the feedforwardpass in a neural network.
We topologically sort allthe nodes in the biparse tree and set the feature vec-tor of each node in the topologically sorted orderas follows:?
If the node is a leaf node, the feature vector isthe corresponding bisegment embedding.?
Else, the biconstituent embedding corre-sponding to the internal node is generated us-ing the feature vectors of the children and theinversion order using Equation 1.
We alsonormalize the length of the computed fea-ture vector so as to prevent the network frommaking the biconstituent embedding arbitrar-ily small in magnitude (Socher et al., 2011).4.3 Feature vector optimizationWe train our current implementation of TRAAM,by optimizing the model parameters to minimizean objective function based on the reconstructionerror over all the nodes in the biparse trees.
Theobjective function is defined as a linear combina-tion of the l2 norm of the reconstruction error ofthe children and the cross-entropy loss of recon-structing the inversion order.
We define the errorat each internal node n as follows:En=?2|[c1; c2]?
[c?1; c?2]|2?
(1?
?)[(1?
o) log(1?
o?)
+ (1 + o) log(1 + o?
)]where c1, c2, o correspond to the left child, rightchild and inversion order, c?1, c?2, o?
are the respec-tive reconstructions and ?
is the linear weightingfactor.
The global objective function J is the sumof the error function at all internal nodesn in the bi-parse trees averaged over the total number of sen-tences T in the corpus.
A regularization parameter?
is used on the norm of the model parameters ?
toavoid overfitting.J =1T?nEn+ ?||?||2 (3)As the bisegment embeddings are also a part ofthe model parameters, the optimization objectiveis similar to a moving target training objective Ro-hwer (1990).
We use backpropagation with struc-ture Goller and Kuchler (1996) to compute the gra-dients efficiently.
L-BFGS algorithm Liu and No-cedal (1989) is used in order to minimize the lossfunction.5 Bilingual representation learningWe expect the TRAAM model to generate clus-ters over cross-lingual relations similar to RAAMmodels on monolingual data.
We test this hypoth-esis by bilingually training our model using a par-allel English-Telugu blocks world dataset.
Thedataset is kept simple to better understand the na-ture of clusters.
Our dataset comprises of com-mands which involves manipulating different col-ored objects over different shapes.5.1 ExampleFigure 1 shows the biparse trees for two English-Telugu sentence pairs.
The preposition on in En-glish translates to ???????
(pinunna) and ????
(pina) re-spectively in the first and second sentence pairs be-cause in the first sentence block is described by itsposition on the square, whereas in the second sen-tence block is the subject and square is the object.Since Telugu is a language with an SOV structure,the verbs ????
(vunchu) and ?????
(teesuko) occur atthe end for both sentences.The sentences in 1 illustrate the importance ofmodeling bilingual relations simultaneously in-stead of focusing only on the input or output lan-guage as the cross-lingual structural relations aresensitive to both the input and output languagecontext.
For example, the constituent whose inputside is block on the square, the corresponding outputlanguage tree structure is determined by whetheror not on is translated to ???????
(pinunna) or ????
(pina).In symbolic frameworks such as ITGs, suchrelations are encoded using different nontermi-nal categories.
However, inducing such cate-117Figure 3: Clustering of biconstituents in the Telugu-English data.gories within a symbolic framework in an un-supervised manner creates extremely challengingcombinatorial scaling issues.
TRAAM modelsare a promising approach for tackling this prob-lem, since the vector representations learned us-ing the TRAAM model inherently yield soft syn-tactic category membership properties, despite be-ing trained only with the unlabeled structural con-straints of simple BITG-style data.5.2 Biconstituent clusteringThe soft membership properties of learned dis-tributed vector representations can be exploredvia cluster analysis.
To illustrate, we trained aTRAAM network bilingually using the algorithmin Section 4, and obtained feature vector represen-tations for each unique biconstituent.
Clusteringthe obtained feature vectors reveals emergence offuzzy nonterminal categories, as shown in Figure3.
It is important to note that each point in thevector space corresponds to a tree-structured bi-constituent as opposed to merely a flat bilingualphrase, as same surface forms with different treestructures will have different vectors.As the full cluster tree is too unwieldy, Figure4 zooms in to shows an enlarged version of a por-tion of the clustering, alongwith the correspondingbracketed bilingual structures.
One can observethat the cluster represents the biconstituents thatdescribe the object by its position on another ob-ject.
We can deduce this from the fact that only asingle sense of on/???????
(pinnuna) seems to be occur-ing in all the biconstituents of the cluster.
Manualinspection of other clusters reveals such similari-ties despite noise expected to be introduced by thesparsity of our dataset.6 ConclusionWe have introduced a fully bilingual generaliza-tion of Pollack?s (1990) monolingual RecursiveAuto-Associative Memory neural network model,TRAAM, in which each distributed vector repre-sents a bilingual constituent?i.e., an instance ofa transduction rule, which specifies a relation be-tween two monolingual constituents and how theirsubconstituents should be permuted.
Bilingual ter-minals are special cases of bilingual constituents,where a vector represents either (1) a bilingual to-ken?a token-to-token or ?word-to-word?
transla-tion rule?or (2) a bilingual segment?a segment-to-segment or ?phrase-to-phrase?
translation rule.TRAAMs can be used for arbitrary rank SDTGs(syntax-directed transduction grammars, a.k.a.synchronous context-free grammars).
Althoughour discussions in this paper have focused on bi-parse trees from SDTGs in a 2-normal form, whichby definition are ITGs due to the binary rank,nothing prevents TRAAMs from being applied tohigher-rank transduction grammars.We believe TRAAMs are worth detailed ex-ploration as their intrinsic properties address keyproblems in bilingual grammar induction and sta-118Figure 4: Typical zoomed view into the Telugu-English biconstituent clusters from Figure 3.tistical machine translation?their sensitivity toboth input and output language context means thatthe learned vector representations tend to reflectthe similarity of bilingual rather than monolingualconstituents, which is what is needed to induce dif-ferentiated bilingual nonterminal categories.7 AcknowledgmentsThis material is based upon work supportedin part by the Defense Advanced ResearchProjects Agency (DARPA) under BOLT contractnos.
HR0011-12-C-0014 and HR0011-12-C-0016,and GALE contract nos.
HR0011-06-C-0022 andHR0011-06-C-0023; by the European Union un-der the FP7 grant agreement no.
287658; and bythe Hong Kong Research Grants Council (RGC)research grants GRF620811, GRF621008, andGRF612806.
Any opinions, findings and conclu-sions or recommendations expressed in this mate-rial are those of the authors and do not necessarilyreflect the views of DARPA, the EU, or RGC.ReferencesAlfred V. Aho and Jeffrey D. Ullman.
The The-ory of Parsing, Translation, and Compiling.Prentice-Halll, Englewood Cliffs, New Jersey,1972.Yoshua Bengio, R?jean Ducharme, Pascal Vin-cent, and Christian Jauvin.
A neural probabilis-tic language model.
Journal of Machine Learn-ing Research, 3:1137?1155, 2003.Yoshua Bengio, J?r?me Louradour, Ronan Col-lobert, and Jason Weston.
Curriculum learning.In Proceedings of the 26th annual internationalconference on machine learning, pages 41?48.ACM, 2009.Marine Carpuat and Dekai Wu.
Context-dependent phrasal translation lexicons for sta-tistical machine translation.
In 11th MachineTranslation Summit (MT Summit XI), pages 73?80, 2007.Lonnie Chrisman.
Learning recursive distributedrepresentations for holistic computation.
Con-nection Science, 3(4):345?366, 1991.Ronan Collobert and Jason Weston.
A unifiedarchitecture for natural language processing:Deep neural networks with multitask learning.In Proceedings of the 25th International Con-ference on Machine Learning, ICML ?08, pages160?167, New York, NY, USA, 2008.
ACM.Jacob Devlin, Rabih Zbib, Zhongqiang Huang,Thomas Lamar, Richard Schwartz, and JohnMakhoul.
Fast and robust neural network jointmodels for statistical machine translation.
In11952nd Annual Meeting of the Association forComputational Linguistics, 2014.Jeffrey L Elman.
Finding structure in time.
Cog-nitive science, 14(2):179?211, 1990.Jianfeng Gao, Xiaodong He, Wen-tau Yih, andLi Deng.
Learning continuous phrase represen-tations for translation modeling.
In 52nd AnnualMeeting of the Association for ComputationalLinguistics (Short Papers), 2014.Christoph Goller and Andreas Kuchler.
Learn-ing task-dependent distributed representationsby backpropagation through structure.
In Neu-ral Networks, 1996., IEEE International Con-ference on, volume 1, pages 347?352.
IEEE,1996.Karl Moritz Hermann and Phil Blunsom.
Multi-lingual models for compositional distributed se-mantics.
In 52nd Annual Meeting of the Asso-ciation for Computational Linguistics, volumeabs/1404.4641, 2014.Kurt Hornik, Maxwell Stinchcombe, and Hal-bert White.
Multilayer feedforward networksare universal approximators.
Neural networks,2(5):359?366, 1989.Nal Kalchbrenner and Phil Blunsom.
Recurrentcontinuous translation models.
In EMNLP,pages 1700?1709, 2013.Alexandre Klementiev, Ivan Titov, and BinodBhattarai.
Inducing crosslingual distributedrepresentations of words.
In 24th Interna-tional Conference on Computational Linguistics(COLING 2012).
Citeseer, 2012.Honglak Lee, Roger Grosse, Rajesh Ranganath,and Andrew Y Ng.
Convolutional deep be-lief networks for scalable unsupervised learningof hierarchical representations.
In Proceedingsof the 26th Annual International Conferenceon Machine Learning, pages 609?616.
ACM,2009.Peng Li, Yang Liu, and Maosong Sun.
Recur-sive autoencoders for itg-based translation.
InEMNLP, pages 567?577, 2013.Dong C Liu and Jorge Nocedal.
On the limitedmemory bfgs method for large scale optimiza-tion.
Mathematical programming, 45(1-3):503?528, 1989.Jordan B Pollack.
Recursive distributed represen-tations.
Artificial Intelligence, 46(1):77?105,1990.Richard Rohwer.
The ?moving targets?training al-gorithm.
In Neural Networks, pages 100?109.Springer, 1990.Markus Saers, Joakim Nivre, and Dekai Wu.Learning stochastic bracketing inversion trans-duction grammars with a cubic time biparsingalgorithm.
In 11th International Conference onParsing Technologies (IWPT?09), pages 29?32,Paris, France, October 2009.Christian Scheible and Hinrich Sch?tze.
Cuttingrecursive autoencoder trees.
In 1st InternationalConference on Learning Representations (ICLR2013), Scottsdale, Arizona, May 2013.Holger Schwenk.
Continuous-space languagemodels for statistical machine translation.
InThe Prague Bulletin of Mathematical Linguis-tics, volume 93, pages 137?146, 2010.Holger Schwenk.
Continuous space transla-tion models for phrase-based statistical machinetranslation.
In Proceedings of COLING 2012:Posters, pages 1071??1080.
Citeseer, 2012.Richard Socher, Jeffrey Pennington, Eric HHuang, Andrew Y Ng, and Christopher D Man-ning.
Semi-supervised recursive autoencodersfor predicting sentiment distributions.
In Pro-ceedings of the Conference on Empirical Meth-ods in Natural Language Processing, pages151?161.
Association for Computational Lin-guistics, 2011.Le Hai Son, Alexandre Allauzen, and Fran?oisYvon.
Continuous space translationmodels withneural networks.
In Proceedings of the 2012conference of the north american chapter of theassociation for computational linguistics: Hu-man language technologies, pages 39?48.
As-sociation for Computational Linguistics, 2012.Andreas Stolcke and Dekai Wu.
Tree match-ing with recursive distributed representations.In AAAI 1992 Workshop on Integrating Neu-ral and Symbolic Processes?The Cognitive Di-mension, 1992.Dekai Wu.
Stochastic inversion transductiongrammars and bilingual parsing of parallel cor-pora.
Computational Linguistics, 23(3):377?403, 1997.Bing Zhao, Yik-Cheung Tam, and Jing Zheng.An autoencoder with bilingual sparse featuresfor improved statistical machine translation.
In120IEEE International Conference on Acoustic,Speech and Signal Processing (ICASSP), 2014.Will Y Zou, Richard Socher, Daniel M Cer, andChristopher DManning.
Bilingual word embed-dings for phrase-based machine translation.
InEMNLP, pages 1393?1398, 2013.121
