Proceedings of the ACL-2012 Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics (ExProM-2012),pages 57?64, Jeju, Republic of Korea, 13 July 2012. c?2012 Association for Computational LinguisticsStatistical Modality Taggingfrom Rule-based Annotations and CrowdsourcingVinodkumar Prabhakaran Michael Bloodgood Mona DiabCS CASL CCLSColumbia University University of Maryland Columbia Universityvinod@cs.columbia.edu meb@umd.edu mdiab@ccls.columbia.eduBonnie Dorr Lori Levin Christine D. PiatkoCS and UMIACS LTI APLUniversity of Maryland Carnegie Mellon University Johns Hopkins Universitybonnie@umiacs.umd.edu lsl@cs.cmu.edu christine.piatko@jhuapl.eduOwen Rambow Benjamin Van DurmeCCLS HLTCOEColumbia University Johns Hopkins Universityrambow@ccls.columbia.edu vandurme@cs.jhu.eduAbstractWe explore training an automatic modalitytagger.
Modality is the attitude that a speakermight have toward an event or state.
One ofthe main hurdles for training a linguistic tag-ger is gathering training data.
This is par-ticularly problematic for training a tagger formodality because modality triggers are sparsefor the overwhelming majority of sentences.We investigate an approach to automaticallytraining a modality tagger where we first gath-ered sentences based on a high-recall simplerule-based modality tagger and then providedthese sentences to Mechanical Turk annotatorsfor further annotation.
We used the resultingset of training data to train a precise modalitytagger using a multi-class SVM that deliversgood performance.1 IntroductionModality is an extra-propositional component ofmeaning.
In John may go to NY, the basic propo-sition is John go to NY and the word may indi-cates modality.
Van Der Auwera and Ammann(2005) define core cases of modality: John mustgo to NY (epistemic necessity), John might go toNY (epistemic possibility), John has to leave now(deontic necessity) and John may leave now (de-ontic possibility).
Many semanticists (e.g.
Kratzer(1981), Kratzer (1991), Kaufmann et al (2006)) de-fine modality as quantification over possible worlds.John might go means that there exist some possi-ble worlds in which John goes.
Another view ofmodality relates more to a speakers attitude towarda proposition (e.g.
McShane et al (2004)).Modality might be construed broadly to includeseveral types of attitudes that a speaker wants to ex-press towards an event, state or proposition.
Modal-ity might indicate factivity, evidentiality, or senti-ment (McShane et al, 2004).
Factivity is related towhether the speaker wishes to convey his or her be-lief that the propositional content is true or not, i.e.,whether it actually obtains in this world or not.
Itdistinguishes things that (the speaker believes) hap-pened from things that he or she desires, plans, orconsiders merely probable.
Evidentiality deals withthe source of information and may provide clues tothe reliability of the information.
Did the speaker57have firsthand knowledge of what he or she is re-porting, or was it hearsay or inferred from indirectevidence?
Sentiment deals with a speaker?s positiveor negative feelings toward an event, state, or propo-sition.In this paper, we focus on the following fivemodalities; we have investigated the belief/factivitymodality previously (Diab et al, 2009b; Prab-hakaran et al, 2010), and we leave other modalitiesto future work.?
Ability: can H do P??
Effort: does H try to do P??
Intention: does H intend P??
Success: does H succeed in P??
Want: does H want P?We investigate automatically training a modalitytagger by using multi-class Support Vector Ma-chines (SVMs).
One of the main hurdles for traininga linguistic tagger is gathering training data.
This isparticularly problematic for training a modality tag-ger because modality triggers are sparse for the over-whelming majority of the sentences.
Baker et al(2010) created a modality tagger by using a semi-automatic approach for creating rules for a rule-based tagger.
A pilot study revealed that it can boostrecall well above the naturally occurring proportionof modality without annotated data but with only60% precision.
We investigated an approach wherewe first gathered sentences based on a simple modal-ity tagger and then provided these sentences to an-notators for further annotation, The resulting anno-tated data also preserved the level of inter-annotatoragreement for each example so that learning algo-rithms could take that into account during training.Finally, the resulting set of annotations was used fortraining a modality tagger using SVMs, which gavea high precision indicating the success of this ap-proach.Section 2 discusses related work.
Section 3 dis-cusses our procedure for gathering training data.Section 4 discusses the machine learning setupand features used to train our modality tagger andpresents experiments and results.
Section 5 con-cludes and discusses future work.2 Related WorkPrevious related work includes TimeML (Sauri etal., 2006), which involves modality annotation onevents, and Factbank (Sauri and Pustejovsky, 2009),where event mentions are marked with degree of fac-tuality.
Modality is also important in the detection ofuncertainty and hedging.
The CoNLL shared task in2010 (Farkas et al, 2010) deals with automatic de-tection of uncertainty and hedging in Wikipedia andbiomedical sentences.Baker et al (2010) and Baker et al (2012) ana-lyze a set of eight modalities which include belief,require and permit, in addition to the five modalitieswe focus on in this paper.
They built a rule-basedmodality tagger using a semi-automatic approach tocreate rules.
This earlier work differs from the workdescribed in this paper in that the our emphasis is onthe creation of an automatic modality tagger usingmachine learning techniques.
Note that the anno-tation and automatic tagging of the belief modality(i.e., factivity) is described in more detail in (Diab etal., 2009b; Prabhakaran et al, 2010).There has been a considerable amount of inter-est in modality in the biomedical domain.
Negation,uncertainty, and hedging are annotated in the Bio-scope corpus (Vincze et al, 2008), along with infor-mation about which words are in the scope of nega-tion/uncertainty.
The i2b2 NLP Shared Task in 2010included a track for detecting assertion status (e.g.present, absent, possible, conditional, hypotheticaletc.)
of medical problems in clinical records.1 Apos-tolova et al (2011) presents a rule-based system forthe detection of negation and speculation scopes us-ing the Bioscope corpus.
Other studies emphasizethe importance of detecting uncertainty in medicaltext summarization (Morante and Daelemans, 2009;Aramaki et al, 2009).Modality has also received some attention in thecontext of certain applications.
Earlier work de-scribing the difficulty of correctly translating modal-ity using machine translation includes (Sigurd andGawro?nska, 1994) and (Murata et al, 2005).
Sig-urd et al (1994) write about rule based frameworksand how using alternate grammatical constructionssuch as the passive can improve the rendering of themodal in the target language.
Murata et al (2005)1https://www.i2b2.org/NLP/Relations/58analyze the translation of Japanese into Englishby several systems, showing they often render thepresent incorrectly as the progressive.
The authorstrained a support vector machine to specifically han-dle modal constructions, while our modal annotationapproach is a part of a full translation system.The textual entailment literature includes modal-ity annotation schemes.
Identifying modalities isimportant to determine whether a text entails a hy-pothesis.
Bar-Haim et al (2007) include polaritybased rules and negation and modality annotationrules.
The polarity rules are based on an indepen-dent polarity lexicon (Nairn et al, 2006).
The an-notation rules for negation and modality of predi-cates are based on identifying modal verbs, as wellas conditional sentences and modal adverbials.
Theauthors read the modality off parse trees directly us-ing simple structural rules for modifiers.3 Constructing Modality Training DataIn this section, we will discuss the procedure wefollowed to construct the training data for build-ing the automatic modality tagger.
In a pilot study,we obtained and ran the modality tagger describedin (Baker et al, 2010) on the English side of theUrdu-English LDC language pack.2 We randomlyselected 1997 sentences that the tagger had labeledas not having the Want modality and posted them onAmazon Mechanical Turk (MTurk).
Three differ-ent Turkers (MTurk annotators) marked, for each ofthe sentences, whether it contained the Want modal-ity.
Using majority rules as the Turker judgment,95 (i.e., 4.76%) of these sentences were marked ashaving a Want modality.
We also posted 1993 sen-tences that the tagger had labeled as having a Wantmodality and only 1238 of them were marked by theTurkers as having a Want modality.
Therefore, theestimated precision of this type of approach is onlyaround 60%.Hence, we will not be able to use the (Baker etal., 2010) tagger to gather training data.
Instead,our approach was to apply a simple tagger as a firstpass, with positive examples subsequently hand-annotated using MTurk.
We made use of sentencedata from the Enron email corpus,3 derived from the2LDC Catalog No.
: LDC2006E110.3http://www-2.cs.cmu.edu/?enron/version owing to Fiore and Heer,4 further processedas described by (Roark, 2009).5To construct the simple tagger (the first pass), weused a lexicon of modality trigger words (e.g., try,plan, aim, wish, want) constructed by Baker et al(2010).
The tagger essentially tags each sentencethat has a word in the lexicon with the correspondingmodality.
We wrote a few simple obvious filters for ahandful of exceptional cases that arise due to the factthat our sentences are from e-mail.
For example, wefiltered out best wishes expressions, which otherwisewould have been tagged as Want because of the wordwishes.The words that trigger modality occur with verydifferent frequencies.
If one is not careful, thetraining data may be dominated by only the com-monly occurring trigger words and the learned tag-ger would then be biased towards these words.
Inorder to ensure that our training data had a diverseset of examples containing many lexical triggers andnot just a lot of examples with the same lexical trig-ger, for each modality we capped the number of sen-tences from a single trigger to be at most 50.
Afterwe had the set of sentences selected by the simpletagger, we posted them on MTurk for annotation.The Turkers were asked to check a box indicat-ing that the modality was not present in the sentenceif the given modality was not expressed.
If they didnot check that box, then they were asked to highlightthe target of the modality.
Table 1 shows the numberof sentences we posted on MTurk for each modal-ity.6 Three Turkers annotated each sentence.
Werestricted the task to Turkers who were adults, hadgreater than a 95% approval rating, and had com-pleted at least 50 HITs (Human Intelligence Tasks)on MTurk.
We paid US$0.10 for each set of ten sen-tences.Since our data was annotated by three Turkers,for training data we used only those examples forwhich at least two Turkers agreed on the modalityand the target of the modality.
This resulted in 1,008examples.
674 examples had two Turkers agreeingand 334 had unanimous agreement.
We kept trackof the level of agreement for each example so that4http://bailando.sims.berkeley.edu/enron/enron.sql.gz5Data received through personal communication6More detailed statistics on MTurk annotations are availableat http://hltcoe.jhu.edu/datasets/.59Modality CountAbility 190Effort 1350Intention 1320Success 1160Want 1390Table 1: For each modality, the number of sentences re-turned by the simple tagger that we posted on MTurk.our learner could weight the examples differentlydepending on the level of inter-annotator agreement.4 Multiclass SVM for ModalityIn this section, we describe the automatic modal-ity tagger we built using the MTurk annotations de-scribed in Section 3 as the training data.
Section 4.1describes the training and evaluation data.
In Sec-tion 4.2, we present the machinery and Section 4.3describes the features we used to train the tagger.In Section 4.4, we present various experiments anddiscuss results.
Section 4.5, presents additional ex-periments using annotator confidence.4.1 DataFor training, we used the data presented in Section 3.We refer to it as MTurk data in the rest of this paper.For evaluation, we selected a part of the LU Corpus(Diab et al, 2009a) (1228 sentences) and our expertannotated it with modality tags.
We first used thehigh-recall simple modality tagger described in Sec-tion 3 to select the sentences with modalities.
Outof the 235 sentences returned by the simple modal-ity tagger, our expert removed the ones which didnot in fact have a modality.
In the remaining sen-tences (94 sentences), our expert annotated the tar-get predicate.
We refer to this as the Gold datasetin this paper.
The MTurk and Gold datasets differ interms of genres as well as annotators (Turker vs. Ex-pert).
The distribution of modalities in both MTurkand Gold annotations are given in Table 2.4.2 ApproachWe applied a supervised learning framework us-ing multi-class SVMs to automatically learn to tagModality MTurk GoldAbility 6% 48%Effort 25% 10%Intention 30% 11%Success 24% 9%Want 15% 23%Table 2: Frequency of Modalitiesmodalities in context.
For tagging, we used the Yam-cha (Kudo and Matsumoto, 2003) sequence labelingsystem which uses the SVMlight (Joachims, 1999)package for classification.
We used One versus Allmethod for multi-class classification on a quadratickernel with a C value of 1.
We report recall and pre-cision on word tokens in our corpus for each modal-ity.
We also report F?=1 (F)-measure as the har-monic mean between (P)recision and (R)ecall.4.3 FeaturesWe used lexical features at the token level which canbe extracted without any parsing with relatively highaccuracy.
We use the term context width to denotethe window of tokens whose features are consideredfor predicting the tag for a given token.
For example,a context width of 2 means that the feature vectorof any given token includes, in addition to its ownfeatures, those of 2 tokens before and after it as wellas the tag prediction for 2 tokens before it.
We didexperiments varying the context width from 1 to 5and found that a context width of 2 gives the optimalperformance.
All results reported in this paper areobtained with a context width of 2.
For each token,we performed experiments using following lexicalfeatures:?
wordStem - Word stem.?
wordLemma - Word lemma.?
POS - Word?s POS tag.?
isNumeric - Word is Numeric??
verbType - Modal/Auxiliary/Regular/Nil?
whichModal - If the word is a modal verb,which modal?60We used the Porter stemmer (Porter, 1997) to ob-tain the stem of a word token.
To determine theword lemma, we used an in-house lemmatizer usingdictionary and morphological analysis to obtain thedictionary form of a word.
We obtained POS tagsfrom Stanford POS tagger and used those tags todetermine verbType and whichModal features.
TheverbType feature is assigned a value ?Nil?
if the wordis not a verb and whichModal feature is assigned avalue ?Nil?
if the word is not a modal verb.
The fea-ture isNumeric is a binary feature denoting whetherthe token contains only digits or not.4.4 Experiments and ResultsIn this section, we present experiments performedconsidering all the MTurk annotations where twoannotators agreed and all the MTurk annotationswhere all three annotators agreed to be equally cor-rect annotations.
We present experiments applyingdifferential weights for these annotations in Section4.5.
We performed 4-fold cross validation (4FCV)on MTurk data in order to select the best featureset configuration ?.
The best feature set obtainedwaswordStem,POS,whichModal with a contextwidth of 2.
For finding the best performing fea-ture set - context width configuration, we did an ex-haustive search on the feature space, pruning awayfeatures which were proven not useful by results atstages.
Table 3 presents results obtained for eachmodality on 4-fold cross validation.Modality Precision Recall F MeasureAbility 82.4 55.5 65.5Effort 95.1 82.8 88.5Intention 84.3 61.3 70.7Success 93.2 76.6 83.8Want 88.4 64.3 74.3Overall 90.1 70.6 79.1Table 3: Per modality results for best feature set ?
on4-fold cross validation on MTurk dataWe also trained a model on the entire MTurk datausing the best feature set ?
and evaluated it againstthe Gold data.
The results obtained for each modal-ity on gold evaluation are given in Table 4.
We at-tribute the lower performance on the Gold dataset toits difference from MTurk data.
MTurk data is en-tirely from email threads, whereas Gold data con-tained sentences from newswire, letters and blogsin addition to emails.
Furthermore, the annotationis different (Turkers vs expert).
Finally, the distri-bution of modalities in both datasets is very differ-ent.
For example, Ability modality was merely 6%of MTurk data compared to 48% in Gold data (seeTable 2).Modality Precision Recall F MeasureAbility 78.6 22.0 34.4Effort 85.7 60.0 70.6Intention 66.7 16.7 26.7Success NA 0.0 NAWant 92.3 50.0 64.9Overall 72.1 29.5 41.9Table 4: Per modality results for best feature set ?
evalu-ated on Gold datasetWe obtained reasonable performances for Effortand Want modalities while the performance for othermodalities was rather low.
Also, the Gold datasetcontained only 8 instances of Success, none of whichwas recognized by the tagger resulting in a recallof 0%.
Precision (and, accordingly, F Measure) forSuccess was considered ?not applicable?
(NA), as nosuch tag was assigned.4.5 Annotation Confidence ExperimentsOur MTurk data contains sentence for which at leasttwo of the three Turkers agreed on the modality andthe target of the modality.
In this section, we investi-gate the role of annotation confidence in training anautomatic tagger.
The annotation confidence is de-noted by whether an annotation was agreed by onlytwo annotators or was unanimous.
We denote the setof sentences for which only two annotators agreed asAgr2 and that for which all three annotators agreedas Agr3.We present four training setups.
The first setupis Tr23 where we train a model using both Agr2and Agr3 with equal weights.
This is the setup weused for results presented in the Section 4.4.
Then,we have Tr2 and Tr3, where we train using onlyAgr2 and Agr3 respectively.
Then, for Tr23W , we61TrainingSetupTested on Agr2 and Agr3 Tested on Agr3 onlyPrecision Recall F Measure Precision Recall F MeasureTr23 90.1 70.6 79.1 95.9 86.8 91.1Tr2 91.0 66.1 76.5 95.6 81.8 88.2Tr3 88.1 52.3 65.6 96.8 71.7 82.3Tr23W 89.9 70.5 79.0 95.8 86.5 90.9Table 5: Annotator Confidence Experiment Results; the best results per column are boldfaced(4-fold cross validation on MTurk Data)train a model giving different cost values for Agr2and Agr3 examples.
The SVMLight package al-lows users to input cost values ci for each traininginstance separately.7 We tuned this cost value forAgr2 and Agr3 examples and found the best valueat 20 and 30 respectively.For all four setups, we used feature set ?.
We per-formed 4-fold cross validation on MTurk data in twoways ?
we tested against a combination of Agr2and Agr3, and we tested against only Agr3.
Resultsof these experiments are presented in Table 5.
Wealso present the results of evaluating a tagger trainedon the whole MTurk data for each setup against theGold annotation in Table 6.
The Tr23 tested on bothAgr2 andAgr3 presented in Table 5 and Tr23 testedon Gold data presented in Table 6 correspond to theresults presented in Table 3 and Table 4 respectively.TrainingSetup Precision Recall F MeasureTr23 72.1 29.5 41.9Tr2 67.4 27.6 39.2Tr3 74.1 19.1 30.3Tr23W 73.3 31.4 44.0Table 6: Annotator Confidence Experiment Results; thebest results per column are boldfaced(Evaluation against Gold)One main observation is that including annota-tions of lower agreement, but still above a threshold(in our case, 66.7%), is definitely helpful.
Tr23 out-performed both Tr2 and Tr3 in both recall and F-7This can be done by specifying ?cost:<value>?
after thelabel in each training instance.
This feature has not yet beendocumented on the SVMlight website.measure in all evaluations.
Also, even when evaluat-ing against only the high confident Agr3 cases, Tr2gave a high gain in recall (10 .1 percentage points)over Tr3, with only a 1.2 percentage point loss onprecision.
We conjecture that this is because thereare far more training instances in Tr2 than in Tr3(674 vs 334), and that quantity beats quality.Another important observation is the increase inperformance by using varied costs for Agr2 andAgr3 examples (the Tr23W condition).
Althoughit dropped the performance by 0.1 to 0.2 pointsin cross-validation F measure on the Enron cor-pora, it gained 2.1 points in Gold evaluation F mea-sure.
These results seem to indicate that differentialweighting based on annotator agreement might havemore beneficial impact when training a model thatwill be applied to a wide range of genres than whentraining a model with genre-specific data for appli-cation to data from the same genre.
Put differently,using varied costs prevents genre over-fitting.
Wedon?t have a full explanation for this difference inbehavior yet.
We plan to explore this in future work.5 ConclusionWe have presented an innovative way of combininga high-recall simple tagger with Mechanical Turkannotations to produce training data for a modalitytagger.
We show that we obtain good performanceon the same genre as this training corpus (annotatedin the same manner), and reasonable performanceacross genres (annotated by an independent expert).We also present experiments utilizing the number ofagreeing Turkers to choose cost values for trainingexamples for the SVM.
As future work, we plan toextend this approach to other modalities which are62not covered in this study.6 AcknowledgmentsThis work is supported, in part, by the Johns Hop-kins Human Language Technology Center of Ex-cellence.
Any opinions, findings, and conclusionsor recommendations expressed in this material arethose of the authors and do not necessarily reflectthe views of the sponsor.
We thank several anony-mous reviewers for their constructive feedback.ReferencesEmilia Apostolova, Noriko Tomuro, and Dina Demner-Fushman.
2011.
Automatic extraction of lexico-syntactic patterns for detection of negation and spec-ulation scopes.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Linguis-tics: Human Language Technologies: short papers -Volume 2, HLT ?11, pages 283?287, Portland, Oregon.Eiji Aramaki, Yasuhide Miura, Masatsugu Tonoike,Tomoko Ohkuma, Hiroshi Mashuichi, and KazuhikoOhe.
2009.
Text2table: Medical text summarizationsystem based on named entity recognition and modal-ity identification.
In Proceedings of the BioNLP 2009Workshop, pages 185?192, Boulder, Colorado, June.Association for Computational Linguistics.Kathryn Baker, Michael Bloodgood, Bonnie J. Dorr,Nathaniel W. Filardo, Lori S. Levin, and Christine D.Piatko.
2010.
A modality lexicon and its use in auto-matic tagging.
In LREC.Kathryn Baker, Michael Bloodgood, Bonnie J. Dorr,Chris Callison-Burch, Nathaniel W. Filardo, ChristinePiatko, Lori Levin, and Scott Miller.
2012.
Use ofmodality and negation in semantically-informed syn-tactic mt.
Computational Linguistics, 38(22).Roy Bar-Haim, Ido Dagan, Iddo Greental, and EyalShnarch.
2007.
Semantic inference at the lexical-syntactic level.
In Proceedings of the 22nd Na-tional Conference on Artificial intelligence - Volume 1,pages 871?876, Vancouver, British Columbia, Canada.AAAI Press.Mona Diab, Bonnie Dorr, Lori Levin, Teruko Mitamura,Rebecca Passonneau, Owen Rambow, and LanceRamshaw.
2009a.
Language Understanding Anno-tation Corpus.
Linguistic Data Consortium (LDC),USA.Mona Diab, Lori Levin, Teruko Mitamura, Owen Ram-bow, Vinodkumar Prabhakaran, and Weiwei Guo.2009b.
Committed belief annotation and tagging.
InProceedings of the Third Linguistic Annotation Work-shop, pages 68?73, Suntec, Singapore, August.
Asso-ciation for Computational Linguistics.Richa?rd Farkas, Veronika Vincze, Gyo?rgy Szarvas,Gyo?rgy Mo?ra, and Ja?nos Csirik, editors.
2010.
Pro-ceedings of the Fourteenth Conference on Computa-tional Natural Language Learning.
Association forComputational Linguistics, Uppsala, Sweden, July.Thorsten Joachims, 1999.
Making large-scale supportvector machine learning practical, pages 169?184.MIT Press, Cambridge, MA, USA.Stefan Kaufmann, Cleo Condoravdi, and ValentinaHarizanov, 2006.
Formal Approaches to Modality,pages 72?106.
Mouton de Gruyter.Angelika Kratzer.
1981.
The Notional Category ofModality.
In H. J. Eikmeyer and H. Rieser, editors,Words, Worlds, and Contexts, pages 38?74.
de Gruyter,Berlin.Angelika Kratzer.
1991.
Modality.
In Arnim von Ste-chow and Dieter Wunderlich, editors, Semantics: AnInternational Handbook of Contemporary Research.de Gruyter.Taku Kudo and Yuji Matsumoto.
2003.
Fast methodsfor kernel-based text analysis.
In 41st Meeting of theAssociation for Computational Linguistics (ACL?03),Sapporo, Japan.Marjorie McShane, Sergei Nirenburg, and RonZacharsky.
2004.
Mood and modality: Out ofthe theory and into the fray.
Natural LanguageEngineering, 19(1):57?89.Roser Morante and Walter Daelemans.
2009.
Learn-ing the scope of hedge cues in biomedical texts.
InProceedings of the BioNLP 2009 Workshop, pages 28?36, Boulder, Colorado, June.
Association for Compu-tational Linguistics.Masaki Murata, Kiyotaka Uchimoto, Qing Ma, ToshiyukiKanamaru, and Hitoshi Isahara.
2005.
Analysis ofmachine translation systems?
errors in tense, aspect,and modality.
In Proceedings of the 19th Asia-PacificConference on Language, Information and Computa-tion (PACLIC), Tapei.Rowan Nairn, Cleo Condorovdi, and Lauri Karttunen.2006.
Computing relative polarity for textual infer-ence.
In Proceedings of the International Workshop onInference in Computational Semantics, ICoS-5, pages66?76, Buxton, England.M.
F. Porter, 1997.
An algorithm for suffix stripping,pages 313?316.
Morgan Kaufmann Publishers Inc.,San Francisco, CA, USA.Vinodkumar Prabhakaran, Owen Rambow, and MonaDiab.
2010.
Automatic committed belief tagging.In Coling 2010: Posters, pages 1014?1022, Beijing,China, August.
Coling 2010 Organizing Committee.63Brian Roark.
2009.
Open vocabulary language model-ing for binary response typing interfaces.
Technicalreport, Oregon Health and Science University.Roser Sauri and James Pustejovsky.
2009.
Factbank:a corpus annotated with event factuality.
LanguageResources and Evaluation, 43(3):227?268.Roser Sauri, Marc Verhagen, and James Pustejovsky.2006.
Annotating and recognizing event modality intext.
In FLAIRS Conference, pages 333?339.Bengt Sigurd and Barbara Gawro?nska.
1994.
Modalsas a problem for MT.
In Proceedings of the 15th In-ternational Conference on Computational Linguistics(COLING) Volume 1, COLING ?94, pages 120?124,Kyoto, Japan.Johan Van Der Auwera and Andreas Ammann, 2005.Overlap between situational and epistemic modalmarking, chapter 76, pages 310?313.
Oxford Univer-sity Press.Veronika Vincze, Gy orgy Szarvas, Richa?d Farkas,Gy orgy Mora, and Ja?nos Csirik.
2008.
The Bio-Scope corpus: biomedical texts annotated for uncer-tainty, negation and their scopes.
BMC Bioinformat-ics, 9(Suppl 11):S9+.64
