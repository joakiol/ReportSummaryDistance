Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 1908?1917, Dublin, Ireland, August 23-29 2014.Recurrent Neural Network-based Tuple Sequence Modelfor Machine TranslationYouzheng Wu, Taro Watanabe, Chiori HoriNational Institute of Information and Communications Technology (NICT), Japanerzhengcn@gmail.com{taro.watanabe, chiori.hori}@nict.go.jpAbstractIn this paper, we propose a recurrent neural network-based tuple sequence model (RNNTSM)that can help phrase-based translation model overcome the phrasal independence assumption.Our RNNTSM can potentially capture arbitrary long contextual information during estimatingprobabilities of tuples in continuous space.
It, however, has severe data sparsity problem dueto the large tuple vocabulary coupled with the limited bilingual training data.
To tackle thisproblem, we propose two improvements.
The first is to factorize bilingual tuples of RNNTSMinto source and target sides, we call factorized RNNTSM.
The second is to decompose phrasalbilingual tuples to word bilingual tuples for providing fine-grained tuple model.
Our extensiveexperimental results on the IWSLT2012 test sets1showed that the proposed approach essentiallyimproved the translation quality over state-of-the-art phrase-based translation systems (baselines)and recurrent neural network language models (RNNLMs).
Compared with the baselines, theBLEU scores on English-French and English-German tasks were greatly enhanced by 2.1%-2.6% and 1.8%-2.1%, respectively.1 IntroductionThe phrase-based translation systems (Koehn et al., 2003) rely on language model and lexicalized re-ordering model to capture lexical dependencies that span phrase boundaries.
Their translation models,however, do not explicitly model context dependencies between translation units.
To address this limi-tation, Marino et al.
(2006) and Crego and Yvon (2010) proposed n-gram-based translation systems tocapture dependencies across phrasal boundaries.
The n-gram translation models have been shown to beeffective in helping the phrase-based translation models overcome the phrasal independence assumption(Durrani et al., 2013; Zhang et al., 2013).
Most of the n-gram translation models (Marino et al., 2006;Durrani et al., 2013; Zhang et al., 2013) employed Markov (n-gram) model over sequence of bilingualtuples also known as minimal translation units (MTUs).Recently, some pioneer studies (Schwenk et al., 2007; Son et al., 2012) proposed feed-forward neuralnetworks with factorizations to model bilingual tuples in a continuous space.
Although the authorsreported some gains over the n-gram model in machine translation tasks, these models can only capturea limited amount of context and remain a kind of n-gram model.
In language modeling, experimentalresults in (Mikolov et al., 2011; Arisoy et al., 2012; Sundermeyer et al., 2013) showed that recurrentneural networks (RNNs) outperform feed-forward neural networks in both perplexity and word error ratein speech recognition even though it is harder to train properly.Therefore, in this paper we take the advantages of RNN and tuple sequence model and propose re-current neural network-based tuple sequence models (RNNTSMs) to improve phrase-based translationsystem.
Our RNNTSMs are capable of modeling long-span context and have better generalization.
Com-pared with such related studies as (Schwenk et al., 2006; Son et al., 2012), our main contributions canThis work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/1The IWSLT workshop aims at translating TED speeches (http://www.ted.com), a collection of public lectures cov-ering a variety of topics.1908be summarized as: (i) our models can be regarded as deep neural network translation models becausethey can capture arbitrary-length context potentially, which are proven to estimate more accurate proba-bilities of bilingual tuples; (ii) we extend the conventional RNNTSM to factorized RNNTSMs that cansignificantly overcome the data sparseness problem caused by the large vocabularies of bilingual tuplesby incorporating the factors from the source and the target sides in addition to bilingual tuples; (iii) weinvestigate heuristic rules to decompose phrasal bilingual tuples to word bilingual tuples for reducingthe out-of-tuple-vocabulary rate and providing fine-grained tuple sequence model; (iv) we integrate theproposed models into the state-of-the-art phrase-based translation system (MOSES) as a supplement ofthe work in (Son et al., 2012) that is a complete n-gram translation system.2 Related WorkThe n-gram translation model (Marino et al., 2006) is a Markov model over phrasal bilingual tuples andcan improve the phrase-based translation system (Koehn et al., 2003) by providing contextual depen-dencies between phrase pairs.
To further improve the n-gram translation model, Crego and Yvon (2010)explored factored bilingual n-gram language models.
Durrani et al.
(2011) proposed a joint sequencemodel for the translation and reordering probabilities.
Zhang et al.
(2013) explored multiple decomposi-tion structures as well as dynamic bidirectional decomposition.
Since neural networks advance the stateof the art in the fields of image processing, acoustic modeling (Seide et al., 2011), language modeling(Bengio et al., 2003), natural language processing (Collobert et al., 2011; Socher et al., 2013), machinetransliteration (Deselaers et al., 2009), etc, some prior studies have been done on neural network-basedtranslation models (NNTMs).One kind of the NNTMs relies on word-to-word alignment information or phrasal bilingual tuples.
Forexample, Schwenk et al.
(2007) investigated feed-forward neural networks to model bilingual tuples incontinuous space.
Son et al.
(2012) improved this idea by decomposing tuple units, i.e., distinguishing thesource and target sides of the tuple units, to address data sparsity issues.
Although the authors reportedsome gains over the n-gram model in the BLEU scores on some tasks, these models can only capturea limited amount of context and remain a kind of n-gram model.
In addition, a feed-forward neuralnetwork independent from bilingual tuples was proposed (Schwenk, 2012), which can infer meaningfultranslation probabilities for phrase pairs not seen in the training data.Another kind of the NNTMs do not rely on alignment.
Auli et al.
(2013) and Kalchbrenner andBlunsom (2013) proposed joint language and translation model with recurrent neural networks, in whichlatent semantic analysis and convolutional sentence model were used to model source-side sentence.Potentially, they can exploit an unbounded history of both source and target words thanks to recurrentconnections.
However, they only modestly observed gains over the recurrent neural network languagemodel.
Previous studies (Wu and Wang, 2007; Yang et al., 2013) showed that the performance of wordalignment (alignment error rate) is nearly 80%.
That means explicit word alignment may be more reliableas a way to represent the corresponding bilingual sentences compared with an implicit compressed vectorrepresentation (Auli et al., 2013).Our RNNTSM takes the advantages of the above NNTMs, that is, RNN enables our model to cap-ture long-span contextual information, while tuple sequence model uses word alignment without muchinformation loss.
Furthermore, factorized RNN and word bilingual tuples are proposed to address datasparsity issue.
To the best of our knowledge, few studies have been done on this aspect.3 Tuple Sequence ModelIn tuple sequence model, bilingual tuples are translation units extracted from word-to-word alignment.They are composed of source phrases and their aligned target phrases that are also known as minimaltranslation units (MTUs) and thus cannot be broken down any further without violating the constrainsof the translation rules.
This condition results in a unique segmentation of the bilingual sentence pairgiven its alignment.
In our implementation, GIZA++ with grow-diag-final-and setting is usedto conduct word-to-word alignments in both directions, source-to-target and target-to-source (Och and1909DZRUGWRZRUGDOLJQHGUHVXOWIURP*,=$ZLWKJURZGLDJILQDODQGFRPSRVHUVDQGRQW ?W? LQWHUURJ?VDIULFDQ DPHULFDQ PXVLFLDQVOHV musiciens DIURDP?ULFDLLQWHUYLHZHGZHUHHW FRPSRVLWHXUVXPXVLFLDQVOHVmusiciensWVEVHTXHQFHRISKUDVDOELOLQJXDOWXSOHVDIULFDQDPHULFDQDIURDP?ULFDLVXWXDQGHWVWXFRPSRVHUVFRPSRVLWHXUVVWXZHUHLQWHUYLHZHGRQW?W?LQWHUURJ?VVWDIULFDQDIURDP?ULFDLVX X X X XPXVLFLDQVOHVW WDQGHWFRPSRVHUVFRPSRVLWHXUVZHUHLQWHUYLHZHGRQW?W?LQWHUURJ?VVV V VW WWXPXVLFLDQVPXVLFLHQVWV DPHULFDQDIURDP?ULFDLVXWFVHTXHQFHRIZRUGELOLQJXDOWXSOH,DIULFDQDIURDP?ULFDLVX X X X XPXVLFLDQVOHVWWDQGHWFRPSRVHUVFRPSRVLWHXUVZHUHRQWVV V VW WWXPXVLFLHQVWV DPHULFDQVX XLQWHUYLHZHG?W?VWXLQWHUURJ?VVWGVHTXHQFHRIZRUGELOLQJXDOWXSOH,,FRS\FRS\18// 18//18//WLQVHUWLQVHUW LQVHUWFigure 1: An example of generating basic bilingual tuples from word alignment information.Ney, 2003).
Ncode toolkit2is used to generate a unique bilingual segmentation of word-to-word alignedsentence.
Figure 1(a)-(b) illustrates the process of generating bilingual tuple.
As can be seen in Figure1, bilingual tuple u1is composed of source phrase s?1(musicians) and target phrase?t1(les musiciens)linked to s?1.
Because this type of bilingual tuples are composed of one or more words from the sourceside and zero or more words from the target side, we call them phrasal bilingual tuples.The phrasal bilingual tuple is not able to provide translations for individual words that appear tied toother words unless they occur alone in some other tuple.
For example, if target phrase?tk=?les musiciens?is always aligned to source phrase s?k=?musicians?
in the training corpus, then no word-to-word trans-lation probability for ?musicians:musiciens?
will exist.
This becomes a serious drawback when a largenumber of phrasal bilingual tuples are extracted from one-to-many, many-to-one, and many-to-manyalignments.
To tackle the issue, we propose to decompose phrasal bilingual tuples into word bilingualtuples for providing fine-grained tuple sequence model.
Suppose source phrase s?k, a sequence of sourceword sk1, sk2, ..., skI, is aligned to target phrase?tk, a sequence of target word tk1, tk2, ..., tkJ, in whichI and J refer to the number of words in source phrase and that in target phrase.
The following two typesof heuristic rules are considered.
(word-bilingual-tuple-I): For one-to-many alignments, we copy skIJ ?
1 times to fill the shortphrase s?k.
For many-to-one alignments, we copy tkJI ?
1 times to fill the phrase?tk.
For many-to-manyalignment, a maximum phrase length, we set it to 5, is used to avoid vocabulary explosion.
That means,if I > 5; then s?k=?unk?, if J > 5; then?tk=?unk?.
(word-bilingual-tuple-II): For one-to-many, many-to-one, and many-to-many alignments, we insert aspecial token ?NULL?
| J ?
I | times to fill the short phrase, and map each word in the extended phrasemonotonically to generate a word-wise tuple sequence.The Figure 1(c)-(d) demonstrate the decomposition results.
As shown in Figure 1(c), the translationprobability of ?musicians?
being aligned to ?musiciens?
can be learned in the word bilingual tuples.
Theword bilingual tuples enable our model use information from source-side of the tuples for computingtranslation probabilities of some tuples.
For example, translating ?musicians:musiciens?
benefits fromits source word ?musicians?.
Table 2 in Section 4 shows the sizes of the tuple vocabularies.
We can see2http://ncode.limsi.fr/1910hk-1hidden layer, hkdelay copy??
?W Uinput layer, xkoutput layer, ykand:etandetuk-1sk-1tk-1Distributionon sourcephrases??????????
?p(?|c(sk), xk)????
?Distributionon classes ofsource phrasesP(?|xk)hk-1hidden layer, hkdelay copy??
?W Uinput layer, xkand:etandetuk-1sk-1tk-1output layer, ykDistribution ontarget phrases???????????????
?Distribution onclasses of targetphrasesP(?|xk)p(?|c(tk), xk)?composersskhk-1hidden layer, hkdelay copy??
?W Uinput layer, xkoutput layer, ykand:etandetuk-1sk-1tk-1Distributionon tuples??????????
?p(?|c(sk), xk)????
?Distributionon classes oftuplesP(?|xk)(a)                                                                           (b)                                                                           (c)Figure 2: (a): factorized RNNTSM, called fRNNTSM for short, which will go back to the RNNTSMmodel when s?k?1and?tk?1are dropped.
(b): fRNNTSMsource.
(c) fRNNTSMtarget.that the word-bilingual-tuple-I has lower out-of-tuple-vocabulary (OOTV) rate, though it increases thetuple vocabulary.
The word-bilingual-tuple-II greatly reduces the tuple vocabulary and the OOTV rate.Note that some words may not be aligned correctly, like ?NULL-musiciens?.
However, generating thesetuples can be viewed as a language model process that exploits previous source and target words, andcurrent source word contained in previous tuples like ?les-musiciens?.Thus, given a target sentence t, a source sentence s, and its alignment a, the tuple sequence model canbe defined over the sequence of bilingual tuples (u1, u2, ..., um) as follows.p(t, s, a) =m?k=1p(uk|uk?1, uk?2, ..., u1) =m?k=1p(uk|uk?1, uk?2, ..., uk?n+1) (1)where ukdenotes the k-th bilingual tuple of a given bilingual sentence pair.
Each bilingual tuple ukcontains a source phrase s?kand its aligned target phrase?tk3.
Formally, uk=s?k:?tk.
The tuple sequencemodel does not make any phrasal independence assumption and generates a tuple by looking at a contextof previous tuples.
The n-gram translation models are Markov models over sequences of tuples, theygenerate a tuple by looking at previous n-1 tuples.4 Recurrent Neural Network-based Tuple Sequence ModelIn order to use long-span context, this paper presents a recurrent neural network-based tuple sequencemodel (RNNTSM) to approximate the probability p(ui|ui?1, ..., u1).
Our RNNTSM can potentiallycapture arbitrary long context rather than n-1 previous tuples.
The input layer encodes bilingual tuplesby using 1-of-n coding, and the output layer produces a probability distribution over all bilingual tuples.The hidden layer maintains a representation of the sentence history.
This RNNTSM, however, has severedata sparsity problem due to the large tuple vocabulary coupled with the limited bilingual training data.4.1 Factorized RNNTSMTo solve the problem, we extend the RNNTSM model with factorizing tuples in input layer, as shownin Figure 2(a).
Specifically, it consists of an input layer x, a hidden layer h (state layer), and an out-put layer y.
The connection weights among layers are denoted by matrixes U and W. Unlike theRNNTSM, which predicts probability p(uk|uk?1, hk?1), the factorized RNNTSM predicts probabili-ty p(uk|uk?1, s?k?1,?tk?1, hk?1) of generating following tuple ukand is explicitly conditioned on thepreceding tuple uk?1, source-side of the tuple s?k?1, and target-side of the tuple?tk?1.
It is implicitlyconditioned on the entire history by the delay copy of hidden layer hk?1.
For those tuples (approximate-ly 20% as shown in Table 2) that are not contained in the training data, i.e., co-occurrence (si?1, ti?1)3Phrases turn to words in the word bilingual tuples.
For convenience, we do not distinguish them in our paper.1911non-exist while either si?1or ti?1exists, the factorized RNNTSM backs off to the source- (si?1) ortarget-side (ti?1).
This process resembles factored n-gram language model (Duh and Kirchhoff, 2004).However, the RNNTSM, computing p(ui|ui?1, hi?1), cannot estimate the probabilities for those tuples.In the special case that s?k?1and?tk?1are dropped, the factorized RNNTSM goes back to the RNNTSM.For convenience, uk?1, s?k?1and?tk?1are called features.
In the input layer, each feature is encodedinto a feature vector using the 1-of-n coding.
The tuple uk?1, the source phrase s?k?1and the targetphrase?tk?1are encoded into |u|-dimension feature vector vuk?1, |s?|-dimension feature vector vs?k?1and|?t|-dimension feature vector v?tk?1, respectively.
Here, |u|, |s?| and |?t| stand for the sizes of the tuple, thesource phrase, and the target phrase vocabularies.
Finally, the input layer xkis formed by concatenatingfeature vectors and hidden layer hk?1at the preceding time step, as shown in the following equation.xk= [vuk?1, vs?k?1, v?tk?1, hk?1](2)The neurons in the hidden and output layers are computed as follows:hk= f(U ?
xk), yk= g(W ?
hk)f(z) =11 + e?z, g(z) =ezm?kezk(3)To speed-up both in the training and testing processes, we map bilingual tuples into classes withfrequency binning and divide the output layer into two parts following (Mikolov et al., 2010).
The firstpart estimates the posterior probability distribution over all classes.
The second computes the posteriorprobability distribution over the tuples that belong to class c(uk), the one that contains predicted tupleuk.
Finally, translation probability p(uk|uk?1, s?k?1,?tk?1, hk?1) is calculated by,p(uk|uk?1, s?k?1,?tk?1, hk?1) = p(c(uk)|xk) ?
p(uk|c(uk), xk) (4)4.2 Factorized RNNTSM on source and target phrasesThe above factorized RNNTSM is conditioned on the previous context during computing the probabilityfor tuple uk.
It does not exploit its source side s?k.
For example, tuple ?composers:compositeurs?
doesnot benefit from ?composers?.
To address this limitation, we rewrite the probability in Equation 1.p(uk|uk?1, uk?2, ..., u1) = p(sk, tk|uk?1, uk?2, ..., u1)= p(sk|uk?1, uk?2, ..., u1) ?
p(tk|sk, uk?1, uk?2, ..., u1)(5)The first sub-model p(sk|uk?1, uk?2, ..., u1) computes the probability distribution over source phras-es.
This model, called fRNNTSMsourcefor short, can be regarded as a reordering model.
The secondsub-model p(tk|sk, uk?1, uk?2, ..., u1) is a translation model, abbreviated as fRNNTSMtarget, whichcomputes the probability distribution over?tkthat are translated from s?k.
The two sub-models are com-puted with the recurrent neural networks shown in Figure 2(b)-(c).
Another advantage of using thefactorized RNNTSM on source and target phrases separately is that their training become faster becausethe vocabulary sizes of the source and target phrases are much smaller than that of the tuples.4.3 TrainingTraining can be performed by the back-propagation through time (BPTT) algorithm (Boden, 2002) byminimizing an error function defined in the following equations.L =12?N?i=1(oi?
pi)2+ ?
?
(?lku2lk+?tlw2tl) (6)where N is the number of training instances, oidenotes the desired output; i.e., the probability shouldbe 1.0 for the predicted tuple in the training sentence and 0.0 for all others.
?
is the regularization term?sweight, which is determined experimentally using a validation set.
The training algorithm randomly ini-tializes the matrixes and updates them with Equation 7 over all the training instances in several iterations.1912English-French English-Germantst2010 tst2011 tst2012 tst2010 tst2011 tst2012Baseline 30.15 35.97 35.48 20.29 21.48 19.30+RNNTSM 30.51(0.3)36.11(0.1)36.44(0.9)20.67(0.4)21.85(0.4)19.56(0.3)+fRNNTSM (1) 31.83(1.6)37.58(1.6)37.74(2.2)21.67(1.4)22.89(1.4)20.60(1.3)+fRNNTSMsource(2)+fRNNTSMtarget(3)31.89(1.7)38.23(2.2)37.82(2.3)21.49(1.2)22.94(1.4)20.41(1.1)+(1) +(2) +(3) 32.26(2.1)38.36(2.4)38.11(2.6)21.80(1.5)22.88(1.4)20.76(1.5)Table 1: BLEU scores of the RNNTSMs, the factorized RNNTSM (fRNNTSM), the fRNNTSMsource(sfRNNTSM), the fRNNTSMtargetwith the word-bilingual-tuple-I and their combination.
The num-bers in the parentheses are the absolute improvements over the Baseline.In Equation 7, ?
stands for one of the connection weights in the neural networks and ?
is the learningrate.
After each iteration, it uses validation data for stopping and controlling the learning rate.
Usually,our RNNs needs 10 to 20 iterations.
?new= ?previous?
?
??L??
(7)5 ExperimentsWe experiment with two language pairs on the IWSLT2012 data sets (Federico et al., 2012), with Englishas source and French, German as target.
The IWSLT data comes from TED speecheds, given by leadersin various fields and covering an open set of topics in technology, entertainment, design, and many others.In the following experiments, the IWSLT dev2010 set is used as the tuning set, the tst2010, tst2011, andtst2012 as the test sets.Phrase-based translation systems are constructed as baselines using standard settings (GIZA++ align-ment, grow-diag-final-and, lexical reordering models, SRILM, and MERT optimizer) in the MOSEStoolkit (Koehn et al., 2007).
The proposed models are used to re-score n-best lists produced by the base-line systems.
The n-best size is set to at most 1000 for each test sentence.
During the n-best re-scoring,the weights are re-tuned on the dev2010 data set with MERT optimizer4.
The proposed RNN-basedmodels are evaluated on a small task and a large task.
For the parameters of all the RNN-based models,we set the number of hidden neurons in the hidden layer to 480 and classes in the output layer to 300.5.1 Small TaskIn the small task, the training data only contains the speech-style bi-text, i.e., the human translation of T-ED speeches.
Specially, the corpora for the English-French and English-German pairs contain 139K and128K parallel sentences.
The language model is a standard 4-gram language model with the Kneser-Neydiscounting.
Both the n-gram LM and the RNNLM are trained on the target side of the bi-text corpus.As the first experiment, we compare the proposed RNNTSMs with the word-bilingual-tuple-I.
Table 1summarizes the results.
The main findings from this experiment are: (1) The RNNTSM yields modestimprovements of 0.3%-0.4% over the baseline system onmost the test sets.
(2) The factorized RNNTSM-s essentially outperform the baseline and the RNNTSM on all the test sets.
Specially, the improvementsof the factorized RNNTSM and the combination of the fRNNTSMsourceand the fRNNTSMtargetoverthe baseline for the English-French task range 1.6%-2.2% and 1.7%-2.3%.
For the English-German pair,these improvements are between 1.3%-1.4% and 1.1%-1.4%.
The results indicate that the factorizedRNNTSMs can well address the data sparsity problem of the RNNTSM.
(3) The improvements for theEnglish-German pair are comparatively smaller than that for the English-French pair.
This is becauseGerman is a morphologically rich language (Fraser et al., 2013), its vocabulary is larger and the sparsity4To get statistically reliable comparison (Clark et al., 2011), replication of the MERT optimizer and test set evaluation areperformed five times.
We finally report the average BLEU scores in the following experiments.1913English-French English-German#Tuple #Source/#Target OOTV #Tuple #Source/#Target OOTVPhrasal bilingual tuple 308K 130K/175K 26.0% 315K 148K/196K 23.4%word-bilingual-tuple-I 332K 100K/111K 23.7% 351K 104K/135K 22.2%word-bilingual-tuple-II 293K 44K/56K 14.9% 327K 43K/86K 14.8%Table 2: Vocabulary sizes.
OOTV refers to the out-of-tuple-vocabulary rate on the dev2010 set.
K standsfor thousands.English-French English-Germantst2010 tst2011 tst2012 tst2010 tst2011 tst2012+fRNNTSMp31.44 37.68 37.34 20.76 21.80 19.57+fRNNTSMI31.83(0.4)37.58(?0.1)37.74(0.4)21.67(0.9)22.89(1.1)20.60(1.0)+fRNNTSMII31.73(0.3)37.66 37.78(0.4)22.00(1.2)23.24(1.4)21.09(1.5)+fRNNTSMI+fRNNTSMII31.98(0.6)37.97(0.3)38.14(0.6)22.19(1.4)23.25(1.4)21.17(1.7)Table 3: BLEU scores of the factorized RNNTSM with various types of bilingual tuples.
fRNNTSMprefers to the fRNNTSM with phrasal bilingual tuples, fRNNTSMIto the fRNNTSM with the word-bilingual-tuple-I, etc.
+ means these models are used with the baseline systems.
The numbers in theparentheses are the absolute improvements over the +fRNNTSMp.problem is more serious.
(4) There is no significant difference between the factorized RNNTSM and thecombination of the fRNNTSMsourceand the fRNNTSMtargeton most of the test sets except for thetst2011 set of the English-French task.
However, the BLEU scores are modestly improved by combiningthe three factorized RNNTSMs.The second experiment is to compare the phrasal bilingual tuples and the word bilingual tuples.
Table2 lists the vocabulary sizes of the tuples, source and target phrases.
For the word-bilingual-tuple-I, thebilingual tuple vocabulary size increases by 10% in both the English-French and the English-Germanpairs.
Compared with the phrasal bilingual tuples, the bilingual tuple vocabulary size in the word-bilingual-tuple-II slightly changes.
In addition, decomposing the tuples is capable to greatly reducethe out-of-tuple-vocabulary rate by approximately 50% in the word-bilingual-tuple-II.
Table 3 comparesbilingual tuples in terms of BLEU scores.
It can be clearly seen that both the word-bilingual-tuple-I andthe word-bilingual-tuple-II achieve better performance than the phrasal bilingual tuple on most of thetest sets.
The BLEU improvements of the word-bilingual-tuple-II over the phrasal tuple range 1.2-1.5points on the English-German task.
The main reason may be lie in: the decomposition can provide word-to-word translation probabilities (such as ?musicians:musiciens?
in the example of Section 2) for thosenon-one-to-one alignments.
Thus the translation system will have a translation option for an isolatedoccurrence of such words.
Another important observation is that the decomposition performs differentlyon the English-French and English-German tasks.
For example, there exists slight difference betweenthe word-bilingual-tuple-I and the word-bilingual-tuple-II for the English-French task.
However, for theEnglish-German task, the word-bilingual-tuple-II significantly outperforms the word-bilingual-tuple-Iby 0.4 BLEU scores.
Lastly, we achieve modest improvements by combining the two types of wordbilingual tuples.This paper proposes three factorized RNNTSMs and two types of word bilingual tuples.
In this ex-periment, we combine all of them (+Combination contains 6 models) and compare with RNN-basedlanguage model (Mikolov et al., 2010).
Table 4 summarizes the results.
As shown in Table 4 and Ta-ble 1, the combination can further enhance the performance on the English-German task.
For example,the combination improves the factorized RNNTSM with the word-bilingual-tuple-I from 20.76 to 21.29on the tst2012 set of the English-German task.
Moreover, the combination significantly outperformsthe RNNLM.
The improvements over the RNNLMs on all test sets range 0.7-1.2 BLEU scores.
The1914English-French English-Germantst2010 tst2010 tst2012 tst2010 tst2010 tst2012Baseline 30.15(?1.2)35.97(?1.2)35.48(?1.5)20.29(?0.8)21.48(?0.9)19.30(?0.8)+RNNLM 31.43 37.23 37.04 21.14 22.39 20.08+Combination 32.10(0.7)38.04(0.8)37.75(0.8)22.13(1.0)23.64(1.2)21.29(1.2)Table 4: BLEU scores of the combination of our proposed models and RNNLM in the small task.
Thenumbers in the parentheses are the absolute improvements over the RNNLM.English-French English-Germantst2010 tst2010 tst2012 tst2010 tst2010 tst2012Baseline 32.92(?1.0)38.67(?1.2)39.41(?1.4)22.29(?0.5)23.67(?0.4)20.83(?0.7)+RNNLM 33.93 39.90 40.82 22.80 24.12 21.49+Combination 34.24(0.3)40.37(0.5)40.92(0.1)23.61(0.8)25.18(1.1)22.64(1.1)Table 5: BLEU scores of the combination of our proposed models and RNNLM.
The numbers in theparentheses are the absolute improvements over the RNNLM.improvement over the baseline are between 1.9-2.3 BLEU points.5.2 Large TaskIn the large task, the training data includes both speech-style and text-style bi-text corpora.
The text-stylebi-text corpora are collected from the WMT2012 campaign5, including CommonCrawl, NewsCommen-tary, and Europarl.
Totally, the numbers of the parallel sentences are 4.35M for the English-Frenchtask and 3.85M for the English-German task.
The language model is obtained by linear interpolationof several 4-gram models trained on the target side of bi-text corpora and the LDC French Gigawordcorpus.Table 5 reports the results.
+Combination means the combination of six models, as described in Table4.
We can observe that: (1) The combination of the proposed RNNTSMs only trained on the speech-style data can essentially enhance the baselines by 1.2-1.8 BLEU points.
(2) The improvements over theRNNLMs are significant on the English-German task but these improvements are modest on the English-French task.
Note that the factorized RNNTSMs and the RNNLMs in the large task are also only trainedthe speech-style parallel corpus.
In future work, we will train them on a bigger corpus, which can beexpected to further increase the performance (Auli et al., 2013; Wu et al., 2012).6 ConclusionMost prior neural network-based translation models either employ feed-forward neural networks to ex-plicitly integrate source information via word-to-word alignment, or use recurrent neural networks inwhich source information is implicitly represented with a compressed vector.
In this paper, we presentrecurrent neural network-based tuple sequence models (RNNTSMs) to compute probabilities of bilin-gual tuples in continuous space.
One of major advantages is their potential to capture long-span historycompared with feed-forward neural networks.
In addition, our models can well address the data sparsityproblem thanks to the fine-grained word bilingual tuples and the factorized recurrent neural networks.
Ascan be concluded from the experimental results on the IWSLT2012 test sets, our factorized RNNTSMswith the proposed bilingual tuples can essentially improve the BLEU scores for the English-French andEnglish-German tasks.We plan to incorporate re-ordering and syntactic features into RNNTSMs and evaluate them on distantlanguage pairs, such as English-Chinese (Japanese) tasks in the future.
Moreover, we will prune largetuple vocabulary and speed up the training on bigger data.5http://www.statmt.org/wmt12/translation-task.html1915ReferencesEbru Arisoy, Tara N. Sainath, Brian Kingsbury, and Bhuvana Ramabhadran.
2012.
Deep neural network languagemodels.
In Proceedings of NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model?
Onthe Future of Language Modeling for HLT, pages 20?28.Michael Auli, Galley Michel, Quirk Chris, and Zweig Geoffrey.
2013.
Joint language and translation modelingwith recurrent neural networks.
In Proceedings of EMNLP2013, pages 1044?1054.Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and Christian Jauvin.
2003.
A neural probabilistic languagemodel.
In Journal of Machine Learning Research, pages 1137?1155.Mikael Boden.
2002.
A guide to recurrent neural networks and backpropagation.
Technical report.Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A. Smith.
2011.
Better hypothesis testing for statisticalmachine translation: Controlling for optimizer instability.
In Proceedings of ACL 2011, pages 176?181.Ronan Collobert, Jason Weston, Leon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011.Natural language processing (almost) from scratch.
Journal of Machine Learning Research, 12(3):2493?2537.Josep M. Crego and Francois Yvon.
2010.
Factored bilingual n-gram language models for statistical machinetranslation.
Machine Translation, Special Issue: Pushing the frontiers of SMT, 24(2):159?175.Thomas Deselaers, Sasa Hasan, Oliver Bender, and Hermann Ney.
2009.
A deep learning approach to machinetransliteration.
In Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 233?241.Kevin Duh and Katrin Kirchhoff.
2004.
Automatic learning of language model structure.
In Proceedings ofCOLING 2004, pages 148?154.Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011.
A joint sequence translation model with integratedreordering.
Proceedings of ACL 2011, pages 1045?1054.Nadir Durrani, Alexander Fraser, Helmut Schmid, Hieu Hoang, and Philipp Koehn.
2013.
Can markov modelsover minimal translation units help phrase-based smt?
Proceedings of ACL 2013, pages 399?405.Marcello Federico, Mauro Cettolo, Luisa Bentivogli, Michael Paul, and Sebastian Stuker.
2012.
Overview of theiwslt 2012 evaluation campaign.
In Proceedings of IWSLT 2012.Alexander Fraser, Helmut Schmid, Richard Farkas, Renjing Wang, and Hinrich Schutze.
2013.
Knowledgesources for constituent parsing of german, a morphologically rich and less-configurational language.
Computa-tional Linguistics, 39(1):57?85.Nal Kalchbrenner and Phil Blunsom.
2013.
Recurrent continuous transaltion models.
In Proceedings ofEMNLP2013.Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003.
Statistical phrase-based translation.
Proceedings ofNAACL 2003, pages 48?54.Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, and etc.
2007.
Moses: open source toolkit forstatistical machine translation.
In Proceedings of ACL2007 on Interactive Poster and Demonstration Sessions,pages 177?180.Jose B. Marino, Rafael E. Banchs, Josep M. Crego, Adria de Gispert, Patrik Lambert, Jose A.R.
Fonollosa, andMarta R. Costa-jussa.
2006.
N-gram-based machine translation.
In Computational Linguistics, volume Volume32 Issue 4, pages 527?549.Tomas Mikolov, Martin Karafiat, Lukas Burget, Jan. H. Cernocky, and Sanjeev Khudanpur.
2010.
Recurrentneural network based language model.
In Proceedings of INTERSPEECH 2010, pages 1045?1048.Tomas Mikolov, Anoop Deoras, Stefan Kombrink, Lukas Burget, and Jan Honza Cernocky.
2011.
Empiricalevaluation and combination of advanced language modeling techniques.
In Proceedings of INTERSPEECH2011, pages 605?608.Franz Josef Och and Hermann Ney.
2003.
A systematic comparison of various statistical alignment models.
InComputational Linguistics, volume 29(1), pages 19?51.Holger Schwenk, Daniel Dchelotte, and Jean-Luc Gauvain.
2006.
Continuous space language models for statisti-cal machine translation.
In Proceedings of COLING/ACL 2006, pages 723?730.1916Holger Schwenk, Marta R. Costa-jussa, and Jose A. R. Fonollosa.
2007.
Smooth bilingual n-gram translation.
InProceedings of EMNLP/HLT 2007, pages 430?438.Holger Schwenk.
2012.
Continuous space translation models for phrase-based statistical machine translation.
InProceedings of COLING 2012, pages 1071?1080.Frank Seide, Gang Li, Xie Chen, and Dong Yu.
2011.
Feature engineering in context-dependentdeep neuralnetworks for conversational speech transcription.
In Proceedings of ASRU 2011, pages 24?29.Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Chris Manning, Andrew Ng, and Chris Potts.
2013.Recursive deep models for semantic compositionality over a sentiment treebank.
In Proceedings of EMNLP2013.Le Hai Son, Alexandre Allauzen, and Francois Yvon.
2012.
Continuous space translation models with neuralnetworks.
In Proceedings of HLT-NAACL 2012, pages 39?48.Martin Sundermeyer, Ilya Oparin, Jean-Luc Gauvain, Ben Freiberg, Ralf Schlter, and Hermann Ney.
2013.
Com-parison of feedforward and recurrent neural network language models.
In Proceedings of ICASSP 2013, pages8430?8433.Hua Wu and Haifeng Wang.
2007.
Comparative study of word alignment heuristics and phrase-based smt.
InProceedings of MT SUMMIT XI, pages 305?312.Youzheng Wu, Xugang Lu, Hitoshi Yamamoto, Shigeki Matsuda, Chiori Hori, and Hideki Kashioka.
2012.
Fac-tored language model based on recurrent neural network.
In Proceedings of COLING 2012, pages 2835?2850.Nan Yang, Shujie Liu, Mu Li, Ming Zhou, and Nenghai Yu.
2013.
Word alignment modeling with contextdependent deep neural network.
In Proceedings of ACL2013, pages 166?175.Hui Zhang, Kristina Toutanova, Chris Quirk, and Jianfeng Gao.
2013.
Beyond left-to-right: Multiple decomposi-tion structures for smt.
Proceedings of NAACL-HLT 2013, pages 12?21.1917
